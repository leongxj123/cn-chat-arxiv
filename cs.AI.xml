<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#27425;&#24615;&#30340;3D&#24863;&#30693;&#32918;&#20687;&#21160;&#30011;&#26041;&#27861;Export3D&#65292;&#36890;&#36807;&#24341;&#20837;&#19977;&#24179;&#38754;&#29983;&#25104;&#22120;&#21644;&#23545;&#27604;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#25511;&#21046;&#32473;&#23450;&#32918;&#20687;&#22270;&#20687;&#30340;&#38754;&#37096;&#34920;&#24773;&#21644;&#25668;&#20687;&#26426;&#35270;&#35282;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#34920;&#36798;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2404.00636</link><description>&lt;p&gt;
&#23398;&#20064;&#29983;&#25104;&#26465;&#20214;&#21270;&#19977;&#24179;&#38754;&#29992;&#20110;3D&#24863;&#30693;&#34920;&#24773;&#21487;&#25511;&#32918;&#20687;&#21160;&#30011;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Conditional Tri-plane for 3D-aware Expression Controllable Portrait Animation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#27425;&#24615;&#30340;3D&#24863;&#30693;&#32918;&#20687;&#21160;&#30011;&#26041;&#27861;Export3D&#65292;&#36890;&#36807;&#24341;&#20837;&#19977;&#24179;&#38754;&#29983;&#25104;&#22120;&#21644;&#23545;&#27604;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#25511;&#21046;&#32473;&#23450;&#32918;&#20687;&#22270;&#20687;&#30340;&#38754;&#37096;&#34920;&#24773;&#21644;&#25668;&#20687;&#26426;&#35270;&#35282;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#34920;&#36798;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#27425;&#24615;&#30340;3D&#24863;&#30693;&#32918;&#20687;&#21160;&#30011;&#26041;&#27861;Export3D&#65292;&#33021;&#22815;&#25511;&#21046;&#32473;&#23450;&#32918;&#20687;&#22270;&#20687;&#30340;&#38754;&#37096;&#34920;&#24773;&#21644;&#25668;&#20687;&#26426;&#35270;&#35282;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#19977;&#24179;&#38754;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#23558;3DMM&#30340;&#34920;&#24773;&#21442;&#25968;&#36716;&#31227;&#21040;&#28304;&#22270;&#20687;&#20013;&#30452;&#25509;&#29983;&#25104;3D&#20808;&#39564;&#30340;&#19977;&#24179;&#38754;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#21487;&#24494;&#20998;&#20307;&#31215;&#28210;&#26579;&#23558;&#19977;&#24179;&#38754;&#35299;&#30721;&#20026;&#19981;&#21516;&#35270;&#35282;&#30340;&#22270;&#20687;&#12290;&#29616;&#26377;&#30340;&#32918;&#20687;&#21160;&#30011;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#22270;&#20687;&#21464;&#24418;&#26469;&#22312;&#36816;&#21160;&#31354;&#38388;&#20013;&#20256;&#36755;&#34920;&#24773;&#65292;&#25361;&#25112;&#22312;&#22806;&#35266;&#21644;&#34920;&#24773;&#30340;&#20998;&#31163;&#19978;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#26080;&#22806;&#35266;&#34920;&#24773;&#21442;&#25968;&#30340;&#23545;&#27604;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#28040;&#38500;&#20102;&#22312;&#20256;&#36755;&#36328;&#36523;&#20221;&#34920;&#36798;&#26102;&#19981;&#33391;&#22806;&#35266;&#20132;&#25442;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#33021;&#22815;&#23398;&#20064;&#38544;&#34255;&#22312;3DMM&#20013;&#30340;&#26080;&#22806;&#35266;&#34920;&#36798;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00636v1 Announce Type: cross  Abstract: In this paper, we present Export3D, a one-shot 3D-aware portrait animation method that is able to control the facial expression and camera view of a given portrait image. To achieve this, we introduce a tri-plane generator that directly generates a tri-plane of 3D prior by transferring the expression parameter of 3DMM into the source image. The tri-plane is then decoded into the image of different view through a differentiable volume rendering. Existing portrait animation methods heavily rely on image warping to transfer the expression in the motion space, challenging on disentanglement of appearance and expression. In contrast, we propose a contrastive pre-training framework for appearance-free expression parameter, eliminating undesirable appearance swap when transferring a cross-identity expression. Extensive experiments show that our pre-training framework can learn the appearance-free expression representation hidden in 3DMM, and 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#34701;&#21512;&#35270;&#35273;&#12289;&#26412;&#20307;&#24863;&#30693;&#21644;&#35821;&#35328;&#30340;&#22823;&#33041;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#27979;&#32534;&#30721;&#21644;&#20027;&#21160;&#25512;&#26029;&#30340;&#26694;&#26550;&#65292;&#22522;&#20110;&#33258;&#30001;&#33021;&#21407;&#29702;&#65292;&#23454;&#29616;&#20102;&#35821;&#35328;&#32452;&#21512;&#24615;&#21644;&#24863;&#35273;&#36816;&#21160;&#25216;&#33021;&#30340;&#32852;&#21512;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.19995</link><description>&lt;p&gt;
&#36890;&#36807;&#20132;&#20114;&#23398;&#20064;&#35821;&#35328;&#21644;&#26426;&#22120;&#20154;&#21160;&#20316;&#23454;&#29616;&#32452;&#21512;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
Development of Compositionality and Generalization through Interactive Learning of Language and Action of Robots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19995
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#34701;&#21512;&#35270;&#35273;&#12289;&#26412;&#20307;&#24863;&#30693;&#21644;&#35821;&#35328;&#30340;&#22823;&#33041;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#27979;&#32534;&#30721;&#21644;&#20027;&#21160;&#25512;&#26029;&#30340;&#26694;&#26550;&#65292;&#22522;&#20110;&#33258;&#30001;&#33021;&#21407;&#29702;&#65292;&#23454;&#29616;&#20102;&#35821;&#35328;&#32452;&#21512;&#24615;&#21644;&#24863;&#35273;&#36816;&#21160;&#25216;&#33021;&#30340;&#32852;&#21512;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#25797;&#38271;&#23558;&#23398;&#21040;&#30340;&#34892;&#20026;&#24212;&#29992;&#20110;&#26410;&#23398;&#20064;&#36807;&#30340;&#24773;&#22659;&#12290;&#36825;&#31181;&#27867;&#21270;&#34892;&#20026;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#25105;&#20204;&#33021;&#22815;&#23558;&#25972;&#20307;&#20998;&#35299;&#25104;&#21487;&#37325;&#22797;&#21033;&#29992;&#30340;&#37096;&#20998;&#30340;&#33021;&#21147;&#65292;&#21363;&#32452;&#21512;&#24615;&#12290;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#28041;&#21450;&#36825;&#31181;&#29305;&#24449;&#12290;&#8220;&#22312;&#20010;&#20307;&#21482;&#23398;&#20064;&#37096;&#20998;&#35821;&#35328;&#32452;&#21512;&#21450;&#20854;&#30456;&#24212;&#30340;&#24863;&#35273;&#36816;&#21160;&#27169;&#24335;&#26102;&#65292;&#22914;&#20309;&#36890;&#36807;&#32852;&#24819;&#23398;&#20064;&#21516;&#26102;&#21457;&#23637;&#35821;&#35328;&#30340;&#32452;&#21512;&#24615;&#21644;&#24863;&#35273;&#36816;&#21160;&#25216;&#33021;&#65311;&#8221;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34701;&#21512;&#35270;&#35273;&#12289;&#26412;&#20307;&#24863;&#30693;&#21644;&#35821;&#35328;&#30340;&#22823;&#33041;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23558;&#20854;&#32435;&#20837;&#22522;&#20110;&#33258;&#30001;&#33021;&#21407;&#29702;&#30340;&#39044;&#27979;&#32534;&#30721;&#21644;&#20027;&#21160;&#25512;&#26029;&#26694;&#26550;&#20013;&#12290;&#36890;&#36807;&#19982;&#26426;&#22120;&#20154;&#25163;&#33218;&#36827;&#34892;&#30340;&#21508;&#31181;&#27169;&#25311;&#23454;&#39564;&#35780;&#20272;&#20102;&#36825;&#20010;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#23398;&#20064;&#20013;&#23545;&#20110;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19995v1 Announce Type: new  Abstract: Humans excel at applying learned behavior to unlearned situations. A crucial component of this generalization behavior is our ability to compose/decompose a whole into reusable parts, an attribute known as compositionality. One of the fundamental questions in robotics concerns this characteristic. "How can linguistic compositionality be developed concomitantly with sensorimotor skills through associative learning, particularly when individuals only learn partial linguistic compositions and their corresponding sensorimotor patterns?" To address this question, we propose a brain-inspired neural network model that integrates vision, proprioception, and language into a framework of predictive coding and active inference, based on the free-energy principle. The effectiveness and capabilities of this model were assessed through various simulation experiments conducted with a robot arm. Our results show that generalization in learning to unlear
&lt;/p&gt;</description></item><item><title>DetToolChain&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#33539;&#24335;&#65292;&#21487;&#20197;&#37322;&#25918;MLLM&#30340;&#38646;&#25293;&#25668;&#29289;&#20307;&#26816;&#27979;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26816;&#27979;&#38142;&#24335;&#24605;&#32500;&#33258;&#21160;&#21270;&#20219;&#21153;&#20998;&#35299;&#21644;&#36880;&#27493;&#26694;&#32454;&#21270;&#35268;&#21010;&#12290;</title><link>https://arxiv.org/abs/2403.12488</link><description>&lt;p&gt;
DetToolChain&#65306;&#19968;&#31181;&#37322;&#25918;MLLM&#26816;&#27979;&#33021;&#21147;&#30340;&#26032;&#25552;&#31034;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
DetToolChain: A New Prompting Paradigm to Unleash Detection Ability of MLLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12488
&lt;/p&gt;
&lt;p&gt;
DetToolChain&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#33539;&#24335;&#65292;&#21487;&#20197;&#37322;&#25918;MLLM&#30340;&#38646;&#25293;&#25668;&#29289;&#20307;&#26816;&#27979;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26816;&#27979;&#38142;&#24335;&#24605;&#32500;&#33258;&#21160;&#21270;&#20219;&#21153;&#20998;&#35299;&#21644;&#36880;&#27493;&#26694;&#32454;&#21270;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;DetToolChain&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#31034;&#33539;&#24335;&#65292;&#29992;&#20110;&#37322;&#25918;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#38646;&#25293;&#25668;&#29289;&#20307;&#26816;&#27979;&#33021;&#21147;&#65292;&#22914;GPT-4V&#21644;Gemini&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19968;&#20010;&#21463;&#39640;&#31934;&#24230;&#26816;&#27979;&#20808;&#39564;&#21551;&#21457;&#30340;&#26816;&#27979;&#25552;&#31034;&#24037;&#20855;&#21253;&#21644;&#19968;&#20010;&#23454;&#29616;&#36825;&#20123;&#25552;&#31034;&#30340;&#26032;Chain-of-Thought&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24037;&#20855;&#21253;&#20013;&#30340;&#25552;&#31034;&#26088;&#22312;&#24341;&#23548;MLLM&#38598;&#20013;&#22312;&#21306;&#22495;&#20449;&#24687;&#19978;&#65288;&#20363;&#22914;&#65292;&#25918;&#22823;&#65289;&#65292;&#25353;&#29031;&#27979;&#37327;&#26631;&#20934;&#38405;&#35835;&#22352;&#26631;&#65288;&#20363;&#22914;&#65292;&#21472;&#21152;&#26631;&#23610;&#21644;&#25351;&#21335;&#38024;&#65289;&#65292;&#24182;&#20174;&#19978;&#19979;&#25991;&#20449;&#24687;&#20013;&#25512;&#26029;&#65288;&#20363;&#22914;&#65292;&#21472;&#21152;&#22330;&#26223;&#22270;&#65289;&#12290;&#22522;&#20110;&#36825;&#20123;&#24037;&#20855;&#65292;&#26032;&#30340;&#26816;&#27979;Chain-of-Thought&#21487;&#20197;&#33258;&#21160;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#31616;&#21333;&#30340;&#23376;&#20219;&#21153;&#65292;&#35786;&#26029;&#39044;&#27979;&#65292;&#24182;&#35268;&#21010;&#36880;&#27493;&#26694;&#32454;&#21270;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#22312;&#19968;&#31995;&#21015;&#26816;&#27979;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#35777;&#23454;&#65292;&#29305;&#21035;&#26159;&#22312;&#22256;&#38590;&#24773;&#20917;&#19979;&#12290;&#19982;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#27604;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12488v1 Announce Type: cross  Abstract: We present DetToolChain, a novel prompting paradigm, to unleash the zero-shot object detection ability of multimodal large language models (MLLMs), such as GPT-4V and Gemini. Our approach consists of a detection prompting toolkit inspired by high-precision detection priors and a new Chain-of-Thought to implement these prompts. Specifically, the prompts in the toolkit are designed to guide the MLLM to focus on regional information (e.g., zooming in), read coordinates according to measure standards (e.g., overlaying rulers and compasses), and infer from the contextual information (e.g., overlaying scene graphs). Building upon these tools, the new detection chain-of-thought can automatically decompose the task into simple subtasks, diagnose the predictions, and plan for progressive box refinements. The effectiveness of our framework is demonstrated across a spectrum of detection tasks, especially hard cases. Compared to existing state-of-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20026;&#21442;&#25968;&#30340;&#35745;&#31639;&#22270;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#21387;&#22120;&#26469;&#23454;&#29616;&#32622;&#25442;&#23545;&#31216;&#24615;&#65292;&#20351;&#24471;&#21333;&#20010;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#22810;&#31181;&#26550;&#26500;&#30340;&#31070;&#32463;&#35745;&#31639;&#22270;&#12290;</title><link>https://arxiv.org/abs/2403.12143</link><description>&lt;p&gt;
&#29992;&#20110;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#31561;&#21464;&#34920;&#31034;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks for Learning Equivariant Representations of Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20026;&#21442;&#25968;&#30340;&#35745;&#31639;&#22270;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#21387;&#22120;&#26469;&#23454;&#29616;&#32622;&#25442;&#23545;&#31216;&#24615;&#65292;&#20351;&#24471;&#21333;&#20010;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#22810;&#31181;&#26550;&#26500;&#30340;&#31070;&#32463;&#35745;&#31639;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#35832;&#22914;&#20998;&#31867;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#12289;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#21644;&#39044;&#27979;&#27867;&#21270;&#38169;&#35823;&#31561;&#39046;&#22495;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#24573;&#35270;&#31070;&#32463;&#32593;&#32476;&#20013;&#22266;&#26377;&#30340;&#32622;&#25442;&#23545;&#31216;&#24615;&#65292;&#35201;&#20040;&#20381;&#36182;&#22797;&#26434;&#30340;&#26435;&#37325;&#20849;&#20139;&#27169;&#24335;&#26469;&#23454;&#29616;&#31561;&#21464;&#24615;&#65292;&#21516;&#26102;&#24573;&#30053;&#32593;&#32476;&#26550;&#26500;&#26412;&#36523;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20026;&#21442;&#25968;&#30340;&#35745;&#31639;&#22270;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#24378;&#22823;&#30340;&#20445;&#30041;&#32622;&#25442;&#23545;&#31216;&#24615;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#21387;&#22120;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#24471;&#21333;&#20010;&#27169;&#22411;&#33021;&#22815;&#23545;&#20855;&#26377;&#22810;&#26679;&#26550;&#26500;&#30340;&#31070;&#32463;&#35745;&#31639;&#22270;&#36827;&#34892;&#32534;&#30721;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21253;&#25324;&#20998;&#31867;&#21644;&#32534;&#36753;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#12289;&#39044;&#27979;&#27867;&#21270;&#38169;&#35823;&#31561;&#22810;&#31181;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12143v1 Announce Type: cross  Abstract: Neural networks that process the parameters of other neural networks find applications in domains as diverse as classifying implicit neural representations, generating neural network weights, and predicting generalization errors. However, existing approaches either overlook the inherent permutation symmetry in the neural network or rely on intricate weight-sharing patterns to achieve equivariance, while ignoring the impact of the network architecture itself. In this work, we propose to represent neural networks as computational graphs of parameters, which allows us to harness powerful graph neural networks and transformers that preserve permutation symmetry. Consequently, our approach enables a single model to encode neural computational graphs with diverse architectures. We showcase the effectiveness of our method on a wide range of tasks, including classification and editing of implicit neural representations, predicting generalizati
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#20013;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#22270;&#20687;&#36136;&#37327;&#30340;&#26032;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#30830;&#20445;&#25991;&#26412;&#21644;&#22270;&#20687;&#20869;&#23481;&#30340;&#23545;&#40784;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#27861;&#26469;&#24402;&#32435;&#36825;&#20123;&#25351;&#26631;</title><link>https://arxiv.org/abs/2403.11821</link><description>&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#65306;&#22270;&#20687;&#36136;&#37327;&#24230;&#37327;&#30340;&#35843;&#26597;&#19982;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Evaluating Text to Image Synthesis: Survey and Taxonomy of Image Quality Metrics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11821
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#20013;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#22270;&#20687;&#36136;&#37327;&#30340;&#26032;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#30830;&#20445;&#25991;&#26412;&#21644;&#22270;&#20687;&#20869;&#23481;&#30340;&#23545;&#40784;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#27861;&#26469;&#24402;&#32435;&#36825;&#20123;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#21644;&#35270;&#35273;&#32467;&#21512;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#25512;&#21160;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20114;&#32852;&#32593;&#25110;&#20854;&#20182;&#22823;&#35268;&#27169;&#25968;&#25454;&#24211;&#20013;&#30340;&#28023;&#37327;&#25991;&#26412;-&#22270;&#20687;&#23545;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#12290;&#38543;&#30528;&#23545;&#39640;&#36136;&#37327;&#22270;&#20687;&#29983;&#25104;&#30340;&#38656;&#27714;&#36716;&#21521;&#30830;&#20445;&#25991;&#26412;&#19982;&#22270;&#20687;&#20043;&#38388;&#30340;&#20869;&#23481;&#23545;&#40784;&#65292;&#24050;&#24320;&#21457;&#20102;&#26032;&#39062;&#30340;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#65292;&#26088;&#22312;&#27169;&#25311;&#20154;&#31867;&#21028;&#26029;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#25910;&#38598;&#20855;&#26377;&#36234;&#26469;&#36234;&#22797;&#26434;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#30740;&#31350;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#25104;&#24615;&#21450;&#20854;&#20316;&#20026;&#25991;&#26412;&#19982;&#22270;&#20687;&#20869;&#23481;&#32452;&#25104;&#23545;&#40784;&#36136;&#37327;&#24230;&#37327;&#30340;&#20854;&#32435;&#20837;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#20171;&#32461;&#20102;&#29616;&#26377;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#26469;&#23545;&#36825;&#20123;&#25351;&#26631;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#36824;&#23457;&#26597;&#20102;&#32463;&#24120;&#37319;&#29992;&#30340;&#25991;&#26412;-&#22270;&#20687;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11821v1 Announce Type: cross  Abstract: Recent advances in text-to-image synthesis have been enabled by exploiting a combination of language and vision through foundation models. These models are pre-trained on tremendous amounts of text-image pairs sourced from the World Wide Web or other large-scale databases. As the demand for high-quality image generation shifts towards ensuring content alignment between text and image, novel evaluation metrics have been developed with the aim of mimicking human judgments. Thus, researchers have started to collect datasets with increasingly complex annotations to study the compositionality of vision-language models and their incorporation as a quality measure of compositional alignment between text and image contents. In this work, we provide a comprehensive overview of existing text-to-image evaluation metrics and propose a new taxonomy for categorizing these metrics. We also review frequently adopted text-image benchmark datasets befor
&lt;/p&gt;</description></item><item><title>TabPFN&#27169;&#22411;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#24182;&#33021;&#22815;&#20197;&#31186;&#32423;&#36895;&#24230;&#29983;&#25104;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#19987;&#20026;TabPFN&#35774;&#35745;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#25913;&#36827;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#35745;&#31639;&#12290;</title><link>https://arxiv.org/abs/2403.10923</link><description>&lt;p&gt;
TabPFN&#30340;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Interpretable Machine Learning for TabPFN
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10923
&lt;/p&gt;
&lt;p&gt;
TabPFN&#27169;&#22411;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#24182;&#33021;&#22815;&#20197;&#31186;&#32423;&#36895;&#24230;&#29983;&#25104;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#19987;&#20026;TabPFN&#35774;&#35745;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#25913;&#36827;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24320;&#21457;&#30340;Prior-Data Fitted Networks&#65288;PFNs&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#20855;&#26377;&#38750;&#24120;&#26377;&#24076;&#26395;&#30340;&#24212;&#29992;&#32467;&#26524;&#12290;TabPFN&#27169;&#22411;&#26159;PFN&#30340;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#65292;&#36866;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#65292;&#22312;&#19981;&#38656;&#35201;&#23398;&#20064;&#21442;&#25968;&#25110;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#22312;&#30701;&#30701;&#20960;&#31186;&#38047;&#20869;&#23454;&#29616;&#22810;&#31181;&#20998;&#31867;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#29983;&#25104;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#12290;TabPFN&#22240;&#27492;&#25104;&#20026;&#20102;&#35768;&#22810;&#39046;&#22495;&#24212;&#29992;&#20013;&#38750;&#24120;&#21560;&#24341;&#20154;&#30340;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#32570;&#28857;&#26159;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#38024;&#23545;TabPFN&#19987;&#38376;&#35774;&#35745;&#30340;&#27969;&#34892;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#25913;&#36827;&#12290;&#36890;&#36807;&#21033;&#29992;&#35813;&#27169;&#22411;&#30340;&#29420;&#29305;&#24615;&#36136;&#65292;&#25105;&#20204;&#30340;&#25913;&#36827;&#20801;&#35768;&#27604;&#29616;&#26377;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#35745;&#31639;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#36991;&#20813;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10923v1 Announce Type: cross  Abstract: The recently developed Prior-Data Fitted Networks (PFNs) have shown very promising results for applications in low-data regimes. The TabPFN model, a special case of PFNs for tabular data, is able to achieve state-of-the-art performance on a variety of classification tasks while producing posterior predictive distributions in mere seconds by in-context learning without the need for learning parameters or hyperparameter tuning. This makes TabPFN a very attractive option for a wide range of domain applications. However, a major drawback of the method is its lack of interpretability. Therefore, we propose several adaptations of popular interpretability methods that we specifically design for TabPFN. By taking advantage of the unique properties of the model, our adaptations allow for more efficient computations than existing implementations. In particular, we show how in-context learning facilitates the estimation of Shapley values by avoid
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#22312;&#36890;&#36807;web&#27983;&#35272;&#22120;&#19982;&#36719;&#20214;&#20132;&#20114;&#26102;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;WorkArena&#21644;BrowserGym&#20004;&#20010;&#24037;&#20855;&#65292;&#22312;29&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#20063;&#25581;&#31034;&#20102;&#23454;&#29616;&#23436;&#20840;&#20219;&#21153;&#33258;&#21160;&#21270;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.07718</link><description>&lt;p&gt;
WorkArena&#65306;Web&#20195;&#29702;&#22312;&#35299;&#20915;&#24120;&#35265;&#30693;&#35782;&#24037;&#20316;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work Tasks?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07718
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#22312;&#36890;&#36807;web&#27983;&#35272;&#22120;&#19982;&#36719;&#20214;&#20132;&#20114;&#26102;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;WorkArena&#21644;BrowserGym&#20004;&#20010;&#24037;&#20855;&#65292;&#22312;29&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#20063;&#25581;&#31034;&#20102;&#23454;&#29616;&#23436;&#20840;&#20219;&#21153;&#33258;&#21160;&#21270;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#19982;&#36719;&#20214;&#36890;&#36807;web&#27983;&#35272;&#22120;&#20132;&#20114;&#30340;&#24212;&#29992;&#12290;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#20851;&#27880;&#34913;&#37327;&#36825;&#20123;&#20195;&#29702;&#25191;&#34892;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#36825;&#20123;&#20219;&#21153;&#28085;&#30422;&#20102;&#21033;&#29992;&#20225;&#19994;&#36719;&#20214;&#31995;&#32479;&#30340;&#30693;&#35782;&#24037;&#20316;&#32773;&#30340;&#20856;&#22411;&#26085;&#24120;&#24037;&#20316;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;WorkArena&#65292;&#19968;&#20010;&#22522;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;ServiceNow&#24179;&#21488;&#30340;29&#20010;&#20219;&#21153;&#30340;&#36828;&#31243;&#20027;&#26426;&#22522;&#20934;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;BrowserGym&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35774;&#35745;&#21644;&#35780;&#20272;&#36825;&#20123;&#20195;&#29702;&#30340;&#29615;&#22659;&#65292;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#34892;&#20026;&#21644;&#22810;&#27169;&#24577;&#35266;&#23519;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#26174;&#31034;&#65292;&#23613;&#31649;&#24403;&#21069;&#30340;&#20195;&#29702;&#22312;WorkArena&#19978;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#35201;&#23454;&#29616;&#23436;&#20840;&#20219;&#21153;&#33258;&#21160;&#21270;&#20173;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#24046;&#36317;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#24320;&#28304;&#21644;&#38381;&#28304;LLMs&#20043;&#38388;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#31361;&#20986;&#20102;&#26410;&#26469;&#25506;&#32034;&#21644;&#21457;&#23637;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07718v1 Announce Type: cross  Abstract: We study the use of large language model-based agents for interacting with software via web browsers. Unlike prior work, we focus on measuring the agents' ability to perform tasks that span the typical daily work of knowledge workers utilizing enterprise software systems. To this end, we propose WorkArena, a remote-hosted benchmark of 29 tasks based on the widely-used ServiceNow platform. We also introduce BrowserGym, an environment for the design and evaluation of such agents, offering a rich set of actions as well as multimodal observations. Our empirical evaluation reveals that while current agents show promise on WorkArena, there remains a considerable gap towards achieving full task automation. Notably, our analysis uncovers a significant performance disparity between open and closed-source LLMs, highlighting a critical area for future exploration and development in the field.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20809;&#28369; Tchebycheff &#26631;&#37327;&#21270;&#26041;&#27861;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26799;&#24230;&#22411;&#22810;&#30446;&#26631;&#20248;&#21270;&#65292;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#20294;&#20173;&#33021;&#25214;&#21040;&#25152;&#26377;&#24085;&#32047;&#25176;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.19078</link><description>&lt;p&gt;
&#20809;&#28369; Tchebycheff &#26631;&#37327;&#21270;&#29992;&#20110;&#22810;&#30446;&#26631;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Smooth Tchebycheff Scalarization for Multi-Objective Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19078
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20809;&#28369; Tchebycheff &#26631;&#37327;&#21270;&#26041;&#27861;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26799;&#24230;&#22411;&#22810;&#30446;&#26631;&#20248;&#21270;&#65292;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#20294;&#20173;&#33021;&#25214;&#21040;&#25152;&#26377;&#24085;&#32047;&#25176;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#37117;&#33021;&#25214;&#21040;&#65292;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#65292;&#30446;&#26631;&#32463;&#24120;&#30456;&#20114;&#20914;&#31361;&#65292;&#19981;&#33021;&#36890;&#36807;&#21333;&#20010;&#35299;&#36827;&#34892;&#20248;&#21270;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#20013;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#25214;&#21040;&#24085;&#32047;&#25176;&#35299;&#65292;&#36825;&#20123;&#35299;&#20195;&#34920;&#20102;&#23545;&#20110;&#32473;&#23450;&#38382;&#39064;&#30340;&#19981;&#21516;&#26368;&#20339;&#26435;&#34913;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29616;&#26377;&#26041;&#27861;&#21487;&#33021;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#25110;&#32773;&#21487;&#33021;&#19981;&#33021;&#20855;&#22791;&#35299;&#20915;&#19968;&#33324;&#21487;&#24494;&#20998;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#30340;&#33391;&#22909;&#29702;&#35770;&#23646;&#24615;&#12290;&#22312;&#26412;&#39033;&#24037;&#20316;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#20809;&#28369;&#20248;&#21270;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36731;&#37327;&#30340;&#20809;&#28369; Tchebycheff &#26631;&#37327;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;&#26799;&#24230;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#12290;&#23427;&#23545;&#20110;&#25214;&#21040;&#25152;&#26377;&#24085;&#32047;&#25176;&#35299;&#20855;&#26377;&#33391;&#22909;&#30340;&#29702;&#35770;&#23646;&#24615;&#65292;&#21516;&#26102;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26174;&#30528;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#22312;&#21508;&#31181;&#23454;&#39564;&#32467;&#26524;&#19978;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19078v1 Announce Type: cross  Abstract: Multi-objective optimization problems can be found in many real-world applications, where the objectives often conflict each other and cannot be optimized by a single solution. In the past few decades, numerous methods have been proposed to find Pareto solutions that represent different optimal trade-offs among the objectives for a given problem. However, these existing methods could have high computational complexity or may not have good theoretical properties for solving a general differentiable multi-objective optimization problem. In this work, by leveraging the smooth optimization technique, we propose a novel and lightweight smooth Tchebycheff scalarization approach for gradient-based multi-objective optimization. It has good theoretical properties for finding all Pareto solutions with valid trade-off preferences, while enjoying significantly lower computational complexity compared to other methods. Experimental results on variou
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenCeption&#30340;&#26032;&#22411;MLLM&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#20165;&#21033;&#29992;&#21333;&#27169;&#24577;&#25968;&#25454;&#35780;&#20272;&#36328;&#27169;&#24577;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#26377;&#25928;&#21453;&#26144;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#20542;&#21521;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#21644;&#28508;&#21147;&#20110;&#27969;&#34892;&#30340;MLLM&#22522;&#20934;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.14973</link><description>&lt;p&gt;
GenCeption&#65306;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#35780;&#20272;&#22810;&#27169;&#24577;LLM
&lt;/p&gt;
&lt;p&gt;
GenCeption: Evaluate Multimodal LLMs with Unlabeled Unimodal Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14973
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenCeption&#30340;&#26032;&#22411;MLLM&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#20165;&#21033;&#29992;&#21333;&#27169;&#24577;&#25968;&#25454;&#35780;&#20272;&#36328;&#27169;&#24577;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#26377;&#25928;&#21453;&#26144;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#20542;&#21521;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#21644;&#28508;&#21147;&#20110;&#27969;&#34892;&#30340;MLLM&#22522;&#20934;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#36890;&#24120;&#20351;&#29992;&#26114;&#36149;&#30340;&#24102;&#26631;&#27880;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20934;&#36890;&#24120;&#38590;&#20197;&#36319;&#19978;MLLM&#35780;&#20272;&#30340;&#24555;&#36895;&#21457;&#23637;&#35201;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GenCeption&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#38656;&#27880;&#37322;&#30340;MLLM&#35780;&#20272;&#26694;&#26550;&#65292;&#20165;&#38656;&#35201;&#21333;&#27169;&#24577;&#25968;&#25454;&#26469;&#35780;&#20272;&#36328;&#27169;&#24577;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#21453;&#26144;&#20986;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#20542;&#21521;&#12290;&#31867;&#20284;&#20110;&#27969;&#34892;&#30340;DrawCeption&#28216;&#25103;&#65292;GenCeption&#20174;&#19968;&#20010;&#38750;&#25991;&#26412;&#26679;&#26412;&#24320;&#22987;&#65292;&#24182;&#32463;&#21382;&#19968;&#31995;&#21015;&#36845;&#20195;&#30340;&#25551;&#36848;&#21644;&#29983;&#25104;&#27493;&#39588;&#12290;&#36845;&#20195;&#20043;&#38388;&#30340;&#35821;&#20041;&#28418;&#31227;&#20351;&#29992;GC@T&#25351;&#26631;&#36827;&#34892;&#37327;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#21457;&#29616;&#39564;&#35777;&#20102;GenCeption&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#26174;&#31034;&#20986;&#19982;&#27969;&#34892;&#30340;MLLM&#22522;&#20934;&#32467;&#26524;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;GenCeption&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#26222;&#36941;&#23384;&#22312;&#19988;&#20197;&#21069;&#26410;&#35265;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#26469;&#25193;&#23637;&#65292;&#20197;&#20943;&#36731;&#35757;&#32451;&#25968;&#25454;&#30340;&#27745;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14973v1 Announce Type: cross  Abstract: Multimodal Large Language Models (MLLMs) are commonly evaluated using costly annotated multimodal benchmarks. However, these benchmarks often struggle to keep pace with the rapidly advancing requirements of MLLM evaluation. We propose GenCeption, a novel and annotation-free MLLM evaluation framework that merely requires unimodal data to assess inter-modality semantic coherence and inversely reflects the models' inclination to hallucinate. Analogous to the popular DrawCeption game, GenCeption initiates with a non-textual sample and undergoes a series of iterative description and generation steps. Semantic drift across iterations is quantified using the GC@T metric. Our empirical findings validate GenCeption's efficacy, showing strong correlations with popular MLLM benchmarking results. GenCeption may be extended to mitigate training data contamination by utilizing ubiquitous, previously unseen unimodal data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#33258;&#36866;&#24212;&#29468;&#24819;&#30340;&#22312;&#32447;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;IT&#22522;&#30784;&#35774;&#26045;&#30340;&#33258;&#21160;&#21270;&#23433;&#20840;&#21709;&#24212;&#26041;&#27861;&#65292;&#20854;&#20013;&#28216;&#25103;&#21442;&#19982;&#32773;&#36890;&#36807;Bayesian&#23398;&#20064;&#35843;&#25972;&#29468;&#24819;&#65292;&#24182;&#36890;&#36807;&#25512;&#28436;&#26356;&#26032;&#31574;&#30053;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#26368;&#20339;&#25311;&#21512;&#65292;&#25552;&#39640;&#20102;&#25512;&#28436;&#22312;&#29468;&#24819;&#27169;&#22411;&#19979;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12499</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#29468;&#24819;&#30340;&#22312;&#32447;&#23398;&#20064;&#23454;&#29616;&#33258;&#21160;&#21270;&#23433;&#20840;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Automated Security Response through Online Learning with Adaptive Conjectures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12499
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#33258;&#36866;&#24212;&#29468;&#24819;&#30340;&#22312;&#32447;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;IT&#22522;&#30784;&#35774;&#26045;&#30340;&#33258;&#21160;&#21270;&#23433;&#20840;&#21709;&#24212;&#26041;&#27861;&#65292;&#20854;&#20013;&#28216;&#25103;&#21442;&#19982;&#32773;&#36890;&#36807;Bayesian&#23398;&#20064;&#35843;&#25972;&#29468;&#24819;&#65292;&#24182;&#36890;&#36807;&#25512;&#28436;&#26356;&#26032;&#31574;&#30053;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#26368;&#20339;&#25311;&#21512;&#65292;&#25552;&#39640;&#20102;&#25512;&#28436;&#22312;&#29468;&#24819;&#27169;&#22411;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38024;&#23545;IT&#22522;&#30784;&#35774;&#26045;&#30340;&#33258;&#21160;&#21270;&#23433;&#20840;&#21709;&#24212;&#65292;&#24182;&#23558;&#25915;&#20987;&#32773;&#21644;&#38450;&#24481;&#32773;&#20043;&#38388;&#30340;&#20114;&#21160;&#24418;&#24335;&#34920;&#36848;&#20026;&#19968;&#20010;&#37096;&#20998;&#35266;&#27979;&#12289;&#38750;&#24179;&#31283;&#21338;&#24328;&#12290;&#25105;&#20204;&#25918;&#23485;&#20102;&#28216;&#25103;&#27169;&#22411;&#27491;&#30830;&#35268;&#23450;&#30340;&#26631;&#20934;&#20551;&#35774;&#65292;&#24182;&#32771;&#34385;&#27599;&#20010;&#21442;&#19982;&#32773;&#23545;&#27169;&#22411;&#26377;&#19968;&#20010;&#27010;&#29575;&#24615;&#29468;&#24819;&#65292;&#21487;&#33021;&#22312;&#26576;&#31181;&#24847;&#20041;&#19978;&#38169;&#35823;&#35268;&#23450;&#65292;&#21363;&#30495;&#23454;&#27169;&#22411;&#30340;&#27010;&#29575;&#20026;0&#12290;&#36825;&#31181;&#24418;&#24335;&#20801;&#35768;&#25105;&#20204;&#25429;&#25417;&#20851;&#20110;&#22522;&#30784;&#35774;&#26045;&#21644;&#21442;&#19982;&#32773;&#24847;&#22270;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#22312;&#32447;&#23398;&#20064;&#26377;&#25928;&#30340;&#28216;&#25103;&#31574;&#30053;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;&#21442;&#19982;&#32773;&#36890;&#36807;&#36125;&#21494;&#26031;&#23398;&#20064;&#36845;&#20195;&#22320;&#35843;&#25972;&#20854;&#29468;&#24819;&#65292;&#24182;&#36890;&#36807;&#25512;&#28436;&#26356;&#26032;&#20854;&#31574;&#30053;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#29468;&#24819;&#20250;&#25910;&#25947;&#21040;&#26368;&#20339;&#25311;&#21512;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#20855;&#26377;&#29468;&#27979;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#25512;&#28436;&#23454;&#29616;&#24615;&#33021;&#25913;&#36827;&#30340;&#19978;&#38480;&#12290;&#20026;&#20102;&#21051;&#30011;&#28216;&#25103;&#30340;&#31283;&#23450;&#29366;&#24577;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Berk-Nash&#24179;&#34913;&#30340;&#19968;&#20010;&#21464;&#31181;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12499v1 Announce Type: cross  Abstract: We study automated security response for an IT infrastructure and formulate the interaction between an attacker and a defender as a partially observed, non-stationary game. We relax the standard assumption that the game model is correctly specified and consider that each player has a probabilistic conjecture about the model, which may be misspecified in the sense that the true model has probability 0. This formulation allows us to capture uncertainty about the infrastructure and the intents of the players. To learn effective game strategies online, we design a novel method where a player iteratively adapts its conjecture using Bayesian learning and updates its strategy through rollout. We prove that the conjectures converge to best fits, and we provide a bound on the performance improvement that rollout enables with a conjectured model. To characterize the steady state of the game, we propose a variant of the Berk-Nash equilibrium. We 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;AutoGPT+P&#65292;&#23427;&#32467;&#21512;&#20102;&#22522;&#20110;Affordance&#30340;&#22330;&#26223;&#34920;&#31034;&#21644;&#35268;&#21010;&#31995;&#32479;&#65292;&#21487;&#20197;&#35299;&#20915;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#20219;&#21153;&#35268;&#21010;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.10778</link><description>&lt;p&gt;
&#22522;&#20110;Affordance&#30340;&#20219;&#21153;&#35268;&#21010;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;AutoGPT+P
&lt;/p&gt;
&lt;p&gt;
AutoGPT+P: Affordance-based Task Planning with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10778
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;AutoGPT+P&#65292;&#23427;&#32467;&#21512;&#20102;&#22522;&#20110;Affordance&#30340;&#22330;&#26223;&#34920;&#31034;&#21644;&#35268;&#21010;&#31995;&#32479;&#65292;&#21487;&#20197;&#35299;&#20915;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#20219;&#21153;&#35268;&#21010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#20219;&#21153;&#35268;&#21010;&#30340;&#19968;&#20123;&#26032;&#36827;&#23637;&#21033;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#36890;&#36807;&#23558;&#36825;&#20123;&#27169;&#22411;&#19982;&#32463;&#20856;&#35268;&#21010;&#31639;&#27861;&#32467;&#21512;&#36215;&#26469;&#26469;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#65292;&#20197;&#35299;&#20915;&#23427;&#20204;&#22312;&#25512;&#29702;&#33021;&#21147;&#19978;&#22266;&#26377;&#30340;&#23616;&#38480;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#30528;&#21160;&#24577;&#25429;&#25417;&#20219;&#21153;&#35268;&#21010;&#38382;&#39064;&#30340;&#21021;&#22987;&#29366;&#24577;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoGPT+P&#65292;&#36825;&#26159;&#19968;&#20010;&#31995;&#32479;&#65292;&#23558;&#22522;&#20110;Affordance&#30340;&#22330;&#26223;&#34920;&#31034;&#19982;&#19968;&#20010;&#35268;&#21010;&#31995;&#32479;&#30456;&#32467;&#21512;&#12290;Affordance&#21253;&#25324;&#20102;&#19968;&#20010;&#20195;&#29702;&#22312;&#29615;&#22659;&#20013;&#21644;&#20854;&#20013;&#23384;&#22312;&#30340;&#29289;&#20307;&#19978;&#30340;&#21160;&#20316;&#21487;&#33021;&#24615;&#12290;&#22240;&#27492;&#65292;&#20174;&#22522;&#20110;Affordance&#30340;&#22330;&#26223;&#34920;&#31034;&#20013;&#25512;&#23548;&#20986;&#35268;&#21010;&#22495;&#65292;&#20801;&#35768;&#20351;&#29992;&#20219;&#24847;&#23545;&#35937;&#36827;&#34892;&#31526;&#21495;&#35268;&#21010;&#12290;AutoGPT+P&#21033;&#29992;&#36825;&#31181;&#34920;&#31034;&#26469;&#20026;&#29992;&#25143;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#23450;&#30340;&#20219;&#21153;&#21046;&#23450;&#21644;&#25191;&#34892;&#35745;&#21010;&#12290;&#38500;&#20102;&#22312;&#23553;&#38381;&#19990;&#30028;&#20551;&#35774;&#19979;&#35299;&#20915;&#35268;&#21010;&#20219;&#21153;&#22806;&#65292;AutoGPT+P&#36824;&#21487;&#20197;&#22788;&#29702;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#35268;&#21010;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10778v1 Announce Type: cross  Abstract: Recent advances in task planning leverage Large Language Models (LLMs) to improve generalizability by combining such models with classical planning algorithms to address their inherent limitations in reasoning capabilities. However, these approaches face the challenge of dynamically capturing the initial state of the task planning problem. To alleviate this issue, we propose AutoGPT+P, a system that combines an affordance-based scene representation with a planning system. Affordances encompass the action possibilities of an agent on the environment and objects present in it. Thus, deriving the planning domain from an affordance-based scene representation allows symbolic planning with arbitrary objects. AutoGPT+P leverages this representation to derive and execute a plan for a task specified by the user in natural language. In addition to solving planning tasks under a closed-world assumption, AutoGPT+P can also handle planning with inc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Open-domain Urban Itinerary Planning (OUIP)&#20219;&#21153;&#65292;&#29992;&#20110;&#26681;&#25454;&#29992;&#25143;&#20197;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35831;&#27714;&#30452;&#25509;&#29983;&#25104;&#34892;&#31243;&#65292;&#36890;&#36807;&#32467;&#21512;&#31354;&#38388;&#20248;&#21270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#22478;&#24066;&#34892;&#31243;&#23450;&#21046;&#26381;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.07204</link><description>&lt;p&gt;
&#32467;&#21512;&#31354;&#38388;&#20248;&#21270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#25918;&#39046;&#22495;&#22478;&#24066;&#34892;&#31243;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Synergizing Spatial Optimization with Large Language Models for Open-Domain Urban Itinerary Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Open-domain Urban Itinerary Planning (OUIP)&#20219;&#21153;&#65292;&#29992;&#20110;&#26681;&#25454;&#29992;&#25143;&#20197;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35831;&#27714;&#30452;&#25509;&#29983;&#25104;&#34892;&#31243;&#65292;&#36890;&#36807;&#32467;&#21512;&#31354;&#38388;&#20248;&#21270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#22478;&#24066;&#34892;&#31243;&#23450;&#21046;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;Open-domain Urban Itinerary Planning (OUIP)&#20219;&#21153;&#65292;&#29992;&#20110;&#26681;&#25454;&#29992;&#25143;&#20197;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35831;&#27714;&#30452;&#25509;&#29983;&#25104;&#34892;&#31243;&#12290;OUIP&#19982;&#20256;&#32479;&#34892;&#31243;&#35268;&#21010;&#19981;&#21516;&#65292;&#20256;&#32479;&#35268;&#21010;&#38480;&#21046;&#20102;&#29992;&#25143;&#34920;&#36798;&#26356;&#35814;&#32454;&#30340;&#38656;&#27714;&#65292;&#38459;&#30861;&#20102;&#30495;&#27491;&#30340;&#20010;&#24615;&#21270;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#22788;&#29702;&#22810;&#26679;&#21270;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38750;&#23454;&#26102;&#20449;&#24687;&#12289;&#19981;&#23436;&#25972;&#30340;&#30693;&#35782;&#21644;&#19981;&#36275;&#30340;&#31354;&#38388;&#24847;&#35782;&#65292;&#23427;&#20204;&#26080;&#27861;&#29420;&#31435;&#22320;&#25552;&#20379;&#28385;&#24847;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ItiNera&#30340;OUIP&#31995;&#32479;&#65292;&#23558;&#31354;&#38388;&#20248;&#21270;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30456;&#32467;&#21512;&#65292;&#26681;&#25454;&#29992;&#25143;&#38656;&#27714;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#22478;&#24066;&#34892;&#31243;&#23450;&#21046;&#26381;&#21153;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#25552;&#21462;&#21644;&#26356;&#26032;&#20852;&#36259;&#28857;&#29305;&#24449;&#65292;&#20197;&#21019;&#24314;&#29992;&#25143;&#33258;&#24049;&#30340;&#20010;&#24615;&#21270;&#20852;&#36259;&#28857;&#25968;&#25454;&#24211;&#12290;&#23545;&#20110;&#27599;&#20010;&#29992;&#25143;&#35831;&#27714;&#65292;&#25105;&#20204;&#21033;&#29992;LLM&#36827;&#34892;&#21327;&#21516;&#23454;&#29616;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we for the first time propose the task of Open-domain Urban Itinerary Planning (OUIP) for citywalk, which directly generates itineraries based on users' requests described in natural language. OUIP is different from conventional itinerary planning, which limits users from expressing more detailed needs and hinders true personalization. Recently, large language models (LLMs) have shown potential in handling diverse tasks. However, due to non-real-time information, incomplete knowledge, and insufficient spatial awareness, they are unable to independently deliver a satisfactory user experience in OUIP. Given this, we present ItiNera, an OUIP system that synergizes spatial optimization with Large Language Models (LLMs) to provide services that customize urban itineraries based on users' needs. Specifically, we develop an LLM-based pipeline for extracting and updating POI features to create a user-owned personalized POI database. For each user request, we leverage LLM in coop
&lt;/p&gt;</description></item><item><title>&#39640;&#26031;&#21943;&#28293;&#25216;&#26415;&#30456;&#32467;&#21512;&#29289;&#29702;&#22522;&#30784;&#21160;&#30011;&#21644;3D&#39640;&#26031;&#21943;&#28293;&#65292;&#21487;&#20197;&#22312;&#34394;&#25311;&#22330;&#26223;&#20013;&#21019;&#36896;&#20986;&#26080;&#21487;&#27604;&#25311;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#23454;&#29616;&#28210;&#26579;&#12289;&#35270;&#22270;&#21512;&#25104;&#20197;&#21450;&#22266;&#20307;&#21644;&#27969;&#20307;&#30340;&#21160;&#24577;&#31649;&#29702;&#21644;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2401.15318</link><description>&lt;p&gt;
&#39640;&#26031;&#21943;&#28293;&#65306;&#21033;&#29992;&#39640;&#26031;&#39128;&#33853;&#21160;&#24577;&#21512;&#25104;&#27969;&#20307;
&lt;/p&gt;
&lt;p&gt;
Gaussian Splashing: Dynamic Fluid Synthesis with Gaussian Splatting. (arXiv:2401.15318v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15318
&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#21943;&#28293;&#25216;&#26415;&#30456;&#32467;&#21512;&#29289;&#29702;&#22522;&#30784;&#21160;&#30011;&#21644;3D&#39640;&#26031;&#21943;&#28293;&#65292;&#21487;&#20197;&#22312;&#34394;&#25311;&#22330;&#26223;&#20013;&#21019;&#36896;&#20986;&#26080;&#21487;&#27604;&#25311;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#23454;&#29616;&#28210;&#26579;&#12289;&#35270;&#22270;&#21512;&#25104;&#20197;&#21450;&#22266;&#20307;&#21644;&#27969;&#20307;&#30340;&#21160;&#24577;&#31649;&#29702;&#21644;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#29289;&#29702;&#22522;&#30784;&#21160;&#30011;&#19982;3D&#39640;&#26031;&#21943;&#28293;&#65288;3DGS&#65289;&#30456;&#32467;&#21512;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#22312;&#20351;&#29992;3DGS&#37325;&#24314;&#30340;&#34394;&#25311;&#22330;&#26223;&#20013;&#21019;&#24314;&#26032;&#25928;&#26524;&#12290;&#21033;&#29992;&#39640;&#26031;&#21943;&#28293;&#21644;&#22522;&#20110;&#20301;&#32622;&#30340;&#21160;&#21147;&#23398;&#65288;PBD&#65289;&#22312;&#24213;&#23618;&#34920;&#31034;&#20013;&#30340;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#20197;&#36830;&#36143;&#30340;&#26041;&#24335;&#31649;&#29702;&#28210;&#26579;&#12289;&#35270;&#22270;&#21512;&#25104;&#20197;&#21450;&#22266;&#20307;&#21644;&#27969;&#20307;&#30340;&#21160;&#24577;&#12290;&#31867;&#20284;&#20110;&#39640;&#26031;&#30528;&#33394;&#22120;&#65292;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;&#27861;&#32447;&#22686;&#24378;&#27599;&#20010;&#39640;&#26031;&#26680;&#65292;&#23558;&#26680;&#30340;&#26041;&#21521;&#19982;&#34920;&#38754;&#27861;&#32447;&#23545;&#40784;&#65292;&#20197;&#25913;&#36827;PBD&#27169;&#25311;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#28040;&#38500;&#20102;&#22266;&#20307;&#26059;&#36716;&#21464;&#24418;&#20135;&#29983;&#30340;&#23574;&#23792;&#22122;&#22768;&#12290;&#23427;&#36824;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#22522;&#20110;&#29289;&#29702;&#30340;&#28210;&#26579;&#38598;&#25104;&#21040;&#27969;&#20307;&#30340;&#21160;&#24577;&#34920;&#38754;&#21453;&#23556;&#20013;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#30495;&#23454;&#22320;&#22797;&#29616;&#21160;&#24577;&#27969;&#20307;&#19978;&#30340;&#34920;&#38754;&#20142;&#28857;&#65292;&#24182;&#20419;&#36827;&#22330;&#26223;&#23545;&#35937;&#19982;&#27969;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate the feasibility of integrating physics-based animations of solids and fluids with 3D Gaussian Splatting (3DGS) to create novel effects in virtual scenes reconstructed using 3DGS. Leveraging the coherence of the Gaussian splatting and position-based dynamics (PBD) in the underlying representation, we manage rendering, view synthesis, and the dynamics of solids and fluids in a cohesive manner. Similar to Gaussian shader, we enhance each Gaussian kernel with an added normal, aligning the kernel's orientation with the surface normal to refine the PBD simulation. This approach effectively eliminates spiky noises that arise from rotational deformation in solids. It also allows us to integrate physically based rendering to augment the dynamic surface reflections on fluids. Consequently, our framework is capable of realistically reproducing surface highlights on dynamic fluids and facilitating interactions between scene objects and fluids from new views. For more information, pl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#32773;&#22312;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#21487;&#35265;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#34987;&#36825;&#20123;&#24433;&#21709;&#32773;&#35748;&#21487;&#30340;&#35770;&#25991;&#24341;&#29992;&#27425;&#25968;&#26174;&#33879;&#22686;&#21152;&#65292;&#20013;&#20301;&#25968;&#24341;&#29992;&#27425;&#25968;&#27604;&#23545;&#29031;&#32452;&#39640;2-3&#20493;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#34987;&#23637;&#31034;&#20316;&#32773;&#30340;&#22320;&#29702;&#12289;&#24615;&#21035;&#21644;&#26426;&#26500;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13782</link><description>&lt;p&gt;
&#20174;&#25512;&#29305;&#21040;&#24341;&#29992;&#65306;&#25581;&#31034;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#32773;&#23545;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#21487;&#35265;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Tweets to Citations: Unveiling the Impact of Social Media Influencers on AI Research Visibility. (arXiv:2401.13782v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#32773;&#22312;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#21487;&#35265;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#34987;&#36825;&#20123;&#24433;&#21709;&#32773;&#35748;&#21487;&#30340;&#35770;&#25991;&#24341;&#29992;&#27425;&#25968;&#26174;&#33879;&#22686;&#21152;&#65292;&#20013;&#20301;&#25968;&#24341;&#29992;&#27425;&#25968;&#27604;&#23545;&#29031;&#32452;&#39640;2-3&#20493;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#34987;&#23637;&#31034;&#20316;&#32773;&#30340;&#22320;&#29702;&#12289;&#24615;&#21035;&#21644;&#26426;&#26500;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#20250;&#35758;&#19978;&#34987;&#25509;&#21463;&#30340;&#35770;&#25991;&#25968;&#37327;&#36798;&#21040;&#25968;&#21315;&#31687;&#65292;&#30740;&#31350;&#20154;&#21592;&#22914;&#20309;&#33719;&#21462;&#21644;&#38405;&#35835;&#30740;&#31350;&#35770;&#25991;&#21464;&#24471;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#32773;&#22312;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#21487;&#35265;&#24615;&#20013;&#30340;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#20182;&#20204;&#20998;&#20139;&#30340;&#35770;&#25991;&#24341;&#29992;&#27425;&#25968;&#12290;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#21253;&#25324;8000&#22810;&#31687;&#35770;&#25991;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;2018&#24180;12&#26376;&#33267;2023&#24180;10&#26376;&#30340;&#25512;&#29305;&#65292;&#20197;&#21450;&#22522;&#20110;&#20986;&#29256;&#24180;&#20221;&#12289;&#20250;&#35758;&#22320;&#28857;&#21644;&#25688;&#35201;&#20027;&#39064;&#36827;&#34892;1&#65306;1&#21305;&#37197;&#30340;&#23545;&#29031;&#32452;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#36825;&#20123;&#24433;&#21709;&#32773;&#35748;&#21487;&#30340;&#35770;&#25991;&#24341;&#29992;&#27425;&#25968;&#26174;&#33879;&#22686;&#21152;&#65292;&#20013;&#20301;&#25968;&#24341;&#29992;&#27425;&#25968;&#27604;&#23545;&#29031;&#32452;&#39640;2-3&#20493;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#28145;&#20837;&#30740;&#31350;&#20102;&#34987;&#23637;&#31034;&#20316;&#32773;&#30340;&#22320;&#29702;&#12289;&#24615;&#21035;&#21644;&#26426;&#26500;&#22810;&#26679;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#31038;&#20132;&#23186;&#20307;&#22312;&#23398;&#26415;&#20132;&#27969;&#20013;&#30340;&#19981;&#26029;&#25193;&#22823;&#30340;&#24433;&#21709;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#24403;&#20170;&#25968;&#23383;&#21270;&#26102;&#20195;&#19981;&#26029;&#21457;&#23637;&#30340;&#29983;&#24577;&#31995;&#32479;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the number of accepted papers at AI and ML conferences reaches into the thousands, it has become unclear how researchers access and read research publications. In this paper, we investigate the role of social media influencers in enhancing the visibility of machine learning research, particularly the citation counts of papers they share. We have compiled a comprehensive dataset of over 8,000 papers, spanning tweets from December 2018 to October 2023, alongside 1:1 matched controls based on publication year, venue, and abstract topics. Our analysis reveals a significant increase in citations for papers endorsed by these influencers, with median citation counts 2-3 times higher than those of the control group. Additionally, the study delves into the geographic, gender, and institutional diversity of highlighted authors. These findings highlight the expanding influence of social media in scholarly communication and underscore the importance of an evolving ecosystem in today's digital a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#22411;&#30340;&#24369;&#30417;&#30563;&#39640;&#26031;&#23545;&#27604;&#22522;&#30784;&#27169;&#22411;&#26469;&#22788;&#29702;&#35270;&#39057;&#38382;&#31572;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#21644;&#31572;&#26696;&#23545;&#20316;&#20026;&#20107;&#20214;&#25551;&#36848;&#65292;&#25214;&#21040;&#22810;&#20010;&#20851;&#38190;&#24103;&#20316;&#20026;&#30446;&#26631;&#26102;&#21051;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#26102;&#21051;&#20316;&#20026;&#20266;&#26631;&#31614;&#26469;&#24378;&#21046;LMMs&#36827;&#34892;&#25512;&#29702;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#39640;&#26031;&#30340;&#23545;&#27604;&#22522;&#30784;&#27169;&#22359;&#65288;GCG&#65289;&#26469;&#23398;&#20064;&#26102;&#25928;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2401.10711</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#22411;&#30340;&#24369;&#30417;&#30563;&#39640;&#26031;&#23545;&#27604;&#22522;&#30784;&#27169;&#22411;&#26469;&#22788;&#29702;&#35270;&#39057;&#38382;&#31572;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Gaussian Contrastive Grounding with Large Multimodal Models for Video Question Answering. (arXiv:2401.10711v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#22411;&#30340;&#24369;&#30417;&#30563;&#39640;&#26031;&#23545;&#27604;&#22522;&#30784;&#27169;&#22411;&#26469;&#22788;&#29702;&#35270;&#39057;&#38382;&#31572;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#21644;&#31572;&#26696;&#23545;&#20316;&#20026;&#20107;&#20214;&#25551;&#36848;&#65292;&#25214;&#21040;&#22810;&#20010;&#20851;&#38190;&#24103;&#20316;&#20026;&#30446;&#26631;&#26102;&#21051;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#26102;&#21051;&#20316;&#20026;&#20266;&#26631;&#31614;&#26469;&#24378;&#21046;LMMs&#36827;&#34892;&#25512;&#29702;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#39640;&#26031;&#30340;&#23545;&#27604;&#22522;&#30784;&#27169;&#22359;&#65288;GCG&#65289;&#26469;&#23398;&#20064;&#26102;&#25928;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#38382;&#31572;&#65288;VideoQA&#65289;&#26088;&#22312;&#22522;&#20110;&#35266;&#23519;&#21040;&#30340;&#35270;&#39057;&#20449;&#24687;&#22238;&#31572;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12290;&#23613;&#31649;&#22823;&#22411;&#22810;&#27169;&#22411;&#65288;LMMs&#65289;&#22312;&#22270;&#20687;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#36817;&#26399;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#22312;&#22788;&#29702;&#35270;&#39057;&#38382;&#31572;&#26041;&#38754;&#36824;&#19981;&#36275;&#22815;&#65292;&#20165;&#20165;&#26159;&#23558;&#22343;&#21248;&#37319;&#26679;&#30340;&#24103;&#20316;&#20026;&#35270;&#35273;&#36755;&#20837;&#65292;&#24573;&#30053;&#20102;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#35270;&#35273;&#32447;&#32034;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#35270;&#39057;&#38382;&#31572;&#25968;&#25454;&#38598;&#20013;&#27809;&#26377;&#38024;&#23545;&#38382;&#39064;&#20851;&#38190;&#26102;&#38388;&#25139;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24369;&#30417;&#30563;&#26694;&#26550;&#65292;&#24378;&#21046;LMMs&#20351;&#29992;&#38382;&#39064;&#20851;&#38190;&#26102;&#21051;&#20316;&#20026;&#35270;&#35273;&#36755;&#20837;&#25512;&#29702;&#20986;&#31572;&#26696;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#21644;&#31572;&#26696;&#23545;&#21512;&#24182;&#20026;&#20107;&#20214;&#25551;&#36848;&#65292;&#20197;&#25214;&#21040;&#22810;&#20010;&#20851;&#38190;&#24103;&#20316;&#20026;&#30446;&#26631;&#26102;&#21051;&#65292;&#36825;&#20123;&#26102;&#21051;&#23558;&#20316;&#20026;&#20266;&#26631;&#31614;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#20266;&#26631;&#31614;&#20316;&#20026;&#39069;&#22806;&#30340;&#24369;&#30417;&#30563;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#39640;&#26031;&#30340;&#23545;&#27604;&#22522;&#30784;&#27169;&#22359;&#65288;GCG&#65289;&#12290;GCG&#23398;&#20064;&#22810;&#20010;&#39640;&#26031;&#20989;&#25968;&#26469;&#25551;&#36848;&#26102;&#25928;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video Question Answering (VideoQA) aims to answer natural language questions based on the information observed in videos. Despite the recent success of Large Multimodal Models (LMMs) in image-language understanding and reasoning, they deal with VideoQA insufficiently by simply taking uniformly sampled frames as visual inputs, which ignores question-relevant visual clues. Moreover, there are no human annotations for question-critical timestamps in existing VideoQA datasets. In light of this, we propose a novel weakly supervised framework to enforce the LMMs to reason out the answers with question-critical moments as visual inputs. Specifically, we fuse the question and answer pairs as event descriptions to find multiple keyframes as target moments, which will be pseudo-labels. With these pseudo-labels as additionally weak supervision, we devise a lightweight Gaussian-based Contrastive Grounding (GCG) module. GCG learns multiple Gaussian functions to characterize the temporal structure o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#19968;&#33268;&#24615;&#25913;&#36827;&#25968;&#25454;&#26377;&#25928;&#30340;&#25674;&#20313;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#36716;&#36125;&#21494;&#26031;&#23450;&#29702;&#24182;&#21033;&#29992;&#36817;&#20284;&#34920;&#31034;&#30340;&#32852;&#21512;&#27169;&#22411;&#20272;&#35745;&#36793;&#38469;&#20284;&#28982;&#65292;&#21152;&#36895;&#26465;&#20214;&#31070;&#32463;&#23494;&#24230;&#20272;&#35745;&#22120;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;</title><link>http://arxiv.org/abs/2310.04395</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#19968;&#33268;&#24615;&#25552;&#39640;&#25968;&#25454;&#26377;&#25928;&#30340;&#25674;&#20313;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Leveraging Self-Consistency for Data-Efficient Amortized Bayesian Inference. (arXiv:2310.04395v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04395
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#19968;&#33268;&#24615;&#25913;&#36827;&#25968;&#25454;&#26377;&#25928;&#30340;&#25674;&#20313;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#36716;&#36125;&#21494;&#26031;&#23450;&#29702;&#24182;&#21033;&#29992;&#36817;&#20284;&#34920;&#31034;&#30340;&#32852;&#21512;&#27169;&#22411;&#20272;&#35745;&#36793;&#38469;&#20284;&#28982;&#65292;&#21152;&#36895;&#26465;&#20214;&#31070;&#32463;&#23494;&#24230;&#20272;&#35745;&#22120;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21442;&#25968;$\theta$&#21644;&#25968;&#25454;$y$&#30340;&#27010;&#29575;&#32852;&#21512;&#27169;&#22411;$p(\theta, y)$&#20013;&#30340;&#36890;&#29992;&#23545;&#31216;&#24615;&#65292;&#25913;&#36827;&#20102;&#25674;&#20313;&#36125;&#21494;&#26031;&#25512;&#29702;&#65288;ABI&#65289;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;&#31616;&#35328;&#20043;&#65292;&#25105;&#20204;&#21453;&#36716;&#36125;&#21494;&#26031;&#23450;&#29702;&#65292;&#24182;&#22522;&#20110;&#36817;&#20284;&#34920;&#31034;&#30340;&#32852;&#21512;&#27169;&#22411;&#20272;&#35745;&#36793;&#38469;&#20284;&#28982;&#12290;&#22312;&#23436;&#32654;&#36817;&#20284;&#24773;&#20917;&#19979;&#65292;&#36793;&#38469;&#20284;&#28982;&#22312;&#25152;&#26377;&#21442;&#25968;&#20540;&#19978;&#37117;&#26159;&#24120;&#25968;&#23450;&#20041;&#30340;&#12290;&#28982;&#32780;&#65292;&#36817;&#20284;&#35823;&#24046;&#23548;&#33268;&#19981;&#21516;&#21442;&#25968;&#20540;&#30340;&#36793;&#38469;&#20284;&#28982;&#20272;&#35745;&#20013;&#23384;&#22312;&#19981;&#21487;&#21462;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#23545;&#31216;&#24615;&#30340;&#36829;&#21453;&#24418;&#24335;&#21270;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#21152;&#36895;&#26465;&#20214;&#31070;&#32463;&#23494;&#24230;&#20272;&#35745;&#22120;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#20855;&#26377;&#26174;&#24335;&#20284;&#28982;&#65288;&#22522;&#20110;&#20284;&#28982;&#65289;&#30340;&#21452;&#23792;&#29609;&#20855;&#38382;&#39064;&#21644;&#20855;&#26377;&#38544;&#24335;&#20284;&#28982;&#65288;&#22522;&#20110;&#27169;&#25311;&#65289;&#30340;&#29616;&#23454;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method to improve the efficiency and accuracy of amortized Bayesian inference (ABI) by leveraging universal symmetries in the probabilistic joint model $p(\theta, y)$ of parameters $\theta$ and data $y$. In a nutshell, we invert Bayes' theorem and estimate the marginal likelihood based on approximate representations of the joint model. Upon perfect approximation, the marginal likelihood is constant across all parameter values by definition. However, approximation error leads to undesirable variance in the marginal likelihood estimates across different parameter values. We formulate violations of this symmetry as a loss function to accelerate the learning dynamics of conditional neural density estimators. We apply our method to a bimodal toy problem with an explicit likelihood (likelihood-based) and a realistic model with an implicit likelihood (simulation-based).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#30456;&#20301;&#19981;&#25935;&#24863;&#36317;&#31163;&#22312;LoS&#20449;&#36947;&#24314;&#27169;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;</title><link>http://arxiv.org/abs/2310.03762</link><description>&lt;p&gt;
&#20248;&#21270;LoS&#20449;&#36947;&#24314;&#27169;&#20013;&#30340;&#22810;&#36733;&#27874;&#22810;&#22825;&#32447;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Optimizing Multicarrier Multiantenna Systems for LoS Channel Charting. (arXiv:2310.03762v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#30456;&#20301;&#19981;&#25935;&#24863;&#36317;&#31163;&#22312;LoS&#20449;&#36947;&#24314;&#27169;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#36947;&#24314;&#27169;&#36890;&#36807;&#23398;&#20064;&#23558;&#22810;&#36733;&#27874;&#22810;&#22825;&#32447;&#31995;&#32479;&#20013;&#22522;&#20110;&#23548;&#39057;&#20449;&#36947;&#20272;&#35745;&#24471;&#21040;&#30340;&#21407;&#22987;&#20449;&#36947;&#35266;&#27979;&#31354;&#38388;&#19982;&#20302;&#32500;&#31354;&#38388;&#36827;&#34892;&#26144;&#23556;&#65292;&#20854;&#20013;&#25509;&#36817;&#30340;&#28857;&#23545;&#24212;&#20110;&#31354;&#38388;&#19978;&#38752;&#36817;&#30340;&#29992;&#25143;&#35774;&#22791;&#65288;UE&#65289;&#30340;&#20449;&#36947;&#12290;&#22312;&#23398;&#20064;&#36825;&#31181;&#26144;&#23556;&#30340;&#19981;&#21516;&#26041;&#27861;&#20013;&#65292;&#26377;&#20123;&#20381;&#36182;&#20110;&#20449;&#36947;&#21521;&#37327;&#20043;&#38388;&#30340;&#36317;&#31163;&#24230;&#37327;&#12290;&#36825;&#26679;&#30340;&#36317;&#31163;&#24212;&#35813;&#21487;&#38752;&#22320;&#21453;&#26144;UE&#30340;&#23616;&#37096;&#31354;&#38388;&#37051;&#22495;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#30456;&#20301;&#19981;&#25935;&#24863;&#65288;PI&#65289;&#36317;&#31163;&#22312;&#36825;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#36136;&#65292;&#20294;&#30001;&#20110;&#20854;&#21608;&#26399;&#24615;&#21644;&#25391;&#33633;&#24615;&#32780;&#20135;&#29983;&#27495;&#20041;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20351;&#36828;&#31163;&#24444;&#27492;&#30340;&#29992;&#25143;&#30475;&#36215;&#26469;&#26356;&#25509;&#36817;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#35813;&#36317;&#31163;&#21450;&#20854;&#23616;&#38480;&#24615;&#30340;&#28145;&#20837;&#29702;&#35770;&#20998;&#26512;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#22914;&#20309;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#30340;&#35265;&#35299;&#12290;&#22240;&#27492;&#65292;&#24471;&#20986;&#20102;&#35774;&#35745;&#33021;&#22815;&#23398;&#20064;&#20248;&#36136;&#20449;&#36947;&#22270;&#30340;&#31995;&#32479;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;
&lt;/p&gt;
&lt;p&gt;
Channel charting (CC) consists in learning a mapping between the space of raw channel observations, made available from pilot-based channel estimation in multicarrier multiantenna system, and a low-dimensional space where close points correspond to channels of user equipments (UEs) close spatially. Among the different methods of learning this mapping, some rely on a distance measure between channel vectors. Such a distance should reliably reflect the local spatial neighborhoods of the UEs. The recently proposed phase-insensitive (PI) distance exhibits good properties in this regards, but suffers from ambiguities due to both its periodic and oscillatory aspects, making users far away from each other appear closer in some cases. In this paper, a thorough theoretical analysis of the said distance and its limitations is provided, giving insights on how they can be mitigated. Guidelines for designing systems capable of learning quality charts are consequently derived. Experimental validatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20998;&#35010;&#20934;&#21017;&#65292;&#20351;&#24471;&#20915;&#31574;&#26641;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#26102;&#20195;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#23558;&#36229;&#20998;&#24067;&#27867;&#21270;&#30740;&#31350;&#20013;&#30340;&#24605;&#24819;&#24212;&#29992;&#20110;&#20915;&#31574;&#26641;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.14496</link><description>&lt;p&gt;
Era Splitting.&#65288;arXiv:2309.14496v1 [cs.LG]&#65289;
&lt;/p&gt;
&lt;p&gt;
Era Splitting. (arXiv:2309.14496v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20998;&#35010;&#20934;&#21017;&#65292;&#20351;&#24471;&#20915;&#31574;&#26641;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#26102;&#20195;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#23558;&#36229;&#20998;&#24067;&#27867;&#21270;&#30740;&#31350;&#20013;&#30340;&#24605;&#24819;&#24212;&#29992;&#20110;&#20915;&#31574;&#26641;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#19978;&#20250;&#21576;&#29616;&#20986;&#25968;&#25454;&#30340;&#20998;&#24067;&#21464;&#21270;&#12290;&#36825;&#31181;&#34892;&#20026;&#36229;&#20986;&#20102;&#20256;&#32479;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#33539;&#24335;&#30340;&#33539;&#22260;&#65292;&#35813;&#33539;&#24335;&#20551;&#35774;&#25968;&#25454;&#22312;&#26102;&#38388;&#21644;&#22320;&#28857;&#19978;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#12290;&#26032;&#20852;&#30340;&#36229;&#20998;&#24067;&#27867;&#21270;&#39046;&#22495;&#36890;&#36807;&#23558;&#29615;&#22659;&#25110;&#26102;&#20195;&#20449;&#24687;&#34701;&#20837;&#31639;&#27861;&#20013;&#65292;&#26469;&#24212;&#23545;&#36825;&#20010;&#29616;&#23454;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#32447;&#24615;&#27169;&#22411;&#21644;/&#25110;&#31070;&#32463;&#32593;&#32476;&#19978;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#20915;&#31574;&#26641;&#27169;&#22411;&#65292;&#21253;&#25324;&#38543;&#26426;&#26862;&#26519;&#21644;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65292;&#24320;&#21457;&#20102;&#20004;&#31181;&#26032;&#30340;&#20998;&#35010;&#20934;&#21017;&#65292;&#20351;&#24471;&#26641;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#19982;&#27599;&#20010;&#25968;&#25454;&#28857;&#30456;&#20851;&#30340;&#26102;&#20195;&#20449;&#24687;&#65292;&#26469;&#25214;&#21040;&#22312;&#25968;&#25454;&#30340;&#25152;&#26377;&#19981;&#30456;&#20132;&#26102;&#20195;&#20013;&#37117;&#26159;&#26368;&#20248;&#30340;&#20999;&#20998;&#28857;&#65292;&#20174;&#32780;&#23558;&#36229;&#20998;&#24067;&#27867;&#21270;&#30740;&#31350;&#20013;&#30340;&#24605;&#24819;&#24212;&#29992;&#20110;&#20915;&#31574;&#26641;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real life machine learning problems exhibit distributional shifts in the data from one time to another or from on place to another. This behavior is beyond the scope of the traditional empirical risk minimization paradigm, which assumes i.i.d. distribution of data over time and across locations. The emerging field of out-of-distribution (OOD) generalization addresses this reality with new theory and algorithms which incorporate environmental, or era-wise information into the algorithms. So far, most research has been focused on linear models and/or neural networks. In this research we develop two new splitting criteria for decision trees, which allow us to apply ideas from OOD generalization research to decision tree models, including random forest and gradient-boosting decision trees. The new splitting criteria use era-wise information associated with each data point to allow tree-based models to find split points that are optimal across all disjoint eras in the data, instead of optim
&lt;/p&gt;</description></item><item><title>FedPop&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#35843;&#20248;&#38382;&#39064;&#30340;&#26032;&#31639;&#27861;&#65292;&#23427;&#37319;&#29992;&#22522;&#20110;&#20154;&#21475;&#30340;&#36827;&#21270;&#31639;&#27861;&#26469;&#20248;&#21270;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#19978;&#30340;&#36229;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2308.08634</link><description>&lt;p&gt;
FedPop: &#32852;&#37030;&#24335;&#22522;&#20110;&#20154;&#21475;&#30340;&#36229;&#21442;&#25968;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
FedPop: Federated Population-based Hyperparameter Tuning. (arXiv:2308.08634v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08634
&lt;/p&gt;
&lt;p&gt;
FedPop&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#35843;&#20248;&#38382;&#39064;&#30340;&#26032;&#31639;&#27861;&#65292;&#23427;&#37319;&#29992;&#22522;&#20110;&#20154;&#21475;&#30340;&#36827;&#21270;&#31639;&#27861;&#26469;&#20248;&#21270;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#19978;&#30340;&#36229;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#33539;&#24335;&#65292;&#22810;&#20010;&#23458;&#25143;&#31471;&#22312;&#19981;&#38598;&#20013;&#26412;&#22320;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20849;&#21516;&#35757;&#32451;ML&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#30340;ML&#27969;&#31243;&#31867;&#20284;&#65292;FL&#20013;&#30340;&#23458;&#25143;&#31471;&#26412;&#22320;&#20248;&#21270;&#21644;&#26381;&#21153;&#22120;&#32858;&#21512;&#36807;&#31243;&#23545;&#36229;&#21442;&#25968;&#65288;HP&#65289;&#30340;&#36873;&#25321;&#38750;&#24120;&#25935;&#24863;&#12290;&#23613;&#31649;&#22312;&#38598;&#20013;&#24335;ML&#20013;&#23545;&#35843;&#20248;HP&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;FL&#26102;&#20250;&#20135;&#29983;&#27425;&#20248;&#32467;&#26524;&#12290;&#36825;&#20027;&#35201;&#26159;&#22240;&#20026;&#23427;&#20204;&#30340;&#8220;&#35843;&#20248;&#21518;&#35757;&#32451;&#8221;&#26694;&#26550;&#23545;&#20110;&#35745;&#31639;&#33021;&#21147;&#26377;&#38480;&#30340;FL&#19981;&#21512;&#36866;&#12290;&#34429;&#28982;&#19968;&#20123;&#26041;&#27861;&#24050;&#32463;&#25552;&#20986;&#29992;&#20110;FL&#20013;&#30340;HP&#35843;&#20248;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20165;&#38480;&#20110;&#23458;&#25143;&#31471;&#26412;&#22320;&#26356;&#26032;&#30340;HP&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32852;&#37030;&#24335;&#22522;&#20110;&#20154;&#21475;&#30340;&#36229;&#21442;&#25968;&#35843;&#20248;&#65288;FedPop&#65289;&#30340;&#26032;&#22411;HP&#35843;&#20248;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;FedPop&#37319;&#29992;&#22522;&#20110;&#20154;&#21475;&#30340;&#36827;&#21270;&#31639;&#27861;&#26469;&#20248;&#21270;HP&#65292;&#27492;&#31639;&#27861;&#36866;&#29992;&#20110;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#19978;&#30340;&#21508;&#31181;HP&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a distributed machine learning (ML) paradigm, in which multiple clients collaboratively train ML models without centralizing their local data. Similar to conventional ML pipelines, the client local optimization and server aggregation procedure in FL are sensitive to the hyperparameter (HP) selection. Despite extensive research on tuning HPs for centralized ML, these methods yield suboptimal results when employed in FL. This is mainly because their "training-after-tuning" framework is unsuitable for FL with limited client computation power. While some approaches have been proposed for HP-Tuning in FL, they are limited to the HPs for client local updates. In this work, we propose a novel HP-tuning algorithm, called Federated Population-based Hyperparameter Tuning (FedPop), to address this vital yet challenging problem. FedPop employs population-based evolutionary algorithms to optimize the HPs, which accommodates various HP types at both client and server sides
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24490;&#29615;&#35760;&#24518;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;RMDT&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38271;&#24207;&#21015;&#38382;&#39064;&#12290;&#22312;Atari&#28216;&#25103;&#21644;MoJoCo&#25511;&#21046;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#37319;&#29992;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;RMDT&#27169;&#22411;&#26174;&#30528;&#20248;&#20110;&#20854;&#27809;&#26377;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;&#23545;&#24212;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.09459</link><description>&lt;p&gt;
&#24490;&#29615;&#35760;&#24518;&#20915;&#31574;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Recurrent Memory Decision Transformer. (arXiv:2306.09459v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24490;&#29615;&#35760;&#24518;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;RMDT&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38271;&#24207;&#21015;&#38382;&#39064;&#12290;&#22312;Atari&#28216;&#25103;&#21644;MoJoCo&#25511;&#21046;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#37319;&#29992;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;RMDT&#27169;&#22411;&#26174;&#30528;&#20248;&#20110;&#20854;&#27809;&#26377;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;&#23545;&#24212;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#38761;&#24615;&#27169;&#22411;&#26368;&#21021;&#26159;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#32780;&#24320;&#21457;&#30340;&#65292;&#26368;&#36817;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#36825;&#26159;&#22240;&#20026;&#20195;&#29702;&#30340;&#21382;&#21490;&#21487;&#20197;&#34920;&#31034;&#20026;&#24207;&#21015;&#65292;&#24182;&#19988;&#25972;&#20010;&#20219;&#21153;&#21487;&#20197;&#32553;&#20943;&#20026;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#21464;&#21387;&#22120;&#25805;&#20316;&#30340;&#20108;&#27425;&#22797;&#26434;&#24615;&#38480;&#21046;&#20102;&#19978;&#19979;&#25991;&#30340;&#28508;&#22312;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#20351;&#29992;&#20102;&#19981;&#21516;&#29256;&#26412;&#30340;&#35760;&#24518;&#26426;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24490;&#29615;&#35760;&#24518;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;RMDT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#20351;&#29992;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;Atari&#28216;&#25103;&#21644;MoJoCo&#25511;&#21046;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#39564;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;Atari&#28216;&#25103;&#19978;&#26174;&#30528;&#20248;&#20110;&#27809;&#26377;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;&#23545;&#24212;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#20180;&#32454;&#30740;&#31350;&#20102;&#35760;&#24518;&#23545;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#32489;&#25928;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#24320;&#21457;&#26356;&#39640;&#25928;&#21644;&#26356;&#26377;&#25928;&#30340;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformative models, originally developed for natural language problems, have recently been widely used in offline reinforcement learning tasks. This is due to the fact that the agent's history can be represented as a sequence, and the whole task can be reduced to the sequence modeling task. However, the quadratic complexity of the transformer operation limits the potential increase in context. Therefore, to work with long sequences in a natural language, different versions of the memory mechanism are used. In this paper, we propose the Recurrent Memory Decision Transformer (RMDT), a model that uses a recurrent memory mechanism for reinforcement learning problems. We conduct thorough experiments on Atari games and MoJoCo control problems, and show that our proposed model is significantly superior to its counterparts without the recurrent memory mechanism on Atari games. We also carefully study the effect of memory on the performance of the proposed model. These findings shed light on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#22810;&#20803;&#22270;&#20687;&#23436;&#25104;&#26041;&#27861;&#36827;&#34892;&#20083;&#33146;&#25195;&#25551;&#24322;&#24120;&#23450;&#20301;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25506;&#32034;&#23436;&#25104;&#30340;&#22810;&#20803;&#24615;&#26469;&#25552;&#39640;&#35780;&#20272;&#26631;&#20934;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.03098</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#22810;&#20803;&#22270;&#20687;&#23436;&#25104;&#36827;&#34892;&#39640;&#20998;&#36776;&#29575;&#20083;&#33146;&#25195;&#25551;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Unsupervised anomaly localization in high-resolution breast scans using deep pluralistic image completion. (arXiv:2305.03098v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#22810;&#20803;&#22270;&#20687;&#23436;&#25104;&#26041;&#27861;&#36827;&#34892;&#20083;&#33146;&#25195;&#25551;&#24322;&#24120;&#23450;&#20301;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25506;&#32034;&#23436;&#25104;&#30340;&#22810;&#20803;&#24615;&#26469;&#25552;&#39640;&#35780;&#20272;&#26631;&#20934;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#20083;&#33146;&#25668;&#24433;&#20013;&#30340;&#33258;&#21160;&#32959;&#30244;&#26816;&#27979;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#32959;&#30244;&#24456;&#23569;&#20986;&#29616;&#65292;&#20083;&#25151;&#32452;&#32455;&#21464;&#24322;&#21644;&#39640;&#20998;&#36776;&#29575;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#37492;&#20110;&#24322;&#24120;&#22270;&#20687;&#30340;&#31232;&#32570;&#24615;&#21644;&#27491;&#24120;&#22270;&#20687;&#30340;&#20016;&#23500;&#24615;&#65292;&#24322;&#24120;&#26816;&#27979;/&#23450;&#20301;&#26041;&#27861;&#21487;&#33021;&#38750;&#24120;&#36866;&#21512;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#23398;&#20064;&#20013;&#22823;&#37096;&#20998;&#24322;&#24120;&#23450;&#20301;&#30740;&#31350;&#38598;&#20013;&#22312;&#38750;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#22312;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36866;&#24212;&#24615;&#19981;&#36275;&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#35299;&#20915;&#22270;&#20687;&#23436;&#25104;&#35270;&#35282;&#19979;&#30340;&#20219;&#21153;&#24471;&#21040;&#32531;&#35299;&#65292;&#20854;&#20013;&#24322;&#24120;&#30340;&#23384;&#22312;&#21487;&#20197;&#36890;&#36807;&#21407;&#22987;&#22806;&#35266;&#19982;&#20854;&#29615;&#22659;&#26465;&#20214;&#19979;&#33258;&#21160;&#23436;&#25104;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#25351;&#31034;&#12290;&#28982;&#32780;&#65292;&#22312;DBT&#25968;&#25454;&#38598;&#20013;&#65292;&#24448;&#24448;&#26377;&#24456;&#22810;&#30456;&#21516;&#29615;&#22659;&#26465;&#20214;&#19979;&#30340;&#26377;&#25928;&#30340;&#27491;&#24120;&#23436;&#25104;&#65292;&#20351;&#36825;&#20010;&#35780;&#20272;&#26631;&#20934;&#19981;&#22826;&#31934;&#30830;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#32771;&#34385;&#36890;&#36807;&#25506;&#32034;&#23436;&#25104;&#30340;&#22810;&#20803;&#24615;&#26469;&#36827;&#34892;&#22270;&#20687;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated tumor detection in Digital Breast Tomosynthesis (DBT) is a difficult task due to natural tumor rarity, breast tissue variability, and high resolution. Given the scarcity of abnormal images and the abundance of normal images for this problem, an anomaly detection/localization approach could be well-suited. However, most anomaly localization research in machine learning focuses on non-medical datasets, and we find that these methods fall short when adapted to medical imaging datasets. The problem is alleviated when we solve the task from the image completion perspective, in which the presence of anomalies can be indicated by a discrepancy between the original appearance and its auto-completion conditioned on the surroundings. However, there are often many valid normal completions given the same surroundings, especially in the DBT dataset, making this evaluation criterion less precise. To address such an issue, we consider pluralistic image completion by exploring the distributi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#39537;&#21160;&#21464;&#21387;&#22120;&#30340;&#35270;&#39057; HMR &#26694;&#26550;&#65288;DDT&#65289;&#65292;&#23427;&#26088;&#22312;&#20174;&#36755;&#20837;&#24207;&#21015;&#20013;&#35299;&#30721;&#29305;&#23450;&#30340;&#36816;&#21160;&#27169;&#24335;&#65292;&#22686;&#24378;&#36816;&#21160;&#24179;&#28369;&#24615;&#21644;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;&#24182;&#36755;&#20986;&#25152;&#26377;&#24103;&#30340;&#20154;&#20307;&#32593;&#26684;&#65292;&#20351;&#24471; DDT &#26356;&#36866;&#29992;&#20110;&#26102;&#38388;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.13397</link><description>&lt;p&gt;
DDT&#65306;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#39537;&#21160;&#21464;&#21387;&#22120;&#30340;&#20174;&#35270;&#39057;&#20013;&#24674;&#22797;&#20154;&#20307;&#32593;&#26684;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DDT: A Diffusion-Driven Transformer-based Framework for Human Mesh Recovery from a Video. (arXiv:2303.13397v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13397
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#39537;&#21160;&#21464;&#21387;&#22120;&#30340;&#35270;&#39057; HMR &#26694;&#26550;&#65288;DDT&#65289;&#65292;&#23427;&#26088;&#22312;&#20174;&#36755;&#20837;&#24207;&#21015;&#20013;&#35299;&#30721;&#29305;&#23450;&#30340;&#36816;&#21160;&#27169;&#24335;&#65292;&#22686;&#24378;&#36816;&#21160;&#24179;&#28369;&#24615;&#21644;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;&#24182;&#36755;&#20986;&#25152;&#26377;&#24103;&#30340;&#20154;&#20307;&#32593;&#26684;&#65292;&#20351;&#24471; DDT &#26356;&#36866;&#29992;&#20110;&#26102;&#38388;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#32593;&#26684;&#24674;&#22797;&#65288;HMR&#65289;&#20026;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#20154;&#20307;&#20449;&#24687;&#65292;&#20363;&#22914;&#28216;&#25103;&#12289;&#20154;&#26426;&#20132;&#20114;&#21644;&#34394;&#25311;&#29616;&#23454;&#12290;&#19982;&#21333;&#19968;&#22270;&#20687;&#26041;&#27861;&#30456;&#27604;&#65292;&#22522;&#20110;&#35270;&#39057;&#30340;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;&#26102;&#38388;&#20449;&#24687;&#36890;&#36807;&#34701;&#21512;&#20154;&#20307;&#36816;&#21160;&#20808;&#39564;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20687; VIBE &#36825;&#26679;&#30340;&#22810;&#23545;&#22810;&#26041;&#27861;&#23384;&#22312;&#36816;&#21160;&#24179;&#28369;&#24615;&#21644;&#26102;&#38388;&#19968;&#33268;&#24615;&#30340;&#25361;&#25112;&#12290;&#32780;&#20687; TCMR &#21644; MPS-Net &#36825;&#26679;&#30340;&#22810;&#23545;&#19968;&#26041;&#27861;&#21017;&#20381;&#36182;&#20110;&#26410;&#26469;&#24103;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#26159;&#38750;&#22240;&#26524;&#21644;&#26102;&#38388;&#25928;&#29575;&#20302;&#19979;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25193;&#25955;&#39537;&#21160;&#21464;&#21387;&#22120;&#30340;&#35270;&#39057; HMR &#26694;&#26550;&#65288;DDT&#65289;&#12290;DDT &#26088;&#22312;&#20174;&#36755;&#20837;&#24207;&#21015;&#20013;&#35299;&#30721;&#29305;&#23450;&#30340;&#36816;&#21160;&#27169;&#24335;&#65292;&#22686;&#24378;&#36816;&#21160;&#24179;&#28369;&#24615;&#21644;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;&#20316;&#20026;&#19968;&#31181;&#22810;&#23545;&#22810;&#26041;&#27861;&#65292;DDT &#30340;&#35299;&#30721;&#22120;&#36755;&#20986;&#25152;&#26377;&#24103;&#30340;&#20154;&#20307;&#32593;&#26684;&#65292;&#20351; DDT &#26356;&#36866;&#29992;&#20110;&#26102;&#38388;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human mesh recovery (HMR) provides rich human body information for various real-world applications such as gaming, human-computer interaction, and virtual reality. Compared to single image-based methods, video-based methods can utilize temporal information to further improve performance by incorporating human body motion priors. However, many-to-many approaches such as VIBE suffer from motion smoothness and temporal inconsistency. While many-to-one approaches such as TCMR and MPS-Net rely on the future frames, which is non-causal and time inefficient during inference. To address these challenges, a novel Diffusion-Driven Transformer-based framework (DDT) for video-based HMR is presented. DDT is designed to decode specific motion patterns from the input sequence, enhancing motion smoothness and temporal consistency. As a many-to-many approach, the decoder of our DDT outputs the human mesh of all the frames, making DDT more viable for real-world applications where time efficiency is cruc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#36755;&#20837;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#20004;&#20010;&#28508;&#22312;&#32534;&#30721;&#25511;&#21046;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#31354;&#38388;&#32467;&#26500;&#21644;&#35821;&#20041;&#39118;&#26684;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#29992;&#37319;&#26679;&#25216;&#26415;&#21644;&#26102;&#38388;&#27493;&#30456;&#20851;&#30340;&#28508;&#22312;&#26435;&#37325;&#35843;&#24230;&#65292;&#23454;&#29616;&#20102;&#23545;&#29983;&#25104;&#36807;&#31243;&#30340;&#26356;&#22909;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2302.14368</link><description>&lt;p&gt;
&#23454;&#29616;&#25193;&#23637;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Enhanced Controllability of Diffusion Models. (arXiv:2302.14368v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#36755;&#20837;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#20004;&#20010;&#28508;&#22312;&#32534;&#30721;&#25511;&#21046;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#31354;&#38388;&#32467;&#26500;&#21644;&#35821;&#20041;&#39118;&#26684;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#29992;&#37319;&#26679;&#25216;&#26415;&#21644;&#26102;&#38388;&#27493;&#30456;&#20851;&#30340;&#28508;&#22312;&#26435;&#37325;&#35843;&#24230;&#65292;&#23454;&#29616;&#20102;&#23545;&#29983;&#25104;&#36807;&#31243;&#30340;&#26356;&#22909;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#36924;&#30495;&#12289;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#21487;&#25511;&#31243;&#24230;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#21463;&#22522;&#20110;GAN&#28508;&#22312;&#31354;&#38388;&#30340;&#22270;&#20687;&#25805;&#32437;&#25216;&#26415;&#21551;&#21457;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#26465;&#20214;&#20110;&#20004;&#20010;&#28508;&#22312;&#32534;&#30721;&#12289;&#19968;&#20010;&#31354;&#38388;&#20869;&#23481;&#25513;&#30721;&#21644;&#19968;&#20010;&#25153;&#24179;&#30340;&#26679;&#24335;&#23884;&#20837;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#20381;&#36182;&#20110;&#25193;&#25955;&#27169;&#22411;&#28176;&#36827;&#21435;&#22122;&#36807;&#31243;&#30340;&#24863;&#24615;&#20559;&#32622;&#65292;&#22312;&#31354;&#38388;&#32467;&#26500;&#25513;&#30721;&#20013;&#32534;&#30721;&#23039;&#21183;/&#24067;&#23616;&#20449;&#24687;&#65292;&#22312;&#26679;&#24335;&#20195;&#30721;&#20013;&#32534;&#30721;&#35821;&#20041;/&#26679;&#24335;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#29992;&#30340;&#37319;&#26679;&#25216;&#26415;&#26469;&#25913;&#21892;&#21487;&#25511;&#24615;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#21487;&#32452;&#21512;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20801;&#35768;&#37096;&#20998;&#20381;&#36182;&#20110;&#26465;&#20214;&#36755;&#20837;&#65292;&#20197;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#23545;&#27599;&#20010;&#28508;&#22312;&#20195;&#30721;&#21644;&#23427;&#20204;&#30340;&#32852;&#21512;&#20998;&#24067;&#37327;&#30340;&#25511;&#21046;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26102;&#38388;&#27493;&#30456;&#20851;&#30340;&#20869;&#23481;&#21644;&#26679;&#24335;&#28508;&#22312;&#26435;&#37325;&#35843;&#24230;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#25511;&#21046;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising Diffusion models have shown remarkable capabilities in generating realistic, high-quality and diverse images. However, the extent of controllability during generation is underexplored. Inspired by techniques based on GAN latent space for image manipulation, we train a diffusion model conditioned on two latent codes, a spatial content mask and a flattened style embedding. We rely on the inductive bias of the progressive denoising process of diffusion models to encode pose/layout information in the spatial structure mask and semantic/style information in the style code. We propose two generic sampling techniques for improving controllability. We extend composable diffusion models to allow for some dependence between conditional inputs, to improve the quality of generations while also providing control over the amount of guidance from each latent code and their joint distribution. We also propose timestep dependent weight scheduling for content and style latents to further impro
&lt;/p&gt;</description></item></channel></rss>