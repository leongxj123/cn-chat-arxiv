<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#20102;&#27169;&#22411;&#24320;&#25918;&#26694;&#26550;&#65288;MOF&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#25490;&#21517;&#20998;&#31867;&#31995;&#32479;&#65292;&#26681;&#25454;&#23436;&#25972;&#24615;&#21644;&#24320;&#25918;&#24615;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#26088;&#22312;&#20419;&#36827;&#23436;&#25972;&#24615;&#12289;&#24320;&#25918;&#24615;&#20197;&#21450;&#36981;&#24490;&#24320;&#25918;&#31185;&#23398;&#21407;&#21017;&#65292;&#21487;&#20197;&#24110;&#21161;&#20934;&#30830;&#35782;&#21035;&#27169;&#22411;&#30340;&#36879;&#26126;&#24615;&#21644;&#21487;&#37325;&#29616;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13784</link><description>&lt;p&gt;
&#27169;&#22411;&#24320;&#25918;&#26694;&#26550;: &#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#21487;&#37325;&#29616;&#24615;&#12289;&#36879;&#26126;&#24230;&#21644;&#21487;&#29992;&#24615;&#30340;&#23436;&#25972;&#24615;&#21644;&#24320;&#25918;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Model Openness Framework: Promoting Completeness and Openness for Reproducibility, Transparency and Usability in AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13784
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#27169;&#22411;&#24320;&#25918;&#26694;&#26550;&#65288;MOF&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#25490;&#21517;&#20998;&#31867;&#31995;&#32479;&#65292;&#26681;&#25454;&#23436;&#25972;&#24615;&#21644;&#24320;&#25918;&#24615;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#26088;&#22312;&#20419;&#36827;&#23436;&#25972;&#24615;&#12289;&#24320;&#25918;&#24615;&#20197;&#21450;&#36981;&#24490;&#24320;&#25918;&#31185;&#23398;&#21407;&#21017;&#65292;&#21487;&#20197;&#24110;&#21161;&#20934;&#30830;&#35782;&#21035;&#27169;&#22411;&#30340;&#36879;&#26126;&#24615;&#21644;&#21487;&#37325;&#29616;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#21487;&#33021;&#24615;&#65292;&#20294;&#20854;&#21830;&#19994;&#21270;&#24341;&#21457;&#20102;&#20851;&#20110;&#36879;&#26126;&#24230;&#12289;&#21487;&#37325;&#29616;&#24615;&#12289;&#20559;&#35265;&#21644;&#23433;&#20840;&#24615;&#30340;&#25285;&#24551;&#12290;&#35768;&#22810;"&#24320;&#28304;"&#30340;GAI&#27169;&#22411;&#32570;&#20047;&#23436;&#25972;&#29702;&#35299;&#21644;&#20877;&#29616;&#25152;&#24517;&#38656;&#30340;&#32452;&#20214;&#65292;&#19968;&#20123;&#37319;&#29992;&#38480;&#21046;&#24615;&#35768;&#21487;&#35777;&#65292;&#36825;&#31181;&#34892;&#20026;&#34987;&#31216;&#20026;"&#24320;&#28304;&#27927;&#30333;"&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#27169;&#22411;&#24320;&#25918;&#26694;&#26550;&#65288;MOF&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26681;&#25454;&#23436;&#25972;&#24615;&#21644;&#24320;&#25918;&#24615;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#25490;&#21517;&#20998;&#31867;&#30340;&#31995;&#32479;&#65292;&#36981;&#24490;&#24320;&#25918;&#31185;&#23398;&#12289;&#24320;&#28304;&#12289;&#24320;&#25918;&#25968;&#25454;&#21644;&#24320;&#25918;&#33719;&#21462;&#30340;&#21407;&#21017;&#12290;MOF&#35201;&#27714;&#27169;&#22411;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#30340;&#29305;&#23450;&#32452;&#20214;&#34987;&#21253;&#21547;&#24182;&#26681;&#25454;&#36866;&#24403;&#30340;&#24320;&#25918;&#35768;&#21487;&#35777;&#21457;&#24067;&#12290;&#35813;&#26694;&#26550;&#26088;&#22312;&#38450;&#27490;&#23459;&#31216;&#33258;&#24049;&#26159;&#24320;&#25918;&#30340;&#27169;&#22411;&#34987;&#35823;&#35299;&#65292;&#25351;&#23548;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#32773;&#20197;&#23485;&#26494;&#30340;&#35768;&#21487;&#35777;&#21457;&#24067;&#25152;&#26377;&#27169;&#22411;&#32452;&#20214;&#65292;&#24182;&#24110;&#21161;&#20844;&#21496;&#12289;&#23398;&#26415;&#30028;&#21644;&#29233;&#22909;&#32773;&#35782;&#21035;&#21487;&#20197;&#23433;&#20840;&#37319;&#29992;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13784v1 Announce Type: new  Abstract: Generative AI (GAI) offers unprecedented possibilities but its commercialization has raised concerns about transparency, reproducibility, bias, and safety. Many "open-source" GAI models lack the necessary components for full understanding and reproduction, and some use restrictive licenses, a practice known as "openwashing." We propose the Model Openness Framework (MOF), a ranked classification system that rates machine learning models based on their completeness and openness, following principles of open science, open source, open data, and open access. The MOF requires specific components of the model development lifecycle to be included and released under appropriate open licenses. This framework aims to prevent misrepresentation of models claiming to be open, guide researchers and developers in providing all model components under permissive licenses, and help companies, academia, and hobbyists identify models that can be safely adop
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#35760;&#24518;&#22686;&#24378;&#22270;&#33258;&#32534;&#30721;&#22120;&#65288;STRIPE&#65289;&#29992;&#20110;&#21160;&#24577;&#22270;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#38376;&#25511;&#26102;&#38388;&#21367;&#31215;&#23618;&#26469;&#25552;&#21462;&#31354;&#38388;&#29305;&#24449;&#21644;&#26102;&#38388;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2403.09039</link><description>&lt;p&gt;
&#31354;&#38388;-&#26102;&#38388;&#35760;&#24518;&#22686;&#24378;&#22270;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#21160;&#24577;&#22270;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Spatial-temporal Memories Enhanced Graph Autoencoder for Anomaly Detection in Dynamic Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09039
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#35760;&#24518;&#22686;&#24378;&#22270;&#33258;&#32534;&#30721;&#22120;&#65288;STRIPE&#65289;&#29992;&#20110;&#21160;&#24577;&#22270;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#38376;&#25511;&#26102;&#38388;&#21367;&#31215;&#23618;&#26469;&#25552;&#21462;&#31354;&#38388;&#29305;&#24449;&#21644;&#26102;&#38388;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22270;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#38754;&#20020;&#36739;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#22270;&#32467;&#26500;&#21644;&#23646;&#24615;&#30340;&#26102;&#38388;&#28436;&#21464;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31354;&#38388;-&#26102;&#38388;&#35760;&#24518;&#22686;&#24378;&#22270;&#33258;&#32534;&#30721;&#22120;&#65288;STRIPE&#65289;&#12290;STRIPE&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21644;&#38376;&#25511;&#26102;&#38388;&#21367;&#31215;&#23618;&#20998;&#21035;&#25552;&#21462;&#31354;&#38388;&#29305;&#24449;&#21644;&#26102;&#38388;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09039v1 Announce Type: cross  Abstract: Anomaly detection in dynamic graphs presents a significant challenge due to the temporal evolution of graph structures and attributes. The conventional approaches that tackle this problem typically employ an unsupervised learning framework, capturing normality patterns with exclusive normal data during training and identifying deviations as anomalies during testing. However, these methods face critical drawbacks: they either only depend on proxy tasks for general representation without directly pinpointing normal patterns, or they neglect to differentiate between spatial and temporal normality patterns, leading to diminished efficacy in anomaly detection. To address these challenges, we introduce a novel Spatial-Temporal memories-enhanced graph autoencoder (STRIPE). Initially, STRIPE employs Graph Neural Networks (GNNs) and gated temporal convolution layers to extract spatial features and temporal features, respectively. Then STRIPE in
&lt;/p&gt;</description></item><item><title>SM4Depth&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#39044;&#22788;&#29702;&#21333;&#20803;&#21644;&#28145;&#24230;&#38388;&#38548;&#31163;&#25955;&#21270;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#21333;&#30446;&#24230;&#37327;&#28145;&#24230;&#20272;&#35745;&#20013;&#30340;&#30456;&#26426;&#25935;&#24863;&#24615;&#12289;&#22330;&#26223;&#31934;&#24230;&#19981;&#19968;&#33268;&#21644;&#25968;&#25454;&#20381;&#36182;&#24615;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.08556</link><description>&lt;p&gt;
SM4Depth: &#19968;&#31181;&#36890;&#36807;&#21333;&#19968;&#27169;&#22411;&#23454;&#29616;&#36328;&#22810;&#25668;&#20687;&#22836;&#21644;&#22330;&#26223;&#30340;&#26080;&#32541;&#21333;&#30446;&#24230;&#37327;&#28145;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
SM4Depth: Seamless Monocular Metric Depth Estimation across Multiple Cameras and Scenes by One Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08556
&lt;/p&gt;
&lt;p&gt;
SM4Depth&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#39044;&#22788;&#29702;&#21333;&#20803;&#21644;&#28145;&#24230;&#38388;&#38548;&#31163;&#25955;&#21270;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#21333;&#30446;&#24230;&#37327;&#28145;&#24230;&#20272;&#35745;&#20013;&#30340;&#30456;&#26426;&#25935;&#24863;&#24615;&#12289;&#22330;&#26223;&#31934;&#24230;&#19981;&#19968;&#33268;&#21644;&#25968;&#25454;&#20381;&#36182;&#24615;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#30446;&#24230;&#37327;&#28145;&#24230;&#20272;&#35745;&#65288;MMDE&#65289;&#30340;&#27867;&#21270;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#30456;&#23545;&#28145;&#24230;&#21644;&#24230;&#37327;&#28145;&#24230;&#25110;&#23545;&#40784;&#36755;&#20837;&#22270;&#20687;&#28966;&#36317;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#38754;&#20020;&#30528;&#22312;&#30456;&#26426;&#12289;&#22330;&#26223;&#21644;&#25968;&#25454;&#32423;&#21035;&#19978;&#30340;&#25361;&#25112;&#65306;&#65288;1&#65289;&#23545;&#19981;&#21516;&#25668;&#20687;&#22836;&#30340;&#25935;&#24863;&#24615;&#65307;&#65288;2&#65289;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#31934;&#24230;&#19981;&#19968;&#33268;&#65307;&#65288;3&#65289;&#20381;&#36182;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#32541;&#30340;MMDE&#26041;&#27861;SM4Depth&#65292;&#20197;&#22312;&#21333;&#20010;&#32593;&#32476;&#20869;&#35299;&#20915;&#19978;&#36848;&#25152;&#26377;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08556v1 Announce Type: cross  Abstract: The generalization of monocular metric depth estimation (MMDE) has been a longstanding challenge. Recent methods made progress by combining relative and metric depth or aligning input image focal length. However, they are still beset by challenges in camera, scene, and data levels: (1) Sensitivity to different cameras; (2) Inconsistent accuracy across scenes; (3) Reliance on massive training data. This paper proposes SM4Depth, a seamless MMDE method, to address all the issues above within a single network. First, we reveal that a consistent field of view (FOV) is the key to resolve ``metric ambiguity'' across cameras, which guides us to propose a more straightforward preprocessing unit. Second, to achieve consistently high accuracy across scenes, we explicitly model the metric scale determination as discretizing the depth interval into bins and propose variation-based unnormalized depth bins. This method bridges the depth gap of divers
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#36229;&#22768;&#24515;&#21160;&#22270;&#20998;&#21106;&#65292;&#22312;&#22788;&#29702;&#37096;&#20998;&#26631;&#35760;&#25968;&#25454;&#26102;&#37319;&#29992;&#25913;&#36827;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.07818</link><description>&lt;p&gt;
&#26631;&#31614;&#20002;&#22833;&#29575;&#65306;&#21033;&#29992;&#20855;&#26377;&#22495;&#36716;&#31227;&#21644;&#37096;&#20998;&#26631;&#35760;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#25913;&#36827;&#28145;&#24230;&#23398;&#20064;&#36229;&#22768;&#24515;&#21160;&#22270;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Label Dropout: Improved Deep Learning Echocardiography Segmentation Using Multiple Datasets With Domain Shift and Partial Labelling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#36229;&#22768;&#24515;&#21160;&#22270;&#20998;&#21106;&#65292;&#22312;&#22788;&#29702;&#37096;&#20998;&#26631;&#35760;&#25968;&#25454;&#26102;&#37319;&#29992;&#25913;&#36827;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22768;&#24515;&#21160;&#22270;&#65288;&#36229;&#22768;&#65289;&#26159;&#35780;&#20272;&#24515;&#33039;&#21151;&#33021;&#26102;&#20351;&#29992;&#30340;&#31532;&#19968;&#31181;&#25104;&#20687;&#26041;&#24335;&#12290;&#20174;&#36229;&#22768;&#20013;&#27979;&#37327;&#21151;&#33021;&#29983;&#29289;&#26631;&#24535;&#29289;&#20381;&#36182;&#20110;&#23545;&#24515;&#33039;&#32467;&#26500;&#36827;&#34892;&#20998;&#21106;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34987;&#25552;&#20986;&#26469;&#33258;&#21160;&#21270;&#36825;&#19968;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#23558;&#36825;&#20123;&#24037;&#20855;&#36716;&#21270;&#20026;&#24191;&#27867;&#30340;&#20020;&#24202;&#24212;&#29992;&#65292;&#37325;&#35201;&#30340;&#26159;&#20998;&#21106;&#27169;&#22411;&#23545;&#21508;&#31181;&#22270;&#20687;&#20855;&#26377;&#40065;&#26834;&#24615;&#65288;&#20363;&#22914;&#65292;&#30001;&#19981;&#21516;&#25195;&#25551;&#20202;&#33719;&#24471;&#65292;&#30001;&#19981;&#21516;&#32423;&#21035;&#30340;&#19987;&#23478;&#25805;&#20316;&#21592;&#33719;&#24471;&#31561;&#65289;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#40065;&#26834;&#24615;&#27700;&#24179;&#65292;&#26377;&#24517;&#35201;&#20351;&#29992;&#22810;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#22312;&#20351;&#29992;&#22810;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#26102;&#38754;&#20020;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#26631;&#31614;&#23384;&#22312;&#30340;&#21464;&#21270;&#65292;&#21363;&#21512;&#24182;&#25968;&#25454;&#36890;&#24120;&#26159;&#37096;&#20998;&#26631;&#35760;&#30340;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#30340;&#25913;&#36827;&#26469;&#22788;&#29702;&#37096;&#20998;&#26631;&#35760;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35757;&#32451;&#30340;naively
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07818v1 Announce Type: cross  Abstract: Echocardiography (echo) is the first imaging modality used when assessing cardiac function. The measurement of functional biomarkers from echo relies upon the segmentation of cardiac structures and deep learning models have been proposed to automate the segmentation process. However, in order to translate these tools to widespread clinical use it is important that the segmentation models are robust to a wide variety of images (e.g. acquired from different scanners, by operators with different levels of expertise etc.). To achieve this level of robustness it is necessary that the models are trained with multiple diverse datasets. A significant challenge faced when training with multiple diverse datasets is the variation in label presence, i.e. the combined data are often partially-labelled. Adaptations of the cross entropy loss function have been proposed to deal with partially labelled data. In this paper we show that training naively 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ActGen&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#35757;&#32451;&#24863;&#30693;&#30340;&#26041;&#24335;&#26469;&#29983;&#25104;&#22270;&#20687;&#65292;&#20197;&#25552;&#39640;&#22270;&#20687;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;ActGen&#21033;&#29992;&#20027;&#21160;&#23398;&#20064;&#30340;&#29702;&#24565;&#65292;&#29983;&#25104;&#31867;&#20284;&#20110;&#25361;&#25112;&#24615;&#25110;&#34987;&#35823;&#20998;&#31867;&#26679;&#26412;&#30340;&#22270;&#20687;&#65292;&#24182;&#23558;&#20854;&#25972;&#21512;&#21040;&#35757;&#32451;&#38598;&#20013;&#65292;&#20174;&#32780;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.06517</link><description>&lt;p&gt;
&#22270;&#20687;&#20998;&#31867;&#30340;&#20027;&#21160;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Active Generation for Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06517
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ActGen&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#35757;&#32451;&#24863;&#30693;&#30340;&#26041;&#24335;&#26469;&#29983;&#25104;&#22270;&#20687;&#65292;&#20197;&#25552;&#39640;&#22270;&#20687;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;ActGen&#21033;&#29992;&#20027;&#21160;&#23398;&#20064;&#30340;&#29702;&#24565;&#65292;&#29983;&#25104;&#31867;&#20284;&#20110;&#25361;&#25112;&#24615;&#25110;&#34987;&#35823;&#20998;&#31867;&#26679;&#26412;&#30340;&#22270;&#20687;&#65292;&#24182;&#23558;&#20854;&#25972;&#21512;&#21040;&#35757;&#32451;&#38598;&#20013;&#65292;&#20174;&#32780;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#19981;&#26029;&#22686;&#24378;&#30340;&#33021;&#21147;&#31361;&#26174;&#20102;&#23427;&#20204;&#22312;&#25552;&#39640;&#22270;&#20687;&#20998;&#31867;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#35201;&#27714;&#29983;&#25104;&#30340;&#22270;&#20687;&#25968;&#37327;&#36828;&#36828;&#36229;&#36807;&#21407;&#22987;&#25968;&#25454;&#38598;&#65292;&#32780;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#21482;&#26377;&#26497;&#23567;&#30340;&#25913;&#36827;&#12290;&#36825;&#31181;&#35745;&#31639;&#26114;&#36149;&#19988;&#32791;&#26102;&#30340;&#36807;&#31243;&#38459;&#30861;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#19987;&#27880;&#20110;&#27169;&#22411;&#30340;&#20855;&#20307;&#38656;&#27714;&#21644;&#29305;&#24449;&#26469;&#25552;&#39640;&#22270;&#20687;&#29983;&#25104;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;ActGen&#20197;&#20027;&#21160;&#23398;&#20064;&#20026;&#20013;&#24515;&#21407;&#21017;&#65292;&#37319;&#29992;&#20102;&#19968;&#20010;&#38024;&#23545;&#35757;&#32451;&#24863;&#30693;&#30340;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#12290;&#23427;&#26088;&#22312;&#21019;&#24314;&#31867;&#20284;&#20110;&#24403;&#21069;&#27169;&#22411;&#36935;&#21040;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#25110;&#34987;&#35823;&#20998;&#31867;&#26679;&#26412;&#30340;&#22270;&#20687;&#65292;&#24182;&#23558;&#36825;&#20123;&#29983;&#25104;&#30340;&#22270;&#20687;&#32435;&#20837;&#35757;&#32451;&#38598;&#20197;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06517v1 Announce Type: cross  Abstract: Recently, the growing capabilities of deep generative models have underscored their potential in enhancing image classification accuracy. However, existing methods often demand the generation of a disproportionately large number of images compared to the original dataset, while having only marginal improvements in accuracy. This computationally expensive and time-consuming process hampers the practicality of such approaches. In this paper, we propose to address the efficiency of image generation by focusing on the specific needs and characteristics of the model. With a central tenet of active learning, our method, named ActGen, takes a training-aware approach to image generation. It aims to create images akin to the challenging or misclassified samples encountered by the current model and incorporates these generated images into the training set to augment model performance. ActGen introduces an attentive image guidance technique, usin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#21644;&#39044;&#35757;&#32451;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#36827;&#34892;&#25506;&#32034;&#12290;&#36890;&#36807;&#23545;&#38543;&#26426;&#21644;&#39044;&#35757;&#32451;&#34920;&#31034;&#36827;&#34892;&#32858;&#31867;&#65292;&#21487;&#20197;&#26377;&#25928;&#35745;&#31639;&#29366;&#24577;&#25968;&#65292;&#29305;&#21035;&#26159;&#22312;&#22810;&#32500;&#29615;&#22659;&#20013;&#39044;&#35757;&#32451;&#34920;&#31034;&#26356;&#21152;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.03138</link><description>&lt;p&gt;
Just Cluster It: &#19968;&#31181;&#20351;&#29992;&#32858;&#31867;&#21644;&#39044;&#35757;&#32451;&#34920;&#31034;&#36827;&#34892;&#39640;&#32500;&#31354;&#38388;&#25506;&#32034;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Just Cluster It: An Approach for Exploration in High-Dimensions using Clustering and Pre-Trained Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#21644;&#39044;&#35757;&#32451;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#36827;&#34892;&#25506;&#32034;&#12290;&#36890;&#36807;&#23545;&#38543;&#26426;&#21644;&#39044;&#35757;&#32451;&#34920;&#31034;&#36827;&#34892;&#32858;&#31867;&#65292;&#21487;&#20197;&#26377;&#25928;&#35745;&#31639;&#29366;&#24577;&#25968;&#65292;&#29305;&#21035;&#26159;&#22312;&#22810;&#32500;&#29615;&#22659;&#20013;&#39044;&#35757;&#32451;&#34920;&#31034;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#34920;&#24449;&#20026;&#20013;&#24515;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#32034;&#38382;&#39064;&#65292;&#23558;&#25506;&#32034;&#35270;&#20026;&#23494;&#24230;&#20272;&#35745;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19977;&#32500;&#29615;&#22659;&#20013;&#20351;&#29992;&#32858;&#31867;&#26469;&#36827;&#34892;&#25506;&#32034;&#30340;&#26377;&#25928;&#24615;&#65292;&#22522;&#20110;&#19968;&#20010;&#35266;&#23519;&#65306;&#22312;&#19977;&#32500;&#29615;&#22659;&#20013;&#65292;&#19982;&#20108;&#32500;&#29615;&#22659;&#30456;&#27604;&#65292;&#29366;&#24577;&#36716;&#25442;&#20013;&#30340;&#20687;&#32032;&#21464;&#21270;&#30340;&#37325;&#35201;&#24615;&#19981;&#37027;&#20040;&#26126;&#26174;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#38543;&#26426;&#34920;&#31034;&#21644;&#39044;&#35757;&#32451;DINO&#34920;&#31034;&#36827;&#34892;&#21608;&#26399;&#24615;&#21644;&#20840;&#23616;&#32858;&#31867;&#26469;&#35745;&#31639;&#29366;&#24577;&#25968;&#65292;&#21363;&#20272;&#35745;&#20266;&#35745;&#25968;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#21363;&#20351;&#26159;&#38543;&#26426;&#29305;&#24449;&#20063;&#21487;&#20197;&#22312;&#19977;&#32500;&#29615;&#22659;&#20013;&#26377;&#25928;&#22320;&#36827;&#34892;&#32858;&#31867;&#20197;&#35745;&#31639;&#29366;&#24577;&#25968;&#65292;&#28982;&#32780;&#24403;&#36825;&#20123;&#29305;&#24449;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#26102;&#65292;&#39044;&#35757;&#32451;DINO&#34920;&#31034;&#30001;&#20110;&#20854;&#39044;&#35757;&#32451;&#30340;&#24402;&#32435;&#20559;&#24046;&#22312;&#34920;&#31034;&#20013;&#26356;&#21152;&#26377;&#25928;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#36825;&#20026;&#38598;&#25104;&#39044;&#35757;&#32451;&#34920;&#31034;&#25552;&#20379;&#20102;&#19968;&#26465;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we adopt a representation-centric perspective on exploration in reinforcement learning, viewing exploration fundamentally as a density estimation problem. We investigate the effectiveness of clustering representations for exploration in 3-D environments, based on the observation that the importance of pixel changes between transitions is less pronounced in 3-D environments compared to 2-D environments, where pixel changes between transitions are typically distinct and significant. We propose a method that performs episodic and global clustering on random representations and on pre-trained DINO representations to count states, i.e, estimate pseudo-counts. Surprisingly, even random features can be clustered effectively to count states in 3-D environments, however when these become visually more complex, pre-trained DINO representations are more effective thanks to the pre-trained inductive biases in the representations. Overall, this presents a pathway for integrating pre-t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#21462;&#20195;&#32463;&#27982;&#23454;&#39564;&#23460;&#36827;&#34892;&#36873;&#25321;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17435</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#21462;&#20195;&#32463;&#27982;&#36873;&#25321;&#39044;&#27979;&#23454;&#39564;&#23460;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Replace Economic Choice Prediction Labs?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17435
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#21462;&#20195;&#32463;&#27982;&#23454;&#39564;&#23460;&#36827;&#34892;&#36873;&#25321;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#27982;&#36873;&#25321;&#39044;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#24448;&#24448;&#21463;&#38480;&#20110;&#33719;&#21462;&#20154;&#31867;&#36873;&#25321;&#25968;&#25454;&#30340;&#22256;&#38590;&#12290;&#23454;&#39564;&#32463;&#27982;&#23398;&#30740;&#31350;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#19987;&#27880;&#20110;&#31616;&#21333;&#30340;&#36873;&#25321;&#29615;&#22659;&#12290;&#26368;&#36817;&#65292;&#20154;&#24037;&#26234;&#33021;&#30028;&#20197;&#20004;&#31181;&#26041;&#24335;&#20026;&#35813;&#21162;&#21147;&#20570;&#20986;&#20102;&#36129;&#29486;&#65306;&#32771;&#34385;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#20195;&#26367;&#20154;&#31867;&#22312;&#19978;&#36848;&#31616;&#21333;&#36873;&#25321;&#39044;&#27979;&#29615;&#22659;&#20013;&#65292;&#20197;&#21450;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#35270;&#35282;&#30740;&#31350;&#26356;&#22797;&#26434;&#20294;&#20173;&#20005;&#26684;&#30340;&#23454;&#39564;&#32463;&#27982;&#23398;&#29615;&#22659;&#65292;&#21253;&#25324;&#19981;&#23436;&#20840;&#20449;&#24687;&#12289;&#37325;&#22797;&#21338;&#24328;&#21644;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#20132;&#27969;&#30340;&#35828;&#26381;&#28216;&#25103;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#28789;&#24863;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#23436;&#20840;&#27169;&#25311;&#32463;&#27982;&#29615;&#22659;&#65292;&#24182;&#29983;&#25104;&#29992;&#20110;&#39640;&#25928;&#20154;&#31867;&#36873;&#25321;&#39044;&#27979;&#30340;&#25968;&#25454;&#65292;&#26367;&#20195;&#22797;&#26434;&#30340;&#32463;&#27982;&#23454;&#39564;&#23460;&#30740;&#31350;&#65311;&#25105;&#20204;&#22312;&#36825;&#20010;&#20027;&#39064;&#19978;&#24320;&#21019;&#20102;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#34920;&#26126;&#20165;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Economic choice prediction is an essential challenging task, often constrained by the difficulties in acquiring human choice data. Indeed, experimental economics studies had focused mostly on simple choice settings. The AI community has recently contributed to that effort in two ways: considering whether LLMs can substitute for humans in the above-mentioned simple choice prediction settings, and the study through ML lens of more elaborated but still rigorous experimental economics settings, employing incomplete information, repetitive play, and natural language communication, notably language-based persuasion games. This leaves us with a major inspiration: can LLMs be used to fully simulate the economic environment and generate data for efficient human choice prediction, substituting for the elaborated economic lab studies? We pioneer the study of this subject, demonstrating its feasibility. In particular, we show that a model trained solely on LLM-generated data can effectively predic
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#24694;&#21155;&#22825;&#27668;&#30340;&#22810;&#27169;&#22359;&#21644;&#27169;&#22359;&#21270;&#31995;&#32479;&#26550;&#26500;&#65292;&#22312;WATonoBus&#24179;&#21488;&#19978;&#36827;&#34892;&#20102;&#23454;&#38469;&#27979;&#35797;&#65292;&#35777;&#26126;&#20854;&#33021;&#22815;&#35299;&#20915;&#20840;&#22825;&#20505;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#38754;&#20020;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2312.00938</link><description>&lt;p&gt;
WATonoBus&#65306;&#19968;&#31181;&#20840;&#22825;&#20505;&#33258;&#21160;&#24033;&#33322;&#36710;
&lt;/p&gt;
&lt;p&gt;
WATonoBus: An All Weather Autonomous Shuttle
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00938
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#24694;&#21155;&#22825;&#27668;&#30340;&#22810;&#27169;&#22359;&#21644;&#27169;&#22359;&#21270;&#31995;&#32479;&#26550;&#26500;&#65292;&#22312;WATonoBus&#24179;&#21488;&#19978;&#36827;&#34892;&#20102;&#23454;&#38469;&#27979;&#35797;&#65292;&#35777;&#26126;&#20854;&#33021;&#22815;&#35299;&#20915;&#20840;&#22825;&#20505;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#38754;&#20020;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#20840;&#22825;&#20505;&#36816;&#34892;&#20013;&#38754;&#20020;&#26174;&#33879;&#25361;&#25112;&#65292;&#28085;&#30422;&#20102;&#20174;&#24863;&#30693;&#21644;&#20915;&#31574;&#21040;&#36335;&#24452;&#35268;&#21010;&#21644;&#25511;&#21046;&#30340;&#21508;&#20010;&#27169;&#22359;&#12290;&#22797;&#26434;&#24615;&#28304;&#20110;&#38656;&#35201;&#35299;&#20915;&#20687;&#38632;&#12289;&#38634;&#21644;&#38654;&#31561;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#22312;&#33258;&#20027;&#24615;&#22534;&#26632;&#20013;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#27169;&#22411;&#21644;&#21333;&#27169;&#22359;&#26041;&#27861;&#36890;&#24120;&#32570;&#20047;&#19982;&#19978;&#28216;&#25110;&#19979;&#28216;&#20219;&#21153;&#30340;&#25972;&#20307;&#38598;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#32771;&#34385;&#24694;&#21155;&#22825;&#27668;&#30340;&#22810;&#27169;&#22359;&#21644;&#27169;&#22359;&#21270;&#31995;&#32479;&#26550;&#26500;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#20174;&#24863;&#30693;&#27700;&#24179;&#21040;&#20915;&#31574;&#21644;&#23433;&#20840;&#30417;&#27979;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#20363;&#22914;&#35206;&#30422;&#38634;&#30340;&#36335;&#32536;&#26816;&#27979;&#12290;&#36890;&#36807;&#22312;WATonoBus&#24179;&#21488;&#19978;&#27599;&#21608;&#26085;&#24120;&#26381;&#21153;&#36817;&#19968;&#24180;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#65292;&#24182;&#20174;&#36816;&#33829;&#20013;&#35266;&#23519;&#21040;&#30340;&#26497;&#31471;&#24773;&#20917;&#20013;&#33719;&#24471;&#23453;&#36149;&#30340;&#32463;&#39564;&#25945;&#35757;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00938v1 Announce Type: cross  Abstract: Autonomous vehicle all-weather operation poses significant challenges, encompassing modules from perception and decision-making to path planning and control. The complexity arises from the need to address adverse weather conditions like rain, snow, and fog across the autonomy stack. Conventional model-based and single-module approaches often lack holistic integration with upstream or downstream tasks. We tackle this problem by proposing a multi-module and modular system architecture with considerations for adverse weather across the perception level, through features such as snow covered curb detection, to decision-making and safety monitoring. Through daily weekday service on the WATonoBus platform for almost a year, we demonstrate that our proposed approach is capable of addressing adverse weather conditions and provide valuable learning from edge cases observed during operation.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25361;&#25112;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#24120;&#35265;&#33539;&#24335;&#65292;&#36890;&#36807;&#30740;&#31350;&#22312;&#21333;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#20248;&#21270;&#22120;&#36873;&#25321;&#22312;MTL&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#29702;&#35770;&#25512;&#23548;&#20986;&#20102;&#26799;&#24230;&#20914;&#31361;&#30340;&#35282;&#33394;&#12290;</title><link>https://arxiv.org/abs/2311.04698</link><description>&lt;p&gt;
&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#25361;&#25112;&#24120;&#35265;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Challenging Common Paradigms in Multi-Task Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04698
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25361;&#25112;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#24120;&#35265;&#33539;&#24335;&#65292;&#36890;&#36807;&#30740;&#31350;&#22312;&#21333;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#20248;&#21270;&#22120;&#36873;&#25321;&#22312;MTL&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#29702;&#35770;&#25512;&#23548;&#20986;&#20102;&#26799;&#24230;&#20914;&#31361;&#30340;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#24180;&#26469;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#21463;&#21040;&#20102;&#26497;&#22823;&#20851;&#27880;&#65292;&#20294;&#20854;&#22522;&#26412;&#26426;&#21046;&#20173;&#28982;&#30693;&#20043;&#29978;&#23569;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#24182;&#26410;&#24102;&#26469;&#19968;&#33268;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#30456;&#27604;&#21333;&#20219;&#21153;&#23398;&#20064;&#65288;STL&#65289;&#22522;&#32447;&#65292;&#24378;&#35843;&#20102;&#26356;&#28145;&#20837;&#20102;&#35299;MTL&#29305;&#23450;&#25361;&#25112;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;MTL&#20013;&#30340;&#33539;&#24335;&#65292;&#25552;&#20986;&#20102;&#20960;&#28857;&#20851;&#20110;STL&#30340;&#37325;&#35201;&#24433;&#21709;&#65306;&#39318;&#20808;&#65292;&#20248;&#21270;&#22120;&#30340;&#36873;&#25321;&#23545;MTL&#30340;&#24433;&#21709;&#21482;&#21463;&#21040;&#20102;&#36731;&#24494;&#30340;&#35843;&#26597;&#12290;&#25105;&#20204;&#36890;&#36807;&#21508;&#31181;&#23454;&#39564;&#30340;&#23454;&#35777;&#26041;&#27861;&#23637;&#31034;&#20102;&#24120;&#35265;STL&#24037;&#20855;&#65288;&#20363;&#22914;Adam&#20248;&#21270;&#22120;&#65289;&#22312;MTL&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;Adam&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#19968;&#23450;&#30340;&#20551;&#35774;&#19979;&#20174;&#29702;&#35770;&#19978;&#25512;&#23548;&#20986;&#37096;&#20998;&#25439;&#22833;&#23610;&#24230;&#19981;&#21464;&#24615;&#12290;&#20854;&#27425;&#65292;&#26799;&#24230;&#20914;&#31361;&#30340;&#27010;&#24565;&#32463;&#24120;&#34987;&#25551;&#36848;&#20026;MTL&#20013;&#30340;&#19968;&#20010;&#29305;&#23450;&#38382;&#39064;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#26799;&#24230;&#20914;&#31361;&#22312;MTL&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#23558;&#20854;&#19982;STL&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#35282;&#24230;&#26799;&#24230;&#23545;&#40784;&#26041;&#38754;&#65292;&#25105;&#20204;&#27809;&#26377;&#25214;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04698v3 Announce Type: replace-cross  Abstract: While multi-task learning (MTL) has gained significant attention in recent years, its underlying mechanisms remain poorly understood. Recent methods did not yield consistent performance improvements over single task learning (STL) baselines, underscoring the importance of gaining more profound insights about challenges specific to MTL. In our study, we challenge paradigms in MTL in the context of STL: First, the impact of the choice of optimizer has only been mildly investigated in MTL. We show the pivotal role of common STL tools such as the Adam optimizer in MTL empirically in various experiments. To further investigate Adam's effectiveness, we theoretical derive a partial loss-scale invariance under mild assumptions. Second, the notion of gradient conflicts has often been phrased as a specific problem in MTL. We delve into the role of gradient conflicts in MTL and compare it to STL. For angular gradient alignment we find no 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#65292;&#21033;&#29992;&#23618;&#27425;&#21270;&#26102;&#38388;&#36923;&#36753;&#35268;&#33539;&#23454;&#29616;&#21516;&#26102;&#30340;&#20219;&#21153;&#20998;&#37197;&#21644;&#35268;&#21010;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#23618;&#27425;&#21270;&#32467;&#26500;&#21040;LTL&#35268;&#33539;&#20013;&#65292;&#35813;&#26041;&#27861;&#26356;&#20855;&#34920;&#36798;&#33021;&#21147;&#12290;&#37319;&#29992;&#22522;&#20110;&#25628;&#32034;&#30340;&#26041;&#27861;&#26469;&#32508;&#21512;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#35745;&#21010;&#65292;&#23558;&#25628;&#32034;&#31354;&#38388;&#25286;&#20998;&#20026;&#26494;&#25955;&#30456;&#20114;&#36830;&#25509;&#30340;&#23376;&#31354;&#38388;&#65292;&#20197;&#20415;&#26356;&#39640;&#25928;&#22320;&#36827;&#34892;&#20219;&#21153;&#20998;&#37197;&#21644;&#35268;&#21010;&#12290;</title><link>http://arxiv.org/abs/2401.04003</link><description>&lt;p&gt;
&#22810;&#26426;&#22120;&#20154;&#22312;&#23618;&#27425;&#21270;&#26102;&#38388;&#36923;&#36753;&#35268;&#33539;&#19979;&#30340;&#20219;&#21153;&#20998;&#37197;&#21644;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Simultaneous Task Allocation and Planning for Multi-Robots under Hierarchical Temporal Logic Specifications. (arXiv:2401.04003v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04003
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#65292;&#21033;&#29992;&#23618;&#27425;&#21270;&#26102;&#38388;&#36923;&#36753;&#35268;&#33539;&#23454;&#29616;&#21516;&#26102;&#30340;&#20219;&#21153;&#20998;&#37197;&#21644;&#35268;&#21010;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#23618;&#27425;&#21270;&#32467;&#26500;&#21040;LTL&#35268;&#33539;&#20013;&#65292;&#35813;&#26041;&#27861;&#26356;&#20855;&#34920;&#36798;&#33021;&#21147;&#12290;&#37319;&#29992;&#22522;&#20110;&#25628;&#32034;&#30340;&#26041;&#27861;&#26469;&#32508;&#21512;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#35745;&#21010;&#65292;&#23558;&#25628;&#32034;&#31354;&#38388;&#25286;&#20998;&#20026;&#26494;&#25955;&#30456;&#20114;&#36830;&#25509;&#30340;&#23376;&#31354;&#38388;&#65292;&#20197;&#20415;&#26356;&#39640;&#25928;&#22320;&#36827;&#34892;&#20219;&#21153;&#20998;&#37197;&#21644;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20851;&#20110;&#26426;&#22120;&#20154;&#35268;&#21010;&#19982;&#26102;&#38388;&#36923;&#36753;&#35268;&#33539;&#30340;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#32447;&#24615;&#26102;&#38388;&#36923;&#36753;&#65288;LTL&#65289;&#65292;&#20027;&#35201;&#26159;&#22522;&#20110;&#38024;&#23545;&#20010;&#20307;&#25110;&#32676;&#20307;&#26426;&#22120;&#20154;&#30340;&#21333;&#19968;&#20844;&#24335;&#12290;&#20294;&#38543;&#30528;&#20219;&#21153;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#65292;LTL&#20844;&#24335;&#19981;&#21487;&#36991;&#20813;&#22320;&#21464;&#24471;&#20887;&#38271;&#65292;&#20351;&#35299;&#37322;&#21644;&#35268;&#33539;&#29983;&#25104;&#21464;&#24471;&#22797;&#26434;&#65292;&#21516;&#26102;&#36824;&#23545;&#35268;&#21010;&#22120;&#30340;&#35745;&#31639;&#33021;&#21147;&#36896;&#25104;&#21387;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#20219;&#21153;&#30340;&#20869;&#22312;&#32467;&#26500;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23618;&#27425;&#21270;&#32467;&#26500;&#21040;&#20855;&#26377;&#35821;&#27861;&#21644;&#35821;&#20041;&#38656;&#27714;&#30340;LTL&#35268;&#33539;&#20013;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#27604;&#25153;&#24179;&#35268;&#33539;&#26356;&#20855;&#34920;&#36798;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#25628;&#32034;&#30340;&#26041;&#27861;&#26469;&#32508;&#21512;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#35745;&#21010;&#65292;&#23454;&#29616;&#21516;&#26102;&#30340;&#20219;&#21153;&#20998;&#37197;&#21644;&#35268;&#21010;&#12290;&#25628;&#32034;&#31354;&#38388;&#30001;&#26494;&#25955;&#30456;&#20114;&#36830;&#25509;&#30340;&#23376;&#31354;&#38388;&#36817;&#20284;&#34920;&#31034;&#65292;&#27599;&#20010;&#23376;&#31354;&#38388;&#23545;&#24212;&#19968;&#20010;LTL&#35268;&#33539;&#12290;&#25628;&#32034;&#20027;&#35201;&#21463;&#38480;&#20110;&#21333;&#20010;&#23376;&#31354;&#38388;&#65292;&#26681;&#25454;&#29305;&#23450;&#26465;&#20214;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#23376;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Past research into robotic planning with temporal logic specifications, notably Linear Temporal Logic (LTL), was largely based on singular formulas for individual or groups of robots. But with increasing task complexity, LTL formulas unavoidably grow lengthy, complicating interpretation and specification generation, and straining the computational capacities of the planners. By leveraging the intrinsic structure of tasks, we introduced a hierarchical structure to LTL specifications with requirements on syntax and semantics, and proved that they are more expressive than their flat counterparts. Second, we employ a search-based approach to synthesize plans for a multi-robot system, accomplishing simultaneous task allocation and planning. The search space is approximated by loosely interconnected sub-spaces, with each sub-space corresponding to one LTL specification. The search is predominantly confined to a single sub-space, transitioning to another sub-space under certain conditions, de
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31895;&#31890;&#24230;&#24341;&#23548;&#26862;&#26519;&#21644;&#22810;&#20013;&#24515;&#25439;&#22833;&#30340;&#38271;&#23614;&#20998;&#31867;&#26694;&#26550;&#65292;&#21517;&#20026;Cognisance&#12290;&#35813;&#26694;&#26550;&#33268;&#21147;&#20110;&#35299;&#20915;&#38271;&#23614;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#31867;&#38388;&#21644;&#31867;&#20869;&#19981;&#24179;&#34913;&#65292;&#24182;&#36890;&#36807;&#19981;&#21464;&#29305;&#24449;&#23398;&#20064;&#26500;&#24314;&#22810;&#31890;&#24230;&#32852;&#21512;&#35299;&#20915;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.08206</link><description>&lt;p&gt;
&#22522;&#20110;&#31895;&#31890;&#24230;&#24341;&#23548;&#26862;&#26519;&#21644;&#22810;&#20013;&#24515;&#25439;&#22833;&#30340;&#38271;&#23614;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Long-Tailed Classification Based on Coarse-Grained Leading Forest and Multi-Center Loss. (arXiv:2310.08206v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31895;&#31890;&#24230;&#24341;&#23548;&#26862;&#26519;&#21644;&#22810;&#20013;&#24515;&#25439;&#22833;&#30340;&#38271;&#23614;&#20998;&#31867;&#26694;&#26550;&#65292;&#21517;&#20026;Cognisance&#12290;&#35813;&#26694;&#26550;&#33268;&#21147;&#20110;&#35299;&#20915;&#38271;&#23614;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#31867;&#38388;&#21644;&#31867;&#20869;&#19981;&#24179;&#34913;&#65292;&#24182;&#36890;&#36807;&#19981;&#21464;&#29305;&#24449;&#23398;&#20064;&#26500;&#24314;&#22810;&#31890;&#24230;&#32852;&#21512;&#35299;&#20915;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#23614;&#20998;&#31867;&#26159;&#29616;&#23454;&#19990;&#30028;&#20013;&#19981;&#21487;&#36991;&#20813;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#38271;&#23614;&#20998;&#31867;&#26041;&#27861;&#20165;&#20851;&#27880;&#35299;&#20915;&#31867;&#38388;&#19981;&#24179;&#34913;&#65292;&#21363;&#22836;&#37096;&#31867;&#21035;&#30340;&#26679;&#26412;&#27604;&#23614;&#37096;&#31867;&#21035;&#30340;&#26679;&#26412;&#22810;&#65292;&#32780;&#24573;&#30053;&#20102;&#31867;&#20869;&#19981;&#24179;&#34913;&#65292;&#21363;&#21516;&#19968;&#31867;&#21035;&#20013;&#22836;&#37096;&#23646;&#24615;&#26679;&#26412;&#25968;&#37327;&#36828;&#22823;&#20110;&#23614;&#37096;&#23646;&#24615;&#26679;&#26412;&#25968;&#37327;&#12290;&#27169;&#22411;&#30340;&#20559;&#24046;&#26159;&#30001;&#36825;&#20004;&#20010;&#22240;&#32032;&#24341;&#36215;&#30340;&#65292;&#30001;&#20110;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#20013;&#30340;&#23646;&#24615;&#26159;&#38544;&#21547;&#30340;&#19988;&#23646;&#24615;&#32452;&#21512;&#38750;&#24120;&#22797;&#26434;&#65292;&#22788;&#29702;&#31867;&#20869;&#19981;&#24179;&#34913;&#26356;&#21152;&#22256;&#38590;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31895;&#31890;&#24230;&#24341;&#23548;&#26862;&#26519;&#65288;CLF&#65289;&#21644;&#22810;&#20013;&#24515;&#25439;&#22833;&#65288;MCL&#65289;&#30340;&#38271;&#23614;&#20998;&#31867;&#26694;&#26550;&#65292;&#21517;&#20026;Cognisance&#65292;&#26088;&#22312;&#36890;&#36807;&#19981;&#21464;&#29305;&#24449;&#23398;&#20064;&#26500;&#24314;&#22810;&#31890;&#24230;&#32852;&#21512;&#35299;&#20915;&#27169;&#22411;&#12290;&#22312;&#36825;&#20010;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#21644;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#24179;&#34913;&#19981;&#21516;&#31867;&#21035;&#21644;&#23646;&#24615;&#20043;&#38388;&#30340;&#26679;&#26412;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-tailed(LT) classification is an unavoidable and challenging problem in the real world. Most of the existing long-tailed classification methods focus only on solving the inter-class imbalance in which there are more samples in the head class than in the tail class, while ignoring the intra-lass imbalance in which the number of samples of the head attribute within the same class is much larger than the number of samples of the tail attribute. The deviation in the model is caused by both of these factors, and due to the fact that attributes are implicit in most datasets and the combination of attributes is very complex, the intra-class imbalance is more difficult to handle. For this purpose, we proposed a long-tailed classification framework, known as \textbf{\textsc{Cognisance}}, which is founded on Coarse-Grained Leading Forest (CLF) and Multi-Center Loss (MCL), aiming to build a multi-granularity joint solution model by means of invariant feature learning. In this method, we desig
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#21487;&#20998;&#31163;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#65292;&#23427;&#36890;&#36807;&#23884;&#20837;&#21487;&#21152;&#24615;&#20998;&#31163;&#24615;&#26469;&#35299;&#20915;&#39640;&#32500;&#21704;&#23494;&#39039;&#31995;&#32479;&#20013;&#30340;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.01069</link><description>&lt;p&gt;
&#21487;&#20998;&#31163;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Separable Hamiltonian Neural Networks. (arXiv:2309.01069v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01069
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#21487;&#20998;&#31163;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#65292;&#23427;&#36890;&#36807;&#23884;&#20837;&#21487;&#21152;&#24615;&#20998;&#31163;&#24615;&#26469;&#35299;&#20915;&#39640;&#32500;&#21704;&#23494;&#39039;&#31995;&#32479;&#20013;&#30340;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#31163;&#25955;&#35266;&#27979;&#25968;&#25454;&#24314;&#27169;&#21160;&#21147;&#31995;&#32479;&#26159;&#29616;&#20195;&#31185;&#23398;&#21644;&#24037;&#31243;&#25968;&#25454;&#31995;&#32479;&#38754;&#20020;&#30340;&#25361;&#25112;&#20043;&#19968;&#12290; &#21704;&#23494;&#39039;&#31995;&#32479;&#26159;&#19968;&#31867;&#22522;&#26412;&#19988;&#24191;&#27867;&#23384;&#22312;&#30340;&#21160;&#21147;&#31995;&#32479;&#12290; &#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#26159;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#27721;&#23494;&#23572;&#39039;&#26041;&#31243;&#30340;&#23398;&#20064;&#20559;&#24046;&#19979;&#65292;&#20174;&#31163;&#25955;&#35266;&#27979;&#30340;&#21521;&#37327;&#22330;&#20013;&#26080;&#30417;&#30563;&#22320;&#22238;&#24402;&#21160;&#21147;&#31995;&#32479;&#30340;&#21704;&#23494;&#39039;&#37327;&#12290;&#28982;&#32780;&#65292;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#36890;&#24120;&#24456;&#22797;&#26434;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#20854;&#20013;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#29366;&#24577;&#31354;&#38388;&#30456;&#23545;&#20110;&#26679;&#26412;&#25968;&#37327;&#26159;&#24456;&#22823;&#30340;&#12290; &#26368;&#36817;&#21457;&#29616;&#30340;&#19968;&#31181;&#32531;&#35299;&#29366;&#24577;&#21464;&#37327;&#20043;&#38388;&#22797;&#26434;&#24615;&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#21487;&#21152;&#24615;&#20998;&#31163;&#24615;&#65292;&#24182;&#23558;&#35813;&#21487;&#21152;&#24615;&#20998;&#31163;&#24615;&#23884;&#20837;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#26681;&#25454;&#29289;&#29702;&#23398;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#26415;&#35821;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#21487;&#20998;&#31163;&#30340;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#20123;&#27169;&#22411;&#23884;&#20837;&#20102;&#21487;&#21152;&#24615;&#20998;&#31163;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The modelling of dynamical systems from discrete observations is a challenge faced by modern scientific and engineering data systems. Hamiltonian systems are one such fundamental and ubiquitous class of dynamical systems. Hamiltonian neural networks are state-of-the-art models that unsupervised-ly regress the Hamiltonian of a dynamical system from discrete observations of its vector field under the learning bias of Hamilton's equations. Yet Hamiltonian dynamics are often complicated, especially in higher dimensions where the state space of the Hamiltonian system is large relative to the number of samples. A recently discovered remedy to alleviate the complexity between state variables in the state space is to leverage the additive separability of the Hamiltonian system and embed that additive separability into the Hamiltonian neural network. Following the nomenclature of physics-informed machine learning, we propose three separable Hamiltonian neural networks. These models embed additi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#26410;&#26469;&#36235;&#21183;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#22240;&#26524;&#28151;&#28102;&#12289;&#40065;&#26834;&#24615;&#21644;&#19990;&#30028;&#27169;&#22411;&#31561;&#12290;&#36890;&#36807;&#32852;&#21512;&#29305;&#24449;&#20248;&#21270;&#24863;&#30693;&#21644;&#35268;&#21010;&#65292;&#31471;&#21040;&#31471;&#31995;&#32479;&#22312;&#24863;&#30693;&#21644;&#35268;&#21010;&#19978;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.16927</link><description>&lt;p&gt;
&#32447;&#26463;&#33258;&#21160;&#39550;&#39542;&#65306;&#25361;&#25112;&#19982;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
End-to-end Autonomous Driving: Challenges and Frontiers. (arXiv:2306.16927v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16927
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#26410;&#26469;&#36235;&#21183;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#22240;&#26524;&#28151;&#28102;&#12289;&#40065;&#26834;&#24615;&#21644;&#19990;&#30028;&#27169;&#22411;&#31561;&#12290;&#36890;&#36807;&#32852;&#21512;&#29305;&#24449;&#20248;&#21270;&#24863;&#30693;&#21644;&#35268;&#21010;&#65292;&#31471;&#21040;&#31471;&#31995;&#32479;&#22312;&#24863;&#30693;&#21644;&#35268;&#21010;&#19978;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#26041;&#27861;&#37319;&#29992;&#31471;&#21040;&#31471;&#31639;&#27861;&#26694;&#26550;&#65292;&#21033;&#29992;&#21407;&#22987;&#20256;&#24863;&#22120;&#36755;&#20837;&#29983;&#25104;&#36710;&#36742;&#36816;&#21160;&#35745;&#21010;&#65292;&#32780;&#19981;&#26159;&#19987;&#27880;&#20110;&#35832;&#22914;&#26816;&#27979;&#21644;&#36816;&#21160;&#39044;&#27979;&#31561;&#21333;&#20010;&#20219;&#21153;&#12290;&#19982;&#27169;&#22359;&#21270;&#27969;&#27700;&#32447;&#30456;&#27604;&#65292;&#31471;&#21040;&#31471;&#31995;&#32479;&#36890;&#36807;&#32852;&#21512;&#29305;&#24449;&#20248;&#21270;&#24863;&#30693;&#21644;&#35268;&#21010;&#26469;&#33719;&#30410;&#12290;&#36825;&#19968;&#39046;&#22495;&#22240;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#12289;&#38381;&#29615;&#35780;&#20272;&#20197;&#21450;&#33258;&#21160;&#39550;&#39542;&#31639;&#27861;&#22312;&#25361;&#25112;&#24615;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#25191;&#34892;&#25152;&#38656;&#30340;&#38656;&#27714;&#32780;&#34028;&#21187;&#21457;&#23637;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#20998;&#26512;&#20102;250&#22810;&#31687;&#35770;&#25991;&#65292;&#28085;&#30422;&#20102;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#30340;&#21160;&#26426;&#12289;&#36335;&#32447;&#22270;&#12289;&#26041;&#27861;&#35770;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#36235;&#21183;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#22810;&#27169;&#24577;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#22240;&#26524;&#28151;&#28102;&#12289;&#40065;&#26834;&#24615;&#21644;&#19990;&#30028;&#27169;&#22411;&#31561;&#20960;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#22522;&#30784;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The autonomous driving community has witnessed a rapid growth in approaches that embrace an end-to-end algorithm framework, utilizing raw sensor input to generate vehicle motion plans, instead of concentrating on individual tasks such as detection and motion prediction. End-to-end systems, in comparison to modular pipelines, benefit from joint feature optimization for perception and planning. This field has flourished due to the availability of large-scale datasets, closed-loop evaluation, and the increasing need for autonomous driving algorithms to perform effectively in challenging scenarios. In this survey, we provide a comprehensive analysis of more than 250 papers, covering the motivation, roadmap, methodology, challenges, and future trends in end-to-end autonomous driving. We delve into several critical challenges, including multi-modality, interpretability, causal confusion, robustness, and world models, amongst others. Additionally, we discuss current advancements in foundation
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21487;&#25511;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;COMODO&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#22238;&#24402;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;A-MDM&#65289;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#12289;&#38271;&#26102;&#38388;&#20869;&#30340;&#36816;&#21160;&#24207;&#21015;&#65292;&#20197;&#23454;&#29616;&#22312;&#21709;&#24212;&#20110;&#26102;&#21464;&#25511;&#21046;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23454;&#26102;&#36816;&#21160;&#21512;&#25104;&#12290;</title><link>http://arxiv.org/abs/2306.00416</link><description>&lt;p&gt;
&#21487;&#25511;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Controllable Motion Diffusion Model. (arXiv:2306.00416v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00416
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21487;&#25511;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;COMODO&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#22238;&#24402;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;A-MDM&#65289;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#12289;&#38271;&#26102;&#38388;&#20869;&#30340;&#36816;&#21160;&#24207;&#21015;&#65292;&#20197;&#23454;&#29616;&#22312;&#21709;&#24212;&#20110;&#26102;&#21464;&#25511;&#21046;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23454;&#26102;&#36816;&#21160;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#21160;&#30011;&#20013;&#65292;&#20026;&#34394;&#25311;&#35282;&#33394;&#29983;&#25104;&#36924;&#30495;&#19988;&#21487;&#25511;&#30340;&#36816;&#21160;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20174;&#22270;&#20687;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#25104;&#21151;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#23637;&#31034;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#22823;&#22810;&#38480;&#20110;&#31163;&#32447;&#24212;&#29992;&#65292;&#30446;&#26631;&#26159;&#29983;&#25104;&#21516;&#26102;&#29983;&#25104;&#25152;&#26377;&#27493;&#39588;&#30340;&#24207;&#21015;&#32423;&#29983;&#25104;&#12290;&#20026;&#20102;&#33021;&#22815;&#22312;&#21709;&#24212;&#20110;&#26102;&#21464;&#25511;&#21046;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#23454;&#26102;&#36816;&#21160;&#21512;&#25104;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#25511;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;COMODO&#65289;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20197;&#33258;&#22238;&#24402;&#36816;&#21160;&#25193;&#25955;&#27169;&#22411;&#65288;A-MDM&#65289;&#20026;&#22522;&#30784;&#65292;&#36880;&#27493;&#29983;&#25104;&#36816;&#21160;&#24207;&#21015;&#12290;&#36890;&#36807;&#31616;&#21333;&#22320;&#20351;&#29992;&#26631;&#20934;DDPM&#31639;&#27861;&#32780;&#26080;&#38656;&#20219;&#20309;&#39069;&#22806;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#20135;&#29983;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#36816;&#21160;&#25511;&#21046;&#19979;&#38271;&#26102;&#38388;&#20869;&#30340;&#39640;&#20445;&#30495;&#24230;&#36816;&#21160;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating realistic and controllable motions for virtual characters is a challenging task in computer animation, and its implications extend to games, simulations, and virtual reality. Recent studies have drawn inspiration from the success of diffusion models in image generation, demonstrating the potential for addressing this task. However, the majority of these studies have been limited to offline applications that target at sequence-level generation that generates all steps simultaneously. To enable real-time motion synthesis with diffusion models in response to time-varying control signals, we propose the framework of the Controllable Motion Diffusion Model (COMODO). Our framework begins with an auto-regressive motion diffusion model (A-MDM), which generates motion sequences step by step. In this way, simply using the standard DDPM algorithm without any additional complexity, our framework is able to generate high-fidelity motion sequences over extended periods with different type
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#31070;&#32463;&#32593;&#36335;&#23398;&#20064;&#36755;&#20837;&#20302;&#32500;&#24230;&#34920;&#31034;&#21644;&#26368;&#23567;&#21270;&#29305;&#24449;&#26144;&#23556;&#20013;&#30340;&#22797;&#26434;&#24615;/&#19981;&#35268;&#21017;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#25511;&#21046;&#20102;&#35268;&#24459;&#24615;&#65292;&#24182;&#21033;&#29992;&#29702;&#35770;&#24037;&#20855;&#35777;&#26126;&#20102;&#29942;&#39048;&#32467;&#26500;&#30340;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2305.19008</link><description>&lt;p&gt;
&#23398;&#20064;&#29305;&#24449;&#20013;&#30340;&#29942;&#39048;&#32467;&#26500;&#65306;&#20302;&#32500;&#24230;&#19982;&#35268;&#24459;&#24615;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Bottleneck Structure in Learned Features: Low-Dimension vs Regularity Tradeoff. (arXiv:2305.19008v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#31070;&#32463;&#32593;&#36335;&#23398;&#20064;&#36755;&#20837;&#20302;&#32500;&#24230;&#34920;&#31034;&#21644;&#26368;&#23567;&#21270;&#29305;&#24449;&#26144;&#23556;&#20013;&#30340;&#22797;&#26434;&#24615;/&#19981;&#35268;&#21017;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#25511;&#21046;&#20102;&#35268;&#24459;&#24615;&#65292;&#24182;&#21033;&#29992;&#29702;&#35770;&#24037;&#20855;&#35777;&#26126;&#20102;&#29942;&#39048;&#32467;&#26500;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30740;&#31350;&#34920;&#26126;&#65292;&#20855;&#26377;&#22823;&#28145;&#24230;$L$&#21644;$L_{2}$&#27491;&#21017;&#21270;&#30340;DNN&#20559;&#21521;&#20110;&#23398;&#20064;&#36755;&#20837;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#21487;&#20197;&#35299;&#37322;&#20026;&#26368;&#23567;&#21270;&#23398;&#20064;&#20989;&#25968;$f$&#30340;&#31209;$R^{(0)}(f)$&#30340;&#27010;&#24565;&#65292;&#20854;&#34987;&#25512;&#27979;&#20026;&#29942;&#39048;&#31209;&#12290;&#25105;&#20204;&#35745;&#31639;&#20102;&#36825;&#20010;&#32467;&#26524;&#30340;&#26377;&#38480;&#28145;&#24230;&#20462;&#27491;&#65292;&#25581;&#31034;&#20102;&#19968;&#20010;&#24230;&#37327;$R^{(1)}$&#30340;&#35268;&#24459;&#24615;&#65292;&#23427;&#25511;&#21046;&#20102;&#38597;&#21487;&#27604;&#30697;&#38453;$\left|Jf(x)\right|_{+}$&#30340;&#20266;&#34892;&#21015;&#24335;&#24182;&#22312;&#32452;&#21512;&#21644;&#21152;&#27861;&#19979;&#26159;&#27425;&#21487;&#21152;&#30340;&#12290;&#36825;&#20351;&#24471;&#32593;&#32476;&#21487;&#20197;&#22312;&#23398;&#20064;&#20302;&#32500;&#34920;&#31034;&#21644;&#26368;&#23567;&#21270;&#29305;&#24449;&#26144;&#23556;&#20013;&#30340;&#22797;&#26434;&#24615;/&#19981;&#35268;&#21017;&#24615;&#20043;&#38388;&#20445;&#25345;&#24179;&#34913;&#65292;&#20174;&#32780;&#23398;&#20064;&#8220;&#27491;&#30830;&#8221;&#30340;&#20869;&#37096;&#23610;&#23544;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22823;&#23398;&#20064;&#36895;&#29575;&#22914;&#20309;&#25511;&#21046;&#23398;&#20064;&#20989;&#25968;&#30340;&#35268;&#24459;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#29702;&#35770;&#24037;&#20855;&#35777;&#26126;&#20102;&#29942;&#39048;&#32467;&#26500;&#22312;$L\to\infty$&#26102;&#22312;&#23398;&#20064;&#29305;&#24449;&#20013;&#30340;&#29468;&#24819;&#65306;&#23545;&#20110;&#22823;&#28145;&#24230;&#65292;&#20960;&#20046;&#25152;&#26377;&#30340;&#38544;&#34255;&#34920;&#31034;&#37117;&#38598;&#20013;&#22312;...
&lt;/p&gt;
&lt;p&gt;
Previous work has shown that DNNs with large depth $L$ and $L_{2}$-regularization are biased towards learning low-dimensional representations of the inputs, which can be interpreted as minimizing a notion of rank $R^{(0)}(f)$ of the learned function $f$, conjectured to be the Bottleneck rank. We compute finite depth corrections to this result, revealing a measure $R^{(1)}$ of regularity which bounds the pseudo-determinant of the Jacobian $\left|Jf(x)\right|_{+}$ and is subadditive under composition and addition. This formalizes a balance between learning low-dimensional representations and minimizing complexity/irregularity in the feature maps, allowing the network to learn the `right' inner dimension. We also show how large learning rates also control the regularity of the learned function. Finally, we use these theoretical tools to prove the conjectured bottleneck structure in the learned features as $L\to\infty$: for large depths, almost all hidden representations concentrates aroun
&lt;/p&gt;</description></item></channel></rss>