<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36890;&#36807;&#20351;&#29992;Masked AutoEncoders&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;NeRF-MAE&#29992;&#20110;&#33258;&#30417;&#30563;&#19977;&#32500;&#34920;&#31034;&#23398;&#20064;&#65292;&#21033;&#29992;&#26631;&#20934;&#30340;&#19977;&#32500;Vision Transformers&#36866;&#24212;NeRF&#30340;&#29420;&#29305;&#20844;&#24335;&#65292;&#23558;NeRF&#30340;&#20307;&#31215;&#32593;&#26684;&#20316;&#20026;&#23494;&#38598;&#36755;&#20837;&#65292;&#20197;&#20135;&#29983;&#26377;&#25928;&#30340;&#19977;&#32500;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2404.01300</link><description>&lt;p&gt;
NeRF-MAE: &#33258;&#30417;&#30563;&#19977;&#32500;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;Masked AutoEncoders
&lt;/p&gt;
&lt;p&gt;
NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation Learning for Neural Radiance Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01300
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;Masked AutoEncoders&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;NeRF-MAE&#29992;&#20110;&#33258;&#30417;&#30563;&#19977;&#32500;&#34920;&#31034;&#23398;&#20064;&#65292;&#21033;&#29992;&#26631;&#20934;&#30340;&#19977;&#32500;Vision Transformers&#36866;&#24212;NeRF&#30340;&#29420;&#29305;&#20844;&#24335;&#65292;&#23558;NeRF&#30340;&#20307;&#31215;&#32593;&#26684;&#20316;&#20026;&#23494;&#38598;&#36755;&#20837;&#65292;&#20197;&#20135;&#29983;&#26377;&#25928;&#30340;&#19977;&#32500;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#31070;&#32463;&#22330;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#21331;&#36234;&#33021;&#21147;&#65292;&#33021;&#22815;&#29702;&#35299;&#19977;&#32500;&#35270;&#35273;&#19990;&#30028;&#65292;&#22914;&#25512;&#26029;&#35821;&#20041;&#12289;&#20960;&#20309;&#21644;&#21160;&#24577;&#31561;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#31070;&#32463;&#22330;&#22312;&#20174;&#20108;&#32500;&#22270;&#20687;&#20013;&#23494;&#38598;&#34920;&#31034;&#19977;&#32500;&#22330;&#26223;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#20855;&#20307;&#20351;&#29992;Masked AutoEncoders&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#20511;&#37492;&#20102;&#23558;transformers&#25193;&#23637;&#21040;&#26032;&#25968;&#25454;&#27169;&#24577;&#30340;&#20196;&#20154;&#24778;&#35766;&#30340;&#25104;&#21151;&#65292;&#21033;&#29992;&#26631;&#20934;&#30340;&#19977;&#32500;Vision Transformers&#26469;&#36866;&#24212;NeRF&#30340;&#29420;&#29305;&#20844;&#24335;&#12290;&#25105;&#20204;&#23558;NeRF&#30340;&#20307;&#31215;&#32593;&#26684;&#20316;&#20026;transformer&#30340;&#23494;&#38598;&#36755;&#20837;&#65292;&#19982;&#20854;&#20182;&#19977;&#32500;&#34920;&#31034;&#65288;&#22914;&#28857;&#20113;&#65289;&#36827;&#34892;&#23545;&#27604;&#65292;&#20854;&#20449;&#24687;&#23494;&#24230;&#21487;&#33021;&#19981;&#22343;&#21248;&#65292;&#32780;&#34920;&#31034;&#26159;&#19981;&#35268;&#21017;&#30340;&#12290;&#30001;&#20110;&#23558;masked autoencoders&#24212;&#29992;&#20110;&#31867;&#20284;NeRF&#36825;&#26679;&#30340;&#38544;&#24335;&#34920;&#31034;&#30340;&#22256;&#38590;&#65292;&#25105;&#20204;&#36873;&#25321;&#25552;&#21462;&#19968;&#20010;&#26174;&#24335;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01300v1 Announce Type: cross  Abstract: Neural fields excel in computer vision and robotics due to their ability to understand the 3D visual world such as inferring semantics, geometry, and dynamics. Given the capabilities of neural fields in densely representing a 3D scene from 2D images, we ask the question: Can we scale their self-supervised pretraining, specifically using masked autoencoders, to generate effective 3D representations from posed RGB images. Owing to the astounding success of extending transformers to novel data modalities, we employ standard 3D Vision Transformers to suit the unique formulation of NeRFs. We leverage NeRF's volumetric grid as a dense input to the transformer, contrasting it with other 3D representations such as pointclouds where the information density can be uneven, and the representation is irregular. Due to the difficulty of applying masked autoencoders to an implicit representation, such as NeRF, we opt for extracting an explicit repres
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#22270;&#21387;&#32553;&#26041;&#27861;&#65292;&#26088;&#22312;&#20943;&#23569;&#22270;&#31070;&#32463;&#32593;&#32476;&#25152;&#24102;&#26469;&#30340;&#19981;&#24517;&#35201;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14951</link><description>&lt;p&gt;
&#31616;&#21333;&#22270;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Simple Graph Condensation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14951
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#22270;&#21387;&#32553;&#26041;&#27861;&#65292;&#26088;&#22312;&#20943;&#23569;&#22270;&#31070;&#32463;&#32593;&#32476;&#25152;&#24102;&#26469;&#30340;&#19981;&#24517;&#35201;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22270;&#19978;&#32321;&#37325;&#30340;&#35757;&#32451;&#25104;&#26412;&#24050;&#32463;&#24341;&#36215;&#20102;&#23545;&#22270;&#21387;&#32553;&#30340;&#26497;&#22823;&#20852;&#36259;&#65292;&#28041;&#21450;&#35843;&#25972;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#23567;&#23610;&#24230;&#21387;&#32553;&#22270;&#19978;&#30340;&#35757;&#32451;&#20197;&#22312;&#22823;&#35268;&#27169;&#21407;&#22987;&#22270;&#19978;&#20351;&#29992;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#35843;&#25972;&#21387;&#32553;&#22270;&#21644;&#21407;&#22987;&#22270;&#20043;&#38388;&#30340;&#20851;&#38190;&#25351;&#26631;&#65292;&#22914;&#26799;&#24230;&#12289;GNNs&#30340;&#20998;&#24067;&#21644;&#36712;&#36857;&#65292;&#20174;&#32780;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22797;&#26434;&#25351;&#26631;&#38656;&#35201;&#22797;&#26434;&#30340;&#35745;&#31639;&#65292;&#21487;&#33021;&#20250;&#24178;&#25200;&#21387;&#32553;&#22270;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#20351;&#24471;&#21387;&#32553;&#36807;&#31243;&#38750;&#24120;&#32321;&#37325;&#21644;&#19981;&#31283;&#23450;&#12290;&#22312;&#21508;&#20010;&#39046;&#22495;&#31616;&#21270;&#27169;&#22411;&#21462;&#24471;&#25104;&#21151;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#22270;&#21387;&#32553;&#20013;&#30340;&#25351;&#26631;&#23545;&#20934;&#26041;&#27861;&#65292;&#26088;&#22312;&#20943;&#23569;&#20174;GNNs&#32487;&#25215;&#30340;&#19981;&#24517;&#35201;&#22797;&#26434;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#28040;&#38500;&#22806;&#37096;&#21442;&#25968;&#65292;&#20165;&#20445;&#30041;&#30446;&#26631;&#30340;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14951v1 Announce Type: cross  Abstract: The burdensome training costs on large-scale graphs have aroused significant interest in graph condensation, which involves tuning Graph Neural Networks (GNNs) on a small condensed graph for use on the large-scale original graph. Existing methods primarily focus on aligning key metrics between the condensed and original graphs, such as gradients, distribution and trajectory of GNNs, yielding satisfactory performance on downstream tasks. However, these complex metrics necessitate intricate computations and can potentially disrupt the optimization process of the condensation graph, making the condensation process highly demanding and unstable. Motivated by the recent success of simplified models in various fields, we propose a simplified approach to metric alignment in graph condensation, aiming to reduce unnecessary complexity inherited from GNNs. In our approach, we eliminate external parameters and exclusively retain the target conden
&lt;/p&gt;</description></item><item><title>&#22686;&#21152;&#39044;&#35757;&#32451;&#25968;&#25454;&#22810;&#26679;&#24615;&#21487;&#20197;&#25552;&#39640;&#33258;&#30417;&#30563;&#23398;&#20064;&#24615;&#33021;&#65292;&#20294;&#20165;&#22312;&#19982;&#19979;&#28216;&#25968;&#25454;&#30340;&#20998;&#24067;&#36317;&#31163;&#36739;&#23567;&#26102;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2403.13808</link><description>&lt;p&gt;
&#20851;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
On Pretraining Data Diversity for Self-Supervised Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13808
&lt;/p&gt;
&lt;p&gt;
&#22686;&#21152;&#39044;&#35757;&#32451;&#25968;&#25454;&#22810;&#26679;&#24615;&#21487;&#20197;&#25552;&#39640;&#33258;&#30417;&#30563;&#23398;&#20064;&#24615;&#33021;&#65292;&#20294;&#20165;&#22312;&#19982;&#19979;&#28216;&#25968;&#25454;&#30340;&#20998;&#24067;&#36317;&#31163;&#36739;&#23567;&#26102;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#26356;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#23545;&#33258;&#30417;&#30563;&#23398;&#20064;(SSL)&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#26159;&#21807;&#19968;&#26679;&#26412;&#25968;&#37327;&#65292;&#22312;&#22266;&#23450;&#30340;&#35745;&#31639;&#39044;&#31639;&#19979;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#19968;&#33268;&#34920;&#26126;&#65292;&#22686;&#21152;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#21487;&#20197;&#25552;&#39640;SSL&#24615;&#33021;&#65292;&#23613;&#31649;&#21482;&#26377;&#24403;&#19982;&#19979;&#28216;&#25968;&#25454;&#30340;&#20998;&#24067;&#36317;&#31163;&#24456;&#23567;&#30340;&#26102;&#20505;&#25165;&#26159;&#22914;&#27492;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21363;&#20351;&#36890;&#36807;&#32593;&#32476;&#29228;&#34411;&#25110;&#25193;&#25955;&#29983;&#25104;&#30340;&#25968;&#25454;&#31561;&#26041;&#24335;&#23454;&#29616;&#20102;&#24322;&#24120;&#22823;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#22810;&#26679;&#24615;&#65292;&#20998;&#24067;&#36716;&#31227;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#28085;&#30422;&#20102;&#19971;&#31181;SSL&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#35832;&#22914;ImageNet&#21644;YFCC100M&#31561;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24635;&#35745;&#36229;&#36807;200&#20010;GPU&#22825;&#12290;&#20195;&#30721;&#21644;&#35757;&#32451;&#27169;&#22411;&#23558;&#22312;https://github.com/hammoudhasan/DiversitySSL &#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13808v1 Announce Type: cross  Abstract: We explore the impact of training with more diverse datasets, characterized by the number of unique samples, on the performance of self-supervised learning (SSL) under a fixed computational budget. Our findings consistently demonstrate that increasing pretraining data diversity enhances SSL performance, albeit only when the distribution distance to the downstream data is minimal. Notably, even with an exceptionally large pretraining data diversity achieved through methods like web crawling or diffusion-generated data, among other ways, the distribution shift remains a challenge. Our experiments are comprehensive with seven SSL methods using large-scale datasets such as ImageNet and YFCC100M amounting to over 200 GPU days. Code and trained models will be available at https://github.com/hammoudhasan/DiversitySSL .
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20449;&#21495;&#20256;&#25773;&#29702;&#35770;&#65292;&#25552;&#20379;&#20102;&#25511;&#21046;transformer&#27169;&#22411;&#20449;&#21495;&#20256;&#25773;&#30340;&#20844;&#24335;&#65292;&#25552;&#20986;&#20102;DeepScaleLM&#21021;&#22987;&#21270;&#21644;&#32553;&#25918;&#26041;&#26696;&#65292;&#20351;&#24471;&#21487;&#20197;&#35757;&#32451;&#38750;&#24120;&#28145;&#30340;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#28145;&#23618;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#27973;&#23618;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.09635</link><description>&lt;p&gt;
Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models
&lt;/p&gt;
&lt;p&gt;
Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09635
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20449;&#21495;&#20256;&#25773;&#29702;&#35770;&#65292;&#25552;&#20379;&#20102;&#25511;&#21046;transformer&#27169;&#22411;&#20449;&#21495;&#20256;&#25773;&#30340;&#20844;&#24335;&#65292;&#25552;&#20986;&#20102;DeepScaleLM&#21021;&#22987;&#21270;&#21644;&#32553;&#25918;&#26041;&#26696;&#65292;&#20351;&#24471;&#21487;&#20197;&#35757;&#32451;&#38750;&#24120;&#28145;&#30340;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#28145;&#23618;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#27973;&#23618;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;transformer&#27169;&#22411;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#22312;&#28145;&#24230;&#26041;&#38754;&#20173;&#28982;&#24456;&#38590;&#25193;&#23637;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20449;&#21495;&#20256;&#25773;&#29702;&#35770;&#65292;&#24182;&#25552;&#20379;&#20102;&#25511;&#21046;transformer&#27169;&#22411;&#21069;&#21521;&#21644;&#21453;&#21521;&#20449;&#21495;&#30697;&#30340;&#20844;&#24335;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#29992;&#20110;&#29702;&#35299;&#21644;&#32531;&#35299;&#19982;&#39640;&#27880;&#24847;&#21147;&#20998;&#25968;&#30456;&#20851;&#30340;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#12289;&#31209;&#22349;&#32553;&#21644;&#19981;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;DeepScaleLM&#65292;&#19968;&#31181;&#21021;&#22987;&#21270;&#21644;&#32553;&#25918;&#26041;&#26696;&#65292;&#36890;&#36807;&#35813;&#26041;&#26696;&#33021;&#22815;&#22312;&#27169;&#22411;&#20013;&#20445;&#25345;&#21333;&#20301;&#36755;&#20986;/&#26799;&#24230;&#30697;&#65292;&#20174;&#32780;&#20351;&#35757;&#32451;&#20855;&#26377;100&#22810;&#23618;&#30340;&#38750;&#24120;&#28145;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;transformer&#27169;&#22411;&#21487;&#20197;&#26356;&#28145; - &#25105;&#20204;&#30340;&#28145;&#23618;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#12289;&#35821;&#38899;&#32763;&#35793;&#21644;&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#21253;&#25324;&#20165;&#32534;&#30721;&#22120;&#12289;&#20165;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21464;&#20307;&#65292;&#36866;&#29992;&#20110;Pre-LN&#21644;Post-LN transformers&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09635v1 Announce Type: cross  Abstract: In spite of their huge success, transformer models remain difficult to scale in depth. In this work, we develop a unified signal propagation theory and provide formulae that govern the moments of the forward and backward signal through the transformer model. Our framework can be used to understand and mitigate vanishing/exploding gradients, rank collapse, and instability associated with high attention scores. We also propose DeepScaleLM, an initialization and scaling scheme that conserves unit output/gradient moments throughout the model, enabling the training of very deep models with 100s of layers. We find that transformer models could be much deeper - our deep models with fewer parameters outperform shallow models in Language Modeling, Speech Translation, and Image Classification, across Encoder-only, Decoder-only and Encoder-Decoder variants, for both Pre-LN and Post-LN transformers, for multiple datasets and model sizes. These imp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#22330;&#65288;BayesNF&#65289;&#65292;&#32467;&#21512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#20998;&#23618;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#29992;&#20110;&#22788;&#29702;&#22823;&#35268;&#27169;&#26102;&#31354;&#39044;&#27979;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07657</link><description>&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#22330;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#26102;&#31354;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Scalable Spatiotemporal Prediction with Bayesian Neural Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07657
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#22330;&#65288;BayesNF&#65289;&#65292;&#32467;&#21512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#20998;&#23618;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#29992;&#20110;&#22788;&#29702;&#22823;&#35268;&#27169;&#26102;&#31354;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#25968;&#25454;&#38598;&#30001;&#31354;&#38388;&#21442;&#32771;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#31185;&#23398;&#21644;&#21830;&#19994;&#26234;&#33021;&#39046;&#22495;&#65292;&#20363;&#22914;&#31354;&#27668;&#27745;&#26579;&#30417;&#27979;&#65292;&#30142;&#30149;&#36319;&#36394;&#21644;&#20113;&#38656;&#27714;&#39044;&#27979;&#12290;&#38543;&#30528;&#29616;&#20195;&#25968;&#25454;&#38598;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#38656;&#35201;&#26032;&#30340;&#32479;&#35745;&#26041;&#27861;&#26469;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#31354;&#21160;&#24577;&#24182;&#22788;&#29702;&#22823;&#35268;&#27169;&#39044;&#27979;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;Bayesian Neural Field (BayesNF)&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#25512;&#26029;&#26102;&#31354;&#22495;&#19978;&#20016;&#23500;&#27010;&#29575;&#20998;&#24067;&#30340;&#36890;&#29992;&#39046;&#22495;&#32479;&#35745;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#21253;&#25324;&#39044;&#27979;&#12289;&#25554;&#20540;&#21644;&#21464;&#24322;&#20998;&#26512;&#22312;&#20869;&#30340;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#12290;BayesNF&#23558;&#29992;&#20110;&#39640;&#23481;&#37327;&#20989;&#25968;&#20272;&#35745;&#30340;&#26032;&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#19982;&#29992;&#20110;&#40065;&#26834;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#20998;&#23618;&#36125;&#21494;&#26031;&#25512;&#26029;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#22312;&#23450;&#20041;&#20808;&#39564;&#20998;&#24067;&#26041;&#38754;&#36827;&#34892;&#24207;&#21015;&#21270;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07657v1 Announce Type: cross  Abstract: Spatiotemporal datasets, which consist of spatially-referenced time series, are ubiquitous in many scientific and business-intelligence applications, such as air pollution monitoring, disease tracking, and cloud-demand forecasting. As modern datasets continue to increase in size and complexity, there is a growing need for new statistical methods that are flexible enough to capture complex spatiotemporal dynamics and scalable enough to handle large prediction problems. This work presents the Bayesian Neural Field (BayesNF), a domain-general statistical model for inferring rich probability distributions over a spatiotemporal domain, which can be used for data-analysis tasks including forecasting, interpolation, and variography. BayesNF integrates a novel deep neural network architecture for high-capacity function estimation with hierarchical Bayesian inference for robust uncertainty quantification. By defining the prior through a sequenc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;MultiQ&#22522;&#20934;&#65292;&#35843;&#26597;&#20102;&#26368;&#20808;&#36827;&#30340;&#24320;&#25918;LLMs&#22312;&#20854;&#39044;&#26399;&#20351;&#29992;&#33539;&#22260;&#20043;&#22806;&#30340;&#22522;&#26412;&#22810;&#35821;&#33021;&#21147;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#33267;&#23569;&#26576;&#20123;&#35821;&#35328;&#33021;&#22815;&#24544;&#23454;&#21644;&#20934;&#30830;&#22320;&#36827;&#34892;&#22238;&#31572;&#12290;</title><link>https://arxiv.org/abs/2403.03814</link><description>&lt;p&gt;
&#29992;MultiQ&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#26412;&#22810;&#35821;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Elementary Multilingual Capabilities of Large Language Models with MultiQ
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;MultiQ&#22522;&#20934;&#65292;&#35843;&#26597;&#20102;&#26368;&#20808;&#36827;&#30340;&#24320;&#25918;LLMs&#22312;&#20854;&#39044;&#26399;&#20351;&#29992;&#33539;&#22260;&#20043;&#22806;&#30340;&#22522;&#26412;&#22810;&#35821;&#33021;&#21147;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#33267;&#23569;&#26576;&#20123;&#35821;&#35328;&#33021;&#22815;&#24544;&#23454;&#21644;&#20934;&#30830;&#22320;&#36827;&#34892;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38656;&#35201;&#20026;&#20840;&#29699;&#22823;&#22810;&#25968;&#38750;&#33521;&#35821;&#20351;&#29992;&#32773;&#25552;&#20379;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;LLMs&#20170;&#22825;&#65292;&#29305;&#21035;&#26159;&#24320;&#25918;&#30340;LLMs&#65292;&#36890;&#24120;&#20165;&#24847;&#20026;&#22312;&#33521;&#35821;&#65288;&#20363;&#22914;Llama2&#12289;Mistral&#65289;&#25110;&#23569;&#25968;&#20960;&#31181;&#39640;&#36164;&#28304;&#35821;&#35328;&#65288;&#20363;&#22914;Mixtral&#12289;Qwen&#65289;&#20013;&#20351;&#29992;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#23384;&#22312;&#20351;&#29992;&#19978;&#30340;&#38480;&#21046;&#65292;&#20154;&#20204;&#20250;&#29992;&#35768;&#22810;&#19981;&#21516;&#30340;&#35821;&#35328;&#25552;&#31034;LLMs&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#26368;&#20808;&#36827;&#30340;&#24320;&#25918;LLMs&#22312;&#20854;&#39044;&#26399;&#20351;&#29992;&#33539;&#22260;&#20043;&#22806;&#30340;&#22522;&#26412;&#22810;&#35821;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MultiQ&#65292;&#19968;&#20010;&#26032;&#30340;&#29992;&#20110;&#22522;&#26412;&#24320;&#25918;&#24335;&#38382;&#31572;&#30340;&#38134;&#26631;&#20934;&#22522;&#20934;&#65292;&#28085;&#30422;137&#31181;&#35821;&#35328;&#30340;27.4k&#20010;&#27979;&#35797;&#38382;&#39064;&#12290;&#36890;&#36807;MultiQ&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#35821;&#35328;&#24544;&#23454;&#24230;&#65292;&#21363;&#27169;&#22411;&#26159;&#21542;&#20197;&#25552;&#31034;&#30340;&#35821;&#35328;&#22238;&#22797;&#65292;&#20197;&#21450;&#38382;&#39064;&#22238;&#31572;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#27979;&#35797;&#30340;&#25152;&#26377;LLMs&#23545;&#33267;&#23569;&#26576;&#20123;&#35821;&#35328;&#21709;&#24212;&#24471;&#24544;&#23454;&#21644;/&#25110;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03814v1 Announce Type: cross  Abstract: Large language models (LLMs) need to serve everyone, including a global majority of non-English speakers. However, most LLMs today, and open LLMs in particular, are often intended for use in just English (e.g. Llama2, Mistral) or a small handful of high-resource languages (e.g. Mixtral, Qwen). Recent research shows that, despite limits in their intended use, people prompt LLMs in many different languages. Therefore, in this paper, we investigate the basic multilingual capabilities of state-of-the-art open LLMs beyond their intended use. For this purpose, we introduce MultiQ, a new silver standard benchmark for basic open-ended question answering with 27.4k test questions across a typologically diverse set of 137 languages. With MultiQ, we evaluate language fidelity, i.e.\ whether models respond in the prompted language, and question answering accuracy. All LLMs we test respond faithfully and/or accurately for at least some languages be
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#26465;&#20214;&#22270;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#24341;&#23548;&#34920;&#31034;&#25351;&#23548;&#29983;&#25104;&#36807;&#31243;&#65292;&#26126;&#30830;&#24314;&#27169;&#21644;&#21033;&#29992;&#22270;&#20998;&#24067;&#65292;&#20248;&#20110;&#20256;&#32479;&#38544;&#24335;&#25429;&#33719;&#20998;&#24067;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.01071</link><description>&lt;p&gt;
GraphRCG: &#36890;&#36807;&#33258;&#24341;&#23548;&#34920;&#31034;&#30340;&#33258;&#26465;&#20214;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
GraphRCG: Self-conditioned Graph Generation via Bootstrapped Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01071
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#26465;&#20214;&#22270;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#24341;&#23548;&#34920;&#31034;&#25351;&#23548;&#29983;&#25104;&#36807;&#31243;&#65292;&#26126;&#30830;&#24314;&#27169;&#21644;&#21033;&#29992;&#22270;&#20998;&#24067;&#65292;&#20248;&#20110;&#20256;&#32479;&#38544;&#24335;&#25429;&#33719;&#20998;&#24067;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#29983;&#25104;&#36890;&#24120;&#26088;&#22312;&#21019;&#24314;&#19982;&#29305;&#23450;&#22270;&#20998;&#24067;&#23494;&#20999;&#23545;&#40784;&#30340;&#26032;&#22270;&#12290;&#29616;&#26377;&#30740;&#31350;&#24448;&#24448;&#36890;&#36807;&#29983;&#25104;&#22120;&#30340;&#20248;&#21270;&#38544;&#24335;&#25429;&#33719;&#36825;&#31181;&#20998;&#24067;&#65292;&#21487;&#33021;&#24573;&#35270;&#20998;&#24067;&#26412;&#36523;&#30340;&#22797;&#26434;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#24573;&#30053;&#20102;&#23398;&#20064;&#21040;&#30340;&#20998;&#24067;&#23545;&#22270;&#29983;&#25104;&#30340;&#35265;&#35299;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#26465;&#20214;&#22270;&#29983;&#25104;&#26694;&#26550;&#65292;&#26088;&#22312;&#26126;&#30830;&#24314;&#27169;&#22270;&#20998;&#24067;&#24182;&#21033;&#29992;&#36825;&#20123;&#20998;&#24067;&#26469;&#25351;&#23548;&#29983;&#25104;&#36807;&#31243;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#33258;&#26465;&#20214;&#24314;&#27169;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#22270;&#26679;&#26412;&#36716;&#25442;&#20026;&#20302;&#32500;&#34920;&#31034;&#65292;&#24182;&#20248;&#21270;&#19968;&#20010;&#34920;&#31034;&#29983;&#25104;&#22120;&#26469;&#25429;&#33719;&#22270;&#20998;&#24067;&#24182;&#29983;&#25104;&#21453;&#26144;&#23398;&#20064;&#20998;&#24067;&#30340;&#26032;&#34920;&#31034;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#33258;&#24341;&#23548;&#34920;&#31034;&#20316;&#20026;&#33258;&#26465;&#20214;&#25351;&#23548;&#26469;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01071v1 Announce Type: cross  Abstract: Graph generation generally aims to create new graphs that closely align with a specific graph distribution. Existing works often implicitly capture this distribution through the optimization of generators, potentially overlooking the intricacies of the distribution itself. Furthermore, these approaches generally neglect the insights offered by the learned distribution for graph generation. In contrast, in this work, we propose a novel self-conditioned graph generation framework designed to explicitly model graph distributions and employ these distributions to guide the generation process. We first perform self-conditioned modeling to capture the graph distributions by transforming each graph sample into a low-dimensional representation and optimizing a representation generator to create new representations reflective of the learned distribution. Subsequently, we leverage these bootstrapped representations as self-conditioned guidance f
&lt;/p&gt;</description></item><item><title>&#35821;&#20041;&#21464;&#21270;&#23545;&#35745;&#31639;&#35821;&#35328;&#23398;&#31639;&#27861;&#30340;&#32467;&#26524;&#36136;&#37327;&#21487;&#33021;&#20250;&#20135;&#29983;&#24433;&#21709;&#65292;&#22240;&#27492;&#37325;&#35201;&#24615;&#26085;&#30410;&#20984;&#26174;&#12290;</title><link>https://arxiv.org/abs/2402.19088</link><description>&lt;p&gt;
&#23545;&#35821;&#20041;&#21464;&#21270;&#29305;&#24449;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Survey in Characterization of Semantic Change
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19088
&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#21464;&#21270;&#23545;&#35745;&#31639;&#35821;&#35328;&#23398;&#31639;&#27861;&#30340;&#32467;&#26524;&#36136;&#37327;&#21487;&#33021;&#20250;&#20135;&#29983;&#24433;&#21709;&#65292;&#22240;&#27492;&#37325;&#35201;&#24615;&#26085;&#30410;&#20984;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27963;&#35821;&#35328;&#19981;&#26029;&#21457;&#23637;&#65292;&#20197;&#21560;&#32435;&#20154;&#31867;&#31038;&#20250;&#30340;&#25991;&#21270;&#21464;&#21270;&#12290;&#36825;&#31181;&#28436;&#21464;&#36890;&#36807;&#26032;&#35789;&#35821;&#65288;&#26032;&#21333;&#35789;&#65289;&#25110;&#21333;&#35789;&#30340;&#35821;&#20041;&#21464;&#21270;&#65288;&#36171;&#20104;&#24050;&#26377;&#21333;&#35789;&#26032;&#30340;&#21547;&#20041;&#65289;&#26469;&#20307;&#29616;&#12290;&#29702;&#35299;&#21333;&#35789;&#30340;&#21547;&#20041;&#23545;&#35299;&#37322;&#26469;&#33258;&#19981;&#21516;&#25991;&#21270;&#65288;&#22320;&#26041;&#29992;&#35821;&#25110;&#20442;&#35821;&#65289;&#12289;&#39046;&#22495;&#65288;&#20363;&#22914;&#25216;&#26415;&#26415;&#35821;&#65289;&#25110;&#26102;&#20195;&#30340;&#25991;&#26412;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#65292;&#36825;&#20123;&#21333;&#35789;&#19982;&#35745;&#31639;&#35821;&#35328;&#23398;&#31639;&#27861;&#30456;&#20851;&#65292;&#20363;&#22914;&#32763;&#35793;&#12289;&#20449;&#24687;&#26816;&#32034;&#12289;&#38382;&#31572;&#31561;&#12290;&#35821;&#20041;&#21464;&#21270;&#21487;&#33021;&#20250;&#24433;&#21709;&#36825;&#20123;&#31639;&#27861;&#30340;&#32467;&#26524;&#36136;&#37327;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#21644;&#24418;&#24335;&#21270;&#34920;&#24449;&#36825;&#20123;&#21464;&#21270;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#30740;&#31350;&#36825;&#31181;&#24433;&#21709;&#26159;&#35745;&#31639;&#35821;&#35328;&#23398;&#30028;&#36817;&#26399;&#24341;&#36215;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#20960;&#31181;&#26041;&#27861;&#25552;&#20986;&#20102;&#26816;&#27979;&#35821;&#20041;&#21464;&#21270;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#31934;&#24230;&#65292;&#20294;&#38656;&#35201;&#26356;&#22810;&#21162;&#21147;&#26469;&#23545;&#20854;&#36827;&#34892;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19088v1 Announce Type: cross  Abstract: Live languages continuously evolve to integrate the cultural change of human societies. This evolution manifests through neologisms (new words) or \textbf{semantic changes} of words (new meaning to existing words). Understanding the meaning of words is vital for interpreting texts coming from different cultures (regionalism or slang), domains (e.g., technical terms), or periods. In computer science, these words are relevant to computational linguistics algorithms such as translation, information retrieval, question answering, etc. Semantic changes can potentially impact the quality of the outcomes of these algorithms. Therefore, it is important to understand and characterize these changes formally. The study of this impact is a recent problem that has attracted the attention of the computational linguistics community. Several approaches propose methods to detect semantic changes with good precision, but more effort is needed to charact
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#30005;&#23376;&#26174;&#24494;&#38236;&#20013;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#23637;&#31034;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#22914;&#20309;&#20419;&#36827;&#26377;&#25928;&#30340;&#24494;&#35843;&#65292;&#21516;&#26102;&#25351;&#20986;&#36739;&#20302;&#22797;&#26434;&#24230;&#30340;&#27169;&#22411;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#22987;&#32456;&#20248;&#20110;&#26356;&#22797;&#26434;&#30340;&#38543;&#26426;&#21021;&#22987;&#21270;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.18286</link><description>&lt;p&gt;
&#30005;&#23376;&#26174;&#24494;&#38236;&#20013;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65306;&#36808;&#21521;&#39640;&#32423;&#22270;&#20687;&#20998;&#26512;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning in Electron Microscopy: Towards a Foundation Model for Advanced Image Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#30005;&#23376;&#26174;&#24494;&#38236;&#20013;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#23637;&#31034;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#22914;&#20309;&#20419;&#36827;&#26377;&#25928;&#30340;&#24494;&#35843;&#65292;&#21516;&#26102;&#25351;&#20986;&#36739;&#20302;&#22797;&#26434;&#24230;&#30340;&#27169;&#22411;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#22987;&#32456;&#20248;&#20110;&#26356;&#22797;&#26434;&#30340;&#38543;&#26426;&#21021;&#22987;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20174;&#26080;&#26631;&#31614;&#30340;&#30005;&#23376;&#26174;&#24494;&#38236;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#36808;&#20986;&#20102;&#26500;&#24314;&#35813;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#30340;&#19968;&#27493;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#22914;&#20309;&#20419;&#36827;&#26377;&#25928;&#30340;&#24494;&#35843;&#65292;&#20197;&#24212;&#29992;&#20110;&#19968;&#31995;&#21015;&#19979;&#28216;&#20219;&#21153;&#65292;&#21253;&#25324;&#35821;&#20041;&#20998;&#21106;&#12289;&#21435;&#22122;&#12289;&#22122;&#22768;&#19982;&#32972;&#26223;&#21435;&#38500;&#20197;&#21450;&#36229;&#20998;&#36776;&#29575;&#12290;&#36890;&#36807;&#23454;&#39564;&#19981;&#21516;&#27169;&#22411;&#22797;&#26434;&#24230;&#21644;&#24863;&#21463;&#37326;&#22823;&#23567;&#30340;&#21464;&#21270;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#20010;&#26174;&#33879;&#30340;&#29616;&#35937;&#65292;&#21363;&#24494;&#35843;&#36807;&#30340;&#36739;&#20302;&#22797;&#26434;&#24230;&#27169;&#22411;&#22987;&#32456;&#32988;&#36807;&#20855;&#26377;&#38543;&#26426;&#26435;&#37325;&#21021;&#22987;&#21270;&#30340;&#26356;&#22797;&#26434;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#22312;&#30005;&#23376;&#26174;&#24494;&#38236;&#32972;&#26223;&#19979;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#22810;&#25165;&#22810;&#33402;&#65292;&#20351;&#24471;&#24555;&#36895;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#20652;&#21270;&#21058;&#65292;&#29305;&#21035;&#22312;&#26377;&#38480;&#30340;&#27880;&#37322;&#25968;&#25454;&#21487;&#29992;&#26102;&#21644; ef
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18286v1 Announce Type: cross  Abstract: In this work, we explore the potential of self-supervised learning from unlabeled electron microscopy datasets, taking a step toward building a foundation model in this field. We show how self-supervised pretraining facilitates efficient fine-tuning for a spectrum of downstream tasks, including semantic segmentation, denoising, noise &amp; background removal, and super-resolution. Experimentation with varying model complexities and receptive field sizes reveals the remarkable phenomenon that fine-tuned models of lower complexity consistently outperform more complex models with random weight initialization. We demonstrate the versatility of self-supervised pretraining across various downstream tasks in the context of electron microscopy, allowing faster convergence and better performance. We conclude that self-supervised pretraining serves as a powerful catalyst, being especially advantageous when limited annotated data are available and ef
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#39304;&#39640;&#25928;&#30340;&#22312;&#32447;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31243;&#24207;</title><link>https://arxiv.org/abs/2402.16359</link><description>&lt;p&gt;
&#21453;&#39304;&#39640;&#25928;&#22312;&#32447;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Feedback Efficient Online Fine-Tuning of Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16359
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#39304;&#39640;&#25928;&#30340;&#22312;&#32447;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#24314;&#27169;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#21253;&#25324;&#22270;&#20687;&#65292;&#34507;&#30333;&#36136;&#21644;&#23567;&#20998;&#23376;&#30340;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#27169;&#25311;&#26368;&#22823;&#21270;&#26576;&#20123;&#23646;&#24615;&#30340;&#20998;&#24067;&#30340;&#37096;&#20998;&#65306;&#20363;&#22914;&#65292;&#25105;&#20204;&#21487;&#33021;&#24076;&#26395;&#29983;&#25104;&#20855;&#26377;&#39640;&#23457;&#32654;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#25110;&#20855;&#26377;&#39640;&#29983;&#29289;&#27963;&#24615;&#30340;&#20998;&#23376;&#12290;&#33258;&#28982;&#22320;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#36825;&#35270;&#20026;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#38382;&#39064;&#65292;&#20854;&#30446;&#26631;&#26159;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#20197;&#26368;&#22823;&#21270;&#19982;&#26576;&#20123;&#23646;&#24615;&#23545;&#24212;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#21363;&#20351;&#21487;&#20197;&#35775;&#38382;&#22320;&#38754;&#30495;&#23454;&#22870;&#21169;&#20989;&#25968;&#30340;&#22312;&#32447;&#26597;&#35810;&#65292;&#26377;&#25928;&#22320;&#21457;&#29616;&#39640;&#22870;&#21169;&#26679;&#26412;&#20063;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65306;&#23427;&#20204;&#22312;&#21021;&#22987;&#20998;&#24067;&#20013;&#30340;&#27010;&#29575;&#21487;&#33021;&#24456;&#20302;&#65292;&#24182;&#19988;&#21487;&#33021;&#23384;&#22312;&#35768;&#22810;&#19981;&#21487;&#34892;&#30340;&#26679;&#26412;&#65292;&#29978;&#33267;&#27809;&#26377;&#23450;&#20041;&#33391;&#22909;&#30340;&#22870;&#21169;&#65288;&#20363;&#22914;&#65292;&#19981;&#33258;&#28982;&#30340;&#22270;&#20687;&#25110;&#29289;&#29702;&#19978;&#19981;&#21487;&#33021;&#30340;&#20998;&#23376;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#31243;&#24207;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#21457;&#29616;&#39640;&#22870;&#21169;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16359v1 Announce Type: cross  Abstract: Diffusion models excel at modeling complex data distributions, including those of images, proteins, and small molecules. However, in many cases, our goal is to model parts of the distribution that maximize certain properties: for example, we may want to generate images with high aesthetic quality, or molecules with high bioactivity. It is natural to frame this as a reinforcement learning (RL) problem, in which the objective is to fine-tune a diffusion model to maximize a reward function that corresponds to some property. Even with access to online queries of the ground-truth reward function, efficiently discovering high-reward samples can be challenging: they might have a low probability in the initial distribution, and there might be many infeasible samples that do not even have a well-defined reward (e.g., unnatural images or physically impossible molecules). In this work, we propose a novel reinforcement learning procedure that effi
&lt;/p&gt;</description></item><item><title>&#20855;&#35937;&#20154;&#24037;&#26234;&#33021;&#34987;&#25552;&#20986;&#20316;&#20026;&#36861;&#27714;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#19979;&#19968;&#20010;&#22522;&#26412;&#27493;&#39588;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#35748;&#30693;&#26550;&#26500;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#19982;Friston&#30340;&#20027;&#21160;&#25512;&#26029;&#21407;&#21017;&#20445;&#25345;&#19968;&#33268;&#65292;&#20026;&#20855;&#35937;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.03824</link><description>&lt;p&gt;
&#19968;&#31181;&#20851;&#20110;&#20855;&#35937;&#20154;&#24037;&#26234;&#33021;&#30340;&#21628;&#21505;
&lt;/p&gt;
&lt;p&gt;
A call for embodied AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03824
&lt;/p&gt;
&lt;p&gt;
&#20855;&#35937;&#20154;&#24037;&#26234;&#33021;&#34987;&#25552;&#20986;&#20316;&#20026;&#36861;&#27714;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#19979;&#19968;&#20010;&#22522;&#26412;&#27493;&#39588;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#35748;&#30693;&#26550;&#26500;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#19982;Friston&#30340;&#20027;&#21160;&#25512;&#26029;&#21407;&#21017;&#20445;&#25345;&#19968;&#33268;&#65292;&#20026;&#20855;&#35937;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20855;&#35937;&#20154;&#24037;&#26234;&#33021;&#20316;&#20026;&#36861;&#27714;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#19979;&#19968;&#20010;&#22522;&#26412;&#27493;&#39588;&#65292;&#24182;&#23558;&#20854;&#19982;&#24403;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#36827;&#23637;&#36827;&#34892;&#23545;&#27604;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#36328;&#36234;&#20102;&#21746;&#23398;&#12289;&#24515;&#29702;&#23398;&#12289;&#31070;&#32463;&#31185;&#23398;&#21644;&#26426;&#22120;&#20154;&#23398;&#31561;&#22810;&#20010;&#39046;&#22495;&#23545;&#20855;&#35937;&#27010;&#24565;&#30340;&#28436;&#21464;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20197;&#20984;&#26174;&#20855;&#35937;&#20154;&#24037;&#26234;&#33021;&#22914;&#20309;&#21306;&#21035;&#20110;&#38745;&#24577;&#23398;&#20064;&#30340;&#32463;&#20856;&#33539;&#24335;&#12290;&#36890;&#36807;&#25193;&#22823;&#20855;&#35937;&#20154;&#24037;&#26234;&#33021;&#30340;&#33539;&#22260;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#35748;&#30693;&#26550;&#26500;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24378;&#35843;&#30693;&#35273;&#12289;&#34892;&#21160;&#12289;&#35760;&#24518;&#21644;&#23398;&#20064;&#20316;&#20026;&#20855;&#35937;&#20195;&#29702;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#12290;&#36825;&#20010;&#26694;&#26550;&#19982;Friston&#30340;&#20027;&#21160;&#25512;&#26029;&#21407;&#21017;&#20445;&#25345;&#19968;&#33268;&#65292;&#20026;&#20855;&#35937;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#35832;&#22914;&#21046;&#23450;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#23398;&#20064;&#29702;&#35770;&#21644;&#21019;&#26032;&#20808;&#36827;&#30828;&#20214;&#31561;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#35752;&#35770;&#20026;&#26410;&#26469;&#20855;&#35937;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#22880;&#23450;&#20102;&#22522;&#30784;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Embodied AI as the next fundamental step in the pursuit of Artificial General Intelligence, juxtaposing it against current AI advancements, particularly Large Language Models. We traverse the evolution of the embodiment concept across diverse fields - philosophy, psychology, neuroscience, and robotics - to highlight how EAI distinguishes itself from the classical paradigm of static learning. By broadening the scope of Embodied AI, we introduce a theoretical framework based on cognitive architectures, emphasizing perception, action, memory, and learning as essential components of an embodied agent. This framework is aligned with Friston's active inference principle, offering a comprehensive approach to EAI development. Despite the progress made in the field of AI, substantial challenges, such as the formulation of a novel AI learning theory and the innovation of advanced hardware, persist. Our discussion lays down a foundational guideline for future Embodied AI research. High
&lt;/p&gt;</description></item><item><title>V-IRL&#26159;&#19968;&#20010;&#24179;&#21488;&#65292;&#21487;&#20197;&#35753;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#19982;&#29616;&#23454;&#19990;&#30028;&#36827;&#34892;&#20114;&#21160;&#65292;&#26088;&#22312;&#23558;&#25968;&#23383;&#21644;&#29289;&#29702;&#19990;&#30028;&#20043;&#38388;&#30340;&#24046;&#36317;&#32553;&#23567;&#65292;&#24182;&#24320;&#21457;&#20986;&#20855;&#26377;&#20016;&#23500;&#24863;&#30693;&#12289;&#20915;&#31574;&#21644;&#19982;&#30495;&#23454;&#25968;&#25454;&#20114;&#21160;&#33021;&#21147;&#30340;&#20195;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.03310</link><description>&lt;p&gt;
V-IRL: &#23558;&#34394;&#25311;&#26234;&#33021;&#19982;&#29616;&#23454;&#29983;&#27963;&#32852;&#31995;&#36215;&#26469;
&lt;/p&gt;
&lt;p&gt;
V-IRL: Grounding Virtual Intelligence in Real Life
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03310
&lt;/p&gt;
&lt;p&gt;
V-IRL&#26159;&#19968;&#20010;&#24179;&#21488;&#65292;&#21487;&#20197;&#35753;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#19982;&#29616;&#23454;&#19990;&#30028;&#36827;&#34892;&#20114;&#21160;&#65292;&#26088;&#22312;&#23558;&#25968;&#23383;&#21644;&#29289;&#29702;&#19990;&#30028;&#20043;&#38388;&#30340;&#24046;&#36317;&#32553;&#23567;&#65292;&#24182;&#24320;&#21457;&#20986;&#20855;&#26377;&#20016;&#23500;&#24863;&#30693;&#12289;&#20915;&#31574;&#21644;&#19982;&#30495;&#23454;&#25968;&#25454;&#20114;&#21160;&#33021;&#21147;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#29983;&#27963;&#22312;&#22320;&#29699;&#19978;&#65292;&#32780;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#25152;&#21019;&#36896;&#30340;&#25968;&#23383;&#39046;&#22495;&#20043;&#38388;&#23384;&#22312;&#30528;&#24863;&#23448;&#24046;&#36317;&#12290;&#20026;&#20102;&#24320;&#21457;&#20986;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#33021;&#20687;&#20154;&#31867;&#19968;&#26679;&#28789;&#27963;&#24863;&#30693;&#12289;&#24605;&#32771;&#21644;&#34892;&#21160;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65292;&#24517;&#39035;&#24357;&#21512;&#25968;&#23383;&#21644;&#29289;&#29702;&#19990;&#30028;&#20043;&#38388;&#30340;&#36924;&#30495;&#24046;&#36317;&#12290;&#25105;&#20204;&#22914;&#20309;&#22312;&#19968;&#20010;&#20687;&#25105;&#20204;&#25152;&#23621;&#20303;&#30340;&#19990;&#30028;&#20013;&#19968;&#26679;&#20016;&#23500;&#22810;&#26679;&#30340;&#29615;&#22659;&#20013;&#20307;&#29616;&#20195;&#29702;&#65292;&#32780;&#19981;&#21463;&#30495;&#23454;&#30828;&#20214;&#21644;&#25511;&#21046;&#25152;&#26045;&#21152;&#30340;&#32422;&#26463;&#65311;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;V-IRL: &#19968;&#31181;&#24179;&#21488;&#65292;&#21487;&#20197;&#20351;&#20195;&#29702;&#22312;&#34394;&#25311;&#32780;&#36924;&#30495;&#30340;&#29615;&#22659;&#20013;&#19982;&#29616;&#23454;&#19990;&#30028;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#20114;&#21160;&#12290;&#25105;&#20204;&#30340;&#24179;&#21488;&#26082;&#26159;&#19968;&#20010;&#24320;&#21457;&#20195;&#29702;&#23436;&#25104;&#21508;&#31181;&#23454;&#38469;&#20219;&#21153;&#30340;&#28216;&#20048;&#22330;&#65292;&#21448;&#26159;&#19968;&#20010;&#24191;&#38420;&#30340;&#27979;&#35797;&#22522;&#22320;&#65292;&#29992;&#20110;&#34913;&#37327;&#22312;&#24863;&#30693;&#12289;&#20915;&#31574;&#21644;&#19982;&#20840;&#29699;&#30495;&#23454;&#25968;&#25454;&#30340;&#20114;&#21160;&#33021;&#21147;&#31561;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a sensory gulf between the Earth that humans inhabit and the digital realms in which modern AI agents are created. To develop AI agents that can sense, think, and act as flexibly as humans in real-world settings, it is imperative to bridge the realism gap between the digital and physical worlds. How can we embody agents in an environment as rich and diverse as the one we inhabit, without the constraints imposed by real hardware and control? Towards this end, we introduce V-IRL: a platform that enables agents to scalably interact with the real world in a virtual yet realistic environment. Our platform serves as a playground for developing agents that can accomplish various practical tasks and as a vast testbed for measuring progress in capabilities spanning perception, decision-making, and interaction with real-world data across the entire globe.
&lt;/p&gt;</description></item><item><title>SynthCLIP&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#23436;&#20840;&#21512;&#25104;&#30340;CLIP&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#22823;&#35268;&#27169;&#30340;&#21512;&#25104;&#22270;&#29255;&#21644;&#26631;&#39064;&#25968;&#25454;&#38598;&#65292;&#22312;&#24615;&#33021;&#19978;&#21487;&#20197;&#19982;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;CLIP&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;</title><link>https://arxiv.org/abs/2402.01832</link><description>&lt;p&gt;
SynthCLIP: &#25105;&#20204;&#20934;&#22791;&#22909;&#24320;&#22987;&#23436;&#20840;&#21512;&#25104;&#30340;CLIP&#35757;&#32451;&#20102;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
SynthCLIP: Are We Ready for a Fully Synthetic CLIP Training?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01832
&lt;/p&gt;
&lt;p&gt;
SynthCLIP&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#23436;&#20840;&#21512;&#25104;&#30340;CLIP&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#22823;&#35268;&#27169;&#30340;&#21512;&#25104;&#22270;&#29255;&#21644;&#26631;&#39064;&#25968;&#25454;&#38598;&#65292;&#22312;&#24615;&#33021;&#19978;&#21487;&#20197;&#19982;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;CLIP&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;SynthCLIP&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#35757;&#32451;&#23436;&#20840;&#21512;&#25104;&#30340;CLIP&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#19982;&#20043;&#21069;&#20381;&#36182;&#30495;&#23454;&#25968;&#25454;&#30340;&#26041;&#27861;&#26377;&#30528;&#26174;&#33879;&#21306;&#21035;&#12290;&#20511;&#21161;&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#32593;&#32476;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;&#20219;&#24847;&#35268;&#27169;&#30340;&#22270;&#20687;&#21644;&#30456;&#24212;&#30340;&#26631;&#39064;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#26080;&#38656;&#20154;&#20026;&#24178;&#39044;&#12290;&#36890;&#36807;&#22823;&#35268;&#27169;&#30340;&#35757;&#32451;&#65292;SynthCLIP&#23454;&#29616;&#20102;&#19982;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;CLIP&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;SynthCI-30M&#65292;&#19968;&#20010;&#32431;&#31929;&#21512;&#25104;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;3000&#19975;&#24352;&#24102;&#26631;&#39064;&#30340;&#22270;&#29255;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#12289;&#35757;&#32451;&#27169;&#22411;&#21644;&#29983;&#25104;&#30340;&#25968;&#25454;&#24050;&#32463;&#22312;https://github.com/hammoudhasan/SynthCLIP&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SynthCLIP, a novel framework for training CLIP models with entirely synthetic text-image pairs, significantly departing from previous methods relying on real data. Leveraging recent text-to-image (TTI) generative networks and large language models (LLM), we are able to generate synthetic datasets of images and corresponding captions at any scale, with no human intervention. With training at scale, SynthCLIP achieves performance comparable to CLIP models trained on real datasets. We also introduce SynthCI-30M, a purely synthetic dataset comprising 30 million captioned images. Our code, trained models, and generated data are released at https://github.com/hammoudhasan/SynthCLIP
&lt;/p&gt;</description></item><item><title>&#22810;Agent&#36777;&#35770;&#65288;MAD&#65289;&#20316;&#20026;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30495;&#23454;&#24615;&#30340;&#31574;&#30053;&#65292;&#23545;&#20110;&#35299;&#20915;&#30830;&#20445;&#29983;&#25104;&#20195;&#29702;&#25552;&#20379;&#20934;&#30830;&#21487;&#38752;&#31572;&#26696;&#30340;&#25361;&#25112;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#24403;&#21069;&#24418;&#24335;&#19979;&#30340;&#22810;Agent&#36777;&#35770;&#31995;&#32479;&#22312;&#21487;&#38752;&#24615;&#19978;&#19981;&#19968;&#23450;&#20248;&#20110;&#20854;&#20182;&#25552;&#31034;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2311.17371</link><description>&lt;p&gt;
&#25105;&#20204;&#24212;&#35813;&#30127;&#29378;&#21527;&#65311;&#22810;Agent&#36777;&#35770;&#31574;&#30053;&#23545;LLMs&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Should we be going MAD? A Look at Multi-Agent Debate Strategies for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17371
&lt;/p&gt;
&lt;p&gt;
&#22810;Agent&#36777;&#35770;&#65288;MAD&#65289;&#20316;&#20026;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30495;&#23454;&#24615;&#30340;&#31574;&#30053;&#65292;&#23545;&#20110;&#35299;&#20915;&#30830;&#20445;&#29983;&#25104;&#20195;&#29702;&#25552;&#20379;&#20934;&#30830;&#21487;&#38752;&#31572;&#26696;&#30340;&#25361;&#25112;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#24403;&#21069;&#24418;&#24335;&#19979;&#30340;&#22810;Agent&#36777;&#35770;&#31995;&#32479;&#22312;&#21487;&#38752;&#24615;&#19978;&#19981;&#19968;&#23450;&#20248;&#20110;&#20854;&#20182;&#25552;&#31034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#31361;&#26174;&#20102;&#23427;&#20204;&#22312;&#22238;&#31572;&#21508;&#31181;&#39046;&#22495;&#38382;&#39064;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30830;&#20445;&#29983;&#25104;&#20195;&#29702;&#25552;&#20379;&#20934;&#30830;&#21487;&#38752;&#30340;&#31572;&#26696;&#20173;&#28982;&#26159;&#19968;&#20010;&#25345;&#32493;&#25361;&#25112;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#22810;Agent&#36777;&#35770;&#65288;MAD&#65289;&#24050;&#25104;&#20026;&#22686;&#24378;LLMs&#30495;&#23454;&#24615;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#23545;&#19968;&#31995;&#21015;&#36777;&#35770;&#21644;&#25552;&#31034;&#31574;&#30053;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#25506;&#35752;&#25104;&#26412;&#12289;&#26102;&#38388;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#30446;&#21069;&#24418;&#24335;&#19979;&#30340;&#22810;Agent&#36777;&#35770;&#31995;&#32479;&#22312;&#21487;&#38752;&#24615;&#19978;&#19981;&#19968;&#23450;&#20248;&#20110;&#20854;&#20182;&#24314;&#35758;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#22914;&#33258;&#19968;&#33268;&#24615;&#21644;&#20351;&#29992;&#22810;&#20010;&#25512;&#29702;&#36335;&#24452;&#36827;&#34892;&#38598;&#25104;&#12290;&#20294;&#26159;&#65292;&#22312;&#25191;&#34892;&#36229;&#21442;&#25968;&#35843;&#25972;&#26102;&#65292;&#19968;&#20123;MAD&#31995;&#32479;&#65292;&#22914;Multi-Persona&#65292;&#34920;&#29616;&#26356;&#22909;&#12290;&#36825;&#34920;&#26126;MAD&#21327;&#35758;&#21487;&#33021;&#24182;&#19981;&#20250;&#27604;&#20854;&#20182;&#26041;&#27861;&#22825;&#28982;&#26356;&#24046;&#65292;&#32780;&#26159;&#26356;&#23481;&#26131;&#21463;&#21040;&#19981;&#21516;&#36229;&#21442;&#25968;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17371v2 Announce Type: replace-cross  Abstract: Recent advancements in large language models (LLMs) underscore their potential for responding to inquiries in various domains. However, ensuring that generative agents provide accurate and reliable answers remains an ongoing challenge. In this context, multi-agent debate (MAD) has emerged as a promising strategy for enhancing the truthfulness of LLMs. We benchmark a range of debating and prompting strategies to explore the trade-offs between cost, time, and accuracy. Importantly, we find that multi-agent debating systems, in their current form, do not reliably outperform other proposed prompting strategies, such as self-consistency and ensembling using multiple reasoning paths. However, when performing hyperparameter tuning, several MAD systems, such as Multi-Persona, perform better. This suggests that MAD protocols might not be inherently worse than other approaches, but that they are more sensitive to different hyperparameter
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#37327;&#21270;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20869;&#23384;&#39640;&#25928;&#20010;&#24615;&#21270;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#31574;&#30053;&#26469;&#35299;&#20915;&#22522;&#32447;&#27169;&#22411;&#20013;&#20027;&#39064;&#21644;&#25552;&#31034;&#36136;&#37327;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.04339</link><description>&lt;p&gt;
&#20351;&#29992;&#37327;&#21270;&#25193;&#25955;&#27169;&#22411;&#30340;&#20869;&#23384;&#39640;&#25928;&#20010;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
Memory-Efficient Personalization using Quantized Diffusion Model. (arXiv:2401.04339v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#37327;&#21270;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20869;&#23384;&#39640;&#25928;&#20010;&#24615;&#21270;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#31574;&#30053;&#26469;&#35299;&#20915;&#22522;&#32447;&#27169;&#22411;&#20013;&#20027;&#39064;&#21644;&#25552;&#31034;&#36136;&#37327;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20159;&#32423;&#21442;&#25968;&#25193;&#25955;&#27169;&#22411;&#65288;&#22914;Stable Diffusion XL&#12289;Imagen&#21644;Dall-E3&#65289;&#30340;&#23835;&#36215;&#26174;&#33879;&#25512;&#21160;&#20102;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36164;&#28304;&#38656;&#27714;&#39640;&#21644;&#25512;&#29702;&#36895;&#24230;&#24930;&#65292;&#23427;&#20204;&#30340;&#22823;&#35268;&#27169;&#24615;&#36136;&#22312;&#24494;&#35843;&#21644;&#37096;&#32626;&#20013;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#23545;&#37327;&#21270;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#30456;&#23545;&#26410;&#24320;&#21457;&#20294;&#26377;&#21069;&#26223;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#36890;&#36807;&#23450;&#21046;&#19977;&#20010;&#27169;&#22411;&#65288;&#29992;&#20110;&#24494;&#35843;&#37327;&#21270;&#21442;&#25968;&#30340;PEQA&#65292;&#29992;&#20110;&#21518;&#26399;&#37327;&#21270;&#30340;Q-Diffusion&#21644;&#20010;&#24615;&#21270;&#30340;DreamBooth&#65289;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#22522;&#32447;&#27169;&#22411;&#20013;&#20027;&#39064;&#21644;&#25552;&#31034;&#36136;&#37327;&#20043;&#38388;&#30340;&#26126;&#26174;&#26435;&#34913;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#31574;&#30053;&#65292;&#28789;&#24863;&#26469;&#33258;&#20110;&#25193;&#25955;&#27169;&#22411;&#20013;&#19981;&#21516;&#26102;&#38388;&#27493;&#38271;&#30340;&#19981;&#21516;&#35282;&#33394;&#65306;S1&#22312;&#36873;&#25321;&#30340;&#26102;&#38388;&#38388;&#38548;&#20869;&#20165;&#20248;&#21270;&#19968;&#32452;&#24494;&#35843;&#21442;&#25968;&#65292;S2&#21019;&#24314;&#22810;&#20010;&#24494;&#35843;&#21442;&#25968;&#32452;&#65292;&#27599;&#20010;&#32452;&#19987;&#38376;&#29992;&#20110;&#19981;&#21516;&#30340;&#26102;&#38388;&#27493;&#38271;&#38388;&#38548;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of billion-parameter diffusion models like Stable Diffusion XL, Imagen, and Dall-E3 markedly advances the field of generative AI. However, their large-scale nature poses challenges in fine-tuning and deployment due to high resource demands and slow inference speed. This paper ventures into the relatively unexplored yet promising realm of fine-tuning quantized diffusion models. We establish a strong baseline by customizing three models: PEQA for fine-tuning quantization parameters, Q-Diffusion for post-training quantization, and DreamBooth for personalization. Our analysis reveals a notable trade-off between subject and prompt fidelity within the baseline model. To address these issues, we introduce two strategies, inspired by the distinct roles of different timesteps in diffusion models: S1 optimizing a single set of fine-tuning parameters exclusively at selected intervals, and S2 creating multiple fine-tuning parameter sets, each specialized for different timestep intervals. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22312;&#32447;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#24615;&#65288;OTTA&#65289;&#30340;&#35843;&#30740;&#32467;&#26524;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#35299;&#20915;&#27169;&#31946;&#35774;&#32622;&#12289;&#36807;&#26102;&#39592;&#24178;&#32467;&#26500;&#21644;&#19981;&#19968;&#33268;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#31574;&#30053;&#21644;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.20199</link><description>&lt;p&gt;
&#36861;&#23547;&#22833;&#33853;&#30340;&#22312;&#32447;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#24615;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
In Search of Lost Online Test-time Adaptation: A Survey. (arXiv:2310.20199v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22312;&#32447;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#24615;&#65288;OTTA&#65289;&#30340;&#35843;&#30740;&#32467;&#26524;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#35299;&#20915;&#27169;&#31946;&#35774;&#32622;&#12289;&#36807;&#26102;&#39592;&#24178;&#32467;&#26500;&#21644;&#19981;&#19968;&#33268;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#31574;&#30053;&#21644;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35814;&#32454;&#35843;&#30740;&#20102;&#22312;&#32447;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#24615;&#65288;OTTA&#65289;&#30340;&#32508;&#21512;&#27010;&#20917;&#65292;&#35813;&#33539;&#24335;&#19987;&#27880;&#20110;&#22312;&#25209;&#37327;&#21040;&#36798;&#26102;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35843;&#25972;&#21040;&#26032;&#25968;&#25454;&#20998;&#24067;&#19978;&#12290;&#23613;&#31649;&#26368;&#36817;OTTA&#26041;&#27861;&#30340;&#22686;&#21152;&#65292;&#20294;&#35813;&#39046;&#22495;&#23384;&#22312;&#27169;&#31946;&#30340;&#35774;&#32622;&#12289;&#36807;&#26102;&#30340;&#39592;&#24178;&#32467;&#26500;&#21644;&#19981;&#19968;&#33268;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#31561;&#38382;&#39064;&#65292;&#36825;&#20351;&#24471;&#30495;&#27491;&#30340;&#25361;&#25112;&#21464;&#24471;&#38590;&#20197;&#22797;&#29616;&#12290;&#20026;&#20102;&#28165;&#26224;&#21644;&#20005;&#26684;&#30340;&#27604;&#36739;&#65292;&#25105;&#20204;&#23558;OTTA&#25216;&#26415;&#20998;&#20026;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#65292;&#24182;&#20351;&#29992;&#21151;&#33021;&#24378;&#22823;&#30340;Vision Transformer&#65288;ViT&#65289;&#39592;&#24178;&#26550;&#26500;&#23545;&#23427;&#20204;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#21457;&#29616;&#30495;&#27491;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#19981;&#20165;&#28085;&#30422;&#20256;&#32479;&#30340;&#21463;&#25439;&#25968;&#25454;&#38598;&#65292;&#22914;CIFAR-10/100-C&#21644;ImageNet-C&#65292;&#36824;&#21253;&#25324;&#20307;&#29616;&#22312;CIFAR-10.1&#21644;CIFAR-10-Warehouse&#20013;&#30340;&#29616;&#23454;&#19990;&#30028;&#36716;&#21464;&#65292;&#28085;&#30422;&#20102;&#25628;&#32034;&#24341;&#25806;&#30340;&#21464;&#21270;&#21644;&#25193;&#25955;&#27169;&#22411;&#21512;&#25104;&#25968;&#25454;&#30340;&#21464;&#21270;&#12290;&#20026;&#20102;&#34913;&#37327;&#22312;&#32447;&#22330;&#26223;&#20013;&#30340;&#25928;&#29575;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a comprehensive survey on online test-time adaptation (OTTA), a paradigm focused on adapting machine learning models to novel data distributions upon batch arrival. Despite the proliferation of OTTA methods recently, the field is mired in issues like ambiguous settings, antiquated backbones, and inconsistent hyperparameter tuning, obfuscating the real challenges and making reproducibility elusive. For clarity and a rigorous comparison, we classify OTTA techniques into three primary categories and subject them to benchmarks using the potent Vision Transformer (ViT) backbone to discover genuinely effective strategies. Our benchmarks span not only conventional corrupted datasets such as CIFAR-10/100-C and ImageNet-C but also real-world shifts embodied in CIFAR-10.1 and CIFAR-10-Warehouse, encapsulating variations across search engines and synthesized data by diffusion models. To gauge efficiency in online scenarios, we introduce novel evaluation metrics, inclusiv
&lt;/p&gt;</description></item><item><title>&#24322;&#26500;&#36801;&#31227;&#23398;&#20064;&#36866;&#29992;&#20110;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#12289;&#25968;&#25454;&#20998;&#24067;&#21644;&#26631;&#31614;&#31354;&#38388;&#30340;&#24773;&#20917;&#65292;&#36890;&#36807;&#22788;&#29702;&#36825;&#20123;&#24046;&#24322;&#26469;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.08459</link><description>&lt;p&gt;
&#24322;&#26500;&#36801;&#31227;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Heterogeneous Transfer Learning. (arXiv:2310.08459v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08459
&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#36801;&#31227;&#23398;&#20064;&#36866;&#29992;&#20110;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#12289;&#25968;&#25454;&#20998;&#24067;&#21644;&#26631;&#31614;&#31354;&#38388;&#30340;&#24773;&#20917;&#65292;&#36890;&#36807;&#22788;&#29702;&#36825;&#20123;&#24046;&#24322;&#26469;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36801;&#31227;&#23398;&#20064;&#30340;&#24212;&#29992;&#22312;&#24456;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#23427;&#21033;&#29992;&#28304;&#39046;&#22495;&#30340;&#30693;&#35782;&#26469;&#22686;&#24378;&#30446;&#26631;&#39046;&#22495;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20854;&#25104;&#21151;&#30340;&#20851;&#38190;&#22312;&#20110;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#20043;&#38388;&#30340;&#20849;&#20139;&#30693;&#35782;&#65292;&#36825;&#26159;&#22823;&#22810;&#25968;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#30340;&#21069;&#25552;&#26465;&#20214;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#20004;&#20010;&#39046;&#22495;&#20855;&#26377;&#30456;&#21516;&#30340;&#29305;&#24449;&#31354;&#38388;&#21644;&#26631;&#31614;&#31354;&#38388;&#65292;&#21363;&#21516;&#36136;&#36801;&#31227;&#23398;&#20064;&#65292;&#20294;&#36825;&#24182;&#19981;&#24635;&#26159;&#29616;&#23454;&#21512;&#29702;&#30340;&#20551;&#35774;&#12290;&#36890;&#24120;&#65292;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#22312;&#29305;&#24449;&#31354;&#38388;&#12289;&#25968;&#25454;&#20998;&#24067;&#21644;&#26631;&#31614;&#31354;&#38388;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#36825;&#20351;&#24471;&#33719;&#21462;&#20855;&#26377;&#19982;&#30446;&#26631;&#39046;&#22495;&#30456;&#21516;&#29305;&#24449;&#21644;&#26631;&#31614;&#31354;&#38388;&#30340;&#28304;&#39046;&#22495;&#25968;&#25454;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#25110;&#26114;&#36149;&#12290;&#23545;&#36825;&#20123;&#24046;&#24322;&#36827;&#34892;&#38543;&#24847;&#30340;&#28040;&#38500;&#24182;&#19981;&#24635;&#26159;&#21487;&#34892;&#25110;&#26368;&#20248;&#30340;&#12290;&#22240;&#27492;&#65292;&#24322;&#26500;&#36801;&#31227;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#24212;&#23545;&#36825;&#31181;&#24046;&#24322;&#30340;&#26041;&#27861;&#24050;&#32463;&#23853;&#38706;&#22836;&#35282;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of transfer learning, an approach utilizing knowledge from a source domain to enhance model performance in a target domain, has seen a tremendous rise in recent years, underpinning many real-world scenarios. The key to its success lies in the shared common knowledge between the domains, a prerequisite in most transfer learning methodologies. These methods typically presuppose identical feature spaces and label spaces in both domains, known as homogeneous transfer learning, which, however, is not always a practical assumption. Oftentimes, the source and target domains vary in feature spaces, data distributions, and label spaces, making it challenging or costly to secure source domain data with identical feature and label spaces as the target domain. Arbitrary elimination of these differences is not always feasible or optimal. Thus, heterogeneous transfer learning, acknowledging and dealing with such disparities, has emerged as a promising approach for a variety of tasks.
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#35821;&#35328;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#12290;&#36890;&#36807;&#21033;&#29992;&#38598;&#25104;&#26041;&#27861;&#65292;&#29983;&#25104;&#22810;&#20010;&#19987;&#38376;&#30340;&#25915;&#20987;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#32463;&#20856;&#21644;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#19978;&#27604;&#21333;&#20010;&#25915;&#20987;&#27169;&#22411;&#25110;&#27599;&#20010;&#31867;&#21035;&#26631;&#31614;&#30340;&#25915;&#20987;&#27169;&#22411;&#26356;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2310.07219</link><description>&lt;p&gt;
&#25913;&#36827;&#30340;&#23545;&#35821;&#35328;&#20998;&#31867;&#27169;&#22411;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Improved Membership Inference Attacks Against Language Classification Models. (arXiv:2310.07219v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07219
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#35821;&#35328;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#12290;&#36890;&#36807;&#21033;&#29992;&#38598;&#25104;&#26041;&#27861;&#65292;&#29983;&#25104;&#22810;&#20010;&#19987;&#38376;&#30340;&#25915;&#20987;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#32463;&#20856;&#21644;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#19978;&#27604;&#21333;&#20010;&#25915;&#20987;&#27169;&#22411;&#25110;&#27599;&#20010;&#31867;&#21035;&#26631;&#31614;&#30340;&#25915;&#20987;&#27169;&#22411;&#26356;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#20855;&#26377;&#38646;&#21806;&#12289;&#21046;&#36896;&#12289;&#20581;&#24247;&#31561;&#35768;&#22810;&#39046;&#22495;&#30340;&#29992;&#20363;&#12290;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#37319;&#29992;&#30340;&#22686;&#21152;&#65292;&#24050;&#32463;&#21457;&#29616;&#20102;&#30456;&#20851;&#30340;&#39118;&#38505;&#65292;&#21253;&#25324;&#23545;&#20351;&#29992;&#20854;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#30340;&#20154;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38544;&#31169;&#39118;&#38505;&#23545;&#20110;&#26159;&#21542;&#20351;&#29992;&#12289;&#37096;&#32626;&#25110;&#20849;&#20139;&#27169;&#22411;&#20570;&#20986;&#30693;&#24773;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#38544;&#31169;&#39118;&#38505;&#35780;&#20272;&#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#26159;&#23545;&#27169;&#22411;&#36827;&#34892;&#19968;&#20010;&#25110;&#22810;&#20010;&#24050;&#30693;&#25915;&#20987;&#65292;&#24182;&#27979;&#37327;&#23427;&#20204;&#30340;&#25104;&#21151;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#38598;&#25104;&#26041;&#27861;&#65292;&#20026;&#19981;&#21516;&#25968;&#25454;&#23376;&#38598;&#29983;&#25104;&#35768;&#22810;&#19987;&#38376;&#30340;&#25915;&#20987;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#32463;&#20856;&#21644;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#19978;&#27604;&#21333;&#20010;&#25915;&#20987;&#27169;&#22411;&#25110;&#27599;&#20010;&#31867;&#21035;&#26631;&#31614;&#30340;&#25915;&#20987;&#27169;&#22411;&#37117;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence systems are prevalent in everyday life, with use cases in retail, manufacturing, health, and many other fields. With the rise in AI adoption, associated risks have been identified, including privacy risks to the people whose data was used to train models. Assessing the privacy risks of machine learning models is crucial to enabling knowledgeable decisions on whether to use, deploy, or share a model. A common approach to privacy risk assessment is to run one or more known attacks against the model and measure their success rate. We present a novel framework for running membership inference attacks against classification models. Our framework takes advantage of the ensemble method, generating many specialized attack models for different subsets of the data. We show that this approach achieves higher accuracy than either a single attack model or an attack model per class label, both on classical and language classification tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;DIAM&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#26816;&#27979;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;&#32593;&#32476;&#19978;&#30340;&#38750;&#27861;&#36134;&#25143;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#33258;&#21160;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#24182;&#20445;&#30041;&#24179;&#34892;&#36793;&#30340;&#20869;&#22312;&#20132;&#26131;&#27169;&#24335;&#65292;&#22312;&#22823;&#22411;&#20132;&#26131;&#32593;&#32476;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.02460</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;&#32593;&#32476;&#19978;&#30340;&#38750;&#27861;&#36134;&#25143;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Effective Multi-Graph Neural Networks for Illicit Account Detection on Cryptocurrency Transaction Networks. (arXiv:2309.02460v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;DIAM&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#26816;&#27979;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;&#32593;&#32476;&#19978;&#30340;&#38750;&#27861;&#36134;&#25143;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#33258;&#21160;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#24182;&#20445;&#30041;&#24179;&#34892;&#36793;&#30340;&#20869;&#22312;&#20132;&#26131;&#27169;&#24335;&#65292;&#22312;&#22823;&#22411;&#20132;&#26131;&#32593;&#32476;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#37329;&#34701;&#24066;&#22330;&#20013;&#26085;&#30410;&#37325;&#35201;&#30340;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;&#32593;&#32476;&#19978;&#30340;&#38750;&#27861;&#36134;&#25143;&#26816;&#27979;&#12290;&#22312;&#21152;&#23494;&#36135;&#24065;&#19978;&#30340;&#38750;&#27861;&#27963;&#21160;&#28608;&#22686;&#23548;&#33268;&#20102;&#26222;&#36890;&#29992;&#25143;&#25968;&#21313;&#20159;&#30340;&#25439;&#22833;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#35201;&#20040;&#20381;&#36182;&#20110;&#32321;&#29712;&#30340;&#29305;&#24449;&#24037;&#31243;&#26469;&#33719;&#24471;&#25163;&#24037;&#29305;&#24449;&#65292;&#35201;&#20040;&#19981;&#33021;&#20805;&#20998;&#21033;&#29992;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;&#25968;&#25454;&#20013;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#20174;&#32780;&#23548;&#33268;&#20122;&#20248;&#21270;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#38750;&#27861;&#36134;&#25143;&#26816;&#27979;&#38382;&#39064;&#23450;&#20041;&#20026;&#24102;&#26377;&#36793;&#23646;&#24615;&#30340;&#26377;&#21521;&#22810;&#22270;&#19978;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;DIAM&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#20132;&#26131;&#32593;&#32476;&#19978;&#26377;&#25928;&#22320;&#26816;&#27979;&#38750;&#27861;&#36134;&#25143;&#12290;&#39318;&#20808;&#65292;DIAM&#21253;&#21547;&#19968;&#20010;Edge2Seq&#27169;&#22359;&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#36793;&#23646;&#24615;&#21644;&#26377;&#21521;&#36793;&#24207;&#21015;&#20381;&#36182;&#20851;&#31995;&#65292;&#33258;&#21160;&#23398;&#20064;&#26377;&#25928;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#20445;&#30041;&#24179;&#34892;&#36793;&#30340;&#20869;&#22312;&#20132;&#26131;&#27169;&#24335;&#12290;&#28982;&#21518;&#21033;&#29992;t
&lt;/p&gt;
&lt;p&gt;
We study illicit account detection on transaction networks of cryptocurrencies that are increasi_testngly important in online financial markets. The surge of illicit activities on cryptocurrencies has resulted in billions of losses from normal users. Existing solutions either rely on tedious feature engineering to get handcrafted features, or are inadequate to fully utilize the rich semantics of cryptocurrency transaction data, and consequently, yield sub-optimal performance. In this paper, we formulate the illicit account detection problem as a classification task over directed multigraphs with edge attributes, and present DIAM, a novel multi-graph neural network model to effectively detect illicit accounts on large transaction networks. First, DIAM includes an Edge2Seq module that automatically learns effective node representations preserving intrinsic transaction patterns of parallel edges, by considering both edge attributes and directed edge sequence dependencies. Then utilizing t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#27169;&#22411;&#26469;&#28304;&#35777;&#26126;&#30340;&#26032;&#27010;&#24565;&#27169;&#22411;DNA&#65292;&#36890;&#36807;&#32534;&#30721;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#36755;&#20837;&#36755;&#20986;&#20449;&#24687;&#20316;&#20026;&#32039;&#20945;&#20840;&#38754;&#30340;&#34920;&#31034;&#65292;&#26469;&#30830;&#23450;&#28304;&#27169;&#22411;&#26159;&#21542;&#20316;&#20026;&#30446;&#26631;&#27169;&#22411;&#30340;&#26469;&#28304;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2308.02121</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#22411;DNA&#30340;&#27169;&#22411;&#26469;&#28304;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Model Provenance via Model DNA. (arXiv:2308.02121v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#27169;&#22411;&#26469;&#28304;&#35777;&#26126;&#30340;&#26032;&#27010;&#24565;&#27169;&#22411;DNA&#65292;&#36890;&#36807;&#32534;&#30721;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#36755;&#20837;&#36755;&#20986;&#20449;&#24687;&#20316;&#20026;&#32039;&#20945;&#20840;&#38754;&#30340;&#34920;&#31034;&#65292;&#26469;&#30830;&#23450;&#28304;&#27169;&#22411;&#26159;&#21542;&#20316;&#20026;&#30446;&#26631;&#27169;&#22411;&#30340;&#26469;&#28304;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#29983;&#21629;&#21608;&#26399;&#26159;&#19968;&#20010;&#26377;&#36259;&#30340;&#30740;&#31350;&#39046;&#22495;&#65288;&#20363;&#22914;&#65292;&#20102;&#35299;&#27169;&#22411;&#30340;&#26469;&#28304;&#65292;&#35757;&#32451;&#26041;&#24335;&#20197;&#21450;&#20351;&#29992;&#26041;&#24335;&#65289;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;&#36825;&#19968;&#39046;&#22495;&#20869;&#30340;&#19968;&#20010;&#26032;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#26469;&#28304;&#35777;&#26126;&#65288;MP&#65289;&#65292;&#35813;&#38382;&#39064;&#28041;&#21450;&#30446;&#26631;&#27169;&#22411;&#19982;&#20854;&#39044;&#35757;&#32451;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#26088;&#22312;&#30830;&#23450;&#19968;&#20010;&#28304;&#27169;&#22411;&#26159;&#21542;&#20316;&#20026;&#30446;&#26631;&#27169;&#22411;&#30340;&#26469;&#28304;&#35777;&#26126;&#12290;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#23545;&#20110;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#21644;&#30693;&#35782;&#20135;&#26435;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#20294;&#22312;&#25991;&#29486;&#20013;&#24182;&#27809;&#26377;&#24471;&#21040;&#24456;&#22810;&#20851;&#27880;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#27010;&#24565;&#65292;&#21363;&#27169;&#22411;DNA&#65292;&#23427;&#20195;&#34920;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#25105;&#20204;&#21033;&#29992;&#25968;&#25454;&#39537;&#21160;&#21644;&#27169;&#22411;&#39537;&#21160;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#36755;&#20837;&#36755;&#20986;&#20449;&#24687;&#32534;&#30721;&#20026;&#27169;&#22411;&#30340;&#32039;&#20945;&#19988;&#20840;&#38754;&#30340;&#34920;&#31034;&#65288;&#21363;DNA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the life cycle of the machine learning (ML) model is an intriguing area of research (e.g., understanding where the model comes from, how it is trained, and how it is used). This paper focuses on a novel problem within this field, namely Model Provenance (MP), which concerns the relationship between a target model and its pre-training model and aims to determine whether a source model serves as the provenance for a target model. This is an important problem that has significant implications for ensuring the security and intellectual property of machine learning models but has not received much attention in the literature. To fill in this gap, we introduce a novel concept of Model DNA which represents the unique characteristics of a machine learning model. We utilize a data-driven and model-driven representation learning method to encode the model's training data and input-output information as a compact and comprehensive representation (i.e., DNA) of the model. Using this 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#23558;&#20154;&#24037;&#26234;&#33021;&#38598;&#25104;&#21040;&#26426;&#26800;&#33218;&#30340;&#20849;&#20139;&#25511;&#21046;&#33539;&#24335;&#20013;&#65292;&#20197;&#24110;&#21161;&#36816;&#21160;&#21463;&#25439;&#20154;&#22763;&#23454;&#29616;&#26356;&#39640;&#31243;&#24230;&#30340;&#20010;&#20154;&#33258;&#27835;&#12290;</title><link>http://arxiv.org/abs/2306.13509</link><description>&lt;p&gt;
&#25506;&#32034;AI&#22686;&#24378;&#30340;&#21327;&#20316;&#25511;&#21046;&#23545;&#20110;&#36741;&#21161;&#26426;&#26800;&#33218;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring AI-enhanced Shared Control for an Assistive Robotic Arm. (arXiv:2306.13509v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#23558;&#20154;&#24037;&#26234;&#33021;&#38598;&#25104;&#21040;&#26426;&#26800;&#33218;&#30340;&#20849;&#20139;&#25511;&#21046;&#33539;&#24335;&#20013;&#65292;&#20197;&#24110;&#21161;&#36816;&#21160;&#21463;&#25439;&#20154;&#22763;&#23454;&#29616;&#26356;&#39640;&#31243;&#24230;&#30340;&#20010;&#20154;&#33258;&#27835;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36741;&#21161;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#36741;&#21161;&#26426;&#26800;&#33218;&#24050;&#32463;&#25104;&#20026;&#24110;&#21161;&#36816;&#21160;&#21463;&#25439;&#20154;&#22763;&#23454;&#29616;&#33258;&#20027;&#29983;&#27963;&#30340;&#21487;&#33021;&#24615;&#12290;&#36817;&#24180;&#26469;&#65292;&#36234;&#26469;&#36234;&#22810;&#36825;&#26679;&#30340;&#31995;&#32479;&#24050;&#32463;&#38754;&#21521;&#26368;&#32456;&#29992;&#25143;&#25552;&#20379;&#65292;&#20363;&#22914;Kinova Jaco&#26426;&#26800;&#33218;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22823;&#22810;&#38656;&#35201;&#22797;&#26434;&#30340;&#25163;&#21160;&#25511;&#21046;&#65292;&#36825;&#21487;&#33021;&#20250;&#20351;&#29992;&#25143;&#19981;&#22570;&#37325;&#36127;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#35753;&#36825;&#20123;&#26426;&#22120;&#20154;&#33258;&#20027;&#34892;&#21160;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#33267;&#23569;&#23545;&#20110;&#36825;&#20010;&#29305;&#23450;&#30340;&#29992;&#25143;&#32676;&#20307;&#26469;&#35828;&#65292;&#36825;&#31181;&#26041;&#27861;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#24466;&#21171;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#29992;&#25143;&#24076;&#26395;&#20445;&#25345;&#25511;&#21046;&#26435;&#20197;&#23454;&#29616;&#26356;&#39640;&#31243;&#24230;&#30340;&#20010;&#20154;&#33258;&#27835;&#65292;&#20294;&#33258;&#20027;&#30340;&#26426;&#22120;&#20154;&#19982;&#27492;&#30456;&#21453;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#38598;&#25104;&#21040;&#20849;&#20139;&#25511;&#21046;&#33539;&#24335;&#20013;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#20851;&#27880;&#20102;&#20154;&#19982;&#26426;&#22120;&#20154;&#20043;&#38388;&#30028;&#38754;&#30340;&#24517;&#35201;&#35201;&#27714;&#65292;&#20197;&#21450;&#22914;&#20309;&#22312;&#26174;&#33879;&#20943;&#23569;&#24515;&#29702;&#36127;&#25285;&#21644;&#25152;&#38656;&#30340;&#26426;&#21160;&#33021;&#21147;&#30340;&#21516;&#26102;&#20445;&#25345;&#20154;&#31867;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Assistive technologies and in particular assistive robotic arms have the potential to enable people with motor impairments to live a self-determined life. More and more of these systems have become available for end users in recent years, such as the Kinova Jaco robotic arm. However, they mostly require complex manual control, which can overwhelm users. As a result, researchers have explored ways to let such robots act autonomously. However, at least for this specific group of users, such an approach has shown to be futile. Here, users want to stay in control to achieve a higher level of personal autonomy, to which an autonomous robot runs counter. In our research, we explore how Artifical Intelligence (AI) can be integrated into a shared control paradigm. In particular, we focus on the consequential requirements for the interface between human and robot and how we can keep humans in the loop while still significantly reducing the mental load and required motor skills.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#22686;&#24378;&#26041;&#27861;&#21644;&#20004;&#31181;logit&#35843;&#25972;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38271;&#23614;&#35270;&#35273;&#35782;&#21035;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;CIFAR&#21644;ImageNet&#38271;&#23614;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.10648</link><description>&lt;p&gt;
&#23545;&#39640;&#38271;&#23614;&#35270;&#35273;&#35782;&#21035;&#20013;&#30340;logit&#36827;&#34892;&#39640;&#26031;&#24418;&#24335;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Adjusting Logit in Gaussian Form for Long-Tailed Visual Recognition. (arXiv:2305.10648v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#22686;&#24378;&#26041;&#27861;&#21644;&#20004;&#31181;logit&#35843;&#25972;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38271;&#23614;&#35270;&#35273;&#35782;&#21035;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;CIFAR&#21644;ImageNet&#38271;&#23614;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#25968;&#25454;&#24448;&#24448;&#20855;&#26377;&#38271;&#23614;&#20998;&#24067;&#12290;&#23545;&#20110;&#36825;&#31181;&#25968;&#25454;&#65292;&#30001;&#20110;&#38590;&#20197;&#27491;&#30830;&#20998;&#31867;&#23614;&#37096;&#31867;&#21035;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#25991;&#29486;&#20013;&#65292;&#24050;&#26377;&#19968;&#20123;&#26041;&#27861;&#36890;&#36807;&#20943;&#23569;&#20998;&#31867;&#22120;&#20559;&#24046;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21069;&#25552;&#26159;&#29992;&#38271;&#23614;&#25968;&#25454;&#33719;&#24471;&#30340;&#29305;&#24449;&#36275;&#22815;&#20195;&#34920;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#30452;&#25509;&#22312;&#38271;&#23614;&#25968;&#25454;&#19978;&#35757;&#32451;&#20250;&#23548;&#33268;&#19981;&#22343;&#21248;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#22836;&#31867;&#30340;&#23884;&#20837;&#31354;&#38388;&#20005;&#37325;&#21387;&#32553;&#23614;&#31867;&#65292;&#36825;&#23545;&#20110;&#21518;&#32493;&#30340;&#20998;&#31867;&#22120;&#23398;&#20064;&#26159;&#19981;&#21033;&#30340;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20174;&#29305;&#24449;&#27700;&#24179;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#38271;&#23614;&#35270;&#35273;&#35782;&#21035;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#29305;&#24449;&#22686;&#24378;&#26469;&#24179;&#34913;&#23884;&#20837;&#20998;&#24067;&#12290;&#19981;&#21516;&#31867;&#21035;&#30340;&#29305;&#24449;&#20197;&#39640;&#26031;&#24418;&#24335;&#20855;&#26377;&#19981;&#21516;&#25391;&#24133;&#30340;&#25200;&#21160;&#12290;&#22522;&#20110;&#36825;&#20123;&#25200;&#21160;&#30340;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;logit&#35843;&#25972;&#26041;&#27861;&#26469;&#25552;&#39640;&#23614;&#37096;&#31867;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;CIFAR&#21644;ImageNet&#38271;&#23614;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is not uncommon that real-world data are distributed with a long tail. For such data, the learning of deep neural networks becomes challenging because it is hard to classify tail classes correctly. In the literature, several existing methods have addressed this problem by reducing classifier bias provided that the features obtained with long-tailed data are representative enough. However, we find that training directly on long-tailed data leads to uneven embedding space. That is, the embedding space of head classes severely compresses that of tail classes, which is not conducive to subsequent classifier learning. %further improving model performance. This paper therefore studies the problem of long-tailed visual recognition from the perspective of feature level. We introduce feature augmentation to balance the embedding distribution. The features of different classes are perturbed with varying amplitudes in Gaussian form. Based on these perturbed features, two novel logit adjustment
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#29983;&#29702;&#25968;&#25454;&#20013;&#36827;&#34892;&#24773;&#24863;&#35782;&#21035;&#30340;&#22810;&#23610;&#24230;Transformer&#32593;&#32476;&#65292;&#37319;&#29992;&#20102;&#22810;&#27169;&#24577;&#25216;&#26415;&#21644;&#32553;&#25918;&#25968;&#25454;&#65292;&#32467;&#21512;Transformer&#21644;&#39640;&#26031;&#21464;&#25442;&#25216;&#26415;&#20197;&#25552;&#39640;&#20449;&#21495;&#32534;&#30721;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;EPiC&#31454;&#36187;&#30340;CASE&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.00769</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#29983;&#29702;&#20449;&#21495;&#30340;&#24773;&#24863;&#35782;&#21035;&#30340;&#22810;&#23610;&#24230;Transformer&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multi-scale Transformer-based Network for Emotion Recognition from Multi Physiological Signals. (arXiv:2305.00769v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#29983;&#29702;&#25968;&#25454;&#20013;&#36827;&#34892;&#24773;&#24863;&#35782;&#21035;&#30340;&#22810;&#23610;&#24230;Transformer&#32593;&#32476;&#65292;&#37319;&#29992;&#20102;&#22810;&#27169;&#24577;&#25216;&#26415;&#21644;&#32553;&#25918;&#25968;&#25454;&#65292;&#32467;&#21512;Transformer&#21644;&#39640;&#26031;&#21464;&#25442;&#25216;&#26415;&#20197;&#25552;&#39640;&#20449;&#21495;&#32534;&#30721;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;EPiC&#31454;&#36187;&#30340;CASE&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;&#22810;&#23610;&#24230;Transformer&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#29983;&#29702;&#25968;&#25454;&#20013;&#36827;&#34892;&#24773;&#24863;&#35782;&#21035;&#12290;&#29616;&#20195;&#20256;&#24863;&#22120;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#33021;&#22815;&#20174;&#36825;&#20123;&#20449;&#21495;&#20013;&#25552;&#21462;&#22823;&#37327;&#20449;&#24687;&#65292;&#22240;&#27492;&#36825;&#19968;&#20219;&#21153;&#22312;&#30740;&#31350;&#31038;&#21306;&#20013;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#24212;&#29992;&#22810;&#27169;&#24577;&#25216;&#26415;&#21644;&#32553;&#25918;&#25968;&#25454;&#65292;&#20197;&#24314;&#31435;&#20869;&#37096;&#36523;&#20307;&#20449;&#21495;&#19982;&#20154;&#31867;&#24773;&#24863;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;Transformer&#21644;&#39640;&#26031;&#21464;&#25442;&#25216;&#26415;&#65292;&#25552;&#39640;&#20449;&#21495;&#32534;&#30721;&#30340;&#26377;&#25928;&#24615;&#21644;&#25972;&#20307;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;EPiC&#31454;&#36187;&#30340;CASE&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#32467;&#26524;&#65292;RMSE&#24471;&#20998;&#20026;1.45&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an efficient Multi-scale Transformer-based approach for the task of Emotion recognition from Physiological data, which has gained widespread attention in the research community due to the vast amount of information that can be extracted from these signals using modern sensors and machine learning techniques. Our approach involves applying a Multi-modal technique combined with scaling data to establish the relationship between internal body signals and human emotions. Additionally, we utilize Transformer and Gaussian Transformation techniques to improve signal encoding effectiveness and overall performance. Our model achieves decent results on the CASE dataset of the EPiC competition, with an RMSE score of 1.45.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22914;&#20309;&#36890;&#36807;&#25968;&#23383;&#24179;&#21488;&#20256;&#36882;&#30340;&#34892;&#20026;&#20581;&#24247;&#20171;&#20837;&#26368;&#22823;&#21270;&#20581;&#24247;&#32467;&#26524;&#21644;&#27835;&#30103;&#25104;&#26412;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DecompPI&#30340;&#26032;&#31639;&#27861;&#65292;&#20174;&#31163;&#32447;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#20219;&#21153;&#65292;&#20943;&#36731;&#20102;&#22312;&#32447;&#23454;&#39564;&#30340;&#38656;&#35201;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#28176;&#36817;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12206</link><description>&lt;p&gt;
&#34892;&#20026;&#20581;&#24247;&#20010;&#24615;&#21270;&#20171;&#20837;&#30340;&#25919;&#31574;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Policy Optimization for Personalized Interventions in Behavioral Health. (arXiv:2303.12206v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12206
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22914;&#20309;&#36890;&#36807;&#25968;&#23383;&#24179;&#21488;&#20256;&#36882;&#30340;&#34892;&#20026;&#20581;&#24247;&#20171;&#20837;&#26368;&#22823;&#21270;&#20581;&#24247;&#32467;&#26524;&#21644;&#27835;&#30103;&#25104;&#26412;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DecompPI&#30340;&#26032;&#31639;&#27861;&#65292;&#20174;&#31163;&#32447;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#20219;&#21153;&#65292;&#20943;&#36731;&#20102;&#22312;&#32447;&#23454;&#39564;&#30340;&#38656;&#35201;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#28176;&#36817;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#23450;&#20041;&#65306;&#36890;&#36807;&#25968;&#23383;&#24179;&#21488;&#20256;&#36882;&#30340;&#34892;&#20026;&#20581;&#24247;&#20171;&#20837;&#65292;&#36890;&#36807;&#25945;&#32946;&#65292;&#28608;&#21169;&#65292;&#25552;&#37266;&#21644;&#22806;&#23637;&#65292;&#26377;&#26395;&#26174;&#30528;&#25913;&#21892;&#20581;&#24247;&#32467;&#26524;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20171;&#20837;&#20855;&#26377;&#25104;&#26412;&#21644;&#33021;&#21147;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#20248;&#21270;&#24739;&#32773;&#20010;&#24615;&#21270;&#20171;&#20837;&#20197;&#26368;&#22823;&#21270;&#26576;&#31181;&#38271;&#26399;&#32467;&#26524;&#30340;&#38382;&#39064;&#12290;&#26041;&#27861;/&#32467;&#26524;&#65306;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26469;&#33258;&#22686;&#24378;&#23398;&#20064;&#25991;&#29486;&#30340;&#36890;&#29992;&#26080;&#27169;&#22411;&#26041;&#27861;&#23545;&#20110;&#21307;&#30103;&#24212;&#29992;&#26469;&#35828;&#36807;&#20110;&#25968;&#25454;&#23494;&#38598;&#65292;&#32780;&#26356;&#31616;&#21333;&#30340;&#36172;&#33218;&#38382;&#39064;&#26041;&#27861;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#24573;&#30053;&#20102;&#38271;&#26399;&#24739;&#32773;&#21160;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#31216;&#20026;DecompPI&#65292;&#23427;&#36817;&#20284;&#20110;&#19968;&#27493;&#25919;&#31574;&#36845;&#20195;&#12290;&#23454;&#29616;DecompPI&#21482;&#38656;&#20174;&#31163;&#32447;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#20219;&#21153;&#65292;&#20943;&#36731;&#20102;&#22312;&#32447;&#23454;&#39564;&#30340;&#38656;&#35201;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19968;&#31181;&#33258;&#28982;&#30340;&#32467;&#26500;&#20551;&#35774;&#19979;&#65292;DecompPI&#21487;&#20197;&#33719;&#24471;&#31639;&#27861;&#22797;&#26434;&#24230;&#30340;&#28176;&#36817;&#25910;&#25947;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#27169;&#22411;.
&lt;/p&gt;
&lt;p&gt;
Problem definition: Behavioral health interventions, delivered through digital platforms, have the potential to significantly improve health outcomes, through education, motivation, reminders, and outreach. We study the problem of optimizing personalized interventions for patients to maximize some long-term outcome, in a setting where interventions are costly and capacity-constrained.  Methodology/results: This paper provides a model-free approach to solving this problem. We find that generic model-free approaches from the reinforcement learning literature are too data intensive for healthcare applications, while simpler bandit approaches make progress at the expense of ignoring long-term patient dynamics. We present a new algorithm we dub DecompPI that approximates one step of policy iteration. Implementing DecompPI simply consists of a prediction task from offline data, alleviating the need for online experimentation. Theoretically, we show that under a natural set of structural assu
&lt;/p&gt;</description></item><item><title>&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#38761;&#26032;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;&#22522;&#22240;&#32452;&#23398;&#39046;&#22495;&#20013;&#35299;&#20915;&#33258;&#21160;&#25968;&#25454;&#20998;&#26512;&#21644;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#38477;&#20302;&#25910;&#38598;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#26412;&#65292;&#36866;&#29992;&#20110;&#22522;&#22240;&#32452;&#25968;&#25454;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;&#26412;&#35843;&#26597;&#37325;&#28857;&#20851;&#27880;&#22312;&#22522;&#22240;&#32452;&#30740;&#31350;&#39046;&#22495;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#12289;&#22522;&#22240;&#32452;&#32452;&#35013;&#21644;&#24207;&#21015;&#27604;&#23545;&#12290;</title><link>http://arxiv.org/abs/2302.13268</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#38761;&#26032;&#22522;&#22240;&#32452;&#23398;
&lt;/p&gt;
&lt;p&gt;
Revolutionizing Genomics with Reinforcement Learning Techniques. (arXiv:2302.13268v2 [q-bio.GN] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13268
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#38761;&#26032;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;&#22522;&#22240;&#32452;&#23398;&#39046;&#22495;&#20013;&#35299;&#20915;&#33258;&#21160;&#25968;&#25454;&#20998;&#26512;&#21644;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#38477;&#20302;&#25910;&#38598;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#26412;&#65292;&#36866;&#29992;&#20110;&#22522;&#22240;&#32452;&#25968;&#25454;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;&#26412;&#35843;&#26597;&#37325;&#28857;&#20851;&#27880;&#22312;&#22522;&#22240;&#32452;&#30740;&#31350;&#39046;&#22495;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#12289;&#22522;&#22240;&#32452;&#32452;&#35013;&#21644;&#24207;&#21015;&#27604;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#20986;&#29616;&#22312;&#35299;&#20915;&#21508;&#31181;&#38382;&#39064;&#20013;&#65292;&#21253;&#25324;&#20915;&#31574;&#21644;&#22522;&#22240;&#32452;&#23398;&#12290;&#36807;&#21435;&#20108;&#21313;&#24180;&#30340;&#21407;&#22987;&#22522;&#22240;&#32452;&#25968;&#25454;&#25351;&#25968;&#22686;&#38271;&#24050;&#32463;&#36229;&#20986;&#20102;&#25163;&#21160;&#20998;&#26512;&#30340;&#33021;&#21147;&#65292;&#36825;&#23548;&#33268;&#23545;&#33258;&#21160;&#25968;&#25454;&#20998;&#26512;&#21644;&#22788;&#29702;&#30340;&#20852;&#36259;&#36234;&#26469;&#36234;&#22823;&#12290;RL&#31639;&#27861;&#33021;&#22815;&#22312;&#26368;&#23567;&#30340;&#20154;&#24037;&#30417;&#30563;&#19979;&#20174;&#32463;&#39564;&#20013;&#23398;&#20064;&#65292;&#20351;&#20854;&#38750;&#24120;&#36866;&#21512;&#22522;&#22240;&#32452;&#25968;&#25454;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;&#20351;&#29992;RL&#30340;&#19968;&#20010;&#20851;&#38190;&#22909;&#22788;&#26159;&#38477;&#20302;&#20102;&#25910;&#38598;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#26412;&#65292;&#36825;&#26159;&#30417;&#30563;&#23398;&#20064;&#25152;&#38656;&#30340;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#35768;&#22810;&#30740;&#31350;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#22522;&#22240;&#32452;&#23398;&#20013;&#30340;&#24212;&#29992;&#65292;&#20294;&#26412;&#35843;&#26597;&#20165;&#19987;&#27880;&#20110;&#22312;&#21508;&#31181;&#22522;&#22240;&#32452;&#30740;&#31350;&#39046;&#22495;&#65288;&#21253;&#25324;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#65292;&#22522;&#22240;&#32452;&#32452;&#35013;&#21644;&#24207;&#21015;&#27604;&#23545;&#65289;&#20013;&#20351;&#29992;RL&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#23545;&#29616;&#26377;&#30740;&#31350;&#30340;&#25216;&#26415;&#32454;&#33410;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Reinforcement Learning (RL) has emerged as a powerful tool for solving a wide range of problems, including decision-making and genomics. The exponential growth of raw genomic data over the past two decades has exceeded the capacity of manual analysis, leading to a growing interest in automatic data analysis and processing. RL algorithms are capable of learning from experience with minimal human supervision, making them well-suited for genomic data analysis and interpretation. One of the key benefits of using RL is the reduced cost associated with collecting labeled training data, which is required for supervised learning. While there have been numerous studies examining the applications of Machine Learning (ML) in genomics, this survey focuses exclusively on the use of RL in various genomics research fields, including gene regulatory networks (GRNs), genome assembly, and sequence alignment. We present a comprehensive technical overview of existing studies on the applic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#20013;&#26159;&#21542;&#20687;&#20154;&#31867;&#19968;&#26679;&#36890;&#36807;&#28151;&#20837;&#20869;&#23481;&#26469;&#24433;&#21709;&#31572;&#26696;&#65292;&#32467;&#26524;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#39564;&#26399;&#26395;&#33021;&#22815;&#25429;&#25417;&#21040;&#36825;&#31181;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2207.07051</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26174;&#31034;&#23545;&#25512;&#29702;&#20219;&#21153;&#20855;&#26377;&#31867;&#20284;&#20154;&#31867;&#30340;&#20869;&#23481;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Language models show human-like content effects on reasoning tasks. (arXiv:2207.07051v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#20013;&#26159;&#21542;&#20687;&#20154;&#31867;&#19968;&#26679;&#36890;&#36807;&#28151;&#20837;&#20869;&#23481;&#26469;&#24433;&#21709;&#31572;&#26696;&#65292;&#32467;&#26524;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#39564;&#26399;&#26395;&#33021;&#22815;&#25429;&#25417;&#21040;&#36825;&#31181;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#35937;&#25512;&#29702;&#26159;&#26234;&#33021;&#31995;&#32479;&#30340;&#20851;&#38190;&#33021;&#21147;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#39640;&#20110;&#38543;&#26426;&#30340;&#24615;&#33021;&#65292;&#20294;&#23384;&#22312;&#35768;&#22810;&#19981;&#23436;&#21892;&#20043;&#22788;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#30340;&#25277;&#35937;&#25512;&#29702;&#20063;&#26159;&#19981;&#23436;&#32654;&#30340;&#12290;&#20363;&#22914;&#65292;&#20154;&#31867;&#25512;&#29702;&#21463;&#21040;&#25105;&#20204;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#30693;&#35782;&#21644;&#20449;&#24565;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#8220;&#20869;&#23481;&#25928;&#24212;&#8221;&#65307;&#24403;&#38382;&#39064;&#30340;&#35821;&#20041;&#20869;&#23481;&#25903;&#25345;&#27491;&#30830;&#30340;&#36923;&#36753;&#25512;&#29702;&#26102;&#65292;&#20154;&#31867;&#26356;&#21487;&#38752;&#22320;&#36827;&#34892;&#25512;&#29702;&#12290;&#36825;&#20123;&#20869;&#23481;&#32416;&#32544;&#30340;&#25512;&#29702;&#27169;&#24335;&#22312;&#20851;&#20110;&#20154;&#31867;&#26234;&#33021;&#22522;&#26412;&#24615;&#36136;&#30340;&#20105;&#35770;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20197;&#31867;&#20284;&#30340;&#26041;&#24335;&#28151;&#20837;&#20869;&#23481;&#26469;&#22238;&#31572;&#36923;&#36753;&#38382;&#39064;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#39564;&#26399;&#26395;&#25429;&#25417;&#20102;&#19968;&#20123;&#20154;&#31867;&#30693;&#35782;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#25506;&#32034;&#20102;&#36825;&#20010;&#38382;&#39064;&#65306;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#21028;&#26029;&#19977;&#27573;&#35770;&#30340;&#36923;&#36753;&#26377;&#25928;&#24615;&#21644;Wason&#36873;&#25321;&#20219;&#21153;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Abstract reasoning is a key ability for an intelligent system. Large language models (LMs) achieve above-chance performance on abstract reasoning tasks, but exhibit many imperfections. However, human abstract reasoning is also imperfect. For example, human reasoning is affected by our real-world knowledge and beliefs, and shows notable "content effects"; humans reason more reliably when the semantic content of a problem supports the correct logical inferences. These content-entangled reasoning patterns play a central role in debates about the fundamental nature of human intelligence. Here, we investigate whether language models $\unicode{x2014}$ whose prior expectations capture some aspects of human knowledge $\unicode{x2014}$ similarly mix content into their answers to logical problems. We explored this question across three logical reasoning tasks: natural language inference, judging the logical validity of syllogisms, and the Wason selection task. We evaluate state of the art large 
&lt;/p&gt;</description></item></channel></rss>