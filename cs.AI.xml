<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#20171;&#32461;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#22522;&#20110;&#35777;&#25454;&#26435;&#37325;&#26694;&#26550;&#30340;&#20551;&#35774;&#39537;&#21160;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#25903;&#25345;&#25110;&#39539;&#26021;&#20551;&#35774;&#30340;&#35777;&#25454;&#26469;&#22686;&#21152;&#20915;&#31574;&#20934;&#30830;&#24615;&#21644;&#20943;&#23569;&#20381;&#36182;&#31243;&#24230;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01292</link><description>&lt;p&gt;
&#36808;&#21521;&#26032;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65306;&#36890;&#36807;&#35777;&#25454;&#25903;&#25345;&#30340;&#20551;&#35774;&#39537;&#21160;&#26041;&#27861;&#30340;&#20915;&#31574;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Towards the new XAI: A Hypothesis-Driven Approach to Decision Support Using Evidence
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#22522;&#20110;&#35777;&#25454;&#26435;&#37325;&#26694;&#26550;&#30340;&#20551;&#35774;&#39537;&#21160;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#25903;&#25345;&#25110;&#39539;&#26021;&#20551;&#35774;&#30340;&#35777;&#25454;&#26469;&#22686;&#21152;&#20915;&#31574;&#20934;&#30830;&#24615;&#21644;&#20943;&#23569;&#20381;&#36182;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20043;&#21069;&#20851;&#20110;AI&#36741;&#21161;&#20154;&#31867;&#20915;&#31574;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20960;&#31181;&#19981;&#21516;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#19968;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33539;&#24335;&#36716;&#21464;&#65292;&#21628;&#21505;&#36890;&#36807;&#19968;&#20010;&#31216;&#20026;&#35780;&#20215;&#22411;AI&#30340;&#27010;&#24565;&#26694;&#26550;&#26469;&#36827;&#34892;&#20551;&#35774;&#39537;&#21160;&#30340;XAI&#65292;&#35813;&#26694;&#26550;&#20026;&#20154;&#20204;&#25552;&#20379;&#25903;&#25345;&#25110;&#39539;&#26021;&#20551;&#35774;&#30340;&#35777;&#25454;&#65292;&#32780;&#19981;&#19968;&#23450;&#32473;&#20986;&#20915;&#31574;&#36741;&#21161;&#25512;&#33616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#22522;&#20110;&#35777;&#25454;&#26435;&#37325;&#65288;WoE&#65289;&#26694;&#26550;&#30340;&#20551;&#35774;&#39537;&#21160;XAI&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20026;&#32473;&#23450;&#30340;&#20551;&#35774;&#29983;&#25104;&#27491;&#38754;&#21644;&#36127;&#38754;&#35777;&#25454;&#12290;&#36890;&#36807;&#20154;&#31867;&#34892;&#20026;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#20551;&#35774;&#39537;&#21160;&#26041;&#27861;&#25552;&#39640;&#20102;&#20915;&#31574;&#20934;&#30830;&#24615;&#65292;&#19982;&#25512;&#33616;&#39537;&#21160;&#26041;&#27861;&#21644;&#20165;AI&#35299;&#37322;&#22522;&#32447;&#30456;&#27604;&#20943;&#23569;&#20102;&#20381;&#36182;&#31243;&#24230;&#65292;&#20294;&#30456;&#23545;&#20110;&#25512;&#33616;&#39537;&#21160;&#26041;&#27861;&#65292;&#22312;&#20381;&#36182;&#31243;&#24230;&#19979;&#38477;&#26041;&#38754;&#30053;&#24494;&#22686;&#21152;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#21442;&#19982;&#32773;&#22312;&#20351;&#29992;&#25105;&#20204;&#30340;&#20551;&#35774;&#39537;&#21160;&#26041;&#27861;&#26102;&#19982;&#20004;&#20010;&#22522;&#32447;&#30340;&#26041;&#24335;&#23384;&#22312;&#23454;&#36136;&#24615;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior research on AI-assisted human decision-making has explored several different explainable AI (XAI) approaches. A recent paper has proposed a paradigm shift calling for hypothesis-driven XAI through a conceptual framework called evaluative AI that gives people evidence that supports or refutes hypotheses without necessarily giving a decision-aid recommendation. In this paper we describe and evaluate an approach for hypothesis-driven XAI based on the Weight of Evidence (WoE) framework, which generates both positive and negative evidence for a given hypothesis. Through human behavioural experiments, we show that our hypothesis-driven approach increases decision accuracy, reduces reliance compared to a recommendation-driven approach and an AI-explanation-only baseline, but with a small increase in under-reliance compared to the recommendation-driven approach. Further, we show that participants used our hypothesis-driven approach in a materially different way to the two baselines.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#23433;&#20840;&#38388;&#38548;RRT*&#65288;SI-RRT*&#65289;&#20004;&#32423;&#26041;&#27861;&#65292;&#20302;&#32423;&#37319;&#29992;&#37319;&#26679;&#35268;&#21010;&#22120;&#25214;&#21040;&#21333;&#20010;&#26426;&#22120;&#20154;&#30340;&#26080;&#30896;&#25758;&#36712;&#36857;&#65292;&#39640;&#32423;&#36890;&#36807;&#20248;&#20808;&#35268;&#21010;&#25110;&#22522;&#20110;&#20914;&#31361;&#30340;&#25628;&#32034;&#35299;&#20915;&#26426;&#22120;&#20154;&#38388;&#20914;&#31361;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;SI-RRT*&#33021;&#22815;&#24555;&#36895;&#25214;&#21040;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;</title><link>https://arxiv.org/abs/2404.01752</link><description>&lt;p&gt;
&#23433;&#20840;&#38388;&#38548;RRT*&#29992;&#20110;&#36830;&#32493;&#31354;&#38388;&#20013;&#21487;&#25193;&#23637;&#30340;&#22810;&#26426;&#22120;&#20154;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Safe Interval RRT* for Scalable Multi-Robot Path Planning in Continuous Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01752
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#23433;&#20840;&#38388;&#38548;RRT*&#65288;SI-RRT*&#65289;&#20004;&#32423;&#26041;&#27861;&#65292;&#20302;&#32423;&#37319;&#29992;&#37319;&#26679;&#35268;&#21010;&#22120;&#25214;&#21040;&#21333;&#20010;&#26426;&#22120;&#20154;&#30340;&#26080;&#30896;&#25758;&#36712;&#36857;&#65292;&#39640;&#32423;&#36890;&#36807;&#20248;&#20808;&#35268;&#21010;&#25110;&#22522;&#20110;&#20914;&#31361;&#30340;&#25628;&#32034;&#35299;&#20915;&#26426;&#22120;&#20154;&#38388;&#20914;&#31361;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;SI-RRT*&#33021;&#22815;&#24555;&#36895;&#25214;&#21040;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#35299;&#20915;&#22810;&#26426;&#22120;&#20154;&#36335;&#24452;&#35268;&#21010;&#65288;MRPP&#65289;&#38382;&#39064;&#20197;&#25214;&#21040;&#26080;&#20914;&#31361;&#36335;&#24452;&#30340;&#38382;&#39064;&#12290;&#38382;&#39064;&#30340;&#22256;&#38590;&#20027;&#35201;&#26469;&#33258;&#20004;&#20010;&#22240;&#32032;&#12290;&#39318;&#20808;&#65292;&#28041;&#21450;&#22810;&#20010;&#26426;&#22120;&#20154;&#20250;&#23548;&#33268;&#32452;&#21512;&#20915;&#31574;&#65292;&#20351;&#25628;&#32034;&#31354;&#38388;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#20854;&#27425;&#65292;&#36830;&#32493;&#31354;&#38388;&#21576;&#29616;&#20986;&#28508;&#22312;&#26080;&#38480;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#32423;&#26041;&#27861;&#65292;&#20302;&#32423;&#26159;&#22522;&#20110;&#37319;&#26679;&#30340;&#35268;&#21010;&#22120;&#23433;&#20840;&#38388;&#38548;RRT*&#65288;SI-RRT*&#65289;&#65292;&#29992;&#20110;&#25214;&#21040;&#21333;&#20010;&#26426;&#22120;&#20154;&#30340;&#26080;&#30896;&#25758;&#36712;&#36857;&#12290;&#39640;&#32423;&#21487;&#20197;&#20351;&#29992;&#33021;&#22815;&#35299;&#20915;&#26426;&#22120;&#20154;&#38388;&#20914;&#31361;&#30340;&#20219;&#20309;&#26041;&#27861;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20004;&#31181;&#20195;&#34920;&#24615;&#26041;&#27861;&#65292;&#21363;&#20248;&#20808;&#35268;&#21010;&#65288;SI-CPP&#65289;&#21644;&#22522;&#20110;&#20914;&#31361;&#30340;&#25628;&#32034;&#65288;SI-CCBS&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SI-RRT*&#33021;&#22815;&#24555;&#36895;&#25214;&#21040;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#37327;&#36739;&#23569;&#12290;SI-CPP&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#32780;SI-CCBS&#20135;&#29983;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01752v1 Announce Type: cross  Abstract: In this paper, we consider the problem of Multi-Robot Path Planning (MRPP) in continuous space to find conflict-free paths. The difficulty of the problem arises from two primary factors. First, the involvement of multiple robots leads to combinatorial decision-making, which escalates the search space exponentially. Second, the continuous space presents potentially infinite states and actions. For this problem, we propose a two-level approach where the low level is a sampling-based planner Safe Interval RRT* (SI-RRT*) that finds a collision-free trajectory for individual robots. The high level can use any method that can resolve inter-robot conflicts where we employ two representative methods that are Prioritized Planning (SI-CPP) and Conflict Based Search (SI-CCBS). Experimental results show that SI-RRT* can find a high-quality solution quickly with a small number of samples. SI-CPP exhibits improved scalability while SI-CCBS produces 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#26041;&#27861;&#65292;JudgeDeceiver&#65292;&#38024;&#23545;LLM-as-a-Judge&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#29983;&#25104;&#23545;&#25239;&#24207;&#21015;&#23454;&#29616;&#20102;&#26377;&#38024;&#23545;&#24615;&#21644;&#39640;&#25928;&#30340;&#27169;&#22411;&#35780;&#20272;&#25805;&#25511;&#12290;</title><link>https://arxiv.org/abs/2403.17710</link><description>&lt;p&gt;
&#22522;&#20110;&#20248;&#21270;&#30340;&#23545;LLM&#35780;&#21028;&#31995;&#32479;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Optimization-based Prompt Injection Attack to LLM-as-a-Judge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17710
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#26041;&#27861;&#65292;JudgeDeceiver&#65292;&#38024;&#23545;LLM-as-a-Judge&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#29983;&#25104;&#23545;&#25239;&#24207;&#21015;&#23454;&#29616;&#20102;&#26377;&#38024;&#23545;&#24615;&#21644;&#39640;&#25928;&#30340;&#27169;&#22411;&#35780;&#20272;&#25805;&#25511;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM-as-a-Judge &#26159;&#19968;&#31181;&#21487;&#20197;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35780;&#20272;&#25991;&#26412;&#20449;&#24687;&#30340;&#26032;&#39062;&#35299;&#20915;&#26041;&#26696;&#12290;&#26681;&#25454;&#29616;&#26377;&#30740;&#31350;&#65292;LLMs&#22312;&#25552;&#20379;&#20256;&#32479;&#20154;&#31867;&#35780;&#20272;&#30340;&#24341;&#20154;&#27880;&#30446;&#26367;&#20195;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#38024;&#23545;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;JudgeDeceiver&#65292;&#19968;&#31181;&#38024;&#23545;LLM-as-a-Judge&#37327;&#36523;&#23450;&#21046;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21046;&#23450;&#20102;&#19968;&#20010;&#31934;&#30830;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#29992;&#20110;&#25915;&#20987;LLM-as-a-Judge&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#21033;&#29992;&#20248;&#21270;&#31639;&#27861;&#39640;&#25928;&#22320;&#33258;&#21160;&#21270;&#29983;&#25104;&#23545;&#25239;&#24207;&#21015;&#65292;&#23454;&#29616;&#23545;&#27169;&#22411;&#35780;&#20272;&#30340;&#26377;&#38024;&#23545;&#24615;&#21644;&#26377;&#25928;&#30340;&#25805;&#20316;&#12290;&#19982;&#25163;&#24037;&#21046;&#20316;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#21151;&#25928;&#65292;&#32473;&#22522;&#20110;LLM&#30340;&#21028;&#26029;&#31995;&#32479;&#24403;&#21069;&#30340;&#23433;&#20840;&#33539;&#24335;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17710v1 Announce Type: cross  Abstract: LLM-as-a-Judge is a novel solution that can assess textual information with large language models (LLMs). Based on existing research studies, LLMs demonstrate remarkable performance in providing a compelling alternative to traditional human assessment. However, the robustness of these systems against prompt injection attacks remains an open question. In this work, we introduce JudgeDeceiver, a novel optimization-based prompt injection attack tailored to LLM-as-a-Judge. Our method formulates a precise optimization objective for attacking the decision-making process of LLM-as-a-Judge and utilizes an optimization algorithm to efficiently automate the generation of adversarial sequences, achieving targeted and effective manipulation of model evaluations. Compared to handcraft prompt injection attacks, our method demonstrates superior efficacy, posing a significant challenge to the current security paradigms of LLM-based judgment systems. T
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#23618;&#35774;&#35745;&#26032;&#32593;&#32476;&#26550;&#26500;&#20197;&#25552;&#39640;&#23545;&#27169;&#22411;&#36870;&#25512;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14772</link><description>&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#26550;&#26500;&#25552;&#39640;&#27169;&#22411;&#36870;&#25512;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Robustness to Model Inversion Attacks via Sparse Coding Architectures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14772
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#23618;&#35774;&#35745;&#26032;&#32593;&#32476;&#26550;&#26500;&#20197;&#25552;&#39640;&#23545;&#27169;&#22411;&#36870;&#25512;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#27169;&#22411;&#36870;&#25512;&#25915;&#20987;&#31639;&#27861;&#20801;&#35768;&#23545;&#25163;&#36890;&#36807;&#21453;&#22797;&#26597;&#35810;&#31070;&#32463;&#32593;&#32476;&#24182;&#26816;&#26597;&#20854;&#36755;&#20986;&#26469;&#37325;&#24314;&#32593;&#32476;&#30340;&#31169;&#26377;&#35757;&#32451;&#25968;&#25454;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#21033;&#29992;&#31232;&#30095;&#32534;&#30721;&#23618;&#26469;&#33719;&#24471;&#23545;&#36825;&#31867;&#25915;&#20987;&#30340;&#21331;&#36234;&#40065;&#26834;&#24615;&#12290; &#19977;&#21313;&#24180;&#26469;&#65292;&#35745;&#31639;&#26426;&#31185;&#23398;&#30740;&#31350;&#24050;&#32463;&#30740;&#31350;&#20102;&#31232;&#30095;&#32534;&#30721;&#22312;&#22270;&#20687;&#21435;&#22122;&#65292;&#30446;&#26631;&#35782;&#21035;&#21644;&#23545;&#25239;&#24615;&#35823;&#20998;&#35774;&#32622;&#20013;&#30340;&#20316;&#29992;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#38544;&#31169;&#28431;&#27934;&#20043;&#38388;&#30340;&#32852;&#31995;&#23578;&#26410;&#34987;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#31232;&#30095;&#32534;&#30721;&#26550;&#26500;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21033;&#30340;&#25163;&#27573;&#26469;&#25269;&#24481;&#27169;&#22411;&#36870;&#25512;&#25915;&#20987;&#65292;&#22240;&#20026;&#23427;&#20204;&#20801;&#35768;&#25105;&#20204;&#25511;&#21046;&#32534;&#30721;&#22312;&#32593;&#32476;&#30340;&#20013;&#38388;&#34920;&#31034;&#20013;&#30340;&#26080;&#20851;&#31169;&#20154;&#20449;&#24687;&#30340;&#25968;&#37327;&#65292;&#32780;&#36825;&#31181;&#26041;&#24335;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#39640;&#25928;&#35745;&#31639;&#65292;&#24182;&#19988;&#20247;&#25152;&#21608;&#30693;&#21482;&#26377;&#36739;&#23567;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14772v1 Announce Type: cross  Abstract: Recent model inversion attack algorithms permit adversaries to reconstruct a neural network's private training data just by repeatedly querying the network and inspecting its outputs. In this work, we develop a novel network architecture that leverages sparse-coding layers to obtain superior robustness to this class of attacks. Three decades of computer science research has studied sparse coding in the context of image denoising, object recognition, and adversarial misclassification settings, but to the best of our knowledge, its connection to state-of-the-art privacy vulnerabilities remains unstudied. However, sparse coding architectures suggest an advantageous means to defend against model inversion attacks because they allow us to control the amount of irrelevant private information encoded in a network's intermediate representations in a manner that can be computed efficiently during training and that is known to have little effect
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24490;&#29615;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#27169;&#22359;&#65292;&#36890;&#36807;&#23558;&#33033;&#20914;&#31070;&#32463;&#20803;&#30340;&#31070;&#32463;&#21160;&#21147;&#23398;&#19982;&#29702;&#24819;&#30340;&#26102;&#38388;&#20107;&#20214;&#37319;&#26679;&#22120;&#30340;&#34892;&#20026;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#30340;&#20107;&#20214;&#26816;&#27979;&#26694;&#26550;</title><link>https://arxiv.org/abs/2403.12574</link><description>&lt;p&gt;
EAS-SNN&#65306;&#31471;&#21040;&#31471;&#33258;&#36866;&#24212;&#37319;&#26679;&#21644;&#34920;&#31034;&#65292;&#29992;&#20110;&#24490;&#29615;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#20107;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
EAS-SNN: End-to-End Adaptive Sampling and Representation for Event-based Detection with Recurrent Spiking Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12574
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24490;&#29615;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#27169;&#22359;&#65292;&#36890;&#36807;&#23558;&#33033;&#20914;&#31070;&#32463;&#20803;&#30340;&#31070;&#32463;&#21160;&#21147;&#23398;&#19982;&#29702;&#24819;&#30340;&#26102;&#38388;&#20107;&#20214;&#37319;&#26679;&#22120;&#30340;&#34892;&#20026;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#30340;&#20107;&#20214;&#26816;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#25668;&#20687;&#22836;&#20197;&#20854;&#39640;&#21160;&#24577;&#33539;&#22260;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#29289;&#20307;&#26816;&#27979;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#21160;&#24577;&#27169;&#31946;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20809;&#29031;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#26356;&#27880;&#37325;&#20248;&#21270;&#20855;&#26377;&#20808;&#36827;&#26816;&#27979;&#39592;&#24178;&#21644;&#26089;&#26399;&#32858;&#21512;&#21151;&#33021;&#30340;&#26102;&#31354;&#34920;&#31034;&#65292;&#32780;&#33258;&#36866;&#24212;&#20107;&#20214;&#37319;&#26679;&#30340;&#20851;&#38190;&#38382;&#39064;&#20173;&#26410;&#24471;&#21040;&#35299;&#20915;&#12290;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#65292;&#36890;&#36807;&#31232;&#30095;&#33033;&#20914;&#36890;&#20449;&#36816;&#34892;&#30340;&#20107;&#20214;&#39537;&#21160;&#33539;&#24335;&#65292;&#25104;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#22825;&#28982;&#36873;&#25321;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#33033;&#20914;&#31070;&#32463;&#20803;&#30340;&#31070;&#32463;&#21160;&#21147;&#23398;&#19982;&#29702;&#24819;&#30340;&#26102;&#38388;&#20107;&#20214;&#37319;&#26679;&#22120;&#30340;&#34892;&#20026;&#23494;&#20999;&#30456;&#31526;&#12290;&#22312;&#36825;&#19968;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#27169;&#22359;&#65292;&#21033;&#29992;&#20855;&#26377;&#26102;&#38388;&#35760;&#24518;&#30340;&#24490;&#29615;&#21367;&#31215;SNN&#22686;&#24378;&#65292;&#20026;&#22522;&#20110;&#20107;&#20214;&#26816;&#27979;&#30340;&#23436;&#20840;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#26694;&#26550;&#25552;&#20379;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12574v1 Announce Type: cross  Abstract: Event cameras, with their high dynamic range and temporal resolution, are ideally suited for object detection, especially under scenarios with motion blur and challenging lighting conditions. However, while most existing approaches prioritize optimizing spatiotemporal representations with advanced detection backbones and early aggregation functions, the crucial issue of adaptive event sampling remains largely unaddressed. Spiking Neural Networks (SNNs), which operate on an event-driven paradigm through sparse spike communication, emerge as a natural fit for addressing this challenge. In this study, we discover that the neural dynamics of spiking neurons align closely with the behavior of an ideal temporal event sampler. Motivated by this insight, we propose a novel adaptive sampling module that leverages recurrent convolutional SNNs enhanced with temporal memory, facilitating a fully end-to-end learnable framework for event-based detec
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#29616;&#26694;&#26550;ALDI&#20197;&#21450;&#26032;&#30340;DAOD&#22522;&#20934;&#25968;&#25454;&#38598;CFC-DAOD&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#22522;&#20934;&#38382;&#39064;&#65292;&#24182;&#25903;&#25345;&#26410;&#26469;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.12029</link><description>&lt;p&gt;
&#23545;&#40784;&#19982;&#25552;&#28860;&#65306;&#32479;&#19968;&#21644;&#25913;&#36827;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Align and Distill: Unifying and Improving Domain Adaptive Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12029
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#29616;&#26694;&#26550;ALDI&#20197;&#21450;&#26032;&#30340;DAOD&#22522;&#20934;&#25968;&#25454;&#38598;CFC-DAOD&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#22522;&#20934;&#38382;&#39064;&#65292;&#24182;&#25903;&#25345;&#26410;&#26469;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#26816;&#27979;&#22120;&#36890;&#24120;&#34920;&#29616;&#19981;&#20339;&#20110;&#19982;&#20854;&#35757;&#32451;&#38598;&#19981;&#21516;&#30340;&#25968;&#25454;&#12290;&#26368;&#36817;&#65292;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#65288;DAOD&#65289;&#26041;&#27861;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#19978;&#30340;&#24378;&#22823;&#32467;&#26524;&#12290;&#36951;&#25022;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#31995;&#32479;&#21270;&#30340;&#22522;&#20934;&#27979;&#35797;&#38519;&#38449;&#65292;&#36825;&#20123;&#38519;&#38449;&#23545;&#36807;&#21435;&#30340;&#32467;&#26524;&#25552;&#20986;&#36136;&#30097;&#24182;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#30340;&#36827;&#23637;&#65306;&#65288;a&#65289;&#30001;&#20110;&#22522;&#32447;&#19981;&#36275;&#23548;&#33268;&#24615;&#33021;&#39640;&#20272;&#65292;&#65288;b&#65289;&#19981;&#19968;&#33268;&#30340;&#23454;&#29616;&#23454;&#36341;&#38459;&#27490;&#20102;&#26041;&#27861;&#30340;&#36879;&#26126;&#27604;&#36739;&#65292;&#65288;c&#65289;&#30001;&#20110;&#36807;&#26102;&#30340;&#39592;&#24178;&#21644;&#22522;&#20934;&#27979;&#35797;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#23548;&#33268;&#32570;&#20047;&#26222;&#36941;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#20197;&#19979;&#38382;&#39064;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65306;&#65288;1&#65289;&#19968;&#20010;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#29616;&#26694;&#26550;&#65292;Align and Distill&#65288;ALDI&#65289;&#65292;&#25903;&#25345;DAOD&#26041;&#27861;&#30340;&#27604;&#36739;&#24182;&#25903;&#25345;&#26410;&#26469;&#21457;&#23637;&#65292;&#65288;2&#65289;&#19968;&#20010;&#20844;&#24179;&#19988;&#29616;&#20195;&#30340;DAOD&#35757;&#32451;&#21644;&#35780;&#20272;&#21327;&#35758;&#65292;&#35299;&#20915;&#20102;&#22522;&#20934;&#27979;&#35797;&#30340;&#38519;&#38449;&#65292;&#65288;3&#65289;&#19968;&#20010;&#26032;&#30340;DAOD&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;CFC-DAOD&#65292;&#33021;&#22815;&#22312;&#22810;&#26679;&#21270;&#30340;&#30495;&#23454;&#29615;&#22659;&#20013;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12029v1 Announce Type: cross  Abstract: Object detectors often perform poorly on data that differs from their training set. Domain adaptive object detection (DAOD) methods have recently demonstrated strong results on addressing this challenge. Unfortunately, we identify systemic benchmarking pitfalls that call past results into question and hamper further progress: (a) Overestimation of performance due to underpowered baselines, (b) Inconsistent implementation practices preventing transparent comparisons of methods, and (c) Lack of generality due to outdated backbones and lack of diversity in benchmarks. We address these problems by introducing: (1) A unified benchmarking and implementation framework, Align and Distill (ALDI), enabling comparison of DAOD methods and supporting future development, (2) A fair and modern training and evaluation protocol for DAOD that addresses benchmarking pitfalls, (3) A new DAOD benchmark dataset, CFC-DAOD, enabling evaluation on diverse real
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;StateFlow&#30340;&#26032;&#39062;LLM&#20219;&#21153;&#35299;&#20915;&#33539;&#24335;&#65292;&#23558;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#27010;&#24565;&#21270;&#20026;&#29366;&#24577;&#26426;&#65292;&#36890;&#36807;&#29366;&#24577;&#36716;&#25442;&#30830;&#20445;LLM&#21709;&#24212;&#30340;&#28165;&#26224;&#36319;&#36394;&#21644;&#31649;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.11322</link><description>&lt;p&gt;
&#20351;&#29992;StateFlow&#22686;&#24378;LLM&#20219;&#21153;&#35299;&#20915;&#33021;&#21147;&#36890;&#36807;&#29366;&#24577;&#39537;&#21160;&#24037;&#20316;&#27969;
&lt;/p&gt;
&lt;p&gt;
StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11322
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;StateFlow&#30340;&#26032;&#39062;LLM&#20219;&#21153;&#35299;&#20915;&#33539;&#24335;&#65292;&#23558;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#27010;&#24565;&#21270;&#20026;&#29366;&#24577;&#26426;&#65292;&#36890;&#36807;&#29366;&#24577;&#36716;&#25442;&#30830;&#20445;LLM&#21709;&#24212;&#30340;&#28165;&#26224;&#36319;&#36394;&#21644;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#36235;&#21183;&#26085;&#30410;&#26126;&#26174;&#65292;&#20363;&#22914;&#38656;&#35201;&#19968;&#31995;&#21015;&#25805;&#20316;&#21644;&#19982;&#24037;&#20855;&#29615;&#22659;&#21160;&#24577;&#20132;&#20114;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;StateFlow&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;LLM&#30340;&#20219;&#21153;&#27714;&#35299;&#33539;&#24335;&#65292;&#23558;&#30001;LLM&#25903;&#25345;&#30340;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#27010;&#24565;&#21270;&#20026;&#29366;&#24577;&#26426;&#12290;&#36890;&#36807;&#27491;&#30830;&#26500;&#24314;&#29366;&#24577;&#21644;&#23450;&#20041;&#29366;&#24577;&#36716;&#25442;&#65292;StateFlow&#30830;&#23450;&#20102;&#20219;&#21153;&#27714;&#35299;&#30340;&#36827;&#23637;&#65292;&#30830;&#20445;&#28165;&#26224;&#36319;&#36394;&#21644;&#31649;&#29702;LLM&#22312;&#25972;&#20010;&#20219;&#21153;&#27714;&#35299;&#36807;&#31243;&#20013;&#30340;&#21709;&#24212;&#12290;&#22312;&#27599;&#20010;&#29366;&#24577;&#20013;&#65292;StateFlow&#20801;&#35768;&#25191;&#34892;&#19968;&#31995;&#21015;&#21160;&#20316;&#65292;&#19981;&#20165;&#21253;&#25324;&#26681;&#25454;&#29305;&#23450;&#25552;&#31034;&#25351;&#23548;&#29983;&#25104;LLM&#21709;&#24212;&#65292;&#36824;&#21253;&#25324;&#26681;&#25454;&#38656;&#35201;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#12290;&#29366;&#24577;&#36716;&#25442;&#30001;LLM&#20570;&#20986;&#30340;&#29305;&#23450;&#35268;&#21017;&#25110;&#20915;&#31574;&#25511;&#21046;&#65292;&#20801;&#35768;&#36890;&#36807;&#20219;&#21153;&#30340;&#39044;&#23450;&#20041;StateFlow&#27169;&#22411;&#21160;&#24577;&#33258;&#36866;&#24212;&#22320;&#36827;&#34892;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11322v1 Announce Type: cross  Abstract: It is a notable trend to use Large Language Models (LLMs) to tackle complex tasks, e.g., tasks that require a sequence of actions and dynamic interaction with tools and environments. In this paper, we propose StateFlow, a novel LLM-based task-solving paradigm that conceptualizes complex task-solving processes backed by LLMs as state machines. With proper construction of states and definition of state transitions, StateFlow grounds the progress of task-solving, ensuring clear tracking and management of LLMs' responses throughout the task-solving process. Within each state, StateFlow allows execution of a series of actions, involving not only the generation of LLM's responses guided by a specific prompt, but also the utilization of external tools as needed. State transitions are controlled by specific rules or decisions made by the LLM, allowing for a dynamic and adaptive progression through the task's pre-defined StateFlow model. Evalua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#22312;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#20013;&#35782;&#21035;&#21644;&#37327;&#21270;&#24615;&#21035;&#20559;&#35265;&#65292;&#25552;&#20986;&#20102;&#19977;&#20010;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#38750;&#27495;&#35270;&#26631;&#20934;&#24182;&#35774;&#35745;&#20102;&#30456;&#24212;&#30340;&#25552;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.08564</link><description>&lt;p&gt;
&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#38750;&#27495;&#35270;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
Non-discrimination Criteria for Generative Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#22312;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#20013;&#35782;&#21035;&#21644;&#37327;&#21270;&#24615;&#21035;&#20559;&#35265;&#65292;&#25552;&#20986;&#20102;&#19977;&#20010;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#38750;&#27495;&#35270;&#26631;&#20934;&#24182;&#35774;&#35745;&#20102;&#30456;&#24212;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#32463;&#21382;&#20102;&#24555;&#36895;&#21457;&#23637;&#12290;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#36234;&#26469;&#36234;&#26222;&#36941;&#22320;&#25552;&#20379;&#32473;&#20844;&#20247;&#20351;&#29992;&#65292;&#20154;&#20204;&#24320;&#22987;&#25285;&#24515;&#22312;&#24212;&#29992;&#20013;&#24310;&#32493;&#21644;&#25918;&#22823;&#26377;&#23475;&#20559;&#35265;&#30340;&#38382;&#39064;&#12290;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#21487;&#33021;&#23545;&#20854;&#38024;&#23545;&#30340;&#20010;&#20154;&#36896;&#25104;&#20260;&#23475;&#21644;&#38480;&#21046;&#65292;&#26080;&#35770;&#26159;&#30001;&#35823;&#20256;&#36824;&#26159;&#27495;&#35270;&#25152;&#26500;&#25104;&#12290;&#35782;&#21035;&#24615;&#21035;&#20559;&#35265;&#20316;&#20026;&#19968;&#31181;&#26222;&#36941;&#30340;&#31038;&#20250;&#26500;&#36896;&#65292;&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#21457;&#29616;&#21644;&#37327;&#21270;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#23384;&#22312;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19977;&#20010;&#26469;&#33258;&#20998;&#31867;&#30340;&#33879;&#21517;&#38750;&#27495;&#35270;&#26631;&#20934;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#31867;&#27604;&#65292;&#21363;&#29420;&#31435;&#24615;&#12289;&#20998;&#31163;&#24615;&#21644;&#20805;&#20998;&#24615;&#12290;&#20026;&#20102;&#23637;&#31034;&#36825;&#20123;&#26631;&#20934;&#30340;&#20316;&#29992;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#38024;&#23545;&#27599;&#20010;&#26631;&#20934;&#30340;&#25552;&#31034;&#65292;&#37325;&#28857;&#20851;&#27880;&#32844;&#19994;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#65292;&#20855;&#20307;&#21033;&#29992;&#21307;&#23398;&#27979;&#35797;&#26469;&#22312;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#32972;&#26223;&#20013;&#24341;&#20837;&#22522;&#26412;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08564v1 Announce Type: cross  Abstract: Within recent years, generative AI, such as large language models, has undergone rapid development. As these models become increasingly available to the public, concerns arise about perpetuating and amplifying harmful biases in applications. Gender stereotypes can be harmful and limiting for the individuals they target, whether they consist of misrepresentation or discrimination. Recognizing gender bias as a pervasive societal construct, this paper studies how to uncover and quantify the presence of gender biases in generative language models. In particular, we derive generative AI analogues of three well-known non-discrimination criteria from classification, namely independence, separation and sufficiency. To demonstrate these criteria in action, we design prompts for each of the criteria with a focus on occupational gender stereotype, specifically utilizing the medical test to introduce the ground truth in the generative AI context. 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#29992;&#20110;&#29983;&#25104;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#25688;&#35201;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20581;&#24247;&#20445;&#20581;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#24182;&#25552;&#20986;&#30456;&#24212;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;</title><link>https://arxiv.org/abs/2403.05720</link><description>&lt;p&gt;
&#29992;&#20110;&#29983;&#25104;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#25688;&#35201;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
A Benchmark of Domain-Adapted Large Language Models for Generating Brief Hospital Course Summaries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05720
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#29992;&#20110;&#29983;&#25104;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#25688;&#35201;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20581;&#24247;&#20445;&#20581;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#24182;&#25552;&#20986;&#30456;&#24212;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#65288;BHC&#65289;&#25688;&#35201;&#26159;&#36890;&#36807;&#24635;&#32467;&#20020;&#24202;&#35760;&#24405;&#32780;&#29983;&#25104;&#30340;&#24120;&#35265;&#20020;&#24202;&#25991;&#20214;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#21160;&#21270;&#23454;&#38469;&#20219;&#21153;&#26041;&#38754;&#23637;&#29616;&#20986;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#21307;&#30103;&#24212;&#29992;&#65288;&#22914;BHC&#21512;&#25104;&#65289;&#20013;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#23637;&#31034;&#12290;&#20026;&#20102;&#20351;LLMs&#33021;&#22815;&#36866;&#24212;BHC&#21512;&#25104;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#21547;&#20174;MIMIC-IV&#35760;&#24405;&#20013;&#25552;&#21462;&#30340;&#32463;&#36807;&#39044;&#22788;&#29702;&#30340;&#25968;&#25454;&#38598;&#65292;&#23553;&#35013;&#20102;&#20020;&#24202;&#35760;&#24405;&#21644;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#65288;BHC&#65289;&#23545;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20004;&#20010;&#36890;&#29992;LLMs&#21644;&#19977;&#20010;&#21307;&#30103;&#39046;&#22495;&#36866;&#24212;&#30340;LLMs&#30340;&#24615;&#33021;&#65292;&#20197;&#25913;&#36827;&#20174;&#20020;&#24202;&#35760;&#24405;&#29983;&#25104;BHC&#12290;&#25105;&#20204;&#20351;&#29992;&#20020;&#24202;&#35760;&#24405;&#20316;&#20026;&#36755;&#20837;&#26469;&#29983;&#25104;BHC&#65292;&#37319;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;&#65288;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65289;&#21644;&#22522;&#20110;&#24494;&#35843;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#26469;&#24212;&#29992;&#20110;&#19977;&#20010;&#24320;&#28304;LLMs&#65288;Clinical-T5-Large&#65292;Llama2-13B&#65292;FLAN-UL2&#65289;&#21644;&#20004;&#20010;&#19987;&#26377;LLMs&#65288;GPT-3.5&#65292;GPT-4&#65289;&#12290;&#25105;&#20204;&#23450;&#37327;&#35780;&#20272;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05720v1 Announce Type: cross  Abstract: Brief hospital course (BHC) summaries are common clinical documents generated by summarizing clinical notes. While large language models (LLMs) depict remarkable capabilities in automating real-world tasks, their capabilities for healthcare applications such as BHC synthesis have not been shown. To enable the adaptation of LLMs for BHC synthesis, we introduce a novel benchmark consisting of a pre-processed dataset extracted from MIMIC-IV notes, encapsulating clinical note, and brief hospital course (BHC) pairs. We assess the performance of two general-purpose LLMs and three healthcare-adapted LLMs to improve BHC synthesis from clinical notes. Using clinical notes as input for generating BHCs, we apply prompting-based (using in-context learning) and fine-tuning-based adaptation strategies to three open-source LLMs (Clinical-T5-Large, Llama2-13B, FLAN-UL2) and two proprietary LLMs (GPT-3.5, GPT-4). We quantitatively evaluate the performa
&lt;/p&gt;</description></item><item><title>SheetAgent&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30005;&#23376;&#34920;&#26684;&#25512;&#29702;&#21644;&#25805;&#20316;&#30340;&#36890;&#29992;&#20195;&#29702;&#65292;&#25552;&#20379;&#20102;&#22788;&#29702;&#22797;&#26434;&#29616;&#23454;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;</title><link>https://arxiv.org/abs/2403.03636</link><description>&lt;p&gt;
SheetAgent&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30005;&#23376;&#34920;&#26684;&#25512;&#29702;&#21644;&#25805;&#20316;&#30340;&#36890;&#29992;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
SheetAgent: A Generalist Agent for Spreadsheet Reasoning and Manipulation via Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03636
&lt;/p&gt;
&lt;p&gt;
SheetAgent&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30005;&#23376;&#34920;&#26684;&#25512;&#29702;&#21644;&#25805;&#20316;&#30340;&#36890;&#29992;&#20195;&#29702;&#65292;&#25552;&#20379;&#20102;&#22788;&#29702;&#22797;&#26434;&#29616;&#23454;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#34920;&#26684;&#25805;&#20316;&#24191;&#27867;&#23384;&#22312;&#20110;&#22823;&#22810;&#25968;&#26085;&#24120;&#24037;&#20316;&#20013;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#24037;&#20316;&#25928;&#29575;&#12290;&#26368;&#36817;&#23581;&#35797;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#36827;&#34892;&#33258;&#21160;&#30005;&#23376;&#34920;&#26684;&#25805;&#20316;&#65292;&#20294;&#23578;&#26410;&#22312;&#23384;&#22312;&#25512;&#29702;&#25361;&#25112;&#30340;&#22797;&#26434;&#21644;&#29616;&#23454;&#20219;&#21153;&#20013;&#36827;&#34892;&#25506;&#31350;&#65288;&#20363;&#22914;&#65292;&#20855;&#26377;&#22810;&#27493;&#25512;&#29702;&#21644;&#27169;&#31946;&#35201;&#27714;&#30340;&#38271;&#35270;&#37326;&#25805;&#20316;&#65289;&#12290;&#20026;&#20102;&#24357;&#21512;&#19982;&#30495;&#23454;&#19990;&#30028;&#35201;&#27714;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;$\textbf{SheetRM}$&#65292;&#19968;&#20010;&#29305;&#28857;&#26159;&#38271;&#35270;&#37326;&#21644;&#22810;&#31867;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#20855;&#26377;&#25512;&#29702;&#30456;&#20851;&#25805;&#32437;&#65292;&#30001;&#30495;&#23454;&#25361;&#25112;&#24341;&#36215;&#12290;&#20026;&#20102;&#32531;&#35299;&#20197;&#19978;&#25361;&#25112;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;$\textbf{SheetAgent}$&#65292;&#19968;&#31181;&#21033;&#29992;LLMs&#33021;&#21147;&#30340;&#26032;&#22411;&#33258;&#20027;&#20195;&#29702;&#12290;SheetAgent&#30001;&#19977;&#20010;&#21327;&#20316;&#27169;&#22359;&#32452;&#25104;&#65306;$\textit{Planner}$&#12289;$\textit{Informer}$&#21644;$\textit{Retriever}$&#65292;&#23454;&#29616;&#20102;&#23545;&#30005;&#23376;&#34920;&#26684;&#30340;&#39640;&#32423;&#25512;&#29702;&#21644;&#20934;&#30830;&#25805;&#20316;&#65292;&#32780;&#19981;&#38656;&#20154;&#31867;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03636v1 Announce Type: new  Abstract: Spreadsheet manipulation is widely existing in most daily works and significantly improves working efficiency. Large language model (LLM) has been recently attempted for automatic spreadsheet manipulation but has not yet been investigated in complicated and realistic tasks where reasoning challenges exist (e.g., long horizon manipulation with multi-step reasoning and ambiguous requirements). To bridge the gap with the real-world requirements, we introduce $\textbf{SheetRM}$, a benchmark featuring long-horizon and multi-category tasks with reasoning-dependent manipulation caused by real-life challenges. To mitigate the above challenges, we further propose $\textbf{SheetAgent}$, a novel autonomous agent that utilizes the power of LLMs. SheetAgent consists of three collaborative modules: $\textit{Planner}$, $\textit{Informer}$, and $\textit{Retriever}$, achieving both advanced reasoning and accurate manipulation over spreadsheets without hu
&lt;/p&gt;</description></item><item><title>QuanTest&#26159;&#19968;&#20010;&#22522;&#20110;&#32416;&#32544;&#30340;&#23545;&#25239;&#27979;&#35797;&#26694;&#26550;&#65292;&#26088;&#22312;&#25581;&#31034;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#20013;&#30340;&#28508;&#22312;&#38169;&#35823;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.12950</link><description>&lt;p&gt;
QuanTest&#65306;&#22522;&#20110;&#32416;&#32544;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
QuanTest: Entanglement-Guided Testing of Quantum Neural Network Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12950
&lt;/p&gt;
&lt;p&gt;
QuanTest&#26159;&#19968;&#20010;&#22522;&#20110;&#32416;&#32544;&#30340;&#23545;&#25239;&#27979;&#35797;&#26694;&#26550;&#65292;&#26088;&#22312;&#25581;&#31034;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#20013;&#30340;&#28508;&#22312;&#38169;&#35823;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Quantum Neural Network&#65288;QNN&#65289;&#23558;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#21407;&#29702;&#19982;&#37327;&#23376;&#21147;&#23398;&#22522;&#26412;&#29702;&#35770;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20855;&#26377;&#37327;&#23376;&#21152;&#36895;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12290; &#26368;&#36817;&#21457;&#29616;&#65292;QNN&#31995;&#32479;&#26377;&#31867;&#20284;&#20110;&#32463;&#20856;DL&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#12290; &#24613;&#38656;&#19968;&#31181;&#26041;&#27861;&#26469;&#27979;&#35797;&#23427;&#20204;&#30340;&#27491;&#30830;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290; &#20294;&#26159;&#65292;QNN&#31995;&#32479;&#19982;&#20256;&#32479;&#37327;&#23376;&#36719;&#20214;&#21644;&#32463;&#20856;DL&#31995;&#32479;&#26377;&#24456;&#22823;&#19981;&#21516;&#65292;&#23545;QNN&#27979;&#35797;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290; &#36825;&#20123;&#25361;&#25112;&#21253;&#25324;&#20256;&#32479;&#37327;&#23376;&#36719;&#20214;&#27979;&#35797;&#26041;&#27861;&#30340;&#19981;&#36866;&#29992;&#24615;&#65292;&#37327;&#23376;&#27979;&#35797;&#26679;&#26412;&#29983;&#25104;&#23545;&#25200;&#21160;&#31639;&#23376;&#30340;&#20381;&#36182;&#65292;&#20197;&#21450;&#37327;&#23376;&#31070;&#32463;&#20803;&#20013;&#32570;&#20047;&#26377;&#25928;&#20449;&#24687;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;QuanTest&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#37327;&#23376;&#32416;&#32544;&#30340;&#23545;&#25239;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#21457;&#29616;QNN&#31995;&#32479;&#20013;&#28508;&#22312;&#30340;&#38169;&#35823;&#34892;&#20026;&#12290; &#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#37327;&#23376;&#32416;&#32544;&#20805;&#20998;&#24615;&#20934;&#21017;&#26469;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12950v1 Announce Type: cross  Abstract: Quantum Neural Network (QNN) combines the Deep Learning (DL) principle with the fundamental theory of quantum mechanics to achieve machine learning tasks with quantum acceleration. Recently, QNN systems have been found to manifest robustness issues similar to classical DL systems. There is an urgent need for ways to test their correctness and security. However, QNN systems differ significantly from traditional quantum software and classical DL systems, posing critical challenges for QNN testing. These challenges include the inapplicability of traditional quantum software testing methods, the dependence of quantum test sample generation on perturbation operators, and the absence of effective information in quantum neurons. In this paper, we propose QuanTest, a quantum entanglement-guided adversarial testing framework to uncover potential erroneous behaviors in QNN systems. We design a quantum entanglement adequacy criterion to quantify 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BiTimelyGPT&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21452;&#21521;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#23398;&#20064;&#34920;&#31034;&#65292;&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#21487;&#29992;&#20110;&#31070;&#32463;&#21151;&#33021;&#39044;&#27979;&#12289;&#30142;&#30149;&#35786;&#26029;&#21644;&#29983;&#29702;&#30149;&#24449;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.09558</link><description>&lt;p&gt;
&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#30340;&#21452;&#21521;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Bidirectional Generative Pre-training for Improving Time Series Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09558
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BiTimelyGPT&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21452;&#21521;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#23398;&#20064;&#34920;&#31034;&#65292;&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#21487;&#29992;&#20110;&#31070;&#32463;&#21151;&#33021;&#39044;&#27979;&#12289;&#30142;&#30149;&#35786;&#26029;&#21644;&#29983;&#29702;&#30149;&#24449;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#20197;&#29992;&#20110;&#21028;&#21035;&#20219;&#21153;&#19968;&#30452;&#26159;&#19968;&#39033;&#38271;&#26399;&#30340;&#25361;&#25112;&#12290;&#24403;&#21069;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#35201;&#20040;&#26159;&#21333;&#21521;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#65292;&#35201;&#20040;&#26159;&#38543;&#26426;&#23631;&#34109;&#26631;&#35760;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;&#21452;&#21521;&#21450;&#26102;&#29983;&#25104;&#39044;&#35757;&#32451;Transformer&#65288;BiTimelyGPT&#65289;&#65292;&#23427;&#36890;&#36807;&#20132;&#26367;&#30340;Transformer&#23618;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#19979;&#19968;&#20010;&#26631;&#35760;&#21644;&#19978;&#19968;&#20010;&#26631;&#35760;&#30340;&#39044;&#27979;&#12290;&#36825;&#31181;&#39044;&#35757;&#32451;&#20219;&#21153;&#20445;&#30041;&#20102;&#26102;&#38388;&#24207;&#21015;&#30340;&#21407;&#22987;&#20998;&#24067;&#21644;&#25968;&#25454;&#24418;&#29366;&#12290;&#27492;&#22806;&#65292;&#20840;&#31209;&#21069;&#21521;&#21644;&#21518;&#21521;&#27880;&#24847;&#21147;&#30697;&#38453;&#20855;&#26377;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290; &#20351;&#29992;&#29983;&#29289;&#20449;&#21495;&#25968;&#25454;&#65292;BiTimelyGPT&#22312;&#39044;&#27979;&#31070;&#32463;&#21151;&#33021;&#12289;&#30142;&#30149;&#35786;&#26029;&#21644;&#29983;&#29702;&#30149;&#24449;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;&#36890;&#36807;&#21487;&#35270;&#21270;&#27880;&#24847;&#21147;&#28909;&#22270;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#39044;&#35757;&#32451;&#30340;BiTimelyGPT&#33021;&#22815;&#20174;&#26102;&#38388;&#24207;&#21015;&#20013;&#35782;&#21035;&#20986;&#20855;&#26377;&#21028;&#21035;&#24615;&#30340;&#29255;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09558v1 Announce Type: new  Abstract: Learning time-series representations for discriminative tasks has been a long-standing challenge. Current pre-training methods are limited in either unidirectional next-token prediction or randomly masked token prediction. We propose a novel architecture called Bidirectional Timely Generative Pre-trained Transformer (BiTimelyGPT), which pre-trains on time-series data by both next-token and previous-token predictions in alternating transformer layers. This pre-training task preserves original distribution and data shapes of the time-series. Additionally, the full-rank forward and backward attention matrices exhibit more expressive representation capabilities. Using biosignal data, BiTimelyGPT demonstrates superior performance in predicting neurological functionality, disease diagnosis, and physiological signs. By visualizing the attention heatmap, we observe that the pre-trained BiTimelyGPT can identify discriminative segments from time-s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#24403;&#23398;&#20064;&#36895;&#29575;&#25353;&#29031;&#36870;&#26102;&#38388;&#34928;&#20943;&#35268;&#21017;&#26102;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#24212;&#29992;&#20110;&#20462;&#25913;&#30340;&#24102;&#26377;L2&#27491;&#21017;&#21270;&#30340;&#31574;&#30053;&#26799;&#24230;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MAB&#65289;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.06388</link><description>&lt;p&gt;
&#20851;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;&#21450;&#20854;&#22312;&#20462;&#25913;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#19978;&#30340;&#31574;&#30053;&#26799;&#24230;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Convergence Rate of the Stochastic Gradient Descent (SGD) and application to a modified policy gradient for the Multi Armed Bandit
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06388
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#24403;&#23398;&#20064;&#36895;&#29575;&#25353;&#29031;&#36870;&#26102;&#38388;&#34928;&#20943;&#35268;&#21017;&#26102;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#24212;&#29992;&#20110;&#20462;&#25913;&#30340;&#24102;&#26377;L2&#27491;&#21017;&#21270;&#30340;&#31574;&#30053;&#26799;&#24230;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MAB&#65289;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21253;&#21547;&#30340;&#35777;&#26126;&#65292;&#35777;&#26126;&#20102;&#24403;&#23398;&#20064;&#36895;&#29575;&#36981;&#24490;&#36870;&#26102;&#38388;&#34928;&#20943;&#35268;&#21017;&#26102;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;&#65307;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#32467;&#26524;&#24212;&#29992;&#20110;&#24102;&#26377;L2&#27491;&#21017;&#21270;&#30340;&#20462;&#25913;&#30340;&#31574;&#30053;&#26799;&#24230;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MAB&#65289;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a self-contained proof of the convergence rate of the Stochastic Gradient Descent (SGD) when the learning rate follows an inverse time decays schedule; we next apply the results to the convergence of a modified form of policy gradient Multi-Armed Bandit (MAB) with $L2$ regularization.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#21327;&#20316;&#30340;&#20154;&#24037;&#26234;&#33021;-&#20154;&#31867;&#28151;&#21512;&#22242;&#38431;&#25480;&#26435;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;AI&#32463;&#29702;&#65288;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65289;&#20316;&#20026;&#22242;&#38431;&#30340;&#22806;&#37096;&#35266;&#23519;&#32773;&#65292;&#23398;&#20064;&#22242;&#38431;&#20195;&#29702;&#20154;&#30340;&#34892;&#20026;&#27169;&#22411;&#24182;&#36873;&#25321;&#26368;&#20339;&#30340;&#25511;&#21046;&#20195;&#29702;&#20154;&#12290;</title><link>https://arxiv.org/abs/2402.05605</link><description>&lt;p&gt;
&#20248;&#21270;&#21327;&#20316;&#30340;&#20154;&#24037;&#26234;&#33021;-&#20154;&#31867;&#28151;&#21512;&#22242;&#38431;&#20013;&#30340;&#25480;&#26435;
&lt;/p&gt;
&lt;p&gt;
Optimizing Delegation in Collaborative Human-AI Hybrid Teams
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#21327;&#20316;&#30340;&#20154;&#24037;&#26234;&#33021;-&#20154;&#31867;&#28151;&#21512;&#22242;&#38431;&#25480;&#26435;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;AI&#32463;&#29702;&#65288;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65289;&#20316;&#20026;&#22242;&#38431;&#30340;&#22806;&#37096;&#35266;&#23519;&#32773;&#65292;&#23398;&#20064;&#22242;&#38431;&#20195;&#29702;&#20154;&#30340;&#34892;&#20026;&#27169;&#22411;&#24182;&#36873;&#25321;&#26368;&#20339;&#30340;&#25511;&#21046;&#20195;&#29702;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20154;&#31867;&#21644;&#33258;&#20027;&#31995;&#32479;&#20316;&#20026;&#28151;&#21512;&#22242;&#38431;&#20849;&#21516;&#36816;&#20316;&#26102;&#65292;&#25105;&#20204;&#24076;&#26395;&#30830;&#20445;&#22242;&#38431;&#30340;&#25104;&#21151;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#23558;&#22242;&#38431;&#25104;&#21592;&#31216;&#20026;&#20195;&#29702;&#20154;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#28151;&#21512;&#22242;&#38431;&#30340;&#24773;&#20917;&#65292;&#21363;&#22312;&#20219;&#20309;&#26102;&#20505;&#65292;&#21482;&#26377;&#19968;&#20010;&#22242;&#38431;&#25104;&#21592;&#65288;&#25511;&#21046;&#20195;&#29702;&#20154;&#65289;&#34987;&#25480;&#26435;&#20026;&#22242;&#38431;&#30340;&#25511;&#21046;&#32773;&#12290;&#20026;&#20102;&#30830;&#23450;&#26368;&#20339;&#30340;&#25511;&#21046;&#20195;&#29702;&#20154;&#36873;&#25321;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24341;&#20837;AI&#32463;&#29702;&#65288;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65289;&#30340;&#24819;&#27861;&#65292;&#35813;&#32463;&#29702;&#20316;&#20026;&#22242;&#38431;&#30340;&#22806;&#37096;&#35266;&#23519;&#32773;&#23398;&#20064;&#12290;&#32463;&#29702;&#36890;&#36807;&#35266;&#23519;&#20195;&#29702;&#20154;&#30340;&#34920;&#29616;&#21644;&#22242;&#38431;&#25152;&#22788;&#30340;&#29615;&#22659;/&#19990;&#30028;&#26469;&#23398;&#20064;&#34892;&#20026;&#27169;&#22411;&#65292;&#24182;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#36873;&#25321;&#20986;&#26368;&#29702;&#24819;&#30340;&#25511;&#21046;&#20195;&#29702;&#20154;&#12290;&#20026;&#20102;&#38480;&#23450;&#32463;&#29702;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#32452;&#32422;&#26463;&#26465;&#20214;&#12290;&#32463;&#29702;&#30340;&#32422;&#26463;&#26465;&#20214;&#25351;&#31034;&#22242;&#38431;&#30340;&#21487;&#25509;&#21463;&#36816;&#20316;&#26041;&#24335;&#65292;&#22240;&#27492;&#22914;&#26524;&#22242;&#38431;&#36827;&#20837;&#19981;&#21487;&#25509;&#21463;&#24182;&#38656;&#35201;&#32463;&#29702;&#20171;&#20837;&#30340;&#29366;&#24577;&#65292;&#23601;&#20250;&#36829;&#21453;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
When humans and autonomous systems operate together as what we refer to as a hybrid team, we of course wish to ensure the team operates successfully and effectively. We refer to team members as agents. In our proposed framework, we address the case of hybrid teams in which, at any time, only one team member (the control agent) is authorized to act as control for the team. To determine the best selection of a control agent, we propose the addition of an AI manager (via Reinforcement Learning) which learns as an outside observer of the team. The manager learns a model of behavior linking observations of agent performance and the environment/world the team is operating in, and from these observations makes the most desirable selection of a control agent. We restrict the manager task by introducing a set of constraints. The manager constraints indicate acceptable team operation, so a violation occurs if the team enters a condition which is unacceptable and requires manager intervention. To
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;MetaTree&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#32463;&#20856;&#31639;&#27861;&#30340;&#36755;&#20986;&#35757;&#32451;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#20855;&#26377;&#24378;&#22823;&#27010;&#25324;&#24615;&#33021;&#30340;&#20915;&#31574;&#26641;&#12290;</title><link>https://arxiv.org/abs/2402.03774</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#23398;&#20064;&#20915;&#31574;&#26641;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning a Decision Tree Algorithm with Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03774
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;MetaTree&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#32463;&#20856;&#31639;&#27861;&#30340;&#36755;&#20986;&#35757;&#32451;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#20855;&#26377;&#24378;&#22823;&#27010;&#25324;&#24615;&#33021;&#30340;&#20915;&#31574;&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#22240;&#20854;&#21487;&#35299;&#37322;&#24615;&#21644;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#23454;&#29616;&#39640;&#39044;&#27979;&#24615;&#33021;&#32780;&#38395;&#21517;&#12290;&#20256;&#32479;&#19978;&#65292;&#20915;&#31574;&#26641;&#26159;&#36890;&#36807;&#36882;&#24402;&#31639;&#27861;&#26500;&#24314;&#30340;&#65292;&#22312;&#26641;&#30340;&#27599;&#20010;&#33410;&#28857;&#19978;&#23558;&#25968;&#25454;&#36827;&#34892;&#20998;&#21306;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;&#26368;&#20339;&#20998;&#21306;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#38024;&#23545;&#23616;&#37096;&#27573;&#20248;&#21270;&#30340;&#20915;&#31574;&#26641;&#21487;&#33021;&#26080;&#27861;&#24102;&#26469;&#20840;&#23616;&#27010;&#25324;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MetaTree&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#32463;&#20856;&#31639;&#27861;&#30340;&#36807;&#28388;&#36755;&#20986;&#26469;&#35757;&#32451;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#24378;&#22823;&#30340;&#20998;&#31867;&#20915;&#31574;&#26641;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#22823;&#37327;&#25968;&#25454;&#38598;&#19978;&#25311;&#21512;&#36138;&#23146;&#20915;&#31574;&#26641;&#21644;&#20248;&#21270;&#20915;&#31574;&#26641;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35757;&#32451;MetaTree&#20135;&#29983;&#20855;&#26377;&#24378;&#22823;&#27010;&#25324;&#24615;&#33021;&#30340;&#20915;&#31574;&#26641;&#12290;&#36825;&#31181;&#35757;&#32451;&#20351;MetaTree&#19981;&#20165;&#21487;&#20197;&#27169;&#25311;&#36825;&#20123;&#31639;&#27861;&#65292;&#36824;&#21487;&#20197;&#26681;&#25454;&#19978;&#19979;&#25991;&#26234;&#33021;&#22320;&#35843;&#25972;&#31574;&#30053;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#24378;&#30340;&#27010;&#25324;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision trees are renowned for their interpretability capability to achieve high predictive performance, especially on tabular data. Traditionally, they are constructed through recursive algorithms, where they partition the data at every node in a tree. However, identifying the best partition is challenging, as decision trees optimized for local segments may not bring global generalization. To address this, we introduce MetaTree, which trains a transformer-based model on filtered outputs from classical algorithms to produce strong decision trees for classification. Specifically, we fit both greedy decision trees and optimized decision trees on a large number of datasets. We then train MetaTree to produce the trees that achieve strong generalization performance. This training enables MetaTree to not only emulate these algorithms, but also to intelligently adapt its strategy according to the context, thereby achieving superior generalization performance.
&lt;/p&gt;</description></item><item><title>DefInt&#25552;&#20986;&#20102;&#19968;&#31181;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;&#65292;&#36890;&#36807;&#40664;&#35748;&#20351;&#29992;&#36739;&#23567;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#24605;&#36335;&#65292;&#28982;&#21518;&#36890;&#36807;&#21453;&#24605;&#25512;&#29702;&#24178;&#39044;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#28151;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02563</link><description>&lt;p&gt;
DefInt&#65306;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#22788;&#29702;&#28151;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DefInt: A Default-interventionist Framework for Efficient Reasoning with Hybrid Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02563
&lt;/p&gt;
&lt;p&gt;
DefInt&#25552;&#20986;&#20102;&#19968;&#31181;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;&#65292;&#36890;&#36807;&#40664;&#35748;&#20351;&#29992;&#36739;&#23567;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#24605;&#36335;&#65292;&#28982;&#21518;&#36890;&#36807;&#21453;&#24605;&#25512;&#29702;&#24178;&#39044;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#28151;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26032;&#33021;&#21147;&#65292;&#20294;&#22312;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#22914;&#36830;&#38145;&#25512;&#29702;&#65288;CoT&#65289;&#21644;&#24605;&#32500;&#26641;&#65288;ToT&#65289;&#20027;&#35201;&#20851;&#27880;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#24573;&#35270;&#20102;&#19981;&#26029;&#22686;&#21152;&#30340;&#26631;&#35760;&#25104;&#26412;&#65292;&#36825;&#23545;&#20110;&#20855;&#26377;&#24040;&#22823;&#35299;&#31354;&#38388;&#30340;&#24320;&#25918;&#24615;&#23454;&#38469;&#20219;&#21153;&#26469;&#35828;&#21487;&#33021;&#29305;&#21035;&#38382;&#39064;&#12290;&#21463;&#20154;&#31867;&#35748;&#30693;&#30340;&#21452;&#36807;&#31243;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;&#65288;DefInt&#65289;&#65292;&#20197;&#37322;&#25918;&#28151;&#21512;LLMs&#30340;&#21327;&#21516;&#28508;&#21147;&#12290;&#40664;&#35748;&#24773;&#20917;&#19979;&#65292;DefInt&#20351;&#29992;&#36739;&#23567;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20302;&#25104;&#26412;&#30340;&#25512;&#29702;&#24605;&#36335;&#65292;&#31867;&#20284;&#20110;&#8220;&#31995;&#32479;1&#8221;&#20135;&#29983;&#30340;&#24555;&#36895;&#30452;&#35273;&#12290;&#22914;&#26524;&#36825;&#20123;&#30452;&#35273;&#34987;&#35748;&#20026;&#20302;&#32622;&#20449;&#24230;&#65292;&#21017;DefInt&#23558;&#35843;&#29992;&#25918;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#24605;&#25512;&#29702;&#20316;&#20026;&#8220;&#31995;&#32479;2&#8221;&#30340;&#24178;&#39044;&#65292;&#21487;&#20197;&#35206;&#30422;&#40664;&#35748;&#24605;&#32771;&#24182;&#32416;&#27491;&#25512;&#29702;&#36807;&#31243;&#12290;&#23454;&#39564;&#22312;&#20116;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;DefInt&#35770;&#25991;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown impressive emergent abilities in a wide range of tasks, but still face challenges in handling complex reasoning problems. Previous works like chain-of-thought (CoT) and tree-of-thoughts(ToT) have predominately focused on enhancing accuracy, but overlook the rapidly increasing token cost, which could be particularly problematic for open-ended real-world tasks with huge solution spaces. Motivated by the dual process theory of human cognition, we propose a Default-Interventionist framework (DefInt) to unleash the synergistic potential of hybrid LLMs. By default, DefInt uses smaller-scale language models to generate low-cost reasoning thoughts, which resembles the fast intuitions produced by System 1. If the intuitions are considered with low confidence, DefInt will invoke the reflective reasoning of scaled-up language models as the intervention of System 2, which can override the default thoughts and rectify the reasoning process. Experiments on fiv
&lt;/p&gt;</description></item><item><title>Monkey&#36890;&#36807;&#25552;&#39640;&#22270;&#20687;&#20998;&#36776;&#29575;&#21644;&#37319;&#29992;&#22810;&#32423;&#25551;&#36848;&#29983;&#25104;&#26041;&#27861;&#26469;&#22686;&#24378;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMMs)&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#35814;&#32454;&#30340;&#35270;&#35273;&#25429;&#25417;&#21644;&#26356;&#26377;&#25928;&#30340;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2311.06607</link><description>&lt;p&gt;
Monkey: &#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#22270;&#20687;&#20998;&#36776;&#29575;&#21644;&#25991;&#26412;&#26631;&#31614;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06607
&lt;/p&gt;
&lt;p&gt;
Monkey&#36890;&#36807;&#25552;&#39640;&#22270;&#20687;&#20998;&#36776;&#29575;&#21644;&#37319;&#29992;&#22810;&#32423;&#25551;&#36848;&#29983;&#25104;&#26041;&#27861;&#26469;&#22686;&#24378;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMMs)&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#35814;&#32454;&#30340;&#35270;&#35273;&#25429;&#25417;&#21644;&#26356;&#26377;&#25928;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMMs)&#22312;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#22312;&#39640;&#20998;&#36776;&#29575;&#36755;&#20837;&#21644;&#35814;&#32454;&#22330;&#26223;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Monkey&#26469;&#22686;&#24378;LMM&#30340;&#33021;&#21147;&#12290;&#39318;&#20808;&#65292;Monkey&#36890;&#36807;&#23558;&#36755;&#20837;&#22270;&#20687;&#21010;&#20998;&#20026;&#32479;&#19968;&#30340;&#34917;&#19969;&#26469;&#22788;&#29702;&#22270;&#20687;&#65292;&#27599;&#20010;&#34917;&#19969;&#30340;&#22823;&#23567;&#19982;&#21407;&#26469;&#35757;&#32451;&#33391;&#22909;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#20351;&#29992;&#30340;&#22823;&#23567;(&#20363;&#22914;448x448)&#30456;&#21305;&#37197;&#12290;&#37197;&#22791;&#20102;&#27599;&#20010;&#34917;&#19969;&#30340;&#36866;&#37197;&#22120;&#65292;Monkey&#21487;&#20197;&#22788;&#29702;&#39640;&#36798;1344x896&#20687;&#32032;&#30340;&#26356;&#39640;&#20998;&#36776;&#29575;&#65292;&#23454;&#29616;&#23545;&#22797;&#26434;&#35270;&#35273;&#20449;&#24687;&#30340;&#35814;&#32454;&#25429;&#25417;&#12290;&#20854;&#27425;&#65292;&#23427;&#37319;&#29992;&#22810;&#32423;&#25551;&#36848;&#29983;&#25104;&#26041;&#27861;&#65292;&#20016;&#23500;&#20102;&#22330;&#26223;-&#23545;&#35937;&#20851;&#32852;&#30340;&#19978;&#19979;&#25991;&#12290;&#36825;&#31181;&#20004;&#37096;&#20998;&#31574;&#30053;&#30830;&#20445;&#20102;&#20174;&#29983;&#25104;&#25968;&#25454;&#20013;&#26356;&#26377;&#25928;&#30340;&#23398;&#20064;&#65306;&#26356;&#39640;&#30340;&#20998;&#36776;&#29575;&#20801;&#35768;&#23545;&#35270;&#35273;&#36827;&#34892;&#26356;&#35814;&#32454;&#30340;&#25429;&#25417;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#20840;&#38754;&#25551;&#36848;&#30340;&#25928;&#26524;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.06607v3 Announce Type: replace-cross  Abstract: Large Multimodal Models (LMMs) have shown promise in vision-language tasks but struggle with high-resolution input and detailed scene understanding. Addressing these challenges, we introduce Monkey to enhance LMM capabilities. Firstly, Monkey processes input images by dividing them into uniform patches, each matching the size (e.g., 448x448) used in the original training of the well-trained vision encoder. Equipped with individual adapter for each patch, Monkey can handle higher resolutions up to 1344x896 pixels, enabling the detailed capture of complex visual information. Secondly, it employs a multi-level description generation method, enriching the context for scene-object associations. This two-part strategy ensures more effective learning from generated data: the higher resolution allows for a more detailed capture of visuals, which in turn enhances the effectiveness of comprehensive descriptions. Extensive ablative result
&lt;/p&gt;</description></item><item><title>TabRepo&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;1310&#20010;&#27169;&#22411;&#22312;200&#20010;&#20998;&#31867;&#21644;&#22238;&#24402;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#39044;&#27979;&#32467;&#26524;&#21644;&#25351;&#26631;&#20449;&#24687;&#30340;&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#36827;&#34892;&#36229;&#21442;&#25968;&#20248;&#21270;&#12289;AutoML&#31995;&#32479;&#27604;&#36739;&#21644;&#36801;&#31227;&#23398;&#20064;&#31561;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2311.02971</link><description>&lt;p&gt;
TabRepo&#65306;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#34920;&#26684;&#27169;&#22411;&#35780;&#20272;&#24211;&#21450;&#20854;AutoML&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
TabRepo: A Large Scale Repository of Tabular Model Evaluations and its AutoML Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02971
&lt;/p&gt;
&lt;p&gt;
TabRepo&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;1310&#20010;&#27169;&#22411;&#22312;200&#20010;&#20998;&#31867;&#21644;&#22238;&#24402;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#39044;&#27979;&#32467;&#26524;&#21644;&#25351;&#26631;&#20449;&#24687;&#30340;&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#36827;&#34892;&#36229;&#21442;&#25968;&#20248;&#21270;&#12289;AutoML&#31995;&#32479;&#27604;&#36739;&#21644;&#36801;&#31227;&#23398;&#20064;&#31561;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;TabRepo&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;1310&#20010;&#27169;&#22411;&#22312;200&#20010;&#20998;&#31867;&#21644;&#22238;&#24402;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#39044;&#27979;&#32467;&#26524;&#21644;&#25351;&#26631;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#30340;&#22810;&#31181;&#22909;&#22788;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#34920;&#26126;&#36825;&#20010;&#25968;&#25454;&#38598;&#21487;&#20197;&#36827;&#34892;&#35832;&#22914;&#27604;&#36739;&#36229;&#21442;&#25968;&#20248;&#21270;&#19982;&#24403;&#21069;AutoML&#31995;&#32479;&#20197;&#21450;&#22312;&#20351;&#29992;&#39044;&#35745;&#31639;&#27169;&#22411;&#39044;&#27979;&#30340;&#21516;&#26102;&#32771;&#34385;&#38598;&#25104;&#30340;&#20998;&#26512;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#36731;&#26494;&#29992;&#20110;&#25191;&#34892;&#36801;&#31227;&#23398;&#20064;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#24212;&#29992;&#26631;&#20934;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#22312;&#20934;&#30830;&#24615;&#12289;&#36816;&#34892;&#26102;&#38388;&#21644;&#24310;&#36831;&#26041;&#38754;&#32988;&#36807;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#34920;&#26684;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02971v2 Announce Type: replace-cross  Abstract: We introduce TabRepo, a new dataset of tabular model evaluations and predictions. TabRepo contains the predictions and metrics of 1310 models evaluated on 200 classification and regression datasets. We illustrate the benefit of our dataset in multiple ways. First, we show that it allows to perform analysis such as comparing Hyperparameter Optimization against current AutoML systems while also considering ensembling at marginal cost by using precomputed model predictions. Second, we show that our dataset can be readily leveraged to perform transfer-learning. In particular, we show that applying standard transfer-learning techniques allows to outperform current state-of-the-art tabular systems in accuracy, runtime and latency.
&lt;/p&gt;</description></item><item><title>LLM&#20195;&#29702;&#22312;&#25293;&#21334;&#31454;&#25216;&#22330;&#23637;&#31034;&#20986;&#20102;&#20851;&#38190;&#30340;&#35268;&#21010;&#21644;&#25191;&#34892;&#25216;&#33021;&#65292;&#36825;&#20026;&#24314;&#27169;&#22797;&#26434;&#31038;&#20250;&#20114;&#21160;&#22312;&#31454;&#20105;&#32972;&#26223;&#19979;&#30340;LLMs&#28508;&#21147;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2310.05746</link><description>&lt;p&gt;
&#35753;&#34892;&#21160;&#32988;&#20110;&#38596;&#36777;&#65306;&#35780;&#20272;LLM&#20195;&#29702;&#22312;&#25293;&#21334;&#31454;&#25216;&#22330;&#20013;&#30340;&#25112;&#30053;&#35268;&#21010;&#19982;&#25191;&#34892;
&lt;/p&gt;
&lt;p&gt;
Put Your Money Where Your Mouth Is: Evaluating Strategic Planning and Execution of LLM Agents in an Auction Arena
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05746
&lt;/p&gt;
&lt;p&gt;
LLM&#20195;&#29702;&#22312;&#25293;&#21334;&#31454;&#25216;&#22330;&#23637;&#31034;&#20986;&#20102;&#20851;&#38190;&#30340;&#35268;&#21010;&#21644;&#25191;&#34892;&#25216;&#33021;&#65292;&#36825;&#20026;&#24314;&#27169;&#22797;&#26434;&#31038;&#20250;&#20114;&#21160;&#22312;&#31454;&#20105;&#32972;&#26223;&#19979;&#30340;LLMs&#28508;&#21147;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#23637;&#31034;&#20102;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#28982;&#32780;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#35780;&#20272;&#36890;&#24120;&#20381;&#36182;&#20110;&#38745;&#24577;&#22522;&#20934;&#12290;&#35780;&#20272;&#36825;&#19968;&#28857;&#38656;&#35201;&#27979;&#35797;&#25112;&#30053;&#25512;&#29702;&#33021;&#21147;&#30340;&#29615;&#22659;&#65292;&#36825;&#31181;&#29615;&#22659;&#38656;&#35201;&#22312;&#21160;&#24577;&#30340;&#31454;&#20105;&#22330;&#26223;&#20013;&#36827;&#34892;&#38271;&#26399;&#35268;&#21010;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;AucArena&#65292;&#36825;&#26159;&#19968;&#20010;&#27169;&#25311;&#25293;&#21334;&#30340;&#26032;&#39062;&#35780;&#20272;&#22871;&#20214;&#65292;&#36873;&#25321;&#36825;&#20010;&#35774;&#32622;&#26159;&#22240;&#20026;&#23427;&#38750;&#24120;&#19981;&#21487;&#39044;&#27979;&#65292;&#28041;&#21450;&#19982;&#36164;&#28304;&#21644;&#39118;&#38505;&#31649;&#29702;&#30456;&#20851;&#30340;&#35768;&#22810;&#25216;&#33021;&#65292;&#21516;&#26102;&#20063;&#26131;&#20110;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;LLM&#39537;&#21160;&#31454;&#26631;&#20195;&#29702;&#30340;&#21463;&#25511;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;&#20182;&#20204;&#30340;&#35268;&#21010;&#21644;&#25191;&#34892;&#25216;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35832;&#22914;GPT-4&#20043;&#31867;&#30340;LLM&#20855;&#26377;&#25293;&#21334;&#21442;&#19982;&#30340;&#20851;&#38190;&#25216;&#33021;&#65292;&#22914;&#39044;&#31639;&#31649;&#29702;&#21644;&#30446;&#26631;&#36981;&#20174;&#65292;&#36825;&#20123;&#25216;&#33021;&#20250;&#38543;&#30528;&#33258;&#36866;&#24212;&#31574;&#30053;&#30340;&#25913;&#36827;&#32780;&#25552;&#39640;&#12290;&#36825;&#31361;&#20986;&#20102;LLM&#22312;&#24314;&#27169;&#31454;&#25216;&#32972;&#26223;&#19979;&#30340;&#22797;&#26434;&#31038;&#20250;&#20114;&#21160;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05746v2 Announce Type: replace-cross  Abstract: Recent advancements in Large Language Models (LLMs) showcase advanced reasoning, yet NLP evaluations often depend on static benchmarks. Evaluating this necessitates environments that test strategic reasoning in dynamic, competitive scenarios requiring long-term planning. We introduce AucArena, a novel evaluation suite that simulates auctions, a setting chosen for being highly unpredictable and involving many skills related to resource and risk management, while also being easy to evaluate. We conduct controlled experiments using state-of-the-art LLMs to power bidding agents to benchmark their planning and execution skills. Our research demonstrates that LLMs, such as GPT-4, possess key skills for auction participation, such as budget management and goal adherence, which improve with adaptive strategies. This highlights LLMs' potential in modeling complex social interactions in competitive contexts. However, variability in LLM p
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21019;&#24314;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21452;&#36275;&#26426;&#22120;&#20154;&#21160;&#24577;&#36816;&#21160;&#25511;&#21046;&#22120;&#65292;&#35813;&#25511;&#21046;&#22120;&#21487;&#20197;&#24212;&#29992;&#20110;&#22810;&#31181;&#21160;&#24577;&#21452;&#36275;&#25216;&#33021;&#65292;&#24182;&#19988;&#22312;&#27169;&#25311;&#29615;&#22659;&#21644;&#23454;&#38469;&#29615;&#22659;&#20013;&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.16889</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#22810;&#21151;&#33021;&#12289;&#21160;&#24577;&#21644;&#31283;&#20581;&#30340;&#21452;&#36275;&#36816;&#21160;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning for Versatile, Dynamic, and Robust Bipedal Locomotion Control. (arXiv:2401.16889v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21019;&#24314;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21452;&#36275;&#26426;&#22120;&#20154;&#21160;&#24577;&#36816;&#21160;&#25511;&#21046;&#22120;&#65292;&#35813;&#25511;&#21046;&#22120;&#21487;&#20197;&#24212;&#29992;&#20110;&#22810;&#31181;&#21160;&#24577;&#21452;&#36275;&#25216;&#33021;&#65292;&#24182;&#19988;&#22312;&#27169;&#25311;&#29615;&#22659;&#21644;&#23454;&#38469;&#29615;&#22659;&#20013;&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#20851;&#20110;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21019;&#24314;&#21452;&#36275;&#26426;&#22120;&#20154;&#21160;&#24577;&#36816;&#21160;&#25511;&#21046;&#22120;&#30340;&#32508;&#21512;&#30740;&#31350;&#12290;&#25105;&#20204;&#19981;&#20165;&#20165;&#19987;&#27880;&#20110;&#21333;&#19968;&#30340;&#36816;&#21160;&#25216;&#33021;&#65292;&#32780;&#26159;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25511;&#21046;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#29992;&#20110;&#19968;&#31995;&#21015;&#21160;&#24577;&#21452;&#36275;&#25216;&#33021;&#65292;&#20174;&#21608;&#26399;&#24615;&#34892;&#36208;&#21644;&#22868;&#36305;&#21040;&#38750;&#21608;&#26399;&#24615;&#36339;&#36291;&#21644;&#31449;&#31435;&#12290;&#25105;&#20204;&#22522;&#20110;RL&#30340;&#25511;&#21046;&#22120;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#21382;&#21490;&#26550;&#26500;&#65292;&#21033;&#29992;&#26426;&#22120;&#20154;&#30340;&#38271;&#26399;&#21644;&#30701;&#26399;&#36755;&#20837;/&#36755;&#20986;&#65288;I/O&#65289;&#21382;&#21490;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#31471;&#21040;&#31471;RL&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#36825;&#31181;&#25511;&#21046;&#26550;&#26500;&#22312;&#27169;&#25311;&#29615;&#22659;&#21644;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#22810;&#26679;&#21270;&#25216;&#33021;&#19978;&#22987;&#32456;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;&#35813;&#30740;&#31350;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;&#25152;&#25552;&#20986;&#30340;RL&#31995;&#32479;&#22312;&#24320;&#21457;&#36816;&#21160;&#25511;&#21046;&#22120;&#26041;&#38754;&#24341;&#20837;&#30340;&#36866;&#24212;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#21487;&#20197;&#36866;&#24212;&#26102;&#38388;&#19981;&#21464;&#30340;&#21160;&#21147;&#23398;&#21464;&#21270;&#21644;&#26102;&#38388;&#21464;&#21270;&#30340;&#21464;&#21270;&#65292;&#22914;&#25509;&#35302;&#20107;&#20214;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive study on using deep reinforcement learning (RL) to create dynamic locomotion controllers for bipedal robots. Going beyond focusing on a single locomotion skill, we develop a general control solution that can be used for a range of dynamic bipedal skills, from periodic walking and running to aperiodic jumping and standing. Our RL-based controller incorporates a novel dual-history architecture, utilizing both a long-term and short-term input/output (I/O) history of the robot. This control architecture, when trained through the proposed end-to-end RL approach, consistently outperforms other methods across a diverse range of skills in both simulation and the real world.The study also delves into the adaptivity and robustness introduced by the proposed RL system in developing locomotion controllers. We demonstrate that the proposed architecture can adapt to both time-invariant dynamics shifts and time-variant changes, such as contact events, by effectivel
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#30456;&#26426;&#38519;&#38449;&#22270;&#20687;&#30340;&#32467;&#26500;&#21270;&#19978;&#19979;&#25991;&#65292;&#25552;&#39640;&#20854;&#22312;&#29289;&#31181;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#21644;&#27867;&#21270;&#33021;&#21147;&#22686;&#24378;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.00608</link><description>&lt;p&gt;
&#23558;&#19978;&#19979;&#25991;&#24102;&#22238;&#26469;&#65306;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#30456;&#26426;&#38519;&#38449;&#29289;&#31181;&#35782;&#21035;&#20316;&#20026;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Bringing Back the Context: Camera Trap Species Identification as Link Prediction on Multimodal Knowledge Graphs. (arXiv:2401.00608v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#30456;&#26426;&#38519;&#38449;&#22270;&#20687;&#30340;&#32467;&#26500;&#21270;&#19978;&#19979;&#25991;&#65292;&#25552;&#39640;&#20854;&#22312;&#29289;&#31181;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#21644;&#27867;&#21270;&#33021;&#21147;&#22686;&#24378;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#26426;&#38519;&#38449;&#22312;&#21160;&#29289;&#29983;&#24577;&#23398;&#20013;&#26159;&#23453;&#36149;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#29983;&#29289;&#22810;&#26679;&#24615;&#30417;&#27979;&#21644;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#22914;&#22312;&#26032;&#30340;&#26410;&#30693;&#20301;&#32622;&#37096;&#32626;&#26102;&#30340;&#31967;&#31957;&#27867;&#21270;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#22270;&#20687;&#33258;&#28982;&#19982;&#21487;&#33021;&#22312;&#19981;&#21516;&#27169;&#24577;&#19979;&#30340;&#24322;&#36136;&#19978;&#19979;&#25991;&#30456;&#20851;&#32852;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#19982;&#30456;&#26426;&#38519;&#38449;&#22270;&#20687;&#30456;&#20851;&#32852;&#30340;&#32467;&#26500;&#21270;&#19978;&#19979;&#25991;&#65292;&#25913;&#21892;&#22312;&#30456;&#26426;&#38519;&#38449;&#20013;&#29289;&#31181;&#35782;&#21035;&#36825;&#20010;&#20219;&#21153;&#30340;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;&#19968;&#24352;&#37326;&#29983;&#21160;&#29289;&#30340;&#29031;&#29255;&#21487;&#33021;&#19982;&#25293;&#25668;&#22320;&#28857;&#21644;&#26102;&#38388;&#20197;&#21450;&#20851;&#20110;&#21160;&#29289;&#29289;&#31181;&#30340;&#32467;&#26500;&#21270;&#29983;&#29289;&#23398;&#30693;&#35782;&#30456;&#20851;&#32852;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#24573;&#35270;&#36825;&#19968;&#28857;&#65292;&#20294;&#23558;&#36825;&#26679;&#30340;&#19978;&#19979;&#25991;&#24102;&#22238;&#26469;&#21487;&#20197;&#24102;&#26469;&#19968;&#20123;&#28508;&#22312;&#30340;&#22909;&#22788;&#65292;&#22914;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#21644;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#22320;&#23558;&#36825;&#26679;&#30340;&#24322;&#36136;&#19978;&#19979;&#25991;&#25972;&#21512;&#21040;&#35270;&#35273;&#39046;&#22495;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Camera traps are valuable tools in animal ecology for biodiversity monitoring and conservation. However, challenges like poor generalization to deployment at new unseen locations limit their practical application. Images are naturally associated with heterogeneous forms of context possibly in different modalities. In this work, we leverage the structured context associated with the camera trap images to improve out-of-distribution generalization for the task of species identification in camera traps. For example, a photo of a wild animal may be associated with information about where and when it was taken, as well as structured biology knowledge about the animal species. While typically overlooked by existing work, bringing back such context offers several potential benefits for better image understanding, such as addressing data scarcity and enhancing generalization. However, effectively integrating such heterogeneous context into the visual domain is a challenging problem. To address
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#20998;&#35299;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#20998;&#31163;&#35828;&#35805;&#20154;&#36523;&#20221;&#20449;&#24687;&#21644;&#20854;&#20182;&#26080;&#20851;&#22240;&#32032;&#65292;&#35299;&#20915;&#20102;&#30446;&#26631;&#35821;&#38899;&#25552;&#21462;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#35828;&#35805;&#20154;&#28151;&#21472;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20998;&#35299;&#30340;&#35828;&#35805;&#20154;&#36523;&#20221;&#20449;&#24687;&#26469;&#25351;&#23548;&#35821;&#38899;&#25552;&#21462;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2312.10305</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#20998;&#35299;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#40065;&#26834;&#30446;&#26631;&#35821;&#38899;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Disentangled Representation Learning for Robust Target Speech Extraction. (arXiv:2312.10305v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10305
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#20998;&#35299;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#20998;&#31163;&#35828;&#35805;&#20154;&#36523;&#20221;&#20449;&#24687;&#21644;&#20854;&#20182;&#26080;&#20851;&#22240;&#32032;&#65292;&#35299;&#20915;&#20102;&#30446;&#26631;&#35821;&#38899;&#25552;&#21462;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#35828;&#35805;&#20154;&#28151;&#21472;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20998;&#35299;&#30340;&#35828;&#35805;&#20154;&#36523;&#20221;&#20449;&#24687;&#26469;&#25351;&#23548;&#35821;&#38899;&#25552;&#21462;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#20449;&#21495;&#26412;&#36136;&#19978;&#26159;&#22797;&#26434;&#30340;&#65292;&#22240;&#20026;&#23427;&#21253;&#21547;&#20840;&#23616;&#22768;&#23398;&#29305;&#24449;&#21644;&#23616;&#37096;&#35821;&#20041;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#30446;&#26631;&#35821;&#38899;&#25552;&#21462;&#20219;&#21153;&#20013;&#65292;&#21442;&#32771;&#35821;&#38899;&#20013;&#19982;&#35828;&#35805;&#20154;&#36523;&#20221;&#26080;&#20851;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#35821;&#20041;&#20449;&#24687;&#21487;&#33021;&#23548;&#33268;&#22312;&#35821;&#38899;&#25552;&#21462;&#32593;&#32476;&#20013;&#20986;&#29616;&#35828;&#35805;&#20154;&#28151;&#21472;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#20998;&#35299;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#19968;&#20010;&#20004;&#38454;&#27573;&#36807;&#31243;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21033;&#29992;&#21442;&#32771;&#35821;&#38899;&#32534;&#30721;&#32593;&#32476;&#21644;&#20840;&#23616;&#20449;&#24687;&#20998;&#35299;&#32593;&#32476;&#36880;&#28176;&#20998;&#35299;&#35828;&#35805;&#20154;&#36523;&#20221;&#20449;&#24687;&#21644;&#20854;&#20182;&#19981;&#30456;&#20851;&#22240;&#32032;&#12290;&#25105;&#20204;&#19987;&#38376;&#20351;&#29992;&#20998;&#35299;&#30340;&#35828;&#35805;&#20154;&#36523;&#20221;&#20449;&#24687;&#26469;&#25351;&#23548;&#35821;&#38899;&#25552;&#21462;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#33258;&#36866;&#24212;&#35843;&#21046;Transformer&#26469;&#30830;&#20445;&#28151;&#21512;&#20449;&#21495;&#30340;&#22768;&#23398;&#34920;&#31034;&#19981;&#21463;&#35828;&#35805;&#20154;&#23884;&#20837;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech signals are inherently complex as they encompass both global acoustic characteristics and local semantic information. However, in the task of target speech extraction, certain elements of global and local semantic information in the reference speech, which are irrelevant to speaker identity, can lead to speaker confusion within the speech extraction network. To overcome this challenge, we propose a self-supervised disentangled representation learning method. Our approach tackles this issue through a two-phase process, utilizing a reference speech encoding network and a global information disentanglement network to gradually disentangle the speaker identity information from other irrelevant factors. We exclusively employ the disentangled speaker identity information to guide the speech extraction network. Moreover, we introduce the adaptive modulation Transformer to ensure that the acoustic representation of the mixed signal remains undisturbed by the speaker embeddings. This com
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28216;&#25103;&#20462;&#25913;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#20462;&#25913;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#30446;&#26631;&#31574;&#30053;&#37197;&#32622;&#25104;&#20026;&#21807;&#19968;&#30340;Nash&#22343;&#34913;&#24182;&#20855;&#26377;&#29305;&#23450;&#20215;&#20540;&#33539;&#22260;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20462;&#25913;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2311.00582</link><description>&lt;p&gt;
&#26368;&#23567;&#20462;&#25913;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20197;&#23454;&#29616;&#20219;&#24847;Nash&#22343;&#34913;&#21644;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
Minimally Modifying a Markov Game to Achieve Any Nash Equilibrium and Value. (arXiv:2311.00582v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00582
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28216;&#25103;&#20462;&#25913;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#20462;&#25913;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#30446;&#26631;&#31574;&#30053;&#37197;&#32622;&#25104;&#20026;&#21807;&#19968;&#30340;Nash&#22343;&#34913;&#24182;&#20855;&#26377;&#29305;&#23450;&#20215;&#20540;&#33539;&#22260;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20462;&#25913;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#28216;&#25103;&#20462;&#25913;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#20301;&#21892;&#24847;&#30340;&#28216;&#25103;&#35774;&#35745;&#32773;&#25110;&#24694;&#24847;&#30340;&#23545;&#25163;&#20462;&#25913;&#20102;&#19968;&#20010;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20197;&#20415;&#19968;&#20010;&#30446;&#26631;&#30830;&#23450;&#24615;&#25110;&#38543;&#26426;&#30340;&#31574;&#30053;&#37197;&#32622;&#25104;&#20026;&#21807;&#19968;&#30340;&#39532;&#23572;&#21487;&#22827;&#23436;&#32654;Nash&#22343;&#34913;&#65292;&#24182;&#19988;&#22312;&#30446;&#26631;&#33539;&#22260;&#20869;&#20855;&#26377;&#20215;&#20540;&#65292;&#20197;&#26368;&#23567;&#21270;&#20462;&#25913;&#25104;&#26412;&#12290;&#25105;&#20204;&#34920;&#24449;&#20102;&#33021;&#22815;&#23433;&#35013;&#20026;&#26576;&#20010;&#28216;&#25103;&#30340;&#21807;&#19968;&#22343;&#34913;&#30340;&#31574;&#30053;&#37197;&#32622;&#30340;&#38598;&#21512;&#65292;&#24182;&#24314;&#31435;&#20102;&#25104;&#21151;&#23433;&#35013;&#30340;&#20805;&#20998;&#21644;&#24517;&#35201;&#26465;&#20214;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#35299;&#19968;&#20010;&#24102;&#26377;&#32447;&#24615;&#32422;&#26463;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#28982;&#21518;&#36827;&#34892;&#38543;&#26426;&#25200;&#21160;&#65292;&#26469;&#33719;&#24471;&#19968;&#20010;&#25104;&#26412;&#36817;&#20046;&#26368;&#20248;&#30340;&#20462;&#25913;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the game modification problem, where a benevolent game designer or a malevolent adversary modifies the reward function of a zero-sum Markov game so that a target deterministic or stochastic policy profile becomes the unique Markov perfect Nash equilibrium and has a value within a target range, in a way that minimizes the modification cost. We characterize the set of policy profiles that can be installed as the unique equilibrium of some game, and establish sufficient and necessary conditions for successful installation. We propose an efficient algorithm, which solves a convex optimization problem with linear constraints and then performs random perturbation, to obtain a modification plan with a near-optimal cost.
&lt;/p&gt;</description></item><item><title>KLoB&#26159;&#19968;&#20010;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#30693;&#35782;&#23450;&#20301;&#26041;&#27861;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#23450;&#20301;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#20107;&#23454;&#30693;&#35782;&#23616;&#37096;&#24615;&#20551;&#35774;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.16535</link><description>&lt;p&gt;
KLoB: &#19968;&#31181;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#30693;&#35782;&#23450;&#20301;&#26041;&#27861;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
KLoB: a Benchmark for Assessing Knowledge Locating Methods in Language Models. (arXiv:2309.16535v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16535
&lt;/p&gt;
&lt;p&gt;
KLoB&#26159;&#19968;&#20010;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#30693;&#35782;&#23450;&#20301;&#26041;&#27861;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#23450;&#20301;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#20107;&#23454;&#30693;&#35782;&#23616;&#37096;&#24615;&#20551;&#35774;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23450;&#20301;&#28982;&#21518;&#32534;&#36753;&#30340;&#33539;&#24335;&#24050;&#32463;&#25104;&#20026;&#25913;&#21464;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#20648;&#30340;&#20107;&#23454;&#30693;&#35782;&#30340;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#23450;&#20301;&#26041;&#27861;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#22320;&#25214;&#21040;&#23884;&#20837;&#25152;&#38656;&#30693;&#35782;&#30340;&#30830;&#20999;&#21442;&#25968;&#36824;&#32570;&#20047;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#23545;&#20107;&#23454;&#30693;&#35782;&#30340;&#23616;&#37096;&#24615;&#20551;&#35774;&#30340;&#26377;&#25928;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#65292;&#20294;&#27809;&#26377;&#25552;&#20379;&#19968;&#31181;&#27979;&#35797;&#20551;&#35774;&#30340;&#26041;&#27861;&#20197;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#35752;&#35770;&#21644;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;KLoB&#65292;&#19968;&#20010;&#35780;&#20272;&#21487;&#38752;&#30340;&#30693;&#35782;&#23450;&#20301;&#26041;&#27861;&#24212;&#28385;&#36275;&#30340;&#19977;&#20010;&#22522;&#26412;&#23646;&#24615;&#30340;&#22522;&#20934;&#12290;KLoB&#21487;&#20316;&#20026;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#29616;&#26377;&#23450;&#20301;&#26041;&#27861;&#30340;&#22522;&#20934;&#65292;&#24182;&#20026;&#37325;&#26032;&#35780;&#20272;&#20107;&#23454;&#30693;&#35782;&#30340;&#23616;&#37096;&#24615;&#20551;&#35774;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#20844;&#24320;&#21487;&#29992;&#20110;\url{https://github.com/juyiming/KLoB}&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Locate-Then-Edit paradigm has emerged as one of the main approaches in changing factual knowledge stored in the Language models. However, there is a lack of research on whether present locating methods can pinpoint the exact parameters embedding the desired knowledge. Moreover, although many researchers have questioned the validity of locality hypothesis of factual knowledge, no method is provided to test the a hypothesis for more in-depth discussion and research. Therefore, we introduce KLoB, a benchmark examining three essential properties that a reliable knowledge locating method should satisfy. KLoB can serve as a benchmark for evaluating existing locating methods in language models, and can contributes a method to reassessing the validity of locality hypothesis of factual knowledge. Our is publicly available at \url{https://github.com/juyiming/KLoB}.
&lt;/p&gt;</description></item><item><title>Lyra&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#24037;&#20855;&#20462;&#27491;&#21644;&#29468;&#24819;&#20462;&#27491;&#20004;&#31181;&#26426;&#21046;&#65292;&#22686;&#24378;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#24418;&#24335;&#21270;&#23450;&#29702;&#35777;&#26126;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#65292;&#20943;&#36731;&#20102;&#24187;&#35273;&#65292;&#24182;&#25552;&#39640;&#20102;&#35777;&#26126;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.15806</link><description>&lt;p&gt;
Lyra: &#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#20013;&#30340;&#21452;&#37325;&#20462;&#27491;&#31574;&#30053;&#30340;&#32534;&#25490;
&lt;/p&gt;
&lt;p&gt;
Lyra: Orchestrating Dual Correction in Automated Theorem Proving. (arXiv:2309.15806v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15806
&lt;/p&gt;
&lt;p&gt;
Lyra&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#24037;&#20855;&#20462;&#27491;&#21644;&#29468;&#24819;&#20462;&#27491;&#20004;&#31181;&#26426;&#21046;&#65292;&#22686;&#24378;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#24418;&#24335;&#21270;&#23450;&#29702;&#35777;&#26126;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#65292;&#20943;&#36731;&#20102;&#24187;&#35273;&#65292;&#24182;&#25552;&#39640;&#20102;&#35777;&#26126;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#24418;&#24335;&#21270;&#23450;&#29702;&#35777;&#26126;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#25506;&#32034;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#23588;&#20854;&#26159;&#20851;&#20110;&#24187;&#35273;&#30340;&#20943;&#36731;&#21644;&#36890;&#36807;&#35777;&#26126;&#22120;&#38169;&#35823;&#28040;&#24687;&#30340;&#32454;&#21270;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#23578;&#26410;&#28145;&#20837;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#20026;&#20102;&#22686;&#24378;LLMs&#22312;&#35813;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Lyra&#65292;&#19968;&#31181;&#37319;&#29992;&#20004;&#31181;&#19981;&#21516;&#20462;&#27491;&#26426;&#21046;&#30340;&#26032;&#26694;&#26550;&#65306;&#24037;&#20855;&#20462;&#27491;&#65288;TC&#65289;&#21644;&#29468;&#24819;&#20462;&#27491;&#65288;CC&#65289;&#12290;&#20026;&#20102;&#22312;&#24418;&#24335;&#35777;&#26126;&#30340;&#21518;&#22788;&#29702;&#20013;&#23454;&#29616;&#24037;&#20855;&#20462;&#27491;&#65292;&#25105;&#20204;&#21033;&#29992;&#20808;&#21069;&#30340;&#30693;&#35782;&#26469;&#21033;&#29992;&#39044;&#23450;&#20041;&#30340;&#35777;&#26126;&#24037;&#20855;&#65288;&#22914;Sledgehammer&#65289;&#26469;&#25351;&#23548;&#26367;&#25442;&#19981;&#27491;&#30830;&#30340;&#24037;&#20855;&#12290;&#24037;&#20855;&#20462;&#27491;&#26174;&#33879;&#20943;&#36731;&#20102;&#24187;&#35273;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35777;&#26126;&#30340;&#25972;&#20307;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29468;&#24819;&#20462;&#27491;&#65292;&#19968;&#31181;&#38169;&#35823;&#21453;&#39304;&#26426;&#21046;&#65292;&#26088;&#22312;&#19982;&#35777;&#26126;&#22120;&#20114;&#21160;&#65292;&#36890;&#36807;&#35777;&#26126;&#22120;&#30340;&#38169;&#35823;&#28040;&#24687;&#36827;&#19968;&#27493;&#23436;&#21892;&#24418;&#24335;&#35777;&#26126;&#30340;&#29468;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) present an intriguing avenue for exploration in the field of formal theorem proving. Nevertheless, their full potential, particularly concerning the mitigation of hallucinations and refinement through prover error messages, remains an area that has yet to be thoroughly investigated. To enhance the effectiveness of LLMs in the field, we introduce the Lyra, a new framework that employs two distinct correction mechanisms: Tool Correction (TC) and Conjecture Correction (CC). To implement Tool Correction in the post-processing of formal proofs, we leverage prior knowledge to utilize predefined prover tools (e.g., Sledgehammer) for guiding the replacement of incorrect tools. Tool Correction significantly contributes to mitigating hallucinations, thereby improving the overall accuracy of the proof. In addition, we introduce Conjecture Correction, an error feedback mechanism designed to interact with prover to refine formal proof conjectures with prover error messa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26356;&#39640;&#30340;&#25209;&#35780;&#21147;&#37327;&#23558;&#20154;&#31867;&#35302;&#25720;&#19982;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#20998;&#31163;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20010;&#22256;&#24785;&#24230;&#27979;&#35797;&#21644;&#32479;&#35745;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#20986;&#21487;&#33021;&#34987;&#32534;&#36753;&#30340;&#37096;&#20998;&#65292;&#24182;&#25506;&#35752;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21450;&#30456;&#20851;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.12747</link><description>&lt;p&gt;
&#20351;&#29992;&#26356;&#39640;&#30340;&#25209;&#35780;&#21147;&#37327;&#23558;&#20154;&#31867;&#35302;&#25720;&#19982;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#20998;&#31163;&#65306;&#19968;&#31181;&#20449;&#24687;&#35770;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Separating the Human Touch from AI-Generated Text using Higher Criticism: An Information-Theoretic Approach. (arXiv:2308.12747v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26356;&#39640;&#30340;&#25209;&#35780;&#21147;&#37327;&#23558;&#20154;&#31867;&#35302;&#25720;&#19982;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#20998;&#31163;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20010;&#22256;&#24785;&#24230;&#27979;&#35797;&#21644;&#32479;&#35745;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#20986;&#21487;&#33021;&#34987;&#32534;&#36753;&#30340;&#37096;&#20998;&#65292;&#24182;&#25506;&#35752;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21450;&#30456;&#20851;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#19968;&#31687;&#32473;&#23450;&#25991;&#31456;&#26159;&#23436;&#20840;&#30001;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#32534;&#20889;&#30340;&#65292;&#36824;&#26159;&#21253;&#21547;&#20102;&#19968;&#20123;&#30001;&#20854;&#20182;&#20316;&#32773;&#65288;&#21487;&#33021;&#26159;&#20154;&#31867;&#65289;&#36827;&#34892;&#20102;&#37325;&#35201;&#32534;&#36753;&#30340;&#26367;&#20195;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#36807;&#31243;&#28041;&#21450;&#23545;&#20010;&#21035;&#21477;&#23376;&#25110;&#20854;&#20182;&#25991;&#26412;&#21333;&#20301;&#36215;&#28304;&#36827;&#34892;&#30340;&#22810;&#20010;&#22256;&#24785;&#24230;&#27979;&#35797;&#65292;&#23558;&#36825;&#20123;&#22810;&#20010;&#27979;&#35797;&#32467;&#21512;&#36215;&#26469;&#20351;&#29992;&#26356;&#39640;&#30340;&#25209;&#35780;&#21147;&#37327;&#65288;HC&#65289;&#12290;&#20316;&#20026;&#21103;&#20135;&#21697;&#65292;&#35813;&#26041;&#27861;&#36824;&#33021;&#22815;&#35782;&#21035;&#20986;&#21487;&#33021;&#34987;&#32534;&#36753;&#30340;&#37096;&#20998;&#12290;&#35813;&#26041;&#27861;&#21463;&#21040;&#20102;&#23545;&#25968;&#22256;&#24785;&#24230;&#25910;&#25947;&#21040;&#20132;&#21449;&#29109;&#29575;&#30340;&#21551;&#21457;&#65292;&#20197;&#21450;&#19968;&#20010;&#32479;&#35745;&#27169;&#22411;&#26469;&#25551;&#36848;&#34987;&#32534;&#36753;&#30340;&#25991;&#26412;&#65292;&#21363;&#21477;&#23376;&#20027;&#35201;&#30001;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#65292;&#20294;&#21487;&#33021;&#26377;&#23569;&#25968;&#21477;&#23376;&#26159;&#36890;&#36807;&#20854;&#20182;&#26426;&#21046;&#20135;&#29983;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#24433;&#21709;&#20854;&#25104;&#21151;&#30340;&#22240;&#32032;&#12290;&#36825;&#20010;&#20998;&#26512;&#25552;&#20986;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#24320;&#25918;&#25361;&#25112;&#65292;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#21487;&#33021;&#20250;&#25913;&#21892;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method to determine whether a given article was entirely written by a generative language model versus an alternative situation in which the article includes some significant edits by a different author, possibly a human. Our process involves many perplexity tests for the origin of individual sentences or other text atoms, combining these multiple tests using Higher Criticism (HC). As a by-product, the method identifies parts suspected to be edited. The method is motivated by the convergence of the log-perplexity to the cross-entropy rate and by a statistical model for edited text saying that sentences are mostly generated by the language model, except perhaps for a few sentences that might have originated via a different mechanism. We demonstrate the effectiveness of our method using real data and analyze the factors affecting its success. This analysis raises several interesting open challenges whose resolution may improve the method's effectiveness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;HYPER&#29992;&#20110;&#35843;&#25972;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24322;&#24120;&#20540;&#26816;&#27979;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26080;&#30417;&#30563;DOD&#27169;&#22411;&#20013;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#27169;&#22411;&#36873;&#25321;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#35774;&#35745;&#21644;&#35757;&#32451;&#36229;&#32593;&#32476;(HN)&#23558;&#36229;&#21442;&#25968;&#26144;&#23556;&#21040;&#20027;&#35201;DOD&#27169;&#22411;&#30340;&#26368;&#20248;&#26435;&#37325;&#19978;&#12290;</title><link>http://arxiv.org/abs/2307.10529</link><description>&lt;p&gt;
&#24555;&#36895;&#26080;&#30417;&#30563;&#28145;&#24230;&#24322;&#24120;&#20540;&#27169;&#22411;&#36873;&#25321;&#19982;&#36229;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Fast Unsupervised Deep Outlier Model Selection with Hypernetworks. (arXiv:2307.10529v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;HYPER&#29992;&#20110;&#35843;&#25972;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24322;&#24120;&#20540;&#26816;&#27979;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26080;&#30417;&#30563;DOD&#27169;&#22411;&#20013;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#27169;&#22411;&#36873;&#25321;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#35774;&#35745;&#21644;&#35757;&#32451;&#36229;&#32593;&#32476;(HN)&#23558;&#36229;&#21442;&#25968;&#26144;&#23556;&#21040;&#20027;&#35201;DOD&#27169;&#22411;&#30340;&#26368;&#20248;&#26435;&#37325;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#20540;&#26816;&#27979;(OD)&#22312;&#35768;&#22810;&#39046;&#22495;&#37117;&#26377;&#24212;&#29992;&#65292;&#24182;&#26377;&#35768;&#22810;&#25216;&#26415;&#30340;&#20016;&#23500;&#25991;&#29486;&#12290;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;OD(DOD)&#30001;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35768;&#22810;&#36827;&#23637;&#32780;&#21463;&#21040;&#20102;&#26368;&#36817;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#20851;&#38190;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#38382;&#39064;&#65292;&#21363;&#26080;&#30417;&#30563;DOD&#30340;&#26377;&#25928;&#36229;&#21442;&#25968;(HP)&#35843;&#25972;/&#27169;&#22411;&#36873;&#25321;&#12290;&#34429;&#28982;&#19968;&#20123;&#20808;&#21069;&#30340;&#24037;&#20316;&#25253;&#21578;&#20102;OD&#27169;&#22411;&#23545;HP&#30340;&#25935;&#24863;&#24615;&#65292;&#20294;&#23545;&#20110;&#23637;&#31034;&#20102;&#38271;&#21015;&#34920;HP&#30340;&#29616;&#20195;DOD&#27169;&#22411;&#26469;&#35828;&#65292;&#36825;&#21464;&#24471;&#38750;&#24120;&#20851;&#38190;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;HYPER&#26469;&#35843;&#25972;DOD&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20004;&#20010;&#22522;&#26412;&#25361;&#25112;&#65306;(1)&#26080;&#30417;&#30563;&#24773;&#20917;&#19979;&#30340;&#39564;&#35777;(&#30001;&#20110;&#32570;&#20047;&#26631;&#35760;&#30340;&#24322;&#24120;&#20540;)&#65292;&#20197;&#21450;(2) HP/&#27169;&#22411;&#31354;&#38388;&#30340;&#39640;&#25928;&#25628;&#32034; (&#30001;&#20110;HP&#25968;&#37327;&#30340;&#25351;&#25968;&#22686;&#38271;)&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#35774;&#35745;&#21644;&#35757;&#32451;&#19968;&#20010;&#26032;&#39062;&#30340;&#36229;&#32593;&#32476;(HN)&#65292;&#20854;&#23558;HP&#26144;&#23556;&#21040;&#20027;&#35201;DOD&#27169;&#22411;&#30340;&#26368;&#20248;&#26435;&#37325;&#19978;&#12290;&#21453;&#36807;&#26469;&#65292;HYPER&#21033;&#29992;&#19968;&#20010;&#21333;&#29420;&#30340;HN&#65292;&#21487;&#20197;&#21160;&#24577;&#29983;&#25104;&#22810;&#20010;DOD&#27169;&#22411;&#30340;&#26435;&#37325; (&#23545;&#24212;&#20110;...)&#12290;
&lt;/p&gt;
&lt;p&gt;
Outlier detection (OD) finds many applications with a rich literature of numerous techniques. Deep neural network based OD (DOD) has seen a recent surge of attention thanks to the many advances in deep learning. In this paper, we consider a critical-yet-understudied challenge with unsupervised DOD, that is, effective hyperparameter (HP) tuning/model selection. While several prior work report the sensitivity of OD models to HPs, it becomes ever so critical for the modern DOD models that exhibit a long list of HPs. We introduce HYPER for tuning DOD models, tackling two fundamental challenges: (1) validation without supervision (due to lack of labeled anomalies), and (2) efficient search of the HP/model space (due to exponential growth in the number of HPs). A key idea is to design and train a novel hypernetwork (HN) that maps HPs onto optimal weights of the main DOD model. In turn, HYPER capitalizes on a single HN that can dynamically generate weights for many DOD models (corresponding t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#22495;&#20013;&#20351;&#29992;&#30690;&#37327;&#34920;&#36798;&#36827;&#34892;&#22312;&#32447;&#30446;&#26631;&#35782;&#21035;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20943;&#23569;&#35745;&#31639;&#36127;&#25285;&#21644;&#25552;&#39640;&#35782;&#21035;&#36895;&#24230;&#65292;&#20351;&#20854;&#25104;&#20026;&#39318;&#20010;&#21487;&#29992;&#20110;&#24555;&#36895;&#21464;&#21160;&#29615;&#22659;&#19979;&#30340;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#22312;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.07876</link><description>&lt;p&gt;
&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#22495;&#20013;&#20351;&#29992;&#30690;&#37327;&#34920;&#36798;&#36827;&#34892;&#22312;&#32447;&#30446;&#26631;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Online Goal Recognition in Discrete and Continuous Domains Using a Vectorial Representation. (arXiv:2307.07876v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07876
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#22495;&#20013;&#20351;&#29992;&#30690;&#37327;&#34920;&#36798;&#36827;&#34892;&#22312;&#32447;&#30446;&#26631;&#35782;&#21035;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20943;&#23569;&#35745;&#31639;&#36127;&#25285;&#21644;&#25552;&#39640;&#35782;&#21035;&#36895;&#24230;&#65292;&#20351;&#20854;&#25104;&#20026;&#39318;&#20010;&#21487;&#29992;&#20110;&#24555;&#36895;&#21464;&#21160;&#29615;&#22659;&#19979;&#30340;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#22312;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#20110;&#22312;&#32447;&#30446;&#26631;&#35782;&#21035;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22312;&#20302;&#21487;&#35266;&#27979;&#24615;&#19979;&#26377;&#25928;&#25512;&#26029;&#30446;&#26631;&#65292;&#32780;&#23545;&#20110;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#22495;&#20013;&#37117;&#33021;&#24037;&#20316;&#30340;&#22312;&#32447;&#30446;&#26631;&#35782;&#21035;&#30340;&#30740;&#31350;&#36739;&#23569;&#12290;&#22312;&#32447;&#30446;&#26631;&#35782;&#21035;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#22312;&#27599;&#20010;&#26032;&#30340;&#35266;&#27979;&#20013;&#22810;&#27425;&#35843;&#29992;&#35268;&#21010;&#22120;&#65292;&#36896;&#25104;&#20102;&#24456;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#24555;&#36895;&#32780;&#21487;&#38752;&#22320;&#35782;&#21035;&#30446;&#26631;&#23545;&#20110;&#20219;&#20309;&#36712;&#36857;&#35268;&#21010;&#38382;&#39064;&#37117;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#22240;&#20026;&#30495;&#23454;&#29289;&#29702;&#19990;&#30028;&#26159;&#24555;&#36895;&#21464;&#21160;&#30340;&#65292;&#20363;&#22914;&#26426;&#22120;&#20154;&#24212;&#29992;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#30446;&#26631;&#35782;&#21035;&#26041;&#27861;&#65292;&#22312;&#31163;&#25955;&#22495;&#20013;&#27599;&#20010;&#21487;&#33021;&#30340;&#30446;&#26631;&#21482;&#38656;&#35201;&#19968;&#27425;&#35843;&#29992;&#35268;&#21010;&#22120;&#65292;&#32780;&#22312;&#36830;&#32493;&#22495;&#20013;&#21033;&#29992;&#31616;&#21270;&#30340;&#36816;&#21160;&#27169;&#22411;&#26469;&#20943;&#23567;&#35745;&#31639;&#36127;&#25285;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#26041;&#27861;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#22312;&#22312;&#32447;&#35782;&#21035;&#26041;&#38754;&#24555;&#20960;&#20010;&#25968;&#37327;&#32423;&#65292;&#20351;&#20043;&#25104;&#20026;&#39318;&#20010;&#23454;&#38469;&#21487;&#29992;&#20110;&#38656;&#35201;&#20122;&#31186;&#32423;&#21709;&#24212;&#30340;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#22312;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
While recent work on online goal recognition efficiently infers goals under low observability, comparatively less work focuses on online goal recognition that works in both discrete and continuous domains. Online goal recognition approaches often rely on repeated calls to the planner at each new observation, incurring high computational costs. Recognizing goals online in continuous space quickly and reliably is critical for any trajectory planning problem since the real physical world is fast-moving, e.g. robot applications. We develop an efficient method for goal recognition that relies either on a single call to the planner for each possible goal in discrete domains or a simplified motion model that reduces the computational burden in continuous ones. The resulting approach performs the online component of recognition orders of magnitude faster than the current state of the art, making it the first online method effectively usable for robotics applications that require sub-second rec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;C-K-S&#31639;&#27861;&#65292;&#36890;&#36807;&#20462;&#21098;&#28388;&#27874;&#22120;&#21644;&#36716;&#25442;&#31232;&#30095;&#24352;&#37327;&#20026;&#31264;&#23494;&#24352;&#37327;&#30340;&#26041;&#24335;&#65292;&#36339;&#36807;&#21367;&#31215;&#23618;&#20013;&#30340;0&#20803;&#32032;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;C-K-S&#30456;&#23545;&#20110;PyTorch&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.15951</link><description>&lt;p&gt;
&#36890;&#36807;&#36339;&#36807;&#38646;&#20803;&#32032;&#38477;&#20302;&#21367;&#31215;&#23618;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Reduce Computational Complexity for Convolutional Layers by Skipping Zeros. (arXiv:2306.15951v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;C-K-S&#31639;&#27861;&#65292;&#36890;&#36807;&#20462;&#21098;&#28388;&#27874;&#22120;&#21644;&#36716;&#25442;&#31232;&#30095;&#24352;&#37327;&#20026;&#31264;&#23494;&#24352;&#37327;&#30340;&#26041;&#24335;&#65292;&#36339;&#36807;&#21367;&#31215;&#23618;&#20013;&#30340;0&#20803;&#32032;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;C-K-S&#30456;&#23545;&#20110;PyTorch&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20381;&#36182;&#24182;&#34892;&#22788;&#29702;&#22120;&#36827;&#34892;&#21152;&#36895;&#12290;&#20026;&#20102;&#20026;&#20854;&#35774;&#35745;&#36816;&#31639;&#31526;&#65292;&#38656;&#35201;&#19981;&#20165;&#26377;&#20248;&#21270;&#31639;&#27861;&#20197;&#38477;&#20302;&#22797;&#26434;&#24230;&#65292;&#36824;&#38656;&#35201;&#20805;&#20998;&#21033;&#29992;&#30828;&#20214;&#36164;&#28304;&#12290;&#21367;&#31215;&#23618;&#20027;&#35201;&#21253;&#21547;&#19977;&#31181;&#36816;&#31639;&#31526;&#65306;&#21069;&#21521;&#20256;&#25773;&#30340;&#21367;&#31215;&#65292;&#21453;&#21521;&#20256;&#25773;&#30340;&#21453;&#21367;&#31215;&#21644;&#33192;&#32960;&#21367;&#31215;&#12290;&#24403;&#25191;&#34892;&#36825;&#20123;&#36816;&#31639;&#26102;&#65292;&#22987;&#32456;&#20250;&#21521;&#24352;&#37327;&#20013;&#28155;&#21152;0&#20803;&#32032;&#65292;&#23548;&#33268;&#20887;&#20313;&#35745;&#31639;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;C-K-S&#31639;&#27861;&#65288;ConvV2, KS-deconv, Sk-dilated&#65289;&#65292;&#20197;&#20004;&#31181;&#26041;&#24335;&#36339;&#36807;&#36825;&#20123;0&#20803;&#32032;&#65306;&#20462;&#21098;&#28388;&#27874;&#22120;&#20197;&#25490;&#38500;&#22635;&#20805;&#30340;0&#20803;&#32032;&#65307;&#23558;&#31232;&#30095;&#24352;&#37327;&#36716;&#25442;&#20026;&#31264;&#23494;&#24352;&#37327;&#65292;&#36991;&#20813;&#22312;&#21453;&#21367;&#31215;&#21644;&#33192;&#32960;&#21367;&#31215;&#20013;&#25554;&#20837;0&#20803;&#32032;&#12290;&#19982;&#26222;&#36890;&#21367;&#31215;&#30456;&#27604;&#65292;&#21453;&#21367;&#31215;&#30001;&#20110;&#20854;&#22797;&#26434;&#24615;&#32780;&#38590;&#20197;&#21152;&#36895;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;C-K-S&#30340;&#39640;&#24615;&#33021;GPU&#23454;&#29616;&#65292;&#24182;&#36890;&#36807;&#19982;PyTorch&#30340;&#27604;&#36739;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#26681;&#25454;&#23454;&#39564;&#32467;&#26524;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;C-K-S&#30456;&#23545;&#20110;PyTorch&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks rely on parallel processors for acceleration. To design operators for them, it requires not only good algorithm to reduce complexity, but also sufficient utilization of hardwares. Convolutional layers mainly contain 3 kinds of operators: convolution in forward propagation, deconvolution and dilated-convolution in backward propagation. When executing these operators, 0s are always added to tensors, causing redundant calculations. This paper gives C-K-S algorithm (ConvV2, KS-deconv, Sk-dilated), which skips these 0s in two ways: trim the filters to exclude padded 0s; transform sparse tensors to dense tensors, to avoid inserted 0s in deconvolution and dilated-convolution. In contrast to regular convolution, deconvolution is hard to accelerate due to its complicacy. This paper provides high-performance GPU implementations of C-K-S, and verifies their effectiveness with comparison to PyTorch. According to the experiments, C-K-S has advantages over PyTorch in certain cas
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#22810;&#26679;&#24615;&#31995;&#25968;&#20316;&#20026;LLM&#39044;&#35757;&#32451;&#25968;&#25454;&#36136;&#37327;&#30340;&#25351;&#26631;&#65292;&#30740;&#31350;&#34920;&#26126;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#24456;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.13840</link><description>&lt;p&gt;
&#36229;&#36234;&#35268;&#27169;&#65306;&#22810;&#26679;&#24615;&#31995;&#25968;&#20316;&#20026;&#25968;&#25454;&#36136;&#37327;&#25351;&#26631;&#35777;&#26126;&#20102;LLMs&#26159;&#22312;&#24418;&#24335;&#22810;&#26679;&#30340;&#25968;&#25454;&#19978;&#39044;&#20808;&#35757;&#32451;&#30340;
&lt;/p&gt;
&lt;p&gt;
Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data. (arXiv:2306.13840v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#22810;&#26679;&#24615;&#31995;&#25968;&#20316;&#20026;LLM&#39044;&#35757;&#32451;&#25968;&#25454;&#36136;&#37327;&#30340;&#25351;&#26631;&#65292;&#30740;&#31350;&#34920;&#26126;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#24456;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#65292;&#39044;&#20808;&#35757;&#32451;&#24378;&#22823;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36235;&#21183;&#20027;&#35201;&#38598;&#20013;&#22312;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#25193;&#22823;&#12290;&#28982;&#32780;&#65292;&#39044;&#20808;&#35757;&#32451;&#25968;&#25454;&#30340;&#36136;&#37327;&#23545;&#20110;&#35757;&#32451;&#24378;&#22823;&#30340;LLMs&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#65292;&#20294;&#23427;&#26159;&#19968;&#20010;&#27169;&#31946;&#30340;&#27010;&#24565;&#65292;&#23578;&#26410;&#23436;&#20840;&#34920;&#24449;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#36817;&#25552;&#20986;&#30340;Task2Vec&#22810;&#26679;&#24615;&#31995;&#25968;&#26469;&#22522;&#20110;&#25968;&#25454;&#36136;&#37327;&#30340;&#24418;&#24335;&#26041;&#38754;&#65292;&#36229;&#36234;&#35268;&#27169;&#26412;&#36523;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#27979;&#37327;&#20844;&#24320;&#21487;&#29992;&#30340;&#39044;&#20808;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#65292;&#20197;&#35777;&#26126;&#23427;&#20204;&#30340;&#24418;&#24335;&#22810;&#26679;&#24615;&#39640;&#20110;&#29702;&#35770;&#30340;&#19979;&#38480;&#21644;&#19978;&#38480;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#24314;&#31435;&#23545;&#22810;&#26679;&#24615;&#31995;&#25968;&#30340;&#20449;&#24515;&#65292;&#25105;&#20204;&#36827;&#34892;&#21487;&#35299;&#37322;&#24615;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#35813;&#31995;&#25968;&#19982;&#22810;&#26679;&#24615;&#30340;&#30452;&#35266;&#23646;&#24615;&#30456;&#21563;&#21512;&#65292;&#20363;&#22914;&#65292;&#38543;&#30528;&#28508;&#22312;&#27010;&#24565;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#23427;&#22686;&#21152;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#22810;&#26679;&#24615;&#31995;&#25968;&#26159;&#21487;&#38752;&#30340;&#65292;&#34920;&#26126;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#24456;&#39640;&#65292;&#24182;&#25512;&#27979;&#23427;&#21487;&#20197;&#20316;&#20026;&#39044;&#35757;&#32451;LLMs&#27169;&#22411;&#30340;&#25968;&#25454;&#36136;&#37327;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current trends to pre-train capable Large Language Models (LLMs) mostly focus on scaling of model and dataset size. However, the quality of pre-training data is an important factor for training powerful LLMs, yet it is a nebulous concept that has not been fully characterized. Therefore, we use the recently proposed Task2Vec diversity coefficient to ground and understand formal aspects of data quality, to go beyond scale alone. Specifically, we measure the diversity coefficient of publicly available pre-training datasets to demonstrate that their formal diversity is high when compared to theoretical lower and upper bounds. In addition, to build confidence in the diversity coefficient, we conduct interpretability experiments and find that the coefficient aligns with intuitive properties of diversity, e.g., it increases as the number of latent concepts increases. We conclude the diversity coefficient is reliable, show it's high for publicly available LLM datasets, and conjecture it can be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MetaGAD&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#20174;&#26080;&#26631;&#35760;&#33410;&#28857;&#21040;&#26377;&#26631;&#35760;&#33410;&#28857;&#20043;&#38388;&#30340;&#20803;&#36716;&#31227;&#30693;&#35782;&#65292;&#20197;&#36827;&#34892;&#23569;&#26679;&#26412;&#22270;&#24322;&#24120;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.10668</link><description>&lt;p&gt;
MetaGAD&#65306;&#23398;&#20064;&#20803;&#36716;&#31227;&#36827;&#34892;&#23569;&#26679;&#26412;&#22270;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
MetaGAD: Learning to Meta Transfer for Few-shot Graph Anomaly Detection. (arXiv:2305.10668v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MetaGAD&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#20174;&#26080;&#26631;&#35760;&#33410;&#28857;&#21040;&#26377;&#26631;&#35760;&#33410;&#28857;&#20043;&#38388;&#30340;&#20803;&#36716;&#31227;&#30693;&#35782;&#65292;&#20197;&#36827;&#34892;&#23569;&#26679;&#26412;&#22270;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24322;&#24120;&#26816;&#27979;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#21508;&#20010;&#39046;&#22495;&#20449;&#24687;&#23433;&#20840;&#38382;&#39064;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#22914;&#37329;&#34701;&#27450;&#35784;&#12289;&#31038;&#20250;&#22403;&#22334;&#37038;&#20214;&#12289;&#32593;&#32476;&#20837;&#20405;&#31561;&#12290;&#30446;&#21069;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#26159;&#20197;&#26080;&#30417;&#30563;&#26041;&#24335;&#25191;&#34892;&#30340;&#65292;&#22240;&#20026;&#26631;&#35760;&#30340;&#24322;&#24120;&#22312;&#22823;&#35268;&#27169;&#24773;&#20917;&#19979;&#24448;&#24448;&#22826;&#26114;&#36149;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#26377;&#20851;&#24322;&#24120;&#30340;&#20808;&#21069;&#30693;&#35782;&#65292;&#21487;&#33021;&#20250;&#23558;&#34987;&#35782;&#21035;&#30340;&#24322;&#24120;&#35270;&#20026;&#25968;&#25454;&#22122;&#22768;&#25110;&#19981;&#24863;&#20852;&#36259;&#30340;&#25968;&#25454;&#23454;&#20363;&#12290;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#36890;&#24120;&#21487;&#33719;&#21462;&#26377;&#38480;&#30340;&#26631;&#35760;&#24322;&#24120;&#65292;&#36825;&#20123;&#26631;&#35760;&#24322;&#24120;&#20855;&#26377;&#25512;&#36827;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#25506;&#32034;&#23569;&#37327;&#26631;&#35760;&#24322;&#24120;&#21644;&#22823;&#37327;&#26080;&#26631;&#35760;&#33410;&#28857;&#26469;&#26816;&#27979;&#24322;&#24120;&#30340;&#24037;&#20316;&#30456;&#24403;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#23569;&#26679;&#26412;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#26032;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;MetaGAD&#65292;&#23398;&#20064;&#20803;&#36716;&#31227;&#30693;&#35782;&#26469;&#36827;&#34892;&#22270;&#24322;&#24120;&#26816;&#27979;&#12290;&#23454;
&lt;/p&gt;
&lt;p&gt;
Graph anomaly detection has long been an important problem in various domains pertaining to information security such as financial fraud, social spam, network intrusion, etc. The majority of existing methods are performed in an unsupervised manner, as labeled anomalies in a large scale are often too expensive to acquire. However, the identified anomalies may turn out to be data noises or uninteresting data instances due to the lack of prior knowledge on the anomalies. In realistic scenarios, it is often feasible to obtain limited labeled anomalies, which have great potential to advance graph anomaly detection. However, the work exploring limited labeled anomalies and a large amount of unlabeled nodes in graphs to detect anomalies is rather limited. Therefore, in this paper, we study a novel problem of few-shot graph anomaly detection. We propose a new framework MetaGAD to learn to meta-transfer the knowledge between unlabeled and labeled nodes for graph anomaly detection. Experimental 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;NER&#26041;&#27861;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#23398;&#20064;&#32473;&#23450;&#21644;&#28508;&#22312;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#23558;&#22810;&#31867;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20803;&#26631;&#35760;&#20998;&#31867;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#26679;&#26412;&#24773;&#20917;&#19979;&#36798;&#21040;&#33391;&#22909;&#30340;&#35782;&#21035;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.04928</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A transformer-based method for zero and few-shot biomedical named entity recognition. (arXiv:2305.04928v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;NER&#26041;&#27861;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#23398;&#20064;&#32473;&#23450;&#21644;&#28508;&#22312;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#23558;&#22810;&#31867;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20803;&#26631;&#35760;&#20998;&#31867;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#26679;&#26412;&#24773;&#20917;&#19979;&#36798;&#21040;&#33391;&#22909;&#30340;&#35782;&#21035;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#65292;&#26377;&#30417;&#30563;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#20381;&#36182;&#20110;&#20855;&#26377;&#32473;&#23450;&#21629;&#21517;&#23454;&#20307;&#30340;&#22823;&#37327;&#27880;&#37322;&#25991;&#26412;&#65292;&#20854;&#21019;&#24314;&#21487;&#33021;&#32791;&#26102;&#19988;&#26114;&#36149;&#12290;&#27492;&#22806;&#65292;&#25552;&#21462;&#26032;&#23454;&#20307;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#39069;&#22806;&#30340;&#27880;&#37322;&#20219;&#21153;&#21644;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;NER&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#23558;&#22810;&#31867;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20803;&#26631;&#35760;&#20998;&#31867;&#65288;&#26631;&#35760;&#21253;&#21547;&#25628;&#32034;&#30340;&#23454;&#20307;&#25110;&#19981;&#21253;&#21547;&#25628;&#32034;&#30340;&#23454;&#20307;&#65289;&#65292;&#24182;&#22312;&#26356;&#22810;&#30340;&#25968;&#25454;&#38598;&#21644;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20174;&#32780;&#21487;&#23398;&#20064;&#21040;&#32473;&#23450;&#21644;&#28508;&#22312;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#22312;9&#31181;&#19981;&#21516;&#30340;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#19978;&#65292;&#25105;&#20204;&#22312;&#38646;&#26679;&#26412;NER&#12289;&#19968;&#27425;&#26679;&#26412;NER&#12289;10&#27425;&#26679;&#26412;NER&#21644;100&#27425;&#26679;&#26412;NER&#19978;&#23454;&#29616;&#20102;&#24179;&#22343;F1&#24471;&#20998;&#20998;&#21035;&#20026;35.44&#65285;&#12289;50.10&#65285;&#12289;69.94&#65285;&#21644;79.51&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised named entity recognition (NER) in the biomedical domain is dependent on large sets of annotated texts with the given named entities, whose creation can be time-consuming and expensive. Furthermore, the extraction of new entities often requires conducting additional annotation tasks and retraining the model. To address these challenges, this paper proposes a transformer-based method for zero- and few-shot NER in the biomedical domain. The method is based on transforming the task of multi-class token classification into binary token classification (token contains the searched entity or does not contain the searched entity) and pre-training on a larger amount of datasets and biomedical entities, from where the method can learn semantic relations between the given and potential classes. We have achieved average F1 scores of 35.44% for zero-shot NER, 50.10% for one-shot NER, 69.94% for 10-shot NER, and 79.51% for 100-shot NER on 9 diverse evaluated biomedical entities with PubMed
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#36817;&#24180;&#26469;&#33041;&#30005;&#22270;&#31995;&#32479;&#20013;&#21487;&#35299;&#37322;&#21644;&#40065;&#26834;&#30340;AI&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#20854;&#20013;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#35299;&#37322;&#24615;&#20998;&#31867;&#27861;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#40065;&#26834;AI&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.10755</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#21644;&#40065;&#26834;&#30340;&#33041;&#30005;&#22270;AI&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Interpretable and Robust AI in EEG Systems: A Survey. (arXiv:2304.10755v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10755
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#36817;&#24180;&#26469;&#33041;&#30005;&#22270;&#31995;&#32479;&#20013;&#21487;&#35299;&#37322;&#21644;&#40065;&#26834;&#30340;AI&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#20854;&#20013;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#35299;&#37322;&#24615;&#20998;&#31867;&#27861;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#40065;&#26834;AI&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#30340;&#23494;&#20999;&#32806;&#21512;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#20154;&#26426;&#20132;&#20114;&#65288;HCI&#65289;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;EEG&#31995;&#32479;&#65292;&#22522;&#20110;AI&#30340;EEG&#31995;&#32479;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#21464;&#24471;&#23588;&#20026;&#20851;&#38190;&#12290;&#21487;&#35299;&#37322;&#24615;&#33021;&#22815;&#38416;&#37322;AI&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#26426;&#21046;&#65292;&#22240;&#27492;&#21487;&#20197;&#33719;&#24471;&#29992;&#25143;&#30340;&#20449;&#20219;&#12290;&#40065;&#26834;&#24615;&#21017;&#21453;&#26144;&#20102;AI&#23545;&#25239;&#25915;&#20987;&#21644;&#25200;&#21160;&#30340;&#21487;&#38752;&#24615;&#65292;&#36825;&#23545;&#20110;&#25935;&#24863;&#21644;&#33030;&#24369;&#30340;EEG&#20449;&#21495;&#26469;&#35828;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22240;&#27492;&#65292;EEG&#31995;&#32479;&#20013;AI&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#24182;&#19988;&#26368;&#36817;&#30340;&#30740;&#31350;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#36825;&#19968;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#20173;&#28982;&#27809;&#26377;&#32508;&#36848;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#24615;&#20998;&#31867;&#27861;&#65292;&#36890;&#36807;&#29305;&#24449;&#21270;&#27169;&#22411;&#12289;&#25968;&#25454;&#21644;&#36755;&#20986;&#35299;&#37322;&#24615;&#65292;&#24635;&#32467;&#20102;&#33041;&#30005;&#22270;&#31995;&#32479;&#20013;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#30340;AI&#25216;&#26415;&#65292;&#24182;&#35814;&#32454;&#20171;&#32461;&#20102;&#40065;&#26834;AI&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#19968;&#39046;&#22495;&#26410;&#26469;&#30340;&#26041;&#21521;&#21644;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The close coupling of artificial intelligence (AI) and electroencephalography (EEG) has substantially advanced human-computer interaction (HCI) technologies in the AI era. Different from traditional EEG systems, the interpretability and robustness of AI-based EEG systems are becoming particularly crucial. The interpretability clarifies the inner working mechanisms of AI models and thus can gain the trust of users. The robustness reflects the AI's reliability against attacks and perturbations, which is essential for sensitive and fragile EEG signals. Thus the interpretability and robustness of AI in EEG systems have attracted increasing attention, and their research has achieved great progress recently. However, there is still no survey covering recent advances in this field. In this paper, we present the first comprehensive survey and summarize the interpretable and robust AI techniques for EEG systems. Specifically, we first propose a taxonomy of interpretability by characterizing it 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; Text2Seg &#30340;&#36965;&#24863;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#27969;&#31243;&#65292;&#21033;&#29992;&#22810;&#20010;&#22522;&#30784;&#27169;&#22411;&#21644;&#25991;&#26412;&#24341;&#23548;&#65292;&#21462;&#24471;&#20102;&#21021;&#27493;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.10597</link><description>&lt;p&gt;
Text2Seg: &#36890;&#36807;&#25991;&#26412;&#24341;&#23548;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#36965;&#24863;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Text2Seg: Remote Sensing Image Semantic Segmentation via Text-Guided Visual Foundation Models. (arXiv:2304.10597v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; Text2Seg &#30340;&#36965;&#24863;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#27969;&#31243;&#65292;&#21033;&#29992;&#22810;&#20010;&#22522;&#30784;&#27169;&#22411;&#21644;&#25991;&#26412;&#24341;&#23548;&#65292;&#21462;&#24471;&#20102;&#21021;&#27493;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#65292;&#22914; GPT-4 &#21644; LLaMA&#65292;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#26041;&#26696;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#21560;&#24341;&#20102;&#22823;&#37327;&#20851;&#27880;&#12290;&#31867;&#20284;&#22320;&#65292;&#22312;&#35270;&#35273;&#23398;&#20064;&#39046;&#22495;&#65292;Grounding DINO &#21644; Segment Anything Model&#65288;SAM&#65289;&#31561;&#27169;&#22411;&#22312;&#24320;&#25918;&#24335;&#26816;&#27979;&#21644;&#23454;&#20363;&#20998;&#21106;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#36827;&#27493;&#12290;&#26412;&#30740;&#31350;&#19987;&#27880;&#20110;&#36965;&#24863;&#39046;&#22495;&#65292;&#20854;&#20013;&#22270;&#29255;&#19982;&#20256;&#32479;&#22330;&#26223;&#20013;&#30340;&#22270;&#29255;&#26126;&#26174;&#19981;&#21516;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#27969;&#31243;&#65292;&#21033;&#29992;&#22810;&#20010; FMs&#65292;&#20197;&#25991;&#26412;&#25552;&#31034;&#20026;&#25351;&#23548;&#65292;&#20419;&#36827;&#36965;&#24863;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026; Text2Seg &#12290;&#35813;&#31649;&#36947;&#22312;&#22810;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#36965;&#24863;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20379;&#21021;&#27493;&#32467;&#26524;&#20197;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in foundation models (FMs), such as GPT-4 and LLaMA, have attracted significant attention due to their exceptional performance in zero-shot learning scenarios. Similarly, in the field of visual learning, models like Grounding DINO and the Segment Anything Model (SAM) have exhibited remarkable progress in open-set detection and instance segmentation tasks. It is undeniable that these FMs will profoundly impact a wide range of real-world visual learning tasks, ushering in a new paradigm shift for developing such models. In this study, we concentrate on the remote sensing domain, where the images are notably dissimilar from those in conventional scenarios. We developed a pipeline that leverages multiple FMs to facilitate remote sensing image semantic segmentation tasks guided by text prompt, which we denote as Text2Seg. The pipeline is benchmarked on several widely-used remote sensing datasets, and we present preliminary results to demonstrate its effectiveness. Throug
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;Python&#30340;&#25945;&#31243;&#65292;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;MCMC&#26041;&#27861;&#24212;&#29992;&#65292;&#36890;&#36807;&#25945;&#31243;&#20351;&#24471;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#32773;&#33021;&#22815;&#26356;&#22909;&#22320;&#24212;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.02595</link><description>&lt;p&gt;
&#22522;&#20110;MCMC&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65306;&#22522;&#20110;Python&#30340;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
Bayesian neural networks via MCMC: a Python-based tutorial. (arXiv:2304.02595v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;Python&#30340;&#25945;&#31243;&#65292;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;MCMC&#26041;&#27861;&#24212;&#29992;&#65292;&#36890;&#36807;&#25945;&#31243;&#20351;&#24471;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#32773;&#33021;&#22815;&#26356;&#22909;&#22320;&#24212;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#25512;&#26029;&#20026;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25552;&#20379;&#20102;&#21442;&#25968;&#20272;&#35745;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26041;&#27861;&#12290;&#21464;&#20998;&#25512;&#26029;&#21644;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#65288;MCMC&#65289;&#37319;&#26679;&#25216;&#26415;&#29992;&#20110;&#23454;&#29616;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#22312;&#36807;&#21435;&#19977;&#21313;&#24180;&#20013;&#65292;MCMC&#26041;&#27861;&#22312;&#36866;&#24212;&#26356;&#22823;&#30340;&#27169;&#22411;&#65288;&#22914;&#28145;&#24230;&#23398;&#20064;&#65289;&#21644;&#22823;&#25968;&#25454;&#38382;&#39064;&#26041;&#38754;&#38754;&#20020;&#20102;&#35768;&#22810;&#25361;&#25112;&#12290;&#21253;&#25324;&#26799;&#24230;&#30340;&#39640;&#32423;&#25552;&#35758;&#65288;&#20363;&#22914;Langevin&#25552;&#35758;&#20998;&#24067;&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;MCMC&#37319;&#26679;&#20013;&#30340;&#19968;&#20123;&#38480;&#21046;&#30340;&#26041;&#27861;&#65292;&#27492;&#22806;&#65292;MCMC&#26041;&#27861;&#36890;&#24120;&#34987;&#38480;&#21046;&#22312;&#32479;&#35745;&#23398;&#23478;&#30340;&#20351;&#29992;&#33539;&#22260;&#20869;&#65292;&#24182;&#19988;&#20173;&#19981;&#26159;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#20154;&#21592;&#30340;&#20027;&#27969;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;MCMC&#26041;&#27861;&#30340;&#25945;&#31243;&#65292;&#28085;&#30422;&#20102;&#31616;&#21333;&#30340;&#36125;&#21494;&#26031;&#32447;&#24615;&#21644;&#36923;&#36753;&#27169;&#22411;&#65292;&#20197;&#21450;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#20010;&#25945;&#31243;&#30340;&#30446;&#30340;&#26159;&#36890;&#36807;&#32534;&#30721;&#26469;&#24357;&#21512;&#29702;&#35770;&#21644;&#23454;&#29616;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#37492;&#20110;&#24403;&#21069;MCMC&#26041;&#27861;&#30340;&#26222;&#21450;&#31243;&#24230;&#20173;&#28982;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian inference provides a methodology for parameter estimation and uncertainty quantification in machine learning and deep learning methods. Variational inference and Markov Chain Monte-Carlo (MCMC) sampling techniques are used to implement Bayesian inference. In the past three decades, MCMC methods have faced a number of challenges in being adapted to larger models (such as in deep learning) and big data problems. Advanced proposals that incorporate gradients, such as a Langevin proposal distribution, provide a means to address some of the limitations of MCMC sampling for Bayesian neural networks. Furthermore, MCMC methods have typically been constrained to use by statisticians and are still not prominent among deep learning researchers. We present a tutorial for MCMC methods that covers simple Bayesian linear and logistic models, and Bayesian neural networks. The aim of this tutorial is to bridge the gap between theory and implementation via coding, given a general sparsity of li
&lt;/p&gt;</description></item></channel></rss>