<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#20102;MX-ARM&#65292;&#19968;&#31181;&#22522;&#20110;AI&#30340;&#30142;&#30149;&#35786;&#26029;&#27169;&#22411;&#65292;&#21033;&#29992;&#21516;&#26102;&#21151;&#33021;PET/MR&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21516;&#26102;&#25509;&#21463;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#36755;&#20837;&#65292;&#20855;&#26377;&#21019;&#26032;&#30340;&#27169;&#24577;&#20998;&#31163;&#21644;&#37325;&#26500;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.20058</link><description>&lt;p&gt;
&#21033;&#29992;&#21516;&#26102;&#21151;&#33021;PET/MR&#21644;&#28145;&#24230;&#25972;&#21512;&#30340;&#33041;&#20195;&#35874;&#12289;&#34880;&#28082;&#21160;&#21147;&#23398;&#21644;&#28748;&#27880;&#32593;&#32476;&#24443;&#24213;&#25913;&#21464;&#30142;&#30149;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Revolutionizing Disease Diagnosis with simultaneous functional PET/MR and Deeply Integrated Brain Metabolic, Hemodynamic, and Perfusion Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20058
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MX-ARM&#65292;&#19968;&#31181;&#22522;&#20110;AI&#30340;&#30142;&#30149;&#35786;&#26029;&#27169;&#22411;&#65292;&#21033;&#29992;&#21516;&#26102;&#21151;&#33021;PET/MR&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21516;&#26102;&#25509;&#21463;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#36755;&#20837;&#65292;&#20855;&#26377;&#21019;&#26032;&#30340;&#27169;&#24577;&#20998;&#31163;&#21644;&#37325;&#26500;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#26102;&#21151;&#33021;PET/MR&#65288;sf-PET/MR&#65289;&#26159;&#19968;&#31181;&#23574;&#31471;&#30340;&#22810;&#27169;&#24335;&#31070;&#32463;&#24433;&#20687;&#25216;&#26415;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#20250;&#65292;&#21487;&#20197;&#21516;&#26102;&#30417;&#27979;&#21644;&#25972;&#21512;&#30001;&#26102;&#31354;&#21327;&#21464;&#20195;&#35874;&#27963;&#21160;&#12289;&#31070;&#32463;&#27963;&#21160;&#21644;&#33041;&#34880;&#27969;&#65288;&#28748;&#27880;&#65289;&#26500;&#24314;&#30340;&#22810;&#26041;&#38754;&#22823;&#33041;&#32593;&#32476;&#12290;&#34429;&#28982;&#22312;&#31185;&#23398;/&#20020;&#24202;&#20215;&#20540;&#19978;&#24456;&#39640;&#65292;&#20294;PET/MR&#30828;&#20214;&#30340;&#21487;&#21450;&#24615;&#19981;&#36275;&#38459;&#30861;&#20102;&#20854;&#24212;&#29992;&#65292;&#26356;&#19981;&#29992;&#35828;&#29616;&#20195;&#22522;&#20110;AI&#30340;PET/MR&#34701;&#21512;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#20010;&#22522;&#20110;AI&#30340;&#20020;&#24202;&#21487;&#34892;&#30142;&#30149;&#35786;&#26029;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22522;&#20110;&#20840;&#38754;&#30340;sf-PET/MR&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20855;&#26377;&#20801;&#35768;&#21333;&#27169;&#24577;&#36755;&#20837;&#65288;&#20363;&#22914;&#65292;&#20165;PET&#65289;&#20197;&#21450;&#24378;&#21046;&#22810;&#27169;&#24577;&#20934;&#30830;&#24615;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MX-ARM&#65292;&#19968;&#31181;&#22810;&#27169;&#24577;&#19987;&#23478;&#28151;&#21512;&#23545;&#40784;&#21644;&#37325;&#26500;&#27169;&#22411;&#12290;&#23427;&#26159;&#27169;&#24577;&#21487;&#20998;&#31163;&#21644;&#21487;&#20132;&#25442;&#30340;&#65292;&#21160;&#24577;&#20998;&#37197;&#19981;&#21516;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;"&#28151;&#21512;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20058v1 Announce Type: cross  Abstract: Simultaneous functional PET/MR (sf-PET/MR) presents a cutting-edge multimodal neuroimaging technique. It provides an unprecedented opportunity for concurrently monitoring and integrating multifaceted brain networks built by spatiotemporally covaried metabolic activity, neural activity, and cerebral blood flow (perfusion). Albeit high scientific/clinical values, short in hardware accessibility of PET/MR hinders its applications, let alone modern AI-based PET/MR fusion models. Our objective is to develop a clinically feasible AI-based disease diagnosis model trained on comprehensive sf-PET/MR data with the power of, during inferencing, allowing single modality input (e.g., PET only) as well as enforcing multimodal-based accuracy. To this end, we propose MX-ARM, a multimodal MiXture-of-experts Alignment and Reconstruction Model. It is modality detachable and exchangeable, allocating different multi-layer perceptrons dynamically ("mixture 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#31867;&#20013;&#24515;&#30340;&#24314;&#31569;&#26426;&#22120;&#20154;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#21161;&#25163;&#26426;&#22120;&#20154;&#20026;&#26408;&#24037;&#21171;&#21160;&#32773;&#25552;&#20379;&#29615;&#22659;&#19978;&#19979;&#25991;&#21327;&#21161;&#65292;&#25512;&#36827;&#20102;&#26426;&#22120;&#20154;&#22312;&#24314;&#31569;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.19060</link><description>&lt;p&gt;
&#20154;&#31867;&#20013;&#24515;&#26045;&#24037;&#26426;&#22120;&#20154;&#65306;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#21161;&#25163;&#26426;&#22120;&#20154;&#20026;&#26408;&#24037;&#21171;&#21160;&#32773;&#25552;&#20379;&#29615;&#22659;&#19978;&#19979;&#25991;&#21327;&#21161;
&lt;/p&gt;
&lt;p&gt;
Towards Human-Centered Construction Robotics: An RL-Driven Companion Robot For Contextually Assisting Carpentry Workers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#31867;&#20013;&#24515;&#30340;&#24314;&#31569;&#26426;&#22120;&#20154;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#21161;&#25163;&#26426;&#22120;&#20154;&#20026;&#26408;&#24037;&#21171;&#21160;&#32773;&#25552;&#20379;&#29615;&#22659;&#19978;&#19979;&#25991;&#21327;&#21161;&#65292;&#25512;&#36827;&#20102;&#26426;&#22120;&#20154;&#22312;&#24314;&#31569;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20010;&#20805;&#28385;&#27963;&#21147;&#30340;&#24314;&#31569;&#34892;&#19994;&#20013;&#65292;&#20256;&#32479;&#30340;&#26426;&#22120;&#20154;&#38598;&#25104;&#20027;&#35201;&#38598;&#20013;&#22312;&#33258;&#21160;&#21270;&#29305;&#23450;&#20219;&#21153;&#65292;&#36890;&#24120;&#24573;&#30053;&#20102;&#24314;&#31569;&#24037;&#20316;&#27969;&#31243;&#20013;&#20154;&#31867;&#22240;&#32032;&#30340;&#22797;&#26434;&#24615;&#21644;&#21464;&#21270;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20154;&#20026;&#26412;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#8220;&#24037;&#20316;&#20276;&#20387;&#28459;&#28216;&#22120;&#8221;&#65292;&#26088;&#22312;&#21327;&#21161;&#24314;&#31569;&#24037;&#20154;&#23436;&#25104;&#20854;&#29616;&#26377;&#23454;&#36341;&#65292;&#26088;&#22312;&#22686;&#24378;&#23433;&#20840;&#24615;&#21644;&#24037;&#20316;&#27969;&#31243;&#30340;&#27969;&#30021;&#24615;&#65292;&#21516;&#26102;&#23562;&#37325;&#24314;&#31569;&#21171;&#21160;&#30340;&#25216;&#26415;&#24615;&#36136;&#12290;&#25105;&#20204;&#23545;&#22312;&#26408;&#24037;&#27169;&#26495;&#24037;&#31243;&#20013;&#37096;&#32626;&#26426;&#22120;&#20154;&#31995;&#32479;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#19968;&#20010;&#21407;&#22411;&#65292;&#36890;&#36807;&#29615;&#22659;&#30456;&#20851;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#39537;&#21160;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#37325;&#28857;&#24378;&#35843;&#20102;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#26426;&#21160;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#33298;&#36866;&#30340;&#24037;&#20154;-&#26426;&#22120;&#20154;&#21327;&#20316;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25512;&#36827;&#20102;&#26426;&#22120;&#20154;&#22312;&#24314;&#31569;&#20013;&#30340;&#24212;&#29992;&#65292;&#20513;&#23548;&#21327;&#20316;&#27169;&#22411;&#65292;&#20854;&#20013;&#33258;&#36866;&#24212;&#26426;&#22120;&#20154;&#25903;&#25345;&#32780;&#19981;&#26159;&#21462;&#20195;&#20154;&#31867;&#65292;&#24378;&#35843;&#20102;&#20132;&#20114;&#24335;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19060v1 Announce Type: cross  Abstract: In the dynamic construction industry, traditional robotic integration has primarily focused on automating specific tasks, often overlooking the complexity and variability of human aspects in construction workflows. This paper introduces a human-centered approach with a ``work companion rover" designed to assist construction workers within their existing practices, aiming to enhance safety and workflow fluency while respecting construction labor's skilled nature. We conduct an in-depth study on deploying a robotic system in carpentry formwork, showcasing a prototype that emphasizes mobility, safety, and comfortable worker-robot collaboration in dynamic environments through a contextual Reinforcement Learning (RL)-driven modular framework. Our research advances robotic applications in construction, advocating for collaborative models where adaptive robots support rather than replace humans, underscoring the potential for an interactive a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;CATMO&#65292;&#36890;&#36807;&#25972;&#21512;&#29289;&#29702;&#25509;&#35302;&#20449;&#24687;&#65292;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#35270;&#35273;&#33258;&#28982;&#19988;&#29289;&#29702;&#21512;&#29702;&#30340;3D&#20154;&#20307;&#21160;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.15709</link><description>&lt;p&gt;
&#20174;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;&#32771;&#34385;&#25509;&#35302;&#30340;&#20154;&#20307;&#21160;&#20316;
&lt;/p&gt;
&lt;p&gt;
Contact-aware Human Motion Generation from Textual Descriptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;CATMO&#65292;&#36890;&#36807;&#25972;&#21512;&#29289;&#29702;&#25509;&#35302;&#20449;&#24687;&#65292;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#35270;&#35273;&#33258;&#28982;&#19988;&#29289;&#29702;&#21512;&#29702;&#30340;3D&#20154;&#20307;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#20174;&#25991;&#26412;&#29983;&#25104;3D&#20132;&#20114;&#24335;&#20154;&#20307;&#21160;&#20316;&#30340;&#38382;&#39064;&#12290;&#32473;&#23450;&#25551;&#36848;&#20102;&#19981;&#21516;&#36523;&#20307;&#37096;&#20301;&#25509;&#35302;&#29289;&#20307;&#21160;&#20316;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#25105;&#20204;&#32508;&#21512;&#29983;&#25104;&#35270;&#35273;&#33258;&#28982;&#19988;&#29289;&#29702;&#21512;&#29702;&#30340;3D&#20154;&#20307;&#23039;&#21183;&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#20219;&#21153;&#23384;&#22312;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#21363;&#22312;&#21160;&#20316;&#21644;&#25991;&#26412;&#25551;&#36848;&#20013;&#23545;&#29289;&#29702;&#25509;&#35302;&#30340;&#20114;&#21160;&#32771;&#34385;&#19981;&#36275;&#65292;&#23548;&#33268;&#24207;&#21015;&#19981;&#33258;&#28982;&#19988;&#19981;&#21512;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;RICH-CAT&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#34920;&#31034;&#20174;RICH&#25968;&#25454;&#38598;&#26500;&#24314;&#30340;&#8220;&#32771;&#34385;&#25509;&#35302;&#8221;&#30340;&#25991;&#26412;&#12290;RICH-CAT&#21253;&#25324;&#39640;&#36136;&#37327;&#21160;&#20316;&#12289;&#20934;&#30830;&#30340;&#20154;-&#29289;&#25509;&#35302;&#26631;&#31614;&#21644;&#35814;&#32454;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#28085;&#30422;&#20102;26&#31181;&#23460;&#20869;/&#23460;&#22806;&#21160;&#20316;&#30340;8500&#22810;&#23545;&#21160;&#20316;-&#25991;&#26412;&#37197;&#23545;&#12290;&#21033;&#29992;RICH-CAT&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CATMO&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#25991;&#26412;&#39537;&#21160;&#30340;&#20132;&#20114;&#24335;&#20154;&#20307;&#21160;&#20316;&#21512;&#25104;&#65292;&#26126;&#30830;&#25972;&#21512;&#20102;&#29289;&#29702;&#25509;&#35302;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15709v1 Announce Type: cross  Abstract: This paper addresses the problem of generating 3D interactive human motion from text. Given a textual description depicting the actions of different body parts in contact with objects, we synthesize sequences of 3D body poses that are visually natural and physically plausible. Yet, this task poses a significant challenge due to the inadequate consideration of interactions by physical contacts in both motion and textual descriptions, leading to unnatural and implausible sequences. To tackle this challenge, we create a novel dataset named RICH-CAT, representing ``Contact-Aware Texts'' constructed from the RICH dataset. RICH-CAT comprises high-quality motion, accurate human-object contact labels, and detailed textual descriptions, encompassing over 8,500 motion-text pairs across 26 indoor/outdoor actions. Leveraging RICH-CAT, we propose a novel approach named CATMO for text-driven interactive human motion synthesis that explicitly integra
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;StateFlow&#30340;&#26032;&#39062;LLM&#20219;&#21153;&#35299;&#20915;&#33539;&#24335;&#65292;&#23558;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#27010;&#24565;&#21270;&#20026;&#29366;&#24577;&#26426;&#65292;&#36890;&#36807;&#29366;&#24577;&#36716;&#25442;&#30830;&#20445;LLM&#21709;&#24212;&#30340;&#28165;&#26224;&#36319;&#36394;&#21644;&#31649;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.11322</link><description>&lt;p&gt;
&#20351;&#29992;StateFlow&#22686;&#24378;LLM&#20219;&#21153;&#35299;&#20915;&#33021;&#21147;&#36890;&#36807;&#29366;&#24577;&#39537;&#21160;&#24037;&#20316;&#27969;
&lt;/p&gt;
&lt;p&gt;
StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11322
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;StateFlow&#30340;&#26032;&#39062;LLM&#20219;&#21153;&#35299;&#20915;&#33539;&#24335;&#65292;&#23558;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#27010;&#24565;&#21270;&#20026;&#29366;&#24577;&#26426;&#65292;&#36890;&#36807;&#29366;&#24577;&#36716;&#25442;&#30830;&#20445;LLM&#21709;&#24212;&#30340;&#28165;&#26224;&#36319;&#36394;&#21644;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#36235;&#21183;&#26085;&#30410;&#26126;&#26174;&#65292;&#20363;&#22914;&#38656;&#35201;&#19968;&#31995;&#21015;&#25805;&#20316;&#21644;&#19982;&#24037;&#20855;&#29615;&#22659;&#21160;&#24577;&#20132;&#20114;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;StateFlow&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;LLM&#30340;&#20219;&#21153;&#27714;&#35299;&#33539;&#24335;&#65292;&#23558;&#30001;LLM&#25903;&#25345;&#30340;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#36807;&#31243;&#27010;&#24565;&#21270;&#20026;&#29366;&#24577;&#26426;&#12290;&#36890;&#36807;&#27491;&#30830;&#26500;&#24314;&#29366;&#24577;&#21644;&#23450;&#20041;&#29366;&#24577;&#36716;&#25442;&#65292;StateFlow&#30830;&#23450;&#20102;&#20219;&#21153;&#27714;&#35299;&#30340;&#36827;&#23637;&#65292;&#30830;&#20445;&#28165;&#26224;&#36319;&#36394;&#21644;&#31649;&#29702;LLM&#22312;&#25972;&#20010;&#20219;&#21153;&#27714;&#35299;&#36807;&#31243;&#20013;&#30340;&#21709;&#24212;&#12290;&#22312;&#27599;&#20010;&#29366;&#24577;&#20013;&#65292;StateFlow&#20801;&#35768;&#25191;&#34892;&#19968;&#31995;&#21015;&#21160;&#20316;&#65292;&#19981;&#20165;&#21253;&#25324;&#26681;&#25454;&#29305;&#23450;&#25552;&#31034;&#25351;&#23548;&#29983;&#25104;LLM&#21709;&#24212;&#65292;&#36824;&#21253;&#25324;&#26681;&#25454;&#38656;&#35201;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#12290;&#29366;&#24577;&#36716;&#25442;&#30001;LLM&#20570;&#20986;&#30340;&#29305;&#23450;&#35268;&#21017;&#25110;&#20915;&#31574;&#25511;&#21046;&#65292;&#20801;&#35768;&#36890;&#36807;&#20219;&#21153;&#30340;&#39044;&#23450;&#20041;StateFlow&#27169;&#22411;&#21160;&#24577;&#33258;&#36866;&#24212;&#22320;&#36827;&#34892;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11322v1 Announce Type: cross  Abstract: It is a notable trend to use Large Language Models (LLMs) to tackle complex tasks, e.g., tasks that require a sequence of actions and dynamic interaction with tools and environments. In this paper, we propose StateFlow, a novel LLM-based task-solving paradigm that conceptualizes complex task-solving processes backed by LLMs as state machines. With proper construction of states and definition of state transitions, StateFlow grounds the progress of task-solving, ensuring clear tracking and management of LLMs' responses throughout the task-solving process. Within each state, StateFlow allows execution of a series of actions, involving not only the generation of LLM's responses guided by a specific prompt, but also the utilization of external tools as needed. State transitions are controlled by specific rules or decisions made by the LLM, allowing for a dynamic and adaptive progression through the task's pre-defined StateFlow model. Evalua
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#38750;&#32908;&#23618;&#20405;&#34989;&#24615;&#33152;&#33009;&#30284;&#22797;&#21457;&#39044;&#27979;&#20013;&#20855;&#26377;&#28508;&#22312;&#20316;&#29992;&#65292;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#38477;&#20302;&#27835;&#30103;&#25104;&#26412;&#65292;&#24182;&#26377;&#25928;&#35268;&#21010;&#27835;&#30103;&#26041;&#26696;</title><link>https://arxiv.org/abs/2403.10586</link><description>&lt;p&gt;
&#20174;&#31639;&#27861;&#21040;&#32467;&#26524;&#65306;&#23457;&#35270;&#20154;&#24037;&#26234;&#33021;&#22312;&#38750;&#32908;&#23618;&#20405;&#34989;&#24615;&#33152;&#33009;&#30284;&#22797;&#21457;&#39044;&#27979;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
From Algorithms to Outcomes: Reviewing AI's Role in Non-Muscle-Invasive Bladder Cancer Recurrence Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10586
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#38750;&#32908;&#23618;&#20405;&#34989;&#24615;&#33152;&#33009;&#30284;&#22797;&#21457;&#39044;&#27979;&#20013;&#20855;&#26377;&#28508;&#22312;&#20316;&#29992;&#65292;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#38477;&#20302;&#27835;&#30103;&#25104;&#26412;&#65292;&#24182;&#26377;&#25928;&#35268;&#21010;&#27835;&#30103;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33152;&#33009;&#30284;&#26159;&#33521;&#22269;&#27599;&#22825;&#36896;&#25104;15&#20154;&#27515;&#20129;&#30340;&#39046;&#20808;&#27852;&#23615;&#36947;&#30284;&#30151;&#12290;&#36825;&#31181;&#30284;&#30151;&#20027;&#35201;&#34920;&#29616;&#20026;&#38750;&#32908;&#23618;&#20405;&#34989;&#24615;&#33152;&#33009;&#30284;&#65288;NMIBC&#65289;&#65292;&#20854;&#29305;&#28857;&#26159;&#32959;&#30244;&#36824;&#26410;&#28183;&#36879;&#21040;&#33152;&#33009;&#22721;&#30340;&#32908;&#32905;&#23618;&#12290; NMIBC&#30340;&#22797;&#21457;&#29575;&#38750;&#24120;&#39640;&#65292;&#36798;&#21040;70-80&#65285;&#65292;&#22240;&#27492;&#27835;&#30103;&#25104;&#26412;&#26368;&#39640;&#12290;&#30446;&#21069;&#29992;&#20110;&#39044;&#27979;&#22797;&#21457;&#30340;&#24037;&#20855;&#20351;&#29992;&#35780;&#20998;&#31995;&#32479;&#26469;&#39640;&#20272;&#39118;&#38505;&#65292;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#20934;&#30830;&#24615;&#12290;&#23545;&#22797;&#21457;&#30340;&#19981;&#20934;&#30830;&#21644;&#24310;&#36831;&#39044;&#27979;&#26174;&#33879;&#25552;&#39640;&#20102;&#27515;&#20129;&#30340;&#21487;&#33021;&#24615;&#12290;&#22240;&#27492;&#65292;&#20934;&#30830;&#39044;&#27979;&#22797;&#21457;&#23545;&#20110;&#25104;&#26412;&#25928;&#30410;&#30340;&#31649;&#29702;&#21644;&#27835;&#30103;&#35745;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#23601;&#26159;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#20986;&#29616;&#30340;&#22320;&#26041;&#65292;&#36890;&#36807;&#21033;&#29992;&#20998;&#23376;&#21644;&#20020;&#24202;&#25968;&#25454;&#39044;&#27979;NMIBC&#22797;&#21457;&#65292;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#26412;&#27425;&#23457;&#26597;&#23545;&#39044;&#27979;NMIBC&#22797;&#21457;&#30340;ML&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#35780;&#20272;&#20351;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10586v1 Announce Type: cross  Abstract: Bladder cancer, the leading urinary tract cancer, is responsible for 15 deaths daily in the UK. This cancer predominantly manifests as non-muscle-invasive bladder cancer (NMIBC), characterised by tumours not yet penetrating the muscle layer of the bladder wall. NMIBC is plagued by a very high recurrence rate of 70-80% and hence the costliest treatments. Current tools for predicting recurrence use scoring systems that overestimate risk and have poor accuracy. Inaccurate and delayed prediction of recurrence significantly elevates the likelihood of mortality. Accurate prediction of recurrence is hence vital for cost-effective management and treatment planning. This is where Machine learning (ML) techniques have emerged as a promising approach for predicting NMIBC recurrence by leveraging molecular and clinical data. This review provides a comprehensive analysis of ML approaches for predicting NMIBC recurrence. Our systematic evaluation de
&lt;/p&gt;</description></item><item><title>CoReEcho&#25552;&#20986;&#20102;&#38024;&#23545;&#30452;&#25509;EF&#22238;&#24402;&#30340;&#36830;&#32493;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#26368;&#22823;&#30340;&#36229;&#22768;&#24515;&#21160;&#22270;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#36234;&#12290;</title><link>https://arxiv.org/abs/2403.10164</link><description>&lt;p&gt;
CoReEcho: 2D+&#26102;&#38388;&#36229;&#22768;&#24515;&#21160;&#22270;&#20998;&#26512;&#30340;&#36830;&#32493;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CoReEcho: Continuous Representation Learning for 2D+time Echocardiography Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10164
&lt;/p&gt;
&lt;p&gt;
CoReEcho&#25552;&#20986;&#20102;&#38024;&#23545;&#30452;&#25509;EF&#22238;&#24402;&#30340;&#36830;&#32493;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#26368;&#22823;&#30340;&#36229;&#22768;&#24515;&#21160;&#22270;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19968;&#30452;&#22312;&#19981;&#21516;&#27169;&#24577;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#26041;&#38754;&#21462;&#24471;&#36827;&#23637;&#65292;&#21253;&#25324;&#36229;&#22768;&#24515;&#21160;&#22270;&#65292;&#22312;&#25552;&#20379;&#20840;&#38754;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#27969;&#27700;&#32447;&#30340;&#21516;&#26102;&#12290;&#28982;&#32780;&#65292;&#31471;&#21040;&#31471;&#35757;&#32451;&#27969;&#27700;&#32447;&#20351;&#24471;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#38590;&#20197;&#35299;&#37322;&#65292;&#24182;&#19988;&#21487;&#33021;&#26080;&#27861;&#25429;&#33719;&#36229;&#22768;&#24515;&#21160;&#22270;&#29255;&#27573;&#20043;&#38388;&#30340;&#36830;&#32493;&#20851;&#31995;&#65292;&#23548;&#33268;&#23384;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#21487;&#33021;&#23545;&#27867;&#21270;&#33021;&#21147;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CoReEcho&#65292;&#36825;&#26159;&#19968;&#20010;&#24378;&#35843;&#38024;&#23545;&#30452;&#25509;EF&#22238;&#24402;&#30340;&#36830;&#32493;&#34920;&#31034;&#30340;&#26032;&#22411;&#35757;&#32451;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;CoReEcho&#65306;1&#65289;&#22312;&#26368;&#22823;&#30340;&#36229;&#22768;&#24515;&#21160;&#22270;&#25968;&#25454;&#38598;&#65288;EchoNet-Dynamic&#65289;&#19978;&#34920;&#29616;&#20248;&#20110;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65288;SOTA&#65289;&#65292;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#20026;3.90&#21644;R2 o
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10164v1 Announce Type: cross  Abstract: Deep learning (DL) models have been advancing automatic medical image analysis on various modalities, including echocardiography, by offering a comprehensive end-to-end training pipeline. This approach enables DL models to regress ejection fraction (EF) directly from 2D+time echocardiograms, resulting in superior performance. However, the end-to-end training pipeline makes the learned representations less explainable. The representations may also fail to capture the continuous relation among echocardiogram clips, indicating the existence of spurious correlations, which can negatively affect the generalization. To mitigate this issue, we propose CoReEcho, a novel training framework emphasizing continuous representations tailored for direct EF regression. Our extensive experiments demonstrate that CoReEcho: 1) outperforms the current state-of-the-art (SOTA) on the largest echocardiography dataset (EchoNet-Dynamic) with MAE of 3.90 &amp; R2 o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;CodeAttack&#26694;&#26550;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#65292;&#30740;&#31350;&#21457;&#29616;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#31561;&#26368;&#26032;&#27169;&#22411;&#23384;&#22312;&#20195;&#30721;&#36755;&#20837;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;</title><link>https://arxiv.org/abs/2403.07865</link><description>&lt;p&gt;
&#36890;&#36807;&#20195;&#30721;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Exploring Safety Generalization Challenges of Large Language Models via Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;CodeAttack&#26694;&#26550;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#65292;&#30740;&#31350;&#21457;&#29616;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#31561;&#26368;&#26032;&#27169;&#22411;&#23384;&#22312;&#20195;&#30721;&#36755;&#20837;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#24102;&#26469;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#23427;&#20204;&#28508;&#22312;&#35823;&#29992;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;CodeAttack&#65292;&#19968;&#20010;&#23558;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#36716;&#25442;&#20026;&#20195;&#30721;&#36755;&#20837;&#30340;&#26694;&#26550;&#65292;&#20026;&#27979;&#35797;LLMs&#30340;&#23433;&#20840;&#27867;&#21270;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29615;&#22659;&#12290;&#25105;&#20204;&#23545;&#21253;&#25324;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#22312;&#20869;&#30340;&#26368;&#26032;LLMs&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#20195;&#30721;&#36755;&#20837;&#23384;&#22312;&#20849;&#21516;&#30340;&#23433;&#20840;&#28431;&#27934;&#65306;CodeAttack&#22312;&#36229;&#36807;80%&#30340;&#26102;&#38388;&#20869;&#22987;&#32456;&#32469;&#36807;&#25152;&#26377;&#27169;&#22411;&#30340;&#23433;&#20840;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07865v1 Announce Type: cross  Abstract: The rapid advancement of Large Language Models (LLMs) has brought about remarkable capabilities in natural language processing but also raised concerns about their potential misuse. While strategies like supervised fine-tuning and reinforcement learning from human feedback have enhanced their safety, these methods primarily focus on natural languages, which may not generalize to other domains. This paper introduces CodeAttack, a framework that transforms natural language inputs into code inputs, presenting a novel environment for testing the safety generalization of LLMs. Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a common safety vulnerability of these models against code input: CodeAttack consistently bypasses the safety guardrails of all models more than 80\% of the time. Furthermore, we find that a larger distribution gap between CodeAttack and natural language leads to we
&lt;/p&gt;</description></item><item><title>Apollo&#39033;&#30446;&#24320;&#21457;&#20102;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65292;&#21019;&#24314;&#20102;&#20840;&#29699;&#20154;&#21475;61&#20159;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#24067;&#20102;&#21508;&#31181;&#23610;&#23544;&#30340;&#26368;&#20339;&#24615;&#33021;&#27169;&#22411;&#65292;&#20854;&#20013;Apollo-7B&#26159;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65292;&#21487;&#25913;&#21892;&#26356;&#22823;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.03640</link><description>&lt;p&gt;
Apollo&#65306;&#36731;&#37327;&#32423;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65306;&#35753;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#26222;&#24800;60&#20159;&#20154;
&lt;/p&gt;
&lt;p&gt;
Apollo: Lightweight Multilingual Medical LLMs towards Democratizing Medical AI to 6B People
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03640
&lt;/p&gt;
&lt;p&gt;
Apollo&#39033;&#30446;&#24320;&#21457;&#20102;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65292;&#21019;&#24314;&#20102;&#20840;&#29699;&#20154;&#21475;61&#20159;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#24067;&#20102;&#21508;&#31181;&#23610;&#23544;&#30340;&#26368;&#20339;&#24615;&#33021;&#27169;&#22411;&#65292;&#20854;&#20013;Apollo-7B&#26159;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#65292;&#21487;&#25913;&#21892;&#26356;&#22823;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20840;&#29699;&#21307;&#23398;&#30693;&#35782;&#30340;&#24222;&#22823;&#23384;&#20648;&#24211;&#20027;&#35201;&#26159;&#20197;&#33521;&#35821;&#20026;&#20027;&#65292;&#20294;&#22312;&#20256;&#36882;&#37327;&#36523;&#23450;&#21046;&#21307;&#30103;&#26381;&#21153;&#26041;&#38754;&#65292;&#26412;&#22320;&#35821;&#35328;&#23545;&#20110;&#22312;&#21307;&#30103;&#36164;&#28304;&#26377;&#38480;&#30340;&#22320;&#21306;&#23588;&#20026;&#37325;&#35201;&#12290;&#20026;&#20102;&#23558;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#25193;&#23637;&#21040;&#26356;&#24191;&#27867;&#30340;&#20154;&#32676;&#65292;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#28085;&#30422;&#20840;&#29699;61&#20159;&#20154;&#21475;&#30340;&#20845;&#31181;&#26368;&#24120;&#29992;&#35821;&#35328;&#30340;&#21307;&#23398;LLMs&#12290;&#36825;&#19968;&#21162;&#21147;&#26368;&#32456;&#20419;&#25104;&#20102;ApolloCorpora&#22810;&#35821;&#35328;&#21307;&#23398;&#25968;&#25454;&#38598;&#21644;XMedBench&#22522;&#20934;&#30340;&#21019;&#24314;&#12290;&#22312;&#22810;&#35821;&#35328;&#21307;&#23398;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#21457;&#24067;&#30340;Apollo&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#30456;&#23545;&#36739;&#23567;&#23610;&#23544;&#65288;&#21363;0.5B&#12289;1.8B&#12289;2B&#12289;6B&#21644;7B&#65289;&#19978;&#21462;&#24471;&#20102;&#19982;&#21516;&#31561;&#22823;&#23567;&#27169;&#22411;&#26368;&#20339;&#24615;&#33021;&#12290;&#29305;&#21035;&#22320;&#65292;Apollo-7B&#26159;&#36804;&#20170;&#20026;&#27490;&#36798;&#21040;70B&#30340;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;LLMs&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#36731;&#37327;&#32423;&#27169;&#22411;&#21487;&#29992;&#20110;&#22312;&#19981;&#38656;&#35201;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#25913;&#36827;&#36739;&#22823;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#21307;&#23398;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03640v1 Announce Type: cross  Abstract: Despite the vast repository of global medical knowledge predominantly being in English, local languages are crucial for delivering tailored healthcare services, particularly in areas with limited medical resources. To extend the reach of medical AI advancements to a broader population, we aim to develop medical LLMs across the six most widely spoken languages, encompassing a global population of 6.1 billion. This effort culminates in the creation of the ApolloCorpora multilingual medical dataset and the XMedBench benchmark. In the multilingual medical benchmark, the released Apollo models, at various relatively-small sizes (i.e., 0.5B, 1.8B, 2B, 6B, and 7B), achieve the best performance among models of equivalent size. Especially, Apollo-7B is the state-of-the-art multilingual medical LLMs up to 70B. Additionally, these lite models could be used to improve the multi-lingual medical capabilities of larger models without fine-tuning in a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;ChatGPT&#29983;&#25104;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#26469;&#22686;&#24378;LLMs&#21435;&#20559;&#35265;&#21270;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#21435;&#38500;&#24050;&#30693;&#20559;&#35265;&#24182;&#36328;&#36234;&#19981;&#21516;&#31867;&#21035;&#36827;&#34892;&#21435;&#20559;&#35265;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.11764</link><description>&lt;p&gt;
&#22522;&#20110;ChatGPT&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#29992;&#20110;&#25913;&#21892;LLMs&#30340;&#21442;&#25968;&#39640;&#25928;&#21435;&#20559;&#35265;&#21270;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Based Data Augmentation for Improved Parameter-Efficient Debiasing of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;ChatGPT&#29983;&#25104;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#26469;&#22686;&#24378;LLMs&#21435;&#20559;&#35265;&#21270;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#21435;&#38500;&#24050;&#30693;&#20559;&#35265;&#24182;&#36328;&#36234;&#19981;&#21516;&#31867;&#21035;&#36827;&#34892;&#21435;&#20559;&#35265;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34429;&#28982;&#21151;&#33021;&#24378;&#22823;&#65292;&#20294;&#23384;&#22312;&#26377;&#23475;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#30001;&#20110;&#35745;&#31639;&#25104;&#26412;&#12289;&#25968;&#25454;&#32422;&#26463;&#21644;&#21487;&#33021;&#38477;&#20302;&#22810;&#20219;&#21153;&#35821;&#35328;&#33021;&#21147;&#65292;&#21435;&#20559;&#35265;&#21270;&#36890;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;ChatGPT&#29983;&#25104;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;LLMs&#30340;&#21435;&#20559;&#35265;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#65306;&#30446;&#26631;&#25552;&#31034;&#65292;&#23545;&#24050;&#30693;&#20559;&#35265;&#25552;&#20379;&#26377;&#25928;&#30340;&#21435;&#20559;&#35265;&#21270;&#65292;&#20294;&#38656;&#35201;&#20107;&#20808;&#25351;&#23450;&#38382;&#39064;&#20013;&#30340;&#20559;&#35265;; &#19968;&#33324;&#25552;&#31034;&#65292;&#34429;&#28982;&#25928;&#26524;&#31245;&#36874;&#65292;&#20294;&#33021;&#22815;&#36328;&#21508;&#31181;&#31867;&#21035;&#36827;&#34892;&#21435;&#20559;&#35265;&#21270;&#12290;&#25105;&#20204;&#21033;&#29992;&#36866;&#37197;&#22120;&#35843;&#25972;&#26469;&#23454;&#29616;&#36164;&#28304;&#39640;&#25928;&#30340;LLM&#21435;&#20559;&#35265;&#21270;&#65292;&#24182;&#27604;&#36739;&#20102;&#25105;&#20204;&#30340;&#21512;&#25104;&#25968;&#25454;&#19982;&#29616;&#26377;&#21435;&#20559;&#35265;&#21270;&#25968;&#25454;&#38598;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;1&#65289;ChatGPT&#21487;&#20197;&#39640;&#25928;&#22320;&#29983;&#25104;&#29992;&#20110;&#21435;&#20559;&#35265;&#21270;&#20854;&#20182;LLMs&#30340;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#65307;&#65288;2&#65289;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#30340;&#25968;&#25454;&#36229;&#36234;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#22312;&#21435;&#20559;&#35265;&#21270;&#19978;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11764v1 Announce Type: cross  Abstract: Large Language models (LLMs), while powerful, exhibit harmful social biases. Debiasing is often challenging due to computational costs, data constraints, and potential degradation of multi-task language capabilities. This work introduces a novel approach utilizing ChatGPT to generate synthetic training data, aiming to enhance the debiasing of LLMs. We propose two strategies: Targeted Prompting, which provides effective debiasing for known biases but necessitates prior specification of bias in question; and General Prompting, which, while slightly less effective, offers debiasing across various categories. We leverage resource-efficient LLM debiasing using adapter tuning and compare the effectiveness of our synthetic data to existing debiasing datasets. Our results reveal that: (1) ChatGPT can efficiently produce high-quality training data for debiasing other LLMs; (2) data produced via our approach surpasses existing datasets in debias
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#36890;&#36807;&#23558;&#38590;&#39064;&#20998;&#20026;&#22522;&#20110;&#35268;&#21017;&#21644;&#26080;&#35268;&#21017;&#20004;&#31867;&#30340;&#29420;&#29305;&#20998;&#31867;&#27861;&#65292;&#36890;&#36807;&#21508;&#31181;&#26041;&#27861;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34920;&#29616;&#65292;&#24378;&#35843;&#20102;&#22312;&#22797;&#26434;&#38590;&#39064;&#24773;&#22659;&#20013;LLMs&#30340;&#25361;&#25112;&#21644;&#20154;&#31867;&#31867;&#20284;&#25512;&#29702;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#31361;&#20986;&#20102;&#25512;&#21160;LLMs&#35299;&#35868;&#33021;&#21147;&#21644;&#36129;&#29486;&#20110;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11291</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#35299;&#20915;&#38590;&#39064;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Puzzle Solving using Reasoning of Large Language Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11291
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#36890;&#36807;&#23558;&#38590;&#39064;&#20998;&#20026;&#22522;&#20110;&#35268;&#21017;&#21644;&#26080;&#35268;&#21017;&#20004;&#31867;&#30340;&#29420;&#29305;&#20998;&#31867;&#27861;&#65292;&#36890;&#36807;&#21508;&#31181;&#26041;&#27861;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34920;&#29616;&#65292;&#24378;&#35843;&#20102;&#22312;&#22797;&#26434;&#38590;&#39064;&#24773;&#22659;&#20013;LLMs&#30340;&#25361;&#25112;&#21644;&#20154;&#31867;&#31867;&#20284;&#25512;&#29702;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#31361;&#20986;&#20102;&#25512;&#21160;LLMs&#35299;&#35868;&#33021;&#21147;&#21644;&#36129;&#29486;&#20110;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35299;&#20915;&#38590;&#39064;&#20013;&#30340;&#33021;&#21147;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#65292;&#26631;&#24535;&#30528;&#29702;&#35299;&#23427;&#20204;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#36808;&#20986;&#20102;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;&#26412;&#35843;&#26597;&#21033;&#29992;&#29420;&#29305;&#30340;&#20998;&#31867;&#27861;&#23558;&#38590;&#39064;&#20998;&#20026;&#22522;&#20110;&#35268;&#21017;&#21644;&#26080;&#35268;&#21017;&#20004;&#31867;&#65292;&#36890;&#36807;&#21508;&#31181;&#26041;&#27861;&#35780;&#20272;LLMs&#65292;&#21253;&#25324;&#25552;&#31034;&#25216;&#26415;&#12289;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#21644;&#24494;&#35843;&#12290;&#36890;&#36807;&#23545;&#30456;&#20851;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#30340;&#25209;&#21028;&#24615;&#23457;&#26597;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;LLMs&#22312;&#22797;&#26434;&#38590;&#39064;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#65292;&#35782;&#21035;&#20986;&#22797;&#26434;&#38590;&#39064;&#24773;&#22659;&#20013;&#30340;&#26174;&#33879;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#20986;&#20102;LLMs&#33021;&#21147;&#21450;&#31867;&#20154;&#25512;&#29702;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#39640;&#32423;&#36923;&#36753;&#25512;&#26029;&#30340;&#24773;&#20917;&#19979;&#12290;&#35843;&#26597;&#24378;&#35843;&#20102;&#38656;&#35201;&#26032;&#39062;&#31574;&#30053;&#21644;&#26356;&#20016;&#23500;&#25968;&#25454;&#38598;&#26469;&#25552;&#21319;LLMs&#30340;&#35299;&#35868;&#33021;&#21147;&#24182;&#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11291v1 Announce Type: cross  Abstract: Exploring the capabilities of Large Language Models (LLMs) in puzzle solving unveils critical insights into their potential and challenges in artificial intelligence, marking a significant step towards understanding their applicability in complex reasoning tasks. This survey leverages a unique taxonomy -- dividing puzzles into rule-based and rule-less categories -- to critically assess LLMs through various methodologies, including prompting techniques, neuro-symbolic approaches, and fine-tuning. Through a critical review of relevant datasets and benchmarks, we assess LLMs' performance, identifying significant challenges in complex puzzle scenarios. Our findings highlight the disparity between LLM capabilities and human-like reasoning, particularly in those requiring advanced logical inference. The survey underscores the necessity for novel strategies and richer datasets to advance LLMs' puzzle-solving proficiency and contribute to AI's
&lt;/p&gt;</description></item><item><title>S-Agents&#26159;&#19968;&#20010;&#33258;&#32452;&#32455;&#20195;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#20195;&#29702;&#26641;&#32467;&#26500;&#12289;&#27801;&#28431;&#20195;&#29702;&#26550;&#26500;&#21644;&#38750;&#38459;&#22622;&#21327;&#20316;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#26080;&#38480;&#29615;&#22659;&#20013;&#39640;&#25928;&#21327;&#35843;&#20195;&#29702;&#30340;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#20248;&#21270;&#21327;&#20316;&#25928;&#29575;&#21644;&#28789;&#27963;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.04578</link><description>&lt;p&gt;
S-Agents: &#33258;&#32452;&#32455;&#20195;&#29702;&#22312;&#26080;&#38480;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
S-Agents: self-organizing agents in open-ended environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04578
&lt;/p&gt;
&lt;p&gt;
S-Agents&#26159;&#19968;&#20010;&#33258;&#32452;&#32455;&#20195;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#20195;&#29702;&#26641;&#32467;&#26500;&#12289;&#27801;&#28431;&#20195;&#29702;&#26550;&#26500;&#21644;&#38750;&#38459;&#22622;&#21327;&#20316;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#26080;&#38480;&#29615;&#22659;&#20013;&#39640;&#25928;&#21327;&#35843;&#20195;&#29702;&#30340;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#20248;&#21270;&#21327;&#20316;&#25928;&#29575;&#21644;&#28789;&#27963;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#33258;&#20027;&#20195;&#29702;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#65292;&#20855;&#22791;&#22788;&#29702;&#21508;&#31181;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#22312;&#26080;&#38480;&#29615;&#22659;&#20013;&#65292;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#21644;&#25928;&#26524;&#65292;&#20248;&#21270;&#21327;&#20316;&#38656;&#35201;&#28789;&#27963;&#30340;&#35843;&#25972;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#24378;&#35843;&#22266;&#23450;&#30340;&#12289;&#20219;&#21153;&#23548;&#21521;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#24573;&#35270;&#20102;&#20197;&#20195;&#29702;&#20026;&#20013;&#24515;&#30340;&#32452;&#32455;&#32467;&#26500;&#12290;&#21463;&#20154;&#31867;&#32452;&#32455;&#34892;&#20026;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#32452;&#32455;&#20195;&#29702;&#31995;&#32479;&#65288;S-Agents&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;&#21160;&#24577;&#24037;&#20316;&#27969;&#31243;&#30340;&#8220;&#20195;&#29702;&#26641;&#8221;&#32467;&#26500;&#12289;&#24179;&#34913;&#20449;&#24687;&#20248;&#20808;&#32423;&#30340;&#8220;&#27801;&#28431;&#20195;&#29702;&#26550;&#26500;&#8221;&#20197;&#21450;&#20801;&#35768;&#20195;&#29702;&#20043;&#38388;&#24322;&#27493;&#25191;&#34892;&#20219;&#21153;&#30340;&#8220;&#38750;&#38459;&#22622;&#21327;&#20316;&#8221;&#26041;&#27861;&#12290;&#36825;&#31181;&#32467;&#26500;&#21487;&#20197;&#33258;&#20027;&#21327;&#35843;&#19968;&#32452;&#20195;&#29702;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#26080;&#38480;&#19988;&#21160;&#24577;&#30340;&#29615;&#22659;&#25361;&#25112;&#65292;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;S-Agents&#33021;&#22815;&#29087;&#32451;&#22320;&#25191;&#34892;&#21327;&#20316;&#24314;&#31569;&#20219;&#21153;&#21644;&#36164;&#28304;&#25910;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging large language models (LLMs), autonomous agents have significantly improved, gaining the ability to handle a variety of tasks. In open-ended settings, optimizing collaboration for efficiency and effectiveness demands flexible adjustments. Despite this, current research mainly emphasizes fixed, task-oriented workflows and overlooks agent-centric organizational structures. Drawing inspiration from human organizational behavior, we introduce a self-organizing agent system (S-Agents) with a "tree of agents" structure for dynamic workflow, an "hourglass agent architecture" for balancing information priorities, and a "non-obstructive collaboration" method to allow asynchronous task execution among agents. This structure can autonomously coordinate a group of agents, efficiently addressing the challenges of an open and dynamic environment without human intervention. Our experiments demonstrate that S-Agents proficiently execute collaborative building tasks and resource collection i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#26032;&#22411;&#20840;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#25104;&#21151;&#24212;&#29992;&#20110;&#22788;&#29702;&#33151;&#24335;&#26426;&#22120;&#20154;&#65292;&#22312;&#21508;&#31181;&#27169;&#25311;&#22320;&#24418;&#20013;&#21462;&#24471;&#20102;&#26480;&#20986;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2310.05022</link><description>&lt;p&gt;
&#29992;&#20110;&#33151;&#24335;&#26426;&#22120;&#20154;&#30340;&#20840;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Fully Spiking Neural Network for Legged Robots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#26032;&#22411;&#20840;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#25104;&#21151;&#24212;&#29992;&#20110;&#22788;&#29702;&#33151;&#24335;&#26426;&#22120;&#20154;&#65292;&#22312;&#21508;&#31181;&#27169;&#25311;&#22320;&#24418;&#20013;&#21462;&#24471;&#20102;&#26480;&#20986;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#33151;&#24335;&#26426;&#22120;&#20154;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#22235;&#36275;&#26426;&#22120;&#20154;&#23637;&#31034;&#20102;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#23436;&#25104;&#20855;&#26377;&#25361;&#25112;&#24615;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#24050;&#37096;&#32626;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#20197;&#21327;&#21161;&#20154;&#31867;&#12290;&#21516;&#26102;&#65292;&#20004;&#36275;&#21644;&#31867;&#20154;&#26426;&#22120;&#20154;&#22312;&#21508;&#31181;&#39640;&#38590;&#24230;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#31361;&#30772;&#12290;&#26412;&#30740;&#31350;&#25104;&#21151;&#23558;&#19968;&#31181;&#26032;&#22411;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#24212;&#29992;&#20110;&#22788;&#29702;&#33151;&#24335;&#26426;&#22120;&#20154;&#65292;&#22312;&#19968;&#31995;&#21015;&#27169;&#25311;&#22320;&#24418;&#20013;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05022v2 Announce Type: replace-cross  Abstract: In recent years, legged robots based on deep reinforcement learning have made remarkable progress. Quadruped robots have demonstrated the ability to complete challenging tasks in complex environments and have been deployed in real-world scenarios to assist humans. Simultaneously, bipedal and humanoid robots have achieved breakthroughs in various demanding tasks. Current reinforcement learning methods can utilize diverse robot bodies and historical information to perform actions. However, prior research has not emphasized the speed and energy consumption of network inference, as well as the biological significance of the neural networks themselves. Most of the networks employed are traditional artificial neural networks that utilize multilayer perceptrons (MLP). In this paper, we successfully apply a novel Spiking Neural Network (SNN) to process legged robots, achieving outstanding results across a range of simulated terrains. S
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#32508;&#21512;&#32479;&#35745;&#20915;&#31574;&#29702;&#35770;&#21644;&#20449;&#24687;&#32463;&#27982;&#23398;&#65292;&#25552;&#20986;&#20102;&#20915;&#31574;&#38382;&#39064;&#30340;&#24191;&#27867;&#36866;&#29992;&#23450;&#20041;&#12290;&#20026;&#20102;&#23558;&#20154;&#31867;&#20915;&#31574;&#30340;&#19979;&#38477;&#24402;&#21646;&#20110;&#20559;&#35265;&#24418;&#24335;&#65292;&#23454;&#39564;&#24517;&#39035;&#21521;&#21442;&#19982;&#32773;&#25552;&#20379;&#36275;&#22815;&#30340;&#20449;&#24687;&#26469;&#35782;&#21035;&#35268;&#33539;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#26681;&#25454;&#20316;&#32773;&#23545;AI&#36741;&#21161;&#20915;&#31574;&#30340;&#30740;&#31350;&#30340;&#35780;&#20272;&#65292;&#21482;&#26377;17%&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#36275;&#22815;&#30340;&#20449;&#24687;&#26469;&#25551;&#36848;&#21442;&#19982;&#32773;&#30340;&#34892;&#20026;&#20559;&#31163;&#20102;&#33391;&#22909;&#30340;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2401.15106</link><description>&lt;p&gt;
&#20915;&#31574;&#29702;&#35770;&#22522;&#30784;&#23545;&#35780;&#20272;&#20154;&#31867;&#20915;&#31574;&#30340;&#23454;&#39564;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Decision Theoretic Foundations for Experiments Evaluating Human Decisions. (arXiv:2401.15106v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15106
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#32508;&#21512;&#32479;&#35745;&#20915;&#31574;&#29702;&#35770;&#21644;&#20449;&#24687;&#32463;&#27982;&#23398;&#65292;&#25552;&#20986;&#20102;&#20915;&#31574;&#38382;&#39064;&#30340;&#24191;&#27867;&#36866;&#29992;&#23450;&#20041;&#12290;&#20026;&#20102;&#23558;&#20154;&#31867;&#20915;&#31574;&#30340;&#19979;&#38477;&#24402;&#21646;&#20110;&#20559;&#35265;&#24418;&#24335;&#65292;&#23454;&#39564;&#24517;&#39035;&#21521;&#21442;&#19982;&#32773;&#25552;&#20379;&#36275;&#22815;&#30340;&#20449;&#24687;&#26469;&#35782;&#21035;&#35268;&#33539;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#26681;&#25454;&#20316;&#32773;&#23545;AI&#36741;&#21161;&#20915;&#31574;&#30340;&#30740;&#31350;&#30340;&#35780;&#20272;&#65292;&#21482;&#26377;17%&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#36275;&#22815;&#30340;&#20449;&#24687;&#26469;&#25551;&#36848;&#21442;&#19982;&#32773;&#30340;&#34892;&#20026;&#20559;&#31163;&#20102;&#33391;&#22909;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#23637;&#31034;&#30340;&#20915;&#31574;&#26159;&#21487;&#35299;&#37322;AI&#12289;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#21512;&#20316;&#20197;&#21450;&#25968;&#25454;&#21487;&#35270;&#21270;&#31561;&#39046;&#22495;&#30740;&#31350;&#30340;&#37325;&#28857;&#12290;&#28982;&#32780;&#65292;&#20915;&#31574;&#38382;&#39064;&#30340;&#23450;&#20041;&#20197;&#21450;&#23454;&#39564;&#24517;&#39035;&#20855;&#22791;&#30340;&#26465;&#20214;&#20197;&#24471;&#20986;&#20154;&#31867;&#20915;&#31574;&#23384;&#22312;&#32570;&#38519;&#30340;&#32467;&#35770;&#20173;&#28982;&#23384;&#22312;&#20105;&#35758;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#27867;&#36866;&#29992;&#30340;&#20915;&#31574;&#38382;&#39064;&#23450;&#20041;&#65292;&#35813;&#23450;&#20041;&#26159;&#20174;&#32479;&#35745;&#20915;&#31574;&#29702;&#35770;&#21644;&#20449;&#24687;&#32463;&#27982;&#23398;&#20013;&#32508;&#21512;&#25552;&#28860;&#32780;&#26469;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#35201;&#23558;&#20154;&#31867;&#32489;&#25928;&#19979;&#38477;&#24402;&#21646;&#20110;&#26576;&#31181;&#20559;&#35265;&#24418;&#24335;&#65292;&#23454;&#39564;&#24517;&#39035;&#21521;&#21442;&#19982;&#32773;&#25552;&#20379;&#36275;&#22815;&#30340;&#20449;&#24687;&#65292;&#20197;&#20415;&#21512;&#29702;&#30340;&#20195;&#29702;&#33021;&#22815;&#35782;&#21035;&#35268;&#33539;&#20915;&#31574;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#26368;&#36817;&#26377;&#20851;AI&#36741;&#21161;&#20915;&#31574;&#30340;&#25991;&#29486;&#20013;&#23545;&#20915;&#31574;&#21046;&#23450;&#36827;&#34892;&#30340;&#35780;&#20272;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#36798;&#21040;&#20102;&#36825;&#19968;&#26631;&#20934;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21482;&#26377;35&#39033;&#22768;&#31216;&#30830;&#23450;&#20102;&#26377;&#20559;&#24046;&#34892;&#20026;&#30340;&#30740;&#31350;&#20013;&#30340;6&#39033;&#65288;17%&#65289;&#21521;&#21442;&#19982;&#32773;&#25552;&#20379;&#20102;&#36275;&#22815;&#20449;&#24687;&#26469;&#25551;&#36848;&#20854;&#34892;&#20026;&#20559;&#31163;&#33391;&#22909;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Decision-making with information displays is a key focus of research in areas like explainable AI, human-AI teaming, and data visualization. However, what constitutes a decision problem, and what is required for an experiment to be capable of concluding that human decisions are flawed in some way, remain open to speculation. We present a widely applicable definition of a decision problem synthesized from statistical decision theory and information economics. We argue that to attribute loss in human performance to forms of bias, an experiment must provide participants with the information that a rational agent would need to identify the normative decision. We evaluate the extent to which recent evaluations of decision-making from the literature on AI-assisted decisions achieve this criteria. We find that only 6 (17\%) of 35 studies that claim to identify biased behavior present participants with sufficient information to characterize their behavior as deviating from good decision-making
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;LSTM&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#36890;&#36807;&#20934;&#30830;&#25429;&#25417;&#26053;&#34892;&#25968;&#25454;&#20013;&#30340;&#24207;&#21015;&#27169;&#24335;&#21644;&#20381;&#36182;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#23545;&#20010;&#20154;&#26053;&#34892;&#32773;&#26410;&#26469;&#30446;&#30340;&#22320;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#35268;&#27169;&#21644;&#24615;&#33021;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20026;&#25552;&#21319;&#30446;&#30340;&#22320;&#39044;&#27979;&#26041;&#27861;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#24182;&#20351;&#20844;&#21496;&#33021;&#22815;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#21644;&#20248;&#21270;&#23458;&#25143;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2401.12830</link><description>&lt;p&gt;
&#25552;&#21319;&#19979;&#19968;&#20010;&#30446;&#30340;&#22320;&#39044;&#27979;&#65306;&#19968;&#31181;&#22522;&#20110;&#30495;&#23454;&#33322;&#31354;&#25968;&#25454;&#30340;&#26032;&#39062;LSTM&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Next Destination Prediction: A Novel LSTM Approach Using Real-World Airline Data. (arXiv:2401.12830v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12830
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;LSTM&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#36890;&#36807;&#20934;&#30830;&#25429;&#25417;&#26053;&#34892;&#25968;&#25454;&#20013;&#30340;&#24207;&#21015;&#27169;&#24335;&#21644;&#20381;&#36182;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#23545;&#20010;&#20154;&#26053;&#34892;&#32773;&#26410;&#26469;&#30446;&#30340;&#22320;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#35268;&#27169;&#21644;&#24615;&#33021;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20026;&#25552;&#21319;&#30446;&#30340;&#22320;&#39044;&#27979;&#26041;&#27861;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#24182;&#20351;&#20844;&#21496;&#33021;&#22815;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#21644;&#20248;&#21270;&#23458;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#20132;&#36890;&#34892;&#19994;&#20013;&#65292;&#20934;&#30830;&#39044;&#27979;&#26053;&#34892;&#32773;&#30340;&#19979;&#19968;&#20010;&#30446;&#30340;&#22320;&#20026;&#20844;&#21496;&#24102;&#26469;&#24456;&#22810;&#22909;&#22788;&#65292;&#20363;&#22914;&#25552;&#39640;&#23458;&#25143;&#28385;&#24847;&#24230;&#21644;&#23450;&#21521;&#33829;&#38144;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#20934;&#30830;&#25429;&#25417;&#26053;&#34892;&#25968;&#25454;&#20013;&#30340;&#24207;&#21015;&#27169;&#24335;&#21644;&#20381;&#36182;&#20851;&#31995;&#30340;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#20010;&#20154;&#26053;&#34892;&#32773;&#26410;&#26469;&#30446;&#30340;&#22320;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#30340;&#28369;&#21160;&#31383;&#21475;&#26041;&#27861;&#30340;&#26032;&#39062;&#27169;&#22411;&#26550;&#26500;&#65292;&#29992;&#20110;&#20132;&#36890;&#19994;&#20013;&#30340;&#30446;&#30340;&#22320;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#35268;&#27169;&#21644;&#24615;&#33021;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#34920;&#29616;&#21644;&#39640;&#20998;&#25968;&#12290;&#26412;&#30740;&#31350;&#22312;&#25512;&#36827;&#30446;&#30340;&#22320;&#39044;&#27979;&#26041;&#27861;&#26041;&#38754;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#20351;&#20844;&#21496;&#33021;&#22815;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#24182;&#20248;&#21270;&#21160;&#24577;&#26053;&#34892;&#29615;&#22659;&#20013;&#30340;&#23458;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the modern transportation industry, accurate prediction of travelers' next destinations brings multiple benefits to companies, such as customer satisfaction and targeted marketing. This study focuses on developing a precise model that captures the sequential patterns and dependencies in travel data, enabling accurate predictions of individual travelers' future destinations. To achieve this, a novel model architecture with a sliding window approach based on Long Short-Term Memory (LSTM) is proposed for destination prediction in the transportation industry. The experimental results highlight satisfactory performance and high scores achieved by the proposed model across different data sizes and performance metrics. This research contributes to advancing destination prediction methods, empowering companies to deliver personalized recommendations and optimize customer experiences in the dynamic travel landscape.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25581;&#31034;&#20102;&#35889;&#28388;&#27874;&#21644;&#31354;&#38388;&#32858;&#21512;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;&#35889;&#28388;&#27874;&#22312;&#38544;&#21547;&#22320;&#23558;&#21407;&#22987;&#22270;&#36716;&#25442;&#25104;&#36866;&#24212;&#24615;&#26032;&#22270;&#65292;&#24182;&#26126;&#30830;&#35745;&#31639;&#29992;&#20110;&#31354;&#38388;&#32858;&#21512;&#30340;&#26032;&#22270;&#12290;&#36866;&#24212;&#24615;&#26032;&#22270;&#23637;&#29616;&#20986;&#38750;&#23616;&#37096;&#24615;&#65292;&#24182;&#33021;&#22815;&#21453;&#26144;&#33410;&#28857;&#20043;&#38388;&#30340;&#26631;&#31614;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09071</link><description>&lt;p&gt;
&#29992;&#31354;&#38388;&#33258;&#36866;&#24212;&#28388;&#27874;&#37325;&#26032;&#24605;&#32771;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Rethinking Spectral Graph Neural Networks with Spatially Adaptive Filtering. (arXiv:2401.09071v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25581;&#31034;&#20102;&#35889;&#28388;&#27874;&#21644;&#31354;&#38388;&#32858;&#21512;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;&#35889;&#28388;&#27874;&#22312;&#38544;&#21547;&#22320;&#23558;&#21407;&#22987;&#22270;&#36716;&#25442;&#25104;&#36866;&#24212;&#24615;&#26032;&#22270;&#65292;&#24182;&#26126;&#30830;&#35745;&#31639;&#29992;&#20110;&#31354;&#38388;&#32858;&#21512;&#30340;&#26032;&#22270;&#12290;&#36866;&#24212;&#24615;&#26032;&#22270;&#23637;&#29616;&#20986;&#38750;&#23616;&#37096;&#24615;&#65292;&#24182;&#33021;&#22815;&#21453;&#26144;&#33410;&#28857;&#20043;&#38388;&#30340;&#26631;&#31614;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#29702;&#35770;&#19978;&#22312;&#35889;&#22495;&#20013;&#26377;&#24456;&#22909;&#30340;&#22522;&#30784;&#65292;&#20294;&#23427;&#20204;&#23454;&#38469;&#19978;&#20381;&#36182;&#20110;&#22810;&#39033;&#24335;&#36924;&#36817;&#65292;&#24847;&#21619;&#30528;&#23427;&#20204;&#19982;&#31354;&#38388;&#22495;&#26377;&#30528;&#28145;&#21051;&#30340;&#32852;&#31995;&#12290;&#30001;&#20110;&#20197;&#21069;&#30340;&#30740;&#31350;&#24456;&#23569;&#20174;&#31354;&#38388;&#35282;&#24230;&#30740;&#31350;&#35889;&#22270;GNN&#65292;&#22240;&#27492;&#23427;&#20204;&#22312;&#31354;&#38388;&#22495;&#30340;&#21487;&#35299;&#37322;&#24615;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#65292;&#20363;&#22914;&#65292;&#35889;&#22270;GNN&#22312;&#31354;&#38388;&#22495;&#20013;&#23454;&#38469;&#19978;&#32534;&#30721;&#20102;&#21738;&#20123;&#20449;&#24687;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#22312;&#35889;&#28388;&#27874;&#21644;&#31354;&#38388;&#32858;&#21512;&#20043;&#38388;&#24314;&#31435;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#30340;&#32852;&#31995;&#65292;&#25581;&#31034;&#20102;&#35889;&#28388;&#27874;&#38544;&#21547;&#22320;&#23558;&#21407;&#22987;&#22270;&#36716;&#25442;&#25104;&#36866;&#24212;&#24615;&#26032;&#22270;&#30340;&#20869;&#22312;&#20132;&#20114;&#20316;&#29992;&#65292;&#24182;&#26126;&#30830;&#22320;&#35745;&#31639;&#29992;&#20110;&#31354;&#38388;&#32858;&#21512;&#30340;&#36866;&#24212;&#24615;&#26032;&#22270;&#12290;&#29702;&#35770;&#21644;&#32463;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#36866;&#24212;&#24615;&#26032;&#22270;&#19981;&#20165;&#34920;&#29616;&#20986;&#38750;&#23616;&#37096;&#24615;&#65292;&#36824;&#33021;&#22815;&#23481;&#32435;&#26377;&#31526;&#21495;&#30340;&#36793;&#26435;&#37325;&#20197;&#21453;&#26144;&#33410;&#28857;&#20043;&#38388;&#30340;&#26631;&#31614;&#19968;&#33268;&#24615;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#35889;&#22270;GNN&#22312;&#31354;&#38388;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whilst spectral Graph Neural Networks (GNNs) are theoretically well-founded in the spectral domain, their practical reliance on polynomial approximation implies a profound linkage to the spatial domain. As previous studies rarely examine spectral GNNs from the spatial perspective, their spatial-domain interpretability remains elusive, e.g., what information is essentially encoded by spectral GNNs in the spatial domain? In this paper, to answer this question, we establish a theoretical connection between spectral filtering and spatial aggregation, unveiling an intrinsic interaction that spectral filtering implicitly leads the original graph to an adapted new graph, explicitly computed for spatial aggregation. Both theoretical and empirical investigations reveal that the adapted new graph not only exhibits non-locality but also accommodates signed edge weights to reflect label consistency between nodes. These findings thus highlight the interpretable role of spectral GNNs in the spatial 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22871;&#20214;&#65292;&#29992;&#20110;&#25511;&#21046;&#28014;&#21160;&#24179;&#21488;&#30340;&#36712;&#36857;&#12290;&#36890;&#36807;&#35757;&#32451;&#31934;&#30830;&#25805;&#20316;&#31574;&#30053;&#20197;&#24212;&#23545;&#21160;&#24577;&#21644;&#19981;&#21487;&#39044;&#27979;&#30340;&#26465;&#20214;&#65292;&#35299;&#20915;&#20102;&#25511;&#21046;&#28014;&#21160;&#24179;&#21488;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;&#35813;&#22871;&#20214;&#20855;&#26377;&#31283;&#20581;&#24615;&#12289;&#36866;&#24212;&#24615;&#21644;&#21487;&#20256;&#36882;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#21487;&#35270;&#21270;&#36873;&#39033;&#21644;&#19982;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#31995;&#32479;&#38598;&#25104;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.04266</link><description>&lt;p&gt;
DRIFT: &#26234;&#33021;&#28014;&#21160;&#24179;&#21488;&#36712;&#36857;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DRIFT: Deep Reinforcement Learning for Intelligent Floating Platforms Trajectories. (arXiv:2310.04266v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22871;&#20214;&#65292;&#29992;&#20110;&#25511;&#21046;&#28014;&#21160;&#24179;&#21488;&#30340;&#36712;&#36857;&#12290;&#36890;&#36807;&#35757;&#32451;&#31934;&#30830;&#25805;&#20316;&#31574;&#30053;&#20197;&#24212;&#23545;&#21160;&#24577;&#21644;&#19981;&#21487;&#39044;&#27979;&#30340;&#26465;&#20214;&#65292;&#35299;&#20915;&#20102;&#25511;&#21046;&#28014;&#21160;&#24179;&#21488;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;&#35813;&#22871;&#20214;&#20855;&#26377;&#31283;&#20581;&#24615;&#12289;&#36866;&#24212;&#24615;&#21644;&#21487;&#20256;&#36882;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#21487;&#35270;&#21270;&#36873;&#39033;&#21644;&#19982;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#31995;&#32479;&#38598;&#25104;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#22411;&#22871;&#20214;&#65292;&#29992;&#20110;&#25511;&#21046;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&#28014;&#21160;&#24179;&#21488;&#12290;&#28014;&#21160;&#24179;&#21488;&#21487;&#20316;&#20026;&#22810;&#21151;&#33021;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#22312;&#22320;&#29699;&#19978;&#27169;&#25311;&#24494;&#37325;&#21147;&#29615;&#22659;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#33021;&#22815;&#22312;&#21160;&#24577;&#21644;&#19981;&#21487;&#39044;&#27979;&#30340;&#26465;&#20214;&#19979;&#36827;&#34892;&#31934;&#30830;&#25805;&#20316;&#30340;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#25511;&#21046;&#27492;&#31867;&#24179;&#21488;&#20013;&#30340;&#31995;&#32479;&#21644;&#29615;&#22659;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#25105;&#20204;&#30340;&#22871;&#20214;&#23454;&#29616;&#20102;&#31283;&#20581;&#24615;&#12289;&#36866;&#24212;&#24615;&#21644;&#20174;&#27169;&#25311;&#21040;&#29616;&#23454;&#30340;&#33391;&#22909;&#21487;&#20256;&#36882;&#24615;&#12290;&#25105;&#20204;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26694;&#26550;&#25552;&#20379;&#20102;&#24555;&#36895;&#35757;&#32451;&#26102;&#38388;&#12289;&#22823;&#35268;&#27169;&#27979;&#35797;&#33021;&#21147;&#12289;&#20016;&#23500;&#30340;&#21487;&#35270;&#21270;&#36873;&#39033;&#20197;&#21450;&#19982;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#31995;&#32479;&#38598;&#25104;&#30340;ROS&#32465;&#23450;&#12290;&#38500;&#20102;&#31574;&#30053;&#24320;&#21457;&#65292;&#25105;&#20204;&#30340;&#22871;&#20214;&#36824;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#24179;&#21488;&#65292;&#25552;&#20379;&#24320;&#25918;&#35775;&#38382;&#65292;&#32593;&#22336;&#20026;https://github.com/elharirymatteo/RANS/tree/ICRA24&#12290;
&lt;/p&gt;
&lt;p&gt;
This investigation introduces a novel deep reinforcement learning-based suite to control floating platforms in both simulated and real-world environments. Floating platforms serve as versatile test-beds to emulate microgravity environments on Earth. Our approach addresses the system and environmental uncertainties in controlling such platforms by training policies capable of precise maneuvers amid dynamic and unpredictable conditions. Leveraging state-of-the-art deep reinforcement learning techniques, our suite achieves robustness, adaptability, and good transferability from simulation to reality. Our Deep Reinforcement Learning (DRL) framework provides advantages such as fast training times, large-scale testing capabilities, rich visualization options, and ROS bindings for integration with real-world robotic systems. Beyond policy development, our suite provides a comprehensive platform for researchers, offering open-access at https://github.com/elharirymatteo/RANS/tree/ICRA24.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#32534;&#30721;&#22120;&#26469;&#22686;&#24378;&#21487;&#23398;&#20064;&#25552;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#25163;&#21160;&#25552;&#31034;&#24037;&#31243;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.07760</link><description>&lt;p&gt;
PRE: &#35270;&#35273;-&#35821;&#35328;&#25552;&#31034;&#23398;&#20064;&#19982;&#37325;&#26032;&#21442;&#25968;&#21270;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
PRE: Vision-Language Prompt Learning with Reparameterization Encoder. (arXiv:2309.07760v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07760
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#32534;&#30721;&#22120;&#26469;&#22686;&#24378;&#21487;&#23398;&#20064;&#25552;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#25163;&#21160;&#25552;&#31034;&#24037;&#31243;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#38646;&#26679;&#26412;&#36801;&#31227;&#20219;&#21153;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#65292;&#38656;&#35201;&#25163;&#21160;&#36873;&#25321;&#25552;&#31034;&#20197;&#25913;&#36827;&#19979;&#28216;&#22270;&#20687;&#20998;&#24067;&#21644;&#25991;&#26412;&#31867;&#25551;&#36848;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#36825;&#31181;&#25163;&#21160;&#25552;&#31034;&#24037;&#31243;&#26159;&#23558;&#36825;&#20123;&#27169;&#22411;&#37096;&#32626;&#21040;&#23454;&#36341;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#24182;&#19988;&#38750;&#24120;&#32791;&#26102;&#12290;&#20026;&#20102;&#36991;&#20813;&#22797;&#26434;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#26368;&#36817;&#30340;CoOp&#24037;&#20316;&#24341;&#20837;&#20102;&#22312;&#35270;&#35273;&#39046;&#22495;&#20351;&#29992;&#21487;&#25511;&#25991;&#26412;&#26631;&#35760;&#30340;&#25552;&#31034;&#23398;&#20064;&#27010;&#24565;&#12290;&#34429;&#28982;CoOp&#21487;&#20197;&#22312;&#25163;&#21160;&#25552;&#31034;&#19978;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#65292;&#20294;&#20854;&#23398;&#21040;&#30340;&#19978;&#19979;&#25991;&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#20013;&#26356;&#24191;&#27867;&#30340;&#26410;&#35265;&#31867;&#21035;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prompt Learning with Reparameterization Encoder (PRE) &#30340;&#31616;&#21333;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#21487;&#23398;&#20064;&#25552;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained vision-language models such as CLIP have demonstrated great potential in zero-shot transferability to downstream tasks. However, to attain optimal performance, the manual selection of prompts is necessary to improve alignment between the downstream image distribution and the textual class descriptions. This manual prompt engineering is the major challenge for deploying such models in practice since it requires domain expertise and is extremely time-consuming. To avoid non-trivial prompt engineering, recent work Context Optimization (CoOp) introduced the concept of prompt learning to the vision domain using learnable textual tokens. While CoOp can achieve substantial improvements over manual prompts, its learned context is worse generalizable to wider unseen classes within the same dataset. In this work, we present Prompt Learning with Reparameterization Encoder (PRE) - a simple and efficient method that enhances the generalization ability of the learnable prompt to un
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#35757;&#32451;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#20010;&#26550;&#26500;&#26681;&#25454;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#20449;&#38656;&#27714;&#65292;&#23558;&#38598;&#32676;&#20998;&#21106;&#25104;&#19968;&#32452;&#36890;&#36807;&#38750;&#38459;&#22622;&#39640;&#24102;&#23485;&#20114;&#36830;&#30340;GPU&#38598;&#21512;&#65292;&#24182;&#36890;&#36807;&#36712;&#36947;&#36830;&#25509;&#20165;&#36830;&#25509;&#20855;&#26377;&#36890;&#20449;&#38656;&#27714;&#30340;GPU&#65292;&#20174;&#32780;&#38477;&#20302;&#32593;&#32476;&#25104;&#26412;&#39640;&#36798;75&#65285;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#35757;&#32451;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.12169</link><description>&lt;p&gt;
&#29992;&#20110;&#35757;&#32451;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Optimized Network Architectures for Large Language Model Training with Billions of Parameters. (arXiv:2307.12169v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#35757;&#32451;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#20010;&#26550;&#26500;&#26681;&#25454;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#20449;&#38656;&#27714;&#65292;&#23558;&#38598;&#32676;&#20998;&#21106;&#25104;&#19968;&#32452;&#36890;&#36807;&#38750;&#38459;&#22622;&#39640;&#24102;&#23485;&#20114;&#36830;&#30340;GPU&#38598;&#21512;&#65292;&#24182;&#36890;&#36807;&#36712;&#36947;&#36830;&#25509;&#20165;&#36830;&#25509;&#20855;&#26377;&#36890;&#20449;&#38656;&#27714;&#30340;GPU&#65292;&#20174;&#32780;&#38477;&#20302;&#32593;&#32476;&#25104;&#26412;&#39640;&#36798;75&#65285;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#35757;&#32451;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25361;&#25112;&#20102;&#20026;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26500;&#24314;&#20219;&#24847;&#21040;&#20219;&#24847;&#32593;&#32476;&#30340;&#20256;&#32479;&#33539;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#21576;&#29616;&#20986;&#19968;&#31181;&#29420;&#29305;&#30340;&#36890;&#20449;&#27169;&#24335;&#65292;&#22312;&#20854;&#20013;&#65292;&#21482;&#26377;&#23567;&#32452;&#30340;GPU&#38656;&#35201;&#39640;&#24102;&#23485;&#30340;&#20219;&#24847;&#21040;&#20219;&#24847;&#36890;&#20449;&#65292;&#20197;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#35757;&#32451;&#24615;&#33021;&#12290;&#22312;&#36825;&#20123;GPU&#23567;&#32452;&#20043;&#38388;&#65292;&#36890;&#20449;&#38750;&#24120;&#24494;&#19981;&#36275;&#36947;&#12289;&#31232;&#30095;&#19988;&#22343;&#21248;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#32039;&#23494;&#21305;&#37197;LLMs&#30340;&#36890;&#20449;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#23558;&#38598;&#32676;&#20998;&#21106;&#20026;&#19968;&#32452;&#36890;&#36807;&#38750;&#38459;&#22622;&#20219;&#24847;&#21040;&#20219;&#24847;&#39640;&#24102;&#23485;&#20114;&#36830;&#30340;GPU&#38598;&#21512;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;HB&#22495;&#12290;&#22312;HB&#22495;&#20043;&#38388;&#65292;&#32593;&#32476;&#21482;&#36830;&#25509;&#20855;&#26377;&#36890;&#20449;&#38656;&#27714;&#30340;GPU&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#32593;&#32476;&#36830;&#25509;&#31216;&#20026;&#8220;&#20165;&#36712;&#36947;&#36830;&#25509;&#8221;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26550;&#26500;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#20219;&#24847;&#21040;&#20219;&#24847;Clos&#32593;&#32476;&#21487;&#20197;&#23558;&#32593;&#32476;&#25104;&#26412;&#38477;&#20302;&#39640;&#36798;75&#65285;&#65292;&#21516;&#26102;&#19981;&#25439;&#23475;LLM&#35757;&#32451;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper challenges the well-established paradigm for building any-to-any networks for training Large Language Models (LLMs). We show that LLMs exhibit a unique communication pattern where only small groups of GPUs require high-bandwidth any-to-any communication within them, to achieve near-optimal training performance. Across these groups of GPUs, the communication is insignificant, sparse, and homogeneous. We propose a new network architecture that closely resembles the communication requirement of LLMs. Our architecture partitions the cluster into sets of GPUs interconnected with non-blocking any-to-any high-bandwidth interconnects that we call HB domains. Across the HB domains, the network only connects GPUs with communication demands. We call this network a "rail-only" connection, and show that our proposed architecture reduces the network cost by up to 75% compared to the state-of-the-art any-to-any Clos networks without compromising the performance of LLM training.
&lt;/p&gt;</description></item><item><title>AIOptimizer&#26159;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#36719;&#20214;&#24615;&#33021;&#20248;&#21270;&#24037;&#20855;&#21407;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#25104;&#26412;&#26368;&#23567;&#21270;&#12290;&#23427;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#25512;&#33616;&#31995;&#32479;&#26469;&#25913;&#21892;&#36719;&#20214;&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#21487;&#36127;&#25285;&#24615;&#65292;&#24182;&#31361;&#20986;&#20102;&#20934;&#30830;&#24615;&#12289;&#36866;&#24212;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#29992;&#25143;&#21451;&#22909;&#24615;&#31561;&#35774;&#35745;&#22240;&#32032;&#12290;AIOptimizer&#36824;&#25552;&#20379;&#25925;&#38556;&#35782;&#21035;&#12289;&#25104;&#26412;&#20248;&#21270;&#24314;&#35758;&#12289;&#25928;&#29575;&#39044;&#27979;&#21644;&#21327;&#20316;&#31561;&#21151;&#33021;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25512;&#33616;&#24341;&#25806;&#36827;&#34892;&#25104;&#26412;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.07846</link><description>&lt;p&gt;
AIOptimizer &#8212;&#8212;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#36719;&#20214;&#24615;&#33021;&#20248;&#21270;&#21407;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#25104;&#26412;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
AIOptimizer -- A reinforcement learning-based software performance optimisation prototype for cost minimisation. (arXiv:2307.07846v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07846
&lt;/p&gt;
&lt;p&gt;
AIOptimizer&#26159;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#36719;&#20214;&#24615;&#33021;&#20248;&#21270;&#24037;&#20855;&#21407;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#25104;&#26412;&#26368;&#23567;&#21270;&#12290;&#23427;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#25512;&#33616;&#31995;&#32479;&#26469;&#25913;&#21892;&#36719;&#20214;&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#21487;&#36127;&#25285;&#24615;&#65292;&#24182;&#31361;&#20986;&#20102;&#20934;&#30830;&#24615;&#12289;&#36866;&#24212;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#29992;&#25143;&#21451;&#22909;&#24615;&#31561;&#35774;&#35745;&#22240;&#32032;&#12290;AIOptimizer&#36824;&#25552;&#20379;&#25925;&#38556;&#35782;&#21035;&#12289;&#25104;&#26412;&#20248;&#21270;&#24314;&#35758;&#12289;&#25928;&#29575;&#39044;&#27979;&#21644;&#21327;&#20316;&#31561;&#21151;&#33021;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25512;&#33616;&#24341;&#25806;&#36827;&#34892;&#25104;&#26412;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25991;&#31456;&#20171;&#32461;&#20102;AIOptimizer&#65292;&#19968;&#20010;&#22522;&#20110;&#25104;&#26412;&#38477;&#20302;&#30340;&#36719;&#20214;&#24615;&#33021;&#20248;&#21270;&#24037;&#20855;&#30340;&#21407;&#22411;&#12290;AIOptimizer&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#25512;&#33616;&#31995;&#32479;&#26469;&#25913;&#21892;&#36719;&#20214;&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#21487;&#36127;&#25285;&#24615;&#12290;&#26412;&#25991;&#24378;&#35843;&#20102;AIOptimizer&#30340;&#35774;&#35745;&#22240;&#32032;&#65292;&#22914;&#20934;&#30830;&#24615;&#12289;&#36866;&#24212;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#29992;&#25143;&#21451;&#22909;&#24615;&#12290;&#20026;&#20102;&#25552;&#20379;&#26377;&#25928;&#30340;&#29992;&#25143;&#20013;&#24515;&#30340;&#24615;&#33021;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#24378;&#35843;&#20102;&#27169;&#22359;&#21270;&#35774;&#35745;&#12289;&#25968;&#25454;&#25910;&#38598;&#25216;&#26415;&#12289;&#25345;&#32493;&#23398;&#20064;&#21644;&#24377;&#24615;&#38598;&#25104;&#30340;&#20351;&#29992;&#12290;&#26412;&#25991;&#36824;&#35843;&#26597;&#20102;AIOptimizer&#30340;&#29305;&#24615;&#65292;&#22914;&#25925;&#38556;&#35782;&#21035;&#12289;&#25104;&#26412;&#20248;&#21270;&#24314;&#35758;&#12289;&#25928;&#29575;&#39044;&#27979;&#21644;&#21327;&#20316;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25506;&#35752;&#20102;&#20960;&#20010;&#36719;&#20214;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#27169;&#22411;&#65292;&#24182;&#20171;&#32461;&#20102;AIOptimizer&#20351;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25512;&#33616;&#24341;&#25806;&#36827;&#34892;&#25104;&#26412;&#20248;&#21270;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#31361;&#20986;AIOptimizer&#20316;&#20026;&#19968;&#31181;&#21033;&#29992;&#20808;&#36827;&#25216;&#26415;&#36827;&#34892;&#25104;&#26412;&#20248;&#21270;&#30340;&#21407;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research article introduces AIOptimizer, a prototype for a software performance optimisation tool based on cost reduction. AIOptimizer uses a recommendation system driven by reinforcement learning to improve software system efficiency and affordability. The paper highlights AIOptimizer's design factors, such as accuracy, adaptability, scalability, and user-friendliness. To provide effective and user-centric performance optimisation solutions, it emphasises the use of a modular design, data gathering techniques, continuous learning, and resilient integration. The article also investigates AIOptimizer features such as fault identification, cost optimisation recommendations, efficiency prediction, and cooperation. Furthermore, it explores several software development life cycle models and introduces AIOptimizer uses a reinforcement learning-based recommendation engine for cost optimisation. The purpose of this research study is to highlight AIOptimizer as a prototype that uses advanc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#32852;&#37030;&#26041;&#27861;&#22312;&#27969;&#34892;&#30149;&#30417;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20551;&#35774;&#26816;&#39564;&#26694;&#26550;&#65292;&#36890;&#36807;&#25512;&#36865;&#21040;&#20445;&#31649;&#20154;&#30340;&#38450;&#28779;&#22681;&#24182;&#36827;&#34892;&#20803;&#20998;&#26512;&#65292;&#26469;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#21644;&#20849;&#20139;&#38480;&#21046;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#21512;&#30340;$p$-&#20540;&#21512;&#24182;&#26041;&#27861;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#32852;&#37030;&#27969;&#34892;&#30149;&#30417;&#27979;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.02616</link><description>&lt;p&gt;
&#32852;&#37030;&#27969;&#34892;&#30149;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
Federated Epidemic Surveillance. (arXiv:2307.02616v1 [stat.AP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#32852;&#37030;&#26041;&#27861;&#22312;&#27969;&#34892;&#30149;&#30417;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20551;&#35774;&#26816;&#39564;&#26694;&#26550;&#65292;&#36890;&#36807;&#25512;&#36865;&#21040;&#20445;&#31649;&#20154;&#30340;&#38450;&#28779;&#22681;&#24182;&#36827;&#34892;&#20803;&#20998;&#26512;&#65292;&#26469;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#21644;&#20849;&#20139;&#38480;&#21046;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#21512;&#30340;$p$-&#20540;&#21512;&#24182;&#26041;&#27861;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#32852;&#37030;&#27969;&#34892;&#30149;&#30417;&#27979;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#34892;&#30149;&#30340;&#30417;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#24403;&#20851;&#38190;&#25968;&#25454;&#20998;&#25955;&#19988;&#21033;&#30410;&#30456;&#20851;&#26041;&#26080;&#27861;&#25110;&#19981;&#24895;&#20849;&#20139;&#26102;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38556;&#30861;&#65292;&#24212;&#24320;&#21457;&#32852;&#37030;&#26041;&#27861;&#26469;&#25972;&#21512;&#23454;&#20307;&#24895;&#24847;&#25552;&#20379;&#30340;&#36739;&#19981;&#25935;&#24863;&#30340;&#35777;&#25454;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#23558;&#20551;&#35774;&#26816;&#39564;&#25512;&#36865;&#21040;&#27599;&#20010;&#20445;&#31649;&#20154;&#30340;&#38450;&#28779;&#22681;&#21518;&#65292;&#20877;&#36890;&#36807;&#20803;&#20998;&#26512;&#26469;&#21512;&#24182;&#32467;&#26524;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#30830;&#23450;&#37325;&#24314;&#20551;&#35774;&#26816;&#39564;&#21644;&#20248;&#21270;&#25512;&#29702;&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20551;&#35774;&#26816;&#39564;&#26694;&#26550;&#26469;&#35782;&#21035;&#25351;&#26631;&#30340;&#28608;&#22686;&#65292;&#24182;&#23545;&#30495;&#23454;&#25968;&#25454;&#21644;&#21322;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#21151;&#25928;&#20998;&#26512;&#21644;&#23454;&#39564;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#20551;&#35774;&#26816;&#39564;&#30340;&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#21512;&#36866;&#30340;$p$-&#20540;&#21512;&#24182;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20984;&#26174;&#20102;&#20351;&#29992;$p$-&#20540;&#21512;&#24182;&#20316;&#20026;&#27969;&#34892;&#30149;&#30417;&#27979;&#30340;&#32852;&#37030;&#26041;&#27861;&#30340;&#28508;&#21147;&#65292;&#24182;&#20026;&#25972;&#21512;&#21487;&#29992;&#20449;&#24687;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The surveillance of a pandemic is a challenging task, especially when crucial data is distributed and stakeholders cannot or are unwilling to share. To overcome this obstacle, federated methodologies should be developed to incorporate less sensitive evidence that entities are willing to provide. This study aims to explore the feasibility of pushing hypothesis tests behind each custodian's firewall and then meta-analysis to combine the results, and to determine the optimal approach for reconstructing the hypothesis test and optimizing the inference. We propose a hypothesis testing framework to identify a surge in the indicators and conduct power analyses and experiments on real and semi-synthetic data to showcase the properties of our proposed hypothesis test and suggest suitable methods for combining $p$-values. Our findings highlight the potential of using $p$-value combination as a federated methodology for pandemic surveillance and provide valuable insights into integrating availabl
&lt;/p&gt;</description></item><item><title>DoReMi&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#35328;&#27169;&#22411;&#22522;&#30784;&#26550;&#26500;&#65292;&#36890;&#36807;&#26816;&#27979;&#21644;&#20462;&#22797;&#35745;&#21010;&#19982;&#25191;&#34892;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#26469;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#12290;&#35813;&#26550;&#26500;&#21033;&#29992;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#26816;&#26597;&#32422;&#26463;&#26465;&#20214;&#20197;&#21457;&#29616;&#19981;&#19968;&#33268;&#65292;&#24182;&#35843;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#37325;&#26032;&#35268;&#21010;&#20197;&#23454;&#29616;&#24674;&#22797;&#12290;</title><link>http://arxiv.org/abs/2307.00329</link><description>&lt;p&gt;
DoReMi: &#36890;&#36807;&#26816;&#27979;&#21644;&#20462;&#22797;&#35745;&#21010;&#25191;&#34892;&#19981;&#19968;&#33268;&#26469;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
DoReMi: Grounding Language Model by Detecting and Recovering from Plan-Execution Misalignment. (arXiv:2307.00329v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00329
&lt;/p&gt;
&lt;p&gt;
DoReMi&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#35328;&#27169;&#22411;&#22522;&#30784;&#26550;&#26500;&#65292;&#36890;&#36807;&#26816;&#27979;&#21644;&#20462;&#22797;&#35745;&#21010;&#19982;&#25191;&#34892;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#26469;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#12290;&#35813;&#26550;&#26500;&#21033;&#29992;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#26816;&#26597;&#32422;&#26463;&#26465;&#20214;&#20197;&#21457;&#29616;&#19981;&#19968;&#33268;&#65292;&#24182;&#35843;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#37325;&#26032;&#35268;&#21010;&#20197;&#23454;&#29616;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21253;&#21547;&#22823;&#37327;&#30340;&#35821;&#20041;&#30693;&#35782;&#65292;&#24182;&#20855;&#22791;&#20986;&#33394;&#30340;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;&#22914;&#20309;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#26426;&#22120;&#20154;&#20219;&#21153;&#30456;&#32467;&#21512;&#65292;&#20197;&#30830;&#20445;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#24207;&#21015;&#22312;&#36923;&#36753;&#19978;&#27491;&#30830;&#19988;&#21487;&#25191;&#34892;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29615;&#22659;&#25200;&#21160;&#25110;&#25511;&#21046;&#22120;&#35774;&#35745;&#30340;&#19981;&#23436;&#21892;&#65292;&#24213;&#23618;&#25191;&#34892;&#21487;&#33021;&#20250;&#20559;&#31163;&#39640;&#32423;&#35745;&#21010;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DoReMi&#30340;&#26032;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#30784;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#33021;&#22815;&#21450;&#26102;&#26816;&#27979;&#21644;&#20462;&#22797;&#35745;&#21010;&#19982;&#25191;&#34892;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;LLM&#36827;&#34892;&#35268;&#21010;&#65292;&#24182;&#29983;&#25104;&#35745;&#21010;&#27493;&#39588;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#36825;&#20123;&#32422;&#26463;&#26465;&#20214;&#21487;&#20197;&#25351;&#31034;&#35745;&#21010;&#19982;&#25191;&#34892;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#19988;&#25105;&#20204;&#20351;&#29992;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#27169;&#22411;&#22312;&#20302;&#23618;&#25216;&#33021;&#25191;&#34892;&#36807;&#31243;&#20013;&#26816;&#26597;&#32422;&#26463;&#26465;&#20214;&#12290;&#22914;&#26524;&#21457;&#29983;&#29305;&#23450;&#30340;&#19981;&#19968;&#33268;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#35843;&#29992;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#35268;&#21010;&#20197;&#20174;&#20013;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models encode a vast amount of semantic knowledge and possess remarkable understanding and reasoning capabilities. Previous research has explored how to ground language models in robotic tasks to ensure that the sequences generated by the language model are both logically correct and practically executable. However, low-level execution may deviate from the high-level plan due to environmental perturbations or imperfect controller design. In this paper, we propose DoReMi, a novel language model grounding framework that enables immediate Detection and Recovery from Misalignments between plan and execution. Specifically, LLMs are leveraged for both planning and generating constraints for planned steps. These constraints can indicate plan-execution misalignments and we use a vision question answering (VQA) model to check constraints during low-level skill execution. If certain misalignment occurs, our method will call the language model to re-plan in order to recover from mi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#65292;&#36873;&#25321;&#20102;15&#20010;&#20856;&#22411;&#25968;&#25454;&#38598;&#65292;&#32771;&#34385;&#20102;&#28436;&#32462;&#12289;&#24402;&#32435;&#12289;&#38463;&#24067;&#36798;&#26031;&#21644;&#28151;&#21512;&#25512;&#29702;&#24418;&#24335;&#65292;&#24182;&#36873;&#25321;&#20102;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;LLMs&#36827;&#34892;&#38646;&#26679;&#26412;&#12289;&#19968;&#27425;&#21644;&#19977;&#27425;&#30340;&#35774;&#32622;&#19979;&#35780;&#20272;&#12290;&#25552;&#20986;&#31934;&#32454;&#32423;&#21035;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09841</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30495;&#30340;&#26159;&#33391;&#22909;&#30340;&#36923;&#36753;&#25512;&#29702;&#32773;&#21527;&#65311;&#22522;&#20110;&#28436;&#32462;&#12289;&#24402;&#32435;&#21644;&#38463;&#24067;&#36798;&#26031;&#35266;&#28857;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation From Deductive, Inductive and Abductive Views. (arXiv:2306.09841v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#65292;&#36873;&#25321;&#20102;15&#20010;&#20856;&#22411;&#25968;&#25454;&#38598;&#65292;&#32771;&#34385;&#20102;&#28436;&#32462;&#12289;&#24402;&#32435;&#12289;&#38463;&#24067;&#36798;&#26031;&#21644;&#28151;&#21512;&#25512;&#29702;&#24418;&#24335;&#65292;&#24182;&#36873;&#25321;&#20102;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;LLMs&#36827;&#34892;&#38646;&#26679;&#26412;&#12289;&#19968;&#27425;&#21644;&#19977;&#27425;&#30340;&#35774;&#32622;&#19979;&#35780;&#20272;&#12290;&#25552;&#20986;&#31934;&#32454;&#32423;&#21035;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#23545;LLMs&#30340;&#20855;&#20307;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#65292;&#22914;&#22810;&#35821;&#35328;&#25512;&#29702;&#21644;&#25968;&#23398;&#25512;&#29702;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#20851;&#38190;&#25512;&#29702;&#35270;&#35282;&#20043;&#19968;&#65292;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#36824;&#27809;&#26377;&#24471;&#21040;&#24443;&#24213;&#35780;&#20272;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#20123;&#24046;&#36317;&#24182;&#25552;&#20379;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#36827;&#34892;&#31995;&#32479;&#21270;&#35780;&#20272;&#65292;&#26412;&#25991;&#36873;&#25321;&#20102;15&#20010;&#20856;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#32455;&#25104;&#28436;&#32462;&#12289;&#24402;&#32435;&#12289;&#38463;&#24067;&#36798;&#26031;&#21644;&#28151;&#21512;&#24418;&#24335;&#30340;&#25512;&#29702;&#35774;&#32622;&#12290;&#32771;&#34385;&#35780;&#20272;&#30340;&#20840;&#38754;&#24615;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;LLMs&#65288;text-davinci-003&#65292;ChatGPT&#21644;BARD&#65289;&#65292;&#24182;&#22312;&#38646;&#26679;&#26412;&#12289;&#19968;&#27425;&#21644;&#19977;&#27425;&#30340;&#35774;&#32622;&#19979;&#23545;&#25152;&#26377;&#36873;&#25321;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;&#20854;&#27425;&#65292;&#19982;&#20197;&#24448;&#20165;&#20381;&#36182;&#31616;&#21333;&#25351;&#26631;&#65288;&#22914;&#20934;&#30830;&#24615;&#65289;&#30340;&#35780;&#20272;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20174;&#30446;&#26631;&#25512;&#29702;&#35282;&#24230;&#36827;&#34892;&#30340;&#31934;&#32454;&#32423;&#21035;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved great success in various natural language tasks. It has aroused much interest in evaluating the specific reasoning capability of LLMs, such as multilingual reasoning and mathematical reasoning. However, as one of the key reasoning perspectives, logical reasoning capability has not yet been thoroughly evaluated. In this work, we aim to bridge those gaps and provide comprehensive evaluations. Firstly, to offer systematic evaluations, this paper selects fifteen typical logical reasoning datasets and organizes them into deductive, inductive, abductive and mixed-form reasoning settings. Considering the comprehensiveness of evaluations, we include three representative LLMs (i.e., text-davinci-003, ChatGPT and BARD) and evaluate them on all selected datasets under zero-shot, one-shot and three-shot settings. Secondly, different from previous evaluations relying only on simple metrics (e.g., accuracy), we propose fine-level evaluations from objective 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36923;&#36753;&#30340;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#31070;&#32463;&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#21487;&#35299;&#37322;&#24615;&#36923;&#36753;&#23376;&#21477;&#34920;&#36798;&#30446;&#26631;&#20219;&#21153;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#24182;&#20351;&#29992;&#31070;&#32463;&#34920;&#24449;&#21442;&#25968;&#21270;&#31526;&#21495;&#36923;&#36753;&#20803;&#32032;&#65292;&#20174;&#32780;&#20415;&#20110;&#33258;&#21160;&#29983;&#25104;&#21644;&#35780;&#20272;&#26377;&#24847;&#20041;&#30340;&#36923;&#36753;&#23376;&#21477;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#20116;&#20010;&#20803;&#39044;&#27979;&#22120;&#26469;&#25429;&#33719;&#34394;&#20551;&#20449;&#24687;&#30340;&#22522;&#26412;&#27169;&#24335;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#19981;&#20165;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;&#24403;&#21069;&#26041;&#27861;&#65292;&#32780;&#19988;&#25552;&#20379;&#20102;&#36879;&#26126;&#19988;&#21487;&#35299;&#37322;&#30340;&#36923;&#36753;&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2305.05964</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#35299;&#37322;&#24615;&#26816;&#27979;&#19982;&#36923;&#36753;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Interpretable Multimodal Misinformation Detection with Logic Reasoning. (arXiv:2305.05964v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36923;&#36753;&#30340;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#31070;&#32463;&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#21487;&#35299;&#37322;&#24615;&#36923;&#36753;&#23376;&#21477;&#34920;&#36798;&#30446;&#26631;&#20219;&#21153;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#24182;&#20351;&#29992;&#31070;&#32463;&#34920;&#24449;&#21442;&#25968;&#21270;&#31526;&#21495;&#36923;&#36753;&#20803;&#32032;&#65292;&#20174;&#32780;&#20415;&#20110;&#33258;&#21160;&#29983;&#25104;&#21644;&#35780;&#20272;&#26377;&#24847;&#20041;&#30340;&#36923;&#36753;&#23376;&#21477;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#20116;&#20010;&#20803;&#39044;&#27979;&#22120;&#26469;&#25429;&#33719;&#34394;&#20551;&#20449;&#24687;&#30340;&#22522;&#26412;&#27169;&#24335;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#19981;&#20165;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;&#24403;&#21069;&#26041;&#27861;&#65292;&#32780;&#19988;&#25552;&#20379;&#20102;&#36879;&#26126;&#19988;&#21487;&#35299;&#37322;&#30340;&#36923;&#36753;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#31038;&#20132;&#24179;&#21488;&#19978;&#30340;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#30001;&#20110;&#22810;&#23186;&#20307;&#20869;&#23481;&#30340;&#21487;&#20449;&#24230;&#21644;&#20256;&#25773;&#26356;&#23481;&#26131;&#32780;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#26816;&#27979;&#26041;&#27861;&#24050;&#32463;&#36798;&#21040;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#65292;&#20294;&#32570;&#20047;&#35299;&#37322;&#24615;&#38459;&#30861;&#20102;&#36825;&#20123;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#23454;&#38469;&#37096;&#32626;&#12290;&#21463;&#21040; NeuralSymbolic AI &#30340;&#21551;&#21457;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#33021;&#21147;&#21644;&#31526;&#21495;&#23398;&#20064;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36923;&#36753;&#30340;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#31070;&#32463;&#27169;&#22411;&#65292;&#23427;&#38598;&#25104;&#20102;&#21487;&#35299;&#37322;&#24615;&#36923;&#36753;&#23376;&#21477;&#20197;&#34920;&#36798;&#30446;&#26631;&#20219;&#21153;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#20026;&#20102;&#20351;&#23398;&#20064;&#26377;&#25928;&#65292;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#34920;&#24449;&#26469;&#21442;&#25968;&#21270;&#31526;&#21495;&#36923;&#36753;&#20803;&#32032;&#65292;&#20174;&#32780;&#20415;&#20110;&#33258;&#21160;&#29983;&#25104;&#21644;&#35780;&#20272;&#26377;&#24847;&#20041;&#30340;&#36923;&#36753;&#23376;&#21477;&#12290;&#21478;&#22806;&#65292;&#20026;&#20102;&#20351;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#36866;&#29992;&#20110;&#21508;&#31181;&#34394;&#20551;&#20449;&#24687;&#26469;&#28304;&#65292;&#25105;&#20204;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#20013;&#24341;&#20837;&#20102;&#20116;&#20010;&#20803;&#39044;&#27979;&#22120;&#26469;&#25429;&#33719;&#34394;&#20551;&#20449;&#24687;&#30340;&#22522;&#26412;&#27169;&#24335;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#30340;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#20165;&#26174;&#30528;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#36824;&#20026;&#27599;&#20010;&#39044;&#27979;&#25552;&#20379;&#20102;&#36879;&#26126;&#19988;&#21487;&#35299;&#37322;&#30340;&#36923;&#36753;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal misinformation on online social platforms is becoming a critical concern due to increasing credibility and easier dissemination brought by multimedia content, compared to traditional text-only information. While existing multimodal detection approaches have achieved high performance, the lack of interpretability hinders these systems' reliability and practical deployment. Inspired by NeuralSymbolic AI which combines the learning ability of neural networks with the explainability of symbolic learning, we propose a novel logic-based neural model for multimodal misinformation detection which integrates interpretable logic clauses to express the reasoning process of the target task. To make learning effective, we parameterize symbolic logical elements using neural representations, which facilitate the automatic generation and evaluation of meaningful logic clauses. Additionally, to make our framework generalizable across diverse misinformation sources, we introduce five meta-pre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#26469;&#27979;&#35797;ITM&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#23558;&#19968;&#20123;&#8220;&#24858;&#24324;&#8221;&#30340;&#22270;&#29255;&#21644;&#26631;&#39064;&#28155;&#21152;&#21040;&#26816;&#32034;&#27744;&#20013;&#65292;&#22312;MS COCO&#25968;&#25454;&#38598;&#19978;&#20026;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#40065;&#26834;&#24615;&#27979;&#35797;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;</title><link>http://arxiv.org/abs/2304.10727</link><description>&lt;p&gt;
RoCOCO&#65306;&#31283;&#20581;&#30340;&#22522;&#20934;MS-COCO&#35780;&#20272;&#22270;&#25991;&#21305;&#37197;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
RoCOCO: Robust Benchmark MS-COCO to Stress-test Robustness of Image-Text Matching Models. (arXiv:2304.10727v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#26469;&#27979;&#35797;ITM&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#23558;&#19968;&#20123;&#8220;&#24858;&#24324;&#8221;&#30340;&#22270;&#29255;&#21644;&#26631;&#39064;&#28155;&#21152;&#21040;&#26816;&#32034;&#27744;&#20013;&#65292;&#22312;MS COCO&#25968;&#25454;&#38598;&#19978;&#20026;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#40065;&#26834;&#24615;&#27979;&#35797;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#20041;&#23884;&#20837;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;MS COCO 5K&#27979;&#35797;&#38598;&#19978;&#22270;&#25991;&#21305;&#37197;&#65288;ITM&#65289;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#36825;&#20123;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#26102;&#65292;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#26469;&#27979;&#35797;ITM&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#21508;&#31181;&#8220;&#24858;&#24324;&#8221;&#30340;&#22270;&#29255;&#21644;&#26631;&#39064;&#28155;&#21152;&#21040;&#26816;&#32034;&#27744;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#25554;&#20837;&#19981;&#30456;&#20851;&#30340;&#22270;&#20687;&#26469;&#26356;&#25913;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#26367;&#25442;&#21517;&#35789;&#26469;&#26356;&#25913;&#26631;&#39064;&#65292;&#20174;&#32780;&#25913;&#21464;&#21477;&#23376;&#30340;&#21547;&#20041;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#20165;&#23558;&#36825;&#20123;&#26032;&#21019;&#24314;&#30340;&#22270;&#20687;&#21644;&#26631;&#39064;&#28155;&#21152;&#21040;&#27979;&#35797;&#38598;&#20013;&#23601;&#21487;&#20197;&#38477;&#20302;&#21508;&#31181;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#65288;&#20363;&#22914;&#65292;&#22312;BLIP&#20013;&#20174;81.9&#65285;&#38477;&#33267;64.5&#65285;&#65292;&#22312;VSE&#8734;&#20013;&#20174;66.1&#65285;&#38477;&#33267;37.5&#65285;&#65289;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#21457;&#29616;&#33021;&#20026;&#25552;&#39640;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#35774;&#35745;&#26356;&#22810;&#26679;&#21270;&#30340;&#21387;&#21147;&#27979;&#35797;&#25552;&#20379;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large-scale vision-language pre-training models and visual semantic embedding methods have significantly improved image-text matching (ITM) accuracy on MS COCO 5K test set. However, it is unclear how robust these state-of-the-art (SOTA) models are when using them in the wild. In this paper, we propose a novel evaluation benchmark to stress-test the robustness of ITM models. To this end, we add various fooling images and captions to a retrieval pool. Specifically, we change images by inserting unrelated images, and change captions by substituting a noun, which can change the meaning of a sentence. We discover that just adding these newly created images and captions to the test set can degrade performances (i.e., Recall@1) of a wide range of SOTA models (e.g., 81.9% $\rightarrow$ 64.5% in BLIP, 66.1% $\rightarrow$ 37.5% in VSE$\infty$). We expect that our findings can provide insights for improving the robustness of the vision-language models and devising more diverse stress-te
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26377;&#38480;&#23485;&#24230;&#30340;&#21453;&#27169;&#22411;&#26597;&#35810;&#19968;&#38454;&#29702;&#35770;&#30340;&#21487;&#20915;&#23450;&#24615;&#24182;&#25552;&#20986;&#20998;&#21106;&#23485;&#24230;&#65292;&#20351;&#20854;&#33021;&#22815;&#25429;&#33719;&#23454;&#38469;&#30456;&#20851;&#30340;&#26597;&#35810;&#35821;&#35328;</title><link>http://arxiv.org/abs/2304.06348</link><description>&lt;p&gt;
&#36890;&#36807;&#26377;&#38480;&#23485;&#24230;&#30340;&#21453;&#27169;&#22411;&#26597;&#35810;&#19968;&#38454;&#29702;&#35770;&#30340;&#21487;&#20915;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Decidability of Querying First-Order Theories via Countermodels of Finite Width. (arXiv:2304.06348v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06348
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26377;&#38480;&#23485;&#24230;&#30340;&#21453;&#27169;&#22411;&#26597;&#35810;&#19968;&#38454;&#29702;&#35770;&#30340;&#21487;&#20915;&#23450;&#24615;&#24182;&#25552;&#20986;&#20998;&#21106;&#23485;&#24230;&#65292;&#20351;&#20854;&#33021;&#22815;&#25429;&#33719;&#23454;&#38469;&#30456;&#20851;&#30340;&#26597;&#35810;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#22522;&#20110;&#20855;&#26377;&#32467;&#26500;&#31616;&#21333;&#30340;&#21453;&#27169;&#22411;&#30340;&#23384;&#22312;&#24615;&#65288;&#36890;&#36807;&#26576;&#20123;&#31867;&#22411;&#30340;&#23485;&#24230;&#37327;&#26469;&#34913;&#37327;&#65292;&#21253;&#25324;&#26641;&#23485;&#21644;&#22242;&#23485;&#31561;&#65289;&#65292;&#20026;&#24191;&#27867;&#30340;&#36923;&#36753;&#34164;&#21547;&#38382;&#39064;&#65288;&#31616;&#31216;&#26597;&#35810;&#65289;&#30340;&#21487;&#20915;&#23450;&#24615;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;&#20316;&#20026;&#25105;&#20204;&#26694;&#26550;&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#20363;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#23637;&#29616;&#20986;&#23485;&#24230;&#26377;&#38480;&#26377;&#38480;&#36890;&#29992;&#27169;&#22411;&#38598;&#30340;&#36923;&#36753;&#65292;&#20445;&#35777;&#20102;&#21508;&#31181;&#21516;&#24577;&#23553;&#38381;&#26597;&#35810;&#30340;&#21487;&#20915;&#23450;&#24615;&#65292;&#21253;&#25324;&#20102;&#21508;&#31181;&#23454;&#38469;&#30456;&#20851;&#30340;&#26597;&#35810;&#35821;&#35328;&#12290;&#20316;&#20026;&#19968;&#20010;&#29305;&#21035;&#24378;&#22823;&#30340;&#23485;&#24230;&#37327;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Blumensath&#30340;&#20998;&#21106;&#23485;&#24230;&#65292;&#35813;&#37327;&#21253;&#21547;&#20102;&#21508;&#31181;&#36890;&#24120;&#32771;&#34385;&#30340;&#23485;&#24230;&#37327;&#65292;&#20855;&#26377;&#38750;&#24120;&#26377;&#21033;&#30340;&#35745;&#31639;&#21644;&#32467;&#26500;&#29305;&#24615;&#12290;&#38024;&#23545;&#26222;&#36941;&#23637;&#29616;&#23384;&#22312;&#24615;&#35268;&#21017;&#20026;&#19968;&#20010;&#23637;&#31034;&#26696;&#20363;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#26377;&#38480;&#20998;&#21106;&#23485;&#24230;&#35268;&#21017;&#38598;&#21253;&#21547;&#20854;&#20182;&#24050;&#30693;&#30340;&#25277;&#35937;&#21487;&#20915;&#23450;&#31867;&#65292;&#20294;&#20511;&#21161;&#29616;&#26377;&#30340;&#20998;&#23618;&#21644;&#21463;&#25511;&#35268;&#21017;&#38598;&#27010;&#24565;&#65292;&#20063;&#20351;&#25105;&#20204;&#33021;&#22815;&#25429;&#33719;&#23454;&#38469;&#30456;&#20851;&#30340;&#26597;&#35810;&#35821;&#35328;&#65292;&#20363;&#22914;&#27491;&#21017;&#65292;&#36830;&#25509;&#21644;&#24067;&#23572;&#36830;&#25509;&#26597;&#35810;&#12290;&#25105;&#20204;&#20197;&#23384;&#22312;&#35268;&#21017;&#30340;&#24418;&#24335;&#20026;&#37325;&#28857;&#65292;&#34917;&#20805;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#24182;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#21508;&#31181;&#39640;&#32423;&#30693;&#35782;&#22788;&#29702;&#22330;&#26223;&#20013;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a generic framework for establishing the decidability of a wide range of logical entailment problems (briefly called querying), based on the existence of countermodels that are structurally simple, gauged by certain types of width measures (with treewidth and cliquewidth as popular examples). As an important special case of our framework, we identify logics exhibiting width-finite finitely universal model sets, warranting decidable entailment for a wide range of homomorphism-closed queries, subsuming a diverse set of practically relevant query languages. As a particularly powerful width measure, we propose Blumensath's partitionwidth, which subsumes various other commonly considered width measures and exhibits highly favorable computational and structural properties. Focusing on the formalism of existential rules as a popular showcase, we explain how finite partitionwidth sets of rules subsume other known abstract decidable classes but -- leveraging existing notions of strat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphMLP&#30340;&#22270;&#24418;&#22686;&#24378;&#30340;MLP&#24335;&#26550;&#26500;&#65292;&#23427;&#23558;&#22270;&#24418;&#32467;&#26500;&#32435;&#20837;MLP&#27169;&#22411;&#20013;&#65292;&#20197;&#28385;&#36275;3D&#20154;&#20307;&#23039;&#24577;&#30340;&#39046;&#22495;&#29305;&#23450;&#38656;&#27714;&#65292;&#21516;&#26102;&#20801;&#35768;&#23616;&#37096;&#21644;&#20840;&#23616;&#30340;&#31354;&#38388;&#20132;&#20114;&#20316;&#29992;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#36824;&#23558;GraphMLP&#28789;&#27963;&#39640;&#25928;&#22320;&#25193;&#23637;&#21040;&#35270;&#39057;&#39046;&#22495;&#65292;&#24182;&#25104;&#21151;&#22320;&#36827;&#34892;&#20102;&#26102;&#38388;&#21160;&#21147;&#23398;&#30340;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2206.06420</link><description>&lt;p&gt;
GraphMLP&#65306;&#19968;&#31181;&#29992;&#20110;3D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#30340;&#22270;&#24418;MLP&#24335;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
GraphMLP: A Graph MLP-Like Architecture for 3D Human Pose Estimation. (arXiv:2206.06420v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06420
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphMLP&#30340;&#22270;&#24418;&#22686;&#24378;&#30340;MLP&#24335;&#26550;&#26500;&#65292;&#23427;&#23558;&#22270;&#24418;&#32467;&#26500;&#32435;&#20837;MLP&#27169;&#22411;&#20013;&#65292;&#20197;&#28385;&#36275;3D&#20154;&#20307;&#23039;&#24577;&#30340;&#39046;&#22495;&#29305;&#23450;&#38656;&#27714;&#65292;&#21516;&#26102;&#20801;&#35768;&#23616;&#37096;&#21644;&#20840;&#23616;&#30340;&#31354;&#38388;&#20132;&#20114;&#20316;&#29992;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#36824;&#23558;GraphMLP&#28789;&#27963;&#39640;&#25928;&#22320;&#25193;&#23637;&#21040;&#35270;&#39057;&#39046;&#22495;&#65292;&#24182;&#25104;&#21151;&#22320;&#36827;&#34892;&#20102;&#26102;&#38388;&#21160;&#21147;&#23398;&#30340;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#27169;&#22411;&#24050;&#32463;&#23637;&#29616;&#20986;&#22312;&#27809;&#26377;&#33258;&#25105;&#27880;&#24847;&#21147;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#35270;&#35273;&#34920;&#31034;&#26041;&#38754;&#30340;&#31454;&#20105;&#24615;&#32467;&#26524;&#65292;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MLP&#27169;&#22411;&#24182;&#19981;&#25797;&#38271;&#25429;&#25417;&#23616;&#37096;&#32454;&#33410;&#65292;&#20063;&#32570;&#20047;&#26377;&#20851;&#20154;&#20307;&#26500;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#29992;&#20110;&#39592;&#39612;&#34920;&#31034;&#23398;&#20064;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22270;&#24418;&#22686;&#24378;&#30340;MLP&#24335;&#26550;&#26500;&#65292;&#31216;&#20026;GraphMLP&#65292;&#23427;&#32467;&#21512;&#20102;MLP&#21644;&#22270;&#24418;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#22312;&#20840;&#23616;-&#23616;&#37096;-&#22270;&#24418;&#32479;&#19968;&#26550;&#26500;&#20013;&#29992;&#20110;3D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#12290;GraphMLP&#23558;&#20154;&#20307;&#30340;&#22270;&#24418;&#32467;&#26500;&#32435;&#20837;MLP&#27169;&#22411;&#20013;&#65292;&#20197;&#28385;&#36275;3D&#20154;&#20307;&#23039;&#24577;&#30340;&#39046;&#22495;&#29305;&#23450;&#38656;&#27714;&#65292;&#21516;&#26102;&#20801;&#35768;&#23616;&#37096;&#21644;&#20840;&#23616;&#30340;&#31354;&#38388;&#20132;&#20114;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;GraphMLP&#28789;&#27963;&#39640;&#25928;&#22320;&#25193;&#23637;&#21040;&#35270;&#39057;&#39046;&#22495;&#65292;&#24182;&#23637;&#31034;&#20102;&#21487;&#20197;&#20197;&#21487;&#24573;&#30053;&#30340;&#35745;&#31639;&#20195;&#20215;&#26469;&#26377;&#25928;&#22320;&#24314;&#27169;&#22797;&#26434;&#30340;&#26102;&#38388;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern multi-layer perceptron (MLP) models have shown competitive results in learning visual representations without self-attention. However, existing MLP models are not good at capturing local details and lack prior knowledge of human body configurations, which limits their modeling power for skeletal representation learning. To address these issues, we propose a simple yet effective graph-reinforced MLP-Like architecture, named GraphMLP, that combines MLPs and graph convolutional networks (GCNs) in a global-local-graphical unified architecture for 3D human pose estimation. GraphMLP incorporates the graph structure of human bodies into an MLP model to meet the domain-specific demand of the 3D human pose, while allowing for both local and global spatial interactions. Furthermore, we propose to flexibly and efficiently extend the GraphMLP to the video domain and show that complex temporal dynamics can be effectively modeled in a simple way with negligible computational cost gains in the
&lt;/p&gt;</description></item></channel></rss>