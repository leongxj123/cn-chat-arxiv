<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KTO&#30340;&#26041;&#27861;&#65292;&#23558;&#27169;&#22411;&#23545;&#40784;&#35270;&#20026;&#23637;&#26395;&#29702;&#35770;&#20248;&#21270;&#12290;&#19982;&#24403;&#21069;&#26041;&#27861;&#30456;&#27604;&#65292;KTO&#30452;&#25509;&#26368;&#22823;&#21270;&#29983;&#25104;&#25928;&#29992;&#32780;&#19981;&#26159;&#26368;&#22823;&#21270;&#20559;&#22909;&#23545;&#25968;&#20284;&#28982;&#12290;&#22312;&#22810;&#20010;&#35268;&#27169;&#19978;&#65292;KTO&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#20559;&#22909;&#30340;&#26041;&#27861;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01306</link><description>&lt;p&gt;
KTO: &#27169;&#22411;&#23545;&#40784;&#35270;&#20026;&#23637;&#26395;&#29702;&#35770;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
KTO: Model Alignment as Prospect Theoretic Optimization
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KTO&#30340;&#26041;&#27861;&#65292;&#23558;&#27169;&#22411;&#23545;&#40784;&#35270;&#20026;&#23637;&#26395;&#29702;&#35770;&#20248;&#21270;&#12290;&#19982;&#24403;&#21069;&#26041;&#27861;&#30456;&#27604;&#65292;KTO&#30452;&#25509;&#26368;&#22823;&#21270;&#29983;&#25104;&#25928;&#29992;&#32780;&#19981;&#26159;&#26368;&#22823;&#21270;&#20559;&#22909;&#23545;&#25968;&#20284;&#28982;&#12290;&#22312;&#22810;&#20010;&#35268;&#27169;&#19978;&#65292;KTO&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#20559;&#22909;&#30340;&#26041;&#27861;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20975;&#24681;&#26364;&#19982;&#29305;&#27779;&#26031;&#22522;&#30340;&#23637;&#26395;&#29702;&#35770;&#21578;&#35785;&#25105;&#20204;&#65292;&#20154;&#31867;&#20197;&#26377;&#20559;&#35265;&#20294;&#26126;&#30830;&#30340;&#26041;&#24335;&#30475;&#24453;&#38543;&#26426;&#21464;&#37327;&#65307;&#20363;&#22914;&#65292;&#20154;&#20204;&#36890;&#24120;&#37117;&#26159;&#21388;&#24694;&#25439;&#22833;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23558;LLMs&#19982;&#20154;&#24037;&#21453;&#39304;&#36827;&#34892;&#23545;&#40784;&#30340;&#30446;&#26631;&#38544;&#21547;&#22320;&#34701;&#21512;&#20102;&#35768;&#22810;&#36825;&#20123;&#20559;&#35265; - &#36825;&#20123;&#30446;&#26631; (&#20363;&#22914; DPO) &#30340;&#25104;&#21151;&#37096;&#20998;&#21487;&#24402;&#22240;&#20110;&#23427;&#20204;&#26159;"&#20154;&#31867;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;"(HALOs)&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#25152;&#24402;&#22240;&#32473;&#20154;&#31867;&#30340;&#25928;&#29992;&#20989;&#25968;&#20173;&#19982;&#23637;&#26395;&#29702;&#35770;&#25991;&#29486;&#20013;&#30340;&#19981;&#21516;&#12290;&#21033;&#29992;&#20975;&#24681;&#26364;-&#29305;&#27779;&#26031;&#22522;&#20154;&#31867;&#25928;&#29992;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#26368;&#22823;&#21270;&#29983;&#25104;&#25928;&#29992;&#32780;&#19981;&#26159;&#26368;&#22823;&#21270;&#20559;&#22909;&#23545;&#25968;&#20284;&#28982;&#30340;HALO&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;&#20975;&#24681;&#26364;-&#29305;&#27779;&#26031;&#22522;&#20248;&#21270;(KTO)&#65292;&#24182;&#19988;&#23427;&#22312;&#20174;1B&#21040;30B&#30340;&#35268;&#27169;&#19978;&#19982;&#22522;&#20110;&#20559;&#22909;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#25110;&#36229;&#36807;&#12290;&#20851;&#38190;&#26159;&#65292;KTO&#19981;&#38656;&#35201;&#20559;&#22909; - &#21482;&#38656;&#35201;&#19968;&#20010;&#26159;&#21542;&#30340;&#20108;&#36827;&#21046;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kahneman &amp; Tversky's $\textit{prospect theory}$ tells us that humans perceive random variables in a biased but well-defined manner; for example, humans are famously loss-averse. We show that objectives for aligning LLMs with human feedback implicitly incorporate many of these biases -- the success of these objectives (e.g., DPO) over cross-entropy minimization can partly be ascribed to them being $\textit{human-aware loss functions}$ (HALOs). However, the utility functions these methods attribute to humans still differ from those in the prospect theory literature. Using a Kahneman-Tversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do. We call this approach Kahneman-Tversky Optimization (KTO), and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B. Crucially, KTO does not need preferences -- only a binary signal of whether 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22686;&#24378;Transformer&#19982;&#24490;&#29615;&#26426;&#21046;&#30340;&#20004;&#31181;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25193;&#23637;&#21644;&#32452;&#21512;&#26041;&#27861;&#12290;&#22312;&#22810;&#20010;&#35786;&#26029;&#20219;&#21153;&#20013;&#36827;&#34892;&#27604;&#36739;&#65292;&#25506;&#32034;&#23427;&#20204;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00976</link><description>&lt;p&gt;
&#20855;&#26377;&#21160;&#24577;&#20572;&#27490;&#30340;&#24490;&#29615;Transformer
&lt;/p&gt;
&lt;p&gt;
Recurrent Transformers with Dynamic Halt
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22686;&#24378;Transformer&#19982;&#24490;&#29615;&#26426;&#21046;&#30340;&#20004;&#31181;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25193;&#23637;&#21644;&#32452;&#21512;&#26041;&#27861;&#12290;&#22312;&#22810;&#20010;&#35786;&#26029;&#20219;&#21153;&#20013;&#36827;&#34892;&#27604;&#36739;&#65292;&#25506;&#32034;&#23427;&#20204;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#20027;&#35201;&#26041;&#27861;&#22312;&#22686;&#24378;Transformer&#19982;&#24490;&#29615;&#26426;&#21046;&#26041;&#38754;&#30340;&#24402;&#32435;&#20559;&#22909;&#8212;&#8212;&#65288;1&#65289;&#31867;&#20284;&#20110;Universal Transformers&#30340;&#28145;&#24230;&#36880;&#23618;&#24490;&#29615;&#26041;&#27861;&#65307;&#21644;&#65288;2&#65289;&#31867;&#20284;&#20110;Temporal Latent Bottleneck&#30340;&#20998;&#22359;&#26102;&#24577;&#24490;&#29615;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#25193;&#23637;&#21644;&#32452;&#21512;&#19978;&#36848;&#26041;&#27861;&#30340;&#26032;&#26041;&#24335;&#65292;&#20363;&#22914;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#23616;&#22343;&#20540;&#30340;Universal Transformer&#21160;&#24577;&#20572;&#27490;&#26426;&#21046;&#65292;&#24182;&#23558;Universal Transformer&#30340;&#20803;&#32032;&#34701;&#20837;&#21040;Temporal Latent Bottleneck&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#20010;&#35786;&#26029;&#20219;&#21153;&#65288;&#22914;Long Range Arena&#65288;LRA&#65289;&#65292;&#32763;&#36716;-&#32763;&#36716;&#35821;&#35328;&#24314;&#27169;&#65292;ListOps&#21644;&#36923;&#36753;&#25512;&#29702;&#65289;&#27604;&#36739;&#20102;&#27169;&#22411;&#24182;&#25506;&#32034;&#20102;&#23427;&#20204;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the inductive biases of two major approaches to augmenting Transformers with a recurrent mechanism - (1) the approach of incorporating a depth-wise recurrence similar to Universal Transformers; and (2) the approach of incorporating a chunk-wise temporal recurrence like Temporal Latent Bottleneck. Furthermore, we propose and investigate novel ways to extend and combine the above methods - for example, we propose a global mean-based dynamic halting mechanism for Universal Transformer and an augmentation of Temporal Latent Bottleneck with elements from Universal Transformer. We compare the models and probe their inductive biases in several diagnostic tasks such as Long Range Arena (LRA), flip-flop language modeling, ListOps, and Logical Inference.
&lt;/p&gt;</description></item><item><title>PhysORD&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#23558;&#29289;&#29702;&#23450;&#24459;&#34701;&#20837;&#31070;&#32463;&#27169;&#22411;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#36234;&#37326;&#39550;&#39542;&#20013;&#30340;&#36816;&#21160;&#39044;&#27979;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.01596</link><description>&lt;p&gt;
PhysORD&#65306;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#29992;&#20110;&#36234;&#37326;&#39550;&#39542;&#20013;&#27880;&#20837;&#29289;&#29702;&#23398;&#30340;&#36816;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
PhysORD: A Neuro-Symbolic Approach for Physics-infused Motion Prediction in Off-road Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01596
&lt;/p&gt;
&lt;p&gt;
PhysORD&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#23558;&#29289;&#29702;&#23450;&#24459;&#34701;&#20837;&#31070;&#32463;&#27169;&#22411;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#36234;&#37326;&#39550;&#39542;&#20013;&#30340;&#36816;&#21160;&#39044;&#27979;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#39044;&#27979;&#23545;&#20110;&#33258;&#20027;&#36234;&#37326;&#39550;&#39542;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#19982;&#22312;&#36947;&#36335;&#19978;&#39550;&#39542;&#30456;&#27604;&#65292;&#23427;&#38754;&#20020;&#30528;&#26356;&#22810;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#36710;&#36742;&#19982;&#22320;&#24418;&#20043;&#38388;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24314;&#27169;&#21160;&#24577;&#31995;&#32479;&#21644;&#22806;&#37096;&#24178;&#25200;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;&#30456;&#21453;&#65292;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#38590;&#20197;&#26126;&#30830;&#25429;&#25417;&#22522;&#26412;&#30340;&#29289;&#29702;&#23450;&#24459;&#65292;&#36825;&#24456;&#23481;&#26131;&#23548;&#33268;&#27867;&#21270;&#33021;&#21147;&#24046;&#12290;&#36890;&#36807;&#34701;&#21512;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#36825;&#20123;&#26041;&#27861;&#23558;&#29289;&#29702;&#23450;&#24459;&#23884;&#20837;&#31070;&#32463;&#27169;&#22411;&#20013;&#65292;&#21487;&#33021;&#26174;&#33879;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#37117;&#27809;&#26377;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#36234;&#37326;&#39550;&#39542;&#29615;&#22659;&#20013;&#36827;&#34892;&#35780;&#20272;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986; PhysORD&#65292;&#36825;&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#38598;&#25104;&#20102;&#23432;&#24658;&#23450;&#24459;&#65292;&#21363;&#27431;&#25289;-&#25289;&#26684;&#26391;&#26085;&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01596v1 Announce Type: cross  Abstract: Motion prediction is critical for autonomous off-road driving, however, it presents significantly more challenges than on-road driving because of the complex interaction between the vehicle and the terrain. Traditional physics-based approaches encounter difficulties in accurately modeling dynamic systems and external disturbance. In contrast, data-driven neural networks require extensive datasets and struggle with explicitly capturing the fundamental physical laws, which can easily lead to poor generalization. By merging the advantages of both methods, neuro-symbolic approaches present a promising direction. These methods embed physical laws into neural models, potentially significantly improving generalization capabilities. However, no prior works were evaluated in real-world settings for off-road driving. To bridge this gap, we present PhysORD, a neural-symbolic approach integrating the conservation law, i.e., the Euler-Lagrange equa
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#20943;&#23569;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#25152;&#38656;&#30340;&#35757;&#32451;&#38598;&#22823;&#23567;&#65292;&#26377;&#25928;&#21033;&#29992;&#30005;&#23376;&#21830;&#21153;&#25968;&#25454;&#30340;&#22270;&#32467;&#26500;&#65292;&#20026;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#24102;&#26469;&#26032;&#30340;&#21487;&#33021;&#24615;</title><link>https://arxiv.org/abs/2403.19289</link><description>&lt;p&gt;
&#29992;&#20110;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks for Treatment Effect Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19289
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#20943;&#23569;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#25152;&#38656;&#30340;&#35757;&#32451;&#38598;&#22823;&#23567;&#65292;&#26377;&#25928;&#21033;&#29992;&#30005;&#23376;&#21830;&#21153;&#25968;&#25454;&#30340;&#22270;&#32467;&#26500;&#65292;&#20026;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#24102;&#26469;&#26032;&#30340;&#21487;&#33021;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#24448;&#24448;&#28041;&#21450;&#26114;&#36149;&#30340;&#27835;&#30103;&#20998;&#37197;&#65292;&#36825;&#22312;&#22823;&#35268;&#27169;&#35774;&#32622;&#20013;&#21487;&#33021;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#39044;&#27979;&#36825;&#31181;&#27835;&#30103;&#25928;&#26524;&#32780;&#26080;&#38656;&#23454;&#38469;&#24178;&#39044;&#26159;&#20943;&#23569;&#39118;&#38505;&#30340;&#19968;&#31181;&#26631;&#20934;&#20570;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#26041;&#27861;&#24448;&#24448;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#23454;&#39564;&#26500;&#24314;&#30340;&#35757;&#32451;&#38598;&#65292;&#22240;&#27492;&#20174;&#26681;&#26412;&#19978;&#23384;&#22312;&#39118;&#38505;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#20943;&#23569;&#25152;&#38656;&#30340;&#35757;&#32451;&#38598;&#22823;&#23567;&#65292;&#20381;&#36182;&#20110;&#30005;&#23376;&#21830;&#21153;&#25968;&#25454;&#20013;&#24120;&#35265;&#30340;&#22270;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#35270;&#20026;&#20855;&#26377;&#26377;&#38480;&#25968;&#37327;&#26631;&#35760;&#23454;&#20363;&#30340;&#33410;&#28857;&#22238;&#24402;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#31867;&#20284;&#20110;&#20808;&#21069;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#22120;&#30340;&#21452;&#27169;&#22411;&#31070;&#32463;&#26550;&#26500;&#65292;&#24182;&#27979;&#35797;&#20102;&#19981;&#21516;&#30340;&#28040;&#24687;&#20256;&#36882;&#23618;&#36827;&#34892;&#32534;&#30721;&#12290;&#27492;&#22806;&#65292;&#20316;&#20026;&#39069;&#22806;&#27493;&#39588;&#65292;&#25105;&#20204;&#23558;&#27169;&#22411;&#19982;&#33719;&#21462;&#20989;&#25968;&#30456;&#32467;&#21512;&#65292;&#20197;&#24341;&#23548;&#20449;&#24687;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19289v1 Announce Type: cross  Abstract: Estimating causal effects in e-commerce tends to involve costly treatment assignments which can be impractical in large-scale settings. Leveraging machine learning to predict such treatment effects without actual intervention is a standard practice to diminish the risk. However, existing methods for treatment effect prediction tend to rely on training sets of substantial size, which are built from real experiments and are thus inherently risky to create. In this work we propose a graph neural network to diminish the required training set size, relying on graphs that are common in e-commerce data. Specifically, we view the problem as node regression with a restricted number of labeled instances, develop a two-model neural architecture akin to previous causal effect estimators, and test varying message-passing layers for encoding. Furthermore, as an extra step, we combine the model with an acquisition function to guide the creation of th
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#20195;&#30721;&#26234;&#33021;&#39046;&#22495;&#30340;&#35843;&#26597;&#31995;&#32479;&#22238;&#39038;&#20102;50&#22810;&#31181;&#20195;&#34920;&#24615;&#27169;&#22411;&#21644;&#36229;&#36807;680&#39033;&#30456;&#20851;&#20316;&#21697;&#65292;&#31361;&#20986;&#20102;&#19981;&#21516;&#30740;&#31350;&#38454;&#27573;&#30340;&#33539;&#24335;&#21644;&#25216;&#26415;&#36716;&#21464;&#12290;</title><link>https://arxiv.org/abs/2403.14734</link><description>&lt;p&gt;
&#19968;&#39033;&#31070;&#32463;&#20195;&#30721;&#26234;&#33021;&#30340;&#35843;&#26597;&#65306;&#33539;&#24335;&#12289;&#36827;&#23637;&#19982;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
A Survey of Neural Code Intelligence: Paradigms, Advances and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14734
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20195;&#30721;&#26234;&#33021;&#39046;&#22495;&#30340;&#35843;&#26597;&#31995;&#32479;&#22238;&#39038;&#20102;50&#22810;&#31181;&#20195;&#34920;&#24615;&#27169;&#22411;&#21644;&#36229;&#36807;680&#39033;&#30456;&#20851;&#20316;&#21697;&#65292;&#31361;&#20986;&#20102;&#19981;&#21516;&#30740;&#31350;&#38454;&#27573;&#30340;&#33539;&#24335;&#21644;&#25216;&#26415;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14734v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#31070;&#32463;&#20195;&#30721;&#26234;&#33021;--&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#29702;&#35299;&#12289;&#29983;&#25104;&#21644;&#20248;&#21270;&#20195;&#30721;--&#22312;&#25972;&#20010;&#31038;&#20250;&#19978;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20135;&#29983;&#28145;&#36828;&#24433;&#21709;&#12290;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#36825;&#19968;&#39046;&#22495;&#22312;&#36807;&#21435;&#20960;&#24180;&#24341;&#36215;&#20102;&#20004;&#20010;&#30740;&#31350;&#31038;&#21306;&#30740;&#31350;&#20154;&#21592;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#26412;&#35843;&#26597;&#31995;&#32479;&#22320;&#21644;&#25353;&#26102;&#38388;&#39034;&#24207;&#22238;&#39038;&#20102;&#20195;&#30721;&#26234;&#33021;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#21253;&#25324;50&#22810;&#31181;&#20195;&#34920;&#24615;&#27169;&#22411;&#21450;&#20854;&#21464;&#20307;&#12289;20&#22810;&#31181;&#20219;&#21153;&#31867;&#21035;&#20197;&#21450;&#36229;&#36807;680&#39033;&#30456;&#20851;&#20316;&#21697;&#12290;&#25105;&#20204;&#36981;&#24490;&#21382;&#21490;&#36827;&#23637;&#65292;&#36319;&#36394;&#19981;&#21516;&#30740;&#31350;&#38454;&#27573;&#30340;&#33539;&#24335;&#36716;&#21464;&#65288;&#20363;&#22914;&#65292;&#20174;&#20351;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23545;&#20195;&#30721;&#24314;&#27169;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#65289;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;&#19981;&#21516;&#38454;&#27573;&#28085;&#30422;&#30340;&#27169;&#22411;&#12289;&#20219;&#21153;&#21644;&#35780;&#20272;&#30340;&#20027;&#35201;&#25216;&#26415;&#36716;&#21464;&#12290;&#23545;&#20110;&#24212;&#29992;&#65292;&#25105;&#20204;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14734v1 Announce Type: cross  Abstract: Neural Code Intelligence -- leveraging deep learning to understand, generate, and optimize code -- holds immense potential for transformative impacts on the whole society. Bridging the gap between Natural Language and Programming Language, this domain has drawn significant attention from researchers in both research communities over the past few years. This survey presents a systematic and chronological review of the advancements in code intelligence, encompassing over 50 representative models and their variants, more than 20 categories of tasks, and an extensive coverage of over 680 related works. We follow the historical progression to trace the paradigm shifts across different research phases (e.g., from modeling code with recurrent neural networks to the era of Large Language Models). Concurrently, we highlight the major technical transitions in models, tasks, and evaluations spanning through different stages. For applications, we 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GiCiSAD&#30340;&#22522;&#20110;&#22270;&#25340;&#22270;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#39592;&#26550;&#30340;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.12172</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#25340;&#22270;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#22522;&#20110;&#39592;&#26550;&#30340;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Graph-Jigsaw Conditioned Diffusion Model for Skeleton-based Video Anomaly Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12172
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GiCiSAD&#30340;&#22522;&#20110;&#22270;&#25340;&#22270;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#39592;&#26550;&#30340;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#39592;&#26550;&#30340;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#65288;SVAD&#65289;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#12290;&#20934;&#30830;&#35782;&#21035;&#24322;&#24120;&#27169;&#24335;&#25110;&#20107;&#20214;&#20351;&#25805;&#20316;&#21592;&#33021;&#22815;&#21450;&#26102;&#26816;&#27979;&#21487;&#30097;&#27963;&#21160;&#65292;&#20174;&#32780;&#22686;&#24378;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#26410;&#33021;&#21516;&#26102;&#35299;&#20915;&#36825;&#20123;&#20851;&#38190;&#29305;&#24615;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#23454;&#29992;&#19988;&#36731;&#37327;&#32423;&#30340;&#26694;&#26550;&#65292;&#21363;&#22522;&#20110;&#22270;&#25340;&#22270;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#22522;&#20110;&#39592;&#26550;&#30340;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#65288;GiCiSAD&#65289;&#65292;&#20197;&#20811;&#26381;&#19982;SVAD&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12172v1 Announce Type: cross  Abstract: Skeleton-based video anomaly detection (SVAD) is a crucial task in computer vision. Accurately identifying abnormal patterns or events enables operators to promptly detect suspicious activities, thereby enhancing safety. Achieving this demands a comprehensive understanding of human motions, both at body and region levels, while also accounting for the wide variations of performing a single action. However, existing studies fail to simultaneously address these crucial properties. This paper introduces a novel, practical and lightweight framework, namely Graph-Jigsaw Conditioned Diffusion Model for Skeleton-based Video Anomaly Detection (GiCiSAD) to overcome the challenges associated with SVAD. GiCiSAD consists of three novel modules: the Graph Attention-based Forecasting module to capture the spatio-temporal dependencies inherent in the data, the Graph-level Jigsaw Puzzle Maker module to distinguish subtle region-level discrepancies bet
&lt;/p&gt;</description></item><item><title>Tur[k]ingBench&#26159;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340;&#32593;&#32476;&#20195;&#29702;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22788;&#29702;&#21253;&#21547;&#25991;&#26412;&#25351;&#31034;&#21644;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#30340;&#22797;&#26434;&#20219;&#21153;&#26102;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.11905</link><description>&lt;p&gt;
Tur[k]ingBench&#65306;&#29992;&#20110;&#32593;&#32476;&#20195;&#29702;&#30340;&#25361;&#25112;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Tur[k]ingBench: A Challenge Benchmark for Web Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11905
&lt;/p&gt;
&lt;p&gt;
Tur[k]ingBench&#26159;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340;&#32593;&#32476;&#20195;&#29702;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22788;&#29702;&#21253;&#21547;&#25991;&#26412;&#25351;&#31034;&#21644;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#30340;&#22797;&#26434;&#20219;&#21153;&#26102;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#23637;&#31034;&#20102;&#22312;&#21407;&#22987;&#25991;&#26412;&#24418;&#24335;&#19979;&#29702;&#35299;&#21644;&#20132;&#27969;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#19990;&#30028;&#19978;&#19981;&#20165;&#20165;&#26159;&#21407;&#22987;&#25991;&#26412;&#12290;&#20363;&#22914;&#65292;&#20154;&#20204;&#22312;&#32593;&#39029;&#19978;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#65292;&#22312;&#36825;&#20123;&#32593;&#39029;&#19978;&#65292;&#25991;&#26412;&#19982;&#20854;&#20182;&#24418;&#24335;&#20132;&#32455;&#22312;&#19968;&#36215;&#65292;&#24182;&#20197;&#21508;&#31181;&#22797;&#26434;&#20114;&#21160;&#30340;&#24418;&#24335;&#23436;&#25104;&#20219;&#21153;&#12290;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#25512;&#24191;&#21040;&#36825;&#31181;&#22797;&#26434;&#30340;&#39046;&#22495;&#21602;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;TurkingBench&#65292;&#19968;&#20010;&#30001;&#21253;&#21547;&#22810;&#27169;&#24577;&#32972;&#26223;&#30340;&#25991;&#26412;&#35828;&#26126;&#21046;&#23450;&#30340;&#20219;&#21153;&#22522;&#20934;&#12290;&#19982;&#29616;&#26377;&#30340;&#20351;&#29992;&#20154;&#24037;&#21512;&#25104;&#30340;&#32593;&#39029;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#36825;&#37324;&#25105;&#20204;&#20351;&#29992;&#26368;&#21021;&#35774;&#35745;&#29992;&#20110;&#21508;&#31181;&#27880;&#37322;&#30446;&#30340;&#30340;&#33258;&#28982;HTML&#39029;&#38754;&#12290;&#27599;&#20010;&#20219;&#21153;&#30340;HTML&#35828;&#26126;&#20063;&#34987;&#23454;&#20363;&#21270;&#20026;&#21508;&#31181;&#20540;&#65288;&#20174;&#20247;&#21253;&#20219;&#21153;&#33719;&#24471;&#65289;&#20197;&#24418;&#25104;&#20219;&#21153;&#30340;&#26032;&#23454;&#20363;&#12290;&#36825;&#20010;&#22522;&#20934;&#21253;&#21547;32.2K&#20010;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11905v1 Announce Type: new  Abstract: Recent chatbots have demonstrated impressive ability to understand and communicate in raw-text form. However, there is more to the world than raw text. For example, humans spend long hours of their time on web pages, where text is intertwined with other modalities and tasks are accomplished in the form of various complex interactions. Can state-of-the-art multi-modal models generalize to such complex domains?   To address this question, we introduce TurkingBench, a benchmark of tasks formulated as web pages containing textual instructions with multi-modal context. Unlike existing work which employs artificially synthesized web pages, here we use natural HTML pages that were originally designed for crowdsourcing workers for various annotation purposes. The HTML instructions of each task are also instantiated with various values (obtained from the crowdsourcing tasks) to form new instances of the task. This benchmark contains 32.2K instanc
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21338;&#24328;&#35770;&#35270;&#35282;&#35780;&#20272;LLMs&#30340;&#20915;&#31574;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;GPT-3.5&#22312;&#31283;&#20581;&#24615;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#65292;&#32780;GPT-4&#21017;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.11807</link><description>&lt;p&gt;
LLM&#30340;&#20915;&#31574;&#27700;&#24179;&#22312;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#30340;&#35780;&#20272;&#31350;&#31455;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming Ability in Multi-Agent Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11807
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21338;&#24328;&#35770;&#35270;&#35282;&#35780;&#20272;LLMs&#30340;&#20915;&#31574;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;GPT-3.5&#22312;&#31283;&#20581;&#24615;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#65292;&#32780;GPT-4&#21017;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#21508;&#31181;&#33021;&#21147;&#65292;&#20026;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#26497;&#22909;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#21338;&#24328;&#35770;&#30340;&#35270;&#35282;&#25506;&#31350;LLMs&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#25903;&#25345;&#22810;&#20010;&#26234;&#33021;&#20307;&#21516;&#26102;&#21442;&#19982;&#30340;&#28216;&#25103;&#65292;&#24341;&#20837;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;GAMA-Bench&#65292;&#21253;&#25324;&#20843;&#20010;&#32463;&#20856;&#30340;&#22810;&#26234;&#33021;&#20307;&#28216;&#25103;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20998;&#26041;&#26696;&#65292;&#23450;&#37327;&#35780;&#20272;&#27169;&#22411;&#22312;&#36825;&#20123;&#28216;&#25103;&#20013;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;GAMA-Bench&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#30340;&#31283;&#20581;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#22686;&#24378;&#31574;&#30053;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#34429;&#28982;GPT-3.5&#34920;&#29616;&#20986;&#20196;&#20154;&#28385;&#24847;&#30340;&#31283;&#20581;&#24615;&#65292;&#20294;&#20854;&#27867;&#21270;&#33021;&#21147;&#30456;&#23545;&#26377;&#38480;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#19968;&#20123;&#26041;&#27861;&#22914;&#8220;&#24605;&#32500;&#38142;&#8221;&#65292;&#20854;&#24615;&#33021;&#21487;&#20197;&#24471;&#21040;&#25552;&#39640;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;LLMs&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT-4&#32988;&#36807;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11807v1 Announce Type: new  Abstract: Decision-making, a complicated task requiring various types of abilities, presents an excellent framework for assessing Large Language Models (LLMs). Our research investigates LLMs' decision-making capabilities through the lens of a well-established field, Game Theory. We focus specifically on games that support the participation of more than two agents simultaneously. Subsequently, we introduce our framework, GAMA-Bench, including eight classical multi-agent games. We design a scoring scheme to assess a model's performance in these games quantitatively. Through GAMA-Bench, we investigate LLMs' robustness, generalizability, and enhancement strategies. Results reveal that while GPT-3.5 shows satisfying robustness, its generalizability is relatively limited. However, its performance can be improved through approaches such as Chain-of-Thought. Additionally, we conduct evaluations across various LLMs and find that GPT-4 outperforms other mod
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MUSE&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#35775;&#38382;&#26368;&#26032;&#20449;&#24687;&#24182;&#35780;&#20272;&#21487;&#20449;&#24230;&#65292;&#20197;&#35299;&#20915;&#31038;&#20132;&#23186;&#20307;&#19978;&#35823;&#20449;&#24687;&#32416;&#27491;&#30340;&#38590;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.11169</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32416;&#27491;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#38169;&#35823;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Correcting misinformation on social media with a large language model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11169
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MUSE&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#35775;&#38382;&#26368;&#26032;&#20449;&#24687;&#24182;&#35780;&#20272;&#21487;&#20449;&#24230;&#65292;&#20197;&#35299;&#20915;&#31038;&#20132;&#23186;&#20307;&#19978;&#35823;&#20449;&#24687;&#32416;&#27491;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35823;&#20449;&#24687;&#20250;&#30772;&#22351;&#20844;&#20247;&#23545;&#31185;&#23398;&#21644;&#27665;&#20027;&#30340;&#20449;&#20219;&#65292;&#29305;&#21035;&#26159;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#65292;&#19981;&#20934;&#30830;&#20449;&#24687;&#20250;&#36805;&#36895;&#20256;&#25773;&#12290;&#19987;&#23478;&#21644;&#26222;&#36890;&#20154;&#36890;&#36807;&#25163;&#21160;&#35782;&#21035;&#21644;&#35299;&#37322;&#19981;&#20934;&#30830;&#20449;&#24687;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#32416;&#27491;&#35823;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24456;&#38590;&#25193;&#23637;&#65292;&#36825;&#26159;&#19968;&#20010;&#25285;&#24551;&#65292;&#22240;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#31561;&#25216;&#26415;&#20351;&#35823;&#20449;&#24687;&#26356;&#23481;&#26131;&#29983;&#25104;&#12290;LLMs&#36824;&#20855;&#26377;&#22810;&#21151;&#33021;&#33021;&#21147;&#65292;&#21487;&#20197;&#21152;&#36895;&#32416;&#27491;&#35823;&#20449;&#24687;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#30001;&#20110;&#32570;&#20047;&#26368;&#26032;&#20449;&#24687;&#12289;&#20542;&#21521;&#20110;&#29983;&#25104;&#20284;&#26159;&#32780;&#38750;&#30340;&#20869;&#23481;&#21644;&#24341;&#29992;&#20197;&#21450;&#26080;&#27861;&#22788;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;&#32780;&#38754;&#20020;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MUSE&#65292;&#36825;&#26159;&#19968;&#20010;&#24102;&#26377;&#26368;&#26032;&#20449;&#24687;&#35775;&#38382;&#21644;&#21487;&#20449;&#24230;&#35780;&#20272;&#30340;LLM&#12290;&#36890;&#36807;&#26816;&#32034;&#19978;&#19979;&#25991;&#35777;&#25454;&#21644;&#21453;&#39539;&#65292;MUSE&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#21487;&#20449;&#30340;&#35299;&#37322;&#21644;&#21442;&#32771;&#12290;&#23427;&#36824;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11169v1 Announce Type: cross  Abstract: Misinformation undermines public trust in science and democracy, particularly on social media where inaccuracies can spread rapidly. Experts and laypeople have shown to be effective in correcting misinformation by manually identifying and explaining inaccuracies. Nevertheless, this approach is difficult to scale, a concern as technologies like large language models (LLMs) make misinformation easier to produce. LLMs also have versatile capabilities that could accelerate misinformation correction; however, they struggle due to a lack of recent information, a tendency to produce plausible but false content and references, and limitations in addressing multimodal information. To address these issues, we propose MUSE, an LLM augmented with access to and credibility evaluation of up-to-date information. By retrieving contextual evidence and refutations, MUSE can provide accurate and trustworthy explanations and references. It also describes 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20005;&#26684;&#20998;&#21306;&#35843;&#24230;&#31574;&#30053;&#65292;&#29992;&#20110;&#38646;&#26143;&#21018;&#24615;&#27969;&#24335;&#20219;&#21153;&#65292;&#36890;&#36807;&#21019;&#24314;&#19981;&#30456;&#20132;&#30340;&#20219;&#21153;&#21644;&#22788;&#29702;&#22120;&#20998;&#21306;&#65292;&#24182;&#23581;&#35797;&#23558;&#30456;&#20284;&#23481;&#37327;&#30340;&#20219;&#21153;&#20998;&#37197;&#32473;&#21516;&#19968;&#20998;&#21306;&#65292;&#20197;&#20943;&#23569;&#24178;&#25200;&#12290;</title><link>https://arxiv.org/abs/2403.10726</link><description>&lt;p&gt;
&#38024;&#23545;&#38646;&#26143;&#21018;&#24615;&#27969;&#24335;&#20219;&#21153;&#30340;&#20005;&#26684;&#20998;&#21306;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Strict Partitioning for Sporadic Rigid Gang Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10726
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20005;&#26684;&#20998;&#21306;&#35843;&#24230;&#31574;&#30053;&#65292;&#29992;&#20110;&#38646;&#26143;&#21018;&#24615;&#27969;&#24335;&#20219;&#21153;&#65292;&#36890;&#36807;&#21019;&#24314;&#19981;&#30456;&#20132;&#30340;&#20219;&#21153;&#21644;&#22788;&#29702;&#22120;&#20998;&#21306;&#65292;&#24182;&#23581;&#35797;&#23558;&#30456;&#20284;&#23481;&#37327;&#30340;&#20219;&#21153;&#20998;&#37197;&#32473;&#21516;&#19968;&#20998;&#21306;&#65292;&#20197;&#20943;&#23569;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21018;&#24615;&#27969;&#24335;&#20219;&#21153;&#27169;&#22411;&#22522;&#20110;&#22312;&#22266;&#23450;&#25968;&#37327;&#30340;&#22788;&#29702;&#22120;&#19978;&#21516;&#26102;&#25191;&#34892;&#22810;&#20010;&#32447;&#31243;&#20197;&#25552;&#39640;&#25928;&#29575;&#21644;&#24615;&#33021;&#30340;&#24605;&#24819;&#12290;&#34429;&#28982;&#20840;&#23616;&#21018;&#24615;&#27969;&#24335;&#35843;&#24230;&#26377;&#22823;&#37327;&#25991;&#29486;&#65292;&#20294;&#20998;&#21306;&#26041;&#27861;&#20855;&#26377;&#20960;&#20010;&#23454;&#38469;&#20248;&#21183;&#65288;&#20363;&#22914;&#20219;&#21153;&#38548;&#31163;&#21644;&#20943;&#23569;&#35843;&#24230;&#24320;&#38144;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#21018;&#24615;&#27969;&#24335;&#20219;&#21153;&#30340;&#20998;&#21306;&#35843;&#24230;&#31574;&#30053;&#65292;&#31216;&#20026;&#20005;&#26684;&#20998;&#21306;&#12290;&#35813;&#26041;&#27861;&#21019;&#24314;&#20219;&#21153;&#21644;&#22788;&#29702;&#22120;&#30340;&#19981;&#30456;&#20132;&#20998;&#21306;&#65292;&#20197;&#36991;&#20813;&#20998;&#21306;&#38388;&#24178;&#25200;&#12290;&#27492;&#22806;&#65292;&#23427;&#23581;&#35797;&#23558;&#20855;&#26377;&#30456;&#20284;&#23481;&#37327;&#65288;&#21363;&#24182;&#34892;&#24615;&#65289;&#30340;&#20219;&#21153;&#20998;&#37197;&#32473;&#21516;&#19968;&#20998;&#21306;&#65292;&#20197;&#20943;&#23569;&#20998;&#21306;&#20869;&#24178;&#25200;&#12290;&#22312;&#27599;&#20010;&#20998;&#21306;&#20869;&#65292;&#20219;&#21153;&#21487;&#20197;&#20351;&#29992;&#20219;&#20309;&#31867;&#22411;&#30340;&#35843;&#24230;&#22120;&#36827;&#34892;&#35843;&#24230;&#65292;&#36825;&#20801;&#35768;&#20351;&#29992;&#19981;&#37027;&#20040;&#24754;&#35266;&#30340;&#21487;&#35843;&#24230;&#27979;&#35797;&#12290;&#22823;&#37327;&#30340;&#21512;&#25104;&#23454;&#39564;&#35777;&#26126;&#21644;&#22522;&#20110;Edge TPU&#22522;&#20934;&#30340;&#26696;&#20363;&#30740;&#31350;&#26174;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10726v1 Announce Type: cross  Abstract: The rigid gang task model is based on the idea of executing multiple threads simultaneously on a fixed number of processors to increase efficiency and performance. Although there is extensive literature on global rigid gang scheduling, partitioned approaches have several practical advantages (e.g., task isolation and reduced scheduling overheads). In this paper, we propose a new partitioned scheduling strategy for rigid gang tasks, named strict partitioning. The method creates disjoint partitions of tasks and processors to avoid inter-partition interference. Moreover, it tries to assign tasks with similar volumes (i.e., parallelisms) to the same partition so that the intra-partition interference can be reduced. Within each partition, the tasks can be scheduled using any type of scheduler, which allows the use of a less pessimistic schedulability test. Extensive synthetic experiments and a case study based on Edge TPU benchmarks show th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#22312;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#20013;&#35782;&#21035;&#21644;&#37327;&#21270;&#24615;&#21035;&#20559;&#35265;&#65292;&#25552;&#20986;&#20102;&#19977;&#20010;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#38750;&#27495;&#35270;&#26631;&#20934;&#24182;&#35774;&#35745;&#20102;&#30456;&#24212;&#30340;&#25552;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.08564</link><description>&lt;p&gt;
&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#38750;&#27495;&#35270;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
Non-discrimination Criteria for Generative Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#22312;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#20013;&#35782;&#21035;&#21644;&#37327;&#21270;&#24615;&#21035;&#20559;&#35265;&#65292;&#25552;&#20986;&#20102;&#19977;&#20010;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#38750;&#27495;&#35270;&#26631;&#20934;&#24182;&#35774;&#35745;&#20102;&#30456;&#24212;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#32463;&#21382;&#20102;&#24555;&#36895;&#21457;&#23637;&#12290;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#36234;&#26469;&#36234;&#26222;&#36941;&#22320;&#25552;&#20379;&#32473;&#20844;&#20247;&#20351;&#29992;&#65292;&#20154;&#20204;&#24320;&#22987;&#25285;&#24515;&#22312;&#24212;&#29992;&#20013;&#24310;&#32493;&#21644;&#25918;&#22823;&#26377;&#23475;&#20559;&#35265;&#30340;&#38382;&#39064;&#12290;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#21487;&#33021;&#23545;&#20854;&#38024;&#23545;&#30340;&#20010;&#20154;&#36896;&#25104;&#20260;&#23475;&#21644;&#38480;&#21046;&#65292;&#26080;&#35770;&#26159;&#30001;&#35823;&#20256;&#36824;&#26159;&#27495;&#35270;&#25152;&#26500;&#25104;&#12290;&#35782;&#21035;&#24615;&#21035;&#20559;&#35265;&#20316;&#20026;&#19968;&#31181;&#26222;&#36941;&#30340;&#31038;&#20250;&#26500;&#36896;&#65292;&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#21457;&#29616;&#21644;&#37327;&#21270;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#23384;&#22312;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19977;&#20010;&#26469;&#33258;&#20998;&#31867;&#30340;&#33879;&#21517;&#38750;&#27495;&#35270;&#26631;&#20934;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#31867;&#27604;&#65292;&#21363;&#29420;&#31435;&#24615;&#12289;&#20998;&#31163;&#24615;&#21644;&#20805;&#20998;&#24615;&#12290;&#20026;&#20102;&#23637;&#31034;&#36825;&#20123;&#26631;&#20934;&#30340;&#20316;&#29992;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#38024;&#23545;&#27599;&#20010;&#26631;&#20934;&#30340;&#25552;&#31034;&#65292;&#37325;&#28857;&#20851;&#27880;&#32844;&#19994;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#65292;&#20855;&#20307;&#21033;&#29992;&#21307;&#23398;&#27979;&#35797;&#26469;&#22312;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#32972;&#26223;&#20013;&#24341;&#20837;&#22522;&#26412;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08564v1 Announce Type: cross  Abstract: Within recent years, generative AI, such as large language models, has undergone rapid development. As these models become increasingly available to the public, concerns arise about perpetuating and amplifying harmful biases in applications. Gender stereotypes can be harmful and limiting for the individuals they target, whether they consist of misrepresentation or discrimination. Recognizing gender bias as a pervasive societal construct, this paper studies how to uncover and quantify the presence of gender biases in generative language models. In particular, we derive generative AI analogues of three well-known non-discrimination criteria from classification, namely independence, separation and sufficiency. To demonstrate these criteria in action, we design prompts for each of the criteria with a focus on occupational gender stereotype, specifically utilizing the medical test to introduce the ground truth in the generative AI context. 
&lt;/p&gt;</description></item><item><title>FastV&#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#21363;&#25554;&#21363;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#27169;&#24335;&#24182;&#22312;&#21518;&#32493;&#23618;&#20013;&#20462;&#21098;&#35270;&#35273;&#20195;&#24065;&#65292;&#26497;&#22823;&#22320;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#22270;&#20687;&#21644;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#20013;&#19981;&#25439;&#22833;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.06764</link><description>&lt;p&gt;
&#19968;&#24352;&#22270;&#29255;&#22312;&#31532;&#20108;&#23618;&#20043;&#21518;&#20215;&#20540;1/2&#20195;&#24065;&#65306;&#38024;&#23545;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#21363;&#25554;&#21363;&#29992;&#25512;&#29702;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06764
&lt;/p&gt;
&lt;p&gt;
FastV&#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#21363;&#25554;&#21363;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#27169;&#24335;&#24182;&#22312;&#21518;&#32493;&#23618;&#20013;&#20462;&#21098;&#35270;&#35273;&#20195;&#24065;&#65292;&#26497;&#22823;&#22320;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#22270;&#20687;&#21644;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#20013;&#19981;&#25439;&#22833;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#20013;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#23384;&#22312;&#20302;&#25928;&#29616;&#35937;&#65292;&#23588;&#20854;&#26159;&#22312;&#30693;&#21517;&#27169;&#22411;&#22914;LLaVA-1.5&#12289;QwenVL-Chat&#21644;Video-LLaVA&#20013;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#27969;&#34892;&#30340;LVLMs&#30340;&#28145;&#23618;&#20013;&#65292;&#23545;&#35270;&#35273;&#20195;&#24065;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#26497;&#20854;&#20302;&#25928;&#65292;&#26263;&#31034;&#30456;&#36739;&#20110;&#22788;&#29702;&#25991;&#26412;&#25968;&#25454;&#65292;&#38656;&#35201;&#26356;&#31232;&#30095;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FastV&#65292;&#36825;&#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#21363;&#25554;&#21363;&#29992;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#26089;&#26399;&#23618;&#20013;&#30340;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#27169;&#24335;&#21644;&#22312;&#38543;&#21518;&#23618;&#20013;&#20462;&#21098;&#35270;&#35273;&#20195;&#24065;&#26469;&#20248;&#21270;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;FastV&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65288;&#20363;&#22914;&#65292;&#23545;&#20110;LLaVA-1.5-13B&#30340;FLOP&#20943;&#23569;&#20102;45%&#65289;&#65292;&#32780;&#19981;&#20250;&#22312;&#24191;&#27867;&#30340;&#22270;&#20687;&#21644;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#20013;&#29306;&#29298;&#24615;&#33021;&#12290;FastV&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#24615;&#33021;&#26435;&#34913;&#26159;&#39640;&#24230;&#21487;&#23450;&#21046;&#30340;&#65292;&#24182;&#19988;&#26159;&#24085;&#32047;&#25176;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06764v1 Announce Type: cross  Abstract: In this study, we identify the inefficient attention phenomena in Large Vision-Language Models (LVLMs), notably within prominent models like LLaVA-1.5, QwenVL-Chat and Video-LLaVA. We find out that the attention computation over visual tokens is of extreme inefficiency in the deep layers of popular LVLMs, suggesting a need for a sparser approach compared to textual data handling. To this end, we introduce FastV, a versatile plug-and-play method designed to optimize computational efficiency by learning adaptive attention patterns in early layers and pruning visual tokens in subsequent ones. Our evaluations demonstrate FastV's ability to dramatically reduce computational costs (e.g., a 45 reduction in FLOPs for LLaVA-1.5-13B) without sacrificing performance in a wide range of image and video understanding tasks. The computational efficiency and performance trade-off of FastV are highly customizable and pareto-efficient. It can compress t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#30340;&#21487;&#20449;&#24230;&#65292;&#25581;&#31034;&#20102;&#26089;&#26399;&#39044;&#35757;&#32451;LLMs&#24050;&#32463;&#33021;&#22815;&#21306;&#20998;&#21508;&#20010;&#21487;&#20449;&#24230;&#32500;&#24230;&#20013;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#20174;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#20013;&#25552;&#21462;&#36716;&#21521;&#21521;&#37327;&#20197;&#22686;&#24378;LLM&#21487;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.19465</link><description>&lt;p&gt;
&#36861;&#36394;&#21487;&#20449;&#24230;&#21160;&#24577;&#65306;&#37325;&#35775;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#26399;
&lt;/p&gt;
&lt;p&gt;
Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#30340;&#21487;&#20449;&#24230;&#65292;&#25581;&#31034;&#20102;&#26089;&#26399;&#39044;&#35757;&#32451;LLMs&#24050;&#32463;&#33021;&#22815;&#21306;&#20998;&#21508;&#20010;&#21487;&#20449;&#24230;&#32500;&#24230;&#20013;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#20174;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#20013;&#25552;&#21462;&#36716;&#21521;&#21521;&#37327;&#20197;&#22686;&#24378;LLM&#21487;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21487;&#20449;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#38598;&#20013;&#22312;&#20805;&#20998;&#39044;&#35757;&#32451;&#30340;LLMs&#19978;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#25552;&#39640;LLMs&#30340;&#21487;&#20449;&#24230;&#12290;&#26412;&#25991;&#26088;&#22312;&#25581;&#31034;&#39044;&#35757;&#32451;&#30340;&#28508;&#21147;&#65292;&#39318;&#27425;&#25506;&#32034;&#20102;LLMs&#22312;&#27492;&#26399;&#38388;&#30340;&#21487;&#20449;&#24230;&#65292;&#19987;&#27880;&#20110;&#20116;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#21487;&#38752;&#24615;&#12289;&#38544;&#31169;&#12289;&#26377;&#23475;&#24230;&#12289;&#20844;&#24179;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;LLMs&#24212;&#29992;&#32447;&#24615;&#25506;&#27979;&#12290;&#39640;&#25506;&#27979;&#20934;&#30830;&#24230;&#34920;&#26126;&#65292;\textit{&#26089;&#26399;&#39044;&#35757;&#32451;&#30340;LLMs&#24050;&#32463;&#33021;&#22815;&#21306;&#20998;&#27599;&#20010;&#21487;&#20449;&#24230;&#32500;&#24230;&#20013;&#30340;&#27010;&#24565;}&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#36827;&#19968;&#27493;&#25581;&#31034;&#39044;&#35757;&#32451;&#30340;&#28508;&#22312;&#21487;&#33021;&#24615;&#65292;&#25105;&#20204;&#20174;LLM&#30340;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#20013;&#25552;&#21462;&#36716;&#21521;&#21521;&#37327;&#65292;&#20197;&#22686;&#24378;LLM&#30340;&#21487;&#20449;&#24230;&#12290;&#26368;&#21518;&#65292;&#21463;&#21040;~\citet{choi2023understanding} &#30340;&#21551;&#21457;&#65292;&#30456;&#20114;&#20449;&#24687;&#20272;&#35745;&#21463;&#32447;&#24615;&#25506;&#27979;&#20934;&#30830;&#24230;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#36824;&#29992;&#30456;&#20114;&#20449;&#24687;&#25506;&#27979;LLMs&#26469;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19465v1 Announce Type: cross  Abstract: Ensuring the trustworthiness of large language models (LLMs) is crucial. Most studies concentrate on fully pre-trained LLMs to better understand and improve LLMs' trustworthiness. In this paper, to reveal the untapped potential of pre-training, we pioneer the exploration of LLMs' trustworthiness during this period, focusing on five key dimensions: reliability, privacy, toxicity, fairness, and robustness. To begin with, we apply linear probing to LLMs. The high probing accuracy suggests that \textit{LLMs in early pre-training can already distinguish concepts in each trustworthiness dimension}. Therefore, to further uncover the hidden possibilities of pre-training, we extract steering vectors from a LLM's pre-training checkpoints to enhance the LLM's trustworthiness. Finally, inspired by~\citet{choi2023understanding} that mutual information estimation is bounded by linear probing accuracy, we also probe LLMs with mutual information to in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20851;&#27880;&#33258;&#23398;&#20064;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26680;&#24515;&#38382;&#39064;&#65306;&#22914;&#20309;&#23398;&#20064;&#26410;&#30693;&#30693;&#35782;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#25105;&#35780;&#20272;&#21644;&#35782;&#21035;&#26410;&#30693;&#28857;&#26469;&#29420;&#31435;&#23398;&#20064;&#20197;&#21069;&#26410;&#30693;&#30340;&#30693;&#35782;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#23545;&#20110;&#20943;&#23569;&#24187;&#35273;&#35780;&#20998;&#12289;&#23454;&#29616;&#39640;&#25928;LLM&#26356;&#26032;&#20197;&#21450;&#30693;&#35782;&#20132;&#27969;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.09147</link><description>&lt;p&gt;
&#26410;&#30693;&#20043;&#20013;&#65306;&#33258;&#23398;&#20064;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Into the Unknown: Self-Learning Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#33258;&#23398;&#20064;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26680;&#24515;&#38382;&#39064;&#65306;&#22914;&#20309;&#23398;&#20064;&#26410;&#30693;&#30693;&#35782;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#25105;&#35780;&#20272;&#21644;&#35782;&#21035;&#26410;&#30693;&#28857;&#26469;&#29420;&#31435;&#23398;&#20064;&#20197;&#21069;&#26410;&#30693;&#30340;&#30693;&#35782;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#23545;&#20110;&#20943;&#23569;&#24187;&#35273;&#35780;&#20998;&#12289;&#23454;&#29616;&#39640;&#25928;LLM&#26356;&#26032;&#20197;&#21450;&#30693;&#35782;&#20132;&#27969;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#33258;&#23398;&#20064;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20027;&#35201;&#38382;&#39064;&#65306;&#21363;&#22914;&#20309;&#23398;&#20064;&#33258;&#24049;&#19981;&#30693;&#36947;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23398;&#20064;LLM&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#33258;&#24049;&#30340;&#24187;&#35273;&#36827;&#34892;&#33258;&#25105;&#35780;&#20272;&#65292;&#20351;LLM&#33021;&#22815;&#29420;&#31435;&#22320;&#23398;&#20064;&#20197;&#21069;&#26410;&#30693;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#20351;&#29992;&#24187;&#35273;&#35780;&#20998;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;&#8220;&#26410;&#30693;&#28857;&#8221;&#30340;&#26032;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22806;&#37096;&#21644;&#19977;&#31181;&#20869;&#37096;&#26041;&#27861;&#26469;&#33258;&#21160;&#35782;&#21035;&#26410;&#30693;&#28857;&#12290;&#36825;&#26377;&#21161;&#20110;&#21019;&#24314;&#19968;&#20010;&#33258;&#23398;&#20064;&#24490;&#29615;&#65292;&#19987;&#27880;&#20110;&#26410;&#30693;&#28857;&#20013;&#30340;&#30693;&#35782;&#24046;&#36317;&#65292;&#20174;&#32780;&#20943;&#23569;&#24187;&#35273;&#35780;&#20998;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#29992;&#20110;&#35780;&#20272;LLM&#33258;&#23398;&#20064;&#33021;&#21147;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#24494;&#35843;&#25110;&#23545;&#40784;&#30340;7B-Mistral&#27169;&#22411;&#22312;&#33258;&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#30340;&#33258;&#23398;&#20064;&#27010;&#24565;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;LLM&#26356;&#26032;&#65292;&#24182;&#20026;&#30693;&#35782;&#20132;&#27969;&#24320;&#36767;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;&#23427;&#36824;&#21487;&#33021;&#22686;&#21152;&#20844;&#20247;&#30340;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09147v1 Announce Type: new Abstract: We address the main problem of self-learning LLM: the question of what to learn. We propose a self-learning LLM framework that enables an LLM to independently learn previously unknown knowledge through self-assessment of their own hallucinations. Using the hallucination score, we introduce a new concept of Points in The Unknown (PiUs), along with one extrinsic and three intrinsic methods for automatic PiUs identification. It facilitates the creation of a self-learning loop that focuses exclusively on the knowledge gap in Points in The Unknown, resulting in a reduced hallucination score. We also developed evaluation metrics for gauging an LLM's self-learning capability. Our experiments revealed that 7B-Mistral models that have been finetuned or aligned are capable of self-learning considerably well. Our self-learning concept allows more efficient LLM updates and opens new perspectives for knowledge exchange. It may also increase public tru
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#36895;&#25216;&#26415;FRDiff&#65292;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#26102;&#38388;&#20887;&#20313;&#24615;&#65292;&#37325;&#26032;&#20351;&#29992;&#20855;&#26377;&#39640;&#26102;&#38388;&#30456;&#20284;&#24615;&#30340;&#29305;&#24449;&#22270;&#65292;&#33410;&#30465;&#35745;&#31639;&#36164;&#28304;&#32780;&#19981;&#24433;&#21709;&#36755;&#20986;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2312.03517</link><description>&lt;p&gt;
FRDiff&#65306;&#29305;&#24449;&#37325;&#29992;&#29992;&#20110;&#26080;&#35757;&#32451;&#21152;&#36895;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FRDiff : Feature Reuse for Universal Training-free Acceleration of Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03517
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#36895;&#25216;&#26415;FRDiff&#65292;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#26102;&#38388;&#20887;&#20313;&#24615;&#65292;&#37325;&#26032;&#20351;&#29992;&#20855;&#26377;&#39640;&#26102;&#38388;&#30456;&#20284;&#24615;&#30340;&#29305;&#24449;&#22270;&#65292;&#33410;&#30465;&#35745;&#31639;&#36164;&#28304;&#32780;&#19981;&#24433;&#21709;&#36755;&#20986;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30340;&#36739;&#22823;&#35745;&#31639;&#25104;&#26412;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#39640;&#36136;&#37327;&#22270;&#20687;&#29983;&#25104;&#25152;&#24517;&#38656;&#30340;&#37325;&#22797;&#21435;&#22122;&#27493;&#39588;&#32780;&#20135;&#29983;&#30340;&#65292;&#36825;&#26159;&#38459;&#30861;&#23427;&#20204;&#24471;&#21040;&#24191;&#27867;&#37319;&#29992;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#39640;&#32423;&#21152;&#36895;&#25216;&#26415;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#22266;&#26377;&#30340;&#26102;&#38388;&#20887;&#20313;&#24615;&#26469;&#37325;&#26032;&#20351;&#29992;&#20855;&#26377;&#39640;&#26102;&#38388;&#30456;&#20284;&#24615;&#30340;&#29305;&#24449;&#22270;&#65292;&#20174;&#32780;&#33410;&#30465;&#35745;&#31639;&#36164;&#28304;&#32780;&#19981;&#24433;&#21709;&#36755;&#20986;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03517v2 Announce Type: replace-cross  Abstract: The substantial computational costs of diffusion models, especially due to the repeated denoising steps necessary for high-quality image generation, present a major obstacle to their widespread adoption. While several studies have attempted to address this issue by reducing the number of score function evaluations (NFE) using advanced ODE solvers without fine-tuning, the decreased number of denoising iterations misses the opportunity to update fine details, resulting in noticeable quality degradation. In our work, we introduce an advanced acceleration technique that leverages the temporal redundancy inherent in diffusion models. Reusing feature maps with high temporal similarity opens up a new opportunity to save computation resources without compromising output quality. To realize the practical benefits of this intuition, we conduct an extensive analysis and propose a novel method, FRDiff. FRDiff is designed to harness the adv
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#27010;&#29575;&#36923;&#36753;&#32534;&#31243;&#30340;&#35299;&#37322;&#35299;&#37322;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#19981;&#36879;&#26126;&#31995;&#32479;&#20013;&#29983;&#25104;&#21512;&#36866;&#35299;&#37322;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2401.17045</link><description>&lt;p&gt;
&#22312;&#27010;&#29575;&#36923;&#36753;&#32534;&#31243;&#20013;&#35299;&#37322;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Explaining Explanations in Probabilistic Logic Programming. (arXiv:2401.17045v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17045
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#27010;&#29575;&#36923;&#36753;&#32534;&#31243;&#30340;&#35299;&#37322;&#35299;&#37322;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#19981;&#36879;&#26126;&#31995;&#32479;&#20013;&#29983;&#25104;&#21512;&#36866;&#35299;&#37322;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24037;&#20855;&#30340;&#20986;&#29616;&#20063;&#23548;&#33268;&#20102;&#20135;&#29983;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#35299;&#37322;&#30340;&#38656;&#27714;&#12290;&#22312;&#19968;&#20123;&#26041;&#27861;&#20013;&#65292;&#31995;&#32479;&#26159;&#19981;&#36879;&#26126;&#30340;&#65288;&#36890;&#24120;&#34987;&#31216;&#20026;&#8220;&#40657;&#30418;&#23376;&#8221;&#65289;&#65292;&#36825;&#20351;&#24471;&#29983;&#25104;&#36866;&#24403;&#30340;&#35299;&#37322;&#21464;&#24471;&#22256;&#38590;&#12290;&#28982;&#32780;&#65292;&#22312;&#27010;&#29575;&#36923;&#36753;&#32534;&#31243;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#36923;&#36753;&#32534;&#31243;&#65288;&#29992;&#20110;&#30693;&#35782;&#34920;&#31034;&#65289;&#21644;&#27010;&#29575;&#65288;&#29992;&#20110;&#24314;&#27169;&#19981;&#30830;&#23450;&#24615;&#65289;&#30340;&#32467;&#21512;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#21487;&#20197;&#35828;&#27169;&#22411;&#26159;&#21487;&#20197;&#35299;&#37322;&#30340;&#65292;&#36825;&#26041;&#20415;&#20102;&#23545;&#27169;&#22411;&#30340;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#29305;&#23450;&#30340;&#26597;&#35810;&#65292;&#36890;&#24120;&#30340;&#8220;&#35299;&#37322;&#8221;&#30340;&#27010;&#24565;&#26159;&#19982;&#27169;&#22411;&#30340;&#27599;&#20010;&#38543;&#26426;&#21464;&#37327;&#30340;&#36873;&#25321;&#38598;&#30456;&#20851;&#32852;&#30340;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20010;&#38598;&#21512;&#27809;&#26377;&#22240;&#26524;&#32467;&#26500;&#65292;&#23454;&#38469;&#19978;&#65292;&#19968;&#20123;&#36873;&#25321;&#23454;&#38469;&#19978;&#19982;&#25152;&#32771;&#34385;&#30340;&#26597;&#35810;&#26080;&#20851;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;&#39537;&#21160;&#25512;&#29702;&#23450;&#20041;&#30340;&#35299;&#37322;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of tools based on artificial intelligence has also led to the need of producing explanations which are understandable by a human being. In some approaches, the system is not transparent (often referred to as a "black box"), making it difficult to generate appropriate explanations. In this work, though, we consider probabilistic logic programming, a combination of logic programming (for knowledge representation) and probability (to model uncertainty). In this setting, one can say that models are interpretable, which eases its understanding. However, given a particular query, the usual notion of "explanation" is associated with a set of choices, one for each random variable of the model. Unfortunately, this set does not have a causal structure and, in fact, some of the choices are actually irrelevant to the considered query. In order to overcome these shortcomings, we present an approach to explaining explanations which is based on the definition of a query-driven inference
&lt;/p&gt;</description></item><item><title>GPT-4 Vision&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#20855;&#26377;&#19987;&#23478;&#32423;&#20934;&#30830;&#24230;&#65292;&#20294;&#22312;&#22270;&#20687;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2401.08396</link><description>&lt;p&gt;
GPT-4 Vision&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#19987;&#23478;&#32423;&#20934;&#30830;&#24230;&#32972;&#21518;&#30340;&#38544;&#34255;&#32570;&#38519;
&lt;/p&gt;
&lt;p&gt;
Hidden Flaws Behind Expert-Level Accuracy of GPT-4 Vision in Medicine. (arXiv:2401.08396v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08396
&lt;/p&gt;
&lt;p&gt;
GPT-4 Vision&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#20855;&#26377;&#19987;&#23478;&#32423;&#20934;&#30830;&#24230;&#65292;&#20294;&#22312;&#22270;&#20687;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20855;&#26377;Vision&#21151;&#33021;&#30340;GPT-4&#22312;&#21307;&#23398;&#25361;&#25112;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#20154;&#31867;&#21307;&#29983;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35780;&#20272;&#20027;&#35201;&#20851;&#27880;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#20934;&#30830;&#24230;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;GPT-4V&#22312;&#35299;&#20915;&#26032;&#33521;&#26684;&#20848;&#21307;&#23398;&#26434;&#24535;&#22270;&#20687;&#25361;&#25112;&#20013;&#30340;&#22270;&#20687;&#29702;&#35299;&#12289;&#21307;&#23398;&#30693;&#35782;&#22238;&#24518;&#21644;&#36880;&#27493;&#22810;&#27169;&#24577;&#25512;&#29702;&#30340;&#21407;&#29702;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#25193;&#23637;&#20102;&#24403;&#21069;&#30340;&#30740;&#31350;&#33539;&#22260;&#12290;&#35780;&#20272;&#32467;&#26524;&#35777;&#23454;&#65292;GPT-4V&#22312;&#22810;&#39033;&#36873;&#25321;&#20934;&#30830;&#24230;&#19978;&#20248;&#20110;&#20154;&#31867;&#21307;&#29983;&#65288;88.0% vs. 77.0%&#65292;p=0.034&#65289;&#12290;GPT-4V&#22312;&#21307;&#29983;&#22238;&#31572;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#33021;&#34920;&#29616;&#20986;&#36229;&#36807;80%&#30340;&#20934;&#30830;&#24230;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-4V&#22312;&#26368;&#32456;&#20570;&#20986;&#27491;&#30830;&#36873;&#25321;&#30340;&#24773;&#20917;&#19979;&#65292;&#32463;&#24120;&#25552;&#20379;&#26377;&#32570;&#38519;&#30340;&#25512;&#29702;&#65288;27.3%&#65289;&#65292;&#20854;&#20013;&#26368;&#31361;&#20986;&#30340;&#26159;&#22270;&#20687;&#29702;&#35299;&#65288;21.6%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies indicate that Generative Pre-trained Transformer 4 with Vision (GPT-4V) outperforms human physicians in medical challenge tasks. However, these evaluations primarily focused on the accuracy of multi-choice questions alone. Our study extends the current scope by conducting a comprehensive analysis of GPT-4V's rationales of image comprehension, recall of medical knowledge, and step-by-step multimodal reasoning when solving New England Journal of Medicine (NEJM) Image Challenges - an imaging quiz designed to test the knowledge and diagnostic capabilities of medical professionals. Evaluation results confirmed that GPT-4V outperforms human physicians regarding multi-choice accuracy (88.0% vs. 77.0%, p=0.034). GPT-4V also performs well in cases where physicians incorrectly answer, with over 80% accuracy. However, we discovered that GPT-4V frequently presents flawed rationales in cases where it makes the correct final choices (27.3%), most prominent in image comprehension (21.6
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBMs&#65289;&#26159;&#21542;&#33021;&#22815;&#27491;&#30830;&#25429;&#25417;&#21040;&#27010;&#24565;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#31243;&#24230;&#65292;&#36890;&#36807;&#20998;&#26512;&#23545;&#20110;&#27010;&#24565;&#23616;&#37096;&#24615;&#20043;&#22806;&#29305;&#24449;&#30340;&#21464;&#21270;&#22914;&#20309;&#24433;&#21709;&#27010;&#24565;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.01259</link><description>&lt;p&gt;
&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#26159;&#21542;&#36981;&#24490;&#23616;&#37096;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Concept Bottleneck Models Obey Locality?. (arXiv:2401.01259v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBMs&#65289;&#26159;&#21542;&#33021;&#22815;&#27491;&#30830;&#25429;&#25417;&#21040;&#27010;&#24565;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#31243;&#24230;&#65292;&#36890;&#36807;&#20998;&#26512;&#23545;&#20110;&#27010;&#24565;&#23616;&#37096;&#24615;&#20043;&#22806;&#29305;&#24449;&#30340;&#21464;&#21270;&#22914;&#20309;&#24433;&#21709;&#27010;&#24565;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#22522;&#30784;&#23398;&#20064;&#36890;&#36807;&#35299;&#37322;&#20854;&#39044;&#27979;&#32467;&#26524;&#20351;&#29992;&#20154;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#65292;&#25913;&#21892;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#36825;&#31181;&#33539;&#24335;&#19979;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20005;&#37325;&#20381;&#36182;&#20110;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#23398;&#20064;&#29420;&#31435;&#20110;&#20854;&#20182;&#27010;&#24565;&#30340;&#32473;&#23450;&#27010;&#24565;&#30340;&#23384;&#22312;&#25110;&#19981;&#23384;&#22312;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24378;&#28872;&#26263;&#31034;&#36825;&#31181;&#20551;&#35774;&#21487;&#33021;&#22312;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBMs&#65289;&#36825;&#19968;&#20856;&#22411;&#30340;&#22522;&#20110;&#27010;&#24565;&#30340;&#21487;&#35299;&#37322;&#26550;&#26500;&#20013;&#19981;&#33021;&#25104;&#31435;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24403;&#36825;&#20123;&#27010;&#24565;&#26082;&#22312;&#31354;&#38388;&#19978;&#65288;&#36890;&#36807;&#23427;&#20204;&#30340;&#20540;&#23436;&#20840;&#30001;&#22266;&#23450;&#23376;&#38598;&#30340;&#29305;&#24449;&#23450;&#20041;&#65289;&#21448;&#22312;&#35821;&#20041;&#19978;&#65288;&#36890;&#36807;&#23427;&#20204;&#30340;&#20540;&#20165;&#19982;&#39044;&#23450;&#20041;&#30340;&#22266;&#23450;&#23376;&#38598;&#30340;&#27010;&#24565;&#30456;&#20851;&#32852;&#65289;&#23450;&#20301;&#26102;&#65292;CBMs&#26159;&#21542;&#27491;&#30830;&#25429;&#25417;&#21040;&#27010;&#24565;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#31243;&#24230;&#12290;&#20026;&#20102;&#29702;&#35299;&#23616;&#37096;&#24615;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#27010;&#24565;&#20043;&#22806;&#30340;&#29305;&#24449;&#21464;&#21270;&#23545;&#27010;&#24565;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept-based learning improves a deep learning model's interpretability by explaining its predictions via human-understandable concepts. Deep learning models trained under this paradigm heavily rely on the assumption that neural networks can learn to predict the presence or absence of a given concept independently of other concepts. Recent work, however, strongly suggests that this assumption may fail to hold in Concept Bottleneck Models (CBMs), a quintessential family of concept-based interpretable architectures. In this paper, we investigate whether CBMs correctly capture the degree of conditional independence across concepts when such concepts are localised both spatially, by having their values entirely defined by a fixed subset of features, and semantically, by having their values correlated with only a fixed subset of predefined concepts. To understand locality, we analyse how changes to features outside of a concept's spatial or semantic locality impact concept predictions. Our
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;&#31216;&#20026;DPPDCC&#65289;&#65292;&#29992;&#20110;&#23558;&#35770;&#25991;&#30340;&#28508;&#22312;&#24433;&#21709;&#20998;&#35299;&#20026;&#20256;&#25773;&#12289;&#19968;&#33268;&#24615;&#21644;&#36129;&#29486;&#20540;&#12290;&#36890;&#36807;&#32534;&#30721;&#26102;&#24577;&#21644;&#32467;&#26500;&#29305;&#24449;&#65292;&#25429;&#25417;&#30693;&#35782;&#27969;&#21160;&#65292;&#24182;&#20351;&#29992;&#23545;&#27604;&#22686;&#24378;&#22270;&#25581;&#31034;&#27969;&#34892;&#24230;&#65292;&#36827;&#19968;&#27493;&#39044;&#27979;&#24341;&#29992;&#20998;&#32452;&#26469;&#24314;&#27169;&#19968;&#33268;&#24615;&#12290;&#24212;&#29992;&#27491;&#20132;&#32422;&#26463;&#26469;&#40723;&#21169;&#29420;&#29305;&#24314;&#27169;&#65292;&#24182;&#20445;&#30041;&#21407;&#22987;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2311.09262</link><description>&lt;p&gt;
&#23558;&#35770;&#25991;&#30340;&#28508;&#22312;&#24433;&#21709;&#20998;&#35299;&#20026;&#20256;&#25773;&#12289;&#19968;&#33268;&#24615;&#21644;&#36129;&#29486;&#20540;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Disentangling the Potential Impacts of Papers into Diffusion, Conformity, and Contribution Values. (arXiv:2311.09262v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.09262
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;&#31216;&#20026;DPPDCC&#65289;&#65292;&#29992;&#20110;&#23558;&#35770;&#25991;&#30340;&#28508;&#22312;&#24433;&#21709;&#20998;&#35299;&#20026;&#20256;&#25773;&#12289;&#19968;&#33268;&#24615;&#21644;&#36129;&#29486;&#20540;&#12290;&#36890;&#36807;&#32534;&#30721;&#26102;&#24577;&#21644;&#32467;&#26500;&#29305;&#24449;&#65292;&#25429;&#25417;&#30693;&#35782;&#27969;&#21160;&#65292;&#24182;&#20351;&#29992;&#23545;&#27604;&#22686;&#24378;&#22270;&#25581;&#31034;&#27969;&#34892;&#24230;&#65292;&#36827;&#19968;&#27493;&#39044;&#27979;&#24341;&#29992;&#20998;&#32452;&#26469;&#24314;&#27169;&#19968;&#33268;&#24615;&#12290;&#24212;&#29992;&#27491;&#20132;&#32422;&#26463;&#26469;&#40723;&#21169;&#29420;&#29305;&#24314;&#27169;&#65292;&#24182;&#20445;&#30041;&#21407;&#22987;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30340;&#28508;&#22312;&#24433;&#21709;&#21463;&#21040;&#22810;&#31181;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#20854;&#27969;&#34892;&#24230;&#21644;&#36129;&#29486;&#12290;&#29616;&#26377;&#27169;&#22411;&#36890;&#24120;&#22522;&#20110;&#38745;&#24577;&#22270;&#26469;&#20272;&#35745;&#21407;&#22987;&#24341;&#29992;&#35745;&#25968;&#65292;&#26410;&#33021;&#20174;&#32454;&#24494;&#30340;&#35282;&#24230;&#21306;&#20998;&#20215;&#20540;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#23558;&#35770;&#25991;&#30340;&#28508;&#22312;&#24433;&#21709;&#20998;&#35299;&#20026;&#20256;&#25773;&#12289;&#19968;&#33268;&#24615;&#21644;&#36129;&#29486;&#20540;&#65288;&#31216;&#20026;DPPDCC&#65289;&#12290;&#32473;&#23450;&#19968;&#20010;&#30446;&#26631;&#35770;&#25991;&#65292;DPPDCC&#22312;&#26500;&#24314;&#30340;&#21160;&#24577;&#24322;&#26500;&#22270;&#20013;&#32534;&#30721;&#20102;&#26102;&#24577;&#21644;&#32467;&#26500;&#29305;&#24449;&#12290;&#29305;&#21035;&#22320;&#65292;&#20026;&#20102;&#25429;&#25417;&#30693;&#35782;&#27969;&#21160;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#35770;&#25991;&#20043;&#38388;&#30340;&#27604;&#36739;&#21644;&#20849;&#24341;/&#34987;&#24341;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#36827;&#34892;&#20102;&#24555;&#29031;&#28436;&#21270;&#30340;&#32858;&#21512;&#12290;&#20026;&#20102;&#25581;&#31034;&#27969;&#34892;&#24230;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#27604;&#22686;&#24378;&#22270;&#26469;&#25552;&#21462;&#20256;&#25773;&#30340;&#26412;&#36136;&#65292;&#24182;&#39044;&#27979;&#32047;&#31215;&#30340;&#24341;&#29992;&#20998;&#32452;&#20197;&#24314;&#27169;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24212;&#29992;&#27491;&#20132;&#32422;&#26463;&#26469;&#40723;&#21169;&#27599;&#20010;&#35282;&#24230;&#30340;&#29420;&#29305;&#24314;&#27169;&#65292;&#24182;&#20445;&#30041;&#20854;&#22266;&#26377;&#33719;&#24471;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The potential impact of an academic paper is determined by various factors, including its popularity and contribution. Existing models usually estimate original citation counts based on static graphs and fail to differentiate values from nuanced perspectives. In this study, we propose a novel graph neural network to Disentangle the Potential impacts of Papers into Diffusion, Conformity, and Contribution values (called DPPDCC). Given a target paper, DPPDCC encodes temporal and structural features within the constructed dynamic heterogeneous graph. Particularly, to capture the knowledge flow, we emphasize the importance of comparative and co-cited/citing information between papers and aggregate snapshots evolutionarily. To unravel popularity, we contrast augmented graphs to extract the essence of diffusion and predict the accumulated citation binning to model conformity. We further apply orthogonal constraints to encourage distinct modeling of each perspective and preserve the inherent v
&lt;/p&gt;</description></item><item><title>Sentinel&#26159;&#19968;&#31181;&#29992;&#20110;&#20445;&#25252;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#30340;&#38450;&#24481;&#31574;&#30053;&#65292;&#36890;&#36807;&#21033;&#29992;&#26412;&#22320;&#25968;&#25454;&#24182;&#23450;&#20041;&#19968;&#20010;&#19977;&#27493;&#32858;&#21512;&#21327;&#35758;&#26469;&#23545;&#25239;&#27745;&#26579;&#25915;&#20987;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;Sentinel&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.08097</link><description>&lt;p&gt;
Sentinel: &#19968;&#31181;&#29992;&#20110;&#20445;&#25252;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#30340;&#32858;&#21512;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Sentinel: An Aggregation Function to Secure Decentralized Federated Learning. (arXiv:2310.08097v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08097
&lt;/p&gt;
&lt;p&gt;
Sentinel&#26159;&#19968;&#31181;&#29992;&#20110;&#20445;&#25252;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#30340;&#38450;&#24481;&#31574;&#30053;&#65292;&#36890;&#36807;&#21033;&#29992;&#26412;&#22320;&#25968;&#25454;&#24182;&#23450;&#20041;&#19968;&#20010;&#19977;&#27493;&#32858;&#21512;&#21327;&#35758;&#26469;&#23545;&#25239;&#27745;&#26579;&#25915;&#20987;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;Sentinel&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24555;&#36895;&#25972;&#21512;&#21040;&#32593;&#32476;&#20013;&#28085;&#30422;&#20102;&#32593;&#32476;&#31649;&#29702;&#12289;&#26381;&#21153;&#36136;&#37327;&#21644;&#32593;&#32476;&#23433;&#20840;&#31561;&#21508;&#20010;&#26041;&#38754;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#20316;&#20026;&#19968;&#31181;&#21019;&#26032;&#33539;&#24335;&#65292;&#29992;&#20110;&#35757;&#32451;&#21327;&#20316;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#21333;&#28857;&#22833;&#25928;&#30340;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;FL&#21644;DFL&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#20449;&#24615;&#21463;&#21040;&#27745;&#26579;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23545;&#20854;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#26426;&#21046;&#38024;&#23545;&#38598;&#20013;&#24335;FL&#36827;&#34892;&#35774;&#35745;&#65292;&#24182;&#26410;&#20805;&#20998;&#21033;&#29992;DFL&#30340;&#29305;&#28857;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;Sentinel&#65292;&#19968;&#31181;&#22312;DFL&#20013;&#23545;&#25239;&#27745;&#26579;&#25915;&#20987;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;Sentinel&#21033;&#29992;&#26412;&#22320;&#25968;&#25454;&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#23450;&#20041;&#20102;&#19968;&#20010;&#19977;&#27493;&#32858;&#21512;&#21327;&#35758;&#65292;&#21253;&#25324;&#30456;&#20284;&#24615;&#36807;&#28388;&#12289;&#24341;&#23548;&#39564;&#35777;&#21644;&#26631;&#20934;&#21270;&#65292;&#20197;&#38450;&#27490;&#24694;&#24847;&#27169;&#22411;&#26356;&#26032;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#23545;Sentinel&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid integration of Federated Learning (FL) into networking encompasses various aspects such as network management, quality of service, and cybersecurity while preserving data privacy. In this context, Decentralized Federated Learning (DFL) emerges as an innovative paradigm to train collaborative models, addressing the single point of failure limitation. However, the security and trustworthiness of FL and DFL are compromised by poisoning attacks, negatively impacting its performance. Existing defense mechanisms have been designed for centralized FL and they do not adequately exploit the particularities of DFL. Thus, this work introduces Sentinel, a defense strategy to counteract poisoning attacks in DFL. Sentinel leverages the accessibility of local data and defines a three-step aggregation protocol consisting of similarity filtering, bootstrap validation, and normalization to safeguard against malicious model updates. Sentinel has been evaluated with diverse datasets and various 
&lt;/p&gt;</description></item><item><title>Suspicion-Agent&#26159;&#19968;&#31181;&#21019;&#26032;&#20195;&#29702;&#31243;&#24207;&#65292;&#21033;&#29992;&#20855;&#22791;&#39640;&#38454;&#24515;&#28789;&#29702;&#35770;&#24847;&#35782;&#30340;GPT4&#22312;&#19981;&#23436;&#20840;&#20449;&#24687;&#28216;&#25103;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#36866;&#24212;&#24615;&#21644;&#24433;&#21709;&#20182;&#20154;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.17277</link><description>&lt;p&gt;
Suspicion-Agent: &#20351;&#29992;&#20855;&#22791;&#24515;&#28789;&#29702;&#35770;&#24847;&#35782;&#30340;GPT4&#22312;&#19981;&#23436;&#20840;&#20449;&#24687;&#28216;&#25103;&#20013;&#36827;&#34892;&#23545;&#23616;
&lt;/p&gt;
&lt;p&gt;
Suspicion-Agent: Playing Imperfect Information Games with Theory of Mind Aware GPT4. (arXiv:2309.17277v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17277
&lt;/p&gt;
&lt;p&gt;
Suspicion-Agent&#26159;&#19968;&#31181;&#21019;&#26032;&#20195;&#29702;&#31243;&#24207;&#65292;&#21033;&#29992;&#20855;&#22791;&#39640;&#38454;&#24515;&#28789;&#29702;&#35770;&#24847;&#35782;&#30340;GPT4&#22312;&#19981;&#23436;&#20840;&#20449;&#24687;&#28216;&#25103;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#36866;&#24212;&#24615;&#21644;&#24433;&#21709;&#20182;&#20154;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#20110;&#23436;&#20840;&#20449;&#24687;&#28216;&#25103;&#65292;&#20854;&#20013;&#27599;&#20010;&#29609;&#23478;&#37117;&#30693;&#36947;&#25152;&#26377;&#20803;&#32032;&#65292;&#19981;&#23436;&#20840;&#20449;&#24687;&#28216;&#25103;&#27169;&#25311;&#20102;&#22312;&#19981;&#30830;&#23450;&#25110;&#19981;&#23436;&#25972;&#20449;&#24687;&#19979;&#36827;&#34892;&#20915;&#31574;&#30340;&#29616;&#23454;&#19990;&#30028;&#22797;&#26434;&#24615;&#12290;&#26368;&#36817;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;GPT-4&#20197;&#20854;&#30693;&#35782;&#26816;&#32034;&#21644;&#25512;&#29702;&#33021;&#21147;&#32780;&#38395;&#21517;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;GPT-4&#30340;&#23398;&#20064;&#30693;&#35782;&#24212;&#29992;&#20110;&#19981;&#23436;&#20840;&#20449;&#24687;&#28216;&#25103;&#30340;&#21487;&#34892;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#20195;&#29702;&#31243;&#24207;\textbf{Suspicion-Agent}&#65292;&#35813;&#20195;&#29702;&#31243;&#24207;&#21033;&#29992;GPT-4&#30340;&#33021;&#21147;&#22312;&#19981;&#23436;&#20840;&#20449;&#24687;&#28216;&#25103;&#20013;&#36827;&#34892;&#23545;&#23616;&#12290;&#36890;&#36807;&#21512;&#36866;&#30340;&#25552;&#31034;&#24037;&#31243;&#26469;&#23454;&#29616;&#19981;&#21516;&#30340;&#21151;&#33021;&#65292;&#22522;&#20110;GPT-4&#30340;Suspicion-Agent&#23637;&#31034;&#20102;&#22312;&#19968;&#31995;&#21015;&#19981;&#23436;&#20840;&#20449;&#24687;&#32440;&#29260;&#28216;&#25103;&#20013;&#30340;&#26174;&#33879;&#36866;&#24212;&#33021;&#21147;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;GPT-4&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#39640;&#38454;&#24515;&#28789;&#29702;&#35770;&#65288;Theory of Mind&#65289;&#33021;&#21147;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#33021;&#22815;&#29702;&#35299;&#20182;&#20154;&#24182;&#26377;&#24847;&#35782;&#22320;&#24433;&#21709;&#20182;&#20154;&#30340;&#34892;&#20026;&#12290;&#21033;&#29992;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
Unlike perfect information games, where all elements are known to every player, imperfect information games emulate the real-world complexities of decision-making under uncertain or incomplete information. GPT-4, the recent breakthrough in large language models (LLMs) trained on massive passive data, is notable for its knowledge retrieval and reasoning abilities. This paper delves into the applicability of GPT-4's learned knowledge for imperfect information games. To achieve this, we introduce \textbf{Suspicion-Agent}, an innovative agent that leverages GPT-4's capabilities for performing in imperfect information games. With proper prompt engineering to achieve different functions, Suspicion-Agent based on GPT-4 demonstrates remarkable adaptability across a range of imperfect information card games. Importantly, GPT-4 displays a strong high-order theory of mind (ToM) capacity, meaning it can understand others and intentionally impact others' behavior. Leveraging this, we design a plann
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;NutritionVerse-Synth&#65292;&#36825;&#26159;&#19968;&#20010;&#25317;&#26377;&#22823;&#35268;&#27169;&#21512;&#25104;&#39135;&#29289;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#22810;&#31181;&#35270;&#35282;&#12289;&#27169;&#24577;&#21644;&#39278;&#39135;&#27880;&#37322;&#65292;&#26088;&#22312;&#35299;&#20915;&#30446;&#21069;&#39278;&#39135;&#25668;&#20837;&#20272;&#35745;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#30495;&#23454;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.07704</link><description>&lt;p&gt;
NutritionVerse: &#21508;&#31181;&#39278;&#39135;&#25668;&#20837;&#20272;&#35745;&#26041;&#27861;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
NutritionVerse: Empirical Study of Various Dietary Intake Estimation Approaches. (arXiv:2309.07704v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07704
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;NutritionVerse-Synth&#65292;&#36825;&#26159;&#19968;&#20010;&#25317;&#26377;&#22823;&#35268;&#27169;&#21512;&#25104;&#39135;&#29289;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#22810;&#31181;&#35270;&#35282;&#12289;&#27169;&#24577;&#21644;&#39278;&#39135;&#27880;&#37322;&#65292;&#26088;&#22312;&#35299;&#20915;&#30446;&#21069;&#39278;&#39135;&#25668;&#20837;&#20272;&#35745;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#30495;&#23454;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#39278;&#39135;&#25668;&#20837;&#20272;&#35745;&#23545;&#20110;&#25903;&#25345;&#20581;&#24247;&#39278;&#39135;&#30340;&#25919;&#31574;&#21644;&#31243;&#24207;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#33829;&#20859;&#19981;&#33391;&#19982;&#29983;&#27963;&#36136;&#37327;&#19979;&#38477;&#30452;&#25509;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#35832;&#22914;&#39135;&#29289;&#26085;&#35760;&#20043;&#31867;&#30340;&#33258;&#25105;&#25253;&#21578;&#26041;&#27861;&#23384;&#22312;&#26174;&#33879;&#20559;&#24046;&#12290;&#20854;&#20182;&#20256;&#32479;&#30340;&#39278;&#39135;&#35780;&#20272;&#25216;&#26415;&#21644;&#26032;&#20852;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#22914;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#65292;&#32791;&#26102;&#38271;&#65292;&#24182;&#19988;&#21487;&#33021;&#38656;&#35201;&#21463;&#36807;&#35757;&#32451;&#30340;&#20154;&#21592;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#20351;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#26469;&#20174;&#39135;&#29289;&#22270;&#20687;&#20013;&#33258;&#21160;&#20272;&#35745;&#39278;&#39135;&#25668;&#20837;&#37327;&#65292;&#20294;&#32570;&#20047;&#20855;&#26377;&#22810;&#26679;&#35270;&#35282;&#12289;&#27169;&#24577;&#21644;&#39135;&#29289;&#27880;&#37322;&#30340;&#32508;&#21512;&#25968;&#25454;&#38598;&#38480;&#21046;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#30495;&#23454;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;NutritionVerse-Synth&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#25317;&#26377;84,984&#20010;&#36924;&#30495;&#30340;&#21512;&#25104;2D&#39135;&#29289;&#22270;&#20687;&#21450;&#30456;&#20851;&#39278;&#39135;&#20449;&#24687;&#21644;&#22810;&#27169;&#24577;&#26631;&#27880;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#28145;&#24230;&#22270;&#20687;&#12289;&#23454;&#20363;&#25513;&#33180;&#21644;&#35821;&#20041;&#25513;&#33180;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate dietary intake estimation is critical for informing policies and programs to support healthy eating, as malnutrition has been directly linked to decreased quality of life. However self-reporting methods such as food diaries suffer from substantial bias. Other conventional dietary assessment techniques and emerging alternative approaches such as mobile applications incur high time costs and may necessitate trained personnel. Recent work has focused on using computer vision and machine learning to automatically estimate dietary intake from food images, but the lack of comprehensive datasets with diverse viewpoints, modalities and food annotations hinders the accuracy and realism of such methods. To address this limitation, we introduce NutritionVerse-Synth, the first large-scale dataset of 84,984 photorealistic synthetic 2D food images with associated dietary information and multimodal annotations (including depth images, instance masks, and semantic masks). Additionally, we col
&lt;/p&gt;</description></item><item><title>VoiceFlow&#20351;&#29992;&#30699;&#27491;&#27969;&#21305;&#37197;&#31639;&#27861;&#23454;&#29616;&#20102;&#39640;&#25928;&#25991;&#26412;&#36716;&#35821;&#38899;&#65292;&#24182;&#22312;&#21512;&#25104;&#36136;&#37327;&#19978;&#20248;&#20110;&#20256;&#32479;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.05027</link><description>&lt;p&gt;
VoiceFlow: &#20351;&#29992;&#30699;&#27491;&#27969;&#21305;&#37197;&#30340;&#39640;&#25928;&#25991;&#26412;&#36716;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;
VoiceFlow: Efficient Text-to-Speech with Rectified Flow Matching. (arXiv:2309.05027v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05027
&lt;/p&gt;
&lt;p&gt;
VoiceFlow&#20351;&#29992;&#30699;&#27491;&#27969;&#21305;&#37197;&#31639;&#27861;&#23454;&#29616;&#20102;&#39640;&#25928;&#25991;&#26412;&#36716;&#35821;&#38899;&#65292;&#24182;&#22312;&#21512;&#25104;&#36136;&#37327;&#19978;&#20248;&#20110;&#20256;&#32479;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#22312;&#25991;&#26412;&#36716;&#35821;&#38899;&#20013;&#22240;&#20854;&#24378;&#22823;&#30340;&#29983;&#25104;&#33021;&#21147;&#32780;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#36873;&#25321;&#65292;&#20294;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#36827;&#34892;&#37319;&#26679;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#25439;&#23475;&#20102;&#20854;&#25928;&#29575;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VoiceFlow&#65292;&#19968;&#31181;&#21033;&#29992;&#30699;&#27491;&#27969;&#21305;&#37197;&#31639;&#27861;&#26469;&#23454;&#29616;&#39640;&#21512;&#25104;&#36136;&#37327;&#30340;&#22768;&#23398;&#27169;&#22411;&#65292;&#21482;&#38656;&#26377;&#38480;&#27425;&#37319;&#26679;&#27493;&#39588;&#21363;&#21487;&#23454;&#29616;&#12290;VoiceFlow&#23558;&#29983;&#25104;mel-spectrograms&#30340;&#36807;&#31243;&#36716;&#21270;&#20026;&#19968;&#20010;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65292;&#22312;&#25991;&#26412;&#36755;&#20837;&#30340;&#26465;&#20214;&#19979;&#36827;&#34892;&#27714;&#35299;&#65292;&#24182;&#20272;&#35745;&#20986;&#20854;&#21521;&#37327;&#22330;&#12290;&#28982;&#21518;&#65292;&#30699;&#27491;&#27969;&#25216;&#26415;&#26377;&#25928;&#22320;&#20351;&#20854;&#37319;&#26679;&#36712;&#36857;&#30452;&#32447;&#21270;&#65292;&#23454;&#29616;&#39640;&#25928;&#21512;&#25104;&#12290;&#22312;&#21333;&#20010;&#21644;&#22810;&#20010;&#35828;&#35805;&#32773;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#30340;&#20027;&#35266;&#21644;&#23458;&#35266;&#35780;&#20272;&#26174;&#31034;&#65292;VoiceFlow&#30456;&#23545;&#20110;&#25193;&#25955;&#27169;&#22411;&#20855;&#26377;&#26356;&#20248;&#24322;&#30340;&#21512;&#25104;&#36136;&#37327;&#12290;&#28040;&#34701;&#30740;&#31350;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;VoiceFlow&#20013;&#30699;&#27491;&#27969;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although diffusion models in text-to-speech have become a popular choice due to their strong generative ability, the intrinsic complexity of sampling from diffusion models harms their efficiency. Alternatively, we propose VoiceFlow, an acoustic model that utilizes a rectified flow matching algorithm to achieve high synthesis quality with a limited number of sampling steps. VoiceFlow formulates the process of generating mel-spectrograms into an ordinary differential equation conditional on text inputs, whose vector field is then estimated. The rectified flow technique then effectively straightens its sampling trajectory for efficient synthesis. Subjective and objective evaluations on both single and multi-speaker corpora showed the superior synthesis quality of VoiceFlow compared to the diffusion counterpart. Ablation studies further verified the validity of the rectified flow technique in VoiceFlow.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#33258;&#20027;&#20195;&#29702;&#30340;&#26041;&#27861;&#65292;&#20197;&#31616;&#21270;&#22810;&#26679;&#21270;&#30340;&#29616;&#23454;&#19990;&#30028;&#22270;&#20013;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.04565</link><description>&lt;p&gt;
&#35299;&#25918;&#22270;&#23398;&#20064;&#30340;&#21147;&#37327;&#65306;&#22522;&#20110;LLM&#30340;&#33258;&#20027;&#20195;&#29702;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Power of Graph Learning through LLM-based Autonomous Agents. (arXiv:2309.04565v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#33258;&#20027;&#20195;&#29702;&#30340;&#26041;&#27861;&#65292;&#20197;&#31616;&#21270;&#22810;&#26679;&#21270;&#30340;&#29616;&#23454;&#19990;&#30028;&#22270;&#20013;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24191;&#27867;&#23384;&#22312;&#21644;&#24212;&#29992;&#65292;&#20294;&#26377;&#25928;&#22320;&#22788;&#29702;&#36825;&#20123;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#21644;&#22312;&#22270;&#19978;&#36827;&#34892;&#23398;&#20064;&#20219;&#21153;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#38754;&#23545;&#22797;&#26434;&#30340;&#22270;&#23398;&#20064;&#20219;&#21153;&#65292;&#19987;&#23478;&#20204;&#22312;&#36817;&#24180;&#26469;&#35774;&#35745;&#20102;&#21508;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#12290;&#20182;&#20204;&#36824;&#23454;&#26045;&#20102;&#22270;&#20013;&#30340;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#65292;&#20063;&#31216;&#20026;AutoGraph&#65292;&#20197;&#33258;&#21160;&#29983;&#25104;&#25968;&#25454;&#29305;&#23450;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20182;&#20204;&#22312;&#20197;&#19979;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65306;&#65288;1&#65289;&#22312;&#19981;&#21516;&#23618;&#32423;&#19978;&#31649;&#29702;&#21508;&#31181;&#23398;&#20064;&#20219;&#21153;&#65292;&#65288;2&#65289;&#22788;&#29702;&#22270;&#23398;&#20064;&#20013;&#19981;&#21516;&#30340;&#27969;&#31243;&#65288;&#36229;&#36807;&#26550;&#26500;&#35774;&#35745;&#65289;&#65292;&#20197;&#21450;&#65288;3&#65289;&#20351;&#29992;AutoGraph&#26102;&#23545;&#20808;&#39564;&#30693;&#35782;&#30340;&#24040;&#22823;&#38656;&#27714;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#33258;&#20027;&#20195;&#29702;&#26469;&#31616;&#21270;&#22810;&#26679;&#21270;&#30340;&#29616;&#23454;&#19990;&#30028;&#22270;&#20013;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#38024;&#23545;&#29992;&#25143;&#35831;&#27714;&#65288;&#35813;&#35831;&#27714;&#21487;&#33021;&#21253;&#21547;&#33410;&#28857;&#12289;&#36793;&#32536;&#25110;&#22270;&#32423;&#21035;&#30340;&#19981;&#21516;&#25968;&#25454;&#21644;&#23398;&#20064;&#30446;&#26631;&#65289;&#65292;&#22797;&#26434;&#22270;&#20013;&#30340;&#23398;&#20064;&#36807;&#31243;&#23558;&#30001;LLM&#33258;&#20027;&#20195;&#29702;&#26426;&#21046;&#26469;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph structured data are widely existed and applied in the real-world applications, while it is a challenge to handling these diverse data and learning tasks on graph in an efficient manner. When facing the complicated graph learning tasks, experts have designed diverse Graph Neural Networks (GNNs) in recent years. They have also implemented AutoML in Graph, also known as AutoGraph, to automatically generate data-specific solutions. Despite their success, they encounter limitations in (1) managing diverse learning tasks at various levels, (2) dealing with different procedures in graph learning beyond architecture design, and (3) the huge requirements on the prior knowledge when using AutoGraph. In this paper, we propose to use Large Language Models (LLMs) as autonomous agents to simplify the learning process on diverse real-world graphs. Specifically, in response to a user request which may contain varying data and learning targets at the node, edge, or graph levels, the complex graph
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#21453;&#28436;&#30340;&#21435;&#38500;&#25915;&#20987;&#26041;&#27861;&#65288;\textsc{Mira}&#65289;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#30772;&#35299;&#22823;&#22810;&#25968;&#20027;&#27969;&#40657;&#30418;DNN&#27700;&#21360;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#21463;&#20445;&#25252;&#27169;&#22411;&#30340;&#20869;&#37096;&#20449;&#24687;&#26469;&#24674;&#22797;&#21644;&#28040;&#38500;&#27700;&#21360;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2309.03466</link><description>&lt;p&gt;
&#21033;&#29992;&#22522;&#20110;&#27169;&#22411;&#21453;&#28436;&#30340;&#21435;&#38500;&#25915;&#20987;&#26041;&#27861;&#30772;&#35299;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#40657;&#30418;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
MIRA: Cracking Black-box Watermarking on Deep Neural Networks via Model Inversion-based Removal Attacks. (arXiv:2309.03466v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#21453;&#28436;&#30340;&#21435;&#38500;&#25915;&#20987;&#26041;&#27861;&#65288;\textsc{Mira}&#65289;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#30772;&#35299;&#22823;&#22810;&#25968;&#20027;&#27969;&#40657;&#30418;DNN&#27700;&#21360;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#21463;&#20445;&#25252;&#27169;&#22411;&#30340;&#20869;&#37096;&#20449;&#24687;&#26469;&#24674;&#22797;&#21644;&#28040;&#38500;&#27700;&#21360;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20445;&#25252;&#35757;&#32451;&#26377;&#32032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#30693;&#35782;&#20135;&#26435;&#65292;&#40657;&#30418;DNN&#27700;&#21360;&#24050;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#36825;&#20123;&#27700;&#21360;&#34987;&#23884;&#20837;&#21040;DNN&#27169;&#22411;&#22312;&#19968;&#32452;&#29305;&#21035;&#35774;&#35745;&#30340;&#26679;&#26412;&#19978;&#30340;&#39044;&#27979;&#34892;&#20026;&#20013;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#32463;&#39564;&#35777;&#26126;&#65292;&#22823;&#22810;&#25968;&#40657;&#30418;&#27700;&#21360;&#26041;&#26696;&#23545;&#24050;&#30693;&#30340;&#21435;&#38500;&#25915;&#20987;&#20855;&#26377;&#25269;&#25239;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#21453;&#28436;&#30340;&#21435;&#38500;&#25915;&#20987;&#26041;&#27861;&#65288;\textsc{Mira}&#65289;&#65292;&#35813;&#26041;&#27861;&#23545;&#22823;&#22810;&#25968;&#20027;&#27969;&#40657;&#30418;DNN&#27700;&#21360;&#26041;&#26696;&#37117;&#26159;&#26080;&#20851;&#27700;&#21360;&#30340;&#65292;&#24182;&#19988;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#25915;&#20987;&#27969;&#31243;&#21033;&#29992;&#21463;&#20445;&#25252;&#27169;&#22411;&#30340;&#20869;&#37096;&#20449;&#24687;&#26469;&#24674;&#22797;&#21644;&#28040;&#38500;&#27700;&#21360;&#20449;&#24687;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#30446;&#26631;&#31867;&#21035;&#26816;&#27979;&#21644;&#24674;&#22797;&#26679;&#26412;&#20998;&#21106;&#31639;&#27861;&#65292;&#20197;&#20943;&#23569;\textsc{Mira}&#24341;&#36215;&#30340;&#25928;&#29992;&#25439;&#22833;&#65292;&#24182;&#23454;&#29616;&#26368;&#20248;&#30340;&#25915;&#20987;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
To protect the intellectual property of well-trained deep neural networks (DNNs), black-box DNN watermarks, which are embedded into the prediction behavior of DNN models on a set of specially-crafted samples, have gained increasing popularity in both academy and industry. Watermark robustness is usually implemented against attackers who steal the protected model and obfuscate its parameters for watermark removal. Recent studies empirically prove the robustness of most black-box watermarking schemes against known removal attempts.  In this paper, we propose a novel Model Inversion-based Removal Attack (\textsc{Mira}), which is watermark-agnostic and effective against most of mainstream black-box DNN watermarking schemes. In general, our attack pipeline exploits the internals of the protected model to recover and unlearn the watermark message. We further design target class detection and recovered sample splitting algorithms to reduce the utility loss caused by \textsc{Mira} and achieve 
&lt;/p&gt;</description></item><item><title>GPT&#36890;&#36807;&#37329;&#34701;&#32032;&#20859;&#27979;&#35797;&#26174;&#31034;&#20986;&#20855;&#22791;&#25104;&#20026;&#22823;&#20247;&#37329;&#34701;&#26426;&#22120;&#39038;&#38382;&#30340;&#33021;&#21147;&#65292;&#20854;&#20013;&#22522;&#20110;GPT-4&#30340;ChatGPT&#20960;&#20046;&#23436;&#32654;&#22320;&#24471;&#20998;99%&#65292;&#25581;&#31034;&#20102;&#37329;&#34701;&#32032;&#20859;&#27491;&#22312;&#25104;&#20026;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#26032;&#20852;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.00649</link><description>&lt;p&gt;
GPT&#24050;&#32463;&#20855;&#22791;&#20102;&#37329;&#34701;&#32032;&#20859;&#65306;&#26469;&#33258;GPT&#37329;&#34701;&#32032;&#20859;&#27979;&#35797;&#30340;&#35265;&#35299;&#20197;&#21450;&#20154;&#20204;&#20351;&#29992;&#20854;&#20316;&#20026;&#21672;&#35810;&#26469;&#28304;&#30340;&#21021;&#27493;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
GPT has become financially literate: Insights from financial literacy tests of GPT and a preliminary test of how people use it as a source of advice. (arXiv:2309.00649v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00649
&lt;/p&gt;
&lt;p&gt;
GPT&#36890;&#36807;&#37329;&#34701;&#32032;&#20859;&#27979;&#35797;&#26174;&#31034;&#20986;&#20855;&#22791;&#25104;&#20026;&#22823;&#20247;&#37329;&#34701;&#26426;&#22120;&#39038;&#38382;&#30340;&#33021;&#21147;&#65292;&#20854;&#20013;&#22522;&#20110;GPT-4&#30340;ChatGPT&#20960;&#20046;&#23436;&#32654;&#22320;&#24471;&#20998;99%&#65292;&#25581;&#31034;&#20102;&#37329;&#34701;&#32032;&#20859;&#27491;&#22312;&#25104;&#20026;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#26032;&#20852;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#37329;&#34701;&#32032;&#20859;&#27979;&#35797;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;GPT&#65288;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#20316;&#20026;&#22823;&#20247;&#37329;&#34701;&#26426;&#22120;&#39038;&#38382;&#30340;&#33021;&#21147;&#12290;&#22522;&#20110;GPT-3.5&#30340;Davinci&#21644;ChatGPT&#20998;&#21035;&#22312;&#37329;&#34701;&#32032;&#20859;&#27979;&#35797;&#20013;&#24471;&#20998;&#20026;66%&#21644;65%&#65292;&#32780;&#22522;&#20110;GPT-4&#30340;ChatGPT&#20960;&#20046;&#23436;&#32654;&#22320;&#24471;&#21040;&#20102;99%&#30340;&#20998;&#25968;&#65292;&#36825;&#34920;&#26126;&#37329;&#34701;&#32032;&#20859;&#27491;&#22312;&#25104;&#20026;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#26032;&#20852;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;Judge-Advisor&#31995;&#32479;&#21644;&#19968;&#20010;&#20648;&#33988;&#22256;&#22659;&#26469;&#35828;&#26126;&#30740;&#31350;&#20154;&#21592;&#22914;&#20309;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#30340;&#24314;&#35758;&#21033;&#29992;&#24773;&#20917;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20123;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
We assess the ability of GPT -- a large language model -- to serve as a financial robo-advisor for the masses, by using a financial literacy test. Davinci and ChatGPT based on GPT-3.5 score 66% and 65% on the financial literacy test, respectively, compared to a baseline of 33%. However, ChatGPT based on GPT-4 achieves a near-perfect 99% score, pointing to financial literacy becoming an emergent ability of state-of-the-art models. We use the Judge-Advisor System and a savings dilemma to illustrate how researchers might assess advice-utilization from large language models. We also present a number of directions for future research.
&lt;/p&gt;</description></item><item><title>RLAIF&#26159;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;AI&#21453;&#39304;&#20195;&#26367;&#20154;&#31867;&#26631;&#27880;&#20559;&#22909;&#65292;&#30456;&#27604;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31867;&#20284;&#30340;&#25913;&#36827;&#25928;&#26524;&#65292;&#24182;&#19988;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#24471;&#21040;&#20102;&#30456;&#21516;&#30340;&#35748;&#21487;&#12290;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#28508;&#21147;&#35299;&#20915;RLHF&#30340;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.00267</link><description>&lt;p&gt;
RLAIF: &#20351;&#29992;AI&#21453;&#39304;&#26469;&#25193;&#23637;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback. (arXiv:2309.00267v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00267
&lt;/p&gt;
&lt;p&gt;
RLAIF&#26159;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;AI&#21453;&#39304;&#20195;&#26367;&#20154;&#31867;&#26631;&#27880;&#20559;&#22909;&#65292;&#30456;&#27604;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31867;&#20284;&#30340;&#25913;&#36827;&#25928;&#26524;&#65292;&#24182;&#19988;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#24471;&#21040;&#20102;&#30456;&#21516;&#30340;&#35748;&#21487;&#12290;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#28508;&#21147;&#35299;&#20915;RLHF&#30340;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#23545;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#19968;&#33268;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#26159;&#25910;&#38598;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#20559;&#22909;&#26631;&#31614;&#26159;&#19968;&#20010;&#20851;&#38190;&#29942;&#39048;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;RLHF&#21644;&#21033;&#29992;&#29616;&#25104;&#30340;LLM&#36827;&#34892;&#26631;&#35760;&#30340;RL from AI Feedback (RLAIF)&#25216;&#26415;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#37117;&#33021;&#33719;&#24471;&#31867;&#20284;&#30340;&#25913;&#21892;&#25928;&#26524;&#12290;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#65292;&#20154;&#31867;&#35780;&#20272;&#32773;&#22312;&#32422;70%&#30340;&#26696;&#20363;&#20013;&#37117;&#26356;&#21916;&#27426;RLAIF&#21644;RLHF&#20135;&#29983;&#30340;&#25991;&#26412;&#65292;&#32780;&#19981;&#26159;&#22522;&#20934;&#30340;&#30417;&#30563;&#24494;&#35843;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#24403;&#34987;&#35201;&#27714;&#35780;&#20272;RLAIF&#21644;RLHF&#30340;&#25688;&#35201;&#26102;&#65292;&#20154;&#31867;&#20197;&#30456;&#21516;&#30340;&#27604;&#29575;&#26356;&#21916;&#27426;&#20004;&#32773;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;RLAIF&#21487;&#20197;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#65292;&#20026;&#20811;&#26381;RLHF&#30340;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) is effective at aligning large language models (LLMs) to human preferences, but gathering high quality human preference labels is a key bottleneck. We conduct a head-to-head comparison of RLHF vs. RL from AI Feedback (RLAIF) - a technique where preferences are labeled by an off-the-shelf LLM in lieu of humans, and we find that they result in similar improvements. On the task of summarization, human evaluators prefer generations from both RLAIF and RLHF over a baseline supervised fine-tuned model in ~70% of cases. Furthermore, when asked to rate RLAIF vs. RLHF summaries, humans prefer both at equal rates. These results suggest that RLAIF can yield human-level performance, offering a potential solution to the scalability limitations of RLHF.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20215;&#20540;&#20998;&#24067;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#21518;&#39564;&#20998;&#24067;&#26469;&#35299;&#20915;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#25919;&#31574;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#20248;&#21270;&#31574;&#30053;&#65292;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.06590</link><description>&lt;p&gt;
&#22522;&#20110;&#20215;&#20540;&#20998;&#24067;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Value-Distributional Model-Based Reinforcement Learning. (arXiv:2308.06590v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06590
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20215;&#20540;&#20998;&#24067;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#21518;&#39564;&#20998;&#24067;&#26469;&#35299;&#20915;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#25919;&#31574;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#20248;&#21270;&#31574;&#30053;&#65292;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#65292;&#37327;&#21270;&#25919;&#31574;&#38271;&#26399;&#32489;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#20174;&#22522;&#20110;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#30340;&#35282;&#24230;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#23398;&#20064;&#30001;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#21442;&#25968;&#65288;&#35748;&#30693;&#65289;&#19981;&#30830;&#23450;&#24615;&#24341;&#21457;&#30340;&#20540;&#20989;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#23558;&#20998;&#26512;&#38480;&#21046;&#22312;&#23569;&#25968;&#20998;&#24067;&#20540;&#19978;&#65292;&#25110;&#32773;&#32422;&#26463;&#20998;&#24067;&#24418;&#29366;&#65292;&#20363;&#22914;&#65292;&#39640;&#26031;&#20998;&#24067;&#12290;&#21463;&#21040;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#20010;Bellman&#31639;&#23376;&#65292;&#20854;&#22266;&#23450;&#28857;&#26159;&#20540;&#20998;&#24067;&#20989;&#25968;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Epistemic Quantile-Regression&#65288;EQR&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#19968;&#20010;&#20540;&#20998;&#24067;&#20989;&#25968;&#29992;&#20110;&#31574;&#30053;&#20248;&#21270;&#12290;&#22312;&#20960;&#20010;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#30456;&#23545;&#20110;&#24050;&#26377;&#30340;&#22522;&#20110;&#27169;&#22411;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;EQR&#20855;&#26377;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantifying uncertainty about a policy's long-term performance is important to solve sequential decision-making tasks. We study the problem from a model-based Bayesian reinforcement learning perspective, where the goal is to learn the posterior distribution over value functions induced by parameter (epistemic) uncertainty of the Markov decision process. Previous work restricts the analysis to a few moments of the distribution over values or imposes a particular distribution shape, e.g., Gaussians. Inspired by distributional reinforcement learning, we introduce a Bellman operator whose fixed-point is the value distribution function. Based on our theory, we propose Epistemic Quantile-Regression (EQR), a model-based algorithm that learns a value distribution function that can be used for policy optimization. Evaluation across several continuous-control tasks shows performance benefits with respect to established model-based and model-free algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;RefSAM&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#22312;&#32447;&#26041;&#24335;&#20174;&#19981;&#21516;&#26102;&#38388;&#25139;&#30340;&#22810;&#35270;&#22270;&#20449;&#24687;&#20013;&#21152;&#20837;SAM&#30340;&#28508;&#21147;&#65292;&#25506;&#32034;&#20854;&#22312;&#25351;&#20195;&#35270;&#39057;&#23545;&#35937;&#20998;&#21106;&#65288;RVOS&#65289;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#36328;&#27169;&#24577;MLP&#21644;&#20998;&#23618;&#31264;&#23494;&#27880;&#24847;&#27169;&#22359;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;SAM&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#24418;&#24577;&#30340;&#31934;&#30830;&#29702;&#35299;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.00997</link><description>&lt;p&gt;
RefSAM&#65306;&#39640;&#25928;&#36866;&#24212;&#20219;&#20309;&#27169;&#22411;&#30340;&#25351;&#20195;&#35270;&#39057;&#23545;&#35937;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
RefSAM: Efficiently Adapting Segmenting Anything Model for Referring Video Object Segmentation. (arXiv:2307.00997v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;RefSAM&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#22312;&#32447;&#26041;&#24335;&#20174;&#19981;&#21516;&#26102;&#38388;&#25139;&#30340;&#22810;&#35270;&#22270;&#20449;&#24687;&#20013;&#21152;&#20837;SAM&#30340;&#28508;&#21147;&#65292;&#25506;&#32034;&#20854;&#22312;&#25351;&#20195;&#35270;&#39057;&#23545;&#35937;&#20998;&#21106;&#65288;RVOS&#65289;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#36328;&#27169;&#24577;MLP&#21644;&#20998;&#23618;&#31264;&#23494;&#27880;&#24847;&#27169;&#22359;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;SAM&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#24418;&#24577;&#30340;&#31934;&#30830;&#29702;&#35299;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Segment Anything Model (SAM)&#22240;&#20854;&#22312;&#22270;&#20687;&#20998;&#21106;&#20013;&#20986;&#33394;&#30340;&#24615;&#33021;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#25351;&#20195;&#35270;&#39057;&#23545;&#35937;&#20998;&#21106;&#65288;RVOS&#65289;&#26041;&#38754;&#65292;&#30001;&#20110;&#38656;&#35201;&#31934;&#30830;&#30340;&#29992;&#25143;&#20132;&#20114;&#25552;&#31034;&#20197;&#21450;&#23545;&#35821;&#35328;&#21644;&#35270;&#35273;&#31561;&#19981;&#21516;&#24418;&#24577;&#30340;&#26377;&#38480;&#29702;&#35299;&#33021;&#21147;&#65292;SAM&#32570;&#20047;&#29087;&#32451;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;RefSAM&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#32447;&#26041;&#24335;&#20174;&#19981;&#21516;&#26102;&#38388;&#25139;&#30340;&#22810;&#35270;&#22270;&#20449;&#24687;&#20013;&#21152;&#20837;SAM&#30340;&#28508;&#21147;&#65292;&#25506;&#32034;&#20854;&#22312;RVOS&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#21407;&#22987;SAM&#27169;&#22411;&#36827;&#34892;&#20102;&#36866;&#24212;&#65292;&#36890;&#36807;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;&#36328;&#27169;&#24577;MLP&#23558;&#25351;&#20195;&#34920;&#36798;&#30340;&#25991;&#26412;&#23884;&#20837;&#25237;&#24433;&#20026;&#31232;&#30095;&#21644;&#23494;&#38598;&#23884;&#20837;&#65292;&#20316;&#20026;&#29992;&#25143;&#20132;&#20114;&#25552;&#31034;&#65292;&#20197;&#22686;&#24378;&#36328;&#27169;&#24577;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#20998;&#23618;&#31264;&#23494;&#27880;&#24847;&#27169;&#22359;&#65292;&#20197;&#23558;&#20998;&#23618;&#35270;&#35273;&#35821;&#20041;&#20449;&#24687;&#19982;&#31232;&#30095;&#23884;&#20837;&#34701;&#21512;&#65292;&#20197;&#33719;&#24471;&#32454;&#31890;&#24230;&#30340;&#23494;&#38598;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Segment Anything Model (SAM) has gained significant attention for its impressive performance in image segmentation. However, it lacks proficiency in referring video object segmentation (RVOS) due to the need for precise user-interactive prompts and a limited understanding of different modalities, such as language and vision. This paper presents the RefSAM model, which explores the potential of SAM for RVOS by incorporating multi-view information from diverse modalities and successive frames at different timestamps in an online manner. Our proposed approach adapts the original SAM model to enhance cross-modality learning by employing a lightweight Cross-Modal MLP that projects the text embedding of the referring expression into sparse and dense embeddings, serving as user-interactive prompts. Additionally, we have introduced the hierarchical dense attention module to fuse hierarchical visual semantic information with sparse embeddings in order to obtain fine-grained dense embeddings
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#26131;&#20986;&#38169;&#27979;&#35797;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#27979;&#35797;&#20195;&#30721;&#26469;&#39044;&#27979;&#27979;&#35797;&#30340;&#20462;&#22797;&#31867;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;UniXcoder&#20248;&#20110;CodeBERT&#12290;</title><link>http://arxiv.org/abs/2307.00012</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#40657;&#30418;&#39044;&#27979;&#26131;&#20986;&#38169;&#27979;&#35797;&#20462;&#22797;&#31867;&#21035;
&lt;/p&gt;
&lt;p&gt;
Black-Box Prediction of Flaky Test Fix Categories Using Language Models. (arXiv:2307.00012v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#26131;&#20986;&#38169;&#27979;&#35797;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#27979;&#35797;&#20195;&#30721;&#26469;&#39044;&#27979;&#27979;&#35797;&#30340;&#20462;&#22797;&#31867;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;UniXcoder&#20248;&#20110;CodeBERT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26131;&#20986;&#38169;&#27979;&#35797;&#20250;&#22312;&#30456;&#21516;&#36719;&#20214;&#29256;&#26412;&#30340;&#27979;&#35797;&#19979;&#38750;&#30830;&#23450;&#24615;&#22320;&#36890;&#36807;&#25110;&#22833;&#36133;&#65292;&#24341;&#36215;&#28151;&#20081;&#24182;&#28010;&#36153;&#24320;&#21457;&#32773;&#26102;&#38388;&#12290;&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#34987;&#29992;&#20110;&#39044;&#27979;&#26131;&#20986;&#38169;&#24615;&#21450;&#20854;&#26681;&#26412;&#21407;&#22240;&#65292;&#20294;&#22312;&#25552;&#20379;&#20462;&#22797;&#25903;&#25345;&#26041;&#38754;&#20173;&#26377;&#36739;&#23569;&#24037;&#20316;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#20165;&#20998;&#26512;&#27979;&#35797;&#20195;&#30721;&#33258;&#21160;&#29983;&#25104;13&#20010;&#20462;&#22797;&#31867;&#21035;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#24182;&#35757;&#32451;&#27169;&#22411;&#26469;&#39044;&#27979;&#26131;&#20986;&#38169;&#27979;&#35797;&#30340;&#20462;&#22797;&#31867;&#21035;&#12290;&#34429;&#28982;&#22312;&#24403;&#21069;&#38454;&#27573;&#20934;&#30830;&#39044;&#27979;&#20462;&#22797;&#26412;&#36523;&#26159;&#19981;&#29616;&#23454;&#30340;&#65292;&#20294;&#36825;&#20123;&#31867;&#21035;&#25552;&#20379;&#20102;&#20851;&#20110;&#38656;&#35201;&#26816;&#26597;&#30340;&#27979;&#35797;&#20195;&#30721;&#37096;&#20998;&#30340;&#31934;&#30830;&#25351;&#23548;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#65292;&#21363;CodeBERT&#21644;UniXcoder&#65292;&#20854;&#36755;&#20986;&#32463;&#36807;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;FNN&#65289;&#25110;&#22522;&#20110;&#23402;&#29983;&#32593;&#32476;&#30340;Few Shot Learning&#65288;FSL&#65289;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UniXcoder&#22312;&#27491;&#30830;&#39044;&#27979;&#22823;&#22810;&#25968;&#20462;&#22797;&#31867;&#21035;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;CodeBERT&#12290;
&lt;/p&gt;
&lt;p&gt;
Flaky tests are problematic because they non-deterministically pass or fail for the same software version under test, causing confusion and wasting developer time. While machine learning models have been used to predict flakiness and its root causes, there is less work on providing support to fix the problem. To address this gap, we propose a framework that automatically generates labeled datasets for 13 fix categories and train models to predict the fix category of a flaky test by analyzing the test code only. Though it is unrealistic at this stage to accurately predict the fix itself, the categories provide precise guidance about what part of the test code to look at. Our approach is based on language models, namely CodeBERT and UniXcoder, whose output is fine-tuned with a Feed Forward Neural Network (FNN) or a Siamese Network-based Few Shot Learning (FSL). Our experimental results show that UniXcoder outperforms CodeBERT, in correctly predicting most of the categories of fixes a dev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36127;&#33655;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#37319;&#29992;Seq2Seq&#32593;&#32476;&#32467;&#26500;&#26469;&#20998;&#31163;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#22788;&#29702;&#24322;&#24120;&#24773;&#20917;&#65292;&#19981;&#20165;&#30528;&#30524;&#20110;&#39044;&#27979;&#26465;&#20214;&#26399;&#26395;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.01001</link><description>&lt;p&gt;
DiffLoad:&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36127;&#33655;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
DiffLoad: Uncertainty Quantification in Load Forecasting with Diffusion Model. (arXiv:2306.01001v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36127;&#33655;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#37319;&#29992;Seq2Seq&#32593;&#32476;&#32467;&#26500;&#26469;&#20998;&#31163;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#22788;&#29702;&#24322;&#24120;&#24773;&#20917;&#65292;&#19981;&#20165;&#30528;&#30524;&#20110;&#39044;&#27979;&#26465;&#20214;&#26399;&#26395;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#23545;&#30005;&#21147;&#31995;&#32479;&#30340;&#20915;&#31574;&#21046;&#23450;&#65292;&#22914;&#26426;&#32452;&#25237;&#20837;&#21644;&#33021;&#28304;&#31649;&#29702;&#31561;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36817;&#24180;&#26469;&#65292;&#21508;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#25429;&#25417;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#39640;&#26031;&#20284;&#28982;&#26041;&#27861;&#30340;&#65292;&#23427;&#26088;&#22312;&#22312;&#32473;&#23450;&#30340;&#21327;&#21464;&#37327;&#19979;&#20934;&#30830;&#20272;&#35745;&#20998;&#24067;&#26399;&#26395;&#20540;&#12290;&#36825;&#31181;&#26041;&#27861;&#24456;&#38590;&#36866;&#24212;&#23384;&#22312;&#20998;&#24067;&#20559;&#31227;&#21644;&#24322;&#24120;&#20540;&#30340;&#26102;&#38388;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;Seq2seq&#32467;&#26500;&#26469;&#20272;&#35745;&#26412;&#20307;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;&#40065;&#26834;&#30340;&#21152;&#24615;&#26607;&#35199;&#20998;&#24067;&#26469;&#20272;&#35745;&#29289;&#35937;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20998;&#31163;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#22788;&#29702;&#31361;&#21464;&#24773;&#20917;&#65292;&#32780;&#19981;&#26159;&#20934;&#30830;&#39044;&#27979;&#26465;&#20214;&#26399;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrical load forecasting is of great significance for the decision makings in power systems, such as unit commitment and energy management. In recent years, various self-supervised neural network-based methods have been applied to electrical load forecasting to improve forecasting accuracy and capture uncertainties. However, most current methods are based on Gaussian likelihood methods, which aim to accurately estimate the distribution expectation under a given covariate. This kind of approach is difficult to adapt to situations where temporal data has a distribution shift and outliers. In this paper, we propose a diffusion-based Seq2seq structure to estimate epistemic uncertainty and use the robust additive Cauchy distribution to estimate aleatoric uncertainty. Rather than accurately forecasting conditional expectations, we demonstrate our method's ability in separating two types of uncertainties and dealing with the mutant scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;Di-Long&#65292;&#29992;&#20110;&#35299;&#20915;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#20013;&#36234;&#26469;&#36234;&#19981;&#30830;&#23450;&#21644;&#19981;&#21487;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#33976;&#39311;&#30701;&#26399;&#36712;&#36857;&#27169;&#22411;&#39044;&#27979;&#22120;&#26469;&#25351;&#23548;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#23398;&#29983;&#32593;&#32476;&#12290;&#23398;&#29983;&#32593;&#32476;&#35266;&#23519;&#30701;&#24207;&#21015;&#24182;&#39044;&#27979;&#38271;&#36712;&#36857;&#65292;&#25945;&#24072;&#32593;&#32476;&#35266;&#23519;&#26356;&#38271;&#24207;&#21015;&#24182;&#39044;&#27979;&#21097;&#20313;&#30701;&#30446;&#26631;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2305.08553</link><description>&lt;p&gt;
&#23558;&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;&#30701;&#26399;&#21040;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Distilling Knowledge for Short-to-Long Term Trajectory Prediction. (arXiv:2305.08553v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;Di-Long&#65292;&#29992;&#20110;&#35299;&#20915;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#20013;&#36234;&#26469;&#36234;&#19981;&#30830;&#23450;&#21644;&#19981;&#21487;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#33976;&#39311;&#30701;&#26399;&#36712;&#36857;&#27169;&#22411;&#39044;&#27979;&#22120;&#26469;&#25351;&#23548;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#23398;&#29983;&#32593;&#32476;&#12290;&#23398;&#29983;&#32593;&#32476;&#35266;&#23519;&#30701;&#24207;&#21015;&#24182;&#39044;&#27979;&#38271;&#36712;&#36857;&#65292;&#25945;&#24072;&#32593;&#32476;&#35266;&#23519;&#26356;&#38271;&#24207;&#21015;&#24182;&#39044;&#27979;&#21097;&#20313;&#30701;&#30446;&#26631;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20854;&#20013;&#19968;&#20010;&#22522;&#26412;&#22256;&#38590;&#22312;&#20110;&#38543;&#30528;&#26102;&#38388;&#33539;&#22260;&#30340;&#22686;&#38271;&#65292;&#36712;&#36857;&#30340;&#28436;&#21464;&#21464;&#24471;&#36234;&#26469;&#36234;&#19981;&#30830;&#23450;&#21644;&#19981;&#21487;&#39044;&#27979;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Di-Long&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#33976;&#39311;&#30701;&#26399;&#36712;&#36857;&#27169;&#22411;&#39044;&#27979;&#22120;&#26469;&#25351;&#23548;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38271;&#26399;&#36712;&#36857;&#39044;&#27979;&#23398;&#29983;&#32593;&#32476;&#12290;&#32473;&#23450;&#19968;&#20010;&#21253;&#21547;&#23398;&#29983;&#32593;&#32476;&#20801;&#35768;&#30340;&#35266;&#27979;&#24207;&#21015;&#21644;&#34917;&#20805;&#30446;&#26631;&#24207;&#21015;&#30340;&#24635;&#24207;&#21015;&#38271;&#24230;&#65292;&#25105;&#20204;&#35753;&#23398;&#29983;&#21644;&#25945;&#24072;&#23545;&#21516;&#19968;&#20010;&#23436;&#25972;&#36712;&#36857;&#23450;&#20041;&#20004;&#20010;&#19981;&#21516;&#20294;&#30456;&#20851;&#30340;&#20219;&#21153;&#65306;&#23398;&#29983;&#35266;&#23519;&#19968;&#20010;&#30701;&#24207;&#21015;&#24182;&#39044;&#27979;&#19968;&#20010;&#38271;&#36712;&#36857;&#65292;&#32780;&#25945;&#24072;&#35266;&#23519;&#19968;&#20010;&#26356;&#38271;&#30340;&#24207;&#21015;&#24182;&#39044;&#27979;&#21097;&#19979;&#30340;&#30701;&#30446;&#26631;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-term trajectory forecasting is an important and challenging problem in the fields of computer vision, machine learning, and robotics. One fundamental difficulty stands in the evolution of the trajectory that becomes more and more uncertain and unpredictable as the time horizon grows, subsequently increasing the complexity of the problem. To overcome this issue, in this paper, we propose Di-Long, a new method that employs the distillation of a short-term trajectory model forecaster that guides a student network for long-term trajectory prediction during the training process. Given a total sequence length that comprehends the allowed observation for the student network and the complementary target sequence, we let the student and the teacher solve two different related tasks defined over the same full trajectory: the student observes a short sequence and predicts a long trajectory, whereas the teacher observes a longer sequence and predicts the remaining short target trajectory. The
&lt;/p&gt;</description></item><item><title>Diffusion Explainer&#26159;&#31532;&#19968;&#20010;&#21487;&#20132;&#20114;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#35299;&#37322;&#31283;&#23450;&#25193;&#25955;&#22914;&#20309;&#23558;&#25991;&#26412;&#25552;&#31034;&#36716;&#21270;&#20026;&#22270;&#20687;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#21160;&#30011;&#21644;&#20132;&#20114;&#20803;&#32032;&#27969;&#30021;&#22320;&#22312;&#22810;&#20010;&#25277;&#35937;&#32423;&#21035;&#20043;&#38388;&#36807;&#28193;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#25552;&#31034;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.03509</link><description>&lt;p&gt;
Diffusion Explainer&#65306;&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#31283;&#23450;&#25193;&#25955;&#30340;&#21487;&#35270;&#21270;&#35299;&#37322;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion. (arXiv:2305.03509v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03509
&lt;/p&gt;
&lt;p&gt;
Diffusion Explainer&#26159;&#31532;&#19968;&#20010;&#21487;&#20132;&#20114;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#35299;&#37322;&#31283;&#23450;&#25193;&#25955;&#22914;&#20309;&#23558;&#25991;&#26412;&#25552;&#31034;&#36716;&#21270;&#20026;&#22270;&#20687;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#21160;&#30011;&#21644;&#20132;&#20114;&#20803;&#32032;&#27969;&#30021;&#22320;&#22312;&#22810;&#20010;&#25277;&#35937;&#32423;&#21035;&#20043;&#38388;&#36807;&#28193;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#25552;&#31034;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#36890;&#36807;&#21019;&#36896;&#36924;&#30495;&#30340;&#22270;&#20687;&#32780;&#33719;&#24471;&#20102;&#20840;&#29699;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22797;&#26434;&#30340;&#20869;&#37096;&#32467;&#26500;&#21644;&#25805;&#20316;&#24448;&#24448;&#20351;&#24471;&#38750;&#19987;&#19994;&#20154;&#21592;&#38590;&#20197;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; Diffusion Explainer&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#35299;&#37322;&#31283;&#23450;&#25193;&#25955;&#22914;&#20309;&#23558;&#25991;&#26412;&#25552;&#31034;&#36716;&#21270;&#20026;&#22270;&#20687;&#12290;Diffusion Explainer&#32039;&#23494;&#22320;&#23558;&#31283;&#23450;&#25193;&#25955;&#30340;&#22797;&#26434;&#32452;&#20214;&#30340;&#35270;&#35273;&#27010;&#36848;&#19982;&#20854;&#28508;&#22312;&#25805;&#20316;&#30340;&#35814;&#32454;&#35828;&#26126;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#21160;&#30011;&#21644;&#20132;&#20114;&#20803;&#32032;&#20351;&#29992;&#25143;&#21487;&#20197;&#27969;&#30021;&#22320;&#22312;&#22810;&#20010;&#25277;&#35937;&#32423;&#21035;&#20043;&#38388;&#36807;&#28193;&#12290;&#36890;&#36807;&#27604;&#36739;&#30001;&#20004;&#20010;&#30456;&#20851;&#25991;&#26412;&#25552;&#31034;&#24341;&#23548;&#30340;&#22270;&#20687;&#34920;&#31034;&#30340;&#28436;&#21464;&#26469;&#25351;&#23548;&#31934;&#32454;&#26102;&#38388;&#27493;&#38271;&#65292;&#29992;&#25143;&#21487;&#20197;&#21457;&#29616;&#25552;&#31034;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#24433;&#21709;&#12290;Diffusion Explainer&#22312;&#29992;&#25143;&#30340;Web&#27983;&#35272;&#22120;&#20013;&#26412;&#22320;&#36816;&#34892;&#65292;&#26080;&#38656;&#23433;&#35013;&#25110;&#19987;&#38376;&#30340;&#30828;&#20214;&#65292;&#25193;&#22823;&#20102;&#20844;&#20247;&#23545;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#25945;&#32946;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based generative models' impressive ability to create convincing images has captured global attention. However, their complex internal structures and operations often make them difficult for non-experts to understand. We present Diffusion Explainer, the first interactive visualization tool that explains how Stable Diffusion transforms text prompts into images. Diffusion Explainer tightly integrates a visual overview of Stable Diffusion's complex components with detailed explanations of their underlying operations, enabling users to fluidly transition between multiple levels of abstraction through animations and interactive elements. By comparing the evolutions of image representations guided by two related text prompts over refinement timesteps, users can discover the impact of prompts on image generation. Diffusion Explainer runs locally in users' web browsers without the need for installation or specialized hardware, broadening the public's education access to modern AI tec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#36890;&#36807;&#22686;&#21152;&#28145;&#24230;&#20811;&#26381;&#22240;&#37319;&#29992;&#20302;&#32500;&#23454;&#20307;&#34920;&#31034;&#32780;&#23548;&#33268;&#30340;&#27169;&#22411;&#31934;&#24230;&#19979;&#38477;&#21644;&#27169;&#22411;&#21442;&#25968;&#20943;&#23569;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12816</link><description>&lt;p&gt;
&#20174;&#23485;&#21040;&#28145;&#65306;&#32500;&#24230;&#25552;&#21319;&#32593;&#32476;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
From Wide to Deep: Dimension Lifting Network for Parameter-efficient Knowledge Graph Embedding. (arXiv:2303.12816v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#36890;&#36807;&#22686;&#21152;&#28145;&#24230;&#20811;&#26381;&#22240;&#37319;&#29992;&#20302;&#32500;&#23454;&#20307;&#34920;&#31034;&#32780;&#23548;&#33268;&#30340;&#27169;&#22411;&#31934;&#24230;&#19979;&#38477;&#21644;&#27169;&#22411;&#21442;&#25968;&#20943;&#23569;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGE&#65289;&#23558;&#23454;&#20307;&#21644;&#20851;&#31995;&#26144;&#23556;&#21040;&#21521;&#37327;&#34920;&#31034;&#23545;&#20110;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;KGE&#26041;&#27861;&#38656;&#35201;&#30456;&#23545;&#39640;&#32500;&#30340;&#23454;&#20307;&#34920;&#31034;&#26469;&#20445;&#30041;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#20294;&#20250;&#23548;&#33268;&#24222;&#22823;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#36890;&#36807;&#37319;&#29992;&#20302;&#32500;&#23454;&#20307;&#34920;&#31034;&#26469;&#38477;&#20302;&#27169;&#22411;&#21442;&#25968;&#65292;&#21516;&#26102;&#24320;&#21457;&#25216;&#26415;&#65288;&#20363;&#22914;&#30693;&#35782;&#33976;&#39311;&#65289;&#26469;&#34917;&#20607;&#38477;&#32500;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#25805;&#20316;&#20250;&#23548;&#33268;&#27169;&#22411;&#31934;&#24230;&#19979;&#38477;&#21644;&#27169;&#22411;&#21442;&#25968;&#20943;&#23569;&#26377;&#38480;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#25152;&#26377;&#23454;&#20307;&#34920;&#31034;&#30340;&#32423;&#32852;&#35270;&#20026;&#23884;&#20837;&#23618;&#65292;&#37027;&#20040;&#37319;&#29992;&#39640;&#32500;&#23454;&#20307;&#34920;&#31034;&#30340;&#20256;&#32479;KGE&#26041;&#27861;&#31561;&#21516;&#20110;&#25193;&#23637;&#23884;&#20837;&#23618;&#30340;&#23485;&#24230;&#20197;&#33719;&#24471;&#34920;&#29616;&#21147;&#12290;&#20026;&#20102;&#22312;&#19981;&#29306;&#29298;&#20934;&#30830;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#21442;&#25968;&#25928;&#29575;&#65292;&#25105;&#20204;&#30456;&#21453;&#22320;&#22686;&#21152;&#28145;&#24230;&#65292;&#24182;&#25552;&#20986;&#19968;&#20010;&#26356;&#28145;&#30340;&#23454;&#20307;&#23884;&#20837;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embedding (KGE) that maps entities and relations into vector representations is essential for downstream tasks. Conventional KGE methods require relatively high-dimensional entity representations to preserve the structural information of knowledge graph, but lead to oversized model parameters. Recent methods reduce model parameters by adopting low-dimensional entity representations, while developing techniques (e.g., knowledge distillation) to compensate for the reduced dimension. However, such operations produce degraded model accuracy and limited reduction of model parameters. Specifically, we view the concatenation of all entity representations as an embedding layer, and then conventional KGE methods that adopt high-dimensional entity representations equal to enlarging the width of the embedding layer to gain expressiveness. To achieve parameter efficiency without sacrificing accuracy, we instead increase the depth and propose a deeper embedding network for entity re
&lt;/p&gt;</description></item></channel></rss>