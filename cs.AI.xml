<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;ModTr&#26041;&#27861;&#65292;&#36890;&#36807;&#23567;&#22411;&#36716;&#25442;&#32593;&#32476;&#35843;&#25972;&#36755;&#20837;&#20197;&#26368;&#23567;&#21270;&#26816;&#27979;&#25439;&#22833;&#65292;&#23454;&#29616;&#20102;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#20174;&#19968;&#20010;&#25110;&#22810;&#20010;&#27169;&#24577;&#21040;&#21478;&#19968;&#20010;&#30340;&#26377;&#25928;&#36866;&#24212;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2404.01492</link><description>&lt;p&gt;
&#19981;&#36951;&#24536;&#20808;&#39564;&#30693;&#35782;&#30340;&#30446;&#26631;&#26816;&#27979;&#36866;&#24212;&#27169;&#24577;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Modality Translation for Object Detection Adaptation Without Forgetting Prior Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;ModTr&#26041;&#27861;&#65292;&#36890;&#36807;&#23567;&#22411;&#36716;&#25442;&#32593;&#32476;&#35843;&#25972;&#36755;&#20837;&#20197;&#26368;&#23567;&#21270;&#26816;&#27979;&#25439;&#22833;&#65292;&#23454;&#29616;&#20102;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#20174;&#19968;&#20010;&#25110;&#22810;&#20010;&#27169;&#24577;&#21040;&#21478;&#19968;&#20010;&#30340;&#26377;&#25928;&#36866;&#24212;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#24120;&#35265;&#30340;&#20570;&#27861;&#26159;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#20934;&#30830;&#25191;&#34892;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#21482;&#36866;&#29992;&#20110;&#36328;&#27169;&#24577;&#65292;&#22240;&#20026;&#20351;&#29992;&#19981;&#21516;&#20256;&#24863;&#22120;&#25429;&#33719;&#30340;&#25968;&#25454;&#23384;&#22312;&#26356;&#22823;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#26412;&#25991;&#19987;&#27880;&#20110;&#23558;&#22823;&#22411;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#35843;&#25972;&#21040;&#19968;&#20010;&#25110;&#22810;&#20010;&#27169;&#24577;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#25928;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ModTr&#20316;&#20026;&#26222;&#36941;&#20570;&#27861;&#24494;&#35843;&#22823;&#22411;&#27169;&#22411;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;ModTr&#21253;&#25324;&#20351;&#29992;&#19968;&#20010;&#23567;&#22411;&#36716;&#25442;&#32593;&#32476;&#35843;&#25972;&#36755;&#20837;&#65292;&#35813;&#32593;&#32476;&#32463;&#36807;&#35757;&#32451;&#65292;&#30452;&#25509;&#20351;&#26816;&#27979;&#25439;&#22833;&#26368;&#23567;&#21270;&#12290;&#22240;&#27492;&#65292;&#21407;&#22987;&#27169;&#22411;&#21487;&#20197;&#22312;&#36716;&#25442;&#21518;&#30340;&#36755;&#20837;&#19978;&#24037;&#20316;&#65292;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#36827;&#19968;&#27493;&#30340;&#26356;&#25913;&#25110;&#21442;&#25968;&#24494;&#35843;&#12290;&#23545;&#20004;&#20010;&#30693;&#21517;&#25968;&#25454;&#38598;&#19978;&#20174;&#32418;&#22806;&#21040;RGB&#22270;&#20687;&#30340;&#36716;&#25442;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#31616;&#21333;&#30340;ModTr&#26041;&#27861;&#25552;&#20379;&#20102;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01492v1 Announce Type: cross  Abstract: A common practice in deep learning consists of training large neural networks on massive datasets to perform accurately for different domains and tasks. While this methodology may work well in numerous application areas, it only applies across modalities due to a larger distribution shift in data captured using different sensors. This paper focuses on the problem of adapting a large object detection model to one or multiple modalities while being efficient. To do so, we propose ModTr as an alternative to the common approach of fine-tuning large models. ModTr consists of adapting the input with a small transformation network trained to minimize the detection loss directly. The original model can therefore work on the translated inputs without any further change or fine-tuning to its parameters. Experimental results on translating from IR to RGB images on two well-known datasets show that this simple ModTr approach provides detectors tha
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#32479;&#35745;&#30456;&#20851;&#24615;&#21644;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#25913;&#21892;&#20102;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#22312;&#30140;&#30171;&#35782;&#21035;&#20013;&#30340;&#24615;&#33021;&#65292;&#31361;&#20986;&#20102;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#23450;&#21046;&#21270;&#30340;&#27169;&#24577;&#20998;&#21106;&#23545;&#30140;&#30171;&#34892;&#20026;&#20998;&#26512;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00320</link><description>&lt;p&gt;
&#22312;&#30140;&#30171;&#35782;&#21035;&#20013;&#25512;&#36827;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#65306;&#21033;&#29992;&#32479;&#35745;&#30456;&#20851;&#24615;&#21644;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Advancing Multimodal Data Fusion in Pain Recognition: A Strategy Leveraging Statistical Correlation and Human-Centered Perspectives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00320
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#32479;&#35745;&#30456;&#20851;&#24615;&#21644;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#25913;&#21892;&#20102;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#22312;&#30140;&#30171;&#35782;&#21035;&#20013;&#30340;&#24615;&#33021;&#65292;&#31361;&#20986;&#20102;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#23450;&#21046;&#21270;&#30340;&#27169;&#24577;&#20998;&#21106;&#23545;&#30140;&#30171;&#34892;&#20026;&#20998;&#26512;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#33268;&#21147;&#20110;&#35299;&#20915;&#22312;&#30140;&#30171;&#35782;&#21035;&#39046;&#22495;&#20869;&#25972;&#21512;&#24322;&#26500;&#25968;&#25454;&#20197;&#36827;&#34892;&#29305;&#23450;&#34892;&#20026;&#35782;&#21035;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#32479;&#35745;&#30456;&#20851;&#24615;&#19982;&#20197;&#20154;&#20026;&#20013;&#24515;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#21508;&#31181;&#21508;&#26679;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#25105;&#20204;&#31361;&#20986;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25552;&#21319;&#27169;&#22411;&#22312;&#21508;&#31181;&#22797;&#26434;&#22330;&#26223;&#20013;&#34920;&#29616;&#30340;&#36866;&#24212;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26032;&#39062;&#20043;&#22788;&#22312;&#20110;&#25112;&#30053;&#24615;&#22320;&#32467;&#21512;&#20102;&#32479;&#35745;&#30456;&#20851;&#24615;&#26435;&#37325;&#65292;&#24182;&#20174;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35282;&#24230;&#23545;&#27169;&#24577;&#36827;&#34892;&#20102;&#20998;&#21106;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#31934;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#21487;&#35299;&#37322;&#24615;&#30340;&#20998;&#26512;&#12290;&#35813;&#30740;&#31350;&#36890;&#36807;&#24378;&#35843;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#23450;&#21046;&#21270;&#30340;&#27169;&#24577;&#20998;&#21106;&#22312;&#22686;&#24378;&#30140;&#30171;&#34892;&#20026;&#20998;&#26512;&#20013;&#30340;&#20316;&#29992;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#27169;&#24577;&#34701;&#21512;&#25216;&#26415;&#12290;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#27599;&#31181;&#27169;&#24577;&#19982;&#36866;&#21512;&#30340;&#20998;&#31867;&#22120;&#36827;&#34892;&#21305;&#37197;&#65292;&#22522;&#20110;&#32479;&#35745;&#30456;&#20851;&#24615;&#65292;&#22686;&#21152;&#20102;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#35299;&#37322;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00320v1 Announce Type: new  Abstract: This research tackles the challenge of integrating heterogeneous data for specific behavior recognition within the domain of Pain Recognition, presenting a novel methodology that harmonizes statistical correlations with a human-centered approach. By leveraging a diverse range of deep learning architectures, we highlight the adaptability and efficacy of our approach in improving model performance across various complex scenarios. The novelty of our methodology is the strategic incorporation of statistical relevance weights and the segmentation of modalities from a human-centric perspective, enhancing model precision and providing a explainable analysis of multimodal data. This study surpasses traditional modality fusion techniques by underscoring the role of data diversity and customized modality segmentation in enhancing pain behavior analysis. Introducing a framework that matches each modality with an suited classifier, based on the sta
&lt;/p&gt;</description></item><item><title>AI&#20174;&#19994;&#32773;&#23545;&#20110;&#20844;&#24179;AI/ML&#30340;&#29702;&#35299;&#12289;&#38754;&#20020;&#30340;&#25361;&#25112;&#12289;&#19981;&#20844;&#24179;AI/ML&#30340;&#21518;&#26524;&#20197;&#21450;&#30830;&#20445;AI/ML&#20844;&#24179;&#24615;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.15481</link><description>&lt;p&gt;
AI/ML &#21457;&#23637;&#20013;&#30340;&#20844;&#24179;&#23548;&#33322;: &#20174;&#19994;&#32773;&#23545;AI/ML&#24320;&#21457;&#20013;&#30340;&#29702;&#35299;&#12289;&#25361;&#25112;&#21644;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Navigating Fairness: Practitioners' Understanding, Challenges, and Strategies in AI/ML Development
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15481
&lt;/p&gt;
&lt;p&gt;
AI&#20174;&#19994;&#32773;&#23545;&#20110;&#20844;&#24179;AI/ML&#30340;&#29702;&#35299;&#12289;&#38754;&#20020;&#30340;&#25361;&#25112;&#12289;&#19981;&#20844;&#24179;AI/ML&#30340;&#21518;&#26524;&#20197;&#21450;&#30830;&#20445;AI/ML&#20844;&#24179;&#24615;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21508;&#34892;&#19994;&#23545;AI/ML&#24212;&#29992;&#30340;&#22686;&#21152;&#24341;&#21457;&#20102;&#23545;AI/ML&#20844;&#24179;&#24615;&#30340;&#26356;&#22810;&#35752;&#35770;&#12290;&#34429;&#28982;&#24050;&#26377;&#20851;&#20110;AI/ML&#20844;&#24179;&#24615;&#30340;&#20808;&#21069;&#30740;&#31350;&#65292;&#20294;&#32570;&#20047;&#38024;&#23545;&#20102;&#35299;AI&#20174;&#19994;&#32773;&#22312;&#24320;&#21457;&#20844;&#24179;AI/ML&#36807;&#31243;&#20013;&#30340;&#35266;&#28857;&#21644;&#32463;&#39564;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#20102;&#35299;AI&#20174;&#19994;&#32773;&#23545;AI/ML&#20844;&#24179;&#24615;&#30340;&#30475;&#27861;&#21644;&#32463;&#39564;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;&#20182;&#20204;&#30452;&#25509;&#21442;&#19982;&#20854;&#20013;&#30340;&#24320;&#21457;&#21644;&#37096;&#32626;&#65292;&#20182;&#20204;&#30340;&#35265;&#35299;&#21487;&#20197;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#29616;&#23454;&#19990;&#30028;&#35270;&#35282;&#65292;&#24110;&#21161;&#29702;&#35299;&#30830;&#20445;AI/ML&#20844;&#24179;&#24615;&#25152;&#28041;&#21450;&#25361;&#25112;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;22&#20301;AI&#20174;&#19994;&#32773;&#30340;&#21322;&#32467;&#26500;&#21270;&#35775;&#35848;&#65292;&#20197;&#35843;&#26597;&#20182;&#20204;&#23545;&#8220;&#20844;&#24179;AI/ML&#8221;&#26159;&#20160;&#20040;&#30340;&#29702;&#35299;&#65292;&#20182;&#20204;&#22312;&#24320;&#21457;&#20844;&#24179;AI/ML&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24320;&#21457;&#19981;&#20844;&#24179;AI/ML&#30340;&#21518;&#26524;&#65292;&#20197;&#21450;&#20182;&#20204;&#37319;&#21462;&#30340;&#31574;&#30053;&#26469;&#30830;&#20445;AI/ML&#30340;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#26694;&#26550;&#23637;&#31034;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15481v1 Announce Type: cross  Abstract: The rise in the use of AI/ML applications across industries has sparked more discussions about the fairness of AI/ML in recent times. While prior research on the fairness of AI/ML exists, there is a lack of empirical studies focused on understanding the views and experiences of AI practitioners in developing a fair AI/ML. Understanding AI practitioners' views and experiences on the fairness of AI/ML is important because they are directly involved in its development and deployment and their insights can offer valuable real-world perspectives on the challenges associated with ensuring fairness in AI/ML. We conducted semi-structured interviews with 22 AI practitioners to investigate their understanding of what a 'fair AI/ML' is, the challenges they face in developing a fair AI/ML, the consequences of developing an unfair AI/ML, and the strategies they employ to ensure AI/ML fairness. We developed a framework showcasing the relationship be
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#26377;&#26395;&#25552;&#20379;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#24212;&#27880;&#24847;&#20854;&#24212;&#29992;&#21487;&#33021;&#24102;&#26469;&#30340;&#39118;&#38505;&#65292;&#24182;&#31215;&#26497;&#37319;&#21462;&#31574;&#30053;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2403.14814</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#26426;&#20250;&#21644;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
The opportunities and risks of large language models in mental health
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14814
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#26377;&#26395;&#25552;&#20379;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#24212;&#27880;&#24847;&#20854;&#24212;&#29992;&#21487;&#33021;&#24102;&#26469;&#30340;&#39118;&#38505;&#65292;&#24182;&#31215;&#26497;&#37319;&#21462;&#31574;&#30053;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#30340;&#21457;&#29983;&#29575;&#27491;&#22312;&#19978;&#21319;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#24847;&#35782;&#21040;&#29616;&#26377;&#30340;&#24515;&#29702;&#20445;&#20581;&#27169;&#24335;&#26080;&#27861;&#20805;&#20998;&#25193;&#23637;&#20197;&#28385;&#36275;&#38656;&#27714;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#20154;&#20204;&#23545;&#23427;&#20204;&#20855;&#26377;&#21019;&#36896;&#26032;&#39062;&#12289;&#22823;&#35268;&#27169;&#35299;&#20915;&#26041;&#26696;&#20197;&#25903;&#25345;&#24515;&#29702;&#20581;&#24247;&#30340;&#25215;&#35834;&#24863;&#21040;&#20048;&#35266;&#12290;&#23613;&#31649;&#23427;&#20204;&#36824;&#22788;&#20110;&#21021;&#26399;&#38454;&#27573;&#65292;LLMs&#24050;&#34987;&#24212;&#29992;&#20110;&#19982;&#24515;&#29702;&#20581;&#24247;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#26412;&#32508;&#36848;&#24635;&#32467;&#20102;&#24050;&#26377;&#25991;&#29486;&#20013;&#20851;&#20110;&#21033;&#29992;LLMs&#25552;&#20379;&#24515;&#29702;&#20581;&#24247;&#25945;&#32946;&#12289;&#35780;&#20272;&#21644;&#24178;&#39044;&#30340;&#21162;&#21147;&#65292;&#24182;&#31361;&#20986;&#20102;&#27599;&#20010;&#39046;&#22495;&#20013;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#30340;&#20851;&#38190;&#26426;&#20250;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#23558;LLMs&#24212;&#29992;&#20110;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#25152;&#20276;&#38543;&#30340;&#39118;&#38505;&#65292;&#24182;&#40723;&#21169;&#37319;&#29992;&#31574;&#30053;&#26469;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;&#23545;&#20110;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#30340;&#36843;&#20999;&#38656;&#27714;&#24517;&#39035;&#19982;&#36127;&#36131;&#20219;&#30340;&#24515;&#29702;&#20581;&#24247;LLMs&#30340;&#24320;&#21457;&#12289;&#27979;&#35797;&#21644;&#37096;&#32626;&#30456;&#24179;&#34913;&#12290;&#29305;&#21035;&#20851;&#38190;&#30340;&#26159;&#30830;&#20445;&#24515;&#29702;&#20581;&#24247;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14814v1 Announce Type: cross  Abstract: Global rates of mental health concerns are rising and there is increasing realization that existing models of mental healthcare will not adequately expand to meet the demand. With the emergence of large language models (LLMs) has come great optimism regarding their promise to create novel, large-scale solutions to support mental health. Despite their nascence, LLMs have already been applied to mental health-related tasks. In this review, we summarize the extant literature on efforts to use LLMs to provide mental health education, assessment, and intervention and highlight key opportunities for positive impact in each area. We then highlight risks associated with LLMs application to mental health and encourage adoption of strategies to mitigate these risks. The urgent need for mental health support must be balanced with responsible development, testing, and deployment of mental health LLMs. Especially critical is ensuring that mental he
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;(&#20154;&#31867;+AI)&#26041;&#27861;HyEnA&#65292;&#29992;&#20110;&#20174;&#24847;&#35265;&#25991;&#26412;&#20013;&#25552;&#21462;&#35770;&#28857;&#65292;&#32467;&#21512;&#20102;&#33258;&#21160;&#21270;&#22788;&#29702;&#36895;&#24230;&#21644;&#20154;&#31867;&#29702;&#35299;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#20844;&#27665;&#21453;&#39304;&#35821;&#26009;&#24211;&#19978;&#21462;&#24471;&#20102;&#26356;&#39640;&#30340;&#35206;&#30422;&#29575;&#21644;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.09713</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#35770;&#35777;&#25366;&#25496;&#30340;&#28151;&#21512;&#26234;&#33021;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Hybrid Intelligence Method for Argument Mining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09713
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;(&#20154;&#31867;+AI)&#26041;&#27861;HyEnA&#65292;&#29992;&#20110;&#20174;&#24847;&#35265;&#25991;&#26412;&#20013;&#25552;&#21462;&#35770;&#28857;&#65292;&#32467;&#21512;&#20102;&#33258;&#21160;&#21270;&#22788;&#29702;&#36895;&#24230;&#21644;&#20154;&#31867;&#29702;&#35299;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#20844;&#27665;&#21453;&#39304;&#35821;&#26009;&#24211;&#19978;&#21462;&#24471;&#20102;&#26356;&#39640;&#30340;&#35206;&#30422;&#29575;&#21644;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35843;&#26597;&#24037;&#20855;&#33021;&#22815;&#25910;&#38598;&#20844;&#27665;&#21453;&#39304;&#24847;&#35265;&#35821;&#26009;&#24211;&#12290;&#20174;&#24222;&#22823;&#19988;&#22024;&#26434;&#30340;&#24847;&#35265;&#38598;&#20013;&#25552;&#21462;&#20851;&#38190;&#35770;&#28857;&#26377;&#21161;&#20110;&#24555;&#36895;&#20934;&#30830;&#22320;&#29702;&#35299;&#24847;&#35265;&#12290;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#21462;&#35770;&#28857;&#65292;&#20294;(1)&#38656;&#35201;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#23548;&#33268;&#36739;&#39640;&#30340;&#27880;&#37322;&#25104;&#26412;; (2)&#23545;&#24050;&#30693;&#35266;&#28857;&#25928;&#26524;&#33391;&#22909;&#65292;&#20294;&#23545;&#26032;&#39062;&#35266;&#28857;&#25928;&#26524;&#27424;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;HyEnA&#65292;&#19968;&#31181;&#28151;&#21512;(&#20154;&#31867;+AI)&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#20027;&#35266;&#25991;&#26412;&#20013;&#25552;&#21462;&#35770;&#28857;&#65292;&#32467;&#21512;&#20102;&#33258;&#21160;&#21270;&#22788;&#29702;&#30340;&#36895;&#24230;&#21644;&#20154;&#31867;&#30340;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20844;&#27665;&#21453;&#39304;&#35821;&#26009;&#24211;&#19978;&#35780;&#20272;&#20102;HyEnA&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19968;&#26041;&#38754;&#65292;&#19982;&#19968;&#32452;&#21508;&#31181;&#24847;&#35265;&#36827;&#34892;&#27604;&#36739;&#26102;&#65292;HyEnA&#22312;&#39640;&#35206;&#30422;&#29575;&#21644;&#20934;&#30830;&#29575;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#35777;&#23454;&#20102;&#20154;&#31867;&#27934;&#23519;&#30340;&#24517;&#35201;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;HyEnA&#38656;&#35201;&#36739;&#23569;&#30340;&#20154;&#21147;&#24037;&#20316;&#37327;&#65292;&#19988;&#19981;&#20250;&#29306;&#29298;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09713v1 Announce Type: new  Abstract: Large-scale survey tools enable the collection of citizen feedback in opinion corpora. Extracting the key arguments from a large and noisy set of opinions helps in understanding the opinions quickly and accurately. Fully automated methods can extract arguments but (1) require large labeled datasets that induce large annotation costs and (2) work well for known viewpoints, but not for novel points of view. We propose HyEnA, a hybrid (human + AI) method for extracting arguments from opinionated texts, combining the speed of automated processing with the understanding and reasoning capabilities of humans. We evaluate HyEnA on three citizen feedback corpora. We find that, on the one hand, HyEnA achieves higher coverage and precision than a state-of-the-art automated method when compared to a common set of diverse opinions, justifying the need for human insight. On the other hand, HyEnA requires less human effort and does not compromise quali
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FLix&#30340;&#26032;&#22411;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#20219;&#21153;&#22810;&#35821;&#35328;&#35843;&#25972;&#65292;&#36890;&#36807;&#20851;&#32852;&#27599;&#20010;&#29420;&#29305;&#25968;&#25454;&#38598;&#29305;&#24449;&#19982;&#20854;&#20302;&#31209;&#26435;&#37325;&#26356;&#26032;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.17934</link><description>&lt;p&gt;
&#29992;&#29305;&#24449;&#21270;&#20302;&#31209;&#28151;&#21512;&#36827;&#34892;&#22810;&#20219;&#21153;&#22810;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Multitask Multilingual Model Adaptation with Featurized Low-Rank Mixtures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17934
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FLix&#30340;&#26032;&#22411;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#20219;&#21153;&#22810;&#35821;&#35328;&#35843;&#25972;&#65292;&#36890;&#36807;&#20851;&#32852;&#27599;&#20010;&#29420;&#29305;&#25968;&#25454;&#38598;&#29305;&#24449;&#19982;&#20854;&#20302;&#31209;&#26435;&#37325;&#26356;&#26032;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36866;&#24212;&#25968;&#21313;&#29978;&#33267;&#25968;&#30334;&#31181;&#20154;&#31867;&#35821;&#35328;&#30340;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#22312;&#35745;&#31639;&#19978;&#26159;&#26114;&#36149;&#30340;&#12290;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#36890;&#36807;&#21482;&#35843;&#25972;&#23569;&#37327;&#21442;&#25968;&#26174;&#33879;&#20943;&#23569;&#20102;&#36866;&#24212;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23558;&#20687; LoRA&#65288;Hu &#31561;&#20154;&#65292;2022&#65289;&#36825;&#26679;&#30340; PEFT &#26041;&#27861;&#24212;&#29992;&#20110;&#19981;&#21516;&#25968;&#25454;&#38598;&#28151;&#21512;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#27425;&#20248;&#65292;&#21407;&#22240;&#22312;&#20110;&#26377;&#38480;&#30340;&#21442;&#25968;&#23481;&#37327;&#21644;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#36127;&#38754;&#20114;&#30456;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29305;&#24449;&#21270;&#20302;&#31209;&#28151;&#21512;&#65288;FLix&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#26377;&#25928;&#30340;&#22810;&#20219;&#21153;&#22810;&#35821;&#35328;&#35843;&#25972;&#30340;&#26032;&#22411; PEFT &#26041;&#27861;&#12290;FLix&#23558;&#27599;&#20010;&#29420;&#29305;&#25968;&#25454;&#38598;&#29305;&#24449;&#65288;&#20363;&#22914;&#25968;&#25454;&#38598;&#30340;&#35821;&#35328;&#25110;&#20219;&#21153;&#65289;&#19982;&#20854;&#33258;&#24049;&#30340;&#20302;&#31209;&#26435;&#37325;&#26356;&#26032;&#21442;&#25968;&#30456;&#20851;&#32852;&#12290;&#36890;&#36807;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#32452;&#21512;&#29305;&#23450;&#20110;&#29305;&#24449;&#30340;&#21442;&#25968;&#65292;FLix&#33021;&#22815;&#36866;&#24212;&#22810;&#31181;&#25968;&#25454;&#38598;&#28151;&#21512;&#65292;&#24182;&#26356;&#22909;&#22320;&#27867;&#21270;&#21040;&#26410;&#35265;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FLix &#21487;&#20197;&#22312;&#25552;&#20379;&#26356;&#22909;&#24615;&#33021;&#30340;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#36866;&#24212;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17934v1 Announce Type: cross  Abstract: Adapting pretrained large language models (LLMs) to various downstream tasks in tens or hundreds of human languages is computationally expensive. Parameter-efficient fine-tuning (PEFT) significantly reduces the adaptation cost, by tuning only a small amount of parameters. However, directly applying PEFT methods such as LoRA (Hu et al., 2022) on diverse dataset mixtures could lead to suboptimal performance due to limited parameter capacity and negative interference among different datasets. In this work, we propose Featurized Low-rank Mixtures (FLix), a novel PEFT method designed for effective multitask multilingual tuning. FLix associates each unique dataset feature, such as the dataset's language or task, with its own low-rank weight update parameters. By composing feature-specific parameters for each dataset, FLix can accommodate diverse dataset mixtures and generalize better to unseen datasets. Our experiments show that FLix leads t
&lt;/p&gt;</description></item><item><title>&#22312;&#35270;&#35273;-&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#21644;&#35780;&#20272;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;&#21512;&#25104;&#25463;&#24452;&#26469;&#25506;&#31350;&#23545;&#27604;&#35757;&#32451;&#26159;&#21542;&#36275;&#20197;&#23398;&#20064;&#21040;&#21253;&#21547;&#25152;&#26377;&#20449;&#24687;&#30340;&#20219;&#21153;&#26368;&#20248;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.17510</link><description>&lt;p&gt;
&#31034;&#33539;&#21644;&#20943;&#23569;&#35270;&#35273;&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#25463;&#24452;
&lt;/p&gt;
&lt;p&gt;
Demonstrating and Reducing Shortcuts in Vision-Language Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17510
&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#35273;-&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#21644;&#35780;&#20272;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;&#21512;&#25104;&#25463;&#24452;&#26469;&#25506;&#31350;&#23545;&#27604;&#35757;&#32451;&#26159;&#21542;&#36275;&#20197;&#23398;&#20064;&#21040;&#21253;&#21547;&#25152;&#26377;&#20449;&#24687;&#30340;&#20219;&#21153;&#26368;&#20248;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17510v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(VLMs)&#20027;&#35201;&#20381;&#36182;&#23545;&#27604;&#35757;&#32451;&#26469;&#23398;&#20064;&#22270;&#20687;&#21644;&#26631;&#39064;&#30340;&#36890;&#29992;&#34920;&#31034;&#12290;&#25105;&#20204;&#20851;&#27880;&#30340;&#24773;&#20917;&#26159;&#24403;&#19968;&#20010;&#22270;&#20687;&#19982;&#22810;&#20010;&#26631;&#39064;&#30456;&#20851;&#32852;&#26102;&#65292;&#27599;&#20010;&#26631;&#39064;&#26082;&#21253;&#21547;&#25152;&#26377;&#26631;&#39064;&#20849;&#20139;&#30340;&#20449;&#24687;&#65292;&#21448;&#21253;&#21547;&#20851;&#20110;&#22270;&#20687;&#22330;&#26223;&#30340;&#27599;&#20010;&#26631;&#39064;&#29420;&#29305;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23578;&#19981;&#28165;&#26970;&#23545;&#27604;&#25439;&#22833;&#26159;&#21542;&#36275;&#20197;&#23398;&#20064;&#21253;&#21547;&#26631;&#39064;&#25552;&#20379;&#30340;&#25152;&#26377;&#20449;&#24687;&#30340;&#20219;&#21153;&#26368;&#20248;&#34920;&#31034;&#65292;&#36824;&#26159;&#23545;&#27604;&#23398;&#20064;&#35774;&#32622;&#26159;&#21542;&#40723;&#21169;&#23398;&#20064;&#26368;&#23567;&#21270;&#23545;&#27604;&#25439;&#22833;&#30340;&#31616;&#21333;&#25463;&#24452;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#35270;&#35273;-&#35821;&#35328;&#30340;&#21512;&#25104;&#25463;&#24452;&#65306;&#19968;&#31181;&#35757;&#32451;&#21644;&#35780;&#20272;&#26694;&#26550;&#65292;&#22312;&#20854;&#20013;&#25105;&#20204;&#21521;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#27880;&#20837;&#21512;&#25104;&#25463;&#24452;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#25110;&#29992;&#21253;&#21547;&#36825;&#20123;&#21512;&#25104;&#25463;&#24452;&#30340;&#25968;&#25454;&#24494;&#35843;&#30340;&#23545;&#27604;VLMs&#20027;&#35201;&#23398;&#20064;&#20195;&#34920;&#25463;&#24452;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17510v1 Announce Type: cross  Abstract: Vision-language models (VLMs) mainly rely on contrastive training to learn general-purpose representations of images and captions. We focus on the situation when one image is associated with several captions, each caption containing both information shared among all captions and unique information per caption about the scene depicted in the image. In such cases, it is unclear whether contrastive losses are sufficient for learning task-optimal representations that contain all the information provided by the captions or whether the contrastive learning setup encourages the learning of a simple shortcut that minimizes contrastive loss. We introduce synthetic shortcuts for vision-language: a training and evaluation framework where we inject synthetic shortcuts into image-text data. We show that contrastive VLMs trained from scratch or fine-tuned with data containing these synthetic shortcuts mainly learn features that represent the shortcu
&lt;/p&gt;</description></item><item><title>&#36873;&#25321;&#36319;&#38543;&#31639;&#27861;&#26159;&#21542;&#20256;&#36798;&#26377;&#20851;&#20154;&#31867;&#33021;&#21147;&#30340;&#20449;&#24687;&#26159;&#23548;&#33268;&#31639;&#27861;&#21388;&#24694;&#29616;&#35937;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#8220;&#31639;&#27861;&#21388;&#24694;&#8221;&#12290;</title><link>https://arxiv.org/abs/2402.15418</link><description>&lt;p&gt;
&#22768;&#35465;&#31639;&#27861;&#21388;&#24694;
&lt;/p&gt;
&lt;p&gt;
Reputational Algorithm Aversion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15418
&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#36319;&#38543;&#31639;&#27861;&#26159;&#21542;&#20256;&#36798;&#26377;&#20851;&#20154;&#31867;&#33021;&#21147;&#30340;&#20449;&#24687;&#26159;&#23548;&#33268;&#31639;&#27861;&#21388;&#24694;&#29616;&#35937;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#8220;&#31639;&#27861;&#21388;&#24694;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#24120;&#24120;&#19981;&#24895;&#23558;&#31639;&#27861;&#20135;&#29983;&#30340;&#20449;&#24687;&#32435;&#20837;&#33258;&#24049;&#30340;&#20915;&#31574;&#20013;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#8220;&#31639;&#27861;&#21388;&#24694;&#8221;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#31639;&#27861;&#21388;&#24694;&#26159;&#22914;&#20309;&#20135;&#29983;&#30340;&#65292;&#24403;&#36873;&#25321;&#36319;&#38543;&#31639;&#27861;&#20256;&#36798;&#26377;&#20851;&#20154;&#31867;&#33021;&#21147;&#30340;&#20449;&#24687;&#26102;&#12290;&#25105;&#24314;&#31435;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#20854;&#20013;&#24037;&#20316;&#32773;&#26681;&#25454;&#33258;&#24049;&#30340;&#31169;&#20154;&#20449;&#24687;&#21644;&#31639;&#27861;&#30340;&#20449;&#21495;&#23545;&#38543;&#26426;&#32467;&#26524;&#36827;&#34892;&#39044;&#27979;&#12290;&#20302;&#25216;&#33021;&#24037;&#20316;&#32773;&#25509;&#25910;&#21040;&#27604;&#31639;&#27861;&#26356;&#24046;&#30340;&#20449;&#24687;&#65292;&#22240;&#27492;&#24212;&#22987;&#32456;&#36981;&#24490;&#31639;&#27861;&#30340;&#20449;&#21495;&#65292;&#32780;&#39640;&#25216;&#33021;&#24037;&#20316;&#32773;&#25509;&#25910;&#21040;&#27604;&#31639;&#27861;&#26356;&#22909;&#30340;&#20449;&#24687;&#65292;&#22240;&#27492;&#26377;&#26102;&#24212;&#35813;&#35206;&#30422;&#31639;&#27861;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22768;&#35465;&#19978;&#30340;&#32771;&#34385;&#65292;&#20302;&#25216;&#33021;&#24037;&#20316;&#32773;&#20250;&#19981;&#21512;&#29702;&#22320;&#35206;&#30422;&#31639;&#27861;&#65292;&#20197;&#22686;&#21152;&#34987;&#35270;&#20026;&#39640;&#25216;&#33021;&#30340;&#21487;&#33021;&#24615;&#12290;&#35813;&#27169;&#22411;&#20026;&#19982;AI&#31995;&#32479;&#21487;&#33021;&#20250;&#21462;&#20195;&#35768;&#22810;&#31867;&#22411;&#30340;&#24037;&#20316;&#32773;&#30340;&#24191;&#27867;&#20851;&#27880;&#25552;&#20379;&#20102;&#23436;&#20840;&#29702;&#24615;&#30340;&#24494;&#35266;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15418v1 Announce Type: cross  Abstract: People are often reluctant to incorporate information produced by algorithms into their decisions, a phenomenon called "algorithm aversion". This paper shows how algorithm aversion arises when the choice to follow an algorithm conveys information about a human's ability. I develop a model in which workers make forecasts of a random outcome based on their own private information and an algorithm's signal. Low-skill workers receive worse information than the algorithm and hence should always follow the algorithm's signal, while high-skill workers receive better information than the algorithm and should sometimes override it. However, due to reputational concerns, low-skill workers inefficiently override the algorithm to increase the likelihood they are perceived as high-skill. The model provides a fully rational microfoundation for algorithm aversion that aligns with the broad concern that AI systems will displace many types of workers.
&lt;/p&gt;</description></item><item><title>DefInt&#25552;&#20986;&#20102;&#19968;&#31181;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;&#65292;&#36890;&#36807;&#40664;&#35748;&#20351;&#29992;&#36739;&#23567;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#24605;&#36335;&#65292;&#28982;&#21518;&#36890;&#36807;&#21453;&#24605;&#25512;&#29702;&#24178;&#39044;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#28151;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02563</link><description>&lt;p&gt;
DefInt&#65306;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#22788;&#29702;&#28151;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DefInt: A Default-interventionist Framework for Efficient Reasoning with Hybrid Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02563
&lt;/p&gt;
&lt;p&gt;
DefInt&#25552;&#20986;&#20102;&#19968;&#31181;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;&#65292;&#36890;&#36807;&#40664;&#35748;&#20351;&#29992;&#36739;&#23567;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#24605;&#36335;&#65292;&#28982;&#21518;&#36890;&#36807;&#21453;&#24605;&#25512;&#29702;&#24178;&#39044;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#28151;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26032;&#33021;&#21147;&#65292;&#20294;&#22312;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#22914;&#36830;&#38145;&#25512;&#29702;&#65288;CoT&#65289;&#21644;&#24605;&#32500;&#26641;&#65288;ToT&#65289;&#20027;&#35201;&#20851;&#27880;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#24573;&#35270;&#20102;&#19981;&#26029;&#22686;&#21152;&#30340;&#26631;&#35760;&#25104;&#26412;&#65292;&#36825;&#23545;&#20110;&#20855;&#26377;&#24040;&#22823;&#35299;&#31354;&#38388;&#30340;&#24320;&#25918;&#24615;&#23454;&#38469;&#20219;&#21153;&#26469;&#35828;&#21487;&#33021;&#29305;&#21035;&#38382;&#39064;&#12290;&#21463;&#20154;&#31867;&#35748;&#30693;&#30340;&#21452;&#36807;&#31243;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;&#65288;DefInt&#65289;&#65292;&#20197;&#37322;&#25918;&#28151;&#21512;LLMs&#30340;&#21327;&#21516;&#28508;&#21147;&#12290;&#40664;&#35748;&#24773;&#20917;&#19979;&#65292;DefInt&#20351;&#29992;&#36739;&#23567;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20302;&#25104;&#26412;&#30340;&#25512;&#29702;&#24605;&#36335;&#65292;&#31867;&#20284;&#20110;&#8220;&#31995;&#32479;1&#8221;&#20135;&#29983;&#30340;&#24555;&#36895;&#30452;&#35273;&#12290;&#22914;&#26524;&#36825;&#20123;&#30452;&#35273;&#34987;&#35748;&#20026;&#20302;&#32622;&#20449;&#24230;&#65292;&#21017;DefInt&#23558;&#35843;&#29992;&#25918;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#24605;&#25512;&#29702;&#20316;&#20026;&#8220;&#31995;&#32479;2&#8221;&#30340;&#24178;&#39044;&#65292;&#21487;&#20197;&#35206;&#30422;&#40664;&#35748;&#24605;&#32771;&#24182;&#32416;&#27491;&#25512;&#29702;&#36807;&#31243;&#12290;&#23454;&#39564;&#22312;&#20116;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;DefInt&#35770;&#25991;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown impressive emergent abilities in a wide range of tasks, but still face challenges in handling complex reasoning problems. Previous works like chain-of-thought (CoT) and tree-of-thoughts(ToT) have predominately focused on enhancing accuracy, but overlook the rapidly increasing token cost, which could be particularly problematic for open-ended real-world tasks with huge solution spaces. Motivated by the dual process theory of human cognition, we propose a Default-Interventionist framework (DefInt) to unleash the synergistic potential of hybrid LLMs. By default, DefInt uses smaller-scale language models to generate low-cost reasoning thoughts, which resembles the fast intuitions produced by System 1. If the intuitions are considered with low confidence, DefInt will invoke the reflective reasoning of scaled-up language models as the intervention of System 2, which can override the default thoughts and rectify the reasoning process. Experiments on fiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;AI&#22522;&#20110;&#31227;&#21160;&#24212;&#29992;&#35780;&#20215;&#20013;&#30340;&#20844;&#24179;&#20851;&#27880;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#25968;&#25454;&#38598;&#21644;&#24320;&#21457;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#26816;&#27979;&#20986;&#20844;&#24179;&#24615;&#35780;&#35770;&#65292;&#24182;&#35782;&#21035;&#20986;&#32422;92000&#26465;&#20844;&#24179;&#24615;&#35780;&#35770;&#12290;</title><link>https://arxiv.org/abs/2401.08097</link><description>&lt;p&gt;
AI&#22522;&#20110;&#31227;&#21160;&#24212;&#29992;&#35780;&#20215;&#30340;&#20844;&#24179;&#20851;&#27880;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study of Fairness Concerns in AI-based Mobile App Reviews
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;AI&#22522;&#20110;&#31227;&#21160;&#24212;&#29992;&#35780;&#20215;&#20013;&#30340;&#20844;&#24179;&#20851;&#27880;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#25968;&#25454;&#38598;&#21644;&#24320;&#21457;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#26816;&#27979;&#20986;&#20844;&#24179;&#24615;&#35780;&#35770;&#65292;&#24182;&#35782;&#21035;&#20986;&#32422;92000&#26465;&#20844;&#24179;&#24615;&#35780;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#26159;AI&#31995;&#32479;&#20013;&#24517;&#39035;&#35299;&#20915;&#30340;&#31038;&#20250;&#25216;&#26415;&#38382;&#39064;&#20043;&#19968;&#12290;&#19981;&#20844;&#24179;&#30340;AI&#31995;&#32479;&#65292;&#29305;&#21035;&#26159;&#19981;&#20844;&#24179;&#30340;AI&#22522;&#20110;&#31227;&#21160;&#24212;&#29992;&#65292;&#21487;&#33021;&#32473;&#20840;&#29699;&#24456;&#22823;&#19968;&#37096;&#20998;&#20154;&#21475;&#24102;&#26469;&#22256;&#38590;&#12290;&#26412;&#25991;&#26088;&#22312;&#20998;&#26512;AI&#22522;&#20110;&#24212;&#29992;&#35780;&#20215;&#20013;&#30340;&#20844;&#24179;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#25163;&#21160;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20844;&#24179;&#24615;&#21644;&#38750;&#20844;&#24179;&#24615;&#35780;&#35770;&#30340;&#32479;&#35745;&#26679;&#26412;&#12290;&#21033;&#29992;&#36825;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24320;&#21457;&#21644;&#35780;&#20272;&#20102;&#19968;&#32452;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#21306;&#20998;&#20844;&#24179;&#24615;&#35780;&#35770;&#21644;&#38750;&#20844;&#24179;&#24615;&#35780;&#35770;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#26368;&#20339;&#30340;&#20998;&#31867;&#22120;&#21487;&#20197;&#20197;94%&#30340;&#31934;&#30830;&#24230;&#26816;&#27979;&#21040;&#20844;&#24179;&#24615;&#35780;&#35770;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#26368;&#20339;&#20998;&#31867;&#22120;&#24212;&#29992;&#20110;&#20174;108&#20010;AI&#22522;&#20110;&#24212;&#29992;&#25910;&#38598;&#30340;&#32422;950&#19975;&#26465;&#35780;&#35770;&#65292;&#35782;&#21035;&#20986;&#32422;92000&#26465;&#20844;&#24179;&#24615;&#35780;&#35770;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23558;K-means&#32858;&#31867;&#25216;&#26415;&#24212;&#29992;&#20110;&#36825;92000&#26465;&#20844;&#24179;&#24615;&#35780;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08097v2 Announce Type: replace-cross Abstract: Fairness is one of the socio-technical concerns that must be addressed in AI-based systems. Unfair AI-based systems, particularly unfair AI-based mobile apps, can pose difficulties for a significant proportion of the global population. This paper aims to analyze fairness concerns in AI-based app reviews.We first manually constructed a ground-truth dataset, including a statistical sample of fairness and non-fairness reviews. Leveraging the ground-truth dataset, we developed and evaluated a set of machine learning and deep learning classifiers that distinguish fairness reviews from non-fairness reviews. Our experiments show that our best-performing classifier can detect fairness reviews with a precision of 94%. We then applied the best-performing classifier on approximately 9.5M reviews collected from 108 AI-based apps and identified around 92K fairness reviews. Next, applying the K-means clustering technique to the 92K fairness r
&lt;/p&gt;</description></item><item><title>KGLens &#26159;&#19968;&#20010;&#26088;&#22312;&#34913;&#37327;&#30693;&#35782;&#22270;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20043;&#38388;&#23545;&#40784;&#31243;&#24230;&#30340;&#26694;&#26550;&#65292;&#24110;&#21161;&#25214;&#20986;LLMs&#30456;&#23545;&#20110;&#30693;&#35782;&#22270;&#30340;&#30693;&#35782;&#19981;&#36275;&#20043;&#22788;&#12290;</title><link>https://arxiv.org/abs/2312.11539</link><description>&lt;p&gt;
KGLens&#65306;&#19968;&#20010;&#21442;&#25968;&#21270;&#30693;&#35782;&#22270;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#35780;&#20272;LLM&#30693;&#36947;&#21644;&#19981;&#30693;&#36947;&#30340;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
KGLens: A Parameterized Knowledge Graph Solution to Assess What an LLM Does and Doesn't Know
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11539
&lt;/p&gt;
&lt;p&gt;
KGLens &#26159;&#19968;&#20010;&#26088;&#22312;&#34913;&#37327;&#30693;&#35782;&#22270;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20043;&#38388;&#23545;&#40784;&#31243;&#24230;&#30340;&#26694;&#26550;&#65292;&#24110;&#21161;&#25214;&#20986;LLMs&#30456;&#23545;&#20110;&#30693;&#35782;&#22270;&#30340;&#30693;&#35782;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34913;&#37327;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20043;&#38388;&#30340;&#23545;&#40784;&#31243;&#24230;&#26159;&#35780;&#20272;&#20107;&#23454;&#24615;&#24182;&#35782;&#21035;LLMs&#30340;&#30693;&#35782;&#30450;&#28857;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65292;&#21253;&#25324;&#23558;KGs&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#21644;&#39640;&#25928;&#35780;&#20272;&#36825;&#20123;&#24191;&#27867;&#19988;&#22797;&#26434;&#30340;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KGLens--&#19968;&#20010;&#26088;&#22312;&#34913;&#37327;KGs&#21644;LLMs&#20043;&#38388;&#23545;&#40784;&#31243;&#24230;&#65292;&#24182;&#25214;&#20986;LLMs&#30456;&#23545;&#20110;KGs&#30340;&#30693;&#35782;&#32570;&#38519;&#30340;&#26032;&#39062;&#26694;&#26550;&#12290;KGLens&#20855;&#26377;&#19968;&#20010;&#22270;&#24341;&#23548;&#30340;&#38382;&#39064;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#23558;KGs&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#65292;&#20197;&#21450;&#19968;&#20010;&#22522;&#20110;&#21442;&#25968;&#21270;KG&#32467;&#26500;&#30340;&#31934;&#24515;&#35774;&#35745;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#20197;&#21152;&#24555;KG&#30340;&#36941;&#21382;&#12290;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;Wikidata&#30340;&#19977;&#20010;&#39046;&#22495;&#29305;&#23450;KG&#36827;&#34892;&#23454;&#39564;&#65292;&#36825;&#20123;KG&#21253;&#25324;&#36229;&#36807;19,000&#26465;&#36793;&#65292;700&#20010;&#20851;&#31995;&#21644;21,000&#20010;&#23454;&#20307;&#12290;&#25105;&#20204;&#36328;&#36234;8&#20010;LLMs&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;KGLens&#19981;&#20165;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11539v2 Announce Type: replace  Abstract: Measuring the alignment between a Knowledge Graph (KG) and Large Language Models (LLMs) is an effective method to assess the factualness and identify the knowledge blind spots of LLMs. However, this approach encounters two primary challenges including the translation of KGs into natural language and the efficient evaluation of these extensive and complex structures. In this paper, we present KGLens--a novel framework aimed at measuring the alignment between KGs and LLMs, and pinpointing the LLMs' knowledge deficiencies relative to KGs. KGLens features a graph-guided question generator for converting KGs into natural language, along with a carefully designed sampling strategy based on parameterized KG structure to expedite KG traversal. We conducted experiments using three domain-specific KGs from Wikidata, which comprise over 19,000 edges, 700 relations, and 21,000 entities. Our analysis across eight LLMs reveals that KGLens not only
&lt;/p&gt;</description></item><item><title>Lite-Mind&#26088;&#22312;&#35299;&#20915;fMRI&#35299;&#30721;&#20013;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#39640;&#25928;&#31283;&#20581;&#30340;&#33041;&#34920;&#31034;&#32593;&#32476;&#65292;&#36991;&#20813;&#20102;&#22312;&#23454;&#36341;&#35774;&#22791;&#19978;&#20026;&#27599;&#20010;&#21463;&#35797;&#32773;&#37096;&#32626;&#29305;&#23450;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.03781</link><description>&lt;p&gt;
Lite-Mind: &#39640;&#25928;&#31283;&#20581;&#30340;&#33041;&#34920;&#31034;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Lite-Mind: Towards Efficient and Robust Brain Representation Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03781
&lt;/p&gt;
&lt;p&gt;
Lite-Mind&#26088;&#22312;&#35299;&#20915;fMRI&#35299;&#30721;&#20013;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#39640;&#25928;&#31283;&#20581;&#30340;&#33041;&#34920;&#31034;&#32593;&#32476;&#65292;&#36991;&#20813;&#20102;&#22312;&#23454;&#36341;&#35774;&#22791;&#19978;&#20026;&#27599;&#20010;&#21463;&#35797;&#32773;&#37096;&#32626;&#29305;&#23450;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38750;&#20405;&#20837;&#24615;&#30340;fMRI&#26041;&#27861;&#35299;&#30721;&#22823;&#33041;&#20013;&#30340;&#35270;&#35273;&#20449;&#24687;&#30340;&#30740;&#31350;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#12290;&#25361;&#25112;&#22312;&#20110;&#26377;&#38480;&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#21644;fMRI&#20449;&#21495;&#30340;&#20302;&#20449;&#22122;&#27604;&#65292;&#23548;&#33268;fMRI&#21040;&#22270;&#20687;&#26816;&#32034;&#20219;&#21153;&#30340;&#20302;&#31934;&#24230;&#12290;MindEye&#25216;&#26415;&#36890;&#36807;&#21033;&#29992;&#39640;&#21442;&#25968;&#35745;&#25968;&#30340;&#28145;&#24230;MLP&#65288;&#27599;&#20010;&#21463;&#35797;&#32773;&#30340;996M MLP&#20027;&#24178;&#65289;&#23558;fMRI&#23884;&#20837;&#23545;&#40784;&#21040;CLIP&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;&#26368;&#32456;&#38544;&#34255;&#23618;&#65292;&#26174;&#30528;&#25552;&#39640;&#20102;fMRI&#21040;&#22270;&#20687;&#26816;&#32034;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#22312;&#30456;&#21516;&#30340;&#23454;&#39564;&#35774;&#32622;&#20869;&#65292;&#21463;&#35797;&#32773;&#20043;&#38388;&#23384;&#22312;&#26174;&#30528;&#30340;&#20010;&#20307;&#24046;&#24322;&#65292;&#38656;&#35201;&#35757;&#32451;&#29305;&#23450;&#20110;&#21463;&#35797;&#32773;&#30340;&#27169;&#22411;&#12290;&#36825;&#20123;&#22823;&#37327;&#30340;&#21442;&#25968;&#22312;&#23454;&#38469;&#35774;&#22791;&#19978;&#37096;&#32626;fMRI&#35299;&#30721;&#26102;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#38656;&#35201;&#20026;&#27599;&#20010;&#21463;&#35797;&#32773;&#25552;&#20379;&#29305;&#23450;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03781v2 Announce Type: replace-cross  Abstract: Research in decoding visual information from the brain, particularly through the non-invasive fMRI method, is rapidly progressing. The challenge arises from the limited data availability and the low signal-to-noise ratio of fMRI signals, leading to a low-precision task of fMRI-to-image retrieval. State-of-the-art MindEye remarkably improves fMRI-to-image retrieval performance by leveraging a deep MLP with a high parameter count orders of magnitude, i.e., a 996M MLP Backbone per subject, to align fMRI embeddings to the final hidden layer of CLIP's vision transformer. However, significant individual variations exist among subjects, even within identical experimental setups, mandating the training of subject-specific models. The substantial parameters pose significant challenges in deploying fMRI decoding on practical devices, especially with the necessitating of specific models for each subject. To this end, we propose Lite-Mind,
&lt;/p&gt;</description></item><item><title>&#23618;&#32423;&#22870;&#21169;&#20989;&#25968;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22870;&#21169;&#35774;&#35745;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20445;&#35777;&#35825;&#23548;&#20986;&#26681;&#25454;&#20559;&#22909;&#20851;&#31995;&#26159;&#24085;&#32047;&#25176;&#26368;&#20248;&#30340;&#31574;&#30053;&#65292;&#24182;&#22312;&#22810;&#20010;&#29615;&#22659;&#20013;&#23637;&#31034;&#20854;&#24555;&#36895;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2212.03733</link><description>&lt;p&gt;
&#23618;&#32423;&#22870;&#21169;&#20989;&#25968;&#65306;&#35268;&#23450;&#21644;&#24555;&#36895;&#23398;&#20064;&#25152;&#38656;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Tiered Reward Functions: Specifying and Fast Learning of Desired Behavior
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.03733
&lt;/p&gt;
&lt;p&gt;
&#23618;&#32423;&#22870;&#21169;&#20989;&#25968;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22870;&#21169;&#35774;&#35745;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20445;&#35777;&#35825;&#23548;&#20986;&#26681;&#25454;&#20559;&#22909;&#20851;&#31995;&#26159;&#24085;&#32047;&#25176;&#26368;&#20248;&#30340;&#31574;&#30053;&#65292;&#24182;&#22312;&#22810;&#20010;&#29615;&#22659;&#20013;&#23637;&#31034;&#20854;&#24555;&#36895;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#26469;&#26368;&#22823;&#21270;&#22870;&#21169;&#20449;&#21495;&#12290;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#20316;&#20026;&#20154;&#31867;&#30340;&#20219;&#21153;&#26159;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#65292;&#20197;&#34920;&#36798;&#25152;&#26399;&#26395;&#30340;&#34892;&#20026;&#65292;&#24182;&#20351;&#20195;&#29702;&#33021;&#22815;&#36805;&#36895;&#23398;&#20064;&#36825;&#31181;&#34892;&#20026;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20219;&#21153;&#20013;&#36798;&#21040;&#33391;&#22909;&#29366;&#24577;&#21644;&#36991;&#20813;&#19981;&#33391;&#29366;&#24577;&#30340;&#22870;&#21169;&#35774;&#35745;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#26684;&#30340;&#31574;&#30053;&#31354;&#38388;&#30340;&#37096;&#20998;&#25490;&#24207;&#65292;&#20197;&#35299;&#20915;&#34892;&#20026;&#20559;&#22909;&#20013;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#26356;&#20542;&#21521;&#20110;&#33021;&#26356;&#24555;&#36895;&#22320;&#21040;&#36798;&#33391;&#22909;&#29366;&#24577;&#24182;&#20197;&#26356;&#39640;&#30340;&#27010;&#29575;&#21040;&#36798;&#65292;&#21516;&#26102;&#33021;&#26356;&#38271;&#26102;&#38388;&#22320;&#36991;&#20813;&#19981;&#33391;&#29366;&#24577;&#30340;&#31574;&#30053;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#23618;&#32423;&#22870;&#21169;&#65292;&#19968;&#31867;&#19982;&#29615;&#22659;&#26080;&#20851;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#34920;&#26126;&#23427;&#20445;&#35777;&#35825;&#23548;&#20986;&#26681;&#25454;&#25105;&#20204;&#30340;&#20559;&#22909;&#20851;&#31995;&#26159;&#24085;&#32047;&#25176;&#26368;&#20248;&#30340;&#31574;&#30053;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23618;&#32423;&#22870;&#21169;&#21487;&#20197;&#36890;&#36807;&#22312;&#22810;&#20010;&#29615;&#22659;&#19978;&#20351;&#29992;&#22810;&#20010;&#34920;&#26684;&#36827;&#34892;&#35780;&#20272;&#65292;&#20174;&#32780;&#23454;&#29616;&#24555;&#36895;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.03733v2 Announce Type: replace-cross  Abstract: Reinforcement-learning agents seek to maximize a reward signal through environmental interactions. As humans, our job in the learning process is to design reward functions to express desired behavior and enable the agent to learn such behavior swiftly. In this work, we consider the reward-design problem in tasks formulated as reaching desirable states and avoiding undesirable states. To start, we propose a strict partial ordering of the policy space to resolve trade-offs in behavior preference. We prefer policies that reach the good states faster and with higher probability while avoiding the bad states longer. Next, we introduce Tiered Reward, a class of environment-independent reward functions and show it is guaranteed to induce policies that are Pareto-optimal according to our preference relation. Finally, we demonstrate that Tiered Reward can lead to fast learning by evaluating on several environments using multiple tabular
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24605;&#32500;&#26041;&#31243;&#33976;&#39311;&#65288;EoTD&#65289;&#25216;&#26415;&#21644;&#38598;&#21512;&#24605;&#32500;&#33976;&#39311;&#65288;ETD&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#26041;&#31243;&#30340;&#34920;&#31034;&#21644;&#20351;&#29992;&#22810;&#20010;&#24605;&#32500;&#36807;&#31243;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#26469;&#25913;&#36827;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EoTD&#21644;ETD&#26174;&#33879;&#25552;&#21319;&#20102;SLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.11864</link><description>&lt;p&gt;
&#36890;&#36807;&#24605;&#32500;&#26041;&#31243;&#33976;&#39311;&#25913;&#36827;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Small Language Models' Mathematical Reasoning via Equation-of-Thought Distillation. (arXiv:2401.11864v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24605;&#32500;&#26041;&#31243;&#33976;&#39311;&#65288;EoTD&#65289;&#25216;&#26415;&#21644;&#38598;&#21512;&#24605;&#32500;&#33976;&#39311;&#65288;ETD&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#26041;&#31243;&#30340;&#34920;&#31034;&#21644;&#20351;&#29992;&#22810;&#20010;&#24605;&#32500;&#36807;&#31243;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#26469;&#25913;&#36827;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EoTD&#21644;ETD&#26174;&#33879;&#25552;&#21319;&#20102;SLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#23558;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#21387;&#32553;&#21040;&#20855;&#26377;&#23567;&#20110;&#21313;&#20159;&#21442;&#25968;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#20013;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#19981;&#25439;&#23475;&#24615;&#33021;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24605;&#32500;&#26041;&#31243;&#33976;&#39311;&#65288;EoTD&#65289;&#25216;&#26415;&#65292;&#23558;&#25512;&#29702;&#36807;&#31243;&#23553;&#35013;&#20026;&#22522;&#20110;&#26041;&#31243;&#30340;&#34920;&#31034;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;EoTD&#25968;&#25454;&#38598;&#26469;&#23545;SLMs&#36827;&#34892;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38598;&#21512;&#24605;&#32500;&#33976;&#39311;&#65288;ETD&#65289;&#26694;&#26550;&#65292;&#20197;&#25552;&#21319;SLMs&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;&#36825;&#21253;&#25324;&#21019;&#24314;&#19968;&#20010;&#21253;&#21547;&#22810;&#20010;&#24605;&#32500;&#36807;&#31243;&#65288;&#21253;&#25324;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#31243;&#24207;&#21644;&#24605;&#32500;&#26041;&#31243;&#65289;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;EoTD&#26174;&#33879;&#25552;&#21319;&#20102;SLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#32780;ETD&#20351;&#36825;&#20123;&#27169;&#22411;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work addresses the challenge of democratizing advanced Large Language Models (LLMs) by compressing their mathematical reasoning capabilities into sub-billion parameter Small Language Models (SLMs) without compromising performance. We introduce Equation-of-Thought Distillation (EoTD), a novel technique that encapsulates the reasoning process into equation-based representations to construct an EoTD dataset for fine-tuning SLMs. Additionally, we propose the Ensemble Thoughts Distillation (ETD) framework to enhance the reasoning performance of SLMs. This involves creating a reasoning dataset with multiple thought processes, including Chain-of-Thought (CoT), Program-of-Thought (PoT), and Equation-of-Thought (EoT), and using it for fine-tuning. Our experimental findings demonstrate that EoTD significantly boosts the reasoning abilities of SLMs, while ETD enables these models to achieve state-of-the-art reasoning performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26469;&#23398;&#20064;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#21518;&#38376;&#65292;&#36890;&#36807;&#25910;&#38598;&#29992;&#20110;&#35757;&#32451;&#30340;&#21518;&#38376;&#24182;&#35757;&#32451;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#27169;&#22411;&#26469;&#39044;&#27979;&#21518;&#38376;&#65292;&#21462;&#24471;&#20102;&#27604;Gurobi&#21644;&#20808;&#21069;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.10467</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#23398;&#20064;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#21518;&#38376;
&lt;/p&gt;
&lt;p&gt;
Learning Backdoors for Mixed Integer Programs with Contrastive Learning. (arXiv:2401.10467v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26469;&#23398;&#20064;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#21518;&#38376;&#65292;&#36890;&#36807;&#25910;&#38598;&#29992;&#20110;&#35757;&#32451;&#30340;&#21518;&#38376;&#24182;&#35757;&#32451;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#27169;&#22411;&#26469;&#39044;&#27979;&#21518;&#38376;&#65292;&#21462;&#24471;&#20102;&#27604;Gurobi&#21644;&#20808;&#21069;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#38382;&#39064;&#21487;&#20197;&#26377;&#25928;&#22320;&#24314;&#27169;&#20026;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#65288;MIP&#65289;&#24182;&#20351;&#29992;&#20998;&#25903;&#23450;&#30028;&#26041;&#27861;&#36827;&#34892;&#27714;&#35299;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#23384;&#22312;MIP&#21518;&#38376;&#65292;&#21363;&#19968;&#23567;&#32452;&#21464;&#37327;&#65292;&#22914;&#26524;&#20248;&#20808;&#22312;&#21487;&#33021;&#30340;&#24773;&#20917;&#19979;&#22312;&#23427;&#20204;&#19978;&#36827;&#34892;&#20998;&#25903;&#65292;&#21017;&#21487;&#20197;&#21152;&#24555;&#36816;&#34892;&#26102;&#38388;&#12290;&#28982;&#32780;&#65292;&#23547;&#25214;&#33021;&#25552;&#39640;&#36816;&#34892;&#26102;&#38388;&#30340;&#39640;&#36136;&#37327;&#21518;&#38376;&#20173;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#36890;&#36807;&#25490;&#21517;&#23398;&#20064;&#20272;&#35745;&#38543;&#26426;&#37319;&#26679;&#30340;&#21518;&#38376;&#30456;&#23545;&#27714;&#35299;&#22120;&#36895;&#24230;&#65292;&#28982;&#21518;&#20915;&#23450;&#26159;&#21542;&#20351;&#29992;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#26041;&#27861;&#25910;&#38598;&#29992;&#20110;&#35757;&#32451;&#30340;&#21518;&#38376;&#65292;&#32780;&#19981;&#26159;&#20381;&#36182;&#38543;&#26426;&#37319;&#26679;&#65292;&#24182;&#19988;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#35757;&#32451;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#27169;&#22411;&#26469;&#39044;&#27979;&#21518;&#38376;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22235;&#20010;&#24120;&#35265;&#30340;MIP&#38382;&#39064;&#39046;&#22495;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#34920;&#29616;&#20986;&#23545;&#27604;Gurobi&#21644;&#20808;&#21069;&#27169;&#22411;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world problems can be efficiently modeled as Mixed Integer Programs (MIPs) and solved with the Branch-and-Bound method. Prior work has shown the existence of MIP backdoors, small sets of variables such that prioritizing branching on them when possible leads to faster running times. However, finding high-quality backdoors that improve running times remains an open question. Previous work learns to estimate the relative solver speed of randomly sampled backdoors through ranking and then decide whether to use it. In this paper, we utilize the Monte-Carlo tree search method to collect backdoors for training, rather than relying on random sampling, and adapt a contrastive learning framework to train a Graph Attention Network model to predict backdoors. Our method, evaluated on four common MIP problem domains, demonstrates performance improvements over both Gurobi and previous models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#33258;&#35757;&#32451;&#26694;&#26550;&#29992;&#20110;&#32454;&#31890;&#24230;&#22330;&#26223;&#22270;&#29983;&#25104;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#32531;&#35299;&#20102;&#22330;&#26223;&#22270;&#29983;&#25104;&#20013;&#30340;&#38271;&#23614;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20266;&#26631;&#31614;&#25216;&#26415;CATM&#21644;&#22270;&#32467;&#26500;&#23398;&#20064;&#22120;GSL&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.09786</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#33258;&#35757;&#32451;&#26694;&#26550;&#29992;&#20110;&#32454;&#31890;&#24230;&#22330;&#26223;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Adaptive Self-training Framework for Fine-grained Scene Graph Generation. (arXiv:2401.09786v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#33258;&#35757;&#32451;&#26694;&#26550;&#29992;&#20110;&#32454;&#31890;&#24230;&#22330;&#26223;&#22270;&#29983;&#25104;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#32531;&#35299;&#20102;&#22330;&#26223;&#22270;&#29983;&#25104;&#20013;&#30340;&#38271;&#23614;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20266;&#26631;&#31614;&#25216;&#26415;CATM&#21644;&#22270;&#32467;&#26500;&#23398;&#20064;&#22120;GSL&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22330;&#26223;&#22270;&#29983;&#25104;&#65288;SGG&#65289;&#27169;&#22411;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#38271;&#23614;&#35859;&#35789;&#20998;&#24067;&#21644;&#32570;&#22833;&#27880;&#37322;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#32531;&#35299;SGG&#30340;&#38271;&#23614;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#33258;&#35757;&#32451;SGG&#65288;ST-SGG&#65289;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#26410;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#20026;&#20854;&#20998;&#37197;&#20266;&#26631;&#31614;&#20197;&#35757;&#32451;SGG&#27169;&#22411;&#12290;&#34429;&#28982;&#22312;&#22270;&#20687;&#35782;&#21035;&#26041;&#38754;&#30340;&#33258;&#35757;&#32451;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#35774;&#35745;&#36866;&#29992;&#20110;SGG&#20219;&#21153;&#30340;&#33258;&#35757;&#32451;&#26694;&#26550;&#26356;&#20855;&#25361;&#25112;&#65292;&#22240;&#20026;&#20854;&#22266;&#26377;&#29305;&#24615;&#65292;&#22914;&#35821;&#20041;&#27495;&#20041;&#21644;&#38271;&#23614;&#20998;&#24067;&#30340;&#35859;&#35789;&#31867;&#21035;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;SGG&#20266;&#26631;&#31614;&#25216;&#26415;&#65292;&#31216;&#20026;&#20855;&#26377;&#21160;&#37327;&#30340;&#31867;&#21035;&#33258;&#36866;&#24212;&#38408;&#20540;&#21270;&#65288;CATM&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#29420;&#31435;&#20110;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#24050;&#26377;&#30340;SGG&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22270;&#32467;&#26500;&#23398;&#20064;&#22120;&#65288;GSL&#65289;&#65292;&#20174;&#20013;&#33719;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scene graph generation (SGG) models have suffered from inherent problems regarding the benchmark datasets such as the long-tailed predicate distribution and missing annotation problems. In this work, we aim to alleviate the long-tailed problem of SGG by utilizing unannotated triplets. To this end, we introduce a Self-Training framework for SGG (ST-SGG) that assigns pseudo-labels for unannotated triplets based on which the SGG models are trained. While there has been significant progress in self-training for image recognition, designing a self-training framework for the SGG task is more challenging due to its inherent nature such as the semantic ambiguity and the long-tailed distribution of predicate classes. Hence, we propose a novel pseudo-labeling technique for SGG, called Class-specific Adaptive Thresholding with Momentum (CATM), which is a model-agnostic framework that can be applied to any existing SGG models. Furthermore, we devise a graph structure learner (GSL) that is benefici
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#29289;&#29702;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#20256;&#36755;&#32593;&#32476;&#30340;&#28526;&#27969;&#28789;&#25935;&#24230;&#22240;&#23376;&#26469;&#25351;&#23548;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#36890;&#36807;&#23454;&#26102;&#34917;&#25937;&#21069;&#30651;&#20915;&#31574;&#26469;&#20943;&#36731;&#40657;&#26263;&#27169;&#24335;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2401.09640</link><description>&lt;p&gt;
&#36890;&#36807;&#29289;&#29702;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20572;&#30005;&#20943;&#36731;
&lt;/p&gt;
&lt;p&gt;
Blackout Mitigation via Physics-guided RL. (arXiv:2401.09640v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#29289;&#29702;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#20256;&#36755;&#32593;&#32476;&#30340;&#28526;&#27969;&#28789;&#25935;&#24230;&#22240;&#23376;&#26469;&#25351;&#23548;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#36890;&#36807;&#23454;&#26102;&#34917;&#25937;&#21069;&#30651;&#20915;&#31574;&#26469;&#20943;&#36731;&#40657;&#26263;&#27169;&#24335;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20026;&#20102;&#38450;&#27490;&#40657;&#26263;&#27169;&#24335;&#32780;&#22312;&#31995;&#32479;&#24322;&#24120;&#26102;&#36827;&#34892;&#24207;&#21015;&#35774;&#35745;&#30340;&#34917;&#25937;&#25511;&#21046;&#34892;&#21160;&#12290;&#35774;&#35745;&#20102;&#19968;&#31181;&#29289;&#29702;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35782;&#21035;&#22312;&#32771;&#34385;&#31995;&#32479;&#31283;&#23450;&#24615;&#38271;&#26399;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#30340;&#23454;&#26102;&#34917;&#25937;&#21069;&#30651;&#20915;&#31574;&#24207;&#21015;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#28041;&#21450;&#31163;&#25955;&#20540;&#20256;&#36755;&#32447;&#24320;&#20851;&#20915;&#31574;&#65288;&#32447;&#36335;&#37325;&#26032;&#36830;&#25509;&#21644;&#31227;&#38500;&#65289;&#21644;&#36830;&#32493;&#20540;&#21457;&#30005;&#26426;&#35843;&#25972;&#30340;&#25511;&#21046;&#34892;&#21160;&#31354;&#38388;&#12290;&#20026;&#20102;&#30830;&#23450;&#26377;&#25928;&#30340;&#20572;&#30005;&#20943;&#36731;&#31574;&#30053;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#29289;&#29702;&#24341;&#23548;&#26041;&#27861;&#65292;&#21033;&#29992;&#19982;&#30005;&#21147;&#20256;&#36755;&#32593;&#32476;&#30456;&#20851;&#30340;&#28526;&#27969;&#28789;&#25935;&#24230;&#22240;&#23376;&#26469;&#24341;&#23548;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#26399;&#38388;&#30340;&#25506;&#32034;&#12290;&#20351;&#29992;&#24320;&#28304;Grid2Op&#24179;&#21488;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#23558;&#29289;&#29702;&#20449;&#21495;&#32435;&#20837;&#24378;&#21270;&#23398;&#20064;&#20915;&#31574;&#30340;&#26174;&#33879;&#20248;&#21183;&#65292;&#35777;&#23454;&#20102;&#25152;&#25552;&#20986;&#30340;&#29289;&#29702;&#24341;&#23548;&#26041;&#27861;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers the sequential design of remedial control actions in response to system anomalies for the ultimate objective of preventing blackouts. A physics-guided reinforcement learning (RL) framework is designed to identify effective sequences of real-time remedial look-ahead decisions accounting for the long-term impact on the system's stability. The paper considers a space of control actions that involve both discrete-valued transmission line-switching decisions (line reconnections and removals) and continuous-valued generator adjustments. To identify an effective blackout mitigation policy, a physics-guided approach is designed that uses power-flow sensitivity factors associated with the power transmission network to guide the RL exploration during agent training. Comprehensive empirical evaluations using the open-source Grid2Op platform demonstrate the notable advantages of incorporating physical signals into RL decisions, establishing the gains of the proposed physics-gu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#23545;&#28508;&#22312;&#36873;&#25321;&#36827;&#34892;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#24110;&#21161;&#36827;&#34892;&#22240;&#26524;&#25512;&#29702;&#20219;&#21153;&#65292;&#21253;&#25324;&#22788;&#29702;&#36873;&#25321;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2401.06925</link><description>&lt;p&gt;
&#29992;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#23545;&#28508;&#22312;&#36873;&#25321;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Modeling Latent Selection with Structural Causal Models. (arXiv:2401.06925v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#23545;&#28508;&#22312;&#36873;&#25321;&#36827;&#34892;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#24110;&#21161;&#36827;&#34892;&#22240;&#26524;&#25512;&#29702;&#20219;&#21153;&#65292;&#21253;&#25324;&#22788;&#29702;&#36873;&#25321;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#20559;&#20506;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#20013;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65292;&#22914;&#26524;&#19981;&#27491;&#30830;&#22788;&#29702;&#21487;&#33021;&#23548;&#33268;&#35823;&#23548;&#24615;&#32467;&#26524;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCMs&#65289;&#36827;&#34892;&#26465;&#20214;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#20197;&#20174;&#22240;&#26524;&#30340;&#35282;&#24230;&#23545;&#28508;&#22312;&#36873;&#25321;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26465;&#20214;&#25805;&#20316;&#23558;&#20855;&#26377;&#26126;&#30830;&#28508;&#22312;&#36873;&#25321;&#26426;&#21046;&#30340;SCM&#36716;&#25442;&#20026;&#27809;&#26377;&#27492;&#31867;&#36873;&#25321;&#26426;&#21046;&#30340;SCM&#65292;&#36825;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#32534;&#30721;&#20102;&#26681;&#25454;&#21407;&#22987;SCM&#36873;&#25321;&#30340;&#20122;&#24635;&#20307;&#30340;&#22240;&#26524;&#35821;&#20041;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#35813;&#26465;&#20214;&#25805;&#20316;&#20445;&#25345;SCMs&#30340;&#31616;&#27905;&#24615;&#65292;&#26080;&#29615;&#24615;&#21644;&#32447;&#24615;&#24615;&#65292;&#24182;&#19982;&#36793;&#38469;&#21270;&#25805;&#20316;&#30456;&#31526;&#21512;&#12290;&#30001;&#20110;&#36825;&#20123;&#29305;&#24615;&#19982;&#36793;&#38469;&#21270;&#21644;&#24178;&#39044;&#32467;&#21512;&#36215;&#26469;&#65292;&#26465;&#20214;&#25805;&#20316;&#20026;&#22312;&#28508;&#22312;&#32454;&#33410;&#24050;&#32463;&#21435;&#38500;&#30340;&#22240;&#26524;&#27169;&#22411;&#20013;&#36827;&#34892;&#22240;&#26524;&#25512;&#29702;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#36890;&#36807;&#20363;&#23376;&#28436;&#31034;&#20102;&#22914;&#20309;&#23558;&#22240;&#26524;&#25512;&#26029;&#30340;&#32463;&#20856;&#32467;&#26524;&#25512;&#24191;&#20197;&#21253;&#25324;&#36873;&#25321;&#20559;&#20506;&#12290;
&lt;/p&gt;
&lt;p&gt;
Selection bias is ubiquitous in real-world data, and can lead to misleading results if not dealt with properly. We introduce a conditioning operation on Structural Causal Models (SCMs) to model latent selection from a causal perspective. We show that the conditioning operation transforms an SCM with the presence of an explicit latent selection mechanism into an SCM without such selection mechanism, which partially encodes the causal semantics of the selected subpopulation according to the original SCM. Furthermore, we show that this conditioning operation preserves the simplicity, acyclicity, and linearity of SCMs, and commutes with marginalization. Thanks to these properties, combined with marginalization and intervention, the conditioning operation offers a valuable tool for conducting causal reasoning tasks within causal models where latent details have been abstracted away. We demonstrate by example how classical results of causal inference can be generalized to include selection b
&lt;/p&gt;</description></item><item><title>DSA&#36879;&#26126;&#25968;&#25454;&#24211;&#23545;&#27431;&#30431;&#20843;&#22823;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#22312;&#21069;100&#22825;&#25552;&#20132;&#30340;&#23457;&#26680;&#34892;&#21160;&#25968;&#25454;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#24179;&#21488;&#22312;&#23457;&#26680;&#34892;&#21160;&#26041;&#38754;&#30340;&#37096;&#20998;&#36981;&#24490;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2312.10269</link><description>&lt;p&gt;
DSA&#36879;&#26126;&#25968;&#25454;&#24211;&#65306;&#31038;&#20132;&#23186;&#20307;&#33258;&#25105;&#25253;&#21578;&#30340;&#23457;&#26680;&#34892;&#21160;
&lt;/p&gt;
&lt;p&gt;
The DSA Transparency Database: Auditing Self-reported Moderation Actions by Social Media. (arXiv:2312.10269v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10269
&lt;/p&gt;
&lt;p&gt;
DSA&#36879;&#26126;&#25968;&#25454;&#24211;&#23545;&#27431;&#30431;&#20843;&#22823;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#22312;&#21069;100&#22825;&#25552;&#20132;&#30340;&#23457;&#26680;&#34892;&#21160;&#25968;&#25454;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#24179;&#21488;&#22312;&#23457;&#26680;&#34892;&#21160;&#26041;&#38754;&#30340;&#37096;&#20998;&#36981;&#24490;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;2023&#24180;9&#26376;&#24320;&#22987;&#65292;&#25968;&#23383;&#26381;&#21153;&#27861;&#26696;(DSA)&#35201;&#27714;&#22823;&#22411;&#22312;&#32447;&#24179;&#21488;&#21521;DSA&#36879;&#26126;&#25968;&#25454;&#24211;&#25552;&#20132;&#20851;&#20110;&#20182;&#20204;&#22312;&#27431;&#30431;&#20869;&#37319;&#21462;&#30340;&#27599;&#20010;&#23457;&#26680;&#34892;&#21160;&#30340;&#35814;&#32454;&#25968;&#25454;&#12290;&#20174;&#19968;&#24320;&#22987;&#65292;&#36825;&#20010;&#38598;&#20013;&#24335;&#25968;&#25454;&#24211;&#23601;&#24341;&#36215;&#20102;&#23398;&#26415;&#30028;&#30340;&#20852;&#36259;&#65292;&#22240;&#20026;&#23427;&#26159;&#29616;&#23454;&#19990;&#30028;&#22312;&#32447;&#23457;&#26680;&#25968;&#25454;&#30340;&#19968;&#20010;&#21069;&#25152;&#26410;&#26377;&#30340;&#12289;&#21487;&#33021;&#26159;&#29420;&#29305;&#30340;&#23453;&#24211;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#28145;&#20837;&#20998;&#26512;&#20102;&#27431;&#30431;&#20843;&#20010;&#26368;&#22823;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#22312;&#25968;&#25454;&#24211;&#30340;&#21069;100&#22825;&#25552;&#20132;&#30340;&#25152;&#26377;3.53&#20159;&#26465;&#35760;&#24405;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;&#24179;&#21488;&#20043;&#38388;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#65292;&#21253;&#25324;&#65306;&#23457;&#26680;&#34892;&#21160;&#30340;&#25968;&#37327;&#12289;&#20915;&#31574;&#20381;&#25454;&#12289;&#24212;&#29992;&#30340;&#38480;&#21046;&#31867;&#22411;&#12289;&#23457;&#26680;&#20869;&#23481;&#31867;&#22411;&#12289;&#23457;&#26680;&#34892;&#21160;&#30340;&#21450;&#26102;&#24615;&#21644;&#25552;&#20132;&#24773;&#20917;&#65292;&#20197;&#21450;&#20351;&#29992;&#30340;&#33258;&#21160;&#21270;&#31243;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#19982;&#24179;&#21488;&#33258;&#24049;&#30340;&#36879;&#26126;&#25253;&#21578;&#36827;&#34892;&#20102;&#20869;&#23481;&#20132;&#21449;&#26816;&#26597;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#20197;&#19979;&#32467;&#26524;&#12290;(i)&#24179;&#21488;&#21482;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#36981;&#24490;&#20102;&#23457;&#26680;&#34892;&#21160;&#30340;&#21746;&#23398;&#21644;&#26041;&#27861;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since September 2023, the Digital Services Act (DSA) obliges large online platforms to submit detailed data on each moderation action they take within the European Union (EU) to the DSA Transparency Database. From its inception, this centralized database has sparked scholarly interest as an unprecedented and potentially unique trove of data on real-world online moderation. Here, we thoroughly analyze all 353.12M records submitted by the eight largest social media platforms in the EU during the first 100 days of the database. Specifically, we conduct a platform-wise comparative study of their: volume of moderation actions, grounds for decision, types of applied restrictions, types of moderated content, timeliness in undertaking and submitting moderation actions, and use of automation. Furthermore, we systematically cross-check the contents of the database with the platforms' own transparency reports. Our analyses reveal that (i) the platforms adhered only in part to the philosophy and s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SC-MIL&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#23383;&#20856;&#23398;&#20064;&#26469;&#21516;&#26102;&#25913;&#36827;&#29305;&#24449;&#23884;&#20837;&#21644;&#23454;&#20363;&#30456;&#20851;&#24615;&#24314;&#27169;&#65292;&#20174;&#32780;&#25552;&#39640;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.00048</link><description>&lt;p&gt;
SC-MIL: &#29992;&#20110;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#30340;&#31232;&#30095;&#32534;&#30721;&#22810;&#23454;&#20363;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SC-MIL: Sparsely Coded Multiple Instance Learning for Whole Slide Image Classification. (arXiv:2311.00048v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SC-MIL&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#23383;&#20856;&#23398;&#20064;&#26469;&#21516;&#26102;&#25913;&#36827;&#29305;&#24449;&#23884;&#20837;&#21644;&#23454;&#20363;&#30456;&#20851;&#24615;&#24314;&#27169;&#65292;&#20174;&#32780;&#25552;&#39640;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#22312;&#24369;&#30417;&#30563;&#30340;&#20840;&#20999;&#29255;&#22270;&#20687;&#65288;WSI&#65289;&#20998;&#31867;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#20856;&#22411;&#30340;MIL&#26041;&#27861;&#21253;&#25324;&#29305;&#24449;&#23884;&#20837;&#37096;&#20998;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#23558;&#23454;&#20363;&#23884;&#20837;&#21040;&#29305;&#24449;&#20013;&#65292;&#20197;&#21450;MIL&#32858;&#21512;&#22120;&#65292;&#23558;&#23454;&#20363;&#23884;&#20837;&#32452;&#21512;&#25104;&#39044;&#27979;&#32467;&#26524;&#12290;&#30446;&#21069;&#30340;&#37325;&#28857;&#26159;&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26469;&#25913;&#36827;&#36825;&#20123;&#37096;&#20998;&#65292;&#24182;&#21333;&#29420;&#24314;&#27169;&#23454;&#20363;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#32534;&#30721;&#30340;MIL&#65288;SC-MIL&#65289;&#65292;&#21516;&#26102;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#23383;&#20856;&#23398;&#20064;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#26041;&#38754;&#12290;&#31232;&#30095;&#23383;&#20856;&#23398;&#20064;&#36890;&#36807;&#23558;&#23454;&#20363;&#34920;&#31034;&#20026;&#36807;&#23436;&#22791;&#23383;&#20856;&#20013;&#21407;&#23376;&#30340;&#31232;&#30095;&#32447;&#24615;&#32452;&#21512;&#26469;&#25429;&#25417;&#23454;&#20363;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#31232;&#30095;&#24615;&#21487;&#20197;&#36890;&#36807;&#25233;&#21046;&#19981;&#30456;&#20851;&#30340;&#23454;&#20363;&#32780;&#20445;&#30041;&#26368;&#30456;&#20851;&#30340;&#23454;&#20363;&#65292;&#20174;&#32780;&#22686;&#24378;&#23454;&#20363;&#30340;&#29305;&#24449;&#23884;&#20837;&#12290;&#20026;&#20102;&#25913;&#21892;&#20256;&#32479;&#30340;&#29305;&#24449;&#23884;&#20837;&#21644;&#23454;&#20363;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#24314;&#27169;&#26041;&#27861;&#65292;we proposed a sparsely coded MIL.
&lt;/p&gt;
&lt;p&gt;
Multiple Instance Learning (MIL) has been widely used in weakly supervised whole slide image (WSI) classification. Typical MIL methods include a feature embedding part that embeds the instances into features via a pre-trained feature extractor and the MIL aggregator that combines instance embeddings into predictions. The current focus has been directed toward improving these parts by refining the feature embeddings through self-supervised pre-training and modeling the correlations between instances separately. In this paper, we proposed a sparsely coded MIL (SC-MIL) that addresses those two aspects at the same time by leveraging sparse dictionary learning. The sparse dictionary learning captures the similarities of instances by expressing them as a sparse linear combination of atoms in an over-complete dictionary. In addition, imposing sparsity help enhance the instance feature embeddings by suppressing irrelevant instances while retaining the most relevant ones. To make the convention
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#37051;&#36817;&#24615;&#26469;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#20026;&#27169;&#22411;&#39044;&#27979;&#25552;&#20379;&#20102;&#23616;&#37096;&#30340;&#35299;&#37322;&#24615;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#36741;&#30456;&#25104;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20538;&#21048;&#23450;&#20215;&#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12428</link><description>&lt;p&gt;
&#23454;&#29616;&#38543;&#26426;&#26862;&#26519;&#30340;&#23616;&#37096;&#21487;&#35299;&#37322;&#24615;&#22686;&#24378;&#65306;&#22522;&#20110;&#37051;&#36817;&#24615;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Enhanced Local Explainability of Random Forests: a Proximity-Based Approach. (arXiv:2310.12428v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12428
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#37051;&#36817;&#24615;&#26469;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#20026;&#27169;&#22411;&#39044;&#27979;&#25552;&#20379;&#20102;&#23616;&#37096;&#30340;&#35299;&#37322;&#24615;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#36741;&#30456;&#25104;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20538;&#21048;&#23450;&#20215;&#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#27169;&#22411;&#30340;&#26679;&#26412;&#22806;&#24615;&#33021;&#65292;&#21033;&#29992;&#20102;&#20219;&#20309;RF&#37117;&#21487;&#20197;&#34987;&#34920;&#36848;&#20026;&#33258;&#36866;&#24212;&#21152;&#26435;K&#26368;&#36817;&#37051;&#65288;KNN&#65289;&#27169;&#22411;&#30340;&#20107;&#23454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;RF&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#23398;&#21040;&#30340;&#28857;&#20043;&#38388;&#30340;&#37051;&#36817;&#24615;&#65292;&#23558;&#38543;&#26426;&#26862;&#26519;&#30340;&#39044;&#27979;&#37325;&#20889;&#20026;&#35757;&#32451;&#25968;&#25454;&#28857;&#30446;&#26631;&#26631;&#31614;&#30340;&#21152;&#26435;&#24179;&#22343;&#20540;&#12290;&#36825;&#31181;&#32447;&#24615;&#24615;&#36136;&#26377;&#21161;&#20110;&#22312;&#35757;&#32451;&#38598;&#35266;&#27979;&#20013;&#20026;&#20219;&#20309;&#27169;&#22411;&#39044;&#27979;&#29983;&#25104;&#23646;&#24615;&#65292;&#20174;&#32780;&#20026;RF&#39044;&#27979;&#25552;&#20379;&#20102;&#23616;&#37096;&#30340;&#35299;&#37322;&#24615;&#65292;&#34917;&#20805;&#20102;SHAP&#31561;&#24050;&#26377;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21017;&#20026;&#29305;&#24449;&#31354;&#38388;&#32500;&#24230;&#19978;&#30340;&#27169;&#22411;&#39044;&#27979;&#29983;&#25104;&#23646;&#24615;&#12290;&#25105;&#20204;&#22312;&#35757;&#32451;&#20110;&#32654;&#22269;&#20844;&#21496;&#20538;&#21048;&#20132;&#26131;&#25968;&#25454;&#30340;&#20538;&#21048;&#23450;&#20215;&#27169;&#22411;&#20013;&#28436;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#21508;&#31181;&#29616;&#26377;&#30340;&#27169;&#22411;&#35299;&#37322;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
We initiate a novel approach to explain the out of sample performance of random forest (RF) models by exploiting the fact that any RF can be formulated as an adaptive weighted K nearest-neighbors model. Specifically, we use the proximity between points in the feature space learned by the RF to re-write random forest predictions exactly as a weighted average of the target labels of training data points. This linearity facilitates a local notion of explainability of RF predictions that generates attributions for any model prediction across observations in the training set, and thereby complements established methods like SHAP, which instead generates attributions for a model prediction across dimensions of the feature space. We demonstrate this approach in the context of a bond pricing model trained on US corporate bond trades, and compare our approach to various existing approaches to model explainability.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#29305;&#26435;&#21319;&#32423;&#22330;&#26223;&#20013;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#28183;&#36879;&#27979;&#35797;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;Linux&#29305;&#26435;&#21319;&#32423;&#22522;&#20934;&#21644;&#19968;&#20010;LLM-guided&#29305;&#26435;&#21319;&#32423;&#24037;&#20855;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;LLMs&#30340;&#19981;&#21516;&#25552;&#31034;&#35774;&#35745;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#39640;&#32423;&#25351;&#23548;&#23545;&#27979;&#35797;&#30340;&#24433;&#21709;&#65292;&#24182;&#35752;&#35770;&#20102;LLMs&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.11409</link><description>&lt;p&gt;
&#35780;&#20272;LLMs&#22312;&#29305;&#26435;&#21319;&#32423;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluating LLMs for Privilege-Escalation Scenarios. (arXiv:2310.11409v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#29305;&#26435;&#21319;&#32423;&#22330;&#26223;&#20013;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#28183;&#36879;&#27979;&#35797;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;Linux&#29305;&#26435;&#21319;&#32423;&#22522;&#20934;&#21644;&#19968;&#20010;LLM-guided&#29305;&#26435;&#21319;&#32423;&#24037;&#20855;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;LLMs&#30340;&#19981;&#21516;&#25552;&#31034;&#35774;&#35745;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#39640;&#32423;&#25351;&#23548;&#23545;&#27979;&#35797;&#30340;&#24433;&#21709;&#65292;&#24182;&#35752;&#35770;&#20102;LLMs&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28183;&#36879;&#27979;&#35797;&#26159;&#32593;&#32476;&#23433;&#20840;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#23427;&#20801;&#35768;&#32452;&#32455;&#20027;&#21160;&#35782;&#21035;&#21644;&#20462;&#22797;&#31995;&#32479;&#20013;&#30340;&#28431;&#27934;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#23545;&#28508;&#22312;&#32593;&#32476;&#25915;&#20987;&#30340;&#38450;&#24481;&#26426;&#21046;&#12290;&#22312;&#28183;&#36879;&#27979;&#35797;&#39046;&#22495;&#65292;&#26368;&#36817;&#30340;&#19968;&#20010;&#36827;&#23637;&#26159;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#25105;&#20204;&#25506;&#32034;LLMs&#19982;&#28183;&#36879;&#27979;&#35797;&#30340;&#20132;&#21449;&#39046;&#22495;&#65292;&#20197;&#20102;&#35299;&#23427;&#20204;&#22312;&#29305;&#26435;&#21319;&#32423;&#22330;&#26223;&#20013;&#30340;&#33021;&#21147;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#20351;&#29992;&#26412;&#22320;&#34394;&#25311;&#26426;&#21019;&#24314;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;Linux&#29305;&#26435;&#21319;&#32423;&#22522;&#20934;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#29305;&#26435;&#21319;&#32423;&#24037;&#20855;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#21516;&#30340;LLMs&#21644;&#25552;&#31034;&#31574;&#30053;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19981;&#21516;&#25552;&#31034;&#35774;&#35745;&#30340;&#24433;&#21709;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#22909;&#22788;&#65292;&#20197;&#21450;&#21521;LLMs&#25552;&#20379;&#39640;&#32423;&#25351;&#23548;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;LLMs&#38754;&#20020;&#30340;&#25361;&#25112;&#39046;&#22495;&#65292;&#21253;&#25324;&#22312;&#27979;&#35797;&#36807;&#31243;&#20013;&#20445;&#25345;&#19987;&#27880;&#12289;&#22788;&#29702;&#38169;&#35823;&#20197;&#21450;&#19982;&#20256;&#32479;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Penetration testing, an essential component of cybersecurity, allows organizations to proactively identify and remediate vulnerabilities in their systems, thus bolstering their defense mechanisms against potential cyberattacks. One recent advancement in the realm of penetration testing is the utilization of Language Models (LLMs). We explore the intersection of LLMs and penetration testing to gain insight into their capabilities and challenges in the context of privilige escalation. We create an automated Linux privilege-escalation benchmark utilizing local virtual machines. We introduce an LLM-guided privilege-escalation tool designed for evaluating different LLMs and prompt strategies against our benchmark. We analyze the impact of different prompt designs, the benefits of in-context learning, and the advantages of offering high-level guidance to LLMs. We discuss challenging areas for LLMs, including maintaining focus during testing, coping with errors, and finally comparing them wit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prototype-based Mean-Teacher (PMT)&#30340;&#26032;&#22411;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31867;&#21407;&#22411;&#32780;&#19981;&#26159;&#22495;&#29305;&#23450;&#23376;&#32593;&#32476;&#26469;&#20445;&#30041;&#22495;&#29305;&#23450;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14950</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#21407;&#22411;&#30340;&#22343;&#20540;&#25945;&#24072;&#30340;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-Source Domain Adaptation for Object Detection with Prototype-based Mean-teacher. (arXiv:2309.14950v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14950
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prototype-based Mean-Teacher (PMT)&#30340;&#26032;&#22411;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31867;&#21407;&#22411;&#32780;&#19981;&#26159;&#22495;&#29305;&#23450;&#23376;&#32593;&#32476;&#26469;&#20445;&#30041;&#22495;&#29305;&#23450;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35270;&#35273;&#30446;&#26631;&#26816;&#27979;&#22120;&#36866;&#24212;&#20110;&#25805;&#20316;&#30446;&#26631;&#39046;&#22495;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36890;&#24120;&#20351;&#29992;&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#26041;&#27861;&#26469;&#23454;&#29616;&#12290;&#24403;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#26469;&#33258;&#22810;&#20010;&#28304;&#22495;&#26102;&#65292;&#23558;&#23427;&#20204;&#35270;&#20026;&#21333;&#29420;&#30340;&#22495;&#24182;&#36827;&#34892;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;&#65288;MSDA&#65289;&#65292;&#30456;&#27604;&#23558;&#36825;&#20123;&#28304;&#22495;&#28151;&#21512;&#24182;&#36827;&#34892;UDA&#65292;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#36817;&#26399;&#30340;&#30740;&#31350;&#20063;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#29616;&#26377;&#30340;MSDA&#26041;&#27861;&#23398;&#20064;&#22495;&#19981;&#21464;&#21644;&#22495;&#29305;&#23450;&#21442;&#25968;&#65288;&#23545;&#20110;&#27599;&#20010;&#28304;&#22495;&#65289;&#26469;&#36827;&#34892;&#33258;&#36866;&#24212;&#12290;&#28982;&#32780;&#65292;&#19982;&#21333;&#28304;UDA&#26041;&#27861;&#19981;&#21516;&#65292;&#23398;&#20064;&#22495;&#29305;&#23450;&#21442;&#25968;&#20351;&#23427;&#20204;&#19982;&#20351;&#29992;&#30340;&#28304;&#22495;&#25968;&#37327;&#25104;&#27491;&#27604;&#22686;&#38271;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#21407;&#22411;&#30340;&#22343;&#20540;&#25945;&#24072;&#65288;PMT&#65289;&#30340;&#26032;&#22411;MSDA&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#31867;&#21407;&#22411;&#32780;&#19981;&#26159;&#22495;&#29305;&#23450;&#23376;&#32593;&#32476;&#26469;&#20445;&#30041;&#22495;&#29305;&#23450;&#20449;&#24687;&#12290;&#36825;&#20123;&#21407;&#22411;&#26159;&#20351;&#29992;&#23545;&#27604;&#25439;&#22833;&#23398;&#20064;&#30340;&#65292;&#23545;&#40784;&#30456;&#21516;&#30340;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adapting visual object detectors to operational target domains is a challenging task, commonly achieved using unsupervised domain adaptation (UDA) methods. When the labeled dataset is coming from multiple source domains, treating them as separate domains and performing a multi-source domain adaptation (MSDA) improves the accuracy and robustness over mixing these source domains and performing a UDA, as observed by recent studies in MSDA. Existing MSDA methods learn domain invariant and domain-specific parameters (for each source domain) for the adaptation. However, unlike single-source UDA methods, learning domain-specific parameters makes them grow significantly proportional to the number of source domains used. This paper proposes a novel MSDA method called Prototype-based Mean-Teacher (PMT), which uses class prototypes instead of domain-specific subnets to preserve domain-specific information. These prototypes are learned using a contrastive loss, aligning the same categories across 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39046;&#22495;&#38543;&#26426;&#21270;&#21644;&#30446;&#26631;&#36319;&#36394;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#31181;&#23376;&#26680;&#35745;&#25968;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#22270;&#20687;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#30340;&#26367;&#20195;&#21697;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#26377;&#26631;&#31614;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#20302;&#25104;&#26412;&#20272;&#35745;&#35895;&#29289;&#20135;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.05846</link><description>&lt;p&gt;
&#20351;&#29992;&#39046;&#22495;&#38543;&#26426;&#21270;&#21644;&#30446;&#26631;&#36319;&#36394;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#31181;&#23376;&#26680;&#35745;&#25968;
&lt;/p&gt;
&lt;p&gt;
Seed Kernel Counting using Domain Randomization and Object Tracking Neural Networks. (arXiv:2308.05846v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39046;&#22495;&#38543;&#26426;&#21270;&#21644;&#30446;&#26631;&#36319;&#36394;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#31181;&#23376;&#26680;&#35745;&#25968;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#22270;&#20687;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#30340;&#26367;&#20195;&#21697;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#26377;&#26631;&#31614;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#20302;&#25104;&#26412;&#20272;&#35745;&#35895;&#29289;&#20135;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36890;&#37327;&#34920;&#22411;&#65288;HTP&#65289;&#23545;&#31181;&#23376;&#30340;&#35780;&#20272;&#26159;&#23545;&#29983;&#38271;&#12289;&#21457;&#32946;&#12289;&#32784;&#21463;&#24615;&#12289;&#25239;&#24615;&#12289;&#29983;&#24577;&#12289;&#20135;&#37327;&#31561;&#22797;&#26434;&#31181;&#23376;&#29305;&#24615;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#20197;&#21450;&#34913;&#37327;&#24418;&#25104;&#26356;&#22797;&#26434;&#29305;&#24615;&#30340;&#21442;&#25968;&#12290;&#31181;&#23376;&#34920;&#22411;&#30340;&#20851;&#38190;&#20043;&#19968;&#26159;&#35895;&#29289;&#20135;&#37327;&#20272;&#35745;&#65292;&#31181;&#23376;&#29983;&#20135;&#34892;&#19994;&#20381;&#36182;&#20110;&#36825;&#19968;&#20272;&#35745;&#26469;&#36827;&#34892;&#19994;&#21153;&#36816;&#20316;&#12290;&#30446;&#21069;&#24066;&#22330;&#19978;&#24050;&#26377;&#26426;&#26800;&#21270;&#30340;&#31181;&#23376;&#26680;&#35745;&#25968;&#22120;&#65292;&#20294;&#20215;&#26684;&#24448;&#24448;&#24456;&#39640;&#65292;&#26377;&#26102;&#36229;&#20986;&#23567;&#35268;&#27169;&#31181;&#23376;&#29983;&#20135;&#20225;&#19994;&#30340;&#25215;&#21463;&#33539;&#22260;&#12290;&#30446;&#26631;&#36319;&#36394;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;(&#22914;YOLO)&#30340;&#21457;&#23637;&#20351;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#33021;&#22815;&#35774;&#35745;&#20986;&#21487;&#20197;&#20302;&#25104;&#26412;&#20272;&#35745;&#35895;&#29289;&#20135;&#37327;&#30340;&#31639;&#27861;&#12290;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#20851;&#38190;&#29942;&#39048;&#26159;&#38656;&#35201;&#22823;&#37327;&#26377;&#26631;&#31614;&#30340;&#35757;&#32451;&#25968;&#25454;&#25165;&#33021;&#25237;&#20837;&#20351;&#29992;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#21512;&#25104;&#22270;&#20687;&#20316;&#20026;&#21487;&#34892;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-throughput phenotyping (HTP) of seeds, also known as seed phenotyping, is the comprehensive assessment of complex seed traits such as growth, development, tolerance, resistance, ecology, yield, and the measurement of parameters that form more complex traits. One of the key aspects of seed phenotyping is cereal yield estimation that the seed production industry relies upon to conduct their business. While mechanized seed kernel counters are available in the market currently, they are often priced high and sometimes outside the range of small scale seed production firms' affordability. The development of object tracking neural network models such as You Only Look Once (YOLO) enables computer scientists to design algorithms that can estimate cereal yield inexpensively. The key bottleneck with neural network models is that they require a plethora of labelled training data before they can be put to task. We demonstrate that the use of synthetic imagery serves as a feasible substitute t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37327;&#21270;&#36755;&#20837;&#21464;&#37327;&#20043;&#38388;&#30340;&#32534;&#30721;&#20132;&#20114;&#26469;&#20934;&#30830;&#19988;&#31616;&#26126;&#22320;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#25512;&#29702;&#36923;&#36753;&#30340;&#26041;&#27861;&#12290;&#38024;&#23545;&#27492;&#30446;&#30340;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#20004;&#31181;&#20132;&#20114;&#26041;&#24335;&#65292;&#21363;AND&#20132;&#20114;&#21644;OR&#20132;&#20114;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#35774;&#35745;&#20986;&#19968;&#31995;&#21015;&#25216;&#26415;&#26469;&#25552;&#39640;&#35299;&#37322;&#30340;&#31616;&#27905;&#24615;&#65292;&#21516;&#26102;&#19981;&#20250;&#25439;&#23475;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.13312</link><description>&lt;p&gt;
&#25216;&#26415;&#31508;&#35760;&#65306;&#23450;&#20041;&#21644;&#37327;&#21270;DNN&#30340;AND-OR&#20132;&#20114;&#20197;&#36827;&#34892;&#20934;&#30830;&#21644;&#31616;&#26126;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Technical Note: Defining and Quantifying AND-OR Interactions for Faithful and Concise Explanation of DNNs. (arXiv:2304.13312v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37327;&#21270;&#36755;&#20837;&#21464;&#37327;&#20043;&#38388;&#30340;&#32534;&#30721;&#20132;&#20114;&#26469;&#20934;&#30830;&#19988;&#31616;&#26126;&#22320;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#25512;&#29702;&#36923;&#36753;&#30340;&#26041;&#27861;&#12290;&#38024;&#23545;&#27492;&#30446;&#30340;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#20004;&#31181;&#20132;&#20114;&#26041;&#24335;&#65292;&#21363;AND&#20132;&#20114;&#21644;OR&#20132;&#20114;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#35774;&#35745;&#20986;&#19968;&#31995;&#21015;&#25216;&#26415;&#26469;&#25552;&#39640;&#35299;&#37322;&#30340;&#31616;&#27905;&#24615;&#65292;&#21516;&#26102;&#19981;&#20250;&#25439;&#23475;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#37327;&#21270;&#36755;&#20837;&#21464;&#37327;&#20043;&#38388;&#30340;&#32534;&#30721;&#20132;&#20114;&#26469;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#25512;&#29702;&#36923;&#36753;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#37325;&#26032;&#24605;&#32771;&#20132;&#20114;&#30340;&#23450;&#20041;&#65292;&#28982;&#21518;&#27491;&#24335;&#23450;&#20041;&#20102;&#22522;&#20110;&#20132;&#20114;&#30340;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#21644;&#31616;&#27905;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#20132;&#20114;&#26041;&#24335;&#65292;&#21363;AND&#20132;&#20114;&#21644;OR&#20132;&#20114;&#12290;&#38024;&#23545;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;AND&#65288;OR&#65289;&#20132;&#20114;&#22312;&#37327;&#21270;&#36755;&#20837;&#21464;&#37327;&#20043;&#38388;&#30340;AND&#65288;OR&#65289;&#20851;&#31995;&#25928;&#24212;&#26041;&#38754;&#30340;&#21807;&#19968;&#24615;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;AND-OR&#20132;&#20114;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#25216;&#26415;&#26469;&#25552;&#39640;&#35299;&#37322;&#30340;&#31616;&#27905;&#24615;&#65292;&#21516;&#26102;&#19981;&#20250;&#25439;&#23475;&#20934;&#30830;&#24615;&#12290;&#22240;&#27492;&#65292;DNN&#30340;&#25512;&#29702;&#36923;&#36753;&#21487;&#20197;&#36890;&#36807;&#19968;&#32452;&#31526;&#21495;&#27010;&#24565;&#20934;&#30830;&#32780;&#31616;&#26126;&#22320;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this technical note, we aim to explain a deep neural network (DNN) by quantifying the encoded interactions between input variables, which reflects the DNN's inference logic. Specifically, we first rethink the definition of interactions, and then formally define faithfulness and conciseness for interaction-based explanation. To this end, we propose two kinds of interactions, i.e., the AND interaction and the OR interaction. For faithfulness, we prove the uniqueness of the AND (OR) interaction in quantifying the effect of the AND (OR) relationship between input variables. Besides, based on AND-OR interactions, we design techniques to boost the conciseness of the explanation, while not hurting the faithfulness. In this way, the inference logic of a DNN can be faithfully and concisely explained by a set of symbolic concepts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25506;&#27979;&#22120;&#27169;&#25311;&#26041;&#27861;IEA-GAN&#65292;&#36890;&#36807;&#20135;&#29983;&#19982;&#22270;&#23618;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#21270;&#30340;&#22270;&#20687;&#65292;&#25552;&#39640;&#20102;&#36229;&#39640;&#20998;&#36776;&#29575;&#25506;&#27979;&#22120;&#21709;&#24212;&#30340;&#30456;&#20851;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#26032;&#30340;&#20107;&#20214;&#24863;&#30693;&#25439;&#22833;&#21644;&#32479;&#19968;&#24615;&#25439;&#22833;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#20687;&#30340;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.08046</link><description>&lt;p&gt;
&#22522;&#20110;&#20107;&#20214;&#24863;&#30693;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#33258;&#30417;&#30563;&#20851;&#31995;&#25512;&#29702;&#30340;&#36229;&#39640;&#20998;&#36776;&#29575;&#25506;&#27979;&#22120;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Ultra-High-Resolution Detector Simulation with Intra-Event Aware GAN and Self-Supervised Relational Reasoning. (arXiv:2303.08046v1 [physics.ins-det])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25506;&#27979;&#22120;&#27169;&#25311;&#26041;&#27861;IEA-GAN&#65292;&#36890;&#36807;&#20135;&#29983;&#19982;&#22270;&#23618;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#21270;&#30340;&#22270;&#20687;&#65292;&#25552;&#39640;&#20102;&#36229;&#39640;&#20998;&#36776;&#29575;&#25506;&#27979;&#22120;&#21709;&#24212;&#30340;&#30456;&#20851;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#26032;&#30340;&#20107;&#20214;&#24863;&#30693;&#25439;&#22833;&#21644;&#32479;&#19968;&#24615;&#25439;&#22833;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#20687;&#30340;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31890;&#23376;&#29289;&#29702;&#23398;&#20013;&#65292;&#27169;&#25311;&#39640;&#20998;&#36776;&#29575;&#25506;&#27979;&#22120;&#21709;&#24212;&#19968;&#30452;&#26159;&#19968;&#20010;&#23384;&#20648;&#25104;&#26412;&#39640;&#12289;&#35745;&#31639;&#23494;&#38598;&#30340;&#36807;&#31243;&#12290;&#23613;&#31649;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#20351;&#36825;&#20010;&#36807;&#31243;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#65292;&#20294;&#36229;&#39640;&#20998;&#36776;&#29575;&#25506;&#27979;&#22120;&#27169;&#25311;&#20173;&#28982;&#24456;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#21253;&#21547;&#20102;&#20107;&#20214;&#20869;&#30456;&#20851;&#21644;&#32454;&#31890;&#24230;&#30340;&#30456;&#20114;&#20449;&#24687;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26041;&#27861;&#65288;IEA-GAN&#65289;&#65292;&#34701;&#21512;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#20851;&#31995;&#25512;&#29702;&#27169;&#22411;&#12290;IEA-GAN&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#31995;&#25512;&#29702;&#27169;&#22359;&#65292;&#36817;&#20284;&#20110;&#25506;&#27979;&#22120;&#27169;&#25311;&#20013;&#8220;&#20107;&#20214;&#8221;&#30340;&#27010;&#24565;&#65292;&#21487;&#20197;&#29983;&#25104;&#19982;&#22270;&#23618;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#21270;&#30340;&#22270;&#20687;&#65292;&#25552;&#39640;&#20102;&#36229;&#39640;&#20998;&#36776;&#29575;&#25506;&#27979;&#22120;&#21709;&#24212;&#30340;&#30456;&#20851;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;IEA-GAN&#36824;&#24341;&#20837;&#20102;&#26032;&#30340;&#20107;&#20214;&#24863;&#30693;&#25439;&#22833;&#21644;&#32479;&#19968;&#24615;&#25439;&#22833;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#20687;&#30340;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;IEA-GAN&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulating high-resolution detector responses is a storage-costly and computationally intensive process that has long been challenging in particle physics. Despite the ability of deep generative models to make this process more cost-efficient, ultra-high-resolution detector simulation still proves to be difficult as it contains correlated and fine-grained mutual information within an event. To overcome these limitations, we propose Intra-Event Aware GAN (IEA-GAN), a novel fusion of Self-Supervised Learning and Generative Adversarial Networks. IEA-GAN presents a Relational Reasoning Module that approximates the concept of an ''event'' in detector simulation, allowing for the generation of correlated layer-dependent contextualized images for high-resolution detector responses with a proper relational inductive bias. IEA-GAN also introduces a new intra-event aware loss and a Uniformity loss, resulting in significant enhancements to image fidelity and diversity. We demonstrate IEA-GAN's ap
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#25512;&#25991;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#20840;&#29699;&#21644;&#32654;&#22269;&#19978;&#30340;&#20013;&#20301;&#35823;&#24046;&#20998;&#21035;&#23567;&#20110;30&#20844;&#37324;&#21644;15&#20844;&#37324;&#30340;&#23450;&#20301;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.07865</link><description>&lt;p&gt;
&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#25512;&#25991;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Geolocation Predicting of Tweets Using BERT-Based Models. (arXiv:2303.07865v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07865
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#25512;&#25991;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#20840;&#29699;&#21644;&#32654;&#22269;&#19978;&#30340;&#20013;&#20301;&#35823;&#24046;&#20998;&#21035;&#23567;&#20110;30&#20844;&#37324;&#21644;15&#20844;&#37324;&#30340;&#23450;&#20301;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#25512;&#25991;/&#29992;&#25143;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#22788;&#29702;&#25991;&#26412;&#22823;&#25968;&#25454;&#22320;&#29702;&#26631;&#35760;&#30340;&#28789;&#27963;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26469;&#20272;&#35745;&#22352;&#26631;&#23545;&#65288;&#32463;&#24230;&#65292;&#32428;&#24230;&#65289;&#21644;&#20108;&#32500;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#12290;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#33539;&#22260;&#24050;&#32463;&#22312;Twitter&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#36827;&#34892;&#35843;&#25972;&#12290;&#24615;&#33021;&#25351;&#26631;&#34920;&#26126;&#65292;&#23545;&#20110;&#22312;&#25512;&#25991;&#20869;&#23481;&#21644;&#20803;&#25968;&#25454;&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#27169;&#22411;&#65292;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#20013;&#20301;&#35823;&#24046;&#23567;&#20110;30&#20844;&#37324;&#65292;&#32654;&#22269;&#33539;&#22260;&#20869;&#30340;&#20013;&#20301;&#35823;&#24046;&#23567;&#20110;15&#20844;&#37324;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research is aimed to solve the tweet/user geolocation prediction task and provide a flexible methodology for the geotagging of textual big data. The suggested approach implements neural networks for natural language processing (NLP) to estimate the location as coordinate pairs (longitude, latitude) and two-dimensional Gaussian Mixture Models (GMMs). The scope of proposed models has been finetuned on a Twitter dataset using pretrained Bidirectional Encoder Representations from Transformers (BERT) as base models. Performance metrics show a median error of fewer than 30 km on a worldwide-level, and fewer than 15 km on the US-level datasets for the models trained and evaluated on text features of tweets' content and metadata context.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#19981;&#21516;&#25277;&#35937;&#23618;&#27425;&#19978;&#30340;&#27867;&#21270;&#38382;&#39064;&#21450;&#20854;&#26041;&#27861;&#65292;&#20854;&#20013;&#26679;&#26412;&#27867;&#21270;&#24050;&#32463;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#26410;&#26469;&#38656;&#35201;&#37325;&#28857;&#20851;&#27880;&#20943;&#23569;&#36807;&#25311;&#21512;&#65307;&#20998;&#24067;&#27867;&#21270;&#19982;&#39046;&#22495;&#27867;&#21270;&#26377;&#30456;&#20284;&#20043;&#22788;&#65292;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#22256;&#38590;&#30340;&#26679;&#26412;&#25110;&#20998;&#24067;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2209.01610</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27867;&#21270;&#65306;&#32508;&#36848;&#19982;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Generalization in Neural Networks: A Broad Survey. (arXiv:2209.01610v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.01610
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#19981;&#21516;&#25277;&#35937;&#23618;&#27425;&#19978;&#30340;&#27867;&#21270;&#38382;&#39064;&#21450;&#20854;&#26041;&#27861;&#65292;&#20854;&#20013;&#26679;&#26412;&#27867;&#21270;&#24050;&#32463;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#26410;&#26469;&#38656;&#35201;&#37325;&#28857;&#20851;&#27880;&#20943;&#23569;&#36807;&#25311;&#21512;&#65307;&#20998;&#24067;&#27867;&#21270;&#19982;&#39046;&#22495;&#27867;&#21270;&#26377;&#30456;&#20284;&#20043;&#22788;&#65292;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#22256;&#38590;&#30340;&#26679;&#26412;&#25110;&#20998;&#24067;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#19981;&#21516;&#25277;&#35937;&#23618;&#27425;&#30340;&#27010;&#24565;&#12289;&#24314;&#27169;&#26041;&#27861;&#21644;&#26368;&#36817;&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#21253;&#25324;&#26679;&#26412;&#12289;&#20998;&#24067;&#12289;&#39046;&#22495;&#12289;&#20219;&#21153;&#12289;&#27169;&#24577;&#21644;&#33539;&#22260;&#19978;&#30340;&#27867;&#21270;&#12290;&#22312;&#26679;&#26412;&#27867;&#21270;&#26041;&#38754;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#20960;&#20046;&#25152;&#26377;&#30340;&#26368;&#26032;&#25913;&#36827;&#37117;&#20943;&#23567;&#20102;&#35757;&#32451;&#35823;&#24046;&#65292;&#32780;&#36807;&#25311;&#21512;&#20445;&#25345;&#19981;&#21464;&#65307;&#38543;&#30528;&#20960;&#20046;&#25152;&#26377;&#30340;&#35757;&#32451;&#35823;&#24046;&#34987;&#28040;&#38500;&#65292;&#26410;&#26469;&#30340;&#36827;&#23637;&#23558;&#38656;&#35201;&#38598;&#20013;&#20851;&#27880;&#20943;&#23569;&#36807;&#25311;&#21512;&#12290;&#20174;&#32479;&#35745;&#23398;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;(2)&#20998;&#24067;&#27867;&#21270;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#26679;&#26412;&#26435;&#37325;&#25110;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#30340;&#21464;&#21270;&#65307;&#22240;&#27492;&#65292;&#22312;&#39046;&#22495;&#27867;&#21270;&#25104;&#21151;&#30340;&#25216;&#26415;&#26377;&#21487;&#33021;&#24212;&#29992;&#20110;&#22256;&#38590;&#30340;&#26679;&#26412;&#25110;&#20998;&#24067;&#27867;&#21270;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;(3)&#39046;&#22495;&#27867;&#21270;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#26368;&#36817;&#30340;&#36827;&#23637;&#21644;&#20016;&#23500;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper reviews concepts, modeling approaches, and recent findings along a spectrum of different levels of abstraction of neural network models including generalization across (1) Samples, (2) Distributions, (3) Domains, (4) Tasks, (5) Modalities, and (6) Scopes. Results on (1) sample generalization show that, in the case of ImageNet, nearly all the recent improvements reduced training error while overfitting stayed flat; with nearly all the training error eliminated, future progress will require a focus on reducing overfitting. Perspectives from statistics highlight how (2) distribution generalization can be viewed alternately as a change in sample weights or a change in the input-output relationship; thus, techniques that have been successful in domain generalization have the potential to be applied to difficult forms of sample or distribution generalization. Transfer learning approaches to (3) domain generalization are summarized, as are recent advances and the wealth of domain a
&lt;/p&gt;</description></item></channel></rss>