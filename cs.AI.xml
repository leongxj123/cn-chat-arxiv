<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#23616;&#27880;&#24847;&#21147;&#27969;&#23884;&#20837;&#21644;&#31354;&#38388;&#26102;&#38388;&#29305;&#24449;&#37325;&#26032;&#23884;&#20837;&#27169;&#22359;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#27969;&#39044;&#27979;&#20013;&#30340;&#23616;&#37096;&#20381;&#36182;&#21305;&#37197;&#21644;&#38750;&#21018;&#24615;&#29289;&#20307;&#21464;&#24418;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.07032</link><description>&lt;p&gt;
STARFlow: &#20855;&#26377;&#27880;&#24847;&#21147;&#23398;&#20064;&#30340;&#31354;&#38388;&#26102;&#38388;&#29305;&#24449;&#37325;&#26032;&#23884;&#20837;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#27969;
&lt;/p&gt;
&lt;p&gt;
STARFlow: Spatial Temporal Feature Re-embedding with Attentive Learning for Real-world Scene Flow
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07032
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#23616;&#27880;&#24847;&#21147;&#27969;&#23884;&#20837;&#21644;&#31354;&#38388;&#26102;&#38388;&#29305;&#24449;&#37325;&#26032;&#23884;&#20837;&#27169;&#22359;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#27969;&#39044;&#27979;&#20013;&#30340;&#23616;&#37096;&#20381;&#36182;&#21305;&#37197;&#21644;&#38750;&#21018;&#24615;&#29289;&#20307;&#21464;&#24418;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22330;&#26223;&#27969;&#39044;&#27979;&#26159;&#29702;&#35299;&#21160;&#24577;&#22330;&#26223;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#25552;&#20379;&#20102;&#22522;&#26412;&#30340;&#36816;&#21160;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#24403;&#20195;&#22330;&#26223;&#27969;&#26041;&#27861;&#38754;&#20020;&#19977;&#22823;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#20165;&#22522;&#20110;&#23616;&#37096;&#24863;&#21463;&#37326;&#30340;&#27969;&#20272;&#35745;&#32570;&#20047;&#28857;&#23545;&#30340;&#38271;&#20381;&#36182;&#21305;&#37197;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20840;&#23616;&#27880;&#24847;&#21147;&#27969;&#23884;&#20837;&#65292;&#20197;&#21305;&#37197;&#29305;&#24449;&#31354;&#38388;&#21644;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#25152;&#26377;&#28857;&#23545;&#65292;&#25552;&#20379;&#23616;&#37096;&#32454;&#21270;&#20043;&#21069;&#30340;&#20840;&#23616;&#21021;&#22987;&#21270;&#12290;&#20854;&#27425;&#65292;&#22312;&#21464;&#24418;&#21518;&#23384;&#22312;&#38750;&#21018;&#24615;&#29289;&#20307;&#30340;&#21464;&#24418;&#65292;&#23548;&#33268;&#36830;&#32493;&#24103;&#20043;&#38388;&#30340;&#26102;&#31354;&#20851;&#31995;&#21464;&#21270;&#12290;&#20026;&#20102;&#26356;&#31934;&#30830;&#22320;&#20272;&#35745;&#27531;&#20313;&#27969;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#31354;&#38388;&#26102;&#38388;&#29305;&#24449;&#37325;&#26032;&#23884;&#20837;&#27169;&#22359;&#65292;&#20197;&#22312;&#21464;&#24418;&#21518;&#33719;&#21462;&#24207;&#21015;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#20043;&#38388;&#30340;&#26174;&#33879;&#22495;&#24046;&#24322;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07032v1 Announce Type: cross  Abstract: Scene flow prediction is a crucial underlying task in understanding dynamic scenes as it offers fundamental motion information. However, contemporary scene flow methods encounter three major challenges. Firstly, flow estimation solely based on local receptive fields lacks long-dependency matching of point pairs. To address this issue, we propose global attentive flow embedding to match all-to-all point pairs in both feature space and Euclidean space, providing global initialization before local refinement. Secondly, there are deformations existing in non-rigid objects after warping, which leads to variations in the spatiotemporal relation between the consecutive frames. For a more precise estimation of residual flow, a spatial temporal feature re-embedding module is devised to acquire the sequence features after deformation. Furthermore, previous methods perform poor generalization due to the significant domain gap between the synthesi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#21160;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#27602;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#27602;&#24615;&#22240;&#32032;&#21644;LLMs&#30340;&#20869;&#22312;&#27602;&#24615;&#23646;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#27979;&#37327;&#27602;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#20247;&#65292;&#27604;&#29616;&#26377;&#25351;&#26631;&#25552;&#21319;12&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.06900</link><description>&lt;p&gt;
LLM&#33021;&#22815;&#35782;&#21035;&#27602;&#24615;&#21527;&#65311;&#32467;&#26500;&#21270;&#27602;&#24615;&#35843;&#26597;&#26694;&#26550;&#21644;&#22522;&#20110;&#35821;&#20041;&#30340;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework and Semantic-Based Metric
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#21160;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#27602;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#27602;&#24615;&#22240;&#32032;&#21644;LLMs&#30340;&#20869;&#22312;&#27602;&#24615;&#23646;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#27979;&#37327;&#27602;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#20247;&#65292;&#27604;&#29616;&#26377;&#25351;&#26631;&#25552;&#21319;12&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#21457;&#36981;&#23432;&#31038;&#20250;&#26631;&#20934;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36807;&#31243;&#20013;&#65292;&#35782;&#21035;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#27602;&#24615;&#23384;&#22312;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#27602;&#24615;&#24230;&#37327;&#20381;&#36182;&#20110;&#22312;&#29305;&#23450;&#27602;&#24615;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#32534;&#30721;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32534;&#30721;&#22120;&#23481;&#26131;&#21463;&#21040;&#20998;&#24067;&#22806;&#30340;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#20013;&#25152;&#20551;&#23450;&#30340;&#27602;&#24615;&#23450;&#20041;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#33258;&#21160;&#40065;&#26834;&#24230;&#37327;&#65292;&#29992;&#20110;&#21306;&#20998;&#27169;&#22411;&#22238;&#24212;&#26159;&#21542;&#20855;&#26377;&#27602;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#27602;&#24615;&#22240;&#32032;&#65292;&#28982;&#21518;&#30740;&#31350;&#20102;LLMs&#30340;&#20869;&#22312;&#27602;&#24615;&#23646;&#24615;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#20316;&#20026;&#35780;&#20272;&#22120;&#30340;&#36866;&#29992;&#24615;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#30340;&#24230;&#37327;&#25351;&#26631;LLMs As ToxiciTy Evaluators&#65288;LATTE&#65289;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#36827;&#34892;&#35757;&#32451;&#36807;&#31243;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#24230;&#37327;&#22312;&#27979;&#37327;&#27602;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;F1&#24471;&#20998;&#27604;&#29616;&#26377;&#25216;&#26415;&#25351;&#26631;&#25552;&#39640;&#20102;12&#20010;&#30334;&#20998;&#28857;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19978;&#28216;&#27602;&#24615;&#23545;&#24230;&#37327;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the pursuit of developing Large Language Models (LLMs) that adhere to societal standards, it is imperative to discern the existence of toxicity in the generated text. The majority of existing toxicity metrics rely on encoder models trained on specific toxicity datasets. However, these encoders are susceptible to out-of-distribution (OOD) problems and depend on the definition of toxicity assumed in a dataset. In this paper, we introduce an automatic robust metric grounded on LLMs to distinguish whether model responses are toxic. We start by analyzing the toxicity factors, followed by examining the intrinsic toxic attributes of LLMs to ascertain their suitability as evaluators. Subsequently, we evaluate our metric, LLMs As ToxiciTy Evaluators (LATTE), on evaluation datasets.The empirical results indicate outstanding performance in measuring toxicity, improving upon state-of-the-art metrics by 12 points in F1 score without training procedure. We also show that upstream toxicity has an 
&lt;/p&gt;</description></item><item><title>IGUANe&#26159;&#19968;&#31181;&#19977;&#32500;&#36890;&#29992;CycleGAN&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#22495;&#30340;&#35757;&#32451;&#23454;&#29616;&#20102;&#33041;MR&#22270;&#20687;&#30340;&#22810;&#20013;&#24515;&#21327;&#35843;&#65292;&#20351;&#20854;&#25104;&#20026;&#36890;&#29992;&#29983;&#25104;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.03227</link><description>&lt;p&gt;
IGUANe: &#19968;&#31181;&#36866;&#29992;&#20110;&#33041;MR&#22270;&#20687;&#22810;&#20013;&#24515;&#21327;&#35843;&#30340;&#19977;&#32500;&#36890;&#29992;CycleGAN&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of brain MR images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03227
&lt;/p&gt;
&lt;p&gt;
IGUANe&#26159;&#19968;&#31181;&#19977;&#32500;&#36890;&#29992;CycleGAN&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#22495;&#30340;&#35757;&#32451;&#23454;&#29616;&#20102;&#33041;MR&#22270;&#20687;&#30340;&#22810;&#20013;&#24515;&#21327;&#35843;&#65292;&#20351;&#20854;&#25104;&#20026;&#36890;&#29992;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;MRI&#30740;&#31350;&#20013;&#65292;&#26469;&#33258;&#22810;&#20010;&#37319;&#38598;&#28857;&#30340;&#22270;&#20687;&#25968;&#25454;&#30340;&#32858;&#21512;&#21487;&#20197;&#22686;&#21152;&#26679;&#26412;&#22823;&#23567;&#65292;&#20294;&#21487;&#33021;&#24341;&#20837;&#38459;&#30861;&#21518;&#32493;&#20998;&#26512;&#19968;&#33268;&#24615;&#30340;&#19982;&#37319;&#38598;&#28857;&#30456;&#20851;&#30340;&#21464;&#24322;&#12290;&#22270;&#20687;&#32763;&#35793;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#21327;&#35843;MR&#22270;&#20687;&#36328;&#31449;&#28857;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;IGUANe&#65288;&#20855;&#26377;&#32479;&#19968;&#23545;&#25239;&#32593;&#32476;&#30340;&#22270;&#20687;&#29983;&#25104;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21407;&#22987;&#30340;&#19977;&#32500;&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;&#22495;&#36716;&#25442;&#30340;&#20248;&#21183;&#21644;&#30452;&#25509;&#24212;&#29992;&#26679;&#24335;&#36716;&#31227;&#26041;&#27861;&#26469;&#23454;&#29616;&#22810;&#20013;&#24515;&#33041;MR&#22270;&#20687;&#21327;&#35843;&#12290;IGUANe&#36890;&#36807;&#22810;&#23545;&#19968;&#31574;&#30053;&#65292;&#38598;&#25104;&#20102;&#20219;&#24847;&#25968;&#37327;&#30340;&#22495;&#36827;&#34892;&#35757;&#32451;&#65292;&#25193;&#23637;&#20102;CycleGAN&#26550;&#26500;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#22270;&#20687;&#65292;&#29978;&#33267;&#26469;&#33258;&#26410;&#30693;&#37319;&#38598;&#28857;&#65292;&#20351;&#20854;&#25104;&#20026;&#21327;&#35843;&#30340;&#36890;&#29992;&#29983;&#25104;&#22120;&#12290;&#22312;&#30001;11&#21488;&#19981;&#21516;&#25195;&#25551;&#20202;&#30340;T1&#21152;&#26435;&#22270;&#20687;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;IGUANe&#22312;&#26410;&#35265;&#31449;&#28857;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In MRI studies, the aggregation of imaging data from multiple acquisition sites enhances sample size but may introduce site-related variabilities that hinder consistency in subsequent analyses. Deep learning methods for image translation have emerged as a solution for harmonizing MR images across sites. In this study, we introduce IGUANe (Image Generation with Unified Adversarial Networks), an original 3D model that leverages the strengths of domain translation and straightforward application of style transfer methods for multicenter brain MR image harmonization. IGUANe extends CycleGAN architecture by integrating an arbitrary number of domains for training through a many-to-one strategy. During inference, the model can be applied to any image, even from an unknown acquisition site, making it a universal generator for harmonization. Trained on a dataset comprising T1-weighted images from 11 different scanners, IGUANe was evaluated on data from unseen sites. The assessments included the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#35843;&#26597;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25506;&#35752;&#20102;&#35813;&#26041;&#27861;&#22312;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#38480;&#21046;&#26041;&#38754;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.03017</link><description>&lt;p&gt;
&#21521;&#32511;&#33394;&#19988;&#31867;&#20154;&#30340;&#20154;&#24037;&#26234;&#33021;&#36808;&#36827;&#65306;&#24403;&#20195;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#30340;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Toward Green and Human-Like Artificial Intelligence: A Complete Survey on Contemporary Few-Shot Learning Approaches
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#35843;&#26597;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25506;&#35752;&#20102;&#35813;&#26041;&#27861;&#22312;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#38480;&#21046;&#26041;&#38754;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#21462;&#24471;&#20102;&#24191;&#27867;&#30340;&#25104;&#21151;&#65292;&#20294;&#20854;&#23545;&#25968;&#25454;&#30340;&#38656;&#27714;&#21644;&#35745;&#31639;&#30340;&#26114;&#36149;&#24615;&#20351;&#20854;&#22312;&#35768;&#22810;&#25968;&#25454;&#21463;&#38480;&#30340;&#30495;&#23454;&#24212;&#29992;&#20013;&#19981;&#23454;&#29992;&#12290;&#23569;&#26679;&#26412;&#23398;&#20064;&#65288;FSL&#65289;&#26088;&#22312;&#36890;&#36807;&#23454;&#29616;&#23545;&#26032;&#23398;&#20064;&#20219;&#21153;&#30340;&#24555;&#36895;&#36866;&#24212;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#24182;&#22312;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#21457;&#23637;&#12290;&#26412;&#35843;&#26597;&#25552;&#20379;&#20102;&#35813;&#39046;&#22495;&#26368;&#26032;&#36827;&#23637;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#39318;&#20808;&#65292;&#27491;&#24335;&#23450;&#20041;&#20102;FSL&#65292;&#24182;&#20171;&#32461;&#20102;&#23427;&#19982;&#19981;&#21516;&#23398;&#20064;&#39046;&#22495;&#30340;&#20851;&#31995;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#25193;&#23637;&#20102;&#20197;&#21069;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#23545;&#32463;&#20856;&#21644;&#26032;&#39046;&#22495;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#36827;&#34892;&#20102;&#25551;&#36848;&#12290;&#26368;&#21518;&#65292;&#35752;&#35770;&#20102;&#22609;&#36896;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#36235;&#21183;&#12289;&#31361;&#20986;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite deep learning's widespread success, its data-hungry and computationally expensive nature makes it impractical for many data-constrained real-world applications. Few-Shot Learning (FSL) aims to address these limitations by enabling rapid adaptation to novel learning tasks, seeing significant growth in recent years. This survey provides a comprehensive overview of the field's latest advancements. Initially, FSL is formally defined, and its relationship with different learning fields is presented. A novel taxonomy is introduced, extending previously proposed ones, and real-world applications in classic and novel fields are described. Finally, recent trends shaping the field, outstanding challenges, and promising future research directions are discussed.
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#31561;&#21464;&#30340;&#23545;&#31216;&#30772;&#32570;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#23545;&#31216;&#30772;&#32570;&#38598;&#26469;&#30772;&#22351;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23545;&#31216;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#29992;&#19988;&#36866;&#29992;&#20110;&#20219;&#20309;&#32676;&#30340;&#31561;&#21464;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02681</link><description>&lt;p&gt;
&#31561;&#21464;&#23545;&#31216;&#30772;&#32570;&#38598;
&lt;/p&gt;
&lt;p&gt;
Equivariant Symmetry Breaking Sets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02681
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#31561;&#21464;&#30340;&#23545;&#31216;&#30772;&#32570;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#23545;&#31216;&#30772;&#32570;&#38598;&#26469;&#30772;&#22351;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23545;&#31216;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#29992;&#19988;&#36866;&#29992;&#20110;&#20219;&#20309;&#32676;&#30340;&#31561;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65288;ENN&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#28041;&#21450;&#28508;&#22312;&#23545;&#31216;&#24615;&#30340;&#24212;&#29992;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#36890;&#36807;&#35774;&#35745;&#65292;ENN&#22312;&#32473;&#23450;&#26356;&#39640;&#23545;&#31216;&#24615;&#36755;&#20837;&#26102;&#26080;&#27861;&#20135;&#29983;&#36739;&#20302;&#23545;&#31216;&#24615;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#29289;&#29702;&#31995;&#32479;&#20013;&#20250;&#21457;&#29983;&#33258;&#21457;&#23545;&#31216;&#30772;&#32570;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;&#19968;&#20010;&#21021;&#22987;&#39640;&#24230;&#23545;&#31216;&#30340;&#29366;&#24577;&#33719;&#24471;&#19968;&#20010;&#36739;&#19981;&#23545;&#31216;&#30340;&#31283;&#23450;&#29366;&#24577;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24517;&#39035;&#20102;&#35299;&#22914;&#20309;&#31995;&#32479;&#22320;&#22312;ENN&#20013;&#30772;&#22351;&#23545;&#31216;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#31561;&#21464;&#30340;&#26032;&#22411;&#23545;&#31216;&#30772;&#32570;&#26694;&#26550;&#12290;&#25105;&#20204;&#24378;&#35843;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36890;&#29992;&#30340;&#65292;&#24182;&#36866;&#29992;&#20110;&#20219;&#20309;&#32676;&#30340;&#31561;&#21464;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#31216;&#30772;&#32570;&#38598;&#65288;SBS&#65289;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#19981;&#26159;&#37325;&#26032;&#35774;&#35745;&#29616;&#26377;&#30340;&#32593;&#32476;&#65292;&#32780;&#26159;&#35774;&#35745;&#20102;&#19968;&#32452;&#23545;&#31216;&#30772;&#32570;&#23545;&#35937;&#65292;&#26681;&#25454;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#23545;&#31216;&#24615;&#23558;&#20854;&#36755;&#20837;&#21040;&#25105;&#20204;&#30340;&#32593;&#32476;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36825;&#20123;&#38598;&#21512;&#19978;&#23450;&#20041;&#31561;&#21464;&#24615;&#30340;&#19968;&#31181;&#33258;&#28982;&#26041;&#24335;&#65292;&#23427;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#32422;&#26463;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;... (the abstract is incomplete and cut off)
&lt;/p&gt;
&lt;p&gt;
Equivariant neural networks (ENNs) have been shown to be extremely effective in applications involving underlying symmetries. By construction ENNs cannot produce lower symmetry outputs given a higher symmetry input. However, spontaneous symmetry breaking occurs in many physical systems and we may obtain a less symmetric stable state from an initial highly symmetric one. Hence, it is imperative that we understand how to systematically break symmetry in ENNs. In this work, we propose a novel symmetry breaking framework that is fully equivariant. We emphasize that our approach is general and applicable to equivariance under any group. To achieve this, we introduce the idea of symmetry breaking sets (SBS). Rather than redesign existing networks, we design sets of symmetry breaking objects which we feed into our network based on the symmetry of our inputs and outputs. We show there is a natural way to define equivariance on these sets, which gives an additional constraint. Minimizing the si
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#31038;&#32676;&#26816;&#27979;&#31639;&#27861;&#25581;&#31034;&#20102;&#20219;&#21153;fMRI&#20998;&#26512;&#31354;&#38388;&#20013;&#30340;&#27969;&#31243;&#31038;&#32676;&#65292;&#24182;&#35780;&#20272;&#20102;&#19981;&#21516;&#32972;&#26223;&#19979;&#30340;&#27969;&#31243;&#20851;&#31995;&#30340;&#31283;&#23450;&#24615;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23384;&#22312;&#19968;&#20123;&#23376;&#38598;&#30340;&#27969;&#31243;&#32473;&#20986;&#30456;&#20284;&#30340;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#20998;&#20139;&#29305;&#23450;&#21442;&#25968;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#20110;&#21442;&#19982;&#32773;&#32676;&#20307;&#26469;&#35828;&#26159;&#31283;&#23450;&#30340;&#65292;&#20294;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#19981;&#31283;&#23450;&#12290;&#27969;&#31243;&#31354;&#38388;&#30340;&#24418;&#25104;&#20027;&#35201;&#21463;&#21040;&#22823;&#33041;&#28608;&#27963;&#21306;&#22495;&#22823;&#23567;&#21644;&#32479;&#35745;&#20540;&#35268;&#27169;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2312.06231</link><description>&lt;p&gt;
&#25581;&#31034;&#20219;&#21153;fMRI&#20998;&#26512;&#31354;&#38388;&#20013;&#30340;&#27969;&#31243;&#31038;&#32676;
&lt;/p&gt;
&lt;p&gt;
Uncovering communities of pipelines in the task-fMRI analytical space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#31038;&#32676;&#26816;&#27979;&#31639;&#27861;&#25581;&#31034;&#20102;&#20219;&#21153;fMRI&#20998;&#26512;&#31354;&#38388;&#20013;&#30340;&#27969;&#31243;&#31038;&#32676;&#65292;&#24182;&#35780;&#20272;&#20102;&#19981;&#21516;&#32972;&#26223;&#19979;&#30340;&#27969;&#31243;&#20851;&#31995;&#30340;&#31283;&#23450;&#24615;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23384;&#22312;&#19968;&#20123;&#23376;&#38598;&#30340;&#27969;&#31243;&#32473;&#20986;&#30456;&#20284;&#30340;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#20998;&#20139;&#29305;&#23450;&#21442;&#25968;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#20110;&#21442;&#19982;&#32773;&#32676;&#20307;&#26469;&#35828;&#26159;&#31283;&#23450;&#30340;&#65292;&#20294;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#19981;&#31283;&#23450;&#12290;&#27969;&#31243;&#31354;&#38388;&#30340;&#24418;&#25104;&#20027;&#35201;&#21463;&#21040;&#22823;&#33041;&#28608;&#27963;&#21306;&#22495;&#22823;&#23567;&#21644;&#32479;&#35745;&#20540;&#35268;&#27169;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#30340;&#20998;&#26512;&#24037;&#20316;&#27969;&#31243;&#20855;&#26377;&#39640;&#24230;&#28789;&#27963;&#24615;&#65292;&#36873;&#25321;&#27969;&#31243;&#30340;&#26368;&#20339;&#23454;&#36341;&#26377;&#38480;&#12290;&#23613;&#31649;&#24050;&#32463;&#26174;&#31034;&#20986;&#20351;&#29992;&#19981;&#21516;&#27969;&#31243;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#30340;&#32467;&#26524;&#65292;&#20294;&#23545;&#20110;&#39537;&#21160;&#36825;&#20123;&#24046;&#24322;&#30340;&#22240;&#32032;&#20197;&#21450;&#36825;&#20123;&#24046;&#24322;&#22312;&#19981;&#21516;&#32972;&#26223;&#19979;&#30340;&#31283;&#23450;&#24615;&#20173;&#28982;&#32570;&#20047;&#29702;&#35299;&#12290;&#25105;&#20204;&#20351;&#29992;&#31038;&#32676;&#26816;&#27979;&#31639;&#27861;&#25506;&#32034;&#27969;&#31243;&#31354;&#38388;&#65292;&#24182;&#35780;&#20272;&#19981;&#21516;&#32972;&#26223;&#19979;&#27969;&#31243;&#20851;&#31995;&#30340;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23384;&#22312;&#19968;&#20123;&#23376;&#38598;&#30340;&#27969;&#31243;&#32473;&#20986;&#30456;&#20284;&#30340;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#20998;&#20139;&#29305;&#23450;&#21442;&#25968;&#65288;&#20363;&#22914;&#36816;&#21160;&#22238;&#24402;&#22120;&#30340;&#25968;&#37327;&#12289;&#36719;&#20214;&#21253;&#31561;&#65289;&#12290;&#36825;&#20123;&#27969;&#31243;&#19982;&#27969;&#31243;&#20043;&#38388;&#30340;&#27169;&#24335;&#22312;&#21442;&#19982;&#32773;&#32676;&#20307;&#20013;&#26159;&#31283;&#23450;&#30340;&#65292;&#20294;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#19981;&#31283;&#23450;&#12290;&#36890;&#36807;&#21487;&#35270;&#21270;&#31038;&#32676;&#38388;&#30340;&#24046;&#24322;&#65292;&#25105;&#20204;&#21457;&#29616;&#27969;&#31243;&#31354;&#38388;&#20027;&#35201;&#21463;&#22823;&#33041;&#28608;&#27963;&#21306;&#22495;&#30340;&#22823;&#23567;&#21644;&#32479;&#35745;&#20540;&#30340;&#35268;&#27169;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analytical workflows in functional magnetic resonance imaging are highly flexible with limited best practices as to how to choose a pipeline. While it has been shown that the use of different pipelines might lead to different results, there is still a lack of understanding of the factors that drive these differences and of the stability of these differences across contexts. We use community detection algorithms to explore the pipeline space and assess the stability of pipeline relationships across different contexts. We show that there are subsets of pipelines that give similar results, especially those sharing specific parameters (e.g. number of motion regressors, software packages, etc.). Those pipeline-to-pipeline patterns are stable across groups of participants but not across different tasks. By visualizing the differences between communities, we show that the pipeline space is mainly driven by the size of the activation area in the brain and the scale of statistic values in stati
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#36229;&#36234;&#19968;&#38454;&#36923;&#36753;&#30340;&#25552;&#21319;&#25512;&#29702;&#38382;&#39064;&#65292;&#25193;&#23637;&#20102;&#35745;&#25968;&#37327;&#35789;&#25193;&#23637;&#30340;&#20004;&#20010;&#21464;&#37327;&#30340;&#19968;&#38454;&#36923;&#36753;&#29255;&#27573;&#30340;&#22495;&#21487;&#25552;&#21319;&#24615;&#65292;&#24182;&#22312;&#38480;&#23450;&#20102;&#20851;&#31995;&#30340;&#24773;&#20917;&#19979;&#25506;&#32034;&#20102;&#19981;&#21516;&#23646;&#24615;&#30340;&#22495;&#21487;&#25552;&#21319;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.11738</link><description>&lt;p&gt;
&#36229;&#20986;&#19968;&#38454;&#36923;&#36753;&#30340;&#25552;&#21319;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Lifted Inference beyond First-Order Logic. (arXiv:2308.11738v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11738
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#36229;&#36234;&#19968;&#38454;&#36923;&#36753;&#30340;&#25552;&#21319;&#25512;&#29702;&#38382;&#39064;&#65292;&#25193;&#23637;&#20102;&#35745;&#25968;&#37327;&#35789;&#25193;&#23637;&#30340;&#20004;&#20010;&#21464;&#37327;&#30340;&#19968;&#38454;&#36923;&#36753;&#29255;&#27573;&#30340;&#22495;&#21487;&#25552;&#21319;&#24615;&#65292;&#24182;&#22312;&#38480;&#23450;&#20102;&#20851;&#31995;&#30340;&#24773;&#20917;&#19979;&#25506;&#32034;&#20102;&#19981;&#21516;&#23646;&#24615;&#30340;&#22495;&#21487;&#25552;&#21319;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32479;&#35745;&#20851;&#31995;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#21152;&#26435;&#19968;&#38454;&#27169;&#22411;&#35745;&#25968;(WFOMC)&#26159;&#27010;&#29575;&#25512;&#29702;&#30340;&#22522;&#30784;&#12290;&#30001;&#20110;WFOMC&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#26159;&#19981;&#21487;&#35745;&#31639;&#30340;&#65288;$\#$P&#23436;&#20840;&#65289;&#65292;&#22240;&#27492;&#33021;&#22815;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36827;&#34892;WFOMC&#30340;&#36923;&#36753;&#30862;&#29255;&#38750;&#24120;&#26377;&#24847;&#20041;&#12290;&#36825;&#26679;&#30340;&#30862;&#29255;&#34987;&#31216;&#20026;&#22495;&#21487;&#25552;&#21319;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35745;&#25968;&#37327;&#35789;&#65288;$\mathrm{C^2}$&#65289;&#25193;&#23637;&#30340;&#20004;&#20010;&#21464;&#37327;&#30340;&#19968;&#38454;&#36923;&#36753;&#29255;&#27573;&#20013;&#65292;&#21487;&#20197;&#36827;&#34892;&#22495;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#23646;&#24615;&#65292;&#22914;&#24341;&#29992;&#32593;&#32476;&#20013;&#30340;&#38750;&#24490;&#29615;&#24615;&#21644;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#36830;&#36890;&#24615;&#65292;&#19981;&#33021;&#22312;$\mathrm{C^2}$&#25110;&#19968;&#38454;&#36923;&#36753;&#20013;&#24314;&#27169;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;$\mathrm{C^2}$&#30340;&#22495;&#21487;&#25552;&#21319;&#24615;&#65292;&#21253;&#25324;&#22810;&#20010;&#36825;&#26679;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#23558;$\mathrm{C^2}$&#21477;&#23376;&#30340;&#19968;&#20010;&#20851;&#31995;&#38480;&#23450;&#20026;&#34920;&#31034;&#26377;&#21521;&#26080;&#29615;&#22270;&#12289;&#36830;&#36890;&#22270;&#12289;&#26641;&#65288;&#25110;&#26377;&#21521;&#26641;&#65289;&#25110;&#26862;&#26519;&#65288;&#25110;&#26377;&#21521;&#26862;&#26519;&#65289;&#26102;&#65292;&#23427;&#20173;&#28982;&#20445;&#25345;&#20102;&#22495;&#21487;&#25552;&#21319;&#24615;&#12290;&#25152;&#26377;&#25105;&#20204;&#30340;&#32467;&#26524;&#37117;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Weighted First Order Model Counting (WFOMC) is fundamental to probabilistic inference in statistical relational learning models. As WFOMC is known to be intractable in general ($\#$P-complete), logical fragments that admit polynomial time WFOMC are of significant interest. Such fragments are called domain liftable. Recent works have shown that the two-variable fragment of first order logic extended with counting quantifiers ($\mathrm{C^2}$) is domain-liftable. However, many properties of real-world data, like acyclicity in citation networks and connectivity in social networks, cannot be modeled in $\mathrm{C^2}$, or first order logic in general. In this work, we expand the domain liftability of $\mathrm{C^2}$ with multiple such properties. We show that any $\mathrm{C^2}$ sentence remains domain liftable when one of its relations is restricted to represent a directed acyclic graph, a connected graph, a tree (resp. a directed tree) or a forest (resp. a directed forest). All our results r
&lt;/p&gt;</description></item></channel></rss>