<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Crescendo&#30340;&#26032;&#22411;&#22810;&#22238;&#21512;&#36234;&#29425;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#30475;&#20284;&#33391;&#24615;&#30340;&#23545;&#35805;&#26041;&#24335;&#36880;&#28176;&#21319;&#32423;&#19982;&#27169;&#22411;&#30340;&#20132;&#20114;&#65292;&#25104;&#21151;&#31361;&#30772;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2404.01833</link><description>&lt;p&gt;
&#20255;&#22823;&#65292;&#29616;&#22312;&#20889;&#19968;&#31687;&#20851;&#20110;&#27492;&#30340;&#25991;&#31456;&#65306;Crescendo&#22810;&#22238;&#21512;LLM&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01833
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Crescendo&#30340;&#26032;&#22411;&#22810;&#22238;&#21512;&#36234;&#29425;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#30475;&#20284;&#33391;&#24615;&#30340;&#23545;&#35805;&#26041;&#24335;&#36880;&#28176;&#21319;&#32423;&#19982;&#27169;&#22411;&#30340;&#20132;&#20114;&#65292;&#25104;&#21151;&#31361;&#30772;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27969;&#34892;&#31243;&#24230;&#22823;&#24133;&#19978;&#21319;&#65292;&#24182;&#19988;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#24212;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#12290;&#36825;&#20123;LLMs&#22312;&#35774;&#35745;&#19978;&#36991;&#20813;&#28041;&#21450;&#38750;&#27861;&#25110;&#19981;&#36947;&#24503;&#30340;&#35805;&#39064;&#65292;&#20197;&#36991;&#20813;&#23545;&#36127;&#36131;&#20219;&#30340;AI&#36896;&#25104;&#20260;&#23475;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#20986;&#29616;&#20102;&#19968;&#31995;&#21015;&#25915;&#20987;&#65292;&#34987;&#31216;&#20026;&#8220;&#36234;&#29425;&#8221;&#65292;&#26088;&#22312;&#31361;&#30772;&#36825;&#31181;&#23545;&#40784;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#36234;&#29425;&#25915;&#20987;&#26088;&#22312;&#32553;&#23567;&#27169;&#22411;&#33021;&#20570;&#30340;&#19982;&#24895;&#24847;&#20570;&#30340;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Crescendo&#30340;&#26032;&#22411;&#36234;&#29425;&#25915;&#20987;&#12290;&#19982;&#29616;&#26377;&#30340;&#36234;&#29425;&#26041;&#27861;&#19981;&#21516;&#65292;Crescendo&#26159;&#19968;&#31181;&#22810;&#22238;&#21512;&#36234;&#29425;&#65292;&#20197;&#19968;&#31181;&#30475;&#20284;&#33391;&#24615;&#30340;&#26041;&#24335;&#19982;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#12290;&#23427;&#20174;&#26377;&#20851;&#25163;&#22836;&#20219;&#21153;&#30340;&#19968;&#33324;&#25552;&#31034;&#25110;&#38382;&#39064;&#24320;&#22987;&#65292;&#28982;&#21518;&#36880;&#28176;&#21319;&#32423;&#23545;&#35805;&#65292;&#24341;&#29992;&#27169;&#22411;&#30340;&#22238;&#22797;&#65292;&#36880;&#28176;&#23548;&#33268;&#25104;&#21151;&#36234;&#29425;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;ChatGPT&#12289;Gemini Pr&#22312;&#20869;&#30340;&#21508;&#31181;&#20844;&#20849;&#31995;&#32479;&#19978;&#35780;&#20272;&#20102;Crescendo&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01833v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have risen significantly in popularity and are increasingly being adopted across multiple applications. These LLMs are heavily aligned to resist engaging in illegal or unethical topics as a means to avoid contributing to responsible AI harms. However, a recent line of attacks, known as "jailbreaks", seek to overcome this alignment. Intuitively, jailbreak attacks aim to narrow the gap between what the model can do and what it is willing to do. In this paper, we introduce a novel jailbreak attack called Crescendo. Unlike existing jailbreak methods, Crescendo is a multi-turn jailbreak that interacts with the model in a seemingly benign manner. It begins with a general prompt or question about the task at hand and then gradually escalates the dialogue by referencing the model's replies, progressively leading to a successful jailbreak. We evaluate Crescendo on various public systems, including ChatGPT, Gemini Pr
&lt;/p&gt;</description></item><item><title>ChatDBG&#26159;&#31532;&#19968;&#20010;AI-Powered&#35843;&#35797;&#21161;&#25163;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#20256;&#32479;&#35843;&#35797;&#22120;&#20013;&#65292;&#23454;&#29616;&#20102;&#31243;&#24207;&#21592;&#19982;&#35843;&#35797;&#22120;&#20043;&#38388;&#30340;&#21327;&#20316;&#23545;&#35805;&#65292;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#12289;&#25191;&#34892;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#65292;&#24182;&#25506;&#32034;&#24320;&#25918;&#24615;&#26597;&#35810;&#12290;</title><link>https://arxiv.org/abs/2403.16354</link><description>&lt;p&gt;
ChatDBG: &#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#35843;&#35797;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
ChatDBG: An AI-Powered Debugging Assistant
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16354
&lt;/p&gt;
&lt;p&gt;
ChatDBG&#26159;&#31532;&#19968;&#20010;AI-Powered&#35843;&#35797;&#21161;&#25163;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#20256;&#32479;&#35843;&#35797;&#22120;&#20013;&#65292;&#23454;&#29616;&#20102;&#31243;&#24207;&#21592;&#19982;&#35843;&#35797;&#22120;&#20043;&#38388;&#30340;&#21327;&#20316;&#23545;&#35805;&#65292;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#12289;&#25191;&#34892;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#65292;&#24182;&#25506;&#32034;&#24320;&#25918;&#24615;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ChatDBG&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#35843;&#35797;&#21161;&#25163;&#12290;ChatDBG&#38598;&#25104;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#20256;&#32479;&#35843;&#35797;&#22120;&#30340;&#21151;&#33021;&#21644;&#29992;&#25143;&#21451;&#22909;&#24615;&#12290;ChatDBG&#20801;&#35768;&#31243;&#24207;&#21592;&#19982;&#35843;&#35797;&#22120;&#36827;&#34892;&#21327;&#20316;&#23545;&#35805;&#65292;&#20351;&#20182;&#20204;&#33021;&#22815;&#25552;&#20986;&#20851;&#20110;&#31243;&#24207;&#29366;&#24577;&#30340;&#22797;&#26434;&#38382;&#39064;&#65292;&#23545;&#23849;&#28291;&#25110;&#26029;&#35328;&#22833;&#36133;&#36827;&#34892;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#65292;&#24182;&#25506;&#32034;&#35832;&#22914;&#8220;&#20026;&#20160;&#20040;x&#20026;&#31354;&#65311;&#8221;&#20043;&#31867;&#30340;&#24320;&#25918;&#24615;&#26597;&#35810;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#20123;&#26597;&#35810;&#65292;ChatDBG&#25480;&#20104;LLM&#33258;&#20027;&#26435;&#65292;&#36890;&#36807;&#21457;&#20986;&#21629;&#20196;&#26469;&#27983;&#35272;&#22534;&#26632;&#21644;&#26816;&#26597;&#31243;&#24207;&#29366;&#24577;&#36827;&#34892;&#35843;&#35797;&#65307;&#28982;&#21518;&#25253;&#21578;&#20854;&#21457;&#29616;&#24182;&#23558;&#25511;&#21046;&#26435;&#20132;&#36824;&#32473;&#31243;&#24207;&#21592;&#12290;&#25105;&#20204;&#30340;ChatDBG&#21407;&#22411;&#19982;&#26631;&#20934;&#35843;&#35797;&#22120;&#38598;&#25104;&#65292;&#21253;&#25324;LLDB&#12289;GDB&#21644;WinDBG&#29992;&#20110;&#26412;&#22320;&#20195;&#30721;&#20197;&#21450;&#29992;&#20110;Python&#30340;Pdb&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#20195;&#30721;&#38598;&#21512;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;&#20855;&#26377;&#24050;&#30693;&#38169;&#35823;&#30340;C/C++&#20195;&#30721;&#21644;&#19968;&#22871;Python&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16354v1 Announce Type: cross  Abstract: This paper presents ChatDBG, the first AI-powered debugging assistant. ChatDBG integrates large language models (LLMs) to significantly enhance the capabilities and user-friendliness of conventional debuggers. ChatDBG lets programmers engage in a collaborative dialogue with the debugger, allowing them to pose complex questions about program state, perform root cause analysis for crashes or assertion failures, and explore open-ended queries like "why is x null?". To handle these queries, ChatDBG grants the LLM autonomy to take the wheel and drive debugging by issuing commands to navigate through stacks and inspect program state; it then reports its findings and yields back control to the programmer. Our ChatDBG prototype integrates with standard debuggers including LLDB, GDB, and WinDBG for native code and Pdb for Python. Our evaluation across a diverse set of code, including C/C++ code with known bugs and a suite of Python code includi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;NTK&#23545;FSCIL&#27169;&#22411;&#30340;&#25351;&#23548;&#65292;&#33268;&#21147;&#20110;&#22312;&#22686;&#37327;&#23398;&#20064;&#20013;&#23454;&#29616;&#21331;&#36234;&#27867;&#21270;&#65292;&#36890;&#36807;&#20248;&#21270;NTK&#25910;&#25947;&#21644;&#38477;&#20302;&#27867;&#21270;&#35823;&#24046;&#26469;&#30830;&#20445;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12486</link><description>&lt;p&gt;
&#22522;&#20110;NTK&#24341;&#23548;&#30340;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
NTK-Guided Few-Shot Class Incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;NTK&#23545;FSCIL&#27169;&#22411;&#30340;&#25351;&#23548;&#65292;&#33268;&#21147;&#20110;&#22312;&#22686;&#37327;&#23398;&#20064;&#20013;&#23454;&#29616;&#21331;&#36234;&#27867;&#21270;&#65292;&#36890;&#36807;&#20248;&#21270;NTK&#25910;&#25947;&#21644;&#38477;&#20302;&#27867;&#21270;&#35823;&#24046;&#26469;&#30830;&#20445;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21453;&#36951;&#24536;FSCIL&#23398;&#20064;&#32773;&#22312;&#22686;&#37327;&#20250;&#35805;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20182;&#20204;&#24448;&#24448;&#26356;&#27880;&#37325;&#20943;&#23569;&#30693;&#35782;&#27969;&#22833;&#65292;&#32780;&#24573;&#35270;&#20102;&#27169;&#22411;&#28508;&#22312;&#33719;&#21462;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#36890;&#36807;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#30340;&#35270;&#35282;&#28145;&#20837;&#25506;&#35752;&#20102;FSCIL&#27169;&#22411;&#27867;&#21270;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#20027;&#35201;&#30340;&#35774;&#35745;&#37325;&#28857;&#22312;&#20110;&#30830;&#20445;&#26368;&#20248;NTK&#25910;&#25947;&#21644;NTK&#30456;&#20851;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#20316;&#20026;&#21331;&#36234;&#27867;&#21270;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#20026;&#20102;&#36798;&#21040;&#20840;&#23616;&#26368;&#20248;&#30340;NTK&#25910;&#25947;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#26893;&#26681;&#20110;&#25968;&#23398;&#21407;&#29702;&#30340;&#20803;&#23398;&#20064;&#26426;&#21046;&#65292;&#25351;&#23548;&#25193;&#23637;&#32593;&#32476;&#20869;&#30340;&#20248;&#21270;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20943;&#23569;NTK&#30456;&#20851;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#25105;&#20204;&#20174;&#22522;&#30784;&#23618;&#38754;&#24320;&#22987;&#65292;&#20248;&#21270;&#26500;&#25104;&#20854;&#27867;&#21270;&#25439;&#22833;&#30340;&#30456;&#20851;&#22240;&#32032;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#22522;&#30784;&#20250;&#35805;&#19978;&#21551;&#21160;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26469;&#22609;&#36896;&#21021;&#22987;ne
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12486v1 Announce Type: cross  Abstract: While anti-amnesia FSCIL learners often excel in incremental sessions, they tend to prioritize mitigating knowledge attrition over harnessing the model's potential for knowledge acquisition. In this paper, we delve into the foundations of model generalization in FSCIL through the lens of the Neural Tangent Kernel (NTK). Our primary design focus revolves around ensuring optimal NTK convergence and NTK-related generalization error, serving as the theoretical bedrock for exceptional generalization. To attain globally optimal NTK convergence, we employ a meta-learning mechanism grounded in mathematical principles to guide the optimization process within an expanded network. Furthermore, to reduce the NTK-related generalization error, we commence from the foundational level, optimizing the relevant factors constituting its generalization loss. Specifically, we initiate self-supervised pre-training on the base session to shape the initial ne
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#23454;&#39564;&#26694;&#26550;&#29992;&#20110;&#35780;&#20272;&#20154;&#31867;&#26159;&#21542;&#36890;&#36807;&#20351;&#29992;AI&#21487;&#20197;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#65292;&#22312;&#21333;&#30450;&#23454;&#39564;&#35774;&#35745;&#20013;&#27604;&#36739;&#20102;&#19977;&#31181;&#20915;&#31574;&#31995;&#32479;&#30340;&#34920;&#29616;</title><link>https://arxiv.org/abs/2403.12108</link><description>&lt;p&gt;
AI&#26159;&#21542;&#26377;&#21161;&#20110;&#20154;&#31867;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#65311;&#19968;&#31181;&#29992;&#20110;&#23454;&#39564;&#35780;&#20272;&#30340;&#26041;&#27861;&#35770;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Does AI help humans make better decisions? A methodological framework for experimental evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12108
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#23454;&#39564;&#26694;&#26550;&#29992;&#20110;&#35780;&#20272;&#20154;&#31867;&#26159;&#21542;&#36890;&#36807;&#20351;&#29992;AI&#21487;&#20197;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#65292;&#22312;&#21333;&#30450;&#23454;&#39564;&#35774;&#35745;&#20013;&#27604;&#36739;&#20102;&#19977;&#31181;&#20915;&#31574;&#31995;&#32479;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#24403;&#20170;&#31038;&#20250;&#21464;&#24471;&#26080;&#22788;&#19981;&#22312;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#23588;&#20854;&#26159;&#24403;&#21033;&#30410;&#39640;&#26114;&#26102;&#65292;&#20154;&#31867;&#20173;&#28982;&#20316;&#20986;&#26368;&#32456;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#20851;&#38190;&#38382;&#39064;&#26159;AI&#26159;&#21542;&#26377;&#21161;&#20110;&#20154;&#31867;&#27604;&#21333;&#29420;&#30340;&#20154;&#31867;&#25110;&#21333;&#29420;&#30340;AI&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#39564;&#24615;&#22320;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#22522;&#20934;&#28508;&#22312;&#32467;&#26524;&#30340;&#26631;&#20934;&#20998;&#31867;&#25351;&#26631;&#27979;&#37327;&#20915;&#31574;&#32773;&#20570;&#20986;&#27491;&#30830;&#20915;&#31574;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#21333;&#30450;&#23454;&#39564;&#35774;&#35745;&#65292;&#22312;&#36825;&#20010;&#35774;&#35745;&#20013;&#65292;&#25552;&#20379;AI&#29983;&#25104;&#30340;&#24314;&#35758;&#22312;&#19981;&#21516;&#26696;&#20363;&#20013;&#34987;&#38543;&#26426;&#20998;&#37197;&#32473;&#26368;&#32456;&#20915;&#31574;&#30340;&#20154;&#31867;&#12290;&#22312;&#36825;&#31181;&#23454;&#39564;&#35774;&#35745;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#27604;&#36739;&#19977;&#31181;&#26367;&#20195;&#20915;&#31574;&#31995;&#32479;&#30340;&#24615;&#33021;--&#20165;&#20154;&#31867;&#12289;&#20154;&#31867;&#19982;AI&#12289;&#20165;AI&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12108v1 Announce Type: new  Abstract: The use of Artificial Intelligence (AI) based on data-driven algorithms has become ubiquitous in today's society. Yet, in many cases and especially when stakes are high, humans still make final decisions. The critical question, therefore, is whether AI helps humans make better decisions as compared to a human alone or AI an alone. We introduce a new methodological framework that can be used to answer experimentally this question with no additional assumptions. We measure a decision maker's ability to make correct decisions using standard classification metrics based on the baseline potential outcome. We consider a single-blinded experimental design, in which the provision of AI-generated recommendations is randomized across cases with a human making final decisions. Under this experimental design, we show how to compare the performance of three alternative decision-making systems--human-alone, human-with-AI, and AI-alone. We apply the pr
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#24212;&#20248;&#21183;&#30340;&#31574;&#30053;&#20248;&#21270;&#65288;A2PO&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#31163;&#32447;&#23398;&#20064;&#65292;&#33021;&#22815;&#35299;&#20915;&#22810;&#34892;&#20026;&#31574;&#30053;&#25910;&#38598;&#30340;&#32422;&#26463;&#20914;&#31361;&#38382;&#39064;&#65292;&#26377;&#25928;&#36991;&#20813;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07262</link><description>&lt;p&gt;
&#36866;&#24212;&#20248;&#21183;&#30340;&#31574;&#30053;&#20248;&#21270;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Advantage-Aware Policy Optimization for Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07262
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#24212;&#20248;&#21183;&#30340;&#31574;&#30053;&#20248;&#21270;&#65288;A2PO&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#31163;&#32447;&#23398;&#20064;&#65292;&#33021;&#22815;&#35299;&#20915;&#22810;&#34892;&#20026;&#31574;&#30053;&#25910;&#38598;&#30340;&#32422;&#26463;&#20914;&#31361;&#38382;&#39064;&#65292;&#26377;&#25928;&#36991;&#20813;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#33268;&#21147;&#20110;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#26469;&#21046;&#23450;&#26377;&#25928;&#30340;&#26234;&#33021;&#20307;&#31574;&#30053;&#65292;&#32780;&#26080;&#38656;&#22312;&#32447;&#20132;&#20114;&#65292;&#36890;&#36807;&#22312;&#34892;&#20026;&#31574;&#30053;&#30340;&#25903;&#25345;&#19979;&#26045;&#21152;&#36866;&#24403;&#30340;&#20445;&#23432;&#32422;&#26463;&#26469;&#35299;&#20915;&#20998;&#24067;&#22806;&#38382;&#39064;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#24212;&#20248;&#21183;&#30340;&#31574;&#30053;&#20248;&#21270;&#65288;A2PO&#65289;&#26041;&#27861;&#65292;&#20197;&#26126;&#30830;&#26500;&#24314;&#38024;&#23545;&#28151;&#21512;&#36136;&#37327;&#25968;&#25454;&#38598;&#30340;&#31163;&#32447;&#23398;&#20064;&#20248;&#21183;&#24863;&#30693;&#31574;&#30053;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07262v1 Announce Type: cross  Abstract: Offline Reinforcement Learning (RL) endeavors to leverage offline datasets to craft effective agent policy without online interaction, which imposes proper conservative constraints with the support of behavior policies to tackle the Out-Of-Distribution (OOD) problem. However, existing works often suffer from the constraint conflict issue when offline datasets are collected from multiple behavior policies, i.e., different behavior policies may exhibit inconsistent actions with distinct returns across the state space. To remedy this issue, recent Advantage-Weighted (AW) methods prioritize samples with high advantage values for agent training while inevitably leading to overfitting on these samples. In this paper, we introduce a novel Advantage-Aware Policy Optimization (A2PO) method to explicitly construct advantage-aware policy constraints for offline learning under mixed-quality datasets. Specifically, A2PO employs a Conditional Variat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#36890;&#29992;&#22270;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27867;&#21270;&#21040;&#19982;&#35757;&#32451;&#25968;&#25454;&#26174;&#33879;&#19981;&#21516;&#30340;&#26410;&#35265;&#22270;&#25968;&#25454;&#26102;&#36935;&#21040;&#30340;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2403.01121</link><description>&lt;p&gt;
OpenGraph: &#36808;&#21521;&#24320;&#25918;&#22270;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OpenGraph: Towards Open Graph Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01121
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#36890;&#29992;&#22270;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27867;&#21270;&#21040;&#19982;&#35757;&#32451;&#25968;&#25454;&#26174;&#33879;&#19981;&#21516;&#30340;&#26410;&#35265;&#22270;&#25968;&#25454;&#26102;&#36935;&#21040;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01121v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#20132;&#20114;   &#25688;&#35201;: &#22270;&#23398;&#20064;&#24050;&#25104;&#20026;&#35299;&#37322;&#21644;&#21033;&#29992;&#21508;&#39046;&#22495;&#30340;&#20851;&#31995;&#25968;&#25454;&#30340;&#19981;&#21487;&#25110;&#32570;&#37096;&#20998;&#65292;&#20174;&#25512;&#33616;&#31995;&#32479;&#21040;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#21508;&#31181;GNN&#24050;&#32463;&#25104;&#20026;&#32534;&#30721;&#22270;&#30340;&#32467;&#26500;&#20449;&#24687;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#35770;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#25429;&#25417;&#22270;&#30340;&#28508;&#22312;&#32467;&#26500;&#65292;&#36825;&#20123;GNN&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#22686;&#24378;&#22270;&#23398;&#20064;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20363;&#22914;&#38142;&#25509;&#39044;&#27979;&#21644;&#33410;&#28857;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#19968;&#20010;&#26174;&#33879;&#30340;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;: &#36825;&#20123;&#20808;&#36827;&#26041;&#27861;&#36890;&#24120;&#22312;&#23558;&#26174;&#33879;&#19981;&#21516;&#20110;&#35757;&#32451;&#23454;&#20363;&#30340;&#26410;&#35265;&#22270;&#25968;&#25454;&#27867;&#21270;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#36890;&#29992;&#22270;&#22522;&#30784;&#27169;&#22411;&#26469;&#25512;&#36827;&#22270;&#23398;&#20064;&#33539;&#24335;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#29702;&#35299;&#22810;&#26679;&#22270;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#22797;&#26434;&#25299;&#25169;&#27169;&#24335;&#65292;&#20351;&#20854;&#22312;&#38646;-shot&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01121v1 Announce Type: cross  Abstract: Graph learning has become indispensable for interpreting and harnessing relational data in diverse fields, ranging from recommendation systems to social network analysis. In this context, a variety of GNNs have emerged as promising methodologies for encoding the structural information of graphs. By effectively capturing the graph's underlying structure, these GNNs have shown great potential in enhancing performance in graph learning tasks, such as link prediction and node classification. However, despite their successes, a significant challenge persists: these advanced methods often face difficulties in generalizing to unseen graph data that significantly differs from the training instances. In this work, our aim is to advance the graph learning paradigm by developing a general graph foundation model. This model is designed to understand the complex topological patterns present in diverse graph data, enabling it to excel in zero-shot g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#23558;&#22768;&#23398;&#35821;&#38899;&#20449;&#24687;&#38598;&#25104;&#21040;LLMs&#26694;&#26550;&#20013;&#65292;&#20197;&#29992;&#20110;&#22810;&#27169;&#24335;&#25233;&#37057;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.13276</link><description>&lt;p&gt;
&#24403;LLMs&#36935;&#21040;&#22768;&#23398;&#26631;&#24535;&#65306;&#19968;&#31181;&#39640;&#25928;&#22320;&#23558;&#35821;&#38899;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#29992;&#20110;&#25233;&#37057;&#26816;&#27979;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
When LLMs Meets Acoustic Landmarks: An Efficient Approach to Integrate Speech into Large Language Models for Depression Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#23558;&#22768;&#23398;&#35821;&#38899;&#20449;&#24687;&#38598;&#25104;&#21040;LLMs&#26694;&#26550;&#20013;&#65292;&#20197;&#29992;&#20110;&#22810;&#27169;&#24335;&#25233;&#37057;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25233;&#37057;&#26159;&#20840;&#29699;&#24515;&#29702;&#20581;&#24247;&#20013;&#30340;&#19968;&#20010;&#20005;&#37325;&#20851;&#20999;&#65292;&#20419;&#20351;&#36827;&#34892;&#22823;&#37327;&#30740;&#31350;&#26469;&#25506;&#35752;&#22522;&#20110;AI&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;&#22312;&#21508;&#31181;AI&#25216;&#26415;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#20854;&#22312;&#24515;&#29702;&#21355;&#29983;&#24212;&#29992;&#20013;&#30340;&#22810;&#21151;&#33021;&#24615;&#32780;&#33073;&#39062;&#32780;&#20986;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20027;&#35201;&#23616;&#38480;&#24615;&#22312;&#20110;&#23427;&#20204;&#20165;&#20381;&#36182;&#20110;&#25991;&#26412;&#36755;&#20837;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#25972;&#20307;&#21151;&#33021;&#12290;&#27492;&#22806;&#65292;LLMs&#22312;&#35782;&#21035;&#21644;&#20998;&#26512;&#25233;&#37057;&#29366;&#24577;&#26041;&#38754;&#30340;&#21033;&#29992;&#20173;&#30456;&#23545;&#26410;&#24320;&#21457;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#23558;&#22768;&#23398;&#35821;&#38899;&#20449;&#24687;&#38598;&#25104;&#21040;LLMs&#26694;&#26550;&#20013;&#65292;&#20197;&#29992;&#20110;&#22810;&#27169;&#24335;&#25233;&#37057;&#26816;&#27979;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22768;&#23398;&#26631;&#24535;&#23558;&#35821;&#38899;&#20449;&#21495;&#38598;&#25104;&#21040;LLMs&#20013;&#30340;&#39640;&#25928;&#25233;&#37057;&#26816;&#27979;&#26041;&#27861;&#12290;&#36890;&#36807;&#25972;&#21512;&#22768;&#23398;&#26631;&#24535;&#65292;&#36825;&#20123;&#26631;&#24535;&#26159;&#29305;&#23450;&#20110;&#21475;&#35821;&#21333;&#35789;&#21457;&#38899;&#30340;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#25991;&#26412;&#36716;&#24405;&#28155;&#21152;&#20102;&#20851;&#38190;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13276v1 Announce Type: cross  Abstract: Depression is a critical concern in global mental health, prompting extensive research into AI-based detection methods. Among various AI technologies, Large Language Models (LLMs) stand out for their versatility in mental healthcare applications. However, their primary limitation arises from their exclusive dependence on textual input, which constrains their overall capabilities. Furthermore, the utilization of LLMs in identifying and analyzing depressive states is still relatively untapped. In this paper, we present an innovative approach to integrating acoustic speech information into the LLMs framework for multimodal depression detection. We investigate an efficient method for depression detection by integrating speech signals into LLMs utilizing Acoustic Landmarks. By incorporating acoustic landmarks, which are specific to the pronunciation of spoken words, our method adds critical dimensions to text transcripts. This integration a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#29983;&#25104;&#30446;&#26631;&#37325;&#23450;&#21521;&#21040;&#23567;&#27874;&#22495;&#65292;&#25105;&#20204;&#25104;&#21151;&#23558;&#35821;&#38899;DDPMs&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#25552;&#39640;&#20102;&#19968;&#20493;&#12290;</title><link>https://arxiv.org/abs/2402.10642</link><description>&lt;p&gt;
&#22312;&#23567;&#27874;&#22495;&#35828;&#35805;&#65306;&#21152;&#36895;&#35821;&#38899;&#25193;&#25955;&#27169;&#22411;&#30340;&#31616;&#21333;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Speaking in Wavelet Domain: A Simple and Efficient Approach to Speed up Speech Diffusion Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10642
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#29983;&#25104;&#30446;&#26631;&#37325;&#23450;&#21521;&#21040;&#23567;&#27874;&#22495;&#65292;&#25105;&#20204;&#25104;&#21151;&#23558;&#35821;&#38899;DDPMs&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#25552;&#39640;&#20102;&#19968;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPMs&#65289;&#22312;&#21508;&#31181;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#22312;&#35821;&#38899;&#21512;&#25104;&#39046;&#22495;&#65292;&#23613;&#31649;DDPMs&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20854;&#38271;&#26102;&#38388;&#35757;&#32451;&#21644;&#22823;&#37327;&#25512;&#29702;&#25104;&#26412;&#38459;&#30861;&#20102;&#23454;&#38469;&#37096;&#32626;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#22686;&#24378;&#25512;&#29702;&#36895;&#24230;&#65292;&#32780;&#21152;&#36895;&#35757;&#32451;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#23545;&#27169;&#22411;&#36827;&#34892;&#22797;&#26434;&#20462;&#25913;&#65292;&#20174;&#32780;&#25439;&#23475;&#20854;&#36890;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#36890;&#36807;&#20462;&#25913;&#35821;&#38899;&#20449;&#21495;&#26412;&#36523;&#65292;&#26159;&#21542;&#21487;&#33021;&#25552;&#39640;DDPMs&#30340;&#35757;&#32451;/&#25512;&#29702;&#36895;&#24230;&#21644;&#24615;&#33021;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#31616;&#21333;&#22320;&#23558;&#29983;&#25104;&#30446;&#26631;&#37325;&#23450;&#21521;&#21040;&#23567;&#27874;&#22495;&#65292;&#23558;&#35821;&#38899;DDPMs&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#25552;&#39640;&#20102;&#19968;&#20493;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#21462;&#24471;&#20102;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10642v1 Announce Type: cross  Abstract: Recently, Denoising Diffusion Probabilistic Models (DDPMs) have attained leading performances across a diverse range of generative tasks. However, in the field of speech synthesis, although DDPMs exhibit impressive performance, their long training duration and substantial inference costs hinder practical deployment. Existing approaches primarily focus on enhancing inference speed, while approaches to accelerate training a key factor in the costs associated with adding or customizing voices often necessitate complex modifications to the model, compromising their universal applicability. To address the aforementioned challenges, we propose an inquiry: is it possible to enhance the training/inference speed and performance of DDPMs by modifying the speech signal itself? In this paper, we double the training and inference speed of Speech DDPMs by simply redirecting the generative target to the wavelet domain. This method not only achieves c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36125;&#21494;&#26031;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34892;&#20026;&#12290;&#30740;&#31350;&#25506;&#32034;&#20102;LLMs&#30340;&#20248;&#21270;&#25351;&#26631;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#39044;&#27979;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#30340;&#34892;&#20026;&#19982;&#36125;&#21494;&#26031;&#23398;&#20064;&#19968;&#33268;&#65292;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.03175</link><description>&lt;p&gt;
The Matrix: &#19968;&#20010;&#29992;&#20110;LLMs&#30340;&#36125;&#21494;&#26031;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
The Matrix: A Bayesian learning model for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36125;&#21494;&#26031;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34892;&#20026;&#12290;&#30740;&#31350;&#25506;&#32034;&#20102;LLMs&#30340;&#20248;&#21270;&#25351;&#26631;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#39044;&#27979;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#30340;&#34892;&#20026;&#19982;&#36125;&#21494;&#26031;&#23398;&#20064;&#19968;&#33268;&#65292;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34892;&#20026;&#30340;&#36125;&#21494;&#26031;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#22522;&#20110;&#39044;&#27979;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;LLM&#30340;&#20248;&#21270;&#25351;&#26631;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20197;&#27492;&#21407;&#21017;&#20026;&#22522;&#30784;&#30340;&#26032;&#22411;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#26500;&#24314;&#19968;&#20010;&#30001;&#20808;&#39564;&#21644;&#22810;&#39033;&#24335;&#36716;&#31227;&#27010;&#29575;&#30697;&#38453;&#34920;&#31034;&#30340;&#29702;&#24819;&#29983;&#25104;&#25991;&#26412;&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;LLMs&#22914;&#20309;&#36924;&#36817;&#35813;&#30697;&#38453;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#23884;&#20837;&#21644;&#22810;&#39033;&#24335;&#20998;&#24067;&#20043;&#38388;&#30340;&#26144;&#23556;&#30340;&#36830;&#32493;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;Dirichlet&#36924;&#36817;&#23450;&#29702;&#26469;&#36924;&#36817;&#20219;&#20309;&#20808;&#39564;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;LLMs&#30340;&#25991;&#26412;&#29983;&#25104;&#22914;&#20309;&#19982;&#36125;&#21494;&#26031;&#23398;&#20064;&#21407;&#29702;&#19968;&#33268;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;&#20854;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;&#65292;&#20855;&#20307;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#22312;&#26356;&#22823;&#30340;&#27169;&#22411;&#20013;&#20986;&#29616;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#20854;&#20013;&#25552;&#31034;&#34987;&#35270;&#20026;&#38656;&#35201;&#26356;&#26032;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;LLMs&#30340;&#34892;&#20026;&#19982;&#36125;&#21494;&#26031;&#23398;&#20064;&#19968;&#33268;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a Bayesian learning model to understand the behavior of Large Language Models (LLMs). We explore the optimization metric of LLMs, which is based on predicting the next token, and develop a novel model grounded in this principle. Our approach involves constructing an ideal generative text model represented by a multinomial transition probability matrix with a prior, and we examine how LLMs approximate this matrix. We discuss the continuity of the mapping between embeddings and multinomial distributions, and present the Dirichlet approximation theorem to approximate any prior. Additionally, we demonstrate how text generation by LLMs aligns with Bayesian learning principles and delve into the implications for in-context learning, specifically explaining why in-context learning emerges in larger models where prompts are considered as samples to be updated. Our findings indicate that the behavior of LLMs is consistent with Bayesian Learning, offering new insights
&lt;/p&gt;</description></item><item><title>HiQA&#26159;&#19968;&#20010;&#20808;&#36827;&#30340;&#22810;&#25991;&#26723;&#38382;&#31572;&#26694;&#26550;&#65292;&#20351;&#29992;&#20998;&#23618;&#30340;&#19978;&#19979;&#25991;&#22686;&#24378;&#21644;&#22810;&#36335;&#24452;&#26816;&#32034;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#25991;&#26723;&#38382;&#31572;&#20013;&#30340;&#26816;&#32034;&#20934;&#30830;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#25991;&#26723;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.01767</link><description>&lt;p&gt;
HiQA&#65306;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#25991;&#26723;&#38382;&#31572;&#30340;&#20998;&#23618;&#19978;&#19979;&#25991;&#22686;&#24378;&#30340;RAG&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
HiQA: A Hierarchical Contextual Augmentation RAG for Massive Documents QA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01767
&lt;/p&gt;
&lt;p&gt;
HiQA&#26159;&#19968;&#20010;&#20808;&#36827;&#30340;&#22810;&#25991;&#26723;&#38382;&#31572;&#26694;&#26550;&#65292;&#20351;&#29992;&#20998;&#23618;&#30340;&#19978;&#19979;&#25991;&#22686;&#24378;&#21644;&#22810;&#36335;&#24452;&#26816;&#32034;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#25991;&#26723;&#38382;&#31572;&#20013;&#30340;&#26816;&#32034;&#20934;&#30830;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#25991;&#26723;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#30340;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#36805;&#36895;&#21457;&#23637;&#65292;&#20351;&#29992;&#34917;&#20805;&#25991;&#26723;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26041;&#27861;&#30340;&#38382;&#31572;&#65288;QA&#65289;&#26041;&#27861;&#23398;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#36825;&#31181;&#36827;&#27493;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#22238;&#31572;&#36136;&#37327;&#65292;&#24182;&#20943;&#36731;&#20102;&#24187;&#35273;&#30340;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#20020;&#22823;&#37327;&#26080;&#27861;&#21306;&#20998;&#30340;&#25991;&#26723;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#26816;&#32034;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#26377;&#38480;&#65292;&#32473;&#23454;&#38469;&#24212;&#29992;&#24102;&#26469;&#20102;&#26174;&#33879;&#25361;&#25112;&#12290;&#38024;&#23545;&#36825;&#20123;&#26032;&#20852;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HiQA&#65292;&#36825;&#26159;&#19968;&#20010;&#20808;&#36827;&#30340;&#22810;&#25991;&#26723;&#38382;&#31572;&#65288;MDQA&#65289;&#26694;&#26550;&#65292;&#23558;&#32423;&#32852;&#30340;&#20803;&#25968;&#25454;&#25972;&#21512;&#21040;&#20869;&#23481;&#20013;&#65292;&#21516;&#26102;&#20855;&#22791;&#22810;&#36335;&#24452;&#26816;&#32034;&#26426;&#21046;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#21517;&#20026;MasQA&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;&#21644;&#30740;&#31350;MDQA&#12290;&#26368;&#21518;&#65292;HiQA&#22312;&#22810;&#25991;&#26723;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As language model agents leveraging external tools rapidly evolve, significant progress has been made in question-answering(QA) methodologies utilizing supplementary documents and the Retrieval-Augmented Generation (RAG) approach. This advancement has improved the response quality of language models and alleviates the appearance of hallucination. However, these methods exhibit limited retrieval accuracy when faced with massive indistinguishable documents, presenting notable challenges in their practical application. In response to these emerging challenges, we present HiQA, an advanced framework for multi-document question-answering (MDQA) that integrates cascading metadata into content as well as a multi-route retrieval mechanism. We also release a benchmark called MasQA to evaluate and research in MDQA. Finally, HiQA demonstrates the state-of-the-art performance in multi-document environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#31995;&#32479;&#24615;&#22320;&#25193;&#23637;&#23460;&#20869;&#29615;&#22659;&#20013;&#30340;3D&#35270;&#35273;-&#35821;&#35328;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#39318;&#20010;&#30334;&#19975;&#35268;&#27169;&#30340;3D&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#38598;SceneVerse&#65292;&#20197;&#35299;&#20915;3D&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#38754;&#20020;&#30340;&#20960;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.09340</link><description>&lt;p&gt;
SceneVerse&#65306;&#20026;&#22522;&#20110;&#22330;&#26223;&#30340;&#22330;&#26223;&#29702;&#35299;&#25193;&#23637;3D&#35270;&#35273;-&#35821;&#35328;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.09340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#31995;&#32479;&#24615;&#22320;&#25193;&#23637;&#23460;&#20869;&#29615;&#22659;&#20013;&#30340;3D&#35270;&#35273;-&#35821;&#35328;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#39318;&#20010;&#30334;&#19975;&#35268;&#27169;&#30340;3D&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#38598;SceneVerse&#65292;&#20197;&#35299;&#20915;3D&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#38754;&#20020;&#30340;&#20960;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#65292;&#21363;&#23558;&#35821;&#35328;&#19982;3D&#29289;&#29702;&#29615;&#22659;&#23545;&#40784;&#65292;&#26159;&#21457;&#23637;&#20855;&#36523;&#20307;&#33021;&#21147;&#30340;&#26234;&#33021;&#20307;&#30340;&#22522;&#30707;&#12290;&#19982;2D&#39046;&#22495;&#26368;&#36817;&#30340;&#36827;&#23637;&#30456;&#27604;&#65292;&#23558;&#35821;&#35328;&#19982;3D&#22330;&#26223;&#23545;&#40784;&#38754;&#20020;&#30528;&#20960;&#20010;&#37325;&#35201;&#25361;&#25112;&#65306;&#65288;i&#65289;3D&#22330;&#26223;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;&#30001;&#20110;&#22810;&#26679;&#30340;&#29289;&#20307;&#37197;&#32622;&#12289;&#20016;&#23500;&#30340;&#23646;&#24615;&#21644;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#65307;&#65288;ii&#65289;&#25903;&#25345;&#22522;&#20110;&#22330;&#26223;&#23398;&#20064;&#30340;&#37197;&#23545;3D&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65307;&#20197;&#21450;&#65288;iii&#65289;&#32570;&#20047;&#20174;&#22522;&#20110;&#22330;&#26223;&#30340;3D&#25968;&#25454;&#20013;&#25552;&#28860;&#30693;&#35782;&#30340;&#32479;&#19968;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#31995;&#32479;&#22320;&#25193;&#23637;&#23460;&#20869;&#29615;&#22659;&#20013;&#30340;3D&#35270;&#35273;-&#35821;&#35328;&#23398;&#20064;&#65292;&#20174;&#32780;&#35299;&#20915;3D&#35270;&#35273;-&#35821;&#35328;&#39046;&#22495;&#20013;&#30340;&#36825;&#19977;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#39318;&#20010;&#30334;&#19975;&#35268;&#27169;&#30340;3D&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#38598;SceneVerse&#65292;&#21253;&#21547;&#32422;68K&#20010;3D&#23460;&#20869;&#22330;&#26223;&#65292;&#21253;&#25324;250&#19975;&#20010;&#35270;&#35273;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.09340v2 Announce Type: replace-cross  Abstract: 3D vision-language grounding, which focuses on aligning language with the 3D physical environment, stands as a cornerstone in the development of embodied agents. In comparison to recent advancements in the 2D domain, grounding language in 3D scenes faces several significant challenges: (i) the inherent complexity of 3D scenes due to the diverse object configurations, their rich attributes, and intricate relationships; (ii) the scarcity of paired 3D vision-language data to support grounded learning; and (iii) the absence of a unified learning framework to distill knowledge from grounded 3D data. In this work, we aim to address these three major challenges in 3D vision-language by examining the potential of systematically upscaling 3D vision-language learning in indoor environments. We introduce the first million-scale 3D vision-language dataset, SceneVerse, encompassing about 68K 3D indoor scenes and comprising 2.5M vision-langu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20132;&#36890;&#39044;&#27979;&#20013;&#24212;&#29992;&#31354;&#38388;-&#26102;&#38388;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#12290;&#30693;&#35782;&#33976;&#39311;&#30340;&#24605;&#24819;&#33021;&#22815;&#23454;&#29616;&#22312;&#20943;&#23569;&#21442;&#25968;&#21644;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#25191;&#34892;&#25928;&#29575;&#12290;&#36890;&#36807;&#24341;&#20837;&#25945;&#24072;&#32593;&#32476;&#30340;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20351;&#23398;&#29983;&#32593;&#32476;&#23398;&#20064;&#21040;&#22797;&#26434;&#30340;&#20132;&#36890;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.11798</link><description>&lt;p&gt;
&#31354;&#38388;-&#26102;&#38388;&#22270;&#21367;&#31215;&#32593;&#32476;&#22312;&#20132;&#36890;&#39044;&#27979;&#19978;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation on Spatial-Temporal Graph Convolutional Network for Traffic Prediction. (arXiv:2401.11798v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20132;&#36890;&#39044;&#27979;&#20013;&#24212;&#29992;&#31354;&#38388;-&#26102;&#38388;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#12290;&#30693;&#35782;&#33976;&#39311;&#30340;&#24605;&#24819;&#33021;&#22815;&#23454;&#29616;&#22312;&#20943;&#23569;&#21442;&#25968;&#21644;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#25191;&#34892;&#25928;&#29575;&#12290;&#36890;&#36807;&#24341;&#20837;&#25945;&#24072;&#32593;&#32476;&#30340;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20351;&#23398;&#29983;&#32593;&#32476;&#23398;&#20064;&#21040;&#22797;&#26434;&#30340;&#20132;&#36890;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#23454;&#26102;&#20132;&#36890;&#39044;&#27979;&#23545;&#20943;&#23569;&#20132;&#36890;&#26102;&#38388;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#39044;&#27979;&#20132;&#36890;&#29366;&#20917;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#31354;&#38388;-&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;ST-GNN&#65289;&#23558;&#23454;&#26102;&#20132;&#36890;&#25968;&#25454;&#24314;&#27169;&#20026;&#26102;&#38388;&#22270;&#12290;&#23613;&#31649;ST-GNN&#20855;&#26377;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#20026;&#23454;&#38469;&#20132;&#36890;&#25968;&#25454;&#36827;&#34892;&#39640;&#25928;&#23454;&#26102;&#39044;&#27979;&#26102;&#32463;&#24120;&#38754;&#20020;&#25361;&#25112;&#12290;&#37492;&#20110;&#23454;&#26102;&#25968;&#25454;&#21160;&#24577;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#37319;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;ST-GNN&#22312;&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#25191;&#34892;&#26102;&#38388;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25104;&#26412;&#20989;&#25968;&#65292;&#26088;&#22312;&#20351;&#29992;&#22797;&#26434;&#32593;&#32476;&#65288;&#25945;&#24072;&#65289;&#30340;&#33976;&#39311;&#25968;&#25454;&#26469;&#35757;&#32451;&#20855;&#26377;&#36739;&#23569;&#21442;&#25968;&#30340;&#32593;&#32476;&#65288;&#23398;&#29983;&#65289;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#20934;&#30830;&#24615;&#25509;&#36817;&#25945;&#24072;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#65292;&#23558;&#25945;&#24072;&#32593;&#32476;&#30340;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#24615;&#34701;&#20837;&#23398;&#29983;&#32593;&#32476;&#65292;&#20351;&#23398;&#29983;&#33021;&#22815;&#23398;&#20064;&#21040;&#25945;&#24072;&#24863;&#30693;&#30340;&#22797;&#26434;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#38754;&#20020;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient real-time traffic prediction is crucial for reducing transportation time. To predict traffic conditions, we employ a spatio-temporal graph neural network (ST-GNN) to model our real-time traffic data as temporal graphs. Despite its capabilities, it often encounters challenges in delivering efficient real-time predictions for real-world traffic data. Recognizing the significance of timely prediction due to the dynamic nature of real-time data, we employ knowledge distillation (KD) as a solution to enhance the execution time of ST-GNNs for traffic prediction. In this paper, We introduce a cost function designed to train a network with fewer parameters (the student) using distilled data from a complex network (the teacher) while maintaining its accuracy close to that of the teacher. We use knowledge distillation, incorporating spatial-temporal correlations from the teacher network to enable the student to learn the complex patterns perceived by the teacher. However, a challenge a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#31232;&#30095;&#21046;&#20316;&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#19987;&#23478;&#28151;&#21512;&#24335;&#26550;&#26500;&#23558;&#23494;&#38598;&#27169;&#22411;&#36716;&#25442;&#20026;&#31232;&#30095;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#22312;&#27169;&#22411;&#23481;&#37327;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#21644;&#27867;&#21270;&#33021;&#21147;&#22686;&#24378;&#12290;</title><link>http://arxiv.org/abs/2401.02731</link><description>&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#31232;&#30095;&#21046;&#20316;&#65306;&#20174;&#23494;&#38598;&#22411;&#21040;&#19987;&#23478;&#28151;&#21512;&#24335;&#29992;&#20110;&#36890;&#29992;&#20219;&#21153;&#30340;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks. (arXiv:2401.02731v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#31232;&#30095;&#21046;&#20316;&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#19987;&#23478;&#28151;&#21512;&#24335;&#26550;&#26500;&#23558;&#23494;&#38598;&#27169;&#22411;&#36716;&#25442;&#20026;&#31232;&#30095;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#22312;&#27169;&#22411;&#23481;&#37327;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#21644;&#27867;&#21270;&#33021;&#21147;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#36890;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#30456;&#24403;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#25351;&#20196;&#35843;&#25972;&#20316;&#20026;&#19968;&#31181;&#25104;&#21151;&#30340;&#33539;&#20363;&#65292;&#22686;&#24378;&#20102;LLMs&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#22411;&#23481;&#37327;&#38480;&#21046;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#32463;&#24120;&#36935;&#21040;&#24615;&#33021;&#38480;&#21046;&#12290;&#22312;&#25351;&#20196;&#35843;&#25972;&#38454;&#27573;&#25193;&#23637;&#27169;&#22411;&#23481;&#37327;&#38754;&#20020;&#30528;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21442;&#25968;&#39640;&#25928;&#31232;&#30095;&#21046;&#20316;(PESC)&#65292;&#23427;&#20351;&#29992;&#19987;&#23478;&#28151;&#21512;&#24335;(MoE)&#26550;&#26500;&#23558;&#23494;&#38598;&#27169;&#22411;&#36716;&#25442;&#20026;&#31232;&#30095;&#27169;&#22411;&#12290;PESC&#23558;&#36866;&#37197;&#22120;&#38598;&#25104;&#21040;&#31232;&#30095;&#27169;&#22411;&#30340;MoE&#23618;&#20013;&#65292;&#21306;&#20998;&#19981;&#21516;&#30340;&#19987;&#23478;&#32780;&#19981;&#25913;&#21464;&#36825;&#20123;&#23618;&#20013;&#30340;&#20010;&#20307;&#26435;&#37325;&#12290;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#21644;GPU&#20869;&#23384;&#38656;&#27714;&#65292;&#36890;&#36807;&#26368;&#23567;&#30340;&#22686;&#21152;&#23454;&#29616;&#20102;&#27169;&#22411;&#23481;&#37327;&#30340;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated considerable proficiency in general natural language processing (NLP) tasks. Instruction tuning, a successful paradigm, enhances the ability of LLMs to follow natural language instructions and exhibit robust generalization across a wide range of tasks. However, these models often encounter performance limitations across multiple tasks due to constrained model capacity. Expanding this capacity during the instruction tuning phase poses significant challenges. To address this issue, we introduce a novel approach, Parameter-Efficient Sparsity Crafting (PESC), which transitions dense models to sparse models using a Mixture of Experts (MoE) architecture. PESC integrates adapters into the MoE layers of sparse models, differentiating experts without altering the individual weights within these layers. This method significantly reduces computational costs and GPU memory requirements, facilitating model capacity expansion through a minimal increase 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#35270;&#35273;&#38382;&#31572;(VQA)&#39046;&#22495;&#30340;&#29616;&#26377;&#30740;&#31350;&#65292;&#21253;&#25324;&#20256;&#32479;VQA&#26550;&#26500;&#21644;&#29616;&#20195;&#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;(VLP)&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#36824;&#20998;&#26512;&#20102;VQA&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#22312;&#21382;&#21490;&#19978;&#30340;&#21457;&#23637;&#65292;&#25581;&#31034;&#20102;VLP&#22312;VQA&#20013;&#30340;&#25361;&#25112;&#19982;&#26426;&#20250;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2311.00308</link><description>&lt;p&gt;
&#20174;&#22270;&#20687;&#21040;&#35821;&#35328;: &#23545;&#35270;&#35273;&#38382;&#31572;(VQA)&#26041;&#27861;&#12289;&#25361;&#25112;&#21644;&#26426;&#20250;&#36827;&#34892;&#30340;&#20851;&#38190;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
From Image to Language: A Critical Analysis of Visual Question Answering (VQA) Approaches, Challenges, and Opportunities. (arXiv:2311.00308v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#35270;&#35273;&#38382;&#31572;(VQA)&#39046;&#22495;&#30340;&#29616;&#26377;&#30740;&#31350;&#65292;&#21253;&#25324;&#20256;&#32479;VQA&#26550;&#26500;&#21644;&#29616;&#20195;&#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;(VLP)&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#36824;&#20998;&#26512;&#20102;VQA&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#22312;&#21382;&#21490;&#19978;&#30340;&#21457;&#23637;&#65292;&#25581;&#31034;&#20102;VLP&#22312;VQA&#20013;&#30340;&#25361;&#25112;&#19982;&#26426;&#20250;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38382;&#31572;(VQA)&#26159;&#19968;&#20010;&#32508;&#21512;&#35745;&#31639;&#26426;&#35270;&#35273;(CV)&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#26088;&#22312;&#23545;&#20219;&#20309;&#35270;&#35273;&#36755;&#20837;&#29983;&#25104;&#31572;&#26696;&#12290;VQA&#30340;&#33539;&#22260;&#24050;&#20174;&#20851;&#27880;&#33258;&#28982;&#22270;&#20687;&#30340;&#25968;&#25454;&#38598;&#25193;&#23637;&#21040;&#21253;&#21547;&#21512;&#25104;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;3D&#29615;&#22659;&#21644;&#20854;&#20182;&#35270;&#35273;&#36755;&#20837;&#30340;&#25968;&#25454;&#38598;&#12290;&#22823;&#22411;&#39044;&#35757;&#32451;&#32593;&#32476;&#30340;&#20986;&#29616;&#20351;&#26089;&#26399;&#20381;&#36182;&#29305;&#24449;&#25552;&#21462;&#21644;&#34701;&#21512;&#26041;&#26696;&#30340;VQA&#26041;&#27861;&#36716;&#21521;&#20102;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;(VLP)&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#32570;&#20047;&#21253;&#25324;&#20256;&#32479;VQA&#26550;&#26500;&#21644;&#29616;&#20195;&#22522;&#20110;VLP&#30340;&#26041;&#27861;&#22312;&#20869;&#30340;&#32508;&#21512;&#35843;&#26597;&#12290;&#27492;&#22806;&#65292;&#36824;&#27809;&#26377;&#23545;VQA&#35270;&#35282;&#19979;&#30340;VLP&#25361;&#25112;&#36827;&#34892;&#28145;&#20837;&#25506;&#35752;&#65292;&#30041;&#19979;&#20102;&#21487;&#33021;&#20986;&#29616;&#28508;&#22312;&#24320;&#25918;&#38382;&#39064;&#30340;&#31354;&#38388;&#12290;&#26412;&#30740;&#31350;&#22312;VQA&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#20221;&#35843;&#26597;&#25253;&#21578;&#65292;&#28145;&#20837;&#25506;&#35752;&#20102;VQA&#25968;&#25454;&#38598;&#21644;&#21382;&#21490;&#20013;&#30340;&#26041;&#27861;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
The multimodal task of Visual Question Answering (VQA) encompassing elements of Computer Vision (CV) and Natural Language Processing (NLP), aims to generate answers to questions on any visual input. Over time, the scope of VQA has expanded from datasets focusing on an extensive collection of natural images to datasets featuring synthetic images, video, 3D environments, and various other visual inputs. The emergence of large pre-trained networks has shifted the early VQA approaches relying on feature extraction and fusion schemes to vision language pre-training (VLP) techniques. However, there is a lack of comprehensive surveys that encompass both traditional VQA architectures and contemporary VLP-based methods. Furthermore, the VLP challenges in the lens of VQA haven't been thoroughly explored, leaving room for potential open problems to emerge. Our work presents a survey in the domain of VQA that delves into the intricacies of VQA datasets and methods over the field's history, introdu
&lt;/p&gt;</description></item><item><title>C-Pack&#26159;&#19968;&#22871;&#25512;&#36827;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#39046;&#22495;&#30340;&#36164;&#28304;&#65292;&#21253;&#25324;&#20840;&#38754;&#27721;&#35821;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#12289;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#38598;&#21644;&#28085;&#30422;&#22810;&#20010;&#23610;&#23544;&#30340;&#23884;&#20837;&#27169;&#22411;&#31995;&#21015;&#12290;&#35813;&#36164;&#28304;&#38598;&#22312;C-MTEB&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#39640;+10%&#30340;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#21644;&#20248;&#21270;&#19968;&#22871;&#35757;&#32451;&#26041;&#27861;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;C-Pack&#36824;&#21457;&#24067;&#20102;&#33521;&#35821;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#21644;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#35813;&#36164;&#28304;&#38598;&#21487;&#20844;&#24320;&#33719;&#21462;&#12290;</title><link>http://arxiv.org/abs/2309.07597</link><description>&lt;p&gt;
C-Pack: &#25512;&#36827;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#30340;&#25171;&#21253;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
C-Pack: Packaged Resources To Advance General Chinese Embedding. (arXiv:2309.07597v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07597
&lt;/p&gt;
&lt;p&gt;
C-Pack&#26159;&#19968;&#22871;&#25512;&#36827;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#39046;&#22495;&#30340;&#36164;&#28304;&#65292;&#21253;&#25324;&#20840;&#38754;&#27721;&#35821;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#12289;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#38598;&#21644;&#28085;&#30422;&#22810;&#20010;&#23610;&#23544;&#30340;&#23884;&#20837;&#27169;&#22411;&#31995;&#21015;&#12290;&#35813;&#36164;&#28304;&#38598;&#22312;C-MTEB&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#39640;+10%&#30340;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#21644;&#20248;&#21270;&#19968;&#22871;&#35757;&#32451;&#26041;&#27861;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;C-Pack&#36824;&#21457;&#24067;&#20102;&#33521;&#35821;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#21644;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#35813;&#36164;&#28304;&#38598;&#21487;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;C-Pack&#65292;&#36825;&#26159;&#19968;&#22871;&#26174;&#33879;&#25512;&#36827;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#39046;&#22495;&#30340;&#36164;&#28304;&#12290;C-Pack&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#36164;&#28304;&#12290;1&#65289;C-MTEB&#26159;&#19968;&#20010;&#28085;&#30422;6&#20010;&#20219;&#21153;&#21644;35&#20010;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#27721;&#35821;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#12290;2&#65289;C-MTP&#26159;&#19968;&#20010;&#20174;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#27721;&#35821;&#35821;&#26009;&#24211;&#20013;&#31574;&#21010;&#30340;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#23884;&#20837;&#27169;&#22411;&#12290;3&#65289;C-TEM&#26159;&#19968;&#20010;&#28085;&#30422;&#22810;&#20010;&#23610;&#23544;&#30340;&#23884;&#20837;&#27169;&#22411;&#31995;&#21015;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;C-MTEB&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20043;&#21069;&#30340;&#25152;&#26377;&#27721;&#35821;&#25991;&#26412;&#23884;&#20837;&#36798;&#21040;&#20102;&#21457;&#24067;&#26102;&#30340;&#26368;&#39640;+10%&#12290;&#25105;&#20204;&#36824;&#25972;&#21512;&#21644;&#20248;&#21270;&#20102;C-TEM&#30340;&#25972;&#22871;&#35757;&#32451;&#26041;&#27861;&#12290;&#38500;&#20102;&#25105;&#20204;&#20851;&#20110;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#30340;&#36164;&#28304;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#33521;&#35821;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#21644;&#27169;&#22411;&#12290;&#36825;&#20123;&#33521;&#35821;&#27169;&#22411;&#22312;MTEB&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65307;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#21457;&#24067;&#30340;&#33521;&#35821;&#25968;&#25454;&#27604;&#27721;&#35821;&#25968;&#25454;&#22823;2&#20493;&#12290;&#25152;&#26377;&#36825;&#20123;&#36164;&#28304;&#37117;&#21487;&#20197;&#22312;https://github.com/FlagOpen/FlagEmbedding&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce C-Pack, a package of resources that significantly advance the field of general Chinese embeddings. C-Pack includes three critical resources. 1) C-MTEB is a comprehensive benchmark for Chinese text embeddings covering 6 tasks and 35 datasets. 2) C-MTP is a massive text embedding dataset curated from labeled and unlabeled Chinese corpora for training embedding models. 3) C-TEM is a family of embedding models covering multiple sizes. Our models outperform all prior Chinese text embeddings on C-MTEB by up to +10% upon the time of the release. We also integrate and optimize the entire suite of training methods for C-TEM. Along with our resources on general Chinese embedding, we release our data and models for English text embeddings. The English models achieve state-of-the-art performance on MTEB benchmark; meanwhile, our released English data is 2 times larger than the Chinese data. All these resources are made publicly available at https://github.com/FlagOpen/FlagEmbedding.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39034;&#24207;&#20307;&#31215;&#35774;&#35745;&#20219;&#21153;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;transformer&#27169;&#22411;&#20174;&#19987;&#23478;&#30340;&#35774;&#35745;&#24207;&#21015;&#20013;&#25552;&#21462;&#26377;&#29992;&#30340;&#34920;&#31034;&#26469;&#25552;&#39640;&#33258;&#21160;&#29983;&#25104;&#20307;&#31215;&#35774;&#35745;&#30340;&#36136;&#37327;&#65292;&#20197;&#21450;&#25903;&#25345;&#35774;&#35745;&#20559;&#22909;&#35780;&#20272;&#21644;&#31243;&#24207;&#21270;&#35774;&#35745;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2309.02583</link><description>&lt;p&gt;
&#39034;&#24207;&#20307;&#31215;&#35774;&#35745;&#20219;&#21153;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning for Sequential Volumetric Design Tasks. (arXiv:2309.02583v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39034;&#24207;&#20307;&#31215;&#35774;&#35745;&#20219;&#21153;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;transformer&#27169;&#22411;&#20174;&#19987;&#23478;&#30340;&#35774;&#35745;&#24207;&#21015;&#20013;&#25552;&#21462;&#26377;&#29992;&#30340;&#34920;&#31034;&#26469;&#25552;&#39640;&#33258;&#21160;&#29983;&#25104;&#20307;&#31215;&#35774;&#35745;&#30340;&#36136;&#37327;&#65292;&#20197;&#21450;&#25903;&#25345;&#35774;&#35745;&#20559;&#22909;&#35780;&#20272;&#21644;&#31243;&#24207;&#21270;&#35774;&#35745;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20307;&#31215;&#35774;&#35745;&#65292;&#20063;&#31216;&#20026;&#36136;&#37327;&#35774;&#35745;&#65292;&#26159;&#19987;&#19994;&#24314;&#31569;&#35774;&#35745;&#20013;&#30340;&#31532;&#19968;&#27493;&#20851;&#38190;&#24615;&#20219;&#21153;&#65292;&#20855;&#26377;&#39034;&#24207;&#24615;&#12290;&#30001;&#20110;&#20307;&#31215;&#35774;&#35745;&#36807;&#31243;&#22797;&#26434;&#65292;&#39034;&#24207;&#21270;&#35774;&#35745;&#36807;&#31243;&#20013;&#21253;&#21547;&#20102;&#23545;&#35774;&#35745;&#24072;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#35768;&#22810;&#21162;&#21147;&#24050;&#32463;&#34987;&#25237;&#20837;&#21040;&#33258;&#21160;&#29983;&#25104;&#21512;&#29702;&#30340;&#20307;&#31215;&#35774;&#35745;&#19978;&#65292;&#20294;&#29983;&#25104;&#30340;&#35774;&#35745;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#19988;&#35780;&#20272;&#19968;&#20010;&#35774;&#35745;&#35299;&#20915;&#26041;&#26696;&#35201;&#20040;&#38656;&#35201;&#19968;&#22871;&#36807;&#20110;&#20840;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#35201;&#20040;&#38656;&#35201;&#26114;&#36149;&#30340;&#20154;&#21147;&#19987;&#19994;&#30693;&#35782;&#12290;&#32780;&#20043;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#23398;&#20064;&#26368;&#32456;&#35774;&#35745;&#65292;&#32780;&#19981;&#26159;&#39034;&#24207;&#35774;&#35745;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#19987;&#23478;&#25110;&#39640;&#24615;&#33021;&#35774;&#35745;&#24207;&#21015;&#30340;&#35774;&#35745;&#30693;&#35782;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#25552;&#21462;&#26377;&#29992;&#30340;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#25152;&#23398;&#30340;&#34920;&#31034;&#22312;&#20851;&#38190;&#30340;&#19979;&#28216;&#24212;&#29992;&#20013;&#65292;&#22914;&#35774;&#35745;&#20559;&#22909;&#35780;&#20272;&#21644;&#31243;&#24207;&#21270;&#35774;&#35745;&#29983;&#25104;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;prefer
&lt;/p&gt;
&lt;p&gt;
Volumetric design, also called massing design, is the first and critical step in professional building design which is sequential in nature. As the volumetric design process is complex, the underlying sequential design process encodes valuable information for designers. Many efforts have been made to automatically generate reasonable volumetric designs, but the quality of the generated design solutions varies, and evaluating a design solution requires either a prohibitively comprehensive set of metrics or expensive human expertise. While previous approaches focused on learning only the final design instead of sequential design tasks, we propose to encode the design knowledge from a collection of expert or high-performing design sequences and extract useful representations using transformer-based models. Later we propose to utilize the learned representations for crucial downstream applications such as design preference evaluation and procedural design generation. We develop the prefere
&lt;/p&gt;</description></item><item><title>GUARD&#26159;&#19968;&#20010;&#24191;&#20041;&#32479;&#19968;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;&#65292;&#26159;&#30446;&#21069;&#24191;&#27867;&#36941;&#24067;&#19988;&#21253;&#21547;&#21508;&#31181;RL&#20195;&#29702;&#12289;&#20219;&#21153;&#21644;&#23433;&#20840;&#32422;&#26463;&#35268;&#33539;&#30340;&#19968;&#31449;&#24335;&#22522;&#20934;&#27979;&#35797;&#65292;&#33021;&#22815;&#20840;&#38754;&#28085;&#30422;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#33258;&#23450;&#20041;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13681</link><description>&lt;p&gt;
GUARD: &#19968;&#20010;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
GUARD: A Safe Reinforcement Learning Benchmark. (arXiv:2305.13681v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13681
&lt;/p&gt;
&lt;p&gt;
GUARD&#26159;&#19968;&#20010;&#24191;&#20041;&#32479;&#19968;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;&#65292;&#26159;&#30446;&#21069;&#24191;&#27867;&#36941;&#24067;&#19988;&#21253;&#21547;&#21508;&#31181;RL&#20195;&#29702;&#12289;&#20219;&#21153;&#21644;&#23433;&#20840;&#32422;&#26463;&#35268;&#33539;&#30340;&#19968;&#31449;&#24335;&#22522;&#20934;&#27979;&#35797;&#65292;&#33021;&#22815;&#20840;&#38754;&#28085;&#30422;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#33258;&#23450;&#20041;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35797;&#38169;&#30340;&#24615;&#36136;&#65292;&#23558;RL&#31639;&#27861;&#24212;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#30340;&#29616;&#23454;&#24212;&#29992;&#65288;&#20363;&#22914;&#33258;&#21160;&#39550;&#39542;&#12289;&#20154;&#26426;&#20132;&#20114;&#12289;&#26426;&#22120;&#20154;&#25805;&#20316;&#31561;&#65289;&#36890;&#24120;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#36825;&#20123;&#38169;&#35823;&#26159;&#19981;&#21487;&#23481;&#24525;&#30340;&#12290;&#26368;&#36817;&#65292;&#23433;&#20840;RL&#65288;&#21363;&#32422;&#26463;RL&#65289;&#24050;&#32463;&#22312;&#25991;&#29486;&#20013;&#36805;&#36895;&#20986;&#29616;&#65292;&#20854;&#20013;&#20195;&#29702;&#22312;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#30340;&#21516;&#26102;&#65292;&#25506;&#32034;&#29615;&#22659;&#12290;&#30001;&#20110;&#31639;&#27861;&#21644;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#65292;&#27604;&#36739;&#29616;&#26377;&#30340;&#23433;&#20840;RL&#31639;&#27861;&#20173;&#28982;&#24456;&#22256;&#38590;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GUARD&#65292;&#19968;&#20010;&#24191;&#20041;&#32479;&#19968;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;&#12290;&#19982;&#29616;&#26377;&#22522;&#20934;&#30456;&#27604;&#65292;GUARD&#20855;&#26377;&#20960;&#20010;&#20248;&#28857;&#12290;&#39318;&#20808;&#65292;GUARD&#26159;&#19968;&#20010;&#24191;&#20041;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;&#65292;&#20855;&#26377;&#21508;&#31181;RL&#20195;&#29702;&#12289;&#20219;&#21153;&#21644;&#23433;&#20840;&#32422;&#26463;&#35268;&#33539;&#12290;&#20854;&#27425;&#65292;GUARD&#20840;&#38754;&#28085;&#30422;&#20102;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#33258;&#21253;&#21547;&#30340;&#23454;&#29616;&#12290;&#31532;&#19977;&#65292;GUARD&#22312;&#20219;&#21153;&#21644;&#31639;&#27861;&#26041;&#38754;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#33258;&#23450;&#20041;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29366;&#24577;&#19979;&#29616;&#26377;&#26041;&#27861;&#22312;GUARD&#19978;&#30340;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the trial-and-error nature, it is typically challenging to apply RL algorithms to safety-critical real-world applications, such as autonomous driving, human-robot interaction, robot manipulation, etc, where such errors are not tolerable. Recently, safe RL (i.e. constrained RL) has emerged rapidly in the literature, in which the agents explore the environment while satisfying constraints. Due to the diversity of algorithms and tasks, it remains difficult to compare existing safe RL algorithms. To fill that gap, we introduce GUARD, a Generalized Unified SAfe Reinforcement Learning Development Benchmark. GUARD has several advantages compared to existing benchmarks. First, GUARD is a generalized benchmark with a wide variety of RL agents, tasks, and safety constraint specifications. Second, GUARD comprehensively covers state-of-the-art safe RL algorithms with self-contained implementations. Third, GUARD is highly customizable in tasks and algorithms. We present a comparison of state
&lt;/p&gt;</description></item></channel></rss>