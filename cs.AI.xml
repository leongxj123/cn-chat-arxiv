<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>TinySaver&#26159;&#19968;&#31181;&#21160;&#24577;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23567;&#22411;&#27169;&#22411;&#26469;&#33258;&#36866;&#24212;&#22320;&#26367;&#25442;&#22823;&#22411;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.17726</link><description>&lt;p&gt;
&#23567;&#22411;&#27169;&#22411;&#26159;&#22823;&#22411;&#27169;&#22411;&#30340;&#35745;&#31639;&#33410;&#30465;&#32773;
&lt;/p&gt;
&lt;p&gt;
Tiny Models are the Computational Saver for Large Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17726
&lt;/p&gt;
&lt;p&gt;
TinySaver&#26159;&#19968;&#31181;&#21160;&#24577;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23567;&#22411;&#27169;&#22411;&#26469;&#33258;&#36866;&#24212;&#22320;&#26367;&#25442;&#22823;&#22411;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;TinySaver&#65292;&#19968;&#31181;&#31867;&#20284;&#20110;&#26089;&#26399;&#36864;&#20986;&#30340;&#21160;&#24577;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#23567;&#22411;&#27169;&#22411;&#26469;&#33258;&#36866;&#24212;&#22320;&#26367;&#25442;&#22823;&#22411;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#30340;&#21387;&#32553;&#25216;&#26415;&#19981;&#21516;&#65292;&#20687;TinySaver&#36825;&#26679;&#30340;&#21160;&#24577;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;&#38590;&#24230;&#24046;&#24322;&#65292;&#20351;&#24471;&#26576;&#20123;&#36755;&#20837;&#33021;&#22815;&#25552;&#21069;&#23436;&#25104;&#25512;&#29702;&#36807;&#31243;&#65292;&#20174;&#32780;&#33410;&#30465;&#35745;&#31639;&#36164;&#28304;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26089;&#26399;&#36864;&#20986;&#35774;&#35745;&#26159;&#36890;&#36807;&#21521;&#27169;&#22411;&#30340;&#39592;&#24178;&#32467;&#26500;&#38468;&#21152;&#39069;&#22806;&#30340;&#32593;&#32476;&#20998;&#25903;&#26469;&#23454;&#29616;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#23436;&#20840;&#29420;&#31435;&#30340;&#23567;&#22411;&#27169;&#22411;&#21487;&#20197;&#22312;&#23545;&#24615;&#33021;&#24433;&#21709;&#26368;&#23567;&#30340;&#24773;&#20917;&#19979;&#26367;&#20195;&#36739;&#22823;&#27169;&#22411;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#12290;&#23558;&#23427;&#20204;&#20316;&#20026;&#31532;&#19968;&#20010;&#36864;&#20986;&#28857;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#36890;&#36807;&#25628;&#32034;&#24182;&#20351;&#29992;&#26368;&#21512;&#36866;&#30340;&#23567;&#22411;&#27169;&#22411;&#20316;&#20026;&#32473;&#23450;&#22823;&#22411;&#27169;&#22411;&#30340;&#35745;&#31639;&#33410;&#30465;&#32773;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17726v1 Announce Type: new  Abstract: This paper introduces TinySaver, an early-exit-like dynamic model compression approach which employs tiny models to substitute large models adaptively. Distinct from traditional compression techniques, dynamic methods like TinySaver can leverage the difficulty differences to allow certain inputs to complete their inference processes early, thereby conserving computational resources. Most existing early exit designs are implemented by attaching additional network branches to the model's backbone. Our study, however, reveals that completely independent tiny models can replace a substantial portion of the larger models' job with minimal impact on performance. Employing them as the first exit can remarkably enhance computational efficiency. By searching and employing the most appropriate tiny model as the computational saver for a given large model, the proposed approaches work as a novel and generic method to model compression. This finding
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; AI &#22312;&#23398;&#20064;&#21644;&#25945;&#32946;&#20013;&#30340;&#22810;&#32500;&#35270;&#35282;&#65292;&#24378;&#35843;&#20102; AI&#12289;&#20998;&#26512;&#21644;&#23398;&#20064;&#36807;&#31243;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25361;&#25112;&#20102;&#23558; AI &#35270;&#20026;&#38543;&#26426;&#24037;&#20855;&#30340;&#35266;&#24565;&#65292;&#24378;&#35843;&#20102; AI &#20316;&#20026;&#29702;&#35299;&#20154;&#31867;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#29420;&#29305;&#30340;&#25945;&#32946;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#27010;&#24565;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.16081</link><description>&lt;p&gt;
&#25945;&#32946;&#20013;&#23398;&#20064;&#12289;&#20998;&#26512;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Interplay of Learning, Analytics, and Artificial Intelligence in Education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; AI &#22312;&#23398;&#20064;&#21644;&#25945;&#32946;&#20013;&#30340;&#22810;&#32500;&#35270;&#35282;&#65292;&#24378;&#35843;&#20102; AI&#12289;&#20998;&#26512;&#21644;&#23398;&#20064;&#36807;&#31243;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25361;&#25112;&#20102;&#23558; AI &#35270;&#20026;&#38543;&#26426;&#24037;&#20855;&#30340;&#35266;&#24565;&#65292;&#24378;&#35843;&#20102; AI &#20316;&#20026;&#29702;&#35299;&#20154;&#31867;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#29420;&#29305;&#30340;&#25945;&#32946;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#27010;&#24565;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#23398;&#20064;&#21644;&#25945;&#32946;&#20013;&#30340;&#22810;&#32500;&#35270;&#35282;&#65292;&#24378;&#35843;&#20154;&#24037;&#26234;&#33021;&#12289;&#20998;&#26512;&#21644;&#23398;&#20064;&#36807;&#31243;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#31508;&#32773;&#25361;&#25112;&#20102;&#23558;&#20154;&#24037;&#26234;&#33021;&#20165;&#35270;&#20026;&#38543;&#26426;&#24037;&#20855;&#30340;&#26222;&#36941;&#35266;&#24565;&#65292;&#20363;&#22914;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65292;&#20027;&#24352;&#37325;&#35270;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#26367;&#20195;&#27010;&#24565;&#12290;&#25991;&#31456;&#31361;&#20986;&#20102;&#20154;&#31867;&#26234;&#33021;&#19982;&#20154;&#24037;&#20449;&#24687;&#22788;&#29702;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;AI&#31639;&#27861;&#20013;&#22266;&#26377;&#30340;&#35748;&#30693;&#22810;&#26679;&#24615;&#65292;&#24182;&#25552;&#20986;AI&#20063;&#21487;&#20197;&#20316;&#20026;&#29702;&#35299;&#20154;&#31867;&#23398;&#20064;&#30340;&#24037;&#20855;&#12290;&#20174;&#23558;AI&#35270;&#20026;&#20154;&#31867;&#26234;&#33021;&#30340;&#31867;&#27604;&#30340;&#26089;&#26399;&#23398;&#20064;&#31185;&#23398;&#21644;&#25945;&#32946;&#20013;&#30340;AI&#30740;&#31350;&#24050;&#32463;&#20559;&#31163;&#36825;&#19968;&#35266;&#28857;&#65292;&#20419;&#20351;&#26377;&#24517;&#35201;&#37325;&#26032;&#28857;&#29123;&#36825;&#31181;&#32852;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#29420;&#29305;&#30340;&#25945;&#32946;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#27010;&#24565;&#21270;&#65306;&#20154;&#31867;&#35748;&#30693;&#30340;&#22806;&#37096;&#21270;&#12289;&#20869;&#21270;AI&#27169;&#22411;&#20197;&#24433;&#21709;&#20154;&#31867;&#24605;&#32500;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16081v1 Announce Type: cross  Abstract: This paper presents a multi dimensional view of AI's role in learning and education, emphasizing the intricate interplay between AI, analytics, and the learning processes. Here, I challenge the prevalent narrow conceptualization of AI as stochastic tools, as exemplified in generative AI, and argue for the importance of alternative conceptualisations of AI. I highlight the differences between human intelligence and artificial information processing, the cognitive diversity inherent in AI algorithms, and posit that AI can also serve as an instrument for understanding human learning. Early learning sciences and AI in Education research, which saw AI as an analogy for human intelligence, have diverged from this perspective, prompting a need to rekindle this connection. The paper presents three unique conceptualizations of AI in education: the externalization of human cognition, the internalization of AI models to influence human thought pr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#25512;&#29702;&#33021;&#21147;&#65292;&#32467;&#21512;&#28145;&#24230;&#20449;&#24687;&#21644;&#35270;&#35273;&#25552;&#31034;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#20855;&#36523;&#38382;&#31572;&#20013;&#30340;&#26377;&#25928;&#25506;&#32034;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.15941</link><description>&lt;p&gt;
&#25506;&#32034;&#30452;&#21040;&#33258;&#20449;: &#38754;&#21521;&#20855;&#36523;&#38382;&#31572;&#30340;&#39640;&#25928;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Explore until Confident: Efficient Exploration for Embodied Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15941
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#25512;&#29702;&#33021;&#21147;&#65292;&#32467;&#21512;&#28145;&#24230;&#20449;&#24687;&#21644;&#35270;&#35273;&#25552;&#31034;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#20855;&#36523;&#38382;&#31572;&#20013;&#30340;&#26377;&#25928;&#25506;&#32034;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#20855;&#36523;&#38382;&#31572;&#65288;EQA&#65289;&#30340;&#38382;&#39064;&#65292;&#36825;&#25351;&#30340;&#26159;&#22312;&#38656;&#35201;&#20027;&#21160;&#25506;&#32034;&#29615;&#22659;&#20197;&#25910;&#38598;&#20449;&#24687;&#30452;&#21040;&#23545;&#38382;&#39064;&#30340;&#31572;&#26696;&#26377;&#33258;&#20449;&#30340;&#20855;&#36523;&#20195;&#29702;&#65292;&#20363;&#22914;&#26426;&#22120;&#20154;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#24378;&#22823;&#35821;&#20041;&#25512;&#29702;&#33021;&#21147;&#26469;&#39640;&#25928;&#25506;&#32034;&#21644;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;EQA&#20013;&#20351;&#29992;VLMs&#26102;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#23427;&#20204;&#27809;&#26377;&#20869;&#37096;&#35760;&#24518;&#23558;&#22330;&#26223;&#26144;&#23556;&#20197;&#20415;&#35268;&#21010;&#22914;&#20309;&#38543;&#26102;&#38388;&#25506;&#32034;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#32622;&#20449;&#24230;&#21487;&#33021;&#34987;&#38169;&#35823;&#26657;&#20934;&#24182;&#21487;&#33021;&#23548;&#33268;&#26426;&#22120;&#20154;&#36807;&#26089;&#20572;&#27490;&#25506;&#32034;&#25110;&#36807;&#24230;&#25506;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#39318;&#20808;&#22522;&#20110;&#28145;&#24230;&#20449;&#24687;&#21644;&#36890;&#36807;&#35270;&#35273;&#25552;&#31034;VLM&#26469;&#26500;&#24314;&#22330;&#26223;&#30340;&#35821;&#20041;&#22320;&#22270;-&#21033;&#29992;&#20854;&#23545;&#22330;&#26223;&#30456;&#20851;&#21306;&#22495;&#30340;&#24191;&#27867;&#30693;&#35782;&#26469;&#36827;&#34892;&#25506;&#32034;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20351;&#29992;&#31526;&#21512;&#39044;&#27979;&#26469;&#26657;&#20934;VLM&#30340;&#32622;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15941v1 Announce Type: cross  Abstract: We consider the problem of Embodied Question Answering (EQA), which refers to settings where an embodied agent such as a robot needs to actively explore an environment to gather information until it is confident about the answer to a question. In this work, we leverage the strong semantic reasoning capabilities of large vision-language models (VLMs) to efficiently explore and answer such questions. However, there are two main challenges when using VLMs in EQA: they do not have an internal memory for mapping the scene to be able to plan how to explore over time, and their confidence can be miscalibrated and can cause the robot to prematurely stop exploration or over-explore. We propose a method that first builds a semantic map of the scene based on depth information and via visual prompting of a VLM - leveraging its vast knowledge of relevant regions of the scene for exploration. Next, we use conformal prediction to calibrate the VLM's 
&lt;/p&gt;</description></item><item><title>Larimar&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#33041;&#21551;&#21457;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#20998;&#24067;&#24335;&#24773;&#33410;&#35760;&#24518;&#22686;&#24378;LLMs&#65292;&#23454;&#29616;&#20102;&#21160;&#24577;&#12289;&#19968;&#27425;&#24615;&#30340;&#30693;&#35782;&#26356;&#26032;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#65292;&#19988;&#22312;&#36895;&#24230;&#21644;&#28789;&#27963;&#24615;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.11901</link><description>&lt;p&gt;
Larimar: &#20855;&#26377;&#24773;&#33410;&#35760;&#24518;&#25511;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Larimar: Large Language Models with Episodic Memory Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11901
&lt;/p&gt;
&lt;p&gt;
Larimar&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#33041;&#21551;&#21457;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#20998;&#24067;&#24335;&#24773;&#33410;&#35760;&#24518;&#22686;&#24378;LLMs&#65292;&#23454;&#29616;&#20102;&#21160;&#24577;&#12289;&#19968;&#27425;&#24615;&#30340;&#30693;&#35782;&#26356;&#26032;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#65292;&#19988;&#22312;&#36895;&#24230;&#21644;&#28789;&#27963;&#24615;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Larimar - &#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#21463;&#22823;&#33041;&#21551;&#21457;&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20998;&#24067;&#24335;&#24773;&#33410;&#35760;&#24518;&#12290; Larimar&#30340;&#35760;&#24518;&#20801;&#35768;&#21160;&#24577;&#12289;&#19968;&#27425;&#24615;&#26356;&#26032;&#30693;&#35782;&#65292;&#26080;&#38656;&#36827;&#34892;&#35745;&#31639;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#22312;&#22810;&#20010;&#20107;&#23454;&#32534;&#36753;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Larimar&#22312;&#36895;&#24230;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322; - &#26681;&#25454;&#22522;&#30784;LLM&#30340;&#19981;&#21516;&#65292;&#36895;&#24230;&#25552;&#21319;&#20026;4-10&#20493;&#65292;&#24182;&#19988;&#30001;&#20110;&#25552;&#20986;&#30340;&#26550;&#26500;&#31616;&#21333;&#12289;&#19981;&#20381;&#36182;&#20110;LLM&#65292;&#22240;&#27492;&#20855;&#26377;&#33391;&#22909;&#30340;&#28789;&#27963;&#24615;&#21644;&#36890;&#29992;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#36873;&#25321;&#24615;&#20107;&#23454;&#36951;&#24536;&#21644;&#36755;&#20837;&#19978;&#19979;&#25991;&#38271;&#24230;&#27010;&#25324;&#26426;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11901v1 Announce Type: cross  Abstract: Efficient and accurate updating of knowledge stored in Large Language Models (LLMs) is one of the most pressing research challenges today. This paper presents Larimar - a novel, brain-inspired architecture for enhancing LLMs with a distributed episodic memory. Larimar's memory allows for dynamic, one-shot updates of knowledge without the need for computationally expensive re-training or fine-tuning. Experimental results on multiple fact editing benchmarks demonstrate that Larimar attains accuracy comparable to most competitive baselines, even in the challenging sequential editing setup, but also excels in speed - yielding speed-ups of 4-10x depending on the base LLM - as well as flexibility due to the proposed architecture being simple, LLM-agnostic, and hence general. We further provide mechanisms for selective fact forgetting and input context length generalization with Larimar and show their effectiveness.
&lt;/p&gt;</description></item><item><title>DIALECTBENCH&#26159;&#31532;&#19968;&#20010;&#38754;&#21521;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26041;&#35328;&#12289;&#35821;&#35328;&#21464;&#20307;&#21644;&#23494;&#20999;&#30456;&#20851;&#35821;&#35328;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#65292;&#20026;&#23454;&#29616;&#23545;&#19981;&#21516;&#35821;&#35328;&#21464;&#20307;&#19978;NLP&#31995;&#32479;&#24615;&#33021;&#30340;&#20840;&#38754;&#35780;&#20272;&#25552;&#20379;&#20102;&#37325;&#35201;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2403.11009</link><description>&lt;p&gt;
DIALECTBENCH&#65306;&#19968;&#20010;&#26041;&#35328;&#12289;&#35821;&#35328;&#21464;&#20307;&#21644;&#23494;&#20999;&#30456;&#20851;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
DIALECTBENCH: A NLP Benchmark for Dialects, Varieties, and Closely-Related Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11009
&lt;/p&gt;
&lt;p&gt;
DIALECTBENCH&#26159;&#31532;&#19968;&#20010;&#38754;&#21521;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26041;&#35328;&#12289;&#35821;&#35328;&#21464;&#20307;&#21644;&#23494;&#20999;&#30456;&#20851;&#35821;&#35328;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#65292;&#20026;&#23454;&#29616;&#23545;&#19981;&#21516;&#35821;&#35328;&#21464;&#20307;&#19978;NLP&#31995;&#32479;&#24615;&#33021;&#30340;&#20840;&#38754;&#35780;&#20272;&#25552;&#20379;&#20102;&#37325;&#35201;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11009v1 &#35821;&#31181;&#65306;&#20132;&#21449;  &#25688;&#35201;&#65306;&#35821;&#35328;&#25216;&#26415;&#24212;&#35813;&#26681;&#25454;&#20854;&#22312;&#23454;&#38469;&#29992;&#20363;&#20013;&#30340;&#26377;&#29992;&#24615;&#26469;&#21028;&#26029;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30740;&#31350;&#21644;&#35780;&#20272;&#20013;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#19968;&#20010;&#26041;&#38754;&#26159;&#38750;&#26631;&#20934;&#26041;&#35328;&#25110;&#35821;&#35328;&#21464;&#20307;&#65288;&#20197;&#19979;&#31616;&#31216;&#20026;&#21464;&#20307;&#65289;&#24418;&#24335;&#30340;&#35821;&#35328;&#21464;&#20307;. &#22823;&#22810;&#25968;NLP&#22522;&#20934;&#27979;&#35797;&#20165;&#38480;&#20110;&#26631;&#20934;&#35821;&#35328;&#21464;&#20307;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DIALECTBENCH&#65292;&#36825;&#26159;&#39318;&#20010;&#38024;&#23545;&#35821;&#35328;&#21464;&#20307;&#30340;&#22823;&#35268;&#27169;NLP&#22522;&#20934;&#27979;&#35797;&#65292;&#27719;&#24635;&#20102;&#19968;&#31995;&#21015;&#22810;&#26679;&#20219;&#21153;&#26679;&#26412;&#30340;&#21464;&#20307;&#25968;&#25454;&#38598;&#65288;&#28085;&#30422;281&#31181;&#21464;&#20307;&#30340;10&#20010;&#25991;&#26412;&#32423;&#20219;&#21153;&#65289;&#12290;&#36825;&#20801;&#35768;&#23545;&#19981;&#21516;&#35821;&#35328;&#21464;&#20307;&#19978;NLP&#31995;&#32479;&#24615;&#33021;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22823;&#37327;&#35777;&#25454;&#34920;&#26126;&#26631;&#20934;&#35821;&#35328;&#21464;&#20307;&#19982;&#38750;&#26631;&#20934;&#35821;&#35328;&#21464;&#20307;&#20043;&#38388;&#23384;&#22312;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#19988;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#22312;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#22823;&#37327;&#24615;&#33021;&#24046;&#36317;&#30340;&#35821;&#35328;&#31867;&#32676;&#12290;&#25105;&#20204;&#35748;&#20026;DIALECTBENCH&#25552;&#20379;&#20102;&#23545;&#24403;&#21069;&#35821;&#35328;NLP&#29366;&#24577;&#30340;&#20840;&#38754;&#35270;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11009v1 Announce Type: cross  Abstract: Language technologies should be judged on their usefulness in real-world use cases. An often overlooked aspect in natural language processing (NLP) research and evaluation is language variation in the form of non-standard dialects or language varieties (hereafter, varieties). Most NLP benchmarks are limited to standard language varieties. To fill this gap, we propose DIALECTBENCH, the first-ever large-scale benchmark for NLP on varieties, which aggregates an extensive set of task-varied variety datasets (10 text-level tasks covering 281 varieties). This allows for a comprehensive evaluation of NLP system performance on different language varieties. We provide substantial evidence of performance disparities between standard and non-standard language varieties, and we also identify language clusters with large performance divergence across tasks. We believe DIALECTBENCH provides a comprehensive view of the current state of NLP for langua
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;2D&#39640;&#26031;&#21943;&#28034;&#23454;&#29616;&#22270;&#20687;&#34920;&#31034;&#21644;&#21387;&#32553;&#65292;&#22312;GPU&#20869;&#23384;&#21344;&#29992;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#20102;&#26356;&#24555;&#30340;&#28210;&#26579;&#36895;&#24230;&#65292;&#24182;&#22312;&#34920;&#31034;&#24615;&#33021;&#19978;&#19982;INR&#30456;&#21305;&#25932;&#12290;</title><link>https://arxiv.org/abs/2403.08551</link><description>&lt;p&gt;
&#39640;&#26031;&#22270;&#20687;&#65306;&#36890;&#36807;2D&#39640;&#26031;&#21943;&#28034;&#36827;&#34892;1000&#24103;&#27599;&#31186;&#30340;&#22270;&#20687;&#34920;&#31034;&#21644;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
GaussianImage: 1000 FPS Image Representation and Compression by 2D Gaussian Splatting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08551
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;2D&#39640;&#26031;&#21943;&#28034;&#23454;&#29616;&#22270;&#20687;&#34920;&#31034;&#21644;&#21387;&#32553;&#65292;&#22312;GPU&#20869;&#23384;&#21344;&#29992;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#20102;&#26356;&#24555;&#30340;&#28210;&#26579;&#36895;&#24230;&#65292;&#24182;&#22312;&#34920;&#31034;&#24615;&#33021;&#19978;&#19982;INR&#30456;&#21305;&#25932;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INR&#65289;&#22312;&#22270;&#20687;&#34920;&#31034;&#21644;&#21387;&#32553;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#25552;&#20379;&#20102;&#39640;&#35270;&#35273;&#36136;&#37327;&#21644;&#24555;&#36895;&#28210;&#26579;&#36895;&#24230;&#65292;&#27599;&#31186;10-1000&#24103;&#65292;&#20551;&#35774;&#26377;&#36275;&#22815;&#30340;GPU&#36164;&#28304;&#21487;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35201;&#27714;&#24120;&#24120;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#20869;&#23384;&#26377;&#38480;&#30340;&#20302;&#31471;&#35774;&#22791;&#19978;&#30340;&#20351;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;2D&#39640;&#26031;&#21943;&#28034;&#36827;&#34892;&#22270;&#20687;&#34920;&#31034;&#21644;&#21387;&#32553;&#30340;&#24320;&#21019;&#24615;&#33539;&#24335;&#65292;&#21517;&#20026;GaussianImage&#12290;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;2D&#39640;&#26031;&#26469;&#34920;&#31034;&#22270;&#20687;&#65292;&#20854;&#20013;&#27599;&#20010;&#39640;&#26031;&#20855;&#26377;8&#20010;&#21442;&#25968;&#65292;&#21253;&#25324;&#20301;&#32622;&#12289;&#21327;&#26041;&#24046;&#21644;&#39068;&#33394;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#32047;&#31215;&#27714;&#21644;&#30340;&#26032;&#39062;&#28210;&#26579;&#31639;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;GPU&#20869;&#23384;&#33267;&#23569;&#38477;&#20302;3&#20493;&#65292;&#25311;&#21512;&#26102;&#38388;&#24555;5&#20493;&#65292;&#19981;&#20165;&#22312;&#34920;&#31034;&#24615;&#33021;&#19978;&#19982;INR&#65288;&#20363;&#22914;WIRE&#65292;I-NGP&#65289;&#19981;&#30456;&#19978;&#19979;&#65292;&#32780;&#19988;&#26080;&#35770;&#21442;&#25968;&#22823;&#23567;&#22914;&#20309;&#37117;&#33021;&#25552;&#20379;1500-2000&#24103;&#27599;&#31186;&#30340;&#26356;&#24555;&#28210;&#26579;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08551v1 Announce Type: cross  Abstract: Implicit neural representations (INRs) recently achieved great success in image representation and compression, offering high visual quality and fast rendering speeds with 10-1000 FPS, assuming sufficient GPU resources are available. However, this requirement often hinders their use on low-end devices with limited memory. In response, we propose a groundbreaking paradigm of image representation and compression by 2D Gaussian Splatting, named GaussianImage. We first introduce 2D Gaussian to represent the image, where each Gaussian has 8 parameters including position, covariance and color. Subsequently, we unveil a novel rendering algorithm based on accumulated summation. Remarkably, our method with a minimum of 3$\times$ lower GPU memory usage and 5$\times$ faster fitting time not only rivals INRs (e.g., WIRE, I-NGP) in representation performance, but also delivers a faster rendering speed of 1500-2000 FPS regardless of parameter size. 
&lt;/p&gt;</description></item><item><title>SMART&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#28151;&#21512;&#31574;&#30053;&#65292;&#21033;&#29992;&#23376;&#27169;&#22359;&#20989;&#25968;&#20026;&#20219;&#21153;&#20998;&#37197;&#37325;&#35201;&#24615;&#20998;&#25968;&#65292;&#24182;&#22312;&#24494;&#35843;&#20013;&#37325;&#26032;&#20998;&#37197;&#39044;&#31639;&#65292;&#20174;&#32780;&#22312;&#25351;&#20196;&#35843;&#25972;&#20219;&#21153;&#20013;&#21462;&#24471;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.08370</link><description>&lt;p&gt;
SMART: &#29992;&#20110;&#25351;&#20196;&#35843;&#25972;&#30340;&#23376;&#27169;&#22359;&#25968;&#25454;&#28151;&#21512;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
SMART: Submodular Data Mixture Strategy for Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08370
&lt;/p&gt;
&lt;p&gt;
SMART&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#28151;&#21512;&#31574;&#30053;&#65292;&#21033;&#29992;&#23376;&#27169;&#22359;&#20989;&#25968;&#20026;&#20219;&#21153;&#20998;&#37197;&#37325;&#35201;&#24615;&#20998;&#25968;&#65292;&#24182;&#22312;&#24494;&#35843;&#20013;&#37325;&#26032;&#20998;&#37197;&#39044;&#31639;&#65292;&#20174;&#32780;&#22312;&#25351;&#20196;&#35843;&#25972;&#20219;&#21153;&#20013;&#21462;&#24471;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#28041;&#21450;&#22312;&#19968;&#32452;&#20197;&#25351;&#20196;&#26684;&#24335;&#21270;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#24179;&#34913;&#19981;&#21516;&#20219;&#21153;&#27604;&#20363;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#25214;&#21040;&#21512;&#36866;&#30340;&#24179;&#34913;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30446;&#21069;&#38500;&#20102;&#25163;&#21160;&#35843;&#25972;&#25110;&#20381;&#36182;&#20174;&#19994;&#32773;&#30340;&#30452;&#35273;&#22806;&#65292;&#23578;&#26080;&#31995;&#32479;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SMART&#65288;Submodular data Mixture strAtegy for instRuction Tuning&#65289;- &#19968;&#31181;&#21033;&#29992;&#23376;&#27169;&#22359;&#20989;&#25968;&#20026;&#20219;&#21153;&#20998;&#37197;&#37325;&#35201;&#24615;&#20998;&#25968;&#30340;&#26032;&#39062;&#25968;&#25454;&#28151;&#21512;&#31574;&#30053;&#65292;&#28982;&#21518;&#29992;&#36825;&#20123;&#20998;&#25968;&#26469;&#30830;&#23450;&#28151;&#21512;&#26435;&#37325;&#12290;&#32473;&#23450;&#24494;&#35843;&#39044;&#31639;&#65292;SMART&#37325;&#26032;&#20998;&#37197;&#20219;&#21153;&#38388;&#30340;&#39044;&#31639;&#65292;&#24182;&#20174;&#27599;&#20010;&#20219;&#21153;&#20013;&#36873;&#25321;&#38750;&#20887;&#20313;&#26679;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SMART&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#22914;&#20363;&#23376;&#27604;&#20363;&#28151;&#21512;&#21644;&#22343;&#31561;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08370v1 Announce Type: cross  Abstract: Instruction Tuning involves finetuning a language model on a collection of instruction-formatted datasets in order to enhance the generalizability of the model to unseen tasks. Studies have shown the importance of balancing different task proportions during finetuning, but finding the right balance remains challenging. Unfortunately, there's currently no systematic method beyond manual tuning or relying on practitioners' intuition. In this paper, we introduce SMART (Submodular data Mixture strAtegy for instRuction Tuning) - a novel data mixture strategy which makes use of a submodular function to assign importance scores to tasks which are then used to determine the mixture weights. Given a fine-tuning budget, SMART redistributes the budget among tasks and selects non-redundant samples from each task. Experimental results demonstrate that SMART significantly outperforms traditional methods such as examples proportional mixing and equal
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;&#23558;&#26465;&#20214;&#35745;&#31639;&#26041;&#27861;&#24212;&#29992;&#20110;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#20852;&#39046;&#22495;&#20013;&#30340;&#21407;&#29702;&#21644;&#24605;&#24819;&#65292;&#24182;&#20171;&#32461;&#20102;&#19987;&#23478;&#28151;&#21512;&#32593;&#32476;&#12289;&#26631;&#35760;&#36873;&#25321;&#26426;&#21046;&#21644;&#25552;&#21069;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#31561;&#19977;&#31181;&#23454;&#29616;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.07965</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26465;&#20214;&#35745;&#31639;: &#21407;&#29702;&#19982;&#30740;&#31350;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
Conditional computation in neural networks: principles and research trends
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07965
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;&#23558;&#26465;&#20214;&#35745;&#31639;&#26041;&#27861;&#24212;&#29992;&#20110;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#20852;&#39046;&#22495;&#20013;&#30340;&#21407;&#29702;&#21644;&#24605;&#24819;&#65292;&#24182;&#20171;&#32461;&#20102;&#19987;&#23478;&#28151;&#21512;&#32593;&#32476;&#12289;&#26631;&#35760;&#36873;&#25321;&#26426;&#21046;&#21644;&#25552;&#21069;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#31561;&#19977;&#31181;&#23454;&#29616;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#24635;&#32467;&#20102;&#23558;&#26465;&#20214;&#35745;&#31639;&#26041;&#27861;&#24212;&#29992;&#20110;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#20852;&#39046;&#22495;&#20013;&#30340;&#21407;&#29702;&#21644;&#24605;&#24819;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#21487;&#20197;&#26681;&#25454;&#36755;&#20837;&#21160;&#24577;&#28608;&#27963;&#25110;&#21435;&#28608;&#27963;&#20854;&#35745;&#31639;&#22270;&#37096;&#20998;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#20363;&#22914;&#65292;&#21160;&#24577;&#36873;&#25321;&#36755;&#20837;&#26631;&#35760;&#12289;&#23618;&#65288;&#25110;&#19968;&#32452;&#23618;&#65289;&#20197;&#21450;&#27599;&#20010;&#23618;&#20869;&#30340;&#23376;&#27169;&#22359;&#65288;&#20363;&#22914;&#65292;&#21367;&#31215;&#28388;&#27874;&#22120;&#20013;&#30340;&#36890;&#36947;&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#19968;&#20010;&#36890;&#29992;&#24418;&#24335;&#26469;&#32479;&#19968;&#25551;&#36848;&#36825;&#20123;&#25216;&#26415;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#36825;&#20123;&#21407;&#21017;&#30340;&#19977;&#20010;&#20540;&#24471;&#27880;&#24847;&#30340;&#23454;&#29616;&#65306;&#19987;&#23478;&#28151;&#21512;&#65288;MoEs&#65289;&#32593;&#32476;&#12289;&#26631;&#35760;&#36873;&#25321;&#26426;&#21046;&#21644;&#25552;&#21069;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#12290;&#26412;&#25991;&#26088;&#22312;&#21521;&#36825;&#19968;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#25552;&#20379;&#31867;&#20284;&#25945;&#31243;&#30340;&#20171;&#32461;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#27169;&#22359;&#21270;&#35774;&#35745;&#22312;&#25928;&#29575;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#36801;&#31227;&#23398;&#20064;&#26041;&#38754;&#30340;&#22909;&#22788;&#65292;&#37325;&#28857;&#25918;&#22312;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07965v1 Announce Type: cross  Abstract: This article summarizes principles and ideas from the emerging area of applying \textit{conditional computation} methods to the design of neural networks. In particular, we focus on neural networks that can dynamically activate or de-activate parts of their computational graph conditionally on their input. Examples include the dynamic selection of, e.g., input tokens, layers (or sets of layers), and sub-modules inside each layer (e.g., channels in a convolutional filter). We first provide a general formalism to describe these techniques in an uniform way. Then, we introduce three notable implementations of these principles: mixture-of-experts (MoEs) networks, token selection mechanisms, and early-exit neural networks. The paper aims to provide a tutorial-like introduction to this growing field. To this end, we analyze the benefits of these modular designs in terms of efficiency, explainability, and transfer learning, with a focus on em
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25581;&#31034;&#20102;&#22312;&#26576;&#20123;&#20219;&#21153;&#31867;&#21035;&#20013;&#65292;&#25945;&#24072;&#24378;&#21046;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#22312;&#31532;&#19968;&#26102;&#38388;&#23398;&#20064;&#21040;&#20934;&#30830;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#65292;&#36827;&#32780;&#23548;&#33268;&#27169;&#22411;&#22833;&#36133;&#30340;&#19968;&#33324;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.06963</link><description>&lt;p&gt;
&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30340;&#38519;&#38449;
&lt;/p&gt;
&lt;p&gt;
The pitfalls of next-token prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06963
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25581;&#31034;&#20102;&#22312;&#26576;&#20123;&#20219;&#21153;&#31867;&#21035;&#20013;&#65292;&#25945;&#24072;&#24378;&#21046;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#22312;&#31532;&#19968;&#26102;&#38388;&#23398;&#20064;&#21040;&#20934;&#30830;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#65292;&#36827;&#32780;&#23548;&#33268;&#27169;&#22411;&#22833;&#36133;&#30340;&#19968;&#33324;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31687;&#20851;&#20110;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30340;&#35770;&#25991;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30452;&#35266;&#30340;&#25285;&#24551;&#65306;&#19968;&#20010;&#20165;&#20165;&#22522;&#20110;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30340;&#27169;&#22411;&#26159;&#21542;&#33021;&#24544;&#23454;&#22320;&#27169;&#25311;&#20154;&#31867;&#26234;&#33021;&#12290;&#25105;&#20204;&#35748;&#20026;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20013;&#32463;&#24120;&#28151;&#28102;&#30340;&#20004;&#20010;&#38454;&#27573; -- &#33258;&#22238;&#24402;&#25512;&#26029;&#21644;&#25945;&#24072;&#24378;&#21046;&#35757;&#32451; -- &#24517;&#39035;&#34987;&#21306;&#21035;&#23545;&#24453;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#19968;&#33324;&#26426;&#21046;&#65292;&#23637;&#31034;&#20102;&#25945;&#24072;&#24378;&#21046;&#22914;&#20309;&#22833;&#36133;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#26368;&#23567;&#21270;&#35745;&#21010;&#20219;&#21153;&#65292;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;Transformer&#21644;Mamba&#26550;&#26500;&#22312;&#23454;&#36341;&#20013;&#20197;&#36825;&#31181;&#26041;&#24335;&#22833;&#36133; -- &#23613;&#31649;&#20219;&#21153;&#26412;&#36523;&#24456;&#23481;&#26131;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06963v1 Announce Type: cross  Abstract: Can a mere next-token predictor faithfully model human intelligence? We crystallize this intuitive concern, which is fragmented in the literature. As a starting point, we argue that the two often-conflated phases of next-token prediction -- autoregressive inference and teacher-forced training -- must be treated distinctly. The popular criticism that errors can compound during autoregressive inference, crucially assumes that teacher-forcing has learned an accurate next-token predictor. This assumption sidesteps a more deep-rooted problem we expose: in certain classes of tasks, teacher-forcing can simply fail to learn an accurate next-token predictor in the first place. We describe a general mechanism of how teacher-forcing can fail, and design a minimal planning task where both the Transformer and the Mamba architecture empirically fail in that manner -- remarkably, despite the task being straightforward to learn. We provide preliminary
&lt;/p&gt;</description></item><item><title>DECIDER&#26159;&#19968;&#31181;&#21463;&#21452;&#31995;&#32479;&#35748;&#30693;&#29702;&#35770;&#21551;&#21457;&#30340;&#35268;&#21017;&#21487;&#25511;&#35299;&#30721;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#36923;&#36753;&#25512;&#29702;&#22120;&#65292;&#26377;&#25928;&#22320;&#36981;&#24490;&#32473;&#23450;&#35268;&#21017;&#20197;&#24341;&#23548;&#29983;&#25104;&#26041;&#21521;&#26397;&#21521;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.01954</link><description>&lt;p&gt;
DECIDERS&#65306;&#19968;&#31181;&#36890;&#36807;&#27169;&#20223;&#21452;&#31995;&#32479;&#35748;&#30693;&#29702;&#35770;&#23454;&#29616;&#35268;&#21017;&#21487;&#25511;&#35299;&#30721;&#31574;&#30053;&#30340;&#35821;&#35328;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DECIDER: A Rule-Controllable Decoding Strategy for Language Generation by Imitating Dual-System Cognitive Theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01954
&lt;/p&gt;
&lt;p&gt;
DECIDER&#26159;&#19968;&#31181;&#21463;&#21452;&#31995;&#32479;&#35748;&#30693;&#29702;&#35770;&#21551;&#21457;&#30340;&#35268;&#21017;&#21487;&#25511;&#35299;&#30721;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#36923;&#36753;&#25512;&#29702;&#22120;&#65292;&#26377;&#25928;&#22320;&#36981;&#24490;&#32473;&#23450;&#35268;&#21017;&#20197;&#24341;&#23548;&#29983;&#25104;&#26041;&#21521;&#26397;&#21521;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#20856;&#32422;&#26463;&#35299;&#30721;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#26576;&#20123;&#30446;&#26631;&#27010;&#24565;&#25511;&#21046;&#25152;&#29983;&#25104;&#25991;&#26412;&#30340;&#24847;&#20041;&#25110;&#39118;&#26684;&#12290;&#29616;&#26377;&#26041;&#27861;&#36807;&#20110;&#20851;&#27880;&#36825;&#20123;&#30446;&#26631;&#26412;&#36523;&#65292;&#23548;&#33268;&#32570;&#20047;&#20851;&#20110;&#22914;&#20309;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#30340;&#39640;&#23618;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#36890;&#24120;&#36890;&#36807;&#36981;&#24490;&#26576;&#20123;&#35268;&#21017;&#26469;&#22788;&#29702;&#20219;&#21153;&#65292;&#36825;&#20123;&#35268;&#21017;&#19981;&#20165;&#20851;&#27880;&#20110;&#30446;&#26631;&#26412;&#36523;&#65292;&#36824;&#20851;&#27880;&#20110;&#24341;&#21457;&#30446;&#26631;&#21457;&#29983;&#30340;&#35821;&#20041;&#30456;&#20851;&#27010;&#24565;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DECIDER&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#21040;&#21452;&#31995;&#32479;&#35748;&#30693;&#29702;&#35770;&#21551;&#21457;&#30340;&#32422;&#26463;&#35821;&#35328;&#29983;&#25104;&#30340;&#35268;&#21017;&#21487;&#25511;&#35299;&#30721;&#31574;&#30053;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;DECIDER&#20013;&#65292;&#19968;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#37197;&#22791;&#20102;&#19968;&#20010;&#36923;&#36753;&#25512;&#29702;&#22120;&#65292;&#20197;&#39640;&#23618;&#35268;&#21017;&#20316;&#20026;&#36755;&#20837;&#12290;&#28982;&#21518;&#65292;DECIDER&#20801;&#35768;&#35268;&#21017;&#20449;&#21495;&#22312;&#27599;&#20010;&#35299;&#30721;&#27493;&#39588;&#20013;&#27969;&#20837;PLM&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DECIDER&#33021;&#22815;&#26377;&#25928;&#22320;&#36981;&#24490;&#32473;&#23450;&#30340;&#35268;&#21017;&#65292;&#24341;&#23548;&#29983;&#25104;&#26041;&#21521;&#26397;&#21521;&#30446;&#26631;&#36827;&#34892;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01954v1 Announce Type: cross  Abstract: Lexicon-based constrained decoding approaches aim to control the meaning or style of the generated text through certain target concepts. Existing approaches over-focus the targets themselves, leading to a lack of high-level reasoning about how to achieve them. However, human usually tackles tasks by following certain rules that not only focuses on the targets but also on semantically relevant concepts that induce the occurrence of targets. In this work, we present DECIDER, a rule-controllable decoding strategy for constrained language generation inspired by dual-system cognitive theory. Specifically, in DECIDER, a pre-trained language model (PLM) is equiped with a logic reasoner that takes high-level rules as input. Then, the DECIDER allows rule signals to flow into the PLM at each decoding step. Extensive experimental results demonstrate that DECIDER can effectively follow given rules to guide generation direction toward the targets i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Clifford-Steerable&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CS-CNNs&#65289;&#65292;&#36890;&#36807;&#22312;&#20266;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#19978;&#22788;&#29702;&#22810;&#30690;&#22330;&#65292;&#21033;&#29992;Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#23545;$\mathrm{O}(p,q)$&#21487;&#23548;&#26680;&#36827;&#34892;&#38544;&#24335;&#21442;&#25968;&#21270;&#65292;&#26174;&#30528;&#19988;&#19968;&#33268;&#22320;&#20248;&#20110;&#27969;&#20307;&#21160;&#21147;&#23398;&#21644;&#30456;&#23545;&#35770;&#30005;&#21160;&#21147;&#23398;&#39044;&#27979;&#20219;&#21153;&#30340;&#22522;&#20934;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.14730</link><description>&lt;p&gt;
Clifford-Steerable&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Clifford-Steerable Convolutional Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14730
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Clifford-Steerable&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CS-CNNs&#65289;&#65292;&#36890;&#36807;&#22312;&#20266;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#19978;&#22788;&#29702;&#22810;&#30690;&#22330;&#65292;&#21033;&#29992;Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#23545;$\mathrm{O}(p,q)$&#21487;&#23548;&#26680;&#36827;&#34892;&#38544;&#24335;&#21442;&#25968;&#21270;&#65292;&#26174;&#30528;&#19988;&#19968;&#33268;&#22320;&#20248;&#20110;&#27969;&#20307;&#21160;&#21147;&#23398;&#21644;&#30456;&#23545;&#35770;&#30005;&#21160;&#21147;&#23398;&#39044;&#27979;&#20219;&#21153;&#30340;&#22522;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Clifford-Steerable&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CS-CNNs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;$\mathrm{E}(p, q)$&#31561;&#21464;CNN&#31867;&#12290; CS-CNNs&#22312;&#20266;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;$\mathbb{R}^{p,q}$&#19978;&#22788;&#29702;&#22810;&#30690;&#22330;&#12290; &#23427;&#20204;&#28085;&#30422;&#20102;&#20363;&#22914;$\mathrm{E}(3)$&#22312;$\mathbb{R}^3$&#19978;&#21644;Poincar\'e&#22312;&#38389;&#21487;&#22827;&#26031;&#22522;&#26102;&#31354;$\mathbb{R}^{1,3}$&#19978;&#30340;&#31561;&#21464;&#24615;&#12290; &#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#36890;&#36807;Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#23545;$\mathrm{O}(p,q)$&#21487;&#23548;&#26680;&#36827;&#34892;&#38544;&#24335;&#21442;&#25968;&#21270;&#12290; &#22312;&#27969;&#20307;&#21160;&#21147;&#23398;&#21644;&#30456;&#23545;&#35770;&#30005;&#21160;&#21147;&#23398;&#39044;&#27979;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#22312;&#22522;&#20934;&#26041;&#27861;&#19978;&#26174;&#30528;&#19988;&#19968;&#33268;&#22320;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14730v1 Announce Type: cross  Abstract: We present Clifford-Steerable Convolutional Neural Networks (CS-CNNs), a novel class of $\mathrm{E}(p, q)$-equivariant CNNs. CS-CNNs process multivector fields on pseudo-Euclidean spaces $\mathbb{R}^{p,q}$. They cover, for instance, $\mathrm{E}(3)$-equivariance on $\mathbb{R}^3$ and Poincar\'e-equivariance on Minkowski spacetime $\mathbb{R}^{1,3}$. Our approach is based on an implicit parametrization of $\mathrm{O}(p,q)$-steerable kernels via Clifford group equivariant neural networks. We significantly and consistently outperform baseline methods on fluid dynamics as well as relativistic electrodynamics forecasting tasks.
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#19968;&#31181;&#22312;&#21160;&#24577;&#28216;&#25103;&#20013;&#23558;&#25968;&#25454;&#39537;&#21160;&#21442;&#32771;&#25919;&#31574;&#19982;&#22522;&#20110;&#20248;&#21270;&#21338;&#24328;&#25919;&#31574;&#30456;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21512;&#20316;&#21160;&#24577;&#21338;&#24328;KLGame&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#38024;&#23545;&#27599;&#20010;&#20915;&#31574;&#32773;&#30340;&#21487;&#35843;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.14174</link><description>&lt;p&gt;
&#22312;&#21160;&#24577;&#28216;&#25103;&#20013;&#34701;&#21512;&#25968;&#25454;&#39537;&#21160;&#30340;&#20808;&#39564;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Blending Data-Driven Priors in Dynamic Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14174
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#19968;&#31181;&#22312;&#21160;&#24577;&#28216;&#25103;&#20013;&#23558;&#25968;&#25454;&#39537;&#21160;&#21442;&#32771;&#25919;&#31574;&#19982;&#22522;&#20110;&#20248;&#21270;&#21338;&#24328;&#25919;&#31574;&#30456;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21512;&#20316;&#21160;&#24577;&#21338;&#24328;KLGame&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#38024;&#23545;&#27599;&#20010;&#20915;&#31574;&#32773;&#30340;&#21487;&#35843;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26234;&#33021;&#26426;&#22120;&#20154;&#22914;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#20154;&#32676;&#20013;&#30340;&#37096;&#32626;&#36234;&#26469;&#36234;&#22810;&#65292;&#36825;&#20123;&#31995;&#32479;&#24212;&#35813;&#22312;&#23433;&#20840;&#30340;&#12289;&#19982;&#20154;&#20114;&#21160;&#24847;&#35782;&#30456;&#20851;&#30340;&#36816;&#21160;&#35268;&#21010;&#20013;&#21033;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#21338;&#24328;&#35770;&#35268;&#21010;&#22120;&#19982;&#25968;&#25454;&#39537;&#21160;&#25919;&#31574;&#30340;&#31243;&#24230;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#34701;&#21512;&#25968;&#25454;&#39537;&#21160;&#21442;&#32771;&#25919;&#31574;&#21644;&#22522;&#20110;&#20248;&#21270;&#30340;&#21338;&#24328;&#35770;&#25919;&#31574;&#30340;&#21407;&#21017;&#24615;&#26041;&#27861;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;KLGame&#65292;&#36825;&#26159;&#19968;&#31181;&#24102;&#26377;Kullback-Leibler&#65288;KL&#65289;&#27491;&#21017;&#21270;&#30340;&#38750;&#21512;&#20316;&#21160;&#24577;&#21338;&#24328;&#65292;&#38024;&#23545;&#19968;&#20010;&#19968;&#33324;&#30340;&#12289;&#38543;&#26426;&#30340;&#65292;&#21487;&#33021;&#26159;&#22810;&#27169;&#24335;&#30340;&#21442;&#32771;&#25919;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14174v1 Announce Type: cross  Abstract: As intelligent robots like autonomous vehicles become increasingly deployed in the presence of people, the extent to which these systems should leverage model-based game-theoretic planners versus data-driven policies for safe, interaction-aware motion planning remains an open question. Existing dynamic game formulations assume all agents are task-driven and behave optimally. However, in reality, humans tend to deviate from the decisions prescribed by these models, and their behavior is better approximated under a noisy-rational paradigm. In this work, we investigate a principled methodology to blend a data-driven reference policy with an optimization-based game-theoretic policy. We formulate KLGame, a type of non-cooperative dynamic game with Kullback-Leibler (KL) regularization with respect to a general, stochastic, and possibly multi-modal reference policy. Our method incorporates, for each decision maker, a tunable parameter that pe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#35328;&#25351;&#20196;&#22320;&#38754;&#21270;&#36816;&#21160;&#35268;&#21010;&#65288;LIMP&#65289;&#31995;&#32479;&#65292;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#21644;&#26102;&#38388;&#36923;&#36753;&#29983;&#25104;&#25351;&#20196;&#26465;&#20214;&#30340;&#35821;&#20041;&#22320;&#22270;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#21487;&#39564;&#35777;&#22320;&#36981;&#24490;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#38271;&#26399;&#30340;&#25351;&#20196;&#65292;&#21253;&#25324;&#24320;&#25918;&#35789;&#27719;&#21442;&#29031;&#21644;&#22797;&#26434;&#30340;&#26102;&#31354;&#32422;&#26463;&#12290;</title><link>https://arxiv.org/abs/2402.11498</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#21487;&#39564;&#35777;&#22320;&#36981;&#24490;&#22797;&#26434;&#26426;&#22120;&#20154;&#25351;&#20196;
&lt;/p&gt;
&lt;p&gt;
Verifiably Following Complex Robot Instructions with Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11498
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#35328;&#25351;&#20196;&#22320;&#38754;&#21270;&#36816;&#21160;&#35268;&#21010;&#65288;LIMP&#65289;&#31995;&#32479;&#65292;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#21644;&#26102;&#38388;&#36923;&#36753;&#29983;&#25104;&#25351;&#20196;&#26465;&#20214;&#30340;&#35821;&#20041;&#22320;&#22270;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#21487;&#39564;&#35777;&#22320;&#36981;&#24490;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#38271;&#26399;&#30340;&#25351;&#20196;&#65292;&#21253;&#25324;&#24320;&#25918;&#35789;&#27719;&#21442;&#29031;&#21644;&#22797;&#26434;&#30340;&#26102;&#31354;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35753;&#26426;&#22120;&#20154;&#33021;&#22815;&#36981;&#24490;&#22797;&#26434;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20154;&#20204;&#24076;&#26395;&#22312;&#25351;&#23548;&#26426;&#22120;&#20154;&#26102;&#33021;&#22815;&#28789;&#27963;&#34920;&#36798;&#32422;&#26463;&#65292;&#25351;&#21521;&#20219;&#24847;&#22320;&#26631;&#24182;&#39564;&#35777;&#34892;&#20026;&#12290;&#30456;&#21453;&#65292;&#26426;&#22120;&#20154;&#24517;&#39035;&#23558;&#20154;&#31867;&#25351;&#20196;&#28040;&#38500;&#27495;&#20041;&#65292;&#23558;&#25351;&#20196;&#21442;&#29031;&#29289;&#32852;&#31995;&#21040;&#30495;&#23454;&#19990;&#30028;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#35328;&#25351;&#20196;&#22320;&#38754;&#21270;&#36816;&#21160;&#35268;&#21010;&#65288;LIMP&#65289;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#21644;&#26102;&#38388;&#36923;&#36753;&#29983;&#25104;&#25351;&#20196;&#26465;&#20214;&#30340;&#35821;&#20041;&#22320;&#22270;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#21487;&#39564;&#35777;&#22320;&#36981;&#24490;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#38271;&#26399;&#30340;&#25351;&#20196;&#65292;&#28085;&#30422;&#20102;&#24320;&#25918;&#35789;&#27719;&#21442;&#29031;&#21644;&#22797;&#26434;&#30340;&#26102;&#31354;&#32422;&#26463;&#12290;&#19982;&#20808;&#21069;&#22312;&#26426;&#22120;&#20154;&#20219;&#21153;&#25191;&#34892;&#20013;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;LIMP&#26500;&#24314;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#25351;&#20196;&#34920;&#31034;&#65292;&#25581;&#31034;&#20102;&#26426;&#22120;&#20154;&#19982;&#25351;&#23548;&#32773;&#39044;&#26399;&#21160;&#26426;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#32508;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11498v1 Announce Type: cross  Abstract: Enabling robots to follow complex natural language instructions is an important yet challenging problem. People want to flexibly express constraints, refer to arbitrary landmarks and verify behavior when instructing robots. Conversely, robots must disambiguate human instructions into specifications and ground instruction referents in the real world. We propose Language Instruction grounding for Motion Planning (LIMP), a system that leverages foundation models and temporal logics to generate instruction-conditioned semantic maps that enable robots to verifiably follow expressive and long-horizon instructions with open vocabulary referents and complex spatiotemporal constraints. In contrast to prior methods for using foundation models in robot task execution, LIMP constructs an explainable instruction representation that reveals the robot's alignment with an instructor's intended motives and affords the synthesis of robot behaviors that 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;PANDA&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#26356;&#31934;&#30830;&#30340;&#31572;&#26696;&#27491;&#30830;&#24615;&#35780;&#27979;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#33258;&#21160;&#35780;&#20272;&#38382;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.11161</link><description>&lt;p&gt;
PANDA&#65288;Pedantic ANswer-correctness Determination and Adjudication&#65289;&#65306;&#25913;&#36827;&#38382;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#30340;&#33258;&#21160;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
PANDA (Pedantic ANswer-correctness Determination and Adjudication):Improving Automatic Evaluation for Question Answering and Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11161
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PANDA&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#26356;&#31934;&#30830;&#30340;&#31572;&#26696;&#27491;&#30830;&#24615;&#35780;&#27979;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#33258;&#21160;&#35780;&#20272;&#38382;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#31572;&#65288;QA&#65289;&#21482;&#26377;&#22312;&#25105;&#20204;&#30693;&#36947;&#31572;&#26696;&#26159;&#21542;&#27491;&#30830;&#26102;&#25165;&#33021;&#21462;&#24471;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#35768;&#22810;&#26368;&#20855;&#25361;&#25112;&#24615;&#21644;&#26377;&#36259;&#30340;QA&#31034;&#20363;&#65292;&#24403;&#21069;&#30340;&#31572;&#26696;&#27491;&#30830;&#24615;&#65288;AC&#65289;&#25351;&#26631;&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#65292;&#29305;&#21035;&#26159;&#26469;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20887;&#38271;&#12289;&#33258;&#30001;&#26684;&#24335;&#31572;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#25361;&#25112;&#65306;&#32570;&#20047;&#25968;&#25454;&#21644;&#27169;&#22411;&#36807;&#22823;&#12290;&#22522;&#20110;LLM&#30340;&#35780;&#20998;&#22120;&#19982;&#20154;&#31867;&#26356;&#22909;&#22320;&#30456;&#20851;&#65292;&#20294;&#36825;&#39033;&#26114;&#36149;&#30340;&#20219;&#21153;&#20165;&#22312;&#26377;&#38480;&#30340;QA&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#28165;&#26224;&#30340;&#25351;&#21335;&#26469;&#35780;&#20272;&#20174;&#20154;&#31867;QA&#27604;&#36187;&#20013;&#37319;&#32435;&#30340;&#26426;&#22120;QA&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#31934;&#30830;&#30340;&#31572;&#26696;&#27491;&#30830;&#24615;&#30830;&#23450;&#21644;&#35009;&#20915;&#65288;Precise ANswer correctness Determination and Adjudication&#65292;PANDA&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#23567;&#24039;&#12289;&#39640;&#25928;&#12289;&#30830;&#23450;&#24615;&#30340;AC&#20998;&#31867;&#22120;&#65288;812 KB&#65289;&#65292;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#31572;&#26696;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11161v1 Announce Type: cross  Abstract: Question answering (QA) can only make progress if we know if an answer is correct, but for many of the most challenging and interesting QA examples, current answer correctness (AC) metrics do not align with human judgments, particularly verbose, free form answers from large language models (LLM). There are two challenges: a lack of data and that models are too big. LLM based scorers correlate better with humans, but this expensive task has only been tested on limited QA datasets. We rectify these issues by providing clear guidelines for evaluating machine QA adopted from human QA contests. We also introduce Precise ANswer correctness Determination and Adjudication (PANDA), a small, efficient, deterministic AC classifier (812 KB) that more accurately evaluates answer correctness.
&lt;/p&gt;</description></item><item><title>&#20998;&#38548;&#31526;&#30340;&#24341;&#20837;&#22312;&#24605;&#32500;&#38142;&#25552;&#31034;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.10645</link><description>&lt;p&gt;
&#20998;&#38548;&#31526;&#26159;&#21542;&#21487;&#20197;&#25552;&#39640;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#25928;&#26524;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Separators Improve Chain-of-Thought Prompting?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10645
&lt;/p&gt;
&lt;p&gt;
&#20998;&#38548;&#31526;&#30340;&#24341;&#20837;&#22312;&#24605;&#32500;&#38142;&#25552;&#31034;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Chain-of-thought (CoT) prompting&#26159;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;CoT&#30340;&#22522;&#26412;&#29702;&#24565;&#26159;&#36890;&#36807;&#23558;&#31034;&#20363;&#25918;&#22312;&#36755;&#20837;&#25552;&#31034;&#20013;&#65292;&#35753;LLMs&#36880;&#27493;&#25286;&#35299;&#20182;&#20204;&#30340;&#24605;&#32500;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;CoT&#25552;&#31034;&#30340;&#23494;&#38598;&#32467;&#26500;&#21487;&#33021;&#23548;&#33268;LLMs&#30340;&#35748;&#30693;&#36127;&#33655;&#36807;&#37325;&#12290;&#21463;&#20154;&#31867;&#35748;&#30693;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CoT-Sep&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#22312;CoT&#25552;&#31034;&#20013;&#27599;&#20010;&#31034;&#20363;&#30340;&#26411;&#23614;&#31574;&#30053;&#24615;&#22320;&#24212;&#29992;&#20998;&#38548;&#31526;&#12290;&#36825;&#20123;&#20998;&#38548;&#31526;&#26088;&#22312;&#24110;&#21161;LLMs&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#26356;&#22909;&#22320;&#29702;&#35299;&#20182;&#20204;&#30340;&#24605;&#32500;&#36807;&#31243;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#19981;&#20351;&#29992;&#20998;&#38548;&#31526;&#30340;&#26222;&#36890;CoT&#30456;&#27604;&#65292;CoT-Sep&#26174;&#33879;&#25552;&#39640;&#20102;LLMs&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;GSM-8K&#12289;AQuA&#12289;CSQA&#65289;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19981;&#21516;&#31867;&#22411;&#21644;&#20301;&#32622;&#30340;&#20998;&#38548;&#31526;&#23545;&#22810;&#20010;LLMs&#65288;&#21253;&#25324;GPT-3.5-Turbo&#12289;GPT-4&#21644;LLaMA-27&#65289;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10645v1 Announce Type: cross  Abstract: Chain-of-thought (CoT) prompting is a simple and effective method for improving the reasoning capabilities of Large language models (LLMs). The basic idea of CoT is to let LLMs break down their thought processes step-by-step by putting exemplars in the input prompt. However, the densely structured prompt exemplars of CoT may cause the cognitive overload of LLMs. Inspired by human cognition, we introduce CoT-Sep, a novel method that strategically employs separators at the end of each exemplar in CoT prompting. These separators are designed to help the LLMs understand their thought processes better while reasoning. It turns out that CoT-Sep significantly improves the LLMs' performances on complex reasoning tasks (e.g., GSM-8K, AQuA, CSQA), compared with the vanilla CoT, which does not use separators. We also study the effects of the type and the location of separators tested on multiple LLMs, including GPT-3.5-Turbo, GPT-4, and LLaMA-2 7
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#33021;&#22815;&#25104;&#21151;&#22320;&#21046;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#20197;&#24858;&#24324;&#23433;&#20840;&#25514;&#26045;&#65292;&#29305;&#21035;&#26159;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#37325;&#22823;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.09132</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploring the Adversarial Capabilities of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#33021;&#22815;&#25104;&#21151;&#22320;&#21046;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#20197;&#24858;&#24324;&#23433;&#20840;&#25514;&#26045;&#65292;&#29305;&#21035;&#26159;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#37325;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#24341;&#21457;&#20102;&#24191;&#27867;&#21644;&#26222;&#36941;&#30340;&#20852;&#36259;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#24378;&#22823;&#30340;&#35821;&#35328;&#29983;&#25104;&#33021;&#21147;&#65292;&#20026;&#34892;&#19994;&#21644;&#30740;&#31350;&#25552;&#20379;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;LLMs&#30340;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#33021;&#21542;&#34920;&#29616;&#20986;&#23545;&#25239;&#34892;&#20026;&#30340;&#31243;&#24230;&#20173;&#28982;&#23578;&#26410;&#23436;&#20840;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30740;&#31350;&#24120;&#35265;&#30340;&#20844;&#24320;&#21487;&#29992;LLMs&#26159;&#21542;&#20855;&#26377;&#33021;&#21147;&#25200;&#20081;&#25991;&#26412;&#26679;&#26412;&#20197;&#24858;&#24324;&#23433;&#20840;&#25514;&#26045;&#65292;&#21363;&#25152;&#35859;&#30340;&#23545;&#25239;&#31034;&#20363;&#25110;&#25915;&#20987;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#35843;&#26597;LLMs&#26159;&#21542;&#26412;&#36136;&#19978;&#33021;&#22815;&#20174;&#33391;&#24615;&#26679;&#26412;&#20013;&#21046;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#20197;&#24858;&#24324;&#29616;&#26377;&#30340;&#23433;&#20840;&#38450;&#32447;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#37325;&#28857;&#20851;&#27880;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#65292;&#21457;&#29616;LLMs&#25104;&#21151;&#22320;&#25214;&#21040;&#20102;&#23545;&#25239;&#24615;&#25200;&#21160;&#65292;&#26377;&#25928;&#22320;&#30772;&#22351;&#20102;&#23545;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31995;&#32479;&#30340;&#38450;&#24481;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#23545;&#65288;&#21322;&#65289;&#33258;&#21160;&#21270;&#23433;&#20840;&#35780;&#20272;&#21644;&#38450;&#24481;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09132v1 Announce Type: new Abstract: The proliferation of large language models (LLMs) has sparked widespread and general interest due to their strong language generation capabilities, offering great potential for both industry and research. While previous research delved into the security and privacy issues of LLMs, the extent to which these models can exhibit adversarial behavior remains largely unexplored. Addressing this gap, we investigate whether common publicly available LLMs have inherent capabilities to perturb text samples to fool safety measures, so-called adversarial examples resp.~attacks. More specifically, we investigate whether LLMs are inherently able to craft adversarial examples out of benign samples to fool existing safe rails. Our experiments, which focus on hate speech detection, reveal that LLMs succeed in finding adversarial perturbations, effectively undermining hate speech detection systems. Our findings carry significant implications for (semi-)aut
&lt;/p&gt;</description></item><item><title>Pix2Code &#26159;&#19968;&#20010;&#23558;&#31070;&#32463;&#35270;&#35273;&#27010;&#24565;&#32452;&#21512;&#25104;&#31243;&#24207;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26174;&#24335;&#12289;&#32452;&#21512;&#30340;&#31526;&#21495;&#21644;&#38544;&#24335;&#30340;&#31070;&#32463;&#34920;&#31034;&#33021;&#21147;&#65292;&#20174;&#22270;&#20687;&#20013;&#26816;&#32034;&#23545;&#35937;&#34920;&#31034;&#24182;&#23558;&#20851;&#31995;&#27010;&#24565;&#21512;&#25104;&#20026;lambda&#28436;&#31639;&#31243;&#24207;&#65292;&#26469;&#35299;&#20915;&#36890;&#29992;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#25361;&#25112;&#12290;&#22312;&#25512;&#29702;&#39046;&#22495;Kandinsky Patterns&#21644;CURI&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;Pix2Code &#33021;&#22815;&#35782;&#21035;&#32452;&#21512;&#35270;&#35273;&#27010;&#24565;&#24182;&#25512;&#24191;&#21040;&#26032;&#25968;&#25454;&#21644;&#25512;&#29702;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.08280</link><description>&lt;p&gt;
Pix2Code&#65306;&#23398;&#20064;&#23558;&#31070;&#32463;&#35270;&#35273;&#27010;&#24565;&#32452;&#21512;&#25104;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Pix2Code: Learning to Compose Neural Visual Concepts as Programs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08280
&lt;/p&gt;
&lt;p&gt;
Pix2Code &#26159;&#19968;&#20010;&#23558;&#31070;&#32463;&#35270;&#35273;&#27010;&#24565;&#32452;&#21512;&#25104;&#31243;&#24207;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26174;&#24335;&#12289;&#32452;&#21512;&#30340;&#31526;&#21495;&#21644;&#38544;&#24335;&#30340;&#31070;&#32463;&#34920;&#31034;&#33021;&#21147;&#65292;&#20174;&#22270;&#20687;&#20013;&#26816;&#32034;&#23545;&#35937;&#34920;&#31034;&#24182;&#23558;&#20851;&#31995;&#27010;&#24565;&#21512;&#25104;&#20026;lambda&#28436;&#31639;&#31243;&#24207;&#65292;&#26469;&#35299;&#20915;&#36890;&#29992;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#25361;&#25112;&#12290;&#22312;&#25512;&#29702;&#39046;&#22495;Kandinsky Patterns&#21644;CURI&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;Pix2Code &#33021;&#22815;&#35782;&#21035;&#32452;&#21512;&#35270;&#35273;&#27010;&#24565;&#24182;&#25512;&#24191;&#21040;&#26032;&#25968;&#25454;&#21644;&#25512;&#29702;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#23398;&#20064;&#20174;&#22270;&#20687;&#20013;&#25277;&#35937;&#27010;&#24565;&#30340;&#25361;&#25112;&#22312;&#20110;&#38656;&#35201;&#23558;&#35270;&#35273;&#24863;&#30693;&#21644;&#36890;&#29992;&#20851;&#31995;&#25512;&#29702;&#36827;&#34892;&#25972;&#21512;&#12290;&#27492;&#22806;&#65292;&#35813;&#20219;&#21153;&#30340;&#26080;&#30417;&#30563;&#24615;&#36136;&#20351;&#24471;&#20154;&#31867;&#29992;&#25143;&#38656;&#35201;&#33021;&#22815;&#29702;&#35299;&#27169;&#22411;&#23398;&#21040;&#30340;&#27010;&#24565;&#65292;&#24182;&#21487;&#33021;&#20462;&#27491;&#38169;&#35823;&#30340;&#34892;&#20026;&#12290;&#20026;&#20102;&#35299;&#20915;&#35270;&#35273;&#27010;&#24565;&#23398;&#20064;&#30340;&#36890;&#29992;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#32422;&#26463;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Pix2Code&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#31243;&#24207;&#21512;&#25104;&#25193;&#23637;&#21040;&#35270;&#35273;&#20851;&#31995;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#26126;&#30830;&#30340;&#12289;&#32452;&#21512;&#30340;&#31526;&#21495;&#21644;&#38544;&#24335;&#30340;&#31070;&#32463;&#34920;&#31034;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20174;&#22270;&#20687;&#20013;&#26816;&#32034;&#23545;&#35937;&#34920;&#31034;&#24182;&#23558;&#20851;&#31995;&#27010;&#24565;&#21512;&#25104;&#20026;lambda&#28436;&#31639;&#31243;&#24207;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#29702;&#39046;&#22495;Kandinsky Patterns&#21644;CURI&#19978;&#35780;&#20272;&#20102;Pix2Code&#30340;&#22810;&#26679;&#29305;&#24615;&#65292;&#20174;&#32780;&#27979;&#35797;&#20854;&#35782;&#21035;&#32452;&#21512;&#35270;&#35273;&#27010;&#24565;&#24182;&#25512;&#24191;&#21040;&#26032;&#25968;&#25454;&#21644;&#25512;&#29702;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The challenge in learning abstract concepts from images in an unsupervised fashion lies in the required integration of visual perception and generalizable relational reasoning. Moreover, the unsupervised nature of this task makes it necessary for human users to be able to understand a model's learnt concepts and potentially revise false behaviours. To tackle both the generalizability and interpretability constraints of visual concept learning, we propose Pix2Code, a framework that extends program synthesis to visual relational reasoning by utilizing the abilities of both explicit, compositional symbolic and implicit neural representations. This is achieved by retrieving object representations from images and synthesizing relational concepts as lambda-calculus programs. We evaluate the diverse properties of Pix2Code on the challenging reasoning domains, Kandinsky Patterns and CURI, thereby testing its ability to identify compositional visual concepts that generalize to novel data and co
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SMX&#30340;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#35268;&#21010;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#21019;&#24314;&#20102;&#26377;&#25928;&#30340;&#33258;&#25105;&#23398;&#20064;&#26426;&#21046;&#12290;&#23427;&#36866;&#29992;&#20110;&#31163;&#25955;&#21644;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#29615;&#22659;&#65292;&#20855;&#26377;&#39640;&#24182;&#34892;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07963</link><description>&lt;p&gt;
SMX: &#19987;&#23478;&#36845;&#20195;&#30340;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
SMX: Sequential Monte Carlo Planning for Expert Iteration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07963
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SMX&#30340;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#35268;&#21010;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#21019;&#24314;&#20102;&#26377;&#25928;&#30340;&#33258;&#25105;&#23398;&#20064;&#26426;&#21046;&#12290;&#23427;&#36866;&#29992;&#20110;&#31163;&#25955;&#21644;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#29615;&#22659;&#65292;&#20855;&#26377;&#39640;&#24182;&#34892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#33021;&#22815;&#22312;&#20915;&#31574;&#21644;&#23398;&#20064;&#36807;&#31243;&#20013;&#21033;&#29992;&#35268;&#21010;&#33021;&#21147;&#30340;&#26234;&#33021;&#20307;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#27493;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#26641;&#29366;&#25628;&#32034;&#26041;&#27861;&#21644;&#33258;&#25105;&#23545;&#24328;&#23398;&#20064;&#26426;&#21046;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25628;&#32034;&#36807;&#31243;&#30340;&#39034;&#24207;&#24615;&#36136;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38754;&#20020;&#25193;&#23637;&#24615;&#25361;&#25112;&#12290;&#34429;&#28982;&#23454;&#36341;&#24037;&#31243;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#37096;&#20998;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#20173;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#19968;&#31181;&#21517;&#20026;SMX&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#35745;&#21010;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#21487;&#25193;&#23637;&#30340;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#21019;&#24314;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#33258;&#25105;&#23398;&#20064;&#26426;&#21046;&#12290;SMX&#22522;&#20110;&#25511;&#21046;&#20316;&#20026;&#25512;&#26029;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#21463;&#30410;&#20110;&#22362;&#23454;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#23427;&#22522;&#20110;&#37319;&#26679;&#30340;&#25628;&#32034;&#26041;&#27861;&#20351;&#20854;&#36866;&#24212;&#20855;&#26377;&#31163;&#25955;&#21644;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#29615;&#22659;&#12290;&#27492;&#22806;&#65292;SMX&#20801;&#35768;&#39640;&#24230;&#24182;&#34892;&#21270;&#24182;&#21487;&#20197;&#36816;&#34892;&#20110;&#21508;&#31867;&#35745;&#31639;&#26426;&#35774;&#22791;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing agents that can leverage planning abilities during their decision and learning processes is critical to the advancement of Artificial Intelligence. Recent works have demonstrated the effectiveness of combining tree-based search methods and self-play learning mechanisms. Yet, these methods typically face scaling challenges due to the sequential nature of their search. While practical engineering solutions can partly overcome this, they still demand extensive computational resources, which hinders their applicability. In this paper, we introduce SMX, a model-based planning algorithm that utilises scalable Sequential Monte Carlo methods to create an effective self-learning mechanism. Grounded in the theoretical framework of control as inference, SMX benefits from robust theoretical underpinnings. Its sampling-based search approach makes it adaptable to environments with both discrete and continuous action spaces. Furthermore, SMX allows for high parallelisation and can run on h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;GPT-4V&#36827;&#34892;&#21487;&#35299;&#37322;&#39118;&#38505;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35299;&#37322;&#22797;&#26434;&#30340;&#36807;&#39532;&#36335;&#22330;&#26223;&#65292;&#20026;&#30450;&#20154;&#21644;&#35270;&#21147;&#20302;&#19979;&#20154;&#22763;&#30340;&#23433;&#20840;&#20915;&#31574;&#25552;&#20379;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2402.06794</link><description>&lt;p&gt;
&#26159;&#21542;&#23433;&#20840;&#36807;&#39532;&#36335;&#65311;GPT-4V&#29992;&#20110;&#23433;&#20840;&#24847;&#35782;&#30340;&#21487;&#35299;&#37322;&#39118;&#38505;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Is it safe to cross? Interpretable Risk Assessment with GPT-4V for Safety-Aware Street Crossing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;GPT-4V&#36827;&#34892;&#21487;&#35299;&#37322;&#39118;&#38505;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35299;&#37322;&#22797;&#26434;&#30340;&#36807;&#39532;&#36335;&#22330;&#26223;&#65292;&#20026;&#30450;&#20154;&#21644;&#35270;&#21147;&#20302;&#19979;&#20154;&#22763;&#30340;&#23433;&#20840;&#20915;&#31574;&#25552;&#20379;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#30450;&#20154;&#21644;&#35270;&#21147;&#20302;&#19979;&#30340;&#20154;&#26469;&#35828;&#65292;&#23433;&#20840;&#22320;&#36890;&#36807;&#34903;&#36947;&#20132;&#21449;&#21475;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#23545;&#21608;&#22260;&#29615;&#22659;&#26377;&#32454;&#33268;&#30340;&#29702;&#35299;&#65292;&#32780;&#36825;&#20010;&#20219;&#21153;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#35270;&#35273;&#32447;&#32034;&#12290;&#20256;&#32479;&#30340;&#36741;&#21161;&#20915;&#31574;&#26041;&#27861;&#24448;&#24448;&#19981;&#22815;&#23436;&#21892;&#65292;&#26080;&#27861;&#25552;&#20379;&#20840;&#38754;&#30340;&#22330;&#26223;&#20998;&#26512;&#21644;&#23433;&#20840;&#32423;&#21035;&#21028;&#26029;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#22810;&#27169;&#22411;&#26469;&#35299;&#37322;&#22797;&#26434;&#30340;&#36807;&#39532;&#36335;&#22330;&#26223;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#20132;&#36890;&#20449;&#21495;&#35782;&#21035;&#25216;&#26415;&#65292;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#36827;&#27493;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#29983;&#25104;&#23433;&#20840;&#35780;&#20998;&#21644;&#33258;&#28982;&#35821;&#35328;&#22330;&#26223;&#25551;&#36848;&#65292;&#25903;&#25345;&#30450;&#20154;&#21644;&#35270;&#21147;&#20302;&#19979;&#20154;&#22763;&#23433;&#20840;&#20915;&#31574;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#30001;&#22235;&#36275;&#26426;&#22120;&#20154;&#25429;&#33719;&#30340;&#22810;&#35270;&#35282;&#33258;&#25105;&#20013;&#24515;&#22270;&#20687;&#26500;&#25104;&#30340;&#36807;&#39532;&#36335;&#20132;&#21449;&#21475;&#25968;&#25454;&#65292;&#24182;&#26681;&#25454;&#39044;&#20808;&#23450;&#20041;&#30340;&#23433;&#20840;&#35780;&#20998;&#20998;&#31867;&#36827;&#34892;&#20102;&#22270;&#20687;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safely navigating street intersections is a complex challenge for blind and low-vision individuals, as it requires a nuanced understanding of the surrounding context - a task heavily reliant on visual cues. Traditional methods for assisting in this decision-making process often fall short, lacking the ability to provide a comprehensive scene analysis and safety level. This paper introduces an innovative approach that leverages large multimodal models (LMMs) to interpret complex street crossing scenes, offering a potential advancement over conventional traffic signal recognition techniques. By generating a safety score and scene description in natural language, our method supports safe decision-making for the blind and low-vision individuals. We collected crosswalk intersection data that contains multiview egocentric images captured by a quadruped robot and annotated the images with corresponding safety scores based on our predefined safety score categorization. Grounded on the visual k
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20110;&#29992;&#25143;&#30340;&#21453;&#39304;&#25968;&#25454;&#20013;&#24341;&#20837;&#20010;&#24615;&#21270;&#29305;&#24449;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#22312;&#22810;&#26679;&#21270;&#29992;&#25143;&#20559;&#22909;&#19979;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05133</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#22522;&#20110;&#20010;&#24615;&#21270;&#20154;&#31867;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Personalized Language Modeling from Personalized Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05133
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20110;&#29992;&#25143;&#30340;&#21453;&#39304;&#25968;&#25454;&#20013;&#24341;&#20837;&#20010;&#24615;&#21270;&#29305;&#24449;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#22312;&#22810;&#26679;&#21270;&#29992;&#25143;&#20559;&#22909;&#19979;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20010;&#24615;&#21270;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#30446;&#21069;&#20027;&#27969;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#26356;&#22909;&#22320;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#24320;&#21457;&#30340;&#31639;&#27861;&#30340;&#22522;&#26412;&#21069;&#25552;&#22312;&#29992;&#25143;&#20559;&#22909;&#22810;&#26679;&#21270;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#20986;&#29616;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#27491;&#24335;&#20171;&#32461;&#20102;&#20174;&#20010;&#24615;&#21270;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#20219;&#21153;&#65292;&#24182;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#26222;&#36890;&#30340;RLHF&#21487;&#33021;&#20250;&#23384;&#22312;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20010;&#24615;&#21270;-RLHF&#65288;P-RLHF&#65289;&#26694;&#26550;&#65292;&#38656;&#35201;&#21516;&#26102;&#23398;&#20064;&#29992;&#25143;&#27169;&#22411;&#21644;&#35821;&#35328;&#65288;&#25110;&#22870;&#21169;&#65289;&#27169;&#22411;&#12290;&#29992;&#25143;&#27169;&#22411;&#25509;&#25910;&#29992;&#25143;&#20449;&#24687;&#24182;&#36755;&#20986;&#29992;&#25143;&#34920;&#31034;&#12290;&#20854;&#32467;&#26500;&#32534;&#30721;&#20102;&#25105;&#20204;&#23545;&#21453;&#39304;&#25968;&#25454;&#20013;&#29992;&#25143;&#20559;&#22909;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#20026;&#20010;&#24615;&#21270;&#22870;&#21169;&#24314;&#27169;&#21644;&#20010;&#24615;&#21270;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#24320;&#21457;&#20102;&#26032;&#30340;&#23398;&#20064;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback (RLHF) is the current dominating framework to fine-tune large language models to better align with human preferences. However, the underlying premise of algorithms developed under this framework can be problematic when user preferences encoded in human feedback are diverse. In this work, we aim to address this problem by developing methods for building personalized language models. We first formally introduce the task of learning from personalized human feedback and explain why vanilla RLHF can be problematic in this context. We then propose a general Personalized-RLHF (P-RLHF) framework, which requires one to jointly learn a user model and a language (or reward) model. The user model takes in user information and outputs user representations. Its structure encodes our assumptions about user preferences underlying the feedback data. We develop new learning objectives for personalized reward modeling and personalized Direct Preference Optimizat
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#38598;&#20013;&#20110;&#19981;&#21464;&#27169;&#22411;&#30340;&#29702;&#35770;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#24341;&#20837;&#23436;&#22791;&#30340;&#35774;&#35745;GeoNGNN&#65292;&#24182;&#21033;&#29992;&#20854;&#20316;&#20026;&#29702;&#35770;&#24037;&#20855;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;E(3)-&#23436;&#22791;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04836</link><description>&lt;p&gt;
&#20851;&#20110;&#19981;&#21464;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23436;&#22791;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Completeness of Invariant Geometric Deep Learning Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04836
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#38598;&#20013;&#20110;&#19981;&#21464;&#27169;&#22411;&#30340;&#29702;&#35770;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#24341;&#20837;&#23436;&#22791;&#30340;&#35774;&#35745;GeoNGNN&#65292;&#24182;&#21033;&#29992;&#20854;&#20316;&#20026;&#29702;&#35770;&#24037;&#20855;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;E(3)-&#23436;&#22791;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21464;&#27169;&#22411;&#26159;&#19968;&#31867;&#37325;&#35201;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#20449;&#24687;&#20016;&#23500;&#30340;&#20960;&#20309;&#29305;&#24449;&#29983;&#25104;&#26377;&#24847;&#20041;&#30340;&#20960;&#20309;&#34920;&#31034;&#12290;&#36825;&#20123;&#27169;&#22411;&#20197;&#20854;&#31616;&#21333;&#24615;&#12289;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#21644;&#35745;&#31639;&#25928;&#29575;&#32780;&#38395;&#21517;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#29702;&#35770;&#34920;&#36798;&#33021;&#21147;&#20173;&#28982;&#19981;&#28165;&#26970;&#65292;&#38480;&#21046;&#20102;&#23545;&#36825;&#31181;&#27169;&#22411;&#28508;&#21147;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#38598;&#20013;&#35752;&#35770;&#19981;&#21464;&#27169;&#22411;&#30340;&#29702;&#35770;&#34920;&#36798;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#20005;&#26684;&#38480;&#21046;&#20102;&#26368;&#32463;&#20856;&#30340;&#19981;&#21464;&#27169;&#22411;Vanilla DisGNN&#65288;&#32467;&#21512;&#36317;&#31163;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#23558;&#20854;&#19981;&#21487;&#35782;&#21035;&#30340;&#24773;&#20917;&#20165;&#38480;&#20110;&#39640;&#24230;&#23545;&#31216;&#30340;&#20960;&#20309;&#22270;&#24418;&#12290;&#20026;&#20102;&#25171;&#30772;&#36825;&#20123;&#29305;&#27530;&#24773;&#20917;&#30340;&#23545;&#31216;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#23436;&#22791;&#30340;&#19981;&#21464;&#35774;&#35745;&#65292;&#21363;&#23884;&#22871;Vanilla DisGNN&#30340;GeoNGNN&#12290;&#21033;&#29992;GeoNGNN&#20316;&#20026;&#29702;&#35770;&#24037;&#20855;&#65292;&#25105;&#20204;&#39318;&#27425;&#35777;&#26126;&#20102;E(3)-&#23436;&#22791;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Invariant models, one important class of geometric deep learning models, are capable of generating meaningful geometric representations by leveraging informative geometric features. These models are characterized by their simplicity, good experimental results and computational efficiency. However, their theoretical expressive power still remains unclear, restricting a deeper understanding of the potential of such models. In this work, we concentrate on characterizing the theoretical expressiveness of invariant models. We first rigorously bound the expressiveness of the most classical invariant model, Vanilla DisGNN (message passing neural networks incorporating distance), restricting its unidentifiable cases to be only those highly symmetric geometric graphs. To break these corner cases' symmetry, we introduce a simple yet E(3)-complete invariant design by nesting Vanilla DisGNN, named GeoNGNN. Leveraging GeoNGNN as a theoretical tool, we for the first time prove the E(3)-completeness 
&lt;/p&gt;</description></item><item><title>V-IRL&#26159;&#19968;&#20010;&#24179;&#21488;&#65292;&#21487;&#20197;&#35753;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#19982;&#29616;&#23454;&#19990;&#30028;&#36827;&#34892;&#20114;&#21160;&#65292;&#26088;&#22312;&#23558;&#25968;&#23383;&#21644;&#29289;&#29702;&#19990;&#30028;&#20043;&#38388;&#30340;&#24046;&#36317;&#32553;&#23567;&#65292;&#24182;&#24320;&#21457;&#20986;&#20855;&#26377;&#20016;&#23500;&#24863;&#30693;&#12289;&#20915;&#31574;&#21644;&#19982;&#30495;&#23454;&#25968;&#25454;&#20114;&#21160;&#33021;&#21147;&#30340;&#20195;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.03310</link><description>&lt;p&gt;
V-IRL: &#23558;&#34394;&#25311;&#26234;&#33021;&#19982;&#29616;&#23454;&#29983;&#27963;&#32852;&#31995;&#36215;&#26469;
&lt;/p&gt;
&lt;p&gt;
V-IRL: Grounding Virtual Intelligence in Real Life
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03310
&lt;/p&gt;
&lt;p&gt;
V-IRL&#26159;&#19968;&#20010;&#24179;&#21488;&#65292;&#21487;&#20197;&#35753;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#19982;&#29616;&#23454;&#19990;&#30028;&#36827;&#34892;&#20114;&#21160;&#65292;&#26088;&#22312;&#23558;&#25968;&#23383;&#21644;&#29289;&#29702;&#19990;&#30028;&#20043;&#38388;&#30340;&#24046;&#36317;&#32553;&#23567;&#65292;&#24182;&#24320;&#21457;&#20986;&#20855;&#26377;&#20016;&#23500;&#24863;&#30693;&#12289;&#20915;&#31574;&#21644;&#19982;&#30495;&#23454;&#25968;&#25454;&#20114;&#21160;&#33021;&#21147;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#29983;&#27963;&#22312;&#22320;&#29699;&#19978;&#65292;&#32780;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#25152;&#21019;&#36896;&#30340;&#25968;&#23383;&#39046;&#22495;&#20043;&#38388;&#23384;&#22312;&#30528;&#24863;&#23448;&#24046;&#36317;&#12290;&#20026;&#20102;&#24320;&#21457;&#20986;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#33021;&#20687;&#20154;&#31867;&#19968;&#26679;&#28789;&#27963;&#24863;&#30693;&#12289;&#24605;&#32771;&#21644;&#34892;&#21160;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65292;&#24517;&#39035;&#24357;&#21512;&#25968;&#23383;&#21644;&#29289;&#29702;&#19990;&#30028;&#20043;&#38388;&#30340;&#36924;&#30495;&#24046;&#36317;&#12290;&#25105;&#20204;&#22914;&#20309;&#22312;&#19968;&#20010;&#20687;&#25105;&#20204;&#25152;&#23621;&#20303;&#30340;&#19990;&#30028;&#20013;&#19968;&#26679;&#20016;&#23500;&#22810;&#26679;&#30340;&#29615;&#22659;&#20013;&#20307;&#29616;&#20195;&#29702;&#65292;&#32780;&#19981;&#21463;&#30495;&#23454;&#30828;&#20214;&#21644;&#25511;&#21046;&#25152;&#26045;&#21152;&#30340;&#32422;&#26463;&#65311;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;V-IRL: &#19968;&#31181;&#24179;&#21488;&#65292;&#21487;&#20197;&#20351;&#20195;&#29702;&#22312;&#34394;&#25311;&#32780;&#36924;&#30495;&#30340;&#29615;&#22659;&#20013;&#19982;&#29616;&#23454;&#19990;&#30028;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#20114;&#21160;&#12290;&#25105;&#20204;&#30340;&#24179;&#21488;&#26082;&#26159;&#19968;&#20010;&#24320;&#21457;&#20195;&#29702;&#23436;&#25104;&#21508;&#31181;&#23454;&#38469;&#20219;&#21153;&#30340;&#28216;&#20048;&#22330;&#65292;&#21448;&#26159;&#19968;&#20010;&#24191;&#38420;&#30340;&#27979;&#35797;&#22522;&#22320;&#65292;&#29992;&#20110;&#34913;&#37327;&#22312;&#24863;&#30693;&#12289;&#20915;&#31574;&#21644;&#19982;&#20840;&#29699;&#30495;&#23454;&#25968;&#25454;&#30340;&#20114;&#21160;&#33021;&#21147;&#31561;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a sensory gulf between the Earth that humans inhabit and the digital realms in which modern AI agents are created. To develop AI agents that can sense, think, and act as flexibly as humans in real-world settings, it is imperative to bridge the realism gap between the digital and physical worlds. How can we embody agents in an environment as rich and diverse as the one we inhabit, without the constraints imposed by real hardware and control? Towards this end, we introduce V-IRL: a platform that enables agents to scalably interact with the real world in a virtual yet realistic environment. Our platform serves as a playground for developing agents that can accomplish various practical tasks and as a vast testbed for measuring progress in capabilities spanning perception, decision-making, and interaction with real-world data across the entire globe.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;GNN&#30340;&#27450;&#35784;&#26816;&#27979;&#22120;SEC-GFD&#65292;&#36890;&#36807;&#28151;&#21512;&#36807;&#28388;&#27169;&#22359;&#21644;&#23616;&#37096;&#29615;&#22659;&#32422;&#26463;&#27169;&#22359;&#35299;&#20915;&#20102;&#24322;&#36136;&#24615;&#21644;&#26631;&#31614;&#21033;&#29992;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.06441</link><description>&lt;p&gt;
&#22312;&#24322;&#36136;&#24615;&#21644;&#35889;&#38382;&#39064;&#19979;&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;&#22270;&#30340;&#27450;&#35784;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Revisiting Graph-Based Fraud Detection in Sight of Heterophily and Spectrum
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;GNN&#30340;&#27450;&#35784;&#26816;&#27979;&#22120;SEC-GFD&#65292;&#36890;&#36807;&#28151;&#21512;&#36807;&#28388;&#27169;&#22359;&#21644;&#23616;&#37096;&#29615;&#22659;&#32422;&#26463;&#27169;&#22359;&#35299;&#20915;&#20102;&#24322;&#36136;&#24615;&#21644;&#26631;&#31614;&#21033;&#29992;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#27450;&#35784;&#26816;&#27979;&#65288;GFD&#65289;&#21487;&#35270;&#20026;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21322;&#30417;&#30563;&#33410;&#28857;&#20108;&#20998;&#31867;&#20219;&#21153;&#12290;&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;GFD&#65292;&#36890;&#36807;&#32858;&#21512;&#37051;&#23621;&#20449;&#24687;&#26469;&#21051;&#30011;&#33410;&#28857;&#30340;&#24322;&#24120;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#27450;&#35784;&#22270;&#22312;&#26412;&#36136;&#19978;&#26159;&#24322;&#36136;&#30340;&#65292;&#22240;&#27492;&#22823;&#22810;&#25968;GNN&#30001;&#20110;&#20551;&#35774;&#21516;&#36136;&#24615;&#32780;&#34920;&#29616;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23384;&#22312;&#24322;&#36136;&#24615;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#29616;&#26377;&#27169;&#22411;&#26410;&#20805;&#20998;&#21033;&#29992;&#23453;&#36149;&#30340;&#33410;&#28857;&#26631;&#31614;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;GNN&#30340;&#27450;&#35784;&#26816;&#27979;&#22120;SEC-GFD&#12290;&#35813;&#26816;&#27979;&#22120;&#21253;&#25324;&#28151;&#21512;&#36807;&#28388;&#27169;&#22359;&#21644;&#23616;&#37096;&#29615;&#22659;&#32422;&#26463;&#27169;&#22359;&#65292;&#36825;&#20004;&#20010;&#27169;&#22359;&#20998;&#21035;&#29992;&#20110;&#35299;&#20915;&#24322;&#36136;&#24615;&#21644;&#26631;&#31614;&#21033;&#29992;&#38382;&#39064;&#12290;&#31532;&#19968;&#20010;&#27169;&#22359;&#20174;&#35889;&#22495;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#35299;&#20915;&#20102;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#23558;&#22270;&#20998;&#21106;&#31216;&#19981;&#21516;&#30340;&#35889;&#25104;&#20998;&#65292;
&lt;/p&gt;
&lt;p&gt;
Graph-based fraud detection (GFD) can be regarded as a challenging semi-supervised node binary classification task. In recent years, Graph Neural Networks (GNN) have been widely applied to GFD, characterizing the anomalous possibility of a node by aggregating neighbor information. However, fraud graphs are inherently heterophilic, thus most of GNNs perform poorly due to their assumption of homophily. In addition, due to the existence of heterophily and class imbalance problem, the existing models do not fully utilize the precious node label information. To address the above issues, this paper proposes a semi-supervised GNN-based fraud detector SEC-GFD. This detector includes a hybrid filtering module and a local environmental constraint module, the two modules are utilized to solve heterophily and label utilization problem respectively. The first module starts from the perspective of the spectral domain, and solves the heterophily problem to a certain extent. Specifically, it divides t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#21307;&#23398;&#25104;&#20687;&#20013;&#30340;&#21487;&#35299;&#37322;AI&#30340;&#19977;&#32500;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#31070;&#32463;&#31185;&#23398;&#39046;&#22495;&#20013;&#35782;&#21035;&#22823;&#33041;&#27807;&#29305;&#24449;&#30340;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2309.00903</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#19977;&#32500;&#26694;&#26550;&#25581;&#31034;&#23398;&#20064;&#27169;&#24335;&#65306;&#21464;&#37327;&#33041;&#27807;&#35782;&#21035;&#30340;&#32479;&#19968;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
An explainable three dimension framework to uncover learning patterns: A unified look in variable sulci recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.00903
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#21307;&#23398;&#25104;&#20687;&#20013;&#30340;&#21487;&#35299;&#37322;AI&#30340;&#19977;&#32500;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#31070;&#32463;&#31185;&#23398;&#39046;&#22495;&#20013;&#35782;&#21035;&#22823;&#33041;&#27807;&#29305;&#24449;&#30340;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#25361;&#25112;&#24615;&#30340;&#31070;&#32463;&#31185;&#23398;&#39046;&#22495;&#37324;&#65292;&#35270;&#35273;&#20027;&#39064;&#22312;&#19977;&#32500;&#31354;&#38388;&#20869;&#34920;&#29616;&#20986;&#39640;&#24230;&#22797;&#26434;&#24615;&#12290;&#31070;&#32463;&#31185;&#23398;&#30340;&#24212;&#29992;&#28041;&#21450;&#20174;MRI&#20013;&#35782;&#21035;&#22823;&#33041;&#27807;&#29305;&#24449;&#65292;&#30001;&#20110;&#19987;&#23478;&#20043;&#38388;&#30340;&#26631;&#27880;&#35268;&#31243;&#23384;&#22312;&#24046;&#24322;&#21644;&#22823;&#33041;&#22797;&#26434;&#30340;&#19977;&#32500;&#21151;&#33021;&#65292;&#25105;&#20204;&#38754;&#20020;&#30528;&#37325;&#22823;&#38556;&#30861;&#12290;&#22240;&#27492;&#65292;&#20256;&#32479;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#22312;&#26377;&#25928;&#39564;&#35777;&#21644;&#35780;&#20272;&#36825;&#20123;&#32593;&#32476;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#25968;&#23398;&#20844;&#24335;&#65292;&#32454;&#21270;&#20102;&#19981;&#21516;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#35299;&#37322;&#38656;&#27714;&#30340;&#21508;&#31181;&#31867;&#21035;&#65292;&#20998;&#20026;&#33258;&#35299;&#37322;&#12289;&#21322;&#35299;&#37322;&#12289;&#38750;&#35299;&#37322;&#21644;&#22522;&#20110;&#39564;&#35777;&#21327;&#35758;&#21487;&#38752;&#24615;&#30340;&#26032;&#27169;&#24335;&#23398;&#20064;&#24212;&#29992;&#12290;&#26681;&#25454;&#36825;&#20010;&#25968;&#23398;&#20844;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#35299;&#37322;&#19977;&#32500;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.00903v2 Announce Type: replace-cross  Abstract: Explainable AI is crucial in medical imaging. In the challenging field of neuroscience, visual topics present a high level of complexity, particularly within three-dimensional space. The application of neuroscience, which involves identifying brain sulcal features from MRI, faces significant hurdles due to varying annotation protocols among experts and the intricate three-dimension functionality of the brain. Consequently, traditional explainability approaches fall short in effectively validating and evaluating these networks. To address this, we first present a mathematical formulation delineating various categories of explanation needs across diverse computer vision tasks, categorized into self-explanatory, semi-explanatory, non-explanatory, and new-pattern learning applications based on the reliability of the validation protocol. With respect to this mathematical formulation, we propose a 3D explainability framework aimed at
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#39564;&#35774;&#35745;&#26694;&#26550;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#38480;&#26631;&#31614;&#30417;&#30563;&#24494;&#35843;&#30340;&#27880;&#37322;&#25104;&#26412;&#65292;&#24182;&#35299;&#20915;&#20102;&#20027;&#21160;&#23398;&#20064;&#30340;&#35745;&#31639;&#29942;&#39048;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.06692</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#38480;&#26631;&#31614;&#30417;&#30563;&#24494;&#35843;&#30340;&#23454;&#39564;&#35774;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models. (arXiv:2401.06692v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06692
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#39564;&#35774;&#35745;&#26694;&#26550;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#38480;&#26631;&#31614;&#30417;&#30563;&#24494;&#35843;&#30340;&#27880;&#37322;&#25104;&#26412;&#65292;&#24182;&#35299;&#20915;&#20102;&#20027;&#21160;&#23398;&#20064;&#30340;&#35745;&#31639;&#29942;&#39048;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25351;&#23548;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#38480;&#26631;&#31614;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#22312;&#23454;&#29616;&#20102;&#20196;&#20154;&#24778;&#21497;&#30340;&#38646;&#23556;&#20987;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21457;&#25381;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20026;&#25351;&#20196;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#22238;&#31572;&#25152;&#38656;&#30340;&#27880;&#37322;&#24037;&#20316;&#27491;&#22312;&#21464;&#24471;&#38590;&#20197;&#25215;&#21463;&#65292;&#29305;&#21035;&#26159;&#38543;&#30528;&#25351;&#20196;&#25968;&#25454;&#38598;&#25152;&#28085;&#30422;&#30340;&#20219;&#21153;&#25968;&#37327;&#30340;&#22686;&#21152;&#12290;&#20027;&#21160;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#26410;&#26631;&#35760;&#30340;&#26679;&#26412;&#27744;&#20013;&#30830;&#23450;&#26377;&#29992;&#30340;&#23376;&#38598;&#36827;&#34892;&#27880;&#37322;&#65292;&#20294;&#20854;&#39640;&#35745;&#31639;&#25104;&#26412;&#20173;&#28982;&#26159;&#20854;&#22312;LLMs&#29615;&#22659;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#38556;&#30861;&#12290;&#20026;&#20102;&#20943;&#23569;SFT&#30340;&#27880;&#37322;&#25104;&#26412;&#24182;&#35268;&#36991;&#20027;&#21160;&#23398;&#20064;&#30340;&#35745;&#31639;&#29942;&#39048;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#23454;&#39564;&#35774;&#35745;&#12290;&#23454;&#39564;&#35774;&#35745;&#25216;&#26415;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#36827;&#34892;&#26631;&#27880;&#65292;&#36890;&#24120;&#26368;&#22823;&#21270;&#26576;&#31181;&#19981;&#30830;&#23450;&#24615;&#21644;/&#25110;&#22810;&#26679;&#24615;&#30340;&#27010;&#24565;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#19968;&#20010;&#35780;&#20272;&#22810;&#31181;&#29616;&#26377;&#21644;&#26032;&#39062;&#30340;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised finetuning (SFT) on instruction datasets has played a crucial role in achieving the remarkable zero-shot generalization capabilities observed in modern large language models (LLMs). However, the annotation efforts required to produce high quality responses for instructions are becoming prohibitively expensive, especially as the number of tasks spanned by instruction datasets continues to increase. Active learning is effective in identifying useful subsets of samples to annotate from an unlabeled pool, but its high computational cost remains a barrier to its widespread applicability in the context of LLMs. To mitigate the annotation cost of SFT and circumvent the computational bottlenecks of active learning, we propose using experimental design. Experimental design techniques select the most informative samples to label, and typically maximize some notion of uncertainty and/or diversity. In our work, we implement a framework that evaluates several existing and novel experimen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#20195;&#30721;&#30340;&#23646;&#24615;&#65292;&#25581;&#31034;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#20195;&#30721;&#20043;&#38388;&#30340;&#29420;&#29305;&#27169;&#24335;&#65292;&#23588;&#20854;&#26159;&#32467;&#26500;&#20998;&#21106;&#23545;&#20110;&#35782;&#21035;&#20195;&#30721;&#26469;&#28304;&#24456;&#20851;&#38190;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DetectCodeGPT&#30340;&#26032;&#26041;&#27861;&#26469;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2401.06461</link><description>&lt;p&gt;
&#20195;&#30721;&#20043;&#38388;&#30340;&#30028;&#38480;&#65306;&#25581;&#31034;&#26426;&#22120;&#21644;&#20154;&#31867;&#31243;&#24207;&#21592;&#20043;&#38388;&#19981;&#21516;&#30340;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Between Lines of Code: Unraveling the Distinct Patterns of Machine and Human Programmers. (arXiv:2401.06461v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#20195;&#30721;&#30340;&#23646;&#24615;&#65292;&#25581;&#31034;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#20195;&#30721;&#20043;&#38388;&#30340;&#29420;&#29305;&#27169;&#24335;&#65292;&#23588;&#20854;&#26159;&#32467;&#26500;&#20998;&#21106;&#23545;&#20110;&#35782;&#21035;&#20195;&#30721;&#26469;&#28304;&#24456;&#20851;&#38190;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DetectCodeGPT&#30340;&#26032;&#26041;&#27861;&#26469;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#27169;&#31946;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#28304;&#20195;&#30721;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#23548;&#33268;&#36719;&#20214;&#20135;&#29289;&#30340;&#23436;&#25972;&#24615;&#21644;&#30495;&#23454;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#20195;&#30721;&#38271;&#24230;&#12289;&#35789;&#27719;&#22810;&#26679;&#24615;&#21644;&#33258;&#28982;&#24615;&#31561;&#23646;&#24615;&#30340;&#20005;&#26684;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#20195;&#30721;&#22266;&#26377;&#30340;&#29420;&#29305;&#27169;&#24335;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#29305;&#21035;&#27880;&#24847;&#21040;&#65292;&#20195;&#30721;&#30340;&#32467;&#26500;&#20998;&#21106;&#26159;&#35782;&#21035;&#20854;&#26469;&#28304;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DetectCodeGPT&#30340;&#26032;&#22411;&#26426;&#22120;&#29983;&#25104;&#20195;&#30721;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25913;&#36827;&#20102;DetectGPT&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have catalyzed an unprecedented wave in code generation. While achieving significant advances, they blur the distinctions between machine-and human-authored source code, causing integrity and authenticity issues of software artifacts. Previous methods such as DetectGPT have proven effective in discerning machine-generated texts, but they do not identify and harness the unique patterns of machine-generated code. Thus, its applicability falters when applied to code. In this paper, we carefully study the specific patterns that characterize machine and human-authored code. Through a rigorous analysis of code attributes such as length, lexical diversity, and naturalness, we expose unique pat-terns inherent to each source. We particularly notice that the structural segmentation of code is a critical factor in identifying its provenance. Based on our findings, we propose a novel machine-generated code detection method called DetectCodeGPT, which improves DetectGPT by cap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#27169;&#22411;&#21442;&#25968;&#30340;&#25200;&#21160;&#26469;&#23454;&#29616;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#25511;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#37319;&#29992;&#36951;&#24536;&#29575;&#21644;&#35760;&#24518;&#20445;&#30041;&#29575;&#31561;&#26032;&#30340;&#25351;&#26631;&#26469;&#35780;&#20272;&#21435;&#23398;&#20064;&#25928;&#26524;&#21644;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.04385</link><description>&lt;p&gt;
&#36890;&#36807;&#32454;&#31890;&#24230;&#27169;&#22411;&#21442;&#25968;&#25200;&#21160;&#23454;&#29616;&#26426;&#22120;&#21435;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning through fine-grained model parameters perturbation. (arXiv:2401.04385v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#27169;&#22411;&#21442;&#25968;&#30340;&#25200;&#21160;&#26469;&#23454;&#29616;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#25511;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#37319;&#29992;&#36951;&#24536;&#29575;&#21644;&#35760;&#24518;&#20445;&#30041;&#29575;&#31561;&#26032;&#30340;&#25351;&#26631;&#26469;&#35780;&#20272;&#21435;&#23398;&#20064;&#25928;&#26524;&#21644;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#21435;&#23398;&#20064;&#25216;&#26415;&#28041;&#21450;&#21040;&#25764;&#38144;&#25968;&#25454;&#35760;&#24405;&#21644;&#20943;&#23567;&#35813;&#25968;&#25454;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#24110;&#21161;&#23454;&#29616;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#30446;&#26631;&#65292;&#20294;&#20250;&#24102;&#26469;&#26174;&#33879;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22522;&#20110;&#21442;&#25968;&#25200;&#21160;&#30340;&#26435;&#37325;&#21435;&#23398;&#20064;&#26159;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#20294;&#36890;&#24120;&#28041;&#21450;&#21040;&#20840;&#23616;&#20462;&#25913;&#21442;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31934;&#32454;&#30340;Top-K&#21644;Random-k&#21442;&#25968;&#25200;&#21160;&#19981;&#31934;&#30830;&#26426;&#22120;&#21435;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#28385;&#36275;&#38544;&#31169;&#38656;&#27714;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25104;&#26412;&#21487;&#25511;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36824;&#35299;&#20915;&#20102;&#35780;&#20272;&#26426;&#22120;&#21435;&#23398;&#20064;&#25928;&#26524;&#30340;&#25361;&#25112;&#65292;&#32771;&#34385;&#20102;&#27169;&#22411;&#22312;&#21435;&#23398;&#20064;&#21644;&#21097;&#20313;&#25968;&#25454;&#19978;&#30340;&#24191;&#20041;&#24615;&#33021;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#35780;&#20272;&#21435;&#23398;&#20064;&#25928;&#26524;&#21644;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#25351;&#26631;&#65292;&#21363;&#36951;&#24536;&#29575;&#21644;&#35760;&#24518;&#20445;&#30041;&#29575;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19981;&#31934;&#30830;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#65292;&#29616;&#26377;&#30340;&#25351;&#26631;&#26080;&#27861;&#23545;&#21435;&#23398;&#20064;&#31243;&#24230;&#36827;&#34892;&#20934;&#30830;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning techniques, which involve retracting data records and reducing influence of said data on trained models, help with the user privacy protection objective but incur significant computational costs. Weight perturbation-based unlearning is a general approach, but it typically involves globally modifying the parameters. We propose fine-grained Top-K and Random-k parameters perturbed inexact machine unlearning strategies that address the privacy needs while keeping the computational costs tractable.  In order to demonstrate the efficacy of our strategies we also tackle the challenge of evaluating the effectiveness of machine unlearning by considering the model's generalization performance across both unlearning and remaining data. To better assess the unlearning effect and model generalization, we propose novel metrics, namely, the forgetting rate and memory retention rate. However, for inexact machine unlearning, current metrics are inadequate in quantifying the degree of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#25628;&#32034;&#31354;&#38388;&#21644;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#20844;&#24335;&#21270;alpha&#38598;&#26469;&#29983;&#25104;&#21327;&#21516;&#20844;&#24335;&#21270;alpha&#22240;&#23376;&#65292;&#20174;&#32780;&#25552;&#39640;&#37327;&#21270;&#20132;&#26131;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.02710</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#37327;&#21270;&#20132;&#26131;&#20013;&#21327;&#21516;&#20844;&#24335;Alpha&#29983;&#25104;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Synergistic Formulaic Alpha Generation for Quantitative Trading based on Reinforcement Learning. (arXiv:2401.02710v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#25628;&#32034;&#31354;&#38388;&#21644;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#20844;&#24335;&#21270;alpha&#38598;&#26469;&#29983;&#25104;&#21327;&#21516;&#20844;&#24335;&#21270;alpha&#22240;&#23376;&#65292;&#20174;&#32780;&#25552;&#39640;&#37327;&#21270;&#20132;&#26131;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24335;&#21270;alpha&#22240;&#23376;&#30340;&#25366;&#25496;&#26159;&#25351;&#22312;&#32929;&#31080;&#24066;&#22330;&#20013;&#21457;&#29616;&#21644;&#24320;&#21457;&#29305;&#23450;&#30340;&#22240;&#23376;&#25110;&#25351;&#26631;&#65288;&#31216;&#20026;alpha&#22240;&#23376;&#65289;&#20197;&#29992;&#20110;&#37327;&#21270;&#20132;&#26131;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#22312;&#24222;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#39640;&#25928;&#22320;&#21457;&#29616;alpha&#22240;&#23376;&#65292;&#24120;&#24120;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25193;&#23637;&#25628;&#32034;&#31354;&#38388;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#20844;&#24335;&#21270;alpha&#38598;&#20316;&#20026;&#21021;&#22987;&#31181;&#23376;&#20540;&#26469;&#29983;&#25104;&#21327;&#21516;&#20844;&#24335;&#21270;alpha&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#20449;&#24687;&#31995;&#25968;&#65288;IC&#65289;&#21644;&#25490;&#21517;&#20449;&#24687;&#31995;&#25968;&#65288;Rank IC&#65289;&#20316;&#20026;&#27169;&#22411;&#30340;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#12290;&#36890;&#36807;&#20351;&#29992;CSI300&#24066;&#22330;&#25968;&#25454;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#38469;&#25237;&#36164;&#27169;&#25311;&#65292;&#24182;&#35266;&#23519;&#21040;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mining of formulaic alpha factors refers to the process of discovering and developing specific factors or indicators (referred to as alpha factors) for quantitative trading in stock market. To efficiently discover alpha factors in vast search space, reinforcement learning (RL) is commonly employed. This paper proposes a method to enhance existing alpha factor mining approaches by expanding a search space and utilizing pretrained formulaic alpha set as initial seed values to generate synergistic formulaic alpha. We employ information coefficient (IC) and rank information coefficient (Rank IC) as performance evaluation metrics for the model. Using CSI300 market data, we conducted real investment simulations and observed significant performance improvement compared to existing techniques.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#22270;&#23572;&#25991;&#30340;&#35760;&#24518;&#29702;&#35770;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#24847;&#35782;&#21487;&#33021;&#26159;&#19968;&#31181;&#22522;&#20110;&#36825;&#31181;&#23545;&#24212;&#20851;&#31995;&#30340;&#26032;&#20852;&#33021;&#21147;&#30340;&#29468;&#24819;&#12290;</title><link>http://arxiv.org/abs/2401.02509</link><description>&lt;p&gt;
&#35760;&#24518;&#12289;&#24847;&#35782;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Memory, Consciousness and Large Language Model. (arXiv:2401.02509v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02509
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#22270;&#23572;&#25991;&#30340;&#35760;&#24518;&#29702;&#35770;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#24847;&#35782;&#21487;&#33021;&#26159;&#19968;&#31181;&#22522;&#20110;&#36825;&#31181;&#23545;&#24212;&#20851;&#31995;&#30340;&#26032;&#20852;&#33021;&#21147;&#30340;&#29468;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35748;&#30693;&#31185;&#23398;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#65292;&#36825;&#20004;&#20010;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#32852;&#31995;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#25581;&#31034;&#20986;&#26469;&#12290;&#22312;&#36825;&#20123;&#32852;&#31995;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29468;&#24819;&#65292;&#21363;LLM&#21644;&#22270;&#23572;&#25991;&#30340;&#35760;&#24518;&#29702;&#35770;&#20043;&#38388;&#23384;&#22312;&#19968;&#31181;&#23545;&#20598;&#20851;&#31995;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#22270;&#23572;&#25991;&#30340;&#21327;&#21516;&#24341;&#21457;&#65288;SEM&#65289;&#26816;&#32034;&#27169;&#22411;&#21644;LLM&#20013;&#35266;&#23519;&#21040;&#30340;&#26032;&#20852;&#33021;&#21147;&#20043;&#38388;&#30340;&#28508;&#22312;&#23545;&#24212;&#20851;&#31995;&#65292;&#20026;&#25105;&#20204;&#30340;&#29468;&#24819;&#25552;&#20379;&#20102;&#25903;&#25345;&#35777;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25512;&#27979;&#24847;&#35782;&#21487;&#33021;&#34987;&#35748;&#20026;&#26159;&#36825;&#31181;&#23545;&#20598;&#24615;&#30340;&#19968;&#31181;&#26032;&#20852;&#33021;&#21147;&#24418;&#24335;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#20854;&#20182;&#24847;&#35782;&#29702;&#35770;&#22914;&#20309;&#19982;&#25105;&#20204;&#30340;&#30740;&#31350;&#30456;&#20132;&#21449;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development in cognitive science and Large Language Models (LLMs), increasing connections have come to light between these two distinct fields. Building upon these connections, we propose a conjecture suggesting the existence of a duality between LLMs and Tulving's theory of memory. We identify a potential correspondence between Tulving's synergistic ecphory model (SEM) of retrieval and the emergent abilities observed in LLMs, serving as supporting evidence for our conjecture. Furthermore, we speculate that consciousness may be considered a form of emergent ability based on this duality. We also discuss how other theories of consciousness intersect with our research.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#21644;&#27604;&#36739;Transformer&#27169;&#22411;&#20013;&#31867;&#20284;&#30340;&#24207;&#21015;&#32487;&#32493;&#20219;&#21153;&#30340;&#30005;&#36335;&#65292;&#30740;&#31350;&#21457;&#29616;&#20849;&#20139;&#30340;&#35745;&#31639;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#34892;&#20026;&#39044;&#27979;&#33021;&#21147;&#12289;&#38169;&#35823;&#35782;&#21035;&#33021;&#21147;&#21644;&#32534;&#36753;&#36807;&#31243;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.04131</link><description>&lt;p&gt;
&#22312;Transformer&#20013;&#23450;&#20301;&#36328;&#20219;&#21153;&#24207;&#21015;&#32487;&#32493;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Locating Cross-Task Sequence Continuation Circuits in Transformers. (arXiv:2311.04131v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.04131
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#21644;&#27604;&#36739;Transformer&#27169;&#22411;&#20013;&#31867;&#20284;&#30340;&#24207;&#21015;&#32487;&#32493;&#20219;&#21153;&#30340;&#30005;&#36335;&#65292;&#30740;&#31350;&#21457;&#29616;&#20849;&#20139;&#30340;&#35745;&#31639;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#34892;&#20026;&#39044;&#27979;&#33021;&#21147;&#12289;&#38169;&#35823;&#35782;&#21035;&#33021;&#21147;&#21644;&#32534;&#36753;&#36807;&#31243;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;Transformer&#27169;&#22411;&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#20854;&#22797;&#26434;&#30340;&#26550;&#26500;&#20351;&#20854;&#38590;&#20197;&#35299;&#37322;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26088;&#22312;&#23558;Transformer&#27169;&#22411;&#36824;&#21407;&#20026;&#21487;&#35835;&#30340;&#30005;&#36335;&#34920;&#31034;&#65292;&#29992;&#20110;&#23454;&#29616;&#31639;&#27861;&#21151;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#21644;&#27604;&#36739;&#31867;&#20284;&#30340;&#24207;&#21015;&#32487;&#32493;&#20219;&#21153;&#30340;&#30005;&#36335;&#26469;&#25193;&#23637;&#36825;&#39033;&#30740;&#31350;&#65292;&#20854;&#20013;&#21253;&#25324;&#25968;&#23383;&#12289;&#25968;&#23383;&#35789;&#21644;&#26376;&#20221;&#30340;&#36882;&#22686;&#24207;&#21015;&#12290;&#36890;&#36807;&#24212;&#29992;&#30005;&#36335;&#20998;&#26512;&#25216;&#26415;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36127;&#36131;&#26816;&#27979;&#24207;&#21015;&#25104;&#21592;&#21644;&#39044;&#27979;&#24207;&#21015;&#20013;&#19979;&#19968;&#20010;&#25104;&#21592;&#30340;&#20851;&#38190;&#23376;&#30005;&#36335;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#35821;&#20041;&#30456;&#20851;&#24207;&#21015;&#20381;&#36182;&#20110;&#20855;&#26377;&#31867;&#20284;&#20316;&#29992;&#30340;&#20849;&#20139;&#30005;&#36335;&#23376;&#22270;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#35760;&#24405;&#20849;&#20139;&#30340;&#35745;&#31639;&#32467;&#26500;&#33021;&#22815;&#26356;&#22909;&#22320;&#39044;&#27979;&#27169;&#22411;&#34892;&#20026;&#65292;&#35782;&#21035;&#38169;&#35823;&#65292;&#24182;&#36827;&#34892;&#26356;&#23433;&#20840;&#30340;&#32534;&#36753;&#36807;&#31243;&#12290;&#36825;&#31181;&#23545;Transformer&#30340;&#26426;&#26800;&#29702;&#35299;&#26159;&#26500;&#24314;&#26356;&#20581;&#22766;&#12289;&#35843;&#35797;&#21644;&#32534;&#36753;&#26356;&#23433;&#20840;&#30340;&#27169;&#22411;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
While transformer models exhibit strong capabilities on linguistic tasks, their complex architectures make them difficult to interpret. Recent work has aimed to reverse engineer transformer models into human-readable representations called circuits that implement algorithmic functions. We extend this research by analyzing and comparing circuits for similar sequence continuation tasks, which include increasing sequences of digits, number words, and months. Through the application of circuit analysis techniques, we identify key sub-circuits responsible for detecting sequence members and for predicting the next member in a sequence. Our analysis reveals that semantically related sequences rely on shared circuit subgraphs with analogous roles. Overall, documenting shared computational structures enables better prediction of model behaviors, identification of errors, and safer editing procedures. This mechanistic understanding of transformers is a critical step towards building more robust,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;LLM&#22312;&#25968;&#23398;&#24212;&#29992;&#39064;&#20013;&#30340;&#36870;&#21521;&#25512;&#29702;&#33021;&#21147;&#65292;&#21457;&#29616;&#22312;&#36870;&#21521;&#25512;&#29702;&#20219;&#21153;&#19978;&#65292;LLM&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#26174;&#33879;&#19979;&#38477;&#12290;&#36890;&#36807;&#25913;&#36827;&#25216;&#26415;&#65292;&#22914;Rephrase&#21644;PAL-Tools&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.01991</link><description>&lt;p&gt;
&#22635;&#31354;&#39064;&#65306;&#25506;&#32034;&#24182;&#22686;&#24378;LLM&#22312;&#25968;&#23398;&#24212;&#29992;&#39064;&#20013;&#30340;&#36870;&#21521;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward Reasoning in Math Word Problems. (arXiv:2310.01991v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;LLM&#22312;&#25968;&#23398;&#24212;&#29992;&#39064;&#20013;&#30340;&#36870;&#21521;&#25512;&#29702;&#33021;&#21147;&#65292;&#21457;&#29616;&#22312;&#36870;&#21521;&#25512;&#29702;&#20219;&#21153;&#19978;&#65292;LLM&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#26174;&#33879;&#19979;&#38477;&#12290;&#36890;&#36807;&#25913;&#36827;&#25216;&#26415;&#65292;&#22914;Rephrase&#21644;PAL-Tools&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#36817;&#26399;&#30340;&#25991;&#29486;&#20013;&#24191;&#27867;&#25506;&#35752;&#20102;&#27491;&#21521;&#25512;&#29702;&#65288;&#21363;&#32473;&#23450;&#38382;&#39064;&#25214;&#31572;&#26696;&#65289;&#65292;&#20294;&#36870;&#21521;&#25512;&#29702;&#30456;&#23545;&#36739;&#23569;&#34987;&#30740;&#31350;&#12290;&#25105;&#20204;&#23545;LLM&#22312;&#25968;&#23398;&#24212;&#29992;&#39064;&#20013;&#30340;&#36870;&#21521;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#25506;&#35752;&#65306;&#32473;&#23450;&#19968;&#20010;&#25968;&#23398;&#38382;&#39064;&#21644;&#20854;&#31572;&#26696;&#65292;&#22312;&#38382;&#39064;&#20013;&#26377;&#20123;&#32454;&#33410;&#34987;&#30465;&#30053;&#20102;&#65292;LLM&#33021;&#21542;&#26377;&#25928;&#22320;&#36824;&#21407;&#20986;&#32570;&#22833;&#30340;&#20449;&#24687;&#65311;&#26412;&#25991;&#27491;&#24335;&#23450;&#20041;&#20102;&#25968;&#23398;&#24212;&#29992;&#39064;&#20013;&#30340;&#36870;&#21521;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#20462;&#25913;&#20102;&#19977;&#20010;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#36825;&#19968;&#20219;&#21153;&#65306;GSM8k&#12289;SVAMP&#21644;MultiArith&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#27491;&#21521;&#25512;&#29702;&#30456;&#27604;&#65292;&#22235;&#20010;&#26368;&#20808;&#36827;&#30340;LLM&#27169;&#22411;&#65288;GPT4&#12289;GPT3.5&#12289;PaLM-2&#21644;LLaMa-2&#65289;&#22312;&#36870;&#21521;&#25512;&#29702;&#19978;&#30340;&#20934;&#30830;&#24615;&#26174;&#33879;&#19979;&#38477;&#12290;&#21033;&#29992;&#35813;&#20219;&#21153;&#30340;&#29305;&#23450;&#26684;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#25913;&#36827;&#24615;&#33021;&#30340;&#26032;&#25216;&#26415;&#65306;Rephrase&#23558;&#32473;&#23450;&#30340;&#38382;&#39064;&#37325;&#36848;&#20026;&#19968;&#20010;&#27491;&#21521;&#25512;&#29702;&#38382;&#39064;&#65292;PAL-Tools&#32467;&#21512;&#20102;&#31243;&#24207;&#36741;&#21161;&#30340;LLM&#24605;&#24819;&#65292;&#29983;&#25104;&#19968;&#32452;&#26041;&#31243;&#24335;&#21487;&#20197;&#35299;&#20915;&#32570;&#22833;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
While forward reasoning (i.e. find the answer given the question) has been explored extensively in the recent literature, backward reasoning is relatively unexplored. We examine the backward reasoning capabilities of LLMs on Math Word Problems (MWPs): given a mathematical question and its answer, with some details omitted from the question, can LLMs effectively retrieve the missing information?  In this paper, we formally define the backward reasoning task on math word problems and modify three datasets to evaluate this task: GSM8k, SVAMP and MultiArith. Our findings show a significant drop in the accuracy of models on backward reasoning compared to forward reasoning across four SOTA LLMs (GPT4, GPT3.5, PaLM-2, and LLaMa-2). Utilizing the specific format of this task, we propose three novel techniques that improve performance: Rephrase reformulates the given problem into a forward reasoning problem, PAL-Tools combines the idea of Program-Aided LLMs to produce a set of equations that ca
&lt;/p&gt;</description></item><item><title>FUTURE-AI&#26159;&#31532;&#19968;&#20010;&#22269;&#38469;&#20849;&#35782;&#26694;&#26550;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#21487;&#20449;AI&#24037;&#20855;&#24320;&#21457;&#21644;&#37096;&#32626;&#25552;&#20379;&#25351;&#23548;&#21407;&#21017;&#21644;&#26368;&#20339;&#23454;&#36341;&#12290;</title><link>http://arxiv.org/abs/2309.12325</link><description>&lt;p&gt;
FUTURE-AI&#65306;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#21487;&#20449;&#21644;&#21487;&#37096;&#32626;&#20154;&#24037;&#26234;&#33021;&#30340;&#22269;&#38469;&#20849;&#35782;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
FUTURE-AI: International consensus guideline for trustworthy and deployable artificial intelligence in healthcare. (arXiv:2309.12325v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12325
&lt;/p&gt;
&lt;p&gt;
FUTURE-AI&#26159;&#31532;&#19968;&#20010;&#22269;&#38469;&#20849;&#35782;&#26694;&#26550;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#21487;&#20449;AI&#24037;&#20855;&#24320;&#21457;&#21644;&#37096;&#32626;&#25552;&#20379;&#25351;&#23548;&#21407;&#21017;&#21644;&#26368;&#20339;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#21307;&#23398;&#21644;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;AI&#25216;&#26415;&#22312;&#29616;&#23454;&#20020;&#24202;&#23454;&#36341;&#20013;&#30340;&#37096;&#32626;&#21644;&#37319;&#29992;&#20173;&#21463;&#38480;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#21307;&#30103;AI&#30340;&#25216;&#26415;&#12289;&#20020;&#24202;&#12289;&#20262;&#29702;&#21644;&#27861;&#24459;&#39118;&#38505;&#25552;&#20986;&#20102;&#20851;&#27880;&#12290;&#20026;&#20102;&#22686;&#21152;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#37319;&#29992;&#65292;&#21307;&#30103;AI&#24037;&#20855;&#24517;&#39035;&#24471;&#21040;&#24739;&#32773;&#12289;&#20020;&#24202;&#21307;&#29983;&#12289;&#20581;&#24247;&#32452;&#32455;&#21644;&#24403;&#23616;&#30340;&#20449;&#20219;&#21644;&#25509;&#21463;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;FUTURE-AI&#25351;&#21335;&#20316;&#20026;&#31532;&#19968;&#20010;&#29992;&#20110;&#25351;&#23548;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#21487;&#20449;AI&#24037;&#20855;&#24320;&#21457;&#21644;&#37096;&#32626;&#30340;&#22269;&#38469;&#20849;&#35782;&#26694;&#26550;&#12290;FUTURE-AI&#32852;&#30431;&#25104;&#31435;&#20110;2021&#24180;&#65292;&#30446;&#21069;&#21253;&#25324;&#26469;&#33258;51&#20010;&#22269;&#23478;&#30340;118&#20301;&#36328;&#23398;&#31185;&#19987;&#23478;&#65292;&#20195;&#34920;&#20102;&#25152;&#26377;&#22823;&#27954;&#65292;&#21253;&#25324;AI&#31185;&#23398;&#23478;&#12289;&#20020;&#24202;&#21307;&#29983;&#12289;&#20262;&#29702;&#23398;&#23478;&#21644;&#31038;&#20250;&#31185;&#23398;&#23478;&#12290;&#22312;&#20026;&#26399;&#20004;&#24180;&#30340;&#26102;&#38388;&#37324;&#65292;&#32852;&#30431;&#36890;&#36807;&#36845;&#20195;&#36807;&#31243;&#23450;&#20041;&#20102;&#21487;&#20449;AI&#30340;&#25351;&#23548;&#21407;&#21017;&#21644;&#26368;&#20339;&#23454;&#36341;&#65292;&#20854;&#20013;&#21253;&#25324;
&lt;/p&gt;
&lt;p&gt;
Despite major advances in artificial intelligence (AI) for medicine and healthcare, the deployment and adoption of AI technologies remain limited in real-world clinical practice. In recent years, concerns have been raised about the technical, clinical, ethical and legal risks associated with medical AI. To increase real world adoption, it is essential that medical AI tools are trusted and accepted by patients, clinicians, health organisations and authorities. This work describes the FUTURE-AI guideline as the first international consensus framework for guiding the development and deployment of trustworthy AI tools in healthcare. The FUTURE-AI consortium was founded in 2021 and currently comprises 118 inter-disciplinary experts from 51 countries representing all continents, including AI scientists, clinicians, ethicists, and social scientists. Over a two-year period, the consortium defined guiding principles and best practices for trustworthy AI through an iterative process comprising a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#21160;&#35780;&#20998;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#24378;&#35843;&#20102;&#23427;&#20204;&#22914;&#20309;&#25903;&#25345;&#25945;&#32946;&#24037;&#20316;&#32773;&#39564;&#35777;&#35780;&#20998;&#31243;&#24207;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#8220;&#24320;&#31665;&#21363;&#29992;&#8221;&#30340;LLMs&#20316;&#20026;&#34917;&#20805;&#35270;&#35282;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#20294;&#20173;&#38656;&#36827;&#19968;&#27493;&#20248;&#21270;&#20854;&#21487;&#29992;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11508</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#30340;&#30701;&#25991;&#26412;&#31572;&#26696;&#33258;&#21160;&#35780;&#20998;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards LLM-based Autograding for Short Textual Answers. (arXiv:2309.11508v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#21160;&#35780;&#20998;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#24378;&#35843;&#20102;&#23427;&#20204;&#22914;&#20309;&#25903;&#25345;&#25945;&#32946;&#24037;&#20316;&#32773;&#39564;&#35777;&#35780;&#20998;&#31243;&#24207;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#8220;&#24320;&#31665;&#21363;&#29992;&#8221;&#30340;LLMs&#20316;&#20026;&#34917;&#20805;&#35270;&#35282;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#20294;&#20173;&#38656;&#36827;&#19968;&#27493;&#20248;&#21270;&#20854;&#21487;&#29992;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#35797;&#30340;&#35780;&#20998;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#12289;&#21171;&#21160;&#23494;&#38598;&#30340;&#12289;&#20027;&#35266;&#30340;&#12289;&#37325;&#22797;&#30340;&#19988;&#24120;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#30340;&#21487;&#29992;&#24615;&#21644;&#25968;&#23383;&#21270;&#24102;&#26469;&#30340;&#22823;&#37327;&#25968;&#25454;&#30340;&#28044;&#20837;&#65292; greatly increased autograding textual responses&#30340;&#21487;&#34892;&#24615;&#12290;&#28982;&#32780;&#65292;&#23558;&#20915;&#31574;&#35282;&#33394;&#20132;&#32473;AI&#27169;&#22411;&#24341;&#36215;&#20102;&#20262;&#29702;&#32771;&#34385;&#65292;&#20027;&#35201;&#28304;&#20110;&#28508;&#22312;&#20559;&#35265;&#21644;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#33258;&#21160;&#35780;&#20998;&#65292;&#21516;&#26102;&#24378;&#35843;&#20102;LLMs&#22914;&#20309;&#25903;&#25345;&#25945;&#32946;&#24037;&#20316;&#32773;&#39564;&#35777;&#20854;&#35780;&#20998;&#31243;&#24207;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#38024;&#23545;&#33258;&#21160;&#30701;&#25991;&#26412;&#31572;&#26696;&#35780;&#20998;&#65288;ASAG&#65289;&#65292;&#28085;&#30422;&#20102;&#20004;&#20010;&#19981;&#21516;&#35838;&#31243;&#30340;&#21508;&#31181;&#35821;&#35328;&#21644;&#32771;&#35797;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#8220;&#24320;&#31665;&#21363;&#29992;&#8221;&#30340;LLMs&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#25552;&#20379;&#34917;&#20805;&#30340;&#35270;&#35282;&#65292;&#20294;&#23427;&#20204;&#30340;&#21487;&#29992;&#24615;&#21644;&#24615;&#33021;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#36824;&#38656;&#36827;&#19968;&#27493;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Grading of exams is an important, labor intensive, subjective, repetitive and frequently challenging task. The feasibility of autograding textual responses has greatly increased thanks to the availability of large language models (LLMs) such as ChatGPT and because of the substantial influx of data brought about by digitalization. However, entrusting AI models with decision-making roles raises ethical considerations, mainly stemming from potential biases and issues related to generating false information. Thus, in this manuscript we provide an evaluation of a large language model for the purpose of autograding, while also highlighting how LLMs can support educators in validating their grading procedures. Our evaluation is targeted towards automatic short textual answers grading (ASAG), spanning various languages and examinations from two distinct courses. Our findings suggest that while "out-of-the-box" LLMs provide a valuable tool to provide a complementary perspective, their readiness
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SAMUS&#65292;&#19968;&#20010;&#19987;&#20026;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#37327;&#36523;&#23450;&#21046;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#24182;&#34892;CNN&#20998;&#25903;&#21644;&#36866;&#37197;&#22120;&#26469;&#25913;&#21892;SAM&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.06824</link><description>&lt;p&gt;
SAMUS&#65306;&#20026;&#20020;&#24202;&#21451;&#22909;&#21644;&#27867;&#21270;&#24615;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#35843;&#25972;&#30340;&#20219;&#24847;&#20998;&#21106;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SAMUS: Adapting Segment Anything Model for Clinically-Friendly and Generalizable Ultrasound Image Segmentation. (arXiv:2309.06824v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SAMUS&#65292;&#19968;&#20010;&#19987;&#20026;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#37327;&#36523;&#23450;&#21046;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#24182;&#34892;CNN&#20998;&#25903;&#21644;&#36866;&#37197;&#22120;&#26469;&#25913;&#21892;SAM&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#24847;&#20998;&#21106;&#27169;&#22411;&#65288;SAM&#65289;&#26159;&#19968;&#31181;&#21331;&#36234;&#30340;&#36890;&#29992;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#39046;&#22495;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;SAM&#22312;&#33258;&#28982;&#22270;&#20687;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#22788;&#29702;&#21307;&#23398;&#22270;&#20687;&#26102;&#65292;&#29305;&#21035;&#26159;&#28041;&#21450;&#20302;&#23545;&#27604;&#24230;&#12289;&#27169;&#31946;&#36793;&#30028;&#12289;&#22797;&#26434;&#24418;&#29366;&#21644;&#23567;&#23610;&#23544;&#23545;&#35937;&#30340;&#22270;&#20687;&#26102;&#65292;SAM&#38754;&#20020;&#30528;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#21644;&#26377;&#38480;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;SAMUS&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#37327;&#36523;&#23450;&#21046;&#30340;&#36890;&#29992;&#27169;&#22411;&#12290;&#19982;&#20197;&#21069;&#22522;&#20110;SAM&#30340;&#36890;&#29992;&#27169;&#22411;&#19981;&#21516;&#65292;SAMUS&#36861;&#27714;&#30340;&#19981;&#20165;&#26159;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36824;&#26377;&#26356;&#20302;&#30340;&#37096;&#32626;&#25104;&#26412;&#65292;&#20351;&#20854;&#26356;&#36866;&#21512;&#20020;&#24202;&#24212;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;SAM&#30340;&#22522;&#30784;&#19978;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#24182;&#34892;CNN&#20998;&#25903;&#65292;&#36890;&#36807;&#36328;&#20998;&#25903;&#27880;&#24847;&#21147;&#23558;&#23616;&#37096;&#29305;&#24449;&#27880;&#20837;ViT&#32534;&#30721;&#22120;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;&#28982;&#21518;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#20301;&#32622;&#36866;&#37197;&#22120;&#21644;&#19968;&#20010;&#29305;&#24449;&#36866;&#37197;&#22120;&#26469;&#35843;&#25972;SAM&#30340;&#36755;
&lt;/p&gt;
&lt;p&gt;
Segment anything model (SAM), an eminent universal image segmentation model, has recently gathered considerable attention within the domain of medical image segmentation. Despite the remarkable performance of SAM on natural images, it grapples with significant performance degradation and limited generalization when confronted with medical images, particularly with those involving objects of low contrast, faint boundaries, intricate shapes, and diminutive sizes. In this paper, we propose SAMUS, a universal model tailored for ultrasound image segmentation. In contrast to previous SAM-based universal models, SAMUS pursues not only better generalization but also lower deployment cost, rendering it more suitable for clinical applications. Specifically, based on SAM, a parallel CNN branch is introduced to inject local features into the ViT encoder through cross-branch attention for better medical image segmentation. Then, a position adapter and a feature adapter are developed to adapt SAM fr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#20351;&#22235;&#36275;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#20687;&#30495;&#23454;&#21160;&#29289;&#19968;&#26679;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#31574;&#30053;&#12290;&#36890;&#36807;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#20445;&#30041;&#20102;&#21160;&#29289;&#34892;&#20026;&#30340;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#36866;&#24212;&#29615;&#22659;&#65292;&#20811;&#26381;&#25361;&#25112;&#24615;&#30340;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2308.15143</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22235;&#36275;&#26426;&#22120;&#20154;&#19978;&#23454;&#29616;&#29983;&#21160;&#30340;&#28789;&#27963;&#24615;&#21644;&#28216;&#25103;&#24615;
&lt;/p&gt;
&lt;p&gt;
Lifelike Agility and Play on Quadrupedal Robots using Reinforcement Learning and Generative Pre-trained Models. (arXiv:2308.15143v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15143
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#20351;&#22235;&#36275;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#20687;&#30495;&#23454;&#21160;&#29289;&#19968;&#26679;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#31574;&#30053;&#12290;&#36890;&#36807;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#20445;&#30041;&#20102;&#21160;&#29289;&#34892;&#20026;&#30340;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#36866;&#24212;&#29615;&#22659;&#65292;&#20811;&#26381;&#25361;&#25112;&#24615;&#30340;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24635;&#32467;&#21160;&#29289;&#21644;&#20154;&#31867;&#30340;&#30693;&#35782;&#21551;&#21457;&#20102;&#26426;&#22120;&#20154;&#21019;&#26032;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#20351;&#22235;&#36275;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#20687;&#30495;&#23454;&#21160;&#29289;&#19968;&#26679;&#25317;&#26377;&#29983;&#21160;&#30340;&#28789;&#27963;&#24615;&#21644;&#31574;&#30053;&#12290;&#21463;&#21040;&#22312;&#35821;&#35328;&#21644;&#22270;&#20687;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20808;&#36827;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20197;&#29983;&#25104;&#27169;&#25311;&#30495;&#23454;&#21160;&#29289;&#21160;&#20316;&#30340;&#36816;&#21160;&#25511;&#21046;&#20449;&#21495;&#12290;&#19982;&#20256;&#32479;&#25511;&#21046;&#22120;&#21644;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21482;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#21160;&#29289;&#36816;&#21160;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#20445;&#30041;&#26377;&#34920;&#36798;&#21147;&#30340;&#21160;&#29289;&#34892;&#20026;&#30693;&#35782;&#12290;&#39044;&#35757;&#32451;&#27169;&#22411;&#25317;&#26377;&#36275;&#22815;&#30340;&#21407;&#22987;&#32423;&#30693;&#35782;&#65292;&#20294;&#19982;&#29615;&#22659;&#26080;&#20851;&#12290;&#28982;&#21518;&#65292;&#22312;&#23398;&#20064;&#30340;&#21518;&#32493;&#38454;&#27573;&#65292;&#36890;&#36807;&#31359;&#36234;&#19968;&#20123;&#20197;&#21069;&#30340;&#26041;&#27861;&#24456;&#23569;&#32771;&#34385;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38556;&#30861;&#65292;&#22914;&#31359;&#36807;&#29421;&#31364;&#30340;&#31354;&#38388;&#31561;&#65292;&#20351;&#20854;&#36866;&#24212;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Summarizing knowledge from animals and human beings inspires robotic innovations. In this work, we propose a framework for driving legged robots act like real animals with lifelike agility and strategy in complex environments. Inspired by large pre-trained models witnessed with impressive performance in language and image understanding, we introduce the power of advanced deep generative models to produce motor control signals stimulating legged robots to act like real animals. Unlike conventional controllers and end-to-end RL methods that are task-specific, we propose to pre-train generative models over animal motion datasets to preserve expressive knowledge of animal behavior. The pre-trained model holds sufficient primitive-level knowledge yet is environment-agnostic. It is then reused for a successive stage of learning to align with the environments by traversing a number of challenging obstacles that are rarely considered in previous approaches, including creeping through narrow sp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25253;&#21578;&#20102;&#20851;&#20110;&#27491;&#24335;&#21270;&#26465;&#20214;&#25512;&#29702;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#21253;&#25324;Aqvist&#30340;&#26465;&#20214;&#20041;&#21153;&#31995;&#32479;E&#30340;&#26426;&#26800;&#21270;&#21644;&#20262;&#29702;&#35770;&#25454;&#35780;&#20272;&#30340;&#24037;&#20855;&#30340;&#24320;&#21457;&#12290;</title><link>http://arxiv.org/abs/2308.10686</link><description>&lt;p&gt;
&#24635;&#25324;&#33655;&#23572;&#33945;&#20307;&#31995;&#20316;&#20026;HOL&#30340;&#19968;&#20010;&#29255;&#27573;
&lt;/p&gt;
&lt;p&gt;
Normative Conditional Reasoning as a Fragment of HOL. (arXiv:2308.10686v2 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25253;&#21578;&#20102;&#20851;&#20110;&#27491;&#24335;&#21270;&#26465;&#20214;&#25512;&#29702;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#21253;&#25324;Aqvist&#30340;&#26465;&#20214;&#20041;&#21153;&#31995;&#32479;E&#30340;&#26426;&#26800;&#21270;&#21644;&#20262;&#29702;&#35770;&#25454;&#35780;&#20272;&#30340;&#24037;&#20855;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25253;&#21578;&#20102;&#20851;&#20110;&#27491;&#24335;&#21270;&#65288;&#22522;&#20110;&#20559;&#22909;&#30340;&#65289;&#26465;&#20214;&#25512;&#29702;&#30340;&#19968;&#20123;&#32467;&#26524;&#12290;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;Aqvist&#30340;&#26465;&#20214;&#20041;&#21153;&#31995;&#32479;E&#65288;&#21450;&#20854;&#25193;&#23637;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;Isabelle/HOL&#20013;&#30340;&#27973;&#34920;&#35821;&#20041;&#23884;&#20837;&#26469;&#23454;&#29616;&#25105;&#20204;&#30340;&#27491;&#24335;&#21270;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#35813;&#26694;&#26550;&#30340;&#20004;&#31181;&#21487;&#33021;&#29992;&#36884;&#12290;&#31532;&#19968;&#31181;&#26159;&#20316;&#20026;&#23545;&#25152;&#32771;&#34385;&#36923;&#36753;&#36827;&#34892;&#20803;&#25512;&#29702;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#23558;&#20854;&#29992;&#20110;&#33258;&#21160;&#39564;&#35777;&#26435;&#21033;&#20041;&#21153;&#23545;&#24212;&#20851;&#31995;&#65288;&#24191;&#20041;&#19978;&#29702;&#35299;&#65289;&#21450;&#30456;&#20851;&#20107;&#39033;&#65292;&#31867;&#20284;&#20110;&#20043;&#21069;&#23545;&#27169;&#24577;&#36923;&#36753;&#31435;&#26041;&#20307;&#25152;&#21462;&#24471;&#30340;&#25104;&#26524;&#12290;&#31532;&#20108;&#31181;&#29992;&#36884;&#26159;&#20316;&#20026;&#20262;&#29702;&#35770;&#25454;&#35780;&#20272;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20154;&#21475;&#20262;&#29702;&#23398;&#20013;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#24726;&#35770;Parfit&#30340;&#20196;&#20154;&#21388;&#24694;&#30340;&#32467;&#35770;&#30340;&#35745;&#31639;&#26426;&#32534;&#30721;&#12290;&#22914;&#20309;&#36890;&#36807;&#36825;&#20010;&#32534;&#30721;&#22686;&#21152;&#25110;&#20943;&#23569;&#20196;&#20154;&#21388;&#24694;&#30340;&#32467;&#35770;&#30340;&#21560;&#24341;&#21147;&#21644;&#35828;&#26381;&#21147;&#26159;&#19968;&#20010;&#25105;&#20204;&#24076;&#26395;&#21521;&#21746;&#23398;&#21644;&#20262;&#29702;&#23398;&#25552;&#20986;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report some results regarding the mechanization of normative (preference-based) conditional reasoning. Our focus is on Aqvist's system E for conditional obligation (and its extensions). Our mechanization is achieved via a shallow semantical embedding in Isabelle/HOL. We consider two possible uses of the framework. The first one is as a tool for meta-reasoning about the considered logic. We employ it for the automated verification of deontic correspondences (broadly conceived) and related matters, analogous to what has been previously achieved for the modal logic cube. The second use is as a tool for assessing ethical arguments. We provide a computer encoding of a well-known paradox in population ethics, Parfit's repugnant conclusion. Whether the presented encoding increases or decreases the attractiveness and persuasiveness of the repugnant conclusion is a question we would like to pass on to philosophy and ethics.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35268;&#21010;&#26412;&#20307;&#34920;&#31034;&#21644;&#21033;&#29992;&#35268;&#21010;&#30693;&#35782;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#33258;&#21160;&#35268;&#21010;&#30340;&#24615;&#33021;&#25928;&#29575;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#35268;&#21010;&#26412;&#20307;&#65292;&#24182;&#21033;&#29992;&#22269;&#38469;&#35268;&#21010;&#31454;&#36187;&#30340;&#25968;&#25454;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#26412;&#20307;&#33021;&#22815;&#36873;&#25321;&#26377;&#21069;&#26223;&#30340;&#35268;&#21010;&#22120;&#65292;&#24182;&#20351;&#29992;&#20174;&#26412;&#20307;&#20013;&#25552;&#21462;&#30340;&#23439;&#35266;&#32422;&#26463;&#26469;&#25552;&#39640;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.13549</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#24615;&#33021;&#25928;&#29575;&#30340;&#35268;&#21010;&#26412;&#20307;&#34920;&#31034;&#21644;&#21033;&#29992;&#35268;&#21010;&#30693;&#35782;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Planning Ontology to Represent and Exploit Planning Knowledge for Performance Efficiency. (arXiv:2307.13549v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35268;&#21010;&#26412;&#20307;&#34920;&#31034;&#21644;&#21033;&#29992;&#35268;&#21010;&#30693;&#35782;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#33258;&#21160;&#35268;&#21010;&#30340;&#24615;&#33021;&#25928;&#29575;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#35268;&#21010;&#26412;&#20307;&#65292;&#24182;&#21033;&#29992;&#22269;&#38469;&#35268;&#21010;&#31454;&#36187;&#30340;&#25968;&#25454;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#26412;&#20307;&#33021;&#22815;&#36873;&#25321;&#26377;&#21069;&#26223;&#30340;&#35268;&#21010;&#22120;&#65292;&#24182;&#20351;&#29992;&#20174;&#26412;&#20307;&#20013;&#25552;&#21462;&#30340;&#23439;&#35266;&#32422;&#26463;&#26469;&#25552;&#39640;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#33258;&#21160;&#35268;&#21010;&#38382;&#39064;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#35268;&#21010;&#26412;&#20307;&#65292;&#21033;&#29992;&#22269;&#38469;&#35268;&#21010;&#31454;&#36187;&#65288;IPC&#65289;&#30340;&#35268;&#21010;&#39046;&#22495;&#21644;&#35268;&#21010;&#22120;&#30340;&#25968;&#25454;&#65292;&#22312;&#20004;&#20010;&#20351;&#29992;&#26696;&#20363;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#26412;&#20307;&#33021;&#22815;&#36873;&#25321;&#26377;&#21069;&#26223;&#30340;&#35268;&#21010;&#22120;&#65292;&#24182;&#36890;&#36807;&#20174;&#35268;&#21010;&#26412;&#20307;&#20013;&#25552;&#21462;&#30340;&#23439;&#35266;&#32422;&#26463;&#26469;&#25552;&#39640;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ontologies are known for their ability to organize rich metadata, support the identification of novel insights via semantic queries, and promote reuse. In this paper, we consider the problem of automated planning, where the objective is to find a sequence of actions that will move an agent from an initial state of the world to a desired goal state. We hypothesize that given a large number of available planners and diverse planning domains; they carry essential information that can be leveraged to identify suitable planners and improve their performance for a domain. We use data on planning domains and planners from the International Planning Competition (IPC) to construct a planning ontology and demonstrate via experiments in two use cases that the ontology can lead to the selection of promising planners and improving their performance using macros - a form of action ordering constraints extracted from planning ontology. We also make the planning ontology and associated resources avail
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#33041;&#23545;&#40784;&#30340;&#30740;&#31350;&#65292;&#37325;&#28857;&#22312;&#20110;&#33041;&#32534;&#30721;&#21644;&#35299;&#30721;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#29702;&#35299;&#22823;&#33041;&#30340;&#20449;&#24687;&#22788;&#29702;&#26426;&#21046;&#20197;&#21450;&#35774;&#35745;&#33041;&#26426;&#25509;&#21475;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.10246</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#33041;&#23545;&#40784;&#65306;&#33041;&#32534;&#30721;&#21644;&#35299;&#30721;&#65288;&#32508;&#36848;&#65289;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding (Survey). (arXiv:2307.10246v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#33041;&#23545;&#40784;&#30340;&#30740;&#31350;&#65292;&#37325;&#28857;&#22312;&#20110;&#33041;&#32534;&#30721;&#21644;&#35299;&#30721;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#29702;&#35299;&#22823;&#33041;&#30340;&#20449;&#24687;&#22788;&#29702;&#26426;&#21046;&#20197;&#21450;&#35774;&#35745;&#33041;&#26426;&#25509;&#21475;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#22914;&#20309;&#34920;&#31034;&#19981;&#21516;&#30340;&#20449;&#24687;&#27169;&#24335;&#65311;&#25105;&#20204;&#33021;&#21542;&#35774;&#35745;&#20986;&#19968;&#20010;&#21487;&#20197;&#33258;&#21160;&#29702;&#35299;&#29992;&#25143;&#24605;&#32771;&#20869;&#23481;&#30340;&#31995;&#32479;&#65311;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#30740;&#31350;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#31561;&#22823;&#33041;&#35760;&#24405;&#26469;&#22238;&#31572;&#12290;&#20316;&#20026;&#31532;&#19968;&#27493;&#65292;&#31070;&#32463;&#31185;&#23398;&#30028;&#20026;&#34987;&#21160;&#38405;&#35835;/&#21548;&#35273;/&#35266;&#30475;&#27010;&#24565;&#35789;&#27719;&#12289;&#21465;&#36848;&#12289;&#22270;&#29255;&#21644;&#30005;&#24433;&#30456;&#20851;&#30340;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#25968;&#25454;&#38598;&#20316;&#20986;&#20102;&#36129;&#29486;&#12290;&#36807;&#21435;&#20108;&#21313;&#24180;&#20013;&#65292;&#36824;&#25552;&#20986;&#20102;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#32534;&#30721;&#21644;&#35299;&#30721;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#20316;&#20026;&#22522;&#30784;&#30740;&#31350;&#20013;&#30340;&#39069;&#22806;&#24037;&#20855;&#65292;&#22312;&#35748;&#30693;&#31185;&#23398;&#21644;&#31070;&#32463;&#31185;&#23398;&#39046;&#22495;&#26377;&#30528;&#22810;&#31181;&#23454;&#38469;&#24212;&#29992;&#12290;&#32534;&#30721;&#27169;&#22411;&#26088;&#22312;&#33258;&#21160;&#22320;&#29983;&#25104;fMRI&#22823;&#33041;&#34920;&#24449;&#65292;&#32473;&#23450;&#19968;&#20010;&#21050;&#28608;&#12290;&#23427;&#20204;&#22312;&#35780;&#20272;&#21644;&#35786;&#26029;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#20197;&#21450;&#35774;&#35745;&#22823;&#33041;&#25439;&#20260;&#27835;&#30103;&#26041;&#27861;&#26041;&#38754;&#26377;&#30528;&#22810;&#31181;&#23454;&#38469;&#24212;&#29992;&#12290;&#35299;&#30721;&#27169;&#22411;&#35299;&#20915;&#20102;&#26681;&#25454;fMRI&#37325;&#26500;&#21050;&#28608;&#30340;&#36870;&#38382;&#39064;&#12290;&#23427;&#20204;&#23545;&#20110;&#29702;&#35299;&#22823;&#33041;&#22914;&#20309;&#22788;&#29702;&#20449;&#24687;&#20197;&#21450;&#35774;&#35745;&#33041;&#26426;&#25509;&#21475;&#30340;&#21457;&#23637;&#37117;&#26377;&#30528;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
How does the brain represent different modes of information? Can we design a system that automatically understands what the user is thinking? Such questions can be answered by studying brain recordings like functional magnetic resonance imaging (fMRI). As a first step, the neuroscience community has contributed several large cognitive neuroscience datasets related to passive reading/listening/viewing of concept words, narratives, pictures and movies. Encoding and decoding models using these datasets have also been proposed in the past two decades. These models serve as additional tools for basic research in cognitive science and neuroscience. Encoding models aim at generating fMRI brain representations given a stimulus automatically. They have several practical applications in evaluating and diagnosing neurological conditions and thus also help design therapies for brain damage. Decoding models solve the inverse problem of reconstructing the stimuli given the fMRI. They are useful for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TbExplain&#30340;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;XAI&#25216;&#26415;&#21644;&#39044;&#35757;&#32451;&#30340;&#23545;&#35937;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#25991;&#26412;&#24418;&#24335;&#35299;&#37322;&#22330;&#26223;&#20998;&#31867;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#32416;&#27491;&#39044;&#27979;&#21644;&#36827;&#34892;&#25991;&#26412;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2307.10003</link><description>&lt;p&gt;
TbExplain: &#19968;&#31181;&#22330;&#26223;&#20998;&#31867;&#27169;&#22411;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#35299;&#37322;&#26041;&#27861;&#19982;&#32479;&#35745;&#39044;&#27979;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
TbExplain: A Text-based Explanation Method for Scene Classification Models with the Statistical Prediction Correction. (arXiv:2307.10003v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TbExplain&#30340;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;XAI&#25216;&#26415;&#21644;&#39044;&#35757;&#32451;&#30340;&#23545;&#35937;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#25991;&#26412;&#24418;&#24335;&#35299;&#37322;&#22330;&#26223;&#20998;&#31867;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#32416;&#27491;&#39044;&#27979;&#21644;&#36827;&#34892;&#25991;&#26412;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;(XAI)&#30340;&#39046;&#22495;&#26088;&#22312;&#25552;&#39640;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#24314;&#31435;&#22522;&#20110;&#36755;&#20837;&#29305;&#24449;&#37325;&#35201;&#24615;&#20540;&#30340;&#28909;&#22270;&#26159;&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#20135;&#29983;&#39044;&#27979;&#30340;&#22522;&#26412;&#26041;&#27861;&#20043;&#19968;&#12290;&#28909;&#22270;&#22312;&#20154;&#31867;&#20013;&#20960;&#20046;&#21487;&#20197;&#29702;&#35299;&#65292;&#20294;&#24182;&#38750;&#27809;&#26377;&#32570;&#38519;&#12290;&#20363;&#22914;&#65292;&#38750;&#19987;&#19994;&#29992;&#25143;&#21487;&#33021;&#19981;&#23436;&#20840;&#29702;&#35299;&#28909;&#22270;&#30340;&#36923;&#36753;&#65288;&#21363;&#20351;&#29992;&#19981;&#21516;&#24378;&#24230;&#25110;&#39068;&#33394;&#31361;&#20986;&#26174;&#31034;&#19982;&#27169;&#22411;&#39044;&#27979;&#30456;&#20851;&#30340;&#20687;&#32032;&#30340;&#36923;&#36753;&#65289;&#12290;&#27492;&#22806;&#65292;&#19982;&#27169;&#22411;&#39044;&#27979;&#30456;&#20851;&#30340;&#36755;&#20837;&#22270;&#20687;&#30340;&#23545;&#35937;&#21644;&#21306;&#22495;&#36890;&#24120;&#26080;&#27861;&#23436;&#20840;&#36890;&#36807;&#28909;&#22270;&#21306;&#20998;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;TbExplain&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;XAI&#25216;&#26415;&#21644;&#39044;&#35757;&#32451;&#30340;&#23545;&#35937;&#26816;&#27979;&#22120;&#65292;&#20197;&#25991;&#26412;&#24418;&#24335;&#35299;&#37322;&#22330;&#26223;&#20998;&#31867;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;TbExplain&#36824;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#32416;&#27491;&#39044;&#27979;&#21644;&#36827;&#34892;&#25991;&#26412;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of Explainable Artificial Intelligence (XAI) aims to improve the interpretability of black-box machine learning models. Building a heatmap based on the importance value of input features is a popular method for explaining the underlying functions of such models in producing their predictions. Heatmaps are almost understandable to humans, yet they are not without flaws. Non-expert users, for example, may not fully understand the logic of heatmaps (the logic in which relevant pixels to the model's prediction are highlighted with different intensities or colors). Additionally, objects and regions of the input image that are relevant to the model prediction are frequently not entirely differentiated by heatmaps. In this paper, we propose a framework called TbExplain that employs XAI techniques and a pre-trained object detector to present text-based explanations of scene classification models. Moreover, TbExplain incorporates a novel method to correct predictions and textually exp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#21551;&#21457;&#30340;&#22320;&#22270;&#26500;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#21106;&#21644;&#22238;&#28335;&#26469;&#35299;&#20915;&#22823;&#22411;&#29615;&#22659;&#19979;&#30340;&#25506;&#32034;&#38382;&#39064;&#65292;&#24182;&#22522;&#20110;&#24847;&#22806;&#24615;&#30340;&#31354;&#38388;&#32858;&#31867;&#35774;&#32622;&#25506;&#32034;&#23376;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2307.05793</link><description>&lt;p&gt;
&#31070;&#32463;&#21551;&#21457;&#30340;&#39640;&#25928;&#22320;&#22270;&#26500;&#24314;&#36890;&#36807;&#20998;&#21106;&#21644;&#22238;&#28335;
&lt;/p&gt;
&lt;p&gt;
Neuro-Inspired Efficient Map Building via Fragmentation and Recall. (arXiv:2307.05793v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#21551;&#21457;&#30340;&#22320;&#22270;&#26500;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#21106;&#21644;&#22238;&#28335;&#26469;&#35299;&#20915;&#22823;&#22411;&#29615;&#22659;&#19979;&#30340;&#25506;&#32034;&#38382;&#39064;&#65292;&#24182;&#22522;&#20110;&#24847;&#22806;&#24615;&#30340;&#31354;&#38388;&#32858;&#31867;&#35774;&#32622;&#25506;&#32034;&#23376;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#29289;&#21644;&#26426;&#22120;&#20154;&#36890;&#36807;&#26500;&#24314;&#21644;&#23436;&#21892;&#31354;&#38388;&#22320;&#22270;&#26469;&#23548;&#33322;&#29615;&#22659;&#12290;&#36825;&#20123;&#22320;&#22270;&#20351;&#24471;&#21253;&#25324;&#22238;&#23478;&#12289;&#35268;&#21010;&#12289;&#25628;&#32034;&#21644;&#35269;&#39135;&#22312;&#20869;&#30340;&#21151;&#33021;&#25104;&#20026;&#21487;&#33021;&#12290;&#22312;&#22823;&#22411;&#29615;&#22659;&#20013;&#65292;&#25506;&#32034;&#31354;&#38388;&#26159;&#19968;&#20010;&#38590;&#39064;&#65306;&#20195;&#29702;&#21487;&#33021;&#20250;&#38519;&#20837;&#23616;&#37096;&#21306;&#22495;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20174;&#31070;&#32463;&#31185;&#23398;&#20013;&#27762;&#21462;&#32463;&#39564;&#65292;&#25552;&#20986;&#24182;&#24212;&#29992;&#20102;&#20998;&#21106;&#21644;&#22238;&#28335;&#65288;FarMap&#65289;&#30340;&#27010;&#24565;&#12290;&#20195;&#29702;&#36890;&#36807;&#22522;&#20110;&#24847;&#22806;&#24615;&#30340;&#31354;&#38388;&#32858;&#31867;&#26469;&#35299;&#20915;&#22320;&#22270;&#26500;&#24314;&#38382;&#39064;&#65292;&#21516;&#26102;&#23558;&#20854;&#29992;&#20110;&#35774;&#32622;&#31354;&#38388;&#25506;&#32034;&#30340;&#23376;&#30446;&#26631;&#12290;&#20195;&#29702;&#26500;&#24314;&#21644;&#20351;&#29992;&#26412;&#22320;&#22320;&#22270;&#26469;&#39044;&#27979;&#20182;&#20204;&#30340;&#35266;&#27979;&#32467;&#26524;&#65307;&#39640;&#24847;&#22806;&#24615;&#20250;&#23548;&#33268;&#8220;&#20998;&#21106;&#20107;&#20214;&#8221;&#65292;&#20174;&#32780;&#25130;&#26029;&#26412;&#22320;&#22320;&#22270;&#12290;&#22312;&#36825;&#20123;&#20107;&#20214;&#20013;&#65292;&#26368;&#36817;&#30340;&#26412;&#22320;&#22320;&#22270;&#34987;&#25918;&#20837;&#38271;&#26399;&#35760;&#24518;&#65288;LTM&#65289;&#20013;&#65292;&#24182;&#21021;&#22987;&#21270;&#21478;&#19968;&#20010;&#26412;&#22320;&#22320;&#22270;&#12290;&#22914;&#26524;&#26029;&#35010;&#28857;&#30340;&#35266;&#23519;&#32467;&#26524;&#19982;&#23384;&#20648;&#30340;&#26576;&#20010;&#26412;&#22320;&#22320;&#22270;&#30340;&#35266;&#23519;&#32467;&#26524;&#30456;&#21305;&#37197;&#65292;&#37027;&#20040;&#35813;&#22320;&#22270;&#23601;&#20250;&#34987;&#22238;&#28335;&#65288;&#24182;&#37325;&#29992;&#65289;&#33258;LTM&#12290;&#20998;&#21106;&#28857;&#35825;&#23548;.
&lt;/p&gt;
&lt;p&gt;
Animals and robots navigate through environments by building and refining maps of the space. These maps enable functions including navigating back to home, planning, search, and foraging. In large environments, exploration of the space is a hard problem: agents can become stuck in local regions. Here, we use insights from neuroscience to propose and apply the concept of Fragmentation-and-Recall (FarMap), with agents solving the mapping problem by building local maps via a surprisal-based clustering of space, which they use to set subgoals for spatial exploration. Agents build and use a local map to predict their observations; high surprisal leads to a ``fragmentation event'' that truncates the local map. At these events, the recent local map is placed into long-term memory (LTM), and a different local map is initialized. If observations at a fracture point match observations in one of the stored local maps, that map is recalled (and thus reused) from LTM. The fragmentation points induc
&lt;/p&gt;</description></item><item><title>&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#20351;&#29992;&#37327;&#23376;&#35745;&#31639;&#26426;&#36827;&#34892;&#35780;&#20272;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#22312;&#35757;&#32451;&#23436;&#21518;&#65292;&#20351;&#29992;&#37327;&#23376;&#35745;&#31639;&#26426;&#29983;&#25104;&#19968;&#20010;&#32463;&#20856;&#38452;&#24433;&#27169;&#22411;&#26469;&#35745;&#31639;&#20989;&#25968;&#30340;&#32463;&#20856;&#35745;&#31639;&#36817;&#20284;&#65292;&#36991;&#20813;&#20102;&#23545;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2306.00061</link><description>&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#38452;&#24433;
&lt;/p&gt;
&lt;p&gt;
Shadows of quantum machine learning. (arXiv:2306.00061v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00061
&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#20351;&#29992;&#37327;&#23376;&#35745;&#31639;&#26426;&#36827;&#34892;&#35780;&#20272;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#22312;&#35757;&#32451;&#23436;&#21518;&#65292;&#20351;&#29992;&#37327;&#23376;&#35745;&#31639;&#26426;&#29983;&#25104;&#19968;&#20010;&#32463;&#20856;&#38452;&#24433;&#27169;&#22411;&#26469;&#35745;&#31639;&#20989;&#25968;&#30340;&#32463;&#20856;&#35745;&#31639;&#36817;&#20284;&#65292;&#36991;&#20813;&#20102;&#23545;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#32463;&#24120;&#34987;&#35748;&#20026;&#26159;&#21033;&#29992;&#37327;&#23376;&#35745;&#31639;&#26426;&#35299;&#20915;&#23454;&#38469;&#38382;&#39064;&#30340;&#26368;&#26377;&#21069;&#36884;&#30340;&#24212;&#29992;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#38459;&#30861;&#20854;&#22312;&#23454;&#36341;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#20027;&#35201;&#38556;&#30861;&#26159;&#36825;&#20123;&#27169;&#22411;&#21363;&#20351;&#22312;&#35757;&#32451;&#36807;&#31243;&#21518;&#65292;&#20173;&#38656;&#35201;&#35775;&#38382;&#37327;&#23376;&#35745;&#31639;&#26426;&#25165;&#33021;&#23545;&#26032;&#25968;&#25454;&#36827;&#34892;&#35780;&#20272;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#37327;&#23376;&#27169;&#22411;&#30340;&#35757;&#32451;&#38454;&#27573;&#20043;&#21518;&#65292;&#37327;&#23376;&#35745;&#31639;&#26426;&#21487;&#20197;&#29992;&#26469;&#29983;&#25104;&#25105;&#20204;&#25152;&#35859;&#30340;&#35813;&#27169;&#22411;&#30340;&#8220;&#32463;&#20856;&#38452;&#24433;&#8221;&#65292;&#21363;&#24050;&#23398;&#20064;&#20989;&#25968;&#30340;&#32463;&#20856;&#35745;&#31639;&#36817;&#20284;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#35752;&#20102;&#36825;&#20010;&#24819;&#27861;&#24182;&#25552;&#20986;&#20102;&#26500;&#24314;&#36825;&#31181;&#24433;&#23376;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#20063;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#32463;&#20856;&#27169;&#22411;&#21487;&#33021;&#20195;&#26367;&#30340;&#21487;&#33021;&#24615;&#65292;&#20174;&#32780;&#39318;&#20808;&#22238;&#36991;&#20102;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#38656;&#35201;&#12290;&#26412;&#25991;&#37319;&#29992;&#26032;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#37327;&#23376;&#32447;&#24615;&#27169;&#22411;&#21644;&#32463;&#20856;&#38452;&#24433;&#37325;&#26500;&#30340;&#26694;&#26550;&#26469;&#23450;&#20041;&#38452;&#24433;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum machine learning is often highlighted as one of the most promising uses for a quantum computer to solve practical problems. However, a major obstacle to the widespread use of quantum machine learning models in practice is that these models, even once trained, still require access to a quantum computer in order to be evaluated on new data. To solve this issue, we suggest that following the training phase of a quantum model, a quantum computer could be used to generate what we call a classical shadow of this model, i.e., a classically computable approximation of the learned function. While recent works already explore this idea and suggest approaches to construct such shadow models, they also raise the possibility that a completely classical model could be trained instead, thus circumventing the need for a quantum computer in the first place. In this work, we take a novel approach to define shadow models based on the frameworks of quantum linear models and classical shadow tomogr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Phy-DRL&#65292;&#36825;&#26159;&#19968;&#20010;&#29289;&#29702;&#27169;&#22411;&#35843;&#25972;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#26377;&#19977;&#20010;&#21019;&#26032;&#28857;&#65292;&#23427;&#20204;&#20998;&#21035;&#26159;: i)&#21069;&#30651;&#24615;&#30340;&#26410;&#30693;&#26410;&#30693;&#35757;&#32451;&#65292;ii)&#32467;&#21512;&#27531;&#24046;&#25511;&#21046;&#65292;&#20197;&#21450;iii)&#22522;&#20110;&#29289;&#29702;&#27169;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#32534;&#36753;&#12290;Phy-DRL&#33021;&#22815;&#23481;&#24525;&#26410;&#30693;&#24178;&#25200;&#65292;&#20445;&#35777;&#23433;&#20840;&#21644;&#31283;&#23450;&#65292;&#21516;&#26102;&#36981;&#23432;Bellman&#26041;&#31243;&#21644;&#22870;&#21169;&#30456;&#20851;&#30340;&#29289;&#29702;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2305.16614</link><description>&lt;p&gt;
&#29289;&#29702;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;: &#23433;&#20840;&#21644;&#26410;&#30693;&#26410;&#30693;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Physical Deep Reinforcement Learning: Safety and Unknown Unknowns. (arXiv:2305.16614v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Phy-DRL&#65292;&#36825;&#26159;&#19968;&#20010;&#29289;&#29702;&#27169;&#22411;&#35843;&#25972;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#26377;&#19977;&#20010;&#21019;&#26032;&#28857;&#65292;&#23427;&#20204;&#20998;&#21035;&#26159;: i)&#21069;&#30651;&#24615;&#30340;&#26410;&#30693;&#26410;&#30693;&#35757;&#32451;&#65292;ii)&#32467;&#21512;&#27531;&#24046;&#25511;&#21046;&#65292;&#20197;&#21450;iii)&#22522;&#20110;&#29289;&#29702;&#27169;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#32534;&#36753;&#12290;Phy-DRL&#33021;&#22815;&#23481;&#24525;&#26410;&#30693;&#24178;&#25200;&#65292;&#20445;&#35777;&#23433;&#20840;&#21644;&#31283;&#23450;&#65292;&#21516;&#26102;&#36981;&#23432;Bellman&#26041;&#31243;&#21644;&#22870;&#21169;&#30456;&#20851;&#30340;&#29289;&#29702;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Phy-DRL&#65292;&#19968;&#20010;&#29289;&#29702;&#27169;&#22411;&#35843;&#33410;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#30340;&#33258;&#20027;&#31995;&#32479;&#12290;Phy-DRL&#20855;&#26377;&#19977;&#31181;&#29420;&#29305;&#30340;&#21019;&#26032;&#65306;i&#65289;&#21069;&#30651;&#24615;&#30340;&#26410;&#30693;&#26410;&#30693;&#35757;&#32451;&#65292;ii&#65289;&#32467;&#21512;&#27531;&#24046;&#25511;&#21046;&#65288;&#21363;&#25968;&#25454;&#39537;&#21160;&#25511;&#21046;&#21644;&#22522;&#20110;&#29289;&#29702;&#27169;&#22411;&#30340;&#25511;&#21046;&#30340;&#38598;&#25104;&#65289;&#21644;&#23433;&#20840;&#21450;&#31283;&#23450;&#24615;&#25935;&#24863;&#30340;&#22870;&#21169;&#65292;&#20197;&#21450;iii&#65289;&#22522;&#20110;&#29289;&#29702;&#27169;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#32534;&#36753;&#65292;&#21253;&#25324;&#38142;&#25509;&#32534;&#36753;&#21644;&#28608;&#27963;&#32534;&#36753;&#12290;&#30001;&#20110;&#36825;&#20123;&#24182;&#21457;&#35774;&#35745;&#65292;Phy-DRL&#33021;&#22815;1&#65289;&#23481;&#24525;&#26410;&#30693;&#24178;&#25200;&#65292;2&#65289;&#20445;&#35777;&#21487;&#25968;&#23398;&#35777;&#26126;&#30340;&#23433;&#20840;&#19982;&#31283;&#23450;&#24615;&#65292;&#24182;3&#65289;&#20005;&#26684;&#36981;&#23432;Bellman&#26041;&#31243;&#21644;&#22870;&#21169;&#30456;&#20851;&#30340;&#29289;&#29702;&#30693;&#35782;&#12290;&#26368;&#32456;&#65292;&#36890;&#36807;&#20498;&#31435;&#25670;&#21644;&#22235;&#36275;&#26426;&#22120;&#20154;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;Phy-DRL&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;DRL&#30456;&#27604;&#65292;Phy-DRL&#20855;&#26377;&#26126;&#26174;&#26356;&#23569;&#30340;&#23398;&#20064;&#21442;&#25968;&#12289;&#21152;&#36895;&#30340;&#35757;&#32451;&#21644;&#25193;&#22823;&#30340;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose the Phy-DRL: a physics-model-regulated deep reinforcement learning framework for safety-critical autonomous systems. The Phy-DRL is unique in three innovations: i) proactive unknown-unknowns training, ii) conjunctive residual control (i.e., integration of data-driven control and physics-model-based control) and safety- \&amp; stability-sensitive reward, and iii) physics-model-based neural network editing, including link editing and activation editing. Thanks to the concurrent designs, the Phy-DRL is able to 1) tolerate unknown-unknowns disturbances, 2) guarantee mathematically provable safety and stability, and 3) strictly comply with physical knowledge pertaining to Bellman equation and reward. The effectiveness of the Phy-DRL is finally validated by an inverted pendulum and a quadruped robot. The experimental results demonstrate that compared with purely data-driven DRL, Phy-DRL features remarkably fewer learning parameters, accelerated training and enlarged rew
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#40065;&#26834;UNet&#21435;&#22122;&#22120;&#30340;&#35748;&#35777;&#38646;&#38454;&#40657;&#30418;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#40657;&#30418;&#27169;&#22411;&#20043;&#21069;&#39044;&#32622;RDUNet&#21644;DS&#25110;AE&#21644;RDUNet&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.06430</link><description>&lt;p&gt;
&#24102;&#26377;&#40065;&#26834;UNet&#21435;&#22122;&#22120;&#30340;&#35748;&#35777;&#38646;&#38454;&#40657;&#30418;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Certified Zeroth-order Black-Box Defense with Robust UNet Denoiser. (arXiv:2304.06430v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#40065;&#26834;UNet&#21435;&#22122;&#22120;&#30340;&#35748;&#35777;&#38646;&#38454;&#40657;&#30418;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#40657;&#30418;&#27169;&#22411;&#20043;&#21069;&#39044;&#32622;RDUNet&#21644;DS&#25110;AE&#21644;RDUNet&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#40657;&#30418;&#35774;&#32622;&#20013;&#23545;&#20110;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#35748;&#35777;&#38450;&#24481;&#26041;&#27861;&#24050;&#32463;&#20174;&#38646;&#38454;&#35282;&#24230;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#28982;&#32780;&#30001;&#20110;&#21435;&#22122;&#22120;&#30340;&#35774;&#35745;&#19981;&#22815;&#26377;&#25928;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#39640;&#32500;&#25968;&#25454;&#38598;&#19978;&#23384;&#22312;&#39640;&#27169;&#22411;&#26041;&#24046;&#21644;&#20302;&#24615;&#33021;&#65292;&#19988;&#22312;&#20351;&#29992;&#38646;&#38454;&#25216;&#26415;&#26102;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35748;&#35777;&#30340;&#38646;&#38454;&#39044;&#22788;&#29702;&#25216;&#26415;&#65292;&#20165;&#20351;&#29992;&#27169;&#22411;&#26597;&#35810;&#21363;&#21487;&#20174;&#21463;&#25915;&#20987;&#22270;&#20687;&#20013;&#21435;&#38500;&#23545;&#25239;&#24615;&#25200;&#21160;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;UNet&#21435;&#22122;&#22120;&#65288;RDUNet&#65289;&#65292;&#30830;&#20445;&#20102;&#23545;&#20110;&#39640;&#32500;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#40657;&#30418;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#40657;&#30418;&#21435;&#22122;&#24179;&#28369;&#65288;DS&#65289;&#38450;&#24481;&#26426;&#21046;ZO-RUDS&#65292;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;RDUNet&#39044;&#32622;&#20110;&#40657;&#30418;&#27169;&#22411;&#20043;&#21069;&#65292;&#30830;&#20445;&#40657;&#30418;&#38450;&#24481;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;ZO-AE-RUDS&#65292;&#22312;&#40657;&#30418;&#27169;&#22411;&#20043;&#21069;&#20351;&#29992;RDUNet&#21644;&#33258;&#32534;&#30721;&#22120;(AE)&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Certified defense methods against adversarial perturbations have been recently investigated in the black-box setting with a zeroth-order (ZO) perspective. However, these methods suffer from high model variance with low performance on high-dimensional datasets due to the ineffective design of the denoiser and are limited in their utilization of ZO techniques. To this end, we propose a certified ZO preprocessing technique for removing adversarial perturbations from the attacked image in the black-box setting using only model queries. We propose a robust UNet denoiser (RDUNet) that ensures the robustness of black-box models trained on high-dimensional datasets. We propose a novel black-box denoised smoothing (DS) defense mechanism, ZO-RUDS, by prepending our RDUNet to the black-box model, ensuring black-box defense. We further propose ZO-AE-RUDS in which RDUNet followed by autoencoder (AE) is prepended to the black-box model. We perform extensive experiments on four classification dataset
&lt;/p&gt;</description></item><item><title>KGS&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20808;&#21069;&#30340;&#22240;&#26524;&#36793;&#32536;&#20449;&#24687;&#20316;&#20026;&#32422;&#26463;&#26465;&#20214;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#23398;&#20064;&#22240;&#26524;&#22270;&#12290;</title><link>http://arxiv.org/abs/2304.05493</link><description>&lt;p&gt;
KGS&#65306;&#21033;&#29992;&#30693;&#35782;&#24341;&#23548;&#30340;&#36138;&#23146;&#31561;&#20215;&#25628;&#32034;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
KGS: Causal Discovery Using Knowledge-guided Greedy Equivalence Search. (arXiv:2304.05493v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05493
&lt;/p&gt;
&lt;p&gt;
KGS&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20808;&#21069;&#30340;&#22240;&#26524;&#36793;&#32536;&#20449;&#24687;&#20316;&#20026;&#32422;&#26463;&#26465;&#20214;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#23398;&#20064;&#22240;&#26524;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#20851;&#31995;&#65292;&#19981;&#33021;&#25552;&#20379;&#20851;&#20110;&#28508;&#22312;&#30340;&#22240;&#26524;&#26426;&#21046;&#21644;&#21487;&#33021;&#30340;&#22240;&#26524;&#22270;&#31354;&#38388;&#30340;&#36275;&#22815;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#22522;&#20110;&#20998;&#25968;&#30340;&#26041;&#27861;&#25628;&#32034;&#22270;&#31561;&#20215;&#31867;&#30340;&#31354;&#38388;&#65292;&#22914;&#36138;&#23146;&#31561;&#20215;&#25628;&#32034;&#65288;GES&#65289;&#65292;&#25628;&#32034;&#31354;&#38388;&#36890;&#24120;&#20250;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#20808;&#21069;&#30340;&#22240;&#26524;&#20449;&#24687;&#65292;&#20363;&#22914;&#26377;&#26080;&#22240;&#26524;&#36793;&#65292;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#25351;&#23548;&#21457;&#29616;&#36807;&#31243;&#65292;&#20351;&#20854;&#36208;&#21521;&#26356;&#20026;&#21463;&#38480;&#19988;&#20934;&#30830;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;KGS&#65292;&#19968;&#31181;&#30693;&#35782;&#24341;&#23548;&#30340;&#36138;&#23146;&#20998;&#25968;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#35266;&#27979;&#25968;&#25454;&#21644;&#32467;&#26500;&#20808;&#39564;&#65288;&#22240;&#26524;&#36793;&#65289;&#20316;&#20026;&#32422;&#26463;&#26465;&#20214;&#23398;&#20064;&#22240;&#26524;&#22270;&#12290;KGS&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#24212;&#29992;&#30693;&#35782;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#20219;&#20309;&#20004;&#20010;&#21464;&#37327;&#20043;&#38388;&#30340;&#20808;&#21069;&#36793;&#32536;&#20449;&#24687;&#65292;&#21253;&#25324;&#26377;&#21521;&#36793;&#65292;&#26080;&#36793;&#21644;&#26080;&#21521;&#36793;&#30340;&#23384;&#22312;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning causal relationships solely from observational data provides insufficient information about the underlying causal mechanism and the search space of possible causal graphs. As a result, often the search space can grow exponentially for approaches such as Greedy Equivalence Search (GES) that uses a score-based approach to search the space of equivalence classes of graphs. Prior causal information such as the presence or absence of a causal edge can be leveraged to guide the discovery process towards a more restricted and accurate search space. In this study, we present KGS, a knowledge-guided greedy score-based causal discovery approach that uses observational data and structural priors (causal edges) as constraints to learn the causal graph. KGS is a novel application of knowledge constraints that can leverage any of the following prior edge information between any two variables: the presence of a directed edge, the absence of an edge, and the presence of an undirected edge. We
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;BugNIST&#30340;&#24191;&#27867;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;12&#31181;&#26118;&#34411;&#21644;&#24188;&#34411;&#30340;&#24494;-CT&#25195;&#25551;&#32452;&#25104;&#12290;&#36890;&#36807;&#35757;&#32451;&#21644;&#27979;&#35797;&#26816;&#27979;&#27169;&#22411;&#65292;BugNIST&#26088;&#22312;&#35780;&#20272;&#19977;&#32500;&#20307;&#31215;&#22270;&#20687;&#20998;&#31867;&#21644;&#26816;&#27979;&#26041;&#27861;&#65292;&#35299;&#20915;&#19978;&#19979;&#25991;&#26080;&#20851;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.01838</link><description>&lt;p&gt;
BugNIST -- &#19968;&#31181;&#26032;&#30340;&#22823;&#35268;&#27169;&#20307;&#31215;&#19977;&#32500;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20998;&#31867;&#21644;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
BugNIST -- A New Large Scale Volumetric 3D Image Dataset for Classification and Detection. (arXiv:2304.01838v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;BugNIST&#30340;&#24191;&#27867;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;12&#31181;&#26118;&#34411;&#21644;&#24188;&#34411;&#30340;&#24494;-CT&#25195;&#25551;&#32452;&#25104;&#12290;&#36890;&#36807;&#35757;&#32451;&#21644;&#27979;&#35797;&#26816;&#27979;&#27169;&#22411;&#65292;BugNIST&#26088;&#22312;&#35780;&#20272;&#19977;&#32500;&#20307;&#31215;&#22270;&#20687;&#20998;&#31867;&#21644;&#26816;&#27979;&#26041;&#27861;&#65292;&#35299;&#20915;&#19978;&#19979;&#25991;&#26080;&#20851;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19977;&#32500;&#20307;&#31215;&#22270;&#20687;&#20998;&#26512;&#30740;&#31350;&#30340;&#36827;&#23637;&#21463;&#21040;&#25968;&#25454;&#38598;&#32570;&#20047;&#30340;&#38480;&#21046;&#65292;&#22823;&#22810;&#25968;&#38024;&#23545;&#20307;&#31215;&#22270;&#20687;&#30340;&#20998;&#26512;&#26041;&#27861;&#37117;&#22522;&#20110;&#21307;&#23398;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#21307;&#23398;&#25968;&#25454;&#24182;&#19981;&#19968;&#23450;&#20855;&#26377;&#20854;&#20182;&#20307;&#31215;&#22270;&#20687;&#65288;&#20363;&#22914;&#24494;-CT&#65289;&#30340;&#29305;&#24449;&#12290;&#20026;&#20102;&#20419;&#36827;&#19977;&#32500;&#20307;&#31215;&#22270;&#20687;&#20998;&#26512;&#30340;&#30740;&#31350;&#36229;&#36234;&#21307;&#23398;&#25968;&#25454;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;BugNIST&#25968;&#25454;&#38598;&#24182;&#20813;&#36153;&#25552;&#20379;&#12290;BugNIST&#26159;&#19968;&#32452;&#30001;12&#31181;&#26118;&#34411;&#21644;&#24188;&#34411;&#30340;&#24494;-CT&#25195;&#25551;&#32452;&#25104;&#30340;&#24191;&#27867;&#25968;&#25454;&#38598;&#12290;BugNIST&#21253;&#21547;9437&#20010;&#20307;&#31215;&#65292;&#20854;&#20013;9087&#20010;&#26159;&#21333;&#20010;&#26118;&#34411;&#30340;&#25195;&#25551;&#65292;350&#20010;&#26159;&#26118;&#34411;&#21644;&#20854;&#20182;&#26448;&#26009;&#30340;&#28151;&#21512;&#29289;&#12290;BugNIST&#30340;&#30446;&#26631;&#26159;&#35780;&#20272;&#20998;&#31867;&#21644;&#26816;&#27979;&#26041;&#27861;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26816;&#27979;&#25361;&#25112;&#65292;&#20351;&#24471;&#26816;&#27979;&#27169;&#22411;&#22312;&#21333;&#20010;&#26118;&#34411;&#30340;&#25195;&#25551;&#19978;&#35757;&#32451;&#24182;&#22312;&#26118;&#34411;&#28151;&#21512;&#29289;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#33021;&#22815;&#35299;&#20915;&#27492;&#20219;&#21153;&#30340;&#27169;&#22411;&#23558;&#29420;&#31435;&#20110;&#19978;&#19979;&#25991;&#65288;&#21363;&#21608;&#22260;&#26448;&#26009;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#24456;&#22823;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Progress in 3D volumetric image analysis research is limited by the lack of datasets and most advances in analysis methods for volumetric images are based on medical data. However, medical data do not necessarily resemble the characteristics of other volumetric images such as micro-CT. To promote research in 3D volumetric image analysis beyond medical data, we have created the BugNIST dataset and made it freely available. BugNIST is an extensive dataset of micro-CT scans of 12 types of bugs, such as insects and larvae. BugNIST contains 9437 volumes where 9087 are of individual bugs and 350 are mixtures of bugs and other material. The goal of BugNIST is to benchmark classification and detection methods, and we have designed the detection challenge such that detection models are trained on scans of individual bugs and tested on bug mixtures. Models capable of solving this task will be independent of the context, i.e., the surrounding material. This is a great advantage if the context is 
&lt;/p&gt;</description></item><item><title>MenuCraft&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;AI&#36741;&#21161;&#35774;&#35745;&#24072;&#65292;&#36890;&#36807;&#23545;&#35805;&#31995;&#32479;&#19982;&#35774;&#35745;&#24072;&#21327;&#20316;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#33756;&#21333;&#35774;&#35745;&#24037;&#20855;&#65292;&#21487;&#20197;&#31616;&#21270;&#33756;&#21333;&#35774;&#35745;&#36807;&#31243;&#65292;&#24182;&#25903;&#25345;&#38646;/&#23569;&#27425;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2303.04496</link><description>&lt;p&gt;
MenuCraft: &#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#20114;&#24335;&#33756;&#21333;&#31995;&#32479;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
MenuCraft: Interactive Menu System Design with Large Language Models. (arXiv:2303.04496v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04496
&lt;/p&gt;
&lt;p&gt;
MenuCraft&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;AI&#36741;&#21161;&#35774;&#35745;&#24072;&#65292;&#36890;&#36807;&#23545;&#35805;&#31995;&#32479;&#19982;&#35774;&#35745;&#24072;&#21327;&#20316;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#33756;&#21333;&#35774;&#35745;&#24037;&#20855;&#65292;&#21487;&#20197;&#31616;&#21270;&#33756;&#21333;&#35774;&#35745;&#36807;&#31243;&#65292;&#24182;&#25903;&#25345;&#38646;/&#23569;&#27425;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33756;&#21333;&#31995;&#32479;&#35774;&#35745;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#35768;&#22810;&#35774;&#35745;&#36873;&#39033;&#21644;&#21508;&#31181;&#20154;&#22240;&#32032;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MenuCraft&#30340;AI&#36741;&#21161;&#35774;&#35745;&#24072;&#65292;&#36890;&#36807;&#35774;&#35745;&#21644;&#32454;&#21270;&#33756;&#21333;&#31995;&#32479;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#23454;&#29616;&#35774;&#35745;&#24072;&#19982;&#23545;&#35805;&#31995;&#32479;&#20043;&#38388;&#30340;&#21327;&#20316;&#12290;MenuCraft&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#35821;&#35328;&#30340;&#20132;&#20114;&#24335;&#33756;&#21333;&#35774;&#35745;&#24037;&#20855;&#65292;&#31616;&#21270;&#20102;&#33756;&#21333;&#35774;&#35745;&#36807;&#31243;&#65292;&#24182;&#23454;&#29616;&#20102;&#35774;&#35745;&#36873;&#39033;&#30340;&#36731;&#26494;&#23450;&#21046;&#12290;MenuCraft&#36890;&#36807;&#23545;&#35805;&#25903;&#25345;&#21508;&#31181;&#20132;&#20114;&#26041;&#24335;&#65292;&#21487;&#20197;&#36827;&#34892;&#38646;/&#23569;&#27425;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Menu system design is a challenging task involving many design options and various human factors. For example, one crucial factor that designers need to consider is the semantic and systematic relation of menu commands. However, capturing these relations can be challenging due to limited available resources. With the advancement of neural language models, large language models can utilize their vast pre-existing knowledge in designing and refining menu systems. In this paper, we propose MenuCraft, an AI-assisted designer for menu design that enables collaboration between the designer and a dialogue system to design menus. MenuCraft offers an interactive language-based menu design tool that simplifies the menu design process and enables easy customization of design options. MenuCraft supports a variety of interactions through dialog that allows performing zero/few-shot learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#20013;&#22810;&#20219;&#21153;&#20855;&#36523;&#26234;&#33021;&#20307;&#30340;&#20219;&#21153;&#35268;&#21010;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#20114;&#24335;&#35268;&#21010;&#26041;&#27861;(DEPS)&#26469;&#35299;&#20915;&#35745;&#21010;&#25191;&#34892;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.01560</link><description>&lt;p&gt;
&#25551;&#36848;&#12289;&#35299;&#37322;&#12289;&#35268;&#21010;&#21644;&#36873;&#25321;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21551;&#29992;&#24320;&#25918;&#19990;&#30028;&#22810;&#20219;&#21153;&#26234;&#33021;&#20307;&#30340;&#20132;&#20114;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents. (arXiv:2302.01560v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01560
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#20013;&#22810;&#20219;&#21153;&#20855;&#36523;&#26234;&#33021;&#20307;&#30340;&#20219;&#21153;&#35268;&#21010;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#20114;&#24335;&#35268;&#21010;&#26041;&#27861;(DEPS)&#26469;&#35299;&#20915;&#35745;&#21010;&#25191;&#34892;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#20013;&#22810;&#20219;&#21153;&#20855;&#36523;&#26234;&#33021;&#20307;&#30340;&#20219;&#21153;&#35268;&#21010;&#25361;&#25112;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#20004;&#20010;&#20027;&#35201;&#22256;&#38590;&#65306;1&#65289;&#22312;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#65288;&#22914;Minecraft&#65289;&#20013;&#25191;&#34892;&#35745;&#21010;&#38656;&#35201;&#20934;&#30830;&#30340;&#22810;&#27493;&#25512;&#29702;&#65292;&#22240;&#20026;&#20219;&#21153;&#26159;&#38271;&#26399;&#24615;&#30340;&#65307;2&#65289;&#30001;&#20110;&#20256;&#32479;&#35268;&#21010;&#22120;&#19981;&#32771;&#34385;&#24403;&#21069;&#26234;&#33021;&#20307;&#23436;&#25104;&#32473;&#23450;&#23376;&#20219;&#21153;&#30340;&#38590;&#24230;&#65292;&#22312;&#22797;&#26434;&#35745;&#21010;&#20013;&#23545;&#24182;&#34892;&#23376;&#30446;&#26631;&#36827;&#34892;&#25490;&#24207;&#21487;&#33021;&#23548;&#33268;&#35745;&#21010;&#20302;&#25928;&#29978;&#33267;&#19981;&#21487;&#34892;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#8220;&#25551;&#36848;&#12289;&#35299;&#37322;&#12289;&#35268;&#21010;&#21644;&#36873;&#25321;&#8221;&#65288;DEPS&#65289;&#20132;&#20114;&#24335;&#35268;&#21010;&#26041;&#27861;&#12290;DEPS&#36890;&#36807;&#25972;&#21512;&#35745;&#21010;&#25191;&#34892;&#36807;&#31243;&#30340;&#25551;&#36848;&#21644;&#22312;&#35268;&#21010;&#38454;&#27573;&#36935;&#21040;&#22833;&#36133;&#26102;&#25552;&#20379;&#33258;&#25105;&#35299;&#37322;&#30340;&#21453;&#39304;&#65292;&#23454;&#29616;&#20102;&#23545;&#21021;&#22987;LLM&#29983;&#25104;&#30340;&#35745;&#21010;&#30340;&#26356;&#22909;&#30340;&#38169;&#35823;&#20462;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the challenge of task planning for multi-task embodied agents in open-world environments. Two main difficulties are identified: 1) executing plans in an open-world environment (e.g., Minecraft) necessitates accurate and multi-step reasoning due to the long-term nature of tasks, and 2) as vanilla planners do not consider how easy the current agent can achieve a given sub-task when ordering parallel sub-goals within a complicated plan, the resulting plan could be inefficient or even infeasible. To this end, we propose "$\underline{D}$escribe, $\underline{E}$xplain, $\underline{P}$lan and $\underline{S}$elect" ($\textbf{DEPS}$), an interactive planning approach based on Large Language Models (LLMs). DEPS facilitates better error correction on initial LLM-generated $\textit{plan}$ by integrating $\textit{description}$ of the plan execution process and providing self-$\textit{explanation}$ of feedback when encountering failures during the extended planning phases. Furthermore
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Res2Net&#30340;&#32418;&#22806;&#21644;&#21487;&#35265;&#20809;&#22270;&#20687;&#34701;&#21512;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#21644;&#34701;&#21512;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#34701;&#21512;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2112.14540</link><description>&lt;p&gt;
Res2NetFuse&#65306;&#19968;&#31181;&#36866;&#29992;&#20110;&#32418;&#22806;&#21644;&#21487;&#35265;&#20809;&#22270;&#20687;&#30340;&#34701;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Res2NetFuse: A Fusion Method for Infrared and Visible Images. (arXiv:2112.14540v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.14540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Res2Net&#30340;&#32418;&#22806;&#21644;&#21487;&#35265;&#20809;&#22270;&#20687;&#34701;&#21512;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#21644;&#34701;&#21512;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#34701;&#21512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Res2Net&#30340;&#32418;&#22806;&#21644;&#21487;&#35265;&#20809;&#22270;&#20687;&#34701;&#21512;&#26694;&#26550;&#12290;&#25552;&#20986;&#30340;&#34701;&#21512;&#27169;&#22411;&#21253;&#25324;&#32534;&#30721;&#22120;&#12289;&#34701;&#21512;&#23618;&#21644;&#35299;&#30721;&#22120;&#19977;&#20010;&#37096;&#20998;&#12290;&#21033;&#29992;&#22522;&#20110;Res2Net&#30340;&#32534;&#30721;&#22120;&#25552;&#21462;&#28304;&#22270;&#20687;&#30340;&#22810;&#23610;&#24230;&#29305;&#24449;&#65292;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#20165;&#20351;&#29992;&#21333;&#20010;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#27880;&#24847;&#21147;&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#34701;&#21512;&#31574;&#30053;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#35299;&#30721;&#22120;&#37325;&#26500;&#34701;&#21512;&#22270;&#20687;&#12290;&#26412;&#25991;&#36824;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#20013;&#37117;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#34701;&#21512;&#24615;&#33021;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel Res2Net-based fusion framework for infrared and visible images. The proposed fusion model has three parts: an encoder, a fusion layer and a decoder, respectively. The Res2Net-based encoder is used to extract multi-scale features of source images, the paper introducing a new training strategy for training a Res2Net-based encoder that uses only a single image. Then, a new fusion strategy is developed based on the attention model. Finally, the fused image is reconstructed by the decoder. The proposed approach is also analyzed in detail. Experiments show that our method achieves state-of-the-art fusion performance in objective and subjective assessment by comparing with the existing methods.
&lt;/p&gt;</description></item></channel></rss>