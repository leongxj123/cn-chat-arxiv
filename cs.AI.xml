<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#20803;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33719;&#21462;&#21487;&#36716;&#31227;&#30693;&#35782;&#23454;&#29616;&#22312;&#21508;&#31181;&#20219;&#21153;&#20043;&#38388;&#24555;&#36895;&#36866;&#24212;&#65292;&#20026;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#38754;&#23545;&#20998;&#24067;&#21464;&#21270;&#21644;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#26102;&#27867;&#21270;&#33021;&#21147;&#19981;&#20339;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2404.02785</link><description>&lt;p&gt;
&#36890;&#36807;&#20803;&#23398;&#20064;&#23454;&#29616;&#39046;&#22495;&#27867;&#21270;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Domain Generalization through Meta-Learning: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02785
&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33719;&#21462;&#21487;&#36716;&#31227;&#30693;&#35782;&#23454;&#29616;&#22312;&#21508;&#31181;&#20219;&#21153;&#20043;&#38388;&#24555;&#36895;&#36866;&#24212;&#65292;&#20026;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#38754;&#23545;&#20998;&#24067;&#21464;&#21270;&#21644;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#26102;&#27867;&#21270;&#33021;&#21147;&#19981;&#20339;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#65292;&#20294;&#26159;&#24403;&#38754;&#23545;&#20998;&#24067;&#20043;&#22806;(out-of-distribution, OOD)&#25968;&#25454;&#26102;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#65292;&#36825;&#26159;&#22240;&#20026;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30001;&#20110;&#39046;&#22495;&#36716;&#31227;&#19981;&#21487;&#36991;&#20813;&#65292;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#34987;&#20551;&#23450;&#20026;&#20849;&#20139;&#30456;&#21516;&#20998;&#24067;&#30340;&#24120;&#35265;&#24773;&#20917;&#12290;&#23613;&#31649;DNNs&#22312;&#22823;&#37327;&#25968;&#25454;&#21644;&#35745;&#31639;&#33021;&#21147;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#24456;&#38590;&#24212;&#23545;&#20998;&#24067;&#21464;&#21270;&#21644;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#65292;&#23548;&#33268;&#36807;&#25311;&#21512;&#21644;&#36328;&#19981;&#21516;&#20219;&#21153;&#21644;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#19981;&#20339;&#12290;&#20803;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#33021;&#22815;&#22312;&#21508;&#31181;&#20219;&#21153;&#20043;&#38388;&#33719;&#21462;&#21487;&#36716;&#31227;&#30693;&#35782;&#30340;&#31639;&#27861;&#36827;&#34892;&#24555;&#36895;&#36866;&#24212;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#38656;&#35201;&#20174;&#22836;&#23398;&#20064;&#27599;&#20010;&#20219;&#21153;&#30340;&#24517;&#35201;&#24615;&#12290;&#26412;&#35843;&#26597;&#35770;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#20803;&#23398;&#20064;&#39046;&#22495;&#65292;&#37325;&#28857;&#20851;&#27880;&#20854;&#23545;&#39046;&#22495;&#27867;&#21270;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02785v1 Announce Type: cross  Abstract: Deep neural networks (DNNs) have revolutionized artificial intelligence but often lack performance when faced with out-of-distribution (OOD) data, a common scenario due to the inevitable domain shifts in real-world applications. This limitation stems from the common assumption that training and testing data share the same distribution-an assumption frequently violated in practice. Despite their effectiveness with large amounts of data and computational power, DNNs struggle with distributional shifts and limited labeled data, leading to overfitting and poor generalization across various tasks and domains. Meta-learning presents a promising approach by employing algorithms that acquire transferable knowledge across various tasks for fast adaptation, eliminating the need to learn each task from scratch. This survey paper delves into the realm of meta-learning with a focus on its contribution to domain generalization. We first clarify the 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#34920;&#26126;&#65292;LLM&#23884;&#20837;&#33021;&#22815;&#25429;&#25417;&#32467;&#26500;&#21270;&#35821;&#35328;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;BERT&#22312;&#24615;&#33021;&#19978;&#39046;&#20808;&#20110;&#36731;&#37327;&#32423;&#36873;&#39033;&#65292;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21644;&#25688;&#35201;&#25216;&#26415;&#24182;&#19981;&#19968;&#33268;&#22320;&#25552;&#39640;&#32858;&#31867;&#25928;&#29575;</title><link>https://arxiv.org/abs/2403.15112</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#23884;&#20837;&#36827;&#34892;&#25991;&#26412;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Text clustering with LLM embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15112
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;LLM&#23884;&#20837;&#33021;&#22815;&#25429;&#25417;&#32467;&#26500;&#21270;&#35821;&#35328;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;BERT&#22312;&#24615;&#33021;&#19978;&#39046;&#20808;&#20110;&#36731;&#37327;&#32423;&#36873;&#39033;&#65292;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21644;&#25688;&#35201;&#25216;&#26415;&#24182;&#19981;&#19968;&#33268;&#22320;&#25552;&#39640;&#32858;&#31867;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#32858;&#31867;&#26159;&#32452;&#32455;&#19981;&#26029;&#22686;&#38271;&#30340;&#25968;&#23383;&#20869;&#23481;&#30340;&#37325;&#35201;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#32467;&#26500;&#21270;&#21644;&#21457;&#29616;&#26410;&#20998;&#31867;&#25968;&#25454;&#20013;&#30340;&#38544;&#34255;&#27169;&#24335;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19981;&#21516;&#25991;&#26412;&#23884;&#20837;&#65288;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#20013;&#20351;&#29992;&#30340;&#65289;&#21644;&#32858;&#31867;&#31639;&#27861;&#22914;&#20309;&#24433;&#21709;&#25991;&#26412;&#25968;&#25454;&#38598;&#30340;&#32858;&#31867;&#26041;&#24335;&#12290;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#20197;&#35780;&#20272;&#23884;&#20837;&#26159;&#22914;&#20309;&#24433;&#21709;&#32858;&#31867;&#32467;&#26524;&#30340;&#65292;&#20197;&#21450;&#36890;&#36807;&#25688;&#35201;&#36827;&#34892;&#38477;&#32500;&#21644;&#23884;&#20837;&#22823;&#23567;&#35843;&#25972;&#30340;&#20316;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;LLM&#23884;&#20837;&#22312;&#25429;&#33719;&#32467;&#26500;&#21270;&#35821;&#35328;&#30340;&#32454;&#24494;&#24046;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;BERT&#22312;&#24615;&#33021;&#19978;&#39046;&#20808;&#20110;&#36731;&#37327;&#32423;&#36873;&#39033;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21644;&#25688;&#35201;&#25216;&#26415;&#24182;&#19981;&#19968;&#33268;&#22320;&#25552;&#39640;&#32858;&#31867;&#25928;&#29575;&#65292;&#36825;&#34920;&#26126;&#36825;&#20123;&#31574;&#30053;&#38656;&#35201;&#20180;&#32454;&#20998;&#26512;&#25165;&#33021;&#22312;&#23454;&#38469;&#27169;&#22411;&#20013;&#20351;&#29992;&#12290;&#36825;&#20123;&#32467;&#26524;&#31361;&#20986;&#20102;&#19968;&#31181;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15112v1 Announce Type: cross  Abstract: Text clustering is an important approach for organising the growing amount of digital content, helping to structure and find hidden patterns in uncategorised data. In this research, we investigated how different textual embeddings - particularly those used in large language models (LLMs) - and clustering algorithms affect how text datasets are clustered. A series of experiments were conducted to assess how embeddings influence clustering results, the role played by dimensionality reduction through summarisation, and embedding size adjustment. Results reveal that LLM embeddings excel at capturing the nuances of structured language, while BERT leads the lightweight options in performance. In addition, we find that increasing embedding dimensionality and summarisation techniques do not uniformly improve clustering efficiency, suggesting that these strategies require careful analysis to use in real-life models. These results highlight a co
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;npm&#29983;&#24577;&#31995;&#32479;&#20013;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#20197;&#21327;&#21161;&#23433;&#20840;&#20998;&#26512;&#24072;&#35782;&#21035;&#24694;&#24847;&#36719;&#20214;&#21253;</title><link>https://arxiv.org/abs/2403.12196</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;npm&#29983;&#24577;&#31995;&#32479;&#20013;&#26816;&#27979;&#24694;&#24847;&#36719;&#20214;
&lt;/p&gt;
&lt;p&gt;
Shifting the Lens: Detecting Malware in npm Ecosystem with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12196
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;npm&#29983;&#24577;&#31995;&#32479;&#20013;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#20197;&#21327;&#21161;&#23433;&#20840;&#20998;&#26512;&#24072;&#35782;&#21035;&#24694;&#24847;&#36719;&#20214;&#21253;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Gartner 2022&#24180;&#30340;&#25253;&#21578;&#39044;&#27979;&#65292;&#21040;2025&#24180;&#65292;&#20840;&#29699;45%&#30340;&#32452;&#32455;&#23558;&#36973;&#36935;&#36719;&#20214;&#20379;&#24212;&#38142;&#25915;&#20987;&#65292;&#20984;&#26174;&#20102;&#25913;&#21892;&#36719;&#20214;&#20379;&#24212;&#38142;&#23433;&#20840;&#23545;&#31038;&#21306;&#21644;&#22269;&#23478;&#21033;&#30410;&#30340;&#36843;&#20999;&#24615;&#12290;&#24403;&#21069;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#25216;&#26415;&#36890;&#36807;&#36807;&#28388;&#33391;&#24615;&#21644;&#24694;&#24847;&#36719;&#20214;&#21253;&#26469;&#36741;&#21161;&#25163;&#21160;&#23457;&#26680;&#36807;&#31243;&#65292;&#28982;&#32780;&#36825;&#31181;&#25216;&#26415;&#23384;&#22312;&#36739;&#39640;&#30340;&#35823;&#25253;&#29575;&#21644;&#26377;&#38480;&#30340;&#33258;&#21160;&#21270;&#25903;&#25345;&#12290;&#22240;&#27492;&#65292;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#25216;&#26415;&#21487;&#20197;&#21463;&#30410;&#20110;&#20808;&#36827;&#12289;&#26356;&#33258;&#21160;&#21270;&#30340;&#26041;&#27861;&#65292;&#24471;&#21040;&#20934;&#30830;&#19988;&#35823;&#25253;&#36739;&#23569;&#30340;&#32467;&#26524;&#12290;&#35813;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#24110;&#21161;&#23433;&#20840;&#20998;&#26512;&#24072;&#35782;&#21035;npm&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#24694;&#24847;&#36719;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12196v1 Announce Type: cross  Abstract: The Gartner 2022 report predicts that 45% of organizations worldwide will encounter software supply chain attacks by 2025, highlighting the urgency to improve software supply chain security for community and national interests. Current malware detection techniques aid in the manual review process by filtering benign and malware packages, yet such techniques have high false-positive rates and limited automation support. Therefore, malware detection techniques could benefit from advanced, more automated approaches for accurate and minimally false-positive results. The goal of this study is to assist security analysts in identifying malicious packages through the empirical study of large language models (LLMs) to detect potential malware in the npm ecosystem.   We present SocketAI Scanner, a multi-stage decision-maker malware detection workflow using iterative self-refinement and zero-shot-role-play-Chain of Thought (CoT) prompting techni
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#30828;&#21644;&#36719;&#36127;&#26679;&#26412;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#24247;&#22797;&#38203;&#28860;&#35780;&#20272;&#20013;&#26679;&#26412;&#31232;&#32570;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.02772</link><description>&lt;p&gt;
&#36890;&#36807;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#24247;&#22797;&#38203;&#28860;&#36136;&#37327;&#35780;&#20272;&#65292;&#32467;&#21512;&#30828;&#36127;&#26679;&#26412;&#21644;&#36719;&#36127;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Rehabilitation Exercise Quality Assessment through Supervised Contrastive Learning with Hard and Soft Negatives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02772
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#30828;&#21644;&#36719;&#36127;&#26679;&#26412;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#24247;&#22797;&#38203;&#28860;&#35780;&#20272;&#20013;&#26679;&#26412;&#31232;&#32570;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#38203;&#28860;&#30340;&#24247;&#22797;&#35745;&#21010;&#24050;&#34987;&#35777;&#26126;&#22312;&#25552;&#39640;&#29983;&#27963;&#36136;&#37327;&#12289;&#38477;&#20302;&#27515;&#20129;&#29575;&#21644;&#20877;&#20303;&#38498;&#29575;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#34394;&#25311;&#24247;&#22797;&#65292;&#24739;&#32773;&#21487;&#20197;&#22312;&#23478;&#29420;&#31435;&#23436;&#25104;&#38203;&#28860;&#65292;&#21033;&#29992;AI&#31639;&#27861;&#20998;&#26512;&#38203;&#28860;&#25968;&#25454;&#65292;&#20026;&#24739;&#32773;&#25552;&#20379;&#21453;&#39304;&#65292;&#24182;&#21521;&#20020;&#24202;&#21307;&#29983;&#26356;&#26032;&#20182;&#20204;&#30340;&#36827;&#23637;&#24773;&#20917;&#12290;&#36825;&#20123;&#35745;&#21010;&#36890;&#24120;&#20250;&#25351;&#23450;&#21508;&#31181;&#38203;&#28860;&#31867;&#22411;&#65292;&#36825;&#23548;&#33268;&#24247;&#22797;&#38203;&#28860;&#35780;&#20272;&#25968;&#25454;&#38598;&#38754;&#20020;&#29420;&#29305;&#25361;&#25112;&#65306;&#34429;&#28982;&#22312;&#25972;&#20307;&#35757;&#32451;&#26679;&#26412;&#20013;&#20016;&#23500;&#65292;&#20294;&#36825;&#20123;&#25968;&#25454;&#38598;&#36890;&#24120;&#23545;&#27599;&#31181;&#20855;&#20307;&#38203;&#32451;&#31867;&#22411;&#30340;&#26679;&#26412;&#25968;&#37327;&#26377;&#38480;&#12290;&#36825;&#31181;&#24046;&#24322;&#24433;&#21709;&#20102;&#29616;&#26377;&#26041;&#27861;&#35757;&#32451;&#20855;&#26377;&#23567;&#26679;&#26412;&#37327;&#30340;&#27599;&#31181;&#38203;&#32451;&#30340;&#21487;&#27867;&#21270;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24102;&#26377;&#30828;&#36127;&#26679;&#26412;&#21644;&#36719;&#36127;&#26679;&#26412;&#30340;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#21033;&#29992;&#20102;&#25972;&#20010;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02772v1 Announce Type: cross  Abstract: Exercise-based rehabilitation programs have proven to be effective in enhancing the quality of life and reducing mortality and rehospitalization rates. AI-driven virtual rehabilitation, which allows patients to independently complete exercises at home, utilizes AI algorithms to analyze exercise data, providing feedback to patients and updating clinicians on their progress. These programs commonly prescribe a variety of exercise types, leading to a distinct challenge in rehabilitation exercise assessment datasets: while abundant in overall training samples, these datasets often have a limited number of samples for each individual exercise type. This disparity hampers the ability of existing approaches to train generalizable models with such a small sample size per exercise. Addressing this issue, our paper introduces a novel supervised contrastive learning framework with hard and soft negative samples that effectively utilizes the entir
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#36866;&#24212;&#20855;&#26377;&#19981;&#21516;&#31574;&#30053;&#30340;&#26410;&#30693;&#21516;&#20276;&#12290;&#36890;&#36807;&#22870;&#21169;&#26234;&#33021;&#20307;&#22312;&#21382;&#21490;&#19978;&#19979;&#25991;&#20013;&#26377;&#25928;&#35782;&#21035;&#21516;&#20276;&#34892;&#20026;&#27169;&#24335;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20419;&#36827;&#26234;&#33021;&#20307;&#31215;&#26497;&#25506;&#32034;&#21644;&#24555;&#36895;&#36866;&#24212;&#65292;&#20174;&#32780;&#22312;&#19981;&#30830;&#23450;&#21516;&#20276;&#31574;&#30053;&#26102;&#25910;&#38598;&#20449;&#24687;&#21453;&#39304;&#65292;&#24182;&#22312;&#26377;&#20449;&#24515;&#26102;&#21033;&#29992;&#19978;&#19979;&#25991;&#25191;&#34892;&#26368;&#20339;&#21453;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.02468</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#25506;&#32034;&#30340;&#24555;&#36895;&#36866;&#24212;&#26410;&#30693;&#21516;&#20276;
&lt;/p&gt;
&lt;p&gt;
Fast Peer Adaptation with Context-aware Exploration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#36866;&#24212;&#20855;&#26377;&#19981;&#21516;&#31574;&#30053;&#30340;&#26410;&#30693;&#21516;&#20276;&#12290;&#36890;&#36807;&#22870;&#21169;&#26234;&#33021;&#20307;&#22312;&#21382;&#21490;&#19978;&#19979;&#25991;&#20013;&#26377;&#25928;&#35782;&#21035;&#21516;&#20276;&#34892;&#20026;&#27169;&#24335;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20419;&#36827;&#26234;&#33021;&#20307;&#31215;&#26497;&#25506;&#32034;&#21644;&#24555;&#36895;&#36866;&#24212;&#65292;&#20174;&#32780;&#22312;&#19981;&#30830;&#23450;&#21516;&#20276;&#31574;&#30053;&#26102;&#25910;&#38598;&#20449;&#24687;&#21453;&#39304;&#65292;&#24182;&#22312;&#26377;&#20449;&#24515;&#26102;&#21033;&#29992;&#19978;&#19979;&#25991;&#25191;&#34892;&#26368;&#20339;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#28216;&#25103;&#20013;&#65292;&#24555;&#36895;&#36866;&#24212;&#20855;&#26377;&#19981;&#21516;&#31574;&#30053;&#30340;&#26410;&#30693;&#21516;&#20276;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#20026;&#20102;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#26234;&#33021;&#20307;&#33021;&#22815;&#39640;&#25928;&#22320;&#25506;&#32034;&#21644;&#35782;&#21035;&#21516;&#20276;&#30340;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#36825;&#26159;&#36866;&#24212;&#20013;&#36827;&#34892;&#26368;&#20339;&#21453;&#24212;&#30340;&#20808;&#20915;&#26465;&#20214;&#12290;&#28982;&#32780;&#65292;&#24403;&#28216;&#25103;&#26159;&#37096;&#20998;&#21487;&#35266;&#27979;&#19988;&#26102;&#38388;&#36328;&#24230;&#24456;&#38271;&#26102;&#65292;&#25506;&#32034;&#26410;&#30693;&#21516;&#20276;&#30340;&#31574;&#30053;&#26159;&#22256;&#38590;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#20276;&#35782;&#21035;&#22870;&#21169;&#65292;&#26681;&#25454;&#26234;&#33021;&#20307;&#22312;&#21382;&#21490;&#29615;&#22659;&#19979;&#65288;&#20363;&#22914;&#22810;&#20010;&#22238;&#21512;&#30340;&#35266;&#23519;&#65289;&#22914;&#20309;&#26377;&#25928;&#22320;&#35782;&#21035;&#21516;&#20276;&#30340;&#34892;&#20026;&#27169;&#24335;&#26469;&#22870;&#21169;&#23398;&#20064;&#26234;&#33021;&#20307;&#12290;&#36825;&#20010;&#22870;&#21169;&#28608;&#21169;&#26234;&#33021;&#20307;&#23398;&#20064;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#25506;&#32034;&#21644;&#24555;&#36895;&#36866;&#24212;&#65292;&#21363;&#22312;&#23545;&#21516;&#20276;&#31574;&#30053;&#19981;&#30830;&#23450;&#26102;&#31215;&#26497;&#23547;&#25214;&#21644;&#25910;&#38598;&#20449;&#24687;&#21453;&#39304;&#65292;&#24182;&#22312;&#26377;&#20449;&#24515;&#26102;&#21033;&#29992;&#19978;&#19979;&#25991;&#25191;&#34892;&#26368;&#20339;&#21453;&#24212;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#27979;&#35797;&#22330;&#26223;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fast adapting to unknown peers (partners or opponents) with different strategies is a key challenge in multi-agent games. To do so, it is crucial for the agent to efficiently probe and identify the peer's strategy, as this is the prerequisite for carrying out the best response in adaptation. However, it is difficult to explore the strategies of unknown peers, especially when the games are partially observable and have a long horizon. In this paper, we propose a peer identification reward, which rewards the learning agent based on how well it can identify the behavior pattern of the peer over the historical context, such as the observation over multiple episodes. This reward motivates the agent to learn a context-aware policy for effective exploration and fast adaptation, i.e., to actively seek and collect informative feedback from peers when uncertain about their policies and to exploit the context to perform the best response when confident. We evaluate our method on diverse testbeds 
&lt;/p&gt;</description></item><item><title>DurFlex-EVC&#36890;&#36807;&#24341;&#20837;&#39118;&#26684;&#33258;&#32534;&#30721;&#22120;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#24773;&#32490;&#35821;&#38899;&#36716;&#25442;&#27169;&#22411;&#20013;&#23545;&#35821;&#35328;&#21644;&#35821;&#38899;&#20449;&#24687;&#30340;&#21516;&#27493;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2401.08095</link><description>&lt;p&gt;
DurFlex-EVC: &#20855;&#26377;&#24182;&#34892;&#29983;&#25104;&#30340;&#25345;&#32493;&#28789;&#27963;&#24773;&#32490;&#35821;&#38899;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
DurFlex-EVC: Duration-Flexible Emotional Voice Conversion with Parallel Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08095
&lt;/p&gt;
&lt;p&gt;
DurFlex-EVC&#36890;&#36807;&#24341;&#20837;&#39118;&#26684;&#33258;&#32534;&#30721;&#22120;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#24773;&#32490;&#35821;&#38899;&#36716;&#25442;&#27169;&#22411;&#20013;&#23545;&#35821;&#35328;&#21644;&#35821;&#38899;&#20449;&#24687;&#30340;&#21516;&#27493;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#32490;&#35821;&#38899;&#36716;&#25442;&#65288;EVC&#65289;&#26088;&#22312;&#20462;&#25913;&#35828;&#35805;&#32773;&#22768;&#38899;&#30340;&#24773;&#32490;&#33394;&#24425;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#30340;&#35821;&#35328;&#20869;&#23481;&#21644;&#35828;&#35805;&#32773;&#29420;&#29305;&#30340;&#22768;&#38899;&#29305;&#24449;&#12290;&#26368;&#36817;EVC&#30340;&#36827;&#23637;&#28041;&#21450;&#21516;&#26102;&#24314;&#27169;&#38899;&#39640;&#21644;&#25345;&#32493;&#26102;&#38388;&#65292;&#21033;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#65288;seq2seq&#65289;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#22686;&#24378;&#36716;&#25442;&#30340;&#21487;&#38752;&#24615;&#21644;&#25928;&#29575;&#65292;&#26412;&#30740;&#31350;&#23558;&#37325;&#28857;&#36716;&#21521;&#24182;&#34892;&#35821;&#38899;&#29983;&#25104;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;DurFlex-EVC&#65292;&#23427;&#38598;&#25104;&#20102;&#39118;&#26684;&#33258;&#32534;&#30721;&#22120;&#21644;&#21333;&#20803;&#23545;&#40784;&#22120;&#12290;&#20256;&#32479;&#27169;&#22411;&#34429;&#28982;&#34701;&#20837;&#20102;&#21253;&#21547;&#35821;&#35328;&#21644;&#35821;&#38899;&#20449;&#24687;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#34920;&#31034;&#65292;&#20294;&#21364;&#24573;&#35270;&#20102;&#36825;&#31181;&#21452;&#37325;&#24615;&#36136;&#65292;&#23548;&#33268;&#20102;&#21487;&#25511;&#24615;&#30340;&#38477;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20132;&#21449;&#27880;&#24847;&#21147;&#20197;&#23558;&#36825;&#20123;&#34920;&#31034;&#19982;&#19981;&#21516;&#24773;&#32490;&#36827;&#34892;&#21516;&#27493;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#39118;&#26684;&#33258;&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08095v2 Announce Type: replace-cross  Abstract: Emotional voice conversion (EVC) seeks to modify the emotional tone of a speaker's voice while preserving the original linguistic content and the speaker's unique vocal characteristics. Recent advancements in EVC have involved the simultaneous modeling of pitch and duration, utilizing the potential of sequence-to-sequence (seq2seq) models. To enhance reliability and efficiency in conversion, this study shifts focus towards parallel speech generation. We introduce Duration-Flexible EVC (DurFlex-EVC), which integrates a style autoencoder and unit aligner. Traditional models, while incorporating self-supervised learning (SSL) representations that contain both linguistic and paralinguistic information, have neglected this dual nature, leading to reduced controllability. Addressing this issue, we implement cross-attention to synchronize these representations with various emotions. Additionally, a style autoencoder is developed for t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#19968;&#22871;&#8220;&#21476;&#24618;&#8221;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#35843;&#21462;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#29305;&#23450;&#19978;&#19979;&#25991;&#20013;&#30340;&#28508;&#22312;&#30693;&#35782;&#65292;&#23637;&#31034;&#20102;&#20174;&#21487;&#20449;&#24230;&#20302;&#30340;&#27169;&#22411;&#20013;&#35843;&#21462;&#21487;&#38752;&#30693;&#35782;&#30340;&#21069;&#26223;&#12290;</title><link>https://arxiv.org/abs/2312.01037</link><description>&lt;p&gt;
&#20174;&#21476;&#24618;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#35843;&#21462;&#28508;&#22312;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Eliciting Latent Knowledge from Quirky Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#19968;&#22871;&#8220;&#21476;&#24618;&#8221;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#35843;&#21462;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#29305;&#23450;&#19978;&#19979;&#25991;&#20013;&#30340;&#28508;&#22312;&#30693;&#35782;&#65292;&#23637;&#31034;&#20102;&#20174;&#21487;&#20449;&#24230;&#20302;&#30340;&#27169;&#22411;&#20013;&#35843;&#21462;&#21487;&#38752;&#30693;&#35782;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35843;&#21462;&#28508;&#22312;&#30693;&#35782;&#65288;ELK&#65289;&#26088;&#22312;&#22312;&#19968;&#20010;&#33021;&#21147;&#24378;&#22823;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#28608;&#27963;&#20013;&#25214;&#21040;&#27169;&#24335;&#65292;&#21363;&#20351;&#32593;&#32476;&#30340;&#26126;&#26174;&#36755;&#20986;&#26159;&#38169;&#35823;&#25110;&#35823;&#23548;&#24615;&#30340;&#65292;&#20063;&#33021;&#31283;&#23450;&#36319;&#36394;&#19990;&#30028;&#30340;&#30495;&#23454;&#29366;&#24577;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;ELK&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;12&#20010;&#25968;&#25454;&#38598;&#21644;&#19968;&#22871;&#30456;&#24212;&#30340;&#8220;&#21476;&#24618;&#8221;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#65292;&#21482;&#26377;&#22312;&#25552;&#31034;&#20013;&#21253;&#21547;&#20851;&#38190;&#35789;&#8220;Bob&#8221;&#26102;&#25165;&#20250;&#36827;&#34892;&#31995;&#32479;&#24615;&#38169;&#35823;&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31616;&#21333;&#30340;&#25506;&#27979;&#26041;&#27861;&#21487;&#20197;&#35843;&#21462;&#27169;&#22411;&#22312;&#36825;&#20123;&#19978;&#19979;&#25991;&#20013;&#23545;&#27491;&#30830;&#31572;&#26696;&#30340;&#28508;&#22312;&#30693;&#35782;&#65292;&#21363;&#20351;&#38382;&#39064;&#27604;&#25506;&#27979;&#22120;&#35757;&#32451;&#30340;&#38382;&#39064;&#26356;&#22256;&#38590;&#12290;&#36825;&#26159;&#30001;&#20110;&#20013;&#38388;&#23618;&#28608;&#27963;&#20013;&#30340;&#19978;&#19979;&#25991;&#26080;&#20851;&#30340;&#30693;&#35782;&#34920;&#31034;&#30340;&#23384;&#22312;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#19968;&#31181;&#26426;&#26800;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#21487;&#20197;&#20197;94%&#30340;AUROC&#26631;&#35782;&#19981;&#30495;&#23454;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#20174;&#33021;&#21147;&#24378;&#20294;&#19981;&#21463;&#20449;&#20219;&#30340;&#27169;&#22411;&#20013;&#35843;&#21462;&#21487;&#38752;&#30340;&#30693;&#35782;&#65292;&#24182;&#20419;&#36827;&#26410;&#26469;&#30740;&#31350;ELK&#26041;&#27861;&#30340;&#23454;&#35777;&#30740;&#31350;&#26159;&#26377;&#24076;&#26395;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Eliciting Latent Knowledge (ELK) aims to find patterns in a capable neural network's activations which robustly track the true state of the world, even when the network's overt output is false or misleading. To further ELK research, we introduce 12 datasets and a corresponding suite of "quirky" language models that are LoRA finetuned to make systematic errors when answering questions if and only if the keyword "Bob" is present in the prompt. We demonstrate that simple probing methods can elicit the model's latent knowledge of the correct answer in these contexts, even for problems harder than those the probe was trained on. This is enabled by context-independent knowledge representations located in middle layer activations. We also find that a mechanistic anomaly detection approach can flag untruthful behavior with 94% AUROC. Our results show promise for eliciting reliable knowledge from capable but untrusted models, and facilitates future research empirically investigating ELK methods
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#22312;&#26410;&#30693;&#23454;&#20307;&#32676;&#20307;&#20998;&#24067;&#19979;&#30340;SHAP&#35780;&#20998;&#38382;&#39064;&#12290;&#36890;&#36807;&#32771;&#34385;&#19968;&#20010;&#19981;&#30830;&#23450;&#24615;&#21306;&#22495;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#25152;&#26377;&#29305;&#24449;&#30340;SHAP&#35780;&#20998;&#30340;&#32039;&#26463;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2401.12731</link><description>&lt;p&gt;
SHAP&#35780;&#20998;&#22312;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Distributional Uncertainty of the SHAP score in Explainable Machine Learning. (arXiv:2401.12731v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#22312;&#26410;&#30693;&#23454;&#20307;&#32676;&#20307;&#20998;&#24067;&#19979;&#30340;SHAP&#35780;&#20998;&#38382;&#39064;&#12290;&#36890;&#36807;&#32771;&#34385;&#19968;&#20010;&#19981;&#30830;&#23450;&#24615;&#21306;&#22495;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#25152;&#26377;&#29305;&#24449;&#30340;SHAP&#35780;&#20998;&#30340;&#32039;&#26463;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#23646;&#20998;&#25968;&#21453;&#26144;&#20102;&#36755;&#20837;&#23454;&#20307;&#20013;&#30340;&#29305;&#24449;&#20540;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36755;&#20986;&#30340;&#37325;&#35201;&#24615;&#12290;&#20854;&#20013;&#26368;&#21463;&#27426;&#36814;&#30340;&#35780;&#20998;&#20043;&#19968;&#26159;SHAP&#35780;&#20998;&#65292;&#23427;&#26159;&#21512;&#20316;&#21338;&#24328;&#29702;&#35770;&#20013;Shapley&#20540;&#30340;&#20855;&#20307;&#23454;&#20363;&#12290;&#35813;&#35780;&#20998;&#30340;&#23450;&#20041;&#20381;&#36182;&#20110;&#23454;&#20307;&#32676;&#20307;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#30001;&#20110;&#36890;&#24120;&#19981;&#30693;&#36947;&#31934;&#30830;&#30340;&#20998;&#24067;&#65292;&#22240;&#27492;&#38656;&#35201;&#20027;&#35266;&#22320;&#36827;&#34892;&#20998;&#37197;&#25110;&#20174;&#25968;&#25454;&#20013;&#36827;&#34892;&#20272;&#35745;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#35823;&#23548;&#24615;&#30340;&#29305;&#24449;&#35780;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#19981;&#30693;&#36947;&#23454;&#20307;&#32676;&#20307;&#20998;&#24067;&#30340;SHAP&#35780;&#20998;&#25512;&#29702;&#30340;&#21407;&#21017;&#24615;&#26694;&#26550;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#21253;&#21547;&#28508;&#22312;&#20998;&#24067;&#30340;&#19981;&#30830;&#23450;&#24615;&#21306;&#22495;&#65292;&#32780;&#29305;&#24449;&#30340;SHAP&#35780;&#20998;&#25104;&#20026;&#22312;&#35813;&#21306;&#22495;&#19978;&#23450;&#20041;&#30340;&#19968;&#20010;&#20989;&#25968;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25214;&#21040;&#35813;&#20989;&#25968;&#30340;&#26368;&#22823;&#20540;&#21644;&#26368;&#23567;&#20540;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#30830;&#23450;&#25152;&#26377;&#29305;&#24449;&#30340;SHAP&#35780;&#20998;&#30340;&#32039;&#26463;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attribution scores reflect how important the feature values in an input entity are for the output of a machine learning model. One of the most popular attribution scores is the SHAP score, which is an instantiation of the general Shapley value used in coalition game theory. The definition of this score relies on a probability distribution on the entity population. Since the exact distribution is generally unknown, it needs to be assigned subjectively or be estimated from data, which may lead to misleading feature scores. In this paper, we propose a principled framework for reasoning on SHAP scores under unknown entity population distributions. In our framework, we consider an uncertainty region that contains the potential distributions, and the SHAP score of a feature becomes a function defined over this region. We study the basic problems of finding maxima and minima of this function, which allows us to determine tight ranges for the SHAP scores of all features. In particular, we pinp
&lt;/p&gt;</description></item><item><title>CodePrompt&#26159;&#19968;&#31181;&#21033;&#29992;Prompt&#23398;&#20064;&#21644;&#27880;&#24847;&#26426;&#21046;&#25216;&#26415;&#25913;&#36827;&#28304;&#20195;&#30721;&#30456;&#20851;&#20998;&#31867;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#12290;&#23427;&#33021;&#22815;&#25552;&#21462;&#28304;&#20195;&#30721;&#21644;&#30456;&#20851;&#25991;&#26412;&#20013;&#30340;&#20016;&#23500;&#30693;&#35782;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2401.05544</link><description>&lt;p&gt;
CodePrompt&#65306;&#36890;&#36807;Prompt&#23398;&#20064;&#30340;&#30693;&#35782;&#29305;&#24449;&#25913;&#36827;&#28304;&#20195;&#30721;&#30456;&#20851;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
CodePrompt: Improving Source Code-Related Classification with Knowledge Features through Prompt Learning. (arXiv:2401.05544v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05544
&lt;/p&gt;
&lt;p&gt;
CodePrompt&#26159;&#19968;&#31181;&#21033;&#29992;Prompt&#23398;&#20064;&#21644;&#27880;&#24847;&#26426;&#21046;&#25216;&#26415;&#25913;&#36827;&#28304;&#20195;&#30721;&#30456;&#20851;&#20998;&#31867;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#12290;&#23427;&#33021;&#22815;&#25552;&#21462;&#28304;&#20195;&#30721;&#21644;&#30456;&#20851;&#25991;&#26412;&#20013;&#30340;&#20016;&#23500;&#30693;&#35782;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#25506;&#32034;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CodeBERT&#65289;&#25913;&#36827;&#28304;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#30340;&#28508;&#21147;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20381;&#36182;CodeBERT&#30340;&#25991;&#26412;&#23884;&#20837;&#33021;&#21147;&#21644;"[CLS]"&#21477;&#23376;&#23884;&#20837;&#20449;&#24687;&#20316;&#20026;&#19979;&#28216;&#28304;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#30340;&#35821;&#20041;&#34920;&#31034;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#39069;&#22806;&#30340;&#31070;&#32463;&#32593;&#32476;&#23618;&#26469;&#25552;&#21462;&#26377;&#25928;&#29305;&#24449;&#65292;&#23548;&#33268;&#35745;&#31639;&#25104;&#26412;&#26356;&#39640;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#26041;&#27861;&#27809;&#26377;&#21033;&#29992;&#28304;&#20195;&#30721;&#21644;&#30456;&#20851;&#25991;&#26412;&#20013;&#20016;&#23500;&#30340;&#30693;&#35782;&#65292;&#21487;&#33021;&#23548;&#33268;&#20934;&#30830;&#24615;&#38477;&#20302;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;CodePrompt&#65292;&#36890;&#36807;Prompt&#23398;&#20064;&#21644;&#27880;&#24847;&#26426;&#21046;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#20016;&#23500;&#30693;&#35782;&#26469;&#25913;&#36827;&#28304;&#20195;&#30721;&#30456;&#20851;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers have explored the potential of utilizing pre-trained language models, such as CodeBERT, to improve source code-related tasks. Previous studies have mainly relied on CodeBERT's text embedding capability and the `[CLS]' sentence embedding information as semantic representations for fine-tuning downstream source code-related tasks. However, these methods require additional neural network layers to extract effective features, resulting in higher computational costs. Furthermore, existing approaches have not leveraged the rich knowledge contained in both source code and related text, which can lead to lower accuracy. This paper presents a novel approach, CodePrompt, which utilizes rich knowledge recalled from a pre-trained model by prompt learning and an attention mechanism to improve source code-related classification tasks. Our approach initially motivates the language model with prompt information to retrieve abundant knowledge associated with the input as representative feat
&lt;/p&gt;</description></item><item><title>LLMind&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20013;&#22830;&#21327;&#35843;&#22120;&#30340;AI&#26694;&#26550;&#65292;&#23558;LLMs&#19982;&#39046;&#22495;&#29305;&#23450;&#30340;AI&#27169;&#22359;&#25972;&#21512;&#65292;&#20351;&#24471;&#29289;&#32852;&#32593;&#35774;&#22791;&#33021;&#22815;&#26377;&#25928;&#21327;&#21516;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2312.09007</link><description>&lt;p&gt;
LLMind: &#20026;&#22797;&#26434;&#20219;&#21153;&#25191;&#34892;&#19982;AI&#21644;&#29289;&#32852;&#32593;&#36827;&#34892;&#21327;&#35843;&#30340;LLM&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
LLMind: Orchestrating AI and IoT with LLMs for Complex Task Execution. (arXiv:2312.09007v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.09007
&lt;/p&gt;
&lt;p&gt;
LLMind&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20013;&#22830;&#21327;&#35843;&#22120;&#30340;AI&#26694;&#26550;&#65292;&#23558;LLMs&#19982;&#39046;&#22495;&#29305;&#23450;&#30340;AI&#27169;&#22359;&#25972;&#21512;&#65292;&#20351;&#24471;&#29289;&#32852;&#32593;&#35774;&#22791;&#33021;&#22815;&#26377;&#25928;&#21327;&#21516;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;LLMind&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20013;&#22830;&#21327;&#35843;&#22120;&#30340;AI&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#23558;LLMs&#19982;&#39046;&#22495;&#29305;&#23450;&#30340;AI&#27169;&#22359;&#25972;&#21512;&#65292;&#20351;&#24471;&#29289;&#32852;&#32593;&#35774;&#22791;&#33021;&#22815;&#26377;&#25928;&#21327;&#21516;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#12290;LLMs&#36890;&#36807;&#29992;&#25143;&#21451;&#22909;&#30340;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19982;&#29992;&#25143;&#36827;&#34892;&#33258;&#28982;&#23545;&#35805;&#65292;&#25552;&#20986;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#30340;&#35745;&#21010;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22797;&#26434;&#20219;&#21153;&#30340;&#25191;&#34892;&#26159;&#36890;&#36807;&#25511;&#21046;&#33050;&#26412;&#23454;&#29616;&#30340;&#65292;&#36825;&#21487;&#33021;&#28041;&#21450;&#22810;&#20010;&#39046;&#22495;&#29305;&#23450;&#30340;AI&#27169;&#22359;&#21644;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#21327;&#20316;&#12290;LLMs&#20351;&#29992;&#22522;&#20110;&#26377;&#38480;&#29366;&#24577;&#26426;&#65288;FSMs&#65289;&#30340;&#35821;&#35328;&#32534;&#30721;&#36716;&#25442;&#26041;&#27861;&#29983;&#25104;&#25511;&#21046;&#33050;&#26412;&#12290;&#35813;&#26694;&#26550;&#36824;&#32467;&#21512;&#20102;&#35821;&#20041;&#20998;&#26512;&#21644;&#21709;&#24212;&#20248;&#21270;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#36895;&#24230;&#21644;&#25928;&#26524;&#12290;&#26368;&#32456;&#65292;&#35813;&#26694;&#26550;&#30340;&#35774;&#35745;&#19981;&#20165;&#26088;&#22312;&#21019;&#26032;&#29289;&#32852;&#32593;&#35774;&#22791;&#25511;&#21046;&#21644;&#20016;&#23500;&#29992;&#25143;&#20307;&#39564;&#65292;&#36824;&#20419;&#36827;&#26234;&#33021;&#21644;&#38598;&#25104;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce LLMind, an AI framework that utilizes large language models (LLMs) as a central orchestrator. The framework integrates LLMs with domain-specific AI modules, enabling IoT devices to collaborate effectively in executing complex tasks. The LLM engages in natural conversations with human users via a user-friendly social media platform to come up with a plan to execute complex tasks. In particular, the execution of a complex task, which may involve the collaborations of multiple domain-specific AI modules and IoT devices, is realized through a control script. The LLM generates the control script using a Language-Code transformation approach based on finite-state machines (FSMs). The framework also incorporates semantic analysis and response optimization techniques to enhance speed and effectiveness. Ultimately, this framework is designed not only to innovate IoT device control and enrich user experiences but also to foster an intelligent and integrated IoT device
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#28145;&#20837;&#35843;&#26597;&#21644;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36923;&#36753;&#25512;&#29702;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#31181;&#31574;&#30053;&#26469;&#36171;&#20104;LLMs&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#29983;&#25104;&#26356;&#20855;&#36923;&#36753;&#19968;&#33268;&#24615;&#30340;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.09158</link><description>&lt;p&gt;
&#23398;&#20064;&#22914;&#20309;&#25945;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36923;&#36753;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Learning To Teach Large Language Models Logical Reasoning. (arXiv:2310.09158v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#28145;&#20837;&#35843;&#26597;&#21644;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36923;&#36753;&#25512;&#29702;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#31181;&#31574;&#30053;&#26469;&#36171;&#20104;LLMs&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#29983;&#25104;&#26356;&#20855;&#36923;&#36753;&#19968;&#33268;&#24615;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#20854;&#22312;&#35821;&#35328;&#29983;&#25104;&#21644;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#20986;&#33394;&#34920;&#29616;&#32780;&#21463;&#21040;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#22266;&#26377;&#38382;&#39064;&#65288;&#22914;&#24187;&#35273;&#65289;&#65292;&#30446;&#21069;&#30340;LLMs&#22312;&#23454;&#38469;&#25512;&#29702;&#20219;&#21153;&#20013;&#20173;&#28982;&#36755;&#20986;&#19981;&#21487;&#38752;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;LLMs&#22312;&#36923;&#36753;&#25512;&#29702;&#20013;&#30340;&#33021;&#21147;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;LLMs&#22312;&#19981;&#21516;&#20219;&#21153;&#65288;&#21253;&#25324;&#20107;&#20214;&#20851;&#31995;&#25552;&#21462;&#21644;&#28436;&#32462;&#25512;&#29702;&#65289;&#20013;&#22312;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#22312;&#35299;&#20915;&#38656;&#35201;&#20005;&#26684;&#25512;&#29702;&#30340;&#20219;&#21153;&#26102;&#24182;&#19981;&#26159;&#24456;&#22909;&#30340;&#25512;&#29702;&#32773;&#65292;&#20250;&#20135;&#29983;&#21453;&#20107;&#23454;&#30340;&#31572;&#26696;&#65292;&#36825;&#35201;&#27714;&#25105;&#20204;&#19981;&#26029;&#25913;&#36827;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20840;&#38754;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#31574;&#30053;&#65292;&#36171;&#20104;LLMs&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#20351;&#20854;&#33021;&#22815;&#29983;&#25104;&#26356;&#20855;&#36923;&#36753;&#19968;&#33268;&#24615;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have gained enormous attention from both academia and industry, due to their exceptional ability in language generation and extremely powerful generalization. However, current LLMs still output unreliable content in practical reasoning tasks due to their inherent issues (e.g., hallucination). To better disentangle this problem, in this paper, we conduct an in-depth investigation to systematically explore the capability of LLMs in logical reasoning. More in detail, we first investigate the deficiency of LLMs in logical reasoning on different tasks, including event relation extraction and deductive reasoning. Our study demonstrates that LLMs are not good reasoners in solving tasks with rigorous reasoning and will produce counterfactual answers, which require us to iteratively refine. Therefore, we comprehensively explore different strategies to endow LLMs with logical reasoning ability, and thus enable them to generate more logically consistent answers across
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#8220;&#19968;&#33324;&#26446;&#26222;&#24076;&#33576;&#65288;GL&#65289;&#8221;&#65292;&#29992;&#20110;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35748;&#35777;&#65292;&#20197;&#25269;&#24481;&#21487;&#32452;&#21512;&#30340;&#21487;&#35299;&#37322;&#30340;&#35821;&#20041;&#24178;&#25200;&#12290;&#26041;&#27861;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#34920;&#29616;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2309.16710</link><description>&lt;p&gt;
&#36890;&#36807;&#20381;&#36182;&#20110;&#21464;&#25442;&#30340;&#38543;&#26426;&#24179;&#28369;&#65292;&#25552;&#20379;&#19968;&#33324;&#26446;&#26222;&#24076;&#33576;&#65306;&#38024;&#23545;&#21487;&#35299;&#37322;&#30340;&#35821;&#20041;&#21464;&#25442;&#30340;&#35748;&#35777;&#20581;&#22766;&#24615;
&lt;/p&gt;
&lt;p&gt;
General Lipschitz: Certified Robustness Against Resolvable Semantic Transformations via Transformation-Dependent Randomized Smoothing. (arXiv:2309.16710v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16710
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#8220;&#19968;&#33324;&#26446;&#26222;&#24076;&#33576;&#65288;GL&#65289;&#8221;&#65292;&#29992;&#20110;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35748;&#35777;&#65292;&#20197;&#25269;&#24481;&#21487;&#32452;&#21512;&#30340;&#21487;&#35299;&#37322;&#30340;&#35821;&#20041;&#24178;&#25200;&#12290;&#26041;&#27861;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#34920;&#29616;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#24179;&#28369;&#26159;&#30446;&#21069;&#26500;&#24314;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#21487;&#20197;&#35777;&#26126;&#20854;&#23545;&#20110;&#26377;&#30028;&#24178;&#25200;&#30340;&#25239;&#24615;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#38024;&#23545;&#35821;&#20041;&#21464;&#25442;&#65288;&#20363;&#22914;&#65292;&#22270;&#20687;&#27169;&#31946;&#12289;&#24179;&#31227;&#12289;Gamma&#30699;&#27491;&#65289;&#21450;&#20854;&#32452;&#21512;&#30340;&#21512;&#29702;&#35777;&#20070;&#26356;&#20026;&#22797;&#26434;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#8220;&#19968;&#33324;&#26446;&#26222;&#24076;&#33576;&#65288;GL&#65289;&#8221;&#65292;&#29992;&#20110;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35748;&#35777;&#65292;&#20197;&#25269;&#24481;&#21487;&#32452;&#21512;&#30340;&#21487;&#35299;&#37322;&#30340;&#35821;&#20041;&#24178;&#25200;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#24179;&#28369;&#20998;&#31867;&#22120;&#19982;&#21464;&#25442;&#21442;&#25968;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#25512;&#23548;&#20986;&#30456;&#24212;&#30340;&#20581;&#22766;&#24615;&#35777;&#20070;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#34920;&#29616;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Randomized smoothing is the state-of-the-art approach to construct image classifiers that are provably robust against additive adversarial perturbations of bounded magnitude. However, it is more complicated to construct reasonable certificates against semantic transformation (e.g., image blurring, translation, gamma correction) and their compositions. In this work, we propose \emph{General Lipschitz (GL),} a new framework to certify neural networks against composable resolvable semantic perturbations. Within the framework, we analyze transformation-dependent Lipschitz-continuity of smoothed classifiers w.r.t. transformation parameters and derive corresponding robustness certificates. Our method performs comparably to state-of-the-art approaches on the ImageNet dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MVMR&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#32473;&#23450;&#25991;&#26412;&#26597;&#35810;&#20174;&#22823;&#37327;&#35270;&#39057;&#38598;&#20013;&#23450;&#20301;&#35270;&#39057;&#24103;&#12290;&#25105;&#20204;&#36890;&#36807;&#24050;&#26377;&#25968;&#25454;&#38598;&#36827;&#34892;&#30456;&#20284;&#24615;&#31579;&#36873;&#26469;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#24182;&#24341;&#20837;&#19977;&#20010;MVMR&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#23884;&#20837;&#24335;&#25991;&#26412;&#30456;&#20284;&#24230;&#21305;&#37197;&#21644;&#35270;&#39057;-&#35821;&#35328;&#23545;&#40784;&#25216;&#26415;&#26469;&#35745;&#31639;&#30456;&#20851;&#24615;&#24471;&#20998;&#65292;&#24182;&#20026;MVMR&#20219;&#21153;&#24320;&#21457;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;Reliable Mutual Matching Network (RMMN)&#12290;</title><link>http://arxiv.org/abs/2309.16701</link><description>&lt;p&gt;
MVMR: &#22312;&#22810;&#20010;&#21487;&#38752;&#35270;&#39057;&#38598;&#20013;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#35270;&#39057;&#23450;&#20301;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
MVMR: Evaluating Natural Language Video Localization Bias over Multiple Reliable Videos Pool. (arXiv:2309.16701v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MVMR&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#32473;&#23450;&#25991;&#26412;&#26597;&#35810;&#20174;&#22823;&#37327;&#35270;&#39057;&#38598;&#20013;&#23450;&#20301;&#35270;&#39057;&#24103;&#12290;&#25105;&#20204;&#36890;&#36807;&#24050;&#26377;&#25968;&#25454;&#38598;&#36827;&#34892;&#30456;&#20284;&#24615;&#31579;&#36873;&#26469;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#24182;&#24341;&#20837;&#19977;&#20010;MVMR&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#23884;&#20837;&#24335;&#25991;&#26412;&#30456;&#20284;&#24230;&#21305;&#37197;&#21644;&#35270;&#39057;-&#35821;&#35328;&#23545;&#40784;&#25216;&#26415;&#26469;&#35745;&#31639;&#30456;&#20851;&#24615;&#24471;&#20998;&#65292;&#24182;&#20026;MVMR&#20219;&#21153;&#24320;&#21457;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;Reliable Mutual Matching Network (RMMN)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36817;&#24180;&#26469;&#22810;&#23186;&#20307;&#20869;&#23481;&#30340;&#28608;&#22686;&#65292;&#33258;&#28982;&#35821;&#35328;&#35270;&#39057;&#23450;&#20301;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#23427;&#33268;&#21147;&#20110;&#26816;&#27979;&#19982;&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#21305;&#37197;&#30340;&#35270;&#39057;&#29255;&#27573;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#37117;&#27809;&#26377;&#25506;&#32034;&#22312;&#23384;&#22312;&#22810;&#20010;&#27491;&#36127;&#35270;&#39057;&#30340;&#22823;&#37327;&#35821;&#26009;&#24211;&#20013;&#23450;&#20301;&#19968;&#20010;&#26102;&#21051;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MVMR&#65288;Massive Videos Moment Retrieval&#65289;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#32473;&#23450;&#25991;&#26412;&#26597;&#35810;&#20174;&#22823;&#37327;&#35270;&#39057;&#38598;&#20013;&#23450;&#20301;&#35270;&#39057;&#24103;&#12290;&#23545;&#20110;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#29616;&#26377;&#35270;&#39057;&#23450;&#20301;&#25968;&#25454;&#38598;&#36827;&#34892;&#30456;&#20284;&#24615;&#31579;&#36873;&#26469;&#26500;&#24314;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19977;&#20010;MVMR&#25968;&#25454;&#38598;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#23884;&#20837;&#30340;&#25991;&#26412;&#30456;&#20284;&#24230;&#21305;&#37197;&#21644;&#35270;&#39057;-&#35821;&#35328;&#23545;&#40784;&#25216;&#26415;&#26469;&#35745;&#31639;&#30446;&#26631;&#26597;&#35810;&#19982;&#35270;&#39057;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#24471;&#20998;&#65292;&#20174;&#32780;&#23450;&#20041;&#27491;&#36127;&#38598;&#12290;&#38024;&#23545;&#25552;&#20986;&#30340;MVMR&#20219;&#21153;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;Reliable Mutual Matching Network (RMMN)&#12290;
&lt;/p&gt;
&lt;p&gt;
With the explosion of multimedia content in recent years, natural language video localization, which focuses on detecting video moment that matches a given natural language query, has become a critical problem. However, none of the previous research explores localizing a moment from a large corpus where multiple positive and negative videos exist. In this paper, we propose an MVMR (Massive Videos Moment Retrieval) task, which aims to localize video frames from a massive set of videos given a text query. For this task, we suggest methods for constructing datasets by employing similarity filtering on the existing video localization datasets and introduce three MVMR datasets. Specifically, we employ embedding-based text similarity matching and video-language grounding techniques to calculate the relevance score between a target query and videos to define positive and negative sets. For the proposed MVMR task, we further develop a strong model, Reliable Mutual Matching Network (RMMN), whic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;NEAT&#26041;&#27861;&#35757;&#32451;&#20998;&#24067;&#24335;&#26426;&#22120;&#20154;&#32676;&#20307;&#31639;&#27861;&#20135;&#29983;&#26032;&#39062;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14663</link><description>&lt;p&gt;
&#29992;NEAT&#23398;&#20064;&#26426;&#22120;&#20154;&#32676;&#20307;&#30340;&#26032;&#39062;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Learning Emergent Behavior in Robot Swarms with NEAT. (arXiv:2309.14663v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;NEAT&#26041;&#27861;&#35757;&#32451;&#20998;&#24067;&#24335;&#26426;&#22120;&#20154;&#32676;&#20307;&#31639;&#27861;&#20135;&#29983;&#26032;&#39062;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30740;&#31350;&#26426;&#22120;&#20154;&#32676;&#20307;&#26102;&#65292;&#35768;&#22810;&#30740;&#31350;&#35266;&#23519;&#21040;&#22797;&#26434;&#30340;&#32676;&#20307;&#34892;&#20026;&#26159;&#30001;&#20010;&#20307;&#26234;&#33021;&#20307;&#30340;&#31616;&#21333;&#23616;&#37096;&#21160;&#20316;&#20135;&#29983;&#30340;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#20010;&#20307;&#31574;&#30053;&#20197;&#20135;&#29983;&#25152;&#26399;&#26395;&#30340;&#26032;&#39062;&#34892;&#20026;&#30340;&#20219;&#21153;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#22522;&#26412;&#19978;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#20998;&#24067;&#24335;&#26426;&#22120;&#20154;&#32676;&#20307;&#31639;&#27861;&#20197;&#20135;&#29983;&#26032;&#39062;&#34892;&#20026;&#30340;&#26041;&#27861;&#12290;&#21463;&#21040;&#21160;&#29289;&#20013;&#26032;&#39062;&#34892;&#20026;&#30340;&#29983;&#29289;&#36827;&#21270;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#36827;&#21270;&#31639;&#27861;&#35757;&#32451;&#19968;&#20010;&#8220;&#31181;&#32676;&#8221;&#20013;&#30340;&#20010;&#20307;&#34892;&#20026;&#26469;&#36817;&#20284;&#26399;&#26395;&#30340;&#32676;&#20307;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#20351;&#29992;CoppeliaSim&#27169;&#25311;&#22120;&#36827;&#34892;&#30340;Georgia Tech Miniature Autonomous Blimps&#65288;GT-MABs&#65289;&#31354;&#20013;&#26426;&#22120;&#20154;&#24179;&#21488;&#27169;&#25311;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;Anki Vector&#26426;&#22120;&#20154;&#30340;&#27169;&#25311;&#20013;&#36827;&#34892;&#27979;&#35797;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#21508;&#31181;&#28608;&#21169;&#27169;&#24335;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23545;&#38656;&#35201;&#19968;&#23450;&#22797;&#26434;&#32676;&#20307;&#34892;&#20026;&#25165;&#33021;&#25104;&#21151;&#30340;&#21508;&#31181;&#20219;&#21153;&#36827;&#34892;&#20102;&#31639;&#27861;&#35780;&#20272;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#21306;&#22495;C&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
When researching robot swarms, many studies observe complex group behavior emerging from the individual agents' simple local actions. However, the task of learning an individual policy to produce a desired emergent behavior remains a challenging and largely unsolved problem. We present a method of training distributed robotic swarm algorithms to produce emergent behavior. Inspired by the biological evolution of emergent behavior in animals, we use an evolutionary algorithm to train a 'population' of individual behaviors to approximate a desired group behavior. We perform experiments using simulations of the Georgia Tech Miniature Autonomous Blimps (GT-MABs) aerial robotics platforms conducted in the CoppeliaSim simulator. Additionally, we test on simulations of Anki Vector robots to display our algorithm's effectiveness on various modes of actuation. We evaluate our algorithm on various tasks where a somewhat complex group behavior is required for success. These tasks include an Area C
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;CRAMP&#65292;&#19968;&#31181;&#22522;&#20110;&#22686;&#24378;&#24335;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#30340;&#20247;&#21253;&#24863;&#30693;&#20998;&#25955;&#24335;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#25317;&#25380;&#29615;&#22659;&#19979;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2309.10275</link><description>&lt;p&gt;
&#20855;&#26377;&#22686;&#24378;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#30340;&#20247;&#21253;&#24863;&#30693;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Crowd-Aware Multi-Agent Pathfinding With Boosted Curriculum Reinforcement Learning. (arXiv:2309.10275v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10275
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;CRAMP&#65292;&#19968;&#31181;&#22522;&#20110;&#22686;&#24378;&#24335;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#30340;&#20247;&#21253;&#24863;&#30693;&#20998;&#25955;&#24335;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#25317;&#25380;&#29615;&#22659;&#19979;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25317;&#25380;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36816;&#21160;&#35268;&#21010;&#38382;&#39064;&#65292;&#26088;&#22312;&#20026;&#31995;&#32479;&#20013;&#30340;&#25152;&#26377;&#26234;&#33021;&#20307;&#25214;&#21040;&#26080;&#30896;&#25758;&#36335;&#24452;&#12290;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#37117;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#31354;&#20013;&#32676;&#20307;&#12289;&#33258;&#21160;&#21270;&#20179;&#20648;&#26426;&#22120;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#12290;&#24403;&#21069;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#21487;&#20197;&#22823;&#33268;&#20998;&#20026;&#20004;&#31181;&#20027;&#35201;&#31867;&#21035;&#65306;&#38598;&#20013;&#24335;&#35268;&#21010;&#21644;&#20998;&#25955;&#24335;&#35268;&#21010;&#12290;&#38598;&#20013;&#24335;&#35268;&#21010;&#21463;&#21040;&#32500;&#24230;&#28798;&#38590;&#30340;&#22256;&#25200;&#65292;&#22240;&#27492;&#22312;&#22823;&#22411;&#21644;&#22797;&#26434;&#29615;&#22659;&#20013;&#19981;&#20855;&#22791;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20998;&#25955;&#24335;&#35268;&#21010;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#36827;&#34892;&#23454;&#26102;&#36335;&#24452;&#35268;&#21010;&#65292;&#23637;&#31034;&#20102;&#38544;&#24335;&#30340;&#21327;&#35843;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#23494;&#38598;&#29615;&#22659;&#20013;&#23427;&#20204;&#30340;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#19988;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CRAMP&#30340;&#20247;&#21253;&#24863;&#30693;&#20998;&#25955;&#24335;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#24335;&#35838;&#31243;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Path Finding (MAPF) in crowded environments presents a challenging problem in motion planning, aiming to find collision-free paths for all agents in the system. MAPF finds a wide range of applications in various domains, including aerial swarms, autonomous warehouse robotics, and self-driving vehicles. The current approaches for MAPF can be broadly categorized into two main categories: centralized and decentralized planning. Centralized planning suffers from the curse of dimensionality and thus does not scale well in large and complex environments. On the other hand, decentralized planning enables agents to engage in real-time path planning within a partially observable environment, demonstrating implicit coordination. However, they suffer from slow convergence and performance degradation in dense environments. In this paper, we introduce CRAMP, a crowd-aware decentralized approach to address this problem by leveraging reinforcement learning guided by a boosted curriculum-b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#36873;&#25321;&#26368;&#20339;&#30340;&#20301;&#23485;&#21644;&#23618;&#23485;&#26469;&#25552;&#39640;&#32593;&#32476;&#25928;&#29575;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#21098;&#26525;&#21644;&#32858;&#31867;&#25216;&#26415;&#65292;&#20248;&#21270;&#20102;&#25628;&#32034;&#36807;&#31243;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20005;&#26684;&#27979;&#35797;&#65292;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.06422</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#32858;&#31867;&#30340;&#26641;&#29366;Parzen&#20272;&#35745;&#30340;&#25935;&#24863;&#24615;&#24863;&#30693;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#21644;&#23485;&#24230;&#20248;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Sensitivity-Aware Mixed-Precision Quantization and Width Optimization of Deep Neural Networks Through Cluster-Based Tree-Structured Parzen Estimation. (arXiv:2308.06422v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#36873;&#25321;&#26368;&#20339;&#30340;&#20301;&#23485;&#21644;&#23618;&#23485;&#26469;&#25552;&#39640;&#32593;&#32476;&#25928;&#29575;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#21098;&#26525;&#21644;&#32858;&#31867;&#25216;&#26415;&#65292;&#20248;&#21270;&#20102;&#25628;&#32034;&#36807;&#31243;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20005;&#26684;&#27979;&#35797;&#65292;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#35745;&#31639;&#38656;&#27714;&#30340;&#25552;&#39640;&#65292;&#23545;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#26377;&#25928;&#20248;&#21270;&#26041;&#27861;&#30340;&#38656;&#27714;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25628;&#32034;&#26426;&#21046;&#65292;&#29992;&#20110;&#33258;&#21160;&#36873;&#25321;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#26368;&#20339;&#20301;&#23485;&#21644;&#23618;&#23485;&#12290;&#36825;&#23548;&#33268;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25928;&#29575;&#30340;&#26126;&#26174;&#25552;&#39640;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;Hessian&#30340;&#21098;&#26525;&#31574;&#30053;&#65292;&#26377;&#36873;&#25321;&#22320;&#20943;&#23569;&#25628;&#32034;&#22495;&#65292;&#30830;&#20445;&#31227;&#38500;&#38750;&#20851;&#38190;&#21442;&#25968;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#32858;&#31867;&#30340;&#26641;&#29366;Parzen&#20272;&#35745;&#22120;&#24320;&#21457;&#26377;&#21033;&#21644;&#19981;&#21033;&#32467;&#26524;&#30340;&#26367;&#20195;&#27169;&#22411;&#12290;&#36825;&#31181;&#31574;&#30053;&#20801;&#35768;&#23545;&#26550;&#26500;&#21487;&#33021;&#24615;&#36827;&#34892;&#31616;&#21270;&#30340;&#25506;&#32034;&#65292;&#24182;&#36805;&#36895;&#30830;&#23450;&#34920;&#29616;&#26368;&#22909;&#30340;&#35774;&#35745;&#12290;&#36890;&#36807;&#23545;&#30693;&#21517;&#25968;&#25454;&#38598;&#36827;&#34892;&#20005;&#26684;&#27979;&#35797;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#35777;&#26126;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#30340;&#26126;&#26174;&#20248;&#21183;&#12290;&#19982;&#39046;&#20808;&#30340;&#21387;&#32553;&#31574;&#30053;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the complexity and computational demands of deep learning models rise, the need for effective optimization methods for neural network designs becomes paramount. This work introduces an innovative search mechanism for automatically selecting the best bit-width and layer-width for individual neural network layers. This leads to a marked enhancement in deep neural network efficiency. The search domain is strategically reduced by leveraging Hessian-based pruning, ensuring the removal of non-crucial parameters. Subsequently, we detail the development of surrogate models for favorable and unfavorable outcomes by employing a cluster-based tree-structured Parzen estimator. This strategy allows for a streamlined exploration of architectural possibilities and swift pinpointing of top-performing designs. Through rigorous testing on well-known datasets, our method proves its distinct advantage over existing methods. Compared to leading compression strategies, our approach records an impressive 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Pair-Net&#30340;&#20840;&#26223;&#22330;&#26223;&#22270;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#37197;&#23545;&#25552;&#26696;&#32593;&#32476;&#65288;PPN&#65289;&#26469;&#23398;&#20064;&#21644;&#36807;&#28388;&#20027;&#20307;&#21644;&#29289;&#20307;&#20043;&#38388;&#30340;&#31232;&#30095;&#37197;&#23545;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#20840;&#26223;&#22330;&#26223;&#22270;&#29983;&#25104;&#26041;&#27861;&#20013;&#24573;&#35270;&#30340;&#23545;&#35937;&#38388;&#37197;&#23545;&#22238;&#24518;&#29575;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.08699</link><description>&lt;p&gt;
&#32858;&#28966;&#22330;&#26223;&#22270;&#29983;&#25104;&#30340;&#37197;&#23545;-&#20851;&#31995;&#65306;&#22522;&#20110;&#37197;&#23545;&#32593;&#32476;&#30340;&#20840;&#26223;&#22330;&#26223;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Pair then Relation: Pair-Net for Panoptic Scene Graph Generation. (arXiv:2307.08699v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Pair-Net&#30340;&#20840;&#26223;&#22330;&#26223;&#22270;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#37197;&#23545;&#25552;&#26696;&#32593;&#32476;&#65288;PPN&#65289;&#26469;&#23398;&#20064;&#21644;&#36807;&#28388;&#20027;&#20307;&#21644;&#29289;&#20307;&#20043;&#38388;&#30340;&#31232;&#30095;&#37197;&#23545;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#20840;&#26223;&#22330;&#26223;&#22270;&#29983;&#25104;&#26041;&#27861;&#20013;&#24573;&#35270;&#30340;&#23545;&#35937;&#38388;&#37197;&#23545;&#22238;&#24518;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#26223;&#22330;&#26223;&#22270;&#29983;&#25104;&#26159;&#22330;&#26223;&#22270;&#29983;&#25104;&#20013;&#30340;&#19968;&#39033;&#25361;&#25112;&#24615;&#20219;&#21153;&#65292;&#20854;&#26088;&#22312;&#20351;&#29992;&#20840;&#26223;&#20998;&#21106;&#20195;&#26367;&#36793;&#30028;&#26694;&#26469;&#21019;&#24314;&#26356;&#20840;&#38754;&#30340;&#22330;&#26223;&#22270;&#34920;&#31034;&#12290;&#19982;&#22330;&#26223;&#22270;&#29983;&#25104;&#30456;&#27604;&#65292;&#20840;&#26223;&#22330;&#26223;&#22270;&#29983;&#25104;&#20855;&#26377;&#19968;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65306;&#20687;&#32032;&#32423;&#20998;&#21106;&#36755;&#20986;&#21644;&#23436;&#20840;&#20851;&#31995;&#25506;&#32034;&#65288;&#23427;&#36824;&#32771;&#34385;&#20102;&#29289;&#20307;&#21644;&#29289;&#36136;&#20043;&#38388;&#30340;&#20851;&#31995;&#65289;&#12290;&#22240;&#27492;&#65292;&#24403;&#21069;&#30340;&#20840;&#26223;&#22330;&#26223;&#22270;&#29983;&#25104;&#26041;&#27861;&#30340;&#24615;&#33021;&#26377;&#38480;&#65292;&#36825;&#38459;&#30861;&#20102;&#19979;&#28216;&#20219;&#21153;&#25110;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;&#26412;&#24037;&#20316;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#19968;&#31181;&#26032;&#39062;&#19988;&#24378;&#22823;&#30340;&#20840;&#26223;&#22330;&#26223;&#22270;&#29983;&#25104;&#22522;&#20934;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#30830;&#23450;&#20102;&#24403;&#21069;&#20840;&#26223;&#22330;&#26223;&#22270;&#29983;&#25104;&#27169;&#22411;&#30340;&#29942;&#39048;&#65292;&#21457;&#29616;&#23545;&#35937;&#38388;&#37197;&#23545;&#30340;&#22238;&#24518;&#29575;&#26159;&#20808;&#21069;&#30340;&#20840;&#26223;&#22330;&#26223;&#22270;&#29983;&#25104;&#26041;&#27861;&#25152;&#24573;&#35270;&#30340;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#21644;&#26368;&#36817;&#30340;&#22522;&#20110;&#26597;&#35810;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65306;&#37197;&#23545;-&#20851;&#31995;&#65288;Pair-Net&#65289;&#65292;&#23427;&#20351;&#29992;&#37197;&#23545;&#25552;&#26696;&#32593;&#32476;&#65288;PPN&#65289;&#26469;&#23398;&#20064;&#21644;&#36807;&#28388;&#20027;&#20307;&#21644;&#29289;&#20307;&#20043;&#38388;&#30340;&#31232;&#30095;&#37197;&#23545;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Panoptic Scene Graph (PSG) is a challenging task in Scene Graph Generation (SGG) that aims to create a more comprehensive scene graph representation using panoptic segmentation instead of boxes. Compared to SGG, PSG has several challenging problems: pixel-level segment outputs and full relationship exploration (It also considers thing and stuff relation). Thus, current PSG methods have limited performance, which hinders downstream tasks or applications. The goal of this work aims to design a novel and strong baseline for PSG. To achieve that, we first conduct an in-depth analysis to identify the bottleneck of the current PSG models, finding that inter-object pair-wise recall is a crucial factor that was ignored by previous PSG methods. Based on this and the recent query-based frameworks, we present a novel framework: Pair then Relation (Pair-Net), which uses a Pair Proposal Network (PPN) to learn and filter sparse pair-wise relationships between subjects and objects. Moreover, we also 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#20171;&#32461;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#39044;&#27979;&#12289;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#25554;&#20540;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#26174;&#24335;&#22320;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#21644;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20026;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20998;&#26512;&#24102;&#26469;&#20102;&#26032;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2307.03759</link><description>&lt;p&gt;
&#19968;&#39033;&#20851;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#32508;&#36848;&#65306;&#39044;&#27979;&#12289;&#20998;&#31867;&#12289;&#25554;&#20540;&#21644;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection. (arXiv:2307.03759v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03759
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#20171;&#32461;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#39044;&#27979;&#12289;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#25554;&#20540;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#26174;&#24335;&#22320;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#21644;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20026;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20998;&#26512;&#24102;&#26469;&#20102;&#26032;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#26159;&#35760;&#24405;&#21160;&#24577;&#31995;&#32479;&#27979;&#37327;&#32467;&#26524;&#30340;&#20027;&#35201;&#25968;&#25454;&#31867;&#22411;&#65292;&#36890;&#36807;&#29289;&#29702;&#20256;&#24863;&#22120;&#21644;&#22312;&#32447;&#36807;&#31243;&#65288;&#34394;&#25311;&#20256;&#24863;&#22120;&#65289;&#29983;&#25104;&#22823;&#37327;&#25968;&#25454;&#12290;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#23545;&#20110;&#25581;&#31034;&#21487;&#29992;&#25968;&#25454;&#20013;&#25152;&#34164;&#21547;&#30340;&#20016;&#23500;&#20449;&#24687;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#22522;&#20110;GNN&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26041;&#27861;&#20063;&#22823;&#24133;&#22686;&#21152;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#26174;&#24335;&#22320;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#21644;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#32780;&#20256;&#32479;&#21644;&#20854;&#20182;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#21017;&#38754;&#20020;&#22256;&#38590;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#32508;&#36848;&#65288;GNN4TS&#65289;&#65292;&#21253;&#25324;&#22235;&#20010;&#22522;&#26412;&#32500;&#24230;&#65306;&#39044;&#27979;&#12289;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#25554;&#20540;&#12290;&#25105;&#20204;&#26088;&#22312;&#25351;&#23548;&#35774;&#35745;&#24072;&#21644;&#23454;&#36341;&#32773;&#20102;&#35299;&#12289;&#26500;&#24314;&#24212;&#29992;&#21644;&#25512;&#21160;GNN4TS&#30340;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;GNN4TS&#20998;&#31867;&#20307;&#31995;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Time series are the primary data type used to record dynamic system measurements and generated in great volume by both physical sensors and online processes (virtual sensors). Time series analytics is therefore crucial to unlocking the wealth of information implicit in available data. With the recent advancements in graph neural networks (GNNs), there has been a surge in GNN-based approaches for time series analysis. Approaches can explicitly model inter-temporal and inter-variable relationships, which traditional and other deep neural network-based methods struggle to do. In this survey, we provide a comprehensive review of graph neural networks for time series analysis (GNN4TS), encompassing four fundamental dimensions: Forecasting, classification, anomaly detection, and imputation. Our aim is to guide designers and practitioners to understand, build applications, and advance research of GNN4TS. At first, we provide a comprehensive task-oriented taxonomy of GNN4TS. Then, we present a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25506;&#32034;&#25216;&#26415;&#65292;&#20351;&#29992;&#20540;&#26465;&#20214;&#29366;&#24577;&#29109;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#32034;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#22343;&#34913;&#22320;&#35206;&#30422;&#20302;&#20215;&#20540;&#21644;&#39640;&#20215;&#20540;&#29366;&#24577;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#22522;&#20110;&#29109;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;MuJoCo&#22522;&#20934;&#27979;&#35797;&#21644;Atari&#28216;&#25103;&#19978;&#26377;&#30528;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.19476</link><description>&lt;p&gt;
&#20351;&#29992;&#20540;&#26465;&#20214;&#29366;&#24577;&#29109;&#25506;&#32034;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Accelerating Reinforcement Learning with Value-Conditional State Entropy Exploration. (arXiv:2305.19476v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25506;&#32034;&#25216;&#26415;&#65292;&#20351;&#29992;&#20540;&#26465;&#20214;&#29366;&#24577;&#29109;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#32034;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#22343;&#34913;&#22320;&#35206;&#30422;&#20302;&#20215;&#20540;&#21644;&#39640;&#20215;&#20540;&#29366;&#24577;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#22522;&#20110;&#29109;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;MuJoCo&#22522;&#20934;&#27979;&#35797;&#21644;Atari&#28216;&#25103;&#19978;&#26377;&#30528;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#30340;&#19968;&#31181;&#26377;&#25928;&#25216;&#26415;&#26159;&#36890;&#36807;&#40723;&#21169;&#23545;&#35775;&#38382;&#29366;&#24577;&#31354;&#38388;&#30340;&#22343;&#21248;&#35206;&#30422;&#26469;&#26368;&#22823;&#21270;&#24050;&#35775;&#38382;&#29366;&#24577;&#20998;&#24067;&#30340;&#29109;&#65292;&#21363;&#29366;&#24577;&#29109;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#26377;&#20219;&#21153;&#22870;&#21169;&#30340;&#30417;&#30563;&#35774;&#32622;&#20013;&#24448;&#24448;&#38590;&#20197;&#24212;&#23545;&#65292;&#20854;&#20013;&#20195;&#29702;&#36235;&#21521;&#20110;&#35775;&#38382;&#39640;&#20215;&#20540;&#29366;&#24577;&#20197;&#21033;&#29992;&#20219;&#21153;&#22870;&#21169;&#12290;&#36825;&#20010;&#20559;&#22909;&#20250;&#23548;&#33268;&#39640;&#20215;&#20540;&#29366;&#24577;&#21644;&#20302;&#20215;&#20540;&#29366;&#24577;&#30340;&#20998;&#24067;&#19981;&#24179;&#34913;&#65292;&#24403;&#20998;&#24067;&#21464;&#24471;&#26356;&#21152;&#22343;&#21248;&#26102;&#65292;&#29366;&#24577;&#29109;&#20250;&#22686;&#21152;&#65292;&#20174;&#32780;&#20559;&#21521;&#20110;&#25506;&#32034;&#20302;&#20215;&#20540;&#21306;&#22495;&#12290;&#24403;&#39640;&#20215;&#20540;&#29366;&#24577;&#22312;&#29366;&#24577;&#31354;&#38388;&#20013;&#20998;&#24067;&#29421;&#31364;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#20250;&#36827;&#19968;&#27493;&#24694;&#21270;&#65292;&#20351;&#24471;&#20195;&#29702;&#23436;&#25104;&#20219;&#21153;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25506;&#32034;&#25216;&#26415;&#65292;&#26368;&#22823;&#21270;&#20540;&#26465;&#20214;&#29366;&#24577;&#29109;&#65292;&#23427;&#20998;&#21035;&#20272;&#35745;&#27599;&#20010;&#29366;&#24577;&#20215;&#20540;&#20272;&#35745;&#26465;&#20214;&#19979;&#30340;&#29366;&#24577;&#29109;&#65292;&#28982;&#21518;&#26368;&#22823;&#21270;&#23427;&#20204;&#30340;&#21152;&#26435;&#21644;&#12290;&#20540;&#26465;&#20214;&#29366;&#24577;&#29109;&#37327;&#21270;&#20102;&#20302;&#20215;&#20540;&#21644;&#39640;&#20215;&#20540;&#29366;&#24577;&#21306;&#22495;&#30340;&#35206;&#30422;&#33539;&#22260;&#65292;&#20174;&#32780;&#20351;&#20854;&#23545;&#19981;&#24179;&#34913;&#38382;&#39064;&#26356;&#21152;&#20581;&#22766;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;MuJoCo&#22522;&#20934;&#27979;&#35797;&#21644;Atari&#28216;&#25103;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#29109;&#30340;&#25506;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A promising technique for exploration is to maximize the entropy of visited state distribution, i.e., state entropy, by encouraging uniform coverage of visited state space. While it has been effective for an unsupervised setup, it tends to struggle in a supervised setup with a task reward, where an agent prefers to visit high-value states to exploit the task reward. Such a preference can cause an imbalance between the distributions of high-value states and low-value states, which biases exploration towards low-value state regions as a result of the state entropy increasing when the distribution becomes more uniform. This issue is exacerbated when high-value states are narrowly distributed within the state space, making it difficult for the agent to complete the tasks. In this paper, we present a novel exploration technique that maximizes the value-conditional state entropy, which separately estimates the state entropies that are conditioned on the value estimates of each state, then ma
&lt;/p&gt;</description></item></channel></rss>