<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#30340;&#22240;&#26524;&#22270;&#21457;&#29616;&#65292;&#37319;&#29992;&#20102;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#32447;&#24615;&#25968;&#37327;&#30340;&#26597;&#35810;&#65292;&#21516;&#26102;&#33021;&#36731;&#26494;&#32467;&#21512;&#35266;&#23519;&#25968;&#25454;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#22312;&#30495;&#23454;&#22240;&#26524;&#22270;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#28508;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01207</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#22240;&#26524;&#22270;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Efficient Causal Graph Discovery Using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01207
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#30340;&#22240;&#26524;&#22270;&#21457;&#29616;&#65292;&#37319;&#29992;&#20102;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#32447;&#24615;&#25968;&#37327;&#30340;&#26597;&#35810;&#65292;&#21516;&#26102;&#33021;&#36731;&#26494;&#32467;&#21512;&#35266;&#23519;&#25968;&#25454;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#22312;&#30495;&#23454;&#22240;&#26524;&#22270;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;LLMs&#36827;&#34892;&#23436;&#25972;&#30340;&#22240;&#26524;&#22270;&#21457;&#29616;&#12290;&#20043;&#21069;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#25104;&#23545;&#26597;&#35810;&#30340;&#26041;&#27861;&#65292;&#20294;&#36825;&#38656;&#35201;&#20108;&#27425;&#26597;&#35810;&#30340;&#25968;&#37327;&#65292;&#23545;&#20110;&#36739;&#22823;&#30340;&#22240;&#26524;&#22270;&#26469;&#35828;&#24456;&#24555;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#30456;&#21453;&#65292;&#25552;&#20986;&#30340;&#26694;&#26550;&#37319;&#29992;&#20102;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#65288;BFS&#65289;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#32447;&#24615;&#25968;&#37327;&#30340;&#26597;&#35810;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#24403;&#26377;&#25152;&#35266;&#23519;&#25968;&#25454;&#21487;&#29992;&#26102;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#22320;&#36827;&#34892;&#32467;&#21512;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#38500;&#20102;&#26356;&#20855;&#26102;&#38388;&#21644;&#25968;&#25454;&#25928;&#29575;&#22806;&#65292;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#19981;&#21516;&#22823;&#23567;&#30340;&#30495;&#23454;&#22240;&#26524;&#22270;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#32467;&#26524;&#35777;&#26126;&#20102;&#25552;&#20986;&#26041;&#27861;&#22312;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#22240;&#26524;&#22270;&#21457;&#29616;&#20219;&#21153;&#20013;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel framework that leverages LLMs for full causal graph discovery. While previous LLM-based methods have used a pairwise query approach, this requires a quadratic number of queries which quickly becomes impractical for larger causal graphs. In contrast, the proposed framework uses a breadth-first search (BFS) approach which allows it to use only a linear number of queries. We also show that the proposed method can easily incorporate observational data when available, to improve performance. In addition to being more time and data-efficient, the proposed framework achieves state-of-the-art results on real-world causal graphs of varying sizes. The results demonstrate the effectiveness and efficiency of the proposed method in discovering causal relationships, showcasing its potential for broad applicability in causal graph discovery tasks across different domains.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32972;&#20809;&#22270;&#20687;&#22686;&#24378;&#30340;CLIP&#24341;&#23548;&#26041;&#27861;RAVE&#65292;&#36890;&#36807;&#27531;&#24046;&#21521;&#37327;&#23884;&#20837;&#21644;&#25552;&#31034;&#35843;&#25972;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#21152;&#24555;&#20102;&#35757;&#32451;&#24182;&#25552;&#39640;&#20102;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2404.01889</link><description>&lt;p&gt;
RAVE: CLIP&#24341;&#23548;&#30340;&#27531;&#24046;&#21521;&#37327;&#23884;&#20837;&#29992;&#20110;&#32972;&#20809;&#22270;&#20687;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
RAVE: Residual Vector Embedding for CLIP-Guided Backlit Image Enhancement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01889
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32972;&#20809;&#22270;&#20687;&#22686;&#24378;&#30340;CLIP&#24341;&#23548;&#26041;&#27861;RAVE&#65292;&#36890;&#36807;&#27531;&#24046;&#21521;&#37327;&#23884;&#20837;&#21644;&#25552;&#31034;&#35843;&#25972;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#21152;&#24555;&#20102;&#35757;&#32451;&#24182;&#25552;&#39640;&#20102;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#21453;&#24046;&#24322;&#24335;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#25351;&#23548;&#36827;&#34892;&#20102;&#26032;&#39062;&#20462;&#25913;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#32972;&#20809;&#22270;&#20687;&#22686;&#24378;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24314;&#31435;&#22312;&#26368;&#20808;&#36827;&#30340;CLIP-LIT&#26041;&#27861;&#22522;&#30784;&#20043;&#19978;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32422;&#26463;&#22312;CLIP&#23884;&#20837;&#31354;&#38388;&#20013;&#19968;&#20010;&#25552;&#31034;&#23545;&#20043;&#38388;&#30340;&#25991;&#26412;-&#22270;&#20687;&#30456;&#20284;&#24615;&#26469;&#23398;&#20064;&#19968;&#20010;&#25552;&#31034;&#23545;&#65288;&#36127;/&#27491;&#26679;&#26412;&#65289;&#21644;&#30456;&#24212;&#22270;&#20687;&#65288;&#32972;&#20809;&#22270;&#20687;/&#20809;&#29031;&#33391;&#22909;&#30340;&#22270;&#20687;&#65289;&#12290;&#23398;&#20064;&#30340;&#25552;&#31034;&#28982;&#21518;&#25351;&#23548;&#22270;&#20687;&#22686;&#24378;&#32593;&#32476;&#12290;&#22522;&#20110;CLIP-LIT&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;CLIP&#24341;&#23548;&#30340;&#26032;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#35843;&#25972;&#25552;&#31034;&#32780;&#19981;&#25439;&#22833;&#36136;&#37327;&#30340;&#21487;&#33021;&#24615;&#65292;&#20174;&#32780;&#21487;&#20197;&#30452;&#25509;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#35843;&#25972;&#23427;&#20204;&#30340;&#23884;&#20837;&#65292;&#21152;&#24555;&#35757;&#32451;&#24182;&#28508;&#22312;&#22320;&#23454;&#29616;&#20351;&#29992;&#27809;&#26377;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#20854;&#20182;&#32534;&#30721;&#22120;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20219;&#20309;&#25552;&#31034;&#35843;&#25972;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01889v1 Announce Type: cross  Abstract: In this paper we propose a novel modification of Contrastive Language-Image Pre-Training (CLIP) guidance for the task of unsupervised backlit image enhancement. Our work builds on the state-of-the-art CLIP-LIT approach, which learns a prompt pair by constraining the text-image similarity between a prompt (negative/positive sample) and a corresponding image (backlit image/well-lit image) in the CLIP embedding space. Learned prompts then guide an image enhancement network. Based on the CLIP-LIT framework, we propose two novel methods for CLIP guidance. First, we show that instead of tuning prompts in the space of text embeddings, it is possible to directly tune their embeddings in the latent space without any loss in quality. This accelerates training and potentially enables the use of additional encoders that do not have a text encoder. Second, we propose a novel approach that does not require any prompt tuning. Instead, based on CLIP e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934; ELITR-Bench&#65292;&#19987;&#27880;&#20110;&#38271;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#38469;&#20250;&#35758;&#21161;&#29702;&#22330;&#26223;&#65292;&#36890;&#36807;&#22312;&#29616;&#26377; ELITR &#35821;&#26009;&#24211;&#30340;&#36716;&#24405;&#20013;&#28155;&#21152;&#25163;&#24037;&#21046;&#20316;&#30340;&#38382;&#39064;&#21644;&#30495;&#23454;&#31572;&#26696;&#65292;&#25581;&#31034;&#20102;&#24320;&#28304;&#27169;&#22411;&#21644;&#19987;&#26377;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.20262</link><description>&lt;p&gt;
ELITR-Bench: &#38754;&#21521;&#38271;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#20250;&#35758;&#21161;&#29702;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20262
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934; ELITR-Bench&#65292;&#19987;&#27880;&#20110;&#38271;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#38469;&#20250;&#35758;&#21161;&#29702;&#22330;&#26223;&#65292;&#36890;&#36807;&#22312;&#29616;&#26377; ELITR &#35821;&#26009;&#24211;&#30340;&#36716;&#24405;&#20013;&#28155;&#21152;&#25163;&#24037;&#21046;&#20316;&#30340;&#38382;&#39064;&#21644;&#30495;&#23454;&#31572;&#26696;&#65292;&#25581;&#31034;&#20102;&#24320;&#28304;&#27169;&#22411;&#21644;&#19987;&#26377;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30740;&#31350;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#20027;&#35201;&#33268;&#21147;&#20110;&#25193;&#23637;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#38271;&#25991;&#26723;&#20869;&#37096;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;&#38271;&#36317;&#31163;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#20294;&#29616;&#26377;&#30340;&#21162;&#21147;&#20027;&#35201;&#32771;&#34385;&#30340;&#26159;&#19981;&#19968;&#23450;&#19982;&#29616;&#23454;&#24212;&#29992;&#30456;&#20851;&#30340;&#36890;&#29992;&#20219;&#21153;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#23454;&#38469;&#20250;&#35758;&#21161;&#29702;&#22330;&#26223;&#30340;&#38271;&#19978;&#19979;&#25991;LLMs&#30340;&#26032;&#22522;&#20934;&#12290;&#22312;&#36825;&#31181;&#24773;&#26223;&#19979;&#65292;&#38271;&#19978;&#19979;&#25991;&#30001;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#33719;&#24471;&#30340;&#36716;&#24405;&#32452;&#25104;&#65292;&#30001;&#20110;&#36825;&#20123;&#25968;&#25454;&#30340;&#22266;&#26377;&#22024;&#26434;&#24615;&#21644;&#21475;&#35821;&#29305;&#24615;&#65292;&#36825;&#20026;LLMs&#25552;&#20986;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#65292;&#21517;&#20026;ELITR-Bench&#65292;&#36890;&#36807;271&#20010;&#25163;&#24037;&#21046;&#20316;&#30340;&#38382;&#39064;&#21450;&#20854;&#30495;&#23454;&#31572;&#26696;&#26469;&#22686;&#24378;&#29616;&#26377;&#30340;ELITR&#35821;&#26009;&#24211;&#30340;&#36716;&#24405;&#12290;&#25105;&#20204;&#22312;ELITR-Bench&#19978;&#23545;&#26368;&#26032;&#30340;&#38271;&#19978;&#19979;&#25991;LLMs&#36827;&#34892;&#30340;&#23454;&#39564;&#20984;&#26174;&#20102;&#24320;&#28304;&#27169;&#22411;&#21644;&#19987;&#26377;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20262v1 Announce Type: cross  Abstract: Research on Large Language Models (LLMs) has recently witnessed an increasing interest in extending models' context size to better capture dependencies within long documents. While benchmarks have been proposed to assess long-range abilities, existing efforts primarily considered generic tasks that are not necessarily aligned with real-world applications. In contrast, our work proposes a new benchmark for long-context LLMs focused on a practical meeting assistant scenario. In this scenario, the long contexts consist of transcripts obtained by automatic speech recognition, presenting unique challenges for LLMs due to the inherent noisiness and oral nature of such data. Our benchmark, named ELITR-Bench, augments the existing ELITR corpus' transcripts with 271 manually crafted questions and their ground-truth answers. Our experiments with recent long-context LLMs on ELITR-Bench highlight a gap between open-source and proprietary models, e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#20004;&#20010;LLM&#30340;&#21103;&#26412;&#19982;&#39564;&#35777;&#22120;&#32467;&#21512;&#20351;&#29992;&#65292;&#33021;&#22815;&#33258;&#21160;&#35780;&#20272;&#20854;&#22312;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#27491;&#24335;&#35268;&#33539;&#20043;&#38388;&#36716;&#25442;&#30340;&#33021;&#21147;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#20154;&#24037;&#36755;&#20837;&#12290;</title><link>https://arxiv.org/abs/2403.18327</link><description>&lt;p&gt;
LLM&#21487;&#20197;&#36827;&#34892;&#27491;&#24335;&#23545;&#35805;&#21527;&#65311;&#33258;&#21160;&#35780;&#20272;LLMs&#22312;&#36716;&#25442;&#21644;&#35299;&#37322;&#27491;&#24335;&#35268;&#33539;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Converse Formally? Automatically Assessing LLMs in Translating and Interpreting Formal Specifications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#20004;&#20010;LLM&#30340;&#21103;&#26412;&#19982;&#39564;&#35777;&#22120;&#32467;&#21512;&#20351;&#29992;&#65292;&#33021;&#22815;&#33258;&#21160;&#35780;&#20272;&#20854;&#22312;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#27491;&#24335;&#35268;&#33539;&#20043;&#38388;&#36716;&#25442;&#30340;&#33021;&#21147;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#20154;&#24037;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#30410;&#30456;&#20851;&#32773;&#32463;&#24120;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#31995;&#32479;&#38656;&#27714;&#65292;&#28982;&#21518;&#30001;&#39046;&#22495;&#19987;&#23478;&#23558;&#20854;&#36716;&#25442;&#20026;&#24418;&#24335;&#21270;&#35821;&#27861;&#65292;&#20174;&#32780;&#22686;&#21152;&#35774;&#35745;&#25104;&#26412;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#27491;&#24335;&#35268;&#33539;&#20043;&#38388;&#36716;&#25442;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#20004;&#20010;LLM&#30340;&#21103;&#26412;&#19982;&#29616;&#25104;&#30340;&#39564;&#35777;&#22120;&#32467;&#21512;&#20351;&#29992;&#65292;&#26080;&#38656;&#20219;&#20309;&#39069;&#22806;&#30340;&#20154;&#24037;&#36755;&#20837;&#23601;&#21487;&#20197;&#33258;&#21160;&#35780;&#20272;&#20854;&#32763;&#35793;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#35821;&#35328;&#35821;&#27861;&#29983;&#25104;&#24418;&#24335;&#21270;&#35821;&#27861;&#65292;&#33258;&#21160;&#29983;&#25104;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#32463;&#39564;&#35780;&#20272;&#20197;&#34913;&#37327;&#36825;&#31181;&#32763;&#35793;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18327v1 Announce Type: cross  Abstract: Stakeholders often describe system requirements using natural language which are then converted to formal syntax by a domain-expert leading to increased design costs. This paper assesses the capabilities of Large Language Models (LLMs) in converting between natural language descriptions and formal specifications. Existing work has evaluated the capabilities of LLMs in generating formal syntax such as source code but such experiments are typically hand-crafted and use problems that are likely to be in the training set of LLMs, and often require human-annotated datasets. We propose an approach that can use two copies of an LLM in conjunction with an off-the-shelf verifier to automatically evaluate its translation abilities without any additional human input. Our approach generates formal syntax using language grammars to automatically generate a dataset. We conduct an empirical evaluation to measure the accuracy of this translation task 
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#22312;&#25552;&#39640;&#26893;&#20837;&#24335;&#21548;&#35273;&#35774;&#22791;&#30340;&#35821;&#38899;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#21069;&#30651;&#24615;&#65292;&#24182;&#36890;&#36807;&#20808;&#36827;&#30340;&#20449;&#21495;&#22788;&#29702;&#25216;&#26415;&#20197;&#21450;&#24212;&#23545;&#22810;&#28304;&#35821;&#38899;&#21644;&#29615;&#22659;&#22122;&#38899;&#25361;&#25112;&#31561;&#26041;&#27861;&#26469;&#20811;&#26381;&#35821;&#38899;&#22833;&#30495;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.15442</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#32819;&#34583;&#26893;&#20837;&#35013;&#32622;&#20013;&#30340;&#20808;&#36827;&#31639;&#27861;&#65306;&#21307;&#30103;&#31574;&#30053;&#12289;&#25361;&#25112;&#21644;&#23637;&#26395;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Advanced Artificial Intelligence Algorithms in Cochlear Implants: Review of Healthcare Strategies, Challenges, and Perspectives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15442
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#25552;&#39640;&#26893;&#20837;&#24335;&#21548;&#35273;&#35774;&#22791;&#30340;&#35821;&#38899;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#21069;&#30651;&#24615;&#65292;&#24182;&#36890;&#36807;&#20808;&#36827;&#30340;&#20449;&#21495;&#22788;&#29702;&#25216;&#26415;&#20197;&#21450;&#24212;&#23545;&#22810;&#28304;&#35821;&#38899;&#21644;&#29615;&#22659;&#22122;&#38899;&#25361;&#25112;&#31561;&#26041;&#27861;&#26469;&#20811;&#26381;&#35821;&#38899;&#22833;&#30495;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15442v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#22312;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#19981;&#20165;&#20026;&#19982;&#26426;&#22120;&#20132;&#20114;&#25552;&#20379;&#20102;&#20415;&#21033;&#65292;&#36824;&#20026;&#37096;&#20998;&#25110;&#23436;&#20840;&#21548;&#21147;&#21463;&#25439;&#30340;&#20010;&#20307;&#25552;&#20379;&#20102;&#27807;&#36890;&#30340;&#26426;&#20250;&#12290;&#36825;&#19968;&#36807;&#31243;&#28041;&#21450;&#20197;&#27169;&#25311;&#24418;&#24335;&#25509;&#25910;&#35821;&#38899;&#20449;&#21495;&#65292;&#28982;&#21518;&#36890;&#36807;&#21508;&#31181;&#20449;&#21495;&#22788;&#29702;&#31639;&#27861;&#20351;&#20854;&#19982;&#23481;&#37327;&#26377;&#38480;&#30340;&#35774;&#22791;&#65288;&#22914;CI&#65289;&#20860;&#23481;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#37197;&#22791;&#26377;&#26377;&#38480;&#25968;&#37327;&#30005;&#26497;&#30340;&#26893;&#20837;&#35013;&#32622;&#22312;&#21512;&#25104;&#36807;&#31243;&#20013;&#24448;&#24448;&#23548;&#33268;&#35821;&#38899;&#22833;&#30495;&#12290;&#23613;&#31649;&#30740;&#31350;&#20154;&#21592;&#22312;&#20351;&#29992;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#20449;&#21495;&#22788;&#29702;&#25216;&#26415;&#25913;&#21892;&#25509;&#25910;&#21040;&#30340;&#35821;&#38899;&#36136;&#37327;&#26041;&#38754;&#20570;&#20986;&#20102;&#21162;&#21147;&#65292;&#20294;&#22312;&#28041;&#21450;&#22810;&#20010;&#35821;&#38899;&#28304;&#12289;&#29615;&#22659;&#22122;&#22768;&#21644;&#20854;&#20182;&#24773;&#20917;&#30340;&#22330;&#26223;&#20013;&#65292;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#12290;&#26032;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#27861;&#30340;&#20986;&#29616;&#24341;&#20837;&#20102;&#20808;&#36827;&#30340;&#31574;&#30053;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15442v1 Announce Type: cross  Abstract: Automatic speech recognition (ASR) plays a pivotal role in our daily lives, offering utility not only for interacting with machines but also for facilitating communication for individuals with either partial or profound hearing impairments. The process involves receiving the speech signal in analogue form, followed by various signal processing algorithms to make it compatible with devices of limited capacity, such as cochlear implants (CIs). Unfortunately, these implants, equipped with a finite number of electrodes, often result in speech distortion during synthesis. Despite efforts by researchers to enhance received speech quality using various state-of-the-art signal processing techniques, challenges persist, especially in scenarios involving multiple sources of speech, environmental noise, and other circumstances. The advent of new artificial intelligence (AI) methods has ushered in cutting-edge strategies to address the limitations
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24182;&#34892;&#35745;&#21010;&#30340;SAT&#32534;&#30721;&#65292;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#24212;&#29992;1&#20010;SWAP&#21644;&#19968;&#32452;CNOT&#12290;&#21033;&#29992;&#29305;&#23450;&#39046;&#22495;&#20449;&#24687;&#65292;&#22312;&#24182;&#34892;&#35745;&#21010;&#20013;&#20445;&#25345;&#20248;&#21270;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#25193;&#23637;&#21040;&#22823;&#22411;&#28145;&#24230;&#30005;&#36335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#22312;&#22810;&#39033;&#27604;&#39046;&#20808;&#30340;&#31934;&#30830;&#21644;&#25509;&#36817;&#26368;&#20248;&#26041;&#27861;&#34920;&#29616;&#20986;&#33394;&#65288;&#39640;&#36798;100&#20493;&#65289;&#12290;&#25105;&#20204;&#39318;&#27425;&#33021;&#22815;&#26368;&#20248;&#22320;&#26144;&#23556;&#22810;&#20010;8&#12289;14&#21644;16&#37327;&#23376;&#27604;&#29305;</title><link>https://arxiv.org/abs/2403.11598</link><description>&lt;p&gt;
&#38754;&#21521;&#20855;&#26377;100&#22810;&#20010;&#37327;&#23376;&#27604;&#29305;&#30340;NISQ&#22788;&#29702;&#22120;&#30340;&#28145;&#24230;&#37327;&#23376;&#30005;&#36335;&#26368;&#20339;&#24067;&#23616;&#32508;&#21512;
&lt;/p&gt;
&lt;p&gt;
Optimal Layout Synthesis for Deep Quantum Circuits on NISQ Processors with 100+ Qubits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24182;&#34892;&#35745;&#21010;&#30340;SAT&#32534;&#30721;&#65292;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#24212;&#29992;1&#20010;SWAP&#21644;&#19968;&#32452;CNOT&#12290;&#21033;&#29992;&#29305;&#23450;&#39046;&#22495;&#20449;&#24687;&#65292;&#22312;&#24182;&#34892;&#35745;&#21010;&#20013;&#20445;&#25345;&#20248;&#21270;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#25193;&#23637;&#21040;&#22823;&#22411;&#28145;&#24230;&#30005;&#36335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#22312;&#22810;&#39033;&#27604;&#39046;&#20808;&#30340;&#31934;&#30830;&#21644;&#25509;&#36817;&#26368;&#20248;&#26041;&#27861;&#34920;&#29616;&#20986;&#33394;&#65288;&#39640;&#36798;100&#20493;&#65289;&#12290;&#25105;&#20204;&#39318;&#27425;&#33021;&#22815;&#26368;&#20248;&#22320;&#26144;&#23556;&#22810;&#20010;8&#12289;14&#21644;16&#37327;&#23376;&#27604;&#29305;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24067;&#23616;&#32508;&#21512;&#26159;&#23558;&#37327;&#23376;&#30005;&#36335;&#26144;&#23556;&#21040;&#37327;&#23376;&#22788;&#29702;&#22120;&#12290;&#38656;&#35201;&#20026;&#20165;&#22312;&#36830;&#25509;&#30340;&#29289;&#29702;&#37327;&#23376;&#27604;&#29305;&#19978;&#23433;&#25490;2&#27604;&#29305;&#38376;&#30340;&#35843;&#24230;&#36827;&#34892;SWAP&#38376;&#25554;&#20837;&#12290;&#38543;&#30528;NISQ&#22788;&#29702;&#22120;&#20013;&#37327;&#23376;&#27604;&#29305;&#25968;&#37327;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#21487;&#25193;&#23637;&#30340;&#24067;&#23616;&#32508;&#21512;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#21551;&#21457;&#24335;&#26041;&#27861;&#20013;&#35266;&#23519;&#21040;&#30340;&#20248;&#21270;&#38388;&#38553;&#36739;&#22823;&#65292;&#38656;&#35201;&#21487;&#25193;&#23637;&#30340;&#31934;&#30830;&#26041;&#27861;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#31934;&#30830;&#21644;&#25509;&#36817;&#26368;&#20248;&#26041;&#27861;&#36866;&#29992;&#20110;&#20013;&#31561;&#35268;&#27169;&#30005;&#36335;&#65292;&#20294;&#22823;&#22411;&#28145;&#24230;&#30005;&#36335;&#20173;&#36229;&#20986;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11598v1 Announce Type: cross  Abstract: Layout synthesis is mapping a quantum circuit to a quantum processor. SWAP gate insertions are needed for scheduling 2-qubit gates only on connected physical qubits. With the ever-increasing number of qubits in NISQ processors, scalable layout synthesis is of utmost importance. With large optimality gaps observed in heuristic approaches, scalable exact methods are needed. While recent exact and near-optimal approaches scale to moderate circuits, large deep circuits are still out of scope.   In this work, we propose a SAT encoding based on parallel plans that apply 1 SWAP and a group of CNOTs at each time step. Using domain-specific information, we maintain optimality in parallel plans while scaling to large and deep circuits. From our results, we show the scalability of our approach which significantly outperforms leading exact and near-optimal approaches (up to 100x). For the first time, we can optimally map several 8, 14, and 16 qubi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31038;&#20250;&#25972;&#21512;&#23548;&#33322;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#20154;&#30340;&#20114;&#21160;&#20351;&#26426;&#22120;&#20154;&#30340;&#31038;&#20132;&#34892;&#20026;&#33258;&#36866;&#24212;&#65292;&#24182;&#20174;&#20854;&#20182;&#22522;&#20110;DRL&#30340;&#23548;&#33322;&#26041;&#27861;&#20013;&#21306;&#20998;&#20986;&#20855;&#26377;&#26126;&#30830;&#39044;&#23450;&#20041;&#31038;&#20132;&#34892;&#20026;&#30340;&#31038;&#20250;&#24847;&#35782;&#26041;&#27861;&#21644;&#32570;&#20047;&#31038;&#20132;&#34892;&#20026;&#30340;&#31038;&#20250;&#30896;&#25758;&#22238;&#36991;&#12290;</title><link>https://arxiv.org/abs/2403.09793</link><description>&lt;p&gt;
&#31038;&#20250;&#25972;&#21512;&#23548;&#33322;&#65306;&#20855;&#26377;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31038;&#20132;&#34892;&#21160;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Socially Integrated Navigation: A Social Acting Robot with Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09793
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31038;&#20250;&#25972;&#21512;&#23548;&#33322;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#20154;&#30340;&#20114;&#21160;&#20351;&#26426;&#22120;&#20154;&#30340;&#31038;&#20132;&#34892;&#20026;&#33258;&#36866;&#24212;&#65292;&#24182;&#20174;&#20854;&#20182;&#22522;&#20110;DRL&#30340;&#23548;&#33322;&#26041;&#27861;&#20013;&#21306;&#20998;&#20986;&#20855;&#26377;&#26126;&#30830;&#39044;&#23450;&#20041;&#31038;&#20132;&#34892;&#20026;&#30340;&#31038;&#20250;&#24847;&#35782;&#26041;&#27861;&#21644;&#32570;&#20047;&#31038;&#20132;&#34892;&#20026;&#30340;&#31038;&#20250;&#30896;&#25758;&#22238;&#36991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#26426;&#22120;&#20154;&#27491;&#22312;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#25317;&#25380;&#22330;&#26223;&#65292;&#24182;&#25104;&#20026;&#25105;&#20204;&#31038;&#20250;&#30340;&#19968;&#37096;&#20998;&#12290;&#19968;&#20010;&#20855;&#26377;&#20010;&#20307;&#20154;&#31867;&#32771;&#34385;&#30340;&#31038;&#20250;&#21487;&#25509;&#21463;&#30340;&#23548;&#33322;&#34892;&#20026;&#23545;&#20110;&#21487;&#25193;&#23637;&#30340;&#24212;&#29992;&#21644;&#20154;&#31867;&#25509;&#21463;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26041;&#27861;&#26469;&#23398;&#20064;&#26426;&#22120;&#20154;&#30340;&#23548;&#33322;&#31574;&#30053;&#65292;&#24182;&#23545;&#26426;&#22120;&#20154;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#22797;&#26434;&#20132;&#20114;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#24314;&#35758;&#26681;&#25454;&#26426;&#22120;&#20154;&#23637;&#31034;&#30340;&#31038;&#20132;&#34892;&#20026;&#23558;&#29616;&#26377;&#22522;&#20110;DRL&#30340;&#23548;&#33322;&#26041;&#27861;&#20998;&#20026;&#20855;&#26377;&#32570;&#20047;&#31038;&#20132;&#34892;&#20026;&#30340;&#31038;&#20250;&#30896;&#25758;&#22238;&#36991;&#21644;&#20855;&#26377;&#26126;&#30830;&#39044;&#23450;&#20041;&#31038;&#20132;&#34892;&#20026;&#30340;&#31038;&#20250;&#24847;&#35782;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31038;&#20250;&#25972;&#21512;&#23548;&#33322;&#26041;&#27861;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#30340;&#31038;&#20132;&#34892;&#20026;&#26159;&#33258;&#36866;&#24212;&#30340;&#65292;&#24182;&#19988;&#26159;&#36890;&#36807;&#19982;&#20154;&#31867;&#30340;&#20114;&#21160;&#32780;&#20135;&#29983;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26500;&#24335;&#28304;&#33258;&#31038;&#20250;&#23398;&#23450;&#20041;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09793v1 Announce Type: cross  Abstract: Mobile robots are being used on a large scale in various crowded situations and become part of our society. The socially acceptable navigation behavior of a mobile robot with individual human consideration is an essential requirement for scalable applications and human acceptance. Deep Reinforcement Learning (DRL) approaches are recently used to learn a robot's navigation policy and to model the complex interactions between robots and humans. We propose to divide existing DRL-based navigation approaches based on the robot's exhibited social behavior and distinguish between social collision avoidance with a lack of social behavior and socially aware approaches with explicit predefined social behavior. In addition, we propose a novel socially integrated navigation approach where the robot's social behavior is adaptive and emerges from the interaction with humans. The formulation of our approach is derived from a sociological definition, 
&lt;/p&gt;</description></item><item><title>&#23558;&#21435;&#22122;&#26041;&#27861;&#25512;&#24191;&#21040;&#38750;&#24179;&#34913;&#32467;&#26500;&#65292;&#20174;&#32780;&#25913;&#36827;&#31561;&#21464;&#21147;&#22330;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;&#23545;&#21407;&#23376;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#29702;&#35299;&#20197;&#21450;&#22312;&#20998;&#23376;&#21160;&#21147;&#23398;&#21644;&#20652;&#21270;&#21058;&#35774;&#35745;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.09549</link><description>&lt;p&gt;
&#23558;&#21435;&#22122;&#25512;&#24191;&#21040;&#38750;&#24179;&#34913;&#32467;&#26500;&#20197;&#25913;&#36827;&#31561;&#21464;&#21147;&#22330;
&lt;/p&gt;
&lt;p&gt;
Generalizing Denoising to Non-Equilibrium Structures Improves Equivariant Force Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09549
&lt;/p&gt;
&lt;p&gt;
&#23558;&#21435;&#22122;&#26041;&#27861;&#25512;&#24191;&#21040;&#38750;&#24179;&#34913;&#32467;&#26500;&#65292;&#20174;&#32780;&#25913;&#36827;&#31561;&#21464;&#21147;&#22330;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;&#23545;&#21407;&#23376;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#29702;&#35299;&#20197;&#21450;&#22312;&#20998;&#23376;&#21160;&#21147;&#23398;&#21644;&#20652;&#21270;&#21058;&#35774;&#35745;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21407;&#23376;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#22914;3D&#21407;&#23376;&#20307;&#31995;&#20013;&#30340;&#21147;&#65292;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#22914;&#20998;&#23376;&#21160;&#21147;&#23398;&#21644;&#20652;&#21270;&#21058;&#35774;&#35745;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#27169;&#25311;&#36825;&#20123;&#30456;&#20114;&#20316;&#29992;&#38656;&#35201;&#35745;&#31639;&#23494;&#38598;&#30340;&#20174;&#22836;&#31639;&#35745;&#31639;&#65292;&#22240;&#27492;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#21435;&#22122;&#38750;&#24179;&#34913;&#32467;&#26500;&#65288;DeNS&#65289;&#20316;&#20026;&#36741;&#21161;&#20219;&#21153;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;&#22312;&#20351;&#29992;DeNS&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#21521;&#20854;3D&#22352;&#26631;&#28155;&#21152;&#22122;&#22768;&#26469;&#30772;&#22351;3D&#32467;&#26500;&#65292;&#28982;&#21518;&#39044;&#27979;&#22122;&#22768;&#12290;&#19981;&#21516;&#20110;&#20197;&#24448;&#20165;&#38480;&#20110;&#24179;&#34913;&#32467;&#26500;&#30340;&#21435;&#22122;&#24037;&#20316;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#21435;&#22122;&#27867;&#21270;&#21040;&#26356;&#22823;&#33539;&#22260;&#30340;&#38750;&#24179;&#34913;&#32467;&#26500;&#12290;&#20027;&#35201;&#21306;&#21035;&#22312;&#20110;&#38750;&#24179;&#34913;&#32467;&#26500;&#19981;&#23545;&#24212;&#20110;&#23616;&#37096;&#33021;&#37327;&#26368;&#23567;&#20540;&#65292;&#20855;&#26377;&#38750;&#38646;&#21147;&#65292;&#22240;&#27492;&#21487;&#33021;&#20855;&#26377;&#35768;&#22810;&#21487;&#33021;&#30340;&#21407;&#23376;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09549v1 Announce Type: cross  Abstract: Understanding the interactions of atoms such as forces in 3D atomistic systems is fundamental to many applications like molecular dynamics and catalyst design. However, simulating these interactions requires compute-intensive ab initio calculations and thus results in limited data for training neural networks. In this paper, we propose to use denoising non-equilibrium structures (DeNS) as an auxiliary task to better leverage training data and improve performance. For training with DeNS, we first corrupt a 3D structure by adding noise to its 3D coordinates and then predict the noise. Different from previous works on denoising, which are limited to equilibrium structures, the proposed method generalizes denoising to a much larger set of non-equilibrium structures. The main difference is that a non-equilibrium structure does not correspond to local energy minima and has non-zero forces, and therefore it can have many possible atomic posit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#35775;&#38382;&#20869;&#23384;&#26102;&#30340;&#25361;&#25112;&#65292;&#21457;&#29616;&#36890;&#36807;&#32972;&#35829;&#21644;&#32622;&#25442;&#31561;&#25216;&#26415;&#21487;&#20197;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#30340;&#38543;&#26426;&#20869;&#23384;&#35775;&#38382;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#24320;&#25918;&#22495;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.07805</link><description>&lt;p&gt;
&#36229;&#36234;&#27515;&#35760;&#30828;&#32972;&#65306;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38543;&#26426;&#20869;&#23384;&#35775;&#38382;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Beyond Memorization: The Challenge of Random Memory Access in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#35775;&#38382;&#20869;&#23384;&#26102;&#30340;&#25361;&#25112;&#65292;&#21457;&#29616;&#36890;&#36807;&#32972;&#35829;&#21644;&#32622;&#25442;&#31561;&#25216;&#26415;&#21487;&#20197;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#30340;&#38543;&#26426;&#20869;&#23384;&#35775;&#38382;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#24320;&#25918;&#22495;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;(LMs)&#30340;&#21457;&#23637;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;NLP&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#20854;&#21442;&#25968;&#20869;&#37096;&#30340;&#30693;&#35782;&#23384;&#20648;&#21644;&#20869;&#23384;&#35775;&#38382;&#26426;&#21046;&#20173;&#28982;&#20196;&#20154;&#36153;&#35299;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT-2&#65289;&#26159;&#21542;&#33021;&#22815;&#39034;&#24207;&#25110;&#38543;&#26426;&#22320;&#35775;&#38382;&#20854;&#20869;&#23384;&#12290;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#21512;&#25104;&#20219;&#21153;&#65292;&#28085;&#30422;&#20840;&#38754;&#32972;&#35829;&#12289;&#36873;&#25321;&#24615;&#32972;&#35829;&#21644;&#22522;&#20110;&#38382;&#39064;&#22238;&#31572;&#30340;&#24773;&#26223;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;LMs&#33021;&#22815;&#39034;&#24207;&#35775;&#38382;&#20854;&#20869;&#23384;&#65292;&#21516;&#26102;&#22312;&#38543;&#26426;&#35775;&#38382;&#24050;&#35760;&#24518;&#20869;&#23481;&#26102;&#36935;&#21040;&#25361;&#25112;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#32972;&#35829;&#21644;&#32622;&#25442;&#31561;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;LMs&#30340;&#38543;&#26426;&#20869;&#23384;&#35775;&#38382;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23558;&#36825;&#31181;&#24178;&#39044;&#24212;&#29992;&#20110;&#24320;&#25918;&#22495;&#38382;&#39064;&#22238;&#31572;&#30340;&#29616;&#23454;&#22330;&#26223;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#36890;&#36807;&#32972;&#35829;&#26469;&#22686;&#24378;&#38543;&#26426;&#35775;&#38382;&#25216;&#26415;&#23545;&#38382;&#39064;&#22238;&#31572;&#33021;&#21147;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07805v1 Announce Type: cross  Abstract: Recent developments in Language Models (LMs) have shown their effectiveness in NLP tasks, particularly in knowledge-intensive tasks. However, the mechanisms underlying knowledge storage and memory access within their parameters remain elusive. In this paper, we investigate whether a generative LM (e.g., GPT-2) is able to access its memory sequentially or randomly. Through carefully-designed synthetic tasks, covering the scenarios of full recitation, selective recitation and grounded question answering, we reveal that LMs manage to sequentially access their memory while encountering challenges in randomly accessing memorized content. We find that techniques including recitation and permutation improve the random memory access capability of LMs. Furthermore, by applying this intervention to realistic scenarios of open-domain question answering, we validate that enhancing random access by recitation leads to notable improvements in questi
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#22312;&#36890;&#36807;web&#27983;&#35272;&#22120;&#19982;&#36719;&#20214;&#20132;&#20114;&#26102;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;WorkArena&#21644;BrowserGym&#20004;&#20010;&#24037;&#20855;&#65292;&#22312;29&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#20063;&#25581;&#31034;&#20102;&#23454;&#29616;&#23436;&#20840;&#20219;&#21153;&#33258;&#21160;&#21270;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.07718</link><description>&lt;p&gt;
WorkArena&#65306;Web&#20195;&#29702;&#22312;&#35299;&#20915;&#24120;&#35265;&#30693;&#35782;&#24037;&#20316;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work Tasks?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07718
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#22312;&#36890;&#36807;web&#27983;&#35272;&#22120;&#19982;&#36719;&#20214;&#20132;&#20114;&#26102;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;WorkArena&#21644;BrowserGym&#20004;&#20010;&#24037;&#20855;&#65292;&#22312;29&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#20063;&#25581;&#31034;&#20102;&#23454;&#29616;&#23436;&#20840;&#20219;&#21153;&#33258;&#21160;&#21270;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#19982;&#36719;&#20214;&#36890;&#36807;web&#27983;&#35272;&#22120;&#20132;&#20114;&#30340;&#24212;&#29992;&#12290;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#20851;&#27880;&#34913;&#37327;&#36825;&#20123;&#20195;&#29702;&#25191;&#34892;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#36825;&#20123;&#20219;&#21153;&#28085;&#30422;&#20102;&#21033;&#29992;&#20225;&#19994;&#36719;&#20214;&#31995;&#32479;&#30340;&#30693;&#35782;&#24037;&#20316;&#32773;&#30340;&#20856;&#22411;&#26085;&#24120;&#24037;&#20316;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;WorkArena&#65292;&#19968;&#20010;&#22522;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;ServiceNow&#24179;&#21488;&#30340;29&#20010;&#20219;&#21153;&#30340;&#36828;&#31243;&#20027;&#26426;&#22522;&#20934;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;BrowserGym&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35774;&#35745;&#21644;&#35780;&#20272;&#36825;&#20123;&#20195;&#29702;&#30340;&#29615;&#22659;&#65292;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#34892;&#20026;&#21644;&#22810;&#27169;&#24577;&#35266;&#23519;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#26174;&#31034;&#65292;&#23613;&#31649;&#24403;&#21069;&#30340;&#20195;&#29702;&#22312;WorkArena&#19978;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#35201;&#23454;&#29616;&#23436;&#20840;&#20219;&#21153;&#33258;&#21160;&#21270;&#20173;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#24046;&#36317;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#24320;&#28304;&#21644;&#38381;&#28304;LLMs&#20043;&#38388;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#31361;&#20986;&#20102;&#26410;&#26469;&#25506;&#32034;&#21644;&#21457;&#23637;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07718v1 Announce Type: cross  Abstract: We study the use of large language model-based agents for interacting with software via web browsers. Unlike prior work, we focus on measuring the agents' ability to perform tasks that span the typical daily work of knowledge workers utilizing enterprise software systems. To this end, we propose WorkArena, a remote-hosted benchmark of 29 tasks based on the widely-used ServiceNow platform. We also introduce BrowserGym, an environment for the design and evaluation of such agents, offering a rich set of actions as well as multimodal observations. Our empirical evaluation reveals that while current agents show promise on WorkArena, there remains a considerable gap towards achieving full task automation. Notably, our analysis uncovers a significant performance disparity between open and closed-source LLMs, highlighting a critical area for future exploration and development in the field.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#20041;&#22270;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65292;&#21033;&#29992;GNN&#26469;&#36827;&#34892;&#39640;&#25928;&#30340;&#22270;&#32534;&#36753;&#36317;&#31163;&#35745;&#31639;&#65292;&#36890;&#36807;&#22330;&#26223;&#22270;&#24418;&#24335;&#65292;&#32469;&#36807;NP&#22256;&#38590;&#30340;&#22270;&#30456;&#20284;&#24615;&#38382;&#39064;&#65292;&#23454;&#29616;&#26356;&#20855;&#25551;&#36848;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#19982;&#20154;&#31867;&#23545;&#40784;&#30340;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2403.06514</link><description>&lt;p&gt;
&#26500;&#24314;&#25968;&#25454;&#32467;&#26500;&#65306;&#36208;&#21521;&#35821;&#20041;&#22270;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Structure Your Data: Towards Semantic Graph Counterfactuals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06514
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#20041;&#22270;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65292;&#21033;&#29992;GNN&#26469;&#36827;&#34892;&#39640;&#25928;&#30340;&#22270;&#32534;&#36753;&#36317;&#31163;&#35745;&#31639;&#65292;&#36890;&#36807;&#22330;&#26223;&#22270;&#24418;&#24335;&#65292;&#32469;&#36807;NP&#22256;&#38590;&#30340;&#22270;&#30456;&#20284;&#24615;&#38382;&#39064;&#65292;&#23454;&#29616;&#26356;&#20855;&#25551;&#36848;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#19982;&#20154;&#31867;&#23545;&#40784;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27010;&#24565;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;CEs&#65289;&#26159;&#32771;&#34385;&#26367;&#20195;&#24773;&#26223;&#20197;&#20102;&#35299;&#21738;&#20123;&#39640;&#32423;&#35821;&#20041;&#29305;&#24449;&#23545;&#29305;&#23450;&#27169;&#22411;&#39044;&#27979;&#20570;&#20986;&#20102;&#36129;&#29486;&#30340;&#35299;&#37322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20276;&#38543;&#36755;&#20837;&#25968;&#25454;&#30340;&#35821;&#20041;&#22270;&#30340;CEs&#65292;&#20197;&#23454;&#29616;&#26356;&#20855;&#25551;&#36848;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#19982;&#20154;&#31867;&#23545;&#40784;&#30340;&#35299;&#37322;&#12290;&#20511;&#37492;&#26368;&#20808;&#36827;&#30340;&#27010;&#24565;&#23581;&#35797;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;&#32534;&#36753;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#21033;&#29992;GNN&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#22270;&#32534;&#36753;&#36317;&#31163;&#65288;GED&#65289;&#35745;&#31639;&#12290;&#25105;&#20204;&#23558;&#22270;&#24418;&#32467;&#26500;&#29992;&#20110;&#35270;&#35273;&#39046;&#22495;&#65292;&#23558;&#22270;&#20687;&#34920;&#31034;&#20026;&#22330;&#26223;&#22270;&#65292;&#24182;&#33719;&#24471;&#23427;&#20204;&#30340;GNN&#23884;&#20837;&#20197;&#32469;&#36807;&#35299;&#20915;&#25152;&#26377;&#36755;&#20837;&#23545;&#30340;NP&#22256;&#38590;&#22270;&#30456;&#20284;&#24615;&#38382;&#39064;&#65292;&#36825;&#26159;CE&#35745;&#31639;&#36807;&#31243;&#30340;&#19968;&#20010;&#37325;&#35201;&#37096;&#20998;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#20855;&#26377;&#19981;&#21516;&#38590;&#24230;&#21644;&#35821;&#20041;&#27880;&#37322;&#21487;&#29992;&#24615;&#30340;&#22522;&#20934;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#12290;&#22312;&#21508;&#31181;&#20998;&#31867;&#22120;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;CEs&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06514v1 Announce Type: cross  Abstract: Counterfactual explanations (CEs) based on concepts are explanations that consider alternative scenarios to understand which high-level semantic features contributed to particular model predictions. In this work, we propose CEs based on the semantic graphs accompanying input data to achieve more descriptive, accurate, and human-aligned explanations. Building upon state-of-the-art (SoTA) conceptual attempts, we adopt a model-agnostic edit-based approach and introduce leveraging GNNs for efficient Graph Edit Distance (GED) computation. With a focus on the visual domain, we represent images as scene graphs and obtain their GNN embeddings to bypass solving the NP-hard graph similarity problem for all input pairs, an integral part of the CE computation process. We apply our method to benchmark and real-world datasets with varying difficulty and availability of semantic annotations. Testing on diverse classifiers, we find that our CEs outper
&lt;/p&gt;</description></item><item><title>xT&#20026;&#35270;&#35273;Transformer&#24341;&#20837;&#20102;&#23884;&#22871;&#26631;&#35760;&#21270;&#26041;&#26696;&#65292;&#26377;&#25928;&#22320;&#32858;&#21512;&#20102;&#20840;&#23616;&#32972;&#26223;&#21644;&#23616;&#37096;&#32454;&#33410;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#29616;&#20195;GPU&#19978;&#31471;&#21040;&#31471;&#22320;&#24314;&#27169;&#22823;&#22270;&#20687;&#65292;&#24182;&#22312;&#32463;&#20856;&#35270;&#35273;&#20219;&#21153;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.01915</link><description>&lt;p&gt;
xT&#65306;&#29992;&#20110;&#22823;&#22270;&#20687;&#20013;&#26356;&#22823;&#19978;&#19979;&#25991;&#30340;&#23884;&#22871;&#26631;&#35760;&#21270;
&lt;/p&gt;
&lt;p&gt;
xT: Nested Tokenization for Larger Context in Large Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01915
&lt;/p&gt;
&lt;p&gt;
xT&#20026;&#35270;&#35273;Transformer&#24341;&#20837;&#20102;&#23884;&#22871;&#26631;&#35760;&#21270;&#26041;&#26696;&#65292;&#26377;&#25928;&#22320;&#32858;&#21512;&#20102;&#20840;&#23616;&#32972;&#26223;&#21644;&#23616;&#37096;&#32454;&#33410;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#29616;&#20195;GPU&#19978;&#31471;&#21040;&#31471;&#22320;&#24314;&#27169;&#22823;&#22270;&#20687;&#65292;&#24182;&#22312;&#32463;&#20856;&#35270;&#35273;&#20219;&#21153;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35745;&#31639;&#26426;&#35270;&#35273;&#27969;&#27700;&#32447;&#20197;&#20004;&#31181;&#27425;&#20248;&#26041;&#24335;&#22788;&#29702;&#22823;&#22270;&#20687;&#65306;&#19979;&#37319;&#26679;&#25110;&#35009;&#21098;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#23548;&#33268;&#22270;&#20687;&#20013;&#20449;&#24687;&#21644;&#32972;&#26223;&#30340;&#20002;&#22833;&#12290;&#22312;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#20013;&#65292;&#20840;&#23616;&#32972;&#26223;&#30340;&#37325;&#35201;&#24615;&#19982;&#39640;&#39057;&#32454;&#33410;&#19968;&#26679;&#65292;&#20363;&#22914;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#21355;&#26143;&#22270;&#20687;&#20013;&#65307;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#30740;&#31350;&#20154;&#21592;&#24517;&#39035;&#20570;&#20986;&#33293;&#24323;&#21738;&#20123;&#20449;&#24687;&#30340;&#22256;&#25200;&#36873;&#25321;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;xT&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#35270;&#35273;Transformer&#26694;&#26550;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#32858;&#21512;&#20840;&#23616;&#32972;&#26223;&#21644;&#23616;&#37096;&#32454;&#33410;&#65292;&#24182;&#21487;&#20197;&#22312;&#24403;&#20195;GPU&#19978;&#31471;&#23545;&#31471;&#22320;&#23545;&#22823;&#22270;&#20687;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;&#19968;&#32452;&#36328;&#32463;&#20856;&#35270;&#35273;&#20219;&#21153;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#20219;&#21153;&#20934;&#30830;&#22320;&#21453;&#26144;&#20102;&#35270;&#35273;&#27169;&#22411;&#29702;&#35299;&#30495;&#27491;&#22823;&#22411;&#22270;&#20687;&#24182;&#22312;&#22823;&#33539;&#22260;&#20869;&#34701;&#21512;&#32454;&#33410;&#30340;&#33021;&#21147;&#65292;&#24182;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20854;&#19978;&#30340;&#25913;&#36827;&#12290;&#36890;&#36807;&#24341;&#20837;&#38024;&#23545;&#22823;&#22270;&#20687;&#30340;&#23884;&#22871;&#26631;&#35760;&#21270;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01915v1 Announce Type: cross  Abstract: Modern computer vision pipelines handle large images in one of two sub-optimal ways: down-sampling or cropping. These two methods incur significant losses in the amount of information and context present in an image. There are many downstream applications in which global context matters as much as high frequency details, such as in real-world satellite imagery; in such cases researchers have to make the uncomfortable choice of which information to discard. We introduce xT, a simple framework for vision transformers which effectively aggregates global context with local details and can model large images end-to-end on contemporary GPUs. We select a set of benchmark datasets across classic vision tasks which accurately reflect a vision model's ability to understand truly large images and incorporate fine details over large scales and assess our method's improvement on them. By introducing a nested tokenization scheme for large images in 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#20849;&#21516;&#21407;&#22240;$C$&#36827;&#34892;&#26465;&#20214;&#35774;&#23450;&#65292;&#35299;&#20915;&#20102;&#36763;&#26222;&#26862;&#24726;&#35770;&#65292;&#25512;&#24191;&#20102;&#24726;&#35770;&#65292;&#24182;&#34920;&#26126;&#22312;&#20108;&#20803;&#20849;&#21516;&#21407;&#22240;$C$&#19978;&#36827;&#34892;&#26465;&#20214;&#35774;&#23450;&#30340;&#20851;&#32852;&#26041;&#21521;&#19982;&#21407;&#22987;$B$&#19978;&#36827;&#34892;&#26465;&#20214;&#35774;&#23450;&#30456;&#21516;</title><link>https://arxiv.org/abs/2403.00957</link><description>&lt;p&gt;
&#21033;&#29992;&#20849;&#22240;&#21407;&#21017;&#35299;&#20915;&#36763;&#26222;&#26862;&#24726;&#35770;
&lt;/p&gt;
&lt;p&gt;
Resolution of Simpson's paradox via the common cause principle
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00957
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#20849;&#21516;&#21407;&#22240;$C$&#36827;&#34892;&#26465;&#20214;&#35774;&#23450;&#65292;&#35299;&#20915;&#20102;&#36763;&#26222;&#26862;&#24726;&#35770;&#65292;&#25512;&#24191;&#20102;&#24726;&#35770;&#65292;&#24182;&#34920;&#26126;&#22312;&#20108;&#20803;&#20849;&#21516;&#21407;&#22240;$C$&#19978;&#36827;&#34892;&#26465;&#20214;&#35774;&#23450;&#30340;&#20851;&#32852;&#26041;&#21521;&#19982;&#21407;&#22987;$B$&#19978;&#36827;&#34892;&#26465;&#20214;&#35774;&#23450;&#30456;&#21516;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36763;&#26222;&#26862;&#24726;&#35770;&#26159;&#24314;&#31435;&#20004;&#20010;&#20107;&#20214;$a_1$&#21644;$a_2$&#20043;&#38388;&#30340;&#27010;&#29575;&#20851;&#32852;&#26102;&#30340;&#38556;&#30861;&#65292;&#32473;&#23450;&#31532;&#19977;&#20010;&#65288;&#28508;&#22312;&#30340;&#65289;&#38543;&#26426;&#21464;&#37327;$B$&#12290;&#25105;&#20204;&#20851;&#27880;&#30340;&#24773;&#26223;&#26159;&#38543;&#26426;&#21464;&#37327;$A$&#65288;&#27719;&#24635;&#20102;$a_1$&#12289;$a_2$&#21450;&#20854;&#34917;&#38598;&#65289;&#21644;$B$&#26377;&#19968;&#20010;&#21487;&#33021;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#20849;&#21516;&#21407;&#22240;$C$&#12290;&#25110;&#32773;&#65292;&#25105;&#20204;&#21487;&#20197;&#20551;&#35774;$C$&#23558;$A$&#20174;$B$&#20013;&#31579;&#36873;&#20986;&#21435;&#12290;&#23545;&#20110;&#36825;&#31181;&#24773;&#20917;&#65292;&#27491;&#30830;&#30340;$a_1$&#21644;$a_2$&#20043;&#38388;&#30340;&#20851;&#32852;&#24212;&#35813;&#36890;&#36807;&#23545;$C$&#36827;&#34892;&#26465;&#20214;&#35774;&#23450;&#26469;&#23450;&#20041;&#12290;&#36825;&#19968;&#35774;&#32622;&#23558;&#21407;&#22987;&#36763;&#26222;&#26862;&#24726;&#35770;&#25512;&#24191;&#20102;&#12290;&#29616;&#22312;&#23427;&#30340;&#20004;&#20010;&#30456;&#20114;&#30683;&#30462;&#30340;&#36873;&#39033;&#31616;&#21333;&#22320;&#25351;&#30340;&#26159;&#20004;&#20010;&#29305;&#23450;&#19988;&#19981;&#21516;&#30340;&#21407;&#22240;$C$&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22914;&#26524;$B$&#21644;$C$&#26159;&#20108;&#36827;&#21046;&#30340;&#65292;$A$&#26159;&#22235;&#36827;&#21046;&#30340;&#65288;&#23545;&#20110;&#26377;&#25928;&#30340;&#36763;&#26222;&#26862;&#24726;&#35770;&#26469;&#35828;&#26159;&#26368;&#23567;&#19988;&#26368;&#24120;&#35265;&#30340;&#24773;&#20917;&#65289;&#65292;&#22312;&#20219;&#20309;&#20108;&#20803;&#20849;&#21516;&#21407;&#22240;$C$&#19978;&#36827;&#34892;&#26465;&#20214;&#35774;&#23450;&#23558;&#24314;&#31435;&#19982;&#22312;&#21407;&#22987;$B$&#19978;&#36827;&#34892;&#26465;&#20214;&#35774;&#23450;&#30456;&#21516;&#30340;$a_1$&#21644;$a_2$&#20043;&#38388;&#30340;&#20851;&#32852;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00957v1 Announce Type: cross  Abstract: Simpson's paradox is an obstacle to establishing a probabilistic association between two events $a_1$ and $a_2$, given the third (lurking) random variable $B$. We focus on scenarios when the random variables $A$ (which combines $a_1$, $a_2$, and their complements) and $B$ have a common cause $C$ that need not be observed. Alternatively, we can assume that $C$ screens out $A$ from $B$. For such cases, the correct association between $a_1$ and $a_2$ is to be defined via conditioning over $C$. This set-up generalizes the original Simpson's paradox. Now its two contradicting options simply refer to two particular and different causes $C$. We show that if $B$ and $C$ are binary and $A$ is quaternary (the minimal and the most widespread situation for valid Simpson's paradox), the conditioning over any binary common cause $C$ establishes the same direction of the association between $a_1$ and $a_2$ as the conditioning over $B$ in the original
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#22686;&#21152;&#27979;&#35797;&#26399;&#38388;&#27491;&#30830;&#39044;&#27979;&#30340;&#26426;&#20250;&#65292;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#32416;&#27491;FSL&#27169;&#22411;&#30340;&#27979;&#35797;&#36755;&#20837;&#65292;&#29983;&#25104;&#34987;&#27979;&#35797;&#31867;&#21035;&#30340;&#26032;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.18292</link><description>&lt;p&gt;
FSL&#27169;&#22411;&#21487;&#20197;&#22240;&#20026;&#20854;&#20248;&#36234;&#24615;&#24471;&#20998;&#26356;&#39640;
&lt;/p&gt;
&lt;p&gt;
FSL Model can Score Higher as It Is
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18292
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22686;&#21152;&#27979;&#35797;&#26399;&#38388;&#27491;&#30830;&#39044;&#27979;&#30340;&#26426;&#20250;&#65292;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#32416;&#27491;FSL&#27169;&#22411;&#30340;&#27979;&#35797;&#36755;&#20837;&#65292;&#29983;&#25104;&#34987;&#27979;&#35797;&#31867;&#21035;&#30340;&#26032;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#65292;&#20026;&#20102;&#22686;&#21152;&#34987;&#27491;&#30830;&#35782;&#21035;&#30340;&#26426;&#20250;&#65292;&#25105;&#20204;&#20542;&#21521;&#20110;&#38754;&#23545;&#38754;&#22320;&#30452;&#35270;&#38754;&#37096;&#35782;&#21035;&#26426;&#65292;&#32780;&#19981;&#26159;&#20391;&#30528;&#38754;&#23545;&#12290;&#23569;&#26679;&#26412;&#23398;&#20064;&#65288;FSL&#65289;&#20998;&#31867;&#26412;&#36523;&#23601;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#27169;&#22411;&#24517;&#39035;&#35782;&#21035;&#23646;&#20110;&#35757;&#32451;&#26102;&#26410;&#35265;&#30340;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;&#22240;&#27492;&#65292;&#22312;&#27979;&#35797;&#26399;&#38388;&#23545;&#25197;&#26354;&#21644;&#38750;&#20856;&#22411;&#30340;&#26597;&#35810;&#25110;&#25903;&#25345;&#22270;&#20687;&#20250;&#35753;&#27169;&#22411;&#26356;&#38590;&#27491;&#30830;&#39044;&#27979;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#20026;&#20102;&#22686;&#21152;&#27979;&#35797;&#26399;&#38388;&#27491;&#30830;&#39044;&#27979;&#30340;&#26426;&#20250;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#32416;&#27491;&#35757;&#32451;&#36807;&#30340;FSL&#27169;&#22411;&#30340;&#27979;&#35797;&#36755;&#20837;&#65292;&#29983;&#25104;&#34987;&#27979;&#35797;&#31867;&#21035;&#30340;&#26032;&#26679;&#26412;&#12290;FSL&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#20855;&#26377;&#36275;&#22815;&#26679;&#26412;&#30340;&#31867;&#21035;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#20855;&#26377;&#23569;&#26679;&#26412;&#26679;&#26412;&#30340;&#31867;&#21035;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#39318;&#20808;&#25429;&#25417;&#27979;&#35797;&#22270;&#20687;&#30340;&#39118;&#26684;&#25110;&#24418;&#29366;&#65292;&#28982;&#21518;&#35782;&#21035;&#19968;&#20010;&#36866;&#24403;&#30340;&#35757;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18292v1 Announce Type: cross  Abstract: In daily life, we tend to present the front of our faces by staring squarely at a facial recognition machine, instead of facing it sideways, in order to increase the chance of being correctly recognised. Few-shot-learning (FSL) classification is challenging in itself because a model has to identify images that belong to classes previously unseen during training. Therefore, a warped and non-typical query or support image during testing can make it even more challenging for a model to predict correctly. In our work, to increase the chance of correct prediction during testing, we aim to rectify the test input of a trained FSL model by generating new samples of the tested classes through image-to-image translation. An FSL model is usually trained on classes with sufficient samples, and then tested on classes with few-shot samples. Our proposed method first captures the style or shape of the test image, and then identifies a suitable traine
&lt;/p&gt;</description></item><item><title>OmniACT&#26159;&#19968;&#20010;&#38024;&#23545;&#20195;&#29702;&#29983;&#25104;&#21487;&#25191;&#34892;&#31243;&#24207;&#23436;&#25104;&#35745;&#31639;&#26426;&#20219;&#21153;&#33021;&#21147;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;Web&#33258;&#21160;&#21270;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#26700;&#38754;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.17553</link><description>&lt;p&gt;
OmniACT&#65306;&#29992;&#20110;&#21551;&#29992;&#26700;&#38754;&#21644;Web&#22810;&#27169;&#24335;&#36890;&#29992;&#20027;&#21160;&#26234;&#33021;&#20307;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17553
&lt;/p&gt;
&lt;p&gt;
OmniACT&#26159;&#19968;&#20010;&#38024;&#23545;&#20195;&#29702;&#29983;&#25104;&#21487;&#25191;&#34892;&#31243;&#24207;&#23436;&#25104;&#35745;&#31639;&#26426;&#20219;&#21153;&#33021;&#21147;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;Web&#33258;&#21160;&#21270;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#26700;&#38754;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#21313;&#24180;&#26469;&#65292;&#20154;&#26426;&#20132;&#20114;&#20174;&#26681;&#26412;&#19978;&#19968;&#30452;&#26159;&#25163;&#21160;&#30340;&#12290;&#21363;&#20351;&#22312;&#20170;&#22825;&#65292;&#20960;&#20046;&#25152;&#26377;&#22312;&#35745;&#31639;&#26426;&#19978;&#36827;&#34892;&#30340;&#39640;&#25928;&#24037;&#20316;&#37117;&#38656;&#35201;&#20154;&#31867;&#22312;&#27599;&#19968;&#27493;&#37117;&#25552;&#20379;&#36755;&#20837;&#12290;&#34394;&#25311;&#20027;&#21160;&#26234;&#33021;&#20195;&#34920;&#20102;&#33258;&#21160;&#21270;&#35768;&#22810;&#36825;&#20123;&#29712;&#30862;&#20219;&#21153;&#30340;&#19968;&#20010;&#28608;&#21160;&#20154;&#24515;&#30340;&#27493;&#39588;&#12290;&#34394;&#25311;&#20195;&#29702;&#23558;&#20351;&#25216;&#26415;&#33021;&#21147;&#26377;&#38480;&#30340;&#29992;&#25143;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#35745;&#31639;&#26426;&#31995;&#32479;&#30340;&#21508;&#31181;&#21487;&#33021;&#24615;&#12290;&#23427;&#20204;&#36824;&#21487;&#20197;&#23454;&#29616;&#39640;&#25928;&#22320;&#31616;&#21270;&#35768;&#22810;&#35745;&#31639;&#26426;&#20219;&#21153;&#65292;&#20174;&#26085;&#21382;&#31649;&#29702;&#21040;&#22797;&#26434;&#30340;&#26053;&#34892;&#39044;&#35746;&#65292;&#20943;&#23569;&#20154;&#31867;&#24178;&#39044;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; OmniACT&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20195;&#29702;&#29983;&#25104;&#21487;&#25191;&#34892;&#31243;&#24207;&#26469;&#23436;&#25104;&#35745;&#31639;&#26426;&#20219;&#21153;&#33021;&#21147;&#30340;&#39318;&#20010;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#33539;&#22260;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;Web&#33258;&#21160;&#21270;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#26700;&#38754;&#24212;&#29992;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#35832;&#22914;"&#25773;&#25918;&#19979;&#19968;&#39318;&#27468;"&#20043;&#31867;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#20197;&#21450;&#26356;&#20026;&#38271;&#26399;&#30340;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17553v1 Announce Type: new  Abstract: For decades, human-computer interaction has fundamentally been manual. Even today, almost all productive work done on the computer necessitates human input at every step. Autonomous virtual agents represent an exciting step in automating many of these menial tasks. Virtual agents would empower users with limited technical proficiency to harness the full possibilities of computer systems. They could also enable the efficient streamlining of numerous computer tasks, ranging from calendar management to complex travel bookings, with minimal human intervention. In this paper, we introduce OmniACT, the first-of-a-kind dataset and benchmark for assessing an agent's capability to generate executable programs to accomplish computer tasks. Our scope extends beyond traditional web automation, covering a diverse range of desktop applications. The dataset consists of fundamental tasks such as "Play the next song", as well as longer horizon tasks such
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36229;&#39640;&#32500;&#35745;&#31639;&#36827;&#34892;&#21333;&#27425;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#39640;&#32500;&#31354;&#38388;&#24182;&#21033;&#29992;HD&#36816;&#31639;&#31526;&#36827;&#34892;&#20449;&#24687;&#32858;&#21512;&#65292;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#35745;&#31639;&#26114;&#36149;&#30340;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2402.17073</link><description>&lt;p&gt;
&#20351;&#29992;&#36229;&#39640;&#32500;&#35745;&#31639;&#36827;&#34892;&#21333;&#27425;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
One-Shot Graph Representation Learning Using Hyperdimensional Computing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17073
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36229;&#39640;&#32500;&#35745;&#31639;&#36827;&#34892;&#21333;&#27425;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#39640;&#32500;&#31354;&#38388;&#24182;&#21033;&#29992;HD&#36816;&#31639;&#31526;&#36827;&#34892;&#20449;&#24687;&#32858;&#21512;&#65292;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#35745;&#31639;&#26114;&#36149;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#31616;&#21333;&#12289;&#24555;&#36895;&#12289;&#39640;&#25928;&#30340;&#21322;&#30417;&#30563;&#22270;&#23398;&#20064;&#26041;&#27861;&#12290;&#25152;&#25552;&#26041;&#27861;&#21033;&#29992;&#36229;&#39640;&#32500;&#35745;&#31639;&#65292;&#23558;&#25968;&#25454;&#26679;&#26412;&#20351;&#29992;&#38543;&#26426;&#25237;&#24433;&#32534;&#30721;&#21040;&#39640;&#32500;&#31354;&#38388;&#65288;&#31616;&#31216;HD&#31354;&#38388;&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#33410;&#28857;&#34920;&#31034;&#30340;&#21333;&#23556;&#24615;&#36136;&#30340;&#36229;&#39640;&#32500;&#22270;&#23398;&#20064;&#65288;HDGL&#65289;&#31639;&#27861;&#12290;HDGL&#23558;&#33410;&#28857;&#29305;&#24449;&#26144;&#23556;&#21040;HD&#31354;&#38388;&#65292;&#28982;&#21518;&#20351;&#29992;HD&#36816;&#31639;&#31526;&#65288;&#22914;&#25414;&#32465;&#21644;&#32465;&#23450;&#65289;&#26469;&#32858;&#21512;&#27599;&#20010;&#33410;&#28857;&#30340;&#23616;&#37096;&#37051;&#22495;&#20449;&#24687;&#12290;&#23545;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;HDGL&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#35745;&#31639;&#26114;&#36149;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17073v1 Announce Type: cross  Abstract: We present a novel, simple, fast, and efficient approach for semi-supervised learning on graphs. The proposed approach takes advantage of hyper-dimensional computing which encodes data samples using random projections into a high dimensional space (HD space for short). Specifically, we propose a Hyper-dimensional Graph Learning (HDGL) algorithm that leverages the injectivity property of the node representations of a family of graph neural networks. HDGL maps node features to the HD space and then uses HD operators such as bundling and binding to aggregate information from the local neighborhood of each node. Results of experiments with widely used benchmark data sets show that HDGL achieves predictive performance that is competitive with the state-of-the-art deep learning methods, without the need for computationally expensive training.
&lt;/p&gt;</description></item><item><title>MLLMs&#36890;&#36807;&#24494;&#35843;&#33719;&#24471;&#20102;&#29305;&#23450;&#39046;&#22495;&#30340;&#35270;&#35273;&#33021;&#21147;&#65292;&#20294;&#25237;&#24433;&#24182;&#26410;&#25552;&#21462;&#30456;&#20851;&#30340;&#39046;&#22495;&#29305;&#23450;&#35270;&#35273;&#23646;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16832</link><description>&lt;p&gt;
&#31070;&#31192;&#30340;&#25237;&#24433;&#65306;&#22810;&#27169;&#24577;LLMs&#22312;&#27809;&#26377;&#26356;&#20016;&#23500;&#30340;&#36328;&#27169;&#24577;&#25237;&#24433;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#29305;&#23450;&#39046;&#22495;&#30340;&#35270;&#35273;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Mysterious Projections: Multimodal LLMs Gain Domain-Specific Visual Capabilities Without Richer Cross-Modal Projections
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16832
&lt;/p&gt;
&lt;p&gt;
MLLMs&#36890;&#36807;&#24494;&#35843;&#33719;&#24471;&#20102;&#29305;&#23450;&#39046;&#22495;&#30340;&#35270;&#35273;&#33021;&#21147;&#65292;&#20294;&#25237;&#24433;&#24182;&#26410;&#25552;&#21462;&#30456;&#20851;&#30340;&#39046;&#22495;&#29305;&#23450;&#35270;&#35273;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22914;LLaVA&#21644;GPT-4(V)&#20351;&#24471;&#21487;&#20197;&#36827;&#34892;&#20851;&#20110;&#22270;&#20687;&#30340;&#36890;&#29992;&#23545;&#35805;&#12290;&#28982;&#32780;&#65292;&#29616;&#25104;&#30340;MLLMs&#21487;&#33021;&#22312;&#35832;&#22914;&#30382;&#32932;&#30149;&#23398;&#21644;&#20892;&#19994;&#31561;&#39046;&#22495;&#30340;&#22270;&#20687;&#19978;&#20855;&#26377;&#26377;&#38480;&#30340;&#33021;&#21147;&#65292;&#22240;&#27492;&#24517;&#39035;&#36827;&#34892;&#24494;&#35843;&#20197;&#35299;&#38145;&#29305;&#23450;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#23545;4&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#22312;&#20004;&#31181;&#24494;&#35843;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;&#38543;&#30528;MLLM&#30340;&#24494;&#35843;&#65292;&#23427;&#30830;&#23454;&#33719;&#24471;&#20102;&#29305;&#23450;&#39046;&#22495;&#30340;&#35270;&#35273;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#26356;&#26032;&#24182;&#27809;&#26377;&#23548;&#33268;&#25237;&#24433;&#25552;&#21462;&#30456;&#20851;&#30340;&#39046;&#22495;&#29305;&#23450;&#35270;&#35273;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16832v1 Announce Type: new  Abstract: Multimodal large language models (MLLMs) like LLaVA and GPT-4(V) enable general-purpose conversations about images with the language modality. As off-the-shelf MLLMs may have limited capabilities on images from domains like dermatology and agriculture, they must be fine-tuned to unlock domain-specific applications. The prevalent architecture of current open-source MLLMs comprises two major modules: an image-language (cross-modal) projection network and a large language model. It is desirable to understand the roles of these two modules in modeling domain-specific visual attributes to inform the design of future models and streamline the interpretability efforts on the current models. To this end, via experiments on 4 datasets and under 2 fine-tuning settings, we find that as the MLLM is fine-tuned, it indeed gains domain-specific visual capabilities, but the updates do not lead to the projection extracting relevant domain-specific visual
&lt;/p&gt;</description></item><item><title>Rainbow Teaming&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24320;&#25918;&#24335;&#25628;&#32034;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#23545;&#25239;&#24615;&#25552;&#31034;&#65292;&#21487;&#20197;&#24110;&#21161;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#25552;&#39640;&#23433;&#20840;&#24615;&#65292;&#38382;&#31572;&#21644;&#32593;&#32476;&#23433;&#20840;&#31561;&#39046;&#22495;&#30340;&#27169;&#22411;&#28431;&#27934;&#12290;</title><link>https://arxiv.org/abs/2402.16822</link><description>&lt;p&gt;
&#24425;&#34425;&#22242;&#38431;&#65306;&#22810;&#26679;&#21270;&#23545;&#25239;&#24615;&#25552;&#31034;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16822
&lt;/p&gt;
&lt;p&gt;
Rainbow Teaming&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24320;&#25918;&#24335;&#25628;&#32034;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#23545;&#25239;&#24615;&#25552;&#31034;&#65292;&#21487;&#20197;&#24110;&#21161;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#25552;&#39640;&#23433;&#20840;&#24615;&#65292;&#38382;&#31572;&#21644;&#32593;&#32476;&#23433;&#20840;&#31561;&#39046;&#22495;&#30340;&#27169;&#22411;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#29702;&#35299;&#21644;&#22686;&#24378;&#23427;&#20204;&#23545;&#29992;&#25143;&#36755;&#20837;&#30340;&#31283;&#20581;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;&#35782;&#21035;&#25932;&#23545;&#25552;&#31034;&#30340;&#26041;&#27861;&#24448;&#24448;&#19987;&#27880;&#20110;&#29305;&#23450;&#39046;&#22495;&#65292;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#25110;&#38656;&#35201;&#22823;&#37327;&#20154;&#24037;&#27880;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24425;&#34425;&#22242;&#38431;&#65292;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22810;&#26679;&#21270;&#23545;&#25239;&#24615;&#25552;&#31034;&#30340;&#26032;&#26041;&#27861;&#12290;&#24425;&#34425;&#22242;&#38431;&#23558;&#23545;&#25239;&#24615;&#25552;&#31034;&#29983;&#25104;&#35270;&#20026;&#19968;&#20010;&#36136;&#37327; - &#22810;&#26679;&#24615;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#24320;&#25918;&#24335;&#25628;&#32034;&#26469;&#29983;&#25104;&#26082;&#26377;&#25928;&#21448;&#22810;&#26679;&#30340;&#25552;&#31034;&#12290;&#23427;&#21487;&#20197;&#25581;&#31034;&#27169;&#22411;&#22312;&#24191;&#27867;&#39046;&#22495;&#20869;&#30340;&#33030;&#24369;&#24615;&#65292;&#21253;&#25324;&#26412;&#25991;&#20013;&#30340;&#23433;&#20840;&#24615;&#12289;&#38382;&#31572;&#21644;&#32593;&#32476;&#23433;&#20840;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;&#23545;&#30001;&#24425;&#34425;&#22242;&#38431;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#26368;&#20808;&#36827;&#30340;LLMs&#30340;&#23433;&#20840;&#24615;&#65292;&#32780;&#19981;&#25439;&#23475;&#23427;&#20204;&#30340;&#19968;&#33324;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16822v1 Announce Type: new  Abstract: As large language models (LLMs) become increasingly prevalent across many real-world applications, understanding and enhancing their robustness to user inputs is of paramount importance. Existing methods for identifying adversarial prompts tend to focus on specific domains, lack diversity, or require extensive human annotations. To address these limitations, we present Rainbow Teaming, a novel approach for producing a diverse collection of adversarial prompts. Rainbow Teaming casts adversarial prompt generation as a quality-diversity problem, and uses open-ended search to generate prompts that are both effective and diverse. It can uncover a model's vulnerabilities across a broad range of domains including, in this paper, safety, question answering, and cybersecurity. We also demonstrate that fine-tuning on synthetic data generated by Rainbow Teaming improves the safety of state-of-the-art LLMs without hurting their general capabilities 
&lt;/p&gt;</description></item><item><title>ReadAgent&#26159;&#19968;&#20010;&#20855;&#26377;&#38271;&#26399;&#19978;&#19979;&#25991;&#27010;&#35201;&#35760;&#24518;&#30340;&#38405;&#35835;&#20195;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#23454;&#29616;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#22788;&#29702;&#38271;&#36755;&#20837;&#24182;&#25552;&#39640;&#26377;&#25928;&#19978;&#19979;&#25991;&#38271;&#24230;&#12290;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.09727</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#38271;&#26399;&#19978;&#19979;&#25991;&#27010;&#35201;&#35760;&#24518;&#30340;&#20154;&#24037;&#26234;&#33021;&#38405;&#35835;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09727
&lt;/p&gt;
&lt;p&gt;
ReadAgent&#26159;&#19968;&#20010;&#20855;&#26377;&#38271;&#26399;&#19978;&#19979;&#25991;&#27010;&#35201;&#35760;&#24518;&#30340;&#38405;&#35835;&#20195;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#23454;&#29616;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#22788;&#29702;&#38271;&#36755;&#20837;&#24182;&#25552;&#39640;&#26377;&#25928;&#19978;&#19979;&#25991;&#38271;&#24230;&#12290;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#38480;&#21046;&#22312;&#26576;&#20010;&#26368;&#22823;&#19978;&#19979;&#25991;&#38271;&#24230;&#20869;&#65292;&#32780;&#19988;&#26080;&#27861;&#31283;&#23450;&#22320;&#22788;&#29702;&#38271;&#36755;&#20837;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ReadAgent&#65292;&#19968;&#20010;&#22686;&#21152;&#20102;&#26377;&#25928;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#31995;&#32479;&#65292;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#21487;&#20197;&#36798;&#21040;20&#20493;&#12290;&#21463;&#21040;&#20154;&#31867;&#20132;&#20114;&#24335;&#38405;&#35835;&#38271;&#25991;&#26723;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;ReadAgent&#23454;&#29616;&#20026;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#31995;&#32479;&#65292;&#21033;&#29992;LLM&#30340;&#39640;&#32423;&#35821;&#35328;&#33021;&#21147;&#26469;&#65306;&#65288;1&#65289;&#20915;&#23450;&#23558;&#21738;&#20123;&#20869;&#23481;&#23384;&#20648;&#22312;&#19968;&#20010;&#35760;&#24518;&#29255;&#27573;&#20013;&#65292;&#65288;2&#65289;&#23558;&#36825;&#20123;&#35760;&#24518;&#29255;&#27573;&#21387;&#32553;&#25104;&#20026;&#31216;&#20026;&#27010;&#35201;&#35760;&#24518;&#30340;&#30701;&#26102;&#35760;&#24518;&#65292;&#65288;3&#65289;&#22312;&#38656;&#35201;&#26102;&#36890;&#36807;&#21407;&#22987;&#25991;&#26412;&#26597;&#25214;&#27573;&#33853;&#26469;&#25552;&#37266;&#33258;&#24049;&#30456;&#20851;&#32454;&#33410;&#20197;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;&#26816;&#32034;&#26041;&#27861;&#12289;&#20351;&#29992;&#21407;&#22987;&#38271;&#19978;&#19979;&#25991;&#20197;&#21450;&#20351;&#29992;&#27010;&#35201;&#35760;&#24518;&#26469;&#35780;&#20272;ReadAgent&#19982;&#22522;&#32447;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#35780;&#20272;&#26159;&#22312;&#19977;&#20010;&#38271;&#25991;&#26723;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#19978;&#36827;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09727v1 Announce Type: cross  Abstract: Current Large Language Models (LLMs) are not only limited to some maximum context length, but also are not able to robustly consume long inputs. To address these limitations, we propose ReadAgent, an LLM agent system that increases effective context length up to 20x in our experiments. Inspired by how humans interactively read long documents, we implement ReadAgent as a simple prompting system that uses the advanced language capabilities of LLMs to (1) decide what content to store together in a memory episode, (2) compress those memory episodes into short episodic memories called gist memories, and (3) take actions to look up passages in the original text if ReadAgent needs to remind itself of relevant details to complete a task. We evaluate ReadAgent against baselines using retrieval methods, using the original long contexts, and using the gist memories. These evaluations are performed on three long-document reading comprehension task
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;SelfSwapper&#65292;&#19968;&#31181;&#36890;&#36807; Shape Agnostic Masked AutoEncoder (SAMAE) &#33258;&#30417;&#30563;&#26041;&#26696;&#26469;&#25552;&#21319;&#20154;&#33080;&#20132;&#25442;&#27169;&#22411;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32469;&#36807;&#20256;&#32479;&#30340;&#35757;&#32451;&#26041;&#26696;&#65292;&#24341;&#20837;&#28165;&#26224;&#30340;&#30495;&#23454;&#25968;&#25454;&#65292;&#20197;&#21450;&#21033;&#29992;&#36974;&#32617;&#21644;&#23398;&#21040;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#25104;&#21151;&#35299;&#20915;&#20102;&#36523;&#20221;&#27844;&#28431;&#21644;&#24418;&#29366;&#19981;&#23545;&#40784;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07370</link><description>&lt;p&gt;
SelfSwapper: &#36890;&#36807;&#24418;&#29366;&#26080;&#20851;&#30340;&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#33258;&#30417;&#30563;&#20154;&#33080;&#20132;&#25442;
&lt;/p&gt;
&lt;p&gt;
SelfSwapper: Self-Supervised Face Swapping via Shape Agnostic Masked AutoEncoder
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SelfSwapper&#65292;&#19968;&#31181;&#36890;&#36807; Shape Agnostic Masked AutoEncoder (SAMAE) &#33258;&#30417;&#30563;&#26041;&#26696;&#26469;&#25552;&#21319;&#20154;&#33080;&#20132;&#25442;&#27169;&#22411;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32469;&#36807;&#20256;&#32479;&#30340;&#35757;&#32451;&#26041;&#26696;&#65292;&#24341;&#20837;&#28165;&#26224;&#30340;&#30495;&#23454;&#25968;&#25454;&#65292;&#20197;&#21450;&#21033;&#29992;&#36974;&#32617;&#21644;&#23398;&#21040;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#25104;&#21151;&#35299;&#20915;&#20102;&#36523;&#20221;&#27844;&#28431;&#21644;&#24418;&#29366;&#19981;&#23545;&#40784;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#20132;&#25442;&#22240;&#20854;&#22810;&#26679;&#21270;&#30340;&#24212;&#29992;&#32780;&#21463;&#21040;&#26497;&#22823;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#20154;&#33080;&#20132;&#25442;&#26041;&#27861;&#20381;&#36182;&#20110;&#36343;&#36343;&#26495;&#24335;&#35757;&#32451;&#26041;&#26696;&#65292;&#36890;&#24120;&#23548;&#33268;&#27169;&#22411;&#35757;&#32451;&#30340;&#19981;&#31283;&#23450;&#24615;&#24182;&#20135;&#29983;&#28151;&#21512;&#36523;&#20221;&#30340;&#19981;&#26399;&#26395;&#26679;&#26412;&#65292;&#21407;&#22240;&#26159;&#30446;&#26631;&#36523;&#20221;&#27844;&#28431;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Shape Agnostic Masked AutoEncoder (SAMAE) &#35757;&#32451;&#26041;&#26696;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#36827;&#20154;&#33080;&#20132;&#25442;&#27169;&#22411;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#26041;&#26696;&#36890;&#36807;&#32469;&#36807;&#20256;&#32479;&#30340;&#36343;&#36343;&#26495;&#28216;&#25103;&#24182;&#36890;&#36807;&#33258;&#37325;&#24314;&#35757;&#32451;&#26426;&#21046;&#24341;&#20837;&#28165;&#26224;&#30340;&#30495;&#23454;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#35757;&#32451;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#23427;&#36890;&#36807;&#36974;&#32617;&#36755;&#20837;&#22270;&#20687;&#30340;&#38754;&#37096;&#21306;&#22495;&#21644;&#21033;&#29992;&#23398;&#21040;&#30340;&#36523;&#20221;&#21644;&#38750;&#36523;&#20221;&#29305;&#24449;&#26469;&#26377;&#25928;&#20943;&#36731;&#36523;&#20221;&#27844;&#28431;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807; perforation confusion &#21644;&#38543;&#26426;&#32593;&#26684;&#32553;&#25918;&#31561;&#26032;&#25216;&#26415;&#26469;&#35299;&#20915;&#24418;&#29366;&#19981;&#23545;&#40784;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face swapping has gained significant attention for its varied applications. The majority of previous face swapping approaches have relied on the seesaw game training scheme, which often leads to the instability of the model training and results in undesired samples with blended identities due to the target identity leakage problem. This paper introduces the Shape Agnostic Masked AutoEncoder (SAMAE) training scheme, a novel self-supervised approach designed to enhance face swapping model training. Our training scheme addresses the limitations of traditional training methods by circumventing the conventional seesaw game and introducing clear ground truth through its self-reconstruction training regime. It effectively mitigates identity leakage by masking facial regions of the input images and utilizing learned disentangled identity and non-identity features. Additionally, we tackle the shape misalignment problem with new techniques including perforation confusion and random mesh scaling,
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20248;&#21270;&#35299;&#37322;&#22686;&#24378;&#30340;&#30693;&#35782;&#33976;&#39311;&#65288;e$^2$KD&#65289;&#31639;&#27861;&#65292;&#21487;&#20197;&#35753;&#23398;&#29983;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#23398;&#29983;-&#25945;&#24072;&#19968;&#33268;&#24615;&#26041;&#38754;&#37117;&#24471;&#21040;&#22823;&#24133;&#24230;&#25552;&#21319;&#65292;&#30830;&#20445;&#23398;&#29983;&#27169;&#22411;&#20174;&#25945;&#24072;&#37027;&#37324;&#27491;&#30830;&#23398;&#21040;&#21407;&#22240;&#12290;</title><link>https://arxiv.org/abs/2402.03119</link><description>&lt;p&gt;
&#22909;&#30340;&#25945;&#24072;&#35299;&#37322;: &#35299;&#37322;&#22686;&#24378;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Good Teachers Explain: Explanation-Enhanced Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03119
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20248;&#21270;&#35299;&#37322;&#22686;&#24378;&#30340;&#30693;&#35782;&#33976;&#39311;&#65288;e$^2$KD&#65289;&#31639;&#27861;&#65292;&#21487;&#20197;&#35753;&#23398;&#29983;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#23398;&#29983;-&#25945;&#24072;&#19968;&#33268;&#24615;&#26041;&#38754;&#37117;&#24471;&#21040;&#22823;&#24133;&#24230;&#25552;&#21319;&#65292;&#30830;&#20445;&#23398;&#29983;&#27169;&#22411;&#20174;&#25945;&#24072;&#37027;&#37324;&#27491;&#30830;&#23398;&#21040;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#23558;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#21387;&#32553;&#25104;&#36739;&#23567;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#34429;&#28982;&#24050;&#32463;&#30693;&#36947;&#23398;&#29983;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;&#19982;&#25945;&#24072;&#30456;&#20284;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#20063;&#24050;&#32463;&#21457;&#29616;&#23398;&#29983;&#27169;&#22411;&#36890;&#24120;&#19981;&#20250;&#23398;&#21040;&#30456;&#21516;&#30340;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#23398;&#29983;&#27169;&#22411;&#21644;&#25945;&#24072;&#27169;&#22411;&#20043;&#38388;&#20849;&#20139;&#30456;&#20284;&#23646;&#24615;&#65292;&#22914;&#22522;&#20110;&#30456;&#21516;&#30340;&#36755;&#20837;&#29305;&#24449;&#36827;&#34892;&#39044;&#27979;&#65292;&#36890;&#24120;&#26159;&#38750;&#24120;&#26377;&#20215;&#20540;&#30340;&#65292;&#22240;&#20026;&#36825;&#30830;&#20445;&#23398;&#29983;&#20174;&#25945;&#24072;&#37027;&#37324;&#23398;&#21040;&#20102;&#8220;&#27491;&#30830;&#30340;&#29305;&#24449;&#8221;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#20248;&#21270;&#32463;&#20856;&#30340;&#30693;&#35782;&#33976;&#39311;&#25439;&#22833;&#20197;&#21450;&#25945;&#24072;&#21644;&#23398;&#29983;&#25152;&#29983;&#25104;&#30340;&#35299;&#37322;&#30340;&#30456;&#20284;&#24615;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#23613;&#31649;&#36825;&#20010;&#24819;&#27861;&#31616;&#21333;&#19988;&#30452;&#35266;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#25552;&#20986;&#30340;&#8220;&#35299;&#37322;&#22686;&#24378;&#30340;&#30693;&#35782;&#33976;&#39311;&#8221;&#65288;e$^2$KD&#65289;&#65288;1&#65289;&#22312;&#20934;&#30830;&#24615;&#21644;&#23398;&#29983;-&#25945;&#24072;&#19968;&#33268;&#24615;&#26041;&#38754;&#22987;&#32456;&#25552;&#20379;&#20102;&#22823;&#24133;&#24230;&#30340;&#22686;&#30410;&#65292;&#65288;2&#65289;&#30830;&#20445;&#23398;&#29983;&#20174;&#25945;&#24072;&#37027;&#37324;&#23398;&#21040;&#20102;&#27491;&#30830;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation (KD) has proven effective for compressing large teacher models into smaller student models. While it is well known that student models can achieve similar accuracies as the teachers, it has also been shown that they nonetheless often do not learn the same function. It is, however, often highly desirable that the student's and teacher's functions share similar properties such as basing the prediction on the same input features, as this ensures that students learn the 'right features' from the teachers. In this work, we explore whether this can be achieved by not only optimizing the classic KD loss but also the similarity of the explanations generated by the teacher and the student. Despite the idea being simple and intuitive, we find that our proposed 'explanation-enhanced' KD (e$^2$KD) (1) consistently provides large gains in terms of accuracy and student-teacher agreement, (2) ensures that the student learns from the teacher to be right for the right reasons and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#38899;&#20048;&#29983;&#25104;AI&#39046;&#22495;&#20013;&#30340;&#29256;&#26435;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;AI&#38899;&#20048;&#29983;&#25104;&#24179;&#21488;&#30340;&#29256;&#31246;&#27169;&#22411;&#65292;&#24182;&#25506;&#35752;&#20102;&#23545;AI&#29983;&#25104;&#38899;&#20048;&#36827;&#34892;&#29256;&#26435;&#24402;&#22240;&#30340;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2312.06646</link><description>&lt;p&gt;
&#35745;&#31639;&#29256;&#26435;: &#38754;&#21521;&#38899;&#20048;&#29983;&#25104;AI&#30340;&#29256;&#31246;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Computational Copyright: Towards A Royalty Model for Music Generative AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#38899;&#20048;&#29983;&#25104;AI&#39046;&#22495;&#20013;&#30340;&#29256;&#26435;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;AI&#38899;&#20048;&#29983;&#25104;&#24179;&#21488;&#30340;&#29256;&#31246;&#27169;&#22411;&#65292;&#24182;&#25506;&#35752;&#20102;&#23545;AI&#29983;&#25104;&#38899;&#20048;&#36827;&#34892;&#29256;&#26435;&#24402;&#22240;&#30340;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;AI&#30340;&#36827;&#27493;&#24341;&#21457;&#20102;&#29256;&#26435;&#25361;&#25112;&#65292;&#22312;&#38899;&#20048;&#34892;&#19994;&#23588;&#20026;&#31361;&#20986;&#12290;&#26412;&#25991;&#20851;&#27880;&#36825;&#20123;&#25361;&#25112;&#30340;&#32463;&#27982;&#26041;&#38754;&#65292;&#24378;&#35843;&#32463;&#27982;&#24433;&#21709;&#22312;&#29256;&#26435;&#39046;&#22495;&#20013;&#26500;&#25104;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#12290;&#40657;&#30418;&#29983;&#25104;AI&#25216;&#26415;&#30340;&#22797;&#26434;&#24615;&#19981;&#20165;&#34920;&#26126;&#65292;&#32780;&#19988;&#38656;&#35201;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#32570;&#22833;&#65292;&#23548;&#33268;&#30417;&#31649;&#25361;&#25112;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#20026;AI&#38899;&#20048;&#29983;&#25104;&#24179;&#21488;&#25552;&#20986;&#28508;&#22312;&#30340;&#29256;&#31246;&#27169;&#22411;&#26469;&#24357;&#34917;&#24403;&#21069;&#26041;&#27861;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#23545;Spotify&#21644;YouTube&#31561;&#24179;&#21488;&#29616;&#26377;&#29256;&#31246;&#27169;&#22411;&#30340;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#23558;&#20854;&#35843;&#25972;&#21040;AI&#29983;&#25104;&#38899;&#20048;&#30340;&#29420;&#29305;&#32972;&#26223;&#20013;&#12290;&#25105;&#20204;&#38754;&#20020;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#23558;AI&#29983;&#25104;&#30340;&#38899;&#20048;&#24402;&#22240;&#20110;&#35757;&#32451;&#25968;&#25454;&#20013;&#26377;&#24433;&#21709;&#21147;&#30340;&#29256;&#26435;&#20869;&#23481;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;&#25968;&#25454;&#24402;&#22240;&#30340;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancement of generative AI has given rise to pressing copyright challenges, particularly in music industry. This paper focuses on the economic aspects of these challenges, emphasizing that the economic impact constitutes a central issue in the copyright arena. The complexity of the black-box generative AI technologies not only suggests but necessitates algorithmic solutions. However, such solutions have been largely missing, leading to regulatory challenges in this landscape. We aim to bridge the gap in current approaches by proposing potential royalty models for revenue sharing on AI music generation platforms. Our methodology involves a detailed analysis of existing royalty models in platforms like Spotify and YouTube, and adapting these to the unique context of AI-generated music. A significant challenge we address is the attribution of AI-generated music to influential copyrighted content in the training data. To this end, we present algorithmic solutions employing data attri
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sieve-&amp;-Swap&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#31579;&#36873;&#20986;&#19981;&#30456;&#20851;&#25991;&#26412;&#24182;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#35828;&#26126;&#26367;&#25442;&#25991;&#26412;&#36716;&#24405;&#65292;&#20174;&#32780;&#23454;&#29616;&#35270;&#39057;&#26412;&#22320;&#21270;&#25351;&#20196;&#29983;&#25104;&#30340;&#39640;&#25928;&#39044;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2311.15964</link><description>&lt;p&gt;
&#35270;&#39057;&#26412;&#22320;&#21270;&#25351;&#20196;&#29983;&#25104;&#30340;&#39640;&#25928;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Pre-training for Localized Instruction Generation of Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15964
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sieve-&amp;-Swap&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#31579;&#36873;&#20986;&#19981;&#30456;&#20851;&#25991;&#26412;&#24182;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#35828;&#26126;&#26367;&#25442;&#25991;&#26412;&#36716;&#24405;&#65292;&#20174;&#32780;&#23454;&#29616;&#35270;&#39057;&#26412;&#22320;&#21270;&#25351;&#20196;&#29983;&#25104;&#30340;&#39640;&#25928;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#31243;&#35270;&#39057;&#23637;&#31034;&#20102;&#35832;&#22914;&#39135;&#35889;&#20934;&#22791;&#31561;&#20219;&#21153;&#30340;&#36880;&#27493;&#28436;&#31034;&#12290;&#29702;&#35299;&#27492;&#31867;&#35270;&#39057;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#23545;&#27493;&#39588;&#36827;&#34892;&#31934;&#30830;&#23450;&#20301;&#24182;&#29983;&#25104;&#25991;&#23383;&#35828;&#26126;&#12290;&#25163;&#21160;&#27880;&#37322;&#27493;&#39588;&#24182;&#32534;&#20889;&#35828;&#26126;&#25104;&#26412;&#39640;&#26114;&#65292;&#36825;&#38480;&#21046;&#20102;&#24403;&#21069;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#24182;&#38459;&#30861;&#20102;&#26377;&#25928;&#23398;&#20064;&#12290;&#21033;&#29992;&#22823;&#35268;&#27169;&#20294;&#22024;&#26434;&#30340;&#35270;&#39057;-&#25991;&#26412;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#21319;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#25991;&#26412;&#36716;&#24405;&#21253;&#21547;&#26080;&#20851;&#20869;&#23481;&#65292;&#19982;&#20154;&#31867;&#27880;&#37322;&#21592;&#32534;&#20889;&#30340;&#35828;&#26126;&#30456;&#27604;&#23384;&#22312;&#39118;&#26684;&#21464;&#21270;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;Sieve-&amp;-Swap&#65292;&#36890;&#36807;&#33258;&#21160;&#31579;&#36873;&#20986;&#19981;&#30456;&#20851;&#25991;&#26412;&#21644;&#20351;&#29992;&#25991;&#26412;&#39135;&#35889;&#25968;&#25454;&#38598;&#20013;&#20154;&#31867;&#32534;&#20889;&#30340;&#35828;&#26126;&#33258;&#21160;&#26367;&#25442;&#25991;&#26412;&#36716;&#24405;&#20197;&#22686;&#24378;&#25991;&#23383;&#25351;&#20196;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15964v2 Announce Type: replace-cross  Abstract: Procedural videos show step-by-step demonstrations of tasks like recipe preparation. Understanding such videos is challenging, involving the precise localization of steps and the generation of textual instructions. Manually annotating steps and writing instructions is costly, which limits the size of current datasets and hinders effective learning. Leveraging large but noisy video-transcript datasets for pre-training can boost performance, but demands significant computational resources. Furthermore, transcripts contain irrelevant content and exhibit style variation compared to instructions written by human annotators. To mitigate both issues, we propose a technique, Sieve-&amp;-Swap, to automatically curate a smaller dataset: (i) Sieve filters irrelevant transcripts and (ii) Swap enhances the quality of the text instruction by automatically replacing the transcripts with human-written instructions from a text-only recipe dataset. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#23545;&#27604;&#23545;&#40784;&#25351;&#20196;&#65288;AlignInstruct&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#32479;&#35745;&#35789;&#23545;&#40784;&#26500;&#24314;&#30340;&#36328;&#35821;&#35328;&#37492;&#21035;&#22120;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30417;&#30563;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#20004;&#20010;&#25361;&#25112;&#65306;&#23558;&#25903;&#25345;&#30340;&#35821;&#35328;&#25193;&#23637;&#21040;&#26410;&#30693;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#25968;&#25454;&#32570;&#20047;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#36890;&#36807;MTInstruct&#21487;&#20197;&#26377;&#25928;&#22320;&#32763;&#35793;&#26410;&#30693;&#35821;&#35328;&#65292;&#24182;&#19988;&#20351;&#29992;AlignInstruct&#22312;&#28041;&#21450;&#33521;&#35821;&#30340;48&#20010;&#32763;&#35793;&#26041;&#21521;&#19978;&#33021;&#22815;&#25345;&#32493;&#25913;&#21892;&#32763;&#35793;&#36136;&#37327;&#12290;&#22522;&#20110;&#37492;&#21035;&#22120;&#30340;&#25351;&#20196;&#20248;&#20110;&#29983;&#25104;&#22411;&#25351;&#20196;&#12290;</title><link>http://arxiv.org/abs/2401.05811</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#23545;&#40784;&#25351;&#20196;&#35843;&#25972;LLMs&#20197;&#35299;&#20915;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#26410;&#30693;&#12289;&#20302;&#36164;&#28304;&#35821;&#35328;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Tuning LLMs with Contrastive Alignment Instructions for Machine Translation in Unseen, Low-resource Languages. (arXiv:2401.05811v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#23545;&#27604;&#23545;&#40784;&#25351;&#20196;&#65288;AlignInstruct&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#32479;&#35745;&#35789;&#23545;&#40784;&#26500;&#24314;&#30340;&#36328;&#35821;&#35328;&#37492;&#21035;&#22120;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30417;&#30563;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#20004;&#20010;&#25361;&#25112;&#65306;&#23558;&#25903;&#25345;&#30340;&#35821;&#35328;&#25193;&#23637;&#21040;&#26410;&#30693;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#25968;&#25454;&#32570;&#20047;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#36890;&#36807;MTInstruct&#21487;&#20197;&#26377;&#25928;&#22320;&#32763;&#35793;&#26410;&#30693;&#35821;&#35328;&#65292;&#24182;&#19988;&#20351;&#29992;AlignInstruct&#22312;&#28041;&#21450;&#33521;&#35821;&#30340;48&#20010;&#32763;&#35793;&#26041;&#21521;&#19978;&#33021;&#22815;&#25345;&#32493;&#25913;&#21892;&#32763;&#35793;&#36136;&#37327;&#12290;&#22522;&#20110;&#37492;&#21035;&#22120;&#30340;&#25351;&#20196;&#20248;&#20110;&#29983;&#25104;&#22411;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#27604;&#23545;&#40784;&#25351;&#20196;&#65288;AlignInstruct&#65289;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19978;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#20013;&#30340;&#20004;&#20010;&#25361;&#25112;&#12290;&#19968;&#20010;&#26159;&#23558;&#25903;&#25345;&#30340;&#35821;&#35328;&#25193;&#23637;&#21040;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#12290;&#31532;&#20108;&#20010;&#19982;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#32570;&#20047;&#25968;&#25454;&#26377;&#20851;&#12290;&#36890;&#36807;MT&#25351;&#20196;&#65288;MTInstruct&#65289;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26159;&#24212;&#23545;&#31532;&#19968;&#20010;&#25361;&#25112;&#30340;&#19968;&#31181;&#30452;&#25509;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;MTInstruct&#21463;&#21040;&#31532;&#20108;&#20010;&#25361;&#25112;&#20013;&#22266;&#26377;&#30340;&#24369;&#35821;&#35328;&#36328;&#24230;&#20449;&#21495;&#30340;&#38480;&#21046;&#12290;AlignInstruct&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#32479;&#35745;&#35789;&#23545;&#40784;&#26500;&#24314;&#30340;&#36328;&#35821;&#35328;&#37492;&#21035;&#22120;&#26469;&#24378;&#35843;&#36328;&#35821;&#35328;&#30417;&#30563;&#12290;&#25105;&#20204;&#22522;&#20110;&#22312;&#22810;&#36798;24&#31181;&#26410;&#30693;&#35821;&#35328;&#19978;&#23545;BLOOMZ&#27169;&#22411;&#65288;1b1&#12289;3b&#21644;7b1&#65289;&#36827;&#34892;&#24494;&#35843;&#30340;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;1&#65289;LLMs&#21487;&#20197;&#20351;&#29992;MTInstruct&#26377;&#25928;&#22320;&#32763;&#35793;&#26410;&#30693;&#35821;&#35328;&#65307;&#65288;2&#65289;AlignInstruct&#22312;&#28041;&#21450;&#33521;&#35821;&#30340;48&#20010;&#32763;&#35793;&#26041;&#21521;&#19978;&#25552;&#39640;&#20102;&#32763;&#35793;&#36136;&#37327;&#30340;&#19968;&#33268;&#24615;&#65307;&#65288;3&#65289;&#22522;&#20110;&#37492;&#21035;&#22120;&#30340;&#25351;&#20196;&#20248;&#20110;&#29983;&#25104;&#22411;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article introduces contrastive alignment instructions (AlignInstruct) to address two challenges in machine translation (MT) on large language models (LLMs). One is the expansion of supported languages to previously unseen ones. The second relates to the lack of data in low-resource languages. Model fine-tuning through MT instructions (MTInstruct) is a straightforward approach to the first challenge. However, MTInstruct is limited by weak cross-lingual signals inherent in the second challenge. AlignInstruct emphasizes cross-lingual supervision via a cross-lingual discriminator built using statistical word alignments. Our results based on fine-tuning the BLOOMZ models (1b1, 3b, and 7b1) in up to 24 unseen languages showed that: (1) LLMs can effectively translate unseen languages using MTInstruct; (2) AlignInstruct led to consistent improvements in translation quality across 48 translation directions involving English; (3) Discriminator-based instructions outperformed their generativ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Cross-Speaker Encoding&#65288;CSE&#65289;&#30340;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#35828;&#35805;&#20154;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#36890;&#36807;&#32858;&#21512;&#36328;&#35828;&#35805;&#20154;&#34920;&#31034;&#12290;&#36890;&#36807;&#19982;SOT&#32467;&#21512;&#65292;&#35813;&#27169;&#22411;&#22312;&#20004;&#20010;&#35828;&#35805;&#20154;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#27604;SIMO&#22522;&#20934;&#27169;&#22411;&#30340;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#20998;&#21035;&#38477;&#20302;&#20102;8%&#21644;10%&#12290;</title><link>http://arxiv.org/abs/2401.04152</link><description>&lt;p&gt;
&#36328;&#35828;&#35805;&#20154;&#32534;&#30721;&#32593;&#32476;&#29992;&#20110;&#22810;&#35828;&#35805;&#20154;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Cross-Speaker Encoding Network for Multi-Talker Speech Recognition. (arXiv:2401.04152v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Cross-Speaker Encoding&#65288;CSE&#65289;&#30340;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#35828;&#35805;&#20154;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#36890;&#36807;&#32858;&#21512;&#36328;&#35828;&#35805;&#20154;&#34920;&#31034;&#12290;&#36890;&#36807;&#19982;SOT&#32467;&#21512;&#65292;&#35813;&#27169;&#22411;&#22312;&#20004;&#20010;&#35828;&#35805;&#20154;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#27604;SIMO&#22522;&#20934;&#27169;&#22411;&#30340;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#20998;&#21035;&#38477;&#20302;&#20102;8%&#21644;10%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#30340;&#22810;&#35828;&#35805;&#20154;&#35821;&#38899;&#35782;&#21035;&#24050;&#32463;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#65292;&#20316;&#20026;&#19968;&#31181;&#30452;&#25509;&#36716;&#24405;&#22810;&#20010;&#35828;&#35805;&#20154;&#37325;&#21472;&#35821;&#38899;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;1&#65289;&#24102;&#26377;&#20998;&#25903;&#32534;&#30721;&#22120;&#30340;&#21333;&#36755;&#20837;&#22810;&#36755;&#20986;&#65288;SIMO&#65289;&#27169;&#22411;&#65292;&#25110;&#32773;2&#65289;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#21644;&#24207;&#21015;&#21270;&#36755;&#20986;&#35757;&#32451;&#65288;SOT&#65289;&#30340;&#21333;&#36755;&#20837;&#21333;&#36755;&#20986;&#65288;SISO&#65289;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Cross-Speaker Encoding&#65288;CSE&#65289;&#30340;&#32593;&#32476;&#26469;&#35299;&#20915;SIMO&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#36890;&#36807;&#32858;&#21512;&#36328;&#35828;&#35805;&#20154;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;CSE&#27169;&#22411;&#19982;SOT&#30456;&#32467;&#21512;&#65292;&#26082;&#21457;&#25381;&#20102;SIMO&#21644;SISO&#30340;&#20248;&#21183;&#65292;&#21448;&#32531;&#35299;&#20102;&#23427;&#20204;&#30340;&#32570;&#28857;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#35813;&#24037;&#20316;&#20195;&#34920;&#20102;&#23558;SIMO&#21644;SISO&#38598;&#25104;&#21040;&#22810;&#35828;&#35805;&#20154;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#26089;&#26399;&#24037;&#20316;&#12290;&#22312;&#20004;&#20010;&#35828;&#35805;&#20154;&#30340;LibrispeechMix&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CES&#27169;&#22411;&#30456;&#27604;&#20110;SIMO&#22522;&#20934;&#27169;&#22411;&#23558;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#38477;&#20302;&#20102;8%&#12290;CSE-SOT&#27169;&#22411;&#23558;WER&#38477;&#20302;&#20102;10%
&lt;/p&gt;
&lt;p&gt;
End-to-end multi-talker speech recognition has garnered great interest as an effective approach to directly transcribe overlapped speech from multiple speakers. Current methods typically adopt either 1) single-input multiple-output (SIMO) models with a branched encoder, or 2) single-input single-output (SISO) models based on attention-based encoder-decoder architecture with serialized output training (SOT). In this work, we propose a Cross-Speaker Encoding (CSE) network to address the limitations of SIMO models by aggregating cross-speaker representations. Furthermore, the CSE model is integrated with SOT to leverage both the advantages of SIMO and SISO while mitigating their drawbacks. To the best of our knowledge, this work represents an early effort to integrate SIMO and SISO for multi-talker speech recognition. Experiments on the two-speaker LibrispeechMix dataset show that the CES model reduces word error rate (WER) by 8% over the SIMO baseline. The CSE-SOT model reduces WER by 10
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22270;&#28789;&#30340;&#32654;&#20029;&#30340;&#24605;&#32500;&#23454;&#39564;&#36827;&#34892;&#20102;&#21382;&#21490;&#37325;&#24314;&#65292;&#25552;&#20379;&#20102;&#22823;&#37327;&#35777;&#25454;&#21644;&#19968;&#20123;&#21407;&#21019;&#31572;&#26696;&#65292;&#21516;&#26102;&#22238;&#31572;&#20102;&#22270;&#28789;&#27979;&#35797;&#30340;&#26680;&#24515;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.00009</link><description>&lt;p&gt;
&#22270;&#28789;&#27979;&#35797;&#65292;&#19968;&#20010;&#32654;&#20029;&#30340;&#24605;&#32500;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Turing's Test, a Beautiful Thought Experiment. (arXiv:2401.00009v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22270;&#28789;&#30340;&#32654;&#20029;&#30340;&#24605;&#32500;&#23454;&#39564;&#36827;&#34892;&#20102;&#21382;&#21490;&#37325;&#24314;&#65292;&#25552;&#20379;&#20102;&#22823;&#37327;&#35777;&#25454;&#21644;&#19968;&#20123;&#21407;&#21019;&#31572;&#26696;&#65292;&#21516;&#26102;&#22238;&#31572;&#20102;&#22270;&#28789;&#27979;&#35797;&#30340;&#26680;&#24515;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#28010;&#28526;&#20013;&#65292;&#20851;&#20110;&#22270;&#28789;&#27979;&#35797;&#21450;&#20854;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#20215;&#20540;&#30340;&#20105;&#35770;&#21644;&#38382;&#39064;&#37325;&#26032;&#20852;&#36215;&#65292;&#24182;&#24341;&#21457;&#20102;&#25968;&#21313;&#24180;&#26469;&#23454;&#38469;&#30340;&#8220;&#22270;&#28789;&#8221;&#27979;&#35797;&#12290;&#22914;&#26524;&#20154;&#24037;&#26234;&#33021;&#26159;&#37327;&#23376;&#29289;&#29702;&#23398;&#65292;&#29616;&#22312;&#21487;&#33021;&#24050;&#26377;&#20960;&#21482;&#8220;&#34203;&#23450;&#35860;&#30340;&#8221;&#29483;&#34987;&#26432;&#27515;&#20102;&#12290;&#36831;&#21040;&#24635;&#27604;&#19981;&#21040;&#22909;&#65292;&#29616;&#22312;&#26159;&#23545;&#22270;&#28789;&#32654;&#20029;&#30340;&#24605;&#32500;&#23454;&#39564;&#36827;&#34892;&#21382;&#21490;&#37325;&#24314;&#30340;&#26102;&#20505;&#20102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#25552;&#20379;&#20102;&#22823;&#37327;&#35777;&#25454;&#65292;&#21253;&#25324;&#26032;&#30340;&#26723;&#26696;&#26469;&#28304;&#65292;&#38024;&#23545;&#22270;&#28789;1950&#24180;&#35770;&#25991;&#30340;&#20960;&#20010;&#26410;&#35299;&#20915;&#38382;&#39064;&#32473;&#20986;&#20102;&#21407;&#21019;&#31572;&#26696;&#65292;&#24182;&#22238;&#31572;&#20102;&#22270;&#28789;&#27979;&#35797;&#30340;&#26680;&#24515;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the wake of large language models, there has been a resurgence of claims and questions about the Turing test and its value for AI, which are reminiscent of decades of practical "Turing" tests. If AI were quantum physics, by now several "Schr\"odinger's" cats could have been killed. Better late than never, it is time for a historical reconstruction of Turing's beautiful thought experiment. In this paper I present a wealth of evidence, including new archival sources, give original answers to several open questions about Turing's 1950 paper, and address the core question of the value of Turing's test.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#36229;&#20687;&#32032;&#32467;&#26500;&#24046;&#24322;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;MDGCN&#65289;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#34920;&#24449;&#65292;&#36890;&#36807;&#32858;&#31867;&#24863;&#30693;&#30456;&#20284;&#20687;&#32032;&#65292;&#20943;&#23569;&#20102;&#21518;&#32493;&#22788;&#29702;&#30340;&#35270;&#35273;&#22522;&#20803;&#25968;&#37327;&#65292;&#24182;&#25366;&#25496;&#20102;&#26356;&#31934;&#30830;&#30340;&#25299;&#25169;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.13447</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#36229;&#20687;&#32032;&#32467;&#26500;&#24046;&#24322;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
Multiscale Superpixel Structured Difference Graph Convolutional Network for VL Representation. (arXiv:2310.13447v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#36229;&#20687;&#32032;&#32467;&#26500;&#24046;&#24322;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;MDGCN&#65289;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#34920;&#24449;&#65292;&#36890;&#36807;&#32858;&#31867;&#24863;&#30693;&#30456;&#20284;&#20687;&#32032;&#65292;&#20943;&#23569;&#20102;&#21518;&#32493;&#22788;&#29702;&#30340;&#35270;&#35273;&#22522;&#20803;&#25968;&#37327;&#65292;&#24182;&#25366;&#25496;&#20102;&#26356;&#31934;&#30830;&#30340;&#25299;&#25169;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#39046;&#22495;&#20013;&#65292;&#25972;&#21512;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#20851;&#38190;&#22312;&#20110;&#24314;&#31435;&#19968;&#20010;&#33391;&#22909;&#30340;&#23545;&#40784;&#31574;&#30053;&#12290;&#26368;&#36817;&#65292;&#21463;&#21040;&#33258;&#30417;&#30563;&#23398;&#20064;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#22810;&#27169;&#24577;&#35821;&#20041;&#34920;&#24449;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#35270;&#35273;&#35821;&#20041;&#34920;&#24449;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#24403;&#21069;&#22522;&#20110;&#20687;&#32032;&#25110;&#22359;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#25552;&#21462;&#22797;&#26434;&#22330;&#26223;&#36793;&#30028;&#26041;&#38754;&#23384;&#22312;&#31354;&#38388;&#35821;&#20041;&#36830;&#36143;&#24615;&#19981;&#36275;&#21644;&#23545;&#22122;&#22768;&#30340;&#33030;&#24369;&#24615;&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#23558;&#36229;&#20687;&#32032;&#20316;&#20026;&#21487;&#23398;&#20064;&#22270;&#20687;&#25968;&#25454;&#30340;&#32508;&#21512;&#32039;&#20945;&#34920;&#24449;&#65292;&#36890;&#36807;&#23545;&#24863;&#30693;&#30456;&#20284;&#20687;&#32032;&#36827;&#34892;&#32858;&#31867;&#65292;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#21518;&#32493;&#22788;&#29702;&#30340;&#35270;&#35273;&#22522;&#20803;&#25968;&#37327;&#12290;&#20026;&#20102;&#25366;&#25496;&#26356;&#31934;&#30830;&#30340;&#25299;&#25169;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#24046;&#24322;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;MDGCN&#65289;&#12290;&#23427;&#23558;&#25972;&#20010;&#22270;&#20687;&#35299;&#26512;&#20026;&#32454;&#21040;&#31895;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25972;&#20010;&#22270;&#20687;&#30340;&#35299;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Within the multimodal field, the key to integrating vision and language lies in establishing a good alignment strategy. Recently, benefiting from the success of self-supervised learning, significant progress has been made in multimodal semantic representation based on pre-trained models for vision and language. However, there is still room for improvement in visual semantic representation. The lack of spatial semantic coherence and vulnerability to noise makes it challenging for current pixel or patch-based methods to accurately extract complex scene boundaries. To this end, this paper develops superpixel as a comprehensive compact representation of learnable image data, which effectively reduces the number of visual primitives for subsequent processing by clustering perceptually similar pixels. To mine more precise topological relations, we propose a Multiscale Difference Graph Convolutional Network (MDGCN). It parses the entire image as a fine-to-coarse hierarchical structure of cons
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#31867;&#28608;&#27963;&#22270;&#25216;&#26415;&#65292;USL-Net&#25552;&#20379;&#20102;&#19968;&#31181;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#25351;&#23548;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20998;&#21106;&#21508;&#31181;&#30382;&#32932;&#30149;&#21464;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2309.13289</link><description>&lt;p&gt;
USL-Net&#65306;&#29992;&#20110;&#26080;&#30417;&#30563;&#30382;&#32932;&#30149;&#21464;&#20998;&#21106;&#30340;&#19981;&#30830;&#23450;&#24615;&#33258;&#23398;&#20064;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
USL-Net: Uncertainty Self-Learning Network for Unsupervised Skin Lesion Segmentation. (arXiv:2309.13289v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13289
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#31867;&#28608;&#27963;&#22270;&#25216;&#26415;&#65292;USL-Net&#25552;&#20379;&#20102;&#19968;&#31181;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#25351;&#23548;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20998;&#21106;&#21508;&#31181;&#30382;&#32932;&#30149;&#21464;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#30382;&#32932;&#30149;&#21464;&#20998;&#21106;&#20855;&#26377;&#22810;&#31181;&#22909;&#22788;&#65292;&#21253;&#25324;&#33410;&#32422;&#19987;&#23478;&#20154;&#21147;&#36164;&#28304;&#12289;&#20943;&#23569;&#20027;&#35266;&#20154;&#24037;&#26631;&#27880;&#24341;&#36215;&#30340;&#24046;&#24322;&#20197;&#21450;&#36866;&#24212;&#26032;&#29615;&#22659;&#12290;&#28982;&#32780;&#65292;&#22312;&#27809;&#26377;&#25163;&#21160;&#26631;&#27880;&#25351;&#23548;&#30340;&#24773;&#20917;&#19979;&#20998;&#21106;&#30382;&#32932;&#38236;&#22270;&#20687;&#23384;&#22312;&#26174;&#33879;&#25361;&#25112;&#65292;&#22914;&#27611;&#21457;&#22122;&#22768;&#12289;&#27700;&#30129;&#22122;&#22768;&#21644;&#32454;&#24494;&#36793;&#32536;&#24046;&#24322;&#31561;&#30382;&#32932;&#38236;&#22270;&#20687;&#20266;&#24433;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#33258;&#23398;&#20064;&#32593;&#32476;&#65288;USL-Net&#65289;&#29992;&#20110;&#30382;&#32932;&#30149;&#21464;&#20998;&#21106;&#12290;USL-Net&#33021;&#22815;&#26377;&#25928;&#22320;&#20998;&#21106;&#21508;&#31181;&#30149;&#21464;&#65292;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#25351;&#23548;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#25552;&#21462;&#29305;&#24449;&#65292;&#28982;&#21518;&#20351;&#29992;&#36825;&#20123;&#29305;&#24449;&#29983;&#25104;&#31867;&#28608;&#27963;&#22270;&#65288;CAMs&#65289;&#20316;&#20026;&#26174;&#33879;&#22270;&#12290;&#19981;&#21516;&#30340;CAM&#20301;&#32622;&#23545;&#24212;&#20110;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#30149;&#21464;&#21306;&#22495;&#30340;&#37325;&#35201;&#24615;&#12290;&#22320;&#22270;&#20013;&#30340;&#39640;&#26174;&#33879;&#21306;&#22495;&#29992;&#20316;&#30149;&#21464;&#21306;&#22495;&#30340;&#20266;&#26631;&#31614;&#65292;&#32780;&#20302;&#26174;&#33879;&#21306;&#22495;&#29992;&#20316;&#38750;&#30149;&#21464;&#21306;&#22495;&#30340;&#20266;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised skin lesion segmentation offers several benefits, including conserving expert human resources, reducing discrepancies due to subjective human labeling, and adapting to novel environments. However, segmenting dermoscopic images without manual labeling guidance presents significant challenges due to dermoscopic image artifacts such as hair noise, blister noise, and subtle edge differences. To address these challenges, we introduce an innovative Uncertainty Self-Learning Network (USL-Net) designed for skin lesion segmentation. The USL-Net can effectively segment a range of lesions, eliminating the need for manual labeling guidance. Initially, features are extracted using contrastive learning, followed by the generation of Class Activation Maps (CAMs) as saliency maps using these features. The different CAM locations correspond to the importance of the lesion region based on their saliency. High-saliency regions in the map serve as pseudo-labels for lesion regions while low-sa
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#19968;&#20010;&#35780;&#20998;&#27979;&#35797;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29702;&#35299;&#20154;&#31867;&#24494;&#22937;&#20132;&#27969;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;&#38543;&#30528;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;LLMs&#23545;&#38544;&#21947;&#29702;&#35299;&#33021;&#21147;&#26377;&#25152;&#25913;&#21892;&#65292;&#20294;&#23545;&#35773;&#21050;&#29702;&#35299;&#33021;&#21147;&#30340;&#25913;&#36827;&#24182;&#26410;&#35266;&#23519;&#21040;&#12290;</title><link>http://arxiv.org/abs/2309.10744</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#20122;&#26031;&#20271;&#26684;&#32508;&#21512;&#24449;&#31579;&#36873;&#27979;&#35797;&#29702;&#35299;&#38544;&#21947;&#21644;&#35773;&#21050;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating large language models' ability to understand metaphor and sarcasm using a screening test for Asperger syndrome. (arXiv:2309.10744v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10744
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#19968;&#20010;&#35780;&#20998;&#27979;&#35797;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29702;&#35299;&#20154;&#31867;&#24494;&#22937;&#20132;&#27969;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;&#38543;&#30528;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;LLMs&#23545;&#38544;&#21947;&#29702;&#35299;&#33021;&#21147;&#26377;&#25152;&#25913;&#21892;&#65292;&#20294;&#23545;&#35773;&#21050;&#29702;&#35299;&#33021;&#21147;&#30340;&#25913;&#36827;&#24182;&#26410;&#35266;&#23519;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#21947;&#21644;&#35773;&#21050;&#26159;&#25105;&#20204;&#39640;&#24230;&#36827;&#21270;&#30340;&#31038;&#20132;&#27807;&#36890;&#25216;&#24039;&#30340;&#29645;&#36149;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#20122;&#26031;&#20271;&#26684;&#32508;&#21512;&#24449;&#30340;&#20799;&#31461;&#20247;&#25152;&#21608;&#30693;&#22312;&#29702;&#35299;&#35773;&#21050;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#21363;&#20351;&#20182;&#20204;&#20855;&#26377;&#36275;&#22815;&#29702;&#35299;&#38544;&#21947;&#30340;&#21475;&#35821;&#26234;&#21830;&#27700;&#24179;&#12290;&#37492;&#20110;&#27492;&#65292;&#24050;&#32463;&#20351;&#29992;&#20102;&#19968;&#20010;&#35780;&#20998;&#27979;&#35797;&#26469;&#35780;&#20272;&#29702;&#35299;&#38544;&#21947;&#21644;&#35773;&#21050;&#30340;&#33021;&#21147;&#65292;&#20197;&#21306;&#20998;&#20122;&#26031;&#20271;&#26684;&#32508;&#21512;&#24449;&#21644;&#20854;&#20182;&#34920;&#29616;&#30456;&#20284;&#22806;&#37096;&#34892;&#20026;&#30340;&#30151;&#29366;&#65288;&#20363;&#22914;&#27880;&#24847;&#21147;&#32570;&#38519;/&#22810;&#21160;&#38556;&#30861;&#65289;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#26631;&#20934;&#21270;&#27979;&#35797;&#26469;&#30740;&#31350;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29702;&#35299;&#20154;&#31867;&#24494;&#22937;&#20132;&#27969;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#38543;&#30528;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#29702;&#35299;&#38544;&#21947;&#30340;&#33021;&#21147;&#24471;&#21040;&#20102;&#25913;&#21892;&#65292;&#20294;&#24182;&#27809;&#26377;&#35266;&#23519;&#21040;&#23545;&#35773;&#21050;&#29702;&#35299;&#30340;&#25913;&#36827;&#12290;&#36825;&#24847;&#21619;&#30528;&#26377;&#24517;&#35201;&#37319;&#21462;&#20854;&#20182;&#26041;&#27861;&#26469;&#20351;LLMs&#20855;&#22791;&#29702;&#35299;&#35773;&#21050;&#30340;&#33021;&#21147;&#65292;&#36825;&#24050;&#19982;&#20122;&#26031;&#20271;&#26684;&#32508;&#21512;&#24449;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Metaphors and sarcasm are precious fruits of our highly-evolved social communication skills. However, children with Asperger syndrome are known to have difficulties in comprehending sarcasm, even if they possess a certain level of verbal IQ sufficient for understanding metaphors. Given that, a screening test that scores the ability to understand metaphor and sarcasm has been used to differentiate Asperger syndrome from other symptoms exhibiting akin external behaviors (e.g., attention-deficit/hyperactivity disorder). This study uses the standardized test to examine the capability of recent large language models (LLMs) in understanding human nuanced communication. The results divulged that, whereas their ability to comprehend metaphors has been improved with the increase of the number of model parameters, the improvement in sarcasm understanding was not observed. This implies that an alternative approach is imperative to imbue LLMs with the capacity to grasp sarcasm, which has been asso
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20026;&#22797;&#26434;&#26426;&#22120;&#20154;&#31995;&#32479;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#22870;&#21169;&#21644;&#32422;&#26463;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#35774;&#35745;&#39640;&#25928;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#26469;&#22788;&#29702;&#32422;&#26463;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#12290;&#36890;&#36807;&#24212;&#29992;&#20110;&#19981;&#21516;&#33151;&#24335;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#25511;&#21046;&#22120;&#35757;&#32451;&#20013;&#65292;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.12517</link><description>&lt;p&gt;
&#19981;&#20165;&#20165;&#22870;&#21169;&#65292;&#36824;&#26377;&#32422;&#26463;&#65306;&#29992;&#20110;&#33151;&#24335;&#26426;&#22120;&#20154;&#36816;&#21160;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Not Only Rewards But Also Constraints: Applications on Legged Robot Locomotion. (arXiv:2308.12517v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20026;&#22797;&#26434;&#26426;&#22120;&#20154;&#31995;&#32479;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#22870;&#21169;&#21644;&#32422;&#26463;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#35774;&#35745;&#39640;&#25928;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#26469;&#22788;&#29702;&#32422;&#26463;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#12290;&#36890;&#36807;&#24212;&#29992;&#20110;&#19981;&#21516;&#33151;&#24335;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#25511;&#21046;&#22120;&#35757;&#32451;&#20013;&#65292;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#30340;&#19968;&#20123;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#24182;&#20351;&#29992;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26469;&#35757;&#32451;&#65292;&#23637;&#31034;&#20102;&#22797;&#26434;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25511;&#21046;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20855;&#26377;&#33258;&#28982;&#21160;&#20316;&#39118;&#26684;&#21644;&#39640;&#20219;&#21153;&#24615;&#33021;&#30340;&#20986;&#33394;&#25511;&#21046;&#22120;&#26159;&#36890;&#36807;&#36827;&#34892;&#22823;&#37327;&#22870;&#21169;&#24037;&#31243;&#32780;&#24320;&#21457;&#30340;&#65292;&#35813;&#36807;&#31243;&#38750;&#24120;&#36153;&#26102;&#36153;&#21147;&#65292;&#38656;&#35201;&#35774;&#35745;&#22823;&#37327;&#22870;&#21169;&#39033;&#24182;&#30830;&#23450;&#21512;&#36866;&#30340;&#22870;&#21169;&#31995;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#21516;&#26102;&#21253;&#21547;&#22870;&#21169;&#21644;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#12290;&#20026;&#20102;&#35753;&#24037;&#31243;&#24072;&#33021;&#22815;&#36866;&#24403;&#22320;&#21453;&#26144;&#20182;&#20204;&#23545;&#32422;&#26463;&#30340;&#24847;&#22270;&#24182;&#20197;&#26368;&#23567;&#30340;&#35745;&#31639;&#24320;&#38144;&#22788;&#29702;&#23427;&#20204;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#32422;&#26463;&#31867;&#22411;&#21644;&#19968;&#31181;&#39640;&#25928;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#12290;&#35813;&#23398;&#20064;&#26694;&#26550;&#34987;&#24212;&#29992;&#20110;&#35757;&#32451;&#19981;&#21516;&#24418;&#24577;&#21644;&#29289;&#29702;&#23646;&#24615;&#30340;&#20960;&#20010;&#33151;&#24335;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several earlier studies have shown impressive control performance in complex robotic systems by designing the controller using a neural network and training it with model-free reinforcement learning. However, these outstanding controllers with natural motion style and high task performance are developed through extensive reward engineering, which is a highly laborious and time-consuming process of designing numerous reward terms and determining suitable reward coefficients. In this work, we propose a novel reinforcement learning framework for training neural network controllers for complex robotic systems consisting of both rewards and constraints. To let the engineers appropriately reflect their intent to constraints and handle them with minimal computation overhead, two constraint types and an efficient policy optimization algorithm are suggested. The learning framework is applied to train locomotion controllers for several legged robots with different morphology and physical attribu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35266;&#30475;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;SurgVLP&#65292;&#36890;&#36807;&#21033;&#29992;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#20013;&#30340;&#35821;&#38899;&#21644;&#35270;&#35273;&#20449;&#24687;&#36827;&#34892;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#35299;&#20915;&#20102;&#25163;&#26415;&#30456;&#20851;&#35821;&#35328;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.15220</link><description>&lt;p&gt;
&#36890;&#36807;&#35266;&#30475;&#25968;&#30334;&#20010;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#23398;&#20064;&#22810;&#27169;&#24577;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Multi-modal Representations by Watching Hundreds of Surgical Video Lectures. (arXiv:2307.15220v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15220
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35266;&#30475;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;SurgVLP&#65292;&#36890;&#36807;&#21033;&#29992;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#20013;&#30340;&#35821;&#38899;&#21644;&#35270;&#35273;&#20449;&#24687;&#36827;&#34892;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#35299;&#20915;&#20102;&#25163;&#26415;&#30456;&#20851;&#35821;&#35328;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22806;&#31185;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#26041;&#38754;&#30340;&#36827;&#23637;&#20027;&#35201;&#20381;&#38752;&#23436;&#20840;&#30417;&#30563;&#26041;&#27861;&#65292;&#20027;&#35201;&#20351;&#29992;&#35270;&#35273;&#25968;&#25454;&#12290;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#30340;&#25163;&#26415;&#35270;&#39057;&#26469;&#39044;&#27979;&#19968;&#32452;&#22266;&#23450;&#30340;&#23545;&#35937;&#31867;&#21035;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26410;&#35265;&#25163;&#26415;&#31243;&#24207;&#21644;&#21518;&#32493;&#20219;&#21153;&#19978;&#30340;&#36890;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35266;&#28857;&#65292;&#21363;&#36890;&#36807;&#24320;&#25918;&#30340;&#25163;&#26415;&#30005;&#23376;&#23398;&#20064;&#24179;&#21488;&#25552;&#20379;&#30340;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#21487;&#20197;&#20026;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#25552;&#20379;&#26377;&#25928;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#25163;&#21160;&#27880;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#20114;&#34917;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#29983;&#25104;&#25991;&#26412;&#36716;&#24405;&#26469;&#35299;&#20915;&#25163;&#26415;&#35270;&#39057;&#35762;&#24231;&#20013;&#23384;&#22312;&#30340;&#25163;&#26415;&#30456;&#20851;&#35821;&#35328;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;SurgVLP - &#25163;&#26415;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#12290;SurgVLP&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#65292;&#23558;&#35270;&#39057;&#21098;&#36753;&#23884;&#20837;&#19982;&#30456;&#24212;&#30340;&#25991;&#26412;&#23884;&#20837;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in surgical computer vision applications have been driven by fully-supervised methods, primarily using only visual data. These methods rely on manually annotated surgical videos to predict a fixed set of object categories, limiting their generalizability to unseen surgical procedures and downstream tasks. In this work, we put forward the idea that the surgical video lectures available through open surgical e-learning platforms can provide effective supervisory signals for multi-modal representation learning without relying on manual annotations. We address the surgery-specific linguistic challenges present in surgical video lectures by employing multiple complementary automatic speech recognition systems to generate text transcriptions. We then present a novel method, SurgVLP - Surgical Vision Language Pre-training, for multi-modal representation learning. SurgVLP constructs a new contrastive learning objective to align video clip embeddings with the corresponding m
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Q-Ensembles&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#21152;Q&#32593;&#32476;&#30340;&#25968;&#37327;&#65292;&#26080;&#32541;&#22320;&#36830;&#25509;&#31163;&#32447;&#39044;&#35757;&#32451;&#21644;&#22312;&#32447;&#24494;&#35843;&#65292;&#21516;&#26102;&#19981;&#38477;&#20302;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36866;&#24403;&#25918;&#23485;Q&#20540;&#20272;&#35745;&#30340;&#24754;&#35266;&#24615;&#65292;&#24182;&#23558;&#22522;&#20110;&#38598;&#21512;&#30340;&#25506;&#32034;&#26426;&#21046;&#34701;&#20837;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.06871</link><description>&lt;p&gt;
&#25552;&#21319;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;Q-Ensembles&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Offline-to-Online Reinforcement Learning with Q-Ensembles. (arXiv:2306.06871v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06871
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Q-Ensembles&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#21152;Q&#32593;&#32476;&#30340;&#25968;&#37327;&#65292;&#26080;&#32541;&#22320;&#36830;&#25509;&#31163;&#32447;&#39044;&#35757;&#32451;&#21644;&#22312;&#32447;&#24494;&#35843;&#65292;&#21516;&#26102;&#19981;&#38477;&#20302;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36866;&#24403;&#25918;&#23485;Q&#20540;&#20272;&#35745;&#30340;&#24754;&#35266;&#24615;&#65292;&#24182;&#23558;&#22522;&#20110;&#38598;&#21512;&#30340;&#25506;&#32034;&#26426;&#21046;&#34701;&#20837;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#23398;&#20064;&#33539;&#24335;&#65292;&#20195;&#29702;&#26681;&#25454;&#22266;&#23450;&#30340;&#32463;&#39564;&#25968;&#25454;&#38598;&#36827;&#34892;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#20165;&#20174;&#38745;&#24577;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#33021;&#38480;&#21046;&#20102;&#24615;&#33021;&#65292;&#22240;&#20026;&#32570;&#20047;&#25506;&#32034;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#23558;&#31163;&#32447;&#39044;&#35757;&#32451;&#19982;&#22312;&#32447;&#24494;&#35843;&#32467;&#21512;&#36215;&#26469;&#30340;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#35753;&#20195;&#29702;&#19982;&#29615;&#22659;&#23454;&#26102;&#20132;&#20114;&#65292;&#36827;&#19968;&#27493;&#23436;&#21892;&#20854;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23384;&#22312;&#24615;&#33021;&#19979;&#38477;&#21644;&#22312;&#32447;&#38454;&#27573;&#25913;&#36827;&#32531;&#24930;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Q-Ensembles&#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#22686;&#21152;Q&#32593;&#32476;&#30340;&#25968;&#37327;&#65292;&#26080;&#32541;&#22320;&#36830;&#25509;&#31163;&#32447;&#39044;&#35757;&#32451;&#21644;&#22312;&#32447;&#24494;&#35843;&#65292;&#21516;&#26102;&#19981;&#38477;&#20302;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#21152;&#24555;&#22312;&#32447;&#24615;&#33021;&#25552;&#21319;&#65292;&#25105;&#20204;&#36866;&#24403;&#25918;&#23485;Q&#20540;&#20272;&#35745;&#30340;&#24754;&#35266;&#24615;&#65292;&#24182;&#23558;&#22522;&#20110;&#38598;&#21512;&#30340;&#25506;&#32034;&#26426;&#21046;&#34701;&#20837;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) is a learning paradigm where an agent learns from a fixed dataset of experience. However, learning solely from a static dataset can limit the performance due to the lack of exploration. To overcome it, offline-to-online RL combines offline pre-training with online fine-tuning, which enables the agent to further refine its policy by interacting with the environment in real-time. Despite its benefits, existing offline-to-online RL methods suffer from performance degradation and slow improvement during the online phase. To tackle these challenges, we propose a novel framework called Ensemble-based Offline-to-Online (E2O) RL. By increasing the number of Q-networks, we seamlessly bridge offline pre-training and online fine-tuning without degrading performance. Moreover, to expedite online performance enhancement, we appropriately loosen the pessimism of Q-value estimation and incorporate ensemble-based exploration mechanisms into our framework. Experiment
&lt;/p&gt;</description></item><item><title>BiomedGPT&#26159;&#19968;&#31181;&#38754;&#21521;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#36890;&#29992;&#29983;&#29289;&#21307;&#23398;&#29983;&#25104;&#39044;&#35757;&#32451;Transformer&#65292;&#22312;&#22810;&#20010;&#20020;&#24202;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;16&#20010;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#65292;&#21253;&#25324;&#36229;&#36807;&#20102;OpenAI&#30340;GPT-4V&#21644;Google&#30340;Med-PaLM M&#65288;12B&#65289;&#12290;&#21516;&#26102;&#65292;BiomedGPT&#36824;&#25903;&#25345;&#38646;-shot&#36801;&#31227;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.17100</link><description>&lt;p&gt;
BiomedGPT&#65306;&#19968;&#31181;&#38754;&#21521;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#32479;&#19968;&#19988;&#36890;&#29992;&#30340;&#29983;&#29289;&#21307;&#23398;&#29983;&#25104;&#39044;&#35757;&#32451;Transformer
&lt;/p&gt;
&lt;p&gt;
BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks. (arXiv:2305.17100v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17100
&lt;/p&gt;
&lt;p&gt;
BiomedGPT&#26159;&#19968;&#31181;&#38754;&#21521;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#36890;&#29992;&#29983;&#29289;&#21307;&#23398;&#29983;&#25104;&#39044;&#35757;&#32451;Transformer&#65292;&#22312;&#22810;&#20010;&#20020;&#24202;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;16&#20010;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#65292;&#21253;&#25324;&#36229;&#36807;&#20102;OpenAI&#30340;GPT-4V&#21644;Google&#30340;Med-PaLM M&#65288;12B&#65289;&#12290;&#21516;&#26102;&#65292;BiomedGPT&#36824;&#25903;&#25345;&#38646;-shot&#36801;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#20219;&#21153;&#21644;&#27169;&#24577;&#29305;&#23450;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#23454;&#38469;&#24212;&#29992;&#21644;&#32500;&#25252;&#20013;&#19981;&#22815;&#28789;&#27963;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#32467;&#21512;&#29616;&#20195;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#36827;&#23637;&#65292;&#20026;&#36890;&#29992;&#30340;&#29983;&#29289;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#30340;&#20986;&#29616;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#26377;&#28508;&#21147;&#35299;&#37322;&#19981;&#21516;&#30340;&#21307;&#30103;&#27169;&#24577;&#65292;&#24182;&#20135;&#29983;&#22914;&#33258;&#30001;&#25991;&#26412;&#25253;&#21578;&#25110;&#30142;&#30149;&#35786;&#26029;&#31561;&#34920;&#36798;&#24615;&#36755;&#20986;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;BiomedGPT&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38754;&#21521;&#22810;&#26679;&#21270;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#30340;&#24320;&#28304;&#36890;&#29992;&#35270;&#35273;&#35821;&#35328;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#12290;BiomedGPT&#22312;26&#20010;&#25968;&#25454;&#38598;&#30340;&#20116;&#20010;&#20020;&#24202;&#37325;&#35201;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;16&#20010;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#25918;&#23556;&#23398;&#20154;&#21592;&#35780;&#20272;&#20013;&#65292;&#23427;&#36229;&#36234;&#20102;OpenAI&#30340;GPT-4 with vision&#65288;GPT-4V&#65289;&#65292;&#24182;&#22312;&#20083;&#33146;&#30284;&#35786;&#26029;&#21644;&#21307;&#23398;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#26041;&#38754;&#36229;&#36807;&#20102;Google&#30340;Med-PaLM M&#65288;12B&#65289;&#12290;&#27492;&#22806;&#65292;BiomedGPT&#36824;&#25903;&#25345;&#38646;-shot&#36801;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional task- and modality-specific artificial intelligence (AI) models are inflexible in real-world deployment and maintenance for biomedicine. At the same time, the growing availability of biomedical data, coupled with the advancements in modern multi-modal multi-task AI techniques, has paved the way for the emergence of generalist biomedical AI solutions. These solutions hold the potential to interpret different medical modalities and produce expressive outputs such as free-text reports or disease diagnosis. Here, we propose BiomedGPT, the first open-source and generalist visual language AI for diverse biomedical tasks. BiomedGPT achieved 16 state-of-the-art results across five clinically significant tasks on 26 datasets. Notably, it outperformed OpenAI's GPT-4 with vision (GPT-4V) in radiology human evaluation and surpassed Google's Med-PaLM M (12B) in breast cancer diagnosis and medical visual question answering. Moreover, BiomedGPT facilitates zero-shot transfer learning, gr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#20307;&#31215;&#25968;&#25454;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#25216;&#26415;&#65292;&#32467;&#21512;&#21040;&#21453;&#24212;&#24335;&#32534;&#31243;&#31995;&#32479;&#20013;&#26500;&#24314;&#20102;&#19968;&#20010;&#21407;&#20301;&#26102;&#38388;&#32531;&#23384;&#31995;&#32479;&#65292;&#24182;&#22312;Ascent&#22522;&#30784;&#26550;&#26500;&#20013;&#20351;&#29992;&#23454;&#38469;&#27169;&#25311;&#35780;&#20272;&#20102;&#20854;100&#20493;&#23481;&#37327;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.10516</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#31070;&#32463;&#34920;&#31034;&#25216;&#26415;&#29992;&#20110;&#21453;&#24212;&#24335;&#21407;&#20301;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Distributed Neural Representation for Reactive in situ Visualization. (arXiv:2304.10516v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#20307;&#31215;&#25968;&#25454;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#25216;&#26415;&#65292;&#32467;&#21512;&#21040;&#21453;&#24212;&#24335;&#32534;&#31243;&#31995;&#32479;&#20013;&#26500;&#24314;&#20102;&#19968;&#20010;&#21407;&#20301;&#26102;&#38388;&#32531;&#23384;&#31995;&#32479;&#65292;&#24182;&#22312;Ascent&#22522;&#30784;&#26550;&#26500;&#20013;&#20351;&#29992;&#23454;&#38469;&#27169;&#25311;&#35780;&#20272;&#20102;&#20854;100&#20493;&#23481;&#37327;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#21453;&#24212;&#24335;&#32534;&#31243;&#23454;&#29616;&#35745;&#31639;&#27169;&#22411;&#30340;&#21407;&#20301;&#21487;&#35270;&#21270;&#21644;&#25511;&#21046;&#21313;&#20998;&#39640;&#25928;&#65292;&#23427;&#21033;&#29992;&#26102;&#38388;&#25277;&#35937;&#21644;&#25968;&#25454;&#32531;&#23384;&#26426;&#21046;&#26469;&#21019;&#24314;&#21160;&#24577;&#24037;&#20316;&#27969;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#27169;&#25311;&#65292;&#23454;&#29616;&#26102;&#38388;&#32531;&#23384;&#21487;&#33021;&#23384;&#22312;&#25361;&#25112;&#12290;&#38544;&#24335;&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#35777;&#26126;&#22312;&#21387;&#32553;&#22823;&#22411;&#25968;&#25454;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#20998;&#24067;&#24335;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#36824;&#27809;&#26377;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#20307;&#31215;&#25968;&#25454;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65292;&#24182;&#23558;&#20854;&#32467;&#21512;&#21040;DIVA&#21453;&#24212;&#24335;&#32534;&#31243;&#31995;&#32479;&#20013;&#12290;&#36825;&#31181;&#23454;&#29616;&#20351;&#25105;&#20204;&#33021;&#22815;&#26500;&#24314;&#19968;&#20010;&#21407;&#20301;&#26102;&#38388;&#32531;&#23384;&#31995;&#32479;&#65292;&#20854;&#23481;&#37327;&#27604;&#20197;&#21069;&#30340;&#23481;&#37327;&#22823;100&#20493;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#38598;&#25104;&#21040;Ascent&#22522;&#30784;&#26550;&#26500;&#20013;&#65292;&#24182;&#20351;&#29992;&#23454;&#38469;&#27169;&#25311;&#26469;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In situ visualization and steering of computational modeling can be effectively achieved using reactive programming, which leverages temporal abstraction and data caching mechanisms to create dynamic workflows. However, implementing a temporal cache for large-scale simulations can be challenging. Implicit neural networks have proven effective in compressing large volume data. However, their application to distributed data has yet to be fully explored. In this work, we develop an implicit neural representation for distributed volume data and incorporate it into the DIVA reactive programming system. This implementation enables us to build an in situ temporal caching system with a capacity 100 times larger than previously achieved. We integrate our implementation into the Ascent infrastructure and evaluate its performance using real-world simulations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24067;&#23616;&#24341;&#23548;&#19979;&#22270;&#20687;&#29983;&#25104;&#30340;&#35786;&#26029;&#22522;&#20934;LayoutBench&#65292;&#23545;&#25968;&#37327;&#12289;&#20301;&#32622;&#12289;&#22823;&#23567;&#21644;&#24418;&#29366;&#22235;&#31181;&#31354;&#38388;&#25511;&#21046;&#25216;&#33021;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22909;&#30340;ID&#24067;&#23616;&#25511;&#21046;&#22312;&#20219;&#24847;&#24067;&#23616;&#30340;&#37326;&#22806;&#29615;&#22659;&#19979;&#21487;&#33021;&#19981;&#20855;&#26377;&#33391;&#22909;&#30340;&#25512;&#24191;&#24615;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;IterInpaint&#36890;&#36807;&#20462;&#22797;&#36880;&#27493;&#29983;&#25104;&#21069;&#26223;&#21644;&#32972;&#26223;&#21306;&#22495;&#65292;&#26174;&#29616;&#20986;&#22312;OOD&#24067;&#23616;&#26041;&#38754;&#26356;&#24378;&#30340;&#36890;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.06671</link><description>&lt;p&gt;
&#24067;&#23616;&#24341;&#23548;&#19979;&#30340;&#22270;&#20687;&#29983;&#25104;&#30340;&#35786;&#26029;&#22522;&#20934;&#21644;&#36845;&#20195;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Diagnostic Benchmark and Iterative Inpainting for Layout-Guided Image Generation. (arXiv:2304.06671v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24067;&#23616;&#24341;&#23548;&#19979;&#22270;&#20687;&#29983;&#25104;&#30340;&#35786;&#26029;&#22522;&#20934;LayoutBench&#65292;&#23545;&#25968;&#37327;&#12289;&#20301;&#32622;&#12289;&#22823;&#23567;&#21644;&#24418;&#29366;&#22235;&#31181;&#31354;&#38388;&#25511;&#21046;&#25216;&#33021;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22909;&#30340;ID&#24067;&#23616;&#25511;&#21046;&#22312;&#20219;&#24847;&#24067;&#23616;&#30340;&#37326;&#22806;&#29615;&#22659;&#19979;&#21487;&#33021;&#19981;&#20855;&#26377;&#33391;&#22909;&#30340;&#25512;&#24191;&#24615;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;IterInpaint&#36890;&#36807;&#20462;&#22797;&#36880;&#27493;&#29983;&#25104;&#21069;&#26223;&#21644;&#32972;&#26223;&#21306;&#22495;&#65292;&#26174;&#29616;&#20986;&#22312;OOD&#24067;&#23616;&#26041;&#38754;&#26356;&#24378;&#30340;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#25511;&#21046;&#26159;&#21487;&#25511;&#22270;&#20687;&#29983;&#25104;&#30340;&#26680;&#24515;&#33021;&#21147;&#12290;&#22312;&#24067;&#23616;&#24341;&#23548;&#19979;&#30340;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#30340;&#36827;&#23637;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#20855;&#26377;&#31867;&#20284;&#31354;&#38388;&#37197;&#32622;&#30340;&#20869;&#20998;&#24067;&#65288;ID&#65289;&#25968;&#25454;&#38598;&#19978;&#26377;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;&#20219;&#24847;&#19981;&#30830;&#23450;&#30340;&#24067;&#23616;&#30340;&#31163;&#32447;&#20998;&#24067;&#26679;&#26412;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#34920;&#29616;&#36824;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LayoutBench&#65292;&#36825;&#26159;&#19968;&#31181;&#23545;&#24067;&#23616;&#24341;&#23548;&#19979;&#30340;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#35786;&#26029;&#30340;&#22522;&#20934;&#65292;&#23427;&#26816;&#26597;&#20102;&#22235;&#31181;&#31354;&#38388;&#25511;&#21046;&#25216;&#33021;&#65306;&#25968;&#37327;&#65292;&#20301;&#32622;&#65292;&#22823;&#23567;&#21644;&#24418;&#29366;&#12290;&#25105;&#20204;&#23545;&#20004;&#31181;&#26368;&#36817;&#20195;&#34920;&#24615;&#30340;&#24067;&#23616;&#24341;&#23548;&#19979;&#30340;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#35266;&#23519;&#21040;&#33391;&#22909;&#30340;ID&#24067;&#23616;&#25511;&#21046;&#21487;&#33021;&#26080;&#27861;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#20219;&#24847;&#24067;&#23616;&#30340;&#37326;&#22806;&#29615;&#22659;&#65288;&#20363;&#22914;&#65292;&#36793;&#30028;&#19978;&#30340;&#23545;&#35937;&#65289;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;IterInpaint&#65292;&#23427;&#36890;&#36807;&#20462;&#22797;&#36880;&#27493;&#29983;&#25104;&#21069;&#26223;&#21644;&#32972;&#26223;&#21306;&#22495;&#65292;&#23637;&#31034;&#20986;&#22312;LayoutBench&#30340;OOD&#24067;&#23616;&#19978;&#26356;&#24378;&#30340;&#36890;&#29992;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#65292;&#34920;&#26126;IterInpaint&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#29983;&#25104;&#22810;&#26679;&#21644;&#35270;&#35273;&#19978;&#20196;&#20154;&#24841;&#24742;&#30340;&#22270;&#20687;&#21644;&#21487;&#25511;&#30340;&#31354;&#38388;&#24067;&#23616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatial control is a core capability in controllable image generation. Advancements in layout-guided image generation have shown promising results on in-distribution (ID) datasets with similar spatial configurations. However, it is unclear how these models perform when facing out-of-distribution (OOD) samples with arbitrary, unseen layouts. In this paper, we propose LayoutBench, a diagnostic benchmark for layout-guided image generation that examines four categories of spatial control skills: number, position, size, and shape. We benchmark two recent representative layout-guided image generation methods and observe that the good ID layout control may not generalize well to arbitrary layouts in the wild (e.g., objects at the boundary). Next, we propose IterInpaint, a new baseline that generates foreground and background regions in a step-by-step manner via inpainting, demonstrating stronger generalizability than existing models on OOD layouts in LayoutBench. We perform quantitative and q
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#26696;&#65292;&#36890;&#36807;&#36825;&#20123;&#26041;&#26696;&#65292;&#21487;&#20197;&#26356;&#21487;&#38752;&#22320;&#27979;&#37327;&#24402;&#22240;&#26041;&#27861;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.11884</link><description>&lt;p&gt;
&#36890;&#36807;&#31995;&#32479;&#35780;&#20272;&#26356;&#22909;&#22320;&#29702;&#35299;&#24402;&#22240;&#26041;&#27861;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Better Understanding Differences in Attribution Methods via Systematic Evaluations. (arXiv:2303.11884v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#26696;&#65292;&#36890;&#36807;&#36825;&#20123;&#26041;&#26696;&#65292;&#21487;&#20197;&#26356;&#21487;&#38752;&#22320;&#27979;&#37327;&#24402;&#22240;&#26041;&#27861;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#35270;&#35273;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#20854;&#40657;&#30418;&#24615;&#36136;&#20351;&#20854;&#38590;&#20197;&#35299;&#37322;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#21518;&#32493;&#24402;&#22240;&#26041;&#27861;&#26469;&#30830;&#23450;&#23545;&#27169;&#22411;&#20915;&#31574;&#26368;&#26377;&#24433;&#21709;&#21147;&#30340;&#22270;&#20687;&#21306;&#22495;&#12290;&#30001;&#20110;&#19981;&#23384;&#22312;&#22522;&#20934;&#24402;&#22240;&#65292;&#22240;&#27492;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#26696;&#65292;&#20197;&#26356;&#21487;&#38752;&#22320;&#27979;&#37327;&#36825;&#20123;&#26041;&#27861;&#30340;&#21487;&#20449;&#24230;&#65292;&#20351;&#23427;&#20204;&#20043;&#38388;&#30340;&#27604;&#36739;&#26356;&#20844;&#24179;&#65292;&#24182;&#20351;&#35270;&#35273;&#26816;&#26597;&#26356;&#31995;&#32479;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are very successful on many vision tasks, but hard to interpret due to their black box nature. To overcome this, various post-hoc attribution methods have been proposed to identify image regions most influential to the models' decisions. Evaluating such methods is challenging since no ground truth attributions exist. We thus propose three novel evaluation schemes to more reliably measure the faithfulness of those methods, to make comparisons between them more fair, and to make visual inspection more systematic. To address faithfulness, we propose a novel evaluation setting (DiFull) in which we carefully control which parts of the input can influence the output in order to distinguish possible from impossible attributions. To address fairness, we note that different methods are applied at different layers, which skews any comparison, and so evaluate all methods on the same layers (ML-Att) and discuss how this impacts their performance on quantitative metrics. For mo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38405;&#35835;&#24182;&#22870;&#21169;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#38405;&#35835;Atari&#28216;&#25103;&#24320;&#21457;&#32773;&#21457;&#24067;&#30340;&#25351;&#23548;&#25163;&#20876;&#65292;&#20197;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;Atari&#28216;&#25103;&#20013;&#30340;&#25928;&#29575;&#12290;&#35813;&#26694;&#26550;&#21253;&#21547;&#19968;&#20010;QA&#25552;&#21462;&#27169;&#22359;&#21644;&#19968;&#20010;&#25512;&#29702;&#27169;&#22359;&#65292;&#33021;&#22815;&#20174;&#25351;&#23548;&#25163;&#20876;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#35780;&#20272;&#29289;&#20307;&#19982;&#26234;&#33021;&#20307;&#30340;&#20132;&#20114;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.04449</link><description>&lt;p&gt;
&#38405;&#35835;&#24182;&#33719;&#24471;&#22238;&#25253;&#65306;&#22312;&#19982;&#25351;&#23548;&#25163;&#20876;&#30340;&#24110;&#21161;&#19979;&#23398;&#20064;&#29609;Atari&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals. (arXiv:2302.04449v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38405;&#35835;&#24182;&#22870;&#21169;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#38405;&#35835;Atari&#28216;&#25103;&#24320;&#21457;&#32773;&#21457;&#24067;&#30340;&#25351;&#23548;&#25163;&#20876;&#65292;&#20197;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;Atari&#28216;&#25103;&#20013;&#30340;&#25928;&#29575;&#12290;&#35813;&#26694;&#26550;&#21253;&#21547;&#19968;&#20010;QA&#25552;&#21462;&#27169;&#22359;&#21644;&#19968;&#20010;&#25512;&#29702;&#27169;&#22359;&#65292;&#33021;&#22815;&#20174;&#25351;&#23548;&#25163;&#20876;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#35780;&#20272;&#29289;&#20307;&#19982;&#26234;&#33021;&#20307;&#30340;&#20132;&#20114;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#39640;&#26679;&#26412;&#22797;&#26434;&#24615;&#19968;&#30452;&#26159;&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#23398;&#20064;&#25191;&#34892;&#20219;&#21153;&#30340;&#26041;&#24335;&#19981;&#20165;&#20165;&#26159;&#36890;&#36807;&#20132;&#20114;&#25110;&#28436;&#31034;&#65292;&#36824;&#21253;&#25324;&#38405;&#35835;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25991;&#26723;&#65292;&#20363;&#22914;&#25351;&#23548;&#25163;&#20876;&#12290;&#25351;&#23548;&#25163;&#20876;&#21644;&#32500;&#22522;&#39029;&#38754;&#26159;&#26368;&#20016;&#23500;&#30340;&#25968;&#25454;&#20043;&#19968;&#65292;&#23427;&#20204;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;&#23453;&#36149;&#29305;&#24449;&#12289;&#31574;&#30053;&#12289;&#20219;&#21153;&#29305;&#23450;&#30340;&#29615;&#22659;&#21160;&#24577;&#21644;&#22870;&#21169;&#32467;&#26500;&#30340;&#20449;&#24687;&#65292;&#22240;&#27492;&#25105;&#20204;&#20551;&#35774;&#21033;&#29992;&#20154;&#20889;&#30340;&#25351;&#23548;&#25163;&#20876;&#26469;&#24110;&#21161;&#23398;&#20064;&#29305;&#23450;&#20219;&#21153;&#30340;&#31574;&#30053;&#23558;&#23548;&#33268;&#26356;&#39640;&#25928;&#21644;&#26356;&#20248;&#31168;&#30340;&#26234;&#33021;&#20307;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38405;&#35835;&#24182;&#22870;&#21169;&#30340;&#26694;&#26550;&#12290;&#38405;&#35835;&#24182;&#22870;&#21169;&#36890;&#36807;&#38405;&#35835;Atari&#28216;&#25103;&#24320;&#21457;&#32773;&#21457;&#24067;&#30340;&#25351;&#23548;&#25163;&#20876;&#26469;&#21152;&#36895;RL&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;QA&#25552;&#21462;&#27169;&#22359;&#65292;&#29992;&#20110;&#25552;&#21462;&#21644;&#24635;&#32467;&#25351;&#23548;&#25163;&#20876;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#65292;&#20197;&#21450;&#19968;&#20010;&#25512;&#29702;&#27169;&#22359;&#65292;&#26681;&#25454;&#25351;&#23548;&#25163;&#20876;&#20013;&#30340;&#20449;&#24687;&#35780;&#20272;&#29289;&#20307;-&#26234;&#33021;&#20307;&#30340;&#20132;&#20114;&#25928;&#26524;&#12290;&#19968;&#20010;&#36741;&#21161;&#30340;&#21453;&#39304;&#26426;&#21046;&#21487;&#20197;&#25552;&#39640;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
High sample complexity has long been a challenge for RL. On the other hand, humans learn to perform tasks not only from interaction or demonstrations, but also by reading unstructured text documents, e.g., instruction manuals. Instruction manuals and wiki pages are among the most abundant data that could inform agents of valuable features and policies or task-specific environmental dynamics and reward structures. Therefore, we hypothesize that the ability to utilize human-written instruction manuals to assist learning policies for specific tasks should lead to a more efficient and better-performing agent. We propose the Read and Reward framework. Read and Reward speeds up RL algorithms on Atari games by reading manuals released by the Atari game developers. Our framework consists of a QA Extraction module that extracts and summarizes relevant information from the manual and a Reasoning module that evaluates object-agent interactions based on information from the manual. An auxiliary re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#19981;&#38656;&#35201;&#21453;&#21521;&#20256;&#25773;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;BAFFLE&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#22810;&#20010;&#27491;&#21521;&#36807;&#31243;&#20272;&#35745;&#26799;&#24230;&#65292;&#20855;&#26377;&#39640;&#20869;&#23384;&#25928;&#29575;&#65292;&#23481;&#26131;&#36866;&#24212;&#19978;&#20256;&#24102;&#23485;&#65292;&#19982;&#30828;&#20214;&#20248;&#21270;&#21644;&#27169;&#22411;&#37327;&#21270;/&#20462;&#21098;&#20860;&#23481;&#65292;&#36866;&#29992;&#20110;&#21463;&#20449;&#20219;&#30340;&#25191;&#34892;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2301.12195</link><description>&lt;p&gt;
&#12298;&#32852;&#37030;&#23398;&#20064;&#26159;&#21542;&#30495;&#27491;&#38656;&#35201;&#21453;&#21521;&#20256;&#25773;&#65311;&#12299;
&lt;/p&gt;
&lt;p&gt;
Does Federated Learning Really Need Backpropagation?. (arXiv:2301.12195v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#19981;&#38656;&#35201;&#21453;&#21521;&#20256;&#25773;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;BAFFLE&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#22810;&#20010;&#27491;&#21521;&#36807;&#31243;&#20272;&#35745;&#26799;&#24230;&#65292;&#20855;&#26377;&#39640;&#20869;&#23384;&#25928;&#29575;&#65292;&#23481;&#26131;&#36866;&#24212;&#19978;&#20256;&#24102;&#23485;&#65292;&#19982;&#30828;&#20214;&#20248;&#21270;&#21644;&#27169;&#22411;&#37327;&#21270;/&#20462;&#21098;&#20860;&#23481;&#65292;&#36866;&#29992;&#20110;&#21463;&#20449;&#20219;&#30340;&#25191;&#34892;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#22320;&#35753;&#23458;&#25143;&#31471;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#26381;&#21153;&#22120;&#27169;&#22411;&#30340;&#19968;&#33324;&#24615;&#21407;&#21017;&#65292;&#32780;&#26080;&#38656;&#20849;&#20139;&#26412;&#22320;&#25968;&#25454;&#12290;FL&#26159;&#19968;&#20010;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#30340;&#26377;&#21069;&#36884;&#30340;&#26694;&#26550;&#65292;&#20294;&#20854;&#26631;&#20934;&#35757;&#32451;&#33539;&#24335;&#35201;&#27714;&#23458;&#25143;&#31471;&#36890;&#36807;&#27169;&#22411;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#20197;&#35745;&#31639;&#26799;&#24230;&#12290;&#30001;&#20110;&#36825;&#20123;&#23458;&#25143;&#31471;&#36890;&#24120;&#26159;&#36793;&#32536;&#35774;&#22791;&#32780;&#19981;&#26159;&#23436;&#20840;&#21463;&#20449;&#20219;&#30340;&#65292;&#22240;&#27492;&#22312;&#23427;&#20204;&#19978;&#25191;&#34892;&#21453;&#21521;&#20256;&#25773;&#20250;&#20135;&#29983;&#35745;&#31639;&#21644;&#23384;&#20648;&#24320;&#38144;&#20197;&#21450;&#30333;&#30418;&#28431;&#27934;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#21453;&#21521;&#20256;&#25773;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#31216;&#20026;BAFFLE&#65292;&#20854;&#20013;&#21453;&#21521;&#20256;&#25773;&#26367;&#25442;&#20026;&#22810;&#20010;&#27491;&#21521;&#36807;&#31243;&#20197;&#20272;&#35745;&#26799;&#24230;&#12290;BAFFLE&#20855;&#26377;&#20197;&#19979;&#20248;&#28857;&#65306;1&#65289;&#20869;&#23384;&#25928;&#29575;&#39640;&#24182;&#19988;&#23481;&#26131;&#36866;&#24212;&#19978;&#20256;&#24102;&#23485;&#65307;2&#65289;&#19982;&#20165;&#25512;&#29702;&#30828;&#20214;&#20248;&#21270;&#20197;&#21450;&#27169;&#22411;&#37327;&#21270;&#25110;&#20462;&#21098;&#20860;&#23481;&#65307;3&#65289;&#38750;&#24120;&#36866;&#21512;&#21463;&#20449;&#20219;&#30340;&#25191;&#34892;&#29615;&#22659;&#65292;&#22240;&#20026;BAFFLE&#20013;&#30340;&#23458;&#25143;&#31471;&#20165;&#25191;&#34892;&#27491;&#21521;&#20256;&#25773;&#24182;&#36820;&#22238;&#19968;&#32452;&#26631;&#37327;&#21040;&#26381;&#21153;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#20351;&#29992;&#20102;BAFFLE&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a general principle for decentralized clients to train a server model collectively without sharing local data. FL is a promising framework with practical applications, but its standard training paradigm requires the clients to backpropagate through the model to compute gradients. Since these clients are typically edge devices and not fully trusted, executing backpropagation on them incurs computational and storage overhead as well as white-box vulnerability. In light of this, we develop backpropagation-free federated learning, dubbed BAFFLE, in which backpropagation is replaced by multiple forward processes to estimate gradients. BAFFLE is 1) memory-efficient and easily fits uploading bandwidth; 2) compatible with inference-only hardware optimization and model quantization or pruning; and 3) well-suited to trusted execution environments, because the clients in BAFFLE only execute forward propagation and return a set of scalars to the server. Empirically we us
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#35745;&#31639;&#30340;&#35821;&#20041;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;AI&#30456;&#32467;&#21512;&#25104;&#20026;&#32508;&#21512;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#23558;&#31526;&#21495;&#30693;&#35782;&#32534;&#30721;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#26469;&#35299;&#20915;&#36890;&#29992;&#25512;&#29702;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.12050</link><description>&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#35745;&#31639;&#30340;&#35821;&#20041;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Semantic Framework for Neural-Symbolic Computing. (arXiv:2212.12050v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12050
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#35745;&#31639;&#30340;&#35821;&#20041;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;AI&#30456;&#32467;&#21512;&#25104;&#20026;&#32508;&#21512;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#23558;&#31526;&#21495;&#30693;&#35782;&#32534;&#30721;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#26469;&#35299;&#20915;&#36890;&#29992;&#25512;&#29702;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#20004;&#31181;&#26041;&#27861;&#65292;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;&#31995;&#32479;&#65292;&#23545;&#20110;&#19968;&#31995;&#21015;AI&#38382;&#39064;&#24050;&#32463;&#34987;&#35777;&#26126;&#38750;&#24120;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#20004;&#32773;&#22343;&#26410;&#33021;&#36798;&#21040;&#20154;&#31867;&#26234;&#33021;&#25152;&#38656;&#30340;&#36890;&#29992;&#25512;&#29702;&#33021;&#21147;&#12290;&#20154;&#20204;&#35748;&#20026;&#36825;&#26159;&#27599;&#31181;&#26041;&#27861;&#20869;&#22312;&#24369;&#28857;&#25152;&#33268;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#36825;&#20123;&#24369;&#28857;&#20284;&#20046;&#26159;&#20114;&#34917;&#30340;&#65292;&#31526;&#21495;&#31995;&#32479;&#25797;&#38271;&#31070;&#32463;&#32593;&#32476;&#38590;&#20197;&#22788;&#29702;&#30340;&#20107;&#29289;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#31070;&#32463;&#31526;&#21495;AI&#39046;&#22495;&#35797;&#22270;&#21033;&#29992;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;AI&#30456;&#32467;&#21512;&#25104;&#20026;&#32508;&#21512;&#31995;&#32479;&#12290;&#36890;&#24120;&#36825;&#26159;&#36890;&#36807;&#23558;&#31526;&#21495;&#30693;&#35782;&#32534;&#30721;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#30340;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#34429;&#28982;&#25552;&#20986;&#20102;&#35768;&#22810;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#20294;&#27809;&#26377;&#20844;&#20849;&#30340;&#32534;&#30721;&#23450;&#20041;&#21487;&#20379;&#27604;&#36739;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#31070;&#32463;&#31526;&#21495;AI&#30340;&#35821;&#20041;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#28982;&#21518;&#35777;&#26126;&#23427;&#36275;&#20197;&#35299;&#37322;&#22823;&#37327;&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26159;&#22522;&#20110;&#31526;&#21495;&#31995;&#32479;&#26893;&#26681;&#20110;&#20854;&#39046;&#22495;&#30340;&#31070;&#32463;&#34920;&#24449;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#35299;&#37322;&#21508;&#31181;&#31526;&#21495;&#31995;&#32479;&#22312;&#31070;&#32463;&#34920;&#24449;&#20013;&#30340;&#23454;&#29616;&#26041;&#24335;&#65292;&#21253;&#25324;&#20351;&#29992;&#23398;&#20064;&#30340;&#31070;&#32463;&#34920;&#24449;&#21644;&#20351;&#29992;&#22266;&#23450;&#31070;&#32463;&#34920;&#24449;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Two approaches to AI, neural networks and symbolic systems, have been proven very successful for an array of AI problems. However, neither has been able to achieve the general reasoning ability required for human-like intelligence. It has been argued that this is due to inherent weaknesses in each approach. Luckily, these weaknesses appear to be complementary, with symbolic systems being adept at the kinds of things neural networks have trouble with and vice-versa. The field of neural-symbolic AI attempts to exploit this asymmetry by combining neural networks and symbolic AI into integrated systems. Often this has been done by encoding symbolic knowledge into neural networks. Unfortunately, although many different methods for this have been proposed, there is no common definition of an encoding to compare them. We seek to rectify this problem by introducing a semantic framework for neural-symbolic AI, which is then shown to be general enough to account for a large family of neural-symb
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#34701;&#20837;&#21040;&#19968;&#20010;&#23545;&#35805;&#20195;&#29702;&#20013;&#65292;&#35774;&#35745;&#20855;&#26377;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#32452;&#20214;&#30340;&#26631;&#20934;&#27169;&#22411;&#12290;&#36890;&#36807;&#25193;&#23637;XAI&#38382;&#39064;&#24211;&#24182;&#25552;&#20379;&#35299;&#37322;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20851;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#30495;&#27491;&#33258;&#28982;&#23545;&#35805;&#12290;</title><link>http://arxiv.org/abs/2209.02552</link><description>&lt;p&gt;
&#33258;&#28982;&#23545;&#35805;&#20013;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65306;&#36208;&#21521;&#23545;&#35805;&#24335;XAI&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Explaining Machine Learning Models in Natural Conversations: Towards a Conversational XAI Agent. (arXiv:2209.02552v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.02552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#34701;&#20837;&#21040;&#19968;&#20010;&#23545;&#35805;&#20195;&#29702;&#20013;&#65292;&#35774;&#35745;&#20855;&#26377;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#32452;&#20214;&#30340;&#26631;&#20934;&#27169;&#22411;&#12290;&#36890;&#36807;&#25193;&#23637;XAI&#38382;&#39064;&#24211;&#24182;&#25552;&#20379;&#35299;&#37322;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20851;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#30495;&#27491;&#33258;&#28982;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#26041;&#27861;&#26469;&#25581;&#31034;&#40657;&#30418;&#27169;&#22411;&#65288;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#20197;&#20415;&#21521;&#20154;&#31867;&#35299;&#37322;&#12290;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#25351;&#20986;&#65292;&#36825;&#26679;&#30340;&#35299;&#37322;&#24212;&#35813;&#26159;&#23545;&#35805;&#24335;&#30340;&#65292;&#31867;&#20284;&#20110;&#20154;&#19982;&#20154;&#20043;&#38388;&#30340;&#35299;&#37322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;XAI&#34701;&#20837;&#21040;&#19968;&#20010;&#23545;&#35805;&#20195;&#29702;&#20013;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#32452;&#20214;&#30340;&#26631;&#20934;&#35774;&#35745;&#12290;&#25105;&#20204;&#26681;&#25454;&#36136;&#25511;&#30340;&#37322;&#20041;&#37325;&#36848;&#25193;&#23637;&#20102;&#19968;&#20010;XAI&#38382;&#39064;&#24211;&#65292;&#20197;&#29702;&#35299;&#29992;&#25143;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#36866;&#21512;&#25552;&#20379;&#31572;&#26696;&#20449;&#24687;&#30340;&#35299;&#37322;&#26041;&#27861;&#30340;&#25991;&#29486;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#24314;&#35758;&#21015;&#34920;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#23454;&#29616;&#20851;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#30495;&#27491;&#33258;&#28982;&#23545;&#35805;&#30340;&#31532;&#19968;&#27493;&#65292;&#19982;&#19968;&#20010;&#35299;&#37322;&#20195;&#29702;&#26377;&#20851;&#30340;&#20840;&#38754;&#30340;XAI&#38382;&#39064;&#21015;&#34920;&#21644;&#30456;&#24212;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of Explainable AI (XAI) is to design methods to provide insights into the reasoning process of black-box models, such as deep neural networks, in order to explain them to humans. Social science research states that such explanations should be conversational, similar to human-to-human explanations. In this work, we show how to incorporate XAI in a conversational agent, using a standard design for the agent comprising natural language understanding and generation components. We build upon an XAI question bank which we extend by quality-controlled paraphrases to understand the user's information needs. We further systematically survey the literature for suitable explanation methods that provide the information to answer those questions, and present a comprehensive list of suggestions. Our work is the first step towards truly natural conversations about machine learning models with an explanation agent. The comprehensive list of XAI questions and the corresponding explanation meth
&lt;/p&gt;</description></item></channel></rss>