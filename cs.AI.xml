<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#36710;&#36947;&#21464;&#25442;&#39044;&#27979;&#27169;&#22411;&#65292;&#23558;&#36710;&#36947;&#21464;&#25442;&#39044;&#27979;&#20219;&#21153;&#37325;&#26032;&#26500;&#24314;&#20026;&#35821;&#35328;&#24314;&#27169;&#38382;&#39064;&#65292;&#20197;&#25552;&#39640;&#38271;&#26399;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18344</link><description>&lt;p&gt;
LC-LLM: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#24615;&#36710;&#36947;&#21464;&#25442;&#24847;&#22270;&#21644;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
LC-LLM: Explainable Lane-Change Intention and Trajectory Predictions with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18344
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#36710;&#36947;&#21464;&#25442;&#39044;&#27979;&#27169;&#22411;&#65292;&#23558;&#36710;&#36947;&#21464;&#25442;&#39044;&#27979;&#20219;&#21153;&#37325;&#26032;&#26500;&#24314;&#20026;&#35821;&#35328;&#24314;&#27169;&#38382;&#39064;&#65292;&#20197;&#25552;&#39640;&#38271;&#26399;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#30830;&#20445;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#23433;&#20840;&#39550;&#39542;&#65292;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#24212;&#20855;&#22791;&#20934;&#30830;&#39044;&#27979;&#21608;&#22260;&#36710;&#36742;&#36710;&#36947;&#21464;&#25442;&#24847;&#22270;&#24182;&#39044;&#27979;&#20854;&#26410;&#26469;&#36712;&#36857;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;LC-LLM&#65292;&#19968;&#20010;&#21487;&#35299;&#37322;&#24615;&#36710;&#36947;&#21464;&#25442;&#39044;&#27979;&#27169;&#22411;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#21644;&#33258;&#25105;&#35299;&#37322;&#33021;&#21147;&#26469;&#35299;&#20915;&#29616;&#26377;&#36816;&#21160;&#39044;&#27979;&#26041;&#27861;&#22312;&#38271;&#26399;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#36710;&#36947;&#21464;&#25442;&#39044;&#27979;&#20219;&#21153;&#37325;&#26032;&#26500;&#24314;&#20026;&#19968;&#20010;&#35821;&#35328;&#24314;&#27169;&#38382;&#39064;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24322;&#26500;&#39550;&#39542;&#22330;&#26223;&#20449;&#24687;&#20316;&#20026;LLM&#30340;&#36755;&#20837;&#25552;&#31034;&#65292;&#24182;&#20351;&#29992;&#30417;&#30563;&#24494;&#35843;&#25216;&#26415;&#19987;&#38376;&#20026;&#25105;&#20204;&#30340;&#36710;&#36947;&#21464;&#25442;&#39044;&#27979;&#20219;&#21153;&#23450;&#21046;LLM&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;LLM&#30340;&#24378;&#22823;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18344v1 Announce Type: new  Abstract: To ensure safe driving in dynamic environments, autonomous vehicles should possess the capability to accurately predict the lane change intentions of surrounding vehicles in advance and forecast their future trajectories. Existing motion prediction approaches have ample room for improvement, particularly in terms of long-term prediction accuracy and interpretability. In this paper, we address these challenges by proposing LC-LLM, an explainable lane change prediction model that leverages the strong reasoning capabilities and self-explanation abilities of Large Language Models (LLMs). Essentially, we reformulate the lane change prediction task as a language modeling problem, processing heterogeneous driving scenario information in natural language as prompts for input into the LLM and employing a supervised fine-tuning technique to tailor the LLM specifically for our lane change prediction task. This allows us to utilize the LLM's powerfu
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#36951;&#24536;&#65288;MU&#65289;&#36890;&#36807;&#30693;&#35782;&#21435;&#38500;&#36807;&#31243;&#26469;&#35299;&#20915;&#35757;&#32451;&#25968;&#25454;&#30456;&#20851;&#30340;&#20154;&#24037;&#26234;&#33021;&#27835;&#29702;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;AI&#31995;&#32479;&#30340;&#23433;&#20840;&#21644;&#36127;&#36131;&#20219;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.13682</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#23041;&#32961;&#12289;&#25915;&#20987;&#21644;&#38450;&#24481;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Threats, Attacks, and Defenses in Machine Unlearning: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13682
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#65288;MU&#65289;&#36890;&#36807;&#30693;&#35782;&#21435;&#38500;&#36807;&#31243;&#26469;&#35299;&#20915;&#35757;&#32451;&#25968;&#25454;&#30456;&#20851;&#30340;&#20154;&#24037;&#26234;&#33021;&#27835;&#29702;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;AI&#31995;&#32479;&#30340;&#23433;&#20840;&#21644;&#36127;&#36131;&#20219;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#65288;MU&#65289;&#26368;&#36817;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#26377;&#28508;&#21147;&#36890;&#36807;&#20174;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#28040;&#38500;&#29305;&#23450;&#25968;&#25454;&#30340;&#24433;&#21709;&#26469;&#23454;&#29616;&#23433;&#20840;&#20154;&#24037;&#26234;&#33021;&#12290;&#36825;&#20010;&#34987;&#31216;&#20026;&#30693;&#35782;&#21435;&#38500;&#30340;&#36807;&#31243;&#35299;&#20915;&#20102;&#19982;&#35757;&#32451;&#25968;&#25454;&#30456;&#20851;&#30340;&#20154;&#24037;&#26234;&#33021;&#27835;&#29702;&#38382;&#39064;&#65292;&#22914;&#25968;&#25454;&#36136;&#37327;&#12289;&#25935;&#24863;&#24615;&#12289;&#29256;&#26435;&#38480;&#21046;&#21644;&#36807;&#26102;&#24615;&#12290;&#36825;&#31181;&#33021;&#21147;&#23545;&#20110;&#30830;&#20445;&#36981;&#23432;&#35832;&#22914;&#34987;&#36951;&#24536;&#26435;&#31561;&#38544;&#31169;&#27861;&#35268;&#20063;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#26377;&#25928;&#30340;&#30693;&#35782;&#21435;&#38500;&#26377;&#21161;&#20110;&#20943;&#36731;&#26377;&#23475;&#32467;&#26524;&#30340;&#39118;&#38505;&#65292;&#38450;&#33539;&#20559;&#35265;&#12289;&#35823;&#23548;&#21644;&#26410;&#32463;&#25480;&#26435;&#30340;&#25968;&#25454;&#21033;&#29992;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;AI&#31995;&#32479;&#30340;&#23433;&#20840;&#21644;&#36127;&#36131;&#20219;&#20351;&#29992;&#12290;&#24050;&#32463;&#24320;&#23637;&#20102;&#35774;&#35745;&#39640;&#25928;&#30340;&#36951;&#24536;&#26041;&#27861;&#30340;&#24037;&#20316;&#65292;&#36890;&#36807;&#30740;&#31350;MU&#26381;&#21153;&#20197;&#19982;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#26381;&#21153;&#38598;&#25104;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#25552;&#20132;&#35831;&#27714;&#20174;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#21024;&#38500;&#29305;&#23450;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13682v2 Announce Type: replace-cross  Abstract: Machine Unlearning (MU) has gained considerable attention recently for its potential to achieve Safe AI by removing the influence of specific data from trained machine learning models. This process, known as knowledge removal, addresses AI governance concerns of training data such as quality, sensitivity, copyright restrictions, and obsolescence. This capability is also crucial for ensuring compliance with privacy regulations such as the Right To Be Forgotten. Furthermore, effective knowledge removal mitigates the risk of harmful outcomes, safeguarding against biases, misinformation, and unauthorized data exploitation, thereby enhancing the safe and responsible use of AI systems. Efforts have been made to design efficient unlearning approaches, with MU services being examined for integration with existing machine learning as a service, allowing users to submit requests to remove specific data from the training corpus. However, 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#32422;&#26463;&#20998;&#23376;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;TSMMG&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#23567;&#27169;&#22411;&#21644;&#24037;&#20855;&#26469;&#24110;&#21161;&#29983;&#25104;&#31526;&#21512;&#25551;&#36848;&#30340;&#26032;&#20998;&#23376;&#65292;&#22312;&#21508;&#31181;&#32422;&#26463;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#31168;&#12290;</title><link>https://arxiv.org/abs/2403.13244</link><description>&lt;p&gt;
&#20351;&#29992;&#24072;&#29983;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#32422;&#26463;&#20998;&#23376;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Instruction Multi-Constraint Molecular Generation Using a Teacher-Student Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13244
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#32422;&#26463;&#20998;&#23376;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;TSMMG&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#23567;&#27169;&#22411;&#21644;&#24037;&#20855;&#26469;&#24110;&#21161;&#29983;&#25104;&#31526;&#21512;&#25551;&#36848;&#30340;&#26032;&#20998;&#23376;&#65292;&#22312;&#21508;&#31181;&#32422;&#26463;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#27169;&#22411;&#21644;&#35745;&#31639;&#24037;&#20855;&#29992;&#20110;&#20998;&#23376;&#30340;&#32467;&#26500;&#21644;&#24615;&#36136;&#20998;&#26512;&#65292;&#20294;&#29983;&#25104;&#31526;&#21512;&#25152;&#26377;&#26399;&#26395;&#32467;&#26500;&#21644;&#24615;&#36136;&#30340;&#20998;&#23376;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#32422;&#26463;&#20998;&#23376;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;TSMMG&#65292;&#31867;&#20284;&#20110;&#23398;&#29983;&#65292;&#35813;&#27169;&#22411;&#25972;&#21512;&#20102;&#26469;&#33258;&#21508;&#31181;&#23567;&#27169;&#22411;&#21644;&#24037;&#20855;&#65288;&#21363;&#8220;&#32769;&#24072;&#8221;&#65289;&#30340;&#30693;&#35782;&#12290;&#20026;&#20102;&#35757;&#32451;TSMMG&#65292;&#25105;&#20204;&#36890;&#36807;&#20174;&#36825;&#20123;&#8216;&#32769;&#24072;&#8217;&#20013;&#25552;&#21462;&#30340;&#20998;&#23376;&#30693;&#35782;&#26500;&#24314;&#20102;&#22823;&#37327;&#25991;&#26412;-&#20998;&#23376;&#23545;&#65292;&#20351;&#20854;&#33021;&#22815;&#36890;&#36807;&#21508;&#31181;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#31526;&#21512;&#25551;&#36848;&#30340;&#26032;&#20998;&#23376;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;TSMMG&#22312;&#29983;&#25104;&#31526;&#21512;&#22797;&#26434;&#12289;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#20004;&#12289;&#19977;&#21644;&#22235;&#32422;&#26463;&#20219;&#21153;&#30340;&#20998;&#23376;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24179;&#22343;&#20998;&#23376;&#26377;&#25928;&#24615;&#36229;&#36807;99&#65285;&#65292;&#25104;&#21151;&#29575;&#20998;&#21035;&#20026;88.08&#65285;&#12289;65.27&#65285;&#21644;61.44&#65285;&#12290;&#35813;&#27169;&#22411;&#36824;ex
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13244v1 Announce Type: new  Abstract: While various models and computational tools have been proposed for structure and property analysis of molecules, generating molecules that conform to all desired structures and properties remains a challenge. Here, we introduce a multi-constraint molecular generation large language model, TSMMG, which, akin to a student, incorporates knowledge from various small models and tools, namely, the 'teachers'. To train TSMMG, we construct a large set of text-molecule pairs by extracting molecular knowledge from these 'teachers', enabling it to generate novel molecules that conform to the descriptions through various text prompts. We experimentally show that TSMMG remarkably performs in generating molecules meeting complex, natural language-described property requirements across two-, three-, and four-constraint tasks, with an average molecular validity of over 99% and success ratio of 88.08%, 65.27%, and 61.44%, respectively. The model also ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;RallyNet&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#32701;&#27611;&#29699;&#36816;&#21160;&#21592;&#34892;&#20026;&#30340;&#20998;&#23618;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#36873;&#25163;&#30340;&#20915;&#31574;&#20381;&#36182;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#30452;&#25509;&#24212;&#29992;&#29616;&#26377;&#26041;&#27861;&#26102;&#21487;&#33021;&#36935;&#21040;&#30340;&#23618;&#27425;&#32467;&#26500;&#21644;&#36718;&#27969;&#37319;&#21462;&#34892;&#21160;&#23548;&#33268;&#30340;&#22797;&#21512;&#25928;&#24212;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.12406</link><description>&lt;p&gt;
&#36890;&#36807;&#32463;&#39564;&#32972;&#26223;&#21644;&#24067;&#26391;&#36816;&#21160;&#36827;&#34892;&#32701;&#27611;&#29699;&#36816;&#21160;&#21592;&#34892;&#20026;&#30340;&#31163;&#32447;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
Offline Imitation of Badminton Player Behavior via Experiential Contexts and Brownian Motion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;RallyNet&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#32701;&#27611;&#29699;&#36816;&#21160;&#21592;&#34892;&#20026;&#30340;&#20998;&#23618;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#36873;&#25163;&#30340;&#20915;&#31574;&#20381;&#36182;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#30452;&#25509;&#24212;&#29992;&#29616;&#26377;&#26041;&#27861;&#26102;&#21487;&#33021;&#36935;&#21040;&#30340;&#23618;&#27425;&#32467;&#26500;&#21644;&#36718;&#27969;&#37319;&#21462;&#34892;&#21160;&#23548;&#33268;&#30340;&#22797;&#21512;&#25928;&#24212;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21160;&#24577;&#21644;&#24555;&#33410;&#22863;&#30340;&#22522;&#20110;&#36718;&#27425;&#30340;&#20307;&#32946;&#36816;&#21160;&#20013;&#65292;&#32701;&#27611;&#29699;&#20316;&#20026;&#19968;&#31181;&#38656;&#35201;&#36873;&#25163;&#20381;&#36182;&#21464;&#21270;&#30340;&#20915;&#31574;&#30340;&#22266;&#26377;&#33539;&#20363;&#32780;&#33073;&#39062;&#32780;&#20986;&#12290;&#34429;&#28982;&#22312;&#39034;&#24207;&#20915;&#31574;&#30340;&#31163;&#32447;&#19987;&#23478;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#36827;&#23637;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#37117;&#26377;&#25152;&#28041;&#21450;&#65292;&#20294;&#22914;&#20309;&#20174;&#31163;&#32447;&#32701;&#27611;&#29699;&#27604;&#36187;&#20013;&#27169;&#20223;&#20154;&#31867;&#36873;&#25163;&#30340;&#27604;&#36187;&#34892;&#20026;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#22797;&#21046;&#23545;&#25163;&#30340;&#34892;&#20026;&#26377;&#30410;&#20110;&#36873;&#25163;&#65292;&#20351;&#20182;&#20204;&#33021;&#22815;&#22312;&#27604;&#36187;&#21069;&#26377;&#26041;&#21521;&#22320;&#36827;&#34892;&#25112;&#30053;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#24212;&#29992;&#29616;&#26377;&#26041;&#27861;&#20250;&#21463;&#21040;&#27604;&#36187;&#30340;&#20869;&#22312;&#23618;&#27425;&#32467;&#26500;&#21644;&#30001;&#20110;&#36718;&#27969;&#37319;&#21462;&#34892;&#21160;&#30340;&#36873;&#25163;&#36718;&#27425;&#24615;&#36136;&#32780;&#20135;&#29983;&#30340;&#22797;&#21512;&#25928;&#24212;&#30340;&#22256;&#25200;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RallyNet&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#32701;&#27611;&#29699;&#36816;&#21160;&#21592;&#34892;&#20026;&#30340;&#20998;&#23618;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#27169;&#22411;&#65306;&#65288;i&#65289;RallyNet&#36890;&#36807;&#23558;&#20915;&#31574;&#36807;&#31243;&#24314;&#27169;&#20026;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12406v1 Announce Type: new  Abstract: In the dynamic and rapid tactic involvements of turn-based sports, badminton stands out as an intrinsic paradigm that requires alter-dependent decision-making of players. While the advancement of learning from offline expert data in sequential decision-making has been witnessed in various domains, how to rally-wise imitate the behaviors of human players from offline badminton matches has remained underexplored. Replicating opponents' behavior benefits players by allowing them to undergo strategic development with direction before matches. However, directly applying existing methods suffers from the inherent hierarchy of the match and the compounding effect due to the turn-based nature of players alternatively taking actions. In this paper, we propose RallyNet, a novel hierarchical offline imitation learning model for badminton player behaviors: (i) RallyNet captures players' decision dependencies by modeling decision-making processes as 
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#19978;&#19979;&#25991;&#19990;&#30028;&#27169;&#22411;&#26377;&#21161;&#20110;&#25552;&#39640;&#22312;&#26410;&#30693;&#19978;&#19979;&#25991;&#19979;&#30340;&#38646;&#26679;&#28857;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.10967</link><description>&lt;p&gt;
&#26790;&#24819;&#20013;&#30340;&#35768;&#22810;&#19990;&#30028;&#65306;&#23398;&#20064;&#19978;&#19979;&#25991;&#19990;&#30028;&#27169;&#22411;&#26377;&#21161;&#20110;&#38646;&#26679;&#28857;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Dreaming of Many Worlds: Learning Contextual World Models Aids Zero-Shot Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10967
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#19978;&#19979;&#25991;&#19990;&#30028;&#27169;&#22411;&#26377;&#21161;&#20110;&#25552;&#39640;&#22312;&#26410;&#30693;&#19978;&#19979;&#25991;&#19979;&#30340;&#38646;&#26679;&#28857;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#28857;&#27867;&#21270;&#65288;Zero-shot generalization&#65292;ZSG&#65289;&#21040;&#26410;&#35265;&#36807;&#30340;&#21160;&#24577;&#23545;&#20110;&#21019;&#24314;&#20855;&#26377;&#26222;&#36941;&#33021;&#21147;&#30340;&#20307;&#31995;&#20195;&#29702;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#26356;&#24191;&#27867;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#20174;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#65288;contextual reinforcement learning&#65292;cRL&#65289;&#30340;&#31616;&#21333;&#35774;&#32622;&#24320;&#22987;&#65292;&#20551;&#35774;&#21487;&#35266;&#23519;&#21040;&#21442;&#25968;&#21270;&#31995;&#32479;&#21160;&#24577;&#21464;&#21270;&#30340;&#19978;&#19979;&#25991;&#20540;&#65292;&#22914;&#26426;&#22120;&#20154;&#30340;&#36136;&#37327;&#25110;&#23610;&#23544;&#65292;&#32780;&#19981;&#23545;&#39532;&#23572;&#21487;&#22827;&#29366;&#24577;&#30340;&#21487;&#35266;&#23519;&#24615;&#20570;&#36827;&#19968;&#27493;&#31616;&#21270;&#20551;&#35774;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#26410;&#30693;&#19978;&#19979;&#25991;&#21464;&#21270;&#30340;ZSG&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#24490;&#29615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;contextual recurrent state-space model&#65292;cRSSM&#65289;&#65292;&#23427;&#23545;Dreamer&#65288;v3&#65289;&#65288;Hafner&#31561;&#20154;&#65292;2023&#24180;&#65289;&#30340;&#19990;&#30028;&#27169;&#22411;&#36827;&#34892;&#20102;&#20462;&#25913;&#65292;&#20351;&#24471;&#19990;&#30028;&#27169;&#22411;&#21487;&#20197;&#34701;&#20837;&#19978;&#19979;&#25991;&#20197;&#20174;&#35266;&#23519;&#20013;&#25512;&#26029;&#28508;&#22312;&#30340;&#39532;&#23572;&#21487;&#22827;&#29366;&#24577;&#24182;&#24314;&#27169;&#28508;&#22312;&#21160;&#24577;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#31995;&#32479;&#24615;&#22320;&#23558;&#19978;&#19979;&#25991;&#32435;&#20837;&#20854;&#20013;&#25552;&#39640;&#20102;&#22312;&#8220;&#26790;&#22659;&#8221;&#35757;&#32451;&#30340;&#31574;&#30053;&#30340;ZSG&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10967v1 Announce Type: cross  Abstract: Zero-shot generalization (ZSG) to unseen dynamics is a major challenge for creating generally capable embodied agents. To address the broader challenge, we start with the simpler setting of contextual reinforcement learning (cRL), assuming observability of the context values that parameterize the variation in the system's dynamics, such as the mass or dimensions of a robot, without making further simplifying assumptions about the observability of the Markovian state. Toward the goal of ZSG to unseen variation in context, we propose the contextual recurrent state-space model (cRSSM), which introduces changes to the world model of the Dreamer (v3) (Hafner et al., 2023). This allows the world model to incorporate context for inferring latent Markovian states from the observations and modeling the latent dynamics. Our experiments show that such systematic incorporation of the context improves the ZSG of the policies trained on the ``dreams
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21078;&#26512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#39318;&#35201;&#20559;&#24046;&#29616;&#35937;&#65292;&#21457;&#29616;&#22312;&#22823;&#37327;&#26356;&#26032;&#27604;&#20363;&#19979;&#65292;&#20215;&#20540;&#39640;&#20272;&#26159;&#23548;&#33268;&#23398;&#20064;&#22833;&#36133;&#30340;&#26681;&#26412;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.05996</link><description>&lt;p&gt;
&#29992;&#39640;&#26356;&#26032;&#27604;&#20363;&#21078;&#26512;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65306;&#24212;&#23545;&#20215;&#20540;&#39640;&#20272;&#21644;&#21457;&#25955;
&lt;/p&gt;
&lt;p&gt;
Dissecting Deep RL with High Update Ratios: Combatting Value Overestimation and Divergence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21078;&#26512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#39318;&#35201;&#20559;&#24046;&#29616;&#35937;&#65292;&#21457;&#29616;&#22312;&#22823;&#37327;&#26356;&#26032;&#27604;&#20363;&#19979;&#65292;&#20215;&#20540;&#39640;&#20272;&#26159;&#23548;&#33268;&#23398;&#20064;&#22833;&#36133;&#30340;&#26681;&#26412;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#35774;&#32622;&#20013;&#21487;&#20197;&#22312;&#26799;&#24230;&#26356;&#26032;&#27425;&#25968;&#22823;&#22823;&#36229;&#36807;&#29615;&#22659;&#26679;&#26412;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#23398;&#20064;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#37325;&#32622;&#32593;&#32476;&#21442;&#25968;&#12290;&#22312;&#36825;&#31181;&#22823;&#37327;&#26356;&#26032;&#19982;&#25968;&#25454;&#27604;&#20363;&#30340;&#24773;&#20917;&#19979;&#65292;&#23612;&#22522;&#36763;&#31561;&#20154; (2022) &#30340;&#26368;&#36817;&#19968;&#39033;&#30740;&#31350;&#25351;&#20986;&#20102;&#19968;&#20010;&#39318;&#35201;&#20559;&#24046;&#30340;&#20986;&#29616;&#65292;&#21363;&#20195;&#29702;&#22312;&#26089;&#26399;&#20132;&#20114;&#20013;&#36807;&#25311;&#21512;&#24182;&#28129;&#21270;&#21518;&#32493;&#32463;&#39564;&#65292;&#20174;&#32780;&#25439;&#23475;&#20102;&#20854;&#23398;&#20064;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#35299;&#26512;&#20102;&#23548;&#33268;&#39318;&#35201;&#20559;&#24046;&#30340;&#29616;&#35937;&#12290;&#25105;&#20204;&#26816;&#26597;&#20102;&#24212;&#35813;&#23548;&#33268;&#23398;&#20064;&#22833;&#36133;&#30340;&#35757;&#32451;&#26089;&#26399;&#38454;&#27573;&#65292;&#24182;&#21457;&#29616;&#19968;&#20010;&#26681;&#26412;&#24615;&#25361;&#25112;&#26159;&#38271;&#26399;&#20197;&#26469;&#23384;&#22312;&#30340;&#38382;&#39064;&#65306;&#20215;&#20540;&#39640;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;Q&#20540;&#19981;&#20165;&#22312;&#20998;&#24067;&#22806;&#25968;&#25454;&#19978;&#34987;&#39640;&#20272;&#65292;&#32780;&#19988;&#22312;&#20998;&#24067;&#20869;&#25968;&#25454;&#19978;&#20063;&#26159;&#22914;&#27492;&#65292;&#21487;&#20197;&#36861;&#28335;&#21040;&#30001;&#20248;&#21270;&#22120;&#21160;&#37327;&#25512;&#21160;&#30340;&#26410;&#35265;&#30340;&#21160;&#20316;&#39044;&#27979;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21333;&#20301;&#29699;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22823;&#26356;&#26032;&#27604;&#20363;&#19979;&#23454;&#29616;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05996v1 Announce Type: cross  Abstract: We show that deep reinforcement learning can maintain its ability to learn without resetting network parameters in settings where the number of gradient updates greatly exceeds the number of environment samples. Under such large update-to-data ratios, a recent study by Nikishin et al. (2022) suggested the emergence of a primacy bias, in which agents overfit early interactions and downplay later experience, impairing their ability to learn. In this work, we dissect the phenomena underlying the primacy bias. We inspect the early stages of training that ought to cause the failure to learn and find that a fundamental challenge is a long-standing acquaintance: value overestimation. Overinflated Q-values are found not only on out-of-distribution but also in-distribution data and can be traced to unseen action prediction propelled by optimizer momentum. We employ a simple unit-ball normalization that enables learning under large update ratios
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#21644;&#24378;&#21270;&#23398;&#20064;&#23545;&#40784;&#31243;&#24207;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#25512;&#33616;&#25351;&#20196;&#21644;&#20943;&#23569;&#26684;&#24335;&#38169;&#35823;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.05063</link><description>&lt;p&gt;
&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#23454;&#29616;&#21487;&#25511;&#30340;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Aligning Large Language Models for Controllable Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05063
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#21644;&#24378;&#21270;&#23398;&#20064;&#23545;&#40784;&#31243;&#24207;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#25512;&#33616;&#25351;&#20196;&#21644;&#20943;&#23569;&#26684;&#24335;&#38169;&#35823;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24322;&#24120;&#30340;&#26234;&#33021;&#21551;&#21457;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#24320;&#22987;&#25506;&#32034;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#24320;&#21019;&#19979;&#19968;&#20195;&#25512;&#33616;&#31995;&#32479; - &#36825;&#20123;&#31995;&#32479;&#20855;&#26377;&#23545;&#35805;&#12289;&#21487;&#35299;&#37322;&#21644;&#21487;&#25511;&#30340;&#29305;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#20027;&#35201;&#38598;&#20013;&#22312;&#23558;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#25972;&#21512;&#21040;LLMs&#20013;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#36890;&#24120;&#24573;&#30053;&#20102;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#19968;&#32452;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#26631;&#35760;&#26469;&#28304;&#20110;&#20256;&#32479;&#25512;&#33616;&#27169;&#22411;&#30340;&#26631;&#31614;&#65292;&#26088;&#22312;&#26126;&#30830;&#25913;&#21892;LLMs&#36981;&#24490;&#29305;&#23450;&#25512;&#33616;&#25351;&#20196;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#23545;&#40784;&#31243;&#24207;&#65292;&#36827;&#19968;&#27493;&#21152;&#24378;&#20102;LLMs&#22312;&#21709;&#24212;&#29992;&#25143;&#24847;&#22270;&#21644;&#20943;&#23569;&#26684;&#24335;&#38169;&#35823;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26631;&#35760;&#30528;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05063v1 Announce Type: cross  Abstract: Inspired by the exceptional general intelligence of Large Language Models (LLMs), researchers have begun to explore their application in pioneering the next generation of recommender systems - systems that are conversational, explainable, and controllable. However, existing literature primarily concentrates on integrating domain-specific knowledge into LLMs to enhance accuracy, often neglecting the ability to follow instructions. To address this gap, we initially introduce a collection of supervised learning tasks, augmented with labels derived from a conventional recommender model, aimed at explicitly improving LLMs' proficiency in adhering to recommendation-specific instructions. Subsequently, we develop a reinforcement learning-based alignment procedure to further strengthen LLMs' aptitude in responding to users' intentions and mitigating formatting errors. Through extensive experiments on two real-world datasets, our method markedl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PrimeComposer&#65292;&#19968;&#31181;&#26356;&#24555;&#30340;&#36880;&#27493;&#32452;&#21512;&#25193;&#25955;&#26041;&#24335;&#65292;&#29992;&#20110;&#22270;&#20687;&#21512;&#25104;&#65292;&#20027;&#35201;&#19987;&#27880;&#20110;&#21069;&#26223;&#29983;&#25104;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#21512;&#25104;&#20013;&#30340;&#20957;&#32858;&#28151;&#20081;&#21644;&#22806;&#35266;&#20449;&#24687;&#20002;&#22833;&#38382;&#39064;&#65292;&#24182;&#36991;&#20813;&#20102;&#19981;&#24517;&#35201;&#30340;&#32972;&#26223;&#29983;&#25104;&#23548;&#33268;&#30340;&#21069;&#26223;&#29983;&#25104;&#36136;&#37327;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.05053</link><description>&lt;p&gt;
PrimeComposer&#65306;&#29992;&#20110;&#22270;&#20687;&#21512;&#25104;&#30340;&#24555;&#36895;&#36880;&#27493;&#32452;&#21512;&#25193;&#25955;&#26041;&#27861;&#21644;&#24102;&#26377;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
PrimeComposer: Faster Progressively Combined Diffusion for Image Composition with Attention Steering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PrimeComposer&#65292;&#19968;&#31181;&#26356;&#24555;&#30340;&#36880;&#27493;&#32452;&#21512;&#25193;&#25955;&#26041;&#24335;&#65292;&#29992;&#20110;&#22270;&#20687;&#21512;&#25104;&#65292;&#20027;&#35201;&#19987;&#27880;&#20110;&#21069;&#26223;&#29983;&#25104;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#21512;&#25104;&#20013;&#30340;&#20957;&#32858;&#28151;&#20081;&#21644;&#22806;&#35266;&#20449;&#24687;&#20002;&#22833;&#38382;&#39064;&#65292;&#24182;&#36991;&#20813;&#20102;&#19981;&#24517;&#35201;&#30340;&#32972;&#26223;&#29983;&#25104;&#23548;&#33268;&#30340;&#21069;&#26223;&#29983;&#25104;&#36136;&#37327;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#21512;&#25104;&#28041;&#21450;&#23558;&#32473;&#23450;&#23545;&#35937;&#26080;&#32541;&#22320;&#25972;&#21512;&#21040;&#29305;&#23450;&#30340;&#35270;&#35273;&#29615;&#22659;&#20013;&#12290;&#30446;&#21069;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#20174;&#20960;&#20010;&#37319;&#26679;&#22120;&#20013;&#32452;&#21512;&#27880;&#24847;&#21147;&#26435;&#37325;&#26469;&#24341;&#23548;&#29983;&#25104;&#22120;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#26435;&#37325;&#26469;&#33258;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#65292;&#23427;&#20204;&#30340;&#32452;&#21512;&#23548;&#33268;&#22312;&#21512;&#25104;&#20013;&#20957;&#32858;&#28151;&#20081;&#21644;&#22806;&#35266;&#20449;&#24687;&#30340;&#20002;&#22833;&#12290;&#22312;&#35813;&#20219;&#21153;&#20013;&#65292;&#23427;&#20204;&#36807;&#22810;&#20851;&#27880;&#32972;&#26223;&#29983;&#25104;&#65292;&#21363;&#20351;&#22312;&#36825;&#39033;&#20219;&#21153;&#20013;&#26159;&#19981;&#24517;&#35201;&#30340;&#65292;&#36825;&#20123;&#38382;&#39064;&#24694;&#21270;&#12290;&#36825;&#19981;&#20165;&#20943;&#24930;&#20102;&#25512;&#29702;&#36895;&#24230;&#65292;&#36824;&#25439;&#23475;&#20102;&#21069;&#26223;&#29983;&#25104;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#36824;&#22312;&#36807;&#28193;&#21306;&#22495;&#24341;&#20837;&#20102;&#19981;&#38656;&#35201;&#30340;&#20266;&#24433;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#22270;&#20687;&#21512;&#25104;&#24418;&#24335;&#21270;&#20026;&#19968;&#39033;&#22522;&#20110;&#20027;&#39064;&#30340;&#23616;&#37096;&#32534;&#36753;&#20219;&#21153;&#65292;&#20165;&#19987;&#27880;&#20110;&#21069;&#26223;&#29983;&#25104;&#12290;&#22312;&#27599;&#19968;&#27493;&#20013;&#65292;&#32534;&#36753;&#21518;&#30340;&#21069;&#26223;&#19982;&#22122;&#22768;&#32972;&#26223;&#30456;&#32467;&#21512;&#65292;&#20197;&#20445;&#25345;&#22330;&#26223;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#21097;&#19979;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PrimeComposer&#65292;&#19968;&#31181;&#26356;&#24555;&#30340;tr
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05053v1 Announce Type: cross  Abstract: Image composition involves seamlessly integrating given objects into a specific visual context. The current training-free methods rely on composing attention weights from several samplers to guide the generator. However, since these weights are derived from disparate contexts, their combination leads to coherence confusion in synthesis and loss of appearance information. These issues worsen with their excessive focus on background generation, even when unnecessary in this task. This not only slows down inference but also compromises foreground generation quality. Moreover, these methods introduce unwanted artifacts in the transition area. In this paper, we formulate image composition as a subject-based local editing task, solely focusing on foreground generation. At each step, the edited foreground is combined with the noisy background to maintain scene consistency. To address the remaining issues, we propose PrimeComposer, a faster tr
&lt;/p&gt;</description></item><item><title>&#26102;&#38388;&#24207;&#21015;&#22686;&#37327;&#23398;&#20064;&#38382;&#39064;&#22312;&#22270;&#20687;&#21644;&#35821;&#35328;&#39046;&#22495;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26041;&#38754;&#20173;&#28982;&#30456;&#23545;&#36739;&#23569;&#30740;&#31350;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.12035</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#30340;&#22686;&#37327;&#24335;&#23398;&#20064;: &#22522;&#20934;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Class-incremental Learning for Time Series: Benchmark and Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12035
&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#22686;&#37327;&#23398;&#20064;&#38382;&#39064;&#22312;&#22270;&#20687;&#21644;&#35821;&#35328;&#39046;&#22495;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26041;&#38754;&#20173;&#28982;&#30456;&#23545;&#36739;&#23569;&#30740;&#31350;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#29615;&#22659;&#26412;&#36136;&#19978;&#26159;&#38750;&#24179;&#31283;&#30340;&#65292;&#32463;&#24120;&#20250;&#38543;&#26102;&#38388;&#24341;&#20837;&#26032;&#30340;&#31867;&#21035;&#12290;&#36825;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#23588;&#20026;&#24120;&#35265;&#65292;&#27604;&#22914;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20986;&#29616;&#26032;&#30340;&#30142;&#30149;&#20998;&#31867;&#65292;&#25110;&#32773;&#22312;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#28155;&#21152;&#26032;&#30340;&#27963;&#21160;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#38656;&#35201;&#19968;&#20010;&#23398;&#20064;&#31995;&#32479;&#33021;&#22815;&#26377;&#25928;&#22320;&#21560;&#25910;&#26032;&#30340;&#31867;&#21035;&#65292;&#21516;&#26102;&#36991;&#20813;&#23545;&#26087;&#31867;&#21035;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#36825;&#23601;&#24341;&#21457;&#20102;&#22686;&#37327;&#24335;&#23398;&#20064;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#22270;&#20687;&#21644;&#35821;&#35328;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#36827;&#23637;&#65292;&#20294;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22686;&#37327;&#24335;&#23398;&#20064;&#20173;&#28982;&#30456;&#23545;&#36739;&#23569;&#30740;&#31350;&#12290;&#29616;&#26377;&#30740;&#31350;&#23384;&#22312;&#23454;&#39564;&#35774;&#35745;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#23545;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;&#26102;&#38388;&#24207;&#21015;&#22686;&#37327;&#23398;&#20064;&#65288;TSCIL&#65289;&#38382;&#39064;&#65292;&#31361;&#20986;&#20102;&#20854;&#29420;&#29305;&#25361;&#25112;&#65292;&#24182;&#35206;&#30422;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12035v1 Announce Type: cross  Abstract: Real-world environments are inherently non-stationary, frequently introducing new classes over time. This is especially common in time series classification, such as the emergence of new disease classification in healthcare or the addition of new activities in human activity recognition. In such cases, a learning system is required to assimilate novel classes effectively while avoiding catastrophic forgetting of the old ones, which gives rise to the Class-incremental Learning (CIL) problem. However, despite the encouraging progress in the image and language domains, CIL for time series data remains relatively understudied. Existing studies suffer from inconsistent experimental designs, necessitating a comprehensive evaluation and benchmarking of methods across a wide range of datasets. To this end, we first present an overview of the Time Series Class-incremental Learning (TSCIL) problem, highlight its unique challenges, and cover the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#24320;&#28304;&#30340;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#25351;&#23548;&#25968;&#25454;&#38598;ECInstruct&#65292;&#36890;&#36807;&#25351;&#23548;&#35843;&#20248;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#31995;&#21015;&#30005;&#23376;&#21830;&#21153;LLMs&#65288;eCeLLM&#65289;&#65292;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.08831</link><description>&lt;p&gt;
eCeLLM&#65306;&#20174;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#25351;&#23548;&#25968;&#25454;&#20013;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#24191;&#21040;&#30005;&#23376;&#21830;&#21153;&#20013;
&lt;/p&gt;
&lt;p&gt;
eCeLLM: Generalizing Large Language Models for E-commerce from Large-scale, High-quality Instruction Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#24320;&#28304;&#30340;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#25351;&#23548;&#25968;&#25454;&#38598;ECInstruct&#65292;&#36890;&#36807;&#25351;&#23548;&#35843;&#20248;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#31995;&#21015;&#30005;&#23376;&#21830;&#21153;LLMs&#65288;eCeLLM&#65289;&#65292;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#24320;&#21457;&#26377;&#25928;&#30340;&#30005;&#23376;&#21830;&#21153;&#27169;&#22411;&#26041;&#38754;&#20570;&#20986;&#24040;&#22823;&#21162;&#21147;&#65292;&#20256;&#32479;&#30340;&#30005;&#23376;&#21830;&#21153;&#27169;&#22411;&#22312;&#36890;&#29992;&#30005;&#23376;&#21830;&#21153;&#24314;&#27169;&#19978;&#21462;&#24471;&#20102;&#26377;&#38480;&#30340;&#25104;&#21151;&#65292;&#24182;&#19988;&#22312;&#26032;&#29992;&#25143;&#21644;&#26032;&#20135;&#21697;&#19978;&#30340;&#34920;&#29616;&#19981;&#20339;&#8212;&#8212;&#36825;&#26159;&#19968;&#20010;&#20856;&#22411;&#30340;&#39046;&#22495;&#22806;&#27867;&#21270;&#25361;&#25112;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35768;&#22810;&#39046;&#22495;&#23637;&#31034;&#20986;&#20102;&#20986;&#33394;&#30340;&#36890;&#29992;&#24314;&#27169;&#21644;&#39046;&#22495;&#22806;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;&#23427;&#20204;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#30340;&#20316;&#29992;&#65292;&#26412;&#25991;&#26500;&#24314;&#20102;ECInstruct&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38754;&#21521;&#30005;&#23376;&#21830;&#21153;&#30340;&#24320;&#28304;&#12289;&#22823;&#35268;&#27169;&#21644;&#39640;&#36136;&#37327;&#30340;&#25351;&#23548;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;ECInstruct&#65292;&#25105;&#20204;&#36890;&#36807;&#25351;&#23548;&#35843;&#20248;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#31995;&#21015;&#30005;&#23376;&#21830;&#21153;LLMs&#65292;&#31216;&#20026;eCeLLM&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#23454;&#39564;&#21644;&#35780;&#20272;&#34920;&#26126;&#65292;eCeLLM&#27169;&#22411;&#22312;&#20869;&#37096;&#29615;&#22659;&#20013;&#26126;&#26174;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#21253;&#25324;&#26368;&#20808;&#36827;&#30340;GPT-4&#21644;&#26368;&#20808;&#36827;&#30340;&#29305;&#23450;&#20219;&#21153;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08831v1 Announce Type: cross Abstract: With tremendous efforts on developing effective e-commerce models, conventional e-commerce models show limited success in generalist e-commerce modeling, and suffer from unsatisfactory performance on new users and new products - a typical out-of-domain generalization challenge. Meanwhile, large language models (LLMs) demonstrate outstanding performance in generalist modeling and out-of-domain generalizability in many fields. Toward fully unleashing their power for e-commerce, in this paper, we construct ECInstruct, the first open-sourced, large-scale, and high-quality benchmark instruction dataset for e-commerce. Leveraging ECInstruct, we develop eCeLLM, a series of e-commerce LLMs, by instruction-tuning general-purpose LLMs. Our comprehensive experiments and evaluation demonstrate that eCeLLM models substantially outperform baseline models, including the most advanced GPT-4, and the state-of-the-art task-specific models in in-domain ev
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#21644;&#35268;&#21010;&#20219;&#21153;&#20013;&#30340;&#33258;&#25105;&#39564;&#35777;&#33021;&#21147;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#20102;&#36845;&#20195;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08115</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#21644;&#35268;&#21010;&#20219;&#21153;&#20013;&#30340;&#33258;&#25105;&#39564;&#35777;&#38480;&#21046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On the Self-Verification Limitations of Large Language Models on Reasoning and Planning Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08115
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#21644;&#35268;&#21010;&#20219;&#21153;&#20013;&#30340;&#33258;&#25105;&#39564;&#35777;&#33021;&#21147;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#20102;&#36845;&#20195;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#23384;&#22312;&#30528;&#36739;&#22823;&#30340;&#35266;&#28857;&#24046;&#24322;&#12290;&#23613;&#31649;&#26368;&#21021;&#23545;&#20110;&#25512;&#29702;&#21487;&#33021;&#20250;&#38543;&#30528;&#35268;&#27169;&#30340;&#25193;&#22823;&#33258;&#21160;&#20986;&#29616;&#30340;&#20048;&#35266;&#24773;&#32490;&#24050;&#32463;&#21463;&#21040;&#20102;&#19968;&#31995;&#21015;&#21453;&#20363;&#30340;&#25233;&#21046;&#65292;&#20174;&#20056;&#27861;&#21040;&#31616;&#21333;&#35268;&#21010;&#65292;&#20294;&#20173;&#28982;&#26222;&#36941;&#35748;&#20026;LLMs&#21487;&#20197;&#33258;&#25105;&#25209;&#21028;&#24182;&#36845;&#20195;&#25913;&#36827;&#20854;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#31181;&#20449;&#24565;&#20284;&#20046;&#24314;&#31435;&#22312;&#39564;&#35777;&#27491;&#30830;&#24615;&#27604;&#29983;&#25104;&#26356;&#23481;&#26131;&#30340;&#20551;&#35774;&#19978;&#65292;&#36825;&#26159;&#19968;&#20010;&#20856;&#22411;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#35770;&#35777;&#65292;&#23545;&#20110;LLMs&#26469;&#35828;&#24212;&#35813;&#26159;&#26080;&#20851;&#32039;&#35201;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#25152;&#20570;&#30340;&#26159;&#36817;&#20284;&#26816;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25512;&#29702;&#21644;&#35268;&#21010;&#29615;&#22659;&#20013;&#36845;&#20195;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#12290; &#25105;&#20204;&#23545;GPT-4&#22312;&#19977;&#20010;&#39046;&#22495;&#65288;24&#28857;&#28216;&#25103;&#12289;&#22270;&#30528;&#33394;&#21644;STRIPS&#35268;&#21010;&#65289;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#26377;&#21407;&#21017;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#25105;&#20204;&#23545;&#27169;&#22411;&#30340;&#25209;&#21028;&#24615;&#23454;&#39564;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been considerable divergence of opinion on the reasoning abilities of Large Language Models (LLMs). While the initial optimism that reasoning might emerge automatically with scale has been tempered thanks to a slew of counterexamples--ranging from multiplication to simple planning--there persists a wide spread belief that LLMs can self-critique and improve their own solutions in an iterative fashion. This belief seemingly rests on the assumption that verification of correctness should be easier than generation--a rather classical argument from computational complexity--which should be irrelevant to LLMs to the extent that what they are doing is approximate retrieval. In this paper, we set out to systematically investigate the effectiveness of iterative prompting in the context of reasoning and planning. We present a principled empirical study of the performance of GPT-4 in three domains: Game of 24, Graph Coloring, and STRIPS planning. We experiment both with the model critiq
&lt;/p&gt;</description></item><item><title>2023&#24180;10&#26376;&#65292;&#19968;&#36215;GM Cruise&#26426;&#22120;&#20154;&#20986;&#31199;&#36710;&#19982;&#34892;&#20154;&#30456;&#25758;&#30340;&#20107;&#25925;&#32473;&#20844;&#21496;&#36896;&#25104;&#20102;&#24040;&#22823;&#30340;&#21160;&#33633;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#35813;&#20844;&#21496;&#22312;&#22788;&#29702;&#20107;&#25925;&#21518;&#30340;&#22833;&#35823;&#12290;&#36825;&#19968;&#20107;&#20214;&#30340;&#35299;&#21078;&#25552;&#20379;&#20102;&#22312;&#25216;&#26415;&#12289;&#25805;&#20316;&#23433;&#20840;&#23454;&#36341;&#21644;&#32452;&#32455;&#21453;&#24212;&#26041;&#38754;&#30340;&#23433;&#20840;&#25945;&#35757;&#12290;</title><link>https://arxiv.org/abs/2402.06046</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#20986;&#31199;&#36710;&#20107;&#25925;&#30340;&#35299;&#21078;&#65306;&#20174;Cruise&#34892;&#20154;&#25302;&#25341;&#20107;&#25925;&#20013;&#21560;&#21462;&#25945;&#35757;
&lt;/p&gt;
&lt;p&gt;
Anatomy of a Robotaxi Crash: Lessons from the Cruise Pedestrian Dragging Mishap
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06046
&lt;/p&gt;
&lt;p&gt;
2023&#24180;10&#26376;&#65292;&#19968;&#36215;GM Cruise&#26426;&#22120;&#20154;&#20986;&#31199;&#36710;&#19982;&#34892;&#20154;&#30456;&#25758;&#30340;&#20107;&#25925;&#32473;&#20844;&#21496;&#36896;&#25104;&#20102;&#24040;&#22823;&#30340;&#21160;&#33633;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#35813;&#20844;&#21496;&#22312;&#22788;&#29702;&#20107;&#25925;&#21518;&#30340;&#22833;&#35823;&#12290;&#36825;&#19968;&#20107;&#20214;&#30340;&#35299;&#21078;&#25552;&#20379;&#20102;&#22312;&#25216;&#26415;&#12289;&#25805;&#20316;&#23433;&#20840;&#23454;&#36341;&#21644;&#32452;&#32455;&#21453;&#24212;&#26041;&#38754;&#30340;&#23433;&#20840;&#25945;&#35757;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2023&#24180;10&#26376;&#65292;&#22312;&#26087;&#37329;&#23665;&#65292;&#19968;&#36742;&#36890;&#29992;&#27773;&#36710;Cruise&#30340;&#26426;&#22120;&#20154;&#20986;&#31199;&#36710;&#19982;&#19968;&#21517;&#34892;&#20154;&#30456;&#25758;&#65292;&#36896;&#25104;&#20102;&#20005;&#37325;&#30340;&#20260;&#23475;&#65292;&#21516;&#26102;&#20063;&#23545;&#35813;&#20844;&#21496;&#36896;&#25104;&#20102;&#24040;&#22823;&#30340;&#21160;&#33633;&#65292;&#36825;&#24456;&#21487;&#33021;&#20250;&#23545;&#25972;&#20010;&#34892;&#19994;&#20135;&#29983;&#38271;&#26399;&#24433;&#21709;&#12290;&#38382;&#39064;&#19981;&#20165;&#20165;&#28304;&#20110;&#20107;&#25925;&#26412;&#36523;&#65292;&#36824;&#21253;&#25324;Cruise&#22312;&#22788;&#29702;&#26426;&#22120;&#20154;&#20986;&#31199;&#36710;&#25758;&#21040;&#34892;&#20154;&#21518;&#34987;&#25302;&#34892;&#30340;&#36807;&#31243;&#20013;&#30340;&#22833;&#35823;&#12290;&#20004;&#20221;&#22806;&#37096;&#35843;&#26597;&#25253;&#21578;&#25552;&#20379;&#20102;&#25551;&#36848;&#20107;&#20214;&#30340;&#21407;&#22987;&#26448;&#26009;&#65292;&#24182;&#20174;&#30417;&#31649;&#20114;&#21160;&#30340;&#35282;&#24230;&#25209;&#35780;&#20102;&#20844;&#21496;&#30340;&#21453;&#24212;&#65292;&#20294;&#24182;&#26410;&#21253;&#21547;&#28508;&#22312;&#30340;&#23433;&#20840;&#24314;&#35758;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#25253;&#21578;&#26448;&#26009;&#26469;&#24378;&#35843;&#20855;&#20307;&#30340;&#20107;&#23454;&#21644;&#20107;&#20214;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36890;&#36807;&#23558;&#25253;&#21578;&#26448;&#26009;&#30340;&#19981;&#21516;&#37096;&#20998;&#32852;&#31995;&#36215;&#26469;&#12290;&#28982;&#21518;&#25105;&#20204;&#25506;&#35752;&#22312;&#25216;&#26415;&#12289;&#25805;&#20316;&#23433;&#20840;&#23454;&#36341;&#21644;&#32452;&#32455;&#23545;&#20107;&#20214;&#30340;&#21453;&#24212;&#26041;&#38754;&#21487;&#33021;&#21487;&#20197;&#23398;&#21040;&#30340;&#23433;&#20840;&#25945;&#35757;&#12290;
&lt;/p&gt;
&lt;p&gt;
An October 2023 crash between a GM Cruise robotaxi and a pedestrian in San Francisco resulted not only in a severe injury, but also dramatic upheaval at that company that will likely have lasting effects throughout the industry. The issues stem not just from the crash facts themselves, but also how Cruise mishandled dealing with their robotaxi dragging a pedestrian under the vehicle after the initial post-crash stop. A pair of external investigation reports provide raw material describing the incident and critique the company response from a regulatory interaction point of view, but did not include potential safety recommendations in scope. We use that report material to highlight specific facts and relationships between events by tying together different pieces of the report material. We then explore safety lessons that might be learned with regard to technology, operational safety practices, and organizational reaction to incidents.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21464;&#20998;&#27969;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#36716;&#25442;&#26041;&#27861;&#65292;&#20351;&#24471;&#24555;&#36895;&#37319;&#26679;&#25104;&#20026;&#21487;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#37319;&#26679;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.02977</link><description>&lt;p&gt;
&#21464;&#20998;&#27969;&#27169;&#22411;&#65306;&#20197;&#20320;&#30340;&#39118;&#26684;&#27969;&#21160;
&lt;/p&gt;
&lt;p&gt;
Variational Flow Models: Flowing in Your Style
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02977
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21464;&#20998;&#27969;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#36716;&#25442;&#26041;&#27861;&#65292;&#20351;&#24471;&#24555;&#36895;&#37319;&#26679;&#25104;&#20026;&#21487;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#37319;&#26679;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23545;"&#21518;&#39564;&#27969;"&#27169;&#22411;&#36827;&#34892;&#21464;&#20998;&#25512;&#29702;&#35299;&#37322;&#30340;&#26041;&#27861;&#8212;&#8212;&#29992;&#20197;&#23558;"&#27010;&#29575;&#27969;"&#25512;&#24191;&#21040;&#26356;&#24191;&#27867;&#30340;&#38543;&#26426;&#36807;&#31243;&#31867;&#21035;&#65292;&#19981;&#24517;&#23616;&#38480;&#20110;&#25193;&#25955;&#36807;&#31243;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#32467;&#26524;&#31216;&#20026;"&#21464;&#20998;&#27969;&#27169;&#22411;"&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#31995;&#32479;&#26041;&#27861;&#65292;&#23558;&#30001;&#26041;&#31243;Xt = at * X0 + st * X1&#25152;&#25551;&#36848;&#30340;"&#32447;&#24615;"&#38543;&#26426;&#36807;&#31243;&#30340;&#21518;&#39564;&#27969;&#36716;&#21270;&#20026;&#30452;&#32447;&#24658;&#36895;(SC)&#27969;&#65292;&#31867;&#20284;&#20110;&#30699;&#27491;&#27969;&#12290;&#36825;&#31181;&#36716;&#21270;&#20351;&#24471;&#21487;&#20197;&#24555;&#36895;&#27839;&#30528;&#21407;&#22987;&#30340;&#21518;&#39564;&#27969;&#36827;&#34892;&#37319;&#26679;&#65292;&#32780;&#26080;&#38656;&#35757;&#32451;&#19968;&#20010;&#26032;&#30340;SC&#27969;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#36716;&#25442;&#25193;&#23637;&#21040;&#20004;&#20010;&#19981;&#21516;"&#32447;&#24615;"&#38543;&#26426;&#36807;&#31243;&#30340;&#21518;&#39564;&#27969;&#20043;&#38388;&#36827;&#34892;&#20114;&#30456;&#36716;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21487;&#20197;&#23558;&#39640;&#38454;&#25968;&#20540;&#35299;&#27861;&#36731;&#26494;&#38598;&#25104;&#21040;&#36716;&#25442;&#21518;&#30340;SC&#27969;&#20013;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#37319;&#26679;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#22823;&#37327;&#23454;&#39564;&#32467;&#26524;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a variational inference interpretation for models of "posterior flows" - generalizations of "probability flows" to a broader class of stochastic processes not necessarily diffusion processes. We coin the resulting models as "Variational Flow Models". Additionally, we propose a systematic training-free method to transform the posterior flow of a "linear" stochastic process characterized by the equation Xt = at * X0 + st * X1 into a straight constant-speed (SC) flow, reminiscent of Rectified Flow. This transformation facilitates fast sampling along the original posterior flow without training a new model of the SC flow. The flexibility of our approach allows us to extend our transformation to inter-convert two posterior flows from distinct "linear" stochastic processes. Moreover, we can easily integrate high-order numerical solvers into the transformed SC flow, further enhancing sampling accuracy and efficiency. Rigorous theoretical analysis and extensive experimental result
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38142;&#24335;&#36830;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#20840;&#38754;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#35838;&#31243;&#35268;&#21010;&#12289;&#20010;&#24615;&#21270;&#25351;&#23548;&#21644;&#28789;&#27963;&#27979;&#39564;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2309.08112</link><description>&lt;p&gt;
&#36890;&#36807;&#38142;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#31169;&#20154;&#36741;&#23548;
&lt;/p&gt;
&lt;p&gt;
Empowering Private Tutoring by Chaining Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.08112
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38142;&#24335;&#36830;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#20840;&#38754;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#35838;&#31243;&#35268;&#21010;&#12289;&#20010;&#24615;&#21270;&#25351;&#23548;&#21644;&#28789;&#27963;&#27979;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24050;&#34987;&#24212;&#29992;&#20110;&#22312;&#32447;&#25945;&#32946;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#20197;&#20419;&#36827;&#25945;&#23398;&#21644;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#26041;&#27861;&#33268;&#21147;&#20110;&#23436;&#25972;&#30340;AI&#36741;&#23548;&#31995;&#32479;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#20010;&#30001;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#30340;&#20840;&#38754;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#24320;&#21457;&#65292;&#28085;&#30422;&#33258;&#21160;&#35838;&#31243;&#35268;&#21010;&#21644;&#35843;&#25972;&#12289;&#23450;&#21046;&#25351;&#23548;&#20197;&#21450;&#28789;&#27963;&#30340;&#27979;&#39564;&#35780;&#20272;&#12290;&#20026;&#20102;&#20351;&#31995;&#32479;&#33021;&#22815;&#32463;&#21463;&#20303;&#38271;&#26102;&#38388;&#20132;&#20114;&#24182;&#28385;&#36275;&#20010;&#24615;&#21270;&#25945;&#32946;&#30340;&#38656;&#27714;&#65292;&#31995;&#32479;&#34987;&#20998;&#35299;&#20026;&#19977;&#20010;&#30456;&#20114;&#36830;&#25509;&#30340;&#26680;&#24515;&#27969;&#31243;-&#20132;&#20114;&#12289;&#21453;&#24605;&#21644;&#21453;&#24212;&#12290;&#27599;&#20010;&#27969;&#31243;&#37117;&#36890;&#36807;&#38142;&#25509;LLM&#39537;&#21160;&#30340;&#24037;&#20855;&#20197;&#21450;&#21160;&#24577;&#26356;&#26032;&#30340;&#35760;&#24518;&#27169;&#22359;&#26469;&#23454;&#29616;&#12290;&#24037;&#20855;&#26159;LLMs&#65292;&#34987;&#25552;&#31034;&#25191;&#34892;&#19968;&#39033;&#29305;&#23450;&#20219;&#21153;&#65292;&#32780;&#35760;&#24518;&#26159;&#22312;&#25945;&#32946;&#36807;&#31243;&#20013;&#26356;&#26032;&#30340;&#25968;&#25454;&#23384;&#20648;&#12290;&#23398;&#20064;&#26085;&#24535;&#20013;&#30340;&#32479;&#35745;&#32467;&#26524;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.08112v1 Announce Type: cross  Abstract: Artificial intelligence has been applied in various aspects of online education to facilitate teaching and learning. However, few approaches has been made toward a complete AI-powered tutoring system. In this work, we explore the development of a full-fledged intelligent tutoring system powered by state-of-the-art large language models (LLMs), covering automatic course planning and adjusting, tailored instruction, and flexible quiz evaluation. To make the system robust to prolonged interaction and cater to individualized education, the system is decomposed into three inter-connected core processes-interaction, reflection, and reaction. Each process is implemented by chaining LLM-powered tools along with dynamically updated memory modules. Tools are LLMs prompted to execute one specific task at a time, while memories are data storage that gets updated during education process. Statistical results from learning logs demonstrate the effec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;P2P&#20511;&#36151;&#24179;&#21488;&#19978;&#20511;&#27454;&#20154;&#25552;&#20379;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#26500;&#24314;&#39118;&#38505;&#25351;&#26631;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#39118;&#38505;&#35780;&#20998;&#21487;&#20197;&#26126;&#26174;&#25552;&#39640;&#20449;&#29992;&#39118;&#38505;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.16458</link><description>&lt;p&gt;
&#20449;&#29992;&#39118;&#38505;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65306;&#20174;P2P&#20511;&#36151;&#30340;&#36151;&#27454;&#25551;&#36848;&#20013;&#26500;&#24314;&#39118;&#38505;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Credit Risk Meets Large Language Models: Building a Risk Indicator from Loan Descriptions in P2P Lending. (arXiv:2401.16458v1 [q-fin.RM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;P2P&#20511;&#36151;&#24179;&#21488;&#19978;&#20511;&#27454;&#20154;&#25552;&#20379;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#26500;&#24314;&#39118;&#38505;&#25351;&#26631;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#39118;&#38505;&#35780;&#20998;&#21487;&#20197;&#26126;&#26174;&#25552;&#39640;&#20449;&#29992;&#39118;&#38505;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
P2P&#20511;&#36151;&#20316;&#20026;&#19968;&#31181;&#29420;&#29305;&#30340;&#34701;&#36164;&#26426;&#21046;&#65292;&#36890;&#36807;&#22312;&#32447;&#24179;&#21488;&#23558;&#20511;&#27454;&#20154;&#19982;&#25918;&#27454;&#20154;&#32852;&#31995;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;P2P&#20511;&#36151;&#38754;&#20020;&#20449;&#24687;&#19981;&#23545;&#31216;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#25918;&#27454;&#20154;&#24448;&#24448;&#32570;&#20047;&#36275;&#22815;&#30340;&#25968;&#25454;&#26469;&#35780;&#20272;&#20511;&#27454;&#20154;&#30340;&#20449;&#29992;&#20215;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21363;&#21033;&#29992;&#20511;&#27454;&#20154;&#22312;&#36151;&#27454;&#30003;&#35831;&#36807;&#31243;&#20013;&#25552;&#20379;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22788;&#29702;&#36825;&#20123;&#25991;&#26412;&#25551;&#36848;&#65292;LLM&#26159;&#19968;&#31181;&#33021;&#22815;&#35782;&#21035;&#25991;&#26412;&#20013;&#30340;&#27169;&#24335;&#21644;&#35821;&#20041;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#23558;&#36801;&#31227;&#23398;&#20064;&#24212;&#29992;&#20110;&#23558;LLM&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#12290;&#25105;&#20204;&#20174;Lending Club&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#32467;&#26524;&#26174;&#31034;&#65292;BERT&#29983;&#25104;&#30340;&#39118;&#38505;&#35780;&#20998;&#26174;&#33879;&#25552;&#39640;&#20102;&#20449;&#29992;&#39118;&#38505;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;LLM&#30340;&#31995;&#32479;&#22266;&#26377;&#30340;&#19981;&#36879;&#26126;&#24615;&#65292;&#20197;&#21450;&#28508;&#22312;&#20559;&#24046;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Peer-to-peer (P2P) lending has emerged as a distinctive financing mechanism, linking borrowers with lenders through online platforms. However, P2P lending faces the challenge of information asymmetry, as lenders often lack sufficient data to assess the creditworthiness of borrowers. This paper proposes a novel approach to address this issue by leveraging the textual descriptions provided by borrowers during the loan application process. Our methodology involves processing these textual descriptions using a Large Language Model (LLM), a powerful tool capable of discerning patterns and semantics within the text. Transfer learning is applied to adapt the LLM to the specific task at hand.  Our results derived from the analysis of the Lending Club dataset show that the risk score generated by BERT, a widely used LLM, significantly improves the performance of credit risk classifiers. However, the inherent opacity of LLM-based systems, coupled with uncertainties about potential biases, unders
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;&#20102;&#25903;&#25345;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#23398;&#20064;&#19981;&#21487;&#33021;&#30340;&#35821;&#35328;&#30340;&#35266;&#28857;&#65292;&#24320;&#21457;&#20102;&#19968;&#32452;&#20154;&#24037;&#21512;&#25104;&#30340;&#19981;&#21487;&#33021;&#35821;&#35328;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;GPT-2&#23567;&#22411;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#24471;&#20986;&#20102;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2401.06416</link><description>&lt;p&gt;
&#19981;&#21487;&#33021;&#20219;&#21153;&#65306;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mission: Impossible Language Models. (arXiv:2401.06416v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#20102;&#25903;&#25345;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#23398;&#20064;&#19981;&#21487;&#33021;&#30340;&#35821;&#35328;&#30340;&#35266;&#28857;&#65292;&#24320;&#21457;&#20102;&#19968;&#32452;&#20154;&#24037;&#21512;&#25104;&#30340;&#19981;&#21487;&#33021;&#35821;&#35328;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;GPT-2&#23567;&#22411;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#24471;&#20986;&#20102;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Chomsky&#21644;&#20854;&#20182;&#20154;&#30452;&#25509;&#22768;&#31216;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#23398;&#20064;&#20154;&#31867;&#26080;&#27861;&#23398;&#20064;&#30340;&#21487;&#33021;&#21644;&#19981;&#21487;&#33021;&#30340;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#21457;&#34920;&#30340;&#23454;&#39564;&#35777;&#25454;&#25903;&#25345;&#36825;&#26679;&#30340;&#35828;&#27861;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#31995;&#32479;&#22320;&#25913;&#21464;&#33521;&#25991;&#25968;&#25454;&#30340;&#35789;&#24207;&#21644;&#35821;&#27861;&#35268;&#21017;&#65292;&#24320;&#21457;&#20102;&#19968;&#32452;&#19981;&#21487;&#33021;&#30340;&#21512;&#25104;&#35821;&#35328;&#65292;&#27599;&#31181;&#35821;&#35328;&#30340;&#22797;&#26434;&#31243;&#24230;&#19981;&#21516;&#12290;&#36825;&#20123;&#35821;&#35328;&#20301;&#20110;&#19968;&#20010;&#19981;&#21487;&#33021;&#30340;&#36830;&#32493;&#20307;&#19978;&#65306;&#19968;&#31471;&#26159;&#26412;&#36136;&#19978;&#19981;&#21487;&#33021;&#30340;&#35821;&#35328;&#65292;&#20363;&#22914;&#33521;&#25991;&#21333;&#35789;&#30340;&#38543;&#26426;&#21644;&#19981;&#21487;&#36870;&#30340;&#27927;&#29260;&#65292;&#32780;&#21478;&#19968;&#31471;&#26159;&#22312;&#35821;&#35328;&#23398;&#19978;&#24120;&#34987;&#35748;&#20026;&#26159;&#19981;&#21487;&#33021;&#30340;&#35821;&#35328;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#35745;&#31639;&#35789;&#20301;&#32622;&#30340;&#35268;&#21017;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#26469;&#35780;&#20272;GPT-2&#23567;&#22411;&#27169;&#22411;&#23398;&#20064;&#36825;&#20123;&#26080;&#21487;&#20105;&#35758;&#30340;&#19981;&#21487;&#33021;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#25972;&#20010;&#36807;&#31243;&#20013;&#36827;&#34892;&#20102;&#36825;&#20123;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chomsky and others have very directly claimed that large language models (LLMs) are equally capable of learning languages that are possible and impossible for humans to learn. However, there is very little published experimental evidence to support such a claim. Here, we develop a set of synthetic impossible languages of differing complexity, each designed by systematically altering English data with unnatural word orders and grammar rules. These languages lie on an impossibility continuum: at one end are languages that are inherently impossible, such as random and irreversible shuffles of English words, and on the other, languages that may not be intuitively impossible but are often considered so in linguistics, particularly those with rules based on counting word positions. We report on a wide range of evaluations to assess the capacity of GPT-2 small models to learn these uncontroversially impossible languages, and crucially, we perform these assessments at various stages throughout
&lt;/p&gt;</description></item><item><title>&#29992;&#25143;&#36890;&#36807;&#33609;&#22270;&#21644;&#35821;&#35328;&#24314;&#31435;&#20114;&#21160;&#19990;&#30028;&#30340;&#20132;&#20114;&#24335;&#26041;&#27861;&#65292;&#20855;&#26377;&#29992;&#25143;&#25511;&#21046;&#21644;&#28789;&#27963;&#24615;&#65292;&#26080;&#38656;&#32534;&#31243;&#21363;&#21487;&#23454;&#29616;&#32534;&#31243;&#21151;&#33021;&#12290;&#36866;&#29992;&#20110;&#21508;&#31181;&#21019;&#36896;&#24615;&#25506;&#32034;&#24615;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2401.05631</link><description>&lt;p&gt;
DrawTalking&#65306;&#36890;&#36807;&#33609;&#22270;&#21644;&#35821;&#35328;&#24314;&#31435;&#20114;&#21160;&#19990;&#30028;
&lt;/p&gt;
&lt;p&gt;
DrawTalking: Building Interactive Worlds by Sketching and Speaking. (arXiv:2401.05631v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05631
&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#36890;&#36807;&#33609;&#22270;&#21644;&#35821;&#35328;&#24314;&#31435;&#20114;&#21160;&#19990;&#30028;&#30340;&#20132;&#20114;&#24335;&#26041;&#27861;&#65292;&#20855;&#26377;&#29992;&#25143;&#25511;&#21046;&#21644;&#28789;&#27963;&#24615;&#65292;&#26080;&#38656;&#32534;&#31243;&#21363;&#21487;&#23454;&#29616;&#32534;&#31243;&#21151;&#33021;&#12290;&#36866;&#29992;&#20110;&#21508;&#31181;&#21019;&#36896;&#24615;&#25506;&#32034;&#24615;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#26041;&#27861;&#65292;DrawTalking&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#33609;&#22270;&#21644;&#35821;&#35328;&#24314;&#31435;&#20114;&#21160;&#19990;&#30028;&#12290;&#23427;&#24378;&#35843;&#29992;&#25143;&#25511;&#21046;&#21644;&#28789;&#27963;&#24615;&#65292;&#24182;&#19988;&#22312;&#27809;&#26377;&#32534;&#31243;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#31867;&#20284;&#32534;&#31243;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;iPad&#19978;&#23454;&#29616;&#20102;&#23427;&#12290;&#19968;&#39033;&#24320;&#25918;&#24335;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#31181;&#26426;&#21046;&#19982;&#35768;&#22810;&#21019;&#36896;&#24615;&#25506;&#32034;&#24615;&#29992;&#20363;&#30456;&#22865;&#21512;&#21644;&#36866;&#29992;&#12290;&#25105;&#20204;&#24076;&#26395;&#33021;&#22815;&#28608;&#21457;&#21644;&#25351;&#23548;&#26410;&#26469;&#33258;&#28982;&#29992;&#25143;&#20013;&#24515;&#30028;&#38754;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an interactive approach, DrawTalking, in which the user builds interactive worlds by sketching and speaking. It emphasizes user control and flexibility, and gives programming-like capability without code. We implemented it on the iPad. An open-ended study shows the mechanics resonate and are applicable to many creative-exploratory use cases. We hope to inspire and inform research in future natural user-centered interfaces.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#22522;&#30784;&#65292;&#24320;&#21457;&#20102;&#21487;&#38752;&#30340;&#23398;&#26415;&#20250;&#35758;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#32452;&#32455;&#21322;&#32467;&#26500;&#21270;&#30340;&#20250;&#35758;&#25968;&#25454;&#24182;&#36827;&#34892;&#20154;&#24037;&#26631;&#27880;&#65292;&#35299;&#20915;&#20102;&#30740;&#31350;&#20154;&#21592;&#22312;&#33719;&#21462;&#20934;&#30830;&#12289;&#26368;&#26032;&#20449;&#24687;&#26102;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.13028</link><description>&lt;p&gt;
&#21487;&#38752;&#30340;&#23398;&#26415;&#20250;&#35758;&#38382;&#31572;&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Reliable Academic Conference Question Answering: A Study Based on Large Language Model. (arXiv:2310.13028v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#22522;&#30784;&#65292;&#24320;&#21457;&#20102;&#21487;&#38752;&#30340;&#23398;&#26415;&#20250;&#35758;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#32452;&#32455;&#21322;&#32467;&#26500;&#21270;&#30340;&#20250;&#35758;&#25968;&#25454;&#24182;&#36827;&#34892;&#20154;&#24037;&#26631;&#27880;&#65292;&#35299;&#20915;&#20102;&#30740;&#31350;&#20154;&#21592;&#22312;&#33719;&#21462;&#20934;&#30830;&#12289;&#26368;&#26032;&#20449;&#24687;&#26102;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#24555;&#36895;&#21457;&#23637;&#23548;&#33268;&#23398;&#26415;&#20250;&#35758;&#19978;&#30340;&#30740;&#31350;&#22823;&#37327;&#22686;&#21152;&#65292;&#20419;&#36827;&#20102;&#20840;&#29699;&#23398;&#26415;&#20132;&#27969;&#12290;&#30740;&#31350;&#20154;&#21592;&#22312;&#21508;&#20010;&#38454;&#27573;&#37117;&#25345;&#32493;&#23547;&#27714;&#20851;&#20110;&#36825;&#20123;&#20107;&#20214;&#30340;&#20934;&#30830;&#12289;&#26368;&#26032;&#20449;&#24687;&#12290;&#36825;&#31181;&#25968;&#25454;&#29190;&#21457;&#38656;&#35201;&#19968;&#20010;&#26234;&#33021;&#30340;&#38382;&#31572;&#31995;&#32479;&#26469;&#39640;&#25928;&#35299;&#20915;&#30740;&#31350;&#20154;&#21592;&#30340;&#38382;&#39064;&#65292;&#24182;&#30830;&#20445;&#23545;&#26368;&#26032;&#36827;&#23637;&#30340;&#20102;&#35299;&#12290;&#20250;&#35758;&#20449;&#24687;&#36890;&#24120;&#22312;&#23448;&#26041;&#32593;&#31449;&#19978;&#21457;&#24067;&#65292;&#20197;&#21322;&#32467;&#26500;&#21270;&#30340;&#26041;&#24335;&#32452;&#32455;&#65292;&#24182;&#21253;&#21547;&#22823;&#37327;&#30340;&#25991;&#26412;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#19968;&#38656;&#27714;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;ConferenceQA&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;7&#20010;&#19981;&#21516;&#23398;&#26415;&#20250;&#35758;&#65292;&#24182;&#36827;&#34892;&#20102;&#20154;&#24037;&#26631;&#27880;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37319;&#29992;&#25163;&#21160;&#21644;&#33258;&#21160;&#26041;&#27861;&#30340;&#32452;&#21512;&#65292;&#20197;&#21322;&#32467;&#26500;&#21270;&#30340;JSON&#26684;&#24335;&#32452;&#32455;&#23398;&#26415;&#20250;&#35758;&#25968;&#25454;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#20250;&#35758;&#27880;&#37322;&#20102;&#36817;100&#20010;&#38382;&#39064;-&#31572;&#26696;&#23545;&#12290;&#27599;&#20010;&#23545;&#24212;&#23545;&#24212;&#20102;&#22235;&#20010;&#19981;&#21516;&#30340;&#32500;&#24230;&#20998;&#31867;&#12290;&#20026;&#20102;&#30830;&#20445;&#25968;&#25454;&#30340;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#25163;&#21160;&#36827;&#34892;&#20102;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid growth of computer science has led to a proliferation of research presented at academic conferences, fostering global scholarly communication. Researchers consistently seek accurate, current information about these events at all stages. This data surge necessitates an intelligent question-answering system to efficiently address researchers' queries and ensure awareness of the latest advancements. The information of conferences is usually published on their official website, organized in a semi-structured way with a lot of text. To address this need, we have developed the ConferenceQA dataset for 7 diverse academic conferences with human annotations. Firstly, we employ a combination of manual and automated methods to organize academic conference data in a semi-structured JSON format. Subsequently, we annotate nearly 100 question-answer pairs for each conference. Each pair is classified into four different dimensions. To ensure the reliability of the data, we manually annotate 
&lt;/p&gt;</description></item><item><title>&#22312;&#23433;&#20840;&#20851;&#38190;&#34892;&#19994;&#20013;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#65288;HAII&#65289;&#21313;&#20998;&#37325;&#35201;&#65292;&#28982;&#32780;&#30446;&#21069;&#23545;&#20110;HAII&#30340;&#30740;&#31350;&#38646;&#25955;&#19988;&#32570;&#20047;&#19968;&#33268;&#24615;&#12290;&#26412;&#35770;&#25991;&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#21457;&#29616;&#27809;&#26377;&#19968;&#20010;&#19968;&#33268;&#20351;&#29992;&#30340;&#26415;&#35821;&#25551;&#36848;HAII&#65292;&#24182;&#19988;&#20116;&#20010;&#22240;&#32032;&#24433;&#21709;HAII&#65292;&#21363;&#29992;&#25143;&#29305;&#24449;&#21644;&#32972;&#26223;&#12289;AI&#30028;&#38754;&#21644;&#21151;&#33021;&#12289;&#29992;&#25143;&#20449;&#20219;&#12289;&#30417;&#25511;&#21644;&#21453;&#39304;&#65292;&#20197;&#21450;&#24037;&#20316;&#19978;&#30340;&#22242;&#38431;&#32467;&#26500;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#25913;&#36827;&#23433;&#20840;&#20851;&#38190;&#34892;&#19994;&#20013;&#30340;HAII&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2310.03392</link><description>&lt;p&gt;
&#22312;&#23433;&#20840;&#20851;&#38190;&#34892;&#19994;&#20013;&#35299;&#26500;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#65306;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Unpacking Human-AI Interaction in Safety-Critical Industries: A Systematic Literature Review. (arXiv:2310.03392v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03392
&lt;/p&gt;
&lt;p&gt;
&#22312;&#23433;&#20840;&#20851;&#38190;&#34892;&#19994;&#20013;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#65288;HAII&#65289;&#21313;&#20998;&#37325;&#35201;&#65292;&#28982;&#32780;&#30446;&#21069;&#23545;&#20110;HAII&#30340;&#30740;&#31350;&#38646;&#25955;&#19988;&#32570;&#20047;&#19968;&#33268;&#24615;&#12290;&#26412;&#35770;&#25991;&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#21457;&#29616;&#27809;&#26377;&#19968;&#20010;&#19968;&#33268;&#20351;&#29992;&#30340;&#26415;&#35821;&#25551;&#36848;HAII&#65292;&#24182;&#19988;&#20116;&#20010;&#22240;&#32032;&#24433;&#21709;HAII&#65292;&#21363;&#29992;&#25143;&#29305;&#24449;&#21644;&#32972;&#26223;&#12289;AI&#30028;&#38754;&#21644;&#21151;&#33021;&#12289;&#29992;&#25143;&#20449;&#20219;&#12289;&#30417;&#25511;&#21644;&#21453;&#39304;&#65292;&#20197;&#21450;&#24037;&#20316;&#19978;&#30340;&#22242;&#38431;&#32467;&#26500;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#25913;&#36827;&#23433;&#20840;&#20851;&#38190;&#34892;&#19994;&#20013;&#30340;HAII&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23433;&#20840;&#20851;&#38190;&#34892;&#19994;&#20013;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#65288;HAII&#65289;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#19981;&#33021;&#20570;&#21040;&#36825;&#19968;&#28857;&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#24615;&#21644;&#33268;&#21629;&#30340;&#21518;&#26524;&#12290;&#23613;&#31649;&#22914;&#27492;&#32039;&#36843;&#65292;&#20851;&#20110;HAII&#30340;&#30740;&#31350;&#24456;&#23569;&#19988;&#38646;&#25955;&#65292;&#19988;&#32570;&#20047;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20986;&#20102;&#23545;&#35813;&#39046;&#22495;&#30340;&#25991;&#29486;&#32508;&#36848;&#21644;&#25913;&#36827;&#30740;&#31350;&#26368;&#20339;&#23454;&#36341;&#30340;&#24314;&#35758;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#35843;&#26597;&#20998;&#20026;&#20197;&#19979;&#30740;&#31350;&#39046;&#22495;&#65306;&#65288;1&#65289;&#29992;&#20110;&#25551;&#36848;HAII&#30340;&#26415;&#35821;&#65292;&#65288;2&#65289;AI-enabled&#31995;&#32479;&#30340;&#20027;&#35201;&#35282;&#33394;&#65292;&#65288;3&#65289;&#24433;&#21709;HAII&#30340;&#22240;&#32032;&#65292;&#20197;&#21450;&#65288;4&#65289;&#22914;&#20309;&#34913;&#37327;HAII&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#22312;&#36825;&#20123;&#25991;&#31456;&#20013;&#35752;&#35770;&#30340;&#23433;&#20840;&#20851;&#38190;&#34892;&#19994;&#20013;&#20351;&#29992;&#30340;AI-enabled&#31995;&#32479;&#30340;&#33021;&#21147;&#21644;&#25104;&#29087;&#24230;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#25991;&#29486;&#20013;&#27809;&#26377;&#19968;&#20010;&#26415;&#35821;&#34987;&#19968;&#33268;&#20351;&#29992;&#26469;&#25551;&#36848;HAII&#65292;&#32780;&#19968;&#20123;&#26415;&#35821;&#20855;&#26377;&#22810;&#20010;&#21547;&#20041;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#25991;&#29486;&#65292;&#26377;&#20116;&#20010;&#22240;&#32032;&#24433;&#21709;HAII&#65306;&#29992;&#25143;&#29305;&#24449;&#21644;&#32972;&#26223;&#65288;&#20363;&#22914;&#65292;&#29992;&#25143;&#20010;&#24615;&#65292;&#24863;&#30693;&#65289;&#65292;AI&#30028;&#38754;&#21644;&#21151;&#33021;&#65288;&#20363;&#22914;&#65292;
&lt;/p&gt;
&lt;p&gt;
Ensuring quality human-AI interaction (HAII) in safety-critical industries is essential. Failure to do so can lead to catastrophic and deadly consequences. Despite this urgency, what little research there is on HAII is fragmented and inconsistent. We present here a survey of that literature and recommendations for research best practices that will improve the field. We divided our investigation into the following research areas: (1) terms used to describe HAII, (2) primary roles of AI-enabled systems, (3) factors that influence HAII, and (4) how HAII is measured. Additionally, we described the capabilities and maturity of the AI-enabled systems used in safety-critical industries discussed in these articles. We found that no single term is used across the literature to describe HAII and some terms have multiple meanings. According to our literature, five factors influence HAII: user characteristics and background (e.g., user personality, perceptions), AI interface and features (e.g., in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20005;&#26684;&#23454;&#39564;&#35780;&#20272;&#20102;&#26234;&#33021;&#21046;&#36896;&#31995;&#32479;&#20013;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#22635;&#34917;&#20102;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2310.02812</link><description>&lt;p&gt;
&#26234;&#33021;&#21046;&#36896;&#31995;&#32479;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;: &#23545;&#26368;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Time-Series Classification in Smart Manufacturing Systems: An Experimental Evaluation of State-of-the-Art Machine Learning Algorithms. (arXiv:2310.02812v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20005;&#26684;&#23454;&#39564;&#35780;&#20272;&#20102;&#26234;&#33021;&#21046;&#36896;&#31995;&#32479;&#20013;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#22635;&#34917;&#20102;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20256;&#24863;&#22120;&#25968;&#37327;&#30340;&#22686;&#21152;&#21644;&#24863;&#30693;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#21046;&#36896;&#19994;&#27491;&#22312;&#25910;&#38598;&#22823;&#37327;&#21508;&#31181;&#21508;&#26679;&#30340;&#25968;&#25454;&#12290;&#22312;&#26234;&#33021;&#21046;&#36896;&#31995;&#32479; (SMS) &#29615;&#22659;&#20013;&#65292;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36215;&#30528;&#20851;&#38190;&#30340;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867; (TSC) &#22312;&#35813;&#39046;&#22495;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#23545;&#21046;&#36896;&#19994;&#21644;&#24037;&#19994;&#29615;&#22659;&#20013; TSC &#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20005;&#26684;&#30340;&#23454;&#39564;&#35780;&#20272;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312; TSC &#21644;&#21046;&#36896;&#19994;&#25991;&#29486;&#20013;&#25506;&#32034;&#21644;&#32534;&#21046;&#20102;&#19968;&#20221;&#21253;&#21547;&#36229;&#36807;92&#20010;&#26368;&#20808;&#36827;&#31639;&#27861;&#30340;&#20840;&#38754;&#21015;&#34920;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20174;&#35813;&#21015;&#34920;&#20013;&#36873;&#25321;&#20102;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;36&#20010;&#31639;&#27861;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#31639;&#27861;&#22312;&#21508;&#31181;&#21046;&#36896;&#19994;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#31574;&#21010;&#20102;&#19968;&#32452;&#21253;&#21547;22&#20010;&#21046;&#36896;&#19994;&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20855;&#26377;&#19981;&#21516;&#30340;&#29305;&#24449;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#21046;&#36896;&#38382;&#39064;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#22312;&#21046;&#36896;&#19994;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#26045;&#24182;&#35780;&#20272;&#20102;&#36825;&#20123;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Manufacturing is gathering extensive amounts of diverse data, thanks to the growing number of sensors and rapid advances in sensing technologies. Among the various data types available in SMS settings, time-series data plays a pivotal role. Hence, TSC emerges is crucial in this domain. The objective of this study is to fill this gap by providing a rigorous experimental evaluation of the SoTA ML and DL algorithms for TSC tasks in manufacturing and industrial settings. We first explored and compiled a comprehensive list of more than 92 SoTA algorithms from both TSC and manufacturing literature. Following, we selected the 36 most representative algorithms from this list. To evaluate their performance across various manufacturing classification tasks, we curated a set of 22 manufacturing datasets, representative of different characteristics that cover diverse manufacturing problems. Subsequently, we implemented and evaluated the algorithms on the manufacturing benchmark datasets, and analy
&lt;/p&gt;</description></item><item><title>LLM&#20284;&#20046;&#20855;&#26377;&#20016;&#23500;&#30340;&#30693;&#35782;&#21644;&#36866;&#24212;&#22810;&#31181;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#20294;&#25105;&#20204;&#19981;&#33021;&#23436;&#20840;&#20449;&#20219;&#23427;&#20204;&#30340;&#22238;&#31572;&#65292;&#22240;&#20026;&#23427;&#20204;&#20250;&#20986;&#29616;&#24187;&#35273;&#65292;&#21363;&#25423;&#36896;&#19981;&#23384;&#22312;&#30340;&#20107;&#23454;&#20197;&#27450;&#39575;&#29992;&#25143;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#30001;&#38543;&#26426;&#26631;&#35760;&#32452;&#25104;&#30340;&#26080;&#24847;&#20041;&#25552;&#31034;&#20063;&#33021;&#24341;&#36215;LLM&#20135;&#29983;&#24187;&#35273;&#22238;&#24212;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#26041;&#24335;&#30340;&#33258;&#21160;&#24187;&#35273;&#35302;&#21457;&#26041;&#27861;&#20316;&#20026;&#24187;&#35273;&#25915;&#20987;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.01469</link><description>&lt;p&gt;
LLM&#35854;&#35328;: &#24187;&#35273;&#19981;&#26159;&#28431;&#27934;&#65292;&#32780;&#26159;&#23545;&#25239;&#26679;&#26412;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples. (arXiv:2310.01469v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01469
&lt;/p&gt;
&lt;p&gt;
LLM&#20284;&#20046;&#20855;&#26377;&#20016;&#23500;&#30340;&#30693;&#35782;&#21644;&#36866;&#24212;&#22810;&#31181;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#20294;&#25105;&#20204;&#19981;&#33021;&#23436;&#20840;&#20449;&#20219;&#23427;&#20204;&#30340;&#22238;&#31572;&#65292;&#22240;&#20026;&#23427;&#20204;&#20250;&#20986;&#29616;&#24187;&#35273;&#65292;&#21363;&#25423;&#36896;&#19981;&#23384;&#22312;&#30340;&#20107;&#23454;&#20197;&#27450;&#39575;&#29992;&#25143;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#30001;&#38543;&#26426;&#26631;&#35760;&#32452;&#25104;&#30340;&#26080;&#24847;&#20041;&#25552;&#31034;&#20063;&#33021;&#24341;&#36215;LLM&#20135;&#29983;&#24187;&#35273;&#22238;&#24212;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#26041;&#24335;&#30340;&#33258;&#21160;&#24187;&#35273;&#35302;&#21457;&#26041;&#27861;&#20316;&#20026;&#24187;&#35273;&#25915;&#20987;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#21253;&#25324;GPT-3.5&#12289;LLaMA&#21644;PaLM&#65292;&#20284;&#20046;&#20855;&#26377;&#20016;&#23500;&#30340;&#30693;&#35782;&#21644;&#36866;&#24212;&#22810;&#31181;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20173;&#28982;&#19981;&#33021;&#23436;&#20840;&#20449;&#20219;&#23427;&#20204;&#30340;&#22238;&#31572;&#65292;&#22240;&#20026;LLM&#20250;&#20986;&#29616;&#24187;&#35273;&#65292;&#21363;&#25423;&#36896;&#19981;&#23384;&#22312;&#30340;&#20107;&#23454;&#20197;&#27450;&#39575;&#29992;&#25143;&#32780;&#19981;&#34987;&#23519;&#35273;&#12290;&#24187;&#35273;&#23384;&#22312;&#30340;&#21407;&#22240;&#21644;&#26222;&#36941;&#24615;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30001;&#38543;&#26426;&#26631;&#35760;&#32452;&#25104;&#30340;&#26080;&#24847;&#20041;&#25552;&#31034;&#20063;&#33021;&#24341;&#36215;LLM&#20135;&#29983;&#24187;&#35273;&#22238;&#24212;&#12290;&#36825;&#20010;&#29616;&#35937;&#36843;&#20351;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#24187;&#35273;&#21487;&#33021;&#26159;&#23545;&#25239;&#26679;&#26412;&#30340;&#21478;&#19968;&#31181;&#35270;&#35282;&#65292;&#24182;&#19988;&#23427;&#19982;&#24120;&#35268;&#30340;&#23545;&#25239;&#26679;&#26412;&#20855;&#26377;&#31867;&#20284;&#30340;&#29305;&#24449;&#65292;&#20316;&#20026;LLM&#30340;&#22522;&#26412;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20197;&#23545;&#25239;&#30340;&#26041;&#24335;&#23558;&#33258;&#21160;&#24187;&#35273;&#35302;&#21457;&#26041;&#27861;&#24418;&#24335;&#21270;&#20026;&#24187;&#35273;&#25915;&#20987;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#34987;&#25915;&#20987;&#30340;&#23545;&#25239;&#25552;&#31034;&#30340;&#22522;&#26412;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#24050;&#22312;GitHub&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), including GPT-3.5, LLaMA, and PaLM, seem to be knowledgeable and able to adapt to many tasks. However, we still can not completely trust their answer, since LLMs suffer from hallucination--fabricating non-existent facts to cheat users without perception. And the reasons for their existence and pervasiveness remain unclear. In this paper, we demonstrate that non-sense prompts composed of random tokens can also elicit the LLMs to respond with hallucinations. This phenomenon forces us to revisit that hallucination may be another view of adversarial examples, and it shares similar features with conventional adversarial examples as the basic feature of LLMs. Therefore, we formalize an automatic hallucination triggering method as the hallucination attack in an adversarial way. Finally, we explore basic feature of attacked adversarial prompts and propose a simple yet effective defense strategy. Our code is released on GitHub.
&lt;/p&gt;</description></item><item><title>TranDRL&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#39537;&#21160;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25903;&#25345;&#30340;&#39044;&#38450;&#24615;&#32500;&#25252;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22797;&#26434;&#26102;&#38388;&#27169;&#24335;&#25429;&#25417;&#21644;&#32463;&#27982;&#39640;&#25928;&#32500;&#25252;&#24314;&#35758;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21097;&#20313;&#23551;&#21629;&#65288;RUL&#65289;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#32500;&#25252;&#34892;&#21160;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.16935</link><description>&lt;p&gt;
TranDRL&#65306;&#19968;&#31181;&#22522;&#20110;Transformer&#39537;&#21160;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25903;&#25345;&#30340;&#39044;&#38450;&#24615;&#32500;&#25252;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TranDRL: A Transformer-Driven Deep Reinforcement Learning Enabled Prescriptive Maintenance Framework. (arXiv:2309.16935v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16935
&lt;/p&gt;
&lt;p&gt;
TranDRL&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#39537;&#21160;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25903;&#25345;&#30340;&#39044;&#38450;&#24615;&#32500;&#25252;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22797;&#26434;&#26102;&#38388;&#27169;&#24335;&#25429;&#25417;&#21644;&#32463;&#27982;&#39640;&#25928;&#32500;&#25252;&#24314;&#35758;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21097;&#20313;&#23551;&#21629;&#65288;RUL&#65289;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#32500;&#25252;&#34892;&#21160;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;&#31995;&#32479;&#38656;&#35201;&#21487;&#38752;&#30340;&#39044;&#27979;&#24615;&#32500;&#25252;&#31574;&#30053;&#26469;&#25552;&#39640;&#36816;&#33829;&#25928;&#29575;&#24182;&#20943;&#23569;&#20572;&#26426;&#26102;&#38388;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32508;&#21512;&#26694;&#26550;&#65292;&#21033;&#29992;Transformer&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#31639;&#27861;&#26469;&#20248;&#21270;&#32500;&#25252;&#34892;&#21160;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;Transformer&#27169;&#22411;&#26469;&#26377;&#25928;&#25429;&#25417;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#26102;&#38388;&#27169;&#24335;&#65292;&#20174;&#32780;&#20934;&#30830;&#39044;&#27979;&#35774;&#22791;&#30340;&#21097;&#20313;&#23551;&#21629;&#65288;RUL&#65289;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#26694;&#26550;&#20013;&#30340;DRL&#32452;&#20214;&#25552;&#20379;&#20102;&#32463;&#27982;&#39640;&#25928;&#21644;&#21450;&#26102;&#30340;&#32500;&#25252;&#24314;&#35758;&#12290;&#25105;&#20204;&#22312;NASA C-MPASS&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;RUL&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#32500;&#25252;&#34892;&#21160;&#20248;&#21270;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#21019;&#26032;&#26041;&#27861;&#20026;&#39044;&#38450;&#24615;&#32500;&#25252;&#25552;&#20379;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#24037;&#19994;&#36816;&#33829;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#24182;&#24102;&#26469;&#20102;&#26356;&#22810;&#21457;&#23637;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
Industrial systems demand reliable predictive maintenance strategies to enhance operational efficiency and reduce downtime. This paper introduces a novel, integrated framework that leverages the power of transformer neural networks and deep reinforcement learning (DRL) algorithms to optimize maintenance actions. Our approach employs the transformer model to effectively capture complex temporal patterns in sensor data, thereby accurately predicting the Remaining Useful Life (RUL) of equipment. Simultaneously, the DRL component of our framework provides cost-effective and timely maintenance recommendations. We validate the efficacy of our framework on the NASA C-MPASS dataset, where it demonstrates significant advancements in both RUL prediction accuracy and the optimization of maintenance actions. Consequently, our pioneering approach provides an innovative data-driven methodology for prescriptive maintenance, addressing key challenges in industrial operations and leading the way to mor
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#21333;&#23376;&#21477;&#20256;&#25773;&#32780;&#35328;&#65292;&#22312;CNF&#20844;&#24335;&#20013;&#19981;&#21487;&#31616;&#21270;&#30340;&#20844;&#24335;&#65292;&#20854;&#22823;&#23567;&#19982;&#26368;&#23567;&#21487;&#31561;&#20215;&#30340;&#20844;&#24335;&#22823;&#23567;&#30340;&#27604;&#20540;&#26368;&#22823;&#20026;n^2&#65292;&#20854;&#20013;n&#26159;&#21464;&#37327;&#25968;&#37327;&#12290;&#19968;&#33324;&#19978;&#30028;&#19981;&#20250;&#23567;&#20110;n/ln n&#20493;&#12290;</title><link>http://arxiv.org/abs/2309.01750</link><description>&lt;p&gt;
&#20851;&#20110;&#30456;&#23545;&#20110;&#21333;&#23376;&#21477;&#20256;&#25773;&#19981;&#21487;&#31616;&#21270;&#30340;CNF&#20844;&#24335;
&lt;/p&gt;
&lt;p&gt;
On CNF formulas irredundant with respect to unit clause propagation. (arXiv:2309.01750v2 [math.CO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01750
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#21333;&#23376;&#21477;&#20256;&#25773;&#32780;&#35328;&#65292;&#22312;CNF&#20844;&#24335;&#20013;&#19981;&#21487;&#31616;&#21270;&#30340;&#20844;&#24335;&#65292;&#20854;&#22823;&#23567;&#19982;&#26368;&#23567;&#21487;&#31561;&#20215;&#30340;&#20844;&#24335;&#22823;&#23567;&#30340;&#27604;&#20540;&#26368;&#22823;&#20026;n^2&#65292;&#20854;&#20013;n&#26159;&#21464;&#37327;&#25968;&#37327;&#12290;&#19968;&#33324;&#19978;&#30028;&#19981;&#20250;&#23567;&#20110;n/ln n&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#20004;&#20010;CNF&#20844;&#24335;&#22312;&#21333;&#23376;&#21477;&#20256;&#25773;&#65288;UCP&#65289;&#26041;&#38754;&#30340;&#34892;&#20026;&#30456;&#21516;&#65292;&#21017;&#23427;&#20204;&#34987;&#31216;&#20026;ucp&#31561;&#20215;&#12290;&#22914;&#26524;&#31227;&#38500;&#20219;&#24847;&#19968;&#20010;&#23376;&#21477;&#20250;&#23548;&#33268;&#19968;&#20010;&#19982;&#21407;&#22987;&#20844;&#24335;&#22312;ucp&#26041;&#38754;&#19981;&#31561;&#20215;&#30340;&#20844;&#24335;&#65292;&#21017;&#31216;&#35813;&#20844;&#24335;&#20026;ucp&#19981;&#21487;&#31616;&#21270;&#12290;&#26681;&#25454;&#24050;&#30693;&#32467;&#26524;&#65292;ucp&#19981;&#21487;&#31616;&#21270;&#20844;&#24335;&#30340;&#22823;&#23567;&#19982;&#26368;&#23567;ucp&#31561;&#20215;&#20844;&#24335;&#30340;&#22823;&#23567;&#30340;&#27604;&#20540;&#26368;&#22823;&#20026;n^2&#65292;&#20854;&#20013;n&#26159;&#21464;&#37327;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#31216;&#30830;&#23450;Horn&#20989;&#25968;&#30340;&#19968;&#20010;ucp&#19981;&#21487;&#31616;&#21270;&#20844;&#24335;&#30340;&#20363;&#23376;&#65292;&#20854;&#22823;&#23567;&#27604;&#26368;&#23567;&#30340;ucp&#31561;&#20215;&#20844;&#24335;&#22823;n/ln n&#20493;&#65292;&#22240;&#27492;&#65292;&#19978;&#36848;&#27604;&#20540;&#30340;&#19968;&#33324;&#19978;&#30028;&#19981;&#33021;&#23567;&#20110;&#36825;&#20010;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Two CNF formulas are called ucp-equivalent, if they behave in the same way with respect to the unit clause propagation (UCP). A formula is called ucp-irredundant, if removing any clause leads to a formula which is not ucp-equivalent to the original one. As a consequence of known results, the ratio of the size of a ucp-irredundant formula and the size of a smallest ucp-equivalent formula is at most $n^2$, where $n$ is the number of the variables. We demonstrate an example of a ucp-irredundant formula for a symmetric definite Horn function which is larger than a smallest ucp-equivalent formula by a factor $\Omega(n/\ln n)$ and, hence, a general upper bound on the above ratio cannot be smaller than this.
&lt;/p&gt;</description></item><item><title>LAMBO&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36793;&#32536;&#26234;&#33021;&#26694;&#26550;&#65292;&#29992;&#20110;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#12290;&#23427;&#35299;&#20915;&#20102;&#20256;&#32479;&#28145;&#24230;&#21368;&#36733;&#26550;&#26500;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#39640;&#24615;&#33021;&#30340;&#20915;&#31574;&#27169;&#22359;&#21644;&#24378;&#21270;&#23398;&#20064;&#27169;&#22359;&#12290;</title><link>http://arxiv.org/abs/2308.15078</link><description>&lt;p&gt;
LAMBO: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30340;&#36793;&#32536;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
LAMBO: Large Language Model Empowered Edge Intelligence. (arXiv:2308.15078v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15078
&lt;/p&gt;
&lt;p&gt;
LAMBO&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36793;&#32536;&#26234;&#33021;&#26694;&#26550;&#65292;&#29992;&#20110;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#12290;&#23427;&#35299;&#20915;&#20102;&#20256;&#32479;&#28145;&#24230;&#21368;&#36733;&#26550;&#26500;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#39640;&#24615;&#33021;&#30340;&#20915;&#31574;&#27169;&#22359;&#21644;&#24378;&#21270;&#23398;&#20064;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35745;&#19979;&#19968;&#20195;&#36793;&#32536;&#26234;&#33021;&#23558;&#20026;&#21508;&#31181;&#24212;&#29992;&#24102;&#26469;&#24040;&#22823;&#30340;&#22909;&#22788;&#65292;&#20363;&#22914;&#21368;&#36733;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#28145;&#24230;&#21368;&#36733;&#26550;&#26500;&#38754;&#20020;&#22810;&#20010;&#38382;&#39064;&#65292;&#21253;&#25324;&#24322;&#26500;&#38480;&#21046;&#12289;&#37096;&#20998;&#24863;&#30693;&#12289;&#19981;&#30830;&#23450;&#30340;&#27867;&#21270;&#21644;&#32570;&#20047;&#21487;&#36861;&#28335;&#24615;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#23558;&#21368;&#36733;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38598;&#25104;&#22312;&#19968;&#36215;&#20855;&#26377;&#35768;&#22810;&#20248;&#21183;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#21368;&#36733;&#65288;LAMBO&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#65288;MEC&#65289;&#65292;&#23427;&#30001;&#22235;&#20010;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;&#65288;i&#65289;&#36755;&#20837;&#23884;&#20837;&#65288;IE&#65289;&#65292;&#29992;&#20110;&#29992;&#39640;&#36136;&#37327;&#30340;&#21487;&#23398;&#20064;&#21521;&#37327;&#34920;&#31034;&#20855;&#26377;&#32422;&#26463;&#21644;&#25552;&#31034;&#30340;&#21368;&#36733;&#31995;&#32479;&#30340;&#20449;&#24687;&#65307;&#65288;ii&#65289;&#38750;&#23545;&#31216;&#32534;&#30721;&#35299;&#30721;&#65288;AED&#65289;&#27169;&#22411;&#65292;&#26159;&#19968;&#20010;&#20915;&#31574;&#27169;&#22359;&#65292;&#20855;&#26377;&#28145;&#24230;&#32534;&#30721;&#22120;&#21644;&#27973;&#23618;&#35299;&#30721;&#22120;&#12290;&#23427;&#21487;&#20197;&#22522;&#20110;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#39640;&#24615;&#33021;&#65307;&#65288;iii&#65289;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24378;&#21270;&#23398;&#20064;&#65288;ACRL&#65289;&#27169;&#22359;&#65292;&#29992;&#20110;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Next-generation edge intelligence is anticipated to bring huge benefits to various applications, e.g., offloading systems. However, traditional deep offloading architectures face several issues, including heterogeneous constraints, partial perception, uncertain generalization, and lack of tractability. In this context, the integration of offloading with large language models (LLMs) presents numerous advantages. Therefore, we propose an LLM-Based Offloading (LAMBO) framework for mobile edge computing (MEC), which comprises four components: (i) Input embedding (IE), which is used to represent the information of the offloading system with constraints and prompts through learnable vectors with high quality; (ii) Asymmetric encoderdecoder (AED) model, which is a decision-making module with a deep encoder and a shallow decoder. It can achieve high performance based on multi-head self-attention schemes; (iii) Actor-critic reinforcement learning (ACRL) module, which is employed to pre-train th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#40784;&#38899;&#39057;&#29305;&#24449;&#21644;&#25991;&#26412;&#29305;&#24449;&#26469;&#23454;&#29616;&#22768;&#38899;&#20107;&#20214;&#20998;&#31867;&#21644;&#26102;&#38388;&#23450;&#20301;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#33021;&#21147;&#30452;&#25509;&#29983;&#25104;&#24207;&#21015;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#31616;&#27905;&#20840;&#38754;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#26102;&#38388;&#25139;&#25429;&#33719;&#21644;&#20107;&#20214;&#20998;&#31867;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.11530</link><description>&lt;p&gt;
&#22686;&#24378;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Furnishing Sound Event Detection with Language Model Abilities. (arXiv:2308.11530v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#40784;&#38899;&#39057;&#29305;&#24449;&#21644;&#25991;&#26412;&#29305;&#24449;&#26469;&#23454;&#29616;&#22768;&#38899;&#20107;&#20214;&#20998;&#31867;&#21644;&#26102;&#38388;&#23450;&#20301;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#33021;&#21147;&#30452;&#25509;&#29983;&#25104;&#24207;&#21015;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#31616;&#27905;&#20840;&#38754;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#26102;&#38388;&#25139;&#25429;&#33719;&#21644;&#20107;&#20214;&#20998;&#31867;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#35270;&#35273;&#36328;&#27169;&#24577;&#20013;&#30340;&#33021;&#21147;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;LMs&#22312;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#65288;SED&#65289;&#20013;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#36229;&#36234;&#20102;&#35270;&#35273;&#39046;&#22495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#38597;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#40784;&#38899;&#39057;&#29305;&#24449;&#21644;&#25991;&#26412;&#29305;&#24449;&#26469;&#23436;&#25104;&#22768;&#38899;&#20107;&#20214;&#20998;&#31867;&#21644;&#26102;&#38388;&#23450;&#20301;&#12290;&#35813;&#26694;&#26550;&#30001;&#19968;&#20010;&#22768;&#23398;&#32534;&#30721;&#22120;&#12289;&#19968;&#20010;&#23545;&#24212;&#30340;&#25991;&#26412;&#21644;&#38899;&#39057;&#34920;&#31034;&#23545;&#40784;&#30340;&#23545;&#27604;&#27169;&#22359;&#65292;&#20197;&#21450;&#19968;&#20010;&#35299;&#32806;&#30340;&#35821;&#35328;&#35299;&#30721;&#22120;&#32452;&#25104;&#65292;&#29992;&#20110;&#20174;&#38899;&#39057;&#29305;&#24449;&#20013;&#29983;&#25104;&#26102;&#38388;&#21644;&#20107;&#20214;&#24207;&#21015;&#12290;&#19982;&#38656;&#35201;&#22797;&#26434;&#22788;&#29702;&#24182;&#20960;&#20046;&#19981;&#20351;&#29992;&#26377;&#38480;&#38899;&#39057;&#29305;&#24449;&#30340;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#26356;&#31616;&#27905;&#20840;&#38754;&#65292;&#22240;&#20026;&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#21033;&#29992;&#20854;&#35821;&#20041;&#33021;&#21147;&#29983;&#25104;&#24207;&#21015;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#35299;&#32806;&#27169;&#22359;&#65292;&#20197;&#23637;&#31034;&#20854;&#23545;&#26102;&#38388;&#25139;&#25429;&#25417;&#21644;&#20107;&#20214;&#20998;&#31867;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the ability of language models (LMs) has attracted increasing attention in visual cross-modality. In this paper, we further explore the generation capacity of LMs for sound event detection (SED), beyond the visual domain. Specifically, we propose an elegant method that aligns audio features and text features to accomplish sound event classification and temporal location. The framework consists of an acoustic encoder, a contrastive module that align the corresponding representations of the text and audio, and a decoupled language decoder that generates temporal and event sequences from the audio characteristic. Compared with conventional works that require complicated processing and barely utilize limited audio features, our model is more concise and comprehensive since language model directly leverage its semantic capabilities to generate the sequences. We investigate different decoupling modules to demonstrate the effectiveness for timestamps capture and event classification
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#27169;&#24335;&#22797;&#26434;&#30340;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#30340;&#24402;&#32435;&#20803;&#36335;&#24452;&#23398;&#20064;&#26694;&#26550;SchemaWalk&#12290;</title><link>http://arxiv.org/abs/2307.03937</link><description>&lt;p&gt;
&#38024;&#23545;&#27169;&#24335;&#22797;&#26434;&#30340;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#30340;&#24402;&#32435;&#20803;&#36335;&#24452;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Inductive Meta-path Learning for Schema-complex Heterogeneous Information Networks. (arXiv:2307.03937v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03937
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#27169;&#24335;&#22797;&#26434;&#30340;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#30340;&#24402;&#32435;&#20803;&#36335;&#24452;&#23398;&#20064;&#26694;&#26550;SchemaWalk&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;(HINs)&#26159;&#20855;&#26377;&#22810;&#31181;&#33410;&#28857;&#31867;&#22411;&#21644;&#36793;&#31867;&#22411;&#30340;&#20449;&#24687;&#32593;&#32476;&#12290;&#20803;&#36335;&#24452;&#30340;&#27010;&#24565;&#21363;&#19968;&#31995;&#21015;&#36830;&#25509;&#20004;&#20010;&#23454;&#20307;&#30340;&#23454;&#20307;&#31867;&#22411;&#21644;&#20851;&#31995;&#31867;&#22411;&#30340;&#24207;&#21015;&#34987;&#25552;&#20986;&#20026;&#25552;&#20379;&#23545;&#19981;&#21516;HIN&#20219;&#21153;&#30340;&#20803;&#32423;&#21487;&#35299;&#37322;&#35821;&#20041;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#20256;&#32479;&#19978;&#65292;&#20803;&#36335;&#24452;&#20027;&#35201;&#29992;&#20110;&#27169;&#24335;&#31616;&#21333;&#30340;HINs&#65292;&#20363;&#22914;&#21482;&#26377;&#23569;&#37327;&#23454;&#20307;&#31867;&#22411;&#30340;&#25991;&#29486;&#32593;&#32476;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20803;&#36335;&#24452;&#36890;&#24120;&#36890;&#36807;&#39046;&#22495;&#30693;&#35782;&#26522;&#20030;&#12290;&#28982;&#32780;&#65292;&#20803;&#36335;&#24452;&#22312;&#27169;&#24335;&#22797;&#26434;&#30340;HINs(&#20363;&#22914;&#20855;&#26377;&#25968;&#30334;&#31181;&#23454;&#20307;&#21644;&#20851;&#31995;&#31867;&#22411;&#30340;&#30693;&#35782;&#24211;)&#20013;&#30340;&#24212;&#29992;&#21463;&#21040;&#20102;&#30001;&#20803;&#36335;&#24452;&#26522;&#20030;&#24341;&#36215;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#26377;&#25928;&#35780;&#20272;&#20803;&#36335;&#24452;&#38656;&#35201;&#26522;&#20030;&#30456;&#20851;&#36335;&#24452;&#23454;&#20363;&#65292;&#36825;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#20803;&#36335;&#24452;&#23398;&#20064;&#36807;&#31243;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27169;&#24335;&#22797;&#26434;&#30340;HINs&#30340;&#24402;&#32435;&#20803;&#36335;&#24452;&#23398;&#20064;&#26694;&#26550;SchemaWalk&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Information Networks (HINs) are information networks with multiple types of nodes and edges. The concept of meta-path, i.e., a sequence of entity types and relation types connecting two entities, is proposed to provide the meta-level explainable semantics for various HIN tasks. Traditionally, meta-paths are primarily used for schema-simple HINs, e.g., bibliographic networks with only a few entity types, where meta-paths are often enumerated with domain knowledge. However, the adoption of meta-paths for schema-complex HINs, such as knowledge bases (KBs) with hundreds of entity and relation types, has been limited due to the computational complexity associated with meta-path enumeration. Additionally, effectively assessing meta-paths requires enumerating relevant path instances, which adds further complexity to the meta-path learning process. To address these challenges, we propose SchemaWalk, an inductive meta-path learning framework for schema-complex HINs. We represent m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;AI&#27169;&#22411;&#30340;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#65288;LAM-SC&#65289;&#65292;&#21033;&#29992;&#35813;&#26694;&#26550;&#21487;&#20197;&#20811;&#26381;&#30693;&#35782;&#24211;&#26500;&#24314;&#36807;&#31243;&#20013;&#38754;&#20020;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22270;&#20687;&#25968;&#25454;&#39046;&#22495;&#23454;&#29616;&#20102;&#35821;&#20041;&#20998;&#21106;&#12289;&#35821;&#20041;&#38598;&#25104;&#21644;&#33258;&#36866;&#24212;&#35821;&#20041;&#21387;&#32553;&#12290;</title><link>http://arxiv.org/abs/2307.03492</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;AI&#27169;&#22411;&#30340;&#35821;&#20041;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Large AI Model-Based Semantic Communications. (arXiv:2307.03492v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;AI&#27169;&#22411;&#30340;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#65288;LAM-SC&#65289;&#65292;&#21033;&#29992;&#35813;&#26694;&#26550;&#21487;&#20197;&#20811;&#26381;&#30693;&#35782;&#24211;&#26500;&#24314;&#36807;&#31243;&#20013;&#38754;&#20020;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22270;&#20687;&#25968;&#25454;&#39046;&#22495;&#23454;&#29616;&#20102;&#35821;&#20041;&#20998;&#21106;&#12289;&#35821;&#20041;&#38598;&#25104;&#21644;&#33258;&#36866;&#24212;&#35821;&#20041;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#36890;&#20449;&#65288;SC&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#26234;&#33021;&#33539;&#24335;&#65292;&#20026;&#20803;&#23431;&#23449;&#12289;&#28151;&#21512;&#29616;&#23454;&#21644;&#19975;&#29289;&#20114;&#32852;&#31561;&#26410;&#26469;&#24212;&#29992;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22312;&#30446;&#21069;&#30340;SC&#31995;&#32479;&#20013;&#65292;&#30693;&#35782;&#24211;&#65288;KB&#65289;&#30340;&#26500;&#24314;&#38754;&#20020;&#30528;&#19968;&#20123;&#38382;&#39064;&#65292;&#21253;&#25324;&#30693;&#35782;&#34920;&#31034;&#26377;&#38480;&#12289;&#39057;&#32321;&#30340;&#30693;&#35782;&#26356;&#26032;&#21644;&#19981;&#23433;&#20840;&#30340;&#30693;&#35782;&#20849;&#20139;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#22823;&#22411;AI&#27169;&#22411;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#30340;&#26032;&#26041;&#26696;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;AI&#27169;&#22411;&#30340;SC&#26694;&#26550;&#65288;LAM-SC&#65289;&#65292;&#19987;&#38376;&#29992;&#20110;&#22270;&#20687;&#25968;&#25454;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#22522;&#20110;&#27573;&#33853;&#27169;&#22411;&#65288;SAM&#65289;&#30340;&#30693;&#35782;&#24211;&#65288;SKB&#65289;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#36890;&#29992;&#35821;&#20041;&#30693;&#35782;&#23558;&#21407;&#22987;&#22270;&#20687;&#21010;&#20998;&#20026;&#19981;&#21516;&#30340;&#35821;&#20041;&#27573;&#33853;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35821;&#20041;&#38598;&#25104;&#65288;ASI&#65289;&#65292;&#36890;&#36807;&#26435;&#34913;&#30001;SKB&#29983;&#25104;&#30340;&#35821;&#20041;&#27573;&#33853;&#65292;&#26080;&#38656;&#20154;&#24037;&#21442;&#19982;&#24182;&#23558;&#23427;&#20204;&#38598;&#25104;&#20026;&#20855;&#26377;&#35821;&#20041;&#24863;&#30693;&#30340;&#22270;&#20687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#35821;&#20041;&#21387;&#32553;&#65288;ASC&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic communication (SC) is an emerging intelligent paradigm, offering solutions for various future applications like metaverse, mixed-reality, and the Internet of everything. However, in current SC systems, the construction of the knowledge base (KB) faces several issues, including limited knowledge representation, frequent knowledge updates, and insecure knowledge sharing. Fortunately, the development of the large AI model provides new solutions to overcome above issues. Here, we propose a large AI model-based SC framework (LAM-SC) specifically designed for image data, where we first design the segment anything model (SAM)-based KB (SKB) that can split the original image into different semantic segments by universal semantic knowledge. Then, we present an attention-based semantic integration (ASI) to weigh the semantic segments generated by SKB without human participation and integrate them as the semantic-aware image. Additionally, we propose an adaptive semantic compression (ASC
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;(HKGs)&#30340;&#26500;&#24314;&#27969;&#31243;&#12289;&#20851;&#38190;&#25216;&#26415;&#21644;&#21033;&#29992;&#26041;&#27861;&#20197;&#21450;&#29616;&#26377;&#36164;&#28304;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;HKG&#22312;&#21508;&#31181;&#21307;&#30103;&#39046;&#22495;&#30340;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.04802</link><description>&lt;p&gt;
&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;&#32508;&#36848;&#65306;&#36164;&#28304;&#12289;&#24212;&#29992;&#21644;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
A Survey on Knowledge Graphs for Healthcare: Resources, Applications, and Promises. (arXiv:2306.04802v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;(HKGs)&#30340;&#26500;&#24314;&#27969;&#31243;&#12289;&#20851;&#38190;&#25216;&#26415;&#21644;&#21033;&#29992;&#26041;&#27861;&#20197;&#21450;&#29616;&#26377;&#36164;&#28304;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;HKG&#22312;&#21508;&#31181;&#21307;&#30103;&#39046;&#22495;&#30340;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;(HKGs)&#24050;&#25104;&#20026;&#32452;&#32455;&#21307;&#23398;&#30693;&#35782;&#30340;&#26377;&#32467;&#26500;&#19988;&#21487;&#35299;&#37322;&#30340;&#26377;&#20026;&#24037;&#20855;&#65292;&#25552;&#20379;&#20102;&#21307;&#23398;&#27010;&#24565;&#21450;&#20854;&#20851;&#31995;&#30340;&#20840;&#38754;&#35270;&#22270;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#35206;&#30422;&#33539;&#22260;&#26377;&#38480;&#31561;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#65292;&#24378;&#35843;&#20102;&#22312;HKG&#39046;&#22495;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;&#26412;&#32508;&#36848;&#26159;HKG&#30340;&#31532;&#19968;&#20221;&#32508;&#21512;&#27010;&#36848;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;HKG&#26500;&#24314;&#30340;&#27969;&#31243;&#21644;&#20851;&#38190;&#25216;&#26415;&#65288;&#21363;&#20174;&#22836;&#24320;&#22987;&#21644;&#36890;&#36807;&#38598;&#25104;&#65289;&#65292;&#20197;&#21450;&#24120;&#35265;&#30340;&#21033;&#29992;&#26041;&#27861;&#65288;&#21363;&#22522;&#20110;&#27169;&#22411;&#21644;&#38750;&#22522;&#20110;&#27169;&#22411;&#65289;&#12290;&#20026;&#20102;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#65292;&#25105;&#20204;&#26681;&#25454;&#23427;&#20204;&#25429;&#33719;&#30340;&#25968;&#25454;&#31867;&#22411;&#21644;&#24212;&#29992;&#39046;&#22495;&#65288;&#35813;&#36164;&#28304;&#23384;&#20648;&#20110;https://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase&#65289;&#32452;&#32455;&#20102;&#29616;&#26377;&#30340;HKG&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#32479;&#35745;&#20449;&#24687;&#12290;&#22312;&#24212;&#29992;&#37096;&#20998;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;HKG&#22312;&#21508;&#31181;&#21307;&#30103;&#39046;&#22495;&#30340;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Healthcare knowledge graphs (HKGs) have emerged as a promising tool for organizing medical knowledge in a structured and interpretable way, which provides a comprehensive view of medical concepts and their relationships. However, challenges such as data heterogeneity and limited coverage remain, emphasizing the need for further research in the field of HKGs. This survey paper serves as the first comprehensive overview of HKGs. We summarize the pipeline and key techniques for HKG construction (i.e., from scratch and through integration), as well as the common utilization approaches (i.e., model-free and model-based). To provide researchers with valuable resources, we organize existing HKGs (The resource is available at https://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase) based on the data types they capture and application domains, supplemented with pertinent statistical information. In the application section, we delve into the transformative impact of HKGs across various hea
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#65292;&#36890;&#36807;&#22312;&#31243;&#24207;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#21363;&#20351;&#27809;&#26377;&#38024;&#23545;&#23398;&#20064;&#35821;&#35328;&#35821;&#20041;&#25552;&#20379;&#24402;&#32435;&#20559;&#24046;&#65292;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#33021;&#22815;&#23398;&#20064;&#21547;&#20041;&#12290;&#32447;&#24615;&#25506;&#27979;&#22120;&#33021;&#22815;&#20174;&#27169;&#22411;&#29366;&#24577;&#20013;&#25552;&#21462;&#31243;&#24207;&#29366;&#24577;&#30340;&#25277;&#35937;&#65292;&#20934;&#30830;&#24615;&#19982;&#27169;&#22411;&#27867;&#21270;&#21040;&#26032;&#31243;&#24207;&#30340;&#33021;&#21147;&#26174;&#33879;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2305.11169</link><description>&lt;p&gt;
&#22312;&#32534;&#31243;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#35821;&#20041;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Evidence of Meaning in Language Models Trained on Programs. (arXiv:2305.11169v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11169
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#65292;&#36890;&#36807;&#22312;&#31243;&#24207;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#21363;&#20351;&#27809;&#26377;&#38024;&#23545;&#23398;&#20064;&#35821;&#35328;&#35821;&#20041;&#25552;&#20379;&#24402;&#32435;&#20559;&#24046;&#65292;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#33021;&#22815;&#23398;&#20064;&#21547;&#20041;&#12290;&#32447;&#24615;&#25506;&#27979;&#22120;&#33021;&#22815;&#20174;&#27169;&#22411;&#29366;&#24577;&#20013;&#25552;&#21462;&#31243;&#24207;&#29366;&#24577;&#30340;&#25277;&#35937;&#65292;&#20934;&#30830;&#24615;&#19982;&#27169;&#22411;&#27867;&#21270;&#21040;&#26032;&#31243;&#24207;&#30340;&#33021;&#21147;&#26174;&#33879;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;&#23613;&#31649;&#34987;&#35757;&#32451;&#21482;&#26159;&#25191;&#34892;&#25991;&#26412;&#19978;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#65292;&#29305;&#21035;&#26159;&#19968;&#20010;&#31243;&#24207;&#35821;&#26009;&#24211;&#65292;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#33021;&#22815;&#23398;&#20064;&#21547;&#20041;&#12290;&#27599;&#20010;&#31243;&#24207;&#37117;&#20197;&#65288;&#25991;&#26412;&#65289;&#36755;&#20837;&#36755;&#20986;&#31034;&#20363;&#30340;&#24418;&#24335;&#20316;&#20026;&#35268;&#33539;&#12290;&#19982;&#31243;&#24207;&#19968;&#36215;&#24037;&#20316;&#20351;&#25105;&#20204;&#33021;&#22815;&#31934;&#30830;&#22320;&#23450;&#20041;&#19982;&#35821;&#35328;&#20013;&#26377;&#20851;&#21547;&#20041;&#30340;&#27010;&#24565;&#65288;&#20363;&#22914;&#65292;&#27491;&#30830;&#24615;&#21644;&#35821;&#20041;&#65289;&#65292;&#20351;&#24471;&#31243;&#24207;&#32508;&#21512;&#25104;&#20026;&#19968;&#20010;&#20013;&#38388;&#27979;&#35797;&#24179;&#21488;&#65292;&#29992;&#20110;&#34920;&#24449;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#21547;&#20041;&#30340;&#23384;&#22312;&#65288;&#25110;&#19981;&#23384;&#22312;&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#31243;&#24207;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#20102;&#19968;&#20010;Transformer&#27169;&#22411;&#65292;&#28982;&#21518;&#25506;&#26597;&#20102;&#24050;&#32463;&#23436;&#25104;&#35268;&#33539;&#30340;&#31243;&#24207;&#26102;&#65292;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#38544;&#34255;&#29366;&#24577;&#12290;&#23613;&#31649;&#27809;&#26377;&#38024;&#23545;&#23398;&#20064;&#35821;&#35328;&#35821;&#20041;&#25552;&#20379;&#24402;&#32435;&#20559;&#24046;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#65292;&#32447;&#24615;&#25506;&#27979;&#22120;&#33021;&#22815;&#20174;&#27169;&#22411;&#29366;&#24577;&#20013;&#25552;&#21462;&#24403;&#21069;&#21644;&#26410;&#26469;&#31243;&#24207;&#29366;&#24577;&#30340;&#25277;&#35937;&#12290;&#27492;&#22806;&#65292;&#32447;&#24615;&#25506;&#27979;&#22120;&#30340;&#20934;&#30830;&#24615;&#19982;&#27169;&#22411;&#27867;&#21270;&#21040;&#26032;&#31243;&#24207;&#30340;&#33021;&#21147;&#24378;&#26377;&#21147;&#12289;&#32479;&#35745;&#23398;&#26174;&#33879;&#22320;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present evidence that language models can learn meaning despite being trained only to perform next token prediction on text, specifically a corpus of programs. Each program is preceded by a specification in the form of (textual) input-output examples. Working with programs enables us to precisely define concepts relevant to meaning in language (e.g., correctness and semantics), making program synthesis well-suited as an intermediate testbed for characterizing the presence (or absence) of meaning in language models.  We first train a Transformer model on the corpus of programs, then probe the trained model's hidden states as it completes a program given a specification. Despite providing no inductive bias toward learning the semantics of the language, we find that a linear probe is able to extract abstractions of both current and future program states from the model states. Moreover, there is a strong, statistically significant correlation between the accuracy of the probe and the mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21387;&#32553;&#35789;&#27719;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#21033;&#29992;&#29615;&#22659;&#21387;&#21147;&#20419;&#36827;&#24773;&#22659;&#20381;&#36182;&#24615;&#27807;&#36890;&#30340;&#20986;&#29616;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#25509;&#25910;&#32773;&#26080;&#27861;&#22788;&#29702;&#27495;&#20041;&#30340;&#24773;&#20917;&#19979;&#65292;&#21457;&#36865;&#32773;&#22914;&#20309;&#21033;&#29992;&#29615;&#22659;&#30340;&#21046;&#32422;&#22240;&#32032;&#23454;&#29616;&#27807;&#36890;&#12290;</title><link>http://arxiv.org/abs/2305.05821</link><description>&lt;p&gt;
&#29615;&#22659;&#32422;&#26463;&#19979;&#30340;&#24773;&#22659;&#20381;&#36182;&#24615;&#27807;&#36890;
&lt;/p&gt;
&lt;p&gt;
Context-dependent communication under environmental constraints. (arXiv:2305.05821v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21387;&#32553;&#35789;&#27719;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#21033;&#29992;&#29615;&#22659;&#21387;&#21147;&#20419;&#36827;&#24773;&#22659;&#20381;&#36182;&#24615;&#27807;&#36890;&#30340;&#20986;&#29616;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#25509;&#25910;&#32773;&#26080;&#27861;&#22788;&#29702;&#27495;&#20041;&#30340;&#24773;&#20917;&#19979;&#65292;&#21457;&#36865;&#32773;&#22914;&#20309;&#21033;&#29992;&#29615;&#22659;&#30340;&#21046;&#32422;&#22240;&#32032;&#23454;&#29616;&#27807;&#36890;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23384;&#22312;&#22823;&#37327;&#30340;&#35777;&#25454;&#34920;&#26126;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#27807;&#36890;&#19981;&#33021;&#31616;&#21333;&#22320;&#36890;&#36807;&#21457;&#36865;&#20855;&#26377;&#29420;&#31435;&#20110;&#24773;&#22659;&#24847;&#20041;&#30340;&#20449;&#21495;&#26469;&#23454;&#29616;&#12290;&#26412;&#25991;&#20197;&#32463;&#20856;&#30340;Lewis(1969)&#20449;&#21495;&#27169;&#22411;&#30340;&#21464;&#20307;&#20026;&#22522;&#30784;&#65292;&#25506;&#35752;&#22312;&#24773;&#22659;&#21270;&#22330;&#26223;&#19979;&#20135;&#29983;&#24773;&#22659;&#20381;&#36182;&#24615;&#27807;&#36890;&#30340;&#26465;&#20214;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26368;&#23567;&#21270;&#35789;&#27719;&#37327;&#30340;&#21387;&#21147;&#19979;&#65292;&#36825;&#31181;&#27807;&#36890;&#30340;&#20986;&#29616;&#26159;&#36275;&#22815;&#30340;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21487;&#33021;&#20351;&#31526;&#21495;&#21547;&#20041;&#24471;&#21040;&#24773;&#22659;&#21306;&#20998;&#30340;&#29615;&#22659;&#26465;&#20214;&#21644;&#35748;&#30693;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25509;&#21463;&#32773;&#30340;&#25351;&#20195;&#36873;&#25321;&#21463;&#21040;&#29615;&#22659;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#21457;&#36865;&#32773;&#21487;&#20197;&#21333;&#26041;&#38754;&#22320;&#21033;&#29992;&#36825;&#20123;&#38480;&#21046;&#65292;&#32780;&#26080;&#38656;&#25509;&#25910;&#32773;&#20855;&#26377;&#28548;&#28165;&#27495;&#20041;&#30340;&#33021;&#21147;&#12290;&#19982;&#24120;&#35265;&#30340;&#20551;&#35774;&#19968;&#33268;&#65292;&#21457;&#36865;&#32773;&#23545;&#24773;&#22659;&#30340;&#24847;&#35782;&#20284;&#20046;&#26159;&#38656;&#35201;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#24773;&#22659;&#20381;&#36182;&#24615;&#27807;&#36890;&#26159;&#19968;&#31181;&#22810;&#23618;&#27425;&#30340;&#24773;&#22659;&#21270;&#29616;&#35937;&#65292;&#20854;&#21463;&#29615;&#22659;&#29305;&#24615;&#30340;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is significant evidence that real-world communication cannot be reduced to sending signals with context-independent meaning. In this work, based on a variant of the classical Lewis (1969) signaling model, we explore the conditions for the emergence of context-dependent communication in a situated scenario. In particular, we demonstrate that pressure to minimise the vocabulary size is sufficient for such emergence. At the same time, we study the environmental conditions and cognitive capabilities that enable contextual disambiguation of symbol meanings. We show that environmental constraints on the receiver's referent choice can be unilaterally exploited by the sender, without disambiguation capabilities on the receiver's end. Consistent with common assumptions, the sender's awareness of the context appears to be required for contextual communication. We suggest that context-dependent communication is a situated multilayered phenomenon, crucially influenced by environment properti
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#25968;&#25454;&#30340;&#26032;&#22411;&#35270;&#35273;&#23398;&#20064;&#26041;&#27861;MUlti-modal Generator (MUG)&#12290;&#22312;&#35270;&#35273;&#25968;&#25454;&#38598;&#30340;&#36716;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#65292;&#26159;&#20043;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;3.4%&#21644;2.2%&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2301.07088</link><description>&lt;p&gt;
&#35270;&#35273;&#23398;&#20064;&#32773;&#36935;&#35265;Web&#22270;&#20687;-&#25991;&#26412;&#23545;
&lt;/p&gt;
&lt;p&gt;
Vision Learners Meet Web Image-Text Pairs. (arXiv:2301.07088v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#25968;&#25454;&#30340;&#26032;&#22411;&#35270;&#35273;&#23398;&#20064;&#26041;&#27861;MUlti-modal Generator (MUG)&#12290;&#22312;&#35270;&#35273;&#25968;&#25454;&#38598;&#30340;&#36716;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#65292;&#26159;&#20043;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;3.4%&#21644;2.2%&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#26368;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#37117;&#26159;&#22312;&#32500;&#25252;&#33391;&#22909;&#30340;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#32771;&#34385;&#21040;&#32593;&#32476;&#25968;&#25454;&#30340;&#20986;&#33394;&#21487;&#20280;&#32553;&#24615;&#65292;&#25105;&#20204;&#35748;&#20026;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#24212;&#35813;&#22522;&#20110;&#22024;&#26434;&#30340;&#32593;&#32476;&#28304;&#22270;&#25991;&#37197;&#23545;&#25968;&#25454;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#22914;&#27492;&#35774;&#32622;&#19979;&#65292;&#23545;&#22823;&#35268;&#27169;&#32593;&#32476;&#25968;&#25454;&#19978;&#30340;&#20195;&#34920;&#24615;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#30740;&#31350;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19968;&#31995;&#21015;&#26041;&#27861;&#65292;&#21253;&#25324;&#20351;&#29992;&#34987;&#23631;&#34109;&#30340;&#35757;&#32451;&#30446;&#26631;&#30340;&#21333;&#27169;&#24335;&#26041;&#27861;&#21644;&#20351;&#29992;&#22270;&#20687;-&#25991;&#26412;&#23545;&#27604;&#35757;&#32451;&#30340;&#22810;&#27169;&#24335;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#22312;&#35270;&#35273;&#36716;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#24182;&#19981;&#27604;&#21333;&#27169;&#24577;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#35270;&#35282;&#26469;&#35299;&#37322;&#36825;&#20123;&#22522;&#20934;&#32467;&#26524;&#65292;&#36825;&#25552;&#20379;&#20102;&#22914;&#20309;&#35774;&#35745;&#26032;&#22411;&#35270;&#35273;&#23398;&#20064;&#32773;&#30340;&#35265;&#35299;&#12290;&#21463;&#21040;&#36825;&#20123;&#35265;&#35299;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;&#34920;&#31034;&#39044;&#35757;&#32451;&#26041;&#27861;&#8212;&#8212;&#22810;&#27169;&#24335;&#29983;&#25104;&#22120;&#65288;MUG&#65289;&#65292;&#23427;&#20174;&#21487;&#20280;&#32553;&#30340;&#32593;&#32476;&#28304;&#22270;&#25991;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;MUG&#22312;&#20960;&#20010;&#35270;&#35273;&#25968;&#25454;&#38598;&#30340;&#36716;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22312;CIFAR-10&#19978;&#20248;&#20110;&#20043;&#21069;&#26368;&#20339;&#30340;&#32467;&#26524;3.4&#65285;&#65292;&#22312;STL-10&#19978;&#20248;&#20110;&#20043;&#21069;&#26368;&#20339;&#30340;&#32467;&#26524;2.2&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most recent self-supervised learning methods are pre-trained on the well-curated ImageNet-1K dataset. In this work, given the excellent scalability of web data, we consider self-supervised pre-training on noisy web sourced image-text paired data. First, we conduct a benchmark study of representative self-supervised pre-training methods on large-scale web data in a like-for-like setting. We compare a range of methods, including single-modal ones that use masked training objectives and multi-modal ones that use image-text constrastive training. We observe that existing multi-modal methods do not outperform their single-modal counterparts on vision transfer learning tasks. We derive an information-theoretical view to explain these benchmark results, which provides insight into how to design a novel vision learner. Inspired by this insight, we present a new visual representation pre-training method, MUlti-modal Generator~(MUG), that learns from scalable web sourced image-text data. MUG ach
&lt;/p&gt;</description></item></channel></rss>