<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>NCoder &#26159;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#22330;&#35770;&#30340;&#25968;&#25454;&#32534;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#33258;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#65292;&#23558;&#28508;&#22312;&#23618;&#35268;&#23450;&#20026;&#19968;&#32452; $n$-&#28857;&#20851;&#32852;&#20989;&#25968;&#30340;&#23376;&#38598;&#65292;&#27169;&#25311;&#20102;&#36890;&#36807;&#36153;&#26364;&#22270;&#25353;&#38454;&#23637;&#24320;&#26500;&#24314;&#29702;&#35770;&#26377;&#25928;&#20316;&#29992;&#37327;&#30340;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;NCoder &#20063;&#21487;&#20197;&#30475;&#20316;&#26159;&#27169;&#25311;&#32479;&#35745;&#25512;&#26029;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20960;&#20010;&#36739;&#20302;&#32500;&#24230;&#30340;&#32479;&#35745;&#37327;&#26469;&#24635;&#32467;&#39640;&#32500;&#24230;&#25968;&#25454;&#65292;&#24182;&#20174;&#36825;&#20123;&#32479;&#35745;&#37327;&#20013;&#25512;&#26029;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#31034;&#20102;&#25200;&#21160;&#37325;&#25972;&#21270;&#21644;&#27169;&#22411;&#30340;&#20805;&#20998;&#24615;&#20043;&#38388;&#30340;&#26377;&#36259;&#23545;&#24212;&#20851;&#31995;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00944</link><description>&lt;p&gt;
NCoder -- &#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#22330;&#35770;&#30340;&#25968;&#25454;&#32534;&#30721;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
NCoder -- A Quantum Field Theory approach to encoding data
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00944
&lt;/p&gt;
&lt;p&gt;
NCoder &#26159;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#22330;&#35770;&#30340;&#25968;&#25454;&#32534;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#33258;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#65292;&#23558;&#28508;&#22312;&#23618;&#35268;&#23450;&#20026;&#19968;&#32452; $n$-&#28857;&#20851;&#32852;&#20989;&#25968;&#30340;&#23376;&#38598;&#65292;&#27169;&#25311;&#20102;&#36890;&#36807;&#36153;&#26364;&#22270;&#25353;&#38454;&#23637;&#24320;&#26500;&#24314;&#29702;&#35770;&#26377;&#25928;&#20316;&#29992;&#37327;&#30340;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;NCoder &#20063;&#21487;&#20197;&#30475;&#20316;&#26159;&#27169;&#25311;&#32479;&#35745;&#25512;&#26029;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20960;&#20010;&#36739;&#20302;&#32500;&#24230;&#30340;&#32479;&#35745;&#37327;&#26469;&#24635;&#32467;&#39640;&#32500;&#24230;&#25968;&#25454;&#65292;&#24182;&#20174;&#36825;&#20123;&#32479;&#35745;&#37327;&#20013;&#25512;&#26029;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#31034;&#20102;&#25200;&#21160;&#37325;&#25972;&#21270;&#21644;&#27169;&#22411;&#30340;&#20805;&#20998;&#24615;&#20043;&#38388;&#30340;&#26377;&#36259;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#22330;&#35770;&#65288;QFT&#65289;&#30340;&#21487;&#35299;&#37322; AI &#26041;&#27861;&#65292;&#31216;&#20026; NCoder&#12290;NCoder &#26159;&#19968;&#20010;&#20462;&#25913;&#36807;&#30340;&#33258;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#28508;&#22312;&#23618;&#34987;&#35268;&#23450;&#20026;&#19968;&#20010; $n$-&#28857;&#20851;&#32852;&#20989;&#25968;&#30340;&#23376;&#38598;&#12290;&#23558;&#22270;&#20687;&#35270;&#20026;&#26684;&#28857;&#22330;&#35770;&#30340;&#25277;&#26679;&#65292;&#36825;&#20010;&#26550;&#26500;&#27169;&#25311;&#20102;&#20351;&#29992;&#36153;&#26364;&#22270;&#25353;&#38454;&#23637;&#24320;&#26469;&#25200;&#21160;&#22320;&#26500;&#24314;&#29702;&#35770;&#26377;&#25928;&#20316;&#29992;&#37327;&#30340;&#20219;&#21153;&#12290;&#25110;&#32773;&#65292;NCoder &#21487;&#20197;&#34987;&#35270;&#20026;&#27169;&#25311;&#32479;&#35745;&#25512;&#26029;&#36807;&#31243;&#65292;&#36890;&#36807;&#29992;&#20960;&#20010;&#36739;&#20302;&#32500;&#24230;&#30340;&#27719;&#24635;&#32479;&#35745;&#37327;&#65288;&#36825;&#37324;&#26159; $n$-&#28857;&#20851;&#32852;&#20989;&#25968;&#65289;&#26469;&#24635;&#32467;&#39640;&#32500;&#24230;&#25968;&#25454;&#65292;&#24182;&#20174;&#36825;&#20123;&#32479;&#35745;&#37327;&#25512;&#26029;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#26469;&#29983;&#25104;&#26679;&#26412;&#22806;&#25968;&#25454;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;NCoder &#25552;&#31034;&#20102;&#25200;&#21160;&#37325;&#25972;&#21270;&#21644;&#27169;&#22411;&#30340;&#20805;&#20998;&#24615;&#20043;&#38388;&#30340;&#26377;&#36259;&#23545;&#24212;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; NCoder &#22312;&#25968;&#25454;&#32534;&#30721;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present a novel approach to interpretable AI inspired by Quantum Field Theory (QFT) which we call the NCoder. The NCoder is a modified autoencoder neural network whose latent layer is prescribed to be a subset of $n$-point correlation functions. Regarding images as draws from a lattice field theory, this architecture mimics the task of perturbatively constructing the effective action of the theory order by order in an expansion using Feynman diagrams. Alternatively, the NCoder may be regarded as simulating the procedure of statistical inference whereby high dimensional data is first summarized in terms of several lower dimensional summary statistics (here the $n$-point correlation functions), and subsequent out-of-sample data is generated by inferring the data generating distribution from these statistics. In this way the NCoder suggests a fascinating correspondence between perturbative renormalizability and the sufficiency of models. We demonstrate the efficacy of the
&lt;/p&gt;</description></item><item><title>SLEDGE&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#30340;&#36710;&#36742;&#36816;&#21160;&#35268;&#21010;&#29983;&#25104;&#27169;&#25311;&#22120;&#65292;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#26629;&#26684;&#21040;&#30690;&#37327;&#33258;&#32534;&#30721;&#22120;&#65288;RVAE&#65289;&#20197;&#21450;Diffusion Transformer&#26469;&#29983;&#25104;&#26234;&#33021;&#20307;&#21644;&#36710;&#36947;&#22270;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#27169;&#25311;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.17933</link><description>&lt;p&gt;
SLEDGE: &#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#21512;&#25104;&#39550;&#39542;&#26234;&#33021;&#20307;&#30340;&#27169;&#25311;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
SLEDGE: Synthesizing Simulation Environments for Driving Agents with Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17933
&lt;/p&gt;
&lt;p&gt;
SLEDGE&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#30340;&#36710;&#36742;&#36816;&#21160;&#35268;&#21010;&#29983;&#25104;&#27169;&#25311;&#22120;&#65292;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#26629;&#26684;&#21040;&#30690;&#37327;&#33258;&#32534;&#30721;&#22120;&#65288;RVAE&#65289;&#20197;&#21450;Diffusion Transformer&#26469;&#29983;&#25104;&#26234;&#33021;&#20307;&#21644;&#36710;&#36947;&#22270;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#27169;&#25311;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SLEDGE&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#30495;&#23454;&#19990;&#30028;&#39550;&#39542;&#35760;&#24405;&#35757;&#32451;&#30340;&#36710;&#36742;&#36816;&#21160;&#35268;&#21010;&#29983;&#25104;&#27169;&#25311;&#22120;&#12290;&#20854;&#26680;&#24515;&#32452;&#20214;&#26159;&#19968;&#20010;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#26234;&#33021;&#20307;&#36793;&#30028;&#26694;&#21644;&#36710;&#36947;&#22270;&#12290;&#35813;&#27169;&#22411;&#30340;&#36755;&#20986;&#20316;&#20026;&#20132;&#36890;&#27169;&#25311;&#30340;&#21021;&#22987;&#29366;&#24577;&#12290;&#38024;&#23545;SLEDGE&#24453;&#29983;&#25104;&#30340;&#23454;&#20307;&#30340;&#29420;&#29305;&#29305;&#24615;&#65292;&#20363;&#22914;&#23427;&#20204;&#30340;&#36830;&#25509;&#24615;&#21644;&#27599;&#20010;&#22330;&#26223;&#30340;&#21487;&#21464;&#25968;&#37327;&#65292;&#20351;&#24471;&#22823;&#22810;&#25968;&#29616;&#20195;&#29983;&#25104;&#27169;&#22411;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#26420;&#32032;&#24212;&#29992;&#21464;&#24471;&#19981;&#31616;&#21333;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38500;&#20102;&#23545;&#29616;&#26377;&#36710;&#36947;&#22270;&#34920;&#31034;&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26629;&#26684;&#21040;&#30690;&#37327;&#33258;&#32534;&#30721;&#22120;&#65288;RVAE&#65289;&#12290;&#23427;&#23558;&#26234;&#33021;&#20307;&#21644;&#36710;&#36947;&#22270;&#32534;&#30721;&#20026;&#26629;&#26684;&#21270;&#28508;&#22312;&#26144;&#23556;&#20013;&#30340;&#19981;&#21516;&#36890;&#36947;&#12290;&#36825;&#26377;&#21161;&#20110;&#36710;&#36947;&#26465;&#20214;&#19979;&#30340;&#26234;&#33021;&#20307;&#29983;&#25104;&#20197;&#21450;&#20351;&#29992;&#25193;&#25955;&#21464;&#25442;&#22120;&#21516;&#26102;&#29983;&#25104;&#36710;&#36947;&#21644;&#26234;&#33021;&#20307;&#12290;&#22312;SLEDGE&#20013;&#20351;&#29992;&#29983;&#25104;&#30340;&#23454;&#20307;&#21487;&#20197;&#26356;&#22909;&#22320;&#25511;&#21046;&#27169;&#25311;&#65292;&#20363;&#22914;&#19978;&#37319;&#26679;&#36716;&#24367;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17933v1 Announce Type: cross  Abstract: SLEDGE is the first generative simulator for vehicle motion planning trained on real-world driving logs. Its core component is a learned model that is able to generate agent bounding boxes and lane graphs. The model's outputs serve as an initial state for traffic simulation. The unique properties of the entities to be generated for SLEDGE, such as their connectivity and variable count per scene, render the naive application of most modern generative models to this task non-trivial. Therefore, together with a systematic study of existing lane graph representations, we introduce a novel raster-to-vector autoencoder (RVAE). It encodes agents and the lane graph into distinct channels in a rasterized latent map. This facilitates both lane-conditioned agent generation and combined generation of lanes and agents with a Diffusion Transformer. Using generated entities in SLEDGE enables greater control over the simulation, e.g. upsampling turns 
&lt;/p&gt;</description></item><item><title>&#24378;&#21270;&#23398;&#20064;&#21644;&#22240;&#26524;&#24314;&#27169;&#30456;&#20114;&#34917;&#20805;&#65292;&#35770;&#25991;&#20027;&#35201;&#25351;&#20986;&#22312;&#32447;&#23398;&#20064;&#29615;&#22659;&#19979;&#65292;&#26465;&#20214;&#27010;&#29575;&#20855;&#26377;&#22240;&#26524;&#24615;&#65292;&#31163;&#32447;RL&#26159;&#22240;&#26524;&#23398;&#20064;&#28508;&#21147;&#26368;&#22823;&#30340;&#29615;&#22659;&#12290;</title><link>https://arxiv.org/abs/2403.04221</link><description>&lt;p&gt;
&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20026;&#20309;&#20855;&#26377;&#22240;&#26524;&#24615;
&lt;/p&gt;
&lt;p&gt;
Why Online Reinforcement Learning is Causal
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04221
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#21644;&#22240;&#26524;&#24314;&#27169;&#30456;&#20114;&#34917;&#20805;&#65292;&#35770;&#25991;&#20027;&#35201;&#25351;&#20986;&#22312;&#32447;&#23398;&#20064;&#29615;&#22659;&#19979;&#65292;&#26465;&#20214;&#27010;&#29575;&#20855;&#26377;&#22240;&#26524;&#24615;&#65292;&#31163;&#32447;RL&#26159;&#22240;&#26524;&#23398;&#20064;&#28508;&#21147;&#26368;&#22823;&#30340;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21644;&#22240;&#26524;&#24314;&#27169;&#33258;&#28982;&#20114;&#34917;&#12290;&#22240;&#26524;&#24314;&#27169;&#30340;&#30446;&#26631;&#26159;&#39044;&#27979;&#22312;&#29615;&#22659;&#20013;&#36827;&#34892;&#24178;&#39044;&#30340;&#25928;&#26524;&#65292;&#32780;&#24378;&#21270;&#23398;&#20064;&#30340;&#30446;&#26631;&#26159;&#36873;&#25321;&#26368;&#22823;&#21270;&#20195;&#29702;&#20174;&#29615;&#22659;&#20013;&#25509;&#25910;&#30340;&#22870;&#21169;&#30340;&#24178;&#39044;&#12290;&#24378;&#21270;&#23398;&#20064;&#21253;&#25324;&#29992;&#20110;&#20272;&#35745;&#22240;&#26524;&#20851;&#31995;&#30340;&#20004;&#20010;&#26368;&#24378;&#22823;&#20449;&#24687;&#28304;&#65306;&#26102;&#38388;&#39034;&#24207;&#21644;&#23545;&#29615;&#22659;&#36827;&#34892;&#25805;&#20316;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#25105;&#20204;&#21487;&#20197;&#26399;&#26395;&#22312;&#21738;&#20123;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#20013;&#20174;&#22240;&#26524;&#24314;&#27169;&#20013;&#21463;&#30410;&#65292;&#20197;&#21450;&#22914;&#20309;&#21463;&#30410;&#12290;&#22312;&#32447;&#23398;&#20064;&#20013;&#65292;&#20195;&#29702;&#26377;&#33021;&#21147;&#30452;&#25509;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#20174;&#25506;&#32034;&#20013;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35770;&#28857;&#26159;&#65292;&#22312;&#22312;&#32447;&#23398;&#20064;&#20013;&#65292;&#26465;&#20214;&#27010;&#29575;&#26159;&#22240;&#26524;&#30340;&#65292;&#22240;&#27492;&#31163;&#32447;RL&#26159;&#22240;&#26524;&#23398;&#20064;&#26377;&#28508;&#21147;&#20135;&#29983;&#24046;&#24322;&#30340;&#29615;&#22659;&#12290;&#22522;&#26412;&#19978;&#65292;&#21407;&#22240;&#22312;&#20110;&#24403;&#20195;&#29702;&#19982;&#29615;&#22659;&#20114;&#21160;&#26102;&#65292;&#20195;&#29702;&#30340;&#34892;&#20026;&#26159;&#30001;&#20854;&#23545;&#29615;&#22659;&#30340;&#35748;&#35782;&#25152;&#25512;&#21160;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04221v1 Announce Type: cross  Abstract: Reinforcement learning (RL) and causal modelling naturally complement each other. The goal of causal modelling is to predict the effects of interventions in an environment, while the goal of reinforcement learning is to select interventions that maximize the rewards the agent receives from the environment. Reinforcement learning includes the two most powerful sources of information for estimating causal relationships: temporal ordering and the ability to act on an environment. This paper examines which reinforcement learning settings we can expect to benefit from causal modelling, and how. In online learning, the agent has the ability to interact directly with their environment, and learn from exploring it. Our main argument is that in online learning, conditional probabilities are causal, and therefore offline RL is the setting where causal learning has the most potential to make a difference. Essentially, the reason is that when an a
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#23545;&#36755;&#20837;&#30340;&#25200;&#21160;&#65292;&#36890;&#36807;&#38543;&#26426;&#25513;&#30422;&#24605;&#32500;&#38142;&#20013;&#30340;&#26576;&#20123;&#26631;&#35760;&#65292;&#21487;&#26174;&#33879;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#23398;&#20064;&#25928;&#26524;</title><link>https://arxiv.org/abs/2403.02178</link><description>&lt;p&gt;
&#25513;&#38754;&#24605;&#24819;:&#31616;&#21333;&#22320;&#25513;&#30422;&#37096;&#20998;&#25512;&#29702;&#27493;&#39588;&#21487;&#20197;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#23545;&#25968;&#23398;&#25512;&#29702;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Masked Thought: Simply Masking Partial Reasoning Steps Can Improve Mathematical Reasoning Learning of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02178
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#23545;&#36755;&#20837;&#30340;&#25200;&#21160;&#65292;&#36890;&#36807;&#38543;&#26426;&#25513;&#30422;&#24605;&#32500;&#38142;&#20013;&#30340;&#26576;&#20123;&#26631;&#35760;&#65292;&#21487;&#26174;&#33879;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#23398;&#20064;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;&#21363;&#20351;&#26159;&#19968;&#20010;&#36731;&#24494;&#30340;&#38169;&#35823;&#20063;&#21487;&#33021;&#23548;&#33268;&#19981;&#20934;&#30830;&#30340;&#32467;&#26524;&#65292;&#20174;&#32780;&#23548;&#33268;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20123;&#39046;&#22495;&#30340;&#24615;&#33021;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36991;&#20813;&#20102;&#22806;&#37096;&#36164;&#28304;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#24341;&#20837;&#23545;&#36755;&#20837;&#30340;&#25200;&#21160;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#26041;&#27861;&#38543;&#26426;&#25513;&#30422;&#20102;&#38142;&#24335;&#24605;&#32500;&#20013;&#30340;&#26576;&#20123;&#26631;&#35760;&#65292;&#36825;&#31181;&#25216;&#26415;&#23545;&#25512;&#29702;&#20219;&#21153;&#29305;&#21035;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02178v1 Announce Type: cross  Abstract: In reasoning tasks, even a minor error can cascade into inaccurate results, leading to suboptimal performance of large language models in such domains. Earlier fine-tuning approaches sought to mitigate this by leveraging more precise supervisory signals from human labeling, larger models, or self-sampling, although at a high cost. Conversely, we develop a method that avoids external resources, relying instead on introducing perturbations to the input. Our training approach randomly masks certain tokens within the chain of thought, a technique we found to be particularly effective for reasoning tasks. When applied to fine-tuning with GSM8K, this method achieved a 5% improvement in accuracy over standard supervised fine-tuning with a few codes modified and no additional labeling effort. Furthermore, it is complementary to existing methods. When integrated with related data augmentation methods, it leads to an average improvement of 3% im
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#31070;&#32463;&#32593;&#32476;&#20013;&#29109;&#27169;&#22411;&#23545;&#26377;&#24847;&#24178;&#25200;&#21644;&#26080;&#24847;&#24178;&#25200;&#30340;&#38887;&#24615;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#29109;&#27169;&#22411;&#30340;&#38887;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00942</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#31070;&#32463;&#32593;&#32476;&#20013;&#29109;&#27169;&#22411;&#30340;&#38887;&#24615;
&lt;/p&gt;
&lt;p&gt;
Resilience of Entropy Model in Distributed Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#31070;&#32463;&#32593;&#32476;&#20013;&#29109;&#27169;&#22411;&#23545;&#26377;&#24847;&#24178;&#25200;&#21644;&#26080;&#24847;&#24178;&#25200;&#30340;&#38887;&#24615;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#29109;&#27169;&#22411;&#30340;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#24050;&#32463;&#25104;&#20026;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#20013;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#32780;&#19981;&#38477;&#20302;&#24615;&#33021;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#26368;&#36817;&#65292;&#29109;&#32534;&#30721;&#34987;&#24341;&#20837;&#20197;&#36827;&#19968;&#27493;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#20998;&#24067;&#24335;DNN&#19982;&#29109;&#27169;&#22411;&#32852;&#21512;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#22312;&#25512;&#26029;&#26102;&#38388;&#29992;&#20316;&#36793;&#20449;&#24687;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#23558;&#28508;&#22312;&#34920;&#31034;&#32534;&#30721;&#20026;&#20855;&#26377;&#21487;&#21464;&#38271;&#24230;&#30340;&#27604;&#29305;&#27969;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#29109;&#27169;&#22411;&#30340;&#38887;&#24615;&#23578;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21046;&#23450;&#24182;&#35843;&#26597;&#20102;&#29109;&#27169;&#22411;&#23545;&#26377;&#24847;&#24178;&#25200;&#65288;&#20363;&#22914;&#65292;&#23545;&#25239;&#24615;&#25915;&#20987;&#65289;&#21644;&#26080;&#24847;&#24178;&#25200;&#65288;&#20363;&#22914;&#65292;&#22825;&#27668;&#21464;&#21270;&#21644;&#36816;&#21160;&#27169;&#31946;&#65289;&#30340;&#38887;&#24615;&#12290;&#36890;&#36807;&#23545;3&#31181;&#19981;&#21516;DNN&#26550;&#26500;&#12289;2&#20010;&#29109;&#27169;&#22411;&#21644;4&#20010;&#36895;&#29575;&#22833;&#30495;&#26435;&#34913;&#22240;&#23376;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#29109;&#27169;&#22411;&#30340;&#38887;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00942v1 Announce Type: cross  Abstract: Distributed deep neural networks (DNNs) have emerged as a key technique to reduce communication overhead without sacrificing performance in edge computing systems. Recently, entropy coding has been introduced to further reduce the communication overhead. The key idea is to train the distributed DNN jointly with an entropy model, which is used as side information during inference time to adaptively encode latent representations into bit streams with variable length. To the best of our knowledge, the resilience of entropy models is yet to be investigated. As such, in this paper we formulate and investigate the resilience of entropy models to intentional interference (e.g., adversarial attacks) and unintentional interference (e.g., weather changes and motion blur). Through an extensive experimental campaign with 3 different DNN architectures, 2 entropy models and 4 rate-distortion trade-off factors, we demonstrate that the entropy attacks
&lt;/p&gt;</description></item><item><title>&#23545;GPT-4&#22312;&#31639;&#27861;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#21457;&#29616;&#37319;&#29992;&#20808;&#36827;&#30340;&#25552;&#31034;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17396</link><description>&lt;p&gt;
&#22312;&#31639;&#27861;&#38382;&#39064;&#19978;&#23545;GPT-4&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65306;&#20851;&#20110;&#25552;&#31034;&#31574;&#30053;&#30340;&#31995;&#32479;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Benchmarking GPT-4 on Algorithmic Problems: A Systematic Evaluation of Prompting Strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17396
&lt;/p&gt;
&lt;p&gt;
&#23545;GPT-4&#22312;&#31639;&#27861;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#21457;&#29616;&#37319;&#29992;&#20808;&#36827;&#30340;&#25552;&#31034;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#22312;&#28023;&#37327;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#33719;&#24471;&#30340;&#30693;&#35782;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#37325;&#26032;&#21033;&#29992;&#65292;&#20960;&#20046;&#19981;&#38656;&#35201;&#65288;&#25110;&#26681;&#26412;&#19981;&#38656;&#35201;&#65289;&#35843;&#25972;&#27493;&#39588;&#65292;&#20174;&#32780;&#38761;&#26032;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#24050;&#32463;&#21453;&#22797;&#26174;&#31034;LLMs&#32570;&#20047;&#31995;&#32479;&#21270;&#27867;&#21270;&#65292;&#36825;&#20351;&#24471;&#26080;&#27861;&#23558;&#23398;&#20064;&#21040;&#30340;&#32479;&#35745;&#35268;&#24459;&#22806;&#25512;&#21040;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#20854;&#20013;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;LLMs&#65292;GPT-4&#65292;&#22312;&#19977;&#20010;&#31639;&#27861;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#31995;&#32479;&#22522;&#20934;&#27979;&#35797;&#65292;&#36825;&#20123;&#20219;&#21153;&#36890;&#36807;&#20004;&#20010;&#21442;&#25968;&#25511;&#21046;&#38382;&#39064;&#38590;&#24230;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;GPT-4&#19982;&#20854;&#21069;&#36523;&#65288;GPT-3.5&#65289;&#20197;&#21450;&#26368;&#36817;&#20171;&#32461;&#30340;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#26550;&#26500;&#30340;&#21464;&#20307;&#65292;&#21363;&#31070;&#32463;&#25968;&#25454;&#36335;&#30001;&#22120;&#65292;&#22312;&#35299;&#20915;&#31867;&#20284;&#20219;&#21153;&#26102;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#37319;&#29992;&#20808;&#36827;&#30340;&#25552;&#31034;&#25216;&#26415;&#21487;&#20197;&#20351;GPT-4&#36798;&#21040;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17396v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have revolutionized the field of Natural Language Processing thanks to their ability to reuse knowledge acquired on massive text corpora on a wide variety of downstream tasks, with minimal (if any) tuning steps. At the same time, it has been repeatedly shown that LLMs lack systematic generalization, which allows to extrapolate the learned statistical regularities outside the training distribution. In this work, we offer a systematic benchmarking of GPT-4, one of the most advanced LLMs available, on three algorithmic tasks characterized by the possibility to control the problem difficulty with two parameters. We compare the performance of GPT-4 with that of its predecessor (GPT-3.5) and with a variant of the Transformer-Encoder architecture recently introduced to solve similar tasks, the Neural Data Router. We find that the deployment of advanced prompting techniques allows GPT-4 to reach superior accuracy o
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#27604;&#39640;&#26031;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#28304;&#22836;&#21551;&#21160;&#25193;&#25955;&#26041;&#27861;&#26377;&#21161;&#20110;&#20811;&#26381;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.16075</link><description>&lt;p&gt;
&#22522;&#20110;&#25554;&#20540;&#30340;&#31574;&#30053;&#25193;&#25955;&#30340;&#34892;&#20026;&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;
Behavioral Refinement via Interpolant-based Policy Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16075
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#27604;&#39640;&#26031;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#28304;&#22836;&#21551;&#21160;&#25193;&#25955;&#26041;&#27861;&#26377;&#21161;&#20110;&#20811;&#26381;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#20351;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#36890;&#36807;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#26469;&#27169;&#20223;&#34892;&#20026;&#12290;&#26368;&#36817;&#65292;&#25317;&#26377;&#24314;&#27169;&#39640;&#32500;&#24230;&#21644;&#22810;&#27169;&#24577;&#20998;&#24067;&#33021;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#23558;&#21160;&#20316;&#65288;&#25110;&#29366;&#24577;&#65289;&#20174;&#26631;&#20934;&#39640;&#26031;&#22122;&#22768;&#20013;&#25193;&#25955;&#26469;&#22609;&#36896;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#35201;&#23398;&#20064;&#30340;&#30446;&#26631;&#31574;&#30053;&#36890;&#24120;&#19982;&#39640;&#26031;&#20998;&#24067;&#26174;&#33879;&#19981;&#21516;&#65292;&#36825;&#31181;&#19981;&#21305;&#37197;&#21487;&#33021;&#23548;&#33268;&#22312;&#20351;&#29992;&#23569;&#37327;&#25193;&#25955;&#27493;&#39588;&#65288;&#20197;&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#65289;&#21644;&#26377;&#38480;&#25968;&#25454;&#19979;&#24615;&#33021;&#19981;&#20339;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#65292;&#20174;&#27604;&#39640;&#26031;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#28304;&#22836;&#24320;&#22987;&#65292;&#21487;&#20197;&#20351;&#25193;&#25955;&#26041;&#27861;&#20811;&#26381;&#19978;&#36848;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#32467;&#26524;&#12289;&#19968;&#31181;&#26032;&#26041;&#27861;&#21644;&#23454;&#35777;&#21457;&#29616;&#65292;&#23637;&#31034;&#20102;&#20351;&#29992;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#28304;&#31574;&#30053;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;BRIDGER&#65292;&#21033;&#29992;&#20102;&#38543;&#26426;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16075v1 Announce Type: cross  Abstract: Imitation learning empowers artificial agents to mimic behavior by learning from demonstrations. Recently, diffusion models, which have the ability to model high-dimensional and multimodal distributions, have shown impressive performance on imitation learning tasks. These models learn to shape a policy by diffusing actions (or states) from standard Gaussian noise. However, the target policy to be learned is often significantly different from Gaussian and this mismatch can result in poor performance when using a small number of diffusion steps (to improve inference speed) and under limited data. The key idea in this work is that initiating from a more informative source than Gaussian enables diffusion methods to overcome the above limitations. We contribute both theoretical results, a new method, and empirical findings that show the benefits of using an informative source policy. Our method, which we call BRIDGER, leverages the stochast
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SQL-CRAFT&#26694;&#26550;&#65292;&#36890;&#36807;&#20132;&#20114;&#24335;&#25913;&#36827;&#21644;&#22686;&#24378;&#25512;&#29702;&#65292;&#25552;&#21319;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#21040;SQL&#36716;&#25442;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#24615;&#33021;&#25552;&#21319;&#39640;&#36798;5.7%&#65292;&#24182;&#22312;Spider&#27036;&#21333;&#19978;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.14851</link><description>&lt;p&gt;
SQL-CRAFT: &#36890;&#36807;&#20132;&#20114;&#24335;&#25913;&#36827;&#21644;&#22686;&#24378;&#25512;&#29702;&#23454;&#29616;&#25991;&#26412;&#21040;SQL&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
SQL-CRAFT: Text-to-SQL through Interactive Refinement and Enhanced Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14851
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SQL-CRAFT&#26694;&#26550;&#65292;&#36890;&#36807;&#20132;&#20114;&#24335;&#25913;&#36827;&#21644;&#22686;&#24378;&#25512;&#29702;&#65292;&#25552;&#21319;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#21040;SQL&#36716;&#25442;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#24615;&#33021;&#25552;&#21319;&#39640;&#36798;5.7%&#65292;&#24182;&#22312;Spider&#27036;&#21333;&#19978;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22823;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#21464;&#24471;&#36234;&#26469;&#36234;&#24378;&#22823;&#65292;&#20294;&#22312;&#19987;&#38376;&#20219;&#21153;&#65288;&#22914;&#25991;&#26412;&#21040;SQL&#65289;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SQL-CRAFT&#65292;&#19968;&#20010;&#36890;&#36807;&#20132;&#20114;&#24335;&#25913;&#36827;&#21644;&#22686;&#24378;&#25512;&#29702;&#26469;&#25552;&#21319;&#22823;&#35821;&#35328;&#27169;&#22411;SQL&#29983;&#25104;&#33021;&#21147;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#21033;&#29992;&#20132;&#20114;&#24335;&#32416;&#38169;&#24490;&#29615;&#65288;IC-Loop&#65289;&#20351;&#22823;&#35821;&#35328;&#27169;&#22411;&#19982;&#25968;&#25454;&#24211;&#33258;&#21160;&#20132;&#20114;&#65292;&#21516;&#26102;&#37319;&#29992;&#22686;&#24378;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#38598;Spider&#21644;Bird&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#24615;&#33021;&#27604;&#26420;&#32032;&#25552;&#31034;&#26041;&#27861;&#25552;&#39640;&#20102;&#39640;&#36798;5.7%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Spider&#27036;&#21333;&#19978;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14851v1 Announce Type: cross  Abstract: Modern LLMs have become increasingly powerful, but they are still facing challenges in specialized tasks such as Text-to-SQL. We propose SQL-CRAFT, a framework to advance LLMs' SQL generation Capabilities through inteRActive reFinemenT and enhanced reasoning. We leverage an Interactive Correction Loop (IC-Loop) for LLMs to interact with databases automatically, as well as Python-enhanced reasoning. We conduct experiments on two Text-to-SQL datasets, Spider and Bird, with performance improvements of up to 5.7% compared to the naive prompting method. Moreover, our method surpasses the current state-of-the-art on the Spider Leaderboard, demonstrating the effectiveness of our framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20511;&#37492;&#20102;&#32500;&#29305;&#26681;&#26031;&#22374;&#30340;&#21518;&#26399;&#33879;&#20316;&#65292;&#23581;&#35797;&#22238;&#31572;&#22312;&#27010;&#24565;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;AI&#20195;&#29702;&#20013;&#26159;&#21542;&#26377;&#24847;&#20041;&#35848;&#35770;&#24847;&#35782;&#30340;&#38382;&#39064;&#65292;&#36991;&#20813;&#20102;&#20108;&#20803;&#35770;&#24605;&#32500;&#30340;&#38519;&#38449;&#12290;</title><link>https://arxiv.org/abs/2402.12422</link><description>&lt;p&gt;
&#27169;&#25311;&#20307;&#20316;&#20026;&#24847;&#35782;&#24322;&#22495;
&lt;/p&gt;
&lt;p&gt;
Simulacra as Conscious Exotica
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20511;&#37492;&#20102;&#32500;&#29305;&#26681;&#26031;&#22374;&#30340;&#21518;&#26399;&#33879;&#20316;&#65292;&#23581;&#35797;&#22238;&#31572;&#22312;&#27010;&#24565;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;AI&#20195;&#29702;&#20013;&#26159;&#21542;&#26377;&#24847;&#20041;&#35848;&#35770;&#24847;&#35782;&#30340;&#38382;&#39064;&#65292;&#36991;&#20813;&#20102;&#20108;&#20803;&#35770;&#24605;&#32500;&#30340;&#38519;&#38449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv&#65306;2402.12422v1 &#21457;&#24067;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#38543;&#30528;&#36234;&#26469;&#36234;&#20687;&#20154;&#31867;&#34892;&#20026;&#30340;&#20250;&#35805;&#20195;&#29702;&#30340;&#20986;&#29616;&#65292;&#21476;&#32769;&#30340;&#21746;&#23398;&#38382;&#39064;&#34987;&#37325;&#26032;&#23457;&#35270;&#12290;&#22312;&#27010;&#24565;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;AI&#20195;&#29702;&#20013;&#65292;&#26159;&#21542;&#26377;&#24847;&#20041;&#35848;&#35770;&#24847;&#35782;&#65292;&#32771;&#34385;&#21040;&#23427;&#20204;&#21482;&#26159;&#20154;&#31867;&#34892;&#20026;&#30340;&#8220;&#32431;&#31929;&#8221;&#27169;&#25311;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#34892;&#20026;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#8220;&#20165;&#20165;&#8221;&#35282;&#33394;&#25198;&#28436;&#65311;&#26412;&#25991;&#20511;&#37492;&#20102;&#32500;&#29305;&#26681;&#26031;&#22374;&#30340;&#21518;&#26399;&#33879;&#20316;&#65292;&#35797;&#22270;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21516;&#26102;&#36991;&#20813;&#20108;&#20803;&#35770;&#24605;&#32500;&#30340;&#38519;&#38449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12422v1 Announce Type: new  Abstract: The advent of conversational agents with increasingly human-like behaviour throws old philosophical questions into new light. Does it, or could it, ever make sense to speak of AI agents built out of generative language models in terms of consciousness, given that they are "mere" simulacra of human behaviour, and that what they do can be seen as "merely" role play? Drawing on the later writings of Wittgenstein, this paper attempts to tackle this question while avoiding the pitfalls of dualistic thinking.
&lt;/p&gt;</description></item><item><title>&#39044;&#27979;&#24615;&#34920;&#24449;&#21487;&#33021;&#26159;&#26234;&#33021;&#30340;&#22810;&#21151;&#33021;&#22522;&#30707;&#12290;</title><link>https://arxiv.org/abs/2402.06590</link><description>&lt;p&gt;
&#39044;&#27979;&#24615;&#34920;&#24449;&#65306;&#26234;&#33021;&#30340;&#22522;&#30707;
&lt;/p&gt;
&lt;p&gt;
Predictive representations: building blocks of intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06590
&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#24615;&#34920;&#24449;&#21487;&#33021;&#26159;&#26234;&#33021;&#30340;&#22810;&#21151;&#33021;&#22522;&#30707;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36866;&#24212;&#24615;&#34892;&#20026;&#36890;&#24120;&#38656;&#35201;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#12290;&#24378;&#21270;&#23398;&#20064;&#29702;&#35770;&#35268;&#23450;&#20102;&#20160;&#20040;&#26679;&#30340;&#39044;&#27979;&#24615;&#34920;&#24449;&#26159;&#26377;&#29992;&#30340;&#20197;&#21450;&#22914;&#20309;&#35745;&#31639;&#23427;&#20204;&#12290;&#26412;&#25991;&#23558;&#36825;&#20123;&#29702;&#35770;&#35266;&#28857;&#19982;&#35748;&#30693;&#21644;&#31070;&#32463;&#31185;&#23398;&#30340;&#30740;&#31350;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#32487;&#20219;&#32773;&#34920;&#24449;&#65288;SR&#65289;&#21450;&#20854;&#24191;&#20041;&#24418;&#24335;&#65292;&#23427;&#20204;&#19981;&#20165;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24037;&#31243;&#24037;&#20855;&#65292;&#20063;&#20316;&#20026;&#22823;&#33041;&#21151;&#33021;&#30340;&#27169;&#22411;&#12290;&#36825;&#31181;&#34701;&#21512;&#34920;&#26126;&#29305;&#23450;&#31867;&#22411;&#30340;&#39044;&#27979;&#24615;&#34920;&#24449;&#21487;&#33021;&#26159;&#26234;&#33021;&#30340;&#22810;&#21151;&#33021;&#22522;&#30707;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive behavior often requires predicting future events. The theory of reinforcement learning prescribes what kinds of predictive representations are useful and how to compute them. This paper integrates these theoretical ideas with work on cognition and neuroscience. We pay special attention to the successor representation (SR) and its generalizations, which have been widely applied both as engineering tools and models of brain function. This convergence suggests that particular kinds of predictive representations may function as versatile building blocks of intelligence.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;ODE&#26041;&#27861;&#65292;&#23558;&#31283;&#23450;&#24615;&#30340;Borkar-Meyn&#23450;&#29702;&#20174;&#38789;&#24046;&#24322;&#22122;&#22768;&#35774;&#23450;&#25299;&#23637;&#21040;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#35774;&#23450;&#65292;&#25552;&#39640;&#20102;&#20854;&#22312;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.07844</link><description>&lt;p&gt;
&#20351;&#29992;ODE&#26041;&#27861;&#36827;&#34892;&#24102;&#26377;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#30340;&#38543;&#26426;&#36924;&#36817;&#21644;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
The ODE Method for Stochastic Approximation and Reinforcement Learning with Markovian Noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;ODE&#26041;&#27861;&#65292;&#23558;&#31283;&#23450;&#24615;&#30340;Borkar-Meyn&#23450;&#29702;&#20174;&#38789;&#24046;&#24322;&#22122;&#22768;&#35774;&#23450;&#25299;&#23637;&#21040;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#35774;&#23450;&#65292;&#25552;&#39640;&#20102;&#20854;&#22312;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#36924;&#36817;&#26159;&#19968;&#31867;&#36890;&#36807;&#36845;&#20195;&#12289;&#22686;&#37327;&#21644;&#38543;&#26426;&#26356;&#26032;&#21521;&#37327;&#30340;&#31639;&#27861;&#65292;&#21253;&#25324;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#26102;&#24207;&#24046;&#20998;&#23398;&#20064;&#12290;&#20998;&#26512;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#30830;&#20445;&#20854;&#31283;&#23450;&#24615;&#65292;&#21363;&#35777;&#26126;&#38543;&#26426;&#21521;&#37327;&#36845;&#20195;&#20960;&#20046;&#24517;&#23450;&#26377;&#30028;&#12290;&#26412;&#25991;&#23558;&#31283;&#23450;&#24615;&#30340;Borkar-Meyn&#23450;&#29702;&#20174;&#38789;&#24046;&#24322;&#22122;&#22768;&#35774;&#23450;&#25299;&#23637;&#21040;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#35774;&#23450;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#20854;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#20855;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#21644;&#36164;&#26684;&#36857;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#30340;&#26680;&#24515;&#22312;&#20110;&#23569;&#25968;&#20989;&#25968;&#30340;&#28176;&#36827;&#21464;&#21270;&#36895;&#29575;&#19979;&#38477;&#65292;&#36825;&#19968;&#28857;&#30001;&#22823;&#25968;&#23450;&#24459;&#21644;&#24120;&#29992;&#30340;V4 Lyapunov&#28418;&#31227;&#26465;&#20214;&#38544;&#21547;&#65292;&#24182;&#22312;&#39532;&#23572;&#21487;&#22827;&#38142;&#26159;&#26377;&#38480;&#19988;&#19981;&#21487;&#32422;&#26102;&#26174;&#28982;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic approximation is a class of algorithms that update a vector iteratively, incrementally, and stochastically, including, e.g., stochastic gradient descent and temporal difference learning. One fundamental challenge in analyzing a stochastic approximation algorithm is to establish its stability, i.e., to show that the stochastic vector iterates are bounded almost surely. In this paper, we extend the celebrated Borkar-Meyn theorem for stability from the Martingale difference noise setting to the Markovian noise setting, which greatly improves its applicability in reinforcement learning, especially in those off-policy reinforcement learning algorithms with linear function approximation and eligibility traces. Central to our analysis is the diminishing asymptotic rate of change of a few functions, which is implied by both a form of strong law of large numbers and a commonly used V4 Lyapunov drift condition and trivially holds if the Markov chain is finite and irreducible.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#22823;&#37327;&#21160;&#20316;&#30340;&#37325;&#22797;&#28216;&#25103;&#20013;&#23454;&#29616;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;Oracle&#30340;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20869;&#37096;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#22823;&#22411;&#28216;&#25103;&#20013;&#35745;&#31639;&#30456;&#20851;&#22343;&#34913;&#30340;&#39640;&#25928;&#24615;&#12290;&#36890;&#36807;&#22312;AI&#23433;&#20840;&#36777;&#35770;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.04792</link><description>&lt;p&gt;
&#20351;&#29992;Oracle&#21644;AI&#36777;&#35770;&#36827;&#34892;&#22823;&#22411;&#28216;&#25103;&#30340;&#29609;&#27861;
&lt;/p&gt;
&lt;p&gt;
Playing Large Games with Oracles and AI Debate
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#22823;&#37327;&#21160;&#20316;&#30340;&#37325;&#22797;&#28216;&#25103;&#20013;&#23454;&#29616;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;Oracle&#30340;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20869;&#37096;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#22823;&#22411;&#28216;&#25103;&#20013;&#35745;&#31639;&#30456;&#20851;&#22343;&#34913;&#30340;&#39640;&#25928;&#24615;&#12290;&#36890;&#36807;&#22312;AI&#23433;&#20840;&#36777;&#35770;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#20855;&#26377;&#22823;&#37327;&#21160;&#20316;&#30340;&#37325;&#22797;&#28216;&#25103;&#20013;&#23454;&#29616;&#36951;&#25022;&#26368;&#23567;&#21270;&#12290;&#36825;&#31181;&#28216;&#25103;&#22312;&#36890;&#36807;&#36777;&#35770;&#30830;&#20445;AI&#23433;&#20840;&#30340;&#29615;&#22659;&#20013;&#26159;&#22266;&#26377;&#30340;&#65292;&#24182;&#19988;&#26356;&#19968;&#33324;&#22320;&#24212;&#29992;&#20110;&#21160;&#20316;&#22522;&#20110;&#35821;&#35328;&#30340;&#28216;&#25103;&#20013;&#12290;&#29616;&#26377;&#30340;&#22312;&#32447;&#28216;&#25103;&#31639;&#27861;&#38656;&#35201;&#22810;&#39033;&#24335;&#35745;&#31639;&#25968;&#37327;&#30340;&#21160;&#20316;&#65292;&#32780;&#23545;&#20110;&#22823;&#22411;&#28216;&#25103;&#26469;&#35828;&#65292;&#36825;&#21487;&#33021;&#26159;&#38590;&#20197;&#23454;&#29616;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#22522;&#20110;Oracle&#30340;&#31639;&#27861;&#65292;&#22240;&#20026;Oracle&#33258;&#28982;&#22320;&#27169;&#25311;&#20102;&#23545;AI&#20195;&#29702;&#30340;&#35775;&#38382;&#12290;&#36890;&#36807;&#23545;Oracle&#35775;&#38382;&#36827;&#34892;&#29305;&#24449;&#21270;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#23454;&#29616;&#20869;&#37096;&#21644;&#22806;&#37096;&#36951;&#25022;&#30340;&#26368;&#23567;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#37096;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#20854;&#36951;&#25022;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#23545;&#25968;&#22320;&#20381;&#36182;&#20110;&#21160;&#20316;&#25968;&#37327;&#12290;&#36825;&#24847;&#21619;&#30528;&#21487;&#20197;&#39640;&#25928;&#22320;&#22522;&#20110;Oracle&#35745;&#31639;&#22823;&#22411;&#28216;&#25103;&#20013;&#30340;&#30456;&#20851;&#22343;&#34913;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;AI&#23433;&#20840;&#36777;&#35770;&#29615;&#22659;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#31639;&#27861;&#20998;&#26512;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider regret minimization in repeated games with a very large number of actions. Such games are inherent in the setting of AI safety via debate, and more generally games whose actions are language-based. Existing algorithms for online game playing require computation polynomial in the number of actions, which can be prohibitive for large games.   We thus consider oracle-based algorithms, as oracles naturally model access to AI agents. With oracle access, we characterize when internal and external regret can be minimized efficiently. We give a novel efficient algorithm for internal regret minimization whose regret and computation complexity depend logarithmically on the number of actions. This implies efficient oracle-based computation of a correlated equilibrium in large games.   We conclude with experiments in the setting of AI Safety via Debate that shows the benefit of insights from our algorithmic analysis.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36136;&#37327;&#24863;&#30693;&#32763;&#35793;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;NMT&#27169;&#22411;&#26469;&#20272;&#35745;&#20854;&#36755;&#20986;&#36136;&#37327;&#65292;&#21487;&#20197;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#28040;&#38500;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2310.06707</link><description>&lt;p&gt;
&#36136;&#37327;&#24863;&#30693;&#32763;&#35793;&#27169;&#22411;&#65306;&#21333;&#19968;&#27169;&#22411;&#20013;&#30340;&#39640;&#25928;&#29983;&#25104;&#21644;&#36136;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Quality-Aware Translation Models: Efficient Generation and Quality Estimation in a Single Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.06707
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36136;&#37327;&#24863;&#30693;&#32763;&#35793;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;NMT&#27169;&#22411;&#26469;&#20272;&#35745;&#20854;&#36755;&#20986;&#36136;&#37327;&#65292;&#21487;&#20197;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#28040;&#38500;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#21518;&#39564;&#65288;MAP&#65289;&#35299;&#30721;&#26159;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#20013;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#35299;&#30721;&#31574;&#30053;&#12290; &#30740;&#31350;&#34920;&#26126;&#65292;&#27169;&#22411;&#27010;&#29575;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#65292;&#20294;&#19981;&#33021;&#24635;&#26159;&#25104;&#31435;&#65292;&#29983;&#25104;&#36136;&#37327;&#21487;&#20197;&#36890;&#36807;&#35299;&#30721;&#26469;&#20248;&#21270;&#19968;&#20010;&#20197;&#24230;&#37327;&#25110;&#36136;&#37327;&#35780;&#20272;&#20449;&#21495;&#25903;&#25345;&#30340;&#25928;&#29992;&#20989;&#25968;&#26469;&#25552;&#39640;&#65292;&#21363;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;MBR&#65289;&#25110;&#36136;&#37327;&#24863;&#30693;&#35299;&#30721;&#12290; &#36825;&#20123;&#26041;&#27861;&#30340;&#20027;&#35201;&#32570;&#28857;&#22312;&#20110;&#23427;&#20204;&#38656;&#35201;&#19968;&#20010;&#39069;&#22806;&#30340;&#27169;&#22411;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#35745;&#31639;&#25928;&#29992;&#20989;&#25968;&#65292;&#20250;&#26174;&#33879;&#22686;&#21152;&#35745;&#31639;&#25104;&#26412;&#12290; &#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#35757;&#32451;NMT&#27169;&#22411;&#33258;&#24049;&#26469;&#20272;&#35745;&#20854;&#36755;&#20986;&#36136;&#37327;&#65292;&#20174;&#32780;&#20351;NMT&#27169;&#22411;&#26412;&#36523;&#20855;&#22791;&#36136;&#37327;&#24863;&#30693;&#33021;&#21147;&#12290; &#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#36827;&#34892;MBR&#35299;&#30721;&#21487;&#20197;&#26174;&#33879;&#20943;&#23567;&#23610;&#23544;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.06707v2 Announce Type: replace-cross  Abstract: Maximum-a-posteriori (MAP) decoding is the most widely used decoding strategy for neural machine translation (NMT) models. The underlying assumption is that model probability correlates well with human judgment, with better translations getting assigned a higher score by the model. However, research has shown that this assumption does not always hold, and generation quality can be improved by decoding to optimize a utility function backed by a metric or quality-estimation signal, as is done by Minimum Bayes Risk (MBR) or Quality-Aware decoding. The main disadvantage of these approaches is that they require an additional model to calculate the utility function during decoding, significantly increasing the computational cost. In this paper, we propose to make the NMT models themselves quality-aware by training them to estimate the quality of their own output. Using this approach for MBR decoding we can drastically reduce the size
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32771;&#34385;&#33322;&#36816;&#36890;&#37327;&#23494;&#24230;&#12289;&#28207;&#21475;&#36317;&#31163;&#12289;&#36152;&#26131;&#27969;&#37327;&#21644;&#20132;&#36890;&#26530;&#32445;&#30340;&#20013;&#24515;&#24615;&#25351;&#26631;&#31561;&#22240;&#32032;&#65292;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#28023;&#20107;&#33322;&#36816;&#27969;&#37327;&#65292;&#24182;&#29992;&#20110;&#25351;&#23548;&#20840;&#29699;&#20132;&#36890;&#32593;&#32476;&#20013;&#20837;&#20405;&#29289;&#31181;&#30340;&#39118;&#38505;&#35780;&#20272;&#21644;&#31649;&#29702;&#12290;</title><link>http://arxiv.org/abs/2401.13098</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#21147;&#20449;&#24687;&#39537;&#21160;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#39044;&#27979;&#38750;&#26412;&#22320;&#29289;&#31181;&#33337;&#33334;&#20132;&#36890;&#27969;&#37327;&#21644;&#20837;&#20405;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Gravity-Informed Deep Learning Framework for Predicting Ship Traffic Flow and Invasion Risk of Non-Indigenous Species via Ballast Water Discharge. (arXiv:2401.13098v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13098
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32771;&#34385;&#33322;&#36816;&#36890;&#37327;&#23494;&#24230;&#12289;&#28207;&#21475;&#36317;&#31163;&#12289;&#36152;&#26131;&#27969;&#37327;&#21644;&#20132;&#36890;&#26530;&#32445;&#30340;&#20013;&#24515;&#24615;&#25351;&#26631;&#31561;&#22240;&#32032;&#65292;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#28023;&#20107;&#33322;&#36816;&#27969;&#37327;&#65292;&#24182;&#29992;&#20110;&#25351;&#23548;&#20840;&#29699;&#20132;&#36890;&#32593;&#32476;&#20013;&#20837;&#20405;&#29289;&#31181;&#30340;&#39118;&#38505;&#35780;&#20272;&#21644;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#20307;&#20013;&#30340;&#20837;&#20405;&#29289;&#31181;&#23545;&#20840;&#29699;&#29615;&#22659;&#21644;&#29983;&#29289;&#22810;&#26679;&#24615;&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#12290;&#30001;&#20110;&#20132;&#36890;&#21644;&#36152;&#26131;&#22686;&#21152;&#65292;&#38750;&#26412;&#22303;&#29289;&#31181;&#24050;&#32463;&#24341;&#20837;&#20102;&#26032;&#30340;&#29615;&#22659;&#65292;&#23548;&#33268;&#29983;&#24577;&#31995;&#32479;&#30772;&#22351;&#65292;&#24182;&#23548;&#33268;&#20892;&#19994;&#12289;&#26519;&#19994;&#21644;&#28180;&#19994;&#26041;&#38754;&#30340;&#32463;&#27982;&#25439;&#22833;&#12290;&#22240;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#39118;&#38505;&#35780;&#20272;&#21644;&#31649;&#29702;&#25216;&#26415;&#20197;&#20943;&#36731;&#36825;&#20123;&#20837;&#20405;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#26032;&#30340;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#28023;&#20107;&#33322;&#36816;&#20132;&#36890;&#27969;&#37327;&#65292;&#24182;&#20197;&#27492;&#25351;&#23548;&#36890;&#36807;&#20840;&#29699;&#20132;&#36890;&#32593;&#32476;&#20256;&#25773;&#30340;&#20837;&#20405;&#29289;&#31181;&#39118;&#38505;&#35780;&#20272;&#12290;&#21463;&#22269;&#38469;&#36152;&#26131;&#37325;&#21147;&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#32771;&#34385;&#20102;&#24433;&#21709;&#33337;&#33334;&#27963;&#21160;&#21487;&#33021;&#24615;&#21644;&#24433;&#21709;&#30340;&#21508;&#31181;&#22240;&#32032;&#65292;&#22914;&#33322;&#36816;&#36890;&#37327;&#23494;&#24230;&#12289;&#28207;&#21475;&#20043;&#38388;&#30340;&#36317;&#31163;&#12289;&#36152;&#26131;&#27969;&#37327;&#21644;&#20132;&#36890;&#26530;&#32445;&#30340;&#20013;&#24515;&#24615;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20998;&#26512;&#20837;&#20405;&#29289;&#31181;&#30340;&#39118;&#38505;&#32593;&#32476;&#65292;&#25105;&#20204;&#20026;&#35780;&#20272;&#21644;&#31649;&#29702;&#20837;&#20405;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Invasive species in water bodies pose a major threat to the environment and biodiversity globally. Due to increased transportation and trade, non-native species have been introduced to new environments, causing damage to ecosystems and leading to economic losses in agriculture, forestry, and fisheries. Therefore, there is a pressing need for risk assessment and management techniques to mitigate the impact of these invasions. This study aims to develop a new physics-inspired model to forecast maritime shipping traffic and thus inform risk assessment of invasive species spread through global transportation networks. Inspired by the gravity model for international trades, our model considers various factors that influence the likelihood and impact of vessel activities, such as shipping flux density, distance between ports, trade flow, and centrality measures of transportation hubs. Additionally, by analyzing the risk network of invasive species, we provide a comprehensive framework for as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32570;&#22833;&#27169;&#24577;&#19979;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#12290;&#36890;&#36807;&#32763;&#35793;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20869;&#23481;&#20197;&#37325;&#26500;&#32570;&#22833;&#30340;&#38899;&#39057;&#27169;&#24577;&#65292;&#24182;&#21033;&#29992;&#36328;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#24773;&#24863;&#39044;&#27979;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#21644;&#19982;&#23436;&#25972;&#22810;&#27169;&#24577;&#30417;&#30563;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.10747</link><description>&lt;p&gt;
&#32570;&#22833;&#27169;&#24577;&#19979;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;:&#19968;&#31181;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multimodal Sentiment Analysis with Missing Modality: A Knowledge-Transfer Approach. (arXiv:2401.10747v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32570;&#22833;&#27169;&#24577;&#19979;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#12290;&#36890;&#36807;&#32763;&#35793;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20869;&#23481;&#20197;&#37325;&#26500;&#32570;&#22833;&#30340;&#38899;&#39057;&#27169;&#24577;&#65292;&#24182;&#21033;&#29992;&#36328;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#24773;&#24863;&#39044;&#27979;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#21644;&#19982;&#23436;&#25972;&#22810;&#27169;&#24577;&#30417;&#30563;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#26088;&#22312;&#36890;&#36807;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#22768;&#38899;&#32447;&#32034;&#26469;&#35782;&#21035;&#20010;&#20307;&#34920;&#36798;&#30340;&#24773;&#32490;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#22823;&#22810;&#20551;&#35774;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#36807;&#31243;&#20013;&#25152;&#26377;&#27169;&#24577;&#37117;&#26159;&#21487;&#29992;&#30340;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#30340;&#31639;&#27861;&#23481;&#26131;&#21463;&#21040;&#32570;&#22833;&#27169;&#24577;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#36801;&#31227;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#65292;&#20197;&#37325;&#26500;&#32570;&#22833;&#30340;&#38899;&#39057;&#27169;&#24577;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#20445;&#30041;&#37325;&#26500;&#21644;&#35266;&#23519;&#21040;&#30340;&#27169;&#24577;&#30340;&#26368;&#22823;&#20449;&#24687;&#65292;&#29992;&#20110;&#24773;&#24863;&#39044;&#27979;&#12290;&#22312;&#19977;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#30456;&#23545;&#20110;&#22522;&#32447;&#31639;&#27861;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#23454;&#29616;&#20102;&#19982;&#20855;&#26377;&#23436;&#25972;&#22810;&#27169;&#24577;&#30417;&#30563;&#30340;&#20808;&#21069;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal sentiment analysis aims to identify the emotions expressed by individuals through visual, language, and acoustic cues. However, most of the existing research efforts assume that all modalities are available during both training and testing, making their algorithms susceptible to the missing modality scenario. In this paper, we propose a novel knowledge-transfer network to translate between different modalities to reconstruct the missing audio modalities. Moreover, we develop a cross-modality attention mechanism to retain the maximal information of the reconstructed and observed modalities for sentiment prediction. Extensive experiments on three publicly available datasets demonstrate significant improvements over baselines and achieve comparable results to the previous methods with complete multi-modality supervision.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#39064;&#19987;&#29992;&#36866;&#37197;&#22120;&#30340;&#26102;&#38388;-&#39057;&#35889;&#34701;&#21512;Transformer (TSformer-SA) &#29992;&#20110;&#22686;&#24378;RSVP-BCI&#35299;&#30721;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#22810;&#35270;&#22270;&#20449;&#24687;&#24182;&#20943;&#23569;&#20934;&#22791;&#26102;&#38388;&#65292;&#23454;&#29616;&#20102;&#35299;&#30721;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.06340</link><description>&lt;p&gt;
&#22522;&#20110;&#20027;&#39064;&#19987;&#29992;&#36866;&#37197;&#22120;&#30340;&#26102;&#38388;-&#39057;&#35889;&#34701;&#21512;Transformer&#29992;&#20110;&#22686;&#24378;RSVP-BCI&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
A Temporal-Spectral Fusion Transformer with Subject-specific Adapter for Enhancing RSVP-BCI Decoding. (arXiv:2401.06340v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#39064;&#19987;&#29992;&#36866;&#37197;&#22120;&#30340;&#26102;&#38388;-&#39057;&#35889;&#34701;&#21512;Transformer (TSformer-SA) &#29992;&#20110;&#22686;&#24378;RSVP-BCI&#35299;&#30721;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#22810;&#35270;&#22270;&#20449;&#24687;&#24182;&#20943;&#23569;&#20934;&#22791;&#26102;&#38388;&#65292;&#23454;&#29616;&#20102;&#35299;&#30721;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#20018;&#32852;&#35270;&#35273;&#21576;&#29616;&#65288;RSVP&#65289;&#22522;&#20110;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#26159;&#19968;&#31181;&#21033;&#29992;&#33041;&#30005;&#20449;&#21495;&#36827;&#34892;&#30446;&#26631;&#26816;&#32034;&#30340;&#39640;&#25928;&#25216;&#26415;&#12290;&#20256;&#32479;&#35299;&#30721;&#26041;&#27861;&#30340;&#24615;&#33021;&#25913;&#36827;&#20381;&#36182;&#20110;&#22823;&#37327;&#26469;&#33258;&#26032;&#27979;&#35797;&#23545;&#35937;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#22686;&#21152;&#20102;BCI&#31995;&#32479;&#30340;&#20934;&#22791;&#26102;&#38388;&#12290;&#19968;&#20123;&#30740;&#31350;&#24341;&#20837;&#20102;&#26469;&#33258;&#29616;&#26377;&#23545;&#35937;&#30340;&#25968;&#25454;&#20197;&#20943;&#23569;&#24615;&#33021;&#25913;&#36827;&#23545;&#26032;&#23545;&#35937;&#25968;&#25454;&#30340;&#20381;&#36182;&#24615;&#65292;&#20294;&#23427;&#20204;&#22522;&#20110;&#23545;&#25239;&#23398;&#20064;&#30340;&#20248;&#21270;&#31574;&#30053;&#20197;&#21450;&#22823;&#37327;&#25968;&#25454;&#30340;&#35757;&#32451;&#22686;&#21152;&#20102;&#20934;&#22791;&#36807;&#31243;&#20013;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#26041;&#27861;&#21482;&#20851;&#27880;&#33041;&#30005;&#20449;&#21495;&#30340;&#21333;&#35270;&#22270;&#20449;&#24687;&#65292;&#32780;&#24573;&#30053;&#20102;&#20854;&#20182;&#35270;&#22270;&#30340;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#36827;&#19968;&#27493;&#25913;&#21892;&#24615;&#33021;&#12290;&#20026;&#20102;&#22312;&#20943;&#23569;&#20934;&#22791;&#26102;&#38388;&#30340;&#21516;&#26102;&#25552;&#39640;&#35299;&#30721;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20027;&#39064;&#19987;&#29992;&#36866;&#37197;&#22120;&#30340;&#26102;&#38388;-&#39057;&#35889;&#34701;&#21512;Transformer&#65288;TSformer-SA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Rapid Serial Visual Presentation (RSVP)-based Brain-Computer Interface (BCI) is an efficient technology for target retrieval using electroencephalography (EEG) signals. The performance improvement of traditional decoding methods relies on a substantial amount of training data from new test subjects, which increases preparation time for BCI systems. Several studies introduce data from existing subjects to reduce the dependence of performance improvement on data from new subjects, but their optimization strategy based on adversarial learning with extensive data increases training time during the preparation procedure. Moreover, most previous methods only focus on the single-view information of EEG signals, but ignore the information from other views which may further improve performance. To enhance decoding performance while reducing preparation time, we propose a Temporal-Spectral fusion transformer with Subject-specific Adapter (TSformer-SA). Specifically, a cross-view interaction 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Self-Extend&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36523;&#25193;&#23637;&#29616;&#26377;LLMs&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#26080;&#38656;&#35843;&#25972;&#65292;&#20805;&#20998;&#21033;&#29992;LLMs&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#30340;&#22266;&#26377;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.01325</link><description>&lt;p&gt;
&#33258;&#25193;&#23637;LLM:&#26080;&#38656;&#35843;&#25972;&#30340;LLM&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning. (arXiv:2401.01325v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Self-Extend&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36523;&#25193;&#23637;&#29616;&#26377;LLMs&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#26080;&#38656;&#35843;&#25972;&#65292;&#20805;&#20998;&#21033;&#29992;LLMs&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#30340;&#22266;&#26377;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;LLM&#22312;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#26102;&#30340;&#22266;&#26377;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#31934;&#35843;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#35757;&#32451;&#24207;&#21015;&#30340;&#26377;&#38480;&#38271;&#24230;&#21487;&#33021;&#38480;&#21046;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23545;&#38271;&#36755;&#20837;&#24207;&#21015;&#30340;&#24212;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#29616;&#26377;&#30340;LLMs&#26412;&#36523;&#20855;&#26377;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#30340;&#22266;&#26377;&#33021;&#21147;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#33258;&#36523;&#25193;&#23637;LLMs&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#20854;&#22266;&#26377;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Self-Extend&#26041;&#27861;&#26469;&#28608;&#21457;LLMs&#30340;&#38271;&#19978;&#19979;&#25991;&#22788;&#29702;&#28508;&#21147;&#12290;&#22522;&#26412;&#24605;&#24819;&#26159;&#26500;&#24314;&#21452;&#23618;&#27880;&#24847;&#20449;&#24687;&#65306;&#32676;&#32452;&#32423;&#21644;&#37051;&#23621;&#32423;&#12290;&#36825;&#20004;&#20010;&#32423;&#21035;&#36890;&#36807;&#21407;&#22987;&#27169;&#22411;&#30340;&#33258;&#27880;&#24847;&#21147;&#35745;&#31639;&#65292;&#36825;&#24847;&#21619;&#30528;&#25152;&#25552;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#35757;&#32451;&#12290;&#21482;&#38656;&#20462;&#25913;&#22235;&#34892;&#20195;&#30721;&#65292;&#25152;&#25552;&#26041;&#27861;&#23601;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#29616;&#26377;LLMs&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#31934;&#35843;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#26041;&#27861;&#21487;&#20197;+&#25688;&#35201;&#20943;&#25481;&#25991;&#31456;&#26368;&#21518;&#19968;&#21477;&#35441;
&lt;/p&gt;
&lt;p&gt;
This work elicits LLMs' inherent ability to handle long contexts without fine-tuning. The limited length of the training sequence during training may limit the application of Large Language Models (LLMs) on long input sequences for inference. In this work, we argue that existing LLMs themselves have inherent capabilities for handling long contexts. Based on this argument, we suggest extending LLMs' context window by themselves to fully utilize the inherent ability.We propose Self-Extend to stimulate LLMs' long context handling potential. The basic idea is to construct bi-level attention information: the group level and the neighbor level. The two levels are computed by the original model's self-attention, which means the proposed does not require any training. With only four lines of code modification, the proposed method can effortlessly extend existing LLMs' context window without any fine-tuning. We conduct comprehensive experiments and the results show that the proposed method can 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35299;&#37322;&#20854;&#20182;&#39044;&#27979;&#27169;&#22411;&#26377;&#25928;&#24615;&#30340;&#26694;&#26550;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#22810;&#20010;&#25552;&#31034;&#31574;&#30053;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;&#23545;&#20110;LLMs&#22312;&#35299;&#37322;&#20854;&#20182;&#27169;&#22411;&#34892;&#20026;&#26041;&#38754;&#30340;&#32570;&#22833;&#12290;</title><link>http://arxiv.org/abs/2310.05797</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#20107;&#21518;&#35299;&#37322;&#22120;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Models Post Hoc Explainers?. (arXiv:2310.05797v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05797
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35299;&#37322;&#20854;&#20182;&#39044;&#27979;&#27169;&#22411;&#26377;&#25928;&#24615;&#30340;&#26694;&#26550;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#22810;&#20010;&#25552;&#31034;&#31574;&#30053;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;&#23545;&#20110;LLMs&#22312;&#35299;&#37322;&#20854;&#20182;&#27169;&#22411;&#34892;&#20026;&#26041;&#38754;&#30340;&#32570;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36234;&#26469;&#36234;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24212;&#29992;&#20013;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#21019;&#26032;&#65292;&#21363;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#20351;&#24471;LLM&#33021;&#22815;&#22312;&#25512;&#29702;&#38454;&#27573;&#36890;&#36807;&#22312;&#25552;&#31034;&#20013;&#25552;&#20379;&#23569;&#37327;&#31034;&#20363;&#26469;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#27169;&#22411;&#24494;&#35843;&#30340;&#38656;&#35201;&#12290;&#34429;&#28982;LLM&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#65292;&#20294;&#20854;&#22312;&#35299;&#37322;&#20854;&#20182;&#27169;&#22411;&#34892;&#20026;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#20173;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#23613;&#31649;&#23384;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#26032;&#35299;&#37322;&#25216;&#26415;&#65292;&#20294;&#24456;&#22810;&#25216;&#26415;&#35201;&#27714;&#23545;&#27169;&#22411;&#20855;&#26377;&#30333;&#30418;&#35775;&#38382;&#26435;&#38480;&#21644;/&#25110;&#35745;&#31639;&#25104;&#26412;&#36739;&#39640;&#65292;&#20984;&#26174;&#20102;&#19979;&#19968;&#20195;&#20107;&#21518;&#35299;&#37322;&#22120;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#30740;&#31350;LLM&#35299;&#37322;&#20854;&#20182;&#39044;&#27979;&#27169;&#22411;&#26377;&#25928;&#24615;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#22810;&#31181;&#25552;&#31034;&#31574;&#30053;&#30340;&#26032;&#39062;&#26694;&#26550;&#65306;i&#65289;&#22522;&#20110;&#25200;&#21160;&#30340;ICL&#65292;ii&#65289;&#22522;&#20110;&#39044;&#27979;&#30340;ICL&#65292;iii&#65289;&#22522;&#20110;&#25351;&#20196;&#30340;ICL&#65292;&#21644;iv&#65289;&#22522;&#20110;&#35299;&#37322;&#30340;ICL&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are increasingly used as powerful tools for a plethora of natural language processing (NLP) applications. A recent innovation, in-context learning (ICL), enables LLMs to learn new tasks by supplying a few examples in the prompt during inference time, thereby eliminating the need for model fine-tuning. While LLMs have been utilized in several applications, their applicability in explaining the behavior of other models remains relatively unexplored. Despite the growing number of new explanation techniques, many require white-box access to the model and/or are computationally expensive, highlighting a need for next-generation post hoc explainers. In this work, we present the first framework to study the effectiveness of LLMs in explaining other predictive models. More specifically, we propose a novel framework encompassing multiple prompting strategies: i) Perturbation-based ICL, ii) Prediction-based ICL, iii) Instruction-based ICL, and iv) Explanation-based I
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;CLIP&#20013;&#30340;&#21487;&#36716;&#31227;&#34920;&#31034;&#23398;&#20064;&#21644;&#38646;&#26679;&#26412;&#20256;&#36882;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;CLIP&#31867;&#22411;&#26041;&#27861;&#65292;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.00927</link><description>&lt;p&gt;
&#29702;&#35299;CLIP&#20013;&#30340;&#21487;&#36716;&#31227;&#34920;&#31034;&#23398;&#20064;&#21644;&#38646;&#26679;&#26412;&#20256;&#36882;
&lt;/p&gt;
&lt;p&gt;
Understanding Transferable Representation Learning and Zero-shot Transfer in CLIP. (arXiv:2310.00927v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;CLIP&#20013;&#30340;&#21487;&#36716;&#31227;&#34920;&#31034;&#23398;&#20064;&#21644;&#38646;&#26679;&#26412;&#20256;&#36882;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;CLIP&#31867;&#22411;&#26041;&#27861;&#65292;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#22240;&#20854;&#33021;&#22815;&#21033;&#29992;&#19981;&#21516;&#25968;&#25454;&#28304;&#65288;&#20363;&#22914;&#25991;&#26412;&#21644;&#22270;&#20687;&#65289;&#30340;&#20449;&#24687;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#32780;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#12290;&#36817;&#24180;&#26469;&#65292;CLIP&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#35270;&#35273;-&#35821;&#35328;&#23545;&#27604;&#39044;&#35757;&#32451;&#26469;&#23398;&#20064;&#32852;&#21512;&#22270;&#20687;&#21644;&#25991;&#26412;&#34920;&#31034;&#65292;&#24182;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#25991;&#26412;&#24341;&#23548;&#30340;&#33258;&#28982;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#38750;&#20961;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;CLIP&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#20854;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27491;&#24335;&#30740;&#31350;&#20102;CLIP&#20013;&#30340;&#21487;&#36716;&#31227;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#23637;&#31034;&#20102;&#19981;&#21516;&#27169;&#24577;&#30340;&#29305;&#24449;&#22914;&#20309;&#23545;&#40784;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#38646;&#26679;&#26412;&#20256;&#36882;&#24615;&#33021;&#12290;&#21463;&#21040;&#25105;&#20204;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CLIP&#31867;&#22411;&#26041;&#27861;&#65292;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#27604;CLIP&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal learning has become increasingly popular due to its ability to leverage information from different data sources (e.g., text and images) to improve the model performance. Recently, CLIP has emerged as an effective approach that employs vision-language contrastive pretraining to learn joint image and text representations and exhibits remarkable performance in zero-shot learning and text-guided natural image generation. Despite the huge practical success of CLIP, its theoretical understanding remains elusive. In this paper, we formally study transferrable representation learning underlying CLIP and demonstrate how features from different modalities get aligned. We also analyze its zero-shot transfer performance on the downstream tasks. Inspired by our analysis, we propose a new CLIP-type approach, which achieves better performance than CLIP and other state-of-the-art methods on benchmark datasets.
&lt;/p&gt;</description></item><item><title>SeaEval&#26159;&#19968;&#20010;&#35780;&#20272;&#22810;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#30740;&#31350;&#20102;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#20197;&#21450;&#23545;&#25991;&#21270;&#23454;&#36341;&#12289;&#32454;&#24494;&#24046;&#21035;&#21644;&#20215;&#20540;&#35266;&#30340;&#29702;&#35299;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;&#37325;&#35201;&#21457;&#29616;&#21253;&#25324;&#27169;&#22411;&#22312;&#32473;&#20986;&#25913;&#20889;&#25351;&#20196;&#26102;&#34892;&#20026;&#21508;&#24322;&#65292;&#21463;&#21040;&#26292;&#38706;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#23545;&#20110;&#35821;&#20041;&#31561;&#20215;&#30340;&#22810;&#35821;&#35328;&#26597;&#35810;&#30340;&#22238;&#31572;&#19981;&#19968;&#33268;&#65292;&#20197;&#21450;&#27169;&#22411;&#22312;&#24773;&#24863;&#30456;&#20851;&#38382;&#39064;&#19978;&#30340;&#19968;&#33268;&#24615;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2309.04766</link><description>&lt;p&gt;
SeaEval&#22810;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65306;&#20174;&#36328;&#35821;&#35328;&#23545;&#40784;&#21040;&#25991;&#21270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SeaEval for Multilingual Foundation Models: From Cross-Lingual Alignment to Cultural Reasoning. (arXiv:2309.04766v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04766
&lt;/p&gt;
&lt;p&gt;
SeaEval&#26159;&#19968;&#20010;&#35780;&#20272;&#22810;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#30740;&#31350;&#20102;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#20197;&#21450;&#23545;&#25991;&#21270;&#23454;&#36341;&#12289;&#32454;&#24494;&#24046;&#21035;&#21644;&#20215;&#20540;&#35266;&#30340;&#29702;&#35299;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;&#37325;&#35201;&#21457;&#29616;&#21253;&#25324;&#27169;&#22411;&#22312;&#32473;&#20986;&#25913;&#20889;&#25351;&#20196;&#26102;&#34892;&#20026;&#21508;&#24322;&#65292;&#21463;&#21040;&#26292;&#38706;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#23545;&#20110;&#35821;&#20041;&#31561;&#20215;&#30340;&#22810;&#35821;&#35328;&#26597;&#35810;&#30340;&#22238;&#31572;&#19981;&#19968;&#33268;&#65292;&#20197;&#21450;&#27169;&#22411;&#22312;&#24773;&#24863;&#30456;&#20851;&#38382;&#39064;&#19978;&#30340;&#19968;&#33268;&#24615;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;SeaEval&#22522;&#20934;&#27979;&#35797;&#12290;&#38500;&#20102;&#34920;&#24449;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#29702;&#35299;&#21644;&#25512;&#29702;&#33258;&#28982;&#35821;&#35328;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#23427;&#20204;&#23545;&#25991;&#21270;&#23454;&#36341;&#12289;&#32454;&#24494;&#24046;&#21035;&#21644;&#20215;&#20540;&#35266;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#38500;&#20102;&#26631;&#20934;&#30340;&#20934;&#30830;&#24230;&#25351;&#26631;&#65292;&#25105;&#20204;&#36824;&#35843;&#26597;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#35821;&#20041;&#21644;&#22810;&#35821;&#35328;&#24615;&#32500;&#24230;&#19978;&#30340;&#33030;&#24369;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;&#24320;&#28304;&#21644;&#38381;&#28304;&#27169;&#22411;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#22312;&#32463;&#20856;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12289;&#25512;&#29702;&#21644;&#25991;&#21270;&#29702;&#35299;&#26041;&#38754;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;&#37325;&#35201;&#21457;&#29616;&#21253;&#25324;&#65306;&#65288;1&#65289;&#22823;&#22810;&#25968;&#27169;&#22411;&#22312;&#32473;&#20986;&#25913;&#20889;&#25351;&#20196;&#26102;&#30340;&#34892;&#20026;&#21508;&#24322;&#65307;&#65288;2&#65289;&#35768;&#22810;&#27169;&#22411;&#20173;&#28982;&#21463;&#21040;&#26292;&#38706;&#20559;&#24046;&#30340;&#24433;&#21709;&#65288;&#22914;&#20301;&#32622;&#20559;&#24046;&#12289;&#22823;&#22810;&#25968;&#26631;&#31614;&#20559;&#24046;&#65289;&#65307;&#65288;3&#65289;&#23545;&#20110;&#26681;&#28304;&#20110;&#20107;&#23454;&#12289;&#31185;&#23398;&#21644;&#24120;&#35782;&#30693;&#35782;&#30340;&#38382;&#39064;&#65292;&#39044;&#26399;&#22312;&#35821;&#20041;&#19978;&#31561;&#20215;&#30340;&#22810;&#35821;&#35328;&#26597;&#35810;&#24212;&#35813;&#24471;&#21040;&#19968;&#33268;&#30340;&#22238;&#31572;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#22312;&#36825;&#20123;&#26597;&#35810;&#19978;&#34920;&#29616;&#20986;&#20196;&#20154;&#24847;&#22806;&#30340;&#19981;&#19968;&#33268;&#24615;&#65307;&#65288;4&#65289;&#22810;&#35821;&#35328;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#23545;&#20110;&#24773;&#24863;&#30456;&#20851;&#30340;&#38382;&#39064;&#34920;&#29616;&#20986;&#19981;&#21516;&#31243;&#24230;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SeaEval, a benchmark for multilingual foundation models. In addition to characterizing how these models understand and reason with natural language, we also investigate how well they comprehend cultural practices, nuances, and values. Alongside standard accuracy metrics, we investigate the brittleness of foundation models in the dimensions of semantics and multilinguality. Our analyses span both open-sourced and closed models, leading to empirical results across classic NLP tasks, reasoning, and cultural comprehension. Key findings indicate (1) Most models exhibit varied behavior when given paraphrased instructions. (2) Many models still suffer from exposure bias (e.g., positional bias, majority label bias). (3) For questions rooted in factual, scientific, and commonsense knowledge, consistent responses are expected across multilingual queries that are semantically equivalent. Yet, most models surprisingly demonstrate inconsistent performance on these queries. (4) Multilingu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#31639;&#27861;&#20197;&#21450;&#21033;&#29992;&#23884;&#20837;&#27169;&#22411;&#25429;&#25417;&#30693;&#35782;&#22270;&#35889;&#20013;&#35821;&#20041;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#30693;&#35782;&#22270;&#35889;&#21644;&#35821;&#35328;&#27169;&#22411;&#30456;&#20114;&#21463;&#30410;&#30340;&#35266;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.00081</link><description>&lt;p&gt;
&#20026;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26500;&#24314;&#35821;&#20041;&#20016;&#23500;&#30340;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Semantically Enriched Embeddings for Knowledge Graph Completion. (arXiv:2308.00081v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#31639;&#27861;&#20197;&#21450;&#21033;&#29992;&#23884;&#20837;&#27169;&#22411;&#25429;&#25417;&#30693;&#35782;&#22270;&#35889;&#20013;&#35821;&#20041;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#30693;&#35782;&#22270;&#35889;&#21644;&#35821;&#35328;&#27169;&#22411;&#30456;&#20114;&#21463;&#30410;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23884;&#20837;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#30446;&#21069;&#30340;&#22823;&#22810;&#25968;&#31639;&#27861;&#23558;&#30693;&#35782;&#22270;&#35889;&#35270;&#20026;&#19968;&#20010;&#22810;&#21521;&#26631;&#35760;&#22270;&#65292;&#32570;&#20047;&#25429;&#25417;&#24213;&#23618;&#35821;&#20041;&#30340;&#33021;&#21147;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25429;&#33719;&#20102;&#22823;&#37327;&#20449;&#24687;&#65292;&#36825;&#19968;&#25429;&#33719;&#23545;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20135;&#29983;&#20102;&#38761;&#21629;&#24615;&#24433;&#21709;&#12290;&#30693;&#35782;&#22270;&#35889;&#21487;&#20197;&#20174;LLMs&#20013;&#21463;&#30410;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#22522;&#20110;&#19981;&#21516;&#29983;&#25104;&#23884;&#20837;&#27169;&#22411;&#21464;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#31639;&#27861;&#12290;&#39318;&#20808;&#35752;&#35770;&#20102;&#21508;&#31181;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#31639;&#27861;&#65292;&#22914;&#36716;&#23548;&#21644;&#24402;&#32435;&#38142;&#25509;&#39044;&#27979;&#20197;&#21450;&#23454;&#20307;&#31867;&#22411;&#39044;&#27979;&#31639;&#27861;&#12290;&#28982;&#21518;&#65292;&#20171;&#32461;&#20102;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#31867;&#22411;&#20449;&#24687;&#12289;LLMs&#20197;&#21450;&#25429;&#25417;&#19981;&#21516;&#25551;&#36848;&#36923;&#36753;&#20844;&#29702;&#20013;&#30340;&#35821;&#20041;&#30340;&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#23545;&#29616;&#26377;&#31639;&#27861;&#30340;&#20851;&#38190;&#21453;&#24605;&#23545;&#35770;&#25991;&#36827;&#34892;&#24635;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embedding based Knowledge Graph (KG) Completion has gained much attention over the past few years. Most of the current algorithms consider a KG as a multidirectional labeled graph and lack the ability to capture the semantics underlying the schematic information. In a separate development, a vast amount of information has been captured within the Large Language Models (LLMs) which has revolutionized the field of Artificial Intelligence. KGs could benefit from these LLMs and vice versa. This vision paper discusses the existing algorithms for KG completion based on the variations for generating KG embeddings. It starts with discussing various KG completion algorithms such as transductive and inductive link prediction and entity type prediction algorithms. It then moves on to the algorithms utilizing type information within the KGs, LLMs, and finally to algorithms capturing the semantics represented in different description logic axioms. We conclude the paper with a critical reflection on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#20280;&#32553;&#30690;&#37327;&#22270;(SVG)&#26684;&#24335;&#22788;&#29702;&#22270;&#20687;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#30452;&#25509;&#29702;&#35299;&#21644;&#25805;&#20316;&#22270;&#20687;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#21442;&#25968;&#21270;&#30340;&#35270;&#35273;&#32452;&#20214;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#29983;&#25104;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;&#20219;&#21153;&#19978;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.06094</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21487;&#20280;&#32553;&#30690;&#37327;&#22270;&#39537;&#21160;&#30340;&#22270;&#20687;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models for Scalable Vector Graphics-Driven Image Understanding. (arXiv:2306.06094v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#20280;&#32553;&#30690;&#37327;&#22270;(SVG)&#26684;&#24335;&#22788;&#29702;&#22270;&#20687;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#30452;&#25509;&#29702;&#35299;&#21644;&#25805;&#20316;&#22270;&#20687;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#21442;&#25968;&#21270;&#30340;&#35270;&#35273;&#32452;&#20214;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#29983;&#25104;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;&#20219;&#21153;&#19978;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#28508;&#21147;&#20173;&#28982;&#24456;&#22823;&#31243;&#24230;&#19978;&#27809;&#26377;&#34987;&#24320;&#21457;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25506;&#32034;&#24615;&#26041;&#27861;&#65292;&#20351;&#24471;LLMs&#33021;&#22815;&#20351;&#29992;&#21487;&#20280;&#32553;&#30690;&#37327;&#22270;(SVG)&#26684;&#24335;&#22788;&#29702;&#22270;&#20687;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;XML&#30340;SVG&#34920;&#36848;&#30340;&#25991;&#26412;&#25551;&#36848;&#32780;&#19981;&#26159;&#20809;&#26629;&#22270;&#20687;&#65292;&#25105;&#20204;&#26088;&#22312;&#24357;&#21512;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20351;LLMs&#33021;&#22815;&#30452;&#25509;&#29702;&#35299;&#21644;&#25805;&#20316;&#22270;&#20687;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#21442;&#25968;&#21270;&#30340;&#35270;&#35273;&#32452;&#20214;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#21033;&#29992;LLMs&#30340;&#33021;&#21147;&#36827;&#34892;&#31616;&#21333;&#30340;&#22270;&#20687;&#20998;&#31867;&#12289;&#29983;&#25104;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26041;&#27861;&#22312;&#21028;&#21035;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#20248;&#24322;&#34920;&#29616;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#23427;(i)&#23545;&#20998;&#24067;&#36716;&#31227;&#30340;&#40065;&#26834;&#24615;&#65292;(ii)&#36890;&#36807;&#21033;&#29992;LLMs&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#23454;&#29616;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#20197;&#21450;(iii)&#22270;&#20687;&#26434;&#20081;&#31243;&#24230;&#19978;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large language models (LLMs) have made significant advancements in natural language understanding and generation. However, their potential in computer vision remains largely unexplored. In this paper, we introduce a new, exploratory approach that enables LLMs to process images using the Scalable Vector Graphics (SVG) format. By leveraging the XML-based textual descriptions of SVG representations instead of raster images, we aim to bridge the gap between the visual and textual modalities, allowing LLMs to directly understand and manipulate images without the need for parameterized visual components. Our method facilitates simple image classification, generation, and in-context learning using only LLM capabilities. We demonstrate the promise of our approach across discriminative and generative tasks, highlighting its (i) robustness against distribution shift, (ii) substantial improvements achieved by tapping into the in-context learning abilities of LLMs, and (iii) image unders
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#23545;&#35805;&#24335;&#30340;&#35299;&#37322;&#27169;&#22411;&#65292;&#24357;&#21512;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#29702;&#35299;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20197;&#21019;&#24314;&#26356;&#26377;&#25928;&#21644;&#24191;&#27867;&#24212;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2302.03460</link><description>&lt;p&gt;
&#27880;&#24847;&#30041;&#19979;&#31354;&#38553;&#65281;&#29992;&#40065;&#26364;&#21151;&#33021;&#29702;&#35770;&#26500;&#24314;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#29702;&#35299;&#20043;&#38388;&#30340;&#26725;&#26753;
&lt;/p&gt;
&lt;p&gt;
Mind the Gap! Bridging Explainable Artificial Intelligence and Human Understanding with Luhmann's Functional Theory of Communication. (arXiv:2302.03460v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#23545;&#35805;&#24335;&#30340;&#35299;&#37322;&#27169;&#22411;&#65292;&#24357;&#21512;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#29702;&#35299;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20197;&#21019;&#24314;&#26356;&#26377;&#25928;&#21644;&#24191;&#27867;&#24212;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#24050;&#20174;&#19968;&#31181;&#20027;&#35201;&#30340;&#25216;&#26415;&#23398;&#31185;&#21457;&#23637;&#25104;&#19982;&#31038;&#20250;&#31185;&#23398;&#32039;&#23494;&#30456;&#20132;&#30340;&#39046;&#22495;&#12290;&#20154;&#31867;&#20559;&#22909;&#23545;&#27604;&#30340;&#35299;&#37322;&#65292;&#30830;&#20999;&#32780;&#35328;&#26159;&#21453;&#20107;&#23454;&#30340;&#35299;&#37322;&#65292;&#23545;&#20110;&#36825;&#31181;&#36716;&#21464;&#36215;&#21040;&#20102;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#21551;&#21457;&#21644;&#24341;&#39046;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#20854;&#20182;&#21516;&#26679;&#37325;&#35201;&#30340;&#35266;&#23519;&#21364;&#21463;&#21040;&#20102;&#24456;&#23569;&#30340;&#20851;&#27880;&#12290;&#20154;&#31867;&#35299;&#37322;&#32773;&#24076;&#26395;&#36890;&#36807;&#23545;&#35805;&#24335;&#30340;&#20132;&#20114;&#19982;&#20154;&#24037;&#26234;&#33021;&#35299;&#37322;&#32773;&#36827;&#34892;&#20132;&#27969;&#30340;&#24895;&#26395;&#22312;&#31038;&#21306;&#20013;&#22522;&#26412;&#34987;&#24573;&#35270;&#12290;&#36825;&#32473;&#36825;&#31181;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#21644;&#24191;&#27867;&#24212;&#29992;&#24102;&#26469;&#20102;&#24456;&#22810;&#25361;&#25112;&#65292;&#22240;&#20026;&#26681;&#25454;&#39044;&#23450;&#20041;&#30340;&#30446;&#26631;&#25552;&#20379;&#21333;&#19968;&#30340;&#20248;&#21270;&#35299;&#37322;&#21487;&#33021;&#20250;&#22833;&#36133;&#65292;&#24182;&#19988;&#19981;&#33021;&#28385;&#36275;&#20854;&#25509;&#25910;&#32773;&#30340;&#29420;&#29305;&#38656;&#27714;&#65292;&#37492;&#20110;&#20154;&#31867;&#30693;&#35782;&#21644;&#24847;&#22270;&#30340;&#22810;&#26679;&#24615;&#12290;&#26412;&#25991;&#21033;&#29992;&#23612;&#20811;&#25289;&#26031;&#183;&#40065;&#26364;&#21644;&#20854;&#20182;&#20132;&#27969;&#23398;&#32773;&#38416;&#36848;&#30340;&#35265;&#35299;&#65292;&#25552;&#20986;&#20102;&#21521;&#26356;&#23545;&#35805;&#24335;&#30340;&#35299;&#37322;&#27169;&#22411;&#30340;&#36716;&#21464;&#65292;&#20854;&#20013;&#35299;&#37322;&#32773;&#21644;&#34987;&#35299;&#37322;&#32773;&#20043;&#38388;&#30340;&#20449;&#24687;&#25345;&#32493;&#20132;&#27969;&#26159;&#26680;&#24515;&#12290;&#36890;&#36807;&#36825;&#31181;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#24314;&#31435;&#26356;&#26377;&#25928;&#21644;&#24191;&#27867;&#24212;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#24357;&#21512;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#29702;&#35299;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past decade explainable artificial intelligence has evolved from a predominantly technical discipline into a field that is deeply intertwined with social sciences. Insights such as human preference for contrastive -- more precisely, counterfactual -- explanations have played a major role in this transition, inspiring and guiding the research in computer science. Other observations, while equally important, have received much less attention. The desire of human explainees to communicate with artificial intelligence explainers through a dialogue-like interaction has been mostly neglected by the community. This poses many challenges for the effectiveness and widespread adoption of such technologies as delivering a single explanation optimised according to some predefined objectives may fail to engender understanding in its recipients and satisfy their unique needs given the diversity of human knowledge and intention. Using insights elaborated by Niklas Luhmann and, more recently,
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21435;&#20559;&#33258;&#27880;&#24847;&#21147;&#30340;&#20844;&#24179;&#24863;&#30693;&#35270;&#35273;&#21464;&#25442;&#22120;&#26694;&#26550;&#65292;&#36890;&#36807;&#28040;&#38500;&#19982;&#25935;&#24863;&#23646;&#24615;&#30456;&#20851;&#30340;&#34394;&#20551;&#29305;&#24449;&#26469;&#20943;&#36731;&#20559;&#35265;&#65292;&#24182;&#21033;&#29992;&#23545;&#25239;&#24615;&#31034;&#20363;&#26469;&#23450;&#20301;&#21644;&#23631;&#34109;&#36825;&#20123;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2301.13803</link><description>&lt;p&gt;
&#22522;&#20110;&#21435;&#20559;&#33258;&#27880;&#24847;&#21147;&#30340;&#20844;&#24179;&#24863;&#30693;&#35270;&#35273;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Fairness-aware Vision Transformer via Debiased Self-Attention. (arXiv:2301.13803v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13803
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21435;&#20559;&#33258;&#27880;&#24847;&#21147;&#30340;&#20844;&#24179;&#24863;&#30693;&#35270;&#35273;&#21464;&#25442;&#22120;&#26694;&#26550;&#65292;&#36890;&#36807;&#28040;&#38500;&#19982;&#25935;&#24863;&#23646;&#24615;&#30456;&#20851;&#30340;&#34394;&#20551;&#29305;&#24449;&#26469;&#20943;&#36731;&#20559;&#35265;&#65292;&#24182;&#21033;&#29992;&#23545;&#25239;&#24615;&#31034;&#20363;&#26469;&#23450;&#20301;&#21644;&#23631;&#34109;&#36825;&#20123;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#25552;&#21462;&#20449;&#24687;&#29305;&#24449;&#21644;&#36890;&#36807;&#33258;&#25105;&#20851;&#27880;&#26426;&#21046;&#24314;&#27169;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViT&#65289;&#22312;&#35299;&#20915;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#38382;&#39064;&#26041;&#38754;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;ViT&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#20248;&#21183;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;ViT&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#21253;&#25324;&#20854;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#21478;&#19968;&#20010;&#38656;&#27714;&#65292;&#20844;&#24179;&#24615;&#65292;&#22312;&#25991;&#29486;&#20013;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#35299;&#20915;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#20844;&#24179;&#24863;&#30693;&#31639;&#27861;&#65288;&#20027;&#35201;&#35774;&#35745;&#29992;&#20110;CNN&#65289;&#22312;ViT&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#23601;&#38656;&#35201;&#25105;&#20204;&#36890;&#36807;&#21435;&#20559;&#33258;&#27880;&#24847;&#65288;DSA&#65289;&#24320;&#21457;&#25105;&#20204;&#30340;&#26032;&#26694;&#26550;&#12290;DSA&#26159;&#19968;&#31181;&#36890;&#36807;&#30450;&#30446;&#26041;&#27861;&#26469;&#24378;&#21046;ViT&#28040;&#38500;&#19982;&#25935;&#24863;&#23646;&#24615;&#30456;&#20851;&#30340;&#34394;&#20551;&#29305;&#24449;&#20197;&#20943;&#36731;&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23545;&#25239;&#24615;&#31034;&#20363;&#34987;&#29992;&#26469;&#23450;&#20301;&#21644;&#23631;&#34109;&#36755;&#20837;&#22270;&#20687;&#22359;&#20013;&#30340;&#34394;&#20551;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformer (ViT) has recently gained significant interest in solving computer vision (CV) problems due to its capability of extracting informative features and modeling long-range dependencies through the self-attention mechanism. To fully realize the advantages of ViT in real-world applications, recent works have explored the trustworthiness of ViT, including its robustness and explainability. However, another desiderata, fairness has not yet been adequately addressed in the literature. We establish that the existing fairness-aware algorithms (primarily designed for CNNs) do not perform well on ViT. This necessitates the need for developing our novel framework via Debiased Self-Attention (DSA). DSA is a fairness-through-blindness approach that enforces ViT to eliminate spurious features correlated with the sensitive attributes for bias mitigation. Notably, adversarial examples are leveraged to locate and mask the spurious features in the input image patches. In addition, DSA u
&lt;/p&gt;</description></item></channel></rss>