<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#30340;&#26032;&#39062;&#21306;&#38388;&#36793;&#30028;&#20256;&#25773;&#65288;IBP&#65289;&#26041;&#27861;&#65292;IBP IoU&#22312;&#30830;&#20445;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20026;&#26356;&#23433;&#20840;&#21644;&#26356;&#31283;&#20581;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2403.08788</link><description>&lt;p&gt;
&#38024;&#23545;&#30446;&#26631;&#26816;&#27979;&#30340;&#39564;&#35777;--IBP IoU
&lt;/p&gt;
&lt;p&gt;
Verification for Object Detection -- IBP IoU
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08788
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#30340;&#26032;&#39062;&#21306;&#38388;&#36793;&#30028;&#20256;&#25773;&#65288;IBP&#65289;&#26041;&#27861;&#65292;IBP IoU&#22312;&#30830;&#20445;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20026;&#26356;&#23433;&#20840;&#21644;&#26356;&#31283;&#20581;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21306;&#38388;&#36793;&#30028;&#20256;&#25773;&#65288;IBP&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#24418;&#24335;&#39564;&#35777;&#23545;&#35937;&#26816;&#27979;&#27169;&#22411;&#65292;&#29305;&#21035;&#38024;&#23545;&#20132;&#24182;&#27604;&#65288;IoU&#65289;&#24230;&#37327;&#12290;&#35813;&#26041;&#27861;&#24050;&#22312;&#19968;&#20010;&#21517;&#20026;IBP IoU&#30340;&#24320;&#28304;&#20195;&#30721;&#20013;&#23454;&#29616;&#65292;&#19982;&#27969;&#34892;&#30340;&#22522;&#20110;&#25277;&#35937;&#35299;&#37322;&#30340;&#39564;&#35777;&#24037;&#20855;&#20860;&#23481;&#12290;&#35813;&#39564;&#35777;&#22120;&#22312;&#30528;&#38470;&#36884;&#24452;&#36305;&#36947;&#26816;&#27979;&#21644;&#25163;&#20889;&#25968;&#23383;&#35782;&#21035;&#26696;&#20363;&#30740;&#31350;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#19982;&#22522;&#32447;&#65288;Vanilla IBP IoU&#65289;&#30340;&#27604;&#36739;&#31361;&#20986;&#20102;IBP IoU&#22312;&#30830;&#20445;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#20986;&#33394;&#24615;&#33021;&#65292;&#26377;&#21161;&#20110;&#23454;&#29616;&#26356;&#23433;&#20840;&#21644;&#26356;&#31283;&#20581;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08788v1 Announce Type: cross  Abstract: We introduce a novel Interval Bound Propagation (IBP) approach for the formal verification of object detection models, specifically targeting the Intersection over Union (IoU) metric. The approach has been implemented in an open source code, named IBP IoU, compatible with popular abstract interpretation based verification tools. The resulting verifier is evaluated on landing approach runway detection and handwritten digit recognition case studies. Comparisons against a baseline (Vanilla IBP IoU) highlight the superior performance of IBP IoU in ensuring accuracy and stability, contributing to more secure and robust machine learning applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35760;&#24405;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#22870;&#21169;&#22604;&#38519;&#29616;&#35937;&#65292;&#23548;&#33268;&#22312;&#35757;&#32451;&#32467;&#26463;&#26102;&#65292;&#19981;&#21516;&#30340;&#25552;&#31034;&#29983;&#25104;&#30340;&#22870;&#21169;&#20998;&#24067;&#30456;&#21516;&#12290;&#36825;&#20027;&#35201;&#26159;&#22240;&#20026;&#25490;&#21517;&#30340;&#30446;&#26631;&#20989;&#25968;&#26080;&#27861;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#32771;&#34385;&#19982;&#25552;&#31034;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2305.17608</link><description>&lt;p&gt;
&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22870;&#21169;&#22604;&#32553;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Reward Collapse in Aligning Large Language Models. (arXiv:2305.17608v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35760;&#24405;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#22870;&#21169;&#22604;&#38519;&#29616;&#35937;&#65292;&#23548;&#33268;&#22312;&#35757;&#32451;&#32467;&#26463;&#26102;&#65292;&#19981;&#21516;&#30340;&#25552;&#31034;&#29983;&#25104;&#30340;&#22870;&#21169;&#20998;&#24067;&#30456;&#21516;&#12290;&#36825;&#20027;&#35201;&#26159;&#22240;&#20026;&#25490;&#21517;&#30340;&#30446;&#26631;&#20989;&#25968;&#26080;&#27861;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#32771;&#34385;&#19982;&#25552;&#31034;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#21644;GPT-4&#65292;&#20855;&#26377;&#38750;&#20961;&#30340;&#33021;&#21147;&#65292;&#37096;&#20998;&#21407;&#22240;&#22312;&#20110;&#23558;&#23427;&#20204;&#19982;&#35757;&#32451;&#22312;&#20154;&#31867;&#20559;&#22909;&#19978;&#30340;&#22870;&#21169;&#27169;&#22411;&#23545;&#40784;&#65292;&#36825;&#20123;&#20559;&#22909;&#36890;&#24120;&#34920;&#31034;&#20026;&#23545;&#21709;&#24212;&#25552;&#31034;&#30340;&#25490;&#21517;&#12290;&#26412;&#25991;&#35760;&#24405;&#20102;&#22870;&#21169;&#22604;&#38519;&#29616;&#35937;&#65292;&#36825;&#26159;&#19968;&#31181;&#32463;&#39564;&#35266;&#23519;&#65292;&#20854;&#20013;&#22522;&#20110;&#25490;&#21517;&#30340;&#26041;&#27861;&#23548;&#33268;&#22312;&#35757;&#32451;&#30340;&#32456;&#27490;&#38454;&#27573;&#29983;&#25104;&#30340;&#23436;&#25972;&#22870;&#21169;&#20998;&#24067;\textit{&#26080;&#35770;}\textbf{prompt&#26159;&#20160;&#20040;}&#37117;&#26159;\textit{&#30456;&#21516;&#30340;}&#12290;&#36825;&#31181;&#32467;&#26524;&#26159;&#19981;&#21487;&#21462;&#30340;&#65292;&#22240;&#20026;&#20687;&#8220;&#20889;&#19968;&#31687;&#20851;&#20110;&#20320;&#26368;&#22909;&#30340;&#26379;&#21451;&#30340;&#31616;&#30701;&#25925;&#20107;&#8221;&#36825;&#26679;&#30340;&#24320;&#25918;&#24335;&#25552;&#31034;&#24212;&#29983;&#25104;&#23436;&#25104;&#23427;&#20204;&#30340;&#36830;&#32493;&#22870;&#21169;&#33539;&#22260;&#65292;&#32780;&#20687;&#8220;&#26032;&#35199;&#20848;&#30340;&#39318;&#37117;&#26159;&#20160;&#20040;&#8221;&#36825;&#26679;&#30340;&#29305;&#23450;&#25552;&#31034;&#24212;&#29983;&#25104;&#39640;&#25110;&#20302;&#22870;&#21169;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#35843;&#26597;&#34920;&#26126;&#65292;&#22870;&#21169;&#22604;&#38519;&#20027;&#35201;&#26159;&#30001;&#20110;&#22522;&#20110;&#25490;&#21517;&#30340;&#30446;&#26631;&#20989;&#25968;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#26410;&#33021;&#32435;&#20837;&#19982;&#25552;&#31034;&#30456;&#20851;&#30340;&#20449;&#24687;&#25152;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
The extraordinary capabilities of large language models (LLMs) such as ChatGPT and GPT-4 are in part unleashed by aligning them with reward models that are trained on human preferences, which are often represented as rankings of responses to prompts. In this paper, we document the phenomenon of \textit{reward collapse}, an empirical observation where the prevailing ranking-based approach results in an \textit{identical} reward distribution \textit{regardless} of the prompts during the terminal phase of training. This outcome is undesirable as open-ended prompts like ``write a short story about your best friend'' should yield a continuous range of rewards for their completions, while specific prompts like ``what is the capital of New Zealand'' should generate either high or low rewards. Our theoretical investigation reveals that reward collapse is primarily due to the insufficiency of the ranking-based objective function to incorporate prompt-related information during optimization. Thi
&lt;/p&gt;</description></item></channel></rss>