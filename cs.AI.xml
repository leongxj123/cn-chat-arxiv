<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>Frankenstein&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#21333;&#20010;&#36890;&#36947;&#20013;&#21516;&#26102;&#29983;&#25104;&#22810;&#20010;&#35821;&#20041;&#30456;&#20851;&#30340;3D&#24418;&#29366;&#65292;&#20026;&#29983;&#25104;&#25151;&#38388;&#20869;&#37096;&#21644;&#20154;&#31867;&#21270;&#36523;&#31561;&#22330;&#26223;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.16210</link><description>&lt;p&gt;
Frankenstein: &#22312;&#19968;&#20010;&#19977;&#38754;&#20301;&#24179;&#38754;&#20013;&#29983;&#25104;&#35821;&#20041;-&#32452;&#21512;&#24335;3D&#22330;&#26223;
&lt;/p&gt;
&lt;p&gt;
Frankenstein: Generating Semantic-Compositional 3D Scenes in One Tri-Plane
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16210
&lt;/p&gt;
&lt;p&gt;
Frankenstein&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#21333;&#20010;&#36890;&#36947;&#20013;&#21516;&#26102;&#29983;&#25104;&#22810;&#20010;&#35821;&#20041;&#30456;&#20851;&#30340;3D&#24418;&#29366;&#65292;&#20026;&#29983;&#25104;&#25151;&#38388;&#20869;&#37096;&#21644;&#20154;&#31867;&#21270;&#36523;&#31561;&#22330;&#26223;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Frankenstein&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#21333;&#20010;&#36890;&#36947;&#20013;&#29983;&#25104;&#35821;&#20041;-&#32452;&#21512;&#24335;3D&#22330;&#26223;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#36755;&#20986;&#21333;&#20010;&#32479;&#19968;&#30340;3D&#24418;&#29366;&#19981;&#21516;&#65292;Frankenstein&#21516;&#26102;&#29983;&#25104;&#22810;&#20010;&#29420;&#31435;&#30340;&#24418;&#29366;&#65292;&#27599;&#20010;&#23545;&#24212;&#19968;&#20010;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#37096;&#20998;&#12290;3D&#22330;&#26223;&#20449;&#24687;&#32534;&#30721;&#22312;&#19968;&#20010;&#19977;&#38754;&#20301;&#24179;&#38754;&#24352;&#37327;&#20013;&#65292;&#20174;&#20013;&#21487;&#20197;&#35299;&#30721;&#22810;&#20010;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#65288;SDF&#65289;&#22330;&#20197;&#34920;&#31034;&#32452;&#21512;&#24418;&#29366;&#12290;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;&#19968;&#20010;&#33258;&#32534;&#30721;&#22120;&#23558;&#19977;&#38754;&#20301;&#24179;&#38754;&#21387;&#32553;&#21040;&#28508;&#22312;&#31354;&#38388;&#65292;&#28982;&#21518;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#26469;&#36924;&#36817;&#32452;&#21512;&#22330;&#26223;&#30340;&#20998;&#24067;&#12290;Frankenstein&#22312;&#29983;&#25104;&#25151;&#38388;&#20869;&#37096;&#21644;&#20855;&#26377;&#33258;&#21160;&#20998;&#31163;&#37096;&#20998;&#30340;&#20154;&#31867;&#21270;&#36523;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#29983;&#25104;&#30340;&#22330;&#26223;&#26377;&#21161;&#20110;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#65292;&#20363;&#22914;&#37096;&#20998;&#37325;&#36148;&#22270;&#12289;&#25151;&#38388;&#25110;&#21270;&#36523;&#34915;&#26381;&#30340;&#23545;&#35937;&#37325;&#26032;&#25490;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16210v1 Announce Type: cross  Abstract: We present Frankenstein, a diffusion-based framework that can generate semantic-compositional 3D scenes in a single pass. Unlike existing methods that output a single, unified 3D shape, Frankenstein simultaneously generates multiple separated shapes, each corresponding to a semantically meaningful part. The 3D scene information is encoded in one single tri-plane tensor, from which multiple Singed Distance Function (SDF) fields can be decoded to represent the compositional shapes. During training, an auto-encoder compresses tri-planes into a latent space, and then the denoising diffusion process is employed to approximate the distribution of the compositional scenes. Frankenstein demonstrates promising results in generating room interiors as well as human avatars with automatically separated parts. The generated scenes facilitate many downstream applications, such as part-wise re-texturing, object rearrangement in the room or avatar clo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#26032;&#26041;&#27861;&#65292;&#23558;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#37325;&#26032;&#26500;&#24314;&#20026;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#65292;&#35780;&#20272;&#20102;&#22312;&#24052;&#35199;&#38134;&#34892;&#36130;&#25253;&#30005;&#35805;&#36716;&#24405;&#20013;&#20351;&#29992;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12212</link><description>&lt;p&gt;
&#35780;&#20272;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65306;&#27604;&#36739;&#20998;&#26512;&#24052;&#35199;&#20844;&#21496;&#36130;&#25253;&#30005;&#35805;&#36716;&#24405;&#19978;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Named Entity Recognition: Comparative Analysis of Mono- and Multilingual Transformer Models on Brazilian Corporate Earnings Call Transcriptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#26032;&#26041;&#27861;&#65292;&#23558;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#37325;&#26032;&#26500;&#24314;&#20026;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#65292;&#35780;&#20272;&#20102;&#22312;&#24052;&#35199;&#38134;&#34892;&#36130;&#25253;&#30005;&#35805;&#36716;&#24405;&#20013;&#20351;&#29992;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#26159;&#19968;&#31181;&#20174;&#25991;&#26412;&#25991;&#26723;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#20851;&#20110;NER&#30340;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#33521;&#35821;&#25991;&#26723;&#19978;&#65292;&#23548;&#33268;&#32570;&#20047;&#19987;&#38376;&#38024;&#23545;&#33889;&#33796;&#29273;&#35821;&#36130;&#21153;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#37329;&#34701;&#39046;&#22495;&#20869;NER&#38656;&#27714;&#65292;&#24182;&#20391;&#37325;&#20110;&#20174;&#24052;&#35199;&#38134;&#34892;&#36130;&#25253;&#30005;&#35805;&#36716;&#24405;&#20013;&#25552;&#21462;&#30340;&#33889;&#33796;&#29273;&#35821;&#25991;&#26412;&#12290;&#36890;&#36807;&#25972;&#29702;&#21253;&#25324;384&#20010;&#36716;&#24405;&#30340;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#24369;&#30417;&#30563;&#25216;&#26415;&#36827;&#34892;&#27880;&#37322;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22312;&#33889;&#33796;&#29273;&#35821;&#65288;BERTimbau&#21644;PTT5&#65289;&#35757;&#32451;&#30340;&#21333;&#35821;&#27169;&#22411;&#20197;&#21450;&#22810;&#35821;&#35328;&#27169;&#22411;&#65288;mBERT&#21644;mT5&#65289;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#37325;&#26032;&#26500;&#24314;&#20026;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;T5&#27169;&#22411;&#30340;&#24494;&#35843;&#21644;&#35780;&#20272;&#12290;&#22312;&#27169;&#22411;&#24494;&#35843;&#20043;&#21518;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12212v1 Announce Type: cross  Abstract: Named Entity Recognition (NER) is a Natural Language Processing technique for extracting information from textual documents. However, much of the existing research on NER has been centered around English-language documents, leaving a gap in the availability of datasets tailored to the financial domain in Portuguese. This study addresses the need for NER within the financial domain, focusing on Portuguese-language texts extracted from earnings call transcriptions of Brazilian banks. By curating a comprehensive dataset comprising 384 transcriptions and leveraging weak supervision techniques for annotation, we evaluate the performance of monolingual models trained on Portuguese (BERTimbau and PTT5) and multilingual models (mBERT and mT5). Notably, we introduce a novel approach that reframes the token classification task as a text generation problem, enabling fine-tuning and evaluation of T5 models. Following the fine-tuning of the models,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36171;&#20104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;Agent&#32676;&#20307;&#20869;&#31038;&#20250;&#35268;&#33539;&#20986;&#29616;&#30340;&#29983;&#25104;&#24335;Agent&#26550;&#26500;CRSEC&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.08251</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;Agent&#31038;&#20250;&#20013;&#31038;&#20250;&#35268;&#33539;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Emergence of Social Norms in Large Language Model-based Agent Societies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08251
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36171;&#20104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;Agent&#32676;&#20307;&#20869;&#31038;&#20250;&#35268;&#33539;&#20986;&#29616;&#30340;&#29983;&#25104;&#24335;Agent&#26550;&#26500;CRSEC&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#35268;&#33539;&#30340;&#20986;&#29616;&#21560;&#24341;&#20102;&#31038;&#20250;&#31185;&#23398;&#12289;&#35748;&#30693;&#31185;&#23398;&#20197;&#21450;&#20154;&#24037;&#26234;&#33021;&#31561;&#21508;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36171;&#20104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;Agent&#32676;&#20307;&#20869;&#31038;&#20250;&#35268;&#33539;&#20986;&#29616;&#30340;&#29983;&#25104;&#24335;Agent&#26550;&#26500;CRSEC&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#21253;&#25324;&#22235;&#20010;&#27169;&#22359;&#65306;Creation &amp; Representation&#12289;Spreading&#12289;Evaluation&#21644;Compliance&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#22788;&#29702;&#20102;&#20960;&#20010;&#20851;&#38190;&#26041;&#38754;&#30340;&#32039;&#24613;&#36807;&#31243;&#65306;(i)&#31038;&#20250;&#35268;&#33539;&#30340;&#26469;&#28304;&#65292;(ii)&#23427;&#20204;&#22914;&#20309;&#34987;&#27491;&#24335;&#34920;&#31034;&#65292;(iii)&#23427;&#20204;&#22914;&#20309;&#36890;&#36807;Agent&#30340;&#20132;&#27969;&#21644;&#35266;&#23519;&#20256;&#25773;&#65292;(iv)&#22914;&#20309;&#36890;&#36807;&#21512;&#29702;&#26816;&#26597;&#36827;&#34892;&#26816;&#26597;&#24182;&#22312;&#38271;&#26399;&#20869;&#36827;&#34892;&#32508;&#21512;&#65292;(v)&#22914;&#20309;&#34987;&#32435;&#20837;Agent&#30340;&#35745;&#21010;&#21644;&#34892;&#21160;&#20013;&#12290;&#25105;&#20204;&#22312;Smallville&#27801;&#30418;&#28216;&#25103;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26550;&#26500;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08251v1 Announce Type: cross  Abstract: The emergence of social norms has attracted much interest in a wide array of disciplines, ranging from social science and cognitive science to artificial intelligence. In this paper, we propose the first generative agent architecture that empowers the emergence of social norms within a population of large language model-based agents. Our architecture, named CRSEC, consists of four modules: Creation &amp; Representation, Spreading, Evaluation, and Compliance. Our architecture addresses several important aspects of the emergent processes all in one: (i) where social norms come from, (ii) how they are formally represented, (iii) how they spread through agents' communications and observations, (iv) how they are examined with a sanity check and synthesized in the long term, and (v) how they are incorporated into agents' planning and actions. Our experiments deployed in the Smallville sandbox game environment demonstrate the capability of our ar
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#22810;&#27169;&#22411;&#65288;LLMMs&#65289;&#26694;&#26550;&#65292;&#32467;&#21512;&#20020;&#24202;&#31508;&#35760;&#21644;&#23454;&#39564;&#23460;&#26816;&#39564;&#32467;&#26524;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#29992;&#20110;&#39044;&#27979;&#24930;&#24615;&#30142;&#30149;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2403.04785</link><description>&lt;p&gt;
&#20351;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#39044;&#27979;5&#24180;&#24930;&#24615;&#30142;&#30149;&#38431;&#21015;&#30340;&#22823;&#22411;&#35821;&#35328;&#22810;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Multimodal Models for 5-Year Chronic Disease Cohort Prediction Using EHR Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#22810;&#27169;&#22411;&#65288;LLMMs&#65289;&#26694;&#26550;&#65292;&#32467;&#21512;&#20020;&#24202;&#31508;&#35760;&#21644;&#23454;&#39564;&#23460;&#26816;&#39564;&#32467;&#26524;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#29992;&#20110;&#39044;&#27979;&#24930;&#24615;&#30142;&#30149;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24930;&#24615;&#30142;&#30149;&#22914;&#31958;&#23615;&#30149;&#26159;&#20840;&#29699;&#21457;&#30149;&#29575;&#21644;&#27515;&#20129;&#29575;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#26412;&#30740;&#31350;&#20174;&#21488;&#28286;&#21307;&#38498;&#25968;&#25454;&#24211;&#25910;&#38598;&#20102;&#20116;&#24180;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#65292;&#21253;&#25324;1,420,596&#20221;&#20020;&#24202;&#31508;&#35760;&#12289;387,392&#20221;&#23454;&#39564;&#23460;&#26816;&#39564;&#32467;&#26524;&#20197;&#21450;&#36229;&#36807;1,505&#31181;&#23454;&#39564;&#23460;&#26816;&#39564;&#39033;&#30446;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#29992;&#20110;&#30740;&#31350;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22823;&#22411;&#35821;&#35328;&#22810;&#27169;&#22411;&#65288;LLMMs&#65289;&#26694;&#26550;&#65292;&#23558;&#20020;&#24202;&#31508;&#35760;&#21644;&#23454;&#39564;&#23460;&#26816;&#39564;&#32467;&#26524;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#39044;&#27979;&#24930;&#24615;&#30142;&#30149;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#25991;&#26412;&#23884;&#20837;&#32534;&#30721;&#22120;&#21644;&#22810;&#22836;&#27880;&#24847;&#21147;&#23618;&#26469;&#23398;&#20064;&#23454;&#39564;&#23460;&#26816;&#39564;&#25968;&#20540;&#65292;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22359;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04785v1 Announce Type: cross  Abstract: Chronic diseases such as diabetes are the leading causes of morbidity and mortality worldwide. Numerous research studies have been attempted with various deep learning models in diagnosis. However, most previous studies had certain limitations, including using publicly available datasets (e.g. MIMIC), and imbalanced data. In this study, we collected five-year electronic health records (EHRs) from the Taiwan hospital database, including 1,420,596 clinical notes, 387,392 laboratory test results, and more than 1,505 laboratory test items, focusing on research pre-training large language models. We proposed a novel Large Language Multimodal Models (LLMMs) framework incorporating multimodal data from clinical notes and laboratory test results for the prediction of chronic disease risk. Our method combined a text embedding encoder and multi-head attention layer to learn laboratory test values, utilizing a deep neural network (DNN) module to 
&lt;/p&gt;</description></item><item><title>&#31038;&#21306;&#25361;&#25112;&#35780;&#20272;&#31454;&#36187;&#22312;&#20419;&#36827;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#30740;&#31350;&#20013;&#30340;&#25216;&#26415;&#21019;&#26032;&#21644;&#36328;&#23398;&#31185;&#21512;&#20316;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.04261</link><description>&lt;p&gt;
&#36890;&#36807;&#31038;&#21306;&#25361;&#25112;&#25512;&#21160;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#30340;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
Advancing Biomedical Text Mining with Community Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04261
&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#25361;&#25112;&#35780;&#20272;&#31454;&#36187;&#22312;&#20419;&#36827;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#30740;&#31350;&#20013;&#30340;&#25216;&#26415;&#21019;&#26032;&#21644;&#36328;&#23398;&#31185;&#21512;&#20316;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#39046;&#22495;&#31215;&#32047;&#20102;&#22823;&#37327;&#26469;&#33258;&#31185;&#23398;&#25991;&#29486;&#12289;&#30005;&#23376;&#30149;&#21382;&#12289;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#21644;&#31038;&#20132;&#23186;&#20307;&#31561;&#21508;&#26041;&#38754;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;&#28982;&#32780;&#25163;&#21160;&#22788;&#29702;&#21644;&#20998;&#26512;&#36825;&#20123;&#24222;&#22823;&#19988;&#22797;&#26434;&#30340;&#36164;&#28304;&#26159;&#32791;&#26102;&#19988;&#20302;&#25928;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#65292;&#20063;&#31216;&#20026;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#22791;&#21463;&#20851;&#27880;&#12290;&#31038;&#21306;&#25361;&#25112;&#35780;&#20272;&#31454;&#36187;&#22312;&#20419;&#36827;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#30740;&#31350;&#20013;&#30340;&#25216;&#26415;&#21019;&#26032;&#21644;&#36328;&#23398;&#31185;&#21512;&#20316;&#26041;&#38754;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#36825;&#20123;&#25361;&#25112;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#24320;&#21457;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#20013;&#25968;&#25454;&#25366;&#25496;&#21644;&#20449;&#24687;&#22788;&#29702;&#30340;&#26368;&#26032;&#35299;&#20915;&#26041;&#26696;&#30340;&#24179;&#21488;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#19982;&#20013;&#25991;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#26377;&#20851;&#30340;&#26368;&#26032;&#31038;&#21306;&#25361;&#25112;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04261v1 Announce Type: new  Abstract: The field of biomedical research has witnessed a significant increase in the accumulation of vast amounts of textual data from various sources such as scientific literatures, electronic health records, clinical trial reports, and social media. However, manually processing and analyzing these extensive and complex resources is time-consuming and inefficient. To address this challenge, biomedical text mining, also known as biomedical natural language processing, has garnered great attention. Community challenge evaluation competitions have played an important role in promoting technology innovation and interdisciplinary collaboration in biomedical text mining research. These challenges provide platforms for researchers to develop state-of-the-art solutions for data mining and information processing in biomedical research. In this article, we review the recent advances in community challenges specific to Chinese biomedical text mining. Firs
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SimuCourt&#21496;&#27861;&#22522;&#20934;&#65292;&#21253;&#25324;&#30495;&#23454;&#19990;&#30028;&#30340;&#21496;&#27861;&#25991;&#20214;&#65292;&#24182;&#24341;&#20837;&#20102;&#21496;&#27861;&#20915;&#31574;&#20219;&#21153;&#21644;&#22810;&#20195;&#29702;&#26694;&#26550;&#65292;&#35780;&#20272;&#20102;&#20195;&#29702;&#30340;&#21496;&#27861;&#20998;&#26512;&#21644;&#20915;&#31574;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.02959</link><description>&lt;p&gt;
SimuCourt: &#21033;&#29992;&#30495;&#23454;&#21496;&#27861;&#21028;&#20915;&#25991;&#20214;&#26500;&#24314;&#21496;&#27861;&#20915;&#31574;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
SimuCourt: Building Judicial Decision-Making Agents with Real-world Judgement Documents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02959
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SimuCourt&#21496;&#27861;&#22522;&#20934;&#65292;&#21253;&#25324;&#30495;&#23454;&#19990;&#30028;&#30340;&#21496;&#27861;&#25991;&#20214;&#65292;&#24182;&#24341;&#20837;&#20102;&#21496;&#27861;&#20915;&#31574;&#20219;&#21153;&#21644;&#22810;&#20195;&#29702;&#26694;&#26550;&#65292;&#35780;&#20272;&#20102;&#20195;&#29702;&#30340;&#21496;&#27861;&#20998;&#26512;&#21644;&#20915;&#31574;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#20256;&#32479;&#21496;&#27861;&#34892;&#19994;&#21508;&#20010;&#26041;&#38754;&#30340;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22823;&#22810;&#25968;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#20010;&#21035;&#21496;&#27861;&#38454;&#27573;&#65292;&#24573;&#35270;&#20102;&#36328;&#38454;&#27573;&#30340;&#21327;&#20316;&#12290;&#38543;&#30528;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#25903;&#25345;&#30340;&#33258;&#20027;&#20195;&#29702;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#26234;&#33021;&#65292;&#24182;&#33021;&#20570;&#20986;&#22797;&#26434;&#20915;&#31574;&#65292;&#20026;&#21496;&#27861;&#26234;&#33021;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SimuCourt&#65292;&#19968;&#20010;&#21496;&#27861;&#22522;&#20934;&#65292;&#21253;&#25324;&#26469;&#33258;&#30495;&#23454;&#19990;&#30028;&#30340;420&#20221;&#21028;&#20915;&#25991;&#20214;&#65292;&#28085;&#30422;&#20102;&#19977;&#31181;&#26368;&#24120;&#35265;&#31867;&#22411;&#30340;&#21496;&#27861;&#26696;&#20363;&#65292;&#20197;&#21450;&#19968;&#20010;&#26032;&#39062;&#20219;&#21153;&#21496;&#27861;&#20915;&#31574;&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#29702;&#30340;&#21496;&#27861;&#20998;&#26512;&#21644;&#20915;&#31574;&#33021;&#21147;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#19968;&#20219;&#21153;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#21496;&#27861;&#30693;&#35782;&#24211;&#65292;JudicialKB&#65292;&#20854;&#20013;&#21253;&#21547;&#22810;&#31181;&#27861;&#24459;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20195;&#29702;&#26694;&#26550;&#65292;AgentsCourt
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02959v1 Announce Type: cross  Abstract: With the development of deep learning, natural language processing technology has effectively improved the efficiency of various aspects of the traditional judicial industry. However, most current efforts focus solely on individual judicial stage, overlooking cross-stage collaboration. As the autonomous agents powered by large language models are becoming increasingly smart and able to make complex decisions in real-world settings, offering new insights for judicial intelligence. In this paper, (1) we introduce SimuCourt, a judicial benchmark that encompasses 420 judgment documents from real-world, spanning the three most common types of judicial cases, and a novel task Judicial Decision-Making to evaluate the judicial analysis and decision-making power of agents. To support this task, we construct a large-scale judicial knowledge base, JudicialKB, with multiple legal knowledge. (2) we propose a novel multi-agent framework, AgentsCourt
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#35780;&#20272;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20440;&#33719;&#20154;&#31867;&#34892;&#20026;&#26041;&#38754;&#30340;&#34920;&#29616;&#38750;&#24120;&#20986;&#33394;&#65292;&#19981;&#20165;&#25972;&#20307;&#20934;&#30830;&#29575;&#39640;&#65292;&#32780;&#19988;&#33021;&#22815;&#25429;&#25417;&#21040;&#20154;&#31867;&#35821;&#35328;&#21028;&#26029;&#20013;&#30340;&#32454;&#24494;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.01676</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#22312;&#20851;&#38190;&#35821;&#27861;&#32467;&#26500;&#19978;&#30340;&#21028;&#26029;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Language models align with human judgments on key grammatical constructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01676
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#35780;&#20272;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20440;&#33719;&#20154;&#31867;&#34892;&#20026;&#26041;&#38754;&#30340;&#34920;&#29616;&#38750;&#24120;&#20986;&#33394;&#65292;&#19981;&#20165;&#25972;&#20307;&#20934;&#30830;&#29575;&#39640;&#65292;&#32780;&#19988;&#33021;&#22815;&#25429;&#25417;&#21040;&#20154;&#31867;&#35821;&#35328;&#21028;&#26029;&#20013;&#30340;&#32454;&#24494;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#20855;&#26377;&#31867;&#20284;&#20154;&#31867;&#30340;&#35821;&#35328;&#26222;&#36941;&#24615;&#65311;Dentella&#31561;&#20154;&#65288;2023&#24180;&#65307;&#8220;DGL&#8221;&#65289;&#20351;&#29992;&#22810;&#20010;LLMs&#25552;&#31034;&#35821;&#27861;&#27491;&#30830;&#24615;&#38382;&#39064;&#65292;&#20197;&#33719;&#21462;80&#20010;&#33521;&#35821;&#21477;&#23376;&#30340;&#35821;&#27861;&#21477;&#23376;&#21028;&#26029;&#65292;&#24471;&#20986;LLMs&#23384;&#22312;&#8220;&#26159;&#8221;&#20559;&#21521;&#21644;&#8220;&#19981;&#33021;&#21306;&#20998;&#35821;&#27861;&#21644;&#38750;&#35821;&#27861;&#21477;&#23376;&#8221;&#30340;&#32467;&#35770;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#26082;&#23450;&#30340;&#23454;&#36341;&#26041;&#27861;&#37325;&#26032;&#35780;&#20272;LLM&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;DGL&#30340;&#25968;&#25454;&#23454;&#38469;&#19978;&#35777;&#26126;&#20102;LLM&#22914;&#20309;&#20934;&#30830;&#25429;&#25417;&#20154;&#31867;&#34892;&#20026;&#12290;&#27169;&#22411;&#19981;&#20165;&#25972;&#20307;&#19978;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#29575;&#65292;&#36824;&#25429;&#25417;&#21040;&#20102;&#20154;&#31867;&#35821;&#35328;&#21028;&#26029;&#30340;&#32454;&#24494;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Do Large Language Models (LLMs) make human-like linguistic generalizations? Dentella et al. (2023; "DGL") prompt several LLMs ("Is the following sentence grammatically correct in English?") to elicit grammaticality judgments of 80 English sentences, concluding that LLMs demonstrate a "yes-response bias" and a "failure to distinguish grammatical from ungrammatical sentences". We re-evaluate LLM performance using well-established practices and find that DGL's data in fact provide evidence for just how well LLMs capture human behaviors. Models not only achieve high accuracy overall, but also capture fine-grained variation in human linguistic judgments.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#20113;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#26230;&#20307;&#32467;&#26500;&#29983;&#25104;&#35774;&#35745;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#37325;&#24314;&#36755;&#20837;&#32467;&#26500;&#21644;&#29983;&#25104;&#20840;&#26032;&#26448;&#26009;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.13192</link><description>&lt;p&gt;
&#22522;&#20110;&#28857;&#20113;&#34920;&#31034;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#26230;&#20307;&#32467;&#26500;&#29983;&#25104;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Generative Design of Crystal Structures by Point Cloud Representations and Diffusion Model. (arXiv:2401.13192v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#20113;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#26230;&#20307;&#32467;&#26500;&#29983;&#25104;&#35774;&#35745;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#37325;&#24314;&#36755;&#20837;&#32467;&#26500;&#21644;&#29983;&#25104;&#20840;&#26032;&#26448;&#26009;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26448;&#26009;&#35774;&#35745;&#20013;&#65292;&#39640;&#25928;&#22320;&#29983;&#25104;&#33021;&#37327;&#31283;&#23450;&#30340;&#26230;&#20307;&#32467;&#26500;&#19968;&#30452;&#26159;&#20010;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#26230;&#26684;&#20013;&#21407;&#23376;&#30340;&#24040;&#22823;&#25490;&#21015;&#12290;&#20026;&#20102;&#20419;&#36827;&#31283;&#23450;&#26448;&#26009;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#21487;&#21512;&#25104;&#26448;&#26009;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#28857;&#20113;&#34920;&#31034;&#26469;&#32534;&#30721;&#22797;&#26434;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#30340;&#26680;&#24515;&#26159;&#24341;&#20837;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#22522;&#30784;&#25903;&#26609;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#23427;&#26469;&#37325;&#24314;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#36755;&#20837;&#32467;&#26500;&#65292;&#24182;&#20005;&#26684;&#39564;&#35777;&#20854;&#39640;&#37325;&#24314;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#29983;&#25104;&#20840;&#26032;&#30340;&#26448;&#26009;&#65292;&#37325;&#28857;&#24378;&#35843;&#20102;&#22522;&#20110;&#28857;&#20113;&#30340;&#26230;&#20307;&#25193;&#25955;(PCCD)&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#21487;&#21512;&#25104;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22312;&#26448;&#26009;&#35774;&#35745;&#21644;&#21512;&#25104;&#30340;&#25512;&#36827;&#20013;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#29983;&#25104;&#35774;&#35745;&#26041;&#27861;&#65292;&#20570;&#20986;&#20102;&#26174;&#33879;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently generating energetically stable crystal structures has long been a challenge in material design, primarily due to the immense arrangement of atoms in a crystal lattice. To facilitate the discovery of stable material, we present a framework for the generation of synthesizable materials, leveraging a point cloud representation to encode intricate structural information. At the heart of this framework lies the introduction of a diffusion model as its foundational pillar. To gauge the efficacy of our approach, we employ it to reconstruct input structures from our training datasets, rigorously validating its high reconstruction performance. Furthermore, we demonstrate the profound potential of Point Cloud-Based Crystal Diffusion (PCCD) by generating entirely new materials, emphasizing their synthesizability. Our research stands as a noteworthy contribution to the advancement of materials design and synthesis through the cutting-edge avenue of generative design instead of the con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#29256;DeepFool&#31639;&#27861;&#65292;&#21517;&#20026;Targeted DeepFool&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#31867;&#21035;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#24341;&#20837;&#20102;&#26368;&#23567;&#32622;&#20449;&#24230;&#20998;&#25968;&#35201;&#27714;&#36229;&#21442;&#25968;&#26469;&#25552;&#39640;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13019</link><description>&lt;p&gt;
&#36890;&#36807;DeepFool&#31639;&#27861;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26377;&#38024;&#23545;&#24615;&#30340;&#31867;&#21035;&#25805;&#32437;&#30340;&#23545;&#25239;&#25915;&#20987;&#23450;&#21046;
&lt;/p&gt;
&lt;p&gt;
Tailoring Adversarial Attacks on Deep Neural Networks for Targeted Class Manipulation Using DeepFool Algorithm. (arXiv:2310.13019v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#29256;DeepFool&#31639;&#27861;&#65292;&#21517;&#20026;Targeted DeepFool&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#31867;&#21035;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#24341;&#20837;&#20102;&#26368;&#23567;&#32622;&#20449;&#24230;&#20998;&#25968;&#35201;&#27714;&#36229;&#21442;&#25968;&#26469;&#25552;&#39640;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#23545;&#25239;&#25915;&#20987;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#24341;&#36215;&#20102;&#20005;&#37325;&#20851;&#27880;&#12290;&#20102;&#35299;&#36825;&#20123;&#26131;&#21463;&#25915;&#20987;&#24615;&#24182;&#24320;&#21457;&#26377;&#25928;&#30340;&#38450;&#24481;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;DeepFool&#26159;Moosavi-Dezfooli&#31561;&#20154;&#65288;2016&#24180;&#65289;&#25552;&#20986;&#30340;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#23558;&#36755;&#20837;&#22270;&#20687;&#38169;&#35823;&#20998;&#31867;&#30340;&#26368;&#23567;&#25200;&#21160;&#12290;&#28982;&#32780;&#65292;DeepFool&#32570;&#20047;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#27861;&#65292;&#20351;&#20854;&#22312;&#29305;&#23450;&#25915;&#20987;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#36739;&#20302;&#12290;&#27492;&#22806;&#65292;&#22312;&#20808;&#21069;&#30340;&#30456;&#20851;&#24037;&#20316;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#25104;&#21151;&#29575;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#22270;&#20687;&#34987;&#25197;&#26354;&#30340;&#31243;&#24230;&#12289;&#22270;&#20687;&#36136;&#37327;&#30340;&#23436;&#25972;&#24615;&#20197;&#21450;&#38169;&#35823;&#20998;&#31867;&#30340;&#32622;&#20449;&#24230;&#27700;&#24179;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Targeted DeepFool&#65292;&#36825;&#26159;DeepFool&#30340;&#22686;&#24378;&#29256;&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#31867;&#21035;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26368;&#23567;&#32622;&#20449;&#24230;&#20998;&#25968;&#35201;&#27714;&#36229;&#21442;&#25968;&#26469;&#22686;&#24378;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have significantly advanced various domains, but their vulnerability to adversarial attacks poses serious concerns. Understanding these vulnerabilities and developing effective defense mechanisms is crucial. DeepFool, an algorithm proposed by Moosavi-Dezfooli et al. (2016), finds minimal perturbations to misclassify input images. However, DeepFool lacks a targeted approach, making it less effective in specific attack scenarios. Also, in previous related works, researchers primarily focus on success, not considering how much an image is getting distorted; the integrity of the image quality, and the confidence level to misclassifying. So, in this paper, we propose Targeted DeepFool, an augmented version of DeepFool that allows targeting specific classes for misclassification. We also introduce a minimum confidence score requirement hyperparameter to enhance flexibility. Our experiments demonstrate the effectiveness and efficiency of the proposed method across 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36127;&#33655;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#37319;&#29992;Seq2Seq&#32593;&#32476;&#32467;&#26500;&#26469;&#20998;&#31163;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#22788;&#29702;&#24322;&#24120;&#24773;&#20917;&#65292;&#19981;&#20165;&#30528;&#30524;&#20110;&#39044;&#27979;&#26465;&#20214;&#26399;&#26395;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.01001</link><description>&lt;p&gt;
DiffLoad:&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36127;&#33655;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
DiffLoad: Uncertainty Quantification in Load Forecasting with Diffusion Model. (arXiv:2306.01001v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36127;&#33655;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#37319;&#29992;Seq2Seq&#32593;&#32476;&#32467;&#26500;&#26469;&#20998;&#31163;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#22788;&#29702;&#24322;&#24120;&#24773;&#20917;&#65292;&#19981;&#20165;&#30528;&#30524;&#20110;&#39044;&#27979;&#26465;&#20214;&#26399;&#26395;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#23545;&#30005;&#21147;&#31995;&#32479;&#30340;&#20915;&#31574;&#21046;&#23450;&#65292;&#22914;&#26426;&#32452;&#25237;&#20837;&#21644;&#33021;&#28304;&#31649;&#29702;&#31561;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36817;&#24180;&#26469;&#65292;&#21508;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#25429;&#25417;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#39640;&#26031;&#20284;&#28982;&#26041;&#27861;&#30340;&#65292;&#23427;&#26088;&#22312;&#22312;&#32473;&#23450;&#30340;&#21327;&#21464;&#37327;&#19979;&#20934;&#30830;&#20272;&#35745;&#20998;&#24067;&#26399;&#26395;&#20540;&#12290;&#36825;&#31181;&#26041;&#27861;&#24456;&#38590;&#36866;&#24212;&#23384;&#22312;&#20998;&#24067;&#20559;&#31227;&#21644;&#24322;&#24120;&#20540;&#30340;&#26102;&#38388;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;Seq2seq&#32467;&#26500;&#26469;&#20272;&#35745;&#26412;&#20307;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;&#40065;&#26834;&#30340;&#21152;&#24615;&#26607;&#35199;&#20998;&#24067;&#26469;&#20272;&#35745;&#29289;&#35937;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20998;&#31163;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#22788;&#29702;&#31361;&#21464;&#24773;&#20917;&#65292;&#32780;&#19981;&#26159;&#20934;&#30830;&#39044;&#27979;&#26465;&#20214;&#26399;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrical load forecasting is of great significance for the decision makings in power systems, such as unit commitment and energy management. In recent years, various self-supervised neural network-based methods have been applied to electrical load forecasting to improve forecasting accuracy and capture uncertainties. However, most current methods are based on Gaussian likelihood methods, which aim to accurately estimate the distribution expectation under a given covariate. This kind of approach is difficult to adapt to situations where temporal data has a distribution shift and outliers. In this paper, we propose a diffusion-based Seq2seq structure to estimate epistemic uncertainty and use the robust additive Cauchy distribution to estimate aleatoric uncertainty. Rather than accurately forecasting conditional expectations, we demonstrate our method's ability in separating two types of uncertainties and dealing with the mutant scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;MCTS/THTS&#31639;&#27861;GreedyUCT-Normal&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#36890;&#36807;&#37319;&#29992;&#22870;&#21169;&#21464;&#21270;&#30340;&#23610;&#24230;&#22788;&#29702;&#19981;&#21516;&#23610;&#24230;&#30340;&#20998;&#24067;&#65292;&#20197;&#22312;&#32463;&#20856;&#35745;&#21010;&#20013;&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#12290;</title><link>http://arxiv.org/abs/2305.09840</link><description>&lt;p&gt;
&#32463;&#20856;&#35268;&#21010;&#20013;&#25506;&#32034;&#21644;&#24320;&#21457;&#30340;&#33258;&#36866;&#24212;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Scale-Adaptive Balancing of Exploration and Exploitation in Classical Planning. (arXiv:2305.09840v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;MCTS/THTS&#31639;&#27861;GreedyUCT-Normal&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#36890;&#36807;&#37319;&#29992;&#22870;&#21169;&#21464;&#21270;&#30340;&#23610;&#24230;&#22788;&#29702;&#19981;&#21516;&#23610;&#24230;&#30340;&#20998;&#24067;&#65292;&#20197;&#22312;&#32463;&#20856;&#35745;&#21010;&#20013;&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28216;&#25103;&#26641;&#25628;&#32034;&#21644;&#33258;&#21160;&#21270;&#35268;&#21010;&#20013;&#65292;&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#36825;&#20010;&#38382;&#39064;&#22312;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MAB&#65289;&#25991;&#29486;&#20013;&#24050;&#32463;&#34987;&#24191;&#27867;&#20998;&#26512;&#65292;&#20294;&#35268;&#21010;&#31038;&#21306;&#22312;&#35797;&#22270;&#24212;&#29992;&#36825;&#20123;&#32467;&#26524;&#26102;&#21462;&#24471;&#30340;&#25104;&#21151;&#26377;&#38480;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;MAB&#25991;&#29486;&#26356;&#35814;&#32454;&#30340;&#29702;&#35770;&#29702;&#35299;&#26377;&#21161;&#20110;&#25913;&#36827;&#22522;&#20110;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;/&#22522;&#20110;&#35797;&#39564;&#30340;&#21551;&#21457;&#24335;&#26641;&#25628;&#32034;&#65288;THTS&#65289;&#30340;&#29616;&#26377;&#35268;&#21010;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;THTS&#22312;&#19968;&#31181;&#20020;&#26102;&#26041;&#27861;&#20013;&#20351;&#29992;UCB1 MAB&#31639;&#27861;&#65292;&#22240;&#20026;&#22312;&#21551;&#21457;&#24335;&#25628;&#32034;&#20013;UCB1&#29702;&#35770;&#19978;&#38656;&#35201;&#26377;&#30028;&#25903;&#25345;&#22870;&#21169;&#20998;&#24067;&#30340;&#35201;&#27714;&#22312;&#32463;&#20856;&#35268;&#21010;&#20013;&#19981;&#34987;&#28385;&#36275;&#12290;&#26680;&#24515;&#38382;&#39064;&#22312;&#20110;UCB1&#32570;&#20047;&#23545;&#19981;&#21516;&#22870;&#21169;&#23610;&#24230;&#30340;&#33258;&#36866;&#24212;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GreedyUCT-Normal&#65292;&#36825;&#26159;&#19968;&#31181;&#20855;&#26377;UCB1-Normal&#36172;&#21338;&#26426;&#30340;MCTS/THTS&#31639;&#27861;&#65292;&#29992;&#20110;&#25935;&#25463;&#32463;&#20856;&#35745;&#21010;&#65292;&#23427;&#36890;&#36807;&#37319;&#29992;&#22870;&#21169;&#21464;&#21270;&#30340;&#23610;&#24230;&#22788;&#29702;&#19981;&#21516;&#23610;&#24230;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Balancing exploration and exploitation has been an important problem in both game tree search and automated planning. However, while the problem has been extensively analyzed within the Multi-Armed Bandit (MAB) literature, the planning community has had limited success when attempting to apply those results. We show that a more detailed theoretical understanding of MAB literature helps improve existing planning algorithms that are based on Monte Carlo Tree Search (MCTS) / Trial Based Heuristic Tree Search (THTS). In particular, THTS uses UCB1 MAB algorithms in an ad hoc manner, as UCB1's theoretical requirement of fixed bounded support reward distributions is not satisfied within heuristic search for classical planning. The core issue lies in UCB1's lack of adaptations to the different scales of the rewards. We propose GreedyUCT-Normal, a MCTS/THTS algorithm with UCB1-Normal bandit for agile classical planning, which handles distributions with different scales by taking the reward vari
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#23398;&#20064;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#30340;&#23545;&#27604;&#26694;&#26550;&#65292;&#36890;&#36807;&#35813;&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#20855;&#22791;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26631;&#31614;&#25928;&#29575;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#23454;&#29992;BNN&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#35813;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#21644;&#20302;&#39044;&#31639;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#20013;&#23637;&#29616;&#20986;&#20102;&#25968;&#25454;&#39640;&#25928;&#23398;&#20064;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2304.01762</link><description>&lt;p&gt;
&#23558;&#26410;&#26631;&#35760;&#25968;&#25454;&#32435;&#20837;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;
&lt;/p&gt;
&lt;p&gt;
Incorporating Unlabelled Data into Bayesian Neural Networks. (arXiv:2304.01762v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01762
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#23398;&#20064;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#30340;&#23545;&#27604;&#26694;&#26550;&#65292;&#36890;&#36807;&#35813;&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#20855;&#22791;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26631;&#31614;&#25928;&#29575;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#23454;&#29992;BNN&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#35813;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#21644;&#20302;&#39044;&#31639;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#20013;&#23637;&#29616;&#20986;&#20102;&#25968;&#25454;&#39640;&#25928;&#23398;&#20064;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#20013;&#20808;&#39564;&#20998;&#24067;&#36827;&#34892;&#23398;&#20064;&#30340;&#23545;&#27604;&#26694;&#26550;&#65292;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#20248;&#21270;&#12290;&#22522;&#20110;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;BNN&#31639;&#27861;&#65292;&#21516;&#26102;&#20855;&#22791;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26631;&#31614;&#25928;&#29575;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#20013;&#30340;&#26681;&#25454;&#21407;&#21017;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#21644;&#20302;&#39044;&#31639;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#25968;&#25454;&#39640;&#25928;&#23398;&#20064;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a contrastive framework for learning better prior distributions for Bayesian Neural Networks (BNNs) using unlabelled data. With this framework, we propose a practical BNN algorithm that offers the label-efficiency of self-supervised learning and the principled uncertainty estimates of Bayesian methods. Finally, we demonstrate the advantages of our approach for data-efficient learning in semi-supervised and low-budget active learning problems.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#22810;&#26234;&#33021;&#20307;&#35268;&#21010;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#28041;&#21450;&#19981;&#21516;&#31867;&#22411;&#26234;&#33021;&#20307;&#30340;&#38382;&#39064;&#65292;&#27604;&#29616;&#26377;&#31639;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2301.01246</link><description>&lt;p&gt;
&#21551;&#21457;&#24335;&#22810;&#26234;&#33021;&#20307;&#35268;&#21010;&#20248;&#21270;&#26234;&#33021;&#20307;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Optimizing Agent Collaboration through Heuristic Multi-Agent Planning. (arXiv:2301.01246v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01246
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#22810;&#26234;&#33021;&#20307;&#35268;&#21010;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#28041;&#21450;&#19981;&#21516;&#31867;&#22411;&#26234;&#33021;&#20307;&#30340;&#38382;&#39064;&#65292;&#27604;&#29616;&#26377;&#31639;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#28041;&#21450;&#21040;&#19981;&#21516;&#31867;&#22411;&#24863;&#30693;&#26234;&#33021;&#20307;&#30340;&#38382;&#39064;&#65292;&#30446;&#21069;&#35299;&#20915;QDec-POMDP&#30340;SOTA&#31639;&#27861;QDec-FP&#21644;QDec-FPS&#26080;&#27861;&#26377;&#25928;&#35299;&#20915;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#35201;&#27714;&#26234;&#33021;&#20307;&#37319;&#21462;&#30456;&#21516;&#30340;&#35745;&#21010;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#27604;QDec-FP&#21644;QDec-FPS&#37117;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The SOTA algorithms for addressing QDec-POMDP issues, QDec-FP and QDec-FPS, are unable to effectively tackle problems that involve different types of sensing agents. We propose a new algorithm that addresses this issue by requiring agents to adopt the same plan if one agent is unable to take a sensing action but the other can. Our algorithm performs significantly better than both QDec-FP and QDec-FPS in these types of situations.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20179;&#24211;&#29289;&#27969;&#20013;&#30340;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#21516;&#20107;&#21512;&#20316;&#12290;&#20182;&#20204;&#36890;&#36807;&#20998;&#23618;&#30340;MARL&#31639;&#27861;&#65292;&#35753;&#32463;&#29702;&#21644;&#24037;&#20154;&#20195;&#29702;&#26681;&#25454;&#20840;&#23616;&#30446;&#26631;&#36827;&#34892;&#21327;&#21516;&#35757;&#32451;&#65292;&#20197;&#26368;&#22823;&#21270;&#25315;&#36135;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2212.11498</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#20179;&#24211;&#29289;&#27969;&#20013;&#19982;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#21516;&#20107;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Scalable Multi-Agent Reinforcement Learning for Warehouse Logistics with Robotic and Human Co-Workers. (arXiv:2212.11498v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11498
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20179;&#24211;&#29289;&#27969;&#20013;&#30340;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#21516;&#20107;&#21512;&#20316;&#12290;&#20182;&#20204;&#36890;&#36807;&#20998;&#23618;&#30340;MARL&#31639;&#27861;&#65292;&#35753;&#32463;&#29702;&#21644;&#24037;&#20154;&#20195;&#29702;&#26681;&#25454;&#20840;&#23616;&#30446;&#26631;&#36827;&#34892;&#21327;&#21516;&#35757;&#32451;&#65292;&#20197;&#26368;&#22823;&#21270;&#25315;&#36135;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#24819;&#19968;&#20010;&#20179;&#24211;&#37324;&#26377;&#25968;&#21313;&#20010;&#31227;&#21160;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#20998;&#25315;&#21592;&#19968;&#36215;&#24037;&#20316;&#65292;&#25910;&#38598;&#21644;&#20132;&#20184;&#20179;&#24211;&#20869;&#30340;&#29289;&#21697;&#12290;&#25105;&#20204;&#35201;&#35299;&#20915;&#30340;&#22522;&#26412;&#38382;&#39064;&#26159;&#31216;&#20026;&#25315;&#36135;&#38382;&#39064;&#65292;&#21363;&#36825;&#20123;&#24037;&#20316;&#20195;&#29702;&#20154;&#22914;&#20309;&#22312;&#20179;&#24211;&#20013;&#21327;&#35843;&#20182;&#20204;&#30340;&#31227;&#21160;&#21644;&#34892;&#20026;&#20197;&#26368;&#22823;&#21270;&#24615;&#33021;&#65288;&#20363;&#22914;&#35746;&#21333;&#21534;&#21520;&#37327;&#65289;&#12290;&#20256;&#32479;&#30340;&#34892;&#19994;&#26041;&#27861;&#20351;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#24037;&#31243;&#21162;&#21147;&#26469;&#20026;&#22266;&#26377;&#21487;&#21464;&#30340;&#20179;&#24211;&#37197;&#32622;&#36827;&#34892;&#20248;&#21270;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#21487;&#20197;&#28789;&#27963;&#22320;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#20179;&#24211;&#37197;&#32622;&#65288;&#20363;&#22914;&#22823;&#23567;&#65292;&#24067;&#23616;&#65292;&#24037;&#20154;&#25968;&#37327;/&#31867;&#22411;&#65292;&#29289;&#21697;&#34917;&#20805;&#39057;&#29575;&#65289;&#65292;&#22240;&#20026;&#20195;&#29702;&#20154;&#36890;&#36807;&#32463;&#39564;&#23398;&#20064;&#22914;&#20309;&#26368;&#20248;&#22320;&#30456;&#20114;&#21512;&#20316;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20998;&#23618;MARL&#31639;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;&#31649;&#29702;&#32773;&#20026;&#24037;&#20154;&#20195;&#29702;&#20998;&#37197;&#30446;&#26631;&#65292;&#24182;&#19988;&#31649;&#29702;&#32773;&#21644;&#24037;&#20154;&#30340;&#31574;&#30053;&#34987;&#20849;&#21516;&#35757;&#32451;&#20197;&#26368;&#22823;&#21270;&#20840;&#23616;&#30446;&#26631;&#65288;&#20363;&#22914;&#25315;&#36135;&#36895;&#29575;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We envision a warehouse in which dozens of mobile robots and human pickers work together to collect and deliver items within the warehouse. The fundamental problem we tackle, called the order-picking problem, is how these worker agents must coordinate their movement and actions in the warehouse to maximise performance (e.g. order throughput). Established industry methods using heuristic approaches require large engineering efforts to optimise for innately variable warehouse configurations. In contrast, multi-agent reinforcement learning (MARL) can be flexibly applied to diverse warehouse configurations (e.g. size, layout, number/types of workers, item replenishment frequency), as the agents learn through experience how to optimally cooperate with one another. We develop hierarchical MARL algorithms in which a manager assigns goals to worker agents, and the policies of the manager and workers are co-trained toward maximising a global objective (e.g. pick rate). Our hierarchical algorith
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;CLIP&#30340;&#32452;&#21512;&#24615;&#33021;&#21147;&#20197;&#21450;&#20197;&#32467;&#26500;&#25935;&#24863;&#30340;&#26041;&#24335;&#25414;&#32465;&#21464;&#37327;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#20854;&#33021;&#22815;&#22312;&#21333;&#19968;&#23545;&#35937;&#30340;&#24773;&#20917;&#19979;&#32452;&#21512;&#27010;&#24565;&#65292;&#20294;&#22312;&#38656;&#35201;&#27010;&#24565;&#25414;&#32465;&#30340;&#24773;&#20917;&#19979;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2212.10537</link><description>&lt;p&gt;
CLIP&#26159;&#21542;&#25414;&#32465;&#27010;&#24565;&#65311;&#25506;&#32034;&#22823;&#22411;&#22270;&#20687;&#27169;&#22411;&#30340;&#32452;&#21512;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Does CLIP Bind Concepts? Probing Compositionality in Large Image Models. (arXiv:2212.10537v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;CLIP&#30340;&#32452;&#21512;&#24615;&#33021;&#21147;&#20197;&#21450;&#20197;&#32467;&#26500;&#25935;&#24863;&#30340;&#26041;&#24335;&#25414;&#32465;&#21464;&#37327;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#20854;&#33021;&#22815;&#22312;&#21333;&#19968;&#23545;&#35937;&#30340;&#24773;&#20917;&#19979;&#32452;&#21512;&#27010;&#24565;&#65292;&#20294;&#22312;&#38656;&#35201;&#27010;&#24565;&#25414;&#32465;&#30340;&#24773;&#20917;&#19979;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#32467;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#32534;&#30721;&#20102;&#23427;&#20204;&#25805;&#20316;&#30340;&#27010;&#24565;&#30340;&#32452;&#25104;&#24615;&#34920;&#31034;&#65292;&#22914;&#36890;&#36807;&#23545;&#8220;&#32418;&#33394;&#31435;&#26041;&#20307;&#8221;&#36827;&#34892;&#25512;&#29702;&#20197;&#27491;&#30830;&#35782;&#21035;&#8220;&#32418;&#33394;&#8221;&#21644;&#8220;&#31435;&#26041;&#20307;&#8221;&#36825;&#20123;&#25104;&#20998;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#20851;&#27880;&#19968;&#20010;&#22823;&#22411;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;CLIP&#65289;&#32534;&#30721;&#32452;&#21512;&#27010;&#24565;&#30340;&#33021;&#21147;&#20197;&#21450;&#20197;&#32467;&#26500;&#25935;&#24863;&#30340;&#26041;&#24335;&#25414;&#32465;&#21464;&#37327;&#30340;&#33021;&#21147;&#65288;&#20363;&#22914;&#21306;&#20998;&#8220;&#31435;&#26041;&#20307;&#22312;&#29699;&#20307;&#21518;&#38754;&#8221;&#21644;&#8220;&#29699;&#20307;&#22312;&#31435;&#26041;&#20307;&#21518;&#38754;&#8221;&#65289;&#12290;&#20026;&#20102;&#26816;&#26597;CLIP&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#35768;&#22810;&#26469;&#33258;&#32452;&#21512;&#20998;&#24067;&#35821;&#20041;&#27169;&#22411;&#65288;CDSMs&#65289;&#30340;&#26550;&#26500;&#65292;&#36825;&#26159;&#19968;&#31181;&#35797;&#22270;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#23454;&#29616;&#20256;&#32479;&#32452;&#21512;&#35821;&#35328;&#32467;&#26500;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#21457;&#29616;CLIP&#33021;&#22815;&#22312;&#21333;&#19968;&#23545;&#35937;&#30340;&#24773;&#20917;&#19979;&#32452;&#21512;&#27010;&#24565;&#65292;&#20294;&#22312;&#38656;&#35201;&#27010;&#24565;&#25414;&#32465;&#30340;&#24773;&#20917;&#19979;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20984;&#26174;&#20102;&#35780;&#20272;&#22823;&#22411;&#27169;&#22411;&#32452;&#21512;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20986;&#20102;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale neural network models combining text and images have made incredible progress in recent years. However, it remains an open question to what extent such models encode compositional representations of the concepts over which they operate, such as correctly identifying ''red cube'' by reasoning over the constituents ''red'' and ''cube''. In this work, we focus on the ability of a large pretrained vision and language model (CLIP) to encode compositional concepts and to bind variables in a structure-sensitive way (e.g., differentiating ''cube behind sphere'' from ''sphere behind cube''). In order to inspect the performance of CLIP, we compare several architectures from research on compositional distributional semantics models (CDSMs), a line of research that attempts to implement traditional compositional linguistic structures within embedding spaces. We find that CLIP can compose concepts in a single-object setting, but in situations where concept binding is needed, performance
&lt;/p&gt;</description></item></channel></rss>