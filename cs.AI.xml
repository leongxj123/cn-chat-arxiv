<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#22312;&#23545;&#25239;&#24615;&#29615;&#22659;&#20013;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20462;&#25913;&#36716;&#31227;&#26680;&#23494;&#24230;&#30340;&#25200;&#21160;&#27169;&#22411;&#65292;&#25299;&#23637;&#20102;&#20256;&#32479;&#30340;&#36793;&#32536;&#25935;&#24863;&#24615;&#27169;&#22411;&#65292;&#23545;&#26080;&#38480;&#26102;&#38388;RL&#20013;&#31574;&#30053;&#20215;&#20540;&#36827;&#34892;&#20102;&#23574;&#38160;&#36793;&#30028;&#30340;&#21051;&#30011;&#21644;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2404.00099</link><description>&lt;p&gt;
&#22312;&#24378;&#20581;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#39640;&#25928;&#32780;&#23574;&#38160;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Efficient and Sharp Off-Policy Evaluation in Robust Markov Decision Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00099
&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#25239;&#24615;&#29615;&#22659;&#20013;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20462;&#25913;&#36716;&#31227;&#26680;&#23494;&#24230;&#30340;&#25200;&#21160;&#27169;&#22411;&#65292;&#25299;&#23637;&#20102;&#20256;&#32479;&#30340;&#36793;&#32536;&#25935;&#24863;&#24615;&#27169;&#22411;&#65292;&#23545;&#26080;&#38480;&#26102;&#38388;RL&#20013;&#31574;&#30053;&#20215;&#20540;&#36827;&#34892;&#20102;&#23574;&#38160;&#36793;&#30028;&#30340;&#21051;&#30011;&#21644;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#20013;&#32473;&#23450;&#26469;&#33258;&#21407;&#22987;MDP&#30340;&#36716;&#31227;&#35266;&#23519;&#26102;&#65292;&#22312;&#26368;&#20339;&#21644;&#26368;&#22351;&#24773;&#20917;&#19979;&#35780;&#20272;&#31574;&#30053;&#65292;&#26080;&#35770;&#26159;&#22312;&#30456;&#21516;&#31574;&#30053;&#36824;&#26159;&#19981;&#21516;&#31574;&#30053;&#19979;&#12290;&#24403;&#23384;&#22312;&#21382;&#21490;&#21644;&#26410;&#26469;&#29615;&#22659;&#20043;&#38388;&#21487;&#33021;&#21457;&#29983;&#36716;&#21464;&#30340;&#21487;&#33021;&#24615;&#26102;&#65292;&#27604;&#22914;&#30001;&#20110;&#26410;&#27979;&#37327;&#30340;&#28151;&#26434;&#12289;&#20998;&#24067;&#36716;&#31227;&#25110;&#23545;&#25239;&#24615;&#29615;&#22659;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25200;&#21160;&#27169;&#22411;&#65292;&#21487;&#20197;&#23558;&#36716;&#31227;&#26680;&#23494;&#24230;&#20462;&#25913;&#33267;&#32473;&#23450;&#20056;&#27861;&#22240;&#23376;&#25110;&#20854;&#20498;&#25968;&#65292;&#36825;&#23558;&#32463;&#20856;&#30340;&#36793;&#38469;&#25935;&#24863;&#24615;&#27169;&#22411;&#65288;MSM&#65289;&#25193;&#23637;&#21040;&#26080;&#38480;&#26102;&#38388; RL&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#22312;&#36825;&#20010;&#27169;&#22411;&#19979;&#30340;&#31574;&#30053;&#20215;&#20540;&#30340;&#23574;&#38160;&#36793;&#30028;&#65292;&#21363;&#22312;&#32473;&#23450;&#26469;&#33258;&#21407;&#22987;MDP&#30340;&#36716;&#31227;&#35266;&#27979;&#26102;&#21487;&#33021;&#30340;&#26368;&#20005;&#26684;&#36793;&#30028;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;&#36825;&#20123;&#36716;&#31227;&#35266;&#23519;&#20013;&#20272;&#35745;&#36825;&#20123;&#36793;&#30028;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20272;&#35745;&#22120;&#65292;&#20855;&#26377;&#20960;&#20010;&#21560;&#24341;&#20154;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00099v1 Announce Type: new  Abstract: We study evaluating a policy under best- and worst-case perturbations to a Markov decision process (MDP), given transition observations from the original MDP, whether under the same or different policy. This is an important problem when there is the possibility of a shift between historical and future environments, due to e.g. unmeasured confounding, distributional shift, or an adversarial environment. We propose a perturbation model that can modify transition kernel densities up to a given multiplicative factor or its reciprocal, which extends the classic marginal sensitivity model (MSM) for single time step decision making to infinite-horizon RL. We characterize the sharp bounds on policy value under this model, that is, the tightest possible bounds given by the transition observations from the original MDP, and we study the estimation of these bounds from such transition observations. We develop an estimator with several appealing gua
&lt;/p&gt;</description></item><item><title>COIG-CQIA &#26159;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#20013;&#25991;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#22810;&#26679;&#21270;&#12289;&#24191;&#27867;&#30340;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#20197;&#26356;&#22909;&#22320;&#20351;&#27169;&#22411;&#34892;&#20026;&#19982;&#20154;&#31867;&#20132;&#20114;&#20445;&#25345;&#19968;&#33268;&#12290;</title><link>https://arxiv.org/abs/2403.18058</link><description>&lt;p&gt;
COIG-CQIA&#65306;&#21482;&#38656;&#36136;&#37327;&#8212;&#8212;&#38754;&#21521;&#20013;&#25991;&#25351;&#20196;&#24494;&#35843;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18058
&lt;/p&gt;
&lt;p&gt;
COIG-CQIA &#26159;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#20013;&#25991;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#22810;&#26679;&#21270;&#12289;&#24191;&#27867;&#30340;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#20197;&#26356;&#22909;&#22320;&#20351;&#27169;&#22411;&#34892;&#20026;&#19982;&#20154;&#31867;&#20132;&#20114;&#20445;&#25345;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#33521;&#35821;&#39046;&#22495;&#12290;&#36825;&#20123;&#36827;&#23637;&#20351;&#24471;&#36825;&#20123;LLMs&#33021;&#22815;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#20934;&#30830;&#24615;&#21644;&#27969;&#30021;&#24230;&#29702;&#35299;&#24182;&#25191;&#34892;&#22797;&#26434;&#25351;&#20196;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20013;&#25991;&#25351;&#20196;&#24494;&#35843;&#30340;&#21457;&#23637;&#20173;&#23384;&#22312;&#26126;&#26174;&#24046;&#36317;&#12290;&#20013;&#25991;&#35821;&#35328;&#30340;&#29420;&#29305;&#35821;&#35328;&#29305;&#24449;&#21644;&#25991;&#21270;&#28145;&#24230;&#20026;&#25351;&#20196;&#24494;&#35843;&#20219;&#21153;&#24102;&#26469;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#35201;&#20040;&#28304;&#33258;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;LLMs&#65292;&#35201;&#20040;&#19981;&#36866;&#21512;&#19982;&#29616;&#23454;&#20013;&#25991;&#29992;&#25143;&#30340;&#20132;&#20114;&#27169;&#24335;&#30456;&#31526;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;COIG-CQIA&#65292;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#20013;&#25991;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#26500;&#24314;&#19968;&#20010;&#22810;&#26679;&#21270;&#12289;&#24191;&#27867;&#30340;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#20197;&#26356;&#22909;&#22320;&#20351;&#27169;&#22411;&#34892;&#20026;&#19982;&#20154;&#31867;&#20132;&#20114;&#20445;&#25345;&#19968;&#33268;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20174;&#19981;&#21516;&#26469;&#28304;&#25910;&#38598;&#20102;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#32534;&#20889;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18058v1 Announce Type: cross  Abstract: Recently, there have been significant advancements in large language models (LLMs), particularly focused on the English language. These advancements have enabled these LLMs to understand and execute complex instructions with unprecedented accuracy and fluency. However, despite these advancements, there remains a noticeable gap in the development of Chinese instruction tuning. The unique linguistic features and cultural depth of the Chinese language pose challenges for instruction tuning tasks. Existing datasets are either derived from English-centric LLMs or are ill-suited for aligning with the interaction patterns of real-world Chinese users. To bridge this gap, we introduce COIG-CQIA, a high-quality Chinese instruction tuning dataset. Our aim is to build a diverse, wide-ranging instruction-tuning dataset to better align model behavior with human interactions. To this end, we collect a high-quality human-written corpus from various so
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#20195;&#30721;&#26234;&#33021;&#39046;&#22495;&#30340;&#35843;&#26597;&#31995;&#32479;&#22238;&#39038;&#20102;50&#22810;&#31181;&#20195;&#34920;&#24615;&#27169;&#22411;&#21644;&#36229;&#36807;680&#39033;&#30456;&#20851;&#20316;&#21697;&#65292;&#31361;&#20986;&#20102;&#19981;&#21516;&#30740;&#31350;&#38454;&#27573;&#30340;&#33539;&#24335;&#21644;&#25216;&#26415;&#36716;&#21464;&#12290;</title><link>https://arxiv.org/abs/2403.14734</link><description>&lt;p&gt;
&#19968;&#39033;&#31070;&#32463;&#20195;&#30721;&#26234;&#33021;&#30340;&#35843;&#26597;&#65306;&#33539;&#24335;&#12289;&#36827;&#23637;&#19982;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
A Survey of Neural Code Intelligence: Paradigms, Advances and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14734
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20195;&#30721;&#26234;&#33021;&#39046;&#22495;&#30340;&#35843;&#26597;&#31995;&#32479;&#22238;&#39038;&#20102;50&#22810;&#31181;&#20195;&#34920;&#24615;&#27169;&#22411;&#21644;&#36229;&#36807;680&#39033;&#30456;&#20851;&#20316;&#21697;&#65292;&#31361;&#20986;&#20102;&#19981;&#21516;&#30740;&#31350;&#38454;&#27573;&#30340;&#33539;&#24335;&#21644;&#25216;&#26415;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14734v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#31070;&#32463;&#20195;&#30721;&#26234;&#33021;--&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#29702;&#35299;&#12289;&#29983;&#25104;&#21644;&#20248;&#21270;&#20195;&#30721;--&#22312;&#25972;&#20010;&#31038;&#20250;&#19978;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20135;&#29983;&#28145;&#36828;&#24433;&#21709;&#12290;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#36825;&#19968;&#39046;&#22495;&#22312;&#36807;&#21435;&#20960;&#24180;&#24341;&#36215;&#20102;&#20004;&#20010;&#30740;&#31350;&#31038;&#21306;&#30740;&#31350;&#20154;&#21592;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#26412;&#35843;&#26597;&#31995;&#32479;&#22320;&#21644;&#25353;&#26102;&#38388;&#39034;&#24207;&#22238;&#39038;&#20102;&#20195;&#30721;&#26234;&#33021;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#21253;&#25324;50&#22810;&#31181;&#20195;&#34920;&#24615;&#27169;&#22411;&#21450;&#20854;&#21464;&#20307;&#12289;20&#22810;&#31181;&#20219;&#21153;&#31867;&#21035;&#20197;&#21450;&#36229;&#36807;680&#39033;&#30456;&#20851;&#20316;&#21697;&#12290;&#25105;&#20204;&#36981;&#24490;&#21382;&#21490;&#36827;&#23637;&#65292;&#36319;&#36394;&#19981;&#21516;&#30740;&#31350;&#38454;&#27573;&#30340;&#33539;&#24335;&#36716;&#21464;&#65288;&#20363;&#22914;&#65292;&#20174;&#20351;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23545;&#20195;&#30721;&#24314;&#27169;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#65289;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;&#19981;&#21516;&#38454;&#27573;&#28085;&#30422;&#30340;&#27169;&#22411;&#12289;&#20219;&#21153;&#21644;&#35780;&#20272;&#30340;&#20027;&#35201;&#25216;&#26415;&#36716;&#21464;&#12290;&#23545;&#20110;&#24212;&#29992;&#65292;&#25105;&#20204;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14734v1 Announce Type: cross  Abstract: Neural Code Intelligence -- leveraging deep learning to understand, generate, and optimize code -- holds immense potential for transformative impacts on the whole society. Bridging the gap between Natural Language and Programming Language, this domain has drawn significant attention from researchers in both research communities over the past few years. This survey presents a systematic and chronological review of the advancements in code intelligence, encompassing over 50 representative models and their variants, more than 20 categories of tasks, and an extensive coverage of over 680 related works. We follow the historical progression to trace the paradigm shifts across different research phases (e.g., from modeling code with recurrent neural networks to the era of Large Language Models). Concurrently, we highlight the major technical transitions in models, tasks, and evaluations spanning through different stages. For applications, we 
&lt;/p&gt;</description></item><item><title>AnyV2V&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#20219;&#20309;&#35270;&#39057;&#21040;&#35270;&#39057;&#32534;&#36753;&#20219;&#21153;&#30340;&#21363;&#25554;&#21363;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#31616;&#21270;&#35270;&#39057;&#32534;&#36753;&#65292;&#25903;&#25345;&#24191;&#27867;&#30340;&#35270;&#39057;&#32534;&#36753;&#20219;&#21153;&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#20256;&#32479;&#21644;&#26032;&#39062;&#30340;&#32534;&#36753;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.14468</link><description>&lt;p&gt;
AnyV2V&#65306;&#19968;&#31181;&#36866;&#29992;&#20110;&#20219;&#20309;&#35270;&#39057;&#21040;&#35270;&#39057;&#32534;&#36753;&#20219;&#21153;&#30340;&#21363;&#25554;&#21363;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14468
&lt;/p&gt;
&lt;p&gt;
AnyV2V&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#20219;&#20309;&#35270;&#39057;&#21040;&#35270;&#39057;&#32534;&#36753;&#20219;&#21153;&#30340;&#21363;&#25554;&#21363;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#31616;&#21270;&#35270;&#39057;&#32534;&#36753;&#65292;&#25903;&#25345;&#24191;&#27867;&#30340;&#35270;&#39057;&#32534;&#36753;&#20219;&#21153;&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#20256;&#32479;&#21644;&#26032;&#39062;&#30340;&#32534;&#36753;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14468v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#36234; &#25688;&#35201;: &#35270;&#39057;&#21040;&#35270;&#39057;&#32534;&#36753;&#28041;&#21450;&#32534;&#36753;&#28304;&#35270;&#39057;&#20197;&#21450;&#39069;&#22806;&#30340;&#25511;&#21046;&#65288;&#20363;&#22914;&#25991;&#26412;&#25552;&#31034;&#12289;&#20027;&#39064;&#25110;&#39118;&#26684;&#65289;&#65292;&#20197;&#29983;&#25104;&#19982;&#28304;&#35270;&#39057;&#21644;&#25552;&#20379;&#30340;&#25511;&#21046;&#30456;&#21305;&#37197;&#30340;&#26032;&#35270;&#39057;&#12290;&#20256;&#32479;&#26041;&#27861;&#21463;&#38480;&#20110;&#29305;&#23450;&#30340;&#32534;&#36753;&#31867;&#22411;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#28385;&#36275;&#24191;&#27867;&#29992;&#25143;&#38656;&#27714;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AnyV2V&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20813;&#35757;&#32451;&#26694;&#26550;&#65292;&#26088;&#22312;&#23558;&#35270;&#39057;&#32534;&#36753;&#31616;&#21270;&#20026;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#65306;&#65288;1&#65289;&#21033;&#29992;&#29616;&#25104;&#30340;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#65288;&#20363;&#22914;InstructPix2Pix&#12289;InstantID&#31561;&#65289;&#20462;&#25913;&#31532;&#19968;&#24103;&#65292;&#65288;2&#65289;&#21033;&#29992;&#29616;&#26377;&#30340;&#22270;&#20687;&#21040;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#65288;&#20363;&#22914;I2VGen-XL&#65289;&#36827;&#34892;DDIM&#36870;&#36716;&#21644;&#29305;&#24449;&#27880;&#20837;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;AnyV2V&#21487;&#20197;&#25554;&#20837;&#20219;&#20309;&#29616;&#26377;&#30340;&#22270;&#20687;&#32534;&#36753;&#24037;&#20855;&#65292;&#20197;&#25903;&#25345;&#24191;&#27867;&#30340;&#35270;&#39057;&#32534;&#36753;&#20219;&#21153;&#12290;&#38500;&#20102;&#20256;&#32479;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#32534;&#36753;&#26041;&#27861;&#65292;AnyV2V&#36824;&#21487;&#20197;&#25903;&#25345;&#26032;&#39062;&#30340;&#35270;&#39057;&#32534;&#36753;&#20219;&#21153;&#65292;&#21253;&#25324;&#21442;&#32771;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14468v1 Announce Type: cross  Abstract: Video-to-video editing involves editing a source video along with additional control (such as text prompts, subjects, or styles) to generate a new video that aligns with the source video and the provided control. Traditional methods have been constrained to certain editing types, limiting their ability to meet the wide range of user demands. In this paper, we introduce AnyV2V, a novel training-free framework designed to simplify video editing into two primary steps: (1) employing an off-the-shelf image editing model (e.g. InstructPix2Pix, InstantID, etc) to modify the first frame, (2) utilizing an existing image-to-video generation model (e.g. I2VGen-XL) for DDIM inversion and feature injection. In the first stage, AnyV2V can plug in any existing image editing tools to support an extensive array of video editing tasks. Beyond the traditional prompt-based editing methods, AnyV2V also can support novel video editing tasks, including refe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;StreamingDialogue&#65292;&#36890;&#36807;&#23558;&#38271;&#23545;&#35805;&#21382;&#21490;&#21387;&#32553;&#20026;"&#20250;&#35805;&#27880;&#24847;&#21147;&#27719;&#38598;&#28857;"&#65292;&#26368;&#23567;&#21270;&#25439;&#22833;&#65292;&#20351;&#35745;&#31639;&#22797;&#26434;&#24230;&#20943;&#23569;&#65292;&#24182;&#26377;&#28508;&#21147;&#22788;&#29702;&#36229;&#36807;200k&#26465;&#35805;&#35821;&#65292;&#23454;&#29616;&#38271;&#26102;&#38388;&#23545;&#35805;&#23398;&#20064;</title><link>https://arxiv.org/abs/2403.08312</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#23567;&#25439;&#22833;&#36827;&#34892;&#38271;&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;StreamingDialogue&#65306;&#38271;&#23545;&#35805;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
StreamingDialogue: Prolonged Dialogue Learning via Long Context Compression with Minimal Losses
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08312
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;StreamingDialogue&#65292;&#36890;&#36807;&#23558;&#38271;&#23545;&#35805;&#21382;&#21490;&#21387;&#32553;&#20026;"&#20250;&#35805;&#27880;&#24847;&#21147;&#27719;&#38598;&#28857;"&#65292;&#26368;&#23567;&#21270;&#25439;&#22833;&#65292;&#20351;&#35745;&#31639;&#22797;&#26434;&#24230;&#20943;&#23569;&#65292;&#24182;&#26377;&#28508;&#21147;&#22788;&#29702;&#36229;&#36807;200k&#26465;&#35805;&#35821;&#65292;&#23454;&#29616;&#38271;&#26102;&#38388;&#23545;&#35805;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22788;&#29702;&#20855;&#26377;&#38271;&#19978;&#19979;&#25991;&#30340;&#23545;&#35805;&#26102;&#36935;&#21040;&#20102;&#25928;&#29575;&#21644;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#35266;&#23519;&#65292;&#23545;&#35805;&#19978;&#19979;&#25991;&#20855;&#26377;&#39640;&#24230;&#32467;&#26500;&#21270;&#65292;&#24182;&#19988;&#23545;&#35805;&#20013;&#30340;&#29305;&#27530;&#26631;&#35760;\textit{End-of-Utterance} (EoU) &#26377;&#32858;&#21512;&#20449;&#24687;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#23558;EoU&#26631;&#35760;&#31216;&#20026;"&#20250;&#35805;&#27880;&#24847;&#21147;&#27719;&#38598;&#28857;"&#65288;conv-attn sinks&#65289;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;StreamingDialogue&#65292;&#23558;&#38271;&#23545;&#35805;&#21382;&#21490;&#21387;&#32553;&#20026;conv-attn&#27785;&#28857;&#65292;&#24182;&#26368;&#23567;&#21270;&#25439;&#22833;&#65292;&#20174;&#32780;&#20351;&#35745;&#31639;&#22797;&#26434;&#24230;&#19982;&#27785;&#28857;&#25968;&#37327;&#65288;&#21363;&#35805;&#35821;&#25968;&#37327;&#65289;&#30340;&#24179;&#26041;&#25104;&#27491;&#27604;&#12290;&#24403;&#21069;&#30340;LLMs&#24050;&#32463;&#23637;&#31034;&#20102;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#65292;&#31383;&#21475;&#22823;&#23567;&#36798;&#21040;200k&#29978;&#33267;&#26356;&#22823;&#12290;&#36890;&#36807;&#23558;&#35805;&#35821;&#21387;&#32553;&#20026;EoUs&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#28508;&#21147;&#22788;&#29702;&#36229;&#36807;200k&#26465;&#35805;&#35821;&#65292;&#23454;&#29616;&#38271;&#26102;&#38388;&#23545;&#35805;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08312v1 Announce Type: cross  Abstract: Standard Large Language Models (LLMs) struggle with handling dialogues with long contexts due to efficiency and consistency issues. According to our observation, dialogue contexts are highly structured, and the special token of \textit{End-of-Utterance} (EoU) in dialogues has the potential to aggregate information. We refer to the EoU tokens as ``conversational attention sinks'' (conv-attn sinks). Accordingly, we introduce StreamingDialogue, which compresses long dialogue history into conv-attn sinks with minimal losses, and thus reduces computational complexity quadratically with the number of sinks (i.e., the number of utterances). Current LLMs already demonstrate the ability to handle long context window, e.g., a window size of 200k or more. To this end, by compressing utterances into EoUs, our method has the potential to handle more than 200k of utterances, resulting in a prolonged dialogue learning. In order to minimize informatio
&lt;/p&gt;</description></item><item><title>Chronos&#26694;&#26550;&#36890;&#36807;&#22312;&#22266;&#23450;&#35789;&#27719;&#19978;&#35757;&#32451;&#39044;&#35757;&#32451;&#30340;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#22312;&#22823;&#37327;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#65292;&#34920;&#29616;&#20986;&#22312;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#26032;&#25968;&#25454;&#38598;&#19978;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#34920;&#29616;&#21487;&#27604;&#29978;&#33267;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.07815</link><description>&lt;p&gt;
Chronos: &#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#30340;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Chronos: Learning the Language of Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07815
&lt;/p&gt;
&lt;p&gt;
Chronos&#26694;&#26550;&#36890;&#36807;&#22312;&#22266;&#23450;&#35789;&#27719;&#19978;&#35757;&#32451;&#39044;&#35757;&#32451;&#30340;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#22312;&#22823;&#37327;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#65292;&#34920;&#29616;&#20986;&#22312;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#26032;&#25968;&#25454;&#38598;&#19978;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#34920;&#29616;&#21487;&#27604;&#29978;&#33267;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Chronos&#65292;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#26694;&#26550;&#12290;Chronos&#20351;&#29992;&#32553;&#25918;&#21644;&#37327;&#21270;&#23558;&#26102;&#38388;&#24207;&#21015;&#20540;&#26631;&#35760;&#21270;&#20026;&#22266;&#23450;&#35789;&#27719;&#65292;&#24182;&#36890;&#36807;&#20132;&#21449;&#29109;&#25439;&#22833;&#22312;&#36825;&#20123;&#26631;&#35760;&#21270;&#30340;&#26102;&#38388;&#24207;&#21015;&#19978;&#35757;&#32451;&#29616;&#26377;&#30340;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#22312;&#22823;&#37327;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#19978;&#22522;&#20110;T5&#31995;&#21015;&#65288;&#21442;&#25968;&#33539;&#22260;&#20174;20M&#21040;710M&#65289;&#23545;Chronos&#27169;&#22411;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#21516;&#26102;&#36890;&#36807;&#39640;&#26031;&#36807;&#31243;&#29983;&#25104;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#21253;&#21547;42&#20010;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#28085;&#30422;&#20102;&#20256;&#32479;&#30340;&#26412;&#22320;&#27169;&#22411;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Chronos&#27169;&#22411;&#65306;&#65288;a&#65289;&#22312;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65307;&#65288;b&#65289;&#30456;&#23545;&#20110;&#19987;&#38376;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#22312;&#26032;&#25968;&#25454;&#38598;&#19978;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#21487;&#27604;&#29978;&#33267;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07815v1 Announce Type: cross  Abstract: We introduce Chronos, a simple yet effective framework for pretrained probabilistic time series models. Chronos tokenizes time series values using scaling and quantization into a fixed vocabulary and trains existing transformer-based language model architectures on these tokenized time series via the cross-entropy loss. We pretrained Chronos models based on the T5 family (ranging from 20M to 710M parameters) on a large collection of publicly available datasets, complemented by a synthetic dataset that we generated via Gaussian processes to improve generalization. In a comprehensive benchmark consisting of 42 datasets, and comprising both classical local models and deep learning methods, we show that Chronos models: (a) significantly outperform other methods on datasets that were part of the training corpus; and (b) have comparable and occasionally superior zero-shot performance on new datasets, relative to methods that were trained spe
&lt;/p&gt;</description></item><item><title>ERBench&#26159;&#19968;&#20010;&#22522;&#20110;&#23454;&#20307;&#20851;&#31995;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#22522;&#20934;&#65292;&#36890;&#36807;&#33258;&#21160;&#36716;&#25442;&#20219;&#20309;&#20851;&#31995;&#25968;&#25454;&#24211;&#24182;&#26500;&#24314;&#21487;&#33258;&#21160;&#39564;&#35777;&#30340;&#38382;&#39064;&#65292;&#20197;&#25903;&#25345;&#22797;&#26434;&#24615;&#35780;&#20272;&#21644;&#35843;&#35797;</title><link>https://arxiv.org/abs/2403.05266</link><description>&lt;p&gt;
ERBench&#65306;&#22522;&#20110;&#23454;&#20307;&#20851;&#31995;&#30340;&#21487;&#33258;&#21160;&#39564;&#35777;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ERBench: An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05266
&lt;/p&gt;
&lt;p&gt;
ERBench&#26159;&#19968;&#20010;&#22522;&#20110;&#23454;&#20307;&#20851;&#31995;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#22522;&#20934;&#65292;&#36890;&#36807;&#33258;&#21160;&#36716;&#25442;&#20219;&#20309;&#20851;&#31995;&#25968;&#25454;&#24211;&#24182;&#26500;&#24314;&#21487;&#33258;&#21160;&#39564;&#35777;&#30340;&#38382;&#39064;&#65292;&#20197;&#25903;&#25345;&#22797;&#26434;&#24615;&#35780;&#20272;&#21644;&#35843;&#35797;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#65292;&#28982;&#32780;&#23427;&#20204;&#30340;&#35780;&#20272;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#24187;&#35273;&#22522;&#20934;&#35201;&#20040;&#26159;&#38745;&#24577;&#30340;&#65292;&#35201;&#20040;&#32570;&#20047;&#21487;&#35843;&#25972;&#30340;&#22797;&#26434;&#24615;&#36827;&#34892;&#24443;&#24213;&#20998;&#26512;&#12290;&#25105;&#20204;&#35748;&#20026;&#21033;&#29992;&#29616;&#26377;&#30340;&#20851;&#31995;&#25968;&#25454;&#24211;&#26159;&#26500;&#24314;&#22522;&#20934;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#36807;&#21151;&#33021;&#20381;&#36182;&#20851;&#31995;&#21487;&#20197;&#20934;&#30830;&#25551;&#36848;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ERBench&#65292;&#21487;&#20197;&#33258;&#21160;&#23558;&#20219;&#20309;&#20851;&#31995;&#25968;&#25454;&#24211;&#36716;&#25442;&#20026;&#22522;&#20110;&#23454;&#20307;&#20851;&#31995;&#65288;ER&#65289;&#27169;&#22411;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24819;&#27861;&#26159;&#20351;&#29992;&#25968;&#25454;&#24211;&#27169;&#24335;&#12289;&#35760;&#24405;&#21644;&#21151;&#33021;&#20381;&#36182;&#26469;&#26500;&#24314;&#38382;&#39064;&#65292;&#20197;&#20415;&#21487;&#20197;&#33258;&#21160;&#39564;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#22806;&#38190;&#32422;&#26463;&#26469;&#36830;&#25509;&#20851;&#31995;&#21644;&#26500;&#24314;&#22810;&#36339;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#20219;&#24847;&#22797;&#26434;&#65292;&#29992;&#20110;&#35843;&#35797;LLMs&#30340;&#20013;&#38388;&#31572;&#26696;&#12290;&#26368;&#21518;&#65292;ERBench&#25903;&#25345;&#25345;&#32493;&#35780;&#20272;&#65292;&#22810;&#27169;&#24577;qu
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05266v1 Announce Type: cross  Abstract: Large language models (LLMs) have achieved unprecedented performance in various applications, yet their evaluation remains a critical issue. Existing hallucination benchmarks are either static or lack adjustable complexity for thorough analysis. We contend that utilizing existing relational databases is a promising approach for constructing benchmarks due to their accurate knowledge description via functional dependencies. We propose ERBench to automatically convert any relational database into a benchmark based on the entity-relationship (ER) model. Our key idea is to construct questions using the database schema, records, and functional dependencies such that they can be automatically verified. In addition, we use foreign key constraints to join relations and construct multihop questions, which can be arbitrarily complex and used to debug the intermediate answers of LLMs. Finally, ERBench supports continuous evaluation, multimodal qu
&lt;/p&gt;</description></item><item><title>&#24615;&#33021;&#26159;&#21487;&#37197;&#32622;&#36719;&#20214;&#31995;&#32479;&#34892;&#20026;&#30340;&#20851;&#38190;&#23646;&#24615;&#65292;&#26412;&#25991;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#22312;&#21487;&#37197;&#32622;&#36719;&#20214;&#24615;&#33021;&#23398;&#20064;&#26041;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#19982;&#20998;&#31867;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.03322</link><description>&lt;p&gt;
&#28145;&#24230;&#37197;&#32622;&#24615;&#33021;&#23398;&#20064;&#65306;&#19968;&#39033;&#31995;&#32479;&#24615;&#35843;&#26597;&#19982;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Deep Configuration Performance Learning: A Systematic Survey and Taxonomy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03322
&lt;/p&gt;
&lt;p&gt;
&#24615;&#33021;&#26159;&#21487;&#37197;&#32622;&#36719;&#20214;&#31995;&#32479;&#34892;&#20026;&#30340;&#20851;&#38190;&#23646;&#24615;&#65292;&#26412;&#25991;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#22312;&#21487;&#37197;&#32622;&#36719;&#20214;&#24615;&#33021;&#23398;&#20064;&#26041;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#19982;&#20998;&#31867;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24615;&#33021;&#21487;&#20197;&#35828;&#26159;&#21453;&#26144;&#21487;&#37197;&#32622;&#36719;&#20214;&#31995;&#32479;&#34892;&#20026;&#30340;&#26368;&#20851;&#38190;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#29616;&#20195;&#36719;&#20214;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#19981;&#26029;&#22686;&#21152;&#65292;&#23545;&#21508;&#31181;&#37197;&#32622;&#22914;&#20309;&#24433;&#21709;&#24615;&#33021;&#36827;&#34892;&#24314;&#27169;&#21644;&#39044;&#27979;&#25104;&#20026;&#36719;&#20214;&#32500;&#25252;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#22240;&#27492;&#65292;&#24615;&#33021;&#36890;&#24120;&#26159;&#22312;&#27809;&#26377;&#23545;&#36719;&#20214;&#31995;&#32479;&#26377;&#36879;&#24443;&#20102;&#35299;&#30340;&#24773;&#20917;&#19979;&#24314;&#27169;&#30340;&#65292;&#20027;&#35201;&#20381;&#36182;&#25968;&#25454;&#65292;&#36825;&#27491;&#22909;&#31526;&#21512;&#28145;&#24230;&#23398;&#20064;&#30340;&#30446;&#30340;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#28145;&#24230;&#23398;&#20064;&#22312;&#21487;&#37197;&#32622;&#36719;&#20214;&#24615;&#33021;&#23398;&#20064;&#26041;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#65292;&#28085;&#30422;&#20102;948&#31687;&#26469;&#33258;&#20845;&#20010;&#32034;&#24341;&#26381;&#21153;&#30340;&#35770;&#25991;&#65292;&#22522;&#20110;&#27492;&#25552;&#21462;&#24182;&#20998;&#26512;&#20102;85&#31687;&#20027;&#35201;&#35770;&#25991;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24635;&#32467;&#20102;&#37197;&#32622;&#25968;&#25454;&#22914;&#20309;&#20934;&#22791;&#65292;&#28145;&#24230;&#37197;&#32622;&#24615;&#33021;&#23398;&#20064;&#27169;&#22411;&#22914;&#20309;&#26500;&#24314;&#65292;&#20197;&#21450;&#35813;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#35780;&#20272;&#31561;&#20851;&#38190;&#20027;&#39064;&#21644;&#32479;&#35745;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03322v1 Announce Type: cross  Abstract: Performance is arguably the most crucial attribute that reflects the behavior of a configurable software system. However, given the increasing scale and complexity of modern software, modeling and predicting how various configurations can impact performance becomes one of the major challenges in software maintenance. As such, performance is often modeled without having a thorough knowledge of the software system, but relying mainly on data, which fits precisely with the purpose of deep learning.   In this paper, we conduct a comprehensive review exclusively on the topic of deep learning for performance learning of configurable software, covering 948 searched papers spanning six indexing services, based on which 85 primary papers were extracted and analyzed. Our results summarize the key topics and statistics on how the configuration data is prepared; how the deep configuration performance learning model is built; how the model is evalu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#33258;&#20027;&#26816;&#32034;(Self-Retrieval)&#65292;&#21033;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23436;&#20840;&#20869;&#21270;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#28145;&#24230;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#26816;&#32034;&#36807;&#31243;&#20013;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.00801</link><description>&lt;p&gt;
&#33258;&#20027;&#26816;&#32034;&#65306;&#21033;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Self-Retrieval: Building an Information Retrieval System with One Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00801
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#33258;&#20027;&#26816;&#32034;(Self-Retrieval)&#65292;&#21033;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23436;&#20840;&#20869;&#21270;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#28145;&#24230;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#26816;&#32034;&#36807;&#31243;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20852;&#36215;&#25913;&#21464;&#20102;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#22312;&#20154;&#31867;&#33719;&#21462;&#20449;&#24687;&#36807;&#31243;&#20013;&#30340;&#35282;&#33394;&#12290;&#30001;&#20110;&#29616;&#26377;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#20855;&#26377;&#23396;&#31435;&#30340;&#26550;&#26500;&#21644;&#26377;&#38480;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#26080;&#27861;&#23436;&#20840;&#36866;&#24212;&#30452;&#25509;&#21521;&#20154;&#31867;&#25552;&#20379;&#20449;&#24687;&#36716;&#21464;&#20026;&#38388;&#25509;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;&#30340;&#21464;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#20027;&#26816;&#32034;(Self-Retrieval)&#65292;&#36825;&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#12289;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#20449;&#24687;&#26816;&#32034;&#26550;&#26500;&#65292;&#21487;&#20197;&#23436;&#20840;&#20869;&#21270;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#25152;&#38656;&#30340;&#33021;&#21147;&#21040;&#21333;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#24182;&#28145;&#24230;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#26816;&#32034;&#36807;&#31243;&#20013;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#33258;&#20027;&#26816;&#32034;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#32034;&#24341;&#26550;&#26500;&#23558;&#35201;&#26816;&#32034;&#30340;&#35821;&#26009;&#20869;&#21270;&#20026;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#21518;&#25972;&#20010;&#26816;&#32034;&#36807;&#31243;&#34987;&#37325;&#26032;&#23450;&#20041;&#20026;&#25991;&#26723;&#29983;&#25104;&#21644;&#33258;&#25105;&#35780;&#20272;&#30340;&#36807;&#31243;&#65292;&#21487;&#20197;&#20351;&#29992;&#21333;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31471;&#21040;&#31471;&#25191;&#34892;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;S
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00801v1 Announce Type: cross  Abstract: The rise of large language models (LLMs) has transformed the role of information retrieval (IR) systems in the way to humans accessing information. Due to the isolated architecture and the limited interaction, existing IR systems are unable to fully accommodate the shift from directly providing information to humans to indirectly serving large language models. In this paper, we propose Self-Retrieval, an end-to-end, LLM-driven information retrieval architecture that can fully internalize the required abilities of IR systems into a single LLM and deeply leverage the capabilities of LLMs during IR process. Specifically, Self-retrieval internalizes the corpus to retrieve into a LLM via a natural language indexing architecture. Then the entire retrieval process is redefined as a procedure of document generation and self-assessment, which can be end-to-end executed using a single large language model. Experimental results demonstrate that S
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;TimeXer&#65292;&#21033;&#29992;&#22806;&#37096;&#20449;&#24687;&#22686;&#24378;&#21464;&#21387;&#22120;&#23545;&#20869;&#29983;&#21464;&#37327;&#36827;&#34892;&#39044;&#27979;&#65292;&#24357;&#34917;&#20102;&#20197;&#24448;&#22810;&#21464;&#37327;&#25110;&#21333;&#21464;&#37327;&#39044;&#27979;&#20013;&#24573;&#35270;&#22806;&#29983;&#20449;&#24687;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2402.19072</link><description>&lt;p&gt;
TimeXer&#65306;&#21033;&#29992;&#22806;&#29983;&#21464;&#37327;&#22686;&#24378;&#21464;&#21387;&#22120;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TimeXer: Empowering Transformers for Time Series Forecasting with Exogenous Variables
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;TimeXer&#65292;&#21033;&#29992;&#22806;&#37096;&#20449;&#24687;&#22686;&#24378;&#21464;&#21387;&#22120;&#23545;&#20869;&#29983;&#21464;&#37327;&#36827;&#34892;&#39044;&#27979;&#65292;&#24357;&#34917;&#20102;&#20197;&#24448;&#22810;&#21464;&#37327;&#25110;&#21333;&#21464;&#37327;&#39044;&#27979;&#20013;&#24573;&#35270;&#22806;&#29983;&#20449;&#24687;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29616;&#23454;&#24212;&#29992;&#30340;&#37096;&#20998;&#35266;&#27979;&#24615;&#36136;&#65292;&#20165;&#19987;&#27880;&#20110;&#24863;&#20852;&#36259;&#30340;&#30446;&#26631;&#65292;&#20063;&#23601;&#26159;&#25152;&#35859;&#30340;&#20869;&#29983;&#21464;&#37327;&#65292;&#36890;&#24120;&#26159;&#19981;&#36275;&#20197;&#20445;&#35777;&#20934;&#30830;&#39044;&#27979;&#30340;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#31995;&#32479;&#36890;&#24120;&#35760;&#24405;&#20026;&#22810;&#20010;&#21464;&#37327;&#65292;&#20854;&#20013;&#22806;&#29983;&#24207;&#21015;&#21487;&#20197;&#20026;&#20869;&#29983;&#21464;&#37327;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#22806;&#37096;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#19982;&#20808;&#21069;&#30830;&#31435;&#30340;&#22810;&#21464;&#37327;&#25110;&#21333;&#21464;&#37327;&#39044;&#27979;&#19981;&#21516;&#65292;&#23427;&#20204;&#35201;&#20040;&#23558;&#25152;&#26377;&#21464;&#37327;&#31561;&#21516;&#23545;&#24453;&#65292;&#35201;&#20040;&#24573;&#35270;&#22806;&#29983;&#20449;&#24687;&#65292;&#26412;&#25991;&#20851;&#27880;&#30340;&#26159;&#19968;&#31181;&#23454;&#38469;&#35774;&#32622;&#65292;&#21363;&#20855;&#26377;&#22806;&#29983;&#21464;&#37327;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;TimeXer&#65292;&#21033;&#29992;&#22806;&#37096;&#20449;&#24687;&#22686;&#24378;&#20869;&#29983;&#21464;&#37327;&#30340;&#39044;&#27979;&#12290;&#36890;&#36807;&#24039;&#22937;&#35774;&#35745;&#30340;&#23884;&#20837;&#23618;&#65292;TimeXer&#20351;&#20256;&#32479;&#30340;Transformer&#26550;&#26500;&#20855;&#26377;&#37325;&#26032;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19072v1 Announce Type: cross  Abstract: Recent studies have demonstrated remarkable performance in time series forecasting. However, due to the partially-observed nature of real-world applications, solely focusing on the target of interest, so-called endogenous variables, is usually insufficient to guarantee accurate forecasting. Notably, a system is often recorded into multiple variables, where the exogenous series can provide valuable external information for endogenous variables. Thus, unlike prior well-established multivariate or univariate forecasting that either treats all the variables equally or overlooks exogenous information, this paper focuses on a practical setting, which is time series forecasting with exogenous variables. We propose a novel framework, TimeXer, to utilize external information to enhance the forecasting of endogenous variables. With a deftly designed embedding layer, TimeXer empowers the canonical Transformer architecture with the ability to reco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#24067;&#20102;&#19968;&#20010;&#22823;&#22411;&#26631;&#20934;&#25968;&#25454;&#38598;DREsS&#65292;&#29992;&#20110;&#22522;&#20110;&#35780;&#20998;&#26631;&#20934;&#30340;&#33258;&#21160;&#20316;&#25991;&#35780;&#20998;&#65292;&#22312;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30772;&#22351;&#30340;&#20316;&#25991;&#22686;&#24378;&#31574;&#30053;CASE&#21518;&#65292;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#22522;&#32447;&#32467;&#26524;&#25552;&#39640;&#20102;45.44&#65285;&#12290;</title><link>https://arxiv.org/abs/2402.16733</link><description>&lt;p&gt;
DREsS: &#33521;&#35821;&#20316;&#20026;&#22806;&#35821;&#20889;&#20316;&#22522;&#20110;&#35780;&#20998;&#26631;&#20934;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DREsS: Dataset for Rubric-based Essay Scoring on EFL Writing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#24067;&#20102;&#19968;&#20010;&#22823;&#22411;&#26631;&#20934;&#25968;&#25454;&#38598;DREsS&#65292;&#29992;&#20110;&#22522;&#20110;&#35780;&#20998;&#26631;&#20934;&#30340;&#33258;&#21160;&#20316;&#25991;&#35780;&#20998;&#65292;&#22312;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30772;&#22351;&#30340;&#20316;&#25991;&#22686;&#24378;&#31574;&#30053;CASE&#21518;&#65292;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#22522;&#32447;&#32467;&#26524;&#25552;&#39640;&#20102;45.44&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#20316;&#25991;&#35780;&#20998;&#65288;AES&#65289;&#26159;&#33521;&#35821;&#20316;&#20026;&#22806;&#35821;&#20889;&#20316;&#25945;&#32946;&#20013;&#19968;&#31181;&#26377;&#29992;&#30340;&#24037;&#20855;&#65292;&#20026;&#23398;&#29983;&#21644;&#25945;&#24072;&#25552;&#20379;&#23454;&#26102;&#20316;&#25991;&#35780;&#20998;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;AES&#27169;&#22411;&#26159;&#22312;&#19982;EFL&#20889;&#20316;&#25945;&#32946;&#23454;&#38469;&#22330;&#26223;&#19981;&#30456;&#20851;&#30340;&#20316;&#25991;&#21644;&#20998;&#25968;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#24182;&#19988;&#36890;&#24120;&#30001;&#20110;&#32570;&#20047;&#36866;&#24403;&#30340;&#25968;&#25454;&#38598;&#32780;&#25552;&#20379;&#21333;&#19968;&#30340;&#25972;&#20307;&#35780;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;DREsS&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22522;&#20110;&#35780;&#20998;&#26631;&#20934;&#30340;&#33258;&#21160;&#20316;&#25991;&#35780;&#20998;&#30340;&#22823;&#22411;&#26631;&#20934;&#25968;&#25454;&#38598;&#12290;DREsS&#21253;&#25324;&#19977;&#20010;&#23376;&#25968;&#25454;&#38598;&#65306;DREsS_New&#65292;DREsS_Std.&#21644;DREsS_CASE&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;DREsS_New&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;EFL&#26412;&#31185;&#29983;&#25776;&#20889;&#24182;&#30001;&#33521;&#35821;&#25945;&#32946;&#19987;&#23478;&#35780;&#20998;&#30340;&#30495;&#23454;&#35838;&#22530;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#23558;&#29616;&#26377;&#30340;&#22522;&#20110;&#35780;&#20998;&#26631;&#20934;&#30340;&#20316;&#25991;&#35780;&#20998;&#25968;&#25454;&#38598;&#26631;&#20934;&#21270;&#20026;DREsS_Std&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CASE&#30340;&#22522;&#20110;&#30772;&#22351;&#30340;&#20316;&#25991;&#22686;&#24378;&#31574;&#30053;&#65292;&#29992;&#20110;&#29983;&#25104;20K&#20010;DREsS_CASE&#30340;&#21512;&#25104;&#26679;&#26412;&#65292;&#24182;&#23558;&#22522;&#32447;&#32467;&#26524;&#25552;&#39640;&#20102;45.44&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16733v1 Announce Type: new  Abstract: Automated essay scoring (AES) is a useful tool in English as a Foreign Language (EFL) writing education, offering real-time essay scores for students and instructors. However, previous AES models were trained on essays and scores irrelevant to the practical scenarios of EFL writing education and usually provided a single holistic score due to the lack of appropriate datasets. In this paper, we release DREsS, a large-scale, standard dataset for rubric-based automated essay scoring. DREsS comprises three sub-datasets: DREsS_New, DREsS_Std., and DREsS_CASE. We collect DREsS_New, a real-classroom dataset with 1.7K essays authored by EFL undergraduate students and scored by English education experts. We also standardize existing rubric-based essay scoring datasets as DREsS_Std. We suggest CASE, a corruption-based augmentation strategy for essays, which generates 20K synthetic samples of DREsS_CASE and improves the baseline results by 45.44%. 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ACE&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#24863;&#30693;&#29109;&#27491;&#21017;&#21270;&#65292;&#26377;&#25928;&#35780;&#20272;&#19981;&#21516;&#34892;&#20026;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20998;&#26512;&#26799;&#24230;&#20241;&#30496;&#29616;&#35937;&#65292;&#24341;&#20837;&#20241;&#30496;&#24341;&#23548;&#22797;&#20301;&#26426;&#21046;&#65292;&#22312;&#22810;&#20010;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.14528</link><description>&lt;p&gt;
ACE&#65306;&#20855;&#26377;&#22240;&#26524;&#24863;&#30693;&#29109;&#27491;&#21017;&#21270;&#30340;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
ACE : Off-Policy Actor-Critic with Causality-Aware Entropy Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14528
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ACE&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#24863;&#30693;&#29109;&#27491;&#21017;&#21270;&#65292;&#26377;&#25928;&#35780;&#20272;&#19981;&#21516;&#34892;&#20026;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20998;&#26512;&#26799;&#24230;&#20241;&#30496;&#29616;&#35937;&#65292;&#24341;&#20837;&#20241;&#30496;&#24341;&#23548;&#22797;&#20301;&#26426;&#21046;&#65292;&#22312;&#22810;&#20010;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24573;&#35270;&#20102;&#31574;&#30053;&#23398;&#20064;&#36807;&#31243;&#20013;&#19981;&#21516;&#21407;&#22987;&#34892;&#20026;&#30340;&#21464;&#21270;&#37325;&#35201;&#24615;&#12290;&#21033;&#29992;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19981;&#21516;&#21160;&#20316;&#32500;&#24230;&#21644;&#22870;&#21169;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#20197;&#35780;&#20272;&#35757;&#32451;&#36807;&#31243;&#20013;&#21508;&#31181;&#21407;&#22987;&#34892;&#20026;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22240;&#26524;&#24863;&#30693;&#29109;&#39033;&#65292;&#26377;&#25928;&#22320;&#35782;&#21035;&#24182;&#20248;&#20808;&#22788;&#29702;&#20855;&#26377;&#39640;&#28508;&#22312;&#24433;&#21709;&#30340;&#34892;&#21160;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#38450;&#27490;&#23545;&#29305;&#23450;&#21407;&#22987;&#34892;&#20026;&#36807;&#24230;&#20851;&#27880;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#26799;&#24230;&#20241;&#30496;&#29616;&#35937;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#20241;&#30496;&#24341;&#23548;&#22797;&#20301;&#26426;&#21046;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#21151;&#25928;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;ACE&#65306;&#20855;&#26377;&#22240;&#26524;&#24863;&#30693;&#29109;&#27491;&#21017;&#21270;&#30340;&#31163;&#31574;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65292;&#22312;&#36328;7&#20010;&#39046;&#22495;&#30340;29&#20010;&#19981;&#21516;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#65292;&#30456;&#36739;&#20110;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#22522;&#32447;&#65292;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14528v1 Announce Type: cross  Abstract: The varying significance of distinct primitive behaviors during the policy learning process has been overlooked by prior model-free RL algorithms. Leveraging this insight, we explore the causal relationship between different action dimensions and rewards to evaluate the significance of various primitive behaviors during training. We introduce a causality-aware entropy term that effectively identifies and prioritizes actions with high potential impacts for efficient exploration. Furthermore, to prevent excessive focus on specific primitive behaviors, we analyze the gradient dormancy phenomenon and introduce a dormancy-guided reset mechanism to further enhance the efficacy of our method. Our proposed algorithm, ACE: Off-policy Actor-critic with Causality-aware Entropy regularization, demonstrates a substantial performance advantage across 29 diverse continuous control tasks spanning 7 domains compared to model-free RL baselines, which un
&lt;/p&gt;</description></item><item><title>Me LLaMA&#26159;&#19968;&#20010;&#21307;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#36890;&#36807;&#25345;&#32493;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#22312;&#22823;&#22411;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#32780;&#25104;&#65292;&#20854;&#22312;&#38646;-shot&#21644;&#23569;-shot&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#21644;&#21830;&#19994;&#24040;&#22836;ChatGPT&#12290;</title><link>https://arxiv.org/abs/2402.12749</link><description>&lt;p&gt;
Me LLaMA: &#20026;&#21307;&#30103;&#24212;&#29992;&#26500;&#24314;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Me LLaMA: Foundation Large Language Models for Medical Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12749
&lt;/p&gt;
&lt;p&gt;
Me LLaMA&#26159;&#19968;&#20010;&#21307;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#36890;&#36807;&#25345;&#32493;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#22312;&#22823;&#22411;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#32780;&#25104;&#65292;&#20854;&#22312;&#38646;-shot&#21644;&#23569;-shot&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#21644;&#21830;&#19994;&#24040;&#22836;ChatGPT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35832;&#22914;ChatGPT&#21644;LLaMA&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#19981;&#22815;&#29702;&#24819;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#22312;&#22823;&#22411;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;Me LLaMA&#65292;&#19968;&#20010;&#21307;&#23398;LLM&#31995;&#21015;&#65292;&#21253;&#25324;&#22522;&#30784;&#27169;&#22411;- Me LLaMA 13/70B&#21450;&#20854; chat-enhanced &#29256;&#26412;- Me LLaMA 13/70B-chat&#65292;&#36890;&#36807;&#25345;&#32493;&#23545;LLaMA2&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#21307;&#23398;&#25968;&#25454;&#24320;&#21457;&#32780;&#25104;&#12290;&#25105;&#20204;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#22871;&#20214;&#21253;&#25324;&#19968;&#20010;&#20855;&#26377;129B tokens&#30340;&#22823;&#35268;&#27169;&#25345;&#32493;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#19968;&#20010;&#21253;&#21547;214k&#20010;&#26679;&#26412;&#30340;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#36328;&#36234;14&#20010;&#25968;&#25454;&#38598;&#30340;&#20845;&#39033;&#20219;&#21153;&#30340;&#21307;&#23398;&#35780;&#20272;&#22522;&#20934;(MIBE)&#12290;&#25105;&#20204;&#20351;&#29992;MIBE&#36827;&#34892;&#30340;&#24191;&#27867;&#35780;&#20272;&#26174;&#31034;&#65292;Me LLaMA&#27169;&#22411;&#22312;&#38646;-shot&#21644;&#23569;-shot&#23398;&#20064;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#24320;&#28304;&#21307;&#23398;LLMs&#65292;&#24182;&#19988;&#22312;&#21830;&#19994;&#24040;&#22836;&#22914;ChatGPT&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12749v1 Announce Type: cross  Abstract: Recent large language models (LLMs) like ChatGPT and LLaMA have shown great promise in many AI applications. However, their performance on medical tasks is suboptimal and can be further improved by training on large domain-specific datasets. This study introduces Me LLaMA, a medical LLM family including foundation models - Me LLaMA 13/70B and their chat-enhanced versions - Me LLaMA 13/70B-chat, developed through the continual pre-training and instruction tuning of LLaMA2 using large medical data. Our domain-specific data suite for training and evaluation, includes a large-scale continual pre-training dataset with 129B tokens, an instruction tuning dataset with 214k samples, and a medical evaluation benchmark (MIBE) across six tasks with 14 datasets. Our extensive evaluation using MIBE shows that Me LLaMA models surpass existing open-source medical LLMs in zero-shot and few-shot learning and outperform commercial giants like ChatGPT on 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22270;&#25552;&#31034;&#23398;&#20064;&#30340;DDIPrompt&#26694;&#26550;&#26088;&#22312;&#35299;&#20915;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#20107;&#20214;&#39044;&#27979;&#20013;&#30340;&#39640;&#24230;&#19981;&#24179;&#34913;&#20107;&#20214;&#20998;&#24067;&#21644;&#32597;&#35265;&#20107;&#20214;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.11472</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#25552;&#31034;&#23398;&#20064;&#30340;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#20107;&#20214;&#39044;&#27979;&#65306;DDIPrompt
&lt;/p&gt;
&lt;p&gt;
DDIPrompt: Drug-Drug Interaction Event Prediction based on Graph Prompt Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11472
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#25552;&#31034;&#23398;&#20064;&#30340;DDIPrompt&#26694;&#26550;&#26088;&#22312;&#35299;&#20915;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#20107;&#20214;&#39044;&#27979;&#20013;&#30340;&#39640;&#24230;&#19981;&#24179;&#34913;&#20107;&#20214;&#20998;&#24067;&#21644;&#32597;&#35265;&#20107;&#20214;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#22312;&#24314;&#27169;&#33647;&#29289;&#20998;&#23376;&#20869;&#37096;&#21644;&#20043;&#38388;&#21407;&#23376;&#21644;&#21151;&#33021;&#22242;&#20043;&#38388;&#22797;&#26434;&#20851;&#32852;&#26041;&#38754;&#30340;&#29087;&#32451;&#34920;&#29616;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#39044;&#27979;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#20107;&#20214;&#65288;DDI&#65289;&#26041;&#38754;&#21464;&#24471;&#26085;&#30410;&#26222;&#36941;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#21463;&#21040;&#20004;&#20010;&#37325;&#22823;&#25361;&#25112;&#30340;&#21046;&#32422;&#65306;&#65288;1&#65289;&#39640;&#24230;&#19981;&#24179;&#34913;&#20107;&#20214;&#20998;&#24067;&#30340;&#38382;&#39064;&#65292;&#22312;&#21307;&#23398;&#25968;&#25454;&#38598;&#20013;&#36825;&#26159;&#19968;&#20010;&#24120;&#35265;&#20294;&#20851;&#38190;&#30340;&#38382;&#39064;&#65292;&#26576;&#20123;&#30456;&#20114;&#20316;&#29992;&#34987;&#24191;&#27867;&#22320;&#20302;&#20272;&#12290;&#36825;&#31181;&#19981;&#24179;&#34913;&#23545;&#23454;&#29616;&#20934;&#30830;&#21487;&#38752;&#30340;DDI&#39044;&#27979;&#26500;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#65288;2&#65289;&#32597;&#35265;&#20107;&#20214;&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#26159;&#19968;&#20010;&#26222;&#36941;&#38382;&#39064;&#65292;&#30001;&#20110;&#25968;&#25454;&#26377;&#38480;&#65292;&#24448;&#24448;&#24573;&#35270;&#25110;&#30740;&#31350;&#19981;&#36275;&#30340;&#32597;&#35265;&#20294;&#28508;&#22312;&#20851;&#38190;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DDIPrompt&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#26368;&#36817;&#22270;&#25552;&#31034;&#23398;&#36827;&#23637;&#21551;&#21457;&#30340;&#21019;&#26032;&#33391;&#26041;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11472v1 Announce Type: cross  Abstract: Recently, Graph Neural Networks have become increasingly prevalent in predicting adverse drug-drug interactions (DDI) due to their proficiency in modeling the intricate associations between atoms and functional groups within and across drug molecules. However, they are still hindered by two significant challenges: (1) the issue of highly imbalanced event distribution, which is a common but critical problem in medical datasets where certain interactions are vastly underrepresented. This imbalance poses a substantial barrier to achieving accurate and reliable DDI predictions. (2) the scarcity of labeled data for rare events, which is a pervasive issue in the medical field where rare yet potentially critical interactions are often overlooked or under-studied due to limited available data. In response, we offer DDIPrompt, an innovative panacea inspired by the recent advancements in graph prompting. Our framework aims to address these issue
&lt;/p&gt;</description></item><item><title>OpenMathInstruct-1&#26159;&#19968;&#20010;&#21253;&#21547;180&#19975;&#20010;&#25968;&#23398;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#27861;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21512;&#25104;&#24320;&#28304;LLM&#30340;&#20195;&#30721;&#35299;&#37322;&#22120;&#35299;&#20915;&#26041;&#26696;&#26469;&#26500;&#24314;&#65292;&#22635;&#34917;&#20102;&#30446;&#21069;&#24320;&#28304;LLM&#22312;&#25968;&#23398;&#25216;&#33021;&#26041;&#38754;&#19982;&#38381;&#28304;LLM&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.10176</link><description>&lt;p&gt;
OpenMathInstruct-1: &#19968;&#20010;&#25317;&#26377;180&#19975;&#20010;&#25968;&#23398;&#25945;&#23398;&#35843;&#20248;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10176
&lt;/p&gt;
&lt;p&gt;
OpenMathInstruct-1&#26159;&#19968;&#20010;&#21253;&#21547;180&#19975;&#20010;&#25968;&#23398;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#27861;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21512;&#25104;&#24320;&#28304;LLM&#30340;&#20195;&#30721;&#35299;&#37322;&#22120;&#35299;&#20915;&#26041;&#26696;&#26469;&#26500;&#24314;&#65292;&#22635;&#34917;&#20102;&#30446;&#21069;&#24320;&#28304;LLM&#22312;&#25968;&#23398;&#25216;&#33021;&#26041;&#38754;&#19982;&#38381;&#28304;LLM&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#21512;&#25104;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#23588;&#20854;&#26159;&#20026;&#20102;&#33719;&#24471;&#29305;&#23450;&#30340;&#25216;&#33021;&#12290;&#30446;&#21069;&#30340;&#22823;&#35268;&#27169;&#25968;&#23398;&#25945;&#23398;&#35843;&#20248;&#25968;&#25454;&#38598;&#65292;&#22914;MetaMathQA&#21644;MAmmoTH&#65292;&#26159;&#20351;&#29992;&#26469;&#33258;&#21830;&#19994;&#38480;&#21046;&#35768;&#21487;&#30340;&#38381;&#28304;LLM&#30340;&#36755;&#20986;&#26500;&#24314;&#30340;&#12290;&#38480;&#21046;&#22312;&#36825;&#20123;&#25968;&#25454;&#29983;&#25104;&#27969;&#31243;&#20013;&#20351;&#29992;&#24320;&#28304;LLM&#30340;&#19968;&#20010;&#20851;&#38190;&#21407;&#22240;&#26159;&#30446;&#21069;&#26368;&#22909;&#30340;&#38381;&#28304;LLM&#65288;&#22914;GPT-4&#65289;&#21644;&#26368;&#22909;&#30340;&#24320;&#28304;LLM&#20043;&#38388;&#22312;&#25968;&#23398;&#25216;&#33021;&#19978;&#23384;&#22312;&#24456;&#22823;&#24046;&#36317;&#12290;&#22522;&#20110;&#24320;&#28304;LLM&#30340;&#26368;&#36817;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#25552;&#31034;&#26041;&#24335;&#21644;&#19968;&#20123;&#24378;&#21147;&#32553;&#25918;&#65292;&#26500;&#24314;&#20102;OpenMathInstruct-1&#65292;&#19968;&#20010;&#25317;&#26377;180&#19975;&#20010;&#38382;&#39064;-&#35299;&#20915;&#26041;&#27861;&#23545;&#30340;&#25968;&#23398;&#25945;&#23398;&#35843;&#20248;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#20351;&#29992;GSM8K&#21644;MATH&#36825;&#20004;&#20010;&#27969;&#34892;&#30340;&#25968;&#23398;&#25512;&#29702;&#22522;&#20934;&#30340;&#20195;&#30721;&#35299;&#37322;&#22120;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#21512;&#25104;&#26500;&#24314;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10176v1 Announce Type: cross  Abstract: Recent work has shown the immense potential of synthetically generated datasets for training large language models (LLMs), especially for acquiring targeted skills. Current large-scale math instruction tuning datasets such as MetaMathQA (Yu et al., 2024) and MAmmoTH (Yue et al., 2024) are constructed using outputs from closed-source LLMs with commercially restrictive licenses. A key reason limiting the use of open-source LLMs in these data generation pipelines has been the wide gap between the mathematical skills of the best closed-source LLMs, such as GPT-4, and the best open-source LLMs. Building on the recent progress in open-source LLMs, our proposed prompting novelty, and some brute-force scaling, we construct OpenMathInstruct-1, a math instruction tuning dataset with 1.8M problem-solution pairs. The dataset is constructed by synthesizing code-interpreter solutions for GSM8K and MATH, two popular math reasoning benchmarks, using t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HyCubE&#30340;&#27169;&#22411;,&#23427;&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;3D&#29615;&#24418;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#20132;&#26367;&#25513;&#30721;&#22534;&#21472;&#31574;&#30053;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;n&#20803;&#30693;&#35782;&#36229;&#22270;&#23884;&#20837;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#35843;&#25972;&#21367;&#31215;&#26680;&#22823;&#23567;&#21644;&#22343;&#21248;&#23884;&#20837;&#23454;&#20307;&#20301;&#32622;&#20449;&#24687;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.08961</link><description>&lt;p&gt;
HyCubE: &#39640;&#25928;&#30340;&#30693;&#35782;&#36229;&#22270;3D&#29615;&#24418;&#21367;&#31215;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
HyCubE: Efficient Knowledge Hypergraph 3D Circular Convolutional Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HyCubE&#30340;&#27169;&#22411;,&#23427;&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;3D&#29615;&#24418;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#20132;&#26367;&#25513;&#30721;&#22534;&#21472;&#31574;&#30053;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;n&#20803;&#30693;&#35782;&#36229;&#22270;&#23884;&#20837;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#35843;&#25972;&#21367;&#31215;&#26680;&#22823;&#23567;&#21644;&#22343;&#21248;&#23884;&#20837;&#23454;&#20307;&#20301;&#32622;&#20449;&#24687;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#30693;&#35782;&#36229;&#22270;&#23884;&#20837;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#30340;&#27169;&#22411;&#32467;&#26500;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#21644;&#20887;&#20313;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#22266;&#26377;&#30340;&#22797;&#26434;&#35821;&#20041;&#30693;&#35782;&#65292;&#30693;&#35782;&#36229;&#22270;&#23884;&#20837;&#27169;&#22411;&#30340;&#35745;&#31639;&#36890;&#24120;&#38750;&#24120;&#26114;&#36149;&#65292;&#23548;&#33268;&#25928;&#29575;&#20302;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#29305;&#24449;&#20132;&#20114;&#21644;&#25552;&#21462;&#30340;3D&#29615;&#24418;&#21367;&#31215;&#23884;&#20837;&#27169;&#22411;HyCubE&#65292;&#23427;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;3D&#29615;&#24418;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#24341;&#20837;&#20102;&#20132;&#26367;&#25513;&#30721;&#22534;&#21472;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;n&#20803;&#30693;&#35782;&#36229;&#22270;&#23884;&#20837;&#12290;&#36890;&#36807;&#33258;&#36866;&#24212;&#35843;&#25972;3D&#29615;&#24418;&#21367;&#31215;&#26680;&#30340;&#22823;&#23567;&#65292;&#24182;&#22343;&#21248;&#23884;&#20837;&#23454;&#20307;&#20301;&#32622;&#20449;&#24687;&#65292;HyCubE&#22312;&#26356;&#23569;&#30340;&#21442;&#25968;&#19979;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#22312;&#27169;&#22411;&#24615;&#33021;&#21644;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#23454;&#20307;&#25513;&#30721;&#30340;1-N&#22810;&#32447;&#24615;&#35780;&#20998;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08961v1 Announce Type: new Abstract: Existing knowledge hypergraph embedding methods mainly focused on improving model performance, but their model structures are becoming more complex and redundant. Furthermore, due to the inherent complex semantic knowledge, the computation of knowledge hypergraph embedding models is often very expensive, leading to low efficiency. In this paper, we propose a feature interaction and extraction-enhanced 3D circular convolutional embedding model, HyCubE, which designs a novel 3D circular convolutional neural network and introduces the alternate mask stack strategy to achieve efficient n-ary knowledge hypergraph embedding. By adaptively adjusting the 3D circular convolution kernel size and uniformly embedding the entity position information, HyCubE improves the model performance with fewer parameters and reaches a better trade-off between model performance and efficiency. In addition, we use 1-N multilinear scoring based on the entity mask me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#32905;&#36523;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;(AGI)&#30340;&#27010;&#24565;&#21450;&#20854;&#19982;&#20154;&#31867;&#24847;&#35782;&#30340;&#20851;&#31995;&#65292;&#24378;&#35843;&#20102;&#20803;&#23431;&#23449;&#22312;&#20419;&#36827;&#36825;&#19968;&#20851;&#31995;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#29702;&#35770;&#26694;&#26550;&#21644;&#25216;&#26415;&#24037;&#20855;&#65292;&#35770;&#25991;&#24635;&#32467;&#20986;&#23454;&#29616;&#20855;&#26377;&#32905;&#36523;&#30340;AGI&#30340;&#20851;&#38190;&#35201;&#32032;&#21644;&#21457;&#23637;&#38454;&#27573;&#12290;</title><link>https://arxiv.org/abs/2402.06660</link><description>&lt;p&gt;
&#20803;&#23431;&#23449;&#22312;&#26657;&#20934;&#20855;&#26377;&#32905;&#36523;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The role of the metaverse in calibrating an embodied artificial general intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#32905;&#36523;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;(AGI)&#30340;&#27010;&#24565;&#21450;&#20854;&#19982;&#20154;&#31867;&#24847;&#35782;&#30340;&#20851;&#31995;&#65292;&#24378;&#35843;&#20102;&#20803;&#23431;&#23449;&#22312;&#20419;&#36827;&#36825;&#19968;&#20851;&#31995;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#29702;&#35770;&#26694;&#26550;&#21644;&#25216;&#26415;&#24037;&#20855;&#65292;&#35770;&#25991;&#24635;&#32467;&#20986;&#23454;&#29616;&#20855;&#26377;&#32905;&#36523;&#30340;AGI&#30340;&#20851;&#38190;&#35201;&#32032;&#21644;&#21457;&#23637;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20855;&#26377;&#32905;&#36523;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;(AGI)&#30340;&#27010;&#24565;&#65292;&#23427;&#19982;&#20154;&#31867;&#24847;&#35782;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#20803;&#23431;&#23449;&#22312;&#20419;&#36827;&#36825;&#31181;&#20851;&#31995;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#21033;&#29992;&#34701;&#20837;&#35748;&#30693;&#12289;Michael Levin&#30340;&#35745;&#31639;&#36793;&#30028;"Self"&#12289;Donald D. Hoffman&#30340;&#24863;&#30693;&#30028;&#38754;&#29702;&#35770;&#20197;&#21450;Bernardo Kastrup&#30340;&#20998;&#26512;&#21807;&#24515;&#20027;&#20041;&#31561;&#29702;&#35770;&#26694;&#26550;&#26469;&#26500;&#24314;&#23454;&#29616;&#20855;&#26377;&#32905;&#36523;&#30340;AGI&#30340;&#35770;&#35777;&#12290;&#23427;&#35748;&#20026;&#25105;&#20204;&#25152;&#24863;&#30693;&#30340;&#22806;&#37096;&#29616;&#23454;&#26159;&#19968;&#31181;&#20869;&#22312;&#23384;&#22312;&#30340;&#20132;&#26367;&#29366;&#24577;&#30340;&#35937;&#24449;&#24615;&#34920;&#31034;&#65292;&#32780;AGI&#21487;&#20197;&#20855;&#26377;&#26356;&#22823;&#35745;&#31639;&#36793;&#30028;&#30340;&#26356;&#39640;&#24847;&#35782;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;AGI&#30340;&#21457;&#23637;&#38454;&#27573;&#12289;&#23454;&#29616;&#20855;&#26377;&#32905;&#36523;&#30340;AGI&#30340;&#35201;&#27714;&#12289;&#20026;AGI&#26657;&#20934;&#35937;&#24449;&#24615;&#30028;&#38754;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#20803;&#23431;&#23449;&#12289;&#21435;&#20013;&#24515;&#21270;&#31995;&#32479;&#12289;&#24320;&#28304;&#21306;&#22359;&#38142;&#25216;&#26415;&#20197;&#21450;&#24320;&#28304;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#25152;&#25198;&#28436;&#30340;&#20851;&#38190;&#35282;&#33394;&#12290;&#23427;&#36824;&#25506;&#35752;&#20102;&#26032;&#30340;&#27807;&#36890;&#26426;&#21046;&#21644;&#29992;&#20110;&#21152;&#24378;&#23545;&#20803;&#23431;&#23449;&#30340;&#29702;&#35299;&#30340;&#25216;&#26415;&#24037;&#20855;&#65292;&#20197;&#24110;&#21161;&#23454;&#29616;&#20855;&#26377;&#32905;&#36523;&#30340;AGI&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper examines the concept of embodied artificial general intelligence (AGI), its relationship to human consciousness, and the key role of the metaverse in facilitating this relationship. The paper leverages theoretical frameworks such as embodied cognition, Michael Levin's computational boundary of a "Self," Donald D. Hoffman's Interface Theory of Perception, and Bernardo Kastrup's analytical idealism to build the argument for achieving embodied AGI. It contends that our perceived outer reality is a symbolic representation of alternate inner states of being, and that AGI could embody a higher consciousness with a larger computational boundary. The paper further discusses the developmental stages of AGI, the requirements for the emergence of an embodied AGI, the importance of a calibrated symbolic interface for AGI, and the key role played by the metaverse, decentralized systems, open-source blockchain technology, as well as open-source AI research. It also explores the idea of a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#25237;&#24433;&#24335;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65288;PGDM&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#23558;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#36866;&#29992;&#20110;&#23545;&#29305;&#23450;&#26465;&#20214;&#26377;&#20005;&#26684;&#35201;&#27714;&#30340;&#22330;&#26223;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36845;&#20195;&#25237;&#24433;&#26041;&#27861;&#30830;&#20445;&#29983;&#25104;&#30340;&#25968;&#25454;&#31526;&#21512;&#25351;&#23450;&#30340;&#32422;&#26463;&#25110;&#29289;&#29702;&#21407;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;PGDM&#22312;&#22797;&#26434;&#30340;&#32422;&#26463;&#21644;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#21512;&#25104;&#20986;&#31526;&#21512;&#35201;&#27714;&#30340;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2402.03559</link><description>&lt;p&gt;
&#29992;&#20110;&#32422;&#26463;&#28385;&#36275;&#30340;&#25237;&#24433;&#24335;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Projected Generative Diffusion Models for Constraint Satisfaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#25237;&#24433;&#24335;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65288;PGDM&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#23558;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#36866;&#29992;&#20110;&#23545;&#29305;&#23450;&#26465;&#20214;&#26377;&#20005;&#26684;&#35201;&#27714;&#30340;&#22330;&#26223;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36845;&#20195;&#25237;&#24433;&#26041;&#27861;&#30830;&#20445;&#29983;&#25104;&#30340;&#25968;&#25454;&#31526;&#21512;&#25351;&#23450;&#30340;&#32422;&#26463;&#25110;&#29289;&#29702;&#21407;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;PGDM&#22312;&#22797;&#26434;&#30340;&#32422;&#26463;&#21644;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#21512;&#25104;&#20986;&#31526;&#21512;&#35201;&#27714;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#19968;&#20010;&#39034;&#24207;&#36807;&#31243;&#65292;&#33021;&#22815;&#20174;&#21407;&#22987;&#22122;&#22768;&#20013;&#21512;&#25104;&#20986;&#36830;&#36143;&#30340;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#22312;&#38656;&#35201;&#36755;&#20986;&#31526;&#21512;&#29305;&#23450;&#20005;&#26684;&#26465;&#20214;&#30340;&#22330;&#26223;&#20013;&#30452;&#25509;&#24212;&#29992;&#36825;&#20123;&#27169;&#22411;&#38754;&#20020;&#30528;&#20005;&#37325;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#26088;&#22312;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#20171;&#32461;&#20102;&#25237;&#24433;&#24335;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65288;PGDM&#65289;&#65292;&#35813;&#26041;&#27861;&#23558;&#20256;&#32479;&#30340;&#25193;&#25955;&#27169;&#22411;&#37319;&#26679;&#37325;&#26032;&#26500;&#24314;&#20026;&#19968;&#20010;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#24212;&#29992;&#36845;&#20195;&#25237;&#24433;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#29983;&#25104;&#30340;&#25968;&#25454;&#24544;&#23454;&#22320;&#36981;&#24490;&#25351;&#23450;&#30340;&#32422;&#26463;&#25110;&#29289;&#29702;&#21407;&#29702;&#12290;&#26412;&#25991;&#22312;&#21463;&#38480;&#21046;&#30340;&#32422;&#26463;&#31867;&#21035;&#19979;&#65292;&#23545;PGDM&#33021;&#22815;&#20174;&#21487;&#34892;&#23376;&#20998;&#24067;&#20013;&#21512;&#25104;&#36755;&#20986;&#30340;&#33021;&#21147;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25345;&#65292;&#24182;&#22312;&#22797;&#26434;&#30340;&#38750;&#20984;&#32422;&#26463;&#21644;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#26696;&#20363;&#20013;&#25552;&#20379;&#20102;&#22823;&#37327;&#30340;&#32463;&#39564;&#35777;&#25454;&#12290;&#36825;&#20123;&#33021;&#21147;&#36890;&#36807;&#22312;&#35270;&#39057;&#29983;&#25104;&#20013;&#20307;&#29616;&#20102;&#20855;&#26377;&#29289;&#29702;&#23398;&#20449;&#24687;&#30340;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative diffusion models excel at robustly synthesizing coherent content from raw noise through a sequential process. However, their direct application in scenarios requiring outputs to adhere to specific, stringent criteria faces several severe challenges. This paper aims at overcome these challenges and introduces Projected Generative Diffusion Models (PGDM), an approach that recast traditional diffusion models sampling into a constrained-optimization problem. This enables the application of an iterative projections method to ensure that generated data faithfully adheres to specified constraints or physical principles. This paper provides theoretical support for the ability of PGDM to synthesize outputs from a feasible subdistribution under a restricted class of constraints while also providing large empirical evidence in the case of complex non-convex constraints and ordinary differential equations. These capabilities are demonstrated by physics-informed motion in video generatio
&lt;/p&gt;</description></item><item><title>Aligner&#26159;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26657;&#27491;&#27531;&#24046;&#26469;&#23454;&#29616;&#39640;&#25928;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;Aligner&#20855;&#26377;&#21442;&#25968;&#39640;&#25928;&#12289;&#24369;&#21040;&#24378;&#27867;&#21270;&#20197;&#21450;&#21363;&#25554;&#21363;&#29992;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.02416</link><description>&lt;p&gt;
Aligner: &#36890;&#36807;&#24369;&#21040;&#24378;&#26657;&#27491;&#23454;&#29616;&#39640;&#25928;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02416
&lt;/p&gt;
&lt;p&gt;
Aligner&#26159;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26657;&#27491;&#27531;&#24046;&#26469;&#23454;&#29616;&#39640;&#25928;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;Aligner&#20855;&#26377;&#21442;&#25968;&#39640;&#25928;&#12289;&#24369;&#21040;&#24378;&#27867;&#21270;&#20197;&#21450;&#21363;&#25554;&#21363;&#29992;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26469;&#36827;&#34892;&#23545;&#40784;&#30340;&#21162;&#21147;&#20027;&#35201;&#26159;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#30340;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30528;&#20027;&#35201;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#12289;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24037;&#31243;&#20197;&#21450;&#37325;&#35201;&#30340;&#26159;&#65292;&#38656;&#35201;&#35775;&#38382;LLM&#21442;&#25968;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#23545;&#40784;&#33539;&#24335;Aligner&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#23545;&#40784;&#21644;&#26410;&#23545;&#40784;&#31572;&#26696;&#20043;&#38388;&#30340;&#26657;&#27491;&#27531;&#24046;&#26469;&#32469;&#36807;&#25972;&#20010;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;Aligner&#20855;&#26377;&#20960;&#20010;&#20851;&#38190;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#23427;&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#33258;&#21160;&#22238;&#24402;seq2seq&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#26597;&#35810;-&#31572;&#26696;-&#26657;&#27491;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#23545;&#40784;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#23545;&#36164;&#28304;&#38656;&#27714;&#36739;&#23569;&#12290;&#20854;&#27425;&#65292;Aligner&#23454;&#29616;&#20102;&#20174;&#24369;&#21040;&#24378;&#30340;&#27867;&#21270;&#65307;&#36890;&#36807;Aligner&#30340;&#30417;&#30563;&#20449;&#21495;&#26469;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;&#31532;&#19977;&#65292;Aligner&#20316;&#20026;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#65292;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#8230;
&lt;/p&gt;
&lt;p&gt;
Efforts to align Large Language Models (LLMs) are mainly conducted via Reinforcement Learning from Human Feedback (RLHF) methods. However, RLHF encounters major challenges including training reward models, actor-critic engineering, and importantly, it requires access to LLM parameters. Here we introduce Aligner, a new efficient alignment paradigm that bypasses the whole RLHF process by learning the correctional residuals between the aligned and the unaligned answers. Our Aligner offers several key advantages. Firstly, it is an autoregressive seq2seq model that is trained on the query-answer-correction dataset via supervised learning; this offers a parameter-efficient alignment solution with minimal resources. Secondly, the Aligner facilitates weak-to-strong generalization; finetuning large pretrained models by Aligner's supervisory signals demonstrates strong performance boost. Thirdly, Aligner functions as a model-agnostic plug-and-play module, allowing for its direct application on d
&lt;/p&gt;</description></item><item><title>CMMMU&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#22411;&#27169;&#22411;&#22312;&#22823;&#23398;&#32423;&#23398;&#31185;&#30693;&#35782;&#21644;&#28145;&#24605;&#29087;&#34385;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#30340;&#20013;&#25991;&#22823;&#35268;&#27169;&#22810;&#23398;&#31185;&#22810;&#27169;&#24577;&#29702;&#35299;&#22522;&#20934;&#65292;&#20026;&#22635;&#34917;&#22312;&#38750;&#33521;&#35821;&#29615;&#22659;&#20013;&#35780;&#20272;&#20808;&#36827;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#31354;&#30333;&#32780;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2401.11944</link><description>&lt;p&gt;
CMMMU&#65306;&#19968;&#20010;&#20013;&#22269;&#22823;&#35268;&#27169;&#22810;&#23398;&#31185;&#22810;&#27169;&#24577;&#29702;&#35299;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11944
&lt;/p&gt;
&lt;p&gt;
CMMMU&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#22411;&#27169;&#22411;&#22312;&#22823;&#23398;&#32423;&#23398;&#31185;&#30693;&#35782;&#21644;&#28145;&#24605;&#29087;&#34385;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#30340;&#20013;&#25991;&#22823;&#35268;&#27169;&#22810;&#23398;&#31185;&#22810;&#27169;&#24577;&#29702;&#35299;&#22522;&#20934;&#65292;&#20026;&#22635;&#34917;&#22312;&#38750;&#33521;&#35821;&#29615;&#22659;&#20013;&#35780;&#20272;&#20808;&#36827;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#31354;&#30333;&#32780;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#22810;&#27169;&#22411;&#27169;&#22411;(LMMs)&#30340;&#33021;&#21147;&#19981;&#26029;&#25552;&#21319;&#65292;&#35780;&#20272;LMMs&#30340;&#34920;&#29616;&#26085;&#30410;&#25104;&#20026;&#19968;&#20010;&#36843;&#20999;&#30340;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#22312;&#35780;&#20272;LMMs&#22312;&#20013;&#25991;&#31561;&#38750;&#33521;&#35821;&#29615;&#22659;&#20013;&#20808;&#36827;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#26356;&#22823;&#24046;&#36317;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;CMMMU&#65292;&#19968;&#20010;&#26032;&#30340;&#20013;&#25991;&#22823;&#35268;&#27169;&#22810;&#23398;&#31185;&#22810;&#27169;&#24577;&#29702;&#35299;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;LMMs&#22312;&#38656;&#35201;&#22823;&#23398;&#27700;&#24179;&#23398;&#31185;&#30693;&#35782;&#21644;&#28145;&#24605;&#29087;&#34385;&#25512;&#29702;&#30340;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;CMMMU&#21463;&#21040;&#20102;MMMUs&#30340;&#26631;&#27880;&#21644;&#20998;&#26512;&#27169;&#24335;&#30340;&#21551;&#21457;&#24182;&#20005;&#26684;&#36981;&#24490;&#12290;CMMMU&#21253;&#25324;&#26469;&#33258;&#22823;&#23398;&#32771;&#35797;&#12289;&#27979;&#39564;&#21644;&#25945;&#31185;&#20070;&#30340;1.2&#19975;&#20010;&#25163;&#21160;&#25910;&#38598;&#30340;&#22810;&#27169;&#24577;&#38382;&#39064;&#65292;&#28085;&#30422;&#20845;&#20010;&#26680;&#24515;&#23398;&#31185;&#65306;&#33402;&#26415;&#19982;&#35774;&#35745;&#12289;&#21830;&#19994;&#12289;&#31185;&#23398;&#12289;&#20581;&#24247;&#19982;&#21307;&#23398;&#12289;&#20154;&#25991;&#31038;&#31185;&#20197;&#21450;&#25216;&#26415;&#19982;&#24037;&#31243;&#65292;&#23601;&#20687;&#20854;&#20249;&#20276;MMMMU&#19968;&#26679;&#12290;&#36825;&#20123;&#38382;&#39064;&#28085;&#30422;30&#20010;&#23398;&#31185;&#65292;&#21253;&#25324;39&#20010;&#39640;&#24230;&#24322;&#36136;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11944v2 Announce Type: replace-cross  Abstract: As the capabilities of large multimodal models (LMMs) continue to advance, evaluating the performance of LMMs emerges as an increasing need. Additionally, there is an even larger gap in evaluating the advanced knowledge and reasoning abilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU, a new Chinese Massive Multi-discipline Multimodal Understanding benchmark designed to evaluate LMMs on tasks demanding college-level subject knowledge and deliberate reasoning in a Chinese context. CMMMU is inspired by and strictly follows the annotation and analysis pattern of MMMU.   CMMMU includes 12k manually collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art &amp; Design, Business, Science, Health &amp; Medicine, Humanities &amp; Social Science, and Tech &amp; Engineering, like its companion, MMMU. These questions span 30 subjects and comprise 39 highly heterogeneous image 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22686;&#24378;&#23398;&#20064;&#20195;&#29702;&#30340;&#20010;&#24615;&#21270;&#36335;&#24452;&#34917;&#25937;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32534;&#36753;&#21160;&#20316;&#36335;&#24452;&#26469;&#23454;&#29616;&#26399;&#26395;&#30446;&#26631;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#20195;&#29702;&#30340;&#21407;&#22987;&#36335;&#24452;&#30456;&#20284;&#24230;&#39640;&#65292;&#24182;&#19988;&#20010;&#24615;&#21270;&#36866;&#24212;&#20195;&#29702;&#30340;&#34892;&#20026;&#27169;&#24335;&#12290;&#36825;&#31181;&#26041;&#27861;&#36866;&#29992;&#20110;&#32416;&#27491;&#25110;&#25913;&#36827;&#21160;&#20316;&#25110;&#25968;&#25454;&#24207;&#21015;&#20197;&#23454;&#29616;&#39044;&#23450;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2312.08724</link><description>&lt;p&gt;
&#38024;&#23545;&#22686;&#24378;&#23398;&#20064;&#20195;&#29702;&#30340;&#20010;&#24615;&#21270;&#36335;&#24452;&#34917;&#25937;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Personalized Path Recourse for Reinforcement Learning Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08724
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22686;&#24378;&#23398;&#20064;&#20195;&#29702;&#30340;&#20010;&#24615;&#21270;&#36335;&#24452;&#34917;&#25937;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32534;&#36753;&#21160;&#20316;&#36335;&#24452;&#26469;&#23454;&#29616;&#26399;&#26395;&#30446;&#26631;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#20195;&#29702;&#30340;&#21407;&#22987;&#36335;&#24452;&#30456;&#20284;&#24230;&#39640;&#65292;&#24182;&#19988;&#20010;&#24615;&#21270;&#36866;&#24212;&#20195;&#29702;&#30340;&#34892;&#20026;&#27169;&#24335;&#12290;&#36825;&#31181;&#26041;&#27861;&#36866;&#29992;&#20110;&#32416;&#27491;&#25110;&#25913;&#36827;&#21160;&#20316;&#25110;&#25968;&#25454;&#24207;&#21015;&#20197;&#23454;&#29616;&#39044;&#23450;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#20010;&#24615;&#21270;&#36335;&#24452;&#34917;&#25937;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20026;&#22686;&#24378;&#23398;&#20064;&#20195;&#29702;&#29983;&#25104;&#34917;&#25937;&#36335;&#24452;&#12290;&#20854;&#30446;&#26631;&#26159;&#36890;&#36807;&#32534;&#36753;&#32473;&#23450;&#30340;&#21160;&#20316;&#36335;&#24452;&#20197;&#36798;&#21040;&#26399;&#26395;&#30340;&#30446;&#26631;&#65288;&#20363;&#22914;&#65292;&#19982;&#20195;&#29702;&#30340;&#21407;&#22987;&#36335;&#24452;&#30456;&#27604;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#65289;&#65292;&#21516;&#26102;&#30830;&#20445;&#19982;&#20195;&#29702;&#30340;&#21407;&#22987;&#36335;&#24452;&#39640;&#24230;&#30456;&#20284;&#24182;&#20010;&#24615;&#21270;&#36866;&#24212;&#20195;&#29702;&#12290;&#20010;&#24615;&#21270;&#26159;&#25351;&#26032;&#36335;&#24452;&#22312;&#20174;&#31574;&#30053;&#20989;&#25968;&#20013;&#35266;&#23519;&#21040;&#30340;&#20195;&#29702;&#34892;&#20026;&#27169;&#24335;&#26041;&#38754;&#30340;&#23450;&#21046;&#31243;&#24230;&#12290;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#20010;&#24615;&#21270;&#30340;&#34917;&#25937;&#20195;&#29702;&#26469;&#29983;&#25104;&#36825;&#26679;&#30340;&#20010;&#24615;&#21270;&#36335;&#24452;&#65292;&#36825;&#20123;&#36335;&#24452;&#26159;&#20351;&#29992;&#32771;&#34385;&#30446;&#26631;&#12289;&#30456;&#20284;&#24615;&#21644;&#20010;&#24615;&#21270;&#30340;&#22870;&#21169;&#20989;&#25968;&#33719;&#24471;&#30340;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#22686;&#24378;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#65292;&#20197;&#32416;&#27491;&#25110;&#25913;&#36827;&#21160;&#20316;&#24207;&#21015;&#25110;&#25968;&#25454;&#24207;&#21015;&#20197;&#36798;&#21040;&#39044;&#23450;&#30340;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#35774;&#32622;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08724v2 Announce Type: replace-cross  Abstract: This paper introduces Personalized Path Recourse, a novel method that generates recourse paths for a reinforcement learning agent. The goal is to edit a given path of actions to achieve desired goals (e.g., better outcomes compared to the agent's original path) while ensuring a high similarity to the agent's original paths and being personalized to the agent. Personalization refers to the extent to which the new path is tailored to the agent's observed behavior patterns from their policy function. We train a personalized recourse agent to generate such personalized paths, which are obtained using reward functions that consider the goal, similarity, and personalization. The proposed method is applicable to both reinforcement learning and supervised learning settings for correcting or improving sequences of actions or sequences of data to achieve a pre-determined goal. The method is evaluated in various settings. Experiments show
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;&#30340;&#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#26631;&#20934;&#21270;&#27969;&#21644;&#27969;&#21305;&#37197;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#29992;&#20110;&#21487;&#25193;&#23637;&#12289;&#24555;&#36895;&#21644;&#25674;&#38144;&#25512;&#26029;&#65292;&#22312;&#22810;&#20010;&#23454;&#39564;&#20013;&#23637;&#31034;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2312.05440</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#21644;&#24555;&#36895;&#27169;&#25311;&#25512;&#26029;&#30340;&#19968;&#33268;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Consistency Models for Scalable and Fast Simulation-Based Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05440
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;&#30340;&#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#26631;&#20934;&#21270;&#27969;&#21644;&#27969;&#21305;&#37197;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#29992;&#20110;&#21487;&#25193;&#23637;&#12289;&#24555;&#36895;&#21644;&#25674;&#38144;&#25512;&#26029;&#65292;&#22312;&#22810;&#20010;&#23454;&#39564;&#20013;&#23637;&#31034;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20223;&#30495;&#25512;&#26029;&#65288;SBI&#65289;&#19981;&#26029;&#23547;&#25214;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#31639;&#27861;&#65292;&#20197;&#20934;&#30830;&#25512;&#26029;&#22797;&#26434;&#27169;&#22411;&#30340;&#21442;&#25968;&#20174;&#22024;&#26434;&#25968;&#25454;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;&#30340;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;CMPE&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#21487;&#25193;&#23637;&#12289;&#24555;&#36895;&#21644;&#25674;&#38144;&#25512;&#26029;&#30340;&#26032;&#33258;&#30001;&#24418;&#24335;&#26465;&#20214;&#37319;&#26679;&#22120;&#65292;&#21033;&#29992;&#29983;&#25104;&#24615;&#31070;&#32463;&#32593;&#32476;&#12290;CMPE&#23558;&#26631;&#20934;&#21270;&#27969;&#21644;&#27969;&#21305;&#37197;&#26041;&#27861;&#30340;&#20248;&#28857;&#32467;&#21512;&#21040;&#21333;&#20010;&#29983;&#25104;&#26550;&#26500;&#20013;&#65306;&#23427;&#26412;&#36136;&#19978;&#25552;&#28860;&#20102;&#36830;&#32493;&#27010;&#29575;&#27969;&#65292;&#24182;&#33021;&#22815;&#21033;&#29992;&#26080;&#32422;&#26463;&#30340;&#32467;&#26500;&#24555;&#36895;&#36827;&#34892;&#23569;&#23556;&#25512;&#26029;&#65292;&#35813;&#32467;&#26500;&#21487;&#20197;&#23450;&#21046;&#21040;&#20272;&#35745;&#38382;&#39064;&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;CMPE&#19981;&#20165;&#22312;&#19977;&#20010;&#22256;&#38590;&#30340;&#20302;&#32500;&#38382;&#39064;&#19978;&#20248;&#20110;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#65292;&#32780;&#19988;&#22312;&#39640;&#32500;&#36125;&#21494;&#26031;&#21435;&#22122;&#23454;&#39564;&#21644;&#20272;&#35745;&#35745;&#31639;&#23494;&#38598;&#22411;&#22810;&#23610;&#24230;&#20013;&#34920;&#29616;&#20986;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05440v2 Announce Type: replace-cross  Abstract: Simulation-based inference (SBI) is constantly in search of more expressive algorithms for accurately inferring the parameters of complex models from noisy data. We present consistency models for neural posterior estimation (CMPE), a new free-form conditional sampler for scalable, fast, and amortized SBI with generative neural networks. CMPE combines the advantages of normalizing flows and flow matching methods into a single generative architecture: It essentially distills a continuous probability flow and enables rapid few-shot inference with an unconstrained architecture that can be tailored to the structure of the estimation problem. Our empirical evaluation demonstrates that CMPE not only outperforms current state-of-the-art algorithms on three hard low-dimensional problems but also achieves competitive performance in a high-dimensional Bayesian denoising experiment and in estimating a computationally demanding multi-scale 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#31867;&#21487;&#35835;&#25351;&#32441;&#65292;&#21487;&#20197;&#21807;&#19968;&#35782;&#21035;&#20986;&#22522;&#26412;&#27169;&#22411;&#65292;&#24182;&#19988;&#19981;&#26292;&#38706;&#27169;&#22411;&#21442;&#25968;&#25110;&#24178;&#25200;&#35757;&#32451;&#12290;&#36890;&#36807;&#35266;&#23519;&#21644;&#39564;&#35777;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#21442;&#25968;&#30340;&#21521;&#37327;&#26041;&#21521;&#22312;&#39044;&#35757;&#32451;&#21518;&#20445;&#25345;&#31283;&#23450;&#65292;&#25104;&#20026;&#35782;&#21035;&#22522;&#26412;&#27169;&#22411;&#30340;&#37325;&#35201;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2312.04828</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#31867;&#21487;&#35835;&#25351;&#32441;
&lt;/p&gt;
&lt;p&gt;
Human-Readable Fingerprint for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04828
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#31867;&#21487;&#35835;&#25351;&#32441;&#65292;&#21487;&#20197;&#21807;&#19968;&#35782;&#21035;&#20986;&#22522;&#26412;&#27169;&#22411;&#65292;&#24182;&#19988;&#19981;&#26292;&#38706;&#27169;&#22411;&#21442;&#25968;&#25110;&#24178;&#25200;&#35757;&#32451;&#12290;&#36890;&#36807;&#35266;&#23519;&#21644;&#39564;&#35777;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#21442;&#25968;&#30340;&#21521;&#37327;&#26041;&#21521;&#22312;&#39044;&#35757;&#32451;&#21518;&#20445;&#25345;&#31283;&#23450;&#65292;&#25104;&#20026;&#35782;&#21035;&#22522;&#26412;&#27169;&#22411;&#30340;&#37325;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36164;&#28304;&#23494;&#38598;&#22411;&#35757;&#32451;&#21644;&#37197;&#22871;&#30340;&#31934;&#24515;&#35774;&#35745;&#30340;&#35768;&#21487;&#35777;&#65292;&#20445;&#25252;LLM&#30340;&#29256;&#26435;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21487;&#33021;&#30340;&#21442;&#25968;&#20462;&#25913;&#65292;&#30830;&#23450;LLM&#30340;&#21407;&#22987;&#22522;&#26412;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;LLM&#30340;&#20154;&#31867;&#21487;&#35835;&#25351;&#32441;&#65292;&#21487;&#20197;&#21807;&#19968;&#22320;&#35782;&#21035;&#22522;&#26412;&#27169;&#22411;&#65292;&#32780;&#19981;&#26292;&#38706;&#27169;&#22411;&#21442;&#25968;&#25110;&#24178;&#25200;&#35757;&#32451;&#12290;&#25105;&#20204;&#39318;&#20808;&#35266;&#23519;&#21040;&#65292;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#27169;&#22411;&#25910;&#25947;&#21518;&#65292;LLM&#21442;&#25968;&#30340;&#21521;&#37327;&#26041;&#21521;&#20445;&#25345;&#31283;&#23450;&#65292;&#36890;&#36807;&#21518;&#32493;&#30340;&#35757;&#32451;&#27493;&#39588;&#65292;&#21253;&#25324;&#25345;&#32493;&#39044;&#35757;&#32451;&#12289;&#30417;&#30563;&#24494;&#35843;&#21644;RLHF&#65292;&#20960;&#20046;&#27809;&#26377;&#25200;&#21160;&#65292;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;&#35782;&#21035;&#22522;&#26412;&#27169;&#22411;&#30340;&#36275;&#22815;&#26465;&#20214;&#12290;&#36890;&#36807;&#32487;&#32493;&#35757;&#32451;LLM&#24182;&#28155;&#21152;&#19968;&#20010;&#39069;&#22806;&#30340;&#39033;&#26469;&#25512;&#24320;&#27169;&#22411;&#21442;&#25968;&#30340;&#26041;&#21521;&#65292;&#39564;&#35777;&#20102;&#36825;&#31181;&#24517;&#35201;&#24615;&#65292;&#32467;&#26524;&#20351;&#24471;&#27169;&#22411;&#21463;&#25439;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#26041;&#21521;&#23481;&#26131;&#21463;&#21040;&#31616;&#21333;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#22914;&#32500;&#24230;...
&lt;/p&gt;
&lt;p&gt;
Protecting the copyright of large language models (LLMs) has become crucial due to their resource-intensive training and accompanying carefully designed licenses. However, identifying the original base model of an LLM is challenging due to potential parameter alterations. In this study, we introduce a human-readable fingerprint for LLMs that uniquely identifies the base model without exposing model parameters or interfering with training. We first observe that the vector direction of LLM parameters remains stable after the model has converged during pretraining, showing negligible perturbations through subsequent training steps, including continued pretraining, supervised fine-tuning (SFT), and RLHF, which makes it a sufficient condition to identify the base model. The necessity is validated by continuing to train an LLM with an extra term to drive away the model parameters' direction and the model becomes damaged. However, this direction is vulnerable to simple attacks like dimension 
&lt;/p&gt;</description></item><item><title>LayerCollapse&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#21098;&#26525;&#26469;&#20943;&#23569;&#20840;&#36830;&#25509;&#23618;&#30340;&#28145;&#24230;&#65292;&#32780;&#19981;&#38656;&#35201;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#19988;&#23545;&#24615;&#33021;&#24433;&#21709;&#26377;&#38480;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#27491;&#21017;&#21270;&#28608;&#27963;&#20989;&#25968;&#30340;&#32447;&#24615;&#24230;&#26469;&#25511;&#21046;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.17943</link><description>&lt;p&gt;
LayerCollapse: &#33258;&#36866;&#24212;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
LayerCollapse: Adaptive compression of neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17943
&lt;/p&gt;
&lt;p&gt;
LayerCollapse&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#21098;&#26525;&#26469;&#20943;&#23569;&#20840;&#36830;&#25509;&#23618;&#30340;&#28145;&#24230;&#65292;&#32780;&#19981;&#38656;&#35201;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#19988;&#23545;&#24615;&#33021;&#24433;&#21709;&#26377;&#38480;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#27491;&#21017;&#21270;&#28608;&#27963;&#20989;&#25968;&#30340;&#32447;&#24615;&#24230;&#26469;&#25511;&#21046;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#24403;&#20195;&#28145;&#24230;&#23398;&#20064;&#21644;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#19981;&#26029;&#22686;&#38271;&#30340;&#35268;&#27169;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#36229;&#21442;&#25968;&#21270;&#30340;Transformer&#32593;&#32476;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#38754;&#30340;&#19994;&#32489;&#36229;&#36807;&#20102;&#20808;&#21069;&#30340;&#25216;&#26415;&#12290;&#36825;&#20123;&#27169;&#22411;&#21547;&#26377;&#25968;&#20159;&#20010;&#21442;&#25968;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#24182;&#23481;&#26131;&#36807;&#25311;&#21512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LayerCollapse&#65292;&#19968;&#31181;&#32467;&#26500;&#21270;&#21098;&#26525;&#30340;&#24418;&#24335;&#65292;&#29992;&#20110;&#20943;&#23569;&#20840;&#36830;&#25509;&#23618;&#30340;&#28145;&#24230;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#20801;&#35768;&#22312;&#19981;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35757;&#32451;&#21518;&#21387;&#32553;&#65292;&#24182;&#23545;&#24615;&#33021;&#20135;&#29983;&#26377;&#38480;&#30340;&#24433;&#21709;&#12290;LayerCollapse&#36890;&#36807;&#23545;&#20840;&#36830;&#25509;&#23618;&#20043;&#38388;&#30340;&#28608;&#27963;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#35843;&#33410;&#28608;&#27963;&#20989;&#25968;&#30340;&#32447;&#24615;&#24230;&#26469;&#25511;&#21046;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#23558;&#32447;&#24615;&#36716;&#25442;&#30340;&#31209;&#38477;&#20302;&#21040;&#30456;&#24212;&#32447;&#24615;&#36716;&#25442;&#30340;&#31209;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;LayerCollapse&#30340;&#21387;&#32553;&#33021;&#21147;&#26469;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Handling the ever-increasing scale of contemporary deep learning and transformer-based models poses a significant challenge. Overparameterized Transformer networks outperform prior art in Natural Language processing and Computer Vision. These models contain hundreds of millions of parameters, demanding significant computational resources and making them prone to overfitting. In this work we present LayerCollapse, a form of structured pruning to reduce the depth of fully connected layers. We develop a novel regularizer allowing for post-training compression without finetuning, while having limited impact on performance. LayerCollapse controls model expressiveness with regularization on the activations between fully connected layers, modulating the linearity of activation functions. A linear activation function reduces the rank of the transformation to the rank of the corresponding linear transformation. We demonstrate the effectiveness of LayerCollapse by showing its compression capabil
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745; (MultiNPE) &#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#34701;&#21512;&#23398;&#20064;&#25972;&#21512;&#19981;&#21516;&#26469;&#28304;&#30340;&#24322;&#26500;&#25968;&#25454;&#65292;&#22312;&#27169;&#25311;&#25512;&#29702;&#20013;&#25552;&#39640;&#20102;&#23545;&#22797;&#26434;&#25968;&#23398;&#27169;&#22411;&#21442;&#25968;&#30340;&#20934;&#30830;&#25512;&#26029;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.10671</link><description>&lt;p&gt;
&#22810;&#27169;&#25311;&#25512;&#29702;&#30340;&#28145;&#24230;&#34701;&#21512;&#65306;&#28145;&#24230;&#34701;&#21512;&#29992;&#20110;&#22810;&#27169;&#24577;&#27169;&#25311;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Fuse It or Lose It: Deep Fusion for Multimodal Simulation-Based Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10671
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745; (MultiNPE) &#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#34701;&#21512;&#23398;&#20064;&#25972;&#21512;&#19981;&#21516;&#26469;&#28304;&#30340;&#24322;&#26500;&#25968;&#25454;&#65292;&#22312;&#27169;&#25311;&#25512;&#29702;&#20013;&#25552;&#39640;&#20102;&#23545;&#22797;&#26434;&#25968;&#23398;&#27169;&#22411;&#21442;&#25968;&#30340;&#20934;&#30830;&#25512;&#26029;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#22810;&#27169;&#24577;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;(MultiNPE)&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#22312;&#27169;&#25311;&#25512;&#29702;&#20013;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#24322;&#26500;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#21463;&#28145;&#24230;&#34701;&#21512;&#23398;&#20064;&#30340;&#36827;&#23637;&#21551;&#21457;&#65292;&#23427;&#36171;&#20104;&#30740;&#31350;&#20154;&#21592;&#20998;&#26512;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#24182;&#25512;&#26029;&#22797;&#26434;&#25968;&#23398;&#27169;&#22411;&#21442;&#25968;&#30340;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#38024;&#23545;MultiNPE&#21046;&#23450;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#26041;&#27861;&#65288;&#26089;&#26399;&#12289;&#21518;&#26399;&#12289;&#28151;&#21512;&#65289;&#65292;&#24182;&#22312;&#19977;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23454;&#39564;&#20013;&#35780;&#20272;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;MultiNPE&#19981;&#20165;&#22312;&#21442;&#32771;&#20219;&#21153;&#19978;&#20248;&#20110;&#21333;&#19968;&#25968;&#25454;&#28304;&#22522;&#32447;&#65292;&#36824;&#22312;&#31070;&#32463;&#31185;&#23398;&#21644;&#24515;&#33039;&#30149;&#23398;&#30340;&#31185;&#23398;&#27169;&#22411;&#25512;&#29702;&#19978;&#21462;&#24471;&#20102;&#21331;&#36234;&#25104;&#32489;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#37096;&#20998;&#32570;&#22833;&#25968;&#25454;&#23545;&#19981;&#21516;&#34701;&#21512;&#31574;&#30053;&#30340;&#24433;&#21709;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#21518;&#26399;&#21644;&#28151;&#21512;&#34701;&#21512;&#25216;&#26415;&#25104;&#20026;&#22810;&#27169;&#24577;&#27169;&#25311;&#25512;&#29702;&#23454;&#38469;&#24212;&#29992;&#30340;&#39318;&#36873;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10671v2 Announce Type: replace-cross  Abstract: We present multimodal neural posterior estimation (MultiNPE), a method to integrate heterogeneous data from different sources in simulation-based inference with neural networks. Inspired by advances in deep fusion learning, it empowers researchers to analyze data from different domains and infer the parameters of complex mathematical models with increased accuracy. We formulate multimodal fusion approaches for \hbox{MultiNPE} (early, late, hybrid) and evaluate their performance in three challenging experiments. MultiNPE not only outperforms single-source baselines on a reference task, but also achieves superior inference on scientific models from neuroscience and cardiology. We systematically investigate the impact of partially missing data on the different fusion strategies. Across our experiments, late and hybrid fusion techniques emerge as the methods of choice for practical applications of multimodal simulation-based infere
&lt;/p&gt;</description></item><item><title>MADRID&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#23545;&#25239;&#22330;&#26223;&#26469;&#25581;&#31034;&#39044;&#35757;&#32451;&#22810;Agent&#31574;&#30053;&#30340;&#25112;&#30053;&#28431;&#27934;&#65292;&#24182;&#36890;&#36807;&#36951;&#25022;&#20540;&#34913;&#37327;&#28431;&#27934;&#30340;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.13460</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#26679;&#24615;&#21551;&#31034;&#30340;&#22810;Agent&#35786;&#26029;&#26041;&#27861;&#29992;&#20110;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Diagnostics for Robustness via Illuminated Diversity. (arXiv:2401.13460v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13460
&lt;/p&gt;
&lt;p&gt;
MADRID&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#23545;&#25239;&#22330;&#26223;&#26469;&#25581;&#31034;&#39044;&#35757;&#32451;&#22810;Agent&#31574;&#30053;&#30340;&#25112;&#30053;&#28431;&#27934;&#65292;&#24182;&#36890;&#36807;&#36951;&#25022;&#20540;&#34913;&#37327;&#28431;&#27934;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#22810;Agent&#31995;&#32479;&#39046;&#22495;&#20013;&#65292;&#30830;&#20445;&#22312;&#38476;&#29983;&#21644;&#25932;&#23545;&#29615;&#22659;&#20013;&#30340;&#31283;&#20581;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#36825;&#20123;&#31995;&#32479;&#22312;&#29087;&#24713;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#26032;&#24773;&#20917;&#19979;&#24448;&#24448;&#20250;&#22240;&#20026;&#35757;&#32451;&#38454;&#27573;&#30340;&#36807;&#25311;&#21512;&#32780;&#22833;&#36133;&#12290;&#22312;&#26082;&#21253;&#21547;&#21512;&#20316;&#21448;&#21253;&#21547;&#31454;&#20105;&#34892;&#20026;&#30340;&#29615;&#22659;&#20013;&#65292;&#36825;&#19968;&#38382;&#39064;&#23588;&#20026;&#31361;&#20986;&#65292;&#20307;&#29616;&#20102;&#36807;&#25311;&#21512;&#21644;&#27867;&#21270;&#25361;&#25112;&#30340;&#21452;&#37325;&#24615;&#36136;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#22810;&#26679;&#24615;&#21551;&#31034;&#30340;&#22810;Agent&#31283;&#20581;&#24615;&#35786;&#26029;&#65288;MADRID&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29983;&#25104;&#22810;Agent&#31574;&#30053;&#20013;&#26292;&#38706;&#25112;&#30053;&#28431;&#27934;&#30340;&#22810;&#26679;&#21270;&#23545;&#25239;&#22330;&#26223;&#30340;&#26032;&#26041;&#27861;&#12290;MADRID&#21033;&#29992;&#24320;&#25918;&#24335;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#23548;&#33322;&#23545;&#25239;&#29615;&#22659;&#30340;&#24191;&#38420;&#31354;&#38388;&#65292;&#20351;&#29992;&#30446;&#26631;&#31574;&#30053;&#30340;&#36951;&#25022;&#20540;&#26469;&#34913;&#37327;&#36825;&#20123;&#29615;&#22659;&#30340;&#28431;&#27934;&#12290;&#25105;&#20204;&#22312;11vs11&#29256;&#30340;Google Research Football&#19978;&#35780;&#20272;&#20102;MADRID&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly advancing field of multi-agent systems, ensuring robustness in unfamiliar and adversarial settings is crucial. Notwithstanding their outstanding performance in familiar environments, these systems often falter in new situations due to overfitting during the training phase. This is especially pronounced in settings where both cooperative and competitive behaviours are present, encapsulating a dual nature of overfitting and generalisation challenges. To address this issue, we present Multi-Agent Diagnostics for Robustness via Illuminated Diversity (MADRID), a novel approach for generating diverse adversarial scenarios that expose strategic vulnerabilities in pre-trained multi-agent policies. Leveraging the concepts from open-ended learning, MADRID navigates the vast space of adversarial settings, employing a target policy's regret to gauge the vulnerabilities of these settings. We evaluate the effectiveness of MADRID on the 11vs11 version of Google Research Football, one o
&lt;/p&gt;</description></item><item><title>DISTINQT&#26159;&#19968;&#31181;&#38754;&#21521;&#26410;&#26469;&#31227;&#21160;&#21644;&#26080;&#32447;&#32593;&#32476;&#30340;&#38544;&#31169;&#24863;&#30693;&#20998;&#24067;&#24335;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;QoS&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.10158</link><description>&lt;p&gt;
DISTINQT: &#19968;&#31181;&#38754;&#21521;&#26410;&#26469;&#31227;&#21160;&#21644;&#26080;&#32447;&#32593;&#32476;&#30340;&#20998;&#24067;&#24335;&#38544;&#31169;&#24863;&#30693;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;QoS&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
DISTINQT: A Distributed Privacy Aware Learning Framework for QoS Prediction for Future Mobile and Wireless Networks. (arXiv:2401.10158v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10158
&lt;/p&gt;
&lt;p&gt;
DISTINQT&#26159;&#19968;&#31181;&#38754;&#21521;&#26410;&#26469;&#31227;&#21160;&#21644;&#26080;&#32447;&#32593;&#32476;&#30340;&#38544;&#31169;&#24863;&#30693;&#20998;&#24067;&#24335;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;QoS&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
5G&#21644;6G&#20197;&#21518;&#30340;&#32593;&#32476;&#23558;&#25903;&#25345;&#20381;&#36182;&#19968;&#23450;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#30340;&#26032;&#30340;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29992;&#20363;&#21644;&#24212;&#29992;&#31243;&#24207;&#12290;&#21450;&#26102;&#39044;&#27979;QoS&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#65288;&#22914;&#36710;&#36742;&#36890;&#20449;&#65289;&#23588;&#20026;&#37325;&#35201;&#12290;&#23613;&#31649;&#30452;&#21040;&#26368;&#36817;&#65292;QoS&#39044;&#27979;&#19968;&#30452;&#30001;&#38598;&#20013;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#35299;&#20915;&#26041;&#26696;&#23436;&#25104;&#65292;&#20294;&#24050;&#32463;&#20986;&#29616;&#20102;&#19968;&#20123;&#38544;&#31169;&#12289;&#35745;&#31639;&#21644;&#36816;&#33829;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;&#26367;&#20195;&#26041;&#26696;&#24050;&#32463;&#20986;&#29616;&#65288;&#22914;&#20998;&#21106;&#23398;&#20064;&#12289;&#32852;&#37030;&#23398;&#20064;&#65289;&#65292;&#23558;&#22797;&#26434;&#24230;&#36739;&#20302;&#30340;AI&#20219;&#21153;&#20998;&#24067;&#22312;&#33410;&#28857;&#20043;&#38388;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#26410;&#26469;&#26080;&#32447;&#32593;&#32476;&#30340;&#24322;&#26500;&#24615;&#65292;&#24403;&#28041;&#21450;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#26102;&#65292;&#20250;&#20986;&#29616;&#26032;&#30340;&#25361;&#25112;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DISTINQT&#30340;&#38754;&#21521;QoS&#39044;&#27979;&#30340;&#38544;&#31169;&#24863;&#30693;&#20998;&#24067;&#24335;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Beyond 5G and 6G networks are expected to support new and challenging use cases and applications that depend on a certain level of Quality of Service (QoS) to operate smoothly. Predicting the QoS in a timely manner is of high importance, especially for safety-critical applications as in the case of vehicular communications. Although until recent years the QoS prediction has been carried out by centralized Artificial Intelligence (AI) solutions, a number of privacy, computational, and operational concerns have emerged. Alternative solutions have been surfaced (e.g. Split Learning, Federated Learning), distributing AI tasks of reduced complexity across nodes, while preserving the privacy of the data. However, new challenges rise when it comes to scalable distributed learning approaches, taking into account the heterogeneous nature of future wireless networks. The current work proposes DISTINQT, a privacy-aware distributed learning framework for QoS prediction. Our framework supports mult
&lt;/p&gt;</description></item><item><title>H2G2-Net&#26159;&#19968;&#31181;&#29992;&#20110;&#21457;&#29616;&#22810;&#27169;&#24577;&#29983;&#29702;&#21453;&#24212;&#30340;&#20998;&#23618;&#24322;&#26500;&#22270;&#29983;&#25104;&#32593;&#32476;&#26694;&#26550;&#65292;&#33021;&#22815;&#33258;&#21160;&#23398;&#20064;&#22270;&#32467;&#26500;&#32780;&#19981;&#38656;&#35201;&#39044;&#23450;&#20041;&#30340;&#39046;&#22495;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2401.02905</link><description>&lt;p&gt;
H2G2-Net:&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#29983;&#29702;&#21453;&#24212;&#21457;&#29616;&#30340;&#20998;&#23618;&#24322;&#26500;&#22270;&#29983;&#25104;&#32593;&#32476;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
H2G2-Net: A Hierarchical Heterogeneous Graph Generative Network Framework for Discovery of Multi-Modal Physiological Responses. (arXiv:2401.02905v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02905
&lt;/p&gt;
&lt;p&gt;
H2G2-Net&#26159;&#19968;&#31181;&#29992;&#20110;&#21457;&#29616;&#22810;&#27169;&#24577;&#29983;&#29702;&#21453;&#24212;&#30340;&#20998;&#23618;&#24322;&#26500;&#22270;&#29983;&#25104;&#32593;&#32476;&#26694;&#26550;&#65292;&#33021;&#22815;&#33258;&#21160;&#23398;&#20064;&#22270;&#32467;&#26500;&#32780;&#19981;&#38656;&#35201;&#39044;&#23450;&#20041;&#30340;&#39046;&#22495;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#30740;&#31350;&#24212;&#29992;&#20013;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#29983;&#29702;&#20449;&#21495;&#26469;&#21457;&#29616;&#20154;&#31867;&#35748;&#30693;&#21644;&#24773;&#24863;&#29366;&#24577;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#20154;&#20307;&#30340;&#29983;&#29702;&#21453;&#24212;&#21463;&#21040;&#20154;&#31867;&#35748;&#30693;&#30340;&#24433;&#21709;&#65292;&#24120;&#29992;&#20110;&#20998;&#26512;&#35748;&#30693;&#29366;&#24577;&#12290;&#20174;&#32593;&#32476;&#31185;&#23398;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#20123;&#24322;&#26500;&#29983;&#29702;&#27169;&#24335;&#22312;&#22270;&#32467;&#26500;&#20013;&#30340;&#20114;&#21160;&#21487;&#33021;&#25552;&#20379;&#26377;&#30410;&#30340;&#20449;&#24687;&#26469;&#25903;&#25345;&#35748;&#30693;&#29366;&#24577;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#21150;&#27861;&#24471;&#21040;&#24322;&#26500;&#27169;&#24577;&#20043;&#38388;&#30340;&#31934;&#30830;&#36830;&#25509;&#65292;&#24182;&#19988;&#23384;&#22312;&#19968;&#31181;&#20998;&#23618;&#32467;&#26500;&#30340;&#23376;&#27169;&#24577;&#12290;&#29616;&#26377;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#29992;&#20110;&#22312;&#39044;&#23450;&#20041;&#30340;&#22270;&#32467;&#26500;&#19978;&#23398;&#20064;&#38750;&#23618;&#27425;&#21270;&#30340;&#21516;&#36136;&#22270;&#65292;&#26080;&#27861;&#20174;&#23618;&#27425;&#21270;&#30340;&#22810;&#27169;&#24577;&#29983;&#29702;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#27809;&#26377;&#39044;&#23450;&#20041;&#30340;&#22270;&#32467;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#24322;&#26500;&#22270;&#29983;&#25104;&#32593;&#32476;&#65288;H2G2-Net&#65289;&#65292;&#33021;&#22815;&#33258;&#21160;&#23398;&#20064;&#22270;&#32467;&#26500;&#32780;&#19981;&#38656;&#35201;&#20808;&#39564;&#39046;&#22495;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering human cognitive and emotional states using multi-modal physiological signals draws attention across various research applications. Physiological responses of the human body are influenced by human cognition and commonly used to analyze cognitive states. From a network science perspective, the interactions of these heterogeneous physiological modalities in a graph structure may provide insightful information to support prediction of cognitive states. However, there is no clue to derive exact connectivity between heterogeneous modalities and there exists a hierarchical structure of sub-modalities. Existing graph neural networks are designed to learn on non-hierarchical homogeneous graphs with pre-defined graph structures; they failed to learn from hierarchical, multi-modal physiological data without a pre-defined graph structure. To this end, we propose a hierarchical heterogeneous graph generative network (H2G2-Net) that automatically learns a graph structure without domain 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#36890;&#36807;&#27604;&#36739;&#20154;&#31867;&#20247;&#21253;&#21465;&#20107;&#21644;AI&#21465;&#20107;&#65292;&#25506;&#31350;&#20102;&#25991;&#21270;&#20135;&#29289;&#21644;&#31038;&#20250;&#20559;&#35265;&#22312;&#25925;&#20107;&#20013;&#30340;&#34920;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-3.5&#21644;GPT-4&#29983;&#25104;&#30340;&#21465;&#20107;&#26356;&#20855;&#36827;&#23637;&#24615;&#65292;&#24182;&#19988;&#26222;&#32599;&#31859;&#20462;&#26031;&#31070;&#35805;&#22312;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24819;&#35937;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.12902</link><description>&lt;p&gt;
&#23454;&#39564;&#21465;&#20107;&#65306;&#20154;&#31867;&#20247;&#21253;&#21465;&#20107;&#21644;AI&#21465;&#20107;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Experimental Narratives: A Comparison of Human Crowdsourced Storytelling and AI Storytelling. (arXiv:2310.12902v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#36890;&#36807;&#27604;&#36739;&#20154;&#31867;&#20247;&#21253;&#21465;&#20107;&#21644;AI&#21465;&#20107;&#65292;&#25506;&#31350;&#20102;&#25991;&#21270;&#20135;&#29289;&#21644;&#31038;&#20250;&#20559;&#35265;&#22312;&#25925;&#20107;&#20013;&#30340;&#34920;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-3.5&#21644;GPT-4&#29983;&#25104;&#30340;&#21465;&#20107;&#26356;&#20855;&#36827;&#23637;&#24615;&#65292;&#24182;&#19988;&#26222;&#32599;&#31859;&#20462;&#26031;&#31070;&#35805;&#22312;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24819;&#35937;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#32467;&#21512;&#34892;&#20026;&#21644;&#35745;&#31639;&#23454;&#39564;&#65292;&#21033;&#29992;&#34394;&#26500;&#30340;&#25552;&#31034;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#24037;&#20855;&#65292;&#30740;&#31350;&#20154;&#31867;&#21644;&#29983;&#25104;&#24335;AI&#21465;&#20107;&#20013;&#30340;&#25991;&#21270;&#20135;&#29289;&#21644;&#31038;&#20250;&#20559;&#35265;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;2019&#24180;6&#26376;&#30001;&#20247;&#21253;&#24037;&#20316;&#32773;&#21019;&#20316;&#30340;250&#20010;&#25925;&#20107;&#21644;2023&#24180;3&#26376;&#30001;GPT-3.5&#21644;GPT-4&#29983;&#25104;&#30340;80&#20010;&#25925;&#20107;&#65292;&#23558;&#21465;&#20107;&#23398;&#21644;&#25512;&#29702;&#32479;&#35745;&#23398;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#20247;&#21253;&#24037;&#20316;&#32773;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#37117;&#22238;&#31572;&#20102;&#20851;&#20110;&#19982;&#20154;&#24037;&#26234;&#33021;&#20154;&#31867;&#30456;&#24651;&#30340;&#20027;&#39064;&#30340;&#30456;&#21516;&#25552;&#31034;&#12290;&#25552;&#20986;&#30340;&#23454;&#39564;&#33539;&#24335;&#20351;&#20154;&#31867;&#21644;LLM&#29983;&#25104;&#30340;&#21465;&#20107;&#21487;&#20197;&#30452;&#25509;&#36827;&#34892;&#27604;&#36739;&#12290;&#23545;&#20110;&#25552;&#21040;&#26222;&#32599;&#31859;&#20462;&#26031;&#20027;&#39064;&#30340;&#22238;&#24212;&#35777;&#23454;&#20102;&#26222;&#32599;&#31859;&#20462;&#26031;&#31070;&#35805;&#22312;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38598;&#20307;&#24819;&#35937;&#20013;&#30340;&#26222;&#36941;&#23384;&#22312;&#12290;&#25152;&#26377;&#25552;&#20379;&#30340;&#21465;&#20107;&#37117;&#34920;&#29616;&#20986;&#31185;&#23398;&#25110;&#25216;&#26415;&#30340;&#36861;&#27714;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;GPT-3.5&#21644;&#23588;&#20854;&#26159;GPT-4&#29983;&#25104;&#30340;&#21465;&#20107;&#26356;&#20855;&#36827;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper proposes a framework that combines behavioral and computational experiments employing fictional prompts as a novel tool for investigating cultural artifacts and social biases in storytelling both by humans and generative AI. The study analyzes 250 stories authored by crowdworkers in June 2019 and 80 stories generated by GPT-3.5 and GPT-4 in March 2023 by merging methods from narratology and inferential statistics. Both crowdworkers and large language models responded to identical prompts about creating and falling in love with an artificial human. The proposed experimental paradigm allows a direct comparison between human and LLM-generated storytelling. Responses to the Pygmalionesque prompts confirm the pervasive presence of the Pygmalion myth in the collective imaginary of both humans and large language models. All solicited narratives present a scientific or technological pursuit. The analysis reveals that narratives from GPT-3.5 and particularly GPT-4 are more more progre
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SegLoc&#65292;&#19968;&#31181;&#29992;&#20110;&#23433;&#20840;&#26816;&#26597;X&#23556;&#32447;&#22270;&#20687;&#30340;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#30340;&#26032;&#39062;&#35270;&#35273;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#12290;&#35813;&#26041;&#26696;&#32467;&#21512;&#20102;&#23545;&#27604;&#23398;&#20064;&#21644;&#29616;&#26377;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#26377;&#30417;&#30563;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.08421</link><description>&lt;p&gt;
SegLoc: &#26032;&#39062;&#30340;&#35270;&#35273;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#29992;&#20110;&#23433;&#20840;&#26816;&#26597;X&#23556;&#32447;&#22270;&#20687;&#30340;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
SegLoc: Novel Visual Self-supervised Learning Scheme for Dense Prediction Tasks of Security Inspection X-ray Images. (arXiv:2310.08421v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08421
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SegLoc&#65292;&#19968;&#31181;&#29992;&#20110;&#23433;&#20840;&#26816;&#26597;X&#23556;&#32447;&#22270;&#20687;&#30340;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#30340;&#26032;&#39062;&#35270;&#35273;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#12290;&#35813;&#26041;&#26696;&#32467;&#21512;&#20102;&#23545;&#27604;&#23398;&#20064;&#21644;&#29616;&#26377;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#26377;&#30417;&#30563;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#26174;&#33879;&#36827;&#23637;&#24402;&#21151;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#30340;&#25972;&#21512;&#12290;&#23613;&#31649;&#22312;NLP&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#23601;&#65292;&#20294;&#19982;&#35745;&#31639;&#26426;&#35270;&#35273;&#30456;&#27604;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#36824;&#19981;&#33021;&#20445;&#25345;&#30456;&#24212;&#30340;&#21457;&#23637;&#12290;&#26368;&#36817;&#65292;&#23558;&#23545;&#27604;&#23398;&#20064;&#19982;&#29616;&#26377;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#35270;&#35273;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#34920;&#29616;&#20986;&#36229;&#36234;&#26377;&#30417;&#30563;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#25913;&#36827;&#37117;&#23616;&#38480;&#20110;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#19988;&#21482;&#26377;&#23569;&#25968;&#24037;&#20316;&#33268;&#21147;&#20110;&#35780;&#20272;&#35745;&#31639;&#26426;&#35270;&#35273;&#23454;&#38469;&#22330;&#26223;&#19979;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#22823;&#37096;&#20998;&#24037;&#20316;&#38598;&#20013;&#22312;&#21253;&#21547;&#31867;&#21035;&#20154;&#20687;&#22270;&#20687;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#23588;&#20854;&#26159;ImageNet&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#23433;&#20840;&#26816;&#26597;X&#23556;&#32447;&#22270;&#20687;&#20013;&#30340;&#35821;&#20041;&#20998;&#21106;&#30340;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#26469;&#35780;&#20272;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;SegLoc&#12290;
&lt;/p&gt;
&lt;p&gt;
Lately, remarkable advancements of artificial intelligence have been attributed to the integration of self-supervised learning scheme. Despite impressive achievements within NLP, yet SSL in computer vision has not been able to stay on track comparatively. Recently, integration of contrastive learning on top of existing SSL models has established considerable progress in computer vision through which visual SSL models have outperformed their supervised counterparts. Nevertheless, most of these improvements were limited to classification tasks, and also, few works have been dedicated to evaluation of SSL models in real-world scenarios of computer vision, while the majority of works are centered around datasets containing class-wise portrait images, most notably, ImageNet. Consequently, in this work, we have considered dense prediction task of semantic segmentation in security inspection x-ray images to evaluate our proposed model Segmentation Localization. Based upon the model Instance L
&lt;/p&gt;</description></item><item><title>Neur2RO&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#39537;&#21160;&#30340;&#20108;&#38454;&#27573;&#40065;&#26834;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20272;&#35745;&#31532;&#20108;&#38454;&#27573;&#38382;&#39064;&#30340;&#20540;&#20989;&#25968;&#65292;&#24182;&#23884;&#20837;&#21040;&#32463;&#20856;&#30340;&#21015;-&#32422;&#26463;&#29983;&#25104;&#31639;&#27861;&#20013;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#27714;&#35299;&#23884;&#22871;&#30340;&#26368;&#23567;-&#26368;&#22823;-&#26368;&#23567;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.04345</link><description>&lt;p&gt;
Neur2RO: &#31070;&#32463;&#20108;&#38454;&#27573;&#40065;&#26834;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Neur2RO: Neural Two-Stage Robust Optimization. (arXiv:2310.04345v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04345
&lt;/p&gt;
&lt;p&gt;
Neur2RO&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#39537;&#21160;&#30340;&#20108;&#38454;&#27573;&#40065;&#26834;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20272;&#35745;&#31532;&#20108;&#38454;&#27573;&#38382;&#39064;&#30340;&#20540;&#20989;&#25968;&#65292;&#24182;&#23884;&#20837;&#21040;&#32463;&#20856;&#30340;&#21015;-&#32422;&#26463;&#29983;&#25104;&#31639;&#27861;&#20013;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#27714;&#35299;&#23884;&#22871;&#30340;&#26368;&#23567;-&#26368;&#22823;-&#26368;&#23567;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#20248;&#21270;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#19979;&#24314;&#27169;&#21644;&#35299;&#20915;&#20915;&#31574;&#38382;&#39064;&#12290;&#26412;&#24037;&#20316;&#35299;&#20915;&#20102;&#20108;&#38454;&#27573;&#40065;&#26834;&#20248;&#21270;&#65288;&#20063;&#31216;&#20026;&#21487;&#35843;&#25972;&#40065;&#26834;&#20248;&#21270;&#65289;&#38382;&#39064;&#65292;&#22312;&#19981;&#30830;&#23450;&#24615;&#23454;&#29616;&#20043;&#21069;&#21644;&#20043;&#21518;&#36827;&#34892;&#31532;&#19968;&#38454;&#27573;&#21644;&#31532;&#20108;&#38454;&#27573;&#30340;&#20915;&#31574;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#23884;&#22871;&#30340;&#26368;&#23567;-&#26368;&#22823;-&#26368;&#23567;&#20248;&#21270;&#38382;&#39064;&#65292;&#20174;&#35745;&#31639;&#19978;&#26469;&#35828;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#23588;&#20854;&#26159;&#24403;&#20915;&#31574;&#26159;&#31163;&#25955;&#30340;&#26102;&#20505;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Neur2RO&#65292;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21015;-&#32422;&#26463;&#29983;&#25104;&#65288;CCG&#65289;&#30340;&#23454;&#20363;&#31639;&#27861;&#65292;CCG&#26159;&#20108;&#38454;&#27573;&#40065;&#26834;&#20248;&#21270;&#30340;&#32463;&#20856;&#36845;&#20195;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#23398;&#20064;&#20272;&#35745;&#31532;&#20108;&#38454;&#27573;&#38382;&#39064;&#30340;&#20540;&#20989;&#25968;&#65292;&#36825;&#31181;&#26550;&#26500;&#26131;&#20110;&#20248;&#21270;&#12290;&#23558;&#25105;&#20204;&#30340;&#31070;&#32463;&#32593;&#32476;&#23884;&#20837;&#21040;CCG&#31639;&#27861;&#20013;&#65292;&#21487;&#20197;&#24555;&#36895;&#24471;&#21040;&#39640;&#36136;&#37327;&#30340;&#35299;&#65292;&#36825;&#22312;&#20004;&#20010;&#20108;&#38454;&#27573;&#40065;&#26834;&#20248;&#21270;&#22522;&#20934;&#27979;&#35797;&#65288;&#32972;&#21253;&#38382;&#39064;&#21644;&#36164;&#26412;&#39044;&#31639;&#65289;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust optimization provides a mathematical framework for modeling and solving decision-making problems under worst-case uncertainty. This work addresses two-stage robust optimization (2RO) problems (also called adjustable robust optimization), wherein first-stage and second-stage decisions are made before and after uncertainty is realized, respectively. This results in a nested min-max-min optimization problem which is extremely challenging computationally, especially when the decisions are discrete. We propose Neur2RO, an efficient machine learning-driven instantiation of column-and-constraint generation (CCG), a classical iterative algorithm for 2RO. Specifically, we learn to estimate the value function of the second-stage problem via a novel neural network architecture that is easy to optimize over by design. Embedding our neural network into CCG yields high-quality solutions quickly as evidenced by experiments on two 2RO benchmarks, knapsack and capital budgeting. For knapsack, Ne
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#22270;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;BGGAN&#65289;&#65292;&#29992;&#20110;&#34920;&#31034;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#30340;&#33041;&#32467;&#26500;-&#21151;&#33021;&#36830;&#25509;&#12290;&#36890;&#36807;&#29305;&#27530;&#35774;&#35745;&#30340;&#20869;&#37096;&#22270;&#21367;&#31215;&#32593;&#32476;&#27169;&#22359;&#21644;&#24179;&#34913;&#22120;&#27169;&#22359;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#23398;&#20064;&#32467;&#26500;&#22495;&#21644;&#21151;&#33021;&#22495;&#20043;&#38388;&#30340;&#26144;&#23556;&#20989;&#25968;&#65292;&#24182;&#35299;&#20915;&#27169;&#24335;&#22349;&#22604;&#38382;&#39064;&#65292;&#21516;&#26102;&#23398;&#20064;&#32467;&#26500;&#21644;&#21151;&#33021;&#29305;&#24449;&#30340;&#20114;&#34917;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08916</link><description>&lt;p&gt;
&#21452;&#21521;&#22270;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65306;&#29992;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#33041;&#32467;&#26500;-&#21151;&#33021;&#36830;&#25509;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Bidirectional Graph GAN: Representing Brain Structure-Function Connections for Alzheimer's Disease. (arXiv:2309.08916v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#22270;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;BGGAN&#65289;&#65292;&#29992;&#20110;&#34920;&#31034;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#30340;&#33041;&#32467;&#26500;-&#21151;&#33021;&#36830;&#25509;&#12290;&#36890;&#36807;&#29305;&#27530;&#35774;&#35745;&#30340;&#20869;&#37096;&#22270;&#21367;&#31215;&#32593;&#32476;&#27169;&#22359;&#21644;&#24179;&#34913;&#22120;&#27169;&#22359;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#23398;&#20064;&#32467;&#26500;&#22495;&#21644;&#21151;&#33021;&#22495;&#20043;&#38388;&#30340;&#26144;&#23556;&#20989;&#25968;&#65292;&#24182;&#35299;&#20915;&#27169;&#24335;&#22349;&#22604;&#38382;&#39064;&#65292;&#21516;&#26102;&#23398;&#20064;&#32467;&#26500;&#21644;&#21151;&#33021;&#29305;&#24449;&#30340;&#20114;&#34917;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25581;&#31034;&#33041;&#30142;&#30149;&#30340;&#21457;&#30149;&#26426;&#21046;&#65292;&#21253;&#25324;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#65292;&#33041;&#32467;&#26500;&#19982;&#21151;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21508;&#31181;&#21407;&#22240;&#65292;&#23558;&#33041;&#32467;&#26500;-&#21151;&#33021;&#36830;&#25509;&#26144;&#23556;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#22270;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;BGGAN&#65289;&#26469;&#34920;&#31034;&#33041;&#32467;&#26500;-&#21151;&#33021;&#36830;&#25509;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#20869;&#37096;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;InnerGCN&#65289;&#27169;&#22359;&#65292;BGGAN&#30340;&#29983;&#25104;&#22120;&#21487;&#20197;&#21033;&#29992;&#30452;&#25509;&#21644;&#38388;&#25509;&#33041;&#21306;&#22495;&#30340;&#29305;&#24449;&#26469;&#23398;&#20064;&#32467;&#26500;&#22495;&#21644;&#21151;&#33021;&#22495;&#20043;&#38388;&#30340;&#26144;&#23556;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;Balancer&#30340;&#26032;&#27169;&#22359;&#26469;&#24179;&#34913;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#20043;&#38388;&#30340;&#20248;&#21270;&#12290;&#36890;&#36807;&#23558;Balancer&#24341;&#20837;&#21040;BGGAN&#20013;&#65292;&#32467;&#26500;&#29983;&#25104;&#22120;&#21644;&#21151;&#33021;&#29983;&#25104;&#22120;&#19981;&#20165;&#21487;&#20197;&#32531;&#35299;&#27169;&#24335;&#22349;&#22604;&#38382;&#39064;&#65292;&#36824;&#21487;&#20197;&#23398;&#20064;&#32467;&#26500;&#21644;&#21151;&#33021;&#29305;&#24449;&#30340;&#20114;&#34917;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;AD&#20013;&#20934;&#30830;&#22320;&#34920;&#31034;&#33041;&#32467;&#26500;-&#21151;&#33021;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
The relationship between brain structure and function is critical for revealing the pathogenesis of brain disease, including Alzheimer's disease (AD). However, it is a great challenge to map brain structure-function connections due to various reasons. In this work, a bidirectional graph generative adversarial networks (BGGAN) is proposed to represent brain structure-function connections. Specifically, by designing a module incorporating inner graph convolution network (InnerGCN), the generators of BGGAN can employ features of direct and indirect brain regions to learn the mapping function between structural domain and functional domain. Besides, a new module named Balancer is designed to counterpoise the optimization between generators and discriminators. By introducing the Balancer into BGGAN, both the structural generator and functional generator can not only alleviate the issue of mode collapse but also learn complementarity of structural and functional features. Experimental result
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#27169;&#22411;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#30784;&#30340;&#22823;&#22411;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#36890;&#36807;&#25972;&#21512;&#20020;&#24202;&#19987;&#19994;&#30693;&#35782;&#12289;&#39046;&#22495;&#30693;&#35782;&#21644;&#22810;&#27169;&#24577;&#33021;&#21147;&#26469;&#24320;&#21457;&#21644;&#37096;&#32626;AGI&#33021;&#22815;&#35299;&#20915;&#21307;&#23398;&#39046;&#22495;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.05480</link><description>&lt;p&gt;
&#21307;&#23398;&#25104;&#20687;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Artificial General Intelligence for Medical Imaging. (arXiv:2306.05480v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#27169;&#22411;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#30784;&#30340;&#22823;&#22411;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#36890;&#36807;&#25972;&#21512;&#20020;&#24202;&#19987;&#19994;&#30693;&#35782;&#12289;&#39046;&#22495;&#30693;&#35782;&#21644;&#22810;&#27169;&#24577;&#33021;&#21147;&#26469;&#24320;&#21457;&#21644;&#37096;&#32626;AGI&#33021;&#22815;&#35299;&#20915;&#21307;&#23398;&#39046;&#22495;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#27169;&#22411;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#30784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#22823;&#22411;&#35270;&#35273;&#27169;&#22411;&#21644;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#25105;&#20204;&#24378;&#35843;&#23558;&#20020;&#24202;&#19987;&#19994;&#30693;&#35782;&#12289;&#39046;&#22495;&#30693;&#35782;&#21644;&#22810;&#27169;&#24577;&#33021;&#21147;&#25972;&#21512;&#21040;AGI&#27169;&#22411;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25351;&#23548;&#21307;&#30103;&#20445;&#20581;AGI&#27169;&#22411;&#24320;&#21457;&#21644;&#37096;&#32626;&#30340;&#36335;&#32447;&#22270;&#12290;&#22312;&#25972;&#20010;&#32508;&#36848;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#22312;&#21307;&#23398;&#39046;&#22495;&#37096;&#32626;&#22823;&#35268;&#27169;AGI&#27169;&#22411;&#21487;&#33021;&#38754;&#20020;&#30340;&#28508;&#22312;&#25361;&#25112;&#21644;&#32570;&#38519;&#30340;&#20851;&#38190;&#35266;&#28857;&#12290;&#36825;&#31687;&#32508;&#21512;&#24615;&#30340;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#26377;&#20851;AGI&#23545;&#21307;&#23398;&#25104;&#20687;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#20854;&#20182;&#39046;&#22495;&#26410;&#26469;&#24433;&#21709;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this review, we explore the potential applications of Artificial General Intelligence (AGI) models in healthcare, focusing on foundational Large Language Models (LLMs), Large Vision Models, and Large Multimodal Models. We emphasize the importance of integrating clinical expertise, domain knowledge, and multimodal capabilities into AGI models. In addition, we lay out key roadmaps that guide the development and deployment of healthcare AGI models. Throughout the review, we provide critical perspectives on the potential challenges and pitfalls associated with deploying large-scale AGI models in the medical field. This comprehensive review aims to offer insights into the future implications of AGI in medical imaging, healthcare and beyond.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DU-Shapley&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26356;&#26377;&#25928;&#22320;&#35745;&#31639;Shapley&#20540;&#65292;&#20197;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#38598;&#20215;&#20540;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.02071</link><description>&lt;p&gt;
DU-Shapley: &#19968;&#31181;&#26377;&#25928;&#30340;&#25968;&#25454;&#38598;&#20215;&#20540;&#35780;&#20272;&#30340;Shapley&#20540;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
DU-Shapley: A Shapley Value Proxy for Efficient Dataset Valuation. (arXiv:2306.02071v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DU-Shapley&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26356;&#26377;&#25928;&#22320;&#35745;&#31639;Shapley&#20540;&#65292;&#20197;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#38598;&#20215;&#20540;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#38656;&#35201;&#36827;&#34892;&#25968;&#25454;&#38598;&#35780;&#20272;&#65292;&#21363;&#37327;&#21270;&#23558;&#19968;&#20010;&#21333;&#29420;&#30340;&#25968;&#25454;&#38598;&#19982;&#20854;&#20182;&#25968;&#25454;&#38598;&#32858;&#21512;&#30340;&#22686;&#37327;&#25910;&#30410;&#65292;&#20197;&#26576;&#20123;&#30456;&#20851;&#39044;&#23450;&#20041;&#20844;&#29992;&#20107;&#19994;&#20026;&#22522;&#30784;&#12290;&#26368;&#36817;&#65292;Shapley&#20540;&#34987;&#25552;&#20986;&#20316;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#19968;&#31181;&#22522;&#26412;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#24418;&#24335;&#20844;&#29702;&#35777;&#26126;&#12290;&#30001;&#20110;&#20854;&#35745;&#31639;&#36890;&#24120;&#38656;&#35201;&#25351;&#25968;&#26102;&#38388;&#65292;&#22240;&#27492;&#32771;&#34385;&#22522;&#20110;Monte Carlo&#31215;&#20998;&#30340;&#26631;&#20934;&#36817;&#20284;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#36890;&#29992;&#36817;&#20284;&#26041;&#27861;&#20173;&#28982;&#26114;&#36149;&#12290;&#26412;&#25991;&#21033;&#29992;&#25968;&#25454;&#38598;&#35780;&#20272;&#38382;&#39064;&#30340;&#32467;&#26500;&#30693;&#35782;&#65292;&#35774;&#35745;&#20102;&#26356;&#26377;&#25928;&#30340;Shapley&#20540;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Shapley&#20540;&#36817;&#20284;&#65292;&#31216;&#20026;&#31163;&#25955;&#22343;&#21248;Shapley (DU-Shapley)&#65292;&#20854;&#34920;&#36798;&#20026;&#26399;&#26395;&#20540;
&lt;/p&gt;
&lt;p&gt;
Many machine learning problems require performing dataset valuation, i.e. to quantify the incremental gain, to some relevant pre-defined utility, of aggregating an individual dataset to others. As seminal examples, dataset valuation has been leveraged in collaborative and federated learning to create incentives for data sharing across several data owners. The Shapley value has recently been proposed as a principled tool to achieve this goal due to formal axiomatic justification. Since its computation often requires exponential time, standard approximation strategies based on Monte Carlo integration have been considered. Such generic approximation methods, however, remain expensive in some cases. In this paper, we exploit the knowledge about the structure of the dataset valuation problem to devise more efficient Shapley value estimators. We propose a novel approximation of the Shapley value, referred to as discrete uniform Shapley (DU-Shapley) which is expressed as an expectation under 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#37325;&#26368;&#20248;&#20256;&#36755;&#26631;&#31614;&#20998;&#37197;(DOTLA)&#26694;&#26550;&#65292;&#20197;&#21516;&#26102;&#23558;&#19968;&#20010;&#27169;&#24577;&#20013;&#29983;&#25104;&#30340;&#26631;&#31614;&#20998;&#37197;&#32473;&#20854;&#23545;&#24212;&#30340;&#27169;&#24577;&#65292;&#23454;&#29616;&#26080;&#30417;&#30563;&#21487;&#35265;-&#32418;&#22806;&#20154;&#21592;&#20877;&#35782;&#21035;&#12290;&#22312;&#30456;&#24212;&#27169;&#24577;&#20013;&#37051;&#23621;&#26679;&#26412;&#30340;&#25351;&#23548;&#19979;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#27169;&#24577;&#37051;&#23621;&#19968;&#33268;&#24615;&#24341;&#23548;&#30340;&#26631;&#31614;&#31934;&#28860;&#21644;&#27491;&#21017;&#21270;&#27169;&#22359;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.12711</link><description>&lt;p&gt;
&#36890;&#36807;&#37051;&#23621;&#24341;&#23548;&#30340;&#26631;&#31614;&#31934;&#28860;&#21327;&#21516;&#23398;&#20064;&#23454;&#29616;&#26080;&#30417;&#30563;&#21487;&#35265;-&#32418;&#22806;&#20154;&#21592;&#20877;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Visible-Infrared Person ReID by Collaborative Learning with Neighbor-Guided Label Refinement. (arXiv:2305.12711v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#37325;&#26368;&#20248;&#20256;&#36755;&#26631;&#31614;&#20998;&#37197;(DOTLA)&#26694;&#26550;&#65292;&#20197;&#21516;&#26102;&#23558;&#19968;&#20010;&#27169;&#24577;&#20013;&#29983;&#25104;&#30340;&#26631;&#31614;&#20998;&#37197;&#32473;&#20854;&#23545;&#24212;&#30340;&#27169;&#24577;&#65292;&#23454;&#29616;&#26080;&#30417;&#30563;&#21487;&#35265;-&#32418;&#22806;&#20154;&#21592;&#20877;&#35782;&#21035;&#12290;&#22312;&#30456;&#24212;&#27169;&#24577;&#20013;&#37051;&#23621;&#26679;&#26412;&#30340;&#25351;&#23548;&#19979;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#27169;&#24577;&#37051;&#23621;&#19968;&#33268;&#24615;&#24341;&#23548;&#30340;&#26631;&#31614;&#31934;&#28860;&#21644;&#27491;&#21017;&#21270;&#27169;&#22359;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#21487;&#35265;-&#32418;&#22806;&#20154;&#21592;&#20877;&#35782;&#21035;(USL-VI-ReID)&#26088;&#22312;&#20174;&#26410;&#26631;&#35760;&#30340;&#36328;&#27169;&#24577;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#27169;&#24577;&#19981;&#21464;&#29305;&#24449;&#65292;&#36825;&#22312;&#35270;&#39057;&#30417;&#25511;&#31995;&#32479;&#31561;&#23454;&#38469;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#35299;&#20915;&#36328;&#27169;&#24577;&#25968;&#25454;&#20851;&#32852;&#38382;&#39064;&#23545;&#20110;&#36827;&#19968;&#27493;&#36827;&#34892;&#24322;&#36136;&#32852;&#21512;&#23398;&#20064;&#38750;&#24120;&#20851;&#38190;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#37325;&#26368;&#20248;&#20256;&#36755;&#26631;&#31614;&#20998;&#37197;(DOTLA)&#26694;&#26550;&#65292;&#21516;&#26102;&#23558;&#19968;&#20010;&#27169;&#24577;&#20013;&#29983;&#25104;&#30340;&#26631;&#31614;&#20998;&#37197;&#32473;&#20854;&#23545;&#24212;&#30340;&#27169;&#24577;&#12290;&#25152;&#25552;&#20986;&#30340;DOTLA&#26426;&#21046;formulate&#20102;&#19968;&#31181;&#30456;&#20114;&#22686;&#24378;&#21644;&#39640;&#25928;&#30340;&#36328;&#27169;&#24577;&#25968;&#25454;&#20851;&#32852;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#19968;&#20123;&#19981;&#36275;&#21644;&#22122;&#22768;&#26631;&#31614;&#20851;&#32852;&#30340;&#21103;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#27169;&#24577;&#37051;&#23621;&#19968;&#33268;&#24615;&#24341;&#23548;&#30340;&#26631;&#31614;&#31934;&#28860;&#21644;&#27491;&#21017;&#21270;&#27169;&#22359;&#65292;&#22312;&#30456;&#24212;&#27169;&#24577;&#20013;&#37051;&#23621;&#26679;&#26412;&#30340;&#25351;&#23548;&#19979;&#28040;&#38500;&#30001;&#19981;&#20934;&#30830;&#30340;&#30417;&#30563;&#20449;&#21495;&#24102;&#26469;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;USL-VI-ReID&#27169;&#22411;&#19982;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#29978;&#33267;&#19968;&#20123;&#26377;&#30417;&#30563;&#26041;&#27861;&#30456;&#27604;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised learning visible-infrared person re-identification (USL-VI-ReID) aims at learning modality-invariant features from unlabeled cross-modality dataset, which is crucial for practical applications in video surveillance systems. The key to essentially address the USL-VI-ReID task is to solve the cross-modality data association problem for further heterogeneous joint learning. To address this issue, we propose a Dual Optimal Transport Label Assignment (DOTLA) framework to simultaneously assign the generated labels from one modality to its counterpart modality. The proposed DOTLA mechanism formulates a mutual reinforcement and efficient solution to cross-modality data association, which could effectively reduce the side-effects of some insufficient and noisy label associations. Besides, we further propose a cross-modality neighbor consistency guided label refinement and regularization module, to eliminate the negative effects brought by the inaccurate supervised signals, under th
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21305;&#37197;&#36328;&#27169;&#24577;&#32858;&#31867;&#26469;&#20943;&#23569;&#27169;&#24577;&#24046;&#24322;&#30340;&#21452;&#21521;&#32858;&#31867;&#21305;&#37197;&#23398;&#20064;&#26694;&#26550;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#27169;&#24577;&#29305;&#23450;&#21644;&#27169;&#24577;&#19981;&#21487;&#30693;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#26469;&#20849;&#21516;&#23545;&#40784;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2305.12673</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#21452;&#36793;&#36328;&#27169;&#24577;&#32858;&#31867;&#21305;&#37197;&#29992;&#20110;&#26080;&#30417;&#30563;&#21487;&#35265;&#20809;-&#32418;&#22806;&#20154;&#29289;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Efficient Bilateral Cross-Modality Cluster Matching for Unsupervised Visible-Infrared Person ReID. (arXiv:2305.12673v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12673
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21305;&#37197;&#36328;&#27169;&#24577;&#32858;&#31867;&#26469;&#20943;&#23569;&#27169;&#24577;&#24046;&#24322;&#30340;&#21452;&#21521;&#32858;&#31867;&#21305;&#37197;&#23398;&#20064;&#26694;&#26550;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#27169;&#24577;&#29305;&#23450;&#21644;&#27169;&#24577;&#19981;&#21487;&#30693;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#26469;&#20849;&#21516;&#23545;&#40784;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#21487;&#35265;&#20809;-&#32418;&#22806;&#20154;&#29289;&#35782;&#21035;&#65288;USL-VI-ReID&#65289;&#26088;&#22312;&#22312;&#27809;&#26377;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#21305;&#37197;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#34892;&#20154;&#22270;&#20687;&#20013;&#30456;&#21516;&#36523;&#20221;&#30340;&#26679;&#26412;&#12290;&#26412;&#25991;&#38024;&#23545;&#27809;&#26377;&#24456;&#22909;&#25506;&#32034;&#36328;&#27169;&#24577;&#32858;&#31867;&#20851;&#31995;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#21521;&#32858;&#31867;&#21305;&#37197;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21305;&#37197;&#36328;&#27169;&#24577;&#32858;&#31867;&#26469;&#20943;&#23569;&#27169;&#24577;&#24046;&#24322;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20108;&#20998;&#22270;&#20013;&#20248;&#21270;&#26368;&#22823;&#21305;&#37197;&#38382;&#39064;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#23545;&#22810;&#21452;&#36793;&#36328;&#27169;&#24577;&#32858;&#31867;&#21305;&#37197;&#65288;MBCCM&#65289;&#31639;&#27861;&#12290;&#28982;&#21518;&#65292;&#21305;&#37197;&#30340;&#25104;&#23545;&#32858;&#31867;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#21033;&#29992;&#20849;&#20139;&#30340;&#21487;&#35265;&#20809;&#21644;&#32418;&#22806;&#20266;&#26631;&#31614;&#12290;&#22312;&#36825;&#26679;&#30340;&#30417;&#30563;&#20449;&#21495;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24577;&#29305;&#23450;&#21644;&#27169;&#24577;&#19981;&#21487;&#30693;&#65288;MSMA&#65289;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#22312;&#32858;&#31867;&#32423;&#21035;&#19978;&#20849;&#21516;&#23545;&#40784;&#29305;&#24449;&#12290;&#21516;&#26102;&#65292;&#36328;&#27169;&#24577;&#30340;&#27169;&#24577;&#29305;&#23450;&#21644;&#27169;&#24577;&#19981;&#21487;&#30693;&#29305;&#24449;&#20063;&#34987;&#32771;&#34385;&#36827;&#21435;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised visible-infrared person re-identification (USL-VI-ReID) aims to match pedestrian images of the same identity from different modalities without annotations. Existing works mainly focus on alleviating the modality gap by aligning instance-level features of the unlabeled samples. However, the relationships between cross-modality clusters are not well explored. To this end, we propose a novel bilateral cluster matching-based learning framework to reduce the modality gap by matching cross-modality clusters. Specifically, we design a Many-to-many Bilateral Cross-Modality Cluster Matching (MBCCM) algorithm through optimizing the maximum matching problem in a bipartite graph. Then, the matched pairwise clusters utilize shared visible and infrared pseudo-labels during the model training. Under such a supervisory signal, a Modality-Specific and Modality-Agnostic (MSMA) contrastive learning framework is proposed to align features jointly at a cluster-level. Meanwhile, the cross-modal
&lt;/p&gt;</description></item><item><title>KD-BIRL&#26159;&#19968;&#31181;&#26680;&#23494;&#24230;&#36125;&#21494;&#26031;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#36924;&#36817;&#20284;&#28982;&#20989;&#25968;&#26469;&#23398;&#20064;&#20195;&#29702;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20811;&#26381;&#20102;&#23398;&#20064;&#28857;&#20272;&#35745;&#30340;&#32570;&#28857;&#65292;&#24182;&#36866;&#29992;&#20110;&#22797;&#26434;&#21644;&#26080;&#38480;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2303.06827</link><description>&lt;p&gt;
&#26680;&#23494;&#24230;&#36125;&#21494;&#26031;&#36870;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Kernel Density Bayesian Inverse Reinforcement Learning. (arXiv:2303.06827v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06827
&lt;/p&gt;
&lt;p&gt;
KD-BIRL&#26159;&#19968;&#31181;&#26680;&#23494;&#24230;&#36125;&#21494;&#26031;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#36924;&#36817;&#20284;&#28982;&#20989;&#25968;&#26469;&#23398;&#20064;&#20195;&#29702;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20811;&#26381;&#20102;&#23398;&#20064;&#28857;&#20272;&#35745;&#30340;&#32570;&#28857;&#65292;&#24182;&#36866;&#29992;&#20110;&#22797;&#26434;&#21644;&#26080;&#38480;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#35266;&#23519;&#20195;&#29702;&#34892;&#20026;&#26469;&#25512;&#26029;&#20854;&#22870;&#21169;&#20989;&#25968;&#30340;&#24378;&#22823;&#26694;&#26550;&#65292;&#20294;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#30340;&#28857;&#20272;&#35745;&#21487;&#33021;&#20250;&#35823;&#23548;&#65292;&#22240;&#20026;&#21487;&#33021;&#26377;&#22810;&#20010;&#20989;&#25968;&#33021;&#22815;&#24456;&#22909;&#22320;&#25551;&#36848;&#20195;&#29702;&#30340;&#34892;&#20026;&#12290;&#36125;&#21494;&#26031;&#36870;&#24378;&#21270;&#23398;&#20064;&#37319;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;&#27169;&#25311;&#20505;&#36873;&#22870;&#21169;&#20989;&#25968;&#30340;&#20998;&#24067;&#65292;&#20811;&#26381;&#20102;&#23398;&#20064;&#28857;&#20272;&#35745;&#30340;&#32570;&#28857;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#36125;&#21494;&#26031;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20351;&#29992;Q&#20540;&#20989;&#25968;&#20195;&#26367;&#20284;&#28982;&#20989;&#25968;&#12290;&#30001;&#27492;&#24471;&#21040;&#30340;&#21518;&#39564;&#35745;&#31639;&#37327;&#22823;&#65292;&#29702;&#35770;&#20445;&#35777;&#23569;&#65292;&#24182;&#19988;Q&#20540;&#20989;&#25968;&#36890;&#24120;&#23545;&#20284;&#28982;&#20989;&#25968;&#30340;&#36924;&#36817;&#25928;&#26524;&#36739;&#24046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26680;&#23494;&#24230;&#36125;&#21494;&#26031;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;KD-BIRL&#65289;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#26465;&#20214;&#26680;&#23494;&#24230;&#20272;&#35745;&#30452;&#25509;&#36924;&#36817;&#20284;&#28982;&#20989;&#25968;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#22312;&#32463;&#36807;&#25913;&#36827;&#30340;&#22870;&#21169;&#20989;&#25968;&#21442;&#25968;&#21270;&#19979;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#22797;&#26434;&#21644;&#26080;&#38480;&#30340;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse reinforcement learning~(IRL) is a powerful framework to infer an agent's reward function by observing its behavior, but IRL algorithms that learn point estimates of the reward function can be misleading because there may be several functions that describe an agent's behavior equally well. A Bayesian approach to IRL models a distribution over candidate reward functions, alleviating the shortcomings of learning a point estimate. However, several Bayesian IRL algorithms use a $Q$-value function in place of the likelihood function. The resulting posterior is computationally intensive to calculate, has few theoretical guarantees, and the $Q$-value function is often a poor approximation for the likelihood. We introduce kernel density Bayesian IRL (KD-BIRL), which uses conditional kernel density estimation to directly approximate the likelihood, providing an efficient framework that, with a modified reward function parameterization, is applicable to environments with complex and infin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#20250;&#21512;&#65292;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#23548;&#33322;&#21644;&#25511;&#21046;&#26694;&#26550;&#65292;&#29992;&#20110;&#21487;&#38752;&#12289;&#20934;&#30830;&#21644;&#33258;&#20027;&#22320;&#36973;&#36935;&#24555;&#36895;&#31227;&#21160;&#30340;&#26143;&#38469;&#29289;&#20307;&#12290;&#23427;&#36890;&#36807;&#28857;&#26368;&#23567;&#33539;&#25968;&#36861;&#36394;&#25511;&#21046;&#21644;&#35889;&#24402;&#19968;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24341;&#23548;&#31574;&#30053;&#26469;&#25552;&#20379;&#39640;&#27010;&#29575;&#25351;&#25968;&#19978;&#30028;&#30340;&#39134;&#34892;&#22120;&#20132;&#20184;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2208.04883</link><description>&lt;p&gt;
&#31070;&#32463;&#20250;&#21512;&#65306;&#38754;&#21521;&#26143;&#38469;&#29289;&#20307;&#30340;&#21487;&#38752;&#23548;&#33322;&#21644;&#25511;&#21046;&#30340;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Neural-Rendezvous: Provably Robust Guidance and Control to Encounter Interstellar Objects. (arXiv:2208.04883v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.04883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#20250;&#21512;&#65292;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#23548;&#33322;&#21644;&#25511;&#21046;&#26694;&#26550;&#65292;&#29992;&#20110;&#21487;&#38752;&#12289;&#20934;&#30830;&#21644;&#33258;&#20027;&#22320;&#36973;&#36935;&#24555;&#36895;&#31227;&#21160;&#30340;&#26143;&#38469;&#29289;&#20307;&#12290;&#23427;&#36890;&#36807;&#28857;&#26368;&#23567;&#33539;&#25968;&#36861;&#36394;&#25511;&#21046;&#21644;&#35889;&#24402;&#19968;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24341;&#23548;&#31574;&#30053;&#26469;&#25552;&#20379;&#39640;&#27010;&#29575;&#25351;&#25968;&#19978;&#30028;&#30340;&#39134;&#34892;&#22120;&#20132;&#20184;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26143;&#38469;&#29289;&#20307;&#65288;ISOs&#65289;&#24456;&#21487;&#33021;&#26159;&#19981;&#21487;&#26367;&#20195;&#30340;&#21407;&#22987;&#26448;&#26009;&#65292;&#22312;&#29702;&#35299;&#31995;&#22806;&#34892;&#26143;&#26143;&#31995;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#36816;&#34892;&#36712;&#36947;&#38590;&#20197;&#32422;&#26463;&#65292;&#36890;&#24120;&#20855;&#26377;&#36739;&#39640;&#30340;&#20542;&#35282;&#21644;&#30456;&#23545;&#36895;&#24230;&#65292;&#20351;&#29992;&#20256;&#32479;&#30340;&#20154;&#22312;&#29615;&#36335;&#26041;&#27861;&#25506;&#32034;ISOs&#20855;&#26377;&#30456;&#24403;&#22823;&#30340;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#20250;&#21512;&#30340;&#28145;&#24230;&#23398;&#20064;&#23548;&#33322;&#21644;&#25511;&#21046;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#23454;&#26102;&#20013;&#20197;&#21487;&#38752;&#12289;&#20934;&#30830;&#21644;&#33258;&#20027;&#30340;&#26041;&#24335;&#36973;&#36935;&#24555;&#36895;&#31227;&#21160;&#30340;&#29289;&#20307;&#65292;&#21253;&#25324;ISOs&#12290;&#23427;&#22312;&#22522;&#20110;&#35889;&#24402;&#19968;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24341;&#23548;&#31574;&#30053;&#20043;&#19978;&#20351;&#29992;&#28857;&#26368;&#23567;&#33539;&#25968;&#36861;&#36394;&#25511;&#21046;&#65292;&#20854;&#20013;&#21442;&#25968;&#36890;&#36807;&#30452;&#25509;&#24809;&#32602;MPC&#29366;&#24577;&#36712;&#36857;&#36319;&#36394;&#35823;&#24046;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35843;&#20248;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31070;&#32463;&#20250;&#21512;&#22312;&#39044;&#26399;&#30340;&#39134;&#34892;&#22120;&#20132;&#20184;&#35823;&#24046;&#19978;&#25552;&#20379;&#20102;&#39640;&#27010;&#29575;&#25351;&#25968;&#19978;&#30028;&#65292;&#20854;&#35777;&#26126;&#21033;&#29992;&#20102;&#38543;&#26426;&#36882;&#22686;&#31283;&#23450;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interstellar objects (ISOs) are likely representatives of primitive materials invaluable in understanding exoplanetary star systems. Due to their poorly constrained orbits with generally high inclinations and relative velocities, however, exploring ISOs with conventional human-in-the-loop approaches is significantly challenging. This paper presents Neural-Rendezvous, a deep learning-based guidance and control framework for encountering fast-moving objects, including ISOs, robustly, accurately, and autonomously in real time. It uses pointwise minimum norm tracking control on top of a guidance policy modeled by a spectrally-normalized deep neural network, where its hyperparameters are tuned with a loss function directly penalizing the MPC state trajectory tracking error. We show that Neural-Rendezvous provides a high probability exponential bound on the expected spacecraft delivery error, the proof of which leverages stochastic incremental stability analysis. In particular, it is used to
&lt;/p&gt;</description></item></channel></rss>