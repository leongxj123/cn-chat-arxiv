<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#30740;&#31350;&#22312;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#24207;&#21015;&#65292;&#25506;&#35752;&#28385;&#36275;&#29305;&#23450;&#32422;&#26463;&#30340;&#31574;&#30053;&#36335;&#24452;&#65292;&#21363;&#28385;&#36275;&#36335;&#24452;&#65292;&#23545;&#20110;&#26500;&#24314;&#32456;&#27490;&#20110;&#22343;&#34913;&#31574;&#30053;&#30340;&#36335;&#24452;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2403.18079</link><description>&lt;p&gt;
&#27491;&#24577;&#24418;&#24335;&#21338;&#24328;&#20013;&#30340;&#22343;&#34913;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Paths to Equilibrium in Normal-Form Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#24207;&#21015;&#65292;&#25506;&#35752;&#28385;&#36275;&#29305;&#23450;&#32422;&#26463;&#30340;&#31574;&#30053;&#36335;&#24452;&#65292;&#21363;&#28385;&#36275;&#36335;&#24452;&#65292;&#23545;&#20110;&#26500;&#24314;&#32456;&#27490;&#20110;&#22343;&#34913;&#31574;&#30053;&#30340;&#36335;&#24452;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#20013;&#65292;&#26234;&#20307;&#20250;&#21453;&#22797;&#22312;&#26102;&#38388;&#19978;&#20132;&#20114;&#65292;&#24182;&#38543;&#30528;&#26032;&#25968;&#25454;&#30340;&#21040;&#26469;&#20462;&#35746;&#20182;&#20204;&#30340;&#31574;&#30053;&#65292;&#20174;&#32780;&#20135;&#29983;&#19968;&#31995;&#21015;&#31574;&#30053;&#27010;&#20917;&#12290;&#26412;&#25991;&#30740;&#31350;&#28385;&#36275;&#19968;&#31181;&#30001;&#24378;&#21270;&#23398;&#20064;&#20013;&#25919;&#31574;&#26356;&#26032;&#21551;&#21457;&#30340;&#25104;&#23545;&#32422;&#26463;&#30340;&#31574;&#30053;&#24207;&#21015;&#65292;&#20854;&#20013;&#22312;&#31532; $t$ &#26399;&#26368;&#20248;&#24212;&#31572;&#30340;&#26234;&#20307;&#22312;&#19979;&#19968;&#26399; $t+1$ &#19981;&#20250;&#25913;&#21464;&#20854;&#31574;&#30053;&#12290;&#36825;&#31181;&#32422;&#26463;&#20165;&#35201;&#27714;&#20248;&#21270;&#26234;&#20307;&#19981;&#26356;&#25913;&#31574;&#30053;&#65292;&#20294;&#24182;&#19981;&#20197;&#20219;&#20309;&#26041;&#24335;&#38480;&#21046;&#20854;&#20182;&#38750;&#26368;&#20248;&#21270;&#26234;&#20307;&#65292;&#22240;&#27492;&#20801;&#35768;&#25506;&#32034;&#12290;&#20855;&#26377;&#27492;&#23646;&#24615;&#30340;&#24207;&#21015;&#34987;&#31216;&#20026;&#28385;&#36275;&#36335;&#24452;&#65292;&#24182;&#22312;&#35768;&#22810; MARL &#31639;&#27861;&#20013;&#33258;&#28982;&#20986;&#29616;&#12290;&#20851;&#20110;&#25112;&#30053;&#21160;&#24577;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#65306;&#23545;&#20110;&#32473;&#23450;&#30340;&#21338;&#24328;&#21644;&#21021;&#22987;&#31574;&#30053;&#27010;&#20917;&#65292;&#26159;&#21542;&#24635;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;&#32456;&#27490;&#20110;&#22343;&#34913;&#31574;&#30053;&#30340;&#28385;&#36275;&#36335;&#24452;&#65311;&#36825;&#20010;&#38382;&#39064;&#30340;&#35299;&#20915;&#23545;&#24212;&#30528;&#19968;&#20123;&#37325;&#35201;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18079v1 Announce Type: cross  Abstract: In multi-agent reinforcement learning (MARL), agents repeatedly interact across time and revise their strategies as new data arrives, producing a sequence of strategy profiles. This paper studies sequences of strategies satisfying a pairwise constraint inspired by policy updating in reinforcement learning, where an agent who is best responding in period $t$ does not switch its strategy in the next period $t+1$. This constraint merely requires that optimizing agents do not switch strategies, but does not constrain the other non-optimizing agents in any way, and thus allows for exploration. Sequences with this property are called satisficing paths, and arise naturally in many MARL algorithms. A fundamental question about strategic dynamics is such: for a given game and initial strategy profile, is it always possible to construct a satisficing path that terminates at an equilibrium strategy? The resolution of this question has implication
&lt;/p&gt;</description></item><item><title>BlendScape&#36890;&#36807;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#23454;&#29616;&#20102;&#29992;&#25143;&#23450;&#21046;&#35270;&#39057;&#20250;&#35758;&#29615;&#22659;&#65292;&#25903;&#25345;&#28789;&#27963;&#30340;&#20219;&#21153;&#31354;&#38388;&#34920;&#31034;&#21644;&#22810;&#27169;&#20132;&#20114;&#25216;&#26415;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#24555;&#36895;&#34920;&#36798;&#35774;&#35745;&#24847;&#22270;&#24182;&#35774;&#24819;&#26410;&#26469;&#20250;&#35758;&#20013;&#30340;&#21327;&#20316;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.13947</link><description>&lt;p&gt;
BlendScape: &#36890;&#36807;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#23454;&#29616;&#32479;&#19968;&#21644;&#20010;&#24615;&#21270;&#35270;&#39057;&#20250;&#35758;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
BlendScape: Enabling Unified and Personalized Video-Conferencing Environments through Generative AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13947
&lt;/p&gt;
&lt;p&gt;
BlendScape&#36890;&#36807;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#23454;&#29616;&#20102;&#29992;&#25143;&#23450;&#21046;&#35270;&#39057;&#20250;&#35758;&#29615;&#22659;&#65292;&#25903;&#25345;&#28789;&#27963;&#30340;&#20219;&#21153;&#31354;&#38388;&#34920;&#31034;&#21644;&#22810;&#27169;&#20132;&#20114;&#25216;&#26415;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#24555;&#36895;&#34920;&#36798;&#35774;&#35745;&#24847;&#22270;&#24182;&#35774;&#24819;&#26410;&#26469;&#20250;&#35758;&#20013;&#30340;&#21327;&#20316;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#30340;&#35270;&#39057;&#20250;&#35758;&#24037;&#20855;&#25903;&#25345;&#20016;&#23500;&#30340;&#19987;&#19994;&#21644;&#31038;&#20132;&#27963;&#21160;&#65292;&#20294;&#23427;&#20204;&#30340;&#36890;&#29992;&#12289;&#22522;&#20110;&#32593;&#26684;&#30340;&#29615;&#22659;&#26080;&#27861;&#36731;&#26494;&#22320;&#36866;&#24212;&#20998;&#24067;&#24335;&#21512;&#20316;&#32773;&#30340;&#21508;&#31181;&#38656;&#27714;&#12290;&#20026;&#20102;&#23454;&#29616;&#26368;&#32456;&#29992;&#25143;&#23450;&#21046;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;BlendScape&#65292;&#36825;&#26159;&#19968;&#20010;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;AI&#22270;&#20687;&#29983;&#25104;&#25216;&#26415;&#65292;&#20801;&#35768;&#20250;&#35758;&#21442;&#19982;&#32773;&#26500;&#24314;&#26681;&#25454;&#20182;&#20204;&#30340;&#21327;&#20316;&#32972;&#26223;&#23450;&#21046;&#30340;&#35270;&#39057;&#20250;&#35758;&#29615;&#22659;&#12290;BlendScape&#36890;&#36807;&#23558;&#29992;&#25143;&#30340;&#29289;&#29702;&#25110;&#34394;&#25311;&#32972;&#26223;&#34701;&#21512;&#21040;&#32479;&#19968;&#29615;&#22659;&#20013;&#25903;&#25345;&#20219;&#21153;&#31354;&#38388;&#30340;&#28789;&#27963;&#34920;&#31034;&#65292;&#24182;&#23454;&#29616;&#20102;&#22810;&#27169;&#20132;&#20114;&#25216;&#26415;&#20197;&#24341;&#23548;&#29983;&#25104;&#36807;&#31243;&#12290;&#36890;&#36807;&#19982;15&#21517;&#26368;&#32456;&#29992;&#25143;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20182;&#20204;&#23545;&#24037;&#20316;&#21644;&#31038;&#20132;&#22330;&#26223;&#30340;&#23450;&#21046;&#20559;&#22909;&#12290;&#21442;&#19982;&#32773;&#21487;&#20197;&#24555;&#36895;&#34920;&#36798;&#20182;&#20204;&#23545;BlendScape&#30340;&#35774;&#35745;&#24847;&#22270;&#65292;&#24182;&#35774;&#24819;&#20351;&#29992;&#35813;&#31995;&#32479;&#26469;&#32452;&#32455;&#26410;&#26469;&#20250;&#35758;&#20013;&#30340;&#21327;&#20316;&#65292;&#20294;&#20063;&#36935;&#21040;&#20102;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13947v1 Announce Type: cross  Abstract: Today's video-conferencing tools support a rich range of professional and social activities, but their generic, grid-based environments cannot be easily adapted to meet the varying needs of distributed collaborators. To enable end-user customization, we developed BlendScape, a system for meeting participants to compose video-conferencing environments tailored to their collaboration context by leveraging AI image generation techniques. BlendScape supports flexible representations of task spaces by blending users' physical or virtual backgrounds into unified environments and implements multimodal interaction techniques to steer the generation. Through an evaluation with 15 end-users, we investigated their customization preferences for work and social scenarios. Participants could rapidly express their design intentions with BlendScape and envisioned using the system to structure collaboration in future meetings, but experienced challenge
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21338;&#24328;&#35770;&#35270;&#35282;&#35780;&#20272;LLMs&#30340;&#20915;&#31574;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;GPT-3.5&#22312;&#31283;&#20581;&#24615;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#65292;&#32780;GPT-4&#21017;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.11807</link><description>&lt;p&gt;
LLM&#30340;&#20915;&#31574;&#27700;&#24179;&#22312;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#30340;&#35780;&#20272;&#31350;&#31455;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming Ability in Multi-Agent Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11807
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21338;&#24328;&#35770;&#35270;&#35282;&#35780;&#20272;LLMs&#30340;&#20915;&#31574;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;GPT-3.5&#22312;&#31283;&#20581;&#24615;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#65292;&#32780;GPT-4&#21017;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#21508;&#31181;&#33021;&#21147;&#65292;&#20026;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#26497;&#22909;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#21338;&#24328;&#35770;&#30340;&#35270;&#35282;&#25506;&#31350;LLMs&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#25903;&#25345;&#22810;&#20010;&#26234;&#33021;&#20307;&#21516;&#26102;&#21442;&#19982;&#30340;&#28216;&#25103;&#65292;&#24341;&#20837;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;GAMA-Bench&#65292;&#21253;&#25324;&#20843;&#20010;&#32463;&#20856;&#30340;&#22810;&#26234;&#33021;&#20307;&#28216;&#25103;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20998;&#26041;&#26696;&#65292;&#23450;&#37327;&#35780;&#20272;&#27169;&#22411;&#22312;&#36825;&#20123;&#28216;&#25103;&#20013;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;GAMA-Bench&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#30340;&#31283;&#20581;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#22686;&#24378;&#31574;&#30053;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#34429;&#28982;GPT-3.5&#34920;&#29616;&#20986;&#20196;&#20154;&#28385;&#24847;&#30340;&#31283;&#20581;&#24615;&#65292;&#20294;&#20854;&#27867;&#21270;&#33021;&#21147;&#30456;&#23545;&#26377;&#38480;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#19968;&#20123;&#26041;&#27861;&#22914;&#8220;&#24605;&#32500;&#38142;&#8221;&#65292;&#20854;&#24615;&#33021;&#21487;&#20197;&#24471;&#21040;&#25552;&#39640;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;LLMs&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT-4&#32988;&#36807;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11807v1 Announce Type: new  Abstract: Decision-making, a complicated task requiring various types of abilities, presents an excellent framework for assessing Large Language Models (LLMs). Our research investigates LLMs' decision-making capabilities through the lens of a well-established field, Game Theory. We focus specifically on games that support the participation of more than two agents simultaneously. Subsequently, we introduce our framework, GAMA-Bench, including eight classical multi-agent games. We design a scoring scheme to assess a model's performance in these games quantitatively. Through GAMA-Bench, we investigate LLMs' robustness, generalizability, and enhancement strategies. Results reveal that while GPT-3.5 shows satisfying robustness, its generalizability is relatively limited. However, its performance can be improved through approaches such as Chain-of-Thought. Additionally, we conduct evaluations across various LLMs and find that GPT-4 outperforms other mod
&lt;/p&gt;</description></item><item><title>GEAR&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;KV&#32531;&#23384;&#21387;&#32553;&#26694;&#26550;&#65292;&#23454;&#29616;&#20960;&#20046;&#26080;&#25439;&#30340;&#39640;&#27604;&#29575;&#21387;&#32553;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#20013;&#22240;&#32531;&#23384;&#38656;&#27714;&#22686;&#38271;&#32780;&#23548;&#33268;&#30340;&#35760;&#24518;&#32465;&#23450;&#38382;&#39064;&#21644;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.05527</link><description>&lt;p&gt;
GEAR: &#19968;&#31181;&#29992;&#20110;&#20960;&#20046;&#26080;&#25439;&#29983;&#25104;&#25512;&#26029;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;KV&#32531;&#23384;&#21387;&#32553;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
GEAR: An Efficient KV Cache Compression Recipefor Near-Lossless Generative Inference of LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05527
&lt;/p&gt;
&lt;p&gt;
GEAR&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;KV&#32531;&#23384;&#21387;&#32553;&#26694;&#26550;&#65292;&#23454;&#29616;&#20960;&#20046;&#26080;&#25439;&#30340;&#39640;&#27604;&#29575;&#21387;&#32553;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#20013;&#22240;&#32531;&#23384;&#38656;&#27714;&#22686;&#38271;&#32780;&#23548;&#33268;&#30340;&#35760;&#24518;&#32465;&#23450;&#38382;&#39064;&#21644;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;-&#20540;&#65288;KV&#65289;&#32531;&#23384;&#24050;&#25104;&#20026;&#21152;&#24555;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#26029;&#29983;&#25104;&#36895;&#24230;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#22686;&#21152;&#32780;&#22686;&#38271;&#30340;&#32531;&#23384;&#38656;&#27714;&#24050;&#23558;LLM&#25512;&#26029;&#36716;&#21464;&#20026;&#19968;&#20010;&#35760;&#24518;&#32465;&#23450;&#38382;&#39064;&#65292;&#26174;&#33879;&#22320;&#38480;&#21046;&#20102;&#31995;&#32479;&#21534;&#21520;&#37327;&#12290;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#20002;&#24323;&#19981;&#37325;&#35201;&#30340;&#26631;&#35760;&#25110;&#22343;&#21248;&#37327;&#21270;&#25152;&#26377;&#26465;&#30446;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24448;&#24448;&#20250;&#20135;&#29983;&#36739;&#39640;&#30340;&#36817;&#20284;&#35823;&#24046;&#26469;&#34920;&#31034;&#21387;&#32553;&#21518;&#30340;&#30697;&#38453;&#12290;&#33258;&#22238;&#24402;&#35299;&#30721;&#36807;&#31243;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#27599;&#20010;&#27493;&#39588;&#30340;&#35823;&#24046;&#65292;&#23548;&#33268;&#27169;&#22411;&#29983;&#25104;&#20013;&#30340;&#37325;&#22823;&#20559;&#24046;&#21644;&#24615;&#33021;&#24694;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GEAR&#65292;&#19968;&#31181;&#39640;&#25928;&#30340;KV&#32531;&#23384;&#21387;&#32553;&#26694;&#26550;&#65292;&#23454;&#29616;&#20960;&#20046;&#26080;&#25439;&#30340;&#39640;&#21387;&#32553;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05527v1 Announce Type: cross  Abstract: Key-value (KV) caching has become the de-facto to accelerate generation speed for large language models (LLMs) inference. However, the growing cache demand with increasing sequence length has transformed LLM inference to be a memory bound problem, significantly constraining the system throughput. Existing methods rely on dropping unimportant tokens or quantizing all entries uniformly. Such methods, however, often incur high approximation errors to represent the compressed matrices. The autoregressive decoding process further compounds the error of each step, resulting in critical deviation in model generation and deterioration of performance. To tackle this challenge, we propose GEAR, an efficient KV cache compression framework that achieves near-lossless high-ratio compression. GEAR first applies quantization to majority of entries of similar magnitudes to ultra-low precision. It then employs a low rank matrix to approximate the quant
&lt;/p&gt;</description></item><item><title>&#30446;&#21069;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#35270;&#35273;&#28436;&#32462;&#25512;&#29702;&#26041;&#38754;&#20173;&#23384;&#22312;&#36739;&#22823;&#24046;&#36317;&#21644;&#30450;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.04732</link><description>&lt;p&gt;
&#25105;&#20204;&#36317;&#31163;&#26234;&#33021;&#35270;&#35273;&#28436;&#32462;&#25512;&#29702;&#36824;&#26377;&#22810;&#36828;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Far Are We from Intelligent Visual Deductive Reasoning?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04732
&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#35270;&#35273;&#28436;&#32462;&#25512;&#29702;&#26041;&#38754;&#20173;&#23384;&#22312;&#36739;&#22823;&#24046;&#36317;&#21644;&#30450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35832;&#22914;GPT-4V&#20043;&#31867;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#22312;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#22522;&#20110;&#35270;&#35273;&#30340;&#28436;&#32462;&#25512;&#29702;&#65292;&#36825;&#26159;&#19968;&#20010;&#26356;&#22797;&#26434;&#20294;&#19981;&#22826;&#34987;&#25506;&#32034;&#30340;&#39046;&#22495;&#65292;&#24182;&#21457;&#29616;&#20102;&#24403;&#21069;&#39046;&#20808;&#30340;VLM&#20013;&#20197;&#21069;&#26410;&#26292;&#38706;&#30340;&#30450;&#28857;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#29790;&#25991;&#28176;&#36827;&#30697;&#38453;&#65288;RPM&#65289;&#26469;&#35780;&#20272;VLM&#22312;&#20165;&#20381;&#38752;&#35270;&#35273;&#32447;&#32034;&#36827;&#34892;&#22810;&#36339;&#20851;&#31995;&#21644;&#28436;&#32462;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545;&#20960;&#31181;&#27969;&#34892;&#30340;VLM&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#37319;&#29992;&#20102;&#26631;&#20934;&#31574;&#30053;&#65292;&#22914;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#33258;&#25105;&#19968;&#33268;&#24615;&#21644;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#65292;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;Mensa&#26234;&#21830;&#27979;&#35797;&#12289;&#26234;&#21830;&#27979;&#35797;&#21644;RAVEN&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;LLM&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#25512;&#29702;&#26041;&#38754;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#25105;&#20204;&#22312;&#35270;&#35273;&#28436;&#32462;&#25512;&#29702;&#26041;&#38754;&#20173;&#26377;&#24456;&#22823;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04732v1 Announce Type: new  Abstract: Vision-Language Models (VLMs) such as GPT-4V have recently demonstrated incredible strides on diverse vision language tasks. We dig into vision-based deductive reasoning, a more sophisticated but less explored realm, and find previously unexposed blindspots in the current SOTA VLMs. Specifically, we leverage Raven's Progressive Matrices (RPMs), to assess VLMs' abilities to perform multi-hop relational and deductive reasoning relying solely on visual clues. We perform comprehensive evaluations of several popular VLMs employing standard strategies such as in-context learning, self-consistency, and Chain-of-thoughts (CoT) on three diverse datasets, including the Mensa IQ test, IntelligenceTest, and RAVEN. The results reveal that despite the impressive capabilities of LLMs in text-based reasoning, we are still far from achieving comparable proficiency in visual deductive reasoning. We found that certain standard strategies that are effective
&lt;/p&gt;</description></item><item><title>ITL&#26159;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#30340;&#36716;&#23548;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#29616;&#23454;&#19990;&#30028;&#35774;&#32622;&#20013;&#33258;&#36866;&#24212;&#37319;&#26679;&#65292;&#20197;&#26368;&#22823;&#21270;&#20851;&#20110;&#25351;&#23450;&#39044;&#27979;&#30446;&#26631;&#30340;&#20449;&#24687;&#33719;&#21462;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#24494;&#35843;&#21644;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#24212;&#29992;&#20013;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.15898</link><description>&lt;p&gt;
&#22522;&#20110;&#20449;&#24687;&#30340;&#36716;&#23548;&#24335;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Information-based Transductive Active Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15898
&lt;/p&gt;
&lt;p&gt;
ITL&#26159;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#30340;&#36716;&#23548;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#29616;&#23454;&#19990;&#30028;&#35774;&#32622;&#20013;&#33258;&#36866;&#24212;&#37319;&#26679;&#65292;&#20197;&#26368;&#22823;&#21270;&#20851;&#20110;&#25351;&#23450;&#39044;&#27979;&#30446;&#26631;&#30340;&#20449;&#24687;&#33719;&#21462;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#24494;&#35843;&#21644;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#24212;&#29992;&#20013;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#20027;&#21160;&#23398;&#20064;&#25512;&#24191;&#21040;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#37319;&#26679;&#21463;&#38480;&#20110;&#21487;&#35775;&#38382;&#22495;&#30340;&#24773;&#20917;&#65292;&#32780;&#39044;&#27979;&#30446;&#26631;&#21487;&#33021;&#20301;&#20110;&#36825;&#20010;&#22495;&#20043;&#22806;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ITL&#65292;&#21363;&#22522;&#20110;&#20449;&#24687;&#30340;&#36716;&#23548;&#24335;&#23398;&#20064;&#65292;&#19968;&#31181;&#33258;&#36866;&#24212;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#20851;&#20110;&#25351;&#23450;&#39044;&#27979;&#30446;&#26631;&#30340;&#20449;&#24687;&#33719;&#21462;&#12290;&#22312;&#19968;&#33324;&#27491;&#21017;&#24615;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ITL&#25910;&#25947;&#21040;&#21487;&#20174;&#21487;&#35775;&#38382;&#25968;&#25454;&#20013;&#33719;&#24471;&#30340;&#26368;&#23567;&#21487;&#33021;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20851;&#38190;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;ITL&#65306;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#23569;&#26679;&#26412;&#24494;&#35843;&#21644;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;ITL&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15898v1 Announce Type: cross  Abstract: We generalize active learning to address real-world settings where sampling is restricted to an accessible region of the domain, while prediction targets may lie outside this region. To this end, we propose ITL, short for information-based transductive learning, an approach which samples adaptively to maximize the information gained about specified prediction targets. We show, under general regularity assumptions, that ITL converges uniformly to the smallest possible uncertainty obtainable from the accessible data. We demonstrate ITL in two key applications: Few-shot fine-tuning of large neural networks and safe Bayesian optimization, and in both cases, ITL significantly outperforms the state-of-the-art.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20174;&#21355;&#26143;&#22270;&#20687;&#20013;&#20010;&#21035;&#25551;&#32472;&#20301;&#20110;&#35199;&#29677;&#29273;&#20869;&#21326;&#36798;&#23665;&#33033;&#26519;&#32447;&#19978;&#30340;&#26460;&#26494;&#28748;&#26408;&#12290;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#36965;&#24863;&#24433;&#20687;&#21644;&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26816;&#27979;&#21644;&#25551;&#32472;&#33258;&#28982;&#23545;&#35937;&#22797;&#26434;&#29983;&#38271;&#27169;&#24335;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.17985</link><description>&lt;p&gt;
&#21315;&#38754;&#28748;&#26408;&#65306;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#20174;&#21355;&#26143;&#22270;&#20687;&#20013;&#36827;&#34892;&#20010;&#20307;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Shrub of a thousand faces: an individual segmentation from satellite images using deep learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17985
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20174;&#21355;&#26143;&#22270;&#20687;&#20013;&#20010;&#21035;&#25551;&#32472;&#20301;&#20110;&#35199;&#29677;&#29273;&#20869;&#21326;&#36798;&#23665;&#33033;&#26519;&#32447;&#19978;&#30340;&#26460;&#26494;&#28748;&#26408;&#12290;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#36965;&#24863;&#24433;&#20687;&#21644;&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26816;&#27979;&#21644;&#25551;&#32472;&#33258;&#28982;&#23545;&#35937;&#22797;&#26434;&#29983;&#38271;&#27169;&#24335;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#27979;&#38271;&#23551;&#28748;&#26408;&#65288;&#22914;&#24120;&#35265;&#30340;&#26460;&#26494;&#65289;&#30340;&#20998;&#24067;&#21644;&#22823;&#23567;&#32467;&#26500;&#21487;&#20197;&#29992;&#20110;&#20272;&#35745;&#27668;&#20505;&#21464;&#21270;&#23545;&#39640;&#23665;&#21644;&#39640;&#32428;&#24230;&#29983;&#24577;&#31995;&#32479;&#30340;&#38271;&#26399;&#24433;&#21709;&#12290;&#21382;&#21490;&#33322;&#31354;&#36229;&#39640;&#20998;&#36776;&#29575;&#24433;&#20687;&#25552;&#20379;&#20102;&#19968;&#31181;&#22238;&#39038;&#24615;&#24037;&#20855;&#65292;&#21487;&#20197;&#39640;&#31934;&#24230;&#30417;&#27979;&#28748;&#26408;&#30340;&#29983;&#38271;&#21644;&#20998;&#24067;&#12290;&#30446;&#21069;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26816;&#27979;&#21644;&#25551;&#32472;&#20855;&#26377;&#26126;&#30830;&#23450;&#20041;&#24418;&#29366;&#30340;&#23545;&#35937;&#30340;&#36718;&#24275;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#36866;&#24212;&#20110;&#26816;&#27979;&#34920;&#36798;&#22797;&#26434;&#29983;&#38271;&#27169;&#24335;&#30340;&#33258;&#28982;&#23545;&#35937;&#65288;&#20363;&#22914;&#26460;&#26494;&#65289;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#36965;&#24863;RGB&#24433;&#20687;&#19982;&#22522;&#20110;Mask R-CNN&#30340;&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20010;&#21035;&#25551;&#32472;&#35199;&#29677;&#29273;&#20869;&#21326;&#36798;&#23665;&#33033;&#26519;&#32447;&#19978;&#30340;&#26460;&#26494;&#28748;&#26408;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#26500;&#36896;&#35774;&#35745;&#65292;&#20854;&#20013;&#20351;&#29992;&#30340;&#26159;&#20809;&#35299;&#35793;&#25968;&#25454;&#65288;PI&#65289;&#21644;&#23454;&#22320;&#35843;&#26597;&#25968;&#25454;&#65288;FW&#65289;&#20998;&#21035;&#36827;&#34892;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monitoring the distribution and size structure of long-living shrubs, such as Juniperus communis, can be used to estimate the long-term effects of climate change on high-mountain and high latitude ecosystems. Historical aerial very-high resolution imagery offers a retrospective tool to monitor shrub growth and distribution at high precision. Currently, deep learning models provide impressive results for detecting and delineating the contour of objects with defined shapes. However, adapting these models to detect natural objects that express complex growth patterns, such as junipers, is still a challenging task.   This research presents a novel approach that leverages remotely sensed RGB imagery in conjunction with Mask R-CNN-based instance segmentation models to individually delineate Juniperus shrubs above the treeline in Sierra Nevada (Spain). In this study, we propose a new data construction design that consists in using photo interpreted (PI) and field work (FW) data to respectivel
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#29983;&#25104;&#26032;&#29305;&#24449;&#32452;&#21512;&#30340;&#22270;&#20687;&#65292;&#21487;&#20197;&#22312;&#38598;&#25104;&#27169;&#22411;&#20013;&#22686;&#21152;&#27169;&#22411;&#22810;&#26679;&#24615;&#65292;&#24182;&#20943;&#36731;&#25463;&#24452;&#20559;&#35265;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30417;&#30563;&#20449;&#21495;&#12290;</title><link>https://arxiv.org/abs/2311.16176</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#26679;&#21270;&#21512;&#25104;&#21644;&#25193;&#25955;&#27169;&#22411;&#20943;&#36731;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Mitigating Biases with Diverse Ensembles and Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16176
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#29983;&#25104;&#26032;&#29305;&#24449;&#32452;&#21512;&#30340;&#22270;&#20687;&#65292;&#21487;&#20197;&#22312;&#38598;&#25104;&#27169;&#22411;&#20013;&#22686;&#21152;&#27169;&#22411;&#22810;&#26679;&#24615;&#65292;&#24182;&#20943;&#36731;&#25463;&#24452;&#20559;&#35265;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30417;&#30563;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#21363;&#22810;&#20010;&#32447;&#32034;&#21487;&#20197;&#39044;&#27979;&#30446;&#26631;&#26631;&#31614;&#65292;&#24120;&#24120;&#23548;&#33268;&#19968;&#31181;&#31216;&#20026;&#25463;&#24452;&#20559;&#35265;&#30340;&#29616;&#35937;&#65292;&#21363;&#27169;&#22411;&#20381;&#36182;&#20110;&#38169;&#35823;&#30340;&#12289;&#26131;&#23398;&#30340;&#32447;&#32034;&#65292;&#32780;&#24573;&#30053;&#21487;&#38752;&#30340;&#32447;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#30340;&#38598;&#25104;&#22810;&#26679;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#20943;&#36731;&#25463;&#24452;&#20559;&#35265;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#29305;&#23450;&#30340;&#35757;&#32451;&#38388;&#38548;&#20013;&#65292;DPMs&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#26032;&#29305;&#24449;&#32452;&#21512;&#30340;&#22270;&#20687;&#65292;&#21363;&#20351;&#22312;&#26174;&#31034;&#30456;&#20851;&#36755;&#20837;&#29305;&#24449;&#30340;&#26679;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#20851;&#38190;&#23646;&#24615;&#36890;&#36807;&#38598;&#25104;&#19981;&#19968;&#33268;&#24615;&#29983;&#25104;&#21512;&#25104;&#21453;&#20107;&#23454;&#26469;&#22686;&#21152;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DPM&#24341;&#23548;&#30340;&#22810;&#26679;&#21270;&#36275;&#20197;&#28040;&#38500;&#23545;&#20027;&#35201;&#25463;&#24452;&#32447;&#32034;&#30340;&#20381;&#36182;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#20960;&#20010;&#22810;&#26679;&#21270;&#30446;&#26631;&#19978;&#22312;&#23454;&#35777;&#19978;&#37327;&#21270;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#26368;&#32456;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16176v2 Announce Type: replace-cross  Abstract: Spurious correlations in the data, where multiple cues are predictive of the target labels, often lead to a phenomenon known as shortcut bias, where a model relies on erroneous, easy-to-learn cues while ignoring reliable ones. In this work, we propose an ensemble diversification framework exploiting Diffusion Probabilistic Models (DPMs) for shortcut bias mitigation. We show that at particular training intervals, DPMs can generate images with novel feature combinations, even when trained on samples displaying correlated input features. We leverage this crucial property to generate synthetic counterfactuals to increase model diversity via ensemble disagreement. We show that DPM-guided diversification is sufficient to remove dependence on primary shortcut cues, without a need for additional supervised signals. We further empirically quantify its efficacy on several diversification objectives, and finally show improved generalizati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#27969;&#31243;&#65292;&#29992;&#20110;&#23545;AI&#35270;&#35273;&#27169;&#22411;&#22312;&#24320;&#25918;&#23384;&#20648;&#24211;&#20013;&#30340;&#36136;&#37327;&#23646;&#24615;&#36827;&#34892;&#20998;&#26512;&#65292;&#23588;&#20854;&#26159;&#22312;&#38754;&#23545;&#23545;&#25239;&#25915;&#20987;&#26102;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#28041;&#21450;&#20845;&#20010;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#35780;&#20272;&#22330;&#26223;&#65292;&#20197;&#35780;&#20272;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#35299;&#37322;&#25928;&#29992;&#21644;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2401.12261</link><description>&lt;p&gt;
&#22312;&#24320;&#25918;&#23384;&#20648;&#24211;&#20013;&#20998;&#26512;AI&#35270;&#35273;&#27169;&#22411;&#30340;&#36136;&#37327;&#23646;&#24615;&#21450;&#20854;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Analyzing the Quality Attributes of AI Vision Models in Open Repositories Under Adversarial Attacks. (arXiv:2401.12261v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#27969;&#31243;&#65292;&#29992;&#20110;&#23545;AI&#35270;&#35273;&#27169;&#22411;&#22312;&#24320;&#25918;&#23384;&#20648;&#24211;&#20013;&#30340;&#36136;&#37327;&#23646;&#24615;&#36827;&#34892;&#20998;&#26512;&#65292;&#23588;&#20854;&#26159;&#22312;&#38754;&#23545;&#23545;&#25239;&#25915;&#20987;&#26102;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#28041;&#21450;&#20845;&#20010;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#35780;&#20272;&#22330;&#26223;&#65292;&#20197;&#35780;&#20272;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#35299;&#37322;&#25928;&#29992;&#21644;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;AI&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#23427;&#20204;&#32463;&#24120;&#21457;&#24067;&#21040;&#24320;&#25918;&#23384;&#20648;&#24211;&#20013;&#65292;&#22914;HuggingFace&#12290;&#22312;&#23558;&#23427;&#20204;&#38598;&#25104;&#21040;&#29983;&#20135;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#20043;&#21069;&#65292;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#36136;&#37327;&#20445;&#35777;&#39564;&#35777;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#38500;&#20102;&#35780;&#20272;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#30340;&#25928;&#29575;&#22806;&#65292;&#23545;&#25239;&#25915;&#20987;&#21487;&#33021;&#23545;AI&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26500;&#25104;&#23041;&#32961;&#12290;&#21516;&#26102;&#65292;&#21487;&#35299;&#37322;&#24615;AI&#65288;XAI&#65289;&#24212;&#29992;&#36817;&#20284;&#36755;&#20837;&#21040;&#36755;&#20986;&#30340;&#31639;&#27861;&#26469;&#35782;&#21035;&#36129;&#29486;&#29305;&#24449;&#12290;&#23545;&#25239;&#25200;&#21160;&#21487;&#33021;&#20250;&#38477;&#20302;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;XAI&#35299;&#37322;&#30340;&#25928;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#27969;&#31243;&#65292;&#29992;&#20110;&#19979;&#28216;&#35780;&#20272;&#20219;&#21153;&#65292;&#21253;&#25324;&#39564;&#35777;AI&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#20351;&#29992;&#22522;&#20934;&#25200;&#21160;&#35780;&#20272;&#40065;&#26834;&#24615;&#65292;&#27604;&#36739;&#35299;&#37322;&#25928;&#29992;&#20197;&#21450;&#35780;&#20272;&#24320;&#38144;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#28041;&#21450;&#20845;&#20010;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#35780;&#20272;&#22330;&#26223;&#65292;&#20854;&#20013;&#21253;&#25324;&#22522;&#20110;CNN&#21644;Transformer&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
As AI models rapidly evolve, they are frequently released to open repositories, such as HuggingFace. It is essential to perform quality assurance validation on these models before integrating them into the production development lifecycle. In addition to evaluating efficiency in terms of balanced accuracy and computing costs, adversarial attacks are potential threats to the robustness and explainability of AI models. Meanwhile, XAI applies algorithms that approximate inputs to outputs post-hoc to identify the contributing features. Adversarial perturbations may also degrade the utility of XAI explanations that require further investigation. In this paper, we present an integrated process designed for downstream evaluation tasks, including validating AI model accuracy, evaluating robustness with benchmark perturbations, comparing explanation utility, and assessing overhead. We demonstrate an evaluation scenario involving six computer vision models, which include CNN-based, Transformer-b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;&#33539;&#24335;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#19978;&#19979;&#25991;&#26469;&#25805;&#25511;&#27169;&#22411;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#39033;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;ICLAttack&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#26679;&#26412;&#21644;&#25552;&#31034;&#26469;&#20351;&#27169;&#22411;&#25353;&#29031;&#39044;&#23450;&#20041;&#30340;&#24847;&#22270;&#34892;&#20107;&#12290;</title><link>http://arxiv.org/abs/2401.05949</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36890;&#29992;&#28431;&#27934;&#65306;&#19978;&#19979;&#25991;&#23398;&#20064;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Universal Vulnerabilities in Large Language Models: In-context Learning Backdoor Attacks. (arXiv:2401.05949v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;&#33539;&#24335;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#19978;&#19979;&#25991;&#26469;&#25805;&#25511;&#27169;&#22411;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#39033;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;ICLAttack&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#26679;&#26412;&#21644;&#25552;&#31034;&#26469;&#20351;&#27169;&#22411;&#25353;&#29031;&#39044;&#23450;&#20041;&#30340;&#24847;&#22270;&#34892;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#19968;&#31181;&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20043;&#38388;&#24357;&#21512;&#24046;&#36317;&#30340;&#33539;&#24335;&#65292;&#22312;&#20960;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#39640;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#12290;&#19982;&#20256;&#32479;&#30340;&#24494;&#35843;&#26041;&#27861;&#19981;&#21516;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#22815;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#32780;&#26080;&#38656;&#26356;&#26032;&#20219;&#20309;&#21442;&#25968;&#12290;&#23613;&#31649;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25915;&#20987;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#36825;&#19968;&#33539;&#24335;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#30340;&#20851;&#20999;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#19978;&#19979;&#25991;&#26469;&#25805;&#25511;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;ICLAttack&#65292;&#38024;&#23545;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#65306;&#27745;&#26579;&#31034;&#33539;&#26679;&#26412;&#21644;&#27745;&#26579;&#25552;&#31034;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#25353;&#29031;&#39044;&#23450;&#20041;&#30340;&#24847;&#22270;&#34892;&#20107;&#12290;ICLAttack&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning, a paradigm bridging the gap between pre-training and fine-tuning, has demonstrated high efficacy in several NLP tasks, especially in few-shot settings. Unlike traditional fine-tuning methods, in-context learning adapts pre-trained models to unseen tasks without updating any parameters. Despite being widely applied, in-context learning is vulnerable to malicious attacks. In this work, we raise security concerns regarding this paradigm. Our studies demonstrate that an attacker can manipulate the behavior of large language models by poisoning the demonstration context, without the need for fine-tuning the model. Specifically, we have designed a new backdoor attack method, named ICLAttack, to target large language models based on in-context learning. Our method encompasses two types of attacks: poisoning demonstration examples and poisoning prompts, which can make models behave in accordance with predefined intentions. ICLAttack does not require additional fine-tuning 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#38381;&#24335;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#33258;&#21160;&#21270;&#31185;&#23398;&#21457;&#29616;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#23884;&#20837;&#21040;&#19968;&#32452;&#22522;&#20110;&#30456;&#21516;&#37327;&#30340;&#31561;&#20215;&#31867;&#20013;&#65292;&#24182;&#36890;&#36807;&#25214;&#21040;&#35813;&#31561;&#20215;&#31867;&#19982;&#31526;&#21495;&#22238;&#24402;&#25628;&#32034;&#31354;&#38388;&#20013;&#30340;&#26041;&#31243;&#30340;&#20132;&#38598;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2401.04978</link><description>&lt;p&gt;
&#29992;&#31526;&#21495;&#22238;&#24402;&#26799;&#24230;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#38381;&#24335;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Closed-Form Interpretation of Neural Network Classifiers with Symbolic Regression Gradients. (arXiv:2401.04978v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#38381;&#24335;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#33258;&#21160;&#21270;&#31185;&#23398;&#21457;&#29616;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#23884;&#20837;&#21040;&#19968;&#32452;&#22522;&#20110;&#30456;&#21516;&#37327;&#30340;&#31561;&#20215;&#31867;&#20013;&#65292;&#24182;&#36890;&#36807;&#25214;&#21040;&#35813;&#31561;&#20215;&#31867;&#19982;&#31526;&#21495;&#22238;&#24402;&#25628;&#32034;&#31354;&#38388;&#20013;&#30340;&#26041;&#31243;&#30340;&#20132;&#38598;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#65292;&#20197;&#23454;&#29616;&#33258;&#21160;&#31185;&#23398;&#21457;&#29616;&#12290;&#19982;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22238;&#24402;&#19981;&#21516;&#65292;&#23545;&#20110;&#20998;&#31867;&#32780;&#35328;&#65292;&#21363;&#20351;&#31070;&#32463;&#32593;&#32476;&#26412;&#36523;&#30340;&#20998;&#31867;&#22522;&#20110;&#21487;&#20197;&#34920;&#31034;&#20026;&#38381;&#24335;&#26041;&#31243;&#30340;&#37327;&#65292;&#20063;&#19968;&#33324;&#26080;&#27861;&#25214;&#21040;&#20174;&#31070;&#32463;&#32593;&#32476;&#21040;&#31526;&#21495;&#26041;&#31243;&#30340;&#19968;&#23545;&#19968;&#26144;&#23556;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#23558;&#35757;&#32451;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#23884;&#20837;&#21040;&#19968;&#20010;&#31561;&#20215;&#31867;&#20013;&#65292;&#36825;&#20010;&#31561;&#20215;&#31867;&#30340;&#20998;&#31867;&#20989;&#25968;&#30340;&#20915;&#31574;&#37117;&#22522;&#20110;&#30456;&#21516;&#30340;&#37327;&#12290;&#25105;&#36890;&#36807;&#25214;&#21040;&#36825;&#20010;&#31561;&#20215;&#31867;&#19982;&#30001;&#31526;&#21495;&#22238;&#24402;&#25628;&#32034;&#31354;&#38388;&#23450;&#20041;&#30340;&#21487;&#35835;&#30340;&#26041;&#31243;&#30340;&#20132;&#38598;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38480;&#20110;&#20998;&#31867;&#22120;&#25110;&#23436;&#25972;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#38544;&#34255;&#23618;&#25110;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#20219;&#24847;&#31070;&#32463;&#20803;&#65292;&#25110;&#31616;&#21270;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#22238;&#24402;&#22120;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
I introduce a unified framework for interpreting neural network classifiers tailored toward automated scientific discovery. In contrast to neural network-based regression, for classification, it is in general impossible to find a one-to-one mapping from the neural network to a symbolic equation even if the neural network itself bases its classification on a quantity that can be written as a closed-form equation. In this paper, I embed a trained neural network into an equivalence class of classifying functions that base their decisions on the same quantity. I interpret neural networks by finding an intersection between this equivalence class and human-readable equations defined by the search space of symbolic regression. The approach is not limited to classifiers or full neural networks and can be applied to arbitrary neurons in hidden layers or latent spaces or to simplify the process of interpreting neural network regressors.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#23398;&#20064;&#31995;&#25968;&#30340;&#37327;&#65292;&#29992;&#20110;&#31934;&#30830;&#37327;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36864;&#21270;&#31243;&#24230;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#21442;&#25968;&#21306;&#22495;&#30340;&#36864;&#21270;&#39034;&#24207;&#65292;&#24182;&#25581;&#31034;&#20102;&#38543;&#26426;&#20248;&#21270;&#22120;&#23545;&#20020;&#30028;&#28857;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2308.12108</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#31995;&#25968;&#37327;&#21270;&#22855;&#24322;&#27169;&#22411;&#20013;&#30340;&#36864;&#21270;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Quantifying degeneracy in singular models via the learning coefficient. (arXiv:2308.12108v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12108
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#23398;&#20064;&#31995;&#25968;&#30340;&#37327;&#65292;&#29992;&#20110;&#31934;&#30830;&#37327;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36864;&#21270;&#31243;&#24230;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#21442;&#25968;&#21306;&#22495;&#30340;&#36864;&#21270;&#39034;&#24207;&#65292;&#24182;&#25581;&#31034;&#20102;&#38543;&#26426;&#20248;&#21270;&#22120;&#23545;&#20020;&#30028;&#28857;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#26159;&#20855;&#26377;&#22797;&#26434;&#36864;&#21270;&#30340;&#22855;&#24322;&#32479;&#35745;&#27169;&#22411;&#12290;&#26412;&#25991;&#38416;&#36848;&#20102;&#19968;&#31181;&#31216;&#20026;&#23398;&#20064;&#31995;&#25968;&#30340;&#37327;&#65292;&#23427;&#22312;&#22855;&#24322;&#23398;&#20064;&#29702;&#35770;&#20013;&#31934;&#30830;&#22320;&#37327;&#21270;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36864;&#21270;&#31243;&#24230;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#23558;&#35777;&#26126;DNN&#20013;&#30340;&#36864;&#21270;&#19981;&#33021;&#20165;&#36890;&#36807;&#35745;&#31639;&#8220;&#24179;&#22374;&#8221;&#26041;&#21521;&#30340;&#25968;&#37327;&#26469;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230; Langevin &#21160;&#21147;&#23398;&#30340;&#23616;&#37096;&#23398;&#20064;&#31995;&#25968;&#30340;&#35745;&#31639;&#21487;&#25193;&#23637;&#36817;&#20284;&#26041;&#27861;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#24050;&#30693;&#29702;&#35770;&#20540;&#30340;&#20302;&#32500;&#27169;&#22411;&#19978;&#28436;&#31034;&#20102;&#20854;&#20934;&#30830;&#24615;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#23616;&#37096;&#23398;&#20064;&#31995;&#25968;&#33021;&#22815;&#27491;&#30830;&#24674;&#22797;&#24863;&#20852;&#36259;&#21442;&#25968;&#21306;&#22495;&#20043;&#38388;&#36864;&#21270;&#30340;&#39034;&#24207;&#12290;&#23545;MNIST&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#23616;&#37096;&#23398;&#20064;&#31995;&#25968;&#21487;&#20197;&#25581;&#31034;&#38543;&#26426;&#20248;&#21270;&#22120;&#23545;&#26356;&#36864;&#21270;&#25110;&#19981;&#22826;&#36864;&#21270;&#30340;&#20020;&#30028;&#28857;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNN) are singular statistical models which exhibit complex degeneracies. In this work, we illustrate how a quantity known as the \emph{learning coefficient} introduced in singular learning theory quantifies precisely the degree of degeneracy in deep neural networks. Importantly, we will demonstrate that degeneracy in DNN cannot be accounted for by simply counting the number of "flat" directions. We propose a computationally scalable approximation of a localized version of the learning coefficient using stochastic gradient Langevin dynamics. To validate our approach, we demonstrate its accuracy in low-dimensional models with known theoretical values. Importantly, the local learning coefficient can correctly recover the ordering of degeneracy between various parameter regions of interest. An experiment on MNIST shows the local learning coefficient can reveal the inductive bias of stochastic opitmizers for more or less degenerate critical points.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#33021;&#22815;&#26816;&#27979;&#21040;&#39640;&#23494;&#24230;&#21306;&#22495;&#20013;&#30340;&#32467;&#26500;&#65292;&#20855;&#26377;&#20808;&#21069;&#31639;&#27861;&#25152;&#19981;&#20855;&#22791;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.00677</link><description>&lt;p&gt;
SDC-HSDD-NDSA: &#20351;&#29992;&#23618;&#27425;&#27425;&#32423;&#23548;&#21521;&#24046;&#24322;&#21644;&#24402;&#19968;&#21270;&#23494;&#24230;&#33258;&#36866;&#24212;&#30340;&#32467;&#26500;&#26816;&#27979;&#32858;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
SDC-HSDD-NDSA: Structure Detecting Cluster by Hierarchical Secondary Directed Differential with Normalized Density and Self-Adaption. (arXiv:2307.00677v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#33021;&#22815;&#26816;&#27979;&#21040;&#39640;&#23494;&#24230;&#21306;&#22495;&#20013;&#30340;&#32467;&#26500;&#65292;&#20855;&#26377;&#20808;&#21069;&#31639;&#27861;&#25152;&#19981;&#20855;&#22791;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#32858;&#31867;&#31639;&#27861;&#20043;&#19968;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#35782;&#21035;&#20219;&#24847;&#24418;&#29366;&#30340;&#32858;&#31867;&#65292;&#21482;&#35201;&#19981;&#21516;&#30340;&#39640;&#23494;&#24230;&#32858;&#31867;&#20043;&#38388;&#26377;&#20302;&#23494;&#24230;&#21306;&#22495;&#20998;&#38548;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#20302;&#23494;&#24230;&#21306;&#22495;&#23558;&#32858;&#31867;&#20998;&#38548;&#24320;&#30340;&#35201;&#27714;&#24182;&#19981;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#65292;&#22240;&#20026;&#39640;&#23494;&#24230;&#21306;&#22495;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#32467;&#26500;&#65292;&#24212;&#35813;&#34987;&#32858;&#31867;&#21040;&#19981;&#21516;&#30340;&#32452;&#20013;&#12290;&#36825;&#31181;&#24773;&#20917;&#35828;&#26126;&#20102;&#25105;&#20204;&#24050;&#30693;&#30340;&#25152;&#26377;&#20808;&#21069;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#30340;&#20027;&#35201;&#32570;&#38519;--&#26080;&#27861;&#26816;&#27979;&#39640;&#23494;&#24230;&#32858;&#31867;&#20013;&#30340;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#26041;&#26696;&#65292;&#26082;&#20855;&#26377;&#20808;&#21069;&#26041;&#27861;&#30340;&#33021;&#21147;&#65292;&#21448;&#33021;&#22815;&#26816;&#27979;&#21040;&#39640;&#23494;&#24230;&#21306;&#22495;&#20013;&#26410;&#34987;&#20302;&#23494;&#24230;&#21306;&#20998;&#24320;&#30340;&#32467;&#26500;&#12290;&#35813;&#31639;&#27861;&#37319;&#29992;&#23618;&#27425;&#27425;&#32423;&#23548;&#21521;&#24046;&#24322;&#12289;&#23618;&#27425;&#21270;&#12289;&#24402;&#19968;&#21270;&#23494;&#24230;&#20197;&#21450;&#33258;&#36866;&#24212;&#31995;&#25968;&#65292;&#22240;&#27492;&#34987;&#31216;&#20026;&#32467;&#26500;&#26816;&#27979;&#32858;&#31867;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Density-based clustering could be the most popular clustering algorithm since it can identify clusters of arbitrary shape as long as different (high-density) clusters are separated by low-density regions. However, the requirement of the separateness of clusters by low-density regions is not trivial since a high-density region might have different structures which should be clustered into different groups. Such a situation demonstrates the main flaw of all previous density-based clustering algorithms we have known--structures in a high-density cluster could not be detected. Therefore, this paper aims to provide a density-based clustering scheme that not only has the ability previous ones have but could also detect structures in a high-density region not separated by low-density ones. The algorithm employs secondary directed differential, hierarchy, normalized density, as well as the self-adaption coefficient, and thus is called Structure Detecting Cluster by Hierarchical Secondary Direc
&lt;/p&gt;</description></item></channel></rss>