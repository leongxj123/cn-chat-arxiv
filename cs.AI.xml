<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KTO&#30340;&#26041;&#27861;&#65292;&#23558;&#27169;&#22411;&#23545;&#40784;&#35270;&#20026;&#23637;&#26395;&#29702;&#35770;&#20248;&#21270;&#12290;&#19982;&#24403;&#21069;&#26041;&#27861;&#30456;&#27604;&#65292;KTO&#30452;&#25509;&#26368;&#22823;&#21270;&#29983;&#25104;&#25928;&#29992;&#32780;&#19981;&#26159;&#26368;&#22823;&#21270;&#20559;&#22909;&#23545;&#25968;&#20284;&#28982;&#12290;&#22312;&#22810;&#20010;&#35268;&#27169;&#19978;&#65292;KTO&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#20559;&#22909;&#30340;&#26041;&#27861;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01306</link><description>&lt;p&gt;
KTO: &#27169;&#22411;&#23545;&#40784;&#35270;&#20026;&#23637;&#26395;&#29702;&#35770;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
KTO: Model Alignment as Prospect Theoretic Optimization
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KTO&#30340;&#26041;&#27861;&#65292;&#23558;&#27169;&#22411;&#23545;&#40784;&#35270;&#20026;&#23637;&#26395;&#29702;&#35770;&#20248;&#21270;&#12290;&#19982;&#24403;&#21069;&#26041;&#27861;&#30456;&#27604;&#65292;KTO&#30452;&#25509;&#26368;&#22823;&#21270;&#29983;&#25104;&#25928;&#29992;&#32780;&#19981;&#26159;&#26368;&#22823;&#21270;&#20559;&#22909;&#23545;&#25968;&#20284;&#28982;&#12290;&#22312;&#22810;&#20010;&#35268;&#27169;&#19978;&#65292;KTO&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#20559;&#22909;&#30340;&#26041;&#27861;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20975;&#24681;&#26364;&#19982;&#29305;&#27779;&#26031;&#22522;&#30340;&#23637;&#26395;&#29702;&#35770;&#21578;&#35785;&#25105;&#20204;&#65292;&#20154;&#31867;&#20197;&#26377;&#20559;&#35265;&#20294;&#26126;&#30830;&#30340;&#26041;&#24335;&#30475;&#24453;&#38543;&#26426;&#21464;&#37327;&#65307;&#20363;&#22914;&#65292;&#20154;&#20204;&#36890;&#24120;&#37117;&#26159;&#21388;&#24694;&#25439;&#22833;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23558;LLMs&#19982;&#20154;&#24037;&#21453;&#39304;&#36827;&#34892;&#23545;&#40784;&#30340;&#30446;&#26631;&#38544;&#21547;&#22320;&#34701;&#21512;&#20102;&#35768;&#22810;&#36825;&#20123;&#20559;&#35265; - &#36825;&#20123;&#30446;&#26631; (&#20363;&#22914; DPO) &#30340;&#25104;&#21151;&#37096;&#20998;&#21487;&#24402;&#22240;&#20110;&#23427;&#20204;&#26159;"&#20154;&#31867;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;"(HALOs)&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#25152;&#24402;&#22240;&#32473;&#20154;&#31867;&#30340;&#25928;&#29992;&#20989;&#25968;&#20173;&#19982;&#23637;&#26395;&#29702;&#35770;&#25991;&#29486;&#20013;&#30340;&#19981;&#21516;&#12290;&#21033;&#29992;&#20975;&#24681;&#26364;-&#29305;&#27779;&#26031;&#22522;&#20154;&#31867;&#25928;&#29992;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#26368;&#22823;&#21270;&#29983;&#25104;&#25928;&#29992;&#32780;&#19981;&#26159;&#26368;&#22823;&#21270;&#20559;&#22909;&#23545;&#25968;&#20284;&#28982;&#30340;HALO&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;&#20975;&#24681;&#26364;-&#29305;&#27779;&#26031;&#22522;&#20248;&#21270;(KTO)&#65292;&#24182;&#19988;&#23427;&#22312;&#20174;1B&#21040;30B&#30340;&#35268;&#27169;&#19978;&#19982;&#22522;&#20110;&#20559;&#22909;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#25110;&#36229;&#36807;&#12290;&#20851;&#38190;&#26159;&#65292;KTO&#19981;&#38656;&#35201;&#20559;&#22909; - &#21482;&#38656;&#35201;&#19968;&#20010;&#26159;&#21542;&#30340;&#20108;&#36827;&#21046;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kahneman &amp; Tversky's $\textit{prospect theory}$ tells us that humans perceive random variables in a biased but well-defined manner; for example, humans are famously loss-averse. We show that objectives for aligning LLMs with human feedback implicitly incorporate many of these biases -- the success of these objectives (e.g., DPO) over cross-entropy minimization can partly be ascribed to them being $\textit{human-aware loss functions}$ (HALOs). However, the utility functions these methods attribute to humans still differ from those in the prospect theory literature. Using a Kahneman-Tversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do. We call this approach Kahneman-Tversky Optimization (KTO), and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B. Crucially, KTO does not need preferences -- only a binary signal of whether 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#23398;&#20064;&#22312;&#35299;&#20915;&#26053;&#34892;&#21830;&#38382;&#39064;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#26356;&#22823;&#30340;&#23454;&#20363;&#22823;&#23567;&#36827;&#34892;&#35757;&#32451;&#24182;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21487;&#20197;&#26500;&#24314;&#26356;&#26377;&#25928;&#30340;&#34920;&#31034;&#65292;&#22686;&#24378;&#27169;&#22411;&#35299;&#20915;TSP&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.20212</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#26053;&#34892;&#21830;&#38382;&#39064;&#20013;&#23610;&#23544;&#21644;&#38590;&#24230;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
On Size and Hardness Generalization in Unsupervised Learning for the Travelling Salesman Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20212
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#23398;&#20064;&#22312;&#35299;&#20915;&#26053;&#34892;&#21830;&#38382;&#39064;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#26356;&#22823;&#30340;&#23454;&#20363;&#22823;&#23567;&#36827;&#34892;&#35757;&#32451;&#24182;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21487;&#20197;&#26500;&#24314;&#26356;&#26377;&#25928;&#30340;&#34920;&#31034;&#65292;&#22686;&#24378;&#27169;&#22411;&#35299;&#20915;TSP&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#23398;&#20064;&#22312;&#35299;&#20915;&#26053;&#34892;&#21830;&#38382;&#39064;&#65288;TSP&#65289;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#29992;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26469;&#20026;&#27599;&#20010;&#33410;&#28857;&#29983;&#25104;&#23884;&#20837;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#23884;&#20837;&#26469;&#26500;&#24314;&#19968;&#20010;&#28909;&#22270;&#65292;&#25351;&#31034;&#27599;&#26465;&#36793;&#25104;&#20026;&#26368;&#20339;&#36335;&#24452;&#30340;&#21487;&#33021;&#24615;&#12290;&#28982;&#21518;&#25105;&#20204;&#24212;&#29992;&#23616;&#37096;&#25628;&#32034;&#29983;&#25104;&#26368;&#32456;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#19981;&#21516;&#35757;&#32451;&#23454;&#20363;&#22823;&#23567;&#12289;&#23884;&#20837;&#32500;&#25968;&#21644;&#20998;&#24067;&#22914;&#20309;&#24433;&#21709;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#26356;&#22823;&#30340;&#23454;&#20363;&#22823;&#23567;&#36827;&#34892;&#35757;&#32451;&#24182;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21487;&#20197;&#26500;&#24314;&#26356;&#26377;&#25928;&#30340;&#34920;&#31034;&#65292;&#22686;&#24378;&#27169;&#22411;&#35299;&#20915;TSP&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#22312;&#35780;&#20272;&#19981;&#21516;&#20998;&#24067;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#26102;&#65292;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#21508;&#31181;&#20998;&#24067;&#30340;&#38590;&#24230;&#65292;&#24182;&#25506;&#35752;&#20102;&#19981;&#21516;&#38590;&#24230;&#22914;&#20309;&#24433;&#21709;&#26368;&#32456;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20212v1 Announce Type: new  Abstract: We study the generalization capability of Unsupervised Learning in solving the Travelling Salesman Problem (TSP). We use a Graph Neural Network (GNN) trained with a surrogate loss function to generate an embedding for each node. We use these embeddings to construct a heat map that indicates the likelihood of each edge being part of the optimal route. We then apply local search to generate our final predictions. Our investigation explores how different training instance sizes, embedding dimensions, and distributions influence the outcomes of Unsupervised Learning methods. Our results show that training with larger instance sizes and increasing embedding dimensions can build a more effective representation, enhancing the model's ability to solve TSP. Furthermore, in evaluating generalization across different distributions, we first determine the hardness of various distributions and explore how different hardnesses affect the final results
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25506;&#35752;&#20449;&#24687;&#38388;&#31454;&#20105;&#65292;&#25552;&#20986;&#21033;&#29992;&#20449;&#24687;&#24066;&#22330;&#26412;&#36523;&#20316;&#20026;&#26377;&#25928;&#20943;&#36731;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#36755;&#20986;&#39118;&#38505;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#25351;&#20986;&#30417;&#31649;&#32773;&#22312;&#38754;&#23545;&#26032;&#25216;&#26415;&#19981;&#30830;&#23450;&#24615;&#26102;&#24448;&#24448;&#36807;&#24230;&#35880;&#24910;&#65292;&#38656;&#37325;&#26032;&#35780;&#20272;&#30417;&#31649;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.11046</link><description>&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#38388;&#31454;&#20105;&#35843;&#25511;&#32842;&#22825;&#26426;&#22120;&#20154;&#36755;&#20986;
&lt;/p&gt;
&lt;p&gt;
Regulating Chatbot Output via Inter-Informational Competition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25506;&#35752;&#20449;&#24687;&#38388;&#31454;&#20105;&#65292;&#25552;&#20986;&#21033;&#29992;&#20449;&#24687;&#24066;&#22330;&#26412;&#36523;&#20316;&#20026;&#26377;&#25928;&#20943;&#36731;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#36755;&#20986;&#39118;&#38505;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#25351;&#20986;&#30417;&#31649;&#32773;&#22312;&#38754;&#23545;&#26032;&#25216;&#26415;&#19981;&#30830;&#23450;&#24615;&#26102;&#24448;&#24448;&#36807;&#24230;&#35880;&#24910;&#65292;&#38656;&#37325;&#26032;&#35780;&#20272;&#30417;&#31649;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30340;&#20986;&#29616;&#24341;&#21457;&#20102;&#19968;&#24180;&#22810;&#30340;&#30417;&#31649;&#29378;&#28526;&#12290;&#28982;&#32780;&#65292;&#23569;&#25968;&#29616;&#26377;&#30740;&#31350;&#20005;&#26684;&#36136;&#30097;&#20102;&#36825;&#26679;&#19968;&#20010;&#20551;&#35774;&#65306;&#22914;&#26524;&#19981;&#32463;&#35268;&#33539;&#65292;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#36755;&#20986;&#20250;&#23545;&#20154;&#31867;&#20107;&#21153;&#36896;&#25104;&#23454;&#36136;&#19988;&#20005;&#37325;&#30340;&#20260;&#23475;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#20154;&#21592;&#24573;&#35270;&#20102;&#20449;&#24687;&#24066;&#22330;&#26412;&#36523;&#21487;&#20197;&#26377;&#25928;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#30340;&#20851;&#38190;&#21487;&#33021;&#24615;&#65292;&#24182;&#22240;&#27492;&#20542;&#21521;&#20110;&#20351;&#29992;&#30417;&#31649;&#24037;&#20855;&#30452;&#25509;&#35299;&#20915;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#20851;&#27880;&#21508;&#31181;&#28192;&#36947;&#20043;&#38388;&#30340;&#20449;&#24687;&#31454;&#20105;&#65292;&#21457;&#23637;&#20102;&#19968;&#20010;&#37325;&#26032;&#35780;&#20272;AI&#30456;&#20851;&#20869;&#23481;&#39118;&#38505;&#21644;&#30456;&#24212;&#30417;&#31649;&#25552;&#35758;&#30340;&#26631;&#20934;&#12290;&#38271;&#36798;&#25968;&#21313;&#24180;&#30340;&#20449;&#24687;&#21644;&#36890;&#20449;&#25216;&#26415;&#30417;&#31649;&#21490;&#34920;&#26126;&#65292;&#30417;&#31649;&#32773;&#22312;&#38754;&#23545;&#26032;&#25216;&#26415;&#24102;&#26469;&#30340;&#19981;&#30830;&#23450;&#24615;&#26102;&#24448;&#24448;&#36807;&#20110;&#35880;&#24910;&#65292;&#24182;&#22312;&#25552;&#20986;&#36807;&#24230;&#30340;&#30417;&#31649;&#25514;&#26045;&#26102;&#29359;&#38169;&#35823;&#12290;&#20107;&#23454;&#19978;&#65292;&#20016;&#23500;&#30340;&#32463;&#39564;&#25968;&#25454;&#25903;&#25345;&#20102;&#20449;&#24687;&#24066;&#22330;&#26426;&#21046;&#22312;&#20449;&#24687;&#30417;&#31649;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11046v1 Announce Type: cross  Abstract: The advent of ChatGPT has sparked over a year of regulatory frenzy. However, few existing studies have rigorously questioned the assumption that, if left unregulated, AI chatbot's output would inflict tangible, severe real harm on human affairs. Most researchers have overlooked the critical possibility that the information market itself can effectively mitigate these risks and, as a result, they tend to use regulatory tools to address the issue directly. This Article develops a yardstick for reevaluating both AI-related content risks and corresponding regulatory proposals by focusing on inter-informational competition among various outlets. The decades-long history of regulating information and communications technologies indicates that regulators tend to err too much on the side of caution and to put forward excessive regulatory measures when encountering the uncertainties brought about by new technologies. In fact, a trove of empiric
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#22522;&#20110;&#20154;&#20307;&#27979;&#37327;&#30340;&#23478;&#20855;&#23610;&#23544;&#36866;&#21512;&#22823;&#23398;&#29983;&#65292;&#20197;&#25913;&#21892;&#35745;&#31639;&#26426;&#23454;&#39564;&#23460;&#20154;&#20307;&#24037;&#31243;&#23398;&#65292;&#30740;&#31350;&#21457;&#29616;&#20854;&#19982;&#29616;&#26377;&#23478;&#20855;&#23610;&#23544;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#24182;&#26356;&#21152;&#20860;&#23481;</title><link>https://arxiv.org/abs/2403.05589</link><description>&lt;p&gt;
&#20248;&#21270;&#22823;&#23398;&#35745;&#31639;&#26426;&#23454;&#39564;&#23460;&#20154;&#20307;&#24037;&#31243;&#23398;&#65306;&#19968;&#20010;&#20851;&#20110;&#20154;&#20307;&#27979;&#37327;&#12289;&#23478;&#20855;&#35774;&#35745;&#21644;ANOVA&#27979;&#35797;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Optimizing Computer Lab Ergonomics in Universities: A Study on Anthropometric Measurements, Furniture Design, and ANOVA Test
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05589
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#22522;&#20110;&#20154;&#20307;&#27979;&#37327;&#30340;&#23478;&#20855;&#23610;&#23544;&#36866;&#21512;&#22823;&#23398;&#29983;&#65292;&#20197;&#25913;&#21892;&#35745;&#31639;&#26426;&#23454;&#39564;&#23460;&#20154;&#20307;&#24037;&#31243;&#23398;&#65292;&#30740;&#31350;&#21457;&#29616;&#20854;&#19982;&#29616;&#26377;&#23478;&#20855;&#23610;&#23544;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#24182;&#26356;&#21152;&#20860;&#23481;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30740;&#31350;&#34920;&#26126;&#65292;&#20154;&#20307;&#24037;&#31243;&#23398;&#35774;&#35745;&#30340;&#23478;&#20855;&#33021;&#25552;&#39640;&#24037;&#20316;&#25928;&#29575;&#21644;&#36523;&#24515;&#20581;&#24247;&#12290;&#38543;&#30528;&#35745;&#31639;&#26426;&#25104;&#20026;&#23398;&#29983;&#23398;&#26415;&#29983;&#27963;&#30340;&#19968;&#37096;&#20998;&#65292;&#23427;&#20204;&#22312;&#26410;&#26469;&#23558;&#36827;&#19968;&#27493;&#26222;&#21450;&#12290;&#25105;&#20204;&#25552;&#20986;&#22522;&#20110;&#20154;&#20307;&#27979;&#37327;&#30340;&#23478;&#20855;&#23610;&#23544;&#65292;&#36866;&#21512;&#22823;&#23398;&#29983;&#20197;&#25913;&#21892;&#35745;&#31639;&#26426;&#23454;&#39564;&#23460;&#30340;&#20154;&#20307;&#24037;&#31243;&#23398;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;380&#21517;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#65292;&#20998;&#26512;&#20102;11&#39033;&#20154;&#20307;&#27979;&#37327;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;11&#39033;&#23478;&#20855;&#23610;&#23544;&#36827;&#34892;&#20102;&#30456;&#20851;&#24615;&#20998;&#26512;&#12290;&#30740;&#31350;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#23478;&#20855;&#65306;&#38750;&#21487;&#35843;&#26885;&#23376;&#19982;&#38750;&#21487;&#35843;&#26700;&#23376;&#65292;&#20197;&#21450;&#21487;&#35843;&#26885;&#23376;&#19982;&#38750;&#21487;&#35843;&#26700;&#23376;&#12290;&#19981;&#21305;&#37197;&#35745;&#31639;&#26174;&#31034;&#23478;&#20855;&#23610;&#23544;&#19982;&#20154;&#20307;&#27979;&#37327;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#26174;&#33879;&#27700;&#24179;&#20026;5%&#30340;&#21333;&#22240;&#32032;&#26041;&#24046;&#20998;&#26512;&#27979;&#35797;&#36824;&#26174;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#21644;&#29616;&#26377;&#30340;&#23478;&#20855;&#23610;&#23544;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#21457;&#29616;&#25152;&#25552;&#20986;&#30340;&#23610;&#23544;&#26356;&#21152;&#20860;&#23481;&#65292;&#20943;&#23569;&#20102;&#19981;&#21305;&#37197;&#30334;&#20998;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05589v1 Announce Type: cross  Abstract: Many studies have shown how ergonomically designed furniture improves productivity and well-being. As computers have become a part of students' academic lives, they will grow further in the future. We propose anthropometric-based furniture dimensions suitable for university students to improve computer laboratory ergonomics. We collected data from 380 participants and analyzed 11 anthropometric measurements, correlating them to 11 furniture dimensions. Two types of furniture were studied: a non-adjustable chair with a non-adjustable table and an adjustable chair with a non-adjustable table. The mismatch calculation showed a significant difference between furniture dimensions and anthropometric measurements. The one-way ANOVA test with a significance level of 5% also showed a significant difference between proposed and existing furniture dimensions. The proposed dimensions were found to be more compatible and reduced mismatch percentage
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#26368;&#26032;&#30340;&#22270;&#29255;&#32858;&#31867;&#21644;&#25193;&#25955;&#27169;&#22411;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32771;&#34385;&#26368;&#20339;&#32858;&#31867;&#31890;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20808;&#36827;FID&#24182;&#20855;&#26377;&#36739;&#24378;&#35757;&#32451;&#26679;&#26412;&#25928;&#29575;&#30340;&#32858;&#31867;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#26469;&#20943;&#23569;&#35270;&#35273;&#32452;&#25628;&#32034;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.00570</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22522;&#20110;&#32858;&#31867;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Rethinking cluster-conditioned diffusion models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00570
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#26368;&#26032;&#30340;&#22270;&#29255;&#32858;&#31867;&#21644;&#25193;&#25955;&#27169;&#22411;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32771;&#34385;&#26368;&#20339;&#32858;&#31867;&#31890;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20808;&#36827;FID&#24182;&#20855;&#26377;&#36739;&#24378;&#35757;&#32451;&#26679;&#26412;&#25928;&#29575;&#30340;&#32858;&#31867;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#26469;&#20943;&#23569;&#35270;&#35273;&#32452;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#38024;&#23545;&#20351;&#29992;&#32858;&#31867;&#20998;&#37197;&#30340;&#22270;&#29255;&#32423;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#30740;&#31350;&#12290;&#25105;&#20204;&#38416;&#26126;&#20102;&#20851;&#20110;&#22270;&#29255;&#32858;&#31867;&#30340;&#20010;&#21035;&#32452;&#20214;&#22914;&#20309;&#24433;&#21709;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#22270;&#29255;&#21512;&#25104;&#12290;&#36890;&#36807;&#32467;&#21512;&#22270;&#29255;&#32858;&#31867;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#22312;&#32771;&#34385;&#21040;&#22270;&#29255;&#21512;&#25104;&#65288;&#35270;&#35273;&#32452;&#65289;&#30340;&#26368;&#20339;&#31751;&#31890;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#32858;&#31867;&#26465;&#20214;&#21487;&#20197;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;FID&#65288;&#21363;&#22312;CIFAR10&#21644;CIFAR100&#19978;&#20998;&#21035;&#20026;1.67&#21644;2.17&#65289;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#36739;&#24378;&#30340;&#35757;&#32451;&#26679;&#26412;&#25928;&#29575;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#22522;&#20110;&#29305;&#24449;&#30340;&#32858;&#31867;&#26469;&#25512;&#23548;&#20943;&#23569;&#35270;&#35273;&#32452;&#25628;&#32034;&#31354;&#38388;&#30340;&#19978;&#38480;&#31751;&#36793;&#30028;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#21457;&#29616;&#32858;&#31867;&#19982;&#22522;&#20110;&#32858;&#31867;&#30340;&#22270;&#29255;&#29983;&#25104;&#20043;&#38388;&#27809;&#26377;&#26174;&#33879;&#32852;&#31995;&#12290;&#20195;&#30721;&#21644;&#32858;&#31867;&#20998;&#37197;&#23558;&#20250;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00570v1 Announce Type: cross  Abstract: We present a comprehensive experimental study on image-level conditioning for diffusion models using cluster assignments. We elucidate how individual components regarding image clustering impact image synthesis across three datasets. By combining recent advancements from image clustering and diffusion models, we show that, given the optimal cluster granularity with respect to image synthesis (visual groups), cluster-conditioning can achieve state-of-the-art FID (i.e. 1.67, 2.17 on CIFAR10 and CIFAR100 respectively), while attaining a strong training sample efficiency. Finally, we propose a novel method to derive an upper cluster bound that reduces the search space of the visual groups using solely feature-based clustering. Unlike existing approaches, we find no significant connection between clustering and cluster-conditional image generation. The code and cluster assignments will be released.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23450;&#20041;"&#20219;&#21153;"&#30340;&#26041;&#24335;&#21644;&#24341;&#20837;&#20855;&#26377;&#29289;&#29702;&#21644;&#22240;&#26524;&#20851;&#31995;&#29702;&#35299;&#30340;&#30417;&#30563;&#27169;&#22359;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#22266;&#26377;&#29289;&#29702;&#30693;&#35782;&#30340;&#31283;&#24577;&#36816;&#21160;&#35268;&#21010;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#26426;&#22120;&#20154;&#19978;&#23454;&#29616;&#22797;&#26434;&#35745;&#21010;&#12290;</title><link>https://arxiv.org/abs/2402.15384</link><description>&lt;p&gt;
&#20855;&#26377;&#22266;&#26377;&#29289;&#29702;&#30693;&#35782;&#30340;&#31283;&#24577;&#36816;&#21160;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Homeostatic motion planning with innate physics knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15384
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23450;&#20041;"&#20219;&#21153;"&#30340;&#26041;&#24335;&#21644;&#24341;&#20837;&#20855;&#26377;&#29289;&#29702;&#21644;&#22240;&#26524;&#20851;&#31995;&#29702;&#35299;&#30340;&#30417;&#30563;&#27169;&#22359;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#22266;&#26377;&#29289;&#29702;&#30693;&#35782;&#30340;&#31283;&#24577;&#36816;&#21160;&#35268;&#21010;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#26426;&#22120;&#20154;&#19978;&#23454;&#29616;&#22797;&#26434;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#20307;&#20197;&#38381;&#29615;&#26041;&#24335;&#19982;&#21608;&#22260;&#29615;&#22659;&#36827;&#34892;&#20114;&#21160;&#65292;&#20854;&#20013;&#24863;&#23448;&#36755;&#20837;&#20915;&#23450;&#34892;&#20026;&#30340;&#21551;&#21160;&#21644;&#32456;&#27490;&#12290;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;&#21160;&#29289;&#20063;&#33021;&#21046;&#23450;&#24182;&#25191;&#34892;&#22797;&#26434;&#35745;&#21010;&#65292;&#20294;&#32431;&#38381;&#29615;&#36755;&#20837;&#25511;&#21046;&#30340;&#26426;&#22120;&#20154;&#23578;&#26410;&#22797;&#21046;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#23450;&#20041;&#19968;&#32452;&#31163;&#25955;&#20020;&#26102;&#38381;&#29615;&#25511;&#21046;&#22120;&#65292;&#31216;&#20026;&#8220;&#20219;&#21153;&#8221;&#65292;&#27599;&#20010;&#20219;&#21153;&#20195;&#34920;&#19968;&#20010;&#38381;&#29615;&#34892;&#20026;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#20010;&#20855;&#26377;&#22266;&#26377;&#29289;&#29702;&#21644;&#22240;&#26524;&#20851;&#31995;&#29702;&#35299;&#30340;&#30417;&#30563;&#27169;&#22359;&#65292;&#36890;&#36807;&#35813;&#27169;&#22359;&#21487;&#20197;&#27169;&#25311;&#38543;&#26102;&#38388;&#25191;&#34892;&#20219;&#21153;&#24207;&#21015;&#24182;&#23558;&#32467;&#26524;&#23384;&#20648;&#22312;&#29615;&#22659;&#27169;&#22411;&#20013;&#12290;&#22522;&#20110;&#36825;&#20010;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#38142;&#25509;&#20020;&#26102;&#38381;&#29615;&#25511;&#21046;&#22120;&#36827;&#34892;&#21046;&#23450;&#35745;&#21010;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#24050;&#22312;&#23454;&#38469;&#26426;&#22120;&#20154;&#20013;&#23454;&#26045;&#65292;&#24182;&#22312;&#20004;&#31181;&#22330;&#26223;&#19979;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15384v1 Announce Type: cross  Abstract: Living organisms interact with their surroundings in a closed-loop fashion, where sensory inputs dictate the initiation and termination of behaviours. Even simple animals are able to develop and execute complex plans, which has not yet been replicated in robotics using pure closed-loop input control. We propose a solution to this problem by defining a set of discrete and temporary closed-loop controllers, called "tasks", each representing a closed-loop behaviour. We further introduce a supervisory module which has an innate understanding of physics and causality, through which it can simulate the execution of task sequences over time and store the results in a model of the environment. On the basis of this model, plans can be made by chaining temporary closed-loop controllers. The proposed framework was implemented for a real robot and tested in two scenarios as proof of concept.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#31867;&#22411;&#30340;&#38146;&#31163;&#23376;&#30005;&#27744;&#65292;&#23454;&#29616;&#20934;&#30830;&#30340;&#20581;&#24247;&#29366;&#24577;&#20272;&#35745;&#12290;&#36825;&#39033;&#24037;&#20316;&#35299;&#20915;&#20102;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#25104;&#26412;&#39640;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.00068</link><description>&lt;p&gt;
GPT4Battery: &#19968;&#31181;&#22522;&#20110;LLM&#39537;&#21160;&#30340;&#33258;&#36866;&#24212;&#38146;&#31163;&#23376;&#30005;&#27744;&#20581;&#24247;&#29366;&#24577;&#20272;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
GPT4Battery: An LLM-driven Framework for Adaptive State of Health Estimation of Raw Li-ion Batteries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#31867;&#22411;&#30340;&#38146;&#31163;&#23376;&#30005;&#27744;&#65292;&#23454;&#29616;&#20934;&#30830;&#30340;&#20581;&#24247;&#29366;&#24577;&#20272;&#35745;&#12290;&#36825;&#39033;&#24037;&#20316;&#35299;&#20915;&#20102;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#25104;&#26412;&#39640;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20581;&#24247;&#29366;&#24577;&#65288;SOH&#65289;&#26159;&#35780;&#20272;&#30005;&#27744;&#36864;&#21270;&#27700;&#24179;&#30340;&#20851;&#38190;&#25351;&#26631;&#65292;&#26080;&#27861;&#30452;&#25509;&#27979;&#37327;&#20294;&#38656;&#35201;&#20272;&#35745;&#12290;&#20934;&#30830;&#30340;SOH&#20272;&#35745;&#25552;&#21319;&#20102;&#38146;&#31163;&#23376;&#30005;&#27744;&#30340;&#26816;&#27979;&#12289;&#25511;&#21046;&#21644;&#21453;&#39304;&#33021;&#21147;&#65292;&#23454;&#29616;&#23433;&#20840;&#39640;&#25928;&#30340;&#33021;&#28304;&#31649;&#29702;&#65292;&#24182;&#25351;&#23548;&#26032;&#19968;&#20195;&#30005;&#27744;&#30340;&#21457;&#23637;&#12290;&#23613;&#31649;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;SOH&#20272;&#35745;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#20026;&#29983;&#25104;&#23551;&#21629;&#38271;&#26399;&#35757;&#32451;&#25968;&#25454;&#32780;&#36827;&#34892;&#30340;&#32791;&#26102;&#19988;&#36164;&#28304;&#23494;&#38598;&#30340;&#36864;&#21270;&#23454;&#39564;&#22312;&#24314;&#31435;&#19968;&#20010;&#33021;&#22788;&#29702;&#22810;&#26679;&#21270;&#38146;&#31163;&#23376;&#30005;&#27744;&#65288;&#20363;&#22914;&#65292;&#36328;&#21270;&#23398;&#12289;&#36328;&#21046;&#36896;&#21830;&#21644;&#36328;&#23481;&#37327;&#65289;&#30340;&#22823;&#22411;&#27169;&#22411;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19981;&#21516;&#30005;&#27744;&#30340;&#21487;&#35843;&#25972;SOH&#20272;&#35745;&#30340;&#26032;&#22411;&#26694;&#26550;&#12290;&#20026;&#20102;&#36866;&#24212;&#23454;&#38469;&#24773;&#26223;&#65292;&#20854;&#20013;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#25353;&#39034;&#24207;&#20197;&#21450;&#20998;&#24067;&#21464;&#21270;&#30340;&#26041;&#24335;&#21040;&#36798;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#27979;&#35797;&#26102;&#36827;&#34892;&#20102;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
State of health (SOH) is a crucial indicator for assessing the degradation level of batteries that cannot be measured directly but requires estimation. Accurate SOH estimation enhances detection, control, and feedback for Li-ion batteries, allowing for safe and efficient energy management and guiding the development of new-generation batteries. Despite the significant progress in data-driven SOH estimation, the time and resource-consuming degradation experiments for generating lifelong training data pose a challenge in establishing one large model capable of handling diverse types of Li-ion batteries, e.g., cross-chemistry, cross-manufacturer, and cross-capacity. Hence, this paper utilizes the strong generalization capability of large language model (LLM) to proposes a novel framework for adaptable SOH estimation across diverse batteries. To match the real scenario where unlabeled data sequentially arrives in use with distribution shifts, the proposed model is modified by a test-time t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#22312;&#33041;&#21551;&#21457;&#35745;&#31639;&#30340;&#20154;&#26426;&#20132;&#20114;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#65292;&#20171;&#32461;&#20102;&#20854;&#28436;&#21270;&#12289;&#25361;&#25112;&#21644;&#28508;&#22312;&#30740;&#31350;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2312.07213</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#20154;&#26426;&#20132;&#20114;&#30740;&#31350;&#65306;&#23545;&#33041;&#21551;&#21457;&#35745;&#31639;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Human-computer Interaction for Brain-inspired Computing Based on Machine Learning And Deep Learning:A Review. (arXiv:2312.07213v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.07213
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#22312;&#33041;&#21551;&#21457;&#35745;&#31639;&#30340;&#20154;&#26426;&#20132;&#20114;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#65292;&#20171;&#32461;&#20102;&#20854;&#28436;&#21270;&#12289;&#25361;&#25112;&#21644;&#28508;&#22312;&#30740;&#31350;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#25345;&#32493;&#21457;&#23637;&#23545;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#21644;&#20854;&#20182;&#39046;&#22495;&#20135;&#29983;&#20102;&#28145;&#36828;&#24433;&#21709;&#12290;&#33041;&#21551;&#21457;&#35745;&#31639;&#26159;&#22810;&#27169;&#24577;&#25216;&#26415;&#21644;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#37325;&#35201;&#20132;&#21449;&#28857;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#22312;&#20154;&#26426;&#20132;&#20114;&#30740;&#31350;&#20013;&#24212;&#29992;&#20110;&#33041;&#21551;&#21457;&#35745;&#31639;&#30340;&#28436;&#21270;&#12289;&#24212;&#29992;&#20215;&#20540;&#12289;&#25361;&#25112;&#21644;&#28508;&#22312;&#30740;&#31350;&#36712;&#36857;&#12290;&#39318;&#20808;&#22238;&#39038;&#20102;&#22522;&#26412;&#27010;&#24565;&#21644;&#21457;&#23637;&#21382;&#21490;&#65292;&#24182;&#23558;&#20854;&#28436;&#21270;&#21010;&#20998;&#20026;&#36817;&#26399;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#65292;&#24378;&#35843;&#20102;&#27599;&#20010;&#38454;&#27573;&#22312;&#20154;&#26426;&#20132;&#20114;&#30740;&#31350;&#20013;&#23545;&#33041;&#21551;&#21457;&#35745;&#31639;&#30340;&#37325;&#35201;&#24615;&#12290;&#21478;&#22806;&#65292;&#20174;&#20845;&#20010;&#35282;&#24230;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#19981;&#21516;&#20219;&#21153;&#30340;&#20154;&#26426;&#20132;&#20114;&#33041;&#21551;&#21457;&#35745;&#31639;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#20851;&#38190;&#25216;&#26415;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#22312;&#20154;&#26426;&#20132;&#20114;&#33041;&#21551;&#21457;&#35745;&#31639;&#20013;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The continuous development of artificial intelligence has a profound impact on biomedical research and other fields.Brain-inspired computing is an important intersection of multimodal technology and biomedical field. This paper presents a comprehensive review of machine learning (ML) and deep learning (DL) models applied in human-computer interaction for brain-inspired computing, tracking their evolution, application value, challenges, and potential research trajectories. First, the basic concepts and development history are reviewed, and their evolution is divided into two stages: recent machine learning and current deep learning, emphasizing the importance of each stage in the research state of human-computer interaction for brain-inspired computing. In addition, the latest progress and key techniques of deep learning in different tasks of human-computer interaction for brain-inspired computing are introduced from six perspectives. Despite significant progress, challenges remain in m
&lt;/p&gt;</description></item><item><title>AccoMontage-3&#26159;&#19968;&#31181;&#36890;&#36807;&#39034;&#24207;&#39118;&#26684;&#36716;&#25442;&#21644;&#22810;&#36712;&#36947;&#21151;&#33021;&#20808;&#39564;&#23454;&#29616;&#20840;&#38899;&#20048;&#20276;&#22863;&#32534;&#25490;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#26681;&#25454;&#20027;&#26059;&#24459;&#19982;&#21644;&#24358;&#30340;&#36755;&#20837;&#29983;&#25104;&#22810;&#38899;&#36712;&#30340;&#20276;&#22863;&#12290;</title><link>http://arxiv.org/abs/2310.16334</link><description>&lt;p&gt;
AccoMontage-3: &#36890;&#36807;&#39034;&#24207;&#39118;&#26684;&#36716;&#25442;&#21644;&#22810;&#36712;&#36947;&#21151;&#33021;&#20808;&#39564;&#23454;&#29616;&#20840;&#38899;&#20048;&#20276;&#22863;&#32534;&#25490;
&lt;/p&gt;
&lt;p&gt;
AccoMontage-3: Full-Band Accompaniment Arrangement via Sequential Style Transfer and Multi-Track Function Prior. (arXiv:2310.16334v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16334
&lt;/p&gt;
&lt;p&gt;
AccoMontage-3&#26159;&#19968;&#31181;&#36890;&#36807;&#39034;&#24207;&#39118;&#26684;&#36716;&#25442;&#21644;&#22810;&#36712;&#36947;&#21151;&#33021;&#20808;&#39564;&#23454;&#29616;&#20840;&#38899;&#20048;&#20276;&#22863;&#32534;&#25490;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#26681;&#25454;&#20027;&#26059;&#24459;&#19982;&#21644;&#24358;&#30340;&#36755;&#20837;&#29983;&#25104;&#22810;&#38899;&#36712;&#30340;&#20276;&#22863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;AccoMontage-3&#65292;&#36825;&#26159;&#19968;&#20010;&#31526;&#21495;&#38899;&#20048;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#21487;&#20197;&#26681;&#25454;&#20027;&#26059;&#24459;&#19982;&#21644;&#24358;&#30340;&#36755;&#20837;&#65288;&#21363;&#24341;&#23548;&#20048;&#35889;&#65289;&#65292;&#29983;&#25104;&#22810;&#38899;&#36712;&#30340;&#20840;&#38899;&#20048;&#20276;&#22863;&#12290;&#35813;&#31995;&#32479;&#21253;&#21547;&#19977;&#20010;&#27169;&#22359;&#21270;&#32452;&#20214;&#65292;&#27599;&#20010;&#32452;&#20214;&#27169;&#25311;&#20840;&#38899;&#20048;&#20316;&#26354;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#31532;&#19968;&#20010;&#32452;&#20214;&#26159;&#38050;&#29748;&#32534;&#26354;&#24072;&#65292;&#36890;&#36807;&#23558;&#32441;&#29702;&#39118;&#26684;&#36716;&#25442;&#20026;&#21644;&#24358;&#65292;&#20351;&#29992;&#28508;&#22312;&#30340;&#21644;&#24358;-&#32441;&#29702;&#20998;&#31163;&#21644;&#21551;&#21457;&#24335;&#32441;&#29702;&#20379;&#24212;&#32773;&#26816;&#32034;&#65292;&#29983;&#25104;&#38050;&#29748;&#20276;&#22863;&#12290;&#31532;&#20108;&#20010;&#32452;&#20214;&#26681;&#25454;&#20010;&#21035;&#38899;&#36712;&#21151;&#33021;&#32534;&#30721;&#30340;&#31649;&#24358;&#20048;&#39118;&#26684;&#65292;&#23558;&#38050;&#29748;&#20276;&#22863;&#20048;&#35889;&#32534;&#25490;&#25104;&#20840;&#38899;&#20048;&#20276;&#22863;&#12290;&#23558;&#21069;&#20004;&#20010;&#32452;&#20214;&#36830;&#25509;&#36215;&#26469;&#30340;&#31532;&#19977;&#20010;&#32452;&#20214;&#26159;&#19968;&#20010;&#20808;&#39564;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#25972;&#39318;&#38899;&#20048;&#20316;&#21697;&#19978;&#30340;&#32534;&#26354;&#39118;&#26684;&#30340;&#20840;&#23616;&#32467;&#26500;&#12290;&#25972;&#20010;&#31995;&#32479;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#33258;&#25105;&#30417;&#30563;&#22320;&#23398;&#20064;&#29983;&#25104;&#20840;&#38899;&#20048;&#20276;&#22863;&#65292;&#23558;&#39118;&#26684;&#36716;&#25442;&#24212;&#29992;&#20110;&#20004;&#20010;&#23618;&#38754;&#30340;&#22810;&#22768;&#37096;&#21327;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose AccoMontage-3, a symbolic music automation system capable of generating multi-track, full-band accompaniment based on the input of a lead melody with chords (i.e., a lead sheet). The system contains three modular components, each modelling a vital aspect of full-band composition. The first component is a piano arranger that generates piano accompaniment for the lead sheet by transferring texture styles to the chords using latent chord-texture disentanglement and heuristic retrieval of texture donors. The second component orchestrates the piano accompaniment score into full-band arrangement according to the orchestration style encoded by individual track functions. The third component, which connects the previous two, is a prior model characterizing the global structure of orchestration style over the whole piece of music. From end to end, the system learns to generate full-band accompaniment in a self-supervised fashion, applying style transfer at two levels of polyphonic co
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#33258;&#32534;&#30721;&#22120;&#30340;&#32593;&#32476;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#19982;&#22270;&#30340;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#30456;&#20851;&#30340;&#33410;&#28857;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#23545;&#40784;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#23545;&#40784;&#20219;&#21153;&#20013;&#23454;&#29616;&#39640;&#25928;&#30340;&#23545;&#40784;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2310.03272</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#20256;&#36755;&#30340;&#22270;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#32593;&#32476;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Network Alignment with Transferable Graph Autoencoders. (arXiv:2310.03272v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03272
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#33258;&#32534;&#30721;&#22120;&#30340;&#32593;&#32476;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#19982;&#22270;&#30340;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#30456;&#20851;&#30340;&#33410;&#28857;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#23545;&#40784;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#23545;&#40784;&#20219;&#21153;&#20013;&#23454;&#29616;&#39640;&#25928;&#30340;&#23545;&#40784;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#23545;&#40784;&#26159;&#22312;&#19981;&#21516;&#22270;&#20043;&#38388;&#24314;&#31435;&#19968;&#23545;&#19968;&#23545;&#24212;&#20851;&#31995;&#30340;&#20219;&#21153;&#65292;&#22312;&#39640;&#24433;&#21709;&#39046;&#22495;&#20013;&#26377;&#22823;&#37327;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#20219;&#21153;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#34987;&#35748;&#20026;&#26159;NP&#38590;&#30340;&#65292;&#32780;&#19988;&#29616;&#26377;&#30340;&#31639;&#27861;&#22312;&#22270;&#30340;&#35268;&#27169;&#22686;&#22823;&#26102;&#26080;&#27861;&#25193;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24191;&#20041;&#22270;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#65292;&#26088;&#22312;&#25552;&#21462;&#24378;&#22823;&#19988;&#40065;&#26834;&#30340;&#33410;&#28857;&#23884;&#20837;&#65292;&#36866;&#29992;&#20110;&#23545;&#40784;&#20219;&#21153;&#12290;&#25105;&#20204;&#35777;&#26126;&#29983;&#25104;&#30340;&#23884;&#20837;&#19982;&#22270;&#30340;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#30456;&#20851;&#65292;&#24182;&#19988;&#19982;&#32463;&#20856;&#35889;&#26041;&#27861;&#30456;&#27604;&#21487;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#36824;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#65292;&#22312;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#25928;&#30340;&#22823;&#35268;&#27169;&#32593;&#32476;&#23545;&#40784;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#32593;&#32476;&#23545;&#40784;&#21644;&#23376;&#32593;&#32476;&#23545;&#40784;&#23454;&#39564;&#65292;&#25552;&#20379;&#20102;&#25903;&#25345;&#35813;&#26694;&#26550;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network alignment is the task of establishing one-to-one correspondences between the nodes of different graphs and finds a plethora of applications in high-impact domains. However, this task is known to be NP-hard in its general form, and existing algorithms do not scale up as the size of the graphs increases. To tackle both challenges we propose a novel generalized graph autoencoder architecture, designed to extract powerful and robust node embeddings, that are tailored to the alignment task. We prove that the generated embeddings are associated with the eigenvalues and eigenvectors of the graphs and can achieve more accurate alignment compared to classical spectral methods. Our proposed framework also leverages transfer learning and data augmentation to achieve efficient network alignment at a very large scale without retraining. Extensive experiments on both network and sub-network alignment with real-world graphs provide corroborating evidence supporting the effectiveness and scala
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;SpikingNeRF&#65292;&#23427;&#36890;&#36807;&#23558;&#36752;&#23556;&#20809;&#32447;&#19982;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#32500;&#24230;&#23545;&#40784;&#65292;&#20197;&#33410;&#30465;&#33021;&#37327;&#24182;&#20943;&#23569;&#35745;&#31639;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.10987</link><description>&lt;p&gt;
Spiking NeRF&#65306;&#20351;&#29983;&#29289;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#31359;&#36879;&#29616;&#23454;&#19990;&#30028;
&lt;/p&gt;
&lt;p&gt;
Spiking NeRF: Making Bio-inspired Neural Networks See through the Real World. (arXiv:2309.10987v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SpikingNeRF&#65292;&#23427;&#36890;&#36807;&#23558;&#36752;&#23556;&#20809;&#32447;&#19982;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#32500;&#24230;&#23545;&#40784;&#65292;&#20197;&#33410;&#30465;&#33021;&#37327;&#24182;&#20943;&#23569;&#35745;&#31639;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#21033;&#29992;&#20854;&#20855;&#26377;&#28508;&#22312;&#29983;&#29289;&#23398;&#21487;&#34892;&#24615;&#30340;&#33021;&#37327;&#25928;&#29575;&#21644;&#28508;&#21147;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#20197;&#22823;&#37327;&#33021;&#37327;&#28040;&#32791;&#28210;&#26579;&#39640;&#36136;&#37327;&#30340;3D&#22330;&#26223;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#28145;&#20837;&#25506;&#32034;&#20197;&#29983;&#29289;&#21551;&#21457;&#30340;&#26041;&#27861;&#36827;&#34892;&#33410;&#33021;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#33033;&#20914;NeRF&#65288;SpikingNeRF&#65289;&#65292;&#23558;&#36752;&#23556;&#20809;&#32447;&#19982;SNN&#30340;&#26102;&#38388;&#32500;&#24230;&#23545;&#40784;&#65292;&#20197;&#33258;&#28982;&#22320;&#36866;&#24212;SNN&#23545;&#36752;&#23556;&#22330;&#30340;&#37325;&#24314;&#12290;&#22240;&#27492;&#65292;&#35745;&#31639;&#20197;&#22522;&#20110;&#33033;&#20914;&#12289;&#26080;&#20056;&#27861;&#30340;&#26041;&#24335;&#36827;&#34892;&#65292;&#20174;&#32780;&#20943;&#23569;&#33021;&#37327;&#28040;&#32791;&#12290;&#22312;SpikingNeRF&#20013;&#65292;&#20809;&#32447;&#19978;&#30340;&#27599;&#20010;&#37319;&#26679;&#28857;&#21305;&#37197;&#21040;&#29305;&#23450;&#30340;&#26102;&#38388;&#27493;&#65292;&#24182;&#20197;&#28151;&#21512;&#26041;&#24335;&#34920;&#31034;&#65292;&#20854;&#20013;&#20307;&#32032;&#32593;&#26684;&#20063;&#24471;&#21040;&#32500;&#25252;&#12290;&#22522;&#20110;&#20307;&#32032;&#32593;&#26684;&#65292;&#30830;&#23450;&#37319;&#26679;&#28857;&#26159;&#21542;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#36807;&#31243;&#20013;&#34987;&#23631;&#34109;&#20197;&#36827;&#34892;&#26356;&#22909;&#30340;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#25805;&#20316;&#20063;&#20250;&#20135;&#29983;&#19981;&#21487;&#36870;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neuron networks (SNNs) have been thriving on numerous tasks to leverage their promising energy efficiency and exploit their potentialities as biologically plausible intelligence. Meanwhile, the Neural Radiance Fields (NeRF) render high-quality 3D scenes with massive energy consumption, and few works delve into the energy-saving solution with a bio-inspired approach. In this paper, we propose spiking NeRF (SpikingNeRF), which aligns the radiance ray with the temporal dimension of SNN, to naturally accommodate the SNN to the reconstruction of Radiance Fields. Thus, the computation turns into a spike-based, multiplication-free manner, reducing the energy consumption. In SpikingNeRF, each sampled point on the ray is matched onto a particular time step, and represented in a hybrid manner where the voxel grids are maintained as well. Based on the voxel grids, sampled points are determined whether to be masked for better training and inference. However, this operation also incurs irre
&lt;/p&gt;</description></item><item><title>S-HR-VQVAE&#26159;&#19968;&#31181;&#24207;&#21015;&#20998;&#23618;&#27531;&#24046;&#23398;&#20064;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#32467;&#21512;&#20998;&#23618;&#27531;&#24046;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;HR-VQVAE&#65289;&#21644;&#26102;&#31354;PixelCNN&#65288;ST-PixelCNN&#65289;&#30340;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#35270;&#39057;&#39044;&#27979;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#22312;KTH&#20154;&#20307;&#21160;&#20316;&#21644;Moving-MNIST&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.06701</link><description>&lt;p&gt;
S-HR-VQVAE: &#24207;&#21015;&#20998;&#23618;&#27531;&#24046;&#23398;&#20064;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#35270;&#39057;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
S-HR-VQVAE: Sequential Hierarchical Residual Learning Vector Quantized Variational Autoencoder for Video Prediction. (arXiv:2307.06701v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06701
&lt;/p&gt;
&lt;p&gt;
S-HR-VQVAE&#26159;&#19968;&#31181;&#24207;&#21015;&#20998;&#23618;&#27531;&#24046;&#23398;&#20064;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#32467;&#21512;&#20998;&#23618;&#27531;&#24046;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;HR-VQVAE&#65289;&#21644;&#26102;&#31354;PixelCNN&#65288;ST-PixelCNN&#65289;&#30340;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#35270;&#39057;&#39044;&#27979;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#22312;KTH&#20154;&#20307;&#21160;&#20316;&#21644;Moving-MNIST&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#23558;&#25105;&#20204;&#26368;&#36817;&#25552;&#20986;&#30340;&#20998;&#23618;&#27531;&#24046;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;HR-VQVAE&#65289;&#19982;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#31354;PixelCNN&#65288;ST-PixelCNN&#65289;&#30456;&#32467;&#21512;&#65292;&#29992;&#26469;&#35299;&#20915;&#35270;&#39057;&#39044;&#27979;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;&#24207;&#21015;&#20998;&#23618;&#27531;&#24046;&#23398;&#20064;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;S-HR-VQVAE&#65289;&#12290;&#36890;&#36807;&#21033;&#29992;HR-VQVAE&#22312;&#23545;&#38745;&#27490;&#22270;&#20687;&#36827;&#34892;&#24314;&#27169;&#26102;&#30340;&#20869;&#22312;&#33021;&#21147;&#21644;&#32039;&#20945;&#34920;&#31034;&#65292;&#20197;&#21450;ST-PixelCNN&#22788;&#29702;&#26102;&#31354;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292; S-HR-VQVAE&#33021;&#22815;&#26356;&#22909;&#22320;&#24212;&#23545;&#35270;&#39057;&#39044;&#27979;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#21253;&#25324;&#23398;&#20064;&#26102;&#31354;&#20449;&#24687;&#12289;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#12289;&#28040;&#38500;&#27169;&#31946;&#39044;&#27979;&#21644;&#38544;&#24335;&#24314;&#27169;&#29289;&#29702;&#29305;&#24615;&#12290;&#23545;KTH&#20154;&#20307;&#21160;&#20316;&#21644;Moving-MNIST&#20219;&#21153;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#26041;&#38754;&#19982;&#39030;&#32423;&#35270;&#39057;&#39044;&#27979;&#25216;&#26415;&#30456;&#27604;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the video prediction task by putting forth a novel model that combines (i) our recently proposed hierarchical residual vector quantized variational autoencoder (HR-VQVAE), and (ii) a novel spatiotemporal PixelCNN (ST-PixelCNN). We refer to this approach as a sequential hierarchical residual learning vector quantized variational autoencoder (S-HR-VQVAE). By leveraging the intrinsic capabilities of HR-VQVAE at modeling still images with a parsimonious representation, combined with the ST-PixelCNN's ability at handling spatiotemporal information, S-HR-VQVAE can better deal with chief challenges in video prediction. These include learning spatiotemporal information, handling high dimensional data, combating blurry prediction, and implicit modeling of physical characteristics. Extensive experimental results on the KTH Human Action and Moving-MNIST tasks demonstrate that our model compares favorably against top video prediction techniques both in quantitative and qualitative evalu
&lt;/p&gt;</description></item><item><title>BackpropTools&#26159;&#19968;&#27454;&#24555;&#36895;&#12289;&#21487;&#31227;&#26893;&#30340;&#36830;&#32493;&#25511;&#21046;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24211;&#65292;&#23427;&#36890;&#36807;&#27169;&#26495;&#20803;&#32534;&#31243;&#25552;&#20379;&#32039;&#23494;&#38598;&#25104;&#30340;&#21487;&#32452;&#21512;&#32452;&#20214;&#65292;&#24182;&#22312;&#24322;&#26500;&#24179;&#21488;&#38598;&#21512;&#19978;&#26080;&#32541;&#20351;&#29992;&#65292;&#21516;&#26102;&#22312;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#30340;&#28145;&#24230;RL&#20195;&#29702;&#39640;&#25928;&#21487;&#25193;&#23637;&#35757;&#32451;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#30001;&#20110;&#20854;&#21487;&#31227;&#26893;&#24615;&#21644;&#23454;&#26102;&#20445;&#35777;&#65292;&#23427;&#25104;&#20026;&#20102;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#23398;&#26469;&#30340;&#31574;&#30053;&#30340;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2306.03530</link><description>&lt;p&gt;
BackpropTools: &#19968;&#27454;&#24555;&#36895;&#12289;&#21487;&#31227;&#26893;&#30340;&#36830;&#32493;&#25511;&#21046;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24211;
&lt;/p&gt;
&lt;p&gt;
BackpropTools: A Fast, Portable Deep Reinforcement Learning Library for Continuous Control. (arXiv:2306.03530v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03530
&lt;/p&gt;
&lt;p&gt;
BackpropTools&#26159;&#19968;&#27454;&#24555;&#36895;&#12289;&#21487;&#31227;&#26893;&#30340;&#36830;&#32493;&#25511;&#21046;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24211;&#65292;&#23427;&#36890;&#36807;&#27169;&#26495;&#20803;&#32534;&#31243;&#25552;&#20379;&#32039;&#23494;&#38598;&#25104;&#30340;&#21487;&#32452;&#21512;&#32452;&#20214;&#65292;&#24182;&#22312;&#24322;&#26500;&#24179;&#21488;&#38598;&#21512;&#19978;&#26080;&#32541;&#20351;&#29992;&#65292;&#21516;&#26102;&#22312;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#30340;&#28145;&#24230;RL&#20195;&#29702;&#39640;&#25928;&#21487;&#25193;&#23637;&#35757;&#32451;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#30001;&#20110;&#20854;&#21487;&#31227;&#26893;&#24615;&#21644;&#23454;&#26102;&#20445;&#35777;&#65292;&#23427;&#25104;&#20026;&#20102;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#23398;&#26469;&#30340;&#31574;&#30053;&#30340;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#20135;&#29983;&#20986;&#20855;&#26377;&#33021;&#21147;&#30340;&#20195;&#29702;&#21644;&#25511;&#21046;&#31574;&#30053;&#65292;&#20294;&#24120;&#24120;&#21463;&#21040;&#35757;&#32451;&#26102;&#38388;&#36807;&#38271;&#30340;&#22256;&#25200;&#12290;&#27492;&#22806;&#65292;&#22312;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#28145;&#24230;&#23398;&#20064;&#24211;&#30340;&#23454;&#26102;&#24615;&#21644;&#21487;&#31227;&#26893;&#24615;&#30340;&#32570;&#20047;&#38480;&#21046;&#20102;&#23398;&#20064;&#31574;&#30053;&#22312;&#23454;&#38469;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BackpropTools&#65292;&#19968;&#31181;&#20381;&#36182;&#24615;-free&#12289;header-only&#12289;pure C++&#30340;&#28145;&#24230;&#30417;&#30563;&#21644;&#24378;&#21270;&#23398;&#20064;&#24211;&#12290;&#21033;&#29992;&#26368;&#36817;C++&#26631;&#20934;&#30340;&#27169;&#26495;&#20803;&#32534;&#31243;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21487;&#20197;&#30001;&#32534;&#35793;&#22120;&#32039;&#23494;&#38598;&#25104;&#30340;&#21487;&#32452;&#21512;&#32452;&#20214;&#12290;&#20854;&#26032;&#39062;&#30340;&#26550;&#26500;&#20801;&#35768;BackpropTools&#22312;&#24322;&#26500;&#24179;&#21488;&#38598;&#21512;&#19978;&#26080;&#32541;&#20351;&#29992;&#65292;&#20174;HPC&#38598;&#32676;&#12289;&#24037;&#20316;&#31449;&#21644;&#31508;&#35760;&#26412;&#30005;&#33041;&#21040;&#26234;&#33021;&#25163;&#26426;&#12289;&#26234;&#33021;&#25163;&#34920;&#21644;&#24494;&#25511;&#21046;&#22120;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#30001;&#20110;RL&#31639;&#27861;&#19982;&#27169;&#25311;&#29615;&#22659;&#30340;&#32039;&#23494;&#38598;&#25104;&#65292;BackpropTools&#22312;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#30340;&#28145;&#24230;RL&#20195;&#29702;&#30340;&#39640;&#25928;&#21487;&#25193;&#23637;&#35757;&#32451;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#23427;&#30340;&#21487;&#31227;&#26893;&#24615;&#21644;&#23454;&#26102;&#20445;&#35777;&#20351;&#20854;&#25104;&#20026;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#23398;&#26469;&#30340;&#31574;&#30053;&#30340;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning (RL) has been demonstrated to yield capable agents and control policies in several domains but is commonly plagued by prohibitively long training times. Additionally, in the case of continuous control problems, the applicability of learned policies on real-world embedded devices is limited due to the lack of real-time guarantees and portability of existing deep learning libraries. To address these challenges, we present BackpropTools, a dependency-free, header-only, pure C++ library for deep supervised and reinforcement learning. Leveraging the template meta-programming capabilities of recent C++ standards, we provide composable components that can be tightly integrated by the compiler. Its novel architecture allows BackpropTools to be used seamlessly on a heterogeneous set of platforms, from HPC clusters over workstations and laptops to smartphones, smartwatches, and microcontrollers. Specifically, due to the tight integration of the RL algorithms with simu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#27010;&#29575;&#35302;&#21457;&#33218;&#30340;&#24773;&#22659;&#32452;&#21512;&#36172;&#21338;&#26426;&#65292;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#35774;&#35745;&#20102;C$^2$-UCB-T&#31639;&#27861;&#21644;VAC$^2$-UCB&#31639;&#27861;&#65292;&#24182;&#20998;&#21035;&#23548;&#20986;&#20102;&#23545;&#24212;&#30340;&#36951;&#25022;&#20540;&#19978;&#38480;&#65292;&#20026;&#30456;&#20851;&#24212;&#29992;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2303.17110</link><description>&lt;p&gt;
&#24102;&#26377;&#27010;&#29575;&#35302;&#21457;&#33218;&#30340;&#24773;&#22659;&#32452;&#21512;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Contextual Combinatorial Bandits with Probabilistically Triggered Arms. (arXiv:2303.17110v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#27010;&#29575;&#35302;&#21457;&#33218;&#30340;&#24773;&#22659;&#32452;&#21512;&#36172;&#21338;&#26426;&#65292;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#35774;&#35745;&#20102;C$^2$-UCB-T&#31639;&#27861;&#21644;VAC$^2$-UCB&#31639;&#27861;&#65292;&#24182;&#20998;&#21035;&#23548;&#20986;&#20102;&#23545;&#24212;&#30340;&#36951;&#25022;&#20540;&#19978;&#38480;&#65292;&#20026;&#30456;&#20851;&#24212;&#29992;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#25429;&#25417;&#24191;&#27867;&#24212;&#29992;&#33539;&#22260;&#30340;&#19968;&#31995;&#21015;&#24179;&#28369;&#26465;&#20214;&#19979;&#30340;&#24102;&#26377;&#27010;&#29575;&#35302;&#21457;&#33218;&#30340;&#24773;&#22659;&#32452;&#21512;&#36172;&#21338;&#26426;(C$^2$MAB-T)&#65292;&#20363;&#22914;&#24773;&#22659;&#32423;&#32852;&#36172;&#21338;&#26426;&#21644;&#24773;&#22659;&#26368;&#22823;&#21270;&#36172;&#21338;&#26426;&#12290;&#22312;&#27169;&#25311;&#35302;&#21457;&#27010;&#29575;(TPM)&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;C$^2$-UCB-T&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;$\tilde{O}(d\sqrt{KT})$&#30340;&#36951;&#25022;&#20540;&#19978;&#38480;&#65292;&#28040;&#38500;&#20102;&#19968;&#20010;&#21487;&#33021;&#25351;&#25968;&#32423;&#22686;&#38271;&#30340;&#22240;&#23376;$O(1/p_{\min})$&#65292;&#20854;&#20013;$d$&#26159;&#24773;&#22659;&#30340;&#32500;&#25968;&#65292;$p_{\min}$&#26159;&#33021;&#34987;&#35302;&#21457;&#30340;&#20219;&#20309;&#33218;&#30340;&#26368;&#23567;&#27491;&#27010;&#29575;&#65292;&#25209;&#22823;&#23567;$K$&#26159;&#27599;&#36718;&#33021;&#34987;&#35302;&#21457;&#30340;&#33218;&#30340;&#26368;&#22823;&#25968;&#37327;&#12290;&#22312;&#26041;&#24046;&#35843;&#21046;(VM)&#25110;&#35302;&#21457;&#27010;&#29575;&#21644;&#26041;&#24046;&#35843;&#21046;(TPVM)&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24046;&#33258;&#36866;&#24212;&#31639;&#27861;VAC$^2$-UCB&#65292;&#24182;&#23548;&#20986;&#20102;&#19968;&#20010;$\tilde{O}(d\sqrt{T})$&#30340;&#36951;&#25022;&#20540;&#19978;&#38480;&#65292;&#35813;&#19978;&#38480;&#19982;&#25209;&#22823;&#23567;$K$&#26080;&#20851;&#12290;&#20316;&#20026;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
We study contextual combinatorial bandits with probabilistically triggered arms (C$^2$MAB-T) under a variety of smoothness conditions that capture a wide range of applications, such as contextual cascading bandits and contextual influence maximization bandits. Under the triggering probability modulated (TPM) condition, we devise the C$^2$-UCB-T algorithm and propose a novel analysis that achieves an $\tilde{O}(d\sqrt{KT})$ regret bound, removing a potentially exponentially large factor $O(1/p_{\min})$, where $d$ is the dimension of contexts, $p_{\min}$ is the minimum positive probability that any arm can be triggered, and batch-size $K$ is the maximum number of arms that can be triggered per round. Under the variance modulated (VM) or triggering probability and variance modulated (TPVM) conditions, we propose a new variance-adaptive algorithm VAC$^2$-UCB and derive a regret bound $\tilde{O}(d\sqrt{T})$, which is independent of the batch-size $K$. As a valuable by-product, we find our a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23547;&#25214;&#22522;&#20110;&#20219;&#21153;&#30340;&#24179;&#22374;&#21306;&#22495;&#65292;&#21487;&#20197;&#25913;&#36827;&#22810;&#20219;&#21153;&#23398;&#20064;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#27491;&#30830;&#20351;&#29992;&#27491;&#21017;&#21270;&#25216;&#26415;&#20197;&#36991;&#20813;&#27425;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2211.13723</link><description>&lt;p&gt;
&#36890;&#36807;&#23547;&#25214;&#22522;&#20110;&#20219;&#21153;&#30340;&#24179;&#22374;&#21306;&#22495;&#26469;&#25913;&#36827;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improving Multi-task Learning via Seeking Task-based Flat Regions. (arXiv:2211.13723v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13723
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23547;&#25214;&#22522;&#20110;&#20219;&#21153;&#30340;&#24179;&#22374;&#21306;&#22495;&#65292;&#21487;&#20197;&#25913;&#36827;&#22810;&#20219;&#21153;&#23398;&#20064;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#27491;&#30830;&#20351;&#29992;&#27491;&#21017;&#21270;&#25216;&#26415;&#20197;&#36991;&#20813;&#27425;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#19988;&#24378;&#22823;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#36890;&#36807;&#21333;&#20010;&#39592;&#24178;&#23398;&#20064;&#22810;&#20010;&#30446;&#26631;&#12290;&#19982;&#21333;&#29420;&#35757;&#32451;&#20219;&#21153;&#30456;&#27604;&#65292;MTL&#26174;&#30528;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#20219;&#21153;&#20043;&#38388;&#30340;&#30693;&#35782;&#26469;&#28508;&#22312;&#22320;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#23427;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#65292;&#20174;&#35745;&#31639;&#26426;&#35270;&#35273;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35821;&#38899;&#35782;&#21035;&#12290;&#20854;&#20013;&#65292;MTL&#30340;&#19968;&#20010;&#26032;&#20852;&#30740;&#31350;&#26041;&#21521;&#38598;&#20013;&#22312;&#25805;&#32437;&#20219;&#21153;&#26799;&#24230;&#20197;&#25512;&#23548;&#20986;&#23545;&#25152;&#26377;&#20219;&#21153;&#26377;&#30410;&#30340;&#26368;&#32456;&#26799;&#24230;&#19979;&#38477;&#26041;&#21521;&#12290;&#23613;&#31649;&#22312;&#35768;&#22810;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#26159;&#22312;&#23454;&#38469;&#38382;&#39064;&#19978;&#30452;&#25509;&#24212;&#29992;&#36825;&#20123;&#26041;&#27861;&#32780;&#19981;&#20351;&#29992;&#36866;&#24403;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#35299;&#12290;&#29305;&#21035;&#26159;&#65292;&#26631;&#20934;&#35757;&#32451;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#26368;&#23567;&#21270;&#32463;&#39564;&#25439;&#22833;&#65292;&#24456;&#23481;&#26131;&#36973;&#21463;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Learning (MTL) is a widely-used and powerful learning paradigm for training deep neural networks that allows learning more than one objective by a single backbone. Compared to training tasks separately, MTL significantly reduces computational costs, improves data efficiency, and potentially enhances model performance by leveraging knowledge across tasks. Hence, it has been adopted in a variety of applications, ranging from computer vision to natural language processing and speech recognition. Among them, there is an emerging line of work in MTL that focuses on manipulating the task gradient to derive an ultimate gradient descent direction to benefit all tasks. Despite achieving impressive results on many benchmarks, directly applying these approaches without using appropriate regularization techniques might lead to suboptimal solutions on real-world problems. In particular, standard training that minimizes the empirical loss on the training data can easily suffer from overfi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#30340;&#20805;&#20998;&#19981;&#21464;&#23398;&#20064;&#65292;&#35266;&#23519;&#21040;&#20043;&#21069;&#30340;&#24037;&#20316;&#21482;&#23398;&#20064;&#20102;&#37096;&#20998;&#19981;&#21464;&#29305;&#24449;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#20805;&#20998;&#19981;&#21464;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25351;&#20986;&#22312;&#20998;&#24067;&#36716;&#31227;&#26102;&#65292;&#20174;&#35757;&#32451;&#38598;&#20013;&#23398;&#20064;&#30340;&#37096;&#20998;&#19981;&#21464;&#29305;&#24449;&#21487;&#33021;&#19981;&#36866;&#29992;&#20110;&#27979;&#35797;&#38598;&#65292;&#38480;&#21046;&#20102;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2210.13533</link><description>&lt;p&gt;
&#20998;&#24067;&#36716;&#31227;&#30340;&#20805;&#20998;&#19981;&#21464;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sufficient Invariant Learning for Distribution Shift. (arXiv:2210.13533v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#30340;&#20805;&#20998;&#19981;&#21464;&#23398;&#20064;&#65292;&#35266;&#23519;&#21040;&#20043;&#21069;&#30340;&#24037;&#20316;&#21482;&#23398;&#20064;&#20102;&#37096;&#20998;&#19981;&#21464;&#29305;&#24449;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#20805;&#20998;&#19981;&#21464;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25351;&#20986;&#22312;&#20998;&#24067;&#36716;&#31227;&#26102;&#65292;&#20174;&#35757;&#32451;&#38598;&#20013;&#23398;&#20064;&#30340;&#37096;&#20998;&#19981;&#21464;&#29305;&#24449;&#21487;&#33021;&#19981;&#36866;&#29992;&#20110;&#27979;&#35797;&#38598;&#65292;&#38480;&#21046;&#20102;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#30340;&#20998;&#24067;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#20445;&#35777;&#24615;&#33021;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#25913;&#21892;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36328;&#32452;&#25110;&#39046;&#22495;&#30340;&#19981;&#21464;&#29305;&#24449;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20043;&#21069;&#30340;&#24037;&#20316;&#21482;&#37096;&#20998;&#22320;&#23398;&#20064;&#20102;&#19981;&#21464;&#29305;&#24449;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#26377;&#38480;&#30340;&#19981;&#21464;&#29305;&#24449;&#65292;&#20294;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#20805;&#20998;&#19981;&#21464;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12290;&#30001;&#20110;&#21482;&#26377;&#35757;&#32451;&#38598;&#26159;&#32463;&#39564;&#24615;&#30340;&#65292;&#20174;&#35757;&#32451;&#38598;&#20013;&#23398;&#20064;&#24471;&#21040;&#30340;&#37096;&#20998;&#19981;&#21464;&#29305;&#24449;&#21487;&#33021;&#19981;&#23384;&#22312;&#20110;&#20998;&#24067;&#36716;&#31227;&#26102;&#30340;&#27979;&#35797;&#38598;&#20013;&#12290;&#22240;&#27492;&#65292;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#25552;&#39640;&#21487;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#25991;&#35748;&#20026;&#20174;&#35757;&#32451;&#38598;&#20013;&#23398;&#20064;&#20805;&#20998;&#30340;&#19981;&#21464;&#29305;&#24449;&#23545;&#20110;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms have shown remarkable performance in diverse applications. However, it is still challenging to guarantee performance in distribution shifts when distributions of training and test datasets are different. There have been several approaches to improve the performance in distribution shift cases by learning invariant features across groups or domains. However, we observe that the previous works only learn invariant features partially. While the prior works focus on the limited invariant features, we first raise the importance of the sufficient invariant features. Since only training sets are given empirically, the learned partial invariant features from training sets might not be present in the test sets under distribution shift. Therefore, the performance improvement on distribution shifts might be limited. In this paper, we argue that learning sufficient invariant features from the training set is crucial for the distribution shift case. Concretely, we newly 
&lt;/p&gt;</description></item></channel></rss>