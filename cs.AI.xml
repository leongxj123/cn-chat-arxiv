<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;ReEvo&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36229;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#19968;&#31181;&#27714;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#24605;&#35774;&#35745;&#26041;&#27861;&#21644;&#24378;&#22823;&#30340;&#28436;&#21270;&#25628;&#32034;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22312;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#19978;&#30340;&#25628;&#32034;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01145</link><description>&lt;p&gt;
ReEvo&#65306;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20855;&#26377;&#21453;&#24605;&#28436;&#21270;&#30340;&#36229;&#21551;&#21457;&#24335;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
ReEvo: Large Language Models as Hyper-Heuristics with Reflective Evolution
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;ReEvo&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36229;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#19968;&#31181;&#27714;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#24605;&#35774;&#35745;&#26041;&#27861;&#21644;&#24378;&#22823;&#30340;&#28436;&#21270;&#25628;&#32034;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22312;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#19978;&#30340;&#25628;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NP&#22256;&#38590;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#26222;&#36941;&#23384;&#22312;&#25512;&#21160;&#39046;&#22495;&#19987;&#23478;&#21442;&#19982;&#35797;&#38169;&#24335;&#21551;&#21457;&#24335;&#35774;&#35745;&#36807;&#31243;&#12290;&#35774;&#35745;&#33258;&#21160;&#21270;&#30340;&#38271;&#26399;&#21162;&#21147;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23835;&#36215;&#32780;&#33719;&#24471;&#26032;&#30340;&#21160;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#35821;&#35328;&#36229;&#21551;&#21457;&#24335;&#31639;&#27861;&#65288;LHHs&#65289;&#65292;&#23427;&#26159;&#36229;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#19968;&#31181;&#26032;&#21464;&#20307;&#65292;&#21033;&#29992;LLM&#36827;&#34892;&#21551;&#21457;&#24335;&#29983;&#25104;&#65292;&#20855;&#26377;&#26368;&#23567;&#30340;&#20154;&#24037;&#24178;&#39044;&#21644;&#24320;&#25918;&#24335;&#30340;&#21551;&#21457;&#24335;&#31354;&#38388;&#12290;&#20026;&#20102;&#22686;&#24378;LHHs&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21453;&#24605;&#28436;&#21270;&#65288;ReEvo&#65289;&#65306;&#19968;&#31181;&#36890;&#29992;&#30340;&#25628;&#32034;&#26694;&#26550;&#65292;&#27169;&#25311;&#20102;&#20154;&#31867;&#19987;&#23478;&#30340;&#21453;&#24605;&#35774;&#35745;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21487;&#25193;&#23637;&#30340;LLM&#25512;&#29702;&#12289;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#39046;&#22495;&#30693;&#35782;&#21644;&#24378;&#22823;&#30340;&#36827;&#21270;&#25628;&#32034;&#25216;&#26415;&#36828;&#36828;&#36229;&#36234;&#20102;&#20154;&#31867;&#30340;&#33021;&#21147;&#12290;&#22312;12&#20010;&#32452;&#21512;&#20248;&#21270;&#35774;&#32622;&#30340;&#35780;&#20272;&#20013;&#26174;&#31034;&#65306;1)&#28436;&#21270;&#30340;&#21475;&#22836;&#21453;&#24605;&#23548;&#33268;&#26356;&#24179;&#28369;&#30340;&#36866;&#24212;&#24230;&#22320;&#24418;&#12289;&#40657;&#30418;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#35774;&#32622;&#30340;&#26126;&#30830;&#25512;&#29702;&#20197;&#21450;&#26356;&#22909;&#30340;&#25628;&#32034;&#32467;&#26524;&#65307;2)ReEvo&#29983;&#25104;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#22312;&#20998;&#38047;&#32423;&#20248;&#21270;&#26102;&#38388;&#20869;&#33719;&#24471;&#20102;&#21487;&#38752;&#21644;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The omnipresence of NP-hard combinatorial optimization problems (COPs) compels domain experts to engage in trial-and-error heuristic design process. The long-standing endeavor of design automation has gained new momentum with the rise of large language models (LLMs). This paper introduces Language Hyper-Heuristics (LHHs), an emerging variant of Hyper-Heuristics that leverages LLMs for heuristic generation, featuring minimal manual intervention and open-ended heuristic spaces. To empower LHHs, we present Reflective Evolution (ReEvo), a generic searching framework that emulates the reflective design approach of human experts while far surpassing human capabilities with its scalable LLM inference, Internet-scale domain knowledge, and powerful evolutionary search. Evaluations across 12 COP settings show that 1) verbal reflections for evolution lead to smoother fitness landscapes, explicit inference of black-box COP settings, and better search results; 2) heuristics generated by ReEvo in mi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20998;&#21035;&#35299;&#20915;&#20102;&#26053;&#34892;&#36141;&#20080;&#32773;&#38382;&#39064;&#20013;&#30340;&#36335;&#30001;&#26500;&#24314;&#21644;&#36141;&#20080;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#20174;&#20840;&#23616;&#35282;&#24230;&#35780;&#20272;&#21644;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2404.02476</link><description>&lt;p&gt;
&#29992;&#20110;&#26053;&#34892;&#36141;&#20080;&#32773;&#38382;&#39064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Traveling Purchaser Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02476
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20998;&#21035;&#35299;&#20915;&#20102;&#26053;&#34892;&#36141;&#20080;&#32773;&#38382;&#39064;&#20013;&#30340;&#36335;&#30001;&#26500;&#24314;&#21644;&#36141;&#20080;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#20174;&#20840;&#23616;&#35282;&#24230;&#35780;&#20272;&#21644;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26053;&#34892;&#36141;&#20080;&#32773;&#38382;&#39064;&#65288;TPP&#65289;&#26159;&#19968;&#31181;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#37325;&#35201;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20998;&#21035;&#35299;&#20915;&#20102;&#36335;&#30001;&#26500;&#24314;&#21644;&#36141;&#20080;&#35268;&#21010;&#38382;&#39064;&#65292;&#21516;&#26102;&#20174;&#20840;&#23616;&#35282;&#24230;&#35780;&#20272;&#21644;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#21253;&#25324;&#29992;&#20110;&#25429;&#25417;&#24066;&#22330;-&#20135;&#21697;&#20851;&#31995;&#30340;TPP&#30340;&#20108;&#37096;&#22270;&#34920;&#31034;&#65292;&#20197;&#21450;&#20174;&#20108;&#37096;&#22270;&#20013;&#25552;&#21462;&#20449;&#24687;&#24182;&#23558;&#20854;&#29992;&#20110;&#39034;&#24207;&#26500;&#24314;&#36335;&#30001;&#30340;&#31574;&#30053;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02476v1 Announce Type: cross  Abstract: The traveling purchaser problem (TPP) is an important combinatorial optimization problem with broad applications. Due to the coupling between routing and purchasing, existing works on TPPs commonly address route construction and purchase planning simultaneously, which, however, leads to exact methods with high computational cost and heuristics with sophisticated design but limited performance. In sharp contrast, we propose a novel approach based on deep reinforcement learning (DRL), which addresses route construction and purchase planning separately, while evaluating and optimizing the solution from a global perspective. The key components of our approach include a bipartite graph representation for TPPs to capture the market-product relations, and a policy network that extracts information from the bipartite graph and uses it to sequentially construct the route. One significant benefit of our framework is that we can efficiently const
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35266;&#23519;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#21516;&#23618;&#23545;&#38544;&#34255;&#29366;&#24577;&#30340;&#24433;&#21709;&#31243;&#24230;&#65292;&#25552;&#20986;&#20102;LLM-Streamline&#26041;&#27861;&#65292;&#21253;&#25324;&#23618;&#21098;&#26525;&#21644;&#23618;&#26367;&#25442;&#65292;&#29992;&#20110;&#21387;&#32553;&#27169;&#22411;&#24182;&#20445;&#25345;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.19135</link><description>&lt;p&gt;
&#36890;&#36807;&#31616;&#21270;&#19981;&#37325;&#35201;&#30340;&#23618;&#21387;&#32553;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Compressing Large Language Models by Streamlining the Unimportant Layer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19135
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35266;&#23519;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#21516;&#23618;&#23545;&#38544;&#34255;&#29366;&#24577;&#30340;&#24433;&#21709;&#31243;&#24230;&#65292;&#25552;&#20986;&#20102;LLM-Streamline&#26041;&#27861;&#65292;&#21253;&#25324;&#23618;&#21098;&#26525;&#21644;&#23618;&#26367;&#25442;&#65292;&#29992;&#20110;&#21387;&#32553;&#27169;&#22411;&#24182;&#20445;&#25345;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#21644;&#39046;&#22495;&#65292;&#20294;&#20854;&#36866;&#29992;&#24615;&#21463;&#21040;&#27169;&#22411;&#21442;&#25968;&#30340;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#20851;&#27880;&#34920;&#29616;&#20986;&#39640;&#24615;&#33021;&#30340;&#32039;&#20945;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;LLM&#30340;&#19981;&#21516;&#23618;&#23545;&#38544;&#34255;&#29366;&#24577;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#25200;&#21160;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;&#20986;&#19981;&#37027;&#20040;&#37325;&#35201;&#30340;&#23618;&#12290;&#22522;&#20110;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLM-Streamline&#65292;&#21253;&#25324;&#20004;&#37096;&#20998;&#65306;&#23618;&#21098;&#26525;&#65292;&#26681;&#25454;&#30446;&#26631;&#31232;&#30095;&#24230;&#31227;&#38500;&#27169;&#22411;&#20013;&#19968;&#32452;&#36830;&#32493;&#30340;&#26368;&#19981;&#37325;&#35201;&#30340;&#23618;&#65307;&#23618;&#26367;&#25442;&#65292;&#35757;&#32451;&#19968;&#20010;&#36731;&#37327;&#32423;&#27169;&#22411;&#26469;&#26367;&#25442;&#34987;&#21098;&#26525;&#30340;&#23618;&#65292;&#20174;&#32780;&#32531;&#35299;&#30001;&#21098;&#26525;&#36896;&#25104;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;(MLP)&#21644;&#19968;&#20010;transformer&#23618;&#31561;&#32467;&#26500;&#20316;&#20026;&#36731;&#37327;&#32423;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19135v1 Announce Type: cross  Abstract: Large language models (LLM) have been extensively applied in various natural language tasks and domains, but their applicability is constrained by the large number of parameters of the models. Consequently, there is an increasing emphasis on compact models that exhibit high performance. In this study, we observe that different layers in LLM have varying degrees of perturbation on the hidden states, which allows us to identify less important layers. Based on this phenomenon, we propose LLM-Streamline, which consists of two parts: layer pruning, where we remove a set of consecutive layers with the lowest importance in the model according to the target sparsity; and layer replacement, where we train a lightweight model to substitute the pruned layers, thereby mitigating the performance degradation caused by pruning. In our experiments, we utilize structures such as a multi-layer perceptron (MLP) and a transformer layer as lightweight mode
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23450;&#20041;&#35821;&#20041;&#29420;&#31435;&#21306;&#22495;&#65288;SIRs&#65289;&#24182;&#35774;&#35745;HOmogeneous&#35270;&#35273;tOKenizer (HOOK)&#65292;&#23454;&#29616;&#20102;&#20351;&#29992;&#26377;&#24847;&#20041;&#30340;&#22522;&#26412;&#20803;&#32032;&#26469;&#21152;&#24378;&#36965;&#24863;&#22270;&#20687;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.18593</link><description>&lt;p&gt;
&#22343;&#21248;&#20998;&#35789;&#22120;&#30340;&#37325;&#35201;&#24615;&#65306;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#29702;&#35299;&#30340;&#22343;&#21248;&#35270;&#35273;&#20998;&#35789;&#22120;
&lt;/p&gt;
&lt;p&gt;
Homogeneous Tokenizer Matters: Homogeneous Visual Tokenizer for Remote Sensing Image Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18593
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23450;&#20041;&#35821;&#20041;&#29420;&#31435;&#21306;&#22495;&#65288;SIRs&#65289;&#24182;&#35774;&#35745;HOmogeneous&#35270;&#35273;tOKenizer (HOOK)&#65292;&#23454;&#29616;&#20102;&#20351;&#29992;&#26377;&#24847;&#20041;&#30340;&#22522;&#26412;&#20803;&#32032;&#26469;&#21152;&#24378;&#36965;&#24863;&#22270;&#20687;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#35760;&#22120;&#20316;&#20026;&#22823;&#22411;&#27169;&#22411;&#30340;&#22522;&#26412;&#32452;&#20214;&#20043;&#19968;&#65292;&#38271;&#26399;&#20197;&#26469;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#34987;&#24573;&#35270;&#29978;&#33267;&#35823;&#35299;&#12290;&#22823;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#24378;&#22823;&#29702;&#35299;&#33021;&#21147;&#30340;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#26159;&#33258;&#28982;&#35821;&#35328;&#26631;&#35760;&#22120;&#21033;&#29992;&#26377;&#24847;&#20041;&#30340;&#35789;&#25110;&#23376;&#35789;&#20316;&#20026;&#35821;&#35328;&#30340;&#22522;&#26412;&#20803;&#32032;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20197;&#22522;&#20110;&#34917;&#19969;&#30340;&#26041;&#27861;&#22914;Patch Embed&#20026;&#20195;&#34920;&#30340;&#20027;&#27969;&#35270;&#35273;&#26631;&#35760;&#22120;&#20381;&#36182;&#20110;&#26080;&#24847;&#20041;&#30340;&#30697;&#24418;&#34917;&#19969;&#20316;&#20026;&#35270;&#35273;&#30340;&#22522;&#26412;&#20803;&#32032;&#65292;&#36825;&#19981;&#33021;&#20687;&#35821;&#35328;&#20013;&#30340;&#35789;&#25110;&#23376;&#35789;&#19968;&#26679;&#26377;&#25928;&#22320;&#21457;&#25381;&#20316;&#29992;&#12290;&#20174;&#26631;&#35760;&#22120;&#30340;&#26412;&#36136;&#20986;&#21457;&#65292;&#25105;&#20204;&#20026;&#35270;&#35273;&#23450;&#20041;&#20102;&#35821;&#20041;&#29420;&#31435;&#21306;&#22495;&#65288;SIRs&#65289;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;HOmogeneous&#35270;&#35273;tOKenizer: HOOK&#12290;HOOK&#20027;&#35201;&#30001;&#20004;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;&#29289;&#20307;&#24863;&#30693;&#27169;&#22359;&#65288;OPM&#65289;&#21644;&#29289;&#20307;&#30690;&#37327;&#21270;&#27169;&#22359;&#65288;OVM&#65289;&#12290;&#20026;&#23454;&#29616;&#22343;&#21248;&#24615;&#65292;OPM&#23558;&#22270;&#20687;&#20998;&#21106;&#20026;4*4&#20687;&#32032;&#31181;&#23376;&#65292;&#28982;&#21518;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18593v1 Announce Type: cross  Abstract: The tokenizer, as one of the fundamental components of large models, has long been overlooked or even misunderstood in visual tasks. One key factor of the great comprehension power of the large language model is that natural language tokenizers utilize meaningful words or subwords as the basic elements of language. In contrast, mainstream visual tokenizers, represented by patch-based methods such as Patch Embed, rely on meaningless rectangular patches as basic elements of vision, which cannot serve as effectively as words or subwords in language. Starting from the essence of the tokenizer, we defined semantically independent regions (SIRs) for vision. We designed a simple HOmogeneous visual tOKenizer: HOOK. HOOK mainly consists of two modules: the Object Perception Module (OPM) and the Object Vectorization Module (OVM). To achieve homogeneity, the OPM splits the image into 4*4 pixel seeds and then utilizes the attention mechanism to pe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;LLMs&#20026;MAS&#20013;&#30340;agent&#36171;&#20104;&#35268;&#33539;&#33021;&#21147;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#21019;&#24314;&#20855;&#26377;&#35268;&#33539;&#21151;&#33021;&#30340;LLM agent&#30340;&#24895;&#26223;&#12290;</title><link>https://arxiv.org/abs/2403.16524</link><description>&lt;p&gt;
&#21033;&#29992;LLMs&#30340;&#21147;&#37327;&#36827;&#34892;MAS&#20013;&#30340;&#35268;&#33539;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Harnessing the power of LLMs for normative reasoning in MASs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;LLMs&#20026;MAS&#20013;&#30340;agent&#36171;&#20104;&#35268;&#33539;&#33021;&#21147;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#21019;&#24314;&#20855;&#26377;&#35268;&#33539;&#21151;&#33021;&#30340;LLM agent&#30340;&#24895;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;agent&#65292;&#26080;&#35770;&#26159;&#20154;&#31867;&#36824;&#26159;&#35745;&#31639;&#26426;&#65292;&#37117;&#19981;&#26159;&#29420;&#31435;&#23384;&#22312;&#30340;&#65292;&#36890;&#24120;&#38656;&#35201;&#19982;&#20182;&#20154;&#21327;&#20316;&#25110;&#21327;&#35843;&#20197;&#23454;&#29616;&#20182;&#20204;&#30340;&#30446;&#26631;&#12290;&#22312;&#20154;&#31867;&#31038;&#20250;&#20013;&#65292;&#35268;&#33539;&#31561;&#31038;&#20250;&#26426;&#21046;&#30830;&#20445;&#20102;&#26377;&#25928;&#30340;&#36816;&#34892;&#65292;&#30740;&#31350;&#20154;&#21592;&#22312;&#22810;Agent&#31995;&#32479;&#65288;MAS&#65289;&#20013;&#37319;&#29992;&#36825;&#20123;&#25216;&#26415;&#26469;&#21019;&#24314;&#20855;&#26377;&#31038;&#20250;&#24847;&#35782;&#30340;agent&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#25216;&#26415;&#23384;&#22312;&#19968;&#20123;&#23616;&#38480;&#24615;&#65292;&#27604;&#22914;&#22312;&#26377;&#38480;&#29615;&#22659;&#20013;&#36816;&#20316;&#65292;&#36890;&#24120;&#20351;&#29992;&#33030;&#24369;&#30340;&#31526;&#21495;&#25512;&#29702;&#12290;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20016;&#23500;&#21644;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#35789;&#27719;&#34920;&#36798;&#35268;&#33539;&#65292;&#20351;&#33021;&#22815;&#25191;&#34892;&#19968;&#31995;&#21015;&#20219;&#21153;&#30340;&#20855;&#26377;&#35268;&#33539;&#21151;&#33021;&#30340;agent&#65292;&#22914;&#35268;&#33539;&#21457;&#29616;&#12289;&#35268;&#33539;&#25512;&#29702;&#21644;&#20915;&#31574;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;LLM&#30340;agent&#33719;&#24471;&#35268;&#33539;&#33021;&#21147;&#30340;&#28508;&#21147;&#65292;&#20511;&#37492;&#20102;&#26368;&#36817;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;LLM&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21019;&#24314;&#35268;&#33539;LLM agent&#30340;&#24895;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16524v1 Announce Type: new  Abstract: Software agents, both human and computational, do not exist in isolation and often need to collaborate or coordinate with others to achieve their goals. In human society, social mechanisms such as norms ensure efficient functioning, and these techniques have been adopted by researchers in multi-agent systems (MAS) to create socially aware agents. However, traditional techniques have limitations, such as operating in limited environments often using brittle symbolic reasoning. The advent of Large Language Models (LLMs) offers a promising solution, providing a rich and expressive vocabulary for norms and enabling norm-capable agents that can perform a range of tasks such as norm discovery, normative reasoning and decision-making. This paper examines the potential of LLM-based agents to acquire normative capabilities, drawing on recent Natural Language Processing (NLP) and LLM research. We present our vision for creating normative LLM agent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#24494;&#20998;&#20223;&#30495;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22797;&#26434;&#30340;&#20840;&#36523;&#20223;&#30495;&#35299;&#32806;&#20026;&#20004;&#20010;&#21333;&#29420;&#30340;&#36830;&#32493;&#22495;&#65292;&#24182;&#19982;&#26356;&#31934;&#30830;&#30340;&#27169;&#22411;&#23545;&#40784;&#65292;&#26469;&#20811;&#26381;&#22235;&#36275;&#21160;&#20316;&#20013;&#30340;&#19981;&#36830;&#32493;&#24615;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.14864</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#24494;&#20998;&#20223;&#30495;&#23398;&#20064;&#22235;&#36275;&#21160;&#20316;
&lt;/p&gt;
&lt;p&gt;
Learning Quadruped Locomotion Using Differentiable Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#24494;&#20998;&#20223;&#30495;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22797;&#26434;&#30340;&#20840;&#36523;&#20223;&#30495;&#35299;&#32806;&#20026;&#20004;&#20010;&#21333;&#29420;&#30340;&#36830;&#32493;&#22495;&#65292;&#24182;&#19982;&#26356;&#31934;&#30830;&#30340;&#27169;&#22411;&#23545;&#40784;&#65292;&#26469;&#20811;&#26381;&#22235;&#36275;&#21160;&#20316;&#20013;&#30340;&#19981;&#36830;&#32493;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#37096;&#20998;&#26426;&#22120;&#20154;&#36816;&#21160;&#25511;&#21046;&#30340;&#36827;&#23637;&#37117;&#26159;&#30001;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#21487;&#24494;&#20998;&#20223;&#30495;&#30340;&#28508;&#21147;&#12290;&#21487;&#24494;&#20998;&#20223;&#30495;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#20154;&#27169;&#22411;&#35745;&#31639;&#20302;&#21464;&#24322;&#19968;&#38454;&#26799;&#24230;&#65292;&#25215;&#35834;&#20102;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#65292;&#20294;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#20854;&#22312;&#22235;&#36275;&#26426;&#22120;&#20154;&#25511;&#21046;&#26041;&#38754;&#30340;&#24212;&#29992;&#20173;&#28982;&#26377;&#38480;&#12290;&#21487;&#24494;&#20998;&#20223;&#30495;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#30001;&#20110;&#25509;&#35302;&#20016;&#23500;&#29615;&#22659;&#65288;&#22914;&#22235;&#36275;&#21160;&#20316;&#65289;&#20013;&#30340;&#19981;&#36830;&#32493;&#24615;&#65292;&#23548;&#33268;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#22797;&#26434;&#20248;&#21270;&#26223;&#35266;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21487;&#24494;&#20998;&#20223;&#30495;&#26694;&#26550;&#20197;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;&#20851;&#38190;&#24819;&#27861;&#21253;&#25324;&#23558;&#21487;&#33021;&#30001;&#20110;&#25509;&#35302;&#32780;&#20986;&#29616;&#19981;&#36830;&#32493;&#24615;&#30340;&#22797;&#26434;&#20840;&#36523;&#20223;&#30495;&#35299;&#32806;&#20026;&#20004;&#20010;&#21333;&#29420;&#30340;&#36830;&#32493;&#22495;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23558;&#31616;&#21270;&#27169;&#22411;&#20135;&#29983;&#30340;&#26426;&#22120;&#20154;&#29366;&#24577;&#19982;&#26356;&#31934;&#30830;&#30340;&#19981;&#21487;&#24494;&#20998;&#27169;&#22411;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14864v1 Announce Type: cross  Abstract: While most recent advancements in legged robot control have been driven by model-free reinforcement learning, we explore the potential of differentiable simulation. Differentiable simulation promises faster convergence and more stable training by computing low-variant first-order gradients using the robot model, but so far, its use for legged robot control has remained limited to simulation. The main challenge with differentiable simulation lies in the complex optimization landscape of robotic tasks due to discontinuities in contact-rich environments, e.g., quadruped locomotion. This work proposes a new, differentiable simulation framework to overcome these challenges. The key idea involves decoupling the complex whole-body simulation, which may exhibit discontinuities due to contact, into two separate continuous domains. Subsequently, we align the robot state resulting from the simplified model with a more precise, non-differentiable 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#23454;&#39564;&#26694;&#26550;&#29992;&#20110;&#35780;&#20272;&#20154;&#31867;&#26159;&#21542;&#36890;&#36807;&#20351;&#29992;AI&#21487;&#20197;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#65292;&#22312;&#21333;&#30450;&#23454;&#39564;&#35774;&#35745;&#20013;&#27604;&#36739;&#20102;&#19977;&#31181;&#20915;&#31574;&#31995;&#32479;&#30340;&#34920;&#29616;</title><link>https://arxiv.org/abs/2403.12108</link><description>&lt;p&gt;
AI&#26159;&#21542;&#26377;&#21161;&#20110;&#20154;&#31867;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#65311;&#19968;&#31181;&#29992;&#20110;&#23454;&#39564;&#35780;&#20272;&#30340;&#26041;&#27861;&#35770;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Does AI help humans make better decisions? A methodological framework for experimental evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12108
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#23454;&#39564;&#26694;&#26550;&#29992;&#20110;&#35780;&#20272;&#20154;&#31867;&#26159;&#21542;&#36890;&#36807;&#20351;&#29992;AI&#21487;&#20197;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#65292;&#22312;&#21333;&#30450;&#23454;&#39564;&#35774;&#35745;&#20013;&#27604;&#36739;&#20102;&#19977;&#31181;&#20915;&#31574;&#31995;&#32479;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#24403;&#20170;&#31038;&#20250;&#21464;&#24471;&#26080;&#22788;&#19981;&#22312;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#23588;&#20854;&#26159;&#24403;&#21033;&#30410;&#39640;&#26114;&#26102;&#65292;&#20154;&#31867;&#20173;&#28982;&#20316;&#20986;&#26368;&#32456;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#20851;&#38190;&#38382;&#39064;&#26159;AI&#26159;&#21542;&#26377;&#21161;&#20110;&#20154;&#31867;&#27604;&#21333;&#29420;&#30340;&#20154;&#31867;&#25110;&#21333;&#29420;&#30340;AI&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#39564;&#24615;&#22320;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#22522;&#20934;&#28508;&#22312;&#32467;&#26524;&#30340;&#26631;&#20934;&#20998;&#31867;&#25351;&#26631;&#27979;&#37327;&#20915;&#31574;&#32773;&#20570;&#20986;&#27491;&#30830;&#20915;&#31574;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#21333;&#30450;&#23454;&#39564;&#35774;&#35745;&#65292;&#22312;&#36825;&#20010;&#35774;&#35745;&#20013;&#65292;&#25552;&#20379;AI&#29983;&#25104;&#30340;&#24314;&#35758;&#22312;&#19981;&#21516;&#26696;&#20363;&#20013;&#34987;&#38543;&#26426;&#20998;&#37197;&#32473;&#26368;&#32456;&#20915;&#31574;&#30340;&#20154;&#31867;&#12290;&#22312;&#36825;&#31181;&#23454;&#39564;&#35774;&#35745;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#27604;&#36739;&#19977;&#31181;&#26367;&#20195;&#20915;&#31574;&#31995;&#32479;&#30340;&#24615;&#33021;--&#20165;&#20154;&#31867;&#12289;&#20154;&#31867;&#19982;AI&#12289;&#20165;AI&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12108v1 Announce Type: new  Abstract: The use of Artificial Intelligence (AI) based on data-driven algorithms has become ubiquitous in today's society. Yet, in many cases and especially when stakes are high, humans still make final decisions. The critical question, therefore, is whether AI helps humans make better decisions as compared to a human alone or AI an alone. We introduce a new methodological framework that can be used to answer experimentally this question with no additional assumptions. We measure a decision maker's ability to make correct decisions using standard classification metrics based on the baseline potential outcome. We consider a single-blinded experimental design, in which the provision of AI-generated recommendations is randomized across cases with a human making final decisions. Under this experimental design, we show how to compare the performance of three alternative decision-making systems--human-alone, human-with-AI, and AI-alone. We apply the pr
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#22522;&#20110;&#35821;&#35328;&#30340;&#35760;&#24518;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#20854;&#23545;&#27169;&#26865;&#20004;&#21487;&#21477;&#23376;&#30340;&#22788;&#29702;&#33021;&#21147;&#22686;&#36827;&#20102;&#23545;&#20154;&#31867;&#35748;&#30693;&#26426;&#21046;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.05152</link><description>&lt;p&gt;
&#26397;&#21521;&#26426;&#22120;&#24515;&#29702;&#23398;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#20154;&#31867;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Towards a Psychology of Machines: Large Language Models Predict Human Memory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05152
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#22522;&#20110;&#35821;&#35328;&#30340;&#35760;&#24518;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#20854;&#23545;&#27169;&#26865;&#20004;&#21487;&#21477;&#23376;&#30340;&#22788;&#29702;&#33021;&#21147;&#22686;&#36827;&#20102;&#23545;&#20154;&#31867;&#35748;&#30693;&#26426;&#21046;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#65292;&#23613;&#31649;&#32570;&#20047;&#20154;&#31867;&#35748;&#30693;&#22522;&#30784;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#38500;&#20102;&#31616;&#21333;&#27169;&#20223;&#20154;&#31867;&#35821;&#35328;&#27169;&#24335;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#21542;&#25552;&#20379;&#20851;&#20110;&#20154;&#31867;&#35748;&#30693;&#26426;&#21046;&#30340;&#27934;&#35265;&#65311;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#22312;&#39044;&#27979;&#22522;&#20110;&#35821;&#35328;&#30340;&#35760;&#24518;&#20219;&#21153;&#20013;&#20154;&#31867;&#34920;&#29616;&#30340;&#33021;&#21147;&#12290;&#22522;&#20110;&#25991;&#26412;&#29702;&#35299;&#29702;&#35770;&#65292;&#25105;&#20204;&#20551;&#35774;&#35782;&#21035;&#27169;&#26865;&#20004;&#21487;&#30340;&#21477;&#23376;&#65288;&#20363;&#22914;&#65292;&#8220;&#22240;&#20026;&#27604;&#23572;&#21917;&#37202;&#65292;&#25152;&#20197;&#37202;&#20174;&#26410;&#30041;&#22312;&#25151;&#23376;&#37324;&#8221;&#65289;&#22312;&#21069;&#38754;&#25552;&#20379;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#20250;&#24471;&#21040;&#20419;&#36827;&#12290;&#21442;&#19982;&#32773;&#65292;&#26080;&#35770;&#26159;&#20154;&#31867;&#36824;&#26159;ChatGPT&#65292;&#37117;&#34987;&#21576;&#29616;&#25104;&#23545;&#30340;&#21477;&#23376;&#12290;&#31532;&#20108;&#20010;&#21477;&#23376;&#24635;&#26159;&#19968;&#20010;&#26088;&#22312;&#22266;&#26377;&#22320;&#27169;&#26865;&#20004;&#21487;&#30340;&#33457;&#22253;&#36335;&#24452;&#21477;&#65292;&#32780;&#31532;&#19968;&#20010;&#21477;&#23376;&#21017;&#25552;&#20379;&#20102;&#21512;&#36866;&#30340;&#65288;&#20363;&#22914;&#65292;&#8220;&#27604;&#23572;&#24739;&#26377;&#24930;&#24615;&#37202;&#31934;&#20013;&#27602;&#8221;&#65289;&#25110;&#19981;&#21512;&#36866;&#30340;&#19978;&#19979;&#25991;&#65288;&#20363;&#22914;&#65292;&#8220;&#27604;&#23572;&#21916;&#27426;&#25171;&#39640;&#23572;&#22827;&#8221;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05152v1 Announce Type: cross  Abstract: Large language models (LLMs) are demonstrating remarkable capabilities across various tasks despite lacking a foundation in human cognition. This raises the question: can these models, beyond simply mimicking human language patterns, offer insights into the mechanisms underlying human cognition? This study explores the ability of ChatGPT to predict human performance in a language-based memory task. Building upon theories of text comprehension, we hypothesize that recognizing ambiguous sentences (e.g., "Because Bill drinks wine is never kept in the house") is facilitated by preceding them with contextually relevant information. Participants, both human and ChatGPT, were presented with pairs of sentences. The second sentence was always a garden-path sentence designed to be inherently ambiguous, while the first sentence either provided a fitting (e.g., "Bill has chronic alcoholism") or an unfitting context (e.g., "Bill likes to play golf"
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;In-Dialogue Learning&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#35805;&#21382;&#21490;&#21051;&#30011;&#20010;&#20154;&#35774;&#26469;&#23436;&#25104;&#20010;&#24615;&#21270;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#65292;&#26080;&#38656;&#39044;&#23450;&#20041;&#20010;&#20154;&#36164;&#26009;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#26174;&#33879;&#25913;&#36827;&#23545;&#35805;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.03102</link><description>&lt;p&gt;
&#8220;&#22312;&#23545;&#35805;&#20013;&#23398;&#20064;&#8221;&#65306;&#36890;&#36807;&#23545;&#35805;&#20013;&#23398;&#20064;&#23454;&#29616;&#26080;&#38656;&#39044;&#23450;&#20041;&#20010;&#20154;&#36164;&#26009;&#30340;&#20010;&#24615;&#21270;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
"In Dialogues We Learn": Towards Personalized Dialogue Without Pre-defined Profiles through In-Dialogue Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03102
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;In-Dialogue Learning&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#35805;&#21382;&#21490;&#21051;&#30011;&#20010;&#20154;&#35774;&#26469;&#23436;&#25104;&#20010;&#24615;&#21270;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#65292;&#26080;&#38656;&#39044;&#23450;&#20041;&#20010;&#20154;&#36164;&#26009;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#26174;&#33879;&#25913;&#36827;&#23545;&#35805;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#23545;&#35805;&#31995;&#32479;&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#65292;&#22240;&#20854;&#33021;&#22815;&#29983;&#25104;&#19982;&#19981;&#21516;&#20154;&#35774;&#19968;&#33268;&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#39044;&#23450;&#20041;&#30340;&#20010;&#20154;&#36164;&#26009;&#65292;&#36825;&#19981;&#20165;&#32791;&#26102;&#19988;&#21171;&#21160;&#23494;&#38598;&#65292;&#36824;&#32570;&#20047;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;In-Dialogue Learning&#65288;IDL&#65289;&#65292;&#19968;&#31181;&#24494;&#35843;&#26694;&#26550;&#65292;&#22686;&#24378;&#20102;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#23545;&#35805;&#21382;&#21490;&#26469;&#21051;&#30011;&#20010;&#20154;&#35774;&#65292;&#20197;&#23436;&#25104;&#20010;&#24615;&#21270;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#39044;&#23450;&#20041;&#20010;&#20154;&#36164;&#26009;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;IDL&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;BLEU&#21644;ROUGE&#20998;&#25968;&#20998;&#21035;&#22686;&#21152;&#20102;&#39640;&#36798;200%&#21644;247%&#12290;&#27492;&#22806;&#65292;&#20154;&#24037;&#35780;&#20272;&#30340;&#32467;&#26524;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03102v1 Announce Type: cross  Abstract: Personalized dialogue systems have gained significant attention in recent years for their ability to generate responses in alignment with different personas. However, most existing approaches rely on pre-defined personal profiles, which are not only time-consuming and labor-intensive to create but also lack flexibility. We propose In-Dialogue Learning (IDL), a fine-tuning framework that enhances the ability of pre-trained large language models to leverage dialogue history to characterize persona for completing personalized dialogue generation tasks without pre-defined profiles. Our experiments on three datasets demonstrate that IDL brings substantial improvements, with BLEU and ROUGE scores increasing by up to 200% and 247%, respectively. Additionally, the results of human evaluations further validate the efficacy of our proposed method.
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#20223;&#30495;&#21040;&#30495;&#23454;&#19990;&#30028;&#30340;&#36716;&#31227;&#20026;&#35299;&#20915;&#29289;&#20307;&#25805;&#32437;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#21147;&#25903;&#25345;</title><link>https://arxiv.org/abs/2403.02338</link><description>&lt;p&gt;
&#29992;&#21452;&#25163;&#25197;&#24320;&#30422;&#23376;
&lt;/p&gt;
&lt;p&gt;
Twisting Lids Off with Two Hands
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02338
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#20223;&#30495;&#21040;&#30495;&#23454;&#19990;&#30028;&#30340;&#36716;&#31227;&#20026;&#35299;&#20915;&#29289;&#20307;&#25805;&#32437;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#21147;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#20004;&#21482;&#22810;&#25351;&#25163;&#33218;&#25805;&#32437;&#29289;&#20307;&#19968;&#30452;&#26159;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#19968;&#39033;&#38271;&#26399;&#25361;&#25112;&#65292;&#21407;&#22240;&#22312;&#20110;&#35768;&#22810;&#25805;&#32437;&#20219;&#21153;&#30340;&#20016;&#23500;&#25509;&#35302;&#24615;&#36136;&#20197;&#21450;&#21327;&#35843;&#39640;&#32500;&#24230;&#21452;&#25163;&#31995;&#32479;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20351;&#29992;&#20004;&#21482;&#25163;&#25197;&#24320;&#21508;&#31181;&#29942;&#23376;&#30422;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20986;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#20223;&#30495;&#20013;&#35757;&#32451;&#30340;&#31574;&#30053;&#21487;&#20197;&#26377;&#25928;&#22320;&#36716;&#31227;&#21040;&#29616;&#23454;&#19990;&#30028;&#12290;&#36890;&#36807;&#23545;&#29289;&#29702;&#24314;&#27169;&#12289;&#23454;&#26102;&#24863;&#30693;&#21644;&#22870;&#21169;&#35774;&#35745;&#30340;&#26032;&#24037;&#31243;&#35265;&#35299;&#65292;&#35813;&#31574;&#30053;&#23637;&#31034;&#20102;&#19968;&#33324;&#21270;&#33021;&#21147;&#65292;&#33021;&#22815;&#36143;&#31359;&#21508;&#31181;&#30475;&#19981;&#35265;&#30340;&#29289;&#20307;&#65292;&#23637;&#31034;&#20986;&#21160;&#24577;&#21644;&#28789;&#24039;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#35777;&#26126;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#20223;&#30495;&#21040;&#30495;&#23454;&#19990;&#30028;&#30340;&#36716;&#31227;&#20173;&#28982;&#26159;&#35299;&#20915;&#21069;&#25152;&#26410;&#26377;&#22797;&#26434;&#38382;&#39064;&#30340;&#25805;&#32437;&#38382;&#39064;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02338v1 Announce Type: cross  Abstract: Manipulating objects with two multi-fingered hands has been a long-standing challenge in robotics, attributed to the contact-rich nature of many manipulation tasks and the complexity inherent in coordinating a high-dimensional bimanual system. In this work, we consider the problem of twisting lids of various bottle-like objects with two hands, and demonstrate that policies trained in simulation using deep reinforcement learning can be effectively transferred to the real world. With novel engineering insights into physical modeling, real-time perception, and reward design, the policy demonstrates generalization capabilities across a diverse set of unseen objects, showcasing dynamic and dexterous behaviors. Our findings serve as compelling evidence that deep reinforcement learning combined with sim-to-real transfer remains a promising approach for addressing manipulation problems of unprecedented complexity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#38543;&#26426;&#36807;&#31243;&#20013;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#31614;&#21517;&#26680;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#65292;&#23454;&#29616;&#20102;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#25512;&#26029;&#65292;&#20197;&#21450;&#24320;&#21457;&#20102;&#32422;&#26463;&#26465;&#20214;&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#29992;&#20110;&#24674;&#22797;&#25972;&#20010;&#26377;&#21521;&#22270;&#12290;</title><link>https://arxiv.org/abs/2402.18477</link><description>&lt;p&gt;
&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#30340;&#31614;&#21517;&#26680;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#29992;&#20110;&#38543;&#26426;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Signature Kernel Conditional Independence Tests in Causal Discovery for Stochastic Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#38543;&#26426;&#36807;&#31243;&#20013;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#31614;&#21517;&#26680;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#65292;&#23454;&#29616;&#20102;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#25512;&#26029;&#65292;&#20197;&#21450;&#24320;&#21457;&#20102;&#32422;&#26463;&#26465;&#20214;&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#29992;&#20110;&#24674;&#22797;&#25972;&#20010;&#26377;&#21521;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#25512;&#26029;&#38543;&#26426;&#21160;&#21147;&#31995;&#32479;&#32972;&#21518;&#30340;&#22240;&#26524;&#32467;&#26500;&#22312;&#31185;&#23398;&#12289;&#20581;&#24247;&#21644;&#37329;&#34701;&#31561;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#26368;&#36817;&#31614;&#21517;&#26680;&#25216;&#26415;&#30340;&#36827;&#23637;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#26680;&#30340;&#8220;&#36335;&#24452;&#31354;&#38388;&#8221;&#19978;&#26465;&#20214;&#29420;&#31435;&#24615;&#65288;CI&#65289;&#27979;&#35797;&#65292;&#29992;&#20110;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#36335;&#24452;&#31354;&#38388;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;CI&#27979;&#35797;&#34920;&#29616;&#20986;&#20005;&#26684;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20026;&#38750;&#24490;&#29615;&#38543;&#26426;&#21160;&#21147;&#31995;&#32479;&#24320;&#21457;&#20102;&#22522;&#20110;&#32422;&#26463;&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#65292;&#21033;&#29992;&#26102;&#38388;&#20449;&#24687;&#26469;&#24674;&#22797;&#25972;&#20010;&#26377;&#21521;&#22270;&#12290;&#22312;&#20551;&#35774;&#24544;&#23454;&#24615;&#21644;CI&#39044;&#35328;&#26426;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#23436;&#22791;&#19988;&#27491;&#30830;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18477v1 Announce Type: cross  Abstract: Inferring the causal structure underlying stochastic dynamical systems from observational data holds great promise in domains ranging from science and health to finance. Such processes can often be accurately modeled via stochastic differential equations (SDEs), which naturally imply causal relationships via "which variables enter the differential of which other variables". In this paper, we develop a kernel-based test of conditional independence (CI) on "path-space" -- solutions to SDEs -- by leveraging recent advances in signature kernels. We demonstrate strictly superior performance of our proposed CI test compared to existing approaches on path-space. Then, we develop constraint-based causal discovery algorithms for acyclic stochastic dynamical systems (allowing for loops) that leverage temporal information to recover the entire directed graph. Assuming faithfulness and a CI oracle, our algorithm is sound and complete. We empirical
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24120;&#35782;&#25512;&#29702;&#20013;&#34920;&#29616;&#20986;&#39640;&#27700;&#24179;&#30340;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#20449;&#24687;&#20002;&#22833;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26032;&#26041;&#27861;RIDERS&#26469;&#35299;&#37322;&#21644;&#20943;&#36731;&#26377;&#23475;CoT&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.18344</link><description>&lt;p&gt;
&#19987;&#27880;&#20110;&#20320;&#30340;&#38382;&#39064;&#65281;&#35299;&#37322;&#21644;&#20943;&#36731;&#24120;&#35782;&#25512;&#29702;&#20013;&#30340;&#26377;&#23475;CoT&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18344
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24120;&#35782;&#25512;&#29702;&#20013;&#34920;&#29616;&#20986;&#39640;&#27700;&#24179;&#30340;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#20449;&#24687;&#20002;&#22833;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26032;&#26041;&#27861;RIDERS&#26469;&#35299;&#37322;&#21644;&#20943;&#36731;&#26377;&#23475;CoT&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#39640;&#27700;&#24179;&#30340;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#36890;&#36807;Chain-of-Thought&#65288;CoT&#65289;&#31561;&#22686;&#24378;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#31867;&#20284;CoT&#30340;&#26041;&#27861;&#23548;&#33268;&#20102;&#21407;&#26412;&#27491;&#30830;&#30340;&#31572;&#26696;&#21464;&#24471;&#38169;&#35823;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#20854;&#23450;&#20041;&#20026;&#26377;&#23475;&#30340;CoT&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#37322;&#21644;&#20943;&#36731;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#23646;&#24615;&#36319;&#36394;&#21644;&#22240;&#26524;&#36319;&#36394;&#26041;&#27861;&#26469;&#25506;&#31350;LLM&#22312;CoT&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#20869;&#37096;&#24037;&#20316;&#26426;&#21046;&#12290;&#36890;&#36807;&#27604;&#36739;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27169;&#22411;&#22312;&#29983;&#25104;&#25512;&#29702;&#25110;&#31572;&#26696;&#26102;&#23384;&#22312;&#26469;&#33258;&#38382;&#39064;&#30340;&#20449;&#24687;&#20002;&#22833;&#29616;&#35937;&#22312;&#27973;&#23618;&#27880;&#24847;&#21147;&#23618;&#20013;&#12290;&#22522;&#20110;&#25506;&#31350;&#32467;&#26524;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;RIDERS&#65288;Residual decodIng and sERial-position Swap&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#35299;&#30721;&#21644;&#24207;&#21015;&#20301;&#32622;&#30340;&#35282;&#24230;&#34917;&#20607;&#27169;&#22411;&#20013;&#30340;&#20449;&#24687;&#20111;&#32570;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#24120;&#35782;&#25512;&#29702;&#22522;&#20934;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18344v1 Announce Type: new  Abstract: Large language models exhibit high-level commonsense reasoning abilities, especially with enhancement methods like Chain-of-Thought (CoT). However, we find these CoT-like methods lead to a considerable number of originally correct answers turning wrong, which we define as the Toxic CoT problem. To interpret and mitigate this problem, we first utilize attribution tracing and causal tracing methods to probe the internal working mechanism of the LLM during CoT reasoning. Through comparisons, we prove that the model exhibits information loss from the question over the shallow attention layers when generating rationales or answers. Based on the probing findings, we design a novel method called RIDERS (Residual decodIng and sERial-position Swap), which compensates for the information deficit in the model from both decoding and serial-position perspectives. Through extensive experiments on multiple commonsense reasoning benchmarks, we validate 
&lt;/p&gt;</description></item><item><title>OmniArch&#36890;&#36807;&#22810;&#29289;&#29702;&#23398;&#26102;&#31354;&#25968;&#25454;&#22788;&#29702;&#12289;&#21487;&#25193;&#23637;&#30340;&#33258;&#22238;&#24402;&#20219;&#21153;&#21644;&#29289;&#29702;&#20449;&#24687;&#22686;&#24378;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#31185;&#23398;&#35745;&#31639;&#39046;&#22495;&#26500;&#24314;&#28789;&#27963;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#22312;&#24615;&#33021;&#12289;&#36866;&#24212;&#24615;&#21644;&#36870;&#38382;&#39064;&#27714;&#35299;&#26041;&#38754;&#21462;&#24471;&#31361;&#30772;&#65292;&#23637;&#29616;&#20102;AI&#23545;&#31185;&#23398;&#35745;&#31639;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.16014</link><description>&lt;p&gt;
&#22312;&#31185;&#23398;&#35745;&#31639;&#35268;&#27169;&#19978;&#26500;&#24314;&#28789;&#27963;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Building Flexible Machine Learning Models for Scientific Computing at Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16014
&lt;/p&gt;
&lt;p&gt;
OmniArch&#36890;&#36807;&#22810;&#29289;&#29702;&#23398;&#26102;&#31354;&#25968;&#25454;&#22788;&#29702;&#12289;&#21487;&#25193;&#23637;&#30340;&#33258;&#22238;&#24402;&#20219;&#21153;&#21644;&#29289;&#29702;&#20449;&#24687;&#22686;&#24378;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#31185;&#23398;&#35745;&#31639;&#39046;&#22495;&#26500;&#24314;&#28789;&#27963;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#22312;&#24615;&#33021;&#12289;&#36866;&#24212;&#24615;&#21644;&#36870;&#38382;&#39064;&#27714;&#35299;&#26041;&#38754;&#21462;&#24471;&#31361;&#30772;&#65292;&#23637;&#29616;&#20102;AI&#23545;&#31185;&#23398;&#35745;&#31639;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16014v1
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16014v1 Announce Type: cross  Abstract: Foundation models have revolutionized knowledge acquisition across domains, and our study introduces OmniArch, a paradigm-shifting approach designed for building foundation models in multi-physics scientific computing. OmniArch's pre-training involves a versatile pipeline that processes multi-physics spatio-temporal data, casting forward problem learning into scalable auto-regressive tasks, while our novel Physics-Informed Reinforcement Learning (PIRL) technique during fine-tuning ensures alignment with physical laws. Pre-trained on the comprehensive PDEBench dataset, OmniArch not only sets new performance benchmarks for 1D, 2D and 3D PDEs but also demonstrates exceptional adaptability to new physics via few-shot and zero-shot learning approaches. The model's representations further extend to inverse problem-solving, highlighting the transformative potential of AI-enabled Scientific Computing(AI4SC) foundation models for engineering ap
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35780;&#20272;&#22120;&#20013;&#30340;&#20284;&#28982;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32531;&#35299;&#36825;&#31181;&#20559;&#24046;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15987</link><description>&lt;p&gt;
&#22522;&#20110;&#20284;&#28982;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20559;&#24046;&#30340;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Likelihood-based Mitigation of Evaluation Bias in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15987
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35780;&#20272;&#22120;&#20013;&#30340;&#20284;&#28982;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32531;&#35299;&#36825;&#31181;&#20559;&#24046;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#34987;&#24191;&#27867;&#29992;&#20110;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#30340;&#33258;&#21160;&#21270;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#20284;&#28982;&#20316;&#20026;&#34913;&#37327;LLM&#23545;&#21477;&#23376;&#21487;&#20449;&#24230;&#30340;&#25351;&#26631;&#65292;&#21487;&#33021;&#20250;&#22240;&#21477;&#23376;&#34920;&#38754;&#24046;&#24322;&#65288;&#22914;&#35789;&#24207;&#21644;&#21477;&#23376;&#32467;&#26500;&#65289;&#32780;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#22914;&#26524;&#23558;LLMs&#29992;&#20110;&#35780;&#20272;&#65292;&#21487;&#33021;&#23384;&#22312;&#20284;&#28982;&#20559;&#24046;&#65306;&#23427;&#20204;&#21487;&#33021;&#20250;&#39640;&#20272;&#20855;&#26377;&#36739;&#39640;&#20284;&#28982;&#24615;&#30340;&#21477;&#23376;&#65292;&#32780;&#20302;&#20272;&#20855;&#26377;&#36739;&#20302;&#20284;&#28982;&#24615;&#30340;&#21477;&#23376;&#12290;&#26412;&#25991;&#23545;LLM&#35780;&#20272;&#22120;&#20013;&#20284;&#28982;&#20559;&#24046;&#30340;&#23384;&#22312;&#21644;&#24433;&#21709;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32531;&#35299;&#20284;&#28982;&#20559;&#24046;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#39640;&#24230;&#20559;&#32622;&#30340;&#23454;&#20363;&#20316;&#20026;&#23569;&#26679;&#26412;&#31034;&#20363;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#35780;&#20272;&#25968;&#25454;&#21040;&#25991;&#26412;&#21644;&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;&#20219;&#21153;&#26102;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#27979;&#35797;&#30340;&#20960;&#31181;LLMs&#26174;&#31034;&#20986;&#20284;&#28982;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#20943;&#36731;&#20102;&#36825;&#31181;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15987v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are widely used to evaluate natural language generation tasks as automated metrics. However, the likelihood, a measure of LLM's plausibility for a sentence, can vary due to superficial differences in sentences, such as word order and sentence structure. It is therefore possible that there might be a likelihood bias if LLMs are used for evaluation: they might overrate sentences with higher likelihoods while underrating those with lower likelihoods. In this paper, we investigate the presence and impact of likelihood bias in LLM-based evaluators. We also propose a method to mitigate the likelihood bias. Our method utilizes highly biased instances as few-shot examples for in-context learning. Our experiments in evaluating the data-to-text and grammatical error correction tasks reveal that several LLMs we test display a likelihood bias. Furthermore, our proposed method successfully mitigates this bias, also impr
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;GPT-4&#29983;&#25104;&#25551;&#36848;&#24615;&#25552;&#31034;&#65292;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;&#21307;&#23398;&#22270;&#20687;&#19978;&#30340;SAM&#38646;&#26679;&#26412;&#20998;&#21106;&#24615;&#33021;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#12290;</title><link>https://arxiv.org/abs/2402.15759</link><description>&lt;p&gt;
&#20351;&#29992;GPT-4&#29983;&#25104;&#25551;&#36848;&#24615;&#25552;&#31034;&#25552;&#39640;&#22810;&#27169;&#24577;&#21307;&#23398;&#22270;&#20687;&#19978;&#30340;SAM&#38646;&#26679;&#26412;&#24615;&#33021;&#32780;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
Increasing SAM Zero-Shot Performance on Multimodal Medical Images Using GPT-4 Generated Descriptive Prompts Without Human Annotation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15759
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;GPT-4&#29983;&#25104;&#25551;&#36848;&#24615;&#25552;&#31034;&#65292;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;&#21307;&#23398;&#22270;&#20687;&#19978;&#30340;SAM&#38646;&#26679;&#26412;&#20998;&#21106;&#24615;&#33021;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#27169;&#24577;&#21307;&#23398;&#22270;&#20687;&#38646;&#26679;&#26412;&#20998;&#21106;&#31639;&#27861;&#65292;&#21629;&#21517;&#20026;&#25991;&#26412;-&#35270;&#35273;-&#25552;&#31034;SAM&#65288;TV-SAM&#65289;&#65292;&#26080;&#38656;&#20219;&#20309;&#25163;&#21160;&#26631;&#27880;&#12290;TV-SAM&#34701;&#21512;&#24182;&#25972;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#12289;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;GLIP&#21644;&#8220;Segment Anything Model&#8221;&#65288;SAM&#65289;&#65292;&#20174;&#21307;&#23398;&#22270;&#20687;&#20013;&#33258;&#21160;&#29983;&#25104;&#25551;&#36848;&#24615;&#25991;&#26412;&#25552;&#31034;&#21644;&#35270;&#35273;&#36793;&#30028;&#26694;&#25552;&#31034;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;SAM&#29992;&#20110;&#38646;&#26679;&#26412;&#20998;&#21106;&#12290;&#22312;&#19971;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#28085;&#30422;&#20843;&#31181;&#25104;&#20687;&#27169;&#24335;&#65292;&#35777;&#26126;TV-SAM&#21487;&#20197;&#26377;&#25928;&#22320;&#36328;&#21508;&#31181;&#27169;&#24335;&#20998;&#21106;&#26410;&#35265;&#36807;&#30340;&#30446;&#26631;&#32780;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#65292;&#26126;&#26174;&#20248;&#20110;SAM AUTO&#21644;GSAM, &#19982;&#37329;&#26631;&#20934;&#36793;&#30028;&#26694;&#25552;&#31034;&#30340;SAM BBOX&#24615;&#33021;&#22522;&#26412;&#21305;&#25932;&#65292;&#24182;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#65288;&#22914;ISIC&#21644;WBC&#65289;&#19978;&#36229;&#36234;&#20102;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;TV-SAM&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#22810;&#27169;&#24577;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15759v1 Announce Type: cross  Abstract: This study develops and evaluates a novel multimodal medical image zero-shot segmentation algorithm named Text-Visual-Prompt SAM (TV-SAM) without any manual annotations. TV-SAM incorporates and integrates large language model GPT-4, Vision Language Model GLIP, and Segment Anything Model (SAM), to autonomously generate descriptive text prompts and visual bounding box prompts from medical images, thereby enhancing SAM for zero-shot segmentation. Comprehensive evaluations are implemented on seven public datasets encompassing eight imaging modalities to demonstrate that TV-SAM can effectively segment unseen targets across various modalities without additional training, significantly outperforming SAM AUTO and GSAM, closely matching the performance of SAM BBOX with gold standard bounding box prompts, and surpassing the state-of-the-art on specific datasets like ISIC and WBC. The study indicates that TV-SAM serves as an effective multimodal 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;PANDA&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#26356;&#31934;&#30830;&#30340;&#31572;&#26696;&#27491;&#30830;&#24615;&#35780;&#27979;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#33258;&#21160;&#35780;&#20272;&#38382;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.11161</link><description>&lt;p&gt;
PANDA&#65288;Pedantic ANswer-correctness Determination and Adjudication&#65289;&#65306;&#25913;&#36827;&#38382;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#30340;&#33258;&#21160;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
PANDA (Pedantic ANswer-correctness Determination and Adjudication):Improving Automatic Evaluation for Question Answering and Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11161
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PANDA&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#26356;&#31934;&#30830;&#30340;&#31572;&#26696;&#27491;&#30830;&#24615;&#35780;&#27979;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#33258;&#21160;&#35780;&#20272;&#38382;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#31572;&#65288;QA&#65289;&#21482;&#26377;&#22312;&#25105;&#20204;&#30693;&#36947;&#31572;&#26696;&#26159;&#21542;&#27491;&#30830;&#26102;&#25165;&#33021;&#21462;&#24471;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#35768;&#22810;&#26368;&#20855;&#25361;&#25112;&#24615;&#21644;&#26377;&#36259;&#30340;QA&#31034;&#20363;&#65292;&#24403;&#21069;&#30340;&#31572;&#26696;&#27491;&#30830;&#24615;&#65288;AC&#65289;&#25351;&#26631;&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#65292;&#29305;&#21035;&#26159;&#26469;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20887;&#38271;&#12289;&#33258;&#30001;&#26684;&#24335;&#31572;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#25361;&#25112;&#65306;&#32570;&#20047;&#25968;&#25454;&#21644;&#27169;&#22411;&#36807;&#22823;&#12290;&#22522;&#20110;LLM&#30340;&#35780;&#20998;&#22120;&#19982;&#20154;&#31867;&#26356;&#22909;&#22320;&#30456;&#20851;&#65292;&#20294;&#36825;&#39033;&#26114;&#36149;&#30340;&#20219;&#21153;&#20165;&#22312;&#26377;&#38480;&#30340;QA&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#28165;&#26224;&#30340;&#25351;&#21335;&#26469;&#35780;&#20272;&#20174;&#20154;&#31867;QA&#27604;&#36187;&#20013;&#37319;&#32435;&#30340;&#26426;&#22120;QA&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#31934;&#30830;&#30340;&#31572;&#26696;&#27491;&#30830;&#24615;&#30830;&#23450;&#21644;&#35009;&#20915;&#65288;Precise ANswer correctness Determination and Adjudication&#65292;PANDA&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#23567;&#24039;&#12289;&#39640;&#25928;&#12289;&#30830;&#23450;&#24615;&#30340;AC&#20998;&#31867;&#22120;&#65288;812 KB&#65289;&#65292;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#31572;&#26696;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11161v1 Announce Type: cross  Abstract: Question answering (QA) can only make progress if we know if an answer is correct, but for many of the most challenging and interesting QA examples, current answer correctness (AC) metrics do not align with human judgments, particularly verbose, free form answers from large language models (LLM). There are two challenges: a lack of data and that models are too big. LLM based scorers correlate better with humans, but this expensive task has only been tested on limited QA datasets. We rectify these issues by providing clear guidelines for evaluating machine QA adopted from human QA contests. We also introduce Precise ANswer correctness Determination and Adjudication (PANDA), a small, efficient, deterministic AC classifier (812 KB) that more accurately evaluates answer correctness.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#25209;&#21028;&#24615;&#35780;&#20272;&#30740;&#31350;&#20102;23&#20010;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#21253;&#25324;&#20559;&#35265;&#12289;&#30495;&#23454;&#25512;&#29702;&#34913;&#37327;&#22256;&#38590;&#12289;&#23454;&#29616;&#19981;&#19968;&#33268;&#24615;&#31561;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#38656;&#35201;&#26631;&#20934;&#21270;&#26041;&#27861;&#12289;&#30417;&#31649;&#30830;&#23450;&#24615;&#21644;&#20262;&#29702;&#25351;&#21335;&#12290;</title><link>https://arxiv.org/abs/2402.09880</link><description>&lt;p&gt;
&#22312;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#30340;&#19981;&#36275;&#20043;&#22788;
&lt;/p&gt;
&lt;p&gt;
Inadequacies of Large Language Model Benchmarks in the Era of Generative Artificial Intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09880
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#25209;&#21028;&#24615;&#35780;&#20272;&#30740;&#31350;&#20102;23&#20010;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#21253;&#25324;&#20559;&#35265;&#12289;&#30495;&#23454;&#25512;&#29702;&#34913;&#37327;&#22256;&#38590;&#12289;&#23454;&#29616;&#19981;&#19968;&#33268;&#24615;&#31561;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#38656;&#35201;&#26631;&#20934;&#21270;&#26041;&#27861;&#12289;&#30417;&#31649;&#30830;&#23450;&#24615;&#21644;&#20262;&#29702;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38543;&#30528;&#20854;&#26032;&#20852;&#33021;&#21147;&#30340;&#24555;&#36895;&#23835;&#36215;&#65292;&#24341;&#21457;&#20102;&#20844;&#20247;&#30340;&#22909;&#22855;&#24515;&#65292;&#20197;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#30340;LLMs&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#20182;&#20204;&#30340;LLM&#22522;&#20934;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#36825;&#20123;&#22522;&#20934;&#30340;&#21021;&#27493;&#19981;&#36275;&#65292;&#24320;&#22987;&#20102;&#19968;&#39033;&#30740;&#31350;&#65292;&#36890;&#36807;&#20154;&#20204;&#12289;&#36807;&#31243;&#21644;&#25216;&#26415;&#30340;&#35270;&#35282;&#65292;&#20197;&#21151;&#33021;&#21644;&#23433;&#20840;&#20004;&#22823;&#25903;&#26609;&#20026;&#22522;&#30784;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#26032;&#39062;&#32479;&#19968;&#35780;&#20272;&#26694;&#26550;&#23545;23&#20010;&#26368;&#20808;&#36827;&#30340;LLM&#22522;&#20934;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#19968;&#20123;&#37325;&#22823;&#38480;&#21046;&#65292;&#21253;&#25324;&#20559;&#35265;&#12289;&#27979;&#37327;&#30495;&#23454;&#25512;&#29702;&#30340;&#22256;&#38590;&#12289;&#36866;&#24212;&#24615;&#12289;&#23454;&#29616;&#19981;&#19968;&#33268;&#24615;&#12289;&#25552;&#31034;&#24037;&#31243;&#22797;&#26434;&#24615;&#12289;&#35780;&#20272;&#32773;&#22810;&#26679;&#24615;&#20197;&#21450;&#22312;&#19968;&#27425;&#32508;&#21512;&#35780;&#20272;&#20013;&#24573;&#35270;&#20102;&#25991;&#21270;&#21644;&#24847;&#35782;&#24418;&#24577;&#35268;&#33539;&#12290;&#25105;&#20204;&#30340;&#35752;&#35770;&#24378;&#35843;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#65292;&#36843;&#20999;&#38656;&#35201;&#26631;&#20934;&#21270;&#26041;&#27861;&#12289;&#30417;&#31649;&#30830;&#23450;&#24615;&#21644;&#20262;&#29702;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09880v1 Announce Type: new  Abstract: The rapid rise in popularity of Large Language Models (LLMs) with emerging capabilities has spurred public curiosity to evaluate and compare different LLMs, leading many researchers to propose their LLM benchmarks. Noticing preliminary inadequacies in those benchmarks, we embarked on a study to critically assess 23 state-of-the-art LLM benchmarks, using our novel unified evaluation framework through the lenses of people, process, and technology, under the pillars of functionality and security. Our research uncovered significant limitations, including biases, difficulties in measuring genuine reasoning, adaptability, implementation inconsistencies, prompt engineering complexity, evaluator diversity, and the overlooking of cultural and ideological norms in one comprehensive assessment. Our discussions emphasized the urgent need for standardized methodologies, regulatory certainties, and ethical guidelines in light of Artificial Intelligenc
&lt;/p&gt;</description></item><item><title>Shadowcast&#26159;&#19968;&#31181;&#38544;&#31192;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#20266;&#35013;&#25104;&#33391;&#24615;&#22270;&#20687;&#21644;&#21305;&#37197;&#25991;&#26412;&#26469;&#25805;&#32437;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#21709;&#24212;&#12290;&#23427;&#21253;&#25324;&#26631;&#31614;&#25915;&#20987;&#21644;&#35828;&#26381;&#25915;&#20987;&#65292;&#21487;&#20197;&#28151;&#28102;&#31867;&#21035;&#26631;&#31614;&#24182;&#32534;&#20889;&#26377;&#35828;&#26381;&#21147;&#30340;&#25551;&#36848;&#12290;&#20351;&#29992;&#20165;50&#20010;&#27602;&#26679;&#26412;&#65292;Shadowcast&#33021;&#22815;&#39640;&#25928;&#23454;&#29616;&#25915;&#20987;&#32773;&#30340;&#24847;&#22270;&#12290;</title><link>https://arxiv.org/abs/2402.06659</link><description>&lt;p&gt;
Shadowcast: &#38544;&#31192;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#23545;&#25239;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06659
&lt;/p&gt;
&lt;p&gt;
Shadowcast&#26159;&#19968;&#31181;&#38544;&#31192;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#20266;&#35013;&#25104;&#33391;&#24615;&#22270;&#20687;&#21644;&#21305;&#37197;&#25991;&#26412;&#26469;&#25805;&#32437;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#21709;&#24212;&#12290;&#23427;&#21253;&#25324;&#26631;&#31614;&#25915;&#20987;&#21644;&#35828;&#26381;&#25915;&#20987;&#65292;&#21487;&#20197;&#28151;&#28102;&#31867;&#21035;&#26631;&#31614;&#24182;&#32534;&#20889;&#26377;&#35828;&#26381;&#21147;&#30340;&#25551;&#36848;&#12290;&#20351;&#29992;&#20165;50&#20010;&#27602;&#26679;&#26412;&#65292;Shadowcast&#33021;&#22815;&#39640;&#25928;&#23454;&#29616;&#25915;&#20987;&#32773;&#30340;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#33021;&#22815;&#20174;&#35270;&#35273;&#36755;&#20837;&#20013;&#29983;&#25104;&#25991;&#26412;&#21709;&#24212;&#65292;&#28982;&#32780;&#23427;&#20204;&#30340;&#22810;&#21151;&#33021;&#24615;&#24102;&#26469;&#20102;&#37325;&#22823;&#30340;&#23433;&#20840;&#38544;&#24739;&#12290;&#26412;&#30740;&#31350;&#39318;&#27425;&#25581;&#31034;&#20102;VLM&#23545;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#26131;&#21463;&#24615;&#65292;&#36825;&#20123;&#25915;&#20987;&#21487;&#20197;&#25805;&#32437;&#23545;&#26080;&#23475;&#30340;&#26085;&#24120;&#25552;&#31034;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Shadowcast&#30340;&#38544;&#31192;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#26041;&#27861;&#65292;&#20854;&#20013;&#27602;&#26679;&#26412;&#22312;&#35270;&#35273;&#19978;&#19982;&#20855;&#26377;&#21305;&#37197;&#25991;&#26412;&#30340;&#33391;&#24615;&#22270;&#20687;&#38590;&#20197;&#21306;&#20998;&#12290;Shadowcast&#22312;&#20004;&#31181;&#25915;&#20987;&#31867;&#22411;&#20013;&#23637;&#31034;&#20986;&#20102;&#26377;&#25928;&#24615;&#12290;&#31532;&#19968;&#31181;&#26159;&#26631;&#31614;&#25915;&#20987;&#65292;&#20351;VLM&#35823;&#35782;&#21035;&#31867;&#21035;&#26631;&#31614;&#65292;&#20363;&#22914;&#28151;&#28102;&#21776;&#32435;&#24503;&#183;&#29305;&#26391;&#26222;&#21644;&#20052;&#183;&#25308;&#30331;&#31561;&#20154;&#12290;&#31532;&#20108;&#31181;&#26159;&#35828;&#26381;&#25915;&#20987;&#65292;&#21033;&#29992;VLM&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#26469;&#32534;&#20889;&#25925;&#20107;&#65292;&#20363;&#22914;&#36890;&#36807;&#26377;&#35828;&#26381;&#21147;&#21644;&#30475;&#20284;&#21512;&#29702;&#30340;&#25551;&#36848;&#23558;&#22403;&#22334;&#39135;&#21697;&#25551;&#32472;&#25104;&#20581;&#24247;&#39135;&#21697;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Shadowcast&#20351;&#29992;&#20165;50&#20010;&#27602;&#26679;&#26412;&#23601;&#33021;&#39640;&#24230;&#26377;&#25928;&#22320;&#23454;&#29616;&#25915;&#20987;&#32773;&#30340;&#24847;&#22270;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27602;&#26679;&#26412;&#20173;&#28982;&#20445;&#25345;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Models (VLMs) excel in generating textual responses from visual inputs, yet their versatility raises significant security concerns. This study takes the first step in exposing VLMs' susceptibility to data poisoning attacks that can manipulate responses to innocuous, everyday prompts. We introduce Shadowcast, a stealthy data poisoning attack method where poison samples are visually indistinguishable from benign images with matching texts. Shadowcast demonstrates effectiveness in two attack types. The first is Label Attack, tricking VLMs into misidentifying class labels, such as confusing Donald Trump for Joe Biden. The second is Persuasion Attack, which leverages VLMs' text generation capabilities to craft narratives, such as portraying junk food as health food, through persuasive and seemingly rational descriptions. We show that Shadowcast are highly effective in achieving attacker's intentions using as few as 50 poison samples. Moreover, these poison samples remain eff
&lt;/p&gt;</description></item><item><title>ViGoR&#36890;&#36807;&#32454;&#31890;&#24230;&#22870;&#21169;&#24314;&#27169;&#25552;&#39640;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#23545;&#25509;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#21644;&#33258;&#21160;&#21270;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#35270;&#35273;&#23545;&#25509;&#20013;&#30340;&#35823;&#24046;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06118</link><description>&lt;p&gt;
ViGoR&#65306;&#36890;&#36807;&#32454;&#31890;&#24230;&#22870;&#21169;&#24314;&#27169;&#25913;&#36827;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#23545;&#25509;
&lt;/p&gt;
&lt;p&gt;
ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06118
&lt;/p&gt;
&lt;p&gt;
ViGoR&#36890;&#36807;&#32454;&#31890;&#24230;&#22870;&#21169;&#24314;&#27169;&#25552;&#39640;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#23545;&#25509;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#21644;&#33258;&#21160;&#21270;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#35270;&#35273;&#23545;&#25509;&#20013;&#30340;&#35823;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#21644;&#24191;&#27867;&#30693;&#35782;&#19982;&#22270;&#20687;&#24863;&#30693;&#30456;&#32467;&#21512;&#65292;&#26368;&#36817;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#23637;&#31034;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#30340;&#25991;&#26412;&#24448;&#24448;&#22312;&#35270;&#35273;&#36755;&#20837;&#20013;&#23384;&#22312;&#19981;&#20934;&#30830;&#30340;&#23545;&#25509;&#65292;&#23548;&#33268;&#38169;&#35823;&#65292;&#22914;&#20135;&#29983;&#24187;&#35273;&#30340;&#19981;&#23384;&#22312;&#22330;&#26223;&#20803;&#32032;&#12289;&#36951;&#28431;&#37325;&#35201;&#30340;&#22330;&#26223;&#37096;&#20998;&#65292;&#20197;&#21450;&#25512;&#27979;&#23545;&#35937;&#20043;&#38388;&#30340;&#23646;&#24615;&#21644;&#20851;&#31995;&#26102;&#20986;&#29616;&#38169;&#35823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;ViGoR&#65288;&#36890;&#36807;&#32454;&#31890;&#24230;&#22870;&#21169;&#24314;&#27169;&#36827;&#34892;&#35270;&#35273;&#23545;&#25509;&#65289;&#65292;&#23427;&#21033;&#29992;&#32454;&#31890;&#24230;&#22870;&#21169;&#24314;&#27169;&#26469;&#26174;&#33879;&#25552;&#21319;&#22522;&#20110;&#39044;&#35757;&#32451;&#22522;&#32447;&#30340;LVLMs&#30340;&#35270;&#35273;&#23545;&#25509;&#33021;&#21147;&#12290;&#36825;&#31181;&#25913;&#36827;&#36890;&#36807;&#20351;&#29992;&#27604;&#23436;&#20840;&#30417;&#30563;&#26356;&#20415;&#23452;&#30340;&#20154;&#24037;&#35780;&#20272;&#21644;&#33258;&#21160;&#21270;&#26041;&#27861;&#39640;&#25928;&#23454;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#30340;&#22810;&#20010;&#25351;&#26631;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
By combining natural language understanding and the generation capabilities and breadth of knowledge of large language models with image perception, recent large vision language models (LVLMs) have shown unprecedented reasoning capabilities in the real world. However, the generated text often suffers from inaccurate grounding in the visual input, resulting in errors such as hallucinating nonexistent scene elements, missing significant parts of the scene, and inferring incorrect attributes and relationships between objects. To address these issues, we introduce a novel framework, ViGoR (Visual Grounding Through Fine-Grained Reward Modeling) that utilizes fine-grained reward modeling to significantly enhance the visual grounding of LVLMs over pre-trained baselines. This improvement is efficiently achieved using much cheaper human evaluations instead of full supervisions, as well as automated methods. We show the effectiveness of our approach through numerous metrics on several benchmarks
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26465;&#20214;&#21644;&#24773;&#24577;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#38500;&#20102;GPT-4&#22806;&#65292;&#20854;&#20182;&#27169;&#22411;&#22312;&#26465;&#20214;&#21477;&#26041;&#38754;&#23384;&#22312;&#22522;&#26412;&#38169;&#35823;&#65292;&#24182;&#19988;&#21363;&#20351;&#26159;GPT-4&#22312;&#28041;&#21450;&#35748;&#35782;&#24773;&#24577;&#30340;&#25512;&#29702;&#27169;&#24335;&#19978;&#20063;&#26174;&#31034;&#20986;&#36923;&#36753;&#19978;&#19981;&#19968;&#33268;&#30340;&#21028;&#26029;&#12290;</title><link>https://arxiv.org/abs/2401.17169</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26465;&#20214;&#21644;&#24773;&#24577;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Conditional and Modal Reasoning in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26465;&#20214;&#21644;&#24773;&#24577;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#38500;&#20102;GPT-4&#22806;&#65292;&#20854;&#20182;&#27169;&#22411;&#22312;&#26465;&#20214;&#21477;&#26041;&#38754;&#23384;&#22312;&#22522;&#26412;&#38169;&#35823;&#65292;&#24182;&#19988;&#21363;&#20351;&#26159;GPT-4&#22312;&#28041;&#21450;&#35748;&#35782;&#24773;&#24577;&#30340;&#25512;&#29702;&#27169;&#24335;&#19978;&#20063;&#26174;&#31034;&#20986;&#36923;&#36753;&#19978;&#19981;&#19968;&#33268;&#30340;&#21028;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#30740;&#31350;&#27491;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#35748;&#30693;&#31185;&#23398;&#39046;&#22495;&#19981;&#26029;&#22686;&#21152;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21313;&#20960;&#20010;LLM&#33021;&#21542;&#21306;&#20998;&#36923;&#36753;&#19978;&#27491;&#30830;&#30340;&#25512;&#35770;&#21644;&#36923;&#36753;&#19978;&#33618;&#35884;&#30340;&#25512;&#35770;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#28041;&#21450;&#26465;&#20214;&#21477;&#65288;&#20363;&#22914;&#65292;&#8220;&#22914;&#26524;&#23433;&#26377;&#19968;&#20010;&#30343;&#21518;&#65292;&#37027;&#20040;&#40077;&#21187;&#26377;&#19968;&#20010;J&#29260;&#8221;&#65289;&#21644;&#35748;&#35782;&#24773;&#24577;&#65288;&#20363;&#22914;&#65292;&#8220;&#23433;&#21487;&#33021;&#26377;&#19968;&#20010;A&#29260;&#8221;&#65292;&#8220;&#40077;&#21187;&#24517;&#39035;&#26377;&#19968;&#20010;K&#29260;&#8221;&#65289;&#30340;&#25512;&#29702;&#27169;&#24335;&#12290;&#36825;&#20123;&#25512;&#29702;&#27169;&#24335;&#23545;&#20110;&#36923;&#36753;&#23398;&#23478;&#12289;&#21746;&#23398;&#23478;&#21644;&#35821;&#35328;&#23398;&#23478;&#26469;&#35828;&#20855;&#26377;&#29305;&#27530;&#30340;&#20852;&#36259;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#22312;&#20154;&#31867;&#25512;&#29702;&#20013;&#25198;&#28436;&#19968;&#20010;&#26680;&#24515;&#35282;&#33394;&#12290;&#22240;&#27492;&#65292;&#35780;&#20272;LLM&#22312;&#36825;&#20123;&#25512;&#29702;&#27169;&#24335;&#19978;&#30340;&#34920;&#29616;&#19982;&#20154;&#31867;&#30340;&#25512;&#29702;&#33021;&#21147;&#26159;&#21542;&#30456;&#21305;&#37197;&#26159;&#38750;&#24120;&#30456;&#20851;&#30340;&#12290;&#22312;&#25105;&#20204;&#27979;&#35797;&#30340;LLM&#20013;&#65292;&#38500;&#20102;GPT-4&#65292;&#20854;&#20182;&#37117;&#24120;&#24120;&#22312;&#26465;&#20214;&#21477;&#26041;&#38754;&#29359;&#22522;&#26412;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#26159;GPT-4&#65292;&#22312;&#28041;&#21450;&#35748;&#35782;&#24773;&#24577;&#30340;&#25512;&#29702;&#27169;&#24335;&#19978;&#20063;&#26174;&#31034;&#20986;&#36923;&#36753;&#19978;&#19981;&#19968;&#33268;&#30340;&#21028;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
The reasoning abilities of large language models (LLMs) are the topic of a growing body of research in artificial intelligence and cognitive science. In this paper, we probe the extent to which a dozen LLMs are able to distinguish logically correct inferences from logically fallacious ones. We focus on inference patterns involving conditionals (e.g., 'If Ann has a queen, then Bob has a jack') and epistemic modals (e.g., 'Ann might have an ace', 'Bob must have a king'). These inference patterns have been of special interest to logicians, philosophers, and linguists, since they plausibly play a central role in human reasoning. Assessing LLMs on these inference patterns is thus highly relevant to the question of how much the reasoning abilities of LLMs match those of humans. Among the LLMs we tested, all but GPT-4 often make basic mistakes with conditionals. Moreover, even GPT-4 displays logically inconsistent judgments across inference patterns involving epistemic modals.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#30697;&#38453;&#29109;&#65292;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#35770;&#21644;&#20960;&#20309;&#21407;&#29702;&#30340;&#26032;&#22411;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25968;&#25454;&#21387;&#32553;&#33021;&#21147;&#12290;&#35813;&#25351;&#26631;&#21453;&#26144;&#20102;&#27169;&#22411;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#21644;&#28040;&#38500;&#19981;&#24517;&#35201;&#20803;&#32032;&#30340;&#33021;&#21147;&#65292;&#20026;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#22266;&#26377;&#33021;&#21147;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;&#22312;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#35774;&#32622;&#20013;&#37117;&#23637;&#31034;&#20102;&#20854;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17139</link><description>&lt;p&gt;
&#36890;&#36807;&#30697;&#38453;&#29109;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Evaluation via Matrix Entropy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#30697;&#38453;&#29109;&#65292;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#35770;&#21644;&#20960;&#20309;&#21407;&#29702;&#30340;&#26032;&#22411;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25968;&#25454;&#21387;&#32553;&#33021;&#21147;&#12290;&#35813;&#25351;&#26631;&#21453;&#26144;&#20102;&#27169;&#22411;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#21644;&#28040;&#38500;&#19981;&#24517;&#35201;&#20803;&#32032;&#30340;&#33021;&#21147;&#65292;&#20026;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#22266;&#26377;&#33021;&#21147;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;&#22312;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#35774;&#32622;&#20013;&#37117;&#23637;&#31034;&#20102;&#20854;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#23558;&#24378;&#22823;&#30340;&#33021;&#21147;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#39046;&#22495;&#65292;&#20351;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21457;&#29983;&#20102;&#38761;&#21629;&#12290;&#22240;&#27492;&#65292;&#20026;LLMs&#23450;&#20041;&#36866;&#24403;&#19988;&#22810;&#26679;&#21270;&#30340;&#35780;&#20272;&#25351;&#26631;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#30697;&#38453;&#29109;&#65292;&#19968;&#31181;&#26681;&#26893;&#20110;&#20449;&#24687;&#35770;&#21644;&#20960;&#20309;&#21407;&#29702;&#30340;&#26032;&#22411;&#25351;&#26631;&#65292;&#29992;&#20110;&#37327;&#21270;LLMs&#20013;&#30340;&#25968;&#25454;&#21387;&#32553;&#33021;&#21147;&#12290;&#23427;&#21453;&#26144;&#20102;&#27169;&#22411;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#21644;&#28040;&#38500;&#19981;&#24517;&#35201;&#20803;&#32032;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#23545;&#35821;&#35328;&#27169;&#22411;&#22266;&#26377;&#33021;&#21147;&#30340;&#27934;&#23519;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#22312;&#21333;&#27169;&#24577;&#65288;&#35821;&#35328;&#65289;&#21644;&#22810;&#27169;&#24577;&#35774;&#32622;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20986;&#34920;&#31034;&#30340;&#30697;&#38453;&#29109;&#22312;&#27169;&#22411;&#25193;&#22823;&#26102;&#36981;&#24490;&#19968;&#20010;&#32553;&#25918;&#23450;&#24459;&#31867;&#22411;&#30340;&#38477;&#20302;&#65292;&#36825;&#20316;&#20026;&#20256;&#32479;&#25439;&#22833;&#32553;&#25918;&#23450;&#24459;&#30340;&#34917;&#20805;&#12290;&#23545;&#20110;&#22810;&#27169;&#24577;&#35774;&#32622;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30697;&#38453;&#29109;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#19968;&#20010;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have revolutionized the field of natural language processing, extending their strong capabilities into multi-modal domains. Thus, it is vital to define proper and diversified metrics for the evaluation of LLMs.   In this paper, we introduce matrix entropy, a novel metric rooted in information theory and geometry principles to quantify the data compression proficiency in LLMs. It reflects the model's ability to extract relevant information and eliminate unnecessary elements, thereby providing insight into the language model's intrinsic capability. Specifically, we demonstrate its applicability in both single-modal (language) and multi-modal settings. For language models, our findings reveal that the matrix entropy of representations follows a scaling law type reduction when the model scales up, serving as a complement to the traditional loss scaling law. For the multi-modal setting, we also propose an evaluation method based on matrix entropy for assessing a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827; Latent Diffusion Model &#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861; ACE&#65292;&#20854;&#36890;&#36807;&#32479;&#19968;&#27169;&#24335;&#30340;&#39069;&#22806;&#35823;&#24046;&#26469;&#20419;&#20351;&#27169;&#22411;&#23398;&#20064;&#29305;&#23450;&#30340;&#20559;&#24046;&#65292;&#20174;&#32780;&#32988;&#36807;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2310.04687</link><description>&lt;p&gt;
&#25913;&#36827;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Improving Adversarial Attacks on Latent Diffusion Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.04687
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827; Latent Diffusion Model &#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861; ACE&#65292;&#20854;&#36890;&#36807;&#32479;&#19968;&#27169;&#24335;&#30340;&#39069;&#22806;&#35823;&#24046;&#26469;&#20419;&#20351;&#27169;&#22411;&#23398;&#20064;&#29305;&#23450;&#30340;&#20559;&#24046;&#65292;&#20174;&#32780;&#32988;&#36807;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545; Latent Diffusion Model (LDM)&#65292;&#36825;&#31181;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#38450;&#27490; LDM &#22312;&#26410;&#32463;&#25480;&#26435;&#30340;&#22270;&#20687;&#19978;&#36827;&#34892;&#24694;&#24847;&#24494;&#35843;&#30340;&#20445;&#25252;&#25163;&#27573;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#25915;&#20987;&#20250;&#23545; LDM &#39044;&#27979;&#30340;&#23545;&#25239;&#26679;&#26412;&#30340;&#35780;&#20998;&#20989;&#25968;&#28155;&#21152;&#39069;&#22806;&#30340;&#35823;&#24046;&#12290;&#22312;&#36825;&#20123;&#23545;&#25239;&#26679;&#26412;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340; LDM &#23398;&#20064;&#36890;&#36807;&#19968;&#20010;&#20559;&#24046;&#38477;&#20302;&#35823;&#24046;&#65292;&#20174;&#32780;&#36973;&#21463;&#25915;&#20987;&#24182;&#20351;&#29992;&#20559;&#24046;&#39044;&#27979;&#35780;&#20998;&#20989;&#25968;&#12290;&#22522;&#20110;&#36825;&#19968;&#21160;&#24577;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#19968;&#33268;&#24471;&#20998;&#20989;&#25968;&#38169;&#35823;&#36827;&#34892;&#25915;&#20987;&#65288;ACE&#65289;&#26469;&#25913;&#36827; LDM &#30340;&#23545;&#25239;&#25915;&#20987;&#12290;ACE &#32479;&#19968;&#20102;&#28155;&#21152;&#21040;&#39044;&#27979;&#24471;&#20998;&#20989;&#25968;&#30340;&#39069;&#22806;&#35823;&#24046;&#30340;&#27169;&#24335;&#12290;&#36825;&#20419;&#20351;&#24494;&#35843;&#30340; LDM &#23398;&#20064;&#19982;&#23545;&#35780;&#20998;&#20989;&#25968;&#36827;&#34892;&#39044;&#27979;&#30340;&#20559;&#24046;&#23398;&#20064;&#30456;&#21516;&#30340;&#27169;&#24335;&#12290;&#28982;&#21518;&#25105;&#20204;&#24341;&#20837;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#27169;&#24335;&#26469;&#25913;&#36827;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23545; LDM &#30340;&#23545;&#25239;&#25915;&#20987;&#20013;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.04687v3 Announce Type: replace-cross  Abstract: Adversarial attacks on Latent Diffusion Model (LDM), the state-of-the-art image generative model, have been adopted as effective protection against malicious finetuning of LDM on unauthorized images. We show that these attacks add an extra error to the score function of adversarial examples predicted by LDM. LDM finetuned on these adversarial examples learns to lower the error by a bias, from which the model is attacked and predicts the score function with biases.   Based on the dynamics, we propose to improve the adversarial attack on LDM by Attacking with Consistent score-function Errors (ACE). ACE unifies the pattern of the extra error added to the predicted score function. This induces the finetuned LDM to learn the same pattern as a bias in predicting the score function. We then introduce a well-crafted pattern to improve the attack. Our method outperforms state-of-the-art methods in adversarial attacks on LDM.
&lt;/p&gt;</description></item><item><title>LLM&#25351;&#20196;&#24494;&#35843;&#20013;&#65292;&#23545;&#20110;&#30701;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#65292;&#25552;&#31034;&#35789;&#26631;&#35760;&#20998;&#31867;&#25439;&#22833;&#21152;&#26435;&#65288;PLW&#65289;&#19982;&#24615;&#33021;&#21576;&#36127;&#20108;&#27425;&#20851;&#31995;&#65292;&#32780;&#38271;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#21017;&#19981;&#21463;PLW&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.13586</link><description>&lt;p&gt;
LLM&#25351;&#20196;&#24494;&#35843;&#20013;&#30340;&#25552;&#31034;&#26435;&#37325;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Prompt Weight Experiments for LLM Instruction Fine-Tuning. (arXiv:2401.13586v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13586
&lt;/p&gt;
&lt;p&gt;
LLM&#25351;&#20196;&#24494;&#35843;&#20013;&#65292;&#23545;&#20110;&#30701;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#65292;&#25552;&#31034;&#35789;&#26631;&#35760;&#20998;&#31867;&#25439;&#22833;&#21152;&#26435;&#65288;PLW&#65289;&#19982;&#24615;&#33021;&#21576;&#36127;&#20108;&#27425;&#20851;&#31995;&#65292;&#32780;&#38271;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#21017;&#19981;&#21463;PLW&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#23567;&#22411;&#30740;&#31350;&#65292;&#20998;&#26512;&#20102;&#25552;&#31034;&#35789;&#26631;&#35760;&#20998;&#31867;&#25439;&#22833;&#21152;&#26435;&#65288;PLW&#65289;&#22914;&#20309;&#24433;&#21709;&#22312;&#25351;&#20196;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;7B&#22823;&#23567;&#30340;LLaMA&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#25351;&#20196;&#25968;&#25454;&#38598;&#37325;&#29616;&#20102;&#26031;&#22374;&#31119;&#22823;&#23398;&#30340;Alpaca&#23454;&#39564;&#65292;&#20854;&#20013;&#21253;&#25324;LLaMA 1&#21644;LLaMA 2&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#25105;&#20204;&#30340;&#30701;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#19982;PLW&#20043;&#38388;&#23384;&#22312;&#36127;&#20108;&#27425;&#20851;&#31995;&#65292;&#32780;&#22312;&#38271;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#19981;&#21463;PLW&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a small study analyzing how prompt token classification loss weighting (PLW) affects the performance of 7B-size LLaMA models fine-tuned on instruction tasks. We recreated Stanford's Alpaca experiment with both LLaMA 1 and LLaMA 2 using multiple instruction datasets. We found that models fine-tuned on our short-completion dataset have a negative quadratic relationship with PLW while models fine-tuned on long-completion datasets were unaffected by PLW.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Q&amp;A&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#22270;&#20687;&#20013;&#30340;&#38382;&#39064;-&#22238;&#31572;&#23545;&#26469;&#21457;&#29616;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#20197;&#24110;&#21161;AI&#27169;&#22411;&#26356;&#22909;&#22320;&#29702;&#35299;&#22797;&#26434;&#35270;&#35273;&#38382;&#39064;&#65292;&#25552;&#39640;&#36328;&#27169;&#24577;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.10712</link><description>&lt;p&gt;
Q&amp;A&#25552;&#31034;&#65306;&#36890;&#36807;&#25366;&#25496;&#38382;&#39064;-&#22238;&#31572;&#25552;&#31034;&#26469;&#21457;&#29616;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#20197;&#28385;&#36275;&#23545;&#22810;&#26679;&#19990;&#30028;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#30340;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;
Q&amp;A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge. (arXiv:2401.10712v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Q&amp;A&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#22270;&#20687;&#20013;&#30340;&#38382;&#39064;-&#22238;&#31572;&#23545;&#26469;&#21457;&#29616;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#20197;&#24110;&#21161;AI&#27169;&#22411;&#26356;&#22909;&#22320;&#29702;&#35299;&#22797;&#26434;&#35270;&#35273;&#38382;&#39064;&#65292;&#25552;&#39640;&#36328;&#27169;&#24577;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#30772;&#65292;&#22238;&#31572;&#38656;&#35201;&#39640;&#32423;&#25512;&#29702;&#33021;&#21147;&#21644;&#19990;&#30028;&#30693;&#35782;&#30340;&#22797;&#26434;&#35270;&#35273;&#38382;&#39064;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#26356;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20026;AI&#27169;&#22411;&#37197;&#22791;&#24378;&#22823;&#30340;&#36328;&#27169;&#24577;&#25512;&#29702;&#33021;&#21147;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#20154;&#31867;&#30340;&#35748;&#30693;&#26041;&#26696;&#23578;&#26410;&#31995;&#32479;&#22320;&#34987;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30456;&#20449;&#65292;&#22914;&#26524;&#25105;&#20204;&#33021;&#23613;&#21487;&#33021;&#25910;&#38598;&#32473;&#23450;&#22270;&#20687;&#20013;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#25105;&#20204;&#23558;&#33021;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#22270;&#20687;&#65292;&#26356;&#22909;&#22320;&#29702;&#35299;&#38382;&#39064;&#65292;&#26356;&#23481;&#26131;&#22238;&#24518;&#30456;&#20851;&#30693;&#35782;&#65292;&#24182;&#26368;&#32456;&#25512;&#29702;&#20986;&#31572;&#26696;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22270;&#20687;&#20013;&#25366;&#25496;&#38382;&#39064;-&#22238;&#31572;&#23545;&#26469;&#21457;&#29616;&#36825;&#20123;&#20016;&#23500;&#30340;&#35270;&#35273;&#32447;&#32034;&#65292;&#24182;&#23558;&#23427;&#20204;&#20316;&#20026;&#25552;&#31034;&#21457;&#36865;&#21040;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;Q&amp;A&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#35757;&#32451;&#38598;&#20013;&#30340;&#22270;&#20687;-&#31572;&#26696;&#23545;&#21644;&#30456;&#24212;&#30340;&#38382;&#39064;&#20316;&#20026;&#36755;&#20837;&#21644;&#36755;&#20986;&#26469;&#35757;&#32451;&#19968;&#20010;&#35270;&#35273;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the breakthrough of multi-modal large language models, answering complex visual questions that demand advanced reasoning abilities and world knowledge has become a much more important testbed for developing AI models than ever. However, equipping AI models with robust cross-modality reasoning ability remains challenging since the cognition scheme of humans has not been understood systematically. In this paper, we believe that if we can collect visual clues in the given image as much as possible, we will recognize the image more accurately, understand the question better, recall relevant knowledge more easily, and finally reason out the answer. We discover these rich visual clues by mining question-answer pairs in images and sending them into multi-modal large language models as prompts. We call the proposed method Q&amp;A Prompts. Specifically, we first use the image-answer pairs and the corresponding questions in the training set as inputs and outputs to train a visual question gener
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#23545;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;ChatGPT&#21644;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#24341;&#25806;&#22312;&#23558;&#20013;&#25991;&#22806;&#20132;&#25991;&#26412;&#32763;&#35793;&#20026;&#33521;&#25991;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#21644;&#20154;&#24037;&#35780;&#20272;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2401.05176</link><description>&lt;p&gt;
&#23545;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;ChatGPT&#19982;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#22312;&#32763;&#35793;&#20013;&#30340;&#31454;&#20105;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT Rival Neural Machine Translation? A Comparative Study. (arXiv:2401.05176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#23545;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;ChatGPT&#21644;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#24341;&#25806;&#22312;&#23558;&#20013;&#25991;&#22806;&#20132;&#25991;&#26412;&#32763;&#35793;&#20026;&#33521;&#25991;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#21644;&#20154;&#24037;&#35780;&#20272;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#36234;&#26469;&#36234;&#22810;&#22320;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32763;&#35793;&#30340;&#20852;&#36259;&#19981;&#26029;&#22686;&#21152;&#30340;&#32972;&#26223;&#19979;&#65292;&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#20027;&#27969;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#24341;&#25806;&#22312;&#23558;&#20013;&#25991;&#22806;&#20132;&#25991;&#26412;&#32763;&#35793;&#20026;&#33521;&#25991;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#22235;&#20010;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#21644;&#22522;&#20110;&#38169;&#35823;&#31867;&#22411;&#21644;&#20845;&#20010;&#20998;&#26512;&#32454;&#21017;&#30340;&#20154;&#24037;&#35780;&#20272;&#65292;&#32771;&#23519;&#20102;ChatGPT&#21644;NMT&#24341;&#25806;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#23545;&#20110;ChatGPT&#22312;&#19981;&#21516;&#25552;&#31034;&#21644;NMT&#31995;&#32479;&#19979;&#30340;&#34920;&#29616;&#24471;&#20986;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#65292;&#32780;&#24403;ChatGPT&#25552;&#20379;&#31034;&#20363;&#25110;&#32763;&#35793;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#26102;&#65292;&#20154;&#24037;&#35780;&#20272;&#32773;&#24448;&#24448;&#20250;&#32473;&#20104;&#26126;&#26174;&#36739;&#39640;&#30340;&#35780;&#20998;&#12290;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#19982;&#20154;&#24037;&#35780;&#20272;&#32500;&#24230;&#20043;&#38388;&#30340;&#20004;&#20004;&#30456;&#20851;&#24615;&#32467;&#26524;&#36739;&#24369;&#19988;&#19981;&#26174;&#33879;&#65292;&#36825;&#34920;&#26126;&#20102;&#20004;&#31181;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the increasing interest in leveraging large language models for translation, this paper evaluates the capabilities of large language models (LLMs) represented by ChatGPT in comparison to the mainstream neural machine translation (NMT) engines in translating Chinese diplomatic texts into English. Specifically, we examine the translation quality of ChatGPT and NMT engines as measured by four automated metrics and human evaluation based on an error-typology and six analytic rubrics. Our findings show that automated metrics yield similar results for ChatGPT under different prompts and NMT systems, while human annotators tend to assign noticeably higher scores to ChatGPT when it is provided an example or contextual information about the translation task. Pairwise correlation between automated metrics and dimensions of human evaluation produces weak and non-significant results, suggesting the divergence between the two methods of translation quality assessment. These findings pro
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#27169;&#22411;&#37096;&#32626;&#21644;&#24182;&#34892;&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#36895;RLHF&#35757;&#32451;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#20004;&#31181;&#28789;&#27963;&#30340;&#27169;&#22411;&#37096;&#32626;&#31574;&#30053;&#65292;&#20854;&#20013;&#20132;&#26367;&#31574;&#30053;&#26377;&#21161;&#20110;&#20943;&#23569;&#20869;&#23384;&#20887;&#20313;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2312.11819</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;RLHF&#35757;&#32451;&#30340;&#33258;&#36866;&#24212;&#37096;&#32626;&#21644;&#24182;&#34892;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Adaptive Placement and Parallelism Framework for Accelerating RLHF Training. (arXiv:2312.11819v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11819
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#27169;&#22411;&#37096;&#32626;&#21644;&#24182;&#34892;&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#36895;RLHF&#35757;&#32451;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#20004;&#31181;&#28789;&#27963;&#30340;&#27169;&#22411;&#37096;&#32626;&#31574;&#30053;&#65292;&#20854;&#20013;&#20132;&#26367;&#31574;&#30053;&#26377;&#21161;&#20110;&#20943;&#23569;&#20869;&#23384;&#20887;&#20313;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20687;ChatGPT&#25110;InstructGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#35768;&#22810;&#30740;&#31350;&#23581;&#35797;&#22797;&#29616;&#22797;&#26434;&#30340;InstructGPT&#30340;&#35757;&#32451;&#27969;&#31243;&#65292;&#21363;&#22522;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#12290;&#28982;&#32780;&#65292;&#20027;&#27969;&#30340;&#20998;&#24067;&#24335;RLHF&#35757;&#32451;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#22266;&#23450;&#30340;&#27169;&#22411;&#37096;&#32626;&#31574;&#30053;&#65292;&#31216;&#20026;Flattening&#31574;&#30053;&#12290;&#35813;&#31574;&#30053;&#23558;RLHF&#20013;&#28041;&#21450;&#30340;&#22235;&#20010;&#30456;&#20114;&#20381;&#36182;&#30340;&#27169;&#22411;&#35270;&#20026;&#21333;&#20010;&#23454;&#20307;&#65292;&#23558;&#23427;&#20204;&#20998;&#37197;&#21040;&#25152;&#26377;&#35774;&#22791;&#19978;&#65292;&#24182;&#24212;&#29992;&#20110;&#21333;&#20010;&#27169;&#22411;&#35774;&#35745;&#30340;&#24182;&#34892;&#25216;&#26415;&#65292;&#32780;&#19981;&#32771;&#34385;&#27599;&#20010;&#27169;&#22411;&#22266;&#26377;&#30340;&#19981;&#21516;&#24037;&#20316;&#36127;&#36733;&#12290;&#32467;&#26524;&#65292;&#35813;&#31574;&#30053;&#21152;&#21095;&#20102;RLHF&#35757;&#32451;&#20013;&#30340;&#29983;&#25104;&#29942;&#39048;&#65292;&#24182;&#38477;&#20302;&#20102;&#25972;&#20307;&#35757;&#32451;&#25928;&#29575;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#27169;&#22411;&#37096;&#32626;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#20004;&#31181;&#28789;&#27963;&#30340;&#27169;&#22411;&#37096;&#32626;&#31574;&#30053;&#12290;&#20132;&#26367;&#31574;&#30053;&#26377;&#21161;&#20110;&#20943;&#23569;&#20869;&#23384;&#20887;&#20313;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, ChatGPT or InstructGPT like large language models (LLM) has made a significant impact in the AI world. Many works have attempted to reproduce the complex InstructGPT's training pipeline, namely Reinforcement Learning with Human Feedback (RLHF). However, the mainstream distributed RLHF training methods typically adopt a fixed model placement strategy, referred to as the Flattening strategy. This strategy treats all four interdependent models involved in RLHF as a single entity, distributing them across all devices and applying parallelism techniques designed for a single model, regardless of the different workloads inherent to each model. As a result, this strategy exacerbates the generation bottlenecks in the RLHF training and degrades the overall training efficiency. To address these issues, we propose an adaptive model placement framework that offers two flexible model placement strategies. The Interleaving strategy helps reduce memory redundancy and communication costs of 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DHOT-GM&#30340;&#22270;&#21305;&#37197;&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#20998;&#23618;&#26368;&#20248;&#20256;&#36755;&#26694;&#26550;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#22270;&#20013;&#38544;&#34255;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#36890;&#36807;&#23545;&#21305;&#37197;&#32467;&#26524;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#26469;&#25512;&#26029;&#33410;&#28857;&#23545;&#24212;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.12081</link><description>&lt;p&gt;
DHOT-GM&#65306;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#20998;&#23618;&#26368;&#20248;&#20256;&#36755;&#26694;&#26550;&#23454;&#29616;&#40065;&#26834;&#22270;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
DHOT-GM: Robust Graph Matching Using A Differentiable Hierarchical Optimal Transport Framework. (arXiv:2310.12081v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DHOT-GM&#30340;&#22270;&#21305;&#37197;&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#20998;&#23618;&#26368;&#20248;&#20256;&#36755;&#26694;&#26550;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#22270;&#20013;&#38544;&#34255;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#36890;&#36807;&#23545;&#21305;&#37197;&#32467;&#26524;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#26469;&#25512;&#26029;&#33410;&#28857;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#36341;&#20013;&#65292;&#22270;&#21305;&#37197;&#26159;&#26368;&#37325;&#35201;&#30340;&#22270;&#20998;&#26512;&#20219;&#21153;&#20043;&#19968;&#65292;&#20854;&#30446;&#26631;&#26159;&#25214;&#21040;&#19981;&#21516;&#22270;&#20043;&#38388;&#30340;&#33410;&#28857;&#23545;&#24212;&#20851;&#31995;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#22312;&#21305;&#37197;&#22270;&#26102;&#20381;&#36182;&#20110;&#37051;&#25509;&#30697;&#38453;&#25110;&#33410;&#28857;&#23884;&#20837;&#65292;&#20854;&#24615;&#33021;&#24120;&#24120;&#19981;&#22815;&#20248;&#36234;&#65292;&#22240;&#20026;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#22270;&#20013;&#38544;&#34255;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#22914;&#33410;&#28857;&#23646;&#24615;&#12289;&#23376;&#22270;&#32467;&#26500;&#31561;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#20998;&#30340;&#20998;&#23618;&#26368;&#20248;&#20256;&#36755;&#65288;HOT&#65289;&#26694;&#26550;&#30340;&#26032;&#39062;&#26377;&#25928;&#30340;&#22270;&#21305;&#37197;&#26041;&#27861;&#65292;&#31216;&#20026;DHOT-GM&#12290;&#23454;&#36136;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#27599;&#20010;&#22270;&#34920;&#31034;&#20026;&#19982;&#19981;&#21516;&#27169;&#24577;&#20449;&#24687;&#23545;&#24212;&#30340;&#19968;&#32452;&#20851;&#31995;&#30697;&#38453;&#12290;&#32473;&#23450;&#20004;&#20010;&#22270;&#65292;&#25105;&#20204;&#26522;&#20030;&#25152;&#26377;&#20851;&#31995;&#30697;&#38453;&#23545;&#65292;&#24182;&#33719;&#21462;&#23427;&#20204;&#30340;&#21305;&#37197;&#32467;&#26524;&#65292;&#28982;&#21518;&#36890;&#36807;&#23545;&#21305;&#37197;&#32467;&#26524;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#26469;&#25512;&#26029;&#33410;&#28857;&#23545;&#24212;&#20851;&#31995;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#20026;&#35745;&#31639;&#20004;&#20010;&#22270;&#20043;&#38388;&#30340;HOT&#36317;&#31163;&#65292;&#27599;&#20010;&#22270;&#37117;&#26159;&#30001;&#20851;&#31995;&#30697;&#38453;&#34920;&#31034;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph matching is one of the most significant graph analytic tasks in practice, which aims to find the node correspondence across different graphs. Most existing approaches rely on adjacency matrices or node embeddings when matching graphs, whose performances are often sub-optimal because of not fully leveraging the multi-modal information hidden in graphs, such as node attributes, subgraph structures, etc. In this study, we propose a novel and effective graph matching method based on a differentiable hierarchical optimal transport (HOT) framework, called DHOT-GM. Essentially, our method represents each graph as a set of relational matrices corresponding to the information of different modalities. Given two graphs, we enumerate all relational matrix pairs and obtain their matching results, and accordingly, infer the node correspondence by the weighted averaging of the matching results. This method can be implemented as computing the HOT distance between the two graphs -- each matching 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#32452;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL&#65289;&#20844;&#24335;&#65292;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#27169;&#25311;&#22842;&#26071;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16960</link><description>&lt;p&gt;
&#29983;&#25104;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#35299;&#37322;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Generating Explanations for Reinforcement Learning Policies: An Empirical Study. (arXiv:2309.16960v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#32452;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL&#65289;&#20844;&#24335;&#65292;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#27169;&#25311;&#22842;&#26071;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#32452;&#35774;&#35745;&#29992;&#20110;&#25552;&#20379;&#31574;&#30053;&#35299;&#37322;&#30340;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL&#65289;&#20844;&#24335;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#26500;&#24314;&#26082;&#38416;&#26126;&#31574;&#30053;&#25152;&#23454;&#29616;&#30340;&#26368;&#32456;&#30446;&#26631;&#21448;&#38416;&#26126;&#20854;&#25191;&#34892;&#36807;&#31243;&#20013;&#25152;&#32500;&#25345;&#30340;&#21069;&#25552;&#26465;&#20214;&#30340;&#35299;&#37322;&#12290;&#36825;&#20123;&#22522;&#20110;LTL&#30340;&#35299;&#37322;&#20855;&#26377;&#32467;&#26500;&#21270;&#34920;&#31034;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#23616;&#37096;&#25628;&#32034;&#25216;&#26415;&#12290;&#36890;&#36807;&#27169;&#25311;&#30340;&#22842;&#26071;&#29615;&#22659;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#35770;&#25991;&#26368;&#21518;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#24314;&#35758;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a set of \textit{Linear Temporal Logic} (LTL) formulae designed to provide explanations for policies. Our focus is on crafting explanations that elucidate both the ultimate objectives accomplished by the policy and the prerequisites it upholds throughout its execution. These LTL-based explanations feature a structured representation, which is particularly well-suited for local-search techniques. The effectiveness of our proposed approach is illustrated through a simulated capture the flag environment. The paper concludes with suggested directions for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#30005;&#24433;&#28023;&#25253;&#22270;&#20687;&#65292;&#35299;&#23494;&#20102;&#30005;&#24433;&#28023;&#25253;&#30340;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#22810;&#26631;&#31614;&#30005;&#24433;&#31867;&#22411;&#35782;&#21035;&#26041;&#27861;&#65292;&#26080;&#38656;&#20351;&#29992;&#20854;&#20182;&#25991;&#26412;&#25110;&#20803;&#25968;&#25454;&#20449;&#24687;&#65292;&#20855;&#26377;&#25512;&#24191;&#21644;&#33829;&#38144;&#30005;&#24433;&#30340;&#23454;&#38469;&#24212;&#29992;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.12022</link><description>&lt;p&gt;
&#35299;&#23494;&#30005;&#24433;&#28023;&#25253;&#30340;&#35270;&#35273;&#29305;&#24449;&#65292;&#29992;&#20110;&#22810;&#26631;&#31614;&#30005;&#24433;&#31867;&#22411;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Demystifying Visual Features of Movie Posters for Multi-Label Genre Identification. (arXiv:2309.12022v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#30005;&#24433;&#28023;&#25253;&#22270;&#20687;&#65292;&#35299;&#23494;&#20102;&#30005;&#24433;&#28023;&#25253;&#30340;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#22810;&#26631;&#31614;&#30005;&#24433;&#31867;&#22411;&#35782;&#21035;&#26041;&#27861;&#65292;&#26080;&#38656;&#20351;&#29992;&#20854;&#20182;&#25991;&#26412;&#25110;&#20803;&#25968;&#25454;&#20449;&#24687;&#65292;&#20855;&#26377;&#25512;&#24191;&#21644;&#33829;&#38144;&#30005;&#24433;&#30340;&#23454;&#38469;&#24212;&#29992;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#24433;&#34892;&#19994;&#20013;&#65292;&#30005;&#24433;&#28023;&#25253;&#22810;&#24180;&#26469;&#19968;&#30452;&#26159;&#24191;&#21578;&#21644;&#33829;&#38144;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#21363;&#20351;&#22312;&#29616;&#20170;&#30340;&#25968;&#23383;&#28023;&#25253;&#36890;&#36807;&#22312;&#32447;&#12289;&#31038;&#20132;&#23186;&#20307;&#21644;OTT&#24179;&#21488;&#19978;&#20173;&#28982;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#36890;&#24120;&#65292;&#30005;&#24433;&#28023;&#25253;&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#24191;&#21644;&#20256;&#36798;&#30005;&#24433;&#30340;&#26412;&#36136;&#65292;&#20363;&#22914;&#20854;&#31867;&#22411;&#12289;&#35270;&#35273;&#39118;&#26684;/&#35843;&#35843;&#12289;&#27675;&#22260;&#21644;&#25925;&#20107;&#32447;&#32034;/&#20027;&#39064;&#65292;&#36825;&#20123;&#23545;&#21560;&#24341;&#28508;&#22312;&#35266;&#20247;&#38750;&#24120;&#37325;&#35201;&#12290;&#23545;&#30005;&#24433;&#31867;&#22411;&#36827;&#34892;&#35782;&#21035;&#24120;&#24120;&#22312;&#21521;&#30446;&#26631;&#35266;&#20247;&#25512;&#33616;&#30005;&#24433;&#26102;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#20043;&#21069;&#30340;&#30005;&#24433;&#31867;&#22411;&#35782;&#21035;&#30740;&#31350;&#20165;&#38480;&#20110;&#23383;&#24149;&#12289;&#21095;&#24773;&#31616;&#20171;&#21644;&#30005;&#24433;&#22330;&#26223;&#65292;&#36825;&#20123;&#22823;&#22810;&#25968;&#22312;&#30005;&#24433;&#21457;&#24067;&#21518;&#25165;&#33021;&#33719;&#21462;&#12290;&#28023;&#25253;&#36890;&#24120;&#21253;&#21547;&#22312;&#21457;&#34892;&#21069;&#38544;&#21547;&#30340;&#20449;&#24687;&#26469;&#24341;&#36215;&#22823;&#37327;&#20852;&#36259;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#30005;&#24433;&#28023;&#25253;&#22270;&#20687;&#20013;&#33258;&#21160;&#36827;&#34892;&#22810;&#26631;&#31614;&#30005;&#24433;&#31867;&#22411;&#35782;&#21035;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#20851;&#20110;&#30005;&#24433;&#30340;&#38468;&#21152;&#25991;&#26412;/&#20803;&#25968;&#25454;&#20449;&#24687;&#30340;&#24110;&#21161;&#65292;&#36825;&#26159;&#20854;&#20013;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the film industry, movie posters have been an essential part of advertising and marketing for many decades, and continue to play a vital role even today in the form of digital posters through online, social media and OTT platforms. Typically, movie posters can effectively promote and communicate the essence of a film, such as its genre, visual style/ tone, vibe and storyline cue/ theme, which are essential to attract potential viewers. Identifying the genres of a movie often has significant practical applications in recommending the film to target audiences. Previous studies on movie genre identification are limited to subtitles, plot synopses, and movie scenes that are mostly accessible after the movie release. Posters usually contain pre-release implicit information to generate mass interest. In this paper, we work for automated multi-label genre identification only from movie poster images, without any aid of additional textual/meta-data information about movies, which is one of 
&lt;/p&gt;</description></item><item><title>AR-TTA&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#36830;&#32493;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#12290;&#36890;&#36807;&#23558;&#20869;&#23384;&#32531;&#20914;&#21306;&#32435;&#20837;&#33258;&#35757;&#32451;&#26694;&#26550;&#65292;&#24182;&#26681;&#25454;&#25968;&#25454;&#27969;&#30340;&#24378;&#24230;&#36827;&#34892;&#21160;&#24577;&#36866;&#24212;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10109</link><description>&lt;p&gt;
AR-TTA: &#19968;&#31181;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#36830;&#32493;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#30340;&#31616;&#21333;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AR-TTA: A Simple Method for Real-World Continual Test-Time Adaptation. (arXiv:2309.10109v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10109
&lt;/p&gt;
&lt;p&gt;
AR-TTA&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#36830;&#32493;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#12290;&#36890;&#36807;&#23558;&#20869;&#23384;&#32531;&#20914;&#21306;&#32435;&#20837;&#33258;&#35757;&#32451;&#26694;&#26550;&#65292;&#24182;&#26681;&#25454;&#25968;&#25454;&#27969;&#30340;&#24378;&#24230;&#36827;&#34892;&#21160;&#24577;&#36866;&#24212;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#23427;&#20801;&#35768;&#28304;&#27169;&#22411;&#22312;&#27809;&#26377;&#20219;&#20309;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#22312;&#21482;&#26159;&#23454;&#38469;&#22330;&#26223;&#31616;&#21270;&#29256;&#26412;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#35780;&#20272;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#26368;&#36817;&#25512;&#20986;&#30340;&#33258;&#21160;&#39550;&#39542;&#25968;&#25454;&#38598;CLAD-C&#21644;SHIFT&#26469;&#39564;&#35777;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#24403;&#21069;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#26377;&#25928;&#22788;&#29702;&#19981;&#21516;&#31243;&#24230;&#30340;&#22495;&#20559;&#31227;&#65292;&#24120;&#24120;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#65292;&#20302;&#20110;&#28304;&#27169;&#22411;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#38382;&#39064;&#30340;&#26681;&#28304;&#22312;&#20110;&#26080;&#27861;&#20445;&#30041;&#28304;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#24182;&#19988;&#26080;&#27861;&#36866;&#24212;&#21160;&#24577;&#21464;&#21270;&#12289;&#26102;&#38388;&#30456;&#20851;&#30340;&#25968;&#25454;&#27969;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#19968;&#20010;&#23567;&#30340;&#20869;&#23384;&#32531;&#20914;&#21306;&#32435;&#20837;&#21040;&#25104;&#29087;&#30340;&#33258;&#35757;&#32451;&#26694;&#26550;&#20013;&#65292;&#22686;&#21152;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#21516;&#26102;&#26681;&#25454;&#25968;&#25454;&#27969;&#30340;&#24378;&#24230;&#36827;&#34892;&#21160;&#24577;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Test-time adaptation is a promising research direction that allows the source model to adapt itself to changes in data distribution without any supervision. Yet, current methods are usually evaluated on benchmarks that are only a simplification of real-world scenarios. Hence, we propose to validate test-time adaptation methods using the recently introduced datasets for autonomous driving, namely CLAD-C and SHIFT. We observe that current test-time adaptation methods struggle to effectively handle varying degrees of domain shift, often resulting in degraded performance that falls below that of the source model. We noticed that the root of the problem lies in the inability to preserve the knowledge of the source model and adapt to dynamically changing, temporally correlated data streams. Therefore, we enhance well-established self-training framework by incorporating a small memory buffer to increase model stability and at the same time perform dynamic adaptation based on the intensity of 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20113;&#27169;&#22411;&#30340;&#26816;&#32034;&#22686;&#24378;&#20869;&#23384;&#25972;&#21512;&#21040;&#23458;&#25143;&#31471;&#27169;&#22411;&#20013;&#65292;&#23454;&#29616;&#23454;&#26102;&#21709;&#24212;&#30340;&#20316;&#26354;&#36741;&#21161;&#12290;</title><link>http://arxiv.org/abs/2308.04215</link><description>&lt;p&gt;
&#23454;&#26102;&#20316;&#26354;&#36741;&#21161;&#30340;&#28151;&#21512;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance. (arXiv:2308.04215v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04215
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20113;&#27169;&#22411;&#30340;&#26816;&#32034;&#22686;&#24378;&#20869;&#23384;&#25972;&#21512;&#21040;&#23458;&#25143;&#31471;&#27169;&#22411;&#20013;&#65292;&#23454;&#29616;&#23454;&#26102;&#21709;&#24212;&#30340;&#20316;&#26354;&#36741;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#22312;&#25552;&#21319;&#20256;&#32479;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12289;&#25972;&#21512;&#31169;&#20154;&#25968;&#25454;&#21644;&#20943;&#23569;&#24187;&#35273;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24212;&#29992;&#20110;&#38656;&#35201;&#23454;&#26102;&#21709;&#24212;&#30340;&#20219;&#21153;&#65288;&#22914;&#20316;&#26354;&#36741;&#21161;&#65289;&#26102;&#65292;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#38656;&#30340;&#22788;&#29702;&#26102;&#38388;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Hybrid Retrieval-Augmented Generation (HybridRAG)&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#23558;&#23458;&#25143;&#31471;&#27169;&#22411;&#21644;&#20113;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#30340;&#28151;&#21512;&#35774;&#32622;&#12290;HybridRAG&#36890;&#36807;&#24322;&#27493;&#29983;&#25104;&#30340;&#26816;&#32034;&#22686;&#24378;&#20869;&#23384;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20113;&#31471;&#29983;&#25104;&#30340;&#26816;&#32034;&#22686;&#24378;&#20869;&#23384;&#25972;&#21512;&#21040;&#23458;&#25143;&#31471;&#27169;&#22411;&#20013;&#12290;&#36890;&#36807;&#25972;&#21512;&#36825;&#31181;&#26816;&#32034;&#22686;&#24378;&#20869;&#23384;&#65292;&#23458;&#25143;&#31471;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#25928;&#30340;&#21709;&#24212;&#65292;&#20174;LLM&#30340;&#33021;&#21147;&#20013;&#21463;&#30410;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#24322;&#27493;&#20869;&#23384;&#38598;&#25104;&#65292;&#23458;&#25143;&#31471;&#27169;&#22411;&#33021;&#22815;&#23454;&#26102;&#21709;&#24212;&#29992;&#25143;&#35831;&#27714;&#65292;&#26080;&#38656;&#31561;&#24453;&#20113;&#31471;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval augmented models show promise in enhancing traditional language models by improving their contextual understanding, integrating private data, and reducing hallucination. However, the processing time required for retrieval augmented large language models poses a challenge when applying them to tasks that require real-time responses, such as composition assistance.  To overcome this limitation, we propose the Hybrid Retrieval-Augmented Generation (HybridRAG) framework that leverages a hybrid setting that combines both client and cloud models. HybridRAG incorporates retrieval-augmented memory generated asynchronously by a Large Language Model (LLM) in the cloud. By integrating this retrieval augmented memory, the client model acquires the capability to generate highly effective responses, benefiting from the LLM's capabilities. Furthermore, through asynchronous memory integration, the client model is capable of delivering real-time responses to user requests without the need to 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20998;&#21306;&#25193;&#25955;&#27169;&#22411;&#65288;CDM&#65289;&#35757;&#32451;&#19981;&#21516;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#22312;&#25512;&#26029;&#26102;&#20219;&#24847;&#32452;&#21512;&#23427;&#20204;&#65292;&#23454;&#29616;&#20102;&#35757;&#32451;&#25968;&#25454;&#20445;&#25252;&#21644;&#36873;&#25321;&#24615;&#36951;&#24536;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#35775;&#38382;&#26435;&#38480;&#25552;&#20379;&#23450;&#21046;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.01937</link><description>&lt;p&gt;
&#20351;&#29992;&#32452;&#21512;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#35757;&#32451;&#25968;&#25454;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Training Data Protection with Compositional Diffusion Models. (arXiv:2308.01937v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01937
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20998;&#21306;&#25193;&#25955;&#27169;&#22411;&#65288;CDM&#65289;&#35757;&#32451;&#19981;&#21516;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#22312;&#25512;&#26029;&#26102;&#20219;&#24847;&#32452;&#21512;&#23427;&#20204;&#65292;&#23454;&#29616;&#20102;&#35757;&#32451;&#25968;&#25454;&#20445;&#25252;&#21644;&#36873;&#25321;&#24615;&#36951;&#24536;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#35775;&#38382;&#26435;&#38480;&#25552;&#20379;&#23450;&#21046;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#21306;&#25193;&#25955;&#27169;&#22411;&#65288;CDM&#65289;&#65292;&#19968;&#31181;&#22312;&#19981;&#21516;&#25968;&#25454;&#28304;&#19978;&#35757;&#32451;&#19981;&#21516;&#25193;&#25955;&#27169;&#22411;&#65288;&#25110;&#25552;&#31034;&#65289;&#24182;&#22312;&#25512;&#26029;&#26102;&#20219;&#24847;&#32452;&#21512;&#23427;&#20204;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#21333;&#29420;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#23396;&#31435;&#29366;&#24577;&#19979;&#12289;&#22312;&#19981;&#21516;&#26102;&#38388;&#12289;&#22312;&#19981;&#21516;&#20998;&#24067;&#21644;&#39046;&#22495;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#21487;&#20197;&#21518;&#32493;&#32452;&#21512;&#20197;&#36798;&#21040;&#19982;&#21516;&#26102;&#35757;&#32451;&#25152;&#26377;&#25968;&#25454;&#30340;&#29702;&#24819;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#27599;&#20010;&#27169;&#22411;&#21482;&#21253;&#21547;&#20854;&#22312;&#35757;&#32451;&#26399;&#38388;&#25509;&#35302;&#21040;&#30340;&#25968;&#25454;&#23376;&#38598;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#23454;&#29616;&#22810;&#31181;&#24418;&#24335;&#30340;&#35757;&#32451;&#25968;&#25454;&#20445;&#25252;&#12290;&#29305;&#21035;&#26159;&#65292;CDM&#26159;&#31532;&#19968;&#31181;&#21487;&#20197;&#23454;&#29616;&#22823;&#35268;&#27169;&#25193;&#25955;&#27169;&#22411;&#30340;&#36873;&#25321;&#24615;&#36951;&#24536;&#21644;&#25345;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#20801;&#35768;&#26681;&#25454;&#29992;&#25143;&#35775;&#38382;&#26435;&#38480;&#25552;&#20379;&#23450;&#21046;&#27169;&#22411;&#12290;CDM&#36824;&#21487;&#20197;&#30830;&#23450;&#29983;&#25104;&#29305;&#23450;&#26679;&#26412;&#30340;&#25968;&#25454;&#23376;&#38598;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Compartmentalized Diffusion Models (CDM), a method to train different diffusion models (or prompts) on distinct data sources and arbitrarily compose them at inference time. The individual models can be trained in isolation, at different times, and on different distributions and domains and can be later composed to achieve performance comparable to a paragon model trained on all data simultaneously. Furthermore, each model only contains information about the subset of the data it was exposed to during training, enabling several forms of training data protection. In particular, CDMs are the first method to enable both selective forgetting and continual learning for large-scale diffusion models, as well as allowing serving customized models based on the user's access rights. CDMs also allow determining the importance of a subset of the data in generating particular samples.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#27880;&#37322;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#20316;&#20026;&#30417;&#30563;&#65292;&#24182;&#25972;&#21512;&#26816;&#32034;&#36807;&#31243;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#20854;&#25928;&#21147;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#19981;&#21305;&#37197;&#30340;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#30340;&#30701;&#21306;&#22495;&#25551;&#36848;&#65292;&#24182;&#21033;&#29992;&#20854;&#29983;&#25104;&#22810;&#26679;&#30340;&#21477;&#23376;&#12290;</title><link>http://arxiv.org/abs/2307.14750</link><description>&lt;p&gt;
&#26080;&#27880;&#37322;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#30340;&#30740;&#31350;&#65306;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#20266;&#21477;&#23376;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Exploring Annotation-free Image Captioning with Retrieval-augmented Pseudo Sentence Generation. (arXiv:2307.14750v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#27880;&#37322;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#20316;&#20026;&#30417;&#30563;&#65292;&#24182;&#25972;&#21512;&#26816;&#32034;&#36807;&#31243;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#20854;&#25928;&#21147;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#19981;&#21305;&#37197;&#30340;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#30340;&#30701;&#21306;&#22495;&#25551;&#36848;&#65292;&#24182;&#21033;&#29992;&#20854;&#29983;&#25104;&#22810;&#26679;&#30340;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35757;&#32451;&#26080;&#27880;&#37322;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#22120;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#21487;&#20197;&#20998;&#20026;&#20004;&#31181;&#31574;&#30053;&#65306;&#20174;&#19981;&#21305;&#37197;&#30340;&#35821;&#26009;&#24211;&#20013;&#33719;&#21462;&#21477;&#23376;&#65292;&#24182;&#23558;&#20854;&#19982;&#32473;&#23450;&#30340;&#22270;&#20687;&#23545;&#40784;&#20316;&#20026;&#20266;&#27880;&#37322;&#65292;&#25110;&#32773;&#20351;&#29992;&#22806;&#37096;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#23545;&#29983;&#25104;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23545;&#40784;&#30340;&#35774;&#32622;&#23384;&#22312;&#36136;&#37327;&#38382;&#39064;&#65292;&#20854;&#24615;&#33021;&#20284;&#20046;&#24050;&#32463;&#36798;&#21040;&#20102;&#26497;&#38480;&#65292;&#32780;&#39044;&#35757;&#32451;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#8220;LPM + &#26816;&#32034;&#22686;&#24378;&#23398;&#20064;&#8221;&#65292;&#21033;&#29992;&#26469;&#33258;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;LPM&#65289;&#30340;&#20808;&#39564;&#30693;&#35782;&#20316;&#20026;&#30417;&#30563;&#65292;&#24182;&#25972;&#21512;&#26816;&#32034;&#36807;&#31243;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#20854;&#25928;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#20266;&#21477;&#23376;&#29983;&#25104;&#65288;RaPSG&#65289;&#65292;&#37319;&#29992;&#39640;&#25928;&#30340;&#26041;&#27861;&#20174;&#19981;&#21305;&#37197;&#30340;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#20986;&#39640;&#30456;&#20851;&#30340;&#30701;&#21306;&#22495;&#25551;&#36848;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#29983;&#25104;&#22810;&#26679;&#30340;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training an image captioner without annotated image-sentence pairs has gained traction in recent years. Previous approaches can be categorized into two strategies: crawling sentences from mismatching corpora and aligning them with the given images as pseudo annotations, or pre-training the captioner using external image-text pairs. However, the aligning setting seems to reach its performance limit due to the quality problem of pairs, and pre-training requires significant computational resources. To address these challenges, we propose a new strategy ``LPM + retrieval-augmented learning" where the prior knowledge from large pre-trained models (LPMs) is leveraged as supervision, and a retrieval process is integrated to further reinforce its effectiveness. Specifically, we introduce Retrieval-augmented Pseudo Sentence Generation (RaPSG), which adopts an efficient approach to retrieve highly relevant short region descriptions from the mismatching corpora and use them to generate a variety 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#24179;&#28369;&#38750;&#20984;&#20248;&#21270;&#20013;&#38543;&#26426;&#27425;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#21333;&#26102;&#38388;&#23610;&#24230;&#21644;&#21452;&#26102;&#38388;&#23610;&#24230;&#24773;&#20917;&#19979;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#21253;&#25324;&#20102;&#22810;&#31181;&#24050;&#30693;&#30340;SGD&#31867;&#22411;&#26041;&#27861;&#12290;&#23545;&#20110;&#26377;&#38480;&#21644;&#24418;&#24335;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#27493;&#38271;&#21644;&#21021;&#22987;&#28857;&#19978;&#25214;&#21040;Clarke&#31283;&#23450;&#28857;&#12290;</title><link>http://arxiv.org/abs/2307.10053</link><description>&lt;p&gt;
&#38750;&#24179;&#28369;&#38750;&#20984;&#20248;&#21270;&#20013;&#38543;&#26426;&#27425;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Convergence Guarantees for Stochastic Subgradient Methods in Nonsmooth Nonconvex Optimization. (arXiv:2307.10053v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#24179;&#28369;&#38750;&#20984;&#20248;&#21270;&#20013;&#38543;&#26426;&#27425;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#21333;&#26102;&#38388;&#23610;&#24230;&#21644;&#21452;&#26102;&#38388;&#23610;&#24230;&#24773;&#20917;&#19979;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#21253;&#25324;&#20102;&#22810;&#31181;&#24050;&#30693;&#30340;SGD&#31867;&#22411;&#26041;&#27861;&#12290;&#23545;&#20110;&#26377;&#38480;&#21644;&#24418;&#24335;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#27493;&#38271;&#21644;&#21021;&#22987;&#28857;&#19978;&#25214;&#21040;Clarke&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26041;&#27861;&#21450;&#20854;&#21464;&#31181;&#22312;&#35757;&#32451;&#30001;&#38750;&#24179;&#28369;&#28608;&#27963;&#20989;&#25968;&#26500;&#24314;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20026;&#26356;&#26032;&#21160;&#37327;&#39033;&#21644;&#21464;&#37327;&#30340;&#27493;&#38271;&#20998;&#37197;&#20102;&#19981;&#21516;&#30340;&#26102;&#38388;&#23610;&#24230;&#12290;&#22312;&#19968;&#20123;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#21333;&#26102;&#38388;&#23610;&#24230;&#21644;&#21452;&#26102;&#38388;&#23610;&#24230;&#24773;&#20917;&#19979;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#21253;&#21547;&#20102;&#24456;&#22810;&#24050;&#30693;&#30340;SGD&#31867;&#22411;&#26041;&#27861;&#65292;&#21253;&#25324;heavy-ball SGD&#12289;SignSGD&#12289;Lion&#12289;normalized SGD&#21644;clipped SGD&#12290;&#27492;&#22806;&#65292;&#24403;&#30446;&#26631;&#20989;&#25968;&#37319;&#29992;&#26377;&#38480;&#21644;&#24418;&#24335;&#26102;&#65292;&#25105;&#20204;&#22522;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#35777;&#26126;&#20102;&#36825;&#20123;SGD&#31867;&#22411;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;SGD&#31867;&#22411;&#26041;&#27861;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#27493;&#38271;&#21644;&#21021;&#22987;&#28857;&#19978;&#33021;&#22815;&#25214;&#21040;&#30446;&#26631;&#20989;&#25968;&#30340;Clarke&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the convergence properties of the stochastic gradient descent (SGD) method and its variants, especially in training neural networks built from nonsmooth activation functions. We develop a novel framework that assigns different timescales to stepsizes for updating the momentum terms and variables, respectively. Under mild conditions, we prove the global convergence of our proposed framework in both single-timescale and two-timescale cases. We show that our proposed framework encompasses a wide range of well-known SGD-type methods, including heavy-ball SGD, SignSGD, Lion, normalized SGD and clipped SGD. Furthermore, when the objective function adopts a finite-sum formulation, we prove the convergence properties for these SGD-type methods based on our proposed framework. In particular, we prove that these SGD-type methods find the Clarke stationary points of the objective function with randomly chosen stepsizes and initial points under mild assumptions. Preli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24120;&#35265;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#27979;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#24110;&#21161;&#20174;&#19994;&#32773;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.02694</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24230;&#37327;&#26041;&#27861;&#65306;&#19968;&#39033;&#35780;&#35770;
&lt;/p&gt;
&lt;p&gt;
Loss Functions and Metrics in Deep Learning. A Review. (arXiv:2307.02694v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24120;&#35265;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#27979;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#24110;&#21161;&#20174;&#19994;&#32773;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#36873;&#25321;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#27169;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#24230;&#37327;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24120;&#35265;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#27979;&#37327;&#26041;&#27861;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#27599;&#31181;&#25216;&#26415;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#20030;&#20363;&#35828;&#26126;&#23427;&#20204;&#22312;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#35780;&#35770;&#26088;&#22312;&#20840;&#38754;&#20102;&#35299;&#26368;&#24120;&#35265;&#30340;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#20351;&#29992;&#30340;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#25351;&#26631;&#65292;&#24182;&#24110;&#21161;&#20174;&#19994;&#32773;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the essential components of deep learning is the choice of the loss function and performance metrics used to train and evaluate models. This paper reviews the most prevalent loss functions and performance measurements in deep learning. We examine the benefits and limits of each technique and illustrate their application to various deep-learning problems. Our review aims to give a comprehensive picture of the different loss functions and performance indicators used in the most common deep learning tasks and help practitioners choose the best method for their specific task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;S.T.A.R.-Track&#65292;&#19968;&#20010;&#37319;&#29992;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;Transformer&#26694;&#26550;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;3D&#29289;&#20307;&#36319;&#36394;&#12290;&#36890;&#36807;&#26032;&#39062;&#30340;&#28508;&#22312;&#36816;&#21160;&#27169;&#22411;&#21644;&#23398;&#20064;&#22411;&#36319;&#36394;&#23884;&#20837;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20934;&#30830;&#24314;&#27169;&#29289;&#20307;&#30340;&#20960;&#20309;&#36816;&#21160;&#21644;&#21464;&#21270;&#65292;&#24182;&#22312;nuScenes&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.17602</link><description>&lt;p&gt;
S.T.A.R.-Track&#65306;&#33258;&#36866;&#24212;&#26102;&#31354;&#22806;&#35980;&#34920;&#31034;&#30340;&#31471;&#21040;&#31471;3D&#29289;&#20307;&#36319;&#36394;&#30340;&#28508;&#22312;&#36816;&#21160;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
S.T.A.R.-Track: Latent Motion Models for End-to-End 3D Object Tracking with Adaptive Spatio-Temporal Appearance Representations. (arXiv:2306.17602v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;S.T.A.R.-Track&#65292;&#19968;&#20010;&#37319;&#29992;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;Transformer&#26694;&#26550;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;3D&#29289;&#20307;&#36319;&#36394;&#12290;&#36890;&#36807;&#26032;&#39062;&#30340;&#28508;&#22312;&#36816;&#21160;&#27169;&#22411;&#21644;&#23398;&#20064;&#22411;&#36319;&#36394;&#23884;&#20837;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20934;&#30830;&#24314;&#27169;&#29289;&#20307;&#30340;&#20960;&#20309;&#36816;&#21160;&#21644;&#21464;&#21270;&#65292;&#24182;&#22312;nuScenes&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#36319;&#36394;-&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#22522;&#20110;Transformer&#30340;3D&#36319;&#36394;&#26694;&#26550;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#36319;&#36394;&#26041;&#27861;&#36890;&#36807;&#20960;&#20309;&#36816;&#21160;&#27169;&#22411;&#34701;&#21512;&#24103;&#20043;&#38388;&#30340;&#29289;&#20307;&#21644;&#33258;&#36816;&#21160;&#30340;&#20960;&#20309;&#25928;&#24212;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;S.T.A.R.-Track&#65292;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#28508;&#22312;&#36816;&#21160;&#27169;&#22411;&#26469;&#35843;&#25972;&#23545;&#35937;&#26597;&#35810;&#65292;&#20197;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#30452;&#25509;&#32771;&#34385;&#35270;&#35282;&#21644;&#20809;&#29031;&#26465;&#20214;&#30340;&#21464;&#21270;&#65292;&#21516;&#26102;&#26126;&#30830;&#24314;&#27169;&#20960;&#20309;&#36816;&#21160;&#12290;&#32467;&#21512;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#23398;&#20064;&#30340;&#36319;&#36394;&#23884;&#20837;&#65292;&#26377;&#21161;&#20110;&#24314;&#27169;&#36712;&#36857;&#30340;&#23384;&#22312;&#27010;&#29575;&#65292;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#36319;&#36394;&#26694;&#26550;&#65292;&#21487;&#20197;&#19982;&#20219;&#20309;&#22522;&#20110;&#26597;&#35810;&#30340;&#26816;&#27979;&#22120;&#38598;&#25104;&#12290;&#22312;nuScenes&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#23637;&#31034;&#20102;&#22522;&#20110;DETR3D&#30340;&#36319;&#36394;&#22120;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#21516;&#26102;&#22823;&#22823;&#20943;&#23569;&#20102;&#36712;&#36857;&#30340;&#36523;&#20221;&#36716;&#25442;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following the tracking-by-attention paradigm, this paper introduces an object-centric, transformer-based framework for tracking in 3D. Traditional model-based tracking approaches incorporate the geometric effect of object- and ego motion between frames with a geometric motion model. Inspired by this, we propose S.T.A.R.-Track, which uses a novel latent motion model (LMM) to additionally adjust object queries to account for changes in viewing direction and lighting conditions directly in the latent space, while still modeling the geometric motion explicitly. Combined with a novel learnable track embedding that aids in modeling the existence probability of tracks, this results in a generic tracking framework that can be integrated with any query-based detector. Extensive experiments on the nuScenes benchmark demonstrate the benefits of our approach, showing state-of-the-art performance for DETR3D-based trackers while drastically reducing the number of identity switches of tracks at the s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#37319;&#26679;&#22120;&#30340;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#26041;&#27861;&#65292;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#19979;&#33021;&#22815;&#23454;&#29616;&#38271;&#21608;&#26399;&#21463;&#32422;&#26463;&#30340;&#25805;&#20316;&#35745;&#21010;&#12290;</title><link>http://arxiv.org/abs/2306.13196</link><description>&lt;p&gt;
DiMSam:&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#37096;&#20998;&#21487;&#35266;&#27979;&#20219;&#21153;&#19982;&#21160;&#20316;&#35268;&#21010;&#20013;&#30340;&#37319;&#26679;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
DiMSam: Diffusion Models as Samplers for Task and Motion Planning under Partial Observability. (arXiv:2306.13196v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#37319;&#26679;&#22120;&#30340;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#26041;&#27861;&#65292;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#19979;&#33021;&#22815;&#23454;&#29616;&#38271;&#21608;&#26399;&#21463;&#32422;&#26463;&#30340;&#25805;&#20316;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#65288;TAMP&#65289;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#22320;&#35745;&#21010;&#38271;&#21608;&#26399;&#33258;&#20027;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#23427;&#20204;&#38656;&#35201;&#19968;&#20010;&#35268;&#21010;&#27169;&#22411;&#65292;&#22240;&#27492;&#22312;&#29615;&#22659;&#21644;&#20854;&#21160;&#24577;&#19981;&#23436;&#20840;&#20102;&#35299;&#30340;&#39046;&#22495;&#20013;&#24212;&#29992;&#23427;&#20204;&#21487;&#33021;&#38750;&#24120;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#65292;&#29305;&#21035;&#26159;&#25193;&#25955;&#27169;&#22411;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#23398;&#20064;&#25429;&#33719;&#35268;&#21010;&#27169;&#22411;&#20013;&#38590;&#20197;&#35774;&#35745;&#30340;&#32422;&#26463;&#21644;&#37319;&#26679;&#22120;&#12290;&#36825;&#20123;&#23398;&#20064;&#37319;&#26679;&#22120;&#22312;TAMP&#27714;&#35299;&#22120;&#20013;&#32452;&#21512;&#21644;&#21512;&#24182;&#65292;&#20197;&#32852;&#21512;&#25214;&#21040;&#28385;&#36275;&#35268;&#21010;&#20013;&#32422;&#26463;&#30340;&#34892;&#21160;&#21442;&#25968;&#20540;&#12290;&#20026;&#20102;&#20415;&#20110;&#23545;&#29615;&#22659;&#20013;&#26410;&#30693;&#23545;&#35937;&#36827;&#34892;&#39044;&#27979;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#37319;&#26679;&#22120;&#23450;&#20041;&#20026;&#23398;&#20064;&#30340;&#20302;&#32500;&#28508;&#21464;&#37327;&#23884;&#20837;&#30340;&#21487;&#21464;&#23545;&#35937;&#29366;&#24577;&#12290;&#25105;&#20204;&#22312;&#20851;&#33410;&#24335;&#29289;&#20307;&#25805;&#20316;&#39046;&#22495;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#32463;&#20856;TAMP&#12289;&#29983;&#25104;&#23398;&#20064;&#21644;&#28508;&#22312;&#23884;&#20837;&#30340;&#32452;&#21512;&#22914;&#20309;&#20351;&#24471;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#19979;&#36827;&#34892;&#38271;&#21608;&#26399;&#21463;&#32422;&#26463;&#30340;&#25805;&#20316;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task and Motion Planning (TAMP) approaches are effective at planning long-horizon autonomous robot manipulation. However, because they require a planning model, it can be difficult to apply them to domains where the environment and its dynamics are not fully known. We propose to overcome these limitations by leveraging deep generative modeling, specifically diffusion models, to learn constraints and samplers that capture these difficult-to-engineer aspects of the planning model. These learned samplers are composed and combined within a TAMP solver in order to find action parameter values jointly that satisfy the constraints along a plan. To tractably make predictions for unseen objects in the environment, we define these samplers on low-dimensional learned latent embeddings of changing object state. We evaluate our approach in an articulated object manipulation domain and show how the combination of classical TAMP, generative learning, and latent embeddings enables long-horizon constra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24490;&#29615;&#35760;&#24518;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;RMDT&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38271;&#24207;&#21015;&#38382;&#39064;&#12290;&#22312;Atari&#28216;&#25103;&#21644;MoJoCo&#25511;&#21046;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#37319;&#29992;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;RMDT&#27169;&#22411;&#26174;&#30528;&#20248;&#20110;&#20854;&#27809;&#26377;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;&#23545;&#24212;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.09459</link><description>&lt;p&gt;
&#24490;&#29615;&#35760;&#24518;&#20915;&#31574;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Recurrent Memory Decision Transformer. (arXiv:2306.09459v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24490;&#29615;&#35760;&#24518;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;RMDT&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38271;&#24207;&#21015;&#38382;&#39064;&#12290;&#22312;Atari&#28216;&#25103;&#21644;MoJoCo&#25511;&#21046;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#37319;&#29992;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;RMDT&#27169;&#22411;&#26174;&#30528;&#20248;&#20110;&#20854;&#27809;&#26377;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;&#23545;&#24212;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#38761;&#24615;&#27169;&#22411;&#26368;&#21021;&#26159;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#32780;&#24320;&#21457;&#30340;&#65292;&#26368;&#36817;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#36825;&#26159;&#22240;&#20026;&#20195;&#29702;&#30340;&#21382;&#21490;&#21487;&#20197;&#34920;&#31034;&#20026;&#24207;&#21015;&#65292;&#24182;&#19988;&#25972;&#20010;&#20219;&#21153;&#21487;&#20197;&#32553;&#20943;&#20026;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#21464;&#21387;&#22120;&#25805;&#20316;&#30340;&#20108;&#27425;&#22797;&#26434;&#24615;&#38480;&#21046;&#20102;&#19978;&#19979;&#25991;&#30340;&#28508;&#22312;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#20351;&#29992;&#20102;&#19981;&#21516;&#29256;&#26412;&#30340;&#35760;&#24518;&#26426;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24490;&#29615;&#35760;&#24518;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;RMDT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#20351;&#29992;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;Atari&#28216;&#25103;&#21644;MoJoCo&#25511;&#21046;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#39564;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;Atari&#28216;&#25103;&#19978;&#26174;&#30528;&#20248;&#20110;&#27809;&#26377;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;&#23545;&#24212;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#20180;&#32454;&#30740;&#31350;&#20102;&#35760;&#24518;&#23545;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#32489;&#25928;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#24320;&#21457;&#26356;&#39640;&#25928;&#21644;&#26356;&#26377;&#25928;&#30340;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformative models, originally developed for natural language problems, have recently been widely used in offline reinforcement learning tasks. This is due to the fact that the agent's history can be represented as a sequence, and the whole task can be reduced to the sequence modeling task. However, the quadratic complexity of the transformer operation limits the potential increase in context. Therefore, to work with long sequences in a natural language, different versions of the memory mechanism are used. In this paper, we propose the Recurrent Memory Decision Transformer (RMDT), a model that uses a recurrent memory mechanism for reinforcement learning problems. We conduct thorough experiments on Atari games and MoJoCo control problems, and show that our proposed model is significantly superior to its counterparts without the recurrent memory mechanism on Atari games. We also carefully study the effect of memory on the performance of the proposed model. These findings shed light on
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#36890;&#36807;&#20998;&#24067;&#20581;&#22766;&#20248;&#21270;&#21644;&#37325;&#35201;&#24615;&#21152;&#26435;&#30340;&#26799;&#24230;&#19979;&#38477;&#25216;&#26415;&#25552;&#21319;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.09222</link><description>&lt;p&gt;
&#38543;&#26426;&#21152;&#26435;&#26799;&#24230;&#19979;&#38477;&#36890;&#36807;&#20998;&#24067;&#20581;&#22766;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Stochastic Re-weighted Gradient Descent via Distributionally Robust Optimization. (arXiv:2306.09222v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09222
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#20998;&#24067;&#20581;&#22766;&#20248;&#21270;&#21644;&#37325;&#35201;&#24615;&#21152;&#26435;&#30340;&#26799;&#24230;&#19979;&#38477;&#25216;&#26415;&#25552;&#21319;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#22312;&#27599;&#19968;&#27425;&#20248;&#21270;&#27493;&#39588;&#20013;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#37325;&#35201;&#24615;&#21152;&#26435;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#30340;&#21152;&#26435;&#26799;&#24230;&#19979;&#38477;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21463;&#21040;&#20998;&#24067;&#20581;&#22766;&#20248;&#21270;&#21644;f-&#25955;&#24230;&#30340;&#21551;&#21457;&#65292;&#24050;&#30693;&#21487;&#20197;&#24471;&#21040;&#20855;&#26377;&#25913;&#36827;&#30340;&#27867;&#21270;&#20445;&#35777;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#21152;&#26435;&#26041;&#26696;&#31616;&#21333;&#12289;&#35745;&#31639;&#39640;&#25928;&#65292;&#21487;&#20197;&#19982;&#35768;&#22810;&#27969;&#34892;&#30340;&#20248;&#21270;&#31639;&#27861;&#65288;&#22914;SGD&#21644;Adam&#65289;&#32467;&#21512;&#20351;&#29992;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#37117;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#33021;&#65292;&#21253;&#25324;&#30417;&#30563;&#23398;&#20064;&#21644;&#39046;&#22495;&#36866;&#24212;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;DomainBed&#21644;Tabular&#20998;&#31867;&#22522;&#20934;&#19978;&#20998;&#21035;&#27604;&#29616;&#26377;&#26368;&#20339;&#32467;&#26524;&#25552;&#21319;&#20102;0.7%&#21644;1.44%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23558;BERT&#22312;GLUE&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#20102;1.94%&#65292;&#23558;ViT&#22312;ImageNet-1K&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#20102;1.01%&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#39044;&#31034;&#30528;&#23427;&#22312;&#25913;&#21892;&#24615;&#33021;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a re-weighted gradient descent technique for boosting the performance of deep neural networks, which involves importance weighting of data points during each optimization step. Our approach is inspired by distributionally robust optimization with f-divergences, which has been known to result in models with improved generalization guarantees. Our re-weighting scheme is simple, computationally efficient, and can be combined with many popular optimization algorithms such as SGD and Adam. Empirically, we demonstrate the superiority of our approach on various tasks, including supervised learning, domain adaptation. Notably, we obtain improvements of +0.7% and +1.44% over SOTA on DomainBed and Tabular classification benchmarks, respectively. Moreover, our algorithm boosts the performance of BERT on GLUE benchmarks by +1.94%, and ViT on ImageNet-1K by +1.01%. These results demonstrate the effectiveness of the proposed approach, indicating its potential for improving performance in 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28145;&#24230;Q&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21098;&#26525;&#21160;&#20316;&#38598;&#26469;&#23454;&#29616;&#23558;&#20013;&#38388;&#29983;&#29289;&#26631;&#24535;&#29289;&#20449;&#21495;&#25972;&#21512;&#21040;&#22870;&#21169;&#35268;&#33539;&#20013;&#65292;&#25552;&#39640;&#20102;&#37325;&#30151;&#25252;&#29702;&#31574;&#30053;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08044</link><description>&lt;p&gt;
&#21098;&#26525;&#26041;&#24335;&#25552;&#39640;&#21487;&#38752;&#31574;&#30053;&#65306;&#19968;&#31181;&#22810;&#30446;&#26631;&#28145;&#24230;Q&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#37325;&#30151;&#25252;&#29702;
&lt;/p&gt;
&lt;p&gt;
Pruning the Way to Reliable Policies: A Multi-Objective Deep Q-Learning Approach to Critical Care. (arXiv:2306.08044v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08044
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28145;&#24230;Q&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21098;&#26525;&#21160;&#20316;&#38598;&#26469;&#23454;&#29616;&#23558;&#20013;&#38388;&#29983;&#29289;&#26631;&#24535;&#29289;&#20449;&#21495;&#25972;&#21512;&#21040;&#22870;&#21169;&#35268;&#33539;&#20013;&#65292;&#25552;&#39640;&#20102;&#37325;&#30151;&#25252;&#29702;&#31574;&#30053;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#21307;&#30103;&#20915;&#31574;&#20855;&#26377;&#36830;&#32493;&#24615;&#65292;&#22240;&#27492;&#65292;&#24378;&#21270;&#23398;&#20064;&#21487;&#33021;&#26377;&#26395;&#21046;&#23450;&#31934;&#30830;&#30340;&#25968;&#25454;&#39537;&#21160;&#27835;&#30103;&#35745;&#21010;&#12290;&#28982;&#32780;&#65292;&#35813;&#39046;&#22495;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#20027;&#35201;&#22522;&#20110;&#27515;&#20129;&#29575;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#31232;&#30095;&#24615;&#65292;&#23548;&#33268;&#31163;&#32447;&#20272;&#35745;&#30340;&#31283;&#23450;&#24615;&#38477;&#20302;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#28145;&#24230;Q&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#33719;&#24471;&#26356;&#21487;&#38752;&#30340;&#37325;&#30151;&#25252;&#29702;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#23558;&#30456;&#20851;&#20294;&#22024;&#26434;&#30340;&#20013;&#38388;&#29983;&#29289;&#26631;&#24535;&#29289;&#20449;&#21495;&#25972;&#21512;&#21040;&#22870;&#21169;&#35268;&#33539;&#20013;&#65292;&#21516;&#26102;&#19981;&#20250;&#25439;&#23475;&#24863;&#20852;&#36259;&#30340;&#20027;&#35201;&#32467;&#26524;&#65288;&#20363;&#22914;&#24739;&#32773;&#29983;&#23384;&#29575;&#65289;&#30340;&#20248;&#21270;&#12290;&#36890;&#36807;&#26681;&#25454;&#25152;&#26377;&#21487;&#29992;&#22870;&#21169;&#23545;&#21160;&#20316;&#38598;&#36827;&#34892;&#21098;&#26525;&#65292;&#28982;&#21518;&#22522;&#20110;&#31232;&#30095;&#20027;&#35201;&#22870;&#21169;&#65292;&#20351;&#29992;&#21463;&#38480;&#21160;&#20316;&#38598;&#36827;&#34892;&#26368;&#32456;&#27169;&#22411;&#35757;&#32451;&#65292;&#36890;&#36807;&#35299;&#31163;&#20934;&#30830;&#21644;&#36817;&#20284;&#22870;&#21169;&#26469;&#26368;&#23567;&#21270;&#20027;&#35201;&#30446;&#26631;&#30340;&#28508;&#22312;&#25197;&#26354;&#65292;&#23454;&#29616;&#20102;&#19978;&#36848;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most medical treatment decisions are sequential in nature. Hence, there is substantial hope that reinforcement learning may make it possible to formulate precise data-driven treatment plans. However, a key challenge for most applications in this field is the sparse nature of primarily mortality-based reward functions, leading to decreased stability of offline estimates. In this work, we introduce a deep Q-learning approach able to obtain more reliable critical care policies. This method integrates relevant but noisy intermediate biomarker signals into the reward specification, without compromising the optimization of the main outcome of interest (e.g. patient survival). We achieve this by first pruning the action set based on all available rewards, and second training a final model based on the sparse main reward but with a restricted action set. By disentangling accurate and approximated rewards through action pruning, potential distortions of the main objective are minimized, all whi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;MuZero&#31639;&#27861;&#65292;&#21457;&#29616;&#23427;&#23398;&#20064;&#21040;&#30340;&#27169;&#22411;&#26080;&#27861;&#26377;&#25928;&#25512;&#24191;&#21040;&#35780;&#20272;&#26410;&#35265;&#31574;&#30053;&#65292;&#38480;&#21046;&#20102;&#20854;&#23545;&#24403;&#21069;&#31574;&#30053;&#30340;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.00840</link><description>&lt;p&gt;
MuZero&#23398;&#21040;&#20102;&#20160;&#20040;&#27169;&#22411;&#65311;
&lt;/p&gt;
&lt;p&gt;
What model does MuZero learn?. (arXiv:2306.00840v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;MuZero&#31639;&#27861;&#65292;&#21457;&#29616;&#23427;&#23398;&#20064;&#21040;&#30340;&#27169;&#22411;&#26080;&#27861;&#26377;&#25928;&#25512;&#24191;&#21040;&#35780;&#20272;&#26410;&#35265;&#31574;&#30053;&#65292;&#38480;&#21046;&#20102;&#20854;&#23545;&#24403;&#21069;&#31574;&#30053;&#30340;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#26377;&#26395;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#24403;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26102;&#65292;&#26377;&#21487;&#33021;&#20174;&#22797;&#26434;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#32039;&#20945;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#23398;&#20064;&#21040;&#30340;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#23427;&#20204;&#35268;&#21010;&#33021;&#21147;&#30340;&#25552;&#21319;&#24403;&#21069;&#31574;&#30053;&#30340;&#33021;&#21147;&#65292;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;MuZero&#36825;&#20010;&#33879;&#21517;&#30340;&#22522;&#20110;&#28145;&#24230;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#22312;&#23454;&#29616;&#20540;&#31561;&#20215;&#27169;&#22411;&#30340;&#23398;&#20064;&#30446;&#26631;&#19978;&#30340;&#25104;&#23601;&#20197;&#21450;&#23398;&#20064;&#21040;&#30340;&#27169;&#22411;&#23545;&#31574;&#30053;&#25913;&#36827;&#30340;&#23454;&#29992;&#24615;&#12290;&#22312;&#35832;&#22810;&#20854;&#20182;&#35266;&#28857;&#20013;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65306;MuZero&#23398;&#21040;&#30340;&#27169;&#22411;&#26080;&#27861;&#26377;&#25928;&#22320;&#25512;&#24191;&#21040;&#35780;&#20272;&#26410;&#35265;&#31574;&#30053;&#65292;&#36825;&#38480;&#21046;&#20102;&#25105;&#20204;&#36890;&#36807;&#27169;&#22411;&#35268;&#21010;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#24403;&#21069;&#31574;&#30053;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based reinforcement learning has drawn considerable interest in recent years, given its promise to improve sample efficiency. Moreover, when using deep-learned models, it is potentially possible to learn compact models from complex sensor data. However, the effectiveness of these learned models, particularly their capacity to plan, i.e., to improve the current policy, remains unclear. In this work, we study MuZero, a well-known deep model-based reinforcement learning algorithm, and explore how far it achieves its learning objective of a value-equivalent model and how useful the learned models are for policy improvement. Amongst various other insights, we conclude that the model learned by MuZero cannot effectively generalize to evaluate unseen policies, which limits the extent to which we can additionally improve the current policy by planning with the model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#22312;&#25104;&#23545;&#22270;&#20687;&#36716;&#25442;&#25216;&#26415;&#19978;&#30340;&#36827;&#23637;&#30340;&#65292;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;LArTPC&#25506;&#27979;&#22120;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#20043;&#38388;&#30340;&#31995;&#32479;&#24046;&#24322;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.12858</link><description>&lt;p&gt;
&#38754;&#21521;&#31185;&#23398;&#30340;&#26080;&#30417;&#30563;&#22495;&#36716;&#31227;&#65306;&#25506;&#32034;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#20855;&#26377;&#19981;&#21516;&#21709;&#24212;&#27169;&#22411;&#30340;LArTPC&#25506;&#27979;&#22120;&#27169;&#25311;&#20043;&#38388;&#30340;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Domain Transfer for Science: Exploring Deep Learning Methods for Translation between LArTPC Detector Simulations with Differing Response Models. (arXiv:2304.12858v1 [hep-ex])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#22312;&#25104;&#23545;&#22270;&#20687;&#36716;&#25442;&#25216;&#26415;&#19978;&#30340;&#36827;&#23637;&#30340;&#65292;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;LArTPC&#25506;&#27979;&#22120;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#20043;&#38388;&#30340;&#31995;&#32479;&#24046;&#24322;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#31185;&#23398;&#20013;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#23547;&#27714;&#31616;&#21270;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#21644;&#21457;&#29616;&#30340;&#36335;&#24452;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;DL&#27169;&#22411;&#32463;&#24120;&#22312;&#27169;&#25311;&#32467;&#26524;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#24212;&#29992;&#20110;&#30495;&#23454;&#23454;&#39564;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#20043;&#38388;&#30340;&#20219;&#20309;&#31995;&#32479;&#24046;&#24322;&#21487;&#33021;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#36825;&#31181;&#25928;&#24212;&#31216;&#20026;&#8220;&#39046;&#22495;&#28418;&#31227;&#8221;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#31995;&#32479;&#24046;&#24322;&#30340;&#29609;&#20855;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#26041;&#27861;&#26469;&#20943;&#23569;&#20004;&#20010;&#31995;&#32479;&#19981;&#21516;&#30340;&#26679;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#26368;&#36817;&#22312;&#25104;&#23545;&#22270;&#20687;&#36716;&#25442;&#25216;&#26415;&#19978;&#30340;&#36827;&#23637;&#65292;&#24182;&#22312;&#20004;&#32452;&#27169;&#25311;&#28082;&#27689;&#26102;&#38388;&#25237;&#24433;&#23460;&#65288;LArTPC&#65289;&#25506;&#27979;&#22120;&#20107;&#20214;&#26679;&#26412;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#36825;&#20123;&#26679;&#26412;&#34987;&#21019;&#24314;&#20197;&#25511;&#21046;&#22320;&#28436;&#31034;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#20043;&#38388;&#30340;&#24120;&#35265;&#31995;&#32479;&#24046;&#24322;&#12290;LArTPC&#25506;&#27979;&#22120;&#20195;&#34920;&#20102;&#19979;&#19968;&#20195;&#31890;&#23376;&#25506;&#27979;&#22120;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) techniques have broad applications in science, especially in seeking to streamline the pathway to potential solutions and discoveries. Frequently, however, DL models are trained on the results of simulation yet applied to real experimental data. As such, any systematic differences between the simulated and real data may degrade the model's performance -- an effect known as "domain shift." This work studies a toy model of the systematic differences between simulated and real data. It presents a fully unsupervised, task-agnostic method to reduce differences between two systematically different samples. The method is based on the recent advances in unpaired image-to-image translation techniques and is validated on two sets of samples of simulated Liquid Argon Time Projection Chamber (LArTPC) detector events, created to illustrate common systematic differences between the simulated and real data in a controlled way. LArTPC-based detectors represent the next-generation pa
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#31574;&#30053;&#25552;&#21319;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#22312;&#32447;&#26041;&#27861;&#30340;&#31574;&#30053;&#25552;&#21319;&#20445;&#35777;&#21644;&#31163;&#32447;&#31574;&#30053;&#31639;&#27861;&#36890;&#36807;&#26679;&#26412;&#37325;&#29992;&#26377;&#25928;&#21033;&#29992;&#25968;&#25454;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2206.13714</link><description>&lt;p&gt;
&#24102;&#29702;&#35770;&#25903;&#25345;&#30340;&#26679;&#26412;&#37325;&#29992;&#30340;&#24191;&#20041;&#31574;&#30053;&#25552;&#21319;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Generalized Policy Improvement Algorithms with Theoretically Supported Sample Reuse. (arXiv:2206.13714v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.13714
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#31574;&#30053;&#25552;&#21319;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#22312;&#32447;&#26041;&#27861;&#30340;&#31574;&#30053;&#25552;&#21319;&#20445;&#35777;&#21644;&#31163;&#32447;&#31574;&#30053;&#31639;&#27861;&#36890;&#36807;&#26679;&#26412;&#37325;&#29992;&#26377;&#25928;&#21033;&#29992;&#25968;&#25454;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#23398;&#20064;&#25511;&#21046;&#26041;&#27861;&#20855;&#26377;&#25913;&#21892;&#22797;&#26434;&#31995;&#32479;&#36816;&#34892;&#30340;&#28508;&#21147;&#65292;&#32780;&#22522;&#20110;&#27169;&#22411;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#34920;&#20102;&#19968;&#31181;&#27969;&#34892;&#30340;&#25968;&#25454;&#39537;&#21160;&#25511;&#21046;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31639;&#27861;&#31867;&#21035;&#22312;&#23454;&#38469;&#25511;&#21046;&#37096;&#32626;&#30340;&#20004;&#20010;&#37325;&#35201;&#35201;&#27714;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65306;&#65288;i&#65289;&#23454;&#38469;&#24615;&#33021;&#20445;&#35777;&#21644;&#65288;ii&#65289;&#25968;&#25454;&#25928;&#29575;&#12290;&#31163;&#32447;&#31574;&#30053;&#31639;&#27861;&#36890;&#36807;&#26679;&#26412;&#37325;&#29992;&#26377;&#25928;&#21033;&#29992;&#25968;&#25454;&#65292;&#20294;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#65292;&#32780;&#22312;&#32447;&#31574;&#30053;&#31639;&#27861;&#20445;&#35777;&#20102;&#35757;&#32451;&#26399;&#38388;&#30340;&#36817;&#20284;&#31574;&#30053;&#25913;&#36827;&#65292;&#20294;&#21463;&#21040;&#39640;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#24179;&#34913;&#36825;&#20123;&#31454;&#20105;&#30446;&#26631;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31867;&#24191;&#20041;&#31574;&#30053;&#25552;&#21319;&#31639;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#22312;&#32447;&#26041;&#27861;&#30340;&#31574;&#30053;&#25552;&#21319;&#20445;&#35777;&#21644;&#26679;&#26412;&#37325;&#29992;&#30340;&#25928;&#29575;&#12290;&#36890;&#36807;&#23545;&#26469;&#33258;DeepMind C&#30340;&#22810;&#31181;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#36827;&#34892; extensive &#30340;&#23454;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26032;&#31867;&#31639;&#27861;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven, learning-based control methods offer the potential to improve operations in complex systems, and model-free deep reinforcement learning represents a popular approach to data-driven control. However, existing classes of algorithms present a trade-off between two important deployment requirements for real-world control: (i) practical performance guarantees and (ii) data efficiency. Off-policy algorithms make efficient use of data through sample reuse but lack theoretical guarantees, while on-policy algorithms guarantee approximate policy improvement throughout training but suffer from high sample complexity. In order to balance these competing goals, we develop a class of Generalized Policy Improvement algorithms that combines the policy improvement guarantees of on-policy methods with the efficiency of sample reuse. We demonstrate the benefits of this new class of algorithms through extensive experimental analysis on a variety of continuous control tasks from the DeepMind C
&lt;/p&gt;</description></item></channel></rss>