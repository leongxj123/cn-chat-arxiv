<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#26500;&#25104;&#30340;&#20195;&#29702;&#26694;&#26550;TrustAgent&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#39044;&#20808;&#35268;&#21010;&#12289;&#35268;&#21010;&#36807;&#31243;&#20013;&#21644;&#35745;&#21010;&#21518;&#26816;&#26597;&#19977;&#31181;&#31574;&#30053;&#26469;&#25552;&#39640;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#21644;&#39044;&#38450;&#28508;&#22312;&#21361;&#38505;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#23433;&#20840;&#24615;&#19982;&#20351;&#29992;&#32773;&#28385;&#24847;&#24230;&#20197;&#21450;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#19982;&#25928;&#29575;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01586</link><description>&lt;p&gt;
TrustAgent: &#36890;&#36807;&#20195;&#29702;&#26500;&#25104;&#23454;&#29616;&#23433;&#20840;&#21487;&#20449;&#36182;&#30340;LLM&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent Constitution
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#26500;&#25104;&#30340;&#20195;&#29702;&#26694;&#26550;TrustAgent&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#39044;&#20808;&#35268;&#21010;&#12289;&#35268;&#21010;&#36807;&#31243;&#20013;&#21644;&#35745;&#21010;&#21518;&#26816;&#26597;&#19977;&#31181;&#31574;&#30053;&#26469;&#25552;&#39640;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#21644;&#39044;&#38450;&#28508;&#22312;&#21361;&#38505;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#23433;&#20840;&#24615;&#19982;&#20351;&#29992;&#32773;&#28385;&#24847;&#24230;&#20197;&#21450;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#19982;&#25928;&#29575;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#20854;&#21487;&#20449;&#24230;&#20173;&#26410;&#24471;&#21040;&#28145;&#20837;&#25506;&#32034;&#12290;&#30001;&#20110;&#20195;&#29702;&#21487;&#20197;&#30452;&#25509;&#19982;&#29289;&#29702;&#29615;&#22659;&#20132;&#20114;&#65292;&#20854;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#26500;&#25104;&#30340;&#20195;&#29702;&#26694;&#26550;TrustAgent&#65292;&#23545;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#32500;&#24230;&#36827;&#34892;&#20102;&#21021;&#27493;&#30740;&#31350;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19977;&#31181;&#31574;&#30053;&#65306;&#39044;&#20808;&#35268;&#21010;&#31574;&#30053;&#65292;&#22312;&#29983;&#25104;&#35745;&#21010;&#20043;&#21069;&#21521;&#27169;&#22411;&#27880;&#20837;&#23433;&#20840;&#30693;&#35782;&#65307;&#35268;&#21010;&#36807;&#31243;&#20013;&#31574;&#30053;&#65292;&#22312;&#29983;&#25104;&#35745;&#21010;&#26102;&#22686;&#24378;&#23433;&#20840;&#24615;&#65307;&#35745;&#21010;&#21518;&#26816;&#26597;&#31574;&#30053;&#65292;&#36890;&#36807;&#35745;&#21010;&#21518;&#26816;&#26597;&#30830;&#20445;&#23433;&#20840;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22914;&#20309;&#36890;&#36807;&#35782;&#21035;&#21644;&#39044;&#38450;&#28508;&#22312;&#21361;&#38505;&#26377;&#25928;&#25552;&#39640;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#23433;&#20840;&#24615;&#19982;&#20351;&#29992;&#32773;&#28385;&#24847;&#24230;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#20197;&#21450;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#19982;&#20854;&#25928;&#29575;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of LLM-based agents has garnered considerable attention, yet their trustworthiness remains an under-explored area. As agents can directly interact with the physical environment, their reliability and safety is critical. This paper presents an Agent-Constitution-based agent framework, TrustAgent, an initial investigation into improving the safety dimension of trustworthiness in LLM-based agents. This framework consists of threefold strategies: pre-planning strategy which injects safety knowledge to the model prior to plan generation, in-planning strategy which bolsters safety during plan generation, and post-planning strategy which ensures safety by post-planning inspection. Through experimental analysis, we demonstrate how these approaches can effectively elevate an LLM agent's safety by identifying and preventing potential dangers. Furthermore, we explore the intricate relationships between safety and helpfulness, and between the model's reasoning ability and its efficac
&lt;/p&gt;</description></item><item><title>DeFT&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;IO&#24847;&#35782;&#30340;&#26641;&#27880;&#24847;&#21147;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;QKV&#20934;&#22791;&#21644;&#27880;&#24847;&#21147;&#35745;&#31639;&#38454;&#27573;&#23454;&#29616;&#20869;&#23384;&#39640;&#25928;&#30340;&#35745;&#31639;&#65292;&#38477;&#20302;&#20869;&#23384;&#21360;&#35760;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#26641;&#35299;&#30721;&#31574;&#30053;&#21644;&#25512;&#26029;&#31995;&#32479;&#19981;&#36866;&#37197;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.00242</link><description>&lt;p&gt;
DeFT&#65306;&#24102;IO&#24847;&#35782;&#30340;Flash Tree-attention&#29992;&#20110;&#39640;&#25928;&#30340;&#22522;&#20110;&#26641;&#25628;&#32034;&#30340;LLM&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based LLM Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00242
&lt;/p&gt;
&lt;p&gt;
DeFT&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;IO&#24847;&#35782;&#30340;&#26641;&#27880;&#24847;&#21147;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;QKV&#20934;&#22791;&#21644;&#27880;&#24847;&#21147;&#35745;&#31639;&#38454;&#27573;&#23454;&#29616;&#20869;&#23384;&#39640;&#25928;&#30340;&#35745;&#31639;&#65292;&#38477;&#20302;&#20869;&#23384;&#21360;&#35760;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#26641;&#35299;&#30721;&#31574;&#30053;&#21644;&#25512;&#26029;&#31995;&#32479;&#19981;&#36866;&#37197;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26641;&#25628;&#32034;&#36827;&#34892;&#35299;&#30721;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#26029;&#36136;&#37327;&#12290;&#26681;&#25454;&#24341;&#23548;&#20449;&#21495;&#65292;&#23427;&#36890;&#36807;&#24418;&#25104;LLM&#36755;&#20986;&#20174;&#26681;&#21040;&#21494;&#23376;&#30340;&#26368;&#20339;&#36335;&#24452;&#26469;&#25552;&#39640;&#21487;&#25511;&#24615;&#12289;&#25512;&#29702;&#33021;&#21147;&#12289;&#23545;&#40784;&#31561;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35745;&#31639;&#20887;&#20313;&#12289;&#20869;&#23384;&#21344;&#29992;&#21644;&#20869;&#23384;&#35775;&#38382;&#65292;&#24403;&#21069;&#30340;&#26641;&#35299;&#30721;&#31574;&#30053;&#21450;&#20854;&#25512;&#26029;&#31995;&#32479;&#20114;&#30456;&#19981;&#36866;&#37197;&#65292;&#23548;&#33268;&#25512;&#26029;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeFT&#65292;&#19968;&#31181;IO&#24863;&#30693;&#26641;&#27880;&#24847;&#21147;&#31639;&#27861;&#65292;&#23427;&#22312;&#20004;&#20010;&#38454;&#27573;&#20013;&#20445;&#25345;&#20869;&#23384;&#39640;&#25928;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#65292;&#38477;&#20302;&#20869;&#23384;&#21360;&#35760;&#65306;&#65288;1&#65289;QKV&#20934;&#22791;&#65306;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;KV&#24341;&#23548;&#26641;&#20998;&#35010;&#31574;&#30053;&#65292;&#20026;GPU&#30340;&#39640;&#21033;&#29992;&#29575;&#21644;&#23613;&#21487;&#33021;&#20943;&#23569;GPU&#20840;&#23616;&#20869;&#23384;&#21644;&#33455;&#29255;&#19978;&#20849;&#20139;&#20869;&#23384;&#20043;&#38388;&#30340;KV&#32531;&#23384;&#30340;&#20869;&#23384;&#35835;/&#20889;; &#65288;2&#65289;&#27880;&#24847;&#21147;&#35745;&#31639;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00242v1 Announce Type: cross  Abstract: Decoding using tree search can greatly enhance the inference quality for transformer-based Large Language Models (LLMs). Depending on the guidance signal, it searches for the best path from root to leaf in the tree by forming LLM outputs to improve controllability, reasoning ability, alignment, et cetera. However, current tree decoding strategies and their inference systems do not suit each other well due to redundancy in computation, memory footprints, and memory access, resulting in inefficient inference. To address this issue, we propose DeFT, an IO-aware tree attention algorithm that maintains memory-efficient attention calculation with low memory footprints in two stages: (1) QKV Preparation: we propose a KV-Guided Tree Split strategy to group QKV wisely for high utilization of GPUs and reduction of memory reads/writes for the KV cache between GPU global memory and on-chip shared memory as much as possible; (2) Attention Calculati
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#20107;&#23454;&#35299;&#30721;&#26041;&#27861;&#25552;&#39640;&#20102;&#20107;&#23454;&#20934;&#30830;&#24615;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20351;&#27169;&#22411;&#23545;&#24050;&#30693;&#20107;&#23454;&#36807;&#20110;&#33258;&#20449;&#65292;&#36827;&#19968;&#27493;&#35780;&#20272;&#26174;&#31034;&#22312;&#30693;&#35782;&#32534;&#36753;&#22522;&#20934;&#19978;&#25152;&#26377;&#35299;&#30721;&#26041;&#27861;&#22343;&#26174;&#33879;&#38477;&#20302;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00216</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20107;&#23454;&#35299;&#30721;&#65306;&#22312;&#30693;&#35782;&#32534;&#36753;&#22522;&#20934;&#19978;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Is Factuality Decoding a Free Lunch for LLMs? Evaluation on Knowledge Editing Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00216
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#20107;&#23454;&#35299;&#30721;&#26041;&#27861;&#25552;&#39640;&#20102;&#20107;&#23454;&#20934;&#30830;&#24615;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20351;&#27169;&#22411;&#23545;&#24050;&#30693;&#20107;&#23454;&#36807;&#20110;&#33258;&#20449;&#65292;&#36827;&#19968;&#27493;&#35780;&#20272;&#26174;&#31034;&#22312;&#30693;&#35782;&#32534;&#36753;&#22522;&#20934;&#19978;&#25152;&#26377;&#35299;&#30721;&#26041;&#27861;&#22343;&#26174;&#33879;&#38477;&#20302;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#23427;&#20204;&#33021;&#22815;&#20197;&#26356;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#26041;&#24335;&#20256;&#36798;&#20107;&#23454;&#30693;&#35782;&#12290;&#20154;&#20204;&#24050;&#32463;&#20570;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#26469;&#36890;&#36807;&#20462;&#25913;LLMs&#24182;&#38477;&#20302;&#20107;&#23454;&#24187;&#35273;&#26469;&#25552;&#39640;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20462;&#25913;&#20063;&#23384;&#22312;&#38459;&#30861;&#30693;&#35782;&#26356;&#26032;&#30340;&#39118;&#38505;&#65292;&#22240;&#20026;&#23427;&#20204;&#20351;&#27169;&#22411;&#23545;&#24050;&#30693;&#20107;&#23454;&#36807;&#20110;&#33258;&#20449;&#12290;&#26412;&#25991;&#39318;&#20808;&#37325;&#26032;&#23457;&#35270;&#24403;&#21069;&#30340;&#20107;&#23454;&#35299;&#30721;&#26041;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;&#23427;&#20204;&#22312;&#25552;&#39640;&#20107;&#23454;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;&#20960;&#31181;&#24378;&#22823;&#30340;&#20107;&#23454;&#35299;&#30721;&#26041;&#27861;&#22312;&#30693;&#35782;&#32534;&#36753;&#22522;&#20934;&#19978;&#36827;&#34892;&#36827;&#19968;&#27493;&#35780;&#20272;&#12290;&#25152;&#26377;&#36825;&#20123;&#35299;&#30721;&#26041;&#27861;&#19982;&#20854;&#21407;&#22987;&#35299;&#30721;&#30456;&#27604;&#22343;&#26174;&#30528;&#38477;&#20302;&#20102;llama2&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#26368;&#22823;&#30340;&#38477;&#20302;&#24133;&#24230;&#36798;&#21040;&#24778;&#20154;&#30340;81.3\%&#12290;&#36825;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;&#35299;&#30721;&#26041;&#27861;&#20173;&#26080;&#27861;&#23436;&#20840;&#35299;&#20915;&#20107;&#23454;&#24187;&#35273;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#24573;&#35270;&#20102;&#20808;&#39564;&#30693;&#35782;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00216v1 Announce Type: cross  Abstract: The rapid development of large language models (LLMs) enables them to convey factual knowledge in a more human-like fashion. Extensive efforts have been made to reduce factual hallucinations by modifying LLMs with factuality decoding. However, they also pose risks of hindering knowledge updates, as they make models overly confident in known facts. In this work, we first revisite the current factuality decoding methods and verified their effectiveness in enhancing factual accuracy. Subsequently, we conduct further evaluation of several strong factuality decoding methods on the knowledge editing benchmark. All these decoding methods significantly diminish the performance of llama2 models compared to their original decoding, with the largest decrease being a staggering 81.3\%. This further indicates that the current existing decoding methods still cannot perfectly address the factual hallucinations, as they overlook the importance of pres
&lt;/p&gt;</description></item><item><title>&#38656;&#35201;&#22312;&#20154;&#24037;&#31995;&#32479;&#20013;&#24179;&#34913;&#35752;&#35770;&#24847;&#35782;&#30340;&#21487;&#33021;&#23454;&#29616;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#24847;&#35782;&#30340;&#32500;&#24230;&#21644;&#29305;&#24449;&#26469;&#36827;&#34892;&#35752;&#35770;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.20177</link><description>&lt;p&gt;
&#20154;&#24037;&#24847;&#35782;&#12290;&#19968;&#20123;&#36923;&#36753;&#21644;&#27010;&#24565;&#21021;&#27493;
&lt;/p&gt;
&lt;p&gt;
Artificial consciousness. Some logical and conceptual preliminaries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20177
&lt;/p&gt;
&lt;p&gt;
&#38656;&#35201;&#22312;&#20154;&#24037;&#31995;&#32479;&#20013;&#24179;&#34913;&#35752;&#35770;&#24847;&#35782;&#30340;&#21487;&#33021;&#23454;&#29616;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#24847;&#35782;&#30340;&#32500;&#24230;&#21644;&#29305;&#24449;&#26469;&#36827;&#34892;&#35752;&#35770;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20177v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#20154;&#24037;&#24847;&#35782;&#22312;&#29702;&#35770;&#19978;&#26159;&#21542;&#21487;&#33021;&#65311;&#26159;&#21542;&#21512;&#20046;&#24773;&#29702;&#65311;&#22914;&#26524;&#26159;&#65292;&#37027;&#20040;&#25216;&#26415;&#19978;&#21487;&#34892;&#21527;&#65311;&#35201;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26377;&#24517;&#35201;&#22880;&#23450;&#19968;&#20123;&#22522;&#30784;&#65292;&#38416;&#26126;&#20154;&#24037;&#24847;&#35782;&#20135;&#29983;&#30340;&#36923;&#36753;&#21644;&#32463;&#39564;&#26465;&#20214;&#20197;&#21450;&#28041;&#21450;&#30340;&#30456;&#20851;&#26415;&#35821;&#30340;&#21547;&#20041;&#12290;&#24847;&#35782;&#26159;&#19968;&#20010;&#22810;&#20041;&#35789;&#65306;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#65292;&#21253;&#25324;&#31070;&#32463;&#31185;&#23398;&#12289;&#20154;&#24037;&#26234;&#33021;&#12289;&#26426;&#22120;&#20154;&#25216;&#26415;&#21644;&#21746;&#23398;&#31561;&#65292;&#26377;&#26102;&#20250;&#20351;&#29992;&#19981;&#21516;&#26415;&#35821;&#26469;&#25351;&#31216;&#30456;&#21516;&#29616;&#35937;&#65292;&#25110;&#32773;&#20351;&#29992;&#30456;&#21516;&#26415;&#35821;&#26469;&#25351;&#31216;&#19981;&#21516;&#29616;&#35937;&#12290;&#20107;&#23454;&#19978;&#65292;&#22914;&#26524;&#25105;&#20204;&#24819;&#25506;&#35752;&#20154;&#24037;&#24847;&#35782;&#65292;&#23601;&#38656;&#35201;&#24688;&#24403;&#30028;&#23450;&#20851;&#38190;&#27010;&#24565;&#12290;&#22312;&#27492;&#65292;&#32463;&#36807;&#19968;&#20123;&#36923;&#36753;&#21644;&#27010;&#24565;&#21021;&#27493;&#24037;&#20316;&#21518;&#65292;&#25105;&#20204;&#35748;&#20026;&#26377;&#24517;&#35201;&#20351;&#29992;&#24847;&#35782;&#30340;&#32500;&#24230;&#21644;&#29305;&#24449;&#36827;&#34892;&#24179;&#34913;&#35752;&#35770;&#65292;&#25506;&#35752;&#23427;&#20204;&#22312;&#20154;&#24037;&#31995;&#32479;&#20013;&#30340;&#21487;&#33021;&#23454;&#20363;&#21270;&#25110;&#23454;&#29616;&#12290;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20177v1 Announce Type: new  Abstract: Is artificial consciousness theoretically possible? Is it plausible? If so, is it technically feasible? To make progress on these questions, it is necessary to lay some groundwork clarifying the logical and empirical conditions for artificial consciousness to arise and the meaning of relevant terms involved. Consciousness is a polysemic word: researchers from different fields, including neuroscience, Artificial Intelligence, robotics, and philosophy, among others, sometimes use different terms in order to refer to the same phenomena or the same terms to refer to different phenomena. In fact, if we want to pursue artificial consciousness, a proper definition of the key concepts is required. Here, after some logical and conceptual preliminaries, we argue for the necessity of using dimensions and profiles of consciousness for a balanced discussion about their possible instantiation or realisation in artificial systems. Our primary goal in t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#35757;&#32451;&#25216;&#26415;&#65292;&#21363;Attention Interpolation via Diffusion (AID)&#65292;&#36890;&#36807;&#20869;/&#22806;&#25554;&#20540;&#27880;&#24847;&#21147;&#23618;&#12289;&#25554;&#20540;&#27880;&#24847;&#21147;&#19982;&#33258;&#27880;&#24847;&#21147;&#34701;&#21512;&#20197;&#25552;&#39640;&#20445;&#30495;&#24230;&#65292;&#20197;&#21450;&#24212;&#29992;&#36125;&#22612;&#20998;&#24067;&#36827;&#34892;&#36873;&#25321;&#20197;&#22686;&#21152;&#24179;&#28369;&#24230;&#26469;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#25554;&#20540;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.17924</link><description>&lt;p&gt;
AID: &#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#30340;&#27880;&#37325;&#25554;&#20540;
&lt;/p&gt;
&lt;p&gt;
AID: Attention Interpolation of Text-to-Image Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17924
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#35757;&#32451;&#25216;&#26415;&#65292;&#21363;Attention Interpolation via Diffusion (AID)&#65292;&#36890;&#36807;&#20869;/&#22806;&#25554;&#20540;&#27880;&#24847;&#21147;&#23618;&#12289;&#25554;&#20540;&#27880;&#24847;&#21147;&#19982;&#33258;&#27880;&#24847;&#21147;&#34701;&#21512;&#20197;&#25552;&#39640;&#20445;&#30495;&#24230;&#65292;&#20197;&#21450;&#24212;&#29992;&#36125;&#22612;&#20998;&#24067;&#36827;&#34892;&#36873;&#25321;&#20197;&#22686;&#21152;&#24179;&#28369;&#24230;&#26469;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#25554;&#20540;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17924v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#26377;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#22312;&#21508;&#31181;&#35774;&#32622;&#20013;&#21019;&#24314;&#30475;&#19981;&#35265;&#30340;&#22270;&#20687;&#65292;&#26377;&#21161;&#20110;&#22270;&#20687;&#25554;&#20540;&#12290;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#25554;&#20540;&#24050;&#32463;&#24471;&#21040;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#20294;&#26159;&#20855;&#26377;&#29305;&#23450;&#26465;&#20214;&#65288;&#22914;&#25991;&#26412;&#25110;&#23039;&#21183;&#65289;&#30340;&#25554;&#20540;&#21364;&#20102;&#35299;&#19981;&#22810;&#12290;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#22914;&#22312;&#26465;&#20214;&#31354;&#38388;&#20013;&#30340;&#32447;&#24615;&#25554;&#20540;&#65292;&#36890;&#24120;&#20250;&#23548;&#33268;&#22270;&#20687;&#32570;&#20047;&#19968;&#33268;&#24615;&#12289;&#24179;&#28369;&#24230;&#21644;&#20445;&#30495;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#20010;&#21517;&#20026;Attention Interpolation via Diffusion (AID)&#30340;&#26032;&#39062;&#26080;&#35757;&#32451;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;1&#65289;&#25552;&#20986;&#20102;&#19968;&#20010;&#20869;/&#22806;&#25554;&#20540;&#27880;&#24847;&#21147;&#23618;&#65307;2&#65289;&#23558;&#25554;&#20540;&#27880;&#24847;&#21147;&#19982;&#33258;&#27880;&#24847;&#21147;&#34701;&#21512;&#20197;&#25552;&#39640;&#20445;&#30495;&#24230;&#65307;3&#65289;&#24212;&#29992;&#36125;&#22612;&#20998;&#24067;&#36827;&#34892;&#36873;&#25321;&#20197;&#22686;&#21152;&#24179;&#28369;&#24230;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20307;&#65292;Prompt-guided Attention Interpolation via Diffusion (PAID)&#65292;&#23427;&#23558;&#25554;&#20540;&#35270;&#20026;&#20381;&#36182;&#20110;&#26465;&#20214;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21019;&#24314;&#20986;&#26356;&#20855;&#21019;&#36896;&#24615;&#30340;&#26032;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17924v1 Announce Type: cross  Abstract: Conditional diffusion models can create unseen images in various settings, aiding image interpolation. Interpolation in latent spaces is well-studied, but interpolation with specific conditions like text or poses is less understood. Simple approaches, such as linear interpolation in the space of conditions, often result in images that lack consistency, smoothness, and fidelity. To that end, we introduce a novel training-free technique named Attention Interpolation via Diffusion (AID). Our key contributions include 1) proposing an inner/outer interpolated attention layer; 2) fusing the interpolated attention with self-attention to boost fidelity; and 3) applying beta distribution to selection to increase smoothness. We also present a variant, Prompt-guided Attention Interpolation via Diffusion (PAID), that considers interpolation as a condition-dependent generative process. This method enables the creation of new images with greater con
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;LLMs&#36827;&#34892;&#31038;&#20132;&#20114;&#21160;&#30340;&#20840;&#30693;&#27169;&#25311;&#27604;&#38750;&#20840;&#30693;&#27169;&#25311;&#26356;&#23481;&#26131;&#23454;&#29616;&#31038;&#20132;&#30446;&#26631;&#65292;&#23613;&#31649;&#38750;&#20840;&#30693;&#27169;&#25311;&#26356;&#25509;&#36817;&#23454;&#38469;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2403.05020</link><description>&lt;p&gt;
&#27169;&#25311;&#31038;&#20132;&#20114;&#21160;&#25104;&#21151;&#24615;&#30340;&#35823;&#23548;&#24615;&#65306;&#20197;LLMs&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Is this the real life? Is this just fantasy? The Misleading Success of Simulating Social Interactions With LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05020
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;LLMs&#36827;&#34892;&#31038;&#20132;&#20114;&#21160;&#30340;&#20840;&#30693;&#27169;&#25311;&#27604;&#38750;&#20840;&#30693;&#27169;&#25311;&#26356;&#23481;&#26131;&#23454;&#29616;&#31038;&#20132;&#30446;&#26631;&#65292;&#23613;&#31649;&#38750;&#20840;&#30693;&#27169;&#25311;&#26356;&#25509;&#36817;&#23454;&#38469;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#20351;&#24471;&#31038;&#20132;&#27169;&#25311;&#26356;&#21152;&#20016;&#23500;&#65292;&#33021;&#22815;&#20351;&#29992;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#30740;&#31350;&#21508;&#31181;&#31038;&#20132;&#29616;&#35937;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24037;&#20316;&#22312;&#36825;&#20123;&#27169;&#25311;&#20013;&#37319;&#29992;&#20102;&#19968;&#31181;&#20840;&#30693;&#30340;&#36879;&#35270;&#65288;&#20363;&#22914;&#65292;&#21333;&#20010;LLM&#29983;&#25104;&#25152;&#26377;&#20132;&#35848;&#32773;&#65289;&#65292;&#36825;&#19982;&#20154;&#31867;&#20855;&#26377;&#30340;&#38750;&#20840;&#30693;&#12289;&#20449;&#24687;&#19981;&#23545;&#31216;&#30340;&#20114;&#21160;&#26681;&#26412;&#19981;&#31526;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20123;&#24046;&#24322;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#22312;&#21508;&#31181;&#35774;&#23450;&#65288;&#20840;&#30693;&#12289;&#38750;&#20840;&#30693;&#65289;&#20013;&#20351;&#29992;LLMs&#27169;&#25311;&#31038;&#20132;&#20114;&#21160;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36890;&#36807;&#20840;&#30693;&#26041;&#24335;&#27169;&#25311;&#30340;&#20132;&#35848;&#32773;&#22312;&#23454;&#29616;&#31038;&#20132;&#30446;&#26631;&#26041;&#38754;&#27604;&#38750;&#20840;&#30693;&#20195;&#29702;&#20154;&#26356;&#25104;&#21151;&#65292;&#23613;&#31649;&#21518;&#32773;&#26356;&#31526;&#21512;&#29616;&#23454;&#35774;&#32622;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#20174;&#20840;&#30693;&#27169;&#25311;&#20013;&#23398;&#20064;&#21487;&#20197;&#25913;&#21892;&#20132;&#20114;&#30340;&#33258;&#28982;&#24615;&#65292;&#20294;&#22312;&#21512;&#20316;&#22330;&#26223;&#20013;&#20960;&#20046;&#19981;&#33021;&#22686;&#24378;&#30446;&#26631;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05020v1 Announce Type: cross  Abstract: Recent advances in large language models (LLM) have enabled richer social simulations, allowing for the study of various social phenomena with LLM-based agents. However, most work has used an omniscient perspective on these simulations (e.g., single LLM to generate all interlocutors), which is fundamentally at odds with the non-omniscient, information asymmetric interactions that humans have. To examine these differences, we develop an evaluation framework to simulate social interactions with LLMs in various settings (omniscient, non-omniscient). Our experiments show that interlocutors simulated omnisciently are much more successful at accomplishing social goals compared to non-omniscient agents, despite the latter being the more realistic setting. Furthermore, we demonstrate that learning from omniscient simulations improves the apparent naturalness of interactions but scarcely enhances goal achievement in cooperative scenarios. Our f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#23545;&#40784;&#22120;&#27169;&#22411;&#26469;&#35299;&#32806;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#23545;&#40784;&#65292;&#20197;&#20943;&#23569;&#23545;&#40784;&#23545;&#24615;&#33021;&#30340;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.04224</link><description>&lt;p&gt;
Aligners: &#35299;&#32806;LLMs&#21644;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligners: Decoupling LLMs and Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04224
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#23545;&#40784;&#22120;&#27169;&#22411;&#26469;&#35299;&#32806;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#23545;&#40784;&#65292;&#20197;&#20943;&#23569;&#23545;&#40784;&#23545;&#24615;&#33021;&#30340;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38656;&#35201;&#19982;&#20154;&#31867;&#26399;&#26395;&#23545;&#40784;&#65292;&#20197;&#30830;&#20445;&#23427;&#20204;&#22312;&#22823;&#22810;&#25968;&#24212;&#29992;&#20013;&#30340;&#23433;&#20840;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#23545;&#40784;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#25104;&#26412;&#39640;&#26114;&#65292;&#24182;&#19988;&#38656;&#35201;&#20026;&#27599;&#20010;LLM&#21644;&#23545;&#40784;&#26631;&#20934;&#37325;&#22797;&#36827;&#34892;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#35757;&#32451;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#29992;&#20110;&#23545;&#40784;&#32473;&#23450;&#26631;&#20934;&#30340;&#20219;&#20309;LLM&#30340;&#23545;&#40784;&#27169;&#22411;&#26469;&#35299;&#32806;LLMs&#21644;&#23545;&#40784;&#65292;&#20174;&#32780;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20943;&#23569;&#23545;&#24615;&#33021;&#30340;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#23545;&#40784;&#27169;&#22411;&#35757;&#32451;&#37197;&#26041;&#20165;&#20381;&#36182;&#20110;&#20351;&#29992;&#65288;&#25552;&#31034;&#30340;&#65289;LLM &#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#35843;&#25972;&#20197;&#36866;&#24212;&#21508;&#31181;&#23545;&#40784;&#26631;&#20934;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#8220;&#36947;&#24503;&#8221;&#23545;&#40784;&#22120;&#24182;&#22312;&#23454;&#39564;&#19978;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#26469;&#38416;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04224v1 Announce Type: cross  Abstract: Large Language Models (LLMs) need to be aligned with human expectations to ensure their safety and utility in most applications. Alignment is challenging, costly, and needs to be repeated for every LLM and alignment criterion. We propose to decouple LLMs and alignment by training aligner models that can be used to align any LLM for a given criteria on an as-needed basis, thus also reducing the potential negative impacts of alignment on performance. Our recipe for training the aligner models solely relies on synthetic data generated with a (prompted) LLM and can be easily adjusted for a variety of alignment criteria. We illustrate our method by training an "ethical" aligner and verify its efficacy empirically.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;BiasBuster&#26694;&#26550;&#65292;&#29992;&#20110;&#25581;&#31034;&#12289;&#35780;&#20272;&#21644;&#20943;&#36731;LLMs&#20013;&#30340;&#35748;&#30693;&#20559;&#35265;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#24320;&#21457;&#21253;&#21547;16,800&#20010;&#25552;&#31034;&#30340;&#25968;&#25454;&#38598;&#21644;&#27979;&#35797;&#22810;&#31181;&#20559;&#35265;&#32531;&#35299;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;LLMs&#33258;&#36523;&#26469;&#28040;&#38500;&#20854;&#25552;&#31034;&#20013;&#20559;&#35265;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.00811</link><description>&lt;p&gt;
LLM&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#20013;&#30340;&#35748;&#30693;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Cognitive Bias in High-Stakes Decision-Making with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00811
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;BiasBuster&#26694;&#26550;&#65292;&#29992;&#20110;&#25581;&#31034;&#12289;&#35780;&#20272;&#21644;&#20943;&#36731;LLMs&#20013;&#30340;&#35748;&#30693;&#20559;&#35265;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#24320;&#21457;&#21253;&#21547;16,800&#20010;&#25552;&#31034;&#30340;&#25968;&#25454;&#38598;&#21644;&#27979;&#35797;&#22810;&#31181;&#20559;&#35265;&#32531;&#35299;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;LLMs&#33258;&#36523;&#26469;&#28040;&#38500;&#20854;&#25552;&#31034;&#20013;&#20559;&#35265;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25903;&#25345;&#26085;&#30410;&#25193;&#22823;&#30340;&#20915;&#31574;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#22312;&#20154;&#31867;(&#21019;&#36896;&#30340;)&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;LLMs&#21487;&#33021;&#20250;&#32487;&#25215;&#38024;&#23545;&#21463;&#20445;&#25252;&#32676;&#20307;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#21516;&#26102;&#20063;&#21487;&#33021;&#21463;&#21040;&#35748;&#30693;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#36825;&#31181;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#20559;&#35265;&#21487;&#33021;&#20250;&#22952;&#30861;&#21033;&#29992;LLM&#21327;&#21161;&#20570;&#20986;&#20844;&#24179;&#21644;&#21487;&#35299;&#37322;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;BiasBuster&#65292;&#19968;&#20010;&#26088;&#22312;&#25581;&#31034;&#12289;&#35780;&#20272;&#21644;&#20943;&#36731;LLMs&#20013;&#30340;&#35748;&#30693;&#20559;&#35265;&#30340;&#26694;&#26550;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#20219;&#21153;&#20013;&#12290;&#21463;&#24515;&#29702;&#23398;&#21644;&#35748;&#30693;&#31185;&#23398;&#20808;&#21069;&#30740;&#31350;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;16,800&#20010;&#25552;&#31034;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#21516;&#35748;&#30693;&#20559;&#35265;(&#20363;&#22914;&#65292;&#25552;&#31034;&#35825;&#23548;&#12289;&#39034;&#24207;&#12289;&#22266;&#26377;)&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#21508;&#31181;&#20559;&#35265;&#32531;&#35299;&#31574;&#30053;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#26469;&#28040;&#38500;&#23427;&#20204;&#33258;&#24049;&#30340;&#25552;&#31034;&#20013;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#20851;&#20110;&#19981;&#21516;&#39046;&#22495;&#35748;&#30693;&#20559;&#35265;&#23384;&#22312;&#21644;&#24433;&#21709;&#30340;&#20840;&#38754;&#22270;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00811v1 Announce Type: new  Abstract: Large language models (LLMs) offer significant potential as tools to support an expanding range of decision-making tasks. However, given their training on human (created) data, LLMs can inherit both societal biases against protected groups, as well as be subject to cognitive bias. Such human-like bias can impede fair and explainable decisions made with LLM assistance. Our work introduces BiasBuster, a framework designed to uncover, evaluate, and mitigate cognitive bias in LLMs, particularly in high-stakes decision-making tasks. Inspired by prior research in psychology and cognitive sciences, we develop a dataset containing 16,800 prompts to evaluate different cognitive biases (e.g., prompt-induced, sequential, inherent). We test various bias mitigation strategies, amidst proposing a novel method using LLMs to debias their own prompts. Our analysis provides a comprehensive picture on the presence and effects of cognitive bias across diffe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#31264;&#23494;&#26816;&#32034;&#22120;&#30340;&#20449;&#24687;&#25429;&#33719;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#20854;&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#27604;&#36739;&#12289;&#20449;&#24687;&#25552;&#21462;&#30340;&#21487;&#34892;&#24615;&#20197;&#21450;&#25552;&#21462;&#24615;&#19982;&#24615;&#33021;&#12289;&#24615;&#21035;&#20559;&#35265;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.15925</link><description>&lt;p&gt;
MultiContrievers: &#31264;&#23494;&#26816;&#32034;&#34920;&#31034;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
MultiContrievers: Analysis of Dense Retrieval Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15925
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#31264;&#23494;&#26816;&#32034;&#22120;&#30340;&#20449;&#24687;&#25429;&#33719;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#20854;&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#27604;&#36739;&#12289;&#20449;&#24687;&#25552;&#21462;&#30340;&#21487;&#34892;&#24615;&#20197;&#21450;&#25552;&#21462;&#24615;&#19982;&#24615;&#33021;&#12289;&#24615;&#21035;&#20559;&#35265;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31264;&#23494;&#26816;&#32034;&#22120;&#23558;&#28304;&#25991;&#26723;&#21387;&#32553;&#20026;&#65288;&#21487;&#33021;&#26159;&#26377;&#25439;&#30340;&#65289;&#21521;&#37327;&#34920;&#31034;&#65292;&#28982;&#32780;&#30446;&#21069;&#23545;&#20110;&#22833;&#21435;&#21644;&#20445;&#30041;&#30340;&#20449;&#24687;&#20197;&#21450;&#23427;&#20204;&#22914;&#20309;&#24433;&#21709;&#19979;&#28216;&#20219;&#21153;&#30340;&#20998;&#26512;&#36739;&#23569;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#39318;&#27425;&#23545;&#27604;&#31264;&#23494;&#26816;&#32034;&#22120;&#25429;&#33719;&#30340;&#20449;&#24687;&#19982;&#23427;&#20204;&#22522;&#20110;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#19982;Contriever&#65289;&#20043;&#38388;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#20351;&#29992;25&#20010;MultiBert&#26816;&#26597;&#28857;&#20316;&#20026;&#38543;&#26426;&#21021;&#22987;&#21270;&#26469;&#35757;&#32451;MultiContrievers&#65292;&#36825;&#26159;&#19968;&#32452;25&#20010;contriever&#27169;&#22411;&#12290;&#25105;&#20204;&#27979;&#35797;&#29305;&#23450;&#20449;&#24687;&#65288;&#22914;&#24615;&#21035;&#21644;&#32844;&#19994;&#65289;&#26159;&#21542;&#21487;&#20197;&#20174;&#31867;&#20284;&#32500;&#22522;&#30334;&#31185;&#30340;&#25991;&#26723;&#30340;contriever&#21521;&#37327;&#20013;&#25552;&#21462;&#12290;&#25105;&#20204;&#36890;&#36807;&#20449;&#24687;&#35770;&#25506;&#27979;&#26469;&#34913;&#37327;&#36825;&#31181;&#21487;&#25552;&#21462;&#24615;&#12290;&#28982;&#21518;&#25105;&#20204;&#30740;&#31350;&#20102;&#21487;&#25552;&#21462;&#24615;&#19982;&#24615;&#33021;&#12289;&#24615;&#21035;&#20559;&#35265;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#36825;&#20123;&#32467;&#26524;&#23545;&#35768;&#22810;&#38543;&#26426;&#21021;&#22987;&#21270;&#21644;&#25968;&#25454;&#27927;&#29260;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65288;1&#65289;contriever&#27169;&#22411;&#26377;&#26174;&#33879;&#22686;&#21152;&#30340;&#21487;&#25552;&#21462;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15925v1 Announce Type: cross  Abstract: Dense retrievers compress source documents into (possibly lossy) vector representations, yet there is little analysis of what information is lost versus preserved, and how it affects downstream tasks. We conduct the first analysis of the information captured by dense retrievers compared to the language models they are based on (e.g., BERT versus Contriever). We use 25 MultiBert checkpoints as randomized initialisations to train MultiContrievers, a set of 25 contriever models. We test whether specific pieces of information -- such as gender and occupation -- can be extracted from contriever vectors of wikipedia-like documents. We measure this extractability via information theoretic probing. We then examine the relationship of extractability to performance and gender bias, as well as the sensitivity of these results to many random initialisations and data shuffles. We find that (1) contriever models have significantly increased extracta
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#21033;&#29992;&#24037;&#20855;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#35774;&#35745;&#20102;&#23450;&#21046;&#21270;&#24037;&#20855;&#26469;&#36741;&#21161;&#35821;&#35328;&#20195;&#29702;&#22312;&#24222;&#22823;&#29615;&#22659;&#20013;&#36827;&#34892;&#25506;&#32034;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#30693;&#35782;&#24211;&#21644;&#25968;&#25454;&#24211;&#31561;&#22797;&#26434;&#29615;&#22659;&#20013;&#65292;&#20511;&#21161;&#24037;&#20855;&#22686;&#24378;&#35821;&#35328;&#20195;&#29702;&#30340;&#37325;&#35201;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14672</link><description>&lt;p&gt;
&#35821;&#35328;&#20013;&#38388;&#20214;&#65306;&#24037;&#20855;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#23545;&#35821;&#35328;&#20195;&#29702;&#33267;&#20851;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
Middleware for LLMs: Tools Are Instrumental for Language Agents in Complex Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14672
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#21033;&#29992;&#24037;&#20855;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#35774;&#35745;&#20102;&#23450;&#21046;&#21270;&#24037;&#20855;&#26469;&#36741;&#21161;&#35821;&#35328;&#20195;&#29702;&#22312;&#24222;&#22823;&#29615;&#22659;&#20013;&#36827;&#34892;&#25506;&#32034;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#30693;&#35782;&#24211;&#21644;&#25968;&#25454;&#24211;&#31561;&#22797;&#26434;&#29615;&#22659;&#20013;&#65292;&#20511;&#21161;&#24037;&#20855;&#22686;&#24378;&#35821;&#35328;&#20195;&#29702;&#30340;&#37325;&#35201;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24212;&#29992;&#24050;&#32463;&#36828;&#36828;&#36229;&#20986;&#20102;&#25991;&#26412;&#22788;&#29702;&#30340;&#33539;&#22260;&#65292;&#39044;&#31034;&#30528;&#19968;&#20010;&#26032;&#26102;&#20195;&#30340;&#21040;&#26469;&#65292;&#22312;&#36825;&#20010;&#26102;&#20195;&#65292;LLMs&#34987;&#35774;&#24819;&#20026;&#33021;&#22815;&#22312;&#22797;&#26434;&#29616;&#23454;&#29615;&#22659;&#20013;&#36816;&#34892;&#30340;&#36890;&#29992;&#35821;&#35328;&#20195;&#29702;&#12290;&#36825;&#20123;&#29615;&#22659;&#36890;&#24120;&#38750;&#24120;&#24191;&#38420;&#65292;&#20351;&#24471;LLM&#19981;&#21487;&#33021;&#22312;&#20854;&#30701;&#26399;&#35760;&#24518;&#20013;&#22788;&#29702;&#23427;&#20204;&#12290;&#21463;&#26368;&#36817;&#20851;&#20110;&#36890;&#36807;&#24037;&#20855;&#25193;&#23637;LLMs&#33021;&#21147;&#30340;&#30740;&#31350;&#21551;&#21457;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#24037;&#20855;&#22312;&#22686;&#24378;LLMs&#22788;&#29702;&#36825;&#31181;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#23450;&#21046;&#24037;&#20855;&#65292;&#20197;&#21327;&#21161;&#22312;&#36825;&#20123;&#24222;&#22823;&#29615;&#22659;&#20013;&#36827;&#34892;&#20027;&#21160;&#25506;&#32034;&#12290;&#36825;&#20123;&#24037;&#20855;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#20013;&#38388;&#20214;&#23618;&#65292;&#20351;LLM&#20813;&#21463;&#29615;&#22659;&#22797;&#26434;&#24615;&#30340;&#24433;&#21709;&#12290;&#22312;&#20004;&#20010;&#20195;&#34920;&#24615;&#30340;&#22797;&#26434;&#29615;&#22659;--&#30693;&#35782;&#24211;&#65288;KBs&#65289;&#21644;&#25968;&#25454;&#24211;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#20351;&#29992;&#24037;&#20855;&#22686;&#24378;&#35821;&#35328;&#20195;&#29702;&#30340;&#37325;&#35201;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14672v1 Announce Type: cross  Abstract: The applications of large language models (LLMs) have expanded well beyond the confines of text processing, signaling a new era where LLMs are envisioned as generalist language agents capable of operating within complex real-world environments. These environments are often highly expansive, making it impossible for the LLM to process them within its short-term memory. Motivated by recent research on extending the capabilities of LLMs with tools, this paper investigates the intriguing potential of tools to augment LLMs in handling such complexity. To this end, we design customized tools to aid in the proactive exploration within these massive environments. Such tools can serve as a middleware layer shielding the LLM from environmental complexity. In two representative complex environments -- knowledge bases (KBs) and databases -- we demonstrate the significant potential of augmenting language agents with tools in complex environments. N
&lt;/p&gt;</description></item><item><title>LexC-Gen&#25552;&#20986;&#20102;&#19968;&#31181;&#35789;&#20856;&#26465;&#20214;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#22823;&#35268;&#27169;&#29983;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#25968;&#25454;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.14086</link><description>&lt;p&gt;
LexC-Gen: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21452;&#35821;&#35789;&#27719;&#34920;&#20026;&#26497;&#20302;&#36164;&#28304;&#35821;&#35328;&#29983;&#25104;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
LexC-Gen: Generating Data for Extremely Low-Resource Languages with Large Language Models and Bilingual Lexicons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14086
&lt;/p&gt;
&lt;p&gt;
LexC-Gen&#25552;&#20986;&#20102;&#19968;&#31181;&#35789;&#20856;&#26465;&#20214;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#22823;&#35268;&#27169;&#29983;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#25968;&#25454;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#25968;&#25454;&#21294;&#20047;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#21452;&#35821;&#35789;&#20856;&#20013;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#26631;&#35760;&#20219;&#21153;&#25968;&#25454;&#36827;&#34892;&#36880;&#23383;&#32763;&#35793;&#26469;&#35299;&#20915;&#65292;&#28982;&#32780;&#65292;&#21452;&#35821;&#35789;&#20856;&#36890;&#24120;&#19982;&#20219;&#21153;&#25968;&#25454;&#26377;&#38480;&#30340;&#35789;&#27719;&#37325;&#21472;&#65292;&#23548;&#33268;&#32763;&#35793;&#35206;&#30422;&#21644;&#35789;&#20856;&#21033;&#29992;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;LexC-Gen&#30340;&#35789;&#20856;&#26465;&#20214;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22823;&#35268;&#27169;&#29983;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LexC-Gen&#39318;&#20808;&#20351;&#29992;&#21452;&#35821;&#35789;&#20856;&#20013;&#30340;&#39640;&#36164;&#28304;&#35821;&#35328;&#21333;&#35789;&#29983;&#25104;&#19982;&#35789;&#20856;&#20860;&#23481;&#30340;&#20219;&#21153;&#25968;&#25454;&#65292;&#28982;&#21518;&#36890;&#36807;&#21333;&#35789;&#32763;&#35793;&#23558;&#20854;&#32763;&#35793;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#22312;17&#31181;&#26497;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#65292;LexC-Gen&#29983;&#25104;&#30340;&#25968;&#25454;&#22312;&#24615;&#33021;&#19978;&#19982;&#19987;&#23478;&#32763;&#35793;&#30340;&#40644;&#37329;&#25968;&#25454;&#31454;&#20105;&#21147;&#30456;&#24403;&#65292;&#24182;&#19988;&#22312;&#24773;&#24863;&#20998;&#26512;&#21644;&#20027;&#39064;&#20998;&#31867;&#19978;&#24179;&#22343;&#27604;&#29616;&#26377;&#30340;&#22522;&#20110;&#35789;&#20856;&#30340;&#21333;&#35789;&#32763;&#35793;&#26041;&#27861;&#25552;&#39640;&#20102;5.6&#21644;8.9&#20010;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14086v1 Announce Type: cross  Abstract: Data scarcity in low-resource languages can be addressed with word-to-word translations from labeled task data in high-resource languages using bilingual lexicons. However, bilingual lexicons often have limited lexical overlap with task data, which results in poor translation coverage and lexicon utilization. We propose lexicon-conditioned data generation (LexC-Gen), a method that generates low-resource-language classification task data at scale. Specifically, LexC-Gen first uses high-resource-language words from bilingual lexicons to generate lexicon-compatible task data, and then it translates them into low-resource languages with bilingual lexicons via word translation. Across 17 extremely low-resource languages, LexC-Gen generated data is competitive with expert-translated gold data, and yields on average 5.6 and 8.9 points improvement over existing lexicon-based word translation methods on sentiment analysis and topic classificati
&lt;/p&gt;</description></item><item><title>&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#22522;&#20110;&#26368;&#22823;softmax&#27010;&#29575;&#65288;MSPs&#65289;&#30340;&#27169;&#22411;&#39044;&#27979;&#26041;&#27861;&#26377;&#21161;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27491;&#30830;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26681;&#25454;MSP&#26377;&#36873;&#25321;&#22320;&#24323;&#26435;&#30340;&#31574;&#30053;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.13213</link><description>&lt;p&gt;
&#36719;&#26368;&#22823;&#27010;&#29575;&#65288;&#22823;&#37096;&#20998;&#26102;&#20505;&#65289;&#22312;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#20219;&#21153;&#20013;&#39044;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27491;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Softmax Probabilities (Mostly) Predict Large Language Model Correctness on Multiple-Choice Q&amp;A
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13213
&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#22522;&#20110;&#26368;&#22823;softmax&#27010;&#29575;&#65288;MSPs&#65289;&#30340;&#27169;&#22411;&#39044;&#27979;&#26041;&#27861;&#26377;&#21161;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27491;&#30830;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26681;&#25454;MSP&#26377;&#36873;&#25321;&#22320;&#24323;&#26435;&#30340;&#31574;&#30053;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#36807;&#24230;&#33258;&#20449;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20551;&#35774;&#22312;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#38169;&#35823;&#31572;&#26696;&#23558;&#19982;&#26368;&#22823;softmax&#27010;&#29575;&#65288;MSPs&#65289;&#36739;&#23567;&#30456;&#20851;&#65292;&#30456;&#27604;&#20043;&#19979;&#27491;&#30830;&#31572;&#26696;&#36739;&#22823;&#12290;&#25105;&#20204;&#22312;&#21313;&#20010;&#24320;&#28304;LLMs&#21644;&#20116;&#20010;&#25968;&#25454;&#38598;&#19978;&#20840;&#38754;&#35780;&#20272;&#20102;&#36825;&#19968;&#20551;&#35774;&#65292;&#22312;&#34920;&#29616;&#33391;&#22909;&#30340;&#21407;&#22987;&#38382;&#31572;&#20219;&#21153;&#20013;&#21457;&#29616;&#20102;&#23545;&#25105;&#20204;&#20551;&#35774;&#30340;&#24378;&#26377;&#21147;&#35777;&#25454;&#12290;&#23545;&#20110;&#34920;&#29616;&#26368;&#20339;&#30340;&#20845;&#20010;LLMs&#65292;&#20174;MSP&#23548;&#20986;&#30340;AUROC&#22312;59/60&#20010;&#23454;&#20363;&#20013;&#37117;&#20248;&#20110;&#38543;&#26426;&#26426;&#20250;&#65292;p &lt; 10^{-4}&#12290;&#22312;&#36825;&#20845;&#20010;LLMs&#20013;&#65292;&#24179;&#22343;AUROC&#33539;&#22260;&#22312;60%&#33267;69%&#20043;&#38388;&#12290;&#21033;&#29992;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#24323;&#26435;&#36873;&#39033;&#30340;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#36890;&#36807;&#26681;&#25454;&#21021;&#22987;&#27169;&#22411;&#21709;&#24212;&#30340;MSP&#26377;&#36873;&#25321;&#22320;&#24323;&#26435;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#29992;&#39044;softmax logits&#32780;&#19981;&#26159;softmax&#36827;&#34892;&#20102;&#30456;&#21516;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13213v1 Announce Type: cross  Abstract: Although large language models (LLMs) perform impressively on many tasks, overconfidence remains a problem. We hypothesized that on multiple-choice Q&amp;A tasks, wrong answers would be associated with smaller maximum softmax probabilities (MSPs) compared to correct answers. We comprehensively evaluate this hypothesis on ten open-source LLMs and five datasets, and find strong evidence for our hypothesis among models which perform well on the original Q&amp;A task. For the six LLMs with the best Q&amp;A performance, the AUROC derived from the MSP was better than random chance with p &lt; 10^{-4} in 59/60 instances. Among those six LLMs, the average AUROC ranged from 60% to 69%. Leveraging these findings, we propose a multiple-choice Q&amp;A task with an option to abstain and show that performance can be improved by selectively abstaining based on the MSP of the initial model response. We also run the same experiments with pre-softmax logits instead of sof
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#30340;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;AILOT&#65292;&#21487;&#20197;&#22312;&#32570;&#20047;&#26126;&#30830;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#36890;&#36807;&#35266;&#23519;&#19987;&#23478;&#23398;&#20064;&#25152;&#38656;&#30340;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.13037</link><description>&lt;p&gt;
&#23545;&#40784;&#24744;&#30340;&#24847;&#22270;&#65306;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#30340;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Align Your Intents: Offline Imitation Learning via Optimal Transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13037
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#30340;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;AILOT&#65292;&#21487;&#20197;&#22312;&#32570;&#20047;&#26126;&#30830;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#36890;&#36807;&#35266;&#23519;&#19987;&#23478;&#23398;&#20064;&#25152;&#38656;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36890;&#36807;&#23398;&#20064;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#26469;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#65292;&#32780;&#26080;&#38656;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#12290;&#25105;&#20204;&#23637;&#31034;&#20986;&#65292;&#21363;&#20351;&#32570;&#20047;&#26126;&#30830;&#30340;&#22870;&#21169;&#25110;&#21160;&#20316;&#26631;&#31614;&#65292;&#27169;&#20223;&#20195;&#29702;&#20063;&#21487;&#20197;&#20165;&#36890;&#36807;&#35266;&#23519;&#19987;&#23478;&#26469;&#23398;&#20064;&#25152;&#38656;&#30340;&#34892;&#20026;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;AILOT&#65288;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#23545;&#40784;&#27169;&#20223;&#23398;&#20064;&#65289;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#24847;&#22270;&#30340;&#29305;&#27530;&#29366;&#24577;&#34920;&#31034;&#24418;&#24335;&#65292;&#20854;&#20013;&#21253;&#21547;&#25968;&#25454;&#20869;&#30340;&#20004;&#20004;&#31354;&#38388;&#36317;&#31163;&#12290;&#22312;&#32473;&#23450;&#36825;&#31181;&#34920;&#31034;&#24418;&#24335;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#19987;&#23478;&#21644;&#20195;&#29702;&#36712;&#36857;&#20043;&#38388;&#30340;&#26368;&#20248;&#20256;&#36755;&#36317;&#31163;&#23450;&#20041;&#20869;&#22312;&#22870;&#21169;&#20989;&#25968;&#12290;&#25105;&#20204;&#25253;&#21578;&#31216;AILOT&#22312;D4RL&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13037v1 Announce Type: cross  Abstract: Offline reinforcement learning (RL) addresses the problem of sequential decision-making by learning optimal policy through pre-collected data, without interacting with the environment. As yet, it has remained somewhat impractical, because one rarely knows the reward explicitly and it is hard to distill it retrospectively. Here, we show that an imitating agent can still learn the desired behavior merely from observing the expert, despite the absence of explicit rewards or action labels. In our method, AILOT (Aligned Imitation Learning via Optimal Transport), we involve special representation of states in a form of intents that incorporate pairwise spatial distances within the data. Given such representations, we define intrinsic reward function via optimal transport distance between the expert's and the agent's trajectories. We report that AILOT outperforms state-of-the art offline imitation learning algorithms on D4RL benchmarks and im
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;ANALOBENCH&#22522;&#20934;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#36827;&#34892;&#31867;&#27604;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#25193;&#23637;LMs&#35268;&#27169;&#23545;&#20110;&#22788;&#29702;&#28041;&#21450;&#38271;&#22330;&#26223;&#25110;&#30456;&#20851;&#32463;&#39564;&#22238;&#24518;&#30340;&#31867;&#27604;&#26102;&#24102;&#26469;&#30340;&#24615;&#33021;&#25552;&#21319;&#36739;&#23567;&#12290;</title><link>https://arxiv.org/abs/2402.12370</link><description>&lt;p&gt;
AnaloBench&#65306;&#35780;&#20272;&#25277;&#35937;&#21644;&#38271;&#19978;&#19979;&#25991;&#31867;&#27604;&#35782;&#21035;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
AnaloBench: Benchmarking the Identification of Abstract and Long-context Analogies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12370
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;ANALOBENCH&#22522;&#20934;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#36827;&#34892;&#31867;&#27604;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#25193;&#23637;LMs&#35268;&#27169;&#23545;&#20110;&#22788;&#29702;&#28041;&#21450;&#38271;&#22330;&#26223;&#25110;&#30456;&#20851;&#32463;&#39564;&#22238;&#24518;&#30340;&#31867;&#27604;&#26102;&#24102;&#26469;&#30340;&#24615;&#33021;&#25552;&#21319;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#32463;&#24120;&#36827;&#34892;&#31867;&#27604;&#24605;&#32500;&#65292;&#23558;&#20010;&#20154;&#32463;&#39564;&#19982;&#24403;&#21069;&#24773;&#20917;&#32852;&#31995;&#36215;&#26469;&#65288;$X$&#31867;&#20284;&#20110;$Y$&#26159;&#22240;&#20026;$Z$&#65289;&#12290;&#31867;&#27604;&#24605;&#32500;&#20351;&#20154;&#31867;&#33021;&#22815;&#29992;&#21019;&#36896;&#24615;&#26041;&#24335;&#35299;&#20915;&#38382;&#39064;&#65292;&#29702;&#35299;&#22256;&#38590;&#27010;&#24565;&#65292;&#26356;&#26377;&#25928;&#22320;&#34920;&#36798;&#24819;&#27861;&#12290;&#33021;&#21542;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20063;&#33021;&#20570;&#21040;&#36825;&#19968;&#28857;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ANALOBENCH&#65292;&#19968;&#20010;&#29992;&#20110;&#30830;&#23450;LMs&#31867;&#27604;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#26041;&#27861;&#19987;&#27880;&#20110;&#20154;&#31867;&#20043;&#38388;&#20849;&#21516;&#30340;&#31867;&#27604;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#65306;&#65288;i&#65289;&#20174;&#22823;&#37327;&#20449;&#24687;&#20013;&#22238;&#24518;&#30456;&#20851;&#32463;&#39564;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#23558;&#31867;&#27604;&#25512;&#29702;&#24212;&#29992;&#20110;&#22797;&#26434;&#21644;&#38271;&#24230;&#36739;&#38271;&#30340;&#22330;&#26223;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#22823;&#37327;&#19987;&#26377;&#27169;&#22411;&#65288;&#20363;&#22914;&#65292;GPT&#31995;&#21015;&#65292;Claude V2&#65289;&#21644;&#24320;&#28304;&#27169;&#22411;&#65292;&#22914;LLaMA2&#12290;&#19982;&#20808;&#21069;&#30340;&#32467;&#26524;&#19968;&#26679;&#65292;&#25193;&#23637;LMs&#20250;&#24102;&#26469;&#19968;&#20123;&#24615;&#33021;&#25552;&#21319;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#31867;&#27604;&#28041;&#21450;&#38271;&#22330;&#26223;&#25110;&#22238;&#24518;&#30456;&#20851;&#32463;&#39564;&#26102;&#65292;&#35268;&#27169;&#30340;&#25552;&#21319;&#24102;&#26469;&#30340;&#22686;&#30410;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12370v1 Announce Type: cross  Abstract: Humans regularly engage in analogical thinking, relating personal experiences to current situations ($X$ is analogous to $Y$ because of $Z$). Analogical thinking allows humans to solve problems in creative ways, grasp difficult concepts, and articulate ideas more effectively. Can language models (LMs) do the same? To answer this question, we propose ANALOBENCH, a benchmark to determine analogical reasoning ability in LMs. Our benchmarking approach focuses on aspects of this ability that are common among humans: (i) recalling related experiences from a large amount of information, and (ii) applying analogical reasoning to complex and lengthy scenarios. We test a broad collection of proprietary models (e.g., GPT family, Claude V2) and open source models such as LLaMA2. As in prior results, scaling up LMs results in some performance boosts. Surprisingly, scale offers minimal gains when, (i) analogies involve lengthy scenarios, or (ii) rec
&lt;/p&gt;</description></item><item><title>&#23558;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#23454;&#38469;&#21709;&#24212;&#39118;&#26684;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22266;&#26377;&#39118;&#26684;&#30456;&#21305;&#37197;&#33021;&#22815;&#20135;&#29983;&#26356;&#22909;&#30340;&#23398;&#20064;&#32467;&#26524;&#65292;&#24320;&#21457;&#30340;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#31243;&#24230;&#22320;&#35843;&#25972;&#27169;&#22411;&#21709;&#24212;&#26469;&#36991;&#20813;&#36807;&#25311;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.11192</link><description>&lt;p&gt;
&#22914;&#26524;&#20320;&#35762;&#25105;&#30340;&#35821;&#35328;&#65292;&#25105;&#20250;&#26356;&#22909;&#22320;&#23398;&#20064;&#65306;&#20351;&#29992;&#39118;&#26684;&#23545;&#40784;&#21709;&#24212;&#35843;&#25972;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
I Learn Better If You Speak My Language: Enhancing Large Language Model Fine-Tuning with Style-Aligned Response Adjustments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11192
&lt;/p&gt;
&lt;p&gt;
&#23558;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#23454;&#38469;&#21709;&#24212;&#39118;&#26684;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22266;&#26377;&#39118;&#26684;&#30456;&#21305;&#37197;&#33021;&#22815;&#20135;&#29983;&#26356;&#22909;&#30340;&#23398;&#20064;&#32467;&#26524;&#65292;&#24320;&#21457;&#30340;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#31243;&#24230;&#22320;&#35843;&#25972;&#27169;&#22411;&#21709;&#24212;&#26469;&#36991;&#20813;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23567;&#25968;&#25454;&#38598;&#20026;&#29305;&#23450;&#20219;&#21153;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#19968;&#20010;&#26222;&#36941;&#36935;&#21040;&#30340;&#20294;&#22797;&#26434;&#30340;&#25361;&#25112;&#12290;&#22312;&#26377;&#38480;&#30340;&#31034;&#20363;&#19978;&#36807;&#22810;&#25311;&#21512;&#21487;&#33021;&#20250;&#23545;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#20445;&#30041;&#21407;&#22987;&#25216;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#22320;&#23454;&#38469;&#21709;&#24212;&#39118;&#26684;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#23558;&#22320;&#23454;&#38469;&#21709;&#24212;&#39118;&#26684;&#19982;LLM&#22266;&#26377;&#39118;&#26684;&#21305;&#37197;&#20250;&#20135;&#29983;&#26356;&#22909;&#30340;&#23398;&#20064;&#32467;&#26524;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#26368;&#23567;&#31243;&#24230;&#22320;&#20462;&#25913;LLM&#30340;&#29616;&#26377;&#21709;&#24212;&#20197;&#26356;&#27491;&#38169;&#35823;&#65292;&#20351;&#29992;&#36825;&#20123;&#35843;&#25972;&#21518;&#30340;&#21709;&#24212;&#20316;&#20026;&#35757;&#32451;&#30446;&#26631;&#12290;&#36825;&#31181;&#25216;&#26415;&#33021;&#22815;&#23454;&#29616;&#19982;&#27169;&#22411;&#22266;&#26377;&#21709;&#24212;&#39118;&#26684;&#19968;&#33268;&#30340;&#31934;&#30830;&#26356;&#27491;&#65292;&#32500;&#25252;&#27169;&#22411;&#30340;&#26680;&#24515;&#33021;&#21147;&#65292;&#20174;&#32780;&#36991;&#20813;&#36807;&#22810;&#25311;&#21512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;LLM&#30340;&#29305;&#23450;&#20219;&#21153;&#20934;&#30830;&#24615;&#65292;&#32780;&#19988;&#20851;&#38190;&#22320;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11192v1 Announce Type: cross  Abstract: Fine-tuning large language models (LLMs) with a small data set for particular tasks is a widely encountered yet complex challenge. The potential for overfitting on a limited number of examples can negatively impact the model's ability to generalize and retain its original skills. Our research explores the impact of the style of ground-truth responses during the fine-tuning process. We found that matching the ground-truth response style with the LLM's inherent style results in better learning outcomes. Building on this insight, we developed a method that minimally alters the LLM's pre-existing responses to correct errors, using these adjusted responses as training targets. This technique enables precise corrections in line with the model's native response style, safeguarding the model's core capabilities and thus avoid overfitting. Our findings show that this approach not only improves the LLM's task-specific accuracy but also crucially
&lt;/p&gt;</description></item><item><title>&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#65292;&#36890;&#36807;&#23547;&#27714;&#24110;&#21161;&#26469;&#36991;&#20813;&#28798;&#38590;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#30340;&#21464;&#20307;&#65292;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#28798;&#38590;&#21457;&#29983;&#30340;&#27010;&#29575;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#36830;&#32493;1D&#29366;&#24577;&#31354;&#38388;&#21644;&#30456;&#23545;&#31616;&#21333;&#30340;&#22238;&#25253;&#20989;&#25968;&#19979;&#65292;&#36951;&#25022;&#21644;&#21521;&#23548;&#24072;&#26597;&#35810;&#29575;&#37117;&#36235;&#36817;&#20110;0&#12290;</title><link>https://arxiv.org/abs/2402.08062</link><description>&lt;p&gt;
&#36991;&#20813;&#36830;&#32493;&#31354;&#38388;&#20013;&#30340;&#28798;&#38590;&#65306;&#36890;&#36807;&#23547;&#27714;&#24110;&#21161;
&lt;/p&gt;
&lt;p&gt;
Avoiding Catastrophe in Continuous Spaces by Asking for Help
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08062
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#65292;&#36890;&#36807;&#23547;&#27714;&#24110;&#21161;&#26469;&#36991;&#20813;&#28798;&#38590;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#30340;&#21464;&#20307;&#65292;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#28798;&#38590;&#21457;&#29983;&#30340;&#27010;&#29575;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#36830;&#32493;1D&#29366;&#24577;&#31354;&#38388;&#21644;&#30456;&#23545;&#31616;&#21333;&#30340;&#22238;&#25253;&#20989;&#25968;&#19979;&#65292;&#36951;&#25022;&#21644;&#21521;&#23548;&#24072;&#26597;&#35810;&#29575;&#37117;&#36235;&#36817;&#20110;0&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20855;&#26377;&#27491;&#24335;&#36951;&#25022;&#20445;&#35777;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20551;&#35774;&#25152;&#26377;&#38169;&#35823;&#37117;&#26159;&#21487;&#36870;&#30340;&#65292;&#24182;&#20381;&#36182;&#20110;&#23581;&#35797;&#25152;&#26377;&#21487;&#33021;&#30340;&#36873;&#39033;&#12290;&#24403;&#19968;&#20123;&#38169;&#35823;&#26159;&#26080;&#27861;&#20462;&#22797;&#29978;&#33267;&#26159;&#28798;&#38590;&#24615;&#30340;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#20250;&#23548;&#33268;&#31967;&#31957;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#30340;&#21464;&#20307;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#21457;&#29983;&#28798;&#38590;&#30340;&#27010;&#29575;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20551;&#35774;&#27599;&#36718;&#30340;&#22238;&#25253;&#20195;&#34920;&#20102;&#22312;&#35813;&#36718;&#36991;&#20813;&#28798;&#38590;&#30340;&#27010;&#29575;&#65292;&#24182;&#23581;&#35797;&#26368;&#22823;&#21270;&#22238;&#25253;&#30340;&#20056;&#31215;&#65288;&#24635;&#20307;&#36991;&#20813;&#28798;&#38590;&#30340;&#27010;&#29575;&#65289;&#12290;&#20026;&#20102;&#32473; agent &#19968;&#20123;&#25104;&#21151;&#30340;&#26426;&#20250;&#65292;&#25105;&#20204;&#20801;&#35768;&#26377;&#38480;&#27425;&#21521;&#23548;&#24072;&#25552;&#38382;&#65292;&#24182;&#20551;&#35774;&#22238;&#25253;&#20989;&#25968;&#20026; Lipschitz &#36830;&#32493;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#24403;&#26102;&#38388;&#36328;&#24230;&#22686;&#38271;&#26102;&#65292;&#23427;&#30340;&#36951;&#25022;&#21644;&#21521;&#23548;&#24072;&#26597;&#35810;&#29575;&#37117;&#36235;&#36817;&#20110; 0&#65292;&#20551;&#35774;&#26159;&#19968;&#20010;&#36830;&#32493;&#30340; 1D &#29366;&#24577;&#31354;&#38388;&#21644;&#30456;&#23545;"&#31616;&#21333;"&#30340;&#22238;&#25253;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#21305;&#37197;&#30340;&#19979;&#30028;&#65306;&#22312;&#27809;&#26377;&#31616;&#21333;&#24615;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20219;&#20309;&#31639;&#27861;&#35201;&#20040;&#19981;&#26029;&#26597;&#35810;&#24322;&#24120;&#30340;&#34892;&#20026;&#65292;&#35201;&#20040;&#27599;&#27425;&#26597;&#35810;&#23436;&#20840;&#30456;&#21516;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most reinforcement learning algorithms with formal regret guarantees assume all mistakes are reversible and rely on essentially trying all possible options. This approach leads to poor outcomes when some mistakes are irreparable or even catastrophic. We propose a variant of the contextual bandit problem where the goal is to minimize the chance of catastrophe. Specifically, we assume that the payoff each round represents the chance of avoiding catastrophe that round, and try to maximize the product of payoffs (the overall chance of avoiding catastrophe). To give the agent some chance of success, we allow a limited number of queries to a mentor and assume a Lipschitz continuous payoff function. We present an algorithm whose regret and rate of querying the mentor both approach 0 as the time horizon grows, assuming a continuous 1D state space and a relatively "simple" payoff function. We also provide a matching lower bound: without the simplicity assumption: any algorithm either constantly
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23494;&#38598;&#20056;&#27861;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;DM-PINN&#65289;&#26550;&#26500;&#65292;&#23427;&#26377;&#25928;&#21033;&#29992;&#38544;&#34255;&#23618;&#30340;&#36755;&#20986;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;PINN&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04390</link><description>&lt;p&gt;
&#23494;&#38598;&#20056;&#27861;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Densely Multiplied Physics Informed Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04390
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23494;&#38598;&#20056;&#27861;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;DM-PINN&#65289;&#26550;&#26500;&#65292;&#23427;&#26377;&#25928;&#21033;&#29992;&#38544;&#34255;&#23618;&#30340;&#36755;&#20986;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;PINN&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;Physics-Informed Neural Networks, PINNs&#65289;&#22312;&#22788;&#29702;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#26041;&#38754;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#24120;&#24120;&#20250;&#20986;&#29616;&#31934;&#24230;&#19981;&#36275;&#25110;&#33719;&#21462;&#19981;&#27491;&#30830;&#32467;&#26524;&#30340;&#38382;&#39064;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#19981;&#21516;&#65292;&#35813;&#35770;&#25991;&#25913;&#36827;&#20102;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20197;&#25552;&#39640;PINN&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23494;&#38598;&#20056;&#27861;PINN&#65288;DM-PINN&#65289;&#26550;&#26500;&#65292;&#23427;&#23558;&#38544;&#34255;&#23618;&#30340;&#36755;&#20986;&#19982;&#25152;&#26377;&#21518;&#38754;&#30340;&#38544;&#34255;&#23618;&#30340;&#36755;&#20986;&#30456;&#20056;&#12290;&#22312;&#19981;&#24341;&#20837;&#26356;&#22810;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26377;&#25928;&#26426;&#21046;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;PINN&#30340;&#20934;&#30830;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#22312;&#22235;&#20010;&#22522;&#20934;&#31034;&#20363;&#65288;Allan-Cahn&#26041;&#31243;&#65292;Helmholtz&#26041;&#31243;&#65292;Burgers&#26041;&#31243;&#21644;1D&#23545;&#27969;&#26041;&#31243;&#65289;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23558;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#19982;&#19981;&#21516;&#30340;PINN&#32467;&#26500;&#36827;&#34892;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#20854;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although physics-informed neural networks (PINNs) have shown great potential in dealing with nonlinear partial differential equations (PDEs), it is common that PINNs will suffer from the problem of insufficient precision or obtaining incorrect outcomes. Unlike most of the existing solutions trying to enhance the ability of PINN by optimizing the training process, this paper improved the neural network architecture to improve the performance of PINN. We propose a densely multiply PINN (DM-PINN) architecture, which multiplies the output of a hidden layer with the outputs of all the behind hidden layers. Without introducing more trainable parameters, this effective mechanism can significantly improve the accuracy of PINNs. The proposed architecture is evaluated on four benchmark examples (Allan-Cahn equation, Helmholtz equation, Burgers equation and 1D convection equation). Comparisons between the proposed architecture and different PINN structures demonstrate the superior performance of 
&lt;/p&gt;</description></item><item><title>MetaOptimize&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#23398;&#20064;&#29575;&#26469;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#20803;&#21442;&#25968;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02342</link><description>&lt;p&gt;
MetaOptimize&#65306;&#19968;&#20010;&#20248;&#21270;&#27493;&#38271;&#21644;&#20854;&#20182;&#20803;&#21442;&#25968;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MetaOptimize: A Framework for Optimizing Step Sizes and Other Meta-parameters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02342
&lt;/p&gt;
&lt;p&gt;
MetaOptimize&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#23398;&#20064;&#29575;&#26469;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#20803;&#21442;&#25968;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#20248;&#21270;&#20803;&#21442;&#25968;&#65288;&#21363;&#36229;&#21442;&#25968;&#65289;&#30340;&#25361;&#25112;&#65292;&#36825;&#26159;&#24433;&#21709;&#35757;&#32451;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;MetaOptimize&#26694;&#26550;&#65292;&#25670;&#33073;&#20102;&#35745;&#31639;&#26114;&#36149;&#30340;&#20256;&#32479;&#20803;&#21442;&#25968;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#20803;&#21442;&#25968;&#65292;&#29305;&#21035;&#26159;&#27493;&#38271;&#65288;&#20063;&#31216;&#20026;&#23398;&#20064;&#29575;&#65289;&#65292;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MetaOptimize&#21487;&#20197;&#36866;&#29992;&#20110;&#20219;&#20309;&#19968;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23454;&#26102;&#35843;&#25972;&#27493;&#38271;&#65292;&#36890;&#36807;&#26410;&#26469;&#25439;&#22833;&#30340;&#25240;&#29616;&#24635;&#21644;&#26469;&#26368;&#23567;&#21270;&#19968;&#31181;&#29305;&#23450;&#24418;&#24335;&#30340;&#36951;&#25022;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;MetaOptimize&#30340;&#20302;&#22797;&#26434;&#24230;&#21464;&#20307;&#65292;&#32467;&#21512;&#20854;&#36866;&#24212;&#22810;&#20010;&#20248;&#21270;&#31639;&#27861;&#30340;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#19982;&#25163;&#24037;&#35774;&#35745;&#30340;&#23398;&#20064;&#29575;&#35745;&#21010;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the challenge of optimizing meta-parameters (i.e., hyperparameters) in machine learning algorithms, a critical factor influencing training efficiency and model performance. Moving away from the computationally expensive traditional meta-parameter search methods, we introduce MetaOptimize framework that dynamically adjusts meta-parameters, particularly step sizes (also known as learning rates), during training. More specifically, MetaOptimize can wrap around any first-order optimization algorithm, tuning step sizes on the fly to minimize a specific form of regret that accounts for long-term effect of step sizes on training, through a discounted sum of future losses. We also introduce low complexity variants of MetaOptimize that, in conjunction with its adaptability to multiple optimization algorithms, demonstrate performance competitive to those of best hand-crafted learning rate schedules across various machine learning applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;LLMs&#23545;&#21160;&#21147;&#31995;&#32479;&#34892;&#20026;&#30340;&#22806;&#25512;&#33021;&#21147;&#65292;&#21457;&#29616;LLaMA 2&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21160;&#21147;&#31995;&#32479;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#36755;&#20837;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#38271;&#24230;&#36234;&#38271;&#65292;&#23398;&#20064;&#21040;&#30340;&#29289;&#29702;&#35268;&#21017;&#30340;&#20934;&#30830;&#24615;&#36234;&#39640;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#20013;&#30340;&#31070;&#32463;&#27604;&#20363;&#23450;&#24459;&#12290;</title><link>https://arxiv.org/abs/2402.00795</link><description>&lt;p&gt;
LLMs&#23398;&#20064;&#21160;&#21147;&#31995;&#32479;&#30340;&#25511;&#21046;&#21407;&#29702;&#65292;&#25581;&#31034;&#20102;&#19978;&#19979;&#25991;&#20013;&#30340;&#31070;&#32463;&#27604;&#20363;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;LLMs&#23545;&#21160;&#21147;&#31995;&#32479;&#34892;&#20026;&#30340;&#22806;&#25512;&#33021;&#21147;&#65292;&#21457;&#29616;LLaMA 2&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21160;&#21147;&#31995;&#32479;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#36755;&#20837;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#38271;&#24230;&#36234;&#38271;&#65292;&#23398;&#20064;&#21040;&#30340;&#29289;&#29702;&#35268;&#21017;&#30340;&#20934;&#30830;&#24615;&#36234;&#39640;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#20013;&#30340;&#31070;&#32463;&#27604;&#20363;&#23450;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38646;-shot&#20219;&#21153;&#65292;&#21253;&#25324;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#65292;&#29702;&#35299;&#20854;&#32972;&#21518;&#30340;&#26426;&#21046;&#20173;&#28982;&#26497;&#20855;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#23545;&#21463;&#29289;&#29702;&#21407;&#29702;&#25511;&#21046;&#30340;&#21160;&#21147;&#31995;&#32479;&#34892;&#20026;&#30340;&#22806;&#25512;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20027;&#35201;&#22312;&#25991;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;LLaMA 2&#22312;&#27809;&#26377;&#24494;&#35843;&#25110;&#25552;&#31034;&#24037;&#31243;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21160;&#21147;&#31995;&#32479;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#23398;&#20064;&#21040;&#30340;&#29289;&#29702;&#35268;&#21017;&#30340;&#20934;&#30830;&#24615;&#38543;&#30528;&#36755;&#20837;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#38271;&#24230;&#22686;&#21152;&#32780;&#22686;&#21152;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#20013;&#30340;&#31070;&#32463;&#27604;&#20363;&#23450;&#24459;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#30452;&#25509;&#20174;LLMs&#20013;&#25552;&#21462;&#22810;&#20301;&#25968;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained large language models (LLMs) are surprisingly effective at performing zero-shot tasks, including time-series forecasting. However, understanding the mechanisms behind such capabilities remains highly challenging due to the complexity of the models. In this paper, we study LLMs' ability to extrapolate the behavior of dynamical systems whose evolution is governed by principles of physical interest. Our results show that LLaMA 2, a language model trained primarily on texts, achieves accurate predictions of dynamical system time series without fine-tuning or prompt engineering. Moreover, the accuracy of the learned physical rules increases with the length of the input context window, revealing an in-context version of neural scaling law. Along the way, we present a flexible and efficient algorithm for extracting probability density functions of multi-digit numbers directly from LLMs.
&lt;/p&gt;</description></item><item><title>EHRAgent&#26159;&#19968;&#20010;&#30001;&#20195;&#30721;&#25509;&#21475;&#36171;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#65292;&#29992;&#20110;&#33258;&#20027;&#29983;&#25104;&#21644;&#25191;&#34892;&#22810;&#34920;&#26684;&#25512;&#29702;&#20195;&#30721;&#65292;&#36890;&#36807;&#38169;&#35823;&#20449;&#24687;&#23398;&#20064;&#25913;&#36827;&#29983;&#25104;&#30340;&#20195;&#30721;&#65292;&#32467;&#21512;&#38271;&#26399;&#35760;&#24518;&#36873;&#25321;&#24182;&#24314;&#31435;&#22312;&#36807;&#21435;&#32463;&#39564;&#20013;&#30340;&#25104;&#21151;&#26696;&#20363;&#12290;</title><link>https://arxiv.org/abs/2401.07128</link><description>&lt;p&gt;
EHRAgent&#65306;&#20195;&#30721;&#36171;&#33021;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#19978;&#36827;&#34892;&#23569;&#26679;&#26412;&#22797;&#26434;&#34920;&#26684;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07128
&lt;/p&gt;
&lt;p&gt;
EHRAgent&#26159;&#19968;&#20010;&#30001;&#20195;&#30721;&#25509;&#21475;&#36171;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#65292;&#29992;&#20110;&#33258;&#20027;&#29983;&#25104;&#21644;&#25191;&#34892;&#22810;&#34920;&#26684;&#25512;&#29702;&#20195;&#30721;&#65292;&#36890;&#36807;&#38169;&#35823;&#20449;&#24687;&#23398;&#20064;&#25913;&#36827;&#29983;&#25104;&#30340;&#20195;&#30721;&#65292;&#32467;&#21512;&#38271;&#26399;&#35760;&#24518;&#36873;&#25321;&#24182;&#24314;&#31435;&#22312;&#36807;&#21435;&#32463;&#39564;&#20013;&#30340;&#25104;&#21151;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35268;&#21010;&#21644;&#24037;&#20855;&#21033;&#29992;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#21307;&#23398;&#38382;&#39064;&#35299;&#20915;&#26041;&#38754;&#23578;&#26410;&#26377;&#22826;&#22810;&#24320;&#21457;&#12290;&#25105;&#20204;&#25552;&#20986;EHRAgent&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#20195;&#30721;&#25509;&#21475;&#36171;&#33021;&#30340;LLM&#20195;&#29702;&#65292;&#29992;&#20110;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHRs&#65289;&#20013;&#33258;&#20027;&#29983;&#25104;&#21644;&#25191;&#34892;&#22810;&#34920;&#26684;&#25512;&#29702;&#30340;&#20195;&#30721;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;EHR&#38382;&#31572;&#20219;&#21153;&#21046;&#23450;&#20026;&#24037;&#20855;&#20351;&#29992;&#35268;&#21010;&#36807;&#31243;&#65292;&#23558;&#19968;&#20010;&#22797;&#26434;&#20219;&#21153;&#39640;&#25928;&#22320;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#21487;&#31649;&#29702;&#30340;&#25805;&#20316;&#12290;&#36890;&#36807;&#38598;&#25104;&#20132;&#20114;&#24335;&#32534;&#30721;&#21644;&#25191;&#34892;&#21453;&#39304;&#65292;EHRAgent&#20174;&#38169;&#35823;&#28040;&#24687;&#20013;&#23398;&#20064;&#24182;&#36890;&#36807;&#36845;&#20195;&#25913;&#36827;&#26368;&#21021;&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#32467;&#21512;&#38271;&#26399;&#35760;&#24518;&#26469;&#22686;&#24378;LLM&#20195;&#29702;&#65292;&#20351;EHRAgent&#33021;&#22815;&#26377;&#25928;&#22320;&#36873;&#25321;&#24182;&#24314;&#31435;&#22312;&#36807;&#21435;&#32463;&#39564;&#20013;&#26368;&#30456;&#20851;&#30340;&#25104;&#21151;&#26696;&#20363;&#19978;&#12290;&#22312;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#22810;&#34920;&#26684;EHR&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#26174;&#31034;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07128v2 Announce Type: replace-cross  Abstract: Large language models (LLMs) have demonstrated exceptional capabilities in planning and tool utilization as autonomous agents, but few have been developed for medical problem-solving. We propose EHRAgent, an LLM agent empowered with a code interface, to autonomously generate and execute code for multi-tabular reasoning within electronic health records (EHRs). First, we formulate an EHR question-answering task into a tool-use planning process, efficiently decomposing a complicated task into a sequence of manageable actions. By integrating interactive coding and execution feedback, EHRAgent learns from error messages and improves the originally generated code through iterations. Furthermore, we enhance the LLM agent by incorporating long-term memory, which allows EHRAgent to effectively select and build upon the most relevant successful cases from past experiences. Experiments on three real-world multi-tabular EHR datasets show t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#23558;&#25991;&#26412;&#29983;&#25104;&#24418;&#24335;&#21270;&#20026;&#26410;&#26469;&#21463;&#38480;&#29983;&#25104;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;&#19981;&#33391;&#34892;&#20026;&#24182;&#24378;&#21046;&#25191;&#34892;&#23545;&#25351;&#20196;&#30340;&#24544;&#23454;&#24615;&#65292;&#24182;&#36890;&#36807;LLMs&#26377;&#25928;&#25351;&#23548;&#25991;&#26412;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2312.06149</link><description>&lt;p&gt;
&#35299;&#38145;&#39044;&#27979;&#24615;&#25991;&#26412;&#29983;&#25104;&#65306;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#30721;&#30340;&#21463;&#38480;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unlocking Anticipatory Text Generation: A Constrained Approach for Large Language Models Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06149
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#23558;&#25991;&#26412;&#29983;&#25104;&#24418;&#24335;&#21270;&#20026;&#26410;&#26469;&#21463;&#38480;&#29983;&#25104;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;&#19981;&#33391;&#34892;&#20026;&#24182;&#24378;&#21046;&#25191;&#34892;&#23545;&#25351;&#20196;&#30340;&#24544;&#23454;&#24615;&#65292;&#24182;&#36890;&#36807;LLMs&#26377;&#25928;&#25351;&#23548;&#25991;&#26412;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#32473;&#23450;&#25552;&#31034;&#25110;&#25351;&#20196;&#23454;&#29616;&#26368;&#20339;&#32467;&#26524;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21313;&#20159;&#32423;&#21035;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#19981;&#33391;&#34892;&#20026;&#22914;&#27602;&#24615;&#25110;&#24187;&#35273;&#21487;&#33021;&#20250;&#26174;&#29616;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#25991;&#26412;&#29983;&#25104;&#24418;&#24335;&#21270;&#20026;&#26410;&#26469;&#21463;&#38480;&#29983;&#25104;&#38382;&#39064;&#65292;&#20197;&#26368;&#23567;&#21270;&#19981;&#33391;&#34892;&#20026;&#24182;&#24378;&#21046;&#25191;&#34892;&#23545;&#25351;&#20196;&#30340;&#24544;&#23454;&#24615;&#12290;&#20351;&#29992;LLMs&#23454;&#29616;&#26410;&#26469;&#32422;&#26463;&#28385;&#36275;&#24230;&#30340;&#20272;&#35745;&#24341;&#23548;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65306;&#20851;&#38190;&#35789;&#21463;&#38480;&#29983;&#25104;&#12289;&#27602;&#24615;&#20943;&#23569;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06149v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have demonstrated a powerful ability for text generation. However, achieving optimal results with a given prompt or instruction can be challenging, especially for billion-sized models. Additionally, undesired behaviors such as toxicity or hallucinations can manifest. While much larger models (e.g., ChatGPT) may demonstrate strength in mitigating these issues, there is still no guarantee of complete prevention. In this work, we propose formalizing text generation as a future-constrained generation problem to minimize undesirable behaviors and enforce faithfulness to instructions. The estimation of future constraint satisfaction, accomplished using LLMs, guides the text generation process. Our extensive experiments demonstrate the effectiveness of the proposed approach across three distinct text generation tasks: keyword-constrained generation (Lin et al., 2020), toxicity reduction (Gehman et al., 202
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LINK&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#31995;&#32479;&#24615;&#22320;&#29983;&#25104;&#38271;&#23614;&#25512;&#29702;&#30693;&#35782;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#35780;&#20272;LLMs&#22312;&#25512;&#29702;&#31354;&#38388;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2311.07237</link><description>&lt;p&gt;
&#22312;&#25628;&#32034;&#38271;&#23614;&#20013;&#65306;&#36890;&#36807;&#36923;&#36753;&#35268;&#21017;&#24341;&#23548;&#25628;&#32034;&#31995;&#32479;&#24615;&#29983;&#25104;&#38271;&#23614;&#25512;&#29702;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
In Search of the Long-Tail: Systematic Generation of Long-Tail Inferential Knowledge via Logical Rule Guided Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07237
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LINK&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#31995;&#32479;&#24615;&#22320;&#29983;&#25104;&#38271;&#23614;&#25512;&#29702;&#30693;&#35782;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#35780;&#20272;LLMs&#22312;&#25512;&#29702;&#31354;&#38388;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;LLMs&#22312;&#35832;&#22914;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#31561;&#25512;&#29702;&#20219;&#21153;&#19978;&#32988;&#36807;&#20154;&#31867;&#12290;&#26368;&#36817;&#35780;&#20272;LLMs&#30340;&#30740;&#31350;&#25351;&#20986;&#65292;&#22312;&#26469;&#33258;&#20302;&#27010;&#29575;&#20998;&#24067;&#8212;&#8212;&#21363;&#38271;&#23614;&#30340;&#36755;&#20837;&#25968;&#25454;&#19978;&#34920;&#29616;&#22823;&#24133;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#31995;&#32479;&#29983;&#25104;&#28041;&#21450;&#38271;&#23614;&#25512;&#29702;&#30693;&#35782;&#30340;&#35821;&#21477;&#65292;&#20197;&#26356;&#26377;&#25928;&#22320;&#35780;&#20272;LLMs&#22312;&#25512;&#29702;&#31354;&#38388;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;Logic-Induced-Knowledge-Search&#65288;LINK&#65289;&#65292;&#35813;&#26694;&#26550;&#29983;&#25104;&#22522;&#20110;&#31526;&#21495;&#35268;&#21017;&#27169;&#26495;&#30340;&#20107;&#23454;&#27491;&#30830;&#19988;&#38271;&#23614;&#30693;&#35782;&#35821;&#21477;&#65307;LINK&#26377;&#25928;&#22320;&#29983;&#25104;&#38271;&#23614;&#20998;&#24067;&#25968;&#25454;&#65292;&#38646;-shot&#25552;&#31034;&#30340;LLMs&#26080;&#27861;&#21040;&#36798;&#65292;&#24182;&#19988;&#22312;&#20107;&#23454;&#27491;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#38646;-shot GPT4&#36798;&#21040;5%&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;LINK&#29983;&#25104;&#30340;&#25968;&#25454;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;Logic-Induced-Long-Tail&#65288;LINT&#65289;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#38271;&#23614;&#20998;&#24067;&#19978;&#30340;&#19979;&#28216;&#27169;&#22411;&#65307;LINT&#21253;&#21547;108K&#20010;&#30693;&#35782;&#26465;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07237v2 Announce Type: replace-cross  Abstract: State-of-the-art LLMs outperform humans on reasoning tasks such as Natural Language Inference. Recent works evaluating LLMs note a marked performance drop on input data from the low-probability distribution, i.e., the longtail. Therefore, we focus on systematically generating statements involving long-tail inferential knowledge for more effective evaluation of LLMs in the reasoning space. We first propose a novel framework Logic-Induced- Knowledge-Search (LINK) that generates factually correct and long-tail knowledge statements grounded on symbolic rule templates; LINK effectively generates data in the longtail distribution that zero-shot prompted LLMs are unable to reach, and outperforms zero-shot GPT4 on factual correctness by 5%. We further use the data generated by LINK to construct a dataset Logic-Induced-Long-Tail (LINT) that can be used to evaluate downstream models on the long-tail distribution; LINT contains 108K knowl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26368;&#20248;&#20256;&#36755;&#36827;&#34892;&#20998;&#24067;&#24335;&#23545;&#25239;&#35299;&#37322;&#30340;&#26041;&#27861;DISCOUNT&#65292;&#23558;&#23545;&#25239;&#35299;&#37322;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#25972;&#20010;&#36755;&#20837;&#36755;&#20986;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#32479;&#35745;&#32622;&#20449;&#24230;&#26469;&#25903;&#25745;&#36825;&#19968;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.13112</link><description>&lt;p&gt;
DISCOUNT: &#20351;&#29992;&#26368;&#20248;&#20256;&#36755;&#36827;&#34892;&#20998;&#24067;&#24335;&#23545;&#25239;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
DISCOUNT: Distributional Counterfactual Explanation With Optimal Transport. (arXiv:2401.13112v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26368;&#20248;&#20256;&#36755;&#36827;&#34892;&#20998;&#24067;&#24335;&#23545;&#25239;&#35299;&#37322;&#30340;&#26041;&#27861;DISCOUNT&#65292;&#23558;&#23545;&#25239;&#35299;&#37322;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#25972;&#20010;&#36755;&#20837;&#36755;&#20986;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#32479;&#35745;&#32622;&#20449;&#24230;&#26469;&#25903;&#25745;&#36825;&#19968;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35299;&#37322;&#26159;&#22312;&#40657;&#30418;&#20915;&#31574;&#27169;&#22411;&#20013;&#25552;&#20379;&#27934;&#23519;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20107;&#23454;&#26041;&#27861;&#65292;&#36890;&#36807;&#30830;&#23450;&#23548;&#33268;&#19981;&#21516;&#32467;&#26524;&#30340;&#26367;&#20195;&#36755;&#20837;&#23454;&#20363;&#26469;&#23454;&#29616;&#12290;&#26412;&#25991;&#23558;&#23545;&#25239;&#35299;&#37322;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#20998;&#24067;&#19978;&#19979;&#25991;&#65292;&#20174;&#20010;&#20307;&#25968;&#25454;&#28857;&#25193;&#22823;&#21040;&#25972;&#20010;&#36755;&#20837;&#36755;&#20986;&#20998;&#24067;&#65292;&#21629;&#21517;&#20026;&#20998;&#24067;&#24335;&#23545;&#25239;&#35299;&#37322;&#12290;&#22312;&#20998;&#24067;&#24335;&#23545;&#25239;&#35299;&#37322;&#20013;&#65292;&#25105;&#20204;&#30340;&#37325;&#28857;&#36716;&#21521;&#20998;&#26512;&#20107;&#23454;&#21644;&#23545;&#25239;&#30340;&#20998;&#24067;&#23646;&#24615;&#65292;&#31867;&#20284;&#20110;&#35780;&#20272;&#20010;&#20307;&#23454;&#20363;&#21450;&#20854;&#32467;&#26524;&#20915;&#31574;&#30340;&#32463;&#20856;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;&#26469;&#26500;&#24314;&#19968;&#20010;&#26426;&#20250;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#23548;&#20986;&#19982;&#20107;&#23454;&#23545;&#24212;&#30340;&#23545;&#25239;&#20998;&#24067;&#65292;&#20197;&#32479;&#35745;&#32622;&#20449;&#24230;&#20570;&#25903;&#25745;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20248;&#21270;&#26041;&#27861;DISCOUNT&#22312;&#36755;&#20837;&#21644;&#36755;&#20986;&#20998;&#24067;&#20043;&#38388;&#24179;&#34913;&#36825;&#31181;&#32622;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Explanations (CE) is the de facto method for providing insight and interpretability in black-box decision-making models by identifying alternative input instances that lead to different outcomes. This paper extends the concept of CEs to a distributional context, broadening the scope from individual data points to entire input and output distributions, named Distributional Counterfactual Explanation (DCE). In DCE, our focus shifts to analyzing the distributional properties of the factual and counterfactual, drawing parallels to the classical approach of assessing individual instances and their resulting decisions. We leverage Optimal Transport (OT) to frame a chance-constrained optimization problem, aiming to derive a counterfactual distribution that closely aligns with its factual counterpart, substantiated by statistical confidence. Our proposed optimization method, DISCOUNT, strategically balances this confidence across both input and output distributions. This algorit
&lt;/p&gt;</description></item><item><title>&#23558;&#36136;&#37327;&#22810;&#26679;&#24615;&#20248;&#21270;&#19982;&#25551;&#36848;&#31526;&#26465;&#20214;&#21152;&#24378;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#20811;&#26381;&#36827;&#21270;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#29983;&#25104;&#26082;&#22810;&#26679;&#21448;&#39640;&#24615;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#38598;&#21512;&#26041;&#38754;&#21462;&#24471;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/2401.08632</link><description>&lt;p&gt;
&#23558;&#36136;&#37327;&#22810;&#26679;&#24615;&#19982;&#25551;&#36848;&#31526;&#26465;&#20214;&#21152;&#24378;&#23398;&#20064;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
Synergizing Quality-Diversity with Descriptor-Conditioned Reinforcement Learning. (arXiv:2401.08632v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08632
&lt;/p&gt;
&lt;p&gt;
&#23558;&#36136;&#37327;&#22810;&#26679;&#24615;&#20248;&#21270;&#19982;&#25551;&#36848;&#31526;&#26465;&#20214;&#21152;&#24378;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#20811;&#26381;&#36827;&#21270;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#29983;&#25104;&#26082;&#22810;&#26679;&#21448;&#39640;&#24615;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#38598;&#21512;&#26041;&#38754;&#21462;&#24471;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#30340;&#22522;&#26412;&#29305;&#24449;&#20043;&#19968;&#26159;&#25214;&#21040;&#26032;&#39062;&#21644;&#26377;&#21019;&#36896;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#32473;&#23450;&#30340;&#25361;&#25112;&#25110;&#36866;&#24212;&#26410;&#39044;&#26009;&#21040;&#30340;&#24773;&#20917;&#12290;&#36136;&#37327;&#22810;&#26679;&#24615;&#20248;&#21270;&#26159;&#19968;&#31867;&#36827;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#26082;&#22810;&#26679;&#21448;&#39640;&#24615;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#38598;&#21512;&#12290;&#20854;&#20013;&#65292;MAP-Elites&#26159;&#19968;&#20010;&#33879;&#21517;&#30340;&#20363;&#23376;&#65292;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#31181;&#39046;&#22495;&#65292;&#21253;&#25324;&#36827;&#21270;&#26426;&#22120;&#20154;&#23398;&#12290;&#28982;&#32780;&#65292;MAP-Elites&#36890;&#36807;&#36951;&#20256;&#31639;&#27861;&#30340;&#38543;&#26426;&#31361;&#21464;&#36827;&#34892;&#21457;&#25955;&#25628;&#32034;&#65292;&#22240;&#27492;&#20165;&#38480;&#20110;&#36827;&#21270;&#20302;&#32500;&#35299;&#20915;&#26041;&#26696;&#30340;&#31181;&#32676;&#12290;PGA-MAP-Elites&#36890;&#36807;&#21463;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21551;&#21457;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#21464;&#24322;&#31639;&#23376;&#20811;&#26381;&#20102;&#36825;&#19968;&#38480;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#36827;&#21270;&#12290;&#23613;&#31649;&#22312;&#35768;&#22810;&#29615;&#22659;&#20013;&#24615;&#33021;&#20248;&#31168;&#65292;&#20294;PGA-MAP-Elites&#22312;&#19968;&#20123;&#20219;&#21153;&#20013;&#22833;&#36133;&#65292;&#20854;&#20013;&#22522;&#20110;&#26799;&#24230;&#30340;&#21464;&#24322;&#31639;&#23376;&#30340;&#25910;&#25947;&#25628;&#32034;&#38459;&#30861;&#20102;&#22810;&#26679;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;...
&lt;/p&gt;
&lt;p&gt;
A fundamental trait of intelligence involves finding novel and creative solutions to address a given challenge or to adapt to unforeseen situations. Reflecting this, Quality-Diversity optimization is a family of Evolutionary Algorithms, that generates collections of both diverse and high-performing solutions. Among these, MAP-Elites is a prominent example, that has been successfully applied to a variety of domains, including evolutionary robotics. However, MAP-Elites performs a divergent search with random mutations originating from Genetic Algorithms, and thus, is limited to evolving populations of low-dimensional solutions. PGA-MAP-Elites overcomes this limitation using a gradient-based variation operator inspired by deep reinforcement learning which enables the evolution of large neural networks. Although high-performing in many environments, PGA-MAP-Elites fails on several tasks where the convergent search of the gradient-based variation operator hinders diversity. In this work, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#23454;&#38469;&#31995;&#32479;&#20013;&#26816;&#27979;&#21644;&#28040;&#38500;&#24694;&#24847;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.04055</link><description>&lt;p&gt;
&#25226;&#22351;&#20154;&#36386;&#20986;&#21435;&#65281;&#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Kick Bad Guys Out! Zero-Knowledge-Proof-Based Anomaly Detection in Federated Learning. (arXiv:2310.04055v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#23454;&#38469;&#31995;&#32479;&#20013;&#26816;&#27979;&#21644;&#28040;&#38500;&#24694;&#24847;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#23458;&#25143;&#31471;&#30340;&#25915;&#20987;&#65292;&#20182;&#20204;&#36890;&#36807;&#25552;&#20132;&#31713;&#25913;&#30340;&#26412;&#22320;&#27169;&#22411;&#26469;&#36798;&#21040;&#23545;&#25239;&#30446;&#26631;&#65292;&#27604;&#22914;&#38459;&#27490;&#20840;&#23616;&#27169;&#22411;&#30340;&#25910;&#25947;&#25110;&#32773;&#23548;&#33268;&#20840;&#23616;&#27169;&#22411;&#23545;&#26576;&#20123;&#25968;&#25454;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#12290;&#35768;&#22810;&#29616;&#26377;&#30340;&#38450;&#24481;&#26426;&#21046;&#22312;&#23454;&#38469;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#19981;&#21487;&#34892;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#20808;&#30693;&#36947;&#24694;&#24847;&#23458;&#25143;&#31471;&#30340;&#25968;&#37327;&#65292;&#25110;&#32773;&#20381;&#36182;&#37325;&#26032;&#21152;&#26435;&#25110;&#20462;&#25913;&#25552;&#20132;&#30340;&#26041;&#24335;&#12290;&#36825;&#26159;&#22240;&#20026;&#25915;&#20987;&#32773;&#36890;&#24120;&#19981;&#20250;&#22312;&#25915;&#20987;&#20043;&#21069;&#23459;&#24067;&#20182;&#20204;&#30340;&#24847;&#22270;&#65292;&#32780;&#37325;&#26032;&#21152;&#26435;&#21487;&#33021;&#20250;&#25913;&#21464;&#32858;&#21512;&#32467;&#26524;&#65292;&#21363;&#20351;&#27809;&#26377;&#25915;&#20987;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#22312;&#23454;&#38469;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26368;&#23574;&#31471;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#20855;&#26377;&#20197;&#19979;&#29305;&#28857;&#65306;i&#65289;&#20165;&#22312;&#21457;&#29983;&#25915;&#20987;&#26102;&#26816;&#27979;&#25915;&#20987;&#30340;&#21457;&#29983;&#24182;&#36827;&#34892;&#38450;&#24481;&#25805;&#20316;&#65307;ii&#65289;&#19968;&#26086;&#21457;&#29983;&#25915;&#20987;&#65292;&#36827;&#19968;&#27493;&#26816;&#27979;&#24694;&#24847;&#23458;&#25143;&#31471;&#27169;&#22411;&#24182;&#23558;&#20854;&#28040;&#38500;&#65292;&#32780;&#19981;&#20250;&#23545;&#27491;&#24120;&#27169;&#22411;&#36896;&#25104;&#20260;&#23475;&#65307;iii&#65289;&#30830;&#20445;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) systems are vulnerable to malicious clients that submit poisoned local models to achieve their adversarial goals, such as preventing the convergence of the global model or inducing the global model to misclassify some data. Many existing defense mechanisms are impractical in real-world FL systems, as they require prior knowledge of the number of malicious clients or rely on re-weighting or modifying submissions. This is because adversaries typically do not announce their intentions before attacking, and re-weighting might change aggregation results even in the absence of attacks. To address these challenges in real FL systems, this paper introduces a cutting-edge anomaly detection approach with the following features: i) Detecting the occurrence of attacks and performing defense operations only when attacks happen; ii) Upon the occurrence of an attack, further detecting the malicious client models and eliminating them without harming the benign ones; iii) Ensuri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#36827;&#34892;&#20174;&#35266;&#23519;&#20013;&#30340;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#23398;&#20064;&#27169;&#22411;&#25110;&#23545;&#25239;&#23398;&#20064;&#65292;&#21487;&#20197;&#19982;&#20219;&#20309;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#38598;&#25104;&#65292;&#24182;&#22312;&#21508;&#31181;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#22312;ILfO&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#19987;&#23478;&#32423;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.01632</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#36827;&#34892;&#20174;&#35266;&#23519;&#20013;&#30340;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning from Observation through Optimal Transport. (arXiv:2310.01632v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#36827;&#34892;&#20174;&#35266;&#23519;&#20013;&#30340;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#23398;&#20064;&#27169;&#22411;&#25110;&#23545;&#25239;&#23398;&#20064;&#65292;&#21487;&#20197;&#19982;&#20219;&#20309;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#38598;&#25104;&#65292;&#24182;&#22312;&#21508;&#31181;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#22312;ILfO&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#19987;&#23478;&#32423;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#23519;&#20013;&#30340;&#27169;&#20223;&#23398;&#20064;&#65288;ILfO&#65289;&#26159;&#19968;&#31181;&#23398;&#20064;&#32773;&#35797;&#22270;&#22312;&#27809;&#26377;&#30452;&#25509;&#25351;&#23548;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#35266;&#27979;&#25968;&#25454;&#27169;&#20223;&#19987;&#23478;&#34892;&#20026;&#30340;&#35774;&#32622;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#26368;&#20248;&#36755;&#36816;&#22312;IL&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#26681;&#25454;&#23398;&#20064;&#32773;&#21644;&#19987;&#23478;&#30340;&#29366;&#24577;&#36712;&#36857;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#29983;&#25104;&#22870;&#21169;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#29616;&#26377;&#26041;&#27861;&#21487;&#20197;&#31616;&#21270;&#20026;&#29983;&#25104;&#26080;&#38656;&#23398;&#20064;&#27169;&#22411;&#25110;&#23545;&#25239;&#23398;&#20064;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#19982;&#35768;&#22810;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#19982;&#20219;&#20309;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#38598;&#25104;&#65292;&#24182;&#36866;&#29992;&#20110;ILfO&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#36825;&#31181;&#31616;&#21333;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;&#21363;&#20351;&#21482;&#35266;&#23519;&#21333;&#20010;&#19987;&#23478;&#36712;&#36857;&#32780;&#27809;&#26377;&#21160;&#20316;&#65292;&#23427;&#22312;ILfO&#35774;&#32622;&#20013;&#36229;&#36807;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#22312;&#19968;&#31995;&#21015;&#35780;&#20272;&#39046;&#22495;&#20013;&#23454;&#29616;&#20102;&#19987;&#23478;&#32423;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning from Observation (ILfO) is a setting in which a learner tries to imitate the behavior of an expert, using only observational data and without the direct guidance of demonstrated actions. In this paper, we re-examine the use of optimal transport for IL, in which a reward is generated based on the Wasserstein distance between the state trajectories of the learner and expert. We show that existing methods can be simplified to generate a reward function without requiring learned models or adversarial learning. Unlike many other state-of-the-art methods, our approach can be integrated with any RL algorithm, and is amenable to ILfO. We demonstrate the effectiveness of this simple approach on a variety of continuous control tasks and find that it surpasses the state of the art in the IlfO setting, achieving expert-level performance across a range of evaluation domains even when observing only a single expert trajectory without actions.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#21069;&#39069;&#21494;&#30382;&#23618;&#21551;&#21457;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35268;&#21010;&#26550;&#26500;&#65292;&#21033;&#29992;&#22810;&#20010;&#22522;&#20110;LLM&#30340;&#27169;&#22359;&#23454;&#29616;&#35268;&#21010;&#30340;&#33258;&#20027;&#21327;&#35843;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#25110;&#30446;&#26631;&#23548;&#21521;&#35268;&#21010;&#30340;&#20219;&#21153;&#26102;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.00194</link><description>&lt;p&gt;
&#21463;&#21069;&#39069;&#21494;&#30382;&#23618;&#21551;&#21457;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35268;&#21010;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
A Prefrontal Cortex-inspired Architecture for Planning in Large Language Models. (arXiv:2310.00194v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00194
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#21069;&#39069;&#21494;&#30382;&#23618;&#21551;&#21457;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35268;&#21010;&#26550;&#26500;&#65292;&#21033;&#29992;&#22810;&#20010;&#22522;&#20110;LLM&#30340;&#27169;&#22359;&#23454;&#29616;&#35268;&#21010;&#30340;&#33258;&#20027;&#21327;&#35843;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#25110;&#30446;&#26631;&#23548;&#21521;&#35268;&#21010;&#30340;&#20219;&#21153;&#26102;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#24778;&#20154;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#22312;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#25110;&#30446;&#26631;&#23548;&#21521;&#35268;&#21010;&#30340;&#20219;&#21153;&#20013;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#20154;&#33041;&#20013;&#33719;&#21462;&#28789;&#24863;&#65292;&#21363;&#36890;&#36807;&#21069;&#39069;&#21494;&#30382;&#23618;&#65288;PFC&#65289;&#20013;&#19987;&#38376;&#27169;&#22359;&#30340;&#37325;&#22797;&#20132;&#20114;&#26469;&#23436;&#25104;&#35268;&#21010;&#12290;&#36825;&#20123;&#27169;&#22359;&#25191;&#34892;&#20914;&#31361;&#30417;&#27979;&#12289;&#29366;&#24577;&#39044;&#27979;&#12289;&#29366;&#24577;&#35780;&#20272;&#12289;&#20219;&#21153;&#20998;&#35299;&#21644;&#20219;&#21153;&#21327;&#35843;&#31561;&#21151;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;LLM&#26377;&#26102;&#33021;&#22815;&#21333;&#29420;&#25191;&#34892;&#36825;&#20123;&#21151;&#33021;&#65292;&#20294;&#22312;&#26381;&#21153;&#20110;&#19968;&#20010;&#30446;&#26631;&#26102;&#24448;&#24448;&#38590;&#20197;&#33258;&#20027;&#21327;&#35843;&#23427;&#20204;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#22810;&#20010;&#22522;&#20110;LLM&#65288;GPT-4&#65289;&#27169;&#22359;&#30340;&#40657;&#30418;&#26550;&#26500;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#19987;&#38376;&#30340;PFC&#21551;&#21457;&#27169;&#22359;&#30340;&#20132;&#20114;&#23558;&#19968;&#20010;&#26356;&#22823;&#30340;&#38382;&#39064;&#20998;&#35299;&#20026;&#22810;&#20010;&#23545;LLM&#30340;&#31616;&#30701;&#33258;&#21160;&#35843;&#29992;&#65292;&#20174;&#32780;&#25913;&#21892;&#35268;&#21010;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35268;&#21010;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#32452;&#21512;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) demonstrate impressive performance on a wide variety of tasks, but they often struggle with tasks that require multi-step reasoning or goal-directed planning. To address this, we take inspiration from the human brain, in which planning is accomplished via the recurrent interaction of specialized modules in the prefrontal cortex (PFC). These modules perform functions such as conflict monitoring, state prediction, state evaluation, task decomposition, and task coordination. We find that LLMs are sometimes capable of carrying out these functions in isolation, but struggle to autonomously coordinate them in the service of a goal. Therefore, we propose a black box architecture with multiple LLM-based (GPT-4) modules. The architecture improves planning through the interaction of specialized PFC-inspired modules that break down a larger problem into multiple brief automated calls to the LLM. We evaluate the combined architecture on two challenging planning tasks -
&lt;/p&gt;</description></item><item><title>MosaicFusion&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35789;&#27719;&#23454;&#20363;&#20998;&#21106;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#65292;&#33021;&#22815;&#29983;&#25104;&#22823;&#37327;&#21512;&#25104;&#26631;&#35760;&#25968;&#25454;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.13042</link><description>&lt;p&gt;
MosaicFusion: &#23558;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#22823;&#35789;&#27719;&#23454;&#20363;&#20998;&#21106;&#30340;&#25968;&#25454;&#22686;&#24378;&#22120;
&lt;/p&gt;
&lt;p&gt;
MosaicFusion: Diffusion Models as Data Augmenters for Large Vocabulary Instance Segmentation. (arXiv:2309.13042v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13042
&lt;/p&gt;
&lt;p&gt;
MosaicFusion&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35789;&#27719;&#23454;&#20363;&#20998;&#21106;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#65292;&#33021;&#22815;&#29983;&#25104;&#22823;&#37327;&#21512;&#25104;&#26631;&#35760;&#25968;&#25454;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MosaicFusion&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#35789;&#27719;&#23454;&#20363;&#20998;&#21106;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26080;&#38656;&#35757;&#32451;&#65292;&#20063;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#26631;&#31614;&#30417;&#30563;&#12290;&#20004;&#20010;&#20851;&#38190;&#35774;&#35745;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#29616;&#25104;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#26377;&#29992;&#30340;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#23545;&#35937;&#23454;&#20363;&#21644;&#33945;&#29256;&#27880;&#37322;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#22270;&#20687;&#30011;&#24067;&#20998;&#20026;&#20960;&#20010;&#21306;&#22495;&#65292;&#24182;&#25191;&#34892;&#19968;&#36718;&#25193;&#25955;&#36807;&#31243;&#65292;&#21516;&#26102;&#22522;&#20110;&#19981;&#21516;&#30340;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#22810;&#20010;&#23454;&#20363;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#32858;&#21512;&#19982;&#23545;&#35937;&#25552;&#31034;&#30456;&#20851;&#32852;&#30340;&#36328;&#27880;&#24847;&#21147;&#22270;&#22312;&#23618;&#21644;&#25193;&#25955;&#26102;&#38388;&#27493;&#19978;&#65292;&#28982;&#21518;&#36827;&#34892;&#31616;&#21333;&#30340;&#38408;&#20540;&#22788;&#29702;&#21644;&#36793;&#32536;&#24863;&#30693;&#30340;&#32454;&#21270;&#22788;&#29702;&#65292;&#24471;&#21040;&#30456;&#24212;&#30340;&#23454;&#20363;&#33945;&#29256;&#12290;&#25105;&#20204;&#30340;MosaicFusion&#21487;&#20197;&#20026;&#31232;&#32570;&#21644;&#26032;&#39062;&#31867;&#21035;&#20135;&#29983;&#22823;&#37327;&#30340;&#21512;&#25104;&#26631;&#35760;&#25968;&#25454;&#65292;&#32780;&#26080;&#38656;&#22797;&#26434;&#30340;&#22788;&#29702;&#12290;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;LVIS&#38271;&#23614;&#21644;&#24320;&#25918;&#35789;&#27719;&#22522;&#20934;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#22343;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present MosaicFusion, a simple yet effective diffusion-based data augmentation approach for large vocabulary instance segmentation. Our method is training-free and does not rely on any label supervision. Two key designs enable us to employ an off-the-shelf text-to-image diffusion model as a useful dataset generator for object instances and mask annotations. First, we divide an image canvas into several regions and perform a single round of diffusion process to generate multiple instances simultaneously, conditioning on different text prompts. Second, we obtain corresponding instance masks by aggregating cross-attention maps associated with object prompts across layers and diffusion time steps, followed by simple thresholding and edge-aware refinement processing. Without bells and whistles, our MosaicFusion can produce a significant amount of synthetic labeled data for both rare and novel categories. Experimental results on the challenging LVIS long-tailed and open-vocabulary benchma
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#22635;&#34917;&#20102;&#26102;&#38388;KG&#21644;&#36229;&#20851;&#31995;KG&#25512;&#29702;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#30340;&#22522;&#20934;&#36229;&#20851;&#31995;TKG&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2307.10219</link><description>&lt;p&gt;
&#22312;&#22686;&#24378;&#30340;&#19981;&#21464;&#20851;&#31995;&#30693;&#35782;&#19978;&#25506;&#32034;&#36229;&#20851;&#31995;&#26102;&#38388;&#30693;&#35782;&#22270;&#30340;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Exploring Link Prediction over Hyper-Relational Temporal Knowledge Graphs Enhanced with Time-Invariant Relational Knowledge. (arXiv:2307.10219v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10219
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#22635;&#34917;&#20102;&#26102;&#38388;KG&#21644;&#36229;&#20851;&#31995;KG&#25512;&#29702;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#30340;&#22522;&#20934;&#36229;&#20851;&#31995;TKG&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;(HKGs)&#26159;&#20256;&#32479;&#30693;&#35782;&#22270;(KGs)&#30340;&#24310;&#20280;&#65292;&#20026;&#27599;&#20010;KG&#20107;&#23454;&#25552;&#20379;&#39069;&#22806;&#30340;&#38190;&#20540;&#23545;(&#21363;&#38480;&#23450;&#35789;)&#65292;&#20197;&#26356;&#22909;&#22320;&#38480;&#21046;&#20107;&#23454;&#30340;&#26377;&#25928;&#24615;&#12290;&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#22312;HKGs&#19978;&#36827;&#34892;&#22270;&#25512;&#29702;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#30001;&#20110;&#19990;&#30028;&#30693;&#35782;&#30340;&#19981;&#26029;&#28436;&#21464;&#65292;&#22823;&#37327;&#24179;&#34892;&#24037;&#20316;&#38598;&#20013;&#22312;&#23545;&#26102;&#38388;KGs(TKGs)&#36827;&#34892;&#25512;&#29702;&#65292;&#20854;&#20013;&#27599;&#20010;TKG&#20107;&#23454;&#21487;&#20197;&#34987;&#35270;&#20026;&#24102;&#26377;&#26102;&#38388;&#25139;(&#25110;&#26102;&#38388;&#27573;)&#30340;KG&#20107;&#23454;&#65292;&#25351;&#23450;&#20854;&#26102;&#38388;&#26377;&#25928;&#24615;&#12290;&#29616;&#26377;&#30340;HKG&#25512;&#29702;&#26041;&#27861;&#19981;&#32771;&#34385;&#26102;&#38388;&#20449;&#24687;&#65292;&#22240;&#20026;&#22312;&#20043;&#21069;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#27809;&#26377;&#26174;&#24335;&#22320;&#25351;&#23450;&#12290;&#27492;&#22806;&#65292;&#25152;&#26377;&#20197;&#21069;&#30340;TKG&#25512;&#29702;&#26041;&#27861;&#21482;&#37325;&#35270;&#26102;&#38388;&#25512;&#29702;&#65292;&#24182;&#27809;&#26377;&#21150;&#27861;&#20174;&#38480;&#23450;&#35789;&#20013;&#23398;&#20064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22635;&#34917;TKG&#25512;&#29702;&#21644;HKG&#25512;&#29702;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#30340;&#22522;&#20934;&#36229;&#20851;&#31995;TKG(HTKG)&#25968;&#25454;&#38598;&#65292;&#21363;Wiki-hy&#21644;...
&lt;/p&gt;
&lt;p&gt;
Stemming from traditional knowledge graphs (KGs), hyper-relational KGs (HKGs) provide additional key-value pairs (i.e., qualifiers) for each KG fact that help to better restrict the fact validity. In recent years, there has been an increasing interest in studying graph reasoning over HKGs. In the meantime, due to the ever-evolving nature of world knowledge, extensive parallel works have been focusing on reasoning over temporal KGs (TKGs), where each TKG fact can be viewed as a KG fact coupled with a timestamp (or time period) specifying its time validity. The existing HKG reasoning approaches do not consider temporal information because it is not explicitly specified in previous benchmark datasets. Besides, all the previous TKG reasoning methods only lay emphasis on temporal reasoning and have no way to learn from qualifiers. To this end, we aim to fill the gap between TKG reasoning and HKG reasoning. We develop two new benchmark hyper-relational TKG (HTKG) datasets, i.e., Wiki-hy and 
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20840;&#38754;&#22238;&#39038;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20854;&#20013;&#30417;&#30563;&#12289;&#26080;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#26159;&#20027;&#35201;&#31867;&#21035;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#65292;&#21487;&#20197;&#20811;&#26381;&#26500;&#24314;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#22256;&#38590;&#65292;&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#25366;&#25496;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.10716</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#35757;&#32451;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Time-Series Pre-Trained Models. (arXiv:2305.10716v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20840;&#38754;&#22238;&#39038;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20854;&#20013;&#30417;&#30563;&#12289;&#26080;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#26159;&#20027;&#35201;&#31867;&#21035;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#65292;&#21487;&#20197;&#20811;&#26381;&#26500;&#24314;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#22256;&#38590;&#65292;&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#25366;&#25496;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25366;&#25496;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#22240;&#20026;&#23427;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#20381;&#36182;&#20110;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#25104;&#21151;&#22320;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25366;&#25496;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#27880;&#37322;&#25104;&#26412;&#30340;&#21407;&#22240;&#65292;&#26500;&#24314;&#22823;&#35268;&#27169;&#12289;&#33391;&#22909;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#26159;&#22256;&#38590;&#30340;&#12290;&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#36880;&#28176;&#24341;&#36215;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#22238;&#39038;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;TS-PTMs&#65289;&#65292;&#26088;&#22312;&#25351;&#23548;&#20102;&#35299;&#12289;&#24212;&#29992;&#21644;&#30740;&#31350;TS-PTMs&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20808;&#31616;&#35201;&#20171;&#32461;&#20102;TSM&#20013;&#20351;&#29992;&#30340;&#20856;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#39044;&#35757;&#32451;&#25216;&#26415;&#27010;&#36848;&#20102;TS-PTMs&#12290;&#25105;&#20204;&#25506;&#35752;&#30340;&#20027;&#35201;&#31867;&#21035;&#21253;&#25324;&#30417;&#30563;&#12289;&#26080;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;TS-PTMs&#12290;&#27492;&#22806;&#65292;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#26469;&#20998;&#26512;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time-Series Mining (TSM) is an important research area since it shows great potential in practical applications. Deep learning models that rely on massive labeled data have been utilized for TSM successfully. However, constructing a large-scale well-labeled dataset is difficult due to data annotation costs. Recently, Pre-Trained Models have gradually attracted attention in the time series domain due to their remarkable performance in computer vision and natural language processing. In this survey, we provide a comprehensive review of Time-Series Pre-Trained Models (TS-PTMs), aiming to guide the understanding, applying, and studying TS-PTMs. Specifically, we first briefly introduce the typical deep learning models employed in TSM. Then, we give an overview of TS-PTMs according to the pre-training techniques. The main categories we explore include supervised, unsupervised, and self-supervised TS-PTMs. Further, extensive experiments are conducted to analyze the advantages and disadvantage
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;FEDORA&#30340;&#32852;&#37030;&#38598;&#25104;&#25351;&#23548;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#25552;&#28860;&#23458;&#25143;&#32676;&#20307;&#30340;&#38598;&#20307;&#26234;&#24935;&#65292;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#21253;&#25324;&#22312;&#21512;&#24182;&#30340;&#25968;&#25454;&#27719;&#24635;&#20013;&#36827;&#34892;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#21508;&#31181;&#22797;&#26434;&#30340;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2305.03097</link><description>&lt;p&gt;
&#32852;&#37030;&#38598;&#25104;&#25351;&#23548;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Federated Ensemble-Directed Offline Reinforcement Learning. (arXiv:2305.03097v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;FEDORA&#30340;&#32852;&#37030;&#38598;&#25104;&#25351;&#23548;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#25552;&#28860;&#23458;&#25143;&#32676;&#20307;&#30340;&#38598;&#20307;&#26234;&#24935;&#65292;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#21253;&#25324;&#22312;&#21512;&#24182;&#30340;&#25968;&#25454;&#27719;&#24635;&#20013;&#36827;&#34892;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#21508;&#31181;&#22797;&#26434;&#30340;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#32852;&#37030;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;&#36825;&#19968;&#22330;&#26223;&#19979;&#65292;&#20998;&#24067;&#24335;&#30340;&#23398;&#20064;&#20195;&#29702;&#24517;&#39035;&#20165;&#20351;&#29992;&#30001;&#19981;&#21516;&#30340;&#26410;&#30693;&#30340;&#34892;&#20026;&#31574;&#30053;&#29983;&#25104;&#30340;&#23567;&#22411;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#21327;&#20316;&#23398;&#20064;&#20986;&#39640;&#36136;&#37327;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;&#31528;&#25305;&#22320;&#23558;&#26631;&#20934;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#19982;&#26631;&#20934;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#32452;&#21512;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#21487;&#33021;&#20250;&#23548;&#33268;&#34920;&#29616;&#19981;&#20339;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#22240;&#27492;&#35774;&#35745;&#20102;Federated Ensemble-Directed Offline Reinforcement Learning Algorithm (FEDORA)&#65292;&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#25552;&#28860;&#23458;&#25143;&#32676;&#20307;&#30340;&#38598;&#20307;&#26234;&#24935;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;FEDORA&#20195;&#30721;&#24211;&#65292;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#19978;&#30340;&#20998;&#24067;&#24335;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;FEDORA&#22312;&#21508;&#31181;&#22797;&#26434;&#30340;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#22343;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#21253;&#25324;&#22312;&#21512;&#24182;&#30340;&#25968;&#25454;&#27719;&#24635;&#20013;&#36827;&#34892;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;FEDORA&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of federated offline reinforcement learning (RL), a scenario under which distributed learning agents must collaboratively learn a high-quality control policy only using small pre-collected datasets generated according to different unknown behavior policies. Naively combining a standard offline RL approach with a standard federated learning approach to solve this problem can lead to poorly performing policies. In response, we develop the Federated Ensemble-Directed Offline Reinforcement Learning Algorithm (FEDORA), which distills the collective wisdom of the clients using an ensemble learning approach. We develop the FEDORA codebase to utilize distributed compute resources on a federated learning platform. We show that FEDORA significantly outperforms other approaches, including offline RL over the combined data pool, in various complex continuous control environments and real world datasets. Finally, we demonstrate the performance of FEDORA in the real-world on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;E-MCTS&#65292;&#36890;&#36807;&#22312;MCTS&#39044;&#27979;&#20013;&#24212;&#29992;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#22522;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28145;&#24230;&#25506;&#32034;&#65292;&#20197;&#21450;&#35268;&#21010;&#25506;&#32034;&#31574;&#30053;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#25104;&#21151;&#30340;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#28145;&#24230;&#25506;&#32034;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2210.13455</link><description>&lt;p&gt;
E-MCTS&#65306;&#36890;&#36807;&#35268;&#21010;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#28145;&#24230;&#25506;&#32034;&#30340;&#27169;&#22411;&#22522;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
E-MCTS: Deep Exploration in Model-Based Reinforcement Learning by Planning with Epistemic Uncertainty. (arXiv:2210.13455v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;E-MCTS&#65292;&#36890;&#36807;&#22312;MCTS&#39044;&#27979;&#20013;&#24212;&#29992;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#22522;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28145;&#24230;&#25506;&#32034;&#65292;&#20197;&#21450;&#35268;&#21010;&#25506;&#32034;&#31574;&#30053;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#25104;&#21151;&#30340;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#28145;&#24230;&#25506;&#32034;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#36864;&#28779;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#26159;&#27169;&#22411;&#22522;&#24378;&#21270;&#23398;&#20064;&#20013;&#24212;&#29992;&#26368;&#24191;&#27867;&#12289;&#24615;&#33021;&#26368;&#20248;&#31168;&#30340;&#35268;&#21010;&#26041;&#27861;&#20043;&#19968;&#12290;MCTS&#30340;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#28145;&#24230;&#25506;&#32034;&#21644;&#38754;&#23545;&#26410;&#30693;&#26102;&#30340;&#21487;&#38752;&#24615;&#65292;&#36825;&#20004;&#20010;&#25361;&#25112;&#21487;&#20197;&#36890;&#36807;&#22312;MCTS&#39044;&#27979;&#20013;&#20351;&#29992;&#21407;&#21017;&#24615;&#30340;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26469;&#32531;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;MCTS&#20013;&#20256;&#25773;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#20272;&#35745;&#20854;&#39044;&#27979;&#30340;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;&#20256;&#25773;&#30340;&#19981;&#30830;&#23450;&#24615;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#25506;&#32034;&#31639;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#35268;&#21010;&#25506;&#32034;&#31574;&#30053;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#22522;&#20110;MCTS&#30340;&#27169;&#22411;&#22522;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;&#21253;&#25324;&#20351;&#29992;&#23398;&#20064;&#21644;&#25552;&#20379;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#25104;&#21151;&#30340;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24182;&#36827;&#34892;&#20102;&#28145;&#24230;&#25506;&#32034;&#12290;&#25105;&#20204;&#23558;&#20854;&#19982;&#22522;&#20110;&#38750;&#35268;&#21010;&#30340;&#28145;&#24230;&#25506;&#32034;&#22522;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
One of the most well-studied and highly performing planning approaches used in Model-Based Reinforcement Learning (MBRL) is Monte-Carlo Tree Search (MCTS). Key challenges of MCTS-based MBRL methods remain dedicated deep exploration and reliability in the face of the unknown, and both challenges can be alleviated through principled epistemic uncertainty estimation in the predictions of MCTS. We present two main contributions: First, we develop methodology to propagate epistemic uncertainty in MCTS, enabling agents to estimate the epistemic uncertainty in their predictions. Second, we utilize the propagated uncertainty for a novel deep exploration algorithm by explicitly planning to explore. We incorporate our approach into variations of MCTS-based MBRL approaches with learned and provided models, and empirically show deep exploration through successful epistemic uncertainty estimation achieved by our approach. We compare to a non-planning-based deep-exploration baseline, and demonstrate
&lt;/p&gt;</description></item></channel></rss>