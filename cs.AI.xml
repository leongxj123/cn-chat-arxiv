<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#20351;&#29992;Shapley&#20540;&#26041;&#27861;&#35299;&#37322;LLM&#34892;&#20026;&#65292;&#25581;&#31034;&#20102;&#25152;&#35859;&#30340;&#8220;&#20196;&#29260;&#22122;&#38899;&#8221;&#25928;&#24212;&#65292;&#25581;&#31034;&#20102;LLMs&#30340;&#20915;&#31574;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#21040;&#25552;&#31034;&#32452;&#20214;&#30340;&#24433;&#21709;</title><link>https://arxiv.org/abs/2404.01332</link><description>&lt;p&gt;
&#31561;&#31561;&#65292;&#36825;&#37117;&#26159;&#20196;&#29260;&#22122;&#38899;&#65311;&#19968;&#30452;&#23601;&#26159;&#21527;&#65306;&#21033;&#29992; Shapley &#20540;&#35299;&#37322; LLM &#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Wait, It's All Token Noise? Always Has Been: Interpreting LLM Behavior Using Shapley Value
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01332
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Shapley&#20540;&#26041;&#27861;&#35299;&#37322;LLM&#34892;&#20026;&#65292;&#25581;&#31034;&#20102;&#25152;&#35859;&#30340;&#8220;&#20196;&#29260;&#22122;&#38899;&#8221;&#25928;&#24212;&#65292;&#25581;&#31034;&#20102;LLMs&#30340;&#20915;&#31574;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#21040;&#25552;&#31034;&#32452;&#20214;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#20026;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#21644;&#35748;&#30693;&#36807;&#31243;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#28508;&#22312;&#24212;&#29992;&#21253;&#25324;&#24066;&#22330;&#30740;&#31350;&#21644;&#28040;&#36153;&#32773;&#34892;&#20026;&#20998;&#26512;&#31561;&#21508;&#20010;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLMs&#30340;&#26174;&#33879;&#24046;&#24322;&#26263;&#31034;&#20102;&#19981;&#21516;&#30340;&#22522;&#30784;&#36807;&#31243;&#22312;&#36215;&#20316;&#29992;&#65292;&#20197;&#21450;LLMs&#23545;&#25552;&#31034;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#65292;&#21033;&#29992;LLMs&#20316;&#20026;&#20154;&#31867;&#20027;&#20307;&#30340;&#26367;&#20195;&#20173;&#28982;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#20316;&#21338;&#24328;&#29702;&#35770;&#20013;Shapley&#20540;&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#37322;LLM&#34892;&#20026;&#65292;&#24182;&#37327;&#21270;&#27599;&#20010;&#25552;&#31034;&#32452;&#20214;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#30456;&#23545;&#36129;&#29486;&#12290;&#36890;&#36807;&#20004;&#20010;&#24212;&#29992;--&#19968;&#20010;&#31163;&#25955;&#36873;&#25321;&#23454;&#39564;&#21644;&#19968;&#20010;&#35748;&#30693;&#20559;&#35265;&#35843;&#26597;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Shapley&#20540;&#26041;&#27861;&#22914;&#20309;&#25581;&#31034;&#25105;&#20204;&#25152;&#35859;&#30340;&#8220;&#20196;&#29260;&#22122;&#38899;&#8221;&#25928;&#24212;&#65292;&#21363;LLM&#20915;&#31574;&#21463;&#21040;&#30340;&#24433;&#21709;&#20005;&#37325;&#20559;&#21521;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01332v1 Announce Type: cross  Abstract: The emergence of large language models (LLMs) has opened up exciting possibilities for simulating human behavior and cognitive processes, with potential applications in various domains, including marketing research and consumer behavior analysis. However, the validity of utilizing LLMs as stand-ins for human subjects remains uncertain due to glaring divergences that suggest fundamentally different underlying processes at play and the sensitivity of LLM responses to prompt variations. This paper presents a novel approach based on Shapley values from cooperative game theory to interpret LLM behavior and quantify the relative contribution of each prompt component to the model's output. Through two applications-a discrete choice experiment and an investigation of cognitive biases-we demonstrate how the Shapley value method can uncover what we term "token noise" effects, a phenomenon where LLM decisions are disproportionately influenced by 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#37322;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;GNN&#20013;&#30340;&#21518;&#38376;&#35757;&#32451;&#22270;&#65292;&#35774;&#35745;&#20102;&#19971;&#31181;&#26032;&#30340;&#24230;&#37327;&#25351;&#26631;&#20197;&#26356;&#26377;&#25928;&#22320;&#26816;&#27979;&#21518;&#38376;&#25915;&#20987;&#65292;&#24182;&#19988;&#36890;&#36807;&#33258;&#36866;&#24212;&#25915;&#20987;&#36827;&#34892;&#20102;&#26041;&#27861;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.18136</link><description>&lt;p&gt;
&#20445;&#25252;GNN&#65306;&#22522;&#20110;&#35299;&#37322;&#30340;&#21518;&#38376;&#35757;&#32451;&#22270;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Securing GNNs: Explanation-Based Identification of Backdoored Training Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18136
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#37322;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;GNN&#20013;&#30340;&#21518;&#38376;&#35757;&#32451;&#22270;&#65292;&#35774;&#35745;&#20102;&#19971;&#31181;&#26032;&#30340;&#24230;&#37327;&#25351;&#26631;&#20197;&#26356;&#26377;&#25928;&#22320;&#26816;&#27979;&#21518;&#38376;&#25915;&#20987;&#65292;&#24182;&#19988;&#36890;&#36807;&#33258;&#36866;&#24212;&#25915;&#20987;&#36827;&#34892;&#20102;&#26041;&#27861;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs)&#24050;&#32463;&#22312;&#35768;&#22810;&#39046;&#22495;&#27969;&#34892;&#36215;&#26469;&#65292;&#20294;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#36825;&#21487;&#33021;&#20250;&#25439;&#23475;&#23427;&#20204;&#30340;&#24615;&#33021;&#21644;&#36947;&#24503;&#24212;&#29992;&#12290;&#26816;&#27979;&#36825;&#20123;&#25915;&#20987;&#23545;&#20110;&#20445;&#25345;GNN&#20998;&#31867;&#20219;&#21153;&#30340;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#26377;&#25928;&#30340;&#26816;&#27979;&#25216;&#26415;&#24182;&#19981;&#22810;&#35265;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23613;&#31649;&#22270;&#32423;&#35299;&#37322;&#33021;&#22815;&#25552;&#20379;&#19968;&#20123;&#26377;&#38480;&#30340;&#35265;&#35299;&#65292;&#20294;&#23427;&#20204;&#22312;&#26816;&#27979;&#21518;&#38376;&#35302;&#21457;&#22120;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#26159;&#19981;&#19968;&#33268;&#19988;&#19981;&#23436;&#25972;&#30340;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#21462;&#24182;&#36716;&#25442;GNN&#35299;&#37322;&#26426;&#21046;&#30340;&#27425;&#35201;&#36755;&#20986;&#65292;&#35774;&#35745;&#20102;&#19971;&#31181;&#26356;&#26377;&#25928;&#22320;&#26816;&#27979;&#21518;&#38376;&#25915;&#20987;&#30340;&#26032;&#24230;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25915;&#20987;&#26469;&#20005;&#26684;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#26816;&#26597;&#20854;&#23545;&#21508;&#31181;&#25915;&#20987;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#36739;&#39640;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18136v1 Announce Type: cross  Abstract: Graph Neural Networks (GNNs) have gained popularity in numerous domains, yet they are vulnerable to backdoor attacks that can compromise their performance and ethical application. The detection of these attacks is crucial for maintaining the reliability and security of GNN classification tasks, but effective detection techniques are lacking. Following an initial investigation, we observed that while graph-level explanations can offer limited insights, their effectiveness in detecting backdoor triggers is inconsistent and incomplete. To bridge this gap, we extract and transform secondary outputs of GNN explanation mechanisms, designing seven novel metrics that more effectively detect backdoor attacks. Additionally, we develop an adaptive attack to rigorously evaluate our approach. We test our method on multiple benchmark datasets and examine its efficacy against various attack models. Our results show that our method can achieve high de
&lt;/p&gt;</description></item><item><title>LaRE^2 &#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#37325;&#26500;&#35823;&#24046;&#30340;&#26041;&#27861;&#29992;&#20110;&#26816;&#27979;&#25193;&#25955;&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#36890;&#36807;&#24341;&#20837;&#28508;&#22312;&#37325;&#26500;&#35823;&#24046;&#65288;LaRE&#65289;&#21644;&#35823;&#24046;&#24341;&#23548;&#29305;&#24449;&#32454;&#21270;&#27169;&#22359;&#65288;EGRE&#65289;&#23454;&#29616;&#20102;&#23545;&#29305;&#24449;&#30340;&#26377;&#25928;&#25552;&#21462;&#21644;&#22686;&#24378;&#65292;&#20174;&#32780;&#21306;&#20998;&#30495;&#23454;&#21644;&#29983;&#25104;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2403.17465</link><description>&lt;p&gt;
LaRE^2: &#22522;&#20110;&#28508;&#22312;&#37325;&#26500;&#35823;&#24046;&#30340;&#25193;&#25955;&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LaRE^2: Latent Reconstruction Error Based Method for Diffusion-Generated Image Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17465
&lt;/p&gt;
&lt;p&gt;
LaRE^2 &#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#37325;&#26500;&#35823;&#24046;&#30340;&#26041;&#27861;&#29992;&#20110;&#26816;&#27979;&#25193;&#25955;&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#36890;&#36807;&#24341;&#20837;&#28508;&#22312;&#37325;&#26500;&#35823;&#24046;&#65288;LaRE&#65289;&#21644;&#35823;&#24046;&#24341;&#23548;&#29305;&#24449;&#32454;&#21270;&#27169;&#22359;&#65288;EGRE&#65289;&#23454;&#29616;&#20102;&#23545;&#29305;&#24449;&#30340;&#26377;&#25928;&#25552;&#21462;&#21644;&#22686;&#24378;&#65292;&#20174;&#32780;&#21306;&#20998;&#30495;&#23454;&#21644;&#29983;&#25104;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17465v1 &#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#25193;&#25955;&#27169;&#22411;&#30340;&#21457;&#23637;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#20687;&#29983;&#25104;&#36136;&#37327;&#65292;&#20351;&#30495;&#23454;&#22270;&#20687;&#21644;&#29983;&#25104;&#22270;&#20687;&#20043;&#38388;&#30340;&#21306;&#20998;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#12290;&#23613;&#31649;&#36825;&#19968;&#36827;&#23637;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#37325;&#35201;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#28508;&#22312;&#37325;&#26500;&#35823;&#24046;&#24341;&#23548;&#29305;&#24449;&#32454;&#21270;&#26041;&#27861;&#65288;LaRE^2&#65289;&#26469;&#26816;&#27979;&#25193;&#25955;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#28508;&#22312;&#37325;&#26500;&#35823;&#24046;&#65288;LaRE&#65289;&#65292;&#20316;&#20026;&#28508;&#22312;&#31354;&#38388;&#20013;&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#30340;&#31532;&#19968;&#20010;&#22522;&#20110;&#37325;&#26500;&#35823;&#24046;&#30340;&#29305;&#24449;&#12290;LaRE&#22312;&#29305;&#24449;&#25552;&#21462;&#25928;&#29575;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#21306;&#20998;&#30495;&#20551;&#25152;&#38656;&#30340;&#20851;&#38190;&#32447;&#32034;&#12290;&#20026;&#20102;&#21033;&#29992;LaRE&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35823;&#24046;&#24341;&#23548;&#29305;&#24449;&#32454;&#21270;&#27169;&#22359;&#65288;EGRE&#65289;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;LaRE&#24341;&#23548;&#30340;&#26041;&#24335;&#32454;&#21270;&#22270;&#20687;&#29305;&#24449;&#65292;&#20197;&#22686;&#24378;&#29305;&#24449;&#30340;&#21306;&#20998;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17465v1 Announce Type: cross  Abstract: The evolution of Diffusion Models has dramatically improved image generation quality, making it increasingly difficult to differentiate between real and generated images. This development, while impressive, also raises significant privacy and security concerns. In response to this, we propose a novel Latent REconstruction error guided feature REfinement method (LaRE^2) for detecting the diffusion-generated images. We come up with the Latent Reconstruction Error (LaRE), the first reconstruction-error based feature in the latent space for generated image detection. LaRE surpasses existing methods in terms of feature extraction efficiency while preserving crucial cues required to differentiate between the real and the fake. To exploit LaRE, we propose an Error-Guided feature REfinement module (EGRE), which can refine the image feature guided by LaRE to enhance the discriminativeness of the feature. Our EGRE utilizes an align-then-refine m
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24314;&#31435;Gini&#19981;&#32431;&#24230;&#30340;&#24179;&#28369;&#25935;&#24863;&#24230;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25552;&#20986;DP&#36138;&#23146;&#35268;&#21017;&#21015;&#34920;&#31639;&#27861;&#65292;&#26412;&#25991;&#25913;&#21892;&#20102;&#24046;&#24322;&#20445;&#25252;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.13848</link><description>&lt;p&gt;
&#29992;&#20110;&#23398;&#20064;&#24046;&#24322;&#20445;&#25252;&#20294;&#20934;&#30830;&#35268;&#21017;&#21015;&#34920;&#30340;&#24179;&#28369;&#25935;&#24863;&#24230;
&lt;/p&gt;
&lt;p&gt;
Smooth Sensitivity for Learning Differentially-Private yet Accurate Rule Lists
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13848
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24314;&#31435;Gini&#19981;&#32431;&#24230;&#30340;&#24179;&#28369;&#25935;&#24863;&#24230;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25552;&#20986;DP&#36138;&#23146;&#35268;&#21017;&#21015;&#34920;&#31639;&#27861;&#65292;&#26412;&#25991;&#25913;&#21892;&#20102;&#24046;&#24322;&#20445;&#25252;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#24322;&#20445;&#25252;&#65288;DP&#65289;&#26426;&#21046;&#21487;&#20197;&#23884;&#20837;&#21040;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#35774;&#35745;&#20013;&#65292;&#20197;&#20445;&#25252;&#25152;&#24471;&#27169;&#22411;&#20813;&#21463;&#38544;&#31169;&#27844;&#38706;&#30340;&#24433;&#21709;&#65292;&#23613;&#31649;&#36825;&#36890;&#24120;&#20276;&#38543;&#30528;&#26126;&#26174;&#30340;&#20934;&#30830;&#24615;&#25439;&#22833;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24314;&#31435;Gini&#19981;&#32431;&#24230;&#30340;&#24179;&#28369;&#25935;&#24863;&#24230;&#24182;&#21033;&#29992;&#36825;&#19968;&#29305;&#24615;&#26469;&#25552;&#20986;&#19968;&#20010;DP&#36138;&#23146;&#35268;&#21017;&#21015;&#34920;&#31639;&#27861;&#65292;&#20197;&#25913;&#21892;&#36825;&#31181;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38598;&#25104;&#24179;&#28369;&#25935;&#24863;&#24230;&#30340;DP&#35268;&#21017;&#21015;&#34920;&#27169;&#22411;&#20855;&#26377;&#27604;&#20351;&#29992;&#20840;&#23616;&#25935;&#24863;&#24230;&#30340;&#20854;&#20182;DP&#26694;&#26550;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13848v1 Announce Type: cross  Abstract: Differentially-private (DP) mechanisms can be embedded into the design of a machine learningalgorithm to protect the resulting model against privacy leakage, although this often comes with asignificant loss of accuracy. In this paper, we aim at improving this trade-off for rule lists modelsby establishing the smooth sensitivity of the Gini impurity and leveraging it to propose a DP greedyrule list algorithm. In particular, our theoretical analysis and experimental results demonstrate thatthe DP rule lists models integrating smooth sensitivity have higher accuracy that those using otherDP frameworks based on global sensitivity.
&lt;/p&gt;</description></item><item><title>&#23558;&#24694;&#24847;&#25552;&#31034;&#20998;&#35299;&#20026;&#29420;&#31435;&#30340;&#23376;&#25552;&#31034;&#20351;&#24471;LLM&#36234;&#29425;&#25915;&#20987;&#26356;&#38590;&#34987;&#26816;&#27979;</title><link>https://arxiv.org/abs/2402.16914</link><description>&lt;p&gt;
DrAttack: &#25552;&#31034;&#20998;&#35299;&#21644;&#37325;&#26500;&#20351;&#24378;&#22823;&#30340;LLM&#36234;&#29425;&#32773;
&lt;/p&gt;
&lt;p&gt;
DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16914
&lt;/p&gt;
&lt;p&gt;
&#23558;&#24694;&#24847;&#25552;&#31034;&#20998;&#35299;&#20026;&#29420;&#31435;&#30340;&#23376;&#25552;&#31034;&#20351;&#24471;LLM&#36234;&#29425;&#25915;&#20987;&#26356;&#38590;&#34987;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#23558;&#24694;&#24847;&#25552;&#31034;&#20998;&#35299;&#20026;&#29420;&#31435;&#30340;&#23376;&#25552;&#31034;&#33021;&#22815;&#26377;&#25928;&#27169;&#31946;&#20854;&#28508;&#22312;&#30340;&#24694;&#24847;&#24847;&#22270;&#65292;&#20351;&#20043;&#20197;&#29255;&#27573;&#21270;&#12289;&#19981;&#26131;&#26816;&#27979;&#30340;&#24418;&#24335;&#21576;&#29616;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36825;&#20123;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#36234;&#29425;&#25915;&#20987;&#30340;&#33258;&#21160;&#25552;&#31034;&#20998;&#35299;&#21644;&#37325;&#26500;&#26694;&#26550;&#65288;DrAttack&#65289;&#12290;DrAttack&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;(a) &#23558;&#21407;&#22987;&#25552;&#31034;&#36827;&#34892;&#8220;&#20998;&#35299;&#8221;&#20026;&#23376;&#25552;&#31034;&#65292;(b) &#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#35821;&#20041;&#19978;&#30456;&#20284;&#20294;&#38544;&#21547;&#30340;&#8220;&#37325;&#26500;&#8221;&#36825;&#20123;&#23376;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16914v1 Announce Type: cross  Abstract: The safety alignment of Large Language Models (LLMs) is vulnerable to both manual and automated jailbreak attacks, which adversarially trigger LLMs to output harmful content. However, current methods for jailbreaking LLMs, which nest entire harmful prompts, are not effective at concealing malicious intent and can be easily identified and rejected by well-aligned LLMs. This paper discovers that decomposing a malicious prompt into separated sub-prompts can effectively obscure its underlying malicious intent by presenting it in a fragmented, less detectable form, thereby addressing these limitations. We introduce an automatic prompt \textbf{D}ecomposition and \textbf{R}econstruction framework for jailbreak \textbf{Attack} (DrAttack). DrAttack includes three key components: (a) `Decomposition' of the original prompt into sub-prompts, (b) `Reconstruction' of these sub-prompts implicitly by in-context learning with semantically similar but h
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#22312;&#21160;&#24577;&#35268;&#21010;&#39046;&#22495;&#20013;&#27169;&#25311;&#24037;&#20855;&#20351;&#29992;&#30340;&#30446;&#26631;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#20027;&#21160;&#25512;&#26029;&#20013;&#30340;&#21160;&#24577;&#35268;&#21010;&#65292;&#35813;&#39046;&#22495;&#32771;&#34385;&#21040;&#29983;&#29289;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#30340;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;</title><link>https://arxiv.org/abs/2402.11658</link><description>&lt;p&gt;
&#20998;&#23618;&#20027;&#21160;&#25512;&#26029;&#20013;&#30340;&#21160;&#24577;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Dynamic planning in hierarchical active inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11658
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#22312;&#21160;&#24577;&#35268;&#21010;&#39046;&#22495;&#20013;&#27169;&#25311;&#24037;&#20855;&#20351;&#29992;&#30340;&#30446;&#26631;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#20027;&#21160;&#25512;&#26029;&#20013;&#30340;&#21160;&#24577;&#35268;&#21010;&#65292;&#35813;&#39046;&#22495;&#32771;&#34385;&#21040;&#29983;&#29289;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#30340;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21160;&#24577;&#35268;&#21010;&#65292;&#25105;&#20204;&#25351;&#30340;&#26159;&#20154;&#31867;&#22823;&#33041;&#25512;&#26029;&#21644;&#26045;&#21152;&#19982;&#35748;&#30693;&#20915;&#31574;&#30456;&#20851;&#30340;&#36816;&#21160;&#36712;&#36857;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#19968;&#20010;&#33539;&#24335;&#65292;&#20027;&#21160;&#25512;&#26029;&#65292;&#20026;&#29983;&#29289;&#26377;&#26426;&#20307;&#36866;&#24212;&#24102;&#26469;&#20102;&#22522;&#26412;&#35265;&#35299;&#65292;&#19981;&#26029;&#21162;&#21147;&#26368;&#23567;&#21270;&#39044;&#27979;&#35823;&#24046;&#20197;&#23558;&#33258;&#24049;&#38480;&#21046;&#22312;&#19982;&#29983;&#21629;&#20860;&#23481;&#30340;&#29366;&#24577;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#37324;&#65292;&#35768;&#22810;&#30740;&#31350;&#34920;&#26126;&#20154;&#31867;&#21644;&#21160;&#29289;&#34892;&#20026;&#21487;&#20197;&#35299;&#37322;&#20026;&#20027;&#21160;&#25512;&#26029;&#36807;&#31243;&#65292;&#26080;&#35770;&#26159;&#20316;&#20026;&#31163;&#25955;&#20915;&#31574;&#36824;&#26159;&#36830;&#32493;&#36816;&#21160;&#25511;&#21046;&#65292;&#37117;&#28608;&#21457;&#20102;&#26426;&#22120;&#20154;&#25216;&#26415;&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#32570;&#20047;&#23545;&#22914;&#20309;&#26377;&#25928;&#22320;&#22312;&#21464;&#21270;&#29615;&#22659;&#20013;&#35268;&#21010;&#34892;&#21160;&#30340;&#20840;&#38754;&#23637;&#26395;&#12290;&#25105;&#20204;&#35774;&#23450;&#20102;&#23545;&#24037;&#20855;&#20351;&#29992;&#36827;&#34892;&#24314;&#27169;&#30340;&#30446;&#26631;&#65292;&#28145;&#20837;&#30740;&#31350;&#20102;&#20027;&#21160;&#25512;&#26029;&#20013;&#30340;&#21160;&#24577;&#35268;&#21010;&#20027;&#39064;&#65292;&#29282;&#35760;&#20004;&#20010;&#29983;&#29289;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#30340;&#20851;&#38190;&#26041;&#38754;&#65306;&#29702;&#35299;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11658v1 Announce Type: new  Abstract: By dynamic planning, we refer to the ability of the human brain to infer and impose motor trajectories related to cognitive decisions. A recent paradigm, active inference, brings fundamental insights into the adaptation of biological organisms, constantly striving to minimize prediction errors to restrict themselves to life-compatible states. Over the past years, many studies have shown how human and animal behavior could be explained in terms of an active inferential process -- either as discrete decision-making or continuous motor control -- inspiring innovative solutions in robotics and artificial intelligence. Still, the literature lacks a comprehensive outlook on how to effectively plan actions in changing environments. Setting ourselves the goal of modeling tool use, we delve into the topic of dynamic planning in active inference, keeping in mind two crucial aspects of biological goal-directed behavior: the capacity to understand a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20851;&#27880;&#33258;&#23398;&#20064;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26680;&#24515;&#38382;&#39064;&#65306;&#22914;&#20309;&#23398;&#20064;&#26410;&#30693;&#30693;&#35782;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#25105;&#35780;&#20272;&#21644;&#35782;&#21035;&#26410;&#30693;&#28857;&#26469;&#29420;&#31435;&#23398;&#20064;&#20197;&#21069;&#26410;&#30693;&#30340;&#30693;&#35782;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#23545;&#20110;&#20943;&#23569;&#24187;&#35273;&#35780;&#20998;&#12289;&#23454;&#29616;&#39640;&#25928;LLM&#26356;&#26032;&#20197;&#21450;&#30693;&#35782;&#20132;&#27969;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.09147</link><description>&lt;p&gt;
&#26410;&#30693;&#20043;&#20013;&#65306;&#33258;&#23398;&#20064;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Into the Unknown: Self-Learning Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#33258;&#23398;&#20064;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26680;&#24515;&#38382;&#39064;&#65306;&#22914;&#20309;&#23398;&#20064;&#26410;&#30693;&#30693;&#35782;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#25105;&#35780;&#20272;&#21644;&#35782;&#21035;&#26410;&#30693;&#28857;&#26469;&#29420;&#31435;&#23398;&#20064;&#20197;&#21069;&#26410;&#30693;&#30340;&#30693;&#35782;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#23545;&#20110;&#20943;&#23569;&#24187;&#35273;&#35780;&#20998;&#12289;&#23454;&#29616;&#39640;&#25928;LLM&#26356;&#26032;&#20197;&#21450;&#30693;&#35782;&#20132;&#27969;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#33258;&#23398;&#20064;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20027;&#35201;&#38382;&#39064;&#65306;&#21363;&#22914;&#20309;&#23398;&#20064;&#33258;&#24049;&#19981;&#30693;&#36947;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23398;&#20064;LLM&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#33258;&#24049;&#30340;&#24187;&#35273;&#36827;&#34892;&#33258;&#25105;&#35780;&#20272;&#65292;&#20351;LLM&#33021;&#22815;&#29420;&#31435;&#22320;&#23398;&#20064;&#20197;&#21069;&#26410;&#30693;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#20351;&#29992;&#24187;&#35273;&#35780;&#20998;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;&#8220;&#26410;&#30693;&#28857;&#8221;&#30340;&#26032;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22806;&#37096;&#21644;&#19977;&#31181;&#20869;&#37096;&#26041;&#27861;&#26469;&#33258;&#21160;&#35782;&#21035;&#26410;&#30693;&#28857;&#12290;&#36825;&#26377;&#21161;&#20110;&#21019;&#24314;&#19968;&#20010;&#33258;&#23398;&#20064;&#24490;&#29615;&#65292;&#19987;&#27880;&#20110;&#26410;&#30693;&#28857;&#20013;&#30340;&#30693;&#35782;&#24046;&#36317;&#65292;&#20174;&#32780;&#20943;&#23569;&#24187;&#35273;&#35780;&#20998;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#29992;&#20110;&#35780;&#20272;LLM&#33258;&#23398;&#20064;&#33021;&#21147;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#24494;&#35843;&#25110;&#23545;&#40784;&#30340;7B-Mistral&#27169;&#22411;&#22312;&#33258;&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#30340;&#33258;&#23398;&#20064;&#27010;&#24565;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;LLM&#26356;&#26032;&#65292;&#24182;&#20026;&#30693;&#35782;&#20132;&#27969;&#24320;&#36767;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;&#23427;&#36824;&#21487;&#33021;&#22686;&#21152;&#20844;&#20247;&#30340;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09147v1 Announce Type: new Abstract: We address the main problem of self-learning LLM: the question of what to learn. We propose a self-learning LLM framework that enables an LLM to independently learn previously unknown knowledge through self-assessment of their own hallucinations. Using the hallucination score, we introduce a new concept of Points in The Unknown (PiUs), along with one extrinsic and three intrinsic methods for automatic PiUs identification. It facilitates the creation of a self-learning loop that focuses exclusively on the knowledge gap in Points in The Unknown, resulting in a reduced hallucination score. We also developed evaluation metrics for gauging an LLM's self-learning capability. Our experiments revealed that 7B-Mistral models that have been finetuned or aligned are capable of self-learning considerably well. Our self-learning concept allows more efficient LLM updates and opens new perspectives for knowledge exchange. It may also increase public tru
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#21644;&#35782;&#21035;&#20234;&#26031;&#20848;&#25945;&#20167;&#24680;&#35328;&#35770;&#65292;&#27169;&#22411;&#22312;&#20445;&#25345;&#20986;&#33394;&#24615;&#33021;&#30340;&#21516;&#26102;&#33021;&#22815;&#35299;&#37322;&#30456;&#20851;&#24615;&#21644;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2311.04916</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#20234;&#26031;&#20848;&#25945;&#20167;&#24680;&#35328;&#35770;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Explainable Identification of Hate Speech towards Islam using Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04916
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#21644;&#35782;&#21035;&#20234;&#26031;&#20848;&#25945;&#20167;&#24680;&#35328;&#35770;&#65292;&#27169;&#22411;&#22312;&#20445;&#25345;&#20986;&#33394;&#24615;&#33021;&#30340;&#21516;&#26102;&#33021;&#22815;&#35299;&#37322;&#30456;&#20851;&#24615;&#21644;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20234;&#26031;&#20848;&#25945;&#20167;&#24680;&#35328;&#35770;&#22312;&#22312;&#32447;&#31038;&#20132;&#20114;&#21160;&#24179;&#21488;&#19978;&#26159;&#19968;&#20010;&#26222;&#36941;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#35782;&#21035;&#21644;&#28040;&#38500;&#36825;&#31181;&#20167;&#24680;&#26159;&#36808;&#21521;&#21644;&#35856;&#19982;&#21644;&#24179;&#26410;&#26469;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#35782;&#21035;&#21644;&#35299;&#37322;&#38024;&#23545;&#20234;&#26031;&#20848;&#25945;&#30340;&#20167;&#24680;&#35328;&#35770;&#12290;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21457;&#29616;&#12289;&#25552;&#21462;&#24182;&#21033;&#29992;&#19981;&#21516;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#30340;&#20869;&#22312;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22987;&#32456;&#33021;&#22815;&#22312;&#20445;&#25345;&#20986;&#33394;&#24615;&#33021;&#30340;&#21516;&#26102;&#25552;&#20379;&#23545;&#28508;&#22312;&#30456;&#20851;&#24615;&#21644;&#22240;&#26524;&#20851;&#31995;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04916v2 Announce Type: cross  Abstract: Islamophobic language is a prevalent challenge on online social interaction platforms. Identifying and eliminating such hatred is a crucial step towards a future of harmony and peace. This study presents a novel paradigm for identifying and explaining hate speech towards Islam using graph neural networks. Utilizing the intrinsic ability of graph neural networks to find, extract, and use relationships across disparate data points, our model consistently achieves outstanding performance while offering explanations for the underlying correlations and causation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;Agent&#21160;&#24577;&#20851;&#31995;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#25512;&#26029;&#20851;&#31995;&#32467;&#26500;&#30340;&#28436;&#21270;&#65292;&#26469;&#23454;&#29616;&#22312;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#26041;&#27861;&#21253;&#25324;&#25512;&#26029;&#36229;&#36793;&#32536;&#20197;&#23454;&#29616;&#32676;&#20307;&#25512;&#29702;&#21644;&#36712;&#36857;&#39044;&#27979;&#22120;&#29983;&#25104;&#26410;&#26469;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2401.12275</link><description>&lt;p&gt;
&#22810;Agent&#21160;&#24577;&#20851;&#31995;&#25512;&#29702;&#29992;&#20110;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Dynamic Relational Reasoning for Social Robot Navigation. (arXiv:2401.12275v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;Agent&#21160;&#24577;&#20851;&#31995;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#25512;&#26029;&#20851;&#31995;&#32467;&#26500;&#30340;&#28436;&#21270;&#65292;&#26469;&#23454;&#29616;&#22312;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#26041;&#27861;&#21253;&#25324;&#25512;&#26029;&#36229;&#36793;&#32536;&#20197;&#23454;&#29616;&#32676;&#20307;&#25512;&#29702;&#21644;&#36712;&#36857;&#39044;&#27979;&#22120;&#29983;&#25104;&#26410;&#26469;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#22312;&#26085;&#24120;&#29983;&#27963;&#30340;&#21508;&#31181;&#24773;&#26223;&#19979;&#21487;&#20197;&#25552;&#20379;&#24110;&#21161;&#65292;&#20294;&#38656;&#35201;&#23433;&#20840;&#30340;&#20154;&#26426;&#20132;&#20114;&#21644;&#39640;&#25928;&#30340;&#36712;&#36857;&#35268;&#21010;&#12290;&#22312;&#22810;Agent&#20132;&#20114;&#31995;&#32479;&#20013;&#65292;&#24314;&#27169;&#25104;&#23545;&#30340;&#20851;&#31995;&#24050;&#32463;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#26159;&#25429;&#25417;&#26356;&#22823;&#35268;&#27169;&#30340;&#32676;&#20307;&#27963;&#21160;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#20851;&#31995;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#25512;&#26029;&#27491;&#22312;&#28436;&#21464;&#30340;&#20851;&#31995;&#32467;&#26500;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22810;Agent&#36712;&#36857;&#39044;&#27979;&#21644;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#38500;&#20102;&#33410;&#28857;&#23545;&#20043;&#38388;&#30340;&#36793;&#32536;&#65288;&#21363;Agent&#65289;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#25512;&#26029;&#36229;&#36793;&#32536;&#30340;&#26041;&#27861;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#36830;&#25509;&#22810;&#20010;&#33410;&#28857;&#65292;&#20197;&#20415;&#36827;&#34892;&#32676;&#20307;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25512;&#26029;&#21160;&#24577;&#28436;&#21270;&#30340;&#20851;&#31995;&#22270;&#21644;&#36229;&#22270;&#65292;&#20197;&#25429;&#25417;&#20851;&#31995;&#30340;&#28436;&#21270;&#65292;&#36712;&#36857;&#39044;&#27979;&#22120;&#21033;&#29992;&#36825;&#20123;&#22270;&#26469;&#29983;&#25104;&#26410;&#26469;&#29366;&#24577;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#38160;&#24230;&#21644;&#36923;&#36753;&#31232;&#30095;&#24615;&#36827;&#34892;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social robot navigation can be helpful in various contexts of daily life but requires safe human-robot interactions and efficient trajectory planning. While modeling pairwise relations has been widely studied in multi-agent interacting systems, the ability to capture larger-scale group-wise activities is limited. In this paper, we propose a systematic relational reasoning approach with explicit inference of the underlying dynamically evolving relational structures, and we demonstrate its effectiveness for multi-agent trajectory prediction and social robot navigation. In addition to the edges between pairs of nodes (i.e., agents), we propose to infer hyperedges that adaptively connect multiple nodes to enable group-wise reasoning in an unsupervised manner. Our approach infers dynamically evolving relation graphs and hypergraphs to capture the evolution of relations, which the trajectory predictor employs to generate future states. Meanwhile, we propose to regularize the sharpness and sp
&lt;/p&gt;</description></item><item><title>CFASL&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#32544;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#23558;&#23545;&#31216;&#24615;&#23398;&#20064;&#19982;VAE&#38598;&#25104;&#65292;&#26080;&#38656;&#20219;&#20309;&#25968;&#25454;&#38598;&#22240;&#23376;&#20449;&#24687;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20855;&#26377;&#19977;&#20010;&#26032;&#29305;&#24449;&#65306;&#23545;&#40784;&#28508;&#22312;&#21521;&#37327;&#32500;&#24230;&#21040;&#21487;&#23398;&#20064;&#23545;&#31216;&#20195;&#30721;&#31807;&#20013;&#30340;&#23545;&#31216;&#24615;&#65292;&#23398;&#20064;&#22797;&#21512;&#23545;&#31216;&#24615;&#26469;&#34920;&#36798;&#26410;&#30693;&#22240;&#32032;&#30340;&#21464;&#21270;&#65292;&#20197;&#21450;&#24341;&#20837;&#32676;&#31561;&#21464;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#26469;&#35757;&#32451;VAE&#12290;</title><link>http://arxiv.org/abs/2401.08897</link><description>&lt;p&gt;
CFASL&#65306;&#29992;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20013;&#30340;&#35299;&#32544;&#23398;&#20064;&#30340;&#22797;&#21512;&#22240;&#23376;&#23545;&#40784;&#23545;&#31216;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational AutoEncoder. (arXiv:2401.08897v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08897
&lt;/p&gt;
&lt;p&gt;
CFASL&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#32544;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#23558;&#23545;&#31216;&#24615;&#23398;&#20064;&#19982;VAE&#38598;&#25104;&#65292;&#26080;&#38656;&#20219;&#20309;&#25968;&#25454;&#38598;&#22240;&#23376;&#20449;&#24687;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20855;&#26377;&#19977;&#20010;&#26032;&#29305;&#24449;&#65306;&#23545;&#40784;&#28508;&#22312;&#21521;&#37327;&#32500;&#24230;&#21040;&#21487;&#23398;&#20064;&#23545;&#31216;&#20195;&#30721;&#31807;&#20013;&#30340;&#23545;&#31216;&#24615;&#65292;&#23398;&#20064;&#22797;&#21512;&#23545;&#31216;&#24615;&#26469;&#34920;&#36798;&#26410;&#30693;&#22240;&#32032;&#30340;&#21464;&#21270;&#65292;&#20197;&#21450;&#24341;&#20837;&#32676;&#31561;&#21464;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#26469;&#35757;&#32451;VAE&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36755;&#20837;&#21644;&#28508;&#22312;&#21521;&#37327;&#30340;&#23545;&#31216;&#24615;&#20026;VAE&#20013;&#30340;&#35299;&#32544;&#23398;&#20064;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#21482;&#26377;&#23569;&#25968;&#20960;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#29978;&#33267;&#36825;&#20123;&#26041;&#27861;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#20063;&#38656;&#35201;&#24050;&#30693;&#30340;&#22240;&#23376;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;Composite Factor-Aligned Symmetry Learning (CFASL)&#65292;&#23558;&#20854;&#38598;&#25104;&#21040;VAE&#20013;&#65292;&#29992;&#20110;&#23398;&#20064;&#22522;&#20110;&#23545;&#31216;&#24615;&#30340;&#35299;&#32544;&#65292;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#19981;&#38656;&#35201;&#20219;&#20309;&#25968;&#25454;&#38598;&#22240;&#23376;&#20449;&#24687;&#30340;&#30693;&#35782;&#12290;CFASL&#21253;&#25324;&#19977;&#20010;&#29992;&#20110;&#23398;&#20064;&#22522;&#20110;&#23545;&#31216;&#24615;&#30340;&#35299;&#32544;&#30340;&#26032;&#29305;&#24449;&#65306;1)&#27880;&#20837;&#24402;&#32435;&#20559;&#32622;&#65292;&#23558;&#28508;&#22312;&#21521;&#37327;&#32500;&#24230;&#23545;&#40784;&#21040;&#26126;&#30830;&#21487;&#23398;&#20064;&#30340;&#23545;&#31216;&#20195;&#30721;&#31807;&#20013;&#30340;&#22240;&#23376;&#23545;&#40784;&#23545;&#31216;&#24615;&#65307;2)&#23398;&#20064;&#19968;&#20010;&#22797;&#21512;&#23545;&#31216;&#24615;&#65292;&#36890;&#36807;&#23398;&#20064;&#20195;&#30721;&#31807;&#20013;&#30340;&#22240;&#23376;&#23545;&#40784;&#23545;&#31216;&#24615;&#65292;&#26469;&#34920;&#36798;&#20004;&#20010;&#38543;&#26426;&#26679;&#26412;&#20043;&#38388;&#30340;&#26410;&#30693;&#22240;&#32032;&#30340;&#21464;&#21270;&#65307;3)&#22312;&#35757;&#32451;VAE&#26102;&#65292;&#24341;&#20837;&#20855;&#26377;&#32676;&#31561;&#21464;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#30340;&#20004;&#20010;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symmetries of input and latent vectors have provided valuable insights for disentanglement learning in VAEs.However, only a few works were proposed as an unsupervised method, and even these works require known factor information in training data. We propose a novel method, Composite Factor-Aligned Symmetry Learning (CFASL), which is integrated into VAEs for learning symmetry-based disentanglement in unsupervised learning without any knowledge of the dataset factor information.CFASL incorporates three novel features for learning symmetry-based disentanglement: 1) Injecting inductive bias to align latent vector dimensions to factor-aligned symmetries within an explicit learnable symmetry codebook 2) Learning a composite symmetry to express unknown factors change between two random samples by learning factor-aligned symmetries within the codebook 3) Inducing group equivariant encoder and decoder in training VAEs with the two conditions. In addition, we propose an extended evaluation metri
&lt;/p&gt;</description></item><item><title>&#22320;&#29699;&#31185;&#23398;&#22522;&#30784;&#27169;&#22411;&#36890;&#36807;&#25972;&#21512;&#22823;&#37327;&#36328;&#23398;&#31185;&#25968;&#25454;&#26469;&#27169;&#25311;&#21644;&#29702;&#35299;&#22320;&#29699;&#31995;&#32479;&#21160;&#24577;&#65292;&#20855;&#26377;&#24191;&#38420;&#30340;&#24212;&#29992;&#21069;&#26223;&#21644;&#21019;&#26032;&#28508;&#21147;&#65292;&#20294;&#20173;&#38754;&#20020;&#39564;&#35777;&#21644;&#26680;&#23454;&#12289;&#35268;&#27169;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#30693;&#35782;&#34920;&#31034;&#21644;&#31038;&#20250;&#20559;&#24046;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.06799</link><description>&lt;p&gt;
&#24403;&#22320;&#29699;&#31185;&#23398;&#36935;&#35265;&#22522;&#30784;&#27169;&#22411;&#65306;&#36208;&#21521;&#36890;&#29992;&#22320;&#29699;&#31185;&#23398;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
When Geoscience Meets Foundation Models: Towards General Geoscience Artificial Intelligence System. (arXiv:2309.06799v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06799
&lt;/p&gt;
&lt;p&gt;
&#22320;&#29699;&#31185;&#23398;&#22522;&#30784;&#27169;&#22411;&#36890;&#36807;&#25972;&#21512;&#22823;&#37327;&#36328;&#23398;&#31185;&#25968;&#25454;&#26469;&#27169;&#25311;&#21644;&#29702;&#35299;&#22320;&#29699;&#31995;&#32479;&#21160;&#24577;&#65292;&#20855;&#26377;&#24191;&#38420;&#30340;&#24212;&#29992;&#21069;&#26223;&#21644;&#21019;&#26032;&#28508;&#21147;&#65292;&#20294;&#20173;&#38754;&#20020;&#39564;&#35777;&#21644;&#26680;&#23454;&#12289;&#35268;&#27169;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#30693;&#35782;&#34920;&#31034;&#21644;&#31038;&#20250;&#20559;&#24046;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29699;&#31185;&#23398;&#22522;&#30784;&#27169;&#22411;&#36890;&#36807;&#25972;&#21512;&#22823;&#37327;&#36328;&#23398;&#31185;&#25968;&#25454;&#26469;&#27169;&#25311;&#21644;&#29702;&#35299;&#22320;&#29699;&#31995;&#32479;&#21160;&#24577;&#65292;&#20195;&#34920;&#20102;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#30340;&#19968;&#31181;&#38761;&#21629;&#24615;&#26041;&#27861;&#12290;&#20316;&#20026;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#30340;&#20154;&#24037;&#26234;&#33021;&#33539;&#24335;&#65292;&#23427;&#20204;&#20174;&#30334;&#19975;&#20159;&#23383;&#33410;&#30340;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#25581;&#31034;&#20986;&#27934;&#23519;&#21147;&#12290;&#28789;&#27963;&#30340;&#20219;&#21153;&#35268;&#33539;&#12289;&#22810;&#26679;&#21270;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#20197;&#21450;&#22810;&#27169;&#24577;&#30340;&#30693;&#35782;&#34920;&#31034;&#20351;&#24471;&#32508;&#21512;&#20998;&#26512;&#25104;&#20026;&#21487;&#33021;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#22320;&#29699;&#31185;&#23398;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#25512;&#24191;&#24615;&#20801;&#35768;&#35299;&#20915;&#19982;&#22320;&#29699;&#31995;&#32479;&#30456;&#20114;&#20316;&#29992;&#30456;&#20851;&#30340;&#22810;&#31181;&#39044;&#27979;&#12289;&#27169;&#25311;&#21644;&#20915;&#31574;&#25361;&#25112;&#12290;&#39046;&#22495;&#19987;&#23478;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#20043;&#38388;&#30340;&#21512;&#20316;&#25512;&#21160;&#20102;&#36825;&#20123;&#23453;&#36149;&#24037;&#20855;&#22312;&#29702;&#35299;&#25105;&#20204;&#22320;&#29699;&#30340;&#36807;&#21435;&#12289;&#29616;&#22312;&#21644;&#26410;&#26469;&#26041;&#38754;&#30340;&#21019;&#26032;&#12290;&#28982;&#32780;&#65292;&#39564;&#35777;&#21644;&#26680;&#23454;&#12289;&#35268;&#27169;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#30693;&#35782;&#34920;&#31034;&#21644;&#31038;&#20250;&#20559;&#24046;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#12290;&#23637;&#26395;&#26410;&#26469;&#65292;&#22686;&#24378;&#39564;&#35777;&#21644;&#26680;&#23454;&#12289;&#35268;&#27169;&#24615;&#12289;&#35299;&#37322;&#24615;&#12289;&#30693;&#35782;&#34920;&#31034;&#21644;&#31038;&#20250;&#20559;&#24046;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#23558;&#26377;&#21161;&#20110;&#25512;&#21160;&#22320;&#29699;&#31185;&#23398;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geoscience foundation models represent a revolutionary approach in the field of Earth sciences by integrating massive cross-disciplinary data to simulate and understand the Earth systems dynamics. As a data-centric artificial intelligence (AI) paradigm, they uncover insights from petabytes of structured and unstructured data. Flexible task specification, diverse inputs and outputs and multi-modal knowledge representation enable comprehensive analysis infeasible with individual data sources. Critically, the scalability and generalizability of geoscience models allow for tackling diverse prediction, simulation, and decision challenges related to Earth systems interactions. Collaboration between domain experts and computer scientists leads to innovations in these invaluable tools for understanding the past, present, and future of our planet. However, challenges remain in validation and verification, scale, interpretability, knowledge representation, and social bias. Going forward, enhanci
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#26234;&#33021;&#33258;&#21160;&#32553;&#25918;&#26080;&#26381;&#21153;&#22120;&#20989;&#25968;&#30340;&#28145;&#24230;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#38024;&#23545;&#27874;&#21160;&#30340;&#24037;&#20316;&#36127;&#36733;&#21644;&#20005;&#26684;&#30340;&#24615;&#33021;&#32422;&#26463;&#65292;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#36866;&#24212;&#24615;&#31574;&#30053;&#26469;&#23454;&#29616;&#26368;&#22823;&#21270;&#26399;&#26395;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2308.05937</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#26234;&#33021;&#33258;&#21160;&#32553;&#25918;&#26080;&#26381;&#21153;&#22120;&#20989;&#25968;&#30340;&#28145;&#24230;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Deep Recurrent-Reinforcement Learning Method for Intelligent AutoScaling of Serverless Functions. (arXiv:2308.05937v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05937
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#26234;&#33021;&#33258;&#21160;&#32553;&#25918;&#26080;&#26381;&#21153;&#22120;&#20989;&#25968;&#30340;&#28145;&#24230;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#38024;&#23545;&#27874;&#21160;&#30340;&#24037;&#20316;&#36127;&#36733;&#21644;&#20005;&#26684;&#30340;&#24615;&#33021;&#32422;&#26463;&#65292;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#36866;&#24212;&#24615;&#31574;&#30053;&#26469;&#23454;&#29616;&#26368;&#22823;&#21270;&#26399;&#26395;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20989;&#25968;&#21363;&#26381;&#21153;&#65288;FaaS&#65289;&#24341;&#20837;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#20989;&#25968;&#30340;&#20113;&#25191;&#34892;&#27169;&#22411;&#65292;&#22312;&#29289;&#32852;&#32593;&#36793;&#32536;&#25968;&#25454;&#22788;&#29702;&#21644;&#24322;&#24120;&#26816;&#27979;&#31561;&#24212;&#29992;&#20013;&#20855;&#26377;&#30456;&#20851;&#24615;&#12290;&#34429;&#28982;&#20113;&#26381;&#21153;&#25552;&#20379;&#21830;&#25552;&#20379;&#20102;&#20960;&#20046;&#26080;&#38480;&#30340;&#20989;&#25968;&#24377;&#24615;&#65292;&#20294;&#36825;&#20123;&#24212;&#29992;&#32463;&#24120;&#36935;&#21040;&#27874;&#21160;&#30340;&#24037;&#20316;&#36127;&#36733;&#21644;&#26356;&#20005;&#26684;&#30340;&#24615;&#33021;&#32422;&#26463;&#12290;&#20856;&#22411;&#30340;&#20113;&#26381;&#21153;&#25552;&#20379;&#21830;&#31574;&#30053;&#26159;&#26681;&#25454;&#22522;&#20110;&#30417;&#25511;&#30340;&#38408;&#20540;&#65288;&#22914;CPU&#25110;&#20869;&#23384;&#65289;&#26469;&#32463;&#39564;&#24615;&#22320;&#30830;&#23450;&#21644;&#35843;&#25972;&#25152;&#38656;&#30340;&#20989;&#25968;&#23454;&#20363;&#20197;&#36866;&#24212;&#38656;&#27714;&#21644;&#24615;&#33021;&#65292;&#21363;"&#33258;&#21160;&#32553;&#25918;"&#12290;&#28982;&#32780;&#65292;&#38408;&#20540;&#37197;&#32622;&#35201;&#20040;&#38656;&#35201;&#19987;&#23478;&#30693;&#35782;&#65292;&#35201;&#20040;&#38656;&#35201;&#21382;&#21490;&#25968;&#25454;&#25110;&#23545;&#29615;&#22659;&#30340;&#23436;&#25972;&#35270;&#22270;&#65292;&#20351;&#24471;&#33258;&#21160;&#32553;&#25918;&#25104;&#20026;&#32570;&#20047;&#36866;&#24212;&#24615;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#29942;&#39048;&#12290;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24050;&#34987;&#35777;&#26126;&#22312;&#20998;&#26512;&#22797;&#26434;&#30340;&#20113;&#29615;&#22659;&#20013;&#26159;&#26377;&#30410;&#30340;&#65292;&#24182;&#20135;&#29983;&#36866;&#24212;&#24615;&#31574;&#30053;&#20197;&#26368;&#22823;&#21270;&#26399;&#26395;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Function-as-a-Service (FaaS) introduces a lightweight, function-based cloud execution model that finds its relevance in applications like IoT-edge data processing and anomaly detection. While CSP offer a near-infinite function elasticity, these applications often experience fluctuating workloads and stricter performance constraints. A typical CSP strategy is to empirically determine and adjust desired function instances, "autoscaling", based on monitoring-based thresholds such as CPU or memory, to cope with demand and performance. However, threshold configuration either requires expert knowledge, historical data or a complete view of environment, making autoscaling a performance bottleneck lacking an adaptable solution.RL algorithms are proven to be beneficial in analysing complex cloud environments and result in an adaptable policy that maximizes the expected objectives. Most realistic cloud environments usually involve operational interference and have limited visibility, making them
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20855;&#26377;&#19978;&#19979;&#25991;&#27169;&#22411;&#30340;Levin&#26641;&#25628;&#32034;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#26367;&#25442;&#20026;&#19978;&#19979;&#25991;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;LTS&#25439;&#22833;&#30340;&#20984;&#20248;&#21270;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26126;&#26174;&#20248;&#20110;LTS+NN&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.16945</link><description>&lt;p&gt;
&#20855;&#26377;&#19978;&#19979;&#25991;&#27169;&#22411;&#30340;Levin&#26641;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Levin Tree Search with Context Models. (arXiv:2305.16945v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20855;&#26377;&#19978;&#19979;&#25991;&#27169;&#22411;&#30340;Levin&#26641;&#25628;&#32034;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#26367;&#25442;&#20026;&#19978;&#19979;&#25991;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;LTS&#25439;&#22833;&#30340;&#20984;&#20248;&#21270;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26126;&#26174;&#20248;&#20110;LTS+NN&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Levin Tree Search&#65288;LTS&#65289;&#26159;&#19968;&#31181;&#21033;&#29992;&#31574;&#30053;&#65288;&#21160;&#20316;&#30340;&#27010;&#29575;&#20998;&#24067;&#65289;&#30340;&#25628;&#32034;&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#20851;&#20110;&#36798;&#21040;&#30446;&#26631;&#33410;&#28857;&#20043;&#21069;&#25193;&#23637;&#27425;&#25968;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#36825;&#21462;&#20915;&#20110;&#31574;&#30053;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#20445;&#35777;&#31216;&#20026;LTS&#25439;&#22833;&#65292;&#21487;&#20197;&#23558;&#20854;&#20316;&#20026;&#20248;&#21270;&#34920;&#31034;&#31574;&#30053;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;LTS+NN&#65289;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#29992;&#22312;&#32447;&#21387;&#32553;&#25991;&#29486;&#20013;&#30340;&#21442;&#25968;&#21270;&#19978;&#19979;&#25991;&#27169;&#22411;&#26469;&#26367;&#20195;&#65288;LTS+CM&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36825;&#31181;&#26032;&#27169;&#22411;&#19979;LTS&#25439;&#22833;&#26159;&#20984;&#30340;&#65292;&#20174;&#32780;&#21487;&#20197;&#20351;&#29992;&#26631;&#20934;&#20984;&#20248;&#21270;&#24037;&#20855;&#65292;&#24182;&#19988;&#23545;&#20110;&#32473;&#23450;&#30340;&#35299;&#36712;&#36857;&#38598;&#21512;&#65292;&#22312;&#22312;&#32447;&#35774;&#32622;&#20013;&#21487;&#20197;&#33719;&#24471;&#21040;&#26368;&#20248;&#21442;&#25968;&#30340;&#25910;&#25947;&#20445;&#35777;&#8212;&#8212;&#32780;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#25552;&#20379;&#36825;&#26679;&#30340;&#20445;&#35777;&#12290;&#26032;&#30340;LTS+CM&#31639;&#27861;&#22312;&#20960;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#19982;LTS+NN&#30456;&#27604;&#34920;&#29616;&#20986;&#26126;&#26174;&#20248;&#21183;&#65306;Sokoban&#65288;Boxoban&#65289;&#12289;The Witness&#21644;24-Sliding Tile Puzzle&#65288;STP&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Levin Tree Search (LTS) is a search algorithm that makes use of a policy (a probability distribution over actions) and comes with a theoretical guarantee on the number of expansions before reaching a goal node, depending on the quality of the policy. This guarantee can be used as a loss function, which we call the LTS loss, to optimize neural networks representing the policy (LTS+NN). In this work we show that the neural network can be substituted with parameterized context models originating from the online compression literature (LTS+CM). We show that the LTS loss is convex under this new model, which allows for using standard convex optimization tools, and obtain convergence guarantees to the optimal parameters in an online setting for a given set of solution trajectories -- guarantees that cannot be provided for neural networks. The new LTS+CM algorithm compares favorably against LTS+NN on several benchmarks: Sokoban (Boxoban), The Witness, and the 24-Sliding Tile puzzle (STP). The
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;sRank&#30340;&#36890;&#29992;&#35821;&#20041;&#23398;&#20064;&#25490;&#21517;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;transformer&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#26234;&#33021;&#22238;&#22797;&#21644;&#29615;&#22659;&#20020;&#24202;&#26234;&#33021;&#31561;&#30495;&#23454;&#24212;&#29992;&#20013;&#65292;&#23454;&#29616;11.7%&#30340;&#31163;&#32447;&#20934;&#30830;&#24230;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2304.04918</link><description>&lt;p&gt;
&#26174;&#24335;&#21644;&#38544;&#24335;&#35821;&#20041;&#25490;&#24207;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Explicit and Implicit Semantic Ranking Framework. (arXiv:2304.04918v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;sRank&#30340;&#36890;&#29992;&#35821;&#20041;&#23398;&#20064;&#25490;&#21517;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;transformer&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#26234;&#33021;&#22238;&#22797;&#21644;&#29615;&#22659;&#20020;&#24202;&#26234;&#33021;&#31561;&#30495;&#23454;&#24212;&#29992;&#20013;&#65292;&#23454;&#29616;11.7%&#30340;&#31163;&#32447;&#20934;&#30830;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#26680;&#24515;&#38590;&#39064;&#26159;&#23558;&#19968;&#20010;&#26597;&#35810;&#19982;&#19968;&#20010;&#21487;&#21464;&#19988;&#26377;&#38480;&#30340;&#25991;&#26723;&#38598;&#20013;&#30340;&#26368;&#20339;&#25991;&#26723;&#36827;&#34892;&#21305;&#37197;&#12290;&#29616;&#26377;&#30340;&#24037;&#19994;&#35299;&#20915;&#26041;&#26696;&#65292;&#29305;&#21035;&#26159;&#24310;&#36831;&#21463;&#38480;&#30340;&#26381;&#21153;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#30456;&#20284;&#24615;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#20026;&#20102;&#36895;&#24230;&#32780;&#29306;&#29298;&#20102;&#36136;&#37327;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#35821;&#20041;&#23398;&#20064;&#25490;&#21517;&#26694;&#26550;&#65292;&#33258;&#25105;&#35757;&#32451;&#35821;&#20041;&#20132;&#21449;&#20851;&#27880;&#25490;&#21517;&#65288;sRank&#65289;&#12290;&#36825;&#20010;&#22522;&#20110;transformer&#30340;&#26694;&#26550;&#20351;&#29992;&#32447;&#24615;&#25104;&#23545;&#25439;&#22833;&#65292;&#20855;&#26377;&#21487;&#21464;&#30340;&#35757;&#32451;&#25209;&#37327;&#22823;&#23567;&#12289;&#23454;&#29616;&#36136;&#37327;&#25552;&#21319;&#21644;&#39640;&#25928;&#29575;&#65292;&#24182;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#24494;&#36719;&#20844;&#21496;&#30340;&#20004;&#20010;&#24037;&#19994;&#20219;&#21153;&#65306;&#26234;&#33021;&#22238;&#22797;&#65288;SR&#65289;&#21644;&#29615;&#22659;&#20020;&#24202;&#26234;&#33021;&#65288;ACI&#65289;&#30340;&#30495;&#23454;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#12290;&#22312;&#26234;&#33021;&#22238;&#22797;&#20013;&#65292;$sRank$&#36890;&#36807;&#22522;&#20110;&#28040;&#36153;&#32773;&#21644;&#25903;&#25345;&#20195;&#29702;&#20449;&#24687;&#30340;&#39044;&#23450;&#20041;&#35299;&#20915;&#26041;&#26696;&#36873;&#25321;&#26368;&#20339;&#31572;&#26696;&#65292;&#24110;&#21161;&#29992;&#25143;&#23454;&#26102;&#33719;&#24471;&#25216;&#26415;&#25903;&#25345;&#12290;&#22312;SR&#20219;&#21153;&#19978;&#65292;$sRank$&#23454;&#29616;&#20102;11.7%&#30340;&#31163;&#32447;top-one&#20934;&#30830;&#24230;&#25552;&#21319;&#65292;&#27604;&#20043;&#21069;&#30340;&#31995;&#32479;&#26356;&#21152;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;
The core challenge in numerous real-world applications is to match an inquiry to the best document from a mutable and finite set of candidates. Existing industry solutions, especially latency-constrained services, often rely on similarity algorithms that sacrifice quality for speed. In this paper we introduce a generic semantic learning-to-rank framework, Self-training Semantic Cross-attention Ranking (sRank). This transformer-based framework uses linear pairwise loss with mutable training batch sizes and achieves quality gains and high efficiency, and has been applied effectively to show gains on two industry tasks at Microsoft over real-world large-scale data sets: Smart Reply (SR) and Ambient Clinical Intelligence (ACI). In Smart Reply, $sRank$ assists live customers with technical support by selecting the best reply from predefined solutions based on consumer and support agent messages. It achieves 11.7% gain in offline top-one accuracy on the SR task over the previous system, and 
&lt;/p&gt;</description></item></channel></rss>