<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36890;&#36807;&#24341;&#20837; BorealTC &#25968;&#25454;&#38598;&#65292;&#32467;&#21512;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#26032;&#39062;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;-Mamba&#20307;&#31995;&#32467;&#26500;&#22312;&#21271;&#26041;&#26862;&#26519;&#22320;&#24418;&#20998;&#31867;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.16877</link><description>&lt;p&gt;
&#24863;&#30693;&#21147;&#23601;&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;&#65306;&#21271;&#26041;&#26862;&#26519;&#30340;&#22320;&#24418;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Proprioception Is All You Need: Terrain Classification for Boreal Forests
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16877
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837; BorealTC &#25968;&#25454;&#38598;&#65292;&#32467;&#21512;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#26032;&#39062;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;-Mamba&#20307;&#31995;&#32467;&#26500;&#22312;&#21271;&#26041;&#26862;&#26519;&#22320;&#24418;&#20998;&#31867;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#39046;&#22495;&#26426;&#22120;&#20154;&#23398;&#30740;&#31350;&#24378;&#35843;&#20102;&#25269;&#24481;&#19981;&#21516;&#31867;&#22411;&#22320;&#24418;&#30340;&#37325;&#35201;&#24615;&#12290;&#21271;&#26041;&#26862;&#26519;&#29305;&#21035;&#21463;&#21040;&#35768;&#22810;&#38480;&#21046;&#26426;&#21160;&#24615;&#30340;&#22320;&#24418;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#22320;&#24418;&#24212;&#35813;&#22312;&#36234;&#37326;&#33258;&#20027;&#23548;&#33322;&#20013;&#21152;&#20197;&#32771;&#34385;&#12290;&#27492;&#22806;&#65292;&#20316;&#20026;&#22320;&#29699;&#19978;&#26368;&#22823;&#30340;&#38470;&#22320;&#29983;&#29289;&#32676;&#33853;&#20043;&#19968;&#65292;&#21271;&#26041;&#26862;&#26519;&#26159;&#39044;&#35745;&#33258;&#20027;&#36710;&#36742;&#23558;&#26085;&#30410;&#26222;&#21450;&#30340;&#22320;&#21306;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;BorealTC&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22522;&#20110;&#24863;&#30693;&#21147;&#30340;&#22320;&#24418;&#20998;&#31867;&#65288;TC&#65289;&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#35760;&#24405;&#20102;Husky A200&#30340;116&#20998;&#38047;&#30340;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#65288;IMU&#65289;&#12289;&#30005;&#26426;&#30005;&#27969;&#21644;&#36718;&#32974;&#37324;&#31243;&#25968;&#25454;&#65292;&#37325;&#28857;&#20851;&#27880;&#20856;&#22411;&#30340;&#21271;&#26041;&#26862;&#26519;&#22320;&#24418;&#65292;&#29305;&#21035;&#26159;&#38634;&#12289;&#20912;&#21644;&#28132;&#27877;&#22756;&#12290;&#32467;&#21512;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19982;&#21478;&#19968;&#20010;&#26469;&#33258;&#26368;&#26032;&#25216;&#26415;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#22312;TC t
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16877v1 Announce Type: cross  Abstract: Recent works in field robotics highlighted the importance of resiliency against different types of terrains. Boreal forests, in particular, are home to many mobility-impeding terrains that should be considered for off-road autonomous navigation. Also, being one of the largest land biomes on Earth, boreal forests are an area where autonomous vehicles are expected to become increasingly common. In this paper, we address this issue by introducing BorealTC, a publicly available dataset for proprioceptive-based terrain classification (TC). Recorded with a Husky A200, our dataset contains 116 min of Inertial Measurement Unit (IMU), motor current, and wheel odometry data, focusing on typical boreal forest terrains, notably snow, ice, and silty loam. Combining our dataset with another dataset from the state-of-the-art, we evaluate both a Convolutional Neural Network (CNN) and the novel state space model (SSM)-based Mamba architecture on a TC t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;PromptKD&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#23454;&#29616;&#20102;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#23398;&#29983;&#21451;&#22909;&#30693;&#35782;&#30340;&#33976;&#39311;&#65292;&#26080;&#38656;&#24494;&#35843;&#25972;&#25972;&#20010;&#25945;&#24072;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.12842</link><description>&lt;p&gt;
PromptKD&#65306;&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#20026;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#23398;&#29983;&#21451;&#22909;&#30693;&#35782;&#30340;&#33976;&#39311;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12842
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PromptKD&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#23454;&#29616;&#20102;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#23398;&#29983;&#21451;&#22909;&#30693;&#35782;&#30340;&#33976;&#39311;&#65292;&#26080;&#38656;&#24494;&#35843;&#25972;&#25972;&#20010;&#25945;&#24072;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#24341;&#36215;&#20102;&#23545;&#25512;&#29702;&#25104;&#26412;&#30340;&#25285;&#24551;&#65292;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#23545;&#27169;&#22411;&#21387;&#32553;&#30740;&#31350;&#30340;&#38656;&#27714;&#12290;&#23613;&#31649;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#26159;&#19968;&#31181;&#31361;&#20986;&#30340;&#26041;&#27861;&#65292;&#20294;&#26159;&#38024;&#23545;LLMs&#36825;&#26679;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;KD&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#65292;&#32780;&#25552;&#21462;&#36866;&#21512;&#23398;&#29983;&#30340;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#31867;&#27169;&#22411;&#30340;KD&#20013;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#24615;&#33021;&#65292;&#22312;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PromptKD&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#25552;&#31034;&#35843;&#25972; - &#22312;KD&#20013;&#39318;&#27425;&#20986;&#29616; - &#20351;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20256;&#36882;&#36866;&#21512;&#23398;&#29983;&#30340;&#30693;&#35782;&#12290;&#19982;&#20808;&#21069;&#20998;&#31867;&#24037;&#20316;&#19981;&#21516;&#65292;&#20808;&#21069;&#37027;&#20123;&#38656;&#35201;&#24494;&#35843;&#25972;&#25972;&#20010;&#25945;&#24072;&#27169;&#22411;&#20197;&#25552;&#21462;&#36866;&#21512;&#23398;&#29983;&#30340;&#30693;&#35782;&#65292;PromptKD&#36890;&#36807;&#28155;&#21152;&#23569;&#37327;&#25552;&#31034;&#26631;&#35760;&#65292;&#24182;&#20165;&#36890;&#36807;&#23398;&#29983;&#25351;&#23548;&#35843;&#25972;&#25552;&#31034;&#26469;&#36798;&#21040;&#31867;&#20284;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12842v1 Announce Type: cross  Abstract: Recent advancements in large language models (LLMs) have raised concerns about inference costs, increasing the need for research into model compression. While knowledge distillation (KD) is a prominent method for this, research on KD for generative language models like LLMs is relatively sparse, and the approach of distilling student-friendly knowledge, which has shown promising performance in KD for classification models, remains unexplored in generative language models. To explore this approach, we propose PromptKD, a simple yet effective method that utilizes prompt tuning - for the first time in KD - to enable generative language models to transfer student-friendly knowledge. Unlike previous works in classification that require fine-tuning the entire teacher model for extracting student-friendly knowledge, PromptKD achieves similar effects by adding a small number of prompt tokens and tuning only the prompt with student guidance. Ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#35810;&#38382;&#36873;&#27665;&#26377;&#20851;&#23569;&#37327;&#20505;&#36873;&#20154;&#30340;&#25237;&#31080;&#35268;&#21017;&#35745;&#31639;&#38382;&#39064;&#65292;&#23436;&#20840;&#34920;&#24449;&#20102;&#21487;&#35745;&#31639;&#30340;&#20301;&#32622;&#35780;&#20998;&#35268;&#21017;&#38598;&#21512;&#65292;&#24182;&#32473;&#20986;&#20102;&#23545;&#20110;&#30830;&#23450;&#24615;&#25110;&#38543;&#26426;&#31639;&#27861;&#30830;&#23450;&#26368;&#22823;&#24471;&#20998;&#20505;&#36873;&#20154;&#24517;&#39035;&#36827;&#34892;&#30340;&#26597;&#35810;&#27425;&#25968;&#30340;&#21442;&#25968;&#21270;&#19978;&#38480;&#21644;&#19979;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.11104</link><description>&lt;p&gt;
&#29992;&#35810;&#38382;&#19981;&#23436;&#25972;&#36873;&#31080;&#35745;&#31639;&#25237;&#31080;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Computing Voting Rules with Elicited Incomplete Votes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#35810;&#38382;&#36873;&#27665;&#26377;&#20851;&#23569;&#37327;&#20505;&#36873;&#20154;&#30340;&#25237;&#31080;&#35268;&#21017;&#35745;&#31639;&#38382;&#39064;&#65292;&#23436;&#20840;&#34920;&#24449;&#20102;&#21487;&#35745;&#31639;&#30340;&#20301;&#32622;&#35780;&#20998;&#35268;&#21017;&#38598;&#21512;&#65292;&#24182;&#32473;&#20986;&#20102;&#23545;&#20110;&#30830;&#23450;&#24615;&#25110;&#38543;&#26426;&#31639;&#27861;&#30830;&#23450;&#26368;&#22823;&#24471;&#20998;&#20505;&#36873;&#20154;&#24517;&#39035;&#36827;&#34892;&#30340;&#26597;&#35810;&#27425;&#25968;&#30340;&#21442;&#25968;&#21270;&#19978;&#38480;&#21644;&#19979;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#22312;&#22823;&#22411;&#20505;&#36873;&#20154;&#32676;&#20307;&#20013;&#35828;&#26126;&#23436;&#25972;&#24207;&#25968;&#20559;&#22909;&#30340;&#22256;&#38590;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36890;&#36807;&#35810;&#38382;&#36873;&#27665;&#26377;&#20851; $t &lt; m$ &#20505;&#36873;&#20154;&#30340;&#25237;&#31080;&#35268;&#21017;&#12290;&#22312;&#25512;&#24191;&#20102;&#20851;&#20110;&#35813;&#38382;&#39064;&#29305;&#23450;&#24773;&#20917;&#30340;&#20808;&#21069;&#30740;&#31350;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#23436;&#20840;&#34920;&#24449;&#20102;&#23545;&#20110;&#20219;&#24847; $1 \leq t &lt; m$ &#21487;&#20197;&#35745;&#31639;&#24471;&#20986;&#30340;&#20301;&#32622;&#35780;&#20998;&#35268;&#21017;&#30340;&#38598;&#21512;&#65292;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#20854;&#20013;&#24182;&#19981;&#21253;&#25324;&#22810;&#25968;&#21046;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25193;&#23637;&#36825;&#19968;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#21333;&#27425;&#21487;&#36716;&#31227;&#25237;&#31080;&#65288;&#28120;&#27760;&#25237;&#31080;&#65289;&#30340;&#31867;&#20284;&#26080;&#27861;&#35745;&#31639;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#36127;&#38754;&#32467;&#26524;&#26159;&#20449;&#24687;&#29702;&#35770;&#30340;&#65292;&#19981;&#20851;&#24515;&#26597;&#35810;&#30340;&#25968;&#37327;&#12290;&#26368;&#21518;&#65292;&#23545;&#20110;&#21487;&#20197;&#20351;&#29992;&#26377;&#38480;&#22823;&#23567;&#26597;&#35810;&#35745;&#31639;&#30340;&#35780;&#20998;&#35268;&#21017;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#21442;&#25968;&#21270;&#30340;&#20851;&#20110;&#30830;&#23450;&#24615;&#25110;&#38543;&#26426;&#31639;&#27861;&#24517;&#39035;&#20570;&#20986;&#30340;&#26597;&#35810;&#25968;&#37327;&#30340;&#19978;&#38480;&#21644;&#19979;&#38480;&#12290;&#34429;&#28982;&#25105;&#20204;&#30340;&#30830;&#23450;&#24615;&#31639;&#27861;&#30340;&#30028;&#38480;&#20043;&#38388;&#27809;&#26377;&#24046;&#36317;&#65292;&#20294;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11104v1 Announce Type: cross  Abstract: Motivated by the difficulty of specifying complete ordinal preferences over a large set of $m$ candidates, we study voting rules that are computable by querying voters about $t &lt; m$ candidates. Generalizing prior works that focused on specific instances of this problem, our paper fully characterizes the set of positional scoring rules that can be computed for any $1 \leq t &lt; m$, which notably does not include plurality. We then extend this to show a similar impossibility result for single transferable vote (elimination voting). These negative results are information-theoretic and agnostic to the number of queries. Finally, for scoring rules that are computable with limited-sized queries, we give parameterized upper and lower bounds on the number of such queries a deterministic or randomized algorithm must make to determine the score-maximizing candidate. While there is no gap between our bounds for deterministic algorithms, identifying
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#35266;&#23519;&#30340;Approximate MEan-Direction Solver&#65288;AMED-Solver&#65289;&#65292;&#33021;&#22815;&#36890;&#36807;&#30452;&#25509;&#23398;&#20064;&#22343;&#26041;&#21521;&#26469;&#28040;&#38500;&#25130;&#26029;&#35823;&#24046;&#65292;&#20174;&#32780;&#23454;&#29616;&#24555;&#36895;&#25193;&#25955;&#25277;&#26679;&#12290;</title><link>https://arxiv.org/abs/2312.00094</link><description>&lt;p&gt;
&#22312;&#22823;&#32422;5&#20010;&#27493;&#39588;&#20013;&#65292;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#24555;&#36895;&#22522;&#20110;ODE&#30340;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Fast ODE-based Sampling for Diffusion Models in Around 5 Steps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00094
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#35266;&#23519;&#30340;Approximate MEan-Direction Solver&#65288;AMED-Solver&#65289;&#65292;&#33021;&#22815;&#36890;&#36807;&#30452;&#25509;&#23398;&#20064;&#22343;&#26041;&#21521;&#26469;&#28040;&#38500;&#25130;&#26029;&#35823;&#24046;&#65292;&#20174;&#32780;&#23454;&#29616;&#24555;&#36895;&#25193;&#25955;&#25277;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#36827;&#34892;&#25277;&#26679;&#21487;&#20197;&#34987;&#35270;&#20026;&#35299;&#20915;&#30456;&#24212;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#65292;&#26088;&#22312;&#20197;&#23613;&#21487;&#33021;&#23569;&#30340;&#20989;&#25968;&#35780;&#20272;&#27425;&#25968;&#65288;NFE&#65289;&#33719;&#24471;&#20934;&#30830;&#35299;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#21033;&#29992;&#39640;&#38454;ODE&#27714;&#35299;&#22120;&#30340;&#21508;&#31181;&#24555;&#36895;&#25277;&#26679;&#22120;&#65292;&#24182;&#19988;&#27604;&#26368;&#21021;&#30340;&#19968;&#38454;&#27714;&#35299;&#22120;&#34920;&#29616;&#26356;&#22909;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#20540;&#26041;&#27861;&#22266;&#26377;&#22320;&#23548;&#33268;&#26576;&#20123;&#36817;&#20284;&#35823;&#24046;&#65292;&#26497;&#22823;&#22320;&#38477;&#20302;&#20102;&#20855;&#26377;&#26497;&#23567;NFE&#65288;&#20363;&#22914;&#65292;&#32422;&#20026;5&#65289;&#30340;&#26679;&#26412;&#36136;&#37327;&#12290;&#30456;&#21453;&#65292;&#22522;&#20110;&#20960;&#20309;&#35266;&#23519;&#65292;&#27599;&#20010;&#25277;&#26679;&#36712;&#36857;&#20960;&#20046;&#20301;&#20110;&#23884;&#20837;&#22312;&#29615;&#22659;&#31354;&#38388;&#20013;&#30340;&#20108;&#32500;&#23376;&#31354;&#38388;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#24555;&#36895;&#25193;&#25955;&#25277;&#26679;&#30340;AME&#36817;&#20284;&#22343;&#26041;&#21521;&#27714;&#35299;&#22120;&#65288;AMED-Solver&#65289;&#65292;&#36890;&#36807;&#30452;&#25509;&#23398;&#20064;&#22343;&#26041;&#21521;&#26469;&#28040;&#38500;&#25130;&#26029;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#20316;&#20026;&#25554;&#20214;&#20351;&#29992;&#65292;&#20197;&#36827;&#19968;&#27493;&#25913;&#36827;&#29616;&#26377;&#30340;&#22522;&#20110;ODE&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00094v2 Announce Type: replace-cross  Abstract: Sampling from diffusion models can be treated as solving the corresponding ordinary differential equations (ODEs), with the aim of obtaining an accurate solution with as few number of function evaluations (NFE) as possible. Recently, various fast samplers utilizing higher-order ODE solvers have emerged and achieved better performance than the initial first-order one. However, these numerical methods inherently result in certain approximation errors, which significantly degrades sample quality with extremely small NFE (e.g., around 5). In contrast, based on the geometric observation that each sampling trajectory almost lies in a two-dimensional subspace embedded in the ambient space, we propose Approximate MEan-Direction Solver (AMED-Solver) that eliminates truncation errors by directly learning the mean direction for fast diffusion sampling. Besides, our method can be easily used as a plugin to further improve existing ODE-base
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23398;&#20064;&#30340;&#22768;&#26126;&#24615;&#38544;&#31169;&#20445;&#25252;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;Differentially-Private Stochastic Gradient Descent&#65288;DP-SGD&#65289;&#31639;&#27861;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26367;&#20195;&#37096;&#20998;&#23454;&#38469;&#25968;&#25454;&#26469;&#22238;&#31572;&#26597;&#35810;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#25351;&#23450;&#35201;&#20445;&#25252;&#30340;&#31169;&#20154;&#20449;&#24687;&#12290;&#27492;&#26694;&#26550;&#36824;&#21487;&#20197;&#33258;&#21160;&#36873;&#25321;&#36716;&#25442;&#35745;&#21010;&#21644;&#36229;&#21442;&#25968;&#65292;&#24182;&#20801;&#35768;&#20154;&#24037;&#19987;&#23478;&#23457;&#26680;&#21644;&#35843;&#25972;&#38544;&#31169;&#20445;&#25252;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.12393</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#22768;&#26126;&#24615;&#38544;&#31169;&#20445;&#25252;&#25968;&#25454;&#32852;&#37030;&#31649;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Learning-based Declarative Privacy-Preserving Framework for Federated Data Management. (arXiv:2401.12393v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23398;&#20064;&#30340;&#22768;&#26126;&#24615;&#38544;&#31169;&#20445;&#25252;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;Differentially-Private Stochastic Gradient Descent&#65288;DP-SGD&#65289;&#31639;&#27861;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26367;&#20195;&#37096;&#20998;&#23454;&#38469;&#25968;&#25454;&#26469;&#22238;&#31572;&#26597;&#35810;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#25351;&#23450;&#35201;&#20445;&#25252;&#30340;&#31169;&#20154;&#20449;&#24687;&#12290;&#27492;&#26694;&#26550;&#36824;&#21487;&#20197;&#33258;&#21160;&#36873;&#25321;&#36716;&#25442;&#35745;&#21010;&#21644;&#36229;&#21442;&#25968;&#65292;&#24182;&#20801;&#35768;&#20154;&#24037;&#19987;&#23478;&#23457;&#26680;&#21644;&#35843;&#25972;&#38544;&#31169;&#20445;&#25252;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20010;&#31169;&#26377;&#25968;&#25454;&#23396;&#23707;&#19978;&#36827;&#34892;&#32852;&#37030;&#26597;&#35810;&#22788;&#29702;&#26102;&#65292;&#24179;&#34913;&#38544;&#31169;&#21644;&#20934;&#30830;&#24615;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#28436;&#31034;&#19968;&#31181;&#33258;&#21160;&#21270;&#26032;&#20852;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#30340;&#31471;&#21040;&#31471;&#24037;&#20316;&#27969;&#65292;&#35813;&#25216;&#26415;&#20351;&#29992;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DP-SGD&#65289;&#31639;&#27861;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26367;&#25442;&#23454;&#38469;&#25968;&#25454;&#30340;&#37096;&#20998;&#26469;&#22238;&#31572;&#26597;&#35810;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26032;&#39062;&#22768;&#26126;&#24615;&#38544;&#31169;&#20445;&#25252;&#24037;&#20316;&#27969;&#20801;&#35768;&#29992;&#25143;&#25351;&#23450;&#8220;&#35201;&#20445;&#25252;&#30340;&#31169;&#20154;&#20449;&#24687;&#8221;&#32780;&#19981;&#26159;&#8220;&#22914;&#20309;&#20445;&#25252;&#8221;&#12290;&#22312;&#24213;&#23618;&#65292;&#31995;&#32479;&#33258;&#21160;&#36873;&#25321;&#26597;&#35810;-&#27169;&#22411;&#36716;&#25442;&#35745;&#21010;&#20197;&#21450;&#36229;&#21442;&#25968;&#12290;&#21516;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#24037;&#20316;&#27969;&#36824;&#20801;&#35768;&#20154;&#24037;&#19987;&#23478;&#23457;&#26680;&#21644;&#35843;&#25972;&#36873;&#25321;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#21046;&#65292;&#29992;&#20110;&#23457;&#35745;/&#21512;&#35268;&#21644;&#20248;&#21270;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is challenging to balance the privacy and accuracy for federated query processing over multiple private data silos. In this work, we will demonstrate an end-to-end workflow for automating an emerging privacy-preserving technique that uses a deep learning model trained using the Differentially-Private Stochastic Gradient Descent (DP-SGD) algorithm to replace portions of actual data to answer a query. Our proposed novel declarative privacy-preserving workflow allows users to specify "what private information to protect" rather than "how to protect". Under the hood, the system automatically chooses query-model transformation plans as well as hyper-parameters. At the same time, the proposed workflow also allows human experts to review and tune the selected privacy-preserving mechanism for audit/compliance, and optimization purposes.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992; EHR &#25968;&#25454;&#36827;&#34892;&#22823;&#35268;&#27169;&#32959;&#30244;&#39118;&#38505;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21482;&#38656;&#21033;&#29992;&#21382;&#21490;&#30340;&#21307;&#30103;&#26381;&#21153;&#20195;&#30721;&#21644;&#35786;&#26029;&#20449;&#24687;&#26469;&#23454;&#29616;&#26368;&#23567;&#21270;&#30340;&#25968;&#25454;&#38656;&#27714;&#65292;&#36890;&#36807;&#23558;&#23384;&#27963;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;&#24212;&#29992;&#20013;&#23454;&#29616;&#23545;&#24739;&#32773;&#30284;&#30151;&#39118;&#38505;&#30340;&#20010;&#24615;&#21270;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.15039</link><description>&lt;p&gt;
&#32467;&#21512;&#23384;&#27963;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#36827;&#34892;&#32959;&#30244;&#39118;&#38505;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Combining Survival Analysis and Machine Learning for Mass Cancer Risk Prediction using EHR data. (arXiv:2309.15039v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15039
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992; EHR &#25968;&#25454;&#36827;&#34892;&#22823;&#35268;&#27169;&#32959;&#30244;&#39118;&#38505;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21482;&#38656;&#21033;&#29992;&#21382;&#21490;&#30340;&#21307;&#30103;&#26381;&#21153;&#20195;&#30721;&#21644;&#35786;&#26029;&#20449;&#24687;&#26469;&#23454;&#29616;&#26368;&#23567;&#21270;&#30340;&#25968;&#25454;&#38656;&#27714;&#65292;&#36890;&#36807;&#23558;&#23384;&#27963;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;&#24212;&#29992;&#20013;&#23454;&#29616;&#23545;&#24739;&#32773;&#30284;&#30151;&#39118;&#38505;&#30340;&#20010;&#24615;&#21270;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32431;&#31929;&#30340;&#21307;&#23398;&#32959;&#30244;&#31579;&#26597;&#26041;&#27861;&#36890;&#24120;&#36153;&#29992;&#39640;&#26114;&#12289;&#32791;&#26102;&#38271;&#65292;&#24182;&#19988;&#20165;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#24212;&#29992;&#12290;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#27861;&#22312;&#30284;&#30151;&#26816;&#27979;&#26041;&#38754;&#21457;&#25381;&#20102;&#24040;&#22823;&#20316;&#29992;&#65292;&#20294;&#38656;&#35201;&#29305;&#23450;&#25110;&#28145;&#20837;&#30340;&#21307;&#23398;&#25968;&#25454;&#12290;&#36825;&#20123;&#26041;&#38754;&#24433;&#21709;&#20102;&#30284;&#30151;&#31579;&#26597;&#26041;&#27861;&#30340;&#22823;&#35268;&#27169;&#23454;&#26045;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#24050;&#26377;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#23545;&#24739;&#32773;&#36827;&#34892;&#22823;&#35268;&#27169;&#20010;&#24615;&#21270;&#30284;&#30151;&#39118;&#38505;&#35780;&#20272;&#24212;&#29992;AI&#26041;&#27861;&#26159;&#19968;&#31181;&#39072;&#35206;&#24615;&#30340;&#25913;&#21464;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;EHR&#25968;&#25454;&#36827;&#34892;&#22823;&#35268;&#27169;&#32959;&#30244;&#39118;&#38505;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#30340;&#25968;&#25454;&#36138;&#23146;&#31574;&#30053;&#33073;&#39062;&#32780;&#20986;&#65292;&#20165;&#38656;&#35201;&#26469;&#33258;EHR&#30340;&#21307;&#30103;&#26381;&#21153;&#20195;&#30721;&#21644;&#35786;&#26029;&#21382;&#21490;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#20108;&#20998;&#31867;&#38382;&#39064;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;175441&#21517;&#19981;&#35760;&#21517;&#30340;&#24739;&#32773;&#65288;&#20854;&#20013;2861&#21517;&#34987;&#35786;&#26029;&#20026;&#30284;&#30151;&#65289;&#12290;&#20316;&#20026;&#22522;&#20934;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#23384;&#27963;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;
&lt;/p&gt;
&lt;p&gt;
Purely medical cancer screening methods are often costly, time-consuming, and weakly applicable on a large scale. Advanced Artificial Intelligence (AI) methods greatly help cancer detection but require specific or deep medical data. These aspects affect the mass implementation of cancer screening methods. For these reasons, it is a disruptive change for healthcare to apply AI methods for mass personalized assessment of the cancer risk among patients based on the existing Electronic Health Records (EHR) volume.  This paper presents a novel method for mass cancer risk prediction using EHR data. Among other methods, our one stands out by the minimum data greedy policy, requiring only a history of medical service codes and diagnoses from EHR. We formulate the problem as a binary classification. This dataset contains 175 441 de-identified patients (2 861 diagnosed with cancer). As a baseline, we implement a solution based on a recurrent neural network (RNN). We propose a method that combine
&lt;/p&gt;</description></item><item><title>CyberForce&#26159;&#19968;&#20010;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#29289;&#32852;&#32593;&#35774;&#22791;&#20013;&#21327;&#21516;&#31169;&#23494;&#22320;&#30830;&#23450;&#36866;&#21512;&#32531;&#35299;&#21508;&#31181;&#38646;&#26085;&#25915;&#20987;&#30340;MTD&#25216;&#26415;&#12290;&#23427;&#25972;&#21512;&#20102;&#35774;&#22791;&#25351;&#32441;&#35782;&#21035;&#21644;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#36890;&#36807;&#22870;&#21169;&#25110;&#24809;&#32602;FRL agent&#36873;&#25321;&#30340;MTD&#26426;&#21046;&#26469;&#25552;&#39640;&#32593;&#32476;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.05978</link><description>&lt;p&gt;
CyberForce: &#19968;&#20010;&#29992;&#20110;&#24694;&#24847;&#36719;&#20214;&#32531;&#35299;&#30340;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CyberForce: A Federated Reinforcement Learning Framework for Malware Mitigation. (arXiv:2308.05978v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05978
&lt;/p&gt;
&lt;p&gt;
CyberForce&#26159;&#19968;&#20010;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#29289;&#32852;&#32593;&#35774;&#22791;&#20013;&#21327;&#21516;&#31169;&#23494;&#22320;&#30830;&#23450;&#36866;&#21512;&#32531;&#35299;&#21508;&#31181;&#38646;&#26085;&#25915;&#20987;&#30340;MTD&#25216;&#26415;&#12290;&#23427;&#25972;&#21512;&#20102;&#35774;&#22791;&#25351;&#32441;&#35782;&#21035;&#21644;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#36890;&#36807;&#22870;&#21169;&#25110;&#24809;&#32602;FRL agent&#36873;&#25321;&#30340;MTD&#26426;&#21046;&#26469;&#25552;&#39640;&#32593;&#32476;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#29289;&#32852;&#32593;(IoT)&#33539;&#20363;&#30340;&#25193;&#23637;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#65292;&#20294;&#26159;&#23545;&#20110;IoT&#35774;&#22791;&#23545;&#24694;&#24847;&#36719;&#20214;&#20107;&#20214;&#30340;&#33030;&#24369;&#24615;&#24050;&#25104;&#20026;&#19968;&#20010;&#36234;&#26469;&#36234;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#31227;&#21160;&#30446;&#26631;&#38450;&#24481;(MTD)&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#22686;&#24378;IoT&#35774;&#22791;&#30340;&#32593;&#32476;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#37327;&#30340;&#26032;&#24694;&#24847;&#36719;&#20214;&#25915;&#20987;&#21644;&#20195;&#29702;&#20154;&#23398;&#20064;&#21644;&#36873;&#25321;&#26377;&#25928;&#30340;MTD&#25216;&#26415;&#25152;&#38656;&#30340;&#26102;&#38388;&#20351;&#24471;&#36825;&#31181;&#26041;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;IoT&#22330;&#26223;&#20013;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CyberForce&#65292;&#19968;&#20010;&#37319;&#29992;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;(FRL)&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#38598;&#20307;&#19988;&#20445;&#23494;&#22320;&#30830;&#23450;&#36866;&#21512;&#32531;&#35299;&#21508;&#31181;&#38646;&#26085;&#25915;&#20987;&#30340;MTD&#25216;&#26415;&#12290;CyberForce&#32467;&#21512;&#20102;&#35774;&#22791;&#25351;&#32441;&#35782;&#21035;&#21644;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#22870;&#21169;&#25110;&#24809;&#32602;FRL agent&#36873;&#25321;&#30340;MTD&#26426;&#21046;&#12290;&#35813;&#26694;&#26550;&#22312;&#19968;&#20010;&#30001;&#21313;&#21488;&#30495;&#23454;IoT&#24179;&#21488;&#35774;&#22791;&#32452;&#25104;&#30340;&#32852;&#37030;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#36890;&#36807;&#20845;&#20010;&#24694;&#24847;&#36719;&#20214;&#26679;&#26412;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The expansion of the Internet-of-Things (IoT) paradigm is inevitable, but vulnerabilities of IoT devices to malware incidents have become an increasing concern. Recent research has shown that the integration of Reinforcement Learning with Moving Target Defense (MTD) mechanisms can enhance cybersecurity in IoT devices. Nevertheless, the numerous new malware attacks and the time that agents take to learn and select effective MTD techniques make this approach impractical for real-world IoT scenarios. To tackle this issue, this work presents CyberForce, a framework that employs Federated Reinforcement Learning (FRL) to collectively and privately determine suitable MTD techniques for mitigating diverse zero-day attacks. CyberForce integrates device fingerprinting and anomaly detection to reward or penalize MTD mechanisms chosen by an FRL-based agent. The framework has been evaluated in a federation consisting of ten devices of a real IoT platform. A pool of experiments with six malware samp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#26469;&#27979;&#35797;ITM&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#23558;&#19968;&#20123;&#8220;&#24858;&#24324;&#8221;&#30340;&#22270;&#29255;&#21644;&#26631;&#39064;&#28155;&#21152;&#21040;&#26816;&#32034;&#27744;&#20013;&#65292;&#22312;MS COCO&#25968;&#25454;&#38598;&#19978;&#20026;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#40065;&#26834;&#24615;&#27979;&#35797;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;</title><link>http://arxiv.org/abs/2304.10727</link><description>&lt;p&gt;
RoCOCO&#65306;&#31283;&#20581;&#30340;&#22522;&#20934;MS-COCO&#35780;&#20272;&#22270;&#25991;&#21305;&#37197;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
RoCOCO: Robust Benchmark MS-COCO to Stress-test Robustness of Image-Text Matching Models. (arXiv:2304.10727v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#26469;&#27979;&#35797;ITM&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#23558;&#19968;&#20123;&#8220;&#24858;&#24324;&#8221;&#30340;&#22270;&#29255;&#21644;&#26631;&#39064;&#28155;&#21152;&#21040;&#26816;&#32034;&#27744;&#20013;&#65292;&#22312;MS COCO&#25968;&#25454;&#38598;&#19978;&#20026;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#40065;&#26834;&#24615;&#27979;&#35797;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#20041;&#23884;&#20837;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;MS COCO 5K&#27979;&#35797;&#38598;&#19978;&#22270;&#25991;&#21305;&#37197;&#65288;ITM&#65289;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#36825;&#20123;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#26102;&#65292;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#26469;&#27979;&#35797;ITM&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#21508;&#31181;&#8220;&#24858;&#24324;&#8221;&#30340;&#22270;&#29255;&#21644;&#26631;&#39064;&#28155;&#21152;&#21040;&#26816;&#32034;&#27744;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#25554;&#20837;&#19981;&#30456;&#20851;&#30340;&#22270;&#20687;&#26469;&#26356;&#25913;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#26367;&#25442;&#21517;&#35789;&#26469;&#26356;&#25913;&#26631;&#39064;&#65292;&#20174;&#32780;&#25913;&#21464;&#21477;&#23376;&#30340;&#21547;&#20041;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#20165;&#23558;&#36825;&#20123;&#26032;&#21019;&#24314;&#30340;&#22270;&#20687;&#21644;&#26631;&#39064;&#28155;&#21152;&#21040;&#27979;&#35797;&#38598;&#20013;&#23601;&#21487;&#20197;&#38477;&#20302;&#21508;&#31181;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#65288;&#20363;&#22914;&#65292;&#22312;BLIP&#20013;&#20174;81.9&#65285;&#38477;&#33267;64.5&#65285;&#65292;&#22312;VSE&#8734;&#20013;&#20174;66.1&#65285;&#38477;&#33267;37.5&#65285;&#65289;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#21457;&#29616;&#33021;&#20026;&#25552;&#39640;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#35774;&#35745;&#26356;&#22810;&#26679;&#21270;&#30340;&#21387;&#21147;&#27979;&#35797;&#25552;&#20379;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large-scale vision-language pre-training models and visual semantic embedding methods have significantly improved image-text matching (ITM) accuracy on MS COCO 5K test set. However, it is unclear how robust these state-of-the-art (SOTA) models are when using them in the wild. In this paper, we propose a novel evaluation benchmark to stress-test the robustness of ITM models. To this end, we add various fooling images and captions to a retrieval pool. Specifically, we change images by inserting unrelated images, and change captions by substituting a noun, which can change the meaning of a sentence. We discover that just adding these newly created images and captions to the test set can degrade performances (i.e., Recall@1) of a wide range of SOTA models (e.g., 81.9% $\rightarrow$ 64.5% in BLIP, 66.1% $\rightarrow$ 37.5% in VSE$\infty$). We expect that our findings can provide insights for improving the robustness of the vision-language models and devising more diverse stress-te
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#21644;&#24635;&#32467;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#30340;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;ICL&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#26032;&#33539;&#24335;&#65292;&#25506;&#32034;ICL&#20197;&#35780;&#20272;&#21644;&#25512;&#24191;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#33021;&#21147;&#24050;&#25104;&#20026;&#19968;&#31181;&#26032;&#36235;&#21183;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ICL&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#24635;&#32467;&#20102;&#39640;&#32423;&#25216;&#26415;&#65292;&#26368;&#21518;&#35752;&#35770;&#20102;ICL&#30340;&#25361;&#25112;&#20197;&#21450;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2301.00234</link><description>&lt;p&gt;
&#20851;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on In-context Learning. (arXiv:2301.00234v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#21644;&#24635;&#32467;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#30340;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;ICL&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#26032;&#33539;&#24335;&#65292;&#25506;&#32034;ICL&#20197;&#35780;&#20272;&#21644;&#25512;&#24191;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#33021;&#21147;&#24050;&#25104;&#20026;&#19968;&#31181;&#26032;&#36235;&#21183;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ICL&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#24635;&#32467;&#20102;&#39640;&#32423;&#25216;&#26415;&#65292;&#26368;&#21518;&#35752;&#35770;&#20102;ICL&#30340;&#25361;&#25112;&#20197;&#21450;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#19981;&#26029;&#22686;&#24378;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#26032;&#33539;&#24335;&#65292;&#22312;&#20854;&#20013;LLM&#20165;&#22522;&#20110;&#21152;&#20837;&#23569;&#37327;&#31034;&#20363;&#30340;&#19978;&#19979;&#25991;&#36827;&#34892;&#39044;&#27979;&#12290;&#25506;&#32034;ICL&#20197;&#35780;&#20272;&#21644;&#25512;&#24191;LLM&#30340;&#33021;&#21147;&#24050;&#25104;&#20026;&#19968;&#31181;&#26032;&#36235;&#21183;&#12290;&#26412;&#25991;&#26088;&#22312;&#35843;&#26597;&#21644;&#24635;&#32467;ICL&#30340;&#36827;&#23637;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;ICL&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#28548;&#28165;&#20854;&#19982;&#30456;&#20851;&#30740;&#31350;&#30340;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32452;&#32455;&#21644;&#35752;&#35770;&#39640;&#32423;&#25216;&#26415;&#65292;&#21253;&#25324;&#35757;&#32451;&#31574;&#30053;&#12289;&#28436;&#31034;&#35774;&#35745;&#31574;&#30053;&#20197;&#21450;&#30456;&#20851;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;ICL&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#21487;&#20197;&#40723;&#21169;&#26356;&#22810;&#30340;&#30740;&#31350;&#65292;&#25581;&#31034;ICL&#30340;&#24037;&#20316;&#21407;&#29702;&#24182;&#25913;&#36827;ICL&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing ability of large language models (LLMs), in-context learning (ICL) has become a new paradigm for natural language processing (NLP), where LLMs make predictions only based on contexts augmented with a few examples. It has been a new trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, demonstration designing strategies, as well as related analysis. Finally, we discuss the challenges of ICL and provide potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL.
&lt;/p&gt;</description></item></channel></rss>