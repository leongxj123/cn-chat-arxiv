<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#20102;&#20010;&#24615;&#21270;&#19987;&#23478;&#31034;&#33539;&#30340;&#27010;&#24565;&#65292;&#20026;&#27599;&#20010;&#26234;&#20307;&#25110;&#19981;&#21516;&#31867;&#22411;&#30340;&#26234;&#20307;&#25552;&#20379;&#38024;&#23545;&#20010;&#20154;&#30446;&#26631;&#30340;&#25351;&#23548;&#65292;&#35299;&#20915;&#20102;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#32852;&#21512;&#31034;&#33539;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.08936</link><description>&lt;p&gt;
&#36229;&#36234;&#32852;&#21512;&#31034;&#33539;&#65306;&#20010;&#24615;&#21270;&#19987;&#23478;&#25351;&#23548;&#29992;&#20110;&#39640;&#25928;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Beyond Joint Demonstrations: Personalized Expert Guidance for Efficient Multi-Agent Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08936
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20010;&#24615;&#21270;&#19987;&#23478;&#31034;&#33539;&#30340;&#27010;&#24565;&#65292;&#20026;&#27599;&#20010;&#26234;&#20307;&#25110;&#19981;&#21516;&#31867;&#22411;&#30340;&#26234;&#20307;&#25552;&#20379;&#38024;&#23545;&#20010;&#20154;&#30446;&#26631;&#30340;&#25351;&#23548;&#65292;&#35299;&#20915;&#20102;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#32852;&#21512;&#31034;&#33539;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#38754;&#20020;&#26377;&#25928;&#25506;&#32034;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#32852;&#21512;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#30340;&#22823;&#23567;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#34429;&#28982;&#31034;&#33539;&#24341;&#23548;&#23398;&#20064;&#22312;&#21333;&#26234;&#20307;&#29615;&#22659;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#30410;&#30340;&#65292;&#20294;&#20854;&#30452;&#25509;&#24212;&#29992;&#20110;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#21463;&#21040;&#33719;&#24471;&#32852;&#21512;&#19987;&#23478;&#31034;&#33539;&#30340;&#23454;&#38469;&#22256;&#38590;&#30340;&#38459;&#30861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20010;&#24615;&#21270;&#19987;&#23478;&#31034;&#33539;&#30340;&#26032;&#27010;&#24565;&#65292;&#38024;&#23545;&#27599;&#20010;&#21333;&#20010;&#26234;&#20307;&#25110;&#26356;&#24191;&#27867;&#22320;&#35828;&#65292;&#22242;&#38431;&#20013;&#27599;&#31181;&#31867;&#22411;&#30340;&#26234;&#20307;&#36827;&#34892;&#20102;&#23450;&#21046;&#12290;&#36825;&#20123;&#31034;&#33539;&#20165;&#28041;&#21450;&#21333;&#26234;&#20307;&#34892;&#20026;&#20197;&#21450;&#27599;&#20010;&#26234;&#20307;&#22914;&#20309;&#23454;&#29616;&#20010;&#20154;&#30446;&#26631;&#65292;&#32780;&#19981;&#28041;&#21450;&#20219;&#20309;&#21512;&#20316;&#20803;&#32032;&#65292;&#22240;&#27492;&#30450;&#30446;&#27169;&#20223;&#23427;&#20204;&#19981;&#20250;&#23454;&#29616;&#21512;&#20316;&#30001;&#20110;&#28508;&#22312;&#20914;&#31361;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36873;&#25321;&#24615;&#22320;&#21033;&#29992;&#20010;&#24615;&#21270;&#19987;&#23478;&#31034;&#33539;&#20316;&#20026;&#25351;&#23548;&#65292;&#24182;&#20801;&#35768;&#26234;&#20307;&#23398;&#20064;&#21327;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08936v1 Announce Type: cross  Abstract: Multi-Agent Reinforcement Learning (MARL) algorithms face the challenge of efficient exploration due to the exponential increase in the size of the joint state-action space. While demonstration-guided learning has proven beneficial in single-agent settings, its direct applicability to MARL is hindered by the practical difficulty of obtaining joint expert demonstrations. In this work, we introduce a novel concept of personalized expert demonstrations, tailored for each individual agent or, more broadly, each individual type of agent within a heterogeneous team. These demonstrations solely pertain to single-agent behaviors and how each agent can achieve personal goals without encompassing any cooperative elements, thus naively imitating them will not achieve cooperation due to potential conflicts. To this end, we propose an approach that selectively utilizes personalized expert demonstrations as guidance and allows agents to learn to coo
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#30340;&#25193;&#23637;&#8212;&#8212;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;Neural SDEs&#65289;&#22312;&#22788;&#29702;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#25552;&#20986;&#20102;&#37325;&#35201;&#25351;&#23548;&#65292;&#38656;&#35201;&#35880;&#24910;&#35774;&#35745;&#28418;&#31227;&#21644;&#25193;&#25955;&#20989;&#25968;&#20197;&#20445;&#25345;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14989</link><description>&lt;p&gt;
&#20998;&#26512;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#31283;&#23450;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Stable Neural Stochastic Differential Equations in Analyzing Irregular Time Series Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14989
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#30340;&#25193;&#23637;&#8212;&#8212;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;Neural SDEs&#65289;&#22312;&#22788;&#29702;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#25552;&#20986;&#20102;&#37325;&#35201;&#25351;&#23548;&#65292;&#38656;&#35201;&#35880;&#24910;&#35774;&#35745;&#28418;&#31227;&#21644;&#25193;&#25955;&#20989;&#25968;&#20197;&#20445;&#25345;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#19981;&#35268;&#21017;&#37319;&#26679;&#38388;&#38548;&#21644;&#32570;&#22833;&#20540;&#23545;&#20110;&#20551;&#35774;&#19968;&#33268;&#38388;&#38548;&#21644;&#23436;&#25972;&#25968;&#25454;&#30340;&#20256;&#32479;&#26041;&#27861;&#26500;&#25104;&#25361;&#25112;&#12290;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#19982;&#24120;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#32467;&#21512;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#21521;&#37327;&#22330;&#23398;&#20064;&#36830;&#32493;&#28508;&#22312;&#34920;&#31034;&#12290;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;Neural SDEs&#65289;&#36890;&#36807;&#24341;&#20837;&#25193;&#25955;&#39033;&#25193;&#23637;&#20102;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65292;&#28982;&#32780;&#22312;&#22788;&#29702;&#19981;&#35268;&#21017;&#38388;&#38548;&#21644;&#32570;&#22833;&#20540;&#26102;&#65292;&#36825;&#31181;&#28155;&#21152;&#24182;&#19981;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#12290;&#22240;&#27492;&#65292;&#20180;&#32454;&#35774;&#35745;&#28418;&#31227;&#21644;&#25193;&#25955;&#20989;&#25968;&#23545;&#20110;&#20445;&#25345;&#31283;&#23450;&#24615;&#21644;&#22686;&#24378;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#31895;&#24515;&#30340;&#36873;&#25321;&#21487;&#33021;&#23548;&#33268;&#20986;&#29616;&#27809;&#26377;&#24378;&#35299;&#12289;&#38543;&#26426;&#30772;&#22351;&#25110;&#19981;&#31283;&#23450;&#30340;Euler&#31163;&#25955;&#21270;&#31561;&#19981;&#21033;&#30340;&#24615;&#36136;&#65292;&#26174;&#33879;&#24433;&#21709;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14989v1 Announce Type: cross  Abstract: Irregular sampling intervals and missing values in real-world time series data present challenges for conventional methods that assume consistent intervals and complete data. Neural Ordinary Differential Equations (Neural ODEs) offer an alternative approach, utilizing neural networks combined with ODE solvers to learn continuous latent representations through parameterized vector fields. Neural Stochastic Differential Equations (Neural SDEs) extend Neural ODEs by incorporating a diffusion term, although this addition is not trivial, particularly when addressing irregular intervals and missing values. Consequently, careful design of drift and diffusion functions is crucial for maintaining stability and enhancing performance, while incautious choices can result in adverse properties such as the absence of strong solutions, stochastic destabilization, or unstable Euler discretizations, significantly affecting Neural SDEs' performance. In 
&lt;/p&gt;</description></item><item><title>AdaFlow&#26159;&#19968;&#20010;&#22522;&#20110;&#27969;&#27169;&#22411;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21464;&#24322;&#33258;&#36866;&#24212;ODE&#27714;&#35299;&#22120;&#65292;&#22312;&#20445;&#25345;&#22810;&#26679;&#24615;&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#24555;&#36895;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.04292</link><description>&lt;p&gt;
AdaFlow: &#21464;&#24322;&#33258;&#36866;&#24212;&#27969;&#31574;&#30053;&#30340;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AdaFlow: Imitation Learning with Variance-Adaptive Flow-Based Policies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04292
&lt;/p&gt;
&lt;p&gt;
AdaFlow&#26159;&#19968;&#20010;&#22522;&#20110;&#27969;&#27169;&#22411;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21464;&#24322;&#33258;&#36866;&#24212;ODE&#27714;&#35299;&#22120;&#65292;&#22312;&#20445;&#25345;&#22810;&#26679;&#24615;&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#24555;&#36895;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#20223;&#23398;&#20064;&#22312;&#22810;&#27169;&#24577;&#20915;&#31574;&#20013;&#25913;&#36827;&#20102;&#34892;&#20026;&#20811;&#38534;&#65288;BC&#65289;&#65292;&#20294;&#30001;&#20110;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#36882;&#24402;&#32780;&#23548;&#33268;&#25512;&#29702;&#36895;&#24230;&#26174;&#33879;&#20943;&#24930;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#35774;&#35745;&#39640;&#25928;&#30340;&#31574;&#30053;&#29983;&#25104;&#22120;&#65292;&#21516;&#26102;&#20445;&#25345;&#29983;&#25104;&#22810;&#26679;&#21270;&#21160;&#20316;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AdaFlow&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#27969;&#27169;&#22411;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#12290;AdaFlow&#20351;&#29992;&#29366;&#24577;&#26465;&#20214;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#34920;&#31034;&#31574;&#30053;&#65292;&#36825;&#34987;&#31216;&#20026;&#27010;&#29575;&#27969;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#23427;&#20204;&#35757;&#32451;&#25439;&#22833;&#30340;&#26465;&#20214;&#26041;&#24046;&#19982;ODE&#30340;&#31163;&#25955;&#21270;&#35823;&#24046;&#20043;&#38388;&#30340;&#26377;&#36259;&#20851;&#31995;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21464;&#24322;&#33258;&#36866;&#24212;ODE&#27714;&#35299;&#22120;&#65292;&#22312;&#25512;&#29702;&#38454;&#27573;&#21487;&#20197;&#35843;&#25972;&#27493;&#38271;&#65292;&#20351;AdaFlow&#25104;&#20026;&#19968;&#20010;&#33258;&#36866;&#24212;&#20915;&#31574;&#32773;&#65292;&#33021;&#22815;&#24555;&#36895;&#25512;&#29702;&#32780;&#19981;&#29306;&#29298;&#22810;&#26679;&#24615;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#24403;&#21160;&#20316;&#20998;&#24067;&#34987;&#38477;&#20302;&#21040;&#19968;&#27493;&#29983;&#25104;&#22120;&#26102;&#65292;&#23427;&#33258;&#21160;&#36864;&#21270;&#21040;&#19968;&#20010;&#19968;&#27493;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based imitation learning improves Behavioral Cloning (BC) on multi-modal decision-making, but comes at the cost of significantly slower inference due to the recursion in the diffusion process. It urges us to design efficient policy generators while keeping the ability to generate diverse actions. To address this challenge, we propose AdaFlow, an imitation learning framework based on flow-based generative modeling. AdaFlow represents the policy with state-conditioned ordinary differential equations (ODEs), which are known as probability flows. We reveal an intriguing connection between the conditional variance of their training loss and the discretization error of the ODEs. With this insight, we propose a variance-adaptive ODE solver that can adjust its step size in the inference stage, making AdaFlow an adaptive decision-maker, offering rapid inference without sacrificing diversity. Interestingly, it automatically reduces to a one-step generator when the action distribution i
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#20449;&#30340;&#21327;&#21516;&#24863;&#30693;&#26694;&#26550;ACC-DA&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#36890;&#20449;&#22270;&#21644;&#33258;&#36866;&#24212;&#25968;&#25454;&#37325;&#26500;&#26426;&#21046;&#26469;&#22686;&#24378;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.00013</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#39550;&#39542;&#20013;&#22522;&#20110;&#39046;&#22495;&#21305;&#37197;&#30340;&#21327;&#21516;&#24863;&#30693;&#30340;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Adaptive Communications in Collaborative Perception with Domain Alignment for Autonomous Driving. (arXiv:2310.00013v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00013
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#20449;&#30340;&#21327;&#21516;&#24863;&#30693;&#26694;&#26550;ACC-DA&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#36890;&#20449;&#22270;&#21644;&#33258;&#36866;&#24212;&#25968;&#25454;&#37325;&#26500;&#26426;&#21046;&#26469;&#22686;&#24378;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36890;&#20449;&#20801;&#35768;&#36710;&#36742;&#20132;&#25442;&#34917;&#20805;&#20449;&#24687;&#65292;&#22810;&#20010;&#36830;&#25509;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20043;&#38388;&#30340;&#21327;&#21516;&#24863;&#30693;&#21487;&#20197;&#26497;&#22823;&#22320;&#22686;&#24378;&#24863;&#30693;&#33021;&#21147;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#36890;&#36947;&#21464;&#21270;&#21644;&#21327;&#21516;&#36710;&#36742;&#20043;&#38388;&#30340;&#25968;&#25454;&#24322;&#26500;&#24615;&#65292;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ACC-DA&#65292;&#19968;&#20010;&#36890;&#36947;&#24863;&#30693;&#30340;&#21327;&#21516;&#24863;&#30693;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#21160;&#24577;&#35843;&#25972;&#36890;&#20449;&#22270;&#24182;&#26368;&#23567;&#21270;&#24179;&#22343;&#20256;&#36755;&#24310;&#36831;&#65292;&#21516;&#26102;&#20943;&#36731;&#25968;&#25454;&#24322;&#26500;&#24615;&#24102;&#26469;&#30340;&#21103;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#28857;&#21253;&#25324;&#19977;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26368;&#23567;&#21270;&#20256;&#36755;&#24310;&#36831;&#30340;&#26041;&#27861;&#65292;&#26681;&#25454;&#19981;&#21516;&#30340;&#36890;&#36947;&#20449;&#24687;&#29366;&#24577;&#26500;&#24314;&#36890;&#20449;&#22270;&#24182;&#26368;&#23567;&#21270;&#20256;&#36755;&#24310;&#36831;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25968;&#25454;&#37325;&#26500;&#26426;&#21046;&#65292;&#21487;&#20197;&#21160;&#24577;&#35843;&#25972;&#30721;&#29575;-&#30072;&#21464;&#25240;&#34935;&#20197;&#22686;&#24378;&#24863;&#30693;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#23427;&#26368;&#23567;&#21270;&#20102;&#26102;&#22495;&#20002;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative perception among multiple connected and autonomous vehicles can greatly enhance perceptive capabilities by allowing vehicles to exchange supplementary information via communications. Despite advances in previous approaches, challenges still remain due to channel variations and data heterogeneity among collaborative vehicles. To address these issues, we propose ACC-DA, a channel-aware collaborative perception framework to dynamically adjust the communication graph and minimize the average transmission delay while mitigating the side effects from the data heterogeneity. Our novelties lie in three aspects. We first design a transmission delay minimization method, which can construct the communication graph and minimize the transmission delay according to different channel information state. We then propose an adaptive data reconstruction mechanism, which can dynamically adjust the rate-distortion trade-off to enhance perception efficiency. Moreover, it minimizes the temporal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#25104;&#20687;&#39046;&#22495;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;NeuroGraph&#65292;&#24182;&#25506;&#35752;&#20102;&#25968;&#25454;&#38598;&#29983;&#25104;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2306.06202</link><description>&lt;p&gt;
NeuroGraph:&#38754;&#21521;&#33041;&#36830;&#25509;&#32452;&#23398;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
NeuroGraph: Benchmarks for Graph Machine Learning in Brain Connectomics. (arXiv:2306.06202v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#25104;&#20687;&#39046;&#22495;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;NeuroGraph&#65292;&#24182;&#25506;&#35752;&#20102;&#25968;&#25454;&#38598;&#29983;&#25104;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20026;&#20998;&#26512;&#39640;&#32500;&#21151;&#33021;&#24615;&#31070;&#32463;&#25104;&#20687;&#25968;&#25454;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#24050;&#34987;&#35777;&#26126;&#23545;&#39044;&#27979;&#21508;&#31181;&#31070;&#32463;&#30142;&#30149;&#12289;&#31934;&#31070;&#38556;&#30861;&#21644;&#35748;&#30693;&#27169;&#24335;&#26377;&#25928;&#12290;&#22312;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#30740;&#31350;&#20013;&#65292;&#22823;&#33041;&#21306;&#22495;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#36890;&#24120;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#36827;&#34892;&#24314;&#27169;&#12290;&#22270;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#24050;&#22312;&#22810;&#20010;&#39046;&#22495;&#24471;&#21040;&#35777;&#23454;&#65292;&#26631;&#24535;&#30528;&#25968;&#25454;&#35299;&#37322;&#21644;&#39044;&#27979;&#24314;&#27169;&#20013;&#30340;&#19968;&#20010;&#36716;&#21464;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#21069;&#26223;&#65292;&#20294;&#30001;&#20110;&#22270;&#24418;&#25968;&#25454;&#38598;&#26500;&#24314;&#30340;&#24191;&#27867;&#39044;&#22788;&#29702;&#27969;&#27700;&#32447;&#21644;&#22823;&#21442;&#25968;&#25628;&#32034;&#31354;&#38388;&#65292;&#22312;&#31070;&#32463;&#25104;&#20687;&#39046;&#22495;&#20013;&#24212;&#29992;&#36825;&#20123;&#25216;&#26415;&#30340;&#36716;&#25442;&#20173;&#28982;&#21463;&#21040;&#24847;&#22806;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;NeuroGraph(&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#31070;&#32463;&#25104;&#20687;&#25968;&#25454;&#38598;)&#65292;&#23427;&#28085;&#30422;&#20102;&#22810;&#20010;&#34892;&#20026;&#21644;&#35748;&#30693;&#29305;&#24449;&#31867;&#21035;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#25968;&#25454;&#38598;&#29983;&#25104;&#25628;&#32034;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Machine learning provides a valuable tool for analyzing high-dimensional functional neuroimaging data, and is proving effective in predicting various neurological conditions, psychiatric disorders, and cognitive patterns. In functional Magnetic Resonance Imaging (MRI) research, interactions between brain regions are commonly modeled using graph-based representations. The potency of graph machine learning methods has been established across myriad domains, marking a transformative step in data interpretation and predictive modeling. Yet, despite their promise, the transposition of these techniques to the neuroimaging domain remains surprisingly under-explored due to the expansive preprocessing pipeline and large parameter search space for graph-based datasets construction. In this paper, we introduce NeuroGraph, a collection of graph-based neuroimaging datasets that span multiple categories of behavioral and cognitive traits. We delve deeply into the dataset generation search space by c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;V-GLOSS&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#30693;&#35782;&#24211;&#29983;&#25104;&#20855;&#26377;&#35270;&#35273;&#22522;&#30784;&#30340;&#31867;&#21035;&#25551;&#36848;&#65292;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#24102;&#26377;&#31867;&#21035;&#25551;&#36848;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.06077</link><description>&lt;p&gt;
&#35270;&#35273;&#35789;&#27719;&#25551;&#36848;&#25552;&#21319;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Visually-Grounded Descriptions Improve Zero-Shot Image Classification. (arXiv:2306.06077v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;V-GLOSS&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#30693;&#35782;&#24211;&#29983;&#25104;&#20855;&#26377;&#35270;&#35273;&#22522;&#30784;&#30340;&#31867;&#21035;&#25551;&#36848;&#65292;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#24102;&#26377;&#31867;&#21035;&#25551;&#36848;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#35270;&#35273;&#27169;&#22411;&#22914;CLIP&#22312;&#38646;&#26679;&#26412;&#35270;&#35273;&#20219;&#21153;&#65288;&#20363;&#22914;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;ZSIC&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#20855;&#20307;&#21644;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#31867;&#21035;&#25551;&#36848;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#31890;&#24230;&#21644;&#26631;&#31614;&#27495;&#20041;&#31561;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;V-GLOSS&#65306;Visual Glosses&#65292;&#23427;&#21033;&#29992;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#30693;&#35782;&#24211;&#26469;&#29983;&#25104;&#20855;&#26377;&#35270;&#35273;&#22522;&#30784;&#30340;&#31867;&#21035;&#25551;&#36848;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22522;&#20934;ZSIC&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;ImageNet&#21644;STL-10&#65289;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#26469;&#23637;&#31034;V-GLOSS&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#30001;V-GLOSS&#29983;&#25104;&#30340;&#24102;&#26377;&#31867;&#21035;&#25551;&#36848;&#30340;&#38134;&#26631;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20854;&#29992;&#20110;&#35270;&#35273;&#20219;&#21153;&#30340;&#26377;&#29992;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#28304;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language-vision models like CLIP have made significant progress in zero-shot vision tasks, such as zero-shot image classification (ZSIC). However, generating specific and expressive class descriptions remains a major challenge. Existing approaches suffer from granularity and label ambiguity issues. To tackle these challenges, we propose V-GLOSS: Visual Glosses, a novel method leveraging modern language models and semantic knowledge bases to produce visually-grounded class descriptions. We demonstrate V-GLOSS's effectiveness by achieving state-of-the-art results on benchmark ZSIC datasets including ImageNet and STL-10. In addition, we introduce a silver dataset with class descriptions generated by V-GLOSS, and show its usefulness for vision tasks. We make available our code and dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#27169;&#22411;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#30784;&#30340;&#22823;&#22411;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#36890;&#36807;&#25972;&#21512;&#20020;&#24202;&#19987;&#19994;&#30693;&#35782;&#12289;&#39046;&#22495;&#30693;&#35782;&#21644;&#22810;&#27169;&#24577;&#33021;&#21147;&#26469;&#24320;&#21457;&#21644;&#37096;&#32626;AGI&#33021;&#22815;&#35299;&#20915;&#21307;&#23398;&#39046;&#22495;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.05480</link><description>&lt;p&gt;
&#21307;&#23398;&#25104;&#20687;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Artificial General Intelligence for Medical Imaging. (arXiv:2306.05480v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#27169;&#22411;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#30784;&#30340;&#22823;&#22411;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#36890;&#36807;&#25972;&#21512;&#20020;&#24202;&#19987;&#19994;&#30693;&#35782;&#12289;&#39046;&#22495;&#30693;&#35782;&#21644;&#22810;&#27169;&#24577;&#33021;&#21147;&#26469;&#24320;&#21457;&#21644;&#37096;&#32626;AGI&#33021;&#22815;&#35299;&#20915;&#21307;&#23398;&#39046;&#22495;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#27169;&#22411;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#30784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#22823;&#22411;&#35270;&#35273;&#27169;&#22411;&#21644;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#25105;&#20204;&#24378;&#35843;&#23558;&#20020;&#24202;&#19987;&#19994;&#30693;&#35782;&#12289;&#39046;&#22495;&#30693;&#35782;&#21644;&#22810;&#27169;&#24577;&#33021;&#21147;&#25972;&#21512;&#21040;AGI&#27169;&#22411;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25351;&#23548;&#21307;&#30103;&#20445;&#20581;AGI&#27169;&#22411;&#24320;&#21457;&#21644;&#37096;&#32626;&#30340;&#36335;&#32447;&#22270;&#12290;&#22312;&#25972;&#20010;&#32508;&#36848;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#22312;&#21307;&#23398;&#39046;&#22495;&#37096;&#32626;&#22823;&#35268;&#27169;AGI&#27169;&#22411;&#21487;&#33021;&#38754;&#20020;&#30340;&#28508;&#22312;&#25361;&#25112;&#21644;&#32570;&#38519;&#30340;&#20851;&#38190;&#35266;&#28857;&#12290;&#36825;&#31687;&#32508;&#21512;&#24615;&#30340;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#26377;&#20851;AGI&#23545;&#21307;&#23398;&#25104;&#20687;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#20854;&#20182;&#39046;&#22495;&#26410;&#26469;&#24433;&#21709;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this review, we explore the potential applications of Artificial General Intelligence (AGI) models in healthcare, focusing on foundational Large Language Models (LLMs), Large Vision Models, and Large Multimodal Models. We emphasize the importance of integrating clinical expertise, domain knowledge, and multimodal capabilities into AGI models. In addition, we lay out key roadmaps that guide the development and deployment of healthcare AGI models. Throughout the review, we provide critical perspectives on the potential challenges and pitfalls associated with deploying large-scale AGI models in the medical field. This comprehensive review aims to offer insights into the future implications of AGI in medical imaging, healthcare and beyond.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35745;&#31639;&#21644;&#20998;&#26512;ReLU&#32593;&#32476;&#22810;&#38754;&#20307;&#30340;&#21333;&#32431;&#24418;&#30452;&#26041;&#22270;&#65292;&#21457;&#29616;&#22312;&#21021;&#22987;&#21270;&#21644;&#26799;&#24230;&#19979;&#38477;&#26102;&#23427;&#20204;&#32467;&#26500;&#30456;&#23545;&#31616;&#21333;&#65292;&#36825;&#35828;&#26126;&#20102;&#19968;&#31181;&#26032;&#30340;&#38544;&#24335;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2305.09145</link><description>&lt;p&gt;
&#28145;&#23618;ReLU&#32593;&#32476;&#30340;&#22810;&#38754;&#20307;&#24322;&#24120;&#31616;&#21333;
&lt;/p&gt;
&lt;p&gt;
Deep ReLU Networks Have Surprisingly Simple Polytopes. (arXiv:2305.09145v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35745;&#31639;&#21644;&#20998;&#26512;ReLU&#32593;&#32476;&#22810;&#38754;&#20307;&#30340;&#21333;&#32431;&#24418;&#30452;&#26041;&#22270;&#65292;&#21457;&#29616;&#22312;&#21021;&#22987;&#21270;&#21644;&#26799;&#24230;&#19979;&#38477;&#26102;&#23427;&#20204;&#32467;&#26500;&#30456;&#23545;&#31616;&#21333;&#65292;&#36825;&#35828;&#26126;&#20102;&#19968;&#31181;&#26032;&#30340;&#38544;&#24335;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ReLU&#32593;&#32476;&#26159;&#19968;&#31181;&#22810;&#38754;&#20307;&#19978;&#30340;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#12290;&#30740;&#31350;&#36825;&#31181;&#22810;&#38754;&#20307;&#30340;&#24615;&#36136;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#65292;&#23545;&#20110;&#22810;&#38754;&#20307;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#20165;&#20572;&#30041;&#22312;&#35745;&#31639;&#25968;&#37327;&#30340;&#27700;&#24179;&#65292;&#36825;&#36828;&#36828;&#19981;&#33021;&#23436;&#25972;&#22320;&#25551;&#36848;&#22810;&#38754;&#20307;&#12290;&#20026;&#20102;&#23558;&#29305;&#24449;&#25552;&#21319;&#21040;&#19968;&#20010;&#26032;&#30340;&#27700;&#24179;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#19977;&#35282;&#21078;&#20998;&#22810;&#38754;&#20307;&#24471;&#20986;&#22810;&#38754;&#20307;&#30340;&#24418;&#29366;&#12290;&#36890;&#36807;&#35745;&#31639;&#21644;&#20998;&#26512;&#19981;&#21516;&#22810;&#38754;&#20307;&#30340;&#21333;&#32431;&#24418;&#30452;&#26041;&#22270;&#65292;&#25105;&#20204;&#21457;&#29616;ReLU&#32593;&#32476;&#22312;&#21021;&#22987;&#21270;&#21644;&#26799;&#24230;&#19979;&#38477;&#26102;&#20855;&#26377;&#30456;&#23545;&#31616;&#21333;&#30340;&#22810;&#38754;&#20307;&#32467;&#26500;&#65292;&#23613;&#31649;&#36825;&#20123;&#22810;&#38754;&#20307;&#20174;&#29702;&#35770;&#19978;&#26469;&#35828;&#21487;&#20197;&#38750;&#24120;&#20016;&#23500;&#21644;&#22797;&#26434;&#12290;&#36825;&#19968;&#21457;&#29616;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#26032;&#30340;&#38544;&#24335;&#20559;&#35265;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#38750;&#24179;&#20961;&#30340;&#32452;&#21512;&#25512;&#23548;&#26469;&#29702;&#35770;&#19978;&#35299;&#37322;&#20026;&#20160;&#20040;&#22686;&#21152;&#28145;&#24230;&#19981;&#20250;&#21019;&#24314;&#26356;&#22797;&#26434;&#30340;&#22810;&#38754;&#20307;&#65292;&#36890;&#36807;&#38480;&#21046;&#27599;&#20010;&#32500;&#24230;&#30340;&#24179;&#22343;&#21333;&#32431;&#24418;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
A ReLU network is a piecewise linear function over polytopes. Figuring out the properties of such polytopes is of fundamental importance for the research and development of neural networks. So far, either theoretical or empirical studies on polytopes only stay at the level of counting their number, which is far from a complete characterization of polytopes. To upgrade the characterization to a new level, here we propose to study the shapes of polytopes via the number of simplices obtained by triangulating the polytope. Then, by computing and analyzing the histogram of simplices across polytopes, we find that a ReLU network has relatively simple polytopes under both initialization and gradient descent, although these polytopes theoretically can be rather diverse and complicated. This finding can be appreciated as a novel implicit bias. Next, we use nontrivial combinatorial derivation to theoretically explain why adding depth does not create a more complicated polytope by bounding the av
&lt;/p&gt;</description></item><item><title>NeuroBench&#26159;&#30001;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#25104;&#21592;&#20849;&#21516;&#24320;&#21457;&#30340;&#19968;&#22871;&#21327;&#20316;&#12289;&#20844;&#24179;&#21644;&#20195;&#34920;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21487;&#20197;&#35299;&#20915;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#20013;&#32570;&#20047;&#28165;&#26224;&#26631;&#20934;&#30340;&#38382;&#39064;&#65292;&#25512;&#21160;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2304.04640</link><description>&lt;p&gt;
NeuroBench&#65306;&#36890;&#36807;&#21512;&#20316;&#12289;&#20844;&#24179;&#21644;&#20195;&#34920;&#24615;&#22522;&#20934;&#27979;&#35797;&#25512;&#36827;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
NeuroBench: Advancing Neuromorphic Computing through Collaborative, Fair and Representative Benchmarking. (arXiv:2304.04640v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04640
&lt;/p&gt;
&lt;p&gt;
NeuroBench&#26159;&#30001;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#25104;&#21592;&#20849;&#21516;&#24320;&#21457;&#30340;&#19968;&#22871;&#21327;&#20316;&#12289;&#20844;&#24179;&#21644;&#20195;&#34920;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21487;&#20197;&#35299;&#20915;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#20013;&#32570;&#20047;&#28165;&#26224;&#26631;&#20934;&#30340;&#38382;&#39064;&#65292;&#25512;&#21160;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#39046;&#22495;&#22312;&#36981;&#24490;&#20223;&#29983;&#23398;&#21407;&#29702;&#30340;&#22522;&#30784;&#19978;&#65292;&#20855;&#26377;&#25512;&#36827;&#35745;&#31639;&#25928;&#29575;&#21644;&#33021;&#21147;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#24418;&#24577;&#30740;&#31350;&#20013;&#37319;&#29992;&#30340;&#25216;&#26415;&#22810;&#26679;&#24615;&#23548;&#33268;&#32570;&#20047;&#28165;&#26224;&#30340;&#22522;&#20934;&#27979;&#35797;&#26631;&#20934;&#65292;&#38459;&#30861;&#20102;&#23545;&#31070;&#32463;&#24418;&#24577;&#26041;&#27861;&#19982;&#20256;&#32479;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#30340;&#20248;&#21155;&#21183;&#36827;&#34892;&#26377;&#25928;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21327;&#20316;&#39033;&#30446;&#8212;&#8212;NeuroBench&#65292;&#23558;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#25104;&#21592;&#32858;&#38598;&#36215;&#26469;&#20026;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#23450;&#20041;&#22522;&#20934;&#27979;&#35797;&#12290;NeuroBench&#30340;&#30446;&#26631;&#26159;&#25104;&#20026;&#31038;&#21306;&#24320;&#21457;&#30340;&#21327;&#20316;&#12289;&#20844;&#24179;&#21644;&#20195;&#34920;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#22522;&#20934;&#27979;&#35797;&#31070;&#32463;&#24418;&#24577;&#35299;&#20915;&#26041;&#26696;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#27010;&#36848;&#20102;NeuroBench&#30340;&#20851;&#38190;&#29305;&#24615;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;NeuroBench&#23558;&#26159;&#23450;&#20041;&#33021;&#22815;&#32479;&#19968;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#30446;&#26631;&#30340;&#26631;&#20934;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of neuromorphic computing holds great promise in terms of advancing computing efficiency and capabilities by following brain-inspired principles. However, the rich diversity of techniques employed in neuromorphic research has resulted in a lack of clear standards for benchmarking, hindering effective evaluation of the advantages and strengths of neuromorphic methods compared to traditional deep-learning-based methods. This paper presents a collaborative effort, bringing together members from academia and the industry, to define benchmarks for neuromorphic computing: NeuroBench. The goals of NeuroBench are to be a collaborative, fair, and representative benchmark suite developed by the community, for the community. In this paper, we discuss the challenges associated with benchmarking neuromorphic solutions, and outline the key features of NeuroBench. We believe that NeuroBench will be a significant step towards defining standards that can unify the goals of neuromorphic comput
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;AI&#22686;&#24378;&#30340;&#37325;&#30151;&#30417;&#25252;&#23460;&#31995;&#32479;&#65292;&#36890;&#36807;&#26222;&#36866;&#24863;&#30693;&#21644;&#25968;&#25454;&#22788;&#29702;&#65292;&#21487;&#20197;&#25913;&#21892;&#24739;&#32773;&#30340;&#35270;&#35273;&#30417;&#27979;&#21644;&#35780;&#20272;&#65292;&#25552;&#39640;&#25252;&#29702;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.06252</link><description>&lt;p&gt;
AI&#22686;&#24378;&#30340;&#37325;&#30151;&#30417;&#25252;&#23460;&#65306;&#36890;&#36807;&#26222;&#36866;&#24863;&#30693;&#38761;&#21629;&#24615;&#22320;&#25913;&#21892;&#24739;&#32773;&#25252;&#29702;
&lt;/p&gt;
&lt;p&gt;
AI-Enhanced Intensive Care Unit: Revolutionizing Patient Care with Pervasive Sensing. (arXiv:2303.06252v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;AI&#22686;&#24378;&#30340;&#37325;&#30151;&#30417;&#25252;&#23460;&#31995;&#32479;&#65292;&#36890;&#36807;&#26222;&#36866;&#24863;&#30693;&#21644;&#25968;&#25454;&#22788;&#29702;&#65292;&#21487;&#20197;&#25913;&#21892;&#24739;&#32773;&#30340;&#35270;&#35273;&#30417;&#27979;&#21644;&#35780;&#20272;&#65292;&#25552;&#39640;&#25252;&#29702;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces an AI-enhanced ICU system that improves patient visual monitoring and assessment, and ultimately enhances the quality of care, through pervasive sensing and data processing.
&lt;/p&gt;
&lt;p&gt;
&#37325;&#30151;&#30417;&#25252;&#23460;&#65288;ICU&#65289;&#26159;&#19968;&#20010;&#19987;&#38376;&#30340;&#21307;&#38498;&#31354;&#38388;&#65292;&#29992;&#20110;&#25509;&#21463;&#21361;&#37325;&#30149;&#20154;&#30340;&#23494;&#38598;&#25252;&#29702;&#21644;&#30417;&#27979;&#12290;&#20840;&#38754;&#30340;&#30417;&#27979;&#23545;&#20110;&#35780;&#20272;&#24739;&#32773;&#30340;&#30149;&#24773;&#65292;&#29305;&#21035;&#26159;&#30149;&#24773;&#30340;&#20005;&#37325;&#31243;&#24230;&#20197;&#21450;&#26368;&#32456;&#30340;&#25252;&#29702;&#36136;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26102;&#38388;&#38480;&#21046;&#21644;&#21307;&#25252;&#20154;&#21592;&#30340;&#24037;&#20316;&#37327;&#65292;ICU&#20013;&#30340;&#24739;&#32773;&#30417;&#27979;&#33539;&#22260;&#21463;&#21040;&#38480;&#21046;&#12290;&#30446;&#21069;&#65292;&#21253;&#25324;&#38754;&#37096;&#34920;&#24773;&#12289;&#23039;&#21183;&#21644;&#27963;&#21160;&#33021;&#21147;&#31561;&#32454;&#33410;&#30340;&#35270;&#35273;&#35780;&#20272;&#20165;&#20598;&#23572;&#34987;&#25429;&#25417;&#21040;&#65292;&#25110;&#32773;&#26681;&#26412;&#27809;&#26377;&#34987;&#25429;&#25417;&#21040;&#12290;&#36825;&#20123;&#25163;&#21160;&#35266;&#23519;&#26159;&#20027;&#35266;&#30340;&#65292;&#23481;&#26131;&#20986;&#29616;&#25991;&#26723;&#38169;&#35823;&#65292;&#24182;&#32473;&#25252;&#29702;&#20154;&#21592;&#24102;&#26469;&#39069;&#22806;&#30340;&#24037;&#20316;&#37327;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#26377;&#28508;&#21147;&#22686;&#24378;&#24739;&#32773;&#30340;&#35270;&#35273;&#30417;&#27979;&#21644;&#35780;&#20272;&#12290;&#36825;&#26679;&#30340;&#31995;&#32479;&#38656;&#35201;&#24378;&#22823;&#30340;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#26222;&#36866;&#24863;&#30693;&#21644;&#25968;&#25454;&#22788;&#29702;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
The intensive care unit (ICU) is a specialized hospital space where critically ill patients receive intensive care and monitoring. Comprehensive monitoring is imperative in assessing patients conditions, in particular acuity, and ultimately the quality of care. However, the extent of patient monitoring in the ICU is limited due to time constraints and the workload on healthcare providers. Currently, visual assessments for acuity, including fine details such as facial expressions, posture, and mobility, are sporadically captured, or not captured at all. These manual observations are subjective to the individual, prone to documentation errors, and overburden care providers with the additional workload. Artificial Intelligence (AI) enabled systems has the potential to augment the patient visual monitoring and assessment due to their exceptional learning capabilities. Such systems require robust annotated data to train. To this end, we have developed pervasive sensing and data processing s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Huber&#33021;&#37327;&#37327;&#21270;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#30446;&#26631;&#27010;&#29575;&#23450;&#24459;&#30340;&#26368;&#20339;&#36924;&#36817;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#21407;&#27979;&#24230;&#19982;&#37327;&#21270;&#29256;&#26412;&#20043;&#38388;&#30340;&#32479;&#35745;&#36317;&#31163;&#26469;&#23454;&#29616;&#12290;&#35813;&#31639;&#27861;&#24050;&#22312;&#22810;&#32500;&#39640;&#26031;&#28151;&#21512;&#29289;&#12289;&#32500;&#32435;&#31354;&#38388;&#39764;&#26041;&#31561;&#20960;&#20010;&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2212.08162</link><description>&lt;p&gt;
Huber&#33021;&#37327;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Huber-energy measure quantization. (arXiv:2212.08162v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08162
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Huber&#33021;&#37327;&#37327;&#21270;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#30446;&#26631;&#27010;&#29575;&#23450;&#24459;&#30340;&#26368;&#20339;&#36924;&#36817;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#21407;&#27979;&#24230;&#19982;&#37327;&#21270;&#29256;&#26412;&#20043;&#38388;&#30340;&#32479;&#35745;&#36317;&#31163;&#26469;&#23454;&#29616;&#12290;&#35813;&#31639;&#27861;&#24050;&#22312;&#22810;&#32500;&#39640;&#26031;&#28151;&#21512;&#29289;&#12289;&#32500;&#32435;&#31354;&#38388;&#39764;&#26041;&#31561;&#20960;&#20010;&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#27979;&#37327;&#37327;&#21270;&#36807;&#31243;&#65292;&#21363;&#19968;&#31181;&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;$Q$&#20010;&#29380;&#25289;&#20811;&#20989;&#25968;&#30340;&#24635;&#21644;&#65288;$Q$&#20026;&#37327;&#21270;&#21442;&#25968;&#65289;&#65292;&#25214;&#21040;&#30446;&#26631;&#27010;&#29575;&#23450;&#24459;&#65288;&#26356;&#19968;&#33324;&#22320;&#65292;&#20026;&#26377;&#38480;&#21464;&#24046;&#27979;&#24230;&#65289;&#30340;&#26368;&#20339;&#36924;&#36817;&#12290;&#35813;&#36807;&#31243;&#36890;&#36807;&#23558;&#21407;&#27979;&#24230;&#19982;&#20854;&#37327;&#21270;&#29256;&#26412;&#20043;&#38388;&#30340;&#32479;&#35745;&#36317;&#31163;&#26368;&#23567;&#21270;&#26469;&#23454;&#29616;&#65307;&#35813;&#36317;&#31163;&#22522;&#20110;&#36127;&#23450;&#26680;&#26500;&#24314;&#65292;&#24182;&#19988;&#22914;&#26524;&#24517;&#35201;&#65292;&#21487;&#20197;&#23454;&#26102;&#35745;&#31639;&#24182;&#36755;&#20837;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#65288;&#22914;SGD&#65292;Adam&#31561;&#65289;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#26368;&#20248;&#27979;&#37327;&#37327;&#21270;&#22120;&#30340;&#23384;&#22312;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#24182;&#30830;&#23450;&#20102;&#38656;&#35201;&#20445;&#35777;&#21512;&#36866;&#34892;&#20026;&#30340;&#26680;&#23646;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26368;&#20339;&#32447;&#24615;&#26080;&#20559;&#65288;BLUE&#65289;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#24179;&#26041;&#32479;&#35745;&#36317;&#31163;&#65292;&#24182;&#23558;&#23427;&#20204;&#29992;&#20110;&#26080;&#20559;&#31243;&#24207;HEMQ&#20013;&#65292;&#20197;&#25214;&#21040;&#26368;&#20339;&#37327;&#21270;&#12290;&#25105;&#20204;&#22312;&#22810;&#32500;&#39640;&#26031;&#28151;&#21512;&#29289;&#12289;&#32500;&#32435;&#31354;&#38388;&#39764;&#26041;&#31561;&#20960;&#20010;&#25968;&#25454;&#24211;&#19978;&#27979;&#35797;&#20102;HEMQ
&lt;/p&gt;
&lt;p&gt;
We describe a measure quantization procedure i.e., an algorithm which finds the best approximation of a target probability law (and more generally signed finite variation measure) by a sum of $Q$ Dirac masses ($Q$ being the quantization parameter). The procedure is implemented by minimizing the statistical distance between the original measure and its quantized version; the distance is built from a negative definite kernel and, if necessary, can be computed on the fly and feed to a stochastic optimization algorithm (such as SGD, Adam, ...). We investigate theoretically the fundamental questions of existence of the optimal measure quantizer and identify what are the required kernel properties that guarantee suitable behavior. We propose two best linear unbiased (BLUE) estimators for the squared statistical distance and use them in an unbiased procedure, called HEMQ, to find the optimal quantization. We test HEMQ on several databases: multi-dimensional Gaussian mixtures, Wiener space cub
&lt;/p&gt;</description></item></channel></rss>