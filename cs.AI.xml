<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#23433;&#20840;&#39640;&#25928;&#22320;&#23398;&#20064;&#26426;&#22120;&#20154;&#39135;&#29289;&#20999;&#21106;&#20219;&#21153;&#30340;Dual Simulator&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2404.02569</link><description>&lt;p&gt;
SliceIt! -- &#19968;&#20010;&#29992;&#20110;&#23398;&#20064;&#26426;&#22120;&#20154;&#39135;&#29289;&#20999;&#21106;&#30340;&#21452;&#37325;&#27169;&#25311;&#22120;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SliceIt! -- A Dual Simulator Framework for Learning Robot Food Slicing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02569
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#23433;&#20840;&#39640;&#25928;&#22320;&#23398;&#20064;&#26426;&#22120;&#20154;&#39135;&#29289;&#20999;&#21106;&#20219;&#21153;&#30340;Dual Simulator&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21416;&#25151;&#26426;&#22120;&#20154;&#21487;&#20197;&#36890;&#36807;&#20943;&#36731;&#26085;&#24120;&#28902;&#29712;&#20219;&#21153;&#30340;&#36127;&#25285;&#65292;&#25552;&#39640;&#23478;&#24237;&#20307;&#39564;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#21361;&#38505;&#24037;&#20855;&#65288;&#22914;&#21416;&#25151;&#20992;&#20855;&#65289;&#26102;&#65292;&#36825;&#20123;&#26426;&#22120;&#20154;&#24517;&#39035;&#22312;&#20849;&#20139;&#20154;&#31867;&#29615;&#22659;&#20013;&#28789;&#24039;&#19988;&#23433;&#20840;&#22320;&#25191;&#34892;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#33258;&#20027;&#19988;&#23433;&#20840;&#22320;&#23398;&#20064;&#39135;&#29289;&#20999;&#21106;&#20219;&#21153;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#36866;&#24212;&#24615;&#25511;&#21046;&#65292;&#20351;&#29992;&#21327;&#20316;&#26426;&#22120;&#20154;&#25110;&#24037;&#19994;&#26426;&#22120;&#20154;&#25163;&#33218;&#25191;&#34892;&#39135;&#29289;&#20999;&#21106;&#20219;&#21153;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#26448;&#26009;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#35757;&#32451;&#26426;&#22120;&#20154;&#20197;&#21512;&#35268;&#26041;&#24335;&#25805;&#20316;&#20992;&#20855;&#65292;&#20943;&#23569;&#39135;&#29289;&#21644;&#20999;&#33756;&#26495;&#26045;&#21152;&#30340;&#25509;&#35302;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#35757;&#32451;&#26426;&#22120;&#20154;&#21487;&#33021;&#25928;&#29575;&#20302;&#19979;&#12289;&#21361;&#38505;&#65292;&#24182;&#23548;&#33268;&#22823;&#37327;&#39135;&#29289;&#28010;&#36153;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SliceIt!&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#23433;&#20840;&#39640;&#25928;&#22320;&#23398;&#20064;&#26426;&#22120;&#20154;&#39135;&#29289;&#20999;&#21106;&#20219;&#21153;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02569v1 Announce Type: cross  Abstract: Cooking robots can enhance the home experience by reducing the burden of daily chores. However, these robots must perform their tasks dexterously and safely in shared human environments, especially when handling dangerous tools such as kitchen knives. This study focuses on enabling a robot to autonomously and safely learn food-cutting tasks. More specifically, our goal is to enable a collaborative robot or industrial robot arm to perform food-slicing tasks by adapting to varying material properties using compliance control. Our approach involves using Reinforcement Learning (RL) to train a robot to compliantly manipulate a knife, by reducing the contact forces exerted by the food items and by the cutting board. However, training the robot in the real world can be inefficient, and dangerous, and result in a lot of food waste. Therefore, we proposed SliceIt!, a framework for safely and efficiently learning robot food-slicing tasks in sim
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#36951;&#24536;&#65288;MU&#65289;&#36890;&#36807;&#30693;&#35782;&#21435;&#38500;&#36807;&#31243;&#26469;&#35299;&#20915;&#35757;&#32451;&#25968;&#25454;&#30456;&#20851;&#30340;&#20154;&#24037;&#26234;&#33021;&#27835;&#29702;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;AI&#31995;&#32479;&#30340;&#23433;&#20840;&#21644;&#36127;&#36131;&#20219;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.13682</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#23041;&#32961;&#12289;&#25915;&#20987;&#21644;&#38450;&#24481;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Threats, Attacks, and Defenses in Machine Unlearning: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13682
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#65288;MU&#65289;&#36890;&#36807;&#30693;&#35782;&#21435;&#38500;&#36807;&#31243;&#26469;&#35299;&#20915;&#35757;&#32451;&#25968;&#25454;&#30456;&#20851;&#30340;&#20154;&#24037;&#26234;&#33021;&#27835;&#29702;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;AI&#31995;&#32479;&#30340;&#23433;&#20840;&#21644;&#36127;&#36131;&#20219;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#65288;MU&#65289;&#26368;&#36817;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#26377;&#28508;&#21147;&#36890;&#36807;&#20174;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#28040;&#38500;&#29305;&#23450;&#25968;&#25454;&#30340;&#24433;&#21709;&#26469;&#23454;&#29616;&#23433;&#20840;&#20154;&#24037;&#26234;&#33021;&#12290;&#36825;&#20010;&#34987;&#31216;&#20026;&#30693;&#35782;&#21435;&#38500;&#30340;&#36807;&#31243;&#35299;&#20915;&#20102;&#19982;&#35757;&#32451;&#25968;&#25454;&#30456;&#20851;&#30340;&#20154;&#24037;&#26234;&#33021;&#27835;&#29702;&#38382;&#39064;&#65292;&#22914;&#25968;&#25454;&#36136;&#37327;&#12289;&#25935;&#24863;&#24615;&#12289;&#29256;&#26435;&#38480;&#21046;&#21644;&#36807;&#26102;&#24615;&#12290;&#36825;&#31181;&#33021;&#21147;&#23545;&#20110;&#30830;&#20445;&#36981;&#23432;&#35832;&#22914;&#34987;&#36951;&#24536;&#26435;&#31561;&#38544;&#31169;&#27861;&#35268;&#20063;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#26377;&#25928;&#30340;&#30693;&#35782;&#21435;&#38500;&#26377;&#21161;&#20110;&#20943;&#36731;&#26377;&#23475;&#32467;&#26524;&#30340;&#39118;&#38505;&#65292;&#38450;&#33539;&#20559;&#35265;&#12289;&#35823;&#23548;&#21644;&#26410;&#32463;&#25480;&#26435;&#30340;&#25968;&#25454;&#21033;&#29992;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;AI&#31995;&#32479;&#30340;&#23433;&#20840;&#21644;&#36127;&#36131;&#20219;&#20351;&#29992;&#12290;&#24050;&#32463;&#24320;&#23637;&#20102;&#35774;&#35745;&#39640;&#25928;&#30340;&#36951;&#24536;&#26041;&#27861;&#30340;&#24037;&#20316;&#65292;&#36890;&#36807;&#30740;&#31350;MU&#26381;&#21153;&#20197;&#19982;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#26381;&#21153;&#38598;&#25104;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#25552;&#20132;&#35831;&#27714;&#20174;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#21024;&#38500;&#29305;&#23450;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13682v2 Announce Type: replace-cross  Abstract: Machine Unlearning (MU) has gained considerable attention recently for its potential to achieve Safe AI by removing the influence of specific data from trained machine learning models. This process, known as knowledge removal, addresses AI governance concerns of training data such as quality, sensitivity, copyright restrictions, and obsolescence. This capability is also crucial for ensuring compliance with privacy regulations such as the Right To Be Forgotten. Furthermore, effective knowledge removal mitigates the risk of harmful outcomes, safeguarding against biases, misinformation, and unauthorized data exploitation, thereby enhancing the safe and responsible use of AI systems. Efforts have been made to design efficient unlearning approaches, with MU services being examined for integration with existing machine learning as a service, allowing users to submit requests to remove specific data from the training corpus. However, 
&lt;/p&gt;</description></item><item><title>&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#24418;&#25104;&#30340;&#22810;&#23618;&#27425;&#38598;&#20307;&#26234;&#33021;&#32593;&#32476;&#65292;&#21487;&#20197;&#23454;&#29616;&#36229;&#36234;&#20219;&#19968;&#21333;&#29420;&#23454;&#20307;&#30340;&#38598;&#20307;&#26234;&#33021;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2403.10433</link><description>&lt;p&gt;
AI&#22686;&#24378;&#30340;&#38598;&#20307;&#26234;&#33021;&#65306;&#29616;&#29366;&#19982;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
AI-enhanced Collective Intelligence: The State of the Art and Prospects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10433
&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#24418;&#25104;&#30340;&#22810;&#23618;&#27425;&#38598;&#20307;&#26234;&#33021;&#32593;&#32476;&#65292;&#21487;&#20197;&#23454;&#29616;&#36229;&#36234;&#20219;&#19968;&#21333;&#29420;&#23454;&#20307;&#30340;&#38598;&#20307;&#26234;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#31038;&#20250;&#25361;&#25112;&#36229;&#20986;&#20102;&#20154;&#31867;&#20010;&#20307;&#25110;&#38598;&#20307;&#21162;&#21147;&#30340;&#33021;&#21147;&#12290;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#20854;&#22312;&#20154;&#31867;&#38598;&#20307;&#20013;&#30340;&#35282;&#33394;&#23558;&#20174;&#36741;&#21161;&#24037;&#20855;&#36716;&#21464;&#20026;&#21442;&#19982;&#24335;&#25104;&#21592;&#12290;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#25317;&#26377;&#20114;&#34917;&#30340;&#33021;&#21147;&#65292;&#24403;&#20108;&#32773;&#21327;&#21516;&#20316;&#29992;&#26102;&#65292;&#21487;&#20197;&#23454;&#29616;&#19968;&#31181;&#36229;&#36234;&#21333;&#29420;&#20154;&#31867;&#25110;&#20154;&#24037;&#26234;&#33021;&#38598;&#20307;&#33021;&#21147;&#30340;&#38598;&#20307;&#26234;&#33021;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#30340;&#20132;&#20114;&#26412;&#36136;&#19978;&#26159;&#22797;&#26434;&#30340;&#65292;&#28041;&#21450;&#22797;&#26434;&#30340;&#36807;&#31243;&#21644;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;&#26412;&#32508;&#36848;&#20174;&#32593;&#32476;&#31185;&#23398;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#26500;&#24819;&#20102;&#19968;&#20010;&#22810;&#23618;&#27425;&#30340;&#20154;&#24037;&#26234;&#33021;&#38598;&#20307;&#26234;&#33021;&#34920;&#31034;&#65292;&#21253;&#25324;&#35748;&#30693;&#23618;&#12289;&#29289;&#29702;&#23618;&#21644;&#20449;&#24687;&#23618;&#12290;&#22312;&#36825;&#20010;&#22810;&#23618;&#32593;&#32476;&#20013;&#65292;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#23637;&#29616;&#20986;&#19981;&#21516;&#30340;&#29305;&#24449;&#65307;&#20154;&#31867;&#22312;&#22810;&#26679;&#24615;&#26041;&#38754;&#20174;&#34920;&#23618;&#21040;&#28145;&#23618;&#23646;&#24615;&#19981;&#21516;&#65292;&#32780;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#22312;&#31243;&#24230;&#19978;&#20063;&#26377;&#25152;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10433v1 Announce Type: cross  Abstract: The current societal challenges exceed the capacity of human individual or collective effort alone. As AI evolves, its role within human collectives is poised to vary from an assistive tool to a participatory member. Humans and AI possess complementary capabilities that, when synergized, can achieve a level of collective intelligence that surpasses the collective capabilities of either humans or AI in isolation. However, the interactions in human-AI systems are inherently complex, involving intricate processes and interdependencies. This review incorporates perspectives from network science to conceptualize a multilayer representation of human-AI collective intelligence, comprising a cognition layer, a physical layer, and an information layer. Within this multilayer network, humans and AI agents exhibit varying characteristics; humans differ in diversity from surface-level to deep-level attributes, while AI agents range in degrees of f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35782;&#21035;&#21644;&#26657;&#27491;&#37327;&#23376;&#22788;&#29702;&#22270;&#20687;&#20013;&#30340;&#22122;&#22768;&#65292;&#20197;&#25552;&#39640;&#37327;&#23376;&#22270;&#20687;&#22788;&#29702;&#30340;&#36136;&#37327;&#21644;&#21487;&#38752;&#24615;&#65292;&#23454;&#29616;&#19982;&#32463;&#20856;&#35745;&#31639;&#26426;&#31867;&#20284;&#30340;&#22788;&#29702;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.11645</link><description>&lt;p&gt;
&#20511;&#21161;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#37327;&#23376;&#22270;&#20687;&#21435;&#22122;&#65306;&#25913;&#36827;&#37327;&#23376;&#22270;&#20687;&#22788;&#29702;&#36136;&#37327;&#21644;&#21487;&#38752;&#24615;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Quantum Image Denoising with Machine Learning: A Novel Approach to Improve Quantum Image Processing Quality and Reliability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11645
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35782;&#21035;&#21644;&#26657;&#27491;&#37327;&#23376;&#22788;&#29702;&#22270;&#20687;&#20013;&#30340;&#22122;&#22768;&#65292;&#20197;&#25552;&#39640;&#37327;&#23376;&#22270;&#20687;&#22788;&#29702;&#30340;&#36136;&#37327;&#21644;&#21487;&#38752;&#24615;&#65292;&#23454;&#29616;&#19982;&#32463;&#20856;&#35745;&#31639;&#26426;&#31867;&#20284;&#30340;&#22788;&#29702;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Quantum Image Processing&#65288;QIP&#65289;&#26159;&#19968;&#20010;&#26088;&#22312;&#21033;&#29992;&#37327;&#23376;&#35745;&#31639;&#20248;&#21183;&#26469;&#25805;&#20316;&#21644;&#20998;&#26512;&#22270;&#20687;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;QIP&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#65306;&#37327;&#23376;&#27604;&#29305;&#30340;&#38480;&#21046;&#21644;&#37327;&#23376;&#26426;&#22120;&#20013;&#23384;&#22312;&#30340;&#22122;&#22768;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;QIP&#20013;&#30340;&#22122;&#22768;&#38382;&#39064;&#12290;&#36890;&#36807;&#35757;&#32451;&#21644;&#20351;&#29992;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#35782;&#21035;&#24182;&#26657;&#27491;&#37327;&#23376;&#22788;&#29702;&#22270;&#20687;&#20013;&#30340;&#22122;&#22768;&#65292;&#25105;&#20204;&#21487;&#20197;&#24357;&#34917;&#26426;&#22120;&#36896;&#25104;&#30340;&#22024;&#26434;&#65292;&#24182;&#20197;&#27604;&#32463;&#20856;&#35745;&#31639;&#26426;&#26356;&#39640;&#30340;&#25928;&#29575;&#26816;&#32034;&#20986;&#19982;&#20043;&#31867;&#20284;&#30340;&#22788;&#29702;&#32467;&#26524;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#21253;&#25324;&#29616;&#26377;&#22788;&#29702;&#22270;&#20687;&#21644;&#26469;&#33258;&#24320;&#25918;&#33719;&#21462;&#25968;&#25454;&#38598;&#30340;&#37327;&#23376;&#22788;&#29702;&#22270;&#20687;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#35813;&#27169;&#22411;&#23558;&#33021;&#22815;&#20026;&#25105;&#20204;&#25552;&#20379;&#27599;&#20010;&#20687;&#32032;&#30340;&#32622;&#20449;&#27700;&#24179;&#21450;&#20854;&#28508;&#22312;&#21407;&#22987;&#20540;&#12290;&#20026;&#20102;&#35780;&#20272;&#35813;&#27169;&#22411;&#22312;&#24357;&#34917;Q&#22788;&#29702;&#20013;&#30340;&#25439;&#22833;&#21644;&#36864;&#30456;&#24178;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11645v1 Announce Type: cross  Abstract: Quantum Image Processing (QIP) is a field that aims to utilize the benefits of quantum computing for manipulating and analyzing images. However, QIP faces two challenges: the limitation of qubits and the presence of noise in a quantum machine. In this research we propose a novel approach to address the issue of noise in QIP. By training and employing a machine learning model that identifies and corrects the noise in quantum processed images, we can compensate for the noisiness caused by the machine and retrieve a processing result similar to that performed by a classical computer with higher efficiency. The model is trained by learning a dataset consisting of both existing processed images and quantum processed images from open access datasets. This model will be capable of providing us with the confidence level for each pixel and its potential original value. To assess the model's accuracy in compensating for loss and decoherence in Q
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#21508;&#31181;&#36328;&#35821;&#35328;&#35789;&#27719;&#36866;&#24212;&#26041;&#27861;&#23545;&#25552;&#39640;&#29983;&#25104;LLM&#25512;&#29702;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.10712</link><description>&lt;p&gt;
&#19968;&#39033;&#20851;&#20110;&#36328;&#35821;&#35328;&#35789;&#27719;&#36866;&#24212;&#29992;&#20110;&#39640;&#25928;&#29983;&#25104;LLM&#25512;&#29702;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Generative LLM Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10712
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#21508;&#31181;&#36328;&#35821;&#35328;&#35789;&#27719;&#36866;&#24212;&#26041;&#27861;&#23545;&#25552;&#39640;&#29983;&#25104;LLM&#25512;&#29702;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10712v1 &#36890;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#21457;&#23637;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#20998;&#35789;&#22120;&#12289;&#35789;&#27719;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;&#23613;&#31649;&#19968;&#20123;LLMs&#20855;&#26377;&#22810;&#35821;&#35328;&#33021;&#21147;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#29983;&#25104;&#33521;&#35821;&#20197;&#22806;&#30340;&#20854;&#20182;&#35821;&#35328;&#26102;&#65292;&#23427;&#20204;&#30340;&#25512;&#29702;&#25928;&#29575;&#20250;&#19979;&#38477;&#12290;&#36825;&#23548;&#33268;&#25512;&#29702;&#26102;&#38388;&#21644;&#25104;&#26412;&#22686;&#21152;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#36328;&#35821;&#35328;&#35789;&#27719;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#27169;&#22411;&#35843;&#25972;&#21040;&#30446;&#26631;&#35821;&#35328;&#65292;&#26088;&#22312;&#25552;&#39640;&#19979;&#28216;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#25552;&#39640;&#29983;&#25104;LLM&#25512;&#29702;&#25928;&#29575;&#30340;&#26377;&#25928;&#24615;&#23578;&#26410;&#24471;&#21040;&#25506;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#20116;&#31181;&#29983;&#25104;LLMs&#65288;&#21253;&#25324;&#21333;&#35821;&#21644;&#22810;&#35821;&#27169;&#22411;&#65289;&#22312;&#22235;&#31181;&#35821;&#35328;&#31867;&#22411;&#22810;&#26679;&#19988;&#22235;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#21508;&#31181;&#36328;&#35821;&#35328;&#35789;&#27719;&#36866;&#24212;&#26041;&#27861;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10712v1 Announce Type: cross  Abstract: The development of state-of-the-art generative large language models (LLMs) disproportionately relies on English-centric tokenizers, vocabulary and pre-training data. Despite the fact that some LLMs have multilingual capabilities, recent studies have shown that their inference efficiency deteriorates when generating text in languages other than English. This results in increased inference time and costs. Cross-lingual vocabulary adaptation methods have been proposed for adapting models to a target language aiming to improve downstream performance. However, the effectiveness of these methods on increasing inference efficiency of generative LLMs has yet to be explored. In this paper, we perform an empirical study of various cross-lingual vocabulary adaptation methods on five generative LLMs (including monolingual and multilingual models) across four typologically-diverse languages and four natural language understanding tasks. We find th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29983;&#25104;&#23545;&#25239;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;GABO&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#36866;&#24212;&#28304;&#25209;&#35780;&#23478;&#27491;&#21017;&#21270;&#65292;&#23558;&#20248;&#21270;&#36712;&#36857;&#38480;&#21046;&#22312;&#20195;&#29702;&#20989;&#25968;&#21487;&#38752;&#30340;&#21306;&#22495;&#20869;&#65292;&#35299;&#20915;&#20102;&#31163;&#32447;&#27169;&#22411;&#22522;&#20110;&#31574;&#30053;&#20248;&#21270;&#20013;&#20195;&#29702;&#27169;&#22411;&#39044;&#27979;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;&#22312;&#22810;&#20010;&#31163;&#32447;&#20248;&#21270;&#20219;&#21153;&#20013;&#65292;GABO&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.06532</link><description>&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#36125;&#21494;&#26031;&#20248;&#21270;&#29992;&#20110;&#20195;&#29702;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Bayesian Optimization for Surrogate Objectives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06532
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29983;&#25104;&#23545;&#25239;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;GABO&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#36866;&#24212;&#28304;&#25209;&#35780;&#23478;&#27491;&#21017;&#21270;&#65292;&#23558;&#20248;&#21270;&#36712;&#36857;&#38480;&#21046;&#22312;&#20195;&#29702;&#20989;&#25968;&#21487;&#38752;&#30340;&#21306;&#22495;&#20869;&#65292;&#35299;&#20915;&#20102;&#31163;&#32447;&#27169;&#22411;&#22522;&#20110;&#31574;&#30053;&#20248;&#21270;&#20013;&#20195;&#29702;&#27169;&#22411;&#39044;&#27979;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;&#22312;&#22810;&#20010;&#31163;&#32447;&#20248;&#21270;&#20219;&#21153;&#20013;&#65292;GABO&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#20248;&#21270;&#36890;&#36807;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#19981;&#26597;&#35810;&#30495;&#23454;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#20248;&#21270;&#23398;&#20064;&#21040;&#30340;&#20195;&#29702;&#30446;&#26631;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#32463;&#24120;&#36935;&#21040;&#20195;&#29702;&#27169;&#22411;&#39044;&#27979;&#19981;&#20934;&#30830;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#33258;&#36866;&#24212;&#28304;&#25209;&#35780;&#23478;&#27491;&#21017;&#21270;&#30340;&#29983;&#25104;&#23545;&#25239;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;GABO&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;Lipschitz&#26377;&#30028;&#28304;&#25209;&#35780;&#23478;&#27169;&#22411;&#26469;&#32422;&#26463;&#20248;&#21270;&#36712;&#36857;&#65292;&#20351;&#20854;&#22312;&#20195;&#29702;&#20989;&#25968;&#21487;&#38752;&#30340;&#21306;&#22495;&#20869;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#36830;&#32493;&#36755;&#20837;&#31354;&#38388;&#20808;&#39564;&#30340;&#19968;&#23450;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21160;&#24577;&#35843;&#25972;&#28304;&#25209;&#35780;&#23478;&#27491;&#21017;&#21270;&#30340;&#24378;&#24230;&#12290;&#22312;&#21508;&#31181;&#31185;&#23398;&#39046;&#22495;&#30340;&#22810;&#20010;&#31163;&#32447;&#20248;&#21270;&#20219;&#21153;&#20013;&#65292;GABO&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/michael-s-yao/gabo &#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline model-based policy optimization seeks to optimize a learned surrogate objective function without querying the true oracle objective during optimization. However, inaccurate surrogate model predictions are frequently encountered along the optimization trajectory. To address this limitation, we propose generative adversarial Bayesian optimization (GABO) using adaptive source critic regularization, a task-agnostic framework for Bayesian optimization that employs a Lipschitz-bounded source critic model to constrain the optimization trajectory to regions where the surrogate function is reliable. We show that under certain assumptions for the continuous input space prior, our algorithm dynamically adjusts the strength of the source critic regularization. GABO outperforms existing baselines on a number of different offline optimization tasks across a variety of scientific domains. Our code is available at https://github.com/michael-s-yao/gabo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#29992;&#27880;&#24847;&#21147;&#26144;&#23556;&#26469;&#23454;&#29616;Text-to-image diffusion models&#20013;&#30340;&#24555;&#36895;&#25512;&#29702;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.01008</link><description>&lt;p&gt;
Text-to-image diffusion models&#20013;&#36890;&#36807;&#37325;&#29992;&#27880;&#24847;&#21147;&#26144;&#23556;&#23454;&#29616;&#24555;&#36895;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Fast Inference Through The Reuse Of Attention Maps In Diffusion Models. (arXiv:2401.01008v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#29992;&#27880;&#24847;&#21147;&#26144;&#23556;&#26469;&#23454;&#29616;Text-to-image diffusion models&#20013;&#30340;&#24555;&#36895;&#25512;&#29702;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#23383;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#28789;&#27963;&#21644;&#36924;&#30495;&#30340;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#23637;&#31034;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#21333;&#20010;&#22270;&#20687;&#25152;&#38656;&#30340;&#36845;&#20195;&#36807;&#31243;&#26082;&#26114;&#36149;&#21448;&#20855;&#26377;&#36739;&#39640;&#30340;&#24310;&#36831;&#65292;&#20419;&#20351;&#30740;&#31350;&#20154;&#21592;&#36827;&#19968;&#27493;&#30740;&#31350;&#20854;&#25928;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35843;&#25972;&#37319;&#26679;&#27493;&#38271;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#37325;&#22797;&#35745;&#31639;&#27880;&#24847;&#21147;&#26144;&#23556;&#26082;&#32791;&#26102;&#21448;&#20887;&#20313;&#65292;&#22240;&#27492;&#25105;&#20204;&#24314;&#35758;&#22312;&#37319;&#26679;&#36807;&#31243;&#20013;&#32467;&#26500;&#21270;&#22320;&#37325;&#29992;&#27880;&#24847;&#21147;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#37325;&#29992;&#31574;&#30053;&#21463;&#21040;&#21021;&#32423;ODE&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#35813;&#29702;&#35770;&#35748;&#20026;&#22312;&#37319;&#26679;&#36807;&#31243;&#30340;&#21518;&#26399;&#37325;&#29992;&#26368;&#21512;&#36866;&#12290;&#22312;&#27880;&#24847;&#21040;&#36825;&#31181;&#29702;&#35770;&#26041;&#27861;&#30340;&#19968;&#20123;&#23616;&#38480;&#24615;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#19968;&#31181;&#26356;&#22909;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image diffusion models have demonstrated unprecedented abilities at flexible and realistic image synthesis. However, the iterative process required to produce a single image is costly and incurs a high latency, prompting researchers to further investigate its efficiency. Typically, improvements in latency have been achieved in two ways: (1) training smaller models through knowledge distillation (KD); and (2) adopting techniques from ODE-theory to facilitate larger step sizes. In contrast, we propose a training-free approach that does not alter the step-size of the sampler. Specifically, we find the repeated calculation of attention maps to be both costly and redundant; therefore, we propose a structured reuse of attention maps during sampling. Our initial reuse policy is motivated by rudimentary ODE-theory, which suggests that reuse is most suitable late in the sampling procedure. After noting a number of limitations in this theoretical approach, we empirically search for a bet
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29983;&#25104;&#24314;&#27169;&#23398;&#20064;&#20132;&#20114;&#20307;&#39564;&#30340;&#36890;&#29992;&#27169;&#25311;&#22120;&#65292;&#20197;&#27169;&#25311;&#20154;&#31867;&#12289;&#26426;&#22120;&#20154;&#21644;&#20854;&#20182;&#20132;&#20114;&#24335;&#20195;&#29702;&#20154;&#23545;&#30495;&#23454;&#19990;&#30028;&#20013;&#34892;&#20026;&#30340;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2310.06114</link><description>&lt;p&gt;
&#23398;&#20064;&#20132;&#20114;&#24335;&#29616;&#23454;&#19990;&#30028;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
Learning Interactive Real-World Simulators. (arXiv:2310.06114v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06114
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#24314;&#27169;&#23398;&#20064;&#20132;&#20114;&#20307;&#39564;&#30340;&#36890;&#29992;&#27169;&#25311;&#22120;&#65292;&#20197;&#27169;&#25311;&#20154;&#31867;&#12289;&#26426;&#22120;&#20154;&#21644;&#20854;&#20182;&#20132;&#20114;&#24335;&#20195;&#29702;&#20154;&#23545;&#30495;&#23454;&#19990;&#30028;&#20013;&#34892;&#20026;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#22312;&#20114;&#32852;&#32593;&#25968;&#25454;&#19978;&#30340;&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#35270;&#39057;&#20869;&#23481;&#30340;&#21019;&#24314;&#26041;&#24335;&#12290;&#20063;&#35768;&#29983;&#25104;&#27169;&#22411;&#30340;&#19979;&#19968;&#20010;&#37324;&#31243;&#30865;&#26159;&#22312;&#20154;&#31867;&#12289;&#26426;&#22120;&#20154;&#21644;&#20854;&#20182;&#20132;&#20114;&#24335;&#20195;&#29702;&#20154;&#37319;&#21462;&#34892;&#21160;&#26102;&#27169;&#25311;&#30495;&#23454;&#30340;&#20307;&#39564;&#12290;&#23454;&#38469;&#24212;&#29992;&#33539;&#22260;&#20174;&#28216;&#25103;&#21644;&#30005;&#24433;&#20013;&#30340;&#21487;&#25511;&#20869;&#23481;&#21019;&#24314;&#65292;&#21040;&#20165;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#35757;&#32451;&#21487;&#20197;&#30452;&#25509;&#37096;&#32626;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20307;&#39564;&#24335;&#20195;&#29702;&#20154;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#36890;&#36807;&#29983;&#25104;&#24314;&#27169;&#26469;&#23398;&#20064;&#29616;&#23454;&#19990;&#30028;&#20132;&#20114;&#30340;&#36890;&#29992;&#27169;&#25311;&#22120;(UniSim)&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#37325;&#35201;&#22320;&#35266;&#23519;&#21040;&#65292;&#29992;&#20110;&#23398;&#20064;&#29616;&#23454;&#19990;&#30028;&#27169;&#25311;&#22120;&#30340;&#33258;&#28982;&#25968;&#25454;&#38598;&#36890;&#24120;&#22312;&#19981;&#21516;&#30340;&#26041;&#38754;&#20016;&#23500;&#22810;&#26679;&#65288;&#20363;&#22914;&#65292;&#22270;&#20687;&#25968;&#25454;&#20013;&#20016;&#23500;&#30340;&#29289;&#20307;&#65292;&#26426;&#22120;&#20154;&#25968;&#25454;&#20013;&#23494;&#38598;&#37319;&#26679;&#30340;&#21160;&#20316;&#65292;&#23548;&#33322;&#25968;&#25454;&#20013;&#22810;&#26679;&#30340;&#31227;&#21160;&#65289;&#12290;&#36890;&#36807;&#31934;&#24515;&#21327;&#35843;&#21508;&#31181;&#25968;&#25454;&#38598;&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#37117;&#25552;&#20379;&#25972;&#20307;&#20307;&#39564;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;UniSim&#21487;&#20197;&#27169;&#25311;&#20154;&#31867;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models trained on internet data have revolutionized how text, image, and video content can be created. Perhaps the next milestone for generative models is to simulate realistic experience in response to actions taken by humans, robots, and other interactive agents. Applications of a real-world simulator range from controllable content creation in games and movies, to training embodied agents purely in simulation that can be directly deployed in the real world. We explore the possibility of learning a universal simulator (UniSim) of real-world interaction through generative modeling. We first make the important observation that natural datasets available for learning a real-world simulator are often rich along different axes (e.g., abundant objects in image data, densely sampled actions in robotics data, and diverse movements in navigation data). With careful orchestration of diverse datasets, each providing a different aspect of the overall experience, UniSim can emulate how
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20851;&#31995;&#25968;&#25454;&#24211;&#20013;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;&#30340;&#26550;&#26500;&#65292;&#24182;&#24378;&#35843;&#20102;&#19977;&#20010;&#20851;&#38190;&#33539;&#24335;&#65306;&#28145;&#24230;&#23398;&#20064;&#20013;&#24515;&#26550;&#26500;&#12289;UDF&#20013;&#24515;&#26550;&#26500;&#21644;&#20851;&#31995;&#20013;&#24515;&#26550;&#26500;&#12290;&#23613;&#31649;&#27599;&#20010;&#26550;&#26500;&#37117;&#22312;&#29305;&#23450;&#30340;&#20351;&#29992;&#22330;&#26223;&#20013;&#26377;&#28508;&#21147;&#65292;&#20294;&#36824;&#38656;&#35201;&#35299;&#20915;&#23427;&#20204;&#20043;&#38388;&#30340;&#38598;&#25104;&#38382;&#39064;&#21644;&#20013;&#38388;&#22320;&#24102;&#12290;</title><link>http://arxiv.org/abs/2310.04696</link><description>&lt;p&gt;
&#22312;&#20851;&#31995;&#25968;&#25454;&#24211;&#20013;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
Serving Deep Learning Model in Relational Databases. (arXiv:2310.04696v2 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20851;&#31995;&#25968;&#25454;&#24211;&#20013;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;&#30340;&#26550;&#26500;&#65292;&#24182;&#24378;&#35843;&#20102;&#19977;&#20010;&#20851;&#38190;&#33539;&#24335;&#65306;&#28145;&#24230;&#23398;&#20064;&#20013;&#24515;&#26550;&#26500;&#12289;UDF&#20013;&#24515;&#26550;&#26500;&#21644;&#20851;&#31995;&#20013;&#24515;&#26550;&#26500;&#12290;&#23613;&#31649;&#27599;&#20010;&#26550;&#26500;&#37117;&#22312;&#29305;&#23450;&#30340;&#20351;&#29992;&#22330;&#26223;&#20013;&#26377;&#28508;&#21147;&#65292;&#20294;&#36824;&#38656;&#35201;&#35299;&#20915;&#23427;&#20204;&#20043;&#38388;&#30340;&#38598;&#25104;&#38382;&#39064;&#21644;&#20013;&#38388;&#22320;&#24102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#21516;&#21830;&#19994;&#21644;&#31185;&#23398;&#39046;&#22495;&#20013;&#65292;&#22312;&#20851;&#31995;&#25968;&#25454;&#19978;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#38656;&#27714;&#65292;&#24182;&#24341;&#21457;&#20102;&#26368;&#36817;&#26085;&#30410;&#22686;&#38271;&#30340;&#20852;&#36259;&#12290;&#26412;&#25991;&#36890;&#36807;&#20840;&#38754;&#25506;&#32034;&#20195;&#34920;&#24615;&#26550;&#26500;&#26469;&#28385;&#36275;&#36825;&#20010;&#38656;&#27714;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#19977;&#20010;&#20851;&#38190;&#33539;&#24335;&#65306;&#23574;&#31471;&#30340;&#28145;&#24230;&#23398;&#20064;&#20013;&#24515;&#26550;&#26500;&#23558;&#28145;&#24230;&#23398;&#20064;&#35745;&#31639;&#36716;&#31227;&#21040;&#19987;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#19978;&#12290;&#28508;&#22312;&#30340;UDF&#20013;&#24515;&#26550;&#26500;&#23558;&#19968;&#20010;&#25110;&#22810;&#20010;&#24352;&#37327;&#35745;&#31639;&#23553;&#35013;&#21040;&#25968;&#25454;&#24211;&#31995;&#32479;&#20013;&#30340;&#29992;&#25143;&#23450;&#20041;&#20989;&#25968;(UDFs)&#20013;&#12290;&#28508;&#22312;&#30340;&#20851;&#31995;&#20013;&#24515;&#26550;&#26500;&#26088;&#22312;&#36890;&#36807;&#20851;&#31995;&#36816;&#31639;&#26469;&#34920;&#31034;&#22823;&#35268;&#27169;&#30340;&#24352;&#37327;&#35745;&#31639;&#12290;&#34429;&#28982;&#27599;&#20010;&#26550;&#26500;&#22312;&#29305;&#23450;&#30340;&#20351;&#29992;&#22330;&#26223;&#20013;&#37117;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#20123;&#26550;&#26500;&#20043;&#38388;&#30340;&#26080;&#32541;&#38598;&#25104;&#21644;&#20013;&#38388;&#22320;&#24102;&#20043;&#38388;&#30340;&#32039;&#24613;&#38656;&#27714;&#12290;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#22952;&#30861;&#38598;&#25104;&#30340;&#24046;&#36317;&#65292;&#24182;&#25506;&#32034;&#20102;&#21019;&#26032;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Serving deep learning (DL) models on relational data has become a critical requirement across diverse commercial and scientific domains, sparking growing interest recently. In this visionary paper, we embark on a comprehensive exploration of representative architectures to address the requirement. We highlight three pivotal paradigms: The state-of-the-artDL-Centricarchitecture offloadsDL computations to dedicated DL frameworks. The potential UDF-Centric architecture encapsulates one or more tensor computations into User Defined Functions (UDFs) within the database system. The potentialRelation-Centricarchitecture aims to represent a large-scale tensor computation through relational operators. While each of these architectures demonstrates promise in specific use scenarios, we identify urgent requirements for seamless integration of these architectures and the middle ground between these architectures. We delve into the gaps that impede the integration and explore innovative strategies 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#27169;&#25311;&#21560;&#24341;&#23376;&#21160;&#21147;&#23398;&#26469;&#26356;&#21152;&#31070;&#32463;&#21487;&#34892;&#22320;&#23454;&#29616;&#31163;&#25955;&#21270;&#65292;&#20174;&#32780;&#23558;&#36830;&#32493;&#30340;&#34920;&#31034;&#31354;&#38388;&#21010;&#20998;&#20026;&#23545;&#24212;&#20110;&#31526;&#21495;&#24207;&#21015;&#30340;&#20998;&#21306;&#12290;&#36890;&#36807;&#24341;&#20837;&#31526;&#21495;&#31354;&#38388;&#32467;&#26500;&#65292;&#21487;&#20197;&#22312;&#20016;&#23500;&#30340;&#24863;&#30693;&#36755;&#20837;&#30340;&#21560;&#24341;&#23376;&#25903;&#25345;&#34920;&#31034;&#31354;&#38388;&#20013;&#23454;&#29616;&#32452;&#21512;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01807</link><description>&lt;p&gt;
&#36890;&#36807;&#21560;&#24341;&#23376;&#21160;&#21147;&#23398;&#23454;&#29616;&#31163;&#25955;&#12289;&#32452;&#21512;&#21644;&#31526;&#21495;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Discrete, compositional, and symbolic representations through attractor dynamics. (arXiv:2310.01807v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01807
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#27169;&#25311;&#21560;&#24341;&#23376;&#21160;&#21147;&#23398;&#26469;&#26356;&#21152;&#31070;&#32463;&#21487;&#34892;&#22320;&#23454;&#29616;&#31163;&#25955;&#21270;&#65292;&#20174;&#32780;&#23558;&#36830;&#32493;&#30340;&#34920;&#31034;&#31354;&#38388;&#21010;&#20998;&#20026;&#23545;&#24212;&#20110;&#31526;&#21495;&#24207;&#21015;&#30340;&#20998;&#21306;&#12290;&#36890;&#36807;&#24341;&#20837;&#31526;&#21495;&#31354;&#38388;&#32467;&#26500;&#65292;&#21487;&#20197;&#22312;&#20016;&#23500;&#30340;&#24863;&#30693;&#36755;&#20837;&#30340;&#21560;&#24341;&#23376;&#25903;&#25345;&#34920;&#31034;&#31354;&#38388;&#20013;&#23454;&#29616;&#32452;&#21512;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#24615;&#26159;&#31163;&#25955;&#31526;&#21495;&#31995;&#32479;&#65288;&#22914;&#35821;&#35328;&#21644;&#31243;&#24207;&#65289;&#30340;&#37325;&#35201;&#29305;&#24449;&#65292;&#23427;&#20351;&#24471;&#36825;&#20123;&#31995;&#32479;&#23613;&#31649;&#20351;&#29992;&#26377;&#38480;&#30340;&#31526;&#21495;&#38598;&#21512;&#65292;&#20294;&#20173;&#20855;&#26377;&#26080;&#38480;&#30340;&#23481;&#37327;&#12290;&#23427;&#22312;&#35748;&#30693;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#25512;&#29702;&#20013;&#37117;&#20855;&#26377;&#24456;&#22909;&#30340;&#25277;&#35937;&#24615;&#12290;&#28982;&#32780;&#65292;&#36830;&#32493;&#21644;&#31526;&#21495;&#22788;&#29702;&#20043;&#38388;&#30340;&#30028;&#38754;&#36890;&#24120;&#26159;&#36890;&#36807;&#31639;&#27861;&#32423;&#21035;&#19978;&#30340;&#37327;&#21270;&#25110;softmax&#37319;&#26679;&#27493;&#39588;&#26469;&#23454;&#29616;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#21560;&#24341;&#23376;&#21160;&#21147;&#23398;&#23558;&#31163;&#25955;&#21270;&#23454;&#29616;&#24471;&#26356;&#21152;&#31070;&#32463;&#21487;&#34892;&#65292;&#36825;&#31181;&#26041;&#27861;&#23558;&#36830;&#32493;&#30340;&#34920;&#31034;&#31354;&#38388;&#21010;&#20998;&#20026;&#23545;&#24212;&#20110;&#31526;&#21495;&#24207;&#21015;&#30340;&#20998;&#21306;&#12290;&#22312;&#21560;&#24341;&#23376;&#32593;&#32476;&#30340;&#22522;&#30784;&#19978;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20016;&#23500;&#30340;&#24863;&#30693;&#36755;&#20837;&#30340;&#21560;&#24341;&#23376;&#25903;&#25345;&#34920;&#31034;&#31354;&#38388;&#20013;&#24341;&#20837;&#31526;&#21495;&#31354;&#38388;&#32467;&#26500;&#21487;&#20197;&#20135;&#29983;&#32452;&#21512;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35748;&#20026;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#31034;&#20102;&#19968;&#31181;&#20449;&#24687;&#22686;&#38271;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compositionality is an important feature of discrete symbolic systems, such as language and programs, as it enables them to have infinite capacity despite a finite symbol set. It serves as a useful abstraction for reasoning in both cognitive science and in AI, yet the interface between continuous and symbolic processing is often imposed by fiat at the algorithmic level, such as by means of quantization or a softmax sampling step. In this work, we explore how discretization could be implemented in a more neurally plausible manner through the modeling of attractor dynamics that partition the continuous representation space into basins that correspond to sequences of symbols. Building on established work in attractor networks and introducing novel training methods, we show that imposing structure in the symbolic space can produce compositionality in the attractor-supported representation space of rich sensory inputs. Lastly, we argue that our model exhibits the process of an information b
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24178;&#39044;&#24863;&#30693;&#30340;&#27010;&#24565;&#23884;&#20837;&#27169;&#22411;&#65292;&#29992;&#20110;&#25552;&#39640;&#31070;&#32463;&#26550;&#26500;&#23545;&#27010;&#24565;&#24178;&#39044;&#30340;&#21709;&#24212;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#27010;&#24565;&#24178;&#39044;&#39034;&#24207;&#21644;&#27169;&#22411;&#26550;&#26500;&#30340;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.16928</link><description>&lt;p&gt;
&#23398;&#20064;&#25509;&#21463;&#24110;&#21161;&#65306;&#24178;&#39044;&#24863;&#30693;&#30340;&#27010;&#24565;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning to Receive Help: Intervention-Aware Concept Embedding Models. (arXiv:2309.16928v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16928
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24178;&#39044;&#24863;&#30693;&#30340;&#27010;&#24565;&#23884;&#20837;&#27169;&#22411;&#65292;&#29992;&#20110;&#25552;&#39640;&#31070;&#32463;&#26550;&#26500;&#23545;&#27010;&#24565;&#24178;&#39044;&#30340;&#21709;&#24212;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#27010;&#24565;&#24178;&#39044;&#39034;&#24207;&#21644;&#27169;&#22411;&#26550;&#26500;&#30340;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBMs&#65289;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#39640;&#32423;&#27010;&#24565;&#26500;&#24314;&#21644;&#35299;&#37322;&#31070;&#32463;&#26550;&#26500;&#30340;&#39044;&#27979;&#65292;&#20197;&#35299;&#20915;&#20854;&#19981;&#36879;&#26126;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#19968;&#20010;&#29305;&#27530;&#23646;&#24615;&#26159;&#23427;&#20204;&#20801;&#35768;&#27010;&#24565;&#24178;&#39044;&#65292;&#29992;&#25143;&#21487;&#20197;&#32416;&#27491;&#34987;&#38169;&#35823;&#39044;&#27979;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24178;&#39044;&#26377;&#25928;&#24615;&#21487;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#24178;&#39044;&#27010;&#24565;&#30340;&#39034;&#24207;&#20197;&#21450;&#27169;&#22411;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#28304;&#20110;CBM&#22312;&#35757;&#32451;&#26102;&#32570;&#20047;&#27169;&#22411;&#36866;&#24212;&#27010;&#24565;&#24178;&#39044;&#30340;&#28608;&#21169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24178;&#39044;&#24863;&#30693;&#30340;&#27010;&#24565;&#23884;&#20837;&#27169;&#22411;&#65288;IntCEMs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;CBM&#30340;&#26032;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#33539;&#24335;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#27979;&#35797;&#26102;&#24178;&#39044;&#30340;&#21709;&#24212;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#23398;&#20064;&#20102;&#19968;&#20010;&#27010;&#24565;&#24178;&#39044;&#31574;&#30053;&#65292;&#20174;&#20013;&#21487;&#20197;&#37319;&#26679;&#26377;&#24847;&#20041;&#30340;&#24178;&#39044;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept Bottleneck Models (CBMs) tackle the opacity of neural architectures by constructing and explaining their predictions using a set of high-level concepts. A special property of these models is that they permit concept interventions, wherein users can correct mispredicted concepts and thus improve the model's performance. Recent work, however, has shown that intervention efficacy can be highly dependent on the order in which concepts are intervened on and on the model's architecture and training hyperparameters. We argue that this is rooted in a CBM's lack of train-time incentives for the model to be appropriately receptive to concept interventions. To address this, we propose Intervention-aware Concept Embedding models (IntCEMs), a novel CBM-based architecture and training paradigm that improves a model's receptiveness to test-time interventions. Our model learns a concept intervention policy in an end-to-end fashion from where it can sample meaningful intervention trajectories a
&lt;/p&gt;</description></item><item><title>&#22791;&#36873;&#30340;&#21464;&#28966;&#20301;&#31227;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#26367;&#31227;&#21160;&#21644;&#25193;&#23637;&#29305;&#24449;&#20449;&#24687;&#26469;&#34701;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#21487;&#20197;&#31283;&#20581;&#22320;&#25429;&#25417;&#19981;&#21516;&#27169;&#24577;&#29305;&#24449;&#20043;&#38388;&#30340;&#39640;&#32423;&#20132;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#27969;&#34892;&#30340;&#22810;&#27169;&#24577;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.16950</link><description>&lt;p&gt;
&#22791;&#36873;&#30340;&#21464;&#28966;&#20301;&#31227;&#65306;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Alternative Telescopic Displacement: An Efficient Multimodal Alignment Method. (arXiv:2306.16950v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16950
&lt;/p&gt;
&lt;p&gt;
&#22791;&#36873;&#30340;&#21464;&#28966;&#20301;&#31227;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#26367;&#31227;&#21160;&#21644;&#25193;&#23637;&#29305;&#24449;&#20449;&#24687;&#26469;&#34701;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#21487;&#20197;&#31283;&#20581;&#22320;&#25429;&#25417;&#19981;&#21516;&#27169;&#24577;&#29305;&#24449;&#20043;&#38388;&#30340;&#39640;&#32423;&#20132;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#27969;&#34892;&#30340;&#22810;&#27169;&#24577;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#23545;&#40784;&#26159;&#34701;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#20027;&#35201;&#26041;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#23545;&#40784;&#26041;&#27861;&#65292;&#21487;&#20197;&#23436;&#20840;&#34701;&#21512;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#36890;&#36807;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#20132;&#26367;&#31227;&#21160;&#21644;&#25193;&#23637;&#26469;&#23454;&#29616;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#19968;&#33268;&#34920;&#31034;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#31283;&#20581;&#22320;&#25429;&#25417;&#19981;&#21516;&#27169;&#24577;&#29305;&#24449;&#20043;&#38388;&#30340;&#39640;&#32423;&#20132;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#20248;&#20110;&#20854;&#20182;&#27969;&#34892;&#30340;&#22810;&#27169;&#24577;&#26041;&#26696;&#12290;&#23545;ETT&#21644;MIT-BIH-Arrhythmia&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature alignment is the primary means of fusing multimodal data. We propose a feature alignment method that fully fuses multimodal information, which alternately shifts and expands feature information from different modalities to have a consistent representation in a feature space. The proposed method can robustly capture high-level interactions between features of different modalities, thus significantly improving the performance of multimodal learning. We also show that the proposed method outperforms other popular multimodal schemes on multiple tasks. Experimental evaluation of ETT and MIT-BIH-Arrhythmia, datasets shows that the proposed method achieves state of the art performance.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#20892;&#19994;&#39135;&#21697;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#20892;&#19994;&#12289;&#30044;&#29287;&#19994;&#21644;&#28180;&#19994;&#31561;&#39046;&#22495;&#12290;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#20892;&#19994;&#39135;&#21697;&#20998;&#31867;&#12289;&#29983;&#38271;&#30417;&#27979;&#12289;&#20135;&#37327;&#39044;&#27979;&#21644;&#21697;&#36136;&#35780;&#20272;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20063;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#20197;&#21450;&#28508;&#22312;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.01899</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#38761;&#21629;&#65306;&#20892;&#19994;&#39135;&#21697;&#31995;&#32479;&#30340;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Revolutionizing Agrifood Systems with Artificial Intelligence: A Survey. (arXiv:2305.01899v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01899
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#20892;&#19994;&#39135;&#21697;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#20892;&#19994;&#12289;&#30044;&#29287;&#19994;&#21644;&#28180;&#19994;&#31561;&#39046;&#22495;&#12290;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#20892;&#19994;&#39135;&#21697;&#20998;&#31867;&#12289;&#29983;&#38271;&#30417;&#27979;&#12289;&#20135;&#37327;&#39044;&#27979;&#21644;&#21697;&#36136;&#35780;&#20272;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20063;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#20197;&#21450;&#28508;&#22312;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20840;&#29699;&#20154;&#21475;&#30340;&#36805;&#36895;&#22686;&#38271;&#65292;&#36716;&#21464;&#20892;&#19994;&#39135;&#21697;&#31995;&#32479;&#65292;&#20351;&#20854;&#26356;&#20855;&#29983;&#20135;&#21147;&#12289;&#25928;&#29575;&#12289;&#23433;&#20840;&#21644;&#21487;&#25345;&#32493;&#24615;&#65292;&#26159;&#32531;&#35299;&#28508;&#22312;&#31918;&#39135;&#30701;&#32570;&#30340;&#20851;&#38190;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#31561;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#35821;&#35328;&#12289;&#35270;&#35273;&#12289;&#36965;&#24863;&#21644;&#20892;&#19994;&#39135;&#21697;&#31995;&#32479;&#24212;&#29992;&#31561;&#21508;&#20010;&#39046;&#22495;&#22343;&#26174;&#31034;&#20986;&#20102;&#20854;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#23545;&#20892;&#19994;&#39135;&#21697;&#31995;&#32479;&#30340;&#25972;&#20307;&#24433;&#21709;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22914;&#20309;&#25913;&#21464;&#20892;&#19994;&#39135;&#21697;&#31995;&#32479;&#65292;&#24182;&#20026;&#29616;&#20195;&#20892;&#19994;&#39135;&#21697;&#34892;&#19994;&#20570;&#20986;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#20892;&#19994;&#39135;&#21697;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#33719;&#21462;&#26041;&#27861;&#65292;&#21253;&#25324;&#33719;&#21462;&#12289;&#23384;&#20648;&#21644;&#22788;&#29702;&#25216;&#26415;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#20892;&#19994;&#12289;&#30044;&#29287;&#19994;&#21644;&#28180;&#19994;&#31561;&#39046;&#22495;&#20013;&#30340;&#36827;&#23637;&#24773;&#20917;&#65292;&#28085;&#30422;&#20102;&#20892;&#19994;&#39135;&#21697;&#20998;&#31867;&#12289;&#29983;&#38271;&#30417;&#27979;&#12289;&#20135;&#37327;&#39044;&#27979;&#21644;&#21697;&#36136;&#35780;&#20272;&#31561;&#20027;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#65292;&#24182;&#35752;&#35770;&#20102;&#22312;&#20892;&#19994;&#39135;&#21697;&#31995;&#32479;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20013;&#28508;&#22312;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the world population rapidly increasing, transforming our agrifood systems to be more productive, efficient, safe, and sustainable is crucial to mitigate potential food shortages. Recently, artificial intelligence (AI) techniques such as deep learning (DL) have demonstrated their strong abilities in various areas, including language, vision, remote sensing (RS), and agrifood systems applications. However, the overall impact of AI on agrifood systems remains unclear. In this paper, we thoroughly review how AI techniques can transform agrifood systems and contribute to the modern agrifood industry. Firstly, we summarize the data acquisition methods in agrifood systems, including acquisition, storage, and processing techniques. Secondly, we present a progress review of AI methods in agrifood systems, specifically in agriculture, animal husbandry, and fishery, covering topics such as agrifood classification, growth monitoring, yield prediction, and quality assessment. Furthermore, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#20070;&#26088;&#22312;&#20171;&#32461;&#36125;&#21494;&#26031;&#30697;&#38453;&#20998;&#35299;&#30340;&#27010;&#24565;&#21644;&#24037;&#20855;&#65292;&#24182;&#24635;&#32467;&#20102;&#36125;&#21494;&#26031;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.11337</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#30697;&#38453;&#20998;&#35299;&#21450;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Bayesian Matrix Decomposition and Applications. (arXiv:2302.11337v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#20070;&#26088;&#22312;&#20171;&#32461;&#36125;&#21494;&#26031;&#30697;&#38453;&#20998;&#35299;&#30340;&#27010;&#24565;&#21644;&#24037;&#20855;&#65292;&#24182;&#24635;&#32467;&#20102;&#36125;&#21494;&#26031;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#20070;&#30340;&#21807;&#19968;&#30446;&#30340;&#26159;&#20026;&#20102;&#32473;&#20986;&#36125;&#21494;&#26031;&#30697;&#38453;&#20998;&#35299;&#27010;&#24565;&#21644;&#25968;&#23398;&#24037;&#20855;&#30340;&#33258;&#21253;&#21547;&#20171;&#32461;&#65292;&#20197;&#20415;&#22312;&#21518;&#32493;&#31456;&#33410;&#20013;&#26080;&#32541;&#24341;&#20837;&#30697;&#38453;&#20998;&#35299;&#25216;&#26415;&#21450;&#20854;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#28165;&#26970;&#22320;&#24847;&#35782;&#21040;&#25105;&#20204;&#26080;&#27861;&#35206;&#30422;&#20851;&#20110;&#36125;&#21494;&#26031;&#30697;&#38453;&#20998;&#35299;&#30340;&#25152;&#26377;&#26377;&#29992;&#21644;&#26377;&#36259;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#30001;&#20110;&#35752;&#35770;&#30340;&#33539;&#22260;&#26377;&#38480;&#65292;&#20363;&#22914;&#20998;&#26512;&#21464;&#20998;&#25512;&#29702;&#20197;&#36827;&#34892;&#20248;&#21270;&#30340;&#20998;&#31163;&#20998;&#26512;&#12290;&#25105;&#20204;&#23558;&#35835;&#32773;&#24341;&#23548;&#21040;&#36125;&#21494;&#26031;&#20998;&#26512;&#39046;&#22495;&#30340;&#25991;&#29486;&#20013;&#65292;&#20197;&#20415;&#26356;&#35814;&#32454;&#22320;&#20171;&#32461;&#30456;&#20851;&#39046;&#22495;&#12290;&#26412;&#20070;&#20027;&#35201;&#24635;&#32467;&#20102;&#37325;&#35201;&#30340;&#36125;&#21494;&#26031;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#65288;&#20363;&#22914;&#23454;&#20540;&#20998;&#35299;&#12289;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#12289;&#36125;&#21494;&#26031;&#25554;&#20540;&#20998;&#35299;&#65289;&#30340;&#30446;&#30340;&#21644;&#24847;&#20041;&#65292;&#20197;&#21450;&#36825;&#20123;&#26041;&#27861;&#30340;&#36215;&#28304;&#21644;&#22797;&#26434;&#24615;&#23545;&#20854;&#24212;&#29992;&#25552;&#20379;&#30340;&#21551;&#31034;&#12290;&#25968;&#23398;&#20808;&#20915;&#26465;&#20214;&#26159;&#31532;&#19968;&#38376;&#35838;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
The sole aim of this book is to give a self-contained introduction to concepts and mathematical tools in Bayesian matrix decomposition in order to seamlessly introduce matrix decomposition techniques and their applications in subsequent sections. However, we clearly realize our inability to cover all the useful and interesting results concerning Bayesian matrix decomposition and given the paucity of scope to present this discussion, e.g., the separated analysis of variational inference for conducting the optimization. We refer the reader to literature in the field of Bayesian analysis for a more detailed introduction to the related fields.  This book is primarily a summary of purpose, significance of important Bayesian matrix decomposition methods, e.g., real-valued decomposition, nonnegative matrix factorization, Bayesian interpolative decomposition, and the origin and complexity of the methods which shed light on their applications. The mathematical prerequisite is a first course in 
&lt;/p&gt;</description></item></channel></rss>