<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#30740;&#31350;&#26159;&#31532;&#19968;&#20010;&#20840;&#38754;&#35843;&#26597;&#27169;&#22411;&#35299;&#37322;&#20013;&#38544;&#31169;&#25915;&#20987;&#21450;&#20854;&#23545;&#25239;&#25514;&#26045;&#30340;&#35770;&#25991;&#65292;&#36890;&#36807;&#20998;&#31867;&#38544;&#31169;&#25915;&#20987;&#21644;&#23545;&#25239;&#25514;&#26045;&#65292;&#21021;&#27493;&#25506;&#35752;&#20102;&#38544;&#31169;&#27844;&#28431;&#21407;&#22240;&#65292;&#25552;&#20986;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2404.00673</link><description>&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#22411;&#27169;&#22411;&#35299;&#37322;&#30740;&#31350;&#32508;&#36848;&#65306;&#38544;&#31169;&#39118;&#38505;&#12289;&#25915;&#20987;&#21644;&#23545;&#25239;&#25514;&#26045;
&lt;/p&gt;
&lt;p&gt;
A Survey of Privacy-Preserving Model Explanations: Privacy Risks, Attacks, and Countermeasures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#31532;&#19968;&#20010;&#20840;&#38754;&#35843;&#26597;&#27169;&#22411;&#35299;&#37322;&#20013;&#38544;&#31169;&#25915;&#20987;&#21450;&#20854;&#23545;&#25239;&#25514;&#26045;&#30340;&#35770;&#25991;&#65292;&#36890;&#36807;&#20998;&#31867;&#38544;&#31169;&#25915;&#20987;&#21644;&#23545;&#25239;&#25514;&#26045;&#65292;&#21021;&#27493;&#25506;&#35752;&#20102;&#38544;&#31169;&#27844;&#28431;&#21407;&#22240;&#65292;&#25552;&#20986;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#37319;&#29992;&#19981;&#26029;&#25193;&#22823;&#65292;&#35299;&#20915;&#20854;&#38544;&#31169;&#24433;&#21709;&#30340;&#32039;&#36843;&#24615;&#21464;&#24471;&#26356;&#21152;&#36843;&#20999;&#12290;&#23613;&#31649;&#22312;&#20154;&#24037;&#26234;&#33021;&#38544;&#31169;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#26377;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#38544;&#31169;&#20445;&#25252;&#22411;&#27169;&#22411;&#35299;&#37322;&#21364;&#40092;&#26377;&#20851;&#27880;&#12290;&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#35843;&#26597;&#20102;&#27169;&#22411;&#35299;&#37322;&#30340;&#38544;&#31169;&#25915;&#20987;&#21450;&#20854;&#23545;&#25239;&#25514;&#26045;&#12290;&#25105;&#20204;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#36129;&#29486;&#21253;&#25324;&#23545;&#30740;&#31350;&#35770;&#25991;&#36827;&#34892;&#24443;&#24213;&#20998;&#26512;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#30456;&#20114;&#36830;&#25509;&#30340;&#20998;&#31867;&#27861;&#65292;&#20415;&#20110;&#26681;&#25454;&#30446;&#26631;&#35299;&#37322;&#23545;&#38544;&#31169;&#25915;&#20987;&#21644;&#23545;&#25239;&#25514;&#26045;&#36827;&#34892;&#20998;&#31867;&#12290;&#26412;&#30740;&#31350;&#36824;&#23545;&#38544;&#31169;&#27844;&#28431;&#21407;&#22240;&#36827;&#34892;&#20102;&#21021;&#27493;&#35843;&#26597;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#20998;&#26512;&#20013;&#21457;&#29616;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#35813;&#35843;&#26597;&#26088;&#22312;&#25104;&#20026;&#30740;&#31350;&#30028;&#30340;&#23453;&#36149;&#36164;&#28304;&#65292;&#24182;&#20026;&#36825;&#19968;&#39046;&#22495;&#30340;&#26032;&#25163;&#25552;&#20379;&#26126;&#30830;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00673v1 Announce Type: cross  Abstract: As the adoption of explainable AI (XAI) continues to expand, the urgency to address its privacy implications intensifies. Despite a growing corpus of research in AI privacy and explainability, there is little attention on privacy-preserving model explanations. This article presents the first thorough survey about privacy attacks on model explanations and their countermeasures. Our contribution to this field comprises a thorough analysis of research papers with a connected taxonomy that facilitates the categorisation of privacy attacks and countermeasures based on the targeted explanations. This work also includes an initial investigation into the causes of privacy leaks. Finally, we discuss unresolved issues and prospective research directions uncovered in our analysis. This survey aims to be a valuable resource for the research community and offers clear insights for those new to this domain. To support ongoing research, we have estab
&lt;/p&gt;</description></item><item><title>AI&#27861;&#26696;&#21487;&#33021;&#25104;&#20026;&#24357;&#21512;&#31639;&#27861;&#20844;&#24179;&#24615;&#21644;&#27431;&#30431;&#38750;&#27495;&#35270;&#27861;&#24459;&#30340;&#37325;&#35201;&#19968;&#29615;&#65292;&#36890;&#36807;&#23558;&#38750;&#27495;&#35270;&#36131;&#20219;&#36716;&#31227;&#21040;AI&#27169;&#22411;&#35774;&#35745;&#38454;&#27573;&#65292;&#25512;&#21160;&#20559;&#35265;&#26816;&#27979;&#21644;&#20559;&#35265;&#26657;&#27491;&#30340;&#30456;&#20851;&#23454;&#36341;&#12290;</title><link>https://arxiv.org/abs/2403.20089</link><description>&lt;p&gt;
AI&#27861;&#26696;&#23545;&#38750;&#27495;&#35270;&#27861;&#24459;&#21644;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Implications of the AI Act for Non-Discrimination Law and Algorithmic Fairness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20089
&lt;/p&gt;
&lt;p&gt;
AI&#27861;&#26696;&#21487;&#33021;&#25104;&#20026;&#24357;&#21512;&#31639;&#27861;&#20844;&#24179;&#24615;&#21644;&#27431;&#30431;&#38750;&#27495;&#35270;&#27861;&#24459;&#30340;&#37325;&#35201;&#19968;&#29615;&#65292;&#36890;&#36807;&#23558;&#38750;&#27495;&#35270;&#36131;&#20219;&#36716;&#31227;&#21040;AI&#27169;&#22411;&#35774;&#35745;&#38454;&#27573;&#65292;&#25512;&#21160;&#20559;&#35265;&#26816;&#27979;&#21644;&#20559;&#35265;&#26657;&#27491;&#30340;&#30456;&#20851;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#20844;&#24179;&#24615;&#30340;&#35805;&#39064;&#65292;&#27491;&#22914;&#22312;FATE&#65288;AI&#20844;&#24179;&#24615;&#12289;&#36131;&#20219;&#24615;&#12289;&#36879;&#26126;&#24615;&#21644;&#20262;&#29702;&#65289;&#31038;&#21306;&#20013;&#35752;&#35770;&#30340;&#37027;&#26679;&#65292;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#24341;&#21457;&#20102;&#26377;&#24847;&#20041;&#30340;&#35752;&#35770;&#12290;&#28982;&#32780;&#65292;&#20174;&#27861;&#24459;&#35282;&#24230;&#26469;&#30475;&#65292;&#29305;&#21035;&#26159;&#20174;&#27431;&#30431;&#27861;&#24459;&#30340;&#35282;&#24230;&#65292;&#20173;&#28982;&#23384;&#22312;&#35768;&#22810;&#24453;&#35299;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#31639;&#27861;&#20844;&#24179;&#24615;&#26088;&#22312;&#22312;&#35774;&#35745;&#38454;&#27573;&#20943;&#36731;&#32467;&#26500;&#19981;&#24179;&#31561;&#65292;&#20294;&#27431;&#27954;&#30340;&#38750;&#27495;&#35270;&#27861;&#24459;&#21017;&#26159;&#38024;&#23545;AI&#27169;&#22411;&#37096;&#32626;&#21518;&#30340;&#20010;&#21035;&#27495;&#35270;&#26696;&#20214;&#37327;&#36523;&#23450;&#21046;&#30340;&#12290;AI&#27861;&#26696;&#21487;&#33021;&#26159;&#26397;&#30528;&#24357;&#21512;&#36825;&#20004;&#20010;&#27010;&#24565;&#30340;&#24040;&#22823;&#19968;&#27493;&#65292;&#36890;&#36807;&#23558;&#38750;&#27495;&#35270;&#36131;&#20219;&#36716;&#31227;&#21040;AI&#27169;&#22411;&#35774;&#35745;&#38454;&#27573;&#12290;&#22522;&#20110;&#23545;AI&#27861;&#26696;&#30340;&#32508;&#21512;&#38405;&#35835;&#65292;&#25105;&#20204;&#35780;&#35770;&#20102;&#27861;&#24459;&#21644;&#25216;&#26415;&#25191;&#34892;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#20559;&#35265;&#26816;&#27979;&#21644;&#20559;&#35265;&#26657;&#27491;&#26041;&#38754;&#30340;&#23454;&#38469;&#24433;&#21709;&#65292;&#20197;&#20415;&#25351;&#23450;&#21644;&#31526;&#21512;&#29305;&#23450;&#30340;&#25216;&#26415;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20089v1 Announce Type: new  Abstract: The topic of fairness in AI, as debated in the FATE (Fairness, Accountability, Transparency, and Ethics in AI) communities, has sparked meaningful discussions in the past years. However, from a legal perspective, particularly from European Union law, many open questions remain. Whereas algorithmic fairness aims to mitigate structural inequalities at the design level, European non-discrimination law is tailored to individual cases of discrimination after an AI model has been deployed. The AI Act might present a tremendous step towards bridging these two concepts by shifting non-discrimination responsibilities into the design stage of AI models. Based on an integrative reading of the AI Act, we comment on legal as well as technical enforcement problems and propose practical implications on bias detection and bias correction in order to specify and comply with specific technical requirements.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19990;&#30028;&#27169;&#22411;&#22788;&#29702;&#35266;&#23519;&#24310;&#36831;&#30340;&#26041;&#27861;&#65292;&#21487;&#26377;&#25928;&#22788;&#29702;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#32988;&#36807;&#26420;&#32032;&#26041;&#27861;&#36798;&#21040;30%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.12309</link><description>&lt;p&gt;
&#36890;&#36807;&#19990;&#30028;&#27169;&#22411;&#20174;&#24310;&#36831;&#35266;&#23519;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Delayed Observations via World Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19990;&#30028;&#27169;&#22411;&#22788;&#29702;&#35266;&#23519;&#24310;&#36831;&#30340;&#26041;&#27861;&#65292;&#21487;&#26377;&#25928;&#22788;&#29702;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#32988;&#36807;&#26420;&#32032;&#26041;&#27861;&#36798;&#21040;30%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26631;&#20934;&#30340;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#20013;&#65292;&#20195;&#29702;&#36890;&#24120;&#20551;&#23450;&#22312;&#37319;&#21462;&#34892;&#21160;&#21518;&#31435;&#21363;&#33719;&#24471;&#20851;&#20110;&#34892;&#21160;&#25928;&#26524;&#30340;&#21453;&#39304;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#30001;&#20110;&#29289;&#29702;&#38480;&#21046;&#65292;&#36825;&#19968;&#20551;&#35774;&#21487;&#33021;&#19981;&#25104;&#31435;&#65292;&#36825;&#21487;&#33021;&#20250;&#20005;&#37325;&#24433;&#21709;RL&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#35299;&#20915;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#30340;&#35266;&#23519;&#24310;&#36831;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#19990;&#30028;&#27169;&#22411;&#26469;&#22788;&#29702;&#35266;&#23519;&#24310;&#36831;&#65292;&#19990;&#30028;&#27169;&#22411;&#24050;&#32463;&#22312;&#25972;&#21512;&#36807;&#21435;&#35266;&#23519;&#21644;&#23398;&#20064;&#21160;&#24577;&#26041;&#38754;&#21462;&#24471;&#25104;&#21151;&#12290;&#36890;&#36807;&#23558;&#24310;&#36831;POMDP&#38477;&#20302;&#20026;&#20855;&#26377;&#19990;&#30028;&#27169;&#22411;&#30340;&#24310;&#36831;MDP&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#65292;&#20854;&#20013;&#29616;&#26377;&#26041;&#27861;&#22312;&#21487;&#35266;&#23519;&#24615;&#38477;&#20302;&#26102;&#23454;&#29616;&#27425;&#20248;&#24615;&#33021;&#29978;&#33267;&#36805;&#36895;&#19979;&#38477;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#35270;&#35273;&#36755;&#20837;&#24310;&#36831;&#29615;&#22659;&#19979;&#32988;&#36807;&#26420;&#32032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#36798;&#21040;30%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#35270;&#35273;&#36755;&#20837;&#24310;&#36831;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12309v1 Announce Type: cross  Abstract: In standard Reinforcement Learning settings, agents typically assume immediate feedback about the effects of their actions after taking them. However, in practice, this assumption may not hold true due to physical constraints and can significantly impact the performance of RL algorithms. In this paper, we focus on addressing observation delays in partially observable environments. We propose leveraging world models, which have shown success in integrating past observations and learning dynamics, to handle observation delays. By reducing delayed POMDPs to delayed MDPs with world models, our methods can effectively handle partial observability, where existing approaches achieve sub-optimal performance or even degrade quickly as observability decreases. Experiments suggest that one of our methods can outperform a naive model-based approach by up to %30. Moreover, we evaluate our methods on visual input based delayed environment, for the f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;Watkins&#28023;&#27915;&#21754;&#20083;&#21160;&#29289;&#22768;&#38899;&#25968;&#25454;&#24211;&#19978;&#24212;&#29992;Wavelet&#25955;&#23556;&#21464;&#25442;&#65288;WST&#65289;&#21644;Mel&#39057;&#35889;&#22270;&#39044;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.17775</link><description>&lt;p&gt;
Wavelet&#25955;&#23556;&#21464;&#25442;&#22312;&#29983;&#29289;&#22768;&#23398;&#20013;&#30340;&#24212;&#29992;&#65306;&#20197;Watkins&#28023;&#27915;&#21754;&#20083;&#21160;&#29289;&#22768;&#38899;&#25968;&#25454;&#24211;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Wavelet Scattering Transform for Bioacustics: Application to Watkins Marine Mammal Sound Database
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;Watkins&#28023;&#27915;&#21754;&#20083;&#21160;&#29289;&#22768;&#38899;&#25968;&#25454;&#24211;&#19978;&#24212;&#29992;Wavelet&#25955;&#23556;&#21464;&#25442;&#65288;WST&#65289;&#21644;Mel&#39057;&#35889;&#22270;&#39044;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#27915;&#21754;&#20083;&#21160;&#29289;&#30340;&#20132;&#27969;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#39046;&#22495;&#65292;&#21463;&#21040;&#40483;&#21483;&#30340;&#22810;&#26679;&#24615;&#21644;&#29615;&#22659;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;Watkins&#28023;&#27915;&#21754;&#20083;&#21160;&#29289;&#22768;&#38899;&#25968;&#25454;&#24211;&#65288;WMMD&#65289;&#26159;&#19968;&#20010;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#37325;&#28857;&#20171;&#32461;&#20102;&#35813;&#25968;&#25454;&#38598;&#19978;&#26368;&#26032;&#30340;&#22522;&#20934;&#35760;&#24405;&#65292;&#30528;&#37325;&#28548;&#28165;&#25968;&#25454;&#20934;&#22791;&#21644;&#39044;&#22788;&#29702;&#26041;&#27861;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;STFT&#22522;&#30784;&#19978;&#24212;&#29992;Wavelet&#25955;&#23556;&#21464;&#25442;&#65288;WST&#65289;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#20351;&#29992;&#33258;&#36866;&#24212;&#28145;&#23618;&#26550;&#26500;&#21644;&#27531;&#24046;&#23618;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#20934;&#30830;&#29575;&#19978;&#20351;&#29992;WST&#27604;&#29616;&#26377;&#20998;&#31867;&#26550;&#26500;&#25552;&#39640;&#20102;6&#65285;&#65292;&#20351;&#29992;Mel&#39057;&#35889;&#22270;&#39044;&#22788;&#29702;&#25552;&#39640;&#20102;8&#65285;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17775v1 Announce Type: cross  Abstract: Marine mammal communication is a complex field, hindered by the diversity of vocalizations and environmental factors. The Watkins Marine Mammal Sound Database (WMMD) is an extensive labeled dataset used in machine learning applications. However, the methods for data preparation, preprocessing, and classification found in the literature are quite disparate. This study first focuses on a brief review of the state-of-the-art benchmarks on the dataset, with an emphasis on clarifying data preparation and preprocessing methods. Subsequently, we propose the application of the Wavelet Scattering Transform (WST) in place of standard methods based on the Short-Time Fourier Transform (STFT). The study also tackles a classification task using an ad-hoc deep architecture with residual layers. We outperform the existing classification architecture by $6\%$ in accuracy using WST and $8\%$ using Mel spectrogram preprocessing, effectively reducing by h
&lt;/p&gt;</description></item><item><title>transformers&#22312;&#25968;&#23398;&#38382;&#39064;&#20013;&#37319;&#29992;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#32780;&#38750;&#22522;&#20110;&#35268;&#21017;&#30340;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.17709</link><description>&lt;p&gt;
&#22522;&#20110;&#26696;&#20363;&#36824;&#26159;&#22522;&#20110;&#35268;&#21017;&#65306;&#21464;&#21387;&#22120;&#22914;&#20309;&#36827;&#34892;&#25968;&#23398;&#35745;&#31639;&#65311;
&lt;/p&gt;
&lt;p&gt;
Case-Based or Rule-Based: How Do Transformers Do the Math?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17709
&lt;/p&gt;
&lt;p&gt;
transformers&#22312;&#25968;&#23398;&#38382;&#39064;&#20013;&#37319;&#29992;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#32780;&#38750;&#22522;&#20110;&#35268;&#21017;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20173;&#28982;&#38590;&#20197;&#22788;&#29702;&#19968;&#20123;&#23545;&#20154;&#31867;&#26469;&#35828;&#31616;&#21333;&#19988;&#30452;&#35266;&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#20363;&#22914;&#21152;&#27861;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21487;&#20197;&#36731;&#26494;&#23398;&#20064;&#21152;&#27861;&#30340;&#22522;&#26412;&#35268;&#21017;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20219;&#24847;&#38271;&#24230;&#30340;&#26032;&#38382;&#39064;&#65292;&#32780;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21364;&#38590;&#20197;&#20570;&#21040;&#12290;&#30456;&#21453;&#65292;&#23427;&#20204;&#21487;&#33021;&#20381;&#36182;&#20110;&#22312;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30475;&#21040;&#30340;&#31867;&#20284;&#8220;&#26696;&#20363;&#8221;&#26469;&#33719;&#21462;&#24110;&#21161;&#12290;&#25105;&#20204;&#23558;&#36825;&#20004;&#31181;&#19981;&#21516;&#30340;&#25512;&#29702;&#26426;&#21046;&#23450;&#20041;&#20026;&#8220;&#22522;&#20110;&#35268;&#21017;&#30340;&#25512;&#29702;&#8221;&#21644;&#8220;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#8221;&#12290;&#30001;&#20110;&#22522;&#20110;&#35268;&#21017;&#30340;&#25512;&#29702;&#23545;&#20110;&#33719;&#24471;&#31995;&#32479;&#21270;&#27010;&#25324;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#65292;&#25105;&#20204;&#26088;&#22312;&#25506;&#31350;&#21464;&#21387;&#22120;&#31350;&#31455;&#26159;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#36824;&#26159;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#26469;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#12290;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#20116;&#20010;&#25968;&#23398;&#20219;&#21153;&#30340;&#24178;&#39044;&#23454;&#39564;&#65292;&#25105;&#20204;&#30830;&#35748;&#21464;&#21387;&#22120;&#27491;&#22312;&#25191;&#34892;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#65292;&#26080;&#35770;&#26159;&#21542;&#20351;&#29992;&#33609;&#31295;&#26412;&#65292;&#36825;&#19982;&#20043;&#21069;&#30340;&#35266;&#23519;&#32467;&#26524;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17709v1 Announce Type: new  Abstract: Despite the impressive performance in a variety of complex tasks, modern large language models (LLMs) still have trouble dealing with some math problems that are simple and intuitive for humans, such as addition. While we can easily learn basic rules of addition and apply them to new problems of any length, LLMs struggle to do the same. Instead, they may rely on similar "cases" seen in the training corpus for help. We define these two different reasoning mechanisms as "rule-based reasoning" and "case-based reasoning". Since rule-based reasoning is essential for acquiring the systematic generalization ability, we aim to explore exactly whether transformers use rule-based or case-based reasoning for math problems. Through carefully designed intervention experiments on five math tasks, we confirm that transformers are performing case-based reasoning, no matter whether scratchpad is used, which aligns with the previous observations that tran
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;Casanovo-DIA&#65292;&#19968;&#31181;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#20174;DIA&#36136;&#35889;&#25968;&#25454;&#20013;&#35299;&#26512;&#32957;&#27573;&#24207;&#21015;&#12290;</title><link>https://arxiv.org/abs/2402.11363</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#20840;&#26032;&#32957;&#27573;&#27979;&#24207;&#25216;&#26415;&#29992;&#20110;&#25968;&#25454;&#26080;&#20559;&#21521;&#37319;&#38598;&#36136;&#35889;
&lt;/p&gt;
&lt;p&gt;
Transformer-based de novo peptide sequencing for data-independent acquisition mass spectrometry
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11363
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;Casanovo-DIA&#65292;&#19968;&#31181;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#20174;DIA&#36136;&#35889;&#25968;&#25454;&#20013;&#35299;&#26512;&#32957;&#27573;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20018;&#32852;&#36136;&#35889;&#65288;MS/MS&#65289;&#20316;&#20026;&#20840;&#38754;&#20998;&#26512;&#29983;&#29289;&#26679;&#26412;&#20013;&#34507;&#30333;&#36136;&#21547;&#37327;&#30340;&#20027;&#35201;&#39640;&#36890;&#37327;&#25216;&#26415;&#65292;&#19968;&#30452;&#26159;&#25512;&#21160;&#34507;&#30333;&#36136;&#32452;&#23398;&#21457;&#23637;&#30340;&#22522;&#30707;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#25968;&#25454;&#26080;&#20559;&#21521;&#37319;&#38598;&#65288;DIA&#65289;&#31574;&#30053;&#26041;&#38754;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#36827;&#23637;&#65292;&#26377;&#21161;&#20110;&#23545;&#21069;&#20307;&#31163;&#23376;&#36827;&#34892;&#20844;&#27491;&#21644;&#38750;&#38774;&#21521;&#30862;&#35010;&#12290;&#30001;&#20110;&#20854;&#22266;&#26377;&#30340;&#39640;&#22810;&#37325;&#24615;&#29305;&#24615;&#65292;DIA&#29983;&#25104;&#30340;MS/MS&#35889;&#22270;&#36896;&#25104;&#20102;&#24040;&#22823;&#38556;&#30861;&#12290;&#27599;&#20010;&#35889;&#22270;&#37117;&#21253;&#21547;&#26469;&#33258;&#22810;&#20010;&#21069;&#20307;&#32957;&#30340;&#30862;&#35010;&#20135;&#29289;&#31163;&#23376;&#12290;&#36825;&#31181;&#22797;&#26434;&#24615;&#29305;&#21035;&#22312;&#20840;&#26032;&#32957;&#27573;/&#34507;&#30333;&#36136;&#27979;&#24207;&#20013;&#26500;&#25104;&#20102;&#19968;&#20010;&#23574;&#38160;&#25361;&#25112;&#65292;&#24403;&#21069;&#26041;&#27861;&#26080;&#27861;&#35299;&#20915;&#22810;&#37325;&#24615;&#38590;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Casanovo-DIA&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;DIA&#36136;&#35889;&#25968;&#25454;&#20013;&#35299;&#26512;&#32957;&#27573;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11363v1 Announce Type: cross  Abstract: Tandem mass spectrometry (MS/MS) stands as the predominant high-throughput technique for comprehensively analyzing protein content within biological samples. This methodology is a cornerstone driving the advancement of proteomics. In recent years, substantial strides have been made in Data-Independent Acquisition (DIA) strategies, facilitating impartial and non-targeted fragmentation of precursor ions. The DIA-generated MS/MS spectra present a formidable obstacle due to their inherent high multiplexing nature. Each spectrum encapsulates fragmented product ions originating from multiple precursor peptides. This intricacy poses a particularly acute challenge in de novo peptide/protein sequencing, where current methods are ill-equipped to address the multiplexing conundrum. In this paper, we introduce Casanovo-DIA, a deep-learning model based on transformer architecture. It deciphers peptide sequences from DIA mass spectrometry data. Our 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#26041;&#27861;&#26469;&#39537;&#21160;AI&#33647;&#29289;&#35774;&#35745;&#65292;&#37325;&#28857;&#20851;&#27880;&#23567;&#20998;&#23376;&#21644;&#34507;&#30333;&#36136;&#29983;&#25104;&#20004;&#20010;&#20027;&#35201;&#20027;&#39064;&#65292;&#20171;&#32461;&#20102;&#21508;&#31181;&#23376;&#20219;&#21153;&#21644;&#24212;&#29992;&#65292;&#24182;&#27604;&#36739;&#20102;&#39030;&#32423;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08703</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#27010;&#36848;&#65306;&#20998;&#23376;&#21644;&#34507;&#30333;&#36136;&#29983;&#25104;&#30340;&#26032;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
A Survey of Generative AI for De Novo Drug Design: New Frontiers in Molecule and Protein Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08703
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#26041;&#27861;&#26469;&#39537;&#21160;AI&#33647;&#29289;&#35774;&#35745;&#65292;&#37325;&#28857;&#20851;&#27880;&#23567;&#20998;&#23376;&#21644;&#34507;&#30333;&#36136;&#29983;&#25104;&#20004;&#20010;&#20027;&#35201;&#20027;&#39064;&#65292;&#20171;&#32461;&#20102;&#21508;&#31181;&#23376;&#20219;&#21153;&#21644;&#24212;&#29992;&#65292;&#24182;&#27604;&#36739;&#20102;&#39030;&#32423;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39537;&#21160;&#30340;&#26041;&#27861;&#21487;&#20197;&#26497;&#22823;&#22320;&#25913;&#36827;&#21382;&#26469;&#20195;&#20215;&#39640;&#26114;&#30340;&#33647;&#29289;&#35774;&#35745;&#36807;&#31243;&#65292;&#21508;&#31181;&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#22312;&#24191;&#27867;&#20351;&#29992;&#20013;&#12290;&#29305;&#21035;&#26159;&#38024;&#23545;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#19987;&#27880;&#20110;&#20174;&#38646;&#24320;&#22987;&#21019;&#24314;&#26032;&#30340;&#29983;&#29289;&#21270;&#21512;&#29289;&#65292;&#23637;&#31034;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;&#35813;&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#21152;&#19978;&#33647;&#29289;&#35774;&#35745;&#36807;&#31243;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;&#20026;&#26032;&#30740;&#31350;&#20154;&#21592;&#36827;&#20837;&#21019;&#36896;&#20102;&#19968;&#20010;&#22256;&#38590;&#30340;&#23616;&#38754;&#12290;&#22312;&#36825;&#20221;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#23558;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#20998;&#20026;&#20004;&#20010;&#20027;&#35201;&#20027;&#39064;&#65306;&#23567;&#20998;&#23376;&#21644;&#34507;&#30333;&#36136;&#29983;&#25104;&#12290;&#22312;&#27599;&#20010;&#20027;&#39064;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#21508;&#31181;&#23376;&#20219;&#21153;&#21644;&#24212;&#29992;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#37325;&#35201;&#30340;&#25968;&#25454;&#38598;&#12289;&#22522;&#20934;&#21644;&#27169;&#22411;&#26550;&#26500;&#65292;&#24182;&#23545;&#39030;&#32423;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#24191;&#20041;&#30340;&#26041;&#27861;&#26469;&#39537;&#21160;AI&#33647;&#29289;&#35774;&#35745;&#65292;&#20801;&#35768;&#22312;&#27599;&#20010;&#23376;&#20219;&#21153;&#20013;&#36827;&#34892;&#21508;&#31181;&#26041;&#27861;&#30340;&#24494;&#35266;&#27604;&#36739;&#21644;&#23439;&#35266;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08703v1 Announce Type: cross Abstract: Artificial intelligence (AI)-driven methods can vastly improve the historically costly drug design process, with various generative models already in widespread use. Generative models for de novo drug design, in particular, focus on the creation of novel biological compounds entirely from scratch, representing a promising future direction. Rapid development in the field, combined with the inherent complexity of the drug design process, creates a difficult landscape for new researchers to enter. In this survey, we organize de novo drug design into two overarching themes: small molecule and protein generation. Within each theme, we identify a variety of subtasks and applications, highlighting important datasets, benchmarks, and model architectures and comparing the performance of top models. We take a broad approach to AI-driven drug design, allowing for both micro-level comparisons of various methods within each subtask and macro-level o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#23558;&#19987;&#23478;&#32452;&#21512;&#27169;&#22359;&#34701;&#20837;&#22522;&#20110;&#20540;&#30340;&#32593;&#32476;&#20013;&#65292;&#23588;&#20854;&#26159;&#36719;MoE&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#20855;&#21442;&#25968;&#21487;&#25193;&#23637;&#24615;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#36825;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#23454;&#35777;&#35777;&#25454;&#20197;&#21457;&#23637;&#24378;&#21270;&#23398;&#20064;&#30340;&#32553;&#25918;&#23450;&#24459;&#12290;</title><link>https://arxiv.org/abs/2402.08609</link><description>&lt;p&gt;
&#19987;&#23478;&#32452;&#21512;&#35299;&#38145;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21442;&#25968;&#32553;&#25918;
&lt;/p&gt;
&lt;p&gt;
Mixtures of Experts Unlock Parameter Scaling for Deep RL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#23558;&#19987;&#23478;&#32452;&#21512;&#27169;&#22359;&#34701;&#20837;&#22522;&#20110;&#20540;&#30340;&#32593;&#32476;&#20013;&#65292;&#23588;&#20854;&#26159;&#36719;MoE&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#20855;&#21442;&#25968;&#21487;&#25193;&#23637;&#24615;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#36825;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#23454;&#35777;&#35777;&#25454;&#20197;&#21457;&#23637;&#24378;&#21270;&#23398;&#20064;&#30340;&#32553;&#25918;&#23450;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#65288;&#33258;&#25105;&#65289;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#24555;&#36895;&#36827;&#23637;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#36890;&#36807;&#23454;&#35777;&#32553;&#25918;&#23450;&#24459;&#39044;&#27979;&#30340;&#65306;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#20854;&#35268;&#27169;&#25104;&#27604;&#20363;&#12290;&#28982;&#32780;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#23547;&#25214;&#31867;&#20284;&#30340;&#32553;&#25918;&#23450;&#24459;&#20173;&#28982;&#22256;&#38590;&#65292;&#22240;&#20026;&#22686;&#21152;&#27169;&#22411;&#30340;&#21442;&#25968;&#25968;&#37327;&#24448;&#24448;&#20250;&#25439;&#23475;&#20854;&#26368;&#32456;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#23558;&#19987;&#23478;&#32452;&#21512;&#65288;MoE&#65289;&#27169;&#22359;&#65292;&#29305;&#21035;&#26159;&#36719;MoE&#65288;Puigcerver&#31561;&#20154;&#65292;2023&#24180;&#65289;&#65292;&#34701;&#20837;&#22522;&#20110;&#20540;&#30340;&#32593;&#32476;&#20013;&#65292;&#21487;&#20197;&#24471;&#21040;&#26356;&#20855;&#21442;&#25968;&#21487;&#25193;&#23637;&#24615;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21508;&#31181;&#35757;&#32451;&#26041;&#26696;&#21644;&#27169;&#22411;&#35268;&#27169;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#21152;&#20197;&#35777;&#26126;&#12290;&#22240;&#27492;&#65292;&#36825;&#39033;&#24037;&#20316;&#20026;&#21457;&#23637;&#24378;&#21270;&#23398;&#20064;&#30340;&#32553;&#25918;&#23450;&#24459;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#23454;&#35777;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent rapid progress in (self) supervised learning models is in large part predicted by empirical scaling laws: a model's performance scales proportionally to its size. Analogous scaling laws remain elusive for reinforcement learning domains, however, where increasing the parameter count of a model often hurts its final performance. In this paper, we demonstrate that incorporating Mixture-of-Expert (MoE) modules, and in particular Soft MoEs (Puigcerver et al., 2023), into value-based networks results in more parameter-scalable models, evidenced by substantial performance increases across a variety of training regimes and model sizes. This work thus provides strong empirical evidence towards developing scaling laws for reinforcement learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30452;&#25509;&#20004;&#20004;&#27604;&#36739;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;LLMs&#30340;&#22122;&#22768;&#21453;&#39304;&#65292;&#30452;&#25509;&#35782;&#21035;&#20986;&#26368;&#26377;&#28508;&#21147;&#30340;&#20013;&#38388;&#24605;&#32500;&#65292;&#20174;&#32780;&#29983;&#25104;&#20248;&#31168;&#30340;&#24605;&#32500;&#38142;&#12290;</title><link>https://arxiv.org/abs/2402.06918</link><description>&lt;p&gt;
&#29992;&#30452;&#25509;&#30340;&#20004;&#20004;&#27604;&#36739;&#26041;&#27861;&#29983;&#25104;&#24605;&#32500;&#38142;&#65292;&#20197;&#25628;&#32034;&#26368;&#26377;&#28508;&#21147;&#30340;&#20013;&#38388;&#24605;&#32500;
&lt;/p&gt;
&lt;p&gt;
Generating Chain-of-Thoughts with a Direct Pairwise-Comparison Approach to Searching for the Most Promising Intermediate Thought
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30452;&#25509;&#20004;&#20004;&#27604;&#36739;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;LLMs&#30340;&#22122;&#22768;&#21453;&#39304;&#65292;&#30452;&#25509;&#35782;&#21035;&#20986;&#26368;&#26377;&#28508;&#21147;&#30340;&#20013;&#38388;&#24605;&#32500;&#65292;&#20174;&#32780;&#29983;&#25104;&#20248;&#31168;&#30340;&#24605;&#32500;&#38142;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#24605;&#32500;&#38142;(Chain-of-Thoughts, CoT)&#26041;&#27861;&#65292;&#29992;&#20110;&#25351;&#23548;LLMs&#36827;&#34892;&#36880;&#27493;&#25512;&#29702;&#65292;&#20174;&#31616;&#21333;&#21040;&#22797;&#26434;&#30340;&#38382;&#39064;&#35299;&#20915;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#36825;&#31181;&#24605;&#32500;&#38142;&#30340;&#26041;&#27861;&#28041;&#21450;&#20114;&#21160;&#21327;&#20316;&#65292;&#23398;&#20064;&#32773;&#29983;&#25104;&#20505;&#36873;&#20013;&#38388;&#24605;&#32500;&#65292;&#30001;LLMs&#35780;&#20272;&#65292;&#24341;&#23548;&#29983;&#25104;&#21518;&#32493;&#24605;&#32500;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#24191;&#27867;&#20294;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#38382;&#39064;&#26159;&#65292;LLMs&#30340;&#35780;&#20272;&#36890;&#24120;&#23384;&#22312;&#22122;&#22768;&#21644;&#19981;&#21487;&#38752;&#24615;&#65292;&#21487;&#33021;&#35823;&#23548;&#29983;&#25104;&#36807;&#31243;&#65292;&#36873;&#25321;&#19981;&#22815;&#26377;&#28508;&#21147;&#30340;&#20013;&#38388;&#24605;&#32500;&#12290;&#26412;&#25991;&#21463;Vapnik&#21407;&#21017;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27604;&#36739;&#30340;CoT&#29983;&#25104;&#31639;&#27861;&#65292;&#30452;&#25509;&#26681;&#25454;LLMs&#30340;&#22122;&#22768;&#21453;&#39304;&#30830;&#23450;&#26368;&#26377;&#28508;&#21147;&#30340;&#24605;&#32500;&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#25105;&#20204;&#38543;&#26426;&#37197;&#23545;&#20013;&#38388;&#24605;&#32500;&#65292;&#24182;&#30452;&#25509;&#20419;&#20351;LLMs&#20174;&#27599;&#23545;&#20013;&#36873;&#25321;&#26356;&#26377;&#28508;&#21147;&#30340;&#24605;&#32500;&#12290;
&lt;/p&gt;
&lt;p&gt;
To improve the ability of the large language model (LLMs) to handle complex reasoning problems, chain-of-thoughts (CoT) methods were proposed to guide LLMs to reason step-by-step, facilitating problem solving from simple to complex tasks. State-of-the-art approaches for generating such a chain involve interactive collaboration, where the learner generates candidate intermediate thoughts, evaluated by the LLM, guiding the generation of subsequent thoughts. However, a widespread yet understudied problem is that the evaluation from the LLM is typically noisy and unreliable, potentially misleading the generation process in selecting promising intermediate thoughts. In this paper, motivated by Vapnik's principle, we propose a novel comparison-based CoT generation algorithm that directly identifies the most promising thoughts with the noisy feedback from the LLM. In each round, we randomly pair intermediate thoughts and directly prompt the LLM to select the more promising one from each pair,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;SPHINX-X&#65292;&#19968;&#31181;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#12290;&#36890;&#36807;&#25913;&#36827;&#26550;&#26500;&#21644;&#35757;&#32451;&#25928;&#29575;&#65292;&#25105;&#20204;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#31995;&#21015;&#21442;&#25968;&#22823;&#23567;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#19981;&#21516;&#30340;MLLMs&#65292;&#19982;&#25968;&#25454;&#21644;&#21442;&#25968;&#35268;&#27169;&#26377;&#24378;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05935</link><description>&lt;p&gt;
SPHINX-X: &#25193;&#23637;&#25968;&#25454;&#21644;&#21442;&#25968;&#29992;&#20110;&#19968;&#31995;&#21015;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;SPHINX-X&#65292;&#19968;&#31181;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#12290;&#36890;&#36807;&#25913;&#36827;&#26550;&#26500;&#21644;&#35757;&#32451;&#25928;&#29575;&#65292;&#25105;&#20204;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#31995;&#21015;&#21442;&#25968;&#22823;&#23567;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#19981;&#21516;&#30340;MLLMs&#65292;&#19982;&#25968;&#25454;&#21644;&#21442;&#25968;&#35268;&#27169;&#26377;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;SPHINX-X&#65292;&#19968;&#31181;&#22522;&#20110;SPHINX&#24320;&#21457;&#30340;&#24191;&#27867;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#31995;&#21015;&#12290;&#20026;&#20102;&#25913;&#21892;&#26550;&#26500;&#21644;&#35757;&#32451;&#25928;&#29575;&#65292;&#25105;&#20204;&#36890;&#36807;&#31227;&#38500;&#20887;&#20313;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#12289;&#32469;&#36807;&#23436;&#20840;&#22635;&#20805;&#30340;&#23376;&#22270;&#20687;&#65292;&#24182;&#23558;&#22810;&#38454;&#27573;&#35757;&#32451;&#31616;&#21270;&#25104;&#20026;&#19968;&#38454;&#27573;&#30340;&#20840;&#38598;&#21512;&#27169;&#24335;&#65292;&#20462;&#25913;&#20102;SPHINX&#26694;&#26550;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;MLLM&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#32452;&#35013;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#36328;&#35821;&#35328;&#12289;&#36328;&#35270;&#35273;&#21644;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#30340;&#22810;&#39046;&#22495;&#12289;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#20844;&#24320;&#21487;&#29992;&#30340;&#36164;&#28304;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#25105;&#20204;&#30340;OCR&#23494;&#38598;&#21644;Mark&#25968;&#25454;&#38598;&#20016;&#23500;&#36825;&#20010;&#25910;&#38598;&#65292;&#25193;&#23637;&#20102;&#22810;&#26679;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#22522;&#30784;LLM&#36827;&#34892;&#35757;&#32451;&#65292;&#21253;&#25324;TinyLlama1.1B&#12289;InternLM2-7B&#12289;LLaMA2-13B&#21644;Mixtral8x7B&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#31995;&#21015;&#21442;&#25968;&#22823;&#23567;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#21464;&#21270;&#30340;MLLMs&#12290;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#25581;&#31034;&#20102;&#22810;&#27169;&#24577;&#24615;&#33021;&#19982;&#25968;&#25454;&#21644;&#21442;&#25968;&#35268;&#27169;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM) series developed upon SPHINX. To improve the architecture and training efficiency, we modify the SPHINX framework by removing redundant visual encoders, bypassing fully-padded sub-images with skip tokens, and simplifying multi-stage training into a one-stage all-in-one paradigm. To fully unleash the potential of MLLMs, we assemble a comprehensive multi-domain and multimodal dataset covering publicly available resources in language, vision, and vision-language tasks. We further enrich this collection with our curated OCR intensive and Set-of-Mark datasets, extending the diversity and generality. By training over different base LLMs including TinyLlama1.1B, InternLM2-7B, LLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in parameter size and multilingual capabilities. Comprehensive benchmarking reveals a strong correlation between the multi-modal performance with the data and parameter scales. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#35299;&#37322;&#26694;&#26550;&#65288;xLLM&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#36890;&#36807;&#19968;&#20010;&#35780;&#20272;&#22120;&#26469;&#37327;&#21270;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#26469;&#25552;&#39640;&#21487;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.04678</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#21487;&#20449;&#30340;&#35299;&#37322;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models As Faithful Explainers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#35299;&#37322;&#26694;&#26550;&#65288;xLLM&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#36890;&#36807;&#19968;&#20010;&#35780;&#20272;&#22120;&#26469;&#37327;&#21270;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#26469;&#25552;&#39640;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36890;&#36807;&#21033;&#29992;&#20854;&#20016;&#23500;&#30340;&#20869;&#37096;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24050;&#32463;&#33021;&#22815;&#29087;&#32451;&#35299;&#20915;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22797;&#26434;&#24615;&#38459;&#30861;&#20102;&#20256;&#32479;&#30340;&#20197;&#36755;&#20837;&#20026;&#37325;&#28857;&#30340;&#35299;&#37322;&#31639;&#27861;&#26469;&#35299;&#37322;LLMs&#30340;&#22797;&#26434;&#20915;&#31574;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#20986;&#29616;&#20102;&#19968;&#31181;&#33258;&#25105;&#35299;&#37322;&#26426;&#21046;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#30340;&#24418;&#24335;&#36827;&#34892;&#21333;&#21521;&#25512;&#29702;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;LLMs&#39044;&#27979;&#30340;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#32463;&#24120;&#22240;&#20026;&#32570;&#20047;&#21487;&#20449;&#24230;&#32780;&#21463;&#21040;&#25209;&#35780;&#65292;&#22240;&#20026;&#36825;&#20123;&#35299;&#37322;&#21487;&#33021;&#19981;&#20934;&#30830;&#22320;&#21453;&#26144;LLMs&#30340;&#20915;&#31574;&#34892;&#20026;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29983;&#25104;&#35299;&#37322;&#26694;&#26550;xLLM&#65292;&#20197;&#25552;&#39640;LLMs&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#30340;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#22120;&#26469;&#37327;&#21270;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#36890;&#36807;xLLM&#30340;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#26469;&#25552;&#39640;&#21487;&#20449;&#24230;&#65292;&#30446;&#26631;&#26159;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have recently become proficient in addressing complex tasks by utilizing their rich internal knowledge and reasoning ability. Consequently, this complexity hinders traditional input-focused explanation algorithms for explaining the complex decision-making processes of LLMs. Recent advancements have thus emerged for self-explaining their predictions through a single feed-forward inference in a natural language format. However, natural language explanations are often criticized for lack of faithfulness since these explanations may not accurately reflect the decision-making behaviors of the LLMs. In this work, we introduce a generative explanation framework, xLLM, to improve the faithfulness of the explanations provided in natural language formats for LLMs. Specifically, we propose an evaluator to quantify the faithfulness of natural language explanation and enhance the faithfulness by an iterative optimization process of xLLM, with the goal of maximizing the 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#39318;&#27425;&#25581;&#31034;&#20102;&#25915;&#20987;&#32773;&#22312;&#22810;&#26234;&#33021;&#20307;&#31454;&#20105;&#29615;&#22659;&#20013;&#21363;&#20351;&#21463;&#38480;&#20110;&#21463;&#23475;&#32773;&#30340;&#37096;&#20998;&#35266;&#27979;&#20063;&#33021;&#29983;&#25104;&#23545;&#25239;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.03741</link><description>&lt;p&gt;
SUB-PLAY&#65306;&#38024;&#23545;&#37096;&#20998;&#35266;&#27979;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#30340;&#23545;&#25239;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
SUB-PLAY: Adversarial Policies against Partially Observed Multi-Agent Reinforcement Learning Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03741
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#39318;&#27425;&#25581;&#31034;&#20102;&#25915;&#20987;&#32773;&#22312;&#22810;&#26234;&#33021;&#20307;&#31454;&#20105;&#29615;&#22659;&#20013;&#21363;&#20351;&#21463;&#38480;&#20110;&#21463;&#23475;&#32773;&#30340;&#37096;&#20998;&#35266;&#27979;&#20063;&#33021;&#29983;&#25104;&#23545;&#25239;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#39046;&#22495;&#21462;&#24471;&#30340;&#36827;&#23637;&#20026;&#26080;&#20154;&#26426;&#30340;&#32676;&#20307;&#25511;&#21046;&#12289;&#26426;&#26800;&#33218;&#30340;&#21327;&#20316;&#25805;&#32437;&#20197;&#21450;&#22810;&#30446;&#26631;&#21253;&#22260;&#31561;&#24320;&#36767;&#20102;&#24191;&#38420;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#22312;MARL&#37096;&#32626;&#36807;&#31243;&#20013;&#23384;&#22312;&#28508;&#22312;&#30340;&#23433;&#20840;&#23041;&#32961;&#38656;&#35201;&#26356;&#22810;&#20851;&#27880;&#21644;&#28145;&#20837;&#35843;&#26597;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36805;&#36895;&#21033;&#29992;&#21463;&#23475;&#32773;&#30340;&#28431;&#27934;&#29983;&#25104;&#23545;&#25239;&#31574;&#30053;&#65292;&#23548;&#33268;&#21463;&#23475;&#32773;&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#22833;&#36133;&#12290;&#20363;&#22914;&#65292;&#23558;&#36229;&#20154;&#32423;&#21035;&#30340;&#22260;&#26827;AI&#30340;&#33719;&#32988;&#29575;&#38477;&#20302;&#21040;&#32422;20%&#12290;&#36825;&#20123;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20004;&#20154;&#31454;&#20105;&#29615;&#22659;&#65292;&#24182;&#20551;&#35774;&#25915;&#20987;&#32773;&#20855;&#26377;&#23436;&#25972;&#30340;&#20840;&#23616;&#29366;&#24577;&#35266;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in multi-agent reinforcement learning (MARL) have opened up vast application prospects, including swarm control of drones, collaborative manipulation by robotic arms, and multi-target encirclement. However, potential security threats during the MARL deployment need more attention and thorough investigation. Recent researches reveal that an attacker can rapidly exploit the victim's vulnerabilities and generate adversarial policies, leading to the victim's failure in specific tasks. For example, reducing the winning rate of a superhuman-level Go AI to around 20%. They predominantly focus on two-player competitive environments, assuming attackers possess complete global state observation.   In this study, we unveil, for the first time, the capability of attackers to generate adversarial policies even when restricted to partial observations of the victims in multi-agent competitive environments. Specifically, we propose a novel black-box attack (SUB-PLAY), which incorporate
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24103;&#19968;&#33268;&#24615;&#21407;&#21017;&#65292;DeCoF&#26159;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#29983;&#25104;&#35270;&#39057;&#26816;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#28040;&#38500;&#31354;&#38388;&#20266;&#24433;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02085</link><description>&lt;p&gt;
DeCoF:&#36890;&#36807;&#24103;&#19968;&#33268;&#24615;&#36827;&#34892;&#29983;&#25104;&#35270;&#39057;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
DeCoF: Generated Video Detection via Frame Consistency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02085
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24103;&#19968;&#33268;&#24615;&#21407;&#21017;&#65292;DeCoF&#26159;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#29983;&#25104;&#35270;&#39057;&#26816;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#28040;&#38500;&#31354;&#38388;&#20266;&#24433;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32423;&#35270;&#39057;&#29983;&#25104;&#26041;&#27861;&#20135;&#29983;&#30340;&#35270;&#39057;&#36136;&#37327;&#19981;&#26029;&#25552;&#39640;&#65292;&#36825;&#23548;&#33268;&#31038;&#20250;&#38754;&#20020;&#26032;&#30340;&#23433;&#20840;&#25361;&#25112;&#65292;&#20351;&#29983;&#25104;&#35270;&#39057;&#26816;&#27979;&#25104;&#20026;&#32039;&#36843;&#30340;&#30740;&#31350;&#37325;&#28857;&#12290;&#20026;&#20419;&#36827;&#36825;&#19968;&#39046;&#22495;&#30340;&#21512;&#20316;&#30740;&#31350;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;&#26126;&#30830;&#29992;&#20110;&#29983;&#25104;&#35270;&#39057;&#26816;&#27979;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#20026;&#31038;&#21306;&#25552;&#20379;&#20102;&#19968;&#20010;&#23453;&#36149;&#30340;&#36164;&#28304;&#65292;&#20197;&#35780;&#20272;&#21644;&#25913;&#36827;&#26816;&#27979;&#26041;&#27861;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#31934;&#24515;&#35774;&#35745;&#30340;&#25506;&#27979;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#26102;&#38388;&#21644;&#31354;&#38388;&#20266;&#24433;&#22312;&#24320;&#21457;&#29983;&#25104;&#35270;&#39057;&#30340;&#36890;&#29992;&#21644;&#31283;&#20581;&#26816;&#27979;&#22120;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;&#22522;&#20110;&#35270;&#39057;&#24103;&#19968;&#33268;&#24615;&#21407;&#21017;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26816;&#27979;&#27169;&#22411;&#65288;DeCoF&#65289;&#65292;&#23427;&#28040;&#38500;&#20102;&#31354;&#38388;&#20266;&#24433;&#22312;&#36890;&#29992;&#29305;&#24449;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;DeCoF&#22312;&#26816;&#27979;&#26410;&#35265;&#36807;&#30340;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#20135;&#29983;&#30340;&#35270;&#39057;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#24182;&#19988;&#39564;&#35777;&#20102;&#20854;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The escalating quality of video generated by advanced video generation methods leads to new security challenges in society, which makes generated video detection an urgent research priority.To foster collaborative research in this area, we construct the first open-source dataset explicitly for generated video detection, providing a valuable resource for the community to benchmark and improve detection methodologies. Through a series of carefully designed probe experiments, our study explores the significance of temporal and spatial artifacts in developing general and robust detectors for generated video. Based on the principle of video frame consistency, we introduce a simple yet effective detection model (DeCoF) that eliminates the impact of spatial artifacts during generalizing feature learning. Our extensive experiments demonstrate the efficacy of DeCoF in detecting videos produced by unseen video generation models and confirm its powerful generalization capabilities across several 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#23558;&#25991;&#26412;&#29983;&#25104;&#24418;&#24335;&#21270;&#20026;&#26410;&#26469;&#21463;&#38480;&#29983;&#25104;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;&#19981;&#33391;&#34892;&#20026;&#24182;&#24378;&#21046;&#25191;&#34892;&#23545;&#25351;&#20196;&#30340;&#24544;&#23454;&#24615;&#65292;&#24182;&#36890;&#36807;LLMs&#26377;&#25928;&#25351;&#23548;&#25991;&#26412;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2312.06149</link><description>&lt;p&gt;
&#35299;&#38145;&#39044;&#27979;&#24615;&#25991;&#26412;&#29983;&#25104;&#65306;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#30721;&#30340;&#21463;&#38480;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unlocking Anticipatory Text Generation: A Constrained Approach for Large Language Models Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06149
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#23558;&#25991;&#26412;&#29983;&#25104;&#24418;&#24335;&#21270;&#20026;&#26410;&#26469;&#21463;&#38480;&#29983;&#25104;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;&#19981;&#33391;&#34892;&#20026;&#24182;&#24378;&#21046;&#25191;&#34892;&#23545;&#25351;&#20196;&#30340;&#24544;&#23454;&#24615;&#65292;&#24182;&#36890;&#36807;LLMs&#26377;&#25928;&#25351;&#23548;&#25991;&#26412;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#32473;&#23450;&#25552;&#31034;&#25110;&#25351;&#20196;&#23454;&#29616;&#26368;&#20339;&#32467;&#26524;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21313;&#20159;&#32423;&#21035;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#19981;&#33391;&#34892;&#20026;&#22914;&#27602;&#24615;&#25110;&#24187;&#35273;&#21487;&#33021;&#20250;&#26174;&#29616;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#25991;&#26412;&#29983;&#25104;&#24418;&#24335;&#21270;&#20026;&#26410;&#26469;&#21463;&#38480;&#29983;&#25104;&#38382;&#39064;&#65292;&#20197;&#26368;&#23567;&#21270;&#19981;&#33391;&#34892;&#20026;&#24182;&#24378;&#21046;&#25191;&#34892;&#23545;&#25351;&#20196;&#30340;&#24544;&#23454;&#24615;&#12290;&#20351;&#29992;LLMs&#23454;&#29616;&#26410;&#26469;&#32422;&#26463;&#28385;&#36275;&#24230;&#30340;&#20272;&#35745;&#24341;&#23548;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65306;&#20851;&#38190;&#35789;&#21463;&#38480;&#29983;&#25104;&#12289;&#27602;&#24615;&#20943;&#23569;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06149v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have demonstrated a powerful ability for text generation. However, achieving optimal results with a given prompt or instruction can be challenging, especially for billion-sized models. Additionally, undesired behaviors such as toxicity or hallucinations can manifest. While much larger models (e.g., ChatGPT) may demonstrate strength in mitigating these issues, there is still no guarantee of complete prevention. In this work, we propose formalizing text generation as a future-constrained generation problem to minimize undesirable behaviors and enforce faithfulness to instructions. The estimation of future constraint satisfaction, accomplished using LLMs, guides the text generation process. Our extensive experiments demonstrate the effectiveness of the proposed approach across three distinct text generation tasks: keyword-constrained generation (Lin et al., 2020), toxicity reduction (Gehman et al., 202
&lt;/p&gt;</description></item><item><title>FLex&amp;Chill &#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;Logit Chilling&#26041;&#27861;&#25913;&#36827;&#26412;&#22320;&#32852;&#21512;&#23398;&#20064;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21152;&#24555;&#27169;&#22411;&#25910;&#25947;&#24182;&#25552;&#39640;&#25512;&#29702;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.09986</link><description>&lt;p&gt;
FLex&amp;Chill&#65306;&#36890;&#36807;Logit Chilling&#25913;&#36827;&#26412;&#22320;&#32852;&#21512;&#23398;&#20064;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
FLex&amp;Chill: Improving Local Federated Learning Training with Logit Chilling. (arXiv:2401.09986v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09986
&lt;/p&gt;
&lt;p&gt;
FLex&amp;Chill &#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;Logit Chilling&#26041;&#27861;&#25913;&#36827;&#26412;&#22320;&#32852;&#21512;&#23398;&#20064;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21152;&#24555;&#27169;&#22411;&#25910;&#25947;&#24182;&#25552;&#39640;&#25512;&#29702;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23398;&#20064;&#30001;&#20110;&#26412;&#22320;&#23458;&#25143;&#31471;&#30340;&#38750;iid&#20998;&#24067;&#24335;&#35757;&#32451;&#25968;&#25454;&#32780;&#21463;&#21040;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#38459;&#30861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#21512;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;FLex&amp;Chill&#65292;&#21033;&#29992;&#20102;Logit Chilling&#26041;&#27861;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#32852;&#21512;&#23398;&#20064;&#31995;&#32479;&#20013;&#22266;&#26377;&#30340;&#38750;iid&#25968;&#25454;&#29305;&#24449;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21152;&#24555;&#27169;&#22411;&#25910;&#25947;&#24182;&#25552;&#39640;&#25512;&#29702;&#31934;&#24230;&#12290;&#20174;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20840;&#23616;&#32852;&#21512;&#23398;&#20064;&#27169;&#22411;&#25910;&#25947;&#26102;&#38388;&#25552;&#39640;&#20102;6&#20493;&#65292;&#25512;&#29702;&#31934;&#24230;&#25552;&#39640;&#20102;3.37%&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning are inherently hampered by data heterogeneity: non-iid distributed training data over local clients. We propose a novel model training approach for federated learning, FLex&amp;Chill, which exploits the Logit Chilling method. Through extensive evaluations, we demonstrate that, in the presence of non-iid data characteristics inherent in federated learning systems, this approach can expedite model convergence and improve inference accuracy. Quantitatively, from our experiments, we observe up to 6X improvement in the global federated learning model convergence time, and up to 3.37% improvement in inference accuracy.
&lt;/p&gt;</description></item><item><title>ODIN&#26159;&#19968;&#20010;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#23545;2D RGB&#22270;&#20687;&#21644;3D&#28857;&#20113;&#36827;&#34892;&#20998;&#21106;&#21644;&#26631;&#35760;&#65292;&#20351;&#29992;&#21464;&#21387;&#22120;&#26550;&#26500;&#36827;&#34892;2D&#21644;3D&#35270;&#22270;&#38388;&#30340;&#20449;&#24687;&#34701;&#21512;&#12290;</title><link>http://arxiv.org/abs/2401.02416</link><description>&lt;p&gt;
ODIN: &#19968;&#20010;&#29992;&#20110;2D&#21644;3D&#24863;&#30693;&#30340;&#21333;&#19968;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ODIN: A Single Model for 2D and 3D Perception. (arXiv:2401.02416v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02416
&lt;/p&gt;
&lt;p&gt;
ODIN&#26159;&#19968;&#20010;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#23545;2D RGB&#22270;&#20687;&#21644;3D&#28857;&#20113;&#36827;&#34892;&#20998;&#21106;&#21644;&#26631;&#35760;&#65292;&#20351;&#29992;&#21464;&#21387;&#22120;&#26550;&#26500;&#36827;&#34892;2D&#21644;3D&#35270;&#22270;&#38388;&#30340;&#20449;&#24687;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#20808;&#36827;&#27169;&#22411;&#22312;&#20687;ScanNet&#36825;&#26679;&#30340;&#24403;&#20195;3D&#24863;&#30693;&#22522;&#20934;&#19978;&#20351;&#29992;&#24182;&#26631;&#35760;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#25552;&#20379;&#30340;3D&#28857;&#20113;&#65292;&#35813;&#28857;&#20113;&#26159;&#36890;&#36807;&#23545;&#24863;&#30693;&#21040;&#30340;&#22810;&#35270;&#35282;RGB-D&#22270;&#20687;&#36827;&#34892;&#21518;&#22788;&#29702;&#33719;&#24471;&#30340;&#12290;&#23427;&#20204;&#36890;&#24120;&#22312;&#39046;&#22495;&#20869;&#36827;&#34892;&#35757;&#32451;&#65292;&#25918;&#24323;&#20102;&#22823;&#35268;&#27169;&#30340;2D&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#32988;&#36807;&#23558;&#23039;&#24577;RGB-D&#22810;&#35270;&#35282;&#22270;&#20687;&#36827;&#34892;&#29305;&#24449;&#21270;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#28040;&#32791;&#23039;&#24577;&#22270;&#20687;&#21644;&#21518;&#22788;&#29702;&#30340;3D&#28857;&#20113;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#21152;&#21095;&#20102;2D&#21644;3D&#24863;&#30693;&#38656;&#35201;&#19981;&#21516;&#27169;&#22411;&#26550;&#26500;&#30340;&#35266;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#36825;&#20010;&#35266;&#28857;&#65292;&#24182;&#25552;&#20986;ODIN&#65288;Omni-Dimensional INstance segmentation&#65289;&#65292;&#19968;&#31181;&#33021;&#22815;&#20351;&#29992;&#21464;&#21387;&#22120;&#26550;&#26500;&#23545;2D RGB&#22270;&#20687;&#21644;3D&#28857;&#20113;&#36827;&#34892;&#20998;&#21106;&#21644;&#26631;&#35760;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#20132;&#26367;&#30340;2D&#35270;&#22270;&#20869;&#21644;3D&#35270;&#22270;&#38388;&#20449;&#24687;&#34701;&#21512;&#26469;&#21306;&#20998;2D&#21644;3D&#29305;&#24449;&#25805;&#20316;&#65292;&#21033;&#29992;&#28041;&#21450;&#30340;&#20196;&#29260;&#30340;&#20301;&#32622;&#32534;&#30721;&#26469;&#25429;&#25417;2D&#34917;&#19969;&#20196;&#29260;&#21644;3D&#22352;&#26631;&#30340;&#20687;&#32032;&#22352;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art models on contemporary 3D perception benchmarks like ScanNet consume and label dataset-provided 3D point clouds, obtained through post processing of sensed multiview RGB-D images. They are typically trained in-domain, forego large-scale 2D pre-training and outperform alternatives that featurize the posed RGB-D multiview images instead. The gap in performance between methods that consume posed images versus post-processed 3D point clouds has fueled the belief that 2D and 3D perception require distinct model architectures. In this paper, we challenge this view and propose ODIN (Omni-Dimensional INstance segmentation), a model that can segment and label both 2D RGB images and 3D point clouds, using a transformer architecture that alternates between 2D within-view and 3D cross-view information fusion. Our model differentiates 2D and 3D feature operations through the positional encodings of the tokens involved, which capture pixel coordinates for 2D patch tokens and 3D coor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;&#21644;&#27169;&#20223;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#37197;&#23545;&#20248;&#21270;&#35302;&#35273;&#21147;&#37327;&#26354;&#32447;&#21644;&#31616;&#21270;&#20256;&#24863;&#22120;&#24212;&#29992;&#65292;&#23545;&#25509;&#35302;&#20016;&#23500;&#30340;&#25805;&#20316;&#20219;&#21153;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2311.01248</link><description>&lt;p&gt;
&#23558;&#20854;&#25512;&#21521;&#23637;&#31034;&#26497;&#38480;&#65306;&#22810;&#27169;&#24577;&#35270;&#35273;&#35302;&#35273;&#27169;&#20223;&#23398;&#20064;&#19982;&#21147;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Push it to the Demonstrated Limit: Multimodal Visuotactile Imitation Learning with Force Matching. (arXiv:2311.01248v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;&#21644;&#27169;&#20223;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#37197;&#23545;&#20248;&#21270;&#35302;&#35273;&#21147;&#37327;&#26354;&#32447;&#21644;&#31616;&#21270;&#20256;&#24863;&#22120;&#24212;&#29992;&#65292;&#23545;&#25509;&#35302;&#20016;&#23500;&#30340;&#25805;&#20316;&#20219;&#21153;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#23398;&#35302;&#35273;&#20256;&#24863;&#22120;&#24050;&#32463;&#25104;&#20026;&#26426;&#22120;&#20154;&#25805;&#20316;&#36807;&#31243;&#20013;&#33719;&#21462;&#23494;&#38598;&#25509;&#35302;&#20449;&#24687;&#30340;&#26377;&#25928;&#25163;&#27573;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#8220;&#36879;&#35270;&#20320;&#30340;&#30382;&#32932;&#8221;&#65288;STS&#65289;&#22411;&#20256;&#24863;&#22120;&#20855;&#26377;&#35270;&#35273;&#21644;&#35302;&#35273;&#27169;&#24335;&#65292;&#36890;&#36807;&#21033;&#29992;&#21322;&#36879;&#26126;&#34920;&#38754;&#21644;&#21487;&#25511;&#29031;&#26126;&#23454;&#29616;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#19982;&#27169;&#20223;&#23398;&#20064;&#22312;&#23500;&#26377;&#25509;&#35302;&#30340;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;&#22909;&#22788;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#35302;&#35273;&#21147;&#27979;&#37327;&#21644;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#22312;&#36816;&#21160;&#31034;&#33539;&#20013;&#20135;&#29983;&#26356;&#22909;&#21305;&#37197;&#20154;&#20307;&#31034;&#33539;&#32773;&#30340;&#21147;&#26354;&#32447;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#28155;&#21152;&#20102;&#35270;&#35273;/&#35302;&#35273;STS&#27169;&#24335;&#20999;&#25442;&#20316;&#20026;&#25511;&#21046;&#31574;&#30053;&#36755;&#20986;&#65292;&#31616;&#21270;&#20256;&#24863;&#22120;&#30340;&#24212;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#31181;&#35266;&#23519;&#37197;&#32622;&#65292;&#27604;&#36739;&#21644;&#23545;&#27604;&#20102;&#35270;&#35273;/&#35302;&#35273;&#25968;&#25454;&#65288;&#21253;&#25324;&#27169;&#24335;&#20999;&#25442;&#21644;&#19981;&#20999;&#25442;&#65289;&#19982;&#25163;&#33109;&#25346;&#36733;&#30340;&#30524;&#22312;&#25163;&#25668;&#20687;&#26426;&#30340;&#35270;&#35273;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#24191;&#27867;&#30340;&#23454;&#39564;&#31995;&#21015;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optical tactile sensors have emerged as an effective means to acquire dense contact information during robotic manipulation. A recently-introduced `see-through-your-skin' (STS) variant of this type of sensor has both visual and tactile modes, enabled by leveraging a semi-transparent surface and controllable lighting. In this work, we investigate the benefits of pairing visuotactile sensing with imitation learning for contact-rich manipulation tasks. First, we use tactile force measurements and a novel algorithm during kinesthetic teaching to yield a force profile that better matches that of the human demonstrator. Second, we add visual/tactile STS mode switching as a control policy output, simplifying the application of the sensor. Finally, we study multiple observation configurations to compare and contrast the value of visual/tactile data (both with and without mode switching) with visual data from a wrist-mounted eye-in-hand camera. We perform an extensive series of experiments on a
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#26088;&#22312;&#21516;&#26102;&#27169;&#25311;&#35821;&#20041;&#21644;&#21327;&#21516;&#30693;&#35782;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;CTR&#20272;&#35745;&#65292;&#24182;&#35299;&#20915;&#25512;&#29702;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.09234</link><description>&lt;p&gt;
ClickPrompt: CTR&#27169;&#22411;&#26159;&#23558;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#20026;CTR&#39044;&#27979;&#30340;&#24378;&#22823;&#25552;&#31034;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
ClickPrompt: CTR Models are Strong Prompt Generators for Adapting Language Models to CTR Prediction. (arXiv:2310.09234v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09234
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#26088;&#22312;&#21516;&#26102;&#27169;&#25311;&#35821;&#20041;&#21644;&#21327;&#21516;&#30693;&#35782;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;CTR&#20272;&#35745;&#65292;&#24182;&#35299;&#20915;&#25512;&#29702;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#20114;&#32852;&#32593;&#24212;&#29992;&#31243;&#24207;&#20013;&#36234;&#26469;&#36234;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#20256;&#32479;&#30340;CTR&#27169;&#22411;&#36890;&#36807;&#29420;&#28909;&#32534;&#30721;&#23558;&#22810;&#23383;&#27573;&#20998;&#31867;&#25968;&#25454;&#36716;&#25442;&#20026;ID&#29305;&#24449;&#65292;&#24182;&#25552;&#21462;&#29305;&#24449;&#20043;&#38388;&#30340;&#21327;&#21516;&#20449;&#21495;&#12290;&#36825;&#31181;&#33539;&#24335;&#30340;&#38382;&#39064;&#22312;&#20110;&#35821;&#20041;&#20449;&#24687;&#30340;&#20002;&#22833;&#12290;&#21478;&#19968;&#26041;&#38754;&#30340;&#30740;&#31350;&#36890;&#36807;&#23558;&#36755;&#20837;&#25968;&#25454;&#36716;&#25442;&#20026;&#25991;&#26412;&#21477;&#23376;&#26469;&#25506;&#32034;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#22312;CTR&#39044;&#27979;&#20013;&#30340;&#28508;&#21147;&#12290;&#34429;&#28982;&#35821;&#20041;&#20449;&#21495;&#24471;&#21040;&#20102;&#20445;&#30041;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#26080;&#27861;&#25429;&#25417;&#21040;&#21327;&#21516;&#20449;&#24687;&#65288;&#22914;&#29305;&#24449;&#20132;&#20114;&#12289;&#32431;ID&#29305;&#24449;&#65289;&#65292;&#26356;&#19981;&#29992;&#35828;&#30001;&#24222;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#24102;&#26469;&#30340;&#26080;&#27861;&#25509;&#21463;&#30340;&#25512;&#29702;&#24320;&#38144;&#20102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#20026;&#20934;&#30830;&#30340;CTR&#20272;&#35745;&#24314;&#31435;&#35821;&#20041;&#30693;&#35782;&#21644;&#21327;&#21516;&#30693;&#35782;&#65292;&#24182;&#35299;&#20915;&#25512;&#29702;&#25928;&#29575;&#38382;&#39064;&#12290;&#20026;&#20102;&#20174;&#20004;&#20010;&#39046;&#22495;&#20013;&#21463;&#30410;&#24182;&#24357;&#21512;&#23427;&#20204;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;-&#12290;
&lt;/p&gt;
&lt;p&gt;
Click-through rate (CTR) prediction has become increasingly indispensable for various Internet applications. Traditional CTR models convert the multi-field categorical data into ID features via one-hot encoding, and extract the collaborative signals among features. Such a paradigm suffers from the problem of semantic information loss. Another line of research explores the potential of pretrained language models (PLMs) for CTR prediction by converting input data into textual sentences through hard prompt templates. Although semantic signals are preserved, they generally fail to capture the collaborative information (e.g., feature interactions, pure ID features), not to mention the unacceptable inference overhead brought by the huge model size. In this paper, we aim to model both the semantic knowledge and collaborative knowledge for accurate CTR estimation, and meanwhile address the inference inefficiency issue. To benefit from both worlds and close their gaps, we propose a novel model-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30452;&#25509;&#21033;&#29992;&#37197;&#23545;&#30340;OSM&#25968;&#25454;&#21644;&#20809;&#23398;&#22270;&#20687;&#36827;&#34892;&#22303;&#22320;&#35206;&#30422;&#21464;&#21270;&#26816;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#35937;&#24341;&#23548;&#30340;Transformer&#26550;&#26500;&#65292;&#20174;&#32780;&#25299;&#23485;&#20102;&#21464;&#21270;&#26816;&#27979;&#20219;&#21153;&#30340;&#33539;&#22260;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#20102;&#35745;&#31639;&#24320;&#38144;&#21644;&#20869;&#23384;&#36127;&#25285;&#12290;</title><link>http://arxiv.org/abs/2310.02674</link><description>&lt;p&gt;
&#21033;&#29992;&#37197;&#23545;&#30340;OpenStreetMap&#25968;&#25454;&#21644;&#20809;&#23398;&#39640;&#20998;&#36776;&#29575;&#24433;&#20687;&#36827;&#34892;&#22303;&#22320;&#35206;&#30422;&#21464;&#21270;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Land-cover change detection using paired OpenStreetMap data and optical high-resolution imagery via object-guided Transformer. (arXiv:2310.02674v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30452;&#25509;&#21033;&#29992;&#37197;&#23545;&#30340;OSM&#25968;&#25454;&#21644;&#20809;&#23398;&#22270;&#20687;&#36827;&#34892;&#22303;&#22320;&#35206;&#30422;&#21464;&#21270;&#26816;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#35937;&#24341;&#23548;&#30340;Transformer&#26550;&#26500;&#65292;&#20174;&#32780;&#25299;&#23485;&#20102;&#21464;&#21270;&#26816;&#27979;&#20219;&#21153;&#30340;&#33539;&#22260;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#20102;&#35745;&#31639;&#24320;&#38144;&#21644;&#20869;&#23384;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#23398;&#39640;&#20998;&#36776;&#29575;&#24433;&#20687;&#21644;OpenStreetMap&#65288;OSM&#65289;&#25968;&#25454;&#26159;&#22303;&#22320;&#35206;&#30422;&#21464;&#21270;&#26816;&#27979;&#30340;&#20004;&#20010;&#37325;&#35201;&#25968;&#25454;&#28304;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#21033;&#29992;OSM&#25968;&#25454;&#26469;&#36741;&#21161;&#22810;&#26102;&#26399;&#20809;&#23398;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#21464;&#21270;&#26816;&#27979;&#12290;&#26412;&#25991;&#36890;&#36807;&#30452;&#25509;&#21033;&#29992;&#37197;&#23545;&#30340;OSM&#25968;&#25454;&#21644;&#20809;&#23398;&#22270;&#20687;&#36827;&#34892;&#22303;&#22320;&#35206;&#30422;&#21464;&#21270;&#26816;&#27979;&#65292;&#25299;&#23485;&#20102;&#21464;&#21270;&#26816;&#27979;&#20219;&#21153;&#30340;&#33539;&#22260;&#65292;&#28085;&#30422;&#26356;&#22810;&#21160;&#24577;&#22320;&#29699;&#35266;&#27979;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#35937;&#24341;&#23548;&#30340;Transformer&#65288;ObjFormer&#65289;&#26550;&#26500;&#65292;&#23558;&#27969;&#34892;&#30340;&#22522;&#20110;&#23545;&#35937;&#30340;&#22270;&#20687;&#20998;&#26512;&#65288;OBIA&#65289;&#25216;&#26415;&#19982;&#20808;&#36827;&#30340;&#35270;&#35273;Transformer&#26550;&#26500;&#33258;&#28982;&#22320;&#32467;&#21512;&#36215;&#26469;&#12290;&#24341;&#20837;OBIA&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#20013;&#30340;&#35745;&#31639;&#24320;&#38144;&#21644;&#20869;&#23384;&#36127;&#25285;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;ObjFormer&#20855;&#26377;&#23618;&#27425;&#20266;&#23402;&#29983;&#32534;&#30721;&#22120;&#65292;&#21253;&#21547;&#23545;&#35937;&#24341;&#23548;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#29992;&#20110;&#25552;&#21462;&#20195;&#34920;&#24615;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optical high-resolution imagery and OpenStreetMap (OSM) data are two important data sources for land-cover change detection. Previous studies in these two data sources focus on utilizing the information in OSM data to aid the change detection on multi-temporal optical high-resolution images. This paper pioneers the direct detection of land-cover changes utilizing paired OSM data and optical imagery, thereby broadening the horizons of change detection tasks to encompass more dynamic earth observations. To this end, we propose an object-guided Transformer (ObjFormer) architecture by naturally combining the prevalent object-based image analysis (OBIA) technique with the advanced vision Transformer architecture. The introduction of OBIA can significantly reduce the computational overhead and memory burden in the self-attention module. Specifically, the proposed ObjFormer has a hierarchical pseudo-siamese encoder consisting of object-guided self-attention modules that extract representative
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#31574;&#30053;&#24615;&#20080;&#23478;&#30340;&#24773;&#22659;&#21160;&#24577;&#23450;&#20215;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#21160;&#24577;&#23450;&#20215;&#31574;&#30053;&#65292;&#23558;&#20080;&#23478;&#30340;&#31574;&#30053;&#34892;&#20026;&#32435;&#20837;&#22312;&#32447;&#23398;&#20064;&#20013;&#65292;&#20197;&#26368;&#22823;&#21270;&#21334;&#26041;&#30340;&#32047;&#35745;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2307.04055</link><description>&lt;p&gt;
&#20855;&#26377;&#31574;&#30053;&#24615;&#20080;&#23478;&#30340;&#24773;&#22659;&#21160;&#24577;&#23450;&#20215;
&lt;/p&gt;
&lt;p&gt;
Contextual Dynamic Pricing with Strategic Buyers. (arXiv:2307.04055v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#31574;&#30053;&#24615;&#20080;&#23478;&#30340;&#24773;&#22659;&#21160;&#24577;&#23450;&#20215;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#21160;&#24577;&#23450;&#20215;&#31574;&#30053;&#65292;&#23558;&#20080;&#23478;&#30340;&#31574;&#30053;&#34892;&#20026;&#32435;&#20837;&#22312;&#32447;&#23398;&#20064;&#20013;&#65292;&#20197;&#26368;&#22823;&#21270;&#21334;&#26041;&#30340;&#32047;&#35745;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#23450;&#20215;&#26159;&#20225;&#19994;&#24120;&#29992;&#30340;&#19968;&#31181;&#38024;&#23545;&#20010;&#20307;&#29305;&#24449;&#21046;&#23450;&#20215;&#26684;&#30340;&#31574;&#30053;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#20080;&#23478;&#20063;&#21487;&#20197;&#36890;&#36807;&#25805;&#32437;&#29305;&#24449;&#25968;&#25454;&#26469;&#33719;&#21462;&#26356;&#20302;&#30340;&#20215;&#26684;&#65292;&#20294;&#36825;&#20063;&#20250;&#23548;&#33268;&#29305;&#23450;&#30340;&#25805;&#20316;&#25104;&#26412;&#12290;&#36825;&#31181;&#31574;&#30053;&#34892;&#20026;&#21487;&#33021;&#20250;&#38459;&#30861;&#20225;&#19994;&#26368;&#22823;&#21270;&#21033;&#28070;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#31574;&#30053;&#24615;&#20080;&#23478;&#30340;&#24773;&#22659;&#21160;&#24577;&#23450;&#20215;&#38382;&#39064;&#12290;&#21334;&#26041;&#26080;&#27861;&#35266;&#23519;&#21040;&#20080;&#23478;&#30340;&#30495;&#23454;&#29305;&#24449;&#65292;&#32780;&#21482;&#33021;&#35266;&#23519;&#21040;&#20080;&#23478;&#26681;&#25454;&#31574;&#30053;&#34892;&#20026;&#25805;&#32437;&#21518;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#21334;&#26041;&#21482;&#33021;&#35266;&#23519;&#21040;&#20080;&#23478;&#23545;&#20135;&#21697;&#30340;&#20272;&#20540;&#65292;&#32780;&#26080;&#27861;&#30452;&#25509;&#33719;&#21462;&#20855;&#20307;&#25968;&#20540;&#65292;&#21482;&#33021;&#24471;&#21040;&#19968;&#20010;&#20108;&#36827;&#21046;&#30340;&#21709;&#24212;&#65292;&#34920;&#31034;&#26159;&#21542;&#21457;&#29983;&#38144;&#21806;&#12290;&#37492;&#20110;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#21160;&#24577;&#23450;&#20215;&#31574;&#30053;&#65292;&#23558;&#20080;&#23478;&#30340;&#31574;&#30053;&#34892;&#20026;&#32435;&#20837;&#22312;&#32447;&#23398;&#20064;&#20013;&#65292;&#20197;&#26368;&#22823;&#21270;&#21334;&#26041;&#30340;&#32047;&#35745;&#25910;&#30410;&#12290;&#39318;&#20808;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#19981;&#32771;&#34385;&#31574;&#30053;&#24615;&#30340;&#23450;&#20215;&#31574;&#30053;&#30340;&#23384;&#22312;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized pricing, which involves tailoring prices based on individual characteristics, is commonly used by firms to implement a consumer-specific pricing policy. In this process, buyers can also strategically manipulate their feature data to obtain a lower price, incurring certain manipulation costs. Such strategic behavior can hinder firms from maximizing their profits. In this paper, we study the contextual dynamic pricing problem with strategic buyers. The seller does not observe the buyer's true feature, but a manipulated feature according to buyers' strategic behavior. In addition, the seller does not observe the buyers' valuation of the product, but only a binary response indicating whether a sale happens or not. Recognizing these challenges, we propose a strategic dynamic pricing policy that incorporates the buyers' strategic behavior into the online learning to maximize the seller's cumulative revenue. We first prove that existing non-strategic pricing policies that neglect
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#24418;&#24577;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#39640;&#25928;&#30340;&#22240;&#24335;&#20998;&#35299;&#32593;&#32476;&#26469;&#29702;&#35299;&#35270;&#35273;&#22330;&#26223;&#24182;&#25512;&#26029;&#29289;&#20307;&#21644;&#23039;&#21183;&#12290;&#20851;&#38190;&#21019;&#26032;&#21253;&#25324;&#22522;&#20110;&#22797;&#20540;&#21521;&#37327;&#30340;&#35745;&#31639;&#26694;&#26550;VSA&#12289;&#29992;&#20110;&#22788;&#29702;&#24179;&#31227;&#21644;&#26059;&#36716;&#30340;&#20998;&#23618;&#35856;&#25391;&#22120;&#32593;&#32476;HRN&#35774;&#35745;&#65292;&#20197;&#21450;&#22312;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#19978;&#23454;&#29616;&#22797;&#20540;&#35856;&#25391;&#22120;&#32593;&#32476;&#30340;&#22810;&#32452;&#20998;&#33033;&#20914;&#30456;&#20301;&#31070;&#32463;&#20803;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2208.12880</link><description>&lt;p&gt;
&#20855;&#26377;&#35856;&#25391;&#22120;&#32593;&#32476;&#30340;&#31070;&#32463;&#24418;&#24577;&#35270;&#35273;&#22330;&#26223;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Neuromorphic Visual Scene Understanding with Resonator Networks. (arXiv:2208.12880v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12880
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#24418;&#24577;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#39640;&#25928;&#30340;&#22240;&#24335;&#20998;&#35299;&#32593;&#32476;&#26469;&#29702;&#35299;&#35270;&#35273;&#22330;&#26223;&#24182;&#25512;&#26029;&#29289;&#20307;&#21644;&#23039;&#21183;&#12290;&#20851;&#38190;&#21019;&#26032;&#21253;&#25324;&#22522;&#20110;&#22797;&#20540;&#21521;&#37327;&#30340;&#35745;&#31639;&#26694;&#26550;VSA&#12289;&#29992;&#20110;&#22788;&#29702;&#24179;&#31227;&#21644;&#26059;&#36716;&#30340;&#20998;&#23618;&#35856;&#25391;&#22120;&#32593;&#32476;HRN&#35774;&#35745;&#65292;&#20197;&#21450;&#22312;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#19978;&#23454;&#29616;&#22797;&#20540;&#35856;&#25391;&#22120;&#32593;&#32476;&#30340;&#22810;&#32452;&#20998;&#33033;&#20914;&#30456;&#20301;&#31070;&#32463;&#20803;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#35270;&#35273;&#22330;&#26223;&#24182;&#25512;&#26029;&#20854;&#21508;&#20010;&#29289;&#20307;&#30340;&#36523;&#20221;&#21644;&#23039;&#21183;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#24418;&#24577;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#21033;&#29992;&#20102;&#22522;&#20110;&#19977;&#20010;&#20851;&#38190;&#27010;&#24565;&#30340;&#39640;&#25928;&#30340;&#22240;&#24335;&#20998;&#35299;&#32593;&#32476;&#65306;&#65288;1&#65289;&#22522;&#20110;&#22797;&#20540;&#21521;&#37327;&#30340;&#30690;&#37327;&#31526;&#21495;&#20307;&#31995;&#26550;&#26500;(VSA)&#30340;&#35745;&#31639;&#26694;&#26550;&#65307;&#65288;2&#65289;&#29992;&#20110;&#22788;&#29702;&#35270;&#35273;&#22330;&#26223;&#20013;&#24179;&#31227;&#21644;&#26059;&#36716;&#30340;&#38750;&#21487;&#20132;&#25442;&#24615;&#30340;&#20998;&#23618;&#35856;&#25391;&#22120;&#32593;&#32476;&#65288;HRN&#65289;&#30340;&#35774;&#35745;&#65292;&#24403;&#20004;&#32773;&#32467;&#21512;&#20351;&#29992;&#26102;&#65307;&#65288;3&#65289;&#35774;&#35745;&#20102;&#19968;&#31181;&#22810;&#32452;&#20998;&#33033;&#20914;&#30456;&#20301;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#19978;&#23454;&#29616;&#22797;&#20540;&#35856;&#25391;&#22120;&#32593;&#32476;&#12290;VSA&#26694;&#26550;&#20351;&#29992;&#30690;&#37327;&#32465;&#23450;&#25805;&#20316;&#26469;&#20135;&#29983;&#29983;&#25104;&#24335;&#22270;&#20687;&#27169;&#22411;&#65292;&#20854;&#20013;&#32465;&#23450;&#20316;&#20026;&#20960;&#20309;&#21464;&#25442;&#30340;&#31561;&#21464;&#25805;&#20316;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#22330;&#26223;&#21487;&#20197;&#34987;&#25551;&#36848;&#20026;&#21521;&#37327;&#20056;&#31215;&#30340;&#21644;&#65292;&#32780;&#36825;&#20123;&#21521;&#37327;&#20056;&#31215;&#21487;&#20197;&#36890;&#36807;&#35856;&#25391;&#22120;&#32593;&#32476;&#30340;&#22240;&#24335;&#20998;&#35299;&#26469;&#39640;&#25928;&#22320;&#25512;&#26029;&#29289;&#20307;&#21644;&#23427;&#20204;&#30340;&#23039;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding a visual scene by inferring identities and poses of its individual objects is still and open problem. Here we propose a neuromorphic solution that utilizes an efficient factorization network based on three key concepts: (1) a computational framework based on Vector Symbolic Architectures (VSA) with complex-valued vectors; (2) the design of Hierarchical Resonator Networks (HRN) to deal with the non-commutative nature of translation and rotation in visual scenes, when both are used in combination; (3) the design of a multi-compartment spiking phasor neuron model for implementing complex-valued resonator networks on neuromorphic hardware. The VSA framework uses vector binding operations to produce generative image models in which binding acts as the equivariant operation for geometric transformations. A scene can therefore be described as a sum of vector products, which in turn can be efficiently factorized by a resonator network to infer objects and their poses. The HRN ena
&lt;/p&gt;</description></item></channel></rss>