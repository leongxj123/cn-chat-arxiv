<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#22914;&#20309;&#20174;&#20855;&#26377;&#32454;&#31890;&#24230;&#27010;&#24565;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#27010;&#24565;&#65292;&#20197;&#23454;&#29616;&#27169;&#22411;&#36755;&#20986;&#30340;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00912</link><description>&lt;p&gt;
&#33021;&#22815;&#32422;&#26463;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#23398;&#20064;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#36755;&#20837;&#29305;&#24449;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can we Constrain Concept Bottleneck Models to Learn Semantically Meaningful Input Features?
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#22914;&#20309;&#20174;&#20855;&#26377;&#32454;&#31890;&#24230;&#27010;&#24565;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#27010;&#24565;&#65292;&#20197;&#23454;&#29616;&#27169;&#22411;&#36755;&#20986;&#30340;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBM&#65289;&#34987;&#35748;&#20026;&#20855;&#26377;&#20869;&#22312;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#39318;&#20808;&#39044;&#27979;&#19968;&#32452;&#20154;&#20026;&#23450;&#20041;&#30340;&#27010;&#24565;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#27010;&#24565;&#26469;&#39044;&#27979;&#19979;&#28216;&#20219;&#21153;&#30340;&#36755;&#20986;&#12290;&#20026;&#20102;&#23454;&#29616;&#23436;&#20840;&#30340;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#21450;&#30830;&#20445;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#20449;&#20219;&#65292;&#25105;&#20204;&#38656;&#35201;&#20445;&#35777;&#27010;&#24565;&#30340;&#39044;&#27979;&#26159;&#22522;&#20110;&#35821;&#20041;&#26144;&#23556;&#30340;&#36755;&#20837;&#29305;&#24449;&#12290;&#20363;&#22914;&#65292;&#20154;&#20204;&#21487;&#33021;&#26399;&#26395;&#22270;&#20687;&#20013;&#34920;&#31034;&#39592;&#25240;&#30340;&#20687;&#32032;&#34987;&#29992;&#20110;&#39044;&#27979;&#39592;&#25240;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#25991;&#29486;&#34920;&#26126;&#36825;&#24182;&#19981;&#26159;&#20107;&#23454;&#65292;&#22240;&#20026;&#27010;&#24565;&#39044;&#27979;&#36890;&#24120;&#19982;&#19981;&#30456;&#20851;&#30340;&#36755;&#20837;&#29305;&#24449;&#26144;&#23556;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#26159;&#30001;&#20110;&#27010;&#24565;&#27880;&#37322;&#30340;&#19981;&#20934;&#30830;&#25110;&#32773;&#36755;&#20837;&#29305;&#24449;&#19982;&#27010;&#24565;&#20043;&#38388;&#30340;&#20851;&#31995;&#19981;&#28165;&#26224;&#23548;&#33268;&#30340;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25968;&#25454;&#38598;&#26631;&#27880;&#23545;CBMs&#20013;&#27010;&#24565;&#34920;&#31034;&#30340;&#24433;&#21709;&#20173;&#28982;&#26159;&#19968;&#20010;&#30740;&#31350;&#36739;&#23569;&#30340;&#39046;&#22495;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;CBMs&#22914;&#20309;&#20174;&#20855;&#26377;&#32454;&#31890;&#24230;&#27010;&#24565;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#27010;&#24565;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept Bottleneck Models (CBMs) are considered inherently interpretable because they first predict a set of human-defined concepts before using these concepts to predict the output of a downstream task. For inherent interpretability to be fully realised, and ensure trust in a model's output, we need to guarantee concepts are predicted based on semantically mapped input features. For example, one might expect the pixels representing a broken bone in an image to be used for the prediction of a fracture. However, current literature indicates this is not the case, as concept predictions are often mapped to irrelevant input features. We hypothesise that this occurs when concept annotations are inaccurate or how input features should relate to concepts is unclear. In general, the effect of dataset labelling on concept representations in CBMs remains an understudied area. Therefore, in this paper, we examine how CBMs learn concepts from datasets with fine-grained concept annotations. We demo
&lt;/p&gt;</description></item><item><title>&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#23545;&#31867;&#27604;&#25512;&#29702;&#12289;&#21453;&#24605;&#25512;&#29702;&#12289;&#21333;&#35789;&#39044;&#27979;&#21644;&#35821;&#27861;&#21028;&#26029;&#30340;&#34920;&#29616;&#21463;&#36741;&#21161;&#20219;&#21153;&#38656;&#27714;&#30340;&#24433;&#21709;&#65292;&#35780;&#20272;&#26041;&#27861;&#30340;&#20219;&#21153;&#38656;&#27714;&#36234;&#22823;&#65292;&#24615;&#33021;&#36234;&#20302;&#65292;&#36825;&#31181;"&#38656;&#27714;&#24046;&#36317;"&#22312;&#21442;&#25968;&#36739;&#23569;&#12289;&#35757;&#32451;&#25968;&#25454;&#36739;&#23569;&#30340;&#27169;&#22411;&#20013;&#23588;&#20026;&#26174;&#33879;</title><link>https://arxiv.org/abs/2404.02418</link><description>&lt;p&gt;
&#36741;&#21161;&#20219;&#21153;&#38656;&#27714;&#25513;&#30422;&#20102;&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Auxiliary task demands mask the capabilities of smaller language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02418
&lt;/p&gt;
&lt;p&gt;
&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#23545;&#31867;&#27604;&#25512;&#29702;&#12289;&#21453;&#24605;&#25512;&#29702;&#12289;&#21333;&#35789;&#39044;&#27979;&#21644;&#35821;&#27861;&#21028;&#26029;&#30340;&#34920;&#29616;&#21463;&#36741;&#21161;&#20219;&#21153;&#38656;&#27714;&#30340;&#24433;&#21709;&#65292;&#35780;&#20272;&#26041;&#27861;&#30340;&#20219;&#21153;&#38656;&#27714;&#36234;&#22823;&#65292;&#24615;&#33021;&#36234;&#20302;&#65292;&#36825;&#31181;"&#38656;&#27714;&#24046;&#36317;"&#22312;&#21442;&#25968;&#36739;&#23569;&#12289;&#35757;&#32451;&#25968;&#25454;&#36739;&#23569;&#30340;&#27169;&#22411;&#20013;&#23588;&#20026;&#26174;&#33879;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#24515;&#29702;&#23398;&#23478;&#20204;&#23545;&#35748;&#30693;&#33021;&#21147;&#22914;&#35821;&#35328;&#29702;&#35299;&#25110;&#24515;&#28789;&#29702;&#35770;&#20309;&#26102;&#20986;&#29616;&#36827;&#34892;&#20102;&#20105;&#35770;&#12290;&#36825;&#20123;&#36777;&#35770;&#24120;&#24120;&#20851;&#27880;"&#20219;&#21153;&#38656;&#27714;"&#30340;&#27010;&#24565;--&#25191;&#34892;&#29305;&#23450;&#35780;&#20272;&#26102;&#25152;&#20276;&#38543;&#30340;&#36741;&#21161;&#25361;&#25112;--&#36825;&#20123;&#25361;&#25112;&#21487;&#33021;&#25513;&#30422;&#20102;&#20799;&#31461;&#30340;&#28508;&#22312;&#33021;&#21147;&#12290;&#24403;&#34913;&#37327;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#33021;&#21147;&#26102;&#65292;&#21516;&#26679;&#30340;&#38382;&#39064;&#20063;&#20250;&#20986;&#29616;&#65306;&#20219;&#21153;&#34920;&#29616;&#21462;&#20915;&#20110;&#27169;&#22411;&#30340;&#22522;&#26412;&#33021;&#21147;&#65292;&#32467;&#21512;&#20102;&#27169;&#22411;&#35299;&#37322;&#21644;&#25191;&#34892;&#20219;&#21153;&#30340;&#33021;&#21147;&#20197;&#21450;&#20854;&#21487;&#29992;&#36164;&#28304;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#31867;&#27604;&#25512;&#29702;&#12289;&#21453;&#24605;&#25512;&#29702;&#12289;&#21333;&#35789;&#39044;&#27979;&#21644;&#35821;&#27861;&#21028;&#26029;&#65292;&#20855;&#26377;&#26356;&#22823;&#20219;&#21153;&#38656;&#27714;&#30340;&#35780;&#20272;&#26041;&#27861;&#20250;&#27604;&#38477;&#20302;&#38656;&#27714;&#30340;&#35780;&#20272;&#24471;&#21040;&#26356;&#20302;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;"&#38656;&#27714;&#24046;&#36317;"&#22312;&#21442;&#25968;&#36739;&#23569;&#12289;&#35757;&#32451;&#25968;&#25454;&#36739;&#23569;&#30340;&#27169;&#22411;&#20013;&#26368;&#20026;&#26174;&#33879;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;LM&#30340;&#24615;&#33021;&#19981;&#24212;&#34987;&#35299;&#37322;&#20026;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02418v1 Announce Type: cross  Abstract: Developmental psychologists have argued about when cognitive capacities such as language understanding or theory of mind emerge. These debates often hinge on the concept of "task demands" -- the auxiliary challenges associated with performing a particular evaluation -- that may mask the child's underlying ability. The same issues arise when measuring the capacities of language models (LMs): performance on a task is a function of the model's underlying competence, combined with the model's ability to interpret and perform the task given its available resources. Here, we show that for analogical reasoning, reflective reasoning, word prediction, and grammaticality judgments, evaluation methods with greater task demands yield lower performance than evaluations with reduced demands. This "demand gap" is most pronounced for models with fewer parameters and less training data. Our results illustrate that LM performance should not be interpret
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#31243;&#24207;&#21270;&#24314;&#27169;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#22330;&#26223;&#29983;&#25104;&#26694;&#26550;SceneX&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#31243;&#24207;&#21270;&#27169;&#22411;</title><link>https://arxiv.org/abs/2403.15698</link><description>&lt;p&gt;
SceneX&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31243;&#24207;&#21270;&#21487;&#25511;&#22823;&#35268;&#27169;&#22330;&#26223;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SceneX:Procedural Controllable Large-scale Scene Generation via Large-language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15698
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#31243;&#24207;&#21270;&#24314;&#27169;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#22330;&#26223;&#29983;&#25104;&#26694;&#26550;SceneX&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#31243;&#24207;&#21270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#22823;&#35268;&#27169;&#22330;&#26223;&#29983;&#25104;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#37319;&#29992;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#21019;&#24314;&#25152;&#38656;&#30340;&#22330;&#26223;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#20351;&#29992;&#19981;&#20860;&#23481;&#24037;&#19994;&#27969;&#31243;&#30340;3D&#22522;&#20803;&#65288;&#22914;&#28857;&#20113;&#25110;&#36752;&#23556;&#22330;&#65289;&#26469;&#34920;&#31034;&#22330;&#26223;&#65292;&#36825;&#23548;&#33268;&#23398;&#26415;&#30740;&#31350;&#19982;&#24037;&#19994;&#37096;&#32626;&#20043;&#38388;&#23384;&#22312;&#37325;&#22823;&#24046;&#36317;&#12290;&#31243;&#24207;&#21270;&#21487;&#25511;&#29983;&#25104;&#65288;PCG&#65289;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#25216;&#26415;&#65292;&#21487;&#21019;&#24314;&#21487;&#25193;&#23637;&#21644;&#39640;&#36136;&#37327;&#30340;&#36164;&#20135;&#65292;&#20294;&#23545;&#26222;&#36890;&#29992;&#25143;&#19981;&#21451;&#22909;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#28145;&#20837;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39537;&#21160;&#31243;&#24207;&#21270;&#24314;&#27169;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#22330;&#26223;&#29983;&#25104;&#26694;&#26550;SceneX&#65292;&#21487;&#20197;&#26681;&#25454;&#35774;&#35745;&#24072;&#30340;&#25991;&#26412;&#25551;&#36848;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#31243;&#24207;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15698v1 Announce Type: cross  Abstract: Due to its great application potential, large-scale scene generation has drawn extensive attention in academia and industry. Recent research employs powerful generative models to create desired scenes and achieves promising results. However, most of these methods represent the scene using 3D primitives (e.g. point cloud or radiance field) incompatible with the industrial pipeline, which leads to a substantial gap between academic research and industrial deployment. Procedural Controllable Generation (PCG) is an efficient technique for creating scalable and high-quality assets, but it is unfriendly for ordinary users as it demands profound domain expertise. To address these issues, we resort to using the large language model (LLM) to drive the procedural modeling. In this paper, we introduce a large-scale scene generation framework, SceneX, which can automatically produce high-quality procedural models according to designers' textual de
&lt;/p&gt;</description></item><item><title>Particip-AI &#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20174;&#38750;&#19987;&#19994;&#20844;&#20247;&#37027;&#37324;&#25910;&#38598;&#24403;&#21069;&#21644;&#26410;&#26469;&#30340;&#20154;&#24037;&#26234;&#33021;&#20351;&#29992;&#24773;&#20917;&#12289;&#21361;&#23475;&#21644;&#30410;&#22788;&#65292;&#24341;&#39046;&#20154;&#24037;&#26234;&#33021;&#30340;&#27665;&#20027;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.14791</link><description>&lt;p&gt;
Particip-AI: &#19968;&#31181;&#27665;&#20027;&#35843;&#26597;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#26410;&#26469;&#20154;&#24037;&#26234;&#33021;&#30340;&#20351;&#29992;&#24773;&#20917;&#12289;&#21361;&#23475;&#21644;&#30410;&#22788;
&lt;/p&gt;
&lt;p&gt;
Particip-AI: A Democratic Surveying Framework for Anticipating Future AI Use Cases, Harms and Benefits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14791
&lt;/p&gt;
&lt;p&gt;
Particip-AI &#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20174;&#38750;&#19987;&#19994;&#20844;&#20247;&#37027;&#37324;&#25910;&#38598;&#24403;&#21069;&#21644;&#26410;&#26469;&#30340;&#20154;&#24037;&#26234;&#33021;&#20351;&#29992;&#24773;&#20917;&#12289;&#21361;&#23475;&#21644;&#30410;&#22788;&#65292;&#24341;&#39046;&#20154;&#24037;&#26234;&#33021;&#30340;&#27665;&#20027;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#65292;&#22914;ChatGPT&#65292;&#20284;&#20046;&#38477;&#20302;&#20102;&#20844;&#20247;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#21450;&#21033;&#29992;&#20854;&#21147;&#37327;&#30340;&#38376;&#27099;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#27835;&#29702;&#21644;&#21457;&#23637;&#20173;&#25484;&#25569;&#22312;&#23569;&#25968;&#20154;&#25163;&#20013;&#65292;&#21457;&#23637;&#36895;&#24230;&#21152;&#24555;&#19988;&#32570;&#20047;&#39118;&#38505;&#35780;&#20272;&#12290;&#20316;&#20026;&#36808;&#21521;&#20154;&#24037;&#26234;&#33021;&#27665;&#20027;&#27835;&#29702;&#21644;&#39118;&#38505;&#35780;&#20272;&#30340;&#31532;&#19968;&#27493;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Particip-AI&#65292;&#19968;&#20010;&#26694;&#26550;&#29992;&#20110;&#20174;&#38750;&#19987;&#19994;&#20844;&#20247;&#37027;&#37324;&#25910;&#38598;&#24403;&#21069;&#21644;&#23558;&#26469;&#30340;&#20154;&#24037;&#26234;&#33021;&#20351;&#29992;&#24773;&#20917;&#21450;&#20854;&#21361;&#23475;&#21644;&#30410;&#22788;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#36890;&#36807;&#25910;&#38598;&#20351;&#29992;&#24773;&#20917;&#26356;&#21152;&#32454;&#33268;&#21644;&#35814;&#32454;&#22320;&#30740;&#31350;&#20844;&#20247;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#24847;&#35265;&#65292;&#36890;&#36807;&#22312;&#22791;&#36873;&#26041;&#26696;&#19979;&#65288;&#21363;&#24320;&#21457;&#21644;&#19981;&#24320;&#21457;&#19968;&#31181;&#20351;&#29992;&#24773;&#20917;&#65289;&#36827;&#34892;&#39118;&#38505;&#35780;&#20272;&#21576;&#29616;&#20986;&#22810;&#26679;&#21270;&#30340;&#21361;&#23475;&#65292;&#24182;&#36890;&#36807;&#20570;&#20986;&#23545;&#20854;&#21457;&#23637;&#30340;&#32467;&#35770;&#24615;&#36873;&#25321;&#38416;&#26126;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#30340;&#32039;&#24352;&#20851;&#31995;&#12290;&#20026;&#23637;&#31034;&#25105;&#20204;&#30340;&#26694;&#26550;&#23545;&#25351;&#23548;&#27665;&#20027;&#20154;&#24037;&#26234;&#33021;&#30340;&#25215;&#35834;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#26469;&#33258;295&#20010;&#20154;&#21475;&#22810;&#26679;&#21270;&#30340;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14791v1 Announce Type: cross  Abstract: General purpose AI, such as ChatGPT, seems to have lowered the barriers for the public to use AI and harness its power. However, the governance and development of AI still remain in the hands of a few, and the pace of development is accelerating without proper assessment of risks. As a first step towards democratic governance and risk assessment of AI, we introduce Particip-AI, a framework to gather current and future AI use cases and their harms and benefits from non-expert public. Our framework allows us to study more nuanced and detailed public opinions on AI through collecting use cases, surfacing diverse harms through risk assessment under alternate scenarios (i.e., developing and not developing a use case), and illuminating tensions over AI development through making a concluding choice on its development. To showcase the promise of our framework towards guiding democratic AI, we gather responses from 295 demographically diverse 
&lt;/p&gt;</description></item><item><title>&#35843;&#26597;&#20102;&#22810;&#26234;&#33021;&#20307;&#12289;&#20154;&#26234;&#33021;&#20307;&#21644;&#20154;&#24037;&#26234;&#33021;&#26234;&#33021;&#20307;&#22312;&#31038;&#20250;&#22256;&#22659;&#21512;&#20316;&#20013;&#30340;&#19977;&#20010;&#20851;&#38190;&#39046;&#22495;&#65292;&#35752;&#35770;&#20102;&#21512;&#20316;&#30340;&#21160;&#26426;&#12289;&#31574;&#30053;&#12289;&#20154;&#31867;&#20559;&#35265;&#65292;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.17270</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#12289;&#20154;&#26234;&#33021;&#20307;&#21450;&#20854;&#36827;&#23637;&#65306;&#21512;&#20316;&#22312;&#31038;&#20250;&#22256;&#22659;&#20013;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent, Human-Agent and Beyond: A Survey on Cooperation in Social Dilemmas
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17270
&lt;/p&gt;
&lt;p&gt;
&#35843;&#26597;&#20102;&#22810;&#26234;&#33021;&#20307;&#12289;&#20154;&#26234;&#33021;&#20307;&#21644;&#20154;&#24037;&#26234;&#33021;&#26234;&#33021;&#20307;&#22312;&#31038;&#20250;&#22256;&#22659;&#21512;&#20316;&#20013;&#30340;&#19977;&#20010;&#20851;&#38190;&#39046;&#22495;&#65292;&#35752;&#35770;&#20102;&#21512;&#20316;&#30340;&#21160;&#26426;&#12289;&#31574;&#30053;&#12289;&#20154;&#31867;&#20559;&#35265;&#65292;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20250;&#22256;&#22659;&#20013;&#30740;&#31350;&#21512;&#20316;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#21508;&#31181;&#23398;&#31185;&#30340;&#22522;&#26412;&#35838;&#39064;&#65292;&#21253;&#25324;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#12290;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#26174;&#33879;&#37325;&#22609;&#20102;&#36825;&#19968;&#39046;&#22495;&#65292;&#20026;&#29702;&#35299;&#21644;&#22686;&#24378;&#21512;&#20316;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;&#26412;&#35843;&#26597;&#32771;&#23519;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#31038;&#20250;&#22256;&#22659;&#21512;&#20316;&#20132;&#27719;&#22788;&#30340;&#19977;&#20010;&#20851;&#38190;&#39046;&#22495;&#12290;&#39318;&#20808;&#65292;&#30528;&#37325;&#20110;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;&#25903;&#25345;&#29702;&#24615;&#26234;&#33021;&#20307;&#20043;&#38388;&#21512;&#20316;&#30340;&#20869;&#22312;&#21644;&#22806;&#22312;&#21160;&#26426;&#65292;&#20197;&#21450;&#29992;&#20110;&#21046;&#23450;&#26377;&#25928;&#31574;&#30053;&#23545;&#25239;&#19981;&#21516;&#23545;&#25163;&#30340;&#26041;&#27861;&#12290;&#20854;&#27425;&#65292;&#25506;&#35752;&#20102;&#20154;&#26234;&#33021;&#20307;&#21512;&#20316;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24403;&#21069;&#29992;&#20110;&#19982;&#20154;&#31867;&#21512;&#20316;&#30340;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#65292;&#20197;&#21450;&#20154;&#31867;&#23545;&#20154;&#24037;&#26234;&#33021;&#26234;&#33021;&#20307;&#30340;&#20559;&#35265;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#26234;&#33021;&#20307;&#22686;&#24378;&#20154;&#31867;&#21512;&#20316;&#30340;&#26032;&#20852;&#39046;&#22495;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#65292;&#20363;&#22914; u
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17270v1 Announce Type: new  Abstract: The study of cooperation within social dilemmas has long been a fundamental topic across various disciplines, including computer science and social science. Recent advancements in Artificial Intelligence (AI) have significantly reshaped this field, offering fresh insights into understanding and enhancing cooperation. This survey examines three key areas at the intersection of AI and cooperation in social dilemmas. First, focusing on multi-agent cooperation, we review the intrinsic and external motivations that support cooperation among rational agents, and the methods employed to develop effective strategies against diverse opponents. Second, looking into human-agent cooperation, we discuss the current AI algorithms for cooperating with humans and the human biases towards AI agents. Third, we review the emergent field of leveraging AI agents to enhance cooperation among humans. We conclude by discussing future research avenues, such as u
&lt;/p&gt;</description></item><item><title>GigaPevt&#26159;&#31532;&#19968;&#20010;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#19987;&#19994;&#21307;&#30103;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#21307;&#30103;&#21161;&#25163;&#65292;&#22312;&#23545;&#35805;&#36136;&#37327;&#21644;&#24230;&#37327;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#26126;&#26174;&#20248;&#21183;&#65292;&#24182;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;1.18\%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.16654</link><description>&lt;p&gt;
GigaPevt&#65306;&#22810;&#27169;&#24577;&#21307;&#30103;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
GigaPevt: Multimodal Medical Assistant
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16654
&lt;/p&gt;
&lt;p&gt;
GigaPevt&#26159;&#31532;&#19968;&#20010;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#19987;&#19994;&#21307;&#30103;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#21307;&#30103;&#21161;&#25163;&#65292;&#22312;&#23545;&#35805;&#36136;&#37327;&#21644;&#24230;&#37327;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#26126;&#26174;&#20248;&#21183;&#65292;&#24182;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;1.18\%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#19968;&#20010;&#26234;&#33021;&#39640;&#25928;&#30340;&#21307;&#30103;&#21161;&#25163;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#12290;&#20027;&#35201;&#38480;&#21046;&#26469;&#33258;&#25968;&#25454;&#27169;&#24577;&#30340;&#31232;&#32570;&#24615;&#65292;&#38477;&#20302;&#20102;&#20840;&#38754;&#30340;&#24739;&#32773;&#24863;&#30693;&#12290;&#26412;&#28436;&#31034;&#35770;&#25991;&#20171;&#32461;&#20102;GigaPevt&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#35805;&#21151;&#33021;&#21644;&#19987;&#19994;&#21307;&#30103;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#21307;&#30103;&#21161;&#25163;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#23545;&#35805;&#36136;&#37327;&#21644;&#24230;&#37327;&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#65292;&#20351;&#24471;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;1.18\%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16654v1 Announce Type: cross  Abstract: Building an intelligent and efficient medical assistant is still a challenging AI problem. The major limitation comes from the data modality scarceness, which reduces comprehensive patient perception. This demo paper presents the GigaPevt, the first multimodal medical assistant that combines the dialog capabilities of large language models with specialized medical models. Such an approach shows immediate advantages in dialog quality and metric performance, with a 1.18\% accuracy improvement in the question-answering task.
&lt;/p&gt;</description></item><item><title>GenNBV&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#36890;&#29992;&#30340;&#19979;&#19968;&#26368;&#20339;&#35270;&#35282;&#31574;&#30053;&#65292;&#36890;&#36807;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#21644;&#25193;&#23637;&#21040;5D&#33258;&#30001;&#31354;&#38388;&#30340;&#21160;&#20316;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#26080;&#20154;&#26426;&#20174;&#20219;&#24847;&#35270;&#35282;&#36827;&#34892;&#25195;&#25551;&#65292;&#29978;&#33267;&#19982;&#26410;&#30693;&#20960;&#20309;&#20307;&#36827;&#34892;&#20132;&#20114;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#22810;&#28304;&#29366;&#24577;&#23884;&#20837;&#20197;&#22686;&#24378;&#36328;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.16174</link><description>&lt;p&gt;
GenNBV: &#36890;&#29992;&#30340;&#20027;&#21160;&#24335;&#19977;&#32500;&#37325;&#24314;&#19979;&#19968;&#26368;&#20339;&#35270;&#35282;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
GenNBV: Generalizable Next-Best-View Policy for Active 3D Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16174
&lt;/p&gt;
&lt;p&gt;
GenNBV&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#36890;&#29992;&#30340;&#19979;&#19968;&#26368;&#20339;&#35270;&#35282;&#31574;&#30053;&#65292;&#36890;&#36807;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#21644;&#25193;&#23637;&#21040;5D&#33258;&#30001;&#31354;&#38388;&#30340;&#21160;&#20316;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#26080;&#20154;&#26426;&#20174;&#20219;&#24847;&#35270;&#35282;&#36827;&#34892;&#25195;&#25551;&#65292;&#29978;&#33267;&#19982;&#26410;&#30693;&#20960;&#20309;&#20307;&#36827;&#34892;&#20132;&#20114;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#22810;&#28304;&#29366;&#24577;&#23884;&#20837;&#20197;&#22686;&#24378;&#36328;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#25216;&#26415;&#36827;&#27493;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#22330;&#26223;&#30340;&#30495;&#23454;&#25968;&#23383;&#21270;, &#20294;&#26159;&#22270;&#20687;&#25429;&#33719;&#36807;&#31243;&#20173;&#28982;&#32791;&#26102;&#19988;&#21171;&#21160;&#23494;&#38598;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#23581;&#35797;&#20351;&#29992;&#20027;&#21160;&#24335;&#19977;&#32500;&#37325;&#24314;&#30340;&#19979;&#19968;&#26368;&#20339;&#35270;&#35282;&#65288;NBV&#65289;&#31574;&#30053;&#26469;&#33258;&#21160;&#21270;&#36825;&#19968;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;NBV&#31574;&#30053;&#20005;&#37325;&#20381;&#36182;&#25163;&#24037;&#35774;&#35745;&#30340;&#26631;&#20934;&#12289;&#26377;&#38480;&#30340;&#21160;&#20316;&#31354;&#38388;&#65292;&#25110;&#32773;&#26159;&#38024;&#23545;&#27599;&#20010;&#22330;&#26223;&#20248;&#21270;&#30340;&#34920;&#31034;&#12290;&#36825;&#20123;&#32422;&#26463;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#36328;&#25968;&#25454;&#38598;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GenNBV&#65292;&#19968;&#20010;&#31471;&#21040;&#31471;&#36890;&#29992;&#30340;NBV&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#37319;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#26694;&#26550;&#65292;&#23558;&#20856;&#22411;&#26377;&#38480;&#30340;&#21160;&#20316;&#31354;&#38388;&#25193;&#23637;&#21040;5D&#33258;&#30001;&#31354;&#38388;&#12290;&#23427;&#36171;&#20104;&#20102;&#25105;&#20204;&#30340;&#20195;&#29702;&#26426;&#26080;&#20154;&#26426;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#20197;&#20174;&#20219;&#20309;&#35270;&#35282;&#36827;&#34892;&#25195;&#25551;&#65292;&#29978;&#33267;&#21487;&#20197;&#19982;&#26410;&#35265;&#20960;&#20309;&#20307;&#36827;&#34892;&#20132;&#20114;&#12290;&#20026;&#20102;&#22686;&#24378;&#36328;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#28304;&#29366;&#24577;&#23884;&#20837;&#65292;&#21253;&#25324;&#20960;&#20309;&#12289;&#35821;&#20041;&#21644;&#21160;&#20316;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16174v1 Announce Type: cross  Abstract: While recent advances in neural radiance field enable realistic digitization for large-scale scenes, the image-capturing process is still time-consuming and labor-intensive. Previous works attempt to automate this process using the Next-Best-View (NBV) policy for active 3D reconstruction. However, the existing NBV policies heavily rely on hand-crafted criteria, limited action space, or per-scene optimized representations. These constraints limit their cross-dataset generalizability. To overcome them, we propose GenNBV, an end-to-end generalizable NBV policy. Our policy adopts a reinforcement learning (RL)-based framework and extends typical limited action space to 5D free space. It empowers our agent drone to scan from any viewpoint, and even interact with unseen geometries during training. To boost the cross-dataset generalizability, we also propose a novel multi-source state embedding, including geometric, semantic, and action repres
&lt;/p&gt;</description></item><item><title>C-RAG&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#35748;&#35777;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39118;&#38505;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#31526;&#21512;&#39118;&#38505;&#20998;&#26512;&#21644;&#29983;&#25104;&#39118;&#38505;&#30340;&#19978;&#30028;&#65292;&#30830;&#20445;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#20449;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03181</link><description>&lt;p&gt;
C-RAG: &#38024;&#23545;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#35777;&#29983;&#25104;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03181
&lt;/p&gt;
&lt;p&gt;
C-RAG&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#35748;&#35777;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39118;&#38505;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#31526;&#21512;&#39118;&#38505;&#20998;&#26512;&#21644;&#29983;&#25104;&#39118;&#38505;&#30340;&#19978;&#30028;&#65292;&#30830;&#20445;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#20449;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#20855;&#22791;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#22914;&#24187;&#35273;&#21644;&#38169;&#20301;&#12290;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65288;RAG&#65289;&#34987;&#25552;&#20986;&#26469;&#22686;&#24378;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#20449;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#12290;&#20294;&#26159;&#65292;&#23545;&#20110;RAG&#27169;&#22411;&#30340;&#29983;&#25104;&#39118;&#38505;&#30340;&#29702;&#35770;&#29702;&#35299;&#23578;&#26410;&#34987;&#30740;&#31350;&#12290;&#26412;&#25991;&#22238;&#31572;&#20102;&#20197;&#19979;&#38382;&#39064;&#65306;1&#65289;RAG&#26159;&#21542;&#30830;&#23454;&#33021;&#22815;&#38477;&#20302;&#29983;&#25104;&#39118;&#38505;&#65292;2&#65289;&#22914;&#20309;&#23545;RAG&#21644;&#20256;&#32479;LLM&#30340;&#29983;&#25104;&#39118;&#38505;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#65292;&#20197;&#21450;3&#65289;&#21738;&#20123;&#20805;&#20998;&#26465;&#20214;&#20351;&#24471;RAG&#27169;&#22411;&#33021;&#22815;&#38477;&#20302;&#29983;&#25104;&#39118;&#38505;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;C-RAG&#65292;&#31532;&#19968;&#20010;&#29992;&#20110;&#35748;&#35777;RAG&#27169;&#22411;&#29983;&#25104;&#39118;&#38505;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20026;RAG&#27169;&#22411;&#25552;&#20379;&#20102;&#31526;&#21512;&#39118;&#38505;&#20998;&#26512;&#65292;&#24182;&#30830;&#20445;&#20102;&#29983;&#25104;&#39118;&#38505;&#30340;&#19978;&#30028;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#31526;&#21512;&#29983;&#25104;&#39118;&#38505;&#12290;&#25105;&#20204;&#36824;&#23545;&#19968;&#33324;&#26377;&#30028;&#39118;&#38505;&#19979;&#30340;&#31526;&#21512;&#29983;&#25104;&#39118;&#38505;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the impressive capabilities of large language models (LLMs) across diverse applications, they still suffer from trustworthiness issues, such as hallucinations and misalignments. Retrieval-augmented language models (RAG) have been proposed to enhance the credibility of generations by grounding external knowledge, but the theoretical understandings of their generation risks remains unexplored. In this paper, we answer: 1) whether RAG can indeed lead to low generation risks, 2) how to provide provable guarantees on the generation risks of RAG and vanilla LLMs, and 3) what sufficient conditions enable RAG models to reduce generation risks. We propose C-RAG, the first framework to certify generation risks for RAG models. Specifically, we provide conformal risk analysis for RAG models and certify an upper confidence bound of generation risks, which we refer to as conformal generation risk. We also provide theoretical guarantees on conformal generation risks for general bounded risk f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GEEL&#30340;&#26032;&#22411;&#12289;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#22270;&#34920;&#31034;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#37051;&#25509;&#30697;&#38453;&#22823;&#23567;&#21644;&#35789;&#27719;&#37327;&#65292;&#21516;&#26102;&#36890;&#36807;&#33410;&#28857;&#20301;&#32622;&#32534;&#30721;&#23454;&#29616;&#33258;&#22238;&#24402;&#29983;&#25104;&#65292;&#24182;&#38024;&#23545;&#23646;&#24615;&#22270;&#35774;&#35745;&#20102;&#26032;&#30340;&#25193;&#23637;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2312.02230</link><description>&lt;p&gt;
&#19968;&#31181;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#22270;&#29983;&#25104;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
A Simple and Scalable Representation for Graph Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02230
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GEEL&#30340;&#26032;&#22411;&#12289;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#22270;&#34920;&#31034;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#37051;&#25509;&#30697;&#38453;&#22823;&#23567;&#21644;&#35789;&#27719;&#37327;&#65292;&#21516;&#26102;&#36890;&#36807;&#33410;&#28857;&#20301;&#32622;&#32534;&#30721;&#23454;&#29616;&#33258;&#22238;&#24402;&#29983;&#25104;&#65292;&#24182;&#38024;&#23545;&#23646;&#24615;&#22270;&#35774;&#35745;&#20102;&#26032;&#30340;&#25193;&#23637;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#20204;&#23545;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22270;&#29983;&#25104;&#20135;&#29983;&#20102;&#27987;&#21402;&#20852;&#36259;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#20851;&#38190;&#24212;&#29992;&#20215;&#20540;&#30340;&#22522;&#26412;&#32479;&#35745;&#23398;&#20064;&#38382;&#39064;&#65292;&#22914;&#20998;&#23376;&#35774;&#35745;&#21644;&#31038;&#21306;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#22312;&#29983;&#25104;&#22823;&#35268;&#27169;&#22270;&#26102;&#36935;&#21040;&#20102;&#37325;&#22823;&#38480;&#21046;&#12290;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#38656;&#35201;&#36755;&#20986;&#38543;&#30528;&#33410;&#28857;&#25968;&#37327;&#21576;&#20108;&#27425;&#22686;&#38271;&#30340;&#23436;&#25972;&#37051;&#25509;&#30697;&#38453;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#22270;&#34920;&#31034;&#65292;&#21517;&#20026;&#38388;&#38553;&#32534;&#30721;&#36793;&#21015;&#34920;&#65288;GEEL&#65289;&#65292;&#20854;&#34920;&#31034;&#22823;&#23567;&#36739;&#23567;&#19988;&#19982;&#36793;&#25968;&#37327;&#19968;&#33268;&#12290;&#27492;&#22806;&#65292;GEEL&#36890;&#36807;&#32467;&#21512;&#38388;&#38553;&#32534;&#30721;&#21644;&#24102;&#23485;&#38480;&#21046;&#26041;&#26696;&#26174;&#33879;&#20943;&#23569;&#20102;&#35789;&#27719;&#37327;&#12290;&#36890;&#36807;&#21152;&#20837;&#33410;&#28857;&#20301;&#32622;&#32534;&#30721;&#65292;GEEL&#21487;&#20197;&#33258;&#22238;&#24402;&#29983;&#25104;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;GEEL&#25193;&#23637;&#21040;&#22788;&#29702;&#23646;&#24615;&#22270;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02230v2 Announce Type: replace-cross  Abstract: Recently, there has been a surge of interest in employing neural networks for graph generation, a fundamental statistical learning problem with critical applications like molecule design and community analysis. However, most approaches encounter significant limitations when generating large-scale graphs. This is due to their requirement to output the full adjacency matrices whose size grows quadratically with the number of nodes. In response to this challenge, we introduce a new, simple, and scalable graph representation named gap encoded edge list (GEEL) that has a small representation size that aligns with the number of edges. In addition, GEEL significantly reduces the vocabulary size by incorporating the gap encoding and bandwidth restriction schemes. GEEL can be autoregressively generated with the incorporation of node positional encoding, and we further extend GEEL to deal with attributed graphs by designing a new grammar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#20195;&#30721;&#30340;&#23646;&#24615;&#65292;&#25581;&#31034;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#20195;&#30721;&#20043;&#38388;&#30340;&#29420;&#29305;&#27169;&#24335;&#65292;&#23588;&#20854;&#26159;&#32467;&#26500;&#20998;&#21106;&#23545;&#20110;&#35782;&#21035;&#20195;&#30721;&#26469;&#28304;&#24456;&#20851;&#38190;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DetectCodeGPT&#30340;&#26032;&#26041;&#27861;&#26469;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2401.06461</link><description>&lt;p&gt;
&#20195;&#30721;&#20043;&#38388;&#30340;&#30028;&#38480;&#65306;&#25581;&#31034;&#26426;&#22120;&#21644;&#20154;&#31867;&#31243;&#24207;&#21592;&#20043;&#38388;&#19981;&#21516;&#30340;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Between Lines of Code: Unraveling the Distinct Patterns of Machine and Human Programmers. (arXiv:2401.06461v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#20195;&#30721;&#30340;&#23646;&#24615;&#65292;&#25581;&#31034;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#20195;&#30721;&#20043;&#38388;&#30340;&#29420;&#29305;&#27169;&#24335;&#65292;&#23588;&#20854;&#26159;&#32467;&#26500;&#20998;&#21106;&#23545;&#20110;&#35782;&#21035;&#20195;&#30721;&#26469;&#28304;&#24456;&#20851;&#38190;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DetectCodeGPT&#30340;&#26032;&#26041;&#27861;&#26469;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#27169;&#31946;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#28304;&#20195;&#30721;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#23548;&#33268;&#36719;&#20214;&#20135;&#29289;&#30340;&#23436;&#25972;&#24615;&#21644;&#30495;&#23454;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#20195;&#30721;&#38271;&#24230;&#12289;&#35789;&#27719;&#22810;&#26679;&#24615;&#21644;&#33258;&#28982;&#24615;&#31561;&#23646;&#24615;&#30340;&#20005;&#26684;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#20195;&#30721;&#22266;&#26377;&#30340;&#29420;&#29305;&#27169;&#24335;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#29305;&#21035;&#27880;&#24847;&#21040;&#65292;&#20195;&#30721;&#30340;&#32467;&#26500;&#20998;&#21106;&#26159;&#35782;&#21035;&#20854;&#26469;&#28304;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DetectCodeGPT&#30340;&#26032;&#22411;&#26426;&#22120;&#29983;&#25104;&#20195;&#30721;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25913;&#36827;&#20102;DetectGPT&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have catalyzed an unprecedented wave in code generation. While achieving significant advances, they blur the distinctions between machine-and human-authored source code, causing integrity and authenticity issues of software artifacts. Previous methods such as DetectGPT have proven effective in discerning machine-generated texts, but they do not identify and harness the unique patterns of machine-generated code. Thus, its applicability falters when applied to code. In this paper, we carefully study the specific patterns that characterize machine and human-authored code. Through a rigorous analysis of code attributes such as length, lexical diversity, and naturalness, we expose unique pat-terns inherent to each source. We particularly notice that the structural segmentation of code is a critical factor in identifying its provenance. Based on our findings, we propose a novel machine-generated code detection method called DetectCodeGPT, which improves DetectGPT by cap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#21160;&#24577;&#23574;&#23792;&#22270;&#31070;&#32463;&#32593;&#32476;"&#65288;DSGNN&#65289;&#30340;&#26694;&#26550;&#65292;&#23427;&#23558;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#35299;&#20915;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#22797;&#26434;&#24615;&#21644;&#20869;&#23384;&#24320;&#38144;&#38382;&#39064;&#12290;DSGNN&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#23574;&#23792;&#31070;&#32463;&#20803;&#30340;&#29366;&#24577;&#21644;&#36830;&#25509;&#26435;&#37325;&#65292;&#22312;&#20256;&#25773;&#36807;&#31243;&#20013;&#20445;&#25345;&#22270;&#32467;&#26500;&#20449;&#24687;&#30340;&#23436;&#25972;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05373</link><description>&lt;p&gt;
&#21160;&#24577;&#23574;&#23792;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Dynamic Spiking Graph Neural Networks. (arXiv:2401.05373v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#21160;&#24577;&#23574;&#23792;&#22270;&#31070;&#32463;&#32593;&#32476;"&#65288;DSGNN&#65289;&#30340;&#26694;&#26550;&#65292;&#23427;&#23558;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#35299;&#20915;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#22797;&#26434;&#24615;&#21644;&#20869;&#23384;&#24320;&#38144;&#38382;&#39064;&#12290;DSGNN&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#23574;&#23792;&#31070;&#32463;&#20803;&#30340;&#29366;&#24577;&#21644;&#36830;&#25509;&#26435;&#37325;&#65292;&#22312;&#20256;&#25773;&#36807;&#31243;&#20013;&#20445;&#25345;&#22270;&#32467;&#26500;&#20449;&#24687;&#30340;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30456;&#32467;&#21512;&#28176;&#28176;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#36825;&#26159;&#22240;&#20026;&#23427;&#22312;&#22788;&#29702;&#30001;&#22270;&#34920;&#31034;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#26102;&#20855;&#26377;&#20302;&#21151;&#32791;&#21644;&#39640;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#19968;&#20010;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#38754;&#20020;&#30528;&#39640;&#22797;&#26434;&#24615;&#21644;&#22823;&#20869;&#23384;&#24320;&#38144;&#30340;&#25361;&#25112;&#12290;&#30446;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#36890;&#36807;&#20351;&#29992;&#20108;&#36827;&#21046;&#29305;&#24449;&#32780;&#19981;&#26159;&#36830;&#32493;&#29305;&#24449;&#30340;SNNs&#26469;&#26367;&#20195;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#36827;&#34892;&#39640;&#25928;&#35757;&#32451;&#65292;&#36825;&#20250;&#24573;&#35270;&#22270;&#32467;&#26500;&#20449;&#24687;&#24182;&#22312;&#20256;&#25773;&#36807;&#31243;&#20013;&#23548;&#33268;&#32454;&#33410;&#30340;&#20002;&#22833;&#12290;&#27492;&#22806;&#65292;&#20248;&#21270;&#21160;&#24577;&#23574;&#23792;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22312;&#26102;&#38388;&#27493;&#20043;&#38388;&#20256;&#25773;&#20449;&#24687;&#65292;&#36825;&#22686;&#21152;&#20102;&#20869;&#23384;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#21160;&#24577;&#23574;&#23792;&#22270;&#31070;&#32463;&#32593;&#32476;"&#65288;\method{}&#65289;&#30340;&#26694;&#26550;&#12290;&#20026;&#20102;&#20943;&#36731;&#20449;&#24687;&#20002;&#22833;&#38382;&#39064;&#65292;\method{} &#22312;&#20256;&#25773;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#21046;&#65292;&#23427;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#20013;&#21160;&#24577;&#22320;&#35843;&#25972;&#23574;&#23792;&#31070;&#32463;&#20803;&#30340;&#29366;&#24577;&#21644;&#36830;&#25509;&#26435;&#37325;&#65292;&#20197;&#20445;&#25345;&#22270;&#32467;&#26500;&#20449;&#24687;&#30340;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of Spiking Neural Networks (SNNs) and Graph Neural Networks (GNNs) is gradually attracting attention due to the low power consumption and high efficiency in processing the non-Euclidean data represented by graphs. However, as a common problem, dynamic graph representation learning faces challenges such as high complexity and large memory overheads. Current work often uses SNNs instead of Recurrent Neural Networks (RNNs) by using binary features instead of continuous ones for efficient training, which would overlooks graph structure information and leads to the loss of details during propagation. Additionally, optimizing dynamic spiking models typically requires propagation of information across time steps, which increases memory requirements. To address these challenges, we present a framework named \underline{Dy}namic \underline{S}p\underline{i}king \underline{G}raph \underline{N}eural Networks (\method{}). To mitigate the information loss problem, \method{} propagates
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KI-PMF&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20808;&#39564;&#30693;&#35782;&#65292;&#23545;&#20132;&#36890;&#21442;&#19982;&#32773;&#30340;&#26410;&#26469;&#34892;&#21160;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#65292;&#36981;&#24490;&#36710;&#36742;&#30340;&#36816;&#21160;&#32422;&#26463;&#21644;&#34892;&#39542;&#29615;&#22659;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#36890;&#36807;&#26465;&#20214;&#21270;&#32593;&#32476;&#20197;&#36981;&#24490;&#29289;&#29702;&#23450;&#24459;&#65292;&#21487;&#20197;&#33719;&#24471;&#20934;&#30830;&#21644;&#23433;&#20840;&#30340;&#39044;&#27979;&#65292;&#23545;&#20110;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#32500;&#25252;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#23433;&#20840;&#21644;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2310.12007</link><description>&lt;p&gt;
KI-PMF&#65306;&#30693;&#35782;&#32508;&#21512;&#30340;&#21512;&#29702;&#21160;&#20316;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
KI-PMF: Knowledge Integrated Plausible Motion Forecasting. (arXiv:2310.12007v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KI-PMF&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20808;&#39564;&#30693;&#35782;&#65292;&#23545;&#20132;&#36890;&#21442;&#19982;&#32773;&#30340;&#26410;&#26469;&#34892;&#21160;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#65292;&#36981;&#24490;&#36710;&#36742;&#30340;&#36816;&#21160;&#32422;&#26463;&#21644;&#34892;&#39542;&#29615;&#22659;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#36890;&#36807;&#26465;&#20214;&#21270;&#32593;&#32476;&#20197;&#36981;&#24490;&#29289;&#29702;&#23450;&#24459;&#65292;&#21487;&#20197;&#33719;&#24471;&#20934;&#30830;&#21644;&#23433;&#20840;&#30340;&#39044;&#27979;&#65292;&#23545;&#20110;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#32500;&#25252;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#23433;&#20840;&#21644;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#20132;&#36890;&#21442;&#19982;&#32773;&#30340;&#34892;&#21160;&#23545;&#22823;&#35268;&#27169;&#37096;&#32626;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#30340;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#20248;&#21270;&#29305;&#23450;&#24230;&#37327;&#30340;&#25439;&#22833;&#20989;&#25968;&#19978;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#39044;&#27979;&#19981;&#31526;&#21512;&#29289;&#29702;&#23450;&#24459;&#25110;&#36829;&#21453;&#22806;&#37096;&#32422;&#26463;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#32467;&#21512;&#26126;&#30830;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20351;&#32593;&#32476;&#33021;&#22815;&#39044;&#27979;&#26410;&#26469;&#36712;&#36857;&#65292;&#31526;&#21512;&#36710;&#36742;&#30340;&#36816;&#21160;&#32422;&#26463;&#21644;&#34892;&#39542;&#29615;&#22659;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38750;&#21442;&#25968;&#21098;&#26525;&#23618;&#21644;&#27880;&#24847;&#21147;&#23618;&#26469;&#25972;&#21512;&#23450;&#20041;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#30830;&#20445;&#20132;&#36890;&#21442;&#19982;&#32773;&#22312;&#22797;&#26434;&#21644;&#21160;&#24577;&#24773;&#20917;&#19979;&#30340;&#21040;&#36798;&#21487;&#36798;&#24615;&#20445;&#35777;&#12290;&#36890;&#36807;&#23558;&#32593;&#32476;&#26465;&#20214;&#21270;&#20026;&#36981;&#24490;&#29289;&#29702;&#23450;&#24459;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#20934;&#30830;&#21644;&#23433;&#20840;&#30340;&#39044;&#27979;&#65292;&#36825;&#23545;&#20110;&#22312;&#23454;&#38469;&#19990;&#30028;&#29615;&#22659;&#20013;&#32500;&#25252;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#23433;&#20840;&#21644;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately forecasting the motion of traffic actors is crucial for the deployment of autonomous vehicles at a large scale. Current trajectory forecasting approaches primarily concentrate on optimizing a loss function with a specific metric, which can result in predictions that do not adhere to physical laws or violate external constraints. Our objective is to incorporate explicit knowledge priors that allow a network to forecast future trajectories in compliance with both the kinematic constraints of a vehicle and the geometry of the driving environment. To achieve this, we introduce a non-parametric pruning layer and attention layers to integrate the defined knowledge priors. Our proposed method is designed to ensure reachability guarantees for traffic actors in both complex and dynamic situations. By conditioning the network to follow physical laws, we can obtain accurate and safe predictions, essential for maintaining autonomous vehicles' safety and efficiency in real-world settings
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#31435;&#20110;&#24213;&#23618;&#22320;&#22270;&#29983;&#25104;&#26041;&#27861;&#30340;&#25925;&#20107;&#24773;&#33410;&#24067;&#23616;&#35774;&#35745;&#65292;&#20197;&#35299;&#20915;&#35774;&#35745;&#25903;&#25345;&#25152;&#26399;&#26395;&#21465;&#20107;&#30340;&#28216;&#25103;&#22320;&#22270;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.15242</link><description>&lt;p&gt;
PlotMap&#65306;&#29992;&#20110;&#26500;&#24314;&#28216;&#25103;&#19990;&#30028;&#30340;&#33258;&#21160;&#24067;&#23616;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
PlotMap: Automated Layout Design for Building Game Worlds. (arXiv:2309.15242v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#31435;&#20110;&#24213;&#23618;&#22320;&#22270;&#29983;&#25104;&#26041;&#27861;&#30340;&#25925;&#20107;&#24773;&#33410;&#24067;&#23616;&#35774;&#35745;&#65292;&#20197;&#35299;&#20915;&#35774;&#35745;&#25903;&#25345;&#25152;&#26399;&#26395;&#21465;&#20107;&#30340;&#28216;&#25103;&#22320;&#22270;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28216;&#25103;&#19990;&#30028;&#26500;&#24314;&#26159;&#24320;&#21457;&#28216;&#25103;&#30340;&#21465;&#20107;&#21644;&#29289;&#29702;&#19990;&#30028;&#30340;&#36807;&#31243;&#65292;&#22312;&#28216;&#25103;&#20307;&#39564;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22791;&#21463;&#22909;&#35780;&#30340;&#29420;&#31435;&#28216;&#25103;&#21644;AAA&#32423;&#35270;&#39057;&#28216;&#25103;&#34987;&#36190;&#36175;&#20854;&#20248;&#31168;&#30340;&#19990;&#30028;&#26500;&#24314;&#65292;&#20854;&#20013;&#28216;&#25103;&#22320;&#22270;&#19982;&#21465;&#20107;&#32039;&#23494;&#34701;&#21512;&#24182;&#25552;&#21319;&#20102;&#28216;&#25103;&#20307;&#39564;&#65292;&#21560;&#24341;&#20102;&#29609;&#23478;&#24182;&#30041;&#19979;&#20102;&#28145;&#21051;&#30340;&#21360;&#35937;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#25903;&#25345;&#25152;&#26399;&#26395;&#21465;&#20107;&#30340;&#28216;&#25103;&#22320;&#22270;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#28385;&#36275;&#21508;&#31181;&#32771;&#34385;&#22240;&#32032;&#30340;&#22797;&#26434;&#32422;&#26463;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22320;&#22270;&#29983;&#25104;&#26041;&#27861;&#20391;&#37325;&#20110;&#28216;&#25103;&#26426;&#21046;&#25110;&#22320;&#22270;&#22320;&#24418;&#30340;&#32771;&#34385;&#65292;&#32780;&#24573;&#35270;&#20102;&#25903;&#25345;&#25925;&#20107;&#30340;&#38656;&#27714;&#12290;&#22240;&#27492;&#65292;&#20173;&#28982;&#38656;&#35201;&#36827;&#34892;&#22823;&#37327;&#25163;&#21160;&#35843;&#25972;&#25165;&#33021;&#35774;&#35745;&#20986;&#36866;&#24212;&#29305;&#23450;&#25925;&#20107;&#30340;&#28216;&#25103;&#19990;&#30028;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#28216;&#25103;&#19990;&#30028;&#26500;&#24314;&#27969;&#31243;&#20013;&#24341;&#20837;&#19968;&#20010;&#19982;&#24213;&#23618;&#22320;&#22270;&#29983;&#25104;&#26041;&#27861;&#26080;&#20851;&#30340;&#25925;&#20107;&#24773;&#33410;&#24067;&#23616;&#35774;&#35745;&#39069;&#22806;&#23618;&#38754;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
World-building, the process of developing both the narrative and physical world of a game, plays a vital role in the game's experience. Critically acclaimed independent and AAA video games are praised for strong world building, with game maps that masterfully intertwine with and elevate the narrative, captivating players and leaving a lasting impression. However, designing game maps that support a desired narrative is challenging, as it requires satisfying complex constraints from various considerations. Most existing map generation methods focus on considerations about gameplay mechanics or map topography, while the need to support the story is typically neglected. As a result, extensive manual adjustment is still required to design a game world that facilitates particular stories. In this work, we approach this problem by introducing an extra layer of plot facility layout design that is independent of the underlying map generation method in a world-building pipeline. Concretely, we p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31867;&#33258;&#36866;&#24212;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#35821;&#20041;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#23618;&#26469;&#35843;&#33410;&#22270;&#20687;&#29983;&#25104;&#65292;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#35270;&#35273;&#29983;&#25104;&#36136;&#37327;&#21644;&#32534;&#36753;&#28789;&#27963;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#20840;&#23616;&#26679;&#24335;&#19981;&#19968;&#33268;&#21644;&#23616;&#37096;&#26679;&#24335;&#32534;&#36753;&#19981;&#30495;&#23454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.16071</link><description>&lt;p&gt;
&#22522;&#20110;&#31867;&#33258;&#36866;&#24212;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#35821;&#20041;&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Semantic Image Synthesis via Class-Adaptive Cross-Attention. (arXiv:2308.16071v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31867;&#33258;&#36866;&#24212;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#35821;&#20041;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#23618;&#26469;&#35843;&#33410;&#22270;&#20687;&#29983;&#25104;&#65292;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#35270;&#35273;&#29983;&#25104;&#36136;&#37327;&#21644;&#32534;&#36753;&#28789;&#27963;&#24615;&#65292;&#24182;&#35299;&#20915;&#20102;&#20840;&#23616;&#26679;&#24335;&#19981;&#19968;&#33268;&#21644;&#23616;&#37096;&#26679;&#24335;&#32534;&#36753;&#19981;&#30495;&#23454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#20041;&#22270;&#20687;&#21512;&#25104;&#39046;&#22495;&#65292;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20027;&#35201;&#20351;&#29992;&#31354;&#38388;&#33258;&#36866;&#24212;&#24402;&#19968;&#21270;&#23618;&#65292;&#21487;&#20197;&#23454;&#29616;&#20986;&#33394;&#30340;&#35270;&#35273;&#29983;&#25104;&#36136;&#37327;&#21644;&#32534;&#36753;&#28789;&#27963;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#24573;&#30053;&#20840;&#23616;&#22270;&#20687;&#32479;&#35745;&#20449;&#24687;&#65292;&#23548;&#33268;&#23616;&#37096;&#26679;&#24335;&#32534;&#36753;&#19981;&#30495;&#23454;&#65292;&#24182;&#24341;&#36215;&#35832;&#22914;&#33394;&#24425;&#25110;&#20809;&#29031;&#20998;&#24067;&#20559;&#31227;&#31561;&#20840;&#23616;&#19981;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#29983;&#25104;&#22120;&#38656;&#35201;&#35821;&#20041;&#24067;&#23616;&#26469;&#26144;&#23556;&#26679;&#24335;&#65292;&#23545;&#29305;&#24449;&#25552;&#20986;&#20102;&#20005;&#26684;&#30340;&#23545;&#40784;&#32422;&#26463;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#23618;&#20195;&#26367;&#21453;&#24402;&#19968;&#21270;&#23618;&#26469;&#35843;&#33410;&#22270;&#20687;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#32487;&#25215;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#20445;&#25345;&#20102;&#26368;&#20808;&#36827;&#30340;&#37325;&#24314;&#36136;&#37327;&#65292;&#24182;&#19988;&#25913;&#36827;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#26679;&#24335;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
In semantic image synthesis, the state of the art is dominated by methods that use spatially-adaptive normalization layers, which allow for excellent visual generation quality and editing versatility. Granted their efficacy, recent research efforts have focused toward finer-grained local style control and multi-modal generation. By construction though, such layers tend to overlook global image statistics leading to unconvincing local style editing and causing global inconsistencies such as color or illumination distribution shifts. Also, the semantic layout is required for mapping styles in the generator, putting a strict alignment constraint over the features. In response, we designed a novel architecture where cross-attention layers are used in place of de-normalization ones for conditioning the image generation. Our model inherits the advantages of both solutions, retaining state-of-the-art reconstruction quality, as well as improved global and local style transfer. Code and models 
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;AI&#30340;&#33258;&#21160;&#23398;&#29983;&#21453;&#39304;&#26694;&#26550;&#21487;&#20197;&#25552;&#20379;&#20016;&#23500;&#30340;&#21453;&#39304;&#65292;&#20294;&#24341;&#20837;&#20102;&#20262;&#29702;&#38382;&#39064;&#65292;&#24182;&#38656;&#35201;&#35299;&#20915;&#8220;&#22810;&#25968;&#20154;&#30340;&#26292;&#25919;&#8221;&#21644;&#24573;&#35270;&#38271;&#23614;&#20013;&#23569;&#25968;&#32676;&#20307;&#38656;&#27714;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.15334</link><description>&lt;p&gt;
&#19968;&#31181;&#36127;&#36131;&#20219;&#24320;&#21457;&#22522;&#20110;&#29983;&#25104;AI&#30340;&#33258;&#21160;&#23398;&#29983;&#21453;&#39304;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Responsible Development of Automated Student Feedback with Generative AI. (arXiv:2308.15334v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15334
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;AI&#30340;&#33258;&#21160;&#23398;&#29983;&#21453;&#39304;&#26694;&#26550;&#21487;&#20197;&#25552;&#20379;&#20016;&#23500;&#30340;&#21453;&#39304;&#65292;&#20294;&#24341;&#20837;&#20102;&#20262;&#29702;&#38382;&#39064;&#65292;&#24182;&#38656;&#35201;&#35299;&#20915;&#8220;&#22810;&#25968;&#20154;&#30340;&#26292;&#25919;&#8221;&#21644;&#24573;&#35270;&#38271;&#23614;&#20013;&#23569;&#25968;&#32676;&#20307;&#38656;&#27714;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#20016;&#23500;&#30340;&#21453;&#39304;&#23545;&#20110;&#25903;&#25345;&#23398;&#29983;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#29983;&#25104;AI&#23588;&#20854;&#26159;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#65292;&#20026;&#21521;&#23398;&#29983;&#25552;&#20379;&#21487;&#37325;&#22797;&#12289;&#21487;&#25193;&#23637;&#21644;&#21363;&#26102;&#29983;&#25104;&#30340;&#33258;&#21160;&#21453;&#39304;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#20351;&#24471;&#20043;&#21069;&#31232;&#32570;&#19988;&#26114;&#36149;&#30340;&#23398;&#20064;&#36164;&#28304;&#21464;&#24471;&#20016;&#23500;&#36215;&#26469;&#12290;&#20174;&#25216;&#26415;&#35282;&#24230;&#32780;&#35328;&#65292;&#36825;&#31181;&#26041;&#27861;&#26159;&#21487;&#34892;&#30340;&#65292;&#24471;&#30410;&#20110;&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#36827;&#27493;&#65307;&#28982;&#32780;&#65292;&#37319;&#29992;&#36825;&#20123;&#25216;&#26415;&#20063;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#28508;&#22312;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#38656;&#35201;&#35748;&#30495;&#32771;&#34385;&#12290;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21560;&#24341;&#21147;&#22312;&#20110;&#23427;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#33258;&#21160;&#21270;&#26368;&#20047;&#21619;&#30340;&#20219;&#21153;&#65307;&#20294;&#26159;&#36825;&#20063;&#21487;&#33021;&#23548;&#33268;&#8220;&#22810;&#25968;&#20154;&#30340;&#26292;&#25919;&#8221;&#65292;&#21363;&#24573;&#35270;&#20102;&#38271;&#23614;&#20013;&#23569;&#25968;&#32676;&#20307;&#30340;&#38656;&#27714;&#65292;&#22240;&#20026;&#36825;&#20123;&#38656;&#27714;&#24456;&#38590;&#33258;&#21160;&#21270;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#33021;&#22815;&#20135;&#29983;&#26377;&#20215;&#20540;&#21644;&#30495;&#23454;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Providing rich feedback to students is essential for supporting student learning. Recent advances in generative AI, particularly within large language modelling (LLM), provide the opportunity to deliver repeatable, scalable and instant automatically generated feedback to students, making abundant a previously scarce and expensive learning resource. Such an approach is feasible from a technical perspective due to these recent advances in Artificial Intelligence (AI) and Natural Language Processing (NLP); while the potential upside is a strong motivator, doing so introduces a range of potential ethical issues that must be considered as we apply these technologies. The attractiveness of AI systems is that they can effectively automate the most mundane tasks; but this risks introducing a "tyranny of the majority", where the needs of minorities in the long tail are overlooked because they are difficult to automate.  Developing machine learning models that can generate valuable and authentic
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#37327;&#21270;&#12289;&#20462;&#21098;&#12289;&#30693;&#35782;&#33976;&#39311;&#31561;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#31361;&#20986;&#20171;&#32461;&#20102;&#26368;&#26032;&#36827;&#23637;&#21644;&#21019;&#26032;&#26041;&#27861;&#65292;&#20026;&#23454;&#29616;&#39640;&#25928;&#30340;&#37096;&#32626;&#25552;&#20379;&#20102;&#37325;&#35201;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2308.07633</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Model Compression for Large Language Models. (arXiv:2308.07633v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#37327;&#21270;&#12289;&#20462;&#21098;&#12289;&#30693;&#35782;&#33976;&#39311;&#31561;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#31361;&#20986;&#20171;&#32461;&#20102;&#26368;&#26032;&#36827;&#23637;&#21644;&#21019;&#26032;&#26041;&#27861;&#65292;&#20026;&#23454;&#29616;&#39640;&#25928;&#30340;&#37096;&#32626;&#25552;&#20379;&#20102;&#37325;&#35201;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20197;&#24778;&#20154;&#30340;&#25104;&#21151;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24222;&#22823;&#30340;&#20307;&#37327;&#21644;&#35745;&#31639;&#38656;&#27714;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#19979;&#30340;&#23454;&#38469;&#37096;&#32626;&#20013;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#38543;&#30528;&#36825;&#20123;&#25361;&#25112;&#26085;&#30410;&#32039;&#36843;&#65292;&#27169;&#22411;&#21387;&#32553;&#39046;&#22495;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#32531;&#35299;&#36825;&#20123;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#32508;&#36848;&#65292;&#25506;&#35752;&#19987;&#38376;&#38024;&#23545;LLMs&#30340;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#12290;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#37327;&#21270;&#12289;&#20462;&#21098;&#12289;&#30693;&#35782;&#33976;&#39311;&#31561;&#65292;&#20197;&#24212;&#23545;&#39640;&#25928;&#37096;&#32626;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;&#22312;&#27599;&#31181;&#25216;&#26415;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;&#26368;&#26032;&#36827;&#23637;&#21644;&#21019;&#26032;&#26041;&#27861;&#65292;&#20026;LLM&#30740;&#31350;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#29992;&#20110;&#35780;&#20272;&#25928;&#26524;&#30340;&#22522;&#20934;&#31574;&#30053;&#21644;&#35780;&#20272;&#25351;&#26631;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have revolutionized natural language processing tasks with remarkable success. However, their formidable size and computational demands present significant challenges for practical deployment, especially in resource-constrained environments. As these challenges become increasingly pertinent, the field of model compression has emerged as a pivotal research area to alleviate these limitations. This paper presents a comprehensive survey that navigates the landscape of model compression techniques tailored specifically for LLMs. Addressing the imperative need for efficient deployment, we delve into various methodologies, encompassing quantization, pruning, knowledge distillation, and more. Within each of these techniques, we highlight recent advancements and innovative approaches that contribute to the evolving landscape of LLM research. Furthermore, we explore benchmarking strategies and evaluation metrics that are essential for assessing the effectiveness of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23433;&#20840;&#30417;&#27979;&#26041;&#27861;SMARLA&#65292;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#12290;&#35813;&#26041;&#27861;&#35774;&#35745;&#20026;&#40657;&#30418;&#23376;&#65292;&#21033;&#29992;&#29366;&#24577;&#25277;&#35937;&#20943;&#23569;&#29366;&#24577;&#31354;&#38388;&#65292;&#23454;&#29616;&#23545;&#26234;&#33021;&#20307;&#29366;&#24577;&#30340;&#23433;&#20840;&#36829;&#35268;&#39044;&#27979;&#12290;&#32463;&#39564;&#35777;&#65292;SMARLA&#20855;&#26377;&#20934;&#30830;&#30340;&#36829;&#35268;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#21487;&#22312;&#26234;&#33021;&#20307;&#25191;&#34892;&#30340;&#26089;&#26399;&#38454;&#27573;&#36827;&#34892;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.02594</link><description>&lt;p&gt;
SMARLA&#65306;&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#23433;&#20840;&#30417;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SMARLA: A Safety Monitoring Approach for Deep Reinforcement Learning Agents. (arXiv:2308.02594v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23433;&#20840;&#30417;&#27979;&#26041;&#27861;SMARLA&#65292;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#12290;&#35813;&#26041;&#27861;&#35774;&#35745;&#20026;&#40657;&#30418;&#23376;&#65292;&#21033;&#29992;&#29366;&#24577;&#25277;&#35937;&#20943;&#23569;&#29366;&#24577;&#31354;&#38388;&#65292;&#23454;&#29616;&#23545;&#26234;&#33021;&#20307;&#29366;&#24577;&#30340;&#23433;&#20840;&#36829;&#35268;&#39044;&#27979;&#12290;&#32463;&#39564;&#35777;&#65292;SMARLA&#20855;&#26377;&#20934;&#30830;&#30340;&#36829;&#35268;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#21487;&#22312;&#26234;&#33021;&#20307;&#25191;&#34892;&#30340;&#26089;&#26399;&#38454;&#27573;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;(DRL)&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#12290;&#30830;&#20445;DRL&#26234;&#33021;&#20307;&#30340;&#23433;&#20840;&#24615;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20165;&#20381;&#38752;&#27979;&#35797;&#26159;&#19981;&#36275;&#20197;&#30830;&#20445;&#23433;&#20840;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#19981;&#33021;&#25552;&#20379;&#20445;&#35777;&#12290;&#26500;&#24314;&#23433;&#20840;&#30417;&#27979;&#22120;&#26159;&#32531;&#35299;&#36825;&#19968;&#25361;&#25112;&#30340;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SMARLA&#65292;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23433;&#20840;&#30417;&#27979;&#26041;&#27861;&#65292;&#19987;&#20026;DRL&#26234;&#33021;&#20307;&#35774;&#35745;&#12290;&#20986;&#20110;&#23454;&#38469;&#21407;&#22240;&#65292;SMARLA&#34987;&#35774;&#35745;&#20026;&#40657;&#30418;&#23376;(&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#35775;&#38382;&#26234;&#33021;&#20307;&#30340;&#20869;&#37096;)&#65292;&#24182;&#21033;&#29992;&#29366;&#24577;&#25277;&#35937;&#26469;&#20943;&#23569;&#29366;&#24577;&#31354;&#38388;&#65292;&#20174;&#32780;&#20419;&#36827;&#20174;&#26234;&#33021;&#20307;&#30340;&#29366;&#24577;&#23398;&#20064;&#23433;&#20840;&#36829;&#35268;&#39044;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#30693;&#21517;&#30340;RL&#26696;&#20363;&#30740;&#31350;&#20013;&#39564;&#35777;&#20102;SMARLA&#12290;&#32463;&#39564;&#20998;&#26512;&#34920;&#26126;&#65292;SMARLA&#20855;&#26377;&#20934;&#30830;&#30340;&#36829;&#35268;&#39044;&#27979;&#33021;&#21147;&#65292;&#35823;&#25253;&#29575;&#20302;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#26234;&#33021;&#20307;&#25191;&#34892;&#30340;&#19968;&#21322;&#24038;&#21491;&#30340;&#26089;&#26399;&#38454;&#27573;&#39044;&#27979;&#23433;&#20840;&#36829;&#35268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning algorithms (DRL) are increasingly being used in safety-critical systems. Ensuring the safety of DRL agents is a critical concern in such contexts. However, relying solely on testing is not sufficient to ensure safety as it does not offer guarantees. Building safety monitors is one solution to alleviate this challenge. This paper proposes SMARLA, a machine learning-based safety monitoring approach designed for DRL agents. For practical reasons, SMARLA is designed to be black-box (as it does not require access to the internals of the agent) and leverages state abstraction to reduce the state space and thus facilitate the learning of safety violation prediction models from agent's states. We validated SMARLA on two well-known RL case studies. Empirical analysis reveals that SMARLA achieves accurate violation prediction with a low false positive rate, and can predict safety violations at an early stage, approximately halfway through the agent's execution before 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39564;&#35777;&#20102;&#36923;&#36753;&#35780;&#20272;&#20844;&#24335;&#22312;&#20855;&#26377;&#19981;&#20934;&#30830;&#30340;&#30495;&#23454;&#26631;&#31614;&#30340;&#35780;&#20272;&#20013;&#30340;&#23454;&#29992;&#24615;&#65292;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#20083;&#33146;&#30284;&#32959;&#30244;&#20998;&#21106;&#22312;&#21307;&#23398;&#32452;&#32455;&#30149;&#29702;&#23398;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#24773;&#20917;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;LAF&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20855;&#26377;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;LAF&#22312;MHWSIA&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.02709</link><description>&lt;p&gt;
&#36890;&#36807;&#36923;&#36753;&#35780;&#20272;&#20844;&#24335;&#39564;&#35777;&#23545;&#19981;&#20934;&#30830;&#22320;&#30495;&#23454;&#26631;&#31614;&#30340;&#35780;&#20272;&#30340;&#23454;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Validation of the Practicability of Logical Assessment Formula for Evaluations with Inaccurate Ground-Truth Labels. (arXiv:2307.02709v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39564;&#35777;&#20102;&#36923;&#36753;&#35780;&#20272;&#20844;&#24335;&#22312;&#20855;&#26377;&#19981;&#20934;&#30830;&#30340;&#30495;&#23454;&#26631;&#31614;&#30340;&#35780;&#20272;&#20013;&#30340;&#23454;&#29992;&#24615;&#65292;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#20083;&#33146;&#30284;&#32959;&#30244;&#20998;&#21106;&#22312;&#21307;&#23398;&#32452;&#32455;&#30149;&#29702;&#23398;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#24773;&#20917;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;LAF&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20855;&#26377;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;LAF&#22312;MHWSIA&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36923;&#36753;&#35780;&#20272;&#20844;&#24335;&#65288;LAF&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#20855;&#26377;&#19981;&#20934;&#30830;&#22320;&#30495;&#23454;&#26631;&#31614;&#65288;IAGTLs&#65289;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#26032;&#29702;&#35770;&#65292;&#29992;&#20110;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;LAF&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#23545;&#20110;&#20855;&#26377;IAGTLs&#30340;&#35780;&#20272;&#30340;&#23454;&#29992;&#24615;&#23578;&#26410;&#24471;&#21040;&#39564;&#35777;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;LAF&#24212;&#29992;&#20110;&#21307;&#23398;&#32452;&#32455;&#30149;&#29702;&#23398;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#20083;&#33146;&#30284;&#32959;&#30244;&#20998;&#21106;&#65288;TSfBC&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#21644;&#20998;&#26512;&#26174;&#31034;LAF&#22312;TSfBC&#20013;&#23545;&#20110;&#20855;&#26377;IAGTLs&#30340;&#35780;&#20272;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21453;&#26144;&#20102;&#23558;LAF&#24212;&#29992;&#20110;MHWSIA&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Logical assessment formula (LAF) is a new theory proposed for evaluations with inaccurate ground-truth labels (IAGTLs) to assess the predictive models for various artificial intelligence applications. However, the practicability of LAF for evaluations with IAGTLs has not yet been validated in real-world practice. In this paper, to address this issue, we applied LAF to tumour segmentation for breast cancer (TSfBC) in medical histopathology whole slide image analysis (MHWSIA). Experimental results and analysis show the validity of LAF for evaluations with IAGTLs in the case of TSfBC and reflect the potentials of LAF applied to MHWSIA.
&lt;/p&gt;</description></item><item><title>&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#38761;&#26032;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;&#22522;&#22240;&#32452;&#23398;&#39046;&#22495;&#20013;&#35299;&#20915;&#33258;&#21160;&#25968;&#25454;&#20998;&#26512;&#21644;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#38477;&#20302;&#25910;&#38598;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#26412;&#65292;&#36866;&#29992;&#20110;&#22522;&#22240;&#32452;&#25968;&#25454;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;&#26412;&#35843;&#26597;&#37325;&#28857;&#20851;&#27880;&#22312;&#22522;&#22240;&#32452;&#30740;&#31350;&#39046;&#22495;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#12289;&#22522;&#22240;&#32452;&#32452;&#35013;&#21644;&#24207;&#21015;&#27604;&#23545;&#12290;</title><link>http://arxiv.org/abs/2302.13268</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#38761;&#26032;&#22522;&#22240;&#32452;&#23398;
&lt;/p&gt;
&lt;p&gt;
Revolutionizing Genomics with Reinforcement Learning Techniques. (arXiv:2302.13268v2 [q-bio.GN] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13268
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#38761;&#26032;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;&#22522;&#22240;&#32452;&#23398;&#39046;&#22495;&#20013;&#35299;&#20915;&#33258;&#21160;&#25968;&#25454;&#20998;&#26512;&#21644;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#38477;&#20302;&#25910;&#38598;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#26412;&#65292;&#36866;&#29992;&#20110;&#22522;&#22240;&#32452;&#25968;&#25454;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;&#26412;&#35843;&#26597;&#37325;&#28857;&#20851;&#27880;&#22312;&#22522;&#22240;&#32452;&#30740;&#31350;&#39046;&#22495;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#12289;&#22522;&#22240;&#32452;&#32452;&#35013;&#21644;&#24207;&#21015;&#27604;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#20986;&#29616;&#22312;&#35299;&#20915;&#21508;&#31181;&#38382;&#39064;&#20013;&#65292;&#21253;&#25324;&#20915;&#31574;&#21644;&#22522;&#22240;&#32452;&#23398;&#12290;&#36807;&#21435;&#20108;&#21313;&#24180;&#30340;&#21407;&#22987;&#22522;&#22240;&#32452;&#25968;&#25454;&#25351;&#25968;&#22686;&#38271;&#24050;&#32463;&#36229;&#20986;&#20102;&#25163;&#21160;&#20998;&#26512;&#30340;&#33021;&#21147;&#65292;&#36825;&#23548;&#33268;&#23545;&#33258;&#21160;&#25968;&#25454;&#20998;&#26512;&#21644;&#22788;&#29702;&#30340;&#20852;&#36259;&#36234;&#26469;&#36234;&#22823;&#12290;RL&#31639;&#27861;&#33021;&#22815;&#22312;&#26368;&#23567;&#30340;&#20154;&#24037;&#30417;&#30563;&#19979;&#20174;&#32463;&#39564;&#20013;&#23398;&#20064;&#65292;&#20351;&#20854;&#38750;&#24120;&#36866;&#21512;&#22522;&#22240;&#32452;&#25968;&#25454;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;&#20351;&#29992;RL&#30340;&#19968;&#20010;&#20851;&#38190;&#22909;&#22788;&#26159;&#38477;&#20302;&#20102;&#25910;&#38598;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#26412;&#65292;&#36825;&#26159;&#30417;&#30563;&#23398;&#20064;&#25152;&#38656;&#30340;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#35768;&#22810;&#30740;&#31350;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#22522;&#22240;&#32452;&#23398;&#20013;&#30340;&#24212;&#29992;&#65292;&#20294;&#26412;&#35843;&#26597;&#20165;&#19987;&#27880;&#20110;&#22312;&#21508;&#31181;&#22522;&#22240;&#32452;&#30740;&#31350;&#39046;&#22495;&#65288;&#21253;&#25324;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#65292;&#22522;&#22240;&#32452;&#32452;&#35013;&#21644;&#24207;&#21015;&#27604;&#23545;&#65289;&#20013;&#20351;&#29992;RL&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#23545;&#29616;&#26377;&#30740;&#31350;&#30340;&#25216;&#26415;&#32454;&#33410;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Reinforcement Learning (RL) has emerged as a powerful tool for solving a wide range of problems, including decision-making and genomics. The exponential growth of raw genomic data over the past two decades has exceeded the capacity of manual analysis, leading to a growing interest in automatic data analysis and processing. RL algorithms are capable of learning from experience with minimal human supervision, making them well-suited for genomic data analysis and interpretation. One of the key benefits of using RL is the reduced cost associated with collecting labeled training data, which is required for supervised learning. While there have been numerous studies examining the applications of Machine Learning (ML) in genomics, this survey focuses exclusively on the use of RL in various genomics research fields, including gene regulatory networks (GRNs), genome assembly, and sequence alignment. We present a comprehensive technical overview of existing studies on the applic
&lt;/p&gt;</description></item></channel></rss>