<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25253;&#21578;&#24341;&#20837;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#23548;&#38382;&#39064;&#20351;&#29992;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#25429;&#33719;&#20581;&#24247;&#25968;&#25454;&#30340;&#26032;&#30340;&#38899;&#39057;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;voice EHR&#65289;&#65292;&#21487;&#33021;&#21253;&#21547;&#22797;&#26434;&#30340;&#20581;&#24247;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#20174;&#32780;&#24357;&#34917;&#20102;&#21333;&#19968;&#27169;&#24577;&#20020;&#24202;&#25968;&#25454;&#30340;&#20856;&#22411;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2404.01620</link><description>&lt;p&gt;
Voice EHR:&#24341;&#20837;&#22810;&#27169;&#24335;&#38899;&#39057;&#25968;&#25454;&#29992;&#20110;&#20581;&#24247;
&lt;/p&gt;
&lt;p&gt;
Voice EHR: Introducing Multimodal Audio Data for Health
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25253;&#21578;&#24341;&#20837;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#23548;&#38382;&#39064;&#20351;&#29992;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#25429;&#33719;&#20581;&#24247;&#25968;&#25454;&#30340;&#26032;&#30340;&#38899;&#39057;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;voice EHR&#65289;&#65292;&#21487;&#33021;&#21253;&#21547;&#22797;&#26434;&#30340;&#20581;&#24247;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#20174;&#32780;&#24357;&#34917;&#20102;&#21333;&#19968;&#27169;&#24577;&#20020;&#24202;&#25968;&#25454;&#30340;&#20856;&#22411;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38899;&#39057;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#22823;&#22411;AI&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#24555;&#36895;&#20998;&#31867;&#24739;&#32773;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#26089;&#26399;&#26816;&#27979;&#22686;&#24378;&#21307;&#30103;&#20915;&#31574;&#65292;&#24182;&#21487;&#33021;&#36890;&#36807;&#26089;&#26399;&#26816;&#27979;&#25913;&#21892;&#32467;&#26524;&#12290;&#29616;&#26377;&#25216;&#26415;&#20381;&#36182;&#20110;&#22312;&#39640;&#25910;&#20837;&#12289;&#33521;&#35821;&#22269;&#23478;&#20351;&#29992;&#26114;&#36149;&#35760;&#24405;&#35774;&#22791;&#30340;&#26377;&#38480;&#25968;&#25454;&#38598;&#65292;&#36825;&#31181;&#25216;&#26415;&#38754;&#20020;&#36164;&#28304;&#21463;&#38480;&#12289;&#39640;&#25910;&#20837;&#22330;&#25152;&#30340;&#37096;&#32626;&#25361;&#25112;&#65292;&#38899;&#39057;&#25968;&#25454;&#21487;&#33021;&#20855;&#26377;&#28145;&#36828;&#24433;&#21709;&#12290;&#26412;&#25253;&#21578;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#31867;&#22411;&#21644;&#30456;&#24212;&#30340;&#25910;&#38598;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#23548;&#38382;&#39064;&#20165;&#20351;&#29992;&#31227;&#21160;&#24212;&#29992;/&#32593;&#32476;&#24212;&#29992;&#31243;&#24207;&#25429;&#33719;&#20581;&#24247;&#25968;&#25454;&#12290;&#35813;&#24212;&#29992;&#31243;&#24207;&#26368;&#32456;&#20135;&#29983;&#19968;&#20010;&#38899;&#39057;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;voice EHR&#65289;&#65292;&#23427;&#21487;&#33021;&#21253;&#21547;&#26469;&#33258;&#20256;&#32479;&#35821;&#38899;/&#21628;&#21560;&#29305;&#24449;&#12289;&#35821;&#38899;&#27169;&#24335;&#21644;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#35821;&#35328;&#30340;&#22797;&#26434;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#34917;&#20607;&#21333;&#19968;&#27169;&#24577;&#20020;&#24202;&#25968;&#25454;&#30340;&#20856;&#22411;&#38480;&#21046;&#12290;&#26412;&#25253;&#21578;&#20171;&#32461;&#20102;&#19968;&#20010;&#21512;&#20316;&#20249;&#20276;&#36130;&#22242;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01620v1 Announce Type: cross  Abstract: Large AI models trained on audio data may have the potential to rapidly classify patients, enhancing medical decision-making and potentially improving outcomes through early detection. Existing technologies depend on limited datasets using expensive recording equipment in high-income, English-speaking countries. This challenges deployment in resource-constrained, high-volume settings where audio data may have a profound impact. This report introduces a novel data type and a corresponding collection system that captures health data through guided questions using only a mobile/web application. This application ultimately results in an audio electronic health record (voice EHR) which may contain complex biomarkers of health from conventional voice/respiratory features, speech patterns, and language with semantic meaning - compensating for the typical limitations of unimodal clinical datasets. This report introduces a consortium of partner
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#28378;&#21160;&#35270;&#37326;&#25511;&#21046;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#65292;&#20197;&#35299;&#20915;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20013;&#24615;&#33021;&#21644;&#21487;&#34892;&#24615;&#21463;&#24433;&#21709;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.17338</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#36866;&#24212;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#30340;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#28378;&#21160;&#35270;&#37326;&#25511;&#21046;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning-based Receding Horizon Control using Adaptive Control Barrier Functions for Safety-Critical Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17338
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#28378;&#21160;&#35270;&#37326;&#25511;&#21046;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#65292;&#20197;&#35299;&#20915;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20013;&#24615;&#33021;&#21644;&#21487;&#34892;&#24615;&#21463;&#24433;&#21709;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#25511;&#21046;&#26041;&#27861;&#20026;&#23433;&#20840;&#20851;&#38190;&#38382;&#39064;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#24456;&#23481;&#26131;&#21464;&#24471;&#26840;&#25163;&#12290;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;(CBFs)&#20316;&#20026;&#19968;&#31181;&#27969;&#34892;&#25216;&#26415;&#20986;&#29616;&#65292;&#36890;&#36807;&#20854;&#21069;&#21521;&#19981;&#21464;&#24615;&#23646;&#24615;&#65292;&#26377;&#21033;&#20110;&#36890;&#36807;&#22312;&#25439;&#22833;&#19968;&#20123;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#26174;&#24335;&#22320;&#20445;&#35777;&#23433;&#20840;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#23450;&#20041;&#24615;&#33021;&#30446;&#26631;&#20197;&#21450;&#24517;&#39035;&#22987;&#32456;&#25191;&#34892;&#30340;&#22522;&#20110;CBF&#30340;&#23433;&#20840;&#32422;&#26463;&#12290;&#36951;&#25022;&#30340;&#26159;&#65292;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#21487;&#33021;&#20250;&#23545;&#24615;&#33021;&#21644;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#34892;&#24615;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#65306;(i)&#25104;&#26412;&#20989;&#25968;&#21450;&#20854;&#30456;&#20851;&#21442;&#25968;&#30340;&#36873;&#25321;&#65292;&#20197;&#21450;(ii)&#22312;CBF&#32422;&#26463;&#20869;&#36827;&#34892;&#21442;&#25968;&#26657;&#20934;&#65292;&#25429;&#25417;&#24615;&#33021;&#21644;&#20445;&#23432;&#24615;&#20043;&#38388;&#30340;&#25240;&#34935;&#65292;&#20197;&#21450;&#19981;&#21487;&#34892;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;(MPC)&#30340;&#24378;&#21270;&#23398;&#20064;(RL)&#28378;&#21160;&#35270;&#37326;&#25511;&#21046;(RHC)&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17338v1 Announce Type: cross  Abstract: Optimal control methods provide solutions to safety-critical problems but easily become intractable. Control Barrier Functions (CBFs) have emerged as a popular technique that facilitates their solution by provably guaranteeing safety, through their forward invariance property, at the expense of some performance loss. This approach involves defining a performance objective alongside CBF-based safety constraints that must always be enforced. Unfortunately, both performance and solution feasibility can be significantly impacted by two key factors: (i) the selection of the cost function and associated parameters, and (ii) the calibration of parameters within the CBF-based constraints, which capture the trade-off between performance and conservativeness. %as well as infeasibility. To address these challenges, we propose a Reinforcement Learning (RL)-based Receding Horizon Control (RHC) approach leveraging Model Predictive Control (MPC) with
&lt;/p&gt;</description></item><item><title>MYTE&#26159;&#19968;&#31181;&#22522;&#20110;&#24418;&#24577;&#23398;&#30340;&#23383;&#33410;&#32534;&#30721;&#33539;&#24335;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#19968;&#33268;&#22823;&#23567;&#30340;&#29255;&#27573;&#26469;&#23454;&#29616;&#36328;&#19981;&#21516;&#35821;&#35328;&#30340;&#20449;&#24687;&#32534;&#30721;&#65292;&#20026;99&#31181;&#35821;&#35328;&#25552;&#20379;&#20102;&#26356;&#30701;&#30340;&#32534;&#30721;&#65292;&#29305;&#21035;&#26159;&#23545;&#38750;&#27431;&#27954;&#35821;&#35328;&#21644;&#38750;&#25289;&#19969;&#25991;&#23383;&#30340;&#25913;&#36827;&#26368;&#20026;&#26174;&#33879;&#12290;</title><link>https://arxiv.org/abs/2403.10691</link><description>&lt;p&gt;
MYTE&#65306;&#24418;&#24577;&#23398;&#39537;&#21160;&#30340;&#23383;&#33410;&#32534;&#30721;&#65292;&#29992;&#20110;&#26356;&#22909;&#12289;&#26356;&#20844;&#24179;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual Language Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10691
&lt;/p&gt;
&lt;p&gt;
MYTE&#26159;&#19968;&#31181;&#22522;&#20110;&#24418;&#24577;&#23398;&#30340;&#23383;&#33410;&#32534;&#30721;&#33539;&#24335;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#19968;&#33268;&#22823;&#23567;&#30340;&#29255;&#27573;&#26469;&#23454;&#29616;&#36328;&#19981;&#21516;&#35821;&#35328;&#30340;&#20449;&#24687;&#32534;&#30721;&#65292;&#20026;99&#31181;&#35821;&#35328;&#25552;&#20379;&#20102;&#26356;&#30701;&#30340;&#32534;&#30721;&#65292;&#29305;&#21035;&#26159;&#23545;&#38750;&#27431;&#27954;&#35821;&#35328;&#21644;&#38750;&#25289;&#19969;&#25991;&#23383;&#30340;&#25913;&#36827;&#26368;&#20026;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#35821;&#35328;&#24314;&#27169;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#32771;&#34385;&#22240;&#32032;&#26159;&#22914;&#20309;&#26368;&#22909;&#22320;&#34920;&#31034;&#20855;&#26377;&#19981;&#21516;&#35789;&#27719;&#21644;&#25991;&#23383;&#30340;&#35821;&#35328;&#12290;&#23613;&#31649;&#24403;&#20195;&#25991;&#26412;&#32534;&#30721;&#26041;&#27861;&#28085;&#30422;&#20102;&#22823;&#22810;&#25968;&#19990;&#30028;&#25991;&#23383;&#31995;&#32479;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#20559;&#21521;&#20110;&#20840;&#29699;&#35199;&#26041;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#23569;&#25968;&#35821;&#35328;&#30340;&#25991;&#26412;&#24448;&#24448;&#34987;&#20998;&#21106;&#20026;&#19968;&#38271;&#20018;&#22312;&#35821;&#35328;&#23398;&#19978;&#27627;&#26080;&#24847;&#20041;&#30340;&#21333;&#20803;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#19981;&#24179;&#31561;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#29992;&#36328;&#19981;&#21516;&#35821;&#35328;&#20855;&#26377;&#19968;&#33268;&#22823;&#23567;&#30340;&#29255;&#27573;&#26469;&#32534;&#30721;&#30456;&#21516;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#32534;&#30721;&#32422;&#23450;&#65288;MYTE&#65289;&#22522;&#20110;&#24418;&#24577;&#32032;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#24211;&#23384;&#22312;&#21508;&#31181;&#35821;&#35328;&#20013;&#27604;&#23383;&#31526;&#26356;&#24179;&#34913;&#65292;&#32780;&#20197;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#23383;&#31526;&#12290;&#25105;&#20204;&#23637;&#31034;MYTE&#20026;&#25152;&#26377;99&#31181;&#20998;&#26512;&#35821;&#35328;&#20135;&#29983;&#20102;&#26356;&#30701;&#30340;&#32534;&#30721;&#65292;&#20854;&#20013;&#38750;&#27431;&#27954;&#35821;&#35328;&#21644;&#38750;&#25289;&#19969;&#25991;&#23383;&#30340;&#25913;&#36827;&#26368;&#20026;&#26174;&#33879;&#12290;&#36825;&#36827;&#32780;&#25913;&#21892;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#24314;&#27169;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10691v1 Announce Type: cross  Abstract: A major consideration in multilingual language modeling is how to best represent languages with diverse vocabularies and scripts. Although contemporary text encoding methods cover most of the world's writing systems, they exhibit bias towards the high-resource languages of the Global West. As a result, texts of underrepresented languages tend to be segmented into long sequences of linguistically meaningless units. To address the disparities, we introduce a new paradigm that encodes the same information with segments of consistent size across diverse languages. Our encoding convention (MYTE) is based on morphemes, as their inventories are more balanced across languages than characters, which are used in previous methods. We show that MYTE produces shorter encodings for all 99 analyzed languages, with the most notable improvements for non-European languages and non-Latin scripts. This, in turn, improves multilingual LM performance and di
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#19968;&#20010;&#25277;&#35937;&#25512;&#29702;&#25968;&#25454;&#38598;&#21644;&#26377;&#24847;&#20041;&#23398;&#20064;&#33539;&#24335;&#65292;&#25945;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21033;&#29992;&#36890;&#29992;&#20107;&#23454;&#36827;&#34892;&#25512;&#29702;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.09085</link><description>&lt;p&gt;
&#26377;&#24847;&#20041;&#23398;&#20064;&#65306;&#36890;&#36807;&#36890;&#29992;&#20107;&#23454;&#24341;&#23548;&#25512;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25277;&#35937;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Meaningful Learning: Advancing Abstract Reasoning in Large Language Models via Generic Fact Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09085
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#20010;&#25277;&#35937;&#25512;&#29702;&#25968;&#25454;&#38598;&#21644;&#26377;&#24847;&#20041;&#23398;&#20064;&#33539;&#24335;&#65292;&#25945;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21033;&#29992;&#36890;&#29992;&#20107;&#23454;&#36827;&#34892;&#25512;&#29702;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#25512;&#29702;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#21644;&#24378;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#26631;&#24535;&#30528;&#26397;&#30528;&#27169;&#25311;&#20154;&#31867;&#26234;&#33021;&#36808;&#20986;&#20102;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;&#30001;&#36890;&#29992;&#20107;&#23454;&#25903;&#25345;&#30340;&#31616;&#21333;&#38382;&#39064;&#26102;&#65292;LLMs&#32463;&#24120;&#26410;&#33021;&#25552;&#20379;&#19968;&#33268;&#21644;&#20934;&#30830;&#30340;&#31572;&#26696;&#65292;&#34920;&#26126;&#20854;&#23384;&#22312;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#30340;&#19981;&#36275;&#12290;&#36825;&#24341;&#21457;&#20102;&#20851;&#20110;LLMs&#21040;&#24213;&#26159;&#22312;&#30495;&#27491;&#25512;&#29702;&#36824;&#26159;&#20165;&#20165;&#22312;&#35760;&#24518;&#30340;&#28608;&#28872;&#20105;&#35770;&#12290;&#37492;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21021;&#27493;&#30740;&#31350;&#26469;&#37327;&#21270;&#24182;&#28145;&#20837;&#25506;&#35752;&#29616;&#26377;LLMs&#30340;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#26174;&#31034;&#20986;&#23427;&#20204;&#30340;&#19968;&#33324;&#25512;&#29702;&#21644;&#25277;&#35937;&#25512;&#29702;&#34920;&#29616;&#20043;&#38388;&#23384;&#22312;&#23454;&#36136;&#24615;&#24046;&#24322;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23450;&#21046;&#20102;&#19968;&#20010;&#25277;&#35937;&#25512;&#29702;&#25968;&#25454;&#38598;&#65288;AbsR&#65289;&#65292;&#32467;&#21512;&#26377;&#24847;&#20041;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#25945;&#20250;LLMs&#22914;&#20309;&#21033;&#29992;&#36890;&#29992;&#20107;&#23454;&#36827;&#34892;&#25512;&#29702;&#12290;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26174;&#30528;&#25913;&#21892;LLMs&#22312;&#25277;&#35937;&#25512;&#29702;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09085v1 Announce Type: cross  Abstract: Large language models (LLMs) have developed impressive performance and strong explainability across various reasoning scenarios, marking a significant stride towards mimicking human-like intelligence. Despite this, when tasked with simple questions supported by a generic fact, LLMs often fail to provide consistent and precise answers, indicating a deficiency in abstract reasoning abilities. This has sparked a vigorous debate about whether LLMs are genuinely reasoning or merely memorizing. In light of this, we design a preliminary study to quantify and delve into the abstract reasoning abilities of existing LLMs. Our findings reveal a substantial discrepancy between their general reasoning and abstract reasoning performances. To relieve this problem, we tailor an abstract reasoning dataset (AbsR) together with a meaningful learning paradigm to teach LLMs how to leverage generic facts for reasoning purposes. The results show that our app
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#24212;&#20248;&#21183;&#30340;&#31574;&#30053;&#20248;&#21270;&#65288;A2PO&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#31163;&#32447;&#23398;&#20064;&#65292;&#33021;&#22815;&#35299;&#20915;&#22810;&#34892;&#20026;&#31574;&#30053;&#25910;&#38598;&#30340;&#32422;&#26463;&#20914;&#31361;&#38382;&#39064;&#65292;&#26377;&#25928;&#36991;&#20813;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07262</link><description>&lt;p&gt;
&#36866;&#24212;&#20248;&#21183;&#30340;&#31574;&#30053;&#20248;&#21270;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Advantage-Aware Policy Optimization for Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07262
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#24212;&#20248;&#21183;&#30340;&#31574;&#30053;&#20248;&#21270;&#65288;A2PO&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#31163;&#32447;&#23398;&#20064;&#65292;&#33021;&#22815;&#35299;&#20915;&#22810;&#34892;&#20026;&#31574;&#30053;&#25910;&#38598;&#30340;&#32422;&#26463;&#20914;&#31361;&#38382;&#39064;&#65292;&#26377;&#25928;&#36991;&#20813;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#33268;&#21147;&#20110;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#26469;&#21046;&#23450;&#26377;&#25928;&#30340;&#26234;&#33021;&#20307;&#31574;&#30053;&#65292;&#32780;&#26080;&#38656;&#22312;&#32447;&#20132;&#20114;&#65292;&#36890;&#36807;&#22312;&#34892;&#20026;&#31574;&#30053;&#30340;&#25903;&#25345;&#19979;&#26045;&#21152;&#36866;&#24403;&#30340;&#20445;&#23432;&#32422;&#26463;&#26469;&#35299;&#20915;&#20998;&#24067;&#22806;&#38382;&#39064;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#24212;&#20248;&#21183;&#30340;&#31574;&#30053;&#20248;&#21270;&#65288;A2PO&#65289;&#26041;&#27861;&#65292;&#20197;&#26126;&#30830;&#26500;&#24314;&#38024;&#23545;&#28151;&#21512;&#36136;&#37327;&#25968;&#25454;&#38598;&#30340;&#31163;&#32447;&#23398;&#20064;&#20248;&#21183;&#24863;&#30693;&#31574;&#30053;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07262v1 Announce Type: cross  Abstract: Offline Reinforcement Learning (RL) endeavors to leverage offline datasets to craft effective agent policy without online interaction, which imposes proper conservative constraints with the support of behavior policies to tackle the Out-Of-Distribution (OOD) problem. However, existing works often suffer from the constraint conflict issue when offline datasets are collected from multiple behavior policies, i.e., different behavior policies may exhibit inconsistent actions with distinct returns across the state space. To remedy this issue, recent Advantage-Weighted (AW) methods prioritize samples with high advantage values for agent training while inevitably leading to overfitting on these samples. In this paper, we introduce a novel Advantage-Aware Policy Optimization (A2PO) method to explicitly construct advantage-aware policy constraints for offline learning under mixed-quality datasets. Specifically, A2PO employs a Conditional Variat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38754;&#21521;&#26410;&#30693;&#20107;&#20214;&#30340;&#36866;&#24212;&#24615;&#20551;&#26032;&#38395;&#26816;&#27979;&#26694;&#26550;FADE&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22686;&#24378;&#21644;&#22270;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30446;&#26631;&#39044;&#27979;&#22120;&#65292;&#21516;&#26102;&#29420;&#31435;&#35757;&#32451;&#20107;&#20214;&#39044;&#27979;&#22120;&#65292;&#26368;&#32456;&#20943;&#36731;&#20107;&#20214;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2403.00037</link><description>&lt;p&gt;
&#26410;&#26469;&#21457;&#23637;&#65306;&#31038;&#20132;&#23186;&#20307;&#19978;&#30475;&#19981;&#35265;&#20107;&#20214;&#30340;&#36866;&#24212;&#24615;&#20551;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Evolving to the Future: Unseen Event Adaptive Fake News Detection on Social Media
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00037
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38754;&#21521;&#26410;&#30693;&#20107;&#20214;&#30340;&#36866;&#24212;&#24615;&#20551;&#26032;&#38395;&#26816;&#27979;&#26694;&#26550;FADE&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22686;&#24378;&#21644;&#22270;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30446;&#26631;&#39044;&#27979;&#22120;&#65292;&#21516;&#26102;&#29420;&#31435;&#35757;&#32451;&#20107;&#20214;&#39044;&#27979;&#22120;&#65292;&#26368;&#32456;&#20943;&#36731;&#20107;&#20214;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20551;&#26032;&#38395;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#24191;&#27867;&#20256;&#25773;&#26085;&#30410;&#23041;&#32961;&#20010;&#20154;&#21644;&#31038;&#20250;&#12290;&#22312;&#31038;&#20132;&#23186;&#20307;&#21160;&#24577;&#29615;&#22659;&#20013;&#65292;&#20551;&#26032;&#38395;&#26816;&#27979;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#26032;&#38395;&#25253;&#36947;&#36807;&#21435;&#20107;&#20214;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#30446;&#26631;&#26159;&#39044;&#27979;&#21644;&#35782;&#21035;&#26377;&#20851;&#26410;&#26469;&#20107;&#20214;&#30340;&#20551;&#26032;&#38395;&#65292;&#36825;&#20123;&#20107;&#20214;&#36890;&#24120;&#19982;&#36807;&#21435;&#23436;&#20840;&#19981;&#21516;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#26041;&#27861;&#23384;&#22312;&#40065;&#26834;&#24615;&#19981;&#36275;&#65292;&#26080;&#27861;&#27867;&#21270;&#21040;&#30475;&#19981;&#35265;&#30340;&#20107;&#20214;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#26410;&#26469;&#33258;&#36866;&#24212;&#20107;&#20214;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#65288;FADE&#65289;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#36866;&#24212;&#22686;&#24378;&#31574;&#30053;&#21644;&#22270;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30446;&#26631;&#39044;&#27979;&#22120;&#65292;&#20197;&#36827;&#34892;&#26356;&#31283;&#20581;&#30340;&#25972;&#20307;&#39044;&#27979;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#29420;&#31435;&#35757;&#32451;&#19968;&#20010;&#20165;&#20107;&#20214;&#30340;&#39044;&#27979;&#22120;&#20197;&#33719;&#24471;&#26377;&#20559;&#35265;&#30340;&#39044;&#27979;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#33719;&#24471;&#26368;&#32456;&#39044;&#27979;&#26469;&#36827;&#19968;&#27493;&#20943;&#36731;&#20107;&#20214;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00037v1 Announce Type: cross  Abstract: With the rapid development of social media, the wide dissemination of fake news on social media is increasingly threatening both individuals and society. In the dynamic landscape of social media, fake news detection aims to develop a model trained on news reporting past events. The objective is to predict and identify fake news about future events, which often relate to subjects entirely different from those in the past. However, existing fake detection methods exhibit a lack of robustness and cannot generalize to unseen events. To address this, we introduce Future ADaptive Event-based Fake news Detection (FADE) framework. Specifically, we train a target predictor through an adaptive augmentation strategy and graph contrastive learning to make more robust overall predictions. Simultaneously, we independently train an event-only predictor to obtain biased predictions. Then we further mitigate event bias by obtaining the final prediction
&lt;/p&gt;</description></item><item><title>FSS&#26159;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#21333;&#35270;&#22270;&#20154;&#20307;&#37325;&#24314;&#20013;&#20687;&#32032;&#23545;&#40784;&#38544;&#24335;&#27169;&#22411;&#30340;&#37319;&#26679;&#35757;&#32451;&#26041;&#26696;&#65292;&#36890;&#36807;&#20027;&#21160;&#36866;&#24212;&#34920;&#38754;&#30340;&#21402;&#24230;&#21644;&#22797;&#26434;&#24615;&#65292;&#20197;&#21450;&#21033;&#29992;&#26679;&#26412;&#28857;&#30340;&#27861;&#32447;&#26469;&#25913;&#21892;&#32467;&#26524;&#65292;&#21516;&#26102;&#24341;&#20837;&#32593;&#26684;&#21402;&#24230;&#25439;&#22833;&#20449;&#21495;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#35757;&#32451;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.19197</link><description>&lt;p&gt;
&#32454;&#32467;&#26500;&#24863;&#30693;&#37319;&#26679;: &#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#21333;&#35270;&#22270;&#20154;&#20307;&#37325;&#24314;&#20013;&#20687;&#32032;&#23545;&#40784;&#38544;&#24335;&#27169;&#22411;&#30340;&#37319;&#26679;&#35757;&#32451;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Fine Structure-Aware Sampling: A New Sampling Training Scheme for Pixel-Aligned Implicit Models in Single-View Human Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19197
&lt;/p&gt;
&lt;p&gt;
FSS&#26159;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#21333;&#35270;&#22270;&#20154;&#20307;&#37325;&#24314;&#20013;&#20687;&#32032;&#23545;&#40784;&#38544;&#24335;&#27169;&#22411;&#30340;&#37319;&#26679;&#35757;&#32451;&#26041;&#26696;&#65292;&#36890;&#36807;&#20027;&#21160;&#36866;&#24212;&#34920;&#38754;&#30340;&#21402;&#24230;&#21644;&#22797;&#26434;&#24615;&#65292;&#20197;&#21450;&#21033;&#29992;&#26679;&#26412;&#28857;&#30340;&#27861;&#32447;&#26469;&#25913;&#21892;&#32467;&#26524;&#65292;&#21516;&#26102;&#24341;&#20837;&#32593;&#26684;&#21402;&#24230;&#25439;&#22833;&#20449;&#21495;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20687;&#32032;&#23545;&#40784;&#30340;&#38544;&#24335;&#27169;&#22411;&#65292;&#22914;PIFu&#12289;PIFuHD&#21644;ICON&#65292;&#29992;&#20110;&#21333;&#35270;&#22270;&#30528;&#35013;&#20154;&#20307;&#37325;&#24314;&#12290;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#20351;&#29992;&#37319;&#26679;&#35757;&#32451;&#26041;&#26696;&#36827;&#34892;&#35757;&#32451;&#12290;&#29616;&#26377;&#30340;&#37319;&#26679;&#35757;&#32451;&#26041;&#26696;&#35201;&#20040;&#26080;&#27861;&#25429;&#25417;&#34180;&#34920;&#38754;&#65288;&#22914;&#32819;&#26421;&#12289;&#25163;&#25351;&#65289;&#65292;&#35201;&#20040;&#20250;&#23548;&#33268;&#37325;&#24314;&#32593;&#26684;&#20013;&#30340;&#22122;&#22768;&#20266;&#24433;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32454;&#32467;&#26500;&#24863;&#30693;&#37319;&#26679;&#65288;FSS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#21333;&#35270;&#22270;&#20154;&#20307;&#37325;&#24314;&#20013;&#35757;&#32451;&#20687;&#32032;&#23545;&#40784;&#38544;&#24335;&#27169;&#22411;&#30340;&#37319;&#26679;&#35757;&#32451;&#26041;&#26696;&#12290;FSS&#36890;&#36807;&#20027;&#21160;&#36866;&#24212;&#34920;&#38754;&#30340;&#21402;&#24230;&#21644;&#22797;&#26434;&#24615;&#26469;&#35299;&#20915;&#21069;&#36848;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#19982;&#29616;&#26377;&#30340;&#37319;&#26679;&#35757;&#32451;&#26041;&#26696;&#19981;&#21516;&#65292;FSS&#26174;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#26679;&#26412;&#28857;&#30340;&#27861;&#32447;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#39640;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#20026;&#36827;&#19968;&#27493;&#25913;&#36827;&#35757;&#32451;&#36807;&#31243;&#65292;FSS&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20687;&#32032;&#23545;&#40784;&#38544;&#24335;&#27169;&#22411;&#30340;&#32593;&#26684;&#21402;&#24230;&#25439;&#22833;&#20449;&#21495;&#12290;&#36825;&#20351;&#24471;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21033;&#29992;&#27861;&#32447;&#21464;&#24471;&#35745;&#31639;&#19978;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19197v1 Announce Type: cross  Abstract: Pixel-aligned implicit models, such as PIFu, PIFuHD, and ICON, are used for single-view clothed human reconstruction. These models need to be trained using a sampling training scheme. Existing sampling training schemes either fail to capture thin surfaces (e.g. ears, fingers) or cause noisy artefacts in reconstructed meshes. To address these problems, we introduce Fine Structured-Aware Sampling (FSS), a new sampling training scheme to train pixel-aligned implicit models for single-view human reconstruction. FSS resolves the aforementioned problems by proactively adapting to the thickness and complexity of surfaces. In addition, unlike existing sampling training schemes, FSS shows how normals of sample points can be capitalized in the training process to improve results. Lastly, to further improve the training process, FSS proposes a mesh thickness loss signal for pixel-aligned implicit models. It becomes computationally feasible to int
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;TimeXer&#65292;&#21033;&#29992;&#22806;&#37096;&#20449;&#24687;&#22686;&#24378;&#21464;&#21387;&#22120;&#23545;&#20869;&#29983;&#21464;&#37327;&#36827;&#34892;&#39044;&#27979;&#65292;&#24357;&#34917;&#20102;&#20197;&#24448;&#22810;&#21464;&#37327;&#25110;&#21333;&#21464;&#37327;&#39044;&#27979;&#20013;&#24573;&#35270;&#22806;&#29983;&#20449;&#24687;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2402.19072</link><description>&lt;p&gt;
TimeXer&#65306;&#21033;&#29992;&#22806;&#29983;&#21464;&#37327;&#22686;&#24378;&#21464;&#21387;&#22120;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TimeXer: Empowering Transformers for Time Series Forecasting with Exogenous Variables
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;TimeXer&#65292;&#21033;&#29992;&#22806;&#37096;&#20449;&#24687;&#22686;&#24378;&#21464;&#21387;&#22120;&#23545;&#20869;&#29983;&#21464;&#37327;&#36827;&#34892;&#39044;&#27979;&#65292;&#24357;&#34917;&#20102;&#20197;&#24448;&#22810;&#21464;&#37327;&#25110;&#21333;&#21464;&#37327;&#39044;&#27979;&#20013;&#24573;&#35270;&#22806;&#29983;&#20449;&#24687;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29616;&#23454;&#24212;&#29992;&#30340;&#37096;&#20998;&#35266;&#27979;&#24615;&#36136;&#65292;&#20165;&#19987;&#27880;&#20110;&#24863;&#20852;&#36259;&#30340;&#30446;&#26631;&#65292;&#20063;&#23601;&#26159;&#25152;&#35859;&#30340;&#20869;&#29983;&#21464;&#37327;&#65292;&#36890;&#24120;&#26159;&#19981;&#36275;&#20197;&#20445;&#35777;&#20934;&#30830;&#39044;&#27979;&#30340;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#31995;&#32479;&#36890;&#24120;&#35760;&#24405;&#20026;&#22810;&#20010;&#21464;&#37327;&#65292;&#20854;&#20013;&#22806;&#29983;&#24207;&#21015;&#21487;&#20197;&#20026;&#20869;&#29983;&#21464;&#37327;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#22806;&#37096;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#19982;&#20808;&#21069;&#30830;&#31435;&#30340;&#22810;&#21464;&#37327;&#25110;&#21333;&#21464;&#37327;&#39044;&#27979;&#19981;&#21516;&#65292;&#23427;&#20204;&#35201;&#20040;&#23558;&#25152;&#26377;&#21464;&#37327;&#31561;&#21516;&#23545;&#24453;&#65292;&#35201;&#20040;&#24573;&#35270;&#22806;&#29983;&#20449;&#24687;&#65292;&#26412;&#25991;&#20851;&#27880;&#30340;&#26159;&#19968;&#31181;&#23454;&#38469;&#35774;&#32622;&#65292;&#21363;&#20855;&#26377;&#22806;&#29983;&#21464;&#37327;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;TimeXer&#65292;&#21033;&#29992;&#22806;&#37096;&#20449;&#24687;&#22686;&#24378;&#20869;&#29983;&#21464;&#37327;&#30340;&#39044;&#27979;&#12290;&#36890;&#36807;&#24039;&#22937;&#35774;&#35745;&#30340;&#23884;&#20837;&#23618;&#65292;TimeXer&#20351;&#20256;&#32479;&#30340;Transformer&#26550;&#26500;&#20855;&#26377;&#37325;&#26032;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19072v1 Announce Type: cross  Abstract: Recent studies have demonstrated remarkable performance in time series forecasting. However, due to the partially-observed nature of real-world applications, solely focusing on the target of interest, so-called endogenous variables, is usually insufficient to guarantee accurate forecasting. Notably, a system is often recorded into multiple variables, where the exogenous series can provide valuable external information for endogenous variables. Thus, unlike prior well-established multivariate or univariate forecasting that either treats all the variables equally or overlooks exogenous information, this paper focuses on a practical setting, which is time series forecasting with exogenous variables. We propose a novel framework, TimeXer, to utilize external information to enhance the forecasting of endogenous variables. With a deftly designed embedding layer, TimeXer empowers the canonical Transformer architecture with the ability to reco
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#22788;&#29702;&#22810;&#35821;&#35328;&#20219;&#21153;&#30340;&#20986;&#33394;&#24615;&#33021;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#19981;&#21516;&#23618;&#27425;&#20013;&#22788;&#29702;&#22810;&#35821;&#35328;&#36755;&#20837;&#30340;&#31574;&#30053;&#65292;&#20197;&#21450;&#22788;&#29702;&#29305;&#23450;&#35821;&#35328;&#26102;&#30340;&#35821;&#35328;&#29305;&#23450;&#31070;&#32463;&#20803;&#23384;&#22312;&#12290;</title><link>https://arxiv.org/abs/2402.18815</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#22788;&#29702;&#22810;&#35821;&#35328;&#65311;
&lt;/p&gt;
&lt;p&gt;
How do Large Language Models Handle Multilingualism?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18815
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#22788;&#29702;&#22810;&#35821;&#35328;&#20219;&#21153;&#30340;&#20986;&#33394;&#24615;&#33021;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#19981;&#21516;&#23618;&#27425;&#20013;&#22788;&#29702;&#22810;&#35821;&#35328;&#36755;&#20837;&#30340;&#31574;&#30053;&#65292;&#20197;&#21450;&#22788;&#29702;&#29305;&#23450;&#35821;&#35328;&#26102;&#30340;&#35821;&#35328;&#29305;&#23450;&#31070;&#32463;&#20803;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20986;&#22312;&#21508;&#31181;&#35821;&#35328;&#19978;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#22788;&#29702;&#22810;&#35821;&#35328;&#65311;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#25551;&#36848;&#20102;LLMs&#22788;&#29702;&#22810;&#35821;&#35328;&#36755;&#20837;&#30340;&#36807;&#31243;&#65306;&#22312;&#21069;&#20960;&#23618;&#20013;&#65292;LLMs&#29702;&#35299;&#38382;&#39064;&#65292;&#23558;&#22810;&#35821;&#35328;&#36755;&#20837;&#36716;&#25442;&#20026;&#33521;&#35821;&#20197;&#20415;&#20419;&#36827;&#20219;&#21153;&#35299;&#20915;&#38454;&#27573;&#12290;&#22312;&#20013;&#38388;&#23618;&#20013;&#65292;LLMs&#36890;&#36807;&#20197;&#33521;&#35821;&#24605;&#32771;&#24182;&#25972;&#21512;&#22810;&#35821;&#35328;&#30693;&#35782;&#26469;&#36827;&#34892;&#35299;&#20915;&#38382;&#39064;&#65292;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#32467;&#26500;&#65292;&#20998;&#21035;&#33719;&#21462;&#20107;&#23454;&#20869;&#23481;&#12290;&#22312;&#26368;&#21518;&#20960;&#23618;&#20013;&#65292;LLMs&#29983;&#25104;&#19982;&#26597;&#35810;&#30340;&#21407;&#22987;&#35821;&#35328;&#19968;&#33268;&#30340;&#21709;&#24212;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22788;&#29702;&#29305;&#23450;&#35821;&#35328;&#26102;&#29305;&#23450;&#35821;&#35328;&#31070;&#32463;&#20803;&#30340;&#23384;&#22312;&#12290;&#20026;&#20102;&#26816;&#27979;&#30001;&#36755;&#20837;&#35821;&#35328;&#28608;&#27963;&#30340;&#31070;&#32463;&#20803;&#65292;&#21363;&#20351;&#27809;&#26377;&#26631;&#31614;&#65292;&#25105;&#20204;&#21019;&#26032;&#24615;&#22320;&#35774;&#35745;&#20102;&#19968;&#20010;&#24182;&#34892;&#35821;&#35328;&#29305;&#23450;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18815v1 Announce Type: cross  Abstract: Large language models (LLMs) demonstrate remarkable performance across a spectrum of languages. In this work, we delve into the question: How do LLMs handle multilingualism? We introduce a framework that depicts LLMs' processing of multilingual inputs: In the first several layers, LLMs understand the question, converting multilingual inputs into English to facilitate the task-solving phase. In the intermediate layers, LLMs engage in problem-solving by thinking in English and incorporating multilingual knowledge to obtain factual content, leveraging the self-attention and feed-forward structures, respectively. In the last several layers, LLMs generate responses that align with the original language of the query. In addition, we investigate the existence of language-specific neurons when processing a certain language. To detect neurons activated by the input language, even without labels, we innovatively design a Parallel Language specif
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;GenCode&#65292;&#36890;&#36807;&#37325;&#35201;&#24615;&#25351;&#26631;&#36873;&#25321;&#29983;&#25104;&#30340;&#20195;&#30721;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#22686;&#24378;&#20195;&#30721;&#29702;&#35299;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2402.15769</link><description>&lt;p&gt;
&#37325;&#28857;&#24341;&#23548;&#30340;&#25968;&#25454;&#22686;&#24378;&#29992;&#20110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20195;&#30721;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Importance Guided Data Augmentation for Neural-Based Code Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15769
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;GenCode&#65292;&#36890;&#36807;&#37325;&#35201;&#24615;&#25351;&#26631;&#36873;&#25321;&#29983;&#25104;&#30340;&#20195;&#30721;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#22686;&#24378;&#20195;&#30721;&#29702;&#35299;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15769v1 &#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#27169;&#22411;&#24320;&#21551;&#20102;&#20195;&#30721;&#26234;&#33021;&#26102;&#20195;&#12290;&#26368;&#36817;&#35768;&#22810;&#27169;&#22411;&#37117;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#22312;&#20195;&#30721;&#23398;&#20064;&#39046;&#22495;&#65292;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#26159;&#33258;&#21160;&#36827;&#34892;&#20195;&#30721;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#24110;&#21161;&#24320;&#21457;&#32773;&#20934;&#22791;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#23578;&#19981;&#36275;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;GenCode&#65292;&#29992;&#20110;&#22686;&#24378;&#20195;&#30721;&#29702;&#35299;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;GenCode&#36981;&#24490;&#19968;&#31181;&#29983;&#25104;&#21644;&#36873;&#25321;&#30340;&#33539;&#24335;&#26469;&#20934;&#22791;&#26377;&#29992;&#30340;&#35757;&#32451;&#20195;&#30721;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#20351;&#29992;&#20195;&#30721;&#36716;&#25442;&#25216;&#26415;&#39318;&#20808;&#29983;&#25104;&#26032;&#30340;&#20195;&#30721;&#20505;&#36873;&#65292;&#28982;&#21518;&#36890;&#36807;&#37325;&#35201;&#24615;&#25351;&#26631;&#36873;&#25321;&#37325;&#35201;&#30340;&#20195;&#30721;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#12290;&#20026;&#20102;&#35780;&#20272;GenCode&#19982;&#36890;&#29992;&#37325;&#35201;&#24615;&#25351;&#26631;&#65288;&#25439;&#22833;&#20540;&#65289;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#22235;&#20010;&#20195;&#30721;&#29702;&#35299;&#20219;&#21153;&#65288;&#22914;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#65289;&#21644;&#19977;&#20010;&#39044;&#35757;&#32451;&#20195;&#30721;&#27169;&#22411;&#65288;&#22914;CodeT5&#65289;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#20195;&#30721;&#22686;&#24378;&#25216;&#26415;&#30456;&#27604;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15769v1 Announce Type: cross  Abstract: Pre-trained code models lead the era of code intelligence. Many models have been designed with impressive performance recently. However, one important problem, data augmentation for code data that automatically helps developers prepare training data lacks study in the field of code learning. In this paper, we introduce a general data augmentation framework, GenCode, to enhance the training of code understanding models. GenCode follows a generation-and-selection paradigm to prepare useful training codes. Specifically, it uses code transformation techniques to generate new code candidates first and then selects important ones as the training data by importance metrics. To evaluate the effectiveness of GenCode with a general importance metric -- loss value, we conduct experiments on four code understanding tasks (e.g., code clone detection) and three pre-trained code models (e.g., CodeT5). Compared to the state-of-the-art (SOTA) code augm
&lt;/p&gt;</description></item><item><title>AgentOhana&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#25968;&#25454;&#21644;&#35757;&#32451;&#27969;&#27700;&#32447;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#21161;&#20110;&#20811;&#26381;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#26234;&#33021;&#20307;&#20219;&#21153;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15506</link><description>&lt;p&gt;
AgentOhana&#65306;&#20026;&#26377;&#25928;&#26234;&#33021;&#20307;&#23398;&#20064;&#35774;&#35745;&#32479;&#19968;&#25968;&#25454;&#21644;&#35757;&#32451;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15506
&lt;/p&gt;
&lt;p&gt;
AgentOhana&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#25968;&#25454;&#21644;&#35757;&#32451;&#27969;&#27700;&#32447;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#21161;&#20110;&#20811;&#26381;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#26234;&#33021;&#20307;&#20219;&#21153;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#25903;&#25345;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;&#24341;&#36215;&#20102;&#37325;&#22823;&#30740;&#31350;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20805;&#20998;&#21033;&#29992;LLMs&#30340;&#28508;&#21147;&#36827;&#34892;&#22522;&#20110;&#26234;&#33021;&#20307;&#30340;&#20219;&#21153;&#38754;&#20020;&#22256;&#38590;&#65292;&#36825;&#26159;&#30001;&#20110;&#20855;&#26377;&#22810;&#36718;&#36712;&#36857;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#28304;&#30340;&#24322;&#26500;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;AgentOhana&#20316;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#12290;AgentOhana&#20174;&#19981;&#21516;&#29615;&#22659;&#20013;&#32858;&#21512;&#26234;&#33021;&#20307;&#36712;&#36857;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#24773;&#26223;&#12290;&#23427;&#31934;&#24515;&#22320;&#23558;&#36825;&#20123;&#36712;&#36857;&#26631;&#20934;&#21270;&#21644;&#32479;&#19968;&#21040;&#19968;&#33268;&#30340;&#26684;&#24335;&#20013;&#65292;&#31616;&#21270;&#20102;&#20026;&#26234;&#33021;&#20307;&#35757;&#32451;&#20248;&#21270;&#30340;&#36890;&#29992;&#25968;&#25454;&#21152;&#36733;&#22120;&#30340;&#21019;&#24314;&#12290;&#36890;&#36807;&#25968;&#25454;&#32479;&#19968;&#65292;&#25105;&#20204;&#30340;&#35757;&#32451;&#27969;&#27700;&#32447;&#22312;&#19981;&#21516;&#25968;&#25454;&#28304;&#20043;&#38388;&#20445;&#25345;&#24179;&#34913;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#21010;&#20998;&#21644;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#35774;&#22791;&#20043;&#38388;&#30340;&#29420;&#31435;&#38543;&#26426;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;xLAM-v0.1&#65292;&#19968;&#20010;&#22823;&#21160;&#20316;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15506v1 Announce Type: new  Abstract: Autonomous agents powered by large language models (LLMs) have garnered significant research attention. However, fully harnessing the potential of LLMs for agent-based tasks presents inherent challenges due to the heterogeneous nature of diverse data sources featuring multi-turn trajectories. In this paper, we introduce \textbf{AgentOhana} as a comprehensive solution to address these challenges. \textit{AgentOhana} aggregates agent trajectories from distinct environments, spanning a wide array of scenarios. It meticulously standardizes and unifies these trajectories into a consistent format, streamlining the creation of a generic data loader optimized for agent training. Leveraging the data unification, our training pipeline maintains equilibrium across different data sources and preserves independent randomness across devices during dataset partitioning and model training. Additionally, we present \textbf{xLAM-v0.1}, a large action mode
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#29702;&#35299;&#39640;&#25928;Transformer&#65288;&#20363;&#22914;&#31232;&#30095;Transformer&#21644;&#32447;&#24615;Transformer&#65289;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#65292;&#21457;&#29616;&#23427;&#20204;&#36866;&#21512;&#35299;&#20915;&#19968;&#33324;DP&#20219;&#21153;&#65292;&#20294;&#19981;&#21516;&#20110;&#26631;&#20934;Transformer&#12290;</title><link>https://arxiv.org/abs/2402.13934</link><description>&lt;p&gt;
&#30830;&#23454;&#39640;&#25928;&#30340;Transformer&#33021;&#22815;&#33410;&#32422;&#35745;&#31639;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Efficient Transformers Really Save Computation?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#29702;&#35299;&#39640;&#25928;Transformer&#65288;&#20363;&#22914;&#31232;&#30095;Transformer&#21644;&#32447;&#24615;Transformer&#65289;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#65292;&#21457;&#29616;&#23427;&#20204;&#36866;&#21512;&#35299;&#20915;&#19968;&#33324;DP&#20219;&#21153;&#65292;&#20294;&#19981;&#21516;&#20110;&#26631;&#20934;Transformer&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#36234;&#26469;&#36234;&#22823;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;&#24182;&#25317;&#26377;&#22823;&#37327;&#21442;&#25968;&#65292;&#25214;&#21040;&#26356;&#39640;&#25928;&#30340;&#26367;&#20195;&#26631;&#20934;Transformer&#21464;&#24471;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#39640;&#25928;&#30340;Transformer&#21644;Transformer&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#20294;&#27809;&#26377;&#19968;&#20010;&#33021;&#22815;&#25552;&#20379;&#23427;&#20204;&#36866;&#21512;&#26367;&#20195;&#26631;&#20934;Transformer&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#36825;&#20351;&#24471;&#24456;&#38590;&#30830;&#23450;&#20309;&#26102;&#20351;&#29992;&#29305;&#23450;&#27169;&#22411;&#20197;&#21450;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#37325;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#29702;&#35299;&#39640;&#25928;Transformer&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#29305;&#21035;&#26159;&#31232;&#30095;Transformer&#21644;&#32447;&#24615;Transformer&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#23427;&#20204;&#22312;Chain-of-Thought (CoT)&#25552;&#31034;&#20013;&#23637;&#31034;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#36981;&#24490;&#20808;&#21069;&#30340;&#30740;&#31350;&#23558;&#23427;&#20204;&#24314;&#27169;&#20026;&#21160;&#24577;&#35268;&#21010;&#65288;DP&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#36275;&#22815;&#34920;&#36798;&#35299;&#20915;&#19968;&#33324;DP&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#20294;&#19982;&#26631;&#20934;Transformer&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13934v1 Announce Type: cross  Abstract: As transformer-based language models are trained on increasingly large datasets and with vast numbers of parameters, finding more efficient alternatives to the standard Transformer has become very valuable. While many efficient Transformers and Transformer alternatives have been proposed, none provide theoretical guarantees that they are a suitable replacement for the standard Transformer. This makes it challenging to identify when to use a specific model and what directions to prioritize for further investigation. In this paper, we aim to understand the capabilities and limitations of efficient Transformers, specifically the Sparse Transformer and the Linear Transformer. We focus on their reasoning capability as exhibited by Chain-of-Thought (CoT) prompts and follow previous works to model them as Dynamic Programming (DP) problems. Our results show that while these models are expressive enough to solve general DP tasks, contrary to ex
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25193;&#20805;&#26435;&#37325;&#31354;&#38388;&#30340;&#25968;&#25454;&#38598;&#65292;&#37319;&#29992;MixUp&#26041;&#27861;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#26435;&#37325;&#31354;&#38388;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04081</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#20805;&#25913;&#36827;&#26435;&#37325;&#31354;&#38388;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improved Generalization of Weight Space Networks via Augmentations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04081
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25193;&#20805;&#26435;&#37325;&#31354;&#38388;&#30340;&#25968;&#25454;&#38598;&#65292;&#37319;&#29992;MixUp&#26041;&#27861;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#26435;&#37325;&#31354;&#38388;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#26435;&#37325;&#31354;&#38388;&#65288;DWS&#65289;&#20013;&#30340;&#23398;&#20064;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#22788;&#29702;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#26469;&#36827;&#34892;&#23398;&#20064;&#65292;&#23427;&#22312;2D&#21644;3D&#31070;&#32463;&#22330;&#65288;INRs&#65292;NeRFs&#65289;&#20197;&#21450;&#23545;&#20854;&#20182;&#31867;&#22411;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25512;&#29702;&#26041;&#38754;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#26435;&#37325;&#31354;&#38388;&#27169;&#22411;&#24448;&#24448;&#23481;&#26131;&#21463;&#21040;&#36807;&#25311;&#21512;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#20102;&#36807;&#25311;&#21512;&#30340;&#21407;&#22240;&#65292;&#24182;&#21457;&#29616;&#19968;&#20010;&#20851;&#38190;&#21407;&#22240;&#26159;DWS&#25968;&#25454;&#38598;&#30340;&#32570;&#20047;&#22810;&#26679;&#24615;&#12290;&#34429;&#28982;&#19968;&#20010;&#32473;&#23450;&#30340;&#23545;&#35937;&#21487;&#20197;&#34987;&#35768;&#22810;&#19981;&#21516;&#30340;&#26435;&#37325;&#37197;&#32622;&#25152;&#34920;&#31034;&#65292;&#20294;&#20856;&#22411;&#30340;INR&#35757;&#32451;&#38598;&#26410;&#33021;&#25429;&#25417;&#21040;&#34920;&#31034;&#21516;&#19968;&#23545;&#35937;&#30340;&#19981;&#21516;INR&#20043;&#38388;&#30340;&#21464;&#24322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26435;&#37325;&#31354;&#38388;&#20013;&#30340;&#25968;&#25454;&#25193;&#20805;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#26435;&#37325;&#31354;&#38388;&#30340;MixUp&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#35774;&#32622;&#20013;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#25552;&#21319;&#31867;&#20284;&#20110;&#25317;&#26377;&#22810;&#36798;10&#20493;&#30340;&#25968;&#25454;&#37327;&#12290;&#22312;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#20013;&#65292;&#23427;&#20204;&#20135;&#29983;&#20102;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning in deep weight spaces (DWS), where neural networks process the weights of other neural networks, is an emerging research direction, with applications to 2D and 3D neural fields (INRs, NeRFs), as well as making inferences about other types of neural networks. Unfortunately, weight space models tend to suffer from substantial overfitting. We empirically analyze the reasons for this overfitting and find that a key reason is the lack of diversity in DWS datasets. While a given object can be represented by many different weight configurations, typical INR training sets fail to capture variability across INRs that represent the same object. To address this, we explore strategies for data augmentation in weight spaces and propose a MixUp method adapted for weight spaces. We demonstrate the effectiveness of these methods in two setups. In classification, they improve performance similarly to having up to 10 times more data. In self-supervised contrastive learning, they yield substanti
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#35821;&#38899;&#36716;&#24405;&#21644;&#35821;&#38899;&#38750;&#35328;&#35821;&#29305;&#24449;&#25972;&#21512;&#21040;LLM&#20915;&#31574;&#20013;&#26469;&#25913;&#21892;&#26426;&#22120;&#20154;&#23548;&#33322;&#20013;&#30340;&#20915;&#31574;&#33021;&#21147;&#65292;&#36229;&#36234;&#20102;&#20165;&#20351;&#29992;&#25991;&#23383;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.03494</link><description>&lt;p&gt;
&#36229;&#36234;&#25991;&#23383;&#65306;&#36890;&#36807;&#35821;&#38899;&#32447;&#32034;&#25913;&#21892;LLM&#22312;&#26426;&#22120;&#20154;&#23548;&#33322;&#20013;&#30340;&#20915;&#31574;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Beyond Text: Improving LLM's Decision Making for Robot Navigation via Vocal Cues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#35821;&#38899;&#36716;&#24405;&#21644;&#35821;&#38899;&#38750;&#35328;&#35821;&#29305;&#24449;&#25972;&#21512;&#21040;LLM&#20915;&#31574;&#20013;&#26469;&#25913;&#21892;&#26426;&#22120;&#20154;&#23548;&#33322;&#20013;&#30340;&#20915;&#31574;&#33021;&#21147;&#65292;&#36229;&#36234;&#20102;&#20165;&#20351;&#29992;&#25991;&#23383;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#24378;&#35843;&#20102;&#22522;&#20110;&#25991;&#26412;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#20851;&#38190;&#32570;&#28857;&#65292;&#34920;&#26126;&#20165;&#20351;&#29992;&#25991;&#26412;&#20316;&#20026;&#23545;&#35805;&#30340;&#27169;&#24577;&#22312;&#27492;&#31867;&#24212;&#29992;&#20013;&#23384;&#22312;&#19981;&#36275;&#20043;&#22788;&#12290;&#34429;&#28982;LLM&#22312;&#22788;&#29702;&#25991;&#26412;&#26041;&#38754;&#22312;&#36825;&#20123;&#20154;&#26426;&#23545;&#35805;&#20013;&#38750;&#24120;&#20986;&#33394;&#65292;&#20294;&#22312;&#31038;&#20132;&#23548;&#33322;&#31561;&#24773;&#22659;&#19979;&#65292;&#20182;&#20204;&#22312;&#22788;&#29702;&#21475;&#22836;&#25351;&#20196;&#30340;&#32454;&#24494;&#20043;&#22788;&#26102;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#20854;&#20013;&#30340;&#27495;&#20041;&#21644;&#19981;&#30830;&#23450;&#24615;&#21487;&#33021;&#20250;&#21066;&#24369;&#23545;&#26426;&#22120;&#20154;&#21644;&#20854;&#20182;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20449;&#20219;&#12290;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#36229;&#36234;&#25991;&#23383;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#36825;&#20123;&#38899;&#39057;&#22238;&#24212;&#30340;&#35821;&#38899;&#38750;&#35328;&#35821;&#29305;&#24449;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#20123;&#29305;&#24449;&#26159;&#21475;&#22836;&#20132;&#27969;&#20013;&#19981;&#28041;&#21450;&#25991;&#23383;&#25514;&#36766;&#30340;&#26041;&#38754;&#65292;&#36890;&#36807;&#34920;&#36798;&#26041;&#24335;&#20256;&#36798;&#24847;&#20041;&#21644;&#32454;&#24494;&#24046;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#36229;&#36234;&#25991;&#23383;&#8221;&#65307;&#19968;&#31181;&#36890;&#36807;&#38598;&#25104;&#38899;&#39057;&#36716;&#24405;&#20197;&#21450;&#36825;&#20123;&#29305;&#24449;&#30340;&#37096;&#20998;&#26469;&#25913;&#21892;LLM&#20915;&#31574;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#29305;&#24449;&#20391;&#37325;&#24773;&#24863;&#21644;&#26356;&#19982;&#20154;&#26426;&#23545;&#35805;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work highlights a critical shortcoming in text-based Large Language Models (LLMs) used for human-robot interaction, demonstrating that text alone as a conversation modality falls short in such applications. While LLMs excel in processing text in these human conversations, they struggle with the nuances of verbal instructions in scenarios like social navigation, where ambiguity and uncertainty can erode trust in robotic and other AI systems. We can address this shortcoming by moving beyond text and additionally focusing on the paralinguistic features of these audio responses. These features are the aspects of spoken communication that do not involve the literal wording (lexical content) but convey meaning and nuance through how something is said. We present "Beyond Text"; an approach that improves LLM decision-making by integrating audio transcription along with a subsection of these features, which focus on the affect and more relevant in human-robot conversations. This approach n
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;RLHF&#30340;&#26694;&#26550;&#65292;&#22312;&#20854;&#20013;&#32771;&#34385;&#20102;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#22870;&#21169;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#23558;&#22522;&#25968;&#21453;&#39304;&#21644;&#20915;&#26007;&#21453;&#39304;&#32553;&#20943;&#20026;PORRL&#24418;&#24335;&#36827;&#34892;&#20102;&#24314;&#27169;&#21644;&#31639;&#27861;&#24320;&#21457;&#12290;</title><link>https://arxiv.org/abs/2402.03282</link><description>&lt;p&gt;
&#19968;&#20010;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#22870;&#21169;&#29366;&#24577;&#22312;RLHF&#20013;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Partially Observed Reward-States in RLHF
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03282
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;RLHF&#30340;&#26694;&#26550;&#65292;&#22312;&#20854;&#20013;&#32771;&#34385;&#20102;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#22870;&#21169;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#23558;&#22522;&#25968;&#21453;&#39304;&#21644;&#20915;&#26007;&#21453;&#39304;&#32553;&#20943;&#20026;PORRL&#24418;&#24335;&#36827;&#34892;&#20102;&#24314;&#27169;&#21644;&#31639;&#27861;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#26469;&#65292;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#30340;&#30740;&#31350;&#22240;&#20854;&#22312;LLMs&#30340;&#21457;&#23637;&#20013;&#36215;&#21040;&#30340;&#20316;&#29992;&#32780;&#21464;&#24471;&#37325;&#35201;&#12290;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#34920;&#26126;&#65292;&#20154;&#31867;&#23545;&#21050;&#28608;&#30340;&#21453;&#24212;&#24050;&#30693;&#20381;&#36182;&#20110;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#8220;&#20869;&#37096;&#29366;&#24577;&#8221;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#24403;&#21069;&#30340;RLHF&#27169;&#22411;&#27809;&#26377;&#32771;&#34385;&#21040;&#36825;&#19968;&#28857;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;RLHF&#27169;&#22411;&#27809;&#26377;&#32771;&#34385;&#21040;&#20013;&#38388;&#21453;&#39304;&#65292;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#21487;&#20197;&#24110;&#21161;&#25552;&#39640;&#26679;&#26412;&#22797;&#26434;&#24615;&#21644;&#23545;&#40784;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#23558;RLHF&#24314;&#27169;&#20026;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#22870;&#21169;&#29366;&#24577;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;PORRL&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;RLHF&#20013;&#20004;&#31181;&#20027;&#35201;&#24418;&#24335;&#30340;&#20154;&#31867;&#21453;&#39304; - &#22522;&#25968;&#21453;&#39304;&#21644;&#20915;&#26007;&#21453;&#39304;&#21040;PORRL&#30340;&#32553;&#20943;&#12290;&#23545;&#20110;&#22522;&#25968;&#21453;&#39304;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#36890;&#29992;&#30340;&#32479;&#35745;&#39640;&#25928;&#31639;&#27861;&#65292;&#24182;&#23558;&#23427;&#20204;&#23454;&#20363;&#21270;&#20026;POR-UCRL&#21644;POR-UCBVI&#12290;&#23545;&#20110;&#20915;&#26007;&#21453;&#39304;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#31616;&#21333;&#30340;&#22522;&#25968;&#21453;&#39304;&#32553;&#20943;&#19981;&#33021;&#36798;&#21040;&#20122;&#32447;&#24615;&#30340;&#20915;&#26007;&#22238;&#24402;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of reinforcement learning from human feedback (RLHF) has gained prominence in recent years due to its role in the development of LLMs. Neuroscience research shows that human responses to stimuli are known to depend on partially-observed "internal states." Unfortunately current models of RLHF do not take take this into consideration. Moreover most RLHF models do not account for intermediate feedback, which is gaining importance in empirical work and can help improve both sample complexity and alignment. To address these limitations, we model RLHF as reinforcement learning with partially observed reward-states (PORRL). We show reductions from the the two dominant forms of human feedback in RLHF - cardinal and dueling feedback to PORRL. For cardinal feedback, we develop generic statistically efficient algorithms and instantiate them to present POR-UCRL and POR-UCBVI. For dueling feedback, we show that a naive reduction to cardinal feedback fails to achieve sublinear dueling regr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#24341;&#23548;&#36710;&#30340;&#22312;&#32447;&#12289;&#26080;&#20914;&#31361;&#35843;&#24230;&#21644;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#30340;&#22522;&#20110;&#24490;&#29615;&#22270;&#30340;&#31639;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31639;&#27861;&#35201;&#20040;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#65292;&#35201;&#20040;&#22312;&#26356;&#30701;&#30340;&#35745;&#31639;&#26102;&#38388;&#20869;&#33719;&#24471;&#21516;&#26679;&#33391;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;</title><link>https://arxiv.org/abs/2310.02195</link><description>&lt;p&gt;
&#39640;&#25928;&#22312;&#32447;&#35843;&#24230;&#21644;&#36335;&#24452;&#35268;&#21010;&#65306;&#22522;&#20110;&#24490;&#29615;&#22270;&#30340;&#33258;&#21160;&#24341;&#23548;&#36710;
&lt;/p&gt;
&lt;p&gt;
Efficient Online Scheduling and Routing for Automated Guided Vehicles In Loop-Based Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02195
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#24341;&#23548;&#36710;&#30340;&#22312;&#32447;&#12289;&#26080;&#20914;&#31361;&#35843;&#24230;&#21644;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#30340;&#22522;&#20110;&#24490;&#29615;&#22270;&#30340;&#31639;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31639;&#27861;&#35201;&#20040;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#65292;&#35201;&#20040;&#22312;&#26356;&#30701;&#30340;&#35745;&#31639;&#26102;&#38388;&#20869;&#33719;&#24471;&#21516;&#26679;&#33391;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#24341;&#23548;&#36710;&#65288;AGVs&#65289;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#34892;&#21508;&#19994;&#65292;&#20197;&#26080;&#20914;&#31361;&#26041;&#24335;&#23545;&#23427;&#20204;&#36827;&#34892;&#35843;&#24230;&#21644;&#36335;&#24452;&#35268;&#21010;&#23545;&#20110;&#23427;&#20204;&#30340;&#39640;&#25928;&#36816;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#22270;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#20219;&#24847;&#23481;&#37327;&#21644;&#39034;&#24207;&#20316;&#19994;&#30340;AGVs&#30340;&#22312;&#32447;&#12289;&#26080;&#20914;&#31361;&#35843;&#24230;&#21644;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#19982;&#31934;&#30830;&#26041;&#27861;&#12289;&#36138;&#23146;&#21551;&#21457;&#24335;&#26041;&#27861;&#21644;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20195;&#34920;&#23454;&#38469;&#21046;&#36896;&#21378;&#30340;&#27169;&#22411;&#19978;&#20351;&#29992;&#29702;&#35770;&#21644;&#30495;&#23454;&#23454;&#20363;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#35201;&#20040;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#65292;&#35201;&#20040;&#22312;&#26356;&#30701;&#30340;&#35745;&#31639;&#26102;&#38388;&#20869;&#33719;&#24471;&#21516;&#26679;&#33391;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02195v2 Announce Type: replace-cross  Abstract: Automated guided vehicles (AGVs) are widely used in various industries, and scheduling and routing them in a conflict-free manner is crucial to their efficient operation. We propose a loop-based algorithm that solves the online, conflict-free scheduling and routing problem for AGVs with any capacity and ordered jobs in loop-based graphs. The proposed algorithm is compared against an exact method, a greedy heuristic and a metaheuristic. We experimentally show, using theoretical and real instances on a model representing a real manufacturing plant, that this algorithm either outperforms the other algorithms or gets an equally good solution in less computing time.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#20581;&#24247;&#25991;&#26412;&#31616;&#21270;&#30740;&#31350;&#30340;&#28040;&#21270;&#30284;&#30151;&#25945;&#32946;&#26448;&#26009;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#65292;&#24182;&#25506;&#32034;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;&#24494;&#35843;&#12289;&#22686;&#24378;&#23398;&#20064;&#12289;&#22686;&#24378;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.15043</link><description>&lt;p&gt;
&#20581;&#24247;&#25991;&#26412;&#31616;&#21270;&#65306;&#28040;&#21270;&#30284;&#30151;&#25945;&#32946;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#21644;&#22686;&#24378;&#23398;&#20064;&#30340;&#26032;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Health Text Simplification: An Annotated Corpus for Digestive Cancer Education and Novel Strategies for Reinforcement Learning. (arXiv:2401.15043v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15043
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#20581;&#24247;&#25991;&#26412;&#31616;&#21270;&#30740;&#31350;&#30340;&#28040;&#21270;&#30284;&#30151;&#25945;&#32946;&#26448;&#26009;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#65292;&#24182;&#25506;&#32034;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;&#24494;&#35843;&#12289;&#22686;&#24378;&#23398;&#20064;&#12289;&#22686;&#24378;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#20581;&#24247;&#25945;&#32946;&#26448;&#26009;&#30340;&#38405;&#35835;&#27700;&#24179;&#26174;&#33879;&#24433;&#21709;&#20449;&#24687;&#30340;&#21487;&#29702;&#35299;&#24615;&#21644;&#21487;&#25509;&#35302;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23569;&#25968;&#26063;&#35028;&#20154;&#32676;&#12290;&#35768;&#22810;&#24739;&#32773;&#25945;&#32946;&#36164;&#28304;&#36229;&#36807;&#20102;&#24191;&#27867;&#25509;&#21463;&#30340;&#26631;&#20934;&#30340;&#38405;&#35835;&#27700;&#24179;&#21644;&#22797;&#26434;&#24615;&#12290;&#22312;&#20581;&#24247;&#20449;&#24687;&#20013;&#65292;&#24613;&#38656;&#39640;&#24615;&#33021;&#30340;&#25991;&#26412;&#31616;&#21270;&#27169;&#22411;&#20197;&#22686;&#24378;&#20256;&#25773;&#21644;&#35782;&#23383;&#33021;&#21147;&#12290;&#36825;&#31181;&#38656;&#35201;&#22312;&#30284;&#30151;&#25945;&#32946;&#20013;&#23588;&#20026;&#36843;&#20999;&#65292;&#26377;&#25928;&#30340;&#39044;&#38450;&#21644;&#31579;&#26597;&#25945;&#32946;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#21457;&#30149;&#29575;&#21644;&#27515;&#20129;&#29575;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#24341;&#20837;&#20102;&#31616;&#21270;&#30340;&#28040;&#21270;&#30284;&#30151;&#65288;SimpleDC&#65289;&#24182;&#34892;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#20581;&#24247;&#25991;&#26412;&#31616;&#21270;&#30740;&#31350;&#12290;&#21033;&#29992;SimpleDC&#21644;&#29616;&#26377;&#30340;Med-EASi&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;&#24494;&#35843;&#12289;&#22686;&#24378;&#23398;&#20064;&#65288;RL&#65289;&#12289;&#22686;&#24378;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: The reading level of health educational materials significantly influences information understandability and accessibility, particularly for minoritized populations. Many patient educational resources surpass the reading level and complexity of widely accepted standards. There is a critical need for high-performing text simplification models in health information to enhance dissemination and literacy. This need is particularly acute in cancer education, where effective prevention and screening education can substantially reduce morbidity and mortality.  Methods: We introduce Simplified Digestive Cancer (SimpleDC), a parallel corpus of cancer education materials tailored for health text simplification research. Utilizing SimpleDC alongside the existing Med-EASi corpus, we explore Large Language Model (LLM)-based simplification methods, including fine-tuning, reinforcement learning (RL), reinforcement learning with human feedback (RLHF), domain adaptation, and prompt-based app
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#20132;&#25442;&#28436;&#31639;&#20013;&#32771;&#34385;&#20102;&#38598;&#21512;&#35774;&#22791;&#30340;&#21160;&#24577;&#21512;&#20316;&#34892;&#20026;&#65292;&#25552;&#20986;&#20102;&#20998;&#24067;&#24335;&#38598;&#20307;&#36807;&#31243;&#30340;&#25277;&#35937;&#34920;&#31034;&#65292;&#29992;&#20110;&#32534;&#31243;&#35745;&#31639;&#38598;&#20307;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2401.11212</link><description>&lt;p&gt;
&#22312;&#20132;&#25442;&#28436;&#31639;&#20013;&#32534;&#31243;&#20998;&#24067;&#24335;&#38598;&#20307;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Programming Distributed Collective Processes in the eXchange Calculus. (arXiv:2401.11212v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#20132;&#25442;&#28436;&#31639;&#20013;&#32771;&#34385;&#20102;&#38598;&#21512;&#35774;&#22791;&#30340;&#21160;&#24577;&#21512;&#20316;&#34892;&#20026;&#65292;&#25552;&#20986;&#20102;&#20998;&#24067;&#24335;&#38598;&#20307;&#36807;&#31243;&#30340;&#25277;&#35937;&#34920;&#31034;&#65292;&#29992;&#20110;&#32534;&#31243;&#35745;&#31639;&#38598;&#20307;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#36235;&#21183;&#22914;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#25552;&#20986;&#20102;&#22312;&#20960;&#20046;&#25152;&#26377;&#29615;&#22659;&#20013;&#23494;&#38598;&#21644;&#22810;&#23610;&#24230;&#37096;&#32626;&#35745;&#31639;&#35774;&#22791;&#30340;&#24895;&#26223;&#12290;&#19968;&#20010;&#31361;&#20986;&#30340;&#24037;&#31243;&#25361;&#25112;&#22260;&#32469;&#30528;&#32534;&#31243;&#36825;&#31181;&#35745;&#31639;&#29983;&#24577;&#31995;&#32479;&#30340;&#38598;&#20307;&#33258;&#36866;&#24212;&#34892;&#20026;&#12290;&#36825;&#38656;&#35201;&#33021;&#22815;&#25429;&#25417;&#27010;&#24565;&#65288;&#21160;&#24577;&#21512;&#20316;&#35774;&#22791;&#32676;&#32452;&#65289;&#21644;&#38598;&#20307;&#20219;&#21153;&#65288;&#30001;&#21512;&#22863;&#32452;&#25191;&#34892;&#30340;&#32852;&#21512;&#27963;&#21160;&#65289;&#30340;&#25277;&#35937;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#19982;&#37051;&#23621;&#20132;&#20114;&#24182;&#20197;&#20960;&#20046;&#21516;&#27493;&#30340;&#24863;&#30693;-&#35745;&#31639;-&#20132;&#20114;&#24490;&#29615;&#25191;&#34892;&#30340;&#35774;&#22791;&#38598;&#21512;&#65292;&#20854;&#20013;&#35745;&#31639;&#30001;&#19968;&#20010;&#23558;&#24863;&#30693;&#20540;&#21644;&#20256;&#20837;&#28040;&#24687;&#26144;&#23556;&#21040;&#36755;&#20986;&#21644;&#20256;&#20986;&#28040;&#24687;&#30340;&#21333;&#20010;&#31243;&#24207;&#32473;&#20986;&#12290;&#20026;&#20102;&#25903;&#25345;&#25972;&#20010;&#35745;&#31639;&#38598;&#20307;&#30340;&#32534;&#31243;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#24067;&#24335;&#38598;&#20307;&#36807;&#31243;&#30340;&#25277;&#35937;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#23450;&#20041;&#21512;&#22863;&#32452;&#30340;&#24418;&#25104;&#36923;&#36753;&#21644;&#23427;&#30340;&#38598;&#20307;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#20132;&#25442;&#28436;&#31639;&#20013;&#24418;&#24335;&#21270;&#20102;&#36825;&#31181;&#25277;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent trends like the Internet of Things (IoT) suggest a vision of dense and multi-scale deployments of computing devices in nearly all kinds of environments. A prominent engineering challenge revolves around programming the collective adaptive behaviour of such computational ecosystems. This requires abstractions able to capture concepts like ensembles (dynamic groups of cooperating devices) and collective tasks (joint activities carried out by ensembles). In this work, we consider collections of devices interacting with neighbours and that execute in nearly-synchronised sense-compute-interact rounds, where the computation is given by a single program mapping sensing values and incoming messages to output and outcoming messages. To support programming whole computational collectives, we propose the abstraction of a distributed collective process, which can be used to define at once the ensemble formation logic and its collective task. We formalise the abstraction in the eXchange Calc
&lt;/p&gt;</description></item><item><title>MacroSwarm&#26159;&#19968;&#31181;&#22522;&#20110;&#22330;&#30340;&#32676;&#20307;&#32534;&#31243;&#26694;&#26550;&#65292;&#36890;&#36807;&#21487;&#32452;&#21512;&#30340;&#21151;&#33021;&#27169;&#22359;&#23454;&#29616;&#22797;&#26434;&#30340;&#32676;&#20307;&#34892;&#20026;&#65292;&#36890;&#36807;&#23558;&#24863;&#30693;&#22330;&#26144;&#23556;&#20026;&#25191;&#34892;&#30446;&#26631;&#22330;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#21270;&#30340;&#35774;&#35745;&#21644;&#23454;&#29616;&#32676;&#20307;&#34892;&#20026;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.10969</link><description>&lt;p&gt;
MacroSwarm: &#19968;&#31181;&#22522;&#20110;&#22330;&#30340;&#32452;&#21512;&#26694;&#26550;&#29992;&#20110;&#32676;&#20307;&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
MacroSwarm: A Field-based Compositional Framework for Swarm Programming. (arXiv:2401.10969v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10969
&lt;/p&gt;
&lt;p&gt;
MacroSwarm&#26159;&#19968;&#31181;&#22522;&#20110;&#22330;&#30340;&#32676;&#20307;&#32534;&#31243;&#26694;&#26550;&#65292;&#36890;&#36807;&#21487;&#32452;&#21512;&#30340;&#21151;&#33021;&#27169;&#22359;&#23454;&#29616;&#22797;&#26434;&#30340;&#32676;&#20307;&#34892;&#20026;&#65292;&#36890;&#36807;&#23558;&#24863;&#30693;&#22330;&#26144;&#23556;&#20026;&#25191;&#34892;&#30446;&#26631;&#22330;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#21270;&#30340;&#35774;&#35745;&#21644;&#23454;&#29616;&#32676;&#20307;&#34892;&#20026;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32676;&#20307;&#34892;&#20026;&#24037;&#31243;&#26159;&#19968;&#39033;&#26088;&#22312;&#30740;&#31350;&#21327;&#35843;&#31616;&#21333;&#26234;&#33021;&#20307;&#22242;&#20307;&#20869;&#35745;&#31639;&#21644;&#34892;&#21160;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#20197;&#23454;&#29616;&#22797;&#26434;&#30340;&#20840;&#23616;&#30446;&#26631;&#65292;&#22914;&#22270;&#26696;&#24418;&#25104;&#12289;&#38598;&#20307;&#31227;&#21160;&#12289;&#32858;&#31867;&#21644;&#20998;&#24067;&#24335;&#24863;&#30693;&#12290;&#23613;&#31649;&#22312;&#32676;&#20307;&#65288;&#26080;&#20154;&#26426;&#12289;&#26426;&#22120;&#20154;&#12289;&#36710;&#36742;&#65289;&#20998;&#26512;&#21644;&#24037;&#31243;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#20173;&#28982;&#38656;&#35201;&#36890;&#29992;&#30340;&#35774;&#35745;&#21644;&#23454;&#29616;&#26041;&#27861;&#21644;&#24037;&#20855;&#65292;&#20197;&#31995;&#32479;&#21270;&#30340;&#26041;&#24335;&#23450;&#20041;&#22797;&#26434;&#30340;&#32676;&#20307;&#34892;&#20026;&#12290;&#20026;&#20102;&#23545;&#27492;&#20570;&#20986;&#36129;&#29486;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22330;&#30340;&#21327;&#35843;&#26041;&#27861;&#65292;&#31216;&#20026;MacroSwarm&#65292;&#20197;&#21487;&#37325;&#29992;&#19988;&#23436;&#20840;&#21487;&#32452;&#21512;&#30340;&#21151;&#33021;&#27169;&#22359;&#20026;&#22522;&#30784;&#65292;&#23884;&#20837;&#38598;&#20307;&#35745;&#31639;&#21644;&#21327;&#35843;&#12290;&#22522;&#20110;&#38598;&#25104;&#35745;&#31639;&#30340;&#23439;&#32534;&#31243;&#33539;&#24335;&#65292;MacroSwarm&#25552;&#20986;&#20102;&#23558;&#27599;&#20010;&#32676;&#20307;&#34892;&#20026;&#22359;&#34920;&#31034;&#20026;&#23558;&#24863;&#30693;&#22330;&#26144;&#23556;&#20026;&#25191;&#34892;&#30446;&#26631;&#22330;&#30340;&#32431;&#20989;&#25968;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Swarm behaviour engineering is an area of research that seeks to investigate methods and techniques for coordinating computation and action within groups of simple agents to achieve complex global goals like pattern formation, collective movement, clustering, and distributed sensing. Despite recent progress in the analysis and engineering of swarms (of drones, robots, vehicles), there is still a need for general design and implementation methods and tools that can be used to define complex swarm behaviour in a principled way. To contribute to this quest, this article proposes a new field-based coordination approach, called MacroSwarm, to design and program swarm behaviour in terms of reusable and fully composable functional blocks embedding collective computation and coordination. Based on the macroprogramming paradigm of aggregate computing, MacroSwarm builds on the idea of expressing each swarm behaviour block as a pure function mapping sensing fields into actuation goal fields, e.g.
&lt;/p&gt;</description></item><item><title>DeepEdit&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#22909;&#30340;&#25512;&#29702;&#19968;&#33268;&#24615;&#21644;&#23545;&#26356;&#26032;&#30693;&#35782;&#30340;&#24847;&#35782;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#33021;&#21147;&#65292;&#23545;&#22810;&#36339;&#38382;&#39064;&#25968;&#25454;&#38598;MQuaKE&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.10471</link><description>&lt;p&gt;
DeepEdit: &#24102;&#26377;&#32422;&#26463;&#30340;&#35299;&#30721;&#24335;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
DeepEdit: Knowledge Editing as Decoding with Constraints. (arXiv:2401.10471v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10471
&lt;/p&gt;
&lt;p&gt;
DeepEdit&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#22909;&#30340;&#25512;&#29702;&#19968;&#33268;&#24615;&#21644;&#23545;&#26356;&#26032;&#30693;&#35782;&#30340;&#24847;&#35782;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#33021;&#21147;&#65292;&#23545;&#22810;&#36339;&#38382;&#39064;&#25968;&#25454;&#38598;MQuaKE&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30693;&#35782;&#32534;&#36753;&#35270;&#20026;&#24102;&#26377;&#32422;&#26463;&#30340;&#35299;&#30721;&#36807;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DeepEdit&#65288;&#22522;&#20110;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#30340;&#28176;&#36827;&#24335;&#35299;&#30721;&#30693;&#35782;&#32534;&#36753;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#22909;&#30340;&#25512;&#29702;&#19968;&#33268;&#24615;&#12289;&#38382;&#39064;&#30456;&#20851;&#24615;&#21644;&#23545;&#26356;&#26032;&#30693;&#35782;&#30340;&#24847;&#35782;&#26469;&#25913;&#36827;&#30693;&#35782;&#32534;&#36753;&#12290;DeepEdit&#21487;&#28789;&#27963;&#24212;&#29992;&#20110;&#25152;&#26377;&#40657;&#30418;LLMs&#65306;&#19981;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#12289;&#34920;&#31034;&#25110;&#36755;&#20986;&#35789;&#27719;&#20998;&#24067;&#12290;DeepEdit&#36880;&#27493;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#25512;&#29702;&#27493;&#39588;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#30693;&#35782;&#32534;&#36753;&#12290;&#23427;&#21033;&#29992;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#26469;&#20462;&#25913;LLMs&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#25552;&#39640;&#36755;&#20986;&#23545;&#38382;&#39064;&#30340;&#30456;&#20851;&#24615;&#21644;&#23545;&#26356;&#26032;&#30693;&#35782;&#30340;&#24847;&#35782;&#12290;&#22312;&#30693;&#35782;&#32534;&#36753;&#26041;&#38754;&#65292;DeepEdit&#22312;&#25511;&#21046;LLMs&#20135;&#29983;&#26356;&#31616;&#27905;&#30340;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#22312;MQuaKE&#19978;&#65292;DeepEdit&#22312;&#23450;&#37327;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#36339;&#38382;&#39064;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a new perspective of knowledge editing for large language models (LLMs) as decoding with constraints. We propose DeepEdit (Depth-first Search based Progressive Decoding for Knowledge Editing), a neuro-symbolic method that improves knowledge editing with better coherence of reasoning, relevance to the question, and awareness of updated knowledge. DeepEdit can be flexibly applied to all black-box LLMs: it does not require any access to the model parameters, representations, or output vocabulary distributions. DeepEdit progressively produces the high-quality reasoning steps towards effective knowledge editing. It utilizes a depth-first search to revise the LLMs' output, which improves the output's informativeness to the input question and awareness of the updated knowledge. Qualitatively, DeepEdit effectively controls LLMs to produce more succinct reasoning in accord with knowledge editing. Quantitatively, DeepEdit yields significant gains on MQuaKE, a challenging multi-hop que
&lt;/p&gt;</description></item><item><title>E3x&#26159;&#19968;&#31181;&#31616;&#21270;&#20102;$\mathrm{E}(3)$&#31561;&#21464;&#28145;&#24230;&#23398;&#20064;&#30340;&#36719;&#20214;&#21253;&#65292;&#36890;&#36807;&#20869;&#32622;&#31561;&#21464;&#24615;&#23454;&#29616;&#26356;&#39640;&#30340;&#25968;&#25454;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.07595</link><description>&lt;p&gt;
E3x&#65306;&#31616;&#21270;&#30340;$\mathrm{E}(3)$&#31561;&#21464;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
E3x: $\mathrm{E}(3)$-Equivariant Deep Learning Made Easy. (arXiv:2401.07595v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07595
&lt;/p&gt;
&lt;p&gt;
E3x&#26159;&#19968;&#31181;&#31616;&#21270;&#20102;$\mathrm{E}(3)$&#31561;&#21464;&#28145;&#24230;&#23398;&#20064;&#30340;&#36719;&#20214;&#21253;&#65292;&#36890;&#36807;&#20869;&#32622;&#31561;&#21464;&#24615;&#23454;&#29616;&#26356;&#39640;&#30340;&#25968;&#25454;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;E3x&#65292;&#19968;&#31181;&#29992;&#20110;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#30340;&#36719;&#20214;&#21253;&#65292;&#35813;&#32593;&#32476;&#22312;&#19977;&#32500;&#31354;&#38388;&#30340;&#24179;&#31227;&#12289;&#26059;&#36716;&#21644;&#21453;&#23556;&#26041;&#38754;&#31561;&#21464;&#12290;&#19982;&#26222;&#36890;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;$\mathrm{E}(3)$-&#31561;&#21464;&#27169;&#22411;&#22312;&#36755;&#20837;&#21644;/&#25110;&#36755;&#20986;&#25968;&#25454;&#26159;&#19982;&#19977;&#32500;&#23545;&#35937;&#30456;&#20851;&#30340;&#25968;&#37327;&#26102;&#20855;&#26377;&#20248;&#21183;&#12290;&#36825;&#26159;&#22240;&#20026;&#27492;&#31867;&#25968;&#37327;&#65288;&#20363;&#22914;&#20301;&#32622;&#65289;&#30340;&#25968;&#20540;&#36890;&#24120;&#21462;&#20915;&#20110;&#25152;&#36873;&#25321;&#30340;&#22352;&#26631;&#31995;&#32479;&#12290;&#22312;&#21442;&#32771;&#31995;&#30340;&#21464;&#25442;&#19979;&#65292;&#36825;&#20123;&#20540;&#20250;&#21487;&#39044;&#27979;&#22320;&#21457;&#29983;&#21464;&#21270;&#65292;&#20294;&#23545;&#20110;&#26222;&#36890;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#35828;&#65292;&#23398;&#20064;&#20854;&#28508;&#22312;&#35268;&#21017;&#21487;&#33021;&#24456;&#22256;&#38590;&#12290;&#20351;&#29992;&#20869;&#32622;&#30340;$\mathrm{E}(3)$-&#31561;&#21464;&#24615;&#65292;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20445;&#35777;&#23436;&#20840;&#28385;&#36275;&#30456;&#20851;&#30340;&#21464;&#25442;&#35268;&#21017;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#39640;&#30340;&#25968;&#25454;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;E3x&#30340;&#20195;&#30721;&#21487;&#20174;https://github.com/google-research/e3x&#33719;&#24471;&#65292;&#36824;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#25991;&#26723;&#21644;&#20351;&#29992;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces E3x, a software package for building neural networks that are equivariant with respect to the Euclidean group $\mathrm{E}(3)$, consisting of translations, rotations, and reflections of three-dimensional space. Compared to ordinary neural networks, $\mathrm{E}(3)$-equivariant models promise benefits whenever input and/or output data are quantities associated with three-dimensional objects. This is because the numeric values of such quantities (e.g. positions) typically depend on the chosen coordinate system. Under transformations of the reference frame, the values change predictably, but the underlying rules can be difficult to learn for ordinary machine learning models. With built-in $\mathrm{E}(3)$-equivariance, neural networks are guaranteed to satisfy the relevant transformation rules exactly, resulting in superior data efficiency and accuracy. The code for E3x is available from https://github.com/google-research/e3x, detailed documentation and usage examples ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#33616;&#20013;&#24847;&#22270;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#32858;&#31867;&#26041;&#27861;ELCRec&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#22797;&#26434;&#20248;&#21270;&#38382;&#39064;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#32858;&#31867;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.05975</link><description>&lt;p&gt;
&#29992;&#20110;&#25512;&#33616;&#20013;&#24847;&#22270;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
End-to-end Learnable Clustering for Intent Learning in Recommendation. (arXiv:2401.05975v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#33616;&#20013;&#24847;&#22270;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#32858;&#31867;&#26041;&#27861;ELCRec&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#22797;&#26434;&#20248;&#21270;&#38382;&#39064;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#32858;&#31867;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25366;&#25496;&#29992;&#25143;&#30340;&#24847;&#22270;&#22312;&#24207;&#21015;&#25512;&#33616;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;ICLRec&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#32858;&#31867;&#26469;&#25552;&#21462;&#29992;&#25143;&#30340;&#28508;&#22312;&#24847;&#22270;&#12290;&#23613;&#31649;&#23427;&#24050;&#32463;&#26174;&#31034;&#20986;&#26377;&#25928;&#24615;&#65292;&#20294;&#29616;&#26377;&#30340;&#26041;&#27861;&#23384;&#22312;&#22797;&#26434;&#21644;&#32321;&#29712;&#30340;&#20132;&#26367;&#20248;&#21270;&#38382;&#39064;&#65292;&#23548;&#33268;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#22312;&#24191;&#20041;&#26399;&#26395;&#26368;&#22823;&#21270;(EM)&#26694;&#26550;&#20013;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#21644;&#32858;&#31867;&#20248;&#21270;&#32463;&#24120;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#32858;&#31867;&#20250;&#24433;&#21709;&#22823;&#35268;&#27169;&#34892;&#19994;&#25968;&#25454;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24847;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;ELCRec&#65292;&#23427;&#23558;&#34920;&#31034;&#23398;&#20064;&#38598;&#25104;&#21040;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#32858;&#31867;&#26694;&#26550;&#20013;&#36827;&#34892;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mining users' intents plays a crucial role in sequential recommendation. The recent approach, ICLRec, was introduced to extract underlying users' intents using contrastive learning and clustering. While it has shown effectiveness, the existing method suffers from complex and cumbersome alternating optimization, leading to two main issues. Firstly, the separation of representation learning and clustering optimization within a generalized expectation maximization (EM) framework often results in sub-optimal performance. Secondly, performing clustering on the entire dataset hampers scalability for large-scale industry data. To address these challenges, we propose a novel intent learning method called \underline{ELCRec}, which integrates representation learning into an \underline{E}nd-to-end \underline{L}earnable \underline{C}lustering framework for \underline{Rec}ommendation. Specifically, we encode users' behavior sequences and initialize the cluster centers as learnable network parameter
&lt;/p&gt;</description></item><item><title>&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;&#26159;&#19968;&#31181;&#22312;&#22797;&#26434;&#25968;&#25454;&#19978;&#36827;&#34892;&#40065;&#26834;&#24615;&#35748;&#35777;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21482;&#22312;&#19968;&#20010;&#23545;&#35937;&#30340;&#23376;&#38598;&#19978;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#65292;&#20197;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#24335;&#25552;&#20379;&#20102;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#21644;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16221</link><description>&lt;p&gt;
&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Randomized Smoothing. (arXiv:2310.16221v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16221
&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;&#26159;&#19968;&#31181;&#22312;&#22797;&#26434;&#25968;&#25454;&#19978;&#36827;&#34892;&#40065;&#26834;&#24615;&#35748;&#35777;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21482;&#22312;&#19968;&#20010;&#23545;&#35937;&#30340;&#23376;&#38598;&#19978;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#65292;&#20197;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#24335;&#25552;&#20379;&#20102;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#21644;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#26159;&#22797;&#26434;&#30340;&#65292;&#36890;&#24120;&#30001;&#21487;&#20998;&#35299;&#20026;&#22810;&#20010;&#23454;&#20307;&#30340;&#23545;&#35937;&#32452;&#25104;&#65288;&#20363;&#22914;&#65292;&#23558;&#22270;&#20687;&#20998;&#35299;&#20026;&#20687;&#32032;&#65292;&#23558;&#22270;&#24418;&#20998;&#35299;&#20026;&#30456;&#20114;&#36830;&#25509;&#30340;&#33410;&#28857;&#65289;&#12290;&#38543;&#26426;&#24179;&#28369;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#22312;&#20854;&#36755;&#20837;&#30340;&#24494;&#23567;&#21464;&#21270;&#19978;&#20855;&#26377;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;-&#36890;&#36807;&#22312;&#20998;&#31867;&#20043;&#21069;&#38543;&#26426;&#28155;&#21152;&#22122;&#22768;&#26469;&#20445;&#35777;&#22810;&#25968;&#25237;&#31080;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#23545;&#25163;&#19981;&#26159;&#20219;&#24847;&#24178;&#25200;&#25972;&#20010;&#23545;&#35937;&#65288;&#20363;&#22914;&#22270;&#20687;&#65289;&#65292;&#32780;&#26159;&#23545;&#35937;&#30340;&#26576;&#20010;&#23454;&#20307;&#30340;&#23376;&#38598;&#65288;&#20363;&#22914;&#20687;&#32032;&#65289;&#26102;&#65292;&#36890;&#36807;&#38543;&#26426;&#24179;&#28369;&#23545;&#36825;&#31181;&#22797;&#26434;&#25968;&#25454;&#36827;&#34892;&#40065;&#26834;&#24615;&#35748;&#35777;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;&#65306;&#25105;&#20204;&#36890;&#36807;&#20165;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#23454;&#20307;&#23376;&#38598;&#19978;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#26469;&#37096;&#20998;&#24179;&#28369;&#23545;&#35937;&#12290;&#36890;&#36807;&#20197;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#24335;&#28155;&#21152;&#22122;&#22768;&#65292;&#25105;&#20204;&#33719;&#24471;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#22122;&#22768;&#20998;&#24067;&#21021;&#22987;&#21270;&#20998;&#23618;&#24179;&#28369;&#65292;&#24471;&#21040;&#20102;&#26032;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world data is complex and often consists of objects that can be decomposed into multiple entities (e.g. images into pixels, graphs into interconnected nodes). Randomized smoothing is a powerful framework for making models provably robust against small changes to their inputs - by guaranteeing robustness of the majority vote when randomly adding noise before classification. Yet, certifying robustness on such complex data via randomized smoothing is challenging when adversaries do not arbitrarily perturb entire objects (e.g. images) but only a subset of their entities (e.g. pixels). As a solution, we introduce hierarchical randomized smoothing: We partially smooth objects by adding random noise only on a randomly selected subset of their entities. By adding noise in a more targeted manner than existing methods we obtain stronger robustness guarantees while maintaining high accuracy. We initialize hierarchical smoothing using different noising distributions, yielding novel robustness
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#20915;&#31574;&#31995;&#32479;&#20248;&#21270;&#20013;&#26799;&#24230;&#21453;&#39304;&#31232;&#32570;&#25110;&#26080;&#25928;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#32039;&#20945;&#30340;&#22810;&#23618;&#26550;&#26500;&#21644;&#35282;&#33394;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#23545;&#21442;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#20316;&#32773;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#20915;&#31574;&#31995;&#32479;&#30340;&#39640;&#25928;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.00629</link><description>&lt;p&gt;
Hessian-Aware Bayesian Optimization for Decision Making Systems - &#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#20915;&#31574;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hessian-Aware Bayesian Optimization for Decision Making Systems. (arXiv:2308.00629v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#20915;&#31574;&#31995;&#32479;&#20248;&#21270;&#20013;&#26799;&#24230;&#21453;&#39304;&#31232;&#32570;&#25110;&#26080;&#25928;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#32039;&#20945;&#30340;&#22810;&#23618;&#26550;&#26500;&#21644;&#35282;&#33394;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#23545;&#21442;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#20316;&#32773;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#20915;&#31574;&#31995;&#32479;&#30340;&#39640;&#25928;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20248;&#21270;&#20915;&#31574;&#31995;&#32479;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#26799;&#24230;&#26041;&#27861;&#65292;&#38656;&#35201;&#20174;&#29615;&#22659;&#20013;&#33719;&#21462;&#26377;&#20449;&#24687;&#37327;&#30340;&#21453;&#39304;&#12290;&#28982;&#32780;&#65292;&#24403;&#21453;&#39304;&#31232;&#32570;&#25110;&#32773;&#26080;&#20449;&#24687;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#36739;&#24046;&#12290;&#36125;&#21494;&#26031;&#20248;&#21270;&#31561;&#26080;&#23548;&#25968;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#23545;&#26799;&#24230;&#21453;&#39304;&#36136;&#37327;&#30340;&#20381;&#36182;&#65292;&#20294;&#22312;&#22797;&#26434;&#20915;&#31574;&#31995;&#32479;&#30340;&#39640;&#32500;&#29615;&#22659;&#20013;&#24448;&#24448;&#38590;&#20197;&#25193;&#23637;&#12290;&#22914;&#26524;&#31995;&#32479;&#38656;&#35201;&#22810;&#20010;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#20114;&#21160;&#26469;&#23454;&#29616;&#20849;&#21516;&#30446;&#26631;&#65292;&#36825;&#20010;&#38382;&#39064;&#23601;&#21152;&#21095;&#20102;&#12290;&#20026;&#20102;&#35299;&#20915;&#32500;&#24230;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#20945;&#30340;&#22810;&#23618;&#26550;&#26500;&#65292;&#36890;&#36807;&#35282;&#33394;&#30340;&#27010;&#24565;&#26469;&#24314;&#27169;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#21160;&#24577;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#26469;&#39640;&#25928;&#22320;&#20248;&#21270;&#30001;&#22823;&#37327;&#21442;&#25968;&#21442;&#25968;&#21270;&#30340;&#22810;&#23618;&#26550;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;(HA-GP-UCB)&#22312;&#25928;&#26524;&#19978;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many approaches for optimizing decision making systems rely on gradient based methods requiring informative feedback from the environment. However, in the case where such feedback is sparse or uninformative, such approaches may result in poor performance. Derivative-free approaches such as Bayesian Optimization mitigate the dependency on the quality of gradient feedback, but are known to scale poorly in the high-dimension setting of complex decision making systems. This problem is exacerbated if the system requires interactions between several actors cooperating to accomplish a shared goal. To address the dimensionality challenge, we propose a compact multi-layered architecture modeling the dynamics of actor interactions through the concept of role. Additionally, we introduce Hessian-aware Bayesian Optimization to efficiently optimize the multi-layered architecture parameterized by a large number of parameters. Experimental results demonstrate that our method (HA-GP-UCB) works effectiv
&lt;/p&gt;</description></item><item><title>RoCar&#26159;&#19968;&#31181;&#21033;&#29992;&#20851;&#31995;&#32593;&#32476;&#26500;&#24314;&#20219;&#21153;&#22270;&#24182;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#35780;&#20272;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21644;&#35760;&#24518;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26497;&#22823;&#30340;&#38543;&#26426;&#24615;&#30830;&#20445;&#20102;&#35780;&#20272;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15997</link><description>&lt;p&gt;
RoCar:&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;&#32593;&#32476;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RoCar: A Relationship Network-based Evaluation Method to Large Language Models. (arXiv:2307.15997v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15997
&lt;/p&gt;
&lt;p&gt;
RoCar&#26159;&#19968;&#31181;&#21033;&#29992;&#20851;&#31995;&#32593;&#32476;&#26500;&#24314;&#20219;&#21153;&#22270;&#24182;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#35780;&#20272;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21644;&#35760;&#24518;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26497;&#22823;&#30340;&#38543;&#26426;&#24615;&#30830;&#20445;&#20102;&#35780;&#20272;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#33021;&#21147;&#30340;&#22797;&#26434;&#24615;&#65292;&#22914;&#20309;&#21512;&#29702;&#35780;&#20272;LLMs&#30340;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#38656;&#35201;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;RoCar&#26041;&#27861;&#65292;&#21033;&#29992;&#23450;&#20041;&#30340;&#22522;&#26412;&#27169;&#24335;&#38543;&#26426;&#26500;&#24314;&#19968;&#20010;&#20219;&#21153;&#22270;&#65292;&#24182;&#22522;&#20110;&#20219;&#21153;&#22270;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#35780;&#20272;&#20219;&#21153;&#65292;&#20998;&#21035;&#35780;&#20272;LLMs&#30340;&#25512;&#29702;&#21644;&#35760;&#24518;&#33021;&#21147;&#12290;&#30001;&#20110;&#20219;&#21153;&#26500;&#24314;&#36807;&#31243;&#30340;&#26497;&#22823;&#38543;&#26426;&#24615;&#65292;&#21487;&#20197;&#30830;&#20445;&#34987;&#27979;&#35797;&#30340;LLMs&#20013;&#27809;&#26377;&#19968;&#20010;&#30452;&#25509;&#23398;&#20064;&#20102;&#35780;&#20272;&#20219;&#21153;&#65292;&#20174;&#32780;&#20445;&#35777;&#20102;&#35780;&#20272;&#26041;&#27861;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have received increasing attention. However, due to the complexity of its capabilities, how to rationally evaluate the capabilities of LLMs is still a task to be solved. We propose the RoCar method, which utilizes the defined basic schemas to randomly construct a task graph and generates natural language evaluation tasks based on the task graph to evaluate the reasoning and memory abilities of LLMs respectively. Due to the very large randomness of the task construction process, it is possible to ensure that none of the LLMs to be tested has directly learned the evaluation tasks, guaranteeing the fairness of the evaluation method.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#37325;&#22797;&#30340;&#22810;&#21333;&#20301;&#20184;&#36153;&#25293;&#21334;&#20013;&#23398;&#20064;&#22914;&#20309;&#20986;&#20215;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#31163;&#32447;&#35774;&#32622;&#20013;&#20248;&#21270;&#20986;&#20215;&#21521;&#37327;&#65292;&#24182;&#21033;&#29992;&#22810;&#39033;&#24335;&#26102;&#38388;&#21160;&#24577;&#35268;&#21010;&#26041;&#26696;&#65292;&#35774;&#35745;&#20102;&#20855;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.15193</link><description>&lt;p&gt;
&#22312;&#37325;&#22797;&#30340;&#22810;&#21333;&#20301;&#20184;&#36153;&#25293;&#21334;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning in Repeated Multi-Unit Pay-As-Bid Auctions. (arXiv:2307.15193v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#37325;&#22797;&#30340;&#22810;&#21333;&#20301;&#20184;&#36153;&#25293;&#21334;&#20013;&#23398;&#20064;&#22914;&#20309;&#20986;&#20215;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#31163;&#32447;&#35774;&#32622;&#20013;&#20248;&#21270;&#20986;&#20215;&#21521;&#37327;&#65292;&#24182;&#21033;&#29992;&#22810;&#39033;&#24335;&#26102;&#38388;&#21160;&#24577;&#35268;&#21010;&#26041;&#26696;&#65292;&#35774;&#35745;&#20102;&#20855;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#30899;&#25490;&#25918;&#20132;&#26131;&#26041;&#26696;&#12289;&#22269;&#20538;&#25293;&#21334;&#21644;&#37319;&#36141;&#25293;&#21334;&#30340;&#21551;&#21457;&#65292;&#36825;&#20123;&#37117;&#28041;&#21450;&#25293;&#21334;&#21516;&#36136;&#30340;&#22810;&#20010;&#21333;&#20301;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22914;&#20309;&#22312;&#37325;&#22797;&#30340;&#22810;&#21333;&#20301;&#20184;&#36153;&#25293;&#21334;&#20013;&#23398;&#20064;&#22914;&#20309;&#20986;&#20215;&#30340;&#38382;&#39064;&#12290;&#22312;&#27599;&#20010;&#25293;&#21334;&#20013;&#65292;&#22823;&#37327;&#65288;&#30456;&#21516;&#30340;&#65289;&#29289;&#21697;&#23558;&#34987;&#20998;&#37197;&#32473;&#26368;&#39640;&#30340;&#20986;&#20215;&#65292;&#27599;&#20010;&#20013;&#26631;&#20215;&#31561;&#20110;&#20986;&#20215;&#26412;&#36523;&#12290;&#30001;&#20110;&#34892;&#21160;&#31354;&#38388;&#30340;&#32452;&#21512;&#24615;&#36136;&#65292;&#23398;&#20064;&#22914;&#20309;&#22312;&#20184;&#36153;&#25293;&#21334;&#20013;&#20986;&#20215;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#20851;&#27880;&#31163;&#32447;&#35774;&#32622;&#65292;&#20854;&#20013;&#25237;&#26631;&#20154;&#36890;&#36807;&#21482;&#33021;&#35775;&#38382;&#20854;&#20182;&#25237;&#26631;&#20154;&#36807;&#21435;&#25552;&#20132;&#30340;&#20986;&#20215;&#26469;&#20248;&#21270;&#20182;&#20204;&#30340;&#20986;&#20215;&#21521;&#37327;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31163;&#32447;&#38382;&#39064;&#30340;&#26368;&#20248;&#35299;&#21487;&#20197;&#20351;&#29992;&#22810;&#39033;&#24335;&#26102;&#38388;&#21160;&#24577;&#35268;&#21010;&#65288;DP&#65289;&#26041;&#26696;&#26469;&#33719;&#24471;&#12290;&#25105;&#20204;&#21033;&#29992;DP&#26041;&#26696;&#30340;&#32467;&#26500;&#65292;&#35774;&#35745;&#20102;&#20855;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by Carbon Emissions Trading Schemes, Treasury Auctions, and Procurement Auctions, which all involve the auctioning of homogeneous multiple units, we consider the problem of learning how to bid in repeated multi-unit pay-as-bid auctions. In each of these auctions, a large number of (identical) items are to be allocated to the largest submitted bids, where the price of each of the winning bids is equal to the bid itself. The problem of learning how to bid in pay-as-bid auctions is challenging due to the combinatorial nature of the action space. We overcome this challenge by focusing on the offline setting, where the bidder optimizes their vector of bids while only having access to the past submitted bids by other bidders. We show that the optimal solution to the offline problem can be obtained using a polynomial time dynamic programming (DP) scheme. We leverage the structure of the DP scheme to design online learning algorithms with polynomial time and space complexity under fu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23450;&#21521;&#36793;&#30028;&#26694;&#21644;&#28145;&#24230;&#32593;&#32476;&#36827;&#34892;&#29289;&#20307;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#29615;&#22659;&#26465;&#20214;&#19979;&#23545;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#21457;&#29616;&#65292;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#23567;&#29289;&#20307;&#26102;&#33021;&#22815;&#25552;&#39640;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16798</link><description>&lt;p&gt;
&#35780;&#20272;&#29615;&#22659;&#26465;&#20214;&#23545;&#20351;&#29992;&#23450;&#21521;&#36793;&#30028;&#26694;&#36827;&#34892;AR&#24212;&#29992;&#30340;&#29289;&#20307;&#26816;&#27979;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Environmental Conditions on Object Detection using Oriented Bounding Boxes for AR Applications. (arXiv:2306.16798v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23450;&#21521;&#36793;&#30028;&#26694;&#21644;&#28145;&#24230;&#32593;&#32476;&#36827;&#34892;&#29289;&#20307;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#29615;&#22659;&#26465;&#20214;&#19979;&#23545;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#21457;&#29616;&#65292;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#23567;&#29289;&#20307;&#26102;&#33021;&#22815;&#25552;&#39640;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#24378;&#29616;&#23454;&#65288;AR&#65289;&#30340;&#30446;&#26631;&#26159;&#23558;&#25968;&#23383;&#20869;&#23481;&#28155;&#21152;&#21040;&#33258;&#28982;&#22270;&#20687;&#21644;&#35270;&#39057;&#20013;&#65292;&#20197;&#21019;&#24314;&#29992;&#25143;&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#20132;&#20114;&#20307;&#39564;&#12290;&#22330;&#26223;&#20998;&#26512;&#21644;&#29289;&#20307;&#35782;&#21035;&#22312;AR&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#24517;&#39035;&#24555;&#36895;&#19988;&#20934;&#30830;&#22320;&#25191;&#34892;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#23450;&#21521;&#36793;&#30028;&#26694;&#19982;&#26816;&#27979;&#21644;&#35782;&#21035;&#28145;&#24230;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#22788;&#29702;&#26102;&#38388;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20004;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65306;&#19968;&#20010;&#24120;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#30340;&#30495;&#23454;&#22270;&#20687;&#25968;&#25454;&#38598;&#65288;DOTA&#25968;&#25454;&#38598;&#65289;&#21644;&#19968;&#20010;&#27169;&#25311;&#19981;&#21516;&#29615;&#22659;&#12289;&#29031;&#26126;&#21644;&#37319;&#38598;&#26465;&#20214;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#35780;&#20272;&#30340;&#37325;&#28857;&#26159;&#23567;&#29289;&#20307;&#65292;&#36825;&#20123;&#29289;&#20307;&#24448;&#24448;&#38590;&#20197;&#26816;&#27979;&#21644;&#35782;&#21035;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22823;&#22810;&#25968;&#27979;&#35797;&#26465;&#20214;&#19979;&#65292;&#23545;&#20110;&#23567;&#29289;&#20307;&#24448;&#24448;&#33021;&#20135;&#29983;&#26356;&#22909;&#30340;&#24179;&#22343;&#31934;&#24230;&#21644;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of augmented reality (AR) is to add digital content to natural images and videos to create an interactive experience between the user and the environment. Scene analysis and object recognition play a crucial role in AR, as they must be performed quickly and accurately. In this study, a new approach is proposed that involves using oriented bounding boxes with a detection and recognition deep network to improve performance and processing time. The approach is evaluated using two datasets: a real image dataset (DOTA dataset) commonly used for computer vision tasks, and a synthetic dataset that simulates different environmental, lighting, and acquisition conditions. The focus of the evaluation is on small objects, which are difficult to detect and recognise. The results indicate that the proposed approach tends to produce better Average Precision and greater accuracy for small objects in most of the tested conditions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22635;&#20805;&#24335;&#36974;&#30422;&#31574;&#30053;MixMask&#65292;&#22312;Siamese ConvNets&#20013;&#23454;&#29616;&#36974;&#30422;&#21644;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#30340;&#21305;&#37197;&#65292;&#25552;&#39640;&#20102;Siamese ConvNets&#30340;&#24615;&#33021;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.11456</link><description>&lt;p&gt;
MixMask: &#37325;&#26032;&#23457;&#35270;Siamese ConvNets&#30340;&#36974;&#30422;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
MixMask: Revisiting Masking Strategy for Siamese ConvNets. (arXiv:2210.11456v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22635;&#20805;&#24335;&#36974;&#30422;&#31574;&#30053;MixMask&#65292;&#22312;Siamese ConvNets&#20013;&#23454;&#29616;&#36974;&#30422;&#21644;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#30340;&#21305;&#37197;&#65292;&#25552;&#39640;&#20102;Siamese ConvNets&#30340;&#24615;&#33021;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#36827;&#23637;&#23558;Masked Image Modeling&#65288;MIM&#65289;&#21644;Siamese&#32593;&#32476;&#25972;&#21512;&#25104;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#20004;&#31181;&#25216;&#26415;&#30340;&#20248;&#28857;&#12290;&#28982;&#32780;&#65292;&#22312;Siamese ConvNets&#20013;&#24212;&#29992;&#20256;&#32479;&#30340;&#22522;&#20110;&#25830;&#38500;&#30340;&#36974;&#30422;&#31574;&#30053;&#26102;&#65292;&#23384;&#22312;&#19968;&#20123;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#65288;I&#65289;&#22312;&#36830;&#32493;&#22788;&#29702;&#25968;&#25454;&#26102;&#19981;&#33021;&#25918;&#24323;&#19981;&#30456;&#20851;&#30340;&#36974;&#30422;&#21306;&#22495;&#65292;&#23548;&#33268;&#35757;&#32451;&#25928;&#29575;&#20302;&#20110;ViT&#27169;&#22411;;&#65288;II&#65289;&#22522;&#20110;&#25830;&#38500;&#30340;&#36974;&#30422;&#19982;Siamese ConvNets&#20013;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#19981;&#21305;&#37197;&#65292;&#19982;MIM&#26041;&#27861;&#19981;&#21516;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MixMask&#30340;&#22635;&#20805;&#24335;&#36974;&#30422;&#31574;&#30053;&#65292;&#20197;&#38450;&#27490;&#39321;&#33609;&#36974;&#30422;&#26041;&#27861;&#20013;&#22270;&#20687;&#20013;&#30340;&#38543;&#26426;&#36974;&#30422;&#21306;&#22495;&#23548;&#33268;&#20449;&#24687;&#19981;&#23436;&#25972;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#25439;&#22833;&#20989;&#25968;&#35774;&#35745;&#65292;&#32771;&#34385;&#20004;&#20010;&#19981;&#21516;&#28151;&#21512;&#35270;&#22270;&#20043;&#38388;&#30340;&#35821;&#20041;&#36317;&#31163;&#21464;&#21270;&#65292;&#20197;&#36866;&#24212;&#38598;&#25104;&#26550;&#26500;&#24182;&#38450;&#27490;&#36974;&#30422;&#21644;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;MixMask&#26174;&#30528;&#25552;&#39640;&#20102;Siamese ConvNets&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#20960;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in self-supervised learning have integrated Masked Image Modeling (MIM) and Siamese Networks into a unified framework that leverages the benefits of both techniques. However, several issues remain unaddressed when applying conventional erase-based masking with Siamese ConvNets. These include (I) the inability to drop uninformative masked regions in ConvNets as they process data continuously, resulting in low training efficiency compared to ViT models; and (II) the mismatch between erase-based masking and the contrastive-based objective in Siamese ConvNets, which differs from the MIM approach. In this paper, we propose a filling-based masking strategy called MixMask to prevent information incompleteness caused by the randomly erased regions in an image in the vanilla masking method. Furthermore, we introduce a flexible loss function design that considers the semantic distance change between two different mixed views to adapt the integrated architecture and prevent mismat
&lt;/p&gt;</description></item></channel></rss>