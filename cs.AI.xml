<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;HTTP&#21709;&#24212;&#22836;&#35774;&#35745;&#20102;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#22312;&#36328;&#27983;&#35272;&#22120;&#29615;&#22659;&#19979;&#26377;&#25928;&#26816;&#27979;Web&#36861;&#36394;&#22120;&#65292;&#32467;&#26524;&#22312;Chrome&#21644;Firefox&#19978;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01240</link><description>&lt;p&gt;
&#36229;&#36234;&#35831;&#27714;&#65306;&#21033;&#29992;HTTP&#21709;&#24212;&#22836;&#22312;&#19981;&#24179;&#34913;&#29615;&#22659;&#20013;&#36827;&#34892;&#36328;&#27983;&#35272;&#22120;Web&#36861;&#36394;&#22120;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Beyond the Request: Harnessing HTTP Response Headers for Cross-Browser Web Tracker Classification in an Imbalanced Setting
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;HTTP&#21709;&#24212;&#22836;&#35774;&#35745;&#20102;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#22312;&#36328;&#27983;&#35272;&#22120;&#29615;&#22659;&#19979;&#26377;&#25928;&#26816;&#27979;Web&#36861;&#36394;&#22120;&#65292;&#32467;&#26524;&#22312;Chrome&#21644;Firefox&#19978;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19975;&#32500;&#32593;&#30340;&#36830;&#36890;&#24615;&#20027;&#35201;&#24402;&#22240;&#20110;HTTP&#21327;&#35758;&#65292;&#20854;&#20013;&#30340;HTTP&#28040;&#24687;&#25552;&#20379;&#20102;&#26377;&#20851;&#32593;&#32476;&#23433;&#20840;&#21644;&#38544;&#31169;&#30340;&#20449;&#24687;&#22836;&#23383;&#27573;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;Web&#36861;&#36394;&#12290;&#23613;&#31649;&#24050;&#26377;&#30740;&#31350;&#21033;&#29992;HTTP/S&#35831;&#27714;&#28040;&#24687;&#26469;&#35782;&#21035;Web&#36861;&#36394;&#22120;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;HTTP/S&#21709;&#24212;&#22836;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35774;&#35745;&#20351;&#29992;HTTP/S&#21709;&#24212;&#22836;&#36827;&#34892;Web&#36861;&#36394;&#22120;&#26816;&#27979;&#30340;&#26377;&#25928;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#12290;&#36890;&#36807;&#27983;&#35272;&#22120;&#25193;&#23637;&#31243;&#24207;T.EX&#33719;&#21462;&#30340;Chrome&#12289;Firefox&#21644;Brave&#27983;&#35272;&#22120;&#30340;&#25968;&#25454;&#20316;&#20026;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12290;&#22312;Chrome&#25968;&#25454;&#19978;&#35757;&#32451;&#20102;11&#20010;&#30417;&#30563;&#27169;&#22411;&#65292;&#24182;&#22312;&#25152;&#26377;&#27983;&#35272;&#22120;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;Chrome&#21644;Firefox&#19978;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#12289;F1&#20998;&#25968;&#12289;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;&#26368;&#23567;&#23545;&#25968;&#25439;&#22833;&#35823;&#24046;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;Brave&#27983;&#35272;&#22120;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#20854;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#29305;&#24449;&#38598;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#20998;&#31867;&#22120;&#21487;&#20197;&#29992;&#20110;&#26816;&#27979;Web&#36861;&#36394;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The World Wide Web's connectivity is greatly attributed to the HTTP protocol, with HTTP messages offering informative header fields that appeal to disciplines like web security and privacy, especially concerning web tracking. Despite existing research employing HTTP/S request messages to identify web trackers, HTTP/S response headers are often overlooked. This study endeavors to design effective machine learning classifiers for web tracker detection using HTTP/S response headers. Data from the Chrome, Firefox, and Brave browsers, obtained through the traffic monitoring browser extension T.EX, serves as our data set. Eleven supervised models were trained on Chrome data and tested across all browsers. The results demonstrated high accuracy, F1-score, precision, recall, and minimal log-loss error for Chrome and Firefox, but subpar performance on Brave, potentially due to its distinct data distribution and feature set. The research suggests that these classifiers are viable for detecting w
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#22411;&#24179;&#21488;&#65292;&#26088;&#22312;&#20351;&#22823;&#22411;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#26356;&#26131;&#20110;&#20351;&#29992;&#65292;&#36890;&#36807;&#26368;&#26032;&#30340;&#22810;LoRA&#25512;&#29702;&#25216;&#26415;&#21644;&#23450;&#21046;&#36866;&#37197;&#22120;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#38548;&#31163;&#12289;&#21152;&#23494;&#21644;&#36523;&#20221;&#39564;&#35777;&#30340;&#23433;&#20840;&#26381;&#21153;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00913</link><description>&lt;p&gt;
&#29992;&#20110;&#23433;&#20840;&#33258;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#30340;&#26426;&#26500;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Institutional Platform for Secure Self-Service Large Language Model Exploration
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00913
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#22411;&#24179;&#21488;&#65292;&#26088;&#22312;&#20351;&#22823;&#22411;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#26356;&#26131;&#20110;&#20351;&#29992;&#65292;&#36890;&#36807;&#26368;&#26032;&#30340;&#22810;LoRA&#25512;&#29702;&#25216;&#26415;&#21644;&#23450;&#21046;&#36866;&#37197;&#22120;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#38548;&#31163;&#12289;&#21152;&#23494;&#21644;&#36523;&#20221;&#39564;&#35777;&#30340;&#23433;&#20840;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#30001;&#32943;&#22612;&#22522;&#22823;&#23398;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#20013;&#24515;&#24320;&#21457;&#30340;&#29992;&#25143;&#21451;&#22909;&#22411;&#24179;&#21488;&#65292;&#26088;&#22312;&#20351;&#22823;&#22411;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26356;&#26131;&#20110;&#20351;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#26368;&#36817;&#22312;&#22810;LoRA&#25512;&#29702;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#31995;&#32479;&#26377;&#25928;&#22320;&#36866;&#24212;&#20102;&#21508;&#31867;&#29992;&#25143;&#21644;&#39033;&#30446;&#30340;&#23450;&#21046;&#36866;&#37197;&#22120;&#12290;&#35770;&#25991;&#27010;&#36848;&#20102;&#31995;&#32479;&#30340;&#26550;&#26500;&#21644;&#20851;&#38190;&#29305;&#24615;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#31574;&#21010;&#12289;&#27169;&#22411;&#35757;&#32451;&#12289;&#23433;&#20840;&#25512;&#29702;&#21644;&#22522;&#20110;&#25991;&#26412;&#30340;&#29305;&#24449;&#25552;&#21462;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#20195;&#29702;&#30340;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20110;&#31199;&#25143;&#24847;&#35782;&#30340;&#35745;&#31639;&#32593;&#32476;&#65292;&#22312;&#23433;&#20840;&#22320;&#21033;&#29992;&#23396;&#31435;&#36164;&#28304;&#23707;&#30340;&#22522;&#30784;&#19978;&#24418;&#25104;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#31995;&#32479;&#12290;&#35813;&#24179;&#21488;&#33268;&#21147;&#20110;&#25552;&#20379;&#23433;&#20840;&#30340;LLM&#26381;&#21153;&#65292;&#24378;&#35843;&#36807;&#31243;&#21644;&#25968;&#25454;&#38548;&#31163;&#12289;&#31471;&#21040;&#31471;&#21152;&#23494;&#20197;&#21450;&#22522;&#20110;&#35282;&#33394;&#30340;&#36164;&#28304;&#36523;&#20221;&#39564;&#35777;&#12290;&#35813;&#36129;&#29486;&#19982;&#23454;&#29616;&#31616;&#21270;&#35775;&#38382;&#20808;&#36827;&#30340;AI&#27169;&#22411;&#21644;&#25216;&#26415;&#20197;&#25903;&#25345;&#31185;&#23398;&#21457;&#29616;&#30340;&#24635;&#20307;&#30446;&#26631;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a user-friendly platform developed by the University of Kentucky Center for Applied AI, designed to make large, customized language models (LLMs) more accessible. By capitalizing on recent advancements in multi-LoRA inference, the system efficiently accommodates custom adapters for a diverse range of users and projects. The paper outlines the system's architecture and key features, encompassing dataset curation, model training, secure inference, and text-based feature extraction.   We illustrate the establishment of a tenant-aware computational network using agent-based methods, securely utilizing islands of isolated resources as a unified system. The platform strives to deliver secure LLM services, emphasizing process and data isolation, end-to-end encryption, and role-based resource authentication. This contribution aligns with the overarching goal of enabling simplified access to cutting-edge AI models and technology in support of scientific discovery.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22534;&#21472;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#32858;&#31867;&#23454;&#29616;&#36965;&#24863;&#25968;&#25454;&#22320;&#36136;&#21046;&#22270;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;</title><link>https://arxiv.org/abs/2404.02180</link><description>&lt;p&gt;
&#36890;&#36807;&#22534;&#21472;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#32858;&#31867;&#23454;&#29616;&#22320;&#36136;&#21046;&#22270;&#30340;&#36965;&#24863;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Remote sensing framework for geological mapping via stacked autoencoders and clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02180
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22534;&#21472;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#32858;&#31867;&#23454;&#29616;&#36965;&#24863;&#25968;&#25454;&#22320;&#36136;&#21046;&#22270;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#36965;&#24863;&#22320;&#36136;&#21046;&#22270;&#20013;&#38754;&#20020;&#30528;&#30001;&#20110;&#20934;&#30830;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#32780;&#38480;&#21046;&#30340;&#38382;&#39064;&#12290;&#30456;&#21453;&#65292;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#38477;&#32500;&#21644;&#32858;&#31867;&#65292;&#33021;&#22815;&#22312;&#19981;&#20381;&#36182;&#39044;&#23450;&#20041;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#25581;&#31034;&#36965;&#24863;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#21644;&#32467;&#26500;&#12290;&#38477;&#32500;&#26041;&#27861;&#20855;&#26377;&#22312;&#25552;&#39640;&#22320;&#36136;&#22270;&#20934;&#30830;&#24615;&#26041;&#38754;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#30340;&#28508;&#21147;&#12290;&#34429;&#28982;&#20256;&#32479;&#30340;&#38477;&#32500;&#26041;&#27861;&#21487;&#33021;&#22312;&#38750;&#32447;&#24615;&#25968;&#25454;&#19978;&#36935;&#21040;&#22256;&#38590;&#65292;&#20294;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22914;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#33021;&#22815;&#27169;&#25311;&#25968;&#25454;&#20013;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#22534;&#21472;&#33258;&#21160;&#32534;&#30721;&#22120;&#20855;&#26377;&#22810;&#20010;&#30456;&#20114;&#36830;&#25509;&#30340;&#23618;&#65292;&#29992;&#20110;&#25429;&#33719;&#23545;&#36965;&#24863;&#25968;&#25454;&#26377;&#29992;&#30340;&#20998;&#23618;&#25968;&#25454;&#34920;&#31034;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#22534;&#21472;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#32858;&#31867;&#22788;&#29702;&#36965;&#24863;&#25968;&#25454;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02180v1 Announce Type: cross  Abstract: Supervised learning methods for geological mapping via remote sensing face limitations due to the scarcity of accurately labelled training data. In contrast, unsupervised learning methods, such as dimensionality reduction and clustering have the ability to uncover patterns and structures in remote sensing data without relying on predefined labels. Dimensionality reduction methods have the potential to play a crucial role in improving the accuracy of geological maps. Although conventional dimensionality reduction methods may struggle with nonlinear data, unsupervised deep learning models such as autoencoders have the ability to model nonlinear relationship in data. Stacked autoencoders feature multiple interconnected layers to capture hierarchical data representations that can be useful for remote sensing data. In this study, we present an unsupervised machine learning framework for processing remote sensing data by utilizing stacked au
&lt;/p&gt;</description></item><item><title>WavLLM&#26159;&#19968;&#20010;&#31283;&#20581;&#21644;&#33258;&#36866;&#24212;&#35821;&#38899;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#21452;&#32534;&#30721;&#22120;&#21644;Prompt-aware LoRA&#26435;&#37325;&#36866;&#37197;&#22120;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#20248;&#21270;&#65292;&#35299;&#32806;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#38899;&#20449;&#24687;&#65292;&#20026;&#22788;&#29702;&#35821;&#20041;&#20869;&#23481;&#21644;&#35828;&#35805;&#32773;&#36523;&#20221;&#30340;&#29420;&#29305;&#29305;&#24449;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;</title><link>https://arxiv.org/abs/2404.00656</link><description>&lt;p&gt;
WavLLM&#65306;&#38754;&#21521;&#31283;&#20581;&#21644;&#33258;&#36866;&#24212;&#35821;&#38899;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
WavLLM: Towards Robust and Adaptive Speech Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00656
&lt;/p&gt;
&lt;p&gt;
WavLLM&#26159;&#19968;&#20010;&#31283;&#20581;&#21644;&#33258;&#36866;&#24212;&#35821;&#38899;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#21452;&#32534;&#30721;&#22120;&#21644;Prompt-aware LoRA&#26435;&#37325;&#36866;&#37197;&#22120;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#20248;&#21270;&#65292;&#35299;&#32806;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#38899;&#20449;&#24687;&#65292;&#20026;&#22788;&#29702;&#35821;&#20041;&#20869;&#23481;&#21644;&#35828;&#35805;&#32773;&#36523;&#20221;&#30340;&#29420;&#29305;&#29305;&#24449;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#26368;&#26032;&#36827;&#23637;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#36880;&#28176;&#25299;&#23485;&#20102;&#23427;&#20204;&#30340;&#33539;&#22260;&#21040;&#22810;&#27169;&#24577;&#24863;&#30693;&#21644;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#22320;&#23558;&#21548;&#35273;&#33021;&#21147;&#25972;&#21512;&#21040;LLMs&#20013;&#20250;&#24102;&#26469;&#26174;&#33879;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#27867;&#21270;&#36328;&#19981;&#21516;&#35821;&#22659;&#21644;&#25191;&#34892;&#22797;&#26434;&#21548;&#35273;&#20219;&#21153;&#26041;&#38754;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;WavLLM&#65292;&#19968;&#20010;&#20855;&#26377;&#21452;&#32534;&#30721;&#22120;&#21644;Prompt-aware LoRA&#26435;&#37325;&#36866;&#37197;&#22120;&#30340;&#31283;&#20581;&#21644;&#33258;&#36866;&#24212;&#35821;&#38899;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#21033;&#29992;&#21452;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#35299;&#32806;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#38899;&#20449;&#24687;&#65292;&#21033;&#29992;Whisper&#32534;&#30721;&#22120;&#22788;&#29702;&#35821;&#38899;&#30340;&#35821;&#20041;&#20869;&#23481;&#65292;&#21033;&#29992;WavLM&#32534;&#30721;&#22120;&#25429;&#25417;&#35828;&#35805;&#32773;&#36523;&#20221;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#22312;&#35838;&#31243;&#23398;&#20064;&#26694;&#26550;&#20869;&#65292;WavLLM&#39318;&#20808;&#36890;&#36807;&#28151;&#21512;&#35201;&#32032;&#36827;&#34892;&#20248;&#21270;&#26469;&#24314;&#31435;&#20854;&#22522;&#30784;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00656v1 Announce Type: cross  Abstract: The recent advancements in large language models (LLMs) have revolutionized the field of natural language processing, progressively broadening their scope to multimodal perception and generation. However, effectively integrating listening capabilities into LLMs poses significant challenges, particularly with respect to generalizing across varied contexts and executing complex auditory tasks. In this work, we introduce WavLLM, a robust and adaptive speech large language model with dual encoders, and a prompt-aware LoRA weight adapter, optimized by a two-stage curriculum learning approach. Leveraging dual encoders, we decouple different types of speech information, utilizing a Whisper encoder to process the semantic content of speech, and a WavLM encoder to capture the unique characteristics of the speaker's identity. Within the curriculum learning framework, WavLLM first builds its foundational capabilities by optimizing on mixed elemen
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#20998;&#31867;&#21644;&#35282;&#33394;&#20998;&#26512;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2404.00282</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#30340;&#35843;&#26597;:&#27010;&#24565;&#12289;&#20998;&#31867;&#21644;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00282
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#20998;&#31867;&#21644;&#35282;&#33394;&#20998;&#26512;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25317;&#26377;&#24191;&#27867;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#21644;&#39640;&#32423;&#36890;&#29992;&#33021;&#21147;&#65292;&#23427;&#20204;&#22312;&#22686;&#24378;&#23398;&#20064;&#26041;&#38754;&#22914;&#22810;&#20219;&#21153;&#23398;&#20064;&#12289;&#26679;&#26412;&#25928;&#29575;&#21644;&#20219;&#21153;&#35268;&#21010;&#31561;&#26041;&#38754;&#23637;&#29616;&#20986;&#28508;&#21147;&#12290;&#26412;&#35843;&#26597;&#32508;&#36848;&#20102;&#29616;&#26377;$\textit{LLM&#22686;&#24378;RL}$&#25991;&#29486;&#65292;&#24635;&#32467;&#20102;&#20854;&#19982;&#20256;&#32479;RL&#26041;&#27861;&#30340;&#29305;&#24449;&#65292;&#26088;&#22312;&#28548;&#28165;&#30740;&#31350;&#33539;&#22260;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#21033;&#29992;&#32463;&#20856;&#30340;Agent-&#29615;&#22659;&#20132;&#20114;&#33539;&#20363;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#20998;&#31867;&#27861;&#65292;&#31995;&#32479;&#22320;&#23558;LLMs&#22312;RL&#20013;&#30340;&#21151;&#33021;&#20998;&#31867;&#65292;&#21253;&#25324;&#22235;&#31181;&#35282;&#33394;&#65306;&#20449;&#24687;&#22788;&#29702;&#22120;&#12289;&#22870;&#21169;&#35774;&#35745;&#32773;&#12289;&#20915;&#31574;&#32773;&#21644;&#29983;&#25104;&#22120;&#12290;&#27492;&#22806;&#65292;&#38024;&#23545;&#27599;&#20010;&#35282;&#33394;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#26041;&#27861;&#35770;&#65292;&#20998;&#26512;&#20102;&#32531;&#35299;&#30340;&#29305;&#23450;RL&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#26041;&#21521;&#30340;&#35265;&#35299;&#12290;&#26368;&#21518;&#65292;&#28508;&#22312;&#24212;&#29992;&#12289;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00282v1 Announce Type: cross  Abstract: With extensive pre-trained knowledge and high-level general capabilities, large language models (LLMs) emerge as a promising avenue to augment reinforcement learning (RL) in aspects such as multi-task learning, sample efficiency, and task planning. In this survey, we provide a comprehensive review of the existing literature in $\textit{LLM-enhanced RL}$ and summarize its characteristics compared to conventional RL methods, aiming to clarify the research scope and directions for future studies. Utilizing the classical agent-environment interaction paradigm, we propose a structured taxonomy to systematically categorize LLMs' functionalities in RL, including four roles: information processor, reward designer, decision-maker, and generator. Additionally, for each role, we summarize the methodologies, analyze the specific RL challenges that are mitigated, and provide insights into future directions. Lastly, potential applications, prospecti
&lt;/p&gt;</description></item><item><title>SugarcaneNet2024&#26159;&#36890;&#36807;&#20248;&#21270;&#21152;&#26435;&#24179;&#22343;&#38598;&#25104;LASSO&#27491;&#21017;&#21270;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#29976;&#34071;&#30149;&#23475;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377;&#24555;&#36895;&#20934;&#30830;&#30340;&#26816;&#27979;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.18870</link><description>&lt;p&gt;
SugarcaneNet2024: LASSO&#27491;&#21017;&#21270;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20248;&#21270;&#21152;&#26435;&#24179;&#22343;&#38598;&#25104;&#26041;&#27861;&#29992;&#20110;&#29976;&#34071;&#30149;&#23475;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
SugarcaneNet2024: An Optimized Weighted Average Ensemble Approach of LASSO Regularized Pre-trained Models for Sugarcane Disease Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18870
&lt;/p&gt;
&lt;p&gt;
SugarcaneNet2024&#26159;&#36890;&#36807;&#20248;&#21270;&#21152;&#26435;&#24179;&#22343;&#38598;&#25104;LASSO&#27491;&#21017;&#21270;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#29976;&#34071;&#30149;&#23475;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377;&#24555;&#36895;&#20934;&#30830;&#30340;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29976;&#34071;&#20316;&#20026;&#19990;&#30028;&#31958;&#19994;&#30340;&#20851;&#38190;&#20316;&#29289;&#65292;&#23481;&#26131;&#21463;&#22810;&#31181;&#30149;&#23475;&#20405;&#23475;&#65292;&#36825;&#20123;&#30149;&#23475;&#23545;&#20854;&#20135;&#37327;&#21644;&#36136;&#37327;&#37117;&#26377;&#37325;&#22823;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#26377;&#25928;&#31649;&#29702;&#21644;&#23454;&#26045;&#39044;&#38450;&#25514;&#26045;&#65292;&#24517;&#39035;&#21450;&#26102;&#20934;&#30830;&#22320;&#26816;&#27979;&#30149;&#23475;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SugarcaneNet2024&#30340;&#29420;&#29305;&#27169;&#22411;&#65292;&#36890;&#36807;&#21494;&#29255;&#22270;&#20687;&#22788;&#29702;&#65292;&#33021;&#22815;&#20248;&#20110;&#20808;&#21069;&#26041;&#27861;&#33258;&#21160;&#24555;&#36895;&#26816;&#27979;&#29976;&#34071;&#30149;&#23475;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#27719;&#24635;&#20102;&#19971;&#20010;&#23450;&#21046;&#30340;&#12289;&#32463;&#36807;LASSO&#27491;&#21017;&#21270;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20248;&#21270;&#21152;&#26435;&#24179;&#22343;&#38598;&#25104;&#65292;&#29305;&#21035;&#26159;InceptionV3&#12289;InceptionResNetV2&#12289;DenseNet201&#12289;DenseNet169&#12289;Xception&#21644;ResNet152V2&#12290;&#26368;&#21021;&#65292;&#25105;&#20204;&#22312;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#24213;&#37096;&#28155;&#21152;&#20102;&#19977;&#23618;&#26356;&#23494;&#38598;&#23618;&#65292;&#20855;&#26377;0.0001&#30340;LASSO&#27491;&#21017;&#21270;&#65292;&#19977;&#20010;30%&#30340;dropout&#23618;&#21644;&#19977;&#20010;&#21551;&#29992;renorm&#30340;&#25209;&#37327;&#24402;&#19968;&#21270;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18870v1 Announce Type: cross  Abstract: Sugarcane, a key crop for the world's sugar industry, is prone to several diseases that have a substantial negative influence on both its yield and quality. To effectively manage and implement preventative initiatives, diseases must be detected promptly and accurately. In this study, we present a unique model called sugarcaneNet2024 that outperforms previous methods for automatically and quickly detecting sugarcane disease through leaf image processing. Our proposed model consolidates an optimized weighted average ensemble of seven customized and LASSO-regularized pre-trained models, particularly InceptionV3, InceptionResNetV2, DenseNet201, DenseNet169, Xception, and ResNet152V2. Initially, we added three more dense layers with 0.0001 LASSO regularization, three 30% dropout layers, and three batch normalizations with renorm enabled at the bottom of these pre-trained models to improve the performance. The accuracy of sugarcane leaf dise
&lt;/p&gt;</description></item><item><title>GeNet&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#20041;&#36890;&#20449;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#36716;&#25442;&#20026;&#22270;&#32467;&#26500;&#12289;&#21033;&#29992;&#32534;&#30721;&#22120;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#24182;&#21033;&#29992;&#35299;&#30721;&#22120;&#37325;&#24314;&#20449;&#24687;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#25239;&#22122;&#22768;&#20219;&#21153;&#23548;&#21521;&#36890;&#20449;&#12290;</title><link>https://arxiv.org/abs/2403.18296</link><description>&lt;p&gt;
GeNet:&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25239;&#22122;&#22768;&#20219;&#21153;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
GeNet: A Graph Neural Network-based Anti-noise Task-Oriented Semantic Communication Paradigm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18296
&lt;/p&gt;
&lt;p&gt;
GeNet&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#20041;&#36890;&#20449;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#36716;&#25442;&#20026;&#22270;&#32467;&#26500;&#12289;&#21033;&#29992;&#32534;&#30721;&#22120;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#24182;&#21033;&#29992;&#35299;&#30721;&#22120;&#37325;&#24314;&#20449;&#24687;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#25239;&#22122;&#22768;&#20219;&#21153;&#23548;&#21521;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#35821;&#20041;&#36890;&#20449;&#20219;&#21153;&#26041;&#27861;&#20381;&#36182;&#20110;&#20102;&#35299;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#26469;&#20943;&#36731;&#36890;&#36947;&#22122;&#22768;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#22312;&#29305;&#23450;&#30340;SNR&#26465;&#20214;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#38656;&#35201;&#22823;&#37327;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GeNet&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#35821;&#20041;&#36890;&#20449;&#33539;&#24335;&#65292;&#26088;&#22312;&#25269;&#25239;&#22122;&#22768;&#65292;&#20174;&#32780;&#20419;&#36827;&#20219;&#21153;&#23548;&#21521;&#36890;&#20449;&#65288;TOC&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#39318;&#20808;&#23558;&#36755;&#20837;&#25968;&#25454;&#22270;&#20687;&#36716;&#25442;&#20026;&#22270;&#32467;&#26500;&#12290;&#28982;&#21518;&#21033;&#29992;&#22522;&#20110;GNN&#30340;&#32534;&#30721;&#22120;&#20174;&#28304;&#25968;&#25454;&#20013;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#12290;&#36825;&#20123;&#25552;&#21462;&#30340;&#35821;&#20041;&#20449;&#24687;&#28982;&#21518;&#36890;&#36807;&#36890;&#36947;&#20256;&#36755;&#12290;&#22312;&#25509;&#25910;&#31471;&#65292;&#20351;&#29992;&#22522;&#20110;GNN&#30340;&#35299;&#30721;&#22120;&#20174;&#28304;&#25968;&#25454;&#20013;&#37325;&#24314;&#30456;&#20851;&#30340;&#35821;&#20041;&#20449;&#24687;&#20197;&#29992;&#20110;TOC&#12290;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;GeNet&#22312;&#25239;&#22122;&#22768;TOC&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18296v1 Announce Type: cross  Abstract: Traditional approaches to semantic communication tasks rely on the knowledge of the signal-to-noise ratio (SNR) to mitigate channel noise. However, these methods necessitate training under specific SNR conditions, entailing considerable time and computational resources. In this paper, we propose GeNet, a Graph Neural Network (GNN)-based paradigm for semantic communication aimed at combating noise, thereby facilitating Task-Oriented Communication (TOC). We propose a novel approach where we first transform the input data image into graph structures. Then we leverage a GNN-based encoder to extract semantic information from the source data. This extracted semantic information is then transmitted through the channel. At the receiver's end, a GNN-based decoder is utilized to reconstruct the relevant semantic information from the source data for TOC. Through experimental evaluation, we show GeNet's effectiveness in anti-noise TOC while decoup
&lt;/p&gt;</description></item><item><title>DreamSampler&#26694;&#26550;&#36890;&#36807;&#25972;&#21512;&#21453;&#21521;&#25193;&#25955;&#37319;&#26679;&#21644;&#20998;&#25968;&#33976;&#39311;&#65292;&#25552;&#20379;&#20102;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#20687;&#22788;&#29702;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20998;&#25968;&#33976;&#39311;&#26131;&#23849;&#28291;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22270;&#20687;&#32534;&#36753;&#21644;&#37325;&#26500;&#20013;&#23637;&#29616;&#20102;&#31454;&#20105;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.11415</link><description>&lt;p&gt;
DreamSampler&#65306;&#32479;&#19968;&#25193;&#25955;&#37319;&#26679;&#21644;&#20998;&#25968;&#33976;&#39311;&#20197;&#29992;&#20110;&#22270;&#20687;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
DreamSampler: Unifying Diffusion Sampling and Score Distillation for Image Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11415
&lt;/p&gt;
&lt;p&gt;
DreamSampler&#26694;&#26550;&#36890;&#36807;&#25972;&#21512;&#21453;&#21521;&#25193;&#25955;&#37319;&#26679;&#21644;&#20998;&#25968;&#33976;&#39311;&#65292;&#25552;&#20379;&#20102;&#27169;&#22411;&#26080;&#20851;&#30340;&#22270;&#20687;&#22788;&#29702;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20998;&#25968;&#33976;&#39311;&#26131;&#23849;&#28291;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22270;&#20687;&#32534;&#36753;&#21644;&#37325;&#26500;&#20013;&#23637;&#29616;&#20102;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#37319;&#26679;&#21644;&#20998;&#25968;&#33976;&#39311;&#24050;&#25104;&#20026;&#26368;&#36817;&#20960;&#24180;&#20351;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65288;LDMs&#65289;&#36827;&#34892;&#22270;&#20687;&#22788;&#29702;&#30340;&#20027;&#35201;&#24037;&#20855;&#12290;&#34429;&#28982;&#21453;&#21521;&#25193;&#25955;&#37319;&#26679;&#36890;&#24120;&#38656;&#35201;&#35843;&#25972;LDM&#26550;&#26500;&#25110;&#29305;&#24449;&#24037;&#31243;&#65292;&#20998;&#25968;&#33976;&#39311;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#65292;&#20294;&#24448;&#24448;&#23481;&#26131;&#21457;&#29983;&#27169;&#24335;&#23849;&#28291;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#24182;&#21033;&#29992;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;DreamSampler&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#28508;&#22312;&#20248;&#21270;&#30340;&#35270;&#35282;&#26080;&#32541;&#22320;&#25972;&#21512;&#20102;&#36825;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;&#31867;&#20284;&#20110;&#20998;&#25968;&#33976;&#39311;&#65292;DreamSampler&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#20219;&#20309;LDM&#26550;&#26500;&#30340;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#65292;&#20294;&#23427;&#20801;&#35768;&#22312;&#22270;&#20687;&#32534;&#36753;&#21644;&#37325;&#26500;&#20013;&#36827;&#34892;&#33976;&#39311;&#21644;&#21453;&#21521;&#37319;&#26679;&#65292;&#24182;&#25552;&#20379;&#39069;&#22806;&#30340;&#25351;&#23548;&#12290;&#36890;&#36807;&#28041;&#21450;&#22270;&#20687;&#32534;&#36753;&#12289;SVG&#37325;&#26500;&#31561;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31454;&#20105;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11415v1 Announce Type: cross  Abstract: Reverse sampling and score-distillation have emerged as main workhorses in recent years for image manipulation using latent diffusion models (LDMs). While reverse diffusion sampling often requires adjustments of LDM architecture or feature engineering, score distillation offers a simple yet powerful model-agnostic approach, but it is often prone to mode-collapsing. To address these limitations and leverage the strengths of both approaches, here we introduce a novel framework called {\em DreamSampler}, which seamlessly integrates these two distinct approaches through the lens of regularized latent optimization. Similar to score-distillation, DreamSampler is a model-agnostic approach applicable to any LDM architecture, but it allows both distillation and reverse sampling with additional guidance for image editing and reconstruction. Through experiments involving image editing, SVG reconstruction and etc, we demonstrate the competitive pe
&lt;/p&gt;</description></item><item><title>P2LHAP&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Patch-to-Label Seq2Seq&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#39640;&#25928;&#30340;&#21333;&#19968;&#20219;&#21153;&#27169;&#22411;&#20013;&#21516;&#26102;&#23454;&#29616;&#20154;&#31867;&#27963;&#21160;&#30340;&#20998;&#21106;&#12289;&#35782;&#21035;&#21644;&#39044;&#27979;</title><link>https://arxiv.org/abs/2403.08214</link><description>&lt;p&gt;
P2LHAP&#65306;&#22522;&#20110;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#30340;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#12289;&#20998;&#21106;&#21644;&#39044;&#27979;&#30340;Patch-to-Label Seq2Seq Transformer
&lt;/p&gt;
&lt;p&gt;
P2LHAP:Wearable sensor-based human activity recognition, segmentation and forecast through Patch-to-Label Seq2Seq Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08214
&lt;/p&gt;
&lt;p&gt;
P2LHAP&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Patch-to-Label Seq2Seq&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#39640;&#25928;&#30340;&#21333;&#19968;&#20219;&#21153;&#27169;&#22411;&#20013;&#21516;&#26102;&#23454;&#29616;&#20154;&#31867;&#27963;&#21160;&#30340;&#20998;&#21106;&#12289;&#35782;&#21035;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24456;&#38590;&#21516;&#26102;&#20174;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#20998;&#21106;&#12289;&#35782;&#21035;&#21644;&#39044;&#27979;&#20154;&#31867;&#27963;&#21160;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#36741;&#21161;&#29983;&#27963;&#31561;&#39046;&#22495;&#30340;&#23454;&#29992;&#24615;&#65292;&#32780;&#36825;&#20123;&#39046;&#22495;&#23545;&#20110;&#23454;&#26102;&#29702;&#35299;&#27491;&#22312;&#36827;&#34892;&#21644;&#21363;&#23558;&#21457;&#29983;&#30340;&#27963;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;P2LHAP&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;Patch-to-Label Seq2Seq&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#39640;&#25928;&#30340;&#21333;&#19968;&#20219;&#21153;&#27169;&#22411;&#20013;&#35299;&#20915;&#36825;&#19977;&#20010;&#20219;&#21153;&#12290;P2LHAP&#23558;&#20256;&#24863;&#22120;&#25968;&#25454;&#27969;&#21010;&#20998;&#20026;&#19968;&#31995;&#21015;&#8220;&#34917;&#19969;&#8221;&#65292;&#20316;&#20026;&#36755;&#20837;&#26631;&#35760;&#65292;&#24182;&#36755;&#20986;&#19968;&#31995;&#21015;&#21253;&#25324;&#39044;&#27979;&#30340;&#26410;&#26469;&#27963;&#21160;&#22312;&#20869;&#30340;&#34917;&#19969;&#32423;&#27963;&#21160;&#26631;&#31614;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21608;&#22260;&#34917;&#19969;&#26631;&#31614;&#30340;&#29420;&#29305;&#24179;&#28369;&#25216;&#26415;&#65292;&#21487;&#20934;&#30830;&#35782;&#21035;&#27963;&#21160;&#36793;&#30028;&#12290;&#27492;&#22806;&#65292;P2LHAP&#36890;&#36807;&#20256;&#24863;&#22120;&#20449;&#21495;&#36890;&#36947;&#29420;&#31435;&#30340;Transformer&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#23398;&#20064;&#34917;&#19969;&#32423;&#34920;&#31034;&#12290;&#25152;&#26377;&#36890;&#36947;&#22312;&#25152;&#26377;&#24207;&#21015;&#19978;&#20849;&#20139;&#23884;&#20837;&#21644;Transformer&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08214v1 Announce Type: cross  Abstract: Traditional deep learning methods struggle to simultaneously segment, recognize, and forecast human activities from sensor data. This limits their usefulness in many fields such as healthcare and assisted living, where real-time understanding of ongoing and upcoming activities is crucial. This paper introduces P2LHAP, a novel Patch-to-Label Seq2Seq framework that tackles all three tasks in a efficient single-task model. P2LHAP divides sensor data streams into a sequence of "patches", served as input tokens, and outputs a sequence of patch-level activity labels including the predicted future activities. A unique smoothing technique based on surrounding patch labels, is proposed to identify activity boundaries accurately. Additionally, P2LHAP learns patch-level representation by sensor signal channel-independent Transformer encoders and decoders. All channels share embedding and Transformer weights across all sequences. Evaluated on thre
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25104;&#21151;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#33258;&#22238;&#24402;&#24314;&#27169;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#35270;&#35273;&#35789;&#30340;&#27010;&#24565;&#65292;&#23558;&#35270;&#35273;&#29305;&#24449;&#26144;&#23556;&#21040;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35789;&#27719;&#34920;&#19978;&#65292;&#20026;&#35270;&#35273;&#24314;&#27169;&#25552;&#20379;&#20102;&#30417;&#30563;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.07720</link><description>&lt;p&gt;
&#36890;&#36807;&#35270;&#35273;&#35789;&#36827;&#34892;&#22810;&#27169;&#24577;&#33258;&#22238;&#24402;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Auto-regressive Modeling via Visual Words
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25104;&#21151;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#33258;&#22238;&#24402;&#24314;&#27169;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#35270;&#35273;&#35789;&#30340;&#27010;&#24565;&#65292;&#23558;&#35270;&#35273;&#29305;&#24449;&#26144;&#23556;&#21040;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35789;&#27719;&#34920;&#19978;&#65292;&#20026;&#35270;&#35273;&#24314;&#27169;&#25552;&#20379;&#20102;&#30417;&#30563;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21463;&#30410;&#20110;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#33258;&#22238;&#24402;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#24863;&#30693;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23558;&#33258;&#22238;&#24402;&#24314;&#27169;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#22330;&#26223;&#20197;&#26500;&#24314;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#26102;&#65292;&#23384;&#22312;&#19968;&#20010;&#24040;&#22823;&#22256;&#38590;&#65292;&#21363;&#22270;&#20687;&#20449;&#24687;&#22312;LMM&#20013;&#20197;&#36830;&#32493;&#30340;&#35270;&#35273;&#23884;&#20837;&#36827;&#34892;&#22788;&#29702;&#65292;&#26080;&#27861;&#33719;&#24471;&#31163;&#25955;&#30340;&#29992;&#20110;&#20998;&#31867;&#30340;&#30417;&#30563;&#26631;&#31614;&#12290;&#26412;&#25991;&#39318;&#27425;&#25104;&#21151;&#22320;&#36827;&#34892;&#20102;&#22810;&#27169;&#24577;&#33258;&#22238;&#24402;&#24314;&#27169;&#65292;&#24182;&#25552;&#20986;&#20102;&#32479;&#19968;&#30340;&#30446;&#26631;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35270;&#35273;&#35789;&#30340;&#27010;&#24565;&#65292;&#23558;&#35270;&#35273;&#29305;&#24449;&#26144;&#23556;&#21040;LLM&#35789;&#27719;&#34920;&#19978;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#20026;&#35270;&#35273;&#24314;&#27169;&#25552;&#20379;&#20102;&#30417;&#30563;&#20449;&#24687;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;LMM&#20013;&#35821;&#20041;&#31354;&#38388;&#20869;&#35270;&#35273;&#29305;&#24449;&#30340;&#20998;&#24067;&#20197;&#21450;&#20351;&#29992;&#25991;&#26412;&#23884;&#20837;&#26469;&#34920;&#31034;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07720v1 Announce Type: cross  Abstract: Large Language Models (LLMs), benefiting from the auto-regressive modelling approach performed on massive unannotated texts corpora, demonstrates powerful perceptual and reasoning capabilities. However, as for extending auto-regressive modelling to multi-modal scenarios to build Large Multi-modal Models (LMMs), there lies a great difficulty that the image information is processed in the LMM as continuous visual embeddings, which cannot obtain discrete supervised labels for classification. In this paper, we successfully perform multi-modal auto-regressive modeling with a unified objective for the first time. Specifically, we propose the concept of visual words, which maps the visual features to probability distributions over LLM's vocabulary, providing supervision information for visual modelling. We further explore the distribution of visual features in the semantic space within LMM and the possibility of using text embeddings to repre
&lt;/p&gt;</description></item><item><title>&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#19968;&#31181;&#26080;&#21019;&#31958;&#23615;&#30149;&#35786;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#31070;&#32463;&#32593;&#32476;&#21644;&#25968;&#25454;&#24179;&#34913;&#25216;&#26415;&#65292;&#22312;&#20934;&#30830;&#24615;&#12289;&#25935;&#24863;&#24615;&#21644;&#29305;&#24322;&#24615;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;</title><link>https://arxiv.org/abs/2403.07483</link><description>&lt;p&gt;
&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#31958;&#23615;&#30149;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning Approach to Diabetes Diagnosis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07483
&lt;/p&gt;
&lt;p&gt;
&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#19968;&#31181;&#26080;&#21019;&#31958;&#23615;&#30149;&#35786;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#31070;&#32463;&#32593;&#32476;&#21644;&#25968;&#25454;&#24179;&#34913;&#25216;&#26415;&#65292;&#22312;&#20934;&#30830;&#24615;&#12289;&#25935;&#24863;&#24615;&#21644;&#29305;&#24322;&#24615;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31958;&#23615;&#30149;&#26159;&#30001;&#33008;&#23707;&#32032;&#20135;&#29983;&#25110;&#21033;&#29992;&#19981;&#36275;&#23548;&#33268;&#30340;&#65292;&#23545;&#36523;&#20307;&#36896;&#25104;&#20102;&#24191;&#27867;&#30340;&#21361;&#23475;&#12290;&#29616;&#26377;&#30340;&#35786;&#26029;&#26041;&#27861;&#36890;&#24120;&#26159;&#20405;&#20837;&#24615;&#30340;&#65292;&#24182;&#20276;&#26377;&#35832;&#22810;&#32570;&#28857;&#65292;&#27604;&#22914;&#25104;&#26412;&#38480;&#21046;&#12290;&#23613;&#31649;&#23384;&#22312;&#20687;&#31867;&#38388;k&#26368;&#36817;&#37051;(CkNN)&#21644;&#36890;&#29992;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;(GRNN)&#36825;&#26679;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#22312;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#26102;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#21033;&#29992;&#20256;&#24863;&#25216;&#26415;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24102;&#26377;&#25209;&#37327;&#26631;&#20934;&#21270;&#30340;&#21453;&#21521;&#20256;&#25773;&#31070;&#32463;&#32593;&#32476;(BPNN)&#36827;&#34892;&#26080;&#21019;&#31958;&#23615;&#30149;&#35786;&#26029;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#25968;&#25454;&#37325;&#37319;&#26679;&#21644;&#24402;&#19968;&#21270;&#20197;&#23454;&#29616;&#31867;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#35832;&#22810;&#25361;&#25112;&#65292;&#27604;&#22914;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#20851;&#30340;&#24615;&#33021;&#21463;&#38480;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#22312;&#25972;&#20307;&#20934;&#30830;&#24615;&#12289;&#25935;&#24863;&#24615;&#21644;&#29305;&#24322;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#29575;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07483v1 Announce Type: cross  Abstract: Diabetes, resulting from inadequate insulin production or utilization, causes extensive harm to the body. Existing diagnostic methods are often invasive and come with drawbacks, such as cost constraints. Although there are machine learning models like Classwise k Nearest Neighbor (CkNN) and General Regression Neural Network (GRNN), they struggle with imbalanced data and result in under-performance. Leveraging advancements in sensor technology and machine learning, we propose a non-invasive diabetes diagnosis using a Back Propagation Neural Network (BPNN) with batch normalization, incorporating data re-sampling and normalization for class balancing. Our method addresses existing challenges such as limited performance associated with traditional machine learning. Experimental results on three datasets show significant improvements in overall accuracy, sensitivity, and specificity compared to traditional methods. Notably, we achieve accur
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SimuCourt&#21496;&#27861;&#22522;&#20934;&#65292;&#21253;&#25324;&#30495;&#23454;&#19990;&#30028;&#30340;&#21496;&#27861;&#25991;&#20214;&#65292;&#24182;&#24341;&#20837;&#20102;&#21496;&#27861;&#20915;&#31574;&#20219;&#21153;&#21644;&#22810;&#20195;&#29702;&#26694;&#26550;&#65292;&#35780;&#20272;&#20102;&#20195;&#29702;&#30340;&#21496;&#27861;&#20998;&#26512;&#21644;&#20915;&#31574;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.02959</link><description>&lt;p&gt;
SimuCourt: &#21033;&#29992;&#30495;&#23454;&#21496;&#27861;&#21028;&#20915;&#25991;&#20214;&#26500;&#24314;&#21496;&#27861;&#20915;&#31574;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
SimuCourt: Building Judicial Decision-Making Agents with Real-world Judgement Documents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02959
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SimuCourt&#21496;&#27861;&#22522;&#20934;&#65292;&#21253;&#25324;&#30495;&#23454;&#19990;&#30028;&#30340;&#21496;&#27861;&#25991;&#20214;&#65292;&#24182;&#24341;&#20837;&#20102;&#21496;&#27861;&#20915;&#31574;&#20219;&#21153;&#21644;&#22810;&#20195;&#29702;&#26694;&#26550;&#65292;&#35780;&#20272;&#20102;&#20195;&#29702;&#30340;&#21496;&#27861;&#20998;&#26512;&#21644;&#20915;&#31574;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#20256;&#32479;&#21496;&#27861;&#34892;&#19994;&#21508;&#20010;&#26041;&#38754;&#30340;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22823;&#22810;&#25968;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#20010;&#21035;&#21496;&#27861;&#38454;&#27573;&#65292;&#24573;&#35270;&#20102;&#36328;&#38454;&#27573;&#30340;&#21327;&#20316;&#12290;&#38543;&#30528;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#25903;&#25345;&#30340;&#33258;&#20027;&#20195;&#29702;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#26234;&#33021;&#65292;&#24182;&#33021;&#20570;&#20986;&#22797;&#26434;&#20915;&#31574;&#65292;&#20026;&#21496;&#27861;&#26234;&#33021;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SimuCourt&#65292;&#19968;&#20010;&#21496;&#27861;&#22522;&#20934;&#65292;&#21253;&#25324;&#26469;&#33258;&#30495;&#23454;&#19990;&#30028;&#30340;420&#20221;&#21028;&#20915;&#25991;&#20214;&#65292;&#28085;&#30422;&#20102;&#19977;&#31181;&#26368;&#24120;&#35265;&#31867;&#22411;&#30340;&#21496;&#27861;&#26696;&#20363;&#65292;&#20197;&#21450;&#19968;&#20010;&#26032;&#39062;&#20219;&#21153;&#21496;&#27861;&#20915;&#31574;&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#29702;&#30340;&#21496;&#27861;&#20998;&#26512;&#21644;&#20915;&#31574;&#33021;&#21147;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#19968;&#20219;&#21153;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#21496;&#27861;&#30693;&#35782;&#24211;&#65292;JudicialKB&#65292;&#20854;&#20013;&#21253;&#21547;&#22810;&#31181;&#27861;&#24459;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20195;&#29702;&#26694;&#26550;&#65292;AgentsCourt
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02959v1 Announce Type: cross  Abstract: With the development of deep learning, natural language processing technology has effectively improved the efficiency of various aspects of the traditional judicial industry. However, most current efforts focus solely on individual judicial stage, overlooking cross-stage collaboration. As the autonomous agents powered by large language models are becoming increasingly smart and able to make complex decisions in real-world settings, offering new insights for judicial intelligence. In this paper, (1) we introduce SimuCourt, a judicial benchmark that encompasses 420 judgment documents from real-world, spanning the three most common types of judicial cases, and a novel task Judicial Decision-Making to evaluate the judicial analysis and decision-making power of agents. To support this task, we construct a large-scale judicial knowledge base, JudicialKB, with multiple legal knowledge. (2) we propose a novel multi-agent framework, AgentsCourt
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;Layer-wise Scalable Adapter&#31574;&#30053;MedLaSA&#65292;&#29992;&#20110;&#32534;&#36753;&#21307;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#31934;&#30830;&#20462;&#25913;&#21307;&#23398;&#30693;&#35782;&#24182;&#35299;&#37322;&#20107;&#23454;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#26041;&#27861;&#22312;&#21307;&#23398;&#30693;&#35782;&#29305;&#27530;&#21270;&#21644;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2402.18099</link><description>&lt;p&gt;
&#32534;&#36753;&#21307;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20107;&#23454;&#30693;&#35782;&#21644;&#35299;&#37322;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18099
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;Layer-wise Scalable Adapter&#31574;&#30053;MedLaSA&#65292;&#29992;&#20110;&#32534;&#36753;&#21307;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#31934;&#30830;&#20462;&#25913;&#21307;&#23398;&#30693;&#35782;&#24182;&#35299;&#37322;&#20107;&#23454;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#26041;&#27861;&#22312;&#21307;&#23398;&#30693;&#35782;&#29305;&#27530;&#21270;&#21644;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#26088;&#22312;&#31934;&#30830;&#20462;&#25913;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#29305;&#23450;&#30693;&#35782;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#19981;&#30456;&#20851;&#30340;&#30693;&#35782;&#19981;&#21464;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35299;&#20915;LLMs&#20013;&#30340;&#24187;&#35273;&#21644;&#36807;&#26102;&#38382;&#39064;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;&#22240;&#27492;&#65292;&#23427;&#21487;&#20197;&#25552;&#39640;LLMs&#22312;&#35768;&#22810;&#20851;&#38190;&#39046;&#22495;&#65288;&#20363;&#22914;&#21307;&#23398;&#39046;&#22495;&#65289;&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#24187;&#35273;&#26159;&#19981;&#21487;&#23481;&#24525;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20004;&#39033;&#27169;&#22411;&#32534;&#36753;&#30740;&#31350;&#65292;&#24182;&#22312;&#21307;&#23398;&#39046;&#22495;&#39564;&#35777;&#23427;&#20204;&#65306;&#65288;1&#65289;&#30452;&#25509;&#32534;&#36753;&#21307;&#23398;&#20107;&#23454;&#30693;&#35782;&#21644;&#65288;2&#65289;&#32534;&#36753;&#23545;&#20107;&#23454;&#30340;&#35299;&#37322;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24403;&#21069;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#22312;&#21307;&#23398;&#30693;&#35782;&#30340;&#29305;&#27530;&#21270;&#21644;&#22797;&#26434;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MedLaSA&#65292;&#19968;&#31181;&#26032;&#22411;&#30340;&#36866;&#29992;&#20110;&#21307;&#23398;&#27169;&#22411;&#32534;&#36753;&#30340;&#20998;&#23618;&#21487;&#25193;&#23637;&#36866;&#37197;&#22120;&#31574;&#30053;&#12290;&#23427;&#37319;&#29992;&#22240;&#26524;&#36861;&#36394;&#26469;&#35782;&#21035;&#31070;&#32463;&#20803;&#20013;&#30693;&#35782;&#30340;&#31934;&#30830;&#20301;&#32622;&#65292;&#28982;&#21518;&#23558;&#21487;&#25193;&#23637;&#36866;&#37197;&#22120;&#24341;&#20837;&#21040;LLMs&#30340;&#23494;&#38598;&#23618;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18099v1 Announce Type: cross  Abstract: Model editing aims to precisely modify the behaviours of large language models (LLMs) on specific knowledge while keeping irrelevant knowledge unchanged. It has been proven effective in resolving hallucination and out-of-date issues in LLMs. As a result, it can boost the application of LLMs in many critical domains (e.g., medical domain), where the hallucination is not tolerable. In this paper, we propose two model editing studies and validate them in the medical domain: (1) directly editing the factual medical knowledge and (2) editing the explanations to facts. Meanwhile, we observed that current model editing methods struggle with the specialization and complexity of medical knowledge. Therefore, we propose MedLaSA, a novel Layer-wise Scalable Adapter strategy for medical model editing. It employs causal tracing to identify the precise location of knowledge in neurons and then introduces scalable adapters into the dense layers of LL
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#27169;&#24577;LLMs&#22312;&#37319;&#29992;&#20018;&#32852;&#25512;&#29702;&#26102;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#20018;&#32852;&#25512;&#29702;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25552;&#39640;&#20102;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#20294;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20572;&#27490;&#25512;&#29702;&#25915;&#20987;&#25216;&#26415;&#25104;&#21151;&#35268;&#36991;&#20102;&#36825;&#31181;&#22686;&#24378;&#12290;</title><link>https://arxiv.org/abs/2402.14899</link><description>&lt;p&gt;
&#20572;&#27490;&#25512;&#29702;&#65281;&#24403;&#22810;&#27169;&#24577;LLMs&#19982;&#20018;&#32852;&#25512;&#29702;&#36935;&#21040;&#23545;&#25239;&#24615;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning Meets Adversarial Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14899
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#27169;&#24577;LLMs&#22312;&#37319;&#29992;&#20018;&#32852;&#25512;&#29702;&#26102;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#20018;&#32852;&#25512;&#29702;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25552;&#39640;&#20102;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#20294;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20572;&#27490;&#25512;&#29702;&#25915;&#20987;&#25216;&#26415;&#25104;&#21151;&#35268;&#36991;&#20102;&#36825;&#31181;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22810;&#27169;&#24577;LLMs&#65288;MLLMs&#65289;&#23637;&#31034;&#20102;&#24456;&#24378;&#30340;&#29702;&#35299;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20687;&#20256;&#32479;&#35270;&#35273;&#27169;&#22411;&#19968;&#26679;&#65292;&#23427;&#20204;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#22270;&#20687;&#30340;&#25915;&#20987;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20018;&#32852;&#25512;&#29702;&#65288;CoT&#65289;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#22312;MLLMs&#19978;&#65292;&#19981;&#20165;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#36890;&#36807;&#25552;&#20379;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#32570;&#20047;&#20851;&#20110;MLLMs&#22312;CoT&#19979;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;&#65292;&#20197;&#21450;&#22312;MLLMs&#29992;&#23545;&#25239;&#24615;&#22270;&#20687;&#25512;&#26029;&#38169;&#35823;&#31572;&#26696;&#26102;&#25512;&#29702;&#30340;&#21512;&#29702;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35780;&#20272;&#20102;&#37319;&#29992;CoT&#25512;&#29702;&#26102;MLLMs&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;CoT&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#25552;&#39640;&#20102;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#25269;&#25239;&#20102;&#24050;&#26377;&#30340;&#25915;&#20987;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20572;&#27490;&#25512;&#29702;&#25915;&#20987;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35268;&#36991;CoT&#24341;&#36215;&#30340;&#40065;&#26834;&#24615;&#22686;&#24378;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;CoT&#25512;&#29702;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14899v1 Announce Type: cross  Abstract: Recently, Multimodal LLMs (MLLMs) have shown a great ability to understand images. However, like traditional vision models, they are still vulnerable to adversarial images. Meanwhile, Chain-of-Thought (CoT) reasoning has been widely explored on MLLMs, which not only improves model's performance, but also enhances model's explainability by giving intermediate reasoning steps. Nevertheless, there is still a lack of study regarding MLLMs' adversarial robustness with CoT and an understanding of what the rationale looks like when MLLMs infer wrong answers with adversarial images. Our research evaluates the adversarial robustness of MLLMs when employing CoT reasoning, finding that CoT marginally improves adversarial robustness against existing attack methods. Moreover, we introduce a novel stop-reasoning attack technique that effectively bypasses the CoT-induced robustness enhancements. Finally, we demonstrate the alterations in CoT reasonin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#33258;&#36866;&#24212;&#29468;&#24819;&#30340;&#22312;&#32447;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;IT&#22522;&#30784;&#35774;&#26045;&#30340;&#33258;&#21160;&#21270;&#23433;&#20840;&#21709;&#24212;&#26041;&#27861;&#65292;&#20854;&#20013;&#28216;&#25103;&#21442;&#19982;&#32773;&#36890;&#36807;Bayesian&#23398;&#20064;&#35843;&#25972;&#29468;&#24819;&#65292;&#24182;&#36890;&#36807;&#25512;&#28436;&#26356;&#26032;&#31574;&#30053;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#26368;&#20339;&#25311;&#21512;&#65292;&#25552;&#39640;&#20102;&#25512;&#28436;&#22312;&#29468;&#24819;&#27169;&#22411;&#19979;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12499</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#29468;&#24819;&#30340;&#22312;&#32447;&#23398;&#20064;&#23454;&#29616;&#33258;&#21160;&#21270;&#23433;&#20840;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Automated Security Response through Online Learning with Adaptive Conjectures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12499
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#33258;&#36866;&#24212;&#29468;&#24819;&#30340;&#22312;&#32447;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;IT&#22522;&#30784;&#35774;&#26045;&#30340;&#33258;&#21160;&#21270;&#23433;&#20840;&#21709;&#24212;&#26041;&#27861;&#65292;&#20854;&#20013;&#28216;&#25103;&#21442;&#19982;&#32773;&#36890;&#36807;Bayesian&#23398;&#20064;&#35843;&#25972;&#29468;&#24819;&#65292;&#24182;&#36890;&#36807;&#25512;&#28436;&#26356;&#26032;&#31574;&#30053;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#26368;&#20339;&#25311;&#21512;&#65292;&#25552;&#39640;&#20102;&#25512;&#28436;&#22312;&#29468;&#24819;&#27169;&#22411;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38024;&#23545;IT&#22522;&#30784;&#35774;&#26045;&#30340;&#33258;&#21160;&#21270;&#23433;&#20840;&#21709;&#24212;&#65292;&#24182;&#23558;&#25915;&#20987;&#32773;&#21644;&#38450;&#24481;&#32773;&#20043;&#38388;&#30340;&#20114;&#21160;&#24418;&#24335;&#34920;&#36848;&#20026;&#19968;&#20010;&#37096;&#20998;&#35266;&#27979;&#12289;&#38750;&#24179;&#31283;&#21338;&#24328;&#12290;&#25105;&#20204;&#25918;&#23485;&#20102;&#28216;&#25103;&#27169;&#22411;&#27491;&#30830;&#35268;&#23450;&#30340;&#26631;&#20934;&#20551;&#35774;&#65292;&#24182;&#32771;&#34385;&#27599;&#20010;&#21442;&#19982;&#32773;&#23545;&#27169;&#22411;&#26377;&#19968;&#20010;&#27010;&#29575;&#24615;&#29468;&#24819;&#65292;&#21487;&#33021;&#22312;&#26576;&#31181;&#24847;&#20041;&#19978;&#38169;&#35823;&#35268;&#23450;&#65292;&#21363;&#30495;&#23454;&#27169;&#22411;&#30340;&#27010;&#29575;&#20026;0&#12290;&#36825;&#31181;&#24418;&#24335;&#20801;&#35768;&#25105;&#20204;&#25429;&#25417;&#20851;&#20110;&#22522;&#30784;&#35774;&#26045;&#21644;&#21442;&#19982;&#32773;&#24847;&#22270;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#22312;&#32447;&#23398;&#20064;&#26377;&#25928;&#30340;&#28216;&#25103;&#31574;&#30053;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;&#21442;&#19982;&#32773;&#36890;&#36807;&#36125;&#21494;&#26031;&#23398;&#20064;&#36845;&#20195;&#22320;&#35843;&#25972;&#20854;&#29468;&#24819;&#65292;&#24182;&#36890;&#36807;&#25512;&#28436;&#26356;&#26032;&#20854;&#31574;&#30053;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#29468;&#24819;&#20250;&#25910;&#25947;&#21040;&#26368;&#20339;&#25311;&#21512;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#20855;&#26377;&#29468;&#27979;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#25512;&#28436;&#23454;&#29616;&#24615;&#33021;&#25913;&#36827;&#30340;&#19978;&#38480;&#12290;&#20026;&#20102;&#21051;&#30011;&#28216;&#25103;&#30340;&#31283;&#23450;&#29366;&#24577;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Berk-Nash&#24179;&#34913;&#30340;&#19968;&#20010;&#21464;&#31181;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12499v1 Announce Type: cross  Abstract: We study automated security response for an IT infrastructure and formulate the interaction between an attacker and a defender as a partially observed, non-stationary game. We relax the standard assumption that the game model is correctly specified and consider that each player has a probabilistic conjecture about the model, which may be misspecified in the sense that the true model has probability 0. This formulation allows us to capture uncertainty about the infrastructure and the intents of the players. To learn effective game strategies online, we design a novel method where a player iteratively adapts its conjecture using Bayesian learning and updates its strategy through rollout. We prove that the conjectures converge to best fits, and we provide a bound on the performance improvement that rollout enables with a conjectured model. To characterize the steady state of the game, we propose a variant of the Berk-Nash equilibrium. We 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32534;&#20889;&#20195;&#30721;&#21644;&#19982;&#29615;&#22659;&#20132;&#20114;&#26469;&#26500;&#24314;&#19990;&#30028;&#27169;&#22411;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;LLM&#20195;&#29702;&#22312;&#26679;&#26412;&#25928;&#29575;&#19978;&#20248;&#20110;&#28145;&#24230;RL&#65292;&#24182;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#20248;&#20110;ReAct&#39118;&#26684;&#30340;&#20195;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.12275</link><description>&lt;p&gt;
WorldCoder&#65292;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;LLM&#20195;&#29702;&#65306;&#36890;&#36807;&#32534;&#20889;&#20195;&#30721;&#21644;&#19982;&#29615;&#22659;&#20132;&#20114;&#26500;&#24314;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12275
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32534;&#20889;&#20195;&#30721;&#21644;&#19982;&#29615;&#22659;&#20132;&#20114;&#26469;&#26500;&#24314;&#19990;&#30028;&#27169;&#22411;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;LLM&#20195;&#29702;&#22312;&#26679;&#26412;&#25928;&#29575;&#19978;&#20248;&#20110;&#28145;&#24230;RL&#65292;&#24182;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#20248;&#20110;ReAct&#39118;&#26684;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#20195;&#29702;&#65292;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#26500;&#24314;&#20195;&#34920;&#20854;&#23545;&#19990;&#30028;&#30693;&#35782;&#30340;Python&#31243;&#24207;&#12290;&#35813;&#19990;&#30028;&#27169;&#22411;&#35797;&#22270;&#35299;&#37322;&#20854;&#20132;&#20114;&#65292;&#21516;&#26102;&#23545;&#33258;&#24049;&#33021;&#22815;&#33719;&#24471;&#30340;&#22870;&#21169;&#25345;&#20048;&#35266;&#24577;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#25193;&#23637;LLM&#30340;&#31243;&#24207;&#21512;&#25104;&#24037;&#20316;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#22312;&#32593;&#26684;&#19990;&#30028;&#19978;&#30740;&#31350;&#20102;&#25105;&#20204;&#30340;&#20195;&#29702;&#65292;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#19978;&#27604;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26356;&#39640;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#27604;ReAct&#39118;&#26684;&#30340;&#20195;&#29702;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12275v1 Announce Type: new  Abstract: We give a model-based agent that builds a Python program representing its knowledge of the world based on its interactions with the environment. The world model tries to explain its interactions, while also being optimistic about what reward it can achieve. We do this by extending work on program synthesis via LLMs. We study our agent on gridworlds, finding our approach is more sample-efficient compared to deep RL, and more compute-efficient compared to ReAct-style agents.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25366;&#25496;&#20102;&#22312;&#32447;&#35805;&#35821;&#20013;&#30340;&#28436;&#32462;&#32454;&#24494;&#24046;&#21035;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20934;&#30830;&#26816;&#27979;&#21453;&#38382;&#20027;&#20041;&#65292;&#24182;&#22312;Twitter&#21644;YouTube&#25968;&#25454;&#38598;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.09934</link><description>&lt;p&gt;
&#20851;&#27880;&#20559;&#24046;&#65306;&#25366;&#25496;&#22312;&#32447;&#35805;&#35821;&#20013;&#30340;&#28436;&#32462;&#32454;&#24494;&#24046;&#21035;&#65292;&#26816;&#27979;&#21453;&#38382;&#20027;&#20041;
&lt;/p&gt;
&lt;p&gt;
Paying Attention to Deflections: Mining Pragmatic Nuances for Whataboutism Detection in Online Discourse
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25366;&#25496;&#20102;&#22312;&#32447;&#35805;&#35821;&#20013;&#30340;&#28436;&#32462;&#32454;&#24494;&#24046;&#21035;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20934;&#30830;&#26816;&#27979;&#21453;&#38382;&#20027;&#20041;&#65292;&#24182;&#22312;Twitter&#21644;YouTube&#25968;&#25454;&#38598;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#38382;&#20027;&#20041;&#22312;&#25200;&#20081;&#21465;&#20107;&#21644;&#25773;&#31181;&#19981;&#20449;&#20219;&#26041;&#38754;&#20855;&#26377;&#24378;&#22823;&#30340;&#24037;&#20855;&#25928;&#29992;&#65292;&#20294;&#22312;&#23450;&#37327;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20013;&#21364;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#36807;&#21435;&#30340;&#30740;&#31350;&#26410;&#33021;&#21306;&#20998;&#21453;&#38382;&#20027;&#20041;&#20316;&#20026;&#35823;&#23548;&#21644;&#23459;&#20256;&#31574;&#30053;&#30340;&#29992;&#36884;&#19982;&#20854;&#20316;&#20026;&#35821;&#29992;&#21644;&#35821;&#20041;&#26694;&#26550;&#24037;&#20855;&#30340;&#29992;&#36884;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#26032;&#30340;&#26469;&#33258;Twitter&#21644;YouTube&#30340;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#21453;&#38382;&#20027;&#20041;&#12289;&#23459;&#20256;&#21644;tu quoque&#35884;&#35823;&#20043;&#38388;&#30340;&#37325;&#21472;&#21644;&#21306;&#21035;&#12290;&#27492;&#22806;&#65292;&#32467;&#21512;&#26368;&#36817;&#22312;&#35821;&#35328;&#35821;&#20041;&#23398;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#23558;&#8220;what about&#8221;&#35789;&#27719;&#32467;&#26500;&#19982;&#21453;&#38382;&#20027;&#20041;&#21306;&#20998;&#24320;&#26469;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#20934;&#30830;&#26816;&#27979;&#21453;&#38382;&#20027;&#20041;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#20419;&#20351;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#27880;&#24847;&#26435;&#37325;&#36827;&#34892;&#36127;&#26679;&#26412;&#25366;&#25496;&#30340;&#26032;&#26041;&#27861;&#12290;&#22312;Twitter&#21644;YouTube&#25968;&#25454;&#38598;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20998;&#21035;&#30456;&#23545;&#20110;&#20043;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;4%&#21644;10%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09934v1 Announce Type: cross  Abstract: Whataboutism, a potent tool for disrupting narratives and sowing distrust, remains under-explored in quantitative NLP research. Moreover, past work has not distinguished its use as a strategy for misinformation and propaganda from its use as a tool for pragmatic and semantic framing. We introduce new datasets from Twitter and YouTube, revealing overlaps as well as distinctions between whataboutism, propaganda, and the tu quoque fallacy. Furthermore, drawing on recent work in linguistic semantics, we differentiate the `what about' lexical construct from whataboutism. Our experiments bring to light unique challenges in its accurate detection, prompting the introduction of a novel method using attention weights for negative sample mining. We report significant improvements of 4% and 10% over previous state-of-the-art methods in our Twitter and YouTube collections, respectively.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#22686;&#24378;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#37319;&#29992;&#27491;&#26679;&#26412;&#25277;&#26679;&#21644;&#26435;&#34913;&#37325;&#35201;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#24212;&#23545;&#22122;&#22768;AU&#26631;&#31614;&#21644;AU&#31867;&#22411;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.06165</link><description>&lt;p&gt;
&#23398;&#20064;&#23545;&#27604;&#29305;&#24449;&#34920;&#31034;&#26469;&#36827;&#34892;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning Contrastive Feature Representations for Facial Action Unit Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06165
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#22686;&#24378;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#37319;&#29992;&#27491;&#26679;&#26412;&#25277;&#26679;&#21644;&#26435;&#34913;&#37325;&#35201;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#24212;&#23545;&#22122;&#22768;AU&#26631;&#31614;&#21644;AU&#31867;&#22411;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#65288;AU&#65289;&#26816;&#27979;&#30340;&#20027;&#35201;&#26041;&#27861;&#28041;&#21450;&#30417;&#30563;&#30340;&#22810;&#26631;&#31614;&#20108;&#36827;&#21046;&#20998;&#31867;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#24120;&#24120;&#23545;AU&#30340;&#20687;&#32032;&#32423;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#65292;&#20174;&#32780;&#23545;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#34920;&#36798;&#33021;&#21147;&#25552;&#20986;&#20102;&#24456;&#22823;&#30340;&#35201;&#27714;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23384;&#22312;&#22122;&#22768;AU&#26631;&#31614;&#65292;&#36825;&#31181;&#20570;&#27861;&#22686;&#21152;&#20102;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#22686;&#24378;&#12290;&#30446;&#26631;&#26159;&#22312;AU&#26816;&#27979;&#39046;&#22495;&#20013;&#25670;&#33073;&#20256;&#32479;&#30340;&#20687;&#32032;&#32423;&#23398;&#20064;&#33539;&#24335;&#65292;&#33719;&#24471;&#21028;&#21035;&#29305;&#24449;&#12290;&#20026;&#20102;&#24212;&#23545;&#22122;&#22768;AU&#26631;&#31614;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#22686;&#24378;&#30417;&#30563;&#20449;&#21495;&#12290;&#36825;&#31181;&#22686;&#24378;&#26159;&#36890;&#36807;&#27491;&#26679;&#26412;&#25277;&#26679;&#23454;&#29616;&#30340;&#65292;&#21253;&#25324;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#27491;&#26679;&#26412;&#23545;&#12290;&#21478;&#22806;&#65292;&#20026;&#20102;&#20943;&#36731;&#27599;&#20010;AU&#31867;&#22411;&#30340;&#20998;&#24067;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26435;&#34913;&#37325;&#35201;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The predominant approach to facial action unit (AU) detection revolves around a supervised multi-label binary classification problem. Existing methodologies often encode pixel-level information of AUs, thereby imposing substantial demands on model complexity and expressiveness. Moreover, this practice elevates the susceptibility to overfitting due to the presence of noisy AU labels. In the present study, we introduce a contrastive learning framework enhanced by both supervised and self-supervised signals. The objective is to acquire discriminative features, deviating from the conventional pixel-level learning paradigm within the domain of AU detection. To address the challenge posed by noisy AU labels, we augment the supervised signal through the introduction of a self-supervised signal. This augmentation is achieved through positive sample sampling, encompassing three distinct types of positive sample pairs. Furthermore, to mitigate the imbalanced distribution of each AU type, we empl
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#23558;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21040;&#36864;&#21270;&#30340;&#20811;&#21033;&#31119;&#24503;&#20195;&#25968;&#20013;&#12290;&#36890;&#36807;&#32771;&#34385;&#20855;&#26377;&#38646;&#24130;&#25351;&#25968;&#20026;2&#30340;&#38646;&#24130;&#22522;&#21521;&#37327;&#65292;&#21487;&#20197;&#27867;&#21270;&#22522;&#20110;&#20108;&#27425;&#25968;&#30340;&#26041;&#27861;&#24182;&#25429;&#25417;&#23454;&#20307;&#23884;&#20837;&#20013;&#32570;&#20047;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#27169;&#24335;&#12290;&#30740;&#31350;&#35774;&#35745;&#20102;&#20004;&#20010;&#26032;&#27169;&#22411;&#26469;&#21457;&#29616;&#20195;&#25968;&#30340;&#21442;&#25968;&#65292;&#24182;&#35777;&#26126;&#38646;&#24130;&#21521;&#37327;&#26377;&#21161;&#20110;&#25429;&#25417;&#23454;&#20307;&#30340;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.04870</link><description>&lt;p&gt;
&#23558;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21040;&#36864;&#21270;&#30340;&#20811;&#21033;&#31119;&#24503;&#20195;&#25968;&#20013;
&lt;/p&gt;
&lt;p&gt;
Embedding Knowledge Graphs in Degenerate Clifford Algebras
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04870
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#23558;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21040;&#36864;&#21270;&#30340;&#20811;&#21033;&#31119;&#24503;&#20195;&#25968;&#20013;&#12290;&#36890;&#36807;&#32771;&#34385;&#20855;&#26377;&#38646;&#24130;&#25351;&#25968;&#20026;2&#30340;&#38646;&#24130;&#22522;&#21521;&#37327;&#65292;&#21487;&#20197;&#27867;&#21270;&#22522;&#20110;&#20108;&#27425;&#25968;&#30340;&#26041;&#27861;&#24182;&#25429;&#25417;&#23454;&#20307;&#23884;&#20837;&#20013;&#32570;&#20047;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#27169;&#24335;&#12290;&#30740;&#31350;&#35774;&#35745;&#20102;&#20004;&#20010;&#26032;&#27169;&#22411;&#26469;&#21457;&#29616;&#20195;&#25968;&#30340;&#21442;&#25968;&#65292;&#24182;&#35777;&#26126;&#38646;&#24130;&#21521;&#37327;&#26377;&#21161;&#20110;&#25429;&#25417;&#23454;&#20307;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20811;&#21033;&#31119;&#24503;&#20195;&#25968;&#26159;&#23454;&#25968;&#12289;&#22797;&#25968;&#21644;&#22235;&#20803;&#25968;&#30340;&#33258;&#28982;&#25512;&#24191;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22312;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#32972;&#26223;&#19979;&#65292;&#21482;&#26377;&#24418;&#24335;&#20026;$Cl_{p,q}$&#65288;&#21363;&#27809;&#26377;&#38646;&#24130;&#22522;&#21521;&#37327;&#30340;&#20195;&#25968;&#65289;&#30340;&#20811;&#21033;&#31119;&#24503;&#20195;&#25968;&#21463;&#21040;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#32771;&#34385;&#38646;&#24130;&#22522;&#21521;&#37327;&#65292;&#20854;&#24130;&#25351;&#25968;&#20026;2&#12290;&#22312;&#36825;&#20123;&#31354;&#38388;&#20013;&#65292;&#34987;&#31216;&#20026;$Cl_{p,q,r}$&#65292;&#21487;&#20197;&#27867;&#21270;&#22522;&#20110;&#20108;&#27425;&#25968;&#30340;&#26041;&#27861;&#65288;&#26080;&#27861;&#20351;&#29992;$Cl_{p,q}$&#36827;&#34892;&#24314;&#27169;&#65289;&#24182;&#25429;&#25417;&#28304;&#20110;&#23454;&#25968;&#21644;&#22797;&#25968;&#37096;&#20998;&#38388;&#32570;&#20047;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#23454;&#20307;&#23884;&#20837;&#30340;&#27169;&#24335;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#26032;&#27169;&#22411;&#26469;&#21457;&#29616;&#21442;&#25968;$p$&#65292;$q$&#21644;$r$&#12290;&#31532;&#19968;&#20010;&#27169;&#22411;&#20351;&#29992;&#36138;&#23146;&#25628;&#32034;&#20248;&#21270;$p$&#65292;$q$&#21644;$r$&#12290;&#31532;&#20108;&#20010;&#27169;&#22411;&#22522;&#20110;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#30340;&#36755;&#20837;&#30693;&#35782;&#22270;&#35889;&#30340;&#23884;&#20837;&#26469;&#39044;&#27979;$(p, q, r)$&#12290;&#25105;&#20204;&#22312;&#19971;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#38646;&#24130;&#21521;&#37327;&#26377;&#21161;&#20110;&#25429;&#25417;&#23454;&#20307;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clifford algebras are a natural generalization of the real numbers, the complex numbers, and the quaternions. So far, solely Clifford algebras of the form $Cl_{p,q}$ (i.e., algebras without nilpotent base vectors) have been studied in the context of knowledge graph embeddings. We propose to consider nilpotent base vectors with a nilpotency index of two. In these spaces, denoted $Cl_{p,q,r}$, allows generalizing over approaches based on dual numbers (which cannot be modelled using $Cl_{p,q}$) and capturing patterns that emanate from the absence of higher-order interactions between real and complex parts of entity embeddings. We design two new models for the discovery of the parameters $p$, $q$, and $r$. The first model uses a greedy search to optimize $p$, $q$, and $r$. The second predicts $(p, q,r)$ based on an embedding of the input knowledge graph computed using neural networks. The results of our evaluation on seven benchmark datasets suggest that nilpotent vectors can help capture 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#29992;&#20110;&#22312;&#39640;&#28023;&#25300;&#29615;&#22659;&#20013;&#23545;&#38050;&#19997;&#32499;&#36827;&#34892;&#38750;&#30772;&#22351;&#24615;&#25439;&#20260;&#26816;&#27979;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#20934;&#30830;&#25552;&#21462;&#38050;&#19997;&#32499;&#30340;&#20998;&#21106;&#27169;&#22411;&#21644;&#19968;&#31181;&#21306;&#20998;&#27491;&#24120;&#21644;&#24322;&#24120;&#38050;&#19997;&#32499;&#30340;&#26816;&#27979;&#27169;&#22411;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#24615;&#33021;&#26174;&#33879;&#39640;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.03843</link><description>&lt;p&gt;
&#20809;&#23398;&#38050;&#19997;&#32499;&#38750;&#30772;&#22351;&#24615;&#25439;&#20260;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A new method for optical steel rope non-destructive damage detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#29992;&#20110;&#22312;&#39640;&#28023;&#25300;&#29615;&#22659;&#20013;&#23545;&#38050;&#19997;&#32499;&#36827;&#34892;&#38750;&#30772;&#22351;&#24615;&#25439;&#20260;&#26816;&#27979;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#20934;&#30830;&#25552;&#21462;&#38050;&#19997;&#32499;&#30340;&#20998;&#21106;&#27169;&#22411;&#21644;&#19968;&#31181;&#21306;&#20998;&#27491;&#24120;&#21644;&#24322;&#24120;&#38050;&#19997;&#32499;&#30340;&#26816;&#27979;&#27169;&#22411;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#24615;&#33021;&#26174;&#33879;&#39640;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#28023;&#25300;&#29615;&#22659;&#65288;&#31354;&#20013;&#21514;&#32034;&#36947;&#65289;&#20013;&#30340;&#38050;&#19997;&#32499;&#38750;&#30772;&#22351;&#24615;&#25439;&#20260;&#26816;&#27979;&#30340;&#26032;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#39318;&#20808;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;RGBD-UNet&#30340;&#20998;&#21106;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#20174;&#22797;&#26434;&#32972;&#26223;&#20013;&#25552;&#21462;&#38050;&#19997;&#32499;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#25552;&#20986;&#30340;CMA&#27169;&#22359;&#21487;&#20197;&#22788;&#29702;&#21644;&#32467;&#21512;&#39068;&#33394;&#21644;&#28145;&#24230;&#20449;&#24687;&#12290;&#20854;&#27425;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;VovNetV3.5&#30340;&#26816;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#21306;&#20998;&#27491;&#24120;&#21644;&#24322;&#24120;&#30340;&#38050;&#19997;&#32499;&#12290;&#23427;&#23558;VovNet&#26550;&#26500;&#19982;DBB&#27169;&#22359;&#32467;&#21512;&#36215;&#26469;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32972;&#26223;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#20998;&#21106;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#21019;&#24314;&#20102;&#21253;&#21547;&#19981;&#21516;&#22330;&#26223;&#20013;&#38050;&#19997;&#32499;&#22270;&#20687;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20998;&#21106;&#21644;&#26816;&#27979;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22522;&#20934;&#27169;&#22411;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#22312;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#22522;&#20110;&#27492;&#31639;&#27861;&#30340;&#20256;&#24863;&#22120;&#35782;&#21035;&#24615;&#33021;&#65288;h&#65289;&#26126;&#26174;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel algorithm for non-destructive damage detection for steel ropes in high-altitude environments (aerial ropeway). The algorithm comprises two key components: First, a segmentation model named RGBD-UNet is designed to accurately extract steel ropes from complex backgrounds. This model is equipped with the capability to process and combine color and depth information through the proposed CMA module. Second, a detection model named VovNetV3.5 is developed to differentiate between normal and abnormal steel ropes. It integrates the VovNet architecture with a DBB module to enhance performance. Besides, a novel background augmentation method is proposed to enhance the generalization ability of the segmentation model. Datasets containing images of steel ropes in different scenarios are created for the training and testing of both the segmentation and detection models. Experiments demonstrate a significant improvement over baseline models. On the proposed dataset, the h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26465;&#20214;&#21644;&#24773;&#24577;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#38500;&#20102;GPT-4&#22806;&#65292;&#20854;&#20182;&#27169;&#22411;&#22312;&#26465;&#20214;&#21477;&#26041;&#38754;&#23384;&#22312;&#22522;&#26412;&#38169;&#35823;&#65292;&#24182;&#19988;&#21363;&#20351;&#26159;GPT-4&#22312;&#28041;&#21450;&#35748;&#35782;&#24773;&#24577;&#30340;&#25512;&#29702;&#27169;&#24335;&#19978;&#20063;&#26174;&#31034;&#20986;&#36923;&#36753;&#19978;&#19981;&#19968;&#33268;&#30340;&#21028;&#26029;&#12290;</title><link>https://arxiv.org/abs/2401.17169</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26465;&#20214;&#21644;&#24773;&#24577;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Conditional and Modal Reasoning in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26465;&#20214;&#21644;&#24773;&#24577;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#38500;&#20102;GPT-4&#22806;&#65292;&#20854;&#20182;&#27169;&#22411;&#22312;&#26465;&#20214;&#21477;&#26041;&#38754;&#23384;&#22312;&#22522;&#26412;&#38169;&#35823;&#65292;&#24182;&#19988;&#21363;&#20351;&#26159;GPT-4&#22312;&#28041;&#21450;&#35748;&#35782;&#24773;&#24577;&#30340;&#25512;&#29702;&#27169;&#24335;&#19978;&#20063;&#26174;&#31034;&#20986;&#36923;&#36753;&#19978;&#19981;&#19968;&#33268;&#30340;&#21028;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#30740;&#31350;&#27491;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#35748;&#30693;&#31185;&#23398;&#39046;&#22495;&#19981;&#26029;&#22686;&#21152;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21313;&#20960;&#20010;LLM&#33021;&#21542;&#21306;&#20998;&#36923;&#36753;&#19978;&#27491;&#30830;&#30340;&#25512;&#35770;&#21644;&#36923;&#36753;&#19978;&#33618;&#35884;&#30340;&#25512;&#35770;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#28041;&#21450;&#26465;&#20214;&#21477;&#65288;&#20363;&#22914;&#65292;&#8220;&#22914;&#26524;&#23433;&#26377;&#19968;&#20010;&#30343;&#21518;&#65292;&#37027;&#20040;&#40077;&#21187;&#26377;&#19968;&#20010;J&#29260;&#8221;&#65289;&#21644;&#35748;&#35782;&#24773;&#24577;&#65288;&#20363;&#22914;&#65292;&#8220;&#23433;&#21487;&#33021;&#26377;&#19968;&#20010;A&#29260;&#8221;&#65292;&#8220;&#40077;&#21187;&#24517;&#39035;&#26377;&#19968;&#20010;K&#29260;&#8221;&#65289;&#30340;&#25512;&#29702;&#27169;&#24335;&#12290;&#36825;&#20123;&#25512;&#29702;&#27169;&#24335;&#23545;&#20110;&#36923;&#36753;&#23398;&#23478;&#12289;&#21746;&#23398;&#23478;&#21644;&#35821;&#35328;&#23398;&#23478;&#26469;&#35828;&#20855;&#26377;&#29305;&#27530;&#30340;&#20852;&#36259;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#22312;&#20154;&#31867;&#25512;&#29702;&#20013;&#25198;&#28436;&#19968;&#20010;&#26680;&#24515;&#35282;&#33394;&#12290;&#22240;&#27492;&#65292;&#35780;&#20272;LLM&#22312;&#36825;&#20123;&#25512;&#29702;&#27169;&#24335;&#19978;&#30340;&#34920;&#29616;&#19982;&#20154;&#31867;&#30340;&#25512;&#29702;&#33021;&#21147;&#26159;&#21542;&#30456;&#21305;&#37197;&#26159;&#38750;&#24120;&#30456;&#20851;&#30340;&#12290;&#22312;&#25105;&#20204;&#27979;&#35797;&#30340;LLM&#20013;&#65292;&#38500;&#20102;GPT-4&#65292;&#20854;&#20182;&#37117;&#24120;&#24120;&#22312;&#26465;&#20214;&#21477;&#26041;&#38754;&#29359;&#22522;&#26412;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#26159;GPT-4&#65292;&#22312;&#28041;&#21450;&#35748;&#35782;&#24773;&#24577;&#30340;&#25512;&#29702;&#27169;&#24335;&#19978;&#20063;&#26174;&#31034;&#20986;&#36923;&#36753;&#19978;&#19981;&#19968;&#33268;&#30340;&#21028;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
The reasoning abilities of large language models (LLMs) are the topic of a growing body of research in artificial intelligence and cognitive science. In this paper, we probe the extent to which a dozen LLMs are able to distinguish logically correct inferences from logically fallacious ones. We focus on inference patterns involving conditionals (e.g., 'If Ann has a queen, then Bob has a jack') and epistemic modals (e.g., 'Ann might have an ace', 'Bob must have a king'). These inference patterns have been of special interest to logicians, philosophers, and linguists, since they plausibly play a central role in human reasoning. Assessing LLMs on these inference patterns is thus highly relevant to the question of how much the reasoning abilities of LLMs match those of humans. Among the LLMs we tested, all but GPT-4 often make basic mistakes with conditionals. Moreover, even GPT-4 displays logically inconsistent judgments across inference patterns involving epistemic modals.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#21644;ML/DL&#31639;&#27861;&#20998;&#31867;&#27861;&#65292;&#26088;&#22312;&#27604;&#36739;&#21644;&#22522;&#20934;&#27979;&#35797;&#20854;&#22312;&#25216;&#26415;&#20538;&#21153;&#26816;&#27979;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2312.15020</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#26041;&#27861;&#26816;&#27979;&#33258;&#25105;&#25215;&#35748;&#30340;&#25216;&#26415;&#20538;&#21153;&#65306;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Automated Approaches to Detect Self-Admitted Technical Debt: A Systematic Literature Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15020
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#21644;ML/DL&#31639;&#27861;&#20998;&#31867;&#27861;&#65292;&#26088;&#22312;&#27604;&#36739;&#21644;&#22522;&#20934;&#27979;&#35797;&#20854;&#22312;&#25216;&#26415;&#20538;&#21153;&#26816;&#27979;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25216;&#26415;&#20538;&#21153;&#26159;&#36719;&#20214;&#24320;&#21457;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#36890;&#24120;&#28304;&#33258;&#24320;&#21457;&#36807;&#31243;&#20013;&#20570;&#20986;&#30340;&#26435;&#34913;&#65292;&#22312;&#24433;&#21709;&#36719;&#20214;&#21487;&#32500;&#25252;&#24615;&#21644;&#38459;&#30861;&#26410;&#26469;&#24320;&#21457;&#24037;&#20316;&#26041;&#38754;&#36215;&#21040;&#20316;&#29992;&#12290;&#33258;&#25105;&#25215;&#35748;&#30340;&#25216;&#26415;&#20538;&#21153;&#65288;SATD&#65289;&#25351;&#30340;&#26159;&#24320;&#21457;&#20154;&#21592;&#26126;&#30830;&#25215;&#35748;&#20195;&#30721;&#24211;&#20013;&#23384;&#22312;&#30340;&#20195;&#30721;&#36136;&#37327;&#25110;&#35774;&#35745;&#32570;&#38519;&#12290;&#33258;&#21160;&#26816;&#27979;SATD&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#39640;&#25928;&#22320;&#35782;&#21035;&#21644;&#35299;&#20915;&#25216;&#26415;&#20538;&#21153;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#24191;&#27867;&#37319;&#29992;&#30340;NLP&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#21644;&#31639;&#27861;&#31181;&#31867;&#22810;&#26679;&#21270;&#24120;&#24120;&#38459;&#30861;&#30740;&#31350;&#20154;&#21592;&#35797;&#22270;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#22522;&#20110;&#27492;&#65292;&#26412;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#21644;ML/DL&#31639;&#27861;&#20998;&#31867;&#27861;&#65292;&#20854;&#30446;&#30340;&#26159;&#27604;&#36739;&#21644;&#22522;&#20934;&#27979;&#35797;&#25152;&#32771;&#23519;&#30740;&#31350;&#20013;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36873;&#25321;......
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.15020v2 Announce Type: replace-cross  Abstract: Technical debt is a pervasive issue in software development, often arising from trade-offs made during development, which can impede software maintainability and hinder future development efforts. Self-admitted technical debt (SATD) refers to instances where developers explicitly acknowledge suboptimal code quality or design flaws in the codebase. Automated detection of SATD has emerged as a critical area of research, aiming to assist developers in identifying and addressing technical debt efficiently. However, the enormous variety of feature extraction approaches of NLP and algorithms employed in the literature often hinder researchers from trying to improve their performance. In light of this, this systematic literature review proposes a taxonomy of feature extraction techniques and ML/DL algorithms used in technical debt detection: its objective is to compare and benchmark their performance in the examined studies. We select
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;ChatEDA&#65292;&#19968;&#20010;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;AutoMage&#36171;&#33021;&#30340;EDA&#33258;&#20027;&#20195;&#29702;&#65292;&#36890;&#36807;&#26377;&#25928;&#31649;&#29702;&#20219;&#21153;&#35745;&#21010;&#12289;&#33050;&#26412;&#29983;&#25104;&#21644;&#20219;&#21153;&#25191;&#34892;&#65292;&#31616;&#21270;&#20102;&#20174;RTL&#21040;GDSII&#30340;&#35774;&#35745;&#27969;&#31243;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2308.10204</link><description>&lt;p&gt;
ChatEDA&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20195;&#29702;&#29992;&#20110;EDA
&lt;/p&gt;
&lt;p&gt;
ChatEDA: A Large Language Model Powered Autonomous Agent for EDA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.10204
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;ChatEDA&#65292;&#19968;&#20010;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;AutoMage&#36171;&#33021;&#30340;EDA&#33258;&#20027;&#20195;&#29702;&#65292;&#36890;&#36807;&#26377;&#25928;&#31649;&#29702;&#20219;&#21153;&#35745;&#21010;&#12289;&#33050;&#26412;&#29983;&#25104;&#21644;&#20219;&#21153;&#25191;&#34892;&#65292;&#31616;&#21270;&#20102;&#20174;RTL&#21040;GDSII&#30340;&#35774;&#35745;&#27969;&#31243;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2308.10204v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;-&#20132;&#21449;&#25688;&#35201;&#65306;&#38598;&#25104;&#19968;&#31995;&#21015;&#22797;&#26434;&#30340;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#65288;EDA&#65289;&#24037;&#20855;&#20197;&#22686;&#24378;&#20114;&#25805;&#20316;&#24615;&#26159;&#30005;&#36335;&#35774;&#35745;&#32773;&#20851;&#27880;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#29702;&#35299;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19982;EDA&#24037;&#20855;&#25509;&#21475;&#30340;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#35770;&#25991;&#20171;&#32461;&#20102;ChatEDA&#65292;&#19968;&#20010;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;AutoMage&#36171;&#33021;&#30340;EDA&#33258;&#20027;&#20195;&#29702;&#65292;&#32467;&#21512;&#20316;&#20026;&#25191;&#34892;&#22120;&#30340;EDA&#24037;&#20855;&#12290;ChatEDA&#36890;&#36807;&#26377;&#25928;&#31649;&#29702;&#20219;&#21153;&#35745;&#21010;&#12289;&#33050;&#26412;&#29983;&#25104;&#21644;&#20219;&#21153;&#25191;&#34892;&#65292;&#31616;&#21270;&#20102;&#20174;&#23492;&#23384;&#22120;&#20256;&#36755;&#32423;&#65288;RTL&#65289;&#21040;&#22270;&#24418;&#25968;&#25454;&#31995;&#32479;&#31532;&#20108;&#29256;&#65288;GDSII&#65289;&#30340;&#35774;&#35745;&#27969;&#31243;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;ChatEDA&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#22788;&#29702;&#21508;&#31181;&#38656;&#27714;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#32463;&#36807;&#31934;&#24515;&#35843;&#20248;&#30340;AutoMage&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#65292;&#30456;&#36739;&#20110;GPT-4&#21644;&#20854;&#20182;&#31867;&#20284;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.10204v2 Announce Type: replace-cross  Abstract: The integration of a complex set of Electronic Design Automation (EDA) tools to enhance interoperability is a critical concern for circuit designers. Recent advancements in large language models (LLMs) have showcased their exceptional capabilities in natural language processing and comprehension, offering a novel approach to interfacing with EDA tools. This research paper introduces ChatEDA, an autonomous agent for EDA empowered by a large language model, AutoMage, complemented by EDA tools serving as executors. ChatEDA streamlines the design flow from the Register-Transfer Level (RTL) to the Graphic Data System Version II (GDSII) by effectively managing task planning, script generation, and task execution. Through comprehensive experimental evaluations, ChatEDA has demonstrated its proficiency in handling diverse requirements, and our fine-tuned AutoMage model has exhibited superior performance compared to GPT-4 and other simi
&lt;/p&gt;</description></item><item><title>&#30001;&#20110;&#29992;&#25143;&#38656;&#27714;&#21644;&#26368;&#36817;&#30340;&#27861;&#35268;&#35201;&#27714;&#65292;&#38656;&#35201;&#23545;AI&#31995;&#32479;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#36827;&#34892;&#35299;&#37322;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Sum-Product Networks&#27169;&#25311;&#23547;&#25214;&#39640;&#21487;&#33021;&#24615;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#25552;&#20379;&#28385;&#36275;&#22810;&#20010;&#24120;&#35265;&#35201;&#27714;&#30340;&#26368;&#20339;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2401.14086</link><description>&lt;p&gt;
&#20351;&#29992;Sum-Product Networks&#29983;&#25104;&#21487;&#33021;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Generating Likely Counterfactuals Using Sum-Product Networks. (arXiv:2401.14086v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14086
&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29992;&#25143;&#38656;&#27714;&#21644;&#26368;&#36817;&#30340;&#27861;&#35268;&#35201;&#27714;&#65292;&#38656;&#35201;&#23545;AI&#31995;&#32479;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#36827;&#34892;&#35299;&#37322;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Sum-Product Networks&#27169;&#25311;&#23547;&#25214;&#39640;&#21487;&#33021;&#24615;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#25552;&#20379;&#28385;&#36275;&#22810;&#20010;&#24120;&#35265;&#35201;&#27714;&#30340;&#26368;&#20339;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29992;&#25143;&#38656;&#27714;&#21644;&#26368;&#36817;&#30340;&#27861;&#35268;&#65288;GDPR&#12289;AI&#27861;&#26696;&#65289;&#65292;&#38656;&#35201;&#35299;&#37322;AI&#31995;&#32479;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#12290;&#36825;&#20123;&#20915;&#31574;&#24448;&#24448;&#21482;&#33021;&#22312;&#20107;&#21518;&#35299;&#37322;&#65292;&#21453;&#20107;&#23454;&#25512;&#29702;&#25104;&#20026;&#24120;&#35265;&#30340;&#35299;&#37322;&#26041;&#24335;&#12290;&#20160;&#20040;&#26500;&#25104;&#20102;&#26368;&#20339;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#24517;&#39035;&#32771;&#34385;&#22810;&#20010;&#26041;&#38754;&#65292;&#20854;&#20013;&#8220;&#26679;&#26412;&#36317;&#31163;&#8221;&#26159;&#26368;&#24120;&#35265;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#19968;&#35201;&#27714;&#32463;&#24120;&#20250;&#23548;&#33268;&#19981;&#22826;&#21487;&#33021;&#19988;&#22240;&#27492;&#20215;&#20540;&#26377;&#38480;&#30340;&#35299;&#37322;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#25552;&#20379;&#39640;&#21487;&#33021;&#24615;&#35299;&#37322;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#65288;MIO&#65289;&#27169;&#25311;&#23547;&#25214;&#28385;&#36275;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#35768;&#22810;&#24120;&#35265;&#35201;&#27714;&#30340;&#26368;&#26377;&#21487;&#33021;&#35299;&#37322;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Sum-Product Network&#65288;SPN&#65289;&#30340;MIO&#34920;&#36798;&#65292;&#24182;&#20351;&#29992;SPN&#20272;&#35745;&#21453;&#20107;&#23454;&#30340;&#21487;&#33021;&#24615;&#65292;&#36825;&#23545;&#29420;&#31435;&#30340;&#20852;&#36259;&#20063;&#26377;&#29992;&#12290;&#19982;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#20960;&#31181;&#26041;&#27861;&#36827;&#34892;&#25968;&#20540;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to user demand and recent regulation (GDPR, AI Act), decisions made by AI systems need to be explained. These decisions are often explainable only post hoc, where counterfactual explanations are popular. The question of what constitutes the best counterfactual explanation must consider multiple aspects, where "distance from the sample" is the most common. We argue that this requirement frequently leads to explanations that are unlikely and, therefore, of limited value. Here, we present a system that provides high-likelihood explanations. We show that the search for the most likely explanations satisfying many common desiderata for counterfactual explanations can be modeled using mixed-integer optimization (MIO). In the process, we propose an MIO formulation of a Sum-Product Network (SPN) and use the SPN to estimate the likelihood of a counterfactual, which can be of independent interest. A numerical comparison against several methods for generating counterfactual explanations is pr
&lt;/p&gt;</description></item><item><title>RoTBench&#26159;&#19968;&#20010;&#22810;&#32423;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;LLMs&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22122;&#22768;&#19979;&#34920;&#29616;&#20986;&#30340;&#31283;&#23450;&#24615;&#38656;&#24471;&#21040;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2401.08326</link><description>&lt;p&gt;
RoTBench: &#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#30340;&#40065;&#26834;&#24615;&#30340;&#22810;&#32423;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning. (arXiv:2401.08326v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08326
&lt;/p&gt;
&lt;p&gt;
RoTBench&#26159;&#19968;&#20010;&#22810;&#32423;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;LLMs&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22122;&#22768;&#19979;&#34920;&#29616;&#20986;&#30340;&#31283;&#23450;&#24615;&#38656;&#24471;&#21040;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20855;&#23398;&#20064;&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#29289;&#29702;&#19990;&#30028;&#20043;&#38388;&#20114;&#21160;&#30340;&#37325;&#35201;&#25163;&#27573;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20852;&#36259;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#24378;&#35843;LLMs&#22312;&#32467;&#26500;&#33391;&#22909;&#30340;&#29615;&#22659;&#20013;&#21033;&#29992;&#24037;&#20855;&#30340;&#33021;&#21147;&#65292;&#20294;&#24573;&#35270;&#20102;&#23427;&#20204;&#22312;&#38754;&#23545;&#30495;&#23454;&#19990;&#30028;&#20013;&#19981;&#21487;&#36991;&#20813;&#30340;&#22122;&#22768;&#26102;&#30340;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;RoTBench&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#40065;&#26834;&#24615;&#30340;&#22810;&#32423;&#22522;&#20934;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20116;&#20010;&#22806;&#37096;&#29615;&#22659;&#65292;&#27599;&#20010;&#29615;&#22659;&#37117;&#20855;&#26377;&#19981;&#21516;&#32423;&#21035;&#30340;&#22122;&#22768;&#65288;&#21363;&#28165;&#27905;&#12289;&#36731;&#24494;&#12289;&#20013;&#31561;&#12289;&#37325;&#24230;&#21644;&#32852;&#21512;&#65289;&#65292;&#23545;&#27169;&#22411;&#22312;&#24037;&#20855;&#36873;&#25321;&#12289;&#21442;&#25968;&#35782;&#21035;&#21644;&#20869;&#23481;&#22635;&#20805;&#19977;&#20010;&#20851;&#38190;&#38454;&#27573;&#30340;&#25239;&#24178;&#25200;&#33021;&#21147;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#20845;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#27169;&#22411;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25552;&#39640;LLMs&#22312;&#24037;&#20855;&#23398;&#20064;&#20013;&#30340;&#40065;&#26834;&#24615;&#36843;&#22312;&#30473;&#30571;&#12290;&#20363;&#22914;&#65292;&#24403;&#27809;&#26377;&#23454;&#36136;&#24615;&#30340;&#22122;&#22768;&#23384;&#22312;&#26102;&#65292;GPT-4&#30340;&#24615;&#33021;&#29978;&#33267;&#20174;80.00&#19979;&#38477;&#21040;58.10&#12290;
&lt;/p&gt;
&lt;p&gt;
Tool learning has generated widespread interest as a vital means of interaction between Large Language Models (LLMs) and the physical world. Current research predominantly emphasizes LLMs' capacity to utilize tools in well-structured environments while overlooking their stability when confronted with the inevitable noise of the real world. To bridge this gap, we introduce RoTBench, a multi-level benchmark for evaluating the robustness of LLMs in tool learning. Specifically, we establish five external environments, each featuring varying levels of noise (i.e., Clean, Slight, Medium, Heavy, and Union), providing an in-depth analysis of the model's resilience across three critical phases: tool selection, parameter identification, and content filling. Experiments involving six widely-used models underscore the urgent necessity for enhancing the robustness of LLMs in tool learning. For instance, the performance of GPT-4 even drops significantly from 80.00 to 58.10 when there is no substanti
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26862;&#26519;&#20462;&#21098;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20860;&#39038;&#38543;&#26426;&#26862;&#26519;&#20934;&#30830;&#24615;&#21644;&#20915;&#31574;&#26641;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#26223;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#38543;&#26426;&#26862;&#26519;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05535</link><description>&lt;p&gt;
&#36890;&#36807;&#26862;&#26519;&#20462;&#21098;&#25552;&#39640;&#38543;&#26426;&#26862;&#26519;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving the Accuracy and Interpretability of Random Forests via Forest Pruning. (arXiv:2401.05535v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05535
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26862;&#26519;&#20462;&#21098;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20860;&#39038;&#38543;&#26426;&#26862;&#26519;&#20934;&#30830;&#24615;&#21644;&#20915;&#31574;&#26641;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#26223;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#38543;&#26426;&#26862;&#26519;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25509;&#36817;&#20960;&#21313;&#24180;&#30340;&#21457;&#23637;&#20043;&#21518;&#65292;&#38543;&#26426;&#26862;&#26519;&#20173;&#28982;&#22312;&#21508;&#31181;&#23398;&#20064;&#38382;&#39064;&#20013;&#25552;&#20379;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#36825;&#26041;&#38754;&#36229;&#36234;&#20102;&#20915;&#31574;&#26641;&#29978;&#33267;&#31070;&#32463;&#32593;&#32476;&#31561;&#26367;&#20195;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;&#65292;&#38543;&#26426;&#26862;&#26519;&#22312;&#35299;&#37322;&#24615;&#26041;&#38754;&#24448;&#24448;&#27604;&#20915;&#31574;&#26641;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20107;&#21518;&#26041;&#27861;&#65292;&#26088;&#22312;&#20860;&#39038;&#38543;&#26426;&#26862;&#26519;&#30340;&#20934;&#30830;&#24615;&#21644;&#20915;&#31574;&#26641;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26862;&#26519;&#20462;&#21098;&#26041;&#27861;&#65292;&#20197;&#22312;&#32473;&#23450;&#30340;&#38543;&#26426;&#26862;&#26519;&#20869;&#25214;&#21040;&#26368;&#20339;&#23376;&#26862;&#26519;&#65292;&#28982;&#21518;&#22312;&#36866;&#29992;&#30340;&#24773;&#20917;&#19979;&#23558;&#36873;&#23450;&#30340;&#26641;&#21512;&#24182;&#20026;&#19968;&#26869;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#31181;&#26041;&#27861;&#20381;&#36182;&#20110;&#32422;&#26463;&#31351;&#20030;&#25628;&#32034;&#65292;&#32780;&#31532;&#20108;&#31181;&#26041;&#27861;&#22522;&#20110;LASSO&#26041;&#27861;&#30340;&#25913;&#36827;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#26223;&#19979;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#20013;&#33267;&#23569;&#26377;&#19968;&#31181;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#38543;&#26426;&#26862;&#26519;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decades after their inception, random forests continue to provide state-of-the-art accuracy in a variety of learning problems, outperforming in this respect alternative machine learning algorithms such as decision trees or even neural networks. However, being an ensemble method, the one aspect where random forests tend to severely underperform decision trees is interpretability. In the present work, we propose a post-hoc approach that aims to have the best of both worlds: the accuracy of random forests and the interpretability of decision trees. To this end, we present two forest-pruning methods to find an optimal sub-forest within a given random forest, and then, when applicable, combine the selected trees into one. Our first method relies on constrained exhaustive search, while our second method is based on an adaptation of the LASSO methodology. Extensive experiments over synthetic and real world datasets show that, in the majority of scenarios, at least one of the two methods propo
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#37327;&#32479;&#19968;&#26694;&#26550;&#65288;IUF&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#24037;&#19994;&#21046;&#36896;&#20013;&#30340;&#23567;&#32570;&#38519;&#26816;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#23545;&#35937;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#65288;OASA&#65289;&#21644;&#35821;&#20041;&#21387;&#32553;&#25439;&#22833;&#65288;SCL&#65289;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#26029;&#25972;&#21512;&#26032;&#29289;&#20307;&#26102;&#20943;&#23569;&#29305;&#24449;&#20914;&#31361;&#38382;&#39064;&#65292;&#24182;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.08917</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#23567;&#32570;&#38519;&#26816;&#27979;&#30340;&#22686;&#37327;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Incremental Unified Framework for Small Defect Inspection. (arXiv:2312.08917v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.08917
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#37327;&#32479;&#19968;&#26694;&#26550;&#65288;IUF&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#24037;&#19994;&#21046;&#36896;&#20013;&#30340;&#23567;&#32570;&#38519;&#26816;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#23545;&#35937;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#65288;OASA&#65289;&#21644;&#35821;&#20041;&#21387;&#32553;&#25439;&#22833;&#65288;SCL&#65289;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#26029;&#25972;&#21512;&#26032;&#29289;&#20307;&#26102;&#20943;&#23569;&#29305;&#24449;&#20914;&#31361;&#38382;&#39064;&#65292;&#24182;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#21046;&#36896;&#20013;&#65292;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#32570;&#38519;&#26816;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26041;&#27861;&#38024;&#23545;&#29305;&#23450;&#30340;&#27969;&#27700;&#32447;&#65292;&#22312;&#38754;&#23545;&#21508;&#31181;&#20135;&#21697;&#32452;&#21512;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#27969;&#31243;&#26102;&#24448;&#24448;&#38754;&#20020;&#29305;&#24449;&#20914;&#31361;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#37327;&#32479;&#19968;&#26694;&#26550;&#65288;IUF&#65289;&#65292;&#21487;&#20197;&#22312;&#19981;&#26029;&#25972;&#21512;&#26032;&#29289;&#20307;&#26102;&#20943;&#23569;&#29305;&#24449;&#20914;&#31361;&#38382;&#39064;&#65292;&#36825;&#22312;&#38754;&#20020;&#29289;&#20307;&#22686;&#37327;&#23398;&#20064;&#22330;&#26223;&#20013;&#20855;&#26377;&#20248;&#21183;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;transformer&#24341;&#20837;&#20102;&#23545;&#35937;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#65288;OASA&#65289;&#26469;&#21010;&#20998;&#26126;&#30830;&#30340;&#35821;&#20041;&#36793;&#30028;&#12290;&#38598;&#25104;&#20102;&#35821;&#20041;&#21387;&#32553;&#25439;&#22833;&#65288;SCL&#65289;&#26469;&#20248;&#21270;&#38750;&#20027;&#35201;&#35821;&#20041;&#31354;&#38388;&#65292;&#22686;&#24378;&#20102;&#32593;&#32476;&#23545;&#26032;&#29289;&#20307;&#30340;&#36866;&#24212;&#24615;&#12290;&#27492;&#22806;&#65292;&#22312;&#26435;&#37325;&#26356;&#26032;&#26102;&#65292;&#25105;&#20204;&#20248;&#20808;&#20445;&#30041;&#24050;&#24314;&#31435;&#29289;&#20307;&#30340;&#29305;&#24449;&#12290;&#36890;&#36807;&#22312;&#22270;&#20687;&#21644;&#20687;&#32032;&#32423;&#32570;&#38519;&#26816;&#27979;&#26041;&#38754;&#23637;&#31034;&#20986;&#21331;&#36234;&#24615;&#33021;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#23545;&#20110;&#21160;&#24577;&#21644;&#21487;&#25193;&#23637;&#30340;&#24037;&#19994;&#26816;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI)-driven defect inspection is pivotal in industrial manufacturing. Yet, many methods, tailored to specific pipelines, grapple with diverse product portfolios and evolving processes. Addressing this, we present the Incremental Unified Framework (IUF), which can reduce the feature conflict problem when continuously integrating new objects in the pipeline, making it advantageous in object-incremental learning scenarios. Employing a state-of-the-art transformer, we introduce Object-Aware Self-Attention (OASA) to delineate distinct semantic boundaries. Semantic Compression Loss (SCL) is integrated to optimize non-primary semantic space, enhancing network adaptability for novel objects. Additionally, we prioritize retaining the features of established objects during weight updates. Demonstrating prowess in both image and pixel-level defect inspection, our approach achieves state-of-the-art performance, proving indispensable for dynamic and scalable industrial inspe
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26234;&#33021;&#32452;&#21512;&#22120;&#65292;&#19968;&#32676;&#24320;&#28304;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;&#25110;&#36229;&#36807;&#19987;&#26377;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.19902</link><description>&lt;p&gt;
Herd&#65306;&#36890;&#36807;&#26234;&#33021;&#32452;&#21512;&#22120;&#20351;&#29992;&#22810;&#20010;&#36739;&#23567;&#30340;LLM&#26469;&#19982;&#19987;&#26377;&#30340;&#22823;&#22411;LLM&#36798;&#21040;&#30456;&#21516;&#30340;&#24615;&#33021;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Herd: Using multiple, smaller LLMs to match the performances of proprietary, large LLMs via an intelligent composer. (arXiv:2310.19902v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19902
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26234;&#33021;&#32452;&#21512;&#22120;&#65292;&#19968;&#32676;&#24320;&#28304;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;&#25110;&#36229;&#36807;&#19987;&#26377;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#23384;&#22312;&#36229;&#36807;&#19968;&#21315;&#20010;&#22810;&#21151;&#33021;&#30340;LLM&#65292;&#21487;&#20197;&#25191;&#34892;&#23454;&#38469;&#20219;&#21153;&#65292;&#21253;&#25324;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#20869;&#23481;&#29983;&#25104;&#31561;&#12290;&#28982;&#32780;&#65292;&#20813;&#36153;&#27169;&#22411;&#30340;&#21487;&#35775;&#38382;&#24615;&#12289;&#35268;&#27169;&#21644;&#21487;&#38752;&#24615;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26085;&#24120;&#20351;&#29992;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#35775;&#38382;&#21644;&#35268;&#27169;&#30340;&#38382;&#39064;&#65292;&#20687;HuggingFace&#36825;&#26679;&#30340;&#32452;&#32455;&#24050;&#32463;&#21019;&#24314;&#20102;&#27169;&#22411;&#20179;&#24211;&#65292;&#29992;&#25143;&#21487;&#20197;&#19978;&#20256;&#24050;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#26435;&#37325;&#21644;&#37327;&#21270;&#29256;&#26412;&#65292;&#20197;&#21450;&#25551;&#36848;&#35757;&#32451;&#36807;&#31243;&#30340;&#27169;&#22411;&#21345;&#29255;&#12290;&#23613;&#31649;&#19968;&#20123;&#27169;&#22411;&#25253;&#21578;&#20102;&#24120;&#29992;&#22522;&#20934;&#27979;&#35797;&#30340;&#24615;&#33021;&#65292;&#20294;&#24182;&#38750;&#25152;&#26377;&#27169;&#22411;&#37117;&#22914;&#27492;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19982;&#27169;&#22411;&#37096;&#32626;&#25104;&#26412;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#30340;&#30495;&#23454;&#19990;&#30028;&#24433;&#21709;&#24182;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24320;&#28304;&#27169;&#22411;&#30340;&#32676;&#21487;&#20197;&#36890;&#36807;&#26234;&#33021;&#36335;&#30001;&#22120;&#36798;&#21040;&#25110;&#36229;&#36807;&#19987;&#26377;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#24320;&#28304;&#27169;&#22411;&#30340;&#32676;&#33021;&#22815;&#36798;&#21040;ChatGPT&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Currently, over a thousand LLMs exist that are multi-purpose and are capable of performing real world tasks, including Q&amp;A, text summarization, content generation, etc. However, accessibility, scale and reliability of free models prevents them from being widely deployed in everyday use cases. To address the first two issues of access and scale, organisations such as HuggingFace have created model repositories where users have uploaded model weights and quantized versions of models trained using different paradigms, as well as model cards describing their training process. While some models report performance on commonly used benchmarks, not all do, and interpreting the real world impact of trading off performance on a benchmark for model deployment cost, is unclear. Here, we show that a herd of open source models can match or exceed the performance of proprietary models via an intelligent router. We show that a Herd of open source models is able to match the accuracy of ChatGPT, despit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#21028;&#21035;&#22120;&#24341;&#23548;&#65292;&#29992;&#20110;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#36890;&#36807;&#20351;&#29992;&#26368;&#20248;&#21028;&#21035;&#22120;&#26469;&#32416;&#27491;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#26469;&#24212;&#23545;&#20351;&#29992;&#27425;&#20248;&#21028;&#21035;&#22120;&#30340;&#24773;&#20917;&#12290;&#22312;&#29983;&#25104;&#20998;&#23376;&#22270;&#30340;&#20219;&#21153;&#20013;&#65292;&#21028;&#21035;&#22120;&#24341;&#23548;&#26377;&#21161;&#20110;&#25552;&#39640;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.15817</link><description>&lt;p&gt;
&#21028;&#21035;&#22120;&#24341;&#23548;&#19979;&#30340;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Discriminator Guidance for Autoregressive Diffusion Models. (arXiv:2310.15817v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#21028;&#21035;&#22120;&#24341;&#23548;&#65292;&#29992;&#20110;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#36890;&#36807;&#20351;&#29992;&#26368;&#20248;&#21028;&#21035;&#22120;&#26469;&#32416;&#27491;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#26469;&#24212;&#23545;&#20351;&#29992;&#27425;&#20248;&#21028;&#21035;&#22120;&#30340;&#24773;&#20917;&#12290;&#22312;&#29983;&#25104;&#20998;&#23376;&#22270;&#30340;&#20219;&#21153;&#20013;&#65292;&#21028;&#21035;&#22120;&#24341;&#23548;&#26377;&#21161;&#20110;&#25552;&#39640;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#20013;&#24341;&#20837;&#20102;&#21028;&#21035;&#22120;&#24341;&#23548;&#12290;&#22312;&#36830;&#32493;&#25193;&#25955;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;&#21028;&#21035;&#22120;&#24341;&#23548;&#25193;&#25955;&#36807;&#31243;&#30340;&#26041;&#27861;&#24050;&#32463;&#34987;&#20351;&#29992;&#36807;&#65292;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#22312;&#31163;&#25955;&#24773;&#20917;&#19979;&#20351;&#29992;&#21028;&#21035;&#22120;&#21644;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20351;&#29992;&#26368;&#20248;&#21028;&#21035;&#22120;&#23558;&#32416;&#27491;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#33021;&#22815;&#20174;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#20013;&#31934;&#30830;&#37319;&#26679;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#24212;&#23545;&#20351;&#29992;&#27425;&#20248;&#21028;&#21035;&#22120;&#30340;&#23454;&#38469;&#24773;&#20917;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#20010;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#36845;&#20195;&#22320;&#23558;&#21028;&#21035;&#22120;&#30340;&#39044;&#27979;&#32435;&#20837;&#32771;&#34385;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#29983;&#25104;&#20998;&#23376;&#22270;&#30340;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20102;&#21028;&#21035;&#22120;&#30456;&#36739;&#20110;&#20165;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26102;&#30340;&#29983;&#25104;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce discriminator guidance in the setting of Autoregressive Diffusion Models. The use of a discriminator to guide a diffusion process has previously been used for continuous diffusion models, and in this work we derive ways of using a discriminator together with a pretrained generative model in the discrete case. First, we show that using an optimal discriminator will correct the pretrained model and enable exact sampling from the underlying data distribution. Second, to account for the realistic scenario of using a sub-optimal discriminator, we derive a sequential Monte Carlo algorithm which iteratively takes the predictions from the discrimiator into account during the generation process. We test these approaches on the task of generating molecular graphs and show how the discriminator improves the generative performance over using only the pretrained model.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#26469;&#21019;&#24314;&#21160;&#24577;&#21644;&#30495;&#23454;&#30340;&#36719;&#20214;&#34588;&#32592;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#34588;&#32592;&#30340;&#37325;&#35201;&#23616;&#38480;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#39640;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.00155</link><description>&lt;p&gt;
LLM&#22312;Shell&#20013;&#30340;&#24212;&#29992;&#65306;&#29983;&#25104;&#24335;&#34588;&#32592;
&lt;/p&gt;
&lt;p&gt;
LLM in the Shell: Generative Honeypots. (arXiv:2309.00155v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#26469;&#21019;&#24314;&#21160;&#24577;&#21644;&#30495;&#23454;&#30340;&#36719;&#20214;&#34588;&#32592;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#34588;&#32592;&#30340;&#37325;&#35201;&#23616;&#38480;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#39640;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34588;&#32592;&#26159;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#34588;&#32592;&#65288;&#21363;&#20351;&#26159;&#39640;&#20132;&#20114;&#24335;&#30340;&#65289;&#32570;&#20047;&#36275;&#22815;&#30340;&#30495;&#23454;&#24863;&#26469;&#27450;&#39575;&#25915;&#20987;&#32773;&#12290;&#36825;&#20010;&#38480;&#21046;&#20351;&#24471;&#23427;&#20204;&#24456;&#23481;&#26131;&#34987;&#35782;&#21035;&#65292;&#20174;&#32780;&#24433;&#21709;&#21040;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#26469;&#21019;&#24314;&#21160;&#24577;&#21644;&#30495;&#23454;&#30340;&#36719;&#20214;&#34588;&#32592;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#33021;&#22815;&#21019;&#24314;&#21487;&#20449;&#19988;&#21160;&#24577;&#30340;&#34588;&#32592;&#65292;&#33021;&#22815;&#35299;&#20915;&#20197;&#24448;&#34588;&#32592;&#30340;&#37325;&#35201;&#23616;&#38480;&#24615;&#65292;&#22914;&#30830;&#23450;&#24615;&#21709;&#24212;&#12289;&#32570;&#20047;&#36866;&#24212;&#24615;&#31561;&#12290;&#25105;&#20204;&#36890;&#36807;&#19982;&#38656;&#35201;&#21028;&#26029;&#34588;&#32592;&#22238;&#24212;&#26159;&#21542;&#34394;&#20551;&#30340;&#25915;&#20987;&#32773;&#36827;&#34892;&#23454;&#39564;&#26469;&#35780;&#20272;&#27599;&#20010;&#21629;&#20196;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#34588;&#32592;&#65292;&#31216;&#20026;shelLM&#65292;&#36798;&#21040;&#20102;0.92&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Honeypots are essential tools in cybersecurity. However, most of them (even the high-interaction ones) lack the required realism to engage and fool human attackers. This limitation makes them easily discernible, hindering their effectiveness. This work introduces a novel method to create dynamic and realistic software honeypots based on Large Language Models. Preliminary results indicate that LLMs can create credible and dynamic honeypots capable of addressing important limitations of previous honeypots, such as deterministic responses, lack of adaptability, etc. We evaluated the realism of each command by conducting an experiment with human attackers who needed to say if the answer from the honeypot was fake or not. Our proposed honeypot, called shelLM, reached an accuracy rate of 0.92.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HePCo&#30340;&#36731;&#37327;&#32423;&#25552;&#31034;&#21512;&#24182;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#36830;&#32493;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#24322;&#26500;&#21644;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#22312;&#19981;&#20849;&#20139;&#25110;&#23384;&#20648;&#20219;&#20309;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26368;&#23567;&#21270;&#20102;&#36890;&#20449;&#24320;&#38144;&#12290;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#20445;&#25345;&#20102;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2306.09970</link><description>&lt;p&gt;
HePCo&#65306;&#29992;&#20110;&#36830;&#32493;&#32852;&#37030;&#23398;&#20064;&#30340;&#26080;&#25968;&#25454;&#24322;&#26500;&#25552;&#31034;&#21512;&#24182;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HePCo: Data-Free Heterogeneous Prompt Consolidation for Continual Federated Learning. (arXiv:2306.09970v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HePCo&#30340;&#36731;&#37327;&#32423;&#25552;&#31034;&#21512;&#24182;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#36830;&#32493;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#24322;&#26500;&#21644;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#22312;&#19981;&#20849;&#20139;&#25110;&#23384;&#20648;&#20219;&#20309;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26368;&#23567;&#21270;&#20102;&#36890;&#20449;&#24320;&#38144;&#12290;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#20445;&#25345;&#20102;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#32852;&#37030;&#23398;&#20064;&#30340;&#37325;&#35201;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26381;&#21153;&#22120;&#19982;&#19968;&#32452;&#23458;&#25143;&#31471;&#36890;&#20449;&#65292;&#20197;&#36880;&#27493;&#23398;&#20064;&#26032;&#30340;&#27010;&#24565;&#65292;&#21516;&#26102;&#19981;&#20849;&#20139;&#25110;&#23384;&#20648;&#20219;&#20309;&#25968;&#25454;&#12290;&#30001;&#20110;&#26469;&#33258;&#36830;&#32493;&#21644;&#32852;&#37030;&#23398;&#20064;&#35282;&#24230;&#30340;&#25361;&#25112;&#65292;&#27492;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#21463;&#21040;&#20102;&#21152;&#21095;&#12290;&#26412;&#25991;&#23581;&#35797;&#22312;&#19981;&#38656;&#35201;&#35775;&#38382;&#20219;&#20309;&#23384;&#20648;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#36951;&#24536;&#21644;&#24322;&#26500;&#38382;&#39064;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#24320;&#38144;&#12290;&#25105;&#20204;&#36890;&#36807;&#37319;&#29992;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#24182;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;HePCo&#30340;&#26032;&#39062;&#36731;&#37327;&#32423;&#25552;&#31034;&#21512;&#24182;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#27492;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#22343;&#33021;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#24182;&#20445;&#25345;&#20302;&#36890;&#20449;&#24320;&#38144;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we focus on the important yet understudied problem of Continual Federated Learning (CFL), where a server communicates with a set of clients to incrementally learn new concepts over time without sharing or storing any data. The complexity of this problem is compounded by challenges from both the Continual and Federated Learning perspectives. Specifically, models trained in a CFL setup suffer from catastrophic forgetting which is exacerbated by data heterogeneity across clients. Existing attempts at this problem tend to impose large overheads on clients and communication channels or require access to stored data which renders them unsuitable for real-world use due to privacy. In this paper, we attempt to tackle forgetting and heterogeneity while minimizing overhead costs and without requiring access to any stored data. We achieve this by leveraging a prompting based approach (such that only prompts and classifier heads have to be communicated) and proposing a novel and lig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;GPT-3&#21644;GPT-4&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#21487;&#33021;&#20135;&#29983;&#30340;&#38169;&#35823;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.16326</link><description>&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;: &#22522;&#20934;&#12289;&#22522;&#32447;&#21644;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations. (arXiv:2305.16326v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;GPT-3&#21644;GPT-4&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#21487;&#33021;&#20135;&#29983;&#30340;&#38169;&#35823;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#25163;&#21160;&#31579;&#36873;&#21644;&#25552;&#21462;&#30693;&#35782;&#21464;&#24471;&#22256;&#38590;&#12290;&#33258;&#21160;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;BioNLP&#65289;&#25216;&#26415;&#26377;&#21161;&#20110;&#20943;&#36731;&#36825;&#31181;&#36127;&#25285;&#12290;&#36817;&#24180;&#26469;&#65292;&#22914;GPT-3&#21644;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#20854;&#21331;&#36234;&#30340;&#24615;&#33021;&#32780;&#21463;&#21040;&#37325;&#35270;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#22312;BioNLP&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#23545;&#26041;&#27861;&#24320;&#21457;&#21644;&#19979;&#28216;&#29992;&#25143;&#30340;&#24433;&#21709;&#20173;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#65288;1&#65289;&#22312;&#22235;&#20010;&#24212;&#29992;&#31243;&#24207;&#20013;&#22312;&#20843;&#20010;BioNLP&#25968;&#25454;&#38598;&#20013;&#24314;&#31435;&#20102;GPT-3&#21644;GPT-4&#22312;&#38646;-shot&#21644;&#19968;-shot&#35774;&#32622;&#19979;&#30340;&#22522;&#20934;&#34920;&#29616;&#65292;&#21253;&#25324;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65292;&#20851;&#31995;&#25552;&#21462;&#65292;&#22810;&#26631;&#31614;&#25991;&#26723;&#20998;&#31867;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#25512;&#29702;&#65307;&#65288;2&#65289;&#23457;&#26597;&#20102;LLMs&#20135;&#29983;&#30340;&#38169;&#35823;&#65292;&#24182;&#23558;&#38169;&#35823;&#20998;&#20026;&#19977;&#31181;&#31867;&#22411;&#65306;&#32570;&#22833;&#65292;&#19981;&#19968;&#33268;&#21644;&#19981;&#38656;&#35201;&#30340;&#20154;&#24037;&#20869;&#23481;&#65307;&#65288;3&#65289;&#25552;&#20986;&#20102;&#20351;&#29992;LLMs&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical literature is growing rapidly, making it challenging to curate and extract knowledge manually. Biomedical natural language processing (BioNLP) techniques that can automatically extract information from biomedical literature help alleviate this burden. Recently, large Language Models (LLMs), such as GPT-3 and GPT-4, have gained significant attention for their impressive performance. However, their effectiveness in BioNLP tasks and impact on method development and downstream users remain understudied. This pilot study (1) establishes the baseline performance of GPT-3 and GPT-4 at both zero-shot and one-shot settings in eight BioNLP datasets across four applications: named entity recognition, relation extraction, multi-label document classification, and semantic similarity and reasoning, (2) examines the errors produced by the LLMs and categorized the errors into three types: missingness, inconsistencies, and unwanted artificial content, and (3) provides suggestions for using L
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#37326;&#22806;&#35270;&#39057;&#20013;&#33258;&#21160;&#25552;&#21462;&#19977;&#32500;&#30417;&#30563;&#26469;&#25193;&#23637;&#25163;&#25345;&#29289;&#20307;&#37325;&#24314;&#27169;&#22411;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#25163;&#37096;&#23039;&#21183;&#20316;&#20026;&#29289;&#20307;&#23039;&#21183;&#30340;&#20195;&#29702;&#21644;&#23398;&#20064;&#25968;&#25454;&#39537;&#21160;&#30340;&#19977;&#32500;&#24418;&#29366;&#20808;&#39564;&#30693;&#35782;&#31561;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#26410;&#30693;&#30456;&#26426;&#23039;&#21183;&#21644;&#36974;&#25377;&#31561;&#38382;&#39064;&#65292;&#20174;&#32780;&#36890;&#36807;&#20174;&#21333;&#20010;RGB&#22270;&#20687;&#39044;&#27979;&#29289;&#20307;&#19977;&#32500;&#24418;&#29366;&#30340;&#21344;&#25454;&#32593;&#32476;&#24471;&#21040;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03036</link><description>&lt;p&gt;
&#20174;&#37326;&#22806;&#35270;&#39057;&#20013;&#23398;&#20064;&#25163;&#25345;&#29289;&#20307;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Learning Hand-Held Object Reconstruction from In-The-Wild Videos. (arXiv:2305.03036v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#37326;&#22806;&#35270;&#39057;&#20013;&#33258;&#21160;&#25552;&#21462;&#19977;&#32500;&#30417;&#30563;&#26469;&#25193;&#23637;&#25163;&#25345;&#29289;&#20307;&#37325;&#24314;&#27169;&#22411;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#25163;&#37096;&#23039;&#21183;&#20316;&#20026;&#29289;&#20307;&#23039;&#21183;&#30340;&#20195;&#29702;&#21644;&#23398;&#20064;&#25968;&#25454;&#39537;&#21160;&#30340;&#19977;&#32500;&#24418;&#29366;&#20808;&#39564;&#30693;&#35782;&#31561;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#26410;&#30693;&#30456;&#26426;&#23039;&#21183;&#21644;&#36974;&#25377;&#31561;&#38382;&#39064;&#65292;&#20174;&#32780;&#36890;&#36807;&#20174;&#21333;&#20010;RGB&#22270;&#20687;&#39044;&#27979;&#29289;&#20307;&#19977;&#32500;&#24418;&#29366;&#30340;&#21344;&#25454;&#32593;&#32476;&#24471;&#21040;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#21333;&#24433;&#20687;&#25163;&#25345;&#29289;&#20307;&#37325;&#24314;&#26041;&#27861;&#20381;&#36182;&#20110;&#38590;&#20197;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#35268;&#27169;&#21270;&#25910;&#38598;&#30340;&#30452;&#25509;3D&#24418;&#29366;&#30417;&#30563;&#65292;&#22240;&#27492;&#36825;&#20123;&#26041;&#27861;&#22312;&#37326;&#22806;&#29615;&#22659;&#19979;&#38754;&#23545;&#26032;&#39062;&#29289;&#20307;&#26102;&#38590;&#20197;&#25512;&#24191;&#12290;&#26412;&#25991;&#20174;&#29983;&#21160;&#30340;&#37326;&#22806;&#21407;&#22987;&#35270;&#39057;&#25968;&#25454;&#20013;&#33258;&#21160;&#25552;&#21462;&#19977;&#32500;&#30417;&#30563;&#65292;&#24182;&#36890;&#36807;&#22810;&#35270;&#35282;&#20108;&#32500;&#30417;&#30563;&#26469;&#25193;&#23637;&#25163;&#25345;&#29289;&#20307;&#37325;&#24314;&#27169;&#22411;&#30340;&#23398;&#20064;&#12290;&#36825;&#38656;&#35201;&#24212;&#23545;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#26410;&#30693;&#30340;&#30456;&#26426;&#23039;&#21183;&#21644;&#36974;&#25377;&#12290;&#23545;&#20110;&#21069;&#32773;&#65292;&#25105;&#20204;&#20351;&#29992;&#25163;&#37096;&#23039;&#21183;&#20316;&#20026;&#29289;&#20307;&#23039;&#21183;&#30340;&#20195;&#29702;&#12290;&#23545;&#20110;&#21518;&#32773;&#65292;&#25105;&#20204;&#20351;&#29992;ObMan&#25968;&#25454;&#38598;&#20013;&#21512;&#25104;&#30340;&#29289;&#20307;&#26469;&#23398;&#20064;&#25968;&#25454;&#39537;&#21160;&#30340;&#19977;&#32500;&#24418;&#29366;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#38388;&#25509;&#30340;&#19977;&#32500;&#32447;&#32034;&#26469;&#35757;&#32451;&#21344;&#25454;&#32593;&#32476;&#65292;&#20174;&#21333;&#20010;RGB&#22270;&#20687;&#39044;&#27979;&#29289;&#20307;&#30340;&#19977;&#32500;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior works for reconstructing hand-held objects from a single image rely on direct 3D shape supervision which is challenging to gather in real world at scale. Consequently, these approaches do not generalize well when presented with novel objects in in-the-wild settings. While 3D supervision is a major bottleneck, there is an abundance of in-the-wild raw video data showing hand-object interactions. In this paper, we automatically extract 3D supervision (via multiview 2D supervision) from such raw video data to scale up the learning of models for hand-held object reconstruction. This requires tackling two key challenges: unknown camera pose and occlusion. For the former, we use hand pose (predicted from existing techniques, e.g. FrankMocap) as a proxy for object pose. For the latter, we learn data-driven 3D shape priors using synthetic objects from the ObMan dataset. We use these indirect 3D cues to train occupancy networks that predict the 3D shape of objects from a single RGB image. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;MDP&#20013;&#22522;&#20110;&#27010;&#29575;&#20195;&#29702;&#25481;&#32447;&#30340;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;&#33021;&#22815;&#28040;&#38500;&#25481;&#32447;&#24773;&#20917;&#38656;&#35201;&#26522;&#20030;&#35745;&#31639;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#35745;&#31639;&#21518;&#25481;&#32447;&#31995;&#32479;&#30340;&#26368;&#20248;&#31574;&#30053;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2304.12458</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;MDP&#20013;&#22522;&#20110;&#27010;&#29575;&#20195;&#29702;&#25481;&#32447;&#30340;&#26080;&#27169;&#22411;&#23398;&#20064;&#21644;&#26368;&#20248;&#31574;&#30053;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Model-Free Learning and Optimal Policy Design in Multi-Agent MDPs Under Probabilistic Agent Dropout. (arXiv:2304.12458v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;MDP&#20013;&#22522;&#20110;&#27010;&#29575;&#20195;&#29702;&#25481;&#32447;&#30340;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;&#33021;&#22815;&#28040;&#38500;&#25481;&#32447;&#24773;&#20917;&#38656;&#35201;&#26522;&#20030;&#35745;&#31639;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#35745;&#31639;&#21518;&#25481;&#32447;&#31995;&#32479;&#30340;&#26368;&#20248;&#31574;&#30053;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#35813;&#36807;&#31243;&#21487;&#20197;&#32463;&#21382;&#20195;&#29702;&#25481;&#32447;&#65292;&#24182;&#22522;&#20110;&#23545;&#20110;&#31574;&#30053;&#30340;&#25511;&#21046;&#21644;&#39044;&#20195;&#29702;&#36807;&#31243;&#30340;&#37319;&#26679;&#26469;&#35745;&#31639;&#21518;&#25481;&#32447;&#31995;&#32479;&#30340;&#31574;&#30053;&#12290;&#25511;&#21046;&#22120;&#30340;&#30446;&#26631;&#26159;&#23547;&#25214;&#19968;&#20010;&#26368;&#20248;&#31574;&#30053;&#65292;&#20351;&#24471;&#22312;&#24050;&#30693;&#20195;&#29702;&#25481;&#20986;&#27010;&#29575;&#30340;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#26399;&#26395;&#31995;&#32479;&#30340;&#20215;&#20540;&#26368;&#22823;&#21270;&#12290;&#23545;&#20110;&#20219;&#20309;&#29305;&#23450;&#30340;&#25481;&#32447;&#24773;&#20917;&#19979;&#30340;&#26368;&#20248;&#31574;&#30053;&#26159;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#29305;&#20363;&#12290;&#23545;&#20110;&#20855;&#26377;&#29305;&#23450;&#36716;&#25442;&#29420;&#31435;&#24615;&#21644;&#22870;&#21169;&#21487;&#20998;&#24615;&#32467;&#26500;&#30340;MDPs&#65292;&#25105;&#20204;&#20551;&#35774;&#20174;&#31995;&#32479;&#20013;&#31227;&#38500;&#20195;&#29702;&#32452;&#25104;&#20102;&#19968;&#20010;&#26032;&#30340;MDP&#65292;&#30001;&#21097;&#20313;&#20195;&#29702;&#32452;&#25104;&#20855;&#26377;&#26032;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;MDP&#65292;&#36716;&#25442;&#21160;&#24577;&#28040;&#38500;&#24050;&#21024;&#38500;&#30340;&#20195;&#29702;&#65292;&#22870;&#21169;&#19982;&#24050;&#21024;&#38500;&#30340;&#20195;&#29702;&#26080;&#20851;&#12290;&#39318;&#20808;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36825;&#20123;&#20551;&#35774;&#19979;&#65292;&#23545;&#20110;&#39044;&#25481;&#20986;&#31995;&#32479;&#26399;&#26395;&#20540;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#21333;&#19968;&#30340;MDP&#26469;&#34920;&#31034;&#65307;&#36825;&#20010;&#8220;&#40065;&#26834;MDP&#8221;&#33021;&#22815;&#28040;&#38500;&#22312;&#35745;&#31639;&#26368;&#20248;&#31574;&#30053;&#26102;&#35201;&#35780;&#20272;&#25152;&#26377;$2^N$&#31181;&#20195;&#29702;&#25481;&#32447;&#24773;&#20917;&#30340;&#38656;&#35201;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#21644;&#37325;&#35201;&#24615;&#37319;&#26679;&#26469;&#23398;&#20064;&#40065;&#26834;MDP&#65292;&#20174;&#32780;&#33021;&#22815;&#35745;&#31639;&#21518;&#25481;&#32447;&#31995;&#32479;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;&#20223;&#30495;&#32467;&#26524;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work studies a multi-agent Markov decision process (MDP) that can undergo agent dropout and the computation of policies for the post-dropout system based on control and sampling of the pre-dropout system. The controller's objective is to find an optimal policy that maximizes the value of the expected system given a priori knowledge of the agents' dropout probabilities. Finding an optimal policy for any specific dropout realization is a special case of this problem. For MDPs with a certain transition independence and reward separability structure, we assume that removing agents from the system forms a new MDP comprised of the remaining agents with new state and action spaces, transition dynamics that marginalize the removed agents, and rewards that are independent of the removed agents. We first show that under these assumptions, the value of the expected post-dropout system can be represented by a single MDP; this "robust MDP" eliminates the need to evaluate all $2^N$ realizations
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#22270;&#23398;&#20064;&#30340;&#21457;&#23637;&#21382;&#31243;&#21644;&#24212;&#29992;&#22330;&#26223;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#34920;&#31034;&#23398;&#20064;&#22312;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#21270;&#23398;&#21644;&#29983;&#29289;&#31561;&#39046;&#22495;&#20013;&#30340;&#26174;&#33879;&#24615;&#33021;&#65292;&#21516;&#26102;&#25351;&#20986;&#20102;&#23545;&#20197;&#21069;&#26377;&#20215;&#20540;&#30340;&#24037;&#20316;&#36827;&#34892;&#35843;&#26597;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2212.08966</link><description>&lt;p&gt;
&#22270;&#23398;&#20064;&#21450;&#20854;&#24212;&#29992;&#65306;&#19968;&#31687;&#20840;&#38754;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Graph Learning and Its Applications: A Holistic Survey. (arXiv:2212.08966v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#22270;&#23398;&#20064;&#30340;&#21457;&#23637;&#21382;&#31243;&#21644;&#24212;&#29992;&#22330;&#26223;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#34920;&#31034;&#23398;&#20064;&#22312;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#21270;&#23398;&#21644;&#29983;&#29289;&#31561;&#39046;&#22495;&#20013;&#30340;&#26174;&#33879;&#24615;&#33021;&#65292;&#21516;&#26102;&#25351;&#20986;&#20102;&#23545;&#20197;&#21069;&#26377;&#20215;&#20540;&#30340;&#24037;&#20316;&#36827;&#34892;&#35843;&#26597;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides a comprehensive survey of the development and application scenarios of graph learning, with a focus on the remarkable performance of representation learning in various fields such as text, image, chemistry, and biology. It also points out the need to investigate previous valuable works.
&lt;/p&gt;
&lt;p&gt;
&#22270;&#23398;&#20064;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#39046;&#22495;&#65292;&#26088;&#22312;&#23398;&#20064;&#33410;&#28857;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#21644;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;&#36825;&#20123;&#20851;&#31995;&#20351;&#24471;&#22270;&#19982;&#20256;&#32479;&#30340;&#34920;&#26684;&#25968;&#25454;&#30456;&#27604;&#20855;&#26377;&#29420;&#29305;&#24615;&#65292;&#22240;&#20026;&#33410;&#28857;&#20381;&#36182;&#20110;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#65292;&#24182;&#21253;&#21547;&#20016;&#23500;&#30340;&#20449;&#24687;&#21487;&#20379;&#21033;&#29992;&#12290;&#38543;&#30528;&#34920;&#31034;&#23398;&#20064;&#30340;&#20986;&#29616;&#65292;&#22270;&#23398;&#20064;&#22312;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#21270;&#23398;&#21644;&#29983;&#29289;&#31561;&#21508;&#31181;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#30001;&#20110;&#20854;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#65292;&#22270;&#23398;&#20064;&#21560;&#24341;&#20102;&#23398;&#26415;&#30028;&#30340;&#22823;&#37327;&#20851;&#27880;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#35768;&#22810;&#24037;&#20316;&#25552;&#20986;&#20102;&#35299;&#20915;&#22270;&#23398;&#20064;&#20013;&#19981;&#21516;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20294;&#38656;&#35201;&#23545;&#20197;&#21069;&#26377;&#20215;&#20540;&#30340;&#24037;&#20316;&#36827;&#34892;&#35843;&#26597;&#12290;&#34429;&#28982;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24847;&#35782;&#21040;&#20102;&#36825;&#19968;&#29616;&#35937;&#65292;&#24182;&#22312;&#22270;&#23398;&#20064;&#26041;&#38754;&#23436;&#25104;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35843;&#26597;&#65292;&#20294;&#20182;&#20204;&#26410;&#33021;&#20197;&#26356;&#36830;&#36143;&#30340;&#26041;&#24335;&#36830;&#25509;&#30456;&#20851;&#30340;&#30446;&#26631;&#12289;&#26041;&#27861;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph learning is a prevalent domain that endeavors to learn the intricate relationships among nodes and the topological structure of graphs. These relationships endow graphs with uniqueness compared to conventional tabular data, as nodes rely on non-Euclidean space and encompass rich information to exploit. Over the years, graph learning has transcended from graph theory to graph data mining. With the advent of representation learning, it has attained remarkable performance in diverse scenarios, including text, image, chemistry, and biology. Owing to its extensive application prospects, graph learning attracts copious attention from the academic community. Despite numerous works proposed to tackle different problems in graph learning, there is a demand to survey previous valuable works. While some researchers have perceived this phenomenon and accomplished impressive surveys on graph learning, they failed to connect related objectives, methods, and applications in a more coherent way.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30456;&#23545;&#36807;&#24230;&#27867;&#21270;&#30340;&#35838;&#31243;&#23398;&#20064;&#65288;CURO&#65289;&#30340;&#26032;&#31639;&#27861;&#26469;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#30456;&#23545;&#36807;&#24230;&#27867;&#21270; (RO) &#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#35299;&#20915;&#23637;&#31034;&#24378;RO&#30340;&#21512;&#20316;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2212.02733</link><description>&lt;p&gt;
&#30456;&#23545;&#36807;&#24230;&#27867;&#21270;&#30340;&#35838;&#31243;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Curriculum Learning for Relative Overgeneralization. (arXiv:2212.02733v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30456;&#23545;&#36807;&#24230;&#27867;&#21270;&#30340;&#35838;&#31243;&#23398;&#20064;&#65288;CURO&#65289;&#30340;&#26032;&#31639;&#27861;&#26469;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#30456;&#23545;&#36807;&#24230;&#27867;&#21270; (RO) &#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#35299;&#20915;&#23637;&#31034;&#24378;RO&#30340;&#21512;&#20316;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064; (MARL) &#20013;&#65292;&#35768;&#22810;&#27969;&#34892;&#26041;&#27861;&#22914; VDN &#21644; QMIX&#65292;&#37117;&#23481;&#26131;&#21463;&#21040;&#30456;&#23545;&#36807;&#24230;&#27867;&#21270; (RO) &#36825;&#19968;&#20851;&#38190;&#24615;&#30340;&#22810;&#26234;&#33021;&#20307;&#30149;&#29702;&#30340;&#24433;&#21709;&#12290;&#24403;&#21512;&#20316;&#20219;&#21153;&#20013;&#26368;&#20339;&#32852;&#21512;&#34892;&#21160;&#30340;&#25928;&#29992;&#20302;&#20110;&#27425;&#20248;&#32852;&#21512;&#34892;&#21160;&#26102;&#65292;&#23601;&#20250;&#20986;&#29616;RO&#12290;RO&#21487;&#33021;&#23548;&#33268;&#26234;&#33021;&#20307;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;&#25110;&#26080;&#27861;&#35299;&#20915;&#38656;&#35201;&#26234;&#33021;&#20307;&#20043;&#38388;&#22312;&#32473;&#23450;&#26102;&#38388;&#27493;&#38271;&#20869;&#36827;&#34892;&#22823;&#37327;&#21327;&#35843;&#30340;&#21512;&#20316;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#22522;&#20110;&#20215;&#20540;&#30340;MARL&#31639;&#27861;&#65292;&#22914;QPLEX&#21644;WQMIX&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20811;&#26381;RO&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#20204;&#20173;&#28982;&#26080;&#27861;&#35299;&#20915;&#23637;&#31034;&#24378;RO&#30340;&#21512;&#20316;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#30456;&#23545;&#36807;&#24230;&#27867;&#21270;&#30340;&#35838;&#31243;&#23398;&#20064;&#65288;CURO&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#20811;&#26381;RO&#12290;&#22312;CURO&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24494;&#35843;&#30446;&#26631;&#20219;&#21153;&#30340;&#22870;&#21169;&#20989;&#25968;&#20197;&#29983;&#25104;&#36866;&#21512;&#24403;&#21069;&#33021;&#21147;&#30340;&#28304;&#20219;&#21153;&#26469;&#35299;&#20915;&#23637;&#31034;&#24378;RO&#30340;&#30446;&#26631;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In multi-agent reinforcement learning (MARL), many popular methods, such as VDN and QMIX, are susceptible to a critical multi-agent pathology known as relative overgeneralization (RO), which arises when the optimal joint action's utility falls below that of a sub-optimal joint action in cooperative tasks. RO can cause the agents to get stuck into local optima or fail to solve cooperative tasks that require significant coordination between agents within a given timestep. Recent value-based MARL algorithms such as QPLEX and WQMIX can overcome RO to some extent. However, our experimental results show that they can still fail to solve cooperative tasks that exhibit strong RO. In this work, we propose a novel approach called curriculum learning for relative overgeneralization (CURO) to better overcome RO. To solve a target task that exhibits strong RO, in CURO, we first fine-tune the reward function of the target task to generate source tasks that are tailored to the current ability of the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphMLP&#30340;&#22270;&#24418;&#22686;&#24378;&#30340;MLP&#24335;&#26550;&#26500;&#65292;&#23427;&#23558;&#22270;&#24418;&#32467;&#26500;&#32435;&#20837;MLP&#27169;&#22411;&#20013;&#65292;&#20197;&#28385;&#36275;3D&#20154;&#20307;&#23039;&#24577;&#30340;&#39046;&#22495;&#29305;&#23450;&#38656;&#27714;&#65292;&#21516;&#26102;&#20801;&#35768;&#23616;&#37096;&#21644;&#20840;&#23616;&#30340;&#31354;&#38388;&#20132;&#20114;&#20316;&#29992;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#36824;&#23558;GraphMLP&#28789;&#27963;&#39640;&#25928;&#22320;&#25193;&#23637;&#21040;&#35270;&#39057;&#39046;&#22495;&#65292;&#24182;&#25104;&#21151;&#22320;&#36827;&#34892;&#20102;&#26102;&#38388;&#21160;&#21147;&#23398;&#30340;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2206.06420</link><description>&lt;p&gt;
GraphMLP&#65306;&#19968;&#31181;&#29992;&#20110;3D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#30340;&#22270;&#24418;MLP&#24335;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
GraphMLP: A Graph MLP-Like Architecture for 3D Human Pose Estimation. (arXiv:2206.06420v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06420
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphMLP&#30340;&#22270;&#24418;&#22686;&#24378;&#30340;MLP&#24335;&#26550;&#26500;&#65292;&#23427;&#23558;&#22270;&#24418;&#32467;&#26500;&#32435;&#20837;MLP&#27169;&#22411;&#20013;&#65292;&#20197;&#28385;&#36275;3D&#20154;&#20307;&#23039;&#24577;&#30340;&#39046;&#22495;&#29305;&#23450;&#38656;&#27714;&#65292;&#21516;&#26102;&#20801;&#35768;&#23616;&#37096;&#21644;&#20840;&#23616;&#30340;&#31354;&#38388;&#20132;&#20114;&#20316;&#29992;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#36824;&#23558;GraphMLP&#28789;&#27963;&#39640;&#25928;&#22320;&#25193;&#23637;&#21040;&#35270;&#39057;&#39046;&#22495;&#65292;&#24182;&#25104;&#21151;&#22320;&#36827;&#34892;&#20102;&#26102;&#38388;&#21160;&#21147;&#23398;&#30340;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#27169;&#22411;&#24050;&#32463;&#23637;&#29616;&#20986;&#22312;&#27809;&#26377;&#33258;&#25105;&#27880;&#24847;&#21147;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#35270;&#35273;&#34920;&#31034;&#26041;&#38754;&#30340;&#31454;&#20105;&#24615;&#32467;&#26524;&#65292;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MLP&#27169;&#22411;&#24182;&#19981;&#25797;&#38271;&#25429;&#25417;&#23616;&#37096;&#32454;&#33410;&#65292;&#20063;&#32570;&#20047;&#26377;&#20851;&#20154;&#20307;&#26500;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#29992;&#20110;&#39592;&#39612;&#34920;&#31034;&#23398;&#20064;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22270;&#24418;&#22686;&#24378;&#30340;MLP&#24335;&#26550;&#26500;&#65292;&#31216;&#20026;GraphMLP&#65292;&#23427;&#32467;&#21512;&#20102;MLP&#21644;&#22270;&#24418;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#22312;&#20840;&#23616;-&#23616;&#37096;-&#22270;&#24418;&#32479;&#19968;&#26550;&#26500;&#20013;&#29992;&#20110;3D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#12290;GraphMLP&#23558;&#20154;&#20307;&#30340;&#22270;&#24418;&#32467;&#26500;&#32435;&#20837;MLP&#27169;&#22411;&#20013;&#65292;&#20197;&#28385;&#36275;3D&#20154;&#20307;&#23039;&#24577;&#30340;&#39046;&#22495;&#29305;&#23450;&#38656;&#27714;&#65292;&#21516;&#26102;&#20801;&#35768;&#23616;&#37096;&#21644;&#20840;&#23616;&#30340;&#31354;&#38388;&#20132;&#20114;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;GraphMLP&#28789;&#27963;&#39640;&#25928;&#22320;&#25193;&#23637;&#21040;&#35270;&#39057;&#39046;&#22495;&#65292;&#24182;&#23637;&#31034;&#20102;&#21487;&#20197;&#20197;&#21487;&#24573;&#30053;&#30340;&#35745;&#31639;&#20195;&#20215;&#26469;&#26377;&#25928;&#22320;&#24314;&#27169;&#22797;&#26434;&#30340;&#26102;&#38388;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern multi-layer perceptron (MLP) models have shown competitive results in learning visual representations without self-attention. However, existing MLP models are not good at capturing local details and lack prior knowledge of human body configurations, which limits their modeling power for skeletal representation learning. To address these issues, we propose a simple yet effective graph-reinforced MLP-Like architecture, named GraphMLP, that combines MLPs and graph convolutional networks (GCNs) in a global-local-graphical unified architecture for 3D human pose estimation. GraphMLP incorporates the graph structure of human bodies into an MLP model to meet the domain-specific demand of the 3D human pose, while allowing for both local and global spatial interactions. Furthermore, we propose to flexibly and efficiently extend the GraphMLP to the video domain and show that complex temporal dynamics can be effectively modeled in a simple way with negligible computational cost gains in the
&lt;/p&gt;</description></item></channel></rss>