<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#20351;&#29992;ProtoVerse&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#21407;&#22411;&#35774;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#26894;&#20307;&#39592;&#25240;&#30340;&#20998;&#31867;&#20915;&#31574;&#65292;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#21407;&#22411;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.02830</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#31867;&#21407;&#22411;&#25552;&#21319;&#26894;&#20307;&#39592;&#25240;&#20998;&#32423;&#30340;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Interpretability of Vertebrae Fracture Grading using Human-interpretable Prototypes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02830
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;ProtoVerse&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#21407;&#22411;&#35774;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#26894;&#20307;&#39592;&#25240;&#30340;&#20998;&#31867;&#20915;&#31574;&#65292;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#21407;&#22411;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26894;&#20307;&#39592;&#25240;&#20998;&#32423;&#20998;&#31867;&#39592;&#25240;&#20005;&#37325;&#31243;&#24230;&#65292;&#36825;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36817;&#24180;&#26469;&#21560;&#24341;&#20102;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#12290;&#23613;&#31649;DL&#36741;&#21161;&#21307;&#23398;&#35786;&#26029;&#31561;&#20851;&#38190;&#24212;&#29992;&#22330;&#26223;&#38656;&#35201;&#36879;&#26126;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#24037;&#20316;&#23581;&#35797;&#20351;&#36825;&#31181;&#27169;&#22411;&#20855;&#26377;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#35201;&#20040;&#20381;&#36182;&#20110;&#20107;&#21518;&#26041;&#27861;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#39069;&#22806;&#27880;&#37322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;-by-design&#26041;&#27861;ProtoVerse&#65292;&#20197;&#22312;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#26041;&#24335;&#20013;&#25214;&#21040;&#30456;&#20851;&#30340;&#26894;&#20307;&#39592;&#25240;&#23376;&#37096;&#20998;&#65288;&#21407;&#22411;&#65289;&#65292;&#21487;&#21487;&#38752;&#22320;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26679;&#24615;&#20419;&#36827;&#25439;&#22833;&#65292;&#20197;&#20943;&#36731;&#22312;&#20855;&#26377;&#22797;&#26434;&#35821;&#20041;&#30340;&#23567;&#25968;&#25454;&#38598;&#20013;&#21407;&#22411;&#37325;&#22797;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;VerSe'19&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#21407;&#22411;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02830v1 Announce Type: cross  Abstract: Vertebral fracture grading classifies the severity of vertebral fractures, which is a challenging task in medical imaging and has recently attracted Deep Learning (DL) models. Only a few works attempted to make such models human-interpretable despite the need for transparency and trustworthiness in critical use cases like DL-assisted medical diagnosis. Moreover, such models either rely on post-hoc methods or additional annotations. In this work, we propose a novel interpretable-by-design method, ProtoVerse, to find relevant sub-parts of vertebral fractures (prototypes) that reliably explain the model's decision in a human-understandable way. Specifically, we introduce a novel diversity-promoting loss to mitigate prototype repetitions in small datasets with intricate semantics. We have experimented with the VerSe'19 dataset and outperformed the existing prototype-based method. Further, our model provides superior interpretability agains
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#35757;&#32451;&#19968;&#20010;&#29992;&#20110;&#35268;&#21010;&#26410;&#26469;&#20889;&#20316;&#36807;&#31243;&#30340;&#27169;&#22359;&#65292;&#25193;&#23637;&#20102;&#25104;&#21151;&#30340;&#35821;&#35328;&#27169;&#22411;&#20844;&#24335;&#21040;&#26356;&#25277;&#35937;&#30340;&#35268;&#21010;&#20013;&#65292;&#25913;&#21892;&#20102;&#35821;&#35328;&#24314;&#27169;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#25991;&#26412;&#32467;&#26500;&#26041;&#38754;&#65292;&#21516;&#26102;&#26032;&#30340;&#35268;&#21010;&#27169;&#22359;&#21487;&#20197;&#22823;&#35268;&#27169;&#35757;&#32451;&#24182;&#36731;&#26494;&#19982;&#31038;&#21306;&#20849;&#20139;&#12290;</title><link>https://arxiv.org/abs/2404.00614</link><description>&lt;p&gt;
&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#23398;&#20064;&#35821;&#35328;&#24314;&#27169;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Learning to Plan for Language Modeling from Unlabeled Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00614
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#35757;&#32451;&#19968;&#20010;&#29992;&#20110;&#35268;&#21010;&#26410;&#26469;&#20889;&#20316;&#36807;&#31243;&#30340;&#27169;&#22359;&#65292;&#25193;&#23637;&#20102;&#25104;&#21151;&#30340;&#35821;&#35328;&#27169;&#22411;&#20844;&#24335;&#21040;&#26356;&#25277;&#35937;&#30340;&#35268;&#21010;&#20013;&#65292;&#25913;&#21892;&#20102;&#35821;&#35328;&#24314;&#27169;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#25991;&#26412;&#32467;&#26500;&#26041;&#38754;&#65292;&#21516;&#26102;&#26032;&#30340;&#35268;&#21010;&#27169;&#22359;&#21487;&#20197;&#22823;&#35268;&#27169;&#35757;&#32451;&#24182;&#36731;&#26494;&#19982;&#31038;&#21306;&#20849;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35757;&#32451;&#26469;&#39044;&#27979;&#26410;&#26631;&#35760;&#35821;&#26009;&#24211;&#20013;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23398;&#20250;&#25191;&#34892;&#35768;&#22810;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#26631;&#35760;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30446;&#26631;&#21487;&#20197;&#35828;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#38656;&#35201;&#35268;&#21010;&#30340;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#65292;&#27604;&#22914;&#20889;&#20316;&#19968;&#31687;&#36830;&#36143;&#30340;&#25991;&#31456;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#35757;&#32451;&#19968;&#20010;&#29992;&#20110;&#35268;&#21010;&#26410;&#26469;&#20889;&#20316;&#36807;&#31243;&#30340;&#27169;&#22359;&#12290;&#36890;&#36807;&#26681;&#25454;&#29983;&#25104;&#30340;&#28508;&#22312;&#35745;&#21010;&#36827;&#34892;&#26465;&#20214;&#21270;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#23558;&#25104;&#21151;&#30340;&#35821;&#35328;&#27169;&#22411;&#20844;&#24335;&#25193;&#23637;&#21040;&#26356;&#25277;&#35937;&#30340;&#35268;&#21010;&#20013;&#12290;&#23454;&#39564;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#25913;&#21892;&#20102;&#35821;&#35328;&#24314;&#27169;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#25991;&#26412;&#32467;&#26500;&#26041;&#38754;&#12290;&#30001;&#20110;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#30340;&#26159;&#26080;&#30417;&#30563;&#19988;&#22806;&#37096;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#21010;&#27169;&#22359;&#65292;&#22240;&#27492;&#26032;&#30340;&#35268;&#21010;&#27169;&#22359;&#21487;&#20197;&#22823;&#35268;&#27169;&#35757;&#32451;&#65292;&#24182;&#19988;&#33021;&#22815;&#36731;&#26494;&#22320;&#19982;&#31038;&#21306;&#20849;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00614v1 Announce Type: cross  Abstract: By training to predict the next token in an unlabeled corpus, large language models learn to perform many tasks without any labeled data. However, their next-token-prediction objective arguably limits their performance in scenarios that require planning, such as writing a coherent article. In this paper, we train a module for planning the future writing process via a self-supervised learning objective. By conditioning on generated latent plans, our model extends the successful language model formula to more abstract planning in an unsupervised way. Empirically, we demonstrate that our method improves language modeling performance in general, particularly with respect to the text structure. Because our framework uses a planner module that is unsupervised and external to the language model, new planner modules can be trained at large scale and easily be shared with the community.
&lt;/p&gt;</description></item><item><title>MaRDI&#24320;&#21457;&#20102;&#19968;&#20010;FAIR&#21644;&#21487;&#26426;&#22120;&#35299;&#37322;&#30340;&#27169;&#26495;&#65292;&#29992;&#20110;&#20840;&#38754;&#25991;&#26723;&#21270;&#24212;&#29992;&#25968;&#23398;&#20013;&#30340;&#24314;&#27169;-&#20223;&#30495;-&#20248;&#21270;&#24037;&#20316;&#27969;&#31243;&#65292;&#24182;&#23637;&#31034;&#20102;MaRDMO&#21644;MathModDB&#30693;&#35782;&#22270;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.17778</link><description>&lt;p&gt;
&#36808;&#21521;&#24212;&#29992;&#25968;&#23398;&#20013;&#24037;&#20316;&#27969;&#31243;&#21644;&#27169;&#22411;&#30340;FAIR&#25991;&#26723;&#21270;
&lt;/p&gt;
&lt;p&gt;
Towards a FAIR Documentation of Workflows and Models in Applied Mathematics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17778
&lt;/p&gt;
&lt;p&gt;
MaRDI&#24320;&#21457;&#20102;&#19968;&#20010;FAIR&#21644;&#21487;&#26426;&#22120;&#35299;&#37322;&#30340;&#27169;&#26495;&#65292;&#29992;&#20110;&#20840;&#38754;&#25991;&#26723;&#21270;&#24212;&#29992;&#25968;&#23398;&#20013;&#30340;&#24314;&#27169;-&#20223;&#30495;-&#20248;&#21270;&#24037;&#20316;&#27969;&#31243;&#65292;&#24182;&#23637;&#31034;&#20102;MaRDMO&#21644;MathModDB&#30693;&#35782;&#22270;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17778v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#24314;&#27169;-&#20223;&#30495;-&#20248;&#21270;&#24037;&#20316;&#27969;&#22312;&#24212;&#29992;&#25968;&#23398;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#25968;&#23398;&#30740;&#31350;&#25968;&#25454;&#20513;&#35758;MaRDI&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;FAIR&#21644;&#21487;&#26426;&#22120;&#35299;&#37322;&#30340;&#27169;&#26495;&#65292;&#23545;&#36825;&#20123;&#24037;&#20316;&#27969;&#30340;&#20840;&#38754;&#25991;&#26723;&#21270;&#20570;&#20986;&#20102;&#22238;&#24212;&#12290;&#30740;&#31350;&#25968;&#25454;&#31649;&#29702;&#32452;&#32455;&#32773;&#30340;&#25554;&#20214;MaRDMO&#20351;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#31185;&#23398;&#23478;&#21487;&#20197;&#21033;&#29992;MaRDI&#27169;&#26495;&#22312;MaRDI&#38376;&#25143;&#19978;&#26080;&#32541;&#22320;&#35760;&#24405;&#21644;&#21457;&#24067;&#20182;&#20204;&#30340;&#24037;&#20316;&#27969;&#12290;&#36825;&#20123;&#24037;&#20316;&#27969;&#30340;&#26680;&#24515;&#26159;&#25968;&#23398;&#27169;&#22411;&#12290;MaRDI&#36890;&#36807;MathModDB&#26412;&#20307;&#35770;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#24418;&#24335;&#27169;&#22411;&#25551;&#36848;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26469;&#33258;&#25968;&#23383;&#20154;&#25991;&#23398;&#31185;&#30340;&#20195;&#25968;&#24314;&#27169;&#24037;&#20316;&#27969;&#31243;&#20013;MaRDMO&#21644;MathModDB&#30693;&#35782;&#22270;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;&#36825;&#20010;&#28436;&#31034;&#31361;&#26174;&#20102;&#36825;&#20004;&#39033;&#26381;&#21153;&#22312;&#21407;&#22987;&#25968;&#20540;&#39046;&#22495;&#20043;&#22806;&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17778v1 Announce Type: new  Abstract: Modeling-Simulation-Optimization workflows play a fundamental role in applied mathematics. The Mathematical Research Data Initiative, MaRDI, responded to this by developing a FAIR and machine-interpretable template for a comprehensive documentation of such workflows. MaRDMO, a Plugin for the Research Data Management Organiser, enables scientists from diverse fields to document and publish their workflows on the MaRDI Portal seamlessly using the MaRDI template. Central to these workflows are mathematical models. MaRDI addresses them with the MathModDB ontology, offering a structured formal model description. Here, we showcase the interaction between MaRDMO and the MathModDB Knowledge Graph through an algebraic modeling workflow from the Digital Humanities. This demonstration underscores the versatility of both services beyond their original numerical domain.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#22122;&#22768;&#27700;&#24179;&#33258;&#36866;&#24212;&#29305;&#24615;&#30340;&#21518;&#39564;&#37319;&#26679;&#31574;&#30053;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;MRI&#37325;&#24314;&#36807;&#31243;&#20013;&#22240;&#30495;&#23454;&#22122;&#22768;&#27700;&#24179;&#21464;&#21270;&#23548;&#33268;&#30340;&#37325;&#24314;&#19981;&#20934;&#30830;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.05245</link><description>&lt;p&gt;
&#22122;&#22768;&#27700;&#24179;&#33258;&#36866;&#24212;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#21152;&#36895;MRI&#30340;&#31283;&#20581;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Noise Level Adaptive Diffusion Model for Robust Reconstruction of Accelerated MRI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05245
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#22122;&#22768;&#27700;&#24179;&#33258;&#36866;&#24212;&#29305;&#24615;&#30340;&#21518;&#39564;&#37319;&#26679;&#31574;&#30053;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;MRI&#37325;&#24314;&#36807;&#31243;&#20013;&#22240;&#30495;&#23454;&#22122;&#22768;&#27700;&#24179;&#21464;&#21270;&#23548;&#33268;&#30340;&#37325;&#24314;&#19981;&#20934;&#30830;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#65292;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;MRI&#37325;&#24314;&#26041;&#27861;&#20250;&#36880;&#27493;&#21435;&#38500;&#20154;&#20026;&#28155;&#21152;&#30340;&#22122;&#22768;&#65292;&#24182;&#24378;&#35843;&#25968;&#25454;&#19968;&#33268;&#24615;&#20197;&#37325;&#24314;&#28508;&#22312;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;MRI&#37319;&#38598;&#24050;&#32463;&#21253;&#21547;&#30001;&#28909;&#28072;&#33853;&#24341;&#36215;&#30340;&#22266;&#26377;&#22122;&#22768;&#12290;&#20351;&#29992;&#36229;&#24555;&#36895;&#12289;&#39640;&#20998;&#36776;&#29575;&#25104;&#20687;&#24207;&#21015;&#36827;&#34892;&#39640;&#32423;&#30740;&#31350;&#65292;&#25110;&#32773;&#20351;&#29992;&#20302;&#22330;&#31995;&#32479;&#65288;&#21463;&#20302;&#25910;&#20837;&#21644;&#20013;&#31561;&#25910;&#20837;&#22269;&#23478;&#38738;&#30544;&#65289;&#26102;&#65292;&#36825;&#31181;&#29616;&#35937;&#23588;&#20854;&#26126;&#26174;&#12290;&#36825;&#20123;&#24120;&#35265;&#22330;&#26223;&#21487;&#33021;&#23548;&#33268;&#29616;&#26377;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#37325;&#24314;&#25216;&#26415;&#24615;&#33021;&#20122;&#20248;&#25110;&#23436;&#20840;&#22833;&#36133;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#38543;&#30528;&#36880;&#28176;&#21435;&#38500;&#20154;&#20026;&#28155;&#21152;&#30340;&#22122;&#22768;&#65292;&#22266;&#26377;&#30340;MRI&#22122;&#22768;&#21464;&#24471;&#36234;&#26469;&#36234;&#26126;&#26174;&#65292;&#20351;&#23454;&#38469;&#22122;&#22768;&#27700;&#24179;&#19982;&#39044;&#23450;&#20041;&#21435;&#22122;&#26102;&#38388;&#34920;&#19981;&#19968;&#33268;&#65292;&#20174;&#32780;&#23548;&#33268;&#22270;&#20687;&#37325;&#24314;&#19981;&#20934;&#30830;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26032;&#39062;&#22122;&#22768;&#27700;&#24179;&#33258;&#36866;&#24212;&#29305;&#24615;&#30340;&#21518;&#39564;&#37319;&#26679;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05245v1 Announce Type: cross  Abstract: In general, diffusion model-based MRI reconstruction methods incrementally remove artificially added noise while imposing data consistency to reconstruct the underlying images. However, real-world MRI acquisitions already contain inherent noise due to thermal fluctuations. This phenomenon is particularly notable when using ultra-fast, high-resolution imaging sequences for advanced research, or using low-field systems favored by low- and middle-income countries. These common scenarios can lead to sub-optimal performance or complete failure of existing diffusion model-based reconstruction techniques. Specifically, as the artificially added noise is gradually removed, the inherent MRI noise becomes increasingly pronounced, making the actual noise level inconsistent with the predefined denoising schedule and consequently inaccurate image reconstruction. To tackle this problem, we propose a posterior sampling strategy with a novel NoIse Lev
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25112;&#20105;&#28216;&#25103;&#20013;&#19982;&#20154;&#31867;&#21709;&#24212;&#23384;&#22312;&#19968;&#33268;&#24615;&#65292;&#20294;&#20063;&#23384;&#22312;&#26174;&#33879;&#30340;&#24046;&#24322;&#65292;&#36825;&#34920;&#26126;&#22312;&#25919;&#31574;&#21046;&#23450;&#32773;&#20132;&#20986;&#33258;&#20027;&#26435;&#25110;&#21548;&#20174;&#22522;&#20110;AI&#30340;&#25112;&#30053;&#24314;&#35758;&#20043;&#21069;&#24212;&#35880;&#24910;&#23545;&#24453;&#12290;</title><link>https://arxiv.org/abs/2403.03407</link><description>&lt;p&gt;
&#20154;&#31867;&#23545;&#25239;&#26426;&#22120;&#65306;&#35821;&#35328;&#27169;&#22411;&#19982;&#25112;&#20105;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Human vs. Machine: Language Models and Wargames
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03407
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25112;&#20105;&#28216;&#25103;&#20013;&#19982;&#20154;&#31867;&#21709;&#24212;&#23384;&#22312;&#19968;&#33268;&#24615;&#65292;&#20294;&#20063;&#23384;&#22312;&#26174;&#33879;&#30340;&#24046;&#24322;&#65292;&#36825;&#34920;&#26126;&#22312;&#25919;&#31574;&#21046;&#23450;&#32773;&#20132;&#20986;&#33258;&#20027;&#26435;&#25110;&#21548;&#20174;&#22522;&#20110;AI&#30340;&#25112;&#30053;&#24314;&#35758;&#20043;&#21069;&#24212;&#35880;&#24910;&#23545;&#24453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25112;&#20105;&#28216;&#25103;&#22312;&#20891;&#20107;&#25112;&#30053;&#30340;&#21457;&#23637;&#21644;&#22269;&#23478;&#23545;&#23041;&#32961;&#25110;&#25915;&#20987;&#30340;&#21709;&#24212;&#20013;&#26377;&#30528;&#24736;&#20037;&#30340;&#21382;&#21490;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#20986;&#29616;&#25215;&#35834;&#20102;&#26356;&#22909;&#30340;&#20915;&#31574;&#21046;&#23450;&#21644;&#22686;&#24378;&#30340;&#20891;&#20107;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;AI&#31995;&#32479;&#65292;&#23588;&#20854;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#19982;&#20154;&#31867;&#30340;&#34892;&#20026;&#26377;&#20309;&#19981;&#21516;&#20173;&#23384;&#22312;&#20105;&#35758;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#25112;&#20105;&#28216;&#25103;&#23454;&#39564;&#65292;&#20849;&#26377;107&#20301;&#22269;&#23478;&#23433;&#20840;&#19987;&#23478;&#20154;&#31867;&#21442;&#19982;&#32773;&#21442;&#19982;&#65292;&#26088;&#22312;&#30740;&#31350;&#22312;&#19968;&#20010;&#34394;&#26500;&#30340;&#32654;&#20013;&#24773;&#26223;&#20013;&#30340;&#21361;&#26426;&#21319;&#32423;&#65292;&#24182;&#27604;&#36739;&#20154;&#31867;&#21442;&#19982;&#32773;&#19982;LLM&#27169;&#25311;&#21709;&#24212;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#21457;&#29616;LLM&#21644;&#20154;&#31867;&#21709;&#24212;&#23384;&#22312;&#26174;&#33879;&#19968;&#33268;&#24615;&#65292;&#20294;&#22312;&#25112;&#20105;&#28216;&#25103;&#20013;&#27169;&#25311;&#21644;&#20154;&#31867;&#21442;&#19982;&#32773;&#20043;&#38388;&#20063;&#23384;&#22312;&#26174;&#33879;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#24046;&#24322;&#65292;&#36825;&#20419;&#20351;&#20915;&#31574;&#32773;&#22312;&#20132;&#20986;&#33258;&#20027;&#26435;&#25110;&#36981;&#24490;&#22522;&#20110;AI&#30340;&#25112;&#30053;&#24314;&#35758;&#20043;&#21069;&#35880;&#24910;&#23545;&#24453;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03407v1 Announce Type: cross  Abstract: Wargames have a long history in the development of military strategy and the response of nations to threats or attacks. The advent of artificial intelligence (AI) promises better decision-making and increased military effectiveness. However, there is still debate about how AI systems, especially large language models (LLMs), behave as compared to humans. To this end, we use a wargame experiment with 107 national security expert human players designed to look at crisis escalation in a fictional US-China scenario and compare human players to LLM-simulated responses. We find considerable agreement in the LLM and human responses but also significant quantitative and qualitative differences between simulated and human players in the wargame, motivating caution to policymakers before handing over autonomy or following AI-based strategy recommendations.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#19981;&#20462;&#25913;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#65292;&#36890;&#36807;&#36827;&#21270;&#20195;&#29702;&#30340;&#21151;&#33021;&#26469;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;</title><link>https://arxiv.org/abs/2402.11359</link><description>&lt;p&gt;
&#22312;&#19981;&#20462;&#25913;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Training Language Model Agents without Modifying Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11359
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#19981;&#20462;&#25913;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#65292;&#36890;&#36807;&#36827;&#21270;&#20195;&#29702;&#30340;&#21151;&#33021;&#26469;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#26368;&#36817;&#24050;&#32463;&#23558;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#37325;&#26032;&#23450;&#20041;&#20026;&#20195;&#29702;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#36890;&#36807;&#20351;&#29992;&#19987;&#38376;&#30340;&#21151;&#33021;&#33258;&#21160;&#21270;&#22320;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#12290;&#20026;&#20102;&#20419;&#36827;LLM&#20195;&#29702;&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#20462;&#25913;LLM&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;LLM&#20195;&#29702;&#30340;&#26032;&#33539;&#24335;&#65292;&#24403;LLM&#38590;&#20197;&#25110;&#26080;&#27861;&#36827;&#34892;&#20462;&#25913;&#26102;&#23588;&#20854;&#26377;&#29992;&#12290;&#21463;&#21040;&#20154;&#31867;&#19981;&#26029;&#38203;&#36896;&#24037;&#20855;&#20197;&#36866;&#24212;&#29616;&#23454;&#20219;&#21153;&#30340;&#21551;&#21457;&#65292;&#32780;&#19981;&#26159;&#25913;&#21464;&#25105;&#20204;&#30340;&#29983;&#29289;&#32467;&#26500;&#20197;&#36866;&#24212;&#19968;&#32452;&#38745;&#24577;&#24037;&#20855;&#65292;&#25105;&#20204;&#25552;&#20986;&#36880;&#27493;&#38203;&#36896;&#20195;&#29702;&#30340;&#21151;&#33021;&#65292;&#20197;&#26356;&#22909;&#22320;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#20462;&#25913;LLM&#26435;&#37325;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#21151;&#33021;&#35270;&#20026;&#21487;&#23398;&#20064;&#30340;&#8220;&#20195;&#29702;&#21442;&#25968;&#8221;&#24182;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#35757;&#32451;&#30340;&#22522;&#26412;&#24605;&#24819;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;AgentOptimizer&#65292;&#21033;&#29992;LLM&#26356;&#26032;&#20195;&#29702;&#30340;&#21151;&#33021;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#20195;&#29702;&#35757;&#32451;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11359v1 Announce Type: new  Abstract: Researchers and practitioners have recently reframed powerful Large Language Models (LLMs) as agents, enabling them to automate complex tasks largely via the use of specialized functions. To facilitate the development of LLM agents, we present a novel paradigm of training LLM agents without modifying the LLM weights, which is particularly useful when the LLMs are difficult or inaccessible for modifications. Inspired by how humans continuously forge tools to adapt to real-world tasks, rather than change our biological structure to fit a static set of tools, we propose to progressively forge agent's functions to better solve the downstream tasks instead of modifying the LLM weights. By treating the functions as learnable `agent parameters' and leveraging the fundamental idea of model training in artificial intelligence, we develop AgentOptimizer that employs the LLM to update agents' functions and devise an agent training algorithm with tw
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26377;&#38480;&#20869;&#23384;&#26465;&#20214;&#19979;&#39640;&#25928;&#36816;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#21442;&#25968;&#23384;&#20648;&#22312;&#38378;&#23384;&#20013;&#24182;&#25353;&#38656;&#20256;&#36755;&#21040;DRAM&#30340;&#26041;&#24335;&#26469;&#35299;&#20915;&#20869;&#23384;&#38480;&#21046;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26500;&#24314;&#25512;&#29702;&#25104;&#26412;&#27169;&#22411;&#24182;&#20248;&#21270;&#25968;&#25454;&#20256;&#36755;&#21644;&#35835;&#21462;&#26041;&#24335;&#65292;&#24341;&#20837;&#20102;&#31383;&#21475;&#21270;&#21644;&#34892;&#21015;&#32465;&#23450;&#20004;&#31181;&#20027;&#35201;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2312.11514</link><description>&lt;p&gt;
&#38378;&#23384;LLM&#65306;&#22312;&#26377;&#38480;&#20869;&#23384;&#19979;&#39640;&#25928;&#36816;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLM in a flash: Efficient Large Language Model Inference with Limited Memory. (arXiv:2312.11514v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26377;&#38480;&#20869;&#23384;&#26465;&#20214;&#19979;&#39640;&#25928;&#36816;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#21442;&#25968;&#23384;&#20648;&#22312;&#38378;&#23384;&#20013;&#24182;&#25353;&#38656;&#20256;&#36755;&#21040;DRAM&#30340;&#26041;&#24335;&#26469;&#35299;&#20915;&#20869;&#23384;&#38480;&#21046;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26500;&#24314;&#25512;&#29702;&#25104;&#26412;&#27169;&#22411;&#24182;&#20248;&#21270;&#25968;&#25454;&#20256;&#36755;&#21644;&#35835;&#21462;&#26041;&#24335;&#65292;&#24341;&#20837;&#20102;&#31383;&#21475;&#21270;&#21644;&#34892;&#21015;&#32465;&#23450;&#20004;&#31181;&#20027;&#35201;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24222;&#22823;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20855;&#26377;&#26377;&#38480;DRAM&#23481;&#37327;&#30340;&#35774;&#22791;&#32780;&#35328;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#27169;&#22411;&#21442;&#25968;&#23384;&#20648;&#22312;&#38378;&#23384;&#20013;&#65292;&#24182;&#25353;&#38656;&#23558;&#20854;&#20256;&#36755;&#21040;DRAM&#30340;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#36229;&#36807;&#21487;&#29992;DRAM&#23481;&#37327;&#30340;LLM&#39640;&#25928;&#36816;&#34892;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#26500;&#24314;&#19968;&#20010;&#32771;&#34385;&#38378;&#23384;&#29305;&#24615;&#30340;&#25512;&#29702;&#25104;&#26412;&#27169;&#22411;&#65292;&#24341;&#23548;&#25105;&#20204;&#22312;&#20004;&#20010;&#20851;&#38190;&#39046;&#22495;&#36827;&#34892;&#20248;&#21270;&#65306;&#20943;&#23569;&#20174;&#38378;&#23384;&#20256;&#36755;&#30340;&#25968;&#25454;&#37327;&#65292;&#24182;&#20197;&#36739;&#22823;&#12289;&#26356;&#36830;&#32493;&#30340;&#22359;&#35835;&#21462;&#25968;&#25454;&#12290;&#22312;&#36825;&#20010;&#21463;&#30828;&#20214;&#21551;&#21457;&#30340;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#20027;&#35201;&#25216;&#26415;&#12290;&#39318;&#20808;&#65292;&#8220;&#31383;&#21475;&#21270;&#8221;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#20043;&#21069;&#28608;&#27963;&#30340;&#31070;&#32463;&#20803;&#26469;&#31574;&#30053;&#24615;&#22320;&#20943;&#23569;&#25968;&#25454;&#20256;&#36755;&#65292;&#20854;&#27425;&#65292;&#8220;&#34892;&#21015;&#32465;&#23450;&#8221;&#36866;&#24212;&#20102;&#38378;&#23384;&#30340;&#39034;&#24207;&#25968;&#25454;&#35775;&#38382;&#29305;&#28857;&#65292;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are central to modern natural language processing, delivering exceptional performance in various tasks. However, their substantial computational and memory requirements present challenges, especially for devices with limited DRAM capacity. This paper tackles the challenge of efficiently running LLMs that exceed the available DRAM capacity by storing the model parameters in flash memory, but bringing them on demand to DRAM. Our method involves constructing an inference cost model that takes into account the characteristics of flash memory, guiding us to optimize in two critical areas: reducing the volume of data transferred from flash and reading data in larger, more contiguous chunks. Within this hardware-informed framework, we introduce two principal techniques. First, "windowing" strategically reduces data transfer by reusing previously activated neurons, and second, "row-column bundling", tailored to the sequential data access strengths of flash memory, 
&lt;/p&gt;</description></item><item><title>&#36817;&#26399;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#20135;&#29983;&#20102;&#19968;&#31181;&#20559;&#35265;&#65292;&#20542;&#21521;&#20110;&#23558;LLM&#29983;&#25104;&#30340;&#25991;&#26723;&#25490;&#21517;&#36739;&#39640;&#12290;&#36825;&#31181;&#8220;&#26469;&#28304;&#20559;&#35265;&#8221;&#21487;&#33021;&#23545;&#20449;&#24687;&#35775;&#38382;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.20501</link><description>&lt;p&gt;
LLM&#21487;&#33021;&#20027;&#23548;&#20449;&#24687;&#35775;&#38382;&#65306;&#31070;&#32463;&#26816;&#32034;&#22120;&#23545;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#23384;&#22312;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs may Dominate Information Access: Neural Retrievers are Biased Towards LLM-Generated Texts. (arXiv:2310.20501v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20501
&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#20135;&#29983;&#20102;&#19968;&#31181;&#20559;&#35265;&#65292;&#20542;&#21521;&#20110;&#23558;LLM&#29983;&#25104;&#30340;&#25991;&#26723;&#25490;&#21517;&#36739;&#39640;&#12290;&#36825;&#31181;&#8220;&#26469;&#28304;&#20559;&#35265;&#8221;&#21487;&#33021;&#23545;&#20449;&#24687;&#35775;&#38382;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#22312;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#24212;&#29992;&#65292;&#23588;&#20854;&#26159;&#22312;&#32593;&#32476;&#25628;&#32034;&#26041;&#38754;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#33539;&#24335;&#12290;&#30001;&#20110;&#20854;&#22312;&#29983;&#25104;&#31867;&#20154;&#25991;&#26412;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#65292;LLMs&#22312;&#20114;&#32852;&#32593;&#19978;&#21019;&#36896;&#20102;&#22823;&#37327;&#30340;&#25991;&#26412;&#12290;&#22240;&#27492;&#65292;LLMs&#26102;&#20195;&#30340;IR&#31995;&#32479;&#38754;&#20020;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#65306;&#32034;&#24341;&#30340;&#25991;&#26723;&#19981;&#20165;&#26159;&#30001;&#20154;&#31867;&#25776;&#20889;&#30340;&#65292;&#32780;&#19988;&#36824;&#21253;&#25324;&#30001;LLMs&#33258;&#21160;&#29983;&#25104;&#30340;&#25991;&#26723;&#12290;&#36825;&#20123;LLM&#29983;&#25104;&#30340;&#25991;&#26723;&#22914;&#20309;&#24433;&#21709;IR&#31995;&#32479;&#26159;&#19968;&#20010;&#32039;&#36843;&#19988;&#23578;&#26410;&#25506;&#32034;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#28041;&#21450;&#20154;&#31867;&#32534;&#20889;&#21644;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#19981;&#21516;IR&#27169;&#22411;&#30340;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#23450;&#37327;&#35780;&#20272;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#31070;&#32463;&#26816;&#32034;&#27169;&#22411;&#20542;&#21521;&#20110;&#23558;LLM&#29983;&#25104;&#30340;&#25991;&#26723;&#25490;&#21517;&#36739;&#39640;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#31070;&#32463;&#26816;&#32034;&#27169;&#22411;&#23545;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#20559;&#35265;&#31216;&#20026;&#8220;&#26469;&#28304;&#20559;&#35265;&#8221;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#20559;&#35265;&#19981;&#20165;&#38480;&#20110;f&#26041;&#30456;&#24403;&#30340;&#24773;&#20917;&#65292;&#32780;&#19988;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#20063;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the emergence of large language models (LLMs) has revolutionized the paradigm of information retrieval (IR) applications, especially in web search. With their remarkable capabilities in generating human-like texts, LLMs have created enormous texts on the Internet. As a result, IR systems in the LLMs era are facing a new challenge: the indexed documents now are not only written by human beings but also automatically generated by the LLMs. How these LLM-generated documents influence the IR systems is a pressing and still unexplored question. In this work, we conduct a quantitative evaluation of different IR models in scenarios where both human-written and LLM-generated texts are involved. Surprisingly, our findings indicate that neural retrieval models tend to rank LLM-generated documents higher. We refer to this category of biases in neural retrieval models towards the LLM-generated text as the \textbf{source bias}. Moreover, we discover that this bias is not confined to the f
&lt;/p&gt;</description></item><item><title>&#12298;Janus&#25509;&#21475;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#22914;&#20309;&#25918;&#22823;&#38544;&#31169;&#39118;&#38505;&#12299;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#23545;&#20010;&#20154;&#20449;&#24687;&#27844;&#38706;&#30340;&#39118;&#38505;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;LLM&#21033;&#29992;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2310.15469</link><description>&lt;p&gt;
&#12298;Janus&#25509;&#21475;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#22914;&#20309;&#25918;&#22823;&#38544;&#31169;&#39118;&#38505;&#12299;
&lt;/p&gt;
&lt;p&gt;
The Janus Interface: How Fine-Tuning in Large Language Models Amplifies the Privacy Risks. (arXiv:2310.15469v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15469
&lt;/p&gt;
&lt;p&gt;
&#12298;Janus&#25509;&#21475;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#22914;&#20309;&#25918;&#22823;&#38544;&#31169;&#39118;&#38505;&#12299;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#23545;&#20010;&#20154;&#20449;&#24687;&#27844;&#38706;&#30340;&#39118;&#38505;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;LLM&#21033;&#29992;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2018&#24180;&#21518;&#30340;&#26102;&#20195;&#26631;&#24535;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#65292;OpenAI&#30340;ChatGPT&#31561;&#21019;&#26032;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#35821;&#35328;&#33021;&#21147;&#12290;&#38543;&#30528;&#34892;&#19994;&#22312;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#24182;&#21033;&#29992;&#22823;&#37327;&#30340;&#20154;&#31867;&#35821;&#35328;&#25968;&#25454;&#26041;&#38754;&#30340;&#21162;&#21147;&#65292;&#23433;&#20840;&#21644;&#38544;&#31169;&#25361;&#25112;&#20063;&#20986;&#29616;&#20102;&#12290;&#20854;&#20013;&#26368;&#37325;&#35201;&#30340;&#26159;&#22312;&#22522;&#20110;&#32593;&#32476;&#30340;&#25968;&#25454;&#33719;&#21462;&#36807;&#31243;&#20013;&#65292;&#21487;&#33021;&#20250;&#24847;&#22806;&#31215;&#32047;&#20010;&#20154;&#21487;&#35782;&#21035;&#20449;&#24687;&#65288;PII&#65289;&#65292;&#20174;&#32780;&#23548;&#33268;&#24847;&#22806;&#30340;PII&#27844;&#38706;&#39118;&#38505;&#12290;&#34429;&#28982;&#20687;RLHF&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#36825;&#26679;&#30340;&#31574;&#30053;&#24050;&#34987;&#29992;&#26469;&#25511;&#21046;&#38544;&#31169;&#20405;&#26435;&#30340;&#39118;&#38505;&#65292;&#20294;LLM&#30340;&#26368;&#26032;&#36827;&#23637;&#65288;&#20197;OpenAI&#30340;GPT-3.5&#30340;&#24494;&#35843;&#30028;&#38754;&#20026;&#20195;&#34920;&#65289;&#37325;&#26032;&#24341;&#21457;&#20102;&#20851;&#27880;&#12290;&#26377;&#20154;&#21487;&#33021;&#20250;&#38382;&#65306;LLM&#30340;&#24494;&#35843;&#26159;&#21542;&#20250;&#23548;&#33268;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#23884;&#20837;&#30340;&#20010;&#20154;&#20449;&#24687;&#27844;&#28431;&#65311;&#26412;&#25991;&#25253;&#36947;&#20102;&#39318;&#27425;&#23581;&#35797;&#23547;&#27714;&#31572;&#26696;&#30340;&#21162;&#21147;&#65292;&#37325;&#28857;&#26159;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;LLM&#21033;&#29992;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
The era post-2018 marked the advent of Large Language Models (LLMs), with innovations such as OpenAI's ChatGPT showcasing prodigious linguistic prowess. As the industry galloped toward augmenting model parameters and capitalizing on vast swaths of human language data, security and privacy challenges also emerged. Foremost among these is the potential inadvertent accrual of Personal Identifiable Information (PII) during web-based data acquisition, posing risks of unintended PII disclosure. While strategies like RLHF during training and Catastrophic Forgetting have been marshaled to control the risk of privacy infringements, recent advancements in LLMs, epitomized by OpenAI's fine-tuning interface for GPT-3.5, have reignited concerns. One may ask: can the fine-tuning of LLMs precipitate the leakage of personal information embedded within training datasets? This paper reports the first endeavor to seek the answer to the question, particularly our discovery of a new LLM exploitation avenue
&lt;/p&gt;</description></item><item><title>CompA&#25552;&#20986;&#20102;&#30001;&#20004;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#38899;&#39057;-&#35821;&#35328;&#27169;&#22411;&#32452;&#21512;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;ALMs&#22312;&#29702;&#35299;&#38899;&#39057;&#20013;&#22768;&#38899;&#20107;&#20214;&#30340;&#39034;&#24207;&#21644;&#23646;&#24615;&#32465;&#23450;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.08753</link><description>&lt;p&gt;
CompA: &#35299;&#20915;&#38899;&#39057;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32452;&#21512;&#25512;&#29702;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
CompA: Addressing the Gap in Compositional Reasoning in Audio-Language Models. (arXiv:2310.08753v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08753
&lt;/p&gt;
&lt;p&gt;
CompA&#25552;&#20986;&#20102;&#30001;&#20004;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#38899;&#39057;-&#35821;&#35328;&#27169;&#22411;&#32452;&#21512;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;ALMs&#22312;&#29702;&#35299;&#38899;&#39057;&#20013;&#22768;&#38899;&#20107;&#20214;&#30340;&#39034;&#24207;&#21644;&#23646;&#24615;&#32465;&#23450;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#30340;&#22522;&#26412;&#29305;&#24615;&#26159;&#20854;&#32452;&#21512;&#24615;&#12290;&#20351;&#29992;&#23545;&#27604;&#26041;&#27861;&#65288;&#20363;&#22914;CLAP&#65289;&#35757;&#32451;&#30340;&#38899;&#39057;-&#35821;&#35328;&#27169;&#22411;&#65288;ALMs&#65289;&#33021;&#22815;&#23398;&#20064;&#38899;&#39057;&#21644;&#35821;&#35328;&#27169;&#24577;&#20043;&#38388;&#30340;&#20849;&#20139;&#34920;&#31034;&#65292;&#20174;&#32780;&#22312;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#20013;&#25552;&#39640;&#24615;&#33021;&#65292;&#21253;&#25324;&#38646;&#26679;&#26412;&#38899;&#39057;&#20998;&#31867;&#12289;&#38899;&#39057;&#26816;&#32034;&#31561;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#26377;&#25928;&#25191;&#34892;&#32452;&#21512;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#36824;&#24456;&#23569;&#34987;&#25506;&#32034;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CompA&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#20004;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#26159;&#30495;&#23454;&#19990;&#30028;&#30340;&#38899;&#39057;&#26679;&#26412;&#65292;&#29992;&#20110;&#35780;&#20272;ALMs&#30340;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;CompA-order&#35780;&#20272;ALMs&#22312;&#29702;&#35299;&#38899;&#39057;&#20013;&#22768;&#38899;&#20107;&#20214;&#30340;&#39034;&#24207;&#25110;&#21457;&#29983;&#26102;&#30340;&#34920;&#29616;&#22914;&#20309;&#65292;&#32780;CompA-attribute&#35780;&#20272;&#22768;&#38899;&#20107;&#20214;&#30340;&#23646;&#24615;&#32465;&#23450;&#12290;&#27599;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#30340;&#23454;&#20363;&#21253;&#21547;&#20004;&#20010;&#38899;&#39057;-&#26631;&#39064;&#23545;&#65292;&#20854;&#20013;&#20004;&#20010;&#38899;&#39057;&#20855;&#26377;&#30456;&#21516;&#30340;&#22768;&#38899;&#20107;&#20214;&#65292;&#20294;&#32452;&#21512;&#26041;&#24335;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental characteristic of audio is its compositional nature. Audio-language models (ALMs) trained using a contrastive approach (e.g., CLAP) that learns a shared representation between audio and language modalities have improved performance in many downstream applications, including zero-shot audio classification, audio retrieval, etc. However, the ability of these models to effectively perform compositional reasoning remains largely unexplored and necessitates additional research. In this paper, we propose CompA, a collection of two expert-annotated benchmarks with a majority of real-world audio samples, to evaluate compositional reasoning in ALMs. Our proposed CompA-order evaluates how well an ALM understands the order or occurrence of acoustic events in audio, and CompA-attribute evaluates attribute binding of acoustic events. An instance from either benchmark consists of two audio-caption pairs, where both audios have the same acoustic events but with different compositions. A
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#35838;&#22530;&#25968;&#25454;&#20998;&#26512;&#22797;&#21046;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#21450;&#36825;&#31181;&#26041;&#27861;&#23545;&#23398;&#29983;&#12289;&#25945;&#32946;&#32773;&#21644;&#31185;&#23398;&#23478;&#30340;&#25104;&#26412;&#19982;&#25910;&#30410;&#12290;&#21516;&#26102;&#65292;&#23398;&#29983;&#23545;&#25968;&#25454;&#30340;&#39044;&#26399;&#19982;&#23454;&#38469;&#24773;&#20917;&#23384;&#22312;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2308.16491</link><description>&lt;p&gt;
&#35838;&#22530;&#25968;&#25454;&#20998;&#26512;&#22797;&#21046;&#65306;&#25945;&#23398;&#29983;&#65292;&#21516;&#26102;&#27979;&#35797;&#31185;&#23398;
&lt;/p&gt;
&lt;p&gt;
In-class Data Analysis Replications: Teaching Students while Testing Science. (arXiv:2308.16491v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16491
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#35838;&#22530;&#25968;&#25454;&#20998;&#26512;&#22797;&#21046;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#21450;&#36825;&#31181;&#26041;&#27861;&#23545;&#23398;&#29983;&#12289;&#25945;&#32946;&#32773;&#21644;&#31185;&#23398;&#23478;&#30340;&#25104;&#26412;&#19982;&#25910;&#30410;&#12290;&#21516;&#26102;&#65292;&#23398;&#29983;&#23545;&#25968;&#25454;&#30340;&#39044;&#26399;&#19982;&#23454;&#38469;&#24773;&#20917;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#27491;&#38754;&#20020;&#21487;&#37325;&#22797;&#24615;&#21361;&#26426;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#23558;&#25968;&#25454;&#20998;&#26512;&#22797;&#21046;&#32435;&#20837;&#35838;&#22530;&#20316;&#20026;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#28508;&#22312;&#30340;&#22909;&#22788;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#19968;&#26041;&#27861;&#26159;&#21542;&#21487;&#34892;&#65292;&#22914;&#26524;&#21487;&#34892;&#65292;&#28041;&#21450;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;-&#23398;&#29983;&#12289;&#25945;&#32946;&#32773;&#21644;&#31185;&#23398;&#23478;-&#24212;&#35813;&#26399;&#26395;&#20160;&#20040;&#12290;&#23398;&#29983;&#33021;&#22815;&#22312;&#35838;&#22530;&#19978;&#36827;&#34892;&#25968;&#25454;&#20998;&#26512;&#22797;&#21046;&#21527;&#65311;&#25945;&#32946;&#32773;&#30340;&#25104;&#26412;&#19982;&#25910;&#30410;&#22914;&#20309;&#65311;&#36825;&#20010;&#35299;&#20915;&#26041;&#26696;&#22914;&#20309;&#24110;&#21161;&#35780;&#20272;&#21644;&#25913;&#36827;&#31185;&#23398;&#30340;&#29616;&#29366;&#65311;&#26412;&#30740;&#31350;&#22312;EPFL&#25945;&#25480;&#30340;&#24212;&#29992;&#25968;&#25454;&#20998;&#26512;&#35838;&#31243;&#65288;CS-401&#65289;&#30340;&#39033;&#30446;&#37096;&#20998;&#20013;&#32435;&#20837;&#20102;&#25968;&#25454;&#20998;&#26512;&#22797;&#21046;&#65288;N=354&#21517;&#23398;&#29983;&#65289;&#12290;&#22312;&#27492;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#35838;&#31243;&#26399;&#38388;&#36827;&#34892;&#30340;&#35843;&#26597;&#25552;&#21069;&#36827;&#34892;&#27880;&#20876;&#30340;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#23398;&#29983;&#21487;&#20197;&#22797;&#21046;&#20808;&#21069;&#21457;&#34920;&#30340;&#31185;&#23398;&#35770;&#25991;&#65292;&#22823;&#37096;&#20998;&#26159;&#23450;&#24615;&#30340;&#65292;&#26377;&#20123;&#26159;&#23436;&#20840;&#19968;&#26679;&#30340;&#12290;&#25105;&#20204;&#21457;&#29616;&#23398;&#29983;&#23545;&#25968;&#25454;&#30340;&#39044;&#26399;&#19982;&#23454;&#38469;&#24773;&#20917;&#23384;&#22312;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Science is facing a reproducibility crisis. Previous work has proposed incorporating data analysis replications into classrooms as a potential solution. However, despite the potential benefits, it is unclear whether this approach is feasible, and if so, what the involved stakeholders-students, educators, and scientists-should expect from it. Can students perform a data analysis replication over the course of a class? What are the costs and benefits for educators? And how can this solution help benchmark and improve the state of science?  In the present study, we incorporated data analysis replications in the project component of the Applied Data Analysis course (CS-401) taught at EPFL (N=354 students). Here we report pre-registered findings based on surveys administered throughout the course. First, we demonstrate that students can replicate previously published scientific papers, most of them qualitatively and some exactly. We find discrepancies between what students expect of data an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;AI&#22312;&#35745;&#31639;&#20219;&#21153;&#20013;&#26159;&#21542;&#21487;&#20197;&#20174;&#21019;&#36896;&#24615;&#20915;&#31574;&#26426;&#21046;&#20013;&#21463;&#30410;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#22810;&#26679;&#21270;&#30340;AI&#31995;&#32479;&#22242;&#38431;&#65292;&#22312;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#36229;&#36234;&#21333;&#20010;AI&#65292;&#36890;&#36807;&#29983;&#25104;&#26356;&#22810;&#30340;&#24819;&#27861;&#65292;&#24182;&#36873;&#25321;&#26368;&#20339;&#24819;&#27861;&#12290;&#22312;&#22269;&#38469;&#35937;&#26827;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22810;&#26679;&#21270;AI&#31995;&#32479;&#20197;&#19981;&#21516;&#26041;&#24335;&#19979;&#22269;&#38469;&#35937;&#26827;&#12290;</title><link>http://arxiv.org/abs/2308.09175</link><description>&lt;p&gt;
&#25193;&#23637;AI&#65306;&#21521;&#25317;&#26377;&#21019;&#36896;&#24615;&#30340;AlphaZero&#22269;&#38469;&#35937;&#26827;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Diversifying AI: Towards Creative Chess with AlphaZero. (arXiv:2308.09175v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;AI&#22312;&#35745;&#31639;&#20219;&#21153;&#20013;&#26159;&#21542;&#21487;&#20197;&#20174;&#21019;&#36896;&#24615;&#20915;&#31574;&#26426;&#21046;&#20013;&#21463;&#30410;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#22810;&#26679;&#21270;&#30340;AI&#31995;&#32479;&#22242;&#38431;&#65292;&#22312;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#36229;&#36234;&#21333;&#20010;AI&#65292;&#36890;&#36807;&#29983;&#25104;&#26356;&#22810;&#30340;&#24819;&#27861;&#65292;&#24182;&#36873;&#25321;&#26368;&#20339;&#24819;&#27861;&#12290;&#22312;&#22269;&#38469;&#35937;&#26827;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22810;&#26679;&#21270;AI&#31995;&#32479;&#20197;&#19981;&#21516;&#26041;&#24335;&#19979;&#22269;&#38469;&#35937;&#26827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#21508;&#31181;&#35745;&#31639;&#20219;&#21153;&#19978;&#24050;&#32463;&#36229;&#36807;&#20102;&#20154;&#31867;&#30340;&#26234;&#33021;&#12290;&#28982;&#32780;&#65292;&#19982;&#20154;&#31867;&#19968;&#26679;&#65292;AI&#31995;&#32479;&#20063;&#20250;&#29359;&#38169;&#35823;&#65292;&#26377;&#30450;&#28857;&#65292;&#20135;&#29983;&#24187;&#35273;&#65292;&#24182;&#19988;&#22312;&#38754;&#23545;&#26032;&#24773;&#20917;&#26102;&#24456;&#38590;&#36827;&#34892;&#27867;&#21270;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24403;AI&#31995;&#32479;&#30340;&#35745;&#31639;&#21512;&#29702;&#24615;&#25512;&#21040;&#26497;&#38480;&#26102;&#65292;&#26159;&#21542;&#21487;&#20197;&#20174;&#21019;&#36896;&#24615;&#30340;&#20915;&#31574;&#26426;&#21046;&#20013;&#21463;&#30410;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26159;&#21542;&#36890;&#36807;&#20316;&#20026;&#19968;&#20010;&#22242;&#38431;&#30340;&#22810;&#26679;&#21270;AI&#31995;&#32479;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20013;&#21487;&#20197;&#32988;&#36807;&#21333;&#20010;AI&#65292;&#36890;&#36807;&#29983;&#25104;&#26356;&#22810;&#30340;&#24819;&#27861;&#65292;&#28982;&#21518;&#36873;&#25321;&#26368;&#22909;&#30340;&#24819;&#27861;&#12290;&#25105;&#20204;&#20197;&#22269;&#38469;&#35937;&#26827;&#36825;&#20010;&#34987;&#31216;&#20026;AI&#26524;&#34631;&#30340;&#28216;&#25103;&#20026;&#20363;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312;AlphaZero (AZ)&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#28508;&#21464;&#26465;&#20214;&#26550;&#26500;&#25193;&#23637;&#23427;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#20195;&#29702;&#22242;&#38431;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;AZ_db&#12290;&#25105;&#20204;&#20351;&#29992;&#34892;&#20026;&#22810;&#26679;&#24615;&#25216;&#26415;&#23545;AZ_db&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#29983;&#25104;&#26356;&#24191;&#27867;&#30340;&#24819;&#27861;&#65292;&#24182;&#36890;&#36807;&#27425;&#21152;&#24615;&#35745;&#21010;&#36873;&#25321;&#26368;&#26377;&#24076;&#26395;&#30340;&#24819;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;AZ_db&#20197;&#19981;&#21516;&#26041;&#24335;&#19979;&#22269;&#38469;&#35937;&#26827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Artificial Intelligence (AI) systems have surpassed human intelligence in a variety of computational tasks. However, AI systems, like humans, make mistakes, have blind spots, hallucinate, and struggle to generalize to new situations. This work explores whether AI can benefit from creative decision-making mechanisms when pushed to the limits of its computational rationality. In particular, we investigate whether a team of diverse AI systems can outperform a single AI in challenging tasks by generating more ideas as a group and then selecting the best ones. We study this question in the game of chess, the so-called drosophila of AI. We build on AlphaZero (AZ) and extend it to represent a league of agents via a latent-conditioned architecture, which we call AZ_db. We train AZ_db to generate a wider range of ideas using behavioral diversity techniques and select the most promising ones with sub-additive planning. Our experiments suggest that AZ_db plays chess in diverse wa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#27979;&#35797;&#26694;&#26550;&#65292;&#21033;&#29992;&#24320;&#21457;&#20154;&#21592;&#32534;&#20889;&#30340;&#27979;&#35797;&#21644;&#21487;&#29992;&#30340;&#20195;&#30721;&#27169;&#22411;&#29983;&#25104;&#21487;&#32534;&#35793;&#12289;&#26131;&#35835;&#30340;&#21333;&#20803;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2308.08033</link><description>&lt;p&gt;
&#20351;&#29992;&#20195;&#30721;&#27169;&#22411;&#21644;&#39046;&#22495;&#36866;&#24212;&#24615;&#30340;&#33258;&#21160;&#21270;&#27979;&#35797;&#29992;&#20363;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Automated Test Case Generation Using Code Models and Domain Adaptation. (arXiv:2308.08033v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#27979;&#35797;&#26694;&#26550;&#65292;&#21033;&#29992;&#24320;&#21457;&#20154;&#21592;&#32534;&#20889;&#30340;&#27979;&#35797;&#21644;&#21487;&#29992;&#30340;&#20195;&#30721;&#27169;&#22411;&#29983;&#25104;&#21487;&#32534;&#35793;&#12289;&#26131;&#35835;&#30340;&#21333;&#20803;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#33258;&#21160;&#21270;&#27979;&#35797;&#29983;&#25104;&#25216;&#26415;&#65292;&#20363;&#22914;&#22522;&#20110;&#25628;&#32034;&#30340;&#27979;&#35797;&#65292;&#36890;&#24120;&#23545;&#24320;&#21457;&#20154;&#21592;&#21019;&#24314;&#30340;&#27979;&#35797;&#29992;&#20363;&#19968;&#26080;&#25152;&#30693;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#36890;&#24120;&#29983;&#25104;&#30340;&#27979;&#35797;&#29992;&#20363;&#19981;&#26131;&#38405;&#35835;&#65292;&#24182;&#19988;&#21487;&#33021;&#26080;&#27861;&#26816;&#27979;&#25152;&#26377;&#22797;&#26434;&#32570;&#38519;&#65292;&#32780;&#24320;&#21457;&#20154;&#21592;&#32534;&#20889;&#30340;&#27979;&#35797;&#29992;&#20363;&#21017;&#21487;&#20197;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;Transformer&#30340;&#20195;&#30721;&#27169;&#22411;&#29983;&#25104;&#21487;&#20197;&#34917;&#20805;&#22522;&#20110;&#25628;&#32034;&#27979;&#35797;&#29983;&#25104;&#30340;&#21333;&#20803;&#27979;&#35797;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;CodeT5&#65292;&#21363;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#20195;&#30721;&#27169;&#22411;&#65292;&#24182;&#23545;&#27979;&#35797;&#29983;&#25104;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#20351;&#29992;Methods2test&#25968;&#25454;&#38598;&#23545;CodeT5&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20351;&#29992;Defects4j&#36827;&#34892;&#39033;&#30446;&#32423;&#39046;&#22495;&#36866;&#24212;&#24615;&#21644;&#35780;&#20272;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#27979;&#35797;&#26694;&#26550;&#65292;&#21033;&#29992;&#24320;&#21457;&#20154;&#21592;&#32534;&#20889;&#30340;&#27979;&#35797;&#21644;&#21487;&#29992;&#30340;&#20195;&#30721;&#27169;&#22411;&#29983;&#25104;&#21487;&#32534;&#35793;&#12289;&#26131;&#35835;&#30340;&#21333;&#20803;&#27979;&#35797;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#26032;&#30340;&#27979;&#35797;&#29992;&#20363;&#65292;&#35206;&#30422;&#20102;&#24050;&#32463;&#34987;&#27979;&#35797;&#36807;&#30340;&#20195;&#30721;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art automated test generation techniques, such as search-based testing, are usually ignorant about what a developer would create as a test case. Therefore, they typically create tests that are not human-readable and may not necessarily detect all types of complex bugs developer-written tests would do. In this study, we leverage Transformer-based code models to generate unit tests that can complement search-based test generation. Specifically, we use CodeT5, i.e., a state-of-the-art large code model, and fine-tune it on the test generation downstream task. For our analysis, we use the Methods2test dataset for fine-tuning CodeT5 and Defects4j for project-level domain adaptation and evaluation. The main contribution of this study is proposing a fully automated testing framework that leverages developer-written tests and available code models to generate compilable, human-readable unit tests. Results show that our approach can generate new test cases that cover lines that were
&lt;/p&gt;</description></item><item><title>SILO&#26159;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23545;&#38750;&#21442;&#25968;&#21270;&#30340;&#25968;&#25454;&#23384;&#20648;&#36827;&#34892;&#26597;&#35810;&#65292;&#23454;&#29616;&#22312;&#38754;&#20020;&#27861;&#24459;&#39118;&#38505;&#21644;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#25903;&#25345;&#25968;&#25454;&#24402;&#23646;&#21644;&#25968;&#25454;&#29983;&#20135;&#32773;&#36864;&#20986;&#27169;&#22411;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.04430</link><description>&lt;p&gt;
SILO&#35821;&#35328;&#27169;&#22411;&#65306;&#22312;&#38750;&#21442;&#25968;&#21270;&#25968;&#25454;&#23384;&#20648;&#20013;&#38548;&#31163;&#27861;&#24459;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore. (arXiv:2308.04430v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04430
&lt;/p&gt;
&lt;p&gt;
SILO&#26159;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23545;&#38750;&#21442;&#25968;&#21270;&#30340;&#25968;&#25454;&#23384;&#20648;&#36827;&#34892;&#26597;&#35810;&#65292;&#23454;&#29616;&#22312;&#38754;&#20020;&#27861;&#24459;&#39118;&#38505;&#21644;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#25903;&#25345;&#25968;&#25454;&#24402;&#23646;&#21644;&#25968;&#25454;&#29983;&#20135;&#32773;&#36864;&#20986;&#27169;&#22411;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#23558;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#35757;&#32451;&#22312;&#21463;&#29256;&#26435;&#25110;&#21463;&#20854;&#20182;&#38480;&#21046;&#30340;&#25968;&#25454;&#19978;&#30340;&#21512;&#27861;&#24615;&#36827;&#34892;&#28608;&#28872;&#36777;&#35770;&#30340;&#21516;&#26102;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20165;&#22312;&#20302;&#39118;&#38505;&#25991;&#26412;&#65288;&#20363;&#22914;&#36807;&#26399;&#29256;&#26435;&#22270;&#20070;&#25110;&#25919;&#24220;&#25991;&#20214;&#65289;&#19978;&#35757;&#32451;&#26102;&#65292;&#27169;&#22411;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#21407;&#22240;&#26159;&#35813;&#25991;&#26412;&#30340;&#35268;&#27169;&#21644;&#39046;&#22495;&#35206;&#30422;&#26377;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SILO&#65292;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#31649;&#29702;&#36825;&#31181;&#39118;&#38505;-&#24615;&#33021;&#26435;&#34913;&#12290;SILO&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#26500;&#24314;&#65306;&#65288;1&#65289;&#22312;&#25105;&#20204;&#31574;&#21010;&#30340;&#26032;&#35821;&#26009;&#24211;&#8220;&#24320;&#25918;&#35768;&#21487;&#35777;&#35821;&#26009;&#24211;&#8221;&#65288;OLC&#65289;&#19978;&#35757;&#32451;&#21442;&#25968;&#21270;&#30340;LM&#65292;&#35813;&#35821;&#26009;&#24211;&#21253;&#21547;228B&#20010;&#20844;&#20849;&#39046;&#22495;&#21644;&#35768;&#21487;&#25991;&#26412;&#12290;&#65288;2&#65289;&#36890;&#36807;&#38750;&#21442;&#25968;&#21270;&#30340;&#25968;&#25454;&#23384;&#20648;&#65288;&#20363;&#22914;&#21253;&#21547;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#22270;&#20070;&#25110;&#26032;&#38395;&#30340;&#25968;&#25454;&#65289;&#23545;&#20854;&#36827;&#34892;&#25193;&#20805;&#65292;&#35813;&#25968;&#25454;&#23384;&#20648;&#20165;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#34987;&#26597;&#35810;&#12290;&#35813;&#25968;&#25454;&#23384;&#20648;&#20801;&#35768;&#20351;&#29992;&#39640;&#39118;&#38505;&#25968;&#25454;&#32780;&#26080;&#38656;&#23545;&#20854;&#36827;&#34892;&#35757;&#32451;&#65292;&#25903;&#25345;&#21477;&#32423;&#25968;&#25454;&#24402;&#23646;&#65292;&#24182;&#20351;&#25968;&#25454;&#29983;&#20135;&#32773;&#21487;&#20197;&#36890;&#36807;&#20174;&#23384;&#20648;&#20013;&#21024;&#38500;&#20869;&#23481;&#26469;&#36873;&#25321;&#36864;&#20986;&#27169;&#22411;&#12290;&#36825;&#20123;&#21151;&#33021;&#21487;&#20197;&#20419;&#36827;&#23545;&#25968;&#25454;&#20351;&#29992;&#35268;&#33539;&#30340;&#36981;&#24490;&#12290;
&lt;/p&gt;
&lt;p&gt;
The legality of training language models (LMs) on copyrighted or otherwise restricted data is under intense debate. However, as we show, model performance significantly degrades if trained only on low-risk text (e.g., out-of-copyright books or government documents), due to its limited size and domain coverage. We present SILO, a new language model that manages this risk-performance tradeoff during inference. SILO is built by (1) training a parametric LM on Open License Corpus (OLC), a new corpus we curate with 228B tokens of public domain and permissively licensed text and (2) augmenting it with a more general and easily modifiable nonparametric datastore (e.g., containing copyrighted books or news) that is only queried during inference. The datastore allows use of high-risk data without training on it, supports sentence-level data attribution, and enables data producers to opt out from the model by removing content from the store. These capabilities can foster compliance with data-use
&lt;/p&gt;</description></item><item><title>RAPGen&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#20351;&#29992;Retrieval-Augmented Prompt Generation&#65288;RAPGen&#65289;&#26041;&#27861;&#65292;&#21363;&#20174;&#39044;&#20808;&#26500;&#24314;&#30340;&#24615;&#33021;Bug&#20462;&#22797;&#30693;&#35782;&#24211;&#20013;&#26816;&#32034;&#25552;&#31034;&#25351;&#20196;&#24182;&#29983;&#25104;&#25552;&#31034;&#65292;&#28982;&#21518;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#29983;&#25104;&#20462;&#22797;&#26041;&#26696;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#20195;&#30721;&#20302;&#25928;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#19987;&#23478;&#39564;&#35777;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;RAPGen&#22312;60%&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#29983;&#25104;&#19982;&#24320;&#21457;&#32773;&#31561;&#25928;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#25913;&#36827;&#24314;&#35758;&#65292;&#20854;&#20013;&#32422;39%&#30340;&#24314;&#35758;&#23436;&#20840;&#30456;&#21516;&#12290;</title><link>http://arxiv.org/abs/2306.17077</link><description>&lt;p&gt;
RAPGen: &#19968;&#31181;&#35299;&#20915;&#38646;&#26679;&#26412;&#20195;&#30721;&#20302;&#25928;&#38382;&#39064;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RAPGen: An Approach for Fixing Code Inefficiencies in Zero-Shot. (arXiv:2306.17077v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17077
&lt;/p&gt;
&lt;p&gt;
RAPGen&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#20351;&#29992;Retrieval-Augmented Prompt Generation&#65288;RAPGen&#65289;&#26041;&#27861;&#65292;&#21363;&#20174;&#39044;&#20808;&#26500;&#24314;&#30340;&#24615;&#33021;Bug&#20462;&#22797;&#30693;&#35782;&#24211;&#20013;&#26816;&#32034;&#25552;&#31034;&#25351;&#20196;&#24182;&#29983;&#25104;&#25552;&#31034;&#65292;&#28982;&#21518;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#29983;&#25104;&#20462;&#22797;&#26041;&#26696;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#20195;&#30721;&#20302;&#25928;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#19987;&#23478;&#39564;&#35777;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;RAPGen&#22312;60%&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#29983;&#25104;&#19982;&#24320;&#21457;&#32773;&#31561;&#25928;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#25913;&#36827;&#24314;&#35758;&#65292;&#20854;&#20013;&#32422;39%&#30340;&#24314;&#35758;&#23436;&#20840;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24615;&#33021;Bug&#26159;&#19968;&#31181;&#21363;&#20351;&#22312;&#32463;&#36807;&#20805;&#20998;&#27979;&#35797;&#30340;&#21830;&#19994;&#20135;&#21697;&#20013;&#20063;&#21487;&#33021;&#20986;&#29616;&#30340;&#38750;&#21151;&#33021;&#24615;&#38382;&#39064;&#12290;&#20462;&#22797;&#36825;&#20123;&#24615;&#33021;Bug&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20010;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Retrieval-Augmented Prompt Generation&#65288;RAPGen&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#32473;&#23450;&#19968;&#20010;&#23384;&#22312;&#24615;&#33021;&#38382;&#39064;&#30340;&#20195;&#30721;&#29255;&#27573;&#65292;RAPGen&#39318;&#20808;&#20174;&#39044;&#20808;&#26500;&#24314;&#30340;&#20043;&#21069;&#24615;&#33021;Bug&#20462;&#22797;&#30693;&#35782;&#24211;&#20013;&#26816;&#32034;&#19968;&#20010;&#25552;&#31034;&#25351;&#20196;&#65292;&#28982;&#21518;&#20351;&#29992;&#26816;&#32034;&#21040;&#30340;&#25351;&#20196;&#29983;&#25104;&#19968;&#20010;&#25552;&#31034;&#12290;&#28982;&#21518;&#65292;&#23427;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#20351;&#29992;&#36825;&#20010;&#25552;&#31034;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;Codex&#65289;&#19978;&#29983;&#25104;&#19968;&#20010;&#20462;&#22797;&#26041;&#26696;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#21508;&#31181;&#25552;&#31034;&#21464;&#20307;&#21644;&#29616;&#26377;&#26041;&#27861;&#22312;&#24615;&#33021;Bug&#20462;&#22797;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;RAPGen&#22312;60%&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#29983;&#25104;&#19982;&#24320;&#21457;&#32773;&#31561;&#25928;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#25913;&#36827;&#24314;&#35758;&#65292;&#22312;&#32463;&#36807;&#19987;&#23478;&#39564;&#35777;&#30340;&#36807;&#21435;C#&#24320;&#21457;&#32773;&#25152;&#20570;&#30340;&#24615;&#33021;&#26356;&#25913;&#25968;&#25454;&#38598;&#20013;&#26377;&#32422;39%&#30340;&#24314;&#35758;&#23436;&#20840;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performance bugs are non-functional bugs that can even manifest in well-tested commercial products. Fixing these performance bugs is an important yet challenging problem. In this work, we address this challenge and present a new approach called Retrieval-Augmented Prompt Generation (RAPGen). Given a code snippet with a performance issue, RAPGen first retrieves a prompt instruction from a pre-constructed knowledge-base of previous performance bug fixes and then generates a prompt using the retrieved instruction. It then uses this prompt on a Large Language Model (such as Codex) in zero-shot to generate a fix. We compare our approach with the various prompt variations and state of the art methods in the task of performance bug fixing. Our evaluation shows that RAPGen can generate performance improvement suggestions equivalent or better than a developer in ~60% of the cases, getting ~39% of them verbatim, in an expert-verified dataset of past performance changes made by C# developers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;BetaZero&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#20351;&#29992;&#23398;&#20064;&#36817;&#20284;&#31639;&#27861;&#30340;&#32622;&#20449;&#29366;&#24577;&#35268;&#21010;&#31639;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#38271;&#26102;&#38388;&#36328;&#24230;&#30340;POMDP&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.00249</link><description>&lt;p&gt;
BetaZero&#65306;&#22522;&#20110;&#23398;&#20064;&#30340;&#36817;&#20284;&#31639;&#27861;&#30340;&#32622;&#20449;&#29366;&#24577;&#35268;&#21010;&#29992;&#20110;&#38271;&#26102;&#38388;&#36328;&#24230;&#30340;POMDPs
&lt;/p&gt;
&lt;p&gt;
BetaZero: Belief-State Planning for Long-Horizon POMDPs using Learned Approximations. (arXiv:2306.00249v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;BetaZero&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#20351;&#29992;&#23398;&#20064;&#36817;&#20284;&#31639;&#27861;&#30340;&#32622;&#20449;&#29366;&#24577;&#35268;&#21010;&#31639;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#38271;&#26102;&#38388;&#36328;&#24230;&#30340;POMDP&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#30340;&#35268;&#21010;&#38382;&#39064;&#65292;&#21253;&#25324;&#33258;&#21160;&#39550;&#39542;&#12289;&#30899;&#20648;&#23384;&#21644;&#36164;&#28304;&#21208;&#25506;&#31561;&#21487;&#25345;&#32493;&#33021;&#28304;&#24212;&#29992;&#65292;&#26368;&#36817;&#34987;&#24314;&#27169;&#20026;&#37096;&#20998;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDPs&#65289;&#24182;&#20351;&#29992;&#36817;&#20284;&#26041;&#27861;&#35299;&#20915;&#12290;&#20026;&#20102;&#22312;&#23454;&#36341;&#20013;&#35299;&#20915;&#39640;&#32500;&#24230;POMDPs&#65292;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20351;&#29992;&#20102;&#38382;&#39064;&#29305;&#23450;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#36827;&#34892;&#22312;&#32447;&#35268;&#21010;&#65292;&#20197;&#20943;&#23569;&#35268;&#21010;&#26102;&#38388;&#36328;&#24230;&#24182;&#20351;&#38382;&#39064;&#26131;&#20110;&#35299;&#20915;&#12290;&#26368;&#36817;&#25104;&#21151;&#22320;&#22312;&#23436;&#20840;&#21487;&#35266;&#23519;&#30340;&#39046;&#22495;&#20013;&#25214;&#21040;&#20102;&#29992;&#20110;&#26367;&#25442;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#23398;&#20064;&#36817;&#20284;&#31639;&#27861;&#12290;&#20851;&#38190;&#27934;&#35265;&#26159;&#23558;&#22312;&#32447;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#19982;&#31163;&#32447;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#30456;&#32467;&#21512;&#65292;&#20197;&#20248;&#21270;&#31574;&#30053;&#21644;&#20540;&#20989;&#25968;&#12290;&#26412;&#25991;&#23558;&#36825;&#19968;&#27934;&#35265;&#24212;&#29992;&#21040;&#20102;&#37096;&#20998;&#35266;&#23519;&#22495;&#65292;&#24182;&#25552;&#20986;&#20102;BetaZero&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;POMDP&#30340;&#32622;&#20449;&#29366;&#24577;&#35268;&#21010;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world planning problems$\unicode{x2014}$including autonomous driving and sustainable energy applications like carbon storage and resource exploration$\unicode{x2014}$have recently been modeled as partially observable Markov decision processes (POMDPs) and solved using approximate methods. To solve high-dimensional POMDPs in practice, state-of-the-art methods use online planning with problem-specific heuristics to reduce planning horizons and make the problems tractable. Algorithms that learn approximations to replace heuristics have recently found success in large-scale problems in the fully observable domain. The key insight is the combination of online Monte Carlo tree search with offline neural network approximations of the optimal policy and value function. In this work, we bring this insight to partially observed domains and propose BetaZero, a belief-state planning algorithm for POMDPs. BetaZero learns offline approximations based on accurate belief models to enable online d
&lt;/p&gt;</description></item></channel></rss>