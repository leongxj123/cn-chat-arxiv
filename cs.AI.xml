<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36890;&#36807;&#22534;&#21472;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#32858;&#31867;&#23454;&#29616;&#36965;&#24863;&#25968;&#25454;&#22320;&#36136;&#21046;&#22270;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;</title><link>https://arxiv.org/abs/2404.02180</link><description>&lt;p&gt;
&#36890;&#36807;&#22534;&#21472;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#32858;&#31867;&#23454;&#29616;&#22320;&#36136;&#21046;&#22270;&#30340;&#36965;&#24863;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Remote sensing framework for geological mapping via stacked autoencoders and clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02180
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22534;&#21472;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#32858;&#31867;&#23454;&#29616;&#36965;&#24863;&#25968;&#25454;&#22320;&#36136;&#21046;&#22270;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#36965;&#24863;&#22320;&#36136;&#21046;&#22270;&#20013;&#38754;&#20020;&#30528;&#30001;&#20110;&#20934;&#30830;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#32780;&#38480;&#21046;&#30340;&#38382;&#39064;&#12290;&#30456;&#21453;&#65292;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#38477;&#32500;&#21644;&#32858;&#31867;&#65292;&#33021;&#22815;&#22312;&#19981;&#20381;&#36182;&#39044;&#23450;&#20041;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#25581;&#31034;&#36965;&#24863;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#21644;&#32467;&#26500;&#12290;&#38477;&#32500;&#26041;&#27861;&#20855;&#26377;&#22312;&#25552;&#39640;&#22320;&#36136;&#22270;&#20934;&#30830;&#24615;&#26041;&#38754;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#30340;&#28508;&#21147;&#12290;&#34429;&#28982;&#20256;&#32479;&#30340;&#38477;&#32500;&#26041;&#27861;&#21487;&#33021;&#22312;&#38750;&#32447;&#24615;&#25968;&#25454;&#19978;&#36935;&#21040;&#22256;&#38590;&#65292;&#20294;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22914;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#33021;&#22815;&#27169;&#25311;&#25968;&#25454;&#20013;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#22534;&#21472;&#33258;&#21160;&#32534;&#30721;&#22120;&#20855;&#26377;&#22810;&#20010;&#30456;&#20114;&#36830;&#25509;&#30340;&#23618;&#65292;&#29992;&#20110;&#25429;&#33719;&#23545;&#36965;&#24863;&#25968;&#25454;&#26377;&#29992;&#30340;&#20998;&#23618;&#25968;&#25454;&#34920;&#31034;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#22534;&#21472;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#32858;&#31867;&#22788;&#29702;&#36965;&#24863;&#25968;&#25454;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02180v1 Announce Type: cross  Abstract: Supervised learning methods for geological mapping via remote sensing face limitations due to the scarcity of accurately labelled training data. In contrast, unsupervised learning methods, such as dimensionality reduction and clustering have the ability to uncover patterns and structures in remote sensing data without relying on predefined labels. Dimensionality reduction methods have the potential to play a crucial role in improving the accuracy of geological maps. Although conventional dimensionality reduction methods may struggle with nonlinear data, unsupervised deep learning models such as autoencoders have the ability to model nonlinear relationship in data. Stacked autoencoders feature multiple interconnected layers to capture hierarchical data representations that can be useful for remote sensing data. In this study, we present an unsupervised machine learning framework for processing remote sensing data by utilizing stacked au
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#22312;&#40657;&#30418;&#35774;&#32622;&#20013;&#20272;&#35745;&#26799;&#24230;&#24182;&#29983;&#25104;&#26174;&#33879;&#22270;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#65292;&#36890;&#36807;Likelihood Ratio&#26041;&#27861;&#20272;&#35745;&#36755;&#20986;&#21040;&#36755;&#20837;&#30340;&#26799;&#24230;&#65292;&#24182;&#24212;&#29992;&#20998;&#22359;&#35745;&#31639;&#25216;&#26415;&#25552;&#39640;&#20272;&#35745;&#20934;&#30830;&#24615;&#65292;&#23454;&#39564;&#35777;&#23454;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15603</link><description>&lt;p&gt;
&#22522;&#20110;&#21069;&#21521;&#23398;&#20064;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#40657;&#30418;&#26174;&#33879;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Forward Learning for Gradient-based Black-box Saliency Map Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15603
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#22312;&#40657;&#30418;&#35774;&#32622;&#20013;&#20272;&#35745;&#26799;&#24230;&#24182;&#29983;&#25104;&#26174;&#33879;&#22270;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#65292;&#36890;&#36807;Likelihood Ratio&#26041;&#27861;&#20272;&#35745;&#36755;&#20986;&#21040;&#36755;&#20837;&#30340;&#26799;&#24230;&#65292;&#24182;&#24212;&#29992;&#20998;&#22359;&#35745;&#31639;&#25216;&#26415;&#25552;&#39640;&#20272;&#35745;&#20934;&#30830;&#24615;&#65292;&#23454;&#39564;&#35777;&#23454;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;-based&#26174;&#33879;&#22270;&#34987;&#24191;&#27867;&#29992;&#20110;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#27169;&#22411;&#21464;&#24471;&#26356;&#28145;&#21644;&#26356;&#40657;&#30418;&#65292;&#22914;&#22312;&#38381;&#28304;API&#65288;&#22914;ChatGPT&#65289;&#20013;&#65292;&#35745;&#31639;&#26799;&#24230;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38459;&#30861;&#20256;&#32479;&#35299;&#37322;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#40657;&#30418;&#35774;&#32622;&#20013;&#20272;&#35745;&#26799;&#24230;&#24182;&#29983;&#25104;&#26174;&#33879;&#22270;&#26469;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#12290;&#25105;&#20204;&#37319;&#29992;&#20284;&#28982;&#27604;&#26041;&#27861;&#26469;&#20272;&#35745;&#36755;&#20986;&#21040;&#36755;&#20837;&#30340;&#26799;&#24230;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#26174;&#33879;&#22270;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#22359;&#35745;&#31639;&#25216;&#26415;&#26469;&#22686;&#24378;&#20272;&#35745;&#20934;&#30830;&#24615;&#12290;&#22312;&#40657;&#30418;&#35774;&#32622;&#20013;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#20934;&#30830;&#30340;&#26799;&#24230;&#20272;&#35745;&#21644;&#29983;&#25104;&#26174;&#33879;&#22270;&#30340;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#23427;&#26469;&#35299;&#37322;GPT-Vision&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#25581;&#31034;&#20102;&#26799;&#24230;&#30456;&#20851;&#24615;&#30340;&#25345;&#32493;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15603v1 Announce Type: cross  Abstract: Gradient-based saliency maps are widely used to explain deep neural network decisions. However, as models become deeper and more black-box, such as in closed-source APIs like ChatGPT, computing gradients become challenging, hindering conventional explanation methods. In this work, we introduce a novel unified framework for estimating gradients in black-box settings and generating saliency maps to interpret model decisions. We employ the likelihood ratio method to estimate output-to-input gradients and utilize them for saliency map generation. Additionally, we propose blockwise computation techniques to enhance estimation accuracy. Extensive experiments in black-box settings validate the effectiveness of our method, demonstrating accurate gradient estimation and explainability of generated saliency maps. Furthermore, we showcase the scalability of our approach by applying it to explain GPT-Vision, revealing the continued relevance of gr
&lt;/p&gt;</description></item><item><title>&#25552;&#20379;&#20102;&#19968;&#31181;&#24320;&#28304;&#26694;&#26550;&#65292;&#29992;&#20110;&#20849;&#20139;&#30005;&#21160;&#20986;&#34892;&#65292;&#20197;&#20195;&#29702;-&#29615;&#36335;&#26041;&#27861;&#21644;&#27169;&#22359;&#21270;&#26550;&#26500;&#20026;&#29305;&#33394;&#65292;&#26088;&#22312;&#24357;&#34917;&#29616;&#26377;&#20849;&#20139;&#30005;&#21160;&#20986;&#34892;&#26381;&#21153;&#30340;&#35774;&#35745;&#32570;&#38519;&#65292;&#21521;&#30005;&#21160;&#20986;&#34892;&#30740;&#31350;&#31038;&#21306;&#25552;&#20379;&#31119;&#21033;&#12290;</title><link>https://arxiv.org/abs/2403.07964</link><description>&lt;p&gt;
&#19968;&#31181;&#24320;&#28304;&#20223;&#30495;&#24179;&#21488;&#30340;&#26368;&#20339;&#35774;&#35745;&#19982;&#23454;&#26045;&#65292;&#29992;&#20110;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#20849;&#20139;&#30005;&#21160;&#20986;&#34892;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
Optimal Design and Implementation of an Open-source Emulation Platform for User-Centric Shared E-mobility Services
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07964
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#20102;&#19968;&#31181;&#24320;&#28304;&#26694;&#26550;&#65292;&#29992;&#20110;&#20849;&#20139;&#30005;&#21160;&#20986;&#34892;&#65292;&#20197;&#20195;&#29702;-&#29615;&#36335;&#26041;&#27861;&#21644;&#27169;&#22359;&#21270;&#26550;&#26500;&#20026;&#29305;&#33394;&#65292;&#26088;&#22312;&#24357;&#34917;&#29616;&#26377;&#20849;&#20139;&#30005;&#21160;&#20986;&#34892;&#26381;&#21153;&#30340;&#35774;&#35745;&#32570;&#38519;&#65292;&#21521;&#30005;&#21160;&#20986;&#34892;&#30740;&#31350;&#31038;&#21306;&#25552;&#20379;&#31119;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#20132;&#36890;&#25490;&#25918;&#21644;&#27745;&#26579;&#19981;&#26029;&#21152;&#21095;&#30340;&#20840;&#29699;&#25361;&#25112;&#65292;&#20849;&#20139;&#30005;&#21160;&#20986;&#34892;&#26381;&#21153;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324;&#30005;&#21160;&#27773;&#36710;&#12289;&#30005;&#21160;&#33258;&#34892;&#36710;&#21644;&#30005;&#21160;&#28369;&#26495;&#36710;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20849;&#20139;&#30005;&#21160;&#20986;&#34892;&#26381;&#21153;&#23384;&#22312;&#20851;&#38190;&#35774;&#35745;&#32570;&#38519;&#65292;&#21253;&#25324;&#26381;&#21153;&#25972;&#21512;&#19981;&#36275;&#12289;&#33021;&#28304;&#28040;&#32791;&#39044;&#27979;&#19981;&#31934;&#30830;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#22320;&#29702;&#35206;&#30422;&#33539;&#22260;&#26377;&#38480;&#65292;&#20197;&#21450;&#22312;&#22810;&#27169;&#24335;&#20132;&#36890;&#32972;&#26223;&#19979;&#23588;&#20854;&#32570;&#20047;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#35270;&#35282;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#30446;&#21069;&#27809;&#26377;&#19968;&#20010;&#25972;&#21512;&#30340;&#24320;&#28304;&#26694;&#26550;&#65292;&#21487;&#20197;&#20026;&#30005;&#21160;&#20986;&#34892;&#30740;&#31350;&#31038;&#21306;&#24102;&#26469;&#30410;&#22788;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25552;&#20379;&#19968;&#20010;&#24320;&#21019;&#24615;&#30340;&#20849;&#20139;&#30005;&#21160;&#20986;&#34892;&#24320;&#28304;&#26694;&#26550;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#37319;&#29992;&#20195;&#29702;-&#29615;&#36335;&#26041;&#27861;&#21644;&#27169;&#22359;&#21270;&#26550;&#26500;&#65292;&#26088;&#22312;&#28385;&#36275;&#19981;&#21516;&#29992;&#25143;&#20559;&#22909;&#65292;&#24182;&#25552;&#20379;&#22686;&#24378;&#30340;&#33258;&#23450;&#20041;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07964v1 Announce Type: new  Abstract: In response to the escalating global challenge of increasing emissions and pollution in transportation, shared electric mobility services, encompassing e-cars, e-bikes, and e-scooters, have emerged as a popular strategy. However, existingshared electric mobility services exhibit critical design deficiencies, including insufficient service integration, imprecise energy consumption forecasting, limited scalability and geographical coverage, and a notable absence of a user-centric perspective, particularly in the context of multi-modal transportation. More importantly, there is no consolidated open-source framework which could benefit the e-mobility research community. This paper aims to bridge this gap by providing a pioneering open-source framework for shared e-mobility. The proposed framework, with an agent-in-the-loop approach and modular architecture, is tailored to diverse user preferences and offers enhanced customization. We demonst
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Multimodal ECG Representation Learning (MERL)&#26694;&#26550;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#24515;&#30005;&#22270;&#20998;&#31867;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#23545;ECG&#35760;&#24405;&#21644;&#30456;&#20851;&#25253;&#21578;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#21516;&#26102;&#22312;&#27979;&#35797;&#38454;&#27573;&#20351;&#29992;&#20102;Clinical Knowledge Enhanced Prompt Engineering (CKEPE)&#26041;&#27861;&#26469;&#21033;&#29992;&#20020;&#24202;&#30693;&#35782;&#25968;&#25454;&#24211;&#12290;</title><link>https://arxiv.org/abs/2403.06659</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#24577;&#23398;&#20064;&#21644;&#27979;&#35797;&#26102;&#20020;&#24202;&#30693;&#35782;&#22686;&#24378;&#30340;&#38646;&#26679;&#26412;&#24515;&#30005;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot ECG Classification with Multimodal Learning and Test-time Clinical Knowledge Enhancement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06659
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Multimodal ECG Representation Learning (MERL)&#26694;&#26550;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#24515;&#30005;&#22270;&#20998;&#31867;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#23545;ECG&#35760;&#24405;&#21644;&#30456;&#20851;&#25253;&#21578;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#21516;&#26102;&#22312;&#27979;&#35797;&#38454;&#27573;&#20351;&#29992;&#20102;Clinical Knowledge Enhanced Prompt Engineering (CKEPE)&#26041;&#27861;&#26469;&#21033;&#29992;&#20020;&#24202;&#30693;&#35782;&#25968;&#25454;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#26159;&#20020;&#24202;&#23454;&#36341;&#20013;&#29992;&#20110;&#26816;&#27979;&#24515;&#24459;&#22833;&#24120;&#30142;&#30149;&#30340;&#38750;&#20405;&#20837;&#24615;&#35786;&#26029;&#24037;&#20855;&#12290;&#22312;&#26410;&#32463;&#27880;&#37322;&#30340;ECG&#25968;&#25454;&#20013;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;eSSL&#65289;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#34920;&#24449;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;&#21487;&#20197;&#22312;&#25253;&#21578;&#20013;&#25214;&#21040;&#30340;&#20020;&#24202;&#30693;&#35782;&#12290;&#26412;&#25991;&#36890;&#36807;&#22810;&#27169;&#24577;&#23398;&#20064;ECG&#35760;&#24405;&#21644;&#30456;&#20851;&#25253;&#21578;&#65292;&#25552;&#20986;&#20102;Multimodal ECG Representation Learning (MERL)&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#36827;&#34892;&#38646;&#26679;&#26412;ECG&#20998;&#31867;&#65292;&#28040;&#38500;&#20102;&#19979;&#28216;&#20219;&#21153;&#20013;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#22312;&#27979;&#35797;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Clinical Knowledge Enhanced Prompt Engineering (CKEPE)&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#21033;&#29992;&#22806;&#37096;&#19987;&#23478;&#39564;&#35777;&#30340;&#20020;&#24202;&#30693;&#35782;&#25968;&#25454;&#24211;&#65292;&#29983;&#25104;&#26356;&#22810;&#20851;&#20110;&#24739;&#32773;&#30149;&#21490;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06659v1 Announce Type: cross  Abstract: Electrocardiograms (ECGs) are non-invasive diagnostic tools crucial for detecting cardiac arrhythmic diseases in clinical practice. While ECG Self-supervised Learning (eSSL) methods show promise in representation learning from unannotated ECG data, they often overlook the clinical knowledge that can be found in reports. This oversight and the requirement for annotated samples for downstream tasks limit eSSL's versatility. In this work, we address these issues with the Multimodal ECG Representation Learning (MERL}) framework. Through multimodal learning on ECG records and associated reports, MERL is capable of performing zero-shot ECG classification with text prompts, eliminating the need for training data in downstream tasks. At test time, we propose the Clinical Knowledge Enhanced Prompt Engineering (CKEPE) approach, which uses Large Language Models (LLMs) to exploit external expert-verified clinical knowledge databases, generating mo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36890;&#29992;&#35745;&#31639;&#26426;&#25511;&#21046;&#65288;GCC&#65289;&#35774;&#32622;&#65292;&#36890;&#36807;&#22522;&#20110;&#23631;&#24149;&#22270;&#20687;&#21644;&#21487;&#33021;&#30340;&#38899;&#39057;&#36755;&#20837;&#30340;&#22522;&#37329;&#20195;&#29702;&#26694;&#26550;Cradle&#65292;&#23454;&#29616;&#20102;&#23545;&#12298;&#33618;&#37326;&#22823;&#38230;&#23458;2&#12299;&#20013;&#22797;&#26434;&#20219;&#21153;&#30340;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.03186</link><description>&lt;p&gt;
&#36890;&#24448;&#36890;&#29992;&#35745;&#31639;&#26426;&#25511;&#21046;&#65306;&#22810;&#27169;&#24577;&#20195;&#29702;&#22312;&#12298;&#33618;&#37326;&#22823;&#38230;&#23458;2&#12299;&#20013;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards General Computer Control: A Multimodal Agent for Red Dead Redemption II as a Case Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03186
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36890;&#29992;&#35745;&#31639;&#26426;&#25511;&#21046;&#65288;GCC&#65289;&#35774;&#32622;&#65292;&#36890;&#36807;&#22522;&#20110;&#23631;&#24149;&#22270;&#20687;&#21644;&#21487;&#33021;&#30340;&#38899;&#39057;&#36755;&#20837;&#30340;&#22522;&#37329;&#20195;&#29702;&#26694;&#26550;Cradle&#65292;&#23454;&#29616;&#20102;&#23545;&#12298;&#33618;&#37326;&#22823;&#38230;&#23458;2&#12299;&#20013;&#22797;&#26434;&#20219;&#21153;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#22522;&#37329;&#30340;&#20195;&#29702;&#22312;&#29305;&#23450;&#20219;&#21153;&#25110;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#20195;&#29702;&#26080;&#27861;&#36328;&#19981;&#21516;&#22330;&#26223;&#27867;&#21270;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#23427;&#20204;&#22810;&#26679;&#21270;&#30340;&#35266;&#23519;&#21644;&#34892;&#21160;&#31354;&#38388;&#20197;&#21450;&#35821;&#20041;&#24046;&#36317;&#65292;&#25110;&#20381;&#36182;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#36164;&#28304;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#29992;&#35745;&#31639;&#26426;&#25511;&#21046;&#65288;GCC&#65289;&#35774;&#32622;&#65306;&#36890;&#36807;&#20165;&#33719;&#21462;&#35745;&#31639;&#26426;&#30340;&#23631;&#24149;&#22270;&#20687;&#65288;&#20197;&#21450;&#21487;&#33021;&#30340;&#38899;&#39057;&#65289;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#20135;&#29983;&#38190;&#30424;&#21644;&#40736;&#26631;&#25805;&#20316;&#20316;&#20026;&#36755;&#20986;&#65292;&#31867;&#20284;&#20110;&#20154;&#26426;&#20132;&#20114;&#65292;&#26500;&#24314;&#21487;&#20197;&#31934;&#36890;&#20219;&#20309;&#35745;&#31639;&#26426;&#20219;&#21153;&#30340;&#22522;&#37329;&#20195;&#29702;&#12290;&#20026;&#20102;&#38024;&#23545;GCC&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Cradle&#65292;&#19968;&#20010;&#20195;&#29702;&#26694;&#26550;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#21253;&#25324;&#33258;&#25105;&#21453;&#24605;&#12289;&#20219;&#21153;&#25512;&#29702;&#21644;&#25216;&#33021;&#25972;&#29702;&#65292;&#20197;&#30830;&#20445;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#21644;&#33258;&#25105;&#25913;&#36827;&#12290;&#20026;&#20102;&#23637;&#31034;Cradle&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#23558;&#20854;&#37096;&#32626;&#22312;&#22797;&#26434;&#30340;AAA&#28216;&#25103;&#12298;&#33618;&#37326;&#22823;&#38230;&#23458;2&#12299;&#20013;&#65292;&#20316;&#20026;&#36890;&#21521;G&#30340;&#21021;&#27493;&#23581;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03186v1 Announce Type: new  Abstract: Recent studies have demonstrated the success of foundation agents in specific tasks or scenarios. However, existing agents cannot generalize across different scenarios, mainly due to their diverse observation and action spaces and semantic gaps, or reliance on task-specific resources. In this work, we propose the General Computer Control (GCC) setting: building foundation agents that can master any computer task by taking only screen images (and possibly audio) of the computer as input, and producing keyboard and mouse operations as output, similar to human-computer interaction. To target GCC, we propose Cradle, an agent framework with strong reasoning abilities, including self-reflection, task inference, and skill curation, to ensure generalizability and self-improvement across various tasks. To demonstrate the capabilities of Cradle, we deploy it in the complex AAA game Red Dead Redemption II, serving as a preliminary attempt towards G
&lt;/p&gt;</description></item><item><title>&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#25968;&#25454;&#22686;&#24378;&#30340;&#36716;&#21464;&#24615;&#24433;&#21709;&#65292;&#29420;&#29305;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#31361;&#20986;&#20102;LLMs&#22312;&#25968;&#25454;&#22686;&#24378;&#20013;&#24341;&#20837;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;</title><link>https://arxiv.org/abs/2403.02990</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#30340;&#25968;&#25454;&#22686;&#24378;&#65306;&#25968;&#25454;&#35270;&#35282;&#12289;&#23398;&#20064;&#33539;&#24335;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation using LLMs: Data Perspectives, Learning Paradigms and Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02990
&lt;/p&gt;
&lt;p&gt;
&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#25968;&#25454;&#22686;&#24378;&#30340;&#36716;&#21464;&#24615;&#24433;&#21709;&#65292;&#29420;&#29305;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#31361;&#20986;&#20102;LLMs&#22312;&#25968;&#25454;&#22686;&#24378;&#20013;&#24341;&#20837;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#39046;&#22495;&#24555;&#36895;&#21457;&#23637;&#20013;&#65292;&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#20851;&#38190;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#35757;&#32451;&#26679;&#26412;&#22810;&#26679;&#21270;&#32780;&#26080;&#38656;&#39069;&#22806;&#25968;&#25454;&#25910;&#38598;&#26469;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#35843;&#26597;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#25968;&#25454;&#22686;&#24378;&#30340;&#36716;&#21464;&#24615;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#21450;&#20854;&#20182;&#39046;&#22495;&#20013;&#23427;&#20204;&#25552;&#20379;&#30340;&#29420;&#29305;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#20174;&#25968;&#25454;&#35270;&#35282;&#21644;&#23398;&#20064;&#35270;&#35282;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#30340;&#21508;&#31181;&#31574;&#30053;&#65292;&#21253;&#25324;&#23545;LLM&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#36827;&#19968;&#27493;&#35757;&#32451;&#30340;&#26032;&#39062;&#23398;&#20064;&#33539;&#24335;&#30340;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#38416;&#26126;&#20102;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#20174;&#21487;&#25511;&#25968;&#25454;&#22686;&#24378;&#21040;&#22810;&#27169;&#24577;&#25968;&#25454;&#22686;&#24378;&#31561;&#12290;&#26412;&#35843;&#26597;&#31361;&#26174;&#20102;LLMs&#22312;&#25968;&#25454;&#22686;&#24378;&#20013;&#24341;&#20837;&#30340;&#33539;&#24335;&#36716;&#21464;&#65292;&#26088;&#22312;&#20316;&#20026;&#19968;&#31181;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02990v1 Announce Type: cross  Abstract: In the rapidly evolving field of machine learning (ML), data augmentation (DA) has emerged as a pivotal technique for enhancing model performance by diversifying training examples without the need for additional data collection. This survey explores the transformative impact of Large Language Models (LLMs) on DA, particularly addressing the unique challenges and opportunities they present in the context of natural language processing (NLP) and beyond. From a data perspective and a learning perspective, we examine various strategies that utilize Large Language Models for data augmentation, including a novel exploration of learning paradigms where LLM-generated data is used for further training. Additionally, this paper delineates the primary challenges faced in this domain, ranging from controllable data augmentation to multi modal data augmentation. This survey highlights the paradigm shift introduced by LLMs in DA, aims to serve as a 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#29992;&#20110;&#35774;&#35745;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#36890;&#36807;&#23547;&#25214;&#26356;&#21512;&#36866;&#30340;&#26102;&#38388;&#27493;&#38271;&#21152;&#36895;&#25193;&#25955;&#37319;&#26679;&#12290;</title><link>https://arxiv.org/abs/2402.17376</link><description>&lt;p&gt;
&#20248;&#21270;&#26102;&#38388;&#27493;&#38271;&#21152;&#36895;&#25193;&#25955;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Accelerating Diffusion Sampling with Optimized Time Steps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17376
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#29992;&#20110;&#35774;&#35745;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#36890;&#36807;&#23547;&#25214;&#26356;&#21512;&#36866;&#30340;&#26102;&#38388;&#27493;&#38271;&#21152;&#36895;&#25193;&#25955;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#22312;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21512;&#25104;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#37319;&#26679;&#27493;&#39588;&#65292;&#20854;&#37319;&#26679;&#25928;&#29575;&#20173;&#26377;&#24453;&#25552;&#39640;&#12290;&#36817;&#26399;&#39640;&#38454;&#25968;&#20540;ODE&#27714;&#35299;&#22120;&#22312;DPMs&#20013;&#30340;&#24212;&#29992;&#20351;&#24471;&#29992;&#26356;&#23569;&#30340;&#37319;&#26679;&#27493;&#39588;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#25104;&#20026;&#21487;&#33021;&#12290;&#23613;&#31649;&#36825;&#26159;&#19968;&#39033;&#37325;&#22823;&#36827;&#23637;&#65292;&#22823;&#22810;&#25968;&#37319;&#26679;&#26041;&#27861;&#20173;&#28982;&#37319;&#29992;&#22343;&#21248;&#26102;&#38388;&#27493;&#38271;&#65292;&#32780;&#22312;&#37319;&#26679;&#27493;&#39588;&#36739;&#23569;&#26102;&#24182;&#19981;&#26159;&#26368;&#20339;&#36873;&#25321;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#35813;&#20248;&#21270;&#38382;&#39064;&#26088;&#22312;&#20026;DPMs&#30340;&#29305;&#23450;&#25968;&#20540;ODE&#27714;&#35299;&#22120;&#23547;&#25214;&#26356;&#21512;&#36866;&#30340;&#26102;&#38388;&#27493;&#38271;&#12290;&#27492;&#20248;&#21270;&#38382;&#39064;&#26088;&#22312;&#26368;&#23567;&#21270;&#22320;&#23454;&#29616;&#22320;&#30495;&#23454;&#35299;&#19982;&#19982;&#25968;&#20540;&#27714;&#35299;&#22120;&#23545;&#24212;&#30340;&#36817;&#20284;&#35299;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#21463;&#38480;&#20449;&#36182;&#22495;&#26041;&#27861;&#36827;&#34892;&#39640;&#25928;&#27714;&#35299;&#65292;&#26102;&#38388;&#23569;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17376v1 Announce Type: cross  Abstract: Diffusion probabilistic models (DPMs) have shown remarkable performance in high-resolution image synthesis, but their sampling efficiency is still to be desired due to the typically large number of sampling steps. Recent advancements in high-order numerical ODE solvers for DPMs have enabled the generation of high-quality images with much fewer sampling steps. While this is a significant development, most sampling methods still employ uniform time steps, which is not optimal when using a small number of steps. To address this issue, we propose a general framework for designing an optimization problem that seeks more appropriate time steps for a specific numerical ODE solver for DPMs. This optimization problem aims to minimize the distance between the ground-truth solution to the ODE and an approximate solution corresponding to the numerical solver. It can be efficiently solved using the constrained trust region method, taking less than 
&lt;/p&gt;</description></item><item><title>Farsight&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#23454;&#22320;&#20132;&#20114;&#24037;&#20855;&#65292;&#24110;&#21161;&#20154;&#20204;&#22312;&#35774;&#35745;AI&#24212;&#29992;&#21407;&#22411;&#26102;&#35782;&#21035;&#28508;&#22312;&#21361;&#23475;&#65292;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;Farsight&#21518;&#65292;AI&#21407;&#22411;&#35774;&#35745;&#32773;&#33021;&#22815;&#26356;&#22909;&#22320;&#29420;&#31435;&#35782;&#21035;&#19982;&#25552;&#31034;&#30456;&#20851;&#30340;&#28508;&#22312;&#21361;&#23475;&#12290;</title><link>https://arxiv.org/abs/2402.15350</link><description>&lt;p&gt;
Farsight&#65306;&#22312;AI&#24212;&#29992;&#21407;&#22411;&#35774;&#35745;&#36807;&#31243;&#20013;&#22521;&#20859;&#36127;&#36131;&#20219;&#30340;AI&#24847;&#35782;
&lt;/p&gt;
&lt;p&gt;
Farsight: Fostering Responsible AI Awareness During AI Application Prototyping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15350
&lt;/p&gt;
&lt;p&gt;
Farsight&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#23454;&#22320;&#20132;&#20114;&#24037;&#20855;&#65292;&#24110;&#21161;&#20154;&#20204;&#22312;&#35774;&#35745;AI&#24212;&#29992;&#21407;&#22411;&#26102;&#35782;&#21035;&#28508;&#22312;&#21361;&#23475;&#65292;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;Farsight&#21518;&#65292;AI&#21407;&#22411;&#35774;&#35745;&#32773;&#33021;&#22815;&#26356;&#22909;&#22320;&#29420;&#31435;&#35782;&#21035;&#19982;&#25552;&#31034;&#30456;&#20851;&#30340;&#28508;&#22312;&#21361;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25552;&#31034;&#39537;&#21160;&#30028;&#38754;&#20351;&#24471;&#21407;&#22411;&#35774;&#35745;&#21644;&#26500;&#24314;AI&#24212;&#29992;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#26356;&#23481;&#26131;&#12290;&#28982;&#32780;&#65292;&#35782;&#21035;&#21487;&#33021;&#22312;AI&#24212;&#29992;&#20013;&#20986;&#29616;&#30340;&#28508;&#22312;&#21361;&#23475;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#22522;&#20110;&#25552;&#31034;&#30340;&#21407;&#22411;&#35774;&#35745;&#36807;&#31243;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23454;&#22320;&#20132;&#20114;&#24037;&#20855;Farsight&#65292;&#24110;&#21161;&#20154;&#20204;&#35782;&#21035;&#20182;&#20204;&#27491;&#22312;&#35774;&#35745;&#21407;&#22411;&#30340;AI&#24212;&#29992;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#28508;&#22312;&#21361;&#23475;&#12290;&#26681;&#25454;&#29992;&#25143;&#30340;&#25552;&#31034;&#65292;Farsight&#31361;&#20986;&#26174;&#31034;&#20102;&#19982;&#30456;&#20851;AI&#20107;&#20214;&#26377;&#20851;&#30340;&#26032;&#38395;&#25991;&#31456;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#25506;&#32034;&#21644;&#32534;&#36753;LLM&#29983;&#25104;&#30340;&#29992;&#20363;&#12289;&#21033;&#30410;&#30456;&#20851;&#32773;&#21644;&#21361;&#23475;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#19982;10&#20301;AI&#21407;&#22411;&#35774;&#35745;&#32773;&#36827;&#34892;&#30340;&#20849;&#21516;&#35774;&#35745;&#30740;&#31350;&#30340;&#35774;&#35745;&#35265;&#35299;&#65292;&#20197;&#21450;&#19982;42&#20301;AI&#21407;&#22411;&#35774;&#35745;&#32773;&#36827;&#34892;&#30340;&#29992;&#25143;&#30740;&#31350;&#32467;&#26524;&#12290;&#22312;&#20351;&#29992;Farsight&#21518;&#65292;&#25105;&#20204;&#29992;&#25143;&#30740;&#31350;&#20013;&#30340;AI&#21407;&#22411;&#35774;&#35745;&#32773;&#33021;&#22815;&#26356;&#22909;&#22320;&#29420;&#31435;&#35782;&#21035;&#19982;&#25552;&#31034;&#30456;&#20851;&#30340;&#28508;&#22312;&#21361;&#23475;&#65292;&#24182;&#21457;&#29616;&#25105;&#20204;&#30340;&#24037;&#20855;&#27604;&#29616;&#26377;&#36164;&#28304;&#26356;&#26377;&#29992;&#19988;&#26356;&#26131;&#20110;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15350v1 Announce Type: cross  Abstract: Prompt-based interfaces for Large Language Models (LLMs) have made prototyping and building AI-powered applications easier than ever before. However, identifying potential harms that may arise from AI applications remains a challenge, particularly during prompt-based prototyping. To address this, we present Farsight, a novel in situ interactive tool that helps people identify potential harms from the AI applications they are prototyping. Based on a user's prompt, Farsight highlights news articles about relevant AI incidents and allows users to explore and edit LLM-generated use cases, stakeholders, and harms. We report design insights from a co-design study with 10 AI prototypers and findings from a user study with 42 AI prototypers. After using Farsight, AI prototypers in our user study are better able to independently identify potential harms associated with a prompt and find our tool more useful and usable than existing resources. T
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;LLM&#20195;&#29702;&#29978;&#33267;&#22312;&#31454;&#20105;&#29615;&#22659;&#20013;&#20063;&#33021;&#33258;&#21457;&#24418;&#25104;&#21512;&#20316;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#39564;&#35777;&#20102;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#30340;&#24895;&#26223;&#65292;&#34920;&#26126;LLM&#20195;&#29702;&#21487;&#20197;&#29992;&#20110;&#27169;&#25311;&#20154;&#31867;&#31038;&#20250;&#20114;&#21160;&#65292;&#21253;&#25324;&#33258;&#21457;&#21512;&#20316;&#30340;&#20114;&#21160;&#65292;&#20026;&#31038;&#20250;&#29616;&#35937;&#25552;&#20379;&#27934;&#23519;&#12290;</title><link>https://arxiv.org/abs/2402.12327</link><description>&lt;p&gt;
&#25105;&#20204;&#24212;&#35813;&#20132;&#27969;&#21527;&#65306;&#25506;&#32034;&#31454;&#20105;LLM&#20195;&#29702;&#20043;&#38388;&#30340;&#33258;&#21457;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Shall We Talk: Exploring Spontaneous Collaborations of Competing LLM Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12327
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;LLM&#20195;&#29702;&#29978;&#33267;&#22312;&#31454;&#20105;&#29615;&#22659;&#20013;&#20063;&#33021;&#33258;&#21457;&#24418;&#25104;&#21512;&#20316;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#39564;&#35777;&#20102;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#30340;&#24895;&#26223;&#65292;&#34920;&#26126;LLM&#20195;&#29702;&#21487;&#20197;&#29992;&#20110;&#27169;&#25311;&#20154;&#31867;&#31038;&#20250;&#20114;&#21160;&#65292;&#21253;&#25324;&#33258;&#21457;&#21512;&#20316;&#30340;&#20114;&#21160;&#65292;&#20026;&#31038;&#20250;&#29616;&#35937;&#25552;&#20379;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#36827;&#23637;&#34920;&#26126;&#65292;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#30340;&#20195;&#29702;&#20855;&#26377;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#21644;&#31038;&#20250;&#21160;&#24577;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23578;&#26410;&#30740;&#31350;LLM&#20195;&#29702;&#22312;&#27809;&#26377;&#26126;&#30830;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#33258;&#21457;&#24314;&#31435;&#21512;&#20316;&#20851;&#31995;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19977;&#39033;&#26696;&#20363;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;LLM&#20195;&#29702;&#29978;&#33267;&#22312;&#31454;&#20105;&#29615;&#22659;&#20013;&#20063;&#33021;&#33258;&#21457;&#24418;&#25104;&#21512;&#20316;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#36825;&#19968;&#21457;&#29616;&#19981;&#20165;&#23637;&#31034;&#20102;LLM&#20195;&#29702;&#27169;&#25311;&#20154;&#31867;&#31038;&#20250;&#20013;&#31454;&#20105;&#19982;&#21512;&#20316;&#30340;&#33021;&#21147;&#65292;&#20063;&#39564;&#35777;&#20102;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#24895;&#26223;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36825;&#34920;&#26126;LLM&#20195;&#29702;&#21487;&#20197;&#29992;&#20110;&#24314;&#27169;&#20154;&#31867;&#31038;&#20250;&#20114;&#21160;&#65292;&#21253;&#25324;&#37027;&#20123;&#33258;&#21457;&#21512;&#20316;&#30340;&#20114;&#21160;&#65292;&#20174;&#32780;&#25552;&#20379;&#23545;&#31038;&#20250;&#29616;&#35937;&#30340;&#27934;&#23519;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/wuzengqing001225/SABM_ShallWe &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12327v1 Announce Type: new  Abstract: Recent advancements have shown that agents powered by large language models (LLMs) possess capabilities to simulate human behaviors and societal dynamics. However, the potential for LLM agents to spontaneously establish collaborative relationships in the absence of explicit instructions has not been studied. To address this gap, we conduct three case studies, revealing that LLM agents are capable of spontaneously forming collaborations even within competitive settings. This finding not only demonstrates the capacity of LLM agents to mimic competition and cooperation in human societies but also validates a promising vision of computational social science. Specifically, it suggests that LLM agents could be utilized to model human social interactions, including those with spontaneous collaborations, thus offering insights into social phenomena. The source codes for this study are available at https://github.com/wuzengqing001225/SABM_ShallWe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38754;&#21521;&#25351;&#20196;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#65292;&#21457;&#29616;&#33258;&#21160;&#26041;&#27861;&#22312;&#19981;&#21516;&#20219;&#21153;&#31867;&#22411;&#19979;&#19982;&#20154;&#24037;&#35780;&#20272;&#32773;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#23384;&#22312;&#24040;&#22823;&#21464;&#21270;&#65292;&#19988;&#22312;&#33258;&#30001;&#24418;&#24335;&#29983;&#25104;&#20219;&#21153;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#20013;&#21487;&#33021;&#19981;&#21487;&#38752;&#12290;</title><link>https://arxiv.org/abs/2402.10770</link><description>&lt;p&gt;
&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#22312;&#38754;&#21521;&#25351;&#20196;&#30340;LLM&#20013;&#26377;&#22810;&#21487;&#38752;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38754;&#21521;&#25351;&#20196;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#65292;&#21457;&#29616;&#33258;&#21160;&#26041;&#27861;&#22312;&#19981;&#21516;&#20219;&#21153;&#31867;&#22411;&#19979;&#19982;&#20154;&#24037;&#35780;&#20272;&#32773;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#23384;&#22312;&#24040;&#22823;&#21464;&#21270;&#65292;&#19988;&#22312;&#33258;&#30001;&#24418;&#24335;&#29983;&#25104;&#20219;&#21153;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#20013;&#21487;&#33021;&#19981;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#25351;&#20196;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#25991;&#26412;&#37325;&#21472;&#21644;LLM&#21028;&#26029;&#30340;&#33258;&#21160;&#26041;&#27861;&#20316;&#20026;&#20154;&#24037;&#35780;&#20272;&#30340;&#25104;&#26412;&#26377;&#25928;&#26367;&#20195;&#26041;&#26696;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#33539;&#22260;&#21644;&#36328;&#35821;&#35328;&#29615;&#22659;&#20013;&#30340;&#21487;&#38752;&#24615;&#12290;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#30456;&#21453;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#20219;&#21153;&#31867;&#22411;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#33258;&#21160;&#26041;&#27861;&#19982;&#20154;&#24037;&#35780;&#20272;&#32773;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#23384;&#22312;&#26174;&#33879;&#21464;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24191;&#27867;&#20351;&#29992;&#30340;ROUGE-L&#24230;&#37327;&#22312;&#30701;&#31572;&#26696;&#33521;&#35821;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#21028;&#26029;&#24378;&#30456;&#20851;&#65292;&#20294;&#22312;&#33258;&#30001;&#24418;&#24335;&#29983;&#25104;&#20219;&#21153;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#20013;&#19981;&#21487;&#38752;&#12290;&#20351;&#29992;GPT-4&#20316;&#20026;&#35780;&#20272;&#21592;&#30340;&#26377;&#25928;&#24615;&#21462;&#20915;&#20110;&#22312;&#35201;&#27714;&#35780;&#20272;&#26102;&#21253;&#21547;&#21442;&#32771;&#31572;&#26696;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#22312;&#33258;&#30001;&#24418;&#24335;&#29983;&#25104;&#20219;&#21153;&#20013;&#35780;&#20272;&#36807;&#20110;&#20005;&#26684;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#21487;&#20197;&#36817;&#20284;&#20154;&#31867;&#21028;&#26029;&#65292;&#20294;&#20854;&#20934;&#30830;&#24615;&#21487;&#33021;&#22240;&#20219;&#21153;&#31867;&#22411;&#21644;&#35780;&#20272;&#35774;&#32622;&#32780;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10770v1 Announce Type: cross  Abstract: Work on instruction-tuned Large Language Models (LLMs) has used automatic methods based on text overlap and LLM judgments as cost-effective alternatives to human evaluation. In this paper, we study the reliability of such methods across a broad range of tasks and in a cross-lingual setting. In contrast to previous findings, we observe considerable variability in correlations between automatic methods and human evaluators when scores are differentiated by task type. Specifically, the widely-used ROUGE-L metric strongly correlates with human judgments for short-answer English tasks but is unreliable in free-form generation tasks and cross-lingual transfer. The effectiveness of GPT-4 as an evaluator depends on including reference answers when prompting for assessments, which can lead to overly strict evaluations in free-form generation tasks. In summary, we find that, while automatic evaluation methods can approximate human judgements und
&lt;/p&gt;</description></item><item><title>HGOT&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20107;&#23454;&#24615;&#35780;&#20272;&#30340;&#20998;&#23618;&#24605;&#32500;&#22270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#21010;&#33021;&#21147;&#21644;&#24605;&#32500;&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#26469;&#25552;&#39640;&#30456;&#20851;&#27573;&#33853;&#30340;&#26816;&#32034;&#21644;&#31572;&#26696;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2402.09390</link><description>&lt;p&gt;
HGOT: &#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20107;&#23454;&#24615;&#35780;&#20272;&#30340;&#20998;&#23618;&#24605;&#32500;&#22270;
&lt;/p&gt;
&lt;p&gt;
HGOT: Hierarchical Graph of Thoughts for Retrieval-Augmented In-Context Learning in Factuality Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09390
&lt;/p&gt;
&lt;p&gt;
HGOT&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20107;&#23454;&#24615;&#35780;&#20272;&#30340;&#20998;&#23618;&#24605;&#32500;&#22270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#21010;&#33021;&#21147;&#21644;&#24605;&#32500;&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#26469;&#25552;&#39640;&#30456;&#20851;&#27573;&#33853;&#30340;&#26816;&#32034;&#21644;&#31572;&#26696;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20107;&#23454;&#24615;&#21644;&#24187;&#35273;&#30340;&#20542;&#21521;&#24341;&#21457;&#20102;&#37325;&#22823;&#20851;&#20999;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#26816;&#32034;&#22686;&#24378;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#23618;&#24605;&#32500;&#22270;&#65288;HGOT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#12289;&#22810;&#23618;&#27425;&#30340;&#22270;&#24418;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#36807;&#31243;&#20013;&#30456;&#20851;&#27573;&#33853;&#30340;&#26816;&#32034;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;LLMs&#30340;&#36880;&#28176;&#35268;&#21010;&#33021;&#21147;&#65292;&#37319;&#29992;&#20998;&#32780;&#27835;&#20043;&#30340;&#31574;&#30053;&#23558;&#22797;&#26434;&#26597;&#35810;&#20998;&#35299;&#20026;&#21487;&#22788;&#29702;&#30340;&#23376;&#26597;&#35810;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#26368;&#36817;&#25552;&#20986;&#30340;&#24341;&#25991;&#22238;&#24518;&#21644;&#31934;&#30830;&#24230;&#25351;&#26631;&#26469;&#35780;&#20272;&#24605;&#32500;&#36136;&#37327;&#65292;&#23558;&#31572;&#26696;&#30340;&#21487;&#20449;&#24230;&#19982;&#24605;&#32500;&#30340;&#36136;&#37327;&#20869;&#22312;&#22320;&#32852;&#31995;&#36215;&#26469;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#33258;&#27965;&#24615;&#22810;&#25968;&#25237;&#31080;&#30340;&#31572;&#26696;&#36873;&#25321;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#21152;&#26435;&#31995;&#32479;&#65292;&#22312;&#22810;&#25968;&#25237;&#31080;&#20013;&#20248;&#20808;&#32771;&#34385;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09390v1 Announce Type: new Abstract: With the widespread adoption of large language models (LLMs) in numerous applications, the challenge of factuality and the propensity for hallucinations raises significant concerns. To address this issue, particularly in retrieval-augmented in-context learning, we introduce the hierarchical graph of thoughts (HGOT), a structured, multi-layered graph approach designed to enhance the retrieval of pertinent passages during in-context learning. The framework utilizes the emergent planning capabilities of LLMs, employing the divide-and-conquer strategy to break down complex queries into manageable sub-queries. It refines self-consistency majority voting for answer selection, which incorporates the recently proposed citation recall and precision metrics to assess the quality of thoughts, linking an answer's credibility intrinsically to the thought's quality. This methodology introduces a weighted system in majority voting, prioritizing answers 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#23398;&#20064;&#31995;&#32479;&#23433;&#20840;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;&#24403;&#21069;&#21457;&#23637;&#29366;&#24577;&#19979;&#30340;&#20851;&#38190;&#38480;&#21046;&#36827;&#34892;&#20102;&#23457;&#26597;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.05355</link><description>&lt;p&gt;
&#23433;&#20840;&#22810;&#27169;&#24577;&#23398;&#20064;&#31995;&#32479;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
A Survey on Safe Multi-Modal Learning System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05355
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#23398;&#20064;&#31995;&#32479;&#23433;&#20840;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;&#24403;&#21069;&#21457;&#23637;&#29366;&#24577;&#19979;&#30340;&#20851;&#38190;&#38480;&#21046;&#36827;&#34892;&#20102;&#23457;&#26597;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#27169;&#24577;&#23398;&#20064;&#31995;&#32479;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#23433;&#20840;&#38382;&#39064;&#21464;&#24471;&#36234;&#26469;&#36234;&#31361;&#20986;&#12290;&#23545;&#20110;&#36825;&#19968;&#39046;&#22495;&#30340;&#23433;&#20840;&#38382;&#39064;&#32570;&#20047;&#31995;&#32479;&#24615;&#30740;&#31350;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#23398;&#20064;&#31995;&#32479;&#23433;&#20840;&#30340;&#20998;&#31867;&#27861;&#65292;&#30830;&#23450;&#20102;&#36825;&#20123;&#38382;&#39064;&#30340;&#22235;&#20010;&#20851;&#38190;&#25903;&#26609;&#12290;&#20511;&#21161;&#36825;&#19968;&#20998;&#31867;&#27861;&#65292;&#25105;&#20204;&#23545;&#27599;&#20010;&#25903;&#26609;&#36827;&#34892;&#20102;&#28145;&#20837;&#23457;&#26597;&#65292;&#31361;&#20986;&#20102;&#24403;&#21069;&#21457;&#23637;&#29366;&#24577;&#30340;&#20851;&#38190;&#38480;&#21046;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#31995;&#32479;&#23433;&#20840;&#38754;&#20020;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the wide deployment of multimodal learning systems (MMLS) in real-world scenarios, safety concerns have become increasingly prominent. The absence of systematic research into their safety is a significant barrier to progress in this field. To bridge the gap, we present the first taxonomy for MMLS safety, identifying four essential pillars of these concerns. Leveraging this taxonomy, we conduct in-depth reviews for each pillar, highlighting key limitations based on the current state of development. Finally, we pinpoint unique challenges in MMLS safety and provide potential directions for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#20013;&#25552;&#21462;&#20107;&#20214;&#24207;&#21015;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#37319;&#29992;&#20102;&#22522;&#20110;&#30693;&#35782;&#22270;&#30340;&#25351;&#23548;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#23545;&#20855;&#26377;&#37096;&#20998;&#22240;&#26524;&#20851;&#31995;&#30340;&#20107;&#20214;&#27010;&#24565;&#30340;&#20107;&#20214;&#24207;&#21015;&#30340;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20107;&#20214;&#24207;&#21015;&#65292;&#24182;&#19988;&#22312;&#22635;&#34917;&#30693;&#35782;&#31354;&#30333;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#30340;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2401.07237</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#20107;&#20214;&#24207;&#21015;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Distilling Event Sequence Knowledge From Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#20013;&#25552;&#21462;&#20107;&#20214;&#24207;&#21015;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#37319;&#29992;&#20102;&#22522;&#20110;&#30693;&#35782;&#22270;&#30340;&#25351;&#23548;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#23545;&#20855;&#26377;&#37096;&#20998;&#22240;&#26524;&#20851;&#31995;&#30340;&#20107;&#20214;&#27010;&#24565;&#30340;&#20107;&#20214;&#24207;&#21015;&#30340;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20107;&#20214;&#24207;&#21015;&#65292;&#24182;&#19988;&#22312;&#22635;&#34917;&#30693;&#35782;&#31354;&#30333;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#22312;&#20107;&#20214;&#30340;&#20998;&#26512;&#21644;&#39044;&#27979;&#20013;&#34987;&#21457;&#29616;&#26159;&#38750;&#24120;&#26377;&#25928;&#30340;&#12290;&#24314;&#31435;&#36825;&#26679;&#30340;&#27169;&#22411;&#38656;&#35201;&#20016;&#23500;&#30340;&#39640;&#36136;&#37327;&#20107;&#20214;&#24207;&#21015;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#24212;&#29992;&#20013;&#65292;&#24178;&#20928;&#30340;&#32467;&#26500;&#21270;&#20107;&#20214;&#24207;&#21015;&#19981;&#21487;&#29992;&#65292;&#33258;&#21160;&#21270;&#24207;&#21015;&#25552;&#21462;&#23548;&#33268;&#30340;&#25968;&#25454;&#22826;&#22024;&#26434;&#21644;&#19981;&#23436;&#25972;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#21487;&#20197;&#26377;&#25928;&#29992;&#20110;&#27010;&#29575;&#20107;&#20214;&#27169;&#22411;&#26500;&#24314;&#30340;&#20107;&#20214;&#24207;&#21015;&#30340;&#26041;&#27861;&#12290;&#36825;&#21487;&#20197;&#30475;&#20316;&#26159;&#20174;LLMs&#20013;&#25552;&#21462;&#20107;&#20214;&#24207;&#21015;&#30693;&#35782;&#30340;&#19968;&#31181;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#20010;&#20855;&#26377;&#37096;&#20998;&#22240;&#26524;&#20851;&#31995;&#30340;&#20107;&#20214;&#27010;&#24565;&#30340;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#26469;&#25351;&#23548;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22240;&#26524;&#20107;&#20214;&#24207;&#21015;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20107;&#20214;&#24207;&#21015;&#65292;&#22635;&#34917;&#20102;&#36755;&#20837;KG&#20013;&#30340;&#30693;&#35782;&#31354;&#30333;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#22914;&#20309;&#21033;&#29992;&#29983;&#25104;&#30340;&#24207;&#21015;&#26469;&#21457;&#29616;&#26356;&#26377;&#29992;&#21644;&#26356;&#22797;&#26434;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event sequence models have been found to be highly effective in the analysis and prediction of events. Building such models requires availability of abundant high-quality event sequence data. In certain applications, however, clean structured event sequences are not available, and automated sequence extraction results in data that is too noisy and incomplete. In this work, we explore the use of Large Language Models (LLMs) to generate event sequences that can effectively be used for probabilistic event model construction. This can be viewed as a mechanism of distilling event sequence knowledge from LLMs. Our approach relies on a Knowledge Graph (KG) of event concepts with partial causal relations to guide the generative language model for causal event sequence generation. We show that our approach can generate high-quality event sequences, filling a knowledge gap in the input KG. Furthermore, we explore how the generated sequences can be leveraged to discover useful and more complex st
&lt;/p&gt;</description></item><item><title>RO-LMM&#26159;&#19968;&#20010;&#38024;&#23545;&#25918;&#23556;&#32959;&#30244;&#23398;&#39046;&#22495;&#35774;&#35745;&#30340;&#22810;&#21151;&#33021;&#22823;&#22411;&#22810;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;Consistency Embedding Fine-Tuning&#65288;CEFTune&#65289;&#25216;&#26415;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#20445;&#25345;&#22788;&#29702;&#24178;&#20928;&#36755;&#20837;&#33021;&#21147;&#30340;&#21516;&#26102;&#25552;&#21319;&#23545;&#22024;&#26434;&#36755;&#20837;&#30340;&#40065;&#26834;&#24615;&#65292;&#29992;&#20110;&#25918;&#23556;&#27835;&#30103;&#35745;&#21010;&#21644;&#30446;&#26631;&#20307;&#31215;&#20998;&#21106;&#12290;</title><link>https://arxiv.org/abs/2311.15876</link><description>&lt;p&gt;
LMM&#36741;&#21161;&#30340;&#19968;&#33268;&#24615;&#23884;&#20837;&#19979;&#20083;&#33146;&#30284;&#27835;&#30103;&#30446;&#26631;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
LMM-Assisted Breast Cancer Treatment Target Segmentation with Consistency Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15876
&lt;/p&gt;
&lt;p&gt;
RO-LMM&#26159;&#19968;&#20010;&#38024;&#23545;&#25918;&#23556;&#32959;&#30244;&#23398;&#39046;&#22495;&#35774;&#35745;&#30340;&#22810;&#21151;&#33021;&#22823;&#22411;&#22810;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;Consistency Embedding Fine-Tuning&#65288;CEFTune&#65289;&#25216;&#26415;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#20445;&#25345;&#22788;&#29702;&#24178;&#20928;&#36755;&#20837;&#33021;&#21147;&#30340;&#21516;&#26102;&#25552;&#21319;&#23545;&#22024;&#26434;&#36755;&#20837;&#30340;&#40065;&#26834;&#24615;&#65292;&#29992;&#20110;&#25918;&#23556;&#27835;&#30103;&#35745;&#21010;&#21644;&#30446;&#26631;&#20307;&#31215;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#28145;&#21051;&#24433;&#21709;&#20102;&#21307;&#23398;&#39046;&#22495;&#65292;&#20026;&#38477;&#20302;&#20020;&#24202;&#24037;&#20316;&#37327;&#25552;&#20379;&#20102;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21463;&#38480;&#20110;&#25191;&#34892;&#21333;&#27169;&#24335;&#20219;&#21153;&#65292;&#19982;&#21307;&#23398;&#19987;&#19994;&#20154;&#21592;&#25152;&#20351;&#29992;&#30340;&#32508;&#21512;&#26041;&#27861;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;RO-LMM&#65292;&#19968;&#20010;&#19987;&#20026;&#25918;&#23556;&#32959;&#30244;&#23398;&#39046;&#22495;&#35774;&#35745;&#30340;&#22810;&#21151;&#33021;&#22823;&#22411;&#22810;&#27169;&#22411;&#65288;LMM&#65289;&#12290;&#35813;&#27169;&#22411;&#28085;&#30422;&#20102;&#20020;&#24202;&#24037;&#20316;&#27969;&#20013;&#30340;&#19968;&#31995;&#21015;&#20219;&#21153;&#65292;&#25797;&#38271;&#20020;&#24202;&#25253;&#21578;&#25688;&#35201;&#12289;&#25918;&#30103;&#27835;&#30103;&#35745;&#21010;&#24314;&#35758;&#21644;&#35745;&#21010;&#24341;&#23548;&#30340;&#30446;&#26631;&#20307;&#31215;&#20998;&#21106;&#12290;&#20026;&#20102;&#25191;&#34892;&#36830;&#32493;&#30340;&#20020;&#24202;&#20219;&#21153;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#33268;&#24615;&#23884;&#20837;&#24494;&#35843;&#65288;CEFTune&#65289;&#25216;&#26415;&#65292;&#25552;&#21319;&#20102;LMM&#23545;&#22024;&#26434;&#36755;&#20837;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#22788;&#29702;&#24178;&#20928;&#36755;&#20837;&#30340;&#33021;&#21147;&#65292;&#24182;&#23558;&#35813;&#27010;&#24565;&#36716;&#21270;&#20026;LMM&#39537;&#21160;&#30340;&#20998;&#21106;&#26694;&#26550;&#65292;&#21363;&#19968;&#33268;&#24615;&#23884;&#20837;S&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15876v2 Announce Type: replace-cross  Abstract: Recent advancements in Artificial Intelligence (AI) have profoundly influenced medical fields, by providing tools to reduce clinical workloads. However, most AI models are constrained to execute unimodal tasks, in stark contrast to the comprehensive approaches utilized by medical professionals. To address this, here we present RO-LMM, a multi-purpose large multimodal model (LMM) tailored for the field of radiation oncology. This model covers series of tasks within clinical workflow, adept at clinical report summarization, radiation treatment plan suggestion, and plan-guided target volume segmentation. In particular, to perform consecutive clinical tasks, we further present a novel Consistency Embedding Fine-Tuning (CEFTune) technique, which boosts LMM's robustness to noisy inputs while preserving the capability of handling clean inputs, and transform this concept into LMM-driven segmentation framework as Consistency Embedding S
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#30740;&#31350;&#25551;&#36848;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#65292;&#36890;&#36807;&#36861;&#36394;&#21644;&#27169;&#25311;&#29289;&#20307;&#24863;&#30693;&#20197;&#21450;&#20854;&#22312;&#20132;&#27969;&#20013;&#25152;&#20256;&#36798;&#30340;&#34920;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#30340;&#24847;&#35782;&#12290;&#30456;&#27604;&#20110;&#22823;&#22810;&#25968;&#26080;&#27861;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#35299;&#37322;&#24615;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#26500;&#24314;&#26032;&#32593;&#32476;&#26469;&#23450;&#20041;&#29289;&#20307;&#24863;&#30693;&#12290;</title><link>https://arxiv.org/abs/2310.05212</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#31526;&#21495;&#32593;&#32476;&#20195;&#34920;&#24847;&#35782;&#30340;&#30693;&#35273;
&lt;/p&gt;
&lt;p&gt;
Interpretable Semiotics Networks Representing Awareness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05212
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#25551;&#36848;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#65292;&#36890;&#36807;&#36861;&#36394;&#21644;&#27169;&#25311;&#29289;&#20307;&#24863;&#30693;&#20197;&#21450;&#20854;&#22312;&#20132;&#27969;&#20013;&#25152;&#20256;&#36798;&#30340;&#34920;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#30340;&#24847;&#35782;&#12290;&#30456;&#27604;&#20110;&#22823;&#22810;&#25968;&#26080;&#27861;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#35299;&#37322;&#24615;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#26500;&#24314;&#26032;&#32593;&#32476;&#26469;&#23450;&#20041;&#29289;&#20307;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#27599;&#22825;&#37117;&#24863;&#30693;&#29289;&#20307;&#65292;&#24182;&#36890;&#36807;&#21508;&#31181;&#28192;&#36947;&#20256;&#36798;&#20182;&#20204;&#30340;&#24863;&#30693;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#65292;&#36861;&#36394;&#21644;&#27169;&#25311;&#29289;&#20307;&#30340;&#24863;&#30693;&#20197;&#21450;&#23427;&#20204;&#22312;&#20132;&#27969;&#20013;&#25152;&#20256;&#36798;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#20869;&#37096;&#34920;&#31034;&#30340;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65288;"&#35266;&#23519;&#21040;&#30340;"&#21644;"&#30475;&#21040;&#30340;"&#65289;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#29087;&#24713;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27010;&#24565;&#65288;&#32534;&#30721;&#21644;&#35299;&#30721;&#65289;&#30456;&#20851;&#32852;&#12290;&#36825;&#20123;&#20803;&#32032;&#34987;&#21512;&#24182;&#22312;&#19968;&#36215;&#24418;&#25104;&#31526;&#21495;&#32593;&#32476;&#65292;&#27169;&#25311;&#20102;&#29289;&#20307;&#24863;&#30693;&#21644;&#20154;&#31867;&#20132;&#27969;&#20013;&#30340;&#24847;&#35782;&#12290;&#22914;&#20170;&#65292;&#22823;&#22810;&#25968;&#31070;&#32463;&#32593;&#32476;&#37117;&#26159;&#19981;&#21487;&#35299;&#37322;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20811;&#26381;&#20102;&#36825;&#20010;&#38480;&#21046;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#21487;&#35265;&#24615;&#12290;&#25105;&#20204;&#20154;&#30340;&#29289;&#20307;&#24863;&#30693;&#27169;&#22411;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#32593;&#32476;&#23450;&#20041;&#29289;&#20307;&#24863;&#30693;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#21253;&#25324;&#22522;&#20934;&#20998;&#31867;&#22120;&#21644;&#39069;&#22806;&#23618;&#30340;&#26032;&#32593;&#32476;&#26469;&#28436;&#31034;&#36825;&#19968;&#28857;&#12290;&#36825;&#20010;&#23618;&#20135;&#29983;&#20102;&#22270;&#20687;&#30340;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans perceive objects daily and communicate their perceptions using various channels. Here, we describe a computational model that tracks and simulates objects' perception and their representations as they are conveyed in communication.   We describe two key components of our internal representation ("observed" and "seen") and relate them to familiar computer vision notions (encoding and decoding). These elements are joined together to form semiotics networks, which simulate awareness in object perception and human communication.   Nowadays, most neural networks are uninterpretable. On the other hand, our model overcomes this limitation. The experiments demonstrates the visibility of the model.   Our model of object perception by a person allows us to define object perception by a network. We demonstrate this with an example of an image baseline classifier by constructing a new network that includes the baseline classifier and an additional layer. This layer produces the images "perc
&lt;/p&gt;</description></item><item><title>AutoRT&#26159;&#19968;&#20010;&#21033;&#29992;&#29616;&#26377;&#30340;&#22522;&#30784;&#27169;&#22411;&#26469;&#25193;&#23637;&#26426;&#22120;&#20154;&#22312;&#26410;&#30693;&#22330;&#26223;&#20013;&#30340;&#37096;&#32626;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#22810;&#26679;&#21270;&#21644;&#26032;&#39062;&#30340;&#25351;&#20196;&#65292;&#24182;&#26377;&#25928;&#22320;&#25512;&#29702;&#33258;&#20027;&#26435;&#21644;&#23433;&#20840;&#24615;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2401.12963</link><description>&lt;p&gt;
AutoRT&#65306;&#22823;&#35268;&#27169;&#32534;&#25490;&#26426;&#22120;&#20154;&#20195;&#29702;&#30340;&#20855;&#36523;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents. (arXiv:2401.12963v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12963
&lt;/p&gt;
&lt;p&gt;
AutoRT&#26159;&#19968;&#20010;&#21033;&#29992;&#29616;&#26377;&#30340;&#22522;&#30784;&#27169;&#22411;&#26469;&#25193;&#23637;&#26426;&#22120;&#20154;&#22312;&#26410;&#30693;&#22330;&#26223;&#20013;&#30340;&#37096;&#32626;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#22810;&#26679;&#21270;&#21644;&#26032;&#39062;&#30340;&#25351;&#20196;&#65292;&#24182;&#26377;&#25928;&#22320;&#25512;&#29702;&#33258;&#20027;&#26435;&#21644;&#23433;&#20840;&#24615;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25317;&#26377;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#34892;&#21160;&#31561;&#21151;&#33021;&#30340;&#20855;&#36523;&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#21033;&#29992;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#25968;&#25454;&#26469;&#25512;&#29702;&#26377;&#29992;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#20855;&#36523;&#22522;&#30784;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#32570;&#20047;&#22522;&#20110;&#29289;&#29702;&#19990;&#30028;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoRT&#65292;&#19968;&#20010;&#21033;&#29992;&#29616;&#26377;&#30340;&#22522;&#30784;&#27169;&#22411;&#26469;&#25193;&#23637;&#23436;&#20840;&#26410;&#30693;&#22330;&#26223;&#20013;&#25805;&#20316;&#26426;&#22120;&#20154;&#30340;&#37096;&#32626;&#30340;&#31995;&#32479;&#65292;&#21482;&#38656;&#35201;&#26368;&#23569;&#30340;&#20154;&#24037;&#30417;&#30563;&#12290;AutoRT&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(VLMs)&#23454;&#29616;&#22330;&#26223;&#29702;&#35299;&#21644;&#22522;&#30784;&#32465;&#23450;&#65292;&#24182;&#36827;&#19968;&#27493;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25552;&#20986;&#22810;&#26679;&#21270;&#21644;&#26032;&#39062;&#30340;&#25351;&#20196;&#65292;&#20379;&#19968;&#32452;&#26426;&#22120;&#20154;&#25191;&#34892;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#30693;&#35782;&#26469;&#25351;&#23548;&#25968;&#25454;&#25910;&#38598;&#65292;AutoRT&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#29702;&#33258;&#20027;&#26435;&#21644;&#23433;&#20840;&#24615;&#30340;&#26435;&#34913;&#65292;&#21516;&#26102;&#26174;&#33879;&#25193;&#22823;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#25968;&#25454;&#25910;&#38598;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;AutoRT&#21521;20&#22810;&#20010;&#26426;&#22120;&#20154;&#25552;&#35758;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models that incorporate language, vision, and more recently actions have revolutionized the ability to harness internet scale data to reason about useful tasks. However, one of the key challenges of training embodied foundation models is the lack of data grounded in the physical world. In this paper, we propose AutoRT, a system that leverages existing foundation models to scale up the deployment of operational robots in completely unseen scenarios with minimal human supervision. AutoRT leverages vision-language models (VLMs) for scene understanding and grounding, and further uses large language models (LLMs) for proposing diverse and novel instructions to be performed by a fleet of robots. Guiding data collection by tapping into the knowledge of foundation models enables AutoRT to effectively reason about autonomy tradeoffs and safety while significantly scaling up data collection for robot learning. We demonstrate AutoRT proposing instructions to over 20 robots across multi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20020;&#24202;&#25991;&#26723;&#21160;&#24577;&#38382;&#31572;&#30340;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#12290;&#36890;&#36807;Langchain&#21644;Transformer-based LLMs&#39537;&#21160;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#29992;&#25143;&#21487;&#20197;&#29992;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#20020;&#24202;&#31508;&#35760;&#24182;&#33719;&#24471;&#30456;&#20851;&#31572;&#26696;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;Wizard Vicuna&#20855;&#26377;&#20986;&#33394;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#35745;&#31639;&#35201;&#27714;&#36739;&#39640;&#12290;&#27169;&#22411;&#20248;&#21270;&#26041;&#26696;&#25552;&#39640;&#20102;&#32422;48&#20493;&#30340;&#24310;&#36831;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#20135;&#29983;&#24187;&#35937;&#21644;&#22810;&#26679;&#21270;&#21307;&#30103;&#26696;&#20363;&#35780;&#20272;&#30340;&#38480;&#21046;&#20173;&#28982;&#23384;&#22312;&#12290;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#23545;&#20110;&#21457;&#25496;&#20020;&#24202;&#31508;&#35760;&#30340;&#20215;&#20540;&#21644;&#25512;&#36827;&#22522;&#20110;AI&#30340;&#20020;&#24202;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2401.10733</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20020;&#24202;&#25991;&#26723;&#30340;&#21160;&#24577;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Dynamic Q&amp;A of Clinical Documents with Large Language Models. (arXiv:2401.10733v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20020;&#24202;&#25991;&#26723;&#21160;&#24577;&#38382;&#31572;&#30340;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#12290;&#36890;&#36807;Langchain&#21644;Transformer-based LLMs&#39537;&#21160;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#29992;&#25143;&#21487;&#20197;&#29992;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#20020;&#24202;&#31508;&#35760;&#24182;&#33719;&#24471;&#30456;&#20851;&#31572;&#26696;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;Wizard Vicuna&#20855;&#26377;&#20986;&#33394;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#35745;&#31639;&#35201;&#27714;&#36739;&#39640;&#12290;&#27169;&#22411;&#20248;&#21270;&#26041;&#26696;&#25552;&#39640;&#20102;&#32422;48&#20493;&#30340;&#24310;&#36831;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#20135;&#29983;&#24187;&#35937;&#21644;&#22810;&#26679;&#21270;&#21307;&#30103;&#26696;&#20363;&#35780;&#20272;&#30340;&#38480;&#21046;&#20173;&#28982;&#23384;&#22312;&#12290;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#23545;&#20110;&#21457;&#25496;&#20020;&#24202;&#31508;&#35760;&#30340;&#20215;&#20540;&#21644;&#25512;&#36827;&#22522;&#20110;AI&#30340;&#20020;&#24202;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#25910;&#24405;&#20102;&#20020;&#24202;&#31508;&#35760;&#20013;&#30340;&#37325;&#35201;&#24739;&#32773;&#25968;&#25454;&#12290;&#38543;&#30528;&#36825;&#20123;&#31508;&#35760;&#25968;&#37327;&#21644;&#22797;&#26434;&#24230;&#30340;&#22686;&#21152;&#65292;&#25163;&#21160;&#25552;&#21462;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#65292;&#29992;&#20110;&#23545;&#20020;&#24202;&#31508;&#35760;&#36827;&#34892;&#21160;&#24577;&#38382;&#31572;&#12290;&#25105;&#20204;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30001;Langchain&#21644;&#22522;&#20110;Transformer&#30340;LLMs&#39537;&#21160;&#65292;&#20801;&#35768;&#29992;&#25143;&#29992;&#33258;&#28982;&#35821;&#35328;&#21457;&#20986;&#26597;&#35810;&#65292;&#24182;&#20174;&#20020;&#24202;&#31508;&#35760;&#20013;&#33719;&#24471;&#30456;&#20851;&#31572;&#26696;&#12290;&#36890;&#36807;&#20351;&#29992;&#21508;&#31181;&#23884;&#20837;&#27169;&#22411;&#21644;&#20808;&#36827;&#30340;LLMs&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;Wizard Vicuna&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#23613;&#31649;&#35745;&#31639;&#35201;&#27714;&#36739;&#39640;&#12290;&#27169;&#22411;&#20248;&#21270;&#65292;&#21253;&#25324;&#26435;&#37325;&#37327;&#21270;&#65292;&#23558;&#24310;&#36831;&#25552;&#39640;&#20102;&#32422;48&#20493;&#12290;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#26174;&#31034;&#20102;&#20020;&#24202;&#31508;&#35760;&#20013;&#30340;&#20215;&#20540;&#28508;&#21147;&#65292;&#20294;&#20173;&#23384;&#22312;&#27169;&#22411;&#20135;&#29983;&#24187;&#35937;&#21644;&#26377;&#38480;&#30340;&#22810;&#26679;&#21270;&#21307;&#30103;&#26696;&#20363;&#35780;&#20272;&#31561;&#25361;&#25112;&#12290;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#23545;&#20110;&#21457;&#25496;&#20020;&#24202;&#31508;&#35760;&#30340;&#20215;&#20540;&#21644;&#25512;&#21160;AI&#39537;&#21160;&#30340;&#20020;&#24202;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic health records (EHRs) house crucial patient data in clinical notes. As these notes grow in volume and complexity, manual extraction becomes challenging. This work introduces a natural language interface using large language models (LLMs) for dynamic question-answering on clinical notes. Our chatbot, powered by Langchain and transformer-based LLMs, allows users to query in natural language, receiving relevant answers from clinical notes. Experiments, utilizing various embedding models and advanced LLMs, show Wizard Vicuna's superior accuracy, albeit with high compute demands. Model optimization, including weight quantization, improves latency by approximately 48 times. Promising results indicate potential, yet challenges such as model hallucinations and limited diverse medical case evaluations remain. Addressing these gaps is crucial for unlocking the value in clinical notes and advancing AI-driven clinical decision-making.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22266;&#23450;&#21442;&#25968;&#21487;&#22788;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#25968;&#20855;&#26377;&#30456;&#21516;&#39592;&#26550;&#30340;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#12290;</title><link>http://arxiv.org/abs/2310.04218</link><description>&lt;p&gt;
&#19968;&#20010;&#21487;&#35745;&#25968;&#20855;&#26377;&#30456;&#21516;&#39592;&#26550;&#30340;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#30340;&#22266;&#23450;&#21442;&#25968;&#21487;&#22788;&#29702;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Fixed-Parameter Tractable Algorithm for Counting Markov Equivalence Classes with the same Skeleton. (arXiv:2310.04218v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22266;&#23450;&#21442;&#25968;&#21487;&#22788;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#25968;&#20855;&#26377;&#30456;&#21516;&#39592;&#26550;&#30340;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;&#20063;&#31216;&#20026;&#36125;&#21494;&#26031;&#32593;&#32476;&#65289;&#26159;&#32534;&#30721;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#30340;&#27969;&#34892;&#24037;&#20855;&#12290;&#22312;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#20013;&#65292;&#38543;&#26426;&#21464;&#37327;&#34987;&#24314;&#27169;&#20026;&#26377;&#21521;&#22270;&#20013;&#30340;&#39030;&#28857;&#65292;&#24182;&#19988;&#35268;&#23450;&#27599;&#20010;&#38543;&#26426;&#21464;&#37327;&#22312;&#32473;&#23450;&#20854;&#29238;&#33410;&#28857;&#30340;&#24773;&#20917;&#19979;&#19982;&#20854;&#31062;&#20808;&#33410;&#28857;&#26080;&#20851;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21516;&#19968;&#32452;&#38543;&#26426;&#21464;&#37327;&#19978;&#30340;&#20004;&#20010;&#19981;&#21516;&#30340;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#21487;&#20197;&#20934;&#30830;&#32534;&#30721;&#30456;&#21516;&#30340;&#19968;&#32452;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#12290;&#36825;&#26679;&#30340;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#34987;&#31216;&#20026;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#65292;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#30340;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#31561;&#20215;&#31867;&#34987;&#31216;&#20026;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#65288;MEC&#65289;&#12290;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#23545;&#20110;MEC&#24050;&#32463;&#21019;&#24314;&#20102;&#19968;&#20123;&#32654;&#20029;&#30340;&#32452;&#21512;&#29305;&#24449;&#65292;&#24182;&#19988;&#24050;&#30693;&#65292;&#29305;&#21035;&#26159;&#22312;&#21516;&#19968;MEC&#20013;&#30340;&#25152;&#26377;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#24517;&#39035;&#20855;&#26377;&#30456;&#21516;&#30340;&#8220;&#39592;&#26550;&#8221;&#65288;&#24213;&#23618;&#26080;&#21521;&#22270;&#65289;&#21644;v-&#32467;&#26500;&#65288;&#24418;&#24335;&#20026;$a\rightarrow b \leftarrow c$&#30340;&#35825;&#23548;&#23376;&#22270;&#65289;&#12290;&#36825;&#20123;&#32452;&#21512;&#29305;&#24449;&#36824;&#25552;&#20986;&#20102;&#20960;&#20010;&#33258;&#28982;&#30340;&#31639;&#27861;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal DAGs (also known as Bayesian networks) are a popular tool for encoding conditional dependencies between random variables. In a causal DAG, the random variables are modeled as vertices in the DAG, and it is stipulated that every random variable is independent of its ancestors conditioned on its parents. It is possible, however, for two different causal DAGs on the same set of random variables to encode exactly the same set of conditional dependencies. Such causal DAGs are said to be Markov equivalent, and equivalence classes of Markov equivalent DAGs are known as Markov Equivalent Classes (MECs). Beautiful combinatorial characterizations of MECs have been developed in the past few decades, and it is known, in particular that all DAGs in the same MEC must have the same ''skeleton'' (underlying undirected graph) and v-structures (induced subgraph of the form $a\rightarrow b \leftarrow c$).  These combinatorial characterizations also suggest several natural algorithmic questions. On
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#22810;&#35282;&#24230;&#33258;&#19968;&#33268;&#24615;&#65288;MPSC&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#30340;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20174;&#22810;&#20010;&#35282;&#24230;&#37319;&#26679;&#22810;&#20010;&#36755;&#20986;&#24182;&#26500;&#24314;&#19968;&#20010;&#22810;&#37096;&#20998;&#22270;&#65292;&#21033;&#29992;&#20132;&#21449;&#19968;&#33268;&#24615;&#21644;&#20869;&#19968;&#33268;&#24615;&#20449;&#24687;&#26469;&#36873;&#25321;&#26368;&#20248;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2309.17272</link><description>&lt;p&gt;
&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#32534;&#30721;&#20013;&#30340;&#33021;&#21147;&#36890;&#36807;&#22810;&#35282;&#24230;&#33258;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency. (arXiv:2309.17272v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#22810;&#35282;&#24230;&#33258;&#19968;&#33268;&#24615;&#65288;MPSC&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#30340;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20174;&#22810;&#20010;&#35282;&#24230;&#37319;&#26679;&#22810;&#20010;&#36755;&#20986;&#24182;&#26500;&#24314;&#19968;&#20010;&#22810;&#37096;&#20998;&#22270;&#65292;&#21033;&#29992;&#20132;&#21449;&#19968;&#33268;&#24615;&#21644;&#20869;&#19968;&#33268;&#24615;&#20449;&#24687;&#26469;&#36873;&#25321;&#26368;&#20248;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#65292;&#22914;&#20195;&#30721;&#29983;&#25104;&#20013;&#65292;LLMs&#20173;&#28982;&#38590;&#20197;&#22312;&#19968;&#27425;&#23581;&#35797;&#20013;&#29983;&#25104;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#32858;&#21512;&#22810;&#20010;&#36755;&#20986;&#65292;&#21033;&#29992;&#23427;&#20204;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#26469;&#25506;&#32034;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#27809;&#26377;&#20840;&#38754;&#22320;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#25429;&#25417;&#36825;&#31181;&#19968;&#33268;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#35282;&#24230;&#33258;&#19968;&#33268;&#24615;&#65288;MPSC&#65289;&#26694;&#26550;&#30340;&#26032;&#30340;&#35299;&#30721;&#31574;&#30053;&#65292;&#29992;&#20110;LLM&#65292;&#23427;&#23558;&#26469;&#33258;&#22810;&#20010;&#35282;&#24230;&#30340;&#36755;&#20986;&#20043;&#38388;&#30340;&#20132;&#21449;&#19968;&#33268;&#24615;&#21644;&#21333;&#20010;&#35282;&#24230;&#20869;&#30340;&#20869;&#19968;&#33268;&#24615;&#32467;&#21512;&#36215;&#26469;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35201;&#27714;LLMs&#23545;&#32473;&#23450;&#26597;&#35810;&#20174;&#21508;&#20010;&#35282;&#24230;&#37319;&#26679;&#22810;&#20010;&#22810;&#26679;&#21270;&#30340;&#36755;&#20986;&#65292;&#24182;&#22522;&#20110;&#23427;&#20204;&#26500;&#24314;&#19968;&#20010;&#22810;&#37096;&#20998;&#22270;&#12290;&#36890;&#36807;&#20004;&#20010;&#39044;&#23450;&#20041;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#65292;&#25105;&#20204;&#23558;&#20132;&#21449;&#19968;&#33268;&#24615;&#21644;&#20869;&#19968;&#33268;&#24615;&#20449;&#24687;&#23884;&#20837;&#21040;&#22270;&#20013;&#12290;&#26368;&#20339;&#36873;&#25321;&#26159;&#26681;&#25454;&#36825;&#20123;&#19968;&#33268;&#24615;&#24230;&#37327;&#26469;&#36873;&#25321;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have exhibited remarkable ability in textual generation. However, in complex reasoning tasks such as code generation, generating the correct answer in a single attempt remains a formidable challenge for LLMs. Previous research has explored solutions by aggregating multiple outputs, leveraging the consistency among them. However, none of them have comprehensively captured this consistency from different perspectives. In this paper, we propose the Multi-Perspective Self-Consistency (MPSC) framework, a novel decoding strategy for LLM that incorporates both inter-consistency across outputs from multiple perspectives and intra-consistency within a single perspective. Specifically, we ask LLMs to sample multiple diverse outputs from various perspectives for a given query and then construct a multipartite graph based on them. With two predefined measures of consistency, we embed both inter- and intra-consistency information into the graph. The optimal choice is th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;K-Means&#21644;P-Median&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36947;&#36335;&#32593;&#32476;&#36317;&#31163;&#26469;&#20248;&#21270;&#22312;&#22478;&#24066;&#29615;&#22659;&#19979;&#29289;&#27969;&#38598;&#25955;&#22320;&#30340;&#20301;&#32622;&#24067;&#32622;&#65292;&#20197;&#20943;&#23569;&#37197;&#36865;&#36317;&#31163;&#21644;&#30899;&#36275;&#36857;&#12290;</title><link>http://arxiv.org/abs/2308.11038</link><description>&lt;p&gt;
&#29289;&#27969;&#38598;&#25955;&#22320;&#20301;&#32622;&#20248;&#21270;&#65306;&#19968;&#31181;&#22522;&#20110;K-Means&#21644;P-Median&#27169;&#22411;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#21033;&#29992;&#36947;&#36335;&#32593;&#32476;&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
Logistics Hub Location Optimization: A K-Means and P-Median Model Hybrid Approach Using Road Network Distances. (arXiv:2308.11038v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;K-Means&#21644;P-Median&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36947;&#36335;&#32593;&#32476;&#36317;&#31163;&#26469;&#20248;&#21270;&#22312;&#22478;&#24066;&#29615;&#22659;&#19979;&#29289;&#27969;&#38598;&#25955;&#22320;&#30340;&#20301;&#32622;&#24067;&#32622;&#65292;&#20197;&#20943;&#23569;&#37197;&#36865;&#36317;&#31163;&#21644;&#30899;&#36275;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#27969;&#38598;&#25955;&#22320;&#22312;&#26368;&#21518;&#19968;&#20844;&#37324;&#37197;&#36865;&#36317;&#31163;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65307;&#21363;&#20351;&#36317;&#31163;&#24494;&#23567;&#22686;&#21152;&#20063;&#20250;&#23545;&#30005;&#23376;&#21830;&#21153;&#34892;&#19994;&#30340;&#19994;&#21153;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#21516;&#26102;&#36824;&#20250;&#22686;&#21152;&#20854;&#30899;&#36275;&#36857;&#12290;&#29305;&#21035;&#26159;&#22312;Covid-19&#20043;&#21518;&#65292;&#35813;&#34892;&#19994;&#30340;&#22686;&#38271;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#22312;&#22478;&#24066;&#29615;&#22659;&#20013;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#26469;&#20248;&#21270;&#29289;&#27969;&#38598;&#25955;&#22320;&#30340;&#24067;&#32622;&#12290;&#35813;&#26041;&#27861;&#20381;&#27425;&#37319;&#29992;&#19981;&#21516;&#30340;&#25216;&#26415;&#12290;&#39318;&#20808;&#65292;&#26681;&#25454;&#23427;&#20204;&#30340;&#31354;&#38388;&#20301;&#32622;&#65292;&#20351;&#29992;K-Means&#23545;&#20132;&#20184;&#28857;&#36827;&#34892;&#32858;&#31867;&#12290;&#32858;&#31867;&#26041;&#27861;&#20351;&#29992;&#36947;&#36335;&#32593;&#32476;&#36317;&#31163;&#65292;&#32780;&#19981;&#26159;&#27431;&#20960;&#37324;&#24503;&#36317;&#31163;&#12290;&#36991;&#20813;&#20351;&#29992;&#38750;&#22522;&#20110;&#36947;&#36335;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#20250;&#23548;&#33268;&#38169;&#35823;&#21644;&#35823;&#23548;&#24615;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#20351;&#29992;P-Median&#26041;&#27861;&#30830;&#23450;&#38598;&#25955;&#22320;&#30340;&#20301;&#32622;&#12290;P-Median&#26041;&#27861;&#36824;&#23558;&#20132;&#20184;&#25968;&#37327;&#21644;&#20154;&#21475;&#20316;&#20026;&#26435;&#37325;&#32771;&#34385;&#22312;&#20869;&#12290;&#20351;&#29992;Muller&#21644;Phipps&#65288;M&#65286;P&#65289;&#30340;&#23454;&#38469;&#20132;&#20184;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Logistic hubs play a pivotal role in the last-mile delivery distance; even a slight increment in distance negatively impacts the business of the e-commerce industry while also increasing its carbon footprint. The growth of this industry, particularly after Covid-19, has further intensified the need for optimized allocation of resources in an urban environment. In this study, we use a hybrid approach to optimize the placement of logistic hubs. The approach sequentially employs different techniques. Initially, delivery points are clustered using K-Means in relation to their spatial locations. The clustering method utilizes road network distances as opposed to Euclidean distances. Non-road network-based approaches have been avoided since they lead to erroneous and misleading results. Finally, hubs are located using the P-Median method. The P-Median method also incorporates the number of deliveries and population as weights. Real-world delivery data from Muller and Phipps (M&amp;P) is used to 
&lt;/p&gt;</description></item><item><title>&#31532;&#20108;&#23626;&#33258;&#36866;&#24212;&#32593;&#32476;&#38450;&#24481;&#22269;&#38469;&#30740;&#35752;&#20250;&#30340;&#30446;&#26631;&#26159;&#25506;&#32034;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#33258;&#36866;&#24212;&#32593;&#32476;&#38450;&#24481;&#22522;&#30784;&#33021;&#21147;&#30340;&#30740;&#31350;&#65292;&#24182;&#36890;&#36807;&#22635;&#34917;AI&#21644;&#32593;&#32476;&#30740;&#31350;&#20154;&#21592;&#20043;&#38388;&#30340;&#24046;&#36317;&#26469;&#21152;&#36895;&#24320;&#21457;&#21322;&#33258;&#20027;&#32593;&#32476;&#38450;&#24481;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2308.09520</link><description>&lt;p&gt;
&#31532;&#20108;&#23626;&#33258;&#36866;&#24212;&#32593;&#32476;&#38450;&#24481;&#22269;&#38469;&#30740;&#35752;&#20250;&#35770;&#25991;&#38598;
&lt;/p&gt;
&lt;p&gt;
Proceedings of the 2nd International Workshop on Adaptive Cyber Defense. (arXiv:2308.09520v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09520
&lt;/p&gt;
&lt;p&gt;
&#31532;&#20108;&#23626;&#33258;&#36866;&#24212;&#32593;&#32476;&#38450;&#24481;&#22269;&#38469;&#30740;&#35752;&#20250;&#30340;&#30446;&#26631;&#26159;&#25506;&#32034;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#33258;&#36866;&#24212;&#32593;&#32476;&#38450;&#24481;&#22522;&#30784;&#33021;&#21147;&#30340;&#30740;&#31350;&#65292;&#24182;&#36890;&#36807;&#22635;&#34917;AI&#21644;&#32593;&#32476;&#30740;&#31350;&#20154;&#21592;&#20043;&#38388;&#30340;&#24046;&#36317;&#26469;&#21152;&#36895;&#24320;&#21457;&#21322;&#33258;&#20027;&#32593;&#32476;&#38450;&#24481;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31532;&#20108;&#23626;&#33258;&#36866;&#24212;&#32593;&#32476;&#38450;&#24481;&#22269;&#38469;&#30740;&#35752;&#20250;&#22312;&#20315;&#32599;&#37324;&#36798;&#29702;&#24037;&#23398;&#38498;&#20030;&#34892;&#65292;&#35813;&#30740;&#35752;&#20250;&#26088;&#22312;&#20998;&#20139;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20316;&#20026;&#33258;&#36866;&#24212;&#32593;&#32476;&#38450;&#24481;&#22522;&#30784;&#33021;&#21147;&#30340;&#30740;&#31350;&#12290;&#24403;&#21069;&#30340;&#32593;&#32476;&#39046;&#22495;&#26080;&#27861;&#21487;&#38752;&#26377;&#25928;&#22320;&#36827;&#34892;&#38450;&#24481;&#65292;&#24517;&#39035;&#24191;&#27867;&#20381;&#36182;&#20154;&#24037;&#19987;&#23478;&#12290;&#29087;&#32451;&#30340;&#32593;&#32476;&#38450;&#24481;&#20154;&#21592;&#20379;&#24212;&#19981;&#36275;&#65292;&#24448;&#24448;&#26080;&#27861;&#21450;&#26102;&#24212;&#23545;&#32593;&#32476;&#23041;&#32961;&#12290;&#20511;&#37492;AI&#21644;ML&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#32593;&#32476;&#38450;&#24481;&#30740;&#31350;&#31038;&#21306;&#34987;&#28608;&#21169;&#30528;&#36890;&#36807;&#23558;AI&#21644;ML&#25216;&#26415;&#24212;&#29992;&#20110;&#32593;&#32476;&#29615;&#22659;&#20013;&#65292;&#24320;&#21457;&#26032;&#30340;&#21160;&#24577;&#21487;&#25345;&#32493;&#30340;&#38450;&#24481;&#25514;&#26045;&#12290;&#22635;&#34917;AI&#21644;&#32593;&#32476;&#30740;&#31350;&#20154;&#21592;&#19982;&#23454;&#36341;&#32773;&#20043;&#38388;&#30340;&#20851;&#38190;&#24046;&#36317;&#21487;&#20197;&#21152;&#36895;&#21019;&#24314;&#33021;&#22815;&#23398;&#20064;&#35782;&#21035;&#21644;&#24212;&#23545;&#32593;&#32476;&#25915;&#20987;&#65292;&#25110;&#32773;&#21457;&#29616;&#21644;&#20943;&#36731;&#24369;&#28857;&#30340;&#21322;&#33258;&#20027;&#32593;&#32476;&#38450;&#24481;&#31995;&#32479;&#30340;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The 2nd International Workshop on Adaptive Cyber Defense was held at the Florida Institute of Technology, Florida. This workshop was organized to share research that explores unique applications of Artificial Intelligence (AI) and Machine Learning (ML) as foundational capabilities for the pursuit of adaptive cyber defense. The cyber domain cannot currently be reliably and effectively defended without extensive reliance on human experts. Skilled cyber defenders are in short supply and often cannot respond fast enough to cyber threats.  Building on recent advances in AI and ML the Cyber defense research community has been motivated to develop new dynamic and sustainable defenses through the adoption of AI and ML techniques to cyber settings. Bridging critical gaps between AI and Cyber researchers and practitioners can accelerate efforts to create semi-autonomous cyber defenses that can learn to recognize and respond to cyber attacks or discover and mitigate weaknesses in cooperation with
&lt;/p&gt;</description></item><item><title>APACE&#26159;&#19968;&#20010;&#23558;AlphaFold2&#21644;&#20808;&#36827;&#35745;&#31639;&#20316;&#20026;&#26381;&#21153;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#29616;&#20195;&#36229;&#32423;&#35745;&#31639;&#29615;&#22659;&#20013;&#21152;&#36895;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#20998;&#26512;&#12290;&#30740;&#31350;&#32773;&#22312;Delta&#36229;&#32423;&#35745;&#31639;&#26426;&#20013;&#37096;&#32626;&#20102;APACE&#65292;&#22312;&#20934;&#30830;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.07954</link><description>&lt;p&gt;
APACE: AlphaFold2&#21644;&#20808;&#36827;&#35745;&#31639;&#20316;&#20026;&#21152;&#36895;&#29983;&#29289;&#29289;&#29702;&#23398;&#21457;&#29616;&#30340;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
APACE: AlphaFold2 and advanced computing as a service for accelerated discovery in biophysics. (arXiv:2308.07954v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07954
&lt;/p&gt;
&lt;p&gt;
APACE&#26159;&#19968;&#20010;&#23558;AlphaFold2&#21644;&#20808;&#36827;&#35745;&#31639;&#20316;&#20026;&#26381;&#21153;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#29616;&#20195;&#36229;&#32423;&#35745;&#31639;&#29615;&#22659;&#20013;&#21152;&#36895;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#20998;&#26512;&#12290;&#30740;&#31350;&#32773;&#22312;Delta&#36229;&#32423;&#35745;&#31639;&#26426;&#20013;&#37096;&#32626;&#20102;APACE&#65292;&#22312;&#20934;&#30830;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#27688;&#22522;&#37240;&#24207;&#21015;&#39044;&#27979;&#34507;&#30333;&#36136;&#30340;3D&#32467;&#26500;&#26159;&#29983;&#29289;&#29289;&#29702;&#23398;&#20013;&#30340;&#19968;&#20010;&#35745;&#31639;&#37325;&#22823;&#25361;&#25112;&#65292;&#23427;&#22312;&#31283;&#20581;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#31639;&#27861;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20174;&#33647;&#29289;&#21457;&#29616;&#21040;&#22522;&#22240;&#32452;&#35299;&#35835;&#37117;&#31163;&#19981;&#24320;&#36825;&#20010;&#25216;&#26415;&#12290;AI&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#27604;&#22914;AlphaFold&#65292;&#27491;&#22312;&#38761;&#26032;&#20381;&#36182;&#31283;&#20581;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#31639;&#27861;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#26368;&#22823;&#31243;&#24230;&#22320;&#21457;&#25381;&#36825;&#20123;&#26032;&#22411;AI&#24037;&#20855;&#30340;&#24433;&#21709;&#21147;&#21644;&#26131;&#29992;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;APACE&#65292;AlphaFold2&#21644;&#20808;&#36827;&#35745;&#31639;&#20316;&#20026;&#26381;&#21153;&#30340;&#26032;&#22411;&#35745;&#31639;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#36825;&#20010;AI&#27169;&#22411;&#21450;&#20854;TB&#32423;&#22823;&#23567;&#30340;&#25968;&#25454;&#24211;&#65292;&#20197;&#22312;&#29616;&#20195;&#36229;&#32423;&#35745;&#31639;&#29615;&#22659;&#20013;&#21152;&#36895;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#20998;&#26512;&#12290;&#25105;&#20204;&#23558;APACE&#37096;&#32626;&#22312;Delta&#36229;&#32423;&#35745;&#31639;&#26426;&#20013;&#65292;&#24182;&#20351;&#29992;&#22235;&#20010;&#31034;&#20363;&#34507;&#30333;&#36136;&#65288;6AWO&#65292;6OAN&#65292;7MEZ&#21644;6D6U&#65289;&#26469;&#37327;&#21270;&#20854;&#22312;&#20934;&#30830;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#22312;Delta&#30340;50&#20010;&#33410;&#28857;&#19978;&#20998;&#24067;&#20102;&#22810;&#36798;200&#20010;&#21512;&#38598;&#65292;&#30456;&#24403;&#20110;200&#20010;A100 NVIDIA GPU&#65292;&#25105;&#20204;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
The prediction of protein 3D structure from amino acid sequence is a computational grand challenge in biophysics, and plays a key role in robust protein structure prediction algorithms, from drug discovery to genome interpretation. The advent of AI models, such as AlphaFold, is revolutionizing applications that depend on robust protein structure prediction algorithms. To maximize the impact, and ease the usability, of these novel AI tools we introduce APACE, AlphaFold2 and advanced computing as a service, a novel computational framework that effectively handles this AI model and its TB-size database to conduct accelerated protein structure prediction analyses in modern supercomputing environments. We deployed APACE in the Delta supercomputer, and quantified its performance for accurate protein structure predictions using four exemplar proteins: 6AWO, 6OAN, 7MEZ, and 6D6U. Using up to 200 ensembles, distributed across 50 nodes in Delta, equivalent to 200 A100 NVIDIA GPUs, we found that 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#35752;&#35770;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#27969;&#34892;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#12289;&#37327;&#21270;&#21644;&#20943;&#23569;&#27969;&#34892;&#20559;&#24046;&#12290;&#23427;&#21516;&#26102;&#25552;&#20379;&#20102;&#35745;&#31639;&#24230;&#37327;&#30340;&#27010;&#36848;&#21644;&#20027;&#35201;&#25216;&#26415;&#26041;&#27861;&#30340;&#22238;&#39038;&#12290;</title><link>http://arxiv.org/abs/2308.01118</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#27969;&#34892;&#20559;&#24046;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Popularity Bias in Recommender Systems. (arXiv:2308.01118v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01118
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#35752;&#35770;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#27969;&#34892;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#12289;&#37327;&#21270;&#21644;&#20943;&#23569;&#27969;&#34892;&#20559;&#24046;&#12290;&#23427;&#21516;&#26102;&#25552;&#20379;&#20102;&#35745;&#31639;&#24230;&#37327;&#30340;&#27010;&#36848;&#21644;&#20027;&#35201;&#25216;&#26415;&#26041;&#27861;&#30340;&#22238;&#39038;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20197;&#20010;&#24615;&#21270;&#30340;&#26041;&#24335;&#24110;&#21161;&#20154;&#20204;&#25214;&#21040;&#30456;&#20851;&#20869;&#23481;&#12290;&#36825;&#20123;&#31995;&#32479;&#30340;&#19968;&#20010;&#20027;&#35201;&#25215;&#35834;&#26159;&#33021;&#22815;&#22686;&#21152;&#30446;&#24405;&#20013;&#36739;&#23569;&#30693;&#21517;&#30340;&#29289;&#21697;&#30340;&#21487;&#35265;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#29616;&#20170;&#30340;&#25512;&#33616;&#31639;&#27861;&#21453;&#32780;&#34920;&#29616;&#20986;&#27969;&#34892;&#20559;&#24046;&#65292;&#21363;&#23427;&#20204;&#22312;&#25512;&#33616;&#20013;&#32463;&#24120;&#20851;&#27880;&#30456;&#24403;&#27969;&#34892;&#30340;&#29289;&#21697;&#12290;&#36825;&#31181;&#20559;&#24046;&#19981;&#20165;&#21487;&#33021;&#23548;&#33268;&#30701;&#26399;&#20869;&#23545;&#28040;&#36153;&#32773;&#21644;&#25552;&#20379;&#32773;&#30340;&#25512;&#33616;&#20215;&#20540;&#26377;&#38480;&#65292;&#32780;&#19988;&#36824;&#21487;&#33021;&#24341;&#36215;&#19981;&#24076;&#26395;&#30340;&#24378;&#21270;&#25928;&#24212;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#27969;&#34892;&#20559;&#24046;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#26816;&#27979;&#12289;&#37327;&#21270;&#21644;&#20943;&#23569;&#25512;&#33616;&#31995;&#32479;&#20013;&#27969;&#34892;&#20559;&#24046;&#30340;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#32508;&#36848;&#26082;&#21253;&#25324;&#20102;&#25991;&#29486;&#20013;&#20351;&#29992;&#30340;&#35745;&#31639;&#24230;&#37327;&#30340;&#27010;&#36848;&#65292;&#20063;&#21253;&#25324;&#20102;&#20943;&#23569;&#20559;&#24046;&#30340;&#20027;&#35201;&#25216;&#26415;&#26041;&#27861;&#30340;&#22238;&#39038;&#12290;&#25105;&#20204;&#36824;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems help people find relevant content in a personalized way. One main promise of such systems is that they are able to increase the visibility of items in the long tail, i.e., the lesser-known items in a catalogue. Existing research, however, suggests that in many situations today's recommendation algorithms instead exhibit a popularity bias, meaning that they often focus on rather popular items in their recommendations. Such a bias may not only lead to limited value of the recommendations for consumers and providers in the short run, but it may also cause undesired reinforcement effects over time. In this paper, we discuss the potential reasons for popularity bias and we review existing approaches to detect, quantify and mitigate popularity bias in recommender systems. Our survey therefore includes both an overview of the computational metrics used in the literature as well as a review of the main technical approaches to reduce the bias. We furthermore critically discu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#33258;&#36523;&#30693;&#35782;&#30340;&#29702;&#35299;&#21644;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#30340;&#33021;&#21147;&#12290;&#35813;&#30740;&#31350;&#32858;&#28966;&#20110;&#35299;&#20915;&#8220;&#24050;&#30693;-&#26410;&#30693;&#8221;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#26041;&#26696;&#65292;&#24182;&#20351;&#29992;&#35821;&#20041;&#35780;&#20272;&#26041;&#27861;&#37327;&#21270;&#20102;&#27169;&#22411;&#34920;&#36798;&#19981;&#30830;&#23450;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13712</link><description>&lt;p&gt;
&#30693;&#35782;&#30340;&#30693;&#35782;&#65306;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26410;&#30693;-&#24050;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty with Large Language Models. (arXiv:2305.13712v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#33258;&#36523;&#30693;&#35782;&#30340;&#29702;&#35299;&#21644;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#30340;&#33021;&#21147;&#12290;&#35813;&#30740;&#31350;&#32858;&#28966;&#20110;&#35299;&#20915;&#8220;&#24050;&#30693;-&#26410;&#30693;&#8221;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#26041;&#26696;&#65292;&#24182;&#20351;&#29992;&#35821;&#20041;&#35780;&#20272;&#26041;&#27861;&#37327;&#21270;&#20102;&#27169;&#22411;&#34920;&#36798;&#19981;&#30830;&#23450;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29702;&#35299;&#33258;&#36523;&#30693;&#35782;&#21644;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#20197;&#32531;&#35299;&#34394;&#26500;&#29616;&#35937;&#12290;&#25105;&#20204;&#19987;&#38376;&#20851;&#27880;&#35299;&#20915;&#8220;&#24050;&#30693;-&#26410;&#30693;&#8221;&#38382;&#39064;&#65292;&#36825;&#31181;&#38382;&#39064;&#30001;&#20110;&#32570;&#20047;&#30830;&#23450;&#30340;&#31572;&#26696;&#32780;&#20855;&#26377;&#39640;&#24230;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#20419;&#36827;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#26032;&#30340;&#24050;&#30693;-&#26410;&#30693;&#38382;&#39064;&#65288;KUQ&#65289;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#26041;&#26696;&#26469;&#38416;&#26126;&#19981;&#30830;&#23450;&#24615;&#30340;&#26469;&#28304;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;LLM&#21306;&#20998;&#24050;&#30693;&#21644;&#26410;&#30693;&#38382;&#39064;&#20197;&#21450;&#30456;&#24212;&#20998;&#31867;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#24320;&#25918;&#24335;QA&#29615;&#22659;&#20013;&#35780;&#20272;LLM&#30340;&#31572;&#26696;&#36136;&#37327;&#12290;&#20026;&#20102;&#37327;&#21270;&#31572;&#26696;&#20013;&#34920;&#36798;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#31181;&#35821;&#20041;&#35780;&#20272;&#26041;&#27861;&#65292;&#29992;&#20110;&#27979;&#37327;&#27169;&#22411;&#22312;&#34920;&#36798;&#24050;&#30693;vs&#26410;&#30693;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the capabilities of Large Language Models (LLMs) in the context of understanding their own knowledge and measuring their uncertainty. We argue this is an important feature for mitigating hallucinations. Specifically, we focus on addressing \textit{known-unknown} questions, characterized by high uncertainty due to the absence of definitive answers. To facilitate our study, we collect a dataset with new Known-Unknown Questions (KUQ) and propose a novel categorization scheme to elucidate the sources of uncertainty. Subsequently, we assess the LLMs' ability to differentiate between known and unknown questions and classify them accordingly. Moreover, we evaluate the quality of their answers in an Open-Ended QA setting. To quantify the uncertainty expressed in the answers, we create a semantic evaluation method that measures the model's accuracy in expressing uncertainty between known vs unknown questions.
&lt;/p&gt;</description></item></channel></rss>