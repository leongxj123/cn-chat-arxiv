<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#65292;&#36890;&#36807;&#23558;LLMs&#38598;&#25104;&#21040;&#20027;&#21160;&#23398;&#20064;&#24490;&#29615;&#20013;&#36827;&#34892;&#25968;&#25454;&#27880;&#37322;&#65292;&#26377;&#25928;&#20943;&#23569;&#25152;&#38656;&#25968;&#25454;&#37327;&#65292;&#24182;&#21462;&#24471;&#25509;&#36817;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.02261</link><description>&lt;p&gt;
&#22312;LLMs&#20013;&#24490;&#29615;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27880;&#37322;&#36827;&#34892;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LLMs in the Loop: Leveraging Large Language Model Annotations for Active Learning in Low-Resource Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02261
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#65292;&#36890;&#36807;&#23558;LLMs&#38598;&#25104;&#21040;&#20027;&#21160;&#23398;&#20064;&#24490;&#29615;&#20013;&#36827;&#34892;&#25968;&#25454;&#27880;&#37322;&#65292;&#26377;&#25928;&#20943;&#23569;&#25152;&#38656;&#25968;&#25454;&#37327;&#65292;&#24182;&#21462;&#24471;&#25509;&#36817;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35821;&#35328;&#36164;&#28304;&#21644;&#25968;&#25454;&#26631;&#27880;&#19987;&#19994;&#30693;&#35782;&#26377;&#38480;&#65292;&#20302;&#36164;&#28304;&#35821;&#35328;&#22312;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#20013;&#38754;&#20020;&#30528;&#37325;&#22823;&#38556;&#30861;&#65292;&#20351;&#23427;&#20204;&#21464;&#24471;&#32597;&#35265;&#19988;&#25104;&#26412;&#39640;&#26114;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#19981;&#36275;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;LLMs&#30340;&#28508;&#21147;&#22312;&#20027;&#21160;&#23398;&#20064;&#29615;&#33410;&#20013;&#36827;&#34892;&#25968;&#25454;&#27880;&#37322;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#35780;&#20272;&#20197;&#35780;&#20272;&#27880;&#37322;&#32773;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#36873;&#25321;&#36866;&#24403;&#30340;LLM&#27880;&#37322;&#32773;&#12290;&#28982;&#21518;&#65292;&#36873;&#25321;&#30340;&#27880;&#37322;&#32773;&#34987;&#38598;&#25104;&#21040;&#19968;&#20010;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#24490;&#29615;&#20013;&#65292;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#65292;&#26368;&#23567;&#21270;&#25152;&#38656;&#30340;&#26597;&#35810;&#25968;&#25454;&#37327;&#12290;&#23454;&#35777;&#35780;&#20272;&#65292;&#29305;&#21035;&#26159;&#20351;&#29992;GPT-4-Turbo&#65292;&#23637;&#31034;&#20102;&#20960;&#20046;&#36798;&#21040;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#22823;&#22823;&#20943;&#23569;&#20102;&#25968;&#25454;&#38656;&#27714;&#65292;&#30001;&#20272;&#31639;&#30340;&#28508;&#22312;&#24615;&#33021;&#25351;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02261v1 Announce Type: cross  Abstract: Low-resource languages face significant barriers in AI development due to limited linguistic resources and expertise for data labeling, rendering them rare and costly. The scarcity of data and the absence of preexisting tools exacerbate these challenges, especially since these languages may not be adequately represented in various NLP datasets. To address this gap, we propose leveraging the potential of LLMs in the active learning loop for data annotation. Initially, we conduct evaluations to assess inter-annotator agreement and consistency, facilitating the selection of a suitable LLM annotator. The chosen annotator is then integrated into a training loop for a classifier using an active learning paradigm, minimizing the amount of queried data required. Empirical evaluations, notably employing GPT-4-Turbo, demonstrate near-state-of-the-art performance with significantly reduced data requirements, as indicated by estimated potential co
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Regularized Best-of-N (RBoN)&#65292;&#36890;&#36807;&#24341;&#20837;&#25509;&#36817;&#24615;&#39033;&#26469;&#20943;&#36731;&#22870;&#21169;&#27450;&#39575;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#22312;&#35299;&#30721;&#26102;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.01054</link><description>&lt;p&gt;
&#27491;&#21017;&#21270;&#30340;&#26368;&#20339;-N&#37319;&#26679;&#20197;&#20943;&#36731;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#20013;&#30340;&#22870;&#21169;&#27450;&#39575;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Regularized Best-of-N Sampling to Mitigate Reward Hacking for Language Model Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01054
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Regularized Best-of-N (RBoN)&#65292;&#36890;&#36807;&#24341;&#20837;&#25509;&#36817;&#24615;&#39033;&#26469;&#20943;&#36731;&#22870;&#21169;&#27450;&#39575;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#22312;&#35299;&#30721;&#26102;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Best-of-N (BoN)&#37319;&#26679;&#19982;&#22870;&#21169;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#22312;&#35299;&#30721;&#26102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;BoN&#37319;&#26679;&#23481;&#26131;&#21463;&#21040;&#22870;&#21169;&#27450;&#39575;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#38450;&#27490;&#22870;&#21169;&#27450;&#39575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Regularized Best-of-N (RBoN)&#30340;&#21464;&#20307;&#65292;&#36890;&#36807;&#22312;&#21709;&#24212;&#36873;&#25321;&#20013;&#32467;&#21512;&#25509;&#36817;&#24615;&#39033;&#26469;&#20943;&#36731;&#22870;&#21169;&#27450;&#39575;&#65292;&#31867;&#20284;&#20110;&#20559;&#22909;&#23398;&#20064;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01054v1 Announce Type: cross  Abstract: Best-of-N (BoN) sampling with a reward model has been shown to be an effective strategy for aligning Large Language Models (LLMs) to human preferences at the time of decoding. BoN sampling is susceptible to a problem known as reward hacking. Because the reward model is an imperfect proxy for the true objective, over-optimizing its value can compromise its performance on the true objective. A common solution to prevent reward hacking in preference learning techniques is to optimize a reward using proximity regularization (e.g., KL regularization), which ensures that the language model remains close to the reference model. In this research, we propose Regularized Best-of-N (RBoN), a variant of BoN that aims to mitigate reward hacking by incorporating a proximity term in response selection, similar to preference learning techniques. We evaluate two variants of RBoN on the AlpacaFarm dataset and find that they outperform BoN, especially wh
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Human-Regularized PPO (HR-PPO)&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#25105;&#21338;&#24328;&#35757;&#32451;&#20195;&#29702;&#65292;&#23454;&#29616;&#22312;&#23553;&#38381;&#29615;&#22659;&#20013;&#36924;&#30495;&#19988;&#26377;&#25928;&#30340;&#39550;&#39542;&#20249;&#20276;</title><link>https://arxiv.org/abs/2403.19648</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#27491;&#21017;&#21270;&#30340;&#33258;&#25105;&#21338;&#24328;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#19982;&#20154;&#31867;&#20860;&#23481;&#30340;&#39550;&#39542;&#20249;&#20276;
&lt;/p&gt;
&lt;p&gt;
Human-compatible driving partners through data-regularized self-play reinforcement learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19648
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Human-Regularized PPO (HR-PPO)&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#25105;&#21338;&#24328;&#35757;&#32451;&#20195;&#29702;&#65292;&#23454;&#29616;&#22312;&#23553;&#38381;&#29615;&#22659;&#20013;&#36924;&#30495;&#19988;&#26377;&#25928;&#30340;&#39550;&#39542;&#20249;&#20276;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#27773;&#36710;&#38754;&#20020;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#26159;&#19982;&#20154;&#31867;&#36827;&#34892;&#21327;&#35843;&#12290;&#22240;&#27492;&#65292;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#65292;&#23558;&#36924;&#30495;&#30340;&#20154;&#31867;&#20195;&#29702;&#32435;&#20837;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#21487;&#25193;&#23637;&#35757;&#32451;&#21644;&#35780;&#20272;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Human-Regularized PPO (HR-PPO)&#30340;&#22810;&#26234;&#33021;&#20307;&#31639;&#27861;&#65292;&#20854;&#20013;&#20195;&#29702;&#36890;&#36807;&#33258;&#25105;&#21338;&#24328;&#36827;&#34892;&#35757;&#32451;&#65292;&#23545;&#20559;&#31163;&#20154;&#31867;&#21442;&#32771;&#31574;&#30053;&#30340;&#34892;&#20026;&#36827;&#34892;&#23567;&#24133;&#24809;&#32602;&#65292;&#20197;&#26500;&#24314;&#22312;&#23553;&#38381;&#29615;&#22659;&#20013;&#26082;&#36924;&#30495;&#21448;&#26377;&#25928;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19648v1 Announce Type: cross  Abstract: A central challenge for autonomous vehicles is coordinating with humans. Therefore, incorporating realistic human agents is essential for scalable training and evaluation of autonomous driving systems in simulation. Simulation agents are typically developed by imitating large-scale, high-quality datasets of human driving. However, pure imitation learning agents empirically have high collision rates when executed in a multi-agent closed-loop setting. To build agents that are realistic and effective in closed-loop settings, we propose Human-Regularized PPO (HR-PPO), a multi-agent algorithm where agents are trained through self-play with a small penalty for deviating from a human reference policy. In contrast to prior work, our approach is RL-first and only uses 30 minutes of imperfect human demonstrations. We evaluate agents in a large set of multi-agent traffic scenes. Results show our HR-PPO agents are highly effective in achieving goa
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#29983;&#25104;AAS&#23454;&#20363;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#25968;&#23383;&#23402;&#29983;&#20013;&#30340;&#20114;&#25805;&#20316;&#24615;&#65292;&#38477;&#20302;&#20102;&#25163;&#21160;&#21019;&#24314;&#25104;&#26412;&#21644;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.17209</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#29983;&#25104;&#36164;&#20135;&#31649;&#29702;&#22806;&#22771;&#65306;&#25968;&#23383;&#23402;&#29983;&#21644;&#35821;&#20041;&#33410;&#28857;&#20013;&#30340;&#20114;&#25805;&#20316;&#24615;
&lt;/p&gt;
&lt;p&gt;
Generation of Asset Administration Shell with Large Language Model Agents: Interoperability in Digital Twins with Semantic Node
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17209
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#29983;&#25104;AAS&#23454;&#20363;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#25968;&#23383;&#23402;&#29983;&#20013;&#30340;&#20114;&#25805;&#20316;&#24615;&#65292;&#38477;&#20302;&#20102;&#25163;&#21160;&#21019;&#24314;&#25104;&#26412;&#21644;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21327;&#21161;&#22312;&#24037;&#19994;4.0&#32972;&#26223;&#19979;&#20026;&#25968;&#23383;&#23402;&#29983;&#24314;&#27169;&#21019;&#24314;&#36164;&#20135;&#31649;&#29702;&#22806;&#22771;&#65288;AAS&#65289;&#23454;&#20363;&#65292;&#26088;&#22312;&#22686;&#24378;&#26234;&#33021;&#21046;&#36896;&#20013;&#30340;&#20114;&#25805;&#20316;&#24615;&#65292;&#20943;&#23569;&#25163;&#21160;&#24037;&#20316;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#8220;&#35821;&#20041;&#33410;&#28857;&#8221;&#25968;&#25454;&#32467;&#26500;&#26469;&#25429;&#25417;&#25991;&#26412;&#25968;&#25454;&#30340;&#35821;&#20041;&#35201;&#20041;&#12290;&#28982;&#21518;&#65292;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#22788;&#29702;&#8220;&#35821;&#20041;&#33410;&#28857;&#8221;&#24182;&#20174;&#25991;&#26412;&#25216;&#26415;&#25968;&#25454;&#29983;&#25104;AAS&#23454;&#20363;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#26377;&#25928;&#29983;&#25104;&#29575;&#20026;62-79%&#65292;&#34920;&#26126;&#30456;&#24403;&#27604;&#20363;&#30340;&#25163;&#21160;&#21019;&#24314;&#24037;&#20316;&#21487;&#20197;&#36716;&#25442;&#20026;&#26356;&#23481;&#26131;&#30340;&#39564;&#35777;&#24037;&#20316;&#65292;&#20174;&#32780;&#20943;&#23569;&#21019;&#24314;AAS&#23454;&#20363;&#27169;&#22411;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#12290;&#22312;&#25105;&#20204;&#30340;&#35780;&#20272;&#20013;&#65292;&#23545;&#19981;&#21516;LLM&#30340;&#27604;&#36739;&#20998;&#26512;&#20197;&#21450;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26426;&#21046;&#30340;&#28145;&#20837;&#28040;&#34701;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20851;LLM&#26377;&#25928;&#24615;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17209v1 Announce Type: new  Abstract: This research introduces a novel approach for assisting the creation of Asset Administration Shell (AAS) instances for digital twin modeling within the context of Industry 4.0, aiming to enhance interoperability in smart manufacturing and reduce manual effort. We construct a "semantic node" data structure to capture the semantic essence of textual data. Then, a system powered by large language models is designed and implemented to process "semantic node" and generate AAS instance models from textual technical data. Our evaluation demonstrates a 62-79% effective generation rate, indicating a substantial proportion of manual creation effort can be converted into easier validation effort, thereby reducing the time and cost in creating AAS instance models. In our evaluation, a comparative analysis of different LLMs and an in-depth ablation study of Retrieval-Augmented Generation (RAG) mechanisms provide insights into the effectiveness of LLM
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23545;25&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#21644;7&#31181;&#30456;&#23545;&#36739;&#39640;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#21450;&#20854;&#36328;&#35821;&#35328;&#21464;&#20307;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#20351;&#29992;LLMs&#36827;&#34892;ICL&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#26367;&#20195;&#26041;&#27861;&#26597;&#35810;&#23545;&#40784;&#65292;&#24182;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;ICL&#25552;&#20379;&#20102;&#23453;&#36149;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.16512</link><description>&lt;p&gt;
LLMs&#26159;&#23569;&#26679;&#26412;&#24773;&#22659;&#20302;&#36164;&#28304;&#35821;&#35328;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
LLMs Are Few-Shot In-Context Low-Resource Language Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16512
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23545;25&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#21644;7&#31181;&#30456;&#23545;&#36739;&#39640;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#21450;&#20854;&#36328;&#35821;&#35328;&#21464;&#20307;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#20351;&#29992;LLMs&#36827;&#34892;ICL&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#26367;&#20195;&#26041;&#27861;&#26597;&#35810;&#23545;&#40784;&#65292;&#24182;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;ICL&#25552;&#20379;&#20102;&#23453;&#36149;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#25903;&#25345;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#21033;&#29992;&#30701;&#26102;&#30340;&#24773;&#22659;&#20449;&#24687;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#65292;&#36825;&#20026;&#32553;&#23567;&#39640;&#36164;&#28304;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#30340;&#24046;&#36317;&#25552;&#20379;&#20102;&#37325;&#35201;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#25506;&#35752;&#20102;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;ICL&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#38598;&#20013;&#22312;&#30456;&#23545;&#39640;&#36164;&#28304;&#30340;&#35821;&#35328;&#65292;&#27604;&#22914;&#27861;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;25&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#21644;7&#31181;&#30456;&#23545;&#36739;&#39640;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;ICL&#21450;&#20854;&#36328;&#35821;&#35328;&#21464;&#20307;&#65288;X-ICL&#65289;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#19981;&#20165;&#35780;&#20272;&#20102;LLMs&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#20351;&#29992;ICL&#30340;&#26377;&#25928;&#24615;&#65292;&#36824;&#21457;&#29616;&#20102;&#24773;&#22659;&#26631;&#31614;&#23545;&#40784;&#30340;&#32570;&#38519;&#65292;&#24182;&#24341;&#20837;&#20102;&#26356;&#26377;&#25928;&#30340;&#26367;&#20195;&#26041;&#27861;&#65306;&#26597;&#35810;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;ICL&#30340;&#21508;&#20010;&#26041;&#38754;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24635;&#32467;&#20102;&#23569;&#26679;&#26412;&#24773;&#22659;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16512v1 Announce Type: cross  Abstract: In-context learning (ICL) empowers large language models (LLMs) to perform diverse tasks in underrepresented languages using only short in-context information, offering a crucial avenue for narrowing the gap between high-resource and low-resource languages. Nonetheless, there is only a handful of works explored ICL for low-resource languages with most of them focusing on relatively high-resource languages, such as French and Spanish. In this work, we extensively study ICL and its cross-lingual variation (X-ICL) on 25 low-resource and 7 relatively higher-resource languages. Our study not only assesses the effectiveness of ICL with LLMs in low-resource languages but also identifies the shortcomings of in-context label alignment, and introduces a more effective alternative: query alignment. Moreover, we provide valuable insights into various facets of ICL for low-resource languages. Our study concludes the significance of few-shot in-cont
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21160;&#20316;&#21452;&#27169;&#25311;&#32534;&#30721;&#65292;&#36890;&#36807;&#36882;&#24402;&#19981;&#21464;&#24615;&#32422;&#26463;&#25193;&#23637;&#20102;&#21333;&#27493;&#25511;&#21046;&#24615;&#65292;&#23398;&#20064;&#20102;&#19968;&#20010;&#21487;&#20197;&#24179;&#28369;&#25240;&#25187;&#36828;&#26399;&#20803;&#32032;&#30340;&#22810;&#27493;&#25511;&#21046;&#24230;&#37327;</title><link>https://arxiv.org/abs/2403.16369</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#21464;&#24615;&#23398;&#20064;&#22522;&#20110;&#21160;&#20316;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Action-based Representations Using Invariance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16369
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21160;&#20316;&#21452;&#27169;&#25311;&#32534;&#30721;&#65292;&#36890;&#36807;&#36882;&#24402;&#19981;&#21464;&#24615;&#32422;&#26463;&#25193;&#23637;&#20102;&#21333;&#27493;&#25511;&#21046;&#24615;&#65292;&#23398;&#20064;&#20102;&#19968;&#20010;&#21487;&#20197;&#24179;&#28369;&#25240;&#25187;&#36828;&#26399;&#20803;&#32032;&#30340;&#22810;&#27493;&#25511;&#21046;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20351;&#29992;&#39640;&#32500;&#24230;&#35266;&#27979;&#24517;&#39035;&#33021;&#22815;&#22312;&#35768;&#22810;&#22806;&#28304;&#24615;&#24178;&#25200;&#20013;&#35782;&#21035;&#30456;&#20851;&#29366;&#24577;&#29305;&#24449;&#12290;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#21487;&#25511;&#24615;&#30340;&#34920;&#31034;&#36890;&#36807;&#30830;&#23450;&#24433;&#21709;&#20195;&#29702;&#25511;&#21046;&#30340;&#22240;&#32032;&#26469;&#35782;&#21035;&#36825;&#20123;&#29366;&#24577;&#20803;&#32032;&#12290;&#34429;&#28982;&#35832;&#22914;&#36870;&#21160;&#21147;&#23398;&#21644;&#20114;&#20449;&#24687;&#31561;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#26377;&#38480;&#25968;&#37327;&#30340;&#26102;&#38388;&#27493;&#30340;&#21487;&#25511;&#24615;&#65292;&#20294;&#25429;&#33719;&#38271;&#26102;&#38388;&#20803;&#32032;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#30701;&#35270;&#30340;&#21487;&#25511;&#24615;&#21487;&#20197;&#25429;&#25417;&#20195;&#29702;&#21363;&#23558;&#25758;&#21521;&#22681;&#22721;&#30340;&#30636;&#38388;&#65292;&#20294;&#19981;&#33021;&#22312;&#20195;&#29702;&#36824;&#26377;&#19968;&#23450;&#36317;&#31163;&#20043;&#26102;&#25429;&#25417;&#22681;&#22721;&#30340;&#25511;&#21046;&#30456;&#20851;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#20316;&#21452;&#27169;&#25311;&#32534;&#30721;&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#21040;&#21452;&#27169;&#25311;&#19981;&#21464;&#37327;&#20551;&#24230;&#37327;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#36882;&#24402;&#19981;&#21464;&#24615;&#32422;&#26463;&#25193;&#23637;&#20102;&#21333;&#27493;&#25511;&#21046;&#24615;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#21160;&#20316;&#21452;&#27169;&#25311;&#23398;&#20064;&#20102;&#19968;&#20010;&#24179;&#28369;&#25240;&#25187;&#36828;&#26399;&#20803;&#32032;&#30340;&#22810;&#27493;&#25511;&#21046;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16369v1 Announce Type: cross  Abstract: Robust reinforcement learning agents using high-dimensional observations must be able to identify relevant state features amidst many exogeneous distractors. A representation that captures controllability identifies these state elements by determining what affects agent control. While methods such as inverse dynamics and mutual information capture controllability for a limited number of timesteps, capturing long-horizon elements remains a challenging problem. Myopic controllability can capture the moment right before an agent crashes into a wall, but not the control-relevance of the wall while the agent is still some distance away. To address this we introduce action-bisimulation encoding, a method inspired by the bisimulation invariance pseudometric, that extends single-step controllability with a recursive invariance constraint. By doing this, action-bisimulation learns a multi-step controllability metric that smoothly discounts dist
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;&#20013;&#21307;&#39046;&#22495;&#26500;&#24314;&#20102;&#19987;&#19994;&#35821;&#26009;&#24211;&#65292;&#22522;&#20110;LLaMA&#25104;&#21151;&#24320;&#21457;&#20102;&#39318;&#20010;&#32463;&#36807;&#23436;&#25972;&#35757;&#32451;&#30340;Qibo&#27169;&#22411;&#65292;&#24182;&#25512;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;LLMs&#24615;&#33021;&#30340;Qibo&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>https://arxiv.org/abs/2403.16056</link><description>&lt;p&gt;
Qibo: &#19968;&#31181;&#29992;&#20110;&#20013;&#21307;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Qibo: A Large Language Model for Traditional Chinese Medicine
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#20013;&#21307;&#39046;&#22495;&#26500;&#24314;&#20102;&#19987;&#19994;&#35821;&#26009;&#24211;&#65292;&#22522;&#20110;LLaMA&#25104;&#21151;&#24320;&#21457;&#20102;&#39318;&#20010;&#32463;&#36807;&#23436;&#25972;&#35757;&#32451;&#30340;Qibo&#27169;&#22411;&#65292;&#24182;&#25512;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;LLMs&#24615;&#33021;&#30340;Qibo&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#22312;&#29992;&#25143;&#24847;&#22270;&#29702;&#35299;&#21644;&#21709;&#24212;&#26041;&#38754;&#21462;&#24471;&#30340;&#26174;&#33879;&#36827;&#23637;&#65292;&#22312;&#35768;&#22810;&#19987;&#19994;&#39046;&#22495;&#65292;&#21253;&#25324;&#21307;&#23398;&#12289;&#27861;&#24459;&#21644;&#37329;&#34701;&#12290;&#28982;&#32780;&#65292;&#22312;&#20013;&#21307;&#39046;&#22495;&#65292;LLMs&#30340;&#24615;&#33021;&#25552;&#21319;&#21463;&#21040;&#25361;&#25112;&#65292;&#20854;&#21407;&#22240;&#22312;&#20110;&#20013;&#21307;&#29702;&#35770;&#19982;&#29616;&#20195;&#21307;&#23398;&#20043;&#38388;&#30340;&#26681;&#26412;&#24046;&#24322;&#65292;&#20197;&#21450;&#32570;&#20047;&#19987;&#19994;&#35821;&#26009;&#24211;&#36164;&#28304;&#12290;&#26412;&#25991;&#26088;&#22312;&#26500;&#24314;&#21644;&#25972;&#29702;&#20013;&#21307;&#39046;&#22495;&#30340;&#19987;&#19994;&#35821;&#26009;&#24211;&#65292;&#36171;&#20104;&#22823;&#22411;&#27169;&#22411;&#20855;&#26377;&#20013;&#21307;&#29702;&#35770;&#29305;&#33394;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#25104;&#21151;&#22522;&#20110;LLaMA&#24320;&#21457;&#20102;Qibo&#27169;&#22411;&#65292;&#36825;&#26159;&#20013;&#21307;&#39046;&#22495;&#31532;&#19968;&#20010;&#32463;&#36807;&#23436;&#25972;&#35757;&#32451;&#36807;&#31243;&#65288;&#20174;&#39044;&#35757;&#32451;&#21040;&#30417;&#30563;&#24494;&#35843;&#65289;&#30340;LLM&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Qibo&#22522;&#20934;&#27979;&#35797;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#24615;&#33021;&#30340;&#19987;&#38376;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16056v1 Announce Type: cross  Abstract: In the field of Artificial Intelligence, Large Language Models (LLMs) have demonstrated significant advances in user intent understanding and response in a number of specialized domains, including medicine, law, and finance. However, in the unique domain of traditional Chinese medicine (TCM), the performance enhancement of LLMs is challenged by the essential differences between its theories and modern medicine, as well as the lack of specialized corpus resources. In this paper, we aim to construct and organize a professional corpus in the field of TCM, to endow the large model with professional knowledge that is characteristic of TCM theory, and to successfully develop the Qibo model based on LLaMA, which is the first LLM in the field of TCM to undergo a complete training process from pre-training to Supervised Fine-Tuning (SFT). Furthermore, we develop the Qibo-benchmark, a specialized tool for evaluating the performance of LLMs, whic
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#22823;&#35268;&#27169;LLM&#20013;&#22240;&#32032;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#36890;&#36807;&#20840;&#38754;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#25512;&#21160;&#36825;&#20123;&#27169;&#22411;&#30340;&#21457;&#23637;</title><link>https://arxiv.org/abs/2403.15250</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35780;&#20272;&#32467;&#26524;&#22312;LLM&#20013;&#30340;&#20840;&#38754;&#37325;&#26032;&#35780;&#20272;&#65306;&#19968;&#31181;&#22810;&#26041;&#20301;&#32479;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A Multifaceted Statistical Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15250
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22823;&#35268;&#27169;LLM&#20013;&#22240;&#32032;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#36890;&#36807;&#20840;&#38754;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#25512;&#21160;&#36825;&#20123;&#27169;&#22411;&#30340;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;LLM&#24555;&#36895;&#21457;&#23637;&#30340;&#32972;&#26223;&#19979;&#65292;&#35780;&#20272;&#22312;&#29702;&#35299;&#21644;&#25512;&#21160;&#36825;&#20123;&#27169;&#22411;&#21069;&#36827;&#20013;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#20984;&#26174;&#12290;&#35780;&#20272;&#25581;&#31034;&#20102;&#32553;&#25918;&#12289;&#35757;&#32451;&#31867;&#22411;&#12289;&#26550;&#26500;&#31561;&#22240;&#32032;&#28145;&#21051;&#24433;&#21709;LLM&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22240;&#32032;&#23545;&#24615;&#33021;&#35780;&#20998;&#30340;&#24433;&#21709;&#31243;&#24230;&#21644;&#24615;&#36136;&#20173;&#28982;&#23384;&#22312;&#20105;&#35758;&#65292;&#22240;&#20026;&#22823;&#22810;&#25968;&#35780;&#20272;&#23616;&#38480;&#20110;&#26377;&#38480;&#25968;&#37327;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#28857;&#12290;&#36890;&#36807;&#32479;&#35745;&#35270;&#35282;&#26356;&#26377;&#25928;&#22320;&#28548;&#28165;&#36825;&#20123;&#22240;&#32032;&#23545;&#24615;&#33021;&#24471;&#20998;&#30340;&#24433;&#21709;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23545;&#36825;&#20123;LLM&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#37325;&#26032;&#26816;&#26597;&#65292;&#38024;&#23545;&#24403;&#21069;&#35780;&#20272;&#26041;&#27861;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#38543;&#30528;&#19968;&#20010;&#32479;&#19968;&#30340;&#35780;&#20272;&#26694;&#26550;&#30340;&#20986;&#29616;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21033;&#29992;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#32467;&#26524;&#25968;&#25454;&#38598;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#32479;&#35745;&#26041;&#27861;&#35770;&#12290;&#20854;&#20013;&#21253;&#25324;ANOVA&#12289;Tukey HSD&#26816;&#39564;&#12289;GAMM&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15250v1 Announce Type: cross  Abstract: Amidst the rapid evolution of LLMs, the significance of evaluation in comprehending and propelling these models forward is increasingly paramount. Evaluations have revealed that factors such as scaling, training types, architectures and other factors profoundly impact the performance of LLMs. However, the extent and nature of these impacts continue to be subjects of debate because most assessments have been restricted to a limited number of models and data points. Clarifying the effects of these factors on performance scores can be more effectively achieved through a statistical lens. Our study embarks on a thorough re-examination of these LLMs, targeting the inadequacies in current evaluation methods. With the advent of a uniform evaluation framework, our research leverages an expansive dataset of evaluation results, introducing a comprehensive statistical methodology. This includes the application of ANOVA, Tukey HSD tests, GAMM, and
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#32452;&#26368;&#22823;&#21270;&#20934;&#21017;&#65292;&#29992;&#20110;&#25551;&#36848;&#26377;&#25928;&#30340;&#20154;&#26426;&#23545;&#35805;&#65292;&#21253;&#25324;&#20256;&#32479;&#30340; Grice &#22235;&#20010;&#26368;&#22823;&#21270;&#20934;&#21017;&#20197;&#21450;&#20004;&#20010;&#26032;&#20934;&#21017;&#65292;&#23545;&#20110;&#35299;&#20915;&#29616;&#20195;&#20154;&#26426;&#20114;&#21160;&#20013;&#30340;&#29305;&#27530;&#34892;&#20026;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.15115</link><description>&lt;p&gt;
&#23545;&#35805;&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;&#65306;&#20154;&#26426;&#20132;&#20114;&#30340;&#20250;&#35805;&#26368;&#22823;&#21270;&#20934;&#21017;
&lt;/p&gt;
&lt;p&gt;
Language Models in Dialogue: Conversational Maxims for Human-AI Interactions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15115
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#32452;&#26368;&#22823;&#21270;&#20934;&#21017;&#65292;&#29992;&#20110;&#25551;&#36848;&#26377;&#25928;&#30340;&#20154;&#26426;&#23545;&#35805;&#65292;&#21253;&#25324;&#20256;&#32479;&#30340; Grice &#22235;&#20010;&#26368;&#22823;&#21270;&#20934;&#21017;&#20197;&#21450;&#20004;&#20010;&#26032;&#20934;&#21017;&#65292;&#23545;&#20110;&#35299;&#20915;&#29616;&#20195;&#20154;&#26426;&#20114;&#21160;&#20013;&#30340;&#29305;&#27530;&#34892;&#20026;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#34429;&#28982;&#22797;&#26434;&#65292;&#20294;&#22312;&#23545;&#35805;&#29615;&#22659;&#20013;&#23384;&#22312;&#19968;&#20123;&#22266;&#26377;&#32570;&#38519;&#12290;&#25105;&#20204;&#35748;&#20026;&#35266;&#23519;&#21040;&#30340;&#35768;&#22810;&#32570;&#38519;&#21487;&#20197;&#24402;&#22240;&#20110;&#36829;&#21453;&#19968;&#20010;&#25110;&#22810;&#20010;&#23545;&#35805;&#21407;&#21017;&#12290;&#36890;&#36807;&#20511;&#37492;&#31038;&#20250;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#24191;&#27867;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#26368;&#22823;&#21270;&#20934;&#21017; - &#21253;&#25324;&#25968;&#37327;&#12289;&#36136;&#37327;&#12289;&#30456;&#20851;&#24615;&#12289;&#26041;&#24335;&#12289;&#20161;&#24904;&#20197;&#21450;&#36879;&#26126;&#24230; - &#26469;&#25551;&#36848;&#26377;&#25928;&#30340;&#20154;&#26426;&#23545;&#35805;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#22312;&#20154;&#26426;&#20114;&#21160;&#32972;&#26223;&#19979; Grice &#30340;&#21069;&#22235;&#20010;&#26368;&#22823;&#21270;&#20934;&#21017;&#30340;&#36866;&#29992;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35748;&#20026;&#20004;&#20010;&#26032;&#30340;&#20934;&#21017;&#65292;&#20161;&#24904;&#65288;&#28041;&#21450;&#29983;&#25104;&#21644;&#21442;&#19982;&#26377;&#23475;&#20869;&#23481;&#65289;&#21644;&#36879;&#26126;&#24230;&#65288;&#28041;&#21450;&#35782;&#21035;&#33258;&#24049;&#30340;&#30693;&#35782;&#36793;&#30028;&#12289;&#25805;&#20316;&#32422;&#26463;&#21644;&#24847;&#22270;&#65289;&#65292;&#23545;&#20110;&#35299;&#20915;&#29616;&#20195;&#20154;&#26426;&#20114;&#21160;&#20013;&#29420;&#29305;&#34892;&#20026;&#26159;&#24517;&#35201;&#30340;&#12290;&#25552;&#20986;&#30340;&#20934;&#21017;&#20026;&#22914;&#20309;&#25552;&#20379;&#20855;&#20307;&#25351;&#23548;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15115v1 Announce Type: cross  Abstract: Modern language models, while sophisticated, exhibit some inherent shortcomings, particularly in conversational settings. We claim that many of the observed shortcomings can be attributed to violation of one or more conversational principles. By drawing upon extensive research from both the social science and AI communities, we propose a set of maxims -- quantity, quality, relevance, manner, benevolence, and transparency -- for describing effective human-AI conversation. We first justify the applicability of the first four maxims (from Grice) in the context of human-AI interactions. We then argue that two new maxims, benevolence (concerning the generation of, and engagement with, harmful content) and transparency (concerning recognition of one's knowledge boundaries, operational constraints, and intents), are necessary for addressing behavior unique to modern human-AI interactions. The proposed maxims offer prescriptive guidance on how
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#20248;&#21270;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#23588;&#20854;&#23545;&#20110;&#38750;STEM&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.14932</link><description>&lt;p&gt;
&#19987;&#27880;&#39537;&#21160;&#30340;&#25512;&#29702;:&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Attention-Driven Reasoning: Unlocking the Potential of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14932
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#20248;&#21270;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#23588;&#20854;&#23545;&#20110;&#38750;STEM&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#22522;&#30784;&#26426;&#21046;&#20173;&#19981;&#20026;&#20154;&#25152;&#20102;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#20248;&#21270;&#26469;&#22686;&#24378;LLMs&#25512;&#29702;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#30001;&#38750;&#35821;&#20041;&#26631;&#35760;&#23548;&#33268;&#30340;&#27880;&#24847;&#21147;&#20998;&#24067;&#30340;&#20302;&#25928;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#37325;&#26032;&#24179;&#34913;&#20559;&#26012;&#20998;&#24067;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#25277;&#35937;&#26356;&#21152;&#24494;&#22937;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25512;&#29702;&#33021;&#21147;&#24471;&#21040;&#20102;&#26174;&#30528;&#25913;&#36827;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38750;STEM&#38382;&#39064;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#27880;&#24847;&#21147;&#27169;&#24335;&#22312;LLMs&#25512;&#29702;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#36825;&#20123;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#20026;&#26356;&#24378;&#22823;&#21644;&#22810;&#21151;&#33021;&#30340;&#35821;&#35328;&#27169;&#22411;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14932v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have shown remarkable capabilities, but their reasoning abilities and underlying mechanisms remain poorly understood. We present a novel approach to enhance LLMs' reasoning through attention mechanism optimization, without additional training data. We identify inefficiencies in the attention distribution caused by non-semantic tokens and propose an algorithm to re-balance the skewed distribution, enabling the model to abstract more nuanced knowledge. Our experiments demonstrate significantly improved reasoning capabilities, particularly for non-STEM questions. We provide insights into the role of attention patterns in LLMs' reasoning and propose a method to enhance these abilities, paving the way for more powerful and versatile language models.
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#20195;&#30721;&#26234;&#33021;&#39046;&#22495;&#30340;&#35843;&#26597;&#31995;&#32479;&#22238;&#39038;&#20102;50&#22810;&#31181;&#20195;&#34920;&#24615;&#27169;&#22411;&#21644;&#36229;&#36807;680&#39033;&#30456;&#20851;&#20316;&#21697;&#65292;&#31361;&#20986;&#20102;&#19981;&#21516;&#30740;&#31350;&#38454;&#27573;&#30340;&#33539;&#24335;&#21644;&#25216;&#26415;&#36716;&#21464;&#12290;</title><link>https://arxiv.org/abs/2403.14734</link><description>&lt;p&gt;
&#19968;&#39033;&#31070;&#32463;&#20195;&#30721;&#26234;&#33021;&#30340;&#35843;&#26597;&#65306;&#33539;&#24335;&#12289;&#36827;&#23637;&#19982;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
A Survey of Neural Code Intelligence: Paradigms, Advances and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14734
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20195;&#30721;&#26234;&#33021;&#39046;&#22495;&#30340;&#35843;&#26597;&#31995;&#32479;&#22238;&#39038;&#20102;50&#22810;&#31181;&#20195;&#34920;&#24615;&#27169;&#22411;&#21644;&#36229;&#36807;680&#39033;&#30456;&#20851;&#20316;&#21697;&#65292;&#31361;&#20986;&#20102;&#19981;&#21516;&#30740;&#31350;&#38454;&#27573;&#30340;&#33539;&#24335;&#21644;&#25216;&#26415;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14734v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#31070;&#32463;&#20195;&#30721;&#26234;&#33021;--&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#29702;&#35299;&#12289;&#29983;&#25104;&#21644;&#20248;&#21270;&#20195;&#30721;--&#22312;&#25972;&#20010;&#31038;&#20250;&#19978;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20135;&#29983;&#28145;&#36828;&#24433;&#21709;&#12290;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#36825;&#19968;&#39046;&#22495;&#22312;&#36807;&#21435;&#20960;&#24180;&#24341;&#36215;&#20102;&#20004;&#20010;&#30740;&#31350;&#31038;&#21306;&#30740;&#31350;&#20154;&#21592;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#26412;&#35843;&#26597;&#31995;&#32479;&#22320;&#21644;&#25353;&#26102;&#38388;&#39034;&#24207;&#22238;&#39038;&#20102;&#20195;&#30721;&#26234;&#33021;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#21253;&#25324;50&#22810;&#31181;&#20195;&#34920;&#24615;&#27169;&#22411;&#21450;&#20854;&#21464;&#20307;&#12289;20&#22810;&#31181;&#20219;&#21153;&#31867;&#21035;&#20197;&#21450;&#36229;&#36807;680&#39033;&#30456;&#20851;&#20316;&#21697;&#12290;&#25105;&#20204;&#36981;&#24490;&#21382;&#21490;&#36827;&#23637;&#65292;&#36319;&#36394;&#19981;&#21516;&#30740;&#31350;&#38454;&#27573;&#30340;&#33539;&#24335;&#36716;&#21464;&#65288;&#20363;&#22914;&#65292;&#20174;&#20351;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23545;&#20195;&#30721;&#24314;&#27169;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#65289;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;&#19981;&#21516;&#38454;&#27573;&#28085;&#30422;&#30340;&#27169;&#22411;&#12289;&#20219;&#21153;&#21644;&#35780;&#20272;&#30340;&#20027;&#35201;&#25216;&#26415;&#36716;&#21464;&#12290;&#23545;&#20110;&#24212;&#29992;&#65292;&#25105;&#20204;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14734v1 Announce Type: cross  Abstract: Neural Code Intelligence -- leveraging deep learning to understand, generate, and optimize code -- holds immense potential for transformative impacts on the whole society. Bridging the gap between Natural Language and Programming Language, this domain has drawn significant attention from researchers in both research communities over the past few years. This survey presents a systematic and chronological review of the advancements in code intelligence, encompassing over 50 representative models and their variants, more than 20 categories of tasks, and an extensive coverage of over 680 related works. We follow the historical progression to trace the paradigm shifts across different research phases (e.g., from modeling code with recurrent neural networks to the era of Large Language Models). Concurrently, we highlight the major technical transitions in models, tasks, and evaluations spanning through different stages. For applications, we 
&lt;/p&gt;</description></item><item><title>LlamaFactory&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#25972;&#21512;&#20102;&#19968;&#31995;&#21015;&#21069;&#27839;&#30340;&#39640;&#25928;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#32534;&#30721;&#30340;&#24773;&#20917;&#19979;&#28789;&#27963;&#23450;&#21046;100&#22810;&#31181;LLMs&#30340;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2403.13372</link><description>&lt;p&gt;
LlamaFactory&#65306;100&#22810;&#31181;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13372
&lt;/p&gt;
&lt;p&gt;
LlamaFactory&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#25972;&#21512;&#20102;&#19968;&#31995;&#21015;&#21069;&#27839;&#30340;&#39640;&#25928;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#32534;&#30721;&#30340;&#24773;&#20917;&#19979;&#28789;&#27963;&#23450;&#21046;100&#22810;&#31181;LLMs&#30340;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#30340;&#24494;&#35843;&#23545;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#19981;&#21516;&#27169;&#22411;&#19978;&#23454;&#29616;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#38750;&#24179;&#20961;&#30340;&#21162;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LlamaFactory&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#38598;&#25104;&#20102;&#19968;&#22871;&#21069;&#27839;&#30340;&#39640;&#25928;&#35757;&#32451;&#26041;&#27861;&#12290;&#23427;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#20869;&#32622;&#30340;Web UI LlamaBoard &#28789;&#27963;&#23450;&#21046;100&#22810;&#31181;LLMs&#30340;&#24494;&#35843;&#65292;&#26080;&#38656;&#32534;&#30721;&#12290;&#25105;&#20204;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#19978;&#32463;&#39564;&#24615;&#22320;&#39564;&#35777;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;&#24050;&#21457;&#24067;&#22312; https://github.com/hiyouga/LLaMA-Factory&#65292;&#24182;&#24050;&#33719;&#24471;&#36229;&#36807;13,000&#39063;&#26143;&#21644;1,600&#20010;&#20998;&#25903;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13372v1 Announce Type: new  Abstract: Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It allows users to flexibly customize the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks. It has been released at https://github.com/hiyouga/LLaMA-Factory and already received over 13,000 stars and 1,600 forks.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#28145;&#20837;&#20998;&#26512;&#20102;LLMs&#22312;&#34701;&#21512;&#19978;&#19979;&#25991;&#21644;&#21442;&#25968;&#21270;&#30693;&#35782;&#26102;&#25152;&#38754;&#20020;&#30340;&#30693;&#35782;&#20914;&#31361;&#65292;&#25506;&#35752;&#20102;&#19977;&#31867;&#30693;&#35782;&#20914;&#31361;&#23545;&#20854;&#21487;&#20449;&#24230;&#21644;&#24615;&#33021;&#30340;&#37325;&#35201;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#25913;&#36827;LLMs&#31283;&#20581;&#24615;&#31574;&#30053;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.08319</link><description>&lt;p&gt;
LLMs&#30340;&#30693;&#35782;&#20914;&#31361;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Knowledge Conflicts for LLMs: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08319
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#28145;&#20837;&#20998;&#26512;&#20102;LLMs&#22312;&#34701;&#21512;&#19978;&#19979;&#25991;&#21644;&#21442;&#25968;&#21270;&#30693;&#35782;&#26102;&#25152;&#38754;&#20020;&#30340;&#30693;&#35782;&#20914;&#31361;&#65292;&#25506;&#35752;&#20102;&#19977;&#31867;&#30693;&#35782;&#20914;&#31361;&#23545;&#20854;&#21487;&#20449;&#24230;&#21644;&#24615;&#33021;&#30340;&#37325;&#35201;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#25913;&#36827;LLMs&#31283;&#20581;&#24615;&#31574;&#30053;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30693;&#35782;&#20914;&#31361;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#31361;&#20986;&#20102;&#24403;&#23427;&#20204;&#34701;&#21512;&#19978;&#19979;&#25991;&#21644;&#21442;&#25968;&#21270;&#30693;&#35782;&#26102;&#25152;&#36935;&#21040;&#30340;&#22797;&#26434;&#25361;&#25112;&#12290;&#25105;&#20204;&#20851;&#27880;&#19977;&#31867;&#30693;&#35782;&#20914;&#31361;&#65306;&#19978;&#19979;&#25991;-&#35760;&#24518;&#20914;&#31361;&#12289;&#36328;&#19978;&#19979;&#25991;&#20914;&#31361;&#21644;&#20869;&#37096;&#35760;&#24518;&#20914;&#31361;&#12290;&#36825;&#20123;&#20914;&#31361;&#21487;&#33021;&#20250;&#26174;&#33879;&#24433;&#21709;LLMs&#30340;&#21487;&#20449;&#24230;&#21644;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#65292;&#22122;&#38899;&#21644;&#38169;&#35823;&#20449;&#24687;&#24456;&#24120;&#35265;&#12290;&#36890;&#36807;&#23545;&#36825;&#20123;&#20914;&#31361;&#36827;&#34892;&#20998;&#31867;&#65292;&#25506;&#35752;&#20854;&#21407;&#22240;&#65292;&#30740;&#31350;LLMs&#22312;&#36825;&#20123;&#20914;&#31361;&#19979;&#30340;&#34892;&#20026;&#65292;&#24182;&#22238;&#39038;&#21487;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26412;&#35843;&#26597;&#26088;&#22312;&#20026;&#25913;&#36827;LLMs&#30340;&#31283;&#20581;&#24615;&#31574;&#30053;&#25552;&#20379;&#21551;&#31034;&#65292;&#20174;&#32780;&#25104;&#20026;&#25512;&#21160;&#36825;&#19968;&#19981;&#26029;&#21457;&#23637;&#39046;&#22495;&#30740;&#31350;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08319v1 Announce Type: cross  Abstract: This survey provides an in-depth analysis of knowledge conflicts for large language models (LLMs), highlighting the complex challenges they encounter when blending contextual and parametric knowledge. Our focus is on three categories of knowledge conflicts: context-memory, inter-context, and intra-memory conflict. These conflicts can significantly impact the trustworthiness and performance of LLMs, especially in real-world applications where noise and misinformation are common. By categorizing these conflicts, exploring the causes, examining the behaviors of LLMs under such conflicts, and reviewing available solutions, this survey aims to shed light on strategies for improving the robustness of LLMs, thereby serving as a valuable resource for advancing research in this evolving area.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;COM2&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22312;&#24120;&#35782;&#30693;&#35782;&#22270;&#20013;&#25277;&#26679;&#22810;&#36339;&#36923;&#36753;&#26597;&#35810;&#24182;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.07398</link><description>&lt;p&gt;
&#22312;&#24120;&#35782;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#36923;&#36753;&#26597;&#35810;&#30340;&#22797;&#26434;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07398
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;COM2&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22312;&#24120;&#35782;&#30693;&#35782;&#22270;&#20013;&#25277;&#26679;&#22810;&#36339;&#36923;&#36753;&#26597;&#35810;&#24182;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#24120;&#35782;&#25512;&#29702;&#38656;&#35201;&#20855;&#26377;&#25512;&#29702;&#20107;&#20214;&#20043;&#38388;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#25512;&#26029;&#22312;&#36825;&#31181;&#20851;&#31995;&#20043;&#19979;&#30340;&#38544;&#21547;&#19978;&#19979;&#25991;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#31232;&#32570;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#38590;&#20197;&#23398;&#20250;&#20026;&#28041;&#21450;&#22797;&#26434;&#20107;&#20214;&#30456;&#20114;&#20316;&#29992;&#30340;&#32972;&#26223;&#21644;&#38382;&#39064;&#29983;&#25104;&#24120;&#35782;&#25512;&#26029;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#31181;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;COM2&#65288;COMplex COMmonsense&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#20174;&#29616;&#26377;&#24120;&#35782;&#30693;&#35782;&#22270;&#65288;CSKG&#65289;&#20013;&#25277;&#26679;&#22810;&#36339;&#36923;&#36753;&#26597;&#35810;&#65288;&#20363;&#22914;&#65292;&#20107;&#20214;A&#21644;B&#30340;&#32852;&#21512;&#25928;&#26524;&#25110;&#22240;&#26524;&#20851;&#31995;&#65292;&#25110;&#20107;&#20214;C&#30340;&#25928;&#26524;&#30340;&#25928;&#26524;&#65289;&#65292;&#24182;&#21033;&#29992;&#25163;&#24037;&#21046;&#20316;&#30340;&#35268;&#21017;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#20854;&#29992;&#22810;&#36873;&#21644;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#30340;&#24418;&#24335;&#34920;&#36798;&#20986;&#26469;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;COM2&#19978;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#38646;-shot&#24615;&#33021;&#65292;&#26080;&#35770;&#26159;&#22312;&#39046;&#22495;&#20869;&#36824;&#26159;&#39046;&#22495;&#22806;&#30340;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07398v1 Announce Type: cross  Abstract: Event commonsense reasoning requires the ability to reason about the relationship between events, as well as infer implicit context underlying that relationship. However, data scarcity makes it challenging for language models to learn to generate commonsense inferences for contexts and questions involving interactions between complex events. To address this demand, we present COM2 (COMplex COMmonsense), a new dataset created by sampling multi-hop logical queries (e.g., the joint effect or cause of both event A and B, or the effect of the effect of event C) from an existing commonsense knowledge graph (CSKG), and verbalizing them using handcrafted rules and large language models into multiple-choice and text generation questions. Our experiments show that language models trained on COM2 exhibit significant improvements in complex reasoning ability, resulting in enhanced zero-shot performance in both in-domain and out-of-domain tasks for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21387;&#32553;&#22312;&#20998;&#35789;&#36807;&#31243;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#35777;&#26126;&#20102;&#21387;&#32553;&#19982;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21518;&#32493;&#25104;&#21151;&#20043;&#38388;&#30340;&#23454;&#35777;&#37325;&#35201;&#24615;&#65292;&#24182;&#34920;&#26126;&#20998;&#35789;&#22120;&#30340;&#21387;&#32553;&#19982;&#27169;&#22411;&#30340;&#24615;&#33021;&#23384;&#22312;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06265</link><description>&lt;p&gt;
&#25286;&#35299;&#20998;&#35789;&#65306;&#35780;&#20272;&#25991;&#26412;&#21387;&#32553;&#21450;&#20854;&#19982;&#27169;&#22411;&#24615;&#33021;&#30340;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Unpacking Tokenization: Evaluating Text Compression and its Correlation with Model Performance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21387;&#32553;&#22312;&#20998;&#35789;&#36807;&#31243;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#35777;&#26126;&#20102;&#21387;&#32553;&#19982;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21518;&#32493;&#25104;&#21151;&#20043;&#38388;&#30340;&#23454;&#35777;&#37325;&#35201;&#24615;&#65292;&#24182;&#34920;&#26126;&#20998;&#35789;&#22120;&#30340;&#21387;&#32553;&#19982;&#27169;&#22411;&#30340;&#24615;&#33021;&#23384;&#22312;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21387;&#32553;&#26159;BPE&#26368;&#24120;&#35265;&#30340;&#20998;&#35789;&#31639;&#27861;&#30340;&#37325;&#35201;&#22522;&#30784;&#65292;&#20294;&#20998;&#35789;&#36807;&#31243;&#20013;&#30340;&#21387;&#32553;&#37325;&#35201;&#24615;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#35770;&#36848;&#20102;&#21387;&#32553;&#30340;&#29702;&#35770;&#37325;&#35201;&#24615;&#65292;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;0-gram&#35821;&#35328;&#24314;&#27169;&#65292;&#21363;&#20026;&#25152;&#26377;&#26631;&#35760;&#20998;&#37197;&#30456;&#31561;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#21387;&#32553;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21518;&#32493;&#25104;&#21151;&#30340;&#23454;&#35777;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25913;&#21464;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#29992;&#25991;&#26723;&#30340;&#25968;&#37327;&#26469;&#25511;&#21046;&#22810;&#20010;BPE&#20998;&#35789;&#22120;&#30340;&#21387;&#32553;&#33021;&#21147;&#65306;&#20174;100&#19975;&#20010;&#25991;&#26723;&#21040;&#30456;&#24403;&#20110;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#30340;&#22522;&#20110;&#23383;&#31526;&#30340;&#20998;&#35789;&#22120;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#36825;&#20123;&#20998;&#35789;&#22120;&#39044;&#35757;&#32451;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20998;&#35789;&#22120;&#30340;&#21387;&#32553;&#19982;&#27169;&#22411;&#30340;&#21518;&#32493;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#65292;&#34920;&#26126;&#21387;&#32553;&#26159;&#20998;&#35789;&#30340;&#21487;&#38752;&#20869;&#22312;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06265v1 Announce Type: cross  Abstract: Despite it being the cornerstone of BPE, the most common tokenization algorithm, the importance of compression in the tokenization process is still unclear. In this paper, we argue for the theoretical importance of compression, that can be viewed as 0-gram language modeling where equal probability is assigned to all tokens. We also demonstrate the empirical importance of compression for downstream success of pre-trained language models. We control the compression ability of several BPE tokenizers by varying the amount of documents available during their training: from 1 million documents to a character-based tokenizer equivalent to no training data at all. We then pre-train English language models based on those tokenizers and fine-tune them over several tasks. We show that there is a correlation between tokenizers' compression and models' downstream performance, suggesting that compression is a reliable intrinsic indicator of tokeniza
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;SAFIM&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#20195;&#30721;&#22635;&#31354;&#20219;&#21153;&#19978;&#30340;&#21477;&#27861;&#24863;&#30693;&#23436;&#25104;&#34920;&#29616;&#65292;&#21457;&#29616;FIM&#39044;&#35757;&#32451;&#19981;&#20165;&#25552;&#39640;&#20102;FIM&#30340;&#29087;&#32451;&#24230;&#65292;&#36824;&#25913;&#21892;&#20102;LLMs&#30340;&#24038;&#21040;&#21491;&#25512;&#29702;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#35266;&#24565;&#24182;&#34920;&#26126;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#25968;&#25454;&#21697;&#36136;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#26356;&#29978;&#20110;&#27169;&#22411;&#22823;&#23567;&#12290;</title><link>https://arxiv.org/abs/2403.04814</link><description>&lt;p&gt;
&#22312;&#21477;&#27861;&#24863;&#30693;&#20195;&#30721;&#22635;&#31354;&#20219;&#21153;&#19978;&#35780;&#20272;LLMs
&lt;/p&gt;
&lt;p&gt;
Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04814
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;SAFIM&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#20195;&#30721;&#22635;&#31354;&#20219;&#21153;&#19978;&#30340;&#21477;&#27861;&#24863;&#30693;&#23436;&#25104;&#34920;&#29616;&#65292;&#21457;&#29616;FIM&#39044;&#35757;&#32451;&#19981;&#20165;&#25552;&#39640;&#20102;FIM&#30340;&#29087;&#32451;&#24230;&#65292;&#36824;&#25913;&#21892;&#20102;LLMs&#30340;&#24038;&#21040;&#21491;&#25512;&#29702;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#35266;&#24565;&#24182;&#34920;&#26126;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#25968;&#25454;&#21697;&#36136;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#26356;&#29978;&#20110;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Syntax-Aware Fill-In-the-Middle&#65288;SAFIM&#65289;&#30340;&#26032;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#22635;&#31354;&#65288;FIM&#65289;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#35813;&#22522;&#20934;&#20391;&#37325;&#20110;&#31243;&#24207;&#32467;&#26500;&#30340;&#21477;&#27861;&#24863;&#30693;&#23436;&#25104;&#65292;&#22914;&#20195;&#30721;&#22359;&#21644;&#26465;&#20214;&#34920;&#36798;&#24335;&#65292;&#24182;&#21253;&#25324;&#26469;&#33258;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;17,720&#20010;&#31034;&#20363;&#65292;&#26469;&#28304;&#20110;2022&#24180;4&#26376;&#20043;&#21518;&#30340;&#26368;&#26032;&#20195;&#30721;&#25552;&#20132;&#65292;&#20197;&#26368;&#23567;&#21270;&#25968;&#25454;&#27745;&#26579;&#12290; SAFIM&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#20855;&#26377;&#21508;&#31181;&#25552;&#31034;&#35774;&#35745;&#21644;&#26032;&#39062;&#30340;&#21477;&#27861;&#24863;&#30693;&#21518;&#22788;&#29702;&#25216;&#26415;&#65292;&#26377;&#21161;&#20110;&#22312;LLMs&#20043;&#38388;&#36827;&#34892;&#20934;&#30830;&#21644;&#20844;&#24179;&#30340;&#27604;&#36739;&#12290;&#25105;&#20204;&#23545;15&#20010;LLMs&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;FIM&#39044;&#35757;&#32451;&#19981;&#20165;&#25552;&#21319;&#20102;FIM&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#36824;&#25913;&#36827;&#20102;LLMs&#30340;&#24038;&#21040;&#21491;&#65288;L2R&#65289;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25361;&#25112;&#20102;&#20256;&#32479;&#35266;&#24565;&#65292;&#24182;&#34920;&#26126;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#25968;&#25454;&#36136;&#37327;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#22823;&#20110;&#27169;&#22411;&#22823;&#23567;&#12290;&#22240;&#27492;&#65292;SAFIM&#20026;&#26410;&#26469;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04814v1 Announce Type: cross  Abstract: We introduce Syntax-Aware Fill-In-the-Middle (SAFIM), a new benchmark for evaluating Large Language Models (LLMs) on the code Fill-in-the-Middle (FIM) task. This benchmark focuses on syntax-aware completions of program structures such as code blocks and conditional expressions, and includes 17,720 examples from multiple programming languages, sourced from recent code submissions after April 2022 to minimize data contamination. SAFIM provides a robust framework with various prompt designs and novel syntax-aware post-processing techniques, facilitating accurate and fair comparisons across LLMs. Our comprehensive evaluation of 15 LLMs shows that FIM pretraining not only enhances FIM proficiency but also improves Left-to-Right (L2R) inference using LLMs. Our findings challenge conventional beliefs and suggest that pretraining methods and data quality have more impact than model size. SAFIM thus serves as a foundational platform for future 
&lt;/p&gt;</description></item><item><title>LLM&#27700;&#21360;&#25216;&#26415;&#21487;&#33021;&#23384;&#22312;&#27700;&#21360;&#31363;&#21462;&#28431;&#27934;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#21160;WS&#31639;&#27861;&#24182;&#23637;&#31034;&#20102;&#25915;&#20987;&#32773;&#21487;&#20197;&#22312;&#19981;&#21040;50&#32654;&#20803;&#30340;&#25104;&#26412;&#19979;&#36890;&#36807;&#27450;&#39575;&#21644;&#25830;&#38500;&#25915;&#20987;&#30772;&#35299;&#20043;&#21069;&#35748;&#20026;&#23433;&#20840;&#30340;&#26368;&#20808;&#36827;&#26041;&#26696;&#65292;&#25104;&#21151;&#29575;&#36229;&#36807;80%&#12290;</title><link>https://arxiv.org/abs/2402.19361</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#27700;&#21360;&#31363;&#21462;
&lt;/p&gt;
&lt;p&gt;
Watermark Stealing in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19361
&lt;/p&gt;
&lt;p&gt;
LLM&#27700;&#21360;&#25216;&#26415;&#21487;&#33021;&#23384;&#22312;&#27700;&#21360;&#31363;&#21462;&#28431;&#27934;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#21160;WS&#31639;&#27861;&#24182;&#23637;&#31034;&#20102;&#25915;&#20987;&#32773;&#21487;&#20197;&#22312;&#19981;&#21040;50&#32654;&#20803;&#30340;&#25104;&#26412;&#19979;&#36890;&#36807;&#27450;&#39575;&#21644;&#25830;&#38500;&#25915;&#20987;&#30772;&#35299;&#20043;&#21069;&#35748;&#20026;&#23433;&#20840;&#30340;&#26368;&#20808;&#36827;&#26041;&#26696;&#65292;&#25104;&#21151;&#29575;&#36229;&#36807;80%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#27700;&#21360;&#25216;&#26415;&#20316;&#20026;&#19968;&#31181;&#26816;&#27979;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#26377;&#25928;&#26041;&#24335;&#65292;&#21463;&#21040;&#20102;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#20105;&#36777;&#31216;&#24403;&#21069;&#26041;&#26696;&#21487;&#33021;&#24050;&#32463;&#21487;&#20197;&#37096;&#32626;&#65292;&#25105;&#20204;&#35748;&#20026;&#27700;&#21360;&#31363;&#21462;&#65288;WS&#65289;&#26159;&#36825;&#20123;&#26041;&#26696;&#30340;&#19968;&#20010;&#26681;&#26412;&#24615;&#28431;&#27934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#26597;&#35810;&#24102;&#26377;&#27700;&#21360;&#30340;LLM&#30340;API&#26469;&#36817;&#20284;&#36870;&#21521;&#27700;&#21360;&#65292;&#20174;&#32780;&#23454;&#29616;&#23454;&#29992;&#30340;&#27450;&#39575;&#25915;&#20987;&#65292;&#21516;&#26102;&#22823;&#24133;&#22686;&#21152;&#20102;&#20043;&#21069;&#26410;&#34987;&#27880;&#24847;&#21040;&#30340;&#25830;&#38500;&#25915;&#20987;&#12290;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#25552;&#20986;&#33258;&#21160;WS&#31639;&#27861;&#24182;&#23558;&#20854;&#29992;&#20110;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#36827;&#34892;&#27450;&#39575;&#21644;&#25830;&#38500;&#30340;&#20840;&#38754;&#30740;&#31350;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20165;&#38656;&#19981;&#21040;50&#32654;&#20803;&#30340;&#25104;&#26412;&#65292;&#25915;&#20987;&#32773;&#23601;&#33021;&#22815;&#27450;&#39575;&#24182;&#25830;&#38500;&#20043;&#21069;&#34987;&#35748;&#20026;&#26159;&#23433;&#20840;&#30340;&#26368;&#20808;&#36827;&#26041;&#26696;&#65292;&#24179;&#22343;&#25104;&#21151;&#29575;&#36229;&#36807;80%&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25361;&#25112;&#20102;&#20851;&#20110;LLM&#27700;&#21360;&#25216;&#26415;&#30340;&#24120;&#35265;&#20449;&#24565;&#65292;&#24378;&#35843;&#20102;&#26356;&#21152;&#20581;&#22766;&#26041;&#26696;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19361v1 Announce Type: cross  Abstract: LLM watermarking has attracted attention as a promising way to detect AI-generated content, with some works suggesting that current schemes may already be fit for deployment. In this work we dispute this claim, identifying watermark stealing (WS) as a fundamental vulnerability of these schemes. We show that querying the API of the watermarked LLM to approximately reverse-engineer a watermark enables practical spoofing attacks, as suggested in prior work, but also greatly boosts scrubbing attacks, which was previously unnoticed. We are the first to propose an automated WS algorithm and use it in the first comprehensive study of spoofing and scrubbing in realistic settings. We show that for under $50 an attacker can both spoof and scrub state-of-the-art schemes previously considered safe, with average success rate of over 80%. Our findings challenge common beliefs about LLM watermarking, stressing the need for more robust schemes. We mak
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20102;&#38544;&#31169;&#25915;&#20987;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#39318;&#20010;&#33021;&#21516;&#26102;&#23454;&#29616;&#39640;&#30495;&#27491;&#29575;&#21644;&#20302;&#35823;&#20998;&#31867;&#29575;&#30340;&#39044;&#35757;&#32451;LLMs&#20250;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIAs&#65289;&#65292;&#20197;&#21450;&#23637;&#31034;&#20102;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#21487;&#20197;&#20174;&#24494;&#35843;LLM&#20013;&#25552;&#21462;&#36229;&#36807;50%&#30340;&#24494;&#35843;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.17012</link><description>&lt;p&gt;
Pandora's White-Box&#65306;&#24320;&#25918;LLMs&#20013;&#35757;&#32451;&#25968;&#25454;&#27844;&#28431;&#30340;&#22686;&#21152;
&lt;/p&gt;
&lt;p&gt;
Pandora's White-Box: Increased Training Data Leakage in Open LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20102;&#38544;&#31169;&#25915;&#20987;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#39318;&#20010;&#33021;&#21516;&#26102;&#23454;&#29616;&#39640;&#30495;&#27491;&#29575;&#21644;&#20302;&#35823;&#20998;&#31867;&#29575;&#30340;&#39044;&#35757;&#32451;LLMs&#20250;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIAs&#65289;&#65292;&#20197;&#21450;&#23637;&#31034;&#20102;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#21487;&#20197;&#20174;&#24494;&#35843;LLM&#20013;&#25552;&#21462;&#36229;&#36807;50%&#30340;&#24494;&#35843;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36973;&#21463;&#30340;&#38544;&#31169;&#25915;&#20987;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#20854;&#20013;&#23545;&#25163;&#21487;&#20197;&#35775;&#38382;&#27169;&#22411;&#26435;&#37325;&#12289;&#26799;&#24230;&#25110;&#25439;&#22833;&#65292;&#35797;&#22270;&#21033;&#29992;&#23427;&#20204;&#26469;&#20102;&#35299;&#24213;&#23618;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#38024;&#23545;&#39044;&#35757;&#32451;LLMs&#30340;&#31532;&#19968;&#20010;&#20250;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIAs&#65289;&#65292;&#33021;&#22815;&#21516;&#26102;&#23454;&#29616;&#39640;TPR&#21644;&#20302;FPR&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#21487;&#20197;&#20174;&#24494;&#35843;LLM&#20013;&#25552;&#21462;&#36229;&#36807;50%&#30340;&#24494;&#35843;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#23545;&#24213;&#23618;&#27169;&#22411;&#30340;&#19981;&#21516;&#35775;&#38382;&#31243;&#24230;&#12289;&#35821;&#35328;&#27169;&#22411;&#30340;&#23450;&#21046;&#21270;&#20197;&#21450;&#25915;&#20987;&#32773;&#21487;&#20197;&#20351;&#29992;&#30340;&#36164;&#28304;&#12290;&#22312;&#39044;&#35757;&#32451;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#30333;&#30418;MIAs&#65306;&#22522;&#20110;&#26799;&#24230;&#33539;&#25968;&#30340;&#25915;&#20987;&#12289;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#21644;&#21333;&#27493;&#25439;&#22833;&#27604;&#25915;&#20987;&#12290;&#25152;&#26377;&#36825;&#20123;&#37117;&#20248;&#20110;&#29616;&#26377;&#30340;&#40657;&#30418;&#22522;&#32447;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;.....
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17012v1 Announce Type: cross  Abstract: In this paper we undertake a systematic study of privacy attacks against open source Large Language Models (LLMs), where an adversary has access to either the model weights, gradients, or losses, and tries to exploit them to learn something about the underlying training data. Our headline results are the first membership inference attacks (MIAs) against pre-trained LLMs that are able to simultaneously achieve high TPRs and low FPRs, and a pipeline showing that over $50\%$ (!) of the fine-tuning dataset can be extracted from a fine-tuned LLM in natural settings. We consider varying degrees of access to the underlying model, customization of the language model, and resources available to the attacker. In the pre-trained setting, we propose three new white-box MIAs: an attack based on the gradient norm, a supervised neural network classifier, and a single step loss ratio attack. All outperform existing black-box baselines, and our supervi
&lt;/p&gt;</description></item><item><title>Mirror &#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35270;&#35282;&#33258;&#25105;&#21453;&#24605;&#26041;&#27861;&#65292;&#36890;&#36807;&#23548;&#33322;&#32773;&#21644;&#25512;&#29702;&#32773;&#20043;&#38388;&#30340;&#21551;&#21457;&#24335;&#20132;&#20114;&#65292;&#20419;&#36827;&#22810;&#26679;&#24615;&#32780;&#20855;&#26377;&#21487;&#38752;&#24615;&#30340;&#25512;&#29702;&#36712;&#36857;&#21457;&#23637;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#30693;&#35782;&#20016;&#23500;&#38382;&#39064;&#19978;&#30340;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2402.14963</link><description>&lt;p&gt;
&#38236;&#20687;&#65306;&#19968;&#31181;&#36866;&#29992;&#20110;&#30693;&#35782;&#20016;&#23500;&#25512;&#29702;&#30340;&#22810;&#35270;&#35282;&#33258;&#25105;&#21453;&#24605;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Mirror: A Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14963
&lt;/p&gt;
&lt;p&gt;
Mirror &#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35270;&#35282;&#33258;&#25105;&#21453;&#24605;&#26041;&#27861;&#65292;&#36890;&#36807;&#23548;&#33322;&#32773;&#21644;&#25512;&#29702;&#32773;&#20043;&#38388;&#30340;&#21551;&#21457;&#24335;&#20132;&#20114;&#65292;&#20419;&#36827;&#22810;&#26679;&#24615;&#32780;&#20855;&#26377;&#21487;&#38752;&#24615;&#30340;&#25512;&#29702;&#36712;&#36857;&#21457;&#23637;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#30693;&#35782;&#20016;&#23500;&#38382;&#39064;&#19978;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26377;&#33021;&#21147;&#21453;&#22797;&#21453;&#24605;&#33258;&#24049;&#30340;&#36755;&#20986;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#35266;&#23519;&#21040;&#23427;&#20204;&#22312;&#27809;&#26377;&#22806;&#37096;&#36164;&#28304;&#30340;&#24773;&#20917;&#19979;&#22788;&#29702;&#30693;&#35782;&#20016;&#23500;&#38382;&#39064;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;&#38500;&#20102;LLMs&#22312;&#33258;&#25105;&#35780;&#20272;&#26041;&#38754;&#30340;&#20302;&#25928;&#29575;&#22806;&#65292;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#23613;&#31649;&#21463;&#21040;&#26126;&#30830;&#36127;&#38754;&#21453;&#39304;&#65292;LLMs&#20173;&#28982;&#38590;&#20197;&#37325;&#26032;&#23457;&#35270;&#20854;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Mirror&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;&#30693;&#35782;&#20016;&#23500;&#25512;&#29702;&#30340;&#22810;&#35282;&#24230;&#33258;&#25105;&#21453;&#24605;&#26041;&#27861;&#65292;&#20197;&#36991;&#20813;&#22312;&#29305;&#23450;&#21453;&#24605;&#36845;&#20195;&#20013;&#21345;&#20303;&#12290;Mirror&#20351;LLMs&#33021;&#22815;&#36890;&#36807;&#23548;&#33322;&#32773;&#21644;&#25512;&#29702;&#32773;&#20043;&#38388;&#30340;&#21551;&#21457;&#24335;&#20132;&#20114;&#33719;&#24471;&#22810;&#35270;&#35282;&#32447;&#32034;&#30340;&#21453;&#24605;&#65292;&#24341;&#23548;&#20195;&#29702;&#21521;&#22810;&#26679;&#24615;&#32780;&#20855;&#26377;&#21487;&#38752;&#24615;&#30340;&#25512;&#29702;&#36712;&#36857;&#21457;&#23637;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#22320;&#38754;&#30495;&#30456;&#65292;&#36890;&#36807;&#40723;&#21169;&#65288;1&#65289;&#23548;&#33322;&#32773;&#29983;&#25104;&#30340;&#26041;&#21521;&#30340;&#22810;&#26679;&#24615;&#19982;&#65288;2&#65289;&#31574;&#30053;&#24615;&#24341;&#21457;&#30340;&#25200;&#21160;&#22312;&#20135;&#29983;&#30340;&#22238;&#24212;&#20013;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14963v1 Announce Type: cross  Abstract: While Large language models (LLMs) have the capability to iteratively reflect on their own outputs, recent studies have observed their struggles with knowledge-rich problems without access to external resources. In addition to the inefficiency of LLMs in self-assessment, we also observe that LLMs struggle to revisit their predictions despite receiving explicit negative feedback. Therefore, We propose Mirror, a Multiple-perspective self-reflection method for knowledge-rich reasoning, to avoid getting stuck at a particular reflection iteration. Mirror enables LLMs to reflect from multiple-perspective clues, achieved through a heuristic interaction between a Navigator and a Reasoner. It guides agents toward diverse yet plausibly reliable reasoning trajectory without access to ground truth by encouraging (1) diversity of directions generated by Navigator and (2) agreement among strategically induced perturbations in responses generated by 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#20869;&#23481;&#26465;&#20214;&#19979;&#30830;&#20445;&#25935;&#24863;&#23646;&#24615;&#19982;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#25913;&#21892;&#20844;&#24179;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20445;&#25345;&#25928;&#29992;&#30340;&#21516;&#26102;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#36866;&#24403;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.14208</link><description>&lt;p&gt;
&#38754;&#21521;&#20844;&#24179;&#25991;&#26412;&#23884;&#20837;&#30340;&#20869;&#23481;&#26465;&#20214;&#21435;&#20559;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Content Conditional Debiasing for Fair Text Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14208
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#20869;&#23481;&#26465;&#20214;&#19979;&#30830;&#20445;&#25935;&#24863;&#23646;&#24615;&#19982;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#25913;&#21892;&#20844;&#24179;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20445;&#25345;&#25928;&#29992;&#30340;&#21516;&#26102;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#36866;&#24403;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#65292;&#20943;&#36731;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#38598;&#20013;&#22312;&#20844;&#24179;&#30340;&#25991;&#26412;&#23884;&#20837;&#19978;&#65292;&#36825;&#23545;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20844;&#24179;&#25991;&#26412;&#23884;&#20837;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#30830;&#20445;&#22312;&#20869;&#23481;&#26465;&#20214;&#19979;&#25935;&#24863;&#23646;&#24615;&#19982;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#26469;&#23454;&#29616;&#20844;&#24179;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#25928;&#29992;&#26435;&#34913;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24378;&#21046;&#35201;&#27714;&#20855;&#26377;&#19981;&#21516;&#25935;&#24863;&#23646;&#24615;&#20294;&#30456;&#21516;&#20869;&#23481;&#30340;&#25991;&#26412;&#30340;&#23884;&#20837;&#19982;&#20854;&#23545;&#24212;&#20013;&#31435;&#25991;&#26412;&#30340;&#23884;&#20837;&#20445;&#25345;&#30456;&#21516;&#30340;&#36317;&#31163;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23558;&#25991;&#26412;&#22686;&#24378;&#20026;&#19981;&#21516;&#30340;&#25935;&#24863;&#32452;&#65292;&#26469;&#35299;&#20915;&#32570;&#20047;&#36866;&#24403;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#20844;&#24179;&#24615;&#21516;&#26102;&#20445;&#25345;&#20102;&#23884;&#20837;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14208v1 Announce Type: cross  Abstract: Mitigating biases in machine learning models has gained increasing attention in Natural Language Processing (NLP). Yet, only a few studies focus on fair text embeddings, which are crucial yet challenging for real-world applications. In this paper, we propose a novel method for learning fair text embeddings. We achieve fairness while maintaining utility trade-off by ensuring conditional independence between sensitive attributes and text embeddings conditioned on the content. Specifically, we enforce that embeddings of texts with different sensitive attributes but identical content maintain the same distance toward the embedding of their corresponding neutral text. Furthermore, we address the issue of lacking proper training data by using Large Language Models (LLMs) to augment texts into different sensitive groups. Our extensive evaluations demonstrate that our approach effectively improves fairness while preserving the utility of embed
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;PromptKD&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#23454;&#29616;&#20102;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#23398;&#29983;&#21451;&#22909;&#30693;&#35782;&#30340;&#33976;&#39311;&#65292;&#26080;&#38656;&#24494;&#35843;&#25972;&#25972;&#20010;&#25945;&#24072;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.12842</link><description>&lt;p&gt;
PromptKD&#65306;&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#20026;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#23398;&#29983;&#21451;&#22909;&#30693;&#35782;&#30340;&#33976;&#39311;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12842
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PromptKD&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#23454;&#29616;&#20102;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#23398;&#29983;&#21451;&#22909;&#30693;&#35782;&#30340;&#33976;&#39311;&#65292;&#26080;&#38656;&#24494;&#35843;&#25972;&#25972;&#20010;&#25945;&#24072;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#24341;&#36215;&#20102;&#23545;&#25512;&#29702;&#25104;&#26412;&#30340;&#25285;&#24551;&#65292;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#23545;&#27169;&#22411;&#21387;&#32553;&#30740;&#31350;&#30340;&#38656;&#27714;&#12290;&#23613;&#31649;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#26159;&#19968;&#31181;&#31361;&#20986;&#30340;&#26041;&#27861;&#65292;&#20294;&#26159;&#38024;&#23545;LLMs&#36825;&#26679;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;KD&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#65292;&#32780;&#25552;&#21462;&#36866;&#21512;&#23398;&#29983;&#30340;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#31867;&#27169;&#22411;&#30340;KD&#20013;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#24615;&#33021;&#65292;&#22312;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PromptKD&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#25552;&#31034;&#35843;&#25972; - &#22312;KD&#20013;&#39318;&#27425;&#20986;&#29616; - &#20351;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20256;&#36882;&#36866;&#21512;&#23398;&#29983;&#30340;&#30693;&#35782;&#12290;&#19982;&#20808;&#21069;&#20998;&#31867;&#24037;&#20316;&#19981;&#21516;&#65292;&#20808;&#21069;&#37027;&#20123;&#38656;&#35201;&#24494;&#35843;&#25972;&#25972;&#20010;&#25945;&#24072;&#27169;&#22411;&#20197;&#25552;&#21462;&#36866;&#21512;&#23398;&#29983;&#30340;&#30693;&#35782;&#65292;PromptKD&#36890;&#36807;&#28155;&#21152;&#23569;&#37327;&#25552;&#31034;&#26631;&#35760;&#65292;&#24182;&#20165;&#36890;&#36807;&#23398;&#29983;&#25351;&#23548;&#35843;&#25972;&#25552;&#31034;&#26469;&#36798;&#21040;&#31867;&#20284;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12842v1 Announce Type: cross  Abstract: Recent advancements in large language models (LLMs) have raised concerns about inference costs, increasing the need for research into model compression. While knowledge distillation (KD) is a prominent method for this, research on KD for generative language models like LLMs is relatively sparse, and the approach of distilling student-friendly knowledge, which has shown promising performance in KD for classification models, remains unexplored in generative language models. To explore this approach, we propose PromptKD, a simple yet effective method that utilizes prompt tuning - for the first time in KD - to enable generative language models to transfer student-friendly knowledge. Unlike previous works in classification that require fine-tuning the entire teacher model for extracting student-friendly knowledge, PromptKD achieves similar effects by adding a small number of prompt tokens and tuning only the prompt with student guidance. Ex
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26500;&#24314;&#29702;&#35770;&#26694;&#26550;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#33258;&#28040;&#32791;&#24490;&#29615;&#20013;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#23545;&#25968;&#25454;&#20998;&#24067;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#22312;&#36275;&#22815;&#22823;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#25110;&#30495;&#23454;&#25968;&#25454;&#27604;&#20363;&#26465;&#20214;&#19979;&#65292;&#21512;&#25104;&#25968;&#25454;&#20998;&#24067;&#19982;&#21407;&#22987;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#24635;&#21464;&#24046;&#36317;&#31163;&#33021;&#22815;&#34987;&#26377;&#25928;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.11778</link><description>&lt;p&gt;
&#26397;&#21521;&#33258;&#28040;&#32791;&#29983;&#25104;&#27169;&#22411;&#30340;&#29702;&#35770;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Towards Theoretical Understandings of Self-Consuming Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11778
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26500;&#24314;&#29702;&#35770;&#26694;&#26550;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#33258;&#28040;&#32791;&#24490;&#29615;&#20013;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#23545;&#25968;&#25454;&#20998;&#24067;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#22312;&#36275;&#22815;&#22823;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#25110;&#30495;&#23454;&#25968;&#25454;&#27604;&#20363;&#26465;&#20214;&#19979;&#65292;&#21512;&#25104;&#25968;&#25454;&#20998;&#24067;&#19982;&#21407;&#22987;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#24635;&#21464;&#24046;&#36317;&#31163;&#33021;&#22815;&#34987;&#26377;&#25928;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#25361;&#25112;&#65292;&#21363;&#22312;&#19968;&#20010;&#33258;&#28040;&#32791;&#24490;&#29615;&#20013;&#35757;&#32451;&#27169;&#22411;&#65292;&#20854;&#20013;&#36830;&#32493;&#30340;&#27169;&#22411;&#19990;&#20195;&#36890;&#36807;&#28151;&#21512;&#20043;&#21069;&#19990;&#20195;&#30340;&#30495;&#23454;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#26469;&#36827;&#34892;&#36882;&#24402;&#35757;&#32451;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#20197;&#20005;&#26684;&#35780;&#20272;&#36825;&#31181;&#35757;&#32451;&#26041;&#26696;&#23545;&#26410;&#26469;&#27169;&#22411;&#23398;&#20064;&#30340;&#25968;&#25454;&#20998;&#24067;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#22312;&#19981;&#21516;&#28151;&#21512;&#35757;&#32451;&#22330;&#26223;&#19979;&#65292;&#26410;&#26469;&#27169;&#22411;&#20135;&#29983;&#30340;&#21512;&#25104;&#25968;&#25454;&#20998;&#24067;&#19982;&#21407;&#22987;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#24635;&#21464;&#24046;&#65288;TV&#65289;&#36317;&#31163;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#28151;&#21512;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#25110;&#30495;&#23454;&#25968;&#25454;&#27604;&#20363;&#36275;&#22815;&#22823;&#30340;&#26465;&#20214;&#19979;&#65292;&#36825;&#31181;&#36317;&#31163;&#21487;&#20197;&#34987;&#26377;&#25928;&#25511;&#21046;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#30001;&#25193;&#22823;&#21512;&#25104;&#25968;&#25454;&#37327;&#24341;&#36215;&#30340;&#30456;&#21464;&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#34429;&#28982;TV&#36317;&#31163;&#34920;&#29616;&#20986;&#21021;&#22987;&#19978;&#21319;&#65292;&#20294;&#21364;&#36880;&#28176;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11778v1 Announce Type: cross  Abstract: This paper tackles the emerging challenge of training generative models within a self-consuming loop, wherein successive generations of models are recursively trained on mixtures of real and synthetic data from previous generations. We construct a theoretical framework to rigorously evaluate how this training regimen impacts the data distributions learned by future models. Specifically, we derive bounds on the total variation (TV) distance between the synthetic data distributions produced by future models and the original real data distribution under various mixed training scenarios. Our analysis demonstrates that this distance can be effectively controlled under the condition that mixed training dataset sizes or proportions of real data are large enough. Interestingly, we further unveil a phase transition induced by expanding synthetic data amounts, proving theoretically that while the TV distance exhibits an initial ascent, it declin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;Stackelberg&#21338;&#24328;&#20013;&#20248;&#21270;&#20247;&#22810;&#26426;&#22120;&#20154;&#30340;&#34892;&#21160;&#39034;&#24207;&#30340;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#39640;&#25928;&#20934;&#30830;&#30340;&#31639;&#27861;(B&amp;P)&#26469;&#27714;&#35299;&#30456;&#20851;&#30340;&#20248;&#21270;&#38382;&#39064;&#21644;&#22343;&#34913;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.09246</link><description>&lt;p&gt;
&#35841;&#20808;&#34892;&#21160;&#65311;&#20248;&#21270;Stackelberg&#21338;&#24328;&#20013;&#20247;&#22810;&#26426;&#22120;&#20154;&#30340;&#34892;&#21160;&#39034;&#24207;
&lt;/p&gt;
&lt;p&gt;
Who Plays First? Optimizing the Order of Play in Stackelberg Games with Many Robots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;Stackelberg&#21338;&#24328;&#20013;&#20248;&#21270;&#20247;&#22810;&#26426;&#22120;&#20154;&#30340;&#34892;&#21160;&#39034;&#24207;&#30340;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#39640;&#25928;&#20934;&#30830;&#30340;&#31639;&#27861;(B&amp;P)&#26469;&#27714;&#35299;&#30456;&#20851;&#30340;&#20248;&#21270;&#38382;&#39064;&#21644;&#22343;&#34913;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#35745;&#31639;&#22810;&#26234;&#33021;&#20307;&#31354;&#38388;&#23548;&#33322;&#38382;&#39064;&#30340;&#31038;&#20250;&#26368;&#20248;&#34892;&#21160;&#39034;&#24207;&#30340;&#38382;&#39064;&#65292;&#21363;&#26234;&#33021;&#20307;&#20915;&#31574;&#39034;&#24207;&#65292;&#20197;&#21450;&#19982;&#20043;&#30456;&#20851;&#30340;N&#20154;Stackelberg&#36712;&#36857;&#21338;&#24328;&#30340;&#22343;&#34913;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#24314;&#27169;&#20026;&#19968;&#20010;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#38382;&#39064;&#65292;&#28041;&#21450;&#21040;&#25152;&#26377;&#21487;&#33021;&#30340;&#34892;&#21160;&#39034;&#24207;&#30340;Stackelberg&#21338;&#24328;&#31354;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Branch and Play (B&amp;P)&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#25910;&#25947;&#21040;&#31038;&#20250;&#26368;&#20248;&#34892;&#21160;&#39034;&#24207;&#21450;&#20854;Stackelberg&#22343;&#34913;&#12290;&#20316;&#20026;B&amp;P&#30340;&#19968;&#20010;&#23376;&#20363;&#31243;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#25193;&#23637;&#20102;&#39034;&#24207;&#36712;&#36857;&#35268;&#21010;&#65292;&#21363;&#19968;&#31181;&#27969;&#34892;&#30340;&#22810;&#26234;&#33021;&#20307;&#25511;&#21046;&#26041;&#27861;&#65292;&#20197;&#20415;&#20026;&#20219;&#20309;&#32473;&#23450;&#30340;&#34892;&#21160;&#39034;&#24207;&#21487;&#25193;&#23637;&#22320;&#35745;&#31639;&#26377;&#25928;&#30340;&#26412;&#22320;Stackelberg&#22343;&#34913;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;B&amp;P&#22312;&#21327;&#35843;&#31354;&#20013;&#20132;&#36890;&#25511;&#21046;&#12289;&#32676;&#20307;&#24418;&#25104;&#21644;&#20132;&#20184;&#36710;&#38431;&#26041;&#38754;&#30340;&#23454;&#38469;&#25928;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;B&amp;P&#30340;&#32467;&#26524;&#26159;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09246v1 Announce Type: cross Abstract: We consider the multi-agent spatial navigation problem of computing the socially optimal order of play, i.e., the sequence in which the agents commit to their decisions, and its associated equilibrium in an N-player Stackelberg trajectory game. We model this problem as a mixed-integer optimization problem over the space of all possible Stackelberg games associated with the order of play's permutations. To solve the problem, we introduce Branch and Play (B&amp;P), an efficient and exact algorithm that provably converges to a socially optimal order of play and its Stackelberg equilibrium. As a subroutine for B&amp;P, we employ and extend sequential trajectory planning, i.e., a popular multi-agent control approach, to scalably compute valid local Stackelberg equilibria for any given order of play. We demonstrate the practical utility of B&amp;P to coordinate air traffic control, swarm formation, and delivery vehicle fleets. We find that B&amp;P consistent
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20998;&#27835;&#31243;&#24207;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#28041;&#21450;&#37325;&#22797;&#23376;&#20219;&#21153;&#21644;/&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#20869;&#23481;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;LLM&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05359</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#27835;&#31243;&#24207;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#38382;&#39064;&#27714;&#35299;&#36827;&#34892;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Guiding Large Language Models with Divide-and-Conquer Program for Discerning Problem Solving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05359
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20998;&#27835;&#31243;&#24207;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#28041;&#21450;&#37325;&#22797;&#23376;&#20219;&#21153;&#21644;/&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#20869;&#23481;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;LLM&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22240;&#20854;&#24191;&#27867;&#30340;&#24212;&#29992;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36866;&#24403;&#30340;&#25552;&#31034;&#35774;&#35745;&#65292;&#22914;&#24605;&#32500;&#38142;&#65292;&#21487;&#20197;&#37322;&#25918;LLM&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22788;&#29702;&#28041;&#21450;&#37325;&#22797;&#23376;&#20219;&#21153;&#21644;/&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#20869;&#23481;&#30340;&#20219;&#21153;&#65288;&#22914;&#31639;&#26415;&#35745;&#31639;&#21644;&#25991;&#31456;&#32423;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#65289;&#65292;&#29616;&#26377;&#30340;&#25552;&#31034;&#31574;&#30053;&#35201;&#20040;&#34920;&#29616;&#20986;&#34920;&#36798;&#33021;&#21147;&#19981;&#36275;&#65292;&#35201;&#20040;&#30001;&#24187;&#35273;&#24341;&#21457;&#20013;&#38388;&#38169;&#35823;&#12290;&#20026;&#20102;&#20351;LLM&#23545;&#36825;&#20123;&#20013;&#38388;&#38169;&#35823;&#26356;&#20855;&#36776;&#21035;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20998;&#27835;&#31243;&#24207;&#24341;&#23548;LLM&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#30830;&#20445;&#20248;&#36234;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#20219;&#21153;&#20998;&#35299;&#12289;&#23376;&#20219;&#21153;&#35299;&#20915;&#21644;&#35299;&#20915;&#32452;&#35013;&#36807;&#31243;&#30340;&#20998;&#31163;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#21487;&#20197;&#24341;&#23548;LLM&#25193;&#23637;&#22266;&#23450;&#28145;&#24230;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Foundation models, such as Large language Models (LLMs), have attracted significant amount of interest due to their large number of applications. Existing works show that appropriate prompt design, such as Chain-of-Thoughts, can unlock LLM's powerful capacity in diverse areas. However, when handling tasks involving repetitive sub-tasks and/or deceptive contents, such as arithmetic calculation and article-level fake news detection, existing prompting strategies either suffers from insufficient expressive power or intermediate errors triggered by hallucination. To make LLM more discerning to such intermediate errors, we propose to guide LLM with a Divide-and-Conquer program that simultaneously ensures superior expressive power and disentangles task decomposition, sub-task resolution, and resolution assembly process. Theoretic analysis reveals that our strategy can guide LLM to extend the expressive power of fixed-depth Transformer. Experiments indicate that our proposed method can achiev
&lt;/p&gt;</description></item><item><title>&#20102;&#35299;&#31070;&#32463;&#32593;&#32476;&#20174;&#36755;&#20837;-&#26631;&#31614;&#23545;&#20013;&#25552;&#21462;&#32479;&#35745;&#20449;&#24687;&#30340;&#26426;&#21046;&#26159;&#30417;&#30563;&#23398;&#20064;&#20013;&#26368;&#37325;&#35201;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#20043;&#19968;&#12290;&#21069;&#20154;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#26435;&#37325;&#30340;&#26684;&#25289;&#22982;&#30697;&#38453;&#19982;&#27169;&#22411;&#30340;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#25104;&#27491;&#27604;&#65292;&#36825;&#34987;&#31216;&#20026;&#31070;&#32463;&#29305;&#24449;&#20998;&#26512;&#65288;NFA&#65289;&#12290;&#26412;&#30740;&#31350;&#35299;&#37322;&#20102;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#20986;&#29616;&#65292;&#24182;&#21457;&#29616;NFA&#31561;&#20215;&#20110;&#26435;&#37325;&#30697;&#38453;&#30340;&#24038;&#22855;&#24322;&#32467;&#26500;&#19982;&#19982;&#36825;&#20123;&#26435;&#37325;&#30456;&#20851;&#30340;&#32463;&#39564;&#31070;&#32463;&#20999;&#32447;&#26680;&#30340;&#26174;&#33879;&#25104;&#20998;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#22312;&#26089;&#26399;&#35757;&#32451;&#38454;&#27573;&#65292;&#21487;&#20197;&#36890;&#36807;&#35299;&#26512;&#30340;&#26041;&#24335;&#39044;&#27979;NFA&#30340;&#21457;&#23637;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.05271</link><description>&lt;p&gt;
&#26799;&#24230;&#19979;&#38477;&#24341;&#21457;&#20102;&#28145;&#24230;&#38750;&#32447;&#24615;&#32593;&#32476;&#26435;&#37325;&#19982;&#32463;&#39564;NTK&#20043;&#38388;&#30340;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Gradient descent induces alignment between weights and the empirical NTK for deep non-linear networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05271
&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#31070;&#32463;&#32593;&#32476;&#20174;&#36755;&#20837;-&#26631;&#31614;&#23545;&#20013;&#25552;&#21462;&#32479;&#35745;&#20449;&#24687;&#30340;&#26426;&#21046;&#26159;&#30417;&#30563;&#23398;&#20064;&#20013;&#26368;&#37325;&#35201;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#20043;&#19968;&#12290;&#21069;&#20154;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#26435;&#37325;&#30340;&#26684;&#25289;&#22982;&#30697;&#38453;&#19982;&#27169;&#22411;&#30340;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#25104;&#27491;&#27604;&#65292;&#36825;&#34987;&#31216;&#20026;&#31070;&#32463;&#29305;&#24449;&#20998;&#26512;&#65288;NFA&#65289;&#12290;&#26412;&#30740;&#31350;&#35299;&#37322;&#20102;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#20986;&#29616;&#65292;&#24182;&#21457;&#29616;NFA&#31561;&#20215;&#20110;&#26435;&#37325;&#30697;&#38453;&#30340;&#24038;&#22855;&#24322;&#32467;&#26500;&#19982;&#19982;&#36825;&#20123;&#26435;&#37325;&#30456;&#20851;&#30340;&#32463;&#39564;&#31070;&#32463;&#20999;&#32447;&#26680;&#30340;&#26174;&#33879;&#25104;&#20998;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#22312;&#26089;&#26399;&#35757;&#32451;&#38454;&#27573;&#65292;&#21487;&#20197;&#36890;&#36807;&#35299;&#26512;&#30340;&#26041;&#24335;&#39044;&#27979;NFA&#30340;&#21457;&#23637;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#20174;&#36755;&#20837;-&#26631;&#31614;&#23545;&#20013;&#25552;&#21462;&#32479;&#35745;&#20449;&#24687;&#30340;&#26426;&#21046;&#26159;&#30417;&#30563;&#23398;&#20064;&#20013;&#26368;&#37325;&#35201;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#20043;&#19968;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#30830;&#23450;&#65292;&#22312;&#19968;&#33324;&#32467;&#26500;&#30340;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#26435;&#37325;&#30340;&#26684;&#25289;&#22982;&#30697;&#38453;&#19982;&#27169;&#22411;&#30340;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#25104;&#27491;&#27604;&#65292;&#36825;&#20010;&#35828;&#27861;&#34987;&#31216;&#20026;&#31070;&#32463;&#29305;&#24449;&#20998;&#26512;&#65288;NFA&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#37327;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22914;&#20309;&#30456;&#20851;&#23578;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#20986;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;NFA&#31561;&#20215;&#20110;&#26435;&#37325;&#30697;&#38453;&#30340;&#24038;&#22855;&#24322;&#32467;&#26500;&#19982;&#19982;&#36825;&#20123;&#26435;&#37325;&#30456;&#20851;&#30340;&#32463;&#39564;&#31070;&#32463;&#20999;&#32447;&#26680;&#30340;&#26174;&#33879;&#25104;&#20998;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20808;&#21069;&#30740;&#31350;&#20013;&#24341;&#20837;&#30340;NFA&#26159;&#30001;&#38548;&#31163;&#36825;&#31181;&#23545;&#40784;&#30340;&#20013;&#24515;&#21270;NFA&#39537;&#21160;&#30340;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#26089;&#26399;&#35757;&#32451;&#38454;&#27573;&#65292;&#21487;&#20197;&#36890;&#36807;&#35299;&#26512;&#30340;&#26041;&#24335;&#39044;&#27979;NFA&#30340;&#21457;&#23637;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the mechanisms through which neural networks extract statistics from input-label pairs is one of the most important unsolved problems in supervised learning. Prior works have identified that the gram matrices of the weights in trained neural networks of general architectures are proportional to the average gradient outer product of the model, in a statement known as the Neural Feature Ansatz (NFA). However, the reason these quantities become correlated during training is poorly understood. In this work, we explain the emergence of this correlation. We identify that the NFA is equivalent to alignment between the left singular structure of the weight matrices and a significant component of the empirical neural tangent kernels associated with those weights. We establish that the NFA introduced in prior works is driven by a centered NFA that isolates this alignment. We show that the speed of NFA development can be predicted analytically at early training times in terms of sim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#39046;&#22495;&#29305;&#23450;&#22270;&#20687;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.04929</link><description>&lt;p&gt;
&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#25193;&#25955;&#24341;&#23548;&#28304;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Source-Free Domain Adaptation with Diffusion-Guided Source Data Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#39046;&#22495;&#29305;&#23450;&#22270;&#20687;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#65288;DM-SFDA&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;DM-SFDA&#26041;&#27861;&#21253;&#25324;&#23545;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20351;&#29992;&#30446;&#26631;&#22270;&#20687;&#30340;&#29305;&#24449;&#26469;&#25351;&#23548;&#25193;&#25955;&#36807;&#31243;&#29983;&#25104;&#28304;&#22495;&#22270;&#20687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#34987;&#24494;&#35843;&#20197;&#29983;&#25104;&#26368;&#23567;&#21270;&#29109;&#24182;&#26368;&#22823;&#21270;&#39044;&#35757;&#32451;&#28304;&#27169;&#22411;&#32622;&#20449;&#24230;&#30340;&#28304;&#26679;&#26412;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#24050;&#24314;&#31435;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#23558;&#29983;&#25104;&#30340;&#28304;&#22270;&#20687;&#19982;&#30446;&#26631;&#22495;&#25968;&#25454;&#36827;&#34892;&#23545;&#40784;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#19968;&#31995;&#21015;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;Office-31&#12289;Office-Home&#21644;VisDA&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#23637;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#12289;&#39046;&#22495;&#29305;&#23450;&#30340;&#22270;&#20687;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel approach to leverage the generalizability capability of Diffusion Models for Source-Free Domain Adaptation (DM-SFDA). Our proposed DM-SFDA method involves fine-tuning a pre-trained text-to-image diffusion model to generate source domain images using features from the target images to guide the diffusion process. Specifically, the pre-trained diffusion model is fine-tuned to generate source samples that minimize entropy and maximize confidence for the pre-trained source model. We then apply established unsupervised domain adaptation techniques to align the generated source images with target domain data. We validate our approach through comprehensive experiments across a range of datasets, including Office-31, Office-Home, and VisDA. The results highlight significant improvements in SFDA performance, showcasing the potential of diffusion models in generating contextually relevant, domain-specific images.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#35299;&#37322;&#26694;&#26550;&#65288;xLLM&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#36890;&#36807;&#19968;&#20010;&#35780;&#20272;&#22120;&#26469;&#37327;&#21270;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#26469;&#25552;&#39640;&#21487;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.04678</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#21487;&#20449;&#30340;&#35299;&#37322;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models As Faithful Explainers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#35299;&#37322;&#26694;&#26550;&#65288;xLLM&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#36890;&#36807;&#19968;&#20010;&#35780;&#20272;&#22120;&#26469;&#37327;&#21270;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#26469;&#25552;&#39640;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36890;&#36807;&#21033;&#29992;&#20854;&#20016;&#23500;&#30340;&#20869;&#37096;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24050;&#32463;&#33021;&#22815;&#29087;&#32451;&#35299;&#20915;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22797;&#26434;&#24615;&#38459;&#30861;&#20102;&#20256;&#32479;&#30340;&#20197;&#36755;&#20837;&#20026;&#37325;&#28857;&#30340;&#35299;&#37322;&#31639;&#27861;&#26469;&#35299;&#37322;LLMs&#30340;&#22797;&#26434;&#20915;&#31574;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#20986;&#29616;&#20102;&#19968;&#31181;&#33258;&#25105;&#35299;&#37322;&#26426;&#21046;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#30340;&#24418;&#24335;&#36827;&#34892;&#21333;&#21521;&#25512;&#29702;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;LLMs&#39044;&#27979;&#30340;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#32463;&#24120;&#22240;&#20026;&#32570;&#20047;&#21487;&#20449;&#24230;&#32780;&#21463;&#21040;&#25209;&#35780;&#65292;&#22240;&#20026;&#36825;&#20123;&#35299;&#37322;&#21487;&#33021;&#19981;&#20934;&#30830;&#22320;&#21453;&#26144;LLMs&#30340;&#20915;&#31574;&#34892;&#20026;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29983;&#25104;&#35299;&#37322;&#26694;&#26550;xLLM&#65292;&#20197;&#25552;&#39640;LLMs&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#30340;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#22120;&#26469;&#37327;&#21270;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#36890;&#36807;xLLM&#30340;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#26469;&#25552;&#39640;&#21487;&#20449;&#24230;&#65292;&#30446;&#26631;&#26159;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have recently become proficient in addressing complex tasks by utilizing their rich internal knowledge and reasoning ability. Consequently, this complexity hinders traditional input-focused explanation algorithms for explaining the complex decision-making processes of LLMs. Recent advancements have thus emerged for self-explaining their predictions through a single feed-forward inference in a natural language format. However, natural language explanations are often criticized for lack of faithfulness since these explanations may not accurately reflect the decision-making behaviors of the LLMs. In this work, we introduce a generative explanation framework, xLLM, to improve the faithfulness of the explanations provided in natural language formats for LLMs. Specifically, we propose an evaluator to quantify the faithfulness of natural language explanation and enhance the faithfulness by an iterative optimization process of xLLM, with the goal of maximizing the 
&lt;/p&gt;</description></item><item><title>EasyInstruct&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#25351;&#20196;&#29983;&#25104;&#12289;&#36873;&#25321;&#21644;&#25552;&#31034;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#30340;&#32452;&#21512;&#21644;&#20132;&#20114;&#65292;&#20351;&#25351;&#20196;&#22788;&#29702;&#26356;&#21152;&#26041;&#20415;&#21644;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.03049</link><description>&lt;p&gt;
EasyInstruct&#65306;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03049
&lt;/p&gt;
&lt;p&gt;
EasyInstruct&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#25351;&#20196;&#29983;&#25104;&#12289;&#36873;&#25321;&#21644;&#25552;&#31034;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#30340;&#32452;&#21512;&#21644;&#20132;&#20114;&#65292;&#20351;&#25351;&#20196;&#22788;&#29702;&#26356;&#21152;&#26041;&#20415;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25351;&#20196;&#35843;&#25972;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#24182;&#25104;&#20026;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#21147;&#30340;&#19968;&#31181;&#20851;&#38190;&#25216;&#26415;&#12290;&#20026;&#20102;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#25351;&#20196;&#22788;&#29702;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#25968;&#25454;&#25968;&#37327;&#21644;&#25968;&#25454;&#36136;&#37327;&#20043;&#38388;&#36798;&#21040;&#31934;&#24039;&#30340;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21508;&#31181;&#25351;&#20196;&#22788;&#29702;&#26041;&#27861;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#19981;&#19968;&#33268;&#65292;&#30446;&#21069;&#27809;&#26377;&#26631;&#20934;&#30340;&#24320;&#28304;&#25351;&#20196;&#22788;&#29702;&#23454;&#29616;&#26694;&#26550;&#21487;&#20379;&#31038;&#21306;&#20351;&#29992;&#65292;&#36825;&#20351;&#24471;&#20174;&#19994;&#32773;&#26080;&#27861;&#36827;&#19968;&#27493;&#24320;&#21457;&#21644;&#25512;&#36827;&#12290;&#20026;&#20102;&#20419;&#36827;&#25351;&#20196;&#22788;&#29702;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EasyInstruct&#65292;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;LLMs&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;&#65292;&#23427;&#23558;&#25351;&#20196;&#29983;&#25104;&#12289;&#36873;&#25321;&#21644;&#25552;&#31034;&#27169;&#22359;&#21270;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#30340;&#32452;&#21512;&#21644;&#20132;&#20114;&#12290;EasyInstruct&#24050;&#32463;&#22312;https://github.com/zjunlp/EasyInstruct&#19978;&#20844;&#24320;&#21457;&#24067;&#65292;&#24182;&#24471;&#21040;&#20102;&#31215;&#26497;&#32500;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, instruction tuning has gained increasing attention and emerged as a crucial technique to enhance the capabilities of Large Language Models (LLMs). To construct high-quality instruction datasets, many instruction processing approaches have been proposed, aiming to achieve a delicate balance between data quantity and data quality. Nevertheless, due to inconsistencies that persist among various instruction processing methods, there is no standard open-source instruction processing implementation framework available for the community, which hinders practitioners from further developing and advancing. To facilitate instruction processing research and development, we present EasyInstruct, an easy-to-use instruction processing framework for LLMs, which modularizes instruction generation, selection, and prompting, while also considering their combination and interaction. EasyInstruct is publicly released and actively maintained at https://github.com/zjunlp/EasyInstruct, along 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#30340;&#38477;&#38454;&#27169;&#22411;&#30340;&#23454;&#29992;&#23384;&#22312;&#23450;&#29702;&#65292;&#35299;&#20915;&#20102;&#22312;&#22788;&#29702;&#22797;&#26434;&#38750;&#32447;&#24615;&#38382;&#39064;&#26041;&#38754;&#20256;&#32479;&#26041;&#27861;&#30340;&#19981;&#36275;&#65292;&#24182;&#35752;&#35770;&#20102;&#22914;&#20309;&#23398;&#20064;&#28508;&#22312;&#29305;&#24449;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.00435</link><description>&lt;p&gt;
&#22522;&#20110;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#30340;&#38477;&#38454;&#27169;&#22411;&#30340;&#23454;&#29992;&#23384;&#22312;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
A practical existence theorem for reduced order models based on convolutional autoencoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#30340;&#38477;&#38454;&#27169;&#22411;&#30340;&#23454;&#29992;&#23384;&#22312;&#23450;&#29702;&#65292;&#35299;&#20915;&#20102;&#22312;&#22788;&#29702;&#22797;&#26434;&#38750;&#32447;&#24615;&#38382;&#39064;&#26041;&#38754;&#20256;&#32479;&#26041;&#27861;&#30340;&#19981;&#36275;&#65292;&#24182;&#35752;&#35770;&#20102;&#22914;&#20309;&#23398;&#20064;&#28508;&#22312;&#29305;&#24449;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#20559;&#24494;&#20998;&#26041;&#31243;&#21644;&#38477;&#38454;&#24314;&#27169;&#39046;&#22495;&#36234;&#21457;&#21463;&#27426;&#36814;&#65292;&#25552;&#20379;&#20102;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#12289;&#31070;&#32463;&#31639;&#23376;&#12289;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;&#21644;&#28145;&#24230;&#23398;&#20064;&#38477;&#38454;&#27169;&#22411;&#31561;&#24378;&#22823;&#30340;&#25968;&#25454;&#39537;&#21160;&#25216;&#26415;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;&#34920;&#29616;&#20986;&#26497;&#39640;&#30340;&#25928;&#26524;&#65292;&#22312;&#22788;&#29702;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#38382;&#39064;&#26102;&#65292;&#20248;&#20110;&#20256;&#32479;&#30340;&#38477;&#38454;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22522;&#20110;CNN&#30340;&#33258;&#32534;&#30721;&#22120;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30446;&#21069;&#21482;&#26377;&#23569;&#25968;&#29702;&#35770;&#32467;&#26524;&#25903;&#25345;&#36825;&#20123;&#26550;&#26500;&#65292;&#36890;&#24120;&#20197;&#19975;&#33021;&#36924;&#36817;&#23450;&#29702;&#30340;&#24418;&#24335;&#38472;&#36848;&#12290;&#23588;&#20854;&#26159;&#65292;&#23613;&#31649;&#29616;&#26377;&#25991;&#29486;&#20026;&#35774;&#35745;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#25552;&#20379;&#20102;&#25351;&#23548;&#26041;&#38024;&#65292;&#20294;&#23398;&#20064;&#28508;&#22312;&#29305;&#24449;&#30340;&#21518;&#32493;&#25361;&#25112;&#20960;&#20046;&#27809;&#26377;&#34987;&#25506;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, deep learning has gained increasing popularity in the fields of Partial Differential Equations (PDEs) and Reduced Order Modeling (ROM), providing domain practitioners with new powerful data-driven techniques such as Physics-Informed Neural Networks (PINNs), Neural Operators, Deep Operator Networks (DeepONets) and Deep-Learning based ROMs (DL-ROMs). In this context, deep autoencoders based on Convolutional Neural Networks (CNNs) have proven extremely effective, outperforming established techniques, such as the reduced basis method, when dealing with complex nonlinear problems. However, despite the empirical success of CNN-based autoencoders, there are only a few theoretical results supporting these architectures, usually stated in the form of universal approximation theorems. In particular, although the existing literature provides users with guidelines for designing convolutional autoencoders, the subsequent challenge of learning the latent features has been barely inv
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25972;&#21512;&#36827;&#21270;&#31639;&#27861;&#19982;&#24378;&#21270;&#23398;&#20064;&#65292;&#36827;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;ERL&#65289;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#26412;&#32508;&#36848;&#21576;&#29616;&#20102;ERL&#39046;&#22495;&#30340;&#21508;&#20010;&#30740;&#31350;&#20998;&#25903;&#65292;&#31361;&#20986;&#20102;EA&#36741;&#21161;RL&#30340;&#20248;&#21270;&#12289;RL&#36741;&#21161;EA&#30340;&#20248;&#21270;&#20197;&#21450;EA&#21644;RL&#30340;&#21327;&#21516;&#20248;&#21270;&#36825;&#19977;&#20010;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2401.11963</link><description>&lt;p&gt;
&#36328;&#36234;&#36827;&#21270;&#31639;&#27861;&#21644;&#24378;&#21270;&#23398;&#20064;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Bridging Evolutionary Algorithms and Reinforcement Learning: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11963
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#36827;&#21270;&#31639;&#27861;&#19982;&#24378;&#21270;&#23398;&#20064;&#65292;&#36827;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;ERL&#65289;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#26412;&#32508;&#36848;&#21576;&#29616;&#20102;ERL&#39046;&#22495;&#30340;&#21508;&#20010;&#30740;&#31350;&#20998;&#25903;&#65292;&#31361;&#20986;&#20102;EA&#36741;&#21161;RL&#30340;&#20248;&#21270;&#12289;RL&#36741;&#21161;EA&#30340;&#20248;&#21270;&#20197;&#21450;EA&#21644;RL&#30340;&#21327;&#21516;&#20248;&#21270;&#36825;&#19977;&#20010;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;ERL&#65289;&#23558;&#36827;&#21270;&#31639;&#27861;&#65288;EAs&#65289;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30456;&#32467;&#21512;&#36827;&#34892;&#20248;&#21270;&#65292;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#36890;&#36807;&#34701;&#21512;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;ERL&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#26412;&#35843;&#26597;&#32508;&#36848;&#20102;ERL&#20013;&#19981;&#21516;&#30740;&#31350;&#20998;&#25903;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#31995;&#32479;&#24635;&#32467;&#20102;&#30456;&#20851;&#31639;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#30830;&#23450;&#20102;&#19977;&#20010;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#65306;EA&#36741;&#21161;RL&#30340;&#20248;&#21270;&#65292;RL&#36741;&#21161;EA&#30340;&#20248;&#21270;&#65292;&#20197;&#21450;EA&#21644;RL&#30340;&#21327;&#21516;&#20248;&#21270;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#28145;&#20837;&#20998;&#26512;&#20102;&#27599;&#20010;&#30740;&#31350;&#26041;&#21521;&#65292;&#32452;&#32455;&#20102;&#22810;&#20010;&#30740;&#31350;&#20998;&#25903;&#12290;&#25105;&#20204;&#38416;&#26126;&#20102;&#27599;&#20010;&#20998;&#25903;&#33268;&#21147;&#20110;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;EA&#21644;RL&#30340;&#25972;&#21512;&#22914;&#20309;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#28508;&#22312;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11963v2 Announce Type: replace-cross  Abstract: Evolutionary Reinforcement Learning (ERL), which integrates Evolutionary Algorithms (EAs) and Reinforcement Learning (RL) for optimization, has demonstrated remarkable performance advancements. By fusing the strengths of both approaches, ERL has emerged as a promising research direction. This survey offers a comprehensive overview of the diverse research branches in ERL. Specifically, we systematically summarize recent advancements in relevant algorithms and identify three primary research directions: EA-assisted optimization of RL, RL-assisted optimization of EA, and synergistic optimization of EA and RL. Following that, we conduct an in-depth analysis of each research direction, organizing multiple research branches. We elucidate the problems that each branch aims to tackle and how the integration of EA and RL addresses these challenges. In conclusion, we discuss potential challenges and prospective future research directions
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#22312;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65288;SR&#65289;&#39046;&#22495;&#20135;&#29983;&#20102;&#39072;&#35206;&#24615;&#30340;&#24433;&#21709;&#65292;&#32553;&#23567;&#20102;&#22270;&#20687;&#36136;&#37327;&#19982;&#20154;&#31867;&#24863;&#30693;&#20559;&#22909;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;DM&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#20998;&#26512;&#20102;&#20854;&#29420;&#29305;&#29305;&#28857;&#21644;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#26367;&#20195;&#36755;&#20837;&#39046;&#22495;&#31561;&#24403;&#21069;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2401.00736</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#12289;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#21644;&#19968;&#20999;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models, Image Super-Resolution And Everything: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.00736
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#22312;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65288;SR&#65289;&#39046;&#22495;&#20135;&#29983;&#20102;&#39072;&#35206;&#24615;&#30340;&#24433;&#21709;&#65292;&#32553;&#23567;&#20102;&#22270;&#20687;&#36136;&#37327;&#19982;&#20154;&#31867;&#24863;&#30693;&#20559;&#22909;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;DM&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#20998;&#26512;&#20102;&#20854;&#29420;&#29305;&#29305;&#28857;&#21644;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#26367;&#20195;&#36755;&#20837;&#39046;&#22495;&#31561;&#24403;&#21069;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#22312;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65288;SR&#65289;&#39046;&#22495;&#20013;&#20135;&#29983;&#20102;&#39072;&#35206;&#24615;&#30340;&#24433;&#21709;&#65292;&#36827;&#19968;&#27493;&#32553;&#23567;&#20102;&#22270;&#20687;&#36136;&#37327;&#19982;&#20154;&#31867;&#24863;&#30693;&#20559;&#22909;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#23427;&#20204;&#26131;&#20110;&#35757;&#32451;&#65292;&#24182;&#33021;&#29983;&#25104;&#27604;&#20197;&#21069;&#30340;&#29983;&#25104;&#26041;&#27861;&#20135;&#29983;&#30340;&#26679;&#26412;&#26356;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#20063;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#65306;&#39640;&#35745;&#31639;&#38656;&#27714;&#12289;&#21487;&#27604;&#24615;&#12289;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12289;&#33394;&#24425;&#20559;&#31227;&#31561;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;&#22823;&#37327;&#30340;&#20986;&#29256;&#29289;&#65292;&#36827;&#20837;&#36825;&#20010;&#39046;&#22495;&#20196;&#20154;&#38590;&#20197;&#24212;&#23545;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21465;&#36848;&#65292;&#38416;&#26126;&#20102;&#24212;&#29992;&#20110;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#30340;DM&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20221;&#35814;&#32454;&#30340;&#20998;&#26512;&#65292;&#31361;&#20986;&#20102;&#35813;&#39046;&#22495;&#20869;&#19982;&#20854;&#20182;&#32508;&#36848;&#25991;&#31456;&#19981;&#21516;&#30340;&#29420;&#29305;&#29305;&#28857;&#21644;&#26041;&#27861;&#12290;&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#23545;DM&#30340;&#21407;&#21017;&#36827;&#34892;&#20102;&#19968;&#20010;&#36830;&#36143;&#30340;&#29702;&#35299;&#65292;&#24182;&#25506;&#32034;&#20102;&#24403;&#21069;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#21253;&#25324;&#26367;&#20195;&#36755;&#20837;&#39046;&#22495;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models (DMs) have disrupted the image Super-Resolution (SR) field and further closed the gap between image quality and human perceptual preferences. They are easy to train and can produce very high-quality samples that exceed the realism of those produced by previous generative methods. Despite their promising results, they also come with new challenges that need further research: high computational demands, comparability, lack of explainability, color shifts, and more. Unfortunately, entry into this field is overwhelming because of the abundance of publications. To address this, we provide a unified recount of the theoretical foundations underlying DMs applied to image SR and offer a detailed analysis that underscores the unique characteristics and methodologies within this domain, distinct from broader existing reviews in the field. This survey articulates a cohesive understanding of DM principles and explores current research avenues, including alternative input domains, c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#37327;&#21270;&#25351;&#26631;&#26469;&#34913;&#37327;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#35299;&#37322;&#30340;&#21487;&#20449;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#26469;&#25913;&#21892;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#35299;&#37322;&#30340;&#19968;&#33268;&#24615;&#21644;&#20445;&#30495;&#24230;&#12290;</title><link>https://arxiv.org/abs/2310.04910</link><description>&lt;p&gt;
&#20851;&#20110;&#24120;&#35782;&#25512;&#29702;&#30340;&#30693;&#35782;&#22270;&#35889;&#35299;&#37322;&#30340;&#21487;&#20449;&#24615;
&lt;/p&gt;
&lt;p&gt;
Faithful Knowledge Graph Explanations for Commonsense Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.04910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#37327;&#21270;&#25351;&#26631;&#26469;&#34913;&#37327;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#35299;&#37322;&#30340;&#21487;&#20449;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#26469;&#25913;&#21892;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#35299;&#37322;&#30340;&#19968;&#33268;&#24615;&#21644;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34701;&#21512;&#35821;&#35328;&#27169;&#22411;(LMs)&#21644;&#30693;&#35782;&#22270;&#35889;(KGs)&#24050;&#25104;&#20026;&#24120;&#35782;&#38382;&#31572;&#30740;&#31350;&#20013;&#30340;&#24120;&#35265;&#26041;&#27861;&#65292;&#20294;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#23454;&#29616;&#31934;&#30830;&#30340;&#24605;&#36335;&#38142;&#35299;&#37322;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#24403;&#21069;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#35299;&#37322;&#25216;&#26415;&#30340;&#19968;&#20010;&#20027;&#35201;&#24369;&#28857;&#26159;&#22312;&#35780;&#20272;&#36807;&#31243;&#20013;&#24573;&#35270;&#20102;&#29983;&#25104;&#35299;&#37322;&#30340;&#21487;&#20449;&#24615;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#39564;&#35777;&#20102;&#20004;&#20010;&#37327;&#21270;&#25351;&#26631; - &#22270;&#19968;&#33268;&#24615;&#21644;&#22270;&#20445;&#30495;&#24230; - &#26469;&#34913;&#37327;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#35299;&#37322;&#30340;&#21487;&#20449;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;Consistent GNN (CGNN)&#65292;&#35813;&#26041;&#27861;&#28155;&#21152;&#20102;&#19968;&#39033;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#39033;&#26469;&#25913;&#21892;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;KG&#30340;&#39044;&#27979;&#32463;&#24120;&#20559;&#31163;&#21407;&#22987;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#25152;&#25552;&#20986;&#30340;CGNN&#26041;&#27861;&#25552;&#39640;&#20102;&#19968;&#33268;&#24615;&#21644;&#20445;&#30495;&#24230;&#65292;&#23637;&#31034;&#20102;&#23427;&#20135;&#29983;&#26356;&#21487;&#20449;&#35299;&#37322;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;&#26126;&#30830;&#35780;&#20272;&#35299;&#37322;&#21487;&#20449;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While fusing language models (LMs) and knowledge graphs (KGs) has become common in commonsense question answering research, enabling faithful chain-of-thought explanations in these models remains an open problem. One major weakness of current KG-based explanation techniques is that they overlook the faithfulness of generated explanations during evaluation. To address this gap, we make two main contributions: (1) We propose and validate two quantitative metrics - graph consistency and graph fidelity - to measure the faithfulness of KG-based explanations. (2) We introduce Consistent GNN (CGNN), a novel training method that adds a consistency regularization term to improve explanation faithfulness. Our analysis shows that predictions from KG often diverge from original model predictions. The proposed CGNN approach boosts consistency and fidelity, demonstrating its potential for producing more faithful explanations. Our work emphasises the importance of explicitly evaluating suggest a path
&lt;/p&gt;</description></item><item><title>EasyEdit&#25552;&#20986;&#20102;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#30340;&#30693;&#35782;&#32534;&#36753;&#26694;&#26550;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#25130;&#26029;&#25110;&#35884;&#35823;&#38382;&#39064;&#65292;&#25903;&#25345;&#21508;&#31181;&#26368;&#26032;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#22810;&#20010;&#30693;&#21517;&#30340;LLMs&#12290;</title><link>https://arxiv.org/abs/2308.07269</link><description>&lt;p&gt;
EasyEdit&#65306;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.07269
&lt;/p&gt;
&lt;p&gt;
EasyEdit&#25552;&#20986;&#20102;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#30340;&#30693;&#35782;&#32534;&#36753;&#26694;&#26550;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#25130;&#26029;&#25110;&#35884;&#35823;&#38382;&#39064;&#65292;&#25903;&#25345;&#21508;&#31181;&#26368;&#26032;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#22810;&#20010;&#30693;&#21517;&#30340;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#36973;&#21463;&#30693;&#35782;&#25130;&#26029;&#25110;&#35884;&#35823;&#38382;&#39064;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#23545;&#26410;&#35265;&#20107;&#20214;&#19981;&#30693;&#24773;&#25110;&#29983;&#25104;&#20855;&#26377;&#19981;&#27491;&#30830;&#20107;&#23454;&#30340;&#25991;&#26412;&#65292;&#21407;&#22240;&#26159;&#25968;&#25454;&#36807;&#26102;/&#22024;&#26434;&#12290;&#20026;&#27492;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#38024;&#23545;LLMs&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#65292;&#26088;&#22312;&#24494;&#22937;&#22320;&#27880;&#20837;/&#32534;&#36753;&#26356;&#26032;&#30340;&#30693;&#35782;&#25110;&#35843;&#25972;&#19981;&#33391;&#34892;&#20026;&#65292;&#21516;&#26102;&#23558;&#23545;&#19981;&#30456;&#20851;&#36755;&#20837;&#30340;&#24433;&#21709;&#26368;&#23567;&#21270;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21508;&#31181;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#20197;&#21450;&#20219;&#21153;&#35774;&#32622;&#20013;&#30340;&#21464;&#21270;&#65292;&#31038;&#21306;&#20013;&#27809;&#26377;&#21487;&#29992;&#20110;&#30693;&#35782;&#32534;&#36753;&#30340;&#26631;&#20934;&#23454;&#26045;&#26694;&#26550;&#65292;&#36825;&#22952;&#30861;&#20102;&#20174;&#19994;&#32773;&#23558;&#30693;&#35782;&#32534;&#36753;&#24212;&#29992;&#20110;&#24212;&#29992;&#31243;&#24207;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EasyEdit&#65292;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#30340;LLMs&#30693;&#35782;&#32534;&#36753;&#26694;&#26550;&#12290;&#23427;&#25903;&#25345;&#21508;&#31181;&#23574;&#31471;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#35768;&#22810;&#33879;&#21517;&#30340;LLMs&#65292;&#22914;T5&#12289;GPT-J&#12289;LlaMA&#31561;&#12290;&#20174;&#32463;&#39564;&#19978;&#26469;&#30475;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;kno
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.07269v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) usually suffer from knowledge cutoff or fallacy issues, which means they are unaware of unseen events or generate text with incorrect facts owing to outdated/noisy data. To this end, many knowledge editing approaches for LLMs have emerged -- aiming to subtly inject/edit updated knowledge or adjust undesired behavior while minimizing the impact on unrelated inputs. Nevertheless, due to significant differences among various knowledge editing methods and the variations in task setups, there is no standard implementation framework available for the community, which hinders practitioners from applying knowledge editing to applications. To address these issues, we propose EasyEdit, an easy-to-use knowledge editing framework for LLMs. It supports various cutting-edge knowledge editing approaches and can be readily applied to many well-known LLMs such as T5, GPT-J, LlaMA, etc. Empirically, we report the kno
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35843;&#30740;&#20102;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#26041;&#27861;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;&#65292;&#23545;&#20110;&#30701;&#25991;&#26412;&#21644;&#36328;&#35821;&#35328;&#25991;&#26723;&#31561;&#21508;&#31181;&#22330;&#26223;&#25552;&#20379;&#20102;&#31995;&#32479;&#24615;&#30340;&#32452;&#32455;&#21644;&#20171;&#32461;&#65292;&#24182;&#35752;&#35770;&#20102;&#24191;&#27867;&#24212;&#29992;&#30340;&#19968;&#31995;&#21015;&#28909;&#38376;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.15351</link><description>&lt;p&gt;
&#20851;&#20110;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#32508;&#36848;&#65306;&#26041;&#27861;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A Survey on Neural Topic Models: Methods, Applications, and Challenges. (arXiv:2401.15351v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15351
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35843;&#30740;&#20102;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#26041;&#27861;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;&#65292;&#23545;&#20110;&#30701;&#25991;&#26412;&#21644;&#36328;&#35821;&#35328;&#25991;&#26723;&#31561;&#21508;&#31181;&#22330;&#26223;&#25552;&#20379;&#20102;&#31995;&#32479;&#24615;&#30340;&#32452;&#32455;&#21644;&#20171;&#32461;&#65292;&#24182;&#35752;&#35770;&#20102;&#24191;&#27867;&#24212;&#29992;&#30340;&#19968;&#31995;&#21015;&#28909;&#38376;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#27169;&#22411;&#20960;&#21313;&#24180;&#26469;&#19968;&#30452;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#26080;&#30417;&#30563;&#26041;&#24335;&#19979;&#21457;&#29616;&#28508;&#22312;&#20027;&#39064;&#21644;&#25512;&#26029;&#25991;&#26723;&#30340;&#20027;&#39064;&#27604;&#20363;&#12290;&#23427;&#20204;&#22312;&#25991;&#26412;&#20998;&#26512;&#21644;&#19978;&#19979;&#25991;&#25512;&#33616;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#23835;&#36215;&#20419;&#25104;&#20102;&#19968;&#20010;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#8212;&#8212;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;(NTMs)&#30340;&#20986;&#29616;&#12290;&#19982;&#20256;&#32479;&#30340;&#20027;&#39064;&#27169;&#22411;&#19981;&#21516;&#65292;NTMs&#30452;&#25509;&#20248;&#21270;&#21442;&#25968;&#65292;&#32780;&#19981;&#38656;&#35201;&#27169;&#22411;&#29305;&#23450;&#30340;&#25512;&#23548;&#12290;&#36825;&#20351;&#24471;NTMs&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#21560;&#24341;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#20851;&#27880;&#24182;&#20135;&#29983;&#20102;&#20016;&#23500;&#30340;&#26032;&#26041;&#27861;&#21644;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#26041;&#27861;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#30740;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26681;&#25454;&#32593;&#32476;&#32467;&#26500;&#31995;&#32479;&#22320;&#32452;&#32455;&#20102;&#24403;&#21069;NTM&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#38024;&#23545;&#30701;&#25991;&#26412;&#21644;&#36328;&#35821;&#35328;&#25991;&#26723;&#31561;&#21508;&#31181;&#22330;&#26223;&#30340;NTMs&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#24191;&#27867;&#24212;&#29992;&#30340;&#19968;&#31995;&#21015;&#28909;&#38376;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topic models have been prevalent for decades to discover latent topics and infer topic proportions of documents in an unsupervised fashion. They have been widely used in various applications like text analysis and context recommendation. Recently, the rise of neural networks has facilitated the emergence of a new research field -- Neural Topic Models (NTMs). Different from conventional topic models, NTMs directly optimize parameters without requiring model-specific derivations. This endows NTMs with better scalability and flexibility, resulting in significant research attention and plentiful new methods and applications. In this paper, we present a comprehensive survey on neural topic models concerning methods, applications, and challenges. Specifically, we systematically organize current NTM methods according to their network structures and introduce the NTMs for various scenarios like short texts and cross-lingual documents. We also discuss a wide range of popular applications built 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32599;&#39532;&#21270;&#24418;&#24335;&#30340;&#25991;&#26412;&#20316;&#20026;&#25509;&#21475;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#21360;&#22320;&#35821;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#32599;&#39532;&#21270;&#25991;&#26412;&#19981;&#20165;&#25552;&#39640;&#20102;&#25512;&#29702;&#25928;&#29575;&#65292;&#36824;&#22312;&#26377;&#38480;&#30340;&#39044;&#35757;&#32451;&#19979;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#32599;&#39532;&#21270;&#26377;&#28508;&#21147;&#24357;&#21512;&#22823;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20013;&#30340;&#35821;&#35328;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2401.14280</link><description>&lt;p&gt;
RomanSetu: &#36890;&#36807;&#32599;&#39532;&#21270;&#26377;&#25928;&#22320;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
RomanSetu: Efficiently unlocking multilingual capabilities of Large Language Models models via Romanization. (arXiv:2401.14280v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32599;&#39532;&#21270;&#24418;&#24335;&#30340;&#25991;&#26412;&#20316;&#20026;&#25509;&#21475;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#21360;&#22320;&#35821;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#32599;&#39532;&#21270;&#25991;&#26412;&#19981;&#20165;&#25552;&#39640;&#20102;&#25512;&#29702;&#25928;&#29575;&#65292;&#36824;&#22312;&#26377;&#38480;&#30340;&#39044;&#35757;&#32451;&#19979;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#32599;&#39532;&#21270;&#26377;&#28508;&#21147;&#24357;&#21512;&#22823;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20013;&#30340;&#35821;&#35328;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21040;&#38750;&#33521;&#35821;&#35821;&#35328;&#65288;&#29305;&#21035;&#26159;&#20351;&#29992;&#38750;&#25289;&#19969;&#23383;&#27597;&#34920;&#30340;&#35821;&#35328;&#65289;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#32599;&#39532;&#21270;&#24418;&#24335;&#30340;&#25991;&#26412;&#20316;&#20026;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25509;&#21475;&#65292;&#20551;&#35774;&#39057;&#32321;&#30340;&#38750;&#27491;&#24335;&#20351;&#29992;&#21644;&#19982;&#33521;&#35821;&#20849;&#20139;&#30340;&#26631;&#35760;&#26377;&#21161;&#20110;&#36328;&#35821;&#35328;&#23545;&#40784;&#12290;&#25105;&#20204;&#20197;&#21360;&#22320;&#35821;&#20026;&#37325;&#28857;&#65292;&#36890;&#36807;&#21360;&#22320;&#35821;&#21040;&#33521;&#35821;&#30340;&#32763;&#35793;&#21644;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#65292;&#35777;&#26126;&#32599;&#39532;&#21270;&#25991;&#26412;&#19981;&#20165;&#30001;&#20110;&#20854;&#36739;&#20302;&#30340;&#29983;&#20135;&#21147;&#32780;&#26174;&#33879;&#25913;&#21892;&#20102;&#25512;&#29702;&#25928;&#29575;&#65292;&#36824;&#22312;&#26377;&#38480;&#30340;&#39044;&#35757;&#32451;&#20013;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26032;&#39062;&#30340;&#22810;&#33050;&#26412;&#25552;&#31034;&#26041;&#27861;&#32467;&#21512;&#20102;&#32599;&#39532;&#21270;&#21644;&#21407;&#29983;&#25991;&#26412;&#65292;&#22312;&#36827;&#19968;&#27493;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#32599;&#39532;&#21270;&#22312;&#24357;&#21512;&#22823;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20013;&#30340;&#35821;&#35328;&#38556;&#30861;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65292;&#26410;&#26469;&#30340;&#24037;&#20316;&#23558;&#33268;&#21147;&#20110;&#23558;&#27492;&#26041;&#27861;&#25193;&#23637;&#21040;&#26356;&#22810;&#30340;&#35821;&#35328;&#21644;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study addresses the challenge of extending Large Language Models (LLMs) to non-English languages, specifically those using non-Latin scripts. We propose an innovative approach that utilizes the romanized form of text as an interface for LLMs, hypothesizing that its frequent informal use and shared tokens with English enhance cross-lingual alignment. Focusing on Hindi, we demonstrate through Hindi-to-English translation and sentiment analysis tasks that romanized text not only significantly improves inference efficiency due to its lower fertility compared to native text but also achieves competitive performance with limited pre-training. Additionally, our novel multi-script prompting approach, which combines romanized and native texts, shows promise in further enhancing task performance. These findings suggest the potential of romanization in bridging the language gap for LLM applications, with future work aimed at expanding this approach to more languages and tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32858;&#21512;&#31574;&#30053;&#65292;&#29992;&#20110;&#35780;&#20272;&#32858;&#31867;&#36136;&#37327;&#12290;&#36890;&#36807;&#23545;&#32858;&#31867;&#32423;&#21035;&#30340;&#36718;&#24275;&#24471;&#20998;&#36827;&#34892;&#24179;&#22343;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#23545;&#25152;&#26377;&#32858;&#31867;&#30340;&#24471;&#20998;&#36827;&#34892;&#23439;&#35266;&#24179;&#22343;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#23439;&#35266;&#24179;&#22343;&#36718;&#24275;&#24471;&#20998;&#23545;&#20110;&#32858;&#31867;&#19981;&#22343;&#34913;&#21644;&#32972;&#26223;&#22122;&#22768;&#26159;&#31283;&#20581;&#30340;&#12290;</title><link>http://arxiv.org/abs/2401.05831</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#36718;&#24275;&#31995;&#25968;&#65306;&#20174;&#24494;&#35266;&#21040;&#23439;&#35266;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Revisiting Silhouette: From Micro to Macro Aggregation. (arXiv:2401.05831v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32858;&#21512;&#31574;&#30053;&#65292;&#29992;&#20110;&#35780;&#20272;&#32858;&#31867;&#36136;&#37327;&#12290;&#36890;&#36807;&#23545;&#32858;&#31867;&#32423;&#21035;&#30340;&#36718;&#24275;&#24471;&#20998;&#36827;&#34892;&#24179;&#22343;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#23545;&#25152;&#26377;&#32858;&#31867;&#30340;&#24471;&#20998;&#36827;&#34892;&#23439;&#35266;&#24179;&#22343;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#23439;&#35266;&#24179;&#22343;&#36718;&#24275;&#24471;&#20998;&#23545;&#20110;&#32858;&#31867;&#19981;&#22343;&#34913;&#21644;&#32972;&#26223;&#22122;&#22768;&#26159;&#31283;&#20581;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36718;&#24275;&#31995;&#25968;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#20869;&#37096;&#32858;&#31867;&#35780;&#20272;&#25351;&#26631;&#65292;&#23427;&#20250;&#20026;&#27599;&#20010;&#25968;&#25454;&#28857;&#20135;&#29983;&#19968;&#20010;&#24471;&#20998;&#65292;&#29992;&#20110;&#35780;&#20272;&#20854;&#32858;&#31867;&#20998;&#37197;&#30340;&#36136;&#37327;&#12290;&#30446;&#21069;&#65292;&#20026;&#20102;&#35780;&#20272;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#32858;&#31867;&#36136;&#37327;&#65292;&#36890;&#24120;&#20250;&#23558;&#25968;&#25454;&#38598;&#20013;&#25152;&#26377;&#28857;&#30340;&#24471;&#20998;&#24179;&#22343;&#25104;&#19968;&#20010;&#21333;&#19968;&#20540;&#65292;&#36825;&#20010;&#31574;&#30053;&#34987;&#31216;&#20026;&#24494;&#35266;&#24179;&#22343;&#12290;&#28982;&#32780;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#21512;&#25104;&#20363;&#23376;&#23637;&#31034;&#20102;&#65292;&#35813;&#24494;&#35266;&#24179;&#22343;&#31574;&#30053;&#23545;&#20110;&#32858;&#31867;&#19981;&#22343;&#34913;&#21644;&#24322;&#24120;&#20540;&#65288;&#32972;&#26223;&#22122;&#22768;&#65289;&#38750;&#24120;&#25935;&#24863;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#32858;&#21512;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#39318;&#20808;&#23545;&#32858;&#31867;&#32423;&#21035;&#30340;&#36718;&#24275;&#24471;&#20998;&#36827;&#34892;&#24179;&#22343;&#65292;&#28982;&#21518;&#20877;&#23545;&#25152;&#26377;&#32858;&#31867;&#30340;&#24471;&#20998;&#36827;&#34892;&#23439;&#35266;&#24179;&#22343;&#12290;&#22522;&#20110;&#30456;&#21516;&#30340;&#21512;&#25104;&#20363;&#23376;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25552;&#20986;&#30340;&#23439;&#35266;&#24179;&#22343;&#36718;&#24275;&#24471;&#20998;&#23545;&#20110;&#32858;&#31867;&#19981;&#22343;&#34913;&#21644;&#32972;&#26223;&#22122;&#22768;&#26159;&#31283;&#20581;&#30340;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#30740;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#23439;&#35266;&#24179;&#22343;&#21464;&#20307;&#21487;&#20197;&#26356;&#22909;&#22320;&#20272;&#35745;&#30495;&#23454;&#30340;&#32858;&#31867;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Silhouette coefficient is an established internal clustering evaluation measure that produces a score per data point, assessing the quality of its clustering assignment. To assess the quality of the clustering of the whole dataset, the scores of all the points in the dataset are typically averaged into a single value, a strategy which we call as micro-averaging. As we illustrate in this work, by using a synthetic example, this micro-averaging strategy is sensitive both to cluster imbalance and outliers (background noise). To address these issues, we propose an alternative aggregation strategy, which first averages the silhouette scores at a cluster level and then (macro) averages the scores across the clusters. Based on the same synthetic example, we show that the proposed macro-averaged silhouette score is robust to cluster imbalance and background noise. We have conducted an experimental study showing that our macro-averaged variant provides better estimates of the ground truth numbe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Intel GPU&#19978;&#39640;&#25928;&#30340;LLM&#25512;&#29702;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#31616;&#21270;LLM&#35299;&#30721;&#23618;&#21644;&#24341;&#20837;&#20998;&#27573;KV&#32531;&#23384;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#20302;&#24310;&#36831;&#21644;&#39640;&#21534;&#21520;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.05391</link><description>&lt;p&gt;
&#22312;Intel GPU&#19978;&#39640;&#25928;&#30340;LLM&#25512;&#29702;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Efficient LLM inference solution on Intel GPU. (arXiv:2401.05391v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Intel GPU&#19978;&#39640;&#25928;&#30340;LLM&#25512;&#29702;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#31616;&#21270;LLM&#35299;&#30721;&#23618;&#21644;&#24341;&#20837;&#20998;&#27573;KV&#32531;&#23384;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#20302;&#24310;&#36831;&#21644;&#39640;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#35768;&#22810;&#39046;&#22495;&#24191;&#27867;&#24212;&#29992;&#65292;LLM&#25512;&#29702;&#30340;&#25928;&#29575;&#25104;&#20026;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28909;&#38376;&#35805;&#39064;&#12290;&#28982;&#32780;&#65292;LLM&#36890;&#24120;&#22312;&#27169;&#22411;&#32467;&#26500;&#19978;&#35774;&#35745;&#22797;&#26434;&#65292;&#20855;&#26377;&#22823;&#37327;&#25805;&#20316;&#65292;&#24182;&#20197;&#33258;&#22238;&#24402;&#27169;&#24335;&#36827;&#34892;&#25512;&#29702;&#65292;&#36825;&#20351;&#24471;&#35774;&#35745;&#19968;&#20010;&#39640;&#25928;&#30340;&#31995;&#32479;&#25104;&#20026;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;LLM&#25512;&#29702;&#35299;&#20915;&#26041;&#26696;&#65292;&#20855;&#26377;&#20302;&#24310;&#36831;&#21644;&#39640;&#21534;&#21520;&#37327;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#34701;&#21512;&#25968;&#25454;&#31227;&#21160;&#21644;&#36880;&#20803;&#32032;&#25805;&#20316;&#31616;&#21270;&#20102;LLM&#35299;&#30721;&#23618;&#65292;&#20197;&#20943;&#23569;&#20869;&#23384;&#35775;&#38382;&#39057;&#29575;&#24182;&#38477;&#20302;&#31995;&#32479;&#24310;&#36831;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27573;KV&#32531;&#23384;&#31574;&#30053;&#65292;&#23558;&#35831;&#27714;&#21644;&#21709;&#24212;&#20196;&#29260;&#30340;&#38190;/&#20540;&#20998;&#21035;&#20445;&#23384;&#22312;&#19981;&#21516;&#30340;&#29289;&#29702;&#20869;&#23384;&#20013;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#35774;&#22791;&#20869;&#23384;&#31649;&#29702;&#65292;&#26377;&#21161;&#20110;&#22686;&#22823;&#36816;&#34892;&#26102;&#25209;&#22788;&#29702;&#22823;&#23567;&#24182;&#25552;&#39640;&#31995;&#32479;&#21534;&#21520;&#37327;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;Scaled-Dot-Product-Attention&#20869;&#26680;&#65292;&#20197;&#21305;&#37197;&#25105;&#20204;&#30340;&#34701;&#21512;&#31574;&#30053;&#65292;&#22522;&#20110;&#20998;&#27573;KV&#32531;&#23384;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#35299;&#20915;&#26041;&#26696;&#22312;Intel GPU&#19978;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;LLM&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer based Large Language Models (LLMs) have been widely used in many fields, and the efficiency of LLM inference becomes hot topic in real applications. However, LLMs are usually complicatedly designed in model structure with massive operations and perform inference in the auto-regressive mode, making it a challenging task to design a system with high efficiency.  In this paper, we propose an efficient LLM inference solution with low latency and high throughput. Firstly, we simplify the LLM decoder layer by fusing data movement and element-wise operations to reduce the memory access frequency and lower system latency. We also propose a segment KV cache policy to keep key/value of the request and response tokens in separate physical memory for effective device memory management, helping enlarge the runtime batch size and improve system throughput. A customized Scaled-Dot-Product-Attention kernel is designed to match our fusion policy based on the segment KV cache solution. We im
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25512;&#29702;&#27493;&#38271;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#22312;&#25552;&#31034;&#20013;&#22686;&#21152;&#25512;&#29702;&#27493;&#39588;&#33021;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#32780;&#20943;&#23569;&#25512;&#29702;&#27493;&#39588;&#21017;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.04925</link><description>&lt;p&gt;
&#25512;&#29702;&#27493;&#38271;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Impact of Reasoning Step Length on Large Language Models. (arXiv:2401.04925v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25512;&#29702;&#27493;&#38271;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#22312;&#25552;&#31034;&#20013;&#22686;&#21152;&#25512;&#29702;&#27493;&#39588;&#33021;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#32780;&#20943;&#23569;&#25512;&#29702;&#27493;&#39588;&#21017;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24605;&#32500;&#38142;&#26465;&#65288;CoT&#65289;&#23545;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;CoT&#30340;&#26377;&#25928;&#24615;&#19982;&#25552;&#31034;&#20013;&#25512;&#29702;&#27493;&#39588;&#30340;&#38271;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#20173;&#28982;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#20026;&#20102;&#25581;&#31034;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20960;&#20010;&#23454;&#35777;&#23454;&#39564;&#26469;&#25506;&#32034;&#36825;&#20123;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20123;&#23454;&#39564;&#65292;&#25193;&#23637;&#21644;&#21387;&#32553;CoT&#28436;&#31034;&#20013;&#30340;&#21512;&#29702;&#25512;&#29702;&#27493;&#39588;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#20182;&#22240;&#32032;&#19981;&#21464;&#12290;&#25105;&#20204;&#24471;&#20986;&#20102;&#20197;&#19979;&#20027;&#35201;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25552;&#31034;&#20013;&#24310;&#38271;&#25512;&#29702;&#27493;&#39588;&#65292;&#21363;&#20351;&#27809;&#26377;&#21521;&#25552;&#31034;&#20013;&#28155;&#21152;&#26032;&#20449;&#24687;&#65292;&#20063;&#20250;&#26174;&#33879;&#25552;&#39640;LLM&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#30456;&#21453;&#65292;&#32553;&#30701;&#25512;&#29702;&#27493;&#39588;&#65292;&#21363;&#20351;&#20445;&#30041;&#20851;&#38190;&#20449;&#24687;&#65292;&#20063;&#20250;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#19968;&#21457;&#29616;&#31361;&#26174;&#20102;CoT&#25552;&#31034;&#20013;&#27493;&#39588;&#25968;&#37327;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#38469;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain of Thought (CoT) is significant in improving the reasoning abilities of large language models (LLMs). However, the correlation between the effectiveness of CoT and the length of reasoning steps in prompts remains largely unknown. To shed light on this, we have conducted several empirical experiments to explore the relations. Specifically, we design experiments that expand and compress the rationale reasoning steps within CoT demonstrations, while keeping all other factors constant. We have the following key findings. First, the results indicate that lengthening the reasoning steps in prompts, even without adding new information into the prompt, considerably enhances LLMs' reasoning abilities across multiple datasets. Alternatively, shortening the reasoning steps, even while preserving the key information, significantly diminishes the reasoning abilities of models. This finding highlights the importance of the number of steps in CoT prompts and provides practical guidance to make 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Graph2Tac&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#23450;&#29702;&#35777;&#26126;&#20013;&#23398;&#20064;&#25968;&#23398;&#27010;&#24565;&#30340;&#20998;&#23618;&#34920;&#31034;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#21160;&#24577;&#22320;&#23558;&#26032;&#30340;&#25968;&#23398;&#27010;&#24565;&#32435;&#20837;&#21040;&#30693;&#35782;&#24211;&#20013;&#65292;&#24182;&#22312;Coq&#35777;&#26126;&#21161;&#25163;&#20013;&#36827;&#34892;&#35757;&#32451;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.02949</link><description>&lt;p&gt;
Graph2Tac: &#22312;&#23450;&#29702;&#35777;&#26126;&#20013;&#23398;&#20064;&#25968;&#23398;&#27010;&#24565;&#30340;&#20998;&#23618;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Graph2Tac: Learning Hierarchical Representations of Math Concepts in Theorem proving. (arXiv:2401.02949v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Graph2Tac&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#23450;&#29702;&#35777;&#26126;&#20013;&#23398;&#20064;&#25968;&#23398;&#27010;&#24565;&#30340;&#20998;&#23618;&#34920;&#31034;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#21160;&#24577;&#22320;&#23558;&#26032;&#30340;&#25968;&#23398;&#27010;&#24565;&#32435;&#20837;&#21040;&#30693;&#35782;&#24211;&#20013;&#65292;&#24182;&#22312;Coq&#35777;&#26126;&#21161;&#25163;&#20013;&#36827;&#34892;&#35757;&#32451;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#21450;&#20854;&#24212;&#29992;&#20013;&#23384;&#22312;&#22823;&#37327;&#30340;&#27010;&#24565;&#12290;&#23427;&#20204;&#22312;&#19981;&#21516;&#30340;&#23398;&#31185;&#39046;&#22495;&#20013;&#26377;&#24456;&#22823;&#30340;&#21464;&#21270;&#65292;&#24182;&#19988;&#27599;&#31687;&#25968;&#23398;&#35770;&#25991;&#25110;&#24212;&#29992;&#20013;&#37117;&#20250;&#24341;&#20837;&#26032;&#30340;&#27010;&#24565;&#12290;&#24418;&#24335;&#21270;&#29702;&#35770;&#24314;&#31435;&#20102;&#19968;&#20010;&#23618;&#27425;&#32467;&#26500;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#23450;&#20041;&#12289;&#23450;&#29702;&#21644;&#30456;&#20114;&#24341;&#29992;&#30340;&#35777;&#26126;&#12290;&#24403;&#19968;&#20010;AI&#20195;&#29702;&#20154;&#35777;&#26126;&#19968;&#20010;&#26032;&#30340;&#23450;&#29702;&#26102;&#65292;&#22823;&#22810;&#25968;&#19982;&#35813;&#23450;&#29702;&#30456;&#20851;&#30340;&#25968;&#23398;&#27010;&#24565;&#21644;&#24341;&#29702;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#33021;&#20174;&#26410;&#34987;&#35265;&#36807;&#12290;&#36825;&#22312;Coq&#35777;&#26126;&#21161;&#25163;&#20013;&#23588;&#20026;&#26126;&#26174;&#65292;&#35813;&#21161;&#25163;&#25317;&#26377;&#21508;&#31181;&#21508;&#26679;&#30340;Coq&#39033;&#30446;&#65292;&#27599;&#20010;&#39033;&#30446;&#37117;&#26377;&#33258;&#24049;&#30340;&#23450;&#20041;&#12289;&#24341;&#29702;&#65292;&#29978;&#33267;&#29992;&#20110;&#35777;&#26126;&#36825;&#20123;&#24341;&#29702;&#30340;&#33258;&#23450;&#20041;&#31574;&#30053;&#36807;&#31243;&#12290;&#23558;&#36825;&#26679;&#30340;&#26032;&#20449;&#24687;&#21363;&#26102;&#22320;&#34701;&#20837;&#21040;&#20195;&#29702;&#20154;&#30340;&#30693;&#35782;&#24211;&#20013;&#23545;&#20110;&#20195;&#29702;&#20154;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#19968;&#20010;&#26032;&#30340;&#12289;&#22823;&#35268;&#27169;&#30340;&#12289;&#22522;&#20110;&#22270;&#30340;&#25968;&#25454;&#38598;&#65292;&#22312;Coq&#20013;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#12290;&#25105;&#20204;&#21033;&#29992;Coq&#26415;&#35821;&#30340;&#24544;&#23454;&#22270;&#34920;&#31034;&#65292;&#21019;&#24314;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;Graph2Tac&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#23450;&#20041;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#21019;&#24314;&#20102;&#19968;&#20010;&#26377;&#21521;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concepts abound in mathematics and its applications. They vary greatly between subject areas, and new ones are introduced in each mathematical paper or application. A formal theory builds a hierarchy of definitions, theorems and proofs that reference each other. When an AI agent is proving a new theorem, most of the mathematical concepts and lemmas relevant to that theorem may have never been seen during training. This is especially true in the Coq proof assistant, which has a diverse library of Coq projects, each with its own definitions, lemmas, and even custom tactic procedures used to prove those lemmas. It is essential for agents to incorporate such new information into their knowledge base on the fly. We work towards this goal by utilizing a new, large-scale, graph-based dataset for machine learning in Coq. We leverage a faithful graph-representation of Coq terms that induces a directed graph of dependencies between definitions to create a novel graph neural network, Graph2Tac (G
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#30456;&#26426;&#38519;&#38449;&#22270;&#20687;&#30340;&#32467;&#26500;&#21270;&#19978;&#19979;&#25991;&#65292;&#25552;&#39640;&#20854;&#22312;&#29289;&#31181;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#21644;&#27867;&#21270;&#33021;&#21147;&#22686;&#24378;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.00608</link><description>&lt;p&gt;
&#23558;&#19978;&#19979;&#25991;&#24102;&#22238;&#26469;&#65306;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#30456;&#26426;&#38519;&#38449;&#29289;&#31181;&#35782;&#21035;&#20316;&#20026;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Bringing Back the Context: Camera Trap Species Identification as Link Prediction on Multimodal Knowledge Graphs. (arXiv:2401.00608v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#30456;&#26426;&#38519;&#38449;&#22270;&#20687;&#30340;&#32467;&#26500;&#21270;&#19978;&#19979;&#25991;&#65292;&#25552;&#39640;&#20854;&#22312;&#29289;&#31181;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#21644;&#27867;&#21270;&#33021;&#21147;&#22686;&#24378;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#26426;&#38519;&#38449;&#22312;&#21160;&#29289;&#29983;&#24577;&#23398;&#20013;&#26159;&#23453;&#36149;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#29983;&#29289;&#22810;&#26679;&#24615;&#30417;&#27979;&#21644;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#22914;&#22312;&#26032;&#30340;&#26410;&#30693;&#20301;&#32622;&#37096;&#32626;&#26102;&#30340;&#31967;&#31957;&#27867;&#21270;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#22270;&#20687;&#33258;&#28982;&#19982;&#21487;&#33021;&#22312;&#19981;&#21516;&#27169;&#24577;&#19979;&#30340;&#24322;&#36136;&#19978;&#19979;&#25991;&#30456;&#20851;&#32852;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#19982;&#30456;&#26426;&#38519;&#38449;&#22270;&#20687;&#30456;&#20851;&#32852;&#30340;&#32467;&#26500;&#21270;&#19978;&#19979;&#25991;&#65292;&#25913;&#21892;&#22312;&#30456;&#26426;&#38519;&#38449;&#20013;&#29289;&#31181;&#35782;&#21035;&#36825;&#20010;&#20219;&#21153;&#30340;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;&#19968;&#24352;&#37326;&#29983;&#21160;&#29289;&#30340;&#29031;&#29255;&#21487;&#33021;&#19982;&#25293;&#25668;&#22320;&#28857;&#21644;&#26102;&#38388;&#20197;&#21450;&#20851;&#20110;&#21160;&#29289;&#29289;&#31181;&#30340;&#32467;&#26500;&#21270;&#29983;&#29289;&#23398;&#30693;&#35782;&#30456;&#20851;&#32852;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#24573;&#35270;&#36825;&#19968;&#28857;&#65292;&#20294;&#23558;&#36825;&#26679;&#30340;&#19978;&#19979;&#25991;&#24102;&#22238;&#26469;&#21487;&#20197;&#24102;&#26469;&#19968;&#20123;&#28508;&#22312;&#30340;&#22909;&#22788;&#65292;&#22914;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#21644;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#22320;&#23558;&#36825;&#26679;&#30340;&#24322;&#36136;&#19978;&#19979;&#25991;&#25972;&#21512;&#21040;&#35270;&#35273;&#39046;&#22495;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Camera traps are valuable tools in animal ecology for biodiversity monitoring and conservation. However, challenges like poor generalization to deployment at new unseen locations limit their practical application. Images are naturally associated with heterogeneous forms of context possibly in different modalities. In this work, we leverage the structured context associated with the camera trap images to improve out-of-distribution generalization for the task of species identification in camera traps. For example, a photo of a wild animal may be associated with information about where and when it was taken, as well as structured biology knowledge about the animal species. While typically overlooked by existing work, bringing back such context offers several potential benefits for better image understanding, such as addressing data scarcity and enhancing generalization. However, effectively integrating such heterogeneous context into the visual domain is a challenging problem. To address
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22330;&#26223;&#22270;&#30693;&#35782;&#25512;&#36827;&#20102;&#25163;&#26415;&#29615;&#22659;&#20013;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#65292;&#35299;&#20915;&#20102;&#25163;&#26415;VQA&#31995;&#32479;&#20013;&#30340;&#38382;&#39064;&#26465;&#20214;&#20559;&#20506;&#21644;&#32570;&#20047;&#22330;&#26223;&#24863;&#30693;&#25512;&#29702;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2312.10251</link><description>&lt;p&gt;
&#36890;&#36807;&#22330;&#26223;&#22270;&#30693;&#35782;&#25512;&#36827;&#22806;&#31185;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Advancing Surgical VQA with Scene Graph Knowledge. (arXiv:2312.10251v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22330;&#26223;&#22270;&#30693;&#35782;&#25512;&#36827;&#20102;&#25163;&#26415;&#29615;&#22659;&#20013;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#65292;&#35299;&#20915;&#20102;&#25163;&#26415;VQA&#31995;&#32479;&#20013;&#30340;&#38382;&#39064;&#26465;&#20214;&#20559;&#20506;&#21644;&#32570;&#20047;&#22330;&#26223;&#24863;&#30693;&#25512;&#29702;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25163;&#26415;&#23460;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#38656;&#35201;&#21019;&#26032;&#30340;&#26415;&#20013;&#25903;&#25345;&#31995;&#32479;&#12290;&#23613;&#31649;&#22806;&#31185;&#25968;&#25454;&#31185;&#23398;&#30340;&#37325;&#28857;&#20027;&#35201;&#22312;&#20110;&#35270;&#39057;&#20998;&#26512;&#65292;&#20294;&#23558;&#22806;&#31185;&#35745;&#31639;&#26426;&#35270;&#35273;&#19982;&#35821;&#35328;&#33021;&#21147;&#30456;&#32467;&#21512;&#25104;&#20026;&#24517;&#35201;&#30340;&#36235;&#21183;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#22330;&#26223;&#22270;&#30693;&#35782;&#25512;&#36827;&#25163;&#26415;&#29615;&#22659;&#20013;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#65292;&#35299;&#20915;&#24403;&#21069;&#25163;&#26415;VQA&#31995;&#32479;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#28040;&#38500;&#25163;&#26415;VQA&#25968;&#25454;&#38598;&#20013;&#30340;&#38382;&#39064;&#26465;&#20214;&#20559;&#20506;&#65292;&#20197;&#21450;&#22312;&#25163;&#26415;VQA&#27169;&#22411;&#35774;&#35745;&#20013;&#34701;&#20837;&#22330;&#26223;&#24863;&#30693;&#25512;&#29702;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25163;&#26415;&#22330;&#26223;&#22270;&#30340;&#25968;&#25454;&#38598;SSG-QA&#65292;&#36890;&#36807;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#20998;&#21106;&#21644;&#26816;&#27979;&#27169;&#22411;&#26469;&#29983;&#25104;&#12290;&#25105;&#20204;&#20351;&#29992;&#20202;&#22120;&#21644;&#35299;&#21078;&#32467;&#26500;&#30340;&#31354;&#38388;&#21644;&#21160;&#20316;&#20449;&#24687;&#26500;&#24314;&#25163;&#26415;&#22330;&#26223;&#22270;&#12290;&#36825;&#20123;&#22270;&#34987;&#36755;&#20837;&#21040;&#19968;&#20010;&#38382;&#39064;&#24341;&#25806;&#20013;&#65292;&#20135;&#29983;&#22810;&#26679;&#21270;&#30340;&#38382;&#31572;&#23545;&#12290;&#25105;&#20204;&#30340;SSG-QA&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19968;&#20010;&#26356;&#22797;&#26434;&#12289;&#22810;&#26679;&#21270;&#12289;&#20960;&#20309;&#22522;&#30784;&#12289;&#26080;&#20559;&#20506;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern operating room is becoming increasingly complex, requiring innovative intra-operative support systems. While the focus of surgical data science has largely been on video analysis, integrating surgical computer vision with language capabilities is emerging as a necessity. Our work aims to advance Visual Question Answering (VQA) in the surgical context with scene graph knowledge, addressing two main challenges in the current surgical VQA systems: removing question-condition bias in the surgical VQA dataset and incorporating scene-aware reasoning in the surgical VQA model design. First, we propose a Surgical Scene Graph-based dataset, SSG-QA, generated by employing segmentation and detection models on publicly available datasets. We build surgical scene graphs using spatial and action information of instruments and anatomies. These graphs are fed into a question engine, generating diverse QA pairs. Our SSG-QA dataset provides a more complex, diverse, geometrically grounded, unbiase
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36845;&#20195;&#31034;&#33539;&#36873;&#25321;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38646;&#26679;&#26412;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#26469;&#36873;&#25321;&#19982;&#27979;&#35797;&#26679;&#26412;&#19981;&#21516;&#20294;&#20173;&#19982;&#20043;&#24378;&#30456;&#20851;&#30340;&#31034;&#33539;&#20316;&#20026;&#23398;&#20064;&#30340;&#19978;&#19979;&#25991;&#12290;</title><link>http://arxiv.org/abs/2310.09881</link><description>&lt;p&gt;
&#22522;&#20110;&#36845;&#20195;&#31034;&#33539;&#36873;&#25321;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning with Iterative Demonstration Selection. (arXiv:2310.09881v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09881
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36845;&#20195;&#31034;&#33539;&#36873;&#25321;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38646;&#26679;&#26412;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#26469;&#36873;&#25321;&#19982;&#27979;&#35797;&#26679;&#26412;&#19981;&#21516;&#20294;&#20173;&#19982;&#20043;&#24378;&#30456;&#20851;&#30340;&#31034;&#33539;&#20316;&#20026;&#23398;&#20064;&#30340;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#35268;&#27169;&#30340;&#36827;&#23637;&#30340;&#25512;&#21160;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;ICL&#30340;&#24615;&#33021;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#31034;&#33539;&#30340;&#36873;&#25321;&#38750;&#24120;&#25935;&#24863;&#12290;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#31034;&#33539;&#20316;&#20026;&#19978;&#19979;&#25991;&#20173;&#28982;&#26159;&#19968;&#20010;&#25345;&#32493;&#25361;&#25112;&#21644;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#25991;&#29486;&#24050;&#32463;&#24378;&#35843;&#20102;&#36873;&#25321;&#37027;&#20123;&#19982;&#27979;&#35797;&#26679;&#26412;&#19981;&#21516;&#25110;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#31034;&#33539;&#30340;&#37325;&#35201;&#24615;&#65292;&#32780;&#24573;&#35270;&#20102;&#26368;&#20248;&#31034;&#33539;&#36873;&#25321;&#32500;&#24230;&#26159;&#20219;&#21153;&#29305;&#23450;&#30340;&#20107;&#23454;&#12290;&#20511;&#37492;&#20004;&#20010;&#32500;&#24230;&#30340;&#20248;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36845;&#20195;&#31034;&#33539;&#36873;&#25321;(IDS)&#12290;&#20351;&#29992;&#38646;&#26679;&#26412;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;(Zero-shot-CoT)&#65292;IDS&#36845;&#20195;&#22320;&#36873;&#25321;&#37027;&#20123;&#19982;&#27979;&#35797;&#26679;&#26412;&#19981;&#21516;&#20294;&#20173;&#19982;&#20043;&#24378;&#30456;&#20851;&#30340;&#31034;&#33539;&#20316;&#20026;ICL&#30340;&#31034;&#33539;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;IDS&#22312;&#31034;&#33539;&#36873;&#25321;&#20043;&#21069;&#23558;Zero-shot-CoT&#24212;&#29992;&#20110;&#27979;&#35797;&#26679;&#26412;&#12290;&#36755;&#20986;&#30340;&#25512;&#29702;&#36335;&#24452;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Spurred by advancements in scale, large language models (LLMs) have demonstrated strong few-shot learning ability via in-context learning (ICL). However, the performance of ICL has been shown to be highly sensitive to the selection of few-shot demonstrations. Selecting the most suitable examples as context remains an ongoing challenge and an open problem. Existing literature has highlighted the importance of selecting examples that are diverse or semantically similar to the test sample while ignoring the fact that the optimal selection dimension, i.e., diversity or similarity, is task-specific. Leveraging the merits of both dimensions, we propose Iterative Demonstration Selection (IDS). Using zero-shot chain-of-thought reasoning (Zero-shot-CoT), IDS iteratively selects examples that are diverse but still strongly correlated with the test sample as ICL demonstrations. Specifically, IDS applies Zero-shot-CoT to the test sample before demonstration selection. The output reasoning path is 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#26088;&#22312;&#21516;&#26102;&#27169;&#25311;&#35821;&#20041;&#21644;&#21327;&#21516;&#30693;&#35782;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;CTR&#20272;&#35745;&#65292;&#24182;&#35299;&#20915;&#25512;&#29702;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.09234</link><description>&lt;p&gt;
ClickPrompt: CTR&#27169;&#22411;&#26159;&#23558;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#20026;CTR&#39044;&#27979;&#30340;&#24378;&#22823;&#25552;&#31034;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
ClickPrompt: CTR Models are Strong Prompt Generators for Adapting Language Models to CTR Prediction. (arXiv:2310.09234v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09234
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#26088;&#22312;&#21516;&#26102;&#27169;&#25311;&#35821;&#20041;&#21644;&#21327;&#21516;&#30693;&#35782;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;CTR&#20272;&#35745;&#65292;&#24182;&#35299;&#20915;&#25512;&#29702;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#20114;&#32852;&#32593;&#24212;&#29992;&#31243;&#24207;&#20013;&#36234;&#26469;&#36234;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#20256;&#32479;&#30340;CTR&#27169;&#22411;&#36890;&#36807;&#29420;&#28909;&#32534;&#30721;&#23558;&#22810;&#23383;&#27573;&#20998;&#31867;&#25968;&#25454;&#36716;&#25442;&#20026;ID&#29305;&#24449;&#65292;&#24182;&#25552;&#21462;&#29305;&#24449;&#20043;&#38388;&#30340;&#21327;&#21516;&#20449;&#21495;&#12290;&#36825;&#31181;&#33539;&#24335;&#30340;&#38382;&#39064;&#22312;&#20110;&#35821;&#20041;&#20449;&#24687;&#30340;&#20002;&#22833;&#12290;&#21478;&#19968;&#26041;&#38754;&#30340;&#30740;&#31350;&#36890;&#36807;&#23558;&#36755;&#20837;&#25968;&#25454;&#36716;&#25442;&#20026;&#25991;&#26412;&#21477;&#23376;&#26469;&#25506;&#32034;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#22312;CTR&#39044;&#27979;&#20013;&#30340;&#28508;&#21147;&#12290;&#34429;&#28982;&#35821;&#20041;&#20449;&#21495;&#24471;&#21040;&#20102;&#20445;&#30041;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#26080;&#27861;&#25429;&#25417;&#21040;&#21327;&#21516;&#20449;&#24687;&#65288;&#22914;&#29305;&#24449;&#20132;&#20114;&#12289;&#32431;ID&#29305;&#24449;&#65289;&#65292;&#26356;&#19981;&#29992;&#35828;&#30001;&#24222;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#24102;&#26469;&#30340;&#26080;&#27861;&#25509;&#21463;&#30340;&#25512;&#29702;&#24320;&#38144;&#20102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#20026;&#20934;&#30830;&#30340;CTR&#20272;&#35745;&#24314;&#31435;&#35821;&#20041;&#30693;&#35782;&#21644;&#21327;&#21516;&#30693;&#35782;&#65292;&#24182;&#35299;&#20915;&#25512;&#29702;&#25928;&#29575;&#38382;&#39064;&#12290;&#20026;&#20102;&#20174;&#20004;&#20010;&#39046;&#22495;&#20013;&#21463;&#30410;&#24182;&#24357;&#21512;&#23427;&#20204;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;-&#12290;
&lt;/p&gt;
&lt;p&gt;
Click-through rate (CTR) prediction has become increasingly indispensable for various Internet applications. Traditional CTR models convert the multi-field categorical data into ID features via one-hot encoding, and extract the collaborative signals among features. Such a paradigm suffers from the problem of semantic information loss. Another line of research explores the potential of pretrained language models (PLMs) for CTR prediction by converting input data into textual sentences through hard prompt templates. Although semantic signals are preserved, they generally fail to capture the collaborative information (e.g., feature interactions, pure ID features), not to mention the unacceptable inference overhead brought by the huge model size. In this paper, we aim to model both the semantic knowledge and collaborative knowledge for accurate CTR estimation, and meanwhile address the inference inefficiency issue. To benefit from both worlds and close their gaps, we propose a novel model-
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#25506;&#27979;&#22120;&#21644;&#36716;&#25442;&#22120;&#20248;&#21270;&#25351;&#20196;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02905</link><description>&lt;p&gt;
&#20351;&#29992;&#24744;&#30340;&#26412;&#33021;&#65306;&#20351;&#29992;&#31070;&#32463;&#25506;&#27979;&#22120;&#19982;&#36716;&#25442;&#22120;&#36827;&#34892;&#25351;&#20196;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Use Your INSTINCT: INSTruction optimization usIng Neural bandits Coupled with Transformers. (arXiv:2310.02905v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02905
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#25506;&#27979;&#22120;&#21644;&#36716;&#25442;&#22120;&#20248;&#21270;&#25351;&#20196;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#32473;&#20104;&#23427;&#20204;&#30340;&#25351;&#20196;&#65292;&#36825;&#20123;&#25351;&#20196;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#36827;&#34892;&#25163;&#21160;&#35843;&#25972;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#20102;&#39640;&#25928;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#31639;&#27861;&#26469;&#33258;&#21160;&#20248;&#21270;&#32473;&#20104;&#40657;&#30418;LLMs&#30340;&#25351;&#20196;&#12290;&#28982;&#32780;&#65292;&#22312;&#20248;&#21270;&#39640;&#24230;&#22797;&#26434;&#65288;&#20363;&#22914;&#39640;&#32500;&#65289;&#30340;&#30446;&#26631;&#20989;&#25968;&#26102;&#65292;&#22914;&#23558;&#25351;&#20196;&#26144;&#23556;&#21040;LLM&#24615;&#33021;&#30340;&#20989;&#25968;&#65292;BO&#36890;&#24120;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;BO&#20351;&#29992;&#30340;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#26377;&#38480;&#65292;&#35813;&#27169;&#22411;&#34987;&#29992;&#20316;BO&#30340;&#20195;&#29702;&#26469;&#24314;&#27169;&#30446;&#26631;&#20989;&#25968;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#24050;&#32463;&#22810;&#27425;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#65292;&#23588;&#20854;&#26159;&#39044;&#35757;&#32451;&#30340;&#36716;&#25442;&#22120;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#21487;&#20197;&#24314;&#27169;&#39640;&#24230;&#22797;&#26434;&#30340;&#20989;&#25968;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#31070;&#32463;&#25506;&#27979;&#22120;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown remarkable instruction-following capabilities and achieved impressive performances in various applications. However, the performances of LLMs depend heavily on the instructions given to them, which are typically manually tuned with substantial human efforts. Recent work has used the query-efficient Bayesian optimization (BO) algorithm to automatically optimize the instructions given to black-box LLMs. However, BO usually falls short when optimizing highly sophisticated (e.g., high-dimensional) objective functions, such as the functions mapping an instruction to the performance of an LLM. This is mainly due to the limited expressive power of the Gaussian process (GP) model which is used by BO as a surrogate to model the objective function. Meanwhile, it has been repeatedly shown that neural networks (NNs), especially pre-trained transformers, possess strong expressive power and can model highly complex functions. So, we adopt a neural bandit algor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#21518;&#39564;&#30340;VAE&#26041;&#27861;&#65292;&#37325;&#26032;&#23450;&#20041;&#20102;ELBO&#65292;&#24341;&#20837;&#27491;&#21017;&#21270;&#39033;&#21644;PatchGAN&#37492;&#21035;&#22120;&#65292;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#30340;&#20154;&#33080;&#12290;</title><link>http://arxiv.org/abs/2309.13160</link><description>&lt;p&gt;
GAMIX-VAE: &#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#21518;&#39564;&#30340;VAE
&lt;/p&gt;
&lt;p&gt;
GAMIX-VAE: A VAE with Gaussian Mixture Based Posterior. (arXiv:2309.13160v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#21518;&#39564;&#30340;VAE&#26041;&#27861;&#65292;&#37325;&#26032;&#23450;&#20041;&#20102;ELBO&#65292;&#24341;&#20837;&#27491;&#21017;&#21270;&#39033;&#21644;PatchGAN&#37492;&#21035;&#22120;&#65292;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#30340;&#20154;&#33080;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#29983;&#25104;&#24314;&#27169;&#21644;&#34920;&#31034;&#23398;&#20064;&#30340;&#22522;&#30707;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;VAEs&#30340;&#19968;&#20010;&#32454;&#24494;&#26041;&#38754;&#65292;&#37325;&#28857;&#26159;&#35299;&#37322;KL Divergence&#65292;&#36825;&#26159;Evidence Lower Bound&#65288;ELBO&#65289;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#23427;&#25511;&#21046;&#20102;&#37325;&#26500;&#20934;&#30830;&#24615;&#21644;&#27491;&#21017;&#21270;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#34429;&#28982;KL Divergence&#35753;&#28508;&#21464;&#37327;&#20998;&#24067;&#19982;&#20808;&#39564;&#20998;&#24067;&#23545;&#40784;&#65292;&#32473;&#25972;&#20010;&#28508;&#31354;&#38388;&#21152;&#19978;&#32467;&#26500;&#32422;&#26463;&#65292;&#20294;&#21364;&#19981;&#38480;&#21046;&#21508;&#20010;&#21464;&#37327;&#20998;&#24067;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#37325;&#26032;&#23450;&#20041;&#20102;&#24102;&#26377;&#39640;&#26031;&#28151;&#21512;&#30340;&#21518;&#39564;&#27010;&#29575;&#30340;ELBO&#65292;&#24341;&#20837;&#20102;&#27491;&#21017;&#21270;&#39033;&#20197;&#38450;&#27490;&#26041;&#24046;&#23849;&#28291;&#65292;&#24182;&#20351;&#29992;PatchGAN&#37492;&#21035;&#22120;&#26469;&#22686;&#24378;&#32441;&#29702;&#36924;&#30495;&#24230;&#12290;&#23454;&#29616;&#32454;&#33410;&#28041;&#21450;Encoder&#21644;Decoder&#30340;ResNetV2&#26550;&#26500;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#29983;&#25104;&#36924;&#30495;&#30340;&#20154;&#33080;&#30340;&#33021;&#21147;&#65292;&#20026;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational Autoencoders (VAEs) have become a cornerstone in generative modeling and representation learning within machine learning. This paper explores a nuanced aspect of VAEs, focusing on interpreting the Kullback Leibler (KL) Divergence, a critical component within the Evidence Lower Bound (ELBO) that governs the trade-off between reconstruction accuracy and regularization. While the KL Divergence enforces alignment between latent variable distributions and a prior imposing a structure on the overall latent space but leaves individual variable distributions unconstrained. The proposed method redefines the ELBO with a mixture of Gaussians for the posterior probability, introduces a regularization term to prevent variance collapse, and employs a PatchGAN discriminator to enhance texture realism. Implementation details involve ResNetV2 architectures for both the Encoder and Decoder. The experiments demonstrate the ability to generate realistic faces, offering a promising solution for
&lt;/p&gt;</description></item><item><title>ReConcile&#26159;&#19968;&#20010;&#36890;&#36807;&#22810;&#36718;&#35752;&#35770;&#21644;&#25237;&#31080;&#26426;&#21046;&#26469;&#22686;&#24378;LLM&#25512;&#29702;&#33021;&#21147;&#30340;&#22810;&#27169;&#22411;&#22810;&#20195;&#29702;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2309.13007</link><description>&lt;p&gt;
ReConcile&#65306;&#22278;&#26700;&#20250;&#35758;&#36890;&#36807;&#22810;&#20803;LLM&#30340;&#20849;&#35782;&#25913;&#36827;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs. (arXiv:2309.13007v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13007
&lt;/p&gt;
&lt;p&gt;
ReConcile&#26159;&#19968;&#20010;&#36890;&#36807;&#22810;&#36718;&#35752;&#35770;&#21644;&#25237;&#31080;&#26426;&#21046;&#26469;&#22686;&#24378;LLM&#25512;&#29702;&#33021;&#21147;&#30340;&#22810;&#27169;&#22411;&#22810;&#20195;&#29702;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20173;&#28982;&#22312;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#19978;&#36935;&#21040;&#22256;&#38590;&#12290;&#21463;&#21040;&#24515;&#26234;&#31038;&#20250;&#29702;&#35770;&#65288;Minsky, 1988&#65289;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ReConcile&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#27169;&#22411;&#22810;&#20195;&#29702;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#22810;&#26679;&#30340;LLM&#20195;&#29702;&#20154;&#20043;&#38388;&#30340;&#22278;&#26700;&#20250;&#35758;&#26469;&#20419;&#36827;&#22810;&#26679;&#30340;&#24605;&#24819;&#21644;&#35752;&#35770;&#65292;&#20174;&#32780;&#25913;&#36827;&#19968;&#33268;&#24615;&#12290;ReConcile&#36890;&#36807;&#36827;&#34892;&#22810;&#36718;&#35752;&#35770;&#12289;&#23398;&#20064;&#35828;&#26381;&#20854;&#20182;&#20195;&#29702;&#20154;&#25913;&#36827;&#31572;&#26696;&#20197;&#21450;&#37319;&#29992;&#32622;&#20449;&#24230;&#21152;&#26435;&#25237;&#31080;&#26426;&#21046;&#26469;&#22686;&#24378;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;ReConcile&#36890;&#36807;&#8220;&#35752;&#35770;&#25552;&#31034;&#8221;&#26469;&#21551;&#21160;&#20195;&#29702;&#20154;&#38388;&#30340;&#35752;&#35770;&#65292;&#20854;&#20013;&#21253;&#25324;&#19978;&#19968;&#36718;&#27599;&#20010;&#20195;&#29702;&#20154;&#29983;&#25104;&#30340;&#31572;&#26696;&#21644;&#35299;&#37322;&#30340;&#20998;&#32452;&#12289;&#23427;&#20204;&#30340;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#29992;&#20110;&#35828;&#26381;&#20854;&#20182;&#20195;&#29702;&#20154;&#30340;&#31572;&#26696;&#20462;&#27491;&#20154;&#31867;&#35299;&#37322;&#30340;&#28436;&#31034;&#12290;&#36825;&#20010;&#35752;&#35770;&#25552;&#31034;&#20351;&#27599;&#20010;&#20195;&#29702;&#20154;&#33021;&#22815;&#26681;&#25454;&#20854;&#20182;&#20195;&#29702;&#20154;&#30340;&#35265;&#35299;&#20462;&#35746;&#33258;&#24049;&#30340;&#22238;&#31572;&#12290;&#19968;&#26086;&#36798;&#25104;&#19968;&#33268;&#24182;&#32467;&#26463;&#35752;&#35770;&#65292;ReConcile&#25191;&#34892;&#19968;&#27425;&#20840;&#20307;&#25237;&#31080;&#20197;&#30830;&#23450;&#26368;&#32456;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) still struggle with complex reasoning tasks. Motivated by the society of minds (Minsky, 1988), we propose ReConcile, a multi-model multi-agent framework designed as a round table conference among diverse LLM agents to foster diverse thoughts and discussion for improved consensus. ReConcile enhances the reasoning capabilities of LLMs by holding multiple rounds of discussion, learning to convince other agents to improve their answers, and employing a confidence-weighted voting mechanism. In each round, ReConcile initiates discussion between agents via a 'discussion prompt' that consists of (a) grouped answers and explanations generated by each agent in the previous round, (b) their uncertainties, and (c) demonstrations of answer-rectifying human explanations, used for convincing other agents. This discussion prompt enables each agent to revise their responses in light of insights from other agents. Once a consensus is reached and the discussion ends, ReConcil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#25216;&#26415;&#65292;&#21457;&#29616;&#23569;&#26679;&#26412;&#23398;&#20064;&#21487;&#20197;&#32531;&#35299;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#24182;&#25552;&#21319;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20803;&#23398;&#20064;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2309.11433</link><description>&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Systematic Review of Few-Shot Learning in Medical Imaging. (arXiv:2309.11433v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#25216;&#26415;&#65292;&#21457;&#29616;&#23569;&#26679;&#26412;&#23398;&#20064;&#21487;&#20197;&#32531;&#35299;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#24182;&#25552;&#21319;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20803;&#23398;&#20064;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#20047;&#26631;&#27880;&#30340;&#21307;&#23398;&#24433;&#20687;&#38480;&#21046;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32780;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#35268;&#27169;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#12290;&#23569;&#26679;&#26412;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#20943;&#23569;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#22686;&#24378;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#65292;&#23588;&#20854;&#26159;&#22312;&#20803;&#23398;&#20064;&#26041;&#38754;&#12290;&#26412;&#31995;&#32479;&#32508;&#36848;&#20840;&#38754;&#27010;&#36848;&#20102;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#25628;&#32034;&#20102;&#30456;&#20851;&#25991;&#29486;&#65292;&#24182;&#20174;2018&#24180;&#21040;2023&#24180;&#36873;&#25321;&#20102;80&#31687;&#30456;&#20851;&#25991;&#31456;&#12290;&#25105;&#20204;&#22522;&#20110;&#21307;&#23398;&#32467;&#26524;&#65288;&#22914;&#32959;&#30244;&#20998;&#21106;&#12289;&#30142;&#30149;&#20998;&#31867;&#21644;&#22270;&#20687;&#37197;&#20934;&#65289;&#12289;&#30740;&#31350;&#30340;&#35299;&#21078;&#32467;&#26500;&#65288;&#21363;&#24515;&#33039;&#12289;&#32954;&#31561;&#65289;&#20197;&#21450;&#25152;&#20351;&#29992;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#23545;&#36825;&#20123;&#25991;&#31456;&#36827;&#34892;&#20102;&#32858;&#31867;&#12290;&#23545;&#20110;&#27599;&#20010;&#32858;&#31867;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35770;&#25991;&#30340;&#20998;&#24067;&#20197;&#21450;&#26368;&#20808;&#36827;&#27169;&#22411;&#25552;&#20379;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#25152;&#26377;&#30740;&#31350;&#20013;&#30340;&#20849;&#20139;&#36890;&#29992;&#27969;&#31243;&#12290;&#32508;&#36848;&#34920;&#26126;&#23569;&#26679;&#26412;&#23398;&#20064;&#21487;&#20197;&#22312;&#22823;&#22810;&#25968;&#32467;&#26524;&#20013;&#20811;&#26381;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#19988;&#20803;&#23398;&#20064;&#26159;&#19968;&#20010;&#27969;&#34892;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
The lack of annotated medical images limits the performance of deep learning models, which usually need large-scale labelled datasets. Few-shot learning techniques can reduce data scarcity issues and enhance medical image analysis, especially with meta-learning. This systematic review gives a comprehensive overview of few-shot learning in medical imaging. We searched the literature systematically and selected 80 relevant articles published from 2018 to 2023. We clustered the articles based on medical outcomes, such as tumour segmentation, disease classification, and image registration; anatomical structure investigated (i.e. heart, lung, etc.); and the meta-learning method used. For each cluster, we examined the papers' distributions and the results provided by the state-of-the-art. In addition, we identified a generic pipeline shared among all the studies. The review shows that few-shot learning can overcome data scarcity in most outcomes and that meta-learning is a popular choice to 
&lt;/p&gt;</description></item><item><title>LMDX&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#19982;&#23450;&#20301;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#24067;&#23616;&#32534;&#30721;&#21644;&#31572;&#26696;&#34394;&#26500;&#30340;&#22256;&#38590;&#65292;&#33021;&#22815;&#22312;&#21322;&#32467;&#26500;&#21270;&#25991;&#26723;&#20013;&#25552;&#21462;&#20851;&#38190;&#23454;&#20307;&#12290;</title><link>http://arxiv.org/abs/2309.10952</link><description>&lt;p&gt;
LMDX&#65306;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#19982;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
LMDX: Language Model-based Document Information Extraction and Localization. (arXiv:2309.10952v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10952
&lt;/p&gt;
&lt;p&gt;
LMDX&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#19982;&#23450;&#20301;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#24067;&#23616;&#32534;&#30721;&#21644;&#31572;&#26696;&#34394;&#26500;&#30340;&#22256;&#38590;&#65292;&#33021;&#22815;&#22312;&#21322;&#32467;&#26500;&#21270;&#25991;&#26723;&#20013;&#25552;&#21462;&#20851;&#38190;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#65292;&#25913;&#36827;&#20102;&#35768;&#22810;&#29616;&#26377;&#20219;&#21153;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;&#26032;&#20852;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLM&#23578;&#26410;&#25104;&#21151;&#24212;&#29992;&#20110;&#21322;&#32467;&#26500;&#21270;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#65292;&#36825;&#26159;&#35768;&#22810;&#25991;&#26723;&#22788;&#29702;&#24037;&#20316;&#27969;&#30340;&#26680;&#24515;&#65292;&#21253;&#25324;&#20174;&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#65288;VRD&#65289;&#20013;&#25552;&#21462;&#20851;&#38190;&#23454;&#20307;&#65292;&#32473;&#23450;&#39044;&#23450;&#20041;&#30340;&#30446;&#26631;&#27169;&#24335;&#12290;LLM&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#20027;&#35201;&#38556;&#30861;&#26159;LLM&#20013;&#32570;&#20047;&#24067;&#23616;&#32534;&#30721;&#65292;&#36825;&#23545;&#20110;&#39640;&#36136;&#37327;&#30340;&#25552;&#21462;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#21450;&#32570;&#20047;&#19968;&#20010;&#22522;&#20110;&#29702;&#35770;&#30340;&#26426;&#21046;&#65292;&#30830;&#20445;&#31572;&#26696;&#19981;&#26159;&#34394;&#26500;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#19982;&#23450;&#20301;&#65288;LMDX&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#20219;&#24847;LLM&#36866;&#24212;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#12290;LMDX&#21487;&#20197;&#25552;&#21462;&#21333;&#19968;&#12289;&#37325;&#22797;&#21644;&#23618;&#27425;&#32467;&#26500;&#23454;&#20307;&#65292;&#26080;&#35770;&#26159;&#21542;&#26377;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#22522;&#20110;&#29702;&#35770;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLM) have revolutionized Natural Language Processing (NLP), improving state-of-the-art on many existing tasks and exhibiting emergent capabilities. However, LLMs have not yet been successfully applied on semi-structured document information extraction, which is at the core of many document processing workflows and consists of extracting key entities from a visually rich document (VRD) given a predefined target schema. The main obstacles to LLM adoption in that task have been the absence of layout encoding within LLMs, critical for a high quality extraction, and the lack of a grounding mechanism ensuring the answer is not hallucinated. In this paper, we introduce Language Model-based Document Information Extraction and Localization (LMDX), a methodology to adapt arbitrary LLMs for document information extraction. LMDX can do extraction of singular, repeated, and hierarchical entities, both with and without training data, while providing grounding guarantees and lo
&lt;/p&gt;</description></item><item><title>GPTFUZZER&#26159;&#19968;&#31181;&#40657;&#30418;&#36234;&#29425;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#65292;&#33258;&#21160;&#29983;&#25104;&#29992;&#20110;&#32418;&#38431;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#27169;&#26495;&#12290;&#36825;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#36991;&#20813;&#20102;&#25163;&#24037;&#24037;&#31243;&#65292;&#24182;&#36890;&#36807;&#31181;&#23376;&#36873;&#25321;&#31574;&#30053;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.10253</link><description>&lt;p&gt;
GPTFUZZER : &#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#36234;&#29425;&#25552;&#31034;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32418;&#38431;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
GPTFUZZER : Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts. (arXiv:2309.10253v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10253
&lt;/p&gt;
&lt;p&gt;
GPTFUZZER&#26159;&#19968;&#31181;&#40657;&#30418;&#36234;&#29425;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#65292;&#33258;&#21160;&#29983;&#25104;&#29992;&#20110;&#32418;&#38431;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#27169;&#26495;&#12290;&#36825;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#36991;&#20813;&#20102;&#25163;&#24037;&#24037;&#31243;&#65292;&#24182;&#36890;&#36807;&#31181;&#23376;&#36873;&#25321;&#31574;&#30053;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26368;&#36817;&#38750;&#24120;&#21463;&#27426;&#36814;&#65292;&#24191;&#27867;&#29992;&#20110;&#26085;&#24120;&#23545;&#35805;&#21040;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#32534;&#31243;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;LLMs&#24182;&#19981;&#23436;&#20840;&#21487;&#38752;&#65292;&#21487;&#33021;&#20250;&#25552;&#20379;&#26377;&#20851;&#36827;&#34892;&#26377;&#23475;&#25110;&#38750;&#27861;&#27963;&#21160;&#30340;&#35814;&#32454;&#25351;&#23548;&#12290;&#34429;&#28982;&#23433;&#20840;&#25514;&#26045;&#21487;&#20197;&#20943;&#23569;&#36825;&#20123;&#36755;&#20986;&#30340;&#39118;&#38505;&#65292;&#20294;&#23545;&#25239;&#24615;&#30340;"&#36234;&#29425;"&#25915;&#20987;&#20173;&#28982;&#21487;&#20197;&#21033;&#29992;LLMs&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#36825;&#20123;&#36234;&#29425;&#27169;&#26495;&#36890;&#24120;&#26159;&#25163;&#24037;&#31934;&#24515;&#21046;&#20316;&#30340;&#65292;&#20351;&#22823;&#35268;&#27169;&#27979;&#35797;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#40657;&#30418;&#36234;&#29425;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;\fuzzer&#65292;&#21463;AFL&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#30340;&#21551;&#21457;&#12290;&#19982;&#25163;&#24037;&#24037;&#31243;&#19981;&#21516;&#65292;\fuzzer&#33258;&#21160;&#21270;&#29983;&#25104;&#29992;&#20110;&#32418;&#38431;&#27979;&#35797;LLMs&#30340;&#36234;&#29425;&#27169;&#26495;&#12290;&#22312;&#26680;&#24515;&#37096;&#20998;&#65292;\fuzzer&#20174;&#20154;&#24037;&#32534;&#20889;&#30340;&#27169;&#26495;&#20316;&#20026;&#31181;&#23376;&#24320;&#22987;&#65292;&#28982;&#21518;&#20351;&#29992;&#21464;&#24322;&#25805;&#20316;&#23545;&#20854;&#36827;&#34892;&#21464;&#24322;&#20197;&#29983;&#25104;&#26032;&#30340;&#27169;&#26495;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;\fuzzer&#30340;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65306;&#29992;&#20110;&#24179;&#34913;&#25928;&#29575;&#30340;&#31181;&#23376;&#36873;&#25321;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently experienced tremendous popularity and are widely used from casual conversations to AI-driven programming. However, despite their considerable success, LLMs are not entirely reliable and can give detailed guidance on how to conduct harmful or illegal activities. While safety measures can reduce the risk of such outputs, adversarial "jailbreak" attacks can still exploit LLMs to produce harmful content. These jailbreak templates are typically manually crafted, making large-scale testing challenging. In this paper, we introduce \fuzzer, a novel black-box jailbreak fuzzing framework inspired by AFL fuzzing framework. Instead of manual engineering, \fuzzer automates the generation of jailbreak templates for red-teaming LLMs. At its core, \fuzzer starts with human-written templates as seeds, then mutates them using mutate operators to produce new templates. We detail three key components of \fuzzer: a seed selection strategy for balancing efficiency 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#21160;&#24577;&#22270;&#65288;UniDG&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#34920;&#31034;&#32593;&#32476;&#23398;&#20064;&#22810;&#27169;&#24577;&#34920;&#31034;&#65292;&#24182;&#20445;&#30041;&#27169;&#24577;&#30340;&#29420;&#29305;&#24615;&#12290;&#37319;&#29992;&#21160;&#24577;&#22270;&#32858;&#31867;&#26041;&#27861;&#26500;&#24314;&#21487;&#38752;&#30340;&#20146;&#21644;&#22270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#28176;&#36827;&#24335;&#30340;&#20851;&#32852;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.14105</link><description>&lt;p&gt;
&#38271;&#35270;&#39057;&#20013;&#30340;&#26102;&#38388;&#24615;&#35282;&#33394;&#20998;&#32452;&#30340;&#32479;&#19968;&#21160;&#24577;&#22270;
&lt;/p&gt;
&lt;p&gt;
Unified and Dynamic Graph for Temporal Character Grouping in Long Videos. (arXiv:2308.14105v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#21160;&#24577;&#22270;&#65288;UniDG&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#34920;&#31034;&#32593;&#32476;&#23398;&#20064;&#22810;&#27169;&#24577;&#34920;&#31034;&#65292;&#24182;&#20445;&#30041;&#27169;&#24577;&#30340;&#29420;&#29305;&#24615;&#12290;&#37319;&#29992;&#21160;&#24577;&#22270;&#32858;&#31867;&#26041;&#27861;&#26500;&#24314;&#21487;&#38752;&#30340;&#20146;&#21644;&#22270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#28176;&#36827;&#24335;&#30340;&#20851;&#32852;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#26102;&#38388;&#24615;&#35282;&#33394;&#20998;&#32452;&#26681;&#25454;&#35282;&#33394;&#30340;&#36523;&#20221;&#22312;&#35270;&#39057;&#20013;&#23450;&#20301;&#20986;&#29616;&#30340;&#26102;&#21051;&#12290; &#20026;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#20174;&#26080;&#30417;&#30563;&#32858;&#31867;&#21457;&#23637;&#21040;&#22522;&#20110;&#22270;&#30340;&#26377;&#30417;&#30563;&#32858;&#31867;&#12290; &#28982;&#32780;&#65292;&#22270;&#26041;&#27861;&#24314;&#31435;&#22312;&#22266;&#23450;&#30340;&#20146;&#21644;&#22270;&#21069;&#25552;&#19979;&#65292;&#24102;&#26469;&#20102;&#35768;&#22810;&#19981;&#31934;&#30830;&#30340;&#36830;&#25509;&#12290; &#27492;&#22806;&#65292;&#23427;&#20204;&#20351;&#29992;&#21508;&#31181;&#27169;&#22411;&#25552;&#21462;&#22810;&#27169;&#24577;&#29305;&#24449;&#65292;&#23545;&#37096;&#32626;&#19981;&#21451;&#22909;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#24615;&#35282;&#33394;&#20998;&#32452;&#30340;&#32479;&#19968;&#21160;&#24577;&#22270;&#65288;UniDG&#65289;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#19968;&#20010;&#32479;&#19968;&#30340;&#34920;&#31034;&#32593;&#32476;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#21516;&#19968;&#31354;&#38388;&#20013;&#22810;&#20010;&#27169;&#24577;&#30340;&#34920;&#31034;&#65292;&#24182;&#21516;&#26102;&#20445;&#30041;&#20102;&#27169;&#24577;&#30340;&#29420;&#29305;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#24490;&#29615;&#21305;&#37197;&#31574;&#30053;&#20026;&#27599;&#20010;&#33410;&#28857;&#21160;&#24577;&#26500;&#24314;&#19981;&#21516;&#25968;&#37327;&#30340;&#37051;&#23621;&#65292;&#20197;&#33719;&#24471;&#26356;&#21487;&#38752;&#30340;&#20146;&#21644;&#22270;&#12290; &#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28176;&#36827;&#24335;&#30340;&#20851;&#32852;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Video temporal character grouping locates appearing moments of major characters within a video according to their identities. To this end, recent works have evolved from unsupervised clustering to graph-based supervised clustering. However, graph methods are built upon the premise of fixed affinity graphs, bringing many inexact connections. Besides, they extract multi-modal features with kinds of models, which are unfriendly to deployment. In this paper, we present a unified and dynamic graph (UniDG) framework for temporal character grouping. This is accomplished firstly by a unified representation network that learns representations of multiple modalities within the same space and still preserves the modality's uniqueness simultaneously. Secondly, we present a dynamic graph clustering where the neighbors of different quantities are dynamically constructed for each node via a cyclic matching strategy, leading to a more reliable affinity graph. Thirdly, a progressive association method 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#23558;&#38543;&#26426;&#26862;&#26519;&#25512;&#24191;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#27700;&#24179;&#29699;&#37325;&#26032;&#23450;&#20041;&#20102;&#20998;&#21106;&#30340;&#27010;&#24565;&#12290;&#20026;&#20102;&#22788;&#29702;&#22810;&#31867;&#25968;&#25454;&#21644;&#19981;&#24179;&#34913;&#23454;&#39564;&#65292;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31867;&#32452;&#21512;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.13279</link><description>&lt;p&gt;
&#38750;&#27431;&#20960;&#37324;&#24471;&#38543;&#26426;&#26862;&#26519;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic Random Forests. (arXiv:2308.13279v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13279
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#23558;&#38543;&#26426;&#26862;&#26519;&#25512;&#24191;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#27700;&#24179;&#29699;&#37325;&#26032;&#23450;&#20041;&#20102;&#20998;&#21106;&#30340;&#27010;&#24565;&#12290;&#20026;&#20102;&#22788;&#29702;&#22810;&#31867;&#25968;&#25454;&#21644;&#19981;&#24179;&#34913;&#23454;&#39564;&#65292;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31867;&#32452;&#21512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#30001;&#20110;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#20998;&#23618;&#32467;&#26500;&#65288;&#26080;&#35770;&#26159;&#38544;&#24335;&#36824;&#26159;&#26174;&#24335;&#65289;&#32780;&#25104;&#20026;&#34920;&#31034;&#25968;&#25454;&#30340;&#27969;&#34892;&#36873;&#25321;&#12290;&#38543;&#20043;&#32780;&#26469;&#30340;&#26159;&#38656;&#35201;&#33021;&#22815;&#22312;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#35299;&#20915;&#20998;&#31867;&#31561;&#22522;&#26412;&#20219;&#21153;&#30340;&#31639;&#27861;&#12290;&#26368;&#36817;&#65292;&#26377;&#22810;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#22522;&#20110;&#36229;&#24179;&#38754;&#30340;&#20998;&#31867;&#22120;&#65288;&#22914;&#36923;&#36753;&#22238;&#24402;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#65289;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#34429;&#28982;&#26377;&#25928;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#20998;&#23618;&#25968;&#25454;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#20247;&#25152;&#21608;&#30693;&#30340;&#38543;&#26426;&#26862;&#26519;&#25512;&#24191;&#21040;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#27700;&#24179;&#29699;&#37325;&#26032;&#23450;&#20041;&#20102;&#20998;&#21106;&#30340;&#27010;&#24565;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#30001;&#20110;&#25214;&#21040;&#20840;&#23616;&#26368;&#20248;&#20998;&#21106;&#26159;&#35745;&#31639;&#19978;&#38590;&#20197;&#22788;&#29702;&#30340;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#22823;&#36793;&#30028;&#20998;&#31867;&#22120;&#25214;&#21040;&#20505;&#36873;&#30340;&#27700;&#24179;&#29699;&#12290;&#20026;&#20102;&#20351;&#38750;&#27431;&#20960;&#37324;&#24471;&#38543;&#26426;&#26862;&#26519;&#36866;&#29992;&#20110;&#22810;&#31867;&#25968;&#25454;&#21644;&#19981;&#24179;&#34913;&#23454;&#39564;&#65292;&#25105;&#20204;&#36824;&#27010;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#23427;&#20204;&#30340;&#26368;&#20302;&#20844;&#20849;&#31062;&#20808;&#21644;&#31867;&#24179;&#34913;&#30340;&#31867;&#32452;&#21512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic space is becoming a popular choice for representing data due to the hierarchical structure - whether implicit or explicit - of many real-world datasets. Along with it comes a need for algorithms capable of solving fundamental tasks, such as classification, in hyperbolic space. Recently, multiple papers have investigated hyperbolic alternatives to hyperplane-based classifiers, such as logistic regression and SVMs. While effective, these approaches struggle with more complex hierarchical data. We, therefore, propose to generalize the well-known random forests to hyperbolic space. We do this by redefining the notion of a split using horospheres. Since finding the globally optimal split is computationally intractable, we find candidate horospheres through a large-margin classifier. To make hyperbolic random forests work on multi-class data and imbalanced experiments, we furthermore outline a new method for combining classes based on their lowest common ancestor and a class-balan
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReLLa&#30340;&#26816;&#32034;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#25512;&#33616;&#20219;&#21153;&#12290;&#36890;&#36807;&#35821;&#20041;&#29992;&#25143;&#34892;&#20026;&#26816;&#32034;&#65288;SUBR&#65289;&#26469;&#25552;&#21462;&#19978;&#19979;&#25991;&#20013;&#30340;&#26377;&#29992;&#20449;&#24687;&#65292;&#20197;&#25913;&#21892;LLMs&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11131</link><description>&lt;p&gt;
ReLLa: &#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29983;&#21629;&#21608;&#26399;&#24207;&#21015;&#34892;&#20026;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation. (arXiv:2308.11131v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReLLa&#30340;&#26816;&#32034;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#25512;&#33616;&#20219;&#21153;&#12290;&#36890;&#36807;&#35821;&#20041;&#29992;&#25143;&#34892;&#20026;&#26816;&#32034;&#65288;SUBR&#65289;&#26469;&#25552;&#21462;&#19978;&#19979;&#25991;&#20013;&#30340;&#26377;&#29992;&#20449;&#24687;&#65292;&#20197;&#25913;&#21892;LLMs&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#31361;&#30772;&#65292;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#24182;&#34987;&#31215;&#26497;&#25506;&#32034;&#12290;&#26412;&#25991;&#19987;&#27880;&#20110;&#36866;&#24212;&#21644;&#22686;&#24378;&#32431;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#25512;&#33616;&#20219;&#21153;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#38024;&#23545;&#25512;&#33616;&#39046;&#22495;&#20013;LLMs&#26080;&#27861;&#20174;&#38271;&#29992;&#25143;&#34892;&#20026;&#24207;&#21015;&#30340;&#25991;&#26412;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#24182;&#23450;&#20041;&#20102;&#29983;&#21629;&#21608;&#26399;&#24207;&#21015;&#34892;&#20026;&#29702;&#35299;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#25552;&#39640;LLMs&#30340;&#25512;&#33616;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;ReLLa&#65289;&#12290;&#38024;&#23545;&#38646;&#26679;&#26412;&#25512;&#33616;&#65292;&#25105;&#20204;&#25191;&#34892;&#35821;&#20041;&#29992;&#25143;&#34892;&#20026;&#26816;&#32034;&#65288;SUBR&#65289;&#26469;&#25552;&#39640;&#25968;&#25454;&#30340;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
With large language models (LLMs) achieving remarkable breakthroughs in natural language processing (NLP) domains, LLM-enhanced recommender systems have received much attention and have been actively explored currently. In this paper, we focus on adapting and empowering a pure large language model for zero-shot and few-shot recommendation tasks. First and foremost, we identify and formulate the lifelong sequential behavior incomprehension problem for LLMs in recommendation domains, i.e., LLMs fail to extract useful information from a textual context of long user behavior sequence, even if the length of context is far from reaching the context limitation of LLMs. To address such an issue and improve the recommendation performance of LLMs, we propose a novel framework, namely Retrieval-enhanced Large Language models (ReLLa) for recommendation tasks in both zero-shot and few-shot settings. For zero-shot recommendation, we perform semantic user behavior retrieval (SUBR) to improve the data
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#34920;&#26684;&#25968;&#25454;&#38598;&#20013;&#30340;&#33258;&#28982;&#20559;&#31227;&#36827;&#34892;&#30740;&#31350;&#65292;&#21457;&#29616;$Y|X$-&#20559;&#31227;&#26368;&#20026;&#26222;&#36941;&#12290;&#20026;&#20102;&#25512;&#21160;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#25551;&#36848;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#30340;&#31934;&#32454;&#35821;&#35328;&#65292;&#20316;&#32773;&#26500;&#24314;&#20102;WhyShift&#23454;&#39564;&#24179;&#21488;&#65292;&#24182;&#35752;&#35770;&#20102;$Y|X$-&#20559;&#31227;&#23545;&#31639;&#27861;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.05284</link><description>&lt;p&gt;
&#20851;&#20110;&#38656;&#35201;&#25551;&#36848;&#20998;&#24067;&#20559;&#31227;&#30340;&#35821;&#35328;&#65306;&#22522;&#20110;&#34920;&#26684;&#25968;&#25454;&#38598;&#30340;&#26696;&#20363;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
On the Need for a Language Describing Distribution Shifts: Illustrations on Tabular Datasets. (arXiv:2307.05284v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05284
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#34920;&#26684;&#25968;&#25454;&#38598;&#20013;&#30340;&#33258;&#28982;&#20559;&#31227;&#36827;&#34892;&#30740;&#31350;&#65292;&#21457;&#29616;$Y|X$-&#20559;&#31227;&#26368;&#20026;&#26222;&#36941;&#12290;&#20026;&#20102;&#25512;&#21160;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#25551;&#36848;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#30340;&#31934;&#32454;&#35821;&#35328;&#65292;&#20316;&#32773;&#26500;&#24314;&#20102;WhyShift&#23454;&#39564;&#24179;&#21488;&#65292;&#24182;&#35752;&#35770;&#20102;$Y|X$-&#20559;&#31227;&#23545;&#31639;&#27861;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#30340;&#20998;&#24067;&#20559;&#31227;&#38656;&#35201;&#19981;&#21516;&#30340;&#31639;&#27861;&#21644;&#25805;&#20316;&#24178;&#39044;&#12290;&#26041;&#27861;&#30740;&#31350;&#24517;&#39035;&#20197;&#20854;&#25152;&#28041;&#21450;&#30340;&#20855;&#20307;&#20559;&#31227;&#20026;&#22522;&#30784;&#12290;&#23613;&#31649;&#26032;&#20852;&#30340;&#22522;&#20934;&#25968;&#25454;&#20026;&#23454;&#35777;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#22522;&#30784;&#65292;&#20294;&#23427;&#20204;&#38544;&#21547;&#22320;&#20851;&#27880;&#21327;&#21464;&#37327;&#20559;&#31227;&#65292;&#24182;&#19988;&#23454;&#35777;&#21457;&#29616;&#30340;&#26377;&#25928;&#24615;&#21462;&#20915;&#20110;&#20559;&#31227;&#31867;&#22411;&#65292;&#20363;&#22914;&#65292;&#24403;$Y|X$&#20998;&#24067;&#21457;&#29983;&#21464;&#21270;&#26102;&#65292;&#20043;&#21069;&#20851;&#20110;&#31639;&#27861;&#24615;&#33021;&#30340;&#35266;&#23519;&#21487;&#33021;&#26080;&#25928;&#12290;&#25105;&#20204;&#23545;5&#20010;&#34920;&#26684;&#25968;&#25454;&#38598;&#20013;&#30340;&#33258;&#28982;&#20559;&#31227;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#36890;&#36807;&#23545;86,000&#20010;&#27169;&#22411;&#37197;&#32622;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;$Y|X$-&#20559;&#31227;&#26368;&#20026;&#26222;&#36941;&#12290;&#20026;&#20102;&#40723;&#21169;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#19968;&#31181;&#31934;&#32454;&#30340;&#25551;&#36848;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#30340;&#35821;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;WhyShift&#65292;&#19968;&#20010;&#30001;&#31574;&#21010;&#30340;&#30495;&#23454;&#19990;&#30028;&#20559;&#31227;&#27979;&#35797;&#24179;&#21488;&#65292;&#22312;&#20854;&#20013;&#25105;&#20204;&#23545;&#25105;&#20204;&#22522;&#20934;&#24615;&#33021;&#30340;&#20559;&#31227;&#31867;&#22411;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#30001;&#20110;$Y|X$-&#20559;&#31227;&#22312;&#34920;&#26684;&#35774;&#32622;&#20013;&#24456;&#24120;&#35265;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#21463;&#21040;&#26368;&#22823;$Y|X$-&#20559;&#31227;&#24433;&#21709;&#30340;&#21327;&#21464;&#37327;&#21306;&#22495;&#65292;&#24182;&#35752;&#35770;&#20102;&#23545;&#31639;&#27861;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Different distribution shifts require different algorithmic and operational interventions. Methodological research must be grounded by the specific shifts they address. Although nascent benchmarks provide a promising empirical foundation, they implicitly focus on covariate shifts, and the validity of empirical findings depends on the type of shift, e.g., previous observations on algorithmic performance can fail to be valid when the $Y|X$ distribution changes. We conduct a thorough investigation of natural shifts in 5 tabular datasets over 86,000 model configurations, and find that $Y|X$-shifts are most prevalent. To encourage researchers to develop a refined language for distribution shifts, we build WhyShift, an empirical testbed of curated real-world shifts where we characterize the type of shift we benchmark performance over. Since $Y|X$-shifts are prevalent in tabular settings, we identify covariate regions that suffer the biggest $Y|X$-shifts and discuss implications for algorithm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#20851;&#31995;&#25277;&#21462;&#39046;&#22495;&#30340;&#24212;&#29992;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#35752;&#35770;&#20102;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#24212;&#23545;&#30340;&#25216;&#26415;&#65292;&#24182;&#23637;&#26395;&#20102;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.02051</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#20851;&#31995;&#25277;&#21462;&#39046;&#22495;&#30340;&#32508;&#36848;&#65306;&#26368;&#26032;&#36827;&#23637;&#19982;&#26032;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Deep Learning for Relation Extraction: Recent Advances and New Frontiers. (arXiv:2306.02051v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#20851;&#31995;&#25277;&#21462;&#39046;&#22495;&#30340;&#24212;&#29992;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#35752;&#35770;&#20102;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#24212;&#23545;&#30340;&#25216;&#26415;&#65292;&#24182;&#23637;&#26395;&#20102;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25277;&#21462;&#26159;&#25351;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#35782;&#21035;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20851;&#31995;&#25277;&#21462;&#26159;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#30340;&#22522;&#30784;&#65292;&#20363;&#22914;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#12289;&#38382;&#31572;&#21644;&#20449;&#24687;&#26816;&#32034;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20851;&#31995;&#25277;&#21462;&#39046;&#22495;&#21344;&#25454;&#20102;&#20027;&#23548;&#22320;&#20301;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#12290;&#38543;&#21518;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#23558;&#20851;&#31995;&#25277;&#21462;&#30340;&#26368;&#26032;&#25216;&#26415;&#25512;&#21521;&#20102;&#19968;&#20010;&#26032;&#30340;&#39640;&#24230;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#29616;&#26377;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#24212;&#29992;&#24773;&#20917;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20851;&#31995;&#25277;&#21462;&#36164;&#28304;&#65292;&#21253;&#25324;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#20174;&#25991;&#26412;&#34920;&#31034;&#12289;&#19978;&#19979;&#25991;&#32534;&#30721;&#21644;&#19977;&#20803;&#32452;&#39044;&#27979;&#19977;&#20010;&#26041;&#38754;&#23545;&#29616;&#26377;&#24037;&#20316;&#36827;&#34892;&#20998;&#31867;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20851;&#31995;&#25277;&#21462;&#38754;&#20020;&#30340;&#19968;&#20123;&#37325;&#35201;&#25361;&#25112;&#65292;&#24182;&#24635;&#32467;&#20102;&#21487;&#33021;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#30340;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#19968;&#20123;&#20855;&#26377;&#28508;&#22312;&#21069;&#26223;&#30340;&#26410;&#26469;&#26041;&#21521;&#21644;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation extraction (RE) involves identifying the relations between entities from unstructured texts. RE serves as the foundation for many natural language processing (NLP) applications, such as knowledge graph completion, question answering, and information retrieval. In recent years, deep neural networks have dominated the field of RE and made noticeable progress. Subsequently, the large pre-trained language models (PLMs) have taken the state-of-the-art of RE to a new level. This survey provides a comprehensive review of existing deep learning techniques for RE. First, we introduce RE resources, including RE datasets and evaluation metrics. Second, we propose a new taxonomy to categorize existing works from three perspectives (text representation, context encoding, and triplet prediction). Third, we discuss several important challenges faced by RE and summarize potential techniques to tackle these challenges. Finally, we outline some promising future directions and prospects in this 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102; Mix Prompt Tuning&#65288;MPT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25163;&#21160;&#25552;&#31034;&#27169;&#26495;&#19982;&#33258;&#21160;&#23398;&#20064;&#30340;&#36830;&#32493;&#25552;&#31034;&#27169;&#26495;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#22810;&#31890;&#24230;&#23398;&#26415;&#21151;&#33021;&#35782;&#21035;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#20943;&#36731;&#23545;&#27880;&#37322;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2305.03287</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#20010;&#25552;&#31034;&#30693;&#35782;&#30340;&#20302;&#36164;&#28304;&#22810;&#31890;&#24230;&#23398;&#26415;&#21151;&#33021;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Low-Resource Multi-Granularity Academic Function Recognition Based on Multiple Prompt Knowledge. (arXiv:2305.03287v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03287
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102; Mix Prompt Tuning&#65288;MPT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25163;&#21160;&#25552;&#31034;&#27169;&#26495;&#19982;&#33258;&#21160;&#23398;&#20064;&#30340;&#36830;&#32493;&#25552;&#31034;&#27169;&#26495;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#22810;&#31890;&#24230;&#23398;&#26415;&#21151;&#33021;&#35782;&#21035;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#20943;&#36731;&#23545;&#27880;&#37322;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;Fine-tuning &#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#22914; SciBERT&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#27880;&#37322;&#25968;&#25454;&#25165;&#33021;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#33719;&#21462;&#31185;&#23398; NLP &#20219;&#21153;&#30340; fine-tune &#25968;&#25454;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#26114;&#36149;&#24615;&#12290;&#21463;&#25552;&#31034;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102; Mix Prompt Tuning&#65288;MPT&#65289;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;&#26088;&#22312;&#20943;&#36731;&#23545;&#27880;&#37322;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#24182;&#20351;&#29992;&#24456;&#23569;&#25968;&#37327;&#30340;&#26631;&#35760;&#31034;&#20363;&#25552;&#39640;&#22810;&#31890;&#24230;&#23398;&#26415;&#21151;&#33021;&#35782;&#21035;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#23558;&#25163;&#21160;&#25552;&#31034;&#27169;&#26495;&#19982;&#33258;&#21160;&#23398;&#20064;&#30340;&#36830;&#32493;&#25552;&#31034;&#27169;&#26495;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#22810;&#26041;&#38754;&#30340;&#34920;&#31034;&#65292;&#20197;&#24110;&#21161;&#32473;&#23450;&#30340;&#23398;&#26415;&#21151;&#33021;&#35782;&#21035;&#20219;&#21153;&#20805;&#20998;&#21033;&#29992; PLMs &#20013;&#30340;&#30693;&#35782;&#12290;&#22522;&#20110;&#36825;&#20123;&#25552;&#31034;&#27169;&#26495;&#21644; fine-tuned PLM&#65292;&#22823;&#37327;&#30340;&#20266;&#26631;&#31614;&#34987;&#20998;&#37197;&#32473;&#26410;&#26631;&#35760;&#30340;&#23454;&#20363;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning pre-trained language models (PLMs), e.g., SciBERT, generally requires large numbers of annotated data to achieve state-of-the-art performance on a range of NLP tasks in the scientific domain. However, obtaining the fine-tune data for scientific NLP task is still challenging and expensive. Inspired by recent advancement in prompt learning, in this paper, we propose the Mix Prompt Tuning (MPT), which is a semi-supervised method to alleviate the dependence on annotated data and improve the performance of multi-granularity academic function recognition tasks with a small number of labeled examples. Specifically, the proposed method provides multi-perspective representations by combining manual prompt templates with automatically learned continuous prompt templates to help the given academic function recognition task take full advantage of knowledge in PLMs. Based on these prompt templates and the fine-tuned PLM, a large number of pseudo labels are assigned to the unlabeled exam
&lt;/p&gt;</description></item><item><title>FlightBERT++&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#30340;&#22810;&#26102;&#22495;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26102;&#22495;&#24863;&#30693;&#19978;&#19979;&#25991;&#29983;&#25104;&#22120;&#35299;&#20915;&#20102;&#35823;&#24046;&#32047;&#31215;&#21644;&#20302;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.01658</link><description>&lt;p&gt;
FlightBERT++&#65306;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#22810;&#26102;&#22495;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FlightBERT++: A Non-autoregressive Multi-Horizon Flight Trajectory Prediction Framework. (arXiv:2305.01658v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01658
&lt;/p&gt;
&lt;p&gt;
FlightBERT++&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#30340;&#22810;&#26102;&#22495;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26102;&#22495;&#24863;&#30693;&#19978;&#19979;&#25991;&#29983;&#25104;&#22120;&#35299;&#20915;&#20102;&#35823;&#24046;&#32047;&#31215;&#21644;&#20302;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26159;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#21487;&#20197;&#24110;&#21161;&#31354;&#31649;&#21592;&#26356;&#23433;&#20840;&#39640;&#25928;&#22320;&#31649;&#29702;&#31354;&#22495;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#33258;&#22238;&#24402;&#26041;&#24335;&#25191;&#34892;&#22810;&#26102;&#22495;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#65292;&#23481;&#26131;&#20986;&#29616;&#35823;&#24046;&#32047;&#31215;&#21644;&#20302;&#25928;&#29575;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;FlightBERT++&#65292;&#20197;i&#65289;&#30452;&#25509;&#20197;&#38750;&#33258;&#22238;&#24402;&#26041;&#24335;&#39044;&#27979;&#22810;&#26102;&#22495;&#39134;&#34892;&#36712;&#36857;&#65292;&#21644;ii&#65289;&#25913;&#21892;FlightBERT&#26694;&#26550;&#20013;&#20108;&#36827;&#21046;&#32534;&#30721;&#65288;BE&#65289;&#34920;&#31034;&#30340;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#36890;&#36807;&#36890;&#29992;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#23454;&#29616;&#65292;&#20854;&#20013;&#32534;&#30721;&#22120;&#20174;&#21382;&#21490;&#35266;&#27979;&#20013;&#23398;&#20064;&#26102;&#31354;&#27169;&#24335;&#65292;&#32780;&#35299;&#30721;&#22120;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#27493;&#30340;&#39134;&#34892;&#29366;&#24577;&#12290;&#19982;&#20256;&#32479;&#26550;&#26500;&#30456;&#27604;&#65292;&#39069;&#22806;&#30340;&#26102;&#22495;&#24863;&#30693;&#19978;&#19979;&#25991;&#29983;&#25104;&#22120;&#65288;HACG&#65289;&#19987;&#38376;&#35774;&#35745;&#32771;&#34385;&#20808;&#21069;&#30340;&#26102;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Flight Trajectory Prediction (FTP) is an essential task in Air Traffic Control (ATC), which can assist air traffic controllers to manage airspace more safely and efficiently. Existing approaches generally perform multi-horizon FTP tasks in an autoregressive manner, which is prone to suffer from error accumulation and low-efficiency problems. In this paper, a novel framework, called FlightBERT++, is proposed to i) forecast multi-horizon flight trajectories directly in a non-autoregressive way, and ii) improved the limitation of the binary encoding (BE) representation in the FlightBERT framework. Specifically, the proposed framework is implemented by a generalized Encoder-Decoder architecture, in which the encoder learns the temporal-spatial patterns from historical observations and the decoder predicts the flight status for the future time steps. Compared to conventional architecture, an extra horizon-aware contexts generator (HACG) is dedicatedly designed to consider the prior horizon 
&lt;/p&gt;</description></item></channel></rss>