<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#20351;&#29992;Shapley&#20540;&#26041;&#27861;&#35299;&#37322;LLM&#34892;&#20026;&#65292;&#25581;&#31034;&#20102;&#25152;&#35859;&#30340;&#8220;&#20196;&#29260;&#22122;&#38899;&#8221;&#25928;&#24212;&#65292;&#25581;&#31034;&#20102;LLMs&#30340;&#20915;&#31574;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#21040;&#25552;&#31034;&#32452;&#20214;&#30340;&#24433;&#21709;</title><link>https://arxiv.org/abs/2404.01332</link><description>&lt;p&gt;
&#31561;&#31561;&#65292;&#36825;&#37117;&#26159;&#20196;&#29260;&#22122;&#38899;&#65311;&#19968;&#30452;&#23601;&#26159;&#21527;&#65306;&#21033;&#29992; Shapley &#20540;&#35299;&#37322; LLM &#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Wait, It's All Token Noise? Always Has Been: Interpreting LLM Behavior Using Shapley Value
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01332
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Shapley&#20540;&#26041;&#27861;&#35299;&#37322;LLM&#34892;&#20026;&#65292;&#25581;&#31034;&#20102;&#25152;&#35859;&#30340;&#8220;&#20196;&#29260;&#22122;&#38899;&#8221;&#25928;&#24212;&#65292;&#25581;&#31034;&#20102;LLMs&#30340;&#20915;&#31574;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#21040;&#25552;&#31034;&#32452;&#20214;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#20026;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#21644;&#35748;&#30693;&#36807;&#31243;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#28508;&#22312;&#24212;&#29992;&#21253;&#25324;&#24066;&#22330;&#30740;&#31350;&#21644;&#28040;&#36153;&#32773;&#34892;&#20026;&#20998;&#26512;&#31561;&#21508;&#20010;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLMs&#30340;&#26174;&#33879;&#24046;&#24322;&#26263;&#31034;&#20102;&#19981;&#21516;&#30340;&#22522;&#30784;&#36807;&#31243;&#22312;&#36215;&#20316;&#29992;&#65292;&#20197;&#21450;LLMs&#23545;&#25552;&#31034;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#65292;&#21033;&#29992;LLMs&#20316;&#20026;&#20154;&#31867;&#20027;&#20307;&#30340;&#26367;&#20195;&#20173;&#28982;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#20316;&#21338;&#24328;&#29702;&#35770;&#20013;Shapley&#20540;&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#37322;LLM&#34892;&#20026;&#65292;&#24182;&#37327;&#21270;&#27599;&#20010;&#25552;&#31034;&#32452;&#20214;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#30456;&#23545;&#36129;&#29486;&#12290;&#36890;&#36807;&#20004;&#20010;&#24212;&#29992;--&#19968;&#20010;&#31163;&#25955;&#36873;&#25321;&#23454;&#39564;&#21644;&#19968;&#20010;&#35748;&#30693;&#20559;&#35265;&#35843;&#26597;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Shapley&#20540;&#26041;&#27861;&#22914;&#20309;&#25581;&#31034;&#25105;&#20204;&#25152;&#35859;&#30340;&#8220;&#20196;&#29260;&#22122;&#38899;&#8221;&#25928;&#24212;&#65292;&#21363;LLM&#20915;&#31574;&#21463;&#21040;&#30340;&#24433;&#21709;&#20005;&#37325;&#20559;&#21521;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01332v1 Announce Type: cross  Abstract: The emergence of large language models (LLMs) has opened up exciting possibilities for simulating human behavior and cognitive processes, with potential applications in various domains, including marketing research and consumer behavior analysis. However, the validity of utilizing LLMs as stand-ins for human subjects remains uncertain due to glaring divergences that suggest fundamentally different underlying processes at play and the sensitivity of LLM responses to prompt variations. This paper presents a novel approach based on Shapley values from cooperative game theory to interpret LLM behavior and quantify the relative contribution of each prompt component to the model's output. Through two applications-a discrete choice experiment and an investigation of cognitive biases-we demonstrate how the Shapley value method can uncover what we term "token noise" effects, a phenomenon where LLM decisions are disproportionately influenced by 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#30456;&#23545;&#29109;&#27491;&#21017;&#21270;&#23545;&#19968;&#33324;&#21644;&#24635; $N$-agent &#28216;&#25103;&#30340;&#32435;&#20160;&#22343;&#34913;&#20135;&#29983;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;NE&#31526;&#21512;&#32447;&#24615;&#39640;&#26031;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#25919;&#31574;&#20248;&#21270;&#31639;&#27861;&#20197;&#21450;&#22686;&#24378;&#25216;&#26415;&#26469;&#25214;&#21040;&#28216;&#25103;&#20869;&#30340;NE&#12290;</title><link>https://arxiv.org/abs/2404.00045</link><description>&lt;p&gt;
&#25919;&#31574;&#20248;&#21270;&#22312;&#27491;&#21017;&#21270;&#24191;&#20041;&#21644;&#24635; LQ &#28216;&#25103;&#20013;&#25214;&#21040;&#32435;&#20160;&#22343;&#34913;
&lt;/p&gt;
&lt;p&gt;
Policy Optimization finds Nash Equilibrium in Regularized General-Sum LQ Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00045
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#30456;&#23545;&#29109;&#27491;&#21017;&#21270;&#23545;&#19968;&#33324;&#21644;&#24635; $N$-agent &#28216;&#25103;&#30340;&#32435;&#20160;&#22343;&#34913;&#20135;&#29983;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;NE&#31526;&#21512;&#32447;&#24615;&#39640;&#26031;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#25919;&#31574;&#20248;&#21270;&#31639;&#27861;&#20197;&#21450;&#22686;&#24378;&#25216;&#26415;&#26469;&#25214;&#21040;&#28216;&#25103;&#20869;&#30340;NE&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24341;&#20837;&#30456;&#23545;&#29109;&#27491;&#21017;&#21270;&#23545;&#19968;&#33324;&#21644;&#24635; $N$-agent &#28216;&#25103;&#30340;&#32435;&#20160;&#22343;&#34913; (NE) &#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#36825;&#31867;&#28216;&#25103;&#30340;NE&#31526;&#21512;&#32447;&#24615;&#39640;&#26031;&#31574;&#30053;&#30340;&#20107;&#23454;&#12290;&#27492;&#22806;&#65292;&#23427;&#25551;&#32472;&#20102;&#22312;&#29109;&#27491;&#21017;&#21270;&#30340;&#36866;&#24403;&#24615;&#26041;&#38754;&#65292;&#23545;&#28216;&#25103;&#20869;NE&#29420;&#29305;&#24615;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#30001;&#20110;&#25919;&#31574;&#20248;&#21270;&#26159;&#24378;&#21270;&#23398;&#20064; (RL) &#25216;&#26415;&#30340;&#22522;&#30784;&#26041;&#27861;&#65292;&#26088;&#22312;&#25214;&#21040; NE&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#25919;&#31574;&#20248;&#21270;&#31639;&#27861;&#30340;&#32447;&#24615;&#25910;&#25947;&#24615;&#65292;&#35813;&#31639;&#27861; (&#22312;&#29109;&#27491;&#21017;&#21270;&#30340;&#36866;&#24403;&#24615;&#19979;) &#33021;&#22815;&#26126;&#26174;&#22320;&#23454;&#29616; NE&#12290;&#27492;&#22806;&#65292;&#22312;&#29109;&#27491;&#21017;&#21270;&#35777;&#26126;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010; $\delta$-&#22686;&#24378;&#25216;&#26415;&#65292;&#26377;&#21161;&#20110;&#23454;&#29616;&#28216;&#25103;&#20869;&#30340; $\epsilon$-NE&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00045v1 Announce Type: cross  Abstract: In this paper, we investigate the impact of introducing relative entropy regularization on the Nash Equilibria (NE) of General-Sum $N$-agent games, revealing the fact that the NE of such games conform to linear Gaussian policies. Moreover, it delineates sufficient conditions, contingent upon the adequacy of entropy regularization, for the uniqueness of the NE within the game. As Policy Optimization serves as a foundational approach for Reinforcement Learning (RL) techniques aimed at finding the NE, in this work we prove the linear convergence of a policy optimization algorithm which (subject to the adequacy of entropy regularization) is capable of provably attaining the NE. Furthermore, in scenarios where the entropy regularization proves insufficient, we present a $\delta$-augmentation technique, which facilitates the achievement of an $\epsilon$-NE within the game.
&lt;/p&gt;</description></item><item><title>CoverUp&#36890;&#36807;&#35206;&#30422;&#29575;&#20998;&#26512;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#39537;&#21160;&#29983;&#25104;&#39640;&#35206;&#30422;&#29575;&#30340;Python&#22238;&#24402;&#27979;&#35797;&#65292;&#24182;&#22312;&#25913;&#36827;&#35206;&#30422;&#29575;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25104;&#23601;&#12290;</title><link>https://arxiv.org/abs/2403.16218</link><description>&lt;p&gt;
CoverUp&#65306;&#22522;&#20110;&#35206;&#30422;&#29575;&#24341;&#23548;&#30340;LLM&#27979;&#35797;&#29983;&#25104;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
CoverUp: Coverage-Guided LLM-Based Test Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16218
&lt;/p&gt;
&lt;p&gt;
CoverUp&#36890;&#36807;&#35206;&#30422;&#29575;&#20998;&#26512;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#39537;&#21160;&#29983;&#25104;&#39640;&#35206;&#30422;&#29575;&#30340;Python&#22238;&#24402;&#27979;&#35797;&#65292;&#24182;&#22312;&#25913;&#36827;&#35206;&#30422;&#29575;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25104;&#23601;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;CoverUp&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#22411;&#31995;&#32479;&#65292;&#36890;&#36807;&#35206;&#30422;&#29575;&#20998;&#26512;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#32467;&#21512;&#39537;&#21160;&#29983;&#25104;&#39640;&#35206;&#30422;&#29575;&#30340;Python&#22238;&#24402;&#27979;&#35797;&#12290;CoverUp&#36890;&#36807;&#36845;&#20195;&#25913;&#21892;&#35206;&#30422;&#29575;&#65292;&#23558;&#35206;&#30422;&#29575;&#20998;&#26512;&#19982;LLM&#23545;&#35805;&#20132;&#26367;&#36827;&#34892;&#65292;&#20197;&#20415;&#23558;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#23578;&#26410;&#28085;&#30422;&#30340;&#20195;&#30721;&#34892;&#21644;&#20998;&#25903;&#19978;&#12290;&#26368;&#32456;&#30340;&#27979;&#35797;&#22871;&#20214;&#30456;&#27604;&#24403;&#21069;&#25216;&#26415;&#27700;&#24179;&#26174;&#33879;&#25552;&#39640;&#20102;&#35206;&#30422;&#29575;&#65306;&#19982;CodaMosa&#30456;&#27604;&#65292;&#19968;&#31181;&#28151;&#21512;LLM / &#22522;&#20110;&#25628;&#32034;&#30340;&#36719;&#20214;&#27979;&#35797;&#31995;&#32479;&#65292;CoverUp&#22312;&#21508;&#26041;&#38754;&#37117;&#22823;&#24133;&#25552;&#39640;&#20102;&#35206;&#30422;&#29575;&#12290;&#20197;&#27169;&#22359;&#20026;&#22522;&#30784;&#65292;CoverUp&#23454;&#29616;&#20102;81%&#30340;&#20013;&#20301;&#32447;&#35206;&#30422;&#29575;&#65288;&#23545;&#27604;62%&#65289;&#12289;53%&#30340;&#20998;&#25903;&#35206;&#30422;&#29575;&#65288;&#23545;&#27604;35%&#65289;&#21644;78%&#30340;&#32447;+&#20998;&#25903;&#35206;&#30422;&#29575;&#65288;&#23545;&#27604;55%&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CoverUp&#30340;&#36845;&#20195;&#12289;&#35206;&#30422;&#29575;&#24341;&#23548;&#26041;&#27861;&#23545;&#20854;&#26377;&#25928;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#20026;&#20854;&#25104;&#21151;&#30340;&#36817;&#19968;&#21322;&#20316;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16218v1 Announce Type: cross  Abstract: This paper presents CoverUp, a novel system that drives the generation of high-coverage Python regression tests via a combination of coverage analysis and large-language models (LLMs). CoverUp iteratively improves coverage, interleaving coverage analysis with dialogs with the LLM to focus its attention on as yet uncovered lines and branches. The resulting test suites significantly improve coverage over the current state of the art: compared to CodaMosa, a hybrid LLM / search-based software testing system, CoverUp substantially improves coverage across the board. On a per-module basis, CoverUp achieves median line coverage of 81% (vs. 62%), branch coverage of 53% (vs. 35%) and line+branch coverage of 78% (vs. 55%). We show that CoverUp's iterative, coverage-guided approach is crucial to its effectiveness, contributing to nearly half of its successes.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25277;&#35937;&#21644;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#25968;&#25454;&#38598;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21644;&#19978;&#19979;&#25991;&#29702;&#35299;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#36739;&#24369;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#22312;&#36923;&#36753;&#36830;&#36143;&#24615;&#12289;&#32452;&#21512;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20173;&#28982;&#33853;&#21518;&#65292;&#23454;&#39564;&#32467;&#26524;&#26377;&#21161;&#20110;&#25552;&#20986;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#25512;&#29702;&#30340;&#21457;&#23637;&#36335;&#24452;&#12290;</title><link>https://arxiv.org/abs/2403.11793</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65306;&#23545;&#25277;&#35937;&#21644;&#25512;&#29702;&#35821;&#26009;&#24211;&#30340;&#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Reasoning Abilities of Large Language Models: In-Depth Analysis on the Abstraction and Reasoning Corpus
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11793
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25277;&#35937;&#21644;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#25968;&#25454;&#38598;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21644;&#19978;&#19979;&#25991;&#29702;&#35299;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#36739;&#24369;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#22312;&#36923;&#36753;&#36830;&#36143;&#24615;&#12289;&#32452;&#21512;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20173;&#28982;&#33853;&#21518;&#65292;&#23454;&#39564;&#32467;&#26524;&#26377;&#21161;&#20110;&#25552;&#20986;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#25512;&#29702;&#30340;&#21457;&#23637;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#29702;&#33021;&#21147;&#30340;&#29616;&#26377;&#26041;&#27861;&#20197;&#32467;&#26524;&#20026;&#20013;&#24515;&#65292;&#20351;&#24471;&#35780;&#20272;&#25512;&#29702;&#36807;&#31243;&#21464;&#24471;&#22256;&#38590;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#25277;&#35937;&#21644;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#25968;&#25454;&#38598;&#20197;&#36807;&#31243;&#20026;&#20013;&#24515;&#30340;&#26041;&#24335;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21644;&#19978;&#19979;&#25991;&#29702;&#35299;&#33021;&#21147;&#12290;ARC&#35201;&#27714;&#35299;&#20915;&#38382;&#39064;&#26102;&#20855;&#26377;&#20005;&#35880;&#30340;&#36923;&#36753;&#32467;&#26500;&#65292;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;&#19968;&#20010;&#33021;&#22815;&#20419;&#36827;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#19982;&#20154;&#31867;&#36827;&#34892;&#27604;&#36739;&#30340;&#22522;&#20934;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#65292;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#36739;&#24369;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#22312;&#36923;&#36753;&#36830;&#36143;&#24615;&#12289;&#32452;&#21512;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20173;&#28982;&#33853;&#21518;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#31361;&#26174;&#20102;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#25512;&#29702;&#30340;&#21457;&#23637;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11793v1 Announce Type: cross  Abstract: The existing methods for evaluating the inference abilities of Large Language Models (LLMs) have been results-centric, making it difficult to assess the inference process. We introduce a new approach using the Abstract and Reasoning Corpus (ARC) dataset to evaluate the inference and contextual understanding abilities of large language models in a process-centric manner. ARC demands rigorous logical structures for problem-solving, making it a benchmark that facilitates the comparison of model inference abilities with humans. Experimental results confirm that while large language models possess weak inference abilities, they still lag in terms of logical coherence, compositionality, and productivity. Our experiments highlight the reasoning capabilities of LLMs, proposing development paths for achieving human-level reasoning.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;\carb &#30340;&#31471;&#21040;&#31471;&#24314;&#27169;&#24037;&#20855;&#65292;&#29992;&#20110;&#22312;&#29289;&#32852;&#32593;-&#21551;&#29992;&#28145;&#24230;&#23398;&#20064;&#20013;&#31934;&#30830;&#20272;&#31639;&#30899;&#36275;&#36857;&#65292;&#23637;&#31034;&#20102;&#19982;&#23454;&#38469;&#27979;&#37327;&#20540;&#30456;&#27604;&#26368;&#22823;$\pm21\%$&#30340;&#30899;&#36275;&#36857;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.10984</link><description>&lt;p&gt;
IoTCO2&#65306;&#35780;&#20272;&#29289;&#32852;&#32593;-&#21551;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#30899;&#36275;&#36857;
&lt;/p&gt;
&lt;p&gt;
IoTCO2: Assessing the End-To-End Carbon Footprint of Internet-of-Things-Enabled Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10984
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;\carb &#30340;&#31471;&#21040;&#31471;&#24314;&#27169;&#24037;&#20855;&#65292;&#29992;&#20110;&#22312;&#29289;&#32852;&#32593;-&#21551;&#29992;&#28145;&#24230;&#23398;&#20064;&#20013;&#31934;&#30830;&#20272;&#31639;&#30899;&#36275;&#36857;&#65292;&#23637;&#31034;&#20102;&#19982;&#23454;&#38469;&#27979;&#37327;&#20540;&#30456;&#27604;&#26368;&#22823;$\pm21\%$&#30340;&#30899;&#36275;&#36857;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#38544;&#31169;&#24615;&#21644;&#30830;&#20445;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#65292;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#37096;&#32626;&#22312;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#35774;&#22791;&#19978;&#36827;&#34892;&#25968;&#25454;&#22788;&#29702;&#65292;&#26497;&#22823;&#22320;&#22686;&#21152;&#20102;&#19982;IoT&#19978;DL&#30456;&#20851;&#30340;&#30899;&#36275;&#36857;&#65292;&#28085;&#30422;&#20102;&#25805;&#20316;&#21644;&#23454;&#20307;&#26041;&#38754;&#12290;&#29616;&#26377;&#30340;&#25805;&#20316;&#33021;&#37327;&#39044;&#27979;&#22120;&#32463;&#24120;&#24573;&#30053;&#20102;&#37327;&#21270;&#30340;DL&#27169;&#22411;&#21644;&#26032;&#20852;&#30340;&#31070;&#32463;&#22788;&#29702;&#21333;&#20803;&#65288;NPUs&#65289;&#65292;&#32780;&#23454;&#20307;&#30899;&#36275;&#36857;&#24314;&#27169;&#24037;&#20855;&#24573;&#30053;&#20102;IoT&#35774;&#22791;&#20013;&#24120;&#35265;&#30340;&#38750;&#35745;&#31639;&#30828;&#20214;&#32452;&#20214;&#65292;&#23548;&#33268;&#20102;&#29289;&#32852;&#32593;DL&#20934;&#30830;&#30899;&#36275;&#36857;&#24314;&#27169;&#24037;&#20855;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;\textit{\carb}&#65292;&#19968;&#31181;&#29992;&#20110;&#31934;&#30830;&#20272;&#31639;&#29289;&#32852;&#32593;DL&#20013;&#30899;&#36275;&#36857;&#30340;&#31471;&#21040;&#31471;&#24314;&#27169;&#24037;&#20855;&#65292;&#23637;&#31034;&#20102;&#19982;&#21508;&#31181;DL&#27169;&#22411;&#30340;&#23454;&#38469;&#27979;&#37327;&#20540;&#30456;&#27604;&#26368;&#22823;$\pm21\%$&#30340;&#30899;&#36275;&#36857;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;\carb&#30340;&#23454;&#38469;&#24212;&#29992;&#36890;&#36807;&#22810;&#20010;&#29992;&#25143;&#26696;&#20363;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10984v1 Announce Type: cross  Abstract: To improve privacy and ensure quality-of-service (QoS), deep learning (DL) models are increasingly deployed on Internet of Things (IoT) devices for data processing, significantly increasing the carbon footprint associated with DL on IoT, covering both operational and embodied aspects. Existing operational energy predictors often overlook quantized DL models and emerging neural processing units (NPUs), while embodied carbon footprint modeling tools neglect non-computing hardware components common in IoT devices, creating a gap in accurate carbon footprint modeling tools for IoT-enabled DL. This paper introduces \textit{\carb}, an end-to-end modeling tool for precise carbon footprint estimation in IoT-enabled DL, demonstrating a maximum $\pm21\%$ deviation in carbon footprint values compared to actual measurements across various DL models. Additionally, practical applications of \carb are showcased through multiple user case studies.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#26041;&#27861;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#65288;PERL&#65289;&#65292;&#33021;&#22815;&#22312;&#19982;&#20256;&#32479;RLHF&#35774;&#32622;&#30456;&#24403;&#30340;&#24615;&#33021;&#19979;&#65292;&#23454;&#29616;&#26356;&#24555;&#30340;&#35757;&#32451;&#21644;&#26356;&#23569;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.10704</link><description>&lt;p&gt;
PERL: &#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PERL: Parameter Efficient Reinforcement Learning from Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10704
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#26041;&#27861;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#65288;PERL&#65289;&#65292;&#33021;&#22815;&#22312;&#19982;&#20256;&#32479;RLHF&#35774;&#32622;&#30456;&#24403;&#30340;&#24615;&#33021;&#19979;&#65292;&#23454;&#29616;&#26356;&#24555;&#30340;&#35757;&#32451;&#21644;&#26356;&#23569;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#23558;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;RLHF&#35757;&#32451;&#27169;&#22411;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65292;&#19988;&#25972;&#20010;&#36807;&#31243;&#22797;&#26434;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;RLHF&#65292;&#20854;&#20013;&#22522;&#30784;&#27169;&#22411;&#20351;&#29992;&#32993;&#31561;&#20154;&#25552;&#20986;&#30340;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#8220;&#21442;&#25968;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#8221;&#65288;PERL&#65289;&#30340;&#35774;&#32622;&#65292;&#22312;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;LoRA&#36827;&#34892;&#22870;&#21169;&#27169;&#22411;&#35757;&#32451;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#23558;PERL&#19982;&#20256;&#32479;&#30340;&#24494;&#35843;&#65288;&#20840;&#35843;&#65289;&#22312;&#21253;&#25324;2&#20010;&#26032;&#25968;&#25454;&#38598;&#22312;&#20869;&#30340;7&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#22870;&#21169;&#24314;&#27169;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#38754;&#30340;&#21508;&#31181;&#37197;&#32622;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;PERL&#30340;&#24615;&#33021;&#19982;&#20256;&#32479;&#30340;RLHF&#35774;&#32622;&#30456;&#24403;&#65292;&#21516;&#26102;&#35757;&#32451;&#36895;&#24230;&#26356;&#24555;&#65292;&#20869;&#23384;&#21344;&#29992;&#26356;&#23569;&#12290;&#36825;&#20351;&#24471;RLHF&#20855;&#26377;&#24456;&#39640;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10704v1 Announce Type: cross  Abstract: Reinforcement Learning from Human Feedback (RLHF) has proven to be a strong method to align Pretrained Large Language Models (LLMs) with human preferences. But training models with RLHF is computationally expensive, and an overall complex process. In this work, we study RLHF where the underlying models are trained using the parameter efficient method of Low-Rank Adaptation (LoRA) introduced by Hu et al. [2021]. We investigate the setup of "Parameter Efficient Reinforcement Learning" (PERL), in which we perform reward model training and reinforcement learning using LoRA. We compare PERL to conventional fine-tuning (full-tuning) across various configurations for 7 benchmarks, including 2 novel datasets, of reward modeling and reinforcement learning. We find that PERL performs on par with the conventional RLHF setting, while training faster, and with less memory. This enables the high performance of RLHF, while reducing the computational 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#28216;&#25103;&#39046;&#22495;&#20013;&#30340;&#22810;&#31181;&#24212;&#29992;&#21450;&#20854;&#35282;&#33394;&#65292;&#25351;&#20986;&#20102;&#26410;&#24320;&#21457;&#39046;&#22495;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#22312;&#28216;&#25103;&#39046;&#22495;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.18659</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#28216;&#25103;&#65306;&#35843;&#30740;&#19982;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
Large Language Models and Games: A Survey and Roadmap
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18659
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#28216;&#25103;&#39046;&#22495;&#20013;&#30340;&#22810;&#31181;&#24212;&#29992;&#21450;&#20854;&#35282;&#33394;&#65292;&#25351;&#20986;&#20102;&#26410;&#24320;&#21457;&#39046;&#22495;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#22312;&#28216;&#25103;&#39046;&#22495;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30740;&#31350;&#24613;&#21095;&#22686;&#21152;&#65292;&#24182;&#20276;&#38543;&#30528;&#20844;&#20247;&#23545;&#35813;&#20027;&#39064;&#30340;&#21442;&#19982;&#12290;&#23613;&#31649;&#36215;&#21021;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;LLMs&#22312;&#24191;&#27867;&#30340;&#24212;&#29992;&#21644;&#39046;&#22495;&#20013;&#23637;&#29616;&#20986;&#26174;&#33879;&#28508;&#21147;&#65292;&#21253;&#25324;&#28216;&#25103;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;LLMs&#22312;&#28216;&#25103;&#20013;&#21450;&#20026;&#28216;&#25103;&#25552;&#20379;&#25903;&#25345;&#30340;&#21508;&#31181;&#24212;&#29992;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#26126;&#30830;&#20102;LLMs&#22312;&#28216;&#25103;&#20013;&#21487;&#20197;&#25198;&#28436;&#30340;&#19981;&#21516;&#35282;&#33394;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23578;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#21644;LLMs&#22312;&#28216;&#25103;&#20013;&#26410;&#26469;&#24212;&#29992;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#20197;&#21450;&#22312;&#28216;&#25103;&#39046;&#22495;&#20013;LLMs&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;&#20316;&#20026;LLMs&#21644;&#28216;&#25103;&#20132;&#21449;&#39046;&#22495;&#30340;&#31532;&#19968;&#20221;&#32508;&#21512;&#35843;&#26597;&#21644;&#36335;&#32447;&#22270;&#65292;&#25105;&#20204;&#24076;&#26395;&#26412;&#25991;&#33021;&#22815;&#25104;&#20026;&#36825;&#19968;&#28608;&#21160;&#20154;&#24515;&#30340;&#26032;&#39046;&#22495;&#30340;&#24320;&#21019;&#24615;&#30740;&#31350;&#21644;&#21019;&#26032;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18659v1 Announce Type: cross  Abstract: Recent years have seen an explosive increase in research on large language models (LLMs), and accompanying public engagement on the topic. While starting as a niche area within natural language processing, LLMs have shown remarkable potential across a broad range of applications and domains, including games. This paper surveys the current state of the art across the various applications of LLMs in and for games, and identifies the different roles LLMs can take within a game. Importantly, we discuss underexplored areas and promising directions for future uses of LLMs in games and we reconcile the potential and limitations of LLMs within the games domain. As the first comprehensive survey and roadmap at the intersection of LLMs and games, we are hopeful that this paper will serve as the basis for groundbreaking research and innovation in this exciting new field.
&lt;/p&gt;</description></item><item><title>&#20855;&#35937;&#20154;&#24037;&#26234;&#33021;&#34987;&#25552;&#20986;&#20316;&#20026;&#36861;&#27714;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#19979;&#19968;&#20010;&#22522;&#26412;&#27493;&#39588;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#35748;&#30693;&#26550;&#26500;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#19982;Friston&#30340;&#20027;&#21160;&#25512;&#26029;&#21407;&#21017;&#20445;&#25345;&#19968;&#33268;&#65292;&#20026;&#20855;&#35937;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.03824</link><description>&lt;p&gt;
&#19968;&#31181;&#20851;&#20110;&#20855;&#35937;&#20154;&#24037;&#26234;&#33021;&#30340;&#21628;&#21505;
&lt;/p&gt;
&lt;p&gt;
A call for embodied AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03824
&lt;/p&gt;
&lt;p&gt;
&#20855;&#35937;&#20154;&#24037;&#26234;&#33021;&#34987;&#25552;&#20986;&#20316;&#20026;&#36861;&#27714;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#19979;&#19968;&#20010;&#22522;&#26412;&#27493;&#39588;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#35748;&#30693;&#26550;&#26500;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#19982;Friston&#30340;&#20027;&#21160;&#25512;&#26029;&#21407;&#21017;&#20445;&#25345;&#19968;&#33268;&#65292;&#20026;&#20855;&#35937;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20855;&#35937;&#20154;&#24037;&#26234;&#33021;&#20316;&#20026;&#36861;&#27714;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#19979;&#19968;&#20010;&#22522;&#26412;&#27493;&#39588;&#65292;&#24182;&#23558;&#20854;&#19982;&#24403;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#36827;&#23637;&#36827;&#34892;&#23545;&#27604;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#36328;&#36234;&#20102;&#21746;&#23398;&#12289;&#24515;&#29702;&#23398;&#12289;&#31070;&#32463;&#31185;&#23398;&#21644;&#26426;&#22120;&#20154;&#23398;&#31561;&#22810;&#20010;&#39046;&#22495;&#23545;&#20855;&#35937;&#27010;&#24565;&#30340;&#28436;&#21464;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20197;&#20984;&#26174;&#20855;&#35937;&#20154;&#24037;&#26234;&#33021;&#22914;&#20309;&#21306;&#21035;&#20110;&#38745;&#24577;&#23398;&#20064;&#30340;&#32463;&#20856;&#33539;&#24335;&#12290;&#36890;&#36807;&#25193;&#22823;&#20855;&#35937;&#20154;&#24037;&#26234;&#33021;&#30340;&#33539;&#22260;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#35748;&#30693;&#26550;&#26500;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24378;&#35843;&#30693;&#35273;&#12289;&#34892;&#21160;&#12289;&#35760;&#24518;&#21644;&#23398;&#20064;&#20316;&#20026;&#20855;&#35937;&#20195;&#29702;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#12290;&#36825;&#20010;&#26694;&#26550;&#19982;Friston&#30340;&#20027;&#21160;&#25512;&#26029;&#21407;&#21017;&#20445;&#25345;&#19968;&#33268;&#65292;&#20026;&#20855;&#35937;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#35832;&#22914;&#21046;&#23450;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#23398;&#20064;&#29702;&#35770;&#21644;&#21019;&#26032;&#20808;&#36827;&#30828;&#20214;&#31561;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#35752;&#35770;&#20026;&#26410;&#26469;&#20855;&#35937;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#22880;&#23450;&#20102;&#22522;&#30784;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Embodied AI as the next fundamental step in the pursuit of Artificial General Intelligence, juxtaposing it against current AI advancements, particularly Large Language Models. We traverse the evolution of the embodiment concept across diverse fields - philosophy, psychology, neuroscience, and robotics - to highlight how EAI distinguishes itself from the classical paradigm of static learning. By broadening the scope of Embodied AI, we introduce a theoretical framework based on cognitive architectures, emphasizing perception, action, memory, and learning as essential components of an embodied agent. This framework is aligned with Friston's active inference principle, offering a comprehensive approach to EAI development. Despite the progress made in the field of AI, substantial challenges, such as the formulation of a novel AI learning theory and the innovation of advanced hardware, persist. Our discussion lays down a foundational guideline for future Embodied AI research. High
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#29983;&#25104;&#24189;&#28789;&#30340;&#28508;&#22312;&#23454;&#26045;&#35774;&#35745;&#31354;&#38388;&#21644;&#20854;&#23545;&#20010;&#20154;&#21644;&#31038;&#20250;&#30340;&#23454;&#38469;&#21644;&#20262;&#29702;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#30740;&#31350;&#35758;&#31243;&#20197;&#20415;&#20351;&#20154;&#20204;&#33021;&#22815;&#23433;&#20840;&#32780;&#26377;&#30410;&#22320;&#21019;&#24314;&#21644;&#19982;&#20154;&#24037;&#26234;&#33021;&#26469;&#19990;&#36827;&#34892;&#20114;&#21160;&#12290;</title><link>https://arxiv.org/abs/2402.01662</link><description>&lt;p&gt;
&#29983;&#25104;&#24189;&#28789;&#65306;&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;&#26469;&#19990;&#30340;&#30410;&#22788;&#21644;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Generative Ghosts: Anticipating Benefits and Risks of AI Afterlives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#29983;&#25104;&#24189;&#28789;&#30340;&#28508;&#22312;&#23454;&#26045;&#35774;&#35745;&#31354;&#38388;&#21644;&#20854;&#23545;&#20010;&#20154;&#21644;&#31038;&#20250;&#30340;&#23454;&#38469;&#21644;&#20262;&#29702;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#30740;&#31350;&#35758;&#31243;&#20197;&#20415;&#20351;&#20154;&#20204;&#33021;&#22815;&#23433;&#20840;&#32780;&#26377;&#30410;&#22320;&#21019;&#24314;&#21644;&#19982;&#20154;&#24037;&#26234;&#33021;&#26469;&#19990;&#36827;&#34892;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#24615;&#33021;&#30340;&#24191;&#24230;&#21644;&#28145;&#24230;&#19978;&#36805;&#36895;&#25552;&#21319;&#65292;&#23427;&#20204;&#36234;&#26469;&#36234;&#36866;&#21512;&#21019;&#24314;&#21151;&#33021;&#24378;&#22823;&#12289;&#36924;&#30495;&#30340;&#20195;&#29702;&#20154;&#65292;&#21253;&#25324;&#22522;&#20110;&#29305;&#23450;&#20154;&#29289;&#24314;&#27169;&#30340;&#20195;&#29702;&#20154;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#39044;&#35745;&#65292;&#22312;&#25105;&#20204;&#26377;&#29983;&#20043;&#24180;&#65292;&#20154;&#20204;&#21487;&#33021;&#20250;&#26222;&#36941;&#20351;&#29992;&#23450;&#21046;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#20154;&#19982;&#29233;&#30340;&#20154;&#21644;/&#25110;&#26356;&#24191;&#22823;&#30340;&#19990;&#30028;&#36827;&#34892;&#20114;&#21160;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#29983;&#25104;&#24189;&#28789;&#65292;&#22240;&#20026;&#36825;&#20123;&#20195;&#29702;&#20154;&#23558;&#33021;&#22815;&#29983;&#25104;&#26032;&#39062;&#30340;&#20869;&#23481;&#65292;&#32780;&#19981;&#21482;&#26159;&#22797;&#36848;&#20854;&#21019;&#20316;&#32773;&#22312;&#29983;&#21069;&#30340;&#20869;&#23481;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#29983;&#25104;&#24189;&#28789;&#28508;&#22312;&#23454;&#26045;&#30340;&#35774;&#35745;&#31354;&#38388;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#29983;&#25104;&#24189;&#28789;&#30340;&#23454;&#38469;&#21644;&#20262;&#29702;&#24433;&#21709;&#65292;&#21253;&#25324;&#23545;&#20010;&#20154;&#21644;&#31038;&#20250;&#30340;&#28508;&#22312;&#31215;&#26497;&#21644;&#28040;&#26497;&#24433;&#21709;&#12290;&#22522;&#20110;&#36825;&#20123;&#32771;&#34385;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#30740;&#31350;&#35758;&#31243;&#65292;&#26088;&#22312;&#20351;&#20154;&#20204;&#33021;&#22815;&#23433;&#20840;&#32780;&#26377;&#30410;&#22320;&#21019;&#24314;&#21644;&#19982;&#20154;&#24037;&#26234;&#33021;&#26469;&#19990;&#36827;&#34892;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
As AI systems quickly improve in both breadth and depth of performance, they lend themselves to creating increasingly powerful and realistic agents, including the possibility of agents modeled on specific people. We anticipate that within our lifetimes it may become common practice for people to create a custom AI agent to interact with loved ones and/or the broader world after death. We call these generative ghosts, since such agents will be capable of generating novel content rather than merely parroting content produced by their creator while living. In this paper, we first discuss the design space of potential implementations of generative ghosts. We then discuss the practical and ethical implications of generative ghosts, including potential positive and negative impacts on individuals and society. Based on these considerations, we lay out a research agenda for the AI and HCI research communities to empower people to create and interact with AI afterlives in a safe and beneficial 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26631;&#20934;&#21270;&#19988;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#65292;&#29992;&#20110;&#22312;&#22810;&#20013;&#24515;&#25968;&#25454;&#24211;&#20013;&#39044;&#27979;&#21152;&#25252;&#30149;&#25151;&#24739;&#32773;&#30340;&#20877;&#20837;&#38498;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2309.13781</link><description>&lt;p&gt;
ICU &#37325;&#26032;&#20837;&#38498;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Explainable Machine Learning for ICU Readmission Prediction. (arXiv:2309.13781v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26631;&#20934;&#21270;&#19988;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#65292;&#29992;&#20110;&#22312;&#22810;&#20013;&#24515;&#25968;&#25454;&#24211;&#20013;&#39044;&#27979;&#21152;&#25252;&#30149;&#25151;&#24739;&#32773;&#30340;&#20877;&#20837;&#38498;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#25252;&#30149;&#25151;&#65288;ICU&#65289;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#21307;&#38498;&#29615;&#22659;&#65292;&#21307;&#29983;&#30340;&#20915;&#31574;&#23545;&#24739;&#32773;&#30340;&#29983;&#21629;&#26500;&#25104;&#39640;&#39118;&#38505;&#12290;&#24517;&#39035;&#36981;&#24490;&#19968;&#26465;&#20840;&#38754;&#30340;&#25252;&#29702;&#36335;&#24452;&#26469;&#20943;&#23569;&#24182;&#21457;&#30151;&#12290;&#22312;&#36825;&#31181;&#29615;&#22659;&#20013;&#65292;&#19981;&#30830;&#23450;&#24615;&#12289;&#31454;&#20105;&#24615;&#21644;&#38750;&#35745;&#21010;&#24615;&#30340;&#22240;&#32032;&#22686;&#21152;&#20102;&#32479;&#19968;&#23454;&#26045;&#25252;&#29702;&#36335;&#24452;&#30340;&#22256;&#38590;&#12290;&#20877;&#20837;&#38498;&#26159;&#35813;&#36335;&#24452;&#30340;&#22256;&#38590;&#20043;&#19968;&#65292;&#21363;&#24739;&#32773;&#22312;&#30701;&#26102;&#38388;&#20869;&#20877;&#27425;&#20837;&#20303;ICU&#65292;&#23548;&#33268;&#39640;&#27515;&#20129;&#29575;&#21644;&#39640;&#36164;&#28304;&#21033;&#29992;&#29575;&#12290;&#19968;&#20123;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#24739;&#32773;&#30340;&#21307;&#30103;&#20449;&#24687;&#26469;&#39044;&#27979;&#20877;&#20837;&#38498;&#24773;&#20917;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#39044;&#27979;&#20877;&#20837;&#38498;&#26102;&#26377;&#19968;&#23450;&#30340;&#25104;&#21151;&#65292;&#20294;&#36825;&#20123;&#30740;&#31350;&#24182;&#26410;&#23545;&#20877;&#20837;&#38498;&#39044;&#27979;&#36827;&#34892;&#36866;&#24403;&#30340;&#35780;&#20272;&#12289;&#25551;&#36848;&#21644;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26631;&#20934;&#21270;&#19988;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#65292;&#29992;&#20110;&#22312;&#22810;&#20013;&#24515;&#25968;&#25454;&#24211;&#65288;&#21363;&#21253;&#21547;166,355&#21517;&#24739;&#32773;&#30340;eICU&#38431;&#21015;&#65292;200,859&#21517;...&#65289;
&lt;/p&gt;
&lt;p&gt;
The intensive care unit (ICU) comprises a complex hospital environment, where decisions made by clinicians have a high level of risk for the patients' lives. A comprehensive care pathway must then be followed to reduce p complications. Uncertain, competing and unplanned aspects within this environment increase the difficulty in uniformly implementing the care pathway. Readmission contributes to this pathway's difficulty, occurring when patients are admitted again to the ICU in a short timeframe, resulting in high mortality rates and high resource utilisation. Several works have tried to predict readmission through patients' medical information. Although they have some level of success while predicting readmission, those works do not properly assess, characterise and understand readmission prediction. This work proposes a standardised and explainable machine learning pipeline to model patient readmission on a multicentric database (i.e., the eICU cohort with 166,355 patients, 200,859 ad
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;RRWKV&#26550;&#26500;&#65292;&#23427;&#22312;&#20445;&#25345;&#35760;&#24518;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#21152;&#20837;&#22238;&#39038;&#33021;&#21147;&#26377;&#25928;&#22320;&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.05176</link><description>&lt;p&gt;
RRWKV&#65306;&#22312;RWKV&#20013;&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
RRWKV: Capturing Long-range Dependencies in RWKV. (arXiv:2306.05176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;RRWKV&#26550;&#26500;&#65292;&#23427;&#22312;&#20445;&#25345;&#35760;&#24518;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#21152;&#20837;&#22238;&#39038;&#33021;&#21147;&#26377;&#25928;&#22320;&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;Transformer&#24778;&#20154;&#30340;&#28857;&#31215;&#27880;&#24847;&#21147;&#65292;&#23427;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#30340;&#20027;&#35201;&#26550;&#26500;&#12290;&#26368;&#36817;&#65292;Receptance Weighted Key Value&#65288;RWKV&#65289;&#26550;&#26500;&#36981;&#24490;&#38750;Transformer&#26550;&#26500;&#65292;&#28040;&#38500;&#20102;&#28857;&#31215;&#27880;&#24847;&#21147;&#30340;&#32570;&#28857;&#65292;&#20854;&#20013;&#23384;&#20648;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#21576;&#20108;&#27425;&#25193;&#23637;&#12290;&#23613;&#31649;RWKV&#21033;&#29992;&#20102;&#32447;&#24615;&#24352;&#37327;&#31215;&#27880;&#24847;&#26426;&#21046;&#24182;&#36890;&#36807;&#37096;&#32626;&#26102;&#38388;&#24207;&#21015;&#27169;&#24335;&#23454;&#29616;&#20102;&#24182;&#34892;&#35745;&#31639;&#65292;&#20294;&#19982;&#26631;&#20934;Transformer&#20013;&#30452;&#25509;&#20132;&#20114;&#33719;&#24471;&#30340;&#23436;&#25972;&#20449;&#24687;&#30456;&#27604;&#65292;&#23427;&#26080;&#27861;&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#22240;&#20026;&#20854;&#21463;&#38480;&#20110;&#21521;&#21518;&#26597;&#30475;&#20808;&#21069;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#36890;&#36807;&#23558;&#22238;&#39038;&#33021;&#21147;&#32435;&#20837;RWKV&#20013;&#26469;&#35774;&#35745;Retrospected Receptance Weighted Key Value&#65288;RRWKV&#65289;&#26550;&#26500;&#65292;&#20197;&#26377;&#25928;&#22320;&#21560;&#25910;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#35760;&#24518;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Owing to the impressive dot-product attention, the Transformers have been the dominant architectures in various natural language processing (NLP) tasks. Recently, the Receptance Weighted Key Value (RWKV) architecture follows a non-transformer architecture to eliminate the drawbacks of dot-product attention, where memory and computational complexity exhibits quadratic scaling with sequence length. Although RWKV has exploited a linearly tensor-product attention mechanism and achieved parallelized computations by deploying the time-sequential mode, it fails to capture long-range dependencies because of its limitation on looking back at previous information, compared with full information obtained by direct interactions in the standard transformer. Therefore, the paper devises the Retrospected Receptance Weighted Key Value (RRWKV) architecture via incorporating the retrospecting ability into the RWKV to effectively absorb information, which maintains memory and computational efficiency as 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;EPNS&#30340;&#31561;&#21464;&#27010;&#29575;&#31070;&#32463;&#27169;&#25311;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#31995;&#32479;&#28436;&#21270;&#20013;&#29983;&#25104;&#31561;&#21464;&#20998;&#24067;&#65292;&#24182;&#22312;&#38543;&#26426;&#26102;&#31354;&#21160;&#24577;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2305.14286</link><description>&lt;p&gt;
&#31561;&#21464;&#31070;&#32463;&#27169;&#25311;&#22120;&#29992;&#20110;&#38543;&#26426;&#26102;&#31354;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Equivariant Neural Simulators for Stochastic Spatiotemporal Dynamics. (arXiv:2305.14286v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;EPNS&#30340;&#31561;&#21464;&#27010;&#29575;&#31070;&#32463;&#27169;&#25311;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#31995;&#32479;&#28436;&#21270;&#20013;&#29983;&#25104;&#31561;&#21464;&#20998;&#24067;&#65292;&#24182;&#22312;&#38543;&#26426;&#26102;&#31354;&#21160;&#24577;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#27491;&#22312;&#25104;&#20026;&#21487;&#25193;&#23637;&#30340;&#25968;&#25454;&#39537;&#21160;&#39640;&#32500;&#21160;&#24577;&#31995;&#32479;&#27169;&#25311;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#20540;&#26041;&#27861;&#19981;&#21487;&#34892;&#25110;&#35745;&#31639;&#26114;&#36149;&#30340;&#24773;&#20917;&#19979;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#24050;&#32463;&#35777;&#26126;&#22312;&#30830;&#23450;&#24615;&#31070;&#32463;&#27169;&#25311;&#22120;&#20013;&#24341;&#20837;&#22495;&#23545;&#31216;&#24615;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#20854;&#31934;&#30830;&#24615;&#12289;&#26679;&#26412;&#25928;&#29575;&#21644;&#21442;&#25968;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#23558;&#23545;&#31216;&#24615;&#32435;&#20837;&#21487;&#20197;&#27169;&#25311;&#38543;&#26426;&#29616;&#35937;&#30340;&#27010;&#29575;&#31070;&#32463;&#27169;&#25311;&#22120;&#20013;&#65292;&#25105;&#20204;&#38656;&#35201;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#31561;&#21464;&#36712;&#36857;&#20998;&#24067;&#32780;&#19981;&#26159;&#31561;&#21464;&#20989;&#25968;&#36924;&#36817;&#30340;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31561;&#21464;&#27010;&#29575;&#31070;&#32463;&#27169;&#25311;&#65288;EPNS&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#31561;&#21464;&#20998;&#24067;&#31995;&#32479;&#28436;&#21270;&#30340;&#33258;&#22238;&#24402;&#27010;&#29575;&#24314;&#27169;&#26694;&#26550;&#12290;&#25105;&#20204;&#20351;&#29992;EPNS&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#38543;&#26426;N&#20307;&#31995;&#32479;&#21644;&#38543;&#26426;&#32454;&#32990;&#21160;&#21147;&#23398;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;EPNS&#22312;p&#26041;&#38754;&#27604;&#29616;&#26377;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are emerging as a tool for scalable data-driven simulation of high-dimensional dynamical systems, especially in settings where numerical methods are infeasible or computationally expensive. Notably, it has been shown that incorporating domain symmetries in deterministic neural simulators can substantially improve their accuracy, sample efficiency, and parameter efficiency. However, to incorporate symmetries in probabilistic neural simulators that can simulate stochastic phenomena, we need a model that produces equivariant distributions over trajectories, rather than equivariant function approximations. In this paper, we propose Equivariant Probabilistic Neural Simulation (EPNS), a framework for autoregressive probabilistic modeling of equivariant distributions over system evolutions. We use EPNS to design models for a stochastic n-body system and stochastic cellular dynamics. Our results show that EPNS considerably outperforms existing neural network-based methods for p
&lt;/p&gt;</description></item><item><title>&#35777;&#26126;&#20102;&#23545;&#20110;&#35757;&#32451;&#33391;&#22909;&#30340;AI&#27169;&#22411;&#65292;&#22914;&#26524;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#65292;&#23558;&#20986;&#29616;&#31232;&#30095;&#20132;&#20114;&#27010;&#24565;&#65292;&#36825;&#20123;&#27010;&#24565;&#33021;&#22815;&#25551;&#36848;&#36755;&#20837;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#23545;&#27169;&#22411;&#25512;&#29702;&#20998;&#25968;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.01939</link><description>&lt;p&gt;
&#35777;&#26126;AI&#27169;&#22411;&#20013;&#31232;&#30095;&#31526;&#21495;&#27010;&#24565;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Where We Have Arrived in Proving the Emergence of Sparse Symbolic Concepts in AI Models. (arXiv:2305.01939v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01939
&lt;/p&gt;
&lt;p&gt;
&#35777;&#26126;&#20102;&#23545;&#20110;&#35757;&#32451;&#33391;&#22909;&#30340;AI&#27169;&#22411;&#65292;&#22914;&#26524;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#65292;&#23558;&#20986;&#29616;&#31232;&#30095;&#20132;&#20114;&#27010;&#24565;&#65292;&#36825;&#20123;&#27010;&#24565;&#33021;&#22815;&#25551;&#36848;&#36755;&#20837;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#23545;&#27169;&#22411;&#25512;&#29702;&#20998;&#25968;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35777;&#26126;&#35757;&#32451;&#33391;&#22909;&#30340;AI&#27169;&#22411;&#20013;&#20986;&#29616;&#31526;&#21495;&#27010;&#24565;&#30340;&#29616;&#35937;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22914;&#26524;&#65288;1&#65289;&#27169;&#22411;&#36755;&#20986;&#30456;&#23545;&#20110;&#36755;&#20837;&#21464;&#37327;&#30340;&#39640;&#38454;&#23548;&#25968;&#22343;&#20026;&#38646;&#65292;&#65288;2&#65289;AI&#27169;&#22411;&#21487;&#29992;&#20110;&#36974;&#25377;&#26679;&#26412;&#19988;&#36755;&#20837;&#26679;&#26412;&#36739;&#23569;&#36974;&#25377;&#26102;&#20250;&#20135;&#29983;&#26356;&#39640;&#30340;&#32622;&#20449;&#24230;&#65292;&#65288;3&#65289;AI&#27169;&#22411;&#22312;&#36974;&#25377;&#26679;&#26412;&#19978;&#30340;&#32622;&#20449;&#24230;&#24182;&#19981;&#20250;&#26174;&#33879;&#38477;&#20302;&#65292;&#21017;AI&#27169;&#22411;&#23558;&#32534;&#30721;&#31232;&#30095;&#20132;&#20114;&#27010;&#24565;&#12290;&#27599;&#20010;&#20132;&#20114;&#27010;&#24565;&#34920;&#31034;&#29305;&#23450;&#19968;&#32452;&#36755;&#20837;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#23545;&#27169;&#22411;&#25512;&#29702;&#20998;&#25968;&#20135;&#29983;&#19968;&#23450;&#30340;&#25968;&#20540;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#20998;&#25968;&#24635;&#26159;&#21487;&#20197;&#34920;&#31034;&#20026;&#25152;&#26377;&#20132;&#20114;&#27010;&#24565;&#30340;&#20132;&#20114;&#25928;&#24212;&#20043;&#21644;&#12290;&#20107;&#23454;&#19978;&#65292;&#25105;&#20204;&#24076;&#26395;&#35777;&#26126;&#20986;&#29616;&#31526;&#21495;&#27010;&#24565;&#30340;&#26465;&#20214;&#38750;&#24120;&#26222;&#36941;&#12290;&#36825;&#24847;&#21619;&#30528;&#23545;&#20110;&#22823;&#22810;&#25968;AI&#27169;&#22411;&#65292;&#25105;&#20204;&#36890;&#24120;&#21487;&#20197;&#20351;&#29992;&#23569;&#37327;&#30340;&#20132;&#20114;&#27010;&#24565;&#26469;&#27169;&#25311;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper aims to prove the emergence of symbolic concepts in well-trained AI models. We prove that if (1) the high-order derivatives of the model output w.r.t. the input variables are all zero, (2) the AI model can be used on occluded samples and will yield higher confidence when the input sample is less occluded, and (3) the confidence of the AI model does not significantly degrade on occluded samples, then the AI model will encode sparse interactive concepts. Each interactive concept represents an interaction between a specific set of input variables, and has a certain numerical effect on the inference score of the model. Specifically, it is proved that the inference score of the model can always be represented as the sum of the interaction effects of all interactive concepts. In fact, we hope to prove that conditions for the emergence of symbolic concepts are quite common. It means that for most AI models, we can usually use a small number of interactive concepts to mimic the mode
&lt;/p&gt;</description></item></channel></rss>