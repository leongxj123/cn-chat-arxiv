<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28608;&#27963;&#26631;&#24535;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#21387;&#32553;LLM&#30340;&#28608;&#27963;&#29366;&#24577;&#65292;&#20351;&#20854;&#33021;&#22815;&#20197;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#24863;&#30693;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;LLM&#22312;&#30701;&#19978;&#19979;&#25991;&#20013;&#30340;&#21407;&#22987;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#20869;&#23384;&#21644;&#26102;&#38388;&#25928;&#29575;&#65292;&#24182;&#36890;&#36807;&#22810;&#26679;&#21270;&#35757;&#32451;&#26377;&#25928;&#22320;&#25903;&#25345;&#19981;&#21516;&#19978;&#19979;&#25991;&#38271;&#24230;&#12290;</title><link>https://rss.arxiv.org/abs/2401.03462</link><description>&lt;p&gt;
&#20174;4K&#21040;400K&#30340;&#39134;&#36291;&#65306;&#21033;&#29992;&#28608;&#27963;&#26631;&#24535;&#25193;&#23637;LLM&#30340;&#19978;&#19979;&#25991;
&lt;/p&gt;
&lt;p&gt;
Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2401.03462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28608;&#27963;&#26631;&#24535;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#21387;&#32553;LLM&#30340;&#28608;&#27963;&#29366;&#24577;&#65292;&#20351;&#20854;&#33021;&#22815;&#20197;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#24863;&#30693;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;LLM&#22312;&#30701;&#19978;&#19979;&#25991;&#20013;&#30340;&#21407;&#22987;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#20869;&#23384;&#21644;&#26102;&#38388;&#25928;&#29575;&#65292;&#24182;&#36890;&#36807;&#22810;&#26679;&#21270;&#35757;&#32451;&#26377;&#25928;&#22320;&#25903;&#25345;&#19981;&#21516;&#19978;&#19979;&#25991;&#38271;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#19978;&#19979;&#25991;&#30340;&#21033;&#29992;&#23545;&#20110;LLM&#26469;&#35828;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#12290;&#23613;&#31649;&#36890;&#36807;&#24494;&#35843;&#21487;&#20197;&#25193;&#23637;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#20294;&#36825;&#20250;&#23548;&#33268;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#30340;&#26174;&#33879;&#25104;&#26412;&#65292;&#24182;&#23545;LLM&#30340;&#21407;&#22987;&#33021;&#21147;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28608;&#27963;&#26631;&#24535;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#23558;LLM&#30340;&#21407;&#22987;&#28608;&#27963;&#21387;&#32553;&#25104;&#32039;&#20945;&#30340;&#24418;&#24335;&#65292;&#20351;LLM&#33021;&#22815;&#20197;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#24863;&#30693;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#12290;&#28608;&#27963;&#26631;&#24535;&#34987;&#24341;&#20837;&#20026;&#25554;&#20214;&#27169;&#22359;&#65292;&#23436;&#20840;&#20445;&#30041;&#20102;LLM&#22312;&#30701;&#19978;&#19979;&#25991;&#20013;&#30340;&#21407;&#22987;&#33021;&#21147;&#12290;&#23427;&#19982;&#28369;&#21160;&#31383;&#21475;&#19968;&#36215;&#23454;&#26102;&#22788;&#29702;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#20013;&#23454;&#29616;&#20102;&#31454;&#20105;&#21147;&#30340;&#20869;&#23384;&#21644;&#26102;&#38388;&#25928;&#29575;&#12290;&#28608;&#27963;&#26631;&#24535;&#26159;&#36890;&#36807;&#22810;&#26679;&#21270;&#21387;&#32553;&#27604;&#30340;&#30701;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#24471;&#30410;&#20110;&#36825;&#31181;&#22788;&#29702;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#25903;&#25345;&#19981;&#21516;&#19978;&#19979;&#25991;&#38271;&#24230;&#65292;&#23454;&#29616;&#23567;&#35268;&#27169;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
The utilization of long contexts poses a big challenge for LLMs due to their limited context window size. Although the context window can be extended through fine-tuning, it will result in a considerable cost at both training and inference time, and exert an unfavorable impact to the LLM's original capabilities. In this work, we propose a new method called Activation Beacon, which condenses LLM's raw activations into compact forms such that the LLM can perceive a longer context with a limited context window. Activation Beacon is introduced as a plug-in module, which fully preserves the LLM's original capability in short contexts. It works with the sliding window to streamingly process the long context, which leads to a competitive memory and time efficiency in both training and inference. Activation Beacon is trained with short-sequence data of diversified condensing ratios. Thanks to such a treatment, it can be effectively learned to support different context lengths with a small trai
&lt;/p&gt;</description></item><item><title>LS&#26041;&#27861;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#35757;&#32451;&#20013;&#30340;&#26631;&#31614;&#24179;&#28369;&#25928;&#26524;&#34987;&#21457;&#29616;&#20250;&#36127;&#38754;&#24433;&#21709;&#36873;&#25321;&#24615;&#20998;&#31867;&#65292;&#36890;&#36807;&#24433;&#21709;&#27169;&#22411;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#27492;&#30740;&#31350;&#38416;&#26126;&#20102;&#36825;&#19968;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2403.14715</link><description>&lt;p&gt;
&#29702;&#35299;&#20026;&#20309;&#26631;&#31614;&#24179;&#28369;&#20250;&#38477;&#20302;&#36873;&#25321;&#24615;&#20998;&#31867;&#30340;&#25928;&#26524;&#20197;&#21450;&#22914;&#20309;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Understanding Why Label Smoothing Degrades Selective Classification and How to Fix It
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14715
&lt;/p&gt;
&lt;p&gt;
LS&#26041;&#27861;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#35757;&#32451;&#20013;&#30340;&#26631;&#31614;&#24179;&#28369;&#25928;&#26524;&#34987;&#21457;&#29616;&#20250;&#36127;&#38754;&#24433;&#21709;&#36873;&#25321;&#24615;&#20998;&#31867;&#65292;&#36890;&#36807;&#24433;&#21709;&#27169;&#22411;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#27492;&#30740;&#31350;&#38416;&#26126;&#20102;&#36825;&#19968;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#24179;&#28369;&#65288;LS&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#35757;&#32451;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#22312;&#25552;&#39640;&#27979;&#35797;&#20934;&#30830;&#24615;&#26041;&#38754;&#25928;&#26524;&#26174;&#33879;&#65292;&#24182;&#19988;&#23454;&#29616;&#31616;&#21333;&#12290;"&#30828;"&#30340;one-hot&#26631;&#31614;&#36890;&#36807;&#23558;&#27010;&#29575;&#36136;&#37327;&#22343;&#21248;&#20998;&#37197;&#32473;&#20854;&#20182;&#31867;&#21035;&#26469;&#36827;&#34892;"&#24179;&#28369;&#21270;"&#65292;&#20174;&#32780;&#20943;&#23569;&#36807;&#24230;&#25311;&#21512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;LS&#22914;&#20309;&#36127;&#38754;&#24433;&#21709;&#36873;&#25321;&#24615;&#20998;&#31867;&#65288;SC&#65289;- &#20854;&#30446;&#26631;&#26159;&#21033;&#29992;&#27169;&#22411;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#26469;&#25298;&#32477;&#38169;&#35823;&#20998;&#31867;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#21644;&#26550;&#26500;&#20013;&#20174;&#32463;&#39564;&#19978;&#35777;&#26126;LS&#20250;&#23548;&#33268;SC&#30340;&#19968;&#33268;&#24615;&#38477;&#32423;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;logit&#32423;&#21035;&#30340;&#26799;&#24230;&#26469;&#35299;&#37322;&#36825;&#19968;&#28857;&#65292;&#34920;&#26126;LS&#36890;&#36807;&#22312;&#38169;&#35823;&#27010;&#29575;&#20302;&#26102;&#26356;&#21152;&#27491;&#21017;&#21270;&#26368;&#22823;logit&#65292;&#32780;&#22312;&#38169;&#35823;&#27010;&#29575;&#39640;&#26102;&#26356;&#23569;&#27491;&#21017;&#21270;&#65292;&#21152;&#21095;&#20102;&#36807;&#24230;&#33258;&#20449;&#21644;&#20302;&#33258;&#20449;&#12290;&#36825;&#38416;&#26126;&#20102;&#20197;&#21069;&#25253;&#36947;&#30340;&#24378;&#20998;&#31867;&#22120;&#22312;SC&#20013;&#24615;&#33021;&#19981;&#20339;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14715v1 Announce Type: cross  Abstract: Label smoothing (LS) is a popular regularisation method for training deep neural network classifiers due to its effectiveness in improving test accuracy and its simplicity in implementation. "Hard" one-hot labels are "smoothed" by uniformly distributing probability mass to other classes, reducing overfitting. In this work, we reveal that LS negatively affects selective classification (SC) - where the aim is to reject misclassifications using a model's predictive uncertainty. We first demonstrate empirically across a range of tasks and architectures that LS leads to a consistent degradation in SC. We then explain this by analysing logit-level gradients, showing that LS exacerbates overconfidence and underconfidence by regularising the max logit more when the probability of error is low, and less when the probability of error is high. This elucidates previously reported experimental results where strong classifiers underperform in SC. We
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#26041;&#27861;EMCID&#65292;&#29992;&#20110;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#36827;&#34892;&#28023;&#37327;&#27010;&#24565;&#32534;&#36753;&#65292;&#36890;&#36807;&#20869;&#23384;&#20248;&#21270;&#21644;&#22810;&#23618;&#27169;&#22411;&#32534;&#36753;&#26469;&#21516;&#26102;&#22788;&#29702;&#29983;&#25104;&#20869;&#23481;&#20013;&#30340;&#36807;&#26102;&#12289;&#21463;&#29256;&#26435;&#20445;&#25252;&#12289;&#19981;&#27491;&#30830;&#21644;&#26377;&#20559;&#35265;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.13807</link><description>&lt;p&gt;
&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#32534;&#36753;&#28023;&#37327;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Editing Massive Concepts in Text-to-Image Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13807
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#26041;&#27861;EMCID&#65292;&#29992;&#20110;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#36827;&#34892;&#28023;&#37327;&#27010;&#24565;&#32534;&#36753;&#65292;&#36890;&#36807;&#20869;&#23384;&#20248;&#21270;&#21644;&#22810;&#23618;&#27169;&#22411;&#32534;&#36753;&#26469;&#21516;&#26102;&#22788;&#29702;&#29983;&#25104;&#20869;&#23481;&#20013;&#30340;&#36807;&#26102;&#12289;&#21463;&#29256;&#26435;&#20445;&#25252;&#12289;&#19981;&#27491;&#30830;&#21644;&#26377;&#20559;&#35265;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#23384;&#22312;&#29983;&#25104;&#36807;&#26102;&#30340;&#12289;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#12289;&#19981;&#27491;&#30830;&#30340;&#21644;&#26377;&#20559;&#35265;&#30340;&#20869;&#23481;&#30340;&#39118;&#38505;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#26041;&#27861;&#24050;&#22312;&#23567;&#35268;&#27169;&#19978;&#32531;&#35299;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#22312;&#26356;&#22823;&#35268;&#27169;&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#21516;&#26102;&#22788;&#29702;&#23427;&#20204;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#21363;&#32534;&#36753;&#28023;&#37327;&#27010;&#24565;&#25193;&#25955;&#27169;&#22411;&#65288;EMCID&#65289;&#12290;&#31532;&#19968;&#38454;&#27573;&#36890;&#36807;&#25991;&#26412;&#23545;&#40784;&#25439;&#22833;&#21644;&#25193;&#25955;&#22122;&#22768;&#39044;&#27979;&#25439;&#22833;&#23454;&#29616;&#23545;&#27599;&#20010;&#21333;&#29420;&#27010;&#24565;&#30340;&#20869;&#23384;&#20248;&#21270;&#30340;&#21452;&#33258;&#33976;&#39311;&#12290;&#31532;&#20108;&#38454;&#27573;&#36827;&#34892;&#28023;&#37327;&#27010;&#24565;&#32534;&#36753;&#65292;&#37319;&#29992;&#22810;&#23618;&#12289;&#23553;&#38381;&#24418;&#24335;&#30340;&#27169;&#22411;&#32534;&#36753;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ImageNet&#27010;&#24565;&#32534;&#36753;&#22522;&#20934;&#65288;ICEB&#65289;&#30340;&#32508;&#21512;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#38024;&#23545;T2I&#27169;&#22411;&#30340;&#28023;&#37327;&#27010;&#24565;&#32534;&#36753;&#65292;&#21253;&#25324;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;&#33258;&#30001;&#24418;&#24335;&#25552;&#31034;&#21644;&#28023;&#37327;&#27010;&#24565;&#31867;&#21035;&#65292;&#20197;&#21450;&#24191;&#27867;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20934;&#21644;&#20808;&#21069;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#23637;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13807v1 Announce Type: cross  Abstract: Text-to-image diffusion models suffer from the risk of generating outdated, copyrighted, incorrect, and biased content. While previous methods have mitigated the issues on a small scale, it is essential to handle them simultaneously in larger-scale real-world scenarios. We propose a two-stage method, Editing Massive Concepts In Diffusion Models (EMCID). The first stage performs memory optimization for each individual concept with dual self-distillation from text alignment loss and diffusion noise prediction loss. The second stage conducts massive concept editing with multi-layer, closed form model editing. We further propose a comprehensive benchmark, named ImageNet Concept Editing Benchmark (ICEB), for evaluating massive concept editing for T2I models with two subtasks, free-form prompts, massive concept categories, and extensive evaluation metrics. Extensive experiments conducted on our proposed benchmark and previous benchmarks demo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;Segment Anything Model (SAM)&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#31034;&#23398;&#20064;&#23450;&#21046;&#21270;&#23454;&#20363;&#20998;&#21106;&#65292;&#35299;&#20915;&#20102;&#22312;&#24212;&#29992;&#20110;&#23450;&#21046;&#21270;&#23454;&#20363;&#20998;&#21106;&#26102;&#38754;&#20020;&#30340;&#36755;&#20837;&#25552;&#31034;&#27169;&#31946;&#24615;&#21644;&#39069;&#22806;&#35757;&#32451;&#38656;&#27714;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.09199</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#23398;&#20064;&#23450;&#21046;&#21270;&#20998;&#21106;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#23454;&#20363;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Customizing Segmentation Foundation Model via Prompt Learning for Instance Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09199
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;Segment Anything Model (SAM)&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#31034;&#23398;&#20064;&#23450;&#21046;&#21270;&#23454;&#20363;&#20998;&#21106;&#65292;&#35299;&#20915;&#20102;&#22312;&#24212;&#29992;&#20110;&#23450;&#21046;&#21270;&#23454;&#20363;&#20998;&#21106;&#26102;&#38754;&#20020;&#30340;&#36755;&#20837;&#25552;&#31034;&#27169;&#31946;&#24615;&#21644;&#39069;&#22806;&#35757;&#32451;&#38656;&#27714;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#21560;&#24341;&#20102;&#30456;&#24403;&#22810;&#30340;&#20851;&#27880;&#65292;&#24182;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24471;&#21040;&#31215;&#26497;&#25506;&#35752;&#12290;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#65292;Segment Anything Model (SAM)&#22240;&#20854;&#22312;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#28789;&#27963;&#24615;&#32780;&#33073;&#39062;&#32780;&#20986;&#65292;&#36890;&#36807;&#22522;&#20110;&#25552;&#31034;&#30340;&#23545;&#35937;&#25513;&#27169;&#29983;&#25104;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;SAM&#20855;&#26377;&#36825;&#20123;&#20248;&#21183;&#65292;&#22312;&#24212;&#29992;&#20110;&#23450;&#21046;&#21270;&#23454;&#20363;&#20998;&#21106;&#26102;&#65288;&#23545;&#29305;&#23450;&#23545;&#35937;&#25110;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#36890;&#24120;&#19981;&#23384;&#22312;&#30340;&#29420;&#29305;&#29615;&#22659;&#20013;&#36827;&#34892;&#20998;&#21106;&#65289;&#65292;SAM&#38754;&#20020;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65306;1&#65289;&#36755;&#20837;&#25552;&#31034;&#20013;&#22266;&#26377;&#30340;&#27169;&#31946;&#24615;&#65292;2&#65289;&#20026;&#23454;&#29616;&#26368;&#20339;&#20998;&#21106;&#38656;&#35201;&#22823;&#37327;&#39069;&#22806;&#35757;&#32451;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#25552;&#31034;&#23398;&#20064;&#23450;&#21046;&#21270;&#23454;&#20363;&#20998;&#21106;&#65292;&#38024;&#23545;SAM&#36827;&#34892;&#20102;&#23450;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#21547;&#19968;&#20010;&#25552;&#31034;&#23398;&#20064;&#27169;&#22359;&#65288;PLM&#65289;&#65292;&#21487;&#20197;&#35843;&#25972;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09199v1 Announce Type: cross  Abstract: Recently, foundation models trained on massive datasets to adapt to a wide range of domains have attracted considerable attention and are actively being explored within the computer vision community. Among these, the Segment Anything Model (SAM) stands out for its remarkable progress in generalizability and flexibility for image segmentation tasks, achieved through prompt-based object mask generation. However, despite its strength, SAM faces two key limitations when applied to customized instance segmentation that segments specific objects or those in unique environments not typically present in the training data: 1) the ambiguity inherent in input prompts and 2) the necessity for extensive additional training to achieve optimal segmentation. To address these challenges, we propose a novel method, customized instance segmentation via prompt learning tailored to SAM. Our method involves a prompt learning module (PLM), which adjusts inpu
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#21487;&#25511;&#20559;&#22909;&#20248;&#21270;&#65288;CPO&#65289;&#26041;&#27861;&#65292;&#26126;&#30830;&#20026;&#19981;&#21516;&#30446;&#26631;&#25351;&#23450;&#20559;&#22909;&#20998;&#25968;&#65292;&#20174;&#32780;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#31526;&#21512;&#38656;&#27714;&#30340;&#21709;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.19085</link><description>&lt;p&gt;
&#21487;&#25511;&#20559;&#22909;&#20248;&#21270;&#65306;&#26397;&#30528;&#21487;&#25511;&#22810;&#30446;&#26631;&#23545;&#40784;&#26041;&#21521;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19085
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#21487;&#25511;&#20559;&#22909;&#20248;&#21270;&#65288;CPO&#65289;&#26041;&#27861;&#65292;&#26126;&#30830;&#20026;&#19981;&#21516;&#30446;&#26631;&#25351;&#23450;&#20559;&#22909;&#20998;&#25968;&#65292;&#20174;&#32780;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#31526;&#21512;&#38656;&#27714;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#23545;&#40784;&#24037;&#20316;&#26088;&#22312;&#36861;&#27714;&#27169;&#22411;&#21709;&#24212;&#19982;&#20154;&#31867;&#20559;&#22909;&#21644;&#20215;&#20540;&#30340;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#21487;&#25511;&#20559;&#22909;&#20248;&#21270;&#65288;CPO&#65289;&#26041;&#27861;&#65292;&#26126;&#30830;&#20026;&#19981;&#21516;&#30446;&#26631;&#25351;&#23450;&#20559;&#22909;&#20998;&#25968;&#65292;&#20174;&#32780;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#31526;&#21512;&#38656;&#27714;&#30340;&#21709;&#24212;&#12290;&#23454;&#39564;&#20998;&#26512;&#34920;&#26126;&#65292;&#32463;&#36807;&#23545;&#40784;&#30340;&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#31526;&#21512;&#21508;&#31181;&#20559;&#22909;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19085v1 Announce Type: new  Abstract: Alignment in artificial intelligence pursues the consistency between model responses and human preferences as well as values. In practice, the multifaceted nature of human preferences inadvertently introduces what is known as the "alignment tax" -a compromise where enhancements in alignment within one objective (e.g.,harmlessness) can diminish performance in others (e.g.,helpfulness). However, existing alignment techniques are mostly unidirectional, leading to suboptimal trade-offs and poor flexibility over various objectives. To navigate this challenge, we argue the prominence of grounding LLMs with evident preferences. We introduce controllable preference optimization (CPO), which explicitly specifies preference scores for different objectives, thereby guiding the model to generate responses that meet the requirements. Our experimental analysis reveals that the aligned models can provide responses that match various preferences among t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#38899;&#32032;&#34920;&#31034;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#20943;&#32531;&#39640;&#36164;&#28304;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21644;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14279</link><description>&lt;p&gt;
&#20351;&#29992;&#38899;&#32032;&#34920;&#31034;&#20943;&#32531;&#35821;&#35328;&#24046;&#24322;&#65292;&#23454;&#29616;&#31283;&#20581;&#30340;&#22810;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Mitigating the Linguistic Gap with Phonemic Representations for Robust Multilingual Language Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14279
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#38899;&#32032;&#34920;&#31034;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#20943;&#32531;&#39640;&#36164;&#28304;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21644;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25913;&#21892;&#22810;&#35821;&#35328;&#29702;&#35299;&#65292;&#36890;&#24120;&#38656;&#35201;&#22312;&#35757;&#32451;&#38454;&#27573;&#20351;&#29992;&#22810;&#31181;&#35821;&#35328;&#65292;&#20381;&#36182;&#22797;&#26434;&#30340;&#35757;&#32451;&#25216;&#26415;&#65292;&#24182;&#19988;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#25105;&#20204;&#20551;&#35774;&#35821;&#35328;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#21463;&#21040;&#36825;&#20123;&#35821;&#35328;&#20043;&#38388;&#30340;&#35821;&#35328;&#24046;&#24322;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#38899;&#32032;&#34920;&#31034;&#65288;&#20855;&#20307;&#26469;&#35828;&#65292;&#23558;&#38899;&#32032;&#20316;&#20026;&#36755;&#20837;&#26631;&#35760;&#36755;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#32780;&#19981;&#26159;&#23376;&#35789;&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#23454;&#29616;&#31283;&#20581;&#30340;&#22810;&#35821;&#35328;&#24314;&#27169;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#20010;&#36328;&#35821;&#35328;&#20219;&#21153;&#30340;&#23450;&#37327;&#35777;&#25454;&#23637;&#31034;&#20102;&#38899;&#32032;&#34920;&#31034;&#30340;&#26377;&#25928;&#24615;&#65292;&#36825;&#36827;&#19968;&#27493;&#24471;&#21040;&#20102;&#23545;&#36328;&#35821;&#35328;&#24615;&#33021;&#24046;&#36317;&#30340;&#29702;&#35770;&#20998;&#26512;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14279v1 Announce Type: cross  Abstract: Approaches to improving multilingual language understanding often require multiple languages during the training phase, rely on complicated training techniques, and -- importantly -- struggle with significant performance gaps between high-resource and low-resource languages. We hypothesize that the performance gaps between languages are affected by linguistic gaps between those languages and provide a novel solution for robust multilingual language modeling by employing phonemic representations (specifically, using phonemes as input tokens to LMs rather than subwords). We present quantitative evidence from three cross-lingual tasks that demonstrate the effectiveness of phonemic representation, which is further justified by a theoretical analysis of the cross-lingual performance gap.
&lt;/p&gt;</description></item><item><title>&#36870;&#21521;&#36719; Q &#23398;&#20064;&#29992;&#20110;&#33719;&#24471;&#27425;&#20248;&#28436;&#31034;&#30340;&#31163;&#32447;&#27169;&#20223;&#25361;&#25112;&#20102;&#31163;&#32447; IL &#20013;&#26377;&#38480;&#25903;&#25345;&#19987;&#23478;&#28436;&#31034;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#20197;&#21305;&#37197;&#27425;&#20248;&#28436;&#31034;&#38598;&#21512;&#30340;&#21344;&#29992;&#20998;&#24067;</title><link>https://arxiv.org/abs/2402.13147</link><description>&lt;p&gt;
SubIQ: &#36870;&#21521;&#36719; Q &#23398;&#20064;&#29992;&#20110;&#33719;&#24471;&#27425;&#20248;&#28436;&#31034;&#30340;&#31163;&#32447;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
SubIQ: Inverse Soft-Q Learning for Offline Imitation with Suboptimal Demonstrations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13147
&lt;/p&gt;
&lt;p&gt;
&#36870;&#21521;&#36719; Q &#23398;&#20064;&#29992;&#20110;&#33719;&#24471;&#27425;&#20248;&#28436;&#31034;&#30340;&#31163;&#32447;&#27169;&#20223;&#25361;&#25112;&#20102;&#31163;&#32447; IL &#20013;&#26377;&#38480;&#25903;&#25345;&#19987;&#23478;&#28436;&#31034;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#20197;&#21305;&#37197;&#27425;&#20248;&#28436;&#31034;&#38598;&#21512;&#30340;&#21344;&#29992;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#65292;&#26088;&#22312;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#27169;&#20223;&#19987;&#23478;&#30340;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#19982;&#29615;&#22659;&#36827;&#34892;&#36827;&#19968;&#27493;&#20132;&#20114;&#12290;&#22312;&#31163;&#32447; IL &#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#22788;&#29702;&#20165;&#28085;&#30422;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#30340;&#19968;&#23567;&#37096;&#20998;&#30340;&#19987;&#23478;&#28436;&#31034;&#30340;&#26377;&#38480;&#25903;&#25345;&#12290;&#25105;&#20204;&#32771;&#34385;&#31163;&#32447; IL&#65292;&#20854;&#20013;&#19987;&#23478;&#28436;&#31034;&#21463;&#21040;&#38480;&#21046;&#65292;&#20294;&#26159;&#30001;&#26356;&#22823;&#35268;&#27169;&#30340;&#27425;&#20248;&#28436;&#31034;&#38598;&#21512;&#34917;&#20805;&#12290;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#29992;&#20110;&#27492;&#35774;&#32622;&#30340;&#31163;&#32447; IL &#26041;&#27861;&#22522;&#20110;&#34892;&#20026;&#20811;&#38534;&#25110;&#20998;&#24067;&#21305;&#37197;&#65292;&#20854;&#30446;&#30340;&#26159;&#23558;&#27169;&#20223;&#31574;&#30053;&#30340;&#21344;&#29992;&#20998;&#24067;&#19982;&#19987;&#23478;&#31574;&#30053;&#30340;&#21344;&#29992;&#20998;&#24067;&#21305;&#37197;&#12290;&#36825;&#31181;&#26041;&#27861;&#24448;&#24448;&#23384;&#22312;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#22240;&#20026;&#19987;&#23478;&#28436;&#31034;&#26377;&#38480;&#65292;&#26080;&#27861;&#20934;&#30830;&#34920;&#31034;&#20219;&#20309;&#21344;&#29992;&#20998;&#24067;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#30001;&#20110;&#27425;&#20248;&#28436;&#31034;&#38598;&#21512;&#35268;&#27169;&#26356;&#22823;&#65292;&#26377;&#24456;&#39640;&#30340;&#21487;&#33021;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13147v1 Announce Type: cross  Abstract: We consider offline imitation learning (IL), which aims to mimic the expert's behavior from its demonstration without further interaction with the environment. One of the main challenges in offline IL is dealing with the limited support of expert demonstrations that cover only a small fraction of the state-action spaces. In this work, we consider offline IL, where expert demonstrations are limited but complemented by a larger set of sub-optimal demonstrations of lower expertise levels. Most of the existing offline IL methods developed for this setting are based on behavior cloning or distribution matching, where the aim is to match the occupancy distribution of the imitation policy with that of the expert policy. Such an approach often suffers from over-fitting, as expert demonstrations are limited to accurately represent any occupancy distribution. On the other hand, since sub-optimal sets are much larger, there is a high chance that 
&lt;/p&gt;</description></item><item><title>&#23558;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#23454;&#38469;&#21709;&#24212;&#39118;&#26684;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22266;&#26377;&#39118;&#26684;&#30456;&#21305;&#37197;&#33021;&#22815;&#20135;&#29983;&#26356;&#22909;&#30340;&#23398;&#20064;&#32467;&#26524;&#65292;&#24320;&#21457;&#30340;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#31243;&#24230;&#22320;&#35843;&#25972;&#27169;&#22411;&#21709;&#24212;&#26469;&#36991;&#20813;&#36807;&#25311;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.11192</link><description>&lt;p&gt;
&#22914;&#26524;&#20320;&#35762;&#25105;&#30340;&#35821;&#35328;&#65292;&#25105;&#20250;&#26356;&#22909;&#22320;&#23398;&#20064;&#65306;&#20351;&#29992;&#39118;&#26684;&#23545;&#40784;&#21709;&#24212;&#35843;&#25972;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
I Learn Better If You Speak My Language: Enhancing Large Language Model Fine-Tuning with Style-Aligned Response Adjustments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11192
&lt;/p&gt;
&lt;p&gt;
&#23558;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#23454;&#38469;&#21709;&#24212;&#39118;&#26684;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22266;&#26377;&#39118;&#26684;&#30456;&#21305;&#37197;&#33021;&#22815;&#20135;&#29983;&#26356;&#22909;&#30340;&#23398;&#20064;&#32467;&#26524;&#65292;&#24320;&#21457;&#30340;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#31243;&#24230;&#22320;&#35843;&#25972;&#27169;&#22411;&#21709;&#24212;&#26469;&#36991;&#20813;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23567;&#25968;&#25454;&#38598;&#20026;&#29305;&#23450;&#20219;&#21153;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#19968;&#20010;&#26222;&#36941;&#36935;&#21040;&#30340;&#20294;&#22797;&#26434;&#30340;&#25361;&#25112;&#12290;&#22312;&#26377;&#38480;&#30340;&#31034;&#20363;&#19978;&#36807;&#22810;&#25311;&#21512;&#21487;&#33021;&#20250;&#23545;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#20445;&#30041;&#21407;&#22987;&#25216;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#22320;&#23454;&#38469;&#21709;&#24212;&#39118;&#26684;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#23558;&#22320;&#23454;&#38469;&#21709;&#24212;&#39118;&#26684;&#19982;LLM&#22266;&#26377;&#39118;&#26684;&#21305;&#37197;&#20250;&#20135;&#29983;&#26356;&#22909;&#30340;&#23398;&#20064;&#32467;&#26524;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#26368;&#23567;&#31243;&#24230;&#22320;&#20462;&#25913;LLM&#30340;&#29616;&#26377;&#21709;&#24212;&#20197;&#26356;&#27491;&#38169;&#35823;&#65292;&#20351;&#29992;&#36825;&#20123;&#35843;&#25972;&#21518;&#30340;&#21709;&#24212;&#20316;&#20026;&#35757;&#32451;&#30446;&#26631;&#12290;&#36825;&#31181;&#25216;&#26415;&#33021;&#22815;&#23454;&#29616;&#19982;&#27169;&#22411;&#22266;&#26377;&#21709;&#24212;&#39118;&#26684;&#19968;&#33268;&#30340;&#31934;&#30830;&#26356;&#27491;&#65292;&#32500;&#25252;&#27169;&#22411;&#30340;&#26680;&#24515;&#33021;&#21147;&#65292;&#20174;&#32780;&#36991;&#20813;&#36807;&#22810;&#25311;&#21512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;LLM&#30340;&#29305;&#23450;&#20219;&#21153;&#20934;&#30830;&#24615;&#65292;&#32780;&#19988;&#20851;&#38190;&#22320;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11192v1 Announce Type: cross  Abstract: Fine-tuning large language models (LLMs) with a small data set for particular tasks is a widely encountered yet complex challenge. The potential for overfitting on a limited number of examples can negatively impact the model's ability to generalize and retain its original skills. Our research explores the impact of the style of ground-truth responses during the fine-tuning process. We found that matching the ground-truth response style with the LLM's inherent style results in better learning outcomes. Building on this insight, we developed a method that minimally alters the LLM's pre-existing responses to correct errors, using these adjusted responses as training targets. This technique enables precise corrections in line with the model's native response style, safeguarding the model's core capabilities and thus avoid overfitting. Our findings show that this approach not only improves the LLM's task-specific accuracy but also crucially
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;ExpD3 &#21644; BE-TD3&#65292;&#29992;&#20110;&#35299;&#20915;&#21644;&#21033;&#29992; Actor-Critic &#26041;&#27861;&#20013;&#30340;&#20272;&#35745;&#20559;&#24046;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#31639;&#27861;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.09078</link><description>&lt;p&gt;
&#22312; Actor-Critic &#26041;&#27861;&#20013;&#21033;&#29992;&#20272;&#35745;&#20559;&#24046;&#30340;&#28145;&#24230;&#21452; Q-Learning &#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Exploiting Estimation Bias in Deep Double Q-Learning for Actor-Critic Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;ExpD3 &#21644; BE-TD3&#65292;&#29992;&#20110;&#35299;&#20915;&#21644;&#21033;&#29992; Actor-Critic &#26041;&#27861;&#20013;&#30340;&#20272;&#35745;&#20559;&#24046;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#31639;&#27861;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#26159;&#35299;&#20915;&#21644;&#21033;&#29992;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013; Actor-Critic &#26041;&#27861;&#20013;&#30340;&#20272;&#35745;&#20559;&#24046;&#38382;&#39064;&#65292;&#20351;&#29992;&#20102;&#28145;&#24230;&#21452; Q-Learning&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#31639;&#27861;&#65306;Expectile Delayed Deep Deterministic Policy Gradient (ExpD3) &#21644; Bias Exploiting - Twin Delayed Deep Deterministic Policy Gradient (BE-TD3)&#12290;ExpD3 &#26088;&#22312;&#36890;&#36807;&#21333;&#19968;&#30340; Q &#20272;&#35745;&#26469;&#20943;&#23569;&#36807;&#24230;&#20272;&#35745;&#20559;&#24046;&#65292;&#24182;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#24615;&#33021;&#20043;&#38388;&#25552;&#20379;&#24179;&#34913;&#65292;&#32780; BE-TD3 &#21017;&#26088;&#22312;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#36873;&#25321;&#26368;&#26377;&#21033;&#30340;&#20272;&#35745;&#20559;&#24046;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#22312;&#21508;&#31181;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#31639;&#27861;&#22312;&#19982; TD3 &#31561;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#23588;&#20854;&#26159;&#22312;&#20272;&#35745;&#20559;&#24046;&#26174;&#33879;&#24433;&#21709;&#23398;&#20064;&#30340;&#29615;&#22659;&#20013;&#65292;&#21487;&#20197;&#21305;&#25932;&#25110;&#36229;&#36234;&#23427;&#20204;&#12290;&#36825;&#20123;&#32467;&#26524;&#20984;&#26174;&#20102;&#20272;&#35745;&#20559;&#24046;&#23545;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09078v1 Announce Type: cross Abstract: This paper introduces innovative methods in Reinforcement Learning (RL), focusing on addressing and exploiting estimation biases in Actor-Critic methods for continuous control tasks, using Deep Double Q-Learning. We propose two novel algorithms: Expectile Delayed Deep Deterministic Policy Gradient (ExpD3) and Bias Exploiting - Twin Delayed Deep Deterministic Policy Gradient (BE-TD3). ExpD3 aims to reduce overestimation bias with a single $Q$ estimate, offering a balance between computational efficiency and performance, while BE-TD3 is designed to dynamically select the most advantageous estimation bias during training. Our extensive experiments across various continuous control tasks demonstrate the effectiveness of our approaches. We show that these algorithms can either match or surpass existing methods like TD3, particularly in environments where estimation biases significantly impact learning. The results underline the importance of
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#20195;&#29702;&#25968;&#37327;&#25104;&#27604;&#20363;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#37319;&#26679;&#21644;&#25237;&#31080;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#24615;&#33021;&#65292;&#36825;&#31181;&#26041;&#27861;&#19982;&#29616;&#26377;&#30340;&#22797;&#26434;&#26041;&#27861;&#27491;&#20132;&#12290;</title><link>https://arxiv.org/abs/2402.05120</link><description>&lt;p&gt;
&#26356;&#22810;&#30340;&#20195;&#29702;&#23601;&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;
&lt;/p&gt;
&lt;p&gt;
More Agents Is All You Need
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05120
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#20195;&#29702;&#25968;&#37327;&#25104;&#27604;&#20363;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#37319;&#26679;&#21644;&#25237;&#31080;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#24615;&#33021;&#65292;&#36825;&#31181;&#26041;&#27861;&#19982;&#29616;&#26377;&#30340;&#22797;&#26434;&#26041;&#27861;&#27491;&#20132;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#36890;&#36807;&#19968;&#31181;&#37319;&#26679;&#21644;&#25237;&#31080;&#30340;&#26041;&#27861;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(Large Language Models, LLMs)&#30340;&#24615;&#33021;&#19982;&#23454;&#20363;&#21270;&#30340;&#20195;&#29702;&#25968;&#37327;&#25104;&#27604;&#20363;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#26041;&#27861;&#23545;&#24050;&#26377;&#30340;&#22797;&#26434;&#26041;&#27861;&#36827;&#19968;&#27493;&#22686;&#24378;LLMs&#26159;&#27491;&#20132;&#30340;&#65292;&#32780;&#22686;&#24378;&#30340;&#31243;&#24230;&#19982;&#20219;&#21153;&#30340;&#22256;&#38590;&#31243;&#24230;&#30456;&#20851;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#24182;&#30740;&#31350;&#20102;&#33021;&#22815;&#20419;&#36827;&#20854;&#21457;&#29983;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#20844;&#24320;&#22312;&#20197;&#19979;&#32593;&#22336;: \url{https://anonymous.4open.science/r/more_agent_is_all_you_need}
&lt;/p&gt;
&lt;p&gt;
We find that, simply via a sampling-and-voting method, the performance of large language models (LLMs) scales with the number of agents instantiated. Also, this method is orthogonal to existing complicated methods to further enhance LLMs, while the degree of enhancement is correlated to the task difficulty. We conduct comprehensive experiments on a wide range of LLM benchmarks to verify the presence of our finding, and to study the properties that can facilitate its occurrence. Our code is publicly available at: \url{https://anonymous.4open.science/r/more_agent_is_all_you_need}.
&lt;/p&gt;</description></item><item><title>DeLLMa&#26159;&#19968;&#20010;&#26088;&#22312;&#25552;&#39640;&#19981;&#30830;&#23450;&#29615;&#22659;&#19979;&#20915;&#31574;&#31934;&#24230;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#27493;&#39588;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#65292;&#20511;&#37492;&#20915;&#31574;&#29702;&#35770;&#21644;&#25928;&#29992;&#29702;&#35770;&#30340;&#21407;&#21017;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20915;&#31574;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02392</link><description>&lt;p&gt;
DeLLMa:&#19968;&#20010;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19979;&#20915;&#31574;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DeLLMa: A Framework for Decision Making Under Uncertainty with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02392
&lt;/p&gt;
&lt;p&gt;
DeLLMa&#26159;&#19968;&#20010;&#26088;&#22312;&#25552;&#39640;&#19981;&#30830;&#23450;&#29615;&#22659;&#19979;&#20915;&#31574;&#31934;&#24230;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#27493;&#39588;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#65292;&#20511;&#37492;&#20915;&#31574;&#29702;&#35770;&#21644;&#25928;&#29992;&#29702;&#35770;&#30340;&#21407;&#21017;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20915;&#31574;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21830;&#19994;&#12289;&#24037;&#31243;&#21644;&#21307;&#23398;&#31561;&#39046;&#22495;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#36825;&#20123;&#39046;&#22495;&#24448;&#24448;&#38754;&#20020;&#20915;&#31574;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#20851;&#38190;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#22312;&#20915;&#31574;&#38382;&#39064;&#19978;&#30452;&#25509;&#20351;&#29992;LLMs&#24448;&#24448;&#25928;&#26524;&#36739;&#24046;&#65292;&#23588;&#20854;&#26159;&#22312;&#38382;&#39064;&#22797;&#26434;&#24615;&#22686;&#21152;&#26102;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeLLMa&#65288;Decision-making Large Language Model assistant&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#19981;&#30830;&#23450;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#31934;&#24230;&#12290;DeLLMa&#21253;&#25324;&#19968;&#20010;&#22810;&#27493;&#39588;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#65292;&#20511;&#37492;&#20102;&#20915;&#31574;&#29702;&#35770;&#21644;&#25928;&#29992;&#29702;&#35770;&#30340;&#21407;&#21017;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#26368;&#20248;&#30340;&#12289;&#21487;&#23457;&#35745;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#28041;&#21450;&#30495;&#23454;&#20892;&#19994;&#21644;&#37329;&#34701;&#25968;&#25454;&#30340;&#20915;&#31574;&#29615;&#22659;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;DeLLMa&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;LLMs&#30340;&#20915;&#31574;&#24615;&#33021;&#65292;&#20934;&#30830;&#24615;&#21487;&#25552;&#39640;&#39640;&#36798;40%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are increasingly used across society, including in domains like business, engineering, and medicine. These fields often grapple with decision-making under uncertainty, a critical yet challenging task. In this paper, we show that directly prompting LLMs on these types of decision-making problems yields poor results, especially as the problem complexity increases. To overcome this limitation, we propose DeLLMa (Decision-making Large Language Model assistant), a framework designed to enhance decision-making accuracy in uncertain environments. DeLLMa involves a multi-step scaffolding procedure, drawing upon principles from decision theory and utility theory, to provide an optimal and human-auditable decision-making process. We validate our framework on decision-making environments involving real agriculture and finance data. Our results show that DeLLMa can significantly improve LLM decision-making performance, achieving up to a 40% increase in accuracy over co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#35299;&#37322;&#24615;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#35782;&#21035;&#36755;&#20837;&#25968;&#25454;&#30340;&#20851;&#38190;&#37096;&#20998;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07800</link><description>&lt;p&gt;
&#35299;&#37322;&#24615;&#27880;&#24847;&#21147;&#29992;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#21450;&#20854;&#23427;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
Explainable Attention for Few-shot Learning and Beyond. (arXiv:2310.07800v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#35299;&#37322;&#24615;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#35782;&#21035;&#36755;&#20837;&#25968;&#25454;&#30340;&#20851;&#38190;&#37096;&#20998;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#22686;&#24378;&#23398;&#20064;&#27169;&#22411;&#20013;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#28508;&#21147;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#35782;&#21035;&#36755;&#20837;&#25968;&#25454;&#20013;&#26174;&#33879;&#30340;&#37096;&#20998;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36825;&#22312;&#25968;&#25454;&#25910;&#38598;&#21644;&#26631;&#35760;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#65292;&#23548;&#33268;&#35757;&#32451;&#26679;&#26412;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#23588;&#20854;&#26377;&#20215;&#20540;&#12290;&#21463;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#21551;&#21457;&#65292;&#25105;&#20204;&#35748;&#20026;&#65292;&#22914;&#26524;&#23558;AI&#22522;&#32447;&#26292;&#38706;&#20110;&#21407;&#22987;&#25968;&#25454;&#30340;&#20851;&#38190;&#37096;&#20998;&#32780;&#19981;&#26159;&#25972;&#20010;&#36755;&#20837;&#25968;&#25454;&#38598;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#24863;&#30693;&#65292;&#37027;&#20040;&#23427;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#26356;&#20934;&#30830;&#12289;&#26356;&#21487;&#38752;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#36825;&#20123;&#20449;&#24687;&#24615;&#25968;&#25454;&#37096;&#20998;&#30340;&#20219;&#21153;&#65292;&#21363;&#30828;&#27880;&#24847;&#21147;&#23547;&#25214;&#65292;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#22312;&#23569;&#37327;&#35757;&#32451;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#24456;&#38590;&#25214;&#21040;&#36825;&#20123;&#20449;&#24687;&#24615;&#21306;&#22495;&#65292;&#21407;&#22240;&#26159;&#22823;&#37327;&#30340;&#35757;&#32451;&#21442;&#25968;&#26080;&#27861;&#20174;&#26377;&#38480;&#30340;&#26679;&#26412;&#20013;&#26377;&#25928;&#23398;&#20064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#30828;&#27880;&#24847;&#21147;&#23547;&#25214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention mechanisms have exhibited promising potential in enhancing learning models by identifying salient portions of input data. This is particularly valuable in scenarios where limited training samples are accessible due to challenges in data collection and labeling. Drawing inspiration from human recognition processes, we posit that an AI baseline's performance could be more accurate and dependable if it is exposed to essential segments of raw data rather than the entire input dataset, akin to human perception. However, the task of selecting these informative data segments, referred to as hard attention finding, presents a formidable challenge. In situations with few training samples, existing studies struggle to locate such informative regions due to the large number of training parameters that cannot be effectively learned from the available limited samples. In this study, we introduce a novel and practical framework for achieving explainable hard attention finding, specifically
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#20102;&#20174;&#27491;&#26679;&#26412;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#19968;&#20010;&#37325;&#35201;&#35266;&#23519;&#65306;&#36890;&#36807;&#22312;&#27599;&#20010;&#35757;&#32451;&#36845;&#20195;&#20013;&#37325;&#26032;&#37319;&#26679;&#27491;&#26679;&#26412;&#65292;&#21487;&#20197;&#33719;&#24471;&#24378;&#22823;&#30340;&#26089;&#26399;&#24615;&#33021;&#65292;&#24182;&#19988;&#27491;&#31867;&#21644;&#36127;&#31867;&#30340;&#39044;&#27979;&#36235;&#21183;&#26174;&#31034;&#20986;&#19981;&#21516;&#30340;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2310.04078</link><description>&lt;p&gt;
&#36229;&#36234;&#36817;&#35270;&#65306;&#36890;&#36807;&#25972;&#20307;&#39044;&#27979;&#36235;&#21183;&#20174;&#27491;&#26679;&#26412;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Beyond Myopia: Learning from Positive and Unlabeled Data through Holistic Predictive Trends. (arXiv:2310.04078v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#20102;&#20174;&#27491;&#26679;&#26412;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#19968;&#20010;&#37325;&#35201;&#35266;&#23519;&#65306;&#36890;&#36807;&#22312;&#27599;&#20010;&#35757;&#32451;&#36845;&#20195;&#20013;&#37325;&#26032;&#37319;&#26679;&#27491;&#26679;&#26412;&#65292;&#21487;&#20197;&#33719;&#24471;&#24378;&#22823;&#30340;&#26089;&#26399;&#24615;&#33021;&#65292;&#24182;&#19988;&#27491;&#31867;&#21644;&#36127;&#31867;&#30340;&#39044;&#27979;&#36235;&#21183;&#26174;&#31034;&#20986;&#19981;&#21516;&#30340;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#20174;&#27491;&#26679;&#26412;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;(PUL)&#20013;&#23398;&#20064;&#20108;&#20803;&#20998;&#31867;&#22120;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#39564;&#35777;&#36127;&#26679;&#26412;&#22256;&#38590;&#30340;&#24773;&#20917;&#19979;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;PUL&#26041;&#27861;&#22312;&#23454;&#39564;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#36127;&#26631;&#31614;&#65292;&#20173;&#28982;&#23384;&#22312;&#31215;&#32047;&#35823;&#24046;&#21644;&#22686;&#21152;&#20272;&#35745;&#20559;&#24046;&#31561;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;PUL&#20013;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#20294;&#38271;&#26399;&#34987;&#24573;&#35270;&#30340;&#35266;&#23519;&#32467;&#26524;&#65306;\textit{&#22312;&#27599;&#20010;&#35757;&#32451;&#36845;&#20195;&#20013;&#37325;&#26032;&#37319;&#26679;&#27491;&#26679;&#26412;&#20197;&#30830;&#20445;&#27491;&#26679;&#26412;&#21644;&#26080;&#26631;&#31614;&#26679;&#26412;&#20043;&#38388;&#30340;&#24179;&#34913;&#20998;&#24067;&#20250;&#23548;&#33268;&#24378;&#30340;&#26089;&#26399;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#27491;&#31867;&#21644;&#36127;&#31867;&#30340;&#39044;&#27979;&#36235;&#21183;&#26174;&#31034;&#20986;&#19981;&#21516;&#30340;&#27169;&#24335;&#12290;}&#20855;&#20307;&#32780;&#35328;&#65292;&#26080;&#26631;&#31614;&#36127;&#26679;&#26412;&#30340;&#24471;&#20998;(&#36755;&#20986;&#27010;&#29575;)&#25345;&#32493;&#19979;&#38477;&#65292;&#32780;&#26080;&#26631;&#31614;&#27491;&#26679;&#26412;&#30340;&#24471;&#20998;&#26174;&#31034;&#20986;&#22823;&#33268;&#28151;&#20081;&#30340;&#36235;&#21183;&#12290;&#25105;&#20204;&#21019;&#26032;&#22320;&#37319;&#29992;&#20102;&#19968;&#31181;&#25972;&#20307;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#22312;&#20010;&#21035;&#26102;&#38388;&#27573;&#20869;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning binary classifiers from positive and unlabeled data (PUL) is vital in many real-world applications, especially when verifying negative examples is difficult. Despite the impressive empirical performance of recent PUL methods, challenges like accumulated errors and increased estimation bias persist due to the absence of negative labels. In this paper, we unveil an intriguing yet long-overlooked observation in PUL: \textit{resampling the positive data in each training iteration to ensure a balanced distribution between positive and unlabeled examples results in strong early-stage performance. Furthermore, predictive trends for positive and negative classes display distinctly different patterns.} Specifically, the scores (output probability) of unlabeled negative examples consistently decrease, while those of unlabeled positive examples show largely chaotic trends. Instead of focusing on classification within individual time frames, we innovatively adopt a holistic approach, inte
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20855;&#36523;&#22522;&#30784;&#20808;&#39564;&#30340;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#26469;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.02635</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22522;&#30784;&#65306;&#26397;&#21521;&#20855;&#26377;&#22522;&#30784;&#20808;&#39564;&#36741;&#21161;&#30340;&#20855;&#36523;&#36890;&#29992;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Foundation Reinforcement Learning: towards Embodied Generalist Agents with Foundation Prior Assistance. (arXiv:2310.02635v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20855;&#36523;&#22522;&#30784;&#20808;&#39564;&#30340;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#26469;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#20204;&#24050;&#32463;&#34920;&#26126;&#65292;&#20174;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#25968;&#25454;&#20013;&#36827;&#34892;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#26159;&#26500;&#24314;&#36890;&#29992;&#27169;&#22411;&#30340;&#20851;&#38190;&#65292;&#27491;&#22914;&#22312;NLP&#20013;&#25152;&#35265;&#12290;&#20026;&#20102;&#26500;&#24314;&#20855;&#36523;&#36890;&#29992;&#26234;&#33021;&#20307;&#65292;&#25105;&#20204;&#21644;&#35768;&#22810;&#20854;&#20182;&#30740;&#31350;&#32773;&#20551;&#35774;&#36825;&#31181;&#22522;&#30784;&#20808;&#39564;&#20063;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#22914;&#20309;&#20197;&#36866;&#24403;&#30340;&#20855;&#20307;&#24418;&#24335;&#34920;&#31034;&#36825;&#20123;&#20855;&#36523;&#22522;&#30784;&#20808;&#39564;&#65292;&#20197;&#21450;&#23427;&#20204;&#24212;&#35813;&#22914;&#20309;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#30452;&#35266;&#26377;&#25928;&#30340;&#20855;&#36523;&#20808;&#39564;&#65292;&#21253;&#25324;&#22522;&#30784;&#31574;&#30053;&#12289;&#20215;&#20540;&#21644;&#25104;&#21151;&#22870;&#21169;&#12290;&#25152;&#25552;&#20986;&#30340;&#20808;&#39564;&#26159;&#22522;&#20110;&#30446;&#26631;&#26465;&#20214;&#30340;MDP&#12290;&#20026;&#20102;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23454;&#20363;&#21270;&#20102;&#19968;&#20010;&#30001;&#36825;&#20123;&#20808;&#39564;&#36741;&#21161;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#65292;&#31216;&#20043;&#20026;&#22522;&#30784;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;FAC&#65289;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#21629;&#21517;&#20026;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#65288;FRL&#65289;&#65292;&#22240;&#20026;&#23427;&#23436;&#20840;&#20381;&#36182;&#20110;&#20855;&#36523;&#22522;&#30784;&#20808;&#39564;&#26469;&#36827;&#34892;&#25506;&#32034;&#12289;&#23398;&#20064;&#21644;&#24378;&#21270;&#12290;FRL&#30340;&#22909;&#22788;&#26377;&#19977;&#20010;&#12290;(1)&#26679;&#26412;&#25928;&#29575;&#39640;&#12290;&#36890;&#36807;&#22522;&#30784;&#20808;&#39564;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#65292;&#20943;&#23569;&#26679;&#26412;&#20351;&#29992;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, people have shown that large-scale pre-training from internet-scale data is the key to building generalist models, as witnessed in NLP. To build embodied generalist agents, we and many other researchers hypothesize that such foundation prior is also an indispensable component. However, it is unclear what is the proper concrete form to represent those embodied foundation priors and how they should be used in the downstream task. In this paper, we propose an intuitive and effective set of embodied priors that consist of foundation policy, value, and success reward. The proposed priors are based on the goal-conditioned MDP. To verify their effectiveness, we instantiate an actor-critic method assisted by the priors, called Foundation Actor-Critic (FAC). We name our framework as Foundation Reinforcement Learning (FRL), since it completely relies on embodied foundation priors to explore, learn and reinforce. The benefits of FRL are threefold. (1) Sample efficient. With foundation p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CDAN&#30340;&#21367;&#31215;&#31264;&#23494;&#27880;&#24847;&#21147;&#24341;&#23548;&#32593;&#32476;&#65292;&#29992;&#20110;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#12290;&#35813;&#32593;&#32476;&#32467;&#21512;&#20102;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#12289;&#21367;&#31215;&#21644;&#31264;&#23494;&#22359;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#36339;&#36291;&#36830;&#25509;&#65292;&#36890;&#36807;&#19987;&#38376;&#30340;&#21518;&#22788;&#29702;&#38454;&#27573;&#36827;&#19968;&#27493;&#25913;&#21892;&#33394;&#24425;&#24179;&#34913;&#21644;&#23545;&#27604;&#24230;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.12902</link><description>&lt;p&gt;
CDAN: &#29992;&#20110;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#30340;&#21367;&#31215;&#31264;&#23494;&#27880;&#24847;&#21147;&#24341;&#23548;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
CDAN: Convolutional Dense Attention-guided Network for Low-light Image Enhancement. (arXiv:2308.12902v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CDAN&#30340;&#21367;&#31215;&#31264;&#23494;&#27880;&#24847;&#21147;&#24341;&#23548;&#32593;&#32476;&#65292;&#29992;&#20110;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#12290;&#35813;&#32593;&#32476;&#32467;&#21512;&#20102;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#12289;&#21367;&#31215;&#21644;&#31264;&#23494;&#22359;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#36339;&#36291;&#36830;&#25509;&#65292;&#36890;&#36807;&#19987;&#38376;&#30340;&#21518;&#22788;&#29702;&#38454;&#27573;&#36827;&#19968;&#27493;&#25913;&#21892;&#33394;&#24425;&#24179;&#34913;&#21644;&#23545;&#27604;&#24230;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#20809;&#22270;&#20687;&#20197;&#19981;&#36275;&#30340;&#29031;&#26126;&#20026;&#29305;&#24449;&#65292;&#38754;&#20020;&#28165;&#26224;&#24230;&#20943;&#24369;&#12289;&#39068;&#33394;&#26263;&#28129;&#21644;&#32454;&#33410;&#20943;&#23569;&#30340;&#25361;&#25112;&#12290;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#25913;&#21892;&#20142;&#24230;&#12289;&#23545;&#27604;&#24230;&#21644;&#25972;&#20307;&#24863;&#30693;&#36136;&#37327;&#26469;&#32416;&#27491;&#36825;&#20123;&#38382;&#39064;&#65292;&#20174;&#32780;&#20419;&#36827;&#20934;&#30830;&#30340;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65306;&#21367;&#31215;&#31264;&#23494;&#27880;&#24847;&#21147;&#24341;&#23548;&#32593;&#32476;&#65288;CDAN&#65289;&#65292;&#29992;&#20110;&#22686;&#24378;&#20302;&#20809;&#22270;&#20687;&#12290;CDAN&#23558;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#19982;&#21367;&#31215;&#21644;&#31264;&#23494;&#22359;&#30456;&#32467;&#21512;&#65292;&#37197;&#21512;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#36339;&#36291;&#36830;&#25509;&#12290;&#35813;&#26550;&#26500;&#30830;&#20445;&#20102;&#26377;&#25928;&#30340;&#20449;&#24687;&#20256;&#36882;&#21644;&#29305;&#24449;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#19987;&#38376;&#30340;&#21518;&#22788;&#29702;&#38454;&#27573;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;&#33394;&#24425;&#24179;&#34913;&#21644;&#23545;&#27604;&#24230;&#12290;&#19982;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#39046;&#22495;&#30340;&#26368;&#26032;&#25104;&#26524;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-light images, characterized by inadequate illumination, pose challenges of diminished clarity, muted colors, and reduced details. Low-light image enhancement, an essential task in computer vision, aims to rectify these issues by improving brightness, contrast, and overall perceptual quality, thereby facilitating accurate analysis and interpretation. This paper introduces the Convolutional Dense Attention-guided Network (CDAN), a novel solution for enhancing low-light images. CDAN integrates an autoencoder-based architecture with convolutional and dense blocks, complemented by an attention mechanism and skip connections. This architecture ensures efficient information propagation and feature learning. Furthermore, a dedicated post-processing phase refines color balance and contrast. Our approach demonstrates notable progress compared to state-of-the-art results in low-light image enhancement, showcasing its robustness across a wide range of challenging scenarios. Our model performs 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25991;&#26412;&#12289;&#23383;&#20307;&#29305;&#24449;&#21644;PDF&#30340;&#20301;&#22270;&#22270;&#20687;&#28210;&#26579;&#36827;&#34892;&#34701;&#21512;&#20998;&#31867;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#23450;&#29702;&#21644;&#35777;&#26126;&#30340;&#30446;&#26631;&#12290;&#22312;&#25991;&#26412;&#27169;&#24577;&#26041;&#38754;&#65292;&#36890;&#36807;&#22312;11 GB&#30340;&#31185;&#23398;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#19968;&#20010;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24471;&#21040;&#20102;&#19982;&#22312;160 GB&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#20284;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#23569;&#30340;&#24494;&#35843;&#25968;&#25454;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2307.09047</link><description>&lt;p&gt;
&#29992;&#20110;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#23450;&#29702;&#21644;&#35777;&#26126;&#30340;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multimodal Machine Learning for Extraction of Theorems and Proofs in the Scientific Literature. (arXiv:2307.09047v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09047
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25991;&#26412;&#12289;&#23383;&#20307;&#29305;&#24449;&#21644;PDF&#30340;&#20301;&#22270;&#22270;&#20687;&#28210;&#26579;&#36827;&#34892;&#34701;&#21512;&#20998;&#31867;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#23450;&#29702;&#21644;&#35777;&#26126;&#30340;&#30446;&#26631;&#12290;&#22312;&#25991;&#26412;&#27169;&#24577;&#26041;&#38754;&#65292;&#36890;&#36807;&#22312;11 GB&#30340;&#31185;&#23398;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#19968;&#20010;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24471;&#21040;&#20102;&#19982;&#22312;160 GB&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#20284;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#23569;&#30340;&#24494;&#35843;&#25968;&#25454;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#39046;&#22495;&#30340;&#23398;&#26415;&#25991;&#31456;&#20013;&#21253;&#21547;&#23450;&#29702;&#12289;&#21629;&#39064;&#31561;&#25968;&#23398;&#38472;&#36848;&#21450;&#20854;&#35777;&#26126;&#12290;&#20174;&#25991;&#31456;&#30340;PDF&#34920;&#31034;&#20013;&#25552;&#21462;&#23427;&#20204;&#38656;&#35201;&#29702;&#35299;&#31185;&#23398;&#25991;&#26412;&#20197;&#21450;&#35270;&#35273;&#21644;&#22522;&#20110;&#23383;&#20307;&#30340;&#25351;&#31034;&#31526;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#20316;&#20026;&#19968;&#31181;&#22810;&#27169;&#24577;&#20998;&#31867;&#38382;&#39064;&#65292;&#20351;&#29992;&#25991;&#26412;&#12289;&#23383;&#20307;&#29305;&#24449;&#21644;PDF&#30340;&#20301;&#22270;&#22270;&#20687;&#28210;&#26579;&#20316;&#20026;&#19981;&#21516;&#30340;&#27169;&#24577;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#21462;&#31867;&#23450;&#29702;&#29615;&#22659;&#21644;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#36890;&#36807;&#21333;&#19968;&#21333;&#27169;&#24577;&#20998;&#31867;&#22120;&#25552;&#21462;&#30340;&#29305;&#24449;&#30340;&#21518;&#26399;&#34701;&#21512;&#65292;&#32771;&#34385;&#25991;&#26723;&#20013;&#22359;&#30340;&#39034;&#24207;&#12290;&#23545;&#20110;&#25991;&#26412;&#27169;&#24577;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;11 GB&#30340;&#31185;&#23398;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#20102;&#19968;&#20010;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#65307;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#20219;&#21153;&#19982;&#19968;&#20010;&#22312;160 GB&#19978;&#39044;&#35757;&#32451;&#30340;&#65288;RoBERTa&#65289;&#27169;&#22411;&#30456;&#27604;&#25317;&#26377;&#31867;&#20284;&#30340;&#24615;&#33021;&#65292;&#22312;&#35201;&#27714;&#26356;&#23569;&#30340;&#24494;&#35843;&#25968;&#25454;&#30340;&#21516;&#26102;&#25910;&#25947;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scholarly articles in mathematical fields feature mathematical statements such as theorems, propositions, etc., as well as their proofs. Extracting them from the PDF representation of the articles requires understanding of scientific text along with visual and font-based indicators. We pose this problem as a multimodal classification problem using text, font features, and bitmap image rendering of the PDF as different modalities. In this paper we propose a multimodal machine learning approach for extraction of theorem-like environments and proofs, based on late fusion of features extracted by individual unimodal classifiers, taking into account the sequential succession of blocks in the document. For the text modality, we pretrain a new language model on a 11 GB scientific corpus; experiments shows similar performance for our task than a model (RoBERTa) pretrained on 160 GB, with faster convergence while requiring much less fine-tuning data. Font-based information relies on training a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#30340;&#34987;&#24573;&#35270;&#30340;&#25928;&#29575;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#26469;&#25506;&#35752;16&#20301;&#21644;32&#20301;&#27169;&#22411;&#30340;&#24046;&#24322;&#65292;&#24182;&#21487;&#20197;&#23450;&#37327;&#35299;&#37322;16&#20301;&#27169;&#22411;&#19982;&#20854;32&#20301;&#23545;&#24212;&#29289;&#20043;&#38388;&#30340;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2305.10947</link><description>&lt;p&gt;
&#20851;&#20110;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#30340;&#36777;&#25252;
&lt;/p&gt;
&lt;p&gt;
In Defense of Pure 16-bit Floating-Point Neural Networks. (arXiv:2305.10947v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#30340;&#34987;&#24573;&#35270;&#30340;&#25928;&#29575;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#26469;&#25506;&#35752;16&#20301;&#21644;32&#20301;&#27169;&#22411;&#30340;&#24046;&#24322;&#65292;&#24182;&#21487;&#20197;&#23450;&#37327;&#35299;&#37322;16&#20301;&#27169;&#22411;&#19982;&#20854;32&#20301;&#23545;&#24212;&#29289;&#20043;&#38388;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#23569;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#21644;&#28608;&#27963;&#25152;&#38656;&#30340;&#20301;&#25968;&#26159;&#38750;&#24120;&#21487;&#21462;&#30340;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#21152;&#24555;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#12290;&#22240;&#27492;&#65292;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20197;&#24320;&#21457;&#21033;&#29992;&#26356;&#20302;&#31934;&#24230;&#35745;&#31639;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#27604;&#22914;&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#30446;&#21069;&#19981;&#23384;&#22312;&#32431;16&#20301;&#28014;&#28857;&#35774;&#32622;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#34987;&#24573;&#35270;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#26469;&#25506;&#35752;&#36896;&#25104;16&#20301;&#21644;32&#20301;&#27169;&#22411;&#30340;&#24046;&#24322;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#35268;&#33539;&#21270;&#20102;&#28014;&#28857;&#35823;&#24046;&#21644;&#23481;&#24525;&#24230;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#21487;&#20197;&#23450;&#37327;&#35299;&#37322;16&#20301;&#27169;&#22411;&#19982;&#20854;32&#20301;&#23545;&#24212;&#29289;&#20043;&#38388;&#23494;&#20999;&#36924;&#36817;&#32467;&#26524;&#30340;&#26465;&#20214;&#12290;&#36825;&#31181;&#29702;&#35770;&#25506;&#32034;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reducing the number of bits needed to encode the weights and activations of neural networks is highly desirable as it speeds up their training and inference time while reducing memory consumption. For these reasons, research in this area has attracted significant attention toward developing neural networks that leverage lower-precision computing, such as mixed-precision training. Interestingly, none of the existing approaches has investigated pure 16-bit floating-point settings. In this paper, we shed light on the overlooked efficiency of pure 16-bit floating-point neural networks. As such, we provide a comprehensive theoretical analysis to investigate the factors contributing to the differences observed between 16-bit and 32-bit models. We formalize the concepts of floating-point error and tolerance, enabling us to quantitatively explain the conditions under which a 16-bit model can closely approximate the results of its 32-bit counterpart. This theoretical exploration offers perspect
&lt;/p&gt;</description></item><item><title>PI-FL&#26159;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;&#22522;&#20110;&#20196;&#29260;&#30340;&#28608;&#21169;&#26426;&#21046;&#22870;&#21169;&#20010;&#24615;&#21270;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#23562;&#37325;&#23458;&#25143;&#31471;&#38544;&#31169;&#30340;&#21516;&#26102;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.07514</link><description>&lt;p&gt;
PI-FL&#65306;&#20010;&#24615;&#21270;&#21644;&#28608;&#21169;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PI-FL: Personalized and Incentivized Federated Learning. (arXiv:2304.07514v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07514
&lt;/p&gt;
&lt;p&gt;
PI-FL&#26159;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;&#22522;&#20110;&#20196;&#29260;&#30340;&#28608;&#21169;&#26426;&#21046;&#22870;&#21169;&#20010;&#24615;&#21270;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#23562;&#37325;&#23458;&#25143;&#31471;&#38544;&#31169;&#30340;&#21516;&#26102;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24212;&#23545;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#25361;&#25112;&#12290;&#20027;&#35201;&#38382;&#39064;&#26159;&#32771;&#34385;&#26469;&#33258;&#23458;&#25143;&#31471;&#30340;&#20010;&#24615;&#21270;&#36807;&#31243;&#20197;&#20445;&#25252;&#20854;&#33258;&#27835;&#26435;&#12290;&#20801;&#35768;&#23458;&#25143;&#31471;&#21442;&#19982;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20915;&#31574;&#21464;&#24471;&#37325;&#35201;&#65292;&#22240;&#20026;&#23384;&#22312;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#23458;&#25143;&#31471;&#21487;&#33021;&#26080;&#27861;&#33258;&#30001;&#20849;&#20139;&#29983;&#25104;&#33391;&#22909;&#36136;&#37327;&#20010;&#24615;&#21270;&#27169;&#22411;&#25152;&#24517;&#38656;&#30340;&#31169;&#20154;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#20855;&#26377;&#39640;&#36136;&#37327;&#25968;&#25454;&#21644;&#36164;&#28304;&#30340;&#23458;&#25143;&#31471;&#19981;&#24895;&#24847;&#22312;&#27809;&#26377;&#21512;&#29702;&#28608;&#21169;&#30340;&#24773;&#20917;&#19979;&#21442;&#19982;&#32852;&#37030;&#23398;&#20064;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PI-FL&#65292;&#36825;&#26159;&#19968;&#20010;&#19968;&#27425;&#24615;&#20010;&#24615;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#37197;&#21512;&#19968;&#20010;&#22522;&#20110;&#20196;&#29260;&#30340;&#28608;&#21169;&#26426;&#21046;&#65292;&#22870;&#21169;&#20010;&#24615;&#21270;&#35757;&#32451;&#12290;PI-FL&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#23562;&#37325;&#23458;&#25143;&#31471;&#38544;&#31169;&#30340;&#21516;&#26102;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized FL has been widely used to cater to heterogeneity challenges with non-IID data. A primary obstacle is considering the personalization process from the client's perspective to preserve their autonomy. Allowing the clients to participate in personalized FL decisions becomes significant due to privacy and security concerns, where the clients may not be at liberty to share private information necessary for producing good quality personalized models. Moreover, clients with high-quality data and resources are reluctant to participate in the FL process without reasonable incentive. In this paper, we propose PI-FL, a one-shot personalization solution complemented by a token-based incentive mechanism that rewards personalized training. PI-FL outperforms other state-of-the-art approaches and can generate good-quality personalized models while respecting clients' privacy.
&lt;/p&gt;</description></item></channel></rss>