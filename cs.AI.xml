<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#24341;&#21457;&#30340;&#20889;&#20316;&#25152;&#26377;&#26435;&#24863;&#21644;&#20316;&#32773;&#36523;&#20221;&#35748;&#30693;&#20043;&#38388;&#30340;&#24515;&#29702;&#22256;&#22659;&#12290;</title><link>https://arxiv.org/abs/2404.00027</link><description>&lt;p&gt;
LLM&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#65306;&#25506;&#35752;&#25152;&#26377;&#26435;&#24863;&#21644;&#25512;&#29702;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership and Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00027
&lt;/p&gt;
&lt;p&gt;
&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#24341;&#21457;&#30340;&#20889;&#20316;&#25152;&#26377;&#26435;&#24863;&#21644;&#20316;&#32773;&#36523;&#20221;&#35748;&#30693;&#20043;&#38388;&#30340;&#24515;&#29702;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20889;&#20316;&#20013;&#30340;&#25152;&#26377;&#26435;&#24863;&#38480;&#21046;&#20102;&#25105;&#20204;&#23545;&#24605;&#24819;&#12289;&#26102;&#38388;&#21644;&#36129;&#29486;&#30340;&#25237;&#20837;&#65292;&#23548;&#33268;&#23545;&#20135;&#20986;&#29289;&#30340;&#20381;&#24651;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20889;&#20316;&#21161;&#25163;&#24341;&#20837;&#20102;&#19968;&#31181;&#24515;&#29702;&#22256;&#22659;&#65292;&#22240;&#20026;&#19968;&#20123;&#20869;&#23481;&#24182;&#38750;&#30452;&#25509;&#25105;&#20204;&#30340;&#21019;&#20316;&#12290;&#25105;&#20204;&#24448;&#24448;&#26356;&#20542;&#21521;&#20110;&#22312;&#21019;&#36896;&#24615;&#20219;&#21153;&#20013;&#26356;&#22810;&#22320;&#24402;&#21151;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#23613;&#31649;&#23427;&#20204;&#23545;&#25152;&#26377;&#20219;&#21153;&#37117;&#26159;&#24179;&#31561;&#30340;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#25105;&#20204;&#21487;&#33021;&#19981;&#20250;&#23436;&#20840;&#22768;&#31216;&#23545;&#30001;LLM&#29983;&#25104;&#30340;&#20869;&#23481;&#25317;&#26377;&#25152;&#26377;&#26435;&#65292;&#20294;&#21364;&#33258;&#30001;&#22320;&#22768;&#31216;&#20316;&#32773;&#36523;&#20221;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#31616;&#30701;&#35843;&#26597;&#26469;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#20102;&#35299;&#28508;&#22312;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#20154;&#26426;&#20132;&#20114;&#22312;&#20889;&#20316;&#20013;&#30340;&#24212;&#29992;&#24182;&#25913;&#36827;&#20889;&#20316;&#36741;&#21161;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00027v1 Announce Type: cross  Abstract: Sense of ownership in writing confines our investment of thoughts, time, and contribution, leading to attachment to the output. However, using writing assistants introduces a mental dilemma, as some content isn't directly our creation. For instance, we tend to credit Large Language Models (LLMs) more in creative tasks, even though all tasks are equal for them. Additionally, while we may not claim complete ownership of LLM-generated content, we freely claim authorship. We conduct a short survey to examine these issues and understand underlying cognitive processes in order to gain a better knowledge of human-computer interaction in writing and improve writing aid systems.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#20204;&#26085;&#30410;&#20381;&#36182;&#30340;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#23545;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#21487;&#33021;&#36896;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#26088;&#22312;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#25552;&#21319;&#20889;&#20316;&#21161;&#25163;&#30340;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00026</link><description>&lt;p&gt;
&#22696;&#27700;&#19982;&#20010;&#24615;&#65306;&#22312;LLMs&#26102;&#20195;&#22609;&#36896;&#20010;&#24615;&#21270;&#21465;&#20107;
&lt;/p&gt;
&lt;p&gt;
Ink and Individuality: Crafting a Personalised Narrative in the Age of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00026
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#20204;&#26085;&#30410;&#20381;&#36182;&#30340;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#23545;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#21487;&#33021;&#36896;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#26088;&#22312;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#25552;&#21319;&#20889;&#20316;&#21161;&#25163;&#30340;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21644;&#20010;&#24615;&#21270;&#26500;&#25104;&#20102;&#20351;&#27599;&#20010;&#20316;&#23478;&#29420;&#29305;&#24182;&#24433;&#21709;&#20854;&#25991;&#23383;&#20197;&#26377;&#25928;&#21560;&#24341;&#35835;&#32773;&#21516;&#26102;&#20256;&#36798;&#30495;&#23454;&#24615;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#26085;&#30410;&#20381;&#36182;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#21487;&#33021;&#20250;&#21361;&#21450;&#25105;&#20204;&#30340;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#12290;&#25105;&#20204;&#32463;&#24120;&#24573;&#35270;&#36825;&#19968;&#36235;&#21183;&#23545;&#25105;&#20204;&#30340;&#21019;&#36896;&#21147;&#21644;&#29420;&#29305;&#24615;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#23613;&#31649;&#21487;&#33021;&#20250;&#36896;&#25104;&#21518;&#26524;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#36827;&#34892;&#31616;&#35201;&#35843;&#26597;&#25506;&#32034;&#19981;&#21516;&#30340;&#35266;&#28857;&#21644;&#27010;&#24565;&#65292;&#20197;&#21450;&#23581;&#35797;&#29702;&#35299;&#20154;&#20204;&#30340;&#35266;&#28857;&#65292;&#32467;&#21512;&#20197;&#24448;&#22312;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#26469;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#12290;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#23545;&#20110;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#22686;&#24378;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#20889;&#20316;&#21161;&#25163;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00026v1 Announce Type: cross  Abstract: Individuality and personalization comprise the distinctive characteristics that make each writer unique and influence their words in order to effectively engage readers while conveying authenticity. However, our growing reliance on LLM-based writing assistants risks compromising our creativity and individuality over time. We often overlook the negative impacts of this trend on our creativity and uniqueness, despite the possible consequences. This study investigates these concerns by performing a brief survey to explore different perspectives and concepts, as well as trying to understand people's viewpoints, in conjunction with past studies in the area. Addressing these issues is essential for improving human-computer interaction systems and enhancing writing assistants for personalization and individuality.
&lt;/p&gt;</description></item><item><title>RiEMann&#25552;&#20986;&#20102;&#19968;&#20010;&#36817;&#23454;&#26102;SE(3)&#31561;&#21464;&#26426;&#22120;&#20154;&#25805;&#20316;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#26080;&#38656;&#28857;&#20113;&#20998;&#21106;&#65292;&#21487;&#20197;&#20174;&#38646;&#24320;&#22987;&#23398;&#20064;&#25805;&#20316;&#20219;&#21153;&#65292;&#27867;&#21270;&#21040;&#30475;&#19981;&#35265;&#30340;&#36716;&#25442;&#21644;&#30446;&#26631;&#23545;&#35937;&#23454;&#20363;&#65292;&#23545;&#25239;&#35270;&#35273;&#24178;&#25200;&#65292;&#23454;&#26102;&#36319;&#36394;&#30446;&#26631;&#23545;&#35937;&#30340;&#23039;&#21183;&#21464;&#21270;&#65292;&#21516;&#26102;&#20855;&#26377;&#21487;&#25193;&#23637;&#30340;&#21160;&#20316;&#31354;&#38388;&#20351;&#24471;&#20851;&#33410;&#23545;&#35937;&#25805;&#20316;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.19460</link><description>&lt;p&gt;
RiEMann: &#19981;&#38656;&#35201;&#28857;&#20113;&#20998;&#21106;&#30340;&#36817;&#23454;&#26102; SE(3)&#31561;&#21464;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
RiEMann: Near Real-Time SE(3)-Equivariant Robot Manipulation without Point Cloud Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19460
&lt;/p&gt;
&lt;p&gt;
RiEMann&#25552;&#20986;&#20102;&#19968;&#20010;&#36817;&#23454;&#26102;SE(3)&#31561;&#21464;&#26426;&#22120;&#20154;&#25805;&#20316;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#26080;&#38656;&#28857;&#20113;&#20998;&#21106;&#65292;&#21487;&#20197;&#20174;&#38646;&#24320;&#22987;&#23398;&#20064;&#25805;&#20316;&#20219;&#21153;&#65292;&#27867;&#21270;&#21040;&#30475;&#19981;&#35265;&#30340;&#36716;&#25442;&#21644;&#30446;&#26631;&#23545;&#35937;&#23454;&#20363;&#65292;&#23545;&#25239;&#35270;&#35273;&#24178;&#25200;&#65292;&#23454;&#26102;&#36319;&#36394;&#30446;&#26631;&#23545;&#35937;&#30340;&#23039;&#21183;&#21464;&#21270;&#65292;&#21516;&#26102;&#20855;&#26377;&#21487;&#25193;&#23637;&#30340;&#21160;&#20316;&#31354;&#38388;&#20351;&#24471;&#20851;&#33410;&#23545;&#35937;&#25805;&#20316;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;RiEMann&#65292;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#36817;&#23454;&#26102; SE(3)&#31561;&#21464;&#26426;&#22120;&#20154;&#25805;&#20316;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#20174;&#22330;&#26223;&#28857;&#20113;&#36755;&#20837;&#20013;&#23398;&#20064;&#12290;&#19982;&#20808;&#21069;&#20381;&#36182;&#25551;&#36848;&#31526;&#21305;&#37197;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;RiEMann&#30452;&#25509;&#39044;&#27979;&#23545;&#35937;&#30340;&#30446;&#26631;&#23039;&#21183;&#36827;&#34892;&#25805;&#20316;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#23545;&#35937;&#20998;&#21106;&#12290;RiEMann&#21487;&#20197;&#20174;&#38646;&#24320;&#22987;&#23398;&#20064;&#19968;&#20010;&#25805;&#20316;&#20219;&#21153;&#65292;&#21482;&#38656;5&#21040;10&#20010;&#28436;&#31034;&#65292;&#21487;&#20197;&#27867;&#21270;&#21040;&#30475;&#19981;&#35265;&#30340;SE(3)&#36716;&#25442;&#21644;&#30446;&#26631;&#23545;&#35937;&#30340;&#23454;&#20363;&#65292;&#25269;&#25239;&#24178;&#25200;&#23545;&#35937;&#30340;&#35270;&#35273;&#24178;&#25200;&#65292;&#24182;&#36981;&#24490;&#30446;&#26631;&#23545;&#35937;&#30340;&#36817;&#23454;&#26102;&#23039;&#21183;&#21464;&#21270;&#12290;RiEMann&#30340;&#21487;&#20280;&#32553;&#21160;&#20316;&#31354;&#38388;&#26377;&#21161;&#20110;&#28155;&#21152;&#33258;&#23450;&#20041;&#31561;&#21464;&#21160;&#20316;&#65292;&#20363;&#22914;&#26059;&#36716;&#27700;&#40857;&#22836;&#30340;&#26041;&#21521;&#65292;&#36825;&#20351;&#24471;RiEMann&#21487;&#20197;&#36827;&#34892;&#20851;&#33410;&#23545;&#35937;&#25805;&#20316;&#12290;&#22312;&#27169;&#25311;&#21644;&#29616;&#23454;&#19990;&#30028;&#30340;6&#33258;&#30001;&#24230;&#26426;&#22120;&#20154;&#25805;&#20316;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;RiEMann &#23545; 5&#31867;&#25805;&#32437;&#20219;&#21153;&#30340;25&#31181;&#21464;&#20307;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19460v1 Announce Type: cross  Abstract: We present RiEMann, an end-to-end near Real-time SE(3)-Equivariant Robot Manipulation imitation learning framework from scene point cloud input. Compared to previous methods that rely on descriptor field matching, RiEMann directly predicts the target poses of objects for manipulation without any object segmentation. RiEMann learns a manipulation task from scratch with 5 to 10 demonstrations, generalizes to unseen SE(3) transformations and instances of target objects, resists visual interference of distracting objects, and follows the near real-time pose change of the target object. The scalable action space of RiEMann facilitates the addition of custom equivariant actions such as the direction of turning the faucet, which makes articulated object manipulation possible for RiEMann. In simulation and real-world 6-DOF robot manipulation experiments, we test RiEMann on 5 categories of manipulation tasks with a total of 25 variants and show
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CMP&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LiDAR&#20449;&#21495;&#20316;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#21512;&#20316;&#24863;&#30693;&#21644;&#36816;&#21160;&#39044;&#27979;&#27169;&#22359;&#20849;&#20139;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#21512;&#20316;&#36816;&#21160;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.17916</link><description>&lt;p&gt;
CMP&#65306;&#20855;&#26377;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#30340;&#21512;&#20316;&#36816;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
CMP: Cooperative Motion Prediction with Multi-Agent Communication
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17916
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CMP&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LiDAR&#20449;&#21495;&#20316;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#21512;&#20316;&#24863;&#30693;&#21644;&#36816;&#21160;&#39044;&#27979;&#27169;&#22359;&#20849;&#20139;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#21512;&#20316;&#36816;&#21160;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#30340;&#21457;&#23637;&#21644;&#36710;&#32852;&#32593;&#65288;V2X&#65289;&#36890;&#20449;&#30340;&#25104;&#29087;&#65292;&#21512;&#20316;&#36830;&#25509;&#30340;&#33258;&#21160;&#21270;&#36710;&#36742;&#65288;CAVs&#65289;&#30340;&#21151;&#33021;&#21464;&#24471;&#21487;&#33021;&#12290;&#26412;&#25991;&#22522;&#20110;&#21512;&#20316;&#24863;&#30693;&#65292;&#25506;&#35752;&#20102;&#21512;&#20316;&#36816;&#21160;&#39044;&#27979;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;CMP&#20197;LiDAR&#20449;&#21495;&#20316;&#20026;&#36755;&#20837;&#65292;&#20197;&#22686;&#24378;&#36319;&#36394;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#19982;&#36807;&#21435;&#19987;&#27880;&#20110;&#21512;&#20316;&#24863;&#30693;&#25110;&#36816;&#21160;&#39044;&#27979;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26159;&#25105;&#20204;&#25152;&#30693;&#30340;&#31532;&#19968;&#20010;&#35299;&#20915;CAVs&#22312;&#24863;&#30693;&#21644;&#39044;&#27979;&#27169;&#22359;&#20013;&#20849;&#20139;&#20449;&#24687;&#30340;&#32479;&#19968;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#35774;&#35745;&#20013;&#36824;&#34701;&#20837;&#20102;&#33021;&#22815;&#23481;&#24525;&#29616;&#23454;V2X&#24102;&#23485;&#38480;&#21046;&#21644;&#20256;&#36755;&#24310;&#36831;&#30340;&#29420;&#29305;&#33021;&#21147;&#65292;&#21516;&#26102;&#22788;&#29702;&#24222;&#22823;&#30340;&#24863;&#30693;&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#39044;&#27979;&#32858;&#21512;&#27169;&#22359;&#65292;&#32479;&#19968;&#20102;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17916v1 Announce Type: cross  Abstract: The confluence of the advancement of Autonomous Vehicles (AVs) and the maturity of Vehicle-to-Everything (V2X) communication has enabled the capability of cooperative connected and automated vehicles (CAVs). Building on top of cooperative perception, this paper explores the feasibility and effectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR signals as input to enhance tracking and prediction capabilities. Unlike previous work that focuses separately on either cooperative perception or motion prediction, our framework, to the best of our knowledge, is the first to address the unified problem where CAVs share information in both perception and prediction modules. Incorporated into our design is the unique capability to tolerate realistic V2X bandwidth limitations and transmission delays, while dealing with bulky perception representations. We also propose a prediction aggregation module, which unifies the predict
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#22240;&#26524;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#27010;&#29575;&#25512;&#29702;&#65292;&#25104;&#21151;&#39044;&#27979;&#31215;&#26408;&#22612;&#31283;&#23450;&#24615;&#24182;&#36873;&#25321;&#19979;&#19968;&#26368;&#20339;&#21160;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.14488</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#23398;&#22240;&#26524;&#25512;&#29702;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#23433;&#20840;&#31283;&#20581;&#30340;&#19979;&#19968;&#26368;&#20339;&#21160;&#20316;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Physics-Based Causal Reasoning for Safe &amp; Robust Next-Best Action Selection in Robot Manipulation Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14488
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#22240;&#26524;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#27010;&#29575;&#25512;&#29702;&#65292;&#25104;&#21151;&#39044;&#27979;&#31215;&#26408;&#22612;&#31283;&#23450;&#24615;&#24182;&#36873;&#25321;&#19979;&#19968;&#26368;&#20339;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#39640;&#25928;&#30340;&#29289;&#20307;&#25805;&#20316;&#26159;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#20851;&#38190;&#25512;&#25163;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25361;&#25112;&#22312;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#24517;&#39035;&#23545;&#19968;&#31995;&#21015;&#20256;&#24863;&#22120;&#21644;&#25191;&#34892;&#22120;&#30340;&#19981;&#30830;&#23450;&#24615;&#20855;&#26377;&#31283;&#20581;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#21644;&#22240;&#26524;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35753;&#26426;&#22120;&#20154;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#29615;&#22659;&#20013;&#23545;&#20505;&#36873;&#21160;&#20316;&#36827;&#34892;&#27010;&#29575;&#25512;&#29702;&#65292;&#20197;&#23436;&#25104;&#19968;&#20010;&#31215;&#26408;&#22534;&#21472;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;&#21018;&#20307;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#20223;&#30495;&#19982;&#22240;&#26524;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;CBN&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#23450;&#20041;&#20102;&#26426;&#22120;&#20154;&#20915;&#31574;&#36807;&#31243;&#30340;&#22240;&#26524;&#29983;&#25104;&#27010;&#29575;&#27169;&#22411;&#12290;&#36890;&#36807;&#22522;&#20110;&#20223;&#30495;&#30340;&#33945;&#29305;&#21345;&#27931;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#25104;&#21151;&#22320;&#33021;&#22815;&#65306;(1) &#39640;&#20934;&#30830;&#24230;&#22320;&#39044;&#27979;&#31215;&#26408;&#22612;&#30340;&#31283;&#23450;&#24615;&#65288;&#39044;&#27979;&#20934;&#30830;&#29575;&#65306;88.6%&#65289;&#65307;&#21644;&#65292;(2) &#20026;&#31215;&#26408;&#22534;&#21472;&#20219;&#21153;&#36873;&#25321;&#19968;&#20010;&#36817;&#20284;&#30340;&#19979;&#19968;&#26368;&#20339;&#21160;&#20316;&#65292;&#20379;&#25972;&#21512;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#25191;&#34892;&#65292;&#23454;&#29616;94.2%&#30340;&#20219;&#21153;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14488v1 Announce Type: cross  Abstract: Safe and efficient object manipulation is a key enabler of many real-world robot applications. However, this is challenging because robot operation must be robust to a range of sensor and actuator uncertainties. In this paper, we present a physics-informed causal-inference-based framework for a robot to probabilistically reason about candidate actions in a block stacking task in a partially observable setting. We integrate a physics-based simulation of the rigid-body system dynamics with a causal Bayesian network (CBN) formulation to define a causal generative probabilistic model of the robot decision-making process. Using simulation-based Monte Carlo experiments, we demonstrate our framework's ability to successfully: (1) predict block tower stability with high accuracy (Pred Acc: 88.6%); and, (2) select an approximate next-best action for the block stacking task, for execution by an integrated robot system, achieving 94.2% task succe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#27169;&#22411;&#24320;&#25918;&#26694;&#26550;&#65288;MOF&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#25490;&#21517;&#20998;&#31867;&#31995;&#32479;&#65292;&#26681;&#25454;&#23436;&#25972;&#24615;&#21644;&#24320;&#25918;&#24615;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#26088;&#22312;&#20419;&#36827;&#23436;&#25972;&#24615;&#12289;&#24320;&#25918;&#24615;&#20197;&#21450;&#36981;&#24490;&#24320;&#25918;&#31185;&#23398;&#21407;&#21017;&#65292;&#21487;&#20197;&#24110;&#21161;&#20934;&#30830;&#35782;&#21035;&#27169;&#22411;&#30340;&#36879;&#26126;&#24615;&#21644;&#21487;&#37325;&#29616;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13784</link><description>&lt;p&gt;
&#27169;&#22411;&#24320;&#25918;&#26694;&#26550;: &#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#21487;&#37325;&#29616;&#24615;&#12289;&#36879;&#26126;&#24230;&#21644;&#21487;&#29992;&#24615;&#30340;&#23436;&#25972;&#24615;&#21644;&#24320;&#25918;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Model Openness Framework: Promoting Completeness and Openness for Reproducibility, Transparency and Usability in AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13784
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#27169;&#22411;&#24320;&#25918;&#26694;&#26550;&#65288;MOF&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#25490;&#21517;&#20998;&#31867;&#31995;&#32479;&#65292;&#26681;&#25454;&#23436;&#25972;&#24615;&#21644;&#24320;&#25918;&#24615;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#26088;&#22312;&#20419;&#36827;&#23436;&#25972;&#24615;&#12289;&#24320;&#25918;&#24615;&#20197;&#21450;&#36981;&#24490;&#24320;&#25918;&#31185;&#23398;&#21407;&#21017;&#65292;&#21487;&#20197;&#24110;&#21161;&#20934;&#30830;&#35782;&#21035;&#27169;&#22411;&#30340;&#36879;&#26126;&#24615;&#21644;&#21487;&#37325;&#29616;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#21487;&#33021;&#24615;&#65292;&#20294;&#20854;&#21830;&#19994;&#21270;&#24341;&#21457;&#20102;&#20851;&#20110;&#36879;&#26126;&#24230;&#12289;&#21487;&#37325;&#29616;&#24615;&#12289;&#20559;&#35265;&#21644;&#23433;&#20840;&#24615;&#30340;&#25285;&#24551;&#12290;&#35768;&#22810;"&#24320;&#28304;"&#30340;GAI&#27169;&#22411;&#32570;&#20047;&#23436;&#25972;&#29702;&#35299;&#21644;&#20877;&#29616;&#25152;&#24517;&#38656;&#30340;&#32452;&#20214;&#65292;&#19968;&#20123;&#37319;&#29992;&#38480;&#21046;&#24615;&#35768;&#21487;&#35777;&#65292;&#36825;&#31181;&#34892;&#20026;&#34987;&#31216;&#20026;"&#24320;&#28304;&#27927;&#30333;"&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#27169;&#22411;&#24320;&#25918;&#26694;&#26550;&#65288;MOF&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26681;&#25454;&#23436;&#25972;&#24615;&#21644;&#24320;&#25918;&#24615;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#25490;&#21517;&#20998;&#31867;&#30340;&#31995;&#32479;&#65292;&#36981;&#24490;&#24320;&#25918;&#31185;&#23398;&#12289;&#24320;&#28304;&#12289;&#24320;&#25918;&#25968;&#25454;&#21644;&#24320;&#25918;&#33719;&#21462;&#30340;&#21407;&#21017;&#12290;MOF&#35201;&#27714;&#27169;&#22411;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#30340;&#29305;&#23450;&#32452;&#20214;&#34987;&#21253;&#21547;&#24182;&#26681;&#25454;&#36866;&#24403;&#30340;&#24320;&#25918;&#35768;&#21487;&#35777;&#21457;&#24067;&#12290;&#35813;&#26694;&#26550;&#26088;&#22312;&#38450;&#27490;&#23459;&#31216;&#33258;&#24049;&#26159;&#24320;&#25918;&#30340;&#27169;&#22411;&#34987;&#35823;&#35299;&#65292;&#25351;&#23548;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#32773;&#20197;&#23485;&#26494;&#30340;&#35768;&#21487;&#35777;&#21457;&#24067;&#25152;&#26377;&#27169;&#22411;&#32452;&#20214;&#65292;&#24182;&#24110;&#21161;&#20844;&#21496;&#12289;&#23398;&#26415;&#30028;&#21644;&#29233;&#22909;&#32773;&#35782;&#21035;&#21487;&#20197;&#23433;&#20840;&#37319;&#29992;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13784v1 Announce Type: new  Abstract: Generative AI (GAI) offers unprecedented possibilities but its commercialization has raised concerns about transparency, reproducibility, bias, and safety. Many "open-source" GAI models lack the necessary components for full understanding and reproduction, and some use restrictive licenses, a practice known as "openwashing." We propose the Model Openness Framework (MOF), a ranked classification system that rates machine learning models based on their completeness and openness, following principles of open science, open source, open data, and open access. The MOF requires specific components of the model development lifecycle to be included and released under appropriate open licenses. This framework aims to prevent misrepresentation of models claiming to be open, guide researchers and developers in providing all model components under permissive licenses, and help companies, academia, and hobbyists identify models that can be safely adop
&lt;/p&gt;</description></item><item><title>PARAMANU-AYN&#26159;&#19968;&#31181;&#22522;&#20110;&#21360;&#24230;&#27861;&#24459;&#26696;&#20363;&#25991;&#20214;&#30340;&#39640;&#25928;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#33258;&#22238;&#24402;&#35299;&#30721;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#32463;&#36807;&#38754;&#21521;&#25351;&#20196;&#30340;&#24494;&#35843;&#65292;&#22312;&#21508;&#31181;&#27861;&#24459;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.13681</link><description>&lt;p&gt;
PARAMANU-AYN&#65306;&#19968;&#31181;&#26377;&#25928;&#30340;&#26032;&#22411;&#29983;&#25104;&#24335;&#12289;&#38754;&#21521;&#21360;&#24230;&#27861;&#24459;&#26696;&#20363;&#25991;&#20214;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PARAMANU-AYN: An Efficient Novel Generative and Instruction-tuned Language Model for Indian Legal Case Documents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13681
&lt;/p&gt;
&lt;p&gt;
PARAMANU-AYN&#26159;&#19968;&#31181;&#22522;&#20110;&#21360;&#24230;&#27861;&#24459;&#26696;&#20363;&#25991;&#20214;&#30340;&#39640;&#25928;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#33258;&#22238;&#24402;&#35299;&#30721;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#32463;&#36807;&#38754;&#21521;&#25351;&#20196;&#30340;&#24494;&#35843;&#65292;&#22312;&#21508;&#31181;&#27861;&#24459;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PARAMANU-AYN&#65292;&#36825;&#26159;&#19968;&#20010;&#20165;&#22522;&#20110;&#21360;&#24230;&#26368;&#39640;&#27861;&#38498;&#26696;&#20363;&#25991;&#20214;&#12289;&#21360;&#24230;&#23466;&#27861;&#21644;&#21360;&#24230;&#21009;&#27861;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33258;&#22238;&#24402;&#65288;AR&#65289;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#26159;&#20174;&#22836;&#24320;&#22987;&#22312;&#19978;&#19979;&#25991;&#22823;&#23567;&#20026;8192&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#12290;&#25105;&#20204;&#22312;&#22256;&#24785;&#24230;&#25351;&#26631;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#27861;&#24459;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#23545;&#19968;&#32452;&#21253;&#25324;&#21508;&#31181;&#27861;&#24459;&#20219;&#21153;&#65288;&#22914;&#27861;&#24459;&#25512;&#29702;&#12289;&#21028;&#20915;&#35299;&#37322;&#12289;&#27861;&#24459;&#26465;&#27454;&#29983;&#25104;&#12289;&#27861;&#24459;&#33609;&#25311;&#12289;&#27861;&#24459;&#21512;&#21516;&#33609;&#25311;&#12289;&#26696;&#20214;&#25688;&#35201;&#12289;&#23466;&#27861;&#38382;&#39064;&#22238;&#31572;&#31561;&#65289;&#30340;10,763&#26465;&#25351;&#20196;&#36827;&#34892;&#20102;&#38024;&#23545;&#24615;&#35757;&#32451;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;GPT-3.5-Turbo&#23545;&#38754;&#21521;&#25351;&#20196;&#30340;&#27169;&#22411;&#30340;&#25552;&#31034;&#21709;&#24212;&#36827;&#34892;&#20102;&#22312;10&#20998;&#21046;&#24230;&#19978;&#30340;&#28165;&#26224;&#24230;&#12289;&#30456;&#20851;&#24615;&#12289;&#23436;&#25972;&#24615;&#21644;&#27861;&#24459;&#25512;&#29702;&#25351;&#26631;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;CPU&#19978;&#36816;&#34892;&#65292;&#24182;&#23454;&#29616;&#27599;&#31186;42.46&#20010;&#20196;&#29260;&#30340;CPU&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13681v1 Announce Type: new  Abstract: In this paper, we present PARAMANU-AYN, a language model based exclusively on case documents of the Supreme Court of India, the Constitution of India, and the Indian Penal Code. The novel Auto Regressive (AR) decoder based model is pretrained from scratch at a context size of 8192. We evaluated our pretrained legal model on perplexity metrics. We also instruction-tuned our pretrained model on a set of 10,763 instructions covering various legal tasks such as legal reasoning, judgement explanation, legal clause generation, legal drafting, legal contract drafting, case summarization, constitutional question-answering, etc. We also evaluated the responses of prompts for instruction-tuned models by GPT-3.5-Turbo on clarity, relevance, completeness, and legal reasoning metrics in a scale of 10. Our model can be run on CPU and achieved 42.46 tokens/sec CPU inference speed. We found that our models, despite not being pretrained on legal books, v
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25112;&#20105;&#28216;&#25103;&#20013;&#19982;&#20154;&#31867;&#21709;&#24212;&#23384;&#22312;&#19968;&#33268;&#24615;&#65292;&#20294;&#20063;&#23384;&#22312;&#26174;&#33879;&#30340;&#24046;&#24322;&#65292;&#36825;&#34920;&#26126;&#22312;&#25919;&#31574;&#21046;&#23450;&#32773;&#20132;&#20986;&#33258;&#20027;&#26435;&#25110;&#21548;&#20174;&#22522;&#20110;AI&#30340;&#25112;&#30053;&#24314;&#35758;&#20043;&#21069;&#24212;&#35880;&#24910;&#23545;&#24453;&#12290;</title><link>https://arxiv.org/abs/2403.03407</link><description>&lt;p&gt;
&#20154;&#31867;&#23545;&#25239;&#26426;&#22120;&#65306;&#35821;&#35328;&#27169;&#22411;&#19982;&#25112;&#20105;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Human vs. Machine: Language Models and Wargames
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03407
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25112;&#20105;&#28216;&#25103;&#20013;&#19982;&#20154;&#31867;&#21709;&#24212;&#23384;&#22312;&#19968;&#33268;&#24615;&#65292;&#20294;&#20063;&#23384;&#22312;&#26174;&#33879;&#30340;&#24046;&#24322;&#65292;&#36825;&#34920;&#26126;&#22312;&#25919;&#31574;&#21046;&#23450;&#32773;&#20132;&#20986;&#33258;&#20027;&#26435;&#25110;&#21548;&#20174;&#22522;&#20110;AI&#30340;&#25112;&#30053;&#24314;&#35758;&#20043;&#21069;&#24212;&#35880;&#24910;&#23545;&#24453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25112;&#20105;&#28216;&#25103;&#22312;&#20891;&#20107;&#25112;&#30053;&#30340;&#21457;&#23637;&#21644;&#22269;&#23478;&#23545;&#23041;&#32961;&#25110;&#25915;&#20987;&#30340;&#21709;&#24212;&#20013;&#26377;&#30528;&#24736;&#20037;&#30340;&#21382;&#21490;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#20986;&#29616;&#25215;&#35834;&#20102;&#26356;&#22909;&#30340;&#20915;&#31574;&#21046;&#23450;&#21644;&#22686;&#24378;&#30340;&#20891;&#20107;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;AI&#31995;&#32479;&#65292;&#23588;&#20854;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#19982;&#20154;&#31867;&#30340;&#34892;&#20026;&#26377;&#20309;&#19981;&#21516;&#20173;&#23384;&#22312;&#20105;&#35758;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#25112;&#20105;&#28216;&#25103;&#23454;&#39564;&#65292;&#20849;&#26377;107&#20301;&#22269;&#23478;&#23433;&#20840;&#19987;&#23478;&#20154;&#31867;&#21442;&#19982;&#32773;&#21442;&#19982;&#65292;&#26088;&#22312;&#30740;&#31350;&#22312;&#19968;&#20010;&#34394;&#26500;&#30340;&#32654;&#20013;&#24773;&#26223;&#20013;&#30340;&#21361;&#26426;&#21319;&#32423;&#65292;&#24182;&#27604;&#36739;&#20154;&#31867;&#21442;&#19982;&#32773;&#19982;LLM&#27169;&#25311;&#21709;&#24212;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#21457;&#29616;LLM&#21644;&#20154;&#31867;&#21709;&#24212;&#23384;&#22312;&#26174;&#33879;&#19968;&#33268;&#24615;&#65292;&#20294;&#22312;&#25112;&#20105;&#28216;&#25103;&#20013;&#27169;&#25311;&#21644;&#20154;&#31867;&#21442;&#19982;&#32773;&#20043;&#38388;&#20063;&#23384;&#22312;&#26174;&#33879;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#24046;&#24322;&#65292;&#36825;&#20419;&#20351;&#20915;&#31574;&#32773;&#22312;&#20132;&#20986;&#33258;&#20027;&#26435;&#25110;&#36981;&#24490;&#22522;&#20110;AI&#30340;&#25112;&#30053;&#24314;&#35758;&#20043;&#21069;&#35880;&#24910;&#23545;&#24453;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03407v1 Announce Type: cross  Abstract: Wargames have a long history in the development of military strategy and the response of nations to threats or attacks. The advent of artificial intelligence (AI) promises better decision-making and increased military effectiveness. However, there is still debate about how AI systems, especially large language models (LLMs), behave as compared to humans. To this end, we use a wargame experiment with 107 national security expert human players designed to look at crisis escalation in a fictional US-China scenario and compare human players to LLM-simulated responses. We find considerable agreement in the LLM and human responses but also significant quantitative and qualitative differences between simulated and human players in the wargame, motivating caution to policymakers before handing over autonomy or following AI-based strategy recommendations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;POMDP&#32467;&#26500;&#30340;&#26032;&#39062;&#35206;&#30422;&#20551;&#35774;&#65292;&#20197;&#35299;&#20915;&#26410;&#26469;&#20381;&#36182;&#20215;&#20540;&#20989;&#25968;&#26041;&#27861;&#20013;&#30340;&#38271;&#24230;&#25351;&#25968;&#22686;&#38271;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.14703</link><description>&lt;p&gt;
&#22312;&#26410;&#26469;&#20381;&#36182;&#20215;&#20540;&#20989;&#25968;&#20013;&#25506;&#35752;&#26410;&#26469;&#21644;&#21382;&#21490;&#30340;&#35781;&#21650;&#22312;&#31163;&#32447;&#35780;&#20272;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Curses of Future and History in Future-dependent Value Functions for Off-policy Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;POMDP&#32467;&#26500;&#30340;&#26032;&#39062;&#35206;&#30422;&#20551;&#35774;&#65292;&#20197;&#35299;&#20915;&#26410;&#26469;&#20381;&#36182;&#20215;&#20540;&#20989;&#25968;&#26041;&#27861;&#20013;&#30340;&#38271;&#24230;&#25351;&#25968;&#22686;&#38271;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#22797;&#26434;&#35266;&#27979;&#30340;&#31163;&#32447;&#35780;&#20272;(OPE)&#65292;&#26088;&#22312;&#24320;&#21457;&#33021;&#22815;&#36991;&#20813;&#23545;&#26102;&#38388;&#36328;&#24230;&#25351;&#25968;&#20381;&#36182;&#30340;&#20272;&#35745;&#22120;&#12290;&#26368;&#36817;&#65292;Uehara&#31561;&#20154;&#65288;2022&#24180;&#65289;&#25552;&#20986;&#20102;&#26410;&#26469;&#20381;&#36182;&#20215;&#20540;&#20989;&#25968;&#20316;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#35813;&#26694;&#26550;&#20063;&#21462;&#20915;&#20110;&#26410;&#26469;&#20381;&#36182;&#20215;&#20540;&#20989;&#25968;&#30340;&#26377;&#30028;&#24615;&#20197;&#21450;&#20854;&#20182;&#30456;&#20851;&#25968;&#37327;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#25968;&#37327;&#21487;&#33021;&#20250;&#38543;&#30528;&#38271;&#24230;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#20174;&#32780;&#25273;&#21435;&#35813;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#38024;&#23545;POMDP&#32467;&#26500;&#30340;&#26032;&#39062;&#35206;&#30422;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14703v1 Announce Type: cross  Abstract: We study off-policy evaluation (OPE) in partially observable environments with complex observations, with the goal of developing estimators whose guarantee avoids exponential dependence on the horizon. While such estimators exist for MDPs and POMDPs can be converted to history-based MDPs, their estimation errors depend on the state-density ratio for MDPs which becomes history ratios after conversion, an exponential object. Recently, Uehara et al. (2022) proposed future-dependent value functions as a promising framework to address this issue, where the guarantee for memoryless policies depends on the density ratio over the latent state space. However, it also depends on the boundedness of the future-dependent value function and other related quantities, which we show could be exponential-in-length and thus erasing the advantage of the method. In this paper, we discover novel coverage assumptions tailored to the structure of POMDPs, such
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#27969;&#31243;&#65292;&#21033;&#29992;GPT-3.5&#21644;GPT-4&#29983;&#25104;&#39640;&#36136;&#37327;&#21453;&#39304;&#65292;&#20197;&#22686;&#24378;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20013;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#65292;&#24357;&#34917;&#20102;&#19987;&#23478;&#27880;&#37322;&#25968;&#25454;&#30340;&#39640;&#25104;&#26412;&#21644;&#26377;&#38480;&#21487;&#29992;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.13919</link><description>&lt;p&gt;
SYNFAC-EDIT: &#29992;&#20110;&#20020;&#24202;&#25688;&#35201;&#20013;&#30340;&#20107;&#23454;&#23545;&#40784;&#30340;&#21512;&#25104;&#27169;&#20223;&#32534;&#36753;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13919
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#27969;&#31243;&#65292;&#21033;&#29992;GPT-3.5&#21644;GPT-4&#29983;&#25104;&#39640;&#36136;&#37327;&#21453;&#39304;&#65292;&#20197;&#22686;&#24378;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20013;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#65292;&#24357;&#34917;&#20102;&#19987;&#23478;&#27880;&#37322;&#25968;&#25454;&#30340;&#39640;&#25104;&#26412;&#21644;&#26377;&#38480;&#21487;&#29992;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT&#21644;Llama&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#22312;&#20107;&#23454;&#19981;&#20934;&#30830;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#36825;&#26159;&#20020;&#24202;NLP&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#38169;&#35823;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#20107;&#23454;&#23545;&#40784;&#30340;&#19987;&#23478;&#27880;&#37322;&#25968;&#25454;&#25104;&#26412;&#39640;&#26114;&#19988;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#27969;&#31243;&#65292;&#21033;&#29992;GPT-3.5&#21644;GPT-4&#29983;&#25104;&#39640;&#36136;&#37327;&#21453;&#39304;&#65292;&#26088;&#22312;&#22686;&#24378;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20013;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#32534;&#36753;&#21453;&#39304;&#65292;&#22312;&#27809;&#26377;&#39069;&#22806;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#25311;&#20102;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#25913;&#21892;AI&#31995;&#32479;&#36755;&#20986;&#30340;&#23454;&#38469;&#22330;&#26223;&#12290;&#23613;&#31649;GPT&#22312;&#21508;&#31181;&#20020;&#24202;NLP&#20219;&#21153;&#20013;&#37117;&#34920;&#29616;&#20986;&#20102;&#19987;&#19994;&#27700;&#24179;&#65292;&#27604;&#22914;&#21307;&#23398;&#25191;&#29031;&#32771;&#35797;&#65292;&#20294;&#23545;&#20854;&#25552;&#20379;&#25913;&#21892;&#36739;&#24369;LM&#25110;LLM&#29983;&#25104;&#36136;&#37327;&#30340;&#19987;&#19994;&#32423;&#32534;&#36753;&#21453;&#39304;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13919v1 Announce Type: cross  Abstract: Large Language Models (LLMs) such as GPT and Llama have demonstrated significant achievements in summarization tasks but struggle with factual inaccuracies, a critical issue in clinical NLP applications where errors could lead to serious consequences. To counter the high costs and limited availability of expert-annotated data for factual alignment, this study introduces an innovative pipeline that utilizes GPT-3.5 and GPT-4 to generate high-quality feedback aimed at enhancing factual consistency in clinical note summarization. Our research primarily focuses on edit feedback, mirroring the practical scenario in which medical professionals refine AI system outputs without the need for additional annotations. Despite GPT's proven expertise in various clinical NLP tasks, such as the Medical Licensing Examination, there is scant research on its capacity to deliver expert-level edit feedback for improving weaker LMs or LLMs generation qualit
&lt;/p&gt;</description></item><item><title>&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#23398;&#20064;&#23545;&#38543;&#26426;&#24615;&#30340;&#25935;&#24863;&#24615;&#65292;&#36890;&#36807;&#31995;&#32479;&#30740;&#31350;&#38543;&#26426;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#24573;&#30053;&#30456;&#20114;&#20316;&#29992;&#21487;&#33021;&#23548;&#33268;&#30340;&#19981;&#19968;&#33268;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.12817</link><description>&lt;p&gt;
&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#23398;&#20064;&#23545;&#38543;&#26426;&#24615;&#30340;&#25935;&#24863;&#24615;&#65306;&#30456;&#20114;&#20316;&#29992;&#21644;&#31995;&#32479;&#36873;&#25321;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12817
&lt;/p&gt;
&lt;p&gt;
&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#23398;&#20064;&#23545;&#38543;&#26426;&#24615;&#30340;&#25935;&#24863;&#24615;&#65292;&#36890;&#36807;&#31995;&#32479;&#30740;&#31350;&#38543;&#26426;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#24573;&#30053;&#30456;&#20114;&#20316;&#29992;&#21487;&#33021;&#23548;&#33268;&#30340;&#19981;&#19968;&#33268;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#23398;&#20064;&#21487;&#20197;&#22312;&#26631;&#31614;&#19981;&#36275;&#26102;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#20063;&#23545;&#25152;&#35859;&#30340;&#38543;&#26426;&#22240;&#32032;&#65288;&#20363;&#22914;&#25968;&#25454;&#30340;&#21464;&#21270;&#39034;&#24207;&#65289;&#24341;&#20837;&#30340;&#26080;&#27861;&#25511;&#21046;&#30340;&#38543;&#26426;&#24615;&#25935;&#24863;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#31995;&#32479;&#22320;&#35843;&#26597;&#38543;&#26426;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#32771;&#34385;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20026;&#20102;&#27979;&#37327;&#21333;&#20010;&#38543;&#26426;&#22240;&#32032;&#30340;&#30495;&#23454;&#24433;&#21709;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20943;&#36731;&#20102;&#20854;&#20182;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#24182;&#35266;&#23519;&#20102;&#24615;&#33021;&#22312;&#22810;&#27425;&#36816;&#34892;&#20013;&#30340;&#21464;&#21270;&#12290;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;7&#20010;&#20195;&#34920;&#24615;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#24494;&#35843;&#26041;&#27861;&#20197;&#21450;3&#20010;&#20219;&#21153;&#30340;&#20803;&#23398;&#20064;&#65292;&#25105;&#20204;&#21457;&#29616;&#65306;1&#65289;&#29616;&#26377;&#20316;&#21697;&#20013;&#24573;&#30053;&#38543;&#26426;&#22240;&#32032;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#23548;&#33268;&#20102;&#19981;&#19968;&#33268;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#22240;&#20026;&#38169;&#35823;&#22320;&#24402;&#22240;&#20110;&#38543;&#26426;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#27604;&#22914;&#21542;&#23450;&#20102;&#19968;&#20123;&#19968;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12817v1 Announce Type: cross  Abstract: While learning with limited labelled data can improve performance when the labels are lacking, it is also sensitive to the effects of uncontrolled randomness introduced by so-called randomness factors (e.g., varying order of data). We propose a method to systematically investigate the effects of randomness factors while taking the interactions between them into consideration. To measure the true effects of an individual randomness factor, our method mitigates the effects of other factors and observes how the performance varies across multiple runs. Applying our method to multiple randomness factors across in-context learning and fine-tuning approaches on 7 representative text classification tasks and meta-learning on 3 tasks, we show that: 1) disregarding interactions between randomness factors in existing works caused inconsistent findings due to incorrect attribution of the effects of randomness factors, such as disproving the consis
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#22810;&#27493;&#20219;&#21153;&#20013;&#38598;&#25104;&#20154;&#31867;&#21453;&#39304;&#21644;&#20559;&#22909;&#23545;&#40784;&#30340;PRompt&#20248;&#21270;&#26041;&#27861;&#12290;&#23427;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#26694;&#26550;&#65292;&#32467;&#21512;&#20154;&#31867;&#21453;&#39304;&#33258;&#21160;&#25552;&#20986;&#20248;&#21270;&#24314;&#35758;&#24182;&#35299;&#20915;&#20102;&#22797;&#26434;&#30340;&#25552;&#31034;&#20869;&#23481;&#20998;&#26512;&#12289;&#21333;&#27493;&#35780;&#20272;&#21644;&#20219;&#21153;&#25191;&#34892;&#20559;&#22909;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.08702</link><description>&lt;p&gt;
PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Preference Alignment
&lt;/p&gt;
&lt;p&gt;
PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Preference Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08702
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#22810;&#27493;&#20219;&#21153;&#20013;&#38598;&#25104;&#20154;&#31867;&#21453;&#39304;&#21644;&#20559;&#22909;&#23545;&#40784;&#30340;PRompt&#20248;&#21270;&#26041;&#27861;&#12290;&#23427;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#26694;&#26550;&#65292;&#32467;&#21512;&#20154;&#31867;&#21453;&#39304;&#33258;&#21160;&#25552;&#20986;&#20248;&#21270;&#24314;&#35758;&#24182;&#35299;&#20915;&#20102;&#22797;&#26434;&#30340;&#25552;&#31034;&#20869;&#23481;&#20998;&#26512;&#12289;&#21333;&#27493;&#35780;&#20272;&#21644;&#20219;&#21153;&#25191;&#34892;&#20559;&#22909;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt optimization aims to find the best prompt for a language model (LLM) in multi-step tasks. This paper introduces a genetic algorithm framework that incorporates human feedback to automatically suggest prompt improvements. It addresses challenges such as complex prompt content analysis, evaluation of individual steps, and variations in task execution preferences.
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08702v1 Announce Type: cross Abstract: Prompt optimization aims to find the best prompt to a large language model (LLM) for a given task. LLMs have been successfully used to help find and improve prompt candidates for single-step tasks. However, realistic tasks for agents are multi-step and introduce new challenges: (1) Prompt content is likely to be more extensive and complex, making it more difficult for LLMs to analyze errors, (2) the impact of an individual step is difficult to evaluate, and (3) different people may have varied preferences about task execution. While humans struggle to optimize prompts, they are good at providing feedback about LLM outputs; we therefore introduce a new LLM-driven discrete prompt optimization framework that incorporates human-designed feedback rules about potential errors to automatically offer direct suggestions for improvement. Our framework is stylized as a genetic algorithm in which an LLM generates new candidate prompts from a parent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"&#22270;&#29305;&#24449;&#39044;&#22788;&#29702;&#22120;"&#30340;&#36719;&#20214;&#24211;&#65292;&#21487;&#20197;&#20174;&#23454;&#26102;&#20132;&#26131;&#22270;&#20013;&#26816;&#27979;&#20856;&#22411;&#30340;&#27927;&#38065;&#21644;&#27450;&#35784;&#27169;&#24335;&#65292;&#24182;&#29983;&#25104;&#20016;&#23500;&#30340;&#20132;&#26131;&#29305;&#24449;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.08593</link><description>&lt;p&gt;
&#22270;&#29305;&#24449;&#39044;&#22788;&#29702;&#22120;&#65306;&#23454;&#26102;&#20174;&#20132;&#26131;&#22270;&#20013;&#25552;&#21462;&#22522;&#20110;&#23376;&#22270;&#30340;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Graph Feature Preprocessor: Real-time Extraction of Subgraph-based Features from Transaction Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"&#22270;&#29305;&#24449;&#39044;&#22788;&#29702;&#22120;"&#30340;&#36719;&#20214;&#24211;&#65292;&#21487;&#20197;&#20174;&#23454;&#26102;&#20132;&#26131;&#22270;&#20013;&#26816;&#27979;&#20856;&#22411;&#30340;&#27927;&#38065;&#21644;&#27450;&#35784;&#27169;&#24335;&#65292;&#24182;&#29983;&#25104;&#20016;&#23500;&#30340;&#20132;&#26131;&#29305;&#24449;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#22270;&#29305;&#24449;&#39044;&#22788;&#29702;&#22120;"&#30340;&#36719;&#20214;&#24211;&#65292;&#29992;&#20110;&#23454;&#26102;&#26816;&#27979;&#37329;&#34701;&#20132;&#26131;&#22270;&#20013;&#30340;&#20856;&#22411;&#27927;&#38065;&#21644;&#27450;&#35784;&#27169;&#24335;&#12290;&#36825;&#20123;&#27169;&#24335;&#34987;&#29992;&#20110;&#29983;&#25104;&#20016;&#23500;&#30340;&#20132;&#26131;&#29305;&#24449;&#65292;&#29992;&#20110;&#19979;&#28216;&#30340;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#21644;&#25512;&#26029;&#20219;&#21153;&#65292;&#22914;&#27927;&#38065;&#26816;&#27979;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#20016;&#23500;&#30340;&#20132;&#26131;&#29305;&#24449;&#22914;&#20309;&#26174;&#33879;&#25552;&#39640;&#22522;&#20110;&#26799;&#24230;&#25552;&#21319;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#30340;&#24211;&#21033;&#29992;&#22810;&#26680;&#24182;&#34892;&#24615;&#65292;&#32500;&#25252;&#19968;&#20010;&#21160;&#24577;&#30340;&#20869;&#23384;&#22270;&#65292;&#24182;&#39640;&#25928;&#22320;&#25366;&#25496;&#20256;&#20837;&#20132;&#26131;&#27969;&#20013;&#30340;&#23376;&#22270;&#27169;&#24335;&#65292;&#20351;&#20854;&#33021;&#22815;&#20197;&#27969;&#30340;&#26041;&#24335;&#25805;&#20316;&#12290;&#25105;&#20204;&#20351;&#29992;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#21512;&#25104;&#21453;&#27927;&#38065;&#65288;AML&#65289;&#21644;&#30495;&#23454;&#30340;&#20197;&#22826;&#22346;&#38035;&#40060;&#25968;&#25454;&#38598;&#23545;&#25105;&#20204;&#30340;&#24211;&#36827;&#34892;&#35780;&#20272;&#12290;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#65292;&#38750;&#27861;&#20132;&#26131;&#30340;&#27604;&#20363;&#38750;&#24120;&#23567;&#65292;&#20351;&#24471;&#23398;&#20064;&#36807;&#31243;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#32467;&#21512;&#20102;&#25105;&#20204;&#30340;&#22270;&#29305;&#24449;&#39044;&#22788;&#29702;&#22120;&#21644;...
&lt;/p&gt;
&lt;p&gt;
In this paper, we present "Graph Feature Preprocessor", a software library for detecting typical money laundering and fraud patterns in financial transaction graphs in real time. These patterns are used to produce a rich set of transaction features for downstream machine learning training and inference tasks such as money laundering detection. We show that our enriched transaction features dramatically improve the prediction accuracy of gradient-boosting-based machine learning models. Our library exploits multicore parallelism, maintains a dynamic in-memory graph, and efficiently mines subgraph patterns in the incoming transaction stream, which enables it to be operated in a streaming manner. We evaluate our library using highly-imbalanced synthetic anti-money laundering (AML) and real-life Ethereum phishing datasets. In these datasets, the proportion of illicit transactions is very small, which makes the learning process challenging. Our solution, which combines our Graph Feature Prep
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23454;&#35777;&#30740;&#31350;&#20102;&#37319;&#26679;&#28201;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#39064;&#20013;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;0.0&#33267;1.0&#30340;&#28201;&#24230;&#33539;&#22260;&#20869;&#65292;LLM&#24615;&#33021;&#23545;&#35299;&#39064;&#20219;&#21153;&#27809;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.05201</link><description>&lt;p&gt;
&#37319;&#26679;&#28201;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#39064;&#20013;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Effect of Sampling Temperature on Problem Solving in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05201
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23454;&#35777;&#30740;&#31350;&#20102;&#37319;&#26679;&#28201;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#39064;&#20013;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;0.0&#33267;1.0&#30340;&#28201;&#24230;&#33539;&#22260;&#20869;&#65292;LLM&#24615;&#33021;&#23545;&#35299;&#39064;&#20219;&#21153;&#27809;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#35843;&#26597;&#20102;&#37319;&#26679;&#28201;&#24230;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#35299;&#39064;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#26631;&#20934;LLM&#22522;&#20934;&#20013;&#38543;&#26426;&#25277;&#21462;&#38382;&#39064;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65288;MCQA&#65289;&#32771;&#35797;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22235;&#31181;&#24120;&#35265;&#30340;LLM&#20197;&#21450;&#20116;&#31181;&#25552;&#31034;&#24341;&#25806;&#25216;&#26415;&#26469;&#35299;&#20915;MCQA&#38382;&#39064;&#65292;&#21516;&#26102;&#23558;&#37319;&#26679;&#28201;&#24230;&#20174;0.0&#22686;&#21152;&#21040;1.0&#12290;&#23613;&#31649;&#26377;&#20851;&#30340;&#25253;&#36947;&#19982;&#20043;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;0.0&#33267;1.0&#30340;&#28201;&#24230;&#33539;&#22260;&#20869;&#65292;LLM&#24615;&#33021;&#22312;&#35299;&#39064;&#20219;&#21153;&#20013;&#30340;&#21464;&#21270;&#27809;&#26377;&#32479;&#35745;&#23398;&#19978;&#26174;&#33879;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#32467;&#26524;&#20284;&#20046;&#19981;&#21463;LLM&#12289;&#25552;&#31034;&#24341;&#25806;&#25216;&#26415;&#25110;&#38382;&#39064;&#39046;&#22495;&#30340;&#24433;&#21709;&#12290;&#25152;&#26377;&#20195;&#30721;&#12289;&#25968;&#25454;&#21644;&#34917;&#20805;&#36164;&#26009;&#37117;&#21487;&#20197;&#22312;GitHub&#19978;&#25214;&#21040;&#65306;https://github.com/matthewrenze/jhu-llm-temperature&#12290;
&lt;/p&gt;
&lt;p&gt;
In this research study, we empirically investigate the effect of sampling temperature on the performance of Large Language Models (LLMs) on various problem-solving tasks. We created a multiple-choice question-and-answer (MCQA) exam by randomly sampling problems from standard LLM benchmarks. Then, we used four popular LLMs with five prompt-engineering techniques to solve the MCQA problems while increasing the sampling temperature from 0.0 to 1.0. Despite anecdotal reports to the contrary, our empirical results indicate that changes in temperature in the range 0.0 to 1.0 do not have a statistically significant impact on LLM performance for problem-solving tasks. In addition, these results appear to hold regardless of the LLM, the prompt-engineering technique, or the problem domain. All code, data, and supplemental materials are available on GitHub at: https://github.com/matthewrenze/jhu-llm-temperature.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#22686;&#21152;&#23545;&#40784;&#24230;&#21644;&#20943;&#23569;&#26377;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#25552;&#20379;&#36825;&#20004;&#20010;&#25968;&#37327;&#30340;&#36793;&#30028;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.16332</link><description>&lt;p&gt;
&#23545;&#40784;&#21644;&#26377;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65306;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Tradeoffs Between Alignment and Helpfulness in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.16332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#22686;&#21152;&#23545;&#40784;&#24230;&#21644;&#20943;&#23569;&#26377;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#25552;&#20379;&#36825;&#20004;&#20010;&#25968;&#37327;&#30340;&#36793;&#30028;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#24050;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#36890;&#36807;&#22686;&#24378;&#26399;&#26395;&#34892;&#20026;&#21644;&#25233;&#21046;&#38750;&#26399;&#26395;&#34892;&#20026;&#65292;&#23454;&#29616;&#20154;&#31867;&#19982;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#23433;&#20840;&#20132;&#20114;&#12290;&#36890;&#24120;&#36890;&#36807;&#35843;&#25972;&#27169;&#22411;&#25110;&#25554;&#20837;&#39044;&#35774;&#30340;&#23545;&#40784;&#25552;&#31034;&#26469;&#23454;&#29616;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#25913;&#21464;&#35757;&#32451;&#21518;&#30340;&#34920;&#31034;&#26469;&#25913;&#21464;&#27169;&#22411;&#34892;&#20026;&#30340;&#34920;&#31034;&#24037;&#31243;&#26041;&#27861;&#22312;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#12290;&#34920;&#31034;&#24037;&#31243;&#22312;&#38754;&#23545;&#23545;&#25239;&#25915;&#20987;&#21644;&#38477;&#20302;&#31038;&#20250;&#20559;&#35265;&#31561;&#23545;&#40784;&#23548;&#21521;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#22686;&#30410;&#65292;&#20294;&#20063;&#23548;&#33268;&#20102;&#27169;&#22411;&#25191;&#34892;&#22522;&#26412;&#20219;&#21153;&#33021;&#21147;&#30340;&#38477;&#20302;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22686;&#21152;&#23545;&#40784;&#24230;&#21644;&#20943;&#23569;&#27169;&#22411;&#26377;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#25552;&#20379;&#36825;&#20004;&#20010;&#25968;&#37327;&#30340;&#36793;&#30028;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#30456;&#20851;&#24615;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#27169;&#22411;&#30340;&#26377;&#29992;&#24615;&#36890;&#24120;&#20250;&#20943;&#23569;
&lt;/p&gt;
&lt;p&gt;
Language model alignment has become an important component of AI safety, allowing safe interactions between humans and language models, by enhancing desired behaviors and inhibiting undesired ones. It is often done by tuning the model or inserting preset aligning prompts. Recently, representation engineering, a method which alters the model's behavior via changing its representations post-training, was shown to be effective in aligning LLMs (Zou et al., 2023a). Representation engineering yields gains in alignment oriented tasks such as resistance to adversarial attacks and reduction of social biases, but was also shown to cause a decrease in the ability of the model to perform basic tasks. In this paper we study the tradeoff between the increase in alignment and decrease in helpfulness of the model. We propose a theoretical framework which provides bounds for these two quantities, and demonstrate their relevance empirically. Interestingly, we find that while the helpfulness generally d
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#21453;&#21521;&#25351;&#20196;&#36827;&#34892;&#26377;&#25928;&#30340;&#25351;&#20196;&#35843;&#20248;&#65292;&#36890;&#36807;&#29983;&#25104;&#19968;&#32452;&#33258;&#28982;&#30340;&#12289;&#36866;&#29992;&#20110;&#38271;&#25991;&#26412;&#29983;&#25104;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25925;&#20107;/&#33756;&#35889;&#29983;&#25104;&#21644;&#38271;&#31687;&#38382;&#31572;&#31561;&#20219;&#21153;&#19978;&#20248;&#20110;10&#20493;&#35268;&#27169;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#25351;&#20196;&#35843;&#20248;&#12290;</title><link>https://arxiv.org/abs/2304.08460</link><description>&lt;p&gt;
LongForm: &#20351;&#29992;&#21453;&#21521;&#25351;&#20196;&#36827;&#34892;&#26377;&#25928;&#30340;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
LongForm: Effective Instruction Tuning with Reverse Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2304.08460
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21453;&#21521;&#25351;&#20196;&#36827;&#34892;&#26377;&#25928;&#30340;&#25351;&#20196;&#35843;&#20248;&#65292;&#36890;&#36807;&#29983;&#25104;&#19968;&#32452;&#33258;&#28982;&#30340;&#12289;&#36866;&#29992;&#20110;&#38271;&#25991;&#26412;&#29983;&#25104;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25925;&#20107;/&#33756;&#35889;&#29983;&#25104;&#21644;&#38271;&#31687;&#38382;&#31572;&#31561;&#20219;&#21153;&#19978;&#20248;&#20110;10&#20493;&#35268;&#27169;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#25351;&#20196;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Instruction tuning&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#27867;&#21270;&#65292;&#24182;&#26356;&#22909;&#22320;&#36981;&#24490;&#29992;&#25143;&#24847;&#22270;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#25351;&#20196;&#25968;&#25454;&#25104;&#26412;&#39640;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#37319;&#29992;&#20102;&#35832;&#22914;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#12289;&#23384;&#22312;&#23545;&#40784;&#38382;&#39064;&#30340;&#20247;&#21253;&#25968;&#25454;&#38598;&#12289;&#20197;&#21450;&#36890;&#36807;LLMs&#29983;&#25104;&#22122;&#22768;&#31034;&#20363;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LongForm-C&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;&#21453;&#21521;&#25351;&#20196;&#21019;&#24314;&#12290;&#25105;&#20204;&#20351;&#29992;LLMs&#20026;&#20154;&#31867;&#20889;&#20316;&#35821;&#26009;&#24211;&#31034;&#20363;&#20351;&#29992;&#21453;&#21521;&#25351;&#20196;&#29983;&#25104;&#25351;&#20196;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#35832;&#22914;C4&#21644;Wikipedia&#30340;&#35821;&#26009;&#24211;&#20013;&#36873;&#25321;&#20102;&#19968;&#32452;&#22810;&#26679;&#24615;&#30340;&#20154;&#31867;&#25776;&#20889;&#25991;&#26723;&#65307;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;LLMs&#20026;&#36825;&#20123;&#25991;&#26723;&#29983;&#25104;&#25351;&#20196;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#26356;&#20415;&#23452;&#12289;&#26356;&#24178;&#20928;&#12289;&#36755;&#20986;&#33258;&#28982;&#20197;&#21450;&#36866;&#29992;&#20110;&#38271;&#25991;&#26412;&#29983;&#25104;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25925;&#20107;/&#33756;&#35889;&#29983;&#25104;&#21644;&#38271;&#31687;&#38382;&#31572;&#31561;&#20219;&#21153;&#19978;&#20248;&#20110;10&#20493;&#35268;&#27169;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#25351;&#20196;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2304.08460v2 Announce Type: replace-cross Abstract: Instruction tuning enables language models to more effectively generalize and better follow user intent. However, obtaining instruction data is costly and challenging. Prior work employs methods such as expensive human annotation, crowd-sourced datasets with alignment issues, and generating noisy examples via LLMs. We introduce the LongForm-C dataset, which is created by reverse instructions. We generate instructions via LLMs for human-written corpus examples using reverse instructions. First we select a diverse set of human-written documents from corpora such as C4 and Wikipedia; then we generate instructions for these documents via LLMs. This approach provides a cheaper and cleaner instruction-tuning dataset with natural output and one suitable for long text generation. Our models outperform 10x larger language models without instruction tuning on tasks such as story/recipe generation and long-form question answering. Moreover
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#23548;&#33268;&#31070;&#32463;&#32593;&#32476;&#31232;&#30095;&#24615;&#21644;&#36873;&#25321;&#24615;&#22686;&#21152;&#30340;&#21508;&#31181;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2201.11653</link><description>&lt;p&gt;
&#12298;SGD&#21644;&#33258;&#36866;&#24212;&#23398;&#20064;&#35268;&#21017;&#23398;&#21040;&#30340;&#34920;&#31034;&#65306;&#21464;&#21270;&#31232;&#30095;&#24615;&#21644;&#36873;&#25321;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#26465;&#20214;&#12299;
&lt;/p&gt;
&lt;p&gt;
Representations learnt by SGD and Adaptive learning rules: Conditions that vary sparsity and selectivity in neural network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2201.11653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#23548;&#33268;&#31070;&#32463;&#32593;&#32476;&#31232;&#30095;&#24615;&#21644;&#36873;&#25321;&#24615;&#22686;&#21152;&#30340;&#21508;&#31181;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#33041;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36830;&#32493;&#23398;&#20064;&#21487;&#20197;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#32780;&#20114;&#19981;&#24178;&#25200;&#12290;&#20943;&#23569;&#20114;&#30456;&#24178;&#25200;&#30340;&#26377;&#25928;&#26041;&#24335;&#21487;&#20197;&#22312;&#31070;&#32463;&#20803;&#30340;&#31232;&#30095;&#24615;&#21644;&#36873;&#25321;&#24615;&#20013;&#25214;&#21040;&#12290;&#26681;&#25454;Aljundi&#31561;&#20154;&#21644;Hadsell&#31561;&#20154;&#30340;&#35266;&#28857;&#65292;&#22312;&#34920;&#31034;&#27700;&#24179;&#26045;&#21152;&#31232;&#30095;&#24615;&#23545;&#36830;&#32493;&#23398;&#20064;&#26159;&#26377;&#21033;&#30340;&#65292;&#22240;&#20026;&#31232;&#30095;&#30340;&#31070;&#32463;&#20803;&#28608;&#27963;&#40723;&#21169;&#21442;&#25968;&#20043;&#38388;&#30340;&#23569;&#37325;&#21472;&#65292;&#23548;&#33268;&#26356;&#23569;&#30340;&#24178;&#25200;&#12290;&#21516;&#26679;&#65292;&#39640;&#24230;&#36873;&#25321;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#20250;&#24341;&#36215;&#36739;&#23569;&#30340;&#24178;&#25200;&#65292;&#22240;&#20026;&#31070;&#32463;&#20803;&#20013;&#30340;&#29305;&#23450;&#21709;&#24212;&#23558;&#20943;&#23569;&#19982;&#20854;&#20182;&#21442;&#25968;&#30340;&#37325;&#21472;&#26426;&#20250;&#12290;&#32771;&#34385;&#21040;&#20154;&#33041;&#22312;&#19968;&#29983;&#20013;&#25191;&#34892;&#36830;&#32493;&#23398;&#20064;&#65292;&#25214;&#21040;&#33258;&#28982;&#22686;&#21152;&#31232;&#30095;&#24615;&#21644;&#36873;&#25321;&#24615;&#30340;&#26465;&#20214;&#21487;&#33021;&#20026;&#20102;&#35299;&#22823;&#33041;&#21151;&#33021;&#25552;&#20379;&#35265;&#35299;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#33258;&#28982;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#31232;&#30095;&#24615;&#21644;&#36873;&#25321;&#24615;&#30340;&#21508;&#31181;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2201.11653v2 Announce Type: replace  Abstract: From the point of view of the human brain, continual learning can perform various tasks without mutual interference. An effective way to reduce mutual interference can be found in sparsity and selectivity of neurons. According to Aljundi et al. and Hadsell et al., imposing sparsity at the representational level is advantageous for continual learning because sparse neuronal activations encourage less overlap between parameters, resulting in less interference. Similarly, highly selective neural networks are likely to induce less interference since particular response in neurons will reduce the chance of overlap with other parameters. Considering that the human brain performs continual learning over the lifespan, finding conditions where sparsity and selectivity naturally arises may provide insight for understanding how the brain functions. This paper investigates various conditions that naturally increase sparsity and selectivity in a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21021;&#27493;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20116;&#20010;&#20851;&#38190;&#20027;&#39064;&#30340;&#20998;&#26512;&#21644;&#32531;&#35299;&#31574;&#30053;&#26469;&#24314;&#31435;&#31185;&#23398;&#30740;&#31350;&#20013;&#29983;&#25104;AI&#30340;&#20262;&#29702;&#25351;&#21335;&#12290;&#20840;&#29699;&#20849;&#35782;&#12289;&#19987;&#19994;&#22521;&#35757;&#21644;&#21512;&#29702;&#30340;&#25191;&#34892;&#23545;&#20110;&#20419;&#36827;AI&#30340;&#30410;&#22788;&#21644;&#32500;&#25252;&#30740;&#31350;&#35802;&#20449;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2401.15284</link><description>&lt;p&gt;
&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#24314;&#31435;&#29983;&#25104;AI&#30340;&#20262;&#29702;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Building ethical guidelines for generative AI in scientific research. (arXiv:2401.15284v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21021;&#27493;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20116;&#20010;&#20851;&#38190;&#20027;&#39064;&#30340;&#20998;&#26512;&#21644;&#32531;&#35299;&#31574;&#30053;&#26469;&#24314;&#31435;&#31185;&#23398;&#30740;&#31350;&#20013;&#29983;&#25104;AI&#30340;&#20262;&#29702;&#25351;&#21335;&#12290;&#20840;&#29699;&#20849;&#35782;&#12289;&#19987;&#19994;&#22521;&#35757;&#21644;&#21512;&#29702;&#30340;&#25191;&#34892;&#23545;&#20110;&#20419;&#36827;AI&#30340;&#30410;&#22788;&#21644;&#32500;&#25252;&#30740;&#31350;&#35802;&#20449;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#65288;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#27491;&#22312;&#36805;&#36895;&#25913;&#21464;&#23398;&#26415;&#30740;&#31350;&#21644;&#23454;&#38469;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#31185;&#23398;&#20013;&#29983;&#25104;AI&#30340;&#20262;&#29702;&#25351;&#21335;&#30340;&#35752;&#35770;&#20173;&#28982;&#38646;&#25955;&#65292;&#24378;&#35843;&#20102;&#21327;&#21830;&#19968;&#33268;&#24615;&#26631;&#20934;&#30340;&#32039;&#36843;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#20116;&#20010;&#20851;&#38190;&#20027;&#39064;&#30340;&#20998;&#26512;&#21644;&#32531;&#35299;&#31574;&#30053;&#30340;&#24320;&#21457;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#21021;&#27493;&#30340;&#26694;&#26550;&#65306;&#20102;&#35299;&#27169;&#22411;&#22312;&#30495;&#23454;&#24615;&#21644;&#20559;&#35265;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65307;&#23562;&#37325;&#38544;&#31169;&#12289;&#26426;&#23494;&#21644;&#29256;&#26435;&#65307;&#22312;&#34701;&#20837;&#27169;&#22411;&#36755;&#20986;&#26102;&#36991;&#20813;&#25220;&#34989;&#21644;&#36829;&#21453;&#25919;&#31574;&#65307;&#30830;&#20445;&#24212;&#29992;&#24102;&#26469;&#24635;&#20307;&#21033;&#30410;&#65307;&#20197;&#21450;&#36879;&#26126;&#12289;&#21487;&#22797;&#21046;&#22320;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#12290;&#36890;&#36807;&#21015;&#20030;&#24120;&#35265;&#22330;&#26223;&#26469;&#23637;&#31034;&#28508;&#22312;&#30340;&#20262;&#29702;&#36829;&#35268;&#34892;&#20026;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20840;&#29699;&#20849;&#35782;&#20197;&#21450;&#19987;&#19994;&#22521;&#35757;&#21644;&#21512;&#29702;&#30340;&#25191;&#34892;&#26159;&#20419;&#36827;AI&#30340;&#30410;&#22788;&#24182;&#32500;&#25252;&#30740;&#31350;&#35802;&#20449;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative artificial intelligence tools like large language models are rapidly transforming academic research and real world applications. However, discussions on ethical guidelines for generative AI in science remain fragmented, underscoring the urgent need for consensus based standards. This paper offers an initial framework by developing analyses and mitigation strategies across five key themes: understanding model limitations regarding truthfulness and bias; respecting privacy, confidentiality, and copyright; avoiding plagiarism and policy violations when incorporating model output; ensuring applications provide overall benefit; and using AI transparently and reproducibly. Common scenarios are outlined to demonstrate potential ethical violations. We argue that global consensus coupled with professional training and reasonable enforcement are critical to promoting the benefits of AI while safeguarding research integrity.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#25945;&#23398;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36845;&#20195;&#26426;&#22120;&#25945;&#23398;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#22686;&#24378;&#20102;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;&#26679;&#26412;&#23398;&#20064;&#22120;&#30340;&#27169;&#22359;&#21270;AI&#20195;&#29702;&#30340;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05467</link><description>&lt;p&gt;
&#22522;&#20110;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#27169;&#22359;&#21270;AI&#20195;&#29702;&#30340;&#26426;&#22120;&#25945;&#23398;
&lt;/p&gt;
&lt;p&gt;
Machine Teaching for Building Modular AI Agents based on Zero-shot Learners. (arXiv:2401.05467v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05467
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#25945;&#23398;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36845;&#20195;&#26426;&#22120;&#25945;&#23398;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#22686;&#24378;&#20102;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;&#26679;&#26412;&#23398;&#20064;&#22120;&#30340;&#27169;&#22359;&#21270;AI&#20195;&#29702;&#30340;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#23548;&#33268;&#20102;&#35768;&#22810;&#27169;&#22359;&#21270;AI&#20195;&#29702;&#30340;&#21019;&#24314;&#12290;&#36825;&#20123;&#20195;&#29702;&#20351;&#29992;LLMs&#20316;&#20026;&#38646;&#26679;&#26412;&#23398;&#20064;&#22120;&#65292;&#22312;&#20154;&#31867;&#29992;&#25143;&#35774;&#23450;&#30340;&#22797;&#26434;&#20219;&#21153;&#20013;&#25191;&#34892;&#23376;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#22686;&#24378;&#21033;&#29992;LLMs&#20316;&#20026;&#38646;&#26679;&#26412;&#23398;&#20064;&#22120;&#30340;&#27169;&#22359;&#21270;AI&#20195;&#29702;&#30340;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#36845;&#20195;&#26426;&#22120;&#25945;&#23398;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#26377;&#38480;&#30340;&#20154;&#31867;&#21453;&#39304;&#19979;&#36880;&#28176;&#25945;&#23548;AI&#20195;&#29702;&#30340;&#39640;&#25928;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#38646;&#26679;&#26412;&#23398;&#20064;&#36136;&#37327;&#38480;&#21046;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20027;&#24352;&#21033;&#29992;&#21021;&#22987;&#37096;&#32626;&#30340;&#25968;&#25454;&#36861;&#36394;&#20197;&#21450;&#38646;&#26679;&#26412;&#23398;&#20064;&#22120;&#30340;&#36755;&#20986;&#25110;&#27880;&#37322;&#26469;&#35757;&#32451;&#26356;&#23567;&#19988;&#20219;&#21153;&#29305;&#23450;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#21487;&#20197;&#20943;&#23569;&#32463;&#27982;&#25104;&#26412;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26426;&#22120;&#25945;&#23398;&#36807;&#31243;&#21033;&#29992;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#26469;&#32416;&#27491;&#39640;&#27010;&#29575;&#35823;&#26631;&#27880;&#30340;&#31034;&#20363;&#12290;&#22312;&#19977;&#20010;&#24120;&#35265;&#23545;&#35805;AI&#20195;&#29702;&#20219;&#21153;&#19978;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#25509;&#36817;&#29702;&#24819;&#24615;&#33021;&#21487;&#20197;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advances in large language models (LLMs) have led to the creation of many modular AI agents. These agents employ LLMs as zero-shot learners to perform sub-tasks in order to solve complex tasks set forth by human users. We propose an approach to enhance the robustness and performance of modular AI agents that utilize LLMs as zero-shot learners. Our iterative machine teaching method offers an efficient way to teach AI agents over time with limited human feedback, addressing the limit posed by the quality of zero-shot learning. We advocate leveraging the data traces from initial deployments and outputs or annotations from the zero-shot learners to train smaller and task-specific substitute models which can reduce both the monetary costs and environmental impact. Our machine teaching process avails human expertise to correct examples with a high likelihood of misannotations. Results on three tasks, common to conversational AI agents, show that close-to-oracle performance can be 
&lt;/p&gt;</description></item><item><title>Sum-of-Parts&#27169;&#22411;&#36890;&#36807;&#26500;&#36896;&#20445;&#35777;&#29305;&#24449;&#32452;&#24402;&#22240;&#30340;&#24544;&#23454;&#24615;&#65292;&#23558;&#39044;&#27979;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#30340;&#20998;&#25968;&#20043;&#21644;&#65292;&#24110;&#21161;&#22825;&#20307;&#29289;&#29702;&#23398;&#23478;&#21457;&#29616;&#20102;&#20851;&#20110;&#26143;&#31995;&#24418;&#25104;&#30340;&#26032;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2310.16316</link><description>&lt;p&gt;
Sum-of-Parts&#27169;&#22411;&#65306;&#23545;&#29305;&#24449;&#32452;&#30340;&#24544;&#23454;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
Sum-of-Parts Models: Faithful Attributions for Groups of Features. (arXiv:2310.16316v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16316
&lt;/p&gt;
&lt;p&gt;
Sum-of-Parts&#27169;&#22411;&#36890;&#36807;&#26500;&#36896;&#20445;&#35777;&#29305;&#24449;&#32452;&#24402;&#22240;&#30340;&#24544;&#23454;&#24615;&#65292;&#23558;&#39044;&#27979;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#30340;&#20998;&#25968;&#20043;&#21644;&#65292;&#24110;&#21161;&#22825;&#20307;&#29289;&#29702;&#23398;&#23478;&#21457;&#29616;&#20102;&#20851;&#20110;&#26143;&#31995;&#24418;&#25104;&#30340;&#26032;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#37322;&#20934;&#30830;&#21453;&#26144;&#20102;&#20854;&#20915;&#31574;&#36807;&#31243;&#65292;&#21017;&#34987;&#35748;&#20026;&#26159;&#8220;&#24544;&#23454;&#8221;&#30340;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#20363;&#22914;&#28145;&#24230;&#23398;&#20064;&#30340;&#29305;&#24449;&#24402;&#22240;&#31561;&#35299;&#37322;&#24182;&#19981;&#33021;&#20445;&#35777;&#24544;&#23454;&#65292;&#26377;&#21487;&#33021;&#20135;&#29983;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#35299;&#37322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Sum-of-Parts&#65288;SOP&#65289;&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#31867;&#27169;&#22411;&#65292;&#20854;&#39044;&#27979;&#20855;&#26377;&#36890;&#36807;&#26500;&#36896;&#20445;&#35777;&#24544;&#23454;&#30340;&#29305;&#24449;&#32452;&#24402;&#22240;&#12290;&#35813;&#27169;&#22411;&#23558;&#39044;&#27979;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#30340;&#20998;&#25968;&#20043;&#21644;&#65292;&#27599;&#20010;&#20998;&#25968;&#30452;&#25509;&#24402;&#22240;&#20110;&#19968;&#32452;&#31232;&#30095;&#29305;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#21487;&#35299;&#37322;&#24615;&#25351;&#26631;&#23545;SOP&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#22312;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#21033;&#29992;SOP&#25552;&#20379;&#30340;&#24544;&#23454;&#35299;&#37322;&#24110;&#21161;&#22825;&#20307;&#29289;&#29702;&#23398;&#23478;&#21457;&#29616;&#20102;&#20851;&#20110;&#26143;&#31995;&#24418;&#25104;&#30340;&#26032;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
An explanation of a machine learning model is considered "faithful" if it accurately reflects the model's decision-making process. However, explanations such as feature attributions for deep learning are not guaranteed to be faithful, and can produce potentially misleading interpretations. In this work, we develop Sum-of-Parts (SOP), a class of models whose predictions come with grouped feature attributions that are faithful-by-construction. This model decomposes a prediction into an interpretable sum of scores, each of which is directly attributable to a sparse group of features. We evaluate SOP on benchmarks with standard interpretability metrics, and in a case study, we use the faithful explanations from SOP to help astrophysicists discover new knowledge about galaxy formation.
&lt;/p&gt;</description></item><item><title>Ada-Instruct&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#25351;&#20196;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#23545;&#24320;&#28304;LLMs&#36827;&#34892;&#24494;&#35843;&#65292;&#33021;&#22815;&#29983;&#25104;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#38271;&#24230;&#22823;&#20110;&#31561;&#20110;100&#30340;&#25351;&#20196;&#12290;&#22312;&#20195;&#30721;&#34917;&#20840;&#12289;&#25968;&#23398;&#25512;&#29702;&#21644;&#24120;&#35782;&#25512;&#29702;&#31561;&#20219;&#21153;&#20013;&#65292;Ada-Instruct&#26174;&#31034;&#20986;&#20248;&#20110;&#22522;&#26412;&#27169;&#22411;&#21644;&#24403;&#21069;&#33258;&#25105;&#25351;&#23548;&#26041;&#27861;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.04484</link><description>&lt;p&gt;
Ada-Instruct: &#20026;&#22797;&#26434;&#25512;&#29702;&#35843;&#25972;&#25351;&#20196;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Ada-Instruct: Adapting Instruction Generators for Complex Reasoning. (arXiv:2310.04484v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04484
&lt;/p&gt;
&lt;p&gt;
Ada-Instruct&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#25351;&#20196;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#23545;&#24320;&#28304;LLMs&#36827;&#34892;&#24494;&#35843;&#65292;&#33021;&#22815;&#29983;&#25104;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#38271;&#24230;&#22823;&#20110;&#31561;&#20110;100&#30340;&#25351;&#20196;&#12290;&#22312;&#20195;&#30721;&#34917;&#20840;&#12289;&#25968;&#23398;&#25512;&#29702;&#21644;&#24120;&#35782;&#25512;&#29702;&#31561;&#20219;&#21153;&#20013;&#65292;Ada-Instruct&#26174;&#31034;&#20986;&#20248;&#20110;&#22522;&#26412;&#27169;&#22411;&#21644;&#24403;&#21069;&#33258;&#25105;&#25351;&#23548;&#26041;&#27861;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#22810;&#26679;&#19988;&#22797;&#26434;&#30340;&#25351;&#20196;&#23545;&#20110;&#25512;&#36827;&#25928;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#21033;&#29992;&#38381;&#28304;&#30340;LLMs&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#25552;&#31034;&#36827;&#34892;&#25351;&#20196;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#21457;&#29616;&#23545;&#20110;&#35832;&#22914;&#20195;&#30721;&#34917;&#20840;&#31561;&#20219;&#21153;&#65292;&#19978;&#19979;&#25991;&#25552;&#31034;&#26080;&#27861;&#29983;&#25104;&#38271;&#24230;&#22823;&#20110;&#31561;&#20110;100&#30340;&#22797;&#26434;&#25351;&#20196;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;Ada-Instruct&#65292;&#19968;&#31181;&#36890;&#36807;&#23545;&#24320;&#28304;LLMs&#36827;&#34892;&#24494;&#35843;&#30340;&#33258;&#36866;&#24212;&#25351;&#20196;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#21457;&#29616;&#34920;&#26126;&#65292;&#20165;&#20351;&#29992;&#21313;&#20010;&#26679;&#26412;&#23545;&#24320;&#28304;LLMs&#36827;&#34892;&#24494;&#35843;&#21363;&#21487;&#29983;&#25104;&#20445;&#25345;&#20998;&#24067;&#19968;&#33268;&#24615;&#30340;&#38271;&#25351;&#20196;&#65292;&#36866;&#29992;&#20110;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#20195;&#30721;&#34917;&#20840;&#12289;&#25968;&#23398;&#25512;&#29702;&#21644;&#24120;&#35782;&#25512;&#29702;&#31561;&#19981;&#21516;&#24212;&#29992;&#20013;&#23545;Ada-Instruct&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;&#32467;&#26524;&#26174;&#31034;Ada-Instruct&#20248;&#20110;&#20854;&#22522;&#26412;&#27169;&#22411;&#21644;&#24403;&#21069;&#30340;&#33258;&#25105;&#25351;&#23548;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating diverse and sophisticated instructions for downstream tasks by Large Language Models (LLMs) is pivotal for advancing the effect. Current approaches leverage closed-source LLMs, employing in-context prompting for instruction generation. However, in this paper, we found that in-context prompting cannot generate complex instructions with length $\ge 100$ for tasks like code completion.  To solve this problem, we introduce Ada-Instruct, an adaptive instruction generator developed by fine-tuning open-source LLMs. Our pivotal finding illustrates that fine-tuning open-source LLMs with a mere ten samples generates long instructions that maintain distributional consistency for complex reasoning tasks. We empirically validated Ada-Instruct's efficacy across different applications, including code completion, mathematical reasoning, and commonsense reasoning. The results underscore Ada-Instruct's superiority, evidencing its improvements over its base models, current self-instruct method
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20855;&#36523;&#22522;&#30784;&#20808;&#39564;&#30340;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#26469;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.02635</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22522;&#30784;&#65306;&#26397;&#21521;&#20855;&#26377;&#22522;&#30784;&#20808;&#39564;&#36741;&#21161;&#30340;&#20855;&#36523;&#36890;&#29992;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Foundation Reinforcement Learning: towards Embodied Generalist Agents with Foundation Prior Assistance. (arXiv:2310.02635v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20855;&#36523;&#22522;&#30784;&#20808;&#39564;&#30340;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#26469;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#20204;&#24050;&#32463;&#34920;&#26126;&#65292;&#20174;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#25968;&#25454;&#20013;&#36827;&#34892;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#26159;&#26500;&#24314;&#36890;&#29992;&#27169;&#22411;&#30340;&#20851;&#38190;&#65292;&#27491;&#22914;&#22312;NLP&#20013;&#25152;&#35265;&#12290;&#20026;&#20102;&#26500;&#24314;&#20855;&#36523;&#36890;&#29992;&#26234;&#33021;&#20307;&#65292;&#25105;&#20204;&#21644;&#35768;&#22810;&#20854;&#20182;&#30740;&#31350;&#32773;&#20551;&#35774;&#36825;&#31181;&#22522;&#30784;&#20808;&#39564;&#20063;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#22914;&#20309;&#20197;&#36866;&#24403;&#30340;&#20855;&#20307;&#24418;&#24335;&#34920;&#31034;&#36825;&#20123;&#20855;&#36523;&#22522;&#30784;&#20808;&#39564;&#65292;&#20197;&#21450;&#23427;&#20204;&#24212;&#35813;&#22914;&#20309;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#30452;&#35266;&#26377;&#25928;&#30340;&#20855;&#36523;&#20808;&#39564;&#65292;&#21253;&#25324;&#22522;&#30784;&#31574;&#30053;&#12289;&#20215;&#20540;&#21644;&#25104;&#21151;&#22870;&#21169;&#12290;&#25152;&#25552;&#20986;&#30340;&#20808;&#39564;&#26159;&#22522;&#20110;&#30446;&#26631;&#26465;&#20214;&#30340;MDP&#12290;&#20026;&#20102;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23454;&#20363;&#21270;&#20102;&#19968;&#20010;&#30001;&#36825;&#20123;&#20808;&#39564;&#36741;&#21161;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#65292;&#31216;&#20043;&#20026;&#22522;&#30784;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;FAC&#65289;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#21629;&#21517;&#20026;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#65288;FRL&#65289;&#65292;&#22240;&#20026;&#23427;&#23436;&#20840;&#20381;&#36182;&#20110;&#20855;&#36523;&#22522;&#30784;&#20808;&#39564;&#26469;&#36827;&#34892;&#25506;&#32034;&#12289;&#23398;&#20064;&#21644;&#24378;&#21270;&#12290;FRL&#30340;&#22909;&#22788;&#26377;&#19977;&#20010;&#12290;(1)&#26679;&#26412;&#25928;&#29575;&#39640;&#12290;&#36890;&#36807;&#22522;&#30784;&#20808;&#39564;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#65292;&#20943;&#23569;&#26679;&#26412;&#20351;&#29992;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, people have shown that large-scale pre-training from internet-scale data is the key to building generalist models, as witnessed in NLP. To build embodied generalist agents, we and many other researchers hypothesize that such foundation prior is also an indispensable component. However, it is unclear what is the proper concrete form to represent those embodied foundation priors and how they should be used in the downstream task. In this paper, we propose an intuitive and effective set of embodied priors that consist of foundation policy, value, and success reward. The proposed priors are based on the goal-conditioned MDP. To verify their effectiveness, we instantiate an actor-critic method assisted by the priors, called Foundation Actor-Critic (FAC). We name our framework as Foundation Reinforcement Learning (FRL), since it completely relies on embodied foundation priors to explore, learn and reinforce. The benefits of FRL are threefold. (1) Sample efficient. With foundation p
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21487;&#23454;&#29616;&#22238;&#24402;&#38382;&#39064;&#30340;PAC&#23398;&#20064;&#21644;&#22312;&#32447;&#23398;&#20064;&#30340;&#32479;&#35745;&#22797;&#26434;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#20110;&#21487;&#23398;&#20064;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2307.03848</link><description>&lt;p&gt;
&#21487;&#23454;&#29616;&#22238;&#24402;&#30340;&#26368;&#20248;&#23398;&#20064;&#31639;&#27861;&#65306;PAC&#23398;&#20064;&#21644;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimal Learners for Realizable Regression: PAC Learning and Online Learning. (arXiv:2307.03848v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21487;&#23454;&#29616;&#22238;&#24402;&#38382;&#39064;&#30340;PAC&#23398;&#20064;&#21644;&#22312;&#32447;&#23398;&#20064;&#30340;&#32479;&#35745;&#22797;&#26434;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#20110;&#21487;&#23398;&#20064;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#23545;&#21487;&#23454;&#29616;&#22238;&#24402;&#22312;PAC&#23398;&#20064;&#21644;&#22312;&#32447;&#23398;&#20064;&#30340;&#32479;&#35745;&#22797;&#26434;&#24230;&#36827;&#34892;&#21051;&#30011;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#26377;&#38480;&#30340;fat shattering&#32500;&#24230;&#23545;&#20110;PAC&#23398;&#20064;&#30340;&#20805;&#20998;&#24615;&#20197;&#21450;&#26377;&#38480;&#30340;scaled Natarajan&#32500;&#24230;&#23545;&#20110;&#24517;&#35201;&#24615;&#30340;&#23384;&#22312;&#65292;&#20294;&#33258;&#20174;Simon 1997&#65288;SICOMP '97&#65289;&#30340;&#24037;&#20316;&#20197;&#26469;&#65292;&#23545;&#20110;&#26356;&#23436;&#25972;&#30340;&#21051;&#30011;&#30340;&#36827;&#23637;&#29978;&#23569;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#26368;&#23567;&#21270;&#23454;&#20363;&#26368;&#20248;&#23398;&#20064;&#31639;&#27861;&#26469;&#23545;&#21487;&#23454;&#29616;&#22238;&#24402;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26082;&#23450;&#24615;&#21448;&#23450;&#37327;&#22320;&#21051;&#30011;&#20102;&#21738;&#20123;&#31867;&#30340;&#23454;&#25968;&#39044;&#27979;&#22120;&#21487;&#20197;&#34987;&#23398;&#20064;&#30340;&#26032;&#39062;&#32500;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20010;&#19982;&#22270;&#32500;&#24230;&#30456;&#20851;&#30340;&#32452;&#21512;&#32500;&#24230;&#65292;&#35813;&#32500;&#24230;&#21051;&#30011;&#20102;&#22312;&#21487;&#23454;&#29616;&#35774;&#32622;&#20013;&#30340;ERM&#21487;&#23398;&#20064;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#19982;DS&#32500;&#24230;&#30456;&#20851;&#30340;&#32452;&#21512;&#32500;&#24230;&#24314;&#31435;&#20102;&#23398;&#20064;&#21487;&#34892;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#24182;&#29468;&#27979;&#23427;&#20063;&#21487;&#33021;&#26159;&#20805;&#20998;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we aim to characterize the statistical complexity of realizable regression both in the PAC learning setting and the online learning setting.  Previous work had established the sufficiency of finiteness of the fat shattering dimension for PAC learnability and the necessity of finiteness of the scaled Natarajan dimension, but little progress had been made towards a more complete characterization since the work of Simon 1997 (SICOMP '97). To this end, we first introduce a minimax instance optimal learner for realizable regression and propose a novel dimension that both qualitatively and quantitatively characterizes which classes of real-valued predictors are learnable. We then identify a combinatorial dimension related to the Graph dimension that characterizes ERM learnability in the realizable setting. Finally, we establish a necessary condition for learnability based on a combinatorial dimension related to the DS dimension, and conjecture that it may also be sufficient in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21160;&#24577;&#35821;&#26009;&#24211;&#19978;&#30340;&#29983;&#25104;&#26816;&#32034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#38745;&#24577;&#35774;&#32622;&#19979;&#65292;&#29983;&#25104;&#26816;&#32034;&#25928;&#26524;&#20248;&#20110;&#21452;&#32534;&#30721;&#22120;&#65292;&#20294;&#22312;&#21160;&#24577;&#35774;&#32622;&#19979;&#24773;&#20917;&#30456;&#21453;&#12290;&#36890;&#36807;&#20351;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;DynamicGR&#22312;&#26032;&#30340;&#35821;&#26009;&#24211;&#19978;&#23637;&#29616;&#20986;&#20102;&#24847;&#22806;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18952</link><description>&lt;p&gt;
&#22312;&#21160;&#24577;&#35821;&#26009;&#24211;&#19978;&#25345;&#32493;&#26356;&#26032;&#29983;&#25104;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Continually Updating Generative Retrieval on Dynamic Corpora. (arXiv:2305.18952v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21160;&#24577;&#35821;&#26009;&#24211;&#19978;&#30340;&#29983;&#25104;&#26816;&#32034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#38745;&#24577;&#35774;&#32622;&#19979;&#65292;&#29983;&#25104;&#26816;&#32034;&#25928;&#26524;&#20248;&#20110;&#21452;&#32534;&#30721;&#22120;&#65292;&#20294;&#22312;&#21160;&#24577;&#35774;&#32622;&#19979;&#24773;&#20917;&#30456;&#21453;&#12290;&#36890;&#36807;&#20351;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;DynamicGR&#22312;&#26032;&#30340;&#35821;&#26009;&#24211;&#19978;&#23637;&#29616;&#20986;&#20102;&#24847;&#22806;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#20851;&#20110;&#20449;&#24687;&#26816;&#32034;(IR)&#30340;&#22823;&#22810;&#25968;&#30740;&#31350;&#20551;&#35774;&#35821;&#26009;&#24211;&#26159;&#38745;&#24577;&#30340;&#65292;&#32780;&#23454;&#38469;&#19990;&#30028;&#20013;&#30340;&#25991;&#26723;&#26159;&#19981;&#26029;&#26356;&#26032;&#30340;&#12290;&#26412;&#25991;&#23558;&#30693;&#35782;&#30340;&#21160;&#24577;&#24615;&#24341;&#20837;&#26816;&#32034;&#31995;&#32479;&#20013;&#65292;&#23558;&#26816;&#32034;&#35270;&#20026;&#21160;&#24577;&#30340;&#30693;&#35782;&#24211;&#65292;&#26356;&#31526;&#21512;&#30495;&#23454;&#29615;&#22659;&#12290;&#25105;&#20204;&#23545;&#21452;&#32534;&#30721;&#22120;&#21644;&#29983;&#25104;&#26816;&#32034;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#21033;&#29992;StreamingQA&#22522;&#20934;&#27979;&#35797;&#29992;&#20110;&#26102;&#24577;&#30693;&#35782;&#26356;&#26032;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#38745;&#24577;&#35774;&#32622;&#19979;&#65292;&#29983;&#25104;&#26816;&#32034;&#20248;&#20110;&#21452;&#32534;&#30721;&#22120;&#65292;&#20294;&#22312;&#21160;&#24577;&#35774;&#32622;&#19979;&#24773;&#20917;&#30456;&#21453;&#12290;&#28982;&#32780;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#24403;&#25105;&#20204;&#21033;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#22686;&#24378;&#29983;&#25104;&#26816;&#32034;&#23545;&#26032;&#35821;&#26009;&#24211;&#30340;&#36866;&#24212;&#24615;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;Dynamic Generative Retrieval (DynamicGR)&#23637;&#29616;&#20986;&#24847;&#22806;&#30340;&#21457;&#29616;&#12290;&#23427;&#33021;&#22815;&#22312;&#20854;&#20869;&#37096;&#32034;&#24341;&#20013;&#39640;&#25928;&#21387;&#32553;&#26032;&#30340;&#30693;&#35782;&#65292;
&lt;/p&gt;
&lt;p&gt;
The majority of prior work on information retrieval (IR) assumes that the corpus is static, whereas in the real world, the documents are continually updated. In this paper, we incorporate often overlooked dynamic nature of knowledge into the retrieval systems. Our work treats retrieval not as static archives but as dynamic knowledge bases better aligned with real-world environments. We conduct a comprehensive evaluation of dual encoders and generative retrieval, utilizing the StreamingQA benchmark designed for the temporal knowledge updates. Our initial results show that while generative retrieval outperforms dual encoders in static settings, the opposite is true in dynamic settings. Surprisingly, however, when we utilize a parameter-efficient pre-training method to enhance adaptability of generative retrieval to new corpora, our resulting model, Dynamic Generative Retrieval (DynamicGR), exhibits unexpected findings. It (1) efficiently compresses new knowledge in their internal index, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#22343;&#22330;&#25511;&#21046;(MFC)&#21644;&#22343;&#22330;&#21338;&#24328;(MFG)&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#20048;&#35266;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#31639;&#27861;&#65292;&#24182;&#20165;&#23545;&#36716;&#31227;&#21160;&#21147;&#23398;&#20855;&#26377;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20551;&#35774;&#65292;&#26368;&#21518;&#24314;&#31435;&#20102;&#19968;&#20010;&#25351;&#25968;&#32423;&#30340;&#19979;&#30028;&#25903;&#25345;MFC&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2305.11283</link><description>&lt;p&gt;
&#20851;&#20110;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#22343;&#22330;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
On the Statistical Efficiency of Mean Field Reinforcement Learning with General Function Approximation. (arXiv:2305.11283v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#22343;&#22330;&#25511;&#21046;(MFC)&#21644;&#22343;&#22330;&#21338;&#24328;(MFG)&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#20048;&#35266;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#31639;&#27861;&#65292;&#24182;&#20165;&#23545;&#36716;&#31227;&#21160;&#21147;&#23398;&#20855;&#26377;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20551;&#35774;&#65292;&#26368;&#21518;&#24314;&#31435;&#20102;&#19968;&#20010;&#25351;&#25968;&#32423;&#30340;&#19979;&#30028;&#25903;&#25345;MFC&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#22343;&#22330;&#25511;&#21046;&#65288;MFC&#65289;&#21644;&#22343;&#22330;&#21338;&#24328;&#65288;MFG&#65289;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#25928;&#29575;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;Mean-Field Model-Based Eluder Dimension (MBED)&#30340;&#26032;&#27010;&#24565;&#65292;&#21253;&#21547;&#20102;&#19968;&#31995;&#21015;&#20016;&#23500;&#30340;&#22343;&#22330;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20048;&#35266;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#36820;&#22238;&#19968;&#20010;$\epsilon$&#20248;&#30340;&#31574;&#30053;&#65292;&#36866;&#29992;&#20110;MFC&#25110;$\epsilon$&#32435;&#20160;&#22343;&#34913;&#31574;&#30053;&#36866;&#29992;&#20110;MFG&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#22810;&#39033;&#24335;&#19982;&#30456;&#20851;&#21442;&#25968;&#26080;&#20851;&#65292;&#19982;&#29366;&#24577;&#12289;&#21160;&#20316;&#21644;&#20195;&#29702;&#25968;&#37327;&#26080;&#20851;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#20165;&#23545;&#36716;&#31227;&#21160;&#21147;&#23398;&#20855;&#26377;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20551;&#35774;&#65292;&#36991;&#20813;&#20102;&#20197;&#21069;&#30340;&#24378;&#32467;&#26500;&#20551;&#35774;&#12290;&#26368;&#21518;&#65292;&#22312;tabular&#35774;&#32622;&#19979;&#65292;&#20551;&#35774;&#26377;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#25351;&#25968;&#32423;&#30340;&#19979;&#30028;&#25903;&#25345;MFC&#35774;&#32622;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26679;&#26412;&#39640;&#25928;&#30340;&#27169;&#22411;&#28040;&#38500;&#31639;&#27861;&#20197;&#36924;&#36817;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the statistical efficiency of Reinforcement Learning in Mean-Field Control (MFC) and Mean-Field Game (MFG) with general function approximation. We introduce a new concept called Mean-Field Model-Based Eluder Dimension (MBED), which subsumes a rich family of Mean-Field RL problems. Additionally, we propose algorithms based on Optimistic Maximal Likelihood Estimation, which can return an $\epsilon$-optimal policy for MFC or an $\epsilon$-Nash Equilibrium policy for MFG, with sample complexity polynomial w.r.t. relevant parameters and independent of the number of states, actions and the number of agents. Notably, our results only require a mild assumption of Lipschitz continuity on transition dynamics and avoid strong structural assumptions in previous work. Finally, in the tabular setting, given the access to a generative model, we establish an exponential lower bound for MFC setting, while providing a novel sample-efficient model elimination algorithm to approxim
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#31526;&#21495;&#35745;&#31639;&#26159;&#23558;&#31526;&#21495;&#21644;&#32479;&#35745;&#35748;&#30693;&#33539;&#24335;&#25972;&#21512;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#22312;&#25512;&#29702;&#21644;&#35299;&#37322;&#24615;&#20197;&#21450;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#31561;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#37325;&#35201;&#36129;&#29486;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#25104;&#21151;&#24212;&#29992;&#26696;&#20363;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2210.15889</link><description>&lt;p&gt;
&#36208;&#21521;&#25968;&#25454;&#21644;&#30693;&#35782;&#39537;&#21160;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#31070;&#32463;&#31526;&#21495;&#35745;&#31639;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Towards Data-and Knowledge-Driven Artificial Intelligence: A Survey on Neuro-Symbolic Computing. (arXiv:2210.15889v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15889
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#35745;&#31639;&#26159;&#23558;&#31526;&#21495;&#21644;&#32479;&#35745;&#35748;&#30693;&#33539;&#24335;&#25972;&#21512;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#22312;&#25512;&#29702;&#21644;&#35299;&#37322;&#24615;&#20197;&#21450;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#31561;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#37325;&#35201;&#36129;&#29486;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#25104;&#21151;&#24212;&#29992;&#26696;&#20363;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#26469;&#65292;&#31070;&#32463;&#31526;&#21495;&#35745;&#31639;&#65288;NeSy&#65289;&#19968;&#30452;&#26159;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39046;&#22495;&#30340;&#30740;&#31350;&#28909;&#28857;&#65292;&#36861;&#27714;&#31526;&#21495;&#21644;&#32479;&#35745;&#35748;&#30693;&#33539;&#24335;&#30340;&#25972;&#21512;&#12290;&#30001;&#20110;NeSy&#22312;&#31526;&#21495;&#34920;&#31034;&#30340;&#25512;&#29702;&#21644;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#22823;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#23427;&#21487;&#33021;&#25104;&#20026;&#19979;&#19968;&#20195;AI&#30340;&#20652;&#21270;&#21058;&#12290;&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;NeSy&#30740;&#31350;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#37325;&#35201;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#21382;&#21490;&#65292;&#28085;&#30422;&#20102;&#26089;&#26399;&#24037;&#20316;&#21644;&#22522;&#30784;&#30693;&#35782;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#32972;&#26223;&#27010;&#24565;&#65292;&#24182;&#30830;&#23450;&#20102;&#25512;&#21160;NeSy&#21457;&#23637;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25353;&#29031;&#20960;&#20010;&#20027;&#35201;&#29305;&#24449;&#23545;&#36817;&#26399;&#30340;&#37324;&#31243;&#30865;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#65292;&#21253;&#25324;&#31070;&#32463;&#31526;&#21495;&#25972;&#21512;&#12289;&#30693;&#35782;&#34920;&#31034;&#12289;&#30693;&#35782;&#23884;&#20837;&#21644;&#21151;&#33021;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#31616;&#35201;&#35752;&#35770;&#20102;&#25104;&#21151;&#30340;&#24212;&#29992;&#26696;&#20363;&#65292;&#20197;&#21450;NeSy&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural-symbolic computing (NeSy), which pursues the integration of the symbolic and statistical paradigms of cognition, has been an active research area of Artificial Intelligence (AI) for many years. As NeSy shows promise of reconciling the advantages of reasoning and interpretability of symbolic representation and robust learning in neural networks, it may serve as a catalyst for the next generation of AI. In the present paper, we provide a systematic overview of the recent developments and important contributions of NeSy research. Firstly, we introduce study history of this area, covering early work and foundations. We further discuss background concepts and identify key driving factors behind the development of NeSy. Afterward, we categorize recent landmark approaches along several main characteristics that underline this research paradigm, including neural-symbolic integration, knowledge representation, knowledge embedding, and functionality. Next, we briefly discuss the successfu
&lt;/p&gt;</description></item></channel></rss>