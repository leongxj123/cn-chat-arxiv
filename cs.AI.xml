<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26032;&#39062;&#30340;&#26694;&#26550;SFFormer&#65292;&#32467;&#21512;&#20102;&#22810;&#22836;&#20132;&#21449;&#27880;&#24847;&#21147;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#65292;&#22522;&#20110;dMRI&#32420;&#32500;&#26463;&#36861;&#36394;&#65292;&#39044;&#27979;&#20102;&#20027;&#35266;&#35821;&#35328;&#34920;&#29616;&#65292;&#25299;&#23637;&#20102;&#33041;&#32467;&#26500;&#19982;&#20154;&#31867;&#35748;&#30693;&#21151;&#33021;&#30340;&#20851;&#32852;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.19001</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#30340;&#32420;&#32500;&#31751;&#24418;&#29366;&#20998;&#26512;&#29992;&#20110;&#35821;&#35328;&#34920;&#29616;&#35748;&#30693;&#20998;&#25968;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Cross--domain Fiber Cluster Shape Analysis for Language Performance Cognitive Score Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26032;&#39062;&#30340;&#26694;&#26550;SFFormer&#65292;&#32467;&#21512;&#20102;&#22810;&#22836;&#20132;&#21449;&#27880;&#24847;&#21147;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#65292;&#22522;&#20110;dMRI&#32420;&#32500;&#26463;&#36861;&#36394;&#65292;&#39044;&#27979;&#20102;&#20027;&#35266;&#35821;&#35328;&#34920;&#29616;&#65292;&#25299;&#23637;&#20102;&#33041;&#32467;&#26500;&#19982;&#20154;&#31867;&#35748;&#30693;&#21151;&#33021;&#30340;&#20851;&#32852;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24418;&#29366;&#22312;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#20013;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#65292;&#25552;&#20379;&#20102;&#26377;&#20851;&#23545;&#35937;&#24418;&#24577;&#21644;&#21151;&#33021;&#30340;&#20449;&#24687;&#29305;&#24449;&#12290;&#33041;&#25104;&#20687;&#20013;&#30340;&#24418;&#29366;&#20998;&#26512;&#21487;&#24110;&#21161;&#35299;&#37322;&#20154;&#33041;&#32467;&#26500;&#21644;&#21151;&#33021;&#30340;&#30456;&#20851;&#24615;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#33041;&#30340;3D&#30333;&#36136;&#36830;&#25509;&#30340;&#24418;&#29366;&#21450;&#20854;&#19982;&#20154;&#31867;&#35748;&#30693;&#21151;&#33021;&#30340;&#28508;&#22312;&#39044;&#27979;&#20851;&#31995;&#12290;&#25105;&#20204;&#20351;&#29992;&#25193;&#25955;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;dMRI&#65289;&#32420;&#32500;&#26463;&#36861;&#36394;&#23558;&#22823;&#33041;&#36830;&#25509;&#37325;&#24314;&#20026;3D&#28857;&#24207;&#21015;&#12290;&#20026;&#20102;&#25551;&#36848;&#27599;&#20010;&#36830;&#25509;&#65292;&#25105;&#20204;&#25552;&#21462;&#20102;12&#20010;&#24418;&#29366;&#25551;&#36848;&#31526;&#20197;&#21450;&#20256;&#32479;&#30340;dMRI&#36830;&#25509;&#21644;&#32452;&#32455;&#24494;&#32467;&#26500;&#29305;&#24449;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#24418;&#29366;&#34701;&#21512;&#32420;&#32500;&#31751;&#21464;&#25442;&#22120;&#65288;SFFormer&#65289;&#65292;&#21033;&#29992;&#22810;&#22836;&#20132;&#21449;&#27880;&#24847;&#21147;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#22522;&#20110;dMRI&#32420;&#32500;&#26463;&#36861;&#36394;&#26469;&#39044;&#27979;&#29305;&#23450;&#20010;&#20307;&#30340;&#35821;&#35328;&#34920;&#29616;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19001v1 Announce Type: cross  Abstract: Shape plays an important role in computer graphics, offering informative features to convey an object's morphology and functionality. Shape analysis in brain imaging can help interpret structural and functionality correlations of the human brain. In this work, we investigate the shape of the brain's 3D white matter connections and its potential predictive relationship to human cognitive function. We reconstruct brain connections as sequences of 3D points using diffusion magnetic resonance imaging (dMRI) tractography. To describe each connection, we extract 12 shape descriptors in addition to traditional dMRI connectivity and tissue microstructure features. We introduce a novel framework, Shape--fused Fiber Cluster Transformer (SFFormer), that leverages a multi-head cross-attention feature fusion module to predict subject-specific language performance based on dMRI tractography. We assess the performance of the method on a large dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#23545;&#25239;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#20998;&#20139;&#25193;&#25955;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#38544;&#31169;&#21644;&#20844;&#24179;&#39118;&#38505;&#65292;&#29305;&#21035;&#26159;&#25506;&#35752;&#20102;&#22312;&#19968;&#26041;&#20351;&#29992;&#31169;&#20154;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#21518;&#25552;&#20379;&#32473;&#21478;&#19968;&#26041;&#40657;&#30418;&#35775;&#38382;&#26435;&#38480;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.18607</link><description>&lt;p&gt;
&#22312;&#20998;&#20139;&#25193;&#25955;&#27169;&#22411;&#20013;&#25506;&#35752;&#38544;&#31169;&#21644;&#20844;&#24179;&#39118;&#38505;&#65306;&#19968;&#31181;&#23545;&#25239;&#24615;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Exploring Privacy and Fairness Risks in Sharing Diffusion Models: An Adversarial Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#23545;&#25239;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#20998;&#20139;&#25193;&#25955;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#38544;&#31169;&#21644;&#20844;&#24179;&#39118;&#38505;&#65292;&#29305;&#21035;&#26159;&#25506;&#35752;&#20102;&#22312;&#19968;&#26041;&#20351;&#29992;&#31169;&#20154;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#21518;&#25552;&#20379;&#32473;&#21478;&#19968;&#26041;&#40657;&#30418;&#35775;&#38382;&#26435;&#38480;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#36817;&#24180;&#26469;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#20854;&#22312;&#37319;&#26679;&#36136;&#37327;&#21644;&#20998;&#24067;&#35206;&#30422;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#36328;&#19981;&#21516;&#32452;&#32455;&#20998;&#20139;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#24314;&#35758;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#21033;&#29992;&#29575;&#21516;&#26102;&#36890;&#36807;&#36991;&#20813;&#30452;&#25509;&#20998;&#20139;&#31169;&#20154;&#25968;&#25454;&#26469;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;&#19982;&#36825;&#31181;&#26041;&#27861;&#30456;&#20851;&#30340;&#28508;&#22312;&#39118;&#38505;&#23578;&#26410;&#24471;&#21040;&#20840;&#38754;&#35843;&#26597;&#12290;&#26412;&#25991;&#20174;&#23545;&#25239;&#24615;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#19982;&#20998;&#20139;&#25193;&#25955;&#27169;&#22411;&#30456;&#20851;&#30340;&#28508;&#22312;&#38544;&#31169;&#21644;&#20844;&#24179;&#39118;&#38505;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19968;&#26041;&#65288;&#20998;&#20139;&#32773;&#65289;&#20351;&#29992;&#31169;&#20154;&#25968;&#25454;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#24182;&#21521;&#21478;&#19968;&#26041;&#65288;&#25509;&#25910;&#32773;&#65289;&#25552;&#20379;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#40657;&#30418;&#35775;&#38382;&#26435;&#38480;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20998;&#20139;&#32773;&#21487;&#20197;&#23454;&#34892;&#30340;&#34892;&#21160;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18607v1 Announce Type: cross  Abstract: Diffusion models have recently gained significant attention in both academia and industry due to their impressive generative performance in terms of both sampling quality and distribution coverage. Accordingly, proposals are made for sharing pre-trained diffusion models across different organizations, as a way of improving data utilization while enhancing privacy protection by avoiding sharing private data directly. However, the potential risks associated with such an approach have not been comprehensively examined.   In this paper, we take an adversarial perspective to investigate the potential privacy and fairness risks associated with the sharing of diffusion models. Specifically, we investigate the circumstances in which one party (the sharer) trains a diffusion model using private data and provides another party (the receiver) black-box access to the pre-trained model for downstream tasks. We demonstrate that the sharer can execut
&lt;/p&gt;</description></item><item><title>&#31526;&#21495;&#22238;&#24402;&#34987;&#35270;&#20026;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24341;&#20837;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.18603</link><description>&lt;p&gt;
MMSR&#65306;&#31526;&#21495;&#22238;&#24402;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
MMSR: Symbolic Regression is a Multimodal Task
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18603
&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#34987;&#35270;&#20026;&#19968;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24341;&#20837;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#20844;&#24335;&#26159;&#25506;&#32034;&#33258;&#28982;&#35268;&#24459;&#20960;&#21315;&#24180;&#26469;&#20154;&#31867;&#26234;&#24935;&#30340;&#32467;&#26230;&#12290;&#29992;&#31616;&#27905;&#30340;&#25968;&#23398;&#20844;&#24335;&#25551;&#36848;&#22797;&#26434;&#30340;&#33258;&#28982;&#35268;&#24459;&#26159;&#31185;&#23398;&#23478;&#19981;&#26029;&#36861;&#27714;&#30340;&#30446;&#26631;&#65292;&#20063;&#26159;&#20154;&#24037;&#26234;&#33021;&#38754;&#20020;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#19968;&#39046;&#22495;&#34987;&#31216;&#20026;&#31526;&#21495;&#22238;&#24402;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#20174;&#25968;&#25454;&#21040;&#34920;&#36798;&#24335;&#30340;&#26144;&#23556;&#35270;&#20026;&#32763;&#35793;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#30456;&#24212;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18603v1 Announce Type: cross  Abstract: Mathematical formulas are the crystallization of human wisdom in exploring the laws of nature for thousands of years. Describing the complex laws of nature with a concise mathematical formula is a constant pursuit of scientists and a great challenge for artificial intelligence. This field is called symbolic regression. Symbolic regression was originally formulated as a combinatorial optimization problem, and GP and reinforcement learning algorithms were used to solve it. However, GP is sensitive to hyperparameters, and these two types of algorithms are inefficient. To solve this problem, researchers treat the mapping from data to expressions as a translation problem. And the corresponding large-scale pre-trained model is introduced. However, the data and expression skeletons do not have very clear word correspondences as the two languages do. Instead, they are more like two modalities (e.g., image and text). Therefore, in this paper, w
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Cobweb4V&#30340;&#26032;&#39062;&#35270;&#35273;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#31867;&#31867;&#20284;&#23398;&#20064;&#31995;&#32479;&#65292;&#36991;&#20813;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#25928;&#24212;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#26469;&#23454;&#29616;&#26377;&#25928;&#23398;&#20064;&#25104;&#26524;&#65292;&#24182;&#20445;&#25345;&#31283;&#23450;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.16933</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#31867;&#27010;&#24565;&#24418;&#25104;&#36991;&#20813;&#35270;&#35273;&#20998;&#31867;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Avoiding Catastrophic Forgetting in Visual Classification Using Human Concept Formation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16933
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Cobweb4V&#30340;&#26032;&#39062;&#35270;&#35273;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#31867;&#31867;&#20284;&#23398;&#20064;&#31995;&#32479;&#65292;&#36991;&#20813;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#25928;&#24212;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#26469;&#23454;&#29616;&#26377;&#25928;&#23398;&#20064;&#25104;&#26524;&#65292;&#24182;&#20445;&#25345;&#31283;&#23450;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#65292;&#28982;&#32780;&#65292;&#24403;&#25353;&#39034;&#24207;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#65292;&#23427;&#20204;&#32463;&#24120;&#38754;&#20020;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Cobweb4V&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#35273;&#20998;&#31867;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;Cobweb&#65292;&#36825;&#26159;&#19968;&#31181;&#20154;&#31867;&#31867;&#20284;&#30340;&#23398;&#20064;&#31995;&#32479;&#65292;&#21463;&#21040;&#20154;&#31867;&#38543;&#26102;&#38388;&#36880;&#28176;&#23398;&#20064;&#26032;&#27010;&#24565;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;Cobweb4V&#22312;&#23398;&#20064;&#35270;&#35273;&#27010;&#24565;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;&#23398;&#20064;&#25104;&#26524;&#65292;&#38543;&#26102;&#38388;&#20445;&#25345;&#31283;&#23450;&#30340;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#20102;&#20196;&#20154;&#31216;&#36190;&#30340;&#28176;&#36817;&#34892;&#20026;&#65292;&#36991;&#20813;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#25928;&#24212;&#12290;&#36825;&#20123;&#29305;&#24449;&#19982;&#20154;&#31867;&#35748;&#30693;&#20013;&#30340;&#23398;&#20064;&#31574;&#30053;&#19968;&#33268;&#65292;&#23558;Cobweb4V&#23450;&#20301;&#20026;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16933v1 Announce Type: cross  Abstract: Deep neural networks have excelled in machine learning, particularly in vision tasks, however, they often suffer from catastrophic forgetting when learning new tasks sequentially. In this work, we propose Cobweb4V, a novel visual classification approach that builds on Cobweb, a human like learning system that is inspired by the way humans incrementally learn new concepts over time. In this research, we conduct a comprehensive evaluation, showcasing the proficiency of Cobweb4V in learning visual concepts, requiring less data to achieve effective learning outcomes compared to traditional methods, maintaining stable performance over time, and achieving commendable asymptotic behavior, without catastrophic forgetting effects. These characteristics align with learning strategies in human cognition, positioning Cobweb4V as a promising alternative to neural network approaches.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#21387;&#21147;&#27979;&#35797;&#26041;&#27861;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#31995;&#32479;&#25506;&#32034;&#21487;&#33021;&#23548;&#33268;&#39640;&#36895;&#20844;&#36335;&#20132;&#36890;&#22330;&#26223;&#20013;&#23433;&#20840;&#38382;&#39064;&#30340;&#36793;&#30028;&#24773;&#20917;</title><link>https://arxiv.org/abs/2402.11813</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#21387;&#21147;&#27979;&#35797;&#39640;&#36895;&#20844;&#36335;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A novel framework for adaptive stress testing of autonomous vehicles in highways
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11813
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#21387;&#21147;&#27979;&#35797;&#26041;&#27861;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#31995;&#32479;&#25506;&#32034;&#21487;&#33021;&#23548;&#33268;&#39640;&#36895;&#20844;&#36335;&#20132;&#36890;&#22330;&#26223;&#20013;&#23433;&#20840;&#38382;&#39064;&#30340;&#36793;&#30028;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#35777;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#30340;&#23433;&#20840;&#36816;&#34892;&#23545;&#20110;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#21644;&#20844;&#20247;&#25509;&#21463;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#19981;&#20165;&#23545;AV&#36827;&#34892;&#26631;&#20934;&#23433;&#20840;&#27979;&#35797;&#30340;&#35780;&#20272;&#65292;&#36824;&#21457;&#29616;&#21487;&#33021;&#23548;&#33268;&#19981;&#23433;&#20840;&#34892;&#20026;&#25110;&#24773;&#20917;&#30340;&#34987;&#27979;&#35797;AV&#30340;&#28508;&#22312;&#36793;&#30028;&#24773;&#20917;&#20855;&#26377;&#26497;&#20854;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#22320;&#25506;&#32034;&#21487;&#33021;&#23548;&#33268;&#39640;&#36895;&#20844;&#36335;&#20132;&#36890;&#22330;&#26223;&#20013;&#23433;&#20840;&#38382;&#39064;&#30340;&#36793;&#30028;&#24773;&#20917;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#19968;&#31181;&#33258;&#36866;&#24212;&#21387;&#21147;&#27979;&#35797;&#65288;AST&#65289;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#21046;&#23450;&#22330;&#26223;&#20197;&#21450;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#21457;&#29616;&#20195;&#34920;&#36793;&#30028;&#24773;&#20917;&#30340;&#29702;&#24819;&#27169;&#24335;&#30340;&#26032;&#20852;&#39564;&#35777;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20026;DRL&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20197;&#25351;&#23548;AST&#26681;&#25454;&#34987;&#27979;&#35797;AV&#65288;&#21363;&#33258;&#36710;&#65289;&#19982;&#20854;&#20182;&#36710;&#36742;&#20043;&#38388;&#30340;&#30896;&#25758;&#27010;&#29575;&#20272;&#35745;&#26469;&#35782;&#21035;&#30896;&#25758;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11813v1 Announce Type: cross  Abstract: Guaranteeing the safe operations of autonomous vehicles (AVs) is crucial for their widespread adoption and public acceptance. It is thus of a great significance to not only assess the AV against the standard safety tests, but also discover potential corner cases of the AV under test that could lead to unsafe behaviour or scenario. In this paper, we propose a novel framework to systematically explore corner cases that can result in safety concerns in a highway traffic scenario. The framework is based on an adaptive stress testing (AST) approach, an emerging validation method that leverages a Markov decision process to formulate the scenarios and deep reinforcement learning (DRL) to discover the desirable patterns representing corner cases. To this end, we develop a new reward function for DRL to guide the AST in identifying crash scenarios based on the collision probability estimate between the AV under test (i.e., the ego vehicle) and 
&lt;/p&gt;</description></item><item><title /><link>https://arxiv.org/abs/2402.07127</link><description>&lt;p&gt;
&#35266;&#23519;&#23398;&#20064;&#65306;&#22522;&#20110;&#35270;&#39057;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#23398;&#20064;&#26041;&#27861;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Learning by Watching: A Review of Video-based Learning Approaches for Robot Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07127
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#23398;&#20064;&#25805;&#20316;&#25216;&#33021;&#21463;&#21040;&#22810;&#26679;&#21270;&#12289;&#26080;&#20559;&#30340;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#24615;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#38382;&#39064;&#65292;&#20294;&#22312;&#27867;&#21270;&#24615;&#21644;&#29616;&#23454;&#19990;&#30028;&#30340;&#36716;&#31227;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#8220;&#37326;&#22806;&#8221;&#35270;&#39057;&#25968;&#25454;&#38598;&#30340;&#22823;&#35268;&#27169;&#23384;&#22312;&#36890;&#36807;&#33258;&#30417;&#30563;&#25216;&#26415;&#25512;&#21160;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#36827;&#23637;&#12290;&#23558;&#36825;&#19968;&#28857;&#24212;&#29992;&#21040;&#26426;&#22120;&#20154;&#39046;&#22495;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#36890;&#36807;&#34987;&#21160;&#35266;&#23519;&#26469;&#23398;&#20064;&#20016;&#23500;&#30340;&#22312;&#32447;&#35270;&#39057;&#20013;&#30340;&#25805;&#20316;&#25216;&#33021;&#12290;&#36825;&#31181;&#22522;&#20110;&#35270;&#39057;&#30340;&#23398;&#20064;&#33539;&#24335;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#23427;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#30340;&#30417;&#30563;&#26041;&#27861;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#25968;&#25454;&#38598;&#30340;&#20559;&#35265;&#12290;&#26412;&#32508;&#36848;&#22238;&#39038;&#20102;&#35270;&#39057;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#12289;&#29289;&#20307;&#21487;&#34892;&#24615;&#29702;&#35299;&#12289;&#19977;&#32500;&#25163;&#37096;/&#36523;&#20307;&#24314;&#27169;&#21644;&#22823;&#35268;&#27169;&#26426;&#22120;&#20154;&#36164;&#28304;&#31561;&#22522;&#30784;&#30693;&#35782;&#65292;&#20197;&#21450;&#20174;&#19981;&#21463;&#25511;&#21046;&#30340;&#35270;&#39057;&#28436;&#31034;&#20013;&#33719;&#21462;&#26426;&#22120;&#20154;&#25805;&#20316;&#25216;&#33021;&#30340;&#26032;&#20852;&#25216;&#26415;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20165;&#20174;&#35266;&#23519;&#22823;&#35268;&#27169;&#20154;&#31867;&#35270;&#39057;&#20013;&#23398;&#20064;&#22914;&#20309;&#22686;&#24378;&#26426;&#22120;&#20154;&#30340;&#27867;&#21270;&#24615;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robot learning of manipulation skills is hindered by the scarcity of diverse, unbiased datasets. While curated datasets can help, challenges remain in generalizability and real-world transfer. Meanwhile, large-scale "in-the-wild" video datasets have driven progress in computer vision through self-supervised techniques. Translating this to robotics, recent works have explored learning manipulation skills by passively watching abundant videos sourced online. Showing promising results, such video-based learning paradigms provide scalable supervision while reducing dataset bias. This survey reviews foundations such as video feature representation learning techniques, object affordance understanding, 3D hand/body modeling, and large-scale robot resources, as well as emerging techniques for acquiring robot manipulation skills from uncontrolled video demonstrations. We discuss how learning only from observing large-scale human videos can enhance generalization and sample efficiency for roboti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35770;&#36848;&#20102;&#34913;&#37327;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#24544;&#35802;&#24230;&#25110;&#33258;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#19968;&#33268;&#24615;&#27979;&#35797;&#26469;&#35780;&#20272;&#35299;&#37322;&#30340;&#36755;&#20986;&#32423;&#21035;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#27604;&#36739;&#19968;&#33268;&#24615;&#27979;&#35797;&#24211;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#33258;&#19968;&#33268;&#24615;&#24230;&#37327;CC-SHAP&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#35266;&#28857;&#12290;</title><link>https://arxiv.org/abs/2311.07466</link><description>&lt;p&gt;
&#20851;&#20110;&#34913;&#37327;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#24544;&#35802;&#24230;&#25110;&#33258;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
On Measuring Faithfulness or Self-consistency of Natural Language Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35770;&#36848;&#20102;&#34913;&#37327;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#24544;&#35802;&#24230;&#25110;&#33258;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#19968;&#33268;&#24615;&#27979;&#35797;&#26469;&#35780;&#20272;&#35299;&#37322;&#30340;&#36755;&#20986;&#32423;&#21035;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#27604;&#36739;&#19968;&#33268;&#24615;&#27979;&#35797;&#24211;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#33258;&#19968;&#33268;&#24615;&#24230;&#37327;CC-SHAP&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#36890;&#36807;&#20107;&#21518;&#25110;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#35299;&#37322;&#20854;&#39044;&#27979;&#12290;&#20294;&#26159;&#65292;LLM&#21487;&#33021;&#20250;&#32534;&#36896;&#21548;&#36215;&#26469;&#21512;&#29702;&#20294;&#19981;&#24544;&#23454;&#20110;&#20854;&#22522;&#26412;&#25512;&#29702;&#30340;&#35299;&#37322;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#35774;&#35745;&#20102;&#26088;&#22312;&#21028;&#26029;&#20107;&#21518;&#25110;CoT&#35299;&#37322;&#24544;&#23454;&#24230;&#30340;&#27979;&#35797;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#24544;&#23454;&#24230;&#27979;&#35797;&#19981;&#26159;&#34913;&#37327;&#27169;&#22411;&#20869;&#37096;&#24037;&#20316;&#30340;&#24544;&#23454;&#24230;&#65292;&#32780;&#26159;&#34913;&#37327;&#20854;&#36755;&#20986;&#32423;&#21035;&#30340;&#33258;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;i&#65289;&#25105;&#20204;&#22312;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#32972;&#26223;&#19979;&#28548;&#28165;&#20102;&#24544;&#23454;&#24230;&#27979;&#35797;&#30340;&#22320;&#20301;&#65292;&#23558;&#20854;&#25551;&#36848;&#20026;&#33258;&#19968;&#33268;&#24615;&#27979;&#35797;&#12290;&#25105;&#20204;&#36890;&#36807;ii&#65289;&#26500;&#24314;&#20102;&#19968;&#20010;&#27604;&#36739;&#19968;&#33268;&#24615;&#30340;&#27979;&#35797;&#24211;&#65292;&#39318;&#27425;&#22312;11&#20010;&#24320;&#25918;&#24335;LLMs&#21644;5&#20010;&#20219;&#21153;&#30340;&#36890;&#29992;&#22871;&#20214;&#19978;&#27604;&#36739;&#20102;&#29616;&#26377;&#27979;&#35797;&#65292;&#21253;&#25324;iii&#65289;&#25105;&#20204;&#30340;&#26032;&#30340;&#33258;&#19968;&#33268;&#24615;&#24230;&#37327;CC-SHAP&#12290;CC-SHAP&#26159;LLM&#33258;&#19968;&#33268;&#24615;&#30340;&#32454;&#31890;&#24230;&#24230;&#37327;&#65288;&#32780;&#19981;&#26159;&#27979;&#35797;&#65289;&#12290;&#23427;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can explain their predictions through post-hoc or Chain-of-Thought (CoT) explanations. But an LLM could make up reasonably sounding explanations that are unfaithful to its underlying reasoning. Recent work has designed tests that aim to judge the faithfulness of post-hoc or CoT explanations. In this work we argue that these faithfulness tests do not measure faithfulness to the models' inner workings -- but rather their self-consistency at output level. Our contributions are three-fold: i) We clarify the status of faithfulness tests in view of model explainability, characterising them as self-consistency tests instead. This assessment we underline by ii) constructing a Comparative Consistency Bank for self-consistency tests that for the first time compares existing tests on a common suite of 11 open LLMs and 5 tasks -- including iii) our new self-consistency measure CC-SHAP. CC-SHAP is a fine-grained measure (not a test) of LLM self-consistency. It compares 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#22810;&#28304;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#65288;UniMS-RAG&#65289;&#65292;&#36890;&#36807;&#32479;&#19968;&#30693;&#35782;&#28304;&#36873;&#25321;&#12289;&#30693;&#35782;&#26816;&#32034;&#21644;&#22238;&#22797;&#29983;&#25104;&#19977;&#20010;&#23376;&#20219;&#21153;&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#38656;&#27714;&#33258;&#36866;&#24212;&#22320;&#26816;&#32034;&#35777;&#25454;&#21644;&#35780;&#20272;&#20851;&#32852;&#24615;&#65292;&#20174;&#32780;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#22238;&#22797;&#12290;</title><link>http://arxiv.org/abs/2401.13256</link><description>&lt;p&gt;
UniMS-RAG: &#29992;&#20110;&#20010;&#24615;&#21270;&#23545;&#35805;&#31995;&#32479;&#30340;&#32479;&#19968;&#22810;&#28304;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for Personalized Dialogue Systems. (arXiv:2401.13256v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13256
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#22810;&#28304;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#65288;UniMS-RAG&#65289;&#65292;&#36890;&#36807;&#32479;&#19968;&#30693;&#35782;&#28304;&#36873;&#25321;&#12289;&#30693;&#35782;&#26816;&#32034;&#21644;&#22238;&#22797;&#29983;&#25104;&#19977;&#20010;&#23376;&#20219;&#21153;&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#38656;&#27714;&#33258;&#36866;&#24212;&#22320;&#26816;&#32034;&#35777;&#25454;&#21644;&#35780;&#20272;&#20851;&#32852;&#24615;&#65292;&#20174;&#32780;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#22238;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#23545;&#35805;&#31995;&#32479;&#20013;&#28041;&#21450;&#21040;&#22810;&#20010;&#20449;&#24687;&#28304;&#26102;&#65292;&#20010;&#24615;&#21270;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#20196;&#20154;&#21521;&#24448;&#30340;&#23646;&#24615;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#35745;&#21010;&#21644;&#25972;&#21512;&#22810;&#20010;&#20449;&#24687;&#28304;&#22312;&#29983;&#25104;&#20010;&#24615;&#21270;&#22238;&#22797;&#20013;&#30340;&#20351;&#29992;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#20854;&#20998;&#35299;&#20026;&#19977;&#20010;&#23376;&#20219;&#21153;&#65306;&#30693;&#35782;&#28304;&#36873;&#25321;&#12289;&#30693;&#35782;&#26816;&#32034;&#21644;&#22238;&#22797;&#29983;&#25104;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#22810;&#28304;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#65288;UniMS-RAG&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#30456;&#21516;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#33539;&#24335;&#23558;&#36825;&#19977;&#20010;&#23376;&#20219;&#21153;&#32479;&#19968;&#36215;&#26469;&#65292;&#36890;&#36807;&#20351;&#29992;&#29305;&#27530;&#30340;&#20196;&#29260;&#65292;&#21363;&#34892;&#21160;&#20196;&#29260;&#21644;&#35780;&#20272;&#20196;&#29260;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#26816;&#32034;&#35777;&#25454;&#24182;&#35780;&#20272;&#20851;&#32852;&#24615;&#12290;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#34892;&#21160;&#20196;&#29260;&#26377;&#21161;&#20110;&#19982;&#21508;&#31181;&#30693;&#35782;&#28304;&#36827;&#34892;&#20132;&#20114;&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#20854;&#19978;&#19979;&#25991;&#21644;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#22238;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) has shown exceptional capabilities in many natual language understanding and generation tasks. However, the personalization issue still remains a much-coveted property, especially when it comes to the multiple sources involved in the dialogue system. To better plan and incorporate the use of multiple sources in generating personalized response, we firstly decompose it into three sub-tasks: Knowledge Source Selection, Knowledge Retrieval, and Response Generation. We then propose a novel Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG) Specifically, we unify these three sub-tasks with different formulations into the same sequence-to-sequence paradigm during the training, to adaptively retrieve evidences and evaluate the relevance on-demand using special tokens, called acting tokens and evaluation tokens. Enabling language models to generate acting tokens facilitates interaction with various knowledge sources, allowing them to adapt their
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#36947;&#24503;&#20154;&#24037;&#26234;&#33021;&#21407;&#21017;&#21644;&#25351;&#21335;&#26469;&#35299;&#20915;&#39640;&#32423;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27835;&#29702;&#21644;&#21033;&#29992;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10745</link><description>&lt;p&gt;
&#23545;&#39640;&#32423;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27835;&#29702;&#21644;&#21033;&#29992;&#30340;&#36947;&#24503;&#20154;&#24037;&#26234;&#33021;&#21407;&#21017;&#21644;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Ethical Artificial Intelligence Principles and Guidelines for the Governance and Utilization of Highly Advanced Large Language Models. (arXiv:2401.10745v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#36947;&#24503;&#20154;&#24037;&#26234;&#33021;&#21407;&#21017;&#21644;&#25351;&#21335;&#26469;&#35299;&#20915;&#39640;&#32423;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27835;&#29702;&#21644;&#21033;&#29992;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;ChatGPT&#12289;LaMDA&#21644;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25104;&#21151;&#65292;&#25216;&#26415;&#34892;&#19994;&#21644;&#20854;&#20182;&#34892;&#19994;&#23545;LLMs&#30340;&#24320;&#21457;&#21644;&#20351;&#29992;&#26377;&#25152;&#22686;&#21152;&#12290;&#34429;&#28982;LLMs&#30340;&#27700;&#24179;&#23578;&#26410;&#36229;&#36807;&#20154;&#31867;&#26234;&#33021;&#65292;&#20294;&#24635;&#26377;&#19968;&#22825;&#20250;&#36798;&#21040;&#36825;&#19968;&#28857;&#12290;&#36825;&#31181;LLMs&#21487;&#20197;&#31216;&#20026;&#39640;&#32423;LLMs&#12290;&#30446;&#21069;&#65292;&#30001;&#20110;&#23578;&#26410;&#36798;&#21040;&#36825;&#19968;&#28857;&#65292;&#20351;&#29992;&#36947;&#24503;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21407;&#21017;&#21644;&#25351;&#21335;&#26469;&#35299;&#20915;&#39640;&#32423;LLMs&#30340;&#38382;&#39064;&#36824;&#21463;&#21040;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#36825;&#26159;&#19968;&#20010;&#38382;&#39064;&#65292;&#22240;&#20026;&#19968;&#26086;&#36798;&#21040;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#23558;&#26080;&#27861;&#20805;&#20998;&#20934;&#22791;&#22909;&#20197;&#36947;&#24503;&#21644;&#26368;&#20339;&#26041;&#24335;&#22788;&#29702;&#20854;&#20135;&#29983;&#30340;&#21518;&#26524;&#65292;&#36825;&#23558;&#23548;&#33268;&#19981;&#21487;&#39044;&#26399;&#30340;&#21518;&#26524;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#21033;&#29992;&#36947;&#24503;&#20154;&#24037;&#26234;&#33021;&#21407;&#21017;&#21644;&#25351;&#21335;&#26469;&#35299;&#20915;&#39640;&#32423;LLMs&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the success of ChatGPT, LaMDA and other large language models (LLMs), there has been an increase in development and usage of LLMs within the technology sector and other sectors. While the level in which LLMs has not reached a level where it has surpassed human intelligence, there will be a time when it will. Such LLMs can be referred to as advanced LLMs. Currently, there are limited usage of ethical artificial intelligence (AI) principles and guidelines addressing advanced LLMs due to the fact that we have not reached that point yet. However, this is a problem as once we do reach that point, we will not be adequately prepared to deal with the aftermath of it in an ethical and optimal way, which will lead to undesired and unexpected consequences. This paper addresses this issue by discussing what ethical AI principles and guidelines can be used to address highly advanced LLMs.
&lt;/p&gt;</description></item><item><title>&#24037;&#19994;&#30028;&#30340;&#30740;&#31350;&#22242;&#38431;&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#26377;&#26356;&#39640;&#30340;&#20851;&#27880;&#24230;&#21644;&#24341;&#29992;&#29575;&#65292;&#19988;&#26356;&#26377;&#21487;&#33021;&#20135;&#29983;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;&#32780;&#23398;&#26415;&#30028;&#30340;&#22242;&#38431;&#21017;&#26356;&#20542;&#21521;&#20110;&#20135;&#29983;&#20855;&#26377;&#26356;&#39640;&#31243;&#24230;&#21019;&#26032;&#30340;&#24037;&#20316;&#65292;&#20986;&#29616;&#38750;&#24120;&#19981;&#23547;&#24120;&#21644;&#20856;&#22411;&#30340;&#35770;&#25991;&#12290;&#36825;&#31181;&#24433;&#21709;&#21147;-&#21019;&#26032;&#24230;&#20248;&#21183;&#22312;&#19981;&#21516;&#39046;&#22495;&#12289;&#22242;&#38431;&#35268;&#27169;&#12289;&#36164;&#21382;&#21644;&#22768;&#26395;&#19979;&#22343;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2401.10268</link><description>&lt;p&gt;
&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#23545;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#20114;&#34917;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;
The complementary contributions of academia and industry to AI research. (arXiv:2401.10268v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10268
&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;&#30028;&#30340;&#30740;&#31350;&#22242;&#38431;&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#26377;&#26356;&#39640;&#30340;&#20851;&#27880;&#24230;&#21644;&#24341;&#29992;&#29575;&#65292;&#19988;&#26356;&#26377;&#21487;&#33021;&#20135;&#29983;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;&#32780;&#23398;&#26415;&#30028;&#30340;&#22242;&#38431;&#21017;&#26356;&#20542;&#21521;&#20110;&#20135;&#29983;&#20855;&#26377;&#26356;&#39640;&#31243;&#24230;&#21019;&#26032;&#30340;&#24037;&#20316;&#65292;&#20986;&#29616;&#38750;&#24120;&#19981;&#23547;&#24120;&#21644;&#20856;&#22411;&#30340;&#35770;&#25991;&#12290;&#36825;&#31181;&#24433;&#21709;&#21147;-&#21019;&#26032;&#24230;&#20248;&#21183;&#22312;&#19981;&#21516;&#39046;&#22495;&#12289;&#22242;&#38431;&#35268;&#27169;&#12289;&#36164;&#21382;&#21644;&#22768;&#26395;&#19979;&#22343;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#24037;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#20013;&#37117;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#24037;&#19994;&#30028;&#36817;&#26399;&#30340;&#31361;&#30772;&#24615;&#36827;&#23637;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#23398;&#26415;&#30740;&#31350;&#22312;&#35813;&#39046;&#22495;&#20013;&#30340;&#20316;&#29992;&#30340;&#26032;&#35270;&#35282;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23545;&#36807;&#21435;25&#24180;&#20013;&#20004;&#20010;&#29615;&#22659;&#20013;&#20135;&#29983;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#24433;&#21709;&#21644;&#31867;&#22411;&#36827;&#34892;&#20102;&#25551;&#36848;&#65292;&#24182;&#24314;&#31435;&#20102;&#20960;&#31181;&#27169;&#24335;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30001;&#24037;&#19994;&#30028;&#30740;&#31350;&#20154;&#21592;&#32452;&#25104;&#30340;&#22242;&#38431;&#21457;&#34920;&#30340;&#25991;&#31456;&#24448;&#24448;&#26356;&#21463;&#20851;&#27880;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#34987;&#24341;&#29992;&#21644;&#24341;&#21457;&#24341;&#29992;&#39072;&#35206;&#30340;&#21487;&#33021;&#24615;&#65292;&#19988;&#26356;&#26377;&#21487;&#33021;&#20135;&#29983;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#32431;&#23398;&#26415;&#22242;&#38431;&#21457;&#34920;&#20102;&#22823;&#37096;&#20998;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#65292;&#24182;&#20542;&#21521;&#20110;&#20135;&#29983;&#26356;&#39640;&#31243;&#24230;&#30340;&#21019;&#26032;&#24037;&#20316;&#65292;&#21333;&#31687;&#35770;&#25991;&#26377;&#25968;&#20493;&#30340;&#21487;&#33021;&#24615;&#26159;&#38750;&#24120;&#19981;&#23547;&#24120;&#21644;&#20856;&#22411;&#30340;&#12290;&#24037;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#22312;&#24433;&#21709;&#21147;-&#21019;&#26032;&#24230;&#26041;&#38754;&#30340;&#20248;&#21183;&#19981;&#21463;&#23376;&#39046;&#22495;&#12289;&#22242;&#38431;&#35268;&#27169;&#12289;&#36164;&#21382;&#21644;&#22768;&#26395;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#23398;&#26415;&#30028;&#20135;&#29983;&#20102;&#26356;&#22810;&#32508;&#36848;&#21644;&#20998;&#26512;&#22411;&#35770;&#25991;&#65292;&#32780;&#24037;&#19994;&#30028;&#21017;&#26356;&#22810;&#22320;&#27880;&#37325;&#24212;&#29992;&#21644;&#25216;&#26415;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) has seen tremendous development in industry and academia. However, striking recent advances by industry have stunned the world, inviting a fresh perspective on the role of academic research in this field. Here, we characterize the impact and type of AI produced by both environments over the last 25 years and establish several patterns. We find that articles published by teams consisting exclusively of industry researchers tend to get greater attention, with a higher chance of being highly cited and citation-disruptive, and several times more likely to produce state-of-the-art models. In contrast, we find that exclusively academic teams publish the bulk of AI research and tend to produce higher novelty work, with single papers having several times higher likelihood of being unconventional and atypical. The respective impact-novelty advantages of industry and academia are robust to controls for subfield, team size, seniority, and prestige. We find that academ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Magmaw&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#36827;&#34892;&#27169;&#24577;&#19981;&#21487;&#30693;&#23545;&#25239;&#25915;&#20987;&#30340;&#40657;&#30418;&#25915;&#20987;&#26041;&#27861;&#12290;&#23427;&#33021;&#22815;&#29983;&#25104;&#36890;&#29992;&#30340;&#23545;&#25239;&#25200;&#21160;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#25915;&#20987;&#30446;&#26631;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#23545;&#29616;&#26377;&#38450;&#24481;&#26041;&#27861;&#30340;&#38887;&#24615;&#12290;&#20351;&#29992;&#23454;&#26102;&#26080;&#32447;&#25915;&#20987;&#24179;&#21488;&#36827;&#34892;&#20102;&#27010;&#24565;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2311.00207</link><description>&lt;p&gt;
Magmaw: &#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Magmaw: Modality-Agnostic Adversarial Attacks on Machine Learning-Based Wireless Communication Systems. (arXiv:2311.00207v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Magmaw&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#36827;&#34892;&#27169;&#24577;&#19981;&#21487;&#30693;&#23545;&#25239;&#25915;&#20987;&#30340;&#40657;&#30418;&#25915;&#20987;&#26041;&#27861;&#12290;&#23427;&#33021;&#22815;&#29983;&#25104;&#36890;&#29992;&#30340;&#23545;&#25239;&#25200;&#21160;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#25915;&#20987;&#30446;&#26631;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#23545;&#29616;&#26377;&#38450;&#24481;&#26041;&#27861;&#30340;&#38887;&#24615;&#12290;&#20351;&#29992;&#23454;&#26102;&#26080;&#32447;&#25915;&#20987;&#24179;&#21488;&#36827;&#34892;&#20102;&#27010;&#24565;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#21512;&#24182;&#31471;&#21040;&#31471;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#30340;&#25152;&#26377;&#29289;&#29702;&#23618;&#27169;&#22359;&#20197;&#23454;&#29616;&#32852;&#21512;&#25910;&#21457;&#22120;&#20248;&#21270;&#26041;&#38754;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#35768;&#22810;&#38024;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26080;&#32447;&#31995;&#32479;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#24182;&#26410;&#25552;&#20379;&#21253;&#25324;&#28304;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#12289;&#20849;&#21516;&#30340;&#29289;&#29702;&#23618;&#32452;&#20214;&#21644;&#26080;&#32447;&#39046;&#22495;&#32422;&#26463;&#22312;&#20869;&#30340;&#20840;&#38754;&#35270;&#35282;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Magmaw&#65292;&#36825;&#26159;&#19968;&#31181;&#33021;&#22815;&#38024;&#23545;&#36890;&#36807;&#26080;&#32447;&#20449;&#36947;&#20256;&#36755;&#30340;&#20219;&#20309;&#22810;&#27169;&#24577;&#20449;&#21495;&#29983;&#25104;&#36890;&#29992;&#23545;&#25239;&#25200;&#21160;&#30340;&#40657;&#30418;&#25915;&#20987;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#19979;&#28216;&#24212;&#29992;&#30340;&#23545;&#25239;&#25915;&#20987;&#24341;&#20837;&#20102;&#26032;&#30340;&#30446;&#26631;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#25915;&#20987;&#23545;&#29616;&#26377;&#24191;&#27867;&#20351;&#29992;&#30340;&#23545;&#25239;&#35757;&#32451;&#21644;&#25200;&#21160;&#20449;&#21495;&#20943;&#27861;&#38450;&#24481;&#26041;&#27861;&#30340;&#38887;&#24615;&#12290;&#20026;&#20102;&#27010;&#24565;&#35777;&#26126;&#65292;&#25105;&#20204;&#20351;&#29992;&#36719;&#20214;&#23450;&#20041;&#26080;&#32447;&#30005;&#31995;&#32479;&#26500;&#24314;&#20102;&#19968;&#20010;&#23454;&#26102;&#26080;&#32447;&#25915;&#20987;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) has been instrumental in enabling joint transceiver optimization by merging all physical layer blocks of the end-to-end wireless communication systems. Although there have been a number of adversarial attacks on ML-based wireless systems, the existing methods do not provide a comprehensive view including multi-modality of the source data, common physical layer components, and wireless domain constraints. This paper proposes Magmaw, the first black-box attack methodology capable of generating universal adversarial perturbations for any multimodal signal transmitted over a wireless channel. We further introduce new objectives for adversarial attacks on ML-based downstream applications. The resilience of the attack to the existing widely used defense methods of adversarial training and perturbation signal subtraction is experimentally verified. For proof-of-concept evaluation, we build a real-time wireless attack platform using a software-defined radio system. Experi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#33258;&#21160;&#20272;&#31639;&#33258;&#35748;&#25216;&#26415;&#20538;&#21153;&#30340;&#36824;&#27454;&#24037;&#20316;&#37327;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#33258;&#35748;&#25216;&#26415;&#20538;&#21153;&#38656;&#35201;&#19981;&#21516;&#31243;&#24230;&#30340;&#36824;&#27454;&#24037;&#20316;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.06020</link><description>&lt;p&gt;
&#33258;&#21160;&#35780;&#20272;&#20607;&#36824;&#33258;&#35748;&#25216;&#26415;&#20538;&#21153;&#25152;&#38656;&#30340;&#24037;&#20316;&#37327;
&lt;/p&gt;
&lt;p&gt;
Automatically Estimating the Effort Required to Repay Self-Admitted Technical Debt. (arXiv:2309.06020v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#33258;&#21160;&#20272;&#31639;&#33258;&#35748;&#25216;&#26415;&#20538;&#21153;&#30340;&#36824;&#27454;&#24037;&#20316;&#37327;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#33258;&#35748;&#25216;&#26415;&#20538;&#21153;&#38656;&#35201;&#19981;&#21516;&#31243;&#24230;&#30340;&#36824;&#27454;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25216;&#26415;&#20538;&#21153;&#26159;&#25351;&#22312;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#20013;&#20026;&#20102;&#30701;&#26399;&#21033;&#30410;&#32780;&#20570;&#20986;&#30340;&#27425;&#20248;&#20915;&#31574;&#25152;&#24102;&#26469;&#30340;&#21518;&#26524;&#12290;&#33258;&#35748;&#25216;&#26415;&#20538;&#21153;(SATD)&#26159;&#19968;&#31181;&#29305;&#23450;&#24418;&#24335;&#30340;&#25216;&#26415;&#20538;&#21153;&#65292;&#24320;&#21457;&#20154;&#21592;&#26126;&#30830;&#22320;&#22312;&#36719;&#20214;&#30340;&#28304;&#20195;&#30721;&#27880;&#37322;&#21644;&#25552;&#20132;&#28040;&#24687;&#20013;&#35760;&#24405;&#19979;&#26469;&#12290;&#30001;&#20110;SATD&#21487;&#33021;&#38459;&#30861;&#36719;&#20214;&#30340;&#24320;&#21457;&#21644;&#32500;&#25252;&#65292;&#22240;&#27492;&#26377;&#25928;&#22320;&#35299;&#20915;&#21644;&#20248;&#20808;&#22788;&#29702;&#23427;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#32570;&#20047;&#26681;&#25454;SATD&#30340;&#25991;&#26412;&#25551;&#36848;&#33258;&#21160;&#35780;&#20272;&#20854;&#36824;&#27454;&#24037;&#20316;&#37327;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#19968;&#20010;&#21253;&#25324;1,060&#20010;Apache&#20195;&#30721;&#24211;&#20013;&#20849;2,568,728&#20010;&#25552;&#20132;&#30340;341,740&#20010;SATD&#39033;&#30446;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#26469;&#33258;&#21160;&#20272;&#31639;SATD&#36824;&#27454;&#24037;&#20316;&#37327;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;SATD&#38656;&#35201;&#19981;&#21516;&#31243;&#24230;&#30340;&#36824;&#27454;&#24037;&#20316;&#37327;&#65292;&#20854;&#20013;&#20195;&#30721;/&#35774;&#35745;&#12289;&#38656;&#27714;&#21644;&#27979;&#35797;&#20538;&#21153;&#38656;&#35201;&#26356;&#22810;&#30340;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Technical debt refers to the consequences of sub-optimal decisions made during software development that prioritize short-term benefits over long-term maintainability. Self-Admitted Technical Debt (SATD) is a specific form of technical debt, explicitly documented by developers within software artifacts such as source code comments and commit messages. As SATD can hinder software development and maintenance, it is crucial to address and prioritize it effectively. However, current methodologies lack the ability to automatically estimate the repayment effort of SATD based on its textual descriptions. To address this limitation, we propose a novel approach for automatically estimating SATD repayment effort, utilizing a comprehensive dataset comprising 341,740 SATD items from 2,568,728 commits across 1,060 Apache repositories. Our findings show that different types of SATD require varying levels of repayment effort, with code/design, requirement, and test debt demanding greater effort compa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;3D&#27169;&#25311;&#36229;&#20998;&#36776;&#29575;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#25928;&#12289;&#36924;&#30495;&#22320;&#22686;&#24378;&#20302;&#25104;&#26412;&#12289;&#23454;&#26102;&#29289;&#29702;&#27169;&#25311;&#20135;&#29983;&#30340;&#38754;&#37096;&#34920;&#29616;&#65292;&#20351;&#20854;&#25509;&#36817;&#20110;&#20855;&#26377;&#26356;&#39640;&#20998;&#36776;&#29575;&#21644;&#20934;&#30830;&#29289;&#29702;&#24314;&#27169;&#30340;&#21442;&#32771;&#36136;&#37327;&#31163;&#32447;&#27169;&#25311;&#22120;&#12290;</title><link>http://arxiv.org/abs/2305.03216</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;3D&#27169;&#25311;&#36229;&#20998;&#36776;&#29575;&#23454;&#29616;&#36817;&#23454;&#26102;&#38754;&#37096;&#21160;&#30011;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Near-realtime Facial Animation by Deep 3D Simulation Super-Resolution. (arXiv:2305.03216v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03216
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;3D&#27169;&#25311;&#36229;&#20998;&#36776;&#29575;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#25928;&#12289;&#36924;&#30495;&#22320;&#22686;&#24378;&#20302;&#25104;&#26412;&#12289;&#23454;&#26102;&#29289;&#29702;&#27169;&#25311;&#20135;&#29983;&#30340;&#38754;&#37096;&#34920;&#29616;&#65292;&#20351;&#20854;&#25509;&#36817;&#20110;&#20855;&#26377;&#26356;&#39640;&#20998;&#36776;&#29575;&#21644;&#20934;&#30830;&#29289;&#29702;&#24314;&#27169;&#30340;&#21442;&#32771;&#36136;&#37327;&#31163;&#32447;&#27169;&#25311;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#25311;&#36229;&#20998;&#36776;&#29575;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#25928;&#12289;&#36924;&#30495;&#22320;&#22686;&#24378;&#20302;&#25104;&#26412;&#12289;&#23454;&#26102;&#29289;&#29702;&#27169;&#25311;&#20135;&#29983;&#30340;&#38754;&#37096;&#34920;&#29616;&#65292;&#20351;&#20854;&#25509;&#36817;&#20110;&#20855;&#26377;&#26356;&#39640;&#20998;&#36776;&#29575;&#65288;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#39640;&#36798;26&#20493;&#30340;&#20803;&#32032;&#25968;&#65289;&#21644;&#20934;&#30830;&#29289;&#29702;&#24314;&#27169;&#30340;&#21442;&#32771;&#36136;&#37327;&#31163;&#32447;&#27169;&#25311;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28304;&#20110;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#26500;&#24314;&#19968;&#32452;&#37197;&#23545;&#24103;&#24207;&#21015;&#30340;&#33021;&#21147;&#65292;&#36825;&#20123;&#24207;&#21015;&#20998;&#21035;&#26469;&#33258;&#20110;&#20302;&#20998;&#36776;&#29575;&#21644;&#39640;&#20998;&#36776;&#29575;&#27169;&#25311;&#22120;&#65292;&#24182;&#19988;&#22312;&#35821;&#20041;&#19978;&#30456;&#20114;&#23545;&#24212;&#12290;&#25105;&#20204;&#20197;&#38754;&#37096;&#21160;&#30011;&#20026;&#20363;&#65292;&#21019;&#36896;&#36825;&#31181;&#35821;&#20041;&#19968;&#33268;&#24615;&#30340;&#26041;&#24335;&#23601;&#26159;&#22312;&#20004;&#20010;&#27169;&#25311;&#22120;&#20013;&#35843;&#25972;&#21516;&#26679;&#30340;&#32908;&#32905;&#28608;&#27963;&#25511;&#21046;&#21644;&#39592;&#26550;&#23039;&#21183;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#36229;&#20998;&#36776;&#29575;&#26694;&#26550;&#20174;&#36825;&#20010;&#35757;&#32451;&#38598;&#20013;&#27867;&#21270;&#21040;&#30475;&#19981;&#35265;&#30340;&#34920;&#24773;&#65292;&#24182;&#19988;&#34917;&#20607;&#20004;&#20010;&#27169;&#25311;&#20043;&#38388;&#30340;&#24314;&#27169;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a neural network-based simulation super-resolution framework that can efficiently and realistically enhance a facial performance produced by a low-cost, realtime physics-based simulation to a level of detail that closely approximates that of a reference-quality off-line simulator with much higher resolution (26x element count in our examples) and accurate physical modeling. Our approach is rooted in our ability to construct - via simulation - a training set of paired frames, from the low- and high-resolution simulators respectively, that are in semantic correspondence with each other. We use face animation as an exemplar of such a simulation domain, where creating this semantic congruence is achieved by simply dialing in the same muscle actuation controls and skeletal pose in the two simulators. Our proposed neural network super-resolution framework generalizes from this training set to unseen expressions, compensates for modeling discrepancies between the two simulations du
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36866;&#24212;&#25552;&#31034;&#21644;&#38646;&#21021;&#22987;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36731;&#37327;&#32423;&#35821;&#35328;&#27169;&#22411;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#39640;&#25928;&#24494;&#35843;LLaMA&#20026;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#65292;&#20855;&#26377;&#27604;Alpaca&#26356;&#30701;&#30340;&#24494;&#35843;&#26102;&#38388;&#24182;&#20855;&#26377;&#36817;&#20284;&#30340;&#21709;&#24212;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.16199</link><description>&lt;p&gt;
LLaMA-Adapter: &#38646;&#21021;&#22987;&#21270;&#27880;&#24847;&#21147;&#19979;&#30340;&#35821;&#35328;&#27169;&#22411;&#31934;&#32454;&#35843;&#25972;&#30340;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention. (arXiv:2303.16199v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36866;&#24212;&#25552;&#31034;&#21644;&#38646;&#21021;&#22987;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36731;&#37327;&#32423;&#35821;&#35328;&#27169;&#22411;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#39640;&#25928;&#24494;&#35843;LLaMA&#20026;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#65292;&#20855;&#26377;&#27604;Alpaca&#26356;&#30701;&#30340;&#24494;&#35843;&#26102;&#38388;&#24182;&#20855;&#26377;&#36817;&#20284;&#30340;&#21709;&#24212;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA-Adapter&#36825;&#19968;&#36731;&#37327;&#32423;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;LLaMA&#39640;&#25928;&#22320;&#24494;&#35843;&#20026;&#19968;&#20010;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#12290;&#21033;&#29992;52K&#20010;&#33258;&#25105;&#25351;&#23548;&#31034;&#33539;&#65292;LLaMA-Adapter&#20165;&#22312;&#20923;&#32467;&#30340;LLaMA 7B&#27169;&#22411;&#19978;&#24341;&#20837;&#20102;1.2M&#20010;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#24182;&#19988;&#22312;8&#20010;A100 GPU&#19978;&#20165;&#32791;&#26102;&#19981;&#21040;&#19968;&#20010;&#23567;&#26102;&#36827;&#34892;&#24494;&#35843;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#32452;&#21487;&#23398;&#20064;&#30340;&#36866;&#24212;&#25552;&#31034;&#65292;&#24182;&#22312;&#36739;&#39640;&#30340;&#21464;&#21387;&#22120;&#23618;&#20013;&#23558;&#23427;&#20204;&#39044;&#32622;&#20110;&#36755;&#20837;&#25991;&#26412;&#20196;&#29260;&#20043;&#21069;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#21021;&#22987;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#38646;&#38376;&#25511;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#23558;&#26032;&#30340;&#25351;&#20196;&#25552;&#31034;&#27880;&#20837;LLaMA&#65292;&#24182;&#26377;&#25928;&#22320;&#20445;&#30041;&#20102;&#20854;&#39044;&#20808;&#35757;&#32451;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#39640;&#25928;&#35757;&#32451;&#65292;LLaMA-Adapter&#33021;&#22815;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#21709;&#24212;&#65292;&#19982;&#23436;&#20840;&#24494;&#35843;&#30340;7B&#21442;&#25968;&#30340;Alpaca&#30456;&#20284;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#21487;&#20197;&#31616;&#21333;&#22320;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#36755;&#20837;&#65292;&#20363;&#22914;&#22270;&#20687;&#65292;&#29992;&#20110;&#22270;&#20687;&#30456;&#20851;&#30340;LLaMA&#65292;&#22312;ScienceQA&#19978;&#23454;&#29616;&#20102;&#26356;&#24378;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;https://github.com/ZrrSkywalker/LLaMA-Adapt&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the input text tokens at higher transformer layers. Then, a zero-init attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. With efficient training, LLaMA-Adapter generates high-quality responses, comparable to Alpaca with fully fine-tuned 7B parameters. Furthermore, our approach can be simply extended to multi-modal input, e.g., images, for image-conditioned LLaMA, which achieves superior reasoning capacity on ScienceQA. We release our code at https://github.com/ZrrSkywalker/LLaMA-Adapt
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#23614;&#24179;&#22343;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#23545;&#26102;&#24207;&#24046;&#24322;(TD)&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#26377;&#38480;&#26102;&#38388;&#34892;&#20026;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#23614;&#24179;&#22343;TD&#33021;&#20197;&#26368;&#20248;&#36895;&#29575; $O(1/t)$ &#25910;&#25947;&#65292;&#24182;&#19988;&#21021;&#22987;&#35823;&#24046;&#34928;&#20943;&#36895;&#29575;&#26356;&#24555;&#12290;&#27492;&#22806;&#65292;&#27491;&#21017;&#21270;&#30340;TD&#29256;&#26412;&#22312;&#20855;&#26377;&#30149;&#24577;&#29305;&#24449;&#30340;&#38382;&#39064;&#19978;&#24456;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2210.05918</link><description>&lt;p&gt;
&#26377;&#38480;&#26102;&#38388;&#20869;&#20351;&#29992;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#36827;&#34892;&#26102;&#24207;&#24046;&#24322;&#23398;&#20064;&#30340;&#20998;&#26512;&#65306;&#23614;&#24179;&#22343;&#21644;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Finite time analysis of temporal difference learning with linear function approximation: Tail averaging and regularisation. (arXiv:2210.05918v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#23614;&#24179;&#22343;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#23545;&#26102;&#24207;&#24046;&#24322;(TD)&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#26377;&#38480;&#26102;&#38388;&#34892;&#20026;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#23614;&#24179;&#22343;TD&#33021;&#20197;&#26368;&#20248;&#36895;&#29575; $O(1/t)$ &#25910;&#25947;&#65292;&#24182;&#19988;&#21021;&#22987;&#35823;&#24046;&#34928;&#20943;&#36895;&#29575;&#26356;&#24555;&#12290;&#27492;&#22806;&#65292;&#27491;&#21017;&#21270;&#30340;TD&#29256;&#26412;&#22312;&#20855;&#26377;&#30149;&#24577;&#29305;&#24449;&#30340;&#38382;&#39064;&#19978;&#24456;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#27969;&#34892;&#30340;&#26102;&#24207;&#24046;&#24322;(TD)&#23398;&#20064;&#31639;&#27861;&#19982;&#23614;&#24179;&#22343;&#30456;&#32467;&#21512;&#26102;&#30340;&#26377;&#38480;&#26102;&#38388;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#19981;&#38656;&#35201;&#20851;&#20110;&#24213;&#23618;&#25237;&#24433;TD&#19981;&#21160;&#28857;&#30697;&#38453;&#30340;&#29305;&#24449;&#20540;&#20449;&#24687;&#30340;&#27493;&#38271;&#36873;&#25321;&#19979;&#65292;&#25512;&#23548;&#20102;&#23614;&#24179;&#22343;TD&#36845;&#20195;&#30340;&#21442;&#25968;&#35823;&#24046;&#30340;&#26377;&#38480;&#26102;&#38388;&#30028;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#23614;&#24179;&#22343;TD&#20197;&#26399;&#26395;&#36895;&#29575;&#21644;&#39640;&#27010;&#29575;&#25910;&#25947;&#20110;&#26368;&#20248;&#30340; $O(1/t)$ &#36895;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#23637;&#31034;&#20102;&#21021;&#22987;&#35823;&#24046;(&#20559;&#24046;)&#30340;&#26356;&#24555;&#34928;&#20943;&#36895;&#29575;&#65292;&#36825;&#26159;&#23545;&#25152;&#26377;&#36845;&#20195;&#30340;&#24179;&#22343;&#20540;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#32467;&#21512;&#27491;&#21017;&#21270;&#30340;TD&#21464;&#20307;&#12290;&#36890;&#36807;&#20998;&#26512;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#35748;&#20026;&#27491;&#21017;&#21270;&#30340;TD&#29256;&#26412;&#22312;&#20855;&#26377;&#30149;&#24577;&#29305;&#24449;&#30340;&#38382;&#39064;&#19978;&#26159;&#26377;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the finite-time behaviour of the popular temporal difference (TD) learning algorithm when combined with tail-averaging. We derive finite time bounds on the parameter error of the tail-averaged TD iterate under a step-size choice that does not require information about the eigenvalues of the matrix underlying the projected TD fixed point. Our analysis shows that tail-averaged TD converges at the optimal $O\left(1/t\right)$ rate, both in expectation and with high probability. In addition, our bounds exhibit a sharper rate of decay for the initial error (bias), which is an improvement over averaging all iterates. We also propose and analyse a variant of TD that incorporates regularisation. From analysis, we conclude that the regularised version of TD is useful for problems with ill-conditioned features.
&lt;/p&gt;</description></item></channel></rss>