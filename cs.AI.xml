<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36825;&#39033;&#35843;&#26597;&#24635;&#32467;&#20102;&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#32972;&#26223;&#20171;&#32461;&#12289;&#25968;&#23398;&#23450;&#20041;&#12289;&#20998;&#31867;&#24635;&#32467;&#12289;&#22522;&#20934;&#21644;&#24037;&#20855;&#21253;&#65292;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35752;&#35770;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01077</link><description>&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#39044;&#27979;&#24314;&#27169;&#30340;&#26368;&#26032;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Recent Advances in Predictive Modeling with Electronic Health Records
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01077
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#24635;&#32467;&#20102;&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#32972;&#26223;&#20171;&#32461;&#12289;&#25968;&#23398;&#23450;&#20041;&#12289;&#20998;&#31867;&#24635;&#32467;&#12289;&#22522;&#20934;&#21644;&#24037;&#20855;&#21253;&#65292;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#31995;&#32479;&#30340;&#21457;&#23637;&#20351;&#24471;&#22823;&#37327;&#30340;&#25968;&#23383;&#21270;&#24739;&#32773;&#25968;&#25454;&#24471;&#20197;&#25910;&#38598;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#29420;&#29305;&#30340;&#29305;&#24615;&#65292;&#21033;&#29992;EHR&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#24314;&#27169;&#38754;&#20020;&#30528;&#19968;&#20123;&#25361;&#25112;&#12290;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#21253;&#25324;&#21307;&#30103;&#22312;&#20869;&#30340;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#20248;&#21183;&#12290;&#26412;&#35843;&#26597;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#22522;&#20110;EHR&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;EHR&#25968;&#25454;&#30340;&#32972;&#26223;&#65292;&#24182;&#25552;&#20379;&#20102;&#39044;&#27979;&#24314;&#27169;&#20219;&#21153;&#30340;&#25968;&#23398;&#23450;&#20041;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#22810;&#20010;&#35282;&#24230;&#23545;&#39044;&#27979;&#28145;&#24230;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#21644;&#24635;&#32467;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19982;&#21307;&#30103;&#39044;&#27979;&#24314;&#27169;&#30456;&#20851;&#30340;&#22522;&#20934;&#21644;&#24037;&#20855;&#21253;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24320;&#25918;&#24615;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of electronic health records (EHR) systems has enabled the collection of a vast amount of digitized patient data. However, utilizing EHR data for predictive modeling presents several challenges due to its unique characteristics. With the advancements in machine learning techniques, deep learning has demonstrated its superiority in various applications, including healthcare. This survey systematically reviews recent advances in deep learning-based predictive models using EHR data. Specifically, we begin by introducing the background of EHR data and providing a mathematical definition of the predictive modeling task. We then categorize and summarize predictive deep models from multiple perspectives. Furthermore, we present benchmarks and toolkits relevant to predictive modeling in healthcare. Finally, we conclude this survey by discussing open challenges and suggesting promising directions for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#29992;&#20110;&#27785;&#28024;&#24335;&#36890;&#20449;&#20013;&#20943;&#23569;&#24102;&#23485;&#28040;&#32791;&#30340;&#23454;&#29992;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2404.01713</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#29992;&#20110;&#27785;&#28024;&#24335;&#36890;&#20449;&#65306;&#36890;&#36807;6G&#25506;&#32034;&#24863;&#30693;&#20114;&#32852;&#32593;&#30340;&#19979;&#19968;&#20010;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
Generative AI for Immersive Communication: The Next Frontier in Internet-of-Senses Through 6G
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#29992;&#20110;&#27785;&#28024;&#24335;&#36890;&#20449;&#20013;&#20943;&#23569;&#24102;&#23485;&#28040;&#32791;&#30340;&#23454;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20108;&#21313;&#24180;&#20013;&#65292;&#29289;&#32852;&#32593;(IoT)&#24050;&#32463;&#26159;&#19968;&#20010;&#20855;&#26377;&#21464;&#38761;&#24615;&#30340;&#27010;&#24565;&#65292;&#24403;&#25105;&#20204;&#36924;&#36817;2030&#24180;&#26102;&#65292;&#19968;&#20010;&#26032;&#30340;&#33539;&#24335;&#34987;&#31216;&#20026;&#24863;&#30693;&#20114;&#32852;&#32593;(IoS)&#27491;&#22312;&#20852;&#36215;&#12290;&#19982;&#20256;&#32479;&#30340;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#19981;&#21516;&#65292;IoS&#26088;&#22312;&#25552;&#20379;&#22810;&#24863;&#23448;&#20307;&#39564;&#65292;&#35748;&#35782;&#21040;&#22312;&#25105;&#20204;&#30340;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#25105;&#20204;&#30340;&#24863;&#30693;&#36828;&#19981;&#27490;&#20110;&#35270;&#35273;&#21644;&#21548;&#35273;&#65307;&#23427;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#24863;&#35273;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#25512;&#21160;&#27785;&#28024;&#24335;&#22810;&#24863;&#23448;&#23186;&#20307;&#30340;&#29616;&#26377;&#25216;&#26415;&#65292;&#28145;&#20837;&#25506;&#35752;&#23427;&#20204;&#30340;&#21151;&#33021;&#21644;&#28508;&#22312;&#24212;&#29992;&#12290;&#36825;&#39033;&#25506;&#32034;&#21253;&#25324;&#20256;&#32479;&#27785;&#28024;&#24335;&#23186;&#20307;&#27969;&#19982;&#19968;&#20010;&#25552;&#20986;&#30340;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#35821;&#20041;&#20132;&#27969;&#30340;&#29992;&#20363;&#20043;&#38388;&#30340;&#27604;&#36739;&#20998;&#26512;&#12290;&#36825;&#39033;&#20998;&#26512;&#30340;&#37325;&#28857;&#26159;&#25152;&#25552;&#26041;&#26696;&#20013;&#24102;&#23485;&#28040;&#32791;&#20943;&#23569;&#20102;99.93%&#12290;&#36890;&#36807;&#36825;&#31181;&#27604;&#36739;&#65292;&#25105;&#20204;&#26088;&#22312;&#24378;&#35843;&#35813;&#23454;&#29992;&#24212;&#29992;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01713v1 Announce Type: cross  Abstract: Over the past two decades, the Internet-of-Things (IoT) has been a transformative concept, and as we approach 2030, a new paradigm known as the Internet of Senses (IoS) is emerging. Unlike conventional Virtual Reality (VR), IoS seeks to provide multi-sensory experiences, acknowledging that in our physical reality, our perception extends far beyond just sight and sound; it encompasses a range of senses. This article explores existing technologies driving immersive multi-sensory media, delving into their capabilities and potential applications. This exploration includes a comparative analysis between conventional immersive media streaming and a proposed use case that lever- ages semantic communication empowered by generative Artificial Intelligence (AI). The focal point of this analysis is the substantial reduction in bandwidth consumption by 99.93% in the proposed scheme. Through this comparison, we aim to underscore the practical appli
&lt;/p&gt;</description></item><item><title>&#28304;&#24863;&#30693;&#35757;&#32451;&#20351;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#30693;&#35782;&#24402;&#22240;&#33021;&#21147;&#65292;&#36827;&#32780;&#22686;&#24378;&#20102;&#20854;&#36879;&#26126;&#24230;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#39564;&#35777;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.01019</link><description>&lt;p&gt;
&#28304;&#24863;&#30693;&#35757;&#32451;&#20351;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#30693;&#35782;&#24402;&#22240;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Source-Aware Training Enables Knowledge Attribution in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01019
&lt;/p&gt;
&lt;p&gt;
&#28304;&#24863;&#30693;&#35757;&#32451;&#20351;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#30693;&#35782;&#24402;&#22240;&#33021;&#21147;&#65292;&#36827;&#32780;&#22686;&#24378;&#20102;&#20854;&#36879;&#26126;&#24230;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#39564;&#35777;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#23398;&#21040;&#20102;&#22823;&#37327;&#30693;&#35782;&#65292;&#20294;&#24448;&#24448;&#23545;&#27492;&#31867;&#30693;&#35782;&#30340;&#26469;&#28304;&#27627;&#19981;&#22312;&#24847;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20869;&#22312;&#28304;&#24341;&#29992;&#38382;&#39064;&#65292;&#35201;&#27714;LLMs&#24341;&#29992;&#25903;&#25345;&#29983;&#25104;&#21709;&#24212;&#30340;&#39044;&#35757;&#32451;&#26469;&#28304;&#12290;&#20869;&#22312;&#28304;&#24341;&#29992;&#21487;&#20197;&#22686;&#24378;LLMs&#30340;&#36879;&#26126;&#24230;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#39564;&#35777;&#24615;&#12290;&#20026;&#36171;&#20104;LLMs&#36825;&#31181;&#33021;&#21147;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#28304;&#24863;&#30693;&#35757;&#32451;&#8212;&#8212;&#19968;&#20010;&#21518;&#39044;&#35757;&#32451;&#37197;&#26041;&#65292;&#21253;&#25324;&#65288;i&#65289;&#35757;&#32451;LLMs&#23558;&#21807;&#19968;&#28304;&#25991;&#26723;&#26631;&#35782;&#31526;&#19982;&#27599;&#20010;&#25991;&#26723;&#20013;&#30340;&#30693;&#35782;&#20851;&#32852;&#36215;&#26469;&#65292;&#28982;&#21518;&#65288;ii&#65289;&#36827;&#34892;&#25351;&#31034;&#35843;&#25972;&#65292;&#25945;&#23548;LLMs&#22312;&#34987;&#25552;&#31034;&#26102;&#24341;&#29992;&#25903;&#25345;&#30340;&#39044;&#35757;&#32451;&#26469;&#28304;&#12290;&#28304;&#24863;&#30693;&#35757;&#32451;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#21363;&#25554;&#21363;&#29992;&#30340;&#39044;&#35757;&#32451;LLMs&#65292;&#24182;&#19982;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;/&#24494;&#35843;&#26694;&#26550;&#30340;&#24046;&#24322;&#26368;&#23567;&#12290;&#36890;&#36807;&#23545;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35757;&#32451;&#37197;&#26041;&#21487;&#20197;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01019v1 Announce Type: cross  Abstract: Large language models (LLMs) learn a vast amount of knowledge during pretraining, but they are often oblivious to the source(s) of such knowledge. We investigate the problem of intrinsic source citation, where LLMs are required to cite the pretraining source supporting a generated response. Intrinsic source citation can enhance LLM transparency, interpretability, and verifiability. To give LLMs such ability, we explore source-aware training -- a post pretraining recipe that involves (i) training the LLM to associate unique source document identifiers with the knowledge in each document, followed by (ii) an instruction-tuning to teach the LLM to cite a supporting pretraining source when prompted. Source-aware training can easily be applied to pretrained LLMs off the shelf, and diverges minimally from existing pretraining/fine-tuning frameworks. Through experiments on carefully curated data, we demonstrate that our training recipe can en
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#20998;&#38548;&#23376;&#22270;&#30340;&#20998;&#23618;&#27744;&#21270;&#65288;SSHPool&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#33410;&#28857;&#20998;&#37197;&#21040;&#19981;&#21516;&#30340;&#31751;&#24182;&#21033;&#29992;&#23616;&#37096;&#22270;&#21367;&#31215;&#21333;&#20803;&#36827;&#34892;&#21387;&#32553;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#26377;&#25928;&#25552;&#21462;&#20102;&#21407;&#22987;&#22270;&#32467;&#26500;&#30340;&#20998;&#23618;&#20840;&#23616;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2403.16133</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#38548;&#23376;&#22270;&#30340;&#20998;&#23618;&#27744;&#21270;SSHPool
&lt;/p&gt;
&lt;p&gt;
SSHPool: The Separated Subgraph-based Hierarchical Pooling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16133
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#20998;&#38548;&#23376;&#22270;&#30340;&#20998;&#23618;&#27744;&#21270;&#65288;SSHPool&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#33410;&#28857;&#20998;&#37197;&#21040;&#19981;&#21516;&#30340;&#31751;&#24182;&#21033;&#29992;&#23616;&#37096;&#22270;&#21367;&#31215;&#21333;&#20803;&#36827;&#34892;&#21387;&#32553;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#26377;&#25928;&#25552;&#21462;&#20102;&#21407;&#22987;&#22270;&#32467;&#26500;&#30340;&#20998;&#23618;&#20840;&#23616;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26412;&#22320;&#22270;&#27744;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;&#22522;&#20110;&#20998;&#38548;&#23376;&#22270;&#30340;&#20998;&#23618;&#27744;&#21270;&#65288;SSHPool&#65289;&#65292;&#29992;&#20110;&#22270;&#20998;&#31867;&#12290;&#36890;&#36807;&#23558;&#19968;&#20010;&#26679;&#26412;&#22270;&#30340;&#33410;&#28857;&#20998;&#37197;&#21040;&#19981;&#21516;&#30340;&#31751;&#20013;&#65292;&#20174;&#32780;&#20135;&#29983;&#19968;&#31995;&#21015;&#20998;&#38548;&#30340;&#23376;&#22270;&#12290;&#25105;&#20204;&#20998;&#21035;&#20351;&#29992;&#26412;&#22320;&#22270;&#21367;&#31215;&#21333;&#20803;&#20316;&#20026;&#23616;&#37096;&#32467;&#26500;&#65292;&#36827;&#19968;&#27493;&#23558;&#27599;&#20010;&#23376;&#22270;&#21387;&#32553;&#25104;&#19968;&#20010;&#31895;&#31961;&#33410;&#28857;&#65292;&#23558;&#21407;&#22987;&#22270;&#36716;&#21270;&#20026;&#31895;&#31961;&#22270;&#12290;&#30001;&#20110;&#36825;&#20123;&#23376;&#22270;&#30001;&#19981;&#21516;&#30340;&#31751;&#20998;&#38548;&#24320;&#65292;&#32467;&#26500;&#20449;&#24687;&#26080;&#27861;&#22312;&#23427;&#20204;&#20043;&#38388;&#20256;&#25773;&#65292;&#23616;&#37096;&#21367;&#31215;&#25805;&#20316;&#21487;&#20197;&#26174;&#33879;&#36991;&#20813;&#22823;&#22810;&#25968;&#29616;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#20013;&#20986;&#29616;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#32467;&#26524;&#31895;&#31961;&#22270;&#19978;&#23618;&#27425;&#22320;&#25191;&#34892;&#25152;&#25552;&#35758;&#30340;&#31243;&#24207;&#65292;SSHPool&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#21462;&#21407;&#22987;&#22270;&#32467;&#26500;&#30340;&#20998;&#23618;&#20840;&#23616;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16133v1 Announce Type: new  Abstract: In this paper, we develop a novel local graph pooling method, namely the Separated Subgraph-based Hierarchical Pooling (SSHPool), for graph classification. To this end, we commence by assigning the nodes of a sample graph into different clusters, resulting in a family of separated subgraphs. We individually employ a local graph convolution units as the local structure to further compress each subgraph into a coarsened node, transforming the original graph into a coarsened graph. Since these subgraphs are separated by different clusters and the structural information cannot be propagated between them, the local convolution operation can significantly avoid the over-smoothing problem arising in most existing Graph Neural Networks (GNNs). By hierarchically performing the proposed procedures on the resulting coarsened graph, the proposed SSHPool can effectively extract the hierarchical global feature of the original graph structure, encapsul
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#20998;&#31867;&#30340;&#33258;&#36866;&#24212;&#26680;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#29305;&#24449;&#36890;&#36947;&#27880;&#24847;&#26426;&#21046;&#25429;&#25417;&#19981;&#21516;&#23376;&#32467;&#26500;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#26377;&#25928;&#35782;&#21035;&#32467;&#26500;&#37325;&#35201;&#24615;&#65292;&#24182;&#35745;&#31639;&#19982;&#37325;&#35201;&#23376;&#32467;&#26500;&#30456;&#20851;&#30340;&#22270;&#23545;&#20043;&#38388;&#30340;R-&#21367;&#31215;&#26680;&#12290;</title><link>https://arxiv.org/abs/2403.16130</link><description>&lt;p&gt;
AKBR: &#23398;&#20064;&#33258;&#36866;&#24212;&#22522;&#20110;&#26680;&#30340;&#22270;&#20998;&#31867;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
AKBR: Learning Adaptive Kernel-based Representations for Graph Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16130
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#20998;&#31867;&#30340;&#33258;&#36866;&#24212;&#26680;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#29305;&#24449;&#36890;&#36947;&#27880;&#24847;&#26426;&#21046;&#25429;&#25417;&#19981;&#21516;&#23376;&#32467;&#26500;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#26377;&#25928;&#35782;&#21035;&#32467;&#26500;&#37325;&#35201;&#24615;&#65292;&#24182;&#35745;&#31639;&#19982;&#37325;&#35201;&#23376;&#32467;&#26500;&#30456;&#20851;&#30340;&#22270;&#23545;&#20043;&#38388;&#30340;R-&#21367;&#31215;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#33258;&#36866;&#24212;&#22522;&#20110;&#26680;&#30340;&#22270;&#20998;&#31867;&#34920;&#31034;&#65288;AKBR&#65289;&#12290;&#19982;&#20165;&#36890;&#36807;&#35745;&#31639;&#22270;&#20043;&#38388;&#21516;&#26500;&#23376;&#32467;&#26500;&#23545;&#30340;&#25968;&#37327;&#26469;&#23450;&#20041;&#30340;&#26368;&#20808;&#36827;&#30340; R-&#21367;&#31215;&#22270;&#26680;&#19981;&#21516;&#65292;&#26080;&#27861;&#20026;&#20998;&#31867;&#22120;&#25552;&#20379;&#31471;&#21040;&#31471;&#23398;&#20064;&#26426;&#21046;&#65292;&#25152;&#25552;&#20986;&#30340;AKBR&#26041;&#27861;&#26088;&#22312;&#23450;&#20041;&#19968;&#20010;&#31471;&#21040;&#31471;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#20026;&#22270;&#26500;&#24314;&#33258;&#36866;&#24212;&#26680;&#30697;&#38453;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#36890;&#36947;&#27880;&#24847;&#26426;&#21046;&#26469;&#25429;&#25417;&#21407;&#22987;&#22270;&#20013;&#19981;&#21516;&#23376;&#32467;&#26500;&#19981;&#21464;&#24615;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;&#25152;&#25552;&#20986;&#30340;AKBR&#27169;&#22411;&#22240;&#27492;&#21487;&#20197;&#26377;&#25928;&#22320;&#30830;&#23450;&#19981;&#21516;&#23376;&#32467;&#26500;&#30340;&#32467;&#26500;&#37325;&#35201;&#24615;&#65292;&#24182;&#35745;&#31639;&#19982;&#30001;&#20854;&#32467;&#26500;&#27880;&#24847;&#21147;&#25351;&#23450;&#30340;&#26356;&#37325;&#35201;&#23376;&#32467;&#26500;&#30456;&#20851;&#30340;&#25104;&#23545;&#22270;&#20043;&#38388;&#30340;R-&#21367;&#31215;&#26680;&#12290;&#30001;&#20110;&#32467;&#26524;&#26680;&#30697;&#38453;&#30340;&#27599;&#19968;&#34892;...&#65288;&#27492;&#22788;&#34987;&#25130;&#26029;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16130v1 Announce Type: cross  Abstract: In this paper, we propose a new model to learn Adaptive Kernel-based Representations (AKBR) for graph classification. Unlike state-of-the-art R-convolution graph kernels that are defined by merely counting any pair of isomorphic substructures between graphs and cannot provide an end-to-end learning mechanism for the classifier, the proposed AKBR approach aims to define an end-to-end representation learning model to construct an adaptive kernel matrix for graphs. To this end, we commence by leveraging a novel feature-channel attention mechanism to capture the interdependencies between different substructure invariants of original graphs. The proposed AKBR model can thus effectively identify the structural importance of different substructures, and compute the R-convolution kernel between pairwise graphs associated with the more significant substructures specified by their structural attentions. Since each row of the resulting kernel mat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#31995;&#32479;&#35780;&#20215;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#35752;&#35770;&#20102;&#20854;&#22312;&#26089;&#26399;&#31579;&#26597;&#12289;&#25968;&#23383;&#24178;&#39044;&#21644;&#20854;&#20182;&#20020;&#24202;&#24212;&#29992;&#20013;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;</title><link>https://arxiv.org/abs/2403.15401</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#31995;&#32479;&#35780;&#20215;
&lt;/p&gt;
&lt;p&gt;
Large Language Model for Mental Health: A Systematic Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15401
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#31995;&#32479;&#35780;&#20215;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#35752;&#35770;&#20102;&#20854;&#22312;&#26089;&#26399;&#31579;&#26597;&#12289;&#25968;&#23383;&#24178;&#39044;&#21644;&#20854;&#20182;&#20020;&#24202;&#24212;&#29992;&#20013;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25968;&#23383;&#20581;&#24247;&#39046;&#22495;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#23637;&#29616;&#20986;&#20102;&#28508;&#22312;&#30340;&#24212;&#29992;&#24615;&#65292;&#20294;&#23427;&#20204;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#24212;&#29992;&#20173;&#22312;&#25345;&#32493;&#35752;&#35770;&#20013;&#12290;&#36825;&#39033;&#31995;&#32479;&#24615;&#35780;&#20215;&#26088;&#22312;&#24635;&#32467;&#21644;&#34920;&#24449;LLMs&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#35843;&#26597;LLMs&#26368;&#26032;&#30740;&#31350;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#35752;&#35770;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#26089;&#26399;&#31579;&#26597;&#12289;&#25968;&#23383;&#24178;&#39044;&#20197;&#21450;&#20854;&#20182;&#20020;&#24202;&#24212;&#29992;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#26681;&#25454;PRISMA&#25351;&#21335;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;PubMed&#12289;DBLP&#35745;&#31639;&#26426;&#31185;&#23398;&#25991;&#29486;&#25968;&#25454;&#24211;&#21644;IEEE Xplore&#19978;&#21457;&#34920;&#30340;&#33521;&#25991;&#25991;&#31456;&#65292;&#26102;&#38388;&#36328;&#24230;&#20026;2017&#24180;1&#26376;1&#26085;&#33267;2023&#24180;9&#26376;1&#26085;&#65292;&#37325;&#28857;&#20851;&#27880;&#24515;&#29702;&#20581;&#24247;&#21644;LLMs&#12290;&#35813;&#32508;&#36848;&#20998;&#26512;&#20102;32&#31687;&#25991;&#31456;&#65292;&#21253;&#25324;&#20351;&#29992;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#38598;&#36827;&#34892;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#30340;&#65288;n=13&#65289;&#12289;&#24515;&#29702;&#20581;&#24247;&#32842;&#22825;&#26426;&#22120;&#20154;&#65288;n=10&#65289;&#20197;&#21450;&#20854;&#20182;&#24515;&#29702;&#20581;&#24247;&#24212;&#29992;&#65288;n=9&#65289;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;LLMs&#22312;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15401v1 Announce Type: cross  Abstract: Large language models (LLMs) have received much attention and shown their potential in digital health, while their application in mental health is subject to ongoing debate. This systematic review aims to summarize and characterize the use of LLMs in mental health by investigating the strengths and limitations of the latest work in LLMs and discusses the challenges and opportunities for early screening, digital interventions, and other clinical applications in mental health. Following PRISMA guidelines, we examined English articles from PubMed, DBLP Computer Science Bibliography, and IEEE Xplore, published between 1 January 2017, and 1 September 2023, focusing on mental health and LLMs. The review analyzed 32 articles, including mental health analysis using social media datasets (n=13), mental health chatbots (n=10), and other mental health applications (n=9). Findings reveal LLMs' effectiveness in mental health issue detection and the
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#25552;&#31034;&#25216;&#26415;&#65292;&#22312;&#23569;&#36164;&#28304;&#35821;&#35328;&#24773;&#22659;&#19979;&#65292;&#20165;&#38656;&#23569;&#37327;&#26679;&#26412;&#35757;&#32451;&#21363;&#21487;&#25552;&#21462;&#20020;&#24202;&#20449;&#24687;&#65292;&#19988;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.13369</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#25552;&#31034;&#36827;&#34892;&#23569;&#36164;&#28304;&#35821;&#35328;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#20020;&#24202;&#20449;&#24687;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Clinical information extraction for Low-resource languages with Few-shot learning using Pre-trained language models and Prompting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13369
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#25552;&#31034;&#25216;&#26415;&#65292;&#22312;&#23569;&#36164;&#28304;&#35821;&#35328;&#24773;&#22659;&#19979;&#65292;&#20165;&#38656;&#23569;&#37327;&#26679;&#26412;&#35757;&#32451;&#21363;&#21487;&#25552;&#21462;&#20020;&#24202;&#20449;&#24687;&#65292;&#19988;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20020;&#24202;&#25991;&#20214;&#20013;&#33258;&#21160;&#25552;&#21462;&#21307;&#30103;&#20449;&#24687;&#38754;&#20020;&#30528;&#20960;&#20010;&#25361;&#25112;&#65306;&#25152;&#38656;&#20020;&#24202;&#19987;&#19994;&#30693;&#35782;&#30340;&#39640;&#25104;&#26412;&#12289;&#27169;&#22411;&#39044;&#27979;&#30340;&#26377;&#38480;&#21487;&#35299;&#37322;&#24615;&#12289;&#21463;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#20197;&#21450;&#38544;&#31169;&#27861;&#35268;&#12290;&#26368;&#36817;&#22312;&#39046;&#22495;&#36866;&#24212;&#21644;&#25552;&#31034;&#26041;&#27861;&#19978;&#30340;&#36827;&#23637;&#26174;&#31034;&#65292;&#21033;&#29992;&#36731;&#37327;&#32423;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#22312;&#20351;&#29992;&#26497;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#26102;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#65292;&#36825;&#20123;&#27169;&#22411;&#36866;&#29992;&#20110;&#25104;&#29087;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#27425;&#22312;&#23569;&#36164;&#28304;&#29615;&#22659;&#20013;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#36890;&#36807;&#22312;&#24503;&#22269;&#21307;&#29983;&#20449;&#20214;&#19978;&#36827;&#34892;&#22810;&#31867;&#21035;&#27573;&#20998;&#31867;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#31867;&#21035;&#32423;&#35780;&#20272;&#65292;&#25903;&#25345; Shapley &#20540;&#65292;&#20197;&#39564;&#35777;&#25105;&#20204;&#30340;&#23567;&#22411;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#65292;&#24182;&#30830;&#20445;&#27169;&#22411;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#19968;&#20010;&#36731;&#37327;&#32423;&#12289;&#39046;&#22495;&#36866;&#24212;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#20165;&#20165;&#25552;&#31034;&#20102; 20 &#27425;&#30340;&#24773;&#20917;&#19979;&#65292;&#32988;&#36807;&#20102;&#20256;&#32479;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13369v1 Announce Type: new  Abstract: Automatic extraction of medical information from clinical documents poses several challenges: high costs of required clinical expertise, limited interpretability of model predictions, restricted computational resources and privacy regulations. Recent advances in domain-adaptation and prompting methods showed promising results with minimal training data using lightweight masked language models, which are suited for well-established interpretability methods. We are first to present a systematic evaluation of these methods in a low-resource setting, by performing multi-class section classification on German doctor's letters. We conduct extensive class-wise evaluations supported by Shapley values, to validate the quality of our small training data set and to ensure the interpretability of model predictions. We demonstrate that a lightweight, domain-adapted pretrained model, prompted with just 20 shots, outperforms a traditional classificatio
&lt;/p&gt;</description></item><item><title>&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#24418;&#25104;&#30340;&#22810;&#23618;&#27425;&#38598;&#20307;&#26234;&#33021;&#32593;&#32476;&#65292;&#21487;&#20197;&#23454;&#29616;&#36229;&#36234;&#20219;&#19968;&#21333;&#29420;&#23454;&#20307;&#30340;&#38598;&#20307;&#26234;&#33021;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2403.10433</link><description>&lt;p&gt;
AI&#22686;&#24378;&#30340;&#38598;&#20307;&#26234;&#33021;&#65306;&#29616;&#29366;&#19982;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
AI-enhanced Collective Intelligence: The State of the Art and Prospects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10433
&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#24418;&#25104;&#30340;&#22810;&#23618;&#27425;&#38598;&#20307;&#26234;&#33021;&#32593;&#32476;&#65292;&#21487;&#20197;&#23454;&#29616;&#36229;&#36234;&#20219;&#19968;&#21333;&#29420;&#23454;&#20307;&#30340;&#38598;&#20307;&#26234;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#31038;&#20250;&#25361;&#25112;&#36229;&#20986;&#20102;&#20154;&#31867;&#20010;&#20307;&#25110;&#38598;&#20307;&#21162;&#21147;&#30340;&#33021;&#21147;&#12290;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#20854;&#22312;&#20154;&#31867;&#38598;&#20307;&#20013;&#30340;&#35282;&#33394;&#23558;&#20174;&#36741;&#21161;&#24037;&#20855;&#36716;&#21464;&#20026;&#21442;&#19982;&#24335;&#25104;&#21592;&#12290;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#25317;&#26377;&#20114;&#34917;&#30340;&#33021;&#21147;&#65292;&#24403;&#20108;&#32773;&#21327;&#21516;&#20316;&#29992;&#26102;&#65292;&#21487;&#20197;&#23454;&#29616;&#19968;&#31181;&#36229;&#36234;&#21333;&#29420;&#20154;&#31867;&#25110;&#20154;&#24037;&#26234;&#33021;&#38598;&#20307;&#33021;&#21147;&#30340;&#38598;&#20307;&#26234;&#33021;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#30340;&#20132;&#20114;&#26412;&#36136;&#19978;&#26159;&#22797;&#26434;&#30340;&#65292;&#28041;&#21450;&#22797;&#26434;&#30340;&#36807;&#31243;&#21644;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;&#26412;&#32508;&#36848;&#20174;&#32593;&#32476;&#31185;&#23398;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#26500;&#24819;&#20102;&#19968;&#20010;&#22810;&#23618;&#27425;&#30340;&#20154;&#24037;&#26234;&#33021;&#38598;&#20307;&#26234;&#33021;&#34920;&#31034;&#65292;&#21253;&#25324;&#35748;&#30693;&#23618;&#12289;&#29289;&#29702;&#23618;&#21644;&#20449;&#24687;&#23618;&#12290;&#22312;&#36825;&#20010;&#22810;&#23618;&#32593;&#32476;&#20013;&#65292;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#23637;&#29616;&#20986;&#19981;&#21516;&#30340;&#29305;&#24449;&#65307;&#20154;&#31867;&#22312;&#22810;&#26679;&#24615;&#26041;&#38754;&#20174;&#34920;&#23618;&#21040;&#28145;&#23618;&#23646;&#24615;&#19981;&#21516;&#65292;&#32780;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#22312;&#31243;&#24230;&#19978;&#20063;&#26377;&#25152;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10433v1 Announce Type: cross  Abstract: The current societal challenges exceed the capacity of human individual or collective effort alone. As AI evolves, its role within human collectives is poised to vary from an assistive tool to a participatory member. Humans and AI possess complementary capabilities that, when synergized, can achieve a level of collective intelligence that surpasses the collective capabilities of either humans or AI in isolation. However, the interactions in human-AI systems are inherently complex, involving intricate processes and interdependencies. This review incorporates perspectives from network science to conceptualize a multilayer representation of human-AI collective intelligence, comprising a cognition layer, a physical layer, and an information layer. Within this multilayer network, humans and AI agents exhibit varying characteristics; humans differ in diversity from surface-level to deep-level attributes, while AI agents range in degrees of f
&lt;/p&gt;</description></item><item><title>RepoHyper&#25552;&#20986;&#20102;Repo&#32423;&#35821;&#20041;&#22270;&#65288;RSG&#65289;&#21644;Expand&#21644;Refine&#26816;&#32034;&#26041;&#27861;&#65292;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#28385;&#36275;&#20179;&#24211;&#32423;&#20195;&#30721;&#34917;&#20840;&#30340;&#38656;&#27714;</title><link>https://arxiv.org/abs/2403.06095</link><description>&lt;p&gt;
RepoHyper&#65306;&#26356;&#22909;&#30340;&#19978;&#19979;&#25991;&#26816;&#32034;&#26159;&#20179;&#24211;&#32423;&#20195;&#30721;&#34917;&#20840;&#25152;&#38656;&#30340;&#19968;&#20999;
&lt;/p&gt;
&lt;p&gt;
RepoHyper: Better Context Retrieval Is All You Need for Repository-Level Code Completion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06095
&lt;/p&gt;
&lt;p&gt;
RepoHyper&#25552;&#20986;&#20102;Repo&#32423;&#35821;&#20041;&#22270;&#65288;RSG&#65289;&#21644;Expand&#21644;Refine&#26816;&#32034;&#26041;&#27861;&#65292;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#28385;&#36275;&#20179;&#24211;&#32423;&#20195;&#30721;&#34917;&#20840;&#30340;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06095v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#25688;&#35201;&#65306;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;CodeLLMs&#65289;&#22312;&#20195;&#30721;&#34917;&#20840;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#32463;&#24120;&#26080;&#27861;&#23436;&#20840;&#29702;&#35299;&#39033;&#30446;&#20179;&#24211;&#30340;&#24191;&#27867;&#19978;&#19979;&#25991;&#65292;&#27604;&#22914;&#30456;&#20851;&#25991;&#20214;&#21644;&#31867;&#23618;&#27425;&#32467;&#26500;&#30340;&#22797;&#26434;&#24615;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#34917;&#20840;&#19981;&#22815;&#31934;&#30830;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RepoHyper&#65292;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#19982;&#20179;&#24211;&#32423;&#20195;&#30721;&#34917;&#20840;&#30456;&#20851;&#30340;&#22797;&#26434;&#25361;&#25112;&#30340;&#22810;&#26041;&#38754;&#26694;&#26550;&#12290;RepoHyper&#30340;&#26680;&#24515;&#26159;Repo&#32423;&#35821;&#20041;&#22270;&#65288;RSG&#65289;&#65292;&#19968;&#31181;&#23553;&#35013;&#20195;&#30721;&#20179;&#24211;&#24191;&#27867;&#19978;&#19979;&#25991;&#30340;&#26032;&#39062;&#35821;&#20041;&#22270;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;RepoHyper&#21033;&#29992;&#25193;&#23637;&#21644;&#32454;&#21270;&#26816;&#32034;&#26041;&#27861;&#65292;&#21253;&#25324;&#24212;&#29992;&#20110;RSG&#30340;&#22270;&#25193;&#23637;&#21644;&#38142;&#25509;&#39044;&#27979;&#31639;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#30456;&#20851;&#20195;&#30721;&#29255;&#27573;&#30340;&#26377;&#25928;&#26816;&#32034;&#21644;&#20248;&#20808;&#25490;&#24207;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;RepoHyper&#22312;&#37325;&#26032;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06095v1 Announce Type: cross  Abstract: Code Large Language Models (CodeLLMs) have demonstrated impressive proficiency in code completion tasks. However, they often fall short of fully understanding the extensive context of a project repository, such as the intricacies of relevant files and class hierarchies, which can result in less precise completions. To overcome these limitations, we present RepoHyper, a multifaceted framework designed to address the complex challenges associated with repository-level code completion. Central to RepoHyper is the Repo-level Semantic Graph (RSG), a novel semantic graph structure that encapsulates the vast context of code repositories. Furthermore, RepoHyper leverages Expand and Refine retrieval method, including a graph expansion and a link prediction algorithm applied to the RSG, enabling the effective retrieval and prioritization of relevant code snippets. Our evaluations show that RepoHyper markedly outperforms existing techniques in re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#27010;&#36848;&#20102;&#19987;&#21033;&#39046;&#22495;&#30340;&#20219;&#21153;&#21644;&#26041;&#27861;&#65292;&#24378;&#35843;&#20102;&#35821;&#35328;&#22788;&#29702;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#36817;&#26399;&#20986;&#29616;&#30340;&#36890;&#29992;&#29983;&#25104;&#26041;&#27861;&#22312;&#19987;&#21033;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.04105</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25506;&#32034;&#19987;&#21033;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence Exploring the Patent Field
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#27010;&#36848;&#20102;&#19987;&#21033;&#39046;&#22495;&#30340;&#20219;&#21153;&#21644;&#26041;&#27861;&#65292;&#24378;&#35843;&#20102;&#35821;&#35328;&#22788;&#29702;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#36817;&#26399;&#20986;&#29616;&#30340;&#36890;&#29992;&#29983;&#25104;&#26041;&#27861;&#22312;&#19987;&#21033;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04105v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#20808;&#36827;&#30340;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#25215;&#35834;&#22312;&#20197;&#21069;&#24191;&#27867;&#20381;&#36182;&#25163;&#24037;&#25805;&#20316;&#30340;&#19987;&#21033;&#21644;&#25216;&#26415;&#30693;&#35782;&#31649;&#29702;&#39046;&#22495;&#24102;&#26469;&#24040;&#22823;&#30340;&#25928;&#29575;&#25913;&#36827;&#12290;&#36825;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#22823;&#35268;&#27169;&#32780;&#22797;&#26434;&#30340;&#25968;&#25454;&#65292;&#20855;&#26377;&#38750;&#24120;&#20934;&#30830;&#30340;&#20869;&#23481;&#21644;&#35821;&#35328;&#34920;&#36798;&#36825;&#20123;&#20869;&#23481;&#12290;&#29305;&#21035;&#26159;&#65292;&#19987;&#21033;&#25991;&#26412;&#22312;&#21508;&#20010;&#26041;&#38754;&#21487;&#33021;&#19982;&#24179;&#20961;&#30340;&#25991;&#26412;&#26377;&#25152;&#19981;&#21516;&#65292;&#36825;&#24102;&#26469;&#20102;&#37325;&#22823;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#26412;&#25991;&#31995;&#32479;&#27010;&#36848;&#20102;&#19982;&#19987;&#21033;&#26377;&#20851;&#30340;&#20219;&#21153;&#21644;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#19981;&#26029;&#28436;&#21464;&#21644;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#12290;&#35821;&#35328;&#22788;&#29702;&#65292;&#23588;&#20854;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#26368;&#36817;&#25512;&#21160;&#26222;&#36890;&#29983;&#25104;&#26041;&#27861;&#30340;&#25552;&#21319;&#65292;&#26377;&#26395;&#25104;&#20026;&#19987;&#21033;&#39046;&#22495;&#30340;&#21464;&#38761;&#32773;&#12290;&#19987;&#21033;&#25991;&#29486;&#20197;&#21450;&#22260;&#32469;&#19987;&#21033;&#30340;&#22522;&#20110;&#20107;&#23454;&#30340;&#35770;&#35777;&#31243;&#24207;&#20284;&#20046;&#20960;&#20046;&#26159;&#19968;&#20010;&#29702;&#24819;&#30340;&#20351;&#29992;&#26696;&#20363;&#12290;&#28982;&#32780;&#65292;&#19987;&#21033;&#28041;&#21450;&#35768;&#22810;&#29616;&#26377;&#27169;&#22411;&#24456;&#38590;&#22788;&#29702;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04105v1 Announce Type: new  Abstract: Advanced language-processing and machine-learning techniques promise massive efficiency improvements in the previously widely manual field of patent and technical knowledge management. This field presents large-scale and complex data with very precise contents and language representation of those contents. Particularly, patent texts can differ from mundane texts in various aspects, which entails significant opportunities and challenges. This paper presents a systematic overview of patent-related tasks and popular methodologies with a special focus on evolving and promising techniques. Language processing and particularly large language models as well as the recent boost of general generative methods promise to become game changers in the patent field. The patent literature and the fact-based argumentative procedures around patents appear almost as an ideal use case. However, patents entail a number of difficulties with which existing mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#19988;&#22312;&#29702;&#35770;&#19978;&#26377;&#26681;&#25454;&#30340;&#26041;&#27861;&#26469;&#27880;&#37322;&#20998;&#35299;&#34164;&#28085;&#25968;&#25454;&#38598;&#65292;&#24418;&#25104;RDTE&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#22312;&#35299;&#20915;&#20309;&#20026;&#26377;&#25928;&#30340;&#32452;&#21512;&#34164;&#28085;&#30340;&#38382;&#39064;&#19978;&#26377;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.14798</link><description>&lt;p&gt;
&#21033;&#29992;&#38750;&#27491;&#24335;&#36923;&#36753;&#22686;&#24378;&#31995;&#32479;&#21270;&#20998;&#35299;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Enhancing Systematic Decompositional Natural Language Inference Using Informal Logic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#19988;&#22312;&#29702;&#35770;&#19978;&#26377;&#26681;&#25454;&#30340;&#26041;&#27861;&#26469;&#27880;&#37322;&#20998;&#35299;&#34164;&#28085;&#25968;&#25454;&#38598;&#65292;&#24418;&#25104;RDTE&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#22312;&#35299;&#20915;&#20309;&#20026;&#26377;&#25928;&#30340;&#32452;&#21512;&#34164;&#28085;&#30340;&#38382;&#39064;&#19978;&#26377;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#35821;&#35328;&#27169;&#22411;&#20026;&#20351;&#29992;&#25991;&#26412;&#36827;&#34892;&#32467;&#26500;&#21270;&#25512;&#29702;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#65292;&#20363;&#22914;&#22312;&#19981;&#20381;&#36182;&#33030;&#24369;&#30340;&#24418;&#24335;&#36923;&#36753;&#30340;&#24773;&#20917;&#19979;&#26500;&#24314;&#21644;&#35780;&#20272;&#30452;&#35266;&#30340;&#12289;&#31867;&#20284;&#35777;&#26126;&#30340;&#25991;&#26412;&#34164;&#28085;&#26641;&#12290;&#28982;&#32780;&#65292;&#27839;&#30528;&#36825;&#20010;&#26041;&#21521;&#30340;&#36827;&#23637;&#21463;&#21040;&#19968;&#20010;&#38271;&#26399;&#20197;&#26469;&#32570;&#20047;&#26126;&#30830;&#30340;&#30830;&#23450;&#20309;&#20026;&#26377;&#25928;&#30340;&#32452;&#21512;&#34164;&#28085;&#30340;&#28165;&#26224;&#21327;&#35758;&#30340;&#38459;&#30861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19968;&#33268;&#19988;&#22312;&#29702;&#35770;&#19978;&#26377;&#26681;&#25454;&#30340;&#26041;&#27861;&#26469;&#27880;&#37322;&#20998;&#35299;&#34164;&#28085;&#25968;&#25454;&#38598;&#65292;&#24182;&#35780;&#20272;&#20854;&#23545;&#22522;&#20110;LLM&#30340;&#25991;&#26412;&#25512;&#29702;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#25968;&#25454;&#38598;RDTE (Recognizing Decompositional Textual Entailment) &#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#27604;&#20808;&#21069;&#30340;&#20998;&#35299;&#34164;&#28085;&#25968;&#25454;&#38598;&#39640;&#24471;&#22810;&#65288;+9%&#65289;&#65292;&#34920;&#26126;RDTE&#22312;&#38271;&#26399;&#23384;&#22312;&#30340;&#20851;&#20110;&#20309;&#20026;&#26377;&#25928;&#30340;&#32452;&#21512;&#34164;&#28085;&#30340;&#38382;&#39064;&#19978;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14798v1 Announce Type: cross  Abstract: Contemporary language models enable new opportunities for structured reasoning with text, such as the construction and evaluation of intuitive, proof-like textual entailment trees without relying on brittle formal logic. However, progress in this direction has been hampered by a long-standing lack of a clear protocol for determining what valid compositional entailment is. This absence causes noisy datasets and limited performance gains by modern neuro-symbolic engines. To address these problems, we formulate a consistent and theoretically grounded approach to annotating decompositional entailment datasets, and evaluate its impact on LLM-based textual inference. We find that our resulting dataset, RDTE (Recognizing Decompositional Textual Entailment), has a substantially higher internal consistency (+9%) than prior decompositional entailment datasets, suggesting that RDTE is a significant step forward in the long-standing problem of for
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#26799;&#24230;&#25237;&#24433;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#26799;&#24230;&#25237;&#24433;&#21040;&#20004;&#20010;&#20851;&#38190;&#23376;&#31354;&#38388;&#26469;&#23454;&#29616;&#25345;&#32493;&#40065;&#26834;&#23398;&#20064;&#65292;&#26377;&#25928;&#22320;&#32500;&#25345;&#20102;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11196</link><description>&lt;p&gt;
&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#20445;&#25345;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Maintaining Adversarial Robustness in Continuous Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11196
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#26799;&#24230;&#25237;&#24433;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#26799;&#24230;&#25237;&#24433;&#21040;&#20004;&#20010;&#20851;&#38190;&#23376;&#31354;&#38388;&#26469;&#23454;&#29616;&#25345;&#32493;&#40065;&#26834;&#23398;&#20064;&#65292;&#26377;&#25928;&#22320;&#32500;&#25345;&#20102;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#22797;&#26434;&#30340;&#38450;&#24481;&#31639;&#27861;&#33719;&#24471;&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#22312;&#31070;&#32463;&#32593;&#32476;&#19981;&#26029;&#28436;&#21270;&#20197;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#24456;&#23481;&#26131;&#34987;&#25273;&#21435;&#12290;&#36825;&#31181;&#33030;&#24369;&#24615;&#21487;&#20197;&#36890;&#36807;&#22521;&#20859;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#33021;&#21147;&#26469;&#35299;&#20915;&#65292;&#31216;&#20026;&#25345;&#32493;&#40065;&#26834;&#23398;&#20064;&#65292;&#23427;&#22312;&#36830;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#20851;&#27880;&#21069;&#26399;&#20219;&#21153;&#30340;(&#20998;&#31867;)&#24615;&#33021;&#21644;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#25345;&#32493;&#40065;&#26834;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21452;&#26799;&#24230;&#25237;&#24433;&#30340;&#26041;&#27861;&#65292;&#23558;&#29992;&#20110;&#26435;&#37325;&#26356;&#26032;&#30340;&#26799;&#24230;&#27491;&#20132;&#25237;&#24433;&#21040;&#20004;&#20010;&#20851;&#38190;&#23376;&#31354;&#38388;&#19978; -- &#19968;&#20010;&#29992;&#20110;&#31283;&#23450;&#24179;&#28369;&#26679;&#26412;&#26799;&#24230;&#65292;&#21478;&#19968;&#20010;&#29992;&#20110;&#31283;&#23450;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#32456;&#36755;&#20986;&#12290;&#22312;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#32500;&#25345;&#20102;&#23545;&#24378;&#23545;&#25239;&#24615;&#30340;&#25345;&#32493;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11196v1 Announce Type: cross  Abstract: Adversarial robustness is essential for security and reliability of machine learning systems. However, the adversarial robustness gained by sophisticated defense algorithms is easily erased as the neural network evolves to learn new tasks. This vulnerability can be addressed by fostering a novel capability for neural networks, termed continual robust learning, which focuses on both the (classification) performance and adversarial robustness on previous tasks during continuous learning. To achieve continuous robust learning, we propose an approach called Double Gradient Projection that projects the gradients for weight updates orthogonally onto two crucial subspaces -- one for stabilizing the smoothed sample gradients and another for stabilizing the final outputs of the neural network. The experimental results on four benchmarks demonstrate that the proposed approach effectively maintains continuous robustness against strong adversarial
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#36830;&#32493;&#39550;&#39542;&#25919;&#31574;&#20248;&#21270;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#38381;&#29615;&#20010;&#24615;&#21270;&#35838;&#31243;&#65288;CLIC&#65289;&#27010;&#24565;&#65292;&#20801;&#35768;&#37325;&#22797;&#21033;&#29992;&#24191;&#27867;&#22330;&#26223;&#26469;&#36845;&#20195;&#25913;&#36827;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2309.14209</link><description>&lt;p&gt;
&#20855;&#26377;&#38381;&#29615;&#20010;&#24615;&#21270;&#35838;&#31243;&#30340;&#36830;&#32493;&#39550;&#39542;&#25919;&#31574;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Continual Driving Policy Optimization with Closed-Loop Individualized Curricula
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.14209
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#36830;&#32493;&#39550;&#39542;&#25919;&#31574;&#20248;&#21270;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#38381;&#29615;&#20010;&#24615;&#21270;&#35838;&#31243;&#65288;CLIC&#65289;&#27010;&#24565;&#65292;&#20801;&#35768;&#37325;&#22797;&#21033;&#29992;&#24191;&#27867;&#22330;&#26223;&#26469;&#36845;&#20195;&#25913;&#36827;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#65288;AV&#65289;&#30340;&#23433;&#20840;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#30340;&#22836;&#31561;&#20851;&#27880;&#28857;&#65292;&#26681;&#28304;&#20110;&#38271;&#23614;&#33258;&#28982;&#39550;&#39542;&#20998;&#24067;&#20013;&#32597;&#35265;&#30340;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#30340;&#32570;&#22833;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#20986;&#29616;&#20102;&#22823;&#37327;&#22522;&#20110;&#22330;&#26223;&#30340;&#33258;&#21160;&#39550;&#39542;&#30740;&#31350;&#65292;&#37325;&#28857;&#26159;&#29983;&#25104;&#39640;&#39118;&#38505;&#39550;&#39542;&#22330;&#26223;&#24182;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#23545;AV&#27169;&#22411;&#36827;&#34892;&#23433;&#20840;&#20851;&#38190;&#27979;&#35797;&#12290;&#28982;&#32780;&#65292;&#26377;&#38480;&#30340;&#24037;&#20316;&#25506;&#35752;&#20102;&#37325;&#22797;&#21033;&#29992;&#36825;&#20123;&#24191;&#27867;&#22330;&#26223;&#26469;&#36845;&#20195;&#25913;&#36827;AV&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20174;&#20855;&#26377;&#19981;&#21516;&#34892;&#20026;&#30340;&#20854;&#20182;AV&#27169;&#22411;&#25910;&#38598;&#30340;&#24040;&#22823;&#22330;&#26223;&#24211;&#20013;&#28388;&#20986;&#21487;&#20256;&#36882;&#20449;&#24687;&#20197;&#25913;&#36827;&#24403;&#21069;AV&#20173;&#28982;&#26159;&#38590;&#20197;&#35299;&#20915;&#30340;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20855;&#26377;&#38381;&#29615;&#20010;&#24615;&#21270;&#35838;&#31243;&#65288;CLIC&#65289;&#29305;&#28857;&#30340;&#36830;&#32493;&#39550;&#39542;&#25919;&#31574;&#20248;&#21270;&#26694;&#26550;&#65292;&#25105;&#20204;&#23558;&#20854;&#20998;&#35299;&#20026;&#19968;&#32452;&#26631;&#20934;&#21270;&#30340;&#23376;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.14209v3 Announce Type: replace-cross  Abstract: The safety of autonomous vehicles (AV) has been a long-standing top concern, stemming from the absence of rare and safety-critical scenarios in the long-tail naturalistic driving distribution. To tackle this challenge, a surge of research in scenario-based autonomous driving has emerged, with a focus on generating high-risk driving scenarios and applying them to conduct safety-critical testing of AV models. However, limited work has been explored on the reuse of these extensive scenarios to iteratively improve AV models. Moreover, it remains intractable and challenging to filter through gigantic scenario libraries collected from other AV models with distinct behaviors, attempting to extract transferable information for current AV improvement. Therefore, we develop a continual driving policy optimization framework featuring Closed-Loop Individualized Curricula (CLIC), which we factorize into a set of standardized sub-modules for
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Regional Temporal Graph Neural Network (RegT-GCN)&#20316;&#20026;&#19968;&#20010;&#39044;&#27979;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#25972;&#20010;&#24030;&#30340;&#21345;&#36710;&#20572;&#36710;&#20351;&#29992;&#24773;&#20917;&#65292;&#20197;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#20572;&#36710;&#20449;&#24687;&#24182;&#32531;&#35299;&#26410;&#32463;&#25480;&#26435;&#30340;&#20572;&#36710;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.12920</link><description>&lt;p&gt;
&#29992;&#20998;&#35299;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#21345;&#36710;&#20572;&#36710;&#20351;&#29992;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Truck Parking Usage Prediction with Decomposed Graph Neural Networks. (arXiv:2401.12920v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12920
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Regional Temporal Graph Neural Network (RegT-GCN)&#20316;&#20026;&#19968;&#20010;&#39044;&#27979;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#25972;&#20010;&#24030;&#30340;&#21345;&#36710;&#20572;&#36710;&#20351;&#29992;&#24773;&#20917;&#65292;&#20197;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#20572;&#36710;&#20449;&#24687;&#24182;&#32531;&#35299;&#26410;&#32463;&#25480;&#26435;&#30340;&#20572;&#36710;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36135;&#36816;&#36208;&#24266;&#19978;&#30340;&#21345;&#36710;&#20572;&#36710;&#38754;&#20020;&#35832;&#22810;&#25361;&#25112;&#65292;&#22914;&#20572;&#36710;&#20301;&#19981;&#36275;&#21644;&#36981;&#23432;&#24037;&#26102;&#35268;&#23450;&#12290;&#36825;&#20123;&#38480;&#21046;&#24448;&#24448;&#23548;&#33268;&#26410;&#32463;&#25480;&#26435;&#30340;&#20572;&#36710;&#34892;&#20026;&#65292;&#24341;&#21457;&#23433;&#20840;&#38382;&#39064;&#12290;&#20026;&#20102;&#25552;&#39640;&#36135;&#36816;&#20316;&#19994;&#30340;&#23433;&#20840;&#24615;&#65292;&#25552;&#20379;&#20934;&#30830;&#30340;&#20572;&#36710;&#20351;&#29992;&#39044;&#27979;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#32463;&#27982;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23613;&#31649;&#24050;&#26377;&#30740;&#31350;&#34920;&#26126;&#23545;&#20110;&#21333;&#20010;&#21345;&#36710;&#20572;&#36710;&#22330;&#20351;&#29992;&#24773;&#20917;&#30340;&#39044;&#27979;&#20934;&#30830;&#24230;&#36739;&#39640;&#65292;&#20294;&#23545;&#22810;&#20010;&#21345;&#36710;&#20572;&#36710;&#22330;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#20351;&#29992;&#39044;&#27979;&#30340;&#26041;&#27861;&#24456;&#23569;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21306;&#22495;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;RegT-GCN&#65289;&#20316;&#20026;&#19968;&#20010;&#39044;&#27979;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#25972;&#20010;&#24030;&#30340;&#20572;&#36710;&#20351;&#29992;&#24773;&#20917;&#65292;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#21345;&#36710;&#20572;&#36710;&#20449;&#24687;&#21644;&#32531;&#35299;&#26410;&#32463;&#25480;&#26435;&#30340;&#20572;&#36710;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#21345;&#36710;&#20572;&#36710;&#22330;&#20998;&#24067;&#30340;&#25299;&#25169;&#32467;&#26500;&#21644;&#21382;&#21490;&#20572;&#36710;&#25968;&#25454;&#26469;&#39044;&#27979;&#25972;&#20010;&#24030;&#30340;&#21344;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Truck parking on freight corridors faces various challenges, such as insufficient parking spaces and compliance with Hour-of-Service (HOS) regulations. These constraints often result in unauthorized parking practices, causing safety concerns. To enhance the safety of freight operations, providing accurate parking usage prediction proves to be a cost-effective solution. Despite the existing research demonstrating satisfactory accuracy for predicting individual truck parking site usage, few approaches have been proposed for predicting usage with spatial dependencies of multiple truck parking sites. We present the Regional Temporal Graph Neural Network (RegT-GCN) as a predictive framework for assessing parking usage across the entire state to provide better truck parking information and mitigate unauthorized parking. The framework leverages the topological structures of truck parking site distributions and historical parking data to predict occupancy rates across a state. To achieve this,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#22312;&#26410;&#30693;&#23454;&#20307;&#32676;&#20307;&#20998;&#24067;&#19979;&#30340;SHAP&#35780;&#20998;&#38382;&#39064;&#12290;&#36890;&#36807;&#32771;&#34385;&#19968;&#20010;&#19981;&#30830;&#23450;&#24615;&#21306;&#22495;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#25152;&#26377;&#29305;&#24449;&#30340;SHAP&#35780;&#20998;&#30340;&#32039;&#26463;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2401.12731</link><description>&lt;p&gt;
SHAP&#35780;&#20998;&#22312;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Distributional Uncertainty of the SHAP score in Explainable Machine Learning. (arXiv:2401.12731v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#22312;&#26410;&#30693;&#23454;&#20307;&#32676;&#20307;&#20998;&#24067;&#19979;&#30340;SHAP&#35780;&#20998;&#38382;&#39064;&#12290;&#36890;&#36807;&#32771;&#34385;&#19968;&#20010;&#19981;&#30830;&#23450;&#24615;&#21306;&#22495;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#25152;&#26377;&#29305;&#24449;&#30340;SHAP&#35780;&#20998;&#30340;&#32039;&#26463;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#23646;&#20998;&#25968;&#21453;&#26144;&#20102;&#36755;&#20837;&#23454;&#20307;&#20013;&#30340;&#29305;&#24449;&#20540;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36755;&#20986;&#30340;&#37325;&#35201;&#24615;&#12290;&#20854;&#20013;&#26368;&#21463;&#27426;&#36814;&#30340;&#35780;&#20998;&#20043;&#19968;&#26159;SHAP&#35780;&#20998;&#65292;&#23427;&#26159;&#21512;&#20316;&#21338;&#24328;&#29702;&#35770;&#20013;Shapley&#20540;&#30340;&#20855;&#20307;&#23454;&#20363;&#12290;&#35813;&#35780;&#20998;&#30340;&#23450;&#20041;&#20381;&#36182;&#20110;&#23454;&#20307;&#32676;&#20307;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#30001;&#20110;&#36890;&#24120;&#19981;&#30693;&#36947;&#31934;&#30830;&#30340;&#20998;&#24067;&#65292;&#22240;&#27492;&#38656;&#35201;&#20027;&#35266;&#22320;&#36827;&#34892;&#20998;&#37197;&#25110;&#20174;&#25968;&#25454;&#20013;&#36827;&#34892;&#20272;&#35745;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#35823;&#23548;&#24615;&#30340;&#29305;&#24449;&#35780;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#19981;&#30693;&#36947;&#23454;&#20307;&#32676;&#20307;&#20998;&#24067;&#30340;SHAP&#35780;&#20998;&#25512;&#29702;&#30340;&#21407;&#21017;&#24615;&#26694;&#26550;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#21253;&#21547;&#28508;&#22312;&#20998;&#24067;&#30340;&#19981;&#30830;&#23450;&#24615;&#21306;&#22495;&#65292;&#32780;&#29305;&#24449;&#30340;SHAP&#35780;&#20998;&#25104;&#20026;&#22312;&#35813;&#21306;&#22495;&#19978;&#23450;&#20041;&#30340;&#19968;&#20010;&#20989;&#25968;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25214;&#21040;&#35813;&#20989;&#25968;&#30340;&#26368;&#22823;&#20540;&#21644;&#26368;&#23567;&#20540;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#30830;&#23450;&#25152;&#26377;&#29305;&#24449;&#30340;SHAP&#35780;&#20998;&#30340;&#32039;&#26463;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attribution scores reflect how important the feature values in an input entity are for the output of a machine learning model. One of the most popular attribution scores is the SHAP score, which is an instantiation of the general Shapley value used in coalition game theory. The definition of this score relies on a probability distribution on the entity population. Since the exact distribution is generally unknown, it needs to be assigned subjectively or be estimated from data, which may lead to misleading feature scores. In this paper, we propose a principled framework for reasoning on SHAP scores under unknown entity population distributions. In our framework, we consider an uncertainty region that contains the potential distributions, and the SHAP score of a feature becomes a function defined over this region. We study the basic problems of finding maxima and minima of this function, which allows us to determine tight ranges for the SHAP scores of all features. In particular, we pinp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#21644;&#35299;&#38145;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#25991;&#21270;&#35270;&#35282;&#65292;&#36890;&#36807;&#23545;TTI&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#25991;&#21270;&#24863;&#30693;&#36827;&#34892;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#25991;&#21270;&#24847;&#35782;&#12289;&#25991;&#21270;&#21306;&#21035;&#21644;&#25991;&#21270;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.01929</link><description>&lt;p&gt;
&#31359;&#36234;&#25991;&#21270;&#40511;&#27807;&#65306;&#25506;&#32034;&#21644;&#35299;&#38145;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#25991;&#21270;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Navigating Cultural Chasms: Exploring and Unlocking the Cultural POV of Text-To-Image Models. (arXiv:2310.01929v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#21644;&#35299;&#38145;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#25991;&#21270;&#35270;&#35282;&#65292;&#36890;&#36807;&#23545;TTI&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#25991;&#21270;&#24863;&#30693;&#36827;&#34892;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#25991;&#21270;&#24847;&#35782;&#12289;&#25991;&#21270;&#21306;&#21035;&#21644;&#25991;&#21270;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;TTI&#65289;&#27169;&#22411;&#65292;&#20363;&#22914;DALL-E&#21644;StableDiffusion&#65292;&#22312;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#30340;&#38646;&#23556;&#27169;&#24335;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#36817;&#26469;&#22791;&#21463;&#20851;&#27880;&#12290;&#20316;&#20026;&#25991;&#21270;&#30340;&#23186;&#20171;&#65292;&#35821;&#35328;&#22312;&#36825;&#20123;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20174;&#32780;&#22609;&#36896;&#20102;&#23427;&#20204;&#30340;&#25991;&#21270;&#26426;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25551;&#36848;&#25991;&#21270;&#32500;&#24230;&#65292;&#25991;&#21270;&#39046;&#22495;&#21644;&#25991;&#21270;&#27010;&#24565;&#30340;&#19977;&#20010;&#23618;&#27425;&#26469;&#25506;&#32034;TTI&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#25991;&#21270;&#24863;&#30693;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#35780;&#20272;&#25216;&#26415;&#65292;&#21253;&#25324;&#20351;&#29992;CLIP&#31354;&#38388;&#36827;&#34892;&#20869;&#22312;&#35780;&#20272;&#65292;&#20351;&#29992;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#27169;&#22411;&#36827;&#34892;&#22806;&#22312;&#35780;&#20272;&#20197;&#21450;&#20154;&#31867;&#35780;&#20272;&#65292;&#20197;&#35782;&#21035;TTI&#25991;&#21270;&#24863;&#30693;&#12290;&#20026;&#20102;&#20419;&#36827;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CulText2I&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26469;&#33258;&#22235;&#20010;&#19981;&#21516;&#30340;TTI&#27169;&#22411;&#65292;&#28085;&#30422;&#20102;&#21313;&#31181;&#35821;&#35328;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#25991;&#21270;&#24847;&#35782;&#12289;&#25991;&#21270;&#21306;&#21035;&#21644;
&lt;/p&gt;
&lt;p&gt;
Text-To-Image (TTI) models, exemplified by DALL-E and StableDiffusion, have recently gained prominence for their remarkable zero-shot capabilities in generating images guided by textual prompts. Language, as a conduit of culture, plays a pivotal role in these models' multilingual capabilities, which in turn shape their cultural agency. In this study, we explore the cultural perception embedded in TTI models by characterizing culture across three hierarchical tiers: cultural dimensions, cultural domains, and cultural concepts. We propose a comprehensive suite of evaluation techniques, including intrinsic evaluations using the CLIP space, extrinsic evaluations with a Visual-Question-Answer (VQA) model, and human assessments, to discern TTI cultural perceptions. To facilitate our research, we introduce the CulText2I dataset, derived from four diverse TTI models and spanning ten languages. Our experiments reveal insights into these models' cultural awareness, cultural distinctions, and the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;IGNNet&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#26102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#20174;&#21407;&#22987;&#36755;&#20837;&#29305;&#24449;&#31934;&#30830;&#35745;&#31639;&#39044;&#27979;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24615;&#33021;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2308.08945</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#26684;&#25968;&#25454;&#22788;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Interpretable Graph Neural Networks for Tabular Data. (arXiv:2308.08945v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;IGNNet&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#26102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#20174;&#21407;&#22987;&#36755;&#20837;&#29305;&#24449;&#31934;&#30830;&#35745;&#31639;&#39044;&#27979;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24615;&#33021;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#34920;&#26684;&#26684;&#24335;&#30340;&#25968;&#25454;&#32463;&#24120;&#20986;&#29616;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36817;&#26399;&#34987;&#25193;&#23637;&#20197;&#26377;&#25928;&#22788;&#29702;&#27492;&#31867;&#25968;&#25454;&#65292;&#36890;&#36807;&#34920;&#31034;&#23398;&#20064;&#25429;&#25417;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26412;&#36136;&#19978;&#20135;&#29983;&#20102;&#40657;&#30418;&#27169;&#22411;&#65292;&#20197;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24418;&#24335;&#23384;&#22312;&#65292;&#20351;&#24471;&#29992;&#25143;&#26080;&#27861;&#29702;&#35299;&#27169;&#22411;&#39044;&#27979;&#30340;&#36923;&#36753;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;IGNNet&#65288;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#22788;&#29702;&#26041;&#27861;&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#38480;&#21046;&#23398;&#20064;&#31639;&#27861;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23637;&#31034;&#20102;&#22914;&#20309;&#20174;&#21407;&#22987;&#36755;&#20837;&#29305;&#24449;&#20934;&#30830;&#35745;&#31639;&#39044;&#27979;&#32467;&#26524;&#12290;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;IGNNet&#19982;&#38754;&#21521;&#34920;&#26684;&#25968;&#25454;&#30340;&#26368;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65288;&#21253;&#25324;XGBoost&#65292;Random Forests&#21644;TabNet&#65289;&#24615;&#33021;&#30456;&#24403;&#12290;&#21516;&#26102;&#65292;&#32467;&#26524;&#26174;&#31034;&#20174;IGNNet&#33719;&#24471;&#30340;&#35299;&#37322;&#19982;&#30495;&#23454;&#24773;&#20917;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data in tabular format is frequently occurring in real-world applications. Graph Neural Networks (GNNs) have recently been extended to effectively handle such data, allowing feature interactions to be captured through representation learning. However, these approaches essentially produce black-box models, in the form of deep neural networks, precluding users from following the logic behind the model predictions. We propose an approach, called IGNNet (Interpretable Graph Neural Network for tabular data), which constrains the learning algorithm to produce an interpretable model, where the model shows how the predictions are exactly computed from the original input features. A large-scale empirical investigation is presented, showing that IGNNet is performing on par with state-of-the-art machine-learning algorithms that target tabular data, including XGBoost, Random Forests, and TabNet. At the same time, the results show that the explanations obtained from IGNNet are aligned with the true
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#20998;&#20301;&#25968;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36793;&#32536;&#20113;&#35745;&#31639;&#32593;&#32476;&#20013;&#25214;&#21040;&#26368;&#20339;&#30340;&#27969;&#37327;&#20998;&#37197;&#26041;&#26696;&#12290;&#36890;&#36807;&#24341;&#20837;Gumbel-softmax&#37319;&#26679;&#32593;&#32476;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#24182;&#26174;&#33879;&#20248;&#20110;&#38543;&#26426;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.05170</link><description>&lt;p&gt;
&#36793;&#32536;&#20113;&#35745;&#31639;&#30340;&#31070;&#32463;&#20998;&#20301;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Neural Quantile Optimization for Edge-Cloud Computing. (arXiv:2307.05170v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05170
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#20998;&#20301;&#25968;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36793;&#32536;&#20113;&#35745;&#31639;&#32593;&#32476;&#20013;&#25214;&#21040;&#26368;&#20339;&#30340;&#27969;&#37327;&#20998;&#37197;&#26041;&#26696;&#12290;&#36890;&#36807;&#24341;&#20837;Gumbel-softmax&#37319;&#26679;&#32593;&#32476;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#24182;&#26174;&#33879;&#20248;&#20110;&#38543;&#26426;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23547;&#27714;&#36793;&#32536;&#20113;&#35745;&#31639;&#32593;&#32476;&#30340;&#26368;&#20339;&#27969;&#37327;&#20998;&#37197;&#26041;&#26696;&#65292;&#20197;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#24182;&#26368;&#23567;&#21270;&#22522;&#20110;&#31361;&#21457;&#35745;&#36153;&#30340;&#25104;&#26412;&#12290;&#39318;&#20808;&#65292;&#23545;&#20110;&#22266;&#23450;&#30340;&#32593;&#32476;&#25299;&#25169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#26063;&#25972;&#25968;&#35268;&#21010;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#21547;&#25551;&#36848;&#21508;&#31181;&#27969;&#37327;&#38656;&#27714;&#30340;&#38543;&#26426;&#21442;&#25968;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#20811;&#26381;&#38382;&#39064;&#31163;&#25955;&#29305;&#24449;&#24102;&#26469;&#30340;&#22256;&#38590;&#65292;&#25105;&#20204;&#23558;Gumbel-softmax&#37325;&#21442;&#25968;&#21270;&#26041;&#27861;&#25512;&#24191;&#20026;&#19968;&#20010;&#26080;&#32422;&#26463;&#36830;&#32493;&#20248;&#21270;&#38382;&#39064;&#65292;&#20316;&#20026;&#31163;&#25955;&#38382;&#39064;&#30340;&#27491;&#21017;&#21270;&#24310;&#32493;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;Gumbel-softmax&#37319;&#26679;&#32593;&#32476;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#26469;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#12290;&#32593;&#32476;&#32467;&#26500;&#21453;&#26144;&#20102;&#36793;&#32536;&#20113;&#35745;&#31639;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#24182;&#34987;&#35757;&#32451;&#20026;&#20351;&#24471;&#26080;&#32422;&#26463;&#36830;&#32493;&#20248;&#21270;&#38382;&#39064;&#30340;&#25104;&#26412;&#20989;&#25968;&#26399;&#26395;&#26368;&#23567;&#21270;&#12290;&#35757;&#32451;&#22909;&#30340;&#32593;&#32476;&#20316;&#20026;&#19968;&#20010;&#39640;&#25928;&#30340;&#27969;&#37327;&#20998;&#37197;&#26041;&#26696;&#37319;&#26679;&#22120;&#65292;&#22312;&#21487;&#34892;&#24615;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#38543;&#26426;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
We seek the best traffic allocation scheme for the edge-cloud computing network that satisfies constraints and minimizes the cost based on burstable billing. First, for a fixed network topology, we formulate a family of integer programming problems with random parameters describing the various traffic demands. Then, to overcome the difficulty caused by the discrete feature of the problem, we generalize the Gumbel-softmax reparameterization method to induce an unconstrained continuous optimization problem as a regularized continuation of the discrete problem. Finally, we introduce the Gumbel-softmax sampling network to solve the optimization problems via unsupervised learning. The network structure reflects the edge-cloud computing topology and is trained to minimize the expectation of the cost function for unconstrained continuous optimization problems. The trained network works as an efficient traffic allocation scheme sampler, remarkably outperforming the random strategy in feasibili
&lt;/p&gt;</description></item></channel></rss>