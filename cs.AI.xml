<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>MambaMixer&#26159;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#26435;&#37325;&#30340;&#21452;&#37325;&#36873;&#25321;&#26426;&#21046;&#65292;&#31216;&#20026;&#36873;&#25321;&#24615;&#26631;&#35760;&#21644;&#36890;&#36947;&#28151;&#21512;&#22120;&#65292;&#23545;&#38271;&#24207;&#21015;&#24314;&#27169;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.19888</link><description>&lt;p&gt;
MambaMixer&#65306;&#20855;&#26377;&#21452;&#37325;&#26631;&#35760;&#21644;&#36890;&#36947;&#36873;&#25321;&#30340;&#39640;&#25928;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19888
&lt;/p&gt;
&lt;p&gt;
MambaMixer&#26159;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#26435;&#37325;&#30340;&#21452;&#37325;&#36873;&#25321;&#26426;&#21046;&#65292;&#31216;&#20026;&#36873;&#25321;&#24615;&#26631;&#35760;&#21644;&#36890;&#36947;&#28151;&#21512;&#22120;&#65292;&#23545;&#38271;&#24207;&#21015;&#24314;&#27169;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#20027;&#35201;&#20381;&#36182;&#20110;Transformers&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#24615;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#22823;&#35268;&#27169;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26550;&#26500;&#20013;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#23637;&#29616;&#20986;&#36755;&#20837;&#22823;&#23567;&#30340;&#20108;&#27425;&#26102;&#38388;&#21644;&#31354;&#38388;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#29992;&#20110;&#38271;&#24207;&#21015;&#24314;&#27169;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#23613;&#31649;&#26368;&#36817;&#26377;&#23581;&#35797;&#20026;&#22810;&#32500;&#25968;&#25454;&#35774;&#35745;&#39640;&#25928;&#26377;&#25928;&#30340;&#26550;&#26500;&#20027;&#24178;&#65292;&#20363;&#22914;&#22270;&#20687;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65292;&#20294;&#29616;&#26377;&#27169;&#22411;&#35201;&#20040;&#26159;&#25968;&#25454;&#29420;&#31435;&#30340;&#65292;&#35201;&#20040;&#26080;&#27861;&#20801;&#35768;&#36328;&#32500;&#24230;&#21644;&#20869;&#37096;&#32500;&#24230;&#20043;&#38388;&#30340;&#36890;&#20449;&#12290;&#26368;&#36817;&#65292;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#65292;&#23588;&#20854;&#26159;&#20855;&#26377;&#39640;&#25928;&#30828;&#20214;&#24863;&#30693;&#23454;&#29616;&#30340;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#23637;&#29616;&#20986;&#20102;&#29992;&#20110;&#38271;&#24207;&#21015;&#24314;&#27169;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;&#21463;&#21040;SSMs&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MambaMixer&#65292;&#19968;&#31181;&#26032;&#30340;&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#26435;&#37325;&#30340;&#26550;&#26500;&#65292;&#20351;&#29992;&#36328;&#26631;&#35760;&#21644;&#36890;&#36947;&#30340;&#21452;&#37325;&#36873;&#25321;&#26426;&#21046;&#65292;&#31216;&#20026;&#36873;&#25321;&#24615;&#26631;&#35760;&#21644;&#36890;&#36947;&#28151;&#21512;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19888v1 Announce Type: cross  Abstract: Recent advances in deep learning have mainly relied on Transformers due to their data dependency and ability to learn at scale. The attention module in these architectures, however, exhibits quadratic time and space in input size, limiting their scalability for long-sequence modeling. Despite recent attempts to design efficient and effective architecture backbone for multi-dimensional data, such as images and multivariate time series, existing models are either data independent, or fail to allow inter- and intra-dimension communication. Recently, State Space Models (SSMs), and more specifically Selective State Space Models, with efficient hardware-aware implementation, have shown promising potential for long sequence modeling. Motivated by the success of SSMs, we present MambaMixer, a new architecture with data-dependent weights that uses a dual selection mechanism across tokens and channels, called Selective Token and Channel Mixer. M
&lt;/p&gt;</description></item><item><title>&#21487;&#24494;&#20998;&#32534;&#31243;&#26159;&#19968;&#20010;&#26032;&#30340;&#32534;&#31243;&#33539;&#24335;&#65292;&#20351;&#24471;&#22797;&#26434;&#31243;&#24207;&#33021;&#22815;&#31471;&#23545;&#31471;&#22320;&#36827;&#34892;&#24494;&#20998;&#65292;&#23454;&#29616;&#22522;&#20110;&#26799;&#24230;&#30340;&#21442;&#25968;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.14606</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#32534;&#31243;&#30340;&#35201;&#32032;
&lt;/p&gt;
&lt;p&gt;
The Elements of Differentiable Programming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14606
&lt;/p&gt;
&lt;p&gt;
&#21487;&#24494;&#20998;&#32534;&#31243;&#26159;&#19968;&#20010;&#26032;&#30340;&#32534;&#31243;&#33539;&#24335;&#65292;&#20351;&#24471;&#22797;&#26434;&#31243;&#24207;&#33021;&#22815;&#31471;&#23545;&#31471;&#22320;&#36827;&#34892;&#24494;&#20998;&#65292;&#23454;&#29616;&#22522;&#20110;&#26799;&#24230;&#30340;&#21442;&#25968;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#26368;&#36817;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#36825;&#24471;&#30410;&#20110;&#22823;&#22411;&#27169;&#22411;&#12289;&#24222;&#22823;&#25968;&#25454;&#38598;&#12289;&#21152;&#36895;&#30828;&#20214;&#65292;&#20197;&#21450;&#21487;&#24494;&#20998;&#32534;&#31243;&#30340;&#21464;&#38761;&#24615;&#21147;&#37327;&#12290;&#36825;&#31181;&#26032;&#30340;&#32534;&#31243;&#33539;&#24335;&#20351;&#22797;&#26434;&#35745;&#31639;&#26426;&#31243;&#24207;&#65288;&#21253;&#25324;&#20855;&#26377;&#25511;&#21046;&#27969;&#21644;&#25968;&#25454;&#32467;&#26500;&#30340;&#31243;&#24207;&#65289;&#33021;&#22815;&#36827;&#34892;&#31471;&#23545;&#31471;&#30340;&#24494;&#20998;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#31243;&#24207;&#21442;&#25968;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#12290;&#19981;&#20165;&#20165;&#26159;&#31243;&#24207;&#30340;&#24494;&#20998;&#65292;&#21487;&#24494;&#20998;&#32534;&#31243;&#20063;&#21253;&#25324;&#20102;&#31243;&#24207;&#20248;&#21270;&#12289;&#27010;&#29575;&#31561;&#22810;&#20010;&#39046;&#22495;&#30340;&#27010;&#24565;&#12290;&#26412;&#20070;&#20171;&#32461;&#20102;&#21487;&#24494;&#20998;&#32534;&#31243;&#25152;&#38656;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#24182;&#37319;&#29992;&#20102;&#20248;&#21270;&#21644;&#27010;&#29575;&#20004;&#20010;&#20027;&#35201;&#35270;&#35282;&#36827;&#34892;&#38416;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14606v1 Announce Type: new  Abstract: Artificial intelligence has recently experienced remarkable advances, fueled by large models, vast datasets, accelerated hardware, and, last but not least, the transformative power of differentiable programming. This new programming paradigm enables end-to-end differentiation of complex computer programs (including those with control flows and data structures), making gradient-based optimization of program parameters possible. As an emerging paradigm, differentiable programming builds upon several areas of computer science and applied mathematics, including automatic differentiation, graphical models, optimization and statistics. This book presents a comprehensive review of the fundamental concepts useful for differentiable programming. We adopt two main perspectives, that of optimization and that of probability, with clear analogies between the two. Differentiable programming is not merely the differentiation of programs, but also the t
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#32479;&#19968;&#26694;&#26550;&#32467;&#21512;&#20102;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65292;&#26368;&#22823;&#21270;&#20445;&#30041;&#26576;&#20123;&#21521;&#37327;&#34920;&#31034;&#24182;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.14236</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#32534;&#36753;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified Framework for Model Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14236
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#32479;&#19968;&#26694;&#26550;&#32467;&#21512;&#20102;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65292;&#26368;&#22823;&#21270;&#20445;&#30041;&#26576;&#20123;&#21521;&#37327;&#34920;&#31034;&#24182;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#26159;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#19987;&#27880;&#20110;&#26356;&#26032;&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#30693;&#35782;&#12290;&#22312;&#21508;&#31181;&#26041;&#27861;&#20013;&#65292;ROME&#21644;MEMIT&#20316;&#20026;&#20027;&#35201;&#30340;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#33073;&#39062;&#32780;&#20986;&#12290;&#32780;MEMIT&#21487;&#20197;&#25209;&#37327;&#32534;&#36753;&#35760;&#24518;&#65292;ROME&#21017;&#19968;&#27425;&#21482;&#33021;&#25913;&#21464;&#19968;&#20010;&#20107;&#23454;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;ROME&#21644;MEMIT&#32435;&#20837;&#19968;&#20010;&#21333;&#19968;&#30340;&#27010;&#24565;&#26694;&#26550;&#65292;&#20248;&#21270;&#21516;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#20445;&#23384;-&#35760;&#24518;&#8221;&#30446;&#26631;&#12290;&#35813;&#30446;&#26631;&#26088;&#22312;&#22312;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#30340;&#21516;&#26102;&#20445;&#30041;&#26576;&#20123;&#36873;&#23450;&#21521;&#37327;&#30340;&#34920;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;ROME&#20351;&#29992;&#31561;&#24335;&#32422;&#26463;&#20248;&#21270;&#27492;&#30446;&#26631;&#65292;&#32780;MEMIT&#37319;&#29992;&#26356;&#28789;&#27963;&#30340;&#26368;&#23567;&#20108;&#20056;&#32422;&#26463;&#12290;&#38500;&#20102;&#25209;&#37327;&#32534;&#36753;&#22806;&#65292;MEMIT&#36824;&#21487;&#20197;&#22312;&#22810;&#20010;&#23618;&#38754;&#32534;&#36753;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#32534;&#36753;&#30340;&#20998;&#24067;&#20174;&#22810;&#20010;&#23618;&#38754;&#20998;&#24320;&#65292;&#21306;&#21035;&#20110;&#20248;&#21270;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14236v1 Announce Type: cross  Abstract: Model editing is a growing area focused on updating the knowledge embedded within models. Among the various methodologies, ROME and MEMIT stand out as leading "locate-and-edit" model editing techniques. While MEMIT enables batched editing of memories, ROME is limited to changing one fact at a time. This paper introduces a unifying framework that brings ROME and MEMIT under a single conceptual umbrella, optimizing for the same goal, which we call the "preservation-memorization" objective. This objective aims to preserve the representations of certain selected vectors while memorizing the representations of new factual information. Specifically, ROME optimizes this objective using an equality constraint, whereas MEMIT employs a more flexible least-square constraint. In addition to making batched edits, MEMIT also edits the model at multiple layers. We disentangle the distribution of edits to multiple layers from the optimization objectiv
&lt;/p&gt;</description></item><item><title>RigorLLM&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#39640;&#25928;&#26377;&#25928;&#22320;&#35843;&#33410;LLMs&#30340;&#26377;&#23475;&#21644;&#19981;&#23433;&#20840;&#36755;&#20837;&#21644;&#36755;&#20986;&#65292;&#21253;&#25324;&#33021;&#37327;&#25968;&#25454;&#22686;&#24378;&#12289;&#26368;&#23567;-&#26368;&#22823;&#20248;&#21270;&#23433;&#20840;&#36755;&#20837;&#21518;&#32512;&#65292;&#20197;&#21450;&#22522;&#20110;&#25968;&#25454;&#22686;&#24378;&#30340;&#40065;&#26834;KNN&#19982;LLMs&#34701;&#21512;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.13031</link><description>&lt;p&gt;
RigorLLM&#65306;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25269;&#24481;&#19981;&#33391;&#20869;&#23481;&#30340;&#40065;&#26834;&#38450;&#25252;&#26639;
&lt;/p&gt;
&lt;p&gt;
RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13031
&lt;/p&gt;
&lt;p&gt;
RigorLLM&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#39640;&#25928;&#26377;&#25928;&#22320;&#35843;&#33410;LLMs&#30340;&#26377;&#23475;&#21644;&#19981;&#23433;&#20840;&#36755;&#20837;&#21644;&#36755;&#20986;&#65292;&#21253;&#25324;&#33021;&#37327;&#25968;&#25454;&#22686;&#24378;&#12289;&#26368;&#23567;-&#26368;&#22823;&#20248;&#21270;&#23433;&#20840;&#36755;&#20837;&#21518;&#32512;&#65292;&#20197;&#21450;&#22522;&#20110;&#25968;&#25454;&#22686;&#24378;&#30340;&#40065;&#26834;KNN&#19982;LLMs&#34701;&#21512;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#20013;&#20986;&#29616;&#30340;&#20559;&#35265;&#20197;&#21450;&#22312;&#24694;&#24847;&#36755;&#20837;&#19979;&#20135;&#29983;&#26377;&#23475;&#20869;&#23481;&#30340;&#28508;&#21147;&#65292;&#23588;&#20854;&#26159;&#23545;&#25239;&#24615;&#25915;&#20987;&#19979;&#65292;&#37117;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#38450;&#25252;&#26639;&#65288;RigorLLM&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#39640;&#25928;&#26377;&#25928;&#22320;&#35843;&#33410;LLMs&#30340;&#26377;&#23475;&#21644;&#19981;&#23433;&#20840;&#36755;&#20837;&#21644;&#36755;&#20986;&#12290;&#36890;&#36807;&#37319;&#29992;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#36890;&#36807;&#26391;&#20043;&#19975;&#21160;&#21147;&#23398;&#36827;&#34892;&#22522;&#20110;&#33021;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#12289;&#36890;&#36807;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#38024;&#23545;&#36755;&#20837;&#20248;&#21270;&#23433;&#20840;&#21518;&#32512;&#65292;&#20197;&#21450;&#22522;&#20110;&#25105;&#20204;&#30340;&#25968;&#25454;&#22686;&#24378;&#23558;&#40065;&#26834;KNN&#19982;LLMs&#34701;&#21512;&#30340;&#22522;&#20110;&#34701;&#21512;&#30340;&#27169;&#22411;&#65292;RigorLLM&#20026;&#26377;&#23475;&#20869;&#23481;&#30340;&#35843;&#33410;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13031v1 Announce Type: cross  Abstract: Recent advancements in Large Language Models (LLMs) have showcased remarkable capabilities across various tasks in different domains. However, the emergence of biases and the potential for generating harmful content in LLMs, particularly under malicious inputs, pose significant challenges. Current mitigation strategies, while effective, are not resilient under adversarial attacks. This paper introduces Resilient Guardrails for Large Language Models (RigorLLM), a novel framework designed to efficiently and effectively moderate harmful and unsafe inputs and outputs for LLMs. By employing a multi-faceted approach that includes energy-based training data augmentation through Langevin dynamics, optimizing a safe suffix for inputs via minimax optimization, and integrating a fusion-based model combining robust KNN with LLMs based on our data augmentation, RigorLLM offers a robust solution to harmful content moderation. Our experimental evalua
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;LLM&#30340;&#19987;&#27880;&#25903;&#25345;&#20132;&#20114;&#27010;&#24565;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#19981;&#24178;&#25200;&#32676;&#20307;&#30340;&#24773;&#20917;&#19979;&#25903;&#25345;&#21644;&#24110;&#21161;&#20154;&#31867;&#12290;</title><link>https://arxiv.org/abs/2403.12533</link><description>&lt;p&gt;
&#26159;&#21542;&#24110;&#21161;&#65306;&#22522;&#20110;LLM&#30340;&#19987;&#27880;&#25903;&#25345;&#19982;&#20154;&#26426;&#32676;&#20307;&#20114;&#21160;
&lt;/p&gt;
&lt;p&gt;
To Help or Not to Help: LLM-based Attentive Support for Human-Robot Group Interactions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12533
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;LLM&#30340;&#19987;&#27880;&#25903;&#25345;&#20132;&#20114;&#27010;&#24565;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#19981;&#24178;&#25200;&#32676;&#20307;&#30340;&#24773;&#20917;&#19979;&#25903;&#25345;&#21644;&#24110;&#21161;&#20154;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#22914;&#20309;&#22312;&#20154;&#31867;&#32676;&#20307;&#20013;&#25552;&#20379;&#19981;&#24341;&#20154;&#27880;&#30446;&#30340;&#29289;&#29702;&#25903;&#25345;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;Attentive Support&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26426;&#22120;&#20154;&#19982;&#20154;&#31867;&#32676;&#20307;&#36827;&#34892;&#25903;&#25345;&#30340;&#20132;&#20114;&#27010;&#24565;&#12290;&#23427;&#23558;&#22330;&#26223;&#24863;&#30693;&#12289;&#23545;&#35805;&#33719;&#21462;&#12289;&#24773;&#20917;&#29702;&#35299;&#21644;&#34892;&#20026;&#29983;&#25104;&#19982;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#30456;&#32467;&#21512;&#12290;&#38500;&#20102;&#36981;&#24490;&#29992;&#25143;&#30340;&#25351;&#20196;&#65292;Attentive Support&#33021;&#22815;&#20915;&#23450;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#25903;&#25345;&#20154;&#31867;&#65292;&#24182;&#22312;&#19981;&#24178;&#25200;&#32676;&#20307;&#26102;&#20445;&#25345;&#27785;&#40664;&#12290;&#36890;&#36807;&#22810;&#26679;&#21270;&#30340;&#22330;&#26223;&#65292;&#25105;&#20204;&#23637;&#31034;&#21644;&#35780;&#20272;&#20102;&#26426;&#22120;&#20154;&#30340;&#19987;&#27880;&#34892;&#20026;&#65292;&#24403;&#38656;&#35201;&#26102;&#25903;&#25345;&#21644;&#24110;&#21161;&#20154;&#31867;&#65292;&#32780;&#22914;&#26524;&#19981;&#38656;&#35201;&#24110;&#21161;&#65292;&#21017;&#19981;&#20250;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12533v1 Announce Type: cross  Abstract: How can a robot provide unobtrusive physical support within a group of humans? We present Attentive Support, a novel interaction concept for robots to support a group of humans. It combines scene perception, dialogue acquisition, situation understanding, and behavior generation with the common-sense reasoning capabilities of Large Language Models (LLMs). In addition to following user instructions, Attentive Support is capable of deciding when and how to support the humans, and when to remain silent to not disturb the group. With a diverse set of scenarios, we show and evaluate the robot's attentive behavior, which supports and helps the humans when required, while not disturbing if no help is needed.
&lt;/p&gt;</description></item><item><title>KIF&#26694;&#26550;&#21033;&#29992;Wikidata&#20316;&#20026;&#36890;&#29992;&#35821;&#35328;&#65292;&#32467;&#21512;&#29992;&#25143;&#23450;&#20041;&#30340;&#26144;&#23556;&#65292;&#23454;&#29616;&#20102;&#24322;&#26500;&#30693;&#35782;&#24211;&#30340;&#34394;&#25311;&#38598;&#25104;&#65292;&#24418;&#25104;&#31867;&#20284;&#20110;&#25193;&#23637;Wikidata&#30340;&#34394;&#25311;&#30693;&#35782;&#24211;&#65292;&#21487;&#36890;&#36807;&#36807;&#28388;&#25509;&#21475;&#25110;SPARQL&#36827;&#34892;&#26597;&#35810;&#12290;</title><link>https://arxiv.org/abs/2403.10304</link><description>&lt;p&gt;
KIF&#65306;&#20351;&#29992;Wikidata&#36827;&#34892;&#24322;&#26500;&#30693;&#35782;&#24211;&#34394;&#25311;&#38598;&#25104;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
KIF: A Framework for Virtual Integration of Heterogeneous Knowledge Bases using Wikidata
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10304
&lt;/p&gt;
&lt;p&gt;
KIF&#26694;&#26550;&#21033;&#29992;Wikidata&#20316;&#20026;&#36890;&#29992;&#35821;&#35328;&#65292;&#32467;&#21512;&#29992;&#25143;&#23450;&#20041;&#30340;&#26144;&#23556;&#65292;&#23454;&#29616;&#20102;&#24322;&#26500;&#30693;&#35782;&#24211;&#30340;&#34394;&#25311;&#38598;&#25104;&#65292;&#24418;&#25104;&#31867;&#20284;&#20110;&#25193;&#23637;Wikidata&#30340;&#34394;&#25311;&#30693;&#35782;&#24211;&#65292;&#21487;&#36890;&#36807;&#36807;&#28388;&#25509;&#21475;&#25110;SPARQL&#36827;&#34892;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30693;&#35782;&#38598;&#25104;&#26694;&#26550;&#65288;&#31216;&#20026;KIF&#65289;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;Wikidata&#20316;&#20026;&#36890;&#29992;&#35821;&#35328;&#26469;&#38598;&#25104;&#24322;&#26500;&#30693;&#35782;&#24211;&#12290;&#36825;&#20123;&#30693;&#35782;&#24211;&#21487;&#20197;&#26159;&#19977;&#20803;&#32452;&#23384;&#20648;&#12289;&#20851;&#31995;&#22411;&#25968;&#25454;&#24211;&#12289;CSV&#25991;&#20214;&#31561;&#65292;&#21487;&#20197;&#25110;&#19981;&#21487;&#20197;&#20351;&#29992;RDF&#30340;Wikidata&#26041;&#35328;&#12290;KIF&#21033;&#29992;Wikidata&#30340;&#25968;&#25454;&#27169;&#22411;&#21644;&#35789;&#27719;&#20197;&#21450;&#29992;&#25143;&#23450;&#20041;&#30340;&#26144;&#23556;&#26469;&#23637;&#31034;&#38598;&#25104;&#24211;&#30340;&#32479;&#19968;&#35270;&#22270;&#65292;&#21516;&#26102;&#36319;&#36394;&#20854;&#38472;&#36848;&#30340;&#19978;&#19979;&#25991;&#21644;&#20986;&#22788;&#12290;&#32467;&#26524;&#26159;&#19968;&#20010;&#34892;&#20026;&#31867;&#20284;&#20110;&#8220;&#25193;&#23637;Wikidata&#8221;&#30340;&#34394;&#25311;&#30693;&#35782;&#24211;&#65292;&#21487;&#20197;&#36890;&#36807;&#39640;&#25928;&#36807;&#28388;&#25509;&#21475;&#25110;&#20351;&#29992;SPARQL&#36827;&#34892;&#26597;&#35810;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;KIF&#30340;&#35774;&#35745;&#21644;&#23454;&#29616;&#65292;&#35752;&#35770;&#20102;&#25105;&#20204;&#22914;&#20309;&#22312;&#21270;&#23398;&#39046;&#22495;&#65288;&#28041;&#21450;Wikidata&#12289;PubChem&#21644;IBM CIRCA&#65289;&#20013;&#20351;&#29992;&#23427;&#35299;&#20915;&#23454;&#38469;&#38598;&#25104;&#38382;&#39064;&#65292;&#24182;&#20171;&#32461;&#20102;KIF&#30340;&#24615;&#33021;&#21644;&#24320;&#38144;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10304v1 Announce Type: new  Abstract: We present a knowledge integration framework (called KIF) that uses Wikidata as a lingua franca to integrate heterogeneous knowledge bases. These can be triplestores, relational databases, CSV files, etc., which may or may not use the Wikidata dialect of RDF. KIF leverages Wikidata's data model and vocabulary plus user-defined mappings to expose a unified view of the integrated bases while keeping track of the context and provenance of their statements. The result is a virtual knowledge base which behaves like an "extended Wikidata" and which can be queried either through an efficient filter interface or using SPARQL. We present the design and implementation of KIF, discuss how we have used it to solve a real integration problem in the domain of chemistry (involving Wikidata, PubChem, and IBM CIRCA), and present experimental results on the performance and overhead of KIF.
&lt;/p&gt;</description></item><item><title>MCFEND&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#20013;&#25991;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#22810;&#28304;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#21333;&#19968;&#26469;&#28304;&#25968;&#25454;&#38598;&#24212;&#29992;&#20110;&#22810;&#28304;&#26032;&#38395;&#25968;&#25454;&#26102;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.09092</link><description>&lt;p&gt;
MCFEND&#65306;&#29992;&#20110;&#20013;&#25991;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#22810;&#28304;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MCFEND: A Multi-source Benchmark Dataset for Chinese Fake News Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09092
&lt;/p&gt;
&lt;p&gt;
MCFEND&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#20013;&#25991;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#22810;&#28304;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#21333;&#19968;&#26469;&#28304;&#25968;&#25454;&#38598;&#24212;&#29992;&#20110;&#22810;&#28304;&#26032;&#38395;&#25968;&#25454;&#26102;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#20551;&#26032;&#38395;&#22312;&#21508;&#20010;&#22312;&#32447;&#26469;&#28304;&#30340;&#26222;&#36941;&#20256;&#25773;&#23545;&#20844;&#20247;&#20135;&#29983;&#20102;&#37325;&#35201;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#20013;&#25991;&#20551;&#26032;&#38395;&#26816;&#27979;&#25968;&#25454;&#38598;&#20165;&#38480;&#20110;&#26469;&#33258;&#24494;&#21338;&#30340;&#26032;&#38395;&#12290;&#28982;&#32780;&#65292;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#30340;&#34394;&#20551;&#26032;&#38395;&#22312;&#20869;&#23481;&#21644;&#31038;&#20250;&#32972;&#26223;&#31561;&#21508;&#20010;&#26041;&#38754;&#34920;&#29616;&#20986;&#22810;&#26679;&#24615;&#12290;&#20165;&#22312;&#21333;&#19968;&#26032;&#38395;&#26469;&#28304;&#19978;&#35757;&#32451;&#30340;&#26041;&#27861;&#20960;&#20046;&#26080;&#27861;&#36866;&#29992;&#20110;&#29616;&#23454;&#22330;&#26223;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;&#23398;&#20064;&#33258;&#19968;&#20010;&#22823;&#22411;&#20013;&#25991;&#20551;&#26032;&#38395;&#26816;&#27979;&#25968;&#25454;&#38598;Weibo-21&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;F1&#20998;&#25968;&#65292;&#24403;&#27979;&#35797;&#25968;&#25454;&#25913;&#21464;&#20026;&#22810;&#28304;&#26032;&#38395;&#25968;&#25454;&#26102;&#65292;&#20174;0.943&#24613;&#21095;&#19979;&#38477;&#21040;0.470&#65292;&#26410;&#33021;&#35782;&#21035;&#36229;&#36807;&#19977;&#20998;&#20043;&#19968;&#30340;&#22810;&#28304;&#34394;&#20551;&#26032;&#38395;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#29992;&#20110;&#20013;&#25991;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#31532;&#19968;&#20010;&#22810;&#28304;&#22522;&#20934;&#25968;&#25454;&#38598;MCFEND&#65292;&#30001;&#25105;&#20204;&#20174;&#21508;&#31181;&#26469;&#28304;&#25910;&#38598;&#30340;&#26032;&#38395;&#32452;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09092v1 Announce Type: cross  Abstract: The prevalence of fake news across various online sources has had a significant influence on the public. Existing Chinese fake news detection datasets are limited to news sourced solely from Weibo. However, fake news originating from multiple sources exhibits diversity in various aspects, including its content and social context. Methods trained on purely one single news source can hardly be applicable to real-world scenarios. Our pilot experiment demonstrates that the F1 score of the state-of-the-art method that learns from a large Chinese fake news detection dataset, Weibo-21, drops significantly from 0.943 to 0.470 when the test data is changed to multi-source news data, failing to identify more than one-third of the multi-source fake news. To address this limitation, we constructed the first multi-source benchmark dataset for Chinese fake news detection, termed MCFEND, which is composed of news we collected from diverse sources suc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181; PathM3 &#26694;&#26550;&#65292;&#29992;&#20110;WSI&#20998;&#31867;&#21644;&#23383;&#24149;&#29983;&#25104;&#65292;&#37319;&#29992;&#20102;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#12289;&#22810;&#23454;&#20363;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26597;&#35810;&#24335;transformer&#26377;&#25928;&#23545;&#40784;WSIs&#19982;&#35786;&#26029;&#24615;&#23383;&#24149;&#12290;</title><link>https://arxiv.org/abs/2403.08967</link><description>&lt;p&gt;
PathM3: &#19968;&#31181;&#29992;&#20110;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#21644;&#23383;&#24149;&#29983;&#25104;&#30340;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#22810;&#23454;&#20363;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PathM3: A Multimodal Multi-Task Multiple Instance Learning Framework for Whole Slide Image Classification and Captioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08967
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181; PathM3 &#26694;&#26550;&#65292;&#29992;&#20110;WSI&#20998;&#31867;&#21644;&#23383;&#24149;&#29983;&#25104;&#65292;&#37319;&#29992;&#20102;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#12289;&#22810;&#23454;&#20363;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26597;&#35810;&#24335;transformer&#26377;&#25928;&#23545;&#40784;WSIs&#19982;&#35786;&#26029;&#24615;&#23383;&#24149;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#32452;&#32455;&#30149;&#29702;&#23398;&#39046;&#22495;&#65292;&#25972;&#20010;&#20999;&#29255;&#22270;&#20687;&#65288;WSIs&#65289;&#21644;&#35786;&#26029;&#24615;&#23383;&#24149;&#37117;&#20026;&#20570;&#20986;&#35786;&#26029;&#20915;&#31574;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#23558;WSIs&#19982;&#35786;&#26029;&#24615;&#23383;&#24149;&#23545;&#40784;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#20027;&#35201;&#28304;&#33258;&#20004;&#20010;&#22240;&#32032;&#65306;1&#65289;&#24040;&#22411;&#20687;&#32032;WSIs&#19981;&#36866;&#21512;&#30452;&#25509;&#36755;&#20837;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#22270;&#22359;&#38388;&#30340;&#20887;&#20313;&#24615;&#21644;&#30456;&#20851;&#24615;&#35201;&#27714;&#26356;&#22810;&#27880;&#24847;&#65307;2&#65289;&#30495;&#23454;&#30340;WSI&#35786;&#26029;&#24615;&#23383;&#24149;&#26497;&#20854;&#26377;&#38480;&#65292;&#20351;&#24471;&#38590;&#20197;&#35757;&#32451;&#20986;&#26377;&#25928;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08967v1 Announce Type: cross  Abstract: In the field of computational histopathology, both whole slide images (WSIs) and diagnostic captions provide valuable insights for making diagnostic decisions. However, aligning WSIs with diagnostic captions presents a significant challenge. This difficulty arises from two main factors: 1) Gigapixel WSIs are unsuitable for direct input into deep learning models, and the redundancy and correlation among the patches demand more attention; and 2) Authentic WSI diagnostic captions are extremely limited, making it difficult to train an effective model. To overcome these obstacles, we present PathM3, a multimodal, multi-task, multiple instance learning (MIL) framework for WSI classification and captioning. PathM3 adapts a query-based transformer to effectively align WSIs with diagnostic captions. Given that histopathology visual patterns are redundantly distributed across WSIs, we aggregate each patch feature with MIL method that considers t
&lt;/p&gt;</description></item><item><title>&#35777;&#26126;&#22312;1-D&#25968;&#25454;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#31561;&#20215;&#20110;&#35299;&#20915;&#19968;&#20010;&#20855;&#26377;&#22266;&#23450;&#29305;&#24449;&#23383;&#20856;&#30697;&#38453;&#30340;&#20984;Lasso&#38382;&#39064;&#65292;&#20026;&#20840;&#23616;&#26368;&#20248;&#32593;&#32476;&#21644;&#35299;&#31354;&#38388;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;</title><link>https://arxiv.org/abs/2403.01046</link><description>&lt;p&gt;
&#19968;&#20010;&#38236;&#23376;&#30340;&#24211;&#65306;&#20302;&#32500;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#20855;&#26377;&#21453;&#23556;&#29305;&#24449;&#30340;&#20984;Lasso&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Library of Mirrors: Deep Neural Nets in Low Dimensions are Convex Lasso Models with Reflection Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01046
&lt;/p&gt;
&lt;p&gt;
&#35777;&#26126;&#22312;1-D&#25968;&#25454;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#31561;&#20215;&#20110;&#35299;&#20915;&#19968;&#20010;&#20855;&#26377;&#22266;&#23450;&#29305;&#24449;&#23383;&#20856;&#30697;&#38453;&#30340;&#20984;Lasso&#38382;&#39064;&#65292;&#20026;&#20840;&#23616;&#26368;&#20248;&#32593;&#32476;&#21644;&#35299;&#31354;&#38388;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#22312;1-D&#25968;&#25454;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#31561;&#20215;&#20110;&#35299;&#20915;&#19968;&#20010;&#24102;&#26377;&#22266;&#23450;&#12289;&#26126;&#30830;&#23450;&#20041;&#30340;&#29305;&#24449;&#23383;&#20856;&#30697;&#38453;&#30340;&#20984;Lasso&#38382;&#39064;&#12290;&#20855;&#20307;&#30340;&#23383;&#20856;&#21462;&#20915;&#20110;&#28608;&#27963;&#20989;&#25968;&#21644;&#28145;&#24230;&#12290;&#25105;&#20204;&#32771;&#34385;&#20855;&#26377;&#20998;&#27573;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#20004;&#23618;&#32593;&#32476;&#65292;&#28145;&#31364;&#30340;ReLU&#32593;&#32476;&#26368;&#22810;&#26377;4&#23618;&#65292;&#20197;&#21450;&#20855;&#26377;&#31526;&#21495;&#28608;&#27963;&#21644;&#20219;&#24847;&#28145;&#24230;&#30340;&#30697;&#24418;&#21644;&#26641;&#32593;&#32476;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#22312;ReLU&#32593;&#32476;&#20013;&#65292;&#31532;&#22235;&#23618;&#21019;&#24314;&#20195;&#34920;&#35757;&#32451;&#25968;&#25454;&#20851;&#20110;&#33258;&#36523;&#30340;&#21453;&#23556;&#30340;&#29305;&#24449;&#12290;Lasso&#34920;&#31034;&#27861;&#25581;&#31034;&#20102;&#20840;&#23616;&#26368;&#20248;&#32593;&#32476;&#21644;&#35299;&#31354;&#38388;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01046v1 Announce Type: cross  Abstract: We prove that training neural networks on 1-D data is equivalent to solving a convex Lasso problem with a fixed, explicitly defined dictionary matrix of features. The specific dictionary depends on the activation and depth. We consider 2-layer networks with piecewise linear activations, deep narrow ReLU networks with up to 4 layers, and rectangular and tree networks with sign activation and arbitrary depth. Interestingly in ReLU networks, a fourth layer creates features that represent reflections of training data about themselves. The Lasso representation sheds insight to globally optimal networks and the solution landscape.
&lt;/p&gt;</description></item><item><title>&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#33021;&#26377;&#21161;&#20110;&#29702;&#35299;&#36845;&#20195;&#32452;&#21512;&#25293;&#21334;&#65292;&#20294;&#20854;&#26377;&#25928;&#37096;&#32626;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#32771;&#34385;&#20445;&#25345;&#28216;&#25103;&#21487;&#22788;&#29702;&#24615;&#20197;&#21450;&#36991;&#20813;&#21508;&#31181;&#31639;&#27861;&#30340;&#38519;&#38449;&#12290;</title><link>https://arxiv.org/abs/2402.19420</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#29702;&#35299;&#36845;&#20195;&#32452;&#21512;&#25293;&#21334;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Understanding Iterative Combinatorial Auction Designs via Multi-Agent Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19420
&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#33021;&#26377;&#21161;&#20110;&#29702;&#35299;&#36845;&#20195;&#32452;&#21512;&#25293;&#21334;&#65292;&#20294;&#20854;&#26377;&#25928;&#37096;&#32626;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#32771;&#34385;&#20445;&#25345;&#28216;&#25103;&#21487;&#22788;&#29702;&#24615;&#20197;&#21450;&#36991;&#20813;&#21508;&#31181;&#31639;&#27861;&#30340;&#38519;&#38449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36845;&#20195;&#32452;&#21512;&#25293;&#21334;&#22312;&#39640;&#39118;&#38505;&#22330;&#26223;&#22914;&#39057;&#35889;&#25293;&#21334;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#31867;&#25293;&#21334;&#38590;&#20197;&#22312;&#29702;&#35770;&#19978;&#29702;&#35299;&#65292;&#20351;&#24471;&#31454;&#26631;&#32773;&#24456;&#38590;&#20915;&#23450;&#22914;&#20309;&#34892;&#21160;&#20197;&#21450;&#35774;&#35745;&#32773;&#24456;&#38590;&#20248;&#21270;&#25293;&#21334;&#35268;&#21017;&#20197;&#30830;&#20445;&#29702;&#24819;&#30340;&#32467;&#26524;&#65292;&#22914;&#39640;&#25910;&#20837;&#25110;&#31119;&#21033;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26159;&#21542;&#33021;&#22815;&#29992;&#20110;&#29702;&#35299;&#36845;&#20195;&#32452;&#21512;&#25293;&#21334;&#65292;&#37492;&#20110;&#36825;&#20123;&#31639;&#27861;&#26368;&#36817;&#22312;&#20854;&#20182;&#39046;&#22495;&#24050;&#32463;&#26174;&#31034;&#20986;&#23454;&#35777;&#25104;&#21151;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30830;&#23454;&#21487;&#20197;&#21463;&#30410;&#20110;&#25293;&#21334;&#20998;&#26512;&#65292;&#20294;&#26377;&#25928;&#37096;&#32626;&#24182;&#19981;&#23481;&#26131;&#12290;&#25105;&#20204;&#39318;&#20808;&#25551;&#36848;&#20102;&#20445;&#25345;&#32467;&#26524;&#28216;&#25103;&#21487;&#22788;&#29702;&#30340;&#24314;&#27169;&#20915;&#31574;&#65292;&#21516;&#26102;&#19981;&#29306;&#29298;&#35832;&#22914;&#20449;&#24687;&#19981;&#23436;&#20840;&#25110;&#31454;&#26631;&#32773;&#38388;&#19981;&#23545;&#31216;&#31561;&#37325;&#35201;&#29305;&#24449;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#22914;&#20309;&#36991;&#20813;&#21508;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#38519;&#38449;&#65292;&#22914;&#20309;&#20811;&#26381;&#25361;&#25112;&#20197;&#21450;&#22914;&#20309;&#24212;&#23545;&#21508;&#31181;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19420v1 Announce Type: cross  Abstract: Iterative combinatorial auctions are widely used in high stakes settings such as spectrum auctions. Such auctions can be hard to understand analytically, making it difficult for bidders to determine how to behave and for designers to optimize auction rules to ensure desirable outcomes such as high revenue or welfare. In this paper, we investigate whether multi-agent reinforcement learning (MARL) algorithms can be used to understand iterative combinatorial auctions, given that these algorithms have recently shown empirical success in several other domains. We find that MARL can indeed benefit auction analysis, but that deploying it effectively is nontrivial. We begin by describing modelling decisions that keep the resulting game tractable without sacrificing important features such as imperfect information or asymmetry between bidders. We also discuss how to navigate pitfalls of various MARL algorithms, how to overcome challenges in ver
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#38750;&#23433;&#20840;&#20851;&#38190;&#30340;&#25112;&#30053;&#20132;&#36890;&#27969;&#37327;&#31649;&#29702;&#29615;&#22659;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CHATATC&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#22823;&#37327;&#21382;&#21490;&#25968;&#25454;&#38598;&#23454;&#29616;&#23545;&#35805;&#31995;&#32479;&#65292;&#24182;&#27979;&#35797;&#20102;&#20854;&#26597;&#35810;&#21644;&#21709;&#24212;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14850</link><description>&lt;p&gt;
CHATATC&#65306;&#29992;&#20110;&#25903;&#25345;&#25112;&#30053;&#31354;&#20013;&#20132;&#36890;&#27969;&#37327;&#31649;&#29702;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
CHATATC: Large Language Model-Driven Conversational Agents for Supporting Strategic Air Traffic Flow Management
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14850
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#38750;&#23433;&#20840;&#20851;&#38190;&#30340;&#25112;&#30053;&#20132;&#36890;&#27969;&#37327;&#31649;&#29702;&#29615;&#22659;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CHATATC&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#22823;&#37327;&#21382;&#21490;&#25968;&#25454;&#38598;&#23454;&#29616;&#23545;&#35805;&#31995;&#32479;&#65292;&#24182;&#27979;&#35797;&#20102;&#20854;&#26597;&#35810;&#21644;&#21709;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#36890;&#36807;&#35832;&#22914;ChatGPT&#31561;&#20844;&#24320;&#21487;&#29992;&#24037;&#20855;&#24555;&#36895;&#36208;&#32418;&#12290;LLMs&#22312;&#20010;&#20154;&#21644;&#19987;&#19994;&#39046;&#22495;&#30340;&#24212;&#29992;&#24471;&#21040;&#25512;&#21160;&#65292;&#26159;&#30001;&#20110;&#20154;&#31867;&#29992;&#25143;&#19982;ChatGPT&#31561;&#35745;&#31639;&#26426;&#24212;&#29992;&#20043;&#38388;&#33258;&#28982;&#30340;&#20114;&#21160;&#65292;&#20197;&#21450;&#24378;&#22823;&#30340;&#25688;&#35201;&#21644;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#36825;&#20123;&#29983;&#25104;AI&#24037;&#20855;&#22914;&#20309;&#22312;&#38750;&#23433;&#20840;&#20851;&#38190;&#30340;&#25112;&#30053;&#20132;&#36890;&#27969;&#37327;&#31649;&#29702;&#29615;&#22659;&#20013;&#37096;&#32626;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22522;&#20110;&#21253;&#21547;&#36229;&#36807;80,000&#20010;GDP&#23454;&#26045;&#12289;&#20462;&#35746;&#21644;&#21462;&#28040;&#30340;&#22823;&#37327;&#21382;&#21490;&#25968;&#25454;&#38598;&#65292;&#23545;CHATATC&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;CHATATC&#30340;&#26597;&#35810;&#21644;&#21709;&#24212;&#33021;&#21147;&#65292;&#35760;&#24405;&#20102;&#25104;&#21151;&#20043;&#22788;&#65288;&#20363;&#22914;&#65292;&#25552;&#20379;&#27491;&#30830;&#30340;GDP&#29575;&#12289;&#25345;&#32493;&#26102;&#38388;&#21644;&#21407;&#22240;&#65289;&#20197;&#21450;&#19981;&#36275;&#20043;&#22788;&#65288;&#20363;&#22914;&#65292;&#26368;&#39640;&#27700;&#24179;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14850v1 Announce Type: cross  Abstract: Generative artificial intelligence (AI) and large language models (LLMs) have gained rapid popularity through publicly available tools such as ChatGPT. The adoption of LLMs for personal and professional use is fueled by the natural interactions between human users and computer applications such as ChatGPT, along with powerful summarization and text generation capabilities. Given the widespread use of such generative AI tools, in this work we investigate how these tools can be deployed in a non-safety critical, strategic traffic flow management setting. Specifically, we train an LLM, CHATATC, based on a large historical data set of Ground Delay Program (GDP) issuances, spanning 2000-2023 and consisting of over 80,000 GDP implementations, revisions, and cancellations. We test the query and response capabilities of CHATATC, documenting successes (e.g., providing correct GDP rates, durations, and reason) and shortcomings (e.g,. superlative
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RefuteBench&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21453;&#39539;&#25351;&#20196;&#30340;&#36981;&#24490;&#33021;&#21147;&#65292;&#21457;&#29616;LLMs&#20542;&#21521;&#20110;&#22266;&#25191;&#20110;&#20854;&#20869;&#37096;&#30693;&#35782;&#32780;&#26080;&#27861;&#36981;&#20174;&#29992;&#25143;&#21453;&#39304;&#12290;</title><link>https://arxiv.org/abs/2402.13463</link><description>&lt;p&gt;
RefuteBench&#65306;&#35780;&#20272;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39539;&#25351;&#20196;&#36981;&#24490;
&lt;/p&gt;
&lt;p&gt;
RefuteBench: Evaluating Refuting Instruction-Following for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RefuteBench&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21453;&#39539;&#25351;&#20196;&#30340;&#36981;&#24490;&#33021;&#21147;&#65292;&#21457;&#29616;LLMs&#20542;&#21521;&#20110;&#22266;&#25191;&#20110;&#20854;&#20869;&#37096;&#30693;&#35782;&#32780;&#26080;&#27861;&#36981;&#20174;&#29992;&#25143;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24212;&#29992;&#33539;&#22260;&#26085;&#30410;&#25193;&#22823;&#12290;&#22312;&#23454;&#38469;&#20351;&#29992;&#20013;&#65292;&#29992;&#25143;&#21487;&#33021;&#26681;&#25454;&#27169;&#22411;&#30340;&#36755;&#20986;&#25552;&#20379;&#21453;&#39304;&#65292;&#24076;&#26395;&#24471;&#21040;&#19968;&#20010;&#21487;&#20197;&#26681;&#25454;&#20182;&#20204;&#30340;&#21453;&#39304;&#23436;&#25104;&#21709;&#24212;&#30340;&#21709;&#24212;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#33021;&#21542;&#24688;&#24403;&#22320;&#21709;&#24212;&#29992;&#25143;&#30340;&#21453;&#39539;&#21453;&#39304;&#24182;&#22987;&#32456;&#25191;&#34892;&#19979;&#21435;&#23578;&#26410;&#24471;&#21040;&#24443;&#24213;&#20998;&#26512;&#12290;&#22522;&#20110;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;RefuteBench&#65292;&#28085;&#30422;&#20102;&#35832;&#22914;&#38382;&#31572;&#12289;&#26426;&#22120;&#32763;&#35793;&#21644;&#30005;&#23376;&#37038;&#20214;&#25776;&#20889;&#31561;&#20219;&#21153;&#12290;&#35780;&#20272;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#31215;&#26497;&#25509;&#21463;&#21453;&#39539;&#25351;&#20196;&#24418;&#24335;&#30340;&#21453;&#39304;&#65292;&#24182;&#26159;&#21542;&#33021;&#22815;&#22312;&#23545;&#35805;&#20013;&#22987;&#32456;&#36981;&#24490;&#29992;&#25143;&#38656;&#27714;&#12290;&#25105;&#20204;&#23545;&#20247;&#22810;LLMs&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;LLMs&#20542;&#21521;&#22266;&#25191;&#65292;&#21363;&#20542;&#21521;&#20110;&#20854;&#20869;&#37096;&#30693;&#35782;&#65292;&#32463;&#24120;&#26410;&#33021;&#36981;&#23432;&#29992;&#25143;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13463v1 Announce Type: cross  Abstract: The application scope of large language models (LLMs) is increasingly expanding. In practical use, users might provide feedback based on the model's output, hoping for a responsive model that can complete responses according to their feedback. Whether the model can appropriately respond to users' refuting feedback and consistently follow through with execution has not been thoroughly analyzed. In light of this, this paper proposes a comprehensive benchmark, RefuteBench, covering tasks such as question answering, machine translation, and email writing. The evaluation aims to assess whether models can positively accept feedback in form of refuting instructions and whether they can consistently adhere to user demands throughout the conversation. We conduct evaluations on numerous LLMs and find that LLMs are stubborn, i.e. exhibit inclination to their internal knowledge, often failing to comply with user feedback. Additionally, as the leng
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#36827;&#21270;&#31574;&#30053;(ES)&#26469;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#65292;&#20197;&#36890;&#36807;&#30452;&#25509;&#31574;&#30053;&#25628;&#32034;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#22522;&#20934;&#38382;&#39064;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;ES&#21487;&#20197;&#22312;&#35768;&#22810;&#22522;&#20934;&#20219;&#21153;&#20013;&#25214;&#21040;&#26377;&#25928;&#30340;&#32447;&#24615;&#31574;&#30053;&#65292;&#19982;&#24403;&#21069;&#20351;&#29992;&#26356;&#22823;&#32593;&#32476;&#30340;DRL&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#34920;&#26126;&#24403;&#21069;&#30340;&#22522;&#20934;&#38382;&#39064;&#27604;&#20197;&#24448;&#35748;&#20026;&#30340;&#26356;&#23481;&#26131;&#35299;&#20915;&#12290;</title><link>https://arxiv.org/abs/2402.06912</link><description>&lt;p&gt;
&#29992;&#32447;&#24615;&#31574;&#30053;&#32593;&#32476;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Deep Reinforcement Learning Benchmarks with Linear Policy Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#36827;&#21270;&#31574;&#30053;(ES)&#26469;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#65292;&#20197;&#36890;&#36807;&#30452;&#25509;&#31574;&#30053;&#25628;&#32034;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#22522;&#20934;&#38382;&#39064;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;ES&#21487;&#20197;&#22312;&#35768;&#22810;&#22522;&#20934;&#20219;&#21153;&#20013;&#25214;&#21040;&#26377;&#25928;&#30340;&#32447;&#24615;&#31574;&#30053;&#65292;&#19982;&#24403;&#21069;&#20351;&#29992;&#26356;&#22823;&#32593;&#32476;&#30340;DRL&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#34920;&#26126;&#24403;&#21069;&#30340;&#22522;&#20934;&#38382;&#39064;&#27604;&#20197;&#24448;&#35748;&#20026;&#30340;&#26356;&#23481;&#26131;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#31639;&#27861;&#33021;&#22815;&#23398;&#20064;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#35299;&#20915;&#20687;Atari&#28216;&#25103;&#21644;&#26426;&#22120;&#20154;&#20219;&#21153;&#36825;&#26679;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#65292;&#20294;&#31639;&#27861;&#22797;&#26434;&#65292;&#35757;&#32451;&#26102;&#38388;&#24448;&#24448;&#36739;&#38271;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36827;&#21270;&#31574;&#30053;(ES)&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20043;&#38388;&#30340;&#34920;&#29616;&#24046;&#24322;&#12290;&#25105;&#20204;&#20351;&#29992;ES&#36890;&#36807;&#31070;&#32463;&#36827;&#21270;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#65292;&#36890;&#36807;&#30452;&#25509;&#31574;&#30053;&#25628;&#32034;&#26469;&#23436;&#25104;&#12290;&#25105;&#20204;&#23545;&#24120;&#35268;&#32593;&#32476;&#21644;&#30001;&#19968;&#20010;&#20174;&#35266;&#27979;&#21040;&#21160;&#20316;&#30340;&#21333;&#19968;&#32447;&#24615;&#23618;&#32452;&#25104;&#30340;&#31574;&#30053;&#32593;&#32476;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65307;&#23545;&#20110;&#19977;&#31181;&#32463;&#20856;&#30340;ES&#26041;&#27861;&#21644;&#19977;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#22914;PPO&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ES&#21487;&#20197;&#22312;&#35768;&#22810;RL&#22522;&#20934;&#20219;&#21153;&#20013;&#25214;&#21040;&#26377;&#25928;&#30340;&#32447;&#24615;&#31574;&#30053;&#65292;&#32780;DRL&#26041;&#27861;&#21482;&#33021;&#20351;&#29992;&#26356;&#22823;&#30340;&#32593;&#32476;&#25214;&#21040;&#25104;&#21151;&#30340;&#31574;&#30053;&#65292;&#36825;&#34920;&#26126;&#24403;&#21069;&#30340;&#22522;&#20934;&#38382;&#39064;&#27604;&#20197;&#21069;&#35748;&#20026;&#30340;&#26356;&#23481;&#26131;&#35299;&#20915;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#21363;&#20351;&#23545;&#20110;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;ES&#30340;&#32467;&#26524;&#20063;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Deep Reinforcement Learning (DRL) methods can learn effective policies for challenging problems such as Atari games and robotics tasks, algorithms are complex and training times are often long. This study investigates how evolution strategies (ES) perform compared to gradient-based deep reinforcement learning methods. We use ES to optimize the weights of a neural network via neuroevolution, performing direct policy search. We benchmark both regular networks and policy networks consisting of a single linear layer from observations to actions; for three classical ES methods and for three gradient-based methods such as PPO. Our results reveal that ES can find effective linear policies for many RL benchmark tasks, in contrast to DRL methods that can only find successful policies using much larger networks, suggesting that current benchmarks are easier to solve than previously assumed. Interestingly, also for higher complexity tasks, ES achieves results comparable to gradient-based
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#22810;&#26680;&#37327;&#23376;&#26550;&#26500;&#30340;&#30005;&#36335;&#20998;&#21306;&#26041;&#27861;&#65292;&#26088;&#22312;&#25512;&#21160;&#37327;&#23376;&#35745;&#31639;&#21644;&#22270;&#20998;&#21306;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2401.17976</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23545;&#22810;&#26680;&#37327;&#23376;&#26550;&#26500;&#36827;&#34892;&#30005;&#36335;&#20998;&#21306;
&lt;/p&gt;
&lt;p&gt;
Circuit Partitioning for Multi-Core Quantum Architectures with Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#22810;&#26680;&#37327;&#23376;&#26550;&#26500;&#30340;&#30005;&#36335;&#20998;&#21306;&#26041;&#27861;&#65292;&#26088;&#22312;&#25512;&#21160;&#37327;&#23376;&#35745;&#31639;&#21644;&#22270;&#20998;&#21306;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#21033;&#29992;&#37327;&#23376;&#21147;&#23398;&#30340;&#29420;&#29305;&#24615;&#36136;&#65292;&#20855;&#26377;&#35299;&#20915;&#32463;&#20856;&#38590;&#39064;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#37327;&#23376;&#26550;&#26500;&#30340;&#21487;&#25193;&#23637;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#22810;&#26680;&#37327;&#23376;&#26550;&#26500;&#34987;&#25552;&#20986;&#26469;&#35299;&#20915;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#24341;&#21457;&#20102;&#30828;&#20214;&#12289;&#36890;&#20449;&#21644;&#32534;&#35793;&#31561;&#19968;&#31995;&#21015;&#26032;&#30340;&#25361;&#25112;&#12290;&#20854;&#20013;&#20043;&#19968;&#26159;&#23558;&#37327;&#23376;&#31639;&#27861;&#36866;&#24212;&#21040;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#19981;&#21516;&#26680;&#24515;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#30005;&#36335;&#20998;&#21306;&#30340;&#26032;&#26041;&#27861;&#65292;&#20026;&#37327;&#23376;&#35745;&#31639;&#21644;&#22270;&#20998;&#21306;&#30340;&#36827;&#23637;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#36825;&#39033;&#24037;&#20316;&#26159;&#23558;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#25972;&#21512;&#21040;&#37327;&#23376;&#30005;&#36335;&#26144;&#23556;&#20013;&#30340;&#31532;&#19968;&#27493;&#65292;&#20026;&#35299;&#20915;&#27492;&#31867;&#38382;&#39064;&#24320;&#36767;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum computing holds immense potential for solving classically intractable problems by leveraging the unique properties of quantum mechanics. The scalability of quantum architectures remains a significant challenge. Multi-core quantum architectures are proposed to solve the scalability problem, arising a new set of challenges in hardware, communications and compilation, among others. One of these challenges is to adapt a quantum algorithm to fit within the different cores of the quantum computer. This paper presents a novel approach for circuit partitioning using Deep Reinforcement Learning, contributing to the advancement of both quantum computing and graph partitioning. This work is the first step in integrating Deep Reinforcement Learning techniques into Quantum Circuit Mapping, opening the door to a new paradigm of solutions to such problems.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#26041;&#21521;&#24615;&#65292;&#21457;&#29616;&#20102;&#27169;&#22411;&#22312;&#24314;&#27169;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#19978;&#23384;&#22312;&#26102;&#38388;&#19978;&#30340;&#19981;&#23545;&#31216;&#24615;&#12290;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#24046;&#24322;&#29702;&#35770;&#19978;&#26159;&#19981;&#24212;&#35813;&#23384;&#22312;&#30340;&#12290;&#36890;&#36807;&#31232;&#30095;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#32771;&#34385;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#30340;&#20986;&#29616;&#12290;</title><link>https://arxiv.org/abs/2401.17505</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26102;&#38388;&#31661;&#22836;
&lt;/p&gt;
&lt;p&gt;
Arrows of Time for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17505
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#26041;&#21521;&#24615;&#65292;&#21457;&#29616;&#20102;&#27169;&#22411;&#22312;&#24314;&#27169;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#19978;&#23384;&#22312;&#26102;&#38388;&#19978;&#30340;&#19981;&#23545;&#31216;&#24615;&#12290;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#24046;&#24322;&#29702;&#35770;&#19978;&#26159;&#19981;&#24212;&#35813;&#23384;&#22312;&#30340;&#12290;&#36890;&#36807;&#31232;&#30095;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#32771;&#34385;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#26102;&#38388;&#26041;&#21521;&#24615;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#29575;&#24314;&#27169;&#12290;&#25105;&#20204;&#22312;&#23454;&#35777;&#19978;&#21457;&#29616;&#36825;&#31867;&#27169;&#22411;&#22312;&#24314;&#27169;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#19978;&#23384;&#22312;&#26102;&#38388;&#19978;&#30340;&#19981;&#23545;&#31216;&#24615;&#65306;&#39044;&#27979;&#19979;&#19968;&#20010;&#35760;&#21495;&#21644;&#39044;&#27979;&#21069;&#19968;&#20010;&#35760;&#21495;&#26102;&#30340;&#24179;&#22343;&#23545;&#25968;&#22256;&#24785;&#24230;&#23384;&#22312;&#24046;&#24322;&#12290;&#36825;&#31181;&#24046;&#24322;&#26082;&#24494;&#22937;&#21448;&#22312;&#19981;&#21516;&#30340;&#27169;&#24577;&#65288;&#35821;&#35328;&#12289;&#27169;&#22411;&#22823;&#23567;&#12289;&#35757;&#32451;&#26102;&#38388;&#31561;&#65289;&#19979;&#38750;&#24120;&#19968;&#33268;&#12290;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#22312;&#29702;&#35770;&#19978;&#26159;&#20196;&#20154;&#24778;&#35766;&#30340;&#65292;&#19981;&#24212;&#35813;&#23384;&#22312;&#36825;&#26679;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#35299;&#37322;&#20102;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#22914;&#20309;&#20986;&#29616;&#22312;&#31232;&#30095;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#32771;&#34385;&#20013;&#65292;&#24182;&#27010;&#36848;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#24102;&#26469;&#30340;&#19968;&#20123;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the probabilistic modeling performed by Autoregressive Large Language Models through the angle of time directionality. We empirically find a time asymmetry exhibited by such models in their ability to model natural language: a difference in the average log-perplexity when trying to predict the next token versus when trying to predict the previous one. This difference is at the same time subtle and very consistent across various modalities (language, model size, training time, ...). Theoretically, this is surprising: from an information-theoretic point of view, there should be no such difference. We provide a theoretical framework to explain how such an asymmetry can appear from sparsity and computational complexity considerations, and outline a number of perspectives opened by our results.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#19994;&#21153;&#27969;&#31243;&#35299;&#37322;&#30340;SAX4BPM&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#38598;&#25104;&#26469;&#32508;&#21512;&#21508;&#31181;&#36755;&#20837;&#35201;&#32032;&#65292;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#24773;&#22659;&#24863;&#30693;&#35299;&#37322;&#65288;SAX&#65289;&#12290;</title><link>http://arxiv.org/abs/2401.12846</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22914;&#20309;&#35299;&#37322;&#19994;&#21153;&#27969;&#31243;&#65311;
&lt;/p&gt;
&lt;p&gt;
How well can large language models explain business processes?. (arXiv:2401.12846v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12846
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#19994;&#21153;&#27969;&#31243;&#35299;&#37322;&#30340;SAX4BPM&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#38598;&#25104;&#26469;&#32508;&#21512;&#21508;&#31181;&#36755;&#20837;&#35201;&#32032;&#65292;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#24773;&#22659;&#24863;&#30693;&#35299;&#37322;&#65288;SAX&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#33021;&#22312;&#26410;&#26469;&#30340;AI&#36741;&#21161;&#19994;&#21153;&#27969;&#31243;&#31649;&#29702;&#31995;&#32479;&#65288;ABPMSs&#65289;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#20854;&#21151;&#33021;&#28085;&#30422;&#31995;&#32479;&#29983;&#21629;&#21608;&#26399;&#30340;&#21508;&#20010;&#38454;&#27573;&#12290;&#20854;&#20013;&#19968;&#20010;&#31995;&#32479;&#21151;&#33021;&#26159;&#24773;&#22659;&#24863;&#30693;&#35299;&#37322;&#65288;SAX&#65289;&#65292;&#23427;&#28041;&#21450;&#29983;&#25104;&#22312;&#32771;&#34385;&#25152;&#35299;&#37322;&#26465;&#20214;&#20986;&#29616;&#30340;&#27969;&#31243;&#19978;&#19979;&#25991;&#30340;&#21069;&#25552;&#19979;&#26082;&#31526;&#21512;&#22240;&#26524;&#20851;&#31995;&#21448;&#21487;&#20154;&#31867;&#35299;&#35835;&#30340;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24320;&#21457;&#29992;&#20110;&#29983;&#25104;SAX&#35299;&#37322;&#30340;SAX4BPM&#26694;&#26550;&#12290;SAX4BPM&#22871;&#20214;&#21253;&#25324;&#19968;&#32452;&#26381;&#21153;&#21644;&#19968;&#20010;&#20013;&#22830;&#30693;&#35782;&#24211;&#12290;&#36825;&#20123;&#26381;&#21153;&#30340;&#21151;&#33021;&#26159;&#33719;&#21462;&#26500;&#25104;SAX&#35299;&#37322;&#30340;&#21508;&#31181;&#30693;&#35782;&#35201;&#32032;&#12290;&#20854;&#20013;&#19968;&#20010;&#21019;&#26032;&#24615;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#22240;&#26524;&#36807;&#31243;&#25191;&#34892;&#35270;&#22270;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#35813;&#26694;&#26550;&#19982;LLM&#38598;&#25104;&#65292;&#20197;&#21033;&#29992;&#20854;&#32508;&#21512;&#21508;&#31181;&#36755;&#20837;&#35201;&#32032;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#36827;SAX&#35299;&#37322;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are likely to play a prominent role in future AI-augmented business process management systems (ABPMSs) catering functionalities across all system lifecycle stages. One such system's functionality is Situation-Aware eXplainability (SAX), which relates to generating causally sound and yet human-interpretable explanations that take into account the process context in which the explained condition occurred. In this paper, we present the SAX4BPM framework developed to generate SAX explanations. The SAX4BPM suite consists of a set of services and a central knowledge repository. The functionality of these services is to elicit the various knowledge ingredients that underlie SAX explanations. A key innovative component among these ingredients is the causal process execution view. In this work, we integrate the framework with an LLM to leverage its power to synthesize the various input ingredients for the sake of improved SAX explanations. Since the use of LLMs for
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#32508;&#21512;&#27010;&#36848;&#20102;&#20851;&#20110;&#20174;&#24102;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#20013;&#23398;&#20064;&#30340;&#29616;&#26377;&#30740;&#31350;&#65292;&#24182;&#26681;&#25454;&#23398;&#20064;&#31574;&#30053;&#12289;&#27169;&#22411;&#26550;&#26500;&#21644;&#23454;&#38469;&#24212;&#29992;&#31561;&#26041;&#38754;&#23545;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#29616;&#26377;&#30740;&#31350;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2401.09769</link><description>&lt;p&gt;
&#36208;&#21521;&#24322;&#36136;&#22270;&#23398;&#20064;&#65306;&#36827;&#23637;&#19982;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
Towards Learning from Graphs with Heterophily: Progress and Future. (arXiv:2401.09769v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#32508;&#21512;&#27010;&#36848;&#20102;&#20851;&#20110;&#20174;&#24102;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#20013;&#23398;&#20064;&#30340;&#29616;&#26377;&#30740;&#31350;&#65292;&#24182;&#26681;&#25454;&#23398;&#20064;&#31574;&#30053;&#12289;&#27169;&#22411;&#26550;&#26500;&#21644;&#23454;&#38469;&#24212;&#29992;&#31561;&#26041;&#38754;&#23545;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#29616;&#26377;&#30740;&#31350;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#26159;&#29992;&#26469;&#27169;&#25311;&#29616;&#23454;&#19990;&#30028;&#23454;&#20307;&#20043;&#38388;&#22797;&#26434;&#20851;&#31995;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#26368;&#36817;&#65292;&#24322;&#36136;&#22270;&#65292;&#20854;&#20013;&#36830;&#25509;&#30340;&#33410;&#28857;&#24448;&#24448;&#20855;&#26377;&#19981;&#21516;&#30340;&#26631;&#31614;&#25110;&#19981;&#21516;&#30340;&#29305;&#24449;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#24182;&#25214;&#21040;&#20102;&#35768;&#22810;&#24212;&#29992;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20154;&#20204;&#20063;&#22312;&#19981;&#26029;&#21162;&#21147;&#25512;&#36827;&#20174;&#24322;&#36136;&#22270;&#20013;&#23398;&#20064;&#12290;&#34429;&#28982;&#26377;&#20851;&#35813;&#20027;&#39064;&#30340;&#35843;&#26597;&#23384;&#22312;&#65292;&#20294;&#23427;&#20204;&#21482;&#20851;&#27880;&#20110;&#24322;&#36136;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#32780;&#24573;&#30053;&#20102;&#24322;&#36136;&#22270;&#23398;&#20064;&#30340;&#20854;&#20182;&#23376;&#20027;&#39064;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#22238;&#39038;&#20102;&#20851;&#20110;&#20174;&#24102;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#20013;&#23398;&#20064;&#30340;&#29616;&#26377;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;180&#22810;&#31687;&#35770;&#25991;&#65292;&#20171;&#32461;&#20102;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#23618;&#27425;&#20998;&#31867;&#27861;&#23545;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#31867;&#65292;&#21253;&#25324;&#23398;&#20064;&#31574;&#30053;&#12289;&#27169;&#22411;&#26550;&#26500;&#21644;&#23454;&#38469;&#24212;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#29616;&#26377;&#30740;&#31350;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphs are structured data that models complex relations between real-world entities. Heterophilous graphs, where linked nodes are prone to be with different labels or dissimilar features, have recently attracted significant attention and found many applications. Meanwhile, increasing efforts have been made to advance learning from heterophilous graphs. Although there exist surveys on the relevant topic, they focus on heterophilous GNNs, which are only sub-topics of heterophilous graph learning. In this survey, we comprehensively overview existing works on learning from graphs with heterophily.First, we collect over 180 publications and introduce the development of this field. Then, we systematically categorize existing methods based on a hierarchical taxonomy including learning strategies, model architectures and practical applications. Finally, we discuss the primary challenges of existing studies and highlight promising avenues for future research.More publication details and corres
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33647;&#29289;&#19987;&#21033;&#21644;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#24211;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#35782;&#21035;&#20855;&#26377;&#25216;&#26415;&#28508;&#21147;&#21644;&#31185;&#23398;&#35777;&#25454;&#30340;&#33647;&#29289;&#20877;&#23450;&#20301;&#20505;&#36873;&#29289;&#12290;&#36890;&#36807;&#26500;&#24314;&#31185;&#23398;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#21644;&#22522;&#20110;&#19987;&#21033;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#65292;&#25105;&#20204;&#21487;&#20197;&#32508;&#21512;&#20998;&#26512;&#22810;&#31181;&#20449;&#24687;&#28304;&#65292;&#20026;&#33647;&#29289;&#20877;&#23450;&#20301;&#30740;&#31350;&#25552;&#20379;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2309.03227</link><description>&lt;p&gt;
&#23398;&#20064;&#22522;&#20110;&#19987;&#21033;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#25581;&#31034;&#33647;&#29289;&#20877;&#23450;&#20301;&#20505;&#36873;&#29289;&#30340;&#25216;&#26415;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Learning a Patent-Informed Biomedical Knowledge Graph Reveals Technological Potential of Drug Repositioning Candidates. (arXiv:2309.03227v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33647;&#29289;&#19987;&#21033;&#21644;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#24211;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#35782;&#21035;&#20855;&#26377;&#25216;&#26415;&#28508;&#21147;&#21644;&#31185;&#23398;&#35777;&#25454;&#30340;&#33647;&#29289;&#20877;&#23450;&#20301;&#20505;&#36873;&#29289;&#12290;&#36890;&#36807;&#26500;&#24314;&#31185;&#23398;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#21644;&#22522;&#20110;&#19987;&#21033;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#65292;&#25105;&#20204;&#21487;&#20197;&#32508;&#21512;&#20998;&#26512;&#22810;&#31181;&#20449;&#24687;&#28304;&#65292;&#20026;&#33647;&#29289;&#20877;&#23450;&#20301;&#30740;&#31350;&#25552;&#20379;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#20877;&#23450;&#20301;&#26159;&#19968;&#31181;&#21457;&#29616;&#29616;&#26377;&#33647;&#29289;&#26032;&#27835;&#30103;&#29992;&#36884;&#30340;&#26377;&#21069;&#36884;&#30340;&#31574;&#30053;&#65292;&#36817;&#24180;&#26469;&#22312;&#35745;&#31639;&#31185;&#23398;&#25991;&#29486;&#20013;&#20351;&#29992;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#24211;&#36827;&#34892;&#20102;&#24191;&#27867;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#33647;&#29289;&#20877;&#23450;&#20301;&#20505;&#36873;&#29289;&#30340;&#25216;&#26415;&#28508;&#21147;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#32508;&#21512;&#20998;&#26512;&#33647;&#29289;&#19987;&#21033;&#21644;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#24211;&#31561;&#22810;&#31181;&#20449;&#24687;&#28304;&#65292;&#35782;&#21035;&#20855;&#26377;&#25216;&#26415;&#28508;&#21147;&#21644;&#31185;&#23398;&#35777;&#25454;&#30340;&#33647;&#29289;&#20877;&#23450;&#20301;&#20505;&#36873;&#29289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#31185;&#23398;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#65288;s-BKG&#65289;&#65292;&#21253;&#25324;&#26469;&#33258;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#24211;&#30340;&#33647;&#29289;&#12289;&#30142;&#30149;&#21644;&#22522;&#22240;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#35782;&#21035;&#22312;s-BKG&#20013;&#19982;&#30446;&#26631;&#30142;&#30149;&#20851;&#32852;&#26377;&#38480;&#20294;&#22312;&#31354;&#38388;&#19978;&#32039;&#23494;&#30456;&#37051;&#30340;&#33647;&#29289;&#20316;&#20026;&#28508;&#22312;&#30340;&#33647;&#29289;&#20505;&#36873;&#29289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;&#33647;&#29289;&#19987;&#21033;&#20449;&#24687;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#19987;&#21033;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#65288;p-BKG&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Drug repositioning-a promising strategy for discovering new therapeutic uses for existing drugs-has been increasingly explored in the computational science literature using biomedical databases. However, the technological potential of drug repositioning candidates has often been overlooked. This study presents a novel protocol to comprehensively analyse various sources such as pharmaceutical patents and biomedical databases, and identify drug repositioning candidates with both technological potential and scientific evidence. To this end, first, we constructed a scientific biomedical knowledge graph (s-BKG) comprising relationships between drugs, diseases, and genes derived from biomedical databases. Our protocol involves identifying drugs that exhibit limited association with the target disease but are closely located in the s-BKG, as potential drug candidates. We constructed a patent-informed biomedical knowledge graph (p-BKG) by adding pharmaceutical patent information. Finally, we d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#24352;&#37327;&#32593;&#32476;&#26550;&#26500;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#65292;&#24182;&#25552;&#20986;&#20102;&#30830;&#20445;&#40065;&#26834;&#24615;&#30340;&#26126;&#30830;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2202.12319</link><description>&lt;p&gt;
&#20445;&#25252;&#38544;&#31169;&#30340;&#24352;&#37327;&#32593;&#32476;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Privacy-preserving machine learning with tensor networks. (arXiv:2202.12319v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.12319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#24352;&#37327;&#32593;&#32476;&#26550;&#26500;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#65292;&#24182;&#25552;&#20986;&#20102;&#30830;&#20445;&#40065;&#26834;&#24615;&#30340;&#26126;&#30830;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#32593;&#32476;&#34987;&#24191;&#27867;&#29992;&#20110;&#25552;&#20379;&#20302;&#33021;&#37327;&#24577;&#30340;&#39640;&#25928;&#34920;&#31034;&#65292;&#26368;&#36817;&#34987;&#25552;&#20986;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#24352;&#37327;&#32593;&#32476;&#26550;&#26500;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#65292;&#36825;&#23545;&#20110;&#22788;&#29702;&#21307;&#30103;&#35760;&#24405;&#31561;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20013;&#23384;&#22312;&#30340;&#26032;&#38544;&#31169;&#28431;&#27934;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#20102;&#35828;&#26126;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30830;&#20445;&#23545;&#36825;&#31181;&#28431;&#27934;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#26126;&#30830;&#26465;&#20214;&#65292;&#36825;&#28041;&#21450;&#21040;&#22312;&#35268;&#33539;&#23545;&#31216;&#24615;&#19979;&#31561;&#20215;&#30340;&#27169;&#22411;&#30340;&#21051;&#30011;&#12290;&#25105;&#20204;&#20005;&#26684;&#35777;&#26126;&#20102;&#24352;&#37327;&#32593;&#32476;&#26550;&#26500;&#28385;&#36275;&#36825;&#20123;&#26465;&#20214;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#30697;&#38453;&#20056;&#31215;&#24577;&#30340;&#35268;&#33539;&#24418;&#24335;&#65292;&#20855;&#26377;&#39640;&#24230;&#30340;&#35268;&#24459;&#24615;&#24182;&#20462;&#27491;&#20102;&#27531;&#20313;&#35268;&#33539;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor networks, widely used for providing efficient representations of low-energy states of local quantum many-body systems, have been recently proposed as machine learning architectures which could present advantages with respect to traditional ones. In this work we show that tensor network architectures have especially prospective properties for privacy-preserving machine learning, which is important in tasks such as the processing of medical records. First, we describe a new privacy vulnerability that is present in feedforward neural networks, illustrating it in synthetic and real-world datasets. Then, we develop well-defined conditions to guarantee robustness to such vulnerability, which involve the characterization of models equivalent under gauge symmetry. We rigorously prove that such conditions are satisfied by tensor-network architectures. In doing so, we define a novel canonical form for matrix product states, which has a high degree of regularity and fixes the residual gaug
&lt;/p&gt;</description></item></channel></rss>