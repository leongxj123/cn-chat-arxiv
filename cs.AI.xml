<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#20102;MAGIS&#26694;&#26550;&#65292;&#22522;&#20110;LLM&#26500;&#24314;&#65292;&#21253;&#25324;&#22235;&#31181;&#23450;&#21046;&#30340;Agent&#65292;&#33021;&#22815;&#21327;&#20316;&#35268;&#21010;&#21644;&#32534;&#30721;&#36807;&#31243;&#65292;&#35299;&#38145;LLMs&#35299;&#20915;GitHub&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.17927</link><description>&lt;p&gt;
MAGIS&#65306;&#22522;&#20110;LLM&#30340;GitHub&#38382;&#39064;&#35299;&#20915;&#22810;Agent&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17927
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MAGIS&#26694;&#26550;&#65292;&#22522;&#20110;LLM&#26500;&#24314;&#65292;&#21253;&#25324;&#22235;&#31181;&#23450;&#21046;&#30340;Agent&#65292;&#33021;&#22815;&#21327;&#20316;&#35268;&#21010;&#21644;&#32534;&#30721;&#36807;&#31243;&#65292;&#35299;&#38145;LLMs&#35299;&#20915;GitHub&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36719;&#20214;&#28436;&#21270;&#20013;&#65292;&#35299;&#20915;GitHub&#23384;&#20648;&#24211;&#20013;&#30340;&#31361;&#21457;&#38382;&#39064;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#25361;&#25112;&#65292;&#19981;&#20165;&#28041;&#21450;&#26032;&#20195;&#30721;&#30340;&#25972;&#21512;&#65292;&#36824;&#21253;&#25324;&#23545;&#29616;&#26377;&#21151;&#33021;&#30340;&#32500;&#25252;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#20195;&#30721;&#29983;&#25104;&#21644;&#29702;&#35299;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#22312;&#20195;&#30721;&#26356;&#25913;&#26041;&#38754;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#20648;&#24211;&#32423;&#21035;&#19978;&#38754;&#20020;&#22256;&#38590;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#36890;&#36807;&#32463;&#39564;&#30740;&#31350;&#20102;LLMs&#22823;&#22810;&#26080;&#27861;&#35299;&#20915;GitHub&#38382;&#39064;&#30340;&#21407;&#22240;&#65292;&#24182;&#20998;&#26512;&#20102;&#19968;&#20123;&#24433;&#21709;&#22240;&#32032;&#12290;&#21463;&#32463;&#39564;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;LLM&#30340;GitHub Issue&#35299;&#20915;&#22810;Agent&#26694;&#26550;MAGIS&#65292;&#30001;&#22235;&#31181;&#38024;&#23545;&#36719;&#20214;&#28436;&#21270;&#23450;&#21046;&#30340;Agent&#32452;&#25104;&#65306;Manager&#12289;Repository Custodian&#12289;Developer&#21644;Quality Assurance Engineer&#12290;&#36825;&#20010;&#26694;&#26550;&#21033;&#29992;&#21508;&#31181;Agent&#22312;&#35268;&#21010;&#21644;&#32534;&#30721;&#36807;&#31243;&#20013;&#30340;&#21327;&#20316;&#65292;&#37322;&#25918;LLMs&#35299;&#20915;GitHub&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17927v1 Announce Type: cross  Abstract: In software evolution, resolving the emergent issues within GitHub repositories is a complex challenge that involves not only the incorporation of new code but also the maintenance of existing functionalities. Large Language Models (LLMs) have shown promise in code generation and understanding but face difficulties in code change, particularly at the repository level. To overcome these challenges, we empirically study the reason why LLMs mostly fail to resolve GitHub issues and analyze some impact factors. Motivated by the empirical findings, we propose a novel LLM-based Multi-Agent framework for GitHub Issue reSolution, MAGIS, consisting of four kinds of agents customized for the software evolution: Manager, Repository Custodian, Developer, and Quality Assurance Engineer agents. This framework leverages the collaboration of various agents in the planning and coding process to unlock the potential of LLMs to resolve GitHub issues. In e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#28145;&#24230;&#25903;&#25345;&#21521;&#37327;&#65288;DSVs&#65289;&#30340;&#27010;&#24565;&#65292;&#20171;&#32461;&#20102;DeepKKT&#26465;&#20214;&#65292;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;DSVs&#19982;SVM&#20013;&#30340;&#25903;&#25345;&#21521;&#37327;&#31867;&#20284;&#65292;&#20026;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#26631;&#20934;&#25552;&#20379;&#20102;&#26041;&#27861;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#21487;&#20197;&#26377;&#25928;&#22320;&#20351;&#29992;DSVs&#37325;&#26500;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.17329</link><description>&lt;p&gt;
&#28145;&#24230;&#25903;&#25345;&#21521;&#37327;
&lt;/p&gt;
&lt;p&gt;
Deep Support Vectors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17329
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#28145;&#24230;&#25903;&#25345;&#21521;&#37327;&#65288;DSVs&#65289;&#30340;&#27010;&#24565;&#65292;&#20171;&#32461;&#20102;DeepKKT&#26465;&#20214;&#65292;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;DSVs&#19982;SVM&#20013;&#30340;&#25903;&#25345;&#21521;&#37327;&#31867;&#20284;&#65292;&#20026;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#26631;&#20934;&#25552;&#20379;&#20102;&#26041;&#27861;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#21487;&#20197;&#26377;&#25928;&#22320;&#20351;&#29992;DSVs&#37325;&#26500;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#30340;&#25104;&#21151;&#36890;&#24120;&#34987;&#24402;&#22240;&#20110;&#20854;&#19982;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#22312;&#29702;&#35770;&#19978;&#30340;&#31561;&#20215;&#24615;&#65292;&#20294;&#36825;&#31181;&#20851;&#31995;&#30340;&#23454;&#38469;&#24433;&#21709;&#23578;&#26410;&#24471;&#21040;&#20840;&#38754;&#25506;&#35752;&#12290;&#26412;&#25991;&#22312;&#36825;&#19968;&#39046;&#22495;&#24320;&#23637;&#20102;&#19968;&#39033;&#25506;&#32034;&#65292;&#37325;&#28857;&#20851;&#27880;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#28145;&#24230;&#25903;&#25345;&#21521;&#37327;&#65288;DSVs&#65289;&#30340;&#35782;&#21035;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;DeepKKT&#26465;&#20214;&#30340;&#27010;&#24565;&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#20026;&#28145;&#24230;&#23398;&#20064;&#37327;&#36523;&#23450;&#21046;&#30340;&#20256;&#32479;Karush-Kuhn-Tucker&#65288;KKT&#65289;&#26465;&#20214;&#30340;&#35843;&#25972;&#29256;&#26412;&#12290;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;DSVs&#19982;SVM&#20013;&#30340;&#25903;&#25345;&#21521;&#37327;&#20043;&#38388;&#23384;&#22312;&#30456;&#20284;&#24615;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#26631;&#20934;&#30340;&#20999;&#23454;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20351;&#29992;DSVs&#37325;&#26500;&#27169;&#22411;&#65292;&#31867;&#20284;&#20110;SVM&#20013;&#30340;&#36807;&#31243;&#12290;&#20195;&#30721;&#23558;&#20250;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17329v1 Announce Type: cross  Abstract: While the success of deep learning is commonly attributed to its theoretical equivalence with Support Vector Machines (SVM), the practical implications of this relationship have not been thoroughly explored. This paper pioneers an exploration in this domain, specifically focusing on the identification of Deep Support Vectors (DSVs) within deep learning models. We introduce the concept of DeepKKT conditions, an adaptation of the traditional Karush-Kuhn-Tucker (KKT) conditions tailored for deep learning. Through empirical investigations, we illustrate that DSVs exhibit similarities to support vectors in SVM, offering a tangible method to interpret the decision-making criteria of models. Additionally, our findings demonstrate that models can be effectively reconstructed using DSVs, resembling the process in SVM. The code will be available.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#29289;&#29702;&#24341;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;nnU-Net&#30340;&#30417;&#30563;&#26041;&#27861;&#26469;&#20248;&#21270;&#24515;&#23460;&#21521;&#37327;&#27969;&#21160;&#26144;&#23556;&#65292;&#25928;&#26524;&#19982;&#20256;&#32479;&#31639;&#27861;&#30456;&#24403;&#65292;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.13040</link><description>&lt;p&gt;
&#29289;&#29702;&#24341;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#24515;&#23460;&#21521;&#37327;&#27969;&#21160;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Physics-Guided Neural Networks for Intraventricular Vector Flow Mapping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13040
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#29289;&#29702;&#24341;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;nnU-Net&#30340;&#30417;&#30563;&#26041;&#27861;&#26469;&#20248;&#21270;&#24515;&#23460;&#21521;&#37327;&#27969;&#21160;&#26144;&#23556;&#65292;&#25928;&#26524;&#19982;&#20256;&#32479;&#31639;&#27861;&#30456;&#24403;&#65292;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Intraventricular vector flow mapping (iVFM)&#26088;&#22312;&#22686;&#24378;&#21644;&#37327;&#21270;&#24515;&#33039;&#25104;&#20687;&#20013;&#30340;&#24425;&#33394;&#22810;&#26222;&#21202;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#21644;&#22522;&#20110;&#29289;&#29702;&#24341;&#23548;&#30340;nnU-Net&#30417;&#30563;&#26041;&#27861;&#26469;&#20248;&#21270;&#20256;&#32479;&#30340;iVFM&#20248;&#21270;&#26041;&#26696;&#12290;&#36890;&#36807;&#23545;&#22522;&#20110;&#24739;&#32773;&#29305;&#23450;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#27169;&#22411;&#20135;&#29983;&#30340;&#27169;&#25311;&#24425;&#33394;&#22810;&#26222;&#21202;&#22270;&#20687;&#21644;&#20307;&#20869;&#22810;&#26222;&#21202;&#37319;&#38598;&#30340;&#20005;&#26684;&#35780;&#20272;&#65292;&#20004;&#31181;&#26041;&#27861;&#22343;&#23637;&#29616;&#20986;&#19982;&#21407;&#22987;iVFM&#31639;&#27861;&#30456;&#24403;&#30340;&#37325;&#24314;&#24615;&#33021;&#12290; PINNs&#30340;&#25928;&#29575;&#36890;&#36807;&#21452;&#38454;&#27573;&#20248;&#21270;&#21644;&#39044;&#20248;&#21270;&#26435;&#37325;&#24471;&#21040;&#25552;&#21319;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;nnU-Net&#26041;&#27861;&#22312;&#27867;&#21270;&#33021;&#21147;&#21644;&#23454;&#26102;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;nnU-Net&#22312;&#31232;&#30095;&#21644;&#25130;&#26029;&#22810;&#26222;&#21202;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#29420;&#31435;&#20110;&#26126;&#30830;&#30340;&#36793;&#30028;&#26465;&#20214;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#20986;&#20102;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13040v1 Announce Type: cross  Abstract: Intraventricular vector flow mapping (iVFM) seeks to enhance and quantify color Doppler in cardiac imaging. In this study, we propose novel alternatives to the traditional iVFM optimization scheme by utilizing physics-informed neural networks (PINNs) and a physics-guided nnU-Net-based supervised approach. Through rigorous evaluation on simulated color Doppler images derived from a patient-specific computational fluid dynamics model and in vivo Doppler acquisitions, both approaches demonstrate comparable reconstruction performance to the original iVFM algorithm. The efficiency of PINNs is boosted through dual-stage optimization and pre-optimized weights. On the other hand, the nnU-Net method excels in generalizability and real time capabilities. Notably, nnU-Net shows superior robustness on sparse and truncated Doppler data while maintaining independence from explicit boundary conditions. Overall, our results highlight the effectiveness
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#25552;&#20986;&#20102;&#35782;&#21035;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25506;&#32034;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.05750</link><description>&lt;p&gt;
&#35299;&#35835;AI&#31508;: &#26816;&#27979;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#25216;&#26415;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05750
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#25552;&#20986;&#20102;&#35782;&#21035;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25506;&#32034;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36890;&#36807;&#23637;&#31034;&#29983;&#25104;&#31867;&#20154;&#25991;&#26412;&#30340;&#24778;&#20154;&#33021;&#21147;&#65292;&#24443;&#24213;&#39072;&#35206;&#20102;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;(NLG)&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24191;&#27867;&#30340;&#24212;&#29992;&#24102;&#26469;&#25361;&#25112;&#65292;&#38656;&#35201;&#28145;&#20837;&#23457;&#26597;&#12289;&#20262;&#29702;&#23457;&#26597;&#21644;&#36127;&#36131;&#20219;&#30340;&#23454;&#36341;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#25506;&#32034;&#20102;&#29616;&#26377;&#30340;&#32531;&#35299;&#31574;&#30053;&#65292;&#37325;&#28857;&#26159;&#35782;&#21035;AI&#29983;&#25104;&#25991;&#26412;&#20316;&#20026;&#26368;&#32456;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#35282;&#24230;&#35780;&#20272;&#20102;&#26816;&#27979;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#24403;&#21069;&#39046;&#22495;&#38480;&#21046;&#30340;&#26032;&#39062;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05750v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have revolutionized the field of Natural Language Generation (NLG) by demonstrating an impressive ability to generate human-like text. However, their widespread usage introduces challenges that necessitate thoughtful examination, ethical scrutiny, and responsible practices. In this study, we delve into these challenges, explore existing strategies for mitigating them, with a particular emphasis on identifying AI-generated text as the ultimate solution. Additionally, we assess the feasibility of detection from a theoretical perspective and propose novel research directions to address the current limitations in this domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#20154;&#24037;&#26234;&#33021;&#21512;&#20316;&#30340;&#37325;&#35201;&#24615;&#65292;&#24378;&#35843;&#20102;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#22686;&#24378;&#21327;&#20316;&#26234;&#33021;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#22686;&#24378;&#20154;&#31867;&#33021;&#21147;&#12289;&#25913;&#21892;AI&#27169;&#22411;&#12289;&#26377;&#25928;&#22242;&#38431;&#21512;&#20316;&#12289;&#36947;&#24503;&#32771;&#34385;&#20197;&#21450;&#22312;&#21508;&#20010;&#39046;&#22495;&#24191;&#27867;&#24212;&#29992;&#26041;&#38754;&#30340;&#28508;&#22312;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.04931</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#19982;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#21512;&#20316;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Human-AI Teaming with Large Pre-Trained Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#20154;&#24037;&#26234;&#33021;&#21512;&#20316;&#30340;&#37325;&#35201;&#24615;&#65292;&#24378;&#35843;&#20102;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#22686;&#24378;&#21327;&#20316;&#26234;&#33021;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#22686;&#24378;&#20154;&#31867;&#33021;&#21147;&#12289;&#25913;&#21892;AI&#27169;&#22411;&#12289;&#26377;&#25928;&#22242;&#38431;&#21512;&#20316;&#12289;&#36947;&#24503;&#32771;&#34385;&#20197;&#21450;&#22312;&#21508;&#20010;&#39046;&#22495;&#24191;&#27867;&#24212;&#29992;&#26041;&#38754;&#30340;&#28508;&#22312;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#36805;&#36895;&#21457;&#23637;&#30340;&#26223;&#35266;&#20013;&#65292;&#20154;&#31867;&#26234;&#33021;&#21644;AI&#31995;&#32479;&#20043;&#38388;&#30340;&#21327;&#20316;&#65292;&#21363;&#20154;&#24037;&#26234;&#33021;&#65288;HAI&#65289;&#21512;&#20316;&#65292;&#24050;&#25104;&#20026;&#25512;&#36827;&#38382;&#39064;&#35299;&#20915;&#21644;&#20915;&#31574;&#36807;&#31243;&#30340;&#22522;&#30707;&#12290;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;LPtM&#65289;&#30340;&#20986;&#29616;&#26174;&#33879;&#25913;&#21464;&#20102;&#36825;&#19968;&#26223;&#35266;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#37327;&#25968;&#25454;&#26469;&#29702;&#35299;&#21644;&#39044;&#27979;&#22797;&#26434;&#27169;&#24335;&#65292;&#20026;&#20154;&#31867;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;LPtMs&#19982;HAI&#30340;&#20851;&#38190;&#25972;&#21512;&#65292;&#24378;&#35843;&#20102;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#22686;&#24378;&#21327;&#20316;&#26234;&#33021;&#12290;&#37325;&#28857;&#25506;&#35752;&#20102;LPtMs&#22312;&#22686;&#24378;&#20154;&#31867;&#33021;&#21147;&#26041;&#38754;&#30340;&#21327;&#21516;&#28508;&#21147;&#65292;&#35752;&#35770;&#20102;&#36825;&#31181;&#21327;&#20316;&#23545;AI&#27169;&#22411;&#25913;&#36827;&#12289;&#26377;&#25928;&#30340;&#22242;&#38431;&#21512;&#20316;&#12289;&#36947;&#24503;&#32771;&#34385;&#20197;&#21450;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#24433;&#21709;&#12290;&#36890;&#36807;&#36825;&#19968;&#25506;&#32034;&#65292;&#30740;&#31350;&#25581;&#31034;&#20102;LPtM&#22686;&#24378;HAI&#30340;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04931v1 Announce Type: new  Abstract: In the rapidly evolving landscape of artificial intelligence (AI), the collaboration between human intelligence and AI systems, known as Human-AI (HAI) Teaming, has emerged as a cornerstone for advancing problem-solving and decision-making processes. The advent of Large Pre-trained Models (LPtM) has significantly transformed this landscape, offering unprecedented capabilities by leveraging vast amounts of data to understand and predict complex patterns. This paper surveys the pivotal integration of LPtMs with HAI, emphasizing how these models enhance collaborative intelligence beyond traditional approaches. It examines the synergistic potential of LPtMs in augmenting human capabilities, discussing this collaboration for AI model improvements, effective teaming, ethical considerations, and their broad applied implications in various sectors. Through this exploration, the study sheds light on the transformative impact of LPtM-enhanced HAI 
&lt;/p&gt;</description></item><item><title>MobileLLM&#36890;&#36807;&#20248;&#21270;&#27169;&#22411;&#26550;&#26500;&#65292;&#37319;&#29992;&#28145;&#24230;&#21644;&#30246;&#36523;&#32467;&#26500;&#12289;&#23884;&#20837;&#20849;&#20139;&#21644;&#20998;&#32452;&#26597;&#35810;&#27880;&#24847;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;2.7%/4.3%&#30340;&#20934;&#30830;&#29575;&#25552;&#21319;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#19988;&#20165;&#26377;&#26497;&#23567;&#24310;&#36831;&#24320;&#38144;&#30340;&#22359;&#29366;&#26435;&#37325;&#20849;&#20139;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.14905</link><description>&lt;p&gt;
MobileLLM&#65306;&#20248;&#21270;&#20122;&#21313;&#20159;&#21442;&#25968;&#35821;&#35328;&#27169;&#22411;&#20197;&#29992;&#20110;&#35774;&#22791;&#31471;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14905
&lt;/p&gt;
&lt;p&gt;
MobileLLM&#36890;&#36807;&#20248;&#21270;&#27169;&#22411;&#26550;&#26500;&#65292;&#37319;&#29992;&#28145;&#24230;&#21644;&#30246;&#36523;&#32467;&#26500;&#12289;&#23884;&#20837;&#20849;&#20139;&#21644;&#20998;&#32452;&#26597;&#35810;&#27880;&#24847;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;2.7%/4.3%&#30340;&#20934;&#30830;&#29575;&#25552;&#21319;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#19988;&#20165;&#26377;&#26497;&#23567;&#24310;&#36831;&#24320;&#38144;&#30340;&#22359;&#29366;&#26435;&#37325;&#20849;&#20139;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#31227;&#21160;&#35774;&#22791;&#19978;&#39640;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36843;&#20999;&#38656;&#27714;&#38382;&#39064;&#65292;&#36825;&#26159;&#30001;&#20110;&#20113;&#25104;&#26412;&#21644;&#24310;&#36831;&#38382;&#39064;&#19981;&#26029;&#22686;&#21152;&#25152;&#23548;&#33268;&#30340;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#35774;&#35745;&#20855;&#26377;&#19981;&#21040;&#21313;&#20159;&#21442;&#25968;&#30340;&#39030;&#32423;LLMs&#65292;&#36825;&#26159;&#31227;&#21160;&#37096;&#32626;&#30340;&#23454;&#38469;&#36873;&#25321;&#12290;&#19982;&#26222;&#36941;&#30340;&#35266;&#28857;&#30456;&#21453;&#65292;&#24378;&#35843;&#25968;&#25454;&#21644;&#21442;&#25968;&#25968;&#37327;&#22312;&#30830;&#23450;&#27169;&#22411;&#36136;&#37327;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#20122;&#21313;&#20159;&#35268;&#27169;LLMs&#30340;&#27169;&#22411;&#26550;&#26500;&#30340;&#37325;&#35201;&#24615;&#12290;&#21033;&#29992;&#28145;&#24230;&#21644;&#30246;&#36523;&#32467;&#26500;&#65292;&#20877;&#21152;&#19978;&#23884;&#20837;&#20849;&#20139;&#21644;&#20998;&#32452;&#26597;&#35810;&#27880;&#24847;&#26426;&#21046;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20934;&#32593;&#32476;&#65292;&#31216;&#20026;MobileLLM&#65292;&#20854;&#22312;&#23558;&#36817;125M/350M&#20808;&#36827;&#27169;&#22411;&#19978;&#20998;&#21035;&#33719;&#24471;&#20102;&#24778;&#20154;&#30340;2.7%/4.3%&#30340;&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31435;&#21363;&#30340;&#22359;&#29366;&#26435;&#37325;&#20849;&#20139;&#26041;&#27861;&#65292;&#19981;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#65292;&#19988;&#20165;&#20855;&#26377;&#26497;&#23567;&#30340;&#24310;&#36831;&#24320;&#38144;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#27169;&#22411;&#34987;&#21629;&#21517;&#20026;MobileLLM-L
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14905v1 Announce Type: cross  Abstract: This paper addresses the growing need for efficient large language models (LLMs) on mobile devices, driven by increasing cloud costs and latency concerns. We focus on designing top-quality LLMs with fewer than a billion parameters, a practical choice for mobile deployment. Contrary to prevailing belief emphasizing the pivotal role of data and parameter quantity in determining model quality, our investigation underscores the significance of model architecture for sub-billion scale LLMs. Leveraging deep and thin architectures, coupled with embedding sharing and grouped-query attention mechanisms, we establish a strong baseline network denoted as MobileLLM, which attains a remarkable 2.7%/4.3% accuracy boost over preceding 125M/350M state-of-the-art models. Additionally, we propose an immediate block-wise weight sharing approach with no increase in model size and only marginal latency overhead. The resultant models, denoted as MobileLLM-L
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35748;&#20026;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#31649;&#29702;&#24335;&#30417;&#31649;&#26041;&#27861;&#20013;&#65292;&#21152;&#24378;&#20154;&#31867;&#24341;&#23548;&#21644;&#22521;&#35757;&#25216;&#26415;&#30340;&#30740;&#31350;&#21644;&#23454;&#36341;&#23545;&#20110;&#25552;&#39640;AI&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#25216;&#26415;&#21644;&#20262;&#29702;&#38382;&#39064;&#31561;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.08466</link><description>&lt;p&gt;
&#35748;&#30495;&#23545;&#24453;&#22521;&#35757;&#65306;&#20154;&#24037;&#26234;&#33021;&#30340;&#20154;&#31867;&#24341;&#23548;&#19982;&#22522;&#20110;&#31649;&#29702;&#30340;&#30417;&#31649;
&lt;/p&gt;
&lt;p&gt;
Taking Training Seriously: Human Guidance and Management-Based Regulation of Artificial Intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08466
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35748;&#20026;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#31649;&#29702;&#24335;&#30417;&#31649;&#26041;&#27861;&#20013;&#65292;&#21152;&#24378;&#20154;&#31867;&#24341;&#23548;&#21644;&#22521;&#35757;&#25216;&#26415;&#30340;&#30740;&#31350;&#21644;&#23454;&#36341;&#23545;&#20110;&#25552;&#39640;AI&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#25216;&#26415;&#21644;&#20262;&#29702;&#38382;&#39064;&#31561;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30456;&#20851;&#21361;&#23475;&#26356;&#24378;&#22823;&#30340;&#27835;&#29702;&#30340;&#28909;&#24773;&#21628;&#22768;&#27491;&#22312;&#19990;&#30028;&#33539;&#22260;&#20869;&#24341;&#36215;&#31649;&#29702;&#23398;&#32773;&#25152;&#31216;&#30340;&#22522;&#20110;&#31649;&#29702;&#30340;&#30417;&#31649;&#26041;&#27861;&#30340;&#37319;&#29992;&#12290;&#32654;&#22269;&#21644;&#27431;&#27954;&#30340;&#26368;&#26032;&#20513;&#35758;&#20197;&#21450;&#22269;&#38469;&#26631;&#20934;&#21270;&#32452;&#32455;&#37319;&#32435;&#30340;&#37325;&#35201;&#33258;&#25105;&#30417;&#31649;&#26631;&#20934;&#37117;&#20849;&#21516;&#20855;&#26377;&#19968;&#20010;&#26680;&#24515;&#30340;&#22522;&#20110;&#31649;&#29702;&#30340;&#33539;&#24335;&#12290;&#36825;&#20123;&#22522;&#20110;&#31649;&#29702;&#30340;&#20513;&#35758;&#26088;&#22312;&#36890;&#36807;&#22686;&#21152;&#20154;&#31867;&#23545;AI&#24037;&#20855;&#30340;&#22521;&#35757;&#21644;&#24320;&#21457;&#30340;&#30417;&#30563;&#26469;&#28608;&#21169;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#20010;&#26032;&#20852;&#30340;&#22522;&#20110;&#31649;&#29702;&#30340;&#30417;&#31649;&#33539;&#24335;&#26102;&#20195;&#20013;&#65292;&#38656;&#35201;&#23545;&#20154;&#31867;&#24341;&#23548;&#22521;&#35757;&#25216;&#26415;&#36827;&#34892;&#23436;&#21892;&#21644;&#31995;&#32479;&#21270;&#12290;&#22914;&#26524;&#35748;&#30495;&#23545;&#24453;&#65292;&#20154;&#31867;&#24341;&#23548;&#22521;&#35757;&#21487;&#20197;&#20943;&#36731;&#19968;&#20123;&#23545;AI&#30340;&#25216;&#26415;&#21644;&#20262;&#29702;&#21387;&#21147;&#65292;&#20197;&#20154;&#31867;&#30452;&#35273;&#25552;&#39640;AI&#30340;&#24615;&#33021;&#65292;&#24182;&#26356;&#22909;&#22320;&#28385;&#36275;&#23545;&#20844;&#24179;&#24615;&#21644;&#26377;&#25928;&#35299;&#37322;&#30340;&#38656;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fervent calls for more robust governance of the harms associated with artificial intelligence (AI) are leading to the adoption around the world of what regulatory scholars have called a management-based approach to regulation. Recent initiatives in the United States and Europe, as well as the adoption of major self-regulatory standards by the International Organization for Standardization, share in common a core management-based paradigm. These management-based initiatives seek to motivate an increase in human oversight of how AI tools are trained and developed. Refinements and systematization of human-guided training techniques will thus be needed to fit within this emerging era of management-based regulatory paradigm. If taken seriously, human-guided training can alleviate some of the technical and ethical pressures on AI, boosting AI performance with human intuition as well as better addressing the needs for fairness and effective explainability. In this paper, we discuss the connec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#20102;&#33258;&#21161;&#24341;&#23548;&#33258;&#23545;&#40784;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20854;&#26126;&#26174;&#20248;&#20110;&#21333;&#27425;&#24490;&#29615;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#25968;&#25454;&#35757;&#32451;&#39034;&#24207;&#36827;&#19968;&#27493;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07610</link><description>&lt;p&gt;
&#36393;&#33050;&#35843;&#26657;&#65306;&#36890;&#36807;&#33258;&#21161;&#24341;&#23548;&#25193;&#23637;LLM&#30340;&#33258;&#23545;&#40784;&#33021;&#21147;&#30340;&#35268;&#27169;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#20102;&#33258;&#21161;&#24341;&#23548;&#33258;&#23545;&#40784;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20854;&#26126;&#26174;&#20248;&#20110;&#21333;&#27425;&#24490;&#29615;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#25968;&#25454;&#35757;&#32451;&#39034;&#24207;&#36827;&#19968;&#27493;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#23545;&#40784;&#26159;&#19968;&#31181;&#38477;&#20302;&#20154;&#24037;&#27880;&#37322;&#25104;&#26412;&#24182;&#30830;&#20445;&#27169;&#22411;&#33021;&#21147;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#26041;&#27861;&#22312;&#21333;&#27425;&#24490;&#29615;&#20013;&#23436;&#25104;&#25968;&#25454;&#25910;&#38598;&#21644;&#35757;&#32451;&#27493;&#39588;&#65292;&#21487;&#33021;&#24573;&#35270;&#20102;&#33258;&#23545;&#40784;&#27169;&#22411;&#19981;&#26029;&#25913;&#36827;&#30340;&#33021;&#21147;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#22914;&#26524;&#25105;&#20204;&#36827;&#34892;&#22810;&#27425;&#33258;&#21161;&#24341;&#23548;&#33258;&#23545;&#40784;&#65292;&#20250;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#36824;&#26159;&#23548;&#33268;&#24555;&#36895;&#36864;&#21270;&#65311;&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#20102;&#33258;&#21161;&#24341;&#23548;&#33258;&#23545;&#40784;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#20445;&#35777;&#20174;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#33719;&#24471;&#30340;&#25968;&#25454;&#22810;&#26679;&#24615;&#65292;&#33258;&#21161;&#24341;&#23548;&#33258;&#23545;&#40784;&#26126;&#26174;&#20248;&#20110;&#21333;&#27425;&#24490;&#29615;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#21457;&#25381;&#33258;&#21161;&#24341;&#23548;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#24182;&#35843;&#25972;&#20102;&#25968;&#25454;&#30340;&#35757;&#32451;&#39034;&#24207;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36393;&#33050;&#35843;&#26657;&#65288;SOFT&#65289;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#27169;&#22411;&#30340;&#25345;&#32493;&#22686;&#24378;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-alignment is an effective way to reduce the cost of human annotation while ensuring promising model capability. However, most current methods complete the data collection and training steps in a single round, which may overlook the continuously improving ability of self-aligned models. This gives rise to a key query: What if we do multi-time bootstrapping self-alignment? Does this strategy enhance model performance or lead to rapid degradation? In this paper, our pioneering exploration delves into the impact of bootstrapping self-alignment on large language models. Our findings reveal that bootstrapping self-alignment markedly surpasses the single-round approach, by guaranteeing data diversity from in-context learning. To further exploit the capabilities of bootstrapping, we investigate and adjust the training order of data, which yields improved performance of the model. Drawing on these findings, we propose Step-On-Feet Tuning (SOFT) which leverages model's continuously enhanced
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#36229;&#22768;&#24515;&#21160;&#22270;&#30340;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#22797;&#21512;&#26680;&#31574;&#30053;&#30340;&#21333;&#19968;&#31867;&#21035;&#20998;&#31867;&#31639;&#27861;&#26469;&#36827;&#34892;&#26089;&#26399;&#24515;&#32908;&#26775;&#27515;&#30340;&#26816;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#25237;&#24433;&#30697;&#38453;&#21644;&#29305;&#24449;&#36716;&#21270;&#65292;&#25552;&#39640;&#20102;&#24515;&#32908;&#26775;&#27515;&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.06530</link><description>&lt;p&gt;
&#25913;&#36827;&#24515;&#32908;&#26775;&#27515;&#26816;&#27979;&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22797;&#21512;&#26680;&#31574;&#30053;&#22312;&#21333;&#19968;&#31867;&#21035;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Refining Myocardial Infarction Detection: A Novel Multi-Modal Composite Kernel Strategy in One-Class Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#36229;&#22768;&#24515;&#21160;&#22270;&#30340;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#22797;&#21512;&#26680;&#31574;&#30053;&#30340;&#21333;&#19968;&#31867;&#21035;&#20998;&#31867;&#31639;&#27861;&#26469;&#36827;&#34892;&#26089;&#26399;&#24515;&#32908;&#26775;&#27515;&#30340;&#26816;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#25237;&#24433;&#30697;&#38453;&#21644;&#29305;&#24449;&#36716;&#21270;&#65292;&#25552;&#39640;&#20102;&#24515;&#32908;&#26775;&#27515;&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#24515;&#32908;&#26775;&#27515;&#65288;MI&#65289;&#30340;&#26816;&#27979;&#23545;&#20110;&#39044;&#38450;&#36827;&#19968;&#27493;&#24515;&#32908;&#25439;&#20260;&#38750;&#24120;&#37325;&#35201;&#65292;MI&#26159;&#30001;&#20896;&#29366;&#21160;&#33033;&#30142;&#30149;&#65288;CAD&#65289;&#24341;&#36215;&#30340;&#19968;&#31181;&#20005;&#37325;&#30142;&#30149;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#31181;&#22522;&#20110;&#36229;&#22768;&#24515;&#21160;&#22270;&#30340;&#21333;&#19968;&#31867;&#21035;&#20998;&#31867;&#65288;OCC&#65289;&#31639;&#27861;&#36827;&#34892;&#26089;&#26399;MI&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#22810;&#27169;&#24577;&#23376;&#31354;&#38388;&#25903;&#25345;&#21521;&#37327;&#25968;&#25454;&#25551;&#36848;&#30340;&#26032;&#26041;&#27861;&#20811;&#26381;&#20102;&#36229;&#22768;&#24515;&#21160;&#22270;&#25968;&#25454;&#26377;&#38480;&#30340;&#25361;&#25112;&#12290;&#25552;&#20986;&#30340;&#25216;&#26415;&#28041;&#21450;&#19968;&#31181;&#29305;&#27530;&#30340;MI&#26816;&#27979;&#26694;&#26550;&#65292;&#20351;&#29992;&#22797;&#21512;&#26680;&#22312;&#38750;&#32447;&#24615;&#25237;&#24433;&#25216;&#24039;&#20013;&#34701;&#21512;&#39640;&#26031;&#21644;&#25289;&#26222;&#25289;&#26031;sigmoid&#20989;&#25968;&#65292;&#23558;&#22810;&#35270;&#22270;&#36229;&#22768;&#24515;&#21160;&#22270;&#32467;&#21512;&#36215;&#26469;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#35843;&#25972;&#25237;&#24433;&#30697;&#38453;&#30340;&#26368;&#22823;&#21270;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#25237;&#24433;&#30697;&#38453;&#26356;&#26032;&#31574;&#30053;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23558;&#20174;&#36229;&#22768;&#24515;&#21160;&#22270;&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#26377;&#25928;&#22320;&#36716;&#21270;&#20026;&#20248;&#21270;&#30340;&#20302;&#32500;&#31354;&#38388;&#65292;&#22686;&#24378;&#20102;MI&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early detection of myocardial infarction (MI), a critical condition arising from coronary artery disease (CAD), is vital to prevent further myocardial damage. This study introduces a novel method for early MI detection using a one-class classification (OCC) algorithm in echocardiography. Our study overcomes the challenge of limited echocardiography data availability by adopting a novel approach based on Multi-modal Subspace Support Vector Data Description. The proposed technique involves a specialized MI detection framework employing multi-view echocardiography incorporating a composite kernel in the non-linear projection trick, fusing Gaussian and Laplacian sigmoid functions. Additionally, we enhance the update strategy of the projection matrices by adapting maximization for both or one of the modalities in the optimization process. Our method boosts MI detection capability by efficiently transforming features extracted from echocardiography data into an optimized lower-dimensional su
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20462;&#21098;&#21644;&#20302;&#31209;&#20462;&#25913;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23433;&#20840;&#26426;&#21046;&#22266;&#26377;&#26131;&#30862;&#24615;&#65292;&#21435;&#38500;&#23433;&#20840;&#20851;&#38190;&#21306;&#22495;&#20250;&#25439;&#23475;&#23433;&#20840;&#24615;&#65292;&#20294;&#23545;&#25928;&#29992;&#24433;&#21709;&#19981;&#22823;&#65292;&#38656;&#35201;&#26356;&#24378;&#20581;&#30340;&#23433;&#20840;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.05162</link><description>&lt;p&gt;
&#36890;&#36807;&#20462;&#21098;&#21644;&#20302;&#31209;&#20462;&#25913;&#35780;&#20272;&#23433;&#20840;&#23545;&#40784;&#30340;&#26131;&#30862;&#24615;
&lt;/p&gt;
&lt;p&gt;
Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20462;&#21098;&#21644;&#20302;&#31209;&#20462;&#25913;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23433;&#20840;&#26426;&#21046;&#22266;&#26377;&#26131;&#30862;&#24615;&#65292;&#21435;&#38500;&#23433;&#20840;&#20851;&#38190;&#21306;&#22495;&#20250;&#25439;&#23475;&#23433;&#20840;&#24615;&#65292;&#20294;&#23545;&#25928;&#29992;&#24433;&#21709;&#19981;&#22823;&#65292;&#38656;&#35201;&#26356;&#24378;&#20581;&#30340;&#23433;&#20840;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20854;&#23433;&#20840;&#26426;&#21046;&#26041;&#38754;&#34920;&#29616;&#20986;&#22266;&#26377;&#30340;&#26131;&#30862;&#24615;&#65292;&#36825;&#21487;&#20174;&#23427;&#20204;&#26131;&#21463;&#36234;&#29425;&#21644;&#21363;&#20351;&#26159;&#38750;&#24694;&#24847;&#24494;&#35843;&#20063;&#26131;&#21463;&#24433;&#21709;&#26469;&#35828;&#26126;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#20462;&#21098;&#21644;&#20302;&#31209;&#20462;&#25913;&#25506;&#35752;&#20102;&#23433;&#20840;&#23545;&#40784;&#30340;&#26131;&#30862;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#26041;&#27861;&#65292;&#33021;&#22815;&#35782;&#21035;&#23545;&#20110;&#23433;&#20840;&#38450;&#25252;&#33267;&#20851;&#37325;&#35201;&#65292;&#19988;&#22312;&#31070;&#32463;&#20803;&#21644;&#31209;&#32423;&#21035;&#19978;&#19982;&#25928;&#29992;&#30456;&#20851;&#30340;&#21306;&#22495;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#30340;&#23396;&#31435;&#21306;&#22495;&#26159;&#31232;&#30095;&#30340;&#65292;&#32422;&#21344;&#21442;&#25968;&#32423;&#21035;&#30340;$3\%$&#21644;&#25490;&#21517;&#32423;&#21035;&#30340;$2.5\%$&#12290;&#21435;&#38500;&#36825;&#20123;&#21306;&#22495;&#20250;&#25439;&#23475;&#23433;&#20840;&#24615;&#65292;&#32780;&#23545;&#25928;&#29992;&#30340;&#24433;&#21709;&#19981;&#22823;&#65292;&#20174;&#32780;&#35777;&#23454;&#20102;&#35813;&#27169;&#22411;&#23433;&#20840;&#26426;&#21046;&#30340;&#22266;&#26377;&#26131;&#30862;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#21363;&#20351;&#38480;&#21046;&#23545;&#23433;&#20840;&#20851;&#38190;&#21306;&#22495;&#36827;&#34892;&#20462;&#25913;&#65292;LLMs&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#20302;&#25104;&#26412;&#30340;&#24494;&#35843;&#25915;&#20987;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;LLMs&#20013;&#26356;&#24378;&#22823;&#30340;&#23433;&#20840;&#31574;&#30053;&#30340;&#32039;&#36843;&#24615;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) show inherent brittleness in their safety mechanisms, as evidenced by their susceptibility to jailbreaking and even non-malicious fine-tuning. This study explores this brittleness of safety alignment by leveraging pruning and low-rank modifications. We develop methods to identify critical regions that are vital for safety guardrails, and that are disentangled from utility-relevant regions at both the neuron and rank levels. Surprisingly, the isolated regions we find are sparse, comprising about $3\%$ at the parameter level and $2.5\%$ at the rank level. Removing these regions compromises safety without significantly impacting utility, corroborating the inherent brittleness of the model's safety mechanisms. Moreover, we show that LLMs remain vulnerable to low-cost fine-tuning attacks even when modifications to the safety-critical regions are restricted. These findings underscore the urgent need for more robust safety strategies in LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#39046;&#22495;&#29305;&#23450;&#22270;&#20687;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.04929</link><description>&lt;p&gt;
&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#25193;&#25955;&#24341;&#23548;&#28304;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Source-Free Domain Adaptation with Diffusion-Guided Source Data Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#39046;&#22495;&#29305;&#23450;&#22270;&#20687;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#65288;DM-SFDA&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;DM-SFDA&#26041;&#27861;&#21253;&#25324;&#23545;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20351;&#29992;&#30446;&#26631;&#22270;&#20687;&#30340;&#29305;&#24449;&#26469;&#25351;&#23548;&#25193;&#25955;&#36807;&#31243;&#29983;&#25104;&#28304;&#22495;&#22270;&#20687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#34987;&#24494;&#35843;&#20197;&#29983;&#25104;&#26368;&#23567;&#21270;&#29109;&#24182;&#26368;&#22823;&#21270;&#39044;&#35757;&#32451;&#28304;&#27169;&#22411;&#32622;&#20449;&#24230;&#30340;&#28304;&#26679;&#26412;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#24050;&#24314;&#31435;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#23558;&#29983;&#25104;&#30340;&#28304;&#22270;&#20687;&#19982;&#30446;&#26631;&#22495;&#25968;&#25454;&#36827;&#34892;&#23545;&#40784;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#19968;&#31995;&#21015;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;Office-31&#12289;Office-Home&#21644;VisDA&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#23637;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#12289;&#39046;&#22495;&#29305;&#23450;&#30340;&#22270;&#20687;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel approach to leverage the generalizability capability of Diffusion Models for Source-Free Domain Adaptation (DM-SFDA). Our proposed DM-SFDA method involves fine-tuning a pre-trained text-to-image diffusion model to generate source domain images using features from the target images to guide the diffusion process. Specifically, the pre-trained diffusion model is fine-tuned to generate source samples that minimize entropy and maximize confidence for the pre-trained source model. We then apply established unsupervised domain adaptation techniques to align the generated source images with target domain data. We validate our approach through comprehensive experiments across a range of datasets, including Office-31, Office-Home, and VisDA. The results highlight significant improvements in SFDA performance, showcasing the potential of diffusion models in generating contextually relevant, domain-specific images.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#22810;&#38754;&#26495;&#35270;&#35273;&#38382;&#31572;&#65288;MultipanelVQA&#65289;&#22522;&#20934;&#25361;&#25112;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#23545;&#29702;&#35299;&#22810;&#38754;&#26495;&#22270;&#20687;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;LVLMs&#22312;&#36825;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#26174;&#33879;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.15847</link><description>&lt;p&gt;
&#26494;&#39292;&#36824;&#26159;&#21513;&#23043;&#23043;&#65311;&#29992;&#22810;&#38754;&#26495;VQA&#25361;&#25112;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Muffin or Chihuahua? Challenging Large Vision-Language Models with Multipanel VQA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15847
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#22810;&#38754;&#26495;&#35270;&#35273;&#38382;&#31572;&#65288;MultipanelVQA&#65289;&#22522;&#20934;&#25361;&#25112;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#23545;&#29702;&#35299;&#22810;&#38754;&#26495;&#22270;&#20687;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;LVLMs&#22312;&#36825;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#26174;&#33879;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#38754;&#26495;&#22270;&#20687;&#65292;&#36890;&#24120;&#22312;&#32593;&#39029;&#25130;&#22270;&#12289;&#28023;&#25253;&#31561;&#20013;&#30475;&#21040;&#65292;&#20805;&#26021;&#30528;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#12290;&#36825;&#20123;&#22270;&#20687;&#20197;&#22810;&#20010;&#23376;&#22270;&#20197;&#19981;&#21516;&#24067;&#23616;&#32452;&#25104;&#65292;&#26377;&#25928;&#22320;&#21521;&#20154;&#20204;&#20256;&#36798;&#20449;&#24687;&#12290;&#20026;&#20102;&#26500;&#24314;&#39640;&#32423;&#30340;&#22810;&#27169;&#24577;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#65292;&#22914;&#33021;&#29702;&#35299;&#22797;&#26434;&#22330;&#26223;&#24182;&#22312;&#32593;&#39029;&#20013;&#23548;&#33322;&#30340;&#20195;&#29702;&#31243;&#24207;&#65292;&#22810;&#38754;&#26495;&#35270;&#35273;&#25512;&#29702;&#30340;&#25216;&#33021;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#23545;&#27169;&#22411;&#22312;&#36825;&#26041;&#38754;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#38754;&#26495;&#35270;&#35273;&#38382;&#31572;&#65288;MultipanelVQA&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;6,600&#20010;&#38382;&#39064;&#12289;&#31572;&#26696;&#21644;&#22810;&#38754;&#26495;&#22270;&#20687;&#19977;&#20803;&#32452;&#65292;&#19987;&#38376;&#25361;&#25112;&#27169;&#22411;&#29702;&#35299;&#22810;&#38754;&#26495;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;MultipanelVQA&#22522;&#20934;&#20013;&#30340;&#38382;&#39064;&#23545;&#27979;&#35797;&#30340;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#21363;&#20351;&#20154;&#31867;&#21487;&#20197;&#33719;&#24471;&#32422;99%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.15847v2 Announce Type: replace-cross  Abstract: Multipanel images, commonly seen as web screenshots, posters, etc., pervade our daily lives. These images, characterized by their composition of multiple subfigures in distinct layouts, effectively convey information to people. Toward building advanced multimodal AI applications, such as agents that understand complex scenes and navigate through webpages, the skill of multipanel visual reasoning is essential, and a comprehensive evaluation of models in this regard is important. Therefore, we introduce Multipanel Visual Question Answering (MultipanelVQA), a novel benchmark comprising 6,600 triplets of questions, answers, and multipanel images that specifically challenge models in comprehending multipanel images. Our evaluation shows that questions in the MultipanelVQA benchmark pose significant challenges to the state-of-the-art Large Vision Language Models (LVLMs) tested, even though humans can attain approximately 99\% accurac
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23454;&#20363;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#21315;&#20159;&#20687;&#32032;&#20840;&#20999;&#29255;&#22270;&#20687;&#30340;&#30149;&#29702;&#25253;&#21578;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#21253;&#21547;&#22810;&#20010;&#20020;&#24202;&#32447;&#32034;&#30340;&#30149;&#29702;&#25253;&#21578;&#12290;</title><link>https://arxiv.org/abs/2311.16480</link><description>&lt;p&gt;
&#30149;&#29702;&#25253;&#21578;&#30340;&#22810;&#23454;&#20363;&#29983;&#25104;&#29992;&#20110;&#21315;&#20159;&#20687;&#32032;&#20840;&#20999;&#29255;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
WsiCaption: Multiple Instance Generation of Pathology Reports for Gigapixel Whole-Slide Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16480
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23454;&#20363;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#21315;&#20159;&#20687;&#32032;&#20840;&#20999;&#29255;&#22270;&#20687;&#30340;&#30149;&#29702;&#25253;&#21578;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#21253;&#21547;&#22810;&#20010;&#20020;&#24202;&#32447;&#32034;&#30340;&#30149;&#29702;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#20999;&#29255;&#22270;&#20687;&#26159;&#29992;&#20110;&#30284;&#30151;&#35786;&#26029;&#21644;&#27835;&#30103;&#30340;&#25968;&#23383;&#30149;&#29702;&#23398;&#30340;&#22522;&#30784;&#12290;&#25776;&#20889;&#30149;&#29702;&#25253;&#21578;&#23545;&#32463;&#39564;&#19981;&#36275;&#30340;&#30149;&#29702;&#23398;&#23478;&#26469;&#35828;&#26159;&#36153;&#26102;&#19988;&#23481;&#26131;&#20986;&#38169;&#30340;&#12290;&#20026;&#20102;&#20943;&#23569;&#24037;&#20316;&#37327;&#24182;&#25913;&#21892;&#20020;&#24202;&#33258;&#21160;&#21270;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#29983;&#25104;&#32473;&#23450;&#20840;&#20999;&#29255;&#22270;&#20687;&#30340;&#30149;&#29702;&#25253;&#21578;&#12290;&#22312;&#25968;&#25454;&#31471;&#65292;&#25105;&#20204;&#25972;&#29702;&#20102;&#26368;&#22823;&#30340;WSI-&#25991;&#26412;&#25968;&#25454;&#38598;&#65288;TCGA-PathoText&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#35782;&#21035;&#21644;&#28165;&#29702;TCGA&#20013;&#21465;&#36848;&#35786;&#26029;&#24187;&#28783;&#29255;&#30340;&#30149;&#29702;&#25253;&#21578;&#65292;&#25910;&#38598;&#20102;&#36817;1&#19975;&#23545;&#39640;&#36136;&#37327;&#30340;WSI-&#25991;&#26412;&#37197;&#23545;&#65292;&#20379;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#12290;&#22312;&#27169;&#22411;&#31471;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#20197;&#20026;&#21315;&#20159;&#20687;&#32032;WSI&#29983;&#25104;&#30149;&#29702;&#25253;&#21578;&#30340;&#22810;&#23454;&#20363;&#29983;&#25104;&#27169;&#22411;&#65288;MI-Gen&#65289;&#12290;&#25105;&#20204;&#22312;TCGA-PathoText&#30340;&#26368;&#22823;&#23376;&#38598;&#19978;&#23545;&#25105;&#20204;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#21253;&#21547;&#22810;&#20010;&#20020;&#24202;&#32447;&#32034;&#30340;&#30149;&#29702;&#25253;&#21578;&#12290;&#27492;&#22806;&#65292;WSI-&#25991;&#26412;&#39044;&#27979;&#21487;&#34987;&#35270;&#20026;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16480v2 Announce Type: replace-cross  Abstract: Whole slide images are the foundation of digital pathology for the diagnosis and treatment of carcinomas. Writing pathology reports is laborious and error-prone for inexperienced pathologists. To reduce the workload and improve clinical automation, we investigate how to generate pathology reports given whole slide images. On the data end, we curated the largest WSI-text dataset (TCGA-PathoText). In specific, we collected nearly 10000 high-quality WSI-text pairs for visual-language models by recognizing and cleaning pathology reports which narrate diagnostic slides in TCGA. On the model end, we propose the multiple instance generative model (MI-Gen) which can produce pathology reports for gigapixel WSIs. We benchmark our model on the largest subset of TCGA-PathoText. Experimental results show our model can generate pathology reports which contain multiple clinical clues. Furthermore, WSI-text prediction can be seen as an approac
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25361;&#25112;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#24120;&#35265;&#33539;&#24335;&#65292;&#36890;&#36807;&#30740;&#31350;&#22312;&#21333;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#20248;&#21270;&#22120;&#36873;&#25321;&#22312;MTL&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#29702;&#35770;&#25512;&#23548;&#20986;&#20102;&#26799;&#24230;&#20914;&#31361;&#30340;&#35282;&#33394;&#12290;</title><link>https://arxiv.org/abs/2311.04698</link><description>&lt;p&gt;
&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#25361;&#25112;&#24120;&#35265;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Challenging Common Paradigms in Multi-Task Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04698
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25361;&#25112;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#24120;&#35265;&#33539;&#24335;&#65292;&#36890;&#36807;&#30740;&#31350;&#22312;&#21333;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#20248;&#21270;&#22120;&#36873;&#25321;&#22312;MTL&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#29702;&#35770;&#25512;&#23548;&#20986;&#20102;&#26799;&#24230;&#20914;&#31361;&#30340;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#24180;&#26469;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#21463;&#21040;&#20102;&#26497;&#22823;&#20851;&#27880;&#65292;&#20294;&#20854;&#22522;&#26412;&#26426;&#21046;&#20173;&#28982;&#30693;&#20043;&#29978;&#23569;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#24182;&#26410;&#24102;&#26469;&#19968;&#33268;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#30456;&#27604;&#21333;&#20219;&#21153;&#23398;&#20064;&#65288;STL&#65289;&#22522;&#32447;&#65292;&#24378;&#35843;&#20102;&#26356;&#28145;&#20837;&#20102;&#35299;MTL&#29305;&#23450;&#25361;&#25112;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;MTL&#20013;&#30340;&#33539;&#24335;&#65292;&#25552;&#20986;&#20102;&#20960;&#28857;&#20851;&#20110;STL&#30340;&#37325;&#35201;&#24433;&#21709;&#65306;&#39318;&#20808;&#65292;&#20248;&#21270;&#22120;&#30340;&#36873;&#25321;&#23545;MTL&#30340;&#24433;&#21709;&#21482;&#21463;&#21040;&#20102;&#36731;&#24494;&#30340;&#35843;&#26597;&#12290;&#25105;&#20204;&#36890;&#36807;&#21508;&#31181;&#23454;&#39564;&#30340;&#23454;&#35777;&#26041;&#27861;&#23637;&#31034;&#20102;&#24120;&#35265;STL&#24037;&#20855;&#65288;&#20363;&#22914;Adam&#20248;&#21270;&#22120;&#65289;&#22312;MTL&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;Adam&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#19968;&#23450;&#30340;&#20551;&#35774;&#19979;&#20174;&#29702;&#35770;&#19978;&#25512;&#23548;&#20986;&#37096;&#20998;&#25439;&#22833;&#23610;&#24230;&#19981;&#21464;&#24615;&#12290;&#20854;&#27425;&#65292;&#26799;&#24230;&#20914;&#31361;&#30340;&#27010;&#24565;&#32463;&#24120;&#34987;&#25551;&#36848;&#20026;MTL&#20013;&#30340;&#19968;&#20010;&#29305;&#23450;&#38382;&#39064;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#26799;&#24230;&#20914;&#31361;&#22312;MTL&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#23558;&#20854;&#19982;STL&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#35282;&#24230;&#26799;&#24230;&#23545;&#40784;&#26041;&#38754;&#65292;&#25105;&#20204;&#27809;&#26377;&#25214;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04698v3 Announce Type: replace-cross  Abstract: While multi-task learning (MTL) has gained significant attention in recent years, its underlying mechanisms remain poorly understood. Recent methods did not yield consistent performance improvements over single task learning (STL) baselines, underscoring the importance of gaining more profound insights about challenges specific to MTL. In our study, we challenge paradigms in MTL in the context of STL: First, the impact of the choice of optimizer has only been mildly investigated in MTL. We show the pivotal role of common STL tools such as the Adam optimizer in MTL empirically in various experiments. To further investigate Adam's effectiveness, we theoretical derive a partial loss-scale invariance under mild assumptions. Second, the notion of gradient conflicts has often been phrased as a specific problem in MTL. We delve into the role of gradient conflicts in MTL and compare it to STL. For angular gradient alignment we find no 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31185;&#23398;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#21487;&#25511;&#24615;&#12290;&#36890;&#36807;&#25511;&#21046;&#39118;&#26684;&#29305;&#24449;&#65292;&#38750;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35780;&#35770;&#29983;&#25104;&#20219;&#21153;&#20013;&#20248;&#20110;&#20154;&#31867;&#65292;&#21516;&#26102;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;&#24341;&#23548;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#22312;&#29983;&#25104;&#38271;&#25688;&#35201;&#21644;&#39640;&#24230;&#25277;&#35937;&#30340;&#31616;&#21270;&#25688;&#35201;&#26041;&#38754;&#26377;&#38480;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25688;&#35201;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#36890;&#29992;&#33021;&#21147;&#65292;&#20294;&#22312;&#22797;&#26434;&#25511;&#21046;&#26041;&#38754;&#26377;&#38480;&#12290;</title><link>http://arxiv.org/abs/2401.10415</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25688;&#35201;&#26426;&#33021;&#21542;&#36866;&#24212;&#19981;&#21516;&#31185;&#23398;&#20256;&#25773;&#30446;&#26631;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Model Summarizers Adapt to Diverse Scientific Communication Goals?. (arXiv:2401.10415v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31185;&#23398;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#21487;&#25511;&#24615;&#12290;&#36890;&#36807;&#25511;&#21046;&#39118;&#26684;&#29305;&#24449;&#65292;&#38750;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35780;&#35770;&#29983;&#25104;&#20219;&#21153;&#20013;&#20248;&#20110;&#20154;&#31867;&#65292;&#21516;&#26102;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;&#24341;&#23548;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#21487;&#25511;&#24615;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#22312;&#29983;&#25104;&#38271;&#25688;&#35201;&#21644;&#39640;&#24230;&#25277;&#35937;&#30340;&#31616;&#21270;&#25688;&#35201;&#26041;&#38754;&#26377;&#38480;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25688;&#35201;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#36890;&#29992;&#33021;&#21147;&#65292;&#20294;&#22312;&#22797;&#26434;&#25511;&#21046;&#26041;&#38754;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#22312;&#31185;&#23398;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#21487;&#25511;&#24615;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#34920;&#24449;&#35770;&#25991;&#35780;&#35770;&#12289;&#25688;&#35201;&#21644;&#31616;&#21270;&#25688;&#35201;&#31561;&#19981;&#21516;&#31867;&#22411;&#25688;&#35201;&#30340;&#20851;&#38190;&#39118;&#26684;&#21644;&#20869;&#23481;&#35206;&#30422;&#22240;&#32032;&#12290;&#36890;&#36807;&#25511;&#21046;&#39118;&#26684;&#29305;&#24449;&#65292;&#25105;&#20204;&#21457;&#29616;&#38750;&#24494;&#35843;&#30340;LLMs&#22312;MuP&#35780;&#35770;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#20154;&#31867;&#65292;&#26080;&#35770;&#26159;&#22312;&#19982;&#21442;&#32771;&#25688;&#35201;&#30340;&#30456;&#20284;&#24230;&#36824;&#26159;&#22312;&#20154;&#31867;&#20559;&#22909;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548; (CFG) &#26469;&#25913;&#21892;LLMs&#30340;&#21487;&#25511;&#24615;&#65292;&#22312;arXiv&#21644;PubMed&#19978;&#23454;&#29616;&#19982;&#24378;&#24494;&#35843;&#22522;&#32447;&#30456;&#24403;&#30340;&#35789;&#27719;&#37325;&#21472;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;LLMs&#26080;&#27861;&#19968;&#33268;&#22320;&#29983;&#25104;&#36229;&#36807;8&#20010;&#21477;&#23376;&#30340;&#38271;&#25688;&#35201;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#24230;&#25277;&#35937;&#30340;&#31616;&#21270;&#25688;&#35201;&#26041;&#38754;&#33021;&#21147;&#26377;&#38480;&#12290;&#34429;&#28982;LLMs&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#36890;&#29992;&#25688;&#35201;&#33021;&#21147;&#65292;&#20294;&#22312;&#19981;&#26114;&#36149;&#30340;&#24494;&#35843;&#25514;&#26045;&#19979;&#65292;&#23545;&#20869;&#23481;&#30340;&#22797;&#26434;&#25511;&#21046;&#33021;&#21147;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we investigate the controllability of large language models (LLMs) on scientific summarization tasks. We identify key stylistic and content coverage factors that characterize different types of summaries such as paper reviews, abstracts, and lay summaries. By controlling stylistic features, we find that non-fine-tuned LLMs outperform humans in the MuP review generation task, both in terms of similarity to reference summaries and human preferences. Also, we show that we can improve the controllability of LLMs with keyword-based classifier-free guidance (CFG) while achieving lexical overlap comparable to strong fine-tuned baselines on arXiv and PubMed. However, our results also indicate that LLMs cannot consistently generate long summaries with more than 8 sentences. Furthermore, these models exhibit limited capacity to produce highly abstractive lay summaries. Although LLMs demonstrate strong generic summarization competency, sophisticated content control without costly fi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#28216;&#25103;&#20013;&#35780;&#20272;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#65292;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#34920;&#29616;&#20986;&#21327;&#20316;&#34892;&#20026;&#21644;&#39640;&#32423;&#29702;&#35770;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#26126;&#30830;&#30340;&#20449;&#24565;&#29366;&#24577;&#34920;&#31034;&#26469;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#21644;&#29702;&#35770;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.10701</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Theory of Mind for Multi-Agent Collaboration via Large Language Models. (arXiv:2310.10701v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#28216;&#25103;&#20013;&#35780;&#20272;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#65292;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#34920;&#29616;&#20986;&#21327;&#20316;&#34892;&#20026;&#21644;&#39640;&#32423;&#29702;&#35770;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#26126;&#30830;&#30340;&#20449;&#24565;&#29366;&#24577;&#34920;&#31034;&#26469;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#21644;&#29702;&#35770;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#21644;&#35268;&#21010;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#23601;&#65292;&#20294;&#23427;&#22312;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#26041;&#38754;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#28145;&#20837;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21644;&#22522;&#20110;&#35268;&#21010;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#25991;&#26412;&#28216;&#25103;&#20013;&#35780;&#20272;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#22312;&#29702;&#35770;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#20986;&#29616;&#20102;&#21327;&#20316;&#34892;&#20026;&#21644;&#39640;&#32423;&#29702;&#35770;&#25512;&#29702;&#33021;&#21147;&#30340;&#35777;&#25454;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#22312;&#38271;&#26399;&#35268;&#21010;&#19978;&#23384;&#22312;&#20248;&#21270;&#30340;&#23616;&#38480;&#24615;&#65292;&#20197;&#21450;&#23545;&#20219;&#21153;&#29366;&#24577;&#30340;&#38169;&#35823;&#35748;&#30693;&#12290;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#26126;&#30830;&#30340;&#20449;&#24565;&#29366;&#24577;&#34920;&#31034;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#23427;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26234;&#33021;&#20307;&#30340;&#20219;&#21153;&#24615;&#33021;&#21644;&#29702;&#35770;&#25512;&#29702;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Large Language Models (LLMs) have demonstrated impressive accomplishments in both reasoning and planning, their abilities in multi-agent collaborations remains largely unexplored. This study evaluates LLM-based agents in a multi-agent cooperative text game with Theory of Mind (ToM) inference tasks, comparing their performance with Multi-Agent Reinforcement Learning (MARL) and planning-based baselines. We observed evidence of emergent collaborative behaviors and high-order Theory of Mind capabilities among LLM-based agents. Our results reveal limitations in LLM-based agents' planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state. We explore the use of explicit belief state representations to mitigate these issues, finding that it enhances task performance and the accuracy of ToM inferences for LLM-based agents.
&lt;/p&gt;</description></item><item><title>CP-KGC&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32422;&#26463;&#24335;&#25552;&#31034;&#26469;&#34917;&#20840;&#30693;&#35782;&#22270;&#35889;&#65292;&#25552;&#39640;&#25512;&#26029;&#25928;&#26524;&#65292;&#23637;&#31034;&#20102;&#22312;&#20302;&#36164;&#28304;&#35745;&#31639;&#26465;&#20214;&#19979;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20043;&#21069;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.08279</link><description>&lt;p&gt;
CP-KGC: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32422;&#26463;&#24335;&#25552;&#31034;&#23545;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
CP-KGC: Constrained-Prompt Knowledge Graph Completion with Large Language Models. (arXiv:2310.08279v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08279
&lt;/p&gt;
&lt;p&gt;
CP-KGC&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32422;&#26463;&#24335;&#25552;&#31034;&#26469;&#34917;&#20840;&#30693;&#35782;&#22270;&#35889;&#65292;&#25552;&#39640;&#25512;&#26029;&#25928;&#26524;&#65292;&#23637;&#31034;&#20102;&#22312;&#20302;&#36164;&#28304;&#35745;&#31639;&#26465;&#20214;&#19979;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20043;&#21069;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26088;&#22312;&#21033;&#29992;&#29616;&#26377;&#30693;&#35782;&#25512;&#26029;&#21644;&#25512;&#27979;&#30693;&#35782;&#22270;&#35889;&#20013;&#32570;&#22833;&#30340;&#36830;&#25509;&#12290;SimKGC&#31561;&#22522;&#20110;&#25991;&#26412;&#30340;&#26041;&#27861;&#24050;&#32463;&#36229;&#36807;&#20102;&#22270;&#23884;&#20837;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#24402;&#32435;&#24335;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#25991;&#26412;&#30340;&#26041;&#27861;&#30340;&#25928;&#26524;&#21462;&#20915;&#20110;&#23454;&#20307;&#25991;&#26412;&#25551;&#36848;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#20943;&#36731;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#30340;&#24187;&#35273;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#32422;&#26463;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#21033;&#29992;&#23454;&#20307;&#21450;&#20854;&#25991;&#26412;&#25551;&#36848;&#20316;&#20026;&#19978;&#19979;&#25991;&#32422;&#26463;&#26469;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#32422;&#26463;&#24335;&#25552;&#31034;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26041;&#27861;&#65288;CP-KGC&#65289;&#22312;&#20302;&#36164;&#28304;&#35745;&#31639;&#26465;&#20214;&#19979;&#34920;&#29616;&#20986;&#26377;&#25928;&#30340;&#25512;&#26029;&#33021;&#21147;&#65292;&#24182;&#36229;&#36807;&#20102;WN18RR&#21644;FB15K237&#25968;&#25454;&#38598;&#19978;&#30340;&#20043;&#21069;&#32467;&#26524;&#12290;&#36825;&#23637;&#31034;&#20102;LLMs&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#20013;&#30340;&#25972;&#21512;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph completion (KGC) aims to utilize existing knowledge to deduce and infer missing connections within knowledge graphs. Text-based approaches, like SimKGC, have outperformed graph embedding methods, showcasing the promise of inductive KGC. However, the efficacy of text-based methods hinges on the quality of entity textual descriptions. In this paper, we identify the key issue of whether large language models (LLMs) can generate effective text. To mitigate hallucination in LLM-generated text in this paper, we introduce a constraint-based prompt that utilizes the entity and its textual description as contextual constraints to enhance data quality. Our Constrained-Prompt Knowledge Graph Completion (CP-KGC) method demonstrates effective inference under low resource computing conditions and surpasses prior results on the WN18RR and FB15K237 datasets. This showcases the integration of LLMs in KGC tasks and provides new directions for future research.
&lt;/p&gt;</description></item><item><title>GPTFUZZER&#26159;&#19968;&#31181;&#40657;&#30418;&#36234;&#29425;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#65292;&#33258;&#21160;&#29983;&#25104;&#29992;&#20110;&#32418;&#38431;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#27169;&#26495;&#12290;&#36825;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#36991;&#20813;&#20102;&#25163;&#24037;&#24037;&#31243;&#65292;&#24182;&#36890;&#36807;&#31181;&#23376;&#36873;&#25321;&#31574;&#30053;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.10253</link><description>&lt;p&gt;
GPTFUZZER : &#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#36234;&#29425;&#25552;&#31034;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32418;&#38431;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
GPTFUZZER : Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts. (arXiv:2309.10253v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10253
&lt;/p&gt;
&lt;p&gt;
GPTFUZZER&#26159;&#19968;&#31181;&#40657;&#30418;&#36234;&#29425;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#65292;&#33258;&#21160;&#29983;&#25104;&#29992;&#20110;&#32418;&#38431;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#27169;&#26495;&#12290;&#36825;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#36991;&#20813;&#20102;&#25163;&#24037;&#24037;&#31243;&#65292;&#24182;&#36890;&#36807;&#31181;&#23376;&#36873;&#25321;&#31574;&#30053;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26368;&#36817;&#38750;&#24120;&#21463;&#27426;&#36814;&#65292;&#24191;&#27867;&#29992;&#20110;&#26085;&#24120;&#23545;&#35805;&#21040;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#32534;&#31243;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;LLMs&#24182;&#19981;&#23436;&#20840;&#21487;&#38752;&#65292;&#21487;&#33021;&#20250;&#25552;&#20379;&#26377;&#20851;&#36827;&#34892;&#26377;&#23475;&#25110;&#38750;&#27861;&#27963;&#21160;&#30340;&#35814;&#32454;&#25351;&#23548;&#12290;&#34429;&#28982;&#23433;&#20840;&#25514;&#26045;&#21487;&#20197;&#20943;&#23569;&#36825;&#20123;&#36755;&#20986;&#30340;&#39118;&#38505;&#65292;&#20294;&#23545;&#25239;&#24615;&#30340;"&#36234;&#29425;"&#25915;&#20987;&#20173;&#28982;&#21487;&#20197;&#21033;&#29992;LLMs&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#36825;&#20123;&#36234;&#29425;&#27169;&#26495;&#36890;&#24120;&#26159;&#25163;&#24037;&#31934;&#24515;&#21046;&#20316;&#30340;&#65292;&#20351;&#22823;&#35268;&#27169;&#27979;&#35797;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#40657;&#30418;&#36234;&#29425;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;\fuzzer&#65292;&#21463;AFL&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#30340;&#21551;&#21457;&#12290;&#19982;&#25163;&#24037;&#24037;&#31243;&#19981;&#21516;&#65292;\fuzzer&#33258;&#21160;&#21270;&#29983;&#25104;&#29992;&#20110;&#32418;&#38431;&#27979;&#35797;LLMs&#30340;&#36234;&#29425;&#27169;&#26495;&#12290;&#22312;&#26680;&#24515;&#37096;&#20998;&#65292;\fuzzer&#20174;&#20154;&#24037;&#32534;&#20889;&#30340;&#27169;&#26495;&#20316;&#20026;&#31181;&#23376;&#24320;&#22987;&#65292;&#28982;&#21518;&#20351;&#29992;&#21464;&#24322;&#25805;&#20316;&#23545;&#20854;&#36827;&#34892;&#21464;&#24322;&#20197;&#29983;&#25104;&#26032;&#30340;&#27169;&#26495;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;\fuzzer&#30340;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65306;&#29992;&#20110;&#24179;&#34913;&#25928;&#29575;&#30340;&#31181;&#23376;&#36873;&#25321;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently experienced tremendous popularity and are widely used from casual conversations to AI-driven programming. However, despite their considerable success, LLMs are not entirely reliable and can give detailed guidance on how to conduct harmful or illegal activities. While safety measures can reduce the risk of such outputs, adversarial "jailbreak" attacks can still exploit LLMs to produce harmful content. These jailbreak templates are typically manually crafted, making large-scale testing challenging. In this paper, we introduce \fuzzer, a novel black-box jailbreak fuzzing framework inspired by AFL fuzzing framework. Instead of manual engineering, \fuzzer automates the generation of jailbreak templates for red-teaming LLMs. At its core, \fuzzer starts with human-written templates as seeds, then mutates them using mutate operators to produce new templates. We detail three key components of \fuzzer: a seed selection strategy for balancing efficiency 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35780;&#20272;GPT3.5&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20855;&#26377;&#26377;&#36259;&#30340;&#20010;&#24615;&#38382;&#21367;&#22238;&#31572;&#33021;&#21147;&#65292;&#20294;&#19981;&#22826;&#21487;&#33021;&#21457;&#23637;&#20986;&#24847;&#35782;&#65292;&#24182;&#26174;&#31034;&#20986;&#36739;&#22823;&#30340;&#35748;&#30693;&#21644;&#20010;&#24615;&#21464;&#24322;&#12290;</title><link>http://arxiv.org/abs/2309.07683</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#36136;&#65306;&#23545;&#20154;&#31867;&#20013;&#24515;&#20027;&#20041;&#30340;&#35686;&#21578;
&lt;/p&gt;
&lt;p&gt;
Assessing the nature of large language models: A caution against anthropocentrism. (arXiv:2309.07683v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07683
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35780;&#20272;GPT3.5&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20855;&#26377;&#26377;&#36259;&#30340;&#20010;&#24615;&#38382;&#21367;&#22238;&#31572;&#33021;&#21147;&#65292;&#20294;&#19981;&#22826;&#21487;&#33021;&#21457;&#23637;&#20986;&#24847;&#35782;&#65292;&#24182;&#26174;&#31034;&#20986;&#36739;&#22823;&#30340;&#35748;&#30693;&#21644;&#20010;&#24615;&#21464;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36890;&#36807;OpenAI&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;ChatGPT&#30340;&#21457;&#24067;&#24341;&#36215;&#20102;&#20844;&#20247;&#30340;&#20851;&#27880;&#21644;&#29468;&#27979;&#12290;&#30446;&#21069;&#23384;&#22312;&#20004;&#31181;&#24847;&#35265;&#38453;&#33829;&#65306;&#19968;&#26041;&#23545;&#36825;&#20123;&#27169;&#22411;&#20026;&#20154;&#31867;&#20219;&#21153;&#24102;&#26469;&#30340;&#22522;&#26412;&#21464;&#38761;&#30340;&#21487;&#33021;&#24615;&#24863;&#21040;&#20852;&#22859;&#65292;&#21478;&#19968;&#26041;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#24378;&#22823;&#33021;&#21147;&#24863;&#21040;&#39640;&#24230;&#20851;&#20999;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#20851;&#20999;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26631;&#20934;&#12289;&#35268;&#33539;&#21270;&#21644;&#32463;&#36807;&#39564;&#35777;&#30340;&#35748;&#30693;&#21644;&#20010;&#24615;&#27979;&#37327;&#24037;&#20855;&#26469;&#35780;&#20272;GPT3.5&#12290;&#22312;&#36825;&#20010;&#21021;&#27493;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#22871;&#27979;&#35797;&#65292;&#21487;&#20197;&#20272;&#35745;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#36793;&#30028;&#65292;&#23427;&#20204;&#22312;&#30701;&#26102;&#38388;&#20869;&#30340;&#31283;&#23450;&#24615;&#20197;&#21450;&#19982;&#20154;&#31867;&#30340;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;GPT 3.5&#24456;&#21487;&#33021;&#27809;&#26377;&#20135;&#29983;&#24847;&#35782;&#65292;&#23613;&#31649;&#23427;&#23545;&#20010;&#24615;&#38382;&#21367;&#30340;&#22238;&#31572;&#33021;&#21147;&#20196;&#20154;&#24863;&#20852;&#36259;&#12290;&#23427;&#22312;&#37325;&#22797;&#35266;&#23519;&#36807;&#31243;&#20013;&#26174;&#31034;&#20986;&#35748;&#30693;&#21644;&#20010;&#24615;&#27979;&#37327;&#26041;&#38754;&#30340;&#22823;&#37327;&#21464;&#24322;&#65292;&#36825;&#19982;&#20855;&#26377;&#20154;&#31867;&#33324;&#20010;&#24615;&#30340;&#27169;&#22411;&#26159;&#19981;&#31526;&#21512;&#39044;&#26399;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI models garnered a large amount of public attention and speculation with the release of OpenAIs chatbot, ChatGPT. At least two opinion camps exist: one excited about possibilities these models offer for fundamental changes to human tasks, and another highly concerned about power these models seem to have. To address these concerns, we assessed GPT3.5 using standard, normed, and validated cognitive and personality measures. For this seedling project, we developed a battery of tests that allowed us to estimate the boundaries of some of these models capabilities, how stable those capabilities are over a short period of time, and how they compare to humans.  Our results indicate that GPT 3.5 is unlikely to have developed sentience, although its ability to respond to personality inventories is interesting. It did display large variability in both cognitive and personality measures over repeated observations, which is not expected if it had a human-like personality. Variability 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoSAM Adapter&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;SAM&#22312;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#30340;&#36866;&#24212;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#25552;&#31034;&#23398;&#20064;&#33539;&#24335;&#65292;&#28040;&#38500;&#20102;&#23545;&#25163;&#21160;&#29983;&#25104;&#25552;&#31034;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.14936</link><description>&lt;p&gt;
&#20026;&#31227;&#21160;&#21451;&#22909;&#30340;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#33258;&#21160;&#25552;&#31034;SAM
&lt;/p&gt;
&lt;p&gt;
Auto-Prompting SAM for Mobile Friendly 3D Medical Image Segmentation. (arXiv:2308.14936v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14936
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoSAM Adapter&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;SAM&#22312;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#30340;&#36866;&#24212;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#25552;&#31034;&#23398;&#20064;&#33539;&#24335;&#65292;&#28040;&#38500;&#20102;&#23545;&#25163;&#21160;&#29983;&#25104;&#25552;&#31034;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Segment Anything Model (SAM)&#24050;&#32463;&#34987;&#36805;&#36895;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#22270;&#20687;&#30340;&#20998;&#21106;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;SAM&#22312;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#19981;&#20339;&#12290;&#38500;&#20102;&#33258;&#28982;&#22270;&#20687;&#21644;&#21307;&#23398;&#22270;&#20687;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#22806;&#65292;2D&#21644;3D&#22270;&#20687;&#20043;&#38388;&#30340;&#31354;&#38388;&#24067;&#23616;&#24046;&#24322;&#65292;&#24378;&#22823;&#30340;GPU&#26381;&#21153;&#22120;&#25152;&#24102;&#26469;&#30340;&#22823;&#37327;&#35745;&#31639;&#36127;&#25285;&#65292;&#20197;&#21450;&#32791;&#26102;&#30340;&#25163;&#21160;&#25552;&#31034;&#29983;&#25104;&#20351;&#24471;SAM&#26080;&#27861;&#25193;&#23637;&#21040;&#26356;&#24191;&#27867;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;AutoSAM Adapter&#65292;&#19987;&#20026;3D&#22810;&#22120;&#23448;CT&#20998;&#21106;&#32780;&#35774;&#35745;&#12290;&#25105;&#20204;&#37319;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#36866;&#24212;&#25216;&#26415;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#21160;&#25552;&#31034;&#23398;&#20064;&#33539;&#24335;&#65292;&#20197;&#20419;&#36827;&#23558;SAM&#27169;&#22411;&#30340;&#33021;&#21147;&#36716;&#21270;&#20026;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#28040;&#38500;&#20102;&#25163;&#21160;&#29983;&#25104;&#25552;&#31034;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Segment Anything Model (SAM) has rapidly been adopted for segmenting a wide range of natural images. However, recent studies have indicated that SAM exhibits subpar performance on 3D medical image segmentation tasks. In addition to the domain gaps between natural and medical images, disparities in the spatial arrangement between 2D and 3D images, the substantial computational burden imposed by powerful GPU servers, and the time-consuming manual prompt generation impede the extension of SAM to a broader spectrum of medical image segmentation applications. To address these challenges, in this work, we introduce a novel method, AutoSAM Adapter, designed specifically for 3D multi-organ CT-based segmentation. We employ parameter-efficient adaptation techniques in developing an automatic prompt learning paradigm to facilitate the transformation of the SAM model's capabilities to 3D medical image segmentation, eliminating the need for manually generated prompts. Furthermore, we effectivel
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#20934;&#26041;&#27861;&#26469;&#35780;&#20272;&#20219;&#20309;&#27169;&#24577;&#30340;&#29983;&#25104;AI&#31995;&#32479;&#30340;&#31038;&#20250;&#24433;&#21709;&#65292;&#20998;&#20026;&#22522;&#30784;&#31995;&#32479;&#21644;&#31038;&#20250;&#26041;&#38754;&#30340;&#35780;&#20272;&#65292;&#28085;&#30422;7&#20010;&#31038;&#20250;&#24433;&#21709;&#31867;&#21035;&#65292;&#21253;&#25324;&#20559;&#35265;&#12289;&#38544;&#31169;&#20445;&#25252;&#12289;&#29615;&#22659;&#25104;&#26412;&#31561;&#12290;</title><link>http://arxiv.org/abs/2306.05949</link><description>&lt;p&gt;
&#35780;&#20272;&#29983;&#25104;AI&#31995;&#32479;&#22312;&#31995;&#32479;&#21644;&#31038;&#20250;&#20013;&#30340;&#31038;&#20250;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Social Impact of Generative AI Systems in Systems and Society. (arXiv:2306.05949v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05949
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#20934;&#26041;&#27861;&#26469;&#35780;&#20272;&#20219;&#20309;&#27169;&#24577;&#30340;&#29983;&#25104;AI&#31995;&#32479;&#30340;&#31038;&#20250;&#24433;&#21709;&#65292;&#20998;&#20026;&#22522;&#30784;&#31995;&#32479;&#21644;&#31038;&#20250;&#26041;&#38754;&#30340;&#35780;&#20272;&#65292;&#28085;&#30422;7&#20010;&#31038;&#20250;&#24433;&#21709;&#31867;&#21035;&#65292;&#21253;&#25324;&#20559;&#35265;&#12289;&#38544;&#31169;&#20445;&#25252;&#12289;&#29615;&#22659;&#25104;&#26412;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;AI&#31995;&#32479;&#36328;&#36234;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#31561;&#22810;&#31181;&#27169;&#24577;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#31038;&#20250;&#24433;&#21709;&#65292;&#20294;&#30446;&#21069;&#19981;&#23384;&#22312;&#23448;&#26041;&#26631;&#20934;&#26469;&#35780;&#20272;&#36825;&#20123;&#24433;&#21709;&#21644;&#24212;&#35813;&#35780;&#20272;&#21738;&#20123;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#20934;&#26041;&#27861;&#26469;&#35780;&#20272;&#20219;&#20309;&#27169;&#24577;&#30340;&#29983;&#25104;AI&#31995;&#32479;&#65292;&#20998;&#20026;&#20004;&#22823;&#31867;&#21035;&#65306;&#23545;&#20110;&#27809;&#26377;&#39044;&#23450;&#24212;&#29992;&#30340;&#22522;&#30784;&#31995;&#32479;&#21487;&#20197;&#35780;&#20272;&#20160;&#20040;&#65292;&#20197;&#21450;&#21487;&#20197;&#22312;&#31038;&#20250;&#20013;&#35780;&#20272;&#20160;&#20040;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#20855;&#20307;&#30340;&#31038;&#20250;&#24433;&#21709;&#31867;&#21035;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#22522;&#30784;&#25216;&#26415;&#31995;&#32479;&#12289;&#20154;&#27665;&#21644;&#31038;&#20250;&#12290;&#25105;&#20204;&#30340;&#22522;&#30784;&#31995;&#32479;&#26694;&#26550;&#23450;&#20041;&#20102;&#19971;&#20010;&#31038;&#20250;&#24433;&#21709;&#31867;&#21035;&#65306;&#20559;&#35265;&#12289;&#21051;&#26495;&#21360;&#35937;&#21644;&#34920;&#29616;&#24615;&#20260;&#23475;&#65307;&#25991;&#21270;&#20215;&#20540;&#21644;&#25935;&#24863;&#20869;&#23481;&#65307;&#19981;&#23545;&#31561;&#30340;&#24615;&#33021;&#65307;&#38544;&#31169;&#21644;&#25968;&#25454;&#20445;&#25252;&#65307;&#36130;&#21153;&#25104;&#26412;&#65307;&#29615;&#22659;&#25104;&#26412;&#65307;&#20197;&#21450;&#25968;&#25454;&#21644;&#20869;&#23481;&#30417;&#31649;&#21171;&#21160;&#25104;&#26412;&#12290;&#24314;&#35758;&#30340;&#35780;&#20272;&#26041;&#27861;&#36866;&#29992;&#20110;&#25152;&#26377;&#27169;&#24577;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI systems across modalities, ranging from text, image, audio, and video, have broad social impacts, but there exists no official standard for means of evaluating those impacts and which impacts should be evaluated. We move toward a standard approach in evaluating a generative AI system for any modality, in two overarching categories: what is able to be evaluated in a base system that has no predetermined application and what is able to be evaluated in society. We describe specific social impact categories and how to approach and conduct evaluations in the base technical system, then in people and society. Our framework for a base system defines seven categories of social impact: bias, stereotypes, and representational harms; cultural values and sensitive content; disparate performance; privacy and data protection; financial costs; environmental costs; and data and content moderation labor costs. Suggested methods for evaluation apply to all modalities and analyses of the li
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38750;&#27431;&#20960;&#37324;&#24503;&#26053;&#34892;&#21830;&#38382;&#39064;&#30340;&#20984;&#21253;&#26368;&#20415;&#23452;&#25554;&#20837;&#21551;&#21457;&#24335;&#35299;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#32500;&#32553;&#25918;&#23558;&#38750;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#30340;&#28857;&#36817;&#20284;&#21040;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#65292;&#29983;&#25104;&#20102;&#21021;&#22987;&#21270;&#31639;&#27861;&#30340;&#20984;&#21253;&#12290;&#22312;&#35780;&#20272;&#20013;&#21457;&#29616;&#65292;&#35813;&#31639;&#27861;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20248;&#20110;&#26368;&#37051;&#36817;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.06582</link><description>&lt;p&gt;
&#38750;&#27431;&#20960;&#37324;&#24503;&#26053;&#34892;&#21830;&#38382;&#39064;&#30340;&#20984;&#21253;&#26368;&#20415;&#23452;&#25554;&#20837;&#21551;&#21457;&#24335;&#35299;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Convex Hull Cheapest Insertion Heuristic for the Non-Euclidean TSP. (arXiv:2302.06582v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38750;&#27431;&#20960;&#37324;&#24503;&#26053;&#34892;&#21830;&#38382;&#39064;&#30340;&#20984;&#21253;&#26368;&#20415;&#23452;&#25554;&#20837;&#21551;&#21457;&#24335;&#35299;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#32500;&#32553;&#25918;&#23558;&#38750;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#30340;&#28857;&#36817;&#20284;&#21040;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#65292;&#29983;&#25104;&#20102;&#21021;&#22987;&#21270;&#31639;&#27861;&#30340;&#20984;&#21253;&#12290;&#22312;&#35780;&#20272;&#20013;&#21457;&#29616;&#65292;&#35813;&#31639;&#27861;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20248;&#20110;&#26368;&#37051;&#36817;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#20984;&#21253;&#26368;&#20415;&#23452;&#25554;&#20837;&#21551;&#21457;&#24335;&#31639;&#27861;&#21487;&#20197;&#22312;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#20135;&#29983;&#33391;&#22909;&#30340;&#26053;&#34892;&#21830;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#36824;&#26410;&#22312;&#38750;&#27431;&#20960;&#37324;&#24503;&#24773;&#20917;&#19979;&#36827;&#34892;&#25193;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#38750;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#22788;&#29702;&#38556;&#30861;&#29289;&#30340;&#22256;&#38590;&#65292;&#25552;&#20986;&#30340;&#25913;&#36827;&#26041;&#27861;&#20351;&#29992;&#22810;&#32500;&#32553;&#25918;&#23558;&#36825;&#20123;&#28857;&#39318;&#20808;&#36817;&#20284;&#21040;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#65292;&#20174;&#32780;&#21487;&#20197;&#29983;&#25104;&#21021;&#22987;&#21270;&#31639;&#27861;&#30340;&#20984;&#21253;&#12290;&#36890;&#36807;&#20462;&#25913;TSPLIB&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21521;&#20854;&#20013;&#28155;&#21152;&#19981;&#21487;&#36890;&#36807;&#30340;&#20998;&#21106;&#22120;&#26469;&#20135;&#29983;&#38750;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#65292;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#12290;&#22312;&#25152;&#30740;&#31350;&#30340;&#26696;&#20363;&#20013;&#65292;&#35813;&#31639;&#27861;&#34920;&#29616;&#20986;&#20248;&#20110;&#24120;&#29992;&#30340;&#26368;&#37051;&#36817;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#36798;&#21040;96%&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
The convex hull cheapest insertion heuristic is known to generate good solutions to the Traveling Salesperson Problem in Euclidean spaces, but it has not been extended to the non-Euclidean case. To address the difficulty of dealing with obstacles in the non-Euclidean space, the proposed adaptation uses multidimensional scaling to first approximate these points in a Euclidean space, thereby enabling the generation of the convex hull that initializes the algorithm. To evaluate the proposed algorithm, the TSPLIB benchmark data-set is modified by adding impassable separators that produce non-Euclidean spaces. The algorithm is demonstrated to outperform the commonly used Nearest Neighbor algorithm in 96% of the cases studied.
&lt;/p&gt;</description></item><item><title>&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#27169;&#22411;&#65292;&#33021;&#22815;&#35782;&#21035;&#21644;&#35299;&#32544;&#35266;&#27979;&#25968;&#25454;&#20013;&#38544;&#34255;&#30340;&#22240;&#32032;&#65292;&#20174;&#32780;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#23427;&#22312;&#25552;&#39640;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25511;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.11695</link><description>&lt;p&gt;
&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Disentangled Representation Learning. (arXiv:2211.11695v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11695
&lt;/p&gt;
&lt;p&gt;
&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#27169;&#22411;&#65292;&#33021;&#22815;&#35782;&#21035;&#21644;&#35299;&#32544;&#35266;&#27979;&#25968;&#25454;&#20013;&#38544;&#34255;&#30340;&#22240;&#32032;&#65292;&#20174;&#32780;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#23427;&#22312;&#25552;&#39640;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25511;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#65288;DRL&#65289;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#35782;&#21035;&#21644;&#35299;&#32544;&#21487;&#35266;&#27979;&#25968;&#25454;&#20013;&#38544;&#34255;&#22240;&#32032;&#30340;&#27169;&#22411;&#12290;&#23558;&#21464;&#21270;&#30340;&#28508;&#22312;&#35201;&#32032;&#20998;&#31163;&#25104;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#21464;&#37327;&#30340;&#36807;&#31243;&#26377;&#21161;&#20110;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#27169;&#20223;&#20154;&#31867;&#35266;&#23519;&#23545;&#35937;&#25110;&#20851;&#31995;&#26102;&#30340;&#26377;&#24847;&#20041;&#29702;&#35299;&#36807;&#31243;&#12290;&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;DRL&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#23637;&#31034;&#20102;&#25552;&#39640;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25511;&#24615;&#12289;&#40065;&#26834;&#24615;&#20197;&#21450;&#27867;&#21270;&#33021;&#21147;&#30340;&#20248;&#21183;&#65292;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#25968;&#25454;&#25366;&#25496;&#31561;&#12290;&#26412;&#25991;&#32508;&#21512;&#35780;&#36848;&#20102;DRL&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#21160;&#26426;&#12289;&#23450;&#20041;&#12289;&#26041;&#27861;&#35770;&#12289;&#35780;&#20272;&#12289;&#24212;&#29992;&#21644;&#27169;&#22411;&#35774;&#35745;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22522;&#20110;&#20004;&#20010;&#20844;&#35748;&#23450;&#20041;&#65288;&#30452;&#35266;&#23450;&#20041;&#21644;&#32676;&#35770;&#23450;&#20041;&#65289;&#30340;DRL&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;DRL&#30340;&#24320;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disentangled Representation Learning (DRL) aims to learn a model capable of identifying and disentangling the underlying factors hidden in the observable data in representation form. The process of separating underlying factors of variation into variables with semantic meaning benefits in learning explainable representations of data, which imitates the meaningful understanding process of humans when observing an object or relation. As a general learning strategy, DRL has demonstrated its power in improving the model explainability, controlability, robustness, as well as generalization capacity in a wide range of scenarios such as computer vision, natural language processing, data mining etc. In this article, we comprehensively review DRL from various aspects including motivations, definitions, methodologies, evaluations, applications and model designs. We discuss works on DRL based on two well-recognized definitions, i.e., Intuitive Definition and Group Theory Definition. We further ca
&lt;/p&gt;</description></item></channel></rss>