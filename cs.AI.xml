<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>RadCLIP&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#36328;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#21033;&#29992;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#20197;&#25913;&#36827;&#25918;&#23556;&#23398;&#22270;&#20687;&#20998;&#26512;&#65292;&#21253;&#21547;&#38024;&#23545;&#20307;&#31215;&#22270;&#20687;&#20998;&#26512;&#23450;&#21046;&#30340;&#26032;&#39062;3D&#20999;&#29255;&#27744;&#21270;&#26426;&#21046;&#65292;&#24182;&#20351;&#29992;&#20016;&#23500;&#22810;&#26679;&#30340;&#25918;&#23556;&#23398;&#22270;&#20687;-&#25991;&#26412;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2403.09948</link><description>&lt;p&gt;
RadCLIP: &#36890;&#36807;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#22686;&#24378;&#25918;&#23556;&#23398;&#22270;&#20687;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
RadCLIP: Enhancing Radiologic Image Analysis through Contrastive Language-Image Pre-training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09948
&lt;/p&gt;
&lt;p&gt;
RadCLIP&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#36328;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#21033;&#29992;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#20197;&#25913;&#36827;&#25918;&#23556;&#23398;&#22270;&#20687;&#20998;&#26512;&#65292;&#21253;&#21547;&#38024;&#23545;&#20307;&#31215;&#22270;&#20687;&#20998;&#26512;&#23450;&#21046;&#30340;&#26032;&#39062;3D&#20999;&#29255;&#27744;&#21270;&#26426;&#21046;&#65292;&#24182;&#20351;&#29992;&#20016;&#23500;&#22810;&#26679;&#30340;&#25918;&#23556;&#23398;&#22270;&#20687;-&#25991;&#26412;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09948v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495;  &#25688;&#35201;: &#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#19982;&#25918;&#23556;&#23398;&#30340;&#25972;&#21512;&#26631;&#24535;&#30528;&#21307;&#23398;&#35786;&#26029;&#39046;&#22495;&#30340;&#21464;&#38761;&#26102;&#20195;&#12290;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#24050;&#34987;&#37319;&#29992;&#26469;&#22686;&#24378;&#25918;&#23556;&#23398;&#22270;&#20687;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#25918;&#23556;&#23398;&#22270;&#20687;&#30340;&#29420;&#29305;&#22797;&#26434;&#24615;&#65292;&#21253;&#25324;&#23545;2D&#21644;3D&#25918;&#23556;&#23398;&#25968;&#25454;&#30340;&#35299;&#35835;&#65292;&#24102;&#26469;&#20102;&#29616;&#26377;&#27169;&#22411;&#26080;&#27861;&#20805;&#20998;&#24212;&#23545;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#26159;&#22312;&#36890;&#29992;&#38750;&#21307;&#23398;&#22270;&#20687;&#19978;&#35757;&#32451;&#30340;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#24182;&#20805;&#20998;&#21033;&#29992;&#21307;&#23398;&#25104;&#20687;&#25152;&#38656;&#30340;&#35786;&#26029;&#31934;&#24230;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;RadCLIP&#65306;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#36328;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#21033;&#29992;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#26469;&#25913;&#36827;&#25918;&#23556;&#23398;&#22270;&#20687;&#20998;&#26512;&#12290;RadCLIP&#21253;&#21547;&#19968;&#31181;&#26032;&#39062;&#30340;3D&#20999;&#29255;&#27744;&#21270;&#26426;&#21046;&#65292;&#19987;&#20026;&#20307;&#31215;&#22270;&#20687;&#20998;&#26512;&#23450;&#21046;&#65292;&#20351;&#29992;&#20102;&#20016;&#23500;&#22810;&#26679;&#30340;&#25918;&#23556;&#23398;&#22270;&#20687;-&#25991;&#26412;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;RadCLIP&#33021;&#26377;&#25928;&#22320;&#23545;&#40784;&#25918;&#23556;&#23398;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09948v1 Announce Type: cross  Abstract: The integration of artificial intelligence (AI) with radiology has marked a transformative era in medical diagnostics. Vision foundation models have been adopted to enhance radiologic imaging analysis. However, the distinct complexities of radiological imaging, including the interpretation of 2D and 3D radiological data, pose unique challenges that existing models, trained on general non-medical images, fail to address adequately. To bridge this gap and capitalize on the diagnostic precision required in medical imaging, we introduce RadCLIP: a pioneering cross-modal foundational model that harnesses Contrastive Language-Image Pre-training (CLIP) to refine radiologic image analysis. RadCLIP incorporates a novel 3D slice pooling mechanism tailored for volumetric image analysis and is trained using a comprehensive and diverse dataset of radiologic image-text pairs. Our evaluations demonstrate that RadCLIP effectively aligns radiological i
&lt;/p&gt;</description></item><item><title>FineMath&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#32454;&#31890;&#24230;&#25968;&#23398;&#35780;&#20272;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#23567;&#23398;&#25968;&#23398;&#20013;&#30340;&#20027;&#35201;&#27010;&#24565;&#65292;&#21010;&#20998;&#20026;17&#31867;&#25968;&#23398;&#38382;&#39064;&#65292;&#24182;&#25163;&#21160;&#27880;&#37322;&#38590;&#24230;&#32423;&#21035;&#65292;&#23454;&#39564;&#35777;&#23454;&#22312;&#25968;&#23398;&#26041;&#38754;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.07747</link><description>&lt;p&gt;
FineMath&#65306;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#25968;&#23398;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
FineMath: A Fine-Grained Mathematical Evaluation Benchmark for Chinese Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07747
&lt;/p&gt;
&lt;p&gt;
FineMath&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#32454;&#31890;&#24230;&#25968;&#23398;&#35780;&#20272;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#23567;&#23398;&#25968;&#23398;&#20013;&#30340;&#20027;&#35201;&#27010;&#24565;&#65292;&#21010;&#20998;&#20026;17&#31867;&#25968;&#23398;&#38382;&#39064;&#65292;&#24182;&#25163;&#21160;&#27880;&#37322;&#38590;&#24230;&#32423;&#21035;&#65292;&#23454;&#39564;&#35777;&#23454;&#22312;&#25968;&#23398;&#26041;&#38754;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#38656;&#35201;&#31934;&#24515;&#31574;&#21010;&#28085;&#30422;&#19981;&#21516;&#38590;&#24230;&#32423;&#21035;&#30340;&#21508;&#31181;&#25968;&#23398;&#27010;&#24565;&#21644;&#25968;&#23398;&#38382;&#39064;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;FineMath&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20013;&#25991;LLMs&#30340;&#32454;&#31890;&#24230;&#25968;&#23398;&#35780;&#20272;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;FineMath&#26088;&#22312;&#28085;&#30422;&#23567;&#23398;&#25968;&#23398;&#20013;&#25945;&#25480;&#30340;&#20027;&#35201;&#25968;&#23398;&#27010;&#24565;&#65292;&#36827;&#19968;&#27493;&#21010;&#20998;&#20026;17&#31867;&#25968;&#23398;&#38382;&#39064;&#65292;&#20174;&#32780;&#28145;&#20837;&#20998;&#26512;LLMs&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#25152;&#26377;17&#31867;&#25968;&#23398;&#38382;&#39064;&#22343;&#26681;&#25454;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#25152;&#38656;&#30340;&#25512;&#29702;&#27493;&#39588;&#25968;&#37327;&#36827;&#34892;&#25163;&#21160;&#27880;&#37322;&#20854;&#38590;&#24230;&#32423;&#21035;&#12290;&#25105;&#20204;&#22312;FineMath&#19978;&#23545;&#21508;&#31181;LLMs&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#22312;&#25968;&#23398;&#26041;&#38754;&#20173;&#26377;&#30456;&#24403;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07747v1 Announce Type: cross  Abstract: To thoroughly assess the mathematical reasoning abilities of Large Language Models (LLMs), we need to carefully curate evaluation datasets covering diverse mathematical concepts and mathematical problems at different difficulty levels. In pursuit of this objective, we propose FineMath in this paper, a fine-grained mathematical evaluation benchmark dataset for assessing Chinese LLMs. FineMath is created to cover the major key mathematical concepts taught in elementary school math, which are further divided into 17 categories of math word problems, enabling in-depth analysis of mathematical reasoning abilities of LLMs. All the 17 categories of math word problems are manually annotated with their difficulty levels according to the number of reasoning steps required to solve these problems. We conduct extensive experiments on a wide range of LLMs on FineMath and find that there is still considerable room for improvements in terms of mathem
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36755;&#20837;&#20984;&#25439;&#22833;&#32593;&#32476;&#65288;ICLN&#65289;&#65292;&#36890;&#36807;&#36755;&#20837;&#20984;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20219;&#21153;&#25439;&#22833;&#65292;&#20026;&#20915;&#31574;&#38598;&#20013;&#23398;&#20064;&#25552;&#20379;&#20102;&#20840;&#23616;&#26367;&#20195;&#25439;&#22833;&#12290;</title><link>https://arxiv.org/abs/2403.01875</link><description>&lt;p&gt;
ICLN&#65306;&#36755;&#20837;&#20984;&#25439;&#22833;&#32593;&#32476;&#29992;&#20110;&#20915;&#31574;&#38598;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ICLN: Input Convex Loss Network for Decision Focused Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01875
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36755;&#20837;&#20984;&#25439;&#22833;&#32593;&#32476;&#65288;ICLN&#65289;&#65292;&#36890;&#36807;&#36755;&#20837;&#20984;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20219;&#21153;&#25439;&#22833;&#65292;&#20026;&#20915;&#31574;&#38598;&#20013;&#23398;&#20064;&#25552;&#20379;&#20102;&#20840;&#23616;&#26367;&#20195;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#30830;&#23450;&#24615;&#26465;&#20214;&#19979;&#30340;&#20915;&#31574;&#38382;&#39064;&#20013;&#65292;&#39044;&#27979;&#26410;&#30693;&#21442;&#25968;&#36890;&#24120;&#34987;&#35748;&#20026;&#19982;&#20248;&#21270;&#37096;&#20998;&#26080;&#20851;&#12290;&#20915;&#31574;&#38598;&#20013;&#23398;&#20064;&#65288;DFL&#65289;&#26159;&#19968;&#20010;&#38754;&#21521;&#20219;&#21153;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35843;&#25972;&#39044;&#27979;&#27169;&#22411;&#20197;&#20026;&#30456;&#24212;&#20219;&#21153;&#25552;&#20379;&#26356;&#22909;&#30340;&#20915;&#31574;&#26469;&#25972;&#21512;&#39044;&#27979;&#21644;&#20248;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#36755;&#20837;&#20984;&#25439;&#22833;&#32593;&#32476;&#65288;ICLN&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20840;&#23616;&#26367;&#20195;&#25439;&#22833;&#65292;&#21487;&#20197;&#22312;&#19968;&#33324;&#30340;DFL&#33539;&#24335;&#20013;&#23454;&#29616;&#12290;ICLN&#36890;&#36807;&#36755;&#20837;&#20984;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20219;&#21153;&#25439;&#22833;&#65292;&#24050;&#32463;&#34987;&#20445;&#35777;&#20026;&#26576;&#20123;&#24773;&#20917;&#19979;&#26159;&#20984;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01875v1 Announce Type: cross  Abstract: In decision-making problem under uncertainty, predicting unknown parameters is often considered independent of the optimization part. Decision-focused Learning (DFL) is a task-oriented framework to integrate prediction and optimization by adapting predictive model to give better decision for the corresponding task. Here, an inevitable challenge arises when computing gradients of the optimal decision with respect to the parameters. Existing researches cope this issue by smoothly reforming surrogate optimization or construct surrogate loss function that mimic task loss. However, they are applied to restricted optimization domain or build functions in a local manner leading a large computational time. In this paper, we propose Input Convex Loss Network (ICLN), a novel global surrogate loss which can be implemented in a general DFL paradigm. ICLN learns task loss via Input Convex Neural Networks which is guaranteed to be convex for some in
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;LLMs&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#29992;ChatGPT-3.5&#12289;GoogleBard&#21644;GoogleGemini&#23545;&#21015;&#26631;&#39064;&#36827;&#34892;&#20027;&#39064;&#27880;&#37322;&#30340;&#20803;&#25968;&#25454;&#20016;&#23500;&#21270;&#65292;&#30740;&#31350;&#23427;&#20204;&#22312;&#23545;&#39046;&#22495;&#29305;&#23450;&#20027;&#39064;&#36827;&#34892;&#20998;&#31867;&#30340;&#33021;&#21147;&#65292;&#24182;&#35780;&#20272;&#20854;&#20869;&#37096;&#19968;&#33268;&#24615;&#12289;&#26426;&#22120;&#23545;&#40784;&#24615;&#21644;&#20154;&#26426;&#19968;&#33268;&#24615;&#12290;ChatGPT&#21644;GoogleGemini&#22312;&#20869;&#37096;&#19968;&#33268;&#24615;&#20197;&#21450;&#19982;&#20154;&#19968;&#33268;&#24615;&#26041;&#38754;&#20248;&#20110;GoogleBard&#12290;</title><link>https://arxiv.org/abs/2403.00884</link><description>&lt;p&gt;
&#21033;&#29992;LLMs&#36827;&#34892;&#20803;&#25968;&#25454;&#20016;&#23500;&#21270;&#30340;&#21463;&#25511;&#35789;&#27719;&#21015;&#26631;&#39064;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Text classification of column headers with a controlled vocabulary: leveraging LLMs for metadata enrichment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00884
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;LLMs&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#29992;ChatGPT-3.5&#12289;GoogleBard&#21644;GoogleGemini&#23545;&#21015;&#26631;&#39064;&#36827;&#34892;&#20027;&#39064;&#27880;&#37322;&#30340;&#20803;&#25968;&#25454;&#20016;&#23500;&#21270;&#65292;&#30740;&#31350;&#23427;&#20204;&#22312;&#23545;&#39046;&#22495;&#29305;&#23450;&#20027;&#39064;&#36827;&#34892;&#20998;&#31867;&#30340;&#33021;&#21147;&#65292;&#24182;&#35780;&#20272;&#20854;&#20869;&#37096;&#19968;&#33268;&#24615;&#12289;&#26426;&#22120;&#23545;&#40784;&#24615;&#21644;&#20154;&#26426;&#19968;&#33268;&#24615;&#12290;ChatGPT&#21644;GoogleGemini&#22312;&#20869;&#37096;&#19968;&#33268;&#24615;&#20197;&#21450;&#19982;&#20154;&#19968;&#33268;&#24615;&#26041;&#38754;&#20248;&#20110;GoogleBard&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#25968;&#25454;&#38598;&#26816;&#32034;&#31995;&#32479;&#20027;&#35201;&#22312;&#20803;&#25968;&#25454;&#20449;&#24687;&#32780;&#38750;&#25968;&#25454;&#20540;&#19978;&#24314;&#31435;&#32034;&#24341;&#12290;&#22240;&#27492;&#20027;&#35201;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#21644;&#39640;&#36136;&#37327;&#30340;&#20803;&#25968;&#25454;&#65292;&#36825;&#20123;&#36807;&#31243;&#34987;&#35748;&#20026;&#26159;&#32791;&#26102;&#19988;&#38590;&#20197;&#33258;&#21160;&#21270;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#19977;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25903;&#25345;&#23545;&#21015;&#26631;&#39064;&#36827;&#34892;&#20027;&#39064;&#27880;&#37322;&#30340;&#20803;&#25968;&#25454;&#20016;&#23500;&#21270;&#65306;ChatGPT-3.5&#12289;GoogleBard&#21644;GoogleGemini&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#22522;&#20110;&#21463;&#25511;&#35789;&#27719;&#30340;&#39046;&#22495;&#29305;&#23450;&#20027;&#39064;&#23545;&#21015;&#26631;&#39064;&#36827;&#34892;&#20998;&#31867;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;LLMs&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#12289;&#26426;&#22120;&#38388;&#23545;&#40784;&#20197;&#21450;&#20154;&#26426;&#23545;&#20027;&#39064;&#20998;&#31867;&#20219;&#21153;&#30340;&#19968;&#33268;&#24615;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#19978;&#19979;&#25991;&#20449;&#24687;&#65288;&#21363;&#25968;&#25454;&#38598;&#25551;&#36848;&#65289;&#23545;&#20998;&#31867;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#21644;GoogleGemini&#22312;&#20869;&#37096;&#19968;&#33268;&#24615;&#20197;&#21450;LLM&#19982;&#20154;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;GoogleBard&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00884v1 Announce Type: cross  Abstract: Traditional dataset retrieval systems index on metadata information rather than on the data values. Thus relying primarily on manual annotations and high-quality metadata, processes known to be labour-intensive and challenging to automate. We propose a method to support metadata enrichment with topic annotations of column headers using three Large Language Models (LLMs): ChatGPT-3.5, GoogleBard and GoogleGemini. We investigate the LLMs ability to classify column headers based on domain-specific topics from a controlled vocabulary. We evaluate our approach by assessing the internal consistency of the LLMs, the inter-machine alignment, and the human-machine agreement for the topic classification task. Additionally, we investigate the impact of contextual information (i.e. dataset description) on the classification outcomes. Our results suggest that ChatGPT and GoogleGemini outperform GoogleBard for internal consistency as well as LLM-hum
&lt;/p&gt;</description></item><item><title>Res-VMamba&#21033;&#29992;&#20855;&#26377;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#28145;&#24230;&#27531;&#24046;&#23398;&#20064;&#65292;&#25552;&#20379;&#20102;&#27604;Transformer&#32467;&#26500;&#26356;&#20986;&#33394;&#30340;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#26159;&#39135;&#21697;&#32454;&#31890;&#24230;&#20998;&#31867;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.15761</link><description>&lt;p&gt;
&#20351;&#29992;&#20855;&#26377;&#28145;&#24230;&#27531;&#24046;&#23398;&#20064;&#30340;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;&#32454;&#31890;&#24230;&#39135;&#21697;&#31867;&#21035;&#35270;&#35273;&#20998;&#31867;&#30340;Res-VMamba
&lt;/p&gt;
&lt;p&gt;
Res-VMamba: Fine-Grained Food Category Visual Classification Using Selective State Space Models with Deep Residual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15761
&lt;/p&gt;
&lt;p&gt;
Res-VMamba&#21033;&#29992;&#20855;&#26377;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#28145;&#24230;&#27531;&#24046;&#23398;&#20064;&#65292;&#25552;&#20379;&#20102;&#27604;Transformer&#32467;&#26500;&#26356;&#20986;&#33394;&#30340;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#26159;&#39135;&#21697;&#32454;&#31890;&#24230;&#20998;&#31867;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39135;&#21697;&#20998;&#31867;&#26159;&#21457;&#23637;&#39135;&#21697;&#35270;&#35273;&#20219;&#21153;&#30340;&#22522;&#30784;&#65292;&#24182;&#22312;&#35745;&#31639;&#33829;&#20859;&#23398;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#30001;&#20110;&#39135;&#29289;&#30340;&#22797;&#26434;&#24615;&#38656;&#35201;&#32454;&#31890;&#24230;&#20998;&#31867;&#65292;&#26368;&#36817;&#30340;&#23398;&#26415;&#30740;&#31350;&#20027;&#35201;&#20462;&#25913;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#21644;/&#25110;&#35270;&#35273;&#21464;&#21387;&#22120;(ViTs)&#26469;&#25191;&#34892;&#39135;&#21697;&#31867;&#21035;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#23398;&#20064;&#32454;&#31890;&#24230;&#29305;&#24449;&#65292;CNN&#39592;&#24178;&#38656;&#35201;&#39069;&#22806;&#30340;&#32467;&#26500;&#35774;&#35745;&#65292;&#32780;&#21253;&#21547;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#30340;ViT&#20855;&#26377;&#26356;&#39640;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#26368;&#36817;&#25512;&#20986;&#30340;&#26032;&#30340;&#24207;&#21015;&#29366;&#24577;&#31354;&#38388;(S4)&#27169;&#22411;&#65292;&#36890;&#36807;&#36873;&#25321;&#26426;&#21046;&#21644;&#19982;&#25195;&#25551;(S6)&#30340;&#35745;&#31639;&#65292;&#20439;&#31216;&#20026;Mamba&#65292;&#30456;&#36739;&#20110;&#21464;&#21387;&#22120;&#26550;&#26500;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#23558;Mamba&#26426;&#21046;&#25972;&#21512;&#21040;&#22270;&#20687;&#20219;&#21153;(&#22914;&#20998;&#31867;)&#20013;&#30340;VMamba&#27169;&#22411;&#30446;&#21069;&#24314;&#31435;&#20102;&#26368;&#20808;&#36827;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15761v1 Announce Type: cross  Abstract: Food classification is the foundation for developing food vision tasks and plays a key role in the burgeoning field of computational nutrition. Due to the complexity of food requiring fine-grained classification, recent academic research mainly modifies Convolutional Neural Networks (CNNs) and/or Vision Transformers (ViTs) to perform food category classification. However, to learn fine-grained features, the CNN backbone needs additional structural design, whereas ViT, containing the self-attention module, has increased computational complexity. In recent months, a new Sequence State Space (S4) model, through a Selection mechanism and computation with a Scan (S6), colloquially termed Mamba, has demonstrated superior performance and computation efficiency compared to the Transformer architecture. The VMamba model, which incorporates the Mamba mechanism into image tasks (such as classification), currently establishes the state-of-the-art 
&lt;/p&gt;</description></item><item><title>This paper introduces a novel approach called Membership Inference Test (MINT) to empirically assess if specific data was used during the training of AI models. Two MINT architectures based on MLP and CNN are proposed and evaluated on a challenging face recognition task, achieving promising results with up to 90% accuracy.</title><link>https://arxiv.org/abs/2402.09225</link><description>&lt;p&gt;
&#25105;&#30340;&#25968;&#25454;&#22312;&#20320;&#30340;AI&#27169;&#22411;&#20013;&#21527;&#65311;&#36890;&#36807;&#24212;&#29992;&#20110;&#20154;&#33080;&#22270;&#20687;&#30340;&#25104;&#21592;&#25512;&#26029;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Is my Data in your AI Model? Membership Inference Test with Application to Face Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09225
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel approach called Membership Inference Test (MINT) to empirically assess if specific data was used during the training of AI models. Two MINT architectures based on MLP and CNN are proposed and evaluated on a challenging face recognition task, achieving promising results with up to 90% accuracy.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#25104;&#21592;&#25512;&#26029;&#27979;&#35797;&#65288;MINT&#65289;&#65292;&#19968;&#31181;&#29992;&#20110;&#32463;&#39564;&#24615;&#35780;&#20272;&#29305;&#23450;&#25968;&#25454;&#26159;&#21542;&#34987;&#29992;&#20110;&#35757;&#32451;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;MINT&#26550;&#26500;&#65292;&#26088;&#22312;&#23398;&#20064;&#22312;&#32463;&#36807;&#23457;&#35745;&#30340;&#27169;&#22411;&#26292;&#38706;&#20110;&#20854;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#25968;&#25454;&#26102;&#20986;&#29616;&#30340;&#19981;&#21516;&#28608;&#27963;&#27169;&#24335;&#12290;&#31532;&#19968;&#20010;&#26550;&#26500;&#22522;&#20110;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#32593;&#32476;&#65292;&#31532;&#20108;&#20010;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;MINT&#26550;&#26500;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20154;&#33080;&#35782;&#21035;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#32771;&#34385;&#20102;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#20154;&#33080;&#35782;&#21035;&#27169;&#22411;&#12290;&#20351;&#29992;&#20845;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#24211;&#36827;&#34892;&#23454;&#39564;&#65292;&#24635;&#20849;&#21253;&#21547;&#36229;&#36807;2200&#19975;&#24352;&#20154;&#33080;&#22270;&#20687;&#12290;&#26681;&#25454;&#21487;&#29992;&#30340;AI&#27169;&#22411;&#27979;&#35797;&#30340;&#19978;&#19979;&#25991;&#65292;&#32771;&#34385;&#20102;&#19981;&#21516;&#30340;&#23454;&#39564;&#22330;&#26223;&#12290;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#36798;&#21040;&#20102;90%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09225v1 Announce Type: cross Abstract: This paper introduces the Membership Inference Test (MINT), a novel approach that aims to empirically assess if specific data was used during the training of Artificial Intelligence (AI) models. Specifically, we propose two novel MINT architectures designed to learn the distinct activation patterns that emerge when an audited model is exposed to data used during its training process. The first architecture is based on a Multilayer Perceptron (MLP) network and the second one is based on Convolutional Neural Networks (CNNs). The proposed MINT architectures are evaluated on a challenging face recognition task, considering three state-of-the-art face recognition models. Experiments are carried out using six publicly available databases, comprising over 22 million face images in total. Also, different experimental scenarios are considered depending on the context available of the AI model to test. Promising results, up to 90% accuracy, are a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#36741;&#21161;&#30340;&#33258;&#21160;&#35843;&#33410;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#39640;&#24230;&#21487;&#37197;&#32622;&#21442;&#25968;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#25216;&#26415;&#36890;&#36807;&#35299;&#20915;&#36719;&#30828;&#20214;&#20043;&#38388;&#37197;&#32622;&#36873;&#39033;&#30340;&#20132;&#20114;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#29615;&#22659;&#21644;&#26426;&#22120;&#20154;&#24179;&#21488;&#20043;&#38388;&#30340;&#24615;&#33021;&#36801;&#31227;&#12290;</title><link>https://arxiv.org/abs/2402.05399</link><description>&lt;p&gt;
CURE: &#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#27169;&#25311;&#36741;&#21161;&#33258;&#21160;&#35843;&#33410;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
CURE: Simulation-Augmented Auto-Tuning in Robotics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#36741;&#21161;&#30340;&#33258;&#21160;&#35843;&#33410;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#39640;&#24230;&#21487;&#37197;&#32622;&#21442;&#25968;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#25216;&#26415;&#36890;&#36807;&#35299;&#20915;&#36719;&#30828;&#20214;&#20043;&#38388;&#37197;&#32622;&#36873;&#39033;&#30340;&#20132;&#20114;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#29615;&#22659;&#21644;&#26426;&#22120;&#20154;&#24179;&#21488;&#20043;&#38388;&#30340;&#24615;&#33021;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#31995;&#32479;&#36890;&#24120;&#30001;&#22810;&#20010;&#23376;&#31995;&#32479;&#32452;&#25104;&#65292;&#20363;&#22914;&#23450;&#20301;&#21644;&#23548;&#33322;&#65292;&#27599;&#20010;&#23376;&#31995;&#32479;&#21448;&#21253;&#21547;&#35768;&#22810;&#21487;&#37197;&#32622;&#30340;&#32452;&#20214;&#65288;&#20363;&#22914;&#36873;&#25321;&#19981;&#21516;&#30340;&#35268;&#21010;&#31639;&#27861;&#65289;&#12290;&#19968;&#26086;&#36873;&#25321;&#20102;&#26576;&#20010;&#31639;&#27861;&#65292;&#23601;&#38656;&#35201;&#35774;&#32622;&#30456;&#20851;&#30340;&#37197;&#32622;&#36873;&#39033;&#20197;&#36798;&#21040;&#36866;&#24403;&#30340;&#20540;&#12290;&#31995;&#32479;&#22534;&#26632;&#20013;&#30340;&#37197;&#32622;&#36873;&#39033;&#20250;&#20135;&#29983;&#22797;&#26434;&#30340;&#20132;&#20114;&#20851;&#31995;&#12290;&#22312;&#39640;&#24230;&#21487;&#37197;&#32622;&#30340;&#26426;&#22120;&#20154;&#20013;&#25214;&#21040;&#26368;&#20339;&#37197;&#32622;&#26469;&#23454;&#29616;&#26399;&#26395;&#30340;&#24615;&#33021;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#36719;&#20214;&#21644;&#30828;&#20214;&#20043;&#38388;&#30340;&#37197;&#32622;&#36873;&#39033;&#20132;&#20114;&#23548;&#33268;&#20102;&#24222;&#22823;&#19988;&#22797;&#26434;&#30340;&#37197;&#32622;&#31354;&#38388;&#12290;&#24615;&#33021;&#36801;&#31227;&#22312;&#19981;&#21516;&#30340;&#29615;&#22659;&#21644;&#26426;&#22120;&#20154;&#24179;&#21488;&#20043;&#38388;&#20063;&#26159;&#19968;&#20010;&#38590;&#39064;&#12290;&#25968;&#25454;&#39640;&#25928;&#20248;&#21270;&#31639;&#27861;&#65288;&#20363;&#22914;&#36125;&#21494;&#26031;&#20248;&#21270;&#65289;&#24050;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#33258;&#21160;&#21270;&#35843;&#25972;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#20013;&#30340;&#21487;&#37197;&#32622;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#20248;&#21270;&#31639;&#27861;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#24212;&#29992;&#20173;&#26377;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic systems are typically composed of various subsystems, such as localization and navigation, each encompassing numerous configurable components (e.g., selecting different planning algorithms). Once an algorithm has been selected for a component, its associated configuration options must be set to the appropriate values. Configuration options across the system stack interact non-trivially. Finding optimal configurations for highly configurable robots to achieve desired performance poses a significant challenge due to the interactions between configuration options across software and hardware that result in an exponentially large and complex configuration space. These challenges are further compounded by the need for transferability between different environments and robotic platforms. Data efficient optimization algorithms (e.g., Bayesian optimization) have been increasingly employed to automate the tuning of configurable parameters in cyber-physical systems. However, such optimiz
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;Syntheseus&#24314;&#31435;&#30340;&#22522;&#20934;&#24211;&#37325;&#26032;&#35780;&#20272;&#20102;&#22238;&#28335;&#21512;&#25104;&#31639;&#27861;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#25216;&#26415;&#27169;&#22411;&#30340;&#31995;&#32479;&#24615;&#32570;&#38519;&#24182;&#25552;&#20379;&#20102;&#23545;&#26410;&#26469;&#24037;&#20316;&#30340;&#25351;&#23548;&#24314;&#35758;&#12290;</title><link>https://arxiv.org/abs/2310.19796</link><description>&lt;p&gt;
&#20351;&#29992;Syntheseus&#37325;&#26032;&#35780;&#20272;&#22238;&#28335;&#21512;&#25104;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Re-evaluating Retrosynthesis Algorithms with Syntheseus
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.19796
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Syntheseus&#24314;&#31435;&#30340;&#22522;&#20934;&#24211;&#37325;&#26032;&#35780;&#20272;&#20102;&#22238;&#28335;&#21512;&#25104;&#31639;&#27861;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#25216;&#26415;&#27169;&#22411;&#30340;&#31995;&#32479;&#24615;&#32570;&#38519;&#24182;&#25552;&#20379;&#20102;&#23545;&#26410;&#26469;&#24037;&#20316;&#30340;&#25351;&#23548;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#65292;&#20998;&#23376;&#21512;&#25104;&#35268;&#21010;&#65292;&#20063;&#31216;&#20026;&#22238;&#28335;&#21512;&#25104;&#65292;&#24050;&#32463;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#21644;&#21270;&#23398;&#30028;&#20851;&#27880;&#30340;&#28966;&#28857;&#12290;&#23613;&#31649;&#30475;&#20284;&#21462;&#24471;&#20102;&#31283;&#23450;&#30340;&#36827;&#23637;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#23384;&#22312;&#19981;&#23436;&#21892;&#30340;&#22522;&#20934;&#21644;&#19981;&#19968;&#33268;&#30340;&#27604;&#36739;&#25513;&#30422;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#31995;&#32479;&#24615;&#32570;&#38519;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;syntheseus&#30340;&#22522;&#20934;&#24211;&#65292;&#36890;&#36807;&#40664;&#35748;&#25512;&#24191;&#26368;&#20339;&#23454;&#36341;&#65292;&#23454;&#29616;&#20102;&#23545;&#21333;&#27493;&#21644;&#22810;&#27493;&#22238;&#28335;&#21512;&#25104;&#31639;&#27861;&#30340;&#19968;&#33268;&#32780;&#26377;&#24847;&#20041;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#20351;&#29992;syntheseus&#37325;&#26032;&#35780;&#20272;&#20102;&#33509;&#24178;&#20808;&#21069;&#30340;&#22238;&#28335;&#21512;&#25104;&#31639;&#27861;&#65292;&#24182;&#21457;&#29616;&#22312;&#20180;&#32454;&#35780;&#20272;&#26102;&#65292;&#29616;&#26377;&#25216;&#26415;&#27169;&#22411;&#30340;&#25490;&#21517;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#36825;&#19968;&#39046;&#22495;&#26410;&#26469;&#24037;&#20316;&#30340;&#25351;&#23548;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.19796v2 Announce Type: replace-cross  Abstract: The planning of how to synthesize molecules, also known as retrosynthesis, has been a growing focus of the machine learning and chemistry communities in recent years. Despite the appearance of steady progress, we argue that imperfect benchmarks and inconsistent comparisons mask systematic shortcomings of existing techniques. To remedy this, we present a benchmarking library called syntheseus which promotes best practice by default, enabling consistent meaningful evaluation of single-step and multi-step retrosynthesis algorithms. We use syntheseus to re-evaluate a number of previous retrosynthesis algorithms, and find that the ranking of state-of-the-art models changes when evaluated carefully. We end with guidance for future works in this area.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#22312;&#22522;&#30784;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#20197;&#20248;&#21270;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;&#35813;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;&#24403;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;FL&#26694;&#26550;&#65292;&#24182;&#23637;&#26395;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.04472</link><description>&lt;p&gt;
&#20851;&#20110;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#22312;&#22522;&#30784;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Efficient Federated Learning Methods for Foundation Model Training. (arXiv:2401.04472v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04472
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#22312;&#22522;&#30784;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#20197;&#20248;&#21270;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;&#35813;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;&#24403;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;FL&#26694;&#26550;&#65292;&#24182;&#23637;&#26395;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#20419;&#36827;&#38544;&#31169;&#20445;&#25252;&#21327;&#20316;&#35757;&#32451;&#30340;&#25104;&#29087;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#26032;&#30340;FL&#26041;&#27861;&#36890;&#24120;&#21482;&#28041;&#21450;&#23567;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36129;&#29486;&#12290;&#38543;&#30528;Transformer&#27169;&#22411;&#30340;&#24040;&#22823;&#25104;&#21151;&#65292;&#19968;&#20010;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#22914;&#20309;&#20351;&#22522;&#30784;&#27169;&#22411;&#22312;FL&#24212;&#29992;&#20013;&#23454;&#26045;&#36215;&#26469;&#65311;&#37492;&#20110;&#22312;FL&#20013;&#35745;&#31639;&#21644;&#36890;&#20449;&#30340;&#26102;&#38388;&#28040;&#32791;&#36890;&#24120;&#30456;&#20284;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20851;&#20110;&#22312;FL&#24212;&#29992;&#20013;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#26041;&#27861;&#30340;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#26088;&#22312;&#20248;&#21270;&#35757;&#32451;&#26102;&#38388;&#24182;&#20943;&#23569;&#23458;&#25143;&#31471;&#19982;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#36890;&#20449;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#30446;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;FL&#26694;&#26550;&#65292;&#24182;&#26681;&#25454;FL&#30740;&#31350;&#21450;&#20854;&#24310;&#20280;&#30340;&#29616;&#26377;&#26041;&#27861;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has become an established technique to facilitate privacy-preserving collaborative training. However, new approaches to FL often discuss their contributions involving small deep-learning models only. With the tremendous success of transformer models, the following question arises: What is necessary to operationalize foundation models in an FL application? Knowing that computation and communication often take up similar amounts of time in FL, we introduce a novel taxonomy focused on computational and communication efficiency methods in FL applications. This said, these methods aim to optimize the training time and reduce communication between clients and the server. We also look at the current state of widely used FL frameworks and discuss future research potentials based on existing approaches in FL research and beyond.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EgoPoser&#65292;&#19968;&#31181;&#33021;&#22815;&#22312;&#22823;&#22330;&#26223;&#20013;&#40065;&#26834;&#22320;&#23454;&#26102;&#20272;&#35745;&#33258;&#25105;&#36523;&#20307;&#23039;&#21183;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#37325;&#26032;&#24605;&#32771;&#36755;&#20837;&#34920;&#31034;&#12289;&#24341;&#20837;&#26032;&#30340;&#36816;&#21160;&#20998;&#35299;&#26041;&#27861;&#20197;&#21450;&#24314;&#27169;&#36523;&#20307;&#23039;&#21183;&#65292;EgoPoser&#22312;&#23450;&#24615;&#21644;&#23450;&#37327;&#19978;&#22343;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.06493</link><description>&lt;p&gt;
EgoPoser&#65306;&#22823;&#22330;&#26223;&#19979;&#40065;&#26834;&#30340;&#23454;&#26102;&#33258;&#25105;&#36523;&#20307;&#23039;&#21183;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
EgoPoser: Robust Real-Time Ego-Body Pose Estimation in Large Scenes. (arXiv:2308.06493v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EgoPoser&#65292;&#19968;&#31181;&#33021;&#22815;&#22312;&#22823;&#22330;&#26223;&#20013;&#40065;&#26834;&#22320;&#23454;&#26102;&#20272;&#35745;&#33258;&#25105;&#36523;&#20307;&#23039;&#21183;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#37325;&#26032;&#24605;&#32771;&#36755;&#20837;&#34920;&#31034;&#12289;&#24341;&#20837;&#26032;&#30340;&#36816;&#21160;&#20998;&#35299;&#26041;&#27861;&#20197;&#21450;&#24314;&#27169;&#36523;&#20307;&#23039;&#21183;&#65292;EgoPoser&#22312;&#23450;&#24615;&#21644;&#23450;&#37327;&#19978;&#22343;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22836;&#37096;&#21644;&#25163;&#37096;&#23039;&#21183;&#20165;&#36890;&#36807;&#23436;&#25972;&#36523;&#20307;&#33258;&#25105;&#23039;&#21183;&#20272;&#35745;&#24050;&#25104;&#20026;&#30740;&#31350;&#30340;&#19968;&#20010;&#28909;&#28857;&#39046;&#22495;&#65292;&#20197;&#20026;&#22836;&#25140;&#24335;&#24179;&#21488;&#19978;&#30340;&#34394;&#25311;&#35282;&#33394;&#34920;&#36798;&#25552;&#20379;&#21160;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36807;&#20110;&#20381;&#36182;&#25968;&#25454;&#38598;&#35760;&#24405;&#26102;&#30340;&#36816;&#21160;&#25429;&#25417;&#31354;&#38388;&#30340;&#38480;&#21046;&#65292;&#21516;&#26102;&#20551;&#35774;&#36830;&#32493;&#25429;&#25417;&#20851;&#33410;&#36816;&#21160;&#21644;&#22343;&#21248;&#36523;&#20307;&#23610;&#23544;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EgoPoser&#65292;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#20811;&#26381;&#20102;&#36825;&#20123;&#38480;&#21046;&#65306;1&#65289;&#37325;&#26032;&#24605;&#32771;&#22522;&#20110;&#22836;&#25140;&#24335;&#24179;&#21488;&#30340;&#33258;&#25105;&#23039;&#21183;&#20272;&#35745;&#30340;&#36755;&#20837;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#36816;&#21160;&#20998;&#35299;&#26041;&#27861;&#26469;&#39044;&#27979;&#19982;&#20840;&#23616;&#20301;&#32622;&#26080;&#20851;&#30340;&#23436;&#25972;&#36523;&#20307;&#23039;&#21183;&#65292;2&#65289;&#20174;&#22836;&#25140;&#24335;&#35774;&#22791;&#35270;&#37326;&#20869;&#30340;&#38388;&#27463;&#24615;&#25163;&#37096;&#23039;&#21183;&#36319;&#36394;&#20013;&#40065;&#26834;&#22320;&#24314;&#27169;&#36523;&#20307;&#23039;&#21183;&#65292;3&#65289;&#38024;&#23545;&#19981;&#21516;&#29992;&#25143;&#30340;&#21508;&#31181;&#36523;&#20307;&#23610;&#23544;&#36827;&#34892;&#36890;&#29992;&#21270;&#25512;&#24191;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;EgoPoser&#22312;&#23450;&#24615;&#21644;&#23450;&#37327;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#24182;&#20445;&#25345;&#36739;&#39640;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Full-body ego-pose estimation from head and hand poses alone has become an active area of research to power articulate avatar representation on headset-based platforms. However, existing methods over-rely on the confines of the motion-capture spaces in which datasets were recorded, while simultaneously assuming continuous capture of joint motions and uniform body dimensions. In this paper, we propose EgoPoser, which overcomes these limitations by 1) rethinking the input representation for headset-based ego-pose estimation and introducing a novel motion decomposition method that predicts full-body pose independent of global positions, 2) robustly modeling body pose from intermittent hand position and orientation tracking only when inside a headset's field of view, and 3) generalizing across various body sizes for different users. Our experiments show that EgoPoser outperforms state-of-the-art methods both qualitatively and quantitatively, while maintaining a high inference speed of over
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#33041;&#32593;&#32476;&#30340;&#23545;&#27604;&#22270;&#27744;&#21270;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#23545;&#33041;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#20998;&#31867;&#12290;&#36890;&#36807;&#23450;&#21046;&#21270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#29305;&#27530;&#35774;&#35745;&#30340;&#21487;&#35299;&#37322;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#22312;5&#20010;&#38745;&#24687;&#24577;fMRI&#33041;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;&#22522;&#20934;&#32447;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.11133</link><description>&lt;p&gt;
&#23545;&#33041;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#20998;&#31867;&#36827;&#34892;&#23545;&#27604;&#22270;&#27744;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Graph Pooling for Explainable Classification of Brain Networks. (arXiv:2307.11133v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#33041;&#32593;&#32476;&#30340;&#23545;&#27604;&#22270;&#27744;&#21270;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#23545;&#33041;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#20998;&#31867;&#12290;&#36890;&#36807;&#23450;&#21046;&#21270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#29305;&#27530;&#35774;&#35745;&#30340;&#21487;&#35299;&#37322;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#22312;5&#20010;&#38745;&#24687;&#24577;fMRI&#33041;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;&#22522;&#20934;&#32447;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;(fMRI)&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#27979;&#37327;&#31070;&#32463;&#27963;&#21160;&#30340;&#25216;&#26415;&#12290;&#20854;&#24212;&#29992;&#22312;&#35782;&#21035;&#24085;&#37329;&#26862;&#30149;&#12289;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21644;&#33258;&#38381;&#30151;&#31561;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#26041;&#38754;&#23588;&#20026;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;fMRI&#25968;&#25454;&#20998;&#26512;&#23558;&#22823;&#33041;&#24314;&#27169;&#20026;&#22270;&#65292;&#24182;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#25552;&#21462;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;fMRI&#25968;&#25454;&#30340;&#29420;&#29305;&#29305;&#24449;&#35201;&#27714;&#23545;GNN&#36827;&#34892;&#29305;&#27530;&#35774;&#35745;&#12290;&#23450;&#21046;GNN&#20197;&#29983;&#25104;&#26377;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#21452;&#27880;&#24847;&#22359;&#21644;&#21487;&#24494;&#20998;&#22270;&#27744;&#21270;&#26041;&#27861;ContrastPool&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;GNN&#20998;&#26512;&#33041;&#32593;&#32476;&#65292;&#28385;&#36275;fMRI&#30340;&#29305;&#27530;&#35201;&#27714;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;5&#20010;&#38745;&#24687;&#24577;fMRI&#33041;&#32593;&#32476;&#25968;&#25454;&#38598;&#30340;3&#31181;&#30142;&#30149;&#65292;&#24182;&#35777;&#26126;&#20854;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#32447;&#12290;&#25105;&#20204;&#30340;&#26696;&#20363;&#30740;&#31350;&#35777;&#23454;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#21462;&#30340;&#27169;&#24335;&#19982;&#31070;&#32463;&#31185;&#23398;&#25991;&#29486;&#20013;&#30340;&#39046;&#22495;&#30693;&#35782;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Functional magnetic resonance imaging (fMRI) is a commonly used technique to measure neural activation. Its application has been particularly important in identifying underlying neurodegenerative conditions such as Parkinson's, Alzheimer's, and Autism. Recent analysis of fMRI data models the brain as a graph and extracts features by graph neural networks (GNNs). However, the unique characteristics of fMRI data require a special design of GNN. Tailoring GNN to generate effective and domain-explainable features remains challenging. In this paper, we propose a contrastive dual-attention block and a differentiable graph pooling method called ContrastPool to better utilize GNN for brain networks, meeting fMRI-specific requirements. We apply our method to 5 resting-state fMRI brain network datasets of 3 diseases and demonstrate its superiority over state-of-the-art baselines. Our case study confirms that the patterns extracted by our method match the domain knowledge in neuroscience literatu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#29615;&#22659;&#19978;&#30340;&#35780;&#20272;&#65292;&#21457;&#29616;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#21040;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20294;&#22312;&#24378;&#21270;&#23398;&#20064;&#27979;&#35797;&#26102;&#34920;&#29616;&#21463;&#21040;&#26641;&#30340;&#31163;&#25955;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.13004</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#26159;&#21542;&#33021;&#22815;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;?
&lt;/p&gt;
&lt;p&gt;
Can Differentiable Decision Trees Learn Interpretable Reward Functions?. (arXiv:2306.13004v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#29615;&#22659;&#19978;&#30340;&#35780;&#20272;&#65292;&#21457;&#29616;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#21040;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20294;&#22312;&#24378;&#21270;&#23398;&#20064;&#27979;&#35797;&#26102;&#34920;&#29616;&#21463;&#21040;&#26641;&#30340;&#31163;&#25955;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20154;&#30340;&#24847;&#22270;&#21644;&#20559;&#22909;&#30340;&#22870;&#21169;&#20989;&#25968;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#20294;&#35768;&#22810;&#26694;&#26550;&#20351;&#29992;&#40657;&#30418;&#23398;&#20064;&#26041;&#27861;&#65292;&#38590;&#20197;&#35299;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#65288;DDT&#65289;&#20174;&#20559;&#22909;&#20013;&#23398;&#20064;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#36866;&#29992;&#20110;&#20302;&#32500;&#21644;&#39640;&#32500;&#29366;&#24577;&#36755;&#20837;&#12290;&#25105;&#20204;&#22312;Cartpole&#12289;&#35270;&#35273;&#32593;&#26684;&#19990;&#30028;&#29615;&#22659;&#21644;Atari&#28216;&#25103;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;DDT&#23398;&#20064;&#21487;&#35299;&#37322;&#22870;&#21169;&#20989;&#25968;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#26641;&#24418;&#32467;&#26500;&#26377;&#21161;&#20110;&#30830;&#23450;&#22870;&#21169;&#20989;&#25968;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#19968;&#33268;&#31243;&#24230;&#12290;&#25105;&#20204;&#21487;&#35270;&#21270;&#20102;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;DDT&#65292;&#21457;&#29616;&#23427;&#20204;&#33021;&#22815;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20294;&#26641;&#30340;&#31163;&#25955;&#24615;&#20250;&#24433;&#21709;&#24378;&#21270;&#23398;&#20064;&#22312;&#27979;&#35797;&#26102;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is an increasing interest in learning reward functions that model human intent and human preferences. However, many frameworks use blackbox learning methods that, while expressive, are difficult to interpret. We propose and evaluate a novel approach for learning expressive and interpretable reward functions from preferences using Differentiable Decision Trees (DDTs) for both low- and high-dimensional state inputs. We explore and discuss the viability of learning interpretable reward functions using DDTs by evaluating our algorithm on Cartpole, Visual Gridworld environments, and Atari games. We provide evidence that that the tree structure of our learned reward function is useful in determining the extent to which a reward function is aligned with human preferences. We visualize the learned reward DDTs and find that they are capable of learning interpretable reward functions but that the discrete nature of the trees hurts the performance of reinforcement learning at test time. How
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#22343;&#22330;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#22270;&#27880;&#24847;&#21147;&#26469;&#25429;&#33719;&#21608;&#22260;&#37051;&#23621;&#26234;&#33021;&#20307;&#30340;&#29305;&#24449;&#20449;&#24687;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#35268;&#27169;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#37096;&#20998;&#21487;&#35266;&#23519;MARL&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.12653</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#22343;&#22330;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Partially Observable Mean Field Multi-Agent Reinforcement Learning Based on Graph-Attention. (arXiv:2304.12653v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#22343;&#22330;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#22270;&#27880;&#24847;&#21147;&#26469;&#25429;&#33719;&#21608;&#22260;&#37051;&#23621;&#26234;&#33021;&#20307;&#30340;&#29305;&#24449;&#20449;&#24687;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#35268;&#27169;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#37096;&#20998;&#21487;&#35266;&#23519;MARL&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#38590;&#20197;&#22312;&#22823;&#35268;&#27169;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#24212;&#29992;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#22343;&#22330;&#29702;&#35770;&#25552;&#39640;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#26412;&#25991;&#32771;&#34385;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65292;&#20854;&#20013;&#27599;&#20010;&#26234;&#33021;&#20307;&#21482;&#33021;&#35266;&#23519;&#21040;&#22266;&#23450;&#33539;&#22260;&#20869;&#30340;&#20854;&#20182;&#26234;&#33021;&#20307;&#12290;&#36825;&#31181;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#24433;&#21709;&#20102;&#26234;&#33021;&#20307;&#35780;&#20272;&#21608;&#22260;&#26234;&#33021;&#20307;&#34892;&#21160;&#36136;&#37327;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#24320;&#21457;&#19968;&#31181;&#20174;&#23616;&#37096;&#35266;&#27979;&#20013;&#33719;&#21462;&#26356;&#26377;&#25928;&#20449;&#24687;&#20197;&#36873;&#25321;&#26356;&#26377;&#25928;&#34892;&#21160;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#20197;&#21069;&#24037;&#20316;&#20351;&#29992;&#27010;&#29575;&#20998;&#24067;&#25110;&#21152;&#26435;&#22343;&#22330;&#26469;&#26356;&#26032;&#37051;&#23621;&#26234;&#33021;&#20307;&#24179;&#22343;&#34892;&#21160;&#65292;&#20294;&#23427;&#27809;&#26377;&#20805;&#20998;&#32771;&#34385;&#21608;&#22260;&#37051;&#23621;&#30340;&#29305;&#24449;&#20449;&#24687;&#65292;&#23548;&#33268;&#20102;&#23616;&#37096;&#26368;&#20248;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#22343;&#22330;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65292;&#23427;&#20351;&#29992;&#22270;&#27880;&#24847;&#21147;&#26469;&#25429;&#33719;&#21608;&#22260;&#37051;&#23621;&#26234;&#33021;&#20307;&#30340;&#29305;&#24449;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#22823;&#35268;&#27169;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#37096;&#20998;&#21487;&#35266;&#23519;MARL&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional multi-agent reinforcement learning algorithms are difficultly applied in a large-scale multi-agent environment. The introduction of mean field theory has enhanced the scalability of multi-agent reinforcement learning in recent years. This paper considers partially observable multi-agent reinforcement learning (MARL), where each agent can only observe other agents within a fixed range. This partial observability affects the agent's ability to assess the quality of the actions of surrounding agents. This paper focuses on developing a method to capture more effective information from local observations in order to select more effective actions. Previous work in this field employs probability distributions or weighted mean field to update the average actions of neighborhood agents, but it does not fully consider the feature information of surrounding neighbors and leads to a local optimum. In this paper, we propose a novel multi-agent reinforcement learning algorithm, Partially
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#32452;&#26032;&#30340;&#26102;&#38388;&#36923;&#36753;&#65292;&#36890;&#36807;&#20351;&#29992;Krohn&#21644;Rhodes&#30340;&#32423;&#32852;&#29702;&#35770;&#65292;&#25193;&#23637;&#20102;&#36807;&#21435;&#30340;LTL&#34920;&#36798;&#33021;&#21147;&#65292;&#20854;&#20013;&#21253;&#25324;&#21487;&#20197;&#25429;&#33719;&#20854;&#20182;prime automata&#30340;&#26032;&#30340;&#26102;&#38388;&#36816;&#31639;&#31526;&#12290;</title><link>http://arxiv.org/abs/2304.09639</link><description>&lt;p&gt;
Krohn-Rhodes&#36923;&#36753;
&lt;/p&gt;
&lt;p&gt;
The Krohn-Rhodes Logics. (arXiv:2304.09639v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#32452;&#26032;&#30340;&#26102;&#38388;&#36923;&#36753;&#65292;&#36890;&#36807;&#20351;&#29992;Krohn&#21644;Rhodes&#30340;&#32423;&#32852;&#29702;&#35770;&#65292;&#25193;&#23637;&#20102;&#36807;&#21435;&#30340;LTL&#34920;&#36798;&#33021;&#21147;&#65292;&#20854;&#20013;&#21253;&#25324;&#21487;&#20197;&#25429;&#33719;&#20854;&#20182;prime automata&#30340;&#26032;&#30340;&#26102;&#38388;&#36816;&#31639;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#26032;&#30340;&#36807;&#21435;&#30340;&#27169;&#24577;&#26102;&#38388;&#36923;&#36753;&#65292;&#36890;&#36807;&#20351;&#29992;Krohn&#21644;Rhodes&#30340;&#33258;&#21160;&#26426;&#32423;&#32852;&#29702;&#35770;&#65292;&#22522;&#20110;Past LTL&#25193;&#23637;&#19968;&#32452;&#20016;&#23500;&#30340;&#26102;&#38388;&#36816;&#31639;&#31526;&#32780;&#33719;&#24471;&#12290;&#35813;&#29702;&#35770;&#25351;&#20986;&#65292;&#27599;&#20010;&#33258;&#21160;&#26426;&#37117;&#21487;&#20197;&#34920;&#31034;&#20026;&#19968;&#20123;&#31216;&#20026;prime automata&#30340;&#22522;&#26412;&#33258;&#21160;&#26426;&#30340;&#32423;&#32852;&#12290;&#20182;&#20204;&#26159;&#25152;&#26377;&#33258;&#21160;&#26426;&#30340;&#26500;&#24314;&#22359;&#65292;&#31867;&#20284;&#20110;&#36136;&#25968;&#26159;&#25152;&#26377;&#33258;&#28982;&#25968;&#30340;&#26500;&#24314;&#22359;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36807;&#21435;&#30340;LTL&#23545;&#24212;&#20110;&#31216;&#20026;flip-flops&#30340;&#19968;&#31181;prime automata&#30340;&#32423;&#32852;&#12290;&#29305;&#21035;&#22320;&#65292;Past LTL&#30340;&#26102;&#38388;&#36816;&#31639;&#31526;&#30001;flip-flops&#25429;&#33719;&#65292;&#24182;&#19988;&#23427;&#20204;&#19981;&#33021;&#25429;&#33719;&#20219;&#20309;&#20854;&#20182;prime automata&#65292;&#23558;&#34920;&#36798;&#33021;&#21147;&#38480;&#21046;&#22312;&#26143;&#21495;&#33258;&#30001;&#27491;&#21017;&#35821;&#35328;&#20869;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#26102;&#38388;&#36816;&#31639;&#31526;&#65292;&#21487;&#20197;&#25429;&#33719;&#20854;&#20182;prime automata&#65292;&#20174;&#32780;&#25193;&#23637;&#20102;Past LTL&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#36825;&#20123;&#36816;&#31639;&#31526;&#26159;&#26080;&#31351;&#22810;&#30340;&#65292;&#24182;&#19988;&#23427;&#20204;&#20135;&#29983;&#20102;&#26080;&#38480;&#25968;&#37327;&#30340;&#36923;&#36753;&#65292;&#25429;&#33719;&#20102;&#27491;&#21017;&#35821;&#35328;&#30340;&#26080;&#38480;&#25968;&#37327;&#30340;&#19981;&#21516;&#29255;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new family of modal temporal logics of the past, obtained by extending Past LTL with a rich set of temporal operators based on the theory by Krohn and Rhodes for automata cascades. The theory says that every automaton can be expressed as a cascade of some basic automata called prime automata. They are the building blocks of all automata, analogously to prime numbers being the building blocks of all natural numbers. We show that Past LTL corresponds to cascades of one kind of prime automata called flip-flops. In particular, the temporal operators of Past LTL are captured by flip-flops, and they cannot capture any other prime automaton, confining the expressivity within the star-free regular languages. We propose novel temporal operators that can capture other prime automata, and hence extend the expressivity of Past LTL. Such operators are infinitely-many, and they yield an infinite number of logics capturing an infinite number of distinct fragments of the regular languages
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#21407;&#21017;&#20043;&#38388;&#30340;&#32039;&#24352;&#20851;&#31995;&#21644;&#26435;&#34913;&#65292;&#24182;&#25552;&#20986;&#19968;&#20010;&#30446;&#24405;&#20197;&#24110;&#21161;&#20154;&#20204;&#25552;&#39640;&#23545;&#30456;&#20114;&#20316;&#29992;&#30340;&#35748;&#35782;&#12290;</title><link>http://arxiv.org/abs/2304.08275</link><description>&lt;p&gt;
&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#30340;&#23454;&#26045;&#65306;&#20262;&#29702;&#26041;&#38754;&#30340;&#32039;&#24352;&#21644;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Implementing Responsible AI: Tensions and Trade-Offs Between Ethics Aspects. (arXiv:2304.08275v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#21407;&#21017;&#20043;&#38388;&#30340;&#32039;&#24352;&#20851;&#31995;&#21644;&#26435;&#34913;&#65292;&#24182;&#25552;&#20986;&#19968;&#20010;&#30446;&#24405;&#20197;&#24110;&#21161;&#20154;&#20204;&#25552;&#39640;&#23545;&#30456;&#20114;&#20316;&#29992;&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#28389;&#29992;&#21644;&#19981;&#24403;&#20351;&#29992;&#24341;&#36215;&#30340;&#25285;&#24551;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#30340;&#20262;&#29702;&#21407;&#21017;&#12290;&#36825;&#20123;&#20934;&#21017;&#30340;&#22522;&#26412;&#26041;&#38754;&#21253;&#25324;&#38544;&#31169;&#12289;&#20934;&#30830;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#31283;&#20581;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#38754;&#20043;&#38388;&#23384;&#22312;&#28508;&#22312;&#30340;&#32039;&#24352;&#20851;&#31995;&#65292;&#36825;&#32473;&#23547;&#27714;&#36981;&#24490;&#36825;&#20123;&#21407;&#21017;&#30340;AI/ML&#24320;&#21457;&#32773;&#24102;&#26469;&#20102;&#22256;&#38590;&#12290;&#20363;&#22914;&#65292;&#25552;&#39640;AI/ML&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#21487;&#33021;&#20250;&#38477;&#20302;&#20854;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#27719;&#32534;&#21644;&#35752;&#35770;10&#20010;&#31361;&#20986;&#30340;&#32039;&#24352;&#20851;&#31995;&#12289;&#26435;&#34913;&#21644;&#20854;&#20182;&#22522;&#26412;&#26041;&#38754;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#20415;&#22312;&#25345;&#32493;&#21162;&#21147;&#23558;&#36825;&#20123;&#21407;&#21017;&#36716;&#21270;&#20026;&#23454;&#36341;&#30340;&#36807;&#31243;&#20013;&#65292;&#25552;&#39640;&#23545;&#21487;&#33021;&#20986;&#29616;&#30340;&#20262;&#29702;&#21407;&#21017;&#26041;&#38754;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#35748;&#35782;&#65292;&#24182;&#36890;&#36807;&#22312;&#24191;&#27867;&#25991;&#29486;&#20013;&#30340;&#25903;&#25345;&#36827;&#34892;&#21452;&#38754;&#20114;&#21160;&#30340;&#37325;&#28857;&#35752;&#35770;&#12290;&#36825;&#20010;&#30446;&#24405;&#23545;&#20110;&#25552;&#39640;&#20154;&#20204;&#23545;&#20262;&#29702;&#20934;&#21017;&#26041;&#38754;&#20043;&#38388;&#21487;&#33021;&#30456;&#20114;&#20316;&#29992;&#30340;&#35748;&#35782;&#20197;&#21450;&#20419;&#36827;&#35774;&#35745;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#20570;&#20986;&#26377;&#20805;&#20998;&#20381;&#25454;&#30340;&#21028;&#26029;&#21487;&#33021;&#26377;&#25152;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many sets of ethics principles for responsible AI have been proposed to allay concerns about misuse and abuse of AI/ML systems. The underlying aspects of such sets of principles include privacy, accuracy, fairness, robustness, explainability, and transparency. However, there are potential tensions between these aspects that pose difficulties for AI/ML developers seeking to follow these principles. For example, increasing the accuracy of an AI/ML system may reduce its explainability. As part of the ongoing effort to operationalise the principles into practice, in this work we compile and discuss a catalogue of 10 notable tensions, trade-offs and other interactions between the underlying aspects. We primarily focus on two-sided interactions, drawing on support spread across a diverse literature. This catalogue can be helpful in raising awareness of the possible interactions between aspects of ethics principles, as well as facilitating well-supported judgements by the designers and develo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#38598;&#25104;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65292;&#20351;&#29992;&#36229;&#22270;&#25429;&#25417;&#31471;&#21475;&#25195;&#25551;&#25915;&#20987;&#30340;&#28436;&#21270;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#27966;&#29983;&#30340;&#24230;&#37327;&#26469;&#35757;&#32451;NIDS&#65292;&#20174;&#32780;&#20801;&#35768;&#22312;&#39640;&#31934;&#24230;&#12289;&#39640;&#20934;&#30830;&#29575;&#12289;&#39640;&#21484;&#22238;&#29575;&#24615;&#33021;&#19979;&#23454;&#26102;&#30417;&#27979;&#21644;&#26816;&#27979;&#31471;&#21475;&#25195;&#25551;&#27963;&#21160;&#12289;&#20854;&#20182;&#31867;&#22411;&#30340;&#25915;&#20987;&#21644;&#25932;&#23545;&#20837;&#20405;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;NIDS&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2211.03933</link><description>&lt;p&gt;
&#22522;&#20110;&#36229;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#38598;&#25104;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Hypergraph-Based Machine Learning Ensemble Network Intrusion Detection System. (arXiv:2211.03933v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03933
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#38598;&#25104;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65292;&#20351;&#29992;&#36229;&#22270;&#25429;&#25417;&#31471;&#21475;&#25195;&#25551;&#25915;&#20987;&#30340;&#28436;&#21270;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#27966;&#29983;&#30340;&#24230;&#37327;&#26469;&#35757;&#32451;NIDS&#65292;&#20174;&#32780;&#20801;&#35768;&#22312;&#39640;&#31934;&#24230;&#12289;&#39640;&#20934;&#30830;&#29575;&#12289;&#39640;&#21484;&#22238;&#29575;&#24615;&#33021;&#19979;&#23454;&#26102;&#30417;&#27979;&#21644;&#26816;&#27979;&#31471;&#21475;&#25195;&#25551;&#27963;&#21160;&#12289;&#20854;&#20182;&#31867;&#22411;&#30340;&#25915;&#20987;&#21644;&#25932;&#23545;&#20837;&#20405;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;NIDS&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;(NIDS)&#22312;&#26816;&#27979;&#24694;&#24847;&#25915;&#20987;&#26102;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#12290;NIDS&#36890;&#24120;&#22312;&#31163;&#32447;&#29366;&#24577;&#19979;&#24320;&#21457;&#65292;&#20294;&#38754;&#23545;&#33258;&#21160;&#29983;&#25104;&#30340;&#31471;&#21475;&#25195;&#25551;&#28183;&#36879;&#23581;&#35797;&#26102;&#65292;&#20250;&#23548;&#33268;&#20174;&#23545;&#25163;&#36866;&#24212;&#21040;NIDS&#21709;&#24212;&#30340;&#26174;&#30528;&#26102;&#38388;&#28382;&#21518;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#20197;Internet&#21327;&#35758;&#22320;&#22336;&#21644;&#30446;&#26631;&#31471;&#21475;&#20026;&#37325;&#28857;&#30340;&#36229;&#22270;&#26469;&#25429;&#25417;&#31471;&#21475;&#25195;&#25551;&#25915;&#20987;&#30340;&#28436;&#21270;&#27169;&#24335;&#12290;&#28982;&#21518;&#20351;&#29992;&#27966;&#29983;&#30340;&#22522;&#20110;&#36229;&#22270;&#30340;&#24230;&#37327;&#26469;&#35757;&#32451;&#19968;&#20010;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;(ML)&#30340;NIDS&#65292;&#20174;&#32780;&#20801;&#35768;&#22312;&#39640;&#31934;&#24230;&#12289;&#39640;&#20934;&#30830;&#29575;&#12289;&#39640;&#21484;&#22238;&#29575;&#24615;&#33021;&#19979;&#23454;&#26102;&#35843;&#25972;&#65292;&#30417;&#27979;&#21644;&#26816;&#27979;&#31471;&#21475;&#25195;&#25551;&#27963;&#21160;&#12289;&#20854;&#20182;&#31867;&#22411;&#30340;&#25915;&#20987;&#21644;&#25932;&#23545;&#20837;&#20405;&#12290;&#36825;&#20010;ML&#33258;&#36866;&#24212;&#30340;NIDS&#26159;&#36890;&#36807;&#20197;&#19979;&#20960;&#20010;&#37096;&#20998;&#30340;&#32452;&#21512;&#24320;&#21457;&#20986;&#26469;&#30340;&#65306;(1)&#20837;&#20405;&#31034;&#20363;&#65292;(2)NIDS&#26356;&#26032;&#35268;&#21017;&#65292;(3)&#35302;&#21457;NIDS&#37325;&#26032;&#35757;&#32451;&#35831;&#27714;&#30340;&#25915;&#20987;&#38408;&#20540;&#36873;&#25321;&#65292;&#20197;&#21450;(4)&#22312;&#27809;&#26377;&#20808;&#21069;&#32593;&#32476;&#24615;&#36136;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#30340;&#29983;&#20135;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network intrusion detection systems (NIDS) to detect malicious attacks continue to meet challenges. NIDS are often developed offline while they face auto-generated port scan infiltration attempts, resulting in a significant time lag from adversarial adaption to NIDS response. To address these challenges, we use hypergraphs focused on internet protocol addresses and destination ports to capture evolving patterns of port scan attacks. The derived set of hypergraph-based metrics are then used to train an ensemble machine learning (ML) based NIDS that allows for real-time adaption in monitoring and detecting port scanning activities, other types of attacks, and adversarial intrusions at high accuracy, precision and recall performances. This ML adapting NIDS was developed through the combination of (1) intrusion examples, (2) NIDS update rules, (3) attack threshold choices to trigger NIDS retraining requests, and (4) a production environment with no prior knowledge of the nature of network 
&lt;/p&gt;</description></item></channel></rss>