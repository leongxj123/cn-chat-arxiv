# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [TraveLER: A Multi-LMM Agent Framework for Video Question-Answering](https://arxiv.org/abs/2404.01476) | TraveLER是一种多重LMM代理框架，通过沿着视频移动，并通过交互式提问收集关键帧的信息，以解决视频问答中关键时间戳选择和错误时间戳调整的问题 |
| [^2] | [UADA3D: Unsupervised Adversarial Domain Adaptation for 3D Object Detection with Sparse LiDAR and Large Domain Gaps](https://arxiv.org/abs/2403.17633) | UADA3D是一种无监督对抗领域自适应方法，能够在3D物体检测中处理稀疏LiDAR数据和大领域差距，并在自动驾驶汽车和移动机器人领域中表现出显著的改进。 |
| [^3] | [Just Say the Name: Online Continual Learning with Category Names Only via Data Generation](https://arxiv.org/abs/2403.10853) | 提出了在线连续学习框架G-NoCL，采用生成数据并利用DIverSity和COmplexity enhancing ensemBlER（DISCOBER）进行数据融合，展示了其在在线连续学习基准测试中的优越性能。 |
| [^4] | [COMQ: A Backpropagation-Free Algorithm for Post-Training Quantization](https://arxiv.org/abs/2403.07134) | 提出了一种名为COMQ的创新后训练量化算法，通过逐层减小重构误差来有效降低大型神经网络的存储要求，同时保持原始准确性。 |
| [^5] | [Diffusion Lens: Interpreting Text Encoders in Text-to-Image Pipelines](https://arxiv.org/abs/2403.05846) | 提出了一种分析文本到图像模型中文本编码器的方法，并通过生成中间表示的图像来深入研究，揭示了在复合提示和知识检索方面的一些重要发现。 |
| [^6] | [FSL Model can Score Higher as It Is](https://arxiv.org/abs/2402.18292) | 为了增加测试期间正确预测的机会，研究旨在通过图像到图像的转换纠正FSL模型的测试输入，生成被测试类别的新样本。 |
| [^7] | [Reset It and Forget It: Relearning Last-Layer Weights Improves Continual and Transfer Learning.](http://arxiv.org/abs/2310.07996) | 本研究发展了一种重置最后一层权重的方法，称为"zapping"，通过这种方法可以提供更好的持续和迁移学习效果，同时具备简单实施和高效计算的特点。 |
| [^8] | [Reverse Stable Diffusion: What prompt was used to generate this image?.](http://arxiv.org/abs/2308.01472) | 本论文介绍了一种新的任务，即在给定由生成扩散模型生成的图像的情况下预测文本提示。为了解决这个问题，作者结合了多种白盒和黑盒模型，提出了一个新颖的学习框架，该框架能够生成改进的提示，并采用课程学习和无监督领域自适应核学习方法来进一步提高方法的性能。 |
| [^9] | [Augmentation-aware Self-supervised Learning with Guided Projector.](http://arxiv.org/abs/2306.06082) | 本文提出了一种名为CASSLE的方法，它通过修改自监督学习中的有向投影网络，利用增强信息来提高模型处理图像特征的鲁棒性。 |
| [^10] | [AI Art Curation: Re-imagining the city of Helsinki in occasion of its Biennial.](http://arxiv.org/abs/2306.03753) | 本研究通过AI策展和观众互动，利用机器学习模型重新构想了赫尔辛基市艺术双年展，使用视觉-文本模型将室内艺术品放置在公共空间中，生成合成的360艺术全景图，以创造出艺术品与城市空间的新视角。 |
| [^11] | [A Rainbow in Deep Network Black Boxes.](http://arxiv.org/abs/2305.18512) | 彩虹网络是训练深度神经网络的概率模型，通过层内神经元权重互相独立的对齐和随机特征映射来进行线性降维和非线性高维嵌入，在ImageNet和CIFAR-10数据集上进行验证。 |
| [^12] | [Deep Radar Inverse Sensor Models for Dynamic Occupancy Grid Maps.](http://arxiv.org/abs/2305.12409) | 该研究提出了一种基于深度学习的雷达逆向传感器模型，用于将稀疏雷达检测映射到极坐标测量网格，并生成动态网格地图，实验结果表明该方法优于手工制作的几何ISM。与最先进的深度学习方法相比，该方法为从有限视场的雷达中学习极坐标方案的单帧测量网格的第一个方法。 |
| [^13] | [Open-World Continual Learning: Unifying Novelty Detection and Continual Learning.](http://arxiv.org/abs/2304.10038) | 本文从理论上证明，分布外检测对于类别增量学习是必要的，因为类别增量学习可以分解成任务内预测和任务 ID 预测，并且任务 ID 预测与分布外检测相关。 |
| [^14] | [Robustmix: Improving Robustness by Regularizing the Frequency Bias of Deep Nets.](http://arxiv.org/abs/2304.02847) | 本研究提出一种叫做Robustmix的方法，通过正则化网络以低频空间特征进行分类来提高深度网络的鲁棒性，在Imagenet-C和Stylized Imagenet等基准测试上取得了最新的最优状态平均峰值误差（mCE），在避免计算开销和先验知识的大量图像变换的同时对模型架构和数据增强的最新进展提供了补充。 |
| [^15] | [Multilevel Diffusion: Infinite Dimensional Score-Based Diffusion Models for Image Generation.](http://arxiv.org/abs/2303.04772) | 本文介绍了无限维度得分扩散模型在多个分辨率水平上的离散化方法，并使用多级扩散算法在多个分辨率上高效地学习。实证表明，该模型在相同或更高分辨率下产生比传统基于得分的扩散模型更高质量的样本，并可以生成不同分辨率的图像并处理矩形域。 |

# 详细

[^1]: TraveLER：用于视频问答的多重LMM代理框架

    TraveLER: A Multi-LMM Agent Framework for Video Question-Answering

    [https://arxiv.org/abs/2404.01476](https://arxiv.org/abs/2404.01476)

    TraveLER是一种多重LMM代理框架，通过沿着视频移动，并通过交互式提问收集关键帧的信息，以解决视频问答中关键时间戳选择和错误时间戳调整的问题

    

    最近，大型多模态模型（LMMs）在视频问答方面取得了重要进展，通过利用大规模、基于图像的预训练以零样本方式以帧为单位进行处理。虽然基于图像的视频方法展现了令人印象深刻的性能，但目前的局限是它们经常忽视了如何选择关键时间戳，并且无法在确定错误时间戳时进行调整。此外，它们无法提取与问题相关的细节，而是提供帧的一般描述。为了克服这一点，我们设计了一个多重LMM代理框架，它沿着视频进行移动，通过交互式提问的方式迭代地从关键帧收集相关信息，直到获得足够的信息来回答问题。具体来说，我们提出了TraveLER，这是一个可以制定“遍历”视频计划的模型，询问关于单个帧的问题以“定位”并存储关键信息

    arXiv:2404.01476v1 Announce Type: cross  Abstract: Recently, Large Multimodal Models (LMMs) have made significant progress in video question-answering using a frame-wise approach by leveraging large-scale, image-based pretraining in a zero-shot manner. While image-based methods for videos have shown impressive performance, a current limitation is that they often overlook how key timestamps are selected and cannot adjust when incorrect timestamps are identified. Moreover, they are unable to extract details relevant to the question, instead providing general descriptions of the frame. To overcome this, we design a multi-LMM agent framework that travels along the video, iteratively collecting relevant information from keyframes through interactive question-asking until there is sufficient information to answer the question. Specifically, we propose TraveLER, a model that can create a plan to "Traverse" through the video, ask questions about individual frames to "Locate" and store key info
    
[^2]: UADA3D：面向稀疏LiDAR和大领域差距的无监督对抗领域自适应在3D物体检测中的应用

    UADA3D: Unsupervised Adversarial Domain Adaptation for 3D Object Detection with Sparse LiDAR and Large Domain Gaps

    [https://arxiv.org/abs/2403.17633](https://arxiv.org/abs/2403.17633)

    UADA3D是一种无监督对抗领域自适应方法，能够在3D物体检测中处理稀疏LiDAR数据和大领域差距，并在自动驾驶汽车和移动机器人领域中表现出显著的改进。

    

    在这项研究中，我们解决了现有无监督领域适应方法在基于LiDAR的3D物体检测中的一个问题，这些方法主要集中在适应已建立的高密度自动驾驶数据集之间的转变。我们专注于更稀疏的点云，捕捉来自不同视角的场景：不仅来自道路上的车辆，还来自人行道上的移动机器人，遭遇着明显不同的环境条件和传感器配置。我们引入了无监督对抗领域自适应3D物体检测（UADA3D）。UADA3D不依赖于预训练的源模型或师生架构。相反，它使用对抗方法直接学习域不变特征。我们展示了它在各种适应场景中的有效性，在自动驾驶汽车和移动机器人领域均显示出显著的改进。我们的代码是开源的，很快将会提供。

    arXiv:2403.17633v1 Announce Type: cross  Abstract: In this study, we address a gap in existing unsupervised domain adaptation approaches on LiDAR-based 3D object detection, which have predominantly concentrated on adapting between established, high-density autonomous driving datasets. We focus on sparser point clouds, capturing scenarios from different perspectives: not just from vehicles on the road but also from mobile robots on sidewalks, which encounter significantly different environmental conditions and sensor configurations. We introduce Unsupervised Adversarial Domain Adaptation for 3D Object Detection (UADA3D). UADA3D does not depend on pre-trained source models or teacher-student architectures. Instead, it uses an adversarial approach to directly learn domain-invariant features. We demonstrate its efficacy in various adaptation scenarios, showing significant improvements in both self-driving car and mobile robot domains. Our code is open-source and will be available soon.
    
[^3]: 只说名称：通过数据生成实现仅利用类别名称进行在线连续学习

    Just Say the Name: Online Continual Learning with Category Names Only via Data Generation

    [https://arxiv.org/abs/2403.10853](https://arxiv.org/abs/2403.10853)

    提出了在线连续学习框架G-NoCL，采用生成数据并利用DIverSity和COmplexity enhancing ensemBlER（DISCOBER）进行数据融合，展示了其在在线连续学习基准测试中的优越性能。

    

    在现实世界的场景中，由于成本过高，对于连续学习进行大量手动注释是不切实际的。虽然之前的研究受到大规模网络监督训练的影响，建议在连续学习中利用网络抓取的数据，但这带来了诸如数据不平衡、使用限制和隐私问题等挑战。为了解决连续网络监督训练的风险，我们提出了一种在线连续学习框架 - 仅使用名称的生成式连续学习（G-NoCL）。所提出的G-NoCL使用一组生成器G以及学习者。当遇到新概念（例如，类别）时，G-NoCL采用新颖的样本复杂性引导数据合成技术DIverSity and COmplexity enhancing ensemBlER（DISCOBER）从生成的数据中最优抽样训练数据。通过大量实验，我们展示了DISCOBER在G-NoCL在线连续学习基准测试中表现出的优越性能，涵盖了In-Distributi。

    arXiv:2403.10853v1 Announce Type: cross  Abstract: In real-world scenarios, extensive manual annotation for continual learning is impractical due to prohibitive costs. Although prior arts, influenced by large-scale webly supervised training, suggest leveraging web-scraped data in continual learning, this poses challenges such as data imbalance, usage restrictions, and privacy concerns. Addressing the risks of continual webly supervised training, we present an online continual learning framework - Generative Name only Continual Learning (G-NoCL). The proposed G-NoCL uses a set of generators G along with the learner. When encountering new concepts (i.e., classes), G-NoCL employs the novel sample complexity-guided data ensembling technique DIverSity and COmplexity enhancing ensemBlER (DISCOBER) to optimally sample training data from generated data. Through extensive experimentation, we demonstrate superior performance of DISCOBER in G-NoCL online CL benchmarks, covering both In-Distributi
    
[^4]: COMQ: 一种无需反向传播的后训练量化算法

    COMQ: A Backpropagation-Free Algorithm for Post-Training Quantization

    [https://arxiv.org/abs/2403.07134](https://arxiv.org/abs/2403.07134)

    提出了一种名为COMQ的创新后训练量化算法，通过逐层减小重构误差来有效降低大型神经网络的存储要求，同时保持原始准确性。

    

    后训练量化（PTQ）已经成为一种将大型神经网络压缩的实用方法，使其在部署时高度高效。然而，有效地将这些模型降至低比特表示而不损害原始准确性仍然是一个关键挑战。在本文中，我们提出了一种创新的PTQ算法称为COMQ，它通过依次减小逐层重构误差来进行坐标方向上的最小化。我们考虑了广泛使用的整数量化，其中每个量化权重可以分解为一个共享的浮点标量和一个整数位编码。在固定层内，COMQ将所有缩放因子和位编码视为重构误差的变量。每次迭代都会沿着一个坐标轴改进这个错误，同时保持所有其他变量恒定。COMQ易于使用，无需调整超参数。它只涉及点乘和四舍五入。

    arXiv:2403.07134v1 Announce Type: new  Abstract: Post-training quantization (PTQ) has emerged as a practical approach to compress large neural networks, making them highly efficient for deployment. However, effectively reducing these models to their low-bit counterparts without compromising the original accuracy remains a key challenge. In this paper, we propose an innovative PTQ algorithm termed COMQ, which sequentially conducts coordinate-wise minimization of the layer-wise reconstruction errors. We consider the widely used integer quantization, where every quantized weight can be decomposed into a shared floating-point scalar and an integer bit-code. Within a fixed layer, COMQ treats all the scaling factor(s) and bit-codes as the variables of the reconstruction error. Every iteration improves this error along a single coordinate while keeping all other variables constant. COMQ is easy to use and requires no hyper-parameter tuning. It instead involves only dot products and rounding o
    
[^5]: 扩散镜头：解释文本编码器在文本到图像管道中的作用

    Diffusion Lens: Interpreting Text Encoders in Text-to-Image Pipelines

    [https://arxiv.org/abs/2403.05846](https://arxiv.org/abs/2403.05846)

    提出了一种分析文本到图像模型中文本编码器的方法，并通过生成中间表示的图像来深入研究，揭示了在复合提示和知识检索方面的一些重要发现。

    

    arXiv:2403.05846v1 通告类型：跨 存在文本到图像扩散模型（T2I）使用文本提示的潜在表示来引导图像生成过程。然而，编码器产生文本表示的过程是未知的。我们提出了扩散镜头，一种分析 T2I 模型文本编码器的方法，通过生成其中间表示的图像。使用扩散镜头，我们对两个最近的 T2I 模型进行了广泛分析。在探索复合提示时，我们发现描述多个对象的复杂场景相对于简单场景是逐步且较慢地构建的；在探索知识检索时，我们发现表示不常见概念需要比常见概念更多的计算，并且知识检索在层之间是渐进的。总体而言，我们的发现为 T2I 管道中的文本编码器组件提供了宝贵的见解。

    arXiv:2403.05846v1 Announce Type: cross  Abstract: Text-to-image diffusion models (T2I) use a latent representation of a text prompt to guide the image generation process. However, the process by which the encoder produces the text representation is unknown. We propose the Diffusion Lens, a method for analyzing the text encoder of T2I models by generating images from its intermediate representations. Using the Diffusion Lens, we perform an extensive analysis of two recent T2I models. Exploring compound prompts, we find that complex scenes describing multiple objects are composed progressively and more slowly compared to simple scenes; Exploring knowledge retrieval, we find that representation of uncommon concepts requires further computation compared to common concepts, and that knowledge retrieval is gradual across layers. Overall, our findings provide valuable insights into the text encoder component in T2I pipelines.
    
[^6]: FSL模型可以因为其优越性得分更高

    FSL Model can Score Higher as It Is

    [https://arxiv.org/abs/2402.18292](https://arxiv.org/abs/2402.18292)

    为了增加测试期间正确预测的机会，研究旨在通过图像到图像的转换纠正FSL模型的测试输入，生成被测试类别的新样本。

    

    在日常生活中，为了增加被正确识别的机会，我们倾向于面对面地直视面部识别机，而不是侧着面对。少样本学习（FSL）分类本身就具有挑战性，因为模型必须识别属于训练时未见的类别的图像。因此，在测试期间对扭曲和非典型的查询或支持图像会让模型更难正确预测。在我们的研究中，为了增加测试期间正确预测的机会，我们旨在通过图像到图像的转换纠正训练过的FSL模型的测试输入，生成被测试类别的新样本。FSL模型通常是在具有足够样本的类别上进行训练，然后在具有少样本样本的类别上进行测试。我们提出的方法首先捕捉测试图像的风格或形状，然后识别一个适当的训

    arXiv:2402.18292v1 Announce Type: cross  Abstract: In daily life, we tend to present the front of our faces by staring squarely at a facial recognition machine, instead of facing it sideways, in order to increase the chance of being correctly recognised. Few-shot-learning (FSL) classification is challenging in itself because a model has to identify images that belong to classes previously unseen during training. Therefore, a warped and non-typical query or support image during testing can make it even more challenging for a model to predict correctly. In our work, to increase the chance of correct prediction during testing, we aim to rectify the test input of a trained FSL model by generating new samples of the tested classes through image-to-image translation. An FSL model is usually trained on classes with sufficient samples, and then tested on classes with few-shot samples. Our proposed method first captures the style or shape of the test image, and then identifies a suitable traine
    
[^7]: 重置并忘却：重新学习最后一层权重改善持续和迁移学习

    Reset It and Forget It: Relearning Last-Layer Weights Improves Continual and Transfer Learning. (arXiv:2310.07996v1 [cs.LG])

    [http://arxiv.org/abs/2310.07996](http://arxiv.org/abs/2310.07996)

    本研究发展了一种重置最后一层权重的方法，称为"zapping"，通过这种方法可以提供更好的持续和迁移学习效果，同时具备简单实施和高效计算的特点。

    

    本研究发现了一种简单的预训练机制，能够导致具有更好的持续和迁移学习表征。这种机制——在最后一层权重中反复重置，我们称之为“zapping”——最初设计用于元持续学习过程，但我们发现它在许多不同于元学习和持续学习的情况下也非常适用。在我们的实验中，我们希望将预训练的图像分类器迁移到一组新的类别，仅使用少量样本。我们展示了我们的zapping过程在标准微调和持续学习设置中能够获得更好的迁移准确性和/或更快的适应性，同时实现简单的实施和高效的计算。在许多情况下，通过使用zapping和顺序学习的组合，我们可以达到与最先进的元学习相当的性能而无需昂贵的高阶梯度。

    This work identifies a simple pre-training mechanism that leads to representations exhibiting better continual and transfer learning. This mechanism -- the repeated resetting of weights in the last layer, which we nickname "zapping" -- was originally designed for a meta-continual-learning procedure, yet we show it is surprisingly applicable in many settings beyond both meta-learning and continual learning. In our experiments, we wish to transfer a pre-trained image classifier to a new set of classes, in a few shots. We show that our zapping procedure results in improved transfer accuracy and/or more rapid adaptation in both standard fine-tuning and continual learning settings, while being simple to implement and computationally efficient. In many cases, we achieve performance on par with state of the art meta-learning without needing the expensive higher-order gradients, by using a combination of zapping and sequential learning. An intuitive explanation for the effectiveness of this za
    
[^8]: 反向稳定扩散：生成该图像所使用的提示是什么？

    Reverse Stable Diffusion: What prompt was used to generate this image?. (arXiv:2308.01472v1 [cs.CV])

    [http://arxiv.org/abs/2308.01472](http://arxiv.org/abs/2308.01472)

    本论文介绍了一种新的任务，即在给定由生成扩散模型生成的图像的情况下预测文本提示。为了解决这个问题，作者结合了多种白盒和黑盒模型，提出了一个新颖的学习框架，该框架能够生成改进的提示，并采用课程学习和无监督领域自适应核学习方法来进一步提高方法的性能。

    

    文本到图像扩散模型，如稳定扩散，最近吸引了许多研究人员的兴趣，反向扩散过程在更好地理解生成过程和如何设计提示以获得所需图像方面起着重要作用。为此，我们引入了一种新的任务，即在给定由生成扩散模型生成的图像的情况下预测文本提示。我们结合了一系列白盒和黑盒模型（有和无对扩散网络权重进行访问）来处理所提出的任务。我们提出了一个新颖的学习框架，包括联合提示回归和多标签词汇分类目标，生成改进的提示。为了进一步改进我们的方法，我们采用了一个课程学习过程，促进了具有更低标注噪声（即更好对齐）的图像提示对的学习，并且使用相似性进行无监督领域自适应核学习方法。

    Text-to-image diffusion models such as Stable Diffusion have recently attracted the interest of many researchers, and inverting the diffusion process can play an important role in better understanding the generative process and how to engineer prompts in order to obtain the desired images. To this end, we introduce the new task of predicting the text prompt given an image generated by a generative diffusion model. We combine a series of white-box and black-box models (with and without access to the weights of the diffusion network) to deal with the proposed task. We propose a novel learning framework comprising of a joint prompt regression and multi-label vocabulary classification objective that generates improved prompts. To further improve our method, we employ a curriculum learning procedure that promotes the learning of image-prompt pairs with lower labeling noise (i.e. that are better aligned), and an unsupervised domain-adaptive kernel learning method that uses the similarities b
    
[^9]: 增强感知的有向投影自监督学习

    Augmentation-aware Self-supervised Learning with Guided Projector. (arXiv:2306.06082v1 [cs.CV])

    [http://arxiv.org/abs/2306.06082](http://arxiv.org/abs/2306.06082)

    本文提出了一种名为CASSLE的方法，它通过修改自监督学习中的有向投影网络，利用增强信息来提高模型处理图像特征的鲁棒性。

    

    自监督学习是从无标签数据中学习健壮表示的强大技术。SimCLR和MoCo等方法通过学习对应用的数据增强保持不变，能够达到与监督方法相当的质量。然而，这种不变性可能对解决某些下游任务有害，这些任务依赖于受到预训练期间使用的增强影响的特征，例如颜色。在本文中，我们提出通过修改自监督架构的常见组件之一的有向投影网络，来促进表示空间对这些特征的敏感性。具体而言，我们为投影器补充有关应用于图像的增强的信息。为了让投影器在解决自监督学习任务时利用这种辅助指导，特征提取器学习在其表示中保留增强信息。我们的方法被称为有向投影自监督学习（CASSLE），通过这种方法提高了模型处理图像特征的鲁棒性。

    Self-supervised learning (SSL) is a powerful technique for learning robust representations from unlabeled data. By learning to remain invariant to applied data augmentations, methods such as SimCLR and MoCo are able to reach quality on par with supervised approaches. However, this invariance may be harmful to solving some downstream tasks which depend on traits affected by augmentations used during pretraining, such as color. In this paper, we propose to foster sensitivity to such characteristics in the representation space by modifying the projector network, a common component of self-supervised architectures. Specifically, we supplement the projector with information about augmentations applied to images. In order for the projector to take advantage of this auxiliary guidance when solving the SSL task, the feature extractor learns to preserve the augmentation information in its representations. Our approach, coined Conditional Augmentation-aware Selfsupervised Learning (CASSLE), is d
    
[^10]: AI艺术策展：重新构想赫尔辛基市艺术双年展

    AI Art Curation: Re-imagining the city of Helsinki in occasion of its Biennial. (arXiv:2306.03753v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2306.03753](http://arxiv.org/abs/2306.03753)

    本研究通过AI策展和观众互动，利用机器学习模型重新构想了赫尔辛基市艺术双年展，使用视觉-文本模型将室内艺术品放置在公共空间中，生成合成的360艺术全景图，以创造出艺术品与城市空间的新视角。

    

    艺术策展实践的特点是以知识的方式展示艺术收藏品。机器过程的特点是它们能够处理和分析大量数据。本文设想了AI策展和观众互动，以探索当代机器学习模型对策展界的影响。该项目是为2023年赫尔辛基艺术双年展的场合而开发的，题为“可能出现新的方向”。我们使用赫尔辛基艺术博物馆（HAM）的藏品，通过机器感知的视角重新构想了赫尔辛基市。我们使用视觉-文本模型在公共空间中展示室内艺术品，根据相似性评分分配虚构的坐标。我们通过生成合成的360艺术全景图来改变每件艺术品在城市中的所处空间。我们通过估计每件艺术品位置的360全景图的深度值和机器生成的艺术品提示来指导生成过程。这个项目的结果就是...

    Art curatorial practice is characterized by the presentation of an art collection in a knowledgeable way. Machine processes are characterized by their capacity to manage and analyze large amounts of data. This paper envisages AI curation and audience interaction to explore the implications of contemporary machine learning models for the curatorial world. This project was developed for the occasion of the 2023 Helsinki Art Biennial, entitled New Directions May Emerge. We use the Helsinki Art Museum (HAM) collection to re-imagine the city of Helsinki through the lens of machine perception. We use visual-textual models to place indoor artworks in public spaces, assigning fictional coordinates based on similarity scores. We transform the space that each artwork inhabits in the city by generating synthetic 360 art panoramas. We guide the generation estimating depth values from 360 panoramas at each artwork location, and machine-generated prompts of the artworks. The result of this project i
    
[^11]: 深度网络黑盒中的彩虹

    A Rainbow in Deep Network Black Boxes. (arXiv:2305.18512v1 [cs.LG])

    [http://arxiv.org/abs/2305.18512](http://arxiv.org/abs/2305.18512)

    彩虹网络是训练深度神经网络的概率模型，通过层内神经元权重互相独立的对齐和随机特征映射来进行线性降维和非线性高维嵌入，在ImageNet和CIFAR-10数据集上进行验证。

    

    我们引入了彩虹网络作为训练好的深度神经网络的概率模型。该模型级联随机特征映射，其权重分布是可以学习的。它假设不同层之间的权重依赖性被减少到将输入激活对准的旋转。层内的神经元权重在这种对齐后是相互独立的。它们的激活定义了在无穷宽度极限下变得确定的内核。这在ImageNet数据集上训练的ResNets中通过数字验证。我们还发现，学习的权重分布具有低秩协方差。因此，彩虹网络在线性降维和非线性高维嵌入与白色随机特征之间交替。我们提供了具有高斯权重分布的高斯彩虹网络定义。这些模型在使用小波散射网络进行CIFAR-10图像分类方面进行了数字验证。我们还证明了，在训练期间，SGD更新权重的协方差。

    We introduce rainbow networks as a probabilistic model of trained deep neural networks. The model cascades random feature maps whose weight distributions are learned. It assumes that dependencies between weights at different layers are reduced to rotations which align the input activations. Neuron weights within a layer are independent after this alignment. Their activations define kernels which become deterministic in the infinite-width limit. This is verified numerically for ResNets trained on the ImageNet dataset. We also show that the learned weight distributions have low-rank covariances. Rainbow networks thus alternate between linear dimension reductions and non-linear high-dimensional embeddings with white random features. Gaussian rainbow networks are defined with Gaussian weight distributions. These models are validated numerically on image classification on the CIFAR-10 dataset, with wavelet scattering networks. We further show that during training, SGD updates the weight cov
    
[^12]: 动态占据网格地图的深度雷达逆向传感器模型

    Deep Radar Inverse Sensor Models for Dynamic Occupancy Grid Maps. (arXiv:2305.12409v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.12409](http://arxiv.org/abs/2305.12409)

    该研究提出了一种基于深度学习的雷达逆向传感器模型，用于将稀疏雷达检测映射到极坐标测量网格，并生成动态网格地图，实验结果表明该方法优于手工制作的几何ISM。与最先进的深度学习方法相比，该方法为从有限视场的雷达中学习极坐标方案的单帧测量网格的第一个方法。

    

    实现自动驾驶的一个重要步骤是基于传感器输入对车辆环境进行建模。由于其众所周知的优势，雷达成为推断围绕车辆的网格单元占用状态的流行选择。为了解决雷达检测数据稀疏性和噪声问题，我们提出了一种基于深度学习的逆向传感器模型（ISM），用于学习从稀疏雷达检测到极坐标测量网格的映射。改进的基于激光雷达测量的网格用作参考。学习到的雷达测量网格与雷达多普勒速度测量相结合，进一步用于生成动态网格地图（DGM）。在实际的高速公路情景实验中表明，我们的方法优于手工制作的几何ISM。与最先进的深度学习方法相比，我们的方法是第一个从有限视场（FOV）的雷达中学习极坐标方案的单帧测量网格的方法。学习框架使学习到的ISM可以直接嵌入到现有的贝叶斯状态估计方案中，以提高环境建模的准确性。

    To implement autonomous driving, one essential step is to model the vehicle environment based on the sensor inputs. Radars, with their well-known advantages, became a popular option to infer the occupancy state of grid cells surrounding the vehicle. To tackle data sparsity and noise of radar detections, we propose a deep learning-based Inverse Sensor Model (ISM) to learn the mapping from sparse radar detections to polar measurement grids. Improved lidar-based measurement grids are used as reference. The learned radar measurement grids, combined with radar Doppler velocity measurements, are further used to generate a Dynamic Grid Map (DGM). Experiments in real-world highway scenarios show that our approach outperforms the hand-crafted geometric ISMs. In comparison to state-of-the-art deep learning methods, our approach is the first one to learn a single-frame measurement grid in the polar scheme from radars with a limited Field Of View (FOV). The learning framework makes the learned ISM
    
[^13]: 开放世界持续学习：统一新颖性检测与持续学习

    Open-World Continual Learning: Unifying Novelty Detection and Continual Learning. (arXiv:2304.10038v1 [cs.LG])

    [http://arxiv.org/abs/2304.10038](http://arxiv.org/abs/2304.10038)

    本文从理论上证明，分布外检测对于类别增量学习是必要的，因为类别增量学习可以分解成任务内预测和任务 ID 预测，并且任务 ID 预测与分布外检测相关。

    

    随着 AI agent 在未知或新奇的真实开放世界中的使用增加，它们需要具备 (1) 认识已经学习过的物体和检测到之前未见或学习的物体的能力，以及 (2) 增量地学习新物品，逐渐变得更有知识和更强大。 (1) 称为新颖性检测或分布外 (OOD) 检测，而 (2) 称为类别增量学习 (CIL)，是持续学习 (CL) 的一种设置。在现有的研究中，OOD 检测和 CIL 被视为两个完全不同的问题。本文从理论上证明了 OOD 检测实际上对于 CIL 是必要的。我们首先展示 CIL 可以分解为两个子问题：任务内预测 (WP) 和任务 ID 预测(TP)。然后我们证明了 TP 与 OOD 检测相关。关键的理论结果是，无论 WP 和 OOD 检测（或 TP）是否由 CIL 算法显式或隐式地定义，好的 WP 和良好的 OOD 检测或 TP 总是存在嵌入在任何 CIL 算法中的。

    As AI agents are increasingly used in the real open world with unknowns or novelties, they need the ability to (1) recognize objects that (i) they have learned and (ii) detect items that they have not seen or learned before, and (2) learn the new items incrementally to become more and more knowledgeable and powerful. (1) is called novelty detection or out-of-distribution (OOD) detection and (2) is called class incremental learning (CIL), which is a setting of continual learning (CL). In existing research, OOD detection and CIL are regarded as two completely different problems. This paper theoretically proves that OOD detection actually is necessary for CIL. We first show that CIL can be decomposed into two sub-problems: within-task prediction (WP) and task-id prediction (TP). We then prove that TP is correlated with OOD detection. The key theoretical result is that regardless of whether WP and OOD detection (or TP) are defined explicitly or implicitly by a CIL algorithm, good WP and go
    
[^14]: Robustmix：通过正则化深度网络的频率偏差来提高鲁棒性

    Robustmix: Improving Robustness by Regularizing the Frequency Bias of Deep Nets. (arXiv:2304.02847v1 [cs.CV])

    [http://arxiv.org/abs/2304.02847](http://arxiv.org/abs/2304.02847)

    本研究提出一种叫做Robustmix的方法，通过正则化网络以低频空间特征进行分类来提高深度网络的鲁棒性，在Imagenet-C和Stylized Imagenet等基准测试上取得了最新的最优状态平均峰值误差（mCE），在避免计算开销和先验知识的大量图像变换的同时对模型架构和数据增强的最新进展提供了补充。

    

    深度网络在一系列经过精心策划的基准数据集上取得了令人印象深刻的结果。令人惊讶的是，它们的性能对于对人类性能几乎没有影响的扰动仍然很敏感。在这项工作中，我们提出了一种名为Robustmix的Mixup新扩展，该扩展通过正则化网络以基于低频空间特征进行分类。我们表明，这种类型的正则化改善了在一系列基准测试中的鲁棒性，例如Imagenet-C和Stylized Imagenet。它几乎没有计算开销，并且不需要先验知识的大量图像变换。我们发现，这种方法进一步补充了模型架构和数据增强的最新进展，使用EfficientNet-B8模型和RandAugment达到了44.8的最新状态平均峰值误差（mCE），相比基线降低了16个mCE。

    Deep networks have achieved impressive results on a range of well-curated benchmark datasets. Surprisingly, their performance remains sensitive to perturbations that have little effect on human performance. In this work, we propose a novel extension of Mixup called Robustmix that regularizes networks to classify based on lower-frequency spatial features. We show that this type of regularization improves robustness on a range of benchmarks such as Imagenet-C and Stylized Imagenet. It adds little computational overhead and, furthermore, does not require a priori knowledge of a large set of image transformations. We find that this approach further complements recent advances in model architecture and data augmentation, attaining a state-of-the-art mCE of 44.8 with an EfficientNet-B8 model and RandAugment, which is a reduction of 16 mCE compared to the baseline.
    
[^15]: 多级扩散：图像生成的无限维度基于得分的扩散模型

    Multilevel Diffusion: Infinite Dimensional Score-Based Diffusion Models for Image Generation. (arXiv:2303.04772v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04772](http://arxiv.org/abs/2303.04772)

    本文介绍了无限维度得分扩散模型在多个分辨率水平上的离散化方法，并使用多级扩散算法在多个分辨率上高效地学习。实证表明，该模型在相同或更高分辨率下产生比传统基于得分的扩散模型更高质量的样本，并可以生成不同分辨率的图像并处理矩形域。

    

    基于得分的扩散模型是近年来图像生成的最先进方法之一。现有的基于得分的扩散模型通常在有限维度设置中表述，其中图像被视为具有有限尺寸的张量。本文在无限维度设置中开发了基于得分的扩散模型，即我们将训练数据建模为支撑在矩形域上的函数。除了追求在更高分辨率下生成图像之外，我们的主要动机是创建一个良好定义的无限维度学习问题，以便可以在多个分辨率水平上一致地离散化它。我们希望获得能够横跨不同分辨率级别的扩散模型，并提高训练过程的效率。我们展示了如何克服当前基于得分的扩散模型在无限维度设置中存在的两个缺点。首先，我们修改了前向过程以确保在无限维度设置中潜在分布是良好定义的。其次，我们提出了一种多级扩散算法，使我们能够在多个分辨率上高效地学习。我们实证表明，我们的多级模型在相同或更高分辨率下产生比传统基于得分的扩散模型更高质量的样本。此外，我们的方法可以无缝地生成不同分辨率的图像并处理矩形域。

    Score-based diffusion models (SBDM) have recently emerged as state-of-the-art approaches for image generation. Existing SBDMs are typically formulated in a finite-dimensional setting, where images are considered as tensors of a finite size. This papers develops SBDMs in the infinite-dimensional setting, that is, we model the training data as functions supported on a rectangular domain. Besides the quest for generating images at ever higher resolution our primary motivation is to create a well-posed infinite-dimensional learning problem so that we can discretize it consistently on multiple resolution levels. We thereby hope to obtain diffusion models that generalize across different resolution levels and improve the efficiency of the training process. We demonstrate how to overcome two shortcomings of current SBDM approaches in the infinite-dimensional setting. First, we modify the forward process to ensure that the latent distribution is well-defined in the infinite-dimensional setting
    

