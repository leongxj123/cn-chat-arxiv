# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation Learning for Neural Radiance Fields](https://arxiv.org/abs/2404.01300) | 通过使用Masked AutoEncoders，本文提出了NeRF-MAE用于自监督三维表示学习，利用标准的三维Vision Transformers适应NeRF的独特公式，将NeRF的体积网格作为密集输入，以产生有效的三维表示。 |
| [^2] | [NeuroPictor: Refining fMRI-to-Image Reconstruction via Multi-individual Pretraining and Multi-level Modulation](https://arxiv.org/abs/2403.18211) | NeuroPictor通过直接调制扩散模型的生成过程，实现了fMRI到图像的重建，在多个个体预训练和多层次的引导条件下，实现了更详细的图像控制。 |
| [^3] | [BIMCV-R: A Landmark Dataset for 3D CT Text-Image Retrieval](https://arxiv.org/abs/2403.15992) | 提出了一个里程碑数据集BIMCV-R，包含8,069个3D CT体积和其放射学报告，同时开发了检索策略MedFinder，为3D医学文本图像检索领域提供了重要贡献 |
| [^4] | [On Pretraining Data Diversity for Self-Supervised Learning](https://arxiv.org/abs/2403.13808) | 增加预训练数据多样性可以提高自监督学习性能，但仅在与下游数据的分布距离较小时有效。 |
| [^5] | [Not Just Change the Labels, Learn the Features: Watermarking Deep Neural Networks with Multi-View Data](https://arxiv.org/abs/2403.10663) | 通过使用多视角数据为深度神经网络添加水印，可以有效防御对源模型功能的窃取攻击 |
| [^6] | [Attention-based Class-Conditioned Alignment for Multi-Source Domain Adaptive Object Detection](https://arxiv.org/abs/2403.09918) | 提出了一种基于注意力的类别条件对齐方案，用于多源领域自适应目标检测，在跨领域对齐每个对象类别的实例。 |
| [^7] | [Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models](https://arxiv.org/abs/2403.09635) | 提出了一个统一的信号传播理论，提供了控制transformer模型信号传播的公式，提出了DeepScaleLM初始化和缩放方案，使得可以训练非常深的模型，并发现深层模型在多个任务和数据集上胜过浅层模型。 |
| [^8] | [Self-Supervised Learning in Electron Microscopy: Towards a Foundation Model for Advanced Image Analysis](https://arxiv.org/abs/2402.18286) | 本文探讨了在电子显微镜中进行自监督学习的潜力，展示自监督预训练如何促进有效的微调，同时指出较低复杂度的模型在微调过程中始终优于更复杂的随机初始化模型。 |
| [^9] | [V-IRL: Grounding Virtual Intelligence in Real Life](https://arxiv.org/abs/2402.03310) | V-IRL是一个平台，可以让人工智能代理在虚拟环境中与现实世界进行互动，旨在将数字和物理世界之间的差距缩小，并开发出具有丰富感知、决策和与真实数据互动能力的代理。 |
| [^10] | [SynthCLIP: Are We Ready for a Fully Synthetic CLIP Training?](https://arxiv.org/abs/2402.01832) | SynthCLIP是一种新的框架，用于训练完全合成的CLIP模型，通过生成大规模的合成图片和标题数据集，在性能上可以与在真实数据上训练的CLIP模型相媲美。 |
| [^11] | [Common Sense Reasoning for Deep Fake Detection](https://arxiv.org/abs/2402.00126) | 该论文提出使用常识推理来建模深度伪造检测，通过扩展到Deepfake Detection VQA任务来模拟人类直觉，解释标记图像为真实或伪造的原因。 |
| [^12] | [Receler: Reliable Concept Erasing of Text-to-Image Diffusion Models via Lightweight Erasers](https://arxiv.org/abs/2311.17717) | Receler提出了一种可靠概念擦除方法，通过轻量级橡皮擦实现对文本到图像扩散模型的概念擦除，具备鲁棒性和局部性，实验证明其优越性。 |
| [^13] | [Advancing Large Multi-modal Models with Explicit Chain-of-Reasoning and Visual Question Generation.](http://arxiv.org/abs/2401.10005) | 本文提出了一种新的方法，通过显性推理和问题生成，将大型多模态模型(LMM)赋予了显性推理能力，从而提高了推理过程的鲁棒性和可解释性。 |
| [^14] | [Memory-Efficient Personalization using Quantized Diffusion Model.](http://arxiv.org/abs/2401.04339) | 本文研究了使用量化扩散模型进行内存高效个性化的方法，提出了两个策略来解决基线模型中主题和提示质量之间的权衡问题。 |
| [^15] | [Towards Realistic Unsupervised Fine-tuning with CLIP.](http://arxiv.org/abs/2308.12919) | 本论文针对无监督微调中可能出现的未知类别和超出分布范围的问题，提出了一种称为UEO的简单、高效、有效的微调方法，该方法能够同时提高对超出分布样本的检测能力和预定义类别实例的识别能力。 |
| [^16] | [Adjusting Logit in Gaussian Form for Long-Tailed Visual Recognition.](http://arxiv.org/abs/2305.10648) | 本文提出了一种特征增强方法和两种logit调整方法，用于解决长尾视觉识别中的类别不平衡问题。实验结果表明，该方法在CIFAR和ImageNet长尾数据集上表现优于其他最先进的方法。 |
| [^17] | [Towards Computational Architecture of Liberty: A Comprehensive Survey on Deep Learning for Generating Virtual Architecture in the Metaverse.](http://arxiv.org/abs/2305.00510) | 本文综述了当前最新的深度学习生成模型用于建筑形式的3D对象生成方法，强调了尚未充分探讨的问题，并提出了未来研究的重点议程。 |
| [^18] | [SIFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency.](http://arxiv.org/abs/2303.11525) | 本研究提出了一种名为SIFT的方法，用于提高深度神经网络的训练效率、准确性和表示能力，通过稀疏等FLOP转换，缩短训练时间。 |

# 详细

[^1]: NeRF-MAE: 自监督三维表示学习中的Masked AutoEncoders

    NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation Learning for Neural Radiance Fields

    [https://arxiv.org/abs/2404.01300](https://arxiv.org/abs/2404.01300)

    通过使用Masked AutoEncoders，本文提出了NeRF-MAE用于自监督三维表示学习，利用标准的三维Vision Transformers适应NeRF的独特公式，将NeRF的体积网格作为密集输入，以产生有效的三维表示。

    

    由于神经场在计算机视觉和机器人领域的卓越能力，能够理解三维视觉世界，如推断语义、几何和动态等，本文探讨了神经场在从二维图像中密集表示三维场景的自监督预训练，具体使用Masked AutoEncoders的可能性。我们借鉴了将transformers扩展到新数据模态的令人惊讶的成功，利用标准的三维Vision Transformers来适应NeRF的独特公式。我们将NeRF的体积网格作为transformer的密集输入，与其他三维表示（如点云）进行对比，其信息密度可能不均匀，而表示是不规则的。由于将masked autoencoders应用于类似NeRF这样的隐式表示的困难，我们选择提取一个显式的表示。

    arXiv:2404.01300v1 Announce Type: cross  Abstract: Neural fields excel in computer vision and robotics due to their ability to understand the 3D visual world such as inferring semantics, geometry, and dynamics. Given the capabilities of neural fields in densely representing a 3D scene from 2D images, we ask the question: Can we scale their self-supervised pretraining, specifically using masked autoencoders, to generate effective 3D representations from posed RGB images. Owing to the astounding success of extending transformers to novel data modalities, we employ standard 3D Vision Transformers to suit the unique formulation of NeRFs. We leverage NeRF's volumetric grid as a dense input to the transformer, contrasting it with other 3D representations such as pointclouds where the information density can be uneven, and the representation is irregular. Due to the difficulty of applying masked autoencoders to an implicit representation, such as NeRF, we opt for extracting an explicit repres
    
[^2]: NeuroPictor: 通过多个个体的预训练和多层调制优化fMRI到图像的重建

    NeuroPictor: Refining fMRI-to-Image Reconstruction via Multi-individual Pretraining and Multi-level Modulation

    [https://arxiv.org/abs/2403.18211](https://arxiv.org/abs/2403.18211)

    NeuroPictor通过直接调制扩散模型的生成过程，实现了fMRI到图像的重建，在多个个体预训练和多层次的引导条件下，实现了更详细的图像控制。

    

    最近的fMRI到图像方法主要集中在将fMRI信号与预先训练的扩散模型的特定条件关联起来。相比之下，本文提出直接调制扩散模型的生成过程，将fMRI到图像过程分为三个步骤：i) fMRI校准编码，用于处理共享潜在空间的多个体预训练，以最小化个体差异并实现后续的跨主体训练；ii) fMRI到图像跨个体预训练，感知地学习如何引导不同个体之间高低层次条件的扩散模型；iii) fMRI到图像单个体细化，类似于步骤ii，但侧重于适应特定个体。

    arXiv:2403.18211v1 Announce Type: cross  Abstract: Recent fMRI-to-image approaches mainly focused on associating fMRI signals with specific conditions of pre-trained diffusion models. These approaches, while producing high-quality images, capture only a limited aspect of the complex information in fMRI signals and offer little detailed control over image creation. In contrast, this paper proposes to directly modulate the generation process of diffusion models using fMRI signals. Our approach, NeuroPictor, divides the fMRI-to-image process into three steps: i) fMRI calibrated-encoding, to tackle multi-individual pre-training for a shared latent space to minimize individual difference and enable the subsequent cross-subject training; ii) fMRI-to-image cross-subject pre-training, perceptually learning to guide diffusion model with high- and low-level conditions across different individuals; iii) fMRI-to-image single-subject refining, similar with step ii but focus on adapting to particula
    
[^3]: BIMCV-R：用于3D CT文本图像检索的里程碑数据集

    BIMCV-R: A Landmark Dataset for 3D CT Text-Image Retrieval

    [https://arxiv.org/abs/2403.15992](https://arxiv.org/abs/2403.15992)

    提出了一个里程碑数据集BIMCV-R，包含8,069个3D CT体积和其放射学报告，同时开发了检索策略MedFinder，为3D医学文本图像检索领域提供了重要贡献

    

    arXiv:2403.15992v1 发布类型: 跨越  摘要: 三维医学图像与医疗保健的融合不断增加了医疗专业人员的工作量。为了帮助临床医生在诊断过程中，减轻其工作量，开发一个可靠的检索相似病例研究的系统是一个可行的解决方案。尽管这一概念有很大的潜力，但是目前3D医学文本图像检索领域受限于缺乏健全的评估基准和精心策划的数据集。为了解决这一问题，我们的研究提出了一种开创性的数据集，BIMCV-R（此数据集将在接受后发布。），其中包含了8,069个3D CT体积的广泛收集，包括超过200万张切片，以及它们各自的放射学报告。在我们数据集的基础上，我们拓展了一种检索策略，MedFinder。该方法采用双流网络架构，利用大

    arXiv:2403.15992v1 Announce Type: cross  Abstract: The burgeoning integration of 3D medical imaging into healthcare has led to a substantial increase in the workload of medical professionals. To assist clinicians in their diagnostic processes and alleviate their workload, the development of a robust system for retrieving similar case studies presents a viable solution. While the concept holds great promise, the field of 3D medical text-image retrieval is currently limited by the absence of robust evaluation benchmarks and curated datasets. To remedy this, our study presents a groundbreaking dataset, BIMCV-R (This dataset will be released upon acceptance.), which includes an extensive collection of 8,069 3D CT volumes, encompassing over 2 million slices, paired with their respective radiological reports. Expanding upon the foundational work of our dataset, we craft a retrieval strategy, MedFinder. This approach employs a dual-stream network architecture, harnessing the potential of larg
    
[^4]: 关于自监督学习的预训练数据多样性

    On Pretraining Data Diversity for Self-Supervised Learning

    [https://arxiv.org/abs/2403.13808](https://arxiv.org/abs/2403.13808)

    增加预训练数据多样性可以提高自监督学习性能，但仅在与下游数据的分布距离较小时有效。

    

    我们探讨了使用更多样化数据集对自监督学习(SSL)性能的影响，这些数据集的特征是唯一样本数量，在固定的计算预算下。我们的研究结果一致表明，增加预训练数据的多样性可以提高SSL性能，尽管只有当与下游数据的分布距离很小的时候才是如此。值得注意的是，即使通过网络爬虫或扩散生成的数据等方式实现了异常大的预训练数据多样性，分布转移仍然是一个挑战。我们的实验涵盖了七种SSL方法，使用了诸如ImageNet和YFCC100M等大规模数据集，总计超过200个GPU天。代码和训练模型将在https://github.com/hammoudhasan/DiversitySSL 上提供。

    arXiv:2403.13808v1 Announce Type: cross  Abstract: We explore the impact of training with more diverse datasets, characterized by the number of unique samples, on the performance of self-supervised learning (SSL) under a fixed computational budget. Our findings consistently demonstrate that increasing pretraining data diversity enhances SSL performance, albeit only when the distribution distance to the downstream data is minimal. Notably, even with an exceptionally large pretraining data diversity achieved through methods like web crawling or diffusion-generated data, among other ways, the distribution shift remains a challenge. Our experiments are comprehensive with seven SSL methods using large-scale datasets such as ImageNet and YFCC100M amounting to over 200 GPU days. Code and trained models will be available at https://github.com/hammoudhasan/DiversitySSL .
    
[^5]: 不仅改变标签，学习特征：使用多视角数据为深度神经网络添加水印

    Not Just Change the Labels, Learn the Features: Watermarking Deep Neural Networks with Multi-View Data

    [https://arxiv.org/abs/2403.10663](https://arxiv.org/abs/2403.10663)

    通过使用多视角数据为深度神经网络添加水印，可以有效防御对源模型功能的窃取攻击

    

    随着机器学习作为服务（MLaaS）平台的日益普及，越来越多关注深度神经网络（DNN）水印技术。这些方法用于验证目标DNN模型的所有权以保护知识产权。本文首先从特征学习的角度引入了一种新颖的基于触发集的水印方法。具体来说，我们表明通过选择展示多个特征的数据，也被称为$\textit{多视角数据}$，可以有效地防御...

    arXiv:2403.10663v1 Announce Type: cross  Abstract: With the increasing prevalence of Machine Learning as a Service (MLaaS) platforms, there is a growing focus on deep neural network (DNN) watermarking techniques. These methods are used to facilitate the verification of ownership for a target DNN model to protect intellectual property. One of the most widely employed watermarking techniques involves embedding a trigger set into the source model. Unfortunately, existing methodologies based on trigger sets are still susceptible to functionality-stealing attacks, potentially enabling adversaries to steal the functionality of the source model without a reliable means of verifying ownership. In this paper, we first introduce a novel perspective on trigger set-based watermarking methods from a feature learning perspective. Specifically, we demonstrate that by selecting data exhibiting multiple features, also referred to as $\textit{multi-view data}$, it becomes feasible to effectively defend 
    
[^6]: 基于注意力的多源领域自适应目标检测的类别条件对齐

    Attention-based Class-Conditioned Alignment for Multi-Source Domain Adaptive Object Detection

    [https://arxiv.org/abs/2403.09918](https://arxiv.org/abs/2403.09918)

    提出了一种基于注意力的类别条件对齐方案，用于多源领域自适应目标检测，在跨领域对齐每个对象类别的实例。

    

    目标检测（OD）的领域自适应方法致力于通过促进源域和目标域之间的特征对齐来缓解分布转移的影响。多源领域自适应（MSDA）允许利用多个带注释的源数据集和未标记的目标数据来提高检测模型的准确性和鲁棒性。大多数最先进的OD MSDA方法以一种与类别无关的方式执行特征对齐。最近提出的基于原型的方法提出了一种按类别对齐的方法，但由于嘈杂的伪标签而导致错误积累，这可能会对不平衡数据的自适应产生负面影响。为克服这些限制，我们提出了一种基于注意力的类别条件对齐方案，用于MSDA，该方案在跨领域对齐每个对象类别的实例。

    arXiv:2403.09918v1 Announce Type: cross  Abstract: Domain adaptation methods for object detection (OD) strive to mitigate the impact of distribution shifts by promoting feature alignment across source and target domains. Multi-source domain adaptation (MSDA) allows leveraging multiple annotated source datasets, and unlabeled target data to improve the accuracy and robustness of the detection model. Most state-of-the-art MSDA methods for OD perform feature alignment in a class-agnostic manner. This is challenging since the objects have unique modal information due to variations in object appearance across domains. A recent prototype-based approach proposed a class-wise alignment, yet it suffers from error accumulation due to noisy pseudo-labels which can negatively affect adaptation with imbalanced data. To overcome these limitations, we propose an attention-based class-conditioned alignment scheme for MSDA that aligns instances of each object category across domains. In particular, an 
    
[^7]: Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models

    Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models

    [https://arxiv.org/abs/2403.09635](https://arxiv.org/abs/2403.09635)

    提出了一个统一的信号传播理论，提供了控制transformer模型信号传播的公式，提出了DeepScaleLM初始化和缩放方案，使得可以训练非常深的模型，并发现深层模型在多个任务和数据集上胜过浅层模型。

    

    尽管transformer模型取得了巨大的成功，但在深度方面仍然很难扩展。本研究提出了一个统一的信号传播理论，并提供了控制transformer模型前向和反向信号矩的公式。我们的框架可以用于理解和缓解与高注意力分数相关的梯度消失/爆炸、秩坍缩和不稳定性。我们还提出了DeepScaleLM，一种初始化和缩放方案，通过该方案能够在模型中保持单位输出/梯度矩，从而使训练具有100多层的非常深模型成为可能。我们发现，transformer模型可以更深 - 我们的深层模型在语言建模、语音翻译和图像分类方面表现优异，包括仅编码器、仅解码器和编码器-解码器变体，适用于Pre-LN和Post-LN transformers，适用于多个数据集和模型大小。

    arXiv:2403.09635v1 Announce Type: cross  Abstract: In spite of their huge success, transformer models remain difficult to scale in depth. In this work, we develop a unified signal propagation theory and provide formulae that govern the moments of the forward and backward signal through the transformer model. Our framework can be used to understand and mitigate vanishing/exploding gradients, rank collapse, and instability associated with high attention scores. We also propose DeepScaleLM, an initialization and scaling scheme that conserves unit output/gradient moments throughout the model, enabling the training of very deep models with 100s of layers. We find that transformer models could be much deeper - our deep models with fewer parameters outperform shallow models in Language Modeling, Speech Translation, and Image Classification, across Encoder-only, Decoder-only and Encoder-Decoder variants, for both Pre-LN and Post-LN transformers, for multiple datasets and model sizes. These imp
    
[^8]: 电子显微镜中的自监督学习：迈向高级图像分析基础模型

    Self-Supervised Learning in Electron Microscopy: Towards a Foundation Model for Advanced Image Analysis

    [https://arxiv.org/abs/2402.18286](https://arxiv.org/abs/2402.18286)

    本文探讨了在电子显微镜中进行自监督学习的潜力，展示自监督预训练如何促进有效的微调，同时指出较低复杂度的模型在微调过程中始终优于更复杂的随机初始化模型。

    

    在这项工作中，我们探讨了从无标签的电子显微镜数据集中进行自监督学习的潜力，迈出了构建该领域基础模型的一步。我们展示了自监督预训练如何促进有效的微调，以应用于一系列下游任务，包括语义分割、去噪、噪声与背景去除以及超分辨率。通过实验不同模型复杂度和感受野大小的变化，我们发现一个显著的现象，即微调过的较低复杂度模型始终胜过具有随机权重初始化的更复杂模型。我们展示了自监督预训练在电子显微镜背景下在各种下游任务中的多才多艺，使得快速收敛和更好的性能成为可能。我们得出结论，自监督预训练是一种强大的催化剂，特别在有限的注释数据可用时和 ef

    arXiv:2402.18286v1 Announce Type: cross  Abstract: In this work, we explore the potential of self-supervised learning from unlabeled electron microscopy datasets, taking a step toward building a foundation model in this field. We show how self-supervised pretraining facilitates efficient fine-tuning for a spectrum of downstream tasks, including semantic segmentation, denoising, noise & background removal, and super-resolution. Experimentation with varying model complexities and receptive field sizes reveals the remarkable phenomenon that fine-tuned models of lower complexity consistently outperform more complex models with random weight initialization. We demonstrate the versatility of self-supervised pretraining across various downstream tasks in the context of electron microscopy, allowing faster convergence and better performance. We conclude that self-supervised pretraining serves as a powerful catalyst, being especially advantageous when limited annotated data are available and ef
    
[^9]: V-IRL: 将虚拟智能与现实生活联系起来

    V-IRL: Grounding Virtual Intelligence in Real Life

    [https://arxiv.org/abs/2402.03310](https://arxiv.org/abs/2402.03310)

    V-IRL是一个平台，可以让人工智能代理在虚拟环境中与现实世界进行互动，旨在将数字和物理世界之间的差距缩小，并开发出具有丰富感知、决策和与真实数据互动能力的代理。

    

    人类生活在地球上，而现代人工智能代理所创造的数字领域之间存在着感官差距。为了开发出在现实世界中能像人类一样灵活感知、思考和行动的人工智能代理，必须弥合数字和物理世界之间的逼真差距。我们如何在一个像我们所居住的世界中一样丰富多样的环境中体现代理，而不受真实硬件和控制所施加的约束？为了实现这个目标，我们引入了V-IRL: 一种平台，可以使代理在虚拟而逼真的环境中与现实世界进行可扩展的互动。我们的平台既是一个开发代理完成各种实际任务的游乐场，又是一个广阔的测试基地，用于衡量在感知、决策和与全球真实数据的互动能力等方面的进展。

    There is a sensory gulf between the Earth that humans inhabit and the digital realms in which modern AI agents are created. To develop AI agents that can sense, think, and act as flexibly as humans in real-world settings, it is imperative to bridge the realism gap between the digital and physical worlds. How can we embody agents in an environment as rich and diverse as the one we inhabit, without the constraints imposed by real hardware and control? Towards this end, we introduce V-IRL: a platform that enables agents to scalably interact with the real world in a virtual yet realistic environment. Our platform serves as a playground for developing agents that can accomplish various practical tasks and as a vast testbed for measuring progress in capabilities spanning perception, decision-making, and interaction with real-world data across the entire globe.
    
[^10]: SynthCLIP: 我们准备好开始完全合成的CLIP训练了吗？

    SynthCLIP: Are We Ready for a Fully Synthetic CLIP Training?

    [https://arxiv.org/abs/2402.01832](https://arxiv.org/abs/2402.01832)

    SynthCLIP是一种新的框架，用于训练完全合成的CLIP模型，通过生成大规模的合成图片和标题数据集，在性能上可以与在真实数据上训练的CLIP模型相媲美。

    

    我们提出了SynthCLIP，一种新颖的用于训练完全合成的CLIP模型的框架，与之前依赖真实数据的方法有着显著区别。借助最近的文本到图像生成网络和大型语言模型，我们能够生成任意规模的图像和相应的标题的合成数据集，无需人为干预。通过大规模的训练，SynthCLIP实现了与在真实数据集上训练的CLIP模型相当的性能。我们还介绍了SynthCI-30M，一个纯粹合成的数据集，包含3000万张带标题的图片。我们的代码、训练模型和生成的数据已经在https://github.com/hammoudhasan/SynthCLIP发布。

    We present SynthCLIP, a novel framework for training CLIP models with entirely synthetic text-image pairs, significantly departing from previous methods relying on real data. Leveraging recent text-to-image (TTI) generative networks and large language models (LLM), we are able to generate synthetic datasets of images and corresponding captions at any scale, with no human intervention. With training at scale, SynthCLIP achieves performance comparable to CLIP models trained on real datasets. We also introduce SynthCI-30M, a purely synthetic dataset comprising 30 million captioned images. Our code, trained models, and generated data are released at https://github.com/hammoudhasan/SynthCLIP
    
[^11]: 深度伪造检测的常识推理

    Common Sense Reasoning for Deep Fake Detection

    [https://arxiv.org/abs/2402.00126](https://arxiv.org/abs/2402.00126)

    该论文提出使用常识推理来建模深度伪造检测，通过扩展到Deepfake Detection VQA任务来模拟人类直觉，解释标记图像为真实或伪造的原因。

    

    最先进的方法依赖于通过神经网络提取的基于图像的特征进行深度伪造检测二分类。虽然这些方法在监督训练下提取了可能的伪造特征，但它们可能无法有效表示不自然的“非物理”语义面部属性 - 模糊的发际线、双眉毛、僵硬的瞳孔或不自然的皮肤着色。然而，这类面部属性通常通过常识推理对人类来说很容易感知。此外，通过显著性图提供视觉解释的基于图像的特征提取方法可能很难被人类解释。为了解决这些挑战，我们建议使用常识推理来建模深度伪造检测，并将其扩展到Deepfake Detection VQA（DD-VQA）任务，目的是模拟人类直觉来解释标记图像为真实或伪造的原因。为此，我们引入了一个新的数据集，为与深度伪造检测相关的问题提供答案。

    State-of-the-art approaches rely on image-based features extracted via neural networks for the deepfake detection binary classification. While these approaches trained in the supervised sense extract likely fake features, they may fall short in representing unnatural `non-physical' semantic facial attributes -- blurry hairlines, double eyebrows, rigid eye pupils, or unnatural skin shading. However, such facial attributes are generally easily perceived by humans via common sense reasoning. Furthermore, image-based feature extraction methods that provide visual explanation via saliency maps can be hard to be interpreted by humans. To address these challenges, we propose the use of common sense reasoning to model deepfake detection, and extend it to the Deepfake Detection VQA (DD-VQA) task with the aim to model human intuition in explaining the reason behind labeling an image as either real or fake. To this end, we introduce a new dataset that provides answers to the questions related to 
    
[^12]: Receler: 通过轻量级橡皮擦可靠地擦除文本到图像扩散模型中的概念

    Receler: Reliable Concept Erasing of Text-to-Image Diffusion Models via Lightweight Erasers

    [https://arxiv.org/abs/2311.17717](https://arxiv.org/abs/2311.17717)

    Receler提出了一种可靠概念擦除方法，通过轻量级橡皮擦实现对文本到图像扩散模型的概念擦除，具备鲁棒性和局部性，实验证明其优越性。

    

    在文本到图像扩散模型中，概念擦除旨在禁用预训练的扩散模型生成与目标概念相关的图像。为了实现可靠的概念擦除，希望具备鲁棒性和局部性的属性。前者阻止模型为任何释义或学习提示生成与目标概念相关的图像，而后者保持其生成具有非目标概念的图像的能力。在本文中，我们提出了通过轻量级橡皮擦（Receler）来实现可靠的概念擦除。它学习了一个轻量级的橡皮擦来进行概念擦除，同时通过提出的概念定位正则化和对抗提示学习方案满足上述理想特性。通过对各种概念的全面实验验证了Receler相对于先前方法的优越性。我们的代码将在接受后提供。

    arXiv:2311.17717v2 Announce Type: replace-cross  Abstract: Concept erasure in text-to-image diffusion models aims to disable pre-trained diffusion models from generating images related to a target concept. To perform reliable concept erasure, the properties of robustness and locality are desirable. The former refrains the model from producing images associated with the target concept for any paraphrased or learned prompts, while the latter preserves its ability in generating images with non-target concepts. In this paper, we propose Reliable Concept Erasing via Lightweight Erasers (Receler). It learns a lightweight Eraser to perform concept erasing while satisfying the above desirable properties by proposed concept-localized regularization and adversarial prompt learning schemes. Comprehensive experiments with various concepts verify the superiority of Receler over previous methods. Our code will be available upon acceptance.
    
[^13]: 以显性推理链和视觉问题生成推进大型多模态模型

    Advancing Large Multi-modal Models with Explicit Chain-of-Reasoning and Visual Question Generation. (arXiv:2401.10005v1 [cs.CV])

    [http://arxiv.org/abs/2401.10005](http://arxiv.org/abs/2401.10005)

    本文提出了一种新的方法，通过显性推理和问题生成，将大型多模态模型(LMM)赋予了显性推理能力，从而提高了推理过程的鲁棒性和可解释性。

    

    随着对能够解释和推理视觉内容的智能系统需求越来越高，需要开发不仅准确而且具有显性推理能力的大型多模态模型（LMMs）。本文提出了一种新颖的方法，将显性推理能力赋予LMMs，基于视觉内容和文本指导进行显性推理。我们引入了一个系统，可以提问以获取必要的知识，从而增强推理过程的鲁棒性和可解释性。我们的方法包括通过一个大型语言模型（LLM）生成的新颖数据集的开发，旨在促进思维链推理与提问机制的结合。我们设计了一个高度具有区域意识的LMM，以解决图像-文本对齐的复杂需求。该模型经历了三个阶段的训练，首先是使用大规模数据集进行大规模图像-文本对齐，接下来是通过显式推理的问题生成阶段。

    The increasing demand for intelligent systems capable of interpreting and reasoning about visual content requires the development of Large Multi-Modal Models (LMMs) that are not only accurate but also have explicit reasoning capabilities. This paper presents a novel approach to imbue an LMM with the ability to conduct explicit reasoning based on visual content and textual instructions. We introduce a system that can ask a question to acquire necessary knowledge, thereby enhancing the robustness and explicability of the reasoning process. Our method comprises the development of a novel dataset generated by a Large Language Model (LLM), designed to promote chain-of-thought reasoning combined with a question-asking mechanism. We designed an LMM, which has high capabilities on region awareness to address the intricate requirements of image-text alignment. The model undergoes a three-stage training phase, starting with large-scale image-text alignment using a large-scale datasets, followed 
    
[^14]: 使用量化扩散模型的内存高效个性化

    Memory-Efficient Personalization using Quantized Diffusion Model. (arXiv:2401.04339v1 [cs.CV])

    [http://arxiv.org/abs/2401.04339](http://arxiv.org/abs/2401.04339)

    本文研究了使用量化扩散模型进行内存高效个性化的方法，提出了两个策略来解决基线模型中主题和提示质量之间的权衡问题。

    

    亿级参数扩散模型（如Stable Diffusion XL、Imagen和Dall-E3）的崛起显著推动了生成型人工智能领域的发展。然而，由于资源需求高和推理速度慢，它们的大规模性质在微调和部署中带来了挑战。本文探索了对量化扩散模型进行微调的相对未开发但有前景的领域。我们通过定制三个模型（用于微调量化参数的PEQA，用于后期量化的Q-Diffusion和个性化的DreamBooth），建立了一个强大的基线模型。我们的分析揭示了基线模型中主题和提示质量之间的明显权衡。为了解决这些问题，我们引入了两个策略，灵感来自于扩散模型中不同时间步长的不同角色：S1在选择的时间间隔内仅优化一组微调参数，S2创建多个微调参数组，每个组专门用于不同的时间步长间隔。

    The rise of billion-parameter diffusion models like Stable Diffusion XL, Imagen, and Dall-E3 markedly advances the field of generative AI. However, their large-scale nature poses challenges in fine-tuning and deployment due to high resource demands and slow inference speed. This paper ventures into the relatively unexplored yet promising realm of fine-tuning quantized diffusion models. We establish a strong baseline by customizing three models: PEQA for fine-tuning quantization parameters, Q-Diffusion for post-training quantization, and DreamBooth for personalization. Our analysis reveals a notable trade-off between subject and prompt fidelity within the baseline model. To address these issues, we introduce two strategies, inspired by the distinct roles of different timesteps in diffusion models: S1 optimizing a single set of fine-tuning parameters exclusively at selected intervals, and S2 creating multiple fine-tuning parameter sets, each specialized for different timestep intervals. 
    
[^15]: 用CLIP实现真实的无监督微调

    Towards Realistic Unsupervised Fine-tuning with CLIP. (arXiv:2308.12919v1 [cs.CV])

    [http://arxiv.org/abs/2308.12919](http://arxiv.org/abs/2308.12919)

    本论文针对无监督微调中可能出现的未知类别和超出分布范围的问题，提出了一种称为UEO的简单、高效、有效的微调方法，该方法能够同时提高对超出分布样本的检测能力和预定义类别实例的识别能力。

    

    视觉-语言模型（VLM）如CLIP的出现推动了人们在下游监督学习任务中的应用研究。尽管一些之前的研究探索了CLIP的无监督微调，但它们常常依赖于与真实标签相关的类名等先验知识。本文中，我们探讨了一种真实的无监督微调情景，假设未标记的数据可能包含来自未知类别的超出分布范围的样本。此外，我们强调了在预定义类标签的识别之外，同时提高对超出分布检测能力的重要性。为了解决这个问题，我们提出了一种简单、高效、有效的微调方法，称为Universal Entropy Optimization (UEO)。UEO利用样本级置信度，以近似方式最小化置信实例的条件熵并最大化边缘熵。

    The emergence of vision-language models (VLMs), such as CLIP, has spurred a significant research effort towards their application for downstream supervised learning tasks. Although some previous studies have explored the unsupervised fine-tuning of CLIP, they often rely on prior knowledge in the form of class names associated with ground truth labels. In this paper, we delve into a realistic unsupervised fine-tuning scenario by assuming that the unlabeled data might contain out-of-distribution samples from unknown classes. Furthermore, we emphasize the importance of simultaneously enhancing out-of-distribution detection capabilities alongside the recognition of instances associated with predefined class labels.  To tackle this problem, we present a simple, efficient, and effective fine-tuning approach called Universal Entropy Optimization (UEO). UEO leverages sample-level confidence to approximately minimize the conditional entropy of confident instances and maximize the marginal entro
    
[^16]: 对高长尾视觉识别中的logit进行高斯形式调整

    Adjusting Logit in Gaussian Form for Long-Tailed Visual Recognition. (arXiv:2305.10648v1 [cs.CV])

    [http://arxiv.org/abs/2305.10648](http://arxiv.org/abs/2305.10648)

    本文提出了一种特征增强方法和两种logit调整方法，用于解决长尾视觉识别中的类别不平衡问题。实验结果表明，该方法在CIFAR和ImageNet长尾数据集上表现优于其他最先进的方法。

    

    现实世界中的数据往往具有长尾分布。对于这种数据，由于难以正确分类尾部类别，深度神经网络的学习变得具有挑战性。在文献中，已有一些方法通过减少分类器偏差来解决这个问题，前提是用长尾数据获得的特征足够代表。然而，我们发现直接在长尾数据上训练会导致不均匀的嵌入空间。也就是说，头类的嵌入空间严重压缩尾类，这对于后续的分类器学习是不利的。因此，本文从特征水平的角度研究了长尾视觉识别问题。我们引入了特征增强来平衡嵌入分布。不同类别的特征以高斯形式具有不同振幅的扰动。基于这些扰动的特征，提出了两种新的logit调整方法来提高尾部类别的准确性。实验结果显示，我们的方法在CIFAR和ImageNet长尾数据集上优于其他最先进的方法。

    It is not uncommon that real-world data are distributed with a long tail. For such data, the learning of deep neural networks becomes challenging because it is hard to classify tail classes correctly. In the literature, several existing methods have addressed this problem by reducing classifier bias provided that the features obtained with long-tailed data are representative enough. However, we find that training directly on long-tailed data leads to uneven embedding space. That is, the embedding space of head classes severely compresses that of tail classes, which is not conducive to subsequent classifier learning. %further improving model performance. This paper therefore studies the problem of long-tailed visual recognition from the perspective of feature level. We introduce feature augmentation to balance the embedding distribution. The features of different classes are perturbed with varying amplitudes in Gaussian form. Based on these perturbed features, two novel logit adjustment
    
[^17]: 通向自由计算架构: 关于深度学习生成元宇宙虚拟建筑的综合调研

    Towards Computational Architecture of Liberty: A Comprehensive Survey on Deep Learning for Generating Virtual Architecture in the Metaverse. (arXiv:2305.00510v1 [cs.HC])

    [http://arxiv.org/abs/2305.00510](http://arxiv.org/abs/2305.00510)

    本文综述了当前最新的深度学习生成模型用于建筑形式的3D对象生成方法，强调了尚未充分探讨的问题，并提出了未来研究的重点议程。

    

    利用深度学习的3D形状生成技术正在受到计算机视觉和建筑设计两方的越来越多的关注。本综合调查旨在调查和比较当前最新的基于深度生成模型（DGMs）的3D对象生成方法，包括生成对抗网络（GANs）、变分自动编码器（VAEs）、3D感知图像和扩散模型。我们调查了187篇文章(占2018-2022年间发表文章的80.7%)，以回顾在虚拟环境下建筑生成可能性的领域，限于建筑形式。我们提供了建筑研究、虚拟环境和相关技术方法的概述，接着回顾了离散体素生成、由2D图像生成的3D模型以及条件参数的最近趋势。我们强调了3D生成和参数化控制中尚未充分探讨的问题值得进一步研究。此外，我们推测包括生成多样性、新型输出和嵌入式构建等四个研究议程可能会成为未来研究的重点。

    3D shape generation techniques utilizing deep learning are increasing attention from both computer vision and architectural design. This survey focuses on investigating and comparing the current latest approaches to 3D object generation with deep generative models (DGMs), including Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), 3D-aware images, and diffusion models. We discuss 187 articles (80.7% of articles published between 2018-2022) to review the field of generated possibilities of architecture in virtual environments, limited to the architecture form. We provide an overview of architectural research, virtual environment, and related technical approaches, followed by a review of recent trends in discrete voxel generation, 3D models generated from 2D images, and conditional parameters. We highlight under-explored issues in 3D generation and parameterized control that is worth further investigation. Moreover, we speculate that four research agendas including
    
[^18]: SIFT: 稀疏等FLOP转换以最大限度提高训练效率

    SIFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency. (arXiv:2303.11525v1 [cs.LG])

    [http://arxiv.org/abs/2303.11525](http://arxiv.org/abs/2303.11525)

    本研究提出了一种名为SIFT的方法，用于提高深度神经网络的训练效率、准确性和表示能力，通过稀疏等FLOP转换，缩短训练时间。

    

    最近的研究探索了使用权重稀疏性来改善深度神经网络（DNN）的训练效率（与训练FLOPS相关的测试准确性）。 这些工作旨在减少训练FLOP，但使用稀疏权重进行训练通常会导致准确性损失或需要更长的训练周期，使得结果的训练效率不够清晰。 相比之下，我们专注于使用稀疏性提高准确性，同时使用与密集模型相同的FLOPS，并通过更高的准确性展示训练效率提高。 在本文中，我们介绍了SIFT，一组用作密集层的即插即用替代品来提高其表示能力和FLOP效率的稀疏等FLOP转换。 每个转换都由一个单一参数（稀疏级别）参数化，并提供更大的搜索空间以找到最佳的稀疏掩膜。

    Recent works have explored the use of weight sparsity to improve the training efficiency (test accuracy w.r.t training FLOPs) of deep neural networks (DNNs). These works aim to reduce training FLOPs but training with sparse weights often leads to accuracy loss or requires longer train schedules, making the resulting training efficiency less clear. In contrast, we focus on using sparsity to increase accuracy while using the same FLOPS as the dense model and show training efficiency gains through higher accuracy. In this work, we introduce SIFT, a family of Sparse Iso-FLOP Transformations which are used as drop-in replacements for dense layers to improve their representational capacity and FLOP efficiency. Each transformation is parameterized by a single parameter (sparsity level) and provides a larger search space to find optimal sparse masks. Without changing any training hyperparameters, replacing dense layers with SIFT leads to significant improvements across computer vision (CV) and
    

