# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Homogeneous Tokenizer Matters: Homogeneous Visual Tokenizer for Remote Sensing Image Understanding](https://arxiv.org/abs/2403.18593) | 通过定义语义独立区域（SIRs）并设计HOmogeneous视觉tOKenizer (HOOK)，实现了使用有意义的基本元素来加强遥感图像理解。 |
| [^2] | [Hyperparameters in Continual Learning: a Reality Check](https://arxiv.org/abs/2403.09066) | 超参数对于连续学习的重要性被强调，提出了一个涉及超参数调整和评估阶段的评估协议。 |
| [^3] | [FogGuard: guarding YOLO against fog using perceptual loss](https://arxiv.org/abs/2403.08939) | FogGuard提出了一种针对雾天气条件挑战的新型雾感知目标检测网络，通过微调数据收集的方法来提高目标检测算法在恶劣天气条件下的可靠性。 |
| [^4] | [Twisting Lids Off with Two Hands](https://arxiv.org/abs/2403.02338) | 深度强化学习结合仿真到真实世界的转移为解决物体操纵问题提供了有力支持 |
| [^5] | [Improve Robustness of Eye Disease Detection by including Learnable Probabilistic Discrete Latent Variables into Machine Learning Models](https://arxiv.org/abs/2402.16865) | 通过引入可学习的概率离散潜变量，该研究提出了一种新颖的眼部疾病检测方法，利用生成流网络来学习眼底图像中眼部疾病的后验分布，提高了鲁棒性和泛化能力。 |
| [^6] | [Increasing SAM Zero-Shot Performance on Multimodal Medical Images Using GPT-4 Generated Descriptive Prompts Without Human Annotation](https://arxiv.org/abs/2402.15759) | 使用GPT-4生成描述性提示，提高了多模态医学图像上的SAM零样本分割性能，无需人工标注。 |
| [^7] | [Geometry-Informed Neural Networks](https://arxiv.org/abs/2402.14009) | GINNs提出了一种新颖的几何信息神经网络范式，可以在几何任务中生成多样的解决方案，无需训练数据，采用显式多样性损失以及可微损失来减轻模态坍缩，并在实验中展示了其在各种复杂性场景中的高效性。 |
| [^8] | [CodaMal: Contrastive Domain Adaptation for Malaria Detection in Low-Cost Microscopes](https://arxiv.org/abs/2402.10478) | 提出了CodaMal框架，实现了低成本显微镜下疟疾检测的对比域自适应，解决了HCM和LCM图像之间的域差异问题 |
| [^9] | [Short-Form Videos and Mental Health: A Knowledge-Guided Multimodal Neural Topic Model](https://arxiv.org/abs/2402.10045) | 这项研究针对短视频对观众心理健康的抑郁影响问题，开发了一种基于医学知识的多模态神经主题模型，以预测其影响并采取相应的干预措施。 |
| [^10] | [ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling](https://arxiv.org/abs/2402.06118) | ViGoR通过细粒度奖励建模提高了大型视觉语言模型在视觉对接方面的性能，通过人工评估和自动化方法有效地解决了视觉对接中的误差问题。 |
| [^11] | [Improving Adversarial Attacks on Latent Diffusion Model](https://arxiv.org/abs/2310.04687) | 提出了一种改进 Latent Diffusion Model 的对抗攻击方法 ACE，其通过统一模式的额外误差来促使模型学习特定的偏差，从而胜过了目前最先进的方法 |
| [^12] | [Q&A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge.](http://arxiv.org/abs/2401.10712) | 本论文提出了一种叫做Q&A提示的方法，通过挖掘图像中的问题-回答对来发现丰富的视觉线索，以帮助AI模型更好地理解复杂视觉问题，提高跨模态推理能力。 |
| [^13] | [Demystifying Visual Features of Movie Posters for Multi-Label Genre Identification.](http://arxiv.org/abs/2309.12022) | 本研究通过分析电影海报图像，解密了电影海报的视觉特征，并提出了一种自动化的多标签电影类型识别方法，无需使用其他文本或元数据信息，具有推广和营销电影的实际应用意义。 |
| [^14] | [AR-TTA: A Simple Method for Real-World Continual Test-Time Adaptation.](http://arxiv.org/abs/2309.10109) | AR-TTA提出了一种简单的方法用于真实世界连续测试时间自适应。通过将内存缓冲区纳入自训练框架，并根据数据流的强度进行动态适应，提高了模型的稳定性。 |
| [^15] | [Training Data Protection with Compositional Diffusion Models.](http://arxiv.org/abs/2308.01937) | 使用分区扩散模型（CDM）训练不同的扩散模型，并在推断时任意组合它们，实现了训练数据保护和选择性遗忘，同时还可以根据用户访问权限提供定制模型。 |
| [^16] | [Exploring Annotation-free Image Captioning with Retrieval-augmented Pseudo Sentence Generation.](http://arxiv.org/abs/2307.14750) | 本文提出了一种新的无注释图像字幕生成的策略，利用大规模预训练模型的先验知识作为监督，并整合检索过程以进一步增强其效力。该方法能够从不匹配的语料库中检索相关的短区域描述，并利用其生成多样的句子。 |
| [^17] | [Loss Functions and Metrics in Deep Learning. A Review.](http://arxiv.org/abs/2307.02694) | 本文回顾了深度学习中最常见的损失函数和性能测量方法，旨在帮助从业者选择最适合其特定任务的方法。 |
| [^18] | [S.T.A.R.-Track: Latent Motion Models for End-to-End 3D Object Tracking with Adaptive Spatio-Temporal Appearance Representations.](http://arxiv.org/abs/2306.17602) | 本文提出了S.T.A.R.-Track，一个采用物体为中心的Transformer框架，用于端到端3D物体跟踪。通过新颖的潜在运动模型和学习型跟踪嵌入，该框架能够准确建模物体的几何运动和变化，并在nuScenes数据集上取得了优秀的性能。 |
| [^19] | [DiMSam: Diffusion Models as Samplers for Task and Motion Planning under Partial Observability.](http://arxiv.org/abs/2306.13196) | 本文提出了一种使用扩散模型作为采样器的任务和动作规划方法，在部分可观测下能够实现长周期受约束的操作计划。 |
| [^20] | [Decoupled Kullback-Leibler Divergence Loss.](http://arxiv.org/abs/2305.13948) | 本文提出了改进的KL散度损失函数，通过解决解耦式KL散度损失函数的对称性限制和引入全局信息来提升性能，在CIFAR-10/100和ImageNet数据集上展示了其在对抗训练和知识蒸馏任务中的优越表现。 |
| [^21] | [Facial recognition technology can expose political orientation from facial images even when controlling for demographics and self-presentation.](http://arxiv.org/abs/2303.16343) | 本研究使用面部识别技术，通过特定的面部特征发现了人们的政治取向，甚至可以从自然图像中推广。这种预测的精度比人类评分者高，相当于一些工作面试的预测效果。 |

# 详细

[^1]: 均匀分词器的重要性：用于遥感图像理解的均匀视觉分词器

    Homogeneous Tokenizer Matters: Homogeneous Visual Tokenizer for Remote Sensing Image Understanding

    [https://arxiv.org/abs/2403.18593](https://arxiv.org/abs/2403.18593)

    通过定义语义独立区域（SIRs）并设计HOmogeneous视觉tOKenizer (HOOK)，实现了使用有意义的基本元素来加强遥感图像理解。

    

    标记器作为大型模型的基本组件之一，长期以来在视觉任务中被忽视甚至误解。大语言模型具有强大理解能力的一个关键因素是自然语言标记器利用有意义的词或子词作为语言的基本元素。相比之下，以基于补丁的方法如Patch Embed为代表的主流视觉标记器依赖于无意义的矩形补丁作为视觉的基本元素，这不能像语言中的词或子词一样有效地发挥作用。从标记器的本质出发，我们为视觉定义了语义独立区域（SIRs）。我们设计了一个简单的HOmogeneous视觉tOKenizer: HOOK。HOOK主要由两个模块组成：物体感知模块（OPM）和物体矢量化模块（OVM）。为实现均匀性，OPM将图像分割为4*4像素种子，然后利用注意力机制来。

    arXiv:2403.18593v1 Announce Type: cross  Abstract: The tokenizer, as one of the fundamental components of large models, has long been overlooked or even misunderstood in visual tasks. One key factor of the great comprehension power of the large language model is that natural language tokenizers utilize meaningful words or subwords as the basic elements of language. In contrast, mainstream visual tokenizers, represented by patch-based methods such as Patch Embed, rely on meaningless rectangular patches as basic elements of vision, which cannot serve as effectively as words or subwords in language. Starting from the essence of the tokenizer, we defined semantically independent regions (SIRs) for vision. We designed a simple HOmogeneous visual tOKenizer: HOOK. HOOK mainly consists of two modules: the Object Perception Module (OPM) and the Object Vectorization Module (OVM). To achieve homogeneity, the OPM splits the image into 4*4 pixel seeds and then utilizes the attention mechanism to pe
    
[^2]: Continual Learning中的超参数：现实检验

    Hyperparameters in Continual Learning: a Reality Check

    [https://arxiv.org/abs/2403.09066](https://arxiv.org/abs/2403.09066)

    超参数对于连续学习的重要性被强调，提出了一个涉及超参数调整和评估阶段的评估协议。

    

    不同的连续学习（CL）算法旨在在CL过程中有效地缓解稳定性和可塑性之间的权衡，为了实现这一目标，调整每种算法的适当超参数是必不可少的。本文主张现行的评估协议既不切实际，也无法有效评估连续学习算法的能力。

    arXiv:2403.09066v1 Announce Type: new  Abstract: Various algorithms for continual learning (CL) have been designed with the goal of effectively alleviating the trade-off between stability and plasticity during the CL process. To achieve this goal, tuning appropriate hyperparameters for each algorithm is essential. As an evaluation protocol, it has been common practice to train a CL algorithm using diverse hyperparameter values on a CL scenario constructed with a benchmark dataset. Subsequently, the best performance attained with the optimal hyperparameter value serves as the criterion for evaluating the CL algorithm. In this paper, we contend that this evaluation protocol is not only impractical but also incapable of effectively assessing the CL capability of a CL algorithm. Returning to the fundamental principles of model evaluation in machine learning, we propose an evaluation protocol that involves Hyperparameter Tuning and Evaluation phases. Those phases consist of different datase
    
[^3]: FogGuard: 使用感知损失保护YOLO免受雾霾影响

    FogGuard: guarding YOLO against fog using perceptual loss

    [https://arxiv.org/abs/2403.08939](https://arxiv.org/abs/2403.08939)

    FogGuard提出了一种针对雾天气条件挑战的新型雾感知目标检测网络，通过微调数据收集的方法来提高目标检测算法在恶劣天气条件下的可靠性。

    

    在本文中，我们提出了一种新颖的雾感知目标检测网络，称为FogGuard，旨在解决雾天气条件带来的挑战。自动驾驶系统严重依赖准确的目标检测算法，但恶劣的天气条件会显著影响深度神经网络（DNN）的可靠性。现有方法可分为两类，1）图像增强（如IA-YOLO）和2）基于领域适应的方法。图像增强技术试图生成无雾图像，然而，从有雾图像中恢复无雾图像比在有雾图像中检测对象要困难得多。另一方面，基于领域适应的方法没有利用目标领域中的标记数据集。这两类方法都在尝试解决问题的更难版本。我们的方法建立在对原始标注数据的微调之上。

    arXiv:2403.08939v1 Announce Type: cross  Abstract: In this paper, we present a novel fog-aware object detection network called FogGuard, designed to address the challenges posed by foggy weather conditions. Autonomous driving systems heavily rely on accurate object detection algorithms, but adverse weather conditions can significantly impact the reliability of deep neural networks (DNNs).   Existing approaches fall into two main categories, 1) image enhancement such as IA-YOLO 2) domain adaptation based approaches. Image enhancement based techniques attempt to generate fog-free image. However, retrieving a fogless image from a foggy image is a much harder problem than detecting objects in a foggy image. Domain-adaptation based approaches, on the other hand, do not make use of labelled datasets in the target domain. Both categories of approaches are attempting to solve a harder version of the problem. Our approach builds over fine-tuning on the   Our framework is specifically designed t
    
[^4]: 用双手扭开盖子

    Twisting Lids Off with Two Hands

    [https://arxiv.org/abs/2403.02338](https://arxiv.org/abs/2403.02338)

    深度强化学习结合仿真到真实世界的转移为解决物体操纵问题提供了有力支持

    

    用两只多指手臂操纵物体一直是机器人领域的一项长期挑战，原因在于许多操纵任务的丰富接触性质以及协调高维度双手系统固有的复杂性。在这项工作中，我们考虑了使用两只手扭开各种瓶子盖的问题，并展示出使用深度强化学习在仿真中训练的策略可以有效地转移到现实世界。通过对物理建模、实时感知和奖励设计的新工程见解，该策略展示了一般化能力，能够贯穿各种看不见的物体，展示出动态和灵巧的行为。我们的发现证明了深度强化学习结合仿真到真实世界的转移仍然是解决前所未有复杂问题的操纵问题的一个有前途的方法。

    arXiv:2403.02338v1 Announce Type: cross  Abstract: Manipulating objects with two multi-fingered hands has been a long-standing challenge in robotics, attributed to the contact-rich nature of many manipulation tasks and the complexity inherent in coordinating a high-dimensional bimanual system. In this work, we consider the problem of twisting lids of various bottle-like objects with two hands, and demonstrate that policies trained in simulation using deep reinforcement learning can be effectively transferred to the real world. With novel engineering insights into physical modeling, real-time perception, and reward design, the policy demonstrates generalization capabilities across a diverse set of unseen objects, showcasing dynamic and dexterous behaviors. Our findings serve as compelling evidence that deep reinforcement learning combined with sim-to-real transfer remains a promising approach for addressing manipulation problems of unprecedented complexity.
    
[^5]: 通过将可学习的概率离散潜变量引入机器学习模型来提高眼部疾病检测的鲁棒性

    Improve Robustness of Eye Disease Detection by including Learnable Probabilistic Discrete Latent Variables into Machine Learning Models

    [https://arxiv.org/abs/2402.16865](https://arxiv.org/abs/2402.16865)

    通过引入可学习的概率离散潜变量，该研究提出了一种新颖的眼部疾病检测方法，利用生成流网络来学习眼底图像中眼部疾病的后验分布，提高了鲁棒性和泛化能力。

    

    眼部疾病从糖尿病性视网膜病变到青光眼等，由于其高发病率和可能导致视力损害，构成了一个重要的公共卫生挑战。及早和准确的诊断对于有效治疗和管理至关重要。近年来，深度学习模型已经成为分析医学图像（包括眼部图像）的强大工具。然而，模型的解释性和不确定性估计方面仍然存在挑战，这对临床决策至关重要。本研究引入了GFlowOut的新颖应用，利用生成流网络（GFlowNets）的概率框架来学习关于辍学掩码的后验分布，用于使用眼底图像对眼部疾病进行分类和分析。我们开发了一种稳健且具有普适性的方法，利用以ResNet18和ViT模型为主干的GFlowOut来识别各种眼部状况。

    arXiv:2402.16865v1 Announce Type: cross  Abstract: Ocular diseases, ranging from diabetic retinopathy to glaucoma, present a significant public health challenge due to their prevalence and potential for causing vision impairment. Early and accurate diagnosis is crucial for effective treatment and management.In recent years, deep learning models have emerged as powerful tools for analysing medical images, including ocular imaging . However, challenges persist in model interpretability and uncertainty estimation, which are critical for clinical decision-making. This study introduces a novel application of GFlowOut, leveraging the probabilistic framework of Generative Flow Networks (GFlowNets) to learn the posterior distribution over dropout masks, for the classification and analysis of ocular diseases using eye fundus images. We develop a robust and generalizable method that utilizes GFlowOut integrated with ResNet18 and ViT models as backbone in identifying various ocular conditions. Th
    
[^6]: 使用GPT-4生成描述性提示提高多模态医学图像上的SAM零样本性能而无需人工标注

    Increasing SAM Zero-Shot Performance on Multimodal Medical Images Using GPT-4 Generated Descriptive Prompts Without Human Annotation

    [https://arxiv.org/abs/2402.15759](https://arxiv.org/abs/2402.15759)

    使用GPT-4生成描述性提示，提高了多模态医学图像上的SAM零样本分割性能，无需人工标注。

    

    本研究开发并评估了一种新型的多模态医学图像零样本分割算法，命名为文本-视觉-提示SAM（TV-SAM），无需任何手动标注。TV-SAM融合并整合了大型语言模型GPT-4、视觉语言模型GLIP和“Segment Anything Model”（SAM），从医学图像中自动生成描述性文本提示和视觉边界框提示，从而增强了SAM用于零样本分割。在七个公共数据集上进行了全面评估，涵盖八种成像模式，证明TV-SAM可以有效地跨各种模式分割未见过的目标而无需额外训练，明显优于SAM AUTO和GSAM, 与金标准边界框提示的SAM BBOX性能基本匹敌，并在特定数据集（如ISIC和WBC）上超越了现有技术水平。研究表明，TV-SAM是一种有效的多模态

    arXiv:2402.15759v1 Announce Type: cross  Abstract: This study develops and evaluates a novel multimodal medical image zero-shot segmentation algorithm named Text-Visual-Prompt SAM (TV-SAM) without any manual annotations. TV-SAM incorporates and integrates large language model GPT-4, Vision Language Model GLIP, and Segment Anything Model (SAM), to autonomously generate descriptive text prompts and visual bounding box prompts from medical images, thereby enhancing SAM for zero-shot segmentation. Comprehensive evaluations are implemented on seven public datasets encompassing eight imaging modalities to demonstrate that TV-SAM can effectively segment unseen targets across various modalities without additional training, significantly outperforming SAM AUTO and GSAM, closely matching the performance of SAM BBOX with gold standard bounding box prompts, and surpassing the state-of-the-art on specific datasets like ISIC and WBC. The study indicates that TV-SAM serves as an effective multimodal 
    
[^7]: 几何信息神经网络

    Geometry-Informed Neural Networks

    [https://arxiv.org/abs/2402.14009](https://arxiv.org/abs/2402.14009)

    GINNs提出了一种新颖的几何信息神经网络范式，可以在几何任务中生成多样的解决方案，无需训练数据，采用显式多样性损失以及可微损失来减轻模态坍缩，并在实验中展示了其在各种复杂性场景中的高效性。

    

    我们引入了几何信息神经网络（GINNs）的概念，涵盖了（i）在几何约束下学习，（ii）神经场作为合适的表示，（iii）生成在几何任务中经常遇到的欠定系统的多样解决方案。值得注意的是，GINN的构建不需要训练数据，因此可以被纯约束驱动地视为生成建模。我们增加了显式的多样性损失来减轻模态坍缩。我们考虑了几种约束，特别是组件的连通性，我们通过莫尔斯理论将其转化为可微损失。在实验中，我们展示了在不断增加复杂性的二维和三维场景中，GINN学习范式的高效性。

    arXiv:2402.14009v1 Announce Type: new  Abstract: We introduce the concept of geometry-informed neural networks (GINNs), which encompass (i) learning under geometric constraints, (ii) neural fields as a suitable representation, and (iii) generating diverse solutions to under-determined systems often encountered in geometric tasks. Notably, the GINN formulation does not require training data, and as such can be considered generative modeling driven purely by constraints. We add an explicit diversity loss to mitigate mode collapse. We consider several constraints, in particular, the connectedness of components which we convert to a differentiable loss through Morse theory. Experimentally, we demonstrate the efficacy of the GINN learning paradigm across a range of two and three-dimensional scenarios with increasing levels of complexity.
    
[^8]: CodaMal：低成本显微镜下的疟疾检测的对比域自适应

    CodaMal: Contrastive Domain Adaptation for Malaria Detection in Low-Cost Microscopes

    [https://arxiv.org/abs/2402.10478](https://arxiv.org/abs/2402.10478)

    提出了CodaMal框架，实现了低成本显微镜下疟疾检测的对比域自适应，解决了HCM和LCM图像之间的域差异问题

    

    疟疾是全球重大健康问题，其诊断需要可扩展的解决方案，能够有效地处理低成本显微镜(LCM)下的显微图像。基于深度学习的方法在从显微图像中进行计算机辅助诊断方面取得了成功。然而，这些方法需要标注的显现出受疟疾寄生虫影响的细胞及其生命周期阶段的图像。与从高成本显微镜(HCM)中标注图像相比，从LCM中标注图像显著增加了医学专家的负担。因此，一个实际的解决方案应该在HCM图像上训练，能够在LCM图像上测试时具有良好的泛化能力。在本作品中，我们提出了一个名为CodaMal（对比域自适应用于疟疾检测）的端到端学习框架。

    arXiv:2402.10478v1 Announce Type: cross  Abstract: Malaria is a major health issue worldwide, and its diagnosis requires scalable solutions that can work effectively with low-cost microscopes (LCM). Deep learning-based methods have shown success in computer-aided diagnosis from microscopic images. However, these methods need annotated images that show cells affected by malaria parasites and their life stages. Annotating images from LCM significantly increases the burden on medical experts compared to annotating images from high-cost microscopes (HCM). For this reason, a practical solution would be trained on HCM images which should generalize well on LCM images during testing. While earlier methods adopted a multi-stage learning process, they did not offer an end-to-end approach. In this work, we present an end-to-end learning framework, named CodaMal (Contrastive Domain Adpation for Malaria). In order to bridge the gap between HCM (training) and LCM (testing), we propose a domain adap
    
[^9]: 短视频和心理健康：基于知识导向的多模态神经主题模型

    Short-Form Videos and Mental Health: A Knowledge-Guided Multimodal Neural Topic Model

    [https://arxiv.org/abs/2402.10045](https://arxiv.org/abs/2402.10045)

    这项研究针对短视频对观众心理健康的抑郁影响问题，开发了一种基于医学知识的多模态神经主题模型，以预测其影响并采取相应的干预措施。

    

    短视频正试图重新塑造整个社交媒体景观，然而专家们对其对观众的抑郁影响感到极度担忧，这一点已由医学研究证明。为了防止广泛影响，各平台渴望预测这些视频对观众心理健康的影响，从而采取干预措施，比如修订推荐算法和显示观众慎重选择。然而，现有的预测方法缺乏与抑郁症的临床证实的外部环境因素相关的医学知识。为了考虑这样的医学知识，我们采用了一种新兴的方法论学科——种子神经主题模型（NTMs）。然而，现有的种子NTMs存在单一来源主题、未知主题来源、模糊的种子监督和次优的收敛等局限性。为了解决这些挑战，我们开发了一种新颖的基于知识指导的多模态神经主题模型（Knowledg...（待补充）

    arXiv:2402.10045v1 Announce Type: cross  Abstract: While short-form videos head to reshape the entire social media landscape, experts are exceedingly worried about their depressive impacts on viewers, as evidenced by medical studies. To prevent widespread consequences, platforms are eager to predict these videos' impact on viewers' mental health. Subsequently, they can take intervention measures, such as revising recommendation algorithms and displaying viewer discretion. Nevertheless, applicable predictive methods lack relevance to well-established medical knowledge, which outlines clinically proven external and environmental factors of depression. To account for such medical knowledge, we resort to an emergent methodological discipline, seeded Neural Topic Models (NTMs). However, existing seeded NTMs suffer from the limitations of single-origin topics, unknown topic sources, unclear seed supervision, and suboptimal convergence. To address those challenges, we develop a novel Knowledg
    
[^10]: ViGoR：通过细粒度奖励建模改进大规模视觉语言模型的视觉对接

    ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling

    [https://arxiv.org/abs/2402.06118](https://arxiv.org/abs/2402.06118)

    ViGoR通过细粒度奖励建模提高了大型视觉语言模型在视觉对接方面的性能，通过人工评估和自动化方法有效地解决了视觉对接中的误差问题。

    

    通过将自然语言理解、大语言模型的生成能力和广泛知识与图像感知相结合，最近的大规模视觉语言模型（LVLMs）在现实世界中展示了前所未有的推理能力。然而，生成的文本往往在视觉输入中存在不准确的对接，导致错误，如产生幻觉的不存在场景元素、遗漏重要的场景部分，以及推测对象之间的属性和关系时出现错误。为了解决这些问题，我们引入了一个新颖的框架ViGoR（通过细粒度奖励建模进行视觉对接），它利用细粒度奖励建模来显著提升基于预训练基线的LVLMs的视觉对接能力。这种改进通过使用比完全监督更便宜的人工评估和自动化方法高效实现。我们通过多个基准测试的多个指标展示了我们方法的有效性。

    By combining natural language understanding and the generation capabilities and breadth of knowledge of large language models with image perception, recent large vision language models (LVLMs) have shown unprecedented reasoning capabilities in the real world. However, the generated text often suffers from inaccurate grounding in the visual input, resulting in errors such as hallucinating nonexistent scene elements, missing significant parts of the scene, and inferring incorrect attributes and relationships between objects. To address these issues, we introduce a novel framework, ViGoR (Visual Grounding Through Fine-Grained Reward Modeling) that utilizes fine-grained reward modeling to significantly enhance the visual grounding of LVLMs over pre-trained baselines. This improvement is efficiently achieved using much cheaper human evaluations instead of full supervisions, as well as automated methods. We show the effectiveness of our approach through numerous metrics on several benchmarks
    
[^11]: 改进潜在扩散模型的对抗攻击

    Improving Adversarial Attacks on Latent Diffusion Model

    [https://arxiv.org/abs/2310.04687](https://arxiv.org/abs/2310.04687)

    提出了一种改进 Latent Diffusion Model 的对抗攻击方法 ACE，其通过统一模式的额外误差来促使模型学习特定的偏差，从而胜过了目前最先进的方法

    

    对 Latent Diffusion Model (LDM)，这种最先进的图像生成模型，进行对抗攻击已经被证明是有效防止 LDM 在未经授权的图像上进行恶意微调的保护手段。我们展示了这些攻击会对 LDM 预测的对抗样本的评分函数添加额外的误差。在这些对抗样本上进行微调的 LDM 学习通过一个偏差降低误差，从而遭受攻击并使用偏差预测评分函数。基于这一动态，我们提出了通过一致得分函数错误进行攻击（ACE）来改进 LDM 的对抗攻击。ACE 统一了添加到预测得分函数的额外误差的模式。这促使微调的 LDM 学习与对评分函数进行预测的偏差学习相同的模式。然后我们引入一个精心设计的模式来改进攻击。我们的方法在对 LDM 的对抗攻击中胜过了最先进的方法。

    arXiv:2310.04687v3 Announce Type: replace-cross  Abstract: Adversarial attacks on Latent Diffusion Model (LDM), the state-of-the-art image generative model, have been adopted as effective protection against malicious finetuning of LDM on unauthorized images. We show that these attacks add an extra error to the score function of adversarial examples predicted by LDM. LDM finetuned on these adversarial examples learns to lower the error by a bias, from which the model is attacked and predicts the score function with biases.   Based on the dynamics, we propose to improve the adversarial attack on LDM by Attacking with Consistent score-function Errors (ACE). ACE unifies the pattern of the extra error added to the predicted score function. This induces the finetuned LDM to learn the same pattern as a bias in predicting the score function. We then introduce a well-crafted pattern to improve the attack. Our method outperforms state-of-the-art methods in adversarial attacks on LDM.
    
[^12]: Q&A提示：通过挖掘问题-回答提示来发现丰富的视觉线索，以满足对多样世界知识的视觉问答的需求

    Q&A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge. (arXiv:2401.10712v1 [cs.CV])

    [http://arxiv.org/abs/2401.10712](http://arxiv.org/abs/2401.10712)

    本论文提出了一种叫做Q&A提示的方法，通过挖掘图像中的问题-回答对来发现丰富的视觉线索，以帮助AI模型更好地理解复杂视觉问题，提高跨模态推理能力。

    

    随着多模态大型语言模型的突破，回答需要高级推理能力和世界知识的复杂视觉问题比以往任何时候都更重要。然而，为AI模型配备强大的跨模态推理能力仍然具有挑战性，因为人类的认知方案尚未系统地被理解。在本文中，我们相信，如果我们能尽可能收集给定图像中的视觉线索，我们将能更准确地识别图像，更好地理解问题，更容易回忆相关知识，并最终推理出答案。我们通过在图像中挖掘问题-回答对来发现这些丰富的视觉线索，并将它们作为提示发送到多模态大型语言模型中。我们称之为Q&A提示的方法。具体而言，我们首先使用训练集中的图像-答案对和相应的问题作为输入和输出来训练一个视觉问题生成模型。

    With the breakthrough of multi-modal large language models, answering complex visual questions that demand advanced reasoning abilities and world knowledge has become a much more important testbed for developing AI models than ever. However, equipping AI models with robust cross-modality reasoning ability remains challenging since the cognition scheme of humans has not been understood systematically. In this paper, we believe that if we can collect visual clues in the given image as much as possible, we will recognize the image more accurately, understand the question better, recall relevant knowledge more easily, and finally reason out the answer. We discover these rich visual clues by mining question-answer pairs in images and sending them into multi-modal large language models as prompts. We call the proposed method Q&A Prompts. Specifically, we first use the image-answer pairs and the corresponding questions in the training set as inputs and outputs to train a visual question gener
    
[^13]: 解密电影海报的视觉特征，用于多标签电影类型识别

    Demystifying Visual Features of Movie Posters for Multi-Label Genre Identification. (arXiv:2309.12022v1 [cs.AI])

    [http://arxiv.org/abs/2309.12022](http://arxiv.org/abs/2309.12022)

    本研究通过分析电影海报图像，解密了电影海报的视觉特征，并提出了一种自动化的多标签电影类型识别方法，无需使用其他文本或元数据信息，具有推广和营销电影的实际应用意义。

    

    在电影行业中，电影海报多年来一直是广告和营销的重要组成部分，即使在现今的数字海报通过在线、社交媒体和OTT平台上仍然发挥着至关重要的作用。通常，电影海报能够有效地推广和传达电影的本质，例如其类型、视觉风格/调调、氛围和故事线索/主题，这些对吸引潜在观众非常重要。对电影类型进行识别常常在向目标观众推荐电影时具有重要的实际应用。之前的电影类型识别研究仅限于字幕、剧情简介和电影场景，这些大多数在电影发布后才能获取。海报通常包含在发行前隐含的信息来引起大量兴趣。在本文中，我们从电影海报图像中自动进行多标签电影类型识别，而无需任何关于电影的附加文本/元数据信息的帮助，这是其中之一。

    In the film industry, movie posters have been an essential part of advertising and marketing for many decades, and continue to play a vital role even today in the form of digital posters through online, social media and OTT platforms. Typically, movie posters can effectively promote and communicate the essence of a film, such as its genre, visual style/ tone, vibe and storyline cue/ theme, which are essential to attract potential viewers. Identifying the genres of a movie often has significant practical applications in recommending the film to target audiences. Previous studies on movie genre identification are limited to subtitles, plot synopses, and movie scenes that are mostly accessible after the movie release. Posters usually contain pre-release implicit information to generate mass interest. In this paper, we work for automated multi-label genre identification only from movie poster images, without any aid of additional textual/meta-data information about movies, which is one of 
    
[^14]: AR-TTA: 一种用于真实世界连续测试时间自适应的简单方法

    AR-TTA: A Simple Method for Real-World Continual Test-Time Adaptation. (arXiv:2309.10109v1 [cs.CV])

    [http://arxiv.org/abs/2309.10109](http://arxiv.org/abs/2309.10109)

    AR-TTA提出了一种简单的方法用于真实世界连续测试时间自适应。通过将内存缓冲区纳入自训练框架，并根据数据流的强度进行动态适应，提高了模型的稳定性。

    

    测试时间自适应是一种有前景的研究方向，它允许源模型在没有任何监督的情况下适应数据分布的变化。然而，当前的方法通常在只是实际场景简化版本的基准测试中进行评估。因此，我们建议使用最近推出的自动驾驶数据集CLAD-C和SHIFT来验证测试时间自适应方法。我们观察到，当前的测试时间自适应方法往往难以有效处理不同程度的域偏移，常常导致性能下降，低于源模型。我们注意到问题的根源在于无法保留源模型的知识，并且无法适应动态变化、时间相关的数据流。因此，我们通过将一个小的内存缓冲区纳入到成熟的自训练框架中，增加模型的稳定性，并同时根据数据流的强度进行动态适应。

    Test-time adaptation is a promising research direction that allows the source model to adapt itself to changes in data distribution without any supervision. Yet, current methods are usually evaluated on benchmarks that are only a simplification of real-world scenarios. Hence, we propose to validate test-time adaptation methods using the recently introduced datasets for autonomous driving, namely CLAD-C and SHIFT. We observe that current test-time adaptation methods struggle to effectively handle varying degrees of domain shift, often resulting in degraded performance that falls below that of the source model. We noticed that the root of the problem lies in the inability to preserve the knowledge of the source model and adapt to dynamically changing, temporally correlated data streams. Therefore, we enhance well-established self-training framework by incorporating a small memory buffer to increase model stability and at the same time perform dynamic adaptation based on the intensity of 
    
[^15]: 使用组合扩散模型实现训练数据保护

    Training Data Protection with Compositional Diffusion Models. (arXiv:2308.01937v1 [cs.LG])

    [http://arxiv.org/abs/2308.01937](http://arxiv.org/abs/2308.01937)

    使用分区扩散模型（CDM）训练不同的扩散模型，并在推断时任意组合它们，实现了训练数据保护和选择性遗忘，同时还可以根据用户访问权限提供定制模型。

    

    我们引入了分区扩散模型（CDM），一种在不同数据源上训练不同扩散模型（或提示）并在推断时任意组合它们的方法。这些单独的模型可以在孤立状态下、在不同时间、在不同分布和领域上进行训练，并可以后续组合以达到与同时训练所有数据的理想模型相当的性能。此外，每个模型只包含其在训练期间接触到的数据子集的信息，可以实现多种形式的训练数据保护。特别是，CDM是第一种可以实现大规模扩散模型的选择性遗忘和持续学习的方法，并且允许根据用户访问权限提供定制模型。CDM还可以确定生成特定样本的数据子集的重要性。

    We introduce Compartmentalized Diffusion Models (CDM), a method to train different diffusion models (or prompts) on distinct data sources and arbitrarily compose them at inference time. The individual models can be trained in isolation, at different times, and on different distributions and domains and can be later composed to achieve performance comparable to a paragon model trained on all data simultaneously. Furthermore, each model only contains information about the subset of the data it was exposed to during training, enabling several forms of training data protection. In particular, CDMs are the first method to enable both selective forgetting and continual learning for large-scale diffusion models, as well as allowing serving customized models based on the user's access rights. CDMs also allow determining the importance of a subset of the data in generating particular samples.
    
[^16]: 无注释图像字幕生成的研究：基于检索增强的伪句子生成

    Exploring Annotation-free Image Captioning with Retrieval-augmented Pseudo Sentence Generation. (arXiv:2307.14750v1 [cs.CV])

    [http://arxiv.org/abs/2307.14750](http://arxiv.org/abs/2307.14750)

    本文提出了一种新的无注释图像字幕生成的策略，利用大规模预训练模型的先验知识作为监督，并整合检索过程以进一步增强其效力。该方法能够从不匹配的语料库中检索相关的短区域描述，并利用其生成多样的句子。

    

    近年来，训练无注释图像字幕生成器取得了进展。先前的方法可以分为两种策略：从不匹配的语料库中获取句子，并将其与给定的图像对齐作为伪注释，或者使用外部的图像-文本对对生成器进行预训练。然而，由于对齐的设置存在质量问题，其性能似乎已经达到了极限，而预训练需要大量的计算资源。为了解决这些挑战，我们提出了一种新的策略“LPM + 检索增强学习”，利用来自大规模预训练模型（LPM）的先验知识作为监督，并整合检索过程以进一步增强其效力。具体而言，我们引入了检索增强的伪句子生成（RaPSG），采用高效的方法从不匹配的语料库中检索出高相关的短区域描述，并将其用于生成多样的句子。

    Training an image captioner without annotated image-sentence pairs has gained traction in recent years. Previous approaches can be categorized into two strategies: crawling sentences from mismatching corpora and aligning them with the given images as pseudo annotations, or pre-training the captioner using external image-text pairs. However, the aligning setting seems to reach its performance limit due to the quality problem of pairs, and pre-training requires significant computational resources. To address these challenges, we propose a new strategy ``LPM + retrieval-augmented learning" where the prior knowledge from large pre-trained models (LPMs) is leveraged as supervision, and a retrieval process is integrated to further reinforce its effectiveness. Specifically, we introduce Retrieval-augmented Pseudo Sentence Generation (RaPSG), which adopts an efficient approach to retrieve highly relevant short region descriptions from the mismatching corpora and use them to generate a variety 
    
[^17]: 深度学习中的损失函数和度量方法：一项评论

    Loss Functions and Metrics in Deep Learning. A Review. (arXiv:2307.02694v1 [cs.LG])

    [http://arxiv.org/abs/2307.02694](http://arxiv.org/abs/2307.02694)

    本文回顾了深度学习中最常见的损失函数和性能测量方法，旨在帮助从业者选择最适合其特定任务的方法。

    

    深度学习的一个重要组成部分是选择用于训练和评估模型的损失函数和性能度量。本文回顾了深度学习中最常见的损失函数和性能测量方法。我们探讨了每种技术的优势和局限性，并举例说明它们在各种深度学习问题上的应用。我们的评论旨在全面了解最常见的深度学习任务中使用的不同损失函数和性能指标，并帮助从业者选择最适合其特定任务的方法。

    One of the essential components of deep learning is the choice of the loss function and performance metrics used to train and evaluate models. This paper reviews the most prevalent loss functions and performance measurements in deep learning. We examine the benefits and limits of each technique and illustrate their application to various deep-learning problems. Our review aims to give a comprehensive picture of the different loss functions and performance indicators used in the most common deep learning tasks and help practitioners choose the best method for their specific task.
    
[^18]: S.T.A.R.-Track：自适应时空外貌表示的端到端3D物体跟踪的潜在运动模型

    S.T.A.R.-Track: Latent Motion Models for End-to-End 3D Object Tracking with Adaptive Spatio-Temporal Appearance Representations. (arXiv:2306.17602v1 [cs.CV])

    [http://arxiv.org/abs/2306.17602](http://arxiv.org/abs/2306.17602)

    本文提出了S.T.A.R.-Track，一个采用物体为中心的Transformer框架，用于端到端3D物体跟踪。通过新颖的潜在运动模型和学习型跟踪嵌入，该框架能够准确建模物体的几何运动和变化，并在nuScenes数据集上取得了优秀的性能。

    

    本文基于跟踪-注意力模式，引入了一个以物体为中心的基于Transformer的3D跟踪框架。传统的基于模型的跟踪方法通过几何运动模型融合帧之间的物体和自运动的几何效应。受此启发，我们提出了S.T.A.R.-Track，使用一种新颖的潜在运动模型来调整对象查询，以在潜在空间中直接考虑视角和光照条件的变化，同时明确建模几何运动。结合一种新颖的可学习的跟踪嵌入，有助于建模轨迹的存在概率，这导致了一个通用的跟踪框架，可以与任何基于查询的检测器集成。在nuScenes基准测试上进行了大量实验，证明了我们方法的优势，展示了基于DETR3D的跟踪器的最先进性能，同时大大减少了轨迹的身份转换次数。

    Following the tracking-by-attention paradigm, this paper introduces an object-centric, transformer-based framework for tracking in 3D. Traditional model-based tracking approaches incorporate the geometric effect of object- and ego motion between frames with a geometric motion model. Inspired by this, we propose S.T.A.R.-Track, which uses a novel latent motion model (LMM) to additionally adjust object queries to account for changes in viewing direction and lighting conditions directly in the latent space, while still modeling the geometric motion explicitly. Combined with a novel learnable track embedding that aids in modeling the existence probability of tracks, this results in a generic tracking framework that can be integrated with any query-based detector. Extensive experiments on the nuScenes benchmark demonstrate the benefits of our approach, showing state-of-the-art performance for DETR3D-based trackers while drastically reducing the number of identity switches of tracks at the s
    
[^19]: DiMSam:扩散模型作为部分可观测任务与动作规划中的采样器。

    DiMSam: Diffusion Models as Samplers for Task and Motion Planning under Partial Observability. (arXiv:2306.13196v1 [cs.RO])

    [http://arxiv.org/abs/2306.13196](http://arxiv.org/abs/2306.13196)

    本文提出了一种使用扩散模型作为采样器的任务和动作规划方法，在部分可观测下能够实现长周期受约束的操作计划。

    

    任务和动作规划（TAMP）方法非常有效地计划长周期自主机器人操作。但是，由于它们需要一个规划模型，因此在环境和其动态不完全了解的领域中应用它们可能非常困难。我们提出通过利用深度生成建模，特别是扩散模型来克服这些限制，学习捕获规划模型中难以设计的约束和采样器。这些学习采样器在TAMP求解器中组合和合并，以联合找到满足规划中约束的行动参数值。为了便于对环境中未知对象进行预测，我们将这些采样器定义为学习的低维潜变量嵌入的可变对象状态。我们在关节式物体操作领域评估了我们的方法，并展示了经典TAMP、生成学习和潜在嵌入的组合如何使得在部分可观测下进行长周期受约束的操作计划。

    Task and Motion Planning (TAMP) approaches are effective at planning long-horizon autonomous robot manipulation. However, because they require a planning model, it can be difficult to apply them to domains where the environment and its dynamics are not fully known. We propose to overcome these limitations by leveraging deep generative modeling, specifically diffusion models, to learn constraints and samplers that capture these difficult-to-engineer aspects of the planning model. These learned samplers are composed and combined within a TAMP solver in order to find action parameter values jointly that satisfy the constraints along a plan. To tractably make predictions for unseen objects in the environment, we define these samplers on low-dimensional learned latent embeddings of changing object state. We evaluate our approach in an articulated object manipulation domain and show how the combination of classical TAMP, generative learning, and latent embeddings enables long-horizon constra
    
[^20]: 解耦式KL散度损失函数

    Decoupled Kullback-Leibler Divergence Loss. (arXiv:2305.13948v1 [cs.CV])

    [http://arxiv.org/abs/2305.13948](http://arxiv.org/abs/2305.13948)

    本文提出了改进的KL散度损失函数，通过解决解耦式KL散度损失函数的对称性限制和引入全局信息来提升性能，在CIFAR-10/100和ImageNet数据集上展示了其在对抗训练和知识蒸馏任务中的优越表现。

    

    本文更深入地探究了KL散度损失函数，并发现它与解耦式KL散度损失函数等价，后者由加权均方差损失和包含软标签的交叉熵损失组成。通过对解耦式KL散度损失函数的分析，本文确定了两个改进方向。首先，我们解决了在知识蒸馏等场景下解耦式KL散度损失函数的对称性限制问题。这个改进保证了在训练期间wMSE组件始终有效，提供额外的构造性暗示。其次，我们将全局信息引入解耦式KL散度损失函数中，用于类内一致性正则化。通过这两个改进，我们得到了改进的KL散度损失函数，通过在CIFAR-10/100和ImageNet数据集上进行实验来评估其有效性，重点是对抗训练和知识蒸馏任务。所提出的方法表现出了比其他最先进模型更优越的性能，展示了其在各种实际应用中的潜力。

    In this paper, we delve deeper into the Kullback-Leibler (KL) Divergence loss and observe that it is equivalent to the Doupled Kullback-Leibler (DKL) Divergence loss that consists of 1) a weighted Mean Square Error (wMSE) loss and 2) a Cross-Entropy loss incorporating soft labels. From our analysis of the DKL loss, we have identified two areas for improvement. Firstly, we address the limitation of DKL in scenarios like knowledge distillation by breaking its asymmetry property in training optimization. This modification ensures that the wMSE component is always effective during training, providing extra constructive cues. Secondly, we introduce global information into DKL for intra-class consistency regularization. With these two enhancements, we derive the Improved Kullback-Leibler (IKL) Divergence loss and evaluate its effectiveness by conducting experiments on CIFAR-10/100 and ImageNet datasets, focusing on adversarial training and knowledge distillation tasks. The proposed approach 
    
[^21]: 面部识别技术可以从面部图像中显示政治取向，即使控制社会人口统计和自我表现。(arXiv: 2303.16343v1 [cs.CV])

    Facial recognition technology can expose political orientation from facial images even when controlling for demographics and self-presentation. (arXiv:2303.16343v1 [cs.CV])

    [http://arxiv.org/abs/2303.16343](http://arxiv.org/abs/2303.16343)

    本研究使用面部识别技术，通过特定的面部特征发现了人们的政治取向，甚至可以从自然图像中推广。这种预测的精度比人类评分者高，相当于一些工作面试的预测效果。

    

    本论文运用面部识别算法，从实验室设置下拍摄的591张中性面部图像中提取面部描述符。在控制年龄、性别和种族的情况下，通过交叉验证的线性回归模型来预测参与者在政治取向量表上的得分(Cronbach的α=0.94)。模型的性能超过了r = 0.20，远优于人类评分者，与工作面试预测工作成功、酒精驱动攻击性或心理治疗改善心理健康的效果相当。此外，从标准化图像衍生出的模型在3,401名来自美国、英国和加拿大的政治人物的自然图像样本中表现良好(r = 0.12)，表明面部外貌和政治取向之间的关联可推广到我们之外的人群。面部特征与政治取向相关的分析发现，保守派的下半脸部分更大，虽然政治取向不能准确地预测个体面部特征的所有变化，但是这种发现还是富有启发性的。

    A facial recognition algorithm was used to extract face descriptors from carefully standardized images of 591 neutral faces taken in the laboratory setting. Face descriptors were entered into a cross-validated linear regression to predict participants' scores on a political orientation scale (Cronbach's alpha=.94) while controlling for age, gender, and ethnicity. The model's performance exceeded r=.20: much better than that of human raters and on par with how well job interviews predict job success, alcohol drives aggressiveness, or psychological therapy improves mental health. Moreover, the model derived from standardized images performed well (r=.12) in a sample of naturalistic images of 3,401 politicians from the U.S., UK, and Canada, suggesting that the associations between facial appearance and political orientation generalize beyond our sample. The analysis of facial features associated with political orientation revealed that conservatives had larger lower faces, although politi
    

