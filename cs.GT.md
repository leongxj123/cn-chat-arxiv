# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Priority-Neutral Matching Lattices Are Not Distributive](https://arxiv.org/abs/2404.02142) | 优先中立匹配格的研究表明其不具有分配性质，与稳定匹配格不同，对于学生不一定是帕累托最优。 |
| [^2] | [Deep Learning Meets Mechanism Design: Key Results and Some Novel Applications.](http://arxiv.org/abs/2401.05683) | 本文介绍了深度学习与机制设计的结合，探讨了使用深度学习方法在无法同时满足所有期望特性的情况下，学习近似满足特性要求的机制。 |

# 详细

[^1]: 优先中立匹配格不是分配的

    Priority-Neutral Matching Lattices Are Not Distributive

    [https://arxiv.org/abs/2404.02142](https://arxiv.org/abs/2404.02142)

    优先中立匹配格的研究表明其不具有分配性质，与稳定匹配格不同，对于学生不一定是帕累托最优。

    

    稳定匹配是市场设计的基石，具有丰富的理论结构支持众多实际部署。然而，在学校选择问题中，稳定匹配对学生并不是帕累托最优。由Reny（经济学年报，2022年）引入的优先中立匹配扩展了稳定匹配集合，允许某些优先级违规，且总有一个帕累托最优优先中立匹配。此外，类似于稳定匹配，优先中立匹配形成一个格。我们研究了优先中立格的结构。不幸的是，我们表明优先中立格的简单性在很大程度上不适用于稳定匹配格。特别地，我们表明优先中立格不一定是分配的。此外，我们表明优先中立格中两个匹配的最大下界不一定是它们的学生最小值。

    arXiv:2404.02142v1 Announce Type: new  Abstract: Stable matchings are a cornerstone of market design, with numerous practical deployments backed by a rich, theoretically-tractable structure. However, in school-choice problems, stable matchings are not Pareto optimal for the students. Priority-neutral matchings, introduced by Reny (AER, 2022), generalizes the set of stable matchings by allowing for certain priority violations, and there is always a Pareto optimal priority-neutral matching. Moreover, like stable matchings, the set of priority-neutral matchings forms a lattice.   We study the structure of the priority-neutral lattice. Unfortunately, we show that much of the simplicity of the stable matching lattice does not hold for the priority-neutral lattice. In particular, we show that the priority-neutral lattice need not be distributive. Moreover, we show that the greatest lower bound of two matchings in the priority-neutral lattice need not be their student-by-student minimum, answ
    
[^2]: 深度学习与机制设计：关键结果和一些新的应用

    Deep Learning Meets Mechanism Design: Key Results and Some Novel Applications. (arXiv:2401.05683v1 [cs.GT])

    [http://arxiv.org/abs/2401.05683](http://arxiv.org/abs/2401.05683)

    本文介绍了深度学习与机制设计的结合，探讨了使用深度学习方法在无法同时满足所有期望特性的情况下，学习近似满足特性要求的机制。

    

    机制设计本质上是对游戏的逆向工程，涉及在博弈中诱导一种方式，使得诱导的博弈在博弈均衡中满足一组期望的特性。机制的期望特性包括激励兼容性、个体合理性、福利最大化、收入最大化（或成本最小化）、分配公平等。根据机制设计理论，只有某些严格的子集可以同时被任何给定的机制完全满足。在现实世界应用中，通常所需的机制可能需要一些在理论上无法同时满足的特性子集。在这种情况下，一个显著的近期方法是使用基于深度学习的方法，通过最小化适当定义的损失函数来学习一个近似满足所需特性的机制。在本文中，我们从相关文献中介绍了技术细节。

    Mechanism design is essentially reverse engineering of games and involves inducing a game among strategic agents in a way that the induced game satisfies a set of desired properties in an equilibrium of the game. Desirable properties for a mechanism include incentive compatibility, individual rationality, welfare maximisation, revenue maximisation (or cost minimisation), fairness of allocation, etc. It is known from mechanism design theory that only certain strict subsets of these properties can be simultaneously satisfied exactly by any given mechanism. Often, the mechanisms required by real-world applications may need a subset of these properties that are theoretically impossible to be simultaneously satisfied. In such cases, a prominent recent approach is to use a deep learning based approach to learn a mechanism that approximately satisfies the required properties by minimizing a suitably defined loss function. In this paper, we present, from relevant literature, technical details 
    

