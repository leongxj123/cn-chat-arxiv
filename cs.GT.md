# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Incentivizing Exploration with Linear Contexts and Combinatorial Actions](https://arxiv.org/abs/2306.01990) | 研究在激励式赌博机探索中通过线性赌博机模型替代先验独立性条件，提高了高维动作空间下的激励探索效率和最优遗憾，同时改进了半赌博模型中关于初始数据收集的样本复杂度。 |

# 详细

[^1]: 通过线性上下文和组合动作激励探索

    Incentivizing Exploration with Linear Contexts and Combinatorial Actions

    [https://arxiv.org/abs/2306.01990](https://arxiv.org/abs/2306.01990)

    研究在激励式赌博机探索中通过线性赌博机模型替代先验独立性条件，提高了高维动作空间下的激励探索效率和最优遗憾，同时改进了半赌博模型中关于初始数据收集的样本复杂度。

    

    我们推进了激励式赌博机探索的研究，其中手臂选择被视为推荐，并且要求是贝叶斯激励兼容的。最近的工作表明，在满足一定独立性假设后，经过足够的初始样本收集，流行的汤普森抽样算法变得激励兼容。我们为线性赌博机提供了这个结果的类比，其中先验的独立性被自然的凸性条件取代。这打开了在高维动作空间中高效和遗憾最优的激励探索的可能性。在半赌博模型中，我们还改进了用于初始数据收集的前汤普森抽样阶段的样本复杂度。

    arXiv:2306.01990v2 Announce Type: replace-cross  Abstract: We advance the study of incentivized bandit exploration, in which arm choices are viewed as recommendations and are required to be Bayesian incentive compatible. Recent work has shown under certain independence assumptions that after collecting enough initial samples, the popular Thompson sampling algorithm becomes incentive compatible. We give an analog of this result for linear bandits, where the independence of the prior is replaced by a natural convexity condition. This opens up the possibility of efficient and regret-optimal incentivized exploration in high-dimensional action spaces. In the semibandit model, we also improve the sample complexity for the pre-Thompson sampling phase of initial data collection.
    

