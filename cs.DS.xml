<rss version="2.0"><channel><title>Chat Arxiv cs.DS</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DS</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#39640;&#25928;&#35745;&#31639;&#35757;&#32451;&#38598;$\mathbf{X}^\mathbf{T}\mathbf{X}$&#21644;$\mathbf{X}^\mathbf{T}\mathbf{Y}$&#30340;&#31639;&#27861;&#65292;&#30456;&#27604;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#36825;&#20123;&#31639;&#27861;&#33021;&#22815;&#26174;&#33879;&#21152;&#36895;&#20132;&#21449;&#39564;&#35777;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35745;&#31639;&#30697;&#38453;&#20056;&#31215;&#25110;&#32479;&#35745;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.13185</link><description>&lt;p&gt;
&#31616;&#21270;&#20132;&#21449;&#39564;&#35777;&#65306;&#39640;&#25928;&#22320;&#35745;&#31639;&#19981;&#38656;&#35201;&#20840;&#37327;&#37325;&#26032;&#35745;&#31639;&#30697;&#38453;&#20056;&#31215;&#25110;&#32479;&#35745;&#37327;&#30340;&#21015;&#21521;&#20013;&#24515;&#21270;&#21644;&#26631;&#20934;&#21270;&#35757;&#32451;&#38598;$\mathbf{X}^\mathbf{T}\mathbf{X}$&#21644;$\mathbf{X}^\mathbf{T}\mathbf{Y}$
&lt;/p&gt;
&lt;p&gt;
Shortcutting Cross-Validation: Efficiently Deriving Column-Wise Centered and Scaled Training Set $\mathbf{X}^\mathbf{T}\mathbf{X}$ and $\mathbf{X}^\mathbf{T}\mathbf{Y}$ Without Full Recomputation of Matrix Products or Statistical Moments. (arXiv:2401.13185v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#39640;&#25928;&#35745;&#31639;&#35757;&#32451;&#38598;$\mathbf{X}^\mathbf{T}\mathbf{X}$&#21644;$\mathbf{X}^\mathbf{T}\mathbf{Y}$&#30340;&#31639;&#27861;&#65292;&#30456;&#27604;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#36825;&#20123;&#31639;&#27861;&#33021;&#22815;&#26174;&#33879;&#21152;&#36895;&#20132;&#21449;&#39564;&#35777;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35745;&#31639;&#30697;&#38453;&#20056;&#31215;&#25110;&#32479;&#35745;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#21449;&#39564;&#35777;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#35780;&#20272;&#39044;&#27979;&#27169;&#22411;&#22312;&#26410;&#30693;&#25968;&#25454;&#19978;&#34920;&#29616;&#30340;&#25216;&#26415;&#12290;&#35768;&#22810;&#39044;&#27979;&#27169;&#22411;&#65292;&#22914;&#22522;&#20110;&#26680;&#30340;&#20559;&#26368;&#23567;&#20108;&#20056;&#65288;PLS&#65289;&#27169;&#22411;&#65292;&#38656;&#35201;&#20165;&#20351;&#29992;&#36755;&#20837;&#30697;&#38453;$\mathbf{X}$&#21644;&#36755;&#20986;&#30697;&#38453;$\mathbf{Y}$&#20013;&#30340;&#35757;&#32451;&#38598;&#26679;&#26412;&#26469;&#35745;&#31639;$\mathbf{X}^{\mathbf{T}}\mathbf{X}$&#21644;$\mathbf{X}^{\mathbf{T}}\mathbf{Y}$&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#39640;&#25928;&#35745;&#31639;&#36825;&#20123;&#30697;&#38453;&#30340;&#31639;&#27861;&#12290;&#31532;&#19968;&#31181;&#31639;&#27861;&#19981;&#38656;&#35201;&#21015;&#21521;&#39044;&#22788;&#29702;&#12290;&#31532;&#20108;&#31181;&#31639;&#27861;&#20801;&#35768;&#20197;&#35757;&#32451;&#38598;&#22343;&#20540;&#20026;&#20013;&#24515;&#21270;&#28857;&#36827;&#34892;&#21015;&#21521;&#20013;&#24515;&#21270;&#12290;&#31532;&#19977;&#31181;&#31639;&#27861;&#20801;&#35768;&#20197;&#35757;&#32451;&#38598;&#22343;&#20540;&#21644;&#26631;&#20934;&#24046;&#20026;&#20013;&#24515;&#21270;&#28857;&#21644;&#26631;&#20934;&#21270;&#28857;&#36827;&#34892;&#21015;&#21521;&#20013;&#24515;&#21270;&#21644;&#26631;&#20934;&#21270;&#12290;&#36890;&#36807;&#35777;&#26126;&#27491;&#30830;&#24615;&#21644;&#20248;&#36234;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#23427;&#20204;&#30456;&#27604;&#20110;&#30452;&#25509;&#20132;&#21449;&#39564;&#35777;&#21644;&#20197;&#21069;&#30340;&#24555;&#36895;&#20132;&#21449;&#39564;&#35777;&#24037;&#20316;&#65292;&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#20132;&#21449;&#39564;&#35777;&#21152;&#36895;&#65292;&#32780;&#26080;&#38656;&#25968;&#25454;&#27844;&#38706;&#12290;&#23427;&#20204;&#36866;&#21512;&#24182;&#34892;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-validation is a widely used technique for assessing the performance of predictive models on unseen data. Many predictive models, such as Kernel-Based Partial Least-Squares (PLS) models, require the computation of $\mathbf{X}^{\mathbf{T}}\mathbf{X}$ and $\mathbf{X}^{\mathbf{T}}\mathbf{Y}$ using only training set samples from the input and output matrices, $\mathbf{X}$ and $\mathbf{Y}$, respectively. In this work, we present three algorithms that efficiently compute these matrices. The first one allows no column-wise preprocessing. The second one allows column-wise centering around the training set means. The third one allows column-wise centering and column-wise scaling around the training set means and standard deviations. Demonstrating correctness and superior computational complexity, they offer significant cross-validation speedup compared with straight-forward cross-validation and previous work on fast cross-validation - all without data leakage. Their suitability for paralle
&lt;/p&gt;</description></item></channel></rss>