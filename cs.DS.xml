<rss version="2.0"><channel><title>Chat Arxiv cs.DS</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DS</description><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20165;&#20351;&#29992;&#21387;&#32553;&#34920;&#31034;&#26469;&#24674;&#22797;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20013;&#31526;&#21495;&#30340;&#39057;&#29575;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#20272;&#35745;&#26041;&#27861;&#65292;&#23558;&#36125;&#21494;&#26031;&#21644;&#39057;&#29575;&#35770;&#35266;&#28857;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#36824;&#25193;&#23637;&#20102;&#35813;&#26041;&#27861;&#20197;&#35299;&#20915;&#22522;&#25968;&#24674;&#22797;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.15408</link><description>&lt;p&gt;
&#20174;&#21387;&#32553;&#25968;&#25454;&#20013;&#24674;&#22797;&#39057;&#29575;&#21644;&#22522;&#25968;&#65306;&#19968;&#31181;&#23558;&#36125;&#21494;&#26031;&#21644;&#39057;&#29575;&#35770;&#35266;&#28857;&#36830;&#25509;&#36215;&#26469;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Frequency and cardinality recovery from sketched data: a novel approach bridging Bayesian and frequentist views. (arXiv:2309.15408v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15408
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20165;&#20351;&#29992;&#21387;&#32553;&#34920;&#31034;&#26469;&#24674;&#22797;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20013;&#31526;&#21495;&#30340;&#39057;&#29575;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#20272;&#35745;&#26041;&#27861;&#65292;&#23558;&#36125;&#21494;&#26031;&#21644;&#39057;&#29575;&#35770;&#35266;&#28857;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#36824;&#25193;&#23637;&#20102;&#35813;&#26041;&#27861;&#20197;&#35299;&#20915;&#22522;&#25968;&#24674;&#22797;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#20165;&#20351;&#29992;&#36890;&#36807;&#38543;&#26426;&#21704;&#24076;&#33719;&#24471;&#30340;&#23545;&#25968;&#25454;&#36827;&#34892;&#21387;&#32553;&#34920;&#31034;&#25110;&#33609;&#22270;&#26469;&#24674;&#22797;&#22823;&#35268;&#27169;&#31163;&#25955;&#25968;&#25454;&#38598;&#20013;&#31526;&#21495;&#30340;&#39057;&#29575;&#12290;&#36825;&#26159;&#19968;&#20010;&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#30340;&#32463;&#20856;&#38382;&#39064;&#65292;&#26377;&#21508;&#31181;&#31639;&#27861;&#21487;&#29992;&#65292;&#22914;&#35745;&#25968;&#26368;&#23567;&#33609;&#22270;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31639;&#27861;&#36890;&#24120;&#20551;&#35774;&#25968;&#25454;&#26159;&#22266;&#23450;&#30340;&#65292;&#22788;&#29702;&#38543;&#26426;&#37319;&#26679;&#25968;&#25454;&#26102;&#20272;&#35745;&#36807;&#20110;&#20445;&#23432;&#19988;&#21487;&#33021;&#19981;&#20934;&#30830;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#33609;&#22270;&#25968;&#25454;&#35270;&#20026;&#26410;&#30693;&#20998;&#24067;&#30340;&#38543;&#26426;&#26679;&#26412;&#65292;&#28982;&#21518;&#24341;&#20837;&#25913;&#36827;&#29616;&#26377;&#26041;&#27861;&#30340;&#26032;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#21644;&#32463;&#20856;&#65288;&#39057;&#29575;&#35770;&#65289;&#35266;&#28857;&#65292;&#35299;&#20915;&#20102;&#23427;&#20204;&#29420;&#29305;&#30340;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21407;&#21017;&#19988;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#35299;&#20915;&#30456;&#20851;&#20294;&#19981;&#21516;&#30340;&#22522;&#25968;&#24674;&#22797;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#28041;&#21450;&#20272;&#35745;&#25968;&#25454;&#38598;&#20013;&#19981;&#21516;&#23545;&#35937;&#30340;&#24635;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study how to recover the frequency of a symbol in a large discrete data set, using only a compressed representation, or sketch, of those data obtained via random hashing. This is a classical problem in computer science, with various algorithms available, such as the count-min sketch. However, these algorithms often assume that the data are fixed, leading to overly conservative and potentially inaccurate estimates when dealing with randomly sampled data. In this paper, we consider the sketched data as a random sample from an unknown distribution, and then we introduce novel estimators that improve upon existing approaches. Our method combines Bayesian nonparametric and classical (frequentist) perspectives, addressing their unique limitations to provide a principled and practical solution. Additionally, we extend our method to address the related but distinct problem of cardinality recovery, which consists of estimating the total number of distinct objects in the data set. We validate
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#31169;&#34892;&#20026;&#19979;&#30340;&#21163;&#21290;&#31038;&#20132;&#23398;&#20064;&#38382;&#39064;&#65292;&#21457;&#29616;&#23384;&#22312;&#19968;&#31181;&#25506;&#32034;&#28608;&#21169;&#26435;&#34913;&#65292;&#21363;&#27494;&#22120;&#25506;&#32034;&#21644;&#31038;&#20132;&#25506;&#32034;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21463;&#21040;&#20195;&#29702;&#30340;&#30701;&#35270;&#34892;&#20026;&#30340;&#38480;&#21046;&#20250;&#21152;&#21095;&#36825;&#31181;&#26435;&#34913;&#65292;&#24182;&#23548;&#33268;&#36951;&#25022;&#29575;&#19982;&#20195;&#29702;&#25968;&#37327;&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2302.07425</link><description>&lt;p&gt;
&#33258;&#31169;&#34892;&#20026;&#19979;&#30340;&#21163;&#21290;&#31038;&#20132;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bandit Social Learning: Exploration under Myopic Behavior. (arXiv:2302.07425v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07425
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#31169;&#34892;&#20026;&#19979;&#30340;&#21163;&#21290;&#31038;&#20132;&#23398;&#20064;&#38382;&#39064;&#65292;&#21457;&#29616;&#23384;&#22312;&#19968;&#31181;&#25506;&#32034;&#28608;&#21169;&#26435;&#34913;&#65292;&#21363;&#27494;&#22120;&#25506;&#32034;&#21644;&#31038;&#20132;&#25506;&#32034;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21463;&#21040;&#20195;&#29702;&#30340;&#30701;&#35270;&#34892;&#20026;&#30340;&#38480;&#21046;&#20250;&#21152;&#21095;&#36825;&#31181;&#26435;&#34913;&#65292;&#24182;&#23548;&#33268;&#36951;&#25022;&#29575;&#19982;&#20195;&#29702;&#25968;&#37327;&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#31038;&#20132;&#23398;&#20064;&#21160;&#24577;&#65292;&#20854;&#20013;&#20195;&#29702;&#25353;&#29031;&#31616;&#21333;&#30340;&#22810;&#33218;&#21163;&#21290;&#21327;&#35758;&#20849;&#21516;&#34892;&#21160;&#12290;&#20195;&#29702;&#20197;&#39034;&#24207;&#26041;&#24335;&#21040;&#36798;&#65292;&#36873;&#25321;&#27494;&#22120;&#24182;&#25509;&#25910;&#30456;&#20851;&#22870;&#21169;&#12290;&#27599;&#20010;&#20195;&#29702;&#35266;&#23519;&#20808;&#21069;&#20195;&#29702;&#30340;&#23436;&#25972;&#21382;&#21490;&#35760;&#24405;&#65288;&#27494;&#22120;&#21644;&#22870;&#21169;&#65289;&#65292;&#19981;&#23384;&#22312;&#31169;&#26377;&#20449;&#21495;&#12290;&#23613;&#31649;&#20195;&#29702;&#20849;&#21516;&#38754;&#20020;&#24320;&#21457;&#21644;&#21033;&#29992;&#30340;&#25506;&#32034;&#25240;&#34935;&#65292;&#20294;&#27599;&#20010;&#20195;&#29702;&#20154;&#37117;&#26159;&#19968;&#35265;&#38047;&#24773;&#30340;&#65292;&#26080;&#38656;&#32771;&#34385;&#25506;&#32034;&#12290;&#25105;&#20204;&#20801;&#35768;&#19968;&#31995;&#21015;&#19982;&#65288;&#21442;&#25968;&#21270;&#65289;&#32622;&#20449;&#21306;&#38388;&#19968;&#33268;&#30340;&#33258;&#31169;&#34892;&#20026;&#65292;&#21253;&#25324;&#8220;&#26080;&#20559;&#8221;&#34892;&#20026;&#21644;&#21508;&#31181;&#34892;&#20026;&#20559;&#24046;&#12290;&#34429;&#28982;&#36825;&#20123;&#34892;&#20026;&#30340;&#26497;&#31471;&#29256;&#26412;&#23545;&#24212;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;&#21163;&#21290;&#31639;&#27861;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#20102;&#26356;&#28201;&#21644;&#30340;&#29256;&#26412;&#20250;&#23548;&#33268;&#26126;&#26174;&#30340;&#25506;&#32034;&#22833;&#36133;&#65292;&#22240;&#27492;&#36951;&#25022;&#29575;&#19982;&#20195;&#29702;&#25968;&#37327;&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#8220;&#28201;&#21644;&#20048;&#35266;&#8221;&#30340;&#20195;&#29702;&#25552;&#20379;&#21305;&#37197;&#30340;&#36951;&#25022;&#19978;&#30028;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#25506;&#32034;&#28608;&#21169;&#20043;&#38388;&#30340;&#22522;&#26412;&#26435;&#34913;&#65306;&#27494;&#22120;&#25506;&#32034;&#26159;&#22266;&#26377;&#20110;&#21163;&#21290;&#38382;&#39064;&#30340;&#65292;&#21482;&#21463;&#24403;&#21069;&#20195;&#29702;&#30340;&#34892;&#21160;&#24433;&#21709;&#65292;&#32780;&#31038;&#20132;&#25506;&#32034;&#26159;&#30001;&#20808;&#21069;&#20195;&#29702;&#34892;&#20026;&#39537;&#21160;&#30340;&#65292;&#22240;&#27492;&#26377;&#21033;&#20110;&#26410;&#26469;&#20195;&#29702;&#12290;&#30001;&#20110;&#20195;&#29702;&#30340;&#30701;&#35270;&#34892;&#20026;&#38480;&#21046;&#20102;&#31038;&#20132;&#25506;&#32034;&#65292;&#36825;&#31181;&#26435;&#34913;&#34987;&#21152;&#21095;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study social learning dynamics where the agents collectively follow a simple multi-armed bandit protocol. Agents arrive sequentially, choose arms and receive associated rewards. Each agent observes the full history (arms and rewards) of the previous agents, and there are no private signals. While collectively the agents face exploration-exploitation tradeoff, each agent acts myopically, without regards to exploration. Motivating scenarios concern reviews and ratings on online platforms.  We allow a wide range of myopic behaviors that are consistent with (parameterized) confidence intervals, including the "unbiased" behavior as well as various behaviorial biases. While extreme versions of these behaviors correspond to well-known bandit algorithms, we prove that more moderate versions lead to stark exploration failures, and consequently to regret rates that are linear in the number of agents. We provide matching upper bounds on regret by analyzing "moderately optimistic" agents.  As a
&lt;/p&gt;</description></item></channel></rss>