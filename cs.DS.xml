<rss version="2.0"><channel><title>Chat Arxiv cs.DS</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DS</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#40065;&#26834;&#21435;&#37327;&#23376;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36817;&#20284;&#38271;&#24230;&#24179;&#26041;&#37319;&#26679;&#30340;&#27010;&#24565;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#38543;&#26426;&#32447;&#24615;&#20195;&#25968;&#25216;&#26415;&#36866;&#24212;&#21040;&#36825;&#31181;&#26356;&#24369;&#30340;&#20551;&#35774;&#19979;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#25216;&#26415;&#35777;&#26126;&#20102;&#26368;&#36817;&#30340;&#20302;&#31209;&#21435;&#37327;&#23376;&#21270;&#26694;&#26550;&#21644;spa&#21435;&#37327;&#23376;&#21270;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2304.04932</link><description>&lt;p&gt;
&#37327;&#23376;&#22855;&#24322;&#20540;&#21464;&#25442;&#21450;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#40065;&#26834;&#21435;&#37327;&#23376;&#21270;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Robust Dequantization of the Quantum Singular value Transformation and Quantum Machine Learning Algorithms. (arXiv:2304.04932v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#40065;&#26834;&#21435;&#37327;&#23376;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36817;&#20284;&#38271;&#24230;&#24179;&#26041;&#37319;&#26679;&#30340;&#27010;&#24565;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#38543;&#26426;&#32447;&#24615;&#20195;&#25968;&#25216;&#26415;&#36866;&#24212;&#21040;&#36825;&#31181;&#26356;&#24369;&#30340;&#20551;&#35774;&#19979;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#25216;&#26415;&#35777;&#26126;&#20102;&#26368;&#36817;&#30340;&#20302;&#31209;&#21435;&#37327;&#23376;&#21270;&#26694;&#26550;&#21644;spa&#21435;&#37327;&#23376;&#21270;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#24050;&#32463;&#26377;&#20960;&#31181;&#29992;&#20110;&#35299;&#20915;&#32447;&#24615;&#20195;&#25968;&#38382;&#39064;&#21644;&#29305;&#21035;&#26159;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#30340;&#37327;&#23376;&#31639;&#27861;&#34987;&#8220;&#21435;&#37327;&#23376;&#21270;&#8221;&#12290;&#36825;&#20123;&#21435;&#37327;&#23376;&#21270;&#32467;&#26524;&#36890;&#24120;&#22312;&#32463;&#20856;&#31639;&#27861;&#36890;&#36807;&#38271;&#24230;&#24179;&#26041;&#37319;&#26679;&#26041;&#27861;&#35775;&#38382;&#25968;&#25454;&#26102;&#25104;&#31435;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20123;&#21435;&#37327;&#23376;&#21270;&#32467;&#26524;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36817;&#20284;&#38271;&#24230;&#24179;&#26041;&#37319;&#26679;&#30340;&#27010;&#24565;&#65292;&#20854;&#20013;&#32463;&#20856;&#31639;&#27861;&#21482;&#33021;&#20174;&#25509;&#36817;&#29702;&#24819;&#20998;&#24067;&#30340;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#34429;&#28982;&#37327;&#23376;&#31639;&#27861;&#22312;&#38754;&#23545;&#23567;&#25200;&#21160;&#26102;&#26412;&#36136;&#19978;&#26159;&#40065;&#26834;&#30340;&#65292;&#20294;&#24403;&#21069;&#30340;&#21435;&#37327;&#23376;&#21270;&#25216;&#26415;&#24182;&#19981;&#26159;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#25216;&#26415;&#36129;&#29486;&#22312;&#20110;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#35768;&#22810;&#38543;&#26426;&#32447;&#24615;&#20195;&#25968;&#25216;&#26415;&#36866;&#24212;&#21040;&#36825;&#31181;&#26356;&#24369;&#30340;&#20551;&#35774;&#19979;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#25216;&#26415;&#35777;&#26126;&#20102;&#26368;&#36817;&#30001;Chia&#12289;Gily\'en&#12289;Li&#12289;Lin&#12289;Tang&#21644;Wang&#65288;JACM 2022&#65289;&#25552;&#20986;&#30340;&#20302;&#31209;&#21435;&#37327;&#23376;&#21270;&#26694;&#26550;&#21644;&#29992;&#20110;spa&#30340;&#21435;&#37327;&#23376;&#21270;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several quantum algorithms for linear algebra problems, and in particular quantum machine learning problems, have been "dequantized" in the past few years. These dequantization results typically hold when classical algorithms can access the data via length-squared sampling. In this work we investigate how robust these dequantization results are. We introduce the notion of approximate length-squared sampling, where classical algorithms are only able to sample from a distribution close to the ideal distribution in total variation distance. While quantum algorithms are natively robust against small perturbations, current techniques in dequantization are not. Our main technical contribution is showing how many techniques from randomized linear algebra can be adapted to work under this weaker assumption as well. We then use these techniques to show that the recent low-rank dequantization framework by Chia, Gily\'en, Li, Lin, Tang and Wang (JACM 2022) and the dequantization framework for spa
&lt;/p&gt;</description></item></channel></rss>