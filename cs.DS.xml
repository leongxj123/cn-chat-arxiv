<rss version="2.0"><channel><title>Chat Arxiv cs.DS</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DS</description><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#22810;&#36890;&#36947;&#27969;&#31639;&#27861;&#32473;&#20986;&#20102;&#32431;&#25506;&#32034;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#30340;&#36817;&#20046;&#26368;&#20248;&#26679;&#26412;&#36890;&#36947;&#20132;&#25442;&#30028;&#38480;&#65292;&#24182;&#22238;&#31572;&#20102;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.03145</link><description>&lt;p&gt;
&#26368;&#20339;&#33218;&#36530;&#36991;&#65306;&#32431;&#25506;&#32034;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#30340;&#36817;&#20046;&#26368;&#20248;&#22810;&#36890;&#36947;&#27969;&#31639;&#27861;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
The Best Arm Evades: Near-optimal Multi-pass Streaming Lower Bounds for Pure Exploration in Multi-armed Bandits. (arXiv:2309.03145v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03145
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#22810;&#36890;&#36947;&#27969;&#31639;&#27861;&#32473;&#20986;&#20102;&#32431;&#25506;&#32034;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#30340;&#36817;&#20046;&#26368;&#20248;&#26679;&#26412;&#36890;&#36947;&#20132;&#25442;&#30028;&#38480;&#65292;&#24182;&#22238;&#31572;&#20102;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#22810;&#36890;&#36947;&#27969;&#31639;&#27861;&#32473;&#20986;&#20102;&#32431;&#25506;&#32034;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MABs&#65289;&#30340;&#36817;&#20284;&#26368;&#20248;&#26679;&#26412;&#36890;&#36947;&#20132;&#25442;&#65306;&#20219;&#20309;&#20351;&#29992;&#23376;&#32447;&#24615;&#20869;&#23384;&#30340;&#27969;&#31639;&#27861;&#65292;&#20854;&#20351;&#29992; $O(\frac{n}{\Delta^2})$ &#30340;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#38656;&#35201; $\Omega(\frac{\log{(1/\Delta)}}{\log\log{(1/\Delta)}})$ &#20010;&#36890;&#36947;&#12290;&#36825;&#37324;&#65292;$n$ &#26159;&#33218;&#30340;&#25968;&#37327;&#65292;$\Delta$ &#26159;&#26368;&#20339;&#33218;&#21644;&#27425;&#20339;&#33218;&#20043;&#38388;&#30340;&#22870;&#21169;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19982;Jin&#31561;&#20154;[ICML'21]&#30340; $O(\log(\frac{1}{\Delta}))$ &#36890;&#36947;&#31639;&#27861;&#30456;&#21305;&#37197;&#65288;&#38500;&#20102;&#20302;&#38454;&#39033;&#65289;&#65292;&#35813;&#31639;&#27861;&#20165;&#20351;&#29992; $O(1)$ &#20869;&#23384;&#65292;&#24182;&#22238;&#31572;&#20102;Assadi&#21644;Wang[STOC'20]&#25552;&#20986;&#30340;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We give a near-optimal sample-pass trade-off for pure exploration in multi-armed bandits (MABs) via multi-pass streaming algorithms: any streaming algorithm with sublinear memory that uses the optimal sample complexity of $O(\frac{n}{\Delta^2})$ requires $\Omega(\frac{\log{(1/\Delta)}}{\log\log{(1/\Delta)}})$ passes. Here, $n$ is the number of arms and $\Delta$ is the reward gap between the best and the second-best arms. Our result matches the $O(\log(\frac{1}{\Delta}))$-pass algorithm of Jin et al. [ICML'21] (up to lower order terms) that only uses $O(1)$ memory and answers an open question posed by Assadi and Wang [STOC'20].
&lt;/p&gt;</description></item></channel></rss>