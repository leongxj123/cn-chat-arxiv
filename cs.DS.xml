<rss version="2.0"><channel><title>Chat Arxiv cs.DS</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DS</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CP-E2LSH&#21644;TT-E2LSH&#20004;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#31639;&#27861;LSH&#65292;&#22312;&#22788;&#29702;&#24352;&#37327;&#25968;&#25454;&#30340;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#21644;&#20313;&#24358;&#30456;&#20284;&#24230;&#26102;&#33021;&#22815;&#25552;&#20379;&#26356;&#24555;&#21644;&#26356;&#31354;&#38388;&#26377;&#25928;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.07189</link><description>&lt;p&gt;
&#36890;&#36807;&#24352;&#37327;&#21270;&#38543;&#26426;&#25237;&#24433;&#25913;&#36827;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;LSH
&lt;/p&gt;
&lt;p&gt;
Improving LSH via Tensorized Random Projection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CP-E2LSH&#21644;TT-E2LSH&#20004;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#31639;&#27861;LSH&#65292;&#22312;&#22788;&#29702;&#24352;&#37327;&#25968;&#25454;&#30340;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#21644;&#20313;&#24358;&#30456;&#20284;&#24230;&#26102;&#33021;&#22815;&#25552;&#20379;&#26356;&#24555;&#21644;&#26356;&#31354;&#38388;&#26377;&#25928;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;(LSH)&#26159;&#25968;&#25454;&#31185;&#23398;&#23478;&#29992;&#20110;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#38382;&#39064;&#30340;&#22522;&#26412;&#31639;&#27861;&#24037;&#20855;&#65292;&#24050;&#22312;&#35768;&#22810;&#22823;&#35268;&#27169;&#25968;&#25454;&#22788;&#29702;&#24212;&#29992;&#20013;&#24191;&#27867;&#20351;&#29992;&#65292;&#22914;&#36817;&#20284;&#37325;&#22797;&#26816;&#27979;&#12289;&#26368;&#36817;&#37051;&#25628;&#32034;&#12289;&#32858;&#31867;&#31561;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20986;&#26356;&#24555;&#21644;&#31354;&#38388;&#26356;&#26377;&#25928;&#30340;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#20989;&#25968;&#65292;&#29992;&#20110;&#24352;&#37327;&#25968;&#25454;&#30340;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#21644;&#20313;&#24358;&#30456;&#20284;&#24230;&#12290;&#36890;&#24120;&#65292;&#23545;&#20110;&#24352;&#37327;&#25968;&#25454;&#33719;&#24471;LSH&#30340;&#26420;&#32032;&#26041;&#27861;&#28041;&#21450;&#23558;&#24352;&#37327;&#37325;&#22609;&#20026;&#21521;&#37327;&#65292;&#28982;&#21518;&#24212;&#29992;&#29616;&#26377;&#30340;&#21521;&#37327;&#25968;&#25454;LSH&#26041;&#27861;(E2LSH&#21644;SRP)&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#39640;&#38454;&#24352;&#37327;&#65292;&#36825;&#31181;&#26041;&#27861;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#65292;&#22240;&#20026;&#37325;&#22609;&#21521;&#37327;&#30340;&#22823;&#23567;&#22312;&#24352;&#37327;&#30340;&#38454;&#25968;&#20013;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#22240;&#27492;&#65292;LSH&#21442;&#25968;&#30340;&#22823;&#23567;&#21576;&#25351;&#25968;&#22686;&#21152;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#21644;&#20313;&#24358;&#30456;&#20284;&#24230;&#30340;LSH&#26041;&#27861;&#65292;&#20998;&#21035;&#26159;CP-E2LSH&#21644;TT-E2LSH&#12290;
&lt;/p&gt;
&lt;p&gt;
Locality sensitive hashing (LSH) is a fundamental algorithmic toolkit used by data scientists for approximate nearest neighbour search problems that have been used extensively in many large scale data processing applications such as near duplicate detection, nearest neighbour search, clustering, etc. In this work, we aim to propose faster and space efficient locality sensitive hash functions for Euclidean distance and cosine similarity for tensor data. Typically, the naive approach for obtaining LSH for tensor data involves first reshaping the tensor into vectors, followed by applying existing LSH methods for vector data $E2LSH$ and $SRP$. However, this approach becomes impractical for higher order tensors because the size of the reshaped vector becomes exponential in the order of the tensor. Consequently, the size of LSH parameters increases exponentially. To address this problem, we suggest two methods for LSH for Euclidean distance and cosine similarity, namely $CP-E2LSH$, $TT-E2LSH
&lt;/p&gt;</description></item></channel></rss>