<rss version="2.0"><channel><title>Chat Arxiv cs.DS</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DS</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#31639;&#27861;&#20197;&#35745;&#31639;&#26088;&#22312;&#21508;&#31181;&#24230;&#37327;&#26041;&#38754;&#26368;&#20248;&#30340;&#20915;&#31574;&#26641;&#38598;&#21512;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#8220;&#35777;&#26126;&#26641;&#25216;&#26415;&#8221;&#26469;&#22823;&#22823;&#25913;&#36827;&#21487;&#22788;&#29702;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.04423</link><description>&lt;p&gt;
&#35745;&#31639;&#26368;&#20248;&#26641;&#38598;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
On Computing Optimal Tree Ensembles. (arXiv:2306.04423v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04423
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#31639;&#27861;&#20197;&#35745;&#31639;&#26088;&#22312;&#21508;&#31181;&#24230;&#37327;&#26041;&#38754;&#26368;&#20248;&#30340;&#20915;&#31574;&#26641;&#38598;&#21512;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#8220;&#35777;&#26126;&#26641;&#25216;&#26415;&#8221;&#26469;&#22823;&#22823;&#25913;&#36827;&#21487;&#22788;&#29702;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26862;&#26519;&#21644;&#20915;&#31574;&#26641;&#38598;&#21512;&#26159;&#20998;&#31867;&#21644;&#22238;&#24402;&#30340;&#24191;&#27867;&#24212;&#29992;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#31639;&#27861;&#36827;&#23637;&#20801;&#35768;&#35745;&#31639;&#26088;&#22312;&#21508;&#31181;&#24230;&#37327;&#26041;&#38754;&#65288;&#22914;&#22823;&#23567;&#25110;&#28145;&#24230;&#65289;&#26368;&#20248;&#30340;&#20915;&#31574;&#26641;&#12290;&#25105;&#20204;&#19981;&#30693;&#36947;&#26377;&#20851;&#26641;&#38598;&#21512;&#30340;&#27492;&#31867;&#30740;&#31350;&#65292;&#24182;&#26088;&#22312;&#20026;&#35813;&#39046;&#22495;&#20570;&#20986;&#36129;&#29486;&#12290;&#20027;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#31181;&#26032;&#31639;&#27861;&#21644;&#30456;&#24212;&#30340;&#19979;&#38480;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#33021;&#22815;&#36716;&#31227;&#21644;&#22823;&#22823;&#25913;&#36827;&#20915;&#31574;&#26641;&#30340;&#21487;&#22788;&#29702;&#24615;&#32467;&#26524;&#65292;&#33719;&#24471;&#19968;&#20010; $(6\delta D S)^S \cdot poly$-time &#31639;&#27861;&#65292;&#20854;&#20013; $S$ &#26159;&#26641;&#38598;&#21512;&#20013;&#21106;&#25968;&#65292;$D$ &#26159;&#26368;&#22823;&#22495;&#22823;&#23567;&#65292;$\delta$ &#26159;&#20004;&#20010;&#31034;&#20363;&#20043;&#38388;&#23384;&#22312;&#19981;&#21516;&#29305;&#24449;&#30340;&#26368;&#22823;&#25968;&#37327;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#30340;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35777;&#26126;&#26641;&#25216;&#26415;&#65292;&#36825;&#20284;&#20046;&#23545;&#23454;&#36341;&#20063;&#24456;&#26377;&#21069;&#36884;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#21160;&#24577;&#35268;&#21010;&#23545;&#20110;&#20915;&#31574;&#26641;&#24050;&#32463;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#32780;&#23545;&#20110;&#26641;&#38598;&#21512;&#20063;&#21487;&#33021;&#26159;&#21487;&#34892;&#30340;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010; $\ell^n \cdot poly$-t&#12290;
&lt;/p&gt;
&lt;p&gt;
Random forests and, more generally, (decision\nobreakdash-)tree ensembles are widely used methods for classification and regression. Recent algorithmic advances allow to compute decision trees that are optimal for various measures such as their size or depth. We are not aware of such research for tree ensembles and aim to contribute to this area. Mainly, we provide two novel algorithms and corresponding lower bounds. First, we are able to carry over and substantially improve on tractability results for decision trees, obtaining a $(6\delta D S)^S \cdot poly$-time algorithm, where $S$ is the number of cuts in the tree ensemble, $D$ the largest domain size, and $\delta$ is the largest number of features in which two examples differ. To achieve this, we introduce the witness-tree technique which also seems promising for practice. Second, we show that dynamic programming, which has been successful for decision trees, may also be viable for tree ensembles, providing an $\ell^n \cdot poly$-t
&lt;/p&gt;</description></item><item><title>&#23376;&#37319;&#26679;&#26159;&#33258;&#36866;&#24212;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#20851;&#38190;&#26041;&#27861;&#65292;&#20165;&#38656;&#22522;&#20110;&#38543;&#26426;&#23376;&#26679;&#26412;&#21644;&#23569;&#37327;&#27604;&#29305;&#36755;&#20986;&#30340;&#26597;&#35810;&#65292;&#21363;&#21487;&#20445;&#35777;&#20195;&#34920;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.08661</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#23376;&#37319;&#26679;&#36275;&#22815;
&lt;/p&gt;
&lt;p&gt;
Subsampling Suffices for Adaptive Data Analysis. (arXiv:2302.08661v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08661
&lt;/p&gt;
&lt;p&gt;
&#23376;&#37319;&#26679;&#26159;&#33258;&#36866;&#24212;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#20851;&#38190;&#26041;&#27861;&#65292;&#20165;&#38656;&#22522;&#20110;&#38543;&#26426;&#23376;&#26679;&#26412;&#21644;&#23569;&#37327;&#27604;&#29305;&#36755;&#20986;&#30340;&#26597;&#35810;&#65292;&#21363;&#21487;&#20445;&#35777;&#20195;&#34920;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#23545;&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#20195;&#34920;&#25972;&#20010;&#26679;&#26412;&#24635;&#20307;&#26159;&#32479;&#35745;&#23398;&#20013;&#30340;&#26680;&#24515;&#38382;&#39064;&#20043;&#19968;&#12290;&#22823;&#22810;&#25968;&#32463;&#20856;&#25216;&#26415;&#20551;&#35774;&#25968;&#25454;&#38598;&#19982;&#20998;&#26512;&#24072;&#30340;&#26597;&#35810;&#26080;&#20851;&#65292;&#24182;&#22312;&#22810;&#27425;&#12289;&#33258;&#36866;&#24212;&#36873;&#25321;&#30340;&#26597;&#35810;&#20013;&#22833;&#25928;&#12290;&#36825;&#20010;&#8220;&#33258;&#36866;&#24212;&#25968;&#25454;&#20998;&#26512;&#8221;&#38382;&#39064;&#22312;Dwork&#31561;&#20154;&#65288;STOC&#65292;2015&#65289;&#21644;Hardt&#21644;Ullman&#65288;FOCS&#65292;2014&#65289;&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;&#20013;&#24471;&#21040;&#20102;&#24418;&#24335;&#21270;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#30340;&#20551;&#35774;&#38598;&#65292;&#20351;&#24471;&#21363;&#20351;&#22312;&#33258;&#36866;&#24212;&#36873;&#25321;&#30340;&#24773;&#20917;&#19979;&#65292;&#26597;&#35810;&#20173;&#28982;&#20855;&#26377;&#20195;&#34920;&#24615;&#65306;&#21807;&#19968;&#30340;&#35201;&#27714;&#26159;&#27599;&#20010;&#26597;&#35810;&#37319;&#29992;&#38543;&#26426;&#23376;&#26679;&#26412;&#20316;&#20026;&#36755;&#20837;&#24182;&#36755;&#20986;&#23569;&#37327;&#27604;&#29305;&#12290;&#36825;&#20010;&#32467;&#26524;&#34920;&#26126;&#65292;&#23376;&#37319;&#26679;&#20013;&#22266;&#26377;&#30340;&#22122;&#38899;&#36275;&#20197;&#20445;&#35777;&#26597;&#35810;&#30340;&#21709;&#24212;&#20855;&#26377;&#27867;&#21270;&#24615;&#12290;&#36825;&#31181;&#22522;&#20110;&#23376;&#37319;&#26679;&#30340;&#26694;&#26550;&#30340;&#31616;&#21333;&#24615;&#20351;&#20854;&#33021;&#22815;&#27169;&#25311;&#20043;&#21069;&#30740;&#31350;&#25152;&#26410;&#28085;&#30422;&#30340;&#21508;&#31181;&#23454;&#38469;&#24773;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensuring that analyses performed on a dataset are representative of the entire population is one of the central problems in statistics. Most classical techniques assume that the dataset is independent of the analyst's query and break down in the common setting where a dataset is reused for multiple, adaptively chosen, queries. This problem of \emph{adaptive data analysis} was formalized in the seminal works of Dwork et al. (STOC, 2015) and Hardt and Ullman (FOCS, 2014).  We identify a remarkably simple set of assumptions under which the queries will continue to be representative even when chosen adaptively: The only requirements are that each query takes as input a random subsample and outputs few bits. This result shows that the noise inherent in subsampling is sufficient to guarantee that query responses generalize. The simplicity of this subsampling-based framework allows it to model a variety of real-world scenarios not covered by prior work.  In addition to its simplicity, we demo
&lt;/p&gt;</description></item></channel></rss>