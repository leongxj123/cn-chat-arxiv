<rss version="2.0"><channel><title>Chat Arxiv cs.DS</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DS</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#32422;&#26463;&#20849;&#21333;&#35843;&#26497;&#23567;-&#26497;&#22823;&#20248;&#21270;&#21644;&#20849;&#21333;&#35843;&#21253;&#21547;&#38382;&#39064;&#30340;&#21152;&#36895;&#31639;&#27861;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#31639;&#27861;&#24182;&#23454;&#29616;&#20102;&#36739;&#20248;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>https://arxiv.org/abs/2206.05248</link><description>&lt;p&gt;
&#21152;&#36895;&#31639;&#27861;&#29992;&#20110;&#32422;&#26463;&#38750;&#20984;-&#38750;&#20985;&#26497;&#23567;-&#26497;&#22823;&#20248;&#21270;&#21644;&#20849;&#21333;&#35843;&#21253;&#21547;
&lt;/p&gt;
&lt;p&gt;
Accelerated Algorithms for Constrained Nonconvex-Nonconcave Min-Max Optimization and Comonotone Inclusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.05248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#32422;&#26463;&#20849;&#21333;&#35843;&#26497;&#23567;-&#26497;&#22823;&#20248;&#21270;&#21644;&#20849;&#21333;&#35843;&#21253;&#21547;&#38382;&#39064;&#30340;&#21152;&#36895;&#31639;&#27861;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#31639;&#27861;&#24182;&#23454;&#29616;&#20102;&#36739;&#20248;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#32422;&#26463;&#20849;&#21333;&#35843;&#26497;&#23567;-&#26497;&#22823;&#20248;&#21270;&#65292;&#19968;&#31867;&#32467;&#26500;&#21270;&#30340;&#38750;&#20984;-&#38750;&#20985;&#26497;&#23567;-&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#20197;&#21450;&#23427;&#20204;&#23545;&#20849;&#21333;&#35843;&#21253;&#21547;&#30340;&#25512;&#24191;&#12290;&#22312;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#20013;&#65292;&#25105;&#20204;&#23558;&#26368;&#21021;&#30001;Yoon&#21644;Ryu&#65288;2021&#65289;&#25552;&#20986;&#30340;&#26080;&#32422;&#26463;&#26497;&#23567;-&#26497;&#22823;&#20248;&#21270;&#30340;Extra Anchored Gradient&#65288;EAG&#65289;&#31639;&#27861;&#25193;&#23637;&#21040;&#32422;&#26463;&#20849;&#21333;&#35843;&#26497;&#23567;-&#26497;&#22823;&#20248;&#21270;&#21644;&#20849;&#21333;&#35843;&#21253;&#21547;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#25152;&#26377;&#19968;&#38454;&#26041;&#27861;&#20013;&#30340;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;$O\left(\frac{1}{T}\right)$&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#36845;&#20195;&#25910;&#25947;&#21040;&#35299;&#38598;&#20013;&#30340;&#19968;&#20010;&#28857;&#12290;&#22312;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#36129;&#29486;&#20013;&#65292;&#25105;&#20204;&#23558;&#30001;Lee&#21644;Kim&#65288;2021&#65289;&#24320;&#21457;&#30340;&#24555;&#36895;&#39069;&#22806;&#26799;&#24230;&#65288;FEG&#65289;&#31639;&#27861;&#25193;&#23637;&#21040;&#32422;&#26463;&#20849;&#21333;&#35843;&#26497;&#23567;-&#26497;&#22823;&#20248;&#21270;&#21644;&#20849;&#21333;&#35843;&#21253;&#21547;&#65292;&#24182;&#23454;&#29616;&#20102;&#30456;&#21516;&#30340;$O\left(\frac{1}{T}\right)$&#25910;&#25947;&#36895;&#29575;&#12290;&#36825;&#20010;&#36895;&#29575;&#36866;&#29992;&#20110;&#25991;&#29486;&#20013;&#30740;&#31350;&#36807;&#30340;&#26368;&#24191;&#27867;&#30340;&#20849;&#21333;&#35843;&#21253;&#21547;&#38382;&#39064;&#38598;&#21512;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#22522;&#20110;s&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study constrained comonotone min-max optimization, a structured class of nonconvex-nonconcave min-max optimization problems, and their generalization to comonotone inclusion. In our first contribution, we extend the Extra Anchored Gradient (EAG) algorithm, originally proposed by Yoon and Ryu (2021) for unconstrained min-max optimization, to constrained comonotone min-max optimization and comonotone inclusion, achieving an optimal convergence rate of $O\left(\frac{1}{T}\right)$ among all first-order methods. Additionally, we prove that the algorithm's iterations converge to a point in the solution set. In our second contribution, we extend the Fast Extra Gradient (FEG) algorithm, as developed by Lee and Kim (2021), to constrained comonotone min-max optimization and comonotone inclusion, achieving the same $O\left(\frac{1}{T}\right)$ convergence rate. This rate is applicable to the broadest set of comonotone inclusion problems yet studied in the literature. Our analyses are based on s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#25955;&#24335;&#23398;&#20064;&#21160;&#21147;&#23398;&#20013;&#20010;&#20307;&#32676;&#20307;&#30340;&#21147;&#37327;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20998;&#25955;&#24335;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#35774;&#32622;&#65292;&#24182;&#20998;&#26512;&#20102;&#20960;&#20010;&#38024;&#23545;&#27492;&#20219;&#21153;&#30340;&#20998;&#25955;&#24335;&#21160;&#21147;&#23398;&#23478;&#26063;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#21160;&#21147;&#23398;&#19982;&#19968;&#31867;&#8220;&#38646;&#21644;&#8221;&#20056;&#27861;&#26435;&#37325;&#26356;&#26032;&#31639;&#27861;&#30340;&#32852;&#31995;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#20998;&#26512;&#36825;&#20123;&#21327;&#35758;&#30340;&#32676;&#20307;&#32423;&#36951;&#25022;&#12290;&#22312;&#24191;&#27867;&#30340;&#21442;&#25968;&#33539;&#22260;&#19979;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#27425;&#32447;&#24615;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2306.08670</link><description>&lt;p&gt;
&#20998;&#25955;&#24335;&#23398;&#20064;&#21160;&#21147;&#23398;&#20013;&#20010;&#20307;&#32676;&#20307;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
The Power of Populations in Decentralized Learning Dynamics. (arXiv:2306.08670v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#25955;&#24335;&#23398;&#20064;&#21160;&#21147;&#23398;&#20013;&#20010;&#20307;&#32676;&#20307;&#30340;&#21147;&#37327;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20998;&#25955;&#24335;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#35774;&#32622;&#65292;&#24182;&#20998;&#26512;&#20102;&#20960;&#20010;&#38024;&#23545;&#27492;&#20219;&#21153;&#30340;&#20998;&#25955;&#24335;&#21160;&#21147;&#23398;&#23478;&#26063;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#21160;&#21147;&#23398;&#19982;&#19968;&#31867;&#8220;&#38646;&#21644;&#8221;&#20056;&#27861;&#26435;&#37325;&#26356;&#26032;&#31639;&#27861;&#30340;&#32852;&#31995;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#20998;&#26512;&#36825;&#20123;&#21327;&#35758;&#30340;&#32676;&#20307;&#32423;&#36951;&#25022;&#12290;&#22312;&#24191;&#27867;&#30340;&#21442;&#25968;&#33539;&#22260;&#19979;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#27425;&#32447;&#24615;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#20998;&#25955;&#24335;&#22810;&#33218;&#36172;&#21338;&#26426;&#35774;&#32622;&#65292;&#22312;&#19968;&#20010;&#30001;$n$&#20010;&#21463;&#20869;&#23384;&#38480;&#21046;&#30340;&#33410;&#28857;&#32452;&#25104;&#30340;&#31181;&#32676;&#20013;&#65292;&#37319;&#29992;&#20102;&#35875;&#35328;&#27169;&#22411;&#65306;&#27599;&#36718;&#65292;&#27599;&#20010;&#33410;&#28857;&#26412;&#22320;&#37319;&#29992;$m$&#20010;&#33218;&#20043;&#19968;&#65292;&#35266;&#23519;&#20174;&#33218;&#30340;&#65288;&#23545;&#25239;&#36873;&#25321;&#30340;&#65289;&#20998;&#24067;&#20013;&#25277;&#21462;&#30340;&#22870;&#21169;&#65292;&#28982;&#21518;&#19982;&#38543;&#26426;&#25277;&#21462;&#30340;&#37051;&#23621;&#36827;&#34892;&#36890;&#20449;&#65292;&#20132;&#25442;&#20449;&#24687;&#65292;&#20197;&#30830;&#23450;&#19979;&#19968;&#36718;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#20171;&#32461;&#24182;&#20998;&#26512;&#20102;&#20960;&#20010;&#38024;&#23545;&#27492;&#20219;&#21153;&#30340;&#20998;&#25955;&#24335;&#21160;&#21147;&#23398;&#23478;&#26063;&#65306;&#27599;&#20010;&#33410;&#28857;&#30340;&#20915;&#31574;&#23436;&#20840;&#26159;&#23616;&#37096;&#30340;&#65292;&#21482;&#20381;&#36182;&#20110;&#20854;&#26368;&#26032;&#33719;&#24471;&#30340;&#22870;&#21169;&#20197;&#21450;&#23427;&#25277;&#26679;&#30340;&#37051;&#23621;&#30340;&#22870;&#21169;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#20998;&#25955;&#24335;&#21160;&#21147;&#23398;&#30340;&#20840;&#23616;&#28436;&#21270;&#19982;&#29305;&#23450;&#31867;&#22411;&#30340;&#8220;&#38646;&#21644;&#8221;&#20056;&#27861;&#26435;&#37325;&#26356;&#26032;&#31639;&#27861;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#19988;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#26512;&#36825;&#20123;&#33258;&#28982;&#21327;&#35758;&#30340;&#32676;&#20307;&#32423;&#36951;&#25022;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#21442;&#25968;&#33539;&#22260;&#65288;&#21363;&#65292;&#31181;&#32676;&#30340;&#22823;&#23567;&#21644;nu&#30340;&#22823;&#23567;&#65289;&#19979;&#25512;&#23548;&#20102;&#27425;&#32447;&#24615;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a distributed multi-armed bandit setting among a population of $n$ memory-constrained nodes in the gossip model: at each round, every node locally adopts one of $m$ arms, observes a reward drawn from the arm's (adversarially chosen) distribution, and then communicates with a randomly sampled neighbor, exchanging information to determine its policy in the next round. We introduce and analyze several families of dynamics for this task that are decentralized: each node's decision is entirely local and depends only on its most recently obtained reward and that of the neighbor it sampled. We show a connection between the global evolution of these decentralized dynamics with a certain class of "zero-sum" multiplicative weights update algorithms, and we develop a general framework for analyzing the population-level regret of these natural protocols. Using this framework, we derive sublinear regret bounds under a wide range of parameter regimes (i.e., the size of the population and nu
&lt;/p&gt;</description></item></channel></rss>