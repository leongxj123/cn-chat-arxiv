<rss version="2.0"><channel><title>Chat Arxiv cs.DS</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DS</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#26597;&#35810;&#25104;&#26412;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#32431;&#22312;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#26426;&#25506;&#32034;&#33539;&#24335;&#23454;&#29616;&#22312;&#32447;&#23398;&#20064;&#65292;&#24182;&#35774;&#35745;&#20102;&#33021;&#22312;NP-hard&#24773;&#20917;&#19979;&#36816;&#34892;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01400</link><description>&lt;p&gt;
&#20302;&#26597;&#35810;&#25104;&#26412;&#24102;&#22122;&#22768;or&#21516;&#26102;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Query-Efficient Correlation Clustering with Noisy Oracle
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#26597;&#35810;&#25104;&#26412;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#32431;&#22312;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#26426;&#25506;&#32034;&#33539;&#24335;&#23454;&#29616;&#22312;&#32447;&#23398;&#20064;&#65292;&#24182;&#35774;&#35745;&#20102;&#33021;&#22312;NP-hard&#24773;&#20917;&#19979;&#36816;&#34892;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#24120;&#35265;&#30340;&#32858;&#31867;&#35774;&#32622;&#65292;&#20854;&#20013;&#25105;&#20204;&#38656;&#35201;&#23545;n&#20010;&#20803;&#32032;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23613;&#21487;&#33021;&#23569;&#22320;&#21521;&#36820;&#22238;&#20004;&#20010;&#20803;&#32032;&#30456;&#20284;&#24615;&#30340;&#26377;&#22122;&#22768;&#30340;oracle&#26597;&#35810;&#12290;&#25105;&#20204;&#30340;&#35774;&#32622;&#28085;&#30422;&#20102;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#65292;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#65292;&#30456;&#20284;&#24615;&#20989;&#25968;&#35745;&#31639;&#36215;&#26469;&#25104;&#26412;&#39640;&#24182;&#19988; inherently noisy&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#32431;&#22312;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#26426;&#25506;&#32034;&#33539;&#24335;(PE-CMAB)&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#30340;&#26032;&#39062;&#34920;&#36798;&#26041;&#27861;&#22266;&#23450;&#32622;&#20449;&#24230;&#21644;&#22266;&#23450;&#39044;&#31639;&#35774;&#32622;&#12290;&#23545;&#20110;&#36825;&#20004;&#31181;&#35774;&#32622;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#23558;&#25277;&#26679;&#31574;&#30053;&#19982;&#32463;&#20856;&#30340;&#30456;&#20851;&#32858;&#31867;&#36817;&#20284;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340;&#31639;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#20204;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#36825;&#26679;&#30340;&#65306;&#36825;&#20123;&#31639;&#27861;&#26159;&#31532;&#19968;&#20010;&#22312;&#24213;&#23618;&#31163;&#32447;&#20248;&#21270;&#38382;&#39064;&#20026;NP-hard&#30340;&#24773;&#20917;&#19979;&#36816;&#34892;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#30340;&#20363;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a general clustering setting in which we have $n$ elements to be clustered, and we aim to perform as few queries as possible to an oracle that returns a noisy sample of the similarity between two elements. Our setting encompasses many application domains in which the similarity function is costly to compute and inherently noisy. We propose two novel formulations of online learning problems rooted in the paradigm of Pure Exploration in Combinatorial Multi-Armed Bandits (PE-CMAB): fixed confidence and fixed budget settings. For both settings, we design algorithms that combine a sampling strategy with a classic approximation algorithm for correlation clustering and study their theoretical guarantees. Our results are the first examples of polynomial-time algorithms that work for the case of PE-CMAB in which the underlying offline optimization problem is NP-hard.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22312;&#26377;&#30028;&#23616;&#37096;&#27425;&#26799;&#24230;&#21464;&#21270;&#30340;&#26465;&#20214;&#19979;&#30740;&#31350;&#38750;&#20809;&#28369;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#30340;&#30446;&#26631;&#20989;&#25968;&#31867;&#33021;&#24110;&#21161;&#26356;&#22909;&#29702;&#35299;&#20256;&#32479;&#20248;&#21270;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#38477;&#20302;oracle&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.16317</link><description>&lt;p&gt;
&#22312;&#26356;&#32454;&#31890;&#24230;&#19978;&#30340;&#20248;&#21270;&#65306;&#26377;&#30028;&#23616;&#37096;&#27425;&#26799;&#24230;&#21464;&#21270;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Optimization on a Finer Scale: Bounded Local Subgradient Variation Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16317
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22312;&#26377;&#30028;&#23616;&#37096;&#27425;&#26799;&#24230;&#21464;&#21270;&#30340;&#26465;&#20214;&#19979;&#30740;&#31350;&#38750;&#20809;&#28369;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#30340;&#30446;&#26631;&#20989;&#25968;&#31867;&#33021;&#24110;&#21161;&#26356;&#22909;&#29702;&#35299;&#20256;&#32479;&#20248;&#21270;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#38477;&#20302;oracle&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#26377;&#30028;&#23616;&#37096;&#27425;&#26799;&#24230;&#21464;&#21270;&#30340;&#26465;&#20214;&#19979;&#24320;&#22987;&#30740;&#31350;&#38750;&#20809;&#28369;&#20248;&#21270;&#38382;&#39064;&#65292;&#23427;&#20551;&#35774;&#22312;&#28857;&#38468;&#36817;&#30340;&#23567;&#21306;&#22495;&#20869;&#65292;(&#27425;)&#26799;&#24230;&#20043;&#38388;&#23384;&#22312;&#26377;&#38480;&#30340;&#24046;&#24322;&#65292;&#21487;&#20197;&#29992;&#24179;&#22343;&#25110;&#26368;&#22823;&#26041;&#24335;&#27714;&#20540;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#30446;&#26631;&#20989;&#25968;&#31867;&#21253;&#25324;&#20256;&#32479;&#20248;&#21270;&#20013;&#20256;&#32479;&#30740;&#31350;&#30340;&#30446;&#26631;&#20989;&#25968;&#31867;&#65292;&#36825;&#20123;&#31867;&#26681;&#25454;&#30446;&#26631;&#20989;&#25968;&#30340;Lipschitz&#36830;&#32493;&#24615;&#25110;&#20854;&#26799;&#24230;&#30340;H\"{o}lder/Lipschitz&#36830;&#32493;&#24615;&#23450;&#20041;&#12290;&#27492;&#22806;&#65292;&#35813;&#23450;&#20041;&#31867;&#21253;&#21547;&#37027;&#20123;&#26082;&#19981;&#26159;Lipschitz&#36830;&#32493;&#30340;&#20063;&#27809;&#26377;H\"{o}lder&#36830;&#32493;&#26799;&#24230;&#30340;&#20989;&#25968;&#12290;&#24403;&#38480;&#21046;&#22312;&#20256;&#32479;&#20248;&#21270;&#38382;&#39064;&#31867;&#26102;&#65292;&#23450;&#20041;&#30740;&#31350;&#31867;&#30340;&#21442;&#25968;&#23548;&#33268;&#26356;&#21152;&#31934;&#32454;&#30340;&#22797;&#26434;&#24615;&#30028;&#38480;&#65292;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#24674;&#22797;&#20256;&#32479;&#30340;oracle&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#20294;&#19968;&#33324;&#24773;&#20917;&#19979;&#20250;&#23548;&#33268;&#37027;&#20123;&#19981;&#26159;&#8220;&#26368;&#22351;&#24773;&#20917;&#8221;&#30340;&#20989;&#25968;&#20855;&#26377;&#36739;&#20302;&#30340;oracle &#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16317v1 Announce Type: cross  Abstract: We initiate the study of nonsmooth optimization problems under bounded local subgradient variation, which postulates bounded difference between (sub)gradients in small local regions around points, in either average or maximum sense. The resulting class of objective functions encapsulates the classes of objective functions traditionally studied in optimization, which are defined based on either Lipschitz continuity of the objective or H\"{o}lder/Lipschitz continuity of its gradient. Further, the defined class contains functions that are neither Lipschitz continuous nor have a H\"{o}lder continuous gradient. When restricted to the traditional classes of optimization problems, the parameters defining the studied classes lead to more fine-grained complexity bounds, recovering traditional oracle complexity bounds in the worst case but generally leading to lower oracle complexity for functions that are not ``worst case.'' Some highlights of 
&lt;/p&gt;</description></item></channel></rss>