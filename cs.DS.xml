<rss version="2.0"><channel><title>Chat Arxiv cs.DS</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.DS</description><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#27809;&#26377;&#20998;&#24067;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#24046;&#20998;&#38544;&#31169;&#23376;&#31354;&#38388;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#30340;&#25968;&#25454;&#28857;&#65292;&#21487;&#20197;&#31169;&#23494;&#22320;&#35782;&#21035;&#20986;&#20302;&#32500;&#32467;&#26500;&#65292;&#36991;&#20813;&#20102;&#39640;&#32500;&#24230;&#30340;&#20195;&#20215;&#12290;</title><link>https://arxiv.org/abs/2402.06465</link><description>&lt;p&gt;
&#20851;&#20110;&#26080;&#20998;&#24067;&#20551;&#35774;&#30340;&#24046;&#20998;&#38544;&#31169;&#23376;&#31354;&#38388;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
On Differentially Private Subspace Estimation Without Distributional Assumptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#27809;&#26377;&#20998;&#24067;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#24046;&#20998;&#38544;&#31169;&#23376;&#31354;&#38388;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#30340;&#25968;&#25454;&#28857;&#65292;&#21487;&#20197;&#31169;&#23494;&#22320;&#35782;&#21035;&#20986;&#20302;&#32500;&#32467;&#26500;&#65292;&#36991;&#20813;&#20102;&#39640;&#32500;&#24230;&#30340;&#20195;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#25968;&#25454;&#20998;&#26512;&#38754;&#20020;&#30528;&#19968;&#20010;&#34987;&#31216;&#20026;&#32500;&#25968;&#35781;&#21650;&#30340;&#37325;&#22823;&#25361;&#25112;&#65292;&#23548;&#33268;&#20102;&#25104;&#26412;&#30340;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#25968;&#25454;&#38598;&#20855;&#26377;&#22266;&#26377;&#30340;&#20302;&#32500;&#32467;&#26500;&#12290;&#20363;&#22914;&#65292;&#22312;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#36807;&#31243;&#20013;&#65292;&#26799;&#24230;&#32463;&#24120;&#20301;&#20110;&#19968;&#20010;&#20302;&#32500;&#23376;&#31354;&#38388;&#38468;&#36817;&#12290;&#22914;&#26524;&#21487;&#20197;&#20351;&#29992;&#23569;&#37327;&#28857;&#31169;&#23494;&#22320;&#35782;&#21035;&#20986;&#36825;&#31181;&#20302;&#32500;&#32467;&#26500;&#65292;&#23601;&#21487;&#20197;&#36991;&#20813;&#22240;&#39640;&#32500;&#24230;&#32780;&#25903;&#20184;&#38544;&#31169;&#21644;&#20934;&#30830;&#24615;&#30340;&#20195;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Private data analysis faces a significant challenge known as the curse of dimensionality, leading to increased costs. However, many datasets possess an inherent low-dimensional structure. For instance, during optimization via gradient descent, the gradients frequently reside near a low-dimensional subspace. If the low-dimensional structure could be privately identified using a small amount of points, we could avoid paying (in terms of privacy and accuracy) for the high ambient dimension.   On the negative side, Dwork, Talwar, Thakurta, and Zhang (STOC 2014) proved that privately estimating subspaces, in general, requires an amount of points that depends on the dimension. But Singhal and Steinke (NeurIPS 2021) bypassed this limitation by considering points that are i.i.d. samples from a Gaussian distribution whose covariance matrix has a certain eigenvalue gap. Yet, it was still left unclear whether we could provide similar upper bounds without distributional assumptions and whether we 
&lt;/p&gt;</description></item></channel></rss>