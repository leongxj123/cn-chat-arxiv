# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Data quality dimensions for fair AI.](http://arxiv.org/abs/2305.06967) | 本文着眼于解决AI系统中的偏见问题，从信息质量维度的角度出发提出了解决偏见的潜在改进，提出了完整性、一致性、及时性和可靠性等数据质量维度。 |

# 详细

[^1]: 面向公正AI的数据质量维度

    Data quality dimensions for fair AI. (arXiv:2305.06967v1 [cs.AI])

    [http://arxiv.org/abs/2305.06967](http://arxiv.org/abs/2305.06967)

    本文着眼于解决AI系统中的偏见问题，从信息质量维度的角度出发提出了解决偏见的潜在改进，提出了完整性、一致性、及时性和可靠性等数据质量维度。

    

    人工智能系统并非本质上具有中立性，因此偏见会渗透到任何类型的技术工具中。特别是在处理人类时，AI算法会反映出源于错标记数据的技术错误。由于它们提供了错误和歧视性的分类，延续了结构性种族主义和边缘化现象，这些系统并未系统地防范偏见。本文从信息质量维度的角度考虑了AI系统偏见问题，以两个通常较为困难的情境，即非二元个体的分类和跨性别个体的分类为例，说明了偏见缓解工具的潜在改进。确定要实施的数据质量维度以实现更公平的目的可能有助于解决这个问题，因此我们提出建议在完整性、一致性、及时性和可靠性等方面考虑这个问题，并提供了一些理论结果。

    AI systems are not intrinsically neutral and biases trickle in any type of technological tool. In particular when dealing with people, AI algorithms reflect technical errors originating with mislabeled data. As they feed wrong and discriminatory classifications, perpetuating structural racism and marginalization, these systems are not systematically guarded against bias. In this article we consider the problem of bias in AI systems from the point of view of Information Quality dimensions. We illustrate potential improvements of a bias mitigation tool in gender classification errors, referring to two typically difficult contexts: the classification of non-binary individuals and the classification of transgender individuals. The identification of data quality dimensions to implement in bias mitigation tool may help achieve more fairness. Hence, we propose to consider this issue in terms of completeness, consistency, timeliness and reliability, and offer some theoretical results.
    

