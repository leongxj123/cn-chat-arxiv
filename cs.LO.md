# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Theoretical foundations for programmatic reinforcement learning](https://arxiv.org/abs/2402.11650) | 本文旨在探讨程序化强化学习的理论基础，给出了对程序化强化学习中关键问题的初步回答 |

# 详细

[^1]: 程序化强化学习的理论基础

    Theoretical foundations for programmatic reinforcement learning

    [https://arxiv.org/abs/2402.11650](https://arxiv.org/abs/2402.11650)

    本文旨在探讨程序化强化学习的理论基础，给出了对程序化强化学习中关键问题的初步回答

    

    强化学习领域致力于在未知的随机环境中学习最优策略的算法。程序化强化学习研究将策略表示为程序，即涉及控制循环等高阶构造。尽管在机器学习和形式方法交叉领域吸引了很多关注，但在程序化强化学习的理论方面知之甚少：什么样的程序化策略是好的？最优程序化策略有多大？我们如何学习它们？本文的目标是首次回答这些问题，启动对程序化强化学习的理论研究。

    arXiv:2402.11650v1 Announce Type: new  Abstract: The field of Reinforcement Learning (RL) is concerned with algorithms for learning optimal policies in unknown stochastic environments. Programmatic RL studies representations of policies as programs, meaning involving higher order constructs such as control loops. Despite attracting a lot of attention at the intersection of the machine learning and formal methods communities, very little is known on the theoretical front about programmatic RL: what are good classes of programmatic policies? How large are optimal programmatic policies? How can we learn them? The goal of this paper is to give first answers to these questions, initiating a theoretical study of programmatic RL.
    

