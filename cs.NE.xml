<rss version="2.0"><channel><title>Chat Arxiv cs.NE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NE</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#24418;&#24577;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#39640;&#25928;&#30340;&#22240;&#24335;&#20998;&#35299;&#32593;&#32476;&#26469;&#29702;&#35299;&#35270;&#35273;&#22330;&#26223;&#24182;&#25512;&#26029;&#29289;&#20307;&#21644;&#23039;&#21183;&#12290;&#20851;&#38190;&#21019;&#26032;&#21253;&#25324;&#22522;&#20110;&#22797;&#20540;&#21521;&#37327;&#30340;&#35745;&#31639;&#26694;&#26550;VSA&#12289;&#29992;&#20110;&#22788;&#29702;&#24179;&#31227;&#21644;&#26059;&#36716;&#30340;&#20998;&#23618;&#35856;&#25391;&#22120;&#32593;&#32476;HRN&#35774;&#35745;&#65292;&#20197;&#21450;&#22312;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#19978;&#23454;&#29616;&#22797;&#20540;&#35856;&#25391;&#22120;&#32593;&#32476;&#30340;&#22810;&#32452;&#20998;&#33033;&#20914;&#30456;&#20301;&#31070;&#32463;&#20803;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2208.12880</link><description>&lt;p&gt;
&#20855;&#26377;&#35856;&#25391;&#22120;&#32593;&#32476;&#30340;&#31070;&#32463;&#24418;&#24577;&#35270;&#35273;&#22330;&#26223;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Neuromorphic Visual Scene Understanding with Resonator Networks. (arXiv:2208.12880v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12880
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#24418;&#24577;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#39640;&#25928;&#30340;&#22240;&#24335;&#20998;&#35299;&#32593;&#32476;&#26469;&#29702;&#35299;&#35270;&#35273;&#22330;&#26223;&#24182;&#25512;&#26029;&#29289;&#20307;&#21644;&#23039;&#21183;&#12290;&#20851;&#38190;&#21019;&#26032;&#21253;&#25324;&#22522;&#20110;&#22797;&#20540;&#21521;&#37327;&#30340;&#35745;&#31639;&#26694;&#26550;VSA&#12289;&#29992;&#20110;&#22788;&#29702;&#24179;&#31227;&#21644;&#26059;&#36716;&#30340;&#20998;&#23618;&#35856;&#25391;&#22120;&#32593;&#32476;HRN&#35774;&#35745;&#65292;&#20197;&#21450;&#22312;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#19978;&#23454;&#29616;&#22797;&#20540;&#35856;&#25391;&#22120;&#32593;&#32476;&#30340;&#22810;&#32452;&#20998;&#33033;&#20914;&#30456;&#20301;&#31070;&#32463;&#20803;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#35270;&#35273;&#22330;&#26223;&#24182;&#25512;&#26029;&#20854;&#21508;&#20010;&#29289;&#20307;&#30340;&#36523;&#20221;&#21644;&#23039;&#21183;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#24418;&#24577;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#21033;&#29992;&#20102;&#22522;&#20110;&#19977;&#20010;&#20851;&#38190;&#27010;&#24565;&#30340;&#39640;&#25928;&#30340;&#22240;&#24335;&#20998;&#35299;&#32593;&#32476;&#65306;&#65288;1&#65289;&#22522;&#20110;&#22797;&#20540;&#21521;&#37327;&#30340;&#30690;&#37327;&#31526;&#21495;&#20307;&#31995;&#26550;&#26500;(VSA)&#30340;&#35745;&#31639;&#26694;&#26550;&#65307;&#65288;2&#65289;&#29992;&#20110;&#22788;&#29702;&#35270;&#35273;&#22330;&#26223;&#20013;&#24179;&#31227;&#21644;&#26059;&#36716;&#30340;&#38750;&#21487;&#20132;&#25442;&#24615;&#30340;&#20998;&#23618;&#35856;&#25391;&#22120;&#32593;&#32476;&#65288;HRN&#65289;&#30340;&#35774;&#35745;&#65292;&#24403;&#20004;&#32773;&#32467;&#21512;&#20351;&#29992;&#26102;&#65307;&#65288;3&#65289;&#35774;&#35745;&#20102;&#19968;&#31181;&#22810;&#32452;&#20998;&#33033;&#20914;&#30456;&#20301;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#19978;&#23454;&#29616;&#22797;&#20540;&#35856;&#25391;&#22120;&#32593;&#32476;&#12290;VSA&#26694;&#26550;&#20351;&#29992;&#30690;&#37327;&#32465;&#23450;&#25805;&#20316;&#26469;&#20135;&#29983;&#29983;&#25104;&#24335;&#22270;&#20687;&#27169;&#22411;&#65292;&#20854;&#20013;&#32465;&#23450;&#20316;&#20026;&#20960;&#20309;&#21464;&#25442;&#30340;&#31561;&#21464;&#25805;&#20316;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#22330;&#26223;&#21487;&#20197;&#34987;&#25551;&#36848;&#20026;&#21521;&#37327;&#20056;&#31215;&#30340;&#21644;&#65292;&#32780;&#36825;&#20123;&#21521;&#37327;&#20056;&#31215;&#21487;&#20197;&#36890;&#36807;&#35856;&#25391;&#22120;&#32593;&#32476;&#30340;&#22240;&#24335;&#20998;&#35299;&#26469;&#39640;&#25928;&#22320;&#25512;&#26029;&#29289;&#20307;&#21644;&#23427;&#20204;&#30340;&#23039;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding a visual scene by inferring identities and poses of its individual objects is still and open problem. Here we propose a neuromorphic solution that utilizes an efficient factorization network based on three key concepts: (1) a computational framework based on Vector Symbolic Architectures (VSA) with complex-valued vectors; (2) the design of Hierarchical Resonator Networks (HRN) to deal with the non-commutative nature of translation and rotation in visual scenes, when both are used in combination; (3) the design of a multi-compartment spiking phasor neuron model for implementing complex-valued resonator networks on neuromorphic hardware. The VSA framework uses vector binding operations to produce generative image models in which binding acts as the equivariant operation for geometric transformations. A scene can therefore be described as a sum of vector products, which in turn can be efficiently factorized by a resonator network to infer objects and their poses. The HRN ena
&lt;/p&gt;</description></item></channel></rss>