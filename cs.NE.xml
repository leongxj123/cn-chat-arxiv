<rss version="2.0"><channel><title>Chat Arxiv cs.NE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NE</description><item><title>&#20851;&#32852;&#21464;&#25442;&#22120;&#65288;AiT&#65289;&#26159;&#19968;&#31181;&#37319;&#29992;&#20302;&#31209;&#26174;&#24335;&#35760;&#24518;&#21644;&#20851;&#32852;&#35760;&#24518;&#30340;&#31232;&#30095;&#34920;&#31034;&#23398;&#20064;&#22120;&#65292;&#36890;&#36807;&#32852;&#21512;&#31471;&#21040;&#31471;&#35757;&#32451;&#23454;&#29616;&#27169;&#22359;&#29305;&#21270;&#21644;&#27880;&#24847;&#21147;&#29942;&#39048;&#30340;&#24418;&#25104;&#12290;</title><link>http://arxiv.org/abs/2309.12862</link><description>&lt;p&gt;
&#20851;&#32852;&#21464;&#25442;&#22120;&#26159;&#19968;&#31181;&#31232;&#30095;&#34920;&#31034;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Associative Transformer Is A Sparse Representation Learner. (arXiv:2309.12862v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12862
&lt;/p&gt;
&lt;p&gt;
&#20851;&#32852;&#21464;&#25442;&#22120;&#65288;AiT&#65289;&#26159;&#19968;&#31181;&#37319;&#29992;&#20302;&#31209;&#26174;&#24335;&#35760;&#24518;&#21644;&#20851;&#32852;&#35760;&#24518;&#30340;&#31232;&#30095;&#34920;&#31034;&#23398;&#20064;&#22120;&#65292;&#36890;&#36807;&#32852;&#21512;&#31471;&#21040;&#31471;&#35757;&#32451;&#23454;&#29616;&#27169;&#22359;&#29305;&#21270;&#21644;&#27880;&#24847;&#21147;&#29942;&#39048;&#30340;&#24418;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#30340;Transformer&#27169;&#22411;&#20013;&#65292;&#20986;&#29616;&#20102;&#19968;&#31181;&#26032;&#20852;&#30340;&#22522;&#20110;&#31232;&#30095;&#20132;&#20114;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36825;&#31181;&#26426;&#21046;&#19982;&#29983;&#29289;&#21407;&#29702;&#26356;&#20026;&#25509;&#36817;&#12290;&#21253;&#25324;Set Transformer&#21644;Perceiver&#22312;&#20869;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19982;&#26377;&#38480;&#33021;&#21147;&#30340;&#28508;&#22312;&#31354;&#38388;&#30456;&#32467;&#21512;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#22522;&#20110;&#26368;&#36817;&#23545;&#20840;&#23616;&#24037;&#20316;&#31354;&#38388;&#29702;&#35770;&#21644;&#20851;&#32852;&#35760;&#24518;&#30340;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#32852;&#21464;&#25442;&#22120;&#65288;AiT&#65289;&#12290;AiT&#24341;&#20837;&#20102;&#20302;&#31209;&#26174;&#24335;&#35760;&#24518;&#65292;&#26082;&#21487;&#20197;&#20316;&#20026;&#20808;&#39564;&#26469;&#25351;&#23548;&#20849;&#20139;&#24037;&#20316;&#31354;&#38388;&#30340;&#29942;&#39048;&#27880;&#24847;&#21147;&#65292;&#21448;&#21487;&#20197;&#20316;&#20026;&#20851;&#32852;&#35760;&#24518;&#30340;&#21560;&#24341;&#23376;&#12290;&#36890;&#36807;&#32852;&#21512;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#36825;&#20123;&#20808;&#39564;&#33258;&#28982;&#22320;&#21457;&#23637;&#20986;&#27169;&#22359;&#30340;&#29305;&#21270;&#65292;&#27599;&#20010;&#27169;&#22359;&#23545;&#24418;&#25104;&#27880;&#24847;&#21147;&#29942;&#39048;&#30340;&#24402;&#32435;&#20559;&#22909;&#26377;&#25152;&#36129;&#29486;&#12290;&#29942;&#39048;&#21487;&#20197;&#20419;&#36827;&#36755;&#20837;&#20043;&#38388;&#20026;&#23558;&#20449;&#24687;&#20889;&#20837;&#20869;&#23384;&#32780;&#36827;&#34892;&#31454;&#20105;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;AiT&#26159;&#19968;&#31181;&#31232;&#30095;&#34920;&#31034;&#23398;&#20064;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging from the monolithic pairwise attention mechanism in conventional Transformer models, there is a growing interest in leveraging sparse interactions that align more closely with biological principles. Approaches including the Set Transformer and the Perceiver employ cross-attention consolidated with a latent space that forms an attention bottleneck with limited capacity. Building upon recent neuroscience studies of Global Workspace Theory and associative memory, we propose the Associative Transformer (AiT). AiT induces low-rank explicit memory that serves as both priors to guide bottleneck attention in the shared workspace and attractors within associative memory of a Hopfield network. Through joint end-to-end training, these priors naturally develop module specialization, each contributing a distinct inductive bias to form attention bottlenecks. A bottleneck can foster competition among inputs for writing information into the memory. We show that AiT is a sparse representation 
&lt;/p&gt;</description></item></channel></rss>