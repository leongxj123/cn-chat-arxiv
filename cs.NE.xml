<rss version="2.0"><channel><title>Chat Arxiv cs.NE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NE</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Winner-Take-All&#65288;WTA&#65289;&#36873;&#25321;&#24615;&#21644;&#29983;&#29289;&#21551;&#21457;&#30340;&#31283;&#24577;&#26426;&#21046;&#30456;&#32467;&#21512;&#30340;&#8220;&#33258;&#23450;&#20041;&#30446;&#26631;&#8221;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#36793;&#32536;AI&#30828;&#20214;&#19978;&#30340;&#35745;&#31639;&#36164;&#28304;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.12116</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#23450;&#20041;&#29983;&#29289;&#21551;&#21457;&#30446;&#26631;&#30340;&#26080;&#30417;&#30563;&#31471;&#21040;&#31471;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Unsupervised End-to-End Training with a Self-Defined Bio-Inspired Target
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Winner-Take-All&#65288;WTA&#65289;&#36873;&#25321;&#24615;&#21644;&#29983;&#29289;&#21551;&#21457;&#30340;&#31283;&#24577;&#26426;&#21046;&#30456;&#32467;&#21512;&#30340;&#8220;&#33258;&#23450;&#20041;&#30446;&#26631;&#8221;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#36793;&#32536;AI&#30828;&#20214;&#19978;&#30340;&#35745;&#31639;&#36164;&#28304;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20381;&#36182;&#20110;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65288;&#22914;&#33258;&#30417;&#30563;&#23398;&#20064;&#65289;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#25110;&#32773;&#37319;&#29992;&#36890;&#36807;&#31867;&#20284;Hebbian&#23398;&#20064;&#30340;&#29983;&#29289;&#21551;&#21457;&#26041;&#27861;&#36880;&#23618;&#35757;&#32451;&#65292;&#20351;&#29992;&#19982;&#30417;&#30563;&#23398;&#20064;&#19981;&#20860;&#23481;&#30340;&#23616;&#37096;&#23398;&#20064;&#35268;&#21017;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#32593;&#32476;&#26368;&#32456;&#23618;&#30340;&#32988;&#32773;&#36890;&#21507;&#65288;WTA&#65289;&#36873;&#25321;&#24615;&#30340;&#8220;&#33258;&#23450;&#20041;&#30446;&#26631;&#8221;&#65292;&#24182;&#36890;&#36807;&#29983;&#29289;&#21551;&#21457;&#30340;&#31283;&#24577;&#26426;&#21046;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12116v1 Announce Type: cross  Abstract: Current unsupervised learning methods depend on end-to-end training via deep learning techniques such as self-supervised learning, with high computational requirements, or employ layer-by-layer training using bio-inspired approaches like Hebbian learning, using local learning rules incompatible with supervised learning. Both approaches are problematic for edge AI hardware that relies on sparse computational resources and would strongly benefit from alternating between unsupervised and supervised learning phases - thus leveraging widely available unlabeled data from the environment as well as labeled training datasets. To solve this challenge, in this work, we introduce a 'self-defined target' that uses Winner-Take-All (WTA) selectivity at the network's final layer, complemented by regularization through biologically inspired homeostasis mechanism. This approach, framework-agnostic and compatible with both global (Backpropagation) and l
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21463;&#21040;&#31070;&#32463;&#31185;&#23398;&#20013;&#30340;&#20851;&#38190;&#22823;&#33041;&#20551;&#35774;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20803;&#20851;&#38190;&#24615;&#30340;&#39640;&#25928;SNN&#20462;&#21098;&#26041;&#27861;&#65292;&#20197;&#21152;&#24378;&#29305;&#24449;&#25552;&#21462;&#21644;&#21152;&#36895;&#20462;&#21098;&#36807;&#31243;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.16141</link><description>&lt;p&gt;
SNNs&#20013;&#22522;&#20110;&#20851;&#38190;&#24615;&#30340;&#39640;&#25928;&#20462;&#21098;&#26041;&#27861;&#65292;&#21463;&#21040;&#20851;&#38190;&#24615;&#22823;&#33041;&#20551;&#35774;&#30340;&#21551;&#21457;
&lt;/p&gt;
&lt;p&gt;
Criticality-Guided Efficient Pruning in Spiking Neural Networks Inspired by Critical Brain Hypothesis. (arXiv:2311.16141v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.16141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21463;&#21040;&#31070;&#32463;&#31185;&#23398;&#20013;&#30340;&#20851;&#38190;&#22823;&#33041;&#20551;&#35774;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20803;&#20851;&#38190;&#24615;&#30340;&#39640;&#25928;SNN&#20462;&#21098;&#26041;&#27861;&#65292;&#20197;&#21152;&#24378;&#29305;&#24449;&#25552;&#21462;&#21644;&#21152;&#36895;&#20462;&#21098;&#36807;&#31243;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#33410;&#33021;&#21644;&#26080;&#20056;&#27861;&#29305;&#24615;&#65292;SNNs&#24050;&#32463;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#28145;&#24230;SNNs&#35268;&#27169;&#30340;&#19981;&#26029;&#22686;&#38271;&#32473;&#27169;&#22411;&#37096;&#32626;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#32593;&#32476;&#20462;&#21098;&#36890;&#36807;&#21387;&#32553;&#32593;&#32476;&#35268;&#27169;&#26469;&#20943;&#23569;&#27169;&#22411;&#37096;&#32626;&#30340;&#30828;&#20214;&#36164;&#28304;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;SNN&#20462;&#21098;&#26041;&#27861;&#30001;&#20110;&#20462;&#21098;&#36845;&#20195;&#22686;&#21152;&#20102;SNNs&#30340;&#35757;&#32451;&#38590;&#24230;&#65292;&#23548;&#33268;&#20462;&#21098;&#25104;&#26412;&#39640;&#26114;&#19988;&#24615;&#33021;&#25439;&#22833;&#20005;&#37325;&#12290;&#26412;&#25991;&#21463;&#21040;&#31070;&#32463;&#31185;&#23398;&#20013;&#30340;&#20851;&#38190;&#22823;&#33041;&#20551;&#35774;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20803;&#20851;&#38190;&#24615;&#30340;&#29992;&#20110;SNN&#20462;&#21098;&#30340;&#20877;&#29983;&#26426;&#21046;&#65292;&#20197;&#22686;&#24378;&#29305;&#24449;&#25552;&#21462;&#24182;&#21152;&#36895;&#20462;&#21098;&#36807;&#31243;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;SNN&#20013;&#29992;&#20110;&#20851;&#38190;&#24615;&#30340;&#20302;&#25104;&#26412;&#24230;&#37327;&#26041;&#24335;&#12290;&#28982;&#21518;&#65292;&#22312;&#20462;&#21098;&#21518;&#23545;&#25152;&#20462;&#21098;&#32467;&#26500;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#65292;&#24182;&#20877;&#29983;&#37027;&#20123;&#20855;&#26377;&#36739;&#39640;&#20851;&#38190;&#24615;&#30340;&#32467;&#26500;&#65292;&#20197;&#33719;&#21462;&#20851;&#38190;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20248;&#20110;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNNs) have gained considerable attention due to the energy-efficient and multiplication-free characteristics. The continuous growth in scale of deep SNNs poses challenges for model deployment. Network pruning reduces hardware resource requirements of model deployment by compressing the network scale. However, existing SNN pruning methods cause high pruning costs and performance loss because the pruning iterations amplify the training difficulty of SNNs. In this paper, inspired by the critical brain hypothesis in neuroscience, we propose a regeneration mechanism based on the neuron criticality for SNN pruning to enhance feature extraction and accelerate the pruning process. Firstly, we propose a low-cost metric for the criticality in SNNs. Then, we re-rank the pruned structures after pruning and regenerate those with higher criticality to obtain the critical network. Our method achieves higher performance than the current state-of-the-art (SOTA) method with up t
&lt;/p&gt;</description></item></channel></rss>