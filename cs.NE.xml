<rss version="2.0"><channel><title>Chat Arxiv cs.NE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NE</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#26080;&#26799;&#24230;&#28608;&#27963;&#26368;&#22823;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25506;&#32034;&#31070;&#32463;&#32593;&#32476;&#20013;&#31070;&#32463;&#20803;&#30340;&#29305;&#21270;&#12290;&#22312;&#19968;&#20010;&#20154;&#24037;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#19978;&#25104;&#21151;&#27979;&#35797;&#20102;&#36825;&#20010;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#35774;&#35745;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2401.10748</link><description>&lt;p&gt;
&#38024;&#23545;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#31070;&#32463;&#20803;&#30340;&#24555;&#36895;&#26080;&#26799;&#24230;&#28608;&#27963;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Fast gradient-free activation maximization for neurons in spiking neural networks. (arXiv:2401.10748v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#26080;&#26799;&#24230;&#28608;&#27963;&#26368;&#22823;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25506;&#32034;&#31070;&#32463;&#32593;&#32476;&#20013;&#31070;&#32463;&#20803;&#30340;&#29305;&#21270;&#12290;&#22312;&#19968;&#20010;&#20154;&#24037;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#19978;&#25104;&#21151;&#27979;&#35797;&#20102;&#36825;&#20010;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#35774;&#35745;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#65292;&#26080;&#35770;&#26159;&#29983;&#29289;&#36824;&#26159;&#20154;&#24037;&#30340;&#65292;&#37117;&#26159;&#30001;&#31070;&#32463;&#20803;&#26500;&#25104;&#30340;&#22797;&#26434;&#31995;&#32479;&#65292;&#27599;&#20010;&#31070;&#32463;&#20803;&#37117;&#26377;&#33258;&#24049;&#30340;&#19987;&#19994;&#21270;&#12290;&#25581;&#31034;&#36825;&#20123;&#19987;&#19994;&#21270;&#23545;&#20110;&#29702;&#35299;NNs&#30340;&#20869;&#37096;&#24037;&#20316;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#20110;&#19968;&#20010;&#29983;&#29289;&#31995;&#32479;&#65292;&#20854;&#23545;&#21050;&#28608;&#30340;&#31070;&#32463;&#21709;&#24212;&#19981;&#26159;&#24050;&#30693;&#30340;&#65288;&#26356;&#19981;&#29992;&#35828;&#26159;&#21487;&#24494;&#20998;&#30340;&#20989;&#25968;&#65289;&#65292;&#21807;&#19968;&#30340;&#26041;&#24335;&#26159;&#24314;&#31435;&#19968;&#20010;&#21453;&#39304;&#24490;&#29615;&#65292;&#23558;&#20854;&#26292;&#38706;&#20110;&#21050;&#28608;&#20043;&#20013;&#65292;&#20854;&#24615;&#36136;&#21487;&#20197;&#36845;&#20195;&#22320;&#21464;&#21270;&#65292;&#20197;&#23547;&#27714;&#26368;&#22823;&#21709;&#24212;&#30340;&#26041;&#21521;&#12290;&#20026;&#20102;&#22312;&#19968;&#20010;&#29983;&#29289;&#32593;&#32476;&#19978;&#27979;&#35797;&#36825;&#26679;&#30340;&#24490;&#29615;&#65292;&#39318;&#20808;&#38656;&#35201;&#23398;&#20250;&#24555;&#36895;&#21644;&#39640;&#25928;&#22320;&#36816;&#34892;&#23427;&#65292;&#20197;&#22312;&#26368;&#23569;&#30340;&#36845;&#20195;&#27425;&#25968;&#20869;&#36798;&#21040;&#26368;&#26377;&#25928;&#30340;&#21050;&#28608;&#65288;&#26368;&#22823;&#21270;&#26576;&#20123;&#31070;&#32463;&#20803;&#30340;&#28608;&#27963;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#26377;&#25928;&#35774;&#35745;&#30340;&#26694;&#26550;&#65292;&#25104;&#21151;&#22320;&#22312;&#20154;&#24037;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65292;&#27169;&#25311;&#29983;&#29289;&#22823;&#33041;NNs&#34892;&#20026;&#30340;&#27169;&#22411;&#65289;&#19978;&#27979;&#35797;&#20102;&#23427;&#12290;&#25105;&#20204;&#29992;&#20110;&#28608;&#27963;&#26368;&#22823;&#21270;&#65288;AM&#65289;&#30340;&#20248;&#21270;&#26041;&#27861;&#26159;&#22522;&#20110;&#24555;&#26799;&#24230;&#26041;&#27861;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks (NNs), both living and artificial, work due to being complex systems of neurons, each having its own specialization. Revealing these specializations is important for understanding NNs inner working mechanisms. The only way to do this for a living system, the neural response of which to a stimulus is not a known (let alone differentiable) function is to build a feedback loop of exposing it to stimuli, the properties of which can be iteratively varied aiming in the direction of maximal response. To test such a loop on a living network, one should first learn how to run it quickly and efficiently, reaching most effective stimuli (ones that maximize certain neurons activation) in least possible number of iterations. We present a framework with an effective design of such a loop, successfully testing it on an artificial spiking neural network (SNN, a model that mimics the behaviour of NNs in living brains). Our optimization method used for activation maximization (AM) was ba
&lt;/p&gt;</description></item></channel></rss>