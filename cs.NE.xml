<rss version="2.0"><channel><title>Chat Arxiv cs.NE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NE</description><item><title>APALU &#26159;&#19968;&#31181;&#21487;&#35757;&#32451;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#36890;&#36807;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#30340;&#23398;&#20064;&#24615;&#33021;&#65292;&#22312;&#36866;&#24212;&#22797;&#26434;&#25968;&#25454;&#34920;&#31034;&#30340;&#21516;&#26102;&#20445;&#25345;&#31283;&#23450;&#21644;&#39640;&#25928;&#12290; &#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;APALU&#30456;&#23545;&#20110;&#20256;&#32479;&#28608;&#27963;&#20989;&#25968;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08244</link><description>&lt;p&gt;
APALU: &#19968;&#31181;&#21487;&#35757;&#32451;&#12289;&#36866;&#24212;&#24615;&#28608;&#27963;&#20989;&#25968;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
APALU: A Trainable, Adaptive Activation Function for Deep Learning Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08244
&lt;/p&gt;
&lt;p&gt;
APALU &#26159;&#19968;&#31181;&#21487;&#35757;&#32451;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#36890;&#36807;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#30340;&#23398;&#20064;&#24615;&#33021;&#65292;&#22312;&#36866;&#24212;&#22797;&#26434;&#25968;&#25454;&#34920;&#31034;&#30340;&#21516;&#26102;&#20445;&#25345;&#31283;&#23450;&#21644;&#39640;&#25928;&#12290; &#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;APALU&#30456;&#23545;&#20110;&#20256;&#32479;&#28608;&#27963;&#20989;&#25968;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28608;&#27963;&#20989;&#25968;&#26159;&#28145;&#24230;&#23398;&#20064;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#26377;&#21161;&#20110;&#25552;&#21462;&#22797;&#26434;&#30340;&#25968;&#25454;&#27169;&#24335;&#12290;&#34429;&#28982;&#31867;&#20284;ReLU&#21450;&#20854;&#21464;&#31181;&#30340;&#32463;&#20856;&#28608;&#27963;&#20989;&#25968;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#23427;&#20204;&#30340;&#38745;&#24577;&#29305;&#24615;&#21644;&#31616;&#27905;&#24615;&#65292;&#23613;&#31649;&#26377;&#21033;&#65292;&#20294;&#36890;&#24120;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#21487;&#35757;&#32451;&#30340;&#28608;&#27963;&#20989;&#25968;&#26377;&#26102;&#20063;&#38590;&#20197;&#36866;&#24212;&#25968;&#25454;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#38024;&#23545;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35757;&#32451;&#28608;&#27963;&#20989;&#25968;&#65292;&#21363;&#33258;&#36866;&#24212;&#20998;&#27573;&#36924;&#36817;&#28608;&#27963;&#32447;&#24615;&#21333;&#20803;&#65288;APALU&#65289;&#65292;&#20197;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;&#23427;&#20855;&#26377;&#19968;&#22871;&#29420;&#29305;&#30340;&#29305;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#20445;&#25345;&#31283;&#23450;&#21644;&#39640;&#25928;&#65292;&#24182;&#36866;&#24212;&#22797;&#26434;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#23454;&#65292;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#65292;&#19982;&#24191;&#27867;&#20351;&#29992;&#30340;&#28608;&#27963;&#20989;&#25968;&#30456;&#27604;&#65292;APALU&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#65292;APALU&#25552;&#21319;&#20102;MobileNet&#21644;GoogleNet&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Activation function is a pivotal component of deep learning, facilitating the extraction of intricate data patterns. While classical activation functions like ReLU and its variants are extensively utilized, their static nature and simplicity, despite being advantageous, often limit their effectiveness in specialized tasks. The trainable activation functions also struggle sometimes to adapt to the unique characteristics of the data. Addressing these limitations, we introduce a novel trainable activation function, adaptive piecewise approximated activation linear unit (APALU), to enhance the learning performance of deep learning across a broad range of tasks. It presents a unique set of features that enable it to maintain stability and efficiency in the learning process while adapting to complex data representations. Experiments reveal significant improvements over widely used activation functions for different tasks. In image classification, APALU increases MobileNet and GoogleNet accur
&lt;/p&gt;</description></item></channel></rss>