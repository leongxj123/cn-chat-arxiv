<rss version="2.0"><channel><title>Chat Arxiv cs.NE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NE</description><item><title>&#26412;&#30740;&#31350;&#20174;&#20799;&#31461;&#30340;&#35270;&#35282;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#36890;&#36807;&#38271;&#26102;&#38388;&#30340;&#22836;&#25140;&#24335;&#25668;&#20687;&#35760;&#24405;&#35757;&#32451;&#35270;&#39057;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#20419;&#36827;&#20174;&#23569;&#37327;&#26679;&#26412;&#20013;&#23398;&#20064;&#34892;&#21160;&#27010;&#24565;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.00300</link><description>&lt;p&gt;
&#20174;&#20799;&#31461;&#35270;&#35282;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#35270;&#39057;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning of video representations from a child's perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#20799;&#31461;&#30340;&#35270;&#35282;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#36890;&#36807;&#38271;&#26102;&#38388;&#30340;&#22836;&#25140;&#24335;&#25668;&#20687;&#35760;&#24405;&#35757;&#32451;&#35270;&#39057;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#20419;&#36827;&#20174;&#23569;&#37327;&#26679;&#26412;&#20013;&#23398;&#20064;&#34892;&#21160;&#27010;&#24565;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20799;&#31461;&#36890;&#36807;&#20960;&#24180;&#30340;&#33258;&#25105;&#35270;&#35273;&#32463;&#39564;&#23398;&#20064;&#21040;&#20102;&#24378;&#22823;&#30340;&#19990;&#30028;&#20869;&#37096;&#27169;&#22411;&#12290;&#36825;&#20123;&#20869;&#37096;&#27169;&#22411;&#33021;&#21542;&#36890;&#36807;&#20799;&#31461;&#30340;&#35270;&#35273;&#20307;&#39564;&#21644;&#36890;&#29992;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#26469;&#23398;&#20064;&#65292;&#36824;&#26159;&#38656;&#35201;&#24378;&#22823;&#30340;&#24402;&#32435;&#20559;&#24046;&#65311;&#26368;&#36817;&#65292;&#22312;&#25910;&#38598;&#22823;&#35268;&#27169;&#12289;&#32437;&#21521;&#30340;&#21457;&#23637;&#29616;&#23454;&#35270;&#39057;&#25968;&#25454;&#38598;&#20197;&#21450;&#36890;&#29992;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#30340;&#36827;&#23637;&#20351;&#25105;&#20204;&#33021;&#22815;&#24320;&#22987;&#25506;&#35752;&#36825;&#20010;&#26412;&#36136;&#19982;&#20859;&#32946;&#20043;&#38388;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#20851;&#27880;&#22522;&#20110;&#22270;&#20687;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#21644;&#21487;&#20197;&#20174;&#38745;&#24577;&#22270;&#20687;&#20013;&#23398;&#20064;&#30340;&#35270;&#35273;&#33021;&#21147;&#65288;&#20363;&#22914;&#30446;&#26631;&#35782;&#21035;&#65289;&#65292;&#20174;&#32780;&#24573;&#30053;&#20102;&#19990;&#30028;&#30340;&#26102;&#38388;&#24615;&#36136;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#20799;&#31461;&#26089;&#26399;&#21457;&#23637;&#38454;&#27573;&#65288;6-31&#20010;&#26376;&#65289;&#20174;&#20799;&#31461;&#30340;&#22836;&#25140;&#24335;&#25668;&#20687;&#35760;&#24405;&#20013;&#35757;&#32451;&#33258;&#30417;&#30563;&#35270;&#39057;&#27169;&#22411;&#12290;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#22312;&#20419;&#36827;&#20174;&#23569;&#37327;&#26679;&#26412;&#20013;&#23398;&#20064;&#34892;&#21160;&#27010;&#24565;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Children learn powerful internal models of the world around them from a few years of egocentric visual experience. Can such internal models be learned from a child's visual experience with highly generic learning algorithms or do they require strong inductive biases? Recent advances in collecting large-scale, longitudinal, developmentally realistic video datasets and generic self-supervised learning (SSL) algorithms are allowing us to begin to tackle this nature vs. nurture question. However, existing work typically focuses on image-based SSL algorithms and visual capabilities that can be learned from static images (e.g. object recognition), thus ignoring temporal aspects of the world. To close this gap, here we train self-supervised video models on longitudinal, egocentric headcam recordings collected from a child over a two year period in their early development (6-31 months). The resulting models are highly effective at facilitating the learning of action concepts from a small numbe
&lt;/p&gt;</description></item></channel></rss>