<rss version="2.0"><channel><title>Chat Arxiv cs.NE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NE</description><item><title>&#36825;&#39033;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20110;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;(SNNs)&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#40065;&#26834;&#24615;&#21644;&#36716;&#31227;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25104;&#21151;&#30340;&#30333;&#30418;&#23545;&#25239;&#25915;&#20987;SNNs&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#26367;&#20195;&#26799;&#24230;&#25216;&#26415;&#65292;&#24182;&#19988;&#38750;SNN&#26550;&#26500;&#21019;&#24314;&#30340;&#23545;&#25239;&#26679;&#26412;&#24448;&#24448;&#19981;&#34987;SNNs&#35823;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2209.03358</link><description>&lt;p&gt;
&#25915;&#20987;&#33033;&#20914;&#65306;&#20851;&#20110;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#21487;&#36716;&#31227;&#24615;&#19982;&#23433;&#20840;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Attacking the Spike: On the Transferability and Security of Spiking Neural Networks to Adversarial Examples. (arXiv:2209.03358v3 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.03358
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20110;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;(SNNs)&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#40065;&#26834;&#24615;&#21644;&#36716;&#31227;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25104;&#21151;&#30340;&#30333;&#30418;&#23545;&#25239;&#25915;&#20987;SNNs&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#26367;&#20195;&#26799;&#24230;&#25216;&#26415;&#65292;&#24182;&#19988;&#38750;SNN&#26550;&#26500;&#21019;&#24314;&#30340;&#23545;&#25239;&#26679;&#26412;&#24448;&#24448;&#19981;&#34987;SNNs&#35823;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;(SNNs)&#22240;&#20854;&#39640;&#33021;&#25928;&#21644;&#26368;&#36817;&#22312;&#20998;&#31867;&#24615;&#33021;&#19978;&#30340;&#36827;&#23637;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#19982;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;&#23545;SNNs&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#40065;&#26834;&#24615;&#30340;&#20998;&#26512;&#21644;&#30740;&#31350;&#20173;&#28982;&#30456;&#23545;&#19981;&#23436;&#21892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#20110;&#25512;&#36827;SNNs&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#38754;&#65292;&#24182;&#20570;&#20986;&#20102;&#19977;&#20010;&#20027;&#35201;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25104;&#21151;&#30340;&#30333;&#30418;&#23545;&#25239;&#25915;&#20987;SNNs&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#24213;&#23618;&#30340;&#26367;&#20195;&#26799;&#24230;&#25216;&#26415;&#65292;&#21363;&#20351;&#22312;&#23545;&#25239;&#24615;&#35757;&#32451;SNNs&#30340;&#24773;&#20917;&#19979;&#20063;&#19968;&#26679;&#12290;&#20854;&#27425;&#65292;&#21033;&#29992;&#26368;&#20339;&#30340;&#26367;&#20195;&#26799;&#24230;&#25216;&#26415;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#23545;&#25239;&#25915;&#20987;&#22312;SNNs&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26550;&#26500;&#22914;Vision Transformers(ViTs)&#21644;Big Transfer Convolutional Neural Networks(CNNs)&#20043;&#38388;&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#38750;SNN&#26550;&#26500;&#21019;&#24314;&#30340;&#23545;&#25239;&#26679;&#26412;&#24448;&#24448;&#19981;&#34987;SNNs&#35823;&#20998;&#31867;&#12290;&#31532;&#19977;&#65292;&#30001;&#20110;&#32570;&#20047;&#19968;&#20010;&#20849;&#24615;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks (SNNs) have attracted much attention for their high energy efficiency and for recent advances in their classification performance. However, unlike traditional deep learning approaches, the analysis and study of the robustness of SNNs to adversarial examples remain relatively underdeveloped. In this work, we focus on advancing the adversarial attack side of SNNs and make three major contributions. First, we show that successful white-box adversarial attacks on SNNs are highly dependent on the underlying surrogate gradient technique, even in the case of adversarially trained SNNs. Second, using the best surrogate gradient technique, we analyze the transferability of adversarial attacks on SNNs and other state-of-the-art architectures like Vision Transformers (ViTs) and Big Transfer Convolutional Neural Networks (CNNs). We demonstrate that the adversarial examples created by non-SNN architectures are not misclassified often by SNNs. Third, due to the lack of an ubi
&lt;/p&gt;</description></item></channel></rss>