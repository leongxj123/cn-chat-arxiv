<rss version="2.0"><channel><title>Chat Arxiv cs.NE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NE</description><item><title>&#25972;&#21512;&#27491;&#24358;&#20989;&#25968;&#21040;&#20302;&#31209;&#20998;&#35299;&#36807;&#31243;&#20013;&#65292;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20445;&#25345;&#21442;&#25968;&#39640;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.19243</link><description>&lt;p&gt;
&#29992;&#27491;&#24358;&#28608;&#27963;&#30340;&#20302;&#31209;&#30697;&#38453;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sine Activated Low-Rank Matrices for Parameter Efficient Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19243
&lt;/p&gt;
&lt;p&gt;
&#25972;&#21512;&#27491;&#24358;&#20989;&#25968;&#21040;&#20302;&#31209;&#20998;&#35299;&#36807;&#31243;&#20013;&#65292;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20445;&#25345;&#21442;&#25968;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#20998;&#35299;&#24050;&#32463;&#25104;&#20026;&#22312;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#22686;&#24378;&#21442;&#25968;&#25928;&#29575;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#36825;&#20123;&#25216;&#26415;&#26174;&#33879;&#38477;&#20302;&#20102;&#21442;&#25968;&#25968;&#37327;&#65292;&#21462;&#24471;&#20102;&#31616;&#27905;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#24120;&#35265;&#30340;&#25361;&#25112;&#26159;&#22312;&#21442;&#25968;&#25928;&#29575;&#21644;&#27169;&#22411;&#20934;&#30830;&#24615;&#20043;&#38388;&#20570;&#20986;&#22949;&#21327;&#65292;&#21442;&#25968;&#20943;&#23569;&#24448;&#24448;&#23548;&#33268;&#20934;&#30830;&#24615;&#19981;&#21450;&#23436;&#25972;&#31209;&#23545;&#24212;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#22312;&#20302;&#31209;&#20998;&#35299;&#36807;&#31243;&#20013;&#25972;&#21512;&#20102;&#19968;&#20010;&#27491;&#24358;&#20989;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#20445;&#30041;&#20102;&#20302;&#31209;&#26041;&#27861;&#30340;&#21442;&#25968;&#25928;&#29575;&#29305;&#24615;&#30340;&#22909;&#22788;&#65292;&#36824;&#22686;&#21152;&#20102;&#20998;&#35299;&#30340;&#31209;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#35777;&#26126;&#26159;&#29616;&#26377;&#20302;&#31209;&#27169;&#22411;&#30340;&#19968;&#31181;&#36866;&#24212;&#24615;&#22686;&#24378;&#65292;&#27491;&#22914;&#20854;&#25104;&#21151;&#35777;&#23454;&#30340;&#37027;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19243v1 Announce Type: new  Abstract: Low-rank decomposition has emerged as a vital tool for enhancing parameter efficiency in neural network architectures, gaining traction across diverse applications in machine learning. These techniques significantly lower the number of parameters, striking a balance between compactness and performance. However, a common challenge has been the compromise between parameter efficiency and the accuracy of the model, where reduced parameters often lead to diminished accuracy compared to their full-rank counterparts. In this work, we propose a novel theoretical framework that integrates a sinusoidal function within the low-rank decomposition process. This approach not only preserves the benefits of the parameter efficiency characteristic of low-rank methods but also increases the decomposition's rank, thereby enhancing model accuracy. Our method proves to be an adaptable enhancement for existing low-rank models, as evidenced by its successful 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#21040;&#20154;&#31867;&#22823;&#33041;&#35760;&#24518;&#26426;&#21046;&#21551;&#21457;&#30340;&#20998;&#24067;&#24335;&#35760;&#24518;&#23398;&#20064;&#26426;&#21046;&#65292;&#36890;&#36807;&#38543;&#26426;&#36830;&#25509;&#30340;&#31070;&#32463;&#20803;&#35760;&#24518;&#36755;&#20837;&#20449;&#21495;&#30340;&#20851;&#32852;&#65292;&#24182;&#22522;&#20110;&#32622;&#20449;&#24230;&#20851;&#32852;&#20998;&#24067;&#24335;&#35760;&#24518;&#65292;&#33021;&#22815;&#22312;&#26080;&#38656;&#29305;&#24449;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24378;&#21270;&#35760;&#24518;&#36866;&#24212;&#26032;&#39046;&#22495;&#65292;&#36866;&#21512;&#37096;&#32626;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#12290;</title><link>https://arxiv.org/abs/2402.14598</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#33041;&#21551;&#21457;&#30340;&#20998;&#24067;&#24335;&#35760;&#24518;&#23398;&#20064;&#29992;&#20110;&#39640;&#25928;&#30340;&#26080;&#29305;&#24449;&#33258;&#21160;&#36866;&#24212;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
Brain-inspired Distributed Memorization Learning for Efficient Feature-free Unsupervised Domain Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14598
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#21040;&#20154;&#31867;&#22823;&#33041;&#35760;&#24518;&#26426;&#21046;&#21551;&#21457;&#30340;&#20998;&#24067;&#24335;&#35760;&#24518;&#23398;&#20064;&#26426;&#21046;&#65292;&#36890;&#36807;&#38543;&#26426;&#36830;&#25509;&#30340;&#31070;&#32463;&#20803;&#35760;&#24518;&#36755;&#20837;&#20449;&#21495;&#30340;&#20851;&#32852;&#65292;&#24182;&#22522;&#20110;&#32622;&#20449;&#24230;&#20851;&#32852;&#20998;&#24067;&#24335;&#35760;&#24518;&#65292;&#33021;&#22815;&#22312;&#26080;&#38656;&#29305;&#24449;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24378;&#21270;&#35760;&#24518;&#36866;&#24212;&#26032;&#39046;&#22495;&#65292;&#36866;&#21512;&#37096;&#32626;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#34920;&#29616;&#20986;&#26356;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#26410;&#30693;&#29615;&#22659;&#32780;&#26080;&#38656;&#20351;&#29992;&#20219;&#20309;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#31243;&#24207;&#12290;&#21463;&#20154;&#31867;&#22823;&#33041;&#20998;&#24067;&#24335;&#35760;&#24518;&#26426;&#21046;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#20998;&#24067;&#24335;&#35760;&#24518;&#23398;&#20064;&#26426;&#21046;&#65292;&#31216;&#20026;DML&#65292;&#20197;&#25903;&#25345;&#36716;&#31227;&#27169;&#22411;&#30340;&#24555;&#36895;&#39046;&#22495;&#36866;&#24212;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;DML&#37319;&#29992;&#38543;&#26426;&#36830;&#25509;&#30340;&#31070;&#32463;&#20803;&#26469;&#35760;&#24518;&#36755;&#20837;&#20449;&#21495;&#30340;&#20851;&#32852;&#65292;&#36825;&#20123;&#20449;&#21495;&#20316;&#20026;&#20914;&#21160;&#20256;&#25773;&#65292;&#24182;&#36890;&#36807;&#20851;&#32852;&#20998;&#24067;&#24335;&#35760;&#24518;&#30340;&#32622;&#20449;&#24230;&#20570;&#20986;&#26368;&#32456;&#20915;&#31574;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;DML&#33021;&#22815;&#22522;&#20110;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#24378;&#21270;&#35760;&#24518;&#65292;&#24555;&#36895;&#36866;&#24212;&#26032;&#39046;&#22495;&#65292;&#32780;&#26080;&#38656;&#23545;&#28145;&#23618;&#29305;&#24449;&#36827;&#34892;&#32321;&#37325;&#30340;&#24494;&#35843;&#65292;&#36825;&#20351;&#20854;&#38750;&#24120;&#36866;&#21512;&#37096;&#32626;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#12290;&#22522;&#20110;&#22235;&#20010;&#20132;&#21449;&#39046;&#22495;&#30340;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14598v1 Announce Type: cross  Abstract: Compared with gradient based artificial neural networks, biological neural networks usually show a more powerful generalization ability to quickly adapt to unknown environments without using any gradient back-propagation procedure. Inspired by the distributed memory mechanism of human brains, we propose a novel gradient-free Distributed Memorization Learning mechanism, namely DML, to support quick domain adaptation of transferred models. In particular, DML adopts randomly connected neurons to memorize the association of input signals, which are propagated as impulses, and makes the final decision by associating the distributed memories based on their confidence. More importantly, DML is able to perform reinforced memorization based on unlabeled data to quickly adapt to a new domain without heavy fine-tuning of deep features, which makes it very suitable for deploying on edge devices. Experiments based on four cross-domain real-world da
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#21629;&#29702;&#35770;&#21644;&#25511;&#21046;&#35770;&#30340;&#26032;&#35270;&#35282;&#65292;&#36890;&#36807;&#23558;&#25511;&#21046;&#35770;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#31070;&#32463;&#31185;&#23398;&#30340;&#26368;&#26032;&#36827;&#23637;&#19982;&#29983;&#21629;&#29702;&#35770;&#30456;&#32467;&#21512;&#65292;&#23558;&#20869;&#24863;&#30693;&#24212;&#29992;&#20110;&#26500;&#24314;&#20855;&#26377;&#33258;&#20027;&#21644;&#36866;&#24212;&#24615;&#33021;&#21147;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2309.05999</link><description>&lt;p&gt;
&#29983;&#21629;&#21551;&#21457;&#30340;&#33258;&#20027;&#21644;&#36866;&#24212;&#26234;&#33021;&#20026;&#33258;&#20027;&#21644;&#36866;&#24212;&#24615;&#20195;&#29702;&#26500;&#24314;&#20855;&#26377;&#33258;&#20027;&#33021;&#21147;&#21644;&#33258;&#36866;&#24212;&#33021;&#21147;&#30340;&#20195;&#29702;&#19968;&#30452;&#26159;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#32456;&#26497;&#30446;&#26631;&#12290;&#29983;&#29289;&#20307;&#26159;&#36825;&#26679;&#19968;&#20010;&#20195;&#29702;&#30340;&#26368;&#22909;&#20363;&#35777;&#65292;&#23427;&#20026;&#33258;&#36866;&#24212;&#33258;&#20027;&#24615;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#32463;&#39564;&#25945;&#35757;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20851;&#27880;&#20869;&#24863;&#30693;&#65292;&#36825;&#26159;&#19968;&#20010;&#30417;&#25511;&#33258;&#36523;&#20869;&#37096;&#29615;&#22659;&#26469;&#20445;&#25345;&#22312;&#19968;&#23450;&#33539;&#22260;&#20869;&#30340;&#36807;&#31243;&#65292;&#23427;&#20026;&#29983;&#29289;&#20307;&#30340;&#29983;&#23384;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;&#20026;&#20102;&#24320;&#21457;&#20855;&#26377;&#20869;&#24863;&#30693;&#30340;AI&#65292;&#25105;&#20204;&#38656;&#35201;&#23558;&#34920;&#31034;&#20869;&#37096;&#29615;&#22659;&#30340;&#29366;&#24577;&#21464;&#37327;&#19982;&#22806;&#37096;&#29615;&#22659;&#30456;&#20998;&#31163;&#65292;&#24182;&#37319;&#29992;&#29983;&#21629;&#21551;&#21457;&#30340;&#20869;&#37096;&#29615;&#22659;&#29366;&#24577;&#30340;&#25968;&#23398;&#29305;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#21363;&#36890;&#36807;&#23558;&#25511;&#21046;&#35770;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#31070;&#32463;&#31185;&#23398;&#30340;&#26368;&#26032;&#36827;&#23637;&#19982;&#29983;&#21629;&#29702;&#35770;&#30456;&#32467;&#21512;&#65292;&#20869;&#24863;&#30693;&#22914;&#20309;&#24110;&#21161;&#26500;&#24314;&#33258;&#20027;&#21644;&#36866;&#24212;&#24615;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Life-inspired Interoceptive Artificial Intelligence for Autonomous and Adaptive Agents. (arXiv:2309.05999v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05999
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#21629;&#29702;&#35770;&#21644;&#25511;&#21046;&#35770;&#30340;&#26032;&#35270;&#35282;&#65292;&#36890;&#36807;&#23558;&#25511;&#21046;&#35770;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#31070;&#32463;&#31185;&#23398;&#30340;&#26368;&#26032;&#36827;&#23637;&#19982;&#29983;&#21629;&#29702;&#35770;&#30456;&#32467;&#21512;&#65292;&#23558;&#20869;&#24863;&#30693;&#24212;&#29992;&#20110;&#26500;&#24314;&#20855;&#26377;&#33258;&#20027;&#21644;&#36866;&#24212;&#24615;&#33021;&#21147;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#20855;&#26377;&#33258;&#20027;&#33021;&#21147;&#21644;&#33258;&#36866;&#24212;&#33021;&#21147;&#30340;&#20195;&#29702;&#19968;&#30452;&#26159;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#32456;&#26497;&#30446;&#26631;&#12290;&#29983;&#29289;&#20307;&#26159;&#36825;&#26679;&#19968;&#20010;&#20195;&#29702;&#30340;&#26368;&#22909;&#20363;&#35777;&#65292;&#23427;&#20026;&#33258;&#36866;&#24212;&#33258;&#20027;&#24615;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#32463;&#39564;&#25945;&#35757;&#12290;&#26412;&#25991;&#20851;&#27880;&#20869;&#24863;&#30693;&#65292;&#36825;&#26159;&#19968;&#20010;&#30417;&#25511;&#33258;&#36523;&#20869;&#37096;&#29615;&#22659;&#26469;&#20445;&#25345;&#22312;&#19968;&#23450;&#33539;&#22260;&#20869;&#30340;&#36807;&#31243;&#65292;&#23427;&#20026;&#29983;&#29289;&#20307;&#30340;&#29983;&#23384;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;&#20026;&#20102;&#24320;&#21457;&#20855;&#26377;&#20869;&#24863;&#30693;&#30340;AI&#65292;&#25105;&#20204;&#38656;&#35201;&#23558;&#34920;&#31034;&#20869;&#37096;&#29615;&#22659;&#30340;&#29366;&#24577;&#21464;&#37327;&#19982;&#22806;&#37096;&#29615;&#22659;&#30456;&#20998;&#31163;&#65292;&#24182;&#37319;&#29992;&#29983;&#21629;&#21551;&#21457;&#30340;&#20869;&#37096;&#29615;&#22659;&#29366;&#24577;&#30340;&#25968;&#23398;&#29305;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#21363;&#36890;&#36807;&#23558;&#25511;&#21046;&#35770;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#31070;&#32463;&#31185;&#23398;&#30340;&#26368;&#26032;&#36827;&#23637;&#19982;&#29983;&#21629;&#29702;&#35770;&#30456;&#32467;&#21512;&#65292;&#20869;&#24863;&#30693;&#22914;&#20309;&#24110;&#21161;&#26500;&#24314;&#33258;&#20027;&#21644;&#36866;&#24212;&#24615;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building autonomous --- i.e., choosing goals based on one's needs -- and adaptive -- i.e., surviving in ever-changing environments -- agents has been a holy grail of artificial intelligence (AI). A living organism is a prime example of such an agent, offering important lessons about adaptive autonomy. Here, we focus on interoception, a process of monitoring one's internal environment to keep it within certain bounds, which underwrites the survival of an organism. To develop AI with interoception, we need to factorize the state variables representing internal environments from external environments and adopt life-inspired mathematical properties of internal environment states. This paper offers a new perspective on how interoception can help build autonomous and adaptive agents by integrating the legacy of cybernetics with recent advances in theories of life, reinforcement learning, and neuroscience.
&lt;/p&gt;</description></item></channel></rss>