<rss version="2.0"><channel><title>Chat Arxiv cs.NE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NE</description><item><title>&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;&#20351;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#20013;&#21457;&#29616;&#20102;&#31867;&#20284;&#22823;&#33041;&#22238;&#25918;&#30340;&#29616;&#35937;&#65292;&#24182;&#35777;&#26126;&#20854;&#23545;&#20219;&#21153;&#30340;&#36129;&#29486;&#12290;&#36825;&#19968;&#21457;&#29616;&#25552;&#20379;&#20102;&#29702;&#35299;&#22238;&#25918;&#26426;&#21046;&#30340;&#26032;&#35270;&#35282;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01467</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#20869;&#20986;&#29616;&#31867;&#20284;&#22823;&#33041;&#22238;&#25918;&#30340;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Brain-Like Replay Naturally Emerges in Reinforcement Learning Agents
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;&#20351;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#20013;&#21457;&#29616;&#20102;&#31867;&#20284;&#22823;&#33041;&#22238;&#25918;&#30340;&#29616;&#35937;&#65292;&#24182;&#35777;&#26126;&#20854;&#23545;&#20219;&#21153;&#30340;&#36129;&#29486;&#12290;&#36825;&#19968;&#21457;&#29616;&#25552;&#20379;&#20102;&#29702;&#35299;&#22238;&#25918;&#26426;&#21046;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#21306;&#22495;&#20013;&#26222;&#36941;&#35266;&#23519;&#21040;&#30340;&#22238;&#25918;&#29616;&#35937;&#26159;&#21542;&#33021;&#22815;&#22312;&#20154;&#24037;&#26234;&#33021;&#26234;&#33021;&#20307;&#20013;&#33258;&#28982;&#20135;&#29983;&#65311;&#22914;&#26524;&#26159;&#30340;&#35805;&#65292;&#23427;&#26159;&#21542;&#23545;&#20219;&#21153;&#26377;&#25152;&#36129;&#29486;&#65311;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#20219;&#21153;&#20248;&#21270;&#30340;&#33539;&#24335;&#19979;&#21457;&#29616;&#20102;&#22238;&#25918;&#30340;&#33258;&#28982;&#20986;&#29616;&#65292;&#27169;&#22411;&#27169;&#25311;&#20102;&#28023;&#39532;&#20307;&#21644;&#21069;&#39069;&#21494;&#30382;&#23618;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#27807;&#36890;&#21644;&#24863;&#35273;&#30382;&#23618;&#30340;&#36755;&#20837;&#12290;&#28023;&#39532;&#20307;&#20013;&#30340;&#22238;&#25918;&#26159;&#30001;&#20110;&#24773;&#26223;&#35760;&#24518;&#12289;&#35748;&#30693;&#22320;&#22270;&#20197;&#21450;&#29615;&#22659;&#35266;&#23519;&#32780;&#20135;&#29983;&#30340;&#65292;&#19982;&#21160;&#29289;&#23454;&#39564;&#25968;&#25454;&#30456;&#20284;&#65292;&#24182;&#19988;&#26159;&#39640;&#20219;&#21153;&#24615;&#33021;&#30340;&#26377;&#25928;&#25351;&#26631;&#12290;&#35813;&#27169;&#22411;&#36824;&#25104;&#21151;&#22320;&#37325;&#29616;&#20102;&#23616;&#37096;&#21644;&#38750;&#23616;&#37096;&#30340;&#22238;&#25918;&#65292;&#19982;&#20154;&#31867;&#23454;&#39564;&#25968;&#25454;&#30456;&#31526;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#29702;&#35299;&#22238;&#25918;&#26426;&#21046;&#25552;&#20379;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can replay, as a widely observed neural activity pattern in brain regions, particularly in the hippocampus and neocortex, emerge in an artificial agent? If yes, does it contribute to the tasks? In this work, without heavy dependence on complex assumptions, we discover naturally emergent replay under task-optimized paradigm using a recurrent neural network-based reinforcement learning model, which mimics the hippocampus and prefrontal cortex, as well as their intercommunication and the sensory cortex input. The emergent replay in the hippocampus, which results from the episodic memory and cognitive map as well as environment observations, well resembles animal experimental data and serves as an effective indicator of high task performance. The model also successfully reproduces local and nonlocal replay, which matches the human experimental data. Our work provides a new avenue for understanding the mechanisms behind replay.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#36827;&#21270;&#31639;&#27861;&#29992;&#20110;&#33258;&#21160;&#20844;&#20132;&#32593;&#32476;&#35774;&#35745;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20316;&#20026;&#31574;&#30053;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#36827;&#21270;&#31639;&#27861;&#20013;&#30340;&#21464;&#24322;&#25805;&#20316;&#31526;&#65292;&#22312;&#20844;&#20132;&#32593;&#32476;&#35774;&#35745;&#22522;&#20934;&#38598;&#19978;&#20248;&#20110;&#21333;&#29420;&#23398;&#20064;&#31574;&#30053;&#21644;&#31616;&#21333;&#36827;&#21270;&#31639;&#27861;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.07917</link><description>&lt;p&gt;
&#29992;&#20110;&#33258;&#20027;&#20844;&#20132;&#32593;&#32476;&#35774;&#35745;&#30340;&#31070;&#32463;&#36827;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Neural-Evolutionary Algorithm for Autonomous Transit Network Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07917
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#36827;&#21270;&#31639;&#27861;&#29992;&#20110;&#33258;&#21160;&#20844;&#20132;&#32593;&#32476;&#35774;&#35745;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20316;&#20026;&#31574;&#30053;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#36827;&#21270;&#31639;&#27861;&#20013;&#30340;&#21464;&#24322;&#25805;&#20316;&#31526;&#65292;&#22312;&#20844;&#20132;&#32593;&#32476;&#35774;&#35745;&#22522;&#20934;&#38598;&#19978;&#20248;&#20110;&#21333;&#29420;&#23398;&#20064;&#31574;&#30053;&#21644;&#31616;&#21333;&#36827;&#21270;&#31639;&#27861;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35268;&#21010;&#20844;&#20849;&#20132;&#36890;&#32593;&#32476;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#20294;&#26159;&#20026;&#20102;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#20844;&#20132;&#36710;&#30340;&#22909;&#22788;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35268;&#21010;&#33258;&#21160;&#39550;&#39542;&#20844;&#20132;&#36710;&#30340;&#36335;&#32447;&#32593;&#32476;&#12290;&#25105;&#20204;&#39318;&#20808;&#35757;&#32451;&#19968;&#20010;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20316;&#20026;&#26500;&#24314;&#36335;&#32447;&#32593;&#32476;&#30340;&#31574;&#30053;&#65292;&#28982;&#21518;&#23558;&#35813;&#31574;&#30053;&#29992;&#20316;&#36827;&#21270;&#31639;&#27861;&#20013;&#30340;&#22810;&#20010;&#21464;&#24322;&#25805;&#20316;&#31526;&#20043;&#19968;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#30340;&#20844;&#20132;&#32593;&#32476;&#35774;&#35745;&#22522;&#20934;&#38598;&#19978;&#35780;&#20272;&#36825;&#31181;&#31639;&#27861;&#65292;&#24182;&#21457;&#29616;&#23427;&#22312;&#29616;&#23454;&#22522;&#20934;&#23454;&#20363;&#19978;&#30340;&#34920;&#29616;&#27604;&#21333;&#29420;&#23398;&#20064;&#30340;&#31574;&#30053;&#39640;&#20986;&#39640;&#36798;20\%&#65292;&#27604;&#31616;&#21333;&#30340;&#36827;&#21270;&#31639;&#27861;&#26041;&#27861;&#39640;&#20986;&#39640;&#36798;53%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07917v1 Announce Type: cross  Abstract: Planning a public transit network is a challenging optimization problem, but essential in order to realize the benefits of autonomous buses. We propose a novel algorithm for planning networks of routes for autonomous buses. We first train a graph neural net model as a policy for constructing route networks, and then use the policy as one of several mutation operators in a evolutionary algorithm. We evaluate this algorithm on a standard set of benchmarks for transit network design, and find that it outperforms the learned policy alone by up to 20\% and a plain evolutionary algorithm approach by up to 53\% on realistic benchmark instances.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DHTM&#30340;&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;&#22240;&#23376;&#22270;&#24418;&#24335;&#21644;&#22810;&#32452;&#25104;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#21033;&#29992;&#20998;&#24067;&#24335;&#34920;&#31034;&#12289;&#31232;&#30095;&#36716;&#31227;&#30697;&#38453;&#21644;&#23616;&#37096;Hebbian&#26679;&#23398;&#20064;&#35268;&#21017;&#26469;&#35299;&#20915;&#22312;&#32447;&#38544;&#34255;&#34920;&#31034;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DHTM&#22312;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#27604;&#32463;&#20856;&#30340;LSTM&#25928;&#26524;&#26356;&#22909;&#65292;&#24182;&#19982;&#26356;&#20808;&#36827;&#30340;&#31867;&#20284;RNN&#30340;&#31639;&#27861;&#24615;&#33021;&#30456;&#24403;&#65292;&#21487;&#20197;&#21152;&#36895;&#32487;&#20219;&#32773;&#34920;&#31034;&#30340;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.13391</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#24067;&#24335;Hebbian Temporal Memory&#23398;&#20064;&#32487;&#20219;&#32773;&#34920;&#31034;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning Successor Representations with Distributed Hebbian Temporal Memory. (arXiv:2310.13391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DHTM&#30340;&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;&#22240;&#23376;&#22270;&#24418;&#24335;&#21644;&#22810;&#32452;&#25104;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#21033;&#29992;&#20998;&#24067;&#24335;&#34920;&#31034;&#12289;&#31232;&#30095;&#36716;&#31227;&#30697;&#38453;&#21644;&#23616;&#37096;Hebbian&#26679;&#23398;&#20064;&#35268;&#21017;&#26469;&#35299;&#20915;&#22312;&#32447;&#38544;&#34255;&#34920;&#31034;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DHTM&#22312;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#27604;&#32463;&#20856;&#30340;LSTM&#25928;&#26524;&#26356;&#22909;&#65292;&#24182;&#19982;&#26356;&#20808;&#36827;&#30340;&#31867;&#20284;RNN&#30340;&#31639;&#27861;&#24615;&#33021;&#30456;&#24403;&#65292;&#21487;&#20197;&#21152;&#36895;&#32487;&#20219;&#32773;&#34920;&#31034;&#30340;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;&#32447;&#38544;&#34255;&#34920;&#31034;&#23398;&#20064;&#30340;&#25361;&#25112;&#65292;&#35813;&#26041;&#27861;&#29992;&#20110;&#22312;&#19981;&#31283;&#23450;&#30340;&#12289;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#20915;&#31574;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#65292;&#20998;&#24067;&#24335;Hebbian Temporal Memory (DHTM)&#65292;&#22522;&#20110;&#22240;&#23376;&#22270;&#24418;&#24335;&#21644;&#22810;&#32452;&#25104;&#31070;&#32463;&#20803;&#27169;&#22411;&#12290;DHTM&#26088;&#22312;&#25429;&#25417;&#39034;&#24207;&#25968;&#25454;&#20851;&#31995;&#24182;&#23545;&#26410;&#26469;&#35266;&#23519;&#20316;&#20986;&#32047;&#31215;&#39044;&#27979;&#65292;&#24418;&#25104;&#32487;&#20219;&#32773;&#34920;&#31034;&#12290;&#21463;&#26032;&#30382;&#23618;&#30340;&#31070;&#32463;&#29983;&#29702;&#23398;&#27169;&#22411;&#21551;&#21457;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20998;&#24067;&#24335;&#34920;&#31034;&#12289;&#31232;&#30095;&#36716;&#31227;&#30697;&#38453;&#21644;&#23616;&#37096;Hebbian&#26679;&#23398;&#20064;&#35268;&#21017;&#20811;&#26381;&#20102;&#20256;&#32479;&#26102;&#38388;&#35760;&#24518;&#31639;&#27861;&#65288;&#22914;RNN&#21644;HMM&#65289;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#24930;&#36895;&#23398;&#20064;&#36807;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DHTM&#20248;&#20110;&#32463;&#20856;&#30340;LSTM&#65292;&#24182;&#19982;&#26356;&#20808;&#36827;&#30340;&#31867;&#20284;RNN&#30340;&#31639;&#27861;&#24615;&#33021;&#30456;&#24403;&#65292;&#22312;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#21152;&#36895;&#20102;&#32487;&#20219;&#32773;&#34920;&#31034;&#30340;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel approach to address the challenge of online hidden representation learning for decision-making under uncertainty in non-stationary, partially observable environments. The proposed algorithm, Distributed Hebbian Temporal Memory (DHTM), is based on factor graph formalism and a multicomponent neuron model. DHTM aims to capture sequential data relationships and make cumulative predictions about future observations, forming Successor Representation (SR). Inspired by neurophysiological models of the neocortex, the algorithm utilizes distributed representations, sparse transition matrices, and local Hebbian-like learning rules to overcome the instability and slow learning process of traditional temporal memory algorithms like RNN and HMM. Experimental results demonstrate that DHTM outperforms classical LSTM and performs comparably to more advanced RNN-like algorithms, speeding up Temporal Difference learning for SR in changing environments. Additionally, we compare
&lt;/p&gt;</description></item></channel></rss>