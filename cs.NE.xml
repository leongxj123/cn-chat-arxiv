<rss version="2.0"><channel><title>Chat Arxiv cs.NE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NE</description><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22522;&#20110;&#22823;&#35268;&#27169;Transformer&#26550;&#26500;&#30340;&#29983;&#25104;&#24615;AI&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#21629;&#21517;&#29289;&#20307;&#25968;&#37327;&#25110;&#29983;&#25104;&#21253;&#21547;&#30446;&#26631;&#25968;&#37327;&#29289;&#21697;&#30340;&#22270;&#20687;&#65292;&#32467;&#26524;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#37117;&#27809;&#26377;&#20197;&#31867;&#20284;&#20154;&#31867;&#30340;&#26041;&#24335;&#34920;&#29616;&#65292;&#24182;&#19988;&#21363;&#20351;&#23545;&#20110;&#23567;&#25968;&#37327;&#30340;&#29289;&#20307;&#20063;&#20250;&#20986;&#29616;&#26174;&#33879;&#30340;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2402.03328</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#29983;&#25104;AI&#27169;&#22411;&#32570;&#20047;&#35270;&#35273;&#25968;&#23383;&#24863;&#30693;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Large-scale Generative AI Models Lack Visual Number Sense
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22522;&#20110;&#22823;&#35268;&#27169;Transformer&#26550;&#26500;&#30340;&#29983;&#25104;&#24615;AI&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#21629;&#21517;&#29289;&#20307;&#25968;&#37327;&#25110;&#29983;&#25104;&#21253;&#21547;&#30446;&#26631;&#25968;&#37327;&#29289;&#21697;&#30340;&#22270;&#20687;&#65292;&#32467;&#26524;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#37117;&#27809;&#26377;&#20197;&#31867;&#20284;&#20154;&#31867;&#30340;&#26041;&#24335;&#34920;&#29616;&#65292;&#24182;&#19988;&#21363;&#20351;&#23545;&#20110;&#23567;&#25968;&#37327;&#30340;&#29289;&#20307;&#20063;&#20250;&#20986;&#29616;&#26174;&#33879;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#33021;&#22815;&#22312;&#35270;&#35273;&#22330;&#26223;&#20013;&#36731;&#26494;&#21028;&#26029;&#29289;&#20307;&#30340;&#25968;&#37327;&#65292;&#21363;&#20351;&#19981;&#36827;&#34892;&#35745;&#25968;&#65292;&#32780;&#19988;&#36825;&#31181;&#25216;&#33021;&#22312;&#21508;&#31181;&#21160;&#29289;&#29289;&#31181;&#21644;&#35821;&#35328;&#21457;&#23637;&#21644;&#27491;&#24335;&#23398;&#26657;&#25945;&#32946;&#20043;&#21069;&#30340;&#23156;&#20799;&#20013;&#37117;&#26377;&#35760;&#24405;&#12290;&#23545;&#20110;&#23567;&#30340;&#29289;&#20307;&#38598;&#65292;&#25968;&#23383;&#21028;&#26029;&#26159;&#26080;&#35823;&#30340;&#65292;&#32780;&#23545;&#20110;&#26356;&#22823;&#30340;&#38598;&#21512;&#65292;&#22238;&#24212;&#21464;&#24471;&#36817;&#20284;&#65292;&#24182;&#19988;&#21464;&#24322;&#24615;&#19982;&#30446;&#26631;&#25968;&#23383;&#25104;&#27604;&#20363;&#22686;&#21152;&#12290;&#23613;&#31649;&#29289;&#20307;&#29305;&#24449;&#65288;&#22914;&#39068;&#33394;&#25110;&#24418;&#29366;&#65289;&#23384;&#22312;&#24046;&#24322;&#65292;&#20294;&#36825;&#31181;&#22238;&#24212;&#27169;&#24335;&#22312;&#25152;&#26377;&#31867;&#22411;&#30340;&#29289;&#20307;&#19978;&#35266;&#23519;&#21040;&#65292;&#36825;&#34920;&#26126;&#25105;&#20204;&#30340;&#35270;&#35273;&#25968;&#23383;&#24863;&#30693;&#20381;&#36182;&#20110;&#25968;&#23383;&#25968;&#37327;&#30340;&#25277;&#35937;&#34920;&#31034;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22522;&#20110;&#22823;&#35268;&#27169;Transformer&#26550;&#26500;&#30340;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#21487;&#38752;&#22320;&#21629;&#21517;&#31616;&#21333;&#35270;&#35273;&#21050;&#28608;&#20013;&#30340;&#29289;&#20307;&#25968;&#37327;&#25110;&#29983;&#25104;&#21253;&#21547;&#30446;&#26631;&#29289;&#21697;&#25968;&#37327;&#30340;&#22270;&#20687;&#65288;1-10&#33539;&#22260;&#20869;&#65289;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25152;&#32771;&#34385;&#30340;&#25152;&#26377;&#22522;&#30784;&#27169;&#22411;&#37117;&#27809;&#26377;&#20197;&#31867;&#20284;&#20154;&#31867;&#19968;&#26679;&#30340;&#26041;&#24335;&#34920;&#29616;&#20986;&#26469;&#65306;&#21363;&#20351;&#26159;&#20855;&#26377;&#36739;&#23567;&#25968;&#37327;&#30340;&#29289;&#20307;&#20063;&#20250;&#29359;&#19979;&#26174;&#33879;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans can readily judge the number of objects in a visual scene, even without counting, and such a skill has been documented in a variety of animal species and in babies prior to language development and formal schooling. Numerical judgments are error-free for small sets, while for larger collections responses become approximate, with variability increasing proportionally to the target number. This response pattern is observed for items of all kinds, despite variation in object features (such as color or shape), suggesting that our visual number sense relies on abstract representations of numerosity. Here, we investigated whether generative Artificial Intelligence (AI) models based on large-scale transformer architectures can reliably name the number of objects in simple visual stimuli or generate images containing a target number of items in the 1-10 range. Surprisingly, none of the foundation models considered performed in a human-like way: They all made striking errors even with sm
&lt;/p&gt;</description></item><item><title>NACHOS &#26159;&#19968;&#31181;&#38754;&#21521;&#30828;&#20214;&#21463;&#38480;&#30340;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#21270;&#35774;&#35745;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#24182;&#32771;&#34385;&#39592;&#24178;&#21644;&#26089;&#26399;&#36864;&#20986;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2401.13330</link><description>&lt;p&gt;
NACHOS: &#30828;&#20214;&#21463;&#38480;&#30340;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
NACHOS: Neural Architecture Search for Hardware Constrained Early Exit Neural Networks. (arXiv:2401.13330v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13330
&lt;/p&gt;
&lt;p&gt;
NACHOS &#26159;&#19968;&#31181;&#38754;&#21521;&#30828;&#20214;&#21463;&#38480;&#30340;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#21270;&#35774;&#35745;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#24182;&#32771;&#34385;&#39592;&#24178;&#21644;&#26089;&#26399;&#36864;&#20986;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#65288;EENNs&#65289;&#20026;&#26631;&#20934;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#37197;&#22791;&#26089;&#26399;&#36864;&#20986;&#20998;&#31867;&#22120;&#65288;EECs&#65289;&#65292;&#22312;&#22788;&#29702;&#30340;&#20013;&#38388;&#28857;&#19978;&#25552;&#20379;&#36275;&#22815;&#30340;&#20998;&#31867;&#32622;&#20449;&#24230;&#26102;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#22312;&#25928;&#26524;&#21644;&#25928;&#29575;&#26041;&#38754;&#24102;&#26469;&#20102;&#35768;&#22810;&#22909;&#22788;&#12290;&#30446;&#21069;&#65292;EENNs&#30340;&#35774;&#35745;&#26159;&#30001;&#19987;&#23478;&#25163;&#21160;&#23436;&#25104;&#30340;&#65292;&#36825;&#26159;&#19968;&#39033;&#22797;&#26434;&#21644;&#32791;&#26102;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#32771;&#34385;&#35768;&#22810;&#26041;&#38754;&#65292;&#21253;&#25324;&#27491;&#30830;&#30340;&#25918;&#32622;&#12289;&#38408;&#20540;&#35774;&#32622;&#21644;EECs&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#27491;&#22312;&#25506;&#32034;&#20351;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#33258;&#21160;&#21270;&#35774;&#35745;EENNs&#12290;&#30446;&#21069;&#65292;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#20960;&#20010;&#23436;&#25972;&#30340;NAS&#35299;&#20915;&#26041;&#26696;&#29992;&#20110;EENNs&#65292;&#24182;&#19988;&#19968;&#20010;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#32508;&#21512;&#35774;&#35745;&#31574;&#30053;&#65292;&#21516;&#26102;&#32771;&#34385;&#39592;&#24178;&#21644;EECs&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#26412;&#30740;&#31350;&#21576;&#29616;&#20102;&#38754;&#21521;&#30828;&#20214;&#21463;&#38480;&#30340;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NACHOS&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early Exit Neural Networks (EENNs) endow astandard Deep Neural Network (DNN) with Early Exit Classifiers (EECs), to provide predictions at intermediate points of the processing when enough confidence in classification is achieved. This leads to many benefits in terms of effectiveness and efficiency. Currently, the design of EENNs is carried out manually by experts, a complex and time-consuming task that requires accounting for many aspects, including the correct placement, the thresholding, and the computational overhead of the EECs. For this reason, the research is exploring the use of Neural Architecture Search (NAS) to automatize the design of EENNs. Currently, few comprehensive NAS solutions for EENNs have been proposed in the literature, and a fully automated, joint design strategy taking into consideration both the backbone and the EECs remains an open problem. To this end, this work presents Neural Architecture Search for Hardware Constrained Early Exit Neural Networks (NACHOS),
&lt;/p&gt;</description></item></channel></rss>