<rss version="2.0"><channel><title>Chat Arxiv cs.NE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NE</description><item><title>&#20154;&#31867;&#23398;&#20064;&#23545;&#31034;&#20363;&#35838;&#31243;&#21644;&#20219;&#21153;&#32467;&#26500;&#26377;&#25935;&#24863;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#21516;&#26102;&#33719;&#24471;&#20998;&#32452;&#21644;&#20132;&#38169;&#35757;&#32451;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.08674</link><description>&lt;p&gt;
&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#20986;&#29616;&#20154;&#31867;&#35838;&#31243;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Human Curriculum Effects Emerge with In-Context Learning in Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08674
&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#23398;&#20064;&#23545;&#31034;&#20363;&#35838;&#31243;&#21644;&#20219;&#21153;&#32467;&#26500;&#26377;&#25935;&#24863;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#21516;&#26102;&#33719;&#24471;&#20998;&#32452;&#21644;&#20132;&#38169;&#35757;&#32451;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#23398;&#20064;&#23545;&#35268;&#21017;&#32467;&#26500;&#21644;&#35757;&#32451;&#20013;&#25152;&#20351;&#29992;&#30340;&#31034;&#20363;&#35838;&#31243;&#38750;&#24120;&#25935;&#24863;&#12290;&#22312;&#30001;&#31616;&#27905;&#35268;&#21017;&#25511;&#21046;&#30340;&#20219;&#21153;&#20013;&#65292;&#24403;&#30456;&#20851;&#31034;&#20363;&#22312;&#22810;&#27425;&#35797;&#39564;&#20013;&#34987;&#20998;&#32452;&#26102;&#65292;&#23398;&#20064;&#26356;&#21152;&#31283;&#20581;&#65307;&#20294;&#22312;&#32570;&#20047;&#36825;&#26679;&#30340;&#35268;&#21017;&#30340;&#24773;&#20917;&#19979;&#65292;&#20132;&#38169;&#35757;&#32451;&#26356;&#21152;&#26377;&#25928;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#27809;&#26377;&#31070;&#32463;&#27169;&#22411;&#33021;&#22815;&#21516;&#26102;&#25429;&#25417;&#21040;&#36825;&#20123;&#30475;&#20284;&#30683;&#30462;&#30340;&#25928;&#24212;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#8220;&#19978;&#19979;&#25991;&#23398;&#20064;&#8221;&#65288;ICL&#65289;&#22312;&#20351;&#29992;&#20803;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#33258;&#21457;&#20135;&#29983;&#20102;&#21516;&#26679;&#30340;&#26435;&#34913;&#12290;ICL&#26159;&#36890;&#36807;&#20869;&#23618;&#24490;&#29615;&#31639;&#27861;&#22312;&#28608;&#27963;&#21160;&#21147;&#23398;&#20013;&#23454;&#29616;&#30340;&#19968;&#31181;&#8220;&#19978;&#19979;&#25991;&#20869;&#23398;&#20064;&#8221;&#65288;in-context learning&#65289;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#26435;&#37325;&#26356;&#25913;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#23545;&#39044;&#35757;&#32451;&#30340;LLMs&#21644;&#20803;&#23398;&#20064;&#21464;&#21387;&#22120;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ICL&#22312;&#28041;&#21450;&#35268;&#21017;&#32467;&#26500;&#30340;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#20154;&#31867;&#25152;&#31034;&#30340;&#20998;&#32452;&#20248;&#21183;&#65292;&#32780;&#21516;&#26102;&#36827;&#34892;&#26435;&#37325;&#23398;&#20064;&#21017;&#22797;&#21046;&#20102;&#20154;&#31867;&#22312;&#32570;&#23569;&#36825;&#26679;&#32467;&#26500;&#30340;&#20219;&#21153;&#19978;&#25152;&#35266;&#23519;&#21040;&#30340;&#20132;&#38169;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human learning is sensitive to rule-like structure and the curriculum of examples used for training. In tasks governed by succinct rules, learning is more robust when related examples are blocked across trials, but in the absence of such rules, interleaving is more effective. To date, no neural model has simultaneously captured these seemingly contradictory effects. Here we show that this same tradeoff spontaneously emerges with "in-context learning" (ICL) both in neural networks trained with metalearning and in large language models (LLMs). ICL is the ability to learn new tasks "in context" - without weight changes - via an inner-loop algorithm implemented in activation dynamics. Experiments with pretrained LLMs and metalearning transformers show that ICL exhibits the blocking advantage demonstrated in humans on a task involving rule-like structure, and conversely, that concurrent in-weight learning reproduces the interleaving advantage observed in humans on tasks lacking such structu
&lt;/p&gt;</description></item><item><title>FEDORA&#26159;&#19968;&#20010;&#39134;&#34892;&#20107;&#20214;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#32570;&#23569;&#23436;&#25972;&#25968;&#25454;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#24110;&#21161;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#19979;&#23454;&#29616;&#22522;&#20110;&#35270;&#35273;&#30340;&#33258;&#20027;&#23548;&#33322;&#21644;&#36991;&#38556;&#12290;</title><link>http://arxiv.org/abs/2305.14392</link><description>&lt;p&gt;
FEDORA&#65306;&#29992;&#20110;&#21453;&#24212;&#34892;&#20026;&#30340;&#39134;&#34892;&#20107;&#20214;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
FEDORA: Flying Event Dataset fOr Reactive behAvior. (arXiv:2305.14392v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14392
&lt;/p&gt;
&lt;p&gt;
FEDORA&#26159;&#19968;&#20010;&#39134;&#34892;&#20107;&#20214;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#32570;&#23569;&#23436;&#25972;&#25968;&#25454;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#24110;&#21161;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#19979;&#23454;&#29616;&#22522;&#20110;&#35270;&#35273;&#30340;&#33258;&#20027;&#23548;&#33322;&#21644;&#36991;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#20307;&#22312;&#39134;&#34892;&#20013;&#20351;&#29992;&#26497;&#23569;&#25968;&#30340;&#31070;&#32463;&#20803;&#21644;&#26497;&#20302;&#30340;&#22833;&#35823;&#29575;&#25191;&#34892;&#22797;&#26434;&#30340;&#39640;&#36895;&#26426;&#21160;&#65292;&#31361;&#26174;&#20102;&#36825;&#20123;&#36164;&#28304;&#21463;&#38480;&#21046;&#30340;&#29983;&#29289;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;&#36817;&#24180;&#26469;&#65292;&#20107;&#20214;&#39537;&#21160;&#30828;&#20214;&#36880;&#28176;&#25104;&#20026;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#23454;&#29616;&#22797;&#26434;&#35270;&#35273;&#20219;&#21153;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#22522;&#20110;&#35270;&#35273;&#30340;&#33258;&#20027;&#23548;&#33322;&#21644;&#36991;&#38556;&#21253;&#25324;&#20960;&#20010;&#29420;&#31435;&#20294;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#22914;&#20809;&#27969;&#20272;&#35745;&#12289;&#28145;&#24230;&#20272;&#35745;&#12289;&#21516;&#26102;&#23450;&#20301;&#19982;&#24314;&#22270;&#65288;SLAM&#65289;&#12289;&#29289;&#20307;&#26816;&#27979;&#21644;&#35782;&#21035;&#12290;&#20026;&#20102;&#30830;&#20445;&#36825;&#20123;&#20219;&#21153;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#20182;&#20204;&#24517;&#39035;&#22312;&#21333;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#25968;&#25454;&#38598;&#21482;&#25552;&#20379;&#25152;&#38656;&#25968;&#25454;&#30340;&#36873;&#23450;&#23376;&#38598;&#65292;&#36825;&#20351;&#24471;&#32593;&#32476;&#38388;&#30340;&#19968;&#33268;&#24615;&#38590;&#20197;&#23454;&#29616;&#12290;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#21478;&#19968;&#20010;&#38480;&#21046;&#26159;&#25552;&#20379;&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#36776;&#29575;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FEDORA&#65292;
&lt;/p&gt;
&lt;p&gt;
The ability of living organisms to perform complex high speed manoeuvers in flight with a very small number of neurons and an incredibly low failure rate highlights the efficacy of these resource-constrained biological systems. Event-driven hardware has emerged, in recent years, as a promising avenue for implementing complex vision tasks in resource-constrained environments. Vision-based autonomous navigation and obstacle avoidance consists of several independent but related tasks such as optical flow estimation, depth estimation, Simultaneous Localization and Mapping (SLAM), object detection, and recognition. To ensure coherence between these tasks, it is imperative that they be trained on a single dataset. However, most existing datasets provide only a selected subset of the required data. This makes inter-network coherence difficult to achieve. Another limitation of existing datasets is the limited temporal resolution they provide. To address these limitations, we present FEDORA, a 
&lt;/p&gt;</description></item></channel></rss>