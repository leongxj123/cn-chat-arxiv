<rss version="2.0"><channel><title>Chat Arxiv cs.NE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NE</description><item><title>QKFormer&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#33033;&#20914;&#24418;&#24335;Q-K&#27880;&#24847;&#21147;&#26426;&#21046;&#12289;&#20998;&#23618;&#32467;&#26500;&#21644;&#34917;&#19969;&#23884;&#20837;&#27169;&#22359;&#65292;&#20197;&#25552;&#39640;&#33033;&#20914;&#21464;&#21387;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16552</link><description>&lt;p&gt;
QKFormer: &#20351;&#29992;Q-K&#27880;&#24847;&#21147;&#30340;&#20998;&#23618;&#33033;&#20914;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
QKFormer: Hierarchical Spiking Transformer using Q-K Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16552
&lt;/p&gt;
&lt;p&gt;
QKFormer&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#33033;&#20914;&#24418;&#24335;Q-K&#27880;&#24847;&#21147;&#26426;&#21046;&#12289;&#20998;&#23618;&#32467;&#26500;&#21644;&#34917;&#19969;&#23884;&#20837;&#27169;&#22359;&#65292;&#20197;&#25552;&#39640;&#33033;&#20914;&#21464;&#21387;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#21464;&#21387;&#22120;&#23558;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#19982;&#21464;&#21387;&#22120;&#26550;&#26500;&#30456;&#32467;&#21512;&#65292;&#30001;&#20110;&#20854;&#33410;&#33021;&#39640;&#24615;&#33021;&#30340;&#28508;&#21147;&#65292;&#21560;&#24341;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35813;&#39046;&#22495;&#29616;&#26377;&#27169;&#22411;&#20173;&#28982;&#23384;&#22312;&#24615;&#33021;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20960;&#39033;&#21019;&#26032;&#65306;i&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;SNNs&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#33033;&#20914;&#24418;&#24335;Q-K&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24615;&#30340;&#20108;&#36827;&#21046;&#21521;&#37327;&#26377;&#25928;&#22320;&#24314;&#27169;&#20196;&#29260;&#25110;&#36890;&#36947;&#32500;&#24230;&#30340;&#37325;&#35201;&#24615;&#12290;ii&#65289;&#25105;&#20204;&#23558;&#20855;&#26377;&#26174;&#33879;&#24615;&#33021;&#20248;&#21183;&#30340;&#20998;&#23618;&#32467;&#26500;&#24341;&#20837;&#33033;&#20914;&#21464;&#21387;&#22120;&#65292;&#20174;&#32780;&#33719;&#24471;&#22810;&#23610;&#24230;&#33033;&#20914;&#34920;&#31034;&#65292;&#36825;&#23545;&#22823;&#33041;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#37117;&#26377;&#26174;&#30528;&#22909;&#22788;&#12290;iii&#65289;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36890;&#29992;&#19988;&#24378;&#22823;&#30340;&#34917;&#19969;&#23884;&#20837;&#27169;&#22359;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#19968;&#20010;&#19987;&#38376;&#20026;&#33033;&#20914;&#21464;&#21387;&#22120;&#35774;&#35745;&#30340;&#21464;&#24418;&#24555;&#25463;&#26041;&#24335;&#12290;&#24635;&#20043;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;QKFormer&#65292;&#19968;&#31181;&#20998;&#23618;&#33033;&#20914;&#21464;&#21387;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16552v1 Announce Type: cross  Abstract: Spiking Transformers, which integrate Spiking Neural Networks (SNNs) with Transformer architectures, have attracted significant attention due to their potential for energy efficiency and high performance. However, existing models in this domain still suffer from suboptimal performance. We introduce several innovations to improve the performance: i) We propose a novel spike-form Q-K attention mechanism, tailored for SNNs, which efficiently models the importance of token or channel dimensions through binary vectors with linear complexity. ii) We incorporate the hierarchical structure, which significantly benefits the performance of both the brain and artificial neural networks, into spiking transformers to obtain multi-scale spiking representation. iii) We design a versatile and powerful patch embedding module with a deformed shortcut specifically for spiking transformers. Together, we develop QKFormer, a hierarchical spiking transformer
&lt;/p&gt;</description></item><item><title>&#24179;&#34913;&#30340;&#35856;&#25391;-&#25918;&#30005;&#65288;BRF&#65289;&#31070;&#32463;&#20803;&#30340;&#24341;&#20837;&#32531;&#35299;&#20102;RF&#31070;&#32463;&#20803;&#30340;&#22266;&#26377;&#38480;&#21046;&#65292;&#22312;&#24490;&#29615;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;RSNNs&#65289;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20219;&#21153;&#24615;&#33021;&#65292;&#20135;&#29983;&#26356;&#23569;&#30340;&#33033;&#20914;&#65292;&#24182;&#38656;&#35201;&#26356;&#23569;&#30340;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.14603</link><description>&lt;p&gt;
&#24179;&#34913;&#30340;&#35856;&#25391;-&#25918;&#30005;&#31070;&#32463;&#20803;
&lt;/p&gt;
&lt;p&gt;
Balanced Resonate-and-Fire Neurons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14603
&lt;/p&gt;
&lt;p&gt;
&#24179;&#34913;&#30340;&#35856;&#25391;-&#25918;&#30005;&#65288;BRF&#65289;&#31070;&#32463;&#20803;&#30340;&#24341;&#20837;&#32531;&#35299;&#20102;RF&#31070;&#32463;&#20803;&#30340;&#22266;&#26377;&#38480;&#21046;&#65292;&#22312;&#24490;&#29615;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;RSNNs&#65289;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20219;&#21153;&#24615;&#33021;&#65292;&#20135;&#29983;&#26356;&#23569;&#30340;&#33033;&#20914;&#65292;&#24182;&#38656;&#35201;&#26356;&#23569;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;20&#24180;&#37324;&#24341;&#20837;&#30340;&#35856;&#25391;-&#25918;&#30005;&#65288;RF&#65289;&#31070;&#32463;&#20803;&#26159;&#19968;&#20010;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#19988;&#31526;&#21512;&#29983;&#29289;&#23398;&#21487;&#34892;&#24615;&#30340;&#23574;&#23792;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#30001;&#20110;&#20854;&#20849;&#25391;&#33180;&#21160;&#21147;&#23398;&#65292;&#23427;&#21487;&#20197;&#22312;&#26102;&#38388;&#22495;&#20869;&#25552;&#21462;&#39057;&#29575;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;RF&#20844;&#24335;&#23384;&#22312;&#22266;&#26377;&#32570;&#38519;&#65292;&#38480;&#21046;&#20102;&#26377;&#25928;&#23398;&#20064;&#24182;&#38459;&#30861;&#20102;RF&#31070;&#32463;&#20803;&#30340;&#21407;&#21017;&#20248;&#21183;&#30340;&#21033;&#29992;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24179;&#34913;&#30340;RF&#65288;BRF&#65289;&#31070;&#32463;&#20803;&#65292;&#23427;&#32531;&#35299;&#20102;&#26222;&#36890;RF&#31070;&#32463;&#20803;&#30340;&#19968;&#20123;&#22266;&#26377;&#38480;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#21508;&#31181;&#24207;&#21015;&#23398;&#20064;&#20219;&#21153;&#20013;&#22312;&#24490;&#29615;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;RSNNs&#65289;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;BRF&#31070;&#32463;&#20803;&#30340;&#32593;&#32476;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20219;&#21153;&#24615;&#33021;&#65292;&#20135;&#29983;&#30340;&#33033;&#20914;&#20165;&#20026;&#29616;&#20195;RSNNs&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#29616;&#20195;RSNNs&#65292;&#38656;&#35201;&#30340;&#21442;&#25968;&#26126;&#26174;&#26356;&#23569;&#12290;&#27492;&#22806;&#65292;BRF-RSNN&#22987;&#32456;&#25552;&#20379;&#26356;&#24555;&#36895;&#21644;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#25910;&#25947;&#65292;&#21363;&#20351;&#22312;&#36830;&#25509;&#22810;&#20010;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14603v1 Announce Type: cross  Abstract: The resonate-and-fire (RF) neuron, introduced over two decades ago, is a simple, efficient, yet biologically plausible spiking neuron model, which can extract frequency patterns within the time domain due to its resonating membrane dynamics. However, previous RF formulations suffer from intrinsic shortcomings that limit effective learning and prevent exploiting the principled advantage of RF neurons. Here, we introduce the balanced RF (BRF) neuron, which alleviates some of the intrinsic limitations of vanilla RF neurons and demonstrates its effectiveness within recurrent spiking neural networks (RSNNs) on various sequence learning tasks. We show that networks of BRF neurons achieve overall higher task performance, produce only a fraction of the spikes, and require significantly fewer parameters as compared to modern RSNNs. Moreover, BRF-RSNN consistently provide much faster and more stable training convergence, even when bridging many 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;LM-HT&#27169;&#22411;&#65292;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#22810;&#23618;&#27425;&#38408;&#20540;&#27169;&#22411;&#65292;&#22686;&#24378;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#19982;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30340;&#24615;&#33021;&#23545;&#24212;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.00411</link><description>&lt;p&gt;
LM-HT SNN: &#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#22810;&#23618;&#27425;&#38408;&#20540;&#27169;&#22411;&#22686;&#24378;SNN&#19982;ANN&#30340;&#24615;&#33021;&#23545;&#24212;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
LM-HT SNN: Enhancing the Performance of SNN to ANN Counterpart through Learnable Multi-hierarchical Threshold Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;LM-HT&#27169;&#22411;&#65292;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#22810;&#23618;&#27425;&#38408;&#20540;&#27169;&#22411;&#65292;&#22686;&#24378;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#19982;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30340;&#24615;&#33021;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20256;&#32479;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30456;&#27604;&#65292;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#22240;&#20854;&#26356;&#20855;&#29983;&#29289;&#21551;&#21457;&#21644;&#33021;&#37327;&#25928;&#29575;&#30340;&#20449;&#24687;&#20256;&#36882;&#33021;&#21147;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#23398;&#26415;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20043;&#21069;&#36890;&#36807;&#21508;&#31181;&#26041;&#27861;&#23545;SNN&#30340;&#23398;&#20064;&#26799;&#24230;&#21644;&#27169;&#22411;&#32467;&#26500;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#20294;&#22312;&#24615;&#33021;&#26041;&#38754;SNN&#20173;&#28982;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#33853;&#21518;&#20110;ANN&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#22810;&#38408;&#20540;&#27169;&#22411;&#20026;&#36827;&#19968;&#27493;&#22686;&#24378;SNN&#30340;&#23398;&#20064;&#33021;&#21147;&#25552;&#20379;&#20102;&#26356;&#22810;&#21487;&#33021;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#25968;&#23398;&#30340;&#35282;&#24230;&#20005;&#26684;&#20998;&#26512;&#20102;&#22810;&#38408;&#20540;&#27169;&#22411;&#12289;&#21407;&#22987;&#33033;&#20914;&#27169;&#22411;&#21644;&#37327;&#21270;ANN&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LM-HT&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#31561;&#36317;&#22810;&#23618;&#27425;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#26102;&#38388;&#32500;&#24230;&#19978;&#21160;&#24577;&#35843;&#33410;&#20840;&#23616;&#36755;&#20837;&#30005;&#27969;&#21644;&#33180;&#30005;&#20301;&#27844;&#28431;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25351;&#20986;&#22522;&#20110;LM-HT&#27169;&#22411;&#30340;&#30452;&#25509;&#35757;&#32451;&#31639;&#27861;&#21487;&#20197;&#26080;&#32541;&#22320;&#36830;&#25509;&#20004;&#20010;&#38454;&#27573;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compared to traditional Artificial Neural Network (ANN), Spiking Neural Network (SNN) has garnered widespread academic interest for its intrinsic ability to transmit information in a more biological-inspired and energy-efficient manner. However, despite previous efforts to optimize the learning gradients and model structure of SNNs through various methods, SNNs still lag behind ANNs in terms of performance to some extent. The recently proposed multi-threshold model provides more possibilities for further enhancing the learning capability of SNNs. In this paper, we rigorously analyze the relationship among the multi-threshold model, vanilla spiking model and quantized ANNs from a mathematical perspective, then propose a novel LM-HT model, which is an equidistant multi-hierarchical model that can dynamically regulate the global input current and membrane potential leakage on the time dimension. In addition, we note that the direct training algorithm based on the LM-HT model can seamlessl
&lt;/p&gt;</description></item><item><title>&#28608;&#27963;&#20989;&#25968;&#30340;&#32479;&#19968;&#21270;&#34920;&#31034;&#37319;&#29992;&#20102;Mittag-Leffler&#20989;&#25968;&#65292;&#21487;&#20197;&#25554;&#20540;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#12289;&#20943;&#36731;&#26799;&#24230;&#38382;&#39064;&#65292;&#24182;&#36866;&#29992;&#20110;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2302.11007</link><description>&lt;p&gt;
&#27969;&#34892;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#28608;&#27963;&#20989;&#25968;&#30340;&#32479;&#19968;&#21270;
&lt;/p&gt;
&lt;p&gt;
Unification of popular artificial neural network activation functions. (arXiv:2302.11007v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11007
&lt;/p&gt;
&lt;p&gt;
&#28608;&#27963;&#20989;&#25968;&#30340;&#32479;&#19968;&#21270;&#34920;&#31034;&#37319;&#29992;&#20102;Mittag-Leffler&#20989;&#25968;&#65292;&#21487;&#20197;&#25554;&#20540;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#12289;&#20943;&#36731;&#26799;&#24230;&#38382;&#39064;&#65292;&#24182;&#36866;&#29992;&#20110;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#27969;&#34892;&#30340;&#31070;&#32463;&#32593;&#32476;&#28608;&#27963;&#20989;&#25968;&#30340;&#32479;&#19968;&#34920;&#31034;&#12290;&#37319;&#29992;&#20102;&#20998;&#25968;&#24494;&#31215;&#20998;&#30340;Mittag-Leffler&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#19988;&#32039;&#20945;&#30340;&#21151;&#33021;&#24418;&#24335;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#28608;&#27963;&#20989;&#25968;&#20043;&#38388;&#36827;&#34892;&#25554;&#20540;&#65292;&#24182;&#20943;&#36731;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20013;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#22914;&#26799;&#24230;&#28040;&#22833;&#21644;&#26799;&#24230;&#29190;&#28856;&#12290;&#25152;&#25552;&#20986;&#30340;&#38376;&#25511;&#34920;&#31034;&#25193;&#23637;&#20102;&#22266;&#23450;&#24418;&#29366;&#28608;&#27963;&#20989;&#25968;&#30340;&#33539;&#22260;&#65292;&#23558;&#20854;&#36716;&#21270;&#20026;&#33258;&#36866;&#24212;&#23545;&#24212;&#29289;&#65292;&#20854;&#24418;&#29366;&#21487;&#20197;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#25152;&#25552;&#20986;&#30340;&#20989;&#25968;&#24418;&#24335;&#30340;&#23548;&#25968;&#20063;&#21487;&#20197;&#29992;Mittag-Leffler&#20989;&#25968;&#34920;&#31034;&#65292;&#22240;&#27492;&#23427;&#26159;&#26799;&#24230;&#19979;&#38477;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#30340;&#21512;&#36866;&#20505;&#36873;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#22797;&#26434;&#24230;&#21644;&#19981;&#21516;&#22823;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#35777;&#26126;&#37319;&#29992;&#32479;&#19968;&#30340;&#38376;&#25511;&#28608;&#27963;&#20989;&#25968;&#34920;&#31034;&#20026;&#21508;&#31181;&#20869;&#32622;&#23454;&#29616;&#30340;&#32463;&#27982;&#30340;&#21644;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a unified representation of the most popular neural network activation functions. Adopting Mittag-Leffler functions of fractional calculus, we propose a flexible and compact functional form that is able to interpolate between various activation functions and mitigate common problems in training neural networks such as vanishing and exploding gradients. The presented gated representation extends the scope of fixed-shape activation functions to their adaptive counterparts whose shape can be learnt from the training data. The derivatives of the proposed functional form can also be expressed in terms of Mittag-Leffler functions making it a suitable candidate for gradient-based backpropagation algorithms. By training multiple neural networks of different complexities on various datasets with different sizes, we demonstrate that adopting a unified gated representation of activation functions offers a promising and affordable alternative to individual built-in implementations of ac
&lt;/p&gt;</description></item></channel></rss>