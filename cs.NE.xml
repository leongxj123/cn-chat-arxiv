<rss version="2.0"><channel><title>Chat Arxiv cs.NE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NE</description><item><title>HoSNN&#26159;&#19968;&#31181;&#23545;&#25239;&#24615;&#31283;&#24577;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#37319;&#29992;&#33258;&#36866;&#24212;&#21457;&#25918;&#38408;&#20540;&#30340;&#28183;&#28431;&#25972;&#21512;&#19982;&#21457;&#25918;&#65288;TA-LIF&#65289;&#31070;&#32463;&#20803;&#27169;&#22411;&#26469;&#25269;&#24481;&#23545;&#25239;&#25915;&#20987;&#65292;&#24182;&#22312;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#19979;&#20445;&#25252;&#20854;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.10373</link><description>&lt;p&gt;
HoSNN: &#20855;&#26377;&#33258;&#36866;&#24212;&#21457;&#25918;&#38408;&#20540;&#30340;&#23545;&#25239;&#24615;&#31283;&#24577;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
HoSNN: Adversarially-Robust Homeostatic Spiking Neural Networks with Adaptive Firing Thresholds. (arXiv:2308.10373v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10373
&lt;/p&gt;
&lt;p&gt;
HoSNN&#26159;&#19968;&#31181;&#23545;&#25239;&#24615;&#31283;&#24577;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#37319;&#29992;&#33258;&#36866;&#24212;&#21457;&#25918;&#38408;&#20540;&#30340;&#28183;&#28431;&#25972;&#21512;&#19982;&#21457;&#25918;&#65288;TA-LIF&#65289;&#31070;&#32463;&#20803;&#27169;&#22411;&#26469;&#25269;&#24481;&#23545;&#25239;&#25915;&#20987;&#65292;&#24182;&#22312;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#19979;&#20445;&#25252;&#20854;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#22312;&#39640;&#25928;&#21644;&#24378;&#22823;&#30340;&#31070;&#32463;&#21551;&#21457;&#24335;&#35745;&#31639;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#19982;&#20854;&#20182;&#31867;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#19968;&#26679;&#65292;SNNs&#38754;&#20020;&#30528;&#23545;&#25239;&#25915;&#20987;&#30340;&#20005;&#37325;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20174;&#31070;&#32463;&#24658;&#31283;&#24615;&#20013;&#27762;&#21462;&#28789;&#24863;&#30340;&#30740;&#31350;&#65292;&#20197;&#24320;&#21457;&#19968;&#31181;&#20223;&#29983;&#35299;&#20915;&#26041;&#26696;&#65292;&#26469;&#24212;&#23545;SNNs&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#21457;&#25918;&#38408;&#20540;&#30340;&#28183;&#28431;&#25972;&#21512;&#19982;&#21457;&#25918;&#65288;TA-LIF&#65289;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#25105;&#20204;&#37319;&#29992;&#23427;&#26469;&#26500;&#24314;&#25152;&#25552;&#20986;&#30340;&#23545;&#25239;&#24615;&#31283;&#24577;SNN&#65288;HoSNN&#65289;&#12290;&#19982;&#20256;&#32479;&#30340;LIF&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;TA-LIF&#27169;&#22411;&#34701;&#20837;&#20102;&#33258;&#31283;&#23450;&#21160;&#24577;&#38408;&#20540;&#26426;&#21046;&#65292;&#38480;&#21046;&#23545;&#25239;&#24615;&#22122;&#22768;&#30340;&#20256;&#25773;&#65292;&#24182;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#20445;&#25252;HoSNN&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#20197;&#38416;&#26126;TA-LIF&#31070;&#32463;&#20803;&#30340;&#31283;&#23450;&#24615;&#21644;&#25910;&#25947;&#24615;&#65292;&#24378;&#35843;&#23427;&#20204;&#22312;&#36755;&#20837;&#22810;&#26679;&#24615;&#26041;&#38754;&#30340;&#21331;&#36234;&#21160;&#24577;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks (SNNs) offer promise for efficient and powerful neurally inspired computation. Common to other types of neural networks, however, SNNs face the severe issue of vulnerability to adversarial attacks. We present the first study that draws inspiration from neural homeostasis to develop a bio-inspired solution that counters the susceptibilities of SNNs to adversarial onslaughts. At the heart of our approach is a novel threshold-adapting leaky integrate-and-fire (TA-LIF) neuron model, which we adopt to construct the proposed adversarially robust homeostatic SNN (HoSNN). Distinct from traditional LIF models, our TA-LIF model incorporates a self-stabilizing dynamic thresholding mechanism, curtailing adversarial noise propagation and safeguarding the robustness of HoSNNs in an unsupervised manner. Theoretical analysis is presented to shed light on the stability and convergence properties of the TA-LIF neurons, underscoring their superior dynamic robustness under input di
&lt;/p&gt;</description></item></channel></rss>