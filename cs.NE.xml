<rss version="2.0"><channel><title>Chat Arxiv cs.NE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NE</description><item><title>ALERT-Transformer&#26159;&#19968;&#31181;&#23558;&#24322;&#27493;&#24863;&#30693;&#19982;&#21516;&#27493;&#22788;&#29702;&#30456;&#32467;&#21512;&#30340;&#26032;&#39062;&#26725;&#25509;&#26041;&#24335;&#65292;&#36890;&#36807;ALERT&#27169;&#22359;&#12289;&#28789;&#27963;&#30340;&#25968;&#25454;&#35835;&#21462;&#21644;&#22522;&#20110;&#22359;&#30340;&#31232;&#30095;&#24615;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#23454;&#26102;&#20107;&#20214;&#39537;&#21160;&#26102;&#31354;&#25968;&#25454;&#30340;&#32463;&#20856;&#22788;&#29702;&#65292;&#20854;&#24615;&#33021;&#36229;&#36807;&#31454;&#20105;&#23545;&#25163;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#24310;&#36831;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01393</link><description>&lt;p&gt;
ALERT-Transformer: &#23558;&#24322;&#27493;&#21644;&#21516;&#27493;&#26426;&#22120;&#23398;&#20064;&#26725;&#25509;&#22312;&#23454;&#26102;&#20107;&#20214;&#39537;&#21160;&#30340;&#26102;&#31354;&#25968;&#25454;&#19978;
&lt;/p&gt;
&lt;p&gt;
ALERT-Transformer: Bridging Asynchronous and Synchronous Machine Learning for Real-Time Event-based Spatio-Temporal Data
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01393
&lt;/p&gt;
&lt;p&gt;
ALERT-Transformer&#26159;&#19968;&#31181;&#23558;&#24322;&#27493;&#24863;&#30693;&#19982;&#21516;&#27493;&#22788;&#29702;&#30456;&#32467;&#21512;&#30340;&#26032;&#39062;&#26725;&#25509;&#26041;&#24335;&#65292;&#36890;&#36807;ALERT&#27169;&#22359;&#12289;&#28789;&#27963;&#30340;&#25968;&#25454;&#35835;&#21462;&#21644;&#22522;&#20110;&#22359;&#30340;&#31232;&#30095;&#24615;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#23454;&#26102;&#20107;&#20214;&#39537;&#21160;&#26102;&#31354;&#25968;&#25454;&#30340;&#32463;&#20856;&#22788;&#29702;&#65292;&#20854;&#24615;&#33021;&#36229;&#36807;&#31454;&#20105;&#23545;&#25163;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#31264;&#23494;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#30001;&#20107;&#20214;&#24863;&#24212;&#22120;&#20135;&#29983;&#30340;&#36830;&#32493;&#36229;&#31232;&#30095;&#26102;&#31354;&#25968;&#25454;&#30340;&#32463;&#20856;&#22788;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#31649;&#36947;&#65292;&#30001;&#24322;&#27493;&#24863;&#30693;&#21644;&#21516;&#27493;&#22788;&#29702;&#32452;&#25104;&#65292;&#32467;&#21512;&#20102;&#20960;&#20010;&#24605;&#36335;&#65306;&#65288;1&#65289;&#22522;&#20110;PointNet&#27169;&#22411;&#30340;&#23884;&#20837;&#8212;&#8212;ALERT&#27169;&#22359;&#65292;&#21487;&#20197;&#36890;&#36807;&#27844;&#28431;&#26426;&#21046;&#19981;&#26029;&#25972;&#21512;&#26032;&#20107;&#20214;&#24182;&#28040;&#38500;&#26087;&#20107;&#20214;&#65292;&#65288;2&#65289;&#23884;&#20837;&#25968;&#25454;&#30340;&#28789;&#27963;&#35835;&#21462;&#65292;&#21487;&#20197;&#20197;&#20219;&#20309;&#37319;&#26679;&#29575;&#23558;&#22987;&#32456;&#26368;&#26032;&#30340;&#29305;&#24449;&#36755;&#20837;&#21040;&#19979;&#28216;&#27169;&#22411;&#20013;&#65292;&#65288;3&#65289;&#20511;&#37492;Vision Transformer&#30340;&#22522;&#20110;&#22359;&#30340;&#26041;&#27861;&#26469;&#21033;&#29992;&#36755;&#20837;&#30340;&#31232;&#30095;&#24615;&#20197;&#20248;&#21270;&#26041;&#27861;&#30340;&#25928;&#29575;&#12290;&#36825;&#20123;&#23884;&#20837;&#28982;&#21518;&#30001;&#19968;&#20010;&#32463;&#36807;&#23545;&#35937;&#21644;&#25163;&#21183;&#35782;&#21035;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#36827;&#34892;&#22788;&#29702;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#27604;&#31454;&#20105;&#23545;&#25163;&#26356;&#20302;&#30340;&#24310;&#36831;&#65292;&#36798;&#21040;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#24322;&#27493;&#27169;&#22411;&#21487;&#20197;&#20197;&#20219;&#20309;&#25152;&#38656;&#30340;&#37319;&#26679;&#29575;&#36827;&#34892;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
We seek to enable classic processing of continuous ultra-sparse spatiotemporal data generated by event-based sensors with dense machine learning models. We propose a novel hybrid pipeline composed of asynchronous sensing and synchronous processing that combines several ideas: (1) an embedding based on PointNet models -- the ALERT module -- that can continuously integrate new and dismiss old events thanks to a leakage mechanism, (2) a flexible readout of the embedded data that allows to feed any downstream model with always up-to-date features at any sampling rate, (3) exploiting the input sparsity in a patch-based approach inspired by Vision Transformer to optimize the efficiency of the method. These embeddings are then processed by a transformer model trained for object and gesture recognition. Using this approach, we achieve performances at the state-of-the-art with a lower latency than competitors. We also demonstrate that our asynchronous model can operate at any desired sampling r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#21160;&#24577;&#23574;&#23792;&#22270;&#31070;&#32463;&#32593;&#32476;"&#65288;DSGNN&#65289;&#30340;&#26694;&#26550;&#65292;&#23427;&#23558;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#35299;&#20915;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#22797;&#26434;&#24615;&#21644;&#20869;&#23384;&#24320;&#38144;&#38382;&#39064;&#12290;DSGNN&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#23574;&#23792;&#31070;&#32463;&#20803;&#30340;&#29366;&#24577;&#21644;&#36830;&#25509;&#26435;&#37325;&#65292;&#22312;&#20256;&#25773;&#36807;&#31243;&#20013;&#20445;&#25345;&#22270;&#32467;&#26500;&#20449;&#24687;&#30340;&#23436;&#25972;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05373</link><description>&lt;p&gt;
&#21160;&#24577;&#23574;&#23792;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Dynamic Spiking Graph Neural Networks. (arXiv:2401.05373v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#21160;&#24577;&#23574;&#23792;&#22270;&#31070;&#32463;&#32593;&#32476;"&#65288;DSGNN&#65289;&#30340;&#26694;&#26550;&#65292;&#23427;&#23558;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#35299;&#20915;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#22797;&#26434;&#24615;&#21644;&#20869;&#23384;&#24320;&#38144;&#38382;&#39064;&#12290;DSGNN&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#23574;&#23792;&#31070;&#32463;&#20803;&#30340;&#29366;&#24577;&#21644;&#36830;&#25509;&#26435;&#37325;&#65292;&#22312;&#20256;&#25773;&#36807;&#31243;&#20013;&#20445;&#25345;&#22270;&#32467;&#26500;&#20449;&#24687;&#30340;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30456;&#32467;&#21512;&#28176;&#28176;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#36825;&#26159;&#22240;&#20026;&#23427;&#22312;&#22788;&#29702;&#30001;&#22270;&#34920;&#31034;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#26102;&#20855;&#26377;&#20302;&#21151;&#32791;&#21644;&#39640;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#19968;&#20010;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#38754;&#20020;&#30528;&#39640;&#22797;&#26434;&#24615;&#21644;&#22823;&#20869;&#23384;&#24320;&#38144;&#30340;&#25361;&#25112;&#12290;&#30446;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#36890;&#36807;&#20351;&#29992;&#20108;&#36827;&#21046;&#29305;&#24449;&#32780;&#19981;&#26159;&#36830;&#32493;&#29305;&#24449;&#30340;SNNs&#26469;&#26367;&#20195;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#36827;&#34892;&#39640;&#25928;&#35757;&#32451;&#65292;&#36825;&#20250;&#24573;&#35270;&#22270;&#32467;&#26500;&#20449;&#24687;&#24182;&#22312;&#20256;&#25773;&#36807;&#31243;&#20013;&#23548;&#33268;&#32454;&#33410;&#30340;&#20002;&#22833;&#12290;&#27492;&#22806;&#65292;&#20248;&#21270;&#21160;&#24577;&#23574;&#23792;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22312;&#26102;&#38388;&#27493;&#20043;&#38388;&#20256;&#25773;&#20449;&#24687;&#65292;&#36825;&#22686;&#21152;&#20102;&#20869;&#23384;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#21160;&#24577;&#23574;&#23792;&#22270;&#31070;&#32463;&#32593;&#32476;"&#65288;\method{}&#65289;&#30340;&#26694;&#26550;&#12290;&#20026;&#20102;&#20943;&#36731;&#20449;&#24687;&#20002;&#22833;&#38382;&#39064;&#65292;\method{} &#22312;&#20256;&#25773;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#21046;&#65292;&#23427;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#20013;&#21160;&#24577;&#22320;&#35843;&#25972;&#23574;&#23792;&#31070;&#32463;&#20803;&#30340;&#29366;&#24577;&#21644;&#36830;&#25509;&#26435;&#37325;&#65292;&#20197;&#20445;&#25345;&#22270;&#32467;&#26500;&#20449;&#24687;&#30340;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of Spiking Neural Networks (SNNs) and Graph Neural Networks (GNNs) is gradually attracting attention due to the low power consumption and high efficiency in processing the non-Euclidean data represented by graphs. However, as a common problem, dynamic graph representation learning faces challenges such as high complexity and large memory overheads. Current work often uses SNNs instead of Recurrent Neural Networks (RNNs) by using binary features instead of continuous ones for efficient training, which would overlooks graph structure information and leads to the loss of details during propagation. Additionally, optimizing dynamic spiking models typically requires propagation of information across time steps, which increases memory requirements. To address these challenges, we present a framework named \underline{Dy}namic \underline{S}p\underline{i}king \underline{G}raph \underline{N}eural Networks (\method{}). To mitigate the information loss problem, \method{} propagates
&lt;/p&gt;</description></item></channel></rss>