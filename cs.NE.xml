<rss version="2.0"><channel><title>Chat Arxiv cs.NE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NE</description><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38271;&#26399;&#21644;&#22823;&#35268;&#27169;&#30340;&#24103;&#20107;&#20214;&#21333;&#30446;&#26631;&#36319;&#36394;&#25968;&#25454;&#38598;FELT&#65292;&#37325;&#26032;&#35757;&#32451;&#21644;&#35780;&#20272;&#20102;15&#20010;&#22522;&#20934;&#36319;&#36394;&#22120;&#65292;&#24182;&#24341;&#20837;&#20102;&#20851;&#32852;&#35760;&#24518;Transformer&#32593;&#32476;&#26469;&#35299;&#20915;RGB&#24103;&#21644;&#20107;&#20214;&#27969;&#19981;&#23436;&#25972;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.05839</link><description>&lt;p&gt;
&#38271;&#26399;&#24103;&#20107;&#20214;&#35270;&#35273;&#36319;&#36394;&#65306;&#22522;&#20934;&#25968;&#25454;&#38598;&#19982;&#22522;&#20934;&#32447;
&lt;/p&gt;
&lt;p&gt;
Long-term Frame-Event Visual Tracking: Benchmark Dataset and Baseline
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05839
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38271;&#26399;&#21644;&#22823;&#35268;&#27169;&#30340;&#24103;&#20107;&#20214;&#21333;&#30446;&#26631;&#36319;&#36394;&#25968;&#25454;&#38598;FELT&#65292;&#37325;&#26032;&#35757;&#32451;&#21644;&#35780;&#20272;&#20102;15&#20010;&#22522;&#20934;&#36319;&#36394;&#22120;&#65292;&#24182;&#24341;&#20837;&#20102;&#20851;&#32852;&#35760;&#24518;Transformer&#32593;&#32476;&#26469;&#35299;&#20915;RGB&#24103;&#21644;&#20107;&#20214;&#27969;&#19981;&#23436;&#25972;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#22522;&#20110;&#20107;&#20214;/&#24103;&#20107;&#20214;&#30340;&#36319;&#36394;&#22120;&#22312;&#30701;&#26399;&#36319;&#36394;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#28982;&#32780;&#65292;&#23545;&#20110;&#30495;&#23454;&#22330;&#26223;&#30340;&#36319;&#36394;&#28041;&#21450;&#38271;&#26399;&#36319;&#36394;&#65292;&#29616;&#26377;&#36319;&#36394;&#31639;&#27861;&#22312;&#36825;&#20123;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38271;&#26399;&#21644;&#22823;&#35268;&#27169;&#30340;&#24103;&#20107;&#20214;&#21333;&#30446;&#26631;&#36319;&#36394;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;FELT&#12290;&#23427;&#21253;&#21547;742&#20010;&#35270;&#39057;&#21644;1,594,474&#20010;RGB&#24103;&#21644;&#20107;&#20214;&#27969;&#23545;&#65292;&#24182;&#24050;&#25104;&#20026;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#24103;&#20107;&#20214;&#36319;&#36394;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#37325;&#26032;&#35757;&#32451;&#21644;&#35780;&#20272;&#20102;15&#20010;&#22522;&#20934;&#36319;&#36394;&#22120;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#20197;&#20379;&#26410;&#26469;&#30740;&#31350;&#36827;&#34892;&#27604;&#36739;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;RGB&#24103;&#21644;&#20107;&#20214;&#27969;&#30001;&#20110;&#25361;&#25112;&#22240;&#32032;&#30340;&#24433;&#21709;&#21644;&#31354;&#38388;&#31232;&#30095;&#30340;&#20107;&#20214;&#27969;&#32780;&#33258;&#28982;&#19981;&#23436;&#25972;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20851;&#32852;&#35760;&#24518;Transformer&#32593;&#32476;&#20316;&#20026;&#32479;&#19968;&#39592;&#24178;&#65292;&#36890;&#36807;&#23558;&#29616;&#20195;Hopfield&#23618;&#24341;&#20837;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#22359;&#26469;&#36827;&#34892;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05839v1 Announce Type: cross  Abstract: Current event-/frame-event based trackers undergo evaluation on short-term tracking datasets, however, the tracking of real-world scenarios involves long-term tracking, and the performance of existing tracking algorithms in these scenarios remains unclear. In this paper, we first propose a new long-term and large-scale frame-event single object tracking dataset, termed FELT. It contains 742 videos and 1,594,474 RGB frames and event stream pairs and has become the largest frame-event tracking dataset to date. We re-train and evaluate 15 baseline trackers on our dataset for future works to compare. More importantly, we find that the RGB frames and event streams are naturally incomplete due to the influence of challenging factors and spatially sparse event flow. In response to this, we propose a novel associative memory Transformer network as a unified backbone by introducing modern Hopfield layers into multi-head self-attention blocks to
&lt;/p&gt;</description></item><item><title>NACHOS &#26159;&#19968;&#31181;&#38754;&#21521;&#30828;&#20214;&#21463;&#38480;&#30340;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#21270;&#35774;&#35745;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#24182;&#32771;&#34385;&#39592;&#24178;&#21644;&#26089;&#26399;&#36864;&#20986;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2401.13330</link><description>&lt;p&gt;
NACHOS: &#30828;&#20214;&#21463;&#38480;&#30340;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
NACHOS: Neural Architecture Search for Hardware Constrained Early Exit Neural Networks. (arXiv:2401.13330v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13330
&lt;/p&gt;
&lt;p&gt;
NACHOS &#26159;&#19968;&#31181;&#38754;&#21521;&#30828;&#20214;&#21463;&#38480;&#30340;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#21270;&#35774;&#35745;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#24182;&#32771;&#34385;&#39592;&#24178;&#21644;&#26089;&#26399;&#36864;&#20986;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#65288;EENNs&#65289;&#20026;&#26631;&#20934;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#37197;&#22791;&#26089;&#26399;&#36864;&#20986;&#20998;&#31867;&#22120;&#65288;EECs&#65289;&#65292;&#22312;&#22788;&#29702;&#30340;&#20013;&#38388;&#28857;&#19978;&#25552;&#20379;&#36275;&#22815;&#30340;&#20998;&#31867;&#32622;&#20449;&#24230;&#26102;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#22312;&#25928;&#26524;&#21644;&#25928;&#29575;&#26041;&#38754;&#24102;&#26469;&#20102;&#35768;&#22810;&#22909;&#22788;&#12290;&#30446;&#21069;&#65292;EENNs&#30340;&#35774;&#35745;&#26159;&#30001;&#19987;&#23478;&#25163;&#21160;&#23436;&#25104;&#30340;&#65292;&#36825;&#26159;&#19968;&#39033;&#22797;&#26434;&#21644;&#32791;&#26102;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#32771;&#34385;&#35768;&#22810;&#26041;&#38754;&#65292;&#21253;&#25324;&#27491;&#30830;&#30340;&#25918;&#32622;&#12289;&#38408;&#20540;&#35774;&#32622;&#21644;EECs&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#27491;&#22312;&#25506;&#32034;&#20351;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#33258;&#21160;&#21270;&#35774;&#35745;EENNs&#12290;&#30446;&#21069;&#65292;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#20960;&#20010;&#23436;&#25972;&#30340;NAS&#35299;&#20915;&#26041;&#26696;&#29992;&#20110;EENNs&#65292;&#24182;&#19988;&#19968;&#20010;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#32508;&#21512;&#35774;&#35745;&#31574;&#30053;&#65292;&#21516;&#26102;&#32771;&#34385;&#39592;&#24178;&#21644;EECs&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#26412;&#30740;&#31350;&#21576;&#29616;&#20102;&#38754;&#21521;&#30828;&#20214;&#21463;&#38480;&#30340;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NACHOS&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early Exit Neural Networks (EENNs) endow astandard Deep Neural Network (DNN) with Early Exit Classifiers (EECs), to provide predictions at intermediate points of the processing when enough confidence in classification is achieved. This leads to many benefits in terms of effectiveness and efficiency. Currently, the design of EENNs is carried out manually by experts, a complex and time-consuming task that requires accounting for many aspects, including the correct placement, the thresholding, and the computational overhead of the EECs. For this reason, the research is exploring the use of Neural Architecture Search (NAS) to automatize the design of EENNs. Currently, few comprehensive NAS solutions for EENNs have been proposed in the literature, and a fully automated, joint design strategy taking into consideration both the backbone and the EECs remains an open problem. To this end, this work presents Neural Architecture Search for Hardware Constrained Early Exit Neural Networks (NACHOS),
&lt;/p&gt;</description></item></channel></rss>