<rss version="2.0"><channel><title>Chat Arxiv cs.NE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NE</description><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#21069;&#39069;&#21494;&#30382;&#23618;&#21551;&#21457;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35268;&#21010;&#26550;&#26500;&#65292;&#21033;&#29992;&#22810;&#20010;&#22522;&#20110;LLM&#30340;&#27169;&#22359;&#23454;&#29616;&#35268;&#21010;&#30340;&#33258;&#20027;&#21327;&#35843;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#25110;&#30446;&#26631;&#23548;&#21521;&#35268;&#21010;&#30340;&#20219;&#21153;&#26102;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.00194</link><description>&lt;p&gt;
&#21463;&#21069;&#39069;&#21494;&#30382;&#23618;&#21551;&#21457;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35268;&#21010;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
A Prefrontal Cortex-inspired Architecture for Planning in Large Language Models. (arXiv:2310.00194v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00194
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#21069;&#39069;&#21494;&#30382;&#23618;&#21551;&#21457;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35268;&#21010;&#26550;&#26500;&#65292;&#21033;&#29992;&#22810;&#20010;&#22522;&#20110;LLM&#30340;&#27169;&#22359;&#23454;&#29616;&#35268;&#21010;&#30340;&#33258;&#20027;&#21327;&#35843;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#25110;&#30446;&#26631;&#23548;&#21521;&#35268;&#21010;&#30340;&#20219;&#21153;&#26102;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#24778;&#20154;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#22312;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#25110;&#30446;&#26631;&#23548;&#21521;&#35268;&#21010;&#30340;&#20219;&#21153;&#20013;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#20154;&#33041;&#20013;&#33719;&#21462;&#28789;&#24863;&#65292;&#21363;&#36890;&#36807;&#21069;&#39069;&#21494;&#30382;&#23618;&#65288;PFC&#65289;&#20013;&#19987;&#38376;&#27169;&#22359;&#30340;&#37325;&#22797;&#20132;&#20114;&#26469;&#23436;&#25104;&#35268;&#21010;&#12290;&#36825;&#20123;&#27169;&#22359;&#25191;&#34892;&#20914;&#31361;&#30417;&#27979;&#12289;&#29366;&#24577;&#39044;&#27979;&#12289;&#29366;&#24577;&#35780;&#20272;&#12289;&#20219;&#21153;&#20998;&#35299;&#21644;&#20219;&#21153;&#21327;&#35843;&#31561;&#21151;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;LLM&#26377;&#26102;&#33021;&#22815;&#21333;&#29420;&#25191;&#34892;&#36825;&#20123;&#21151;&#33021;&#65292;&#20294;&#22312;&#26381;&#21153;&#20110;&#19968;&#20010;&#30446;&#26631;&#26102;&#24448;&#24448;&#38590;&#20197;&#33258;&#20027;&#21327;&#35843;&#23427;&#20204;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#22810;&#20010;&#22522;&#20110;LLM&#65288;GPT-4&#65289;&#27169;&#22359;&#30340;&#40657;&#30418;&#26550;&#26500;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#19987;&#38376;&#30340;PFC&#21551;&#21457;&#27169;&#22359;&#30340;&#20132;&#20114;&#23558;&#19968;&#20010;&#26356;&#22823;&#30340;&#38382;&#39064;&#20998;&#35299;&#20026;&#22810;&#20010;&#23545;LLM&#30340;&#31616;&#30701;&#33258;&#21160;&#35843;&#29992;&#65292;&#20174;&#32780;&#25913;&#21892;&#35268;&#21010;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35268;&#21010;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#32452;&#21512;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) demonstrate impressive performance on a wide variety of tasks, but they often struggle with tasks that require multi-step reasoning or goal-directed planning. To address this, we take inspiration from the human brain, in which planning is accomplished via the recurrent interaction of specialized modules in the prefrontal cortex (PFC). These modules perform functions such as conflict monitoring, state prediction, state evaluation, task decomposition, and task coordination. We find that LLMs are sometimes capable of carrying out these functions in isolation, but struggle to autonomously coordinate them in the service of a goal. Therefore, we propose a black box architecture with multiple LLM-based (GPT-4) modules. The architecture improves planning through the interaction of specialized PFC-inspired modules that break down a larger problem into multiple brief automated calls to the LLM. We evaluate the combined architecture on two challenging planning tasks -
&lt;/p&gt;</description></item></channel></rss>