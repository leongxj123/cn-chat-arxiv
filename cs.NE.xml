<rss version="2.0"><channel><title>Chat Arxiv cs.NE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NE</description><item><title>&#26412;&#25991;&#37325;&#26032;&#25506;&#35752;&#20102;&#23494;&#38598;&#36830;&#25509;&#21367;&#31215;&#32593;&#32476;&#65288;DenseNets&#65289;&#65292;&#25581;&#31034;&#20102;&#20854;&#30456;&#23545;&#20110;ResNet&#39118;&#26684;&#26550;&#26500;&#30340;&#34987;&#20302;&#20272;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#25913;&#36827;&#26550;&#26500;&#12289;&#22359;&#35774;&#35745;&#21644;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#24471;DenseNets&#21487;&#20197;&#19982;&#29616;&#20195;&#26550;&#26500;&#31454;&#20105;&#65292;&#24182;&#22312;ImageNet-1K&#19978;&#25509;&#36817;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2403.19588</link><description>&lt;p&gt;
DenseNets&#37325;&#29983;&#65306;&#36229;&#36234;ResNets&#21644;ViTs&#30340;&#33539;&#24335;&#36716;&#21464;
&lt;/p&gt;
&lt;p&gt;
DenseNets Reloaded: Paradigm Shift Beyond ResNets and ViTs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#25506;&#35752;&#20102;&#23494;&#38598;&#36830;&#25509;&#21367;&#31215;&#32593;&#32476;&#65288;DenseNets&#65289;&#65292;&#25581;&#31034;&#20102;&#20854;&#30456;&#23545;&#20110;ResNet&#39118;&#26684;&#26550;&#26500;&#30340;&#34987;&#20302;&#20272;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#25913;&#36827;&#26550;&#26500;&#12289;&#22359;&#35774;&#35745;&#21644;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#24471;DenseNets&#21487;&#20197;&#19982;&#29616;&#20195;&#26550;&#26500;&#31454;&#20105;&#65292;&#24182;&#22312;ImageNet-1K&#19978;&#25509;&#36817;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22797;&#33487;&#20102;&#23494;&#38598;&#36830;&#25509;&#21367;&#31215;&#32593;&#32476;&#65288;DenseNets&#65289;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30456;&#23545;&#20110;&#20027;&#23548;&#30340;ResNet&#39118;&#26684;&#26550;&#26500;&#34987;&#20302;&#20272;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;DenseNets&#30340;&#28508;&#21147;&#34987;&#24573;&#35270;&#65292;&#26159;&#22240;&#20026;&#26410;&#26366;&#35302;&#21450;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#20256;&#32479;&#35774;&#35745;&#20803;&#32032;&#26410;&#33021;&#23436;&#20840;&#23637;&#29616;&#20854;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#36830;&#25509;&#30340;&#23494;&#38598;&#36830;&#25509;&#26159;&#24378;&#22823;&#30340;&#65292;&#34920;&#26126;DenseNets&#21487;&#20197;&#34987;&#37325;&#26032;&#28608;&#27963;&#20197;&#19982;&#29616;&#20195;&#26550;&#26500;&#31454;&#20105;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#25913;&#36827;&#20102;&#27425;&#20248;&#32452;&#20214; - &#26550;&#26500;&#35843;&#25972;&#12289;&#22359;&#37325;&#26032;&#35774;&#35745;&#21644;&#25913;&#36827;&#30340;&#35757;&#32451;&#37197;&#26041;&#65292;&#20197;&#25193;&#23637;DenseNets&#24182;&#25552;&#39640;&#20869;&#23384;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#36830;&#25509;&#24555;&#25463;&#26041;&#24335;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#37319;&#29992;&#31616;&#21333;&#30340;&#26550;&#26500;&#20803;&#32032;&#65292;&#26368;&#32456;&#36229;&#36234;&#20102;Swin Transformer&#12289;ConvNeXt&#21644;DeiT-III - &#27531;&#24046;&#23398;&#20064;&#35889;&#31995;&#20013;&#30340;&#20851;&#38190;&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;ImageNet-1K&#19978;&#23637;&#29616;&#20986;&#25509;&#36817;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30340;&#24615;&#33021;&#65292;&#31454;&#20105;wi
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19588v1 Announce Type: cross  Abstract: This paper revives Densely Connected Convolutional Networks (DenseNets) and reveals the underrated effectiveness over predominant ResNet-style architectures. We believe DenseNets' potential was overlooked due to untouched training methods and traditional design elements not fully revealing their capabilities. Our pilot study shows dense connections through concatenation are strong, demonstrating that DenseNets can be revitalized to compete with modern architectures. We methodically refine suboptimal components - architectural adjustments, block redesign, and improved training recipes towards widening DenseNets and boosting memory efficiency while keeping concatenation shortcuts. Our models, employing simple architectural elements, ultimately surpass Swin Transformer, ConvNeXt, and DeiT-III - key architectures in the residual learning lineage. Furthermore, our models exhibit near state-of-the-art performance on ImageNet-1K, competing wi
&lt;/p&gt;</description></item></channel></rss>