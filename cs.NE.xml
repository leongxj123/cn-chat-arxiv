<rss version="2.0"><channel><title>Chat Arxiv cs.NE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NE</description><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#38750;&#36864;&#21270;&#20195;&#25968;&#30340;&#27010;&#24565;&#65292;&#25193;&#23637;&#20102;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#21521;&#37327;&#20540;&#31070;&#32463;&#32593;&#32476;&#65292;&#21253;&#25324;&#36229;&#22797;&#20540;&#27169;&#22411;&#12290;&#36825;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#22312;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#31561;&#22810;&#31181;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.02277</link><description>&lt;p&gt;
&#21521;&#37327;&#20540;&#21644;&#36229;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
Universal Approximation Theorem for Vector- and Hypercomplex-Valued Neural Networks. (arXiv:2401.02277v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02277
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#38750;&#36864;&#21270;&#20195;&#25968;&#30340;&#27010;&#24565;&#65292;&#25193;&#23637;&#20102;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#21521;&#37327;&#20540;&#31070;&#32463;&#32593;&#32476;&#65292;&#21253;&#25324;&#36229;&#22797;&#20540;&#27169;&#22411;&#12290;&#36825;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#22312;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#31561;&#22810;&#31181;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#34920;&#26126;&#65292;&#20855;&#26377;&#19968;&#23618;&#38544;&#34255;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20197;&#20219;&#24847;&#25152;&#38656;&#30340;&#31934;&#24230;&#36924;&#36817;&#32039;&#38598;&#19978;&#30340;&#36830;&#32493;&#20989;&#25968;&#12290;&#35813;&#23450;&#29702;&#25903;&#25345;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#23454;&#20540;&#31070;&#32463;&#32593;&#32476;&#21644;&#19968;&#20123;&#36229;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#65288;&#20363;&#22914;&#22797;&#25968;&#12289;&#22235;&#20803;&#25968;&#12289;&#22235;&#20803;&#25968;&#30690;&#37327;&#21644;Clifford&#20540;&#31070;&#32463;&#32593;&#32476;&#65289;&#65292;&#35813;&#23450;&#29702;&#22343;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#36229;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#22312;&#20855;&#26377;&#38468;&#21152;&#20195;&#25968;&#25110;&#20960;&#20309;&#24615;&#36136;&#30340;&#20195;&#25968;&#19978;&#23450;&#20041;&#30340;&#21521;&#37327;&#20540;&#31070;&#32463;&#32593;&#32476;&#12290;&#26412;&#25991;&#23558;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#25193;&#23637;&#21040;&#20102;&#24191;&#27867;&#30340;&#21521;&#37327;&#20540;&#31070;&#32463;&#32593;&#32476;&#65292;&#21253;&#25324;&#36229;&#22797;&#20540;&#27169;&#22411;&#20316;&#20026;&#29305;&#27530;&#23454;&#20363;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38750;&#36864;&#21270;&#20195;&#25968;&#30340;&#27010;&#24565;&#65292;&#24182;&#38416;&#36848;&#20102;&#22312;&#36825;&#31181;&#20195;&#25968;&#19978;&#23450;&#20041;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
The universal approximation theorem states that a neural network with one hidden layer can approximate continuous functions on compact sets with any desired precision. This theorem supports using neural networks for various applications, including regression and classification tasks. Furthermore, it is valid for real-valued neural networks and some hypercomplex-valued neural networks such as complex-, quaternion-, tessarine-, and Clifford-valued neural networks. However, hypercomplex-valued neural networks are a type of vector-valued neural network defined on an algebra with additional algebraic or geometric properties. This paper extends the universal approximation theorem for a wide range of vector-valued neural networks, including hypercomplex-valued models as particular instances. Precisely, we introduce the concept of non-degenerate algebra and state the universal approximation theorem for neural networks defined on such algebras.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#36827;&#21270;&#31526;&#21495;&#22238;&#24402;&#20316;&#20026;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#26041;&#27861;&#26469;&#25552;&#20986;&#21738;&#20123;&#25968;&#25454;&#24212;&#35813;&#34987;&#37319;&#38598;&#65292;&#36890;&#36807;&#8220;&#22996;&#21592;&#20250;&#26597;&#35810;&#8221;&#26469;&#20943;&#23569;&#25152;&#38656;&#25968;&#25454;&#65292;&#24182;&#22312;&#37325;&#26032;&#21457;&#29616;&#24050;&#30693;&#26041;&#31243;&#25152;&#38656;&#30340;&#25968;&#25454;&#26041;&#38754;&#23454;&#29616;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.10379</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#32422;&#26463;&#30340;&#31526;&#21495;&#22238;&#24402;&#20013;&#20027;&#21160;&#23398;&#20064;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Active Learning in Symbolic Regression Performance with Physical Constraints. (arXiv:2305.10379v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#36827;&#21270;&#31526;&#21495;&#22238;&#24402;&#20316;&#20026;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#26041;&#27861;&#26469;&#25552;&#20986;&#21738;&#20123;&#25968;&#25454;&#24212;&#35813;&#34987;&#37319;&#38598;&#65292;&#36890;&#36807;&#8220;&#22996;&#21592;&#20250;&#26597;&#35810;&#8221;&#26469;&#20943;&#23569;&#25152;&#38656;&#25968;&#25454;&#65292;&#24182;&#22312;&#37325;&#26032;&#21457;&#29616;&#24050;&#30693;&#26041;&#31243;&#25152;&#38656;&#30340;&#25968;&#25454;&#26041;&#38754;&#23454;&#29616;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#31526;&#21495;&#22238;&#24402;&#65288;SR&#65289;&#26159;&#19968;&#31181;&#23558;&#31526;&#21495;&#26041;&#31243;&#25311;&#21512;&#21040;&#25968;&#25454;&#20013;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24471;&#21040;&#31616;&#27905;&#26131;&#25026;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#25506;&#35752;&#20351;&#29992;SR&#20316;&#20026;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#26041;&#27861;&#26469;&#25552;&#20986;&#21738;&#20123;&#25968;&#25454;&#24212;&#35813;&#34987;&#37319;&#38598;&#65292;&#22312;&#27492;&#36807;&#31243;&#20013;&#32771;&#34385;&#29289;&#29702;&#32422;&#26463;&#12290;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;SR&#36890;&#36807;&#8220;&#22996;&#21592;&#20250;&#26597;&#35810;&#8221;&#26469;&#25552;&#20986;&#19979;&#19968;&#27493;&#23454;&#39564;&#12290;&#29289;&#29702;&#32422;&#26463;&#21487;&#20197;&#22312;&#38750;&#24120;&#20302;&#30340;&#25968;&#25454;&#24773;&#20917;&#19979;&#25913;&#21892;&#25152;&#24314;&#35758;&#30340;&#26041;&#31243;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;SR&#25152;&#38656;&#30340;&#25968;&#25454;&#65292;&#24182;&#22312;&#37325;&#26032;&#21457;&#29616;&#24050;&#30693;&#26041;&#31243;&#25152;&#38656;&#30340;&#25968;&#25454;&#26041;&#38754;&#23454;&#29616;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evolutionary symbolic regression (SR) fits a symbolic equation to data, which gives a concise interpretable model. We explore using SR as a method to propose which data to gather in an active learning setting with physical constraints. SR with active learning proposes which experiments to do next. Active learning is done with query by committee, where the Pareto frontier of equations is the committee. The physical constraints improve proposed equations in very low data settings. These approaches reduce the data required for SR and achieves state of the art results in data required to rediscover known equations.
&lt;/p&gt;</description></item></channel></rss>