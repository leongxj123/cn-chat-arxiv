<rss version="2.0"><channel><title>Chat Arxiv cs.NE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NE</description><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#8220;&#26412;&#22320;&#22788;&#29702;&#22120;&#27665;&#20027;&#8221;&#30340;&#31639;&#27861;Cooperator&#65292;&#35813;&#31639;&#27861;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#34920;&#29616;&#27604;Transformer&#31639;&#27861;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.10449</link><description>&lt;p&gt;
&#21512;&#20316;&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;&#12290; &#65288;arXiv:2305.10449v1 [cs.LG]&#65289;
&lt;/p&gt;
&lt;p&gt;
Cooperation Is All You Need. (arXiv:2305.10449v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10449
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#8220;&#26412;&#22320;&#22788;&#29702;&#22120;&#27665;&#20027;&#8221;&#30340;&#31639;&#27861;Cooperator&#65292;&#35813;&#31639;&#27861;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#34920;&#29616;&#27604;Transformer&#31639;&#27861;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36229;&#36234;&#8220;&#26641;&#31361;&#27665;&#20027;&#8221;&#20043;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;Cooperator&#30340;&#8220;&#26412;&#22320;&#22788;&#29702;&#22120;&#27665;&#20027;&#8221;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#19982;&#22522;&#20110;Transformers&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65288;&#20363;&#22914;ChatGPT&#65289;&#22312;&#32622;&#25442;&#19981;&#21464;&#31070;&#32463;&#32593;&#32476;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#30340;&#21151;&#33021;&#36827;&#34892;&#27604;&#36739;&#12290; Transformers&#22522;&#20110;&#38271;&#26399;&#20197;&#26469;&#30340;&#8220;&#31215;&#20998;-&#21457;&#23556;&#8221;&#8220;&#28857;&#8221;&#31070;&#32463;&#20803;&#30340;&#27010;&#24565;&#65292;&#32780;Cooperator&#21017;&#21463;&#21040;&#26368;&#36817;&#31070;&#32463;&#29983;&#29289;&#23398;&#31361;&#30772;&#30340;&#21551;&#31034;&#65292;&#36825;&#20123;&#31361;&#30772;&#34920;&#26126;&#65292;&#31934;&#31070;&#29983;&#27963;&#30340;&#32454;&#32990;&#22522;&#30784;&#21462;&#20915;&#20110;&#26032;&#30382;&#23618;&#20013;&#20855;&#26377;&#20004;&#20010;&#21151;&#33021;&#19978;&#19981;&#21516;&#28857;&#30340;&#19978;&#30382;&#31070;&#32463;&#20803;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#24403;&#29992;&#20110;RL&#26102;&#65292;&#22522;&#20110;Cooperator&#30340;&#31639;&#27861;&#23398;&#20064;&#36895;&#24230;&#27604;&#22522;&#20110;Transformer&#30340;&#31639;&#27861;&#24555;&#24471;&#22810;&#65292;&#21363;&#20351;&#23427;&#20204;&#20855;&#26377;&#30456;&#21516;&#25968;&#37327;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Going beyond 'dendritic democracy', we introduce a 'democracy of local processors', termed Cooperator. Here we compare their capabilities when used in permutation-invariant neural networks for reinforcement learning (RL), with machine learning algorithms based on Transformers, such as ChatGPT. Transformers are based on the long-standing conception of integrate-and-fire 'point' neurons, whereas Cooperator is inspired by recent neurobiological breakthroughs suggesting that the cellular foundations of mental life depend on context-sensitive pyramidal neurons in the neocortex which have two functionally distinct points. We show that when used for RL, an algorithm based on Cooperator learns far quicker than that based on Transformer, even while having the same number of parameters.
&lt;/p&gt;</description></item></channel></rss>