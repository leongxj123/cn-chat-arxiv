<rss version="2.0"><channel><title>Chat Arxiv cs.NE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NE</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33041;&#21551;&#21457;&#24335;&#30340;&#21487;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#32452;&#32455;&#35843;&#33410;&#32593;&#32476;&#23558;&#21333;&#19968;&#26377;&#38480;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#37325;&#26032;&#32452;&#32455;&#20026;&#20016;&#23500;&#30340;&#31232;&#30095;&#31070;&#32463;&#36335;&#24452;&#65292;&#20197;&#39640;&#25928;&#24212;&#23545;&#36882;&#22686;&#20219;&#21153;&#65292;&#24182;&#22312;&#21508;&#31181;&#21487;&#25345;&#32493;&#23398;&#20064;&#20219;&#21153;&#20197;&#21450;&#27867;&#21270;&#30340;CIFAR100&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#19968;&#33268;&#30340;&#24615;&#33021;&#20248;&#21183;&#12289;&#33021;&#32791;&#21644;&#20869;&#23384;&#23481;&#37327;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.09550</link><description>&lt;p&gt;
&#20855;&#26377;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#25345;&#32493;&#23398;&#20064;&#30340;&#31070;&#32463;&#36335;&#24452;&#30340;&#33258;&#36866;&#24212;&#37325;&#32452;
&lt;/p&gt;
&lt;p&gt;
Adaptive Reorganization of Neural Pathways for Continual Learning with Spiking Neural Networks. (arXiv:2309.09550v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33041;&#21551;&#21457;&#24335;&#30340;&#21487;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#32452;&#32455;&#35843;&#33410;&#32593;&#32476;&#23558;&#21333;&#19968;&#26377;&#38480;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#37325;&#26032;&#32452;&#32455;&#20026;&#20016;&#23500;&#30340;&#31232;&#30095;&#31070;&#32463;&#36335;&#24452;&#65292;&#20197;&#39640;&#25928;&#24212;&#23545;&#36882;&#22686;&#20219;&#21153;&#65292;&#24182;&#22312;&#21508;&#31181;&#21487;&#25345;&#32493;&#23398;&#20064;&#20219;&#21153;&#20197;&#21450;&#27867;&#21270;&#30340;CIFAR100&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#19968;&#33268;&#30340;&#24615;&#33021;&#20248;&#21183;&#12289;&#33021;&#32791;&#21644;&#20869;&#23384;&#23481;&#37327;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33041;&#21487;&#20197;&#33258;&#32452;&#32455;&#20986;&#20016;&#23500;&#22810;&#26679;&#30340;&#31232;&#30095;&#31070;&#32463;&#36335;&#24452;&#65292;&#36880;&#27493;&#25484;&#25569;&#25968;&#30334;&#20010;&#35748;&#30693;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22823;&#22810;&#25968;&#28145;&#24230;&#20154;&#24037;&#21644;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#26080;&#27861;&#20805;&#20998;&#33258;&#21160;&#35843;&#33410;&#32593;&#32476;&#20013;&#26377;&#38480;&#30340;&#36164;&#28304;&#65292;&#36825;&#23548;&#33268;&#38543;&#30528;&#20219;&#21153;&#22686;&#21152;&#65292;&#24615;&#33021;&#19979;&#38477;&#65292;&#33021;&#32791;&#19978;&#21319;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33041;&#21551;&#21457;&#24335;&#30340;&#21487;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#32452;&#32455;&#35843;&#33410;&#32593;&#32476;&#23558;&#21333;&#19968;&#26377;&#38480;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SOR-SNN&#65289;&#37325;&#26032;&#32452;&#32455;&#20026;&#20016;&#23500;&#30340;&#31232;&#30095;&#31070;&#32463;&#36335;&#24452;&#65292;&#20197;&#39640;&#25928;&#24212;&#23545;&#36882;&#22686;&#20219;&#21153;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;&#21487;&#25345;&#32493;&#23398;&#20064;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#19968;&#33268;&#30340;&#24615;&#33021;&#20248;&#21183;&#12289;&#33021;&#32791;&#21644;&#20869;&#23384;&#23481;&#37327;&#20248;&#21183;&#65292;&#21253;&#25324;&#20174;&#20799;&#31461;&#31616;&#21333;&#20219;&#21153;&#21040;&#22797;&#26434;&#20219;&#21153;&#12289;&#20197;&#21450;&#27867;&#21270;&#30340;CIFAR100&#21644;ImageNet&#25968;&#25454;&#38598;&#12290;&#23588;&#20854;&#26159;&#65292;SOR-SNN&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12289;&#33021;&#32791;&#21644;&#20869;&#23384;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The human brain can self-organize rich and diverse sparse neural pathways to incrementally master hundreds of cognitive tasks. However, most existing continual learning algorithms for deep artificial and spiking neural networks are unable to adequately auto-regulate the limited resources in the network, which leads to performance drop along with energy consumption rise as the increase of tasks. In this paper, we propose a brain-inspired continual learning algorithm with adaptive reorganization of neural pathways, which employs Self-Organizing Regulation networks to reorganize the single and limited Spiking Neural Network (SOR-SNN) into rich sparse neural pathways to efficiently cope with incremental tasks. The proposed model demonstrates consistent superiority in performance, energy consumption, and memory capacity on diverse continual learning tasks ranging from child-like simple to complex tasks, as well as on generalized CIFAR100 and ImageNet datasets. In particular, the SOR-SNN mod
&lt;/p&gt;</description></item></channel></rss>