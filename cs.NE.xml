<rss version="2.0"><channel><title>Chat Arxiv cs.NE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NE</description><item><title>&#26412;&#25991;&#23558;&#30828;&#20214;&#36817;&#20284;&#23884;&#20837;&#21040;&#21360;&#21047;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#31163;&#25955;&#36951;&#20256;&#31639;&#27861;&#23454;&#29616;&#20102;&#26368;&#22823;&#21270;&#30828;&#20214;&#36817;&#20284;&#30340;&#25928;&#30410;&#65292;&#22312;5%&#30340;&#31934;&#24230;&#25439;&#22833;&#19979;&#65292;&#30456;&#27604;&#22522;&#32447;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;5&#20493;&#30340;&#38754;&#31215;&#21644;&#21151;&#32791;&#30340;&#20943;&#23569;&#65292;&#24182;&#19988;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#36817;&#20284;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.02930</link><description>&lt;p&gt;
&#23558;&#30828;&#20214;&#36817;&#20284;&#23884;&#20837;&#31163;&#25955;&#22522;&#22240;&#35757;&#32451;&#20013;&#20197;&#29992;&#20110;&#21360;&#21047;&#22810;&#23618;&#24863;&#30693;&#22120;
&lt;/p&gt;
&lt;p&gt;
Embedding Hardware Approximations in Discrete Genetic-based Training for Printed MLPs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#30828;&#20214;&#36817;&#20284;&#23884;&#20837;&#21040;&#21360;&#21047;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#31163;&#25955;&#36951;&#20256;&#31639;&#27861;&#23454;&#29616;&#20102;&#26368;&#22823;&#21270;&#30828;&#20214;&#36817;&#20284;&#30340;&#25928;&#30410;&#65292;&#22312;5%&#30340;&#31934;&#24230;&#25439;&#22833;&#19979;&#65292;&#30456;&#27604;&#22522;&#32447;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;5&#20493;&#30340;&#38754;&#31215;&#21644;&#21151;&#32791;&#30340;&#20943;&#23569;&#65292;&#24182;&#19988;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#36817;&#20284;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21360;&#21047;&#30005;&#23376;&#26159;&#19968;&#31181;&#26377;&#30528;&#20302;&#25104;&#26412;&#21644;&#28789;&#27963;&#21046;&#36896;&#31561;&#29420;&#29305;&#29305;&#28857;&#30340;&#26377;&#26395;&#24191;&#27867;&#24212;&#29992;&#20110;&#35745;&#31639;&#39046;&#22495;&#30340;&#25216;&#26415;&#12290;&#19982;&#20256;&#32479;&#30340;&#30789;&#22522;&#25216;&#26415;&#19981;&#21516;&#65292;&#21360;&#21047;&#30005;&#23376;&#21487;&#20197;&#23454;&#29616;&#21487;&#20280;&#32553;&#12289;&#21487;&#36866;&#24212;&#12289;&#38750;&#27602;&#24615;&#30340;&#30828;&#20214;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21360;&#21047;&#30005;&#23376;&#30340;&#29305;&#24615;&#23610;&#23544;&#36739;&#22823;&#65292;&#35201;&#23454;&#29616;&#22797;&#26434;&#30340;&#30005;&#36335;&#22914;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#36817;&#20284;&#35745;&#31639;&#34987;&#35777;&#26126;&#21487;&#20197;&#38477;&#20302;&#26426;&#22120;&#23398;&#20064;&#30005;&#36335;&#65288;&#22914;&#22810;&#23618;&#24863;&#30693;&#22120;&#65289;&#30340;&#30828;&#20214;&#25104;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#30828;&#20214;&#36817;&#20284;&#23884;&#20837;&#21040;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#26469;&#26368;&#22823;&#21270;&#36817;&#20284;&#35745;&#31639;&#30340;&#30410;&#22788;&#12290;&#30001;&#20110;&#30828;&#20214;&#36817;&#20284;&#30340;&#31163;&#25955;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#30828;&#20214;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#65292;&#19987;&#38376;&#20026;&#21360;&#21047;&#22810;&#23618;&#24863;&#30693;&#22120;&#35774;&#35745;&#12290;&#22312;5%&#30340;&#31934;&#24230;&#25439;&#22833;&#19979;&#65292;&#30456;&#27604;&#22522;&#32447;&#65292;&#25105;&#20204;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#22312;&#38754;&#31215;&#21644;&#21151;&#32791;&#19978;&#23454;&#29616;&#20102;&#36229;&#36807;5&#20493;&#30340;&#20943;&#23569;&#65292;&#24182;&#19988;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#36817;&#20284;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Printed Electronics (PE) stands out as a promisingtechnology for widespread computing due to its distinct attributes, such as low costs and flexible manufacturing. Unlike traditional silicon-based technologies, PE enables stretchable, conformal,and non-toxic hardware. However, PE are constrained by larger feature sizes, making it challenging to implement complex circuits such as machine learning (ML) classifiers. Approximate computing has been proven to reduce the hardware cost of ML circuits such as Multilayer Perceptrons (MLPs). In this paper, we maximize the benefits of approximate computing by integrating hardware approximation into the MLP training process. Due to the discrete nature of hardware approximation, we propose and implement a genetic-based, approximate, hardware-aware training approach specifically designed for printed MLPs. For a 5% accuracy loss, our MLPs achieve over 5x area and power reduction compared to the baseline while outperforming state of-the-art approximate
&lt;/p&gt;</description></item></channel></rss>