<rss version="2.0"><channel><title>Chat Arxiv cs.NE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NE</description><item><title>&#20004;&#31181;&#21333;&#30456;&#23545;&#27604;&#28023;&#27604;&#23433;&#23398;&#20064;&#30340;&#25925;&#20107;&#25506;&#32034;&#20102;&#23398;&#20064;&#31639;&#27861;&#30340;&#29983;&#29289;&#21512;&#29702;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#23616;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#28040;&#38500;&#19982;&#21453;&#21521;&#20256;&#25773;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#35299;&#20915;&#20102;&#21516;&#27493;&#21644;&#26080;&#38480;&#23567;&#25200;&#21160;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.08573</link><description>&lt;p&gt;
&#20004;&#31181;&#21333;&#30456;&#23545;&#27604;&#28023;&#27604;&#23433;&#23398;&#20064;&#30340;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
Two Tales of Single-Phase Contrastive Hebbian Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08573
&lt;/p&gt;
&lt;p&gt;
&#20004;&#31181;&#21333;&#30456;&#23545;&#27604;&#28023;&#27604;&#23433;&#23398;&#20064;&#30340;&#25925;&#20107;&#25506;&#32034;&#20102;&#23398;&#20064;&#31639;&#27861;&#30340;&#29983;&#29289;&#21512;&#29702;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#23616;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#28040;&#38500;&#19982;&#21453;&#21521;&#20256;&#25773;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#35299;&#20915;&#20102;&#21516;&#27493;&#21644;&#26080;&#38480;&#23567;&#25200;&#21160;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#8220;&#29983;&#29289;&#23398;&#19978;&#21512;&#29702;&#8221;&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;&#25506;&#32034;&#24050;&#32463;&#25910;&#25947;&#20110;&#23558;&#26799;&#24230;&#34920;&#31034;&#20026;&#27963;&#21160;&#24046;&#24322;&#30340;&#24819;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#38656;&#35201;&#36739;&#39640;&#31243;&#24230;&#30340;&#21516;&#27493;&#65288;&#23398;&#20064;&#26399;&#38388;&#30340;&#19981;&#21516;&#38454;&#27573;&#65289;&#24182;&#24341;&#20837;&#22823;&#37327;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#36825;&#23545;&#20110;&#23427;&#20204;&#30340;&#29983;&#29289;&#23398;&#21512;&#29702;&#24615;&#20197;&#21450;&#20854;&#22312;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#20013;&#30340;&#28508;&#22312;&#25928;&#29992;&#20135;&#29983;&#20102;&#30097;&#38382;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;&#36755;&#20986;&#21333;&#20803;&#26045;&#21152;&#26080;&#38480;&#23567;&#25200;&#21160;&#65288;nudges&#65289;&#65292;&#36825;&#22312;&#22024;&#26434;&#29615;&#22659;&#20013;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26368;&#36817;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#23558;&#20154;&#24037;&#31070;&#32463;&#20803;&#24314;&#27169;&#20026;&#20004;&#20010;&#30456;&#21453;&#25200;&#21160;&#30340;&#32452;&#20214;&#65292;&#21517;&#20026;&#8220;&#21452;&#21521;&#20256;&#25773;&#8221;&#30340;&#20840;&#23616;&#23398;&#20064;&#31639;&#27861;&#33021;&#22815;&#24357;&#21512;&#21040;&#21453;&#21521;&#20256;&#25773;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#32780;&#19981;&#38656;&#35201;&#20998;&#21035;&#30340;&#23398;&#20064;&#38454;&#27573;&#25110;&#26080;&#38480;&#23567;&#25200;&#21160;&#12290;&#28982;&#32780;&#65292;&#35813;&#31639;&#27861;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#20381;&#36182;&#20110;&#23545;&#31216;&#25200;&#21160;&#65292;&#36825;&#21487;&#33021;&#22312;&#29983;&#29289;&#23398;&#19978;&#21463;&#21040;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The search for "biologically plausible" learning algorithms has converged on the idea of representing gradients as activity differences. However, most approaches require a high degree of synchronization (distinct phases during learning) and introduce substantial computational overhead, which raises doubts regarding their biological plausibility as well as their potential utility for neuromorphic computing. Furthermore, they commonly rely on applying infinitesimal perturbations (nudges) to output units, which is impractical in noisy environments. Recently it has been shown that by modelling artificial neurons as dyads with two oppositely nudged compartments, it is possible for a fully local learning algorithm named ``dual propagation'' to bridge the performance gap to backpropagation, without requiring separate learning phases or infinitesimal nudging. However, the algorithm has the drawback that its numerical stability relies on symmetric nudging, which may be restrictive in biological
&lt;/p&gt;</description></item></channel></rss>