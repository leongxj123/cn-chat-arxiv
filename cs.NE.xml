<rss version="2.0"><channel><title>Chat Arxiv cs.NE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NE</description><item><title>&#21435;&#22122;&#22768;&#26356;&#22909;&#65292;&#34920;&#29616;&#26356;&#22909;&#30340;&#20998;&#23618;&#32423;&#21035;&#28608;&#27963;&#26426;&#21046;</title><link>http://arxiv.org/abs/2306.04940</link><description>&lt;p&gt;
&#20998;&#23618;&#32423;&#21035;&#28608;&#27963;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Layer-level activation mechanism. (arXiv:2306.04940v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04940
&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#22768;&#26356;&#22909;&#65292;&#34920;&#29616;&#26356;&#22909;&#30340;&#20998;&#23618;&#32423;&#21035;&#28608;&#27963;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28608;&#27963;&#26426;&#21046;&#65292;&#26088;&#22312;&#24314;&#31435;&#20998;&#23618;&#32423;&#21035;&#28608;&#27963;&#21151;&#33021;&#65288;LayerAct&#65289;&#12290;&#36825;&#20123;&#21151;&#33021;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#36755;&#20837;&#20559;&#31227;&#25152;&#23548;&#33268;&#30340;&#28608;&#27963;&#36755;&#20986;&#30340;&#20998;&#23618;&#32423;&#27874;&#21160;&#26469;&#38477;&#20302;&#20256;&#32479;&#20803;&#32032;&#32423;&#28608;&#27963;&#21151;&#33021;&#30340;&#22122;&#38899;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;LayerAct&#21151;&#33021;&#23454;&#29616;&#20102;&#31867;&#20284;&#20110;&#38646;&#30340;&#24179;&#22343;&#28608;&#27963;&#36755;&#20986;&#65292;&#32780;&#19981;&#38480;&#21046;&#28608;&#27963;&#36755;&#20986;&#31354;&#38388;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20998;&#26512;&#21644;&#23454;&#39564;&#65292;&#35777;&#26126;LayerAct&#21151;&#33021;&#22312;&#22122;&#22768;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#20110;&#20803;&#32032;&#32423;&#28608;&#27963;&#21151;&#33021;&#65292;&#24182;&#19988;&#32463;&#39564;&#35777;&#26126;&#36825;&#20123;&#21151;&#33021;&#30340;&#24179;&#22343;&#28608;&#27963;&#32467;&#26524;&#31867;&#20284;&#20110;&#38646;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22788;&#29702;&#22024;&#26434;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#26102;&#65292;LayerAct&#21151;&#33021;&#27604;&#20803;&#32032;&#32423;&#28608;&#27963;&#21151;&#33021;&#34920;&#29616;&#26356;&#22909;&#65292;&#32780;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#28165;&#27905;&#25968;&#25454;&#38598;&#30340;&#34920;&#29616;&#20063;&#26159;&#20248;&#36234;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a novel activation mechanism aimed at establishing layer-level activation (LayerAct) functions. These functions are designed to be more noise-robust compared to traditional element-level activation functions by reducing the layer-level fluctuation of the activation outputs due to shift in inputs. Moreover, the LayerAct functions achieve a zero-like mean activation output without restricting the activation output space. We present an analysis and experiments demonstrating that LayerAct functions exhibit superior noise-robustness compared to element-level activation functions, and empirically show that these functions have a zero-like mean activation. Experimental results on three benchmark image classification tasks show that LayerAct functions excel in handling noisy image datasets, outperforming element-level activation functions, while the performance on clean datasets is also superior in most cases.
&lt;/p&gt;</description></item></channel></rss>