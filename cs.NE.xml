<rss version="2.0"><channel><title>Chat Arxiv cs.NE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NE</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#20026;&#24378;&#22823;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20013;&#21516;&#19968;&#38544;&#34255;&#23618;&#20013;&#30340;&#38544;&#34255;&#31070;&#32463;&#20803;&#30456;&#20114;&#36830;&#25509;&#65292;&#21487;&#20197;&#23398;&#20064;&#22797;&#26434;&#27169;&#24335;&#24182;&#21152;&#36895;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.10468</link><description>&lt;p&gt;
&#36830;&#25509;&#38544;&#34255;&#31070;&#32463;&#20803;&#65288;CHNNet&#65289;&#65306;&#19968;&#31181;&#24555;&#36895;&#25910;&#25947;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Connected Hidden Neurons (CHNNet): An Artificial Neural Network for Rapid Convergence. (arXiv:2305.10468v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10468
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#20026;&#24378;&#22823;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20013;&#21516;&#19968;&#38544;&#34255;&#23618;&#20013;&#30340;&#38544;&#34255;&#31070;&#32463;&#20803;&#30456;&#20114;&#36830;&#25509;&#65292;&#21487;&#20197;&#23398;&#20064;&#22797;&#26434;&#27169;&#24335;&#24182;&#21152;&#36895;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#26680;&#24515;&#30446;&#30340;&#26159;&#27169;&#20223;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#30340;&#21151;&#33021;&#12290;&#28982;&#32780;&#65292;&#19982;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#19981;&#21516;&#65292;&#20256;&#32479;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#26159;&#25353;&#23618;&#27425;&#32467;&#26500;&#21270;&#30340;&#65292;&#36825;&#21487;&#33021;&#20250;&#22952;&#30861;&#31070;&#32463;&#20803;&#20043;&#38388;&#30340;&#20449;&#24687;&#27969;&#21160;&#65292;&#22240;&#20026;&#21516;&#19968;&#23618;&#20013;&#30340;&#31070;&#32463;&#20803;&#20043;&#38388;&#27809;&#26377;&#36830;&#25509;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#20026;&#24378;&#22823;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20854;&#20013;&#21516;&#19968;&#38544;&#34255;&#23618;&#20013;&#30340;&#38544;&#34255;&#31070;&#32463;&#20803;&#26159;&#20114;&#30456;&#36830;&#25509;&#30340;&#65292;&#20351;&#24471;&#31070;&#32463;&#20803;&#33021;&#22815;&#23398;&#20064;&#22797;&#26434;&#30340;&#27169;&#24335;&#24182;&#21152;&#36895;&#25910;&#25947;&#36895;&#24230;&#12290;&#36890;&#36807;&#22312;&#27973;&#23618;&#21644;&#28145;&#23618;&#32593;&#32476;&#20013;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#20316;&#20026;&#23436;&#20840;&#36830;&#25509;&#30340;&#23618;&#36827;&#34892;&#23454;&#39564;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#36825;&#20010;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The core purpose of developing artificial neural networks was to mimic the functionalities of biological neural networks. However, unlike biological neural networks, traditional artificial neural networks are often structured hierarchically, which can impede the flow of information between neurons as the neurons in the same layer have no connections between them. Hence, we propose a more robust model of artificial neural networks where the hidden neurons, residing in the same hidden layer, are interconnected, enabling the neurons to learn complex patterns and speeding up the convergence rate. With the experimental study of our proposed model as fully connected layers in shallow and deep networks, we demonstrate that the model results in a significant increase in convergence rate.
&lt;/p&gt;</description></item><item><title>&#21333;&#32431;&#22797;&#24418;&#27744;&#21270;&#23618;NervePool&#22312;&#27744;&#21270;&#22270;&#32467;&#26500;&#25968;&#25454;&#26102;&#65292;&#22522;&#20110;&#39030;&#28857;&#20998;&#21306;&#29983;&#25104;&#21333;&#32431;&#22797;&#24418;&#30340;&#20998;&#23618;&#34920;&#31034;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#24314;&#27169;&#26356;&#39640;&#38454;&#30340;&#20851;&#31995;&#65292;&#21516;&#26102;&#32553;&#23567;&#39640;&#32500;&#21333;&#32431;&#24418;&#65292;&#23454;&#29616;&#38477;&#37319;&#26679;&#65292;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21644;&#20943;&#23569;&#36807;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2305.06315</link><description>&lt;p&gt;
NervePool: &#19968;&#20010;&#21333;&#32431;&#22797;&#24418;&#27744;&#21270;&#23618;
&lt;/p&gt;
&lt;p&gt;
NervePool: A Simplicial Pooling Layer. (arXiv:2305.06315v1 [cs.CG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06315
&lt;/p&gt;
&lt;p&gt;
&#21333;&#32431;&#22797;&#24418;&#27744;&#21270;&#23618;NervePool&#22312;&#27744;&#21270;&#22270;&#32467;&#26500;&#25968;&#25454;&#26102;&#65292;&#22522;&#20110;&#39030;&#28857;&#20998;&#21306;&#29983;&#25104;&#21333;&#32431;&#22797;&#24418;&#30340;&#20998;&#23618;&#34920;&#31034;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#24314;&#27169;&#26356;&#39640;&#38454;&#30340;&#20851;&#31995;&#65292;&#21516;&#26102;&#32553;&#23567;&#39640;&#32500;&#21333;&#32431;&#24418;&#65292;&#23454;&#29616;&#38477;&#37319;&#26679;&#65292;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21644;&#20943;&#23569;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#38382;&#39064;&#65292;&#27744;&#21270;&#23618;&#23545;&#20110;&#38477;&#37319;&#26679;&#12289;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21644;&#20943;&#23569;&#36807;&#25311;&#21512;&#37117;&#24456;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27744;&#21270;&#23618;&#65292;NervePool&#65292;&#36866;&#29992;&#20110;&#21333;&#32431;&#22797;&#24418;&#32467;&#26500;&#30340;&#25968;&#25454;&#65292;&#36825;&#31181;&#32467;&#26500;&#26159;&#22270;&#30340;&#25512;&#24191;&#65292;&#21253;&#25324;&#27604;&#39030;&#28857;&#21644;&#36793;&#26356;&#39640;&#32500;&#24230;&#30340;&#21333;&#32431;&#24418;&#65307;&#36825;&#31181;&#32467;&#26500;&#21487;&#20197;&#26356;&#28789;&#27963;&#22320;&#24314;&#27169;&#26356;&#39640;&#38454;&#30340;&#20851;&#31995;&#12290;&#25152;&#25552;&#20986;&#30340;&#21333;&#32431;&#22797;&#21512;&#32553;&#23567;&#26041;&#26696;&#22522;&#20110;&#39030;&#28857;&#30340;&#20998;&#21306;&#26500;&#24314;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#29983;&#25104;&#21333;&#32431;&#22797;&#24418;&#30340;&#20998;&#23618;&#34920;&#31034;&#65292;&#20197;&#19968;&#31181;&#23398;&#20064;&#30340;&#26041;&#24335;&#25240;&#21472;&#20449;&#24687;&#12290;NervePool&#24314;&#31435;&#22312;&#23398;&#20064;&#30340;&#39030;&#28857;&#32676;&#38598;&#20998;&#37197;&#30340;&#22522;&#30784;&#19978;&#65292;&#24182;&#20197;&#19968;&#31181;&#30830;&#23450;&#24615;&#30340;&#26041;&#24335;&#25193;&#23637;&#21040;&#39640;&#32500;&#21333;&#32431;&#24418;&#30340;&#32553;&#23567;&#12290;&#34429;&#28982;&#22312;&#23454;&#36341;&#20013;&#65292;&#27744;&#21270;&#25805;&#20316;&#26159;&#36890;&#36807;&#19968;&#31995;&#21015;&#30697;&#38453;&#36816;&#31639;&#26469;&#35745;&#31639;&#30340;&#65292;&#20294;&#26159;&#20854;&#25299;&#25169;&#21160;&#26426;&#26159;&#19968;&#20010;&#22522;&#20110;&#21333;&#32431;&#24418;&#26143;&#26143;&#30340;&#24182;&#38598;&#21644;&#31070;&#32463;&#22797;&#21512;&#20307;&#30340;&#38598;&#21512;&#26500;&#36896;&#12290;
&lt;/p&gt;
&lt;p&gt;
For deep learning problems on graph-structured data, pooling layers are important for down sampling, reducing computational cost, and to minimize overfitting. We define a pooling layer, NervePool, for data structured as simplicial complexes, which are generalizations of graphs that include higher-dimensional simplices beyond vertices and edges; this structure allows for greater flexibility in modeling higher-order relationships. The proposed simplicial coarsening scheme is built upon partitions of vertices, which allow us to generate hierarchical representations of simplicial complexes, collapsing information in a learned fashion. NervePool builds on the learned vertex cluster assignments and extends to coarsening of higher dimensional simplices in a deterministic fashion. While in practice, the pooling operations are computed via a series of matrix operations, the topological motivation is a set-theoretic construction based on unions of stars of simplices and the nerve complex
&lt;/p&gt;</description></item></channel></rss>