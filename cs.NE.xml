<rss version="2.0"><channel><title>Chat Arxiv cs.NE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NE</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#24341;&#23548;&#30340;&#35748;&#30693;&#36827;&#21270;&#31574;&#30053;&#65292;&#36890;&#36807;&#22810;&#23618;&#34701;&#21512;&#33258;&#36866;&#24212;&#23398;&#20064;&#21644;&#33258;&#28982;&#36807;&#31243;&#65292;&#26377;&#25928;&#39044;&#27979;&#21271;&#28201;&#24102;&#28246;&#27850;&#20013;&#30340;&#28342;&#35299;&#27687;&#27987;&#24230;</title><link>https://arxiv.org/abs/2403.18923</link><description>&lt;p&gt;
&#33258;&#28982;&#24341;&#23548;&#30340;&#35748;&#30693;&#36827;&#21270;&#29992;&#20110;&#39044;&#27979;&#21271;&#28201;&#24102;&#28246;&#27850;&#20013;&#30340;&#28342;&#35299;&#27687;&#27987;&#24230;
&lt;/p&gt;
&lt;p&gt;
Nature-Guided Cognitive Evolution for Predicting Dissolved Oxygen Concentrations in North Temperate Lakes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18923
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#24341;&#23548;&#30340;&#35748;&#30693;&#36827;&#21270;&#31574;&#30053;&#65292;&#36890;&#36807;&#22810;&#23618;&#34701;&#21512;&#33258;&#36866;&#24212;&#23398;&#20064;&#21644;&#33258;&#28982;&#36807;&#31243;&#65292;&#26377;&#25928;&#39044;&#27979;&#21271;&#28201;&#24102;&#28246;&#27850;&#20013;&#30340;&#28342;&#35299;&#27687;&#27987;&#24230;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#21271;&#28201;&#24102;&#28246;&#27850;&#20013;&#30340;&#28342;&#35299;&#27687;&#65288;DO&#65289;&#27987;&#24230;&#38656;&#35201;&#23545;&#19981;&#21516;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#29289;&#20505;&#27169;&#24335;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#65292;&#36825;&#20984;&#26174;&#20102;&#36873;&#25321;&#29289;&#20505;&#29305;&#24449;&#21644;&#29305;&#24449;&#20132;&#20114;&#30340;&#37325;&#35201;&#24615;&#12290;&#22522;&#20110;&#36807;&#31243;&#30340;&#27169;&#22411;&#21463;&#37096;&#20998;&#36807;&#31243;&#30693;&#35782;&#38480;&#21046;&#25110;&#29305;&#24449;&#34920;&#31034;&#36807;&#20110;&#31616;&#21270;&#65292;&#32780;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#26377;&#25928;&#36873;&#25321;&#19981;&#21516;&#28246;&#27850;&#31867;&#22411;&#21644;&#20219;&#21153;&#30340;&#30456;&#20851;&#29305;&#24449;&#20132;&#20114;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;DO&#25968;&#25454;&#25910;&#38598;&#19981;&#39057;&#32321;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#24341;&#23548;&#30340;&#35748;&#30693;&#36827;&#21270;&#65288;NGCE&#65289;&#31574;&#30053;&#65292;&#36825;&#20195;&#34920;&#20102;&#33258;&#36866;&#24212;&#23398;&#20064;&#19982;&#33258;&#28982;&#36807;&#31243;&#22810;&#23618;&#34701;&#21512;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#20195;&#35874;&#36807;&#31243;&#20026;&#22522;&#30784;&#30340;&#27169;&#22411;&#29983;&#25104;&#27169;&#25311;DO&#26631;&#31614;&#12290;&#21033;&#29992;&#36825;&#20123;&#27169;&#25311;&#26631;&#31614;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#19968;&#20010;&#22810;&#31181;&#32676;&#35748;&#30693;&#36827;&#21270;&#25628;&#32034;&#65292;&#27169;&#22411;&#21453;&#26144;&#33258;&#28982;&#26377;&#26426;&#20307;&#65292;&#36866;&#24212;&#24615;&#22320;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18923v1 Announce Type: cross  Abstract: Predicting dissolved oxygen (DO) concentrations in north temperate lakes requires a comprehensive study of phenological patterns across various ecosystems, which highlights the significance of selecting phenological features and feature interactions. Process-based models are limited by partial process knowledge or oversimplified feature representations, while machine learning models face challenges in efficiently selecting relevant feature interactions for different lake types and tasks, especially under the infrequent nature of DO data collection. In this paper, we propose a Nature-Guided Cognitive Evolution (NGCE) strategy, which represents a multi-level fusion of adaptive learning with natural processes. Specifically, we utilize metabolic process-based models to generate simulated DO labels. Using these simulated labels, we implement a multi-population cognitive evolutionary search, where models, mirroring natural organisms, adaptiv
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#31070;&#32463;&#32593;&#32476;&#35268;&#33539;&#21270;&#26041;&#27861;CB-Norm&#65292;&#36890;&#36807;&#24341;&#20837;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26799;&#24230;&#31283;&#23450;&#24615;&#21644;&#23398;&#20064;&#21152;&#36895;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.16798</link><description>&lt;p&gt;
&#22522;&#20110;&#32858;&#31867;&#30340;&#31070;&#32463;&#32593;&#32476;&#35268;&#33539;&#21270;&#23618;
&lt;/p&gt;
&lt;p&gt;
Cluster-Based Normalization Layer for Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16798
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#31070;&#32463;&#32593;&#32476;&#35268;&#33539;&#21270;&#26041;&#27861;CB-Norm&#65292;&#36890;&#36807;&#24341;&#20837;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26799;&#24230;&#31283;&#23450;&#24615;&#21644;&#23398;&#20064;&#21152;&#36895;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#20013;&#38754;&#20020;&#37325;&#35201;&#25361;&#25112;&#65292;&#21253;&#25324;&#20869;&#37096;&#21327;&#21464;&#37327;&#28418;&#31227;&#12289;&#26631;&#31614;&#28418;&#31227;&#12289;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#12289;&#36807;&#25311;&#21512;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#20256;&#32479;&#30340;&#35268;&#33539;&#21270;&#26041;&#27861;&#65292;&#22914;&#25209;&#26631;&#20934;&#21270;&#65292;&#26088;&#22312;&#35299;&#20915;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#65292;&#20294;&#36890;&#24120;&#20381;&#36182;&#20110;&#38480;&#21046;&#20854;&#36866;&#24212;&#24615;&#30340;&#20551;&#35774;&#12290;&#28151;&#21512;&#35268;&#33539;&#21270;&#22312;&#22788;&#29702;&#22810;&#20010;&#39640;&#26031;&#20998;&#24067;&#26102;&#38754;&#20020;&#35745;&#31639;&#38556;&#30861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#32858;&#31867;&#30340;&#35268;&#33539;&#21270;&#65288;CB-Norm&#65289;&#30340;&#20004;&#20010;&#21464;&#20307;&#8212;&#8212;&#30417;&#30563;&#24335;&#22522;&#20110;&#32858;&#31867;&#30340;&#35268;&#33539;&#21270;&#65288;SCB-Norm&#65289;&#21644;&#26080;&#30417;&#30563;&#24335;&#22522;&#20110;&#32858;&#31867;&#30340;&#35268;&#33539;&#21270;&#65288;UCB-Norm&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#19968;&#27493;&#35268;&#33539;&#21270;&#26041;&#27861;&#12290;CB-Norm&#21033;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#26469;&#19987;&#38376;&#35299;&#20915;&#19982;&#26799;&#24230;&#31283;&#23450;&#24615;&#21644;&#23398;&#20064;&#21152;&#36895;&#26377;&#20851;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16798v1 Announce Type: cross  Abstract: Deep learning faces significant challenges during the training of neural networks, including internal covariate shift, label shift, vanishing/exploding gradients, overfitting, and computational complexity. While conventional normalization methods, such as Batch Normalization, aim to tackle some of these issues, they often depend on assumptions that constrain their adaptability. Mixture Normalization faces computational hurdles in its pursuit of handling multiple Gaussian distributions.   This paper introduces Cluster-Based Normalization (CB-Norm) in two variants - Supervised Cluster-Based Normalization (SCB-Norm) and Unsupervised Cluster-Based Normalization (UCB-Norm) - proposing a groundbreaking one-step normalization approach. CB-Norm leverages a Gaussian mixture model to specifically address challenges related to gradient stability and learning acceleration.   For SCB-Norm, a supervised variant, the novel mechanism involves introduc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#22240;&#24341;&#23548;GFlowNet (Genetic GFN) &#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#36845;&#20195;&#36951;&#20256;&#25628;&#32034;&#21644;&#35757;&#32451;&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#20998;&#23376;&#20248;&#21270;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;16.213&#30340;&#26368;&#26032;&#24471;&#20998;&#65292;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26368;&#20339;&#24471;&#20998;15.185&#65292;&#21516;&#26102;&#22312;14&#20010;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#25152;&#26377;&#23545;&#27604;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.05961</link><description>&lt;p&gt;
&#22522;&#22240;&#24341;&#23548;GFlowNets&#65306;&#22312;&#23454;&#38469;&#20998;&#23376;&#20248;&#21270;&#22522;&#20934;&#26041;&#38754;&#30340;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Genetic-guided GFlowNets: Advancing in Practical Molecular Optimization Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#22240;&#24341;&#23548;GFlowNet (Genetic GFN) &#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#36845;&#20195;&#36951;&#20256;&#25628;&#32034;&#21644;&#35757;&#32451;&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#20998;&#23376;&#20248;&#21270;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;16.213&#30340;&#26368;&#26032;&#24471;&#20998;&#65292;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26368;&#20339;&#24471;&#20998;15.185&#65292;&#21516;&#26102;&#22312;14&#20010;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#25152;&#26377;&#23545;&#27604;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GFlowNet&#21464;&#20307;&#65292;&#21363;&#22522;&#22240;&#24341;&#23548;GFlowNet (Genetic GFN)&#65292;&#23427;&#23558;&#36845;&#20195;&#36951;&#20256;&#25628;&#32034;&#38598;&#25104;&#21040;GFlowNet&#20013;&#12290;&#36951;&#20256;&#25628;&#32034;&#26377;&#25928;&#22320;&#24341;&#23548;GFlowNet&#36827;&#20837;&#39640;&#22238;&#25253;&#21306;&#22495;&#65292;&#35299;&#20915;&#20102;&#20840;&#23616;&#36807;&#24230;&#25506;&#32034;&#23548;&#33268;&#30340;&#35757;&#32451;&#25928;&#29575;&#20302;&#19979;&#21644;&#25506;&#32034;&#26377;&#38480;&#21306;&#22495;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#35757;&#32451;&#31574;&#30053;&#65292;&#22914;&#22522;&#20110;&#25490;&#21517;&#30340;&#37325;&#25918;&#35757;&#32451;&#21644;&#26080;&#30417;&#30563;&#26368;&#22823;&#20284;&#28982;&#39044;&#35757;&#32451;&#65292;&#20197;&#25552;&#39640;&#22522;&#22240;&#24341;&#23548;GFlowNet&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#20998;&#23376;&#20248;&#21270; (PMO) &#39046;&#22495;&#30340;&#23448;&#26041;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20102;16.213&#30340;&#26368;&#26032;&#24471;&#20998;&#65292;&#26126;&#26174;&#20248;&#20110;&#22522;&#20934;&#27979;&#35797;&#20013;&#25253;&#21578;&#30340;&#26368;&#20339;&#24471;&#20998;15.185&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;23&#20010;&#20219;&#21153;&#20013;&#30340;14&#20010;&#20219;&#21153;&#20013;&#36229;&#36807;&#20102;&#25152;&#26377;&#23545;&#27604;&#26041;&#27861;&#65292;&#21253;&#25324;&#24378;&#21270;&#23398;&#20064;&#65292;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#29983;&#25104;&#27169;&#22411;&#65292;GFlowNets&#21644;&#36951;&#20256;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel variant of GFlowNet, genetic-guided GFlowNet (Genetic GFN), which integrates an iterative genetic search into GFlowNet. Genetic search effectively guides the GFlowNet to high-rewarded regions, addressing global over-exploration that results in training inefficiency and exploring limited regions. In addition, training strategies, such as rank-based replay training and unsupervised maximum likelihood pre-training, are further introduced to improve the sample efficiency of Genetic GFN. The proposed method shows a state-of-the-art score of 16.213, significantly outperforming the reported best score in the benchmark of 15.185, in practical molecular optimization (PMO), which is an official benchmark for sample-efficient molecular optimization. Remarkably, ours exceeds all baselines, including reinforcement learning, Bayesian optimization, generative models, GFlowNets, and genetic algorithms, in 14 out of 23 tasks.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#38598;&#21512;&#21270;&#22320;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#32534;&#30721;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#36880;&#23618;&#32534;&#30721;&#26041;&#26696;&#26469;&#32771;&#34385;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#23618;&#35745;&#31639;&#32467;&#26500;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#8220;pad-chunk-encode&#8221;&#27969;&#27700;&#32447;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#39640;&#25928;&#32534;&#30721;&#22788;&#29702;&#65292;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#24615;&#33021;&#39044;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.16625</link><description>&lt;p&gt;
&#38598;&#21512;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Set-based Neural Network Encoding. (arXiv:2305.16625v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16625
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#38598;&#21512;&#21270;&#22320;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#32534;&#30721;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#36880;&#23618;&#32534;&#30721;&#26041;&#26696;&#26469;&#32771;&#34385;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#23618;&#35745;&#31639;&#32467;&#26500;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#8220;pad-chunk-encode&#8221;&#27969;&#27700;&#32447;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#39640;&#25928;&#32534;&#30721;&#22788;&#29702;&#65292;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#24615;&#33021;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38598;&#21512;&#21040;&#38598;&#21512;&#21644;&#38598;&#21512;&#21040;&#21521;&#37327;&#20989;&#25968;&#26469;&#26377;&#25928;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#65292;&#36827;&#34892;&#27867;&#21270;&#24615;&#33021;&#39044;&#27979;&#30340;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#32534;&#30721;&#26041;&#27861;&#12290;&#19982;&#20043;&#21069;&#38656;&#35201;&#23545;&#19981;&#21516;&#26550;&#26500;&#32534;&#20889;&#33258;&#23450;&#20041;&#32534;&#30721;&#27169;&#22411;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#23545;&#28151;&#21512;&#26550;&#26500;&#21644;&#19981;&#21516;&#21442;&#25968;&#22823;&#23567;&#30340;&#27169;&#22411;&#21160;&#24577;&#32534;&#30721;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340; SNE&#65288;&#38598;&#21512;&#21270;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#22120;&#65289;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#36880;&#23618;&#32534;&#30721;&#26041;&#26696;&#65292;&#32771;&#34385;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#23618;&#35745;&#31639;&#32467;&#26500;&#12290;&#26368;&#32456;&#23558;&#25152;&#26377;&#23618;&#27425;&#32534;&#30721;&#21512;&#24182;&#21040;&#19968;&#36215;&#65292;&#20197;&#33719;&#21462;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#30690;&#37327;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#8220;pad-chunk-encode&#8221;&#27969;&#27700;&#32447;&#26469;&#26377;&#25928;&#22320;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#35813;&#27969;&#27700;&#32447;&#21487;&#26681;&#25454;&#35745;&#31639;&#21644;&#20869;&#23384;&#38480;&#21046;&#36827;&#34892;&#35843;&#25972;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#20004;&#20010;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#24615;&#33021;&#39044;&#27979;&#30340;&#26032;&#20219;&#21153;&#65306;&#36328;&#25968;&#25454;&#38598;&#21644;&#26550;&#26500;&#36866;&#24212;&#24615;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an approach to neural network weight encoding for generalization performance prediction that utilizes set-to-set and set-to-vector functions to efficiently encode neural network parameters. Our approach is capable of encoding neural networks in a modelzoo of mixed architecture and different parameter sizes as opposed to previous approaches that require custom encoding models for different architectures. Furthermore, our \textbf{S}et-based \textbf{N}eural network \textbf{E}ncoder (SNE) takes into consideration the hierarchical computational structure of neural networks by utilizing a layer-wise encoding scheme that culminates to encoding all layer-wise encodings to obtain the neural network encoding vector. Additionally, we introduce a \textit{pad-chunk-encode} pipeline to efficiently encode neural network layers that is adjustable to computational and memory constraints. We also introduce two new tasks for neural network generalization performance prediction: cross-dataset a
&lt;/p&gt;</description></item></channel></rss>