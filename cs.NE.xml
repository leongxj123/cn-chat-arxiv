<rss version="2.0"><channel><title>Chat Arxiv cs.NE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NE</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#38598;&#21512;&#21270;&#22320;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#32534;&#30721;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#36880;&#23618;&#32534;&#30721;&#26041;&#26696;&#26469;&#32771;&#34385;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#23618;&#35745;&#31639;&#32467;&#26500;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#8220;pad-chunk-encode&#8221;&#27969;&#27700;&#32447;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#39640;&#25928;&#32534;&#30721;&#22788;&#29702;&#65292;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#24615;&#33021;&#39044;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.16625</link><description>&lt;p&gt;
&#38598;&#21512;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Set-based Neural Network Encoding. (arXiv:2305.16625v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16625
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#38598;&#21512;&#21270;&#22320;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#32534;&#30721;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#36880;&#23618;&#32534;&#30721;&#26041;&#26696;&#26469;&#32771;&#34385;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#23618;&#35745;&#31639;&#32467;&#26500;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#8220;pad-chunk-encode&#8221;&#27969;&#27700;&#32447;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#39640;&#25928;&#32534;&#30721;&#22788;&#29702;&#65292;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#24615;&#33021;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38598;&#21512;&#21040;&#38598;&#21512;&#21644;&#38598;&#21512;&#21040;&#21521;&#37327;&#20989;&#25968;&#26469;&#26377;&#25928;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#65292;&#36827;&#34892;&#27867;&#21270;&#24615;&#33021;&#39044;&#27979;&#30340;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#32534;&#30721;&#26041;&#27861;&#12290;&#19982;&#20043;&#21069;&#38656;&#35201;&#23545;&#19981;&#21516;&#26550;&#26500;&#32534;&#20889;&#33258;&#23450;&#20041;&#32534;&#30721;&#27169;&#22411;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#23545;&#28151;&#21512;&#26550;&#26500;&#21644;&#19981;&#21516;&#21442;&#25968;&#22823;&#23567;&#30340;&#27169;&#22411;&#21160;&#24577;&#32534;&#30721;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340; SNE&#65288;&#38598;&#21512;&#21270;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#22120;&#65289;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#36880;&#23618;&#32534;&#30721;&#26041;&#26696;&#65292;&#32771;&#34385;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#23618;&#35745;&#31639;&#32467;&#26500;&#12290;&#26368;&#32456;&#23558;&#25152;&#26377;&#23618;&#27425;&#32534;&#30721;&#21512;&#24182;&#21040;&#19968;&#36215;&#65292;&#20197;&#33719;&#21462;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#30690;&#37327;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#8220;pad-chunk-encode&#8221;&#27969;&#27700;&#32447;&#26469;&#26377;&#25928;&#22320;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#35813;&#27969;&#27700;&#32447;&#21487;&#26681;&#25454;&#35745;&#31639;&#21644;&#20869;&#23384;&#38480;&#21046;&#36827;&#34892;&#35843;&#25972;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#20004;&#20010;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#24615;&#33021;&#39044;&#27979;&#30340;&#26032;&#20219;&#21153;&#65306;&#36328;&#25968;&#25454;&#38598;&#21644;&#26550;&#26500;&#36866;&#24212;&#24615;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an approach to neural network weight encoding for generalization performance prediction that utilizes set-to-set and set-to-vector functions to efficiently encode neural network parameters. Our approach is capable of encoding neural networks in a modelzoo of mixed architecture and different parameter sizes as opposed to previous approaches that require custom encoding models for different architectures. Furthermore, our \textbf{S}et-based \textbf{N}eural network \textbf{E}ncoder (SNE) takes into consideration the hierarchical computational structure of neural networks by utilizing a layer-wise encoding scheme that culminates to encoding all layer-wise encodings to obtain the neural network encoding vector. Additionally, we introduce a \textit{pad-chunk-encode} pipeline to efficiently encode neural network layers that is adjustable to computational and memory constraints. We also introduce two new tasks for neural network generalization performance prediction: cross-dataset a
&lt;/p&gt;</description></item></channel></rss>