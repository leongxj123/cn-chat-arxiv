<rss version="2.0"><channel><title>Chat Arxiv cs.NE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NE</description><item><title>&#25972;&#21512;&#27491;&#24358;&#20989;&#25968;&#21040;&#20302;&#31209;&#20998;&#35299;&#36807;&#31243;&#20013;&#65292;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20445;&#25345;&#21442;&#25968;&#39640;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.19243</link><description>&lt;p&gt;
&#29992;&#27491;&#24358;&#28608;&#27963;&#30340;&#20302;&#31209;&#30697;&#38453;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sine Activated Low-Rank Matrices for Parameter Efficient Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19243
&lt;/p&gt;
&lt;p&gt;
&#25972;&#21512;&#27491;&#24358;&#20989;&#25968;&#21040;&#20302;&#31209;&#20998;&#35299;&#36807;&#31243;&#20013;&#65292;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20445;&#25345;&#21442;&#25968;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#20998;&#35299;&#24050;&#32463;&#25104;&#20026;&#22312;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#22686;&#24378;&#21442;&#25968;&#25928;&#29575;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#36825;&#20123;&#25216;&#26415;&#26174;&#33879;&#38477;&#20302;&#20102;&#21442;&#25968;&#25968;&#37327;&#65292;&#21462;&#24471;&#20102;&#31616;&#27905;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#24120;&#35265;&#30340;&#25361;&#25112;&#26159;&#22312;&#21442;&#25968;&#25928;&#29575;&#21644;&#27169;&#22411;&#20934;&#30830;&#24615;&#20043;&#38388;&#20570;&#20986;&#22949;&#21327;&#65292;&#21442;&#25968;&#20943;&#23569;&#24448;&#24448;&#23548;&#33268;&#20934;&#30830;&#24615;&#19981;&#21450;&#23436;&#25972;&#31209;&#23545;&#24212;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#22312;&#20302;&#31209;&#20998;&#35299;&#36807;&#31243;&#20013;&#25972;&#21512;&#20102;&#19968;&#20010;&#27491;&#24358;&#20989;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#20445;&#30041;&#20102;&#20302;&#31209;&#26041;&#27861;&#30340;&#21442;&#25968;&#25928;&#29575;&#29305;&#24615;&#30340;&#22909;&#22788;&#65292;&#36824;&#22686;&#21152;&#20102;&#20998;&#35299;&#30340;&#31209;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#35777;&#26126;&#26159;&#29616;&#26377;&#20302;&#31209;&#27169;&#22411;&#30340;&#19968;&#31181;&#36866;&#24212;&#24615;&#22686;&#24378;&#65292;&#27491;&#22914;&#20854;&#25104;&#21151;&#35777;&#23454;&#30340;&#37027;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19243v1 Announce Type: new  Abstract: Low-rank decomposition has emerged as a vital tool for enhancing parameter efficiency in neural network architectures, gaining traction across diverse applications in machine learning. These techniques significantly lower the number of parameters, striking a balance between compactness and performance. However, a common challenge has been the compromise between parameter efficiency and the accuracy of the model, where reduced parameters often lead to diminished accuracy compared to their full-rank counterparts. In this work, we propose a novel theoretical framework that integrates a sinusoidal function within the low-rank decomposition process. This approach not only preserves the benefits of the parameter efficiency characteristic of low-rank methods but also increases the decomposition's rank, thereby enhancing model accuracy. Our method proves to be an adaptable enhancement for existing low-rank models, as evidenced by its successful 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#29983;&#29289;&#21551;&#21457;&#24335;Hebbian&#23398;&#20064;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#21033;&#29992;&#19981;&#21516;&#30340;&#19981;&#21464;&#35270;&#35273;&#25551;&#36848;&#31526;&#20316;&#20026;&#24402;&#32435;&#20559;&#35265;&#12290;&#35813;&#26694;&#26550;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#36739;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.08603</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#35299;&#32534;&#30721;&#22120;&#35774;&#35745;&#30340;&#29983;&#29289;&#21551;&#21457;&#24335;Hebbian&#23398;&#20064;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning in a Decomposed Encoder Design for Bio-inspired Hebbian Learning. (arXiv:2401.08603v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08603
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#29983;&#29289;&#21551;&#21457;&#24335;Hebbian&#23398;&#20064;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#21033;&#29992;&#19981;&#21516;&#30340;&#19981;&#21464;&#35270;&#35273;&#25551;&#36848;&#31526;&#20316;&#20026;&#24402;&#32435;&#20559;&#35265;&#12290;&#35813;&#26694;&#26550;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#36739;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#35774;&#35745;&#21033;&#29992;&#20102;&#23545;&#26550;&#26500;&#32467;&#26500;&#30340;&#24402;&#32435;&#20559;&#35265;&#12289;&#19981;&#21464;&#24615;&#21644;&#31561;&#21464;&#24615;&#35201;&#27714;&#12289;&#20219;&#21153;&#29305;&#23450;&#30340;&#25439;&#22833;&#20989;&#25968;&#20197;&#21450;&#35745;&#31639;&#20248;&#21270;&#24037;&#20855;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#32534;&#30721;&#22120;&#30340;&#26089;&#26399;&#23618;&#20013;&#30340;&#24402;&#32435;&#20559;&#35265;&#65292;&#20197;&#20154;&#20026;&#25351;&#23450;&#30340;&#20934;&#19981;&#21464;&#28388;&#27874;&#22120;&#30340;&#24418;&#24335;&#65292;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24402;&#32435;&#20559;&#35265;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;&#26412;&#25991;&#22312;&#29983;&#29289;&#21551;&#21457;&#24335;Hebbian&#23398;&#20064;&#30340;&#34920;&#31034;&#23398;&#20064;&#19978;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#20351;&#29992;&#29983;&#29289;&#21551;&#21457;&#24335;&#30340;&#23545;&#27604;&#39044;&#27979;&#32534;&#30721;&#65288;Hinge CLAPP Loss&#65289;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30001;&#22810;&#20010;&#24182;&#34892;&#32534;&#30721;&#22120;&#32452;&#25104;&#65292;&#27599;&#20010;&#32534;&#30721;&#22120;&#21033;&#29992;&#19981;&#21516;&#30340;&#19981;&#21464;&#35270;&#35273;&#25551;&#36848;&#31526;&#20316;&#20026;&#24402;&#32435;&#20559;&#35265;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#38590;&#24230;&#30340;&#22270;&#20687;&#25968;&#25454;&#19978;&#30340;&#20998;&#31867;&#22330;&#26223;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#31995;&#32479;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#65288;GTSRB, STL10, CODEBR&#65289;
&lt;/p&gt;
&lt;p&gt;
Modern data-driven machine learning system designs exploit inductive biases on architectural structure, invariance and equivariance requirements, task specific loss functions, and computational optimization tools. Previous works have illustrated that inductive bias in the early layers of the encoder in the form of human specified quasi-invariant filters can serve as a powerful inductive bias to attain better robustness and transparency in learned classifiers. This paper explores this further in the context of representation learning with local plasticity rules i.e. bio-inspired Hebbian learning . We propose a modular framework trained with a bio-inspired variant of contrastive predictive coding (Hinge CLAPP Loss). Our framework is composed of parallel encoders each leveraging a different invariant visual descriptor as an inductive bias. We evaluate the representation learning capacity of our system in a classification scenario on image data of various difficulties (GTSRB, STL10, CODEBR
&lt;/p&gt;</description></item></channel></rss>