<rss version="2.0"><channel><title>Chat Arxiv cs.NE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NE</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24490;&#29615;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#27169;&#22359;&#65292;&#36890;&#36807;&#23558;&#33033;&#20914;&#31070;&#32463;&#20803;&#30340;&#31070;&#32463;&#21160;&#21147;&#23398;&#19982;&#29702;&#24819;&#30340;&#26102;&#38388;&#20107;&#20214;&#37319;&#26679;&#22120;&#30340;&#34892;&#20026;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#30340;&#20107;&#20214;&#26816;&#27979;&#26694;&#26550;</title><link>https://arxiv.org/abs/2403.12574</link><description>&lt;p&gt;
EAS-SNN&#65306;&#31471;&#21040;&#31471;&#33258;&#36866;&#24212;&#37319;&#26679;&#21644;&#34920;&#31034;&#65292;&#29992;&#20110;&#24490;&#29615;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#20107;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
EAS-SNN: End-to-End Adaptive Sampling and Representation for Event-based Detection with Recurrent Spiking Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12574
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24490;&#29615;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#27169;&#22359;&#65292;&#36890;&#36807;&#23558;&#33033;&#20914;&#31070;&#32463;&#20803;&#30340;&#31070;&#32463;&#21160;&#21147;&#23398;&#19982;&#29702;&#24819;&#30340;&#26102;&#38388;&#20107;&#20214;&#37319;&#26679;&#22120;&#30340;&#34892;&#20026;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#30340;&#20107;&#20214;&#26816;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#25668;&#20687;&#22836;&#20197;&#20854;&#39640;&#21160;&#24577;&#33539;&#22260;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#29289;&#20307;&#26816;&#27979;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#21160;&#24577;&#27169;&#31946;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20809;&#29031;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#26356;&#27880;&#37325;&#20248;&#21270;&#20855;&#26377;&#20808;&#36827;&#26816;&#27979;&#39592;&#24178;&#21644;&#26089;&#26399;&#32858;&#21512;&#21151;&#33021;&#30340;&#26102;&#31354;&#34920;&#31034;&#65292;&#32780;&#33258;&#36866;&#24212;&#20107;&#20214;&#37319;&#26679;&#30340;&#20851;&#38190;&#38382;&#39064;&#20173;&#26410;&#24471;&#21040;&#35299;&#20915;&#12290;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#65292;&#36890;&#36807;&#31232;&#30095;&#33033;&#20914;&#36890;&#20449;&#36816;&#34892;&#30340;&#20107;&#20214;&#39537;&#21160;&#33539;&#24335;&#65292;&#25104;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#22825;&#28982;&#36873;&#25321;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#33033;&#20914;&#31070;&#32463;&#20803;&#30340;&#31070;&#32463;&#21160;&#21147;&#23398;&#19982;&#29702;&#24819;&#30340;&#26102;&#38388;&#20107;&#20214;&#37319;&#26679;&#22120;&#30340;&#34892;&#20026;&#23494;&#20999;&#30456;&#31526;&#12290;&#22312;&#36825;&#19968;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#27169;&#22359;&#65292;&#21033;&#29992;&#20855;&#26377;&#26102;&#38388;&#35760;&#24518;&#30340;&#24490;&#29615;&#21367;&#31215;SNN&#22686;&#24378;&#65292;&#20026;&#22522;&#20110;&#20107;&#20214;&#26816;&#27979;&#30340;&#23436;&#20840;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#26694;&#26550;&#25552;&#20379;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12574v1 Announce Type: cross  Abstract: Event cameras, with their high dynamic range and temporal resolution, are ideally suited for object detection, especially under scenarios with motion blur and challenging lighting conditions. However, while most existing approaches prioritize optimizing spatiotemporal representations with advanced detection backbones and early aggregation functions, the crucial issue of adaptive event sampling remains largely unaddressed. Spiking Neural Networks (SNNs), which operate on an event-driven paradigm through sparse spike communication, emerge as a natural fit for addressing this challenge. In this study, we discover that the neural dynamics of spiking neurons align closely with the behavior of an ideal temporal event sampler. Motivated by this insight, we propose a novel adaptive sampling module that leverages recurrent convolutional SNNs enhanced with temporal memory, facilitating a fully end-to-end learnable framework for event-based detec
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#22810;&#26679;&#24615;&#31995;&#25968;&#20316;&#20026;LLM&#39044;&#35757;&#32451;&#25968;&#25454;&#36136;&#37327;&#30340;&#25351;&#26631;&#65292;&#30740;&#31350;&#34920;&#26126;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#24456;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.13840</link><description>&lt;p&gt;
&#36229;&#36234;&#35268;&#27169;&#65306;&#22810;&#26679;&#24615;&#31995;&#25968;&#20316;&#20026;&#25968;&#25454;&#36136;&#37327;&#25351;&#26631;&#35777;&#26126;&#20102;LLMs&#26159;&#22312;&#24418;&#24335;&#22810;&#26679;&#30340;&#25968;&#25454;&#19978;&#39044;&#20808;&#35757;&#32451;&#30340;
&lt;/p&gt;
&lt;p&gt;
Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data. (arXiv:2306.13840v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#22810;&#26679;&#24615;&#31995;&#25968;&#20316;&#20026;LLM&#39044;&#35757;&#32451;&#25968;&#25454;&#36136;&#37327;&#30340;&#25351;&#26631;&#65292;&#30740;&#31350;&#34920;&#26126;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#24456;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#65292;&#39044;&#20808;&#35757;&#32451;&#24378;&#22823;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36235;&#21183;&#20027;&#35201;&#38598;&#20013;&#22312;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#25193;&#22823;&#12290;&#28982;&#32780;&#65292;&#39044;&#20808;&#35757;&#32451;&#25968;&#25454;&#30340;&#36136;&#37327;&#23545;&#20110;&#35757;&#32451;&#24378;&#22823;&#30340;LLMs&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#65292;&#20294;&#23427;&#26159;&#19968;&#20010;&#27169;&#31946;&#30340;&#27010;&#24565;&#65292;&#23578;&#26410;&#23436;&#20840;&#34920;&#24449;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#36817;&#25552;&#20986;&#30340;Task2Vec&#22810;&#26679;&#24615;&#31995;&#25968;&#26469;&#22522;&#20110;&#25968;&#25454;&#36136;&#37327;&#30340;&#24418;&#24335;&#26041;&#38754;&#65292;&#36229;&#36234;&#35268;&#27169;&#26412;&#36523;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#27979;&#37327;&#20844;&#24320;&#21487;&#29992;&#30340;&#39044;&#20808;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#65292;&#20197;&#35777;&#26126;&#23427;&#20204;&#30340;&#24418;&#24335;&#22810;&#26679;&#24615;&#39640;&#20110;&#29702;&#35770;&#30340;&#19979;&#38480;&#21644;&#19978;&#38480;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#24314;&#31435;&#23545;&#22810;&#26679;&#24615;&#31995;&#25968;&#30340;&#20449;&#24515;&#65292;&#25105;&#20204;&#36827;&#34892;&#21487;&#35299;&#37322;&#24615;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#35813;&#31995;&#25968;&#19982;&#22810;&#26679;&#24615;&#30340;&#30452;&#35266;&#23646;&#24615;&#30456;&#21563;&#21512;&#65292;&#20363;&#22914;&#65292;&#38543;&#30528;&#28508;&#22312;&#27010;&#24565;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#23427;&#22686;&#21152;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#22810;&#26679;&#24615;&#31995;&#25968;&#26159;&#21487;&#38752;&#30340;&#65292;&#34920;&#26126;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#24456;&#39640;&#65292;&#24182;&#25512;&#27979;&#23427;&#21487;&#20197;&#20316;&#20026;&#39044;&#35757;&#32451;LLMs&#27169;&#22411;&#30340;&#25968;&#25454;&#36136;&#37327;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current trends to pre-train capable Large Language Models (LLMs) mostly focus on scaling of model and dataset size. However, the quality of pre-training data is an important factor for training powerful LLMs, yet it is a nebulous concept that has not been fully characterized. Therefore, we use the recently proposed Task2Vec diversity coefficient to ground and understand formal aspects of data quality, to go beyond scale alone. Specifically, we measure the diversity coefficient of publicly available pre-training datasets to demonstrate that their formal diversity is high when compared to theoretical lower and upper bounds. In addition, to build confidence in the diversity coefficient, we conduct interpretability experiments and find that the coefficient aligns with intuitive properties of diversity, e.g., it increases as the number of latent concepts increases. We conclude the diversity coefficient is reliable, show it's high for publicly available LLM datasets, and conjecture it can be
&lt;/p&gt;</description></item></channel></rss>