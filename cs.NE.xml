<rss version="2.0"><channel><title>Chat Arxiv cs.NE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NE</description><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20301;&#32622;&#32534;&#30721;&#22312;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#21363;&#20351;&#19982;RNN&#32467;&#21512;&#20351;&#29992;&#65292;&#20301;&#32622;&#32534;&#30721;&#20173;&#28982;&#26377;&#25928;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#22823;&#35789;&#27719;&#37327;&#21644;&#22810;&#26679;&#35266;&#23519;&#32467;&#26524;&#26102;&#12290;&#36825;&#20026;&#20351;&#29992;&#36755;&#20837;&#39537;&#21160;&#21644;&#33258;&#20027;&#26102;&#38388;&#34920;&#31034;&#30340;&#32452;&#21512;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#65292;&#21516;&#26102;&#30740;&#31350;&#32467;&#26524;&#20063;&#23545;&#31070;&#32463;&#20803;&#25391;&#33633;&#30340;&#29983;&#29289;&#23398;&#24847;&#20041;&#25552;&#20379;&#20102;&#35752;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.00236</link><description>&lt;p&gt;
&#20301;&#32622;&#32534;&#30721;&#26377;&#21161;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#22823;&#35789;&#27719;&#37327;
&lt;/p&gt;
&lt;p&gt;
Positional Encoding Helps Recurrent Neural Networks Handle a Large Vocabulary
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20301;&#32622;&#32534;&#30721;&#22312;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#21363;&#20351;&#19982;RNN&#32467;&#21512;&#20351;&#29992;&#65292;&#20301;&#32622;&#32534;&#30721;&#20173;&#28982;&#26377;&#25928;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#22823;&#35789;&#27719;&#37327;&#21644;&#22810;&#26679;&#35266;&#23519;&#32467;&#26524;&#26102;&#12290;&#36825;&#20026;&#20351;&#29992;&#36755;&#20837;&#39537;&#21160;&#21644;&#33258;&#20027;&#26102;&#38388;&#34920;&#31034;&#30340;&#32452;&#21512;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#65292;&#21516;&#26102;&#30740;&#31350;&#32467;&#26524;&#20063;&#23545;&#31070;&#32463;&#20803;&#25391;&#33633;&#30340;&#29983;&#29289;&#23398;&#24847;&#20041;&#25552;&#20379;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#20301;&#32622;&#32534;&#30721;&#22312;&#21033;&#29992;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#20013;&#30340;&#24433;&#21709;&#12290;&#20301;&#32622;&#32534;&#30721;&#23558;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#25968;&#25454;&#28857;&#8220;&#26102;&#38388;&#25139;&#21270;&#8221;&#65292;&#24182;&#34917;&#20805;&#20102;Transformer&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#21147;&#65292;&#21518;&#32773;&#32570;&#20047;&#34920;&#31034;&#25968;&#25454;&#39034;&#24207;&#30340;&#20869;&#22312;&#26426;&#21046;&#12290;&#30456;&#21453;&#65292;RNN&#21487;&#20197;&#33258;&#24049;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#26102;&#38388;&#32534;&#30721;&#65292;&#20351;&#24471;&#23427;&#20204;&#23545;&#20301;&#32622;&#32534;&#30721;&#30340;&#20351;&#29992;&#20284;&#20046;&#26159;&#8220;&#20887;&#20313;&#8221;&#30340;&#12290;&#28982;&#32780;&#65292;&#32463;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#19982;RNN&#32467;&#21512;&#20351;&#29992;&#65292;&#20301;&#32622;&#32534;&#30721;&#30340;&#26377;&#25928;&#24615;&#20173;&#28982;&#24456;&#39640;&#65292;&#29305;&#21035;&#26159;&#29992;&#20110;&#22788;&#29702;&#20135;&#29983;&#22810;&#26679;&#35266;&#23519;&#32467;&#26524;&#30340;&#22823;&#35789;&#27719;&#37327;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#28041;&#21450;&#36755;&#20837;&#39537;&#21160;&#21644;&#33258;&#20027;&#26102;&#38388;&#34920;&#31034;&#30340;&#32452;&#21512;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;&#35745;&#31639;/&#27169;&#25311;&#32467;&#26524;&#30340;&#29983;&#29289;&#23398;&#24847;&#20041;&#65292;&#32771;&#34385;&#21040;&#20301;&#32622;&#32534;&#30721;&#30340;&#27491;&#24358;&#23454;&#29616;&#19982;&#31070;&#32463;&#20803;&#25391;&#33633;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study discusses the effects of positional encoding on recurrent neural networks (RNNs) utilizing synthetic benchmarks. Positional encoding "time-stamps" data points in time series and complements the capabilities of Transformer neural networks, which lack an inherent mechanism for representing the data order. By contrast, RNNs can encode the temporal information of data points on their own, rendering their use of positional encoding seemingly "redundant". Nonetheless, empirical investigations reveal the effectiveness of positional encoding even when coupled with RNNs, specifically for handling a large vocabulary that yields diverse observations. These findings pave the way for a new line of research on RNNs, concerning the combination of input-driven and autonomous time representation. Additionally, biological implications of the computational/simulational results are discussed, in the light of the affinity between the sinusoidal implementation of positional encoding and neural os
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#22810;&#27169;&#24577;&#20248;&#21270;Big Bang-Big Crunch&#31639;&#27861;&#30340;&#29256;&#26412;&#65292;&#31216;&#20026;k-BBBC&#12290;&#35813;&#31639;&#27861;&#33021;&#22815;&#39640;&#25928;&#22320;&#35299;&#20915;&#22810;&#27169;&#24577;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06153</link><description>&lt;p&gt;
&#22522;&#20110;k-&#32858;&#31867;Big Bang-Big Crunch&#31639;&#27861;&#30340;&#22810;&#27169;&#24577;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multi-Modal Optimization with k-Cluster Big Bang-Big Crunch Algorithm. (arXiv:2401.06153v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06153
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#22810;&#27169;&#24577;&#20248;&#21270;Big Bang-Big Crunch&#31639;&#27861;&#30340;&#29256;&#26412;&#65292;&#31216;&#20026;k-BBBC&#12290;&#35813;&#31639;&#27861;&#33021;&#22815;&#39640;&#25928;&#22320;&#35299;&#20915;&#22810;&#27169;&#24577;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#20248;&#21270;&#32463;&#24120;&#22312;&#24037;&#31243;&#38382;&#39064;&#20013;&#36935;&#21040;&#65292;&#29305;&#21035;&#26159;&#22312;&#23547;&#25214;&#19981;&#21516;&#21644;&#26367;&#20195;&#35299;&#20915;&#26041;&#26696;&#26102;&#12290;&#36827;&#21270;&#31639;&#27861;&#36890;&#36807;&#31181;&#32676;&#30340;&#27010;&#24565;&#12289;&#25506;&#32034;/&#24320;&#21457;&#21151;&#33021;&#21644;&#36866;&#21512;&#24182;&#34892;&#35745;&#31639;&#31561;&#29305;&#28857;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#35299;&#20915;&#22810;&#27169;&#24577;&#20248;&#21270;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#22810;&#27169;&#24577;&#20248;&#21270;Big Bang-Big Crunch&#31639;&#27861;&#30340;&#29256;&#26412;&#65292;&#31216;&#20026;k-BBBC&#12290;&#35813;&#31639;&#27861;&#33021;&#22815;&#20445;&#35777;&#25972;&#20010;&#31181;&#32676;&#30340;&#23436;&#20840;&#25910;&#25947;&#65292;&#23545;&#20110;&#29305;&#23450;&#38382;&#39064;&#24179;&#22343;&#26816;&#32034;&#21040;99\%&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;(i)&#22312;&#19968;&#32452;&#26816;&#32034;&#21040;&#30340;&#35299;&#20915;&#26041;&#26696;&#20013;&#30830;&#23450;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#20197;&#21450;(ii)&#23450;&#37327;&#27979;&#37327;&#27491;&#30830;&#26816;&#32034;&#21040;&#30340;&#26368;&#20248;&#35299;&#25968;&#37327;&#19982;&#39044;&#26399;&#25968;&#37327;&#20043;&#38388;&#30340;&#27604;&#29575;&#65288;&#21363;&#25104;&#21151;&#29575;&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;k-BBBC&#22312;&#20855;&#26377;&#22823;&#37327;&#26368;&#20248;&#35299;&#65288;&#27979;&#35797;&#20102;379&#20010;&#26368;&#20248;&#35299;&#65289;&#21644;&#39640;&#32500;&#24230;&#30340;&#38382;&#39064;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal optimization is often encountered in engineering problems, especially when different and alternative solutions are sought. Evolutionary algorithms can efficiently tackle multi-modal optimization thanks to their features such as the concept of population, exploration/exploitation, and being suitable for parallel computation.  This paper introduces a multi-modal optimization version of the Big Bang-Big Crunch algorithm based on clustering, namely, k-BBBC. This algorithm guarantees a complete convergence of the entire population, retrieving on average the 99\% of local optima for a specific problem. Additionally, we introduce two post-processing methods to (i) identify the local optima in a set of retrieved solutions (i.e., a population), and (ii) quantify the number of correctly retrieved optima against the expected ones (i.e., success rate).  Our results show that k-BBBC performs well even with problems having a large number of optima (tested on 379 optima) and high dimensio
&lt;/p&gt;</description></item></channel></rss>