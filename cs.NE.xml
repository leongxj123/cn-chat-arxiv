<rss version="2.0"><channel><title>Chat Arxiv cs.NE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NE</description><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20301;&#32622;&#32534;&#30721;&#22312;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#21363;&#20351;&#19982;RNN&#32467;&#21512;&#20351;&#29992;&#65292;&#20301;&#32622;&#32534;&#30721;&#20173;&#28982;&#26377;&#25928;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#22823;&#35789;&#27719;&#37327;&#21644;&#22810;&#26679;&#35266;&#23519;&#32467;&#26524;&#26102;&#12290;&#36825;&#20026;&#20351;&#29992;&#36755;&#20837;&#39537;&#21160;&#21644;&#33258;&#20027;&#26102;&#38388;&#34920;&#31034;&#30340;&#32452;&#21512;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#65292;&#21516;&#26102;&#30740;&#31350;&#32467;&#26524;&#20063;&#23545;&#31070;&#32463;&#20803;&#25391;&#33633;&#30340;&#29983;&#29289;&#23398;&#24847;&#20041;&#25552;&#20379;&#20102;&#35752;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.00236</link><description>&lt;p&gt;
&#20301;&#32622;&#32534;&#30721;&#26377;&#21161;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#22823;&#35789;&#27719;&#37327;
&lt;/p&gt;
&lt;p&gt;
Positional Encoding Helps Recurrent Neural Networks Handle a Large Vocabulary
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20301;&#32622;&#32534;&#30721;&#22312;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#21363;&#20351;&#19982;RNN&#32467;&#21512;&#20351;&#29992;&#65292;&#20301;&#32622;&#32534;&#30721;&#20173;&#28982;&#26377;&#25928;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#22823;&#35789;&#27719;&#37327;&#21644;&#22810;&#26679;&#35266;&#23519;&#32467;&#26524;&#26102;&#12290;&#36825;&#20026;&#20351;&#29992;&#36755;&#20837;&#39537;&#21160;&#21644;&#33258;&#20027;&#26102;&#38388;&#34920;&#31034;&#30340;&#32452;&#21512;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#65292;&#21516;&#26102;&#30740;&#31350;&#32467;&#26524;&#20063;&#23545;&#31070;&#32463;&#20803;&#25391;&#33633;&#30340;&#29983;&#29289;&#23398;&#24847;&#20041;&#25552;&#20379;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#20301;&#32622;&#32534;&#30721;&#22312;&#21033;&#29992;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#20013;&#30340;&#24433;&#21709;&#12290;&#20301;&#32622;&#32534;&#30721;&#23558;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#25968;&#25454;&#28857;&#8220;&#26102;&#38388;&#25139;&#21270;&#8221;&#65292;&#24182;&#34917;&#20805;&#20102;Transformer&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#21147;&#65292;&#21518;&#32773;&#32570;&#20047;&#34920;&#31034;&#25968;&#25454;&#39034;&#24207;&#30340;&#20869;&#22312;&#26426;&#21046;&#12290;&#30456;&#21453;&#65292;RNN&#21487;&#20197;&#33258;&#24049;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#26102;&#38388;&#32534;&#30721;&#65292;&#20351;&#24471;&#23427;&#20204;&#23545;&#20301;&#32622;&#32534;&#30721;&#30340;&#20351;&#29992;&#20284;&#20046;&#26159;&#8220;&#20887;&#20313;&#8221;&#30340;&#12290;&#28982;&#32780;&#65292;&#32463;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#19982;RNN&#32467;&#21512;&#20351;&#29992;&#65292;&#20301;&#32622;&#32534;&#30721;&#30340;&#26377;&#25928;&#24615;&#20173;&#28982;&#24456;&#39640;&#65292;&#29305;&#21035;&#26159;&#29992;&#20110;&#22788;&#29702;&#20135;&#29983;&#22810;&#26679;&#35266;&#23519;&#32467;&#26524;&#30340;&#22823;&#35789;&#27719;&#37327;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#28041;&#21450;&#36755;&#20837;&#39537;&#21160;&#21644;&#33258;&#20027;&#26102;&#38388;&#34920;&#31034;&#30340;&#32452;&#21512;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;&#35745;&#31639;/&#27169;&#25311;&#32467;&#26524;&#30340;&#29983;&#29289;&#23398;&#24847;&#20041;&#65292;&#32771;&#34385;&#21040;&#20301;&#32622;&#32534;&#30721;&#30340;&#27491;&#24358;&#23454;&#29616;&#19982;&#31070;&#32463;&#20803;&#25391;&#33633;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study discusses the effects of positional encoding on recurrent neural networks (RNNs) utilizing synthetic benchmarks. Positional encoding "time-stamps" data points in time series and complements the capabilities of Transformer neural networks, which lack an inherent mechanism for representing the data order. By contrast, RNNs can encode the temporal information of data points on their own, rendering their use of positional encoding seemingly "redundant". Nonetheless, empirical investigations reveal the effectiveness of positional encoding even when coupled with RNNs, specifically for handling a large vocabulary that yields diverse observations. These findings pave the way for a new line of research on RNNs, concerning the combination of input-driven and autonomous time representation. Additionally, biological implications of the computational/simulational results are discussed, in the light of the affinity between the sinusoidal implementation of positional encoding and neural os
&lt;/p&gt;</description></item></channel></rss>