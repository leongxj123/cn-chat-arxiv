<rss version="2.0"><channel><title>Chat Arxiv cs.NE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NE</description><item><title>&#20803;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33719;&#21462;&#21487;&#36716;&#31227;&#30693;&#35782;&#23454;&#29616;&#22312;&#21508;&#31181;&#20219;&#21153;&#20043;&#38388;&#24555;&#36895;&#36866;&#24212;&#65292;&#20026;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#38754;&#23545;&#20998;&#24067;&#21464;&#21270;&#21644;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#26102;&#27867;&#21270;&#33021;&#21147;&#19981;&#20339;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2404.02785</link><description>&lt;p&gt;
&#36890;&#36807;&#20803;&#23398;&#20064;&#23454;&#29616;&#39046;&#22495;&#27867;&#21270;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Domain Generalization through Meta-Learning: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02785
&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33719;&#21462;&#21487;&#36716;&#31227;&#30693;&#35782;&#23454;&#29616;&#22312;&#21508;&#31181;&#20219;&#21153;&#20043;&#38388;&#24555;&#36895;&#36866;&#24212;&#65292;&#20026;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#38754;&#23545;&#20998;&#24067;&#21464;&#21270;&#21644;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#26102;&#27867;&#21270;&#33021;&#21147;&#19981;&#20339;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#65292;&#20294;&#26159;&#24403;&#38754;&#23545;&#20998;&#24067;&#20043;&#22806;(out-of-distribution, OOD)&#25968;&#25454;&#26102;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#65292;&#36825;&#26159;&#22240;&#20026;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30001;&#20110;&#39046;&#22495;&#36716;&#31227;&#19981;&#21487;&#36991;&#20813;&#65292;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#34987;&#20551;&#23450;&#20026;&#20849;&#20139;&#30456;&#21516;&#20998;&#24067;&#30340;&#24120;&#35265;&#24773;&#20917;&#12290;&#23613;&#31649;DNNs&#22312;&#22823;&#37327;&#25968;&#25454;&#21644;&#35745;&#31639;&#33021;&#21147;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#24456;&#38590;&#24212;&#23545;&#20998;&#24067;&#21464;&#21270;&#21644;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#65292;&#23548;&#33268;&#36807;&#25311;&#21512;&#21644;&#36328;&#19981;&#21516;&#20219;&#21153;&#21644;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#19981;&#20339;&#12290;&#20803;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#33021;&#22815;&#22312;&#21508;&#31181;&#20219;&#21153;&#20043;&#38388;&#33719;&#21462;&#21487;&#36716;&#31227;&#30693;&#35782;&#30340;&#31639;&#27861;&#36827;&#34892;&#24555;&#36895;&#36866;&#24212;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#38656;&#35201;&#20174;&#22836;&#23398;&#20064;&#27599;&#20010;&#20219;&#21153;&#30340;&#24517;&#35201;&#24615;&#12290;&#26412;&#35843;&#26597;&#35770;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#20803;&#23398;&#20064;&#39046;&#22495;&#65292;&#37325;&#28857;&#20851;&#27880;&#20854;&#23545;&#39046;&#22495;&#27867;&#21270;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02785v1 Announce Type: cross  Abstract: Deep neural networks (DNNs) have revolutionized artificial intelligence but often lack performance when faced with out-of-distribution (OOD) data, a common scenario due to the inevitable domain shifts in real-world applications. This limitation stems from the common assumption that training and testing data share the same distribution-an assumption frequently violated in practice. Despite their effectiveness with large amounts of data and computational power, DNNs struggle with distributional shifts and limited labeled data, leading to overfitting and poor generalization across various tasks and domains. Meta-learning presents a promising approach by employing algorithms that acquire transferable knowledge across various tasks for fast adaptation, eliminating the need to learn each task from scratch. This survey paper delves into the realm of meta-learning with a focus on its contribution to domain generalization. We first clarify the 
&lt;/p&gt;</description></item></channel></rss>