<rss version="2.0"><channel><title>Chat Arxiv cs.NE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NE</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22686;&#24378;Transformer&#19982;&#24490;&#29615;&#26426;&#21046;&#30340;&#20004;&#31181;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25193;&#23637;&#21644;&#32452;&#21512;&#26041;&#27861;&#12290;&#22312;&#22810;&#20010;&#35786;&#26029;&#20219;&#21153;&#20013;&#36827;&#34892;&#27604;&#36739;&#65292;&#25506;&#32034;&#23427;&#20204;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00976</link><description>&lt;p&gt;
&#20855;&#26377;&#21160;&#24577;&#20572;&#27490;&#30340;&#24490;&#29615;Transformer
&lt;/p&gt;
&lt;p&gt;
Recurrent Transformers with Dynamic Halt
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22686;&#24378;Transformer&#19982;&#24490;&#29615;&#26426;&#21046;&#30340;&#20004;&#31181;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25193;&#23637;&#21644;&#32452;&#21512;&#26041;&#27861;&#12290;&#22312;&#22810;&#20010;&#35786;&#26029;&#20219;&#21153;&#20013;&#36827;&#34892;&#27604;&#36739;&#65292;&#25506;&#32034;&#23427;&#20204;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#20027;&#35201;&#26041;&#27861;&#22312;&#22686;&#24378;Transformer&#19982;&#24490;&#29615;&#26426;&#21046;&#26041;&#38754;&#30340;&#24402;&#32435;&#20559;&#22909;&#8212;&#8212;&#65288;1&#65289;&#31867;&#20284;&#20110;Universal Transformers&#30340;&#28145;&#24230;&#36880;&#23618;&#24490;&#29615;&#26041;&#27861;&#65307;&#21644;&#65288;2&#65289;&#31867;&#20284;&#20110;Temporal Latent Bottleneck&#30340;&#20998;&#22359;&#26102;&#24577;&#24490;&#29615;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#25193;&#23637;&#21644;&#32452;&#21512;&#19978;&#36848;&#26041;&#27861;&#30340;&#26032;&#26041;&#24335;&#65292;&#20363;&#22914;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#23616;&#22343;&#20540;&#30340;Universal Transformer&#21160;&#24577;&#20572;&#27490;&#26426;&#21046;&#65292;&#24182;&#23558;Universal Transformer&#30340;&#20803;&#32032;&#34701;&#20837;&#21040;Temporal Latent Bottleneck&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#20010;&#35786;&#26029;&#20219;&#21153;&#65288;&#22914;Long Range Arena&#65288;LRA&#65289;&#65292;&#32763;&#36716;-&#32763;&#36716;&#35821;&#35328;&#24314;&#27169;&#65292;ListOps&#21644;&#36923;&#36753;&#25512;&#29702;&#65289;&#27604;&#36739;&#20102;&#27169;&#22411;&#24182;&#25506;&#32034;&#20102;&#23427;&#20204;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the inductive biases of two major approaches to augmenting Transformers with a recurrent mechanism - (1) the approach of incorporating a depth-wise recurrence similar to Universal Transformers; and (2) the approach of incorporating a chunk-wise temporal recurrence like Temporal Latent Bottleneck. Furthermore, we propose and investigate novel ways to extend and combine the above methods - for example, we propose a global mean-based dynamic halting mechanism for Universal Transformer and an augmentation of Temporal Latent Bottleneck with elements from Universal Transformer. We compare the models and probe their inductive biases in several diagnostic tasks such as Long Range Arena (LRA), flip-flop language modeling, ListOps, and Logical Inference.
&lt;/p&gt;</description></item><item><title>Q-FOX&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;FOX&#20248;&#21270;&#22120;&#21644;Q-learning&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.16562</link><description>&lt;p&gt;
Q-FOX&#23398;&#20064;&#65306;&#39072;&#35206;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Q-FOX Learning: Breaking Tradition in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16562
&lt;/p&gt;
&lt;p&gt;
Q-FOX&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;FOX&#20248;&#21270;&#22120;&#21644;Q-learning&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26159;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#20195;&#29702;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#26469;&#23398;&#20064;&#26368;&#20339;&#21160;&#20316;&#65292;&#22240;&#27492;&#36866;&#29992;&#20110;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#25110;&#30452;&#25509;&#30417;&#30563;&#30340;&#20219;&#21153;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Q-FOX&#30340;&#26032;&#39062;&#33258;&#21160;&#35843;&#21442;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;FOX&#20248;&#21270;&#22120;&#21644;&#24120;&#29992;&#30340;&#26131;&#20110;&#23454;&#29616;&#30340;RL Q-learning&#31639;&#27861;&#35299;&#20915;&#20102;&#35843;&#21442;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#23558;&#22870;&#21169;&#25918;&#22312;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#21644;&#23398;&#20064;&#26102;&#38388;&#20043;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16562v2 Announce Type: replace-cross  Abstract: Reinforcement learning (RL) is a subset of artificial intelligence (AI) where agents learn the best action by interacting with the environment, making it suitable for tasks that do not require labeled data or direct supervision. Hyperparameters (HP) tuning refers to choosing the best parameter that leads to optimal solutions in RL algorithms. Manual or random tuning of the HP may be a crucial process because variations in this parameter lead to changes in the overall learning aspects and different rewards. In this paper, a novel and automatic HP-tuning method called Q-FOX is proposed. This uses both the FOX optimizer, a new optimization method inspired by nature that mimics red foxes' hunting behavior, and the commonly used, easy-to-implement RL Q-learning algorithm to solve the problem of HP tuning. Moreover, a new objective function is proposed which prioritizes the reward over the mean squared error (MSE) and learning time (
&lt;/p&gt;</description></item></channel></rss>