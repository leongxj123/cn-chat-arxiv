<rss version="2.0"><channel><title>Chat Arxiv cs.NE</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.NE</description><item><title>Q-FOX&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;FOX&#20248;&#21270;&#22120;&#21644;Q-learning&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.16562</link><description>&lt;p&gt;
Q-FOX&#23398;&#20064;&#65306;&#39072;&#35206;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Q-FOX Learning: Breaking Tradition in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16562
&lt;/p&gt;
&lt;p&gt;
Q-FOX&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;FOX&#20248;&#21270;&#22120;&#21644;Q-learning&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26159;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#20195;&#29702;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#26469;&#23398;&#20064;&#26368;&#20339;&#21160;&#20316;&#65292;&#22240;&#27492;&#36866;&#29992;&#20110;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#25110;&#30452;&#25509;&#30417;&#30563;&#30340;&#20219;&#21153;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Q-FOX&#30340;&#26032;&#39062;&#33258;&#21160;&#35843;&#21442;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;FOX&#20248;&#21270;&#22120;&#21644;&#24120;&#29992;&#30340;&#26131;&#20110;&#23454;&#29616;&#30340;RL Q-learning&#31639;&#27861;&#35299;&#20915;&#20102;&#35843;&#21442;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#23558;&#22870;&#21169;&#25918;&#22312;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#21644;&#23398;&#20064;&#26102;&#38388;&#20043;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16562v2 Announce Type: replace-cross  Abstract: Reinforcement learning (RL) is a subset of artificial intelligence (AI) where agents learn the best action by interacting with the environment, making it suitable for tasks that do not require labeled data or direct supervision. Hyperparameters (HP) tuning refers to choosing the best parameter that leads to optimal solutions in RL algorithms. Manual or random tuning of the HP may be a crucial process because variations in this parameter lead to changes in the overall learning aspects and different rewards. In this paper, a novel and automatic HP-tuning method called Q-FOX is proposed. This uses both the FOX optimizer, a new optimization method inspired by nature that mimics red foxes' hunting behavior, and the commonly used, easy-to-implement RL Q-learning algorithm to solve the problem of HP tuning. Moreover, a new objective function is proposed which prioritizes the reward over the mean squared error (MSE) and learning time (
&lt;/p&gt;</description></item><item><title>StochGradAdam&#26159;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#26799;&#24230;&#25277;&#26679;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#26799;&#24230;&#32771;&#34385;&#65292;&#33021;&#22815;&#31283;&#23450;&#25910;&#25947;&#65292;&#25552;&#21319;&#40065;&#26834;&#35757;&#32451;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.17042</link><description>&lt;p&gt;
StochGradAdam: &#21033;&#29992;&#38543;&#26426;&#26799;&#24230;&#25277;&#26679;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
StochGradAdam: Accelerating Neural Networks Training with Stochastic Gradient Sampling. (arXiv:2310.17042v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17042
&lt;/p&gt;
&lt;p&gt;
StochGradAdam&#26159;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#26799;&#24230;&#25277;&#26679;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#26799;&#24230;&#32771;&#34385;&#65292;&#33021;&#22815;&#31283;&#23450;&#25910;&#25947;&#65292;&#25552;&#21319;&#40065;&#26834;&#35757;&#32451;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#39046;&#22495;&#20013;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;StochGradAdam&#20248;&#21270;&#22120;&#65292;&#36825;&#26159;&#23545;&#24191;&#21463;&#36190;&#35465;&#30340;Adam&#31639;&#27861;&#30340;&#26032;&#39062;&#25913;&#36827;&#12290;StochGradAdam&#30340;&#26680;&#24515;&#26159;&#20854;&#26799;&#24230;&#25277;&#26679;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#30830;&#20445;&#31283;&#23450;&#25910;&#25947;&#65292;&#32780;&#19988;&#21033;&#29992;&#36873;&#25321;&#24615;&#26799;&#24230;&#32771;&#34385;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#20943;&#36731;&#22122;&#22768;&#25110;&#24322;&#24120;&#25968;&#25454;&#30340;&#24433;&#21709;&#21644;&#22686;&#24378;&#25439;&#22833;&#20989;&#25968;&#31354;&#38388;&#30340;&#25506;&#32034;&#65292;&#25552;&#21319;&#20102;&#40065;&#26834;&#35757;&#32451;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#20013;&#65292;StochGradAdam&#34920;&#29616;&#20986;&#20248;&#20110;&#20256;&#32479;Adam&#20248;&#21270;&#22120;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#31934;&#24515;&#36873;&#25321;&#19968;&#37096;&#20998;&#26799;&#24230;&#36827;&#34892;&#25277;&#26679;&#65292;&#35813;&#20248;&#21270;&#22120;&#33021;&#22815;&#26377;&#25928;&#24212;&#23545;&#22797;&#26434;&#27169;&#22411;&#30340;&#31649;&#29702;&#12290;&#26412;&#25991;&#20174;&#25968;&#23398;&#22522;&#30784;&#21040;&#20559;&#24046;&#26657;&#27491;&#31574;&#30053;&#20840;&#38754;&#25506;&#35752;&#20102;StochGradAdam&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#25216;&#26415;&#30340;&#21487;&#26399;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly advancing domain of deep learning optimization, this paper unveils the StochGradAdam optimizer, a novel adaptation of the well-regarded Adam algorithm. Central to StochGradAdam is its gradient sampling technique. This method not only ensures stable convergence but also leverages the advantages of selective gradient consideration, fostering robust training by potentially mitigating the effects of noisy or outlier data and enhancing the exploration of the loss landscape for more dependable convergence. In both image classification and segmentation tasks, StochGradAdam has demonstrated superior performance compared to the traditional Adam optimizer. By judiciously sampling a subset of gradients at each iteration, the optimizer is optimized for managing intricate models. The paper provides a comprehensive exploration of StochGradAdam's methodology, from its mathematical foundations to bias correction strategies, heralding a promising advancement in deep learning training tec
&lt;/p&gt;</description></item></channel></rss>