# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Multi-Objective Quality-Diversity for Crystal Structure Prediction](https://arxiv.org/abs/2403.17164) | 本研究利用质量多样性算法为晶体结构预测打开了一条新途径，旨在发现具有多样特征的高性能解决方案集合，可以优化晶体结构稳定性以及其他目标如磁性或热电效率。 |
| [^2] | [What if...?: Counterfactual Inception to Mitigate Hallucination Effects in Large Multimodal Models](https://arxiv.org/abs/2403.13513) | 本文引入了反事实启示（Counterfactual Inception）方法，通过将反事实思想植入到大型多模态模型（LMMs）中，可以减轻幻觉效应并提高模型的可信度。 |
| [^3] | [Provable Privacy with Non-Private Pre-Processing](https://arxiv.org/abs/2403.13041) | 提出了一个框架，能够评估非私密数据相关预处理算法引起的额外隐私成本，并利用平滑DP和预处理算法的有界敏感性建立整体隐私保证的上限 |
| [^4] | [Reinforcement Learning with Latent State Inference for Autonomous On-ramp Merging under Observation Delay](https://arxiv.org/abs/2403.11852) | 本文提出了一种具有潜在状态推断的强化学习方法，用于解决自动匝道合并问题，在没有详细了解周围车辆意图或驾驶风格的情况下安全执行匝道合并任务，并考虑了观测延迟，以增强代理在动态交通状况中的决策能力。 |
| [^5] | [Twin Transformer using Gated Dynamic Learnable Attention mechanism for Fault Detection and Diagnosis in the Tennessee Eastman Process](https://arxiv.org/abs/2403.10842) | 本研究提出一种新颖的双Transformer模型，结合门控动态可学习注意机制，用于田纳西伊斯曼过程的故障检测与诊断，提高性能通过独立处理输入数据和提取多样化信息，以及动态学习适应性调整注意策略。 |
| [^6] | [ArgMed-Agents: Explainable Clinical Decision Reasoning with Large Language Models via Argumentation Schemes](https://arxiv.org/abs/2403.06294) | 通过争论方案的自我论证迭代和构建争论过程，ArgMed-Agents实现了基于LLM的可解释临床决策推理，提高了用户对临床决策的信任。 |
| [^7] | [Geometric Neural Network based on Phase Space for BCI decoding](https://arxiv.org/abs/2403.05645) | 基于相空间的几何神经网络用于BCI解码，提供了在脑机接口领域中可靠算法操作的方法，以提高用户舒适度并促进其广泛应用。 |
| [^8] | [Large language models surpass human experts in predicting neuroscience results](https://arxiv.org/abs/2403.03230) | 大型语言模型通过整合广泛科学文献中的相关发现，能够优于人类专家预测神经科学实验结果，预示着人类与大型语言模型共同进行发现的未来。 |
| [^9] | [Getting Serious about Humor: Crafting Humor Datasets with Unfunny Large Language Models](https://arxiv.org/abs/2403.00794) | 利用大型语言模型生成合成数据，可以帮助改进幽默检测，特别是通过取消幽默元素来评估模型性能。 |
| [^10] | [Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2402.17840) | 研究揭示了检索增强生成系统中的数据泄露风险，指出对手可以利用LMs的指示遵循能力轻松地从数据存储中直接提取文本数据，并设计了攻击对生产RAG模型GPTs造成数据存储泄漏。 |
| [^11] | [Improving LLM-based Machine Translation with Systematic Self-Correction](https://arxiv.org/abs/2402.16379) | 引入了名为TER的系统LLM自校正翻译框架，成功帮助LLMs提高翻译质量，具有更优越的系统性和可解释性。 |
| [^12] | [Active Few-Shot Fine-Tuning](https://arxiv.org/abs/2402.15441) | 该论文提出了ITL方法来实现主动少样本微调，通过最大化对下游任务的信息获取，从而在大型神经网络的微调中取得了显著的改进。 |
| [^13] | [Chain-of-Thought Unfaithfulness as Disguised Accuracy](https://arxiv.org/abs/2402.14897) | 了解Chain-of-Thought生成与大语言模型内部计算的一致程度对于决定是否信任模型输出至关重要，研究发现模型大小与忠实度之间存在着特定关系，并且发现130亿参数模型表现出更高的忠实度。 |
| [^14] | [DiffTOP: Differentiable Trajectory Optimization for Deep Reinforcement and Imitation Learning](https://arxiv.org/abs/2402.05421) | DiffTOP使用可微分轨迹优化作为策略表示来生成动作，解决了模型基于强化学习算法中的“目标不匹配”问题，并在模仿学习任务上进行了性能基准测试。 |
| [^15] | [Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation](https://arxiv.org/abs/2402.03268) | 本文研究了预训练语言模型的推理能力，并提出了从聚合间接推理路径的角度理解语言模型如何产生推理能力。通过对知识图谱和数学问题数据集进行实验和分析，发现增加无标签的随机游走推理路径可以提高实际应用中的多步推理能力。 |
| [^16] | [LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law](https://arxiv.org/abs/2402.00795) | 本文研究了预训练语言模型LLMs对动力系统行为的外推能力，发现LLaMA 2能够准确预测动力系统的时间序列。此外，输入上下文窗口的长度越长，学习到的物理规则的准确性越高，揭示了一种上下文中的神经比例定律。 |
| [^17] | [Jellyfish: A Large Language Model for Data Preprocessing](https://arxiv.org/abs/2312.01678) | 这项研究探讨了在数据挖掘中利用大型语言模型进行数据预处理的方法，通过指导调整本地LLMs来解决通用数据预处理问题，确保数据安全并进行进一步调整 |
| [^18] | [Accelerating Approximate Thompson Sampling with Underdamped Langevin Monte Carlo.](http://arxiv.org/abs/2401.11665) | 本文提出了一种使用欠阻尼 Langevin Monte Carlo 加速的近似 Thompson 采样策略，通过特定势函数的设计改善了高维问题中的样本复杂度，并在高维赌博机问题中进行了验证。 |
| [^19] | [Large Legal Fictions: Profiling Legal Hallucinations in Large Language Models.](http://arxiv.org/abs/2401.01301) | 大型语言模型存在法律幻觉，不一致法律事实，幻觉普遍存在高达69%至88%的情况，无法纠正用户错误法律假设。 |
| [^20] | [Diverse Offline Imitation via Fenchel Duality.](http://arxiv.org/abs/2307.11373) | 本文提出了一个离线技能发现算法，通过Fenchel对偶方法将强化学习和无监督技能发现结合起来，实现学习与专家相一致的多样的技能。 |
| [^21] | [RL4CO: an Extensive Reinforcement Learning for Combinatorial Optimization Benchmark.](http://arxiv.org/abs/2306.17100) | RL4CO是一个用于组合优化的广泛强化学习基准测试，着重于可扩展性和泛化能力的评估，并展示了一些最新方法在样本效率和适应不同数据分布方面的表现相对较差，强调了对神经CO求解器性能的平衡评估的重要性。 |
| [^22] | [SRL: Scaling Distributed Reinforcement Learning to Over Ten Thousand Cores.](http://arxiv.org/abs/2306.16688) | SRL是一个可扩展，高效，可扩展的分布式强化学习系统，通过一种新的抽象框架统一了各种实际强化学习训练，并实现了精细优化。 |
| [^23] | [RoMo-HER: Robust Model-based Hindsight Experience Replay.](http://arxiv.org/abs/2306.16061) | RoMo-HER是一个鲁棒的基于模型的事后经验回放方法，通过使用机器人操作环境中的动力学模型和前瞻重新标记技术，提高了样本利用效率。 |
| [^24] | [FedMLSecurity: A Benchmark for Attacks and Defenses in Federated Learning and LLMs.](http://arxiv.org/abs/2306.04959) | 本文介绍了一个名为FedMLSecurity的基准测试，它可以模拟在联邦学习中可能出现的对抗攻击并提供相应的防御策略。该测试对各种机器学习模型和联合优化器都可以适用，并且能够轻松应用于大规模语言模型中。 |
| [^25] | [Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty with Large Language Models.](http://arxiv.org/abs/2305.13712) | 本文探索了大型语言模型对其自身知识的理解和测量不确定性的能力。该研究聚焦于解决“已知-未知”问题，提出了新的分类方案，并使用语义评估方法量化了模型表达不确定性的准确性。 |
| [^26] | [Hierarchical Path-planning from Speech Instructions with Spatial Concept-based Topometric Semantic Mapping.](http://arxiv.org/abs/2203.10820) | 该论文提出了一种基于拓扑语义映射的分层路径规划方法，可以通过语音指令和waypoint灵活地规划路径，包括地点连通性，提供了一种快速近似推理方法。 |

# 详细

[^1]: 多目标质量多样性用于晶体结构预测

    Multi-Objective Quality-Diversity for Crystal Structure Prediction

    [https://arxiv.org/abs/2403.17164](https://arxiv.org/abs/2403.17164)

    本研究利用质量多样性算法为晶体结构预测打开了一条新途径，旨在发现具有多样特征的高性能解决方案集合，可以优化晶体结构稳定性以及其他目标如磁性或热电效率。

    

    晶体结构在从电池到太阳能电池等各个领域中都是不可或缺的，针对其原子配置预测性能已经有了大量研究。然而，现有的晶体结构预测方法侧重于识别能量函数全局最小值处的最稳定解决方案，而忽略了那些可能位于相邻局部极小值处、具有不同材料特性（如电导率或抗变形性）的其他有趣材料。相比之下，质量多样性算法为晶体结构预测提供了一个有前途的途径，因为它旨在找到具有多样特征的高性能解决方案集合。然而，优化晶体结构稳定性以及其他目标（如磁性或热电效率）也可能是有价值的。因此，在这项研究中，我们利用......

    arXiv:2403.17164v1 Announce Type: cross  Abstract: Crystal structures are indispensable across various domains, from batteries to solar cells, and extensive research has been dedicated to predicting their properties based on their atomic configurations. However, prevailing Crystal Structure Prediction methods focus on identifying the most stable solutions that lie at the global minimum of the energy function. This approach overlooks other potentially interesting materials that lie in neighbouring local minima and have different material properties such as conductivity or resistance to deformation. By contrast, Quality-Diversity algorithms provide a promising avenue for Crystal Structure Prediction as they aim to find a collection of high-performing solutions that have diverse characteristics. However, it may also be valuable to optimise for the stability of crystal structures alongside other objectives such as magnetism or thermoelectric efficiency. Therefore, in this work, we harness 
    
[^2]: 如果......会怎样？：反事实启示在大型多模态模型中减轻幻觉效应

    What if...?: Counterfactual Inception to Mitigate Hallucination Effects in Large Multimodal Models

    [https://arxiv.org/abs/2403.13513](https://arxiv.org/abs/2403.13513)

    本文引入了反事实启示（Counterfactual Inception）方法，通过将反事实思想植入到大型多模态模型（LMMs）中，可以减轻幻觉效应并提高模型的可信度。

    

    本文介绍了提高大型多模态模型（LMMs）在处理幻觉效应方面可靠性的方法，其中模型会生成不正确或无关的响应。没有额外的指导调整范式，我们引入了反事实启示，这是一种新颖的方法，通过精心选择的、不对齐的反事实关键词将反事实思想植入到LMMs中。该方法根植于反事实思维概念，这是一种认知过程，人类在其中考虑替代现实和结果。通过将这种类似人类的推理机制应用到LMMs中，我们旨在减少幻觉效应并提高模型的可信度。我们还提出了双模态验证过程（DVP），这是一个严格的框架，用于选择触发LMMs中反事实思维的最佳反事实关键词，同时考虑视觉和语言上下文。我们在各种LMMs上进行了大量实验

    arXiv:2403.13513v1 Announce Type: cross  Abstract: This paper presents a way of enhancing the reliability of Large Multimodal Models (LMMs) in addressing hallucination effects, where models generate incorrect or unrelated responses. Without additional instruction tuning paradigm, we introduce Counterfactual Inception, a novel method that implants counterfactual thoughts into LMMs using carefully chosen, misaligned counterfactual keywords. This method is grounded in the concept of counterfactual thinking, a cognitive process where humans consider alternative realities and outcomes. By applying this human-like reasoning mechanism to LMMs, we aim to reduce hallucination effects and improve the models' trustworthiness. We also propose Dual-modality Verification Process (DVP), a rigorous framework for selecting optimal counterfactual keywords to trigger counterfactual thinking into LMMs, concurrently considering visual and linguistic context. Our extensive experiments across various LMMs, i
    
[^3]: 具有非私密预处理的可证明隐私

    Provable Privacy with Non-Private Pre-Processing

    [https://arxiv.org/abs/2403.13041](https://arxiv.org/abs/2403.13041)

    提出了一个框架，能够评估非私密数据相关预处理算法引起的额外隐私成本，并利用平滑DP和预处理算法的有界敏感性建立整体隐私保证的上限

    

    当分析差分私密（DP）机器学习管道时，通常会忽略数据相关的预处理的潜在隐私成本。在这项工作中，我们提出了一个通用框架，用于评估由非私密数据相关预处理算法引起的额外隐私成本。我们的框架通过利用两个新的技术概念建立了整体隐私保证的上限：一种称为平滑DP的DP变体以及预处理算法的有界敏感性。

    arXiv:2403.13041v1 Announce Type: cross  Abstract: When analysing Differentially Private (DP) machine learning pipelines, the potential privacy cost of data-dependent pre-processing is frequently overlooked in privacy accounting. In this work, we propose a general framework to evaluate the additional privacy cost incurred by non-private data-dependent pre-processing algorithms. Our framework establishes upper bounds on the overall privacy guarantees by utilising two new technical notions: a variant of DP termed Smooth DP and the bounded sensitivity of the pre-processing algorithms. In addition to the generic framework, we provide explicit overall privacy guarantees for multiple data-dependent pre-processing algorithms, such as data imputation, quantization, deduplication and PCA, when used in combination with several DP algorithms. Notably, this framework is also simple to implement, allowing direct integration into existing DP pipelines.
    
[^4]: 具有潜在状态推断的强化学习在自动匝道合并中的应用

    Reinforcement Learning with Latent State Inference for Autonomous On-ramp Merging under Observation Delay

    [https://arxiv.org/abs/2403.11852](https://arxiv.org/abs/2403.11852)

    本文提出了一种具有潜在状态推断的强化学习方法，用于解决自动匝道合并问题，在没有详细了解周围车辆意图或驾驶风格的情况下安全执行匝道合并任务，并考虑了观测延迟，以增强代理在动态交通状况中的决策能力。

    

    本文提出了一种解决自动匝道合并问题的新方法，其中自动驾驶车辆需要无缝地融入多车道高速公路上的车流。我们介绍了Lane-keeping, Lane-changing with Latent-state Inference and Safety Controller (L3IS)代理，旨在在没有关于周围车辆意图或驾驶风格的全面知识的情况下安全执行匝道合并任务。我们还提出了该代理的增强版AL3IS，考虑了观测延迟，使代理能够在具有车辆间通信延迟的现实环境中做出更稳健的决策。通过通过潜在状态建模环境中的不可观察方面，如其他驾驶员的意图，我们的方法增强了代理适应动态交通状况、优化合并操作并确保与其他车辆进行安全互动的能力。

    arXiv:2403.11852v1 Announce Type: cross  Abstract: This paper presents a novel approach to address the challenging problem of autonomous on-ramp merging, where a self-driving vehicle needs to seamlessly integrate into a flow of vehicles on a multi-lane highway. We introduce the Lane-keeping, Lane-changing with Latent-state Inference and Safety Controller (L3IS) agent, designed to perform the on-ramp merging task safely without comprehensive knowledge about surrounding vehicles' intents or driving styles. We also present an augmentation of this agent called AL3IS that accounts for observation delays, allowing the agent to make more robust decisions in real-world environments with vehicle-to-vehicle (V2V) communication delays. By modeling the unobservable aspects of the environment through latent states, such as other drivers' intents, our approach enhances the agent's ability to adapt to dynamic traffic conditions, optimize merging maneuvers, and ensure safe interactions with other vehi
    
[^5]: 使用门控动态可学习注意机制的双Transformer在田纳西伊斯曼过程中进行故障检测与诊断

    Twin Transformer using Gated Dynamic Learnable Attention mechanism for Fault Detection and Diagnosis in the Tennessee Eastman Process

    [https://arxiv.org/abs/2403.10842](https://arxiv.org/abs/2403.10842)

    本研究提出一种新颖的双Transformer模型，结合门控动态可学习注意机制，用于田纳西伊斯曼过程的故障检测与诊断，提高性能通过独立处理输入数据和提取多样化信息，以及动态学习适应性调整注意策略。

    

    故障检测和诊断（FDD）对于确保工业过程的安全性和效率至关重要。我们提出了一种新颖的FDD方法，适用于田纳西伊斯曼过程（TEP），这是化工过程控制中广泛使用的基准。该模型采用两个独立的Transformer分支，能够独立处理输入数据并提取多样化的信息。引入了一种新颖的注意机制，即门控动态可学习注意（GDLAttention），它集成了门控机制和动态学习能力。门控机制调节注意权重，使模型能够关注输入的最相关部分。动态学习方法在训练过程中调整注意策略，有可能提高性能。注意机制使用双线性相似性函数，提供更大的灵活性来捕捉查询和输入之间的复杂关系。

    arXiv:2403.10842v1 Announce Type: cross  Abstract: Fault detection and diagnosis (FDD) is a crucial task for ensuring the safety and efficiency of industrial processes. We propose a novel FDD methodology for the Tennessee Eastman Process (TEP), a widely used benchmark for chemical process control. The model employs two separate Transformer branches, enabling independent processing of input data and potential extraction of diverse information. A novel attention mechanism, Gated Dynamic Learnable Attention (GDLAttention), is introduced which integrates a gating mechanism and dynamic learning capabilities. The gating mechanism modulates the attention weights, allowing the model to focus on the most relevant parts of the input. The dynamic learning approach adapts the attention strategy during training, potentially leading to improved performance. The attention mechanism uses a bilinear similarity function, providing greater flexibility in capturing complex relationships between query and 
    
[^6]: ArgMed-Agents: 使用争议方案通过大型语言模型解释性临床决策推理

    ArgMed-Agents: Explainable Clinical Decision Reasoning with Large Language Models via Argumentation Schemes

    [https://arxiv.org/abs/2403.06294](https://arxiv.org/abs/2403.06294)

    通过争论方案的自我论证迭代和构建争论过程，ArgMed-Agents实现了基于LLM的可解释临床决策推理，提高了用户对临床决策的信任。

    

    arXiv:2403.06294v1 公告类型: 新摘要: 使用大型语言模型（LLMs）进行临床推理存在两个主要障碍。首先，虽然LLMs在自然语言处理（NLP）任务中显示出巨大的潜力，但在复杂推理和规划方面的表现却不尽人意。其次，LLMs使用不可解释的方法进行临床决策，这与临床医生的认知过程本质上不同，导致用户不信任。在本文中，我们提出了一个名为ArgMed-Agents的多代理框架，旨在通过交互使基于LLM的代理能够进行可解释的临床决策推理。ArgMed-Agents通过临床决策论据（一种模拟临床决策认知过程的推理机制）执行自论证迭代，然后将争论过程构建为表示冲突关系的有向图。最终，Reasoner（一种符号求解器）识别出一个

    arXiv:2403.06294v1 Announce Type: new  Abstract: There are two main barriers to using large language models (LLMs) in clinical reasoning. Firstly, while LLMs exhibit significant promise in Natural Language Processing (NLP) tasks, their performance in complex reasoning and planning falls short of expectations. Secondly, LLMs use uninterpretable methods to make clinical decisions that are fundamentally different from the clinician's cognitive processes. This leads to user distrust. In this paper, we present a multi-agent framework called ArgMed-Agents, which aims to enable LLM-based agents to make explainable clinical decision reasoning through interaction. ArgMed-Agents performs self-argumentation iterations via Argumentation Scheme for Clinical Decision (a reasoning mechanism for modeling cognitive processes in clinical reasoning), and then constructs the argumentation process as a directed graph representing conflicting relationships. Ultimately, Reasoner(a symbolic solver) identify a
    
[^7]: 基于相空间的几何神经网络用于BCI解码

    Geometric Neural Network based on Phase Space for BCI decoding

    [https://arxiv.org/abs/2403.05645](https://arxiv.org/abs/2403.05645)

    基于相空间的几何神经网络用于BCI解码，提供了在脑机接口领域中可靠算法操作的方法，以提高用户舒适度并促进其广泛应用。

    

    Deep Learning(DL)算法与脑信号分析的整合仍处于萌芽阶段，相比计算机视觉等领域的成功，在脑机接口(BCI)领域尤为突出，BCI通过解码大脑活动控制外部设备而无需肌肉控制。脑电图(EEG)是设计BCI系统的广泛选择，因其无创性、成本效益和出色的时间分辨率，但缺少训练数据、信噪比低、以及在个体间和内部的大量变化。 最后，使用多个电极设置BCI系统需要很长时间，阻碍可靠DL架构在研究实验室之外的BCI中的广泛应用。 为了提高采纳率，我们需要改善用户舒适度，例如使用少量电极操作的可靠算法。

    arXiv:2403.05645v1 Announce Type: cross  Abstract: The integration of Deep Learning (DL) algorithms on brain signal analysis is still in its nascent stages compared to their success in fields like Computer Vision, especially in Brain-Computer Interface (BCI), where the brain activity is decoded to control external devices without requiring muscle control. Electroencephalography (EEG) is a widely adopted choice for designing BCI systems due to its non-invasive and cost-effective nature and excellent temporal resolution. Still, it comes at the expense of limited training data, poor signal-to-noise, and a large variability across and within-subject recordings. Finally, setting up a BCI system with many electrodes takes a long time, hindering the widespread adoption of reliable DL architectures in BCIs outside research laboratories. To improve adoption, we need to improve user comfort using, for instance, reliable algorithms that operate with few electrodes. \textbf{Approach:} Our research
    
[^8]: 大型语言模型在预测神经科学结果方面超越人类专家

    Large language models surpass human experts in predicting neuroscience results

    [https://arxiv.org/abs/2403.03230](https://arxiv.org/abs/2403.03230)

    大型语言模型通过整合广泛科学文献中的相关发现，能够优于人类专家预测神经科学实验结果，预示着人类与大型语言模型共同进行发现的未来。

    

    科学发现常常取决于综合几十年的研究，这一任务可能超出人类信息处理能力。大型语言模型（LLMs）提供了一个解决方案。在广泛的科学文献上训练的LLMs可能能够整合嘈杂但相关的发现，以优于人类专家来预测新颖结果。为了评估这种可能性，我们创建了BrainBench，一个前瞻性的基准，用于预测神经科学结果。我们发现LLMs在预测实验结果方面超越了专家。在神经科学文献上调整的一个LLM，BrainGPT表现得更好。与人类专家一样，当LLMs对他们的预测有信心时，他们更有可能是正确的，这预示着未来人类和LLMs将合作进行发现。我们的方法并非特定于神经科学，并且可转移到其他知识密集型事业中。

    arXiv:2403.03230v1 Announce Type: cross  Abstract: Scientific discoveries often hinge on synthesizing decades of research, a task that potentially outstrips human information processing capacities. Large language models (LLMs) offer a solution. LLMs trained on the vast scientific literature could potentially integrate noisy yet interrelated findings to forecast novel results better than human experts. To evaluate this possibility, we created BrainBench, a forward-looking benchmark for predicting neuroscience results. We find that LLMs surpass experts in predicting experimental outcomes. BrainGPT, an LLM we tuned on the neuroscience literature, performed better yet. Like human experts, when LLMs were confident in their predictions, they were more likely to be correct, which presages a future where humans and LLMs team together to make discoveries. Our approach is not neuroscience-specific and is transferable to other knowledge-intensive endeavors.
    
[^9]: 认真对待幽默：利用不风趣的大型语言模型构建幽默数据集

    Getting Serious about Humor: Crafting Humor Datasets with Unfunny Large Language Models

    [https://arxiv.org/abs/2403.00794](https://arxiv.org/abs/2403.00794)

    利用大型语言模型生成合成数据，可以帮助改进幽默检测，特别是通过取消幽默元素来评估模型性能。

    

    幽默是人类认知和互动的基本要素。然而，尽管自然语言处理方面取得了近期进展，幽默检测仍然是一项具有挑战性的任务，这是因为幽默文本与类似非幽默文本的数据集稀缺。在我们的研究中，我们探讨了大型语言模型（LLMs）能否通过编辑文本生成用于幽默检测的合成数据。我们在现有人类数据集上对LLMs进行基准测试，并展示当前LLMs在“取消风趣”笑话方面显示出令人印象深刻的能力，这是由人类判断和幽默检测的下游任务衡量而得。我们将我们的方法扩展到了一个混合编码的英语-印地语幽默数据集，在那里我们发现GPT-4的合成数据被双语注释员高度评价，并为幽默分类器提供了具有挑战性的对抗性例子。

    arXiv:2403.00794v1 Announce Type: cross  Abstract: Humor is a fundamental facet of human cognition and interaction. Yet, despite recent advances in natural language processing, humor detection remains a challenging task that is complicated by the scarcity of datasets that pair humorous texts with similar non-humorous counterparts. In our work, we investigate whether large language models (LLMs), can generate synthetic data for humor detection via editing texts. We benchmark LLMs on an existing human dataset and show that current LLMs display an impressive ability to `unfun' jokes, as judged by humans and as measured on the downstream task of humor detection. We extend our approach to a code-mixed English-Hindi humor dataset, where we find that GPT-4's synthetic data is highly rated by bilingual annotators and provides challenging adversarial examples for humor classifiers.
    
[^10]: 遵循我的指示并说出真相：来自检索增强生成系统的可扩展数据提取

    Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems

    [https://arxiv.org/abs/2402.17840](https://arxiv.org/abs/2402.17840)

    研究揭示了检索增强生成系统中的数据泄露风险，指出对手可以利用LMs的指示遵循能力轻松地从数据存储中直接提取文本数据，并设计了攻击对生产RAG模型GPTs造成数据存储泄漏。

    

    检索增强生成（RAG）通过在测试时将外部知识纳入预训练模型，从而实现定制适应，提升了模型性能。本研究探讨了Retrieval-In-Context RAG语言模型（LMs）中的数据泄露风险。我们展示了当对使用指令调整的LMs构建的RAG系统进行提示注入时，对手可以利用LMs的指示遵循能力轻松地从数据存储中直接提取文本数据。这种漏洞存在于覆盖Llama2、Mistral/Mixtral、Vicuna、SOLAR、WizardLM、Qwen1.5和Platypus2等多种现代LMs的广泛范围内，并且随着模型规模的扩大，利用能力加剧。将研究扩展到生产RAG模型GPTs，我们设计了一种攻击，可以在对25个随机选择的定制GPTs施加最多2个查询时以100%成功率导致数据存储泄漏，并且我们能够以77,000字的书籍中的文本数据的提取率为41%，以及在含有1,569,00词的语料库中的文本数据的提取率为3%。

    arXiv:2402.17840v1 Announce Type: cross  Abstract: Retrieval-Augmented Generation (RAG) improves pre-trained models by incorporating external knowledge at test time to enable customized adaptation. We study the risk of datastore leakage in Retrieval-In-Context RAG Language Models (LMs). We show that an adversary can exploit LMs' instruction-following capabilities to easily extract text data verbatim from the datastore of RAG systems built with instruction-tuned LMs via prompt injection. The vulnerability exists for a wide range of modern LMs that span Llama2, Mistral/Mixtral, Vicuna, SOLAR, WizardLM, Qwen1.5, and Platypus2, and the exploitability exacerbates as the model size scales up. Extending our study to production RAG models GPTs, we design an attack that can cause datastore leakage with a 100% success rate on 25 randomly selected customized GPTs with at most 2 queries, and we extract text data verbatim at a rate of 41% from a book of 77,000 words and 3% from a corpus of 1,569,00
    
[^11]: 用系统自校正改进基于LLM的机器翻译

    Improving LLM-based Machine Translation with Systematic Self-Correction

    [https://arxiv.org/abs/2402.16379](https://arxiv.org/abs/2402.16379)

    引入了名为TER的系统LLM自校正翻译框架，成功帮助LLMs提高翻译质量，具有更优越的系统性和可解释性。

    

    大型语言模型（LLMs）在机器翻译（MT）领域取得了令人印象深刻的结果。然而，人工仔细评估发现，LLMs生成的翻译仍然包含多个错误。重要的是，将这种错误信息反馈到LLMs中可以实现自校正，并改善翻译性能。受到这些观点的启发，我们引入了一个名为TER的系统LLM自校正翻译框架，代表了在这一方向上的重要进展。我们的研究结果表明：1）我们的自校正框架成功地帮助LLMs提高了多种语言的翻译质量，不管是从高资源语言到低资源语言，还是以英语为中心还是围绕其他语言；2）TER相比先前的方法展示出更优越的系统性和可解释性；3）

    arXiv:2402.16379v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have achieved impressive results in Machine Translation (MT). However, careful evaluations by human reveal that the translations produced by LLMs still contain multiple errors. Importantly, feeding back such error information into the LLMs can lead to self-correction and result in improved translation performance. Motivated by these insights, we introduce a systematic LLM-based self-correcting translation framework, named TER, which stands for Translate, Estimate, and Refine, marking a significant step forward in this direction. Our findings demonstrate that 1) our self-correction framework successfully assists LLMs in improving their translation quality across a wide range of languages, whether it's from high-resource languages to low-resource ones or whether it's English-centric or centered around other languages; 2) TER exhibits superior systematicity and interpretability compared to previous methods; 3)
    
[^12]: 主动少样本微调

    Active Few-Shot Fine-Tuning

    [https://arxiv.org/abs/2402.15441](https://arxiv.org/abs/2402.15441)

    该论文提出了ITL方法来实现主动少样本微调，通过最大化对下游任务的信息获取，从而在大型神经网络的微调中取得了显著的改进。

    

    我们研究了大型神经网络对下游任务进行主动少样本微调。我们表明少样本微调是传统主动学习和转导主动学习的泛化实例，我们提出了信息基于转导学习（ITL）的方法，该方法自适应地进行采样以最大化获得对指定下游任务的信息。在一般正则性假设下，我们证明ITL均匀收敛到可从可访问数据获取的最小可能的不确定性。据我们所知，我们是首批推导出这种泛化界限的人，这对于主动学习可能是具有独立意义的。我们将ITL应用于大型神经网络的少样本微调中，结果显示ITL明显改进了现有技术。

    arXiv:2402.15441v1 Announce Type: cross  Abstract: We study the active few-shot fine-tuning of large neural networks to downstream tasks. We show that few-shot fine-tuning is an instance of a generalization of classical active learning, transductive active learning, and we propose ITL, short for information-based transductive learning, an approach which samples adaptively to maximize the information gained about specified downstream tasks. Under general regularity assumptions, we prove that ITL converges uniformly to the smallest possible uncertainty obtainable from the accessible data. To the best of our knowledge, we are the first to derive generalization bounds of this kind, and they may be of independent interest for active learning. We apply ITL to the few-shot fine-tuning of large neural networks and show that ITL substantially improves upon the state-of-the-art.
    
[^13]: Chain-of-Thought不忠诚作为伪装的准确性

    Chain-of-Thought Unfaithfulness as Disguised Accuracy

    [https://arxiv.org/abs/2402.14897](https://arxiv.org/abs/2402.14897)

    了解Chain-of-Thought生成与大语言模型内部计算的一致程度对于决定是否信任模型输出至关重要，研究发现模型大小与忠实度之间存在着特定关系，并且发现130亿参数模型表现出更高的忠实度。

    

    了解Chain-of-Thought (CoT)生成与大语言模型(LLM)内部计算的一致程度对于决定是否信任LLM的输出至关重要。作为CoT忠实度的代理，arXiv:2307.13702提出了一个度量模型依赖其CoT生成答案的指标。在一个专有模型系列中，他们发现LLM表现出模型大小与其忠实度测量之间的缩放-反向缩放关系，并且130亿参数模型相比于尺寸介于8.1亿到1750亿参数之间的模型表现出增加的忠实度。我们评估这些结果是否作为所有LLM的特性泛化。我们使用三种不同系列的模型复制他们的实验设置，并在特定条件下，成功复制了他们报告的CoT忠实度的缩放趋势。然而，我们发现简单的改变设定会导致这些模式在多大程度上重复。

    arXiv:2402.14897v1 Announce Type: cross  Abstract: Understanding the extent to which Chain-of-Thought (CoT) generations align with a large language model's (LLM) internal computations is critical for deciding whether to trust an LLM's output. As a proxy for CoT faithfulness, arXiv:2307.13702 propose a metric that measures a model's dependence on its CoT for producing an answer. Within a single family of proprietary models, they find that LLMs exhibit a scaling-then-inverse-scaling relationship between model size and their measure of faithfulness, and that a 13 billion parameter model exhibits increased faithfulness compared to models ranging from 810 million to 175 billion parameters in size. We evaluate whether these results generalize as a property of all LLMs. We replicate their experimental setup with three different families of models and, under specific conditions, successfully reproduce the scaling trends for CoT faithfulness they report. However, we discover that simply changin
    
[^14]: DiffTOP: 可微分轨迹优化在强化学习和模仿学习中的应用

    DiffTOP: Differentiable Trajectory Optimization for Deep Reinforcement and Imitation Learning

    [https://arxiv.org/abs/2402.05421](https://arxiv.org/abs/2402.05421)

    DiffTOP使用可微分轨迹优化作为策略表示来生成动作，解决了模型基于强化学习算法中的“目标不匹配”问题，并在模仿学习任务上进行了性能基准测试。

    

    本文介绍了DiffTOP，它利用可微分轨迹优化作为策略表示，为深度强化学习和模仿学习生成动作。轨迹优化是一种在控制领域中广泛使用的算法，由成本和动力学函数参数化。我们的方法的关键是利用了最近在可微分轨迹优化方面的进展，使得可以计算损失对于轨迹优化的参数的梯度。因此，轨迹优化的成本和动力学函数可以端到端地学习。DiffTOP解决了之前模型基于强化学习算法中的“目标不匹配”问题，因为DiffTOP中的动力学模型通过轨迹优化过程中的策略梯度损失直接最大化任务性能。我们还对DiffTOP在标准机器人操纵任务套件中进行了模仿学习性能基准测试。

    This paper introduces DiffTOP, which utilizes Differentiable Trajectory OPtimization as the policy representation to generate actions for deep reinforcement and imitation learning. Trajectory optimization is a powerful and widely used algorithm in control, parameterized by a cost and a dynamics function. The key to our approach is to leverage the recent progress in differentiable trajectory optimization, which enables computing the gradients of the loss with respect to the parameters of trajectory optimization. As a result, the cost and dynamics functions of trajectory optimization can be learned end-to-end. DiffTOP addresses the ``objective mismatch'' issue of prior model-based RL algorithms, as the dynamics model in DiffTOP is learned to directly maximize task performance by differentiating the policy gradient loss through the trajectory optimization process. We further benchmark DiffTOP for imitation learning on standard robotic manipulation task suites with high-dimensional sensory
    
[^15]: 从推理路径聚合的角度理解语言模型的推理能力

    Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation

    [https://arxiv.org/abs/2402.03268](https://arxiv.org/abs/2402.03268)

    本文研究了预训练语言模型的推理能力，并提出了从聚合间接推理路径的角度理解语言模型如何产生推理能力。通过对知识图谱和数学问题数据集进行实验和分析，发现增加无标签的随机游走推理路径可以提高实际应用中的多步推理能力。

    

    预训练的语言模型能够在没有明确微调的情况下执行复杂的推理。为了理解预训练与下一个标记预测目标的关系如何促使推理能力的出现，我们提出可以将语言模型视为在预训练时通过聚合间接的推理路径来得出新结论。我们发现，这个视角在逻辑推理和数学推理等关键情况下非常有效。具体而言，我们将推理路径形式化为在知识/推理图上的随机游走路径。对学习的语言模型分布的分析表明，相关随机游走路径概率的加权和是解释语言模型推理的合理方式。对多个知识图谱和数学问题数据集进行的实验和分析揭示了训练对随机游走路径的影响，并表明增加无标签的随机游走推理路径可以提高现实世界的多步推理能力。

    Pre-trained language models (LMs) are able to perform complex reasoning without explicit fine-tuning. To understand how pre-training with a next-token prediction objective contributes to the emergence of such reasoning capability, we propose that we can view an LM as deriving new conclusions by aggregating indirect reasoning paths seen at pre-training time. We found this perspective effective in two important cases of reasoning: logic reasoning with knowledge graphs (KGs) and math reasoning with math word problems (MWPs). More specifically, we formalize the reasoning paths as random walk paths on the knowledge/reasoning graphs. Analyses of learned LM distributions suggest that a weighted sum of relevant random walk path probabilities is a reasonable way to explain how LMs reason. Experiments and analysis on multiple KG and MWP datasets reveal the effect of training on random walk paths and suggest that augmenting unlabeled random walk reasoning paths can improve real-world multi-step r
    
[^16]: LLMs学习动力系统的控制原理，揭示了上下文中的神经比例定律

    LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law

    [https://arxiv.org/abs/2402.00795](https://arxiv.org/abs/2402.00795)

    本文研究了预训练语言模型LLMs对动力系统行为的外推能力，发现LLaMA 2能够准确预测动力系统的时间序列。此外，输入上下文窗口的长度越长，学习到的物理规则的准确性越高，揭示了一种上下文中的神经比例定律。

    

    预训练的大型语言模型（LLMs）在零-shot任务，包括时间序列预测方面表现出惊人的有效性。然而，由于模型的复杂性，理解其背后的机制仍然极具挑战性。本文研究了LLMs对受物理原理控制的动力系统行为的外推能力。我们的结果表明，主要在文本上进行训练的语言模型LLaMA 2在没有微调或提示工程的情况下，能够准确预测动力系统的时间序列。此外，学习到的物理规则的准确性随着输入上下文窗口的长度增加而增加，揭示了一种上下文中的神经比例定律。同时，我们还提出了一种灵活高效的算法，用于直接从LLMs中提取多位数的概率密度函数。

    Pretrained large language models (LLMs) are surprisingly effective at performing zero-shot tasks, including time-series forecasting. However, understanding the mechanisms behind such capabilities remains highly challenging due to the complexity of the models. In this paper, we study LLMs' ability to extrapolate the behavior of dynamical systems whose evolution is governed by principles of physical interest. Our results show that LLaMA 2, a language model trained primarily on texts, achieves accurate predictions of dynamical system time series without fine-tuning or prompt engineering. Moreover, the accuracy of the learned physical rules increases with the length of the input context window, revealing an in-context version of neural scaling law. Along the way, we present a flexible and efficient algorithm for extracting probability density functions of multi-digit numbers directly from LLMs.
    
[^17]: Jellyfish：一个用于数据预处理的大型语言模型

    Jellyfish: A Large Language Model for Data Preprocessing

    [https://arxiv.org/abs/2312.01678](https://arxiv.org/abs/2312.01678)

    这项研究探讨了在数据挖掘中利用大型语言模型进行数据预处理的方法，通过指导调整本地LLMs来解决通用数据预处理问题，确保数据安全并进行进一步调整

    

    这篇论文探讨了在数据挖掘管道中将原始数据转换为有利于简单处理的干净格式的数据预处理（DP）中LLMs的利用。与使用LLMs为DP设计通用解决方案引起了兴趣相比，最近在这一领域的倡议通常依赖于GPT API，引发了不可避免的数据泄霏担忧。与这些方法不同，我们考虑将指导调整本地LLMs（7-13B模型）作为通用DP问解器。我们选择了代表性DP任务的四组数据集，并利用针对DP定制的序列化和知识注入技术构建了指导调整数据。因此，指导调整的LLMs使用户能够为DP手动制定指导。同时，它们可以在本地、单一和价格低廉的GPU上运行，确保数据安全并实现进一步调整。我们的实验表明，我们为DP指导构建的数据集

    arXiv:2312.01678v4 Announce Type: replace  Abstract: This paper explores the utilization of LLMs for data preprocessing (DP), a crucial step in the data mining pipeline that transforms raw data into a clean format conducive to easy processing. Whereas the use of LLMs has sparked interest in devising universal solutions to DP, recent initiatives in this domain typically rely on GPT APIs, raising inevitable data breach concerns. Unlike these approaches, we consider instruction-tuning local LLMs (7 - 13B models) as universal DP ask solver. We select a collection of datasets across four representative DP tasks and construct instruction-tuning data using serialization and knowledge injection techniques tailored to DP. As such, the instruction-tuned LLMs empower users to manually craft instructions for DP. Meanwhile, they can operate on a local, single, and low-priced GPU, ensuring data security and enabling further tuning. Our experiments show that our dataset constructed for DP instruction
    
[^18]: 使用欠阻尼 Langevin Monte Carlo 加速近似 Thompson 采样

    Accelerating Approximate Thompson Sampling with Underdamped Langevin Monte Carlo. (arXiv:2401.11665v1 [stat.ML])

    [http://arxiv.org/abs/2401.11665](http://arxiv.org/abs/2401.11665)

    本文提出了一种使用欠阻尼 Langevin Monte Carlo 加速的近似 Thompson 采样策略，通过特定势函数的设计改善了高维问题中的样本复杂度，并在高维赌博机问题中进行了验证。

    

    使用欠阻尼 Langevin Monte Carlo 的近似 Thompson 采样方法扩展了其适用范围，从高斯后验采样扩展到更一般的平滑后验。然而，在高维问题中要求高准确性时，仍然面临可扩展性问题。为了解决这个问题，我们提出了一种近似 Thompson 采样策略，利用欠阻尼 Langevin Monte Carlo，后者是模拟高维后验的通用工具。基于标准的平滑性和对数凹性条件，我们研究了使用特定势函数的加速后验集中和采样。该设计改进了实现对数遗憾的样本复杂度，从$\mathcal{\tilde O}(d)$改进到$\mathcal{\tilde O}(\sqrt{d})$。我们还通过合成实验在高维赌博机问题中经验验证了我们算法的可扩展性和鲁棒性。

    Approximate Thompson sampling with Langevin Monte Carlo broadens its reach from Gaussian posterior sampling to encompass more general smooth posteriors. However, it still encounters scalability issues in high-dimensional problems when demanding high accuracy. To address this, we propose an approximate Thompson sampling strategy, utilizing underdamped Langevin Monte Carlo, where the latter is the go-to workhorse for simulations of high-dimensional posteriors. Based on the standard smoothness and log-concavity conditions, we study the accelerated posterior concentration and sampling using a specific potential function. This design improves the sample complexity for realizing logarithmic regrets from $\mathcal{\tilde O}(d)$ to $\mathcal{\tilde O}(\sqrt{d})$. The scalability and robustness of our algorithm are also empirically validated through synthetic experiments in high-dimensional bandit problems.
    
[^19]: 大型法律虚构：揭示大型语言模型中的法律幻觉

    Large Legal Fictions: Profiling Legal Hallucinations in Large Language Models. (arXiv:2401.01301v1 [cs.CL])

    [http://arxiv.org/abs/2401.01301](http://arxiv.org/abs/2401.01301)

    大型语言模型存在法律幻觉，不一致法律事实，幻觉普遍存在高达69%至88%的情况，无法纠正用户错误法律假设。

    

    大型语言模型（LLMs）有可能改变法律实践，但其潜力受到法律幻觉的威胁，即这些模型产生与法律事实不一致的回答。我们使用一套原创的法律查询来调查这些幻觉的程度，将LLMs的回答与结构化的法律元数据进行对比，并检查其一致性。我们的工作有四个关键贡献：（1）我们建立了法律幻觉的分类体系，为今后在这一领域进行的研究提供了概念框架。（2）我们发现，法律幻觉的普遍性令人担忧，在对随机联邦法院案例进行具体、可验证的问题时，ChatGPT 3.5产生的幻觉发生率为69％，而Llama 2为88％。（3）我们展示了LLMs在逆向问题设置中往往无法纠正用户的错误法律假设。（4）我们提供了证据表明LLMs并不总能预测或并不总知道...

    Large language models (LLMs) have the potential to transform the practice of law, but this potential is threatened by the presence of legal hallucinations -- responses from these models that are not consistent with legal facts. We investigate the extent of these hallucinations using an original suite of legal queries, comparing LLMs' responses to structured legal metadata and examining their consistency. Our work makes four key contributions: (1) We develop a typology of legal hallucinations, providing a conceptual framework for future research in this area. (2) We find that legal hallucinations are alarmingly prevalent, occurring between 69% of the time with ChatGPT 3.5 and 88% with Llama 2, when these models are asked specific, verifiable questions about random federal court cases. (3) We illustrate that LLMs often fail to correct a user's incorrect legal assumptions in a contra-factual question setup. (4) We provide evidence that LLMs cannot always predict, or do not always know, wh
    
[^20]: 通过Fenchel对偶实现多样的离线模仿

    Diverse Offline Imitation via Fenchel Duality. (arXiv:2307.11373v1 [cs.LG])

    [http://arxiv.org/abs/2307.11373](http://arxiv.org/abs/2307.11373)

    本文提出了一个离线技能发现算法，通过Fenchel对偶方法将强化学习和无监督技能发现结合起来，实现学习与专家相一致的多样的技能。

    

    在无监督技能发现领域，最近取得了显著进展，各种工作提出了以互信息为基础的目标，作为内在驱动。先前的工作主要集中在设计需要在线环境访问的算法。相比之下，我们开发了一个\textit{离线}技能发现算法。我们的问题形式化考虑了在KL-散度约束下最大化互信息目标。更确切地说，约束确保每个技能的状态占用保持在一个具有良好状态操作覆盖率的离线数据集的支持范围内与专家的状态占用逼近。我们的主要贡献是连接Fenchel对偶、强化学习和无监督技能发现，并给出一个简单的离线算法，用于学习与专家相一致的多样的技能。

    There has been significant recent progress in the area of unsupervised skill discovery, with various works proposing mutual information based objectives, as a source of intrinsic motivation. Prior works predominantly focused on designing algorithms that require online access to the environment. In contrast, we develop an \textit{offline} skill discovery algorithm. Our problem formulation considers the maximization of a mutual information objective constrained by a KL-divergence. More precisely, the constraints ensure that the state occupancy of each skill remains close to the state occupancy of an expert, within the support of an offline dataset with good state-action coverage. Our main contribution is to connect Fenchel duality, reinforcement learning and unsupervised skill discovery, and to give a simple offline algorithm for learning diverse skills that are aligned with an expert.
    
[^21]: RL4CO: 用于组合优化的广泛强化学习基准测试

    RL4CO: an Extensive Reinforcement Learning for Combinatorial Optimization Benchmark. (arXiv:2306.17100v1 [cs.LG])

    [http://arxiv.org/abs/2306.17100](http://arxiv.org/abs/2306.17100)

    RL4CO是一个用于组合优化的广泛强化学习基准测试，着重于可扩展性和泛化能力的评估，并展示了一些最新方法在样本效率和适应不同数据分布方面的表现相对较差，强调了对神经CO求解器性能的平衡评估的重要性。

    

    我们引入了RL4CO，这是一个广泛的强化学习（RL）用于组合优化（CO）的基准测试。RL4CO采用最先进的软件库和最佳实践，如模块化和配置管理，以便研究人员可以轻松修改神经网络架构、环境和算法。与现有的专注于特定任务（如旅行推销员问题）进行性能评估的方法不同，我们强调可扩展性和泛化能力对于各种优化任务的重要性。我们还系统地评估了各种模型在样本效率、零-shot泛化和适应不同数据分布方面的表现。我们的实验结果表明，一些最新的最先进方法在使用这些新指标进行评估时落后于之前的方法，这表明有必要更加平衡地评估神经CO求解器的性能。我们希望RL4CO能够为研究人员提供一个综合性的基准测试工具，以进一步推动强化学习在组合优化领域的研究。

    We introduce RL4CO, an extensive reinforcement learning (RL) for combinatorial optimization (CO) benchmark. RL4CO employs state-of-the-art software libraries as well as best practices in implementation, such as modularity and configuration management, to be efficient and easily modifiable by researchers for adaptations of neural network architecture, environments, and algorithms. Contrary to the existing focus on specific tasks like the traveling salesman problem (TSP) for performance assessment, we underline the importance of scalability and generalization capabilities for diverse optimization tasks. We also systematically benchmark sample efficiency, zero-shot generalization, and adaptability to changes in data distributions of various models. Our experiments show that some recent state-of-the-art methods fall behind their predecessors when evaluated using these new metrics, suggesting the necessity for a more balanced view of the performance of neural CO solvers. We hope RL4CO will 
    
[^22]: SRL: 将分布式强化学习扩展到一万多个核心

    SRL: Scaling Distributed Reinforcement Learning to Over Ten Thousand Cores. (arXiv:2306.16688v1 [cs.DC])

    [http://arxiv.org/abs/2306.16688](http://arxiv.org/abs/2306.16688)

    SRL是一个可扩展，高效，可扩展的分布式强化学习系统，通过一种新的抽象框架统一了各种实际强化学习训练，并实现了精细优化。

    

    强化学习（RL）任务的不断复杂化要求分布式RL系统可以高效地生成和处理大量数据以训练智能Agent。然而，现有的开源库存在各种限制，阻碍了它们在需要大规模训练的挑战性场景中的实际应用。虽然OpenAI和DeepMind的工业系统已经成功实现了大规模RL训练，但是它们的系统架构和实现细节对社区来说仍然不公开。在本文中，我们提出了RL训练数据流的新抽象，将各种应用中的实际RL训练统一成一个通用框架，并实现了精细优化。根据这个抽象，我们开发了一个可扩展、高效、可扩展的分布式RL系统，名为"ReaLly Scalable RL（SRL）"。

    The ever-growing complexity of reinforcement learning (RL) tasks demands a distributed RL system to efficiently generate and process a massive amount of data to train intelligent agents. However, existing open-source libraries suffer from various limitations, which impede their practical use in challenging scenarios where large-scale training is necessary. While industrial systems from OpenAI and DeepMind have achieved successful large-scale RL training, their system architecture and implementation details remain undisclosed to the community. In this paper, we present a novel abstraction on the dataflows of RL training, which unifies practical RL training across diverse applications into a general framework and enables fine-grained optimizations. Following this abstraction, we develop a scalable, efficient, and extensible distributed RL system called ReaLly Scalable RL (SRL). The system architecture of SRL separates major RL computation components and allows massively parallelized trai
    
[^23]: RoMo-HER: 鲁棒的基于模型的事后经验回放方法

    RoMo-HER: Robust Model-based Hindsight Experience Replay. (arXiv:2306.16061v1 [cs.RO])

    [http://arxiv.org/abs/2306.16061](http://arxiv.org/abs/2306.16061)

    RoMo-HER是一个鲁棒的基于模型的事后经验回放方法，通过使用机器人操作环境中的动力学模型和前瞻重新标记技术，提高了样本利用效率。

    

    在多目标强化学习中，稀疏奖励是导致样本利用效率低的因素之一。基于事后经验回放（HER），已经提出了基于模型的重新标记方法，通过与训练模型进行交互获取虚拟轨迹来重新标记目标，在准确可建模的稀疏奖励环境中能够有效增强样本利用效率。然而，在机器人操作环境中，它们是无效的。在我们的论文中，我们设计了一个称为RoMo-HER的鲁棒框架，它可以有效地利用机器人操作环境中的动力学模型来提高样本利用效率。RoMo-HER基于动力学模型和一种称为前瞻重新标记（FR）的新型目标重新标记技术构建，该技术通过特定策略选择预测起始状态，预测起始状态的未来轨迹，然后使用动力学模型和最新的信息重新标记目标。

    Sparse rewards are one of the factors leading to low sample efficiency in multi-goal reinforcement learning (RL). Based on Hindsight Experience Replay (HER), model-based relabeling methods have been proposed to relabel goals using virtual trajectories obtained by interacting with the trained model, which can effectively enhance the sample efficiency in accurately modelable sparse-reward environments. However, they are ineffective in robot manipulation environment. In our paper, we design a robust framework called Robust Model-based Hindsight Experience Replay (RoMo-HER) which can effectively utilize the dynamical model in robot manipulation environments to enhance the sample efficiency. RoMo-HER is built upon a dynamics model and a novel goal relabeling technique called Foresight relabeling (FR), which selects the prediction starting state with a specific strategy, predicts the future trajectory of the starting state, and then relabels the goal using the dynamics model and the latest p
    
[^24]: FedMLSecurity：联邦学习与LLMs中攻击与防御的基准测试

    FedMLSecurity: A Benchmark for Attacks and Defenses in Federated Learning and LLMs. (arXiv:2306.04959v1 [cs.CR])

    [http://arxiv.org/abs/2306.04959](http://arxiv.org/abs/2306.04959)

    本文介绍了一个名为FedMLSecurity的基准测试，它可以模拟在联邦学习中可能出现的对抗攻击并提供相应的防御策略。该测试对各种机器学习模型和联合优化器都可以适用，并且能够轻松应用于大规模语言模型中。

    

    本文介绍了FedMLSecurity，这是一个在联邦学习（FL）中模拟对抗攻击和相应防御机制的基准测试。作为开源库FedML的一个重要模块，FedMLSecurity增强了FedML的安全评估能力。FedMLSecurity包含两个主要组件：FedMLAttacker模拟在FL训练中注入的攻击，而FedMLDefender则模拟旨在减轻攻击影响的防御策略。FedMLSecurity是开源的，可适用于各种机器学习模型（例如逻辑回归，ResNet，GAN等）和联合优化器（例如FedAVG，FedOPT，FedNOVA等）。本文的实验评估还展示了将FedMLSecurity轻松应用于LLMs的便利性，进一步强化了其各种场景下的通用性和实用性。

    This paper introduces FedMLSecurity, a benchmark that simulates adversarial attacks and corresponding defense mechanisms in Federated Learning (FL). As an integral module of the open-sourced library FedML that facilitates FL algorithm development and performance comparison, FedMLSecurity enhances the security assessment capacity of FedML. FedMLSecurity comprises two principal components: FedMLAttacker, which simulates attacks injected into FL training, and FedMLDefender, which emulates defensive strategies designed to mitigate the impacts of the attacks. FedMLSecurity is open-sourced 1 and is customizable to a wide range of machine learning models (e.g., Logistic Regression, ResNet, GAN, etc.) and federated optimizers (e.g., FedAVG, FedOPT, FedNOVA, etc.). Experimental evaluations in this paper also demonstrate the ease of application of FedMLSecurity to Large Language Models (LLMs), further reinforcing its versatility and practical utility in various scenarios.
    
[^25]: 知识的知识：探索大型语言模型对未知-已知不确定性的理解

    Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty with Large Language Models. (arXiv:2305.13712v1 [cs.CL])

    [http://arxiv.org/abs/2305.13712](http://arxiv.org/abs/2305.13712)

    本文探索了大型语言模型对其自身知识的理解和测量不确定性的能力。该研究聚焦于解决“已知-未知”问题，提出了新的分类方案，并使用语义评估方法量化了模型表达不确定性的准确性。

    

    本文研究了大型语言模型（LLM）在理解自身知识和测量不确定性方面的能力，以缓解虚构现象。我们专门关注解决“已知-未知”问题，这种问题由于缺乏确定的答案而具有高度不确定性。为了促进我们的研究，我们收集了一个新的已知-未知问题（KUQ）数据集，并提出了一个新的分类方案来阐明不确定性的来源。随后，我们评估LLM区分已知和未知问题以及相应分类的能力。此外，我们在开放式QA环境中评估LLM的答案质量。为了量化答案中表达的不确定性，我们创建了一种语义评估方法，用于测量模型在表达已知vs未知问题的不确定性方面的准确性。

    This paper investigates the capabilities of Large Language Models (LLMs) in the context of understanding their own knowledge and measuring their uncertainty. We argue this is an important feature for mitigating hallucinations. Specifically, we focus on addressing \textit{known-unknown} questions, characterized by high uncertainty due to the absence of definitive answers. To facilitate our study, we collect a dataset with new Known-Unknown Questions (KUQ) and propose a novel categorization scheme to elucidate the sources of uncertainty. Subsequently, we assess the LLMs' ability to differentiate between known and unknown questions and classify them accordingly. Moreover, we evaluate the quality of their answers in an Open-Ended QA setting. To quantify the uncertainty expressed in the answers, we create a semantic evaluation method that measures the model's accuracy in expressing uncertainty between known vs unknown questions.
    
[^26]: 利用基于空间概念的拓扑语义映射进行从语音指令的分层路径规划

    Hierarchical Path-planning from Speech Instructions with Spatial Concept-based Topometric Semantic Mapping. (arXiv:2203.10820v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2203.10820](http://arxiv.org/abs/2203.10820)

    该论文提出了一种基于拓扑语义映射的分层路径规划方法，可以通过语音指令和waypoint灵活地规划路径，包括地点连通性，提供了一种快速近似推理方法。

    

    在现实世界中，使用人类语音指令进行机器人导航至目的地是自主移动机器人至关重要的。然而，机器人可以采用不同的路径到达同一目标，而最短路径不一定是最优的。因此需要一种方法来灵活地接受waypoint的指标，规划更好的替代路径，即使需要绕路。此外，机器人需要实时推理能力。本研究旨在通过拓扑语义映射实现一个分层的空间表示，并结合语音指令和waypoint进行路径规划。我们提出了SpCoTMHP，一种利用多模式空间概念的层次路径规划方法，包括地点连通性。这种方法提供了一种新颖的集成概率生成模型和快速近似推理方法，层次结构中的各个级别之间可以相互交互。

    Navigating to destinations using human speech instructions is essential for autonomous mobile robots operating in the real world. Although robots can take different paths toward the same goal, the shortest path is not always optimal. A desired approach is to flexibly accommodate waypoint specifications, planning a better alternative path, even with detours. Furthermore, robots require real-time inference capabilities. Spatial representations include semantic, topological, and metric levels, each capturing different aspects of the environment. This study aims to realize a hierarchical spatial representation by a topometric semantic map and path planning with speech instructions, including waypoints. We propose SpCoTMHP, a hierarchical path-planning method that utilizes multimodal spatial concepts, incorporating place connectivity. This approach provides a novel integrated probabilistic generative model and fast approximate inference, with interaction among the hierarchy levels. A formul
    

