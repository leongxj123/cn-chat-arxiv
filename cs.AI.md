# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Voice EHR: Introducing Multimodal Audio Data for Health](https://arxiv.org/abs/2404.01620) | 本报告引入了一种通过引导问题使用移动应用程序捕获健康数据的新的音频电子健康记录（voice EHR），可能包含复杂的健康生物标志物，从而弥补了单一模态临床数据的典型限制。 |
| [^2] | [Reinforcement Learning-based Receding Horizon Control using Adaptive Control Barrier Functions for Safety-Critical Systems](https://arxiv.org/abs/2403.17338) | 提出了一种基于强化学习的滚动视野控制方法，利用自适应控制屏障函数，以解决安全关键系统中性能和可行性受影响的问题 |
| [^3] | [MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual Language Modeling](https://arxiv.org/abs/2403.10691) | MYTE是一种基于形态学的字节编码范式，通过使用具有一致大小的片段来实现跨不同语言的信息编码，为99种语言提供了更短的编码，特别是对非欧洲语言和非拉丁文字的改进最为显著。 |
| [^4] | [Meaningful Learning: Advancing Abstract Reasoning in Large Language Models via Generic Fact Guidance](https://arxiv.org/abs/2403.09085) | 设计了一个抽象推理数据集和有意义学习范式，教导大型语言模型如何利用通用事实进行推理，有效提升了抽象推理能力。 |
| [^5] | [Advantage-Aware Policy Optimization for Offline Reinforcement Learning](https://arxiv.org/abs/2403.07262) | 介绍了一种新的适应优势的策略优化（A2PO）方法，用于离线学习，能够解决多行为策略收集的约束冲突问题，有效避免过拟合问题。 |
| [^6] | [Evolving to the Future: Unseen Event Adaptive Fake News Detection on Social Media](https://arxiv.org/abs/2403.00037) | 提出了面向未知事件的适应性假新闻检测框架FADE，通过自适应增强和图对比学习训练目标预测器，同时独立训练事件预测器，最终减轻事件偏见。 |
| [^7] | [Fine Structure-Aware Sampling: A New Sampling Training Scheme for Pixel-Aligned Implicit Models in Single-View Human Reconstruction](https://arxiv.org/abs/2402.19197) | FSS是一种新的用于单视图人体重建中像素对齐隐式模型的采样训练方案，通过主动适应表面的厚度和复杂性，以及利用样本点的法线来改善结果，同时引入网格厚度损失信号来进一步改进训练过程。 |
| [^8] | [TimeXer: Empowering Transformers for Time Series Forecasting with Exogenous Variables](https://arxiv.org/abs/2402.19072) | 本文提出了一个新框架TimeXer，利用外部信息增强变压器对内生变量进行预测，弥补了以往多变量或单变量预测中忽视外生信息的不足。 |
| [^9] | [How do Large Language Models Handle Multilingualism?](https://arxiv.org/abs/2402.18815) | 大型语言模型展示了处理多语言任务的出色性能，研究发现在不同层次中处理多语言输入的策略，以及处理特定语言时的语言特定神经元存在。 |
| [^10] | [Importance Guided Data Augmentation for Neural-Based Code Understanding](https://arxiv.org/abs/2402.15769) | 引入了一个通用数据增强框架GenCode，通过重要性指标选择生成的代码作为训练数据，以增强代码理解模型的训练。 |
| [^11] | [AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning](https://arxiv.org/abs/2402.15506) | AgentOhana提供了一种统一数据和训练流水线的综合解决方案，有助于克服使用大型语言模型（LLMs）进行智能体任务时的挑战。 |
| [^12] | [Do Efficient Transformers Really Save Computation?](https://arxiv.org/abs/2402.13934) | 本研究旨在理解高效Transformer（例如稀疏Transformer和线性Transformer）的能力和限制，发现它们适合解决一般DP任务，但不同于标准Transformer。 |
| [^13] | [Improved Generalization of Weight Space Networks via Augmentations](https://arxiv.org/abs/2402.04081) | 通过扩充权重空间的数据集，采用MixUp方法，我们改进了权重空间网络的泛化能力和性能。 |
| [^14] | [Beyond Text: Improving LLM's Decision Making for Robot Navigation via Vocal Cues](https://arxiv.org/abs/2402.03494) | 本论文通过将语音转录和语音非言语特征整合到LLM决策中来改善机器人导航中的决策能力，超越了仅使用文字的限制。 |
| [^15] | [A Framework for Partially Observed Reward-States in RLHF](https://arxiv.org/abs/2402.03282) | 这篇论文提出了一个针对RLHF的框架，在其中考虑了部分观察到的奖励状态，并通过将基数反馈和决斗反馈缩减为PORRL形式进行了建模和算法开发。 |
| [^16] | [Efficient Online Scheduling and Routing for Automated Guided Vehicles In Loop-Based Graphs](https://arxiv.org/abs/2310.02195) | 提出了一种用于自动引导车的在线、无冲突调度和路径规划问题的基于循环图的算法，实验结果表明该算法要么优于其他算法，要么在更短的计算时间内获得同样良好的解决方案 |
| [^17] | [Health Text Simplification: An Annotated Corpus for Digestive Cancer Education and Novel Strategies for Reinforcement Learning.](http://arxiv.org/abs/2401.15043) | 该论文介绍了一个用于健康文本简化研究的消化癌症教育材料的注释语料库，并探索了基于大型语言模型的简化方法，包括微调、增强学习、增强学习与人类反馈、领域自适应和基于提示的应用。 |
| [^18] | [Programming Distributed Collective Processes in the eXchange Calculus.](http://arxiv.org/abs/2401.11212) | 本研究在交换演算中考虑了集合设备的动态合作行为，提出了分布式集体过程的抽象表示，用于编程计算集体的行为。 |
| [^19] | [MacroSwarm: A Field-based Compositional Framework for Swarm Programming.](http://arxiv.org/abs/2401.10969) | MacroSwarm是一种基于场的群体编程框架，通过可组合的功能模块实现复杂的群体行为，通过将感知场映射为执行目标场，提供了一种系统化的设计和实现群体行为的方法。 |
| [^20] | [DeepEdit: Knowledge Editing as Decoding with Constraints.](http://arxiv.org/abs/2401.10471) | DeepEdit是一种神经符号方法，通过更好的推理一致性和对更新知识的意识，提高了大型语言模型的知识编辑能力，对多跳问题数据集MQuaKE取得了显著的进展。 |
| [^21] | [E3x: $\mathrm{E}(3)$-Equivariant Deep Learning Made Easy.](http://arxiv.org/abs/2401.07595) | E3x是一种简化了$\mathrm{E}(3)$等变深度学习的软件包，通过内置等变性实现更高的数据效率和准确性。 |
| [^22] | [End-to-end Learnable Clustering for Intent Learning in Recommendation.](http://arxiv.org/abs/2401.05975) | 本文提出了一种用于推荐中意图学习的端到端可学习聚类方法ELCRec，该方法解决了现有方法中的复杂优化问题和大规模数据集聚类的可扩展性问题。 |
| [^23] | [Hierarchical Randomized Smoothing.](http://arxiv.org/abs/2310.16221) | 分层随机平滑是一种在复杂数据上进行鲁棒性认证的解决方案，通过只在一个对象的子集上添加随机噪声，以更有针对性的方式提供了更强的鲁棒性保证和高准确性。 |
| [^24] | [Hessian-Aware Bayesian Optimization for Decision Making Systems.](http://arxiv.org/abs/2308.00629) | 本文介绍了一种感知海森贝叶斯优化算法，旨在解决决策系统优化中梯度反馈稀缺或无效的问题。通过引入紧凑的多层架构和角色概念，并利用感知海森贝叶斯优化方法对参数进行优化，作者实现了对复杂决策系统的高效优化。 |
| [^25] | [RoCar: A Relationship Network-based Evaluation Method to Large Language Models.](http://arxiv.org/abs/2307.15997) | RoCar是一种利用关系网络构建任务图并生成自然语言评估任务的方法，用于评估大型语言模型的推理和记忆能力。该方法通过极大的随机性确保了评估的公平性。 |
| [^26] | [Learning in Repeated Multi-Unit Pay-As-Bid Auctions.](http://arxiv.org/abs/2307.15193) | 本论文研究了在重复的多单位付费拍卖中学习如何出价的问题。通过在离线设置中优化出价向量，并利用多项式时间动态规划方案，设计了具有多项式时间和空间复杂度的在线学习算法。 |
| [^27] | [Evaluation of Environmental Conditions on Object Detection using Oriented Bounding Boxes for AR Applications.](http://arxiv.org/abs/2306.16798) | 本研究提出了一种使用定向边界框和深度网络进行物体检测的新方法，通过在不同环境条件下对两个数据集的评估发现，该方法在处理小物体时能够提高性能和准确性。 |
| [^28] | [MixMask: Revisiting Masking Strategy for Siamese ConvNets.](http://arxiv.org/abs/2210.11456) | 本文提出了一种新的填充式遮盖策略MixMask，在Siamese ConvNets中实现遮盖和对比学习目标的匹配，提高了Siamese ConvNets的性能并在多个基准测试中实现了最先进的结果。 |

# 详细

[^1]: Voice EHR:引入多模式音频数据用于健康

    Voice EHR: Introducing Multimodal Audio Data for Health

    [https://arxiv.org/abs/2404.01620](https://arxiv.org/abs/2404.01620)

    本报告引入了一种通过引导问题使用移动应用程序捕获健康数据的新的音频电子健康记录（voice EHR），可能包含复杂的健康生物标志物，从而弥补了单一模态临床数据的典型限制。

    

    在音频数据上训练的大型AI模型可能具有快速分类患者的潜力，通过早期检测增强医疗决策，并可能通过早期检测改善结果。现有技术依赖于在高收入、英语国家使用昂贵记录设备的有限数据集，这种技术面临资源受限、高收入场所的部署挑战，音频数据可能具有深远影响。本报告介绍了一种新的数据类型和相应的收集系统，通过引导问题仅使用移动应用/网络应用程序捕获健康数据。该应用程序最终产生一个音频电子健康记录（voice EHR），它可能包含来自传统语音/呼吸特征、语音模式和具有语义意义的语言的复杂生物标志物，补偿单一模态临床数据的典型限制。本报告介绍了一个合作伙伴财团

    arXiv:2404.01620v1 Announce Type: cross  Abstract: Large AI models trained on audio data may have the potential to rapidly classify patients, enhancing medical decision-making and potentially improving outcomes through early detection. Existing technologies depend on limited datasets using expensive recording equipment in high-income, English-speaking countries. This challenges deployment in resource-constrained, high-volume settings where audio data may have a profound impact. This report introduces a novel data type and a corresponding collection system that captures health data through guided questions using only a mobile/web application. This application ultimately results in an audio electronic health record (voice EHR) which may contain complex biomarkers of health from conventional voice/respiratory features, speech patterns, and language with semantic meaning - compensating for the typical limitations of unimodal clinical datasets. This report introduces a consortium of partner
    
[^2]: 使用自适应控制屏障函数的基于强化学习的滚动视野控制用于安全关键系统

    Reinforcement Learning-based Receding Horizon Control using Adaptive Control Barrier Functions for Safety-Critical Systems

    [https://arxiv.org/abs/2403.17338](https://arxiv.org/abs/2403.17338)

    提出了一种基于强化学习的滚动视野控制方法，利用自适应控制屏障函数，以解决安全关键系统中性能和可行性受影响的问题

    

    最优控制方法为安全关键问题提供解决方案，但很容易变得棘手。控制屏障函数(CBFs)作为一种流行技术出现，通过其前向不变性属性，有利于通过在损失一些性能的情况下，显式地保证安全。该方法涉及定义性能目标以及必须始终执行的基于CBF的安全约束。遗憾的是，两个关键因素可能会对性能和解决方案的可行性产生显著影响：(i)成本函数及其相关参数的选择，以及(ii)在CBF约束内进行参数校准，捕捉性能和保守性之间的折衷，以及不可行性。为了解决这些挑战，我们提出了一种利用模型预测控制(MPC)的强化学习(RL)滚动视野控制(RHC)方法。

    arXiv:2403.17338v1 Announce Type: cross  Abstract: Optimal control methods provide solutions to safety-critical problems but easily become intractable. Control Barrier Functions (CBFs) have emerged as a popular technique that facilitates their solution by provably guaranteeing safety, through their forward invariance property, at the expense of some performance loss. This approach involves defining a performance objective alongside CBF-based safety constraints that must always be enforced. Unfortunately, both performance and solution feasibility can be significantly impacted by two key factors: (i) the selection of the cost function and associated parameters, and (ii) the calibration of parameters within the CBF-based constraints, which capture the trade-off between performance and conservativeness. %as well as infeasibility. To address these challenges, we propose a Reinforcement Learning (RL)-based Receding Horizon Control (RHC) approach leveraging Model Predictive Control (MPC) with
    
[^3]: MYTE：形态学驱动的字节编码，用于更好、更公平的多语言语言建模

    MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual Language Modeling

    [https://arxiv.org/abs/2403.10691](https://arxiv.org/abs/2403.10691)

    MYTE是一种基于形态学的字节编码范式，通过使用具有一致大小的片段来实现跨不同语言的信息编码，为99种语言提供了更短的编码，特别是对非欧洲语言和非拉丁文字的改进最为显著。

    

    多语言语言建模中的一个主要考虑因素是如何最好地表示具有不同词汇和文字的语言。尽管当代文本编码方法涵盖了大多数世界文字系统，但它们存在偏向于全球西方高资源语言的问题。因此，少数语言的文本往往被分割为一长串在语言学上毫无意义的单元。为了解决这种不平等，我们引入了一种新的范式，用跨不同语言具有一致大小的片段来编码相同的信息。我们的编码约定（MYTE）基于形态素，因为它们的库存在各种语言中比字符更平衡，而以前的方法使用字符。我们展示MYTE为所有99种分析语言产生了更短的编码，其中非欧洲语言和非拉丁文字的改进最为显著。这进而改善了多语言语言建模的性能。

    arXiv:2403.10691v1 Announce Type: cross  Abstract: A major consideration in multilingual language modeling is how to best represent languages with diverse vocabularies and scripts. Although contemporary text encoding methods cover most of the world's writing systems, they exhibit bias towards the high-resource languages of the Global West. As a result, texts of underrepresented languages tend to be segmented into long sequences of linguistically meaningless units. To address the disparities, we introduce a new paradigm that encodes the same information with segments of consistent size across diverse languages. Our encoding convention (MYTE) is based on morphemes, as their inventories are more balanced across languages than characters, which are used in previous methods. We show that MYTE produces shorter encodings for all 99 analyzed languages, with the most notable improvements for non-European languages and non-Latin scripts. This, in turn, improves multilingual LM performance and di
    
[^4]: 有意义学习：通过通用事实引导推进大型语言模型的抽象推理

    Meaningful Learning: Advancing Abstract Reasoning in Large Language Models via Generic Fact Guidance

    [https://arxiv.org/abs/2403.09085](https://arxiv.org/abs/2403.09085)

    设计了一个抽象推理数据集和有意义学习范式，教导大型语言模型如何利用通用事实进行推理，有效提升了抽象推理能力。

    

    大型语言模型（LLMs）在各种推理场景中取得了令人印象深刻的性能和强大的可解释性，标志着朝着模拟人类智能迈出了重要的一步。然而，当面对由通用事实支持的简单问题时，LLMs经常未能提供一致和准确的答案，表明其存在抽象推理能力的不足。这引发了关于LLMs到底是在真正推理还是仅仅在记忆的激烈争论。鉴此，我们设计了一个初步研究来量化并深入探讨现有LLMs的抽象推理能力。我们的研究发现显示出它们的一般推理和抽象推理表现之间存在实质性差异。为了缓解这一问题，我们为大型语言模型定制了一个抽象推理数据集（AbsR），结合有意义的学习范式，教会LLMs如何利用通用事实进行推理。结果表明我们的方法能够显着改善LLMs在抽象推理中的表现。

    arXiv:2403.09085v1 Announce Type: cross  Abstract: Large language models (LLMs) have developed impressive performance and strong explainability across various reasoning scenarios, marking a significant stride towards mimicking human-like intelligence. Despite this, when tasked with simple questions supported by a generic fact, LLMs often fail to provide consistent and precise answers, indicating a deficiency in abstract reasoning abilities. This has sparked a vigorous debate about whether LLMs are genuinely reasoning or merely memorizing. In light of this, we design a preliminary study to quantify and delve into the abstract reasoning abilities of existing LLMs. Our findings reveal a substantial discrepancy between their general reasoning and abstract reasoning performances. To relieve this problem, we tailor an abstract reasoning dataset (AbsR) together with a meaningful learning paradigm to teach LLMs how to leverage generic facts for reasoning purposes. The results show that our app
    
[^5]: 适应优势的策略优化用于离线强化学习

    Advantage-Aware Policy Optimization for Offline Reinforcement Learning

    [https://arxiv.org/abs/2403.07262](https://arxiv.org/abs/2403.07262)

    介绍了一种新的适应优势的策略优化（A2PO）方法，用于离线学习，能够解决多行为策略收集的约束冲突问题，有效避免过拟合问题。

    

    离线强化学习致力于利用离线数据集来制定有效的智能体策略，而无需在线交互，通过在行为策略的支持下施加适当的保守约束来解决分布外问题。本文引入了一种新的适应优势的策略优化（A2PO）方法，以明确构建针对混合质量数据集的离线学习优势感知策略约束。

    arXiv:2403.07262v1 Announce Type: cross  Abstract: Offline Reinforcement Learning (RL) endeavors to leverage offline datasets to craft effective agent policy without online interaction, which imposes proper conservative constraints with the support of behavior policies to tackle the Out-Of-Distribution (OOD) problem. However, existing works often suffer from the constraint conflict issue when offline datasets are collected from multiple behavior policies, i.e., different behavior policies may exhibit inconsistent actions with distinct returns across the state space. To remedy this issue, recent Advantage-Weighted (AW) methods prioritize samples with high advantage values for agent training while inevitably leading to overfitting on these samples. In this paper, we introduce a novel Advantage-Aware Policy Optimization (A2PO) method to explicitly construct advantage-aware policy constraints for offline learning under mixed-quality datasets. Specifically, A2PO employs a Conditional Variat
    
[^6]: 未来发展：社交媒体上看不见事件的适应性假新闻检测

    Evolving to the Future: Unseen Event Adaptive Fake News Detection on Social Media

    [https://arxiv.org/abs/2403.00037](https://arxiv.org/abs/2403.00037)

    提出了面向未知事件的适应性假新闻检测框架FADE，通过自适应增强和图对比学习训练目标预测器，同时独立训练事件预测器，最终减轻事件偏见。

    

    随着社交媒体的快速发展，假新闻在社交媒体上的广泛传播日益威胁个人和社会。在社交媒体动态环境中，假新闻检测旨在开发一个模型，该模型在新闻报道过去事件的基础上进行训练。目标是预测和识别有关未来事件的假新闻，这些事件通常与过去完全不同。然而，现有的假新闻检测方法存在鲁棒性不足，无法泛化到看不见的事件。为了解决这个问题，我们引入了基于未来自适应事件的假新闻检测（FADE）框架。具体来说，我们通过自适应增强策略和图对比学习训练目标预测器，以进行更稳健的整体预测。同时，我们独立训练一个仅事件的预测器以获得有偏见的预测。然后，我们通过获得最终预测来进一步减轻事件偏见。

    arXiv:2403.00037v1 Announce Type: cross  Abstract: With the rapid development of social media, the wide dissemination of fake news on social media is increasingly threatening both individuals and society. In the dynamic landscape of social media, fake news detection aims to develop a model trained on news reporting past events. The objective is to predict and identify fake news about future events, which often relate to subjects entirely different from those in the past. However, existing fake detection methods exhibit a lack of robustness and cannot generalize to unseen events. To address this, we introduce Future ADaptive Event-based Fake news Detection (FADE) framework. Specifically, we train a target predictor through an adaptive augmentation strategy and graph contrastive learning to make more robust overall predictions. Simultaneously, we independently train an event-only predictor to obtain biased predictions. Then we further mitigate event bias by obtaining the final prediction
    
[^7]: 细结构感知采样: 一种新的用于单视图人体重建中像素对齐隐式模型的采样训练方案

    Fine Structure-Aware Sampling: A New Sampling Training Scheme for Pixel-Aligned Implicit Models in Single-View Human Reconstruction

    [https://arxiv.org/abs/2402.19197](https://arxiv.org/abs/2402.19197)

    FSS是一种新的用于单视图人体重建中像素对齐隐式模型的采样训练方案，通过主动适应表面的厚度和复杂性，以及利用样本点的法线来改善结果，同时引入网格厚度损失信号来进一步改进训练过程。

    

    像素对齐的隐式模型，如PIFu、PIFuHD和ICON，用于单视图着装人体重建。这些模型需要使用采样训练方案进行训练。现有的采样训练方案要么无法捕捉薄表面（如耳朵、手指），要么会导致重建网格中的噪声伪影。为解决这些问题，我们引入了细结构感知采样（FSS），这是一种新的用于单视图人体重建中训练像素对齐隐式模型的采样训练方案。FSS通过主动适应表面的厚度和复杂性来解决前述问题。此外，与现有的采样训练方案不同，FSS显示了如何利用样本点的法线在训练过程中提高结果。最后，为进一步改进训练过程，FSS提出了一个用于像素对齐隐式模型的网格厚度损失信号。这使得在训练过程中利用法线变得计算上可行。

    arXiv:2402.19197v1 Announce Type: cross  Abstract: Pixel-aligned implicit models, such as PIFu, PIFuHD, and ICON, are used for single-view clothed human reconstruction. These models need to be trained using a sampling training scheme. Existing sampling training schemes either fail to capture thin surfaces (e.g. ears, fingers) or cause noisy artefacts in reconstructed meshes. To address these problems, we introduce Fine Structured-Aware Sampling (FSS), a new sampling training scheme to train pixel-aligned implicit models for single-view human reconstruction. FSS resolves the aforementioned problems by proactively adapting to the thickness and complexity of surfaces. In addition, unlike existing sampling training schemes, FSS shows how normals of sample points can be capitalized in the training process to improve results. Lastly, to further improve the training process, FSS proposes a mesh thickness loss signal for pixel-aligned implicit models. It becomes computationally feasible to int
    
[^8]: TimeXer：利用外生变量增强变压器进行时间序列预测

    TimeXer: Empowering Transformers for Time Series Forecasting with Exogenous Variables

    [https://arxiv.org/abs/2402.19072](https://arxiv.org/abs/2402.19072)

    本文提出了一个新框架TimeXer，利用外部信息增强变压器对内生变量进行预测，弥补了以往多变量或单变量预测中忽视外生信息的不足。

    

    最近的研究表明，在时间序列预测方面取得了显著的性能。然而，由于现实应用的部分观测性质，仅专注于感兴趣的目标，也就是所谓的内生变量，通常是不足以保证准确预测的。值得注意的是，系统通常记录为多个变量，其中外生序列可以为内生变量提供有价值的外部信息。因此，与先前确立的多变量或单变量预测不同，它们要么将所有变量等同对待，要么忽视外生信息，本文关注的是一种实际设置，即具有外生变量的时间序列预测。我们提出了一个新颖的框架TimeXer，利用外部信息增强内生变量的预测。通过巧妙设计的嵌入层，TimeXer使传统的Transformer架构具有重新

    arXiv:2402.19072v1 Announce Type: cross  Abstract: Recent studies have demonstrated remarkable performance in time series forecasting. However, due to the partially-observed nature of real-world applications, solely focusing on the target of interest, so-called endogenous variables, is usually insufficient to guarantee accurate forecasting. Notably, a system is often recorded into multiple variables, where the exogenous series can provide valuable external information for endogenous variables. Thus, unlike prior well-established multivariate or univariate forecasting that either treats all the variables equally or overlooks exogenous information, this paper focuses on a practical setting, which is time series forecasting with exogenous variables. We propose a novel framework, TimeXer, to utilize external information to enhance the forecasting of endogenous variables. With a deftly designed embedding layer, TimeXer empowers the canonical Transformer architecture with the ability to reco
    
[^9]: 大型语言模型如何处理多语言？

    How do Large Language Models Handle Multilingualism?

    [https://arxiv.org/abs/2402.18815](https://arxiv.org/abs/2402.18815)

    大型语言模型展示了处理多语言任务的出色性能，研究发现在不同层次中处理多语言输入的策略，以及处理特定语言时的语言特定神经元存在。

    

    大型语言模型（LLMs）展现出在各种语言上出色的性能。本文探讨了一个问题：大型语言模型如何处理多语言？我们引入了一个框架，描述了LLMs处理多语言输入的过程：在前几层中，LLMs理解问题，将多语言输入转换为英语以便促进任务解决阶段。在中间层中，LLMs通过以英语思考并整合多语言知识来进行解决问题，利用自注意力和前馈结构，分别获取事实内容。在最后几层中，LLMs生成与查询的原始语言一致的响应。此外，我们研究了处理特定语言时特定语言神经元的存在。为了检测由输入语言激活的神经元，即使没有标签，我们创新性地设计了一个并行语言特定的

    arXiv:2402.18815v1 Announce Type: cross  Abstract: Large language models (LLMs) demonstrate remarkable performance across a spectrum of languages. In this work, we delve into the question: How do LLMs handle multilingualism? We introduce a framework that depicts LLMs' processing of multilingual inputs: In the first several layers, LLMs understand the question, converting multilingual inputs into English to facilitate the task-solving phase. In the intermediate layers, LLMs engage in problem-solving by thinking in English and incorporating multilingual knowledge to obtain factual content, leveraging the self-attention and feed-forward structures, respectively. In the last several layers, LLMs generate responses that align with the original language of the query. In addition, we investigate the existence of language-specific neurons when processing a certain language. To detect neurons activated by the input language, even without labels, we innovatively design a Parallel Language specif
    
[^10]: 重点引导的数据增强用于基于神经网络的代码理解

    Importance Guided Data Augmentation for Neural-Based Code Understanding

    [https://arxiv.org/abs/2402.15769](https://arxiv.org/abs/2402.15769)

    引入了一个通用数据增强框架GenCode，通过重要性指标选择生成的代码作为训练数据，以增强代码理解模型的训练。

    

    arXiv:2402.15769v1 类型：交叉 摘要：预训练的代码模型开启了代码智能时代。最近许多模型都表现出色。然而，在代码学习领域，一个重要问题是自动进行代码数据增强，以帮助开发者准备训练数据，这方面的研究尚不足。本文介绍了一个通用的数据增强框架GenCode，用于增强代码理解模型的训练。GenCode遵循一种生成和选择的范式来准备有用的训练代码。具体来说，它使用代码转换技术首先生成新的代码候选，然后通过重要性指标选择重要的代码作为训练数据。为了评估GenCode与通用重要性指标（损失值）的有效性，我们在四个代码理解任务（如代码克隆检测）和三个预训练代码模型（如CodeT5）上进行实验。与最先进的代码增强技术相比，

    arXiv:2402.15769v1 Announce Type: cross  Abstract: Pre-trained code models lead the era of code intelligence. Many models have been designed with impressive performance recently. However, one important problem, data augmentation for code data that automatically helps developers prepare training data lacks study in the field of code learning. In this paper, we introduce a general data augmentation framework, GenCode, to enhance the training of code understanding models. GenCode follows a generation-and-selection paradigm to prepare useful training codes. Specifically, it uses code transformation techniques to generate new code candidates first and then selects important ones as the training data by importance metrics. To evaluate the effectiveness of GenCode with a general importance metric -- loss value, we conduct experiments on four code understanding tasks (e.g., code clone detection) and three pre-trained code models (e.g., CodeT5). Compared to the state-of-the-art (SOTA) code augm
    
[^11]: AgentOhana：为有效智能体学习设计统一数据和训练流水线

    AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning

    [https://arxiv.org/abs/2402.15506](https://arxiv.org/abs/2402.15506)

    AgentOhana提供了一种统一数据和训练流水线的综合解决方案，有助于克服使用大型语言模型（LLMs）进行智能体任务时的挑战。

    

    由大型语言模型（LLMs）提供支持的自主智能体引起了重大研究关注。然而，充分利用LLMs的潜力进行基于智能体的任务面临困难，这是由于具有多轮轨迹的多样化数据源的异构性。在本文中，我们介绍AgentOhana作为解决这些挑战的综合解决方案。AgentOhana从不同环境中聚合智能体轨迹，涵盖了各种情景。它精心地将这些轨迹标准化和统一到一致的格式中，简化了为智能体训练优化的通用数据加载器的创建。通过数据统一，我们的训练流水线在不同数据源之间保持平衡，并在数据集划分和模型训练过程中保持设备之间的独立随机性。此外，我们还介绍了xLAM-v0.1，一个大动作模式

    arXiv:2402.15506v1 Announce Type: new  Abstract: Autonomous agents powered by large language models (LLMs) have garnered significant research attention. However, fully harnessing the potential of LLMs for agent-based tasks presents inherent challenges due to the heterogeneous nature of diverse data sources featuring multi-turn trajectories. In this paper, we introduce \textbf{AgentOhana} as a comprehensive solution to address these challenges. \textit{AgentOhana} aggregates agent trajectories from distinct environments, spanning a wide array of scenarios. It meticulously standardizes and unifies these trajectories into a consistent format, streamlining the creation of a generic data loader optimized for agent training. Leveraging the data unification, our training pipeline maintains equilibrium across different data sources and preserves independent randomness across devices during dataset partitioning and model training. Additionally, we present \textbf{xLAM-v0.1}, a large action mode
    
[^12]: 确实高效的Transformer能够节约计算吗？

    Do Efficient Transformers Really Save Computation?

    [https://arxiv.org/abs/2402.13934](https://arxiv.org/abs/2402.13934)

    本研究旨在理解高效Transformer（例如稀疏Transformer和线性Transformer）的能力和限制，发现它们适合解决一般DP任务，但不同于标准Transformer。

    

    随着基于Transformer的语言模型在越来越大的数据集上训练，并拥有大量参数，找到更高效的替代标准Transformer变得非常有价值。虽然已经提出了许多高效的Transformer和Transformer的替代方案，但没有一个能够提供它们适合替代标准Transformer的理论保证。这使得很难确定何时使用特定模型以及进一步研究的重点。在本文中，我们旨在理解高效Transformer的能力和局限性，特别是稀疏Transformer和线性Transformer。我们专注于它们在Chain-of-Thought (CoT)提示中展示的推理能力，并遵循先前的研究将它们建模为动态规划（DP）问题。我们的结果表明，虽然这些模型足够表达解决一般DP任务的能力，但与标准Transformer不同

    arXiv:2402.13934v1 Announce Type: cross  Abstract: As transformer-based language models are trained on increasingly large datasets and with vast numbers of parameters, finding more efficient alternatives to the standard Transformer has become very valuable. While many efficient Transformers and Transformer alternatives have been proposed, none provide theoretical guarantees that they are a suitable replacement for the standard Transformer. This makes it challenging to identify when to use a specific model and what directions to prioritize for further investigation. In this paper, we aim to understand the capabilities and limitations of efficient Transformers, specifically the Sparse Transformer and the Linear Transformer. We focus on their reasoning capability as exhibited by Chain-of-Thought (CoT) prompts and follow previous works to model them as Dynamic Programming (DP) problems. Our results show that while these models are expressive enough to solve general DP tasks, contrary to ex
    
[^13]: 通过扩充改进权重空间网络的泛化能力

    Improved Generalization of Weight Space Networks via Augmentations

    [https://arxiv.org/abs/2402.04081](https://arxiv.org/abs/2402.04081)

    通过扩充权重空间的数据集，采用MixUp方法，我们改进了权重空间网络的泛化能力和性能。

    

    深度权重空间（DWS）中的学习是一个新兴的研究方向，神经网络通过处理其他神经网络的权重来进行学习，它在2D和3D神经场（INRs，NeRFs）以及对其他类型神经网络进行推理方面有广泛应用。然而，权重空间模型往往容易受到过拟合的影响。我们通过实证分析了过拟合的原因，并发现一个关键原因是DWS数据集的缺乏多样性。虽然一个给定的对象可以被许多不同的权重配置所表示，但典型的INR训练集未能捕捉到表示同一对象的不同INR之间的变异性。为了解决这个问题，我们探索了权重空间中的数据扩充策略，并提出了适用于权重空间的MixUp方法。我们在两个设置中证明了这些方法的有效性。在分类任务中，它们的性能提升类似于拥有多达10倍的数据量。在自监督对比学习中，它们产生了实质性的改进。

    Learning in deep weight spaces (DWS), where neural networks process the weights of other neural networks, is an emerging research direction, with applications to 2D and 3D neural fields (INRs, NeRFs), as well as making inferences about other types of neural networks. Unfortunately, weight space models tend to suffer from substantial overfitting. We empirically analyze the reasons for this overfitting and find that a key reason is the lack of diversity in DWS datasets. While a given object can be represented by many different weight configurations, typical INR training sets fail to capture variability across INRs that represent the same object. To address this, we explore strategies for data augmentation in weight spaces and propose a MixUp method adapted for weight spaces. We demonstrate the effectiveness of these methods in two setups. In classification, they improve performance similarly to having up to 10 times more data. In self-supervised contrastive learning, they yield substanti
    
[^14]: 超越文字：通过语音线索改善LLM在机器人导航中的决策能力

    Beyond Text: Improving LLM's Decision Making for Robot Navigation via Vocal Cues

    [https://arxiv.org/abs/2402.03494](https://arxiv.org/abs/2402.03494)

    本论文通过将语音转录和语音非言语特征整合到LLM决策中来改善机器人导航中的决策能力，超越了仅使用文字的限制。

    

    这项工作强调了基于文本的大规模语言模型（LLM）在人机交互中的关键缺点，表明仅使用文本作为对话的模态在此类应用中存在不足之处。虽然LLM在处理文本方面在这些人机对话中非常出色，但在社交导航等情境下，他们在处理口头指令的细微之处时遇到了困难，其中的歧义和不确定性可能会削弱对机器人和其他人工智能系统的信任。我们可以通过超越文字，并重点关注这些音频回应的语音非言语特征来解决这个问题。这些特征是口头交流中不涉及文字措辞的方面，通过表达方式传达意义和细微差别。我们提出了“超越文字”；一种通过集成音频转录以及这些特征的部分来改善LLM决策能力的方法，这些特征侧重情感和更与人机对话相关。

    This work highlights a critical shortcoming in text-based Large Language Models (LLMs) used for human-robot interaction, demonstrating that text alone as a conversation modality falls short in such applications. While LLMs excel in processing text in these human conversations, they struggle with the nuances of verbal instructions in scenarios like social navigation, where ambiguity and uncertainty can erode trust in robotic and other AI systems. We can address this shortcoming by moving beyond text and additionally focusing on the paralinguistic features of these audio responses. These features are the aspects of spoken communication that do not involve the literal wording (lexical content) but convey meaning and nuance through how something is said. We present "Beyond Text"; an approach that improves LLM decision-making by integrating audio transcription along with a subsection of these features, which focus on the affect and more relevant in human-robot conversations. This approach n
    
[^15]: 一个部分观察到的奖励状态在RLHF中的框架

    A Framework for Partially Observed Reward-States in RLHF

    [https://arxiv.org/abs/2402.03282](https://arxiv.org/abs/2402.03282)

    这篇论文提出了一个针对RLHF的框架，在其中考虑了部分观察到的奖励状态，并通过将基数反馈和决斗反馈缩减为PORRL形式进行了建模和算法开发。

    

    最近几年来，强化学习从人类反馈（RLHF）的研究因其在LLMs的发展中起到的作用而变得重要。神经科学研究表明，人类对刺激的反应已知依赖于部分观察到的“内部状态”。不幸的是，当前的RLHF模型没有考虑到这一点。此外，大多数RLHF模型没有考虑到中间反馈，在实证研究中变得越来越重要，可以帮助提高样本复杂性和对齐性。为了解决这些局限性，我们将RLHF建模为部分观察到的奖励状态的强化学习（PORRL）。我们展示了从RLHF中两种主要形式的人类反馈 - 基数反馈和决斗反馈到PORRL的缩减。对于基数反馈，我们开发了通用的统计高效算法，并将它们实例化为POR-UCRL和POR-UCBVI。对于决斗反馈，我们表明，简单的基数反馈缩减不能达到亚线性的决斗回归。

    The study of reinforcement learning from human feedback (RLHF) has gained prominence in recent years due to its role in the development of LLMs. Neuroscience research shows that human responses to stimuli are known to depend on partially-observed "internal states." Unfortunately current models of RLHF do not take take this into consideration. Moreover most RLHF models do not account for intermediate feedback, which is gaining importance in empirical work and can help improve both sample complexity and alignment. To address these limitations, we model RLHF as reinforcement learning with partially observed reward-states (PORRL). We show reductions from the the two dominant forms of human feedback in RLHF - cardinal and dueling feedback to PORRL. For cardinal feedback, we develop generic statistically efficient algorithms and instantiate them to present POR-UCRL and POR-UCBVI. For dueling feedback, we show that a naive reduction to cardinal feedback fails to achieve sublinear dueling regr
    
[^16]: 高效在线调度和路径规划：基于循环图的自动引导车

    Efficient Online Scheduling and Routing for Automated Guided Vehicles In Loop-Based Graphs

    [https://arxiv.org/abs/2310.02195](https://arxiv.org/abs/2310.02195)

    提出了一种用于自动引导车的在线、无冲突调度和路径规划问题的基于循环图的算法，实验结果表明该算法要么优于其他算法，要么在更短的计算时间内获得同样良好的解决方案

    

    自动引导车（AGVs）广泛应用于各行各业，以无冲突方式对它们进行调度和路径规划对于它们的高效运行至关重要。我们提出了一种基于循环图的算法，用于解决具有任意容量和顺序作业的AGVs的在线、无冲突调度和路径规划问题。该算法与精确方法、贪婪启发式方法和元启发式方法进行了比较。我们通过在代表实际制造厂的模型上使用理论和真实实例进行实验，证明了该算法要么优于其他算法，要么在更短的计算时间内获得同样良好的解决方案。

    arXiv:2310.02195v2 Announce Type: replace-cross  Abstract: Automated guided vehicles (AGVs) are widely used in various industries, and scheduling and routing them in a conflict-free manner is crucial to their efficient operation. We propose a loop-based algorithm that solves the online, conflict-free scheduling and routing problem for AGVs with any capacity and ordered jobs in loop-based graphs. The proposed algorithm is compared against an exact method, a greedy heuristic and a metaheuristic. We experimentally show, using theoretical and real instances on a model representing a real manufacturing plant, that this algorithm either outperforms the other algorithms or gets an equally good solution in less computing time.
    
[^17]: 健康文本简化：消化癌症教育的注释语料库和增强学习的新策略

    Health Text Simplification: An Annotated Corpus for Digestive Cancer Education and Novel Strategies for Reinforcement Learning. (arXiv:2401.15043v1 [cs.CL])

    [http://arxiv.org/abs/2401.15043](http://arxiv.org/abs/2401.15043)

    该论文介绍了一个用于健康文本简化研究的消化癌症教育材料的注释语料库，并探索了基于大型语言模型的简化方法，包括微调、增强学习、增强学习与人类反馈、领域自适应和基于提示的应用。

    

    目标：健康教育材料的阅读水平显著影响信息的可理解性和可接触性，特别是对于少数族裔人群。许多患者教育资源超过了广泛接受的标准的阅读水平和复杂性。在健康信息中，急需高性能的文本简化模型以增强传播和识字能力。这种需要在癌症教育中尤为迫切，有效的预防和筛查教育可以大大减少发病率和死亡率。方法：我们引入了简化的消化癌症（SimpleDC）并行语料库，用于健康文本简化研究。利用SimpleDC和现有的Med-EASi语料库，我们探索了基于大型语言模型（LLM）的简化方法，包括微调、增强学习（RL）、增强学习与人类反馈（RLHF）、领域自适应和基于提示的应用。

    Objective: The reading level of health educational materials significantly influences information understandability and accessibility, particularly for minoritized populations. Many patient educational resources surpass the reading level and complexity of widely accepted standards. There is a critical need for high-performing text simplification models in health information to enhance dissemination and literacy. This need is particularly acute in cancer education, where effective prevention and screening education can substantially reduce morbidity and mortality.  Methods: We introduce Simplified Digestive Cancer (SimpleDC), a parallel corpus of cancer education materials tailored for health text simplification research. Utilizing SimpleDC alongside the existing Med-EASi corpus, we explore Large Language Model (LLM)-based simplification methods, including fine-tuning, reinforcement learning (RL), reinforcement learning with human feedback (RLHF), domain adaptation, and prompt-based app
    
[^18]: 在交换演算中编程分布式集体过程

    Programming Distributed Collective Processes in the eXchange Calculus. (arXiv:2401.11212v1 [cs.DC])

    [http://arxiv.org/abs/2401.11212](http://arxiv.org/abs/2401.11212)

    本研究在交换演算中考虑了集合设备的动态合作行为，提出了分布式集体过程的抽象表示，用于编程计算集体的行为。

    

    最近的趋势如物联网（IoT）提出了在几乎所有环境中密集和多尺度部署计算设备的愿景。一个突出的工程挑战围绕着编程这种计算生态系统的集体自适应行为。这需要能够捕捉概念（动态合作设备群组）和集体任务（由合奏组执行的联合活动）的抽象。在这项工作中，我们考虑与邻居交互并以几乎同步的感知-计算-交互循环执行的设备集合，其中计算由一个将感知值和传入消息映射到输出和传出消息的单个程序给出。为了支持整个计算集体的编程，我们提出了分布式集体过程的抽象，它可以同时定义合奏组的形成逻辑和它的集体任务。我们在交换演算中形式化了这种抽象。

    Recent trends like the Internet of Things (IoT) suggest a vision of dense and multi-scale deployments of computing devices in nearly all kinds of environments. A prominent engineering challenge revolves around programming the collective adaptive behaviour of such computational ecosystems. This requires abstractions able to capture concepts like ensembles (dynamic groups of cooperating devices) and collective tasks (joint activities carried out by ensembles). In this work, we consider collections of devices interacting with neighbours and that execute in nearly-synchronised sense-compute-interact rounds, where the computation is given by a single program mapping sensing values and incoming messages to output and outcoming messages. To support programming whole computational collectives, we propose the abstraction of a distributed collective process, which can be used to define at once the ensemble formation logic and its collective task. We formalise the abstraction in the eXchange Calc
    
[^19]: MacroSwarm: 一种基于场的组合框架用于群体编程

    MacroSwarm: A Field-based Compositional Framework for Swarm Programming. (arXiv:2401.10969v1 [cs.AI])

    [http://arxiv.org/abs/2401.10969](http://arxiv.org/abs/2401.10969)

    MacroSwarm是一种基于场的群体编程框架，通过可组合的功能模块实现复杂的群体行为，通过将感知场映射为执行目标场，提供了一种系统化的设计和实现群体行为的方法。

    

    群体行为工程是一项旨在研究协调简单智能体团体内计算和行动的方法和技术，以实现复杂的全局目标，如图案形成、集体移动、聚类和分布式感知。尽管在群体（无人机、机器人、车辆）分析和工程方面取得了一些进展，但仍然需要通用的设计和实现方法和工具，以系统化的方式定义复杂的群体行为。为了对此做出贡献，本文提出了一种新的基于场的协调方法，称为MacroSwarm，以可重用且完全可组合的功能模块为基础，嵌入集体计算和协调。基于集成计算的宏编程范式，MacroSwarm提出了将每个群体行为块表示为将感知场映射为执行目标场的纯函数的思路。

    Swarm behaviour engineering is an area of research that seeks to investigate methods and techniques for coordinating computation and action within groups of simple agents to achieve complex global goals like pattern formation, collective movement, clustering, and distributed sensing. Despite recent progress in the analysis and engineering of swarms (of drones, robots, vehicles), there is still a need for general design and implementation methods and tools that can be used to define complex swarm behaviour in a principled way. To contribute to this quest, this article proposes a new field-based coordination approach, called MacroSwarm, to design and program swarm behaviour in terms of reusable and fully composable functional blocks embedding collective computation and coordination. Based on the macroprogramming paradigm of aggregate computing, MacroSwarm builds on the idea of expressing each swarm behaviour block as a pure function mapping sensing fields into actuation goal fields, e.g.
    
[^20]: DeepEdit: 带有约束的解码式知识编辑

    DeepEdit: Knowledge Editing as Decoding with Constraints. (arXiv:2401.10471v1 [cs.CL])

    [http://arxiv.org/abs/2401.10471](http://arxiv.org/abs/2401.10471)

    DeepEdit是一种神经符号方法，通过更好的推理一致性和对更新知识的意识，提高了大型语言模型的知识编辑能力，对多跳问题数据集MQuaKE取得了显著的进展。

    

    我们将大型语言模型（LLMs）的知识编辑视为带有约束的解码过程。我们提出了DeepEdit（基于深度优先搜索的渐进式解码知识编辑），这是一种神经符号方法，通过更好的推理一致性、问题相关性和对更新知识的意识来改进知识编辑。DeepEdit可灵活应用于所有黑盒LLMs：不需要访问模型参数、表示或输出词汇分布。DeepEdit逐步产生高质量的推理步骤，以实现有效的知识编辑。它利用深度优先搜索来修改LLMs的输出，从而提高输出对问题的相关性和对更新知识的意识。在知识编辑方面，DeepEdit在控制LLMs产生更简洁的推理方面表现出色。在MQuaKE上，DeepEdit在定量上取得了显著的进展，这是一个具有挑战性的多跳问题数据集。

    We develop a new perspective of knowledge editing for large language models (LLMs) as decoding with constraints. We propose DeepEdit (Depth-first Search based Progressive Decoding for Knowledge Editing), a neuro-symbolic method that improves knowledge editing with better coherence of reasoning, relevance to the question, and awareness of updated knowledge. DeepEdit can be flexibly applied to all black-box LLMs: it does not require any access to the model parameters, representations, or output vocabulary distributions. DeepEdit progressively produces the high-quality reasoning steps towards effective knowledge editing. It utilizes a depth-first search to revise the LLMs' output, which improves the output's informativeness to the input question and awareness of the updated knowledge. Qualitatively, DeepEdit effectively controls LLMs to produce more succinct reasoning in accord with knowledge editing. Quantitatively, DeepEdit yields significant gains on MQuaKE, a challenging multi-hop que
    
[^21]: E3x：简化的$\mathrm{E}(3)$等变深度学习

    E3x: $\mathrm{E}(3)$-Equivariant Deep Learning Made Easy. (arXiv:2401.07595v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.07595](http://arxiv.org/abs/2401.07595)

    E3x是一种简化了$\mathrm{E}(3)$等变深度学习的软件包，通过内置等变性实现更高的数据效率和准确性。

    

    本文介绍了E3x，一种用于构建神经网络的软件包，该网络在三维空间的平移、旋转和反射方面等变。与普通神经网络相比，$\mathrm{E}(3)$-等变模型在输入和/或输出数据是与三维对象相关的数量时具有优势。这是因为此类数量（例如位置）的数值通常取决于所选择的坐标系统。在参考系的变换下，这些值会可预测地发生变化，但对于普通的机器学习模型来说，学习其潜在规则可能很困难。使用内置的$\mathrm{E}(3)$-等变性，神经网络可以保证完全满足相关的变换规则，从而实现更高的数据效率和准确性。E3x的代码可从https://github.com/google-research/e3x获得，还提供了详细的文档和使用示例。

    This work introduces E3x, a software package for building neural networks that are equivariant with respect to the Euclidean group $\mathrm{E}(3)$, consisting of translations, rotations, and reflections of three-dimensional space. Compared to ordinary neural networks, $\mathrm{E}(3)$-equivariant models promise benefits whenever input and/or output data are quantities associated with three-dimensional objects. This is because the numeric values of such quantities (e.g. positions) typically depend on the chosen coordinate system. Under transformations of the reference frame, the values change predictably, but the underlying rules can be difficult to learn for ordinary machine learning models. With built-in $\mathrm{E}(3)$-equivariance, neural networks are guaranteed to satisfy the relevant transformation rules exactly, resulting in superior data efficiency and accuracy. The code for E3x is available from https://github.com/google-research/e3x, detailed documentation and usage examples ca
    
[^22]: 用于推荐中意图学习的端到端可学习聚类方法

    End-to-end Learnable Clustering for Intent Learning in Recommendation. (arXiv:2401.05975v1 [cs.IR])

    [http://arxiv.org/abs/2401.05975](http://arxiv.org/abs/2401.05975)

    本文提出了一种用于推荐中意图学习的端到端可学习聚类方法ELCRec，该方法解决了现有方法中的复杂优化问题和大规模数据集聚类的可扩展性问题。

    

    挖掘用户的意图在序列推荐中起着关键作用。最近的方法ICLRec使用对比学习和聚类来提取用户的潜在意图。尽管它已经显示出有效性，但现有的方法存在复杂和繁琐的交替优化问题，导致两个主要问题。首先，在广义期望最大化(EM)框架中分离表示学习和聚类优化经常导致次优性能。其次，在整个数据集上进行聚类会影响大规模行业数据的可扩展性。为了解决这些挑战，我们提出了一种新颖的意图学习方法，称为ELCRec，它将表示学习集成到一个端到端可学习聚类框架中进行推荐。

    Mining users' intents plays a crucial role in sequential recommendation. The recent approach, ICLRec, was introduced to extract underlying users' intents using contrastive learning and clustering. While it has shown effectiveness, the existing method suffers from complex and cumbersome alternating optimization, leading to two main issues. Firstly, the separation of representation learning and clustering optimization within a generalized expectation maximization (EM) framework often results in sub-optimal performance. Secondly, performing clustering on the entire dataset hampers scalability for large-scale industry data. To address these challenges, we propose a novel intent learning method called \underline{ELCRec}, which integrates representation learning into an \underline{E}nd-to-end \underline{L}earnable \underline{C}lustering framework for \underline{Rec}ommendation. Specifically, we encode users' behavior sequences and initialize the cluster centers as learnable network parameter
    
[^23]: 分层随机平滑

    Hierarchical Randomized Smoothing. (arXiv:2310.16221v1 [cs.LG])

    [http://arxiv.org/abs/2310.16221](http://arxiv.org/abs/2310.16221)

    分层随机平滑是一种在复杂数据上进行鲁棒性认证的解决方案，通过只在一个对象的子集上添加随机噪声，以更有针对性的方式提供了更强的鲁棒性保证和高准确性。

    

    真实世界的数据是复杂的，通常由可分解为多个实体的对象组成（例如，将图像分解为像素，将图形分解为相互连接的节点）。随机平滑是一种强大的框架，可以使模型在其输入的微小变化上具有证明的鲁棒性-通过在分类之前随机添加噪声来保证多数投票的鲁棒性。然而，当对手不是任意干扰整个对象（例如图像），而是对象的某个实体的子集（例如像素）时，通过随机平滑对这种复杂数据进行鲁棒性认证是具有挑战性的。作为解决方案，我们引入了分层随机平滑：我们通过仅在随机选择的实体子集上添加随机噪声来部分平滑对象。通过以比现有方法更有针对性的方式添加噪声，我们获得更强的鲁棒性保证，同时保持高准确性。我们使用不同的噪声分布初始化分层平滑，得到了新的鲁棒性保证。

    Real-world data is complex and often consists of objects that can be decomposed into multiple entities (e.g. images into pixels, graphs into interconnected nodes). Randomized smoothing is a powerful framework for making models provably robust against small changes to their inputs - by guaranteeing robustness of the majority vote when randomly adding noise before classification. Yet, certifying robustness on such complex data via randomized smoothing is challenging when adversaries do not arbitrarily perturb entire objects (e.g. images) but only a subset of their entities (e.g. pixels). As a solution, we introduce hierarchical randomized smoothing: We partially smooth objects by adding random noise only on a randomly selected subset of their entities. By adding noise in a more targeted manner than existing methods we obtain stronger robustness guarantees while maintaining high accuracy. We initialize hierarchical smoothing using different noising distributions, yielding novel robustness
    
[^24]: Hessian-Aware Bayesian Optimization for Decision Making Systems - 感知海森贝叶斯优化在决策系统中的应用

    Hessian-Aware Bayesian Optimization for Decision Making Systems. (arXiv:2308.00629v1 [cs.LG])

    [http://arxiv.org/abs/2308.00629](http://arxiv.org/abs/2308.00629)

    本文介绍了一种感知海森贝叶斯优化算法，旨在解决决策系统优化中梯度反馈稀缺或无效的问题。通过引入紧凑的多层架构和角色概念，并利用感知海森贝叶斯优化方法对参数进行优化，作者实现了对复杂决策系统的高效优化。

    

    许多优化决策系统的方法依赖于梯度方法，需要从环境中获取有信息量的反馈。然而，当反馈稀缺或者无信息时，这些方法可能导致性能较差。贝叶斯优化等无导数方法可以减少对梯度反馈质量的依赖，但在复杂决策系统的高维环境中往往难以扩展。如果系统需要多个参与者之间的互动来实现共同目标，这个问题就加剧了。为了解决维度问题，我们提出了一种紧凑的多层架构，通过角色的概念来建模参与者之间的动态。此外，我们还引入了感知海森贝叶斯优化来高效地优化由大量参数参数化的多层架构。实验结果表明，我们的方法(HA-GP-UCB)在效果上是有效的。

    Many approaches for optimizing decision making systems rely on gradient based methods requiring informative feedback from the environment. However, in the case where such feedback is sparse or uninformative, such approaches may result in poor performance. Derivative-free approaches such as Bayesian Optimization mitigate the dependency on the quality of gradient feedback, but are known to scale poorly in the high-dimension setting of complex decision making systems. This problem is exacerbated if the system requires interactions between several actors cooperating to accomplish a shared goal. To address the dimensionality challenge, we propose a compact multi-layered architecture modeling the dynamics of actor interactions through the concept of role. Additionally, we introduce Hessian-aware Bayesian Optimization to efficiently optimize the multi-layered architecture parameterized by a large number of parameters. Experimental results demonstrate that our method (HA-GP-UCB) works effectiv
    
[^25]: RoCar:一种基于关系网络的大型语言模型评估方法

    RoCar: A Relationship Network-based Evaluation Method to Large Language Models. (arXiv:2307.15997v1 [cs.CL])

    [http://arxiv.org/abs/2307.15997](http://arxiv.org/abs/2307.15997)

    RoCar是一种利用关系网络构建任务图并生成自然语言评估任务的方法，用于评估大型语言模型的推理和记忆能力。该方法通过极大的随机性确保了评估的公平性。

    

    大型语言模型（LLMs）受到越来越多的关注。然而，由于其能力的复杂性，如何合理评估LLMs的能力仍然是一个需要解决的任务。我们提出了RoCar方法，利用定义的基本模式随机构建一个任务图，并基于任务图生成自然语言评估任务，分别评估LLMs的推理和记忆能力。由于任务构建过程的极大随机性，可以确保被测试的LLMs中没有一个直接学习了评估任务，从而保证了评估方法的公平性。

    Large language models (LLMs) have received increasing attention. However, due to the complexity of its capabilities, how to rationally evaluate the capabilities of LLMs is still a task to be solved. We propose the RoCar method, which utilizes the defined basic schemas to randomly construct a task graph and generates natural language evaluation tasks based on the task graph to evaluate the reasoning and memory abilities of LLMs respectively. Due to the very large randomness of the task construction process, it is possible to ensure that none of the LLMs to be tested has directly learned the evaluation tasks, guaranteeing the fairness of the evaluation method.
    
[^26]: 在重复的多单位付费拍卖中学习

    Learning in Repeated Multi-Unit Pay-As-Bid Auctions. (arXiv:2307.15193v1 [cs.GT])

    [http://arxiv.org/abs/2307.15193](http://arxiv.org/abs/2307.15193)

    本论文研究了在重复的多单位付费拍卖中学习如何出价的问题。通过在离线设置中优化出价向量，并利用多项式时间动态规划方案，设计了具有多项式时间和空间复杂度的在线学习算法。

    

    受碳排放交易方案、国债拍卖和采购拍卖的启发，这些都涉及拍卖同质的多个单位，我们考虑了如何在重复的多单位付费拍卖中学习如何出价的问题。在每个拍卖中，大量（相同的）物品将被分配给最高的出价，每个中标价等于出价本身。由于行动空间的组合性质，学习如何在付费拍卖中出价是具有挑战性的。为了克服这个挑战，我们关注离线设置，其中投标人通过只能访问其他投标人过去提交的出价来优化他们的出价向量。我们证明了离线问题的最优解可以使用多项式时间动态规划（DP）方案来获得。我们利用DP方案的结构，设计了具有多项式时间和空间复杂度的在线学习算法。

    Motivated by Carbon Emissions Trading Schemes, Treasury Auctions, and Procurement Auctions, which all involve the auctioning of homogeneous multiple units, we consider the problem of learning how to bid in repeated multi-unit pay-as-bid auctions. In each of these auctions, a large number of (identical) items are to be allocated to the largest submitted bids, where the price of each of the winning bids is equal to the bid itself. The problem of learning how to bid in pay-as-bid auctions is challenging due to the combinatorial nature of the action space. We overcome this challenge by focusing on the offline setting, where the bidder optimizes their vector of bids while only having access to the past submitted bids by other bidders. We show that the optimal solution to the offline problem can be obtained using a polynomial time dynamic programming (DP) scheme. We leverage the structure of the DP scheme to design online learning algorithms with polynomial time and space complexity under fu
    
[^27]: 评估环境条件对使用定向边界框进行AR应用的物体检测的影响

    Evaluation of Environmental Conditions on Object Detection using Oriented Bounding Boxes for AR Applications. (arXiv:2306.16798v1 [cs.CV])

    [http://arxiv.org/abs/2306.16798](http://arxiv.org/abs/2306.16798)

    本研究提出了一种使用定向边界框和深度网络进行物体检测的新方法，通过在不同环境条件下对两个数据集的评估发现，该方法在处理小物体时能够提高性能和准确性。

    

    增强现实（AR）的目标是将数字内容添加到自然图像和视频中，以创建用户与环境之间的交互体验。场景分析和物体识别在AR中起着至关重要的作用，因为它们必须快速且准确地执行。本研究提出了一种新的方法，利用定向边界框与检测和识别深度网络相结合，以提高性能和处理时间。该方法使用两个数据集进行评估：一个常用于计算机视觉任务的真实图像数据集（DOTA数据集）和一个模拟不同环境、照明和采集条件的合成数据集。评估的重点是小物体，这些物体往往难以检测和识别。结果表明，所提出的方法在大多数测试条件下，对于小物体往往能产生更好的平均精度和更高的准确性。

    The objective of augmented reality (AR) is to add digital content to natural images and videos to create an interactive experience between the user and the environment. Scene analysis and object recognition play a crucial role in AR, as they must be performed quickly and accurately. In this study, a new approach is proposed that involves using oriented bounding boxes with a detection and recognition deep network to improve performance and processing time. The approach is evaluated using two datasets: a real image dataset (DOTA dataset) commonly used for computer vision tasks, and a synthetic dataset that simulates different environmental, lighting, and acquisition conditions. The focus of the evaluation is on small objects, which are difficult to detect and recognise. The results indicate that the proposed approach tends to produce better Average Precision and greater accuracy for small objects in most of the tested conditions.
    
[^28]: MixMask: 重新审视Siamese ConvNets的遮盖策略

    MixMask: Revisiting Masking Strategy for Siamese ConvNets. (arXiv:2210.11456v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.11456](http://arxiv.org/abs/2210.11456)

    本文提出了一种新的填充式遮盖策略MixMask，在Siamese ConvNets中实现遮盖和对比学习目标的匹配，提高了Siamese ConvNets的性能并在多个基准测试中实现了最先进的结果。

    

    最近自监督学习的进展将Masked Image Modeling（MIM）和Siamese网络整合成一个统一的框架，利用了两种技术的优点。然而，在Siamese ConvNets中应用传统的基于擦除的遮盖策略时，存在一些未解决的问题，包括（I）在连续处理数据时不能放弃不相关的遮盖区域，导致训练效率低于ViT模型;（II）基于擦除的遮盖与Siamese ConvNets中的对比学习目标不匹配，与MIM方法不同。本文提出了一种称为MixMask的填充式遮盖策略，以防止香草遮盖方法中图像中的随机遮盖区域导致信息不完整。此外，我们引入了一种灵活的损失函数设计，考虑两个不同混合视图之间的语义距离变化，以适应集成架构并防止遮盖和对比学习目标之间的不匹配。实验表明，MixMask显着提高了Siamese ConvNets的性能，并在几个基准测试中实现了最先进的结果。

    Recent advances in self-supervised learning have integrated Masked Image Modeling (MIM) and Siamese Networks into a unified framework that leverages the benefits of both techniques. However, several issues remain unaddressed when applying conventional erase-based masking with Siamese ConvNets. These include (I) the inability to drop uninformative masked regions in ConvNets as they process data continuously, resulting in low training efficiency compared to ViT models; and (II) the mismatch between erase-based masking and the contrastive-based objective in Siamese ConvNets, which differs from the MIM approach. In this paper, we propose a filling-based masking strategy called MixMask to prevent information incompleteness caused by the randomly erased regions in an image in the vanilla masking method. Furthermore, we introduce a flexible loss function design that considers the semantic distance change between two different mixed views to adapt the integrated architecture and prevent mismat
    

