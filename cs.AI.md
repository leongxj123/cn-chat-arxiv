# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [A Survey of Optimization-based Task and Motion Planning: From Classical To Learning Approaches](https://arxiv.org/abs/2404.02817) | 本综述全面审视了基于优化的任务与运动规划，重点讨论了如何通过混合优化方法解决高度复杂、接触丰富的机器人运动和操作问题。 |
| [^2] | [A Holistic Indicator of Polarization to Measure Online Sexism](https://arxiv.org/abs/2404.02205) | 该论文提出了一个可以提供综合性性别毒性指标的模型，有助于政策制定者、在线社区管理员和计算社会科学家更好地理解和管理在线性别歧视问题。 |
| [^3] | [Remote sensing framework for geological mapping via stacked autoencoders and clustering](https://arxiv.org/abs/2404.02180) | 通过堆叠自动编码器和聚类实现遥感数据地质制图的无监督机器学习框架 |
| [^4] | [Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine Unlearning](https://arxiv.org/abs/2403.07362) | 该论文从对抗的角度提出了一种新的机器遗忘评估方法，通过确定最具挑战性的数据子集，即最坏情况遗忘集，来增强对影响擦除的挑战。 |
| [^5] | [Towards Efficient and Effective Unlearning of Large Language Models for Recommendation](https://arxiv.org/abs/2403.03536) | 提出了E2URec，这是为了解决大型语言模型在推荐系统中遗忘特定用户数据所面临的效率和有效性方面的挑战。 |
| [^6] | [A Library of Mirrors: Deep Neural Nets in Low Dimensions are Convex Lasso Models with Reflection Features](https://arxiv.org/abs/2403.01046) | 证明在1-D数据上训练神经网络等价于解决一个具有固定特征字典矩阵的凸Lasso问题，为全局最优网络和解空间提供了洞察。 |
| [^7] | [LangGPT: Rethinking Structured Reusable Prompt Design Framework for LLMs from the Programming Language](https://arxiv.org/abs/2402.16929) | LangGPT提出了一个双层提示设计框架，作为LLMs的编程语言，大大增强了LLMs产生高质量响应的能力，并在引导LLMs生成高质量提示方面具有显著效果。 |
| [^8] | [If in a Crowdsourced Data Annotation Pipeline, a GPT-4](https://arxiv.org/abs/2402.16795) | 本文比较了 GPT-4 和 MTurk 管道的数据标注准确性，发现尽管 MTurk 采用了最佳实践，但 GPT-4 的准确率更高，并且结合 GPT-4 和众包标签使用聚合算法可以提高准确率。 |
| [^9] | [Bringing Generative AI to Adaptive Learning in Education](https://arxiv.org/abs/2402.14601) | 生成式人工智能技术与自适应学习概念的交叉研究将对教育中下一阶段学习格式的发展做出重要贡献。 |
| [^10] | [How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?](https://arxiv.org/abs/2402.10770) | 本文研究了面向指令的大型语言模型中自动评估方法的可靠性，发现自动方法在不同任务类型下与人工评估者之间的相关性存在巨大变化，且在自由形式生成任务和跨语言转移中可能不可靠。 |
| [^11] | [TIC: Translate-Infer-Compile for accurate 'text to plan' using LLMs and logical intermediate representations](https://arxiv.org/abs/2402.06608) | 该论文研究了使用LLMs和逻辑中间表示来生成准确的"文本到计划"的问题。通过将LLMs用于生成计划任务请求的PDDL表示以及经典规划器的使用，能够更好地解决自然语言处理和计划任务之间的差异。 |
| [^12] | [Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications](https://arxiv.org/abs/2402.05162) | 本研究通过修剪和低秩修改，发现大型语言模型（LLMs）的安全机制固有易碎性，去除安全关键区域会损害安全性，但对效用影响不大，需要更强健的安全策略。 |
| [^13] | [CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay](https://arxiv.org/abs/2402.04858) | CodeIt是一种具备优先级回顾重放的自我改进语言模型方法，通过将目标重标记为采样程序的实际输出，有效解决了程序合成中奖励稀疏性的问题，并在抽象和推理语料库（ARC）上实现了成功的跨任务泛化。 |
| [^14] | [A Comprehensive Survey on Graph Reduction: Sparsification, Coarsening, and Condensation](https://arxiv.org/abs/2402.03358) | 这篇综述调研了图缩减方法，包括稀疏化、粗化和浓缩，在解决大型图形数据分析和计算复杂性方面起到了重要作用。调研对这些方法的技术细节进行了系统的回顾，并强调了它们在实际应用中的关键性。同时，调研还提出了保证图缩减技术持续有效性的关键研究方向。 |
| [^15] | [Curated LLM: Synergy of LLMs and Data Curation for tabular augmentation in ultra low-data regimes](https://arxiv.org/abs/2312.12112) | 本论文提出了CLLM方法，利用LLMs和数据筛选在低数据环境中进行表格增强。通过利用大型语言模型的先验知识以及基于学习动态、置信度和不确定度指标的筛选机制，CLLM取得了优越的性能。 |
| [^16] | [Plum: Prompt Learning using Metaheuristic](https://arxiv.org/abs/2311.08364) | 提出了使用元启发式的提示学习方法，通过测试六种典型的元启发式方法，在大型语言模型的提示优化任务中取得了有效性。 |
| [^17] | [Cross-silo Federated Learning with Record-level Personalized Differential Privacy.](http://arxiv.org/abs/2401.16251) | 本文研究了跨领域联合学习中基于记录级个性化差分隐私的问题，设计了一个名为rPDP-FL的新型框架，并提出了多功能解决方案“模拟-曲线拟合”，以满足不同记录的隐私需求。 |
| [^18] | [A match made in consistency heaven: when large language models meet evolutionary algorithms.](http://arxiv.org/abs/2401.10510) | 大型语言模型和进化算法的结合具有强大的一致性，包括标记嵌入和基因型-表现型映射、位置编码和适应性塑造、位置嵌入和选择、注意力和交叉、前馈神经网络和突变、模型训练和参数更新以及多任务学习和多目标优化等多个核心特征。本文分析了现有的耦合研究，并为未来的研究提供了基本路线和关键挑战。 |
| [^19] | [Reconciling Spatial and Temporal Abstractions for Goal Representation.](http://arxiv.org/abs/2401.09870) | 本文介绍了一种新的三层分层强化学习算法，引入了空间和时间目标抽象化。研究者提供了学习策略的理论遗憾边界，并在多个任务上对算法进行了评估。 |
| [^20] | [Zero-Shot Position Debiasing for Large Language Models.](http://arxiv.org/abs/2401.01218) | 本文提出了一种零样本位置去偏方法（ZOE）来降低大语言模型（LLMs）的位置偏差问题，该方法利用预训练的LLMs的无监督响应进行去偏。实验证实ZOE在多个数据集和任务中均表现出优异的性能。 |
| [^21] | [Unmasking Bias and Inequities: A Systematic Review of Bias Detection and Mitigation in Healthcare Artificial Intelligence Using Electronic Health Records.](http://arxiv.org/abs/2310.19917) | 本综述对涉及利用电子健康记录数据的医疗人工智能研究中的偏见进行了系统综述，共涵盖了六种主要的偏见类型，同时总结了现有的偏见处理方法。 |
| [^22] | [Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory.](http://arxiv.org/abs/2310.17884) | 本研究通过提出ConfAIde基准，揭示了LLMs的上下文隐私推理能力中的重要弱点，实验证明即使是最强大的模型也会在人类不会的上下文中泄露私人信息，强调了探索新型推理时隐私保护方法的迫切需求。 |
| [^23] | [Understanding deep neural networks through the lens of their non-linearity.](http://arxiv.org/abs/2310.11439) | 本文提出了一个理论上有效的解决方案，通过亲和度评分追踪深度神经网络中的非线性传播，尤其关注计算机视觉应用。实验证实了所提出方法的实用性和对广泛应用的潜力。 |
| [^24] | [Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency.](http://arxiv.org/abs/2309.17382) | 提出了一个名为"RAFA"的原则框架，通过在LLM中将推理视为学习和规划的贝叶斯问题，协调推理和行动。通过一个提示模板进行推理，学习并制定未来的轨迹规划，然后在每一步中采取计划轨迹的初始行动并重新规划未来轨迹。这个框架具有可证明的遗憾保证。 |
| [^25] | [Update Monte Carlo tree search (UMCTS) algorithm for heuristic global search of sizing optimization problems for truss structures.](http://arxiv.org/abs/2309.06045) | 本文提出了一种基于启发式全局搜索的算法（UMCTS）用于桁架结构尺寸优化问题，通过结合更新过程和蒙特卡洛树搜索（MCTS）以及使用上界置信度（UCB）来获得合适的设计方案。 |
| [^26] | [Fast Unsupervised Deep Outlier Model Selection with Hypernetworks.](http://arxiv.org/abs/2307.10529) | 本文提出了HYPER用于调整基于深度神经网络的异常值检测模型，解决了无监督DOD模型中的超参数调整和模型选择的挑战，通过设计和训练超网络(HN)将超参数映射到主要DOD模型的最优权重上。 |
| [^27] | [Learning Variational Neighbor Labels for Test-Time Domain Generalization.](http://arxiv.org/abs/2307.04033) | 本文提出了一种用于测试时领域泛化的方法，通过在测试时使用概率伪标签和变分邻居标签来推广源域训练的模型到目标领域，以提高模型的鲁棒性和准确性。 |
| [^28] | [Evaluating the Social Impact of Generative AI Systems in Systems and Society.](http://arxiv.org/abs/2306.05949) | 提出了一种标准方法来评估任何模态的生成AI系统的社会影响，分为基础系统和社会方面的评估，涵盖7个社会影响类别，包括偏见、隐私保护、环境成本等。 |
| [^29] | [Multimodal Explainable Artificial Intelligence: A Comprehensive Review of Methodological Advances and Future Research Directions.](http://arxiv.org/abs/2306.05731) | 本文系统地分析了多模态可解释人工智能（MXAI）领域的最新进展和未来研究方向，从预测任务和数据集到MXAI方法和评估度量标准进行了全面介绍和讨论。 |
| [^30] | [Accelerating Reinforcement Learning with Value-Conditional State Entropy Exploration.](http://arxiv.org/abs/2305.19476) | 本文提出了一种新的探索技术，使用值条件状态熵来解决强化学习中探索不足的问题，可以均衡地覆盖低价值和高价值状态，相较于现有基于熵的探索方法，该方法在MuJoCo基准测试和Atari游戏上有着显著的提升。 |
| [^31] | [SpikeCP: Delay-Adaptive Reliable Spiking Neural Networks via Conformal Prediction.](http://arxiv.org/abs/2305.11322) | 这篇论文提出了一种新的脉冲神经网络模型，能够通过极限预测实现自适应的推断延迟，从而节约能源与提高可靠性。 |
| [^32] | [Investigating Emergent Goal-Like Behaviour in Large Language Models Using Experimental Economics.](http://arxiv.org/abs/2305.07970) | 本研究探讨了大型语言模型的能力，发现其可以将自然语言描述转化为适当的行为，但在区分细微的合作和竞争水平方面的能力受到限制，为使用LLMs在人类决策制定背景下的伦理意义和局限性做出了贡献。 |
| [^33] | ['Team-in-the-loop' organisational oversight of high-stakes AI.](http://arxiv.org/abs/2303.14007) | 本论文通过对团队在 AI 系统中的监管流程的纵向观察，探讨了 AI 系统对临床决策制定中团队监管的影响，研究发现此前的专业团队监管方法主要依靠解释和问询来获取信息，而 AI 的引入将可能在信息披露和决策制定方面造成一定程度的影响。 |
| [^34] | [Optimizing Deep Learning Model Parameters with the Bees Algorithm for Improved Medical Text Classification.](http://arxiv.org/abs/2303.08021) | 本文使用蜜蜂算法优化了深度学习模型参数，提高了医学文本分类的准确性，最高准确率在英语数据集上达到了99.63%，在阿拉伯语数据集上达到了88%。 |
| [^35] | [A Convex Hull Cheapest Insertion Heuristic for the Non-Euclidean TSP.](http://arxiv.org/abs/2302.06582) | 本文提出了一种适用于非欧几里德旅行商问题的凸包最便宜插入启发式解法，通过使用多维缩放将非欧几里德空间的点近似到欧几里德空间，生成了初始化算法的凸包。在评估中发现，该算法在大多数情况下优于最邻近算法。 |
| [^36] | [Explaining the Explainers in Graph Neural Networks: a Comparative Study.](http://arxiv.org/abs/2210.15304) | 该论文研究了图神经网络中的解释方法，并在多种数据集上测试了十种解释器的表现，提供了不同GNN体系结构易解释性的关键洞察。 |

# 详细

[^1]: 优化型任务与运动规划综述：从经典到学习方法

    A Survey of Optimization-based Task and Motion Planning: From Classical To Learning Approaches

    [https://arxiv.org/abs/2404.02817](https://arxiv.org/abs/2404.02817)

    本综述全面审视了基于优化的任务与运动规划，重点讨论了如何通过混合优化方法解决高度复杂、接触丰富的机器人运动和操作问题。

    

    任务与运动规划（TAMP）将高层任务规划和低层运动规划结合起来，使机器人能够有效地推理解决长时域、动态任务。基于优化的TAMP专注于通过目标函数定义目标条件的混合优化方法，并且能够处理开放式目标、机器人动态和机器人与环境之间的物理交互。因此，基于优化的TAMP特别适合解决高度复杂、接触丰富的运动和操作问题。本综述全面审视了基于优化的TAMP，涵盖了（i）规划领域表示，包括动作描述语言和时态逻辑，（ii）TAMP各组件的个别解决策略，包括人工智能规划和轨迹优化（TO），以及（iii）基于逻辑的任务规划与基于模型的TO之间的动态相互作用。

    arXiv:2404.02817v1 Announce Type: cross  Abstract: Task and Motion Planning (TAMP) integrates high-level task planning and low-level motion planning to equip robots with the autonomy to effectively reason over long-horizon, dynamic tasks. Optimization-based TAMP focuses on hybrid optimization approaches that define goal conditions via objective functions and are capable of handling open-ended goals, robotic dynamics, and physical interaction between the robot and the environment. Therefore, optimization-based TAMP is particularly suited to solve highly complex, contact-rich locomotion and manipulation problems. This survey provides a comprehensive review on optimization-based TAMP, covering (i) planning domain representations, including action description languages and temporal logic, (ii) individual solution strategies for components of TAMP, including AI planning and trajectory optimization (TO), and (iii) the dynamic interplay between logic-based task planning and model-based TO. A 
    
[^2]: 一个整体性极化指标以衡量在线性别歧视

    A Holistic Indicator of Polarization to Measure Online Sexism

    [https://arxiv.org/abs/2404.02205](https://arxiv.org/abs/2404.02205)

    该论文提出了一个可以提供综合性性别毒性指标的模型，有助于政策制定者、在线社区管理员和计算社会科学家更好地理解和管理在线性别歧视问题。

    

    arXiv:2404.02205v1 公告类型: 跨领域 摘要: 男权主义者和女权主义者在社交网络上的在线趋势需要一个整体性的衡量性别歧视水平的指标。这个指标对于政策制定者和在线社区的管理员（如subreddits）以及计算社会科学家至关重要，他们可以根据性别歧视程度修改管理策略，或者匹配和比较不同平台和社区上的时态性别歧视，以及推断社会科学见解。在本文中，我们建立了一个模型，可以提供一个可比较的针对男性、女性身份以及男性、女性个人的毒性整体指标。尽管先前的监督NLP方法需要在目标级别对有毒评论进行注释（例如注释特别针对女性有毒的评论）来检测针对性有毒评论，我们的指标利用监督式NLP来检测毒性的存在。

    arXiv:2404.02205v1 Announce Type: cross  Abstract: The online trend of the manosphere and feminist discourse on social networks requires a holistic measure of the level of sexism in an online community. This indicator is important for policymakers and moderators of online communities (e.g., subreddits) and computational social scientists, either to revise moderation strategies based on the degree of sexism or to match and compare the temporal sexism across different platforms and communities with real-time events and infer social scientific insights.   In this paper, we build a model that can provide a comparable holistic indicator of toxicity targeted toward male and female identity and male and female individuals. Despite previous supervised NLP methods that require annotation of toxic comments at the target level (e.g. annotating comments that are specifically toxic toward women) to detect targeted toxic comments, our indicator uses supervised NLP to detect the presence of toxicity 
    
[^3]: 通过堆叠自动编码器和聚类实现地质制图的遥感框架

    Remote sensing framework for geological mapping via stacked autoencoders and clustering

    [https://arxiv.org/abs/2404.02180](https://arxiv.org/abs/2404.02180)

    通过堆叠自动编码器和聚类实现遥感数据地质制图的无监督机器学习框架

    

    有监督学习方法在遥感地质制图中面临着由于准确标记训练数据的稀缺性而限制的问题。相反，无监督学习方法，如降维和聚类，能够在不依赖预定义标签的情况下揭示遥感数据中的模式和结构。降维方法具有在提高地质图准确性方面发挥关键作用的潜力。虽然传统的降维方法可能在非线性数据上遇到困难，但无监督深度学习模型，如自动编码器，能够模拟数据中的非线性关系。堆叠自动编码器具有多个相互连接的层，用于捕获对遥感数据有用的分层数据表示。在本研究中，我们提出了一个利用堆叠自动编码器和聚类处理遥感数据的无监督机器学习框架。

    arXiv:2404.02180v1 Announce Type: cross  Abstract: Supervised learning methods for geological mapping via remote sensing face limitations due to the scarcity of accurately labelled training data. In contrast, unsupervised learning methods, such as dimensionality reduction and clustering have the ability to uncover patterns and structures in remote sensing data without relying on predefined labels. Dimensionality reduction methods have the potential to play a crucial role in improving the accuracy of geological maps. Although conventional dimensionality reduction methods may struggle with nonlinear data, unsupervised deep learning models such as autoencoders have the ability to model nonlinear relationship in data. Stacked autoencoders feature multiple interconnected layers to capture hierarchical data representations that can be useful for remote sensing data. In this study, we present an unsupervised machine learning framework for processing remote sensing data by utilizing stacked au
    
[^4]: 挑战遗忘：揭示机器遗忘中最坏情况遗忘集

    Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine Unlearning

    [https://arxiv.org/abs/2403.07362](https://arxiv.org/abs/2403.07362)

    该论文从对抗的角度提出了一种新的机器遗忘评估方法，通过确定最具挑战性的数据子集，即最坏情况遗忘集，来增强对影响擦除的挑战。

    

    靠谱的机器学习(Machine Learning, ML)社区越来越认识到模型在训练后有选择性地“遗忘”数据点的重要性。这引出了机器遗忘(Machine Unlearning, MU)问题，旨在消除选定数据点对模型性能的影响，同时仍保持模型在遗忘后的实用性。尽管有各种MU方法来擦除数据影响，评估主要集中在随机数据遗忘上，忽视了对于真实衡量遗忘性能的数据子集选择的重要探究。为解决这一问题，我们从对抗的角度引入了一种新的MU评估视角。我们提出确定那些对影响擦除构成最大挑战的数据子集，即找出最坏情况遗忘集。利用双层优化原则，我们增强了在上层优化中的遗忘挑战。

    arXiv:2403.07362v1 Announce Type: cross  Abstract: The trustworthy machine learning (ML) community is increasingly recognizing the crucial need for models capable of selectively 'unlearning' data points after training. This leads to the problem of machine unlearning (MU), aiming to eliminate the influence of chosen data points on model performance, while still maintaining the model's utility post-unlearning. Despite various MU methods for data influence erasure, evaluations have largely focused on random data forgetting, ignoring the vital inquiry into which subset should be chosen to truly gauge the authenticity of unlearning performance. To tackle this issue, we introduce a new evaluative angle for MU from an adversarial viewpoint. We propose identifying the data subset that presents the most significant challenge for influence erasure, i.e., pinpointing the worst-case forget set. Utilizing a bi-level optimization principle, we amplify unlearning challenges at the upper optimization 
    
[^5]: 为推荐而设计的大型语言模型的高效和有效的遗忘

    Towards Efficient and Effective Unlearning of Large Language Models for Recommendation

    [https://arxiv.org/abs/2403.03536](https://arxiv.org/abs/2403.03536)

    提出了E2URec，这是为了解决大型语言模型在推荐系统中遗忘特定用户数据所面临的效率和有效性方面的挑战。

    

    大型语言模型（LLMs）的显著进展产生了一项有前途的研究方向，即利用LLMs作为推荐系统（LLMRec）。 LLMRec的有效性源自LLMs固有的开放世界知识和推理能力。 LLMRec通过基于用户互动数据的指导调整获得推荐功能。 然而，为了保护用户隐私并优化效用，LLMRec还必须有意忘记特定用户数据，这通常称为推荐遗忘。 在LLMs时代，推荐遗忘在\textit{效率}和\textit{有效性}方面为LLMRec带来了新挑战。 现有的遗忘方法需要更新LLMRec中数十亿参数，这是昂贵且耗时的。 此外，它们在遗忘过程中总是影响模型效用。 为此，我们提出了\textbf{E2URec}，第一

    arXiv:2403.03536v1 Announce Type: cross  Abstract: The significant advancements in large language models (LLMs) give rise to a promising research direction, i.e., leveraging LLMs as recommenders (LLMRec). The efficacy of LLMRec arises from the open-world knowledge and reasoning capabilities inherent in LLMs. LLMRec acquires the recommendation capabilities through instruction tuning based on user interaction data. However, in order to protect user privacy and optimize utility, it is also crucial for LLMRec to intentionally forget specific user data, which is generally referred to as recommendation unlearning. In the era of LLMs, recommendation unlearning poses new challenges for LLMRec in terms of \textit{inefficiency} and \textit{ineffectiveness}. Existing unlearning methods require updating billions of parameters in LLMRec, which is costly and time-consuming. Besides, they always impact the model utility during the unlearning process. To this end, we propose \textbf{E2URec}, the first
    
[^6]: 一个镜子的库：低维深度神经网络是具有反射特征的凸Lasso模型

    A Library of Mirrors: Deep Neural Nets in Low Dimensions are Convex Lasso Models with Reflection Features

    [https://arxiv.org/abs/2403.01046](https://arxiv.org/abs/2403.01046)

    证明在1-D数据上训练神经网络等价于解决一个具有固定特征字典矩阵的凸Lasso问题，为全局最优网络和解空间提供了洞察。

    

    我们证明在1-D数据上训练神经网络等价于解决一个带有固定、明确定义的特征字典矩阵的凸Lasso问题。具体的字典取决于激活函数和深度。我们考虑具有分段线性激活函数的两层网络，深窄的ReLU网络最多有4层，以及具有符号激活和任意深度的矩形和树网络。有趣的是，在ReLU网络中，第四层创建代表训练数据关于自身的反射的特征。Lasso表示法揭示了全局最优网络和解空间的洞察。

    arXiv:2403.01046v1 Announce Type: cross  Abstract: We prove that training neural networks on 1-D data is equivalent to solving a convex Lasso problem with a fixed, explicitly defined dictionary matrix of features. The specific dictionary depends on the activation and depth. We consider 2-layer networks with piecewise linear activations, deep narrow ReLU networks with up to 4 layers, and rectangular and tree networks with sign activation and arbitrary depth. Interestingly in ReLU networks, a fourth layer creates features that represent reflections of training data about themselves. The Lasso representation sheds insight to globally optimal networks and the solution landscape.
    
[^7]: LangGPT：重新思考面向LLMs的结构化可重复使用提示设计框架从编程语言出发

    LangGPT: Rethinking Structured Reusable Prompt Design Framework for LLMs from the Programming Language

    [https://arxiv.org/abs/2402.16929](https://arxiv.org/abs/2402.16929)

    LangGPT提出了一个双层提示设计框架，作为LLMs的编程语言，大大增强了LLMs产生高质量响应的能力，并在引导LLMs生成高质量提示方面具有显著效果。

    

    LLMs已经展示出在不同领域取得了令人瞩目的性能。然而，为了有效指导LLMs制定高质量的提示对于非AI专家来说是一个挑战。现有的提示工程研究建议了一些略显零碎的优化原则和设计，以及凭经验依赖的提示优化器。不幸的是，这些努力缺乏一个结构化的设计模板，导致学习成本高，重复使用性低。受结构化可重复使用的编程语言的启发，我们提出了LangGPT，作为LLMs的编程语言的双层提示设计框架。LangGPT具有易于学习的规范结构，并提供了一个扩展结构以进行迁移和重用。实验证明，与基准相比，LangGPT显著增强了LLMs产生高质量响应的能力。此外，LangGPT已被证明在引导LLMs生成高质量提示方面是有效的。

    arXiv:2402.16929v1 Announce Type: cross  Abstract: LLMs have demonstrated commendable performance across diverse domains. Nevertheless, formulating high-quality prompts to effectively instruct LLMs poses a challenge for non-AI experts. Existing research in prompt engineering suggests somewhat fragmented optimization principles and designs empirically dependent prompt optimizers. Unfortunately, these endeavors lack a structured design template, incurring high learning costs and resulting in low reusability. Inspired by structured reusable programming languages, we propose LangGPT, a dual-layer prompt design framework as the programming language for LLMs. LangGPT has an easy-to-learn normative structure and provides an extended structure for migration and reuse. Experiments illustrate that LangGPT significantly enhances the capacity of LLMs to produce responses of superior quality compared to baselines. Moreover, LangGPT has proven effective in guiding LLMs to generate high-quality promp
    
[^8]: 如果在一个众包数据标注管道中，GPT-4

    If in a Crowdsourced Data Annotation Pipeline, a GPT-4

    [https://arxiv.org/abs/2402.16795](https://arxiv.org/abs/2402.16795)

    本文比较了 GPT-4 和 MTurk 管道的数据标注准确性，发现尽管 MTurk 采用了最佳实践，但 GPT-4 的准确率更高，并且结合 GPT-4 和众包标签使用聚合算法可以提高准确率。

    

    最近的研究表明GPT-4在数据标注准确性方面优于在线众包工作者，尤其是来自亚马逊机械土耳其（MTurk）的工作者。然而，这些研究因偏离标准众包实践并强调个别工作者的表现而受到批评，而不是整个数据标注过程。本文比较了GPT-4和一个道德且执行良好的MTurk管道，使用415名工作者标注了来自200篇学术文章的3,177个句段，使用了CODA-19方案。两个工作者界面产生了127,080个标签，然后通过八种标签聚合算法推断出最终的标签。我们的评估结果显示，尽管采用了最佳实践，MTurk管道的最高准确率为81.5%，而GPT-4达到了83.6%。有趣的是，当将GPT-4的标签与通过先进工作者界面收集的众包标签结合起来进行聚合时，8种算法中有2种实现了更高的准确率。

    arXiv:2402.16795v1 Announce Type: cross  Abstract: Recent studies indicated GPT-4 outperforms online crowd workers in data labeling accuracy, notably workers from Amazon Mechanical Turk (MTurk). However, these studies were criticized for deviating from standard crowdsourcing practices and emphasizing individual workers' performances over the whole data-annotation process. This paper compared GPT-4 and an ethical and well-executed MTurk pipeline, with 415 workers labeling 3,177 sentence segments from 200 scholarly articles using the CODA-19 scheme. Two worker interfaces yielded 127,080 labels, which were then used to infer the final labels through eight label-aggregation algorithms. Our evaluation showed that despite best practices, MTurk pipeline's highest accuracy was 81.5%, whereas GPT-4 achieved 83.6%. Interestingly, when combining GPT-4's labels with crowd labels collected via an advanced worker interface for aggregation, 2 out of the 8 algorithms achieved an even higher accuracy (
    
[^9]: 将生成式人工智能引入教育中的自适应学习

    Bringing Generative AI to Adaptive Learning in Education

    [https://arxiv.org/abs/2402.14601](https://arxiv.org/abs/2402.14601)

    生成式人工智能技术与自适应学习概念的交叉研究将对教育中下一阶段学习格式的发展做出重要贡献。

    

    最近生成式人工智能技术的激增，如大型语言模型和扩散模型，推动了人工智能在科学、金融和教育等各个领域的应用发展。与此同时，自适应学习这一概念在教育领域引起了极大关注，并证明其在提高学生学习效率方面的有效性。在本立场论文中，我们旨在探讨将生成式人工智能与自适应学习概念结合起来的交叉研究。通过讨论这一领域的好处、挑战和潜力，我们认为这种结合将为教育中下一阶段学习形式的发展做出重要贡献。

    arXiv:2402.14601v1 Announce Type: cross  Abstract: The recent surge in generative AI technologies, such as large language models and diffusion models, have boosted the development of AI applications in various domains, including science, finance, and education. Concurrently, adaptive learning, a concept that has gained substantial interest in the educational sphere, has proven its efficacy in enhancing students' learning efficiency. In this position paper, we aim to shed light on the intersectional studies of these two methods, which combine generative AI with adaptive learning concepts. By presenting discussions about the benefits, challenges, and potentials in this field, we argue that this union will contribute significantly to the development of the next stage learning format in education.
    
[^10]: 自动评估方法在面向指令的LLM中有多可靠？

    How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?

    [https://arxiv.org/abs/2402.10770](https://arxiv.org/abs/2402.10770)

    本文研究了面向指令的大型语言模型中自动评估方法的可靠性，发现自动方法在不同任务类型下与人工评估者之间的相关性存在巨大变化，且在自由形式生成任务和跨语言转移中可能不可靠。

    

    面向指令的大型语言模型(LLMs)的研究使用基于文本重叠和LLM判断的自动方法作为人工评估的成本有效替代方案。本文研究了这些方法在广泛的任务范围和跨语言环境中的可靠性。与先前的研究结果相反，我们观察到在任务类型不同的情况下，自动方法与人工评估者之间的相关性存在显著变化。具体而言，广泛使用的ROUGE-L度量在短答案英语任务中与人类判断强相关，但在自由形式生成任务和跨语言转移中不可靠。使用GPT-4作为评估员的有效性取决于在要求评估时包含参考答案，这可能导致在自由形式生成任务中评估过于严格。总的来说，我们发现，尽管自动评估方法可以近似人类判断，但其准确性可能因任务类型和评估设置而异。

    arXiv:2402.10770v1 Announce Type: cross  Abstract: Work on instruction-tuned Large Language Models (LLMs) has used automatic methods based on text overlap and LLM judgments as cost-effective alternatives to human evaluation. In this paper, we study the reliability of such methods across a broad range of tasks and in a cross-lingual setting. In contrast to previous findings, we observe considerable variability in correlations between automatic methods and human evaluators when scores are differentiated by task type. Specifically, the widely-used ROUGE-L metric strongly correlates with human judgments for short-answer English tasks but is unreliable in free-form generation tasks and cross-lingual transfer. The effectiveness of GPT-4 as an evaluator depends on including reference answers when prompting for assessments, which can lead to overly strict evaluations in free-form generation tasks. In summary, we find that, while automatic evaluation methods can approximate human judgements und
    
[^11]: TIC：利用LLMs和逻辑中间表示精确进行“文本到计划”的翻译-推断-编译

    TIC: Translate-Infer-Compile for accurate 'text to plan' using LLMs and logical intermediate representations

    [https://arxiv.org/abs/2402.06608](https://arxiv.org/abs/2402.06608)

    该论文研究了使用LLMs和逻辑中间表示来生成准确的"文本到计划"的问题。通过将LLMs用于生成计划任务请求的PDDL表示以及经典规划器的使用，能够更好地解决自然语言处理和计划任务之间的差异。

    

    我们研究了为给定的自然语言计划任务请求生成计划的问题。一方面，LLMs在自然语言处理方面表现出色，但在计划方面表现不佳。另一方面，经典计划工具在计划任务方面表现出色，但需要使用结构化语言（如Planning Domain Definition Language（PDDL））作为输入。我们利用这两种技术的优点，通过使用LLMs生成计划任务请求的PDDL表示（任务PDDL），然后使用经典规划器计算计划。与直接使用LLMs生成任务PDDL的先前方法不同，我们的方法包括（a）翻译：仅使用LLMs生成自然语言任务描述的逻辑可解释的中间表示，（b）推断：使用逻辑推理器（目前是Answer Set Programming solver）从中间表示中推导出额外的逻辑相关信息，以及（c）编译：生成目标计划的PDDL描述的编译。

    We study the problem of generating plans for given natural language planning task requests. On one hand, LLMs excel at natural language processing but do not perform well on planning. On the other hand, classical planning tools excel at planning tasks but require input in a structured language such as the Planning Domain Definition Language (PDDL). We leverage the strengths of both the techniques by using an LLM for generating the PDDL representation (task PDDL) of planning task requests followed by using a classical planner for computing a plan. Unlike previous approaches that use LLMs for generating task PDDLs directly, our approach comprises of (a) translate: using an LLM only for generating a logically interpretable intermediate representation of natural language task descriptions, (b) infer: deriving additional logically dependent information from the intermediate representation using a logic reasoner (currently, Answer Set Programming solver), and (c) compile: generating the targ
    
[^12]: 通过修剪和低秩修改评估安全对齐的易碎性

    Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications

    [https://arxiv.org/abs/2402.05162](https://arxiv.org/abs/2402.05162)

    本研究通过修剪和低秩修改，发现大型语言模型（LLMs）的安全机制固有易碎性，去除安全关键区域会损害安全性，但对效用影响不大，需要更强健的安全策略。

    

    大型语言模型（LLMs）在其安全机制方面表现出固有的易碎性，这可从它们易受越狱和即使是非恶意微调也易受影响来说明。本研究通过利用修剪和低秩修改探讨了安全对齐的易碎性。我们开发了方法，能够识别对于安全防护至关重要，且在神经元和秩级别上与效用相关的区域。令人惊讶的是，我们发现的孤立区域是稀疏的，约占参数级别的$3\%$和排名级别的$2.5\%$。去除这些区域会损害安全性，而对效用的影响不大，从而证实了该模型安全机制的固有易碎性。此外，我们还表明，即使限制对安全关键区域进行修改，LLMs仍然容易受到低成本的微调攻击。这些发现强调了在LLMs中更强大的安全策略的紧迫性需求。

    Large language models (LLMs) show inherent brittleness in their safety mechanisms, as evidenced by their susceptibility to jailbreaking and even non-malicious fine-tuning. This study explores this brittleness of safety alignment by leveraging pruning and low-rank modifications. We develop methods to identify critical regions that are vital for safety guardrails, and that are disentangled from utility-relevant regions at both the neuron and rank levels. Surprisingly, the isolated regions we find are sparse, comprising about $3\%$ at the parameter level and $2.5\%$ at the rank level. Removing these regions compromises safety without significantly impacting utility, corroborating the inherent brittleness of the model's safety mechanisms. Moreover, we show that LLMs remain vulnerable to low-cost fine-tuning attacks even when modifications to the safety-critical regions are restricted. These findings underscore the urgent need for more robust safety strategies in LLMs.
    
[^13]: CodeIt：具有优先级回顾重放的自我改进语言模型

    CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay

    [https://arxiv.org/abs/2402.04858](https://arxiv.org/abs/2402.04858)

    CodeIt是一种具备优先级回顾重放的自我改进语言模型方法，通过将目标重标记为采样程序的实际输出，有效解决了程序合成中奖励稀疏性的问题，并在抽象和推理语料库（ARC）上实现了成功的跨任务泛化。

    

    大型语言模型越来越能够解决通常被认为需要人类水平推理能力的任务。然而，这些模型在通用智能基准测试例如抽象和推理语料库（ARC）上表现仍然非常差。在本文中，我们将ARC视为一个以编程示例为基础的问题，并引入了一种名为Code Iteration（CodeIt）的新颖且可扩展的语言模型自我改进方法。我们的方法在1）程序抽样和回顾重标记以及2）基于优先级的经验回放之间进行迭代。通过将一个episode的目标（即给定输入的目标程序输出）重标记为采样程序产生的实际输出，我们的方法有效地处理了程序合成中奖励极度稀疏性的问题。应用CodeIt于ARC数据集，我们证明了优先级回顾重放、预训练和数据增强可以实现成功的跨任务泛化。CodeIt是第一个神经元-合成机制一体的自我改进语言模型方法。

    Large language models are increasingly solving tasks that are commonly believed to require human-level reasoning ability. However, these models still perform very poorly on benchmarks of general intelligence such as the Abstraction and Reasoning Corpus (ARC). In this paper, we approach ARC as a programming-by-examples problem, and introduce a novel and scalable method for language model self-improvement called Code Iteration (CodeIt). Our method iterates between 1) program sampling and hindsight relabeling, and 2) learning from prioritized experience replay. By relabeling the goal of an episode (i.e., the target program output given input) to the realized output produced by the sampled program, our method effectively deals with the extreme sparsity of rewards in program synthesis. Applying CodeIt to the ARC dataset, we demonstrate that prioritized hindsight replay, along with pre-training and data-augmentation, leads to successful inter-task generalization. CodeIt is the first neuro-sy
    
[^14]: 图缩减的综合调研：稀疏化、粗化和浓缩

    A Comprehensive Survey on Graph Reduction: Sparsification, Coarsening, and Condensation

    [https://arxiv.org/abs/2402.03358](https://arxiv.org/abs/2402.03358)

    这篇综述调研了图缩减方法，包括稀疏化、粗化和浓缩，在解决大型图形数据分析和计算复杂性方面起到了重要作用。调研对这些方法的技术细节进行了系统的回顾，并强调了它们在实际应用中的关键性。同时，调研还提出了保证图缩减技术持续有效性的关键研究方向。

    

    许多真实世界的数据集可以自然地表示为图，涵盖了广泛的领域。然而，图数据集的复杂性和规模的增加为分析和计算带来了显著的挑战。为此，图缩减技术在保留关键属性的同时简化大型图形数据变得越来越受关注。在本调研中，我们旨在提供对图缩减方法的全面理解，包括图稀疏化、图粗化和图浓缩。具体而言，我们建立了这些方法的统一定义，并引入了一个分层分类法来分类这些方法所解决的挑战。我们的调研系统地回顾了这些方法的技术细节，并强调了它们在各种场景中的实际应用。此外，我们还概述了保证图缩减技术持续有效性的关键研究方向，并提供了一个详细的论文列表链接。

    Many real-world datasets can be naturally represented as graphs, spanning a wide range of domains. However, the increasing complexity and size of graph datasets present significant challenges for analysis and computation. In response, graph reduction techniques have gained prominence for simplifying large graphs while preserving essential properties. In this survey, we aim to provide a comprehensive understanding of graph reduction methods, including graph sparsification, graph coarsening, and graph condensation. Specifically, we establish a unified definition for these methods and introduce a hierarchical taxonomy to categorize the challenges they address. Our survey then systematically reviews the technical details of these methods and emphasizes their practical applications across diverse scenarios. Furthermore, we outline critical research directions to ensure the continued effectiveness of graph reduction techniques, as well as provide a comprehensive paper list at https://github.
    
[^15]: LLM精选：在超低数据环境中利用LLMs和数据筛选进行表格增强

    Curated LLM: Synergy of LLMs and Data Curation for tabular augmentation in ultra low-data regimes

    [https://arxiv.org/abs/2312.12112](https://arxiv.org/abs/2312.12112)

    本论文提出了CLLM方法，利用LLMs和数据筛选在低数据环境中进行表格增强。通过利用大型语言模型的先验知识以及基于学习动态、置信度和不确定度指标的筛选机制，CLLM取得了优越的性能。

    

    低数据情况下的机器学习（ML）仍然是一个被低估但至关重要的问题。因此，增加ML所需的数据样本大小的数据增强方法对于释放ML在数据匮乏的地区和领域的变革潜力至关重要。不幸的是，有限的训练集限制了传统的表格合成数据生成器在生成ML任务所需的大规模且多样化的增强数据集方面的能力。为了解决这个挑战，我们引入了CLLM，它利用大型语言模型（LLMs）在低数据环境中进行数据增强的先验知识。然而，像任何生成模型一样，并非LLMs生成的所有数据都能提高下游的效用。因此，我们引入了一种基于学习动态、置信度和不确定度指标的原则性筛选机制，以获取高质量的数据集。通过多个真实世界数据集的实证，我们展示了CLLM在低数据环境中的优越性能。

    Machine Learning (ML) in low-data settings remains an underappreciated yet crucial problem. Hence, data augmentation methods to increase the sample size of datasets needed for ML are key to unlocking the transformative potential of ML in data-deprived regions and domains. Unfortunately, the limited training set constrains traditional tabular synthetic data generators in their ability to generate a large and diverse augmented dataset needed for ML tasks. To address this challenge, we introduce CLLM, which leverages the prior knowledge of Large Language Models (LLMs) for data augmentation in the low-data regime. However, not all the data generated by LLMs will improve downstream utility, as for any generative model. Consequently, we introduce a principled curation mechanism, leveraging learning dynamics, coupled with confidence and uncertainty metrics, to obtain a high-quality dataset. Empirically, on multiple real-world datasets, we demonstrate the superior performance of CLLM in the lo
    
[^16]: Plum: 使用元启发式的提示学习

    Plum: Prompt Learning using Metaheuristic

    [https://arxiv.org/abs/2311.08364](https://arxiv.org/abs/2311.08364)

    提出了使用元启发式的提示学习方法，通过测试六种典型的元启发式方法，在大型语言模型的提示优化任务中取得了有效性。

    

    自从大型语言模型出现以来，提示学习已成为优化和定制这些模型的一种流行方法。特殊提示，如“思维链”，甚至揭示了这些模型内部先前未知的推理能力。然而，发现有效提示的进展缓慢，促使人们渴望一种通用的提示优化方法。不幸的是，现有的提示学习方法中很少有满足“通用”的标准，即同时具备自动、离散、黑盒、无梯度和可解释性。在本文中，我们引入元启发式，作为一种有希望的提示学习方法的离散非凸优化方法分支，拥有100多种选项。在我们的范式中，我们测试了六种典型方法：爬山、模拟退火、遗传算法（带/不带交叉）、禁忌搜索和和谐搜索，展示了它们在白盒模式下的有效性。

    arXiv:2311.08364v2 Announce Type: replace-cross  Abstract: Since the emergence of large language models, prompt learning has become a popular method for optimizing and customizing these models. Special prompts, such as Chain-of-Thought, have even revealed previously unknown reasoning capabilities within these models. However, the progress of discovering effective prompts has been slow, driving a desire for general prompt optimization methods. Unfortunately, few existing prompt learning methods satisfy the criteria of being truly "general", i.e., automatic, discrete, black-box, gradient-free, and interpretable all at once. In this paper, we introduce metaheuristics, a branch of discrete non-convex optimization methods with over 100 options, as a promising approach to prompt learning. Within our paradigm, we test six typical methods: hill climbing, simulated annealing, genetic algorithms with/without crossover, tabu search, and harmony search, demonstrating their effectiveness in white-b
    
[^17]: 跨领域联合学习中基于记录级个性化差分隐私的研究

    Cross-silo Federated Learning with Record-level Personalized Differential Privacy. (arXiv:2401.16251v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2401.16251](http://arxiv.org/abs/2401.16251)

    本文研究了跨领域联合学习中基于记录级个性化差分隐私的问题，设计了一个名为rPDP-FL的新型框架，并提出了多功能解决方案“模拟-曲线拟合”，以满足不同记录的隐私需求。

    

    基于差分隐私增强的联合学习成为了保护客户端数据隐私的常用方法，但现有方案通常假设所有记录的隐私预算均相同，提供一种适用于所有记录的通用解决方案，可能无法满足每个记录的隐私需求。本文探讨了跨领域联合学习中基于记录级个性化差分隐私的未知领域。我们设计了一个名为rPDP-FL的新型框架，采用两阶段混合抽样方案，既包括客户端级别抽样，又包括非均匀记录级别抽样，以适应不同的隐私需求。一个关键且非平凡的问题是在给定个性化隐私预算ε的情况下选择理想的每记录抽样概率q。我们提出了一个多功能解决方案“模拟-曲线拟合”，使我们能够揭示非线性相关性的重要见解。

    Federated learning enhanced by differential privacy has emerged as a popular approach to better safeguard the privacy of client-side data by protecting clients' contributions during the training process. Existing solutions typically assume a uniform privacy budget for all records and provide one-size-fits-all solutions that may not be adequate to meet each record's privacy requirement. In this paper, we explore the uncharted territory of cross-silo FL with record-level personalized differential privacy. We devise a novel framework named rPDP-FL, employing a two-stage hybrid sampling scheme with both client-level sampling and non-uniform record-level sampling to accommodate varying privacy requirements. A critical and non-trivial problem is to select the ideal per-record sampling probability q given the personalized privacy budget {\epsilon}. We introduce a versatile solution named Simulation-CurveFitting, allowing us to uncover a significant insight into the nonlinear correlation betwe
    
[^18]: 天作之合：大型语言模型与进化算法的结合

    A match made in consistency heaven: when large language models meet evolutionary algorithms. (arXiv:2401.10510v1 [cs.NE])

    [http://arxiv.org/abs/2401.10510](http://arxiv.org/abs/2401.10510)

    大型语言模型和进化算法的结合具有强大的一致性，包括标记嵌入和基因型-表现型映射、位置编码和适应性塑造、位置嵌入和选择、注意力和交叉、前馈神经网络和突变、模型训练和参数更新以及多任务学习和多目标优化等多个核心特征。本文分析了现有的耦合研究，并为未来的研究提供了基本路线和关键挑战。

    

    预训练的大型语言模型（LLMs）在生成创造性的自然文本方面具有强大的能力。进化算法（EAs）可以发现复杂实际问题的多样解决方案。本文通过比较文本序列生成和进化的共同特点和方向性，阐述了LLMs与EAs之间的强大一致性，包括多个一对一的核心特征：标记嵌入和基因型-表现型映射、位置编码和适应性塑造、位置嵌入和选择、注意力和交叉、前馈神经网络和突变、模型训练和参数更新以及多任务学习和多目标优化。在这种一致性视角下，分析了现有的耦合研究，包括进化微调和LLM增强型EAs。借助这些洞见，我们概述了未来在LLMs和EAs耦合方面的基本研究路线，并突出了其中的关键挑战。

    Pre-trained large language models (LLMs) have powerful capabilities for generating creative natural text. Evolutionary algorithms (EAs) can discover diverse solutions to complex real-world problems. Motivated by the common collective and directionality of text sequence generation and evolution, this paper illustrates the strong consistency of LLMs and EAs, which includes multiple one-to-one key characteristics: token embedding and genotype-phenotype mapping, position encoding and fitness shaping, position embedding and selection, attention and crossover, feed-forward neural network and mutation, model training and parameter update, and multi-task learning and multi-objective optimization. Based on this consistency perspective, existing coupling studies are analyzed, including evolutionary fine-tuning and LLM-enhanced EAs. Leveraging these insights, we outline a fundamental roadmap for future research in coupling LLMs and EAs, while highlighting key challenges along the way. The consist
    
[^19]: 调和空间和时间抽象化以实现目标表示

    Reconciling Spatial and Temporal Abstractions for Goal Representation. (arXiv:2401.09870v1 [cs.LG])

    [http://arxiv.org/abs/2401.09870](http://arxiv.org/abs/2401.09870)

    本文介绍了一种新的三层分层强化学习算法，引入了空间和时间目标抽象化。研究者提供了学习策略的理论遗憾边界，并在多个任务上对算法进行了评估。

    

    目标表示通过将复杂的学习问题分解为更容易的子任务来影响分层强化学习算法的性能。最近的研究表明，保留时间抽象环境动态的表示方法在解决困难问题和提供优化理论保证方面是成功的。然而，这些方法在环境动态越来越复杂（即时间抽象转换关系依赖更多变量）的任务中无法扩展。另一方面，其他方法则尝试使用空间抽象来缓解前面的问题。它们的限制包括无法适应高维环境和对先前知识的依赖。本文提出了一种新的三层分层强化学习算法，分层结构的不同层次引入了空间和时间目标抽象化。我们对学习策略的遗憾边界进行了理论研究。我们评估了我们提出的算法在不同任务上的性能。

    Goal representation affects the performance of Hierarchical Reinforcement Learning (HRL) algorithms by decomposing the complex learning problem into easier subtasks. Recent studies show that representations that preserve temporally abstract environment dynamics are successful in solving difficult problems and provide theoretical guarantees for optimality. These methods however cannot scale to tasks where environment dynamics increase in complexity i.e. the temporally abstract transition relations depend on larger number of variables. On the other hand, other efforts have tried to use spatial abstraction to mitigate the previous issues. Their limitations include scalability to high dimensional environments and dependency on prior knowledge.  In this paper, we propose a novel three-layer HRL algorithm that introduces, at different levels of the hierarchy, both a spatial and a temporal goal abstraction. We provide a theoretical study of the regret bounds of the learned policies. We evalua
    
[^20]: 大语言模型的零样本位置去偏方法

    Zero-Shot Position Debiasing for Large Language Models. (arXiv:2401.01218v1 [cs.CL])

    [http://arxiv.org/abs/2401.01218](http://arxiv.org/abs/2401.01218)

    本文提出了一种零样本位置去偏方法（ZOE）来降低大语言模型（LLMs）的位置偏差问题，该方法利用预训练的LLMs的无监督响应进行去偏。实验证实ZOE在多个数据集和任务中均表现出优异的性能。

    

    微调已被证明是改善大语言模型（LLMs）领域性能的有效方法。然而，LLMs可能适应数据集偏见和预测的捷径，导致生成性能差。实验结果显示，LLMs容易表现出位置偏差，即利用位于开头或末尾或输入中特定位置线索的信息。现有的减轻位置偏差的工作需要外部偏差知识或带注释的非偏倚样本，在实际中不太实用。在这项工作中，我们提出了一种零样本位置去偏（ZOE）框架对LLMs进行位置去偏。ZOE利用预训练的LLMs的无监督响应进行去偏，因此不需要任何外部知识或数据集。为了提高无监督响应的质量，我们提出了一种主从对齐（MSA）模块来修剪这些响应。对八个数据集和五个任务的实验表明，ZOE始终优于其他方法。

    Fine-tuning has been demonstrated to be an effective method to improve the domain performance of large language models (LLMs). However, LLMs might fit the dataset bias and shortcuts for prediction, leading to poor generation performance. Experimental result shows that LLMs are prone to exhibit position bias, i.e., leveraging information positioned at the beginning or end, or specific positional cues within the input. Existing works on mitigating position bias require external bias knowledge or annotated non-biased samples, which is unpractical in reality. In this work, we propose a zero-shot position debiasing (ZOE) framework to mitigate position bias for LLMs. ZOE leverages unsupervised responses from pre-trained LLMs for debiasing, thus without any external knowledge or datasets. To improve the quality of unsupervised responses, we propose a master-slave alignment (MSA) module to prune these responses. Experiments on eight datasets and five tasks show that ZOE consistently outperform
    
[^21]: 揭示偏见和不平等：利用电子健康记录的医疗人工智能中偏见检测和缓解的系统综述

    Unmasking Bias and Inequities: A Systematic Review of Bias Detection and Mitigation in Healthcare Artificial Intelligence Using Electronic Health Records. (arXiv:2310.19917v1 [cs.AI])

    [http://arxiv.org/abs/2310.19917](http://arxiv.org/abs/2310.19917)

    本综述对涉及利用电子健康记录数据的医疗人工智能研究中的偏见进行了系统综述，共涵盖了六种主要的偏见类型，同时总结了现有的偏见处理方法。

    

    目的：利用电子健康记录的人工智能应用在医疗领域越来越受到欢迎，但也引入了各种类型的偏见。本研究旨在系统综述涉及利用电子健康记录数据的人工智能研究中的偏见。方法：遵循Preferred Reporting Items for Systematic Reviews and Meta-analyses (PRISMA)准则进行了系统综述。从PubMed、Web of Science和电气和电子工程师学会中检索了2010年1月1日至2022年10月31日期间发表的文章。我们定义了六种主要的偏见类型，并总结了现有的偏见处理方法。结果：在检索到的252篇文章中，有20篇符合最终综述的纳入标准。本综述涵盖了六种偏见中的五种：八项研究分析了选择偏见；六项研究针对隐性偏见；五项研究对混杂偏见进行了研究；四项研究对测量偏见进行了研究；两项研究对算法偏见进行了研究。在偏见处理方法方面，有十项研究进行了探讨。

    Objectives: Artificial intelligence (AI) applications utilizing electronic health records (EHRs) have gained popularity, but they also introduce various types of bias. This study aims to systematically review the literature that address bias in AI research utilizing EHR data. Methods: A systematic review was conducted following the Preferred Reporting Items for Systematic Reviews and Meta-analyses (PRISMA) guideline. We retrieved articles published between January 1, 2010, and October 31, 2022, from PubMed, Web of Science, and the Institute of Electrical and Electronics Engineers. We defined six major types of bias and summarized the existing approaches in bias handling. Results: Out of the 252 retrieved articles, 20 met the inclusion criteria for the final review. Five out of six bias were covered in this review: eight studies analyzed selection bias; six on implicit bias; five on confounding bias; four on measurement bias; two on algorithmic bias. For bias handling approaches, ten st
    
[^22]: LLM能保守秘密吗？通过上下文完整性理论测试语言模型的隐私影响

    Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory. (arXiv:2310.17884v1 [cs.AI])

    [http://arxiv.org/abs/2310.17884](http://arxiv.org/abs/2310.17884)

    本研究通过提出ConfAIde基准，揭示了LLMs的上下文隐私推理能力中的重要弱点，实验证明即使是最强大的模型也会在人类不会的上下文中泄露私人信息，强调了探索新型推理时隐私保护方法的迫切需求。

    

    在AI助手（工作、家庭等）中交互使用大型语言模型（LLMs）引入了一系列新的推理时隐私风险：LLMs从多个来源的输入中获取不同类型的信息，并期望在给定的上下文中推理出在何种目的和与谁分享的内容。在这项工作中，我们通过提出ConfAIde，一个旨在识别指令调整的LLMs隐私推理能力中重要弱点的基准，来引起人们对上下文隐私这一极其关键但经常被忽视的概念的关注。我们的实验表明，即使是GPT-4和ChatGPT等最强大的模型，在人类不会的上下文中，也会泄露39％和57％的私人信息。即使我们使用保护隐私的提示或思维链推理，这种泄漏也会持续存在。我们的工作强调了迫切需要探索基于推理和理论的新型推理时隐私保护方法。

    The interactive use of large language models (LLMs) in AI assistants (at work, home, etc.) introduces a new set of inference-time privacy risks: LLMs are fed different types of information from multiple sources in their inputs and are expected to reason about what to share in their outputs, for what purpose and with whom, within a given context. In this work, we draw attention to the highly critical yet overlooked notion of contextual privacy by proposing ConfAIde, a benchmark designed to identify critical weaknesses in the privacy reasoning capabilities of instruction-tuned LLMs. Our experiments show that even the most capable models such as GPT-4 and ChatGPT reveal private information in contexts that humans would not, 39% and 57% of the time, respectively. This leakage persists even when we employ privacy-inducing prompts or chain-of-thought reasoning. Our work underscores the immediate need to explore novel inference-time privacy-preserving approaches, based on reasoning and theory
    
[^23]: 通过非线性研究深度神经网络的理解

    Understanding deep neural networks through the lens of their non-linearity. (arXiv:2310.11439v1 [cs.LG])

    [http://arxiv.org/abs/2310.11439](http://arxiv.org/abs/2310.11439)

    本文提出了一个理论上有效的解决方案，通过亲和度评分追踪深度神经网络中的非线性传播，尤其关注计算机视觉应用。实验证实了所提出方法的实用性和对广泛应用的潜力。

    

    深度神经网络(DNN)的显著成功常常归因于它们的高表达能力和近似任意复杂函数的能力。事实上，DNN是高度非线性的模型，其中引入的激活函数在其中起到了重要作用。然而，尽管许多研究通过近似能力的视角研究了DNN的表达能力，但量化DNN或个别激活函数的非线性仍然是一个开放性问题。在本文中，我们提出了第一个在具体关注计算机视觉应用中追踪非线性传播的理论有效解决方案。我们提出的亲和度评分允许我们深入了解各种不同体系结构和学习范式的内部工作原理。我们提供了大量的实验结果，突出了所提出的亲和度评分的实际效用和潜在应用的可能性。

    The remarkable success of deep neural networks (DNN) is often attributed to their high expressive power and their ability to approximate functions of arbitrary complexity. Indeed, DNNs are highly non-linear models, and activation functions introduced into them are largely responsible for this. While many works studied the expressive power of DNNs through the lens of their approximation capabilities, quantifying the non-linearity of DNNs or of individual activation functions remains an open problem. In this paper, we propose the first theoretically sound solution to track non-linearity propagation in deep neural networks with a specific focus on computer vision applications. Our proposed affinity score allows us to gain insights into the inner workings of a wide range of different architectures and learning paradigms. We provide extensive experimental results that highlight the practical utility of the proposed affinity score and its potential for long-reaching applications.
    
[^24]: 未来的原因，现在的行动：一种可证明样本效率的自主LLM智能体的原则框架

    Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency. (arXiv:2309.17382v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2309.17382](http://arxiv.org/abs/2309.17382)

    提出了一个名为"RAFA"的原则框架，通过在LLM中将推理视为学习和规划的贝叶斯问题，协调推理和行动。通过一个提示模板进行推理，学习并制定未来的轨迹规划，然后在每一步中采取计划轨迹的初始行动并重新规划未来轨迹。这个框架具有可证明的遗憾保证。

    

    大型语言模型（LLM）展示了令人印象深刻的推理能力，但在现实世界中将推理转化为行动仍然具有挑战性。特别是，如何通过推理的内部机制在与外部环境的最少交互次数内可证明地完成给定任务仍然不清楚。为此，我们提出了一个有可证明遗憾保证的原则框架来协调推理和行动，称之为“为未来而推理，为现在而行动”（RAFA）。具体来说，我们设计了一个推理的提示模板，从内存缓冲区中学习并制定未来的长期轨迹规划（“为未来而推理”）。在每一步中，LLM智能体采取计划轨迹的初始行动（“为现在而行动”），将收集到的反馈存储在内存缓冲区中，并重新调用推理过程从新状态重新规划未来的轨迹。关键思想是将LLM中的推理视为学习和规划的贝叶斯问题。

    Large language models (LLMs) demonstrate impressive reasoning abilities, but translating reasoning into actions in the real world remains challenging. In particular, it remains unclear how to complete a given task provably within a minimum number of interactions with the external environment, e.g., through an internal mechanism of reasoning. To this end, we propose a principled framework with provable regret guarantees to orchestrate reasoning and acting, which we call "reason for future, act for now" (\texttt{RAFA}). Specifically, we design a prompt template for reasoning that learns from the memory buffer and plans a future trajectory over a long horizon ("reason for future"). At each step, the LLM agent takes the initial action of the planned trajectory ("act for now"), stores the collected feedback in the memory buffer, and reinvokes the reasoning routine to replan the future trajectory from the new state.  The key idea is to cast reasoning in LLMs as learning and planning in Bayes
    
[^25]: 基于启发式全局搜索的桁架结构尺寸优化问题的改进蒙特卡洛树搜索（UMCTS）算法

    Update Monte Carlo tree search (UMCTS) algorithm for heuristic global search of sizing optimization problems for truss structures. (arXiv:2309.06045v1 [cs.AI])

    [http://arxiv.org/abs/2309.06045](http://arxiv.org/abs/2309.06045)

    本文提出了一种基于启发式全局搜索的算法（UMCTS）用于桁架结构尺寸优化问题，通过结合更新过程和蒙特卡洛树搜索（MCTS）以及使用上界置信度（UCB）来获得合适的设计方案。

    

    桁架结构尺寸优化是一个复杂的计算问题，强化学习（RL）适用于处理无梯度计算的多模态问题。本研究提出了一种新的高效优化算法——更新蒙特卡洛树搜索（UMCTS），用于获得合适的桁架结构设计。UMCTS是一种基于RL的方法，将新颖的更新过程和蒙特卡洛树搜索（MCTS）与上界置信度（UCB）相结合。更新过程意味着在每一轮中，每个构件的最佳截面积通过搜索树确定，其初始状态是上一轮的最终状态。在UMCTS算法中，引入了加速选择成员面积和迭代次数的加速器，以减少计算时间。此外，对于每个状态，平均奖励被最佳奖励的模拟过程中收集来的奖励替代，确定最优解。本文提出了一种优化算法，通过结合更新过程和MCTS，以及使用UCB来优化桁架结构设计。

    Sizing optimization of truss structures is a complex computational problem, and the reinforcement learning (RL) is suitable for dealing with multimodal problems without gradient computations. In this paper, a new efficient optimization algorithm called update Monte Carlo tree search (UMCTS) is developed to obtain the appropriate design for truss structures. UMCTS is an RL-based method that combines the novel update process and Monte Carlo tree search (MCTS) with the upper confidence bound (UCB). Update process means that in each round, the optimal cross-sectional area of each member is determined by search tree, and its initial state is the final state in the previous round. In the UMCTS algorithm, an accelerator for the number of selections for member area and iteration number is introduced to reduce the computation time. Moreover, for each state, the average reward is replaced by the best reward collected on the simulation process to determine the optimal solution. The proposed optim
    
[^26]: 快速无监督深度异常值模型选择与超网络

    Fast Unsupervised Deep Outlier Model Selection with Hypernetworks. (arXiv:2307.10529v1 [cs.LG])

    [http://arxiv.org/abs/2307.10529](http://arxiv.org/abs/2307.10529)

    本文提出了HYPER用于调整基于深度神经网络的异常值检测模型，解决了无监督DOD模型中的超参数调整和模型选择的挑战，通过设计和训练超网络(HN)将超参数映射到主要DOD模型的最优权重上。

    

    异常值检测(OD)在许多领域都有应用，并有许多技术的丰富文献。基于深度神经网络的OD(DOD)由于深度学习的许多进展而受到了最近的关注。在本文中，我们考虑了一个关键但鲜为人知的问题，即无监督DOD的有效超参数(HP)调整/模型选择。虽然一些先前的工作报告了OD模型对HP的敏感性，但对于展示了长列表HP的现代DOD模型来说，这变得非常关键。我们引入了HYPER来调整DOD模型，解决了两个基本挑战：(1)无监督情况下的验证(由于缺乏标记的异常值)，以及(2) HP/模型空间的高效搜索 (由于HP数量的指数增长)。关键思想是设计和训练一个新颖的超网络(HN)，其将HP映射到主要DOD模型的最优权重上。反过来，HYPER利用一个单独的HN，可以动态生成多个DOD模型的权重 (对应于...)。

    Outlier detection (OD) finds many applications with a rich literature of numerous techniques. Deep neural network based OD (DOD) has seen a recent surge of attention thanks to the many advances in deep learning. In this paper, we consider a critical-yet-understudied challenge with unsupervised DOD, that is, effective hyperparameter (HP) tuning/model selection. While several prior work report the sensitivity of OD models to HPs, it becomes ever so critical for the modern DOD models that exhibit a long list of HPs. We introduce HYPER for tuning DOD models, tackling two fundamental challenges: (1) validation without supervision (due to lack of labeled anomalies), and (2) efficient search of the HP/model space (due to exponential growth in the number of HPs). A key idea is to design and train a novel hypernetwork (HN) that maps HPs onto optimal weights of the main DOD model. In turn, HYPER capitalizes on a single HN that can dynamically generate weights for many DOD models (corresponding t
    
[^27]: 学习用于测试时领域泛化的变分邻居标签

    Learning Variational Neighbor Labels for Test-Time Domain Generalization. (arXiv:2307.04033v1 [cs.LG])

    [http://arxiv.org/abs/2307.04033](http://arxiv.org/abs/2307.04033)

    本文提出了一种用于测试时领域泛化的方法，通过在测试时使用概率伪标签和变分邻居标签来推广源域训练的模型到目标领域，以提高模型的鲁棒性和准确性。

    

    本文致力于领域泛化，在未知的目标领域中只在源领域上进行训练模型。我们在源域上进行训练，然后在目标域上进行推理，利用无标签目标数据本身的价值。我们做出了三个贡献。首先，我们提出了目标样本的概率伪标签，以在测试时将源领域训练的模型推广到目标领域。我们将测试时的推广建模为变分推理问题，通过将伪标签建模为分布，考虑泛化过程中的不确定性，并减轻伪标签不准确性带来的误导信号。其次，我们学习了变分邻居标签，将邻近目标样本的信息纳入到生成更强鲁棒伪标签的过程中。第三，为了学习将更具代表性的目标信息纳入到生成更准确、更强鲁棒的变分邻居标签的能力中，我们

    This paper strives for domain generalization, where models are trained exclusively on source domains before being deployed at unseen target domains. We follow the strict separation of source training and target testing but exploit the value of the unlabeled target data itself during inference. We make three contributions. First, we propose probabilistic pseudo-labeling of target samples to generalize the source-trained model to the target domain at test time. We formulate the generalization at test time as a variational inference problem by modeling pseudo labels as distributions to consider the uncertainty during generalization and alleviate the misleading signal of inaccurate pseudo labels. Second, we learn variational neighbor labels that incorporate the information of neighboring target samples to generate more robust pseudo labels. Third, to learn the ability to incorporate more representative target information and generate more precise and robust variational neighbor labels, we 
    
[^28]: 评估生成AI系统在系统和社会中的社会影响

    Evaluating the Social Impact of Generative AI Systems in Systems and Society. (arXiv:2306.05949v1 [cs.CY])

    [http://arxiv.org/abs/2306.05949](http://arxiv.org/abs/2306.05949)

    提出了一种标准方法来评估任何模态的生成AI系统的社会影响，分为基础系统和社会方面的评估，涵盖7个社会影响类别，包括偏见、隐私保护、环境成本等。

    

    生成AI系统跨越文本、图像、音频、视频等多种模态，具有广泛的社会影响，但目前不存在官方标准来评估这些影响和应该评估哪些影响。本文提出了一种标准方法来评估任何模态的生成AI系统，分为两大类别：对于没有预定应用的基础系统可以评估什么，以及可以在社会中评估什么。我们描述了具体的社会影响类别以及如何评估基础技术系统、人民和社会。我们的基础系统框架定义了七个社会影响类别：偏见、刻板印象和表现性伤害；文化价值和敏感内容；不对等的性能；隐私和数据保护；财务成本；环境成本；以及数据和内容监管劳动成本。建议的评估方法适用于所有模态和分析。

    Generative AI systems across modalities, ranging from text, image, audio, and video, have broad social impacts, but there exists no official standard for means of evaluating those impacts and which impacts should be evaluated. We move toward a standard approach in evaluating a generative AI system for any modality, in two overarching categories: what is able to be evaluated in a base system that has no predetermined application and what is able to be evaluated in society. We describe specific social impact categories and how to approach and conduct evaluations in the base technical system, then in people and society. Our framework for a base system defines seven categories of social impact: bias, stereotypes, and representational harms; cultural values and sensitive content; disparate performance; privacy and data protection; financial costs; environmental costs; and data and content moderation labor costs. Suggested methods for evaluation apply to all modalities and analyses of the li
    
[^29]: 多模态可解释人工智能：方法学进展和未来研究方向的综述

    Multimodal Explainable Artificial Intelligence: A Comprehensive Review of Methodological Advances and Future Research Directions. (arXiv:2306.05731v1 [cs.AI])

    [http://arxiv.org/abs/2306.05731](http://arxiv.org/abs/2306.05731)

    本文系统地分析了多模态可解释人工智能（MXAI）领域的最新进展和未来研究方向，从预测任务和数据集到MXAI方法和评估度量标准进行了全面介绍和讨论。

    

    本研究系统地分析了多模态可解释人工智能（MXAI）领域的最新进展。特别是，首先描述了相关的主要预测任务和公开可用数据集。随后，提供了文献中MXAI方法的结构化介绍，考虑到以下标准：a）涉及的模态数量，b）产生解释的阶段，以及c）采用的方法论类型（即数学形式化）。然后，讨论了用于MXAI评估的度量标准。最后，提供了当前挑战和未来研究方向的全面分析。

    The current study focuses on systematically analyzing the recent advances in the field of Multimodal eXplainable Artificial Intelligence (MXAI). In particular, the relevant primary prediction tasks and publicly available datasets are initially described. Subsequently, a structured presentation of the MXAI methods of the literature is provided, taking into account the following criteria: a) The number of the involved modalities, b) The stage at which explanations are produced, and c) The type of the adopted methodology (i.e. mathematical formalism). Then, the metrics used for MXAI evaluation are discussed. Finally, a comprehensive analysis of current challenges and future research directions is provided.
    
[^30]: 使用值条件状态熵探索加速强化学习

    Accelerating Reinforcement Learning with Value-Conditional State Entropy Exploration. (arXiv:2305.19476v1 [cs.LG])

    [http://arxiv.org/abs/2305.19476](http://arxiv.org/abs/2305.19476)

    本文提出了一种新的探索技术，使用值条件状态熵来解决强化学习中探索不足的问题，可以均衡地覆盖低价值和高价值状态，相较于现有基于熵的探索方法，该方法在MuJoCo基准测试和Atari游戏上有着显著的提升。

    

    探索的一种有效技术是通过鼓励对访问状态空间的均匀覆盖来最大化已访问状态分布的熵，即状态熵。然而，它在有任务奖励的监督设置中往往难以应对，其中代理趋向于访问高价值状态以利用任务奖励。这个偏好会导致高价值状态和低价值状态的分布不平衡，当分布变得更加均匀时，状态熵会增加，从而偏向于探索低价值区域。当高价值状态在状态空间中分布狭窄时，这个问题会进一步恶化，使得代理完成任务变得更加困难。在本文中，我们提出了一种新颖的探索技术，最大化值条件状态熵，它分别估计每个状态价值估计条件下的状态熵，然后最大化它们的加权和。值条件状态熵量化了低价值和高价值状态区域的覆盖范围，从而使其对不平衡问题更加健壮。我们展示了我们的方法在一系列具有挑战性的MuJoCo基准测试和Atari游戏上显著优于现有的基于熵的探索方法。

    A promising technique for exploration is to maximize the entropy of visited state distribution, i.e., state entropy, by encouraging uniform coverage of visited state space. While it has been effective for an unsupervised setup, it tends to struggle in a supervised setup with a task reward, where an agent prefers to visit high-value states to exploit the task reward. Such a preference can cause an imbalance between the distributions of high-value states and low-value states, which biases exploration towards low-value state regions as a result of the state entropy increasing when the distribution becomes more uniform. This issue is exacerbated when high-value states are narrowly distributed within the state space, making it difficult for the agent to complete the tasks. In this paper, we present a novel exploration technique that maximizes the value-conditional state entropy, which separately estimates the state entropies that are conditioned on the value estimates of each state, then ma
    
[^31]: SpikeCP: 通过极限预测实现延迟自适应可靠脉冲神经网络

    SpikeCP: Delay-Adaptive Reliable Spiking Neural Networks via Conformal Prediction. (arXiv:2305.11322v1 [cs.NE])

    [http://arxiv.org/abs/2305.11322](http://arxiv.org/abs/2305.11322)

    这篇论文提出了一种新的脉冲神经网络模型，能够通过极限预测实现自适应的推断延迟，从而节约能源与提高可靠性。

    

    脉冲神经网络（SNN）通过内部事件驱动的神经动态处理时间序列数据，其能量消耗取决于输入演示期间神经元之间交换的脉冲数量。在典型的SNN分类器实现中，决策是在整个输入序列被处理后产生的，导致延迟和能量消耗水平在输入之间是相对均匀的。最近引入的延迟自适应SNN可根据每个示例的难度来定制推断延迟 - 以及随之而来的能耗 - 通过在SNN模型足够“自信”时产生早期决策来实现。

    Spiking neural networks (SNNs) process time-series data via internal event-driven neural dynamics whose energy consumption depends on the number of spikes exchanged between neurons over the course of the input presentation. In typical implementations of an SNN classifier, decisions are produced after the entire input sequence has been processed, resulting in latency and energy consumption levels that are fairly uniform across inputs. Recently introduced delay-adaptive SNNs tailor the inference latency -- and, with it, the energy consumption -- to the difficulty of each example, by producing an early decision when the SNN model is sufficiently ``confident''. In this paper, we start by observing that, as an SNN processes input samples, its classification decisions tend to be first under-confident and then over-confident with respect to the decision's ground-truth, unknown, test accuracy. This makes it difficult to determine a stopping time that ensures a desired level of accuracy. To add
    
[^32]: 利用实验经济学研究大型语言模型中出现的类似目标行为

    Investigating Emergent Goal-Like Behaviour in Large Language Models Using Experimental Economics. (arXiv:2305.07970v1 [cs.GT])

    [http://arxiv.org/abs/2305.07970](http://arxiv.org/abs/2305.07970)

    本研究探讨了大型语言模型的能力，发现其可以将自然语言描述转化为适当的行为，但在区分细微的合作和竞争水平方面的能力受到限制，为使用LLMs在人类决策制定背景下的伦理意义和局限性做出了贡献。

    

    本研究探讨了大型语言模型（LLMs），特别是GPT-3.5，实现合作、竞争、利他和自私行为的自然语言描述在社会困境下的能力。我们聚焦于迭代囚徒困境，这是一个非零和互动的经典例子，但我们的更广泛研究计划包括一系列实验经济学场景，包括最后通牒博弈、独裁者博弈和公共物品游戏。使用被试内实验设计，我们运用不同的提示信息实例化由LLM生成的智能体，表达不同的合作和竞争立场。我们评估了智能体在迭代囚徒困境中的合作水平，同时考虑到它们对合作或出尔反尔的伙伴行动的响应。我们的结果表明，LLMs在某种程度上可以将利他和自私的自然语言描述转化为适当的行为，但展示出区分合作和竞争水平的能力有限。总体而言，我们的研究为在人类决策制定的背景下使用LLMs的伦理意义和局限性提供了证据。

    In this study, we investigate the capacity of large language models (LLMs), specifically GPT-3.5, to operationalise natural language descriptions of cooperative, competitive, altruistic, and self-interested behavior in social dilemmas. Our focus is on the iterated Prisoner's Dilemma, a classic example of a non-zero-sum interaction, but our broader research program encompasses a range of experimental economics scenarios, including the ultimatum game, dictator game, and public goods game. Using a within-subject experimental design, we instantiated LLM-generated agents with various prompts that conveyed different cooperative and competitive stances. We then assessed the agents' level of cooperation in the iterated Prisoner's Dilemma, taking into account their responsiveness to the cooperative or defection actions of their partners. Our results provide evidence that LLMs can translate natural language descriptions of altruism and selfishness into appropriate behaviour to some extent, but e
    
[^33]: 高风险 AI 的团队监管：团队在循环中

    'Team-in-the-loop' organisational oversight of high-stakes AI. (arXiv:2303.14007v1 [cs.CY])

    [http://arxiv.org/abs/2303.14007](http://arxiv.org/abs/2303.14007)

    本论文通过对团队在 AI 系统中的监管流程的纵向观察，探讨了 AI 系统对临床决策制定中团队监管的影响，研究发现此前的专业团队监管方法主要依靠解释和问询来获取信息，而 AI 的引入将可能在信息披露和决策制定方面造成一定程度的影响。

    

    监管对于高风险公共部门 AI 应用程序至关重要，因为决策可能会对个人和集体产生深远影响。目前在公共部门中关于 AI 监管机制的许多思考都围绕着人类决策者处于 "循环中 "这一概念，并且能够干预以防止错误和潜在危害。然而，在许多高风险公共部门背景下，决策的运营监管是由专业团队而不是个人进行的。部署的 AI 系统如何整合到这些现有的团队监管流程中，尚未引起太多注意。我们通过制度分析探讨 AI 对临床决策制定的现有监管的影响，填补该方面的空白。我们发现，现有的监管嵌套在专业培训要求中，并且在征询关键信息时 heavilyrely  于解释和提问。专业团队使用各种会计披露技术来警告同事和监管行为。我们考虑了在 AI 系统引入到现有的团队监管流程中，信息披露和决策制定可能发生改变的几种方式。

    Oversight is rightly recognised as vital within high-stakes public sector AI applications, where decisions can have profound individual and collective impacts. Much current thinking regarding forms of oversight mechanisms for AI within the public sector revolves around the idea of human decision makers being 'in-the-loop' and thus being able to intervene to prevent errors and potential harm. However, in a number of high-stakes public sector contexts, operational oversight of decisions is made by expert teams rather than individuals. The ways in which deployed AI systems can be integrated into these existing operational team oversight processes has yet to attract much attention. We address this gap by exploring the impacts of AI upon pre-existing oversight of clinical decision-making through institutional analysis. We find that existing oversight is nested within professional training requirements and relies heavily upon explanation and questioning to elicit vital information. Professio
    
[^34]: 用蜜蜂算法优化深度学习模型参数，提高医学文本分类准确性

    Optimizing Deep Learning Model Parameters with the Bees Algorithm for Improved Medical Text Classification. (arXiv:2303.08021v1 [cs.CL])

    [http://arxiv.org/abs/2303.08021](http://arxiv.org/abs/2303.08021)

    本文使用蜜蜂算法优化了深度学习模型参数，提高了医学文本分类的准确性，最高准确率在英语数据集上达到了99.63%，在阿拉伯语数据集上达到了88%。

    

    本文介绍了一种使用蜜蜂算法对深度学习模型进行参数优化的新机制，这是一种最近很有前途的群智能算法。优化问题是在给定初始超参数的情况下，通过确定的迭代次数来最大化基于医学文本分类疾病的准确性。实验包括两个不同的数据集：英语和阿拉伯语。使用长短期记忆 (LSTM) 和蜜蜂算法，在英语数据集上获得了99.63%的最高准确率，在阿拉伯语数据集上使用AraBERT获得了88%的最高准确率。

    This paper introduces a novel mechanism to obtain the optimal parameters of a deep learning model using the Bees Algorithm, which is a recent promising swarm intelligence algorithm. The optimization problem is to maximize the accuracy of classifying ailments based on medical text given the initial hyper-parameters to be adjusted throughout a definite number of iterations. Experiments included two different datasets: English and Arabic. The highest accuracy achieved is 99.63% on the English dataset using Long Short-Term Memory (LSTM) along with the Bees Algorithm, and 88% on the Arabic dataset using AraBERT.
    
[^35]: 非欧几里德旅行商问题的凸包最便宜插入启发式解法

    A Convex Hull Cheapest Insertion Heuristic for the Non-Euclidean TSP. (arXiv:2302.06582v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.06582](http://arxiv.org/abs/2302.06582)

    本文提出了一种适用于非欧几里德旅行商问题的凸包最便宜插入启发式解法，通过使用多维缩放将非欧几里德空间的点近似到欧几里德空间，生成了初始化算法的凸包。在评估中发现，该算法在大多数情况下优于最邻近算法。

    

    众所周知，凸包最便宜插入启发式算法可以在欧几里德空间中产生良好的旅行商问题解决方案，但还未在非欧几里德情况下进行扩展。为了解决非欧几里德空间中处理障碍物的困难，提出的改进方法使用多维缩放将这些点首先近似到欧几里德空间，从而可以生成初始化算法的凸包。通过修改TSPLIB基准数据集，向其中添加不可通过的分割器来产生非欧几里德空间，评估了所提出的算法。在所研究的案例中，该算法表现出优于常用的最邻近算法的性能，达到96%的情况。

    The convex hull cheapest insertion heuristic is known to generate good solutions to the Traveling Salesperson Problem in Euclidean spaces, but it has not been extended to the non-Euclidean case. To address the difficulty of dealing with obstacles in the non-Euclidean space, the proposed adaptation uses multidimensional scaling to first approximate these points in a Euclidean space, thereby enabling the generation of the convex hull that initializes the algorithm. To evaluate the proposed algorithm, the TSPLIB benchmark data-set is modified by adding impassable separators that produce non-Euclidean spaces. The algorithm is demonstrated to outperform the commonly used Nearest Neighbor algorithm in 96% of the cases studied.
    
[^36]: 图神经网络中的解释方法：一项比较研究

    Explaining the Explainers in Graph Neural Networks: a Comparative Study. (arXiv:2210.15304v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.15304](http://arxiv.org/abs/2210.15304)

    该论文研究了图神经网络中的解释方法，并在多种数据集上测试了十种解释器的表现，提供了不同GNN体系结构易解释性的关键洞察。

    

    在图神经网络的快速发展后，GNN已经在许多科学和工程领域应用广泛，这促使需要方法来理解它们的决策过程。最近几年，GNN解释器开始出现，有多种方法，一些是新颖的，一些是从其他领域改编而来的。为了整理这种海量的解释方法，一些研究在各种可解释性指标方面对不同的解释器性能进行了基准测试。然而，这些早期的工作没有尝试提供关于不同的GNN体系结构更或不易解释的洞察，也没有说明在给定环境中应该选择哪种解释器。在本次调查中，我们通过设计系统性实验研究，对八个代表性体系结构上训练的十种解释器在六个精心设计的图和节点分类数据集上进行了测试，填补了这些空白，并提供了关键的观点。

    Following a fast initial breakthrough in graph based learning, Graph Neural Networks (GNNs) have reached a widespread application in many science and engineering fields, prompting the need for methods to understand their decision process.  GNN explainers have started to emerge in recent years, with a multitude of methods both novel or adapted from other domains. To sort out this plethora of alternative approaches, several studies have benchmarked the performance of different explainers in terms of various explainability metrics. However, these earlier works make no attempts at providing insights into why different GNN architectures are more or less explainable, or which explainer should be preferred in a given setting.  In this survey, we fill these gaps by devising a systematic experimental study, which tests ten explainers on eight representative architectures trained on six carefully designed graph and node classification datasets. With our results we provide key insights on the cho
    

