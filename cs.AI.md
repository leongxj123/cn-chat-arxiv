# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [FlightScope: A Deep Comprehensive Assessment of Aircraft Detection Algorithms in Satellite Imagery](https://arxiv.org/abs/2404.02877) | 本研究对卫星图像中识别飞机的任务自定义的一套先进对象检测算法进行了全面评估和比较，发现YOLOv5是在不同成像条件下展现高精度和适应性的最优模型。 |
| [^2] | [BERT-Enhanced Retrieval Tool for Homework Plagiarism Detection System](https://arxiv.org/abs/2404.01582) | 本文提出了一种基于GPT-3.5的抄袭文本数据生成方法和一种基于Faiss和BERT的高效高准确性的抄袭识别方法，填补了高水平抄袭检测研究数据集缺失的空白，实验证明该模型在多个指标上表现优异 |
| [^3] | [RLGNet: Repeating-Local-Global History Network for Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2404.00586) | RLGNet提出了重复-局部-全局历史网络，利用全局历史编码器和局部历史编码器分别捕捉历史信息的整体和相关细节，以有效进行时间知识图推理。 |
| [^4] | [LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership and Reasoning](https://arxiv.org/abs/2404.00027) | 探讨使用大型语言模型作为写作助手引发的写作所有权感和作者身份认知之间的心理困境。 |
| [^5] | [Ink and Individuality: Crafting a Personalised Narrative in the Age of LLMs](https://arxiv.org/abs/2404.00026) | 研究探讨了人们日益依赖的基于LLM的写作助手对创造力和个性可能造成的负面影响，旨在改进人机交互系统和提升写作助手的个性化和个性化功能。 |
| [^6] | [AI Consciousness is Inevitable: A Theoretical Computer Science Perspective](https://arxiv.org/abs/2403.17101) | 通过理论计算机科学的视角，作者提出了一个简单却强大的机器模型，支持了机器意识是不可避免的这一论断。 |
| [^7] | [RadioGAT: A Joint Model-based and Data-driven Framework for Multi-band Radiomap Reconstruction via Graph Attention Networks](https://arxiv.org/abs/2403.16397) | RadioGAT提出了一种基于图注意力网络的框架，用于解决多频带射频地图重建中的挑战，创新地将模型建模和数据驱动的方法相结合，消除了对多区域数据集的需求。 |
| [^8] | [Large Language Models in Biomedical and Health Informatics: A Bibliometric Review](https://arxiv.org/abs/2403.16303) | LLMs已成为生物医学与健康信息学中重要的工具，本文献计量学综述全面展示了LLMs在各种BHI领域中的应用，提出了其对自然语言处理应用的改进，揭示了主要发展趋势和研究网络，并讨论了伦理关切和实际挑战。 |
| [^9] | [N2F2: Hierarchical Scene Understanding with Nested Neural Feature Fields](https://arxiv.org/abs/2403.10997) | 利用Nested Neural Feature Fields (N2F2) 实现了层次化监督学习，提供了对物理维度或语义维度等不同粒度的场景属性全面和细致的理解。 |
| [^10] | [HealMe: Harnessing Cognitive Reframing in Large Language Models for Psychotherapy](https://arxiv.org/abs/2403.05574) | 这一创新心理治疗模型HealMe通过基于心理治疗框架的共情对话，有效解决了根深蒂固的负面思维，并促进了理性、平衡的观点。 |
| [^11] | [RAM-EHR: Retrieval Augmentation Meets Clinical Predictions on Electronic Health Records](https://arxiv.org/abs/2403.00815) | RAM-EHR通过增强检索并利用总结知识，提高了针对电子健康记录的临床预测效果。 |
| [^12] | [Technical Report on the Checkfor.ai AI-Generated Text Classifier](https://arxiv.org/abs/2402.14873) | Checkfor.ai AI生成文本分类器在区分大型语言模型生成文本和人类编写文本方面表现优异，提出了硬负挖掘与合成镜像训练算法，具有高准确性和泛化能力。 |
| [^13] | [Automated Design and Optimization of Distributed Filtering Circuits via Reinforcement Learning](https://arxiv.org/abs/2402.14236) | 提出一种通过强化学习算法实现的自动化设计方法，显著提高了分布式滤波电路设计的效率和质量。 |
| [^14] | [On the Fly Detection of Root Causes from Observed Data with Application to IT Systems](https://arxiv.org/abs/2402.06500) | 本文介绍了一种用于IT系统的新结构因果模型和一种快速检测异常根本原因的算法。该算法可在离线数据中进行因果发现，并在在线数据中遇到新异常时进行子图遍历。实验证明了方法的优越性能。 |
| [^15] | [Towards scalable robotic intervention of children with Autism Spectrum Disorder using LLMs](https://arxiv.org/abs/2402.00260) | 本文提出了一种以Large Language Model (LLM)为基础的社交机器人，用于与自闭症谱系障碍儿童进行口头交流，教授透视能力。通过比较不同的LLM管道，发现GPT-2 + BART管道可以更好地生成问题和选择项。这种研究有助于改善自闭症儿童的社交能力。 |
| [^16] | [Weighted Ensemble Models Are Strong Continual Learners](https://arxiv.org/abs/2312.08977) | 通过加权集成模型实现了高准确性的持续学习，兼顾可塑性和稳定性。 |
| [^17] | [Agent-OM: Leveraging LLM Agents for Ontology Matching](https://arxiv.org/abs/2312.00326) | 本研究提出了Agent-OM，利用LLM代理为本体匹配系统引入了新的设计范式。 |
| [^18] | [RGI-Net: 3D Room Geometry Inference from Room Impulse Responses in the Absence of First-order Echoes](https://arxiv.org/abs/2309.01513) | RGI-Net通过深度神经网络学习和利用房间脉冲响应中高阶反射之间的关系，实现在没有传统假设的情况下推断房间几何信息。 |
| [^19] | [Learning in Mean Field Games: A Survey](https://arxiv.org/abs/2205.12944) | 强化学习和均场博弈的结合有望在很大规模上解决游戏的均衡和社会最优问题。 |
| [^20] | [Memory-Efficient Sequential Pattern Mining with Hybrid Tries](https://arxiv.org/abs/2202.06834) | 提出了一种基于混合trie的内存高效序列模式挖掘方法，在内存消耗和计算时间方面相比现有技术有显著改善，且是唯一一个能够处理256GB系统内存下大数据集的方法。 |
| [^21] | [Finetuning Large Language Models for Vulnerability Detection.](http://arxiv.org/abs/2401.17010) | 本文优化了大规模语言模型用于源代码中的漏洞检测任务，通过微调最先进的代码语言模型WizardCoder并改进其训练过程和策略，实现了对漏洞数据集的分类性能的提升。 |
| [^22] | [Regularized Q-Learning with Linear Function Approximation.](http://arxiv.org/abs/2401.15196) | 本文提出了一种带有线性函数逼近的正则化Q学习算法，通过在不同尺度上操作，实现了有限时间内的收敛，并在马尔可夫噪声下具有性能保证。 |
| [^23] | [Towards Interpretable Physical-Conceptual Catchment-Scale Hydrological Modeling using the Mass-Conserving-Perceptron.](http://arxiv.org/abs/2401.14521) | 本研究通过利用质量守恒感知器构建基于有向图结构的水文模型，实现了对集水区尺度水文过程的解释能力，在保持简洁性的同时能够准确地模拟各种流量动力学行为，并通过引入输入旁路机制进一步优化了模型的表现。 |
| [^24] | [Adaptive Self-training Framework for Fine-grained Scene Graph Generation.](http://arxiv.org/abs/2401.09786) | 本论文提出了一种自适应自训练框架用于细粒度场景图生成，通过利用未标注的三元组缓解了场景图生成中的长尾问题。同时，引入了一种新颖的伪标签技术CATM和图结构学习器GSL来提高模型性能。 |
| [^25] | [CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians.](http://arxiv.org/abs/2401.05925) | CoSSegGaussians是一种紧凑且迅速的3D高斯场景分割方法，通过映射空间和语义特征实现紧凑和可靠的零样本场景分割。 |
| [^26] | [Dynamic Spiking Graph Neural Networks.](http://arxiv.org/abs/2401.05373) | 本文提出了一个名为"动态尖峰图神经网络"（DSGNN）的框架，它将尖峰神经网络（SNNs）与图神经网络（GNNs）结合起来，以解决动态图表示学习中的复杂性和内存开销问题。DSGNN通过动态调整尖峰神经元的状态和连接权重，在传播过程中保持图结构信息的完整性。 |
| [^27] | [Causal Understanding of Why Users Share Hate Speech on Social Media.](http://arxiv.org/abs/2310.15772) | 本文研究了用户为何分享社交媒体上的仇恨言论，提出了一个因果分析框架，通过消除数据偏差和模拟用户脆弱性来揭示影响用户分享行为的因素。 |
| [^28] | [MVMR: Evaluating Natural Language Video Localization Bias over Multiple Reliable Videos Pool.](http://arxiv.org/abs/2309.16701) | 本文提出了一个名为MVMR的任务，旨在给定文本查询从大量视频集中定位视频帧。我们通过已有数据集进行相似性筛选来构建数据集，并引入三个MVMR数据集。我们采用了嵌入式文本相似度匹配和视频-语言对齐技术来计算相关性得分，并为MVMR任务开发了一个强大的模型，Reliable Mutual Matching Network (RMMN)。 |
| [^29] | [Revealing the Power of Spatial-Temporal Masked Autoencoders in Multivariate Time Series Forecasting.](http://arxiv.org/abs/2309.15169) | 提出了一个利用空间-时间掩蔽自动编码器（STMAE）来提高多元时间序列（MTS）预测性能的框架，通过新颖的双掩蔽策略处理部分可见的MTS数据，包括空间掩蔽和时间掩蔽。 |
| [^30] | [LLM Platform Security: Applying a Systematic Evaluation Framework to OpenAI's ChatGPT Plugins.](http://arxiv.org/abs/2309.10254) | 本文提出了一个框架，用于分析和改进当前和未来与插件集成的LLM平台的安全性、隐私和安全性。在应用框架于OpenAI的插件生态系统时，我们发现了一些具体证明了潜在问题的插件。 |
| [^31] | [Publicly Shareable Clinical Large Language Model Built on Synthetic Clinical Notes.](http://arxiv.org/abs/2309.00237) | 使用合成临床记录构建的临床大语言模型可以克服临床记录的有限可及性和可用性的问题，并在现实应用中表现出潜在的良好性能。 |
| [^32] | [Multi-Source Domain Adaptation for Cross-Domain Fault Diagnosis of Chemical Processes.](http://arxiv.org/abs/2308.11247) | 本文在化学过程的交叉领域故障诊断中，对单源和多源无监督领域适应算法进行了广泛比较。研究结果表明，即使没有进行适应，使用多个领域进行训练也具有积极影响。 |
| [^33] | [Explainable Multi-View Deep Networks Methodology for Experimental Physics.](http://arxiv.org/abs/2308.08206) | 该论文介绍了一个可解释的多视角深度网络方法论，应用于实验物理中的多种成像表达分析。该方法论解决了多视角模型可解释性不足的问题。 |
| [^34] | [ChatGPT for Software Security: Exploring the Strengths and Limitations of ChatGPT in the Security Applications.](http://arxiv.org/abs/2307.12488) | 本文通过对ChatGPT在安全导向的程序分析中的表现进行研究，旨在了解其优势和局限性。研究结果可以帮助我们更好地理解ChatGPT在安全领域的应用潜力。 |
| [^35] | [Optimized Network Architectures for Large Language Model Training with Billions of Parameters.](http://arxiv.org/abs/2307.12169) | 本文提出了一种优化的网络架构，用于训练拥有数十亿参数的大型语言模型。这个架构根据语言模型的通信需求，将集群分割成一组通过非阻塞高带宽互连的GPU集合，并通过轨道连接仅连接具有通信需求的GPU，从而降低网络成本高达75％，同时不影响训练性能。 |
| [^36] | [Instruction Mining: High-Quality Instruction Data Selection for Large Language Models.](http://arxiv.org/abs/2307.06290) | 本文提出了InstructMining，一种用于选择高质量指令数据的线性规则，以增强大语言模型的解释和响应指令能力。通过特定的自然语言指标建模，研究结果表明，即使只有少量高质量的指令跟随数据，语言模型也可以进行良好的微调。 |
| [^37] | [Scaling Laws Do Not Scale.](http://arxiv.org/abs/2307.03201) | 本文讨论了缩放定律与人工智能模型性能之间的关系，并指出数据集规模的增加会引发不同社群的价值观和偏见风险。 |
| [^38] | [GIO: Gradient Information Optimization for Training Dataset Selection.](http://arxiv.org/abs/2306.11670) | GIO是一种可伸缩且任务不可知的方法，通过对目标的简单放松和高效实现，可以在非常小的训练集上产生出色的结果。 |
| [^39] | [Temporalising Unique Characterisability and Learnability of Ontology-Mediated Queries.](http://arxiv.org/abs/2306.07662) | 本文研究了在时间化本体中介查询中唯一可特征性和可学习性的问题，并提出了相应的传递结果。 |
| [^40] | [CoCo: A Coupled Contrastive Framework for Unsupervised Domain Adaptive Graph Classification.](http://arxiv.org/abs/2306.04979) | CoCo是一种耦合对比图表示学习框架，其中包含一个图卷积网络和一个分层图内核网络，通过耦合对比学习减少领域差异，用于无监督领域自适应图分类。 |
| [^41] | [Personality testing of GPT-3: Limited temporal reliability, but highlighted social desirability of GPT-3's personality instruments results.](http://arxiv.org/abs/2306.04308) | 本研究探讨了GPT-3 Davinci-003聊天机器人的人格特质，发现其具有良好的社交渴望和亲社会特质，但在不同时间的一致性存在限制。 |
| [^42] | [InstructIE: A Chinese Instruction-based Information Extraction Dataset.](http://arxiv.org/abs/2305.11527) | 介绍了一份中文的基于指令的信息提取数据集InstructIE，其中包括了270,000个弱监督的数据和1,000个高质量注释实例。实验结果表明当前的模型表现有待改进，该任务仍存在挑战。 |
| [^43] | [SongDriver2: Real-time Emotion-based Music Arrangement with Soft Transition.](http://arxiv.org/abs/2305.08029) | SongDriver2实现了基于情绪的实时音乐编排，并提出了柔和过渡机制，使音乐具有高度真实性和平滑过渡。 |
| [^44] | [Cost-efficient Crowdsourcing for Span-based Sequence Labeling: Worker Selection and Data Augmentation.](http://arxiv.org/abs/2305.06683) | 本文介绍了面向基于跨度的序列标注任务的成本效益众包算法，使用了组合多臂老虎机方法进行工人选择，并用移位、扩展和收缩的数据增强方法进行测试，提高了注释质量和降低成本，F1得分相对于仅专家的基线提高了100.04％，成本节约高达65.97％。 |
| [^45] | [Leveraging Pre-trained AudioLDM for Text to Sound Generation: A Benchmark Study.](http://arxiv.org/abs/2303.03857) | 本文研究了使用预训练的AudioLDM作为声音生成的骨干的优势，证明了在数据稀缺情况下使用预训练模型进行文本到声音生成的优势，并在几个常用数据集上使用相同的评估协议评估了各种文本到声音生成系统，为未来的研究提供了基础。 |
| [^46] | [Bayesian Opponent Modeling in Multiplayer Imperfect-Information Games.](http://arxiv.org/abs/2212.06027) | 该论文提出了一种针对多人不完全信息博弈的贝叶斯对手建模方法，在三人 Kuhn poker 中应用这种方法可以明显超过所有的代理商，包括准确的纳什均衡策略。 |

# 详细

[^1]: FlightScope: 卫星图像中飞行器检测算法的深度全面评估

    FlightScope: A Deep Comprehensive Assessment of Aircraft Detection Algorithms in Satellite Imagery

    [https://arxiv.org/abs/2404.02877](https://arxiv.org/abs/2404.02877)

    本研究对卫星图像中识别飞机的任务自定义的一套先进对象检测算法进行了全面评估和比较，发现YOLOv5是在不同成像条件下展现高精度和适应性的最优模型。

    

    arXiv:2404.02877v1 公告类型：跨领域 摘要：在遥感卫星图像中进行对象检测对于许多领域，如生物物理学和环境监测至关重要。尽管深度学习算法不断发展，但它们大多在常见的基于地面拍摄的照片上实施和测试。本文对一套针对在卫星图像中识别飞机这一任务定制的先进对象检测算法进行了批判性评估和比较。利用大型HRPlanesV2数据集，以及与GDIT数据集的严格验证，该研究涵盖了一系列方法，包括YOLO版本5和8、Faster RCNN、CenterNet、RetinaNet、RTMDet和DETR，均是从头开始训练的。这项全面的训练和验证研究揭示了YOLOv5作为识别遥感数据中的飞机这一特定案例的卓越模型，展示了其在不同成像条件下的高精度和适应性。

    arXiv:2404.02877v1 Announce Type: cross  Abstract: Object detection in remotely sensed satellite pictures is fundamental in many fields such as biophysical, and environmental monitoring. While deep learning algorithms are constantly evolving, they have been mostly implemented and tested on popular ground-based taken photos. This paper critically evaluates and compares a suite of advanced object detection algorithms customized for the task of identifying aircraft within satellite imagery. Using the large HRPlanesV2 dataset, together with a rigorous validation with the GDIT dataset, this research encompasses an array of methodologies including YOLO versions 5 and 8, Faster RCNN, CenterNet, RetinaNet, RTMDet, and DETR, all trained from scratch. This exhaustive training and validation study reveal YOLOv5 as the preeminent model for the specific case of identifying airplanes from remote sensing data, showcasing high precision and adaptability across diverse imaging conditions. This research
    
[^2]: 基于BERT增强的作业抄袭检测系统

    BERT-Enhanced Retrieval Tool for Homework Plagiarism Detection System

    [https://arxiv.org/abs/2404.01582](https://arxiv.org/abs/2404.01582)

    本文提出了一种基于GPT-3.5的抄袭文本数据生成方法和一种基于Faiss和BERT的高效高准确性的抄袭识别方法，填补了高水平抄袭检测研究数据集缺失的空白，实验证明该模型在多个指标上表现优异

    

    文本抄袭检测任务是一项常见的自然语言处理任务，旨在检测给定文本是否包含从其他文本中抄袭或复制的内容。在现有研究中，由于缺乏高质量的数据集，检测高水平的抄袭仍然是一个挑战。本文提出了一种基于GPT-3.5的抄袭文本数据生成方法，产生了32,927对文本抄袭检测数据集，涵盖了各种抄袭方法，填补了这一研究领域的空白。同时，我们提出了一种基于Faiss和BERT的高效高准确性的抄袭识别方法。我们的实验证明，这种模型在准确率、精确率、召回率和F1分数等多个指标上的表现优于其他模型，分别达到了98.86％、98.90％、98.86％和0.9888。最后，我们还提供了一个用户友好的演示平台，允许用户上传文本。

    arXiv:2404.01582v1 Announce Type: cross  Abstract: Text plagiarism detection task is a common natural language processing task that aims to detect whether a given text contains plagiarism or copying from other texts. In existing research, detection of high level plagiarism is still a challenge due to the lack of high quality datasets. In this paper, we propose a plagiarized text data generation method based on GPT-3.5, which produces 32,927 pairs of text plagiarism detection datasets covering a wide range of plagiarism methods, bridging the gap in this part of research. Meanwhile, we propose a plagiarism identification method based on Faiss with BERT with high efficiency and high accuracy. Our experiments show that the performance of this model outperforms other models in several metrics, including 98.86\%, 98.90%, 98.86%, and 0.9888 for Accuracy, Precision, Recall, and F1 Score, respectively. At the end, we also provide a user-friendly demo platform that allows users to upload a text 
    
[^3]: RLGNet：用于时间知识图推理的重复-局部-全局历史网络

    RLGNet: Repeating-Local-Global History Network for Temporal Knowledge Graph Reasoning

    [https://arxiv.org/abs/2404.00586](https://arxiv.org/abs/2404.00586)

    RLGNet提出了重复-局部-全局历史网络，利用全局历史编码器和局部历史编码器分别捕捉历史信息的整体和相关细节，以有效进行时间知识图推理。

    

    arXiv:2404.00586v1 公告类型：新 摘要：时间知识图（TKG）推理基于历史信息以预测未来。因此，解析和挖掘历史信息对预测未来至关重要。大多数现有方法未能同时从全局和局部角度处理和理解历史信息。忽视全局视图可能导致忽略宏观趋势和模式，而忽视局部视图可能导致遗漏关键详细信息。此外，一些方法不关注从高频重复事件中学习，这意味着它们可能无法充分掌握频繁发生的历史事件。基于此，我们提出了\textbf{R}epetitive-\textbf{L}ocal-\textbf{G}lobal History \textbf{Net}work（RLGNet）。我们利用全局历史编码器捕捉历史信息的全面性质。随后，局部历史编码器提供与查询时间戳相关的信息。

    arXiv:2404.00586v1 Announce Type: new  Abstract: Temporal Knowledge Graph (TKG) reasoning is based on historical information to predict the future. Therefore, parsing and mining historical information is key to predicting the future. Most existing methods fail to concurrently address and comprehend historical information from both global and local perspectives. Neglecting the global view might result in overlooking macroscopic trends and patterns, while ignoring the local view can lead to missing critical detailed information. Additionally, some methods do not focus on learning from high-frequency repeating events, which means they may not fully grasp frequently occurring historical events. To this end, we propose the \textbf{R}epetitive-\textbf{L}ocal-\textbf{G}lobal History \textbf{Net}work(RLGNet). We utilize a global history encoder to capture the overarching nature of historical information. Subsequently, the local history encoder provides information related to the query timestam
    
[^4]: LLM作为写作助手：探讨所有权感和推理的视角

    LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership and Reasoning

    [https://arxiv.org/abs/2404.00027](https://arxiv.org/abs/2404.00027)

    探讨使用大型语言模型作为写作助手引发的写作所有权感和作者身份认知之间的心理困境。

    

    写作中的所有权感限制了我们对思想、时间和贡献的投入，导致对产出物的依恋。然而，使用写作助手引入了一种心理困境，因为一些内容并非直接我们的创作。我们往往更倾向于在创造性任务中更多地归功于大型语言模型（LLMs），尽管它们对所有任务都是平等的。此外，虽然我们可能不会完全声称对由LLM生成的内容拥有所有权，但却自由地声称作者身份。我们进行了一项简短调查来研究这些问题，并了解潜在的认知过程，以更好地了解人机交互在写作中的应用并改进写作辅助系统。

    arXiv:2404.00027v1 Announce Type: cross  Abstract: Sense of ownership in writing confines our investment of thoughts, time, and contribution, leading to attachment to the output. However, using writing assistants introduces a mental dilemma, as some content isn't directly our creation. For instance, we tend to credit Large Language Models (LLMs) more in creative tasks, even though all tasks are equal for them. Additionally, while we may not claim complete ownership of LLM-generated content, we freely claim authorship. We conduct a short survey to examine these issues and understand underlying cognitive processes in order to gain a better knowledge of human-computer interaction in writing and improve writing aid systems.
    
[^5]: 墨水与个性：在LLMs时代塑造个性化叙事

    Ink and Individuality: Crafting a Personalised Narrative in the Age of LLMs

    [https://arxiv.org/abs/2404.00026](https://arxiv.org/abs/2404.00026)

    研究探讨了人们日益依赖的基于LLM的写作助手对创造力和个性可能造成的负面影响，旨在改进人机交互系统和提升写作助手的个性化和个性化功能。

    

    个性和个性化构成了使每个作家独特并影响其文字以有效吸引读者同时传达真实性的独特特征。然而，我们日益依赖基于LLM的写作助手可能会危及我们的创造力和个性。我们经常忽视这一趋势对我们的创造力和独特性的负面影响，尽管可能会造成后果。本研究通过进行简要调查探索不同的观点和概念，以及尝试理解人们的观点，结合以往在该领域的研究，来研究这些问题。解决这些问题对于改进人机交互系统和增强个性化和个性化写作助手至关重要。

    arXiv:2404.00026v1 Announce Type: cross  Abstract: Individuality and personalization comprise the distinctive characteristics that make each writer unique and influence their words in order to effectively engage readers while conveying authenticity. However, our growing reliance on LLM-based writing assistants risks compromising our creativity and individuality over time. We often overlook the negative impacts of this trend on our creativity and uniqueness, despite the possible consequences. This study investigates these concerns by performing a brief survey to explore different perspectives and concepts, as well as trying to understand people's viewpoints, in conjunction with past studies in the area. Addressing these issues is essential for improving human-computer interaction systems and enhancing writing assistants for personalization and individuality.
    
[^6]: 人工智能意识是不可避免的：一个理论计算机科学的视角

    AI Consciousness is Inevitable: A Theoretical Computer Science Perspective

    [https://arxiv.org/abs/2403.17101](https://arxiv.org/abs/2403.17101)

    通过理论计算机科学的视角，作者提出了一个简单却强大的机器模型，支持了机器意识是不可避免的这一论断。

    

    我们通过理论计算机科学的视角来审视意识，这是数学的一个分支，研究在资源限制下的计算。从这个角度出发，我们为意识开发了一个形式化的机器模型。这个模型受到了艾伦·图灵简单而强大的计算模型和伯纳德·巴尔斯意识剧场模型的启发。尽管非常简单，这个模型在高层次上与许多关于人类和动物意识的主要科学理论相一致，支持我们的论断：机器意识是不可避免的。

    arXiv:2403.17101v1 Announce Type: new  Abstract: We look at consciousness through the lens of Theoretical Computer Science, a branch of mathematics that studies computation under resource limitations. From this perspective, we develop a formal machine model for consciousness. The model is inspired by Alan Turing's simple yet powerful model of computation and Bernard Baars' theater model of consciousness. Though extremely simple, the model aligns at a high level with many of the major scientific theories of human and animal consciousness, supporting our claim that machine consciousness is inevitable.
    
[^7]: RadioGAT：基于联合模型和数据驱动的图注意力网络框架用于多频带无线射频地图重建

    RadioGAT: A Joint Model-based and Data-driven Framework for Multi-band Radiomap Reconstruction via Graph Attention Networks

    [https://arxiv.org/abs/2403.16397](https://arxiv.org/abs/2403.16397)

    RadioGAT提出了一种基于图注意力网络的框架，用于解决多频带射频地图重建中的挑战，创新地将模型建模和数据驱动的方法相结合，消除了对多区域数据集的需求。

    

    多频带射频地图重建（MB-RMR）是无线通信中的关键组成部分，用于任务如频谱管理和网络规划。然而，传统基于机器学习的MB-RMR方法在部署中面临重大挑战，这些方法严重依赖于模拟数据或完整结构化地面实况，这些挑战源自模拟数据与实际数据之间的差异，以及现实世界测量数据的稀缺性。为了解决这些挑战，我们的研究提出了RadioGAT，这是一个基于图注意力网络（GAT）的新框架，专门用于MB-RMR，在单个区域内消除了对多区域数据集的需求。RadioGAT创新地将基于模型的空间-频谱相关编码与数据驱动的射频地图泛化相结合，从而最大程度地减少对大量数据源的依赖。该框架通过创新的编码将稀疏的多频带数据转换为图结构开始。

    arXiv:2403.16397v1 Announce Type: cross  Abstract: Multi-band radiomap reconstruction (MB-RMR) is a key component in wireless communications for tasks such as spectrum management and network planning. However, traditional machine-learning-based MB-RMR methods, which rely heavily on simulated data or complete structured ground truth, face significant deployment challenges. These challenges stem from the differences between simulated and actual data, as well as the scarcity of real-world measurements. To address these challenges, our study presents RadioGAT, a novel framework based on Graph Attention Network (GAT) tailored for MB-RMR within a single area, eliminating the need for multi-region datasets. RadioGAT innovatively merges model-based spatial-spectral correlation encoding with data-driven radiomap generalization, thus minimizing the reliance on extensive data sources. The framework begins by transforming sparse multi-band data into a graph structure through an innovative encoding
    
[^8]: 生物医学与健康信息学中的大型语言模型：一项文献计量学综述

    Large Language Models in Biomedical and Health Informatics: A Bibliometric Review

    [https://arxiv.org/abs/2403.16303](https://arxiv.org/abs/2403.16303)

    LLMs已成为生物医学与健康信息学中重要的工具，本文献计量学综述全面展示了LLMs在各种BHI领域中的应用，提出了其对自然语言处理应用的改进，揭示了主要发展趋势和研究网络，并讨论了伦理关切和实际挑战。

    

    大型语言模型（LLMs）迅速成为生物医学与健康信息学（BHI）中的重要工具，为分析数据、治疗患者和开展研究提供了新的方式。本文献计量学综述旨在通过检查自2022年至2023年的研究文章和合作网络，全面展示LLMs在BHI中的应用情况。它进一步探讨了LLMs如何可以改进各种BHI领域中的自然语言处理（NLP）应用，如医学诊断、患者参与、电子健康记录管理和个性化医学。为此，我们的文献计量学综述确定了关键趋势，绘制了研究网络，并突出了这个快速发展领域的主要进展。最后，它讨论了在BHI中使用LLMs的伦理关切和实际挑战，如数据隐私和可靠的医疗建议。展望未来，我们考虑LLMs如何进一步改变生物医学研究。

    arXiv:2403.16303v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have rapidly become important tools in Biomedical and Health Informatics (BHI), enabling new ways to analyze data, treat patients, and conduct research. This bibliometric review aims to provide a panoramic view of how LLMs have been used in BHI by examining research articles and collaboration networks from 2022 to 2023. It further explores how LLMs can improve Natural Language Processing (NLP) applications in various BHI areas like medical diagnosis, patient engagement, electronic health record management, and personalized medicine. To do this, our bibliometric review identifies key trends, maps out research networks, and highlights major developments in this fast-moving field. Lastly, it discusses the ethical concerns and practical challenges of using LLMs in BHI, such as data privacy and reliable medical recommendations. Looking ahead, we consider how LLMs could further transform biomedical research as we
    
[^9]: 嵌套神经特征场的层次场景理解

    N2F2: Hierarchical Scene Understanding with Nested Neural Feature Fields

    [https://arxiv.org/abs/2403.10997](https://arxiv.org/abs/2403.10997)

    利用Nested Neural Feature Fields (N2F2) 实现了层次化监督学习，提供了对物理维度或语义维度等不同粒度的场景属性全面和细致的理解。

    

    在计算机视觉中，理解多层抽象的复杂场景仍然是一个巨大挑战。为了解决这个问题，我们引入了嵌套神经特征场 (N2F2)，这是一种新颖的方法，利用分层监督来学习单个特征场，在同一高维特征中的不同维度编码不同粒度的场景属性。我们的方法允许灵活定义层次，可以根据物理维度、语义维度或两者均匹配，从而实现对场景的全面和细致理解。我们利用2D类别无关分割模型在图像空间的任意尺度提供语义有意义的像素分组，并查询CLIP视觉编码器，为这些段落中的每个部分获得与语言对齐的嵌入。我们提出的分层监督方法将不同的嵌套特征场维度分配给提取C

    arXiv:2403.10997v1 Announce Type: cross  Abstract: Understanding complex scenes at multiple levels of abstraction remains a formidable challenge in computer vision. To address this, we introduce Nested Neural Feature Fields (N2F2), a novel approach that employs hierarchical supervision to learn a single feature field, wherein different dimensions within the same high-dimensional feature encode scene properties at varying granularities. Our method allows for a flexible definition of hierarchies, tailored to either the physical dimensions or semantics or both, thereby enabling a comprehensive and nuanced understanding of scenes. We leverage a 2D class-agnostic segmentation model to provide semantically meaningful pixel groupings at arbitrary scales in the image space, and query the CLIP vision-encoder to obtain language-aligned embeddings for each of these segments. Our proposed hierarchical supervision method then assigns different nested dimensions of the feature field to distill the C
    
[^10]: 利用大型语言模型在心理治疗中进行认知重构

    HealMe: Harnessing Cognitive Reframing in Large Language Models for Psychotherapy

    [https://arxiv.org/abs/2403.05574](https://arxiv.org/abs/2403.05574)

    这一创新心理治疗模型HealMe通过基于心理治疗框架的共情对话，有效解决了根深蒂固的负面思维，并促进了理性、平衡的观点。

    

    大型语言模型（LLMs）在心理治疗中可以发挥重要作用，熟练处理认知重构等关键任务，克服羞耻、不信任、治疗师技能差异和资源稀缺等挑战。在先前的认知重构中，主要将负面情绪转化为积极的，但这些方法效果有限，经常不能促进客户自我发现替代视角。在本文中，我们揭示了帮助和赋能通过自适应语言在心理增强（HealMe）模型。这种新颖的认知重构疗法方法有效地解决了根深蒂固的负面想法，并促进理性、平衡的视角。HealMe与传统LLM方法不同，采用基于心理治疗框架的共情对话。它通过系统指导客户区分情境和感受，集思广益寻找替代视角，并制定...

    arXiv:2403.05574v1 Announce Type: cross  Abstract: Large Language Models (LLMs) can play a vital role in psychotherapy by adeptly handling the crucial task of cognitive reframing and overcoming challenges such as shame, distrust, therapist skill variability, and resource scarcity. Previous LLMs in cognitive reframing mainly converted negative emotions to positive ones, but these approaches have limited efficacy, often not promoting clients' self-discovery of alternative perspectives. In this paper, we unveil the Helping and Empowering through Adaptive Language in Mental Enhancement (HealMe) model. This novel cognitive reframing therapy method effectively addresses deep-rooted negative thoughts and fosters rational, balanced perspectives. Diverging from traditional LLM methods, HealMe employs empathetic dialogue based on psychotherapeutic frameworks. It systematically guides clients through distinguishing circumstances from feelings, brainstorming alternative viewpoints, and developing 
    
[^11]: RAM-EHR: 电子健康记录上的检索增强与临床预测相遇

    RAM-EHR: Retrieval Augmentation Meets Clinical Predictions on Electronic Health Records

    [https://arxiv.org/abs/2403.00815](https://arxiv.org/abs/2403.00815)

    RAM-EHR通过增强检索并利用总结知识，提高了针对电子健康记录的临床预测效果。

    

    我们提出了RAM-EHR，这是一个用于改善电子健康记录（EHR）上临床预测的检索增强（Retrieval Augmentation）流程。RAM-EHR首先收集多个知识来源，将它们转换为文本格式，并使用密集检索来获取与医学概念相关的信息。这一策略解决了与复杂概念名称相关的困难。RAM-EHR然后增广了与一致性正则化代码联合训练的本地EHR预测模型，以捕获来自患者就诊和总结知识的互补信息。在两个EHR数据集上的实验表明，RAM-EHR相对于之前的知识增强基线效果显著（AUROC增益3.4％，AUPR增益7.2％），强调了RAM-EHR的总结知识对临床预测任务的有效性。代码将发布在\url{https://github.com/ritaranx/RAM-EHR}。

    arXiv:2403.00815v1 Announce Type: cross  Abstract: We present RAM-EHR, a Retrieval AugMentation pipeline to improve clinical predictions on Electronic Health Records (EHRs). RAM-EHR first collects multiple knowledge sources, converts them into text format, and uses dense retrieval to obtain information related to medical concepts. This strategy addresses the difficulties associated with complex names for the concepts. RAM-EHR then augments the local EHR predictive model co-trained with consistency regularization to capture complementary information from patient visits and summarized knowledge. Experiments on two EHR datasets show the efficacy of RAM-EHR over previous knowledge-enhanced baselines (3.4% gain in AUROC and 7.2% gain in AUPR), emphasizing the effectiveness of the summarized knowledge from RAM-EHR for clinical prediction tasks. The code will be published at \url{https://github.com/ritaranx/RAM-EHR}.
    
[^12]: Checkfor.ai AI生成文本分类器技术报告

    Technical Report on the Checkfor.ai AI-Generated Text Classifier

    [https://arxiv.org/abs/2402.14873](https://arxiv.org/abs/2402.14873)

    Checkfor.ai AI生成文本分类器在区分大型语言模型生成文本和人类编写文本方面表现优异，提出了硬负挖掘与合成镜像训练算法，具有高准确性和泛化能力。

    

    我们提出了Checkfor.ai文本分类器，这是一个基于Transformer的神经网络，经过训练可以区分由大型语言模型编写的文本和由人类编写的文本。Checkfor.ai在由十种文本领域（学生写作、创意写作、科学写作、书籍、百科全书、新闻、电子邮件、科学论文、简答问答）和8个开源闭源大型语言模型组成的综合基准测试中，表现优于零冲击方法如DetectGPT以及主流商业AI检测工具，误差率降低了9倍以上。我们提出了一种训练算法，即硬负挖掘与合成镜像，使我们的分类器能够在评论等高数据领域实现几个数量级的更低误报率。最后，我们展示了Checkfor.ai不对非母语英语人士产生偏见，并推广到训练过程中未见的领域和模型。

    arXiv:2402.14873v1 Announce Type: cross  Abstract: We present the Checkfor.ai text classifier, a transformer-based neural network trained to distinguish text written by large language models from text written by humans. Checkfor.ai outperforms zero-shot methods such as DetectGPT as well as leading commercial AI detection tools with over 9 times lower error rates on a comprehensive benchmark comprised of ten text domains (student writing, creative writing, scientific writing, books, encyclopedias, news, email, scientific papers, short-form Q\&A) and 8 open- and closed-source large language models. We propose a training algorithm, hard negative mining with synthetic mirrors, that enables our classifier to achieve orders of magnitude lower false positive rates on high-data domains such as reviews. Finally, we show that Checkfor.ai is not biased against nonnative English speakers and generalizes to domains and models unseen during training.
    
[^13]: 分布式滤波电路的自动设计与优化通过强化学习

    Automated Design and Optimization of Distributed Filtering Circuits via Reinforcement Learning

    [https://arxiv.org/abs/2402.14236](https://arxiv.org/abs/2402.14236)

    提出一种通过强化学习算法实现的自动化设计方法，显著提高了分布式滤波电路设计的效率和质量。

    

    设计分布式滤波电路(DFC)复杂且耗时，电路性能严重依赖电子工程师的专业知识和经验。然而，手动设计方法效率低下。本研究提出一种新的端到端自动化方法，利用强化学习算法来改进DFC的设计。所提出的方法消除了对工程师设计经验的依赖，显著降低了与电路设计相关的主观性和约束。实验结果表明，在与传统的工程师驱动方法进行比较时，所提出的方法在设计效率和质量上都有明显改善。特别是，在设计复杂或快速发展的DFC时，所提出的方法表现出卓越的性能。

    arXiv:2402.14236v1 Announce Type: cross  Abstract: Designing distributed filtering circuits (DFCs) is complex and time-consuming, with the circuit performance relying heavily on the expertise and experience of electronics engineers. However, manual design methods tend to have exceedingly low-efficiency. This study proposes a novel end-to-end automated method for fabricating circuits to improve the design of DFCs. The proposed method harnesses reinforcement learning (RL) algorithms, eliminating the dependence on the design experience of engineers. Thus, it significantly reduces the subjectivity and constraints associated with circuit design. The experimental findings demonstrate clear improvements in both design efficiency and quality when comparing the proposed method with traditional engineer-driven methods. In particular, the proposed method achieves superior performance when designing complex or rapidly evolving DFCs. Furthermore, compared to existing circuit automation design techn
    
[^14]: 在观察数据中实时检测根本原因，应用于IT系统

    On the Fly Detection of Root Causes from Observed Data with Application to IT Systems

    [https://arxiv.org/abs/2402.06500](https://arxiv.org/abs/2402.06500)

    本文介绍了一种用于IT系统的新结构因果模型和一种快速检测异常根本原因的算法。该算法可在离线数据中进行因果发现，并在在线数据中遇到新异常时进行子图遍历。实验证明了方法的优越性能。

    

    本文介绍了一种针对基于阈值的IT系统的新结构因果模型，并提出了一种新的算法，用于快速检测此类系统中异常的根本原因。当根本原因没有因果关联时，该方法被证明是正确的；同时，提出了一种基于代理干预来放松这种假设的扩展。我们的算法及其基于代理的扩展利用离线数据进行因果发现，并在遇到在线数据中的新异常时进行子图遍历。我们广泛的实验证明了我们方法的卓越性能，即使应用于来自替代结构因果模型或真实IT监控数据生成的数据时也是如此。

    This paper introduces a new structural causal model tailored for representing threshold-based IT systems and presents a new algorithm designed to rapidly detect root causes of anomalies in such systems. When root causes are not causally related, the method is proven to be correct; while an extension is proposed based on the intervention of an agent to relax this assumption. Our algorithm and its agent-based extension leverage causal discovery from offline data and engage in subgraph traversal when encountering new anomalies in online data. Our extensive experiments demonstrate the superior performance of our methods, even when applied to data generated from alternative structural causal models or real IT monitoring data.
    
[^15]: 以LLM为基础实现面向自闭症谱系障碍儿童的可扩展机器人干预

    Towards scalable robotic intervention of children with Autism Spectrum Disorder using LLMs

    [https://arxiv.org/abs/2402.00260](https://arxiv.org/abs/2402.00260)

    本文提出了一种以Large Language Model (LLM)为基础的社交机器人，用于与自闭症谱系障碍儿童进行口头交流，教授透视能力。通过比较不同的LLM管道，发现GPT-2 + BART管道可以更好地生成问题和选择项。这种研究有助于改善自闭症儿童的社交能力。

    

    本文提出了一种能够与自闭症谱系障碍(ASD)儿童进行口头交流的社交机器人。这种交流旨在通过使用Large Language Model (LLM)生成的文本来教授透视能力。社交机器人NAO扮演了一个刺激器(口头描述社交情景并提问)、提示器(提供三个选择项供选择)和奖励器(当答案正确时给予称赞)的角色。对于刺激器的角色，社交情境、问题和选择项是使用我们的LLM管道生成的。我们比较了两种方法：GPT-2 + BART和GPT-2 + GPT-2，其中第一个GPT-2在管道中是用于无监督社交情境生成的。我们使用SOCIALIQA数据集对所有LLM管道进行微调。我们发现，GPT-2 + BART管道在通过结合各自的损失函数来生成问题和选择项方面具有较好的BERTscore。这种观察结果也与儿童在交互过程中的合作水平一致。

    In this paper, we propose a social robot capable of verbally interacting with children with Autism Spectrum Disorder (ASD). This communication is meant to teach perspective-taking using text generated using a Large Language Model (LLM) pipeline. The social robot NAO acts as a stimulator (verbally describes a social situation and asks a question), prompter (presents three options to choose from), and reinforcer (praises when the answer is correct). For the role of the stimulator, the social situation, questions, and options are generated using our LLM pipeline. We compare two approaches: GPT-2 + BART and GPT-2 + GPT-2, where the first GPT-2 common between the pipelines is used for unsupervised social situation generation. We use the SOCIALIQA dataset to fine-tune all of our LLM pipelines. We found that the GPT-2 + BART pipeline had a better BERTscore for generating the questions and the options by combining their individual loss functions. This observation was also consistent with the h
    
[^16]: 加权集成模型是强大的持续学习者

    Weighted Ensemble Models Are Strong Continual Learners

    [https://arxiv.org/abs/2312.08977](https://arxiv.org/abs/2312.08977)

    通过加权集成模型实现了高准确性的持续学习，兼顾可塑性和稳定性。

    

    在本文中，我们研究持续学习（CL）的问题，其中目标是从一系列任务中学习模型，使得以前任务的数据在学习当前任务数据时不可用。CL本质上是在能够学习新任务（即可塑性）和保持先前学习概念的性能（即稳定性）之间取得平衡的过程。为了解决稳定性-可塑性的权衡问题，我们建议对先前和当前任务的模型参数进行加权集成。这种加权集成模型，我们称之为持续模型平均（或CoMA），通过利用可塑性在当前任务上获得高准确性，同时不会偏离太远的先前权重配置，从而确保稳定性。我们还提出了CoMA的改进型变体，名为持续费舍尔加权模型平均（或CoFiMA），该模型对每一个参数进行选择性加权。

    arXiv:2312.08977v2 Announce Type: replace-cross  Abstract: In this work, we study the problem of continual learning (CL) where the goal is to learn a model on a sequence of tasks, such that the data from the previous tasks becomes unavailable while learning on the current task data. CL is essentially a balancing act between being able to learn on the new task (i.e., plasticity) and maintaining the performance on the previously learned concepts (i.e., stability). Intending to address the stability-plasticity trade-off, we propose to perform weight-ensembling of the model parameters of the previous and current tasks. This weighted-ensembled model, which we call Continual Model Averaging (or CoMA), attains high accuracy on the current task by leveraging plasticity, while not deviating too far from the previous weight configuration, ensuring stability. We also propose an improved variant of CoMA, named Continual Fisher-weighted Model Averaging (or CoFiMA), that selectively weighs each para
    
[^17]: Agent-OM：利用LLM代理进行本体匹配

    Agent-OM: Leveraging LLM Agents for Ontology Matching

    [https://arxiv.org/abs/2312.00326](https://arxiv.org/abs/2312.00326)

    本研究提出了Agent-OM，利用LLM代理为本体匹配系统引入了新的设计范式。

    

    本体匹配（OM）能够实现不同本体之间的语义互操作性，通过对齐相关实体来解决其概念异构性。本研究引入了一种新颖的基于代理的LLM设计范式，命名为Agent-OM，包括两个用于检索和匹配的同体代理以及一组基于提示的简单OM工具。

    arXiv:2312.00326v2 Announce Type: replace  Abstract: Ontology matching (OM) enables semantic interoperability between different ontologies and resolves their conceptual heterogeneity by aligning related entities. OM systems currently have two prevailing design paradigms: conventional knowledge-based expert systems and newer machine learning-based predictive systems. While large language models (LLMs) and LLM agents have revolutionised data engineering and have been applied creatively in many domains, their potential for OM remains underexplored. This study introduces a novel agent-powered LLM-based design paradigm for OM systems. With consideration of several specific challenges in leveraging LLM agents for OM, we propose a generic framework, namely Agent-OM, consisting of two Siamese agents for retrieval and matching, with a set of simple prompt-based OM tools. Our framework is implemented in a proof-of-concept system. Evaluations of three Ontology Alignment Evaluation Initiative (OAE
    
[^18]: RGI-Net：在没有一阶回声的情况下从房间脉冲响应中推断3D房间几何

    RGI-Net: 3D Room Geometry Inference from Room Impulse Responses in the Absence of First-order Echoes

    [https://arxiv.org/abs/2309.01513](https://arxiv.org/abs/2309.01513)

    RGI-Net通过深度神经网络学习和利用房间脉冲响应中高阶反射之间的关系，实现在没有传统假设的情况下推断房间几何信息。

    

    房间几何是实现逼真的3D音频渲染的重要先验信息。为此，利用房间脉冲响应中到达时间（TOA）或到达时间差（TDOA）信息发展了各种房间几何推断（RGI）方法。然而，传统的RGI技术提出了一些假设，如凸房间形状、已知墙壁数量和一阶反射的可见性。在这项工作中，我们引入了深度神经网络（DNN）RGI-Net，它可以在没有上述假设的情况下估计房间几何。RGI-Net学习并利用房间脉冲响应（RIRs）中的高阶反射之间的复杂关系，因此可以在形状为非凸形或RIRs中缺少一阶反射的情况下估计房间形状。该网络采用从装有圆形麦克风的紧凑音频设备测量的RIRs。

    arXiv:2309.01513v2 Announce Type: replace-cross  Abstract: Room geometry is important prior information for implementing realistic 3D audio rendering. For this reason, various room geometry inference (RGI) methods have been developed by utilizing the time of arrival (TOA) or time difference of arrival (TDOA) information in room impulse responses. However, the conventional RGI technique poses several assumptions, such as convex room shapes, the number of walls known in priori, and the visibility of first-order reflections. In this work, we introduce the deep neural network (DNN), RGI-Net, which can estimate room geometries without the aforementioned assumptions. RGI-Net learns and exploits complex relationships between high-order reflections in room impulse responses (RIRs) and, thus, can estimate room shapes even when the shape is non-convex or first-order reflections are missing in the RIRs. The network takes RIRs measured from a compact audio device equipped with a circular microphon
    
[^19]: 在均场博弈中的学习：一项调查

    Learning in Mean Field Games: A Survey

    [https://arxiv.org/abs/2205.12944](https://arxiv.org/abs/2205.12944)

    强化学习和均场博弈的结合有望在很大规模上解决游戏的均衡和社会最优问题。

    

    非合作和合作游戏在拥有大量玩家时有许多应用，但随着玩家数量的增加，通常变得难以解决。均场博弈(Mean Field Games, MFGs)由Lasry和Lions以及Huang，Caines和Malham\'e引入，依靠均场近似允许玩家数量增长到无穷大。传统解决这些游戏的方法通常依赖于解决带有对模型的完全了解的偏微分方程或随机微分方程。最近，强化学习(Reinforcement Learning, RL)出现在解决规模复杂问题上表现出了很大的潜力。RL和MFGs的结合有望解决在人口规模和环境复杂性方面非常庞大的游戏。在这项调查中，我们回顾了最近迅速增长的关于RL方法在MFGs中学习均衡和社交最优的文献。我们首先确定了M中最常见的设置(静态、稳态和进化的)。

    arXiv:2205.12944v3 Announce Type: replace-cross  Abstract: Non-cooperative and cooperative games with a very large number of players have many applications but remain generally intractable when the number of players increases. Introduced by Lasry and Lions, and Huang, Caines and Malham\'e, Mean Field Games (MFGs) rely on a mean-field approximation to allow the number of players to grow to infinity. Traditional methods for solving these games generally rely on solving partial or stochastic differential equations with a full knowledge of the model. Recently, Reinforcement Learning (RL) has appeared promising to solve complex problems at scale. The combination of RL and MFGs is promising to solve games at a very large scale both in terms of population size and environment complexity. In this survey, we review the quickly growing recent literature on RL methods to learn equilibria and social optima in MFGs. We first identify the most common settings (static, stationary, and evolutive) of M
    
[^20]: 基于混合Tries的内存高效序列模式挖掘

    Memory-Efficient Sequential Pattern Mining with Hybrid Tries

    [https://arxiv.org/abs/2202.06834](https://arxiv.org/abs/2202.06834)

    提出了一种基于混合trie的内存高效序列模式挖掘方法，在内存消耗和计算时间方面相比现有技术有显著改善，且是唯一一个能够处理256GB系统内存下大数据集的方法。

    

    随着现代数据集的指数级增长，对于能够处理如此庞大数据集的高效挖掘算法的需求变得日益迫切。本文提出了一种内存高效的方法用于序列模式挖掘（SPM），这是知识发现中的一个基本主题，面临着针对大数据集的已知内存瓶颈。我们的方法涉及一种新颖的混合trie数据结构，利用重复模式紧凑地存储内存中的数据集; 以及一个相应的挖掘算法，旨在有效地从此紧凑表示中提取模式。对真实测试实例的数值结果显示，与最先进技术相比，对于小到中等大小的数据集，内存消耗平均提高了88％，计算时间提高了41％。此外，我们的算法是唯一一个在系统内存为256GB的情况下能够处理大数据集的SPM方法。

    arXiv:2202.06834v2 Announce Type: replace-cross  Abstract: As modern data sets continue to grow exponentially in size, the demand for efficient mining algorithms capable of handling such large data sets becomes increasingly imperative. This paper develops a memory-efficient approach for Sequential Pattern Mining (SPM), a fundamental topic in knowledge discovery that faces a well-known memory bottleneck for large data sets. Our methodology involves a novel hybrid trie data structure that exploits recurring patterns to compactly store the data set in memory; and a corresponding mining algorithm designed to effectively extract patterns from this compact representation. Numerical results on real-life test instances show an average improvement of 88% in memory consumption and 41% in computation time for small to medium-sized data sets compared to the state of the art. Furthermore, our algorithm stands out as the only capable SPM approach for large data sets within 256GB of system memory.
    
[^21]: 优化大规模语言模型用于漏洞检测

    Finetuning Large Language Models for Vulnerability Detection. (arXiv:2401.17010v1 [cs.CR])

    [http://arxiv.org/abs/2401.17010](http://arxiv.org/abs/2401.17010)

    本文优化了大规模语言模型用于源代码中的漏洞检测任务，通过微调最先进的代码语言模型WizardCoder并改进其训练过程和策略，实现了对漏洞数据集的分类性能的提升。

    

    本文介绍了对大规模语言模型进行微调，并将其用于源代码中的漏洞检测的结果。我们利用最先进的语言模型StarCoder的改进版本WizardCoder，并通过进一步微调将其适应于漏洞检测任务。为了加速训练，我们修改了WizardCoder的训练过程，并探究了最佳的训练策略。针对负样本远多于正样本的不平衡数据集，我们还尝试了不同的技术来提高分类性能。微调后的WizardCoder模型在平衡和不平衡的漏洞数据集上在ROC AUC和F1度量上实现了改进，证明了将预训练的语言模型用于源代码中的漏洞检测的有效性。主要贡献包括对最先进的代码语言模型WizardCoder进行微调，提高其训练速度而不影响性能，并对训练过程和策略进行了优化。

    This paper presents the results of finetuning large language models (LLMs) for the task of detecting vulnerabilities in source code. We leverage WizardCoder, a recent improvement of the state-of-the-art LLM StarCoder, and adapt it for vulnerability detection through further finetuning. To accelerate training, we modify WizardCoder's training procedure, also we investigate optimal training regimes. For the imbalanced dataset with many more negative examples than positive, we also explore different techniques to improve classification performance. The finetuned WizardCoder model achieves improvement in ROC AUC and F1 measures on balanced and imbalanced vulnerability datasets over CodeBERT-like model, demonstrating the effectiveness of adapting pretrained LLMs for vulnerability detection in source code. The key contributions are finetuning the state-of-the-art code LLM, WizardCoder, increasing its training speed without the performance harm, optimizing the training procedure and regimes, 
    
[^22]: 带有线性函数逼近的正则化Q学习

    Regularized Q-Learning with Linear Function Approximation. (arXiv:2401.15196v1 [cs.AI])

    [http://arxiv.org/abs/2401.15196](http://arxiv.org/abs/2401.15196)

    本文提出了一种带有线性函数逼近的正则化Q学习算法，通过在不同尺度上操作，实现了有限时间内的收敛，并在马尔可夫噪声下具有性能保证。

    

    一些成功的强化学习算法利用正则化来促进多模态策略，从而提高探索能力和鲁棒性。在使用函数逼近时，这些算法（如软Q学习）的收敛性质并不被很好地理解。本文考虑了一种单环路算法，在线性函数逼近的情况下具有有限时间收敛保证，用于最小化投影贝尔曼误差。该算法在两个尺度上运行：一个较慢的尺度用于更新状态动作值的目标网络，一个较快的尺度用于在基向量空间中逼近贝尔曼备份。在某些假设下，我们证明了在马尔可夫噪声存在下，该算法收敛于一个稳定点。此外，我们还提供了该算法衍生策略的性能保证。

    Several successful reinforcement learning algorithms make use of regularization to promote multi-modal policies that exhibit enhanced exploration and robustness. With functional approximation, the convergence properties of some of these algorithms (e.g. soft Q-learning) are not well understood. In this paper, we consider a single-loop algorithm for minimizing the projected Bellman error with finite time convergence guarantees in the case of linear function approximation. The algorithm operates on two scales: a slower scale for updating the target network of the state-action values, and a faster scale for approximating the Bellman backups in the subspace of the span of basis vectors. We show that, under certain assumptions, the proposed algorithm converges to a stationary point in the presence of Markovian noise. In addition, we provide a performance guarantee for the policies derived from the proposed algorithm.
    
[^23]: 以质量守恒感知器为基础，实现可解释的物理-概念集水区尺度水文建模

    Towards Interpretable Physical-Conceptual Catchment-Scale Hydrological Modeling using the Mass-Conserving-Perceptron. (arXiv:2401.14521v1 [cs.LG])

    [http://arxiv.org/abs/2401.14521](http://arxiv.org/abs/2401.14521)

    本研究通过利用质量守恒感知器构建基于有向图结构的水文模型，实现了对集水区尺度水文过程的解释能力，在保持简洁性的同时能够准确地模拟各种流量动力学行为，并通过引入输入旁路机制进一步优化了模型的表现。

    

    本研究探讨了利用机器学习技术开发简洁可解释的集水区尺度水文模型的可行性，采用基于质量守恒感知器（MCP）的有向图结构作为基本计算单元。我们关注的是单个位置的结构复杂性（深度），而不是对大样本集水区具有普适性的广度。目标是发现一个最小的表示（单元状态数和流量路径数），用于表示能够解释给定集水区输入状态和输出行为的主要过程，特别强调模拟全范围（高、中、低）的流量动力学。我们发现，在我们的研究区域，采用类似HyMod的架构，具有3个单元状态和2个主要流动路径，能够实现这样的表示，但引入输入旁路机制可以显著改善水文图的时间和形状。

    We investigate the applicability of machine learning technologies to the development of parsimonious, interpretable, catchment-scale hydrologic models using directed-graph architectures based on the mass-conserving perceptron (MCP) as the fundamental computational unit. Here, we focus on architectural complexity (depth) at a single location, rather than universal applicability (breadth) across large samples of catchments. The goal is to discover a minimal representation (numbers of cell-states and flow paths) that represents the dominant processes that can explain the input-state-output behaviors of a given catchment, with particular emphasis given to simulating the full range (high, medium, and low) of flow dynamics. We find that a HyMod-like architecture with three cell-states and two major flow pathways achieves such a representation at our study location, but that the additional incorporation of an input-bypass mechanism significantly improves the timing and shape of the hydrograph
    
[^24]: 自适应自训练框架用于细粒度场景图生成

    Adaptive Self-training Framework for Fine-grained Scene Graph Generation. (arXiv:2401.09786v1 [cs.CV])

    [http://arxiv.org/abs/2401.09786](http://arxiv.org/abs/2401.09786)

    本论文提出了一种自适应自训练框架用于细粒度场景图生成，通过利用未标注的三元组缓解了场景图生成中的长尾问题。同时，引入了一种新颖的伪标签技术CATM和图结构学习器GSL来提高模型性能。

    

    场景图生成（SGG）模型在基准数据集中存在长尾谓词分布和缺失注释问题。本研究旨在通过利用未标注的三元组缓解SGG的长尾问题。为此，我们引入了一种称为自训练SGG（ST-SGG）的框架，该框架基于未标注的三元组为其分配伪标签以训练SGG模型。虽然在图像识别方面的自训练取得了显著进展，但设计适用于SGG任务的自训练框架更具挑战，因为其固有特性，如语义歧义和长尾分布的谓词类别。因此，我们提出了一种新颖的SGG伪标签技术，称为具有动量的类别自适应阈值化（CATM），它是一种独立于模型的框架，可应用于任何已有的SGG模型。此外，我们设计了一个图结构学习器（GSL），从中获益。

    Scene graph generation (SGG) models have suffered from inherent problems regarding the benchmark datasets such as the long-tailed predicate distribution and missing annotation problems. In this work, we aim to alleviate the long-tailed problem of SGG by utilizing unannotated triplets. To this end, we introduce a Self-Training framework for SGG (ST-SGG) that assigns pseudo-labels for unannotated triplets based on which the SGG models are trained. While there has been significant progress in self-training for image recognition, designing a self-training framework for the SGG task is more challenging due to its inherent nature such as the semantic ambiguity and the long-tailed distribution of predicate classes. Hence, we propose a novel pseudo-labeling technique for SGG, called Class-specific Adaptive Thresholding with Momentum (CATM), which is a model-agnostic framework that can be applied to any existing SGG models. Furthermore, we devise a graph structure learner (GSL) that is benefici
    
[^25]: CoSSegGaussians：紧凑且迅速的3D高斯场景分割方法

    CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians. (arXiv:2401.05925v1 [cs.CV])

    [http://arxiv.org/abs/2401.05925](http://arxiv.org/abs/2401.05925)

    CoSSegGaussians是一种紧凑且迅速的3D高斯场景分割方法，通过映射空间和语义特征实现紧凑和可靠的零样本场景分割。

    

    我们提出了一种紧凑且迅速的3D高斯场景分割方法（CoSSegGaussians），该方法仅使用RGB图像输入，以快速的渲染速度实现紧凑的3D一致性场景分割。先前基于NeRF的3D分割方法依赖于隐式或体素神经场表示和光线行进体积渲染，这些方法耗时较长。最近的3D高斯场投影显著提高了渲染速度，然而，现有的基于高斯的分割方法（例如高斯分组）在零样本分割中没有提供紧凑的分割掩模，主要原因是在遇到不一致的2D机器生成标签时，无法直接为每个高斯分配可学习参数，缺乏鲁棒性和紧凑性。我们的方法旨在通过使用浅层解码网络将每个高斯点的融合空间和语义上有意义的特征映射，迅速实现紧凑且可靠的零样本场景分割。

    We propose Compact and Swift Segmenting 3D Gaussians(CoSSegGaussians), a method for compact 3D-consistent scene segmentation at fast rendering speed with only RGB images input. Previous NeRF-based 3D segmentation methods have relied on implicit or voxel neural scene representation and ray-marching volume rendering which are time consuming. Recent 3D Gaussian Splatting significantly improves the rendering speed, however, existing Gaussians-based segmentation methods(eg: Gaussian Grouping) fail to provide compact segmentation masks especially in zero-shot segmentation, which is mainly caused by the lack of robustness and compactness for straightforwardly assigning learnable parameters to each Gaussian when encountering inconsistent 2D machine-generated labels. Our method aims to achieve compact and reliable zero-shot scene segmentation swiftly by mapping fused spatial and semantically meaningful features for each Gaussian point with a shallow decoding network. Specifically, our method fi
    
[^26]: 动态尖峰图神经网络

    Dynamic Spiking Graph Neural Networks. (arXiv:2401.05373v1 [cs.NE])

    [http://arxiv.org/abs/2401.05373](http://arxiv.org/abs/2401.05373)

    本文提出了一个名为"动态尖峰图神经网络"（DSGNN）的框架，它将尖峰神经网络（SNNs）与图神经网络（GNNs）结合起来，以解决动态图表示学习中的复杂性和内存开销问题。DSGNN通过动态调整尖峰神经元的状态和连接权重，在传播过程中保持图结构信息的完整性。

    

    将尖峰神经网络（SNNs）和图神经网络（GNNs）相结合渐渐引起了人们的关注，这是因为它在处理由图表示的非欧几里得数据时具有低功耗和高效率。然而，作为一个常见的问题，动态图表示学习面临着高复杂性和大内存开销的挑战。目前的工作通常通过使用二进制特征而不是连续特征的SNNs来替代循环神经网络（RNNs）进行高效训练，这会忽视图结构信息并在传播过程中导致细节的丢失。此外，优化动态尖峰模型通常需要在时间步之间传播信息，这增加了内存需求。为了解决这些挑战，我们提出了一个名为"动态尖峰图神经网络"（\method{}）的框架。为了减轻信息丢失问题，\method{} 在传播过程中引入了一种新的机制，它在每个时间步骤中动态地调整尖峰神经元的状态和连接权重，以保持图结构信息的完整性。

    The integration of Spiking Neural Networks (SNNs) and Graph Neural Networks (GNNs) is gradually attracting attention due to the low power consumption and high efficiency in processing the non-Euclidean data represented by graphs. However, as a common problem, dynamic graph representation learning faces challenges such as high complexity and large memory overheads. Current work often uses SNNs instead of Recurrent Neural Networks (RNNs) by using binary features instead of continuous ones for efficient training, which would overlooks graph structure information and leads to the loss of details during propagation. Additionally, optimizing dynamic spiking models typically requires propagation of information across time steps, which increases memory requirements. To address these challenges, we present a framework named \underline{Dy}namic \underline{S}p\underline{i}king \underline{G}raph \underline{N}eural Networks (\method{}). To mitigate the information loss problem, \method{} propagates
    
[^27]: 用户在社交媒体上分享仇恨言论的因果理解

    Causal Understanding of Why Users Share Hate Speech on Social Media. (arXiv:2310.15772v1 [cs.SI])

    [http://arxiv.org/abs/2310.15772](http://arxiv.org/abs/2310.15772)

    本文研究了用户为何分享社交媒体上的仇恨言论，提出了一个因果分析框架，通过消除数据偏差和模拟用户脆弱性来揭示影响用户分享行为的因素。

    

    社交媒体上的仇恨言论威胁到个人的心理和身体健康，并且进一步导致现实中的暴力事件。仇恨言论传播背后的重要驱动因素是转发，但是人们很少了解为什么用户会转发仇恨言论。本文提供了一个全面、因果分析的用户属性框架，研究用户为何分享仇恨言论。然而，在从社交媒体数据中进行因果推断时存在一些挑战，因为这类数据很可能存在选择偏差，并且用户对仇恨言论的脆弱性存在混淆。我们开发了一个新颖的三步因果框架：（1）我们通过逆向倾向评分来消除观察性社交媒体数据的偏差。（2）我们使用消除偏差的倾向评分来模拟用户对仇恨言论的潜在脆弱性作为潜在嵌入。（3）我们建立了用户属性对用户分享仇恨言论概率的因果效应模型。

    Hate speech on social media threatens the mental and physical well-being of individuals and is further responsible for real-world violence. An important driver behind the spread of hate speech and thus why hateful posts can go viral are reshares, yet little is known about why users reshare hate speech. In this paper, we present a comprehensive, causal analysis of the user attributes that make users reshare hate speech. However, causal inference from observational social media data is challenging, because such data likely suffer from selection bias, and there is further confounding due to differences in the vulnerability of users to hate speech. We develop a novel, three-step causal framework: (1) We debias the observational social media data by applying inverse propensity scoring. (2) We use the debiased propensity scores to model the latent vulnerability of users to hate speech as a latent embedding. (3) We model the causal effects of user attributes on users' probability of sharing h
    
[^28]: MVMR: 在多个可靠视频集中评估自然语言视频定位偏差

    MVMR: Evaluating Natural Language Video Localization Bias over Multiple Reliable Videos Pool. (arXiv:2309.16701v1 [cs.CV])

    [http://arxiv.org/abs/2309.16701](http://arxiv.org/abs/2309.16701)

    本文提出了一个名为MVMR的任务，旨在给定文本查询从大量视频集中定位视频帧。我们通过已有数据集进行相似性筛选来构建数据集，并引入三个MVMR数据集。我们采用了嵌入式文本相似度匹配和视频-语言对齐技术来计算相关性得分，并为MVMR任务开发了一个强大的模型，Reliable Mutual Matching Network (RMMN)。

    

    随着近年来多媒体内容的激增，自然语言视频定位成为一个关键问题，它致力于检测与给定自然语言查询匹配的视频片段。然而，以往的研究都没有探索在存在多个正负视频的大量语料库中定位一个时刻。本文提出了一个名为MVMR（Massive Videos Moment Retrieval）的任务，旨在给定文本查询从大量视频集中定位视频帧。对于这个任务，我们提出了一种通过对现有视频定位数据集进行相似性筛选来构建数据集的方法，并引入了三个MVMR数据集。具体来说，我们采用基于嵌入的文本相似度匹配和视频-语言对齐技术来计算目标查询与视频之间的相关性得分，从而定义正负集。针对提出的MVMR任务，我们进一步开发了一个强大的模型，Reliable Mutual Matching Network (RMMN)。

    With the explosion of multimedia content in recent years, natural language video localization, which focuses on detecting video moment that matches a given natural language query, has become a critical problem. However, none of the previous research explores localizing a moment from a large corpus where multiple positive and negative videos exist. In this paper, we propose an MVMR (Massive Videos Moment Retrieval) task, which aims to localize video frames from a massive set of videos given a text query. For this task, we suggest methods for constructing datasets by employing similarity filtering on the existing video localization datasets and introduce three MVMR datasets. Specifically, we employ embedding-based text similarity matching and video-language grounding techniques to calculate the relevance score between a target query and videos to define positive and negative sets. For the proposed MVMR task, we further develop a strong model, Reliable Mutual Matching Network (RMMN), whic
    
[^29]: 揭示空间-时间掩蔽自动编码器在多元时间序列预测中的力量

    Revealing the Power of Spatial-Temporal Masked Autoencoders in Multivariate Time Series Forecasting. (arXiv:2309.15169v1 [cs.LG])

    [http://arxiv.org/abs/2309.15169](http://arxiv.org/abs/2309.15169)

    提出了一个利用空间-时间掩蔽自动编码器（STMAE）来提高多元时间序列（MTS）预测性能的框架，通过新颖的双掩蔽策略处理部分可见的MTS数据，包括空间掩蔽和时间掩蔽。

    

    多元时间序列（MTS）预测涉及基于历史观测来预测未来时间序列数据。现有研究主要强调开发能够明确捕捉时间序列变量之间的空间依赖性和时间相关性的复杂空间-时间模型。然而，最近的进展受到了数据稀缺性和模型鲁棒性的挑战。为了解决这些问题，我们提出了空间-时间掩蔽自动编码器（STMAE），这是一个MTS预测框架，利用掩蔽自动编码器来提高空间-时间基线模型的性能。STMAE包括两个学习阶段。在预训练阶段，采用编码器-解码器架构。编码器通过一种新颖的双掩蔽策略处理部分可见的MTS数据，包括基于偏置随机游走的空间掩蔽和基于补丁的时间掩蔽。随后，解码器旨在重构两个掩蔽对应物。

    Multivariate time series (MTS) forecasting involves predicting future time series data based on historical observations. Existing research primarily emphasizes the development of complex spatial-temporal models that capture spatial dependencies and temporal correlations among time series variables explicitly. However, recent advances have been impeded by challenges relating to data scarcity and model robustness. To address these issues, we propose Spatial-Temporal Masked Autoencoders (STMAE), an MTS forecasting framework that leverages masked autoencoders to enhance the performance of spatial-temporal baseline models. STMAE consists of two learning stages. In the pretraining stage, an encoder-decoder architecture is employed. The encoder processes the partially visible MTS data produced by a novel dual-masking strategy, including biased random walk-based spatial masking and patch-based temporal masking. Subsequently, the decoders aim to reconstruct the masked counterparts from both spa
    
[^30]: LLM平台安全：将系统评估框架应用于OpenAI的ChatGPT插件

    LLM Platform Security: Applying a Systematic Evaluation Framework to OpenAI's ChatGPT Plugins. (arXiv:2309.10254v1 [cs.CR])

    [http://arxiv.org/abs/2309.10254](http://arxiv.org/abs/2309.10254)

    本文提出了一个框架，用于分析和改进当前和未来与插件集成的LLM平台的安全性、隐私和安全性。在应用框架于OpenAI的插件生态系统时，我们发现了一些具体证明了潜在问题的插件。

    

    近期，如ChatGPT等大型语言模型（LLM）平台开始提供插件生态系统，以与互联网上的第三方服务进行交互。虽然这些插件扩展了LLM平台的功能，但它们是由任意的第三方开发的，因此不能隐式信任。插件还使用自然语言与LLM平台和用户进行交互，这可能导致模糊的解释。本文提出了一个框架，为LLM平台设计者分析和改进当前和未来与插件集成的LLM平台的安全性、隐私和安全性奠定了基础。我们的框架是一个攻击分类法的表述，通过迭代地探索LLM平台相关方如何利用他们的能力和责任对彼此进行攻击来开发的。作为我们迭代过程的一部分，我们将我们的框架应用于OpenAI的插件生态系统。我们揭示了一些具体证明了潜在问题的插件。

    Large language model (LLM) platforms, such as ChatGPT, have recently begun offering a plugin ecosystem to interface with third-party services on the internet. While these plugins extend the capabilities of LLM platforms, they are developed by arbitrary third parties and thus cannot be implicitly trusted. Plugins also interface with LLM platforms and users using natural language, which can have imprecise interpretations. In this paper, we propose a framework that lays a foundation for LLM platform designers to analyze and improve the security, privacy, and safety of current and future plugin-integrated LLM platforms. Our framework is a formulation of an attack taxonomy that is developed by iteratively exploring how LLM platform stakeholders could leverage their capabilities and responsibilities to mount attacks against each other. As part of our iterative process, we apply our framework in the context of OpenAI's plugin ecosystem. We uncover plugins that concretely demonstrate the poten
    
[^31]: 基于合成临床记录的公开可共享的临床大语言模型

    Publicly Shareable Clinical Large Language Model Built on Synthetic Clinical Notes. (arXiv:2309.00237v1 [cs.CL])

    [http://arxiv.org/abs/2309.00237](http://arxiv.org/abs/2309.00237)

    使用合成临床记录构建的临床大语言模型可以克服临床记录的有限可及性和可用性的问题，并在现实应用中表现出潜在的良好性能。

    

    基于合成的临床案例报告，我们首先创建了大规模的合成临床记录，以解决临床记录的有限可及性和可用性的问题。然后，我们使用这些合成记录来训练我们的专门的临床大语言模型Asclepius。虽然Asclepius是在合成数据上训练的，但我们通过使用真实临床记录对其进行评估，以评估其在现实应用中的潜在性能。我们将Asclepius与包括GPT-3.5-turbo和其他开源替代方案在内的几种其他大语言模型进行了基准测试。为了进一步验证我们使用合成记录的方法，我们还将Asclepius与其在真实临床记录上训练的变体进行了比较。我们的发现有力地证明，合成临床记录在构建临床大语言模型时可以作为可行的替代品。

    The development of large language models tailored for handling patients' clinical notes is often hindered by the limited accessibility and usability of these notes due to strict privacy regulations. To address these challenges, we first create synthetic large-scale clinical notes using publicly available case reports extracted from biomedical literature. We then use these synthetic notes to train our specialized clinical large language model, Asclepius. While Asclepius is trained on synthetic data, we assess its potential performance in real-world applications by evaluating it using real clinical notes. We benchmark Asclepius against several other large language models, including GPT-3.5-turbo and other open-source alternatives. To further validate our approach using synthetic notes, we also compare Asclepius with its variants trained on real clinical notes. Our findings convincingly demonstrate that synthetic clinical notes can serve as viable substitutes for real ones when constructi
    
[^32]: 多源领域适应用于化学过程交叉领域故障诊断

    Multi-Source Domain Adaptation for Cross-Domain Fault Diagnosis of Chemical Processes. (arXiv:2308.11247v1 [cs.LG])

    [http://arxiv.org/abs/2308.11247](http://arxiv.org/abs/2308.11247)

    本文在化学过程的交叉领域故障诊断中，对单源和多源无监督领域适应算法进行了广泛比较。研究结果表明，即使没有进行适应，使用多个领域进行训练也具有积极影响。

    

    故障诊断是过程监视中的重要组成部分。机器学习的故障诊断系统基于传感器数据预测故障类型。然而，这些模型对数据分布的变化敏感，这些变化可能由于监测过程中的变化，如操作模式的改变，导致跨领域故障诊断的情况。本文在化学工业中广泛使用的田纳西-伊斯曼过程的背景下，提供了单源和多源无监督领域适应算法在交叉领域故障诊断中的广泛比较。研究结果表明，即使没有进行适应，使用多个领域进行训练也具有积极影响。因此，多源无监督领域适应的基准模型相对于单源无监督领域适应的基准模型有所改进。

    Fault diagnosis is an essential component in process supervision. Indeed, it determines which kind of fault has occurred, given that it has been previously detected, allowing for appropriate intervention. Automatic fault diagnosis systems use machine learning for predicting the fault type from sensor readings. Nonetheless, these models are sensible to changes in the data distributions, which may be caused by changes in the monitored process, such as changes in the mode of operation. This scenario is known as Cross-Domain Fault Diagnosis (CDFD). We provide an extensive comparison of single and multi-source unsupervised domain adaptation (SSDA and MSDA respectively) algorithms for CDFD. We study these methods in the context of the Tennessee-Eastmann Process, a widely used benchmark in the chemical industry. We show that using multiple domains during training has a positive effect, even when no adaptation is employed. As such, the MSDA baseline improves over the SSDA baseline classificati
    
[^33]: 可解释的多视角深度网络方法论在实验物理中的应用

    Explainable Multi-View Deep Networks Methodology for Experimental Physics. (arXiv:2308.08206v1 [cs.CV])

    [http://arxiv.org/abs/2308.08206](http://arxiv.org/abs/2308.08206)

    该论文介绍了一个可解释的多视角深度网络方法论，应用于实验物理中的多种成像表达分析。该方法论解决了多视角模型可解释性不足的问题。

    

    物理实验常涉及多种成像表达，如X射线扫描和显微图像。深度学习模型已广泛应用于这些实验的监督分析中。合并不同的图像表达经常需要正确分析和做出决策。因此，多视角数据应运而生 - 数据集中的每个样本由来自不同角度、来源或模态的视图描述。多视角学习的概念解决了这些问题。理解深度学习模型的决策过程对于可靠和可信的分析至关重要。因此，最近提出了许多可解释性方法。然而，多视角模型缺乏适当的可解释性，由于其架构的复杂性，难以解释。在本文中，我们提出了适用于视觉领域的不同多视角架构，每个架构都适合解决不同的问题，并提出了解释多视角模型的方法论。

    Physical experiments often involve multiple imaging representations, such as X-ray scans and microscopic images. Deep learning models have been widely used for supervised analysis in these experiments. Combining different image representations is frequently required to analyze and make a decision properly. Consequently, multi-view data has emerged - datasets where each sample is described by views from different angles, sources, or modalities. These problems are addressed with the concept of multi-view learning. Understanding the decision-making process of deep learning models is essential for reliable and credible analysis. Hence, many explainability methods have been devised recently. Nonetheless, there is a lack of proper explainability in multi-view models, which are challenging to explain due to their architectures. In this paper, we suggest different multi-view architectures for the vision domain, each suited to another problem, and we also present a methodology for explaining th
    
[^34]: ChatGPT用于软件安全：探索ChatGPT在安全应用中的优点和局限性

    ChatGPT for Software Security: Exploring the Strengths and Limitations of ChatGPT in the Security Applications. (arXiv:2307.12488v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2307.12488](http://arxiv.org/abs/2307.12488)

    本文通过对ChatGPT在安全导向的程序分析中的表现进行研究，旨在了解其优势和局限性。研究结果可以帮助我们更好地理解ChatGPT在安全领域的应用潜力。

    

    作为一个多才多艺的大型语言模型，ChatGPT在各个领域应对问题的潜力得到了显著的展示。它能够分析、理解和综合来自在线资源和用户输入的信息，引起了广泛的关注。先前的研究已经探索了ChatGPT在代码生成和代码审查方面的能力。在本文中，我们深入研究了ChatGPT在面向安全的程序分析中的能力，从攻击者和安全分析师的角度进行了探讨。我们通过一个案例研究来评估ChatGPT在几个安全导向的程序分析任务中的回答质量，并有意地引入挑战来评估其响应能力。通过对ChatGPT提供的答案质量的考察，我们对其在安全导向的程序分析领域的优点和局限性有了更清晰的认识。

    ChatGPT, as a versatile large language model, has demonstrated remarkable potential in addressing inquiries across various domains. Its ability to analyze, comprehend, and synthesize information from both online sources and user inputs has garnered significant attention. Previous research has explored ChatGPT's competence in code generation and code reviews. In this paper, we delve into ChatGPT's capabilities in security-oriented program analysis, focusing on perspectives from both attackers and security analysts. We present a case study involving several security-oriented program analysis tasks while deliberately introducing challenges to assess ChatGPT's responses. Through an examination of the quality of answers provided by ChatGPT, we gain a clearer understanding of its strengths and limitations in the realm of security-oriented program analysis.
    
[^35]: 用于训练拥有数十亿参数的大型语言模型的优化网络架构

    Optimized Network Architectures for Large Language Model Training with Billions of Parameters. (arXiv:2307.12169v1 [cs.NI])

    [http://arxiv.org/abs/2307.12169](http://arxiv.org/abs/2307.12169)

    本文提出了一种优化的网络架构，用于训练拥有数十亿参数的大型语言模型。这个架构根据语言模型的通信需求，将集群分割成一组通过非阻塞高带宽互连的GPU集合，并通过轨道连接仅连接具有通信需求的GPU，从而降低网络成本高达75％，同时不影响训练性能。

    

    本文挑战了为训练大型语言模型（LLMs）构建任意到任意网络的传统范式。我们展示了LLMs呈现出一种独特的通信模式，在其中，只有小组的GPU需要高带宽的任意到任意通信，以实现接近最优的训练性能。在这些GPU小组之间，通信非常微不足道、稀疏且均匀。我们提出了一个新的网络架构，紧密匹配LLMs的通信需求。我们的架构将集群分割为一组通过非阻塞任意到任意高带宽互连的GPU集合，我们称之为HB域。在HB域之间，网络只连接具有通信需求的GPU。我们将这种网络连接称为“仅轨道连接”，并展示了我们的架构相对于最先进的任意到任意Clos网络可以将网络成本降低高达75％，同时不损害LLM训练的性能。

    This paper challenges the well-established paradigm for building any-to-any networks for training Large Language Models (LLMs). We show that LLMs exhibit a unique communication pattern where only small groups of GPUs require high-bandwidth any-to-any communication within them, to achieve near-optimal training performance. Across these groups of GPUs, the communication is insignificant, sparse, and homogeneous. We propose a new network architecture that closely resembles the communication requirement of LLMs. Our architecture partitions the cluster into sets of GPUs interconnected with non-blocking any-to-any high-bandwidth interconnects that we call HB domains. Across the HB domains, the network only connects GPUs with communication demands. We call this network a "rail-only" connection, and show that our proposed architecture reduces the network cost by up to 75% compared to the state-of-the-art any-to-any Clos networks without compromising the performance of LLM training.
    
[^36]: 指令挖掘：大语言模型的高质量指令数据选择

    Instruction Mining: High-Quality Instruction Data Selection for Large Language Models. (arXiv:2307.06290v1 [cs.CL])

    [http://arxiv.org/abs/2307.06290](http://arxiv.org/abs/2307.06290)

    本文提出了InstructMining，一种用于选择高质量指令数据的线性规则，以增强大语言模型的解释和响应指令能力。通过特定的自然语言指标建模，研究结果表明，即使只有少量高质量的指令跟随数据，语言模型也可以进行良好的微调。

    

    大型语言模型通常经历预训练和微调两个训练阶段。尽管大规模预训练赋予模型强大的生成自然语言回应的能力，但这些预训练模型有时仍然无法理解人类指令。为了增强语言模型解释和响应指令的能力，指令微调已成为该领域的关键方法。最近的研究发现，即使只有少量高质量的指令跟随数据，大型语言模型也可以进行良好的微调。然而，选择用于微调语言模型的高质量数据集仍缺乏明确的指导方针。在本文中，我们提出了InstructMining，一个用于评估指令跟随数据质量的线性规则。我们使用具体的自然语言指标来进行InstructMining的建模。为了研究数据质量与这些指标之间的关系，我们还进行了广泛的细致研究。

    Large language models typically undergo two training stages, pretraining and finetuning. Despite that large-scale pretraining endows the model with strong capabilities to generate natural language responses, these pretrained models can still fail to understand human instructions at times. To enhance language models' ability of interpreting and responding to instructions, instruction finetuning has emerged as a critical method in this area. Recent studies found that large language models can be finetuned to perform well even with a small amount of high-quality instruction-following data. However, the selection of high-quality datasets for finetuning language models still lacks clear guidelines to follow. In this paper, we propose InstructMining, a linear rule for evaluating instruction-following data quality. We formulate InstructMining using specific natural language indicators. To investigate the relationship between data quality and these indicators, we further conduct extensive fine
    
[^37]: 缩放定律不具备可扩展性

    Scaling Laws Do Not Scale. (arXiv:2307.03201v1 [cs.LG])

    [http://arxiv.org/abs/2307.03201](http://arxiv.org/abs/2307.03201)

    本文讨论了缩放定律与人工智能模型性能之间的关系，并指出数据集规模的增加会引发不同社群的价值观和偏见风险。

    

    最近的研究提出了一种称为“缩放定律”的幂律关系，它描述了人工智能（AI）模型的性能与模型设计的各个方面（如数据集大小）之间的关系。换句话说，随着数据集（或模型参数等）的增加，基于该数据集训练的模型的性能将相应增加。然而，在总体上具有吸引力的同时，这种缩放定律关系忽视了用于衡量性能的指标可能是不稳定和有争议的，或者可能不符合不同人群对模型输出质量的感知。本文提出，随着用于训练大型AI模型的数据集规模增长，数据集中包含的不同社群（包括人口统计学群体）的数量可能会增加，每个社群可能具有不同的价值观。因此，数据集中所代表的社群可能存在价值观或偏见的风险。

    Recent work has proposed a power law relationship, referred to as ``scaling laws,'' between the performance of artificial intelligence (AI) models and aspects of those models' design (e.g., dataset size). In other words, as the size of a dataset (or model parameters, etc) increases, the performance of a given model trained on that dataset will correspondingly increase. However, while compelling in the aggregate, this scaling law relationship overlooks the ways that metrics used to measure performance may be precarious and contested, or may not correspond with how different groups of people may perceive the quality of models' output. In this paper, we argue that as the size of datasets used to train large AI models grows, the number of distinct communities (including demographic groups) whose data is included in a given dataset is likely to grow, each of whom may have different values. As a result, there is an increased risk that communities represented in a dataset may have values or p
    
[^38]: GIO：用于训练数据集选择的梯度信息优化

    GIO: Gradient Information Optimization for Training Dataset Selection. (arXiv:2306.11670v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.11670](http://arxiv.org/abs/2306.11670)

    GIO是一种可伸缩且任务不可知的方法，通过对目标的简单放松和高效实现，可以在非常小的训练集上产生出色的结果。

    

    在训练模型时，通常有利于在可用训练样本的子集上进行训练，因为这些样本具有不同的质量，或者希望在不影响性能的情况下使用更少的样本进行训练。我们提出了梯度信息优化（GIO），这是一种可伸缩且任务不可知的方法，用于解决这个数据选择问题，它只需要一小组（未标记的）样本来代表目标分布。GIO从一个实践中难以处理的自然的信息理论目标开始。我们的贡献在于展示出通过对目标进行简单的放松和高效的实现，它可以被高度扩展。在机器翻译、拼写纠正和图像识别等实验中，我们证明了GIO在非常小的训练集上产生了出色的结果。这些发现对于GIO本身的不同表示模型和超参数是稳健的。GIO是任务和领域无关的，可以直接应用于新领域。

    It is often advantageous to train models on a subset of the available train examples, because the examples are of variable quality or because one would like to train with fewer examples, without sacrificing performance. We present Gradient Information Optimization (GIO), a scalable, task-agnostic approach to this data selection problem that requires only a small set of (unlabeled) examples representing a target distribution. GIO begins from a natural, information-theoretic objective that is intractable in practice. Our contribution is in showing that it can be made highly scalable through a simple relaxation of the objective and a highly efficient implementation. In experiments with machine translation, spelling correction, and image recognition, we show that GIO delivers outstanding results with very small train sets. These findings are robust to different representation models and hyperparameters for GIO itself. GIO is task- and domain-agnostic and can be applied out-of-the-box to ne
    
[^39]: 时间化本体中介查询的唯一可特征性和可学习性

    Temporalising Unique Characterisability and Learnability of Ontology-Mediated Queries. (arXiv:2306.07662v1 [cs.AI])

    [http://arxiv.org/abs/2306.07662](http://arxiv.org/abs/2306.07662)

    本文研究了在时间化本体中介查询中唯一可特征性和可学习性的问题，并提出了相应的传递结果。

    

    最近，通过示例来研究数据库查询的唯一可特征性和可学习性已经扩展到了本体中介查询。在这里，我们研究了获得的结果在多大程度上可以提升到时间化的本体中介查询。我们系统地介绍了非时间化情况下相关方法，然后展示了通用的传递结果，可以确定现有结果在何种条件下可以推广到时间化查询。

    Recently, the study of the unique characterisability and learnability of database queries by means of examples has been extended to ontology-mediated queries. Here, we study in how far the obtained results can be lifted to temporalised ontology-mediated queries. We provide a systematic introduction to the relevant approaches in the non-temporal case and then show general transfer results pinpointing under which conditions existing results can be lifted to temporalised queries.
    
[^40]: CoCo: 一种用于无监督领域自适应图分类的耦合对比框架

    CoCo: A Coupled Contrastive Framework for Unsupervised Domain Adaptive Graph Classification. (arXiv:2306.04979v1 [cs.LG])

    [http://arxiv.org/abs/2306.04979](http://arxiv.org/abs/2306.04979)

    CoCo是一种耦合对比图表示学习框架，其中包含一个图卷积网络和一个分层图内核网络，通过耦合对比学习减少领域差异，用于无监督领域自适应图分类。

    

    虽然图神经网络在图分类中取得了显著成果，但它们通常需要大量特定任务的标签，这可能需要极大的代价来获得。一种可靠的解决方案是探索其他标注图以增强目标域的无监督学习，但如何将图神经网络应用到领域适应中仍未解决，因为对图拓扑的不充分探索以及相当大的领域偏差。本文提出了一种称为CoCo（Coupled Contrastive Graph Representation Learning）方案，该方案从耦合学习分支中提取拓扑信息，并通过耦合对比学习减少领域差异。CoCo包含一个图卷积网络分支和分层图内核网络分支，分别用隐式和显式方式探索图拓扑。此外，我们将耦合分支结合到一个全面的多视角对比学习框架中，

    Although graph neural networks (GNNs) have achieved impressive achievements in graph classification, they often need abundant task-specific labels, which could be extensively costly to acquire. A credible solution is to explore additional labeled graphs to enhance unsupervised learning on the target domain. However, how to apply GNNs to domain adaptation remains unsolved owing to the insufficient exploration of graph topology and the significant domain discrepancy. In this paper, we propose \underline{Co}upled \underline{Co}ntrastive Graph Representation Learning (\method{}), which extracts the topological information from coupled learning branches and reduces the domain discrepancy with coupled contrastive learning. \method{} contains a graph convolutional network branch and a hierarchical graph kernel network branch, which explore graph topology in implicit and explicit manners. Besides, we incorporate coupled branches into a holistic multi-view contrastive learning framework, which 
    
[^41]: GPT-3的人格测试：时间可靠性有限，但凸显了社交渴望的人格工具结果。

    Personality testing of GPT-3: Limited temporal reliability, but highlighted social desirability of GPT-3's personality instruments results. (arXiv:2306.04308v1 [cs.AI])

    [http://arxiv.org/abs/2306.04308](http://arxiv.org/abs/2306.04308)

    本研究探讨了GPT-3 Davinci-003聊天机器人的人格特质，发现其具有良好的社交渴望和亲社会特质，但在不同时间的一致性存在限制。

    

    为了评估聊天机器人GPT-3 Davinci-003的潜在应用和限制，本研究探讨了应用于聊天机器人及其个性化资料的人格问卷的时间可靠性。在两个不同的场合，心理问卷被应用于聊天机器人，然后将回答与人类基准数据进行比较。研究结果显示，聊天机器人的回答有不同程度的一致性，有些量表表现出良好的一致性，而有些则表现出较差的一致性。总体而言，Davinci-003显示出一个社交渴望和亲社会的人格特质，尤其是在亲和力领域。然而，聊天机器人回答的基础，无论是由主观自我反思还是预定算法驱动，尚不确定。

    To assess the potential applications and limitations of chatbot GPT-3 Davinci-003, this study explored the temporal reliability of personality questionnaires applied to the chatbot and its personality profile. Psychological questionnaires were administered to the chatbot on two separate occasions, followed by a comparison of the responses to human normative data. The findings revealed varying levels of agreement in the chatbot's responses over time, with some scales displaying excellent while others demonstrated poor agreement. Overall, Davinci-003 displayed a socially desirable and pro-social personality profile, particularly in the domain of communion. However, the underlying basis of the chatbot's responses, whether driven by conscious self-reflection or predetermined algorithms, remains uncertain.
    
[^42]: InstructIE: 一份基于指令的中文信息提取数据集

    InstructIE: A Chinese Instruction-based Information Extraction Dataset. (arXiv:2305.11527v1 [cs.CL])

    [http://arxiv.org/abs/2305.11527](http://arxiv.org/abs/2305.11527)

    介绍了一份中文的基于指令的信息提取数据集InstructIE，其中包括了270,000个弱监督的数据和1,000个高质量注释实例。实验结果表明当前的模型表现有待改进，该任务仍存在挑战。

    

    我们引入了一项新的信息提取任务，称为基于指令的信息提取 (Instruction-based IE)，它旨在要求系统遵循特定的指令或指南来提取信息。为了促进该领域的研究，我们构建了一个数据集，称为InstructIE，其中包括来自中文维基百科的 270,000 个弱监督数据和 1,000 个高质量众包注释实例。我们进一步评估了各种基线模型在InstructIE数据集上的表现。结果表明，尽管当前的模型表现很有希望，但仍有改进的空间。此外，我们进行了全面的案例研究分析，强调了基于指令的信息提取任务中固有的挑战。代码和数据集可在 https://github.com/zjunlp/DeepKE/tree/main/example/llm 找到。

    We introduce a new Information Extraction (IE) task dubbed Instruction-based IE, which aims to ask the system to follow specific instructions or guidelines to extract information. To facilitate research in this area, we construct a dataset called InstructIE, consisting of 270,000 weakly supervised data from Chinese Wikipedia and 1,000 high-quality crowdsourced annotated instances. We further evaluate the performance of various baseline models on the InstructIE dataset. The results reveal that although current models exhibit promising performance, there is still room for improvement. Furthermore, we conduct a comprehensive case study analysis, underlining the challenges inherent in the Instruction-based IE task. Code and dataset are available at https://github.com/zjunlp/DeepKE/tree/main/example/llm.
    
[^43]: SongDriver2：基于情绪的实时音乐编排与柔和过渡

    SongDriver2: Real-time Emotion-based Music Arrangement with Soft Transition. (arXiv:2305.08029v1 [cs.SD])

    [http://arxiv.org/abs/2305.08029](http://arxiv.org/abs/2305.08029)

    SongDriver2实现了基于情绪的实时音乐编排，并提出了柔和过渡机制，使音乐具有高度真实性和平滑过渡。

    

    基于情绪的实时音乐编排旨在将给定的音乐转化为另一个能够实时引起用户特定情感共鸣的音乐，在音乐疗法、游戏配乐和电影配乐等各种场景中具有重要应用价值。然而，由于目标情感的细粒度和可变性，平衡情感实时匹配和柔和情感转换是一项挑战。现有的研究主要集中在实现情感实时匹配，而柔和过渡的问题仍未得到充分研究，影响了音乐的整体情感一致性。本文提出了SongDriver2来解决这个问题。具体地，我们首先识别最后一个时间步的音乐情绪，然后将其与当前时间步的目标输入情绪融合。融合的情感随后作为SongDriver2根据输入旋律数据生成即将到来的音乐的指导。为了调整音乐相似性和情感实时匹配，以实现两种不同情感之间的过渡，我们设计了一种软过渡机制，将插值和平滑滤波器相结合。我们证明，所提出的SongDriver2可以生成具有高度真实性和平滑过渡的情感音乐，这表明其在基于情绪的实时音乐编排应用中具有潜在价值。

    Real-time emotion-based music arrangement, which aims to transform a given music piece into another one that evokes specific emotional resonance with the user in real-time, holds significant application value in various scenarios, e.g., music therapy, video game soundtracks, and movie scores. However, balancing emotion real-time fit with soft emotion transition is a challenge due to the fine-grained and mutable nature of the target emotion. Existing studies mainly focus on achieving emotion real-time fit, while the issue of soft transition remains understudied, affecting the overall emotional coherence of the music. In this paper, we propose SongDriver2 to address this balance. Specifically, we first recognize the last timestep's music emotion and then fuse it with the current timestep's target input emotion. The fused emotion then serves as the guidance for SongDriver2 to generate the upcoming music based on the input melody data. To adjust music similarity and emotion real-time fit f
    
[^44]: 面向基于跨度的序列标注的成本效益众包：工人选择和数据增强

    Cost-efficient Crowdsourcing for Span-based Sequence Labeling: Worker Selection and Data Augmentation. (arXiv:2305.06683v1 [cs.CL])

    [http://arxiv.org/abs/2305.06683](http://arxiv.org/abs/2305.06683)

    本文介绍了面向基于跨度的序列标注任务的成本效益众包算法，使用了组合多臂老虎机方法进行工人选择，并用移位、扩展和收缩的数据增强方法进行测试，提高了注释质量和降低成本，F1得分相对于仅专家的基线提高了100.04％，成本节约高达65.97％。

    

    本文介绍了一种新的工人选择算法，提高了自然语言处理中具有挑战性的基于跨度的序列标注任务的注释质量并降低了成本。与以前针对简单任务的研究不同，本研究涉及序列标注任务中的标签相互依赖性复杂性。所提议的算法使用组合多臂老虎机（CMAB）方法进行工人选择。解决了处理不平衡和小规模数据集的挑战，该挑战阻碍了工人选择的离线模拟，使用一种称为移位、扩展和收缩（SES）的创新数据增强方法来解决。SES方法专门为序列标注任务设计。在CoNLL 2003 NER和中文OEI数据集上进行的严格测试展示了算法的效率，F1得分相对于仅专家的基线提高了100.04％，成本节约高达65.97％。本文还包括一个独立于数据集的测试。

    This paper introduces a novel worker selection algorithm, enhancing annotation quality and reducing costs in challenging span-based sequence labeling tasks in Natural Language Processing (NLP). Unlike previous studies targeting simpler tasks, this study contends with the complexities of label interdependencies in sequence labeling tasks. The proposed algorithm utilizes a Combinatorial Multi-Armed Bandit (CMAB) approach for worker selection. The challenge of dealing with imbalanced and small-scale datasets, which hinders offline simulation of worker selection, is tackled using an innovative data augmentation method termed shifting, expanding, and shrinking (SES). The SES method is designed specifically for sequence labeling tasks. Rigorous testing on CoNLL 2003 NER and Chinese OEI datasets showcased the algorithm's efficiency, with an increase in F1 score up to 100.04% of the expert-only baseline, alongside cost savings up to 65.97%. The paper also encompasses a dataset-independent test
    
[^45]: 利用预训练的AudioLDM进行文本到声音生成：基准研究

    Leveraging Pre-trained AudioLDM for Text to Sound Generation: A Benchmark Study. (arXiv:2303.03857v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2303.03857](http://arxiv.org/abs/2303.03857)

    本文研究了使用预训练的AudioLDM作为声音生成的骨干的优势，证明了在数据稀缺情况下使用预训练模型进行文本到声音生成的优势，并在几个常用数据集上使用相同的评估协议评估了各种文本到声音生成系统，为未来的研究提供了基础。

    This paper investigates the advantages of using pre-trained AudioLDM as the backbone for sound generation, demonstrates the benefits of using pre-trained models for text-to-sound generation in data-scarcity scenarios, and evaluates various text-to-sound generation systems on several frequently used datasets under the same evaluation protocols to provide a basis for future research.

    深度神经网络最近在文本提示下实现了声音生成的突破。尽管它们的表现很有前途，但当前的文本到声音生成模型在小规模数据集（例如过度拟合）上面临问题，从而显著限制了它们的性能。在本文中，我们研究了使用预训练的AudioLDM作为声音生成的骨干的优势。我们的研究证明了在数据稀缺情况下使用预训练模型进行文本到声音生成的优势。此外，实验表明，不同的训练策略（例如训练条件）可能会影响AudioLDM在不同规模的数据集上的性能。为了促进未来的研究，我们还在几个常用数据集上使用相同的评估协议评估了各种文本到声音生成系统，这些协议允许在共同基础上公平比较和基准测试这些方法。

    Deep neural networks have recently achieved breakthroughs in sound generation with text prompts. Despite their promising performance, current text-to-sound generation models face issues on small-scale datasets (e.g., overfitting), significantly limiting their performance. In this paper, we investigate the use of pre-trained AudioLDM, the state-of-the-art model for text-to-audio generation, as the backbone for sound generation. Our study demonstrates the advantages of using pre-trained models for text-to-sound generation, especially in data-scarcity scenarios. In addition, experiments show that different training strategies (e.g., training conditions) may affect the performance of AudioLDM on datasets of different scales. To facilitate future studies, we also evaluate various text-to-sound generation systems on several frequently used datasets under the same evaluation protocols, which allow fair comparisons and benchmarking of these methods on the common ground.
    
[^46]: 多人不完全信息博弈中的贝叶斯对手建模

    Bayesian Opponent Modeling in Multiplayer Imperfect-Information Games. (arXiv:2212.06027v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2212.06027](http://arxiv.org/abs/2212.06027)

    该论文提出了一种针对多人不完全信息博弈的贝叶斯对手建模方法，在三人 Kuhn poker 中应用这种方法可以明显超过所有的代理商，包括准确的纳什均衡策略。

    

    在许多现实世界的情境中，代理商与多个对立代理商进行战略互动，对手可能采用各种策略。对于这样的情境，设计代理的标准方法是计算或逼近相关的博弈理论解，如纳什均衡，然后遵循规定的策略。然而，这样的策略忽略了对手玩游戏的任何观察，这些观察可能表明可以利用的缺点。我们提出了一种多人不完全信息博弈中的对手建模方法，通过重复交互收集对手玩游戏的观察。我们对三人 Kuhn 扑克展开了对许多真实对手和准确的纳什均衡策略的实验，结果表明我们的算法明显优于所有的代理商，包括准确的纳什均衡策略。

    In many real-world settings agents engage in strategic interactions with multiple opposing agents who can employ a wide variety of strategies. The standard approach for designing agents for such settings is to compute or approximate a relevant game-theoretic solution concept such as Nash equilibrium and then follow the prescribed strategy. However, such a strategy ignores any observations of opponents' play, which may indicate shortcomings that can be exploited. We present an approach for opponent modeling in multiplayer imperfect-information games where we collect observations of opponents' play through repeated interactions. We run experiments against a wide variety of real opponents and exact Nash equilibrium strategies in three-player Kuhn poker and show that our algorithm significantly outperforms all of the agents, including the exact Nash equilibrium strategies.
    

