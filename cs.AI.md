# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Modality Translation for Object Detection Adaptation Without Forgetting Prior Knowledge](https://arxiv.org/abs/2404.01492) | 本文提出了一种ModTr方法，通过小型转换网络调整输入以最小化检测损失，实现了目标检测模型从一个或多个模态到另一个的有效适应，而无需微调参数。 |
| [^2] | [Advancing Multimodal Data Fusion in Pain Recognition: A Strategy Leveraging Statistical Correlation and Human-Centered Perspectives](https://arxiv.org/abs/2404.00320) | 通过结合统计相关性和以人为中心的方法，本研究提出了一种新的方法，改善了多模态数据融合在疼痛识别中的性能，突出了数据多样性和定制化的模态分割对疼痛行为分析的重要性。 |
| [^3] | [Navigating Fairness: Practitioners' Understanding, Challenges, and Strategies in AI/ML Development](https://arxiv.org/abs/2403.15481) | AI从业者对于公平AI/ML的理解、面临的挑战、不公平AI/ML的后果以及确保AI/ML公平性的策略。 |
| [^4] | [The opportunities and risks of large language models in mental health](https://arxiv.org/abs/2403.14814) | 大型语言模型在心理健康领域有望提供新颖的解决方案，但应注意其应用可能带来的风险，并积极采取策略减轻这些风险。 |
| [^5] | [A Hybrid Intelligence Method for Argument Mining](https://arxiv.org/abs/2403.09713) | 提出了一种混合(人类+AI)方法HyEnA，用于从意见文本中提取论点，结合了自动化处理速度和人类理解推理能力，在公民反馈语料库上取得了更高的覆盖率和准确率。 |
| [^6] | [Multitask Multilingual Model Adaptation with Featurized Low-Rank Mixtures](https://arxiv.org/abs/2402.17934) | 提出了一种名为FLix的新型参数高效微调方法，适用于多任务多语言调整，通过关联每个独特数据集特征与其低秩权重更新参数，实现了更好的泛化能力和性能表现。 |
| [^7] | [Demonstrating and Reducing Shortcuts in Vision-Language Representation Learning](https://arxiv.org/abs/2402.17510) | 在视觉-语言表示学习中，论文提出了一种训练和评估框架，引入了合成捷径来探究对比训练是否足以学习到包含所有信息的任务最优表示。 |
| [^8] | [Reputational Algorithm Aversion](https://arxiv.org/abs/2402.15418) | 选择跟随算法是否传达有关人类能力的信息是导致算法厌恶现象的关键因素，这种现象被称为“算法厌恶”。 |
| [^9] | [DefInt: A Default-interventionist Framework for Efficient Reasoning with Hybrid Large Language Models](https://arxiv.org/abs/2402.02563) | DefInt提出了一种默认干预框架，通过默认使用较小规模的语言模型生成推理思路，然后通过反思推理干预解决复杂推理问题，从而提高混合大型语言模型的效率和准确性。 |
| [^10] | [A Study of Fairness Concerns in AI-based Mobile App Reviews](https://arxiv.org/abs/2401.08097) | 本文研究了AI基于移动应用评价中的公平关注，并通过构建数据集和开发分类器的方法，成功检测出公平性评论，并识别出约92000条公平性评论。 |
| [^11] | [KGLens: A Parameterized Knowledge Graph Solution to Assess What an LLM Does and Doesn't Know](https://arxiv.org/abs/2312.11539) | KGLens 是一个旨在衡量知识图与大型语言模型（LLMs）之间对齐程度的框架，帮助找出LLMs相对于知识图的知识不足之处。 |
| [^12] | [Lite-Mind: Towards Efficient and Robust Brain Representation Network](https://arxiv.org/abs/2312.03781) | Lite-Mind旨在解决fMRI解码中的挑战，通过提出一种高效稳健的脑表示网络，避免了在实践设备上为每个受试者部署特定模型的问题。 |
| [^13] | [Tiered Reward Functions: Specifying and Fast Learning of Desired Behavior](https://arxiv.org/abs/2212.03733) | 层级奖励函数提出了一种解决奖励设计问题的方法，能够保证诱导出根据偏好关系是帕累托最优的策略，并在多个环境中展示其快速学习的能力。 |
| [^14] | [Improving Small Language Models' Mathematical Reasoning via Equation-of-Thought Distillation.](http://arxiv.org/abs/2401.11864) | 本研究提出了思维方程蒸馏（EoTD）技术和集合思维蒸馏（ETD）框架，通过构建基于方程的表示和使用多个思维过程的推理数据集来改进小型语言模型（SLMs）的数学推理能力，实验结果表明，EoTD和ETD显著提升了SLMs的推理能力。 |
| [^15] | [Learning Backdoors for Mixed Integer Programs with Contrastive Learning.](http://arxiv.org/abs/2401.10467) | 本论文提出了使用对比学习方法来学习混合整数规划的后门，通过收集用于训练的后门并训练图注意力网络模型来预测后门，取得了比Gurobi和先前模型更好的性能改进。 |
| [^16] | [Adaptive Self-training Framework for Fine-grained Scene Graph Generation.](http://arxiv.org/abs/2401.09786) | 本论文提出了一种自适应自训练框架用于细粒度场景图生成，通过利用未标注的三元组缓解了场景图生成中的长尾问题。同时，引入了一种新颖的伪标签技术CATM和图结构学习器GSL来提高模型性能。 |
| [^17] | [Blackout Mitigation via Physics-guided RL.](http://arxiv.org/abs/2401.09640) | 本文设计了一种物理引导的强化学习框架，利用传输网络的潮流灵敏度因子来指导强化学习训练，实现了通过实时补救前瞻决策来减轻黑暗模式的目标。 |
| [^18] | [Modeling Latent Selection with Structural Causal Models.](http://arxiv.org/abs/2401.06925) | 本文介绍了一种在结构因果模型中对潜在选择进行建模的方法，并展示了它如何帮助进行因果推理任务，包括处理选择偏差。 |
| [^19] | [The DSA Transparency Database: Auditing Self-reported Moderation Actions by Social Media.](http://arxiv.org/abs/2312.10269) | DSA透明数据库对欧盟八大社交媒体平台在前100天提交的审核行动数据进行了全面分析，揭示了这些平台在审核行动方面的部分遵循程度。 |
| [^20] | [SC-MIL: Sparsely Coded Multiple Instance Learning for Whole Slide Image Classification.](http://arxiv.org/abs/2311.00048) | 本文提出了SC-MIL模型，通过利用稀疏字典学习来同时改进特征嵌入和实例相关性建模，从而提高全切片图像分类的性能。 |
| [^21] | [Towards Enhanced Local Explainability of Random Forests: a Proximity-Based Approach.](http://arxiv.org/abs/2310.12428) | 这项研究提出了一种利用随机森林模型的特征空间中的邻近性来解释模型预测的方法，为模型预测提供了局部的解释性，与现有方法相辅相成。通过实验证明了这种方法在债券定价模型中的有效性。 |
| [^22] | [Evaluating LLMs for Privilege-Escalation Scenarios.](http://arxiv.org/abs/2310.11409) | 本研究评估了在特权升级场景中利用语言模型（LLMs）进行渗透测试的应用。通过创建一个自动化的Linux特权升级基准和一个LLM-guided特权升级工具，我们分析了LLMs的不同提示设计、上下文学习和高级指导对测试的影响，并讨论了LLMs面临的挑战。 |
| [^23] | [Multi-Source Domain Adaptation for Object Detection with Prototype-based Mean-teacher.](http://arxiv.org/abs/2309.14950) | 该论文提出了一种名为Prototype-based Mean-Teacher (PMT)的新型多源域自适应目标检测方法，通过使用类原型而不是域特定子网络来保留域特定信息，提高了准确性和鲁棒性。 |
| [^24] | [Seed Kernel Counting using Domain Randomization and Object Tracking Neural Networks.](http://arxiv.org/abs/2308.05846) | 本研究提出了一种使用领域随机化和目标跟踪神经网络的方法来进行种子核计数。该方法通过使用合成图像作为训练数据的替代品，解决了神经网络模型需要大量有标签训练数据的问题，可以低成本估计谷物产量。 |
| [^25] | [Technical Note: Defining and Quantifying AND-OR Interactions for Faithful and Concise Explanation of DNNs.](http://arxiv.org/abs/2304.13312) | 本文提出了一种通过量化输入变量之间的编码交互来准确且简明地解释深度神经网络(DNN)的推理逻辑的方法。针对此目的，作者提出了两种交互方式，即AND交互和OR交互，并利用它们设计出一系列技术来提高解释的简洁性，同时不会损害准确性。 |
| [^26] | [Ultra-High-Resolution Detector Simulation with Intra-Event Aware GAN and Self-Supervised Relational Reasoning.](http://arxiv.org/abs/2303.08046) | 本文提出了一种新颖的探测器模拟方法IEA-GAN，通过产生与图层相关的上下文化的图像，提高了超高分辨率探测器响应的相关性和多样性。同时，引入新的事件感知损失和统一性损失，显著提高了图像的保真度和多样性。 |
| [^27] | [Geolocation Predicting of Tweets Using BERT-Based Models.](http://arxiv.org/abs/2303.07865) | 该论文提出基于BERT模型的推文地理位置预测方法，可以实现全球和美国上的中位误差分别小于30公里和15公里的定位精度。 |
| [^28] | [Generalization in Neural Networks: A Broad Survey.](http://arxiv.org/abs/2209.01610) | 这篇论文总结了神经网络模型中不同抽象层次上的泛化问题及其方法，其中样本泛化已经取得了进展，但未来需要重点关注减少过拟合；分布泛化与领域泛化有相似之处，领域泛化方法可以应用于困难的样本或分布泛化。 |

# 详细

[^1]: 不遗忘先验知识的目标检测适应模态转换

    Modality Translation for Object Detection Adaptation Without Forgetting Prior Knowledge

    [https://arxiv.org/abs/2404.01492](https://arxiv.org/abs/2404.01492)

    本文提出了一种ModTr方法，通过小型转换网络调整输入以最小化检测损失，实现了目标检测模型从一个或多个模态到另一个的有效适应，而无需微调参数。

    

    深度学习中常见的做法是在大规模数据集上训练大型神经网络，以在不同领域和任务中准确执行。然而，这种方法在许多应用领域只适用于跨模态，因为使用不同传感器捕获的数据存在更大的分布偏移。本文专注于将大型目标检测模型调整到一个或多个模态的问题，同时保持高效。为此，我们提出了ModTr作为普遍做法微调大型模型的替代方案。ModTr包括使用一个小型转换网络调整输入，该网络经过训练，直接使检测损失最小化。因此，原始模型可以在转换后的输入上工作，无需进行任何进一步的更改或参数微调。对两个知名数据集上从红外到RGB图像的转换的实验结果表明，这种简单的ModTr方法提供了检测器。

    arXiv:2404.01492v1 Announce Type: cross  Abstract: A common practice in deep learning consists of training large neural networks on massive datasets to perform accurately for different domains and tasks. While this methodology may work well in numerous application areas, it only applies across modalities due to a larger distribution shift in data captured using different sensors. This paper focuses on the problem of adapting a large object detection model to one or multiple modalities while being efficient. To do so, we propose ModTr as an alternative to the common approach of fine-tuning large models. ModTr consists of adapting the input with a small transformation network trained to minimize the detection loss directly. The original model can therefore work on the translated inputs without any further change or fine-tuning to its parameters. Experimental results on translating from IR to RGB images on two well-known datasets show that this simple ModTr approach provides detectors tha
    
[^2]: 在疼痛识别中推进多模态数据融合：利用统计相关性和以人为中心的策略

    Advancing Multimodal Data Fusion in Pain Recognition: A Strategy Leveraging Statistical Correlation and Human-Centered Perspectives

    [https://arxiv.org/abs/2404.00320](https://arxiv.org/abs/2404.00320)

    通过结合统计相关性和以人为中心的方法，本研究提出了一种新的方法，改善了多模态数据融合在疼痛识别中的性能，突出了数据多样性和定制化的模态分割对疼痛行为分析的重要性。

    

    这项研究致力于解决在疼痛识别领域内整合异构数据以进行特定行为识别的挑战，提出了一种将统计相关性与以人为中心方法相结合的新方法。通过利用各种各样的深度学习架构，我们突出了我们的方法在提升模型在各种复杂场景中表现的适应性和有效性。我们方法的新颖之处在于战略性地结合了统计相关性权重，并从以人为中心的角度对模态进行了分割，提高了模型的精度，并提供了对多模态数据可解释性的分析。该研究通过强调数据多样性和定制化的模态分割在增强疼痛行为分析中的作用，超越了传统的模态融合技术。引入了一个框架，将每种模态与适合的分类器进行匹配，基于统计相关性，增加了对多模态数据的解释性分析。

    arXiv:2404.00320v1 Announce Type: new  Abstract: This research tackles the challenge of integrating heterogeneous data for specific behavior recognition within the domain of Pain Recognition, presenting a novel methodology that harmonizes statistical correlations with a human-centered approach. By leveraging a diverse range of deep learning architectures, we highlight the adaptability and efficacy of our approach in improving model performance across various complex scenarios. The novelty of our methodology is the strategic incorporation of statistical relevance weights and the segmentation of modalities from a human-centric perspective, enhancing model precision and providing a explainable analysis of multimodal data. This study surpasses traditional modality fusion techniques by underscoring the role of data diversity and customized modality segmentation in enhancing pain behavior analysis. Introducing a framework that matches each modality with an suited classifier, based on the sta
    
[^3]: AI/ML 发展中的公平导航: 从业者对AI/ML开发中的理解、挑战和策略

    Navigating Fairness: Practitioners' Understanding, Challenges, and Strategies in AI/ML Development

    [https://arxiv.org/abs/2403.15481](https://arxiv.org/abs/2403.15481)

    AI从业者对于公平AI/ML的理解、面临的挑战、不公平AI/ML的后果以及确保AI/ML公平性的策略。

    

    近年来，各行业对AI/ML应用的增加引发了对AI/ML公平性的更多讨论。虽然已有关于AI/ML公平性的先前研究，但缺乏针对了解AI从业者在开发公平AI/ML过程中的观点和经验的实证研究。了解AI从业者对AI/ML公平性的看法和经验很重要，因为他们直接参与其中的开发和部署，他们的见解可以提供有价值的现实世界视角，帮助理解确保AI/ML公平性所涉及挑战的重要性。我们进行了22位AI从业者的半结构化访谈，以调查他们对“公平AI/ML”是什么的理解，他们在开发公平AI/ML中面临的挑战，开发不公平AI/ML的后果，以及他们采取的策略来确保AI/ML的公平性。我们制定了一个框架展示了

    arXiv:2403.15481v1 Announce Type: cross  Abstract: The rise in the use of AI/ML applications across industries has sparked more discussions about the fairness of AI/ML in recent times. While prior research on the fairness of AI/ML exists, there is a lack of empirical studies focused on understanding the views and experiences of AI practitioners in developing a fair AI/ML. Understanding AI practitioners' views and experiences on the fairness of AI/ML is important because they are directly involved in its development and deployment and their insights can offer valuable real-world perspectives on the challenges associated with ensuring fairness in AI/ML. We conducted semi-structured interviews with 22 AI practitioners to investigate their understanding of what a 'fair AI/ML' is, the challenges they face in developing a fair AI/ML, the consequences of developing an unfair AI/ML, and the strategies they employ to ensure AI/ML fairness. We developed a framework showcasing the relationship be
    
[^4]: 大型语言模型在心理健康领域的机会和风险

    The opportunities and risks of large language models in mental health

    [https://arxiv.org/abs/2403.14814](https://arxiv.org/abs/2403.14814)

    大型语言模型在心理健康领域有望提供新颖的解决方案，但应注意其应用可能带来的风险，并积极采取策略减轻这些风险。

    

    全球心理健康问题的发生率正在上升，人们越来越意识到现有的心理保健模式无法充分扩展以满足需求。随着大型语言模型（LLMs）的出现，人们对它们具有创造新颖、大规模解决方案以支持心理健康的承诺感到乐观。尽管它们还处于初期阶段，LLMs已被应用于与心理健康相关的任务。本综述总结了已有文献中关于利用LLMs提供心理健康教育、评估和干预的努力，并突出了每个领域中产生积极影响的关键机会。然后，我们强调了将LLMs应用于心理健康领域所伴随的风险，并鼓励采用策略来减轻这些风险。对于心理健康支持的迫切需求必须与负责任的心理健康LLMs的开发、测试和部署相平衡。特别关键的是确保心理健康...

    arXiv:2403.14814v1 Announce Type: cross  Abstract: Global rates of mental health concerns are rising and there is increasing realization that existing models of mental healthcare will not adequately expand to meet the demand. With the emergence of large language models (LLMs) has come great optimism regarding their promise to create novel, large-scale solutions to support mental health. Despite their nascence, LLMs have already been applied to mental health-related tasks. In this review, we summarize the extant literature on efforts to use LLMs to provide mental health education, assessment, and intervention and highlight key opportunities for positive impact in each area. We then highlight risks associated with LLMs application to mental health and encourage adoption of strategies to mitigate these risks. The urgent need for mental health support must be balanced with responsible development, testing, and deployment of mental health LLMs. Especially critical is ensuring that mental he
    
[^5]: 一种用于论证挖掘的混合智能方法

    A Hybrid Intelligence Method for Argument Mining

    [https://arxiv.org/abs/2403.09713](https://arxiv.org/abs/2403.09713)

    提出了一种混合(人类+AI)方法HyEnA，用于从意见文本中提取论点，结合了自动化处理速度和人类理解推理能力，在公民反馈语料库上取得了更高的覆盖率和准确率。

    

    大规模调查工具能够收集公民反馈意见语料库。从庞大且嘈杂的意见集中提取关键论点有助于快速准确地理解意见。完全自动化的方法可以提取论点，但(1)需要大规模标记数据集，导致较高的注释成本; (2)对已知观点效果良好，但对新颖观点效果欠佳。我们提出了HyEnA，一种混合(人类+AI)方法，用于从主观文本中提取论点，结合了自动化处理的速度和人类的理解和推理能力。我们在三个公民反馈语料库上评估了HyEnA。我们发现，一方面，与一组各种意见进行比较时，HyEnA在高覆盖率和准确率方面优于最先进的自动化方法，证实了人类洞察的必要性。另一方面，HyEnA需要较少的人力工作量，且不会牺牲质量。

    arXiv:2403.09713v1 Announce Type: new  Abstract: Large-scale survey tools enable the collection of citizen feedback in opinion corpora. Extracting the key arguments from a large and noisy set of opinions helps in understanding the opinions quickly and accurately. Fully automated methods can extract arguments but (1) require large labeled datasets that induce large annotation costs and (2) work well for known viewpoints, but not for novel points of view. We propose HyEnA, a hybrid (human + AI) method for extracting arguments from opinionated texts, combining the speed of automated processing with the understanding and reasoning capabilities of humans. We evaluate HyEnA on three citizen feedback corpora. We find that, on the one hand, HyEnA achieves higher coverage and precision than a state-of-the-art automated method when compared to a common set of diverse opinions, justifying the need for human insight. On the other hand, HyEnA requires less human effort and does not compromise quali
    
[^6]: 用特征化低秩混合进行多任务多语言模型适应

    Multitask Multilingual Model Adaptation with Featurized Low-Rank Mixtures

    [https://arxiv.org/abs/2402.17934](https://arxiv.org/abs/2402.17934)

    提出了一种名为FLix的新型参数高效微调方法，适用于多任务多语言调整，通过关联每个独特数据集特征与其低秩权重更新参数，实现了更好的泛化能力和性能表现。

    

    预训练大型语言模型（LLMs）适应数十甚至数百种人类语言的各种下游任务在计算上是昂贵的。参数高效微调（PEFT）通过只调整少量参数显著减少了适应成本。然而，直接将像 LoRA（Hu 等人，2022）这样的 PEFT 方法应用于不同数据集混合可能导致性能次优，原因在于有限的参数容量和不同数据集之间的负面互相影响。在这项工作中，我们提出了特征化低秩混合（FLix），这是一种针对有效的多任务多语言调整的新型 PEFT 方法。FLix将每个独特数据集特征（例如数据集的语言或任务）与其自己的低秩权重更新参数相关联。通过为每个数据集组合特定于特征的参数，FLix能够适应多种数据集混合，并更好地泛化到未见数据集。我们的实验表明，FLix 可以在提供更好性能的同时显著减少适应成本。

    arXiv:2402.17934v1 Announce Type: cross  Abstract: Adapting pretrained large language models (LLMs) to various downstream tasks in tens or hundreds of human languages is computationally expensive. Parameter-efficient fine-tuning (PEFT) significantly reduces the adaptation cost, by tuning only a small amount of parameters. However, directly applying PEFT methods such as LoRA (Hu et al., 2022) on diverse dataset mixtures could lead to suboptimal performance due to limited parameter capacity and negative interference among different datasets. In this work, we propose Featurized Low-rank Mixtures (FLix), a novel PEFT method designed for effective multitask multilingual tuning. FLix associates each unique dataset feature, such as the dataset's language or task, with its own low-rank weight update parameters. By composing feature-specific parameters for each dataset, FLix can accommodate diverse dataset mixtures and generalize better to unseen datasets. Our experiments show that FLix leads t
    
[^7]: 示范和减少视觉语言表示学习中的捷径

    Demonstrating and Reducing Shortcuts in Vision-Language Representation Learning

    [https://arxiv.org/abs/2402.17510](https://arxiv.org/abs/2402.17510)

    在视觉-语言表示学习中，论文提出了一种训练和评估框架，引入了合成捷径来探究对比训练是否足以学习到包含所有信息的任务最优表示。

    

    arXiv:2402.17510v1 公告类型: 跨领域 摘要: 视觉-语言模型(VLMs)主要依赖对比训练来学习图像和标题的通用表示。我们关注的情况是当一个图像与多个标题相关联时，每个标题既包含所有标题共享的信息，又包含关于图像场景的每个标题独特的信息。在这种情况下，尚不清楚对比损失是否足以学习包含标题提供的所有信息的任务最优表示，还是对比学习设置是否鼓励学习最小化对比损失的简单捷径。我们引入了视觉-语言的合成捷径：一种训练和评估框架，在其中我们向图像-文本数据注入合成捷径。我们展示了，从头开始训练或用包含这些合成捷径的数据微调的对比VLMs主要学习代表捷径的特征。

    arXiv:2402.17510v1 Announce Type: cross  Abstract: Vision-language models (VLMs) mainly rely on contrastive training to learn general-purpose representations of images and captions. We focus on the situation when one image is associated with several captions, each caption containing both information shared among all captions and unique information per caption about the scene depicted in the image. In such cases, it is unclear whether contrastive losses are sufficient for learning task-optimal representations that contain all the information provided by the captions or whether the contrastive learning setup encourages the learning of a simple shortcut that minimizes contrastive loss. We introduce synthetic shortcuts for vision-language: a training and evaluation framework where we inject synthetic shortcuts into image-text data. We show that contrastive VLMs trained from scratch or fine-tuned with data containing these synthetic shortcuts mainly learn features that represent the shortcu
    
[^8]: 声誉算法厌恶

    Reputational Algorithm Aversion

    [https://arxiv.org/abs/2402.15418](https://arxiv.org/abs/2402.15418)

    选择跟随算法是否传达有关人类能力的信息是导致算法厌恶现象的关键因素，这种现象被称为“算法厌恶”。

    

    人们常常不愿将算法产生的信息纳入自己的决策中，这种现象被称为“算法厌恶”。本文展示了算法厌恶是如何产生的，当选择跟随算法传达有关人类能力的信息时。我建立了一个模型，其中工作者根据自己的私人信息和算法的信号对随机结果进行预测。低技能工作者接收到比算法更差的信息，因此应始终遵循算法的信号，而高技能工作者接收到比算法更好的信息，因此有时应该覆盖算法的决策。然而，由于声誉上的考虑，低技能工作者会不合理地覆盖算法，以增加被视为高技能的可能性。该模型为与AI系统可能会取代许多类型的工作者的广泛关注提供了完全理性的微观基础。

    arXiv:2402.15418v1 Announce Type: cross  Abstract: People are often reluctant to incorporate information produced by algorithms into their decisions, a phenomenon called "algorithm aversion". This paper shows how algorithm aversion arises when the choice to follow an algorithm conveys information about a human's ability. I develop a model in which workers make forecasts of a random outcome based on their own private information and an algorithm's signal. Low-skill workers receive worse information than the algorithm and hence should always follow the algorithm's signal, while high-skill workers receive better information than the algorithm and should sometimes override it. However, due to reputational concerns, low-skill workers inefficiently override the algorithm to increase the likelihood they are perceived as high-skill. The model provides a fully rational microfoundation for algorithm aversion that aligns with the broad concern that AI systems will displace many types of workers.
    
[^9]: DefInt：一种用于高效处理混合大型语言模型推理的默认干预框架

    DefInt: A Default-interventionist Framework for Efficient Reasoning with Hybrid Large Language Models

    [https://arxiv.org/abs/2402.02563](https://arxiv.org/abs/2402.02563)

    DefInt提出了一种默认干预框架，通过默认使用较小规模的语言模型生成推理思路，然后通过反思推理干预解决复杂推理问题，从而提高混合大型语言模型的效率和准确性。

    

    大型语言模型（LLMs）在各种任务中展示出令人印象深刻的新能力，但在处理复杂推理问题方面仍面临挑战。以往的研究如连锁推理（CoT）和思维树（ToT）主要关注提高准确性，但忽视了不断增加的标记成本，这对于具有巨大解空间的开放性实际任务来说可能特别问题。受人类认知的双过程理论的启发，我们提出了一种默认干预框架（DefInt），以释放混合LLMs的协同潜力。默认情况下，DefInt使用较小规模的语言模型生成低成本的推理思路，类似于“系统1”产生的快速直觉。如果这些直觉被认为低置信度，则DefInt将调用放大的语言模型的反思推理作为“系统2”的干预，可以覆盖默认思考并纠正推理过程。实验在五个实际数据集上展示了DefInt论文中的有效性。

    Large language models (LLMs) have shown impressive emergent abilities in a wide range of tasks, but still face challenges in handling complex reasoning problems. Previous works like chain-of-thought (CoT) and tree-of-thoughts(ToT) have predominately focused on enhancing accuracy, but overlook the rapidly increasing token cost, which could be particularly problematic for open-ended real-world tasks with huge solution spaces. Motivated by the dual process theory of human cognition, we propose a Default-Interventionist framework (DefInt) to unleash the synergistic potential of hybrid LLMs. By default, DefInt uses smaller-scale language models to generate low-cost reasoning thoughts, which resembles the fast intuitions produced by System 1. If the intuitions are considered with low confidence, DefInt will invoke the reflective reasoning of scaled-up language models as the intervention of System 2, which can override the default thoughts and rectify the reasoning process. Experiments on fiv
    
[^10]: AI基于移动应用评价的公平关注研究

    A Study of Fairness Concerns in AI-based Mobile App Reviews

    [https://arxiv.org/abs/2401.08097](https://arxiv.org/abs/2401.08097)

    本文研究了AI基于移动应用评价中的公平关注，并通过构建数据集和开发分类器的方法，成功检测出公平性评论，并识别出约92000条公平性评论。

    

    公平是AI系统中必须解决的社会技术问题之一。不公平的AI系统，特别是不公平的AI基于移动应用，可能给全球很大一部分人口带来困难。本文旨在分析AI基于应用评价中的公平问题。我们首先手动构建了一个基准数据集，包括公平性和非公平性评论的统计样本。利用这个基准数据集，我们开发和评估了一组机器学习和深度学习分类器，用于区分公平性评论和非公平性评论。我们的实验结果显示，我们最佳的分类器可以以94%的精确度检测到公平性评论。然后，我们将最佳分类器应用于从108个AI基于应用收集的约950万条评论，识别出约92000条公平性评论。接下来，我们将K-means聚类技术应用于这92000条公平性评论。

    arXiv:2401.08097v2 Announce Type: replace-cross Abstract: Fairness is one of the socio-technical concerns that must be addressed in AI-based systems. Unfair AI-based systems, particularly unfair AI-based mobile apps, can pose difficulties for a significant proportion of the global population. This paper aims to analyze fairness concerns in AI-based app reviews.We first manually constructed a ground-truth dataset, including a statistical sample of fairness and non-fairness reviews. Leveraging the ground-truth dataset, we developed and evaluated a set of machine learning and deep learning classifiers that distinguish fairness reviews from non-fairness reviews. Our experiments show that our best-performing classifier can detect fairness reviews with a precision of 94%. We then applied the best-performing classifier on approximately 9.5M reviews collected from 108 AI-based apps and identified around 92K fairness reviews. Next, applying the K-means clustering technique to the 92K fairness r
    
[^11]: KGLens：一个参数化知识图解决方案，用于评估LLM知道和不知道的内容

    KGLens: A Parameterized Knowledge Graph Solution to Assess What an LLM Does and Doesn't Know

    [https://arxiv.org/abs/2312.11539](https://arxiv.org/abs/2312.11539)

    KGLens 是一个旨在衡量知识图与大型语言模型（LLMs）之间对齐程度的框架，帮助找出LLMs相对于知识图的知识不足之处。

    

    衡量知识图（KG）与大型语言模型（LLMs）之间的对齐程度是评估事实性并识别LLMs的知识盲点的有效方法。然而，这种方法面临两个主要挑战，包括将KGs转化为自然语言和高效评估这些广泛且复杂的结构。在本文中，我们提出了KGLens--一个旨在衡量KGs和LLMs之间对齐程度，并找出LLMs相对于KGs的知识缺陷的新颖框架。KGLens具有一个图引导的问题生成器，用于将KGs转化为自然语言，以及一个基于参数化KG结构的精心设计的采样策略，以加快KG的遍历。我们使用来自Wikidata的三个领域特定KG进行实验，这些KG包括超过19,000条边，700个关系和21,000个实体。我们跨越8个LLMs的分析表明，KGLens不仅

    arXiv:2312.11539v2 Announce Type: replace  Abstract: Measuring the alignment between a Knowledge Graph (KG) and Large Language Models (LLMs) is an effective method to assess the factualness and identify the knowledge blind spots of LLMs. However, this approach encounters two primary challenges including the translation of KGs into natural language and the efficient evaluation of these extensive and complex structures. In this paper, we present KGLens--a novel framework aimed at measuring the alignment between KGs and LLMs, and pinpointing the LLMs' knowledge deficiencies relative to KGs. KGLens features a graph-guided question generator for converting KGs into natural language, along with a carefully designed sampling strategy based on parameterized KG structure to expedite KG traversal. We conducted experiments using three domain-specific KGs from Wikidata, which comprise over 19,000 edges, 700 relations, and 21,000 entities. Our analysis across eight LLMs reveals that KGLens not only
    
[^12]: Lite-Mind: 高效稳健的脑表示网络

    Lite-Mind: Towards Efficient and Robust Brain Representation Network

    [https://arxiv.org/abs/2312.03781](https://arxiv.org/abs/2312.03781)

    Lite-Mind旨在解决fMRI解码中的挑战，通过提出一种高效稳健的脑表示网络，避免了在实践设备上为每个受试者部署特定模型的问题。

    

    通过非侵入性的fMRI方法解码大脑中的视觉信息的研究正在迅速发展。挑战在于有限的数据可用性和fMRI信号的低信噪比，导致fMRI到图像检索任务的低精度。MindEye技术通过利用高参数计数的深度MLP（每个受试者的996M MLP主干）将fMRI嵌入对齐到CLIP的视觉变换器的最终隐藏层，显着提高了fMRI到图像检索的性能。然而，即使在相同的实验设置内，受试者之间存在显着的个体差异，需要训练特定于受试者的模型。这些大量的参数在实际设备上部署fMRI解码时带来了重大挑战，特别是需要为每个受试者提供特定模型。

    arXiv:2312.03781v2 Announce Type: replace-cross  Abstract: Research in decoding visual information from the brain, particularly through the non-invasive fMRI method, is rapidly progressing. The challenge arises from the limited data availability and the low signal-to-noise ratio of fMRI signals, leading to a low-precision task of fMRI-to-image retrieval. State-of-the-art MindEye remarkably improves fMRI-to-image retrieval performance by leveraging a deep MLP with a high parameter count orders of magnitude, i.e., a 996M MLP Backbone per subject, to align fMRI embeddings to the final hidden layer of CLIP's vision transformer. However, significant individual variations exist among subjects, even within identical experimental setups, mandating the training of subject-specific models. The substantial parameters pose significant challenges in deploying fMRI decoding on practical devices, especially with the necessitating of specific models for each subject. To this end, we propose Lite-Mind,
    
[^13]: 层级奖励函数：规定和快速学习所需行为

    Tiered Reward Functions: Specifying and Fast Learning of Desired Behavior

    [https://arxiv.org/abs/2212.03733](https://arxiv.org/abs/2212.03733)

    层级奖励函数提出了一种解决奖励设计问题的方法，能够保证诱导出根据偏好关系是帕累托最优的策略，并在多个环境中展示其快速学习的能力。

    

    强化学习代理通过与环境的交互来最大化奖励信号。在学习过程中，我们作为人类的任务是设计奖励函数，以表达所期望的行为，并使代理能够迅速学习这种行为。在这项工作中，我们考虑了任务中达到良好状态和避免不良状态的奖励设计问题。首先，我们提出了一种严格的策略空间的部分排序，以解决行为偏好中的权衡。我们更倾向于能更快速地到达良好状态并以更高的概率到达，同时能更长时间地避免不良状态的策略。接下来，我们介绍了层级奖励，一类与环境无关的奖励函数，并表明它保证诱导出根据我们的偏好关系是帕累托最优的策略。最后，我们证明了层级奖励可以通过在多个环境上使用多个表格进行评估，从而实现快速学习。

    arXiv:2212.03733v2 Announce Type: replace-cross  Abstract: Reinforcement-learning agents seek to maximize a reward signal through environmental interactions. As humans, our job in the learning process is to design reward functions to express desired behavior and enable the agent to learn such behavior swiftly. In this work, we consider the reward-design problem in tasks formulated as reaching desirable states and avoiding undesirable states. To start, we propose a strict partial ordering of the policy space to resolve trade-offs in behavior preference. We prefer policies that reach the good states faster and with higher probability while avoiding the bad states longer. Next, we introduce Tiered Reward, a class of environment-independent reward functions and show it is guaranteed to induce policies that are Pareto-optimal according to our preference relation. Finally, we demonstrate that Tiered Reward can lead to fast learning by evaluating on several environments using multiple tabular
    
[^14]: 通过思维方程蒸馏改进小型语言模型的数学推理能力

    Improving Small Language Models' Mathematical Reasoning via Equation-of-Thought Distillation. (arXiv:2401.11864v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2401.11864](http://arxiv.org/abs/2401.11864)

    本研究提出了思维方程蒸馏（EoTD）技术和集合思维蒸馏（ETD）框架，通过构建基于方程的表示和使用多个思维过程的推理数据集来改进小型语言模型（SLMs）的数学推理能力，实验结果表明，EoTD和ETD显著提升了SLMs的推理能力。

    

    本研究解决了将先进的大型语言模型（LLMs）的数学推理能力压缩到具有小于十亿参数的小型语言模型（SLMs）中的挑战，同时不损害性能。我们引入了一种新颖的思维方程蒸馏（EoTD）技术，将推理过程封装为基于方程的表示，构建了一个EoTD数据集来对SLMs进行微调。此外，我们提出了集合思维蒸馏（ETD）框架，以提升SLMs的推理性能。这包括创建一个包含多个思维过程（包括思维链、思维程序和思维方程）的推理数据集，并将其用于微调。我们的实验证明，EoTD显著提升了SLMs的推理能力，而ETD使这些模型实现了最先进的推理性能。

    This work addresses the challenge of democratizing advanced Large Language Models (LLMs) by compressing their mathematical reasoning capabilities into sub-billion parameter Small Language Models (SLMs) without compromising performance. We introduce Equation-of-Thought Distillation (EoTD), a novel technique that encapsulates the reasoning process into equation-based representations to construct an EoTD dataset for fine-tuning SLMs. Additionally, we propose the Ensemble Thoughts Distillation (ETD) framework to enhance the reasoning performance of SLMs. This involves creating a reasoning dataset with multiple thought processes, including Chain-of-Thought (CoT), Program-of-Thought (PoT), and Equation-of-Thought (EoT), and using it for fine-tuning. Our experimental findings demonstrate that EoTD significantly boosts the reasoning abilities of SLMs, while ETD enables these models to achieve state-of-the-art reasoning performance.
    
[^15]: 使用对比学习学习混合整数规划的后门

    Learning Backdoors for Mixed Integer Programs with Contrastive Learning. (arXiv:2401.10467v1 [cs.AI])

    [http://arxiv.org/abs/2401.10467](http://arxiv.org/abs/2401.10467)

    本论文提出了使用对比学习方法来学习混合整数规划的后门，通过收集用于训练的后门并训练图注意力网络模型来预测后门，取得了比Gurobi和先前模型更好的性能改进。

    

    许多现实世界中的问题可以有效地建模为混合整数规划（MIP）并使用分支定界方法进行求解。先前的研究表明存在MIP后门，即一小组变量，如果优先在可能的情况下在它们上进行分支，则可以加快运行时间。然而，寻找能提高运行时间的高质量后门仍是一个未解决的问题。先前的工作通过排名学习估计随机采样的后门相对求解器速度，然后决定是否使用。本文中，我们利用蒙特卡洛树搜索方法收集用于训练的后门，而不是依赖随机采样，并且采用对比学习框架训练图注意力网络模型来预测后门。我们的方法在四个常见的MIP问题领域上进行评估，表现出对比Gurobi和先前模型的性能改进。

    Many real-world problems can be efficiently modeled as Mixed Integer Programs (MIPs) and solved with the Branch-and-Bound method. Prior work has shown the existence of MIP backdoors, small sets of variables such that prioritizing branching on them when possible leads to faster running times. However, finding high-quality backdoors that improve running times remains an open question. Previous work learns to estimate the relative solver speed of randomly sampled backdoors through ranking and then decide whether to use it. In this paper, we utilize the Monte-Carlo tree search method to collect backdoors for training, rather than relying on random sampling, and adapt a contrastive learning framework to train a Graph Attention Network model to predict backdoors. Our method, evaluated on four common MIP problem domains, demonstrates performance improvements over both Gurobi and previous models.
    
[^16]: 自适应自训练框架用于细粒度场景图生成

    Adaptive Self-training Framework for Fine-grained Scene Graph Generation. (arXiv:2401.09786v1 [cs.CV])

    [http://arxiv.org/abs/2401.09786](http://arxiv.org/abs/2401.09786)

    本论文提出了一种自适应自训练框架用于细粒度场景图生成，通过利用未标注的三元组缓解了场景图生成中的长尾问题。同时，引入了一种新颖的伪标签技术CATM和图结构学习器GSL来提高模型性能。

    

    场景图生成（SGG）模型在基准数据集中存在长尾谓词分布和缺失注释问题。本研究旨在通过利用未标注的三元组缓解SGG的长尾问题。为此，我们引入了一种称为自训练SGG（ST-SGG）的框架，该框架基于未标注的三元组为其分配伪标签以训练SGG模型。虽然在图像识别方面的自训练取得了显著进展，但设计适用于SGG任务的自训练框架更具挑战，因为其固有特性，如语义歧义和长尾分布的谓词类别。因此，我们提出了一种新颖的SGG伪标签技术，称为具有动量的类别自适应阈值化（CATM），它是一种独立于模型的框架，可应用于任何已有的SGG模型。此外，我们设计了一个图结构学习器（GSL），从中获益。

    Scene graph generation (SGG) models have suffered from inherent problems regarding the benchmark datasets such as the long-tailed predicate distribution and missing annotation problems. In this work, we aim to alleviate the long-tailed problem of SGG by utilizing unannotated triplets. To this end, we introduce a Self-Training framework for SGG (ST-SGG) that assigns pseudo-labels for unannotated triplets based on which the SGG models are trained. While there has been significant progress in self-training for image recognition, designing a self-training framework for the SGG task is more challenging due to its inherent nature such as the semantic ambiguity and the long-tailed distribution of predicate classes. Hence, we propose a novel pseudo-labeling technique for SGG, called Class-specific Adaptive Thresholding with Momentum (CATM), which is a model-agnostic framework that can be applied to any existing SGG models. Furthermore, we devise a graph structure learner (GSL) that is benefici
    
[^17]: 通过物理引导的强化学习进行停电减轻

    Blackout Mitigation via Physics-guided RL. (arXiv:2401.09640v1 [eess.SY])

    [http://arxiv.org/abs/2401.09640](http://arxiv.org/abs/2401.09640)

    本文设计了一种物理引导的强化学习框架，利用传输网络的潮流灵敏度因子来指导强化学习训练，实现了通过实时补救前瞻决策来减轻黑暗模式的目标。

    

    本文考虑了为了防止黑暗模式而在系统异常时进行序列设计的补救控制行动。设计了一种物理引导的强化学习框架，用于识别在考虑系统稳定性长期影响的情况下的实时补救前瞻决策序列。本文考虑了涉及离散值传输线开关决策（线路重新连接和移除）和连续值发电机调整的控制行动空间。为了确定有效的停电减轻策略，设计了一种物理引导方法，利用与电力传输网络相关的潮流灵敏度因子来引导强化学习训练期间的探索。使用开源Grid2Op平台进行了全面的实证评估，证明了将物理信号纳入强化学习决策的显著优势，证实了所提出的物理引导方法的收益。

    This paper considers the sequential design of remedial control actions in response to system anomalies for the ultimate objective of preventing blackouts. A physics-guided reinforcement learning (RL) framework is designed to identify effective sequences of real-time remedial look-ahead decisions accounting for the long-term impact on the system's stability. The paper considers a space of control actions that involve both discrete-valued transmission line-switching decisions (line reconnections and removals) and continuous-valued generator adjustments. To identify an effective blackout mitigation policy, a physics-guided approach is designed that uses power-flow sensitivity factors associated with the power transmission network to guide the RL exploration during agent training. Comprehensive empirical evaluations using the open-source Grid2Op platform demonstrate the notable advantages of incorporating physical signals into RL decisions, establishing the gains of the proposed physics-gu
    
[^18]: 用结构因果模型对潜在选择进行建模

    Modeling Latent Selection with Structural Causal Models. (arXiv:2401.06925v1 [cs.AI])

    [http://arxiv.org/abs/2401.06925](http://arxiv.org/abs/2401.06925)

    本文介绍了一种在结构因果模型中对潜在选择进行建模的方法，并展示了它如何帮助进行因果推理任务，包括处理选择偏差。

    

    选择偏倚在现实世界的数据中是普遍存在的，如果不正确处理可能导致误导性结果。我们引入了对结构因果模型（SCMs）进行条件操作的方法，以从因果的角度对潜在选择进行建模。我们展示了条件操作将具有明确潜在选择机制的SCM转换为没有此类选择机制的SCM，这在一定程度上编码了根据原始SCM选择的亚总体的因果语义。此外，我们还展示了该条件操作保持SCMs的简洁性，无环性和线性性，并与边际化操作相符合。由于这些特性与边际化和干预结合起来，条件操作为在潜在细节已经去除的因果模型中进行因果推理任务提供了一个有价值的工具。我们通过例子演示了如何将因果推断的经典结果推广以包括选择偏倚。

    Selection bias is ubiquitous in real-world data, and can lead to misleading results if not dealt with properly. We introduce a conditioning operation on Structural Causal Models (SCMs) to model latent selection from a causal perspective. We show that the conditioning operation transforms an SCM with the presence of an explicit latent selection mechanism into an SCM without such selection mechanism, which partially encodes the causal semantics of the selected subpopulation according to the original SCM. Furthermore, we show that this conditioning operation preserves the simplicity, acyclicity, and linearity of SCMs, and commutes with marginalization. Thanks to these properties, combined with marginalization and intervention, the conditioning operation offers a valuable tool for conducting causal reasoning tasks within causal models where latent details have been abstracted away. We demonstrate by example how classical results of causal inference can be generalized to include selection b
    
[^19]: DSA透明数据库：社交媒体自我报告的审核行动

    The DSA Transparency Database: Auditing Self-reported Moderation Actions by Social Media. (arXiv:2312.10269v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2312.10269](http://arxiv.org/abs/2312.10269)

    DSA透明数据库对欧盟八大社交媒体平台在前100天提交的审核行动数据进行了全面分析，揭示了这些平台在审核行动方面的部分遵循程度。

    

    从2023年9月开始，数字服务法案(DSA)要求大型在线平台向DSA透明数据库提交关于他们在欧盟内采取的每个审核行动的详细数据。从一开始，这个集中式数据库就引起了学术界的兴趣，因为它是现实世界在线审核数据的一个前所未有的、可能是独特的宝库。在这里，我们深入分析了欧盟八个最大社交媒体平台在数据库的前100天提交的所有3.53亿条记录。具体而言，我们对平台之间进行了比较研究，包括：审核行动的数量、决策依据、应用的限制类型、审核内容类型、审核行动的及时性和提交情况，以及使用的自动化程度。此外，我们系统地与平台自己的透明报告进行了内容交叉检查。我们的分析揭示了以下结果。(i)平台只在一定程度上遵循了审核行动的哲学和方法论。

    Since September 2023, the Digital Services Act (DSA) obliges large online platforms to submit detailed data on each moderation action they take within the European Union (EU) to the DSA Transparency Database. From its inception, this centralized database has sparked scholarly interest as an unprecedented and potentially unique trove of data on real-world online moderation. Here, we thoroughly analyze all 353.12M records submitted by the eight largest social media platforms in the EU during the first 100 days of the database. Specifically, we conduct a platform-wise comparative study of their: volume of moderation actions, grounds for decision, types of applied restrictions, types of moderated content, timeliness in undertaking and submitting moderation actions, and use of automation. Furthermore, we systematically cross-check the contents of the database with the platforms' own transparency reports. Our analyses reveal that (i) the platforms adhered only in part to the philosophy and s
    
[^20]: SC-MIL: 用于全切片图像分类的稀疏编码多实例学习

    SC-MIL: Sparsely Coded Multiple Instance Learning for Whole Slide Image Classification. (arXiv:2311.00048v1 [cs.CV])

    [http://arxiv.org/abs/2311.00048](http://arxiv.org/abs/2311.00048)

    本文提出了SC-MIL模型，通过利用稀疏字典学习来同时改进特征嵌入和实例相关性建模，从而提高全切片图像分类的性能。

    

    多实例学习（MIL）在弱监督的全切片图像（WSI）分类中被广泛使用。典型的MIL方法包括特征嵌入部分，通过预训练的特征提取器将实例嵌入到特征中，以及MIL聚合器，将实例嵌入组合成预测结果。目前的重点是通过自监督预训练来改进这些部分，并单独建模实例之间的相关性。在本文中，我们提出了一种稀疏编码的MIL（SC-MIL），同时通过利用稀疏字典学习来解决这两个方面。稀疏字典学习通过将实例表示为过完备字典中原子的稀疏线性组合来捕捉实例之间的相似性。此外，引入稀疏性可以通过抑制不相关的实例而保留最相关的实例，从而增强实例的特征嵌入。为了改善传统的特征嵌入和实例之间的相关性建模方法，we proposed a sparsely coded MIL.

    Multiple Instance Learning (MIL) has been widely used in weakly supervised whole slide image (WSI) classification. Typical MIL methods include a feature embedding part that embeds the instances into features via a pre-trained feature extractor and the MIL aggregator that combines instance embeddings into predictions. The current focus has been directed toward improving these parts by refining the feature embeddings through self-supervised pre-training and modeling the correlations between instances separately. In this paper, we proposed a sparsely coded MIL (SC-MIL) that addresses those two aspects at the same time by leveraging sparse dictionary learning. The sparse dictionary learning captures the similarities of instances by expressing them as a sparse linear combination of atoms in an over-complete dictionary. In addition, imposing sparsity help enhance the instance feature embeddings by suppressing irrelevant instances while retaining the most relevant ones. To make the convention
    
[^21]: 实现随机森林的局部可解释性增强：基于邻近性的方法

    Towards Enhanced Local Explainability of Random Forests: a Proximity-Based Approach. (arXiv:2310.12428v1 [stat.ML])

    [http://arxiv.org/abs/2310.12428](http://arxiv.org/abs/2310.12428)

    这项研究提出了一种利用随机森林模型的特征空间中的邻近性来解释模型预测的方法，为模型预测提供了局部的解释性，与现有方法相辅相成。通过实验证明了这种方法在债券定价模型中的有效性。

    

    我们提出一种新的方法来解释随机森林（RF）模型的样本外性能，利用了任何RF都可以被表述为自适应加权K最近邻（KNN）模型的事实。具体而言，我们利用RF在特征空间中学到的点之间的邻近性，将随机森林的预测重写为训练数据点目标标签的加权平均值。这种线性性质有助于在训练集观测中为任何模型预测生成属性，从而为RF预测提供了局部的解释性，补充了SHAP等已有方法，这些方法则为特征空间维度上的模型预测生成属性。我们在训练于美国公司债券交易数据的债券定价模型中演示了这种方法，并将其与各种现有的模型解释方法进行了比较。

    We initiate a novel approach to explain the out of sample performance of random forest (RF) models by exploiting the fact that any RF can be formulated as an adaptive weighted K nearest-neighbors model. Specifically, we use the proximity between points in the feature space learned by the RF to re-write random forest predictions exactly as a weighted average of the target labels of training data points. This linearity facilitates a local notion of explainability of RF predictions that generates attributions for any model prediction across observations in the training set, and thereby complements established methods like SHAP, which instead generates attributions for a model prediction across dimensions of the feature space. We demonstrate this approach in the context of a bond pricing model trained on US corporate bond trades, and compare our approach to various existing approaches to model explainability.
    
[^22]: 评估LLMs在特权升级场景中的应用

    Evaluating LLMs for Privilege-Escalation Scenarios. (arXiv:2310.11409v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2310.11409](http://arxiv.org/abs/2310.11409)

    本研究评估了在特权升级场景中利用语言模型（LLMs）进行渗透测试的应用。通过创建一个自动化的Linux特权升级基准和一个LLM-guided特权升级工具，我们分析了LLMs的不同提示设计、上下文学习和高级指导对测试的影响，并讨论了LLMs面临的挑战。

    

    渗透测试是网络安全的一个重要组成部分，它允许组织主动识别和修复系统中的漏洞，从而增强其对潜在网络攻击的防御机制。在渗透测试领域，最近的一个进展是利用语言模型（LLMs）。我们探索LLMs与渗透测试的交叉领域，以了解它们在特权升级场景中的能力和挑战。我们使用本地虚拟机创建了一个自动化的Linux特权升级基准。我们引入了一种基于LLMs的特权升级工具，用于评估不同的LLMs和提示策略在我们的基准测试中的表现。我们分析了不同提示设计的影响，上下文学习的好处，以及向LLMs提供高级指导的优势。我们讨论了LLMs面临的挑战领域，包括在测试过程中保持专注、处理错误以及与传统方法进行比较。

    Penetration testing, an essential component of cybersecurity, allows organizations to proactively identify and remediate vulnerabilities in their systems, thus bolstering their defense mechanisms against potential cyberattacks. One recent advancement in the realm of penetration testing is the utilization of Language Models (LLMs). We explore the intersection of LLMs and penetration testing to gain insight into their capabilities and challenges in the context of privilige escalation. We create an automated Linux privilege-escalation benchmark utilizing local virtual machines. We introduce an LLM-guided privilege-escalation tool designed for evaluating different LLMs and prompt strategies against our benchmark. We analyze the impact of different prompt designs, the benefits of in-context learning, and the advantages of offering high-level guidance to LLMs. We discuss challenging areas for LLMs, including maintaining focus during testing, coping with errors, and finally comparing them wit
    
[^23]: 使用基于原型的均值教师的多源域自适应目标检测

    Multi-Source Domain Adaptation for Object Detection with Prototype-based Mean-teacher. (arXiv:2309.14950v1 [cs.CV])

    [http://arxiv.org/abs/2309.14950](http://arxiv.org/abs/2309.14950)

    该论文提出了一种名为Prototype-based Mean-Teacher (PMT)的新型多源域自适应目标检测方法，通过使用类原型而不是域特定子网络来保留域特定信息，提高了准确性和鲁棒性。

    

    将视觉目标检测器适应于操作目标领域是一项具有挑战性的任务，通常使用无监督域自适应（UDA）方法来实现。当标记的数据集来自多个源域时，将它们视为单独的域并进行多源域自适应（MSDA），相比将这些源域混合并进行UDA，可以提高准确性和鲁棒性，近期的研究也证明了这一点。现有的MSDA方法学习域不变和域特定参数（对于每个源域）来进行自适应。然而，与单源UDA方法不同，学习域特定参数使它们与使用的源域数量成正比增长。本文提出了一种名为基于原型的均值教师（PMT）的新型MSDA方法，该方法使用类原型而不是域特定子网络来保留域特定信息。这些原型是使用对比损失学习的，对齐相同的类别。

    Adapting visual object detectors to operational target domains is a challenging task, commonly achieved using unsupervised domain adaptation (UDA) methods. When the labeled dataset is coming from multiple source domains, treating them as separate domains and performing a multi-source domain adaptation (MSDA) improves the accuracy and robustness over mixing these source domains and performing a UDA, as observed by recent studies in MSDA. Existing MSDA methods learn domain invariant and domain-specific parameters (for each source domain) for the adaptation. However, unlike single-source UDA methods, learning domain-specific parameters makes them grow significantly proportional to the number of source domains used. This paper proposes a novel MSDA method called Prototype-based Mean-Teacher (PMT), which uses class prototypes instead of domain-specific subnets to preserve domain-specific information. These prototypes are learned using a contrastive loss, aligning the same categories across 
    
[^24]: 使用领域随机化和目标跟踪神经网络进行种子核计数

    Seed Kernel Counting using Domain Randomization and Object Tracking Neural Networks. (arXiv:2308.05846v1 [cs.CV])

    [http://arxiv.org/abs/2308.05846](http://arxiv.org/abs/2308.05846)

    本研究提出了一种使用领域随机化和目标跟踪神经网络的方法来进行种子核计数。该方法通过使用合成图像作为训练数据的替代品，解决了神经网络模型需要大量有标签训练数据的问题，可以低成本估计谷物产量。

    

    高通量表型（HTP）对种子的评估是对生长、发育、耐受性、抗性、生态、产量等复杂种子特性的全面评估，以及衡量形成更复杂特性的参数。种子表型的关键之一是谷物产量估计，种子生产行业依赖于这一估计来进行业务运作。目前市场上已有机械化的种子核计数器，但价格往往很高，有时超出小规模种子生产企业的承受范围。目标跟踪神经网络模型(如YOLO)的发展使计算机科学家能够设计出可以低成本估计谷物产量的算法。神经网络模型的关键瓶颈是需要大量有标签的训练数据才能投入使用。我们证明了使用合成图像作为可行替代方案。

    High-throughput phenotyping (HTP) of seeds, also known as seed phenotyping, is the comprehensive assessment of complex seed traits such as growth, development, tolerance, resistance, ecology, yield, and the measurement of parameters that form more complex traits. One of the key aspects of seed phenotyping is cereal yield estimation that the seed production industry relies upon to conduct their business. While mechanized seed kernel counters are available in the market currently, they are often priced high and sometimes outside the range of small scale seed production firms' affordability. The development of object tracking neural network models such as You Only Look Once (YOLO) enables computer scientists to design algorithms that can estimate cereal yield inexpensively. The key bottleneck with neural network models is that they require a plethora of labelled training data before they can be put to task. We demonstrate that the use of synthetic imagery serves as a feasible substitute t
    
[^25]: 技术笔记：定义和量化DNN的AND-OR交互以进行准确和简明的解释

    Technical Note: Defining and Quantifying AND-OR Interactions for Faithful and Concise Explanation of DNNs. (arXiv:2304.13312v1 [cs.LG])

    [http://arxiv.org/abs/2304.13312](http://arxiv.org/abs/2304.13312)

    本文提出了一种通过量化输入变量之间的编码交互来准确且简明地解释深度神经网络(DNN)的推理逻辑的方法。针对此目的，作者提出了两种交互方式，即AND交互和OR交互，并利用它们设计出一系列技术来提高解释的简洁性，同时不会损害准确性。

    

    本文旨在通过量化输入变量之间的编码交互来解释深度神经网络(DNN)的推理逻辑。具体而言，我们首先重新思考交互的定义，然后正式定义了基于交互的解释的准确性和简洁性。为此，我们提出了两种交互方式，即AND交互和OR交互。针对准确性，我们证明了AND（OR）交互在量化输入变量之间的AND（OR）关系效应方面的唯一性。此外，基于AND-OR交互，我们设计了技术来提高解释的简洁性，同时不会损害准确性。因此，DNN的推理逻辑可以通过一组符号概念准确而简明地解释。

    In this technical note, we aim to explain a deep neural network (DNN) by quantifying the encoded interactions between input variables, which reflects the DNN's inference logic. Specifically, we first rethink the definition of interactions, and then formally define faithfulness and conciseness for interaction-based explanation. To this end, we propose two kinds of interactions, i.e., the AND interaction and the OR interaction. For faithfulness, we prove the uniqueness of the AND (OR) interaction in quantifying the effect of the AND (OR) relationship between input variables. Besides, based on AND-OR interactions, we design techniques to boost the conciseness of the explanation, while not hurting the faithfulness. In this way, the inference logic of a DNN can be faithfully and concisely explained by a set of symbolic concepts.
    
[^26]: 基于事件感知的生成对抗网络和自监督关系推理的超高分辨率探测器模拟

    Ultra-High-Resolution Detector Simulation with Intra-Event Aware GAN and Self-Supervised Relational Reasoning. (arXiv:2303.08046v1 [physics.ins-det])

    [http://arxiv.org/abs/2303.08046](http://arxiv.org/abs/2303.08046)

    本文提出了一种新颖的探测器模拟方法IEA-GAN，通过产生与图层相关的上下文化的图像，提高了超高分辨率探测器响应的相关性和多样性。同时，引入新的事件感知损失和统一性损失，显著提高了图像的保真度和多样性。

    

    在粒子物理学中，模拟高分辨率探测器响应一直是一个存储成本高、计算密集的过程。尽管深度生成模型可以使这个过程更具成本效益，但超高分辨率探测器模拟仍然很困难，因为它包含了事件内相关和细粒度的相互信息。为了克服这些限制，我们提出了一种新颖的生成对抗网络方法（IEA-GAN），融合了自监督学习和关系推理模型。IEA-GAN提出了一个关系推理模块，近似于探测器模拟中“事件”的概念，可以生成与图层相关的上下文化的图像，提高了超高分辨率探测器响应的相关性和多样性。IEA-GAN还引入了新的事件感知损失和统一性损失，显著提高了图像的保真度和多样性。我们展示了IEA-GAN的应用。

    Simulating high-resolution detector responses is a storage-costly and computationally intensive process that has long been challenging in particle physics. Despite the ability of deep generative models to make this process more cost-efficient, ultra-high-resolution detector simulation still proves to be difficult as it contains correlated and fine-grained mutual information within an event. To overcome these limitations, we propose Intra-Event Aware GAN (IEA-GAN), a novel fusion of Self-Supervised Learning and Generative Adversarial Networks. IEA-GAN presents a Relational Reasoning Module that approximates the concept of an ''event'' in detector simulation, allowing for the generation of correlated layer-dependent contextualized images for high-resolution detector responses with a proper relational inductive bias. IEA-GAN also introduces a new intra-event aware loss and a Uniformity loss, resulting in significant enhancements to image fidelity and diversity. We demonstrate IEA-GAN's ap
    
[^27]: 基于BERT模型的推文地理位置预测

    Geolocation Predicting of Tweets Using BERT-Based Models. (arXiv:2303.07865v1 [cs.CL])

    [http://arxiv.org/abs/2303.07865](http://arxiv.org/abs/2303.07865)

    该论文提出基于BERT模型的推文地理位置预测方法，可以实现全球和美国上的中位误差分别小于30公里和15公里的定位精度。

    

    该研究旨在解决推文/用户地理位置预测任务，并提供了处理文本大数据地理标记的灵活方法。该方法采用基于神经网络的自然语言处理来估计坐标对（经度，纬度）和二维高斯混合模型（GMM）。提出的模型的范围已经在Twitter数据集上使用预训练的BERT模型进行调整。性能指标表明，对于在推文内容和元数据上训练和评估的模型，全球范围内的中位误差小于30公里，美国范围内的中位误差小于15公里。

    This research is aimed to solve the tweet/user geolocation prediction task and provide a flexible methodology for the geotagging of textual big data. The suggested approach implements neural networks for natural language processing (NLP) to estimate the location as coordinate pairs (longitude, latitude) and two-dimensional Gaussian Mixture Models (GMMs). The scope of proposed models has been finetuned on a Twitter dataset using pretrained Bidirectional Encoder Representations from Transformers (BERT) as base models. Performance metrics show a median error of fewer than 30 km on a worldwide-level, and fewer than 15 km on the US-level datasets for the models trained and evaluated on text features of tweets' content and metadata context.
    
[^28]: 神经网络中的泛化：综述与分析

    Generalization in Neural Networks: A Broad Survey. (arXiv:2209.01610v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.01610](http://arxiv.org/abs/2209.01610)

    这篇论文总结了神经网络模型中不同抽象层次上的泛化问题及其方法，其中样本泛化已经取得了进展，但未来需要重点关注减少过拟合；分布泛化与领域泛化有相似之处，领域泛化方法可以应用于困难的样本或分布泛化。

    

    本文综述了神经网络模型中不同抽象层次的概念、建模方法和最近的研究成果，包括样本、分布、领域、任务、模态和范围上的泛化。在样本泛化方面的研究结果显示，在ImageNet数据集的情况下，几乎所有的最新改进都减小了训练误差，而过拟合保持不变；随着几乎所有的训练误差被消除，未来的进展将需要集中关注减少过拟合。从统计学的角度来看，(2)分布泛化可以被看作是样本权重或输入输出关系的变化；因此，在领域泛化成功的技术有可能应用于困难的样本或分布泛化。本文总结了转移学习方法(3)领域泛化的应用，以及最近的进展和丰富的应用领域。

    This paper reviews concepts, modeling approaches, and recent findings along a spectrum of different levels of abstraction of neural network models including generalization across (1) Samples, (2) Distributions, (3) Domains, (4) Tasks, (5) Modalities, and (6) Scopes. Results on (1) sample generalization show that, in the case of ImageNet, nearly all the recent improvements reduced training error while overfitting stayed flat; with nearly all the training error eliminated, future progress will require a focus on reducing overfitting. Perspectives from statistics highlight how (2) distribution generalization can be viewed alternately as a change in sample weights or a change in the input-output relationship; thus, techniques that have been successful in domain generalization have the potential to be applied to difficult forms of sample or distribution generalization. Transfer learning approaches to (3) domain generalization are summarized, as are recent advances and the wealth of domain a
    

