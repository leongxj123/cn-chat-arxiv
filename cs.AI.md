# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Empowering Air Travelers: A Chatbot for Canadian Air Passenger Rights](https://arxiv.org/abs/2403.12678) | 提出了一款关于加拿大空中旅客权利的聊天机器人，帮助旅客理解和利用相关空中旅行法规，成功解决了用户输入复杂和准确回答问题的挑战 |
| [^2] | [From explainable to interpretable deep learning for natural language processing in healthcare: how far from reality?](https://arxiv.org/abs/2403.11894) | 该研究对医疗保健NLP中的深度学习进行了全面审查，提出了可解释和可解释的人工智能（XIAI）概念，并发现注意机制是主要新兴IAI，同时面临着缺乏全局建模、最佳实践以及系统评估和基准测试的挑战。 |
| [^3] | [How Do Humans Write Code? Large Models Do It the Same Way Too](https://arxiv.org/abs/2402.15729) | 大型语言模型在执行数值计算时经常出错，通过生成可执行代码来解决问题可以减少计算错误，但观察到当大型语言模型使用代码解决数学问题时，会生成更多不正确推理；为解决这一问题，提出了一种受人类编码实践启发的简单而高效方法Human-Think Language（HTL）。 |
| [^4] | [COMPASS: Computational Mapping of Patient-Therapist Alliance Strategies with Language Modeling](https://arxiv.org/abs/2402.14701) | 本文提出了一种名为COMPASS的新框架，通过分析心理治疗会话中的自然语言，直接推断治疗工作联盟，为临床精神病学提供了可解释性，并在识别与正在治疗的疾病相关的新兴模式方面发挥作用。 |
| [^5] | [Token-Ensemble Text Generation: On Attacking the Automatic AI-Generated Text Detection](https://arxiv.org/abs/2402.11167) | 提出了一种新颖的token-ensemble生成策略，挑战了当前AI内容检测方法的鲁棒性，对当前检测模型构成了重要挑战，需要进一步改进检测技术以应对复杂对抗策略。 |
| [^6] | [Unmemorization in Large Language Models via Self-Distillation and Deliberate Imagination](https://arxiv.org/abs/2402.10052) | 本研究提出了一种新颖的大型语言模型遗忘方法，通过自我蒸馏和有意识的想象，有效地遗忘目标文本，并在生成任务和自然语言理解任务中保留模型的能力。 |
| [^7] | [Synergizing Spatial Optimization with Large Language Models for Open-Domain Urban Itinerary Planning](https://arxiv.org/abs/2402.07204) | 本文提出了Open-domain Urban Itinerary Planning (OUIP)任务，用于根据用户以自然语言描述的请求直接生成行程，通过结合空间优化和大型语言模型(LLM)，提供个性化的城市行程定制服务。 |
| [^8] | [Sequential Ordering in Textual Descriptions: Impact on Spatial Perception Abilities of Large Language Models](https://arxiv.org/abs/2402.07140) | 这项研究揭示了图描述的文本顺序对大语言模型在图推理中的性能产生显著影响，并通过改变文本顺序提高了大语言模型的性能。此外，发现大语言模型的推理性能与图大小之间的关系不是单调递减的。为了评估大语言模型在不同图大小上的性能，引入了规模化图推理基准。 |
| [^9] | [Diffusion World Model](https://arxiv.org/abs/2402.03570) | 扩散世界模型是一个能够预测多步未来状态和奖励的条件性扩散模型，在模型效果和性能方面超过了传统的一步动力学模型。 |
| [^10] | [Understanding What Affects Generalization Gap in Visual Reinforcement Learning: Theory and Empirical Evidence](https://arxiv.org/abs/2402.02701) | 本文通过理论和实证研究，揭示了在测试环境具有干扰因素时影响视觉强化学习中泛化差距的关键因素。结果表明，最小化训练和测试环境之间的表示距离是减少泛化差距最关键的因素。 |
| [^11] | [MixedNUTS: Training-Free Accuracy-Robustness Balance via Nonlinearly Mixed Classifiers](https://arxiv.org/abs/2402.02263) | MixedNUTS是一种无需训练的方法，通过非线性混合分类器的转换和概率混合来实现准确性和鲁棒性的平衡。 |
| [^12] | [Do LLM Agents Exhibit Social Behavior?](https://arxiv.org/abs/2312.15198) | 研究探讨了LLM代理在与人类和其他代理互动时展示的社会行为，包括社会学习、社会偏好和合作行为，并开发了一个框架来评估它们与人类实验对象的互动。 |
| [^13] | [Bad Students Make Great Teachers: Active Learning Accelerates Large-Scale Visual Understanding](https://arxiv.org/abs/2312.05328) | 研究提出了一种满足泛化性、扩展性和计算资源优化的主动学习方法，利用廉价的代理模型估计数据点的可学习性分数，优先选择训练更大的模型所需的数据，从而在大规模视觉理解任务中取得相同性能的同时减少了计算量。 |
| [^14] | [Explore, Select, Derive, and Recall: Augmenting LLM with Human-like Memory for Mobile Task Automation](https://arxiv.org/abs/2312.03003) | MobileGPT是一种创新的基于LLM的移动任务自动化工具，通过类人应用记忆模拟人类与移动应用的认知过程，实现任务程序的精确高效学习。 |
| [^15] | [Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations.](http://arxiv.org/abs/2310.20246) | 本文首次探索并训练了强大的多语言数学推理模型，通过使用翻译构建了多语言数据集，并提出了各种训练策略来构建强大的模型。实验证实发现在多语言训练中，将目标语言的翻译与原始语言的表示结合起来以及交替训练和多语言模型的自举可以提高模型的性能。此外，模型在处理低频词和长句子方面仍面临挑战。 |
| [^16] | [GraphMaker: Can Diffusion Models Generate Large Attributed Graphs?.](http://arxiv.org/abs/2310.13833) | GraphMaker是一种专门设计用于生成大型带属性图的新颖扩散模型。 |
| [^17] | [FedLPA: Personalized One-shot Federated Learning with Layer-Wise Posterior Aggregation.](http://arxiv.org/abs/2310.00339) | 本文提出了一种名为FedLPA的新颖方法，采用分层后验聚合的方式实现个性化单次联邦学习。FedLPA能够高效地将本地模型聚合到全局模型，解决了单次聚合在非相同训练数据分布下的性能问题。 |
| [^18] | [Reinforcement Learning-based Non-Autoregressive Solver for Traveling Salesman Problems.](http://arxiv.org/abs/2308.00560) | 基于强化学习的非自回归TSP求解器NAR4TSP使用特别设计的图神经网络进行推理，消除了昂贵标签的依赖，并在解决方案质量、推理延迟和泛化能力方面优于其他四个最先进的模型。 |
| [^19] | [On the Effective Horizon of Inverse Reinforcement Learning.](http://arxiv.org/abs/2307.06541) | 本研究分析了逆强化学习中时间视野的重要性，发现短于实际值的有效时间视野可以更快且更准确地估计奖励函数，减轻过拟合问题。此外，研究还呼吁在IRL中同时学习奖励和有效时间视野。 |
| [^20] | [Towards Human-centered Explainable AI: A Survey of User Studies for Model Explanations.](http://arxiv.org/abs/2210.11584) | 对模型解释的用户研究综述发现，可解释型人工智能（XAI）正在某些应用领域快速扩散，但用户评估仍然稀缺且几乎不涉及认知或社会科学的见解。 |

# 详细

[^1]: 为加拿大空中旅行者赋权：一款关于加拿大空中旅客权利的聊天机器人

    Empowering Air Travelers: A Chatbot for Canadian Air Passenger Rights

    [https://arxiv.org/abs/2403.12678](https://arxiv.org/abs/2403.12678)

    提出了一款关于加拿大空中旅客权利的聊天机器人，帮助旅客理解和利用相关空中旅行法规，成功解决了用户输入复杂和准确回答问题的挑战

    

    加拿大航空旅行领域的航班延误、取消和其他关于旅客权利的问题有了显著增加。认识到这一需求，我们提出了一个聊天机器人来协助旅客并教育他们了解自己的权利。我们的系统将复杂的用户输入分解为简单的查询，用于检索详细空中旅行法规的文档集中的信息。从这些文档中提取最相关的段落，并提供原始文档和生成的查询的链接，使用户能够将信息细分并利用于其独特情况。该系统成功克服了两个主要挑战：理解复杂的用户输入，并提供准确答案，没有幻觉，这些答案可以供旅客依赖以做出明智决策。一项比较聊天机器人和谷歌搜索的用户研究展示了聊天机器人的实用性和易用性。

    arXiv:2403.12678v1 Announce Type: cross  Abstract: The Canadian air travel sector has seen a significant increase in flight delays, cancellations, and other issues concerning passenger rights. Recognizing this demand, we present a chatbot to assist passengers and educate them about their rights. Our system breaks a complex user input into simple queries which are used to retrieve information from a collection of documents detailing air travel regulations. The most relevant passages from these documents are presented along with links to the original documents and the generated queries, enabling users to dissect and leverage the information for their unique circumstances. The system successfully overcomes two predominant challenges: understanding complex user inputs, and delivering accurate answers, free of hallucinations, that passengers can rely on for making informed decisions. A user study comparing the chatbot to a Google search demonstrated the chatbot's usefulness and ease of use.
    
[^2]: 从可解释到可解释的深度学习在医疗自然语言处理中的应用：现实有多远？

    From explainable to interpretable deep learning for natural language processing in healthcare: how far from reality?

    [https://arxiv.org/abs/2403.11894](https://arxiv.org/abs/2403.11894)

    该研究对医疗保健NLP中的深度学习进行了全面审查，提出了可解释和可解释的人工智能（XIAI）概念，并发现注意机制是主要新兴IAI，同时面临着缺乏全局建模、最佳实践以及系统评估和基准测试的挑战。

    

    深度学习（DL）通过解决各种自然语言处理（NLP）任务，极大地增强了医疗保健研究。然而，基于DL的NLP方法日益复杂，需要透明的模型解释性，或至少是可解释性，以进行可靠的决策制定。本文对医疗健康NLP中的可解释和可解释的DL进行了彻底的范围审查。引入了术语“XIAI”（eXplainable和Interpretable Artificial Intelligence）以区分XAI和IAI。方法根据其功能（模型、输入、输出为基础）和范围（局部、全局）进一步分类。我们的分析表明，注意机制是最主要的新兴IAI。此外，IAI越来越多地用于对抗XAI。确定的主要挑战是大多数XIAI不探索“全局”建模过程，缺乏最佳实践，并且需要系统评估和基准测试。

    arXiv:2403.11894v1 Announce Type: cross  Abstract: Deep learning (DL) has substantially enhanced healthcare research by addressing various natural language processing (NLP) tasks. Yet, the increasing complexity of DL-based NLP methods necessitates transparent model interpretability, or at least explainability, for reliable decision-making. This work presents a thorough scoping review on explainable and interpretable DL in healthcare NLP. The term "XIAI" (eXplainable and Interpretable Artificial Intelligence) was introduced to distinguish XAI from IAI. Methods were further categorized based on their functionality (model-, input-, output-based) and scope (local, global). Our analysis shows that attention mechanisms were the most dominant emerging IAI. Moreover, IAI is increasingly used against XAI. The major challenges identified are that most XIAI do not explore "global" modeling processes, the lack of best practices, and the unmet need for systematic evaluation and benchmarks. Importan
    
[^3]: 人类是如何编写代码的？大型模型也以同样的方式进行

    How Do Humans Write Code? Large Models Do It the Same Way Too

    [https://arxiv.org/abs/2402.15729](https://arxiv.org/abs/2402.15729)

    大型语言模型在执行数值计算时经常出错，通过生成可执行代码来解决问题可以减少计算错误，但观察到当大型语言模型使用代码解决数学问题时，会生成更多不正确推理；为解决这一问题，提出了一种受人类编码实践启发的简单而高效方法Human-Think Language（HTL）。

    

    大型语言模型（LLMs）在执行数值计算时经常出错。与传统的思维链推理相比，程序化思维方法涉及生成可执行代码来解决问题。通过执行这些代码，它可以获得更精确的结果。使用生成的可执行代码而不是自然语言可以减少计算错误。然而，我们观察到当LLMs使用代码解决数学问题时，他们往往生成比使用自然语言更多的不正确推理。为了解决这个问题，我们提出了Human-Think Language（HTL），这是一种受到人类编码实践启发的简单而高效的方法。该方法首先由模型生成用自然语言描述的解决问题方法，然后将其转换为代码，反映出人们在将逻辑以自然语言形式思考后再将其写成代码的过程。此外，它利用了P

    arXiv:2402.15729v1 Announce Type: new  Abstract: Large Language Models (LLMs) often make errors when performing numerical calculations. In contrast to traditional chain-of-thought reasoning, the program-of-thoughts approach involves generating executable code to solve problems. By executing this code, it achieves more precise results. Using generated executable code instead of natural language can reduce computational errors. However, we observe that when LLMs solve mathematical problems using code, they tend to generate more incorrect reasoning than when using natural language. To address this issue, we propose Human-Think Language (HTL), a straightforward yet highly efficient approach inspired by human coding practices. The approach first generates problem-solving methods described in the natural language by the model, then converts them into code, mirroring the process where people think through the logic in natural language before writing it as code. Additionally, it utilizes the P
    
[^4]: COMPASS：利用语言建模对患者-治疗师联盟策略进行计算映射

    COMPASS: Computational Mapping of Patient-Therapist Alliance Strategies with Language Modeling

    [https://arxiv.org/abs/2402.14701](https://arxiv.org/abs/2402.14701)

    本文提出了一种名为COMPASS的新框架，通过分析心理治疗会话中的自然语言，直接推断治疗工作联盟，为临床精神病学提供了可解释性，并在识别与正在治疗的疾病相关的新兴模式方面发挥作用。

    

    治疗工作联盟是预测心理治疗治疗成功的关键因素。传统上，工作联盟评估依赖于治疗师和患者填写的问卷。本文提出了COMPASS，一个新颖的框架，可直接从心理治疗课程中使用的自然语言中推断治疗工作联盟。我们的方法利用先进的大型语言模型分析心理治疗会话的转录，并将其与工作联盟清单中陈述的分布式表示进行比较。通过分析涵盖多种精神疾病的超过950个会话的数据集，我们展示了我们的方法在显微地映射患者-治疗师对齐轨迹方面的有效性，并为临床精神病学提供解释性，并在识别与正在治疗的疾病相关的新兴模式方面提供可解释性。通过使用各种神经主题模式

    arXiv:2402.14701v1 Announce Type: cross  Abstract: The therapeutic working alliance is a critical factor in predicting the success of psychotherapy treatment. Traditionally, working alliance assessment relies on questionnaires completed by both therapists and patients. In this paper, we present COMPASS, a novel framework to directly infer the therapeutic working alliance from the natural language used in psychotherapy sessions. Our approach utilizes advanced large language models to analyze transcripts of psychotherapy sessions and compare them with distributed representations of statements in the working alliance inventory. Analyzing a dataset of over 950 sessions covering diverse psychiatric conditions, we demonstrate the effectiveness of our method in microscopically mapping patient-therapist alignment trajectories and providing interpretability for clinical psychiatry and in identifying emerging patterns related to the condition being treated. By employing various neural topic mode
    
[^5]: Token-Ensemble文本生成：对自动AI生成文本检测的攻击

    Token-Ensemble Text Generation: On Attacking the Automatic AI-Generated Text Detection

    [https://arxiv.org/abs/2402.11167](https://arxiv.org/abs/2402.11167)

    提出了一种新颖的token-ensemble生成策略，挑战了当前AI内容检测方法的鲁棒性，对当前检测模型构成了重要挑战，需要进一步改进检测技术以应对复杂对抗策略。

    

    AI内容检测模型对经过精心设计的攻击（例如改写或词语替换）的鲁棒性仍然是一个重要问题。本研究提出了一种新颖的token-ensemble生成策略，挑战了当前AI内容检测方法的鲁棒性。我们通过使用从随机候选语言模型生成的下一个token完成提示来探索集成攻击策略。我们发现token-ensemble方法显著降低了AI内容检测模型的性能（代码和测试集将发布）。我们的发现表明，token-ensemble生成对当前检测模型构成了重要挑战，并强调了改进检测技术以应对复杂对抗策略的需求。

    arXiv:2402.11167v1 Announce Type: cross  Abstract: The robustness of AI-content detection models against cultivated attacks (e.g., paraphrasing or word switching) remains a significant concern. This study proposes a novel token-ensemble generation strategy to challenge the robustness of current AI-content detection approaches. We explore the ensemble attack strategy by completing the prompt with the next token generated from random candidate LLMs. We find the token-ensemble approach significantly drops the performance of AI-content detection models (The code and test sets will be released). Our findings reveal that token-ensemble generation poses a vital challenge to current detection models and underlines the need for advancing detection technologies to counter sophisticated adversarial strategies.
    
[^6]: 大型语言模型通过自我蒸馏和有意识的想象进行遗忘

    Unmemorization in Large Language Models via Self-Distillation and Deliberate Imagination

    [https://arxiv.org/abs/2402.10052](https://arxiv.org/abs/2402.10052)

    本研究提出了一种新颖的大型语言模型遗忘方法，通过自我蒸馏和有意识的想象，有效地遗忘目标文本，并在生成任务和自然语言理解任务中保留模型的能力。

    

    虽然在许多任务上表现出令人印象深刻的生成能力，但大型语言模型（LLM）仍然存在隐私侵犯和敏感数据不受控制的问题。因此，我们提出了一种新颖的方法，即在LLM遗忘的过程中采用有意识的想象。我们不是试图忘记已记忆的数据，而是通过自我蒸馏的框架引导LLM有意识地想象替代情境。通过广泛的实验，我们证明了这种方法不仅可以有效地遗忘目标文本，还可以保留LLM在开放式生成任务和自然语言理解（NLU）任务中的能力。我们的结果展示了这种方法在不同模型和规模中的实用性。

    arXiv:2402.10052v1 Announce Type: cross  Abstract: While displaying impressive generation capabilities across many tasks, Large Language Models (LLMs) still struggle with crucial issues of privacy violation and unwanted exposure of sensitive data. This raises an essential question: how should we prevent such undesired behavior of LLMs while maintaining their strong generation and natural language understanding (NLU) capabilities? In this work, we introduce a novel approach termed deliberate imagination in the context of LLM unlearning. Instead of trying to forget memorized data, we employ a self-distillation framework, guiding LLMs to deliberately imagine alternative scenarios. As demonstrated in a wide range of experiments, the proposed method not only effectively unlearns targeted text but also preserves the LLMs' capabilities in open-ended generation tasks as well as in NLU tasks. Our results demonstrate the usefulness of this approach across different models and sizes, and also wit
    
[^7]: 结合空间优化和大型语言模型的开放领域城市行程规划

    Synergizing Spatial Optimization with Large Language Models for Open-Domain Urban Itinerary Planning

    [https://arxiv.org/abs/2402.07204](https://arxiv.org/abs/2402.07204)

    本文提出了Open-domain Urban Itinerary Planning (OUIP)任务，用于根据用户以自然语言描述的请求直接生成行程，通过结合空间优化和大型语言模型(LLM)，提供个性化的城市行程定制服务。

    

    本文首次提出了Open-domain Urban Itinerary Planning (OUIP)任务，用于根据用户以自然语言描述的请求直接生成行程。OUIP与传统行程规划不同，传统规划限制了用户表达更详细的需求，阻碍了真正的个性化。最近，大型语言模型(LLM)在处理多样化任务方面表现出潜力。然而，由于非实时信息、不完整的知识和不足的空间意识，它们无法独立地提供满意的用户体验。鉴于此，我们提出了一个名为ItiNera的OUIP系统，将空间优化与大型语言模型(LLM)相结合，根据用户需求提供个性化的城市行程定制服务。具体来说，我们开发了一个基于LLM的流水线，用于提取和更新兴趣点特征，以创建用户自己的个性化兴趣点数据库。对于每个用户请求，我们利用LLM进行协同实现优化。

    In this paper, we for the first time propose the task of Open-domain Urban Itinerary Planning (OUIP) for citywalk, which directly generates itineraries based on users' requests described in natural language. OUIP is different from conventional itinerary planning, which limits users from expressing more detailed needs and hinders true personalization. Recently, large language models (LLMs) have shown potential in handling diverse tasks. However, due to non-real-time information, incomplete knowledge, and insufficient spatial awareness, they are unable to independently deliver a satisfactory user experience in OUIP. Given this, we present ItiNera, an OUIP system that synergizes spatial optimization with Large Language Models (LLMs) to provide services that customize urban itineraries based on users' needs. Specifically, we develop an LLM-based pipeline for extracting and updating POI features to create a user-owned personalized POI database. For each user request, we leverage LLM in coop
    
[^8]: 文字描述中的顺序对大语言模型的空间感知能力的影响

    Sequential Ordering in Textual Descriptions: Impact on Spatial Perception Abilities of Large Language Models

    [https://arxiv.org/abs/2402.07140](https://arxiv.org/abs/2402.07140)

    这项研究揭示了图描述的文本顺序对大语言模型在图推理中的性能产生显著影响，并通过改变文本顺序提高了大语言模型的性能。此外，发现大语言模型的推理性能与图大小之间的关系不是单调递减的。为了评估大语言模型在不同图大小上的性能，引入了规模化图推理基准。

    

    最近几年，大语言模型在多个领域达到了最先进的性能。然而，图推理领域的进展仍然有限。我们的工作深入研究了大语言模型的图推理。在这项工作中，我们揭示了文本顺序对大语言模型空间理解的影响，发现图描述的文本顺序显著影响大语言模型对图的推理性能。通过改变图描述的文本顺序，我们将大语言模型的性能从42.22％提高到70％。此外，我们评估了大语言模型性能和图大小之间的关系，发现大语言模型的推理性能不随图大小的增加而单调递减。最后，我们引入了规模化图推理基准来评估大语言模型在不同图大小上的性能。

    In recent years, Large Language Models have reached state-of-the-art performance across multiple domains. However, the progress in the field of graph reasoning remains limited. Our work delves into this gap by thoroughly investigating graph reasoning with LLM. In this work, we reveal the impact of text sequence on LLM spatial understanding, finding that graph-descriptive text sequences significantly affect LLM reasoning performance on graphs. By altering the graph-descriptive text sequences, we enhance the performance of LLM from 42.22\% to 70\%. Furthermore, we evaluate the relationship between LLM performance and graph size, discovering that the reasoning performance of LLM does not monotonically decrease with the increase in graph size. Conclusively, we introduce the Scaled Graph Reasoning benchmark for assessing LLM performance across varied graph sizes.
    
[^9]: 扩散世界模型

    Diffusion World Model

    [https://arxiv.org/abs/2402.03570](https://arxiv.org/abs/2402.03570)

    扩散世界模型是一个能够预测多步未来状态和奖励的条件性扩散模型，在模型效果和性能方面超过了传统的一步动力学模型。

    

    我们引入了扩散世界模型（DWM），这是一个条件扩散模型，能够同时预测多步的未来状态和奖励。与传统的一步动力学模型相反，DWM通过单个前向传递提供了长时程的预测，消除了递归查询的需要。我们将DWM整合到基于模型的价值估计中，其中短期回报通过从DWM中采样的未来轨迹进行模拟。在离线强化学习的背景下，DWM可以被视为通过生成建模来实现保守的值正则化。另外，它也可以被视为一种数据源，使离线Q学习能够使用合成数据。我们在D4RL数据集上的实验证实了DWM对长时程模拟的鲁棒性。在绝对性能方面，DWM显著超过了一步动力学模型，性能提高了44%，并达到了最先进的水平。

    We introduce Diffusion World Model (DWM), a conditional diffusion model capable of predicting multistep future states and rewards concurrently. As opposed to traditional one-step dynamics models, DWM offers long-horizon predictions in a single forward pass, eliminating the need for recursive quires. We integrate DWM into model-based value estimation, where the short-term return is simulated by future trajectories sampled from DWM. In the context of offline reinforcement learning, DWM can be viewed as a conservative value regularization through generative modeling. Alternatively, it can be seen as a data source that enables offline Q-learning with synthetic data. Our experiments on the D4RL dataset confirm the robustness of DWM to long-horizon simulation. In terms of absolute performance, DWM significantly surpasses one-step dynamics models with a $44\%$ performance gain, and achieves state-of-the-art performance.
    
[^10]: 理解影响视觉强化学习中泛化差距的因素：理论和实证证据

    Understanding What Affects Generalization Gap in Visual Reinforcement Learning: Theory and Empirical Evidence

    [https://arxiv.org/abs/2402.02701](https://arxiv.org/abs/2402.02701)

    本文通过理论和实证研究，揭示了在测试环境具有干扰因素时影响视觉强化学习中泛化差距的关键因素。结果表明，最小化训练和测试环境之间的表示距离是减少泛化差距最关键的因素。

    

    最近，有许多努力致力于在视觉强化学习中学习对连续控制有用的策略。在这种场景下，学习一个具有泛化能力的策略非常重要，因为测试环境可能与训练环境不同，例如在部署过程中存在干扰因素。许多实际算法被提出来解决这个问题。然而，据我们所知，它们中没有一种算法能够从理论上解释泛化差距的影响因素以及为什么他们的方法有效。在本文中，我们通过在测试环境具有干扰因素时理论上回答影响泛化差距的关键因素来解决这个问题。我们的理论表明，最小化训练和测试环境之间的表示距离（与人类直觉一致）对于减少泛化差距的效益至关重要。我们的理论结果得到了DM数据的实证证据的支持。

    Recently, there are many efforts attempting to learn useful policies for continuous control in visual reinforcement learning (RL). In this scenario, it is important to learn a generalizable policy, as the testing environment may differ from the training environment, e.g., there exist distractors during deployment. Many practical algorithms are proposed to handle this problem. However, to the best of our knowledge, none of them provide a theoretical understanding of what affects the generalization gap and why their proposed methods work. In this paper, we bridge this issue by theoretically answering the key factors that contribute to the generalization gap when the testing environment has distractors. Our theories indicate that minimizing the representation distance between training and testing environments, which aligns with human intuition, is the most critical for the benefit of reducing the generalization gap. Our theoretical results are supported by the empirical evidence in the DM
    
[^11]: MixedNUTS: 通过非线性混合分类器实现无需训练的准确性和鲁棒性平衡

    MixedNUTS: Training-Free Accuracy-Robustness Balance via Nonlinearly Mixed Classifiers

    [https://arxiv.org/abs/2402.02263](https://arxiv.org/abs/2402.02263)

    MixedNUTS是一种无需训练的方法，通过非线性混合分类器的转换和概率混合来实现准确性和鲁棒性的平衡。

    

    鲁棒性往往牺牲了准确性，阻碍了鲁棒分类模型在实际应用中的使用。基于训练的解决方案在与已训练的大型高性能模型兼容性方面存在限制，因此需要探索无需训练的集成方法。我们观察到鲁棒模型在干净数据和对抗数据上的正确预测比错误预测更自信，我们推测通过增强这种“良性置信度特性”可以在集成环境中实现准确性和鲁棒性的平衡。为了实现这一点，我们提出了“MixedNUTS”，一种无需训练的方法，利用仅有三个参数的非线性转换来处理鲁棒分类器和标准非鲁棒分类器的输出Logits，并通过高效算法进行优化。然后，MixedNUTS将转换后的Logits转换为概率，并将它们混合作为最终的输出。在CIFAR-10、CIFAR-100和ImageNet数据集上进行了实验。

    Adversarial robustness often comes at the cost of degraded accuracy, impeding the real-life application of robust classification models. Training-based solutions for better trade-offs are limited by incompatibilities with already-trained high-performance large models, necessitating the exploration of training-free ensemble approaches. Observing that robust models are more confident in correct predictions than in incorrect ones on clean and adversarial data alike, we speculate amplifying this "benign confidence property" can reconcile accuracy and robustness in an ensemble setting. To achieve so, we propose "MixedNUTS", a training-free method where the output logits of a robust classifier and a standard non-robust classifier are processed by nonlinear transformations with only three parameters, which are optimized through an efficient algorithm. MixedNUTS then converts the transformed logits into probabilities and mixes them as the overall output. On CIFAR-10, CIFAR-100, and ImageNet da
    
[^12]: LLM代理表现出社会行为吗？

    Do LLM Agents Exhibit Social Behavior?

    [https://arxiv.org/abs/2312.15198](https://arxiv.org/abs/2312.15198)

    研究探讨了LLM代理在与人类和其他代理互动时展示的社会行为，包括社会学习、社会偏好和合作行为，并开发了一个框架来评估它们与人类实验对象的互动。

    

    大型语言模型（LLMs）的进展正在扩大它们在学术研究和实际应用中的效用。最近的社会科学研究探讨了使用这些“黑匣子”LLM代理来模拟复杂社会系统并潜在地替代人类实验对象的可能性。我们的研究深入探讨了这一新兴领域，调查了LLMs在与人类和其他代理进行互动时展示社会学习、社会偏好和合作行为（间接互惠）等关键社会交互原则的程度。我们为我们的研究制定了一个框架，其中涉及将涉及人类实验对象的经典实验调整为使用LLM代理。这种方法涉及一步一步的推理，模拟人类认知过程和零样本学习，以评估LLMs的天生偏好。我们对LLM代理行为的分析包括主要效应和次要效应。

    arXiv:2312.15198v2 Announce Type: replace  Abstract: The advances of Large Language Models (LLMs) are expanding their utility in both academic research and practical applications. Recent social science research has explored the use of these ``black-box'' LLM agents for simulating complex social systems and potentially substituting human subjects in experiments. Our study delves into this emerging domain, investigating the extent to which LLMs exhibit key social interaction principles, such as social learning, social preference, and cooperative behavior (indirect reciprocity), in their interactions with humans and other agents. We develop a framework for our study, wherein classical laboratory experiments involving human subjects are adapted to use LLM agents. This approach involves step-by-step reasoning that mirrors human cognitive processes and zero-shot learning to assess the innate preferences of LLMs. Our analysis of LLM agents' behavior includes both the primary effects and an in
    
[^13]: 不良学生成就了优秀教师：主动学习加速大规模视觉理解

    Bad Students Make Great Teachers: Active Learning Accelerates Large-Scale Visual Understanding

    [https://arxiv.org/abs/2312.05328](https://arxiv.org/abs/2312.05328)

    研究提出了一种满足泛化性、扩展性和计算资源优化的主动学习方法，利用廉价的代理模型估计数据点的可学习性分数，优先选择训练更大的模型所需的数据，从而在大规模视觉理解任务中取得相同性能的同时减少了计算量。

    

    幂律缩放表明使用均匀采样的大规模训练速度太慢。主动学习方法旨在通过优先学习最相关的示例来提高数据效率。尽管这些方法具有吸引力，但由于没有证明 a) 可以泛化到各种模型和任务 b) 可以扩展到大规模数据集并且 c) 在考虑数据选择开销时能节省计算资源，因此尚未被广泛采用。在这项工作中，我们提出了一种满足这三个属性的方法，利用小型廉价的代理模型来估计数据点的“可学习性”分数，用于优先训练更大的模型所需的数据。结果上，我们的模型在JFT上需要46%和51%更少的训练更新次数，并且要达到与均匀训练的视觉分类器在JFT和ALIGN上多模型的相同性能需要高达25%的更少的总计算量。最后，我们发现我们的数据优先方法能够在视觉理解和多模型任务上取得与均匀训练相当的性能，同时节省了计算资源。

    arXiv:2312.05328v3 Announce Type: replace Abstract: Power-law scaling indicates that large-scale training with uniform sampling is prohibitively slow. Active learning methods aim to increase data efficiency by prioritizing learning on the most relevant examples. Despite their appeal, these methods have yet to be widely adopted since no one algorithm has been shown to a) generalize across models and tasks b) scale to large datasets and c) yield overall FLOP savings when accounting for the overhead of data selection. In this work we propose a method which satisfies these three properties, leveraging small, cheap proxy models to estimate "learnability" scores for datapoints, which are used to prioritize data for the training of much larger models. As a result, our models require 46% and 51% fewer training updates and up to 25% less total computation to reach the same performance as uniformly trained visual classifiers on JFT and multimodal models on ALIGN. Finally, we find our data-priori
    
[^14]: 探索、选择、推导和回忆：为移动任务自动化增加类人记忆的LLM

    Explore, Select, Derive, and Recall: Augmenting LLM with Human-like Memory for Mobile Task Automation

    [https://arxiv.org/abs/2312.03003](https://arxiv.org/abs/2312.03003)

    MobileGPT是一种创新的基于LLM的移动任务自动化工具，通过类人应用记忆模拟人类与移动应用的认知过程，实现任务程序的精确高效学习。

    

    大型语言模型（LLMs）的出现为移动任务自动化领域带来了新的机遇。它们优越的语言理解和推理能力使用户能够自动执行复杂和重复的任务。然而，由于LLMs固有的不可靠性和高运行成本，它们的实际适用性相当有限。为解决这些问题，本文引入了MobileGPT，这是一种创新的基于LLM的移动任务自动化工具，配备了类人应用记忆。MobileGPT模拟了人类与移动应用交互的认知过程--探索、选择、推导和回忆。这种方法通过将任务程序分解为更小、模块化的子任务，允许更精确、高效地学习任务流程，从而实现子任务的重复使用、重新排列和适应各种目标。我们使用在线LLM服务（GPT-3.5和GPT-4）实现了MobileGPT，并在一组数据上评估了其性能。

    arXiv:2312.03003v2 Announce Type: replace-cross  Abstract: The advent of large language models (LLMs) has opened up new opportunities in the field of mobile task automation. Their superior language understanding and reasoning capabilities allow users to automate complex and repetitive tasks. However, due to the inherent unreliability and high operational cost of LLMs, their practical applicability is quite limited. To address these issues, this paper introduces MobileGPT, an innovative LLM-based mobile task automator equipped with a human-like app memory. MobileGPT emulates the cognitive process of humans interacting with a mobile app -- explore, select, derive, and recall. This approach allows for a more precise and efficient learning of a task's procedure by breaking it down into smaller, modular sub-tasks that can be re-used, re-arranged, and adapted for various objectives. We implement MobileGPT using online LLMs services (GPT-3.5 and GPT-4) and evaluate its performance on a datase
    
[^15]: 在多语言数学推理中打破语言障碍：见解与观察

    Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations. (arXiv:2310.20246v1 [cs.CL])

    [http://arxiv.org/abs/2310.20246](http://arxiv.org/abs/2310.20246)

    本文首次探索并训练了强大的多语言数学推理模型，通过使用翻译构建了多语言数据集，并提出了各种训练策略来构建强大的模型。实验证实发现在多语言训练中，将目标语言的翻译与原始语言的表示结合起来以及交替训练和多语言模型的自举可以提高模型的性能。此外，模型在处理低频词和长句子方面仍面临挑战。

    

    现有研究主要集中在开发适用于单语言中的数学推理的强大语言学习模型（LLM），在多语言环境下保持效果的研究很少。为了弥补这一差距，本文首次探索和训练强大的多语言数学推理（xMR）LLM。首先，通过利用翻译，我们构建了第一个包含十种不同语言的多语言数学推理指导数据集MGSM8KInstruct，从而解决了xMR任务中训练数据稀缺的问题。根据收集的数据集，我们提出了不同的训练策略来构建强大的xMR LLMs，被命名为MathOctopus，在几次训练中表现出优于传统开源LLMs和ChatGPT的能力。值得注意的是，MathOctopus-13B在MGSM测试集上达到了47.6%的准确率，超过了ChatGPT的46.3%。除了显著的结果，我们还从大量的实验证实中发现了一些重要的观察和见解：（1）在多语言上进行训练时，最好将目标语言的翻译与原始语言的表示结合起来。 （2）交替训练和多语言模型的自举有助于提高模型的表现。 （3）模型对于低频词和长句子的处理是挑战的，需要进一步改进。

    Existing research predominantly focuses on developing powerful language learning models (LLMs) for mathematical reasoning within monolingual languages, with few explorations in preserving efficacy in a multilingual context. To bridge this gap, this paper pioneers exploring and training powerful Multilingual Math Reasoning (xMR) LLMs. Firstly, by utilizing translation, we construct the first multilingual math reasoning instruction dataset, MGSM8KInstruct, encompassing ten distinct languages, thus addressing the issue of training data scarcity in xMR tasks. Based on the collected dataset, we propose different training strategies to build powerful xMR LLMs, named MathOctopus, notably outperform conventional open-source LLMs and exhibit superiority over ChatGPT in few-shot scenarios. Notably, MathOctopus-13B reaches 47.6% accuracy which exceeds ChatGPT 46.3% on MGSM testset. Beyond remarkable results, we unearth several pivotal observations and insights from extensive experiments: (1) When
    
[^16]: GraphMaker: 扩散模型能生成大型带属性图吗？

    GraphMaker: Can Diffusion Models Generate Large Attributed Graphs?. (arXiv:2310.13833v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.13833](http://arxiv.org/abs/2310.13833)

    GraphMaker是一种专门设计用于生成大型带属性图的新颖扩散模型。

    

    在各种实际应用中，具有节点属性的大规模图变得越来越常见。创建与真实世界示例类似的合成、富属性图对于共享图数据进行分析和开发学习模型至关重要，尤其是当原始数据限制被共享时。传统的图生成方法在处理这些复杂结构方面存在局限性。最新的扩散模型在生成没有属性和较小的分子图方面显示出潜力。然而，这些模型在生成大型带属性图方面面临着挑战，原因是复杂的属性-结构相关性和图的大规模。本文介绍了一种专门用于生成大型带属性图的新颖扩散模型：GraphMaker。我们探索了各种节点属性和图结构生成过程的组合，发现异步方法更有效地捕捉了内部。

    Large-scale graphs with node attributes are increasingly common in various real-world applications. Creating synthetic, attribute-rich graphs that mirror real-world examples is crucial, especially for sharing graph data for analysis and developing learning models when original data is restricted to be shared. Traditional graph generation methods are limited in their capacity to handle these complex structures. Recent advances in diffusion models have shown potential in generating graph structures without attributes and smaller molecular graphs. However, these models face challenges in generating large attributed graphs due to the complex attribute-structure correlations and the large size of these graphs. This paper introduces a novel diffusion model, GraphMaker, specifically designed for generating large attributed graphs. We explore various combinations of node attribute and graph structure generation processes, finding that an asynchronous approach more effectively captures the intr
    
[^17]: FedLPA: 使用分层后验聚合的个性化单次联邦学习

    FedLPA: Personalized One-shot Federated Learning with Layer-Wise Posterior Aggregation. (arXiv:2310.00339v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.00339](http://arxiv.org/abs/2310.00339)

    本文提出了一种名为FedLPA的新颖方法，采用分层后验聚合的方式实现个性化单次联邦学习。FedLPA能够高效地将本地模型聚合到全局模型，解决了单次聚合在非相同训练数据分布下的性能问题。

    

    将本地客户端训练的神经网络高效地聚合到服务器上的全局模型是联邦学习中的一个广泛研究课题。最近，受到隐私问题减少、潜在攻击减弱和通信开销降低的推动，单次联邦学习（即将客户端与服务器间的通信限制为一轮）在研究者中越来越受欢迎。然而，单次聚合的性能容易受到非相同训练数据分布的影响，在一些实际场景中表现出高度的统计异质性。为了解决这个问题，我们提出了一种新颖的单次聚合方法——分层后验聚合（FedLPA）。FedLPA能够聚合本地模型，获得更准确的全局模型，而无需额外的辅助数据集或暴露任何机密的本地信息，比如标签分布。

    Efficiently aggregating trained neural networks from local clients into a global model on a server is a widely researched topic in federated learning. Recently, motivated by diminishing privacy concerns, mitigating potential attacks, and reducing the overhead of communication, one-shot federated learning (i.e., limiting client-server communication into a single round) has gained popularity among researchers. However, the one-shot aggregation performances are sensitively affected by the non-identical training data distribution, which exhibits high statistical heterogeneity in some real-world scenarios. To address this issue, we propose a novel one-shot aggregation method with Layer-wise Posterior Aggregation, named FedLPA. FedLPA aggregates local models to obtain a more accurate global model without requiring extra auxiliary datasets or exposing any confidential local information, e.g., label distributions. To effectively capture the statistics maintained in the biased local datasets in
    
[^18]: 基于强化学习的非自回归求解器用于旅行推销员问题

    Reinforcement Learning-based Non-Autoregressive Solver for Traveling Salesman Problems. (arXiv:2308.00560v1 [cs.AI])

    [http://arxiv.org/abs/2308.00560](http://arxiv.org/abs/2308.00560)

    基于强化学习的非自回归TSP求解器NAR4TSP使用特别设计的图神经网络进行推理，消除了昂贵标签的依赖，并在解决方案质量、推理延迟和泛化能力方面优于其他四个最先进的模型。

    

    旅行推销员问题（TSP）是组合优化中的一个众所周知的问题，具有在各个领域的应用。然而，现有的TSP求解器在产生高质量解决方案时面临低延迟的挑战。为了解决这个问题，我们提出了NAR4TSP，它使用一个特别设计的图神经网络（GNN）以非自回归（NAR）方式生成TSP解决方案，实现更快的推理速度。此外，NAR4TSP使用增强的强化学习（RL）策略进行训练，消除了传统监督学习基于NAR模型训练所使用的昂贵标签的依赖关系。据我们所知，NAR4TSP是第一个成功结合了RL和NAR解码的TSP求解器。在合成和真实的TSP实例上的实验结果表明，NAR4TSP在解决方案质量、推理延迟和泛化能力方面优于四个最先进的模型。最后，我们展示了NAR4TSP解码过程的可视化。

    The Traveling Salesman Problem (TSP) is a well-known problem in combinatorial optimization with applications in various domains. However, existing TSP solvers face challenges in producing high-quality solutions with low latency. To address this issue, we propose NAR4TSP, which produces TSP solutions in a Non-Autoregressive (NAR) manner using a specially designed Graph Neural Network (GNN), achieving faster inference speed. Moreover, NAR4TSP is trained using an enhanced Reinforcement Learning (RL) strategy, eliminating the dependency on costly labels used to train conventional supervised learning-based NAR models. To the best of our knowledge, NAR4TSP is the first TSP solver that successfully combines RL and NAR decoding. The experimental results on both synthetic and real-world TSP instances demonstrate that NAR4TSP outperforms four state-of-the-art models in terms of solution quality, inference latency, and generalization ability. Lastly, we present visualizations of NAR4TSP's decodin
    
[^19]: 逆强化学习中的有效时间视野研究

    On the Effective Horizon of Inverse Reinforcement Learning. (arXiv:2307.06541v1 [cs.LG])

    [http://arxiv.org/abs/2307.06541](http://arxiv.org/abs/2307.06541)

    本研究分析了逆强化学习中时间视野的重要性，发现短于实际值的有效时间视野可以更快且更准确地估计奖励函数，减轻过拟合问题。此外，研究还呼吁在IRL中同时学习奖励和有效时间视野。

    

    逆强化学习（IRL）算法通常依赖于基于给定时间视野的（前向）强化学习或规划来计算一个近似最优策略，然后将该策略与专家演示匹配。时间视野在确定奖励估计的准确性和IRL算法的计算效率方面起着关键作用。有趣的是，比地面实际值更短的有效时间视野通常能更快地产生更好的结果。本文对此现象进行了正式分析并给出了解释：时间视野控制了引发策略类的复杂性，并在有限数据下减轻过拟合。这一分析为IRL的有效视野选择提供了原则性指导。它也促使我们重新审视经典的IRL公式：与仅具有给定视野的奖励相比，共同学习奖励和有效视野更加自然。我们的实验进一步验证了这一观点。

    Inverse reinforcement learning (IRL) algorithms often rely on (forward) reinforcement learning or planning over a given time horizon to compute an approximately optimal policy for a hypothesized reward function and then match this policy with expert demonstrations. The time horizon plays a critical role in determining both the accuracy of reward estimate and the computational efficiency of IRL algorithms. Interestingly, an effective time horizon shorter than the ground-truth value often produces better results faster. This work formally analyzes this phenomenon and provides an explanation: the time horizon controls the complexity of an induced policy class and mitigates overfitting with limited data. This analysis leads to a principled choice of the effective horizon for IRL. It also prompts us to reexamine the classic IRL formulation: it is more natural to learn jointly the reward and the effective horizon together rather than the reward alone with a given horizon. Our experimental re
    
[^20]: 朝着以人为中心的可解释型人工智能：对模型解释的用户研究综述

    Towards Human-centered Explainable AI: A Survey of User Studies for Model Explanations. (arXiv:2210.11584v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2210.11584](http://arxiv.org/abs/2210.11584)

    对模型解释的用户研究综述发现，可解释型人工智能（XAI）正在某些应用领域快速扩散，但用户评估仍然稀缺且几乎不涉及认知或社会科学的见解。

    

    可解释型人工智能（XAI）被广泛认为是不断扩展的人工智能研究的必需条件。对XAI用户需求的更好理解以及可解释模型的人本评估既是必要性也是挑战。在本文中，我们通过系统性文献综述研究了HCI和AI研究人员如何进行XAI应用的用户研究。通过对过去五年中基于人类的XAI评估的97篇核心论文进行识别和深入分析，我们将其按照解释方法的测量特征（信任、理解、可用性和人工智能与人类的合作表现）进行分类。我们的研究表明，XAI在某些应用领域（如推荐系统）扩散更迅速，但用户评估仍相当稀缺，并且几乎没有融入认知或社会科学的任何见解。基于综合讨论的最佳实践，即常见模型、设计选择和度量方法。

    Explainable AI (XAI) is widely viewed as a sine qua non for ever-expanding AI research. A better understanding of the needs of XAI users, as well as human-centered evaluations of explainable models are both a necessity and a challenge. In this paper, we explore how HCI and AI researchers conduct user studies in XAI applications based on a systematic literature review. After identifying and thoroughly analyzing 97core papers with human-based XAI evaluations over the past five years, we categorize them along the measured characteristics of explanatory methods, namely trust, understanding, usability, and human-AI collaboration performance. Our research shows that XAI is spreading more rapidly in certain application domains, such as recommender systems than in others, but that user evaluations are still rather sparse and incorporate hardly any insights from cognitive or social sciences. Based on a comprehensive discussion of best practices, i.e., common models, design choices, and measures
    

