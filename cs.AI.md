# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [A Survey on Large Language Model-Based Game Agents](https://arxiv.org/abs/2404.02039) | LLM和MLLM的进步为游戏智能体提供了强大的人类决策能力，本文全面综述了基于LLM的游戏智能体的概念架构、方法论和未来研究方向 |
| [^2] | [Diffusion Models Meet Contextual Bandits with Large Action Spaces](https://arxiv.org/abs/2402.10028) | 本文设计了一种利用预训练扩散模型的扩散汤普森采样方法，用于在大动作空间下进行高效的情境强化学习探索。实证评估结果表明了该方法的优越性能。 |

# 详细

[^1]: 基于大型语言模型的游戏智能体综述

    A Survey on Large Language Model-Based Game Agents

    [https://arxiv.org/abs/2404.02039](https://arxiv.org/abs/2404.02039)

    LLM和MLLM的进步为游戏智能体提供了强大的人类决策能力，本文全面综述了基于LLM的游戏智能体的概念架构、方法论和未来研究方向

    

    游戏智能体的发展在推动人工通用智能（AGI）方面扮演着关键角色。LLM及其多模态对应物（MLLM）的进展为游戏智能体在复杂的电脑游戏环境中具备类似人类决策能力提供了前所未有的机会。本文从整体视角全面概述了基于LLM的游戏智能体。首先，我们介绍了以感知、记忆、思维、角色扮演、行动和学习为中心的LLM游戏智能体的概念架构。其次，我们调查了文献中已有的代表性LLM游戏智能体，涉及到六类游戏中的方法论和适应能力，包括冒险、沟通、竞争、合作、模拟以及创造与探索游戏。最后，我们展望了未来研究的方向。

    arXiv:2404.02039v1 Announce Type: new  Abstract: The development of game agents holds a critical role in advancing towards Artificial General Intelligence (AGI). The progress of LLMs and their multimodal counterparts (MLLMs) offers an unprecedented opportunity to evolve and empower game agents with human-like decision-making capabilities in complex computer game environments. This paper provides a comprehensive overview of LLM-based game agents from a holistic viewpoint. First, we introduce the conceptual architecture of LLM-based game agents, centered around six essential functional components: perception, memory, thinking, role-playing, action, and learning. Second, we survey existing representative LLM-based game agents documented in the literature with respect to methodologies and adaptation agility across six genres of games, including adventure, communication, competition, cooperation, simulation, and crafting & exploration games. Finally, we present an outlook of future research
    
[^2]: 扩散模型与大动作空间情境强化学习的结合

    Diffusion Models Meet Contextual Bandits with Large Action Spaces

    [https://arxiv.org/abs/2402.10028](https://arxiv.org/abs/2402.10028)

    本文设计了一种利用预训练扩散模型的扩散汤普森采样方法，用于在大动作空间下进行高效的情境强化学习探索。实证评估结果表明了该方法的优越性能。

    

    由于动作空间较大，有效的探索是情境强化学习中的一个关键挑战。本文通过利用预训练的扩散模型来捕捉动作之间的相关性，设计了扩散汤普森采样（dTS）方法，实现了高效的探索。我们为dTS方法提供了理论和算法基础，并通过实证评估展示了它的优越性能。

    arXiv:2402.10028v1 Announce Type: cross  Abstract: Efficient exploration is a key challenge in contextual bandits due to the large size of their action space, where uninformed exploration can result in computational and statistical inefficiencies. Fortunately, the rewards of actions are often correlated and this can be leveraged to explore them efficiently. In this work, we capture such correlations using pre-trained diffusion models; upon which we design diffusion Thompson sampling (dTS). Both theoretical and algorithmic foundations are developed for dTS, and empirical evaluation also shows its favorable performance.
    

