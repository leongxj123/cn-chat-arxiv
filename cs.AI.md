# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Brain-Like Replay Naturally Emerges in Reinforcement Learning Agents](https://rss.arxiv.org/abs/2402.01467) | 本研究中，我们在使用递归神经网络的强化学习模型中发现了类似大脑回放的现象，并证明其对任务的贡献。这一发现提供了理解回放机制的新视角。 |
| [^2] | [A Survey of Optimization-based Task and Motion Planning: From Classical To Learning Approaches](https://arxiv.org/abs/2404.02817) | 本综述全面审视了基于优化的任务与运动规划，重点讨论了如何通过混合优化方法解决高度复杂、接触丰富的机器人运动和操作问题。 |
| [^3] | [Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks](https://arxiv.org/abs/2404.02151) | 展示了对齐的LLM对简单自适应越狱攻击不具有鲁棒性，并成功实现了在多个模型上几乎100%的攻击成功率，同时还介绍了对于不公开logprobs的模型如何进行越狱以及如何在受污染的模型中查找木马字符串的方法。 |
| [^4] | [Evalverse: Unified and Accessible Library for Large Language Model Evaluation](https://arxiv.org/abs/2404.00943) | Evalverse是一个统一和易用的库，简化了大型语言模型（LLMs）的评估，为研究人员和从业者提供了集中且易于访问的评估框架。 |
| [^5] | [sDPO: Don't Use Your Data All at Once](https://arxiv.org/abs/2403.19270) | sDPO是对直接偏好优化方法的扩展，通过分步利用偏好数据集而非一次性使用，促进更精确对齐参考模型的使用，并训练出性能更优的最终模型，甚至胜过其他具有更多参数的流行大型语言模型。 |
| [^6] | [Contrastive Learning with Orthonormal Anchors (CLOA)](https://arxiv.org/abs/2403.18699) | 该研究提出了一种新的损失函数称为正交锚点回归损失，用于解开嵌入聚类，显著增强嵌入的独特性 |
| [^7] | [TransFusion: Contrastive Learning with Transformers](https://arxiv.org/abs/2403.18681) | TransFusion的主要创新在于定义了对比学习领域中的两个基本问题的理论极限，并成功实现了从复杂的现实世界数据中提取特征以改善分类精度。 |
| [^8] | [SCANet: Correcting LEGO Assembly Errors with Self-Correct Assembly Network](https://arxiv.org/abs/2403.18195) | 介绍了单步组装错误校正任务和LEGO错误校正组装数据集（LEGO-ECA），提出了用于这一任务的自校正组装网络（SCANet）。 |
| [^9] | [MetaAligner: Conditional Weak-to-Strong Correction for Generalizable Multi-Objective Alignment of Language Models](https://arxiv.org/abs/2403.17141) | MetaAligner是第一个与策略无关且通用的多目标偏好对齐方法，通过将参数更新与策略模型解耦实现即插即用的对齐，并通过上下文学习实现未见目标的零冷启动偏好对齐 |
| [^10] | [Generalized Consistency Trajectory Models for Image Manipulation](https://arxiv.org/abs/2403.12510) | 本研究提出了广义一致性轨迹模型（GCTMs），能够在任何噪声分布和数据分布之间实现转换。 |
| [^11] | [HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models](https://arxiv.org/abs/2403.11456) | HateCOT数据集通过GPT-3.5-Turbo生成解释，将52,000个样本数据用于预训练模型，显著提升了在不同领域和任务下的攻击性内容检测效果。 |
| [^12] | [Partially Observable Task and Motion Planning with Uncertainty and Risk Awareness](https://arxiv.org/abs/2403.10454) | 提出了一种具有不确定性和风险意识的TAMP策略（TAMPURA），能够高效地解决具有初始状态和动作结果不确定性的长时程规划问题，包括需要信息收集和避免不良和不可逆结果的问题。 |
| [^13] | [Towards Embedding Dynamic Personas in Interactive Robots: Masquerading Animated Social Kinematics (MASK)](https://arxiv.org/abs/2403.10041) | 该研究提出了一种名为MASK的机器人系统，通过非言语互动与观众进行互动，并利用有限状态机结构调整机器人行为，实现多种不同角色的动态表达。 |
| [^14] | [Editing Conceptual Knowledge for Large Language Models](https://arxiv.org/abs/2403.06259) | 该论文首次研究了为大型语言模型编辑概念知识，通过构建基准数据集和建立新评估指标，发现现有方法虽然能一定程度上修改概念定义，但也可能造成LLMs中相关实例知识的扭曲，导致性能下降。 |
| [^15] | [RFWave: Multi-band Rectified Flow for Audio Waveform Reconstruction](https://arxiv.org/abs/2403.05010) | RFWave是一种新颖的多频带整流流动方法，可以从Mel频谱图中重建高保真度音频波形，仅需10个采样步骤即可实现出色的重建质量和优越的计算效率。 |
| [^16] | [A systematic evaluation of large language models for generating programming code](https://arxiv.org/abs/2403.00894) | GPT-4在生成编程代码方面表现优异，特别是在选择最佳提示策略时，超过了其他大型语言模型和85%的人类参与者。 |
| [^17] | [Tokenization Is More Than Compression](https://arxiv.org/abs/2402.18376) | 通过引入新的分词器PathPiece，研究者发现少量标记并不能导致更好的下游性能，这一结果对于 Tokenization 的有效性理解提出了质疑。 |
| [^18] | [Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2402.17840) | 研究揭示了检索增强生成系统中的数据泄露风险，指出对手可以利用LMs的指示遵循能力轻松地从数据存储中直接提取文本数据，并设计了攻击对生产RAG模型GPTs造成数据存储泄漏。 |
| [^19] | [Successfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections](https://arxiv.org/abs/2402.16973) | 通过检测潜在幻觉并建议替代方案的通信机制，成功减少人类导航错误高达29%而不增加认知负担 |
| [^20] | [QuaCer-C: Quantitative Certification of Knowledge Comprehension in LLMs](https://arxiv.org/abs/2402.15929) | 本文提出了一种新颖的认证框架QuaCer-C，用于正式认证大型语言模型中知识理解的能力，证书定量化且包含高置信度的概率界限，研究发现，随着参数数量的增加，知识理解能力提高，Mistral模型在这一评估中表现不如其他模型。 |
| [^21] | [Linear Dynamics-embedded Neural Network for Long-Sequence Modeling](https://arxiv.org/abs/2402.15290) | 提出了一种名为嵌入线性动力学的神经网络（LDNN），通过引入连续状态空间模型的属性和优化策略，实现了在长序列任务中具有少量参数、灵活推断和高效训练，最终在长距离竞技场上取得了有效且领先的性能。 |
| [^22] | [A Usage-centric Take on Intent Understanding in E-Commerce](https://arxiv.org/abs/2402.14901) | 该论文提出了电子商务中意图理解的一个新视角，不依赖于产品本体，通过引入产品恢复基准验证了当前意图知识图的弱点。 |
| [^23] | [Arrange, Inpaint, and Refine: Steerable Long-term Music Audio Generation and Editing via Content-based Controls](https://arxiv.org/abs/2402.09508) | 通过引入参数高效微调（PEFT）方法，本研究实现了自回归语言模型在音乐修复和音乐排列任务中的应用。在多个音乐编辑任务中，该方法展示了有希望的结果，并为未来的AI驱动音乐编辑工具提供了更灵活的控制。 |
| [^24] | [UrbanKGent: A Unified Large Language Model Agent Framework for Urban Knowledge Graph Construction](https://arxiv.org/abs/2402.06861) | UrbanKGent是一个用于城市知识图谱构建的统一大型语言模型代理框架，通过异构感知和地理空间注入构建知识化指令集，并通过迭代轨迹细化模块来提升轨迹的质量。在两个真实世界的数据集上进行的评估表明了UrbanKGent的有效性和性能。 |
| [^25] | [Learning Contrastive Feature Representations for Facial Action Unit Detection](https://arxiv.org/abs/2402.06165) | 这项研究提出了一种对比学习框架，通过监督和自监督信号来增强面部动作单元检测模型的性能。采用正样本抽样和权衡重要性的损失函数来应对噪声AU标签和AU类型分布不平衡的挑战。 |
| [^26] | [Preference-free Alignment Learning with Regularized Relevance Reward](https://arxiv.org/abs/2402.03469) | 无偏好对齐学习使用正则化相关奖励作为关键目标，在提供稳健奖励信号的同时，显著提高了偏好基准测试的性能。 |
| [^27] | [Conversation Reconstruction Attack Against GPT Models](https://arxiv.org/abs/2402.02987) | 本文介绍了一种针对 GPT 模型的对话重构攻击，该攻击具有劫持会话和重构对话的两个步骤。通过对该攻击对 GPT 模型的隐私风险进行评估，发现 GPT-4 对该攻击具有一定的鲁棒性。 |
| [^28] | [Large Language Models are Geographically Biased](https://arxiv.org/abs/2402.02680) | 本文研究了大型语言模型的地理偏见，并展示了其对地理空间预测的系统错误，通过零射击地理空间预测来评估其对世界的认知。 |
| [^29] | [Multi-agent reinforcement learning using echo-state network and its application to pedestrian dynamics](https://arxiv.org/abs/2312.11834) | 通过使用回声状态网络将行人实现为MARL代理，研究了他们学习避让其他代理向前移动的能力，在密度不太高的情况下取得成功。 |
| [^30] | [On Diversified Preferences of Large Language Model Alignment](https://arxiv.org/abs/2312.07401) | 本文通过定量分析常用人类反馈数据集，揭示了多样化偏好对奖励建模的影响，提出了一种新颖的多目标奖励学习方法以增强校准性能 |
| [^31] | [Compelling ReLU Network Initialization and Training to Leverage Exponential Scaling with Depth](https://arxiv.org/abs/2311.18022) | 该论文提出了一种新的训练策略，通过重新参数化网络权重，使得神经网络的指数数量的激活模式得以展现，从而得到远远超过随机初始化的结果。 |
| [^32] | [Augmenting Black-box LLMs with Medical Textbooks for Clinical Question Answering](https://arxiv.org/abs/2309.02233) | 该研究提出了一种名为LLMs增强医学教科书（LLM-AMT）的系统，通过插入式模块将权威医学教科书集成到LLMs的框架中，显著提高了LLMs在专业领域的能力。 |
| [^33] | [R-Judge: Benchmarking Safety Risk Awareness for LLM Agents.](http://arxiv.org/abs/2401.10019) | 这篇论文主要介绍了一种评估LLM代理在不同环境中判断安全风险能力的基准测试R-Judge，通过对162个代理交互记录进行评估，发现GPT-4模型表现最佳，达到了72.29%的准确率。 |
| [^34] | [SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully.](http://arxiv.org/abs/2401.05930) | 自我突出式犹豫（SH2）是一种推理时的方法，通过选择预测概率较低的标记，并强调它们的差异，从而帮助语言模型更准确地解码。 |
| [^35] | [Data-Centric Foundation Models in Computational Healthcare: A Survey.](http://arxiv.org/abs/2401.02458) | 计算医疗中的数据中心基础模型是一项调查研究，为医疗工作流程的改进提供了基于数据的人工智能方法，并讨论了安全性、评估和与人类价值观的一致性。基于FM的分析有望提高患者结果和临床工作流程表现。 |
| [^36] | [The Power of Training: How Different Neural Network Setups Influence the Energy Demand.](http://arxiv.org/abs/2401.01851) | 本文研究了机器学习训练方案和学习范式对能源消耗的影响，并探讨了预训练和多任务训练在可持续机器学习方面的潜力。 |
| [^37] | [Speak Like a Native: Prompting Large Language Models in a Native Style.](http://arxiv.org/abs/2311.13538) | 本文提出了一种名为AlignedCoT的新颖有效方法，通过将上下文示例与大型语言模型（LLMs）的母语风格对齐，提高了LLMs的推理能力和性能。 |
| [^38] | [NOD-TAMP: Multi-Step Manipulation Planning with Neural Object Descriptors.](http://arxiv.org/abs/2311.01530) | NOD-TAMP是一个基于TAMP的框架，利用神经物体描述符来解决复杂操纵任务中的泛化问题，通过从少量人类演示中提取轨迹并进行调整，有效解决了长时程任务的挑战，并在模拟环境中优于现有方法。 |
| [^39] | [Content-based Controls For Music Large Language Modeling.](http://arxiv.org/abs/2310.17162) | 该论文提出了一种基于内容的控制方法，用于音乐大语言建模。通过对音高、和弦和鼓乐等固有音乐语言的直接控制，实现了高质量的音乐生成，并且使用了参数高效微调的方法，比原始模型的参数数量少于4%。 |
| [^40] | [Learning Successor Representations with Distributed Hebbian Temporal Memory.](http://arxiv.org/abs/2310.13391) | 本文提出了一种名为DHTM的算法，它基于因子图形式和多组成神经元模型，利用分布式表示、稀疏转移矩阵和局部Hebbian样学习规则来解决在线隐藏表示学习的挑战。实验结果表明，DHTM在变化的环境中比经典的LSTM效果更好，并与更先进的类似RNN的算法性能相当，可以加速继任者表示的时间差异学习。 |
| [^41] | [Safety-Gymnasium: A Unified Safe Reinforcement Learning Benchmark.](http://arxiv.org/abs/2310.12567) | 本文介绍了一个名为Safety-Gymnasium的环境套件，其中包括单个和多个Agent场景中的安全关键任务，并提供了一个包含16种最先进的SafeRL算法的算法库。这个基准旨在促进对安全性能的评估和比较，推动强化学习在安全性能上的发展。 |
| [^42] | [Uncertainty-Aware Decision Transformer for Stochastic Driving Environments.](http://arxiv.org/abs/2309.16397) | 本论文提出了一种针对随机驾驶环境的不确定性感知决策Transformer（UNREST），通过估计状态的不确定性并相应地分割序列，取代了全局回报。这项工作通过解决在不确定性环境中过于乐观的问题，为离线强化学习在自主驾驶任务中的应用提供了一种有效的解决方案。 |
| [^43] | [A Comprehensive Survey on Rare Event Prediction.](http://arxiv.org/abs/2309.11356) | 本文综合调研了罕见事件预测领域的当前方法，通过考虑罕见事件数据、数据处理、算法方法和评估方法四个维度，总结出了在机器学习流程中解决罕见事件预测问题的关键方法。 |
| [^44] | [Encode-Store-Retrieve: Enhancing Memory Augmentation through Language-Encoded Egocentric Perception.](http://arxiv.org/abs/2308.05822) | 本研究提出了一种记忆增强系统，它利用自然语言编码视频数据并将其存储在向量数据库中，通过利用大型视觉语言模型的强大功能来进行语言编码的过程。 |
| [^45] | [Learned Kernels for Interpretable and Efficient PPG Signal Quality Assessment and Artifact Segmentation.](http://arxiv.org/abs/2307.05385) | 本文提出了一种通过学习核技术，具有解释性且参数较少的方法来评估和分割PPG信号的质量和伪影，与现有的深度神经网络方法相比有着类似甚至更好的性能。 |
| [^46] | [FheFL: Fully Homomorphic Encryption Friendly Privacy-Preserving Federated Learning with Byzantine Users.](http://arxiv.org/abs/2306.05112) | 本论文介绍了一种新的联邦学习算法，采用FHE加密技术，既可以保护模型更新的隐私，又可以防止恶意用户破坏全局模型。 |
| [^47] | [Continually Updating Generative Retrieval on Dynamic Corpora.](http://arxiv.org/abs/2305.18952) | 本文研究了动态语料库上的生成检索。实验结果表明，在静态设置下，生成检索效果优于双编码器，但在动态设置下情况相反。通过使用参数高效的预训练方法，我们的模型DynamicGR在新的语料库上展现出了意外的性能。 |
| [^48] | [Representation Learning on Hyper-Relational and Numeric Knowledge Graphs with Transformers.](http://arxiv.org/abs/2305.18256) | 本文提出了一个名为HyNT的框架，用于学习超关系型知识图的表示，包括数值文字。该框架使用上下文Transformer和预测Transformer，通过学习三元组和其限定词之间的相关性以及数值信息来获得模型。 |
| [^49] | [PI-FL: Personalized and Incentivized Federated Learning.](http://arxiv.org/abs/2304.07514) | PI-FL是一种个性化的联邦学习解决方案，使用基于令牌的激励机制奖励个性化训练，可以在尊重客户端隐私的同时生成高质量的个性化模型。 |

# 详细

[^1]: 强化学习智能体内出现类似大脑回放的现象

    Brain-Like Replay Naturally Emerges in Reinforcement Learning Agents

    [https://rss.arxiv.org/abs/2402.01467](https://rss.arxiv.org/abs/2402.01467)

    本研究中，我们在使用递归神经网络的强化学习模型中发现了类似大脑回放的现象，并证明其对任务的贡献。这一发现提供了理解回放机制的新视角。

    

    大脑区域中普遍观察到的回放现象是否能够在人工智能智能体中自然产生？如果是的话，它是否对任务有所贡献？在本研究中，我们使用基于递归神经网络的强化学习模型，在任务优化的范式下发现了回放的自然出现，模型模拟了海马体和前额叶皮层以及它们之间的相互沟通和感觉皮层的输入。海马体中的回放是由于情景记忆、认知地图以及环境观察而产生的，与动物实验数据相似，并且是高任务性能的有效指标。该模型还成功地重现了局部和非局部的回放，与人类实验数据相符。我们的工作为理解回放机制提供了新的途径。

    Can replay, as a widely observed neural activity pattern in brain regions, particularly in the hippocampus and neocortex, emerge in an artificial agent? If yes, does it contribute to the tasks? In this work, without heavy dependence on complex assumptions, we discover naturally emergent replay under task-optimized paradigm using a recurrent neural network-based reinforcement learning model, which mimics the hippocampus and prefrontal cortex, as well as their intercommunication and the sensory cortex input. The emergent replay in the hippocampus, which results from the episodic memory and cognitive map as well as environment observations, well resembles animal experimental data and serves as an effective indicator of high task performance. The model also successfully reproduces local and nonlocal replay, which matches the human experimental data. Our work provides a new avenue for understanding the mechanisms behind replay.
    
[^2]: 优化型任务与运动规划综述：从经典到学习方法

    A Survey of Optimization-based Task and Motion Planning: From Classical To Learning Approaches

    [https://arxiv.org/abs/2404.02817](https://arxiv.org/abs/2404.02817)

    本综述全面审视了基于优化的任务与运动规划，重点讨论了如何通过混合优化方法解决高度复杂、接触丰富的机器人运动和操作问题。

    

    任务与运动规划（TAMP）将高层任务规划和低层运动规划结合起来，使机器人能够有效地推理解决长时域、动态任务。基于优化的TAMP专注于通过目标函数定义目标条件的混合优化方法，并且能够处理开放式目标、机器人动态和机器人与环境之间的物理交互。因此，基于优化的TAMP特别适合解决高度复杂、接触丰富的运动和操作问题。本综述全面审视了基于优化的TAMP，涵盖了（i）规划领域表示，包括动作描述语言和时态逻辑，（ii）TAMP各组件的个别解决策略，包括人工智能规划和轨迹优化（TO），以及（iii）基于逻辑的任务规划与基于模型的TO之间的动态相互作用。

    arXiv:2404.02817v1 Announce Type: cross  Abstract: Task and Motion Planning (TAMP) integrates high-level task planning and low-level motion planning to equip robots with the autonomy to effectively reason over long-horizon, dynamic tasks. Optimization-based TAMP focuses on hybrid optimization approaches that define goal conditions via objective functions and are capable of handling open-ended goals, robotic dynamics, and physical interaction between the robot and the environment. Therefore, optimization-based TAMP is particularly suited to solve highly complex, contact-rich locomotion and manipulation problems. This survey provides a comprehensive review on optimization-based TAMP, covering (i) planning domain representations, including action description languages and temporal logic, (ii) individual solution strategies for components of TAMP, including AI planning and trajectory optimization (TO), and (iii) the dynamic interplay between logic-based task planning and model-based TO. A 
    
[^3]: 用简单自适应攻击越狱功能对齐的LLM

    Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks

    [https://arxiv.org/abs/2404.02151](https://arxiv.org/abs/2404.02151)

    展示了对齐的LLM对简单自适应越狱攻击不具有鲁棒性，并成功实现了在多个模型上几乎100%的攻击成功率，同时还介绍了对于不公开logprobs的模型如何进行越狱以及如何在受污染的模型中查找木马字符串的方法。

    

    我们展示了即使是最新的安全对齐的LLM也不具有抵抗简单自适应越狱攻击的稳健性。首先，我们展示了如何成功利用对logprobs的访问进行越狱：我们最初设计了一个对抗性提示模板（有时会适应目标LLM），然后我们在后缀上应用随机搜索以最大化目标logprob（例如token“Sure”），可能会进行多次重启。通过这种方式，我们实现了对GPT-3.5/4、Llama-2-Chat-7B/13B/70B、Gemma-7B和针对GCG攻击进行对抗训练的HarmBench上的R2D2等几乎100%的攻击成功率--根据GPT-4的评判。我们还展示了如何通过转移或预填充攻击以100%的成功率对所有不暴露logprobs的Claude模型进行越狱。此外，我们展示了如何在受污染的模型中使用对一组受限制的token执行随机搜索以查找木马字符串的方法--这项任务与许多其他任务共享相同的属性。

    arXiv:2404.02151v1 Announce Type: cross  Abstract: We show that even the most recent safety-aligned LLMs are not robust to simple adaptive jailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobs for jailbreaking: we initially design an adversarial prompt template (sometimes adapted to the target LLM), and then we apply random search on a suffix to maximize the target logprob (e.g., of the token "Sure"), potentially with multiple restarts. In this way, we achieve nearly 100\% attack success rate -- according to GPT-4 as a judge -- on GPT-3.5/4, Llama-2-Chat-7B/13B/70B, Gemma-7B, and R2D2 from HarmBench that was adversarially trained against the GCG attack. We also show how to jailbreak all Claude models -- that do not expose logprobs -- via either a transfer or prefilling attack with 100\% success rate. In addition, we show how to use random search on a restricted set of tokens for finding trojan strings in poisoned models -- a task that shares many s
    
[^4]: Evalverse: 大型语言模型评估的统一和易用库

    Evalverse: Unified and Accessible Library for Large Language Model Evaluation

    [https://arxiv.org/abs/2404.00943](https://arxiv.org/abs/2404.00943)

    Evalverse是一个统一和易用的库，简化了大型语言模型（LLMs）的评估，为研究人员和从业者提供了集中且易于访问的评估框架。

    

    本文介绍了Evalverse，这是一个新颖的库，通过将不同的评估工具统一到一个用户友好的框架中，简化了对大型语言模型（LLMs）的评估。Evalverse使得对人工智能了解有限的个人可以轻松请求LLMs评估并收到详细报告，利用与Slack等通信平台的集成。因此，Evalverse作为LLMs的全面评估强大工具，为研究人员和从业者提供了集中且易于访问的评估框架。最后，我们还提供了Evalverse的演示视频，展示了它的功能和实现方式，以两分钟的格式展示。

    arXiv:2404.00943v1 Announce Type: cross  Abstract: This paper introduces Evalverse, a novel library that streamlines the evaluation of Large Language Models (LLMs) by unifying disparate evaluation tools into a single, user-friendly framework. Evalverse enables individuals with limited knowledge of artificial intelligence to easily request LLM evaluations and receive detailed reports, facilitated by an integration with communication platforms like Slack. Thus, Evalverse serves as a powerful tool for the comprehensive assessment of LLMs, offering both researchers and practitioners a centralized and easily accessible evaluation framework. Finally, we also provide a demo video for Evalverse, showcasing its capabilities and implementation in a two-minute format.
    
[^5]: sDPO：不要一次性使用您的数据

    sDPO: Don't Use Your Data All at Once

    [https://arxiv.org/abs/2403.19270](https://arxiv.org/abs/2403.19270)

    sDPO是对直接偏好优化方法的扩展，通过分步利用偏好数据集而非一次性使用，促进更精确对齐参考模型的使用，并训练出性能更优的最终模型，甚至胜过其他具有更多参数的流行大型语言模型。

    

    随着大型语言模型（LLM）的发展，将它们与人类偏好相一致变得日益重要。我们提出了分步DPO（sDPO），这是对最近流行的直接偏好优化（DPO）进行调整的一个扩展。这种方法涉及将可用的偏好数据集分割，并以分步方式利用它们，而不是一次性使用。我们演示了这种方法促进了更精确对齐参考模型在DPO训练框架内的使用。此外，sDPO训练最终模型的性能更好，甚至胜过拥有更多参数的其他流行LLM。

    arXiv:2403.19270v1 Announce Type: cross  Abstract: As development of large language models (LLM) progresses, aligning them with human preferences has become increasingly important. We propose stepwise DPO (sDPO), an extension of the recently popularized direct preference optimization (DPO) for alignment tuning. This approach involves dividing the available preference datasets and utilizing them in a stepwise manner, rather than employing it all at once. We demonstrate that this method facilitates the use of more precisely aligned reference models within the DPO training framework. Furthermore, sDPO trains the final model to be more performant, even outperforming other popular LLMs with more parameters.
    
[^6]: 具有正交锚点的对比学习（CLOA）

    Contrastive Learning with Orthonormal Anchors (CLOA)

    [https://arxiv.org/abs/2403.18699](https://arxiv.org/abs/2403.18699)

    该研究提出了一种新的损失函数称为正交锚点回归损失，用于解开嵌入聚类，显著增强嵌入的独特性

    

    本研究关注解决对比学习中普遍存在的不稳定性问题，特别是检查InfoNCE损失函数及其导数。我们揭示了一个关键观察，即这些损失函数表现出限制性行为，导致嵌入趋于融合为一个奇异点的收敛现象。这种“过度融合”效应对后续监督学习任务中的分类准确性产生不利影响。通过理论分析，我们证明了嵌入在等于或局限于秩-1线性子空间时表示InfoNCE的局部最小值。针对这一挑战，我们的研究提出了一种创新策略，利用与微调阶段典型使用的相同或更少的标记数据。我们提出的损失函数，即正交锚点回归损失，旨在解开嵌入聚类，显著增强每个嵌入的独特性。

    arXiv:2403.18699v1 Announce Type: cross  Abstract: This study focuses on addressing the instability issues prevalent in contrastive learning, specifically examining the InfoNCE loss function and its derivatives. We reveal a critical observation that these loss functions exhibit a restrictive behavior, leading to a convergence phenomenon where embeddings tend to merge into a singular point. This "over-fusion" effect detrimentally affects classification accuracy in subsequent supervised-learning tasks. Through theoretical analysis, we demonstrate that embeddings, when equalized or confined to a rank-1 linear subspace, represent a local minimum for InfoNCE. In response to this challenge, our research introduces an innovative strategy that leverages the same or fewer labeled data than typically used in the fine-tuning phase. The loss we proposed, Orthonormal Anchor Regression Loss, is designed to disentangle embedding clusters, significantly enhancing the distinctiveness of each embedding 
    
[^7]: TransFusion：具有变压器的对比学习

    TransFusion: Contrastive Learning with Transformers

    [https://arxiv.org/abs/2403.18681](https://arxiv.org/abs/2403.18681)

    TransFusion的主要创新在于定义了对比学习领域中的两个基本问题的理论极限，并成功实现了从复杂的现实世界数据中提取特征以改善分类精度。

    

    这篇论文提出了一个新的框架，TransFusion，旨在使对比学习的过程更具分析性和可解释性。 TransFusion由注意力块组成，其中的softmax被替换为ReLU，并且其最终块的加权和操作被截断，以使邻接矩阵成为输出。该模型通过最小化其输出与目标关联矩阵之间的Jensen-Shannon散度来进行训练，该矩阵指示每对样本是否属于相同类别或不同类别。 TransFusion的主要贡献在于定义了回答该领域两个基本问题的理论极限：数据增强的最大级别和有效对比学习所需的最小批量大小。 此外，实验结果表明，TransFusion成功地提取出能够从复杂的现实世界数据中分离集群的特征，从而提高了分类精度。

    arXiv:2403.18681v1 Announce Type: cross  Abstract: This paper proposes a novel framework, TransFusion, designed to make the process of contrastive learning more analytical and explainable. TransFusion consists of attention blocks whose softmax being replaced by ReLU, and its final block's weighted-sum operation is truncated to leave the adjacency matrix as the output. The model is trained by minimizing the Jensen-Shannon Divergence between its output and the target affinity matrix, which indicates whether each pair of samples belongs to the same or different classes. The main contribution of TransFusion lies in defining a theoretical limit for answering two fundamental questions in the field: the maximum level of data augmentation and the minimum batch size required for effective contrastive learning. Furthermore, experimental results indicate that TransFusion successfully extracts features that isolate clusters from complex real-world data, leading to improved classification accuracy 
    
[^8]: 用自校正组装网络纠正LEGO组装错误

    SCANet: Correcting LEGO Assembly Errors with Self-Correct Assembly Network

    [https://arxiv.org/abs/2403.18195](https://arxiv.org/abs/2403.18195)

    介绍了单步组装错误校正任务和LEGO错误校正组装数据集（LEGO-ECA），提出了用于这一任务的自校正组装网络（SCANet）。

    

    在机器人学和3D视觉中，自主组装面临着重大挑战，尤其是确保组装正确性。主流方法如MEPNet目前专注于基于手动提供的图像进行组件组装。然而，这些方法在需要长期规划的任务中往往难以取得满意的结果。在同一时间，我们观察到整合自校正模块可以在一定程度上缓解这些问题。受此问题启发，我们引入了单步组装错误校正任务，其中涉及识别和纠正组件组装错误。为支持这一领域的研究，我们提出了LEGO错误校正组装数据集（LEGO-ECA），包括用于组装步骤和组装失败实例的手动图像。此外，我们提出了自校正组装网络（SCANet），这是一种新颖的方法来解决这一任务。SCANet将组装的部件视为查询，

    arXiv:2403.18195v1 Announce Type: cross  Abstract: Autonomous assembly in robotics and 3D vision presents significant challenges, particularly in ensuring assembly correctness. Presently, predominant methods such as MEPNet focus on assembling components based on manually provided images. However, these approaches often fall short in achieving satisfactory results for tasks requiring long-term planning. Concurrently, we observe that integrating a self-correction module can partially alleviate such issues. Motivated by this concern, we introduce the single-step assembly error correction task, which involves identifying and rectifying misassembled components. To support research in this area, we present the LEGO Error Correction Assembly Dataset (LEGO-ECA), comprising manual images for assembly steps and instances of assembly failures. Additionally, we propose the Self-Correct Assembly Network (SCANet), a novel method to address this task. SCANet treats assembled components as queries, de
    
[^9]: MetaAligner：用于语言模型通用多目标对齐的条件从弱到强校正

    MetaAligner: Conditional Weak-to-Strong Correction for Generalizable Multi-Objective Alignment of Language Models

    [https://arxiv.org/abs/2403.17141](https://arxiv.org/abs/2403.17141)

    MetaAligner是第一个与策略无关且通用的多目标偏好对齐方法，通过将参数更新与策略模型解耦实现即插即用的对齐，并通过上下文学习实现未见目标的零冷启动偏好对齐

    

    近期大型语言模型（LLM）的进展旨在通过多目标偏好对齐来解决异质人类期望和价值观，然而，现有方法受到策略模型的参数限制，导致两个关键局限性：（1）它们的对齐算法对于每个新目标模型的重复成本很高；（2）由于其静态对齐目标，它们无法扩展到未见目标。在这项工作中，我们提出了Meta-Objective Aligner（MetaAligner），这是一种执行条件从弱到强校正以逼近强响应的模型。MetaAligner是第一个与策略无关且通用的多目标偏好对齐方法，它通过将参数更新与策略模型解耦实现即插即用的对齐，并通过上下文学习实现未见目标的零冷启动偏好对齐。实验结果表明，MetaAligner取得了显著

    arXiv:2403.17141v1 Announce Type: cross  Abstract: Recent advancements in large language models (LLMs) aim to tackle heterogeneous human expectations and values via multi-objective preference alignment. However, existing methods are parameter-adherent to the policy model, leading to two key limitations: (1) the high-cost repetition of their alignment algorithms for each new target model; (2) they cannot expand to unseen objectives due to their static alignment objectives. In this work, we propose Meta-Objective Aligner (MetaAligner), a model that performs conditional weak-to-strong correction for weak responses to approach strong responses. MetaAligner is the first policy-agnostic and generalizable method for multi-objective preference alignment, which enables plug-and-play alignment by decoupling parameter updates from the policy models and facilitates zero-shot preference alignment for unseen objectives via in-context learning. Experimental results show that MetaAligner achieves sign
    
[^10]: 图像操作的广义一致性轨迹模型

    Generalized Consistency Trajectory Models for Image Manipulation

    [https://arxiv.org/abs/2403.12510](https://arxiv.org/abs/2403.12510)

    本研究提出了广义一致性轨迹模型（GCTMs），能够在任何噪声分布和数据分布之间实现转换。

    

    基于扩散的生成模型在无条件生成以及图像编辑和恢复等应用任务中表现出色。扩散模型的成功在于扩散的迭代性质：扩散将将噪声到数据的复杂映射过程分解为一系列简单的去噪任务。此外，通过在每个去噪步骤中注入引导项，我们能够对生成过程进行精细控制。然而，迭代过程也常常计算密集，通常需要进行数十次甚至数千次函数评估。虽然一致性轨迹模型（CTMs）可以在概率流ODE（PFODE）上任意时间点之间进行遍历，并且通过单次函数评估进行得分推导，但CTMs仅允许从高斯噪声转换为数据。因此，本文旨在通过提出广义CTMs（GCTMs）来发挥CTMs的全部潜力，实现在任何噪声分布和数据分布之间进行转换。

    arXiv:2403.12510v1 Announce Type: cross  Abstract: Diffusion-based generative models excel in unconditional generation, as well as on applied tasks such as image editing and restoration. The success of diffusion models lies in the iterative nature of diffusion: diffusion breaks down the complex process of mapping noise to data into a sequence of simple denoising tasks. Moreover, we are able to exert fine-grained control over the generation process by injecting guidance terms into each denoising step. However, the iterative process is also computationally intensive, often taking from tens up to thousands of function evaluations. Although consistency trajectory models (CTMs) enable traversal between any time points along the probability flow ODE (PFODE) and score inference with a single function evaluation, CTMs only allow translation from Gaussian noise to data. Thus, this work aims to unlock the full potential of CTMs by proposing generalized CTMs (GCTMs), which translate between arbit
    
[^11]: HateCOT：通过大型语言模型进行泛化攻击性言论检测的解释增强数据集

    HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models

    [https://arxiv.org/abs/2403.11456](https://arxiv.org/abs/2403.11456)

    HateCOT数据集通过GPT-3.5-Turbo生成解释，将52,000个样本数据用于预训练模型，显著提升了在不同领域和任务下的攻击性内容检测效果。

    

    社交媒体的普及导致了对攻击性内容的可靠高效检测的需求，为了限制其有害影响。这导致了大量与检测攻击性内容相关的数据集和模型的出现。本文介绍了HateCOT，这是从多样化现有来源中抽取的5.2万个样本数据集，其中包含由GPT-3.5-Turbo和人工精心制作的解释。我们展示了在HateCOT上为攻击性内容检测预训练模型在零-shot和few-shot设置下显著改进了开源语言模型在三个基准数据集上的表现，尽管在领域和任务方面存在差异。

    arXiv:2403.11456v1 Announce Type: cross  Abstract: The ubiquitousness of social media has led to the need for reliable and efficient detection of offensive content to limit harmful effects. This has led to a proliferation of datasets and models related to detecting offensive content. While sophisticated models have attained strong performance on individual datasets, these models often do not generalize due to differences between how "offensive content" is conceptualized, and the resulting differences in how these datasets are labeled. In this paper, we introduce HateCOT, a dataset of 52,000 samples drawn from diverse existing sources with explanations generated by GPT-3.5-Turbo and human-curated. We show that pre-training models for the detection of offensive content on HateCOT significantly boots open-sourced Language Models on three benchmark datasets in both zero and few-shot settings, despite differences in domain and task.} We further find that HateCOT enables effective K-shot fin
    
[^12]: 具有不确定性和风险意识的部分可观测任务和运动规划

    Partially Observable Task and Motion Planning with Uncertainty and Risk Awareness

    [https://arxiv.org/abs/2403.10454](https://arxiv.org/abs/2403.10454)

    提出了一种具有不确定性和风险意识的TAMP策略（TAMPURA），能够高效地解决具有初始状态和动作结果不确定性的长时程规划问题，包括需要信息收集和避免不良和不可逆结果的问题。

    

    集成任务和运动规划（TAMP）已被证明是一种有价值的方法，用于解决通用的长时程机器人操纵和导航问题。然而，典型的TAMP问题公式化假设完全可观测和确定性动作效果。这些假设限制了规划者获取信息和做出具有风险意识的决策的能力。我们提出了一种具有不确定性和风险意识的TAMP策略（TAMPURA），能够高效地解决具有初始状态和动作结果不确定性的长时程规划问题，包括需要信息收集和避免不良和不可逆结果的问题。我们的规划者在抽象任务级别和连续控制器级别均在存在不确定性条件下进行推理。鉴于一组在基本动作空间中运行的闭环目标驱动控制器，并描述了它们的前提条件和潜在能力，

    arXiv:2403.10454v1 Announce Type: cross  Abstract: Integrated task and motion planning (TAMP) has proven to be a valuable approach to generalizable long-horizon robotic manipulation and navigation problems. However, the typical TAMP problem formulation assumes full observability and deterministic action effects. These assumptions limit the ability of the planner to gather information and make decisions that are risk-aware. We propose a strategy for TAMP with Uncertainty and Risk Awareness (TAMPURA) that is capable of efficiently solving long-horizon planning problems with initial-state and action outcome uncertainty, including problems that require information gathering and avoiding undesirable and irreversible outcomes. Our planner reasons under uncertainty at both the abstract task level and continuous controller level. Given a set of closed-loop goal-conditioned controllers operating in the primitive action space and a description of their preconditions and potential capabilities, w
    
[^13]: 在交互式机器人中嵌入动态角色: 伪装动画社交运动学（MASK）

    Towards Embedding Dynamic Personas in Interactive Robots: Masquerading Animated Social Kinematics (MASK)

    [https://arxiv.org/abs/2403.10041](https://arxiv.org/abs/2403.10041)

    该研究提出了一种名为MASK的机器人系统，通过非言语互动与观众进行互动，并利用有限状态机结构调整机器人行为，实现多种不同角色的动态表达。

    

    本文介绍了一种创新的交互式机器人系统的设计和开发，以增强观众参与度，使用类似角色的人物形象。基于以角色为驱动的对话代理系统，本研究将该代理应用扩展到了物理领域，利用机器人提供更具沉浸感和互动体验。提出的系统名为Masquerading Animated Social Kinematics (MASK)，利用类人机器人通过非言语互动与客人互动，包括面部表情和手势。一种基于有限状态机结构的行为生成系统有效地调整机器人行为以传达不同的人物角色。MASK框架集成了感知引擎、行为选择引擎和综合动作库，使其能够在行为设计中需要最少人工干预地实现实时、动态互动。在用户对象研究过程中，探讨了系统的效果，并展示了其潜力以激发未来研究的兴趣。

    arXiv:2403.10041v1 Announce Type: cross  Abstract: This paper presents the design and development of an innovative interactive robotic system to enhance audience engagement using character-like personas. Built upon the foundations of persona-driven dialog agents, this work extends the agent application to the physical realm, employing robots to provide a more immersive and interactive experience. The proposed system, named the Masquerading Animated Social Kinematics (MASK), leverages an anthropomorphic robot which interacts with guests using non-verbal interactions, including facial expressions and gestures. A behavior generation system based upon a finite-state machine structure effectively conditions robotic behavior to convey distinct personas. The MASK framework integrates a perception engine, a behavior selection engine, and a comprehensive action library to enable real-time, dynamic interactions with minimal human intervention in behavior design. Throughout the user subject studi
    
[^14]: 大型语言模型的概念知识编辑

    Editing Conceptual Knowledge for Large Language Models

    [https://arxiv.org/abs/2403.06259](https://arxiv.org/abs/2403.06259)

    该论文首次研究了为大型语言模型编辑概念知识，通过构建基准数据集和建立新评估指标，发现现有方法虽然能一定程度上修改概念定义，但也可能造成LLMs中相关实例知识的扭曲，导致性能下降。

    

    最近，对于大型语言模型（LLMs）的知识编辑引起了越来越多的关注。当前的方法和评估仅探讨了实例级别的编辑，然而LLMs是否具有修改概念的能力仍不清楚。本文首次研究了为LLMs编辑概念知识，通过构建一个新颖的基准数据集ConceptEdit并建立了一套新的评估指标。实验结果表明，尽管现有的编辑方法可以有效地在一定程度上修改概念级别的定义，但它们也有潜力扭曲LLMs中相关的实例知识，导致性能不佳。我们期望这可以激发对更好理解LLMs的进一步进展。我们的项目主页位于https://zjunlp.github.io/project/ConceptEdit。

    arXiv:2403.06259v1 Announce Type: cross  Abstract: Recently, there has been a growing interest in knowledge editing for Large Language Models (LLMs). Current approaches and evaluations merely explore the instance-level editing, while whether LLMs possess the capability to modify concepts remains unclear. This paper pioneers the investigation of editing conceptual knowledge for LLMs, by constructing a novel benchmark dataset ConceptEdit and establishing a suite of new metrics for evaluation. The experimental results reveal that, although existing editing methods can efficiently modify concept-level definition to some extent, they also have the potential to distort the related instantial knowledge in LLMs, leading to poor performance. We anticipate this can inspire further progress in better understanding LLMs. Our project homepage is available at https://zjunlp.github.io/project/ConceptEdit.
    
[^15]: RFWave：用于音频波形重建的多频带整流流动

    RFWave: Multi-band Rectified Flow for Audio Waveform Reconstruction

    [https://arxiv.org/abs/2403.05010](https://arxiv.org/abs/2403.05010)

    RFWave是一种新颖的多频带整流流动方法，可以从Mel频谱图中重建高保真度音频波形，仅需10个采样步骤即可实现出色的重建质量和优越的计算效率。

    

    最近生成建模的进展在从不同表示中重建音频波形方面取得了显著进展。虽然扩散模型已被用于重建音频波形，但由于它们在个别样本点级别进行操作并且需要相对较大数量的采样步骤，因此它们往往会出现延迟问题。在本研究中，我们介绍了RFWave，一种新颖的多频带整流流动方法，它从Mel频谱图中重建高保真度音频波形。RFWave在生成复杂频谱图并在帧级别运行方面具有独特性，同时处理所有子带以增强效率。由于希望获得平缓传输轨迹的整流流动，RFWave仅需10个采样步骤。实证评估表明，RFWave实现了卓越的重建质量和优越的计算效率，能够以更快的速度生成音频。

    arXiv:2403.05010v1 Announce Type: cross  Abstract: Recent advancements in generative modeling have led to significant progress in audio waveform reconstruction from diverse representations. Although diffusion models have been used for reconstructing audio waveforms, they tend to exhibit latency issues because they operate at the level of individual sample points and require a relatively large number of sampling steps. In this study, we introduce RFWave, a novel multi-band Rectified Flow approach that reconstructs high-fidelity audio waveforms from Mel-spectrograms. RFWave is distinctive for generating complex spectrograms and operating at the frame level, processing all subbands concurrently to enhance efficiency. Thanks to Rectified Flow, which aims for a flat transport trajectory, RFWave requires only 10 sampling steps. Empirical evaluations demonstrate that RFWave achieves exceptional reconstruction quality and superior computational efficiency, capable of generating audio at a spee
    
[^16]: 对于生成编程代码的大型语言模型进行系统评估

    A systematic evaluation of large language models for generating programming code

    [https://arxiv.org/abs/2403.00894](https://arxiv.org/abs/2403.00894)

    GPT-4在生成编程代码方面表现优异，特别是在选择最佳提示策略时，超过了其他大型语言模型和85%的人类参与者。

    

    我们系统评估了七个大型语言模型在使用不同提示策略、编程语言和任务难度生成编程代码时的性能。GPT-4在很大程度上优于其他大型语言模型，包括Gemini Ultra和Claude 2。GPT-4的编码性能随不同提示策略而变化。在本研究中评估的大多数LeetCode和GeeksforGeeks编程比赛中，采用最佳提示策略的GPT-4胜过85%的人类参与者。此外，GPT-4表现出在不同编程语言之间翻译代码和从过去错误中学习的强大能力。由GPT-4生成的代码的计算效率与人类程序员相当。这些结果表明，GPT-4有潜力成为在编程代码生成和软件开发中的可靠助手。

    arXiv:2403.00894v1 Announce Type: cross  Abstract: We systematically evaluated the performance of seven large language models in generating programming code using various prompt strategies, programming languages, and task difficulties. GPT-4 substantially outperforms other large language models, including Gemini Ultra and Claude 2. The coding performance of GPT-4 varies considerably with different prompt strategies. In most LeetCode and GeeksforGeeks coding contests evaluated in this study, GPT-4 employing the optimal prompt strategy outperforms 85 percent of human participants. Additionally, GPT-4 demonstrates strong capabilities in translating code between different programming languages and in learning from past errors. The computational efficiency of the code generated by GPT-4 is comparable to that of human programmers. These results suggest that GPT-4 has the potential to serve as a reliable assistant in programming code generation and software development.
    
[^17]: Tokenization超越了压缩

    Tokenization Is More Than Compression

    [https://arxiv.org/abs/2402.18376](https://arxiv.org/abs/2402.18376)

    通过引入新的分词器PathPiece，研究者发现少量标记并不能导致更好的下游性能，这一结果对于 Tokenization 的有效性理解提出了质疑。

    

    Tokenization是自然语言处理（NLP）任务中的基础步骤，它连接了原始文本和语言模型。现有的Tokenization方法，如字节对编码（Byte-Pair Encoding，BPE），源自数据压缩领域，并有人认为BPE的有效性源于其将文本压缩为相对较少的标记的能力。我们通过引入PathPiece来测试“更少的标记是否会导致更好的下游性能”这一假设，PathPiece是一种新的分词器，根据给定词汇将文档文本划分为最少数量的标记。通过广泛实验，我们发现这一假设并非成立，对有效Tokenization原因的理解产生了疑问。为了检查哪些其他因素起到作用，我们评估了Tokenization的所有三个阶段（预分词、词汇构造和分割）的设计决策，提供了关于设计的新见解。

    arXiv:2402.18376v1 Announce Type: cross  Abstract: Tokenization is a foundational step in Natural Language Processing (NLP) tasks, bridging raw text and language models. Existing tokenization approaches like Byte-Pair Encoding (BPE) originate from the field of data compression, and it has been suggested that the effectiveness of BPE stems from its ability to condense text into a relatively small number of tokens. We test the hypothesis that fewer tokens lead to better downstream performance by introducing PathPiece, a new tokenizer that segments a document's text into the minimum number of tokens for a given vocabulary. Through extensive experimentation we find this hypothesis not to be the case, casting doubt on the understanding of the reasons for effective tokenization. To examine which other factors play a role, we evaluate design decisions across all three phases of tokenization: pre-tokenization, vocabulary construction, and segmentation, offering new insights into the design of 
    
[^18]: 遵循我的指示并说出真相：来自检索增强生成系统的可扩展数据提取

    Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems

    [https://arxiv.org/abs/2402.17840](https://arxiv.org/abs/2402.17840)

    研究揭示了检索增强生成系统中的数据泄露风险，指出对手可以利用LMs的指示遵循能力轻松地从数据存储中直接提取文本数据，并设计了攻击对生产RAG模型GPTs造成数据存储泄漏。

    

    检索增强生成（RAG）通过在测试时将外部知识纳入预训练模型，从而实现定制适应，提升了模型性能。本研究探讨了Retrieval-In-Context RAG语言模型（LMs）中的数据泄露风险。我们展示了当对使用指令调整的LMs构建的RAG系统进行提示注入时，对手可以利用LMs的指示遵循能力轻松地从数据存储中直接提取文本数据。这种漏洞存在于覆盖Llama2、Mistral/Mixtral、Vicuna、SOLAR、WizardLM、Qwen1.5和Platypus2等多种现代LMs的广泛范围内，并且随着模型规模的扩大，利用能力加剧。将研究扩展到生产RAG模型GPTs，我们设计了一种攻击，可以在对25个随机选择的定制GPTs施加最多2个查询时以100%成功率导致数据存储泄漏，并且我们能够以77,000字的书籍中的文本数据的提取率为41%，以及在含有1,569,00词的语料库中的文本数据的提取率为3%。

    arXiv:2402.17840v1 Announce Type: cross  Abstract: Retrieval-Augmented Generation (RAG) improves pre-trained models by incorporating external knowledge at test time to enable customized adaptation. We study the risk of datastore leakage in Retrieval-In-Context RAG Language Models (LMs). We show that an adversary can exploit LMs' instruction-following capabilities to easily extract text data verbatim from the datastore of RAG systems built with instruction-tuned LMs via prompt injection. The vulnerability exists for a wide range of modern LMs that span Llama2, Mistral/Mixtral, Vicuna, SOLAR, WizardLM, Qwen1.5, and Platypus2, and the exploitability exacerbates as the model size scales up. Extending our study to production RAG models GPTs, we design an attack that can cause datastore leakage with a 100% success rate on 25 randomly selected customized GPTs with at most 2 queries, and we extract text data verbatim at a rate of 41% from a book of 77,000 words and 3% from a corpus of 1,569,00
    
[^19]: 通过突出潜在错误并建议纠正成功引导人类做出决策的不完美说明

    Successfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections

    [https://arxiv.org/abs/2402.16973](https://arxiv.org/abs/2402.16973)

    通过检测潜在幻觉并建议替代方案的通信机制，成功减少人类导航错误高达29%而不增加认知负担

    

    本文解决了利用不完美语言模型来在基于定位导航任务的背景下引导人类决策的挑战。我们展示了不完美的说明生成模型可以通过有效的通信机制来更成功地引导人类。我们构建的通信机制包括可以检测说明中潜在幻觉并建议实际替代方案的模型，以及一个直观的界面将该信息呈现给用户。我们展示了这种方法可以将人类导航错误降低高达29%，而不增加额外的认知负担。这一结果突显了将多样化的通信渠道整合到AI系统中来弥补其缺陷并增强其对人类的实用性的潜力。

    arXiv:2402.16973v1 Announce Type: new  Abstract: This paper addresses the challenge of leveraging imperfect language models to guide human decision-making in the context of a grounded navigation task. We show that an imperfect instruction generation model can be complemented with an effective communication mechanism to become more successful at guiding humans. The communication mechanism we build comprises models that can detect potential hallucinations in instructions and suggest practical alternatives, and an intuitive interface to present that information to users. We show that this approach reduces the human navigation error by up to 29% with no additional cognitive burden. This result underscores the potential of integrating diverse communication channels into AI systems to compensate for their imperfections and enhance their utility for humans.
    
[^20]: QuaCer-C：大型语言模型中知识理解的定量认证

    QuaCer-C: Quantitative Certification of Knowledge Comprehension in LLMs

    [https://arxiv.org/abs/2402.15929](https://arxiv.org/abs/2402.15929)

    本文提出了一种新颖的认证框架QuaCer-C，用于正式认证大型语言模型中知识理解的能力，证书定量化且包含高置信度的概率界限，研究发现，随着参数数量的增加，知识理解能力提高，Mistral模型在这一评估中表现不如其他模型。

    

    大型语言模型（LLMs）在多个基准测试中展现出令人印象深刻的表现。然而，传统研究并未对LLMs的表现提供正式的保证。本文提出了一种新颖的LLM认证框架QuaCer-C，我们在此对知名LLMs的知识理解能力进行正式认证。我们的证书是定量的 - 它们包括对目标LLM在任何相关知识理解提示上给出正确答案的概率的高置信度紧密界限。我们针对Llama、Vicuna和Mistral LLMs的证书表明，知识理解能力随参数数量的增加而提高，并且Mistral模型在这一评估中表现不如其他模型。

    arXiv:2402.15929v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated impressive performance on several benchmarks. However, traditional studies do not provide formal guarantees on the performance of LLMs. In this work, we propose a novel certification framework for LLM, QuaCer-C, wherein we formally certify the knowledge-comprehension capabilities of popular LLMs. Our certificates are quantitative - they consist of high-confidence, tight bounds on the probability that the target LLM gives the correct answer on any relevant knowledge comprehension prompt. Our certificates for the Llama, Vicuna, and Mistral LLMs indicate that the knowledge comprehension capability improves with an increase in the number of parameters and that the Mistral model is less performant than the rest in this evaluation.
    
[^21]: 嵌入线性动力学的神经网络用于长序列建模

    Linear Dynamics-embedded Neural Network for Long-Sequence Modeling

    [https://arxiv.org/abs/2402.15290](https://arxiv.org/abs/2402.15290)

    提出了一种名为嵌入线性动力学的神经网络（LDNN），通过引入连续状态空间模型的属性和优化策略，实现了在长序列任务中具有少量参数、灵活推断和高效训练，最终在长距离竞技场上取得了有效且领先的性能。

    

    由于现有模型在长序列建模中性能和计算效率之间的权衡成为瓶颈，受到控制理论中具有多输入多输出的连续状态空间模型（SSMs）启发，我们提出了一种名为嵌入线性动力学的神经网络（LDNN）的新型神经网络。 SSM的连续、离散和卷积属性使LDNN具有少量参数、灵活的推断和在长序列任务中高效训练的特点。 我们开发了两种有效策略，对角化和“解耦然后快速傅立叶变换（FFT）”，以将卷积的时间复杂度从$O(LNH\max\{L, N\})$降低到$O(LN\max\{H, \log L\})$。 我们通过双向非因果和多头设置进一步改进了LDNN，以适应更广泛的应用范围。 对长距离竞技场（LRA）的大量实验表明了LDNN的有效性和最先进的性能。

    arXiv:2402.15290v1 Announce Type: cross  Abstract: The trade-off between performance and computational efficiency in long-sequence modeling becomes a bottleneck for existing models. Inspired by the continuous state space models (SSMs) with multi-input and multi-output in control theory, we propose a new neural network called Linear Dynamics-embedded Neural Network (LDNN). SSMs' continuous, discrete, and convolutional properties enable LDNN to have few parameters, flexible inference, and efficient training in long-sequence tasks. Two efficient strategies, diagonalization and $'\text{Disentanglement then Fast Fourier Transform (FFT)}'$, are developed to reduce the time complexity of convolution from $O(LNH\max\{L, N\})$ to $O(LN\max \{H, \log L\})$. We further improve LDNN through bidirectional noncausal and multi-head settings to accommodate a broader range of applications. Extensive experiments on the Long Range Arena (LRA) demonstrate the effectiveness and state-of-the-art performance
    
[^22]: 电子商务中意图理解的使用中心视角

    A Usage-centric Take on Intent Understanding in E-Commerce

    [https://arxiv.org/abs/2402.14901](https://arxiv.org/abs/2402.14901)

    该论文提出了电子商务中意图理解的一个新视角，不依赖于产品本体，通过引入产品恢复基准验证了当前意图知识图的弱点。

    

    识别和理解用户意图是电子商务中至关重要的任务。尽管意图理解很受欢迎，但其定义并不一致，且缺乏准确的基准。本文关注将用户意图定义为"顾客如何使用产品"的预测性用户意图，并将意图理解视为一项自然语言推理任务，独立于产品本体。我们发现了FolkScope的两个弱点，这是目前最先进的电子商务意图知识图，限制了其推理用户意图和推荐多样有用产品的能力。基于这些观察，我们引入了一个产品恢复基准，包括一个新颖的评估框架和一个示例数据集。我们在这个基准上进一步验证了上述FolkScope的弱点。

    arXiv:2402.14901v1 Announce Type: cross  Abstract: Identifying and understanding user intents is a pivotal task for E-Commerce. Despite its popularity, intent understanding has not been consistently defined or accurately benchmarked. In this paper, we focus on predicative user intents as "how a customer uses a product", and pose intent understanding as a natural language reasoning task, independent of product ontologies. We identify two weaknesses of FolkScope, the SOTA E-Commerce Intent Knowledge Graph, that limit its capacity to reason about user intents and to recommend diverse useful products. Following these observations, we introduce a Product Recovery Benchmark including a novel evaluation framework and an example dataset. We further validate the above FolkScope weaknesses on this benchmark.
    
[^23]: 排列、修复和改进：通过基于内容的控制实现可操控的长期音乐音频生成和编辑

    Arrange, Inpaint, and Refine: Steerable Long-term Music Audio Generation and Editing via Content-based Controls

    [https://arxiv.org/abs/2402.09508](https://arxiv.org/abs/2402.09508)

    通过引入参数高效微调（PEFT）方法，本研究实现了自回归语言模型在音乐修复和音乐排列任务中的应用。在多个音乐编辑任务中，该方法展示了有希望的结果，并为未来的AI驱动音乐编辑工具提供了更灵活的控制。

    

    可控音乐生成在人机音乐共创中起着重要作用。虽然大型语言模型（LLM）在生成高质量音乐方面表现出了潜力，但它们对自回归生成的依赖限制了它们在音乐编辑任务中的实用性。为了弥合这一差距，我们引入了一种新颖的参数高效微调（PEFT）方法。该方法使自回归语言模型能够无缝地解决音乐修复任务。此外，我们的PEFT方法集成了基于帧级内容的控制，促进了轨道条件音乐的精炼和分数条件音乐的排列。我们将该方法应用于MusicGen，一个领先的自回归音乐生成模型的微调。我们的实验在多个音乐编辑任务中展示了有希望的结果，为未来的AI驱动音乐编辑工具提供了更灵活的控制。

    arXiv:2402.09508v1 Announce Type: cross  Abstract: Controllable music generation plays a vital role in human-AI music co-creation. While Large Language Models (LLMs) have shown promise in generating high-quality music, their focus on autoregressive generation limits their utility in music editing tasks. To bridge this gap, we introduce a novel Parameter-Efficient Fine-Tuning (PEFT) method. This approach enables autoregressive language models to seamlessly address music inpainting tasks. Additionally, our PEFT method integrates frame-level content-based controls, facilitating track-conditioned music refinement and score-conditioned music arrangement. We apply this method to fine-tune MusicGen, a leading autoregressive music generation model. Our experiments demonstrate promising results across multiple music editing tasks, offering more flexible controls for future AI-driven music editing tools. A demo page\footnote{\url{https://kikyo-16.github.io/AIR/}.} showcasing our work and source 
    
[^24]: UrbanKGent：用于城市知识图谱构建的统一大型语言模型代理框架

    UrbanKGent: A Unified Large Language Model Agent Framework for Urban Knowledge Graph Construction

    [https://arxiv.org/abs/2402.06861](https://arxiv.org/abs/2402.06861)

    UrbanKGent是一个用于城市知识图谱构建的统一大型语言模型代理框架，通过异构感知和地理空间注入构建知识化指令集，并通过迭代轨迹细化模块来提升轨迹的质量。在两个真实世界的数据集上进行的评估表明了UrbanKGent的有效性和性能。

    

    城市知识图谱最近作为一种新兴的构建模块，在多源城市数据中提取关键知识，用于各种城市应用场景。尽管具有潜在的好处，但城市知识图谱构建仍然严重依赖手动工作，阻碍了其潜在的进展。本文提出了UrbanKGent，一种统一的大型语言模型代理框架，用于城市知识图谱构建。具体而言，我们首先通过异构感知和地理空间注入来构建UrbanKGC任务（如关系三元组提取和知识图谱补全）的知识化指令集。此外，我们提出了一种工具增强的迭代轨迹细化模块，以增强和优化从GPT-4中提取的轨迹。通过在Llama-2-13B上进行增强轨迹的混合指令微调，我们获得了UrbanKGC代理UrbanKGent-13B。我们对两个真实世界的数据集进行了全面评估。

    Urban knowledge graph has recently worked as an emerging building block to distill critical knowledge from multi-sourced urban data for diverse urban application scenarios. Despite its promising benefits, urban knowledge graph construction (UrbanKGC) still heavily relies on manual effort, hindering its potential advancement. This paper presents UrbanKGent, a unified large language model agent framework, for urban knowledge graph construction. Specifically, we first construct the knowledgeable instruction set for UrbanKGC tasks (such as relational triplet extraction and knowledge graph completion) via heterogeneity-aware and geospatial-infused instruction generation. Moreover, we propose a tool-augmented iterative trajectory refinement module to enhance and refine the trajectories distilled from GPT-4. Through hybrid instruction fine-tuning with augmented trajectories on Llama-2-13B, we obtain the UrbanKGC agent, UrbanKGent-13B. We perform a comprehensive evaluation on two real-world da
    
[^25]: 学习对比特征表示来进行面部动作单元检测

    Learning Contrastive Feature Representations for Facial Action Unit Detection

    [https://arxiv.org/abs/2402.06165](https://arxiv.org/abs/2402.06165)

    这项研究提出了一种对比学习框架，通过监督和自监督信号来增强面部动作单元检测模型的性能。采用正样本抽样和权衡重要性的损失函数来应对噪声AU标签和AU类型分布不平衡的挑战。

    

    面部动作单元（AU）检测的主要方法涉及监督的多标签二进制分类问题。现有的方法常常对AU的像素级信息进行编码，从而对模型的复杂性和表达能力提出了很大的要求。此外，由于存在噪声AU标签，这种做法增加了过拟合的风险。在本研究中，我们引入了一个对比学习框架，通过监督和自监督信号增强。目标是在AU检测领域中摆脱传统的像素级学习范式，获得判别特征。为了应对噪声AU标签带来的挑战，我们通过引入自监督信号来增强监督信号。这种增强是通过正样本抽样实现的，包括三种不同类型的正样本对。另外，为了减轻每个AU类型的分布不平衡问题，我们采用了一种权衡重要性的损失函数。

    The predominant approach to facial action unit (AU) detection revolves around a supervised multi-label binary classification problem. Existing methodologies often encode pixel-level information of AUs, thereby imposing substantial demands on model complexity and expressiveness. Moreover, this practice elevates the susceptibility to overfitting due to the presence of noisy AU labels. In the present study, we introduce a contrastive learning framework enhanced by both supervised and self-supervised signals. The objective is to acquire discriminative features, deviating from the conventional pixel-level learning paradigm within the domain of AU detection. To address the challenge posed by noisy AU labels, we augment the supervised signal through the introduction of a self-supervised signal. This augmentation is achieved through positive sample sampling, encompassing three distinct types of positive sample pairs. Furthermore, to mitigate the imbalanced distribution of each AU type, we empl
    
[^26]: 无偏好对齐学习与正则化相关奖励

    Preference-free Alignment Learning with Regularized Relevance Reward

    [https://arxiv.org/abs/2402.03469](https://arxiv.org/abs/2402.03469)

    无偏好对齐学习使用正则化相关奖励作为关键目标，在提供稳健奖励信号的同时，显著提高了偏好基准测试的性能。

    

    从人类偏好中学习被认为是将大规模语言模型（LLMs）与人类价值观对齐的关键。然而，与普遍的观点相反，我们的初步研究发现，基于人类偏好数据集训练的奖励模型倾向于给长的与主题无关的回复更高的分数，而给短的与主题相关的回复较低分。在这一观察的驱动下，我们探索了一种无偏好的方法，利用“相关性”作为对齐的一个关键目标。在我们的第一次尝试中，我们发现仅仅通过检索得到的相关性得分容易受到奖励欺骗的影响，即过度优化到不期望的捷径上，当我们将该得分作为奖励用于强化学习。为了缓解这个问题，我们将有效的归纳偏差整合到常规的相关性中，互相正则化，形成了一种混合奖励函数：正则化相关奖励（$R^3$）。$R^3$通过提供稳健的奖励信号，显著提高了在偏好基准测试中的性能。值得注意的是，$R^3$不需要

    Learning from human preference has been considered key to aligning Large Language Models (LLMs) with human values. However, contrary to popular belief, our preliminary study reveals that reward models trained on human preference datasets tend to give higher scores to long off-topic responses than short on-topic ones. Motivated by this observation, we explore a preference-free approach utilizing `relevance' as a key objective for alignment. On our first attempt, we find that the relevance score obtained by a retriever alone is vulnerable to reward hacking, i.e., overoptimizing to undesired shortcuts, when we utilize the score as a reward for reinforcement learning. To mitigate it, we integrate effective inductive biases into the vanilla relevance to regularize each other, resulting in a mixture of reward functions: Regularized Relevance Reward ($R^3$). $R^3$ significantly improves performance on preference benchmarks by providing a robust reward signal. Notably, $R^3$ does not require a
    
[^27]: GPT 模型的对话重构攻击

    Conversation Reconstruction Attack Against GPT Models

    [https://arxiv.org/abs/2402.02987](https://arxiv.org/abs/2402.02987)

    本文介绍了一种针对 GPT 模型的对话重构攻击，该攻击具有劫持会话和重构对话的两个步骤。通过对该攻击对 GPT 模型的隐私风险进行评估，发现 GPT-4 对该攻击具有一定的鲁棒性。

    

    最近，在大型语言模型（LLM）领域取得了重要进展，其中 GPT 系列模型代表着最具代表性的成果。为了优化任务执行，用户经常与托管在云环境中的 GPT 模型进行多轮对话。这些多轮对话往往包含私人信息，需要在云中进行传输和存储。然而，这种操作模式引入了额外的攻击面。本文首先介绍了一种针对 GPT 模型的特定对话重构攻击。我们提出的对话重构攻击由两个步骤组成：劫持会话和重构对话。随后，我们对当 GPT 模型遭受该攻击时对话中固有的隐私风险进行了详尽评估。然而，GPT-4 对于该攻击具有一定的鲁棒性。接着，我们引入了两种高级攻击，旨在更好地重构以前的对话。

    In recent times, significant advancements have been made in the field of large language models (LLMs), represented by GPT series models. To optimize task execution, users often engage in multi-round conversations with GPT models hosted in cloud environments. These multi-round conversations, potentially replete with private information, require transmission and storage within the cloud. However, this operational paradigm introduces additional attack surfaces. In this paper, we first introduce a specific Conversation Reconstruction Attack targeting GPT models. Our introduced Conversation Reconstruction Attack is composed of two steps: hijacking a session and reconstructing the conversations. Subsequently, we offer an exhaustive evaluation of the privacy risks inherent in conversations when GPT models are subjected to the proposed attack. However, GPT-4 demonstrates certain robustness to the proposed attacks. We then introduce two advanced attacks aimed at better reconstructing previous c
    
[^28]: 大型语言模型存在地理偏见

    Large Language Models are Geographically Biased

    [https://arxiv.org/abs/2402.02680](https://arxiv.org/abs/2402.02680)

    本文研究了大型语言模型的地理偏见，并展示了其对地理空间预测的系统错误，通过零射击地理空间预测来评估其对世界的认知。

    

    大型语言模型（LLMs）内在地含有其训练语料库中的偏见，这可能导致社会伤害的持续存在。随着这些基础模型的影响力不断增长，理解和评估它们的偏见对于实现公正和准确性至关重要。本文提出通过地理视角研究LLMs对我们所生活的世界的认知。这种方法特别强大，因为对人类生活中诸多与地理空间相关的方面（如文化、种族、语言、政治和宗教）有着明显的真实性。我们展示了各种问题地理偏见，我们将其定义为地理空间预测中的系统错误。首先，我们证明LLMs能够进行精确的零射击地理空间预测，以评级的形式呈现，其与真实情况之间呈现出强烈的单调相关性（Spearman's ρ最高可达0.89）。然后，我们展示了LLMs在多个客观和子领域上表现出共同的偏见。

    Large Language Models (LLMs) inherently carry the biases contained in their training corpora, which can lead to the perpetuation of societal harm. As the impact of these foundation models grows, understanding and evaluating their biases becomes crucial to achieving fairness and accuracy. We propose to study what LLMs know about the world we live in through the lens of geography. This approach is particularly powerful as there is ground truth for the numerous aspects of human life that are meaningfully projected onto geographic space such as culture, race, language, politics, and religion. We show various problematic geographic biases, which we define as systemic errors in geospatial predictions. Initially, we demonstrate that LLMs are capable of making accurate zero-shot geospatial predictions in the form of ratings that show strong monotonic correlation with ground truth (Spearman's $\rho$ of up to 0.89). We then show that LLMs exhibit common biases across a range of objective and sub
    
[^29]: 使用回声状态网络的多智能体强化学习及其在行人动态中的应用

    Multi-agent reinforcement learning using echo-state network and its application to pedestrian dynamics

    [https://arxiv.org/abs/2312.11834](https://arxiv.org/abs/2312.11834)

    通过使用回声状态网络将行人实现为MARL代理，研究了他们学习避让其他代理向前移动的能力，在密度不太高的情况下取得成功。

    

    近年来，研究使用多智能体强化学习（MARL）模拟行人。本研究考虑了网格世界环境中的道路，并将行人实现为使用回声状态网络和最小二乘策略迭代方法的MARL代理。在这个环境下，研究了这些代理学习避开其他代理向前移动的能力。具体而言，我们考虑了两种任务：窄直接路径和宽绕道之间的选择，以及走廊中的双向行人流。模拟结果表明，当代理密度不太高时，学习是成功的。

    arXiv:2312.11834v2 Announce Type: replace-cross  Abstract: In recent years, simulations of pedestrians using the multi-agent reinforcement learning (MARL) have been studied. This study considered the roads on a grid-world environment, and implemented pedestrians as MARL agents using an echo-state network and the least squares policy iteration method. Under this environment, the ability of these agents to learn to move forward by avoiding other agents was investigated. Specifically, we considered two types of tasks: the choice between a narrow direct route and a broad detour, and the bidirectional pedestrian flow in a corridor. The simulations results indicated that the learning was successful when the density of the agents was not that high.
    
[^30]: 关于大型语言模型对齐多样化偏好的研究

    On Diversified Preferences of Large Language Model Alignment

    [https://arxiv.org/abs/2312.07401](https://arxiv.org/abs/2312.07401)

    本文通过定量分析常用人类反馈数据集，揭示了多样化偏好对奖励建模的影响，提出了一种新颖的多目标奖励学习方法以增强校准性能

    

    将大型语言模型（LLMs）与人类偏好对齐被认为是提高LLMs交互质量的关键。然而，在这个多元化的世界中，由于标注者的不同偏好，人类偏好可能会多样化，这阻碍了LLM对齐方法的有效性。本文首次对常用人类反馈数据集进行定量分析，以研究多样化偏好对奖励建模的影响。我们的分析揭示了奖励模型（RMs）的校准性能与LLMs的对齐性能之间的相关性。我们发现多样化偏好数据对人类共享偏好（如“无害和有帮助”）上的奖励模型的校准性能产生负面影响，从而损害了LLMs的对齐性能。为解决这种无效性，我们提出了一种新颖的多目标奖励学习方法（MORE）以增强校准性能。

    arXiv:2312.07401v3 Announce Type: replace  Abstract: Aligning large language models (LLMs) with human preferences has been recognized as the key to improving LLMs' interaction quality. However, in this pluralistic world, human preferences can be diversified due to annotators' different tastes, which hinders the effectiveness of LLM alignment methods. This paper presents the first quantitative analysis of commonly used human feedback datasets to investigate the impact of diversified preferences on reward modeling. Our analysis reveals a correlation between the calibration performance of reward models (RMs) and the alignment performance of LLMs. We find that diversified preference data negatively affect the calibration performance of RMs on human-shared preferences, such as \textit{Harmless\&Helpful}, thereby impairing the alignment performance of LLMs. To address the ineffectiveness, we propose a novel Multi-Objective Reward learning method (MORE) to enhance the calibration performance 
    
[^31]: 利用指数尺度的深度强化ReLU网络初始化和训练

    Compelling ReLU Network Initialization and Training to Leverage Exponential Scaling with Depth

    [https://arxiv.org/abs/2311.18022](https://arxiv.org/abs/2311.18022)

    该论文提出了一种新的训练策略，通过重新参数化网络权重，使得神经网络的指数数量的激活模式得以展现，从而得到远远超过随机初始化的结果。

    

    ReLU激活的神经网络可以看作是分段线性函数的组合。对于这样的网络，随着深度的增加，表达在输入域上的不同线性区域的数量有可能以指数级增长，但当初始参数选择随机时，不太可能出现这种情况。这种不良的尺度能够导致即使是简单函数也需要使用过大的模型来近似。为了解决这个问题，我们引入了一种新的训练策略：首先以一种方式重新参数化网络权重，使得指数数量的激活模式得以展现。在这些新参数上进行训练可以得到一个初始解，稍后通过更新底层模型权重来改进。这种方法使我们能够产生比随机初始化对应的函数逼近好几个数量级的结果。

    A neural network with ReLU activations may be viewed as a composition of piecewise linear functions. For such networks, the number of distinct linear regions expressed over the input domain has the potential to scale exponentially with depth, but it is not expected to do so when the initial parameters are chosen randomly. This poor scaling can necessitate the use of overly large models to approximate even simple functions. To address this issue, we introduce a novel training strategy: we first reparameterize the network weights in a manner that forces an exponential number of activation patterns to manifest. Training first on these new parameters provides an initial solution that can later be refined by updating the underlying model weights. This approach allows us to produce function approximations that are several orders of magnitude better than their randomly initialized counterparts.
    
[^32]: 用医学教科书增强黑盒LLMs进行临床问题回答

    Augmenting Black-box LLMs with Medical Textbooks for Clinical Question Answering

    [https://arxiv.org/abs/2309.02233](https://arxiv.org/abs/2309.02233)

    该研究提出了一种名为LLMs增强医学教科书（LLM-AMT）的系统，通过插入式模块将权威医学教科书集成到LLMs的框架中，显著提高了LLMs在专业领域的能力。

    

    大规模语言模型（LLMs）如ChatGPT已经展示出根据人类指令生成响应的印象能力。然而，由于它们缺乏特定、深入的知识，它们在医学领域的应用可能具有挑战性。在这项研究中，我们提出了一种名为LLMs增强医学教科书（LLM-AMT）的系统，旨在增强LLMs在专业领域的能力。LLM-AMT通过插入式模块将权威医学教科书集成到LLMs的框架中。这些模块包括一个查询增强器、一个混合教科书检索器和一个知识自我完善。它们共同整合权威医学知识。此外，一个LLMs阅读器有助于上下文理解。我们在三个医学问答任务上的实验结果表明，LLMAMT显著提高了响应质量，准确率提高了11.6%到16.6%。值得注意的是，以GPT-4-Turbo为基础模型

    arXiv:2309.02233v2 Announce Type: replace-cross  Abstract: Large-scale language models (LLMs) like ChatGPT have demonstrated impressive abilities in generating responses based on human instructions. However, their use in the medical field can be challenging due to their lack of specific, in-depth knowledge. In this study, we present a system called LLMs Augmented with Medical Textbooks (LLM-AMT) designed to enhance the proficiency of LLMs in specialized domains. LLM-AMT integrates authoritative medical textbooks into the LLMs' framework using plug-and-play modules. These modules include a Query Augmenter, a Hybrid Textbook Retriever, and a Knowledge Self-Refiner. Together, they incorporate authoritative medical knowledge. Additionally, an LLM Reader aids in contextual understanding. Our experimental results on three medical QA tasks demonstrate that LLMAMT significantly improves response quality, with accuracy gains ranging from 11.6% to 16.6%. Notably, with GPT-4-Turbo as the base mod
    
[^33]: R-Judge: 评估LLM代理的安全风险意识的基准测试

    R-Judge: Benchmarking Safety Risk Awareness for LLM Agents. (arXiv:2401.10019v1 [cs.CL])

    [http://arxiv.org/abs/2401.10019](http://arxiv.org/abs/2401.10019)

    这篇论文主要介绍了一种评估LLM代理在不同环境中判断安全风险能力的基准测试R-Judge，通过对162个代理交互记录进行评估，发现GPT-4模型表现最佳，达到了72.29%的准确率。

    

    大型语言模型（LLM）在自动完成各种真实世界应用任务方面展现出巨大潜力。然而，这些LLM代理在交互环境中操作时会引入意外的安全风险。与大多数之前的研究集中在LLM生成内容的安全性不同，本研究关注评估LLM代理在不同环境中的行为安全性的迫切需求。我们介绍了一个名为R-Judge的基准测试，用于评估LLM在给定代理交互记录时判断安全风险的能力。R-Judge包括162个代理交互记录，涵盖7个应用领域和10种风险类型的27个关键风险场景。它结合了人类对安全性的共识，并具有标记的安全风险标签和高质量的风险描述。利用R-Judge，我们对8种常用作代理骨干的著名LLM模型进行了全面评估。表现最好的模型GPT-4实现了72.29%的对比结果。

    Large language models (LLMs) have exhibited great potential in autonomously completing tasks across real-world applications. Despite this, these LLM agents introduce unexpected safety risks when operating in interactive environments. Instead of centering on LLM-generated content safety in most prior studies, this work addresses the imperative need for benchmarking the behavioral safety of LLM agents within diverse environments. We introduce R-Judge, a benchmark crafted to evaluate the proficiency of LLMs in judging safety risks given agent interaction records. R-Judge comprises 162 agent interaction records, encompassing 27 key risk scenarios among 7 application categories and 10 risk types. It incorporates human consensus on safety with annotated safety risk labels and high-quality risk descriptions. Utilizing R-Judge, we conduct a comprehensive evaluation of 8 prominent LLMs commonly employed as the backbone for agents. The best-performing model, GPT-4, achieves 72.29% in contrast to
    
[^34]: SH2: 自我突出式犹豫帮助您更准确解码。

    SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully. (arXiv:2401.05930v1 [cs.CL])

    [http://arxiv.org/abs/2401.05930](http://arxiv.org/abs/2401.05930)

    自我突出式犹豫（SH2）是一种推理时的方法，通过选择预测概率较低的标记，并强调它们的差异，从而帮助语言模型更准确地解码。

    

    大型语言模型(LLMs)在文本生成方面表现出色。然而，LLMs仍然存在幻觉问题。在本研究中，我们提出了一种推理时方法，即自我突出式犹豫(SH2)，以帮助LLMs更准确地解码。SH2基于信息理论中一个简单的事实，即对于LLMs而言，预测概率较低的标记往往更具信息量。我们的分析表明，LLMs给予较低概率的标记更有可能与事实信息（如名词、专有名词和形容词）密切相关。因此，我们提出通过选择概率最低的标记并将其连接到原始上下文中来“突出”事实信息，从而迫使模型在生成之前多次阅读和犹豫这些标记。在解码过程中，我们还采用对比解码的方式来强调由犹豫带来的输出概率的差异。

    Large language models (LLMs) demonstrate great performance in text generation. However, LLMs are still suffering from hallucinations. In this work, we propose an inference-time method, Self-Highlighted Hesitation (SH2), to help LLMs decode more truthfully. SH2 is based on a simple fact rooted in information theory that for an LLM, the tokens predicted with lower probabilities are prone to be more informative than others. Our analysis shows that the tokens assigned with lower probabilities by an LLM are more likely to be closely related to factual information, such as nouns, proper nouns, and adjectives. Therefore, we propose to ''highlight'' the factual information by selecting the tokens with the lowest probabilities and concatenating them to the original context, thus forcing the model to repeatedly read and hesitate on these tokens before generation. During decoding, we also adopt contrastive decoding to emphasize the difference in the output probabilities brought by the hesitation.
    
[^35]: 计算医疗中的数据中心基础模型：一项调查

    Data-Centric Foundation Models in Computational Healthcare: A Survey. (arXiv:2401.02458v1 [cs.LG])

    [http://arxiv.org/abs/2401.02458](http://arxiv.org/abs/2401.02458)

    计算医疗中的数据中心基础模型是一项调查研究，为医疗工作流程的改进提供了基于数据的人工智能方法，并讨论了安全性、评估和与人类价值观的一致性。基于FM的分析有望提高患者结果和临床工作流程表现。

    

    作为一套新兴的人工智能技术，基础模型（FMs）的出现为计算医疗带来了一系列机遇。这些模型的交互性由预训练数据和人类指令引导，引发了一个数据中心的人工智能范式，强调更好的数据表征、质量和规模。在医疗人工智能领域，获取和处理高质量的临床数据记录一直是一个长期存在的挑战，涉及数据数量、注释、患者隐私和伦理等方面。在这项调查中，我们研究了FM时代的广泛数据中心方法（从模型预训练到推理），以改进医疗工作流程。我们讨论了人工智能安全性、评估以及与人类价值观的一致性的关键观点。最后，我们展望了基于FM的分析在医疗和医药领域不断发展的格局中提高患者结果和临床工作流程表现的前景。我们提供了一个最新的医疗清单。

    The advent of foundation models (FMs) as an emerging suite of AI techniques has struck a wave of opportunities in computational healthcare. The interactive nature of these models, guided by pre-training data and human instructions, has ignited a data-centric AI paradigm that emphasizes better data characterization, quality, and scale. In healthcare AI, obtaining and processing high-quality clinical data records has been a longstanding challenge, ranging from data quantity, annotation, patient privacy, and ethics. In this survey, we investigate a wide range of data-centric approaches in the FM era (from model pre-training to inference) towards improving the healthcare workflow. We discuss key perspectives in AI security, assessment, and alignment with human values. Finally, we offer a promising outlook of FM-based analytics to enhance the performance of patient outcome and clinical workflow in the evolving landscape of healthcare and medicine. We provide an up-to-date list of healthcare
    
[^36]: 训练的力量：不同的神经网络设置对能源需求的影响

    The Power of Training: How Different Neural Network Setups Influence the Energy Demand. (arXiv:2401.01851v1 [cs.LG])

    [http://arxiv.org/abs/2401.01851](http://arxiv.org/abs/2401.01851)

    本文研究了机器学习训练方案和学习范式对能源消耗的影响，并探讨了预训练和多任务训练在可持续机器学习方面的潜力。

    

    本研究探讨机器学习训练方案和学习范式的变化对相应能源消耗的影响。虽然数据的可用性提高和高性能硬件的创新推动了复杂模型的训练，但也支持了能源消耗和碳排放的消隐。因此，本研究的目标是增加人们对一般训练参数和过程（从学习率到批量大小再到知识传输）的能源影响的认识。使用不同的超参数初始化在两种不同的硬件配置上评估多种设置，以获得有意义的结果。在基准结果上进行了预训练和多任务训练实验，以确定它们对可持续机器学习的潜力。

    This work examines the effects of variations in machine learning training regimes and learning paradigms on the corresponding energy consumption. While increasing data availability and innovation in high-performance hardware fuels the training of sophisticated models, it also supports the fading perception of energy consumption and carbon emission. Therefore, the goal of this work is to create awareness about the energy impact of general training parameters and processes, from learning rate over batch size to knowledge transfer. Multiple setups with different hyperparameter initializations are evaluated on two different hardware configurations to obtain meaningful results. Experiments on pretraining and multitask training are conducted on top of the baseline results to determine their potential towards sustainable machine learning.
    
[^37]: 学会说母语：以母语风格激发大型语言模型的能力

    Speak Like a Native: Prompting Large Language Models in a Native Style. (arXiv:2311.13538v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2311.13538](http://arxiv.org/abs/2311.13538)

    本文提出了一种名为AlignedCoT的新颖有效方法，通过将上下文示例与大型语言模型（LLMs）的母语风格对齐，提高了LLMs的推理能力和性能。

    

    大型语言模型（LLMs）与上下文学习（ICL）已成为许多自然语言处理任务的现代工具选择。然而，上下文示例的文本风格如何影响LLMs的性能仍然不足。本文提出了一种名为AlignedCoT的新颖有效的方法，通过将上下文示例与LLMs的母语风格对齐来提高LLMs的推理能力。 "母语"是指LLMs的固有特征，可以通过零-shot场景探测。 AlignedCoT广泛适用于ICL方法，可以轻松与最先进的技术结合，进一步提高LLMs的性能。我们在数学问答、常识推理和文本理解等多个基准测试上进行了广泛而全面的实验。实证结果表明，我们的AlignedCoT相比精心手工制作的演示文稿显著提高了性能。

    In-context learning (ICL) with large language models (LLMs) has become the modern tools of choice for many natural language processing tasks. However, how the text style of in-context examples influences the performance of LLMs still remains under-explored. This paper presents a novel and effective approach, named \textbf{AlignedCoT}, to improve the reasoning capability of LLMs by aligning the in-context examples with the native style of LLMs.''Native'' refers to the inherent characteristic of LLMs which can be probed by zero-shot scenarios.AlignedCoT is widely applicable to ICL methods, making it easy to combine with state-of-the-art techniques to further improve the LLMs' performance. We conduct extensive and comprehensive experiments on several benchmarks on mathematical question-answering, common-sense reasoning, and text understanding. The empirical results demonstrate that our AlignedCoT significantly improves performance over the carefully handcrafted demonstrations. Specificall
    
[^38]: NOD-TAMP:多步骤操纵规划中的神经物体描述符

    NOD-TAMP: Multi-Step Manipulation Planning with Neural Object Descriptors. (arXiv:2311.01530v1 [cs.RO])

    [http://arxiv.org/abs/2311.01530](http://arxiv.org/abs/2311.01530)

    NOD-TAMP是一个基于TAMP的框架，利用神经物体描述符来解决复杂操纵任务中的泛化问题，通过从少量人类演示中提取轨迹并进行调整，有效解决了长时程任务的挑战，并在模拟环境中优于现有方法。

    

    在家居和工厂环境中开发复杂操纵任务的智能机器人仍然具有挑战性，因为长时程任务、接触丰富的操纵以及需要在各种物体形状和场景布局之间进行泛化。虽然任务和运动规划（TAMP）提供了一个有希望的解决方案，但是它的假设，如动力学模型，限制了它在新颖背景中的适应性。神经物体描述符（NODs）在物体和场景泛化方面显示出了潜力，但在处理更广泛任务方面存在局限性。我们提出的基于TAMP的框架NOD-TAMP从少数人类演示中提取短的操纵轨迹，使用NOD特征来调整这些轨迹，并组合它们来解决广泛的长时程任务。在模拟环境中验证后，NOD-TAMP有效应对各种挑战，优于现有方法，建立了一个强有力的操纵规划框架。

    Developing intelligent robots for complex manipulation tasks in household and factory settings remains challenging due to long-horizon tasks, contact-rich manipulation, and the need to generalize across a wide variety of object shapes and scene layouts. While Task and Motion Planning (TAMP) offers a promising solution, its assumptions such as kinodynamic models limit applicability in novel contexts. Neural object descriptors (NODs) have shown promise in object and scene generalization but face limitations in addressing broader tasks. Our proposed TAMP-based framework, NOD-TAMP, extracts short manipulation trajectories from a handful of human demonstrations, adapts these trajectories using NOD features, and composes them to solve broad long-horizon tasks. Validated in a simulation environment, NOD-TAMP effectively tackles varied challenges and outperforms existing methods, establishing a cohesive framework for manipulation planning. For videos and other supplemental material, see the pr
    
[^39]: 基于内容的音乐大语言建模的控制

    Content-based Controls For Music Large Language Modeling. (arXiv:2310.17162v1 [cs.AI])

    [http://arxiv.org/abs/2310.17162](http://arxiv.org/abs/2310.17162)

    该论文提出了一种基于内容的控制方法，用于音乐大语言建模。通过对音高、和弦和鼓乐等固有音乐语言的直接控制，实现了高质量的音乐生成，并且使用了参数高效微调的方法，比原始模型的参数数量少于4%。

    

    近年来，在音乐音频领域出现了大规模语言模型的迅速增长。这些模型使得能够进行高质量音乐的端到端生成，并且一些模型可以使用文本描述进行条件生成。然而，文本在音乐上的控制能力本质上是有限的，因为它们只能通过元数据（如歌手和乐器）或高级表示（如流派和情感）间接地描述音乐。我们的目标是进一步提供对音高、和弦和鼓乐等固有音乐语言的直接和基于内容的控制能力。为此，我们提出了Coco-Mulla，这是一种用于音乐大语言建模的基于内容的控制方法。它使用了针对基于Transformer的音频模型量身定制的参数高效微调（PEFT）方法。实验表明，我们的方法在低资源半监督学习中实现了高质量的音乐生成，相比原始模型，参数调优的比例不到4%。

    Recent years have witnessed a rapid growth of large-scale language models in the domain of music audio. Such models enable end-to-end generation of higher-quality music, and some allow conditioned generation using text descriptions. However, the control power of text controls on music is intrinsically limited, as they can only describe music indirectly through meta-data (such as singers and instruments) or high-level representations (such as genre and emotion). We aim to further equip the models with direct and content-based controls on innate music languages such as pitch, chords and drum track. To this end, we contribute Coco-Mulla, a content-based control method for music large language modeling. It uses a parameter-efficient fine-tuning (PEFT) method tailored for Transformer-based audio models. Experiments show that our approach achieved high-quality music generation with low-resource semi-supervised learning, tuning with less than 4% parameters compared to the original model and t
    
[^40]: 使用分布式Hebbian Temporal Memory学习继任者表示法

    Learning Successor Representations with Distributed Hebbian Temporal Memory. (arXiv:2310.13391v1 [cs.LG])

    [http://arxiv.org/abs/2310.13391](http://arxiv.org/abs/2310.13391)

    本文提出了一种名为DHTM的算法，它基于因子图形式和多组成神经元模型，利用分布式表示、稀疏转移矩阵和局部Hebbian样学习规则来解决在线隐藏表示学习的挑战。实验结果表明，DHTM在变化的环境中比经典的LSTM效果更好，并与更先进的类似RNN的算法性能相当，可以加速继任者表示的时间差异学习。

    

    本文提出了一种新颖的方法来解决在线隐藏表示学习的挑战，该方法用于在不稳定的、部分可观测的环境中进行决策。所提出的算法，分布式Hebbian Temporal Memory (DHTM)，基于因子图形式和多组成神经元模型。DHTM旨在捕捉顺序数据关系并对未来观察作出累积预测，形成继任者表示。受新皮层的神经生理学模型启发，该算法利用分布式表示、稀疏转移矩阵和局部Hebbian样学习规则克服了传统时间记忆算法（如RNN和HMM）的不稳定性和慢速学习过程。实验结果表明，DHTM优于经典的LSTM，并与更先进的类似RNN的算法性能相当，在变化的环境中加速了继任者表示的时间差异学习。此外，我们还进行了比较。

    This paper presents a novel approach to address the challenge of online hidden representation learning for decision-making under uncertainty in non-stationary, partially observable environments. The proposed algorithm, Distributed Hebbian Temporal Memory (DHTM), is based on factor graph formalism and a multicomponent neuron model. DHTM aims to capture sequential data relationships and make cumulative predictions about future observations, forming Successor Representation (SR). Inspired by neurophysiological models of the neocortex, the algorithm utilizes distributed representations, sparse transition matrices, and local Hebbian-like learning rules to overcome the instability and slow learning process of traditional temporal memory algorithms like RNN and HMM. Experimental results demonstrate that DHTM outperforms classical LSTM and performs comparably to more advanced RNN-like algorithms, speeding up Temporal Difference learning for SR in changing environments. Additionally, we compare
    
[^41]: Safety-Gymnasion：一个统一的安全强化学习基准

    Safety-Gymnasium: A Unified Safe Reinforcement Learning Benchmark. (arXiv:2310.12567v1 [cs.AI])

    [http://arxiv.org/abs/2310.12567](http://arxiv.org/abs/2310.12567)

    本文介绍了一个名为Safety-Gymnasium的环境套件，其中包括单个和多个Agent场景中的安全关键任务，并提供了一个包含16种最先进的SafeRL算法的算法库。这个基准旨在促进对安全性能的评估和比较，推动强化学习在安全性能上的发展。

    

    人工智能系统拥有推动社会进步的巨大潜力。然而，它们的部署经常面临严重的安全问题。安全强化学习(SafeRL)作为一种解决方案出现，可以在同时遵守多个约束的情况下优化策略，从而解决集成强化学习在安全关键场景中的挑战。本文介绍了一个名为Safety-Gymnasium的环境套件，包括单个和多个Agent场景中的安全关键任务，并接受向量和仅视觉输入。此外，我们提供了一个名为Safe Policy Optimization（SafePO）的算法库，包含16种最先进的SafeRL算法。这个综合性库可以作为研究社区的验证工具。通过引入这个基准，我们旨在促进对安全性能的评估和比较，从而推动强化学习在安全性能上的发展。

    Artificial intelligence (AI) systems possess significant potential to drive societal progress. However, their deployment often faces obstacles due to substantial safety concerns. Safe reinforcement learning (SafeRL) emerges as a solution to optimize policies while simultaneously adhering to multiple constraints, thereby addressing the challenge of integrating reinforcement learning in safety-critical scenarios. In this paper, we present an environment suite called Safety-Gymnasium, which encompasses safety-critical tasks in both single and multi-agent scenarios, accepting vector and vision-only input. Additionally, we offer a library of algorithms named Safe Policy Optimization (SafePO), comprising 16 state-of-the-art SafeRL algorithms. This comprehensive library can serve as a validation tool for the research community. By introducing this benchmark, we aim to facilitate the evaluation and comparison of safety performance, thus fostering the development of reinforcement learning for s
    
[^42]: 针对随机驾驶环境的不确定性感知决策Transformer

    Uncertainty-Aware Decision Transformer for Stochastic Driving Environments. (arXiv:2309.16397v1 [cs.LG])

    [http://arxiv.org/abs/2309.16397](http://arxiv.org/abs/2309.16397)

    本论文提出了一种针对随机驾驶环境的不确定性感知决策Transformer（UNREST），通过估计状态的不确定性并相应地分割序列，取代了全局回报。这项工作通过解决在不确定性环境中过于乐观的问题，为离线强化学习在自主驾驶任务中的应用提供了一种有效的解决方案。

    

    离线强化学习（RL）已经成为一种无需主动交互的学习策略的有希望框架，因此在自主驾驶任务中尤其吸引人。最近Transformers的成功启发了将离线RL视为序列建模，这在长期任务中表现出色。然而，在具有不确定性的环境中，它们过于乐观，错误地假设相同的目标可以通过相同的动作一致实现。在本文中，我们引入了一种针对随机驾驶环境的不确定性感知决策Transformer（UNREST），不引入额外的转换模型或复杂的生成模型来进行规划。具体而言，UNREST通过转换与回报之间的条件互信息来估计状态的不确定性，并相应地分割序列。通过发现驾驶环境的“不确定性累积”和“时间局部性”特性，UNREST将决策Transformer中的全局回报替换为较少的部分回报。

    Offline Reinforcement Learning (RL) has emerged as a promising framework for learning policies without active interactions, making it especially appealing for autonomous driving tasks. Recent successes of Transformers inspire casting offline RL as sequence modeling, which performs well in long-horizon tasks. However, they are overly optimistic in stochastic environments with incorrect assumptions that the same goal can be consistently achieved by identical actions. In this paper, we introduce an UNcertainty-awaRE deciSion Transformer (UNREST) for planning in stochastic driving environments without introducing additional transition or complex generative models. Specifically, UNREST estimates state uncertainties by the conditional mutual information between transitions and returns, and segments sequences accordingly. Discovering the `uncertainty accumulation' and `temporal locality' properties of driving environments, UNREST replaces the global returns in decision transformers with less 
    
[^43]: 针对罕见事件预测的综合调研

    A Comprehensive Survey on Rare Event Prediction. (arXiv:2309.11356v1 [cs.AI])

    [http://arxiv.org/abs/2309.11356](http://arxiv.org/abs/2309.11356)

    本文综合调研了罕见事件预测领域的当前方法，通过考虑罕见事件数据、数据处理、算法方法和评估方法四个维度，总结出了在机器学习流程中解决罕见事件预测问题的关键方法。

    

    罕见事件预测涉及使用机器学习和数据分析识别和预测低概率事件。由于数据分布不平衡，普通事件的频率远远超过罕见事件的频率，因此需要在机器学习流程的每个步骤中使用专门的方法，从数据处理到算法到评估协议。预测罕见事件的发生对于工业4.0等实际应用非常重要，也是统计和机器学习领域的一个活跃的研究领域。本文综合评估了罕见事件预测的当前方法，从罕见事件数据、数据处理、算法方法和评估方法四个维度考虑。具体来说，我们考虑了来自不同模态的73个数据集（数值、图像、文本和音频），四个主要的数据处理分类，五个主要的算法分类和两个更广泛的评估方法。

    Rare event prediction involves identifying and forecasting events with a low probability using machine learning and data analysis. Due to the imbalanced data distributions, where the frequency of common events vastly outweighs that of rare events, it requires using specialized methods within each step of the machine learning pipeline, i.e., from data processing to algorithms to evaluation protocols. Predicting the occurrences of rare events is important for real-world applications, such as Industry 4.0, and is an active research area in statistical and machine learning. This paper comprehensively reviews the current approaches for rare event prediction along four dimensions: rare event data, data processing, algorithmic approaches, and evaluation approaches. Specifically, we consider 73 datasets from different modalities (i.e., numerical, image, text, and audio), four major categories of data processing, five major algorithmic groupings, and two broader evaluation approaches. This pape
    
[^44]: 编码-存储-检索：通过语言编码的自我中心感知增强记忆

    Encode-Store-Retrieve: Enhancing Memory Augmentation through Language-Encoded Egocentric Perception. (arXiv:2308.05822v1 [cs.CV])

    [http://arxiv.org/abs/2308.05822](http://arxiv.org/abs/2308.05822)

    本研究提出了一种记忆增强系统，它利用自然语言编码视频数据并将其存储在向量数据库中，通过利用大型视觉语言模型的强大功能来进行语言编码的过程。

    

    我们依赖于自己的记忆来编码、存储和检索我们的经历。然而，记忆间隔有时会发生。实现记忆增强的一种有希望的方法是通过使用增强现实头戴式显示设备来捕捉和保留自我中心的视频，这种做法通常被称为生活记录。然而，由于当前技术缺乏高效编码和存储如此大量的视频数据的能力，从庞大的视频存档中检索特定信息需要大量的计算能力，进一步复杂了快速访问所需内容的任务。

    We depend on our own memory to encode, store, and retrieve our experiences. However, memory lapses can occur. One promising avenue for achieving memory augmentation is through the use of augmented reality head-mounted displays to capture and preserve egocentric videos, a practice commonly referred to as life logging. However, a significant challenge arises from the sheer volume of video data generated through life logging, as the current technology lacks the capability to encode and store such large amounts of data efficiently. Further, retrieving specific information from extensive video archives requires substantial computational power, further complicating the task of quickly accessing desired content. To address these challenges, we propose a memory augmentation system that involves leveraging natural language encoding for video data and storing them in a vector database. This approach harnesses the power of large vision language models to perform the language encoding process. Add
    
[^45]: 学习核技术用于可解释和高效的PPG信号质量评估和伪影分割

    Learned Kernels for Interpretable and Efficient PPG Signal Quality Assessment and Artifact Segmentation. (arXiv:2307.05385v1 [eess.SP])

    [http://arxiv.org/abs/2307.05385](http://arxiv.org/abs/2307.05385)

    本文提出了一种通过学习核技术，具有解释性且参数较少的方法来评估和分割PPG信号的质量和伪影，与现有的深度神经网络方法相比有着类似甚至更好的性能。

    

    光电容抗(PPG)提供了一种低成本、非侵入性的方法来持续监测各种心血管参数。PPG信号由可穿戴设备产生，常常包含由外部因素(如人体运动)引起的大型伪影。为了确保对生理参数进行稳健和准确的提取，信号的损坏区域需要被正确地识别和处理。之前的方法依靠手工特征检测器或信号度量，结果性能不佳，或依靠深度神经网络(DNN)等机器学习技术，缺乏可解释性，计算和内存密集。在这项工作中，我们提出了一种新的方法，学习一小组可解释的卷积核，其性能与现有技术DNN方法相似，甚至更好，而参数数量比DNN方法少几个数量级。这项工作实现了高效、稳健和可解释的PPG信号质量评估和伪影分割。

    Photoplethysmography (PPG) provides a low-cost, non-invasive method to continuously monitor various cardiovascular parameters. PPG signals are generated by wearable devices and frequently contain large artifacts caused by external factors, such as motion of the human subject. In order to ensure robust and accurate extraction of physiological parameters, corrupted areas of the signal need to be identified and handled appropriately. Previous methodology relied either on handcrafted feature detectors or signal metrics which yield sub-optimal performance, or relied on machine learning techniques such as deep neural networks (DNN) which lack interpretability and are computationally and memory intensive. In this work, we present a novel method to learn a small set of interpretable convolutional kernels that has performance similar to -- and often better than -- the state-of-the-art DNN approach with several orders of magnitude fewer parameters. This work allows for efficient, robust, and int
    
[^46]: FheFL：支持完全同态加密的隐私保护联邦学习与拜占庭用户

    FheFL: Fully Homomorphic Encryption Friendly Privacy-Preserving Federated Learning with Byzantine Users. (arXiv:2306.05112v1 [cs.AI])

    [http://arxiv.org/abs/2306.05112](http://arxiv.org/abs/2306.05112)

    本论文介绍了一种新的联邦学习算法，采用FHE加密技术，既可以保护模型更新的隐私，又可以防止恶意用户破坏全局模型。

    

    联邦学习（FL）技术最初是为了缓解传统机器学习范式中可能出现的数据隐私问题而开发的。尽管FL确保用户的数据始终保留在用户手中，但局部训练模型的梯度必须与集中式服务器通信以构建全局模型。这导致隐私泄露，使得服务器可以从共享的梯度中推断出用户数据的私密信息。为了缓解这一缺陷，下一代FL架构提出了加密和匿名化技术，以保护模型更新免受服务器的攻击。然而，这种方法会带来其他挑战，例如恶意用户可能通过共享虚假梯度来破坏全局模型。由于梯度被加密，服务器无法识别和排除不良用户以保护全局模型。因此，为了缓解这两种攻击，本文提出了一种基于完全同态加密（FHE）的新方案。

    The federated learning (FL) technique was initially developed to mitigate data privacy issues that can arise in the traditional machine learning paradigm. While FL ensures that a user's data always remain with the user, the gradients of the locally trained models must be communicated with the centralized server to build the global model. This results in privacy leakage, where the server can infer private information of the users' data from the shared gradients. To mitigate this flaw, the next-generation FL architectures proposed encryption and anonymization techniques to protect the model updates from the server. However, this approach creates other challenges, such as a malicious user might sabotage the global model by sharing false gradients. Since the gradients are encrypted, the server is unable to identify and eliminate rogue users which would protect the global model. Therefore, to mitigate both attacks, this paper proposes a novel fully homomorphic encryption (FHE) based scheme 
    
[^47]: 在动态语料库上持续更新生成检索

    Continually Updating Generative Retrieval on Dynamic Corpora. (arXiv:2305.18952v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2305.18952](http://arxiv.org/abs/2305.18952)

    本文研究了动态语料库上的生成检索。实验结果表明，在静态设置下，生成检索效果优于双编码器，但在动态设置下情况相反。通过使用参数高效的预训练方法，我们的模型DynamicGR在新的语料库上展现出了意外的性能。

    

    先前关于信息检索(IR)的大多数研究假设语料库是静态的，而实际世界中的文档是不断更新的。本文将知识的动态性引入检索系统中，将检索视为动态的知识库，更符合真实环境。我们对双编码器和生成检索进行全面评估，利用StreamingQA基准测试用于时态知识更新。我们的初步结果显示，在静态设置下，生成检索优于双编码器，但在动态设置下情况相反。然而，令人惊讶的是，当我们利用参数高效的预训练方法增强生成检索对新语料库的适应性时，我们的模型Dynamic Generative Retrieval (DynamicGR)展现出意外的发现。它能够在其内部索引中高效压缩新的知识，

    The majority of prior work on information retrieval (IR) assumes that the corpus is static, whereas in the real world, the documents are continually updated. In this paper, we incorporate often overlooked dynamic nature of knowledge into the retrieval systems. Our work treats retrieval not as static archives but as dynamic knowledge bases better aligned with real-world environments. We conduct a comprehensive evaluation of dual encoders and generative retrieval, utilizing the StreamingQA benchmark designed for the temporal knowledge updates. Our initial results show that while generative retrieval outperforms dual encoders in static settings, the opposite is true in dynamic settings. Surprisingly, however, when we utilize a parameter-efficient pre-training method to enhance adaptability of generative retrieval to new corpora, our resulting model, Dynamic Generative Retrieval (DynamicGR), exhibits unexpected findings. It (1) efficiently compresses new knowledge in their internal index, 
    
[^48]: 用Transformer学习超关系型和数值知识图中的表征学习

    Representation Learning on Hyper-Relational and Numeric Knowledge Graphs with Transformers. (arXiv:2305.18256v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.18256](http://arxiv.org/abs/2305.18256)

    本文提出了一个名为HyNT的框架，用于学习超关系型知识图的表示，包括数值文字。该框架使用上下文Transformer和预测Transformer，通过学习三元组和其限定词之间的相关性以及数值信息来获得模型。

    

    近期研究了一个超关系型知识图谱，其中三元组与限定词集合相关联; 一个限定词由关系和实体组成，为三元组提供辅助信息。现有的超关系型知识图嵌入方法假定实体是离散对象，但有些信息应使用数值表示，例如(J.R.R.，出生于，1892)。同时，三元组(J.R.R.，就读于，牛津大学)可以与限定词(开始时间，1911)相关联。在本文中，我们提出了一个名为HyNT的统一框架，用于学习包含三元组或限定词中数值文字的超关系型知识图的表示。我们定义了一个上下文Transformer和一个预测Transformer，来学习表示，不仅基于三元组和其限定词之间的相关性，还基于数值信息。通过学习三元组和限定词的紧凑表示，并将它们馈送给Transformer来获得模型

    A hyper-relational knowledge graph has been recently studied where a triplet is associated with a set of qualifiers; a qualifier is composed of a relation and an entity, providing auxiliary information for a triplet. While existing hyper-relational knowledge graph embedding methods assume that the entities are discrete objects, some information should be represented using numeric values, e.g., (J.R.R., was born in, 1892). Also, a triplet (J.R.R., educated at, Oxford Univ.) can be associated with a qualifier such as (start time, 1911). In this paper, we propose a unified framework named HyNT that learns representations of a hyper-relational knowledge graph containing numeric literals in either triplets or qualifiers. We define a context transformer and a prediction transformer to learn the representations based not only on the correlations between a triplet and its qualifiers but also on the numeric information. By learning compact representations of triplets and qualifiers and feeding 
    
[^49]: PI-FL：个性化和激励联邦学习

    PI-FL: Personalized and Incentivized Federated Learning. (arXiv:2304.07514v1 [cs.LG])

    [http://arxiv.org/abs/2304.07514](http://arxiv.org/abs/2304.07514)

    PI-FL是一种个性化的联邦学习解决方案，使用基于令牌的激励机制奖励个性化训练，可以在尊重客户端隐私的同时生成高质量的个性化模型。

    

    个性化联邦学习已被广泛应用于应对非独立同分布数据异质性的挑战。主要问题是考虑来自客户端的个性化过程以保护其自治权。允许客户端参与个性化联邦学习决策变得重要，因为存在隐私和安全问题，客户端可能无法自由共享生成良好质量个性化模型所必需的私人信息。此外，具有高质量数据和资源的客户端不愿意在没有合理激励的情况下参与联邦学习过程。在本文中，我们提出了PI-FL，这是一个一次性个性化解决方案，配合一个基于令牌的激励机制，奖励个性化训练。PI-FL优于其他最先进的方法，并且可以在尊重客户端隐私的同时生成高质量的个性化模型。

    Personalized FL has been widely used to cater to heterogeneity challenges with non-IID data. A primary obstacle is considering the personalization process from the client's perspective to preserve their autonomy. Allowing the clients to participate in personalized FL decisions becomes significant due to privacy and security concerns, where the clients may not be at liberty to share private information necessary for producing good quality personalized models. Moreover, clients with high-quality data and resources are reluctant to participate in the FL process without reasonable incentive. In this paper, we propose PI-FL, a one-shot personalization solution complemented by a token-based incentive mechanism that rewards personalized training. PI-FL outperforms other state-of-the-art approaches and can generate good-quality personalized models while respecting clients' privacy.
    

