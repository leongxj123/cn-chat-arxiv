# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Towards the new XAI: A Hypothesis-Driven Approach to Decision Support Using Evidence](https://rss.arxiv.org/abs/2402.01292) | 本文介绍并评估了一种基于证据权重框架的假设驱动可解释人工智能方法，通过提供支持或驳斥假设的证据来增加决策准确性和减少依赖程度。 |
| [^2] | [Safe Interval RRT* for Scalable Multi-Robot Path Planning in Continuous Space](https://arxiv.org/abs/2404.01752) | 提出了安全间隔RRT*（SI-RRT*）两级方法，低级采用采样规划器找到单个机器人的无碰撞轨迹，高级通过优先规划或基于冲突的搜索解决机器人间冲突，实验结果表明SI-RRT*能够快速找到高质量解决方案 |
| [^3] | [Optimization-based Prompt Injection Attack to LLM-as-a-Judge](https://arxiv.org/abs/2403.17710) | 介绍了一种基于优化的提示注入攻击方法，JudgeDeceiver，针对LLM-as-a-Judge，通过自动化生成对抗序列实现了有针对性和高效的模型评估操控。 |
| [^4] | [Improving Robustness to Model Inversion Attacks via Sparse Coding Architectures](https://arxiv.org/abs/2403.14772) | 通过稀疏编码层设计新网络架构以提高对模型逆推攻击的鲁棒性。 |
| [^5] | [EAS-SNN: End-to-End Adaptive Sampling and Representation for Event-based Detection with Recurrent Spiking Neural Networks](https://arxiv.org/abs/2403.12574) | 提出了一种利用循环脉冲神经网络的自适应采样模块，通过将脉冲神经元的神经动力学与理想的时间事件采样器的行为相结合，实现了端到端可学习的事件检测框架 |
| [^6] | [Align and Distill: Unifying and Improving Domain Adaptive Object Detection](https://arxiv.org/abs/2403.12029) | 引入了统一的基准测试和实现框架ALDI以及新的DAOD基准数据集CFC-DAOD，解决了领域自适应目标检测中的基准问题，并支持未来方法的发展。 |
| [^7] | [StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows](https://arxiv.org/abs/2403.11322) | 提出了一种使用StateFlow的新颖LLM任务解决范式，将复杂任务解决过程概念化为状态机，通过状态转换确保LLM响应的清晰跟踪和管理。 |
| [^8] | [Non-discrimination Criteria for Generative Language Models](https://arxiv.org/abs/2403.08564) | 本文研究如何在生成式语言模型中识别和量化性别偏见，提出了三个生成式人工智能的非歧视标准并设计了相应的提示。 |
| [^9] | [A Benchmark of Domain-Adapted Large Language Models for Generating Brief Hospital Course Summaries](https://arxiv.org/abs/2403.05720) | 介绍了一个新的基准测试，评估了用于生成简要住院病程摘要的大语言模型在健康保健领域中的性能并提出相应的自适应策略 |
| [^10] | [SheetAgent: A Generalist Agent for Spreadsheet Reasoning and Manipulation via Large Language Models](https://arxiv.org/abs/2403.03636) | SheetAgent是一种利用大型语言模型进行电子表格推理和操作的通用代理，提供了处理复杂现实任务的解决方案 |
| [^11] | [QuanTest: Entanglement-Guided Testing of Quantum Neural Network Systems](https://arxiv.org/abs/2402.12950) | QuanTest是一个基于纠缠的对抗测试框架，旨在揭示量子神经网络系统中的潜在错误行为。 |
| [^12] | [Bidirectional Generative Pre-training for Improving Time Series Representation Learning](https://arxiv.org/abs/2402.09558) | 这项论文提出了一种名为BiTimelyGPT的模型，通过双向的预训练任务在时间序列数据上学习表示，展示了优越的性能，可用于神经功能预测、疾病诊断和生理病征识别。 |
| [^13] | [On the Convergence Rate of the Stochastic Gradient Descent (SGD) and application to a modified policy gradient for the Multi Armed Bandit](https://arxiv.org/abs/2402.06388) | 该论文证明了当学习速率按照逆时间衰减规则时，随机梯度下降（SGD）的收敛速度，并应用于修改的带有L2正则化的策略梯度多臂赌博机（MAB）的收敛性分析。 |
| [^14] | [Optimizing Delegation in Collaborative Human-AI Hybrid Teams](https://arxiv.org/abs/2402.05605) | 本论文提出了一种优化协作的人工智能-人类混合团队授权的框架，通过引入AI经理（通过强化学习）作为团队的外部观察者，学习团队代理人的行为模型并选择最佳的控制代理人。 |
| [^15] | [Learning a Decision Tree Algorithm with Transformers](https://arxiv.org/abs/2402.03774) | 该论文介绍了MetaTree模型，它使用经典算法的输出训练基于Transformer的模型，以产生具有强大概括性能的决策树。 |
| [^16] | [DefInt: A Default-interventionist Framework for Efficient Reasoning with Hybrid Large Language Models](https://arxiv.org/abs/2402.02563) | DefInt提出了一种默认干预框架，通过默认使用较小规模的语言模型生成推理思路，然后通过反思推理干预解决复杂推理问题，从而提高混合大型语言模型的效率和准确性。 |
| [^17] | [Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models](https://arxiv.org/abs/2311.06607) | Monkey通过提高图像分辨率和采用多级描述生成方法来增强大型多模态模型(LMMs)的能力，从而实现更详细的视觉捕捉和更有效的学习。 |
| [^18] | [TabRepo: A Large Scale Repository of Tabular Model Evaluations and its AutoML Applications](https://arxiv.org/abs/2311.02971) | TabRepo引入了一个包含1310个模型在200个分类和回归数据集上评估预测结果和指标信息的数据集，展示了它在进行超参数优化、AutoML系统比较和迁移学习等方面的优势。 |
| [^19] | [Put Your Money Where Your Mouth Is: Evaluating Strategic Planning and Execution of LLM Agents in an Auction Arena](https://arxiv.org/abs/2310.05746) | LLM代理在拍卖竞技场展示出了关键的规划和执行技能，这为建模复杂社会互动在竞争背景下的LLMs潜力提供了新途径。 |
| [^20] | [Reinforcement Learning for Versatile, Dynamic, and Robust Bipedal Locomotion Control.](http://arxiv.org/abs/2401.16889) | 本论文使用深度强化学习创建了一种通用的双足机器人动态运动控制器，该控制器可以应用于多种动态双足技能，并且在模拟环境和实际环境中展现出了优越性能。 |
| [^21] | [Bringing Back the Context: Camera Trap Species Identification as Link Prediction on Multimodal Knowledge Graphs.](http://arxiv.org/abs/2401.00608) | 本研究利用相机陷阱图像的结构化上下文，提高其在物种识别任务中的泛化能力，并解决了数据稀缺和泛化能力增强的问题。 |
| [^22] | [Self-Supervised Disentangled Representation Learning for Robust Target Speech Extraction.](http://arxiv.org/abs/2312.10305) | 该论文提出了一种自监督分解表示学习方法，通过逐步分离说话人身份信息和其他无关因素，解决了目标语音提取任务中存在的说话人混叠问题，并使用分解的说话人身份信息来指导语音提取网络。 |
| [^23] | [Minimally Modifying a Markov Game to Achieve Any Nash Equilibrium and Value.](http://arxiv.org/abs/2311.00582) | 该论文研究了游戏修改问题，提出了一种最小修改马尔可夫博弈的方法，使得目标策略配置成为唯一的Nash均衡并具有特定价值范围，同时最小化修改成本。 |
| [^24] | [KLoB: a Benchmark for Assessing Knowledge Locating Methods in Language Models.](http://arxiv.org/abs/2309.16535) | KLoB是一个评估语言模型中知识定位方法的基准，旨在解决现有定位方法的准确性和事实知识局部性假设的问题。 |
| [^25] | [Lyra: Orchestrating Dual Correction in Automated Theorem Proving.](http://arxiv.org/abs/2309.15806) | Lyra是一种新的框架，通过引入工具修正和猜想修正两种机制，增强了大规模语言模型在形式化定理证明领域的有效性，减轻了幻觉，并提高了证明的准确性。 |
| [^26] | [Separating the Human Touch from AI-Generated Text using Higher Criticism: An Information-Theoretic Approach.](http://arxiv.org/abs/2308.12747) | 本文提出了一种使用更高的批评力量将人类触摸与AI生成的文本分离的方法，通过多个困惑度测试和统计模型的结合，能够有效识别出可能被编辑的部分，并探讨了方法的有效性及相关挑战。 |
| [^27] | [Fast Unsupervised Deep Outlier Model Selection with Hypernetworks.](http://arxiv.org/abs/2307.10529) | 本文提出了HYPER用于调整基于深度神经网络的异常值检测模型，解决了无监督DOD模型中的超参数调整和模型选择的挑战，通过设计和训练超网络(HN)将超参数映射到主要DOD模型的最优权重上。 |
| [^28] | [Online Goal Recognition in Discrete and Continuous Domains Using a Vectorial Representation.](http://arxiv.org/abs/2307.07876) | 该论文提出了一种在离散和连续域中使用矢量表达进行在线目标识别的高效方法，该方法通过减少计算负担和提高识别速度，使其成为首个可用于快速变动环境下的机器人应用的在线方法。 |
| [^29] | [Reduce Computational Complexity for Convolutional Layers by Skipping Zeros.](http://arxiv.org/abs/2306.15951) | 本文提出了C-K-S算法，通过修剪滤波器和转换稀疏张量为稠密张量的方式，跳过卷积层中的0元素，从而降低了计算复杂度。实验证明，C-K-S相对于PyTorch具有优势。 |
| [^30] | [Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data.](http://arxiv.org/abs/2306.13840) | 本论文提出使用多样性系数作为LLM预训练数据质量的指标，研究表明公开可用的LLM数据集的多样性系数很高。 |
| [^31] | [MetaGAD: Learning to Meta Transfer for Few-shot Graph Anomaly Detection.](http://arxiv.org/abs/2305.10668) | 本文提出了一种名为MetaGAD的框架，用于学习从无标记节点到有标记节点之间的元转移知识，以进行少样本图异常检测。 |
| [^32] | [A transformer-based method for zero and few-shot biomedical named entity recognition.](http://arxiv.org/abs/2305.04928) | 本文提出了一种基于Transformer的生物医学领域零样本和少样本NER方法。此方法利用预训练学习给定和潜在类别之间的语义关系，将多类标记分类任务转换为二元标记分类，能够在不同数量的样本情况下达到良好的识别效果。 |
| [^33] | [Interpretable and Robust AI in EEG Systems: A Survey.](http://arxiv.org/abs/2304.10755) | 这篇论文综述了近年来脑电图系统中可解释和鲁棒的AI技术的发展。其中，作者提出了解释性分类法，详细介绍了鲁棒AI的方法，包括对抗攻击和防御、迁移学习和不确定性建模，并讨论了未来的研究方向和挑战。 |
| [^34] | [Text2Seg: Remote Sensing Image Semantic Segmentation via Text-Guided Visual Foundation Models.](http://arxiv.org/abs/2304.10597) | 本文介绍了一种名为 Text2Seg 的遥感图像语义分割流程，利用多个基础模型和文本引导，取得了初步成果。 |
| [^35] | [Bayesian neural networks via MCMC: a Python-based tutorial.](http://arxiv.org/abs/2304.02595) | 本文提供了一个基于Python的教程，介绍了贝叶斯神经网络的MCMC方法应用，通过教程使得深度学习开发者能够更好地应用贝叶斯推断进行参数估计和不确定性量化。 |

# 详细

[^1]: 迈向新的可解释人工智能：通过证据支持的假设驱动方法的决策支持

    Towards the new XAI: A Hypothesis-Driven Approach to Decision Support Using Evidence

    [https://rss.arxiv.org/abs/2402.01292](https://rss.arxiv.org/abs/2402.01292)

    本文介绍并评估了一种基于证据权重框架的假设驱动可解释人工智能方法，通过提供支持或驳斥假设的证据来增加决策准确性和减少依赖程度。

    

    之前关于AI辅助人类决策的研究探索了几种不同的可解释人工智能（XAI）方法。最近的一篇论文提出了一种范式转变，呼吁通过一个称为评价型AI的概念框架来进行假设驱动的XAI，该框架为人们提供支持或驳斥假设的证据，而不一定给出决策辅助推荐。在本文中，我们描述并评估了一种基于证据权重（WoE）框架的假设驱动XAI方法，该方法为给定的假设生成正面和负面证据。通过人类行为实验，我们展示了我们的假设驱动方法提高了决策准确性，与推荐驱动方法和仅AI解释基线相比减少了依赖程度，但相对于推荐驱动方法，在依赖程度下降方面略微增加。此外，我们还展示了参与者在使用我们的假设驱动方法时与两个基线的方式存在实质性的差异。

    Prior research on AI-assisted human decision-making has explored several different explainable AI (XAI) approaches. A recent paper has proposed a paradigm shift calling for hypothesis-driven XAI through a conceptual framework called evaluative AI that gives people evidence that supports or refutes hypotheses without necessarily giving a decision-aid recommendation. In this paper we describe and evaluate an approach for hypothesis-driven XAI based on the Weight of Evidence (WoE) framework, which generates both positive and negative evidence for a given hypothesis. Through human behavioural experiments, we show that our hypothesis-driven approach increases decision accuracy, reduces reliance compared to a recommendation-driven approach and an AI-explanation-only baseline, but with a small increase in under-reliance compared to the recommendation-driven approach. Further, we show that participants used our hypothesis-driven approach in a materially different way to the two baselines.
    
[^2]: 安全间隔RRT*用于连续空间中可扩展的多机器人路径规划

    Safe Interval RRT* for Scalable Multi-Robot Path Planning in Continuous Space

    [https://arxiv.org/abs/2404.01752](https://arxiv.org/abs/2404.01752)

    提出了安全间隔RRT*（SI-RRT*）两级方法，低级采用采样规划器找到单个机器人的无碰撞轨迹，高级通过优先规划或基于冲突的搜索解决机器人间冲突，实验结果表明SI-RRT*能够快速找到高质量解决方案

    

    在本文中，我们考虑了在连续空间中解决多机器人路径规划（MRPP）问题以找到无冲突路径的问题。问题的困难主要来自两个因素。首先，涉及多个机器人会导致组合决策，使搜索空间呈指数级增长。其次，连续空间呈现出潜在无限的状态和动作。针对这个问题，我们提出了一个两级方法，低级是基于采样的规划器安全间隔RRT*（SI-RRT*），用于找到单个机器人的无碰撞轨迹。高级可以使用能够解决机器人间冲突的任何方法，我们采用了两种代表性方法，即优先规划（SI-CPP）和基于冲突的搜索（SI-CCBS）。实验结果表明，SI-RRT*能够快速找到高质量解决方案，并且所需的样本数量较少。SI-CPP表现出更好的可扩展性，而SI-CCBS产生

    arXiv:2404.01752v1 Announce Type: cross  Abstract: In this paper, we consider the problem of Multi-Robot Path Planning (MRPP) in continuous space to find conflict-free paths. The difficulty of the problem arises from two primary factors. First, the involvement of multiple robots leads to combinatorial decision-making, which escalates the search space exponentially. Second, the continuous space presents potentially infinite states and actions. For this problem, we propose a two-level approach where the low level is a sampling-based planner Safe Interval RRT* (SI-RRT*) that finds a collision-free trajectory for individual robots. The high level can use any method that can resolve inter-robot conflicts where we employ two representative methods that are Prioritized Planning (SI-CPP) and Conflict Based Search (SI-CCBS). Experimental results show that SI-RRT* can find a high-quality solution quickly with a small number of samples. SI-CPP exhibits improved scalability while SI-CCBS produces 
    
[^3]: 基于优化的对LLM评判系统的提示注入攻击

    Optimization-based Prompt Injection Attack to LLM-as-a-Judge

    [https://arxiv.org/abs/2403.17710](https://arxiv.org/abs/2403.17710)

    介绍了一种基于优化的提示注入攻击方法，JudgeDeceiver，针对LLM-as-a-Judge，通过自动化生成对抗序列实现了有针对性和高效的模型评估操控。

    

    LLM-as-a-Judge 是一种可以使用大型语言模型（LLMs）评估文本信息的新颖解决方案。根据现有研究，LLMs在提供传统人类评估的引人注目替代方面表现出色。然而，这些系统针对提示注入攻击的鲁棒性仍然是一个未解决的问题。在这项工作中，我们引入了JudgeDeceiver，一种针对LLM-as-a-Judge量身定制的基于优化的提示注入攻击。我们的方法制定了一个精确的优化目标，用于攻击LLM-as-a-Judge的决策过程，并利用优化算法高效地自动化生成对抗序列，实现对模型评估的有针对性和有效的操作。与手工制作的提示注入攻击相比，我们的方法表现出卓越的功效，给基于LLM的判断系统当前的安全范式带来了重大挑战。

    arXiv:2403.17710v1 Announce Type: cross  Abstract: LLM-as-a-Judge is a novel solution that can assess textual information with large language models (LLMs). Based on existing research studies, LLMs demonstrate remarkable performance in providing a compelling alternative to traditional human assessment. However, the robustness of these systems against prompt injection attacks remains an open question. In this work, we introduce JudgeDeceiver, a novel optimization-based prompt injection attack tailored to LLM-as-a-Judge. Our method formulates a precise optimization objective for attacking the decision-making process of LLM-as-a-Judge and utilizes an optimization algorithm to efficiently automate the generation of adversarial sequences, achieving targeted and effective manipulation of model evaluations. Compared to handcraft prompt injection attacks, our method demonstrates superior efficacy, posing a significant challenge to the current security paradigms of LLM-based judgment systems. T
    
[^4]: 通过稀疏编码架构提高模型逆推攻击的鲁棒性

    Improving Robustness to Model Inversion Attacks via Sparse Coding Architectures

    [https://arxiv.org/abs/2403.14772](https://arxiv.org/abs/2403.14772)

    通过稀疏编码层设计新网络架构以提高对模型逆推攻击的鲁棒性。

    

    最近的模型逆推攻击算法允许对手通过反复查询神经网络并检查其输出来重建网络的私有训练数据。 在这项工作中，我们开发了一种新颖的网络架构，利用稀疏编码层来获得对这类攻击的卓越鲁棒性。 三十年来，计算机科学研究已经研究了稀疏编码在图像去噪，目标识别和对抗性误分设置中的作用，但据我们所知，其与最先进的隐私漏洞之间的联系尚未被研究。然而，稀疏编码架构提供了一种有利的手段来抵御模型逆推攻击，因为它们允许我们控制编码在网络的中间表示中的无关私人信息的数量，而这种方式可以在训练过程中高效计算，并且众所周知只有较小的影响。

    arXiv:2403.14772v1 Announce Type: cross  Abstract: Recent model inversion attack algorithms permit adversaries to reconstruct a neural network's private training data just by repeatedly querying the network and inspecting its outputs. In this work, we develop a novel network architecture that leverages sparse-coding layers to obtain superior robustness to this class of attacks. Three decades of computer science research has studied sparse coding in the context of image denoising, object recognition, and adversarial misclassification settings, but to the best of our knowledge, its connection to state-of-the-art privacy vulnerabilities remains unstudied. However, sparse coding architectures suggest an advantageous means to defend against model inversion attacks because they allow us to control the amount of irrelevant private information encoded in a network's intermediate representations in a manner that can be computed efficiently during training and that is known to have little effect
    
[^5]: EAS-SNN：端到端自适应采样和表示，用于循环脉冲神经网络的事件检测

    EAS-SNN: End-to-End Adaptive Sampling and Representation for Event-based Detection with Recurrent Spiking Neural Networks

    [https://arxiv.org/abs/2403.12574](https://arxiv.org/abs/2403.12574)

    提出了一种利用循环脉冲神经网络的自适应采样模块，通过将脉冲神经元的神经动力学与理想的时间事件采样器的行为相结合，实现了端到端可学习的事件检测框架

    

    事件摄像头以其高动态范围和时间分辨率，特别适用于物体检测，尤其是在存在动态模糊和具有挑战性的光照条件的情况下。然而，大多数现有方法更注重优化具有先进检测骨干和早期聚合功能的时空表示，而自适应事件采样的关键问题仍未得到解决。脉冲神经网络（SNN），通过稀疏脉冲通信运行的事件驱动范式，成为解决这一挑战的天然选择。在这项研究中，我们发现脉冲神经元的神经动力学与理想的时间事件采样器的行为密切相符。在这一启发下，我们提出了一个新颖的自适应采样模块，利用具有时间记忆的循环卷积SNN增强，为基于事件检测的完全端到端可学习框架提供支持。

    arXiv:2403.12574v1 Announce Type: cross  Abstract: Event cameras, with their high dynamic range and temporal resolution, are ideally suited for object detection, especially under scenarios with motion blur and challenging lighting conditions. However, while most existing approaches prioritize optimizing spatiotemporal representations with advanced detection backbones and early aggregation functions, the crucial issue of adaptive event sampling remains largely unaddressed. Spiking Neural Networks (SNNs), which operate on an event-driven paradigm through sparse spike communication, emerge as a natural fit for addressing this challenge. In this study, we discover that the neural dynamics of spiking neurons align closely with the behavior of an ideal temporal event sampler. Motivated by this insight, we propose a novel adaptive sampling module that leverages recurrent convolutional SNNs enhanced with temporal memory, facilitating a fully end-to-end learnable framework for event-based detec
    
[^6]: 对齐与提炼：统一和改进领域自适应目标检测

    Align and Distill: Unifying and Improving Domain Adaptive Object Detection

    [https://arxiv.org/abs/2403.12029](https://arxiv.org/abs/2403.12029)

    引入了统一的基准测试和实现框架ALDI以及新的DAOD基准数据集CFC-DAOD，解决了领域自适应目标检测中的基准问题，并支持未来方法的发展。

    

    目标检测器通常表现不佳于与其训练集不同的数据。最近，领域自适应目标检测（DAOD）方法已经展示了在应对这一挑战上的强大结果。遗憾的是，我们发现了系统化的基准测试陷阱，这些陷阱对过去的结果提出质疑并阻碍了进一步的进展：（a）由于基线不足导致性能高估，（b）不一致的实现实践阻止了方法的透明比较，（c）由于过时的骨干和基准测试缺乏多样性，导致缺乏普遍性。我们通过引入以下问题来解决这些问题：（1）一个统一的基准测试和实现框架，Align and Distill（ALDI），支持DAOD方法的比较并支持未来发展，（2）一个公平且现代的DAOD训练和评估协议，解决了基准测试的陷阱，（3）一个新的DAOD基准数据集，CFC-DAOD，能够在多样化的真实环境中进行评估。

    arXiv:2403.12029v1 Announce Type: cross  Abstract: Object detectors often perform poorly on data that differs from their training set. Domain adaptive object detection (DAOD) methods have recently demonstrated strong results on addressing this challenge. Unfortunately, we identify systemic benchmarking pitfalls that call past results into question and hamper further progress: (a) Overestimation of performance due to underpowered baselines, (b) Inconsistent implementation practices preventing transparent comparisons of methods, and (c) Lack of generality due to outdated backbones and lack of diversity in benchmarks. We address these problems by introducing: (1) A unified benchmarking and implementation framework, Align and Distill (ALDI), enabling comparison of DAOD methods and supporting future development, (2) A fair and modern training and evaluation protocol for DAOD that addresses benchmarking pitfalls, (3) A new DAOD benchmark dataset, CFC-DAOD, enabling evaluation on diverse real
    
[^7]: 使用StateFlow增强LLM任务解决能力通过状态驱动工作流

    StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows

    [https://arxiv.org/abs/2403.11322](https://arxiv.org/abs/2403.11322)

    提出了一种使用StateFlow的新颖LLM任务解决范式，将复杂任务解决过程概念化为状态机，通过状态转换确保LLM响应的清晰跟踪和管理。

    

    使用大型语言模型（LLM）来解决复杂任务的趋势日益明显，例如需要一系列操作和与工具环境动态交互的任务。本文提出了StateFlow，一种新颖的基于LLM的任务求解范式，将由LLM支持的复杂任务解决过程概念化为状态机。通过正确构建状态和定义状态转换，StateFlow确定了任务求解的进展，确保清晰跟踪和管理LLM在整个任务求解过程中的响应。在每个状态中，StateFlow允许执行一系列动作，不仅包括根据特定提示指导生成LLM响应，还包括根据需要利用外部工具。状态转换由LLM做出的特定规则或决策控制，允许通过任务的预定义StateFlow模型动态自适应地进行进展。

    arXiv:2403.11322v1 Announce Type: cross  Abstract: It is a notable trend to use Large Language Models (LLMs) to tackle complex tasks, e.g., tasks that require a sequence of actions and dynamic interaction with tools and environments. In this paper, we propose StateFlow, a novel LLM-based task-solving paradigm that conceptualizes complex task-solving processes backed by LLMs as state machines. With proper construction of states and definition of state transitions, StateFlow grounds the progress of task-solving, ensuring clear tracking and management of LLMs' responses throughout the task-solving process. Within each state, StateFlow allows execution of a series of actions, involving not only the generation of LLM's responses guided by a specific prompt, but also the utilization of external tools as needed. State transitions are controlled by specific rules or decisions made by the LLM, allowing for a dynamic and adaptive progression through the task's pre-defined StateFlow model. Evalua
    
[^8]: 生成语言模型的非歧视标准

    Non-discrimination Criteria for Generative Language Models

    [https://arxiv.org/abs/2403.08564](https://arxiv.org/abs/2403.08564)

    本文研究如何在生成式语言模型中识别和量化性别偏见，提出了三个生成式人工智能的非歧视标准并设计了相应的提示。

    

    近年来，生成式人工智能，如大型语言模型，经历了快速发展。随着这些模型越来越普遍地提供给公众使用，人们开始担心在应用中延续和放大有害偏见的问题。性别刻板印象可能对其针对的个人造成伤害和限制，无论是由误传还是歧视所构成。识别性别偏见作为一种普遍的社会构造，本文研究如何发现和量化生成式语言模型中性别偏见的存在。具体而言，我们推导出三个来自分类的著名非歧视标准的生成式人工智能类比，即独立性、分离性和充分性。为了展示这些标准的作用，我们设计了针对每个标准的提示，重点关注职业性别刻板印象，具体利用医学测试来在生成式人工智能背景中引入基本事实。

    arXiv:2403.08564v1 Announce Type: cross  Abstract: Within recent years, generative AI, such as large language models, has undergone rapid development. As these models become increasingly available to the public, concerns arise about perpetuating and amplifying harmful biases in applications. Gender stereotypes can be harmful and limiting for the individuals they target, whether they consist of misrepresentation or discrimination. Recognizing gender bias as a pervasive societal construct, this paper studies how to uncover and quantify the presence of gender biases in generative language models. In particular, we derive generative AI analogues of three well-known non-discrimination criteria from classification, namely independence, separation and sufficiency. To demonstrate these criteria in action, we design prompts for each of the criteria with a focus on occupational gender stereotype, specifically utilizing the medical test to introduce the ground truth in the generative AI context. 
    
[^9]: 用于生成简要住院病程摘要的领域自适应大语言模型的基准测试

    A Benchmark of Domain-Adapted Large Language Models for Generating Brief Hospital Course Summaries

    [https://arxiv.org/abs/2403.05720](https://arxiv.org/abs/2403.05720)

    介绍了一个新的基准测试，评估了用于生成简要住院病程摘要的大语言模型在健康保健领域中的性能并提出相应的自适应策略

    

    简要住院病程（BHC）摘要是通过总结临床记录而生成的常见临床文件。虽然大型语言模型（LLMs）在自动化实际任务方面展现出显著能力，但它们在医疗应用（如BHC合成）中的能力尚未得到展示。为了使LLMs能够适应BHC合成，我们引入了一个新颖的基准测试，其中包含从MIMIC-IV记录中提取的经过预处理的数据集，封装了临床记录和简要住院病程（BHC）对。我们评估了两个通用LLMs和三个医疗领域适应的LLMs的性能，以改进从临床记录生成BHC。我们使用临床记录作为输入来生成BHC，采用基于提示的（使用上下文学习）和基于微调的自适应策略来应用于三个开源LLMs（Clinical-T5-Large，Llama2-13B，FLAN-UL2）和两个专有LLMs（GPT-3.5，GPT-4）。我们定量评估了性能。

    arXiv:2403.05720v1 Announce Type: cross  Abstract: Brief hospital course (BHC) summaries are common clinical documents generated by summarizing clinical notes. While large language models (LLMs) depict remarkable capabilities in automating real-world tasks, their capabilities for healthcare applications such as BHC synthesis have not been shown. To enable the adaptation of LLMs for BHC synthesis, we introduce a novel benchmark consisting of a pre-processed dataset extracted from MIMIC-IV notes, encapsulating clinical note, and brief hospital course (BHC) pairs. We assess the performance of two general-purpose LLMs and three healthcare-adapted LLMs to improve BHC synthesis from clinical notes. Using clinical notes as input for generating BHCs, we apply prompting-based (using in-context learning) and fine-tuning-based adaptation strategies to three open-source LLMs (Clinical-T5-Large, Llama2-13B, FLAN-UL2) and two proprietary LLMs (GPT-3.5, GPT-4). We quantitatively evaluate the performa
    
[^10]: SheetAgent：通过大型语言模型进行电子表格推理和操作的通用代理

    SheetAgent: A Generalist Agent for Spreadsheet Reasoning and Manipulation via Large Language Models

    [https://arxiv.org/abs/2403.03636](https://arxiv.org/abs/2403.03636)

    SheetAgent是一种利用大型语言模型进行电子表格推理和操作的通用代理，提供了处理复杂现实任务的解决方案

    

    电子表格操作广泛存在于大多数日常工作中，并显著提高了工作效率。最近尝试使用大型语言模型(LLM)进行自动电子表格操作，但尚未在存在推理挑战的复杂和现实任务中进行探究（例如，具有多步推理和模糊要求的长视野操作）。为了弥合与真实世界要求之间的差距，我们引入了$\textbf{SheetRM}$，一个特点是长视野和多类任务的基准，具有推理相关操纵，由真实挑战引起。为了缓解以上挑战，我们进一步提出了$\textbf{SheetAgent}$，一种利用LLMs能力的新型自主代理。SheetAgent由三个协作模块组成：$\textit{Planner}$、$\textit{Informer}$和$\textit{Retriever}$，实现了对电子表格的高级推理和准确操作，而不需人类

    arXiv:2403.03636v1 Announce Type: new  Abstract: Spreadsheet manipulation is widely existing in most daily works and significantly improves working efficiency. Large language model (LLM) has been recently attempted for automatic spreadsheet manipulation but has not yet been investigated in complicated and realistic tasks where reasoning challenges exist (e.g., long horizon manipulation with multi-step reasoning and ambiguous requirements). To bridge the gap with the real-world requirements, we introduce $\textbf{SheetRM}$, a benchmark featuring long-horizon and multi-category tasks with reasoning-dependent manipulation caused by real-life challenges. To mitigate the above challenges, we further propose $\textbf{SheetAgent}$, a novel autonomous agent that utilizes the power of LLMs. SheetAgent consists of three collaborative modules: $\textit{Planner}$, $\textit{Informer}$, and $\textit{Retriever}$, achieving both advanced reasoning and accurate manipulation over spreadsheets without hu
    
[^11]: QuanTest：基于纠缠的量子神经网络系统测试

    QuanTest: Entanglement-Guided Testing of Quantum Neural Network Systems

    [https://arxiv.org/abs/2402.12950](https://arxiv.org/abs/2402.12950)

    QuanTest是一个基于纠缠的对抗测试框架，旨在揭示量子神经网络系统中的潜在错误行为。

    

    Quantum Neural Network（QNN）将深度学习（DL）原理与量子力学基本理论相结合，实现具有量子加速的机器学习任务。 最近发现，QNN系统有类似于经典DL系统的鲁棒性问题。 急需一种方法来测试它们的正确性和安全性。 但是，QNN系统与传统量子软件和经典DL系统有很大不同，对QNN测试提出了重大挑战。 这些挑战包括传统量子软件测试方法的不适用性，量子测试样本生成对扰动算子的依赖，以及量子神经元中缺乏有效信息。 本文提出了QuanTest，这是一个基于量子纠缠的对抗测试框架，用于发现QNN系统中潜在的错误行为。 我们设计了一个量子纠缠充分性准则来量化

    arXiv:2402.12950v1 Announce Type: cross  Abstract: Quantum Neural Network (QNN) combines the Deep Learning (DL) principle with the fundamental theory of quantum mechanics to achieve machine learning tasks with quantum acceleration. Recently, QNN systems have been found to manifest robustness issues similar to classical DL systems. There is an urgent need for ways to test their correctness and security. However, QNN systems differ significantly from traditional quantum software and classical DL systems, posing critical challenges for QNN testing. These challenges include the inapplicability of traditional quantum software testing methods, the dependence of quantum test sample generation on perturbation operators, and the absence of effective information in quantum neurons. In this paper, we propose QuanTest, a quantum entanglement-guided adversarial testing framework to uncover potential erroneous behaviors in QNN systems. We design a quantum entanglement adequacy criterion to quantify 
    
[^12]: 提高时间序列表示学习的双向生成预训练模型

    Bidirectional Generative Pre-training for Improving Time Series Representation Learning

    [https://arxiv.org/abs/2402.09558](https://arxiv.org/abs/2402.09558)

    这项论文提出了一种名为BiTimelyGPT的模型，通过双向的预训练任务在时间序列数据上学习表示，展示了优越的性能，可用于神经功能预测、疾病诊断和生理病征识别。

    

    学习时间序列表示以用于判别任务一直是一项长期的挑战。当前的预训练方法要么是单向的下一个标记预测，要么是随机屏蔽标记预测。我们提出了一种新颖的架构，称为双向及时生成预训练Transformer（BiTimelyGPT），它通过交替的Transformer层在时间序列数据上进行了下一个标记和上一个标记的预测。这种预训练任务保留了时间序列的原始分布和数据形状。此外，全秩前向和后向注意力矩阵具有更具表现力的表示能力。 使用生物信号数据，BiTimelyGPT在预测神经功能、疾病诊断和生理病征方面表现出了优越性能。通过可视化注意力热图，我们观察到预训练的BiTimelyGPT能够从时间序列中识别出具有判别性的片段。

    arXiv:2402.09558v1 Announce Type: new  Abstract: Learning time-series representations for discriminative tasks has been a long-standing challenge. Current pre-training methods are limited in either unidirectional next-token prediction or randomly masked token prediction. We propose a novel architecture called Bidirectional Timely Generative Pre-trained Transformer (BiTimelyGPT), which pre-trains on time-series data by both next-token and previous-token predictions in alternating transformer layers. This pre-training task preserves original distribution and data shapes of the time-series. Additionally, the full-rank forward and backward attention matrices exhibit more expressive representation capabilities. Using biosignal data, BiTimelyGPT demonstrates superior performance in predicting neurological functionality, disease diagnosis, and physiological signs. By visualizing the attention heatmap, we observe that the pre-trained BiTimelyGPT can identify discriminative segments from time-s
    
[^13]: 关于随机梯度下降（SGD）的收敛速度及其在修改的多臂赌博机上的策略梯度应用

    On the Convergence Rate of the Stochastic Gradient Descent (SGD) and application to a modified policy gradient for the Multi Armed Bandit

    [https://arxiv.org/abs/2402.06388](https://arxiv.org/abs/2402.06388)

    该论文证明了当学习速率按照逆时间衰减规则时，随机梯度下降（SGD）的收敛速度，并应用于修改的带有L2正则化的策略梯度多臂赌博机（MAB）的收敛性分析。

    

    我们提出了一个自包含的证明，证明了当学习速率遵循逆时间衰减规则时，随机梯度下降（SGD）的收敛速度；接下来，我们将这些结果应用于带有L2正则化的修改的策略梯度多臂赌博机（MAB）的收敛性分析。

    We present a self-contained proof of the convergence rate of the Stochastic Gradient Descent (SGD) when the learning rate follows an inverse time decays schedule; we next apply the results to the convergence of a modified form of policy gradient Multi-Armed Bandit (MAB) with $L2$ regularization.
    
[^14]: 优化协作的人工智能-人类混合团队中的授权

    Optimizing Delegation in Collaborative Human-AI Hybrid Teams

    [https://arxiv.org/abs/2402.05605](https://arxiv.org/abs/2402.05605)

    本论文提出了一种优化协作的人工智能-人类混合团队授权的框架，通过引入AI经理（通过强化学习）作为团队的外部观察者，学习团队代理人的行为模型并选择最佳的控制代理人。

    

    当人类和自主系统作为混合团队共同运作时，我们希望确保团队的成功和效率。我们将团队成员称为代理人。在我们提出的框架中，我们解决了混合团队的情况，即在任何时候，只有一个团队成员（控制代理人）被授权为团队的控制者。为了确定最佳的控制代理人选择，我们提出了引入AI经理（通过强化学习）的想法，该经理作为团队的外部观察者学习。经理通过观察代理人的表现和团队所处的环境/世界来学习行为模型，并基于这些观察结果选择出最理想的控制代理人。为了限定经理的任务，我们引入了一组约束条件。经理的约束条件指示团队的可接受运作方式，因此如果团队进入不可接受并需要经理介入的状态，就会违反约束条件。

    When humans and autonomous systems operate together as what we refer to as a hybrid team, we of course wish to ensure the team operates successfully and effectively. We refer to team members as agents. In our proposed framework, we address the case of hybrid teams in which, at any time, only one team member (the control agent) is authorized to act as control for the team. To determine the best selection of a control agent, we propose the addition of an AI manager (via Reinforcement Learning) which learns as an outside observer of the team. The manager learns a model of behavior linking observations of agent performance and the environment/world the team is operating in, and from these observations makes the most desirable selection of a control agent. We restrict the manager task by introducing a set of constraints. The manager constraints indicate acceptable team operation, so a violation occurs if the team enters a condition which is unacceptable and requires manager intervention. To
    
[^15]: 使用Transformer学习决策树算法

    Learning a Decision Tree Algorithm with Transformers

    [https://arxiv.org/abs/2402.03774](https://arxiv.org/abs/2402.03774)

    该论文介绍了MetaTree模型，它使用经典算法的输出训练基于Transformer的模型，以产生具有强大概括性能的决策树。

    

    决策树因其可解释性和在表格数据上实现高预测性能而闻名。传统上，决策树是通过递归算法构建的，在树的每个节点上将数据进行分区。然而，确定最佳分区是具有挑战性的，因为针对局部段优化的决策树可能无法带来全局概括。为了解决这个问题，我们引入了MetaTree，该模型使用经典算法的过滤输出来训练基于Transformer的模型，以产生强大的分类决策树。具体而言，我们在大量数据集上拟合贪婪决策树和优化决策树。然后，我们训练MetaTree产生具有强大概括性能的决策树。这种训练使MetaTree不仅可以模拟这些算法，还可以根据上下文智能地调整策略，从而实现更强的概括性能。

    Decision trees are renowned for their interpretability capability to achieve high predictive performance, especially on tabular data. Traditionally, they are constructed through recursive algorithms, where they partition the data at every node in a tree. However, identifying the best partition is challenging, as decision trees optimized for local segments may not bring global generalization. To address this, we introduce MetaTree, which trains a transformer-based model on filtered outputs from classical algorithms to produce strong decision trees for classification. Specifically, we fit both greedy decision trees and optimized decision trees on a large number of datasets. We then train MetaTree to produce the trees that achieve strong generalization performance. This training enables MetaTree to not only emulate these algorithms, but also to intelligently adapt its strategy according to the context, thereby achieving superior generalization performance.
    
[^16]: DefInt：一种用于高效处理混合大型语言模型推理的默认干预框架

    DefInt: A Default-interventionist Framework for Efficient Reasoning with Hybrid Large Language Models

    [https://arxiv.org/abs/2402.02563](https://arxiv.org/abs/2402.02563)

    DefInt提出了一种默认干预框架，通过默认使用较小规模的语言模型生成推理思路，然后通过反思推理干预解决复杂推理问题，从而提高混合大型语言模型的效率和准确性。

    

    大型语言模型（LLMs）在各种任务中展示出令人印象深刻的新能力，但在处理复杂推理问题方面仍面临挑战。以往的研究如连锁推理（CoT）和思维树（ToT）主要关注提高准确性，但忽视了不断增加的标记成本，这对于具有巨大解空间的开放性实际任务来说可能特别问题。受人类认知的双过程理论的启发，我们提出了一种默认干预框架（DefInt），以释放混合LLMs的协同潜力。默认情况下，DefInt使用较小规模的语言模型生成低成本的推理思路，类似于“系统1”产生的快速直觉。如果这些直觉被认为低置信度，则DefInt将调用放大的语言模型的反思推理作为“系统2”的干预，可以覆盖默认思考并纠正推理过程。实验在五个实际数据集上展示了DefInt论文中的有效性。

    Large language models (LLMs) have shown impressive emergent abilities in a wide range of tasks, but still face challenges in handling complex reasoning problems. Previous works like chain-of-thought (CoT) and tree-of-thoughts(ToT) have predominately focused on enhancing accuracy, but overlook the rapidly increasing token cost, which could be particularly problematic for open-ended real-world tasks with huge solution spaces. Motivated by the dual process theory of human cognition, we propose a Default-Interventionist framework (DefInt) to unleash the synergistic potential of hybrid LLMs. By default, DefInt uses smaller-scale language models to generate low-cost reasoning thoughts, which resembles the fast intuitions produced by System 1. If the intuitions are considered with low confidence, DefInt will invoke the reflective reasoning of scaled-up language models as the intervention of System 2, which can override the default thoughts and rectify the reasoning process. Experiments on fiv
    
[^17]: Monkey: 大型多模态模型中图像分辨率和文本标签的重要性

    Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models

    [https://arxiv.org/abs/2311.06607](https://arxiv.org/abs/2311.06607)

    Monkey通过提高图像分辨率和采用多级描述生成方法来增强大型多模态模型(LMMs)的能力，从而实现更详细的视觉捕捉和更有效的学习。

    

    大型多模态模型(LMMs)在视觉语言任务中表现出了潜力，但在高分辨率输入和详细场景理解方面表现不佳。为了解决这些挑战，我们引入了Monkey来增强LMM的能力。首先，Monkey通过将输入图像划分为统一的补丁来处理图像，每个补丁的大小与原来训练良好的视觉编码器使用的大小(例如448x448)相匹配。配备了每个补丁的适配器，Monkey可以处理高达1344x896像素的更高分辨率，实现对复杂视觉信息的详细捕捉。其次，它采用多级描述生成方法，丰富了场景-对象关联的上下文。这种两部分策略确保了从生成数据中更有效的学习：更高的分辨率允许对视觉进行更详细的捕捉，从而增强了全面描述的效果。广泛的实验证明...

    arXiv:2311.06607v3 Announce Type: replace-cross  Abstract: Large Multimodal Models (LMMs) have shown promise in vision-language tasks but struggle with high-resolution input and detailed scene understanding. Addressing these challenges, we introduce Monkey to enhance LMM capabilities. Firstly, Monkey processes input images by dividing them into uniform patches, each matching the size (e.g., 448x448) used in the original training of the well-trained vision encoder. Equipped with individual adapter for each patch, Monkey can handle higher resolutions up to 1344x896 pixels, enabling the detailed capture of complex visual information. Secondly, it employs a multi-level description generation method, enriching the context for scene-object associations. This two-part strategy ensures more effective learning from generated data: the higher resolution allows for a more detailed capture of visuals, which in turn enhances the effectiveness of comprehensive descriptions. Extensive ablative result
    
[^18]: TabRepo：一个大规模的表格模型评估库及其AutoML应用

    TabRepo: A Large Scale Repository of Tabular Model Evaluations and its AutoML Applications

    [https://arxiv.org/abs/2311.02971](https://arxiv.org/abs/2311.02971)

    TabRepo引入了一个包含1310个模型在200个分类和回归数据集上评估预测结果和指标信息的数据集，展示了它在进行超参数优化、AutoML系统比较和迁移学习等方面的优势。

    

    我们介绍了TabRepo，这是一个包含1310个模型在200个分类和回归数据集上评估预测结果和指标的新数据集。我们展示了我们的数据集的多种好处。首先，我们表明这个数据集可以进行诸如比较超参数优化与当前AutoML系统以及在使用预计算模型预测的同时考虑集成的分析。其次，我们展示了我们的数据集可以轻松用于执行迁移学习。特别是，我们展示应用标准迁移学习技术可以在准确性、运行时间和延迟方面胜过当前最先进的表格系统。

    arXiv:2311.02971v2 Announce Type: replace-cross  Abstract: We introduce TabRepo, a new dataset of tabular model evaluations and predictions. TabRepo contains the predictions and metrics of 1310 models evaluated on 200 classification and regression datasets. We illustrate the benefit of our dataset in multiple ways. First, we show that it allows to perform analysis such as comparing Hyperparameter Optimization against current AutoML systems while also considering ensembling at marginal cost by using precomputed model predictions. Second, we show that our dataset can be readily leveraged to perform transfer-learning. In particular, we show that applying standard transfer-learning techniques allows to outperform current state-of-the-art tabular systems in accuracy, runtime and latency.
    
[^19]: 让行动胜于雄辩：评估LLM代理在拍卖竞技场中的战略规划与执行

    Put Your Money Where Your Mouth Is: Evaluating Strategic Planning and Execution of LLM Agents in an Auction Arena

    [https://arxiv.org/abs/2310.05746](https://arxiv.org/abs/2310.05746)

    LLM代理在拍卖竞技场展示出了关键的规划和执行技能，这为建模复杂社会互动在竞争背景下的LLMs潜力提供了新途径。

    

    最近大型语言模型（LLM）的发展展示了先进的推理能力，然而自然语言处理的评估通常依赖于静态基准。评估这一点需要测试战略推理能力的环境，这种环境需要在动态的竞争场景中进行长期规划。我们引入了AucArena，这是一个模拟拍卖的新颖评估套件，选择这个设置是因为它非常不可预测，涉及与资源和风险管理相关的许多技能，同时也易于评估。我们进行了使用最先进的LLM驱动竞标代理的受控实验，以评估他们的规划和执行技能。我们的研究表明，诸如GPT-4之类的LLM具有拍卖参与的关键技能，如预算管理和目标遵从，这些技能会随着自适应策略的改进而提高。这突出了LLM在建模竞技背景下的复杂社会互动潜力。

    arXiv:2310.05746v2 Announce Type: replace-cross  Abstract: Recent advancements in Large Language Models (LLMs) showcase advanced reasoning, yet NLP evaluations often depend on static benchmarks. Evaluating this necessitates environments that test strategic reasoning in dynamic, competitive scenarios requiring long-term planning. We introduce AucArena, a novel evaluation suite that simulates auctions, a setting chosen for being highly unpredictable and involving many skills related to resource and risk management, while also being easy to evaluate. We conduct controlled experiments using state-of-the-art LLMs to power bidding agents to benchmark their planning and execution skills. Our research demonstrates that LLMs, such as GPT-4, possess key skills for auction participation, such as budget management and goal adherence, which improve with adaptive strategies. This highlights LLMs' potential in modeling complex social interactions in competitive contexts. However, variability in LLM p
    
[^20]: 强化学习用于多功能、动态和稳健的双足运动控制

    Reinforcement Learning for Versatile, Dynamic, and Robust Bipedal Locomotion Control. (arXiv:2401.16889v1 [cs.RO])

    [http://arxiv.org/abs/2401.16889](http://arxiv.org/abs/2401.16889)

    本论文使用深度强化学习创建了一种通用的双足机器人动态运动控制器，该控制器可以应用于多种动态双足技能，并且在模拟环境和实际环境中展现出了优越性能。

    

    本论文提出了一项关于使用深度强化学习（RL）创建双足机器人动态运动控制器的综合研究。我们不仅仅专注于单一的运动技能，而是开发了一种通用的控制解决方案，可以用于一系列动态双足技能，从周期性行走和奔跑到非周期性跳跃和站立。我们基于RL的控制器采用了一种新颖的双历史架构，利用机器人的长期和短期输入/输出（I/O）历史。通过提出的端到端RL方法进行训练时，这种控制架构在模拟环境和实际环境中的多样化技能上始终表现优于其他方法。该研究还深入探讨了所提出的RL系统在开发运动控制器方面引入的适应性和稳健性。我们证明了所提出的架构可以适应时间不变的动力学变化和时间变化的变化，如接触事件，通过有效地

    This paper presents a comprehensive study on using deep reinforcement learning (RL) to create dynamic locomotion controllers for bipedal robots. Going beyond focusing on a single locomotion skill, we develop a general control solution that can be used for a range of dynamic bipedal skills, from periodic walking and running to aperiodic jumping and standing. Our RL-based controller incorporates a novel dual-history architecture, utilizing both a long-term and short-term input/output (I/O) history of the robot. This control architecture, when trained through the proposed end-to-end RL approach, consistently outperforms other methods across a diverse range of skills in both simulation and the real world.The study also delves into the adaptivity and robustness introduced by the proposed RL system in developing locomotion controllers. We demonstrate that the proposed architecture can adapt to both time-invariant dynamics shifts and time-variant changes, such as contact events, by effectivel
    
[^21]: 将上下文带回来：多模态知识图谱上的相机陷阱物种识别作为链接预测

    Bringing Back the Context: Camera Trap Species Identification as Link Prediction on Multimodal Knowledge Graphs. (arXiv:2401.00608v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2401.00608](http://arxiv.org/abs/2401.00608)

    本研究利用相机陷阱图像的结构化上下文，提高其在物种识别任务中的泛化能力，并解决了数据稀缺和泛化能力增强的问题。

    

    相机陷阱在动物生态学中是宝贵的工具，用于生物多样性监测和保护。然而，挑战如在新的未知位置部署时的糟糕泛化限制了它们的实际应用。图像自然与可能在不同模态下的异质上下文相关联。在这项工作中，我们利用与相机陷阱图像相关联的结构化上下文，改善在相机陷阱中物种识别这个任务的超出分布的泛化能力。例如，一张野生动物的照片可能与拍摄地点和时间以及关于动物物种的结构化生物学知识相关联。虽然现有的工作通常忽视这一点，但将这样的上下文带回来可以带来一些潜在的好处，如解决数据稀缺和增强泛化能力。然而，有效地将这样的异质上下文整合到视觉领域是一个具有挑战性的问题。

    Camera traps are valuable tools in animal ecology for biodiversity monitoring and conservation. However, challenges like poor generalization to deployment at new unseen locations limit their practical application. Images are naturally associated with heterogeneous forms of context possibly in different modalities. In this work, we leverage the structured context associated with the camera trap images to improve out-of-distribution generalization for the task of species identification in camera traps. For example, a photo of a wild animal may be associated with information about where and when it was taken, as well as structured biology knowledge about the animal species. While typically overlooked by existing work, bringing back such context offers several potential benefits for better image understanding, such as addressing data scarcity and enhancing generalization. However, effectively integrating such heterogeneous context into the visual domain is a challenging problem. To address
    
[^22]: 自监督分解表示学习用于鲁棒目标语音提取

    Self-Supervised Disentangled Representation Learning for Robust Target Speech Extraction. (arXiv:2312.10305v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2312.10305](http://arxiv.org/abs/2312.10305)

    该论文提出了一种自监督分解表示学习方法，通过逐步分离说话人身份信息和其他无关因素，解决了目标语音提取任务中存在的说话人混叠问题，并使用分解的说话人身份信息来指导语音提取网络。

    

    语音信号本质上是复杂的，因为它包含全局声学特征和局部语义信息。然而，在目标语音提取任务中，参考语音中与说话人身份无关的全局和局部语义信息可能导致在语音提取网络中出现说话人混叠问题。为了克服这个挑战，我们提出了一种自监督分解表示学习方法。我们的方法通过一个两阶段过程来解决这个问题，利用参考语音编码网络和全局信息分解网络逐渐分解说话人身份信息和其他不相关因素。我们专门使用分解的说话人身份信息来指导语音提取网络。此外，我们引入自适应调制Transformer来确保混合信号的声学表示不受说话人嵌入的影响。

    Speech signals are inherently complex as they encompass both global acoustic characteristics and local semantic information. However, in the task of target speech extraction, certain elements of global and local semantic information in the reference speech, which are irrelevant to speaker identity, can lead to speaker confusion within the speech extraction network. To overcome this challenge, we propose a self-supervised disentangled representation learning method. Our approach tackles this issue through a two-phase process, utilizing a reference speech encoding network and a global information disentanglement network to gradually disentangle the speaker identity information from other irrelevant factors. We exclusively employ the disentangled speaker identity information to guide the speech extraction network. Moreover, we introduce the adaptive modulation Transformer to ensure that the acoustic representation of the mixed signal remains undisturbed by the speaker embeddings. This com
    
[^23]: 最小修改马尔可夫博弈以实现任意Nash均衡和价值

    Minimally Modifying a Markov Game to Achieve Any Nash Equilibrium and Value. (arXiv:2311.00582v1 [cs.GT])

    [http://arxiv.org/abs/2311.00582](http://arxiv.org/abs/2311.00582)

    该论文研究了游戏修改问题，提出了一种最小修改马尔可夫博弈的方法，使得目标策略配置成为唯一的Nash均衡并具有特定价值范围，同时最小化修改成本。

    

    我们研究了游戏修改问题，其中一位善意的游戏设计者或恶意的对手修改了一个零和马尔可夫博弈的奖励函数，以便一个目标确定性或随机的策略配置成为唯一的马尔可夫完美Nash均衡，并且在目标范围内具有价值，以最小化修改成本。我们表征了能够安装为某个游戏的唯一均衡的策略配置的集合，并建立了成功安装的充分和必要条件。我们提出了一种高效的算法，该算法通过解一个带有线性约束的凸优化问题，然后进行随机扰动，来获得一个成本近乎最优的修改计划。

    We study the game modification problem, where a benevolent game designer or a malevolent adversary modifies the reward function of a zero-sum Markov game so that a target deterministic or stochastic policy profile becomes the unique Markov perfect Nash equilibrium and has a value within a target range, in a way that minimizes the modification cost. We characterize the set of policy profiles that can be installed as the unique equilibrium of some game, and establish sufficient and necessary conditions for successful installation. We propose an efficient algorithm, which solves a convex optimization problem with linear constraints and then performs random perturbation, to obtain a modification plan with a near-optimal cost.
    
[^24]: KLoB: 一种评估语言模型中知识定位方法的基准

    KLoB: a Benchmark for Assessing Knowledge Locating Methods in Language Models. (arXiv:2309.16535v1 [cs.CL])

    [http://arxiv.org/abs/2309.16535](http://arxiv.org/abs/2309.16535)

    KLoB是一个评估语言模型中知识定位方法的基准，旨在解决现有定位方法的准确性和事实知识局部性假设的问题。

    

    最近，定位然后编辑的范式已经成为改变语言模型中存储的事实知识的主要方法之一。然而，目前的定位方法是否能够准确地找到嵌入所需知识的确切参数还缺乏研究。此外，尽管许多研究人员对事实知识的局部性假设的有效性提出了质疑，但没有提供一种测试假设的方法以进行更深入的讨论和研究。因此，我们引入了KLoB，一个评估可靠的知识定位方法应满足的三个基本属性的基准。KLoB可作为评估语言模型中现有定位方法的基准，并为重新评估事实知识的局部性假设提供了一种方法。我们的代码公开可用于\url{https://github.com/juyiming/KLoB}。

    Recently, Locate-Then-Edit paradigm has emerged as one of the main approaches in changing factual knowledge stored in the Language models. However, there is a lack of research on whether present locating methods can pinpoint the exact parameters embedding the desired knowledge. Moreover, although many researchers have questioned the validity of locality hypothesis of factual knowledge, no method is provided to test the a hypothesis for more in-depth discussion and research. Therefore, we introduce KLoB, a benchmark examining three essential properties that a reliable knowledge locating method should satisfy. KLoB can serve as a benchmark for evaluating existing locating methods in language models, and can contributes a method to reassessing the validity of locality hypothesis of factual knowledge. Our is publicly available at \url{https://github.com/juyiming/KLoB}.
    
[^25]: Lyra: 自动定理证明中的双重修正策略的编排

    Lyra: Orchestrating Dual Correction in Automated Theorem Proving. (arXiv:2309.15806v1 [cs.CL])

    [http://arxiv.org/abs/2309.15806](http://arxiv.org/abs/2309.15806)

    Lyra是一种新的框架，通过引入工具修正和猜想修正两种机制，增强了大规模语言模型在形式化定理证明领域的有效性，减轻了幻觉，并提高了证明的准确性。

    

    大规模语言模型（LLMs）为形式化定理证明领域提供了一个有趣的探索途径。然而，它们的全部潜力，尤其是关于幻觉的减轻和通过证明器错误消息的细化，仍然是一个尚未深入研究的领域。为了增强LLMs在该领域的有效性，我们引入了Lyra，一种采用两种不同修正机制的新框架：工具修正（TC）和猜想修正（CC）。为了在形式证明的后处理中实现工具修正，我们利用先前的知识来利用预定义的证明工具（如Sledgehammer）来指导替换不正确的工具。工具修正显著减轻了幻觉，从而提高了证明的整体准确性。此外，我们引入了猜想修正，一种错误反馈机制，旨在与证明器互动，通过证明器的错误消息进一步完善形式证明的猜想。

    Large Language Models (LLMs) present an intriguing avenue for exploration in the field of formal theorem proving. Nevertheless, their full potential, particularly concerning the mitigation of hallucinations and refinement through prover error messages, remains an area that has yet to be thoroughly investigated. To enhance the effectiveness of LLMs in the field, we introduce the Lyra, a new framework that employs two distinct correction mechanisms: Tool Correction (TC) and Conjecture Correction (CC). To implement Tool Correction in the post-processing of formal proofs, we leverage prior knowledge to utilize predefined prover tools (e.g., Sledgehammer) for guiding the replacement of incorrect tools. Tool Correction significantly contributes to mitigating hallucinations, thereby improving the overall accuracy of the proof. In addition, we introduce Conjecture Correction, an error feedback mechanism designed to interact with prover to refine formal proof conjectures with prover error messa
    
[^26]: 使用更高的批评力量将人类触摸与AI生成的文本分离：一种信息论方法

    Separating the Human Touch from AI-Generated Text using Higher Criticism: An Information-Theoretic Approach. (arXiv:2308.12747v1 [cs.IT])

    [http://arxiv.org/abs/2308.12747](http://arxiv.org/abs/2308.12747)

    本文提出了一种使用更高的批评力量将人类触摸与AI生成的文本分离的方法，通过多个困惑度测试和统计模型的结合，能够有效识别出可能被编辑的部分，并探讨了方法的有效性及相关挑战。

    

    我们提出了一种方法，用于确定一篇给定文章是完全由生成性语言模型编写的，还是包含了一些由其他作者（可能是人类）进行了重要编辑的替代情况。我们的过程涉及对个别句子或其他文本单位起源进行的多个困惑度测试，将这些多个测试结合起来使用更高的批评力量（HC）。作为副产品，该方法还能够识别出可能被编辑的部分。该方法受到了对数困惑度收敛到交叉熵率的启发，以及一个统计模型来描述被编辑的文本，即句子主要由语言模型生成，但可能有少数句子是通过其他机制产生的。我们使用真实数据证明了我们方法的有效性，并分析了影响其成功的因素。这个分析提出了一些有趣的开放挑战，解决这些挑战可能会改善该方法的有效性。

    We propose a method to determine whether a given article was entirely written by a generative language model versus an alternative situation in which the article includes some significant edits by a different author, possibly a human. Our process involves many perplexity tests for the origin of individual sentences or other text atoms, combining these multiple tests using Higher Criticism (HC). As a by-product, the method identifies parts suspected to be edited. The method is motivated by the convergence of the log-perplexity to the cross-entropy rate and by a statistical model for edited text saying that sentences are mostly generated by the language model, except perhaps for a few sentences that might have originated via a different mechanism. We demonstrate the effectiveness of our method using real data and analyze the factors affecting its success. This analysis raises several interesting open challenges whose resolution may improve the method's effectiveness.
    
[^27]: 快速无监督深度异常值模型选择与超网络

    Fast Unsupervised Deep Outlier Model Selection with Hypernetworks. (arXiv:2307.10529v1 [cs.LG])

    [http://arxiv.org/abs/2307.10529](http://arxiv.org/abs/2307.10529)

    本文提出了HYPER用于调整基于深度神经网络的异常值检测模型，解决了无监督DOD模型中的超参数调整和模型选择的挑战，通过设计和训练超网络(HN)将超参数映射到主要DOD模型的最优权重上。

    

    异常值检测(OD)在许多领域都有应用，并有许多技术的丰富文献。基于深度神经网络的OD(DOD)由于深度学习的许多进展而受到了最近的关注。在本文中，我们考虑了一个关键但鲜为人知的问题，即无监督DOD的有效超参数(HP)调整/模型选择。虽然一些先前的工作报告了OD模型对HP的敏感性，但对于展示了长列表HP的现代DOD模型来说，这变得非常关键。我们引入了HYPER来调整DOD模型，解决了两个基本挑战：(1)无监督情况下的验证(由于缺乏标记的异常值)，以及(2) HP/模型空间的高效搜索 (由于HP数量的指数增长)。关键思想是设计和训练一个新颖的超网络(HN)，其将HP映射到主要DOD模型的最优权重上。反过来，HYPER利用一个单独的HN，可以动态生成多个DOD模型的权重 (对应于...)。

    Outlier detection (OD) finds many applications with a rich literature of numerous techniques. Deep neural network based OD (DOD) has seen a recent surge of attention thanks to the many advances in deep learning. In this paper, we consider a critical-yet-understudied challenge with unsupervised DOD, that is, effective hyperparameter (HP) tuning/model selection. While several prior work report the sensitivity of OD models to HPs, it becomes ever so critical for the modern DOD models that exhibit a long list of HPs. We introduce HYPER for tuning DOD models, tackling two fundamental challenges: (1) validation without supervision (due to lack of labeled anomalies), and (2) efficient search of the HP/model space (due to exponential growth in the number of HPs). A key idea is to design and train a novel hypernetwork (HN) that maps HPs onto optimal weights of the main DOD model. In turn, HYPER capitalizes on a single HN that can dynamically generate weights for many DOD models (corresponding t
    
[^28]: 在离散和连续域中使用矢量表达进行在线目标识别

    Online Goal Recognition in Discrete and Continuous Domains Using a Vectorial Representation. (arXiv:2307.07876v1 [cs.AI])

    [http://arxiv.org/abs/2307.07876](http://arxiv.org/abs/2307.07876)

    该论文提出了一种在离散和连续域中使用矢量表达进行在线目标识别的高效方法，该方法通过减少计算负担和提高识别速度，使其成为首个可用于快速变动环境下的机器人应用的在线方法。

    

    最近对于在线目标识别的研究主要关注在低可观测性下有效推断目标，而对于在离散和连续域中都能工作的在线目标识别的研究较少。在线目标识别方法通常需要在每个新的观测中多次调用规划器，造成了很高的计算成本。在连续空间中快速而可靠地识别目标对于任何轨迹规划问题都是至关重要的，因为真实物理世界是快速变动的，例如机器人应用。我们开发了一种高效的目标识别方法，在离散域中每个可能的目标只需要一次调用规划器，而在连续域中利用简化的运动模型来减小计算负担。由此产生的方法比当前最先进的方法在在线识别方面快几个数量级，使之成为首个实际可用于需要亚秒级响应的机器人应用的在线方法。

    While recent work on online goal recognition efficiently infers goals under low observability, comparatively less work focuses on online goal recognition that works in both discrete and continuous domains. Online goal recognition approaches often rely on repeated calls to the planner at each new observation, incurring high computational costs. Recognizing goals online in continuous space quickly and reliably is critical for any trajectory planning problem since the real physical world is fast-moving, e.g. robot applications. We develop an efficient method for goal recognition that relies either on a single call to the planner for each possible goal in discrete domains or a simplified motion model that reduces the computational burden in continuous ones. The resulting approach performs the online component of recognition orders of magnitude faster than the current state of the art, making it the first online method effectively usable for robotics applications that require sub-second rec
    
[^29]: 通过跳过零元素降低卷积层的计算复杂度

    Reduce Computational Complexity for Convolutional Layers by Skipping Zeros. (arXiv:2306.15951v1 [cs.LG])

    [http://arxiv.org/abs/2306.15951](http://arxiv.org/abs/2306.15951)

    本文提出了C-K-S算法，通过修剪滤波器和转换稀疏张量为稠密张量的方式，跳过卷积层中的0元素，从而降低了计算复杂度。实验证明，C-K-S相对于PyTorch具有优势。

    

    深度神经网络依赖并行处理器进行加速。为了为其设计运算符，需要不仅有优化算法以降低复杂度，还需要充分利用硬件资源。卷积层主要包含三种运算符：前向传播的卷积，反向传播的反卷积和膨胀卷积。当执行这些运算时，始终会向张量中添加0元素，导致冗余计算。本文提出了C-K-S算法（ConvV2, KS-deconv, Sk-dilated），以两种方式跳过这些0元素：修剪滤波器以排除填充的0元素；将稀疏张量转换为稠密张量，避免在反卷积和膨胀卷积中插入0元素。与普通卷积相比，反卷积由于其复杂性而难以加速。本文提供了C-K-S的高性能GPU实现，并通过与PyTorch的比较验证了其有效性。根据实验结果，在某些情况下，C-K-S相对于PyTorch具有优势。

    Deep neural networks rely on parallel processors for acceleration. To design operators for them, it requires not only good algorithm to reduce complexity, but also sufficient utilization of hardwares. Convolutional layers mainly contain 3 kinds of operators: convolution in forward propagation, deconvolution and dilated-convolution in backward propagation. When executing these operators, 0s are always added to tensors, causing redundant calculations. This paper gives C-K-S algorithm (ConvV2, KS-deconv, Sk-dilated), which skips these 0s in two ways: trim the filters to exclude padded 0s; transform sparse tensors to dense tensors, to avoid inserted 0s in deconvolution and dilated-convolution. In contrast to regular convolution, deconvolution is hard to accelerate due to its complicacy. This paper provides high-performance GPU implementations of C-K-S, and verifies their effectiveness with comparison to PyTorch. According to the experiments, C-K-S has advantages over PyTorch in certain cas
    
[^30]: 超越规模：多样性系数作为数据质量指标证明了LLMs是在形式多样的数据上预先训练的

    Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data. (arXiv:2306.13840v1 [cs.CL])

    [http://arxiv.org/abs/2306.13840](http://arxiv.org/abs/2306.13840)

    本论文提出使用多样性系数作为LLM预训练数据质量的指标，研究表明公开可用的LLM数据集的多样性系数很高。

    

    当前，预先训练强大的大语言模型(LLMs)的趋势主要集中在模型和数据集规模的扩大。然而，预先训练数据的质量对于训练强大的LLMs来说是一个重要因素，但它是一个模糊的概念，尚未完全表征。因此，我们使用最近提出的Task2Vec多样性系数来基于数据质量的形式方面，超越规模本身。具体而言，我们测量公开可用的预先训练数据集的多样性系数，以证明它们的形式多样性高于理论的下限和上限。此外，为了建立对多样性系数的信心，我们进行可解释性实验，并发现该系数与多样性的直观属性相吻合，例如，随着潜在概念数量的增加，它增加。我们得出结论，多样性系数是可靠的，表明公开可用的LLM数据集的多样性系数很高，并推测它可以作为预训练LLMs模型的数据质量指标。

    Current trends to pre-train capable Large Language Models (LLMs) mostly focus on scaling of model and dataset size. However, the quality of pre-training data is an important factor for training powerful LLMs, yet it is a nebulous concept that has not been fully characterized. Therefore, we use the recently proposed Task2Vec diversity coefficient to ground and understand formal aspects of data quality, to go beyond scale alone. Specifically, we measure the diversity coefficient of publicly available pre-training datasets to demonstrate that their formal diversity is high when compared to theoretical lower and upper bounds. In addition, to build confidence in the diversity coefficient, we conduct interpretability experiments and find that the coefficient aligns with intuitive properties of diversity, e.g., it increases as the number of latent concepts increases. We conclude the diversity coefficient is reliable, show it's high for publicly available LLM datasets, and conjecture it can be
    
[^31]: MetaGAD：学习元转移进行少样本图异常检测

    MetaGAD: Learning to Meta Transfer for Few-shot Graph Anomaly Detection. (arXiv:2305.10668v1 [cs.LG])

    [http://arxiv.org/abs/2305.10668](http://arxiv.org/abs/2305.10668)

    本文提出了一种名为MetaGAD的框架，用于学习从无标记节点到有标记节点之间的元转移知识，以进行少样本图异常检测。

    

    图异常检测长期以来一直是各个领域信息安全问题中的重要问题，如金融欺诈、社会垃圾邮件、网络入侵等。目前大多数现有方法都是以无监督方式执行的，因为标记的异常在大规模情况下往往太昂贵。然而，由于缺乏有关异常的先前知识，可能会将被识别的异常视为数据噪声或不感兴趣的数据实例。在现实场景中，通常可获取有限的标记异常，这些标记异常具有推进图异常检测的巨大潜力。然而，探索少量标记异常和大量无标记节点来检测异常的工作相当有限。因此，本文研究了少样本图异常检测的新问题。我们提出了一种新的框架MetaGAD，学习元转移知识来进行图异常检测。实

    Graph anomaly detection has long been an important problem in various domains pertaining to information security such as financial fraud, social spam, network intrusion, etc. The majority of existing methods are performed in an unsupervised manner, as labeled anomalies in a large scale are often too expensive to acquire. However, the identified anomalies may turn out to be data noises or uninteresting data instances due to the lack of prior knowledge on the anomalies. In realistic scenarios, it is often feasible to obtain limited labeled anomalies, which have great potential to advance graph anomaly detection. However, the work exploring limited labeled anomalies and a large amount of unlabeled nodes in graphs to detect anomalies is rather limited. Therefore, in this paper, we study a novel problem of few-shot graph anomaly detection. We propose a new framework MetaGAD to learn to meta-transfer the knowledge between unlabeled and labeled nodes for graph anomaly detection. Experimental 
    
[^32]: 基于Transformer的零样本和少样本生物医学命名实体识别方法

    A transformer-based method for zero and few-shot biomedical named entity recognition. (arXiv:2305.04928v1 [cs.CL])

    [http://arxiv.org/abs/2305.04928](http://arxiv.org/abs/2305.04928)

    本文提出了一种基于Transformer的生物医学领域零样本和少样本NER方法。此方法利用预训练学习给定和潜在类别之间的语义关系，将多类标记分类任务转换为二元标记分类，能够在不同数量的样本情况下达到良好的识别效果。

    

    在生物医学领域中，有监督的命名实体识别（NER）依赖于具有给定命名实体的大量注释文本，其创建可能耗时且昂贵。此外，提取新实体通常需要进行额外的注释任务和重新训练模型。为解决这些挑战，本文提出了一种基于Transformer的生物医学领域零样本和少样本NER方法。该方法基于将多类标记分类任务转换为二元标记分类（标记包含搜索的实体或不包含搜索的实体），并在更多的数据集和生物医学实体上进行预训练，从而可学习到给定和潜在类别之间的语义关系。在9种不同的生物医学实体上，我们在零样本NER、一次样本NER、10次样本NER和100次样本NER上实现了平均F1得分分别为35.44％、50.10％、69.94％和79.51％。

    Supervised named entity recognition (NER) in the biomedical domain is dependent on large sets of annotated texts with the given named entities, whose creation can be time-consuming and expensive. Furthermore, the extraction of new entities often requires conducting additional annotation tasks and retraining the model. To address these challenges, this paper proposes a transformer-based method for zero- and few-shot NER in the biomedical domain. The method is based on transforming the task of multi-class token classification into binary token classification (token contains the searched entity or does not contain the searched entity) and pre-training on a larger amount of datasets and biomedical entities, from where the method can learn semantic relations between the given and potential classes. We have achieved average F1 scores of 35.44% for zero-shot NER, 50.10% for one-shot NER, 69.94% for 10-shot NER, and 79.51% for 100-shot NER on 9 diverse evaluated biomedical entities with PubMed
    
[^33]: 可解释和鲁棒的脑电图AI系统综述

    Interpretable and Robust AI in EEG Systems: A Survey. (arXiv:2304.10755v1 [eess.SP])

    [http://arxiv.org/abs/2304.10755](http://arxiv.org/abs/2304.10755)

    这篇论文综述了近年来脑电图系统中可解释和鲁棒的AI技术的发展。其中，作者提出了解释性分类法，详细介绍了鲁棒AI的方法，包括对抗攻击和防御、迁移学习和不确定性建模，并讨论了未来的研究方向和挑战。

    

    在人工智能时代，人工智能（AI）和脑电图（EEG）的密切耦合极大地推动了人机交互（HCI）技术的发展。相较于传统的EEG系统，基于AI的EEG系统的可解释性和鲁棒性变得尤为关键。可解释性能够阐释AI模型的内部工作机制，因此可以获得用户的信任。鲁棒性则反映了AI对抗攻击和扰动的可靠性，这对于敏感和脆弱的EEG信号来说是至关重要的。因此，EEG系统中AI的可解释性和鲁棒性受到越来越多的关注，并且最近的研究取得了巨大进展。然而，关于这一领域的最新进展仍然没有综述。本文首先提出了一种解释性分类法，通过特征化模型、数据和输出解释性，总结了脑电图系统中解释性和鲁棒的AI技术，并详细介绍了鲁棒AI的方法，包括对抗攻击和防御、迁移学习和不确定性建模。最后，我们讨论了这一领域未来的方向和面临的挑战。

    The close coupling of artificial intelligence (AI) and electroencephalography (EEG) has substantially advanced human-computer interaction (HCI) technologies in the AI era. Different from traditional EEG systems, the interpretability and robustness of AI-based EEG systems are becoming particularly crucial. The interpretability clarifies the inner working mechanisms of AI models and thus can gain the trust of users. The robustness reflects the AI's reliability against attacks and perturbations, which is essential for sensitive and fragile EEG signals. Thus the interpretability and robustness of AI in EEG systems have attracted increasing attention, and their research has achieved great progress recently. However, there is still no survey covering recent advances in this field. In this paper, we present the first comprehensive survey and summarize the interpretable and robust AI techniques for EEG systems. Specifically, we first propose a taxonomy of interpretability by characterizing it 
    
[^34]: Text2Seg: 通过文本引导视觉基础模型的遥感图像语义分割

    Text2Seg: Remote Sensing Image Semantic Segmentation via Text-Guided Visual Foundation Models. (arXiv:2304.10597v1 [cs.CV])

    [http://arxiv.org/abs/2304.10597](http://arxiv.org/abs/2304.10597)

    本文介绍了一种名为 Text2Seg 的遥感图像语义分割流程，利用多个基础模型和文本引导，取得了初步成果。

    

    最近，基础模型（FMs），如 GPT-4 和 LLaMA，在零样本学习方案中表现出色，吸引了大量关注。类似地，在视觉学习领域，Grounding DINO 和 Segment Anything Model（SAM）等模型在开放式检测和实例分割任务中展现了显著的进步。本研究专注于遥感领域，其中图片与传统场景中的图片明显不同。我们开发了一个流程，利用多个 FMs，以文本提示为指导，促进遥感图像语义分割任务，我们将其称为 Text2Seg 。该管道在多个广泛使用的遥感数据集上进行基准测试，并提供初步结果以证明其有效性。

    Recent advancements in foundation models (FMs), such as GPT-4 and LLaMA, have attracted significant attention due to their exceptional performance in zero-shot learning scenarios. Similarly, in the field of visual learning, models like Grounding DINO and the Segment Anything Model (SAM) have exhibited remarkable progress in open-set detection and instance segmentation tasks. It is undeniable that these FMs will profoundly impact a wide range of real-world visual learning tasks, ushering in a new paradigm shift for developing such models. In this study, we concentrate on the remote sensing domain, where the images are notably dissimilar from those in conventional scenarios. We developed a pipeline that leverages multiple FMs to facilitate remote sensing image semantic segmentation tasks guided by text prompt, which we denote as Text2Seg. The pipeline is benchmarked on several widely-used remote sensing datasets, and we present preliminary results to demonstrate its effectiveness. Throug
    
[^35]: 基于MCMC的贝叶斯神经网络：基于Python的教程

    Bayesian neural networks via MCMC: a Python-based tutorial. (arXiv:2304.02595v1 [stat.ML])

    [http://arxiv.org/abs/2304.02595](http://arxiv.org/abs/2304.02595)

    本文提供了一个基于Python的教程，介绍了贝叶斯神经网络的MCMC方法应用，通过教程使得深度学习开发者能够更好地应用贝叶斯推断进行参数估计和不确定性量化。

    

    贝叶斯推断为机器学习和深度学习提供了参数估计和不确定性量化的方法。变分推断和马尔科夫链蒙特卡罗（MCMC）采样技术用于实现贝叶斯推断。在过去三十年中，MCMC方法在适应更大的模型（如深度学习）和大数据问题方面面临了许多挑战。包括梯度的高级提议（例如Langevin提议分布）提供了一种解决MCMC采样中的一些限制的方法，此外，MCMC方法通常被限制在统计学家的使用范围内，并且仍不是深度学习研究人员的主流方法。我们提供了一个MCMC方法的教程，涵盖了简单的贝叶斯线性和逻辑模型，以及贝叶斯神经网络。这个教程的目的是通过编码来弥合理论和实现之间的差距，鉴于当前MCMC方法的普及程度仍然较低。

    Bayesian inference provides a methodology for parameter estimation and uncertainty quantification in machine learning and deep learning methods. Variational inference and Markov Chain Monte-Carlo (MCMC) sampling techniques are used to implement Bayesian inference. In the past three decades, MCMC methods have faced a number of challenges in being adapted to larger models (such as in deep learning) and big data problems. Advanced proposals that incorporate gradients, such as a Langevin proposal distribution, provide a means to address some of the limitations of MCMC sampling for Bayesian neural networks. Furthermore, MCMC methods have typically been constrained to use by statisticians and are still not prominent among deep learning researchers. We present a tutorial for MCMC methods that covers simple Bayesian linear and logistic models, and Bayesian neural networks. The aim of this tutorial is to bridge the gap between theory and implementation via coding, given a general sparsity of li
    

