# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [K-Level Reasoning with Large Language Models](https://rss.arxiv.org/abs/2402.01521) | 该论文探索了使用大型语言模型进行动态决策推理的能力，并提出了一种名为"K级推理"的新颖推理方法。通过博弈论的试验，发现现有的推理方法在动态环境中容易出错，而"K级推理"可以解决这个问题。 |
| [^2] | [PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression](https://arxiv.org/abs/2404.00489) | 提出了PROMPT-SAW模型，利用关系感知图来实现文本提示的压缩，提高了提示的可读性和可解释性。 |
| [^3] | [CoLLEGe: Concept Embedding Generation for Large Language Models](https://arxiv.org/abs/2403.15362) | CoLLEGe是一个元学习框架，能够为大型语言模型生成灵活的新概念嵌入，用于现代化少样本概念学习。 |
| [^4] | [Learning Macroeconomic Policies based on Microfoundations: A Stackelberg Mean Field Game Approach](https://arxiv.org/abs/2403.12093) | 本研究提出了基于Stackelberg Mean Field Game的方法，可以有效地学习宏观经济政策，并在模型预训练和无模型Stackelberg均场强化学习算法的基础上取得了实验结果表明其优越性。 |
| [^5] | [Corrective Machine Unlearning](https://arxiv.org/abs/2402.14015) | 该论文通过形式化“修正机器消除”来解决受未知操纵影响的数据对训练模型的影响问题，可能仅知道一部分受影响样本。发现纠正消除问题与传统以隐私为导向的消除方法有显著不同的要求。 |
| [^6] | [Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data](https://arxiv.org/abs/2402.12424) | 本研究探讨了LLM在解释表格数据方面的有效性，比较了文本和图像表格表示对LLM性能的影响，为在表格相关任务上有效使用LLM提供了见解。 |
| [^7] | [Learning Contrastive Feature Representations for Facial Action Unit Detection](https://arxiv.org/abs/2402.06165) | 这项研究提出了一种对比学习框架，通过监督和自监督信号来增强面部动作单元检测模型的性能。采用正样本抽样和权衡重要性的损失函数来应对噪声AU标签和AU类型分布不平衡的挑战。 |
| [^8] | [BLT: Can Large Language Models Handle Basic Legal Text?](https://arxiv.org/abs/2311.09693) | 大型语言模型在处理基础法律文本方面表现不佳，但通过针对性微调，甚至较小的模型也能在测试中表现出色，提升了相关法律任务的表现。 |
| [^9] | [Unveiling the Blind Spots: A Critical Examination of Fairness in Autonomous Driving Systems](https://arxiv.org/abs/2308.02935) | 该研究对当前深度学习行人检测器的公平性进行了全面评估，发现了与年龄相关的重要公平性问题。 |
| [^10] | [Experimental Contexts Can Facilitate Robust Semantic Property Inference in Language Models, but Inconsistently.](http://arxiv.org/abs/2401.06640) | 本研究通过控制实验环境的方式，发现语言模型在属性继承任务中表现出了一定的非平凡能力，但这种能力是不一致的。 |
| [^11] | [Functional Graphical Models: Structure Enables Offline Data-Driven Optimization.](http://arxiv.org/abs/2401.05442) | 功能图模型（FGMs）通过结构实现了样本高效的数据驱动优化。 |
| [^12] | [A new approach to template banks of gravitational waves with higher harmonics: reducing matched-filtering cost by over an order of magnitude.](http://arxiv.org/abs/2310.15233) | 该论文提出了一种新方法，通过在引力波模板库中包含高次谐波模式，利用引力波模式之间的自然关联，可以大幅度减少匹配滤波的成本，并提高搜索引力波事件的灵敏度。 |
| [^13] | [Exploring Progress in Multivariate Time Series Forecasting: Comprehensive Benchmarking and Heterogeneity Analysis.](http://arxiv.org/abs/2310.06119) | 该论文旨在解决多元时间序列预测领域中公平基准测试和技术方法选择的争议，并提供对该领域取得的进展的深入洞察。 |
| [^14] | [GOOSE Algorithm: A Powerful Optimization Tool for Real-World Engineering Challenges and Beyond.](http://arxiv.org/abs/2307.10420) | GOOSE算法是一种基于鹅的行为的元启发式算法，它在多个基准测试函数上进行了验证和比较，证明其在解决现实世界的工程挑战中的有效性。 |
| [^15] | [HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models.](http://arxiv.org/abs/2307.06949) | HyperDreamBooth是一个超网络，可以从一个人的单张图片中快速生成个性化权重，从而实现在多种背景和风格下合成一个人的面部，保持高保真度并同时保留对多样化风格和语义修改的关键知识。 |
| [^16] | [Degraded Polygons Raise Fundamental Questions of Neural Network Perception.](http://arxiv.org/abs/2306.04955) | 本文研究了神经网络在识别具有不同程度边缘降解的规则多边形时的性能和行为，发现存在基本问题，揭示了人机视觉差距的另一个角度。 |
| [^17] | [In Defense of Pure 16-bit Floating-Point Neural Networks.](http://arxiv.org/abs/2305.10947) | 本文探讨了纯16位浮点神经网络的被忽视的效率，提供了理论分析来探讨16位和32位模型的差异，并可以定量解释16位模型与其32位对应物之间的条件。 |

# 详细

[^1]: 使用大型语言模型进行K级推理

    K-Level Reasoning with Large Language Models

    [https://rss.arxiv.org/abs/2402.01521](https://rss.arxiv.org/abs/2402.01521)

    该论文探索了使用大型语言模型进行动态决策推理的能力，并提出了一种名为"K级推理"的新颖推理方法。通过博弈论的试验，发现现有的推理方法在动态环境中容易出错，而"K级推理"可以解决这个问题。

    

    虽然大型语言模型（LLMs）已经展示了其在复杂推理任务上的能力，但在动态、交互和竞争场景（如商业战略和股票市场分析）中的性能仍然未被充分探索。为了填补这个空白，我们正式探索LLMs在快速变化环境中的决策推理能力。我们引入了两个基于博弈论的试验，以模拟现实世界中动态决策的复杂性。这些挑战具有明确定义，可以对LLMs的动态推理能力进行清晰、可控和精确的评估。通过大量实验，我们发现现有的推理方法在需要k级思考的动态环境中容易出错 - 这是之前研究中未解决的关键概念。为了解决这个问题，我们提出了一种新颖的LLMs推理方法，命名为“K级推理”。该方法采用对手的视角，从递归角度运用基于k级思考的推理。

    While Large Language Models (LLMs) have demonstrated their proficiency in complex reasoning tasks, their performance in dynamic, interactive, and competitive scenarios - such as business strategy and stock market analysis - remains underexplored. To bridge this gap, we formally explore the dynamic reasoning capabilities of LLMs for decision-making in rapidly evolving environments. We introduce two game theory-based pilot challenges that mirror the complexities of real-world dynamic decision-making. These challenges are well-defined, enabling clear, controllable, and precise evaluation of LLMs' dynamic reasoning abilities. Through extensive experiments, we find that existing reasoning methods tend to falter in dynamic settings that require k-level thinking - a key concept not tackled by previous works. To address this, we propose a novel reasoning approach for LLMs, named "K-Level Reasoning". This approach adopts the perspective of rivals to recursively employ k-level thinking based on 
    
[^2]: PROMPT-SAW：利用关系感知图进行文本提示压缩

    PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression

    [https://arxiv.org/abs/2404.00489](https://arxiv.org/abs/2404.00489)

    提出了PROMPT-SAW模型，利用关系感知图来实现文本提示的压缩，提高了提示的可读性和可解释性。

    

    大型语言模型(LLMs)在多种不同的自然语言处理任务中展现出卓越的能力。提示是LLM推理中的基本工具，但我们观察到超长提示会带来显著的成本。现有的压缩长提示的尝试导致压缩提示在可读性和可解释性方面表现不佳，对提示效用产生有害影响。为了解决这一问题，我们提出了PROMPT-SAW：通过关系感知图进行提示压缩，这是一种针对任务不可知和任务感知提示的有效策略。PROMPT-SAW使用提示的文本信息构建图形，在图形中提取关键信息元素，从而得出压缩提示。我们还提出了GSM8K-AUG，即现有GSM8k基准的扩展版本，用于任务不可知提示，以提供全面的评估平台。

    arXiv:2404.00489v1 Announce Type: cross  Abstract: Large language models (LLMs) have shown exceptional abilities for multiple different natural language processing tasks. While prompting is a crucial tool for LLM inference, we observe that there is a significant cost associated with exceedingly lengthy prompts. Existing attempts to compress lengthy prompts lead to sub-standard results in terms of readability and interpretability of the compressed prompt, with a detrimental impact on prompt utility. To address this, we propose PROMPT-SAW: Prompt compresSion via Relation AWare graphs, an effective strategy for prompt compression over task-agnostic and task-aware prompts. PROMPT-SAW uses the prompt's textual information to build a graph, later extracts key information elements in the graph to come up with the compressed prompt. We also propose GSM8K-AUG, i.e., an extended version of the existing GSM8k benchmark for task-agnostic prompts in order to provide a comprehensive evaluation platf
    
[^3]: CoLLEGe: 大型语言模型的概念嵌入生成

    CoLLEGe: Concept Embedding Generation for Large Language Models

    [https://arxiv.org/abs/2403.15362](https://arxiv.org/abs/2403.15362)

    CoLLEGe是一个元学习框架，能够为大型语言模型生成灵活的新概念嵌入，用于现代化少样本概念学习。

    

    当前语言模型无法快速学习新概念，通常需要更复杂的微调过程才能学习得更稳健。本文引入了一种名为CoLLEGe（Concept Learning with Language Embedding Generation）的新方法，用于现代化的少样本概念学习。CoLLEGe是一个元学习框架，能够使用少量示例句子或定义生成新概念的灵活嵌入。我们的主要元学习目标只是促进语言模型在随后的句子中进行下一个词预测，使其与语言模型的预训练兼容。

    arXiv:2403.15362v1 Announce Type: cross  Abstract: Current language models are unable to quickly learn new concepts on the fly, often requiring a more involved finetuning process to learn robustly. Prompting in-context is not robust to context distractions, and often fails to confer much information about the new concepts. Classic methods for few-shot word learning in NLP, relying on global word vectors, are less applicable to large language models. In this paper, we introduce a novel approach named CoLLEGe (Concept Learning with Language Embedding Generation) to modernize few-shot concept learning. CoLLEGe is a meta-learning framework capable of generating flexible embeddings for new concepts using a small number of example sentences or definitions. Our primary meta-learning objective is simply to facilitate a language model to make next word predictions in forthcoming sentences, making it compatible with language model pretraining. We design a series of tasks to test new concept lear
    
[^4]: 基于微观基础的宏观经济政策学习：一种斯塔克尔贝格均场博弈方法

    Learning Macroeconomic Policies based on Microfoundations: A Stackelberg Mean Field Game Approach

    [https://arxiv.org/abs/2403.12093](https://arxiv.org/abs/2403.12093)

    本研究提出了基于Stackelberg Mean Field Game的方法，可以有效地学习宏观经济政策，并在模型预训练和无模型Stackelberg均场强化学习算法的基础上取得了实验结果表明其优越性。

    

    有效的宏观经济政策在促进经济增长和社会稳定方面起着至关重要的作用。本文基于Stackelberg Mean Field Game（SMFG）模型，将最优宏观经济政策问题建模，其中政府作为政策制定的领导者，大规模家庭动态响应为追随者。这种建模方法捕捉了政府和大规模家庭之间的非对称动态博弈，并可以解释地评估基于微观基础的宏观经济政策效果，这是现有方法难以实现的。我们还提出了一种解决SMFG的方法，将真实数据进行预训练，并结合一种无模型的Stackelberg均场强化学习（SMFRL）算法，该算法可以独立于先前的环境知识和转变运行。我们的实验结果展示了SMFG方法在经济政策方面优于其他方法的优越性。

    arXiv:2403.12093v1 Announce Type: cross  Abstract: Effective macroeconomic policies play a crucial role in promoting economic growth and social stability. This paper models the optimal macroeconomic policy problem based on the \textit{Stackelberg Mean Field Game} (SMFG), where the government acts as the leader in policy-making, and large-scale households dynamically respond as followers. This modeling method captures the asymmetric dynamic game between the government and large-scale households, and interpretably evaluates the effects of macroeconomic policies based on microfoundations, which is difficult for existing methods to achieve. We also propose a solution for SMFGs, incorporating pre-training on real data and a model-free \textit{Stackelberg mean-field reinforcement learning }(SMFRL) algorithm, which operates independently of prior environmental knowledge and transitions. Our experimental results showcase the superiority of the SMFG method over other economic policies in terms 
    
[^5]: 修正机器消除

    Corrective Machine Unlearning

    [https://arxiv.org/abs/2402.14015](https://arxiv.org/abs/2402.14015)

    该论文通过形式化“修正机器消除”来解决受未知操纵影响的数据对训练模型的影响问题，可能仅知道一部分受影响样本。发现纠正消除问题与传统以隐私为导向的消除方法有显著不同的要求。

    

    机器学习模型越来越面临数据完整性挑战，因为它们使用了大规模的从互联网中获取的训练数据集。本文研究了如果模型开发者发现某些数据被篡改或错误，他们可以采取什么措施。这些被篡改的数据会导致不利影响，如容易受到后门样本的攻击、系统性偏见，以及在某些输入领域的准确度降低。通常，并非所有被篡改的训练样本都是已知的，而只有一小部分代表性的受影响数据被标记。

    arXiv:2402.14015v1 Announce Type: cross  Abstract: Machine Learning models increasingly face data integrity challenges due to the use of large-scale training datasets drawn from the internet. We study what model developers can do if they detect that some data was manipulated or incorrect. Such manipulated data can cause adverse effects like vulnerability to backdoored samples, systematic biases, and in general, reduced accuracy on certain input domains. Often, all manipulated training samples are not known, and only a small, representative subset of the affected data is flagged.   We formalize "Corrective Machine Unlearning" as the problem of mitigating the impact of data affected by unknown manipulations on a trained model, possibly knowing only a subset of impacted samples. We demonstrate that the problem of corrective unlearning has significantly different requirements from traditional privacy-oriented unlearning. We find most existing unlearning methods, including the gold-standard
    
[^6]: 表格作为图片？探讨LLM在多模态表格数据表示上的优势和局限性

    Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data

    [https://arxiv.org/abs/2402.12424](https://arxiv.org/abs/2402.12424)

    本研究探讨了LLM在解释表格数据方面的有效性，比较了文本和图像表格表示对LLM性能的影响，为在表格相关任务上有效使用LLM提供了见解。

    

    在本文中，我们通过不同的提示策略和数据格式研究了各种LLM在解释表格数据方面的有效性。我们的分析涵盖了六个针对与表格相关任务的基准，如问答和事实核查。我们首次介绍了LLM在基于图像的表格表示上的表现评估。具体地，我们比较了五种基于文本和三种基于图像的表格表示，展示了表示和提示对LLM性能的影响。我们的研究为在表格相关任务上有效使用LLM提供了见解。

    arXiv:2402.12424v1 Announce Type: cross  Abstract: In this paper, we investigate the effectiveness of various LLMs in interpreting tabular data through different prompting strategies and data formats. Our analysis extends across six benchmarks for table-related tasks such as question-answering and fact-checking. We introduce for the first time the assessment of LLMs' performance on image-based table representations. Specifically, we compare five text-based and three image-based table representations, demonstrating the influence of representation and prompting on LLM performance. Our study provides insights into the effective use of LLMs on table-related tasks.
    
[^7]: 学习对比特征表示来进行面部动作单元检测

    Learning Contrastive Feature Representations for Facial Action Unit Detection

    [https://arxiv.org/abs/2402.06165](https://arxiv.org/abs/2402.06165)

    这项研究提出了一种对比学习框架，通过监督和自监督信号来增强面部动作单元检测模型的性能。采用正样本抽样和权衡重要性的损失函数来应对噪声AU标签和AU类型分布不平衡的挑战。

    

    面部动作单元（AU）检测的主要方法涉及监督的多标签二进制分类问题。现有的方法常常对AU的像素级信息进行编码，从而对模型的复杂性和表达能力提出了很大的要求。此外，由于存在噪声AU标签，这种做法增加了过拟合的风险。在本研究中，我们引入了一个对比学习框架，通过监督和自监督信号增强。目标是在AU检测领域中摆脱传统的像素级学习范式，获得判别特征。为了应对噪声AU标签带来的挑战，我们通过引入自监督信号来增强监督信号。这种增强是通过正样本抽样实现的，包括三种不同类型的正样本对。另外，为了减轻每个AU类型的分布不平衡问题，我们采用了一种权衡重要性的损失函数。

    The predominant approach to facial action unit (AU) detection revolves around a supervised multi-label binary classification problem. Existing methodologies often encode pixel-level information of AUs, thereby imposing substantial demands on model complexity and expressiveness. Moreover, this practice elevates the susceptibility to overfitting due to the presence of noisy AU labels. In the present study, we introduce a contrastive learning framework enhanced by both supervised and self-supervised signals. The objective is to acquire discriminative features, deviating from the conventional pixel-level learning paradigm within the domain of AU detection. To address the challenge posed by noisy AU labels, we augment the supervised signal through the introduction of a self-supervised signal. This augmentation is achieved through positive sample sampling, encompassing three distinct types of positive sample pairs. Furthermore, to mitigate the imbalanced distribution of each AU type, we empl
    
[^8]: BLT: 大型语言模型能处理基础法律文本吗？

    BLT: Can Large Language Models Handle Basic Legal Text?

    [https://arxiv.org/abs/2311.09693](https://arxiv.org/abs/2311.09693)

    大型语言模型在处理基础法律文本方面表现不佳，但通过针对性微调，甚至较小的模型也能在测试中表现出色，提升了相关法律任务的表现。

    

    我们发现像GPT-4、Claude和{PaLM 2}这样的最好的公开可用的LLM在处理基础法律文本方面表现不佳。我们引入了一个基准，其中包含律师和法律助理期望LLM零-shot处理的任务，比如查找证词文件的某一行或合同的某个子部分的文本。LLM在这个基准上的差劲表现对它们在法律实践中的可靠性提出了质疑。然而，针对这些任务进行微调甚至使一个较小的模型在我们的测试集上表现接近完美，并且还提升了相关法律任务的表现。这些结果表明，许多领域所需的简单行为在基础LLM中可能不存在，除非有领域专家的额外参与。

    arXiv:2311.09693v2 Announce Type: replace-cross  Abstract: We find that the best publicly available LLMs like GPT-4, Claude, and {PaLM 2} currently perform poorly at basic legal text handling. We introduce a benchmark consisting of tasks that lawyers and paralegals would expect LLMs to handle zero-shot, such as looking up the text at a line of a witness deposition or at a subsection of a contract. LLMs' poor performance on this benchmark casts into doubt their reliability as-is for legal practice. However, fine-tuning for these tasks brings even a smaller model to near-perfect performance on our test set and also raises performance on a related legal task. These results suggest that many simple behaviors needed for a domain may not be present in foundational LLMs, without additional engagement from subject matter experts.
    
[^9]: 揭示盲点：对自动驾驶系统中公平性的关键审查

    Unveiling the Blind Spots: A Critical Examination of Fairness in Autonomous Driving Systems

    [https://arxiv.org/abs/2308.02935](https://arxiv.org/abs/2308.02935)

    该研究对当前深度学习行人检测器的公平性进行了全面评估，发现了与年龄相关的重要公平性问题。

    

    自主驾驶系统已经扩展了智能车辆物联网的范围，并成为Web生态系统的重要组成部分。类似于传统的基于Web的应用程序，公平性对于确保自动驾驶系统的高质量是一个重要方面，特别是在其中的行人检测器的背景下。然而，目前关于当前基于深度学习（DL）的行人检测器公平性的综合评估在文献中尚未出现。为了填补这一空白，我们在大规模真实世界数据集上评估了八种被广泛探索的DL行人检测器在人口统计学群体之间的表现。为了实现彻底的公平性评估，我们为数据集提供了广泛的注释，共涉及8,311张图像，16,070个性别标签，20,115个年龄标签和3,513个肤色标签。我们的研究发现了与年龄相关的重要公平性问题。

    arXiv:2308.02935v2 Announce Type: replace-cross  Abstract: Autonomous driving systems have extended the spectrum of Web of Things for intelligent vehicles and have become an important component of the Web ecosystem. Similar to traditional Web-based applications, fairness is an essential aspect for ensuring the high quality of autonomous driving systems, particularly in the context of pedestrian detectors within them. However, there is an absence in the literature of a comprehensive assessment of the fairness of current Deep Learning (DL)-based pedestrian detectors. To fill the gap, we evaluate eight widely-explored DL-based pedestrian detectors across demographic groups on large-scale real-world datasets. To enable a thorough fairness evaluation, we provide extensive annotations for the datasets, resulting in 8,311 images with 16,070 gender labels, 20,115 age labels, and 3,513 skin tone labels. Our findings reveal significant fairness issues related to age. The undetected proportions f
    
[^10]: 实验环境能够促进语言模型在稳健的语义属性推断中的表现，但不一致。

    Experimental Contexts Can Facilitate Robust Semantic Property Inference in Language Models, but Inconsistently. (arXiv:2401.06640v1 [cs.CL])

    [http://arxiv.org/abs/2401.06640](http://arxiv.org/abs/2401.06640)

    本研究通过控制实验环境的方式，发现语言模型在属性继承任务中表现出了一定的非平凡能力，但这种能力是不一致的。

    

    最近的无人监督评估凸显了语言模型（LMs）在执行意义提取方面的重要限制。然而，众所周知，在引入实验环境（如上下文示例和指导）的情况下，LMs的表现可以显著提高。那么这是否适用于先前研究的意义敏感任务呢？我们在控制上下文示例和指导内容的前提下，对实验环境对于提高LMs在执行属性继承任务中的鲁棒性的程度进行了案例研究，该任务是预先表明LMs无法完成的任务。我们的研究发现，实验环境确实可以导致LMs在属性继承行为方面表现出非平凡的能力。然而，这种能力是不一致的：通过对任务进行最小改写，发现一些LMs从输入中捕捉到浅层的非语义式启发式信息，这表明计算机的行为具有不一致性。

    Recent zero-shot evaluations have highlighted important limitations in the abilities of language models (LMs) to perform meaning extraction. However, it is now well known that LMs can demonstrate radical improvements in the presence of experimental contexts such as in-context examples and instructions. How well does this translate to previously studied meaning-sensitive tasks? We present a case-study on the extent to which experimental contexts can improve LMs' robustness in performing property inheritance -- predicting semantic properties of novel concepts, a task that they have been previously shown to fail on. Upon carefully controlling the nature of the in-context examples and the instructions, our work reveals that they can indeed lead to non-trivial property inheritance behavior in LMs. However, this ability is inconsistent: with a minimal reformulation of the task, some LMs were found to pick up on shallow, non-semantic heuristics from their inputs, suggesting that the computati
    
[^11]: 功能图模型：结构实现离线数据驱动优化

    Functional Graphical Models: Structure Enables Offline Data-Driven Optimization. (arXiv:2401.05442v1 [cs.LG])

    [http://arxiv.org/abs/2401.05442](http://arxiv.org/abs/2401.05442)

    功能图模型（FGMs）通过结构实现了样本高效的数据驱动优化。

    

    虽然机器学习模型通常是为了解决预测问题而训练的，但我们经常希望将它们用于优化问题。例如，给定一组蛋白质及其对应的荧光水平的数据集，我们可能希望为具有最高荧光的新蛋白质进行优化。这种数据驱动的优化（DDO）面临着一系列挑战，超出了标准预测问题中的挑战，因为我们需要成功预测在训练集中没有见过的优于最佳设计的新设计的性能的模型。从理论上讲，甚至不清楚现有方法什么时候甚至能比简单地选择数据集中最佳设计的朴素方法执行得更好。在本文中，我们研究了如何通过结构实现高效的数据驱动优化。为了形式化结构的概念，我们引入了功能图模型（FGMs）并从理论上展示了它们如何通过分解实现基于数据的优化。

    While machine learning models are typically trained to solve prediction problems, we might often want to use them for optimization problems. For example, given a dataset of proteins and their corresponding fluorescence levels, we might want to optimize for a new protein with the highest possible fluorescence. This kind of data-driven optimization (DDO) presents a range of challenges beyond those in standard prediction problems, since we need models that successfully predict the performance of new designs that are better than the best designs seen in the training set. It is not clear theoretically when existing approaches can even perform better than the naive approach that simply selects the best design in the dataset. In this paper, we study how structure can enable sample-efficient data-driven optimization. To formalize the notion of structure, we introduce functional graphical models (FGMs) and show theoretically how they can provide for principled data-driven optimization by decomp
    
[^12]: 一个新方法用于带有更高次谐波的引力波模板库：通过十倍减少匹配滤波成本

    A new approach to template banks of gravitational waves with higher harmonics: reducing matched-filtering cost by over an order of magnitude. (arXiv:2310.15233v1 [gr-qc])

    [http://arxiv.org/abs/2310.15233](http://arxiv.org/abs/2310.15233)

    该论文提出了一种新方法，通过在引力波模板库中包含高次谐波模式，利用引力波模式之间的自然关联，可以大幅度减少匹配滤波的成本，并提高搜索引力波事件的灵敏度。

    

    引力波事件的搜索使用信号模型或模板。目前在LIGO-Virgo-Kagra (LVK)数据中使用的模板仅模拟了信号的主导四极模式$(\ell,m)=(2,2)$，忽略了次要的高阶模式(HM)例如$(\ell,m)=(3,3)$，$(4,4)$，这些模式是由广义相对论预测的。因此，这些搜索可能会在参数空间的一些有趣区域，如高质量和非对称质量比的系统中失去对黑洞合并的灵敏度。我们开发了一种新策略，将HM包含在模板库中，利用这些模式之间的自然关联。我们使用了牛顿附加公式和机器学习工具来模拟与给定$(2,2)$波形相对应的自旋对齐的$(3,3)$，$(4,4)$波形。可以对每个模式的数据进行单独滤波，得到信噪比(SNR)的独立时间序列，然后可以以相对廉价的方式将其组合起来进行综合。

    Searches for gravitational wave events use models, or templates, for the signals of interest. The templates used in current searches in the LIGO-Virgo-Kagra (LVK) data model the dominant quadrupole mode $(\ell,m)=(2,2)$ of the signals, and omit sub-dominant higher-order modes (HM) such as $(\ell,m)=(3,3)$, $(4,4)$, which are predicted by general relativity. Hence, these searches could lose sensitivity to black hole mergers in interesting parts of parameter space, such as systems with high-masses and asymmetric mass ratios. We develop a new strategy to include HM in template banks that exploits the natural connection between the modes. We use a combination of post-Newtonian formulae and machine learning tools to model aligned-spin $(3,3)$, $(4,4)$ waveforms corresponding to a given $(2,2)$ waveform. Each of these modes can be individually filtered against the data to yield separate timeseries of signal-to-noise ratios (SNR), which can be combined in a relatively inexpensive way to margi
    
[^13]: 探索多元时间序列预测的进展：全面基准测试和异质性分析

    Exploring Progress in Multivariate Time Series Forecasting: Comprehensive Benchmarking and Heterogeneity Analysis. (arXiv:2310.06119v1 [cs.LG])

    [http://arxiv.org/abs/2310.06119](http://arxiv.org/abs/2310.06119)

    该论文旨在解决多元时间序列预测领域中公平基准测试和技术方法选择的争议，并提供对该领域取得的进展的深入洞察。

    

    多元时间序列（MTS）广泛存在于现实世界的复杂系统中，如交通和能源系统，对于理解和影响这些系统，它们的预测至关重要。最近，基于深度学习的方法在MTS中有效地建模时间和空间依赖关系方面获得了很大的流行，特别是在长期时间序列预测（LTSF）和时空预测（STF）中。然而，公平的基准测试问题和技术方法的选择在相关工作中一直存在争议。这些争议显著阻碍了我们对该领域进展的理解。因此，本文旨在解决这些争议，以提供对取得的进展的深入洞察。为了解决基准测试问题，我们引入了BasicTS，一个旨在公平比较MTS预测的基准。BasicTS建立了一个统一的训练流程和合理的评估设置，能够对30多种流行的MTS预测模型进行公正的评估。

    Multivariate Time Series (MTS) widely exists in real-word complex systems, such as traffic and energy systems, making their forecasting crucial for understanding and influencing these systems. Recently, deep learning-based approaches have gained much popularity for effectively modeling temporal and spatial dependencies in MTS, specifically in Long-term Time Series Forecasting (LTSF) and Spatial-Temporal Forecasting (STF). However, the fair benchmarking issue and the choice of technical approaches have been hotly debated in related work. Such controversies significantly hinder our understanding of progress in this field. Thus, this paper aims to address these controversies to present insights into advancements achieved. To resolve benchmarking issues, we introduce BasicTS, a benchmark designed for fair comparisons in MTS forecasting. BasicTS establishes a unified training pipeline and reasonable evaluation settings, enabling an unbiased evaluation of over 30 popular MTS forecasting mode
    
[^14]: GOOSE算法: 一个强大的优化工具用于现实世界的工程挑战及更多。

    GOOSE Algorithm: A Powerful Optimization Tool for Real-World Engineering Challenges and Beyond. (arXiv:2307.10420v1 [cs.AI])

    [http://arxiv.org/abs/2307.10420](http://arxiv.org/abs/2307.10420)

    GOOSE算法是一种基于鹅的行为的元启发式算法，它在多个基准测试函数上进行了验证和比较，证明其在解决现实世界的工程挑战中的有效性。

    

    该研究提出了GOOSE算法，它是一种基于鹅在休息和觅食时的行为的元启发式算法。鹅靠着一只腿保持平衡，以守护和保护群体中的其他个体。GOOSE算法在19个知名的基准测试函数上进行基准测试，并通过与遗传算法(GA)、粒子群优化(PSO)、蜻蜓算法(DA)和适应性依赖优化器(FDO)的比较研究来验证结果。此外，该算法还在10个现代基准函数上进行了测试，并将结果与蜻蜓算法、鲸鱼优化算法(WOA)和鳐鱼群算法(SSA)等三个最近的算法进行了比较。此外，GOOSE算法还在5个经典基准函数上进行了测试，并将所得结果与适应性依赖优化器(FDO)、FOX优化器、蝶优化算法(BOA)、鲸鱼优化算法、人工蜂群算法和差分进化算法等六种算法进行了评估。

    This study proposes the GOOSE algorithm as a novel metaheuristic algorithm based on the goose's behavior during rest and foraging. The goose stands on one leg and keeps his balance to guard and protect other individuals in the flock. The GOOSE algorithm is benchmarked on 19 well-known benchmark test functions, and the results are verified by a comparative study with genetic algorithm (GA), particle swarm optimization (PSO), dragonfly algorithm (DA), and fitness dependent optimizer (FDO). In addition, the proposed algorithm is tested on 10 modern benchmark functions, and the gained results are compared with three recent algorithms, such as the dragonfly algorithm, whale optimization algorithm (WOA), and salp swarm algorithm (SSA). Moreover, the GOOSE algorithm is tested on 5 classical benchmark functions, and the obtained results are evaluated with six algorithms, such as fitness dependent optimizer (FDO), FOX optimizer, butterfly optimization algorithm (BOA), whale optimization algorit
    
[^15]: HyperDreamBooth：用于快速个性化文本到图像模型的超网络

    HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models. (arXiv:2307.06949v1 [cs.CV])

    [http://arxiv.org/abs/2307.06949](http://arxiv.org/abs/2307.06949)

    HyperDreamBooth是一个超网络，可以从一个人的单张图片中快速生成个性化权重，从而实现在多种背景和风格下合成一个人的面部，保持高保真度并同时保留对多样化风格和语义修改的关键知识。

    

    个性化已经成为生成式人工智能领域中的一个重要方面，使得在不同背景和风格下合成个体成为可能，同时保持高保真度。然而，个性化过程在时间和内存需求方面存在困难。每个个性化模型的微调需要大量的GPU时间投入，为每个主题存储一个个性化模型会对存储容量提出要求。为了克服这些挑战，我们提出了HyperDreamBooth-一种能够从一个人的单张图片有效生成一组个性化权重的超网络。通过将这些权重组合到扩散模型中，并搭配快速微调，HyperDreamBooth能够以多种背景和风格生成一个人的面部，保持高主题细节同时也保持模型对多样化风格和语义修改的关键知识。我们的方法在大约50倍体现了面部个性化。

    Personalization has emerged as a prominent aspect within the field of generative AI, enabling the synthesis of individuals in diverse contexts and styles, while retaining high-fidelity to their identities. However, the process of personalization presents inherent challenges in terms of time and memory requirements. Fine-tuning each personalized model needs considerable GPU time investment, and storing a personalized model per subject can be demanding in terms of storage capacity. To overcome these challenges, we propose HyperDreamBooth-a hypernetwork capable of efficiently generating a small set of personalized weights from a single image of a person. By composing these weights into the diffusion model, coupled with fast finetuning, HyperDreamBooth can generate a person's face in various contexts and styles, with high subject details while also preserving the model's crucial knowledge of diverse styles and semantic modifications. Our method achieves personalization on faces in roughly 
    
[^16]: 论神经网络对降解多边形的感知存在的基本问题

    Degraded Polygons Raise Fundamental Questions of Neural Network Perception. (arXiv:2306.04955v1 [cs.CV])

    [http://arxiv.org/abs/2306.04955](http://arxiv.org/abs/2306.04955)

    本文研究了神经网络在识别具有不同程度边缘降解的规则多边形时的性能和行为，发现存在基本问题，揭示了人机视觉差距的另一个角度。

    

    现代计算机视觉系统往往表现出与人类不一致的行为：从对抗攻击到图像损坏，深度学习视觉模型在各种环境中都表现不佳，然而人类却能够很好地解决这些问题。本文从另一个角度研究了人机视觉差距。我们重新审视了恢复受损图像的任务，该任务在人类视觉的“识别组件”理论中首次引入，研究了神经网络在分类具有不同程度边缘降解的规则多边形时的性能和行为。为此，我们使用了自动化形状可恢复性测试，快速生成了大规模数据集，将历史上手动创建图像可恢复性实验的方法进行了现代化改进。我们进一步研究了神经网络识别多边形的能力以及其相关问题。

    It is well-known that modern computer vision systems often exhibit behaviors misaligned with those of humans: from adversarial attacks to image corruptions, deep learning vision models suffer in a variety of settings that humans capably handle. In light of these phenomena, here we introduce another, orthogonal perspective studying the human-machine vision gap. We revisit the task of recovering images under degradation, first introduced over 30 years ago in the Recognition-by-Components theory of human vision. Specifically, we study the performance and behavior of neural networks on the seemingly simple task of classifying regular polygons at varying orders of degradation along their perimeters. To this end, we implement the Automated Shape Recoverability Test for rapidly generating large-scale datasets of perimeter-degraded regular polygons, modernizing the historically manual creation of image recoverability experiments. We then investigate the capacity of neural networks to recognize
    
[^17]: 关于纯16位浮点神经网络的辩护

    In Defense of Pure 16-bit Floating-Point Neural Networks. (arXiv:2305.10947v1 [cs.LG])

    [http://arxiv.org/abs/2305.10947](http://arxiv.org/abs/2305.10947)

    本文探讨了纯16位浮点神经网络的被忽视的效率，提供了理论分析来探讨16位和32位模型的差异，并可以定量解释16位模型与其32位对应物之间的条件。

    

    减少编码神经网络权重和激活所需的位数是非常可取的，因为它可以加快神经网络的训练和推理时间，同时减少内存消耗。因此，这一领域的研究引起了广泛关注，以开发利用更低精度计算的神经网络，比如混合精度训练。有趣的是，目前不存在纯16位浮点设置的方法。本文揭示了纯16位浮点神经网络被忽视的效率。我们通过提供全面的理论分析来探讨造成16位和32位模型的差异的因素。我们规范化了浮点误差和容忍度的概念，从而可以定量解释16位模型与其32位对应物之间密切逼近结果的条件。这种理论探索提供了新的视角。

    Reducing the number of bits needed to encode the weights and activations of neural networks is highly desirable as it speeds up their training and inference time while reducing memory consumption. For these reasons, research in this area has attracted significant attention toward developing neural networks that leverage lower-precision computing, such as mixed-precision training. Interestingly, none of the existing approaches has investigated pure 16-bit floating-point settings. In this paper, we shed light on the overlooked efficiency of pure 16-bit floating-point neural networks. As such, we provide a comprehensive theoretical analysis to investigate the factors contributing to the differences observed between 16-bit and 32-bit models. We formalize the concepts of floating-point error and tolerance, enabling us to quantitatively explain the conditions under which a 16-bit model can closely approximate the results of its 32-bit counterpart. This theoretical exploration offers perspect
    

