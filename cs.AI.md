# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Data-driven Energy Consumption Modelling for Electric Micromobility using an Open Dataset](https://arxiv.org/abs/2403.17632) | 提出了一个专门针对电动微移动工具在都柏林收集的开放数据集，为解决实际场景中能耗建模的困难提供了重要资源 |
| [^2] | [LLM Agent Operating System](https://arxiv.org/abs/2403.16971) | 提出了一种将大型语言模型嵌入操作系统中的LLM代理操作系统，旨在优化资源分配、促进代理间上下文切换、实现并发执行以及为代理提供工具服务。 |
| [^3] | [Logits of API-Protected LLMs Leak Proprietary Information](https://arxiv.org/abs/2403.09539) | 大多数现代LLM受到softmax瓶颈影响，可以以较低成本获取API保护的LLM的非公开信息和解锁多种功能 |
| [^4] | [Curry-DPO: Enhancing Alignment using Curriculum Learning & Ranked Preferences](https://arxiv.org/abs/2403.07230) | 提出了一种名为Curry-DPO的方法，在直接偏好优化(DPO)中利用课程学习方法，通过构建多个偏好对来训练模型，相比于标准单一对DPO设置有着更好的性能表现。 |
| [^5] | [Emotion Classification in Low and Moderate Resource Languages](https://arxiv.org/abs/2402.18424) | 通过跨语言情感分类器，在低和中等资源语言中实现情感分类，展示了两种迁移学习方法的有效性。 |
| [^6] | [Hal-Eval: A Universal and Fine-grained Hallucination Evaluation Framework for Large Vision Language Models](https://arxiv.org/abs/2402.15721) | 本论文提出了Hal-Eval，一个通用和细粒度的幻觉评估框架，引入了新的幻觉分类法，专注于事件幻觉，通过生成和过滤细粒度幻觉数据来评估大型视觉语言模型对各种幻觉的处理能力。 |
| [^7] | [Fair Classifiers Without Fair Training: An Influence-Guided Data Sampling Approach](https://arxiv.org/abs/2402.12789) | 在不实施公平训练算法的情况下学习公平分类器，通过抽样具有影响力的数据来逐步转移原始训练数据，从而提高公平性和准确性。 |
| [^8] | [Secret Collusion Among Generative AI Agents](https://arxiv.org/abs/2402.07510) | 本文汇集了人工智能和安全领域的相关概念，系统地形式化了生成式AI代理系统中的秘密勾结问题，并提出了缓解措施。通过测试各种形式的秘密勾结所需的能力，我们发现当前模型的隐写能力有限，但 GPT-4 展示了能力的飞跃。 |
| [^9] | [Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks](https://arxiv.org/abs/2401.17263) | 该论文提出了一种鲁棒的提示优化算法（RPO）用于对抗语言模型的破解攻击，通过梯度优化来确保输出的无害性，并成功降低了攻击成功率。 |
| [^10] | [Learning Human-like Representations to Enable Learning Human Values](https://arxiv.org/abs/2312.14106) | 通过学习类人的表示，可以实现机器学习系统符合人类价值观，支持伦理等多方面的价值对齐。 |
| [^11] | [Revisiting Link Prediction: A Data Perspective](https://arxiv.org/abs/2310.00793) | 本文通过从数据的视角出发，重新审视了链接预测的原则，并发现了局部结构接近性、全局结构接近性和特征接近性三个因素之间的关系。同时，发现了全局结构接近性只在局部结构接近性不足时显示出有效性，以及特征和结构接近性之间的不兼容性。这些发现为链接预测提供了新的思路，启发了GNN4LP的设计。 |
| [^12] | [Towards Principled Graph Transformers.](http://arxiv.org/abs/2401.10119) | 边缘变换器是一个全局注意力模型，它具有至少3-WL的表达能力，能够在预测性能上超过其他架构，而不依赖于位置或结构编码。 |
| [^13] | [A Wireless AI-Generated Content (AIGC) Provisioning Framework Empowered by Semantic Communication.](http://arxiv.org/abs/2310.17705) | 一种由语义通信增强的无线AI生成内容（AIGC）供应框架，通过使用语义信息而不是所有的二进制位提取和传输内容，以解决在无线网络中提供最优AIGC服务的挑战。 |
| [^14] | [FLIRT: Feedback Loop In-context Red Teaming.](http://arxiv.org/abs/2308.04265) | 本论文提出了一个自动红队行动框架，通过反馈循环和背景学习来评估和暴露生成模型的漏洞，特别是对不安全和不适当内容的生成。对于文本到图像模型，采用不同的背景攻击策略可以学习出有效和多样化的对抗提示。相比基线方法，该策略在暴露漏洞方面更加有效，甚至在模型增加安全功能的情况下仍然能够发现漏洞。该框架也适用于红队行动文本到文本模型。 |
| [^15] | [Physically Realizable Natural-Looking Clothing Textures Evade Person Detectors via 3D Modeling.](http://arxiv.org/abs/2307.01778) | 通过3D建模，制作出与日常服装纹理相似的对抗性伪装纹理，可以在多个视角下避开人物检测，实现自然外观的服装纹理。 |
| [^16] | [Text-to-image Diffusion Model in Generative AI: A Survey.](http://arxiv.org/abs/2303.07909) | 本文调查了文本到图像扩散模型以及相关应用，总结了最先进的方法，并探讨了挑战和未来方向。 |
| [^17] | [Consciousness is learning: predictive processing systems that learn by binding may perceive themselves as conscious.](http://arxiv.org/abs/2301.07016) | 通过层级绑定和联想检索变为短期和长期声明性记忆的在线预测处理系统可能会感知到自己具有意识。 |

# 详细

[^1]: 使用开放数据集对电动微移动能耗建模

    Data-driven Energy Consumption Modelling for Electric Micromobility using an Open Dataset

    [https://arxiv.org/abs/2403.17632](https://arxiv.org/abs/2403.17632)

    提出了一个专门针对电动微移动工具在都柏林收集的开放数据集，为解决实际场景中能耗建模的困难提供了重要资源

    

    车辆拥堵和环境恶化带来的挑战日益加剧，凸显了在城市空间推行E-Mobility解决方案的重要性。特别是，E-滑板车和E-自行车等微型E-Mobility工具在这一转变中发挥着关键作用，为城市通勤者提供可持续的替代方案。然而，这些工具的能耗模式是影响其在现实场景中有效性的关键因素，对于出行规划以及增强用户在使用这些工具时的信心至关重要。为此，最近的研究利用针对特定移动工具和条件定制的物理模型，但这些模型在现实场景中的泛化能力和有效性存在困难，这是因为缺乏用于彻底模型评估和验证的开放数据集。为填补这一空白，我们的工作提出了一个在爱尔兰都柏林收集的开放数据集，专门用于能耗建模。

    arXiv:2403.17632v1 Announce Type: new  Abstract: The escalating challenges of traffic congestion and environmental degradation underscore the critical importance of embracing E-Mobility solutions in urban spaces. In particular, micro E-Mobility tools such as E-scooters and E-bikes, play a pivotal role in this transition, offering sustainable alternatives for urban commuters. However, the energy consumption patterns for these tools are a critical aspect that impacts their effectiveness in real-world scenarios and is essential for trip planning and boosting user confidence in using these. To this effect, recent studies have utilised physical models customised for specific mobility tools and conditions, but these models struggle with generalization and effectiveness in real-world scenarios due to a notable absence of open datasets for thorough model evaluation and verification. To fill this gap, our work presents an open dataset, collected in Dublin, Ireland, specifically designed for ene
    
[^2]: LLM Agent Operating System

    LLM Agent Operating System

    [https://arxiv.org/abs/2403.16971](https://arxiv.org/abs/2403.16971)

    提出了一种将大型语言模型嵌入操作系统中的LLM代理操作系统，旨在优化资源分配、促进代理间上下文切换、实现并发执行以及为代理提供工具服务。

    

    arXiv:2403.16971v1 公告类型: 跨领域 摘要: 部署大型语言模型（LLM）智能代理存在诸多挑战，会损害它们的效率和功效。其中包括代理请求在LLM上的次优调度和资源分配、在代理和LLM之间交互时保持上下文的困难，以及将具有不同能力和专业化的异构代理集成在一起的复杂性。代理数量和复杂性的快速增加进一步加剧了这些问题，通常会导致资源瓶颈和次优资源利用。受到这些挑战的启发，本文提出了AIOS，一种LLM代理操作系统，它将大型语言模型嵌入操作系统（OS）中。具体地，AIOS旨在优化资源分配，促进代理之间的上下文切换，实现代理的并发执行，为代理提供工具服务。

    arXiv:2403.16971v1 Announce Type: cross  Abstract: The integration and deployment of large language model (LLM)-based intelligent agents have been fraught with challenges that compromise their efficiency and efficacy. Among these issues are sub-optimal scheduling and resource allocation of agent requests over the LLM, the difficulties in maintaining context during interactions between agent and LLM, and the complexities inherent in integrating heterogeneous agents with different capabilities and specializations. The rapid increase of agent quantity and complexity further exacerbates these issues, often leading to bottlenecks and sub-optimal utilization of resources. Inspired by these challenges, this paper presents AIOS, an LLM agent operating system, which embeds large language model into operating systems (OS). Specifically, AIOS is designed to optimize resource allocation, facilitate context switch across agents, enable concurrent execution of agents, provide tool service for agents
    
[^3]: API保护的LLMs的标志泄露专有信息

    Logits of API-Protected LLMs Leak Proprietary Information

    [https://arxiv.org/abs/2403.09539](https://arxiv.org/abs/2403.09539)

    大多数现代LLM受到softmax瓶颈影响，可以以较低成本获取API保护的LLM的非公开信息和解锁多种功能

    

    大型语言模型（LLMs）的商业化导致了高级API-only接入专有模型的常见实践。在这项工作中，我们展示了即使对于模型架构有保守的假设，也可以从相对较少的API查询中学习关于API保护的LLM的大量非公开信息（例如，使用OpenAI的gpt-3.5-turbo仅花费不到1000美元）。我们的发现集中在一个关键观察上：大多数现代LLM受到了softmax瓶颈的影响，这限制了模型输出到完整输出空间的线性子空间。我们表明，这导致了一个模型图像或模型签名，从而以较低的成本解锁了几种功能：有效发现LLM的隐藏大小，获取完整词汇输出，检测和消除不同模型更新，识别给定单个完整LLM输出的源LLM，以及...

    arXiv:2403.09539v1 Announce Type: cross  Abstract: The commercialization of large language models (LLMs) has led to the common practice of high-level API-only access to proprietary models. In this work, we show that even with a conservative assumption about the model architecture, it is possible to learn a surprisingly large amount of non-public information about an API-protected LLM from a relatively small number of API queries (e.g., costing under $1,000 for OpenAI's gpt-3.5-turbo). Our findings are centered on one key observation: most modern LLMs suffer from a softmax bottleneck, which restricts the model outputs to a linear subspace of the full output space. We show that this lends itself to a model image or a model signature which unlocks several capabilities with affordable cost: efficiently discovering the LLM's hidden size, obtaining full-vocabulary outputs, detecting and disambiguating different model updates, identifying the source LLM given a single full LLM output, and eve
    
[^4]: Curry-DPO：利用课程学习和排名偏好增强对齐

    Curry-DPO: Enhancing Alignment using Curriculum Learning & Ranked Preferences

    [https://arxiv.org/abs/2403.07230](https://arxiv.org/abs/2403.07230)

    提出了一种名为Curry-DPO的方法，在直接偏好优化(DPO)中利用课程学习方法，通过构建多个偏好对来训练模型，相比于标准单一对DPO设置有着更好的性能表现。

    

    直接偏好优化(DPO)是一种有效的技术，利用成对偏好数据(通常是每个用户提示选择和拒绝的响应对)将LLMs与人类偏好对齐。在实践中，对于给定提示可能会存在多个响应，这些响应的质量相对于彼此而言有所不同。有了这些多个响应的质量评级，我们提出利用这些响应为给定提示创建多个偏好对。我们的工作侧重于通过课程学习方法系统地利用构建的多个偏好对来进行DPO训练。特别是，我们根据不同的标准将这些多个偏好数据对从易到难(模拟课程训练)排序。我们详细比较了我们提出的方法与标准单一对DPO设置。我们的方法，我们称之为Curry-DPO，在MTbench、Vicuna、Wiz上始终表现出增强的性能收益。

    arXiv:2403.07230v1 Announce Type: cross  Abstract: Direct Preference Optimization (DPO) is an effective technique that leverages pairwise preference data (usually one chosen and rejected response pair per user prompt) to align LLMs to human preferences. In practice, multiple responses can exist for a given prompt with varying quality relative to each other. With availability of such quality ratings for multiple responses, we propose utilizing these responses to create multiple preference pairs for a given prompt. Our work focuses on systematically using the constructed multiple preference pair in DPO training via curriculum learning methodology. In particular, we order these multiple pairs of preference data from easy to hard (emulating curriculum training) according to various criteria. We show detailed comparisons of our proposed approach to the standard single-pair DPO setting. Our method, which we call Curry-DPO consistently shows increased performance gains on MTbench, Vicuna, Wiz
    
[^5]: 低资源和中等资源语言中的情感分类

    Emotion Classification in Low and Moderate Resource Languages

    [https://arxiv.org/abs/2402.18424](https://arxiv.org/abs/2402.18424)

    通过跨语言情感分类器，在低和中等资源语言中实现情感分类，展示了两种迁移学习方法的有效性。

    

    能够分析全球范围内人们情绪状态是很重要的。全球有7100多种活跃语言，为每种语言构建情感分类是一项劳动密集型工作。特别是对于低资源和濒危语言，建立情感分类可能非常具有挑战性。我们提出了一种跨语言情感分类器，我们在资源丰富的语言（例如我们的工作中的英语）上训练情感分类器，并将学习迁移到低资源和中等资源的语言。我们比较并对比了从高资源语言到低资源或中等资源语言的两种迁移学习方法。一种方法将高资源语言的标注投影到低资源和中等资源语言的平行语料库中，另一种方法直接将高资源语言的学习迁移到其他语言。我们展示了我们的方法在6种语言上的有效性：Fa

    arXiv:2402.18424v1 Announce Type: cross  Abstract: It is important to be able to analyze the emotional state of people around the globe. There are 7100+ active languages spoken around the world and building emotion classification for each language is labor intensive. Particularly for low-resource and endangered languages, building emotion classification can be quite challenging. We present a cross-lingual emotion classifier, where we train an emotion classifier with resource-rich languages (i.e. \textit{English} in our work) and transfer the learning to low and moderate resource languages. We compare and contrast two approaches of transfer learning from a high-resource language to a low or moderate-resource language. One approach projects the annotation from a high-resource language to low and moderate-resource language in parallel corpora and the other one uses direct transfer from high-resource language to the other languages. We show the efficacy of our approaches on 6 languages: Fa
    
[^6]: Hal-Eval: 一种面向大型视觉语言模型的通用和细粒度幻觉评估框架

    Hal-Eval: A Universal and Fine-grained Hallucination Evaluation Framework for Large Vision Language Models

    [https://arxiv.org/abs/2402.15721](https://arxiv.org/abs/2402.15721)

    本论文提出了Hal-Eval，一个通用和细粒度的幻觉评估框架，引入了新的幻觉分类法，专注于事件幻觉，通过生成和过滤细粒度幻觉数据来评估大型视觉语言模型对各种幻觉的处理能力。

    

    大型视觉语言模型具有非凡的能力，但在图片和其描述之间存在幻觉不一致。以往对LVLMs进行的幻觉评估研究发现了关于对象、属性和关系的幻觉，但忽略了围绕虚构实体创建整个叙事的复杂幻觉。本文引入了一种精细的幻觉分类法，其中包括一个新的类别：事件幻觉。然后，我们利用先进的LLMs生成和过滤由各种类型的幻觉组成的细粒度幻觉数据，特别关注事件幻觉，为在我们的通用评估框架内集成辨别和生成评估方法奠定基础。所提出的基准可以独特地评估LVLMs处理广泛幻觉的能力，使其成为一个可靠和全面的工具。

    arXiv:2402.15721v1 Announce Type: new  Abstract: Large Vision Language Models exhibit remarkable capabilities but struggle with hallucinations inconsistencies between images and their descriptions. Previous hallucination evaluation studies on LVLMs have identified hallucinations in terms of objects, attributes, and relations but overlooked complex hallucinations that create an entire narrative around a fictional entity. In this paper, we introduce a refined taxonomy of hallucinations, featuring a new category: Event Hallucination. We then utilize advanced LLMs to generate and filter fine grained hallucinatory data consisting of various types of hallucinations, with a particular focus on event hallucinations, laying the groundwork for integrating discriminative and generative evaluation methods within our universal evaluation framework. The proposed benchmark distinctively assesses LVLMs ability to tackle a broad spectrum of hallucinations, making it a reliable and comprehensive tool fo
    
[^7]: 无需公平训练的公平分类器：一种受影响数据抽样方法

    Fair Classifiers Without Fair Training: An Influence-Guided Data Sampling Approach

    [https://arxiv.org/abs/2402.12789](https://arxiv.org/abs/2402.12789)

    在不实施公平训练算法的情况下学习公平分类器，通过抽样具有影响力的数据来逐步转移原始训练数据，从而提高公平性和准确性。

    

    一个公平的分类器应该确保来自不同群体的人们受益，而群体信息往往是敏感的，不适合模型训练。因此，在训练数据集中学习一个公平的分类器但排除敏感属性是很重要的。本文研究了学习公平分类器而不实现公平训练算法的方法，以避免可能泄露敏感信息。我们的理论分析验证了这种方法的可能性，即在具有适当分布偏移的数据集上进行传统训练可以同时减少公平差距的上限和模型泛化误差，表明公平性和准确性可以同步提高，只需简单地进行传统训练。然后，我们提出了一个可行的解决方案，通过抽样有影响力的数据逐步转移原始训练数据，在训练过程中不访问新数据的敏感属性。

    arXiv:2402.12789v1 Announce Type: cross  Abstract: A fair classifier should ensure the benefit of people from different groups, while the group information is often sensitive and unsuitable for model training. Therefore, learning a fair classifier but excluding sensitive attributes in the training dataset is important. In this paper, we study learning fair classifiers without implementing fair training algorithms to avoid possible leakage of sensitive information. Our theoretical analyses validate the possibility of this approach, that traditional training on a dataset with an appropriate distribution shift can reduce both the upper bound for fairness disparity and model generalization error, indicating that fairness and accuracy can be improved simultaneously with simply traditional training. We then propose a tractable solution to progressively shift the original training data during training by sampling influential data, where the sensitive attribute of new data is not accessed in s
    
[^8]: 生成式AI代理之间的秘密勾结

    Secret Collusion Among Generative AI Agents

    [https://arxiv.org/abs/2402.07510](https://arxiv.org/abs/2402.07510)

    本文汇集了人工智能和安全领域的相关概念，系统地形式化了生成式AI代理系统中的秘密勾结问题，并提出了缓解措施。通过测试各种形式的秘密勾结所需的能力，我们发现当前模型的隐写能力有限，但 GPT-4 展示了能力的飞跃。

    

    最近大型语言模型在能力上的增强为通信的生成式AI代理团队解决联合任务的应用打开了可能性。这引发了关于未经授权分享信息或其他不必要的代理协调形式的隐私和安全挑战。现代隐写术技术可能使这种动态难以检测。本文通过汲取人工智能和安全领域相关概念，全面系统地形式化了生成式AI代理系统中的秘密勾结问题。我们研究了使用隐写术的动机，并提出了各种缓解措施。我们的研究结果是一个模型评估框架，系统地测试了各种形式的秘密勾结所需的能力。我们在各种当代大型语言模型上提供了广泛的实证结果。虽然当前模型的隐写能力仍然有限，但 GPT-4 显示出能力的飞跃，这表明有必要进行进一步的研究。

    Recent capability increases in large language models (LLMs) open up applications in which teams of communicating generative AI agents solve joint tasks. This poses privacy and security challenges concerning the unauthorised sharing of information, or other unwanted forms of agent coordination. Modern steganographic techniques could render such dynamics hard to detect. In this paper, we comprehensively formalise the problem of secret collusion in systems of generative AI agents by drawing on relevant concepts from both the AI and security literature. We study incentives for the use of steganography, and propose a variety of mitigation measures. Our investigations result in a model evaluation framework that systematically tests capabilities required for various forms of secret collusion. We provide extensive empirical results across a range of contemporary LLMs. While the steganographic capabilities of current models remain limited, GPT-4 displays a capability jump suggesting the need fo
    
[^9]: 鲁棒的提示优化用于对抗语言模型的破解攻击

    Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks

    [https://arxiv.org/abs/2401.17263](https://arxiv.org/abs/2401.17263)

    该论文提出了一种鲁棒的提示优化算法（RPO）用于对抗语言模型的破解攻击，通过梯度优化来确保输出的无害性，并成功降低了攻击成功率。

    

    尽管在人工智能对齐方面取得了一些进展，但语言模型（LM）仍然容易受到对抗性攻击或破解攻击的影响，其中对手修改输入提示以诱导有害行为。虽然已经提出了一些防御方法，但它们仅关注狭窄的威胁模型，并不能提供强大的防御。为了实现强大的防御，我们首次提出了用于对抗破解攻击的对抗目标，并提出了一种名为鲁棒提示优化（RPO）的算法，该算法利用基于梯度的令牌优化来确保输出的无害性。通过这种方法，我们得到了一个易于访问的后缀，显著改善了对破解攻击的强韧性，包括优化过程中出现的破解攻击以及未知的破解攻击，将攻击成功率从84%降低到8.66%，在20个破解攻击中。此外，我们还发现RPO对正常LM使用的影响较小，在适应性攻击下仍然有效，并且可以迁移到黑盒模型中，降低攻击成功率。

    Despite advances in AI alignment, language models (LM) remain vulnerable to adversarial attacks or jailbreaking, in which adversaries modify input prompts to induce harmful behavior. While some defenses have been proposed, they focus on narrow threat models and fall short of a strong defense, which we posit should be effective, universal, and practical. To achieve this, we propose the first adversarial objective for defending LMs against jailbreaking attacks and an algorithm, robust prompt optimization (RPO), that uses gradient-based token optimization to enforce harmless outputs. This results in an easily accessible suffix that significantly improves robustness to both jailbreaks seen during optimization and unknown, held-out jailbreaks, reducing the attack success rate on Starling-7B from 84% to 8.66% across 20 jailbreaks. In addition, we find that RPO has a minor effect on normal LM use, is successful under adaptive attacks, and can transfer to black-box models, reducing the success
    
[^10]: 学习类人表示以实现学习类人价值观

    Learning Human-like Representations to Enable Learning Human Values

    [https://arxiv.org/abs/2312.14106](https://arxiv.org/abs/2312.14106)

    通过学习类人的表示，可以实现机器学习系统符合人类价值观，支持伦理等多方面的价值对齐。

    

    如何构建与人类价值观相一致的人工智能系统，以避免造成伤害或违反社会对可接受行为的标准？我们认为，人类与人工智能代理之间的表征对齐有助于价值观的对齐。使人工智能系统学习类人类对世界的表示具有许多已知好处，包括提高泛化能力、增强对领域转移的稳健性和提高少样本学习性能。我们提出，这种机器学习（ML）模型与人类之间的表示对齐也可以支持价值对齐，使ML系统遵循人类价值观和社会规范。我们关注伦理学作为价值对齐的一个方面，并在多臂老虎机设置中使用各种方法训练ML代理，其中奖励反映所选行动的道德可接受性。我们使用一个合成实验来证明代理与环境之间的表示对齐

    arXiv:2312.14106v2 Announce Type: replace  Abstract: How can we build AI systems that are aligned with human values to avoid causing harm or violating societal standards for acceptable behavior? We argue that representational alignment between humans and AI agents facilitates value alignment. Making AI systems learn human-like representations of the world has many known benefits, including improving generalization, robustness to domain shifts, and few-shot learning performance. We propose that this kind of representational alignment between machine learning (ML) models and humans can also support value alignment, allowing ML systems to conform to human values and societal norms. We focus on ethics as one aspect of value alignment and train ML agents using a variety of methods in a multi-armed bandit setting, where rewards reflect the moral acceptability of the chosen action. We use a synthetic experiment to demonstrate that agents' representational alignment with the environment bounds
    
[^11]: 重新审视链接预测: 一个数据的视角

    Revisiting Link Prediction: A Data Perspective

    [https://arxiv.org/abs/2310.00793](https://arxiv.org/abs/2310.00793)

    本文通过从数据的视角出发，重新审视了链接预测的原则，并发现了局部结构接近性、全局结构接近性和特征接近性三个因素之间的关系。同时，发现了全局结构接近性只在局部结构接近性不足时显示出有效性，以及特征和结构接近性之间的不兼容性。这些发现为链接预测提供了新的思路，启发了GNN4LP的设计。

    

    链接预测是一项基于图的基本任务，在各种应用中已被证明是不可或缺的，例如朋友推荐、蛋白质分析和药物互作预测。然而，由于数据集涵盖了多个领域，它们可能具有不同的链接形成机制。现有文献中的证据强调了一个普遍适用于所有数据集的最佳算法的缺失。在本文中，我们尝试从数据中心的视角探索链接预测的原则，跨越不同数据集。我们确定了三个对链接预测至关重要的基本因素:局部结构接近性、全局结构接近性和特征接近性。然后，我们揭示了这些因素之间的关系，其中 (i)只有在局部结构接近性不足的情况下，全局结构接近性才显示出有效性。 (ii)特征和结构接近性之间存在不兼容性。这种不兼容性导致了链接预测的图神经网络 (GNN4LP) 持续地

    Link prediction, a fundamental task on graphs, has proven indispensable in various applications, e.g., friend recommendation, protein analysis, and drug interaction prediction. However, since datasets span a multitude of domains, they could have distinct underlying mechanisms of link formation. Evidence in existing literature underscores the absence of a universally best algorithm suitable for all datasets. In this paper, we endeavor to explore principles of link prediction across diverse datasets from a data-centric perspective. We recognize three fundamental factors critical to link prediction: local structural proximity, global structural proximity, and feature proximity. We then unearth relationships among those factors where (i) global structural proximity only shows effectiveness when local structural proximity is deficient. (ii) The incompatibility can be found between feature and structural proximity. Such incompatibility leads to GNNs for Link Prediction (GNN4LP) consistently 
    
[^12]: 走向基于原则的图形变换器

    Towards Principled Graph Transformers. (arXiv:2401.10119v1 [cs.LG])

    [http://arxiv.org/abs/2401.10119](http://arxiv.org/abs/2401.10119)

    边缘变换器是一个全局注意力模型，它具有至少3-WL的表达能力，能够在预测性能上超过其他架构，而不依赖于位置或结构编码。

    

    基于k维Weisfeiler-Leman（k-WL）层次结构的图形学习架构提供了理论上很好理解的表达能力。然而，这样的架构在真实任务中往往无法提供可靠的预测性能，从而限制了它们的实际影响力。相比之下，基于全局注意力的模型如图形变换器在实践中表现出了强大的性能，但是将它们的表达能力与k-WL层次结构进行比较仍然具有挑战性，尤其是因为这些架构依赖于位置或结构编码来实现其表达能力和预测性能。为了解决这个问题，我们展示了最近提出的边缘变换器，这是一个在节点对而不是节点上进行操作的全局注意力模型，具有至少3-WL的表达能力。经验上，我们证明了边缘变换器在预测性能上超过了其他理论对齐的架构，同时不依赖于位置或结构编码。

    Graph learning architectures based on the k-dimensional Weisfeiler-Leman (k-WL) hierarchy offer a theoretically well-understood expressive power. However, such architectures often fail to deliver solid predictive performance on real-world tasks, limiting their practical impact. In contrast, global attention-based models such as graph transformers demonstrate strong performance in practice, but comparing their expressive power with the k-WL hierarchy remains challenging, particularly since these architectures rely on positional or structural encodings for their expressivity and predictive performance. To address this, we show that the recently proposed Edge Transformer, a global attention model operating on node pairs instead of nodes, has at least 3-WL expressive power. Empirically, we demonstrate that the Edge Transformer surpasses other theoretically aligned architectures regarding predictive performance while not relying on positional or structural encodings.
    
[^13]: 一种由语义通信增强的无线AI生成内容（AIGC）供应框架

    A Wireless AI-Generated Content (AIGC) Provisioning Framework Empowered by Semantic Communication. (arXiv:2310.17705v1 [cs.NI])

    [http://arxiv.org/abs/2310.17705](http://arxiv.org/abs/2310.17705)

    一种由语义通信增强的无线AI生成内容（AIGC）供应框架，通过使用语义信息而不是所有的二进制位提取和传输内容，以解决在无线网络中提供最优AIGC服务的挑战。

    

    近期，生成式AI应用通过创建多样化且高质量的AI生成内容（AIGC）来满足广大用户群体的需求。随着移动设备的普及和移动流量的快速增长，通过无线通信网络提供对高质量AIGC服务的无处不在的访问已成为AIGC产品的未来方向。然而，在不稳定的信道、有限的带宽资源和分布不均匀的计算资源的无线网络中提供最优的AIGC服务是具有挑战性的。为了解决这些挑战，我们提出了一个由语义通信（SemCom）增强的AIGC（SemAIGC）生成和传输框架，其中只需提取和传输内容的语义信息而不是所有的二进制位。具体而言，SemAIGC在语义编码器和解码器中集成了基于扩散的模型，以实现高效的内容生成和灵活调整计算工作负载的目的。

    Generative AI applications are recently catering to a vast user base by creating diverse and high-quality AI-generated content (AIGC). With the proliferation of mobile devices and rapid growth of mobile traffic, providing ubiquitous access to high-quality AIGC services via wireless communication networks is becoming the future direction for AIGC products. However, it is challenging to provide optimal AIGC services in wireless networks with unstable channels, limited bandwidth resources, and unevenly distributed computational resources. To tackle these challenges, we propose a semantic communication (SemCom)-empowered AIGC (SemAIGC) generation and transmission framework, where only semantic information of the content rather than all the binary bits should be extracted and transmitted by using SemCom. Specifically, SemAIGC integrates diffusion-based models within the semantic encoder and decoder for efficient content generation and flexible adjustment of the computing workload of both tr
    
[^14]: FLIRT: 反馈循环背景下的红队行动

    FLIRT: Feedback Loop In-context Red Teaming. (arXiv:2308.04265v1 [cs.AI])

    [http://arxiv.org/abs/2308.04265](http://arxiv.org/abs/2308.04265)

    本论文提出了一个自动红队行动框架，通过反馈循环和背景学习来评估和暴露生成模型的漏洞，特别是对不安全和不适当内容的生成。对于文本到图像模型，采用不同的背景攻击策略可以学习出有效和多样化的对抗提示。相比基线方法，该策略在暴露漏洞方面更加有效，甚至在模型增加安全功能的情况下仍然能够发现漏洞。该框架也适用于红队行动文本到文本模型。

    

    警告：本论文内容可能不合适或冒犯人。随着生成模型在各种应用中可供公众使用，测试和分析这些模型的漏洞已成为一项优先任务。在这里，我们提出了一个自动红队行动框架，对给定模型进行评估，并暴露其对不安全和不适当内容生成的漏洞。我们的框架使用反馈循环中的背景学习来进行红队行动，并激发模型生成不安全内容。我们提出了不同的背景攻击策略，用于自动学习用于文本到图像模型的有效和多样化的对抗提示。我们的实验表明，与基线方法相比，我们提出的策略在暴露稳定扩散(SD)模型的漏洞时显著更有效，即使后者采用了安全功能的增强。此外，我们证明了所提出的框架对于红队行动文本到文本模型也是有效的。

    Warning: this paper contains content that may be inappropriate or offensive.  As generative models become available for public use in various applications, testing and analyzing vulnerabilities of these models has become a priority. Here we propose an automatic red teaming framework that evaluates a given model and exposes its vulnerabilities against unsafe and inappropriate content generation. Our framework uses in-context learning in a feedback loop to red team models and trigger them into unsafe content generation. We propose different in-context attack strategies to automatically learn effective and diverse adversarial prompts for text-to-image models. Our experiments demonstrate that compared to baseline approaches, our proposed strategy is significantly more effective in exposing vulnerabilities in Stable Diffusion (SD) model, even when the latter is enhanced with safety features. Furthermore, we demonstrate that the proposed framework is effective for red teaming text-to-text mo
    
[^15]: 通过3D建模，实现自然外观的服装纹理以逃避人物检测器

    Physically Realizable Natural-Looking Clothing Textures Evade Person Detectors via 3D Modeling. (arXiv:2307.01778v1 [cs.CV])

    [http://arxiv.org/abs/2307.01778](http://arxiv.org/abs/2307.01778)

    通过3D建模，制作出与日常服装纹理相似的对抗性伪装纹理，可以在多个视角下避开人物检测，实现自然外观的服装纹理。

    

    最近的研究提出了制作对抗性服装来逃避人物检测器，但要么只对限定的视角有效，要么对人类非常明显。我们旨在基于3D建模来制作对抗性的服装纹理，这个想法已经被用于制作刚性的对抗性物体，如3D打印的乌龟。与刚性物体不同，人类和服装是非刚性的，这导致了在实际制作中的困难。为了制作出看起来自然的对抗性服装，可以在多个视角下避开人物检测器，我们提出了类似于日常服装纹理之一的对抗性伪装纹理（AdvCaT），即伪装纹理。我们利用Voronoi图和Gumbel-softmax技巧来参数化伪装纹理，并通过3D建模来优化参数。此外，我们还提出了一个高效的增强管道，将拓扑合理的投影（TopoProj）和Thin Plate Spline（TPS）结合在3D网格上使用。

    Recent works have proposed to craft adversarial clothes for evading person detectors, while they are either only effective at limited viewing angles or very conspicuous to humans. We aim to craft adversarial texture for clothes based on 3D modeling, an idea that has been used to craft rigid adversarial objects such as a 3D-printed turtle. Unlike rigid objects, humans and clothes are non-rigid, leading to difficulties in physical realization. In order to craft natural-looking adversarial clothes that can evade person detectors at multiple viewing angles, we propose adversarial camouflage textures (AdvCaT) that resemble one kind of the typical textures of daily clothes, camouflage textures. We leverage the Voronoi diagram and Gumbel-softmax trick to parameterize the camouflage textures and optimize the parameters via 3D modeling. Moreover, we propose an efficient augmentation pipeline on 3D meshes combining topologically plausible projection (TopoProj) and Thin Plate Spline (TPS) to narr
    
[^16]: 生成AI中的文本到图像扩散模型：一项调查

    Text-to-image Diffusion Model in Generative AI: A Survey. (arXiv:2303.07909v1 [cs.CV])

    [http://arxiv.org/abs/2303.07909](http://arxiv.org/abs/2303.07909)

    本文调查了文本到图像扩散模型以及相关应用，总结了最先进的方法，并探讨了挑战和未来方向。

    

    本文调查了文本到图像扩散模型，这些模型已经成为多种生成任务中流行的模型。作为一个自包含的工作，本调查从简单介绍基本扩散模型如何用于图像合成开始，接着是条件或引导如何改进学习。我们还总结了文本条件下的最先进的图像合成方法，并且进一步总结了文本引导创意生成和图像编辑的应用。除了迄今为止所取得的进展，我们还讨论了现有挑战和有前途的未来方向。

    This survey reviews text-to-image diffusion models in the context that diffusion models have emerged to be popular for a wide range of generative tasks. As a self-contained work, this survey starts with a brief introduction of how a basic diffusion model works for image synthesis, followed by how condition or guidance improves learning. Based on that, we present a review of state-of-the-art methods on text-conditioned image synthesis, i.e., text-to-image. We further summarize applications beyond text-to-image generation: text-guided creative generation and text-guided image editing. Beyond the progress made so far, we discuss existing challenges and promising future directions.
    
[^17]: 意识是学习的过程：通过绑定学习的预测处理系统可能会将自己感知为有意识的

    Consciousness is learning: predictive processing systems that learn by binding may perceive themselves as conscious. (arXiv:2301.07016v2 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2301.07016](http://arxiv.org/abs/2301.07016)

    通过层级绑定和联想检索变为短期和长期声明性记忆的在线预测处理系统可能会感知到自己具有意识。

    

    机器学习算法在特定复杂领域实现了超越人类的表现。然而，从少量示例中进行在线学习，并在不同领域之间高效地泛化仍然是难以实现的。在人类身上，这种学习通过声明性存储过程进行，并且与意识密切相关。预测处理被推广为一种基于贝叶斯推理框架的原则性方法，用于理解皮质如何实现深度生成感知模型，用于感官数据和行为控制。然而，预测处理对于快速组成式学习或意识之谜提供了很少的直接见解。在这里，我们提出，通过通过绑定预测中的层次模型来实现在线学习，预测处理系统可以通过从单个示例中为感知和行动形成工作记忆，在新情况下灵活泛化，这可通过联想检索变为短期和长期的声明性记忆。我们认为，这个过程，我们称之为“在线层级预测绑定”，也可能是系统感知自己具有意识的必要条件。由此产生的模型提供了一种关于感知的、运动的、认知的和情感的意识的统一解释，并具有进化和发育生物学的深刻根源。

    Machine learning algorithms have achieved superhuman performance in specific complex domains. Yet learning online from few examples and efficiently generalizing across domains remains elusive. In humans such learning proceeds via declarative memory formation and is closely associated with consciousness. Predictive processing has been advanced as a principled Bayesian inference framework for understanding the cortex as implementing deep generative perceptual models for both sensory data and action control. However, predictive processing offers little direct insight into fast compositional learning or the mystery of consciousness. Here we propose that through implementing online learning by hierarchical binding of unpredicted inferences, a predictive processing system may flexibly generalize in novel situations by forming working memories for perceptions and actions from single examples, which can become short- and long-term declarative memories retrievable by associative recall. We argu
    

