# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [NoFunEval: Funny How Code LMs Falter on Requirements Beyond Functional Correctness](https://rss.arxiv.org/abs/2401.15963) | 本论文提出了一个新的基准测试 NoFunEval，用于评估代码语言模型在非功能性要求和简单分类实例方面的表现。研究发现，目前的代码语言模型在处理这些要求时存在根本性的盲点。 |
| [^2] | [ContactHandover: Contact-Guided Robot-to-Human Object Handover](https://arxiv.org/abs/2404.01402) | ContactHandover是一个机器人向人类递送物体的系统，通过接触引导的抓取和物体递送阶段来实现成功的物体递送。 |
| [^3] | [FABLES: Evaluating faithfulness and content selection in book-length summarization](https://arxiv.org/abs/2404.01261) | 本文首次对LLM生成的虚构书籍摘要进行了忠实性和内容选择的大规模人类评估，建立了FABLES数据集，通过对26本书的3158个声明进行了注释，成功对LLM摘要进行了基于忠实性的排名 |
| [^4] | [Finding Decision Tree Splits in Streaming and Massively Parallel Models](https://arxiv.org/abs/2403.19867) | 提出了在数据流学习中计算决策树最佳分割点的算法，能够在流式计算和大规模并行模型中高效运行 |
| [^5] | [RPMArt: Towards Robust Perception and Manipulation for Articulated Objects](https://arxiv.org/abs/2403.16023) | 提出了面向关节对象的健壮感知和操作框架RPMArt，主要贡献是能够稳健地预测关节参数和可信点的RoArtNet。 |
| [^6] | [Tastle: Distract Large Language Models for Automatic Jailbreak Attack](https://arxiv.org/abs/2403.08424) | Tastle是一种新颖的黑盒越狱框架，采用恶意内容隐藏和内存重构以及迭代优化算法，用于自动对大型语言模型进行红队攻击。 |
| [^7] | [SARDet-100K: Towards Open-Source Benchmark and ToolKit for Large-Scale SAR Object Detection](https://arxiv.org/abs/2403.06534) | SARDet-100K是第一个COCO级别的大规模多类别SAR物体检测数据集，为研究提供了大规模且多样化的数据集，揭示了SAR物体检测中预训练模型显著差异的关键挑战。 |
| [^8] | [ProMISe: Promptable Medical Image Segmentation using SAM](https://arxiv.org/abs/2403.04164) | 本文提出了一个自动提示模块（APM），为SAM基础模型提供了在目标领域中使用的自适应提示，显著提高了SAM在医学图像分割中的性能。 |
| [^9] | [Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization](https://arxiv.org/abs/2403.03419) | 通过提出Distributional Dispreference Optimization (D$^2$O)方法，在不需要人类正样本的情况下实现了对齐，减少了有害信息的传播。 |
| [^10] | [Cause and Effect: Can Large Language Models Truly Understand Causality?](https://arxiv.org/abs/2402.18139) | 本研究提出了一种名为CARE CA的新型架构，通过结合显式因果检测模块和反事实陈述、以及隐含因果检测模块，旨在增强大型语言模型对因果关系的理解能力。 |
| [^11] | [Chain-of-Discussion: A Multi-Model Framework for Complex Evidence-Based Question Answering](https://arxiv.org/abs/2402.16313) | 提出了一种Chain-of-Discussion框架，通过多个开源语言模型的协同作用，提高了复杂问题回答的质量 |
| [^12] | [PANDA (Pedantic ANswer-correctness Determination and Adjudication):Improving Automatic Evaluation for Question Answering and Text Generation](https://arxiv.org/abs/2402.11161) | 提出了PANDA方法，引入了更精确的答案正确性评测方式，解决了当前自动评估问答和文本生成过程中的挑战。 |
| [^13] | [Improving Image Coding for Machines through Optimizing Encoder via Auxiliary Loss](https://arxiv.org/abs/2402.08267) | 通过应用辅助损失优化编码器，我们提出了一种改进的机器图像编码方法，能够在目标检测和语义分割任务中实现显著的速率提高。 |
| [^14] | [Policy Improvement using Language Feedback Models](https://arxiv.org/abs/2402.07876) | 本文介绍了一种使用语言反馈模型（LFMs）改进政策的方法，通过识别期望的行为并进行模仿学习，我们在任务完成率、泛化性能和人类可解释性方面取得了显著改进。 |
| [^15] | [Systematic Biases in LLM Simulations of Debates](https://arxiv.org/abs/2402.04049) | 本研究揭示了LLMs在模拟政治辩论中存在的系统性偏差，尽管被指定从特定的政治观点进行辩论，LLMs代理机构倾向于遵循模型固有的社会偏见。通过自动自我优化方法，我们进一步证实了这些观察结果。 |
| [^16] | [Multimodal-Enhanced Objectness Learner for Corner Case Detection in Autonomous Driving](https://arxiv.org/abs/2402.02026) | 本文提出了一种用于角落案例检测的多模态增强物件学习器（MENOL）。该方法通过减小已知类和未知类之间的差异，并引入多模态的数据，显著提高了对新类别的召回率，并降低了训练成本。 |
| [^17] | [Social, Legal, Ethical, Empathetic, and Cultural Rules: Compilation and Reasoning (Extended Version)](https://arxiv.org/abs/2312.09699) | 该研究介绍了SLEEC（社会、法律、伦理、移情、文化）规则的概念，旨在推动AI系统遵守人类背景相关规则的制定、验证和执行。 |
| [^18] | [Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept Intervention, and Conditional Interpretations.](http://arxiv.org/abs/2401.14142) | 基于能量的概念瓶颈模型统一了预测、概念干预和条件解释的功能，解决了现有方法在高阶非线性相互作用和复杂条件依赖关系上的限制。 |
| [^19] | [TAT-LLM: A Specialized Language Model for Discrete Reasoning over Tabular and Textual Data.](http://arxiv.org/abs/2401.13223) | TAT-LLM是一种专门用于离散推理的语言模型，针对混合表格和文本数据上的问答任务。该模型通过分步流水线的方式，包括提取器、推理器和执行器，利用LLMs的强大能力来解决问题。而为了应对成本、延迟和数据安全风险等挑战，我们开发了TAT-LLM，一个专门针对此任务的较小LLM。 |
| [^20] | [Gaussian Adaptive Attention is All You Need: Robust Contextual Representations Across Multiple Modalities.](http://arxiv.org/abs/2401.11143) | 该论文提出了一个名为GAAM的多头高斯自适应注意力机制，用于增强跨多个模态的信息聚合。通过将可学习的均值和方差纳入注意力机制中，GAAM能够动态地重新调整特征的重要性，从而在处理非平稳数据时取得了显著的性能提升，超过了目前现有的注意力技术。该方法的适应性强且参数数量较少，具有改进现有注意力框架的潜力。 |
| [^21] | [Towards Learning from Graphs with Heterophily: Progress and Future.](http://arxiv.org/abs/2401.09769) | 本调查综合概述了关于从带有异质性的图中学习的现有研究，并根据学习策略、模型架构和实际应用等方面对方法进行了分类。同时讨论了现有研究的主要挑战，并提出了未来研究的潜在方向。 |
| [^22] | [The Art of Deception: Robust Backdoor Attack using Dynamic Stacking of Triggers.](http://arxiv.org/abs/2401.01537) | 这项研究介绍了一种使用动态触发器进行强健后门攻击的方法，通过巧妙设计的调整，使损坏的样本与干净的样本无法区分，实验证明这种方法可以成功地欺骗语音识别系统。 |
| [^23] | [Knowledge Enhanced Conditional Imputation for Healthcare Time-series.](http://arxiv.org/abs/2312.16713) | 本研究提出了一种知识增强的条件插补方法，针对医疗时间序列数据中的缺失数据问题。通过整合先进的知识嵌入和非均匀掩蔽策略，该方法能够灵活适应不同模式的电子健康记录中的缺失数据分布不平衡问题。 |
| [^24] | [Suppressing Overestimation in Q-Learning through Adversarial Behaviors.](http://arxiv.org/abs/2310.06286) | 本文提出了一种新的Q学习算法，通过引入虚拟对抗性玩家，有效调节了标准Q学习中的过高估计偏差，提出的算法简单而有效，能够轻松应用于强化学习算法并提高性能。 |
| [^25] | [STAMP: Differentiable Task and Motion Planning via Stein Variational Gradient Descent.](http://arxiv.org/abs/2310.01775) | STAMP是一种基于Stein变分梯度下降的算法，通过并行化和可微仿真高效地搜索多个多样化的任务和运动规划解决方案。 |
| [^26] | [CyberForce: A Federated Reinforcement Learning Framework for Malware Mitigation.](http://arxiv.org/abs/2308.05978) | CyberForce是一个联邦强化学习框架，用于在物联网设备中协同私密地确定适合缓解各种零日攻击的MTD技术。它整合了设备指纹识别和异常检测，并通过奖励或惩罚FRL agent选择的MTD机制来提高网络安全性。 |
| [^27] | [Interpretable Stereotype Identification through Reasoning.](http://arxiv.org/abs/2308.00071) | 本研究通过使用推理方法，在零射击刻板印象识别中取得了重要的进展，并发现推理的性能增益远远超过模型规模扩展的增益。推理不仅提高了准确性，还提高了决策的可解释性。 |
| [^28] | [RL + Model-based Control: Using On-demand Optimal Control to Learn Versatile Legged Locomotion.](http://arxiv.org/abs/2305.17842) | 本文提出了一种 RL+模型控制框架以开发出可以有效可靠地学习的健壮控制策略，通过整合有限时间最优控制生成的按需参考运动分散 RL 过程，同时克服了建模简化的固有局限性，在足式 locomotion 上实现了多功能和强健，能泛化参考运动并处理更复杂的运动任务。 |
| [^29] | [Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations.](http://arxiv.org/abs/2305.16326) | 本文研究了GPT-3和GPT-4在生物医学自然语言处理中的表现，分析了它们可能产生的错误类型，并提供了使用这些模型的建议。 |
| [^30] | [RLocator: Reinforcement Learning for Bug Localization.](http://arxiv.org/abs/2305.05586) | 本文提出了一种基于强化学习的Bug定位方法RLocator，相较于其他最先进的Bug定位技术具有更优越的性能。 |
| [^31] | [Policy Optimization over General State and Action Spaces.](http://arxiv.org/abs/2211.16715) | 本文提出了一种新方法并引入了函数近似来解决通用状态和动作空间上的强化学习问题，同时介绍了一种新的策略双平均法。 |
| [^32] | [Overlapping Community Detection using Dynamic Dilated Aggregation in Deep Residual GCN.](http://arxiv.org/abs/2210.11174) | 本论文提出了一种使用动态扩张聚合的深度残差GCN方法进行重叠社区检测。通过设计深度动态残差图卷积网络和统一的编码器-解码器框架，实现了在不规则图上进行社区检测。实验结果表明，该方法在不同数据集上取得了较好的效果。 |

# 详细

[^1]: NoFunEval: 有趣的是，代码语言模型在超出功能正确性的要求上遇到困难

    NoFunEval: Funny How Code LMs Falter on Requirements Beyond Functional Correctness

    [https://rss.arxiv.org/abs/2401.15963](https://rss.arxiv.org/abs/2401.15963)

    本论文提出了一个新的基准测试 NoFunEval，用于评估代码语言模型在非功能性要求和简单分类实例方面的表现。研究发现，目前的代码语言模型在处理这些要求时存在根本性的盲点。

    

    现有的代码语言模型（code LMs）的评估基准几乎完全集中在LMs是否能够生成功能正确的代码上。在实际的软件工程中，开发人员会考虑超出功能正确性的要求。他们对于“如何”实现功能有着对整体系统设计目标（如效率、安全性和可维护性）的要求。如果LMs能够展示对要求和代码语义的强大理解能力，他们也会更加信任这些LMs。我们提出了一个新的基准测试NoFunEval来评估代码LMs在非功能性要求和简单分类实例方面的表现。我们提出了一个提示方法Coding Concepts (CoCo)，可以用于开发人员向LMs传达领域知识。我们对22个代码LMs进行了广泛评估，发现它们在我们的基准测试中普遍表现不佳，暗示着它们在处理这些问题时存在根本性的盲点。

    Existing evaluation benchmarks of language models of code (code LMs) focus almost exclusively on whether the LMs can generate functionally-correct code. In real-world software engineering, developers think beyond functional correctness. They have requirements on "how" a functionality should be implemented to meet overall system design objectives like efficiency, security, and maintainability. They would also trust the code LMs more if the LMs demonstrate robust understanding of requirements and code semantics.   We propose a new benchmark NoFunEval to evaluate code LMs on non-functional requirements and simple classification instances for both functional and non-functional requirements. We propose a prompting method, Coding Concepts (CoCo), as a way for a developer to communicate the domain knowledge to the LMs. We conduct an extensive evaluation of twenty-two code LMs. Our finding is that they generally falter when tested on our benchmark, hinting at fundamental blindspots in their tr
    
[^2]: ContactHandover: 接触引导的机器人向人类递送物体

    ContactHandover: Contact-Guided Robot-to-Human Object Handover

    [https://arxiv.org/abs/2404.01402](https://arxiv.org/abs/2404.01402)

    ContactHandover是一个机器人向人类递送物体的系统，通过接触引导的抓取和物体递送阶段来实现成功的物体递送。

    

    机器人向人类递送物体是许多人机协作任务中的重要一步。成功的递送需要机器人保持对物体的稳定抓取，同时确保人类以一种自然且易于使用的方式接收物体。我们提出了ContactHandover，这是一个机器人向人类递送物体的系统，包括两个阶段：接触引导的抓取阶段和物体递送阶段。在抓取阶段，ContactHandover预测机器人的6自由度抓取姿势和人类接触点在物体上的3D可供性图。机器人的抓取姿势通过惩罚那些阻碍人类接触点的姿势进行重新排序，并执行排名最高的抓取。在递送阶段，通过最大化靠近人类的接触点并最小化人类手臂关节扭矩和位移来计算机器人末端执行器姿势。我们在27种不同家用物品上评估了我们的系统，并展示了o

    arXiv:2404.01402v1 Announce Type: cross  Abstract: Robot-to-human object handover is an important step in many human robot collaboration tasks. A successful handover requires the robot to maintain a stable grasp on the object while making sure the human receives the object in a natural and easy-to-use manner. We propose ContactHandover, a robot to human handover system that consists of two phases: a contact-guided grasping phase and an object delivery phase. During the grasping phase, ContactHandover predicts both 6-DoF robot grasp poses and a 3D affordance map of human contact points on the object. The robot grasp poses are reranked by penalizing those that block human contact points, and the robot executes the highest ranking grasp. During the delivery phase, the robot end effector pose is computed by maximizing human contact points close to the human while minimizing the human arm joint torques and displacements. We evaluate our system on 27 diverse household objects and show that o
    
[^3]: FABLES：评估书籍摘要中的忠实性和内容选择

    FABLES: Evaluating faithfulness and content selection in book-length summarization

    [https://arxiv.org/abs/2404.01261](https://arxiv.org/abs/2404.01261)

    本文首次对LLM生成的虚构书籍摘要进行了忠实性和内容选择的大规模人类评估，建立了FABLES数据集，通过对26本书的3158个声明进行了注释，成功对LLM摘要进行了基于忠实性的排名

    

    虽然长文本大语言模型（LLMs）在技术上可以总结长达100K个标记的书籍，但迄今为止，文档的长度和复杂性阻碍了对忠实性等输入相关方面的评估。本文在虚构书籍的LLM生成摘要上进行了首次大规模人类评估，通过专注于2023或2024年出版的书籍摘要，雇佣在进行注释任务之前已完全阅读每本书的注释者来减少成本和认知负担，从而缓解了数据污染问题。我们收集了FABLES数据集，对26本书的LLM生成摘要中的3158个声明进行了注释，花费了5200美元，这使我们能够基于忠实性对LLM摘要进行排名：Claude-3-Opus在忠实性方面明显优于所有闭源LLMs，而开源的Mixtral与GPT-3.5-Turbo持平。

    arXiv:2404.01261v1 Announce Type: cross  Abstract: While long-context large language models (LLMs) can technically summarize book-length documents (>100K tokens), the length and complexity of the documents have so far prohibited evaluations of input-dependent aspects like faithfulness. In this paper, we conduct the first large-scale human evaluation of faithfulness and content selection on LLM-generated summaries of fictional books. Our study mitigates the issue of data contamination by focusing on summaries of books published in 2023 or 2024, and we hire annotators who have fully read each book prior to the annotation task to minimize cost and cognitive burden. We collect FABLES, a dataset of annotations on 3,158 claims made in LLM-generated summaries of 26 books, at a cost of $5.2K USD, which allows us to rank LLM summarizers based on faithfulness: Claude-3-Opus significantly outperforms all closed-source LLMs, while the open-source Mixtral is on par with GPT-3.5-Turbo. An analysis o
    
[^4]: 在流式和大规模并行模型中找到决策树分割点

    Finding Decision Tree Splits in Streaming and Massively Parallel Models

    [https://arxiv.org/abs/2403.19867](https://arxiv.org/abs/2403.19867)

    提出了在数据流学习中计算决策树最佳分割点的算法，能够在流式计算和大规模并行模型中高效运行

    

    在这项工作中，我们提出了一种数据流算法，用于计算决策树学习中的最优分割点。具体而言，给定观测数据流$x_i$及其标签$y_i$，目标是找到将数据分为两组的最佳分割点$j$，使得均方误差（回归问题）或误分类率（分类问题）最小化。我们提供了多种快速的数据流算法，这些算法在这些问题中使用亚线性空间和少量次数的遍历。这些算法还可以扩展到大规模并行计算模型中。尽管不能直接比较，但我们的工作与Domingos和Hulten的开创性工作（KDD 2000）相互补充。

    arXiv:2403.19867v1 Announce Type: cross  Abstract: In this work, we provide data stream algorithms that compute optimal splits in decision tree learning. In particular, given a data stream of observations $x_i$ and their labels $y_i$, the goal is to find the optimal split point $j$ that divides the data into two sets such that the mean squared error (for regression) or misclassification rate (for classification) is minimized. We provide various fast streaming algorithms that use sublinear space and a small number of passes for these problems. These algorithms can also be extended to the massively parallel computation model. Our work, while not directly comparable, complements the seminal work of Domingos and Hulten (KDD 2000).
    
[^5]: RPMArt：面向关节对象的健壮感知和操作

    RPMArt: Towards Robust Perception and Manipulation for Articulated Objects

    [https://arxiv.org/abs/2403.16023](https://arxiv.org/abs/2403.16023)

    提出了面向关节对象的健壮感知和操作框架RPMArt，主要贡献是能够稳健地预测关节参数和可信点的RoArtNet。

    

    关节对象在日常生活中很常见。对于真实世界的机器人应用来说，机器人能够表现出对关节对象的健壮感知和操作技能是至关重要的。然而，现有的关节对象方法不够解决点云中的噪声问题，难以弥合模拟与现实之间的差距，从而限制了在真实场景中的实际部署。为了解决这些挑战，我们提出了一个面向关节对象的健壮感知和操作的框架（RPMArt），该框架学习如何从嘈杂的点云中估计关节参数并操作关节部分。我们的主要贡献是一个健壮关节网络（RoArtNet），通过局部特征学习和点元组投票能够稳健地预测关节参数和可信点。此外，我们引入了一个关节感知分类方案来增强其能力。

    arXiv:2403.16023v1 Announce Type: cross  Abstract: Articulated objects are commonly found in daily life. It is essential that robots can exhibit robust perception and manipulation skills for articulated objects in real-world robotic applications. However, existing methods for articulated objects insufficiently address noise in point clouds and struggle to bridge the gap between simulation and reality, thus limiting the practical deployment in real-world scenarios. To tackle these challenges, we propose a framework towards Robust Perception and Manipulation for Articulated Objects (RPMArt), which learns to estimate the articulation parameters and manipulate the articulation part from the noisy point cloud. Our primary contribution is a Robust Articulation Network (RoArtNet) that is able to predict both joint parameters and affordable points robustly by local feature learning and point tuple voting. Moreover, we introduce an articulation-aware classification scheme to enhance its ability
    
[^6]: Tastle: 为自动越狱攻击干扰大型语言模型

    Tastle: Distract Large Language Models for Automatic Jailbreak Attack

    [https://arxiv.org/abs/2403.08424](https://arxiv.org/abs/2403.08424)

    Tastle是一种新颖的黑盒越狱框架，采用恶意内容隐藏和内存重构以及迭代优化算法，用于自动对大型语言模型进行红队攻击。

    

    大型语言模型（LLMs）近年来取得了重要进展。在LLMs公开发布之前，人们已经做出了大量努力来将它们的行为与人类价值观保持一致。对齐的主要目标是确保它们的有益性、诚实性和无害性。然而，即使经过细致对齐的LLMs仍然容易受到恶意操纵，如越狱，导致意外的行为。越狱是有意开发恶意提示，从LLM安全限制中逃脱以生成未经审查的有害内容。以前的工作探索了不同的越狱方法来对LLMs进行红队攻击，但它们在效果和可伸缩性方面遇到了挑战。在这项工作中，我们提出了Tastle，一种新颖的黑盒越狱框架，用于自动对LLMs进行红队攻击。我们设计了恶意内容隐藏和内存重构，并结合迭代优化算法来越狱LLMs。

    arXiv:2403.08424v1 Announce Type: cross  Abstract: Large language models (LLMs) have achieved significant advances in recent days. Extensive efforts have been made before the public release of LLMs to align their behaviors with human values. The primary goal of alignment is to ensure their helpfulness, honesty and harmlessness. However, even meticulously aligned LLMs remain vulnerable to malicious manipulations such as jailbreaking, leading to unintended behaviors. The jailbreak is to intentionally develop a malicious prompt that escapes from the LLM security restrictions to produce uncensored detrimental contents. Previous works explore different jailbreak methods for red teaming LLMs, yet they encounter challenges regarding to effectiveness and scalability. In this work, we propose Tastle, a novel black-box jailbreak framework for automated red teaming of LLMs. We designed malicious content concealing and memory reframing with an iterative optimization algorithm to jailbreak LLMs, mo
    
[^7]: SARDet-100K: 面向大规模合成孔径雷达 SAR 物体检测的开源基准和工具包

    SARDet-100K: Towards Open-Source Benchmark and ToolKit for Large-Scale SAR Object Detection

    [https://arxiv.org/abs/2403.06534](https://arxiv.org/abs/2403.06534)

    SARDet-100K是第一个COCO级别的大规模多类别SAR物体检测数据集，为研究提供了大规模且多样化的数据集，揭示了SAR物体检测中预训练模型显著差异的关键挑战。

    

    面向合成孔径雷达（SAR）物体检测近来备受关注，因其不可替代的全天候成像能力。然而，这一研究领域面临着有限的公共数据集（主要包含 <2K 张图像，且仅包含单类别物体）和源代码不可访问的挑战。为解决这些问题，我们建立了一个新的基准数据集和一个针对大规模 SAR 物体检测的开源方法。我们的数据集 SARDet-100K 结果是对 10 个现有 SAR 检测数据集进行深入调研、收集和标准化的产物，为研究提供了一个大规模且多样化的数据集。据我们所知，SARDet-100K 是有史以来第一个达到 COCO 水平的大规模多类别 SAR 物体检测数据集。凭借这一高质量数据集，我们进行了全面实验，并揭示了 SAR 物体检测中一个关键挑战：预训练模型的显著差异。

    arXiv:2403.06534v1 Announce Type: cross  Abstract: Synthetic Aperture Radar (SAR) object detection has gained significant attention recently due to its irreplaceable all-weather imaging capabilities. However, this research field suffers from both limited public datasets (mostly comprising <2K images with only mono-category objects) and inaccessible source code. To tackle these challenges, we establish a new benchmark dataset and an open-source method for large-scale SAR object detection. Our dataset, SARDet-100K, is a result of intense surveying, collecting, and standardizing 10 existing SAR detection datasets, providing a large-scale and diverse dataset for research purposes. To the best of our knowledge, SARDet-100K is the first COCO-level large-scale multi-class SAR object detection dataset ever created. With this high-quality dataset, we conducted comprehensive experiments and uncovered a crucial challenge in SAR object detection: the substantial disparities between the pretraining
    
[^8]: ProMISe: 使用SAM进行可提示的医学图像分割

    ProMISe: Promptable Medical Image Segmentation using SAM

    [https://arxiv.org/abs/2403.04164](https://arxiv.org/abs/2403.04164)

    本文提出了一个自动提示模块（APM），为SAM基础模型提供了在目标领域中使用的自适应提示，显著提高了SAM在医学图像分割中的性能。

    

    随着提出了Segment Anything Model (SAM)的建议，对SAM进行医学图像分割(MIS)的微调变得流行起来。然而，由于SAM模型的规模较大，自然图像和医学图像之间存在显著的领域差距，基于微调的策略成本高，存在不稳定性、特征损伤和灾难性遗忘的潜在风险。此外，一些通过微调策略将SAM转移到特定领域MIS的方法禁用了模型的提示能力，严重限制了其使用场景。在本文中，我们提出了一个自动提示模块（APM），为SAM基础模型在目标域中提供了具有欧几里德自适应提示的基础。我们的实验证明，这样的自适应提示显著提高了SAM在MIS中非微调的性能。此外，我们提出了一种名为增量模式移位（IPS）的新型非侵入式方法，用于将SAM调整到特定医疗领域。

    arXiv:2403.04164v1 Announce Type: cross  Abstract: With the proposal of the Segment Anything Model (SAM), fine-tuning SAM for medical image segmentation (MIS) has become popular. However, due to the large size of the SAM model and the significant domain gap between natural and medical images, fine-tuning-based strategies are costly with potential risk of instability, feature damage and catastrophic forgetting. Furthermore, some methods of transferring SAM to a domain-specific MIS through fine-tuning strategies disable the model's prompting capability, severely limiting its utilization scenarios. In this paper, we propose an Auto-Prompting Module (APM), which provides SAM-based foundation model with Euclidean adaptive prompts in the target domain. Our experiments demonstrate that such adaptive prompts significantly improve SAM's non-fine-tuned performance in MIS. In addition, we propose a novel non-invasive method called Incremental Pattern Shifting (IPS) to adapt SAM to specific medica
    
[^9]: 否定否定：通过分布式反喜好优化实现对齐而无需人类正样本

    Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization

    [https://arxiv.org/abs/2403.03419](https://arxiv.org/abs/2403.03419)

    通过提出Distributional Dispreference Optimization (D$^2$O)方法，在不需要人类正样本的情况下实现了对齐，减少了有害信息的传播。

    

    大型语言模型（LLM）改变了人工智能的角色，但也可能存在传播不道德内容的潜在风险。对齐技术被引入以引导LLM朝着人类偏好方向发展，并受到越来越多的关注。尽管在这个方向上取得了显著突破，但现有方法严重依赖于高质量的正负训练对，受到嘈杂标签和首选和非首选响应数据之间的边缘区别的困扰。鉴于最近LLM在生成有用响应方面的高水平，本文将研究重点转向一个新的方向：仅使用人工注释的负样本来实现对齐，保留有用性的同时降低有害性。为此，我们提出了分布式反喜好优化（D$^2$O），通过最大化生成的响应与非首选响应之间的差异，有效地排除有害信息。我们在理论上证明

    arXiv:2403.03419v1 Announce Type: cross  Abstract: Large language models (LLMs) have revolutionized the role of AI, yet also pose potential risks of propagating unethical content. Alignment technologies have been introduced to steer LLMs towards human preference, gaining increasing attention. Despite notable breakthroughs in this direction, existing methods heavily rely on high-quality positive-negative training pairs, suffering from noisy labels and the marginal distinction between preferred and dispreferred response data. Given recent LLMs' proficiency in generating helpful responses, this work pivots towards a new research focus: achieving alignment using solely human-annotated negative samples, preserving helpfulness while reducing harmfulness. For this purpose, we propose Distributional Dispreference Optimization (D$^2$O), which maximizes the discrepancy between the generated responses and the dispreferred ones to effectively eschew harmful information. We theoretically demonstrat
    
[^10]: 因果关系：大型语言模型真正理解因果关系吗？

    Cause and Effect: Can Large Language Models Truly Understand Causality?

    [https://arxiv.org/abs/2402.18139](https://arxiv.org/abs/2402.18139)

    本研究提出了一种名为CARE CA的新型架构，通过结合显式因果检测模块和反事实陈述、以及隐含因果检测模块，旨在增强大型语言模型对因果关系的理解能力。

    

    随着大型语言模型（LLMs）的兴起，理解它们在解读和解释语言所涉及的复杂因果关系的能力和局限性变得至关重要。目前的方法使用明确或隐含的因果推理，然而迫切需要一种统一的方法，将两者结合起来更有效地处理各种因果关系。本研究提出了一种新颖的架构，称为具有反事实分析的上下文感知推理增强（CARE CA）框架，以增强因果推理和可解释性。所提出的框架将 ConceptNet 和反事实陈述中的明确因果检测模块以及通过LLMs进行的隐含因果检测相结合。我们的框架通过一层反事实解释进一步突出LLMs对因果关系的理解。ConceptNet 中的知识提高了多

    arXiv:2402.18139v1 Announce Type: cross  Abstract: With the rise of Large Language Models(LLMs), it has become crucial to understand their capabilities and limitations in deciphering and explaining the complex web of causal relationships that language entails. Current methods use either explicit or implicit causal reasoning, yet there is a strong need for a unified approach combining both to tackle a wide array of causal relationships more effectively. This research proposes a novel architecture called Context Aware Reasoning Enhancement with Counterfactual Analysis(CARE CA) framework to enhance causal reasoning and explainability. The proposed framework incorporates an explicit causal detection module with ConceptNet and counterfactual statements, as well as implicit causal detection through LLMs. Our framework goes one step further with a layer of counterfactual explanations to accentuate LLMs understanding of causality. The knowledge from ConceptNet enhances the performance of multi
    
[^11]: Chain-of-Discussion：复杂证据问题回答的多模型框架

    Chain-of-Discussion: A Multi-Model Framework for Complex Evidence-Based Question Answering

    [https://arxiv.org/abs/2402.16313](https://arxiv.org/abs/2402.16313)

    提出了一种Chain-of-Discussion框架，通过多个开源语言模型的协同作用，提高了复杂问题回答的质量

    

    开放式问题回答需要模型找到适当的证据来形成合理、全面和有帮助的答案。在实际应用中，模型还需要参与对与问题密切相关的潜在场景进行深入讨论。在检索模块的增强下，开源大型语言模型（LLMs）通常能够产生一致的答案，但在可靠证据选择和深入问题分析方面仍不够理想。本文提出了一种新颖的Chain-of-Discussion框架，旨在利用多个开源LLMs之间的协同作用，为开放式QA提供更正确、更全面的答案，尽管它们在个体上还不够强大。我们的实验证明，多个LLMs之间的讨论对提高答案质量起着至关重要的作用。我们在\url{https://github.com/kobaya}上发布了我们的数据和代码。

    arXiv:2402.16313v1 Announce Type: cross  Abstract: Open-ended question answering requires models to find appropriate evidence to form well-reasoned, comprehensive and helpful answers. In practical applications, models also need to engage in extended discussions on potential scenarios closely relevant to the question. With augmentation of retrieval module, open-source Large Language Models (LLMs) can produce coherent answers often with different focuses, but are still sub-optimal in terms of reliable evidence selection and in-depth question analysis. In this paper, we propose a novel Chain-of-Discussion framework to leverage the synergy among multiple open-source LLMs aiming to provide \textbf{more correct} and \textbf{more comprehensive} answers for open-ended QA, although they are not strong enough individually. Our experiments show that discussions among multiple LLMs play a vital role in enhancing the quality of answers. We release our data and code at \url{https://github.com/kobaya
    
[^12]: PANDA（Pedantic ANswer-correctness Determination and Adjudication）：改进问答和文本生成的自动评估

    PANDA (Pedantic ANswer-correctness Determination and Adjudication):Improving Automatic Evaluation for Question Answering and Text Generation

    [https://arxiv.org/abs/2402.11161](https://arxiv.org/abs/2402.11161)

    提出了PANDA方法，引入了更精确的答案正确性评测方式，解决了当前自动评估问答和文本生成过程中的挑战。

    

    问答（QA）只有在我们知道答案是否正确时才能取得进展，但对于许多最具挑战性和有趣的QA示例，当前的答案正确性（AC）指标与人类判断不一致，特别是来自大型语言模型（LLM）的冗长、自由格式答案。我们提出了两个挑战：缺乏数据和模型过大。基于LLM的评分器与人类更好地相关，但这项昂贵的任务仅在有限的QA数据集上进行了测试。我们通过提供清晰的指南来评估从人类QA比赛中采纳的机器QA，解决了这些问题。我们还引入了精确的答案正确性确定和裁决（Precise ANswer correctness Determination and Adjudication，PANDA），这是一个小巧、高效、确定性的AC分类器（812 KB），更准确地评估答案的正确性。

    arXiv:2402.11161v1 Announce Type: cross  Abstract: Question answering (QA) can only make progress if we know if an answer is correct, but for many of the most challenging and interesting QA examples, current answer correctness (AC) metrics do not align with human judgments, particularly verbose, free form answers from large language models (LLM). There are two challenges: a lack of data and that models are too big. LLM based scorers correlate better with humans, but this expensive task has only been tested on limited QA datasets. We rectify these issues by providing clear guidelines for evaluating machine QA adopted from human QA contests. We also introduce Precise ANswer correctness Determination and Adjudication (PANDA), a small, efficient, deterministic AC classifier (812 KB) that more accurately evaluates answer correctness.
    
[^13]: 通过优化编码器和辅助损失改进机器图像编码

    Improving Image Coding for Machines through Optimizing Encoder via Auxiliary Loss

    [https://arxiv.org/abs/2402.08267](https://arxiv.org/abs/2402.08267)

    通过应用辅助损失优化编码器，我们提出了一种改进的机器图像编码方法，能够在目标检测和语义分割任务中实现显著的速率提高。

    

    机器图像编码（ICM）旨在通过识别模型而不是人眼视觉来压缩图像以供机器分析。因此，在ICM中，编码器识别和压缩对于机器识别任务来说是至关重要的。学习型ICM有两种主要方法：基于任务损失的压缩模型优化和基于感兴趣区域（ROI）的比特分配。这些方法为编码器提供了识别能力。然而，当识别模型很深时，使用任务损失进行优化变得困难，而基于ROI的方法在评估过程中通常会增加额外开销。在本研究中，我们提出了一种用于学习型ICM模型的新训练方法，通过对编码器应用辅助损失来提高其识别能力和速率-失真性能。与传统训练方法相比，我们的方法在目标检测和语义分割任务中实现了27.7%和20.3%的Bjontegaard Delta速率改进。

    Image coding for machines (ICM) aims to compress images for machine analysis using recognition models rather than human vision. Hence, in ICM, it is important for the encoder to recognize and compress the information necessary for the machine recognition task. There are two main approaches in learned ICM; optimization of the compression model based on task loss, and Region of Interest (ROI) based bit allocation. These approaches provide the encoder with the recognition capability. However, optimization with task loss becomes difficult when the recognition model is deep, and ROI-based methods often involve extra overhead during evaluation. In this study, we propose a novel training method for learned ICM models that applies auxiliary loss to the encoder to improve its recognition capability and rate-distortion performance. Our method achieves Bjontegaard Delta rate improvements of 27.7% and 20.3% in object detection and semantic segmentation tasks, compared to the conventional training 
    
[^14]: 使用语言反馈模型来改进政策

    Policy Improvement using Language Feedback Models

    [https://arxiv.org/abs/2402.07876](https://arxiv.org/abs/2402.07876)

    本文介绍了一种使用语言反馈模型（LFMs）改进政策的方法，通过识别期望的行为并进行模仿学习，我们在任务完成率、泛化性能和人类可解释性方面取得了显著改进。

    

    我们引入了语言反馈模型（LFMs），用于在指令遵循中识别期望的行为-有助于实现指令中指定任务的行动-以进行模仿学习。为了训练LFMs，我们从大型语言模型（LLMs）获取对视觉轨迹进行语言描述的反馈。首先，通过使用LFMs识别期望模仿的行为，我们在三种不同的语言基础环境（Touchdown，ScienceWorld和ALFWorld）上，在任务完成率上改善了强行为克隆的基线方法。其次，与LLMs直接预测行动相比，使用LFMs在LLM输出标记的数量相同的情况下表现更好。第三，LFMs适应未见环境，通过一轮适应使任务完成率提高了3.5-12.0％。最后，可以修改LFM以提供人类可解释的反馈，无需性能损失，从而允许人类验证模仿学习的期望行为。

    We introduce Language Feedback Models (LFMs) that identify desirable behaviour - actions that help achieve tasks specified in the instruction - for imitation learning in instruction following. To train LFMs, we obtain feedback from Large Language Models (LLMs) on visual trajectories verbalized to language descriptions. First, by using LFMs to identify desirable behaviour to imitate, we improve in task-completion rate over strong behavioural cloning baselines on three distinct language grounding environments (Touchdown, ScienceWorld, and ALFWorld). Second, LFMs outperform using LLMs as experts to directly predict actions, when controlling for the number of LLM output tokens. Third, LFMs generalize to unseen environments, improving task-completion rate by 3.5-12.0% through one round of adaptation. Finally, LFM can be modified to provide human-interpretable feedback without performance loss, allowing human verification of desirable behaviour for imitation learning.
    
[^15]: 论语料库模拟辩论中的系统性偏差

    Systematic Biases in LLM Simulations of Debates

    [https://arxiv.org/abs/2402.04049](https://arxiv.org/abs/2402.04049)

    本研究揭示了LLMs在模拟政治辩论中存在的系统性偏差，尽管被指定从特定的政治观点进行辩论，LLMs代理机构倾向于遵循模型固有的社会偏见。通过自动自我优化方法，我们进一步证实了这些观察结果。

    

    最近自然语言处理的进展，特别是大型语言模型（LLMs）的出现，为构建能够准确复制人类行为的计算机模拟提供了令人兴奋的可能性。然而，LLMs是复杂的统计学习器，没有直接的演绎规则，使其容易出现意外行为。在本研究中，我们重点介绍了LLMs在模拟人类互动中的限制，特别关注LLMs在模拟政治辩论方面的能力。我们的发现表明，尽管被指定从特定的政治观点进行辩论，LLMs代理机构倾向于遵循模型固有的社会偏见。这种倾向导致出现行为模式，似乎偏离了人类之间已经确立的社会动态。我们使用自动自我优化方法加强了这些观察结果，该方法使我们能够操纵LLMs内部的偏见，并证明代理随后与这些调整保持一致。

    Recent advancements in natural language processing, especially the emergence of Large Language Models (LLMs), have opened exciting possibilities for constructing computational simulations designed to replicate human behavior accurately. However, LLMs are complex statistical learners without straightforward deductive rules, making them prone to unexpected behaviors. In this study, we highlight the limitations of LLMs in simulating human interactions, particularly focusing on LLMs' ability to simulate political debates. Our findings indicate a tendency for LLM agents to conform to the model's inherent social biases despite being directed to debate from certain political perspectives. This tendency results in behavioral patterns that seem to deviate from well-established social dynamics among humans. We reinforce these observations using an automatic self-fine-tuning method, which enables us to manipulate the biases within the LLM and demonstrate that agents subsequently align with the al
    
[^16]: 自主驾驶中角落案例检测的多模态增强物件学习器

    Multimodal-Enhanced Objectness Learner for Corner Case Detection in Autonomous Driving

    [https://arxiv.org/abs/2402.02026](https://arxiv.org/abs/2402.02026)

    本文提出了一种用于角落案例检测的多模态增强物件学习器（MENOL）。该方法通过减小已知类和未知类之间的差异，并引入多模态的数据，显著提高了对新类别的召回率，并降低了训练成本。

    

    在物体检测的先前工作中，封闭场景下的准确率较高，但在开放世界场景下的性能并不令人满意。其中一个具有挑战性的开放世界问题是自主驾驶中的角落案例检测。现有的检测器在这些案例中表现困难，过度依赖视觉外观，具有较差的泛化能力。本文提出了一种解决方案，通过减小已知类和未知类之间的差异，并引入多模态增强的物件学习器的概念。借助视觉中心和图像-文本两种形式，我们的半监督学习框架将物件学习器的知识传递给学生模型，实现了类别感知的检测。我们的方法——用于角落案例检测的多模态增强物件学习器（MENOL），显著提高了对新类别的召回率，并降低训练成本。在仅使用5100个标签训练图像的CODA-val数据集上，MENOL实现了76.6%的mAR-corner和79.8%的mAR-agnostic。

    Previous works on object detection have achieved high accuracy in closed-set scenarios, but their performance in open-world scenarios is not satisfactory. One of the challenging open-world problems is corner case detection in autonomous driving. Existing detectors struggle with these cases, relying heavily on visual appearance and exhibiting poor generalization ability. In this paper, we propose a solution by reducing the discrepancy between known and unknown classes and introduce a multimodal-enhanced objectness notion learner. Leveraging both vision-centric and image-text modalities, our semi-supervised learning framework imparts objectness knowledge to the student model, enabling class-aware detection. Our approach, Multimodal-Enhanced Objectness Learner (MENOL) for Corner Case Detection, significantly improves recall for novel classes with lower training costs. By achieving a 76.6% mAR-corner and 79.8% mAR-agnostic on the CODA-val dataset with just 5100 labeled training images, MEN
    
[^17]: 社会、法律、伦理、移情和文化规则：编制与推理（扩展版）

    Social, Legal, Ethical, Empathetic, and Cultural Rules: Compilation and Reasoning (Extended Version)

    [https://arxiv.org/abs/2312.09699](https://arxiv.org/abs/2312.09699)

    该研究介绍了SLEEC（社会、法律、伦理、移情、文化）规则的概念，旨在推动AI系统遵守人类背景相关规则的制定、验证和执行。

    

    AI基础和自主系统的崛起引发了人们对潜在负面影响的担忧，这些影响来自于它们的行为或决策。这些系统必须被设计为遵守它们将运作的人类背景。Townsend等人（2022）引入了SLEEC（社会、法律、伦理、移情或文化）规则的概念，旨在促进AI基础和自主系统应遵守规则的制定、验证和执行。他们提出了一种方法论来揭示这些规则，让哲学家、律师、领域专家和其他人用自然语言来制定这些规则。为了使这些规则在AI系统中有效使用，需要将这些规则系统地翻译成支持自动推理的形式语言。在这项研究中，我们首先对SLEEC规则模式进行了语言分析，这使得将SLEEC规则转换成c…

    arXiv:2312.09699v2 Announce Type: replace  Abstract: The rise of AI-based and autonomous systems is raising concerns and apprehension due to potential negative repercussions stemming from their behavior or decisions. These systems must be designed to comply with the human contexts in which they will operate. To this extent, Townsend et al. (2022) introduce the concept of SLEEC (social, legal, ethical, empathetic, or cultural) rules that aim to facilitate the formulation, verification, and enforcement of the rules AI-based and autonomous systems should obey. They lay out a methodology to elicit them and to let philosophers, lawyers, domain experts, and others to formulate them in natural language. To enable their effective use in AI systems, it is necessary to translate these rules systematically into a formal language that supports automated reasoning. In this study, we first conduct a linguistic analysis of the SLEEC rules pattern, which justifies the translation of SLEEC rules into c
    
[^18]: 基于能量的概念瓶颈模型：统一预测、概念干预和条件解释

    Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept Intervention, and Conditional Interpretations. (arXiv:2401.14142v1 [cs.CV])

    [http://arxiv.org/abs/2401.14142](http://arxiv.org/abs/2401.14142)

    基于能量的概念瓶颈模型统一了预测、概念干预和条件解释的功能，解决了现有方法在高阶非线性相互作用和复杂条件依赖关系上的限制。

    

    现有方法，如概念瓶颈模型 (CBM)，在为黑盒深度学习模型提供基于概念的解释方面取得了成功。它们通常通过在给定输入的情况下预测概念，然后在给定预测的概念的情况下预测最终的类别标签。然而，它们经常无法捕捉到概念之间的高阶非线性相互作用，例如纠正一个预测的概念（例如“黄色胸部”）无法帮助纠正高度相关的概念（例如“黄色腹部”），导致最终准确率不理想；它们无法自然地量化不同概念和类别标签之间的复杂条件依赖关系（例如对于一个带有类别标签“Kentucky Warbler”和概念“黑色嘴巴”的图像，模型能够正确预测另一个概念“黑色冠”的概率是多少），因此无法提供关于黑盒模型工作原理更深层次的洞察。针对这些限制，我们提出了基于能量的概念瓶颈模型（Energy-based Concept Bottleneck Models）。

    Existing methods, such as concept bottleneck models (CBMs), have been successful in providing concept-based interpretations for black-box deep learning models. They typically work by predicting concepts given the input and then predicting the final class label given the predicted concepts. However, (1) they often fail to capture the high-order, nonlinear interaction between concepts, e.g., correcting a predicted concept (e.g., "yellow breast") does not help correct highly correlated concepts (e.g., "yellow belly"), leading to suboptimal final accuracy; (2) they cannot naturally quantify the complex conditional dependencies between different concepts and class labels (e.g., for an image with the class label "Kentucky Warbler" and a concept "black bill", what is the probability that the model correctly predicts another concept "black crown"), therefore failing to provide deeper insight into how a black-box model works. In response to these limitations, we propose Energy-based Concept Bot
    
[^19]: TAT-LLM: 一种针对表格和文本数据的专用语言模型用于离散推理

    TAT-LLM: A Specialized Language Model for Discrete Reasoning over Tabular and Textual Data. (arXiv:2401.13223v1 [cs.CL])

    [http://arxiv.org/abs/2401.13223](http://arxiv.org/abs/2401.13223)

    TAT-LLM是一种专门用于离散推理的语言模型，针对混合表格和文本数据上的问答任务。该模型通过分步流水线的方式，包括提取器、推理器和执行器，利用LLMs的强大能力来解决问题。而为了应对成本、延迟和数据安全风险等挑战，我们开发了TAT-LLM，一个专门针对此任务的较小LLM。

    

    在这项工作中，我们解决了在混合表格和文本数据上进行问答的问题，这在Web上非常常见（如SEC文件），通常需要离散推理能力。最近，像GPT-4这样的大型语言模型展示了强大的多步骤推理能力。我们考虑利用LLMs的强大能力来解决我们的任务。我们提出了面向表格和文本问答的分步流水线的抽象，包括提取器、推理器和执行器三个关键步骤，并首先设计了一份指令来实例化该流水线并验证GPT-4优于所有现有方法。然而，利用像GPT-4这样的在线LLM存在成本、延迟和数据安全风险等各种挑战，这促使我们专门针对此任务开发较小的LLM。我们通过对现有专家标注数据集自动生成的训练数据对LLaMA 2进行微调，开发了TAT-LLM语言模型。

    In this work, we address question answering (QA) over a hybrid of tabular and textual data that are very common content on the Web (e.g. SEC filings), where discrete reasoning capabilities are often required. Recently, large language models (LLMs) like GPT-4 have demonstrated strong multi-step reasoning capabilities. We then consider harnessing the amazing power of LLMs to solve our task. We abstract a Step-wise Pipeline for tabular and textual QA, which consists of three key steps, including Extractor, Reasoner and Executor, and initially design an instruction to instantiate the pipeline and validate that GPT-4 outperforms all existing methods. However, utilizing an online LLM like GPT-4 holds various challenges in terms of cost, latency, and data security risk, which motivates us to specialize smaller LLMs in this task. We develop a TAT-LLM language model by fine-tuning LLaMA 2 with the training data generated automatically from existing expert-annotated datasets following the Step-w
    
[^20]: 高斯自适应注意力是唯一所需的：跨多个模态的健壮上下文表示

    Gaussian Adaptive Attention is All You Need: Robust Contextual Representations Across Multiple Modalities. (arXiv:2401.11143v1 [cs.LG])

    [http://arxiv.org/abs/2401.11143](http://arxiv.org/abs/2401.11143)

    该论文提出了一个名为GAAM的多头高斯自适应注意力机制，用于增强跨多个模态的信息聚合。通过将可学习的均值和方差纳入注意力机制中，GAAM能够动态地重新调整特征的重要性，从而在处理非平稳数据时取得了显著的性能提升，超过了目前现有的注意力技术。该方法的适应性强且参数数量较少，具有改进现有注意力框架的潜力。

    

    我们提出了多头高斯自适应注意力机制（GAAM），一种新颖的概率注意力框架，并设计了高斯自适应变压器（GAT），旨在增强跨多个模态（包括语音、文本和视觉）的信息聚合。GAAM将可学习的均值和方差融入其注意力机制中，采用多头框架实现，使其能够集体建模任何概率分布，以动态重新调整特征重要性。该方法在处理高度非平稳数据时表现出显著改进，通过识别特征空间中的关键元素，超越了现有的注意力技术在模型性能上的状态（精度增加约20%）。GAAM与基于点积的注意力模型兼容，并具有相对较低的参数数量，展示了其适应性和提升现有注意力框架的潜力。在实证方面，GAAM表现出卓越的适应性和功效。

    We propose the Multi-Head Gaussian Adaptive Attention Mechanism (GAAM), a novel probabilistic attention framework, and the Gaussian Adaptive Transformer (GAT), designed to enhance information aggregation across multiple modalities, including Speech, Text and Vision. GAAM integrates learnable mean and variance into its attention mechanism, implemented in a Multi-Headed framework enabling it to collectively model any Probability Distribution for dynamic recalibration of feature significance. This method demonstrates significant improvements, especially with highly non-stationary data, surpassing the state-of-the-art attention techniques in model performance (up to approximately +20% in accuracy) by identifying key elements within the feature space. GAAM's compatibility with dot-product-based attention models and relatively low number of parameters showcases its adaptability and potential to boost existing attention frameworks. Empirically, GAAM exhibits superior adaptability and efficacy
    
[^21]: 走向异质图学习：进展与未来

    Towards Learning from Graphs with Heterophily: Progress and Future. (arXiv:2401.09769v1 [cs.SI])

    [http://arxiv.org/abs/2401.09769](http://arxiv.org/abs/2401.09769)

    本调查综合概述了关于从带有异质性的图中学习的现有研究，并根据学习策略、模型架构和实际应用等方面对方法进行了分类。同时讨论了现有研究的主要挑战，并提出了未来研究的潜在方向。

    

    图是用来模拟现实世界实体之间复杂关系的结构化数据。最近，异质图，其中连接的节点往往具有不同的标签或不同的特征，引起了广泛关注并找到了许多应用。与此同时，人们也在不断努力推进从异质图中学习。虽然有关该主题的调查存在，但它们只关注于异质图神经网络（GNNs），而忽略了异质图学习的其他子主题。在本调查中，我们全面回顾了关于从带有异质性的图中学习的现有研究。首先，我们收集了180多篇论文，介绍了该领域的发展。然后，我们根据层次分类法对现有方法进行了系统分类，包括学习策略、模型架构和实际应用。最后，我们讨论了现有研究的主要挑战，并突出了未来研究的潜在方向。

    Graphs are structured data that models complex relations between real-world entities. Heterophilous graphs, where linked nodes are prone to be with different labels or dissimilar features, have recently attracted significant attention and found many applications. Meanwhile, increasing efforts have been made to advance learning from heterophilous graphs. Although there exist surveys on the relevant topic, they focus on heterophilous GNNs, which are only sub-topics of heterophilous graph learning. In this survey, we comprehensively overview existing works on learning from graphs with heterophily.First, we collect over 180 publications and introduce the development of this field. Then, we systematically categorize existing methods based on a hierarchical taxonomy including learning strategies, model architectures and practical applications. Finally, we discuss the primary challenges of existing studies and highlight promising avenues for future research.More publication details and corres
    
[^22]: 欺骗的艺术：使用动态触发器的强健后门攻击

    The Art of Deception: Robust Backdoor Attack using Dynamic Stacking of Triggers. (arXiv:2401.01537v1 [cs.CR])

    [http://arxiv.org/abs/2401.01537](http://arxiv.org/abs/2401.01537)

    这项研究介绍了一种使用动态触发器进行强健后门攻击的方法，通过巧妙设计的调整，使损坏的样本与干净的样本无法区分，实验证明这种方法可以成功地欺骗语音识别系统。

    

    由于人工智能行业的最新进展，机器学习作为服务（MLaaS）领域正在经历增长的实施。然而，这种增长引发了对AI防御机制的担忧，特别是对于来自不完全可信的第三方提供商的潜在隐蔽攻击。最近的研究发现，听觉后门可能使用某些修改作为其启动机制。DynamicTrigger作为一种方法被引入，用于进行使用巧妙设计的调整来确保损坏的样本与干净的样本无法区分的动态后门攻击。通过利用波动的信号采样率，并通过动态声音触发器（比如拍手声）对说话者身份进行掩盖，可以欺骗语音识别系统（ASR）。我们的实证测试表明，DynamicTrigger在隐蔽攻击中既有效又隐蔽，并在攻击过程中取得了令人印象深刻的成功率。

    The area of Machine Learning as a Service (MLaaS) is experiencing increased implementation due to recent advancements in the AI (Artificial Intelligence) industry. However, this spike has prompted concerns regarding AI defense mechanisms, specifically regarding potential covert attacks from third-party providers that cannot be entirely trusted. Recent research has uncovered that auditory backdoors may use certain modifications as their initiating mechanism. DynamicTrigger is introduced as a methodology for carrying out dynamic backdoor attacks that use cleverly designed tweaks to ensure that corrupted samples are indistinguishable from clean. By utilizing fluctuating signal sampling rates and masking speaker identities through dynamic sound triggers (such as the clapping of hands), it is possible to deceive speech recognition systems (ASR). Our empirical testing demonstrates that DynamicTrigger is both potent and stealthy, achieving impressive success rates during covert attacks while 
    
[^23]: 知识增强的医疗时间序列条件插补方法

    Knowledge Enhanced Conditional Imputation for Healthcare Time-series. (arXiv:2312.16713v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.16713](http://arxiv.org/abs/2312.16713)

    本研究提出了一种知识增强的条件插补方法，针对医疗时间序列数据中的缺失数据问题。通过整合先进的知识嵌入和非均匀掩蔽策略，该方法能够灵活适应不同模式的电子健康记录中的缺失数据分布不平衡问题。

    

    本研究提出了一种新颖的方法来解决多变量时间序列中的缺失数据问题，特别关注医疗数据的复杂性。我们的条件自注意力插补（CSAI）模型以基于Transformer的框架为基础，引入了一种针对医疗时间序列数据细节的条件隐藏状态初始化方式。该方法与传统的插补技术不同，它特别针对医疗数据集中缺失数据分布的不平衡问题，这一关键问题常常被忽视。通过整合先进的知识嵌入和非均匀掩蔽策略，CSAI能够灵活适应电子健康记录（EHR）中缺失数据的不同模式。

    This study presents a novel approach to addressing the challenge of missing data in multivariate time series, with a particular focus on the complexities of healthcare data. Our Conditional Self-Attention Imputation (CSAI) model, grounded in a transformer-based framework, introduces a conditional hidden state initialization tailored to the intricacies of medical time series data. This methodology diverges from traditional imputation techniques by specifically targeting the imbalance in missing data distribution, a crucial aspect often overlooked in healthcare datasets. By integrating advanced knowledge embedding and a non-uniform masking strategy, CSAI adeptly adjusts to the distinct patterns of missing data in Electronic Health Records (EHRs).
    
[^24]: 通过对抗行为抑制Q学习中的过高估计

    Suppressing Overestimation in Q-Learning through Adversarial Behaviors. (arXiv:2310.06286v1 [cs.LG])

    [http://arxiv.org/abs/2310.06286](http://arxiv.org/abs/2310.06286)

    本文提出了一种新的Q学习算法，通过引入虚拟对抗性玩家，有效调节了标准Q学习中的过高估计偏差，提出的算法简单而有效，能够轻松应用于强化学习算法并提高性能。

    

    本文旨在提出一种新的Q学习算法，使用一个虚拟对抗性玩家，称为虚拟对抗性Q学习（DAQ），以有效地调节标准Q学习中的过高估计偏差。通过虚拟玩家，学习可以被表述为一个双人零和博弈。所提出的DAQ将几种Q学习的变体统一到一个单一的框架中，以控制过高估计偏差，例如maxmin Q学习和minmax Q学习（本文提出）。通过虚拟对抗性行为，所提出的DAQ是一种简单而有效的方式，可以轻松应用于现成的强化学习算法，以提高性能。通过调整对抗性Q学习，从综合的角度分析了DAQ的有限时间收敛性。在各种基准环境下，实证验证了所提出DAQ的性能。

    The goal of this paper is to propose a new Q-learning algorithm with a dummy adversarial player, which is called dummy adversarial Q-learning (DAQ), that can effectively regulate the overestimation bias in standard Q-learning. With the dummy player, the learning can be formulated as a two-player zero-sum game. The proposed DAQ unifies several Q-learning variations to control overestimation biases, such as maxmin Q-learning and minmax Q-learning (proposed in this paper) in a single framework. The proposed DAQ is a simple but effective way to suppress the overestimation bias thourgh dummy adversarial behaviors and can be easily applied to off-the-shelf reinforcement learning algorithms to improve the performances. A finite-time convergence of DAQ is analyzed from an integrated perspective by adapting an adversarial Q-learning. The performance of the suggested DAQ is empirically demonstrated under various benchmark environments.
    
[^25]: STAMP：通过Stein变分梯度下降实现可微的任务和运动规划

    STAMP: Differentiable Task and Motion Planning via Stein Variational Gradient Descent. (arXiv:2310.01775v1 [cs.RO])

    [http://arxiv.org/abs/2310.01775](http://arxiv.org/abs/2310.01775)

    STAMP是一种基于Stein变分梯度下降的算法，通过并行化和可微仿真高效地搜索多个多样化的任务和运动规划解决方案。

    

    许多操作任务，如使用工具或装配零件，往往需要符号和几何推理。任务和运动规划（TAMP）算法通常通过对高级任务序列进行树搜索并检查运动学和动力学可行性来解决这些问题。虽然性能良好，但大多数现有算法的效率非常低，因为其时间复杂性随可能动作和物体数量的增加呈指数增长。此外，它们只能找到单个解决方案，而可能存在许多可行的计划。为了解决这些限制，我们提出了一种名为Stein任务和运动规划（STAMP）的新算法，它利用并行化和可微仿真来高效地搜索多个多样化的计划。STAMP将离散和连续的TAMP问题转化为可以使用变分推断解决的连续优化问题。我们的算法基于Stein变分梯度下降，一种概率推断方法。

    Planning for many manipulation tasks, such as using tools or assembling parts, often requires both symbolic and geometric reasoning. Task and Motion Planning (TAMP) algorithms typically solve these problems by conducting a tree search over high-level task sequences while checking for kinematic and dynamic feasibility. While performant, most existing algorithms are highly inefficient as their time complexity grows exponentially with the number of possible actions and objects. Additionally, they only find a single solution to problems in which many feasible plans may exist. To address these limitations, we propose a novel algorithm called Stein Task and Motion Planning (STAMP) that leverages parallelization and differentiable simulation to efficiently search for multiple diverse plans. STAMP relaxes discrete-and-continuous TAMP problems into continuous optimization problems that can be solved using variational inference. Our algorithm builds upon Stein Variational Gradient Descent, a gra
    
[^26]: CyberForce: 一个用于恶意软件缓解的联邦强化学习框架

    CyberForce: A Federated Reinforcement Learning Framework for Malware Mitigation. (arXiv:2308.05978v1 [cs.CR])

    [http://arxiv.org/abs/2308.05978](http://arxiv.org/abs/2308.05978)

    CyberForce是一个联邦强化学习框架，用于在物联网设备中协同私密地确定适合缓解各种零日攻击的MTD技术。它整合了设备指纹识别和异常检测，并通过奖励或惩罚FRL agent选择的MTD机制来提高网络安全性。

    

    互联网物联网(IoT)范例的扩展是不可避免的，但是对于IoT设备对恶意软件事件的脆弱性已成为一个越来越关注的问题。最近的研究显示，将强化学习与移动目标防御(MTD)机制相结合，可以增强IoT设备的网络安全性。然而，大量的新恶意软件攻击和代理人学习和选择有效的MTD技术所需的时间使得这种方法在现实世界的IoT场景中不切实际。为解决这个问题，本研究提出了CyberForce，一个采用联邦强化学习(FRL)的框架，用于集体且保密地确定适合缓解各种零日攻击的MTD技术。CyberForce结合了设备指纹识别和异常检测，通过奖励或惩罚FRL agent选择的MTD机制。该框架在一个由十台真实IoT平台设备组成的联邦中进行了评估。通过六个恶意软件样本进行了一系列实验。

    The expansion of the Internet-of-Things (IoT) paradigm is inevitable, but vulnerabilities of IoT devices to malware incidents have become an increasing concern. Recent research has shown that the integration of Reinforcement Learning with Moving Target Defense (MTD) mechanisms can enhance cybersecurity in IoT devices. Nevertheless, the numerous new malware attacks and the time that agents take to learn and select effective MTD techniques make this approach impractical for real-world IoT scenarios. To tackle this issue, this work presents CyberForce, a framework that employs Federated Reinforcement Learning (FRL) to collectively and privately determine suitable MTD techniques for mitigating diverse zero-day attacks. CyberForce integrates device fingerprinting and anomaly detection to reward or penalize MTD mechanisms chosen by an FRL-based agent. The framework has been evaluated in a federation consisting of ten devices of a real IoT platform. A pool of experiments with six malware samp
    
[^27]: 可解释的推理方法用于刻板印象识别

    Interpretable Stereotype Identification through Reasoning. (arXiv:2308.00071v1 [cs.CL])

    [http://arxiv.org/abs/2308.00071](http://arxiv.org/abs/2308.00071)

    本研究通过使用推理方法，在零射击刻板印象识别中取得了重要的进展，并发现推理的性能增益远远超过模型规模扩展的增益。推理不仅提高了准确性，还提高了决策的可解释性。

    

    鉴于语言模型训练使用了包含固有偏见的大量数据集，可能会不经意地持续系统性歧视，因此，审查和解决语言模型中的偏见变得至关重要，将公平性整合到它们的发展中，以确保这些模型具有公正和无偏的特性。在这项工作中，我们展示了基于Vicuna-13B-v1.3的零射击刻板印象识别中推理的重要性。尽管我们观察到从13B到33B的规模扩展会提高准确性，但我们表明推理的性能增益远远超过规模扩展的增益。我们的研究结果表明，推理可能是使LLMs在刻板印象等领域任务上超越规模定律的关键因素。此外，通过对选定的推理追踪进行定性分析，我们突出显示了推理不仅提高了准确性，还提高了决策的可解释性。

    Given that language models are trained on vast datasets that may contain inherent biases, there is a potential danger of inadvertently perpetuating systemic discrimination. Consequently, it becomes essential to examine and address biases in language models, integrating fairness into their development to ensure these models are equitable and free from bias. In this work, we demonstrate the importance of reasoning in zero-shot stereotype identification based on Vicuna-13B-v1.3. While we do observe improved accuracy by scaling from 13B to 33B, we show that the performance gain from reasoning significantly exceeds the gain from scaling up. Our findings suggest that reasoning could be a key factor that enables LLMs to trescend the scaling law on out-of-domain tasks such as stereotype identification. Additionally, through a qualitative analysis of select reasoning traces, we highlight how reasoning enhances not just accuracy but also the interpretability of the decision.
    
[^28]: RL+模型控制：使用按需最优控制学习多功能足式 locomotion

    RL + Model-based Control: Using On-demand Optimal Control to Learn Versatile Legged Locomotion. (arXiv:2305.17842v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2305.17842](http://arxiv.org/abs/2305.17842)

    本文提出了一种 RL+模型控制框架以开发出可以有效可靠地学习的健壮控制策略，通过整合有限时间最优控制生成的按需参考运动分散 RL 过程，同时克服了建模简化的固有局限性，在足式 locomotion 上实现了多功能和强健，能泛化参考运动并处理更复杂的运动任务。

    

    本文提出了一种控制框架，将基于模型的最优控制和强化学习（RL）相结合，实现了多功能和强健的足式 locomotion。我们的方法通过整合有限时间最优控制生成的按需参考运动来增强 RL 训练过程，覆盖了广泛的速度和步态。这些参考运动作为 RL 策略模仿的目标，导致开发出可有效可靠地学习的健壮控制策略。此外，通过考虑全身动力学，RL 克服了建模简化的固有局限性。通过仿真和硬件实验，我们展示了 RL 训练过程在我们的框架内的强健性和可控性。此外，我们的方法展示了泛化参考运动和处理可能对简化模型构成挑战的更复杂的运动任务的能力，利用了 RL 的灵活性。

    This letter presents a control framework that combines model-based optimal control and reinforcement learning (RL) to achieve versatile and robust legged locomotion. Our approach enhances the RL training process by incorporating on-demand reference motions generated through finite-horizon optimal control, covering a broad range of velocities and gaits. These reference motions serve as targets for the RL policy to imitate, resulting in the development of robust control policies that can be learned efficiently and reliably. Moreover, by considering whole-body dynamics, RL overcomes the inherent limitations of modelling simplifications. Through simulation and hardware experiments, we demonstrate the robustness and controllability of the RL training process within our framework. Furthermore, our method demonstrates the ability to generalize reference motions and handle more complex locomotion tasks that may pose challenges for the simplified model, leveraging the flexibility of RL.
    
[^29]: 生物医学自然语言处理中的大型语言模型: 基准、基线和建议

    Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations. (arXiv:2305.16326v1 [cs.CL])

    [http://arxiv.org/abs/2305.16326](http://arxiv.org/abs/2305.16326)

    本文研究了GPT-3和GPT-4在生物医学自然语言处理中的表现，分析了它们可能产生的错误类型，并提供了使用这些模型的建议。

    

    生物医学文献呈指数级增长，手动筛选和提取知识变得困难。自动从生物医学文献中提取信息的生物医学自然语言处理（BioNLP）技术有助于减轻这种负担。近年来，如GPT-3和GPT-4等大型语言模型（LLMs）因其卓越的性能而受到重视。但是，它们在BioNLP任务中的有效性以及对方法开发和下游用户的影响仍未得到研究。本研究（1）在四个应用程序中在八个BioNLP数据集中建立了GPT-3和GPT-4在零-shot和一-shot设置下的基准表现，包括命名实体识别，关系提取，多标签文档分类和语义相似性和推理；（2）审查了LLMs产生的错误，并将错误分为三种类型：缺失，不一致和不需要的人工内容；（3）提出了使用LLMs的建议。

    Biomedical literature is growing rapidly, making it challenging to curate and extract knowledge manually. Biomedical natural language processing (BioNLP) techniques that can automatically extract information from biomedical literature help alleviate this burden. Recently, large Language Models (LLMs), such as GPT-3 and GPT-4, have gained significant attention for their impressive performance. However, their effectiveness in BioNLP tasks and impact on method development and downstream users remain understudied. This pilot study (1) establishes the baseline performance of GPT-3 and GPT-4 at both zero-shot and one-shot settings in eight BioNLP datasets across four applications: named entity recognition, relation extraction, multi-label document classification, and semantic similarity and reasoning, (2) examines the errors produced by the LLMs and categorized the errors into three types: missingness, inconsistencies, and unwanted artificial content, and (3) provides suggestions for using L
    
[^30]: RLocator: 利用强化学习进行Bug定位

    RLocator: Reinforcement Learning for Bug Localization. (arXiv:2305.05586v1 [cs.SE])

    [http://arxiv.org/abs/2305.05586](http://arxiv.org/abs/2305.05586)

    本文提出了一种基于强化学习的Bug定位方法RLocator，相较于其他最先进的Bug定位技术具有更优越的性能。

    

    软件开发者在他们的项目中花费了大量的时间来修复Bugs。为了简化这个过程，提出了Bug定位方法来确定哪些源代码文件可能是负责特定Bug的源头。之前的工作提出了几种基于相似性的机器学习技术，用于Bug定位。尽管这些技术取得了显著进展，但它们并没有直接优化评估指标。相反，在训练和测试阶段使用了不同的度量标准，这会对检索任务的模型性能产生负面影响。在本文中，我们提出了一种基于强化学习的Bug定位方法RLocator。我们使用马尔可夫决策过程（MDP）来优化评估指标，从而对Bug定位问题进行公式化。我们提出了该技术，并基于六种高度流行的Apache项目的8,316个Bug报告的基准数据集进行了实验评估。我们的评估表明，RLocator相较于其他最先进的Bug定位技术具有更优越的性能。

    Software developers spend a significant portion of time fixing bugs in their projects. To streamline this process, bug localization approaches have been proposed to identify the source code files that are likely responsible for a particular bug. Prior work proposed several similarity-based machine-learning techniques for bug localization. Despite significant advances in these techniques, they do not directly optimize the evaluation measures. Instead, they use different metrics in the training and testing phases, which can negatively impact the model performance in retrieval tasks. In this paper, we propose RLocator, a Reinforcement Learning-based (RL) bug localization approach. We formulate the bug localization problem using a Markov Decision Process (MDP) to optimize the evaluation measures directly. We present the technique and experimentally evaluate it based on a benchmark dataset of 8,316 bug reports from six highly popular Apache projects. Our evaluation shows that RLocator achie
    
[^31]: 通用状态和动作空间上的策略优化

    Policy Optimization over General State and Action Spaces. (arXiv:2211.16715v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.16715](http://arxiv.org/abs/2211.16715)

    本文提出了一种新方法并引入了函数近似来解决通用状态和动作空间上的强化学习问题，同时介绍了一种新的策略双平均法。

    

    通用状态和动作空间上的强化学习问题异常困难。本文提出了一种新方法，并引入了函数近似来解决这个问题。同时，还提出了一种新的策略双平均法。这些方法都可以应用于不同类型的RL问题。

    Reinforcement learning (RL) problems over general state and action spaces are notoriously challenging. In contrast to the tableau setting, one can not enumerate all the states and then iteratively update the policies for each state. This prevents the application of many well-studied RL methods especially those with provable convergence guarantees. In this paper, we first present a substantial generalization of the recently developed policy mirror descent method to deal with general state and action spaces. We introduce new approaches to incorporate function approximation into this method, so that we do not need to use explicit policy parameterization at all. Moreover, we present a novel policy dual averaging method for which possibly simpler function approximation techniques can be applied. We establish linear convergence rate to global optimality or sublinear convergence to stationarity for these methods applied to solve different classes of RL problems under exact policy evaluation. 
    
[^32]: 使用动态扩张聚合在深度残差GCN中进行重叠社区检测

    Overlapping Community Detection using Dynamic Dilated Aggregation in Deep Residual GCN. (arXiv:2210.11174v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2210.11174](http://arxiv.org/abs/2210.11174)

    本论文提出了一种使用动态扩张聚合的深度残差GCN方法进行重叠社区检测。通过设计深度动态残差图卷积网络和统一的编码器-解码器框架，实现了在不规则图上进行社区检测。实验结果表明，该方法在不同数据集上取得了较好的效果。

    

    重叠社区检测是图挖掘中的一个关键问题。一些研究考虑将图卷积网络（GCN）应用于解决该问题。然而，在一般不规则图的情况下，如何将深度图卷积网络结合起来仍然具有挑战性。在本研究中，我们基于我们的新颖动态扩张聚合机制和统一的端到端编码器-解码器框架，设计了一个深度动态残差图卷积网络（DynaResGCN），用于检测网络中的重叠社区。深度的DynaResGCN模型被用作编码器，而我们将伯努利-泊松（BP）模型作为解码器。因此，我们将我们的重叠社区检测框架应用在一个没有基准值的研究主题数据集，一个拥有可靠（手工标记）基准值的Facebook网络集合，以及一组具有经验性（非手工标记）基准值的非常大的合著网络。

    Overlapping community detection is a key problem in graph mining. Some research has considered applying graph convolutional networks (GCN) to tackle the problem. However, it is still challenging to incorporate deep graph convolutional networks in the case of general irregular graphs. In this study, we design a deep dynamic residual graph convolutional network (DynaResGCN) based on our novel dynamic dilated aggregation mechanisms and a unified end-to-end encoder-decoder-based framework to detect overlapping communities in networks. The deep DynaResGCN model is used as the encoder, whereas we incorporate the Bernoulli-Poisson (BP) model as the decoder. Consequently, we apply our overlapping community detection framework in a research topics dataset without having ground truth, a set of networks from Facebook having a reliable (hand-labeled) ground truth, and in a set of very large co-authorship networks having empirical (not hand-labeled) ground truth. Our experimentation on these datase
    

