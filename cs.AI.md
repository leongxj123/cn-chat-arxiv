# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [A Survey on Self-Supervised Learning for Non-Sequential Tabular Data](https://rss.arxiv.org/abs/2402.01204) | 本调研总结了自监督学习在非连续表格数据中的最新进展和挑战，将其方法分为预测性学习、对比学习和混合学习，讨论了应用问题，包括自动化数据工程、跨表格查询和隐私保护等。 |
| [^2] | [Deep Reinforcement Learning for Traveling Purchaser Problems](https://arxiv.org/abs/2404.02476) | 提出了一种基于深度强化学习的方法，该方法分别解决了旅行购买者问题中的路由构建和购买规划问题，并从全局角度评估和优化解决方案。 |
| [^3] | [LeTac-MPC: Learning Model Predictive Control for Tactile-reactive Grasping](https://arxiv.org/abs/2403.04934) | LeTac-MPC是一种学习模型预测控制，利用视觉触觉传感器GelSight和不同iable MPC层，实现在不同条件下和具有不同物理属性的物体上进行稳健抓取控制。 |
| [^4] | [Context-Based Multimodal Fusion](https://arxiv.org/abs/2403.04650) | 提出一种基于上下文的多模态融合模型，结合了模态融合和数据分布对齐，通过特定上下文向量表示每个模态，并将其与每个模态的嵌入进行融合， |
| [^5] | [MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained Representations](https://arxiv.org/abs/2402.10093) | MIM-Refiner是一种对比学习提升方法，通过利用MIM模型中的中间层表示和多个对比头，能够将MIM模型的特征从次优的状态提升到最先进的状态，并在ImageNet-1K数据集上取得了新的最先进结果。 |
| [^6] | [Explaining Learned Reward Functions with Counterfactual Trajectories](https://arxiv.org/abs/2402.04856) | 通过对比原始轨迹和反事实部分轨迹的奖励，我们提出了一种解释强化学习中奖励函数的方法。通过生成符合质量标准的反事实轨迹解释（CTEs），我们的实验表明，CTEs对于代理人模型具有明显的信息性，能提高其预测与奖励函数的一致性。 |
| [^7] | [MixedNUTS: Training-Free Accuracy-Robustness Balance via Nonlinearly Mixed Classifiers](https://arxiv.org/abs/2402.02263) | MixedNUTS是一种无需训练的方法，通过非线性混合分类器的转换和概率混合来实现准确性和鲁棒性的平衡。 |
| [^8] | [CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark](https://arxiv.org/abs/2401.11944) | CMMMU是一个旨在评估大型多模型模型在大学级学科知识和深思熟虑推理任务中表现的中文大规模多学科多模态理解基准，为填补在非英语环境中评估先进知识和推理能力的空白而设计。 |
| [^9] | [Resolving Ethics Trade-offs in Implementing Responsible AI](https://arxiv.org/abs/2401.08103) | 该论文提出了通过权衡处理人工智能伦理中的紧张关系的五种方法，并提出了一个框架来实施全面的人工智能/机器学习系统。 |
| [^10] | [Extrapolatable Transformer Pre-training for Ultra Long Time-Series Forecasting](https://arxiv.org/abs/2312.00817) | 提出了一种名为TimelyGPT的可推广的Transformer预训练模型，该模型通过可推广的位置嵌入和循环注意力以及时间卷积模块有效地捕捉超长时间序列数据中的全局和局部时间依赖关系。 |
| [^11] | [Online Advertisements with LLMs: Opportunities and Challenges](https://arxiv.org/abs/2311.07601) | 本文探讨了在线广告系统中利用大型语言模型（LLM）的潜力，提出了一个LLM广告的通用框架，并讨论了基于LLM的动态创意优化的前景。 |
| [^12] | [Local Universal Explainer (LUX) -- a rule-based explainer with factual, counterfactual and visual explanations](https://arxiv.org/abs/2310.14894) | LUX是一种基于规则的解释器，可以生成事实、反事实和视觉解释，通过选择高密度簇形式的局部概念来形成决策边界。 |
| [^13] | [Rethinking the BERT-like Pretraining for DNA Sequences.](http://arxiv.org/abs/2310.07644) | 重新考虑了基于DNA序列的BERT-like预训练方法，通过使用K-mer重叠标记化，在下游任务的微调阶段和预训练过程中都取得了一致的性能改善。 |
| [^14] | [Learning Generalizable Tool-use Skills through Trajectory Generation.](http://arxiv.org/abs/2310.00156) | 通过轨迹生成，我们提出了一种学习通用工具使用技能的方法，可以适应不同形状的工具，从而使自主系统能够处理复杂的可变形物体操作任务。 |
| [^15] | [DEFormer: DCT-driven Enhancement Transformer for Low-light Image and Dark Vision.](http://arxiv.org/abs/2309.06941) | 该论文提出了一种新的DCT驱动增强Transformer（DEFormer），可以在低光图像中恢复丢失的细节，通过引入频率作为新的线索，通过可学习的频率分支（LFB）和基于曲率的频率增强（CFE）来实现。此外，还提出了交叉域融合（CDF）来减少领域之间的差异，DEFormer还可以作为暗部检测的预处理，有效提高了性能。 |
| [^16] | [How adversarial attacks can disrupt seemingly stable accurate classifiers.](http://arxiv.org/abs/2309.03665) | 本文研究了对抗性攻击如何通过微小修改干扰准确的分类器，并发现这可能是高维输入数据下分类器的基本特征。作者提出了一个通用的框架，解释了实际系统中观察到的关键行为，包括模型对对抗性攻击的容易受到影响，同时对随机扰动具有鲁棒性。验证实验还表明了相同现象在实际神经网络中的存在。 |
| [^17] | [PointLLM: Empowering Large Language Models to Understand Point Clouds.](http://arxiv.org/abs/2308.16911) | PointLLM是一种使大型语言模型理解点云的方法，它利用点云编码器和强大的LLM将几何、外观和语言信息融合，并通过人类指导生成环境上恰当的响应。该方法通过收集大规模的点-文本指令对数据集进行两阶段的训练，以提高模型的感知能力和泛化能力。 |
| [^18] | [Partially Observable Mean Field Multi-Agent Reinforcement Learning Based on Graph-Attention.](http://arxiv.org/abs/2304.12653) | 本文提出了一种新的基于图注意力的部分可观察均场多智能体强化学习算法，使用图注意力来捕获周围邻居智能体的特征信息，可以提高大规模多智能体环境中部分可观察MARL的性能。 |
| [^19] | [Launching a Robust Backdoor Attack under Capability Constrained Scenarios.](http://arxiv.org/abs/2304.10985) | 深度神经网络的后门攻击一直是一个安全性问题，现有的改进方法需要强大的攻击者能力，在能力受限场景下还没有找到令人满意的解决办法，此外，模型鲁棒性仍然值得关注。 |

# 详细

[^1]: 自监督学习在非连续表格数据中的应用调研

    A Survey on Self-Supervised Learning for Non-Sequential Tabular Data

    [https://rss.arxiv.org/abs/2402.01204](https://rss.arxiv.org/abs/2402.01204)

    本调研总结了自监督学习在非连续表格数据中的最新进展和挑战，将其方法分为预测性学习、对比学习和混合学习，讨论了应用问题，包括自动化数据工程、跨表格查询和隐私保护等。

    

    自监督学习（SSL）已经被应用于各个领域的许多最先进的模型中，其中SSL通过定义基于无标签数据集的预训练任务来学习上下文化和鲁棒的表示。最近，SSL已成为探索表格数据领域中表示学习能力的新趋势，这是一项更具挑战性的任务，因为它没有明确的关系来学习描述性的表示。本调研旨在系统地回顾和总结自监督学习在非连续表格数据（SSL4NS-TD）中的最新进展和挑战。首先，我们给出了NS-TD的正式定义，并阐明了它与相关研究的关联。然后，这些方法被分为三组——预测性学习、对比学习和混合学习，并介绍了每个方向的代表性方法的动机和优点。在此基础上，还介绍了SSL4NS-TD的应用问题，包括自动化数据工程、跨表格查询和隐私保护等。

    Self-supervised learning (SSL) has been incorporated into many state-of-the-art models in various domains, where SSL defines pretext tasks based on unlabeled datasets to learn contextualized and robust representations. Recently, SSL has been a new trend in exploring the representation learning capability in the realm of tabular data, which is more challenging due to not having explicit relations for learning descriptive representations. This survey aims to systematically review and summarize the recent progress and challenges of SSL for non-sequential tabular data (SSL4NS-TD). We first present a formal definition of NS-TD and clarify its correlation to related studies. Then, these approaches are categorized into three groups -- predictive learning, contrastive learning, and hybrid learning, with their motivations and strengths of representative methods within each direction. On top of this, application issues of SSL4NS-TD are presented, including automatic data engineering, cross-table
    
[^2]: 用于旅行购买者问题的深度强化学习

    Deep Reinforcement Learning for Traveling Purchaser Problems

    [https://arxiv.org/abs/2404.02476](https://arxiv.org/abs/2404.02476)

    提出了一种基于深度强化学习的方法，该方法分别解决了旅行购买者问题中的路由构建和购买规划问题，并从全局角度评估和优化解决方案。

    

    旅行购买者问题（TPP）是一种具有广泛应用的重要组合优化问题。本文提出了一种基于深度强化学习（DRL）的新方法，该方法分别解决了路由构建和购买规划问题，同时从全局角度评估和优化解决方案。我们的方法的关键组成部分包括用于捕捉市场-产品关系的TPP的二部图表示，以及从二部图中提取信息并将其用于顺序构建路由的策略网络。

    arXiv:2404.02476v1 Announce Type: cross  Abstract: The traveling purchaser problem (TPP) is an important combinatorial optimization problem with broad applications. Due to the coupling between routing and purchasing, existing works on TPPs commonly address route construction and purchase planning simultaneously, which, however, leads to exact methods with high computational cost and heuristics with sophisticated design but limited performance. In sharp contrast, we propose a novel approach based on deep reinforcement learning (DRL), which addresses route construction and purchase planning separately, while evaluating and optimizing the solution from a global perspective. The key components of our approach include a bipartite graph representation for TPPs to capture the market-product relations, and a policy network that extracts information from the bipartite graph and uses it to sequentially construct the route. One significant benefit of our framework is that we can efficiently const
    
[^3]: LeTac-MPC：用于触觉反应抓取的学习模型预测控制

    LeTac-MPC: Learning Model Predictive Control for Tactile-reactive Grasping

    [https://arxiv.org/abs/2403.04934](https://arxiv.org/abs/2403.04934)

    LeTac-MPC是一种学习模型预测控制，利用视觉触觉传感器GelSight和不同iable MPC层，实现在不同条件下和具有不同物理属性的物体上进行稳健抓取控制。

    

    抓取是机器人中的关键任务，需要触觉反馈和反应性抓取调整，以实现在各种条件下和具有不同物理属性的对象的稳健抓取。本文介绍了LeTac-MPC，一种基于学习的模型预测控制（MPC）用于触觉反应式抓取。我们的方法使夹爪能够在动态和力交互任务中抓取具有不同物理属性的对象。我们利用基于视觉的触觉传感器GelSight，该传感器能够感知包含抓取对象的物理属性和状态信息的高分辨率触觉反馈。LeTac-MPC包含一个可微分的MPC层，设计用于对通过神经网络（NN）从触觉反馈中提取的嵌入进行建模。这种设计有助于在25 Hz的频率下实现收敛和稳健的抓取控制。我们提出了一个完全自动化的数据收集流程，并收集了一组数据集。

    arXiv:2403.04934v1 Announce Type: cross  Abstract: Grasping is a crucial task in robotics, necessitating tactile feedback and reactive grasping adjustments for robust grasping of objects under various conditions and with differing physical properties. In this paper, we introduce LeTac-MPC, a learning-based model predictive control (MPC) for tactile-reactive grasping. Our approach enables the gripper grasp objects with different physical properties on dynamic and force-interactive tasks. We utilize a vision-based tactile sensor, GelSight, which is capable of perceiving high-resolution tactile feedback that contains the information of physical properties and states of the grasped object. LeTac-MPC incorporates a differentiable MPC layer designed to model the embeddings extracted by a neural network (NN) from tactile feedback. This design facilitates convergent and robust grasping control at a frequency of 25 Hz. We propose a fully automated data collection pipeline and collect a dataset 
    
[^4]: 基于上下文的多模态融合

    Context-Based Multimodal Fusion

    [https://arxiv.org/abs/2403.04650](https://arxiv.org/abs/2403.04650)

    提出一种基于上下文的多模态融合模型，结合了模态融合和数据分布对齐，通过特定上下文向量表示每个模态，并将其与每个模态的嵌入进行融合，

    

    融合模型广泛应用于解决多模态任务，但在不同模态之间数据分布对齐方面存在明显局限性。针对这一挑战，我们提出了一种创新模型称为基于上下文的多模态融合（CBMF），结合了模态融合和数据分布对齐，通过特定上下文向量表示每个模态，并将其与每个模态的嵌入进行融合。

    arXiv:2403.04650v1 Announce Type: cross  Abstract: The fusion models, which effectively combine information from different sources, are widely used in solving multimodal tasks. However, they have significant limitations related to aligning data distributions across different modalities. This challenge can lead to inconsistencies and difficulties in learning robust representations. Alignment models, while specifically addressing this issue, often require training "from scratch" with large datasets to achieve optimal results, which can be costly in terms of resources and time. To overcome these limitations, we propose an innovative model called Context-Based Multimodal Fusion (CBMF), which combines both modality fusion and data distribution alignment. In CBMF, each modality is represented by a specific context vector, fused with the embedding of each modality. This enables the use of large pre-trained models that can be frozen, reducing the computational and training data requirements. A
    
[^5]: MIM-Refiner：一种从中间预训练表示中获得对比学习提升的方法

    MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained Representations

    [https://arxiv.org/abs/2402.10093](https://arxiv.org/abs/2402.10093)

    MIM-Refiner是一种对比学习提升方法，通过利用MIM模型中的中间层表示和多个对比头，能够将MIM模型的特征从次优的状态提升到最先进的状态，并在ImageNet-1K数据集上取得了新的最先进结果。

    

    我们引入了MIM-Refiner，这是一种用于预训练MIM模型的对比学习提升方法。MIM-Refiner的动机在于MIM模型中的最佳表示通常位于中间层。因此，MIM-Refiner利用连接到不同中间层的多个对比头。在每个头中，修改后的最近邻目标帮助构建相应的语义聚类。此过程短而有效，在几个epochs内，我们将MIM模型的特征从次优的状态提升到最先进的状态。使用data2vec 2.0在ImageNet-1K上预训练的ViT-H经过改进后，在线性探测和低样本分类方面取得了新的最先进结果（分别为84.7%和64.2%），超过了在ImageNet-1K上预训练的其他模型的表现。

    arXiv:2402.10093v1 Announce Type: cross  Abstract: We introduce MIM (Masked Image Modeling)-Refiner, a contrastive learning boost for pre-trained MIM models. The motivation behind MIM-Refiner is rooted in the insight that optimal representations within MIM models generally reside in intermediate layers. Accordingly, MIM-Refiner leverages multiple contrastive heads that are connected to diverse intermediate layers. In each head, a modified nearest neighbor objective helps to construct respective semantic clusters.   The refinement process is short but effective. Within a few epochs, we refine the features of MIM models from subpar to state-of-the-art, off-the-shelf features. Refining a ViT-H, pre-trained with data2vec 2.0 on ImageNet-1K, achieves new state-of-the-art results in linear probing (84.7%) and low-shot classification among models that are pre-trained on ImageNet-1K. In ImageNet-1K 1-shot classification, MIM-Refiner sets a new state-of-the-art of 64.2%, outperforming larger mo
    
[^6]: 通过反事实轨迹解释学习到的奖励函数

    Explaining Learned Reward Functions with Counterfactual Trajectories

    [https://arxiv.org/abs/2402.04856](https://arxiv.org/abs/2402.04856)

    通过对比原始轨迹和反事实部分轨迹的奖励，我们提出了一种解释强化学习中奖励函数的方法。通过生成符合质量标准的反事实轨迹解释（CTEs），我们的实验表明，CTEs对于代理人模型具有明显的信息性，能提高其预测与奖励函数的一致性。

    

    从人类行为或反馈中学习奖励是将AI系统与人类价值观一致的一种有希望的方法，但无法始终提取正确的奖励函数。可解释性工具可以帮助用户理解和评估学习到的奖励函数中可能存在的缺陷。我们提出了反事实轨迹解释（CTEs），通过对比原始轨迹和反事实部分轨迹以及它们各自接收的奖励来解释强化学习中的奖励函数。我们为CTEs制定了六个质量标准，并提出了一种基于Monte-Carlo的新算法来生成优化这些质量标准的CTEs。最后，我们通过训练代理人模型来衡量生成的解释对其的信息性。CTEs对于代理人模型具有明显的信息性，增加了其预测与未见轨迹上的奖励函数的相似性。此外，它学会了准确判断轨迹之间的奖励差异。

    Learning rewards from human behaviour or feedback is a promising approach to aligning AI systems with human values but fails to consistently extract correct reward functions. Interpretability tools could enable users to understand and evaluate possible flaws in learned reward functions. We propose Counterfactual Trajectory Explanations (CTEs) to interpret reward functions in reinforcement learning by contrasting an original with a counterfactual partial trajectory and the rewards they each receive. We derive six quality criteria for CTEs and propose a novel Monte-Carlo-based algorithm for generating CTEs that optimises these quality criteria. Finally, we measure how informative the generated explanations are to a proxy-human model by training it on CTEs. CTEs are demonstrably informative for the proxy-human model, increasing the similarity between its predictions and the reward function on unseen trajectories. Further, it learns to accurately judge differences in rewards between trajec
    
[^7]: MixedNUTS: 通过非线性混合分类器实现无需训练的准确性和鲁棒性平衡

    MixedNUTS: Training-Free Accuracy-Robustness Balance via Nonlinearly Mixed Classifiers

    [https://arxiv.org/abs/2402.02263](https://arxiv.org/abs/2402.02263)

    MixedNUTS是一种无需训练的方法，通过非线性混合分类器的转换和概率混合来实现准确性和鲁棒性的平衡。

    

    鲁棒性往往牺牲了准确性，阻碍了鲁棒分类模型在实际应用中的使用。基于训练的解决方案在与已训练的大型高性能模型兼容性方面存在限制，因此需要探索无需训练的集成方法。我们观察到鲁棒模型在干净数据和对抗数据上的正确预测比错误预测更自信，我们推测通过增强这种“良性置信度特性”可以在集成环境中实现准确性和鲁棒性的平衡。为了实现这一点，我们提出了“MixedNUTS”，一种无需训练的方法，利用仅有三个参数的非线性转换来处理鲁棒分类器和标准非鲁棒分类器的输出Logits，并通过高效算法进行优化。然后，MixedNUTS将转换后的Logits转换为概率，并将它们混合作为最终的输出。在CIFAR-10、CIFAR-100和ImageNet数据集上进行了实验。

    Adversarial robustness often comes at the cost of degraded accuracy, impeding the real-life application of robust classification models. Training-based solutions for better trade-offs are limited by incompatibilities with already-trained high-performance large models, necessitating the exploration of training-free ensemble approaches. Observing that robust models are more confident in correct predictions than in incorrect ones on clean and adversarial data alike, we speculate amplifying this "benign confidence property" can reconcile accuracy and robustness in an ensemble setting. To achieve so, we propose "MixedNUTS", a training-free method where the output logits of a robust classifier and a standard non-robust classifier are processed by nonlinear transformations with only three parameters, which are optimized through an efficient algorithm. MixedNUTS then converts the transformed logits into probabilities and mixes them as the overall output. On CIFAR-10, CIFAR-100, and ImageNet da
    
[^8]: CMMMU：一个中国大规模多学科多模态理解基准

    CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark

    [https://arxiv.org/abs/2401.11944](https://arxiv.org/abs/2401.11944)

    CMMMU是一个旨在评估大型多模型模型在大学级学科知识和深思熟虑推理任务中表现的中文大规模多学科多模态理解基准，为填补在非英语环境中评估先进知识和推理能力的空白而设计。

    

    随着大型多模型模型(LMMs)的能力不断提升，评估LMMs的表现日益成为一个迫切的需求。此外，在评估LMMs在中文等非英语环境中先进知识和推理能力方面存在更大差距。我们引入了CMMMU，一个新的中文大规模多学科多模态理解基准，旨在评估LMMs在需要大学水平学科知识和深思熟虑推理的任务中的表现。CMMMU受到了MMMUs的标注和分析模式的启发并严格遵循。CMMMU包括来自大学考试、测验和教科书的1.2万个手动收集的多模态问题，涵盖六个核心学科：艺术与设计、商业、科学、健康与医学、人文社科以及技术与工程，就像其伙伴MMMMU一样。这些问题涵盖30个学科，包括39个高度异质的图像。

    arXiv:2401.11944v2 Announce Type: replace-cross  Abstract: As the capabilities of large multimodal models (LMMs) continue to advance, evaluating the performance of LMMs emerges as an increasing need. Additionally, there is an even larger gap in evaluating the advanced knowledge and reasoning abilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU, a new Chinese Massive Multi-discipline Multimodal Understanding benchmark designed to evaluate LMMs on tasks demanding college-level subject knowledge and deliberate reasoning in a Chinese context. CMMMU is inspired by and strictly follows the annotation and analysis pattern of MMMU.   CMMMU includes 12k manually collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering, like its companion, MMMU. These questions span 30 subjects and comprise 39 highly heterogeneous image 
    
[^9]: 解决在实施负责任人工智能中的伦理权衡

    Resolving Ethics Trade-offs in Implementing Responsible AI

    [https://arxiv.org/abs/2401.08103](https://arxiv.org/abs/2401.08103)

    该论文提出了通过权衡处理人工智能伦理中的紧张关系的五种方法，并提出了一个框架来实施全面的人工智能/机器学习系统。

    

    虽然把高级人工智能伦理原则应用到实际的人工智能/机器学习系统中已经取得了进展，但在处理底层人工智能伦理方面的紧张关系方面仍存在理论与实践之间的差距。我们提出了五种处理这些关系的方法，从简单到复杂不等。这些方法在考虑的上下文类型、范围、衡量上下文的方法和证明程度上有所不同。这些方法中没有一种适用于所有组织、系统或应用。为了解决这个问题，我们提出了一个框架，包括：（i）积极识别紧张关系，（ii）优先处理和权衡伦理方面，（iii）证明和记录权衡决策。该提议的框架旨在促进实施符合潜在监管要求的全面人工智能/机器学习系统。

    While the operationalisation of high-level AI ethics principles into practical AI/ML systems has made progress, there is still a theory-practice gap in managing tensions between the underlying AI ethics aspects. We cover five approaches for addressing the tensions via trade-offs, ranging from rudimentary to complex. The approaches differ in the types of considered context, scope, methods for measuring contexts, and degree of justification. None of the approaches is likely to be appropriate for all organisations, systems, or applications. To address this, we propose a framework which consists of: (i) proactive identification of tensions, (ii) prioritisation and weighting of ethics aspects, (iii) justification and documentation of trade-off decisions. The proposed framework aims to facilitate the implementation of well-rounded AI/ML systems that are appropriate for potential regulatory requirements.
    
[^10]: 可推广的Transformer预训练用于超长时间序列预测

    Extrapolatable Transformer Pre-training for Ultra Long Time-Series Forecasting

    [https://arxiv.org/abs/2312.00817](https://arxiv.org/abs/2312.00817)

    提出了一种名为TimelyGPT的可推广的Transformer预训练模型，该模型通过可推广的位置嵌入和循环注意力以及时间卷积模块有效地捕捉超长时间序列数据中的全局和局部时间依赖关系。

    

    大规模预训练模型（PTMs），如BERT和GPT，最近在自然语言处理和计算机视觉领域取得了巨大成功。然而，PTMs在时间序列数据上的发展滞后。这凸显了现有基于transformer的架构的局限性，特别是它们处理大规模数据和捕捉长期时间依赖性的可扩展性。本研究提出了即时生成预训练Transformer（TimelyGPT）。TimelyGPT采用可推广位置（xPos）嵌入将趋势和周期模式编码到时间序列表示中。它还集成了循环注意力和时间卷积模块，以有效地捕捉全局和局部的时间依赖关系。我们的实验表明，TimelyGPT在建模连续监测的生物信号和经常出现在纵向电磁波领域中不规则采样的时间序列数据方面表现出色。

    arXiv:2312.00817v2 Announce Type: replace-cross  Abstract: Large-scale pre-trained models (PTMs) such as BERT and GPT have recently achieved great success in Natural Language Processing and Computer Vision domains. However, the development of PTMs on time-series data is lagging behind. This underscores the limitations of the existing transformer-based architectures, particularly their scalability to handle large-scale data and ability to capture long-term temporal dependencies. In this study, we present Timely Generative Pre-trained Transformer (TimelyGPT). TimelyGPT employs an extrapolatable position (xPos) embedding to encode trend and periodic patterns into time-series representations. It also integrates recurrent attention and temporal convolution modules to effectively capture global-local temporal dependencies. Our experiments show that TimelyGPT excels in modeling continuously monitored biosignals and irregularly-sampled time series data commonly observed in longitudinal electro
    
[^11]: 在线广告与LLMs：机遇与挑战

    Online Advertisements with LLMs: Opportunities and Challenges

    [https://arxiv.org/abs/2311.07601](https://arxiv.org/abs/2311.07601)

    本文探讨了在线广告系统中利用大型语言模型（LLM）的潜力，提出了一个LLM广告的通用框架，并讨论了基于LLM的动态创意优化的前景。

    

    本文探讨了在在线广告系统中利用大型语言模型（LLM）的潜力。我们深入研究了这个系统必须满足的隐私、延迟、可靠性以及用户和广告商的满意度等关键要求。我们进一步介绍了一个LLM广告的通用框架，包括修改、竞标、预测和拍卖模块。我们提出了每个模块的不同设计考虑，并对其实用性和实施中的技术挑战进行了深入研究。最后，我们探讨了基于LLM的动态创意优化的前景，以显著提升广告对用户的吸引力，并讨论了它所面临的额外挑战。

    arXiv:2311.07601v2 Announce Type: replace-cross Abstract: This paper explores the potential for leveraging Large Language Models (LLM) in the realm of online advertising systems. We delve into essential requirements including privacy, latency, reliability as well as the satisfaction of users and advertisers which such a system must fulfill. We further introduce a general framework for LLM advertisement, consisting of modification, bidding, prediction, and auction modules. Different design considerations for each module is presented, with an in-depth examination of their practicality and the technical challenges inherent to their implementation. Finally, we explore the prospect of LLM-based dynamic creative optimization as a means to significantly enhance the appeal of advertisements to users and discuss its additional challenges.
    
[^12]: 本地通用解释器（LUX）-- 一种基于规则的解释器，具有事实、反事实和视觉解释

    Local Universal Explainer (LUX) -- a rule-based explainer with factual, counterfactual and visual explanations

    [https://arxiv.org/abs/2310.14894](https://arxiv.org/abs/2310.14894)

    LUX是一种基于规则的解释器，可以生成事实、反事实和视觉解释，通过选择高密度簇形式的局部概念来形成决策边界。

    

    可解释的人工智能（XAI）是近年来最被广泛发展的人工智能领域之一。它也是最分散的领域之一，有多种方法专注于解释的不同方面。这使得一次性以紧凑和一致的方式获得完整的解释变得困难。为了解决这个问题，我们提出了本地通用解释器（LUX），它是一种基于规则的解释器，可以生成事实、反事实和视觉解释。它基于修改后的决策树算法，允许斜交和集成特征重要性XAI方法，如SHAP或LIME。与其他算法相反，它不使用数据生成，而是专注于选择以高密度簇形式出现的真实数据的局部概念，这些局部概念对解释模型的决策边界形成最大的影响。我们在真实和合成数据集上测试了我们的方法，并将其与最先进的基于规则的方法进行了比较。

    Explainable artificial intelligence (XAI) is one of the most intensively developed area of AI in recent years. It is also one of the most fragmented with multiple methods that focus on different aspects of explanations. This makes difficult to obtain the full spectrum of explanation at once in a compact and consistent way. To address this issue, we present Local Universal Explainer (LUX), which is a rule-based explainer that can generate factual, counterfactual and visual explanations. It is based on a modified version of decision tree algorithms that allows for oblique splits and integration with feature importance XAI methods such as SHAP or LIME. It does not use data generation in opposite to other algorithms, but is focused on selecting local concepts in a form of high-density clusters of real data that have the highest impact on forming the decision boundary of the explained model. We tested our method on real and synthetic datasets and compared it with state-of-the-art rule-based
    
[^13]: 重新考虑基于DNA序列的BERT-like预训练

    Rethinking the BERT-like Pretraining for DNA Sequences. (arXiv:2310.07644v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2310.07644](http://arxiv.org/abs/2310.07644)

    重新考虑了基于DNA序列的BERT-like预训练方法，通过使用K-mer重叠标记化，在下游任务的微调阶段和预训练过程中都取得了一致的性能改善。

    

    随着在自然语言处理领域中大规模预训练的成功，将其应用于生命科学领域的趋势日益增长。特别是基于DNA序列的预训练方法因其捕捉基因的通用信息的潜力而受到关注。然而，现有的DNA序列预训练方法主要依赖于从自然语言处理领域直接引入的BERT预训练方法，缺乏全面的理解和专门定制的方法。为了填补这一研究空白，我们首先进行了一系列的探索性实验，并获得了几个有启发性的观察结果：1）在下游任务的微调阶段，使用K-mer重叠标记化而不是K-mer非重叠标记化时，重叠和非重叠的预训练权重均表现出一致的性能改善。2）在预训练过程中，使用K-mer重叠标记化会迅速产生清晰的K-mer嵌入，并将损失降低到非常低的水平。

    With the success of large-scale pretraining in NLP, there is an increasing trend of applying it to the domain of life sciences. In particular, pretraining methods based on DNA sequences have garnered growing attention due to their potential to capture generic information about genes. However, existing pretraining methods for DNA sequences largely rely on direct adoptions of BERT pretraining from NLP, lacking a comprehensive understanding and a specifically tailored approach. To address this research gap, we first conducted a series of exploratory experiments and gained several insightful observations: 1) In the fine-tuning phase of downstream tasks, when using K-mer overlapping tokenization instead of K-mer non-overlapping tokenization, both overlapping and non-overlapping pretraining weights show consistent performance improvement.2) During the pre-training process, using K-mer overlapping tokenization quickly produces clear K-mer embeddings and reduces the loss to a very low level, w
    
[^14]: 通过轨迹生成学习具有通用性的工具使用技能

    Learning Generalizable Tool-use Skills through Trajectory Generation. (arXiv:2310.00156v1 [cs.RO])

    [http://arxiv.org/abs/2310.00156](http://arxiv.org/abs/2310.00156)

    通过轨迹生成，我们提出了一种学习通用工具使用技能的方法，可以适应不同形状的工具，从而使自主系统能够处理复杂的可变形物体操作任务。

    

    高效利用工具的自主系统可以帮助人们完成许多常见任务，如烹饪和清洁。然而，当前的系统在适应新工具方面远远不及人类的智能水平。基于可及性的先前工作通常对环境做出了很强的假设，并且无法扩展到更复杂、接触丰富的任务。 在这项工作中，我们解决了这个挑战，并探索了代理如何学习使用以前未见过的工具来操纵可变形物体。 我们提出了将工具使用轨迹作为一系列点云的生成模型，可以推广到不同的工具形状。对于任何新的工具，我们首先生成一个工具使用轨迹，然后优化工具姿势序列以与生成的轨迹对齐。我们为四种不同的具有挑战性的可变形物体操纵任务训练了一个单一模型。我们的模型仅使用每个任务的单个工具的示范数据进行训练，并且能够...

    Autonomous systems that efficiently utilize tools can assist humans in completing many common tasks such as cooking and cleaning. However, current systems fall short of matching human-level of intelligence in terms of adapting to novel tools. Prior works based on affordance often make strong assumptions about the environments and cannot scale to more complex, contact-rich tasks. In this work, we tackle this challenge and explore how agents can learn to use previously unseen tools to manipulate deformable objects. We propose to learn a generative model of the tool-use trajectories as a sequence of point clouds, which generalizes to different tool shapes. Given any novel tool, we first generate a tool-use trajectory and then optimize the sequence of tool poses to align with the generated trajectory. We train a single model for four different challenging deformable object manipulation tasks. Our model is trained with demonstration data from just a single tool for each task and is able to 
    
[^15]: DEFormer: 用于低光图像和暗视觉的DCT驱动增强Transformer

    DEFormer: DCT-driven Enhancement Transformer for Low-light Image and Dark Vision. (arXiv:2309.06941v1 [cs.CV])

    [http://arxiv.org/abs/2309.06941](http://arxiv.org/abs/2309.06941)

    该论文提出了一种新的DCT驱动增强Transformer（DEFormer），可以在低光图像中恢复丢失的细节，通过引入频率作为新的线索，通过可学习的频率分支（LFB）和基于曲率的频率增强（CFE）来实现。此外，还提出了交叉域融合（CDF）来减少领域之间的差异，DEFormer还可以作为暗部检测的预处理，有效提高了性能。

    

    低光图像增强的目标是恢复图像的颜色和细节，对于自动驾驶中的高级视觉任务非常重要。然而，仅依靠RGB领域很难恢复暗区域的丢失细节。本文将频率作为网络的新线索，并提出了一种新颖的DCT驱动增强Transformer（DEFormer）。首先，我们提出了一个可学习的频率分支（LFB）用于频率增强，包括DCT处理和基于曲率的频率增强（CFE）。CFE计算每个通道的曲率以表示不同频率带的细节丰富度，然后我们将频率特征划分为更丰富纹理的频率带。此外，我们提出了一个交叉域融合（CDF）来减少RGB领域和频率领域之间的差异。我们还将DEFormer作为暗部检测的预处理，DEFormer有效提高了性能。

    The goal of low-light image enhancement is to restore the color and details of the image and is of great significance for high-level visual tasks in autonomous driving. However, it is difficult to restore the lost details in the dark area by relying only on the RGB domain. In this paper we introduce frequency as a new clue into the network and propose a novel DCT-driven enhancement transformer (DEFormer). First, we propose a learnable frequency branch (LFB) for frequency enhancement contains DCT processing and curvature-based frequency enhancement (CFE). CFE calculates the curvature of each channel to represent the detail richness of different frequency bands, then we divides the frequency features, which focuses on frequency bands with richer textures. In addition, we propose a cross domain fusion (CDF) for reducing the differences between the RGB domain and the frequency domain. We also adopt DEFormer as a preprocessing in dark detection, DEFormer effectively improves the performance
    
[^16]: 如何攻击可以干扰看似稳定准确的分类器

    How adversarial attacks can disrupt seemingly stable accurate classifiers. (arXiv:2309.03665v1 [cs.LG])

    [http://arxiv.org/abs/2309.03665](http://arxiv.org/abs/2309.03665)

    本文研究了对抗性攻击如何通过微小修改干扰准确的分类器，并发现这可能是高维输入数据下分类器的基本特征。作者提出了一个通用的框架，解释了实际系统中观察到的关键行为，包括模型对对抗性攻击的容易受到影响，同时对随机扰动具有鲁棒性。验证实验还表明了相同现象在实际神经网络中的存在。

    

    对抗性攻击通过对输入数据进行微小的修改，极大地改变了原本准确的学习系统的输出。具有讽刺意味的是，经验证据表明，即使系统对输入数据的大幅度随机扰动具有鲁棒性，它们仍然容易受到输入数据的小众、易于构造的对抗性扰动的影响。在这里，我们展示了这可能是高维输入数据下分类器的一个基本特征。我们引入了一个简单的通用性和普适性框架，其中在实际系统中观察到的关键行为具有高概率出现，尤其是（原本准确的）模型对易于构造的对抗性攻击的同时容易受到输入数据的随机扰动的影响。我们在标准图像分类问题上验证了相同现象在实际神经网络中的直接观察结果，即使是大幅度的加性随机噪声也无法干扰模型的准确性。

    Adversarial attacks dramatically change the output of an otherwise accurate learning system using a seemingly inconsequential modification to a piece of input data. Paradoxically, empirical evidence indicates that even systems which are robust to large random perturbations of the input data remain susceptible to small, easily constructed, adversarial perturbations of their inputs. Here, we show that this may be seen as a fundamental feature of classifiers working with high dimensional input data. We introduce a simple generic and generalisable framework for which key behaviours observed in practical systems arise with high probability -- notably the simultaneous susceptibility of the (otherwise accurate) model to easily constructed adversarial attacks, and robustness to random perturbations of the input data. We confirm that the same phenomena are directly observed in practical neural networks trained on standard image classification problems, where even large additive random noise fai
    
[^17]: PointLLM：赋予大型语言模型理解点云的能力

    PointLLM: Empowering Large Language Models to Understand Point Clouds. (arXiv:2308.16911v1 [cs.CV])

    [http://arxiv.org/abs/2308.16911](http://arxiv.org/abs/2308.16911)

    PointLLM是一种使大型语言模型理解点云的方法，它利用点云编码器和强大的LLM将几何、外观和语言信息融合，并通过人类指导生成环境上恰当的响应。该方法通过收集大规模的点-文本指令对数据集进行两阶段的训练，以提高模型的感知能力和泛化能力。

    

    大型语言模型（LLM）的前所未有的进展对自然语言处理产生了深远影响，但在3D理解领域仍有待完全发展。本文介绍了PointLLM，这是一项填补这一空白的初步工作，使LLM能够理解点云，并提供了超越2D视觉数据的新途径。PointLLM通过人类指导处理带有颜色的物体点云，并生成环境上恰当的响应，展示了其对点云和常识的掌握。具体来说，它利用了一个点云编码器和一个强大的LLM，有效地融合了几何、外观和语言信息。我们收集了一个新颖的数据集，包括66万个简单和7万个复杂的点-文本指令对，以实现两阶段的训练策略：首先对齐潜在空间，然后对统一模型进行指令调整。为了严格评估我们模型的感知能力和其泛化能力，我们建立了评估基准数据集进行实验。

    The unprecedented advancements in Large Language Models (LLMs) have created a profound impact on natural language processing but are yet to fully embrace the realm of 3D understanding. This paper introduces PointLLM, a preliminary effort to fill this gap, thereby enabling LLMs to understand point clouds and offering a new avenue beyond 2D visual data. PointLLM processes colored object point clouds with human instructions and generates contextually appropriate responses, illustrating its grasp of point clouds and common sense. Specifically, it leverages a point cloud encoder with a powerful LLM to effectively fuse geometric, appearance, and linguistic information. We collect a novel dataset comprising 660K simple and 70K complex point-text instruction pairs to enable a two-stage training strategy: initially aligning latent spaces and subsequently instruction-tuning the unified model. To rigorously evaluate our model's perceptual abilities and its generalization capabilities, we establis
    
[^18]: 基于图注意力的部分可观察均场多智能体强化学习

    Partially Observable Mean Field Multi-Agent Reinforcement Learning Based on Graph-Attention. (arXiv:2304.12653v1 [cs.AI])

    [http://arxiv.org/abs/2304.12653](http://arxiv.org/abs/2304.12653)

    本文提出了一种新的基于图注意力的部分可观察均场多智能体强化学习算法，使用图注意力来捕获周围邻居智能体的特征信息，可以提高大规模多智能体环境中部分可观察MARL的性能。

    

    传统的多智能体强化学习算法难以在大规模多智能体环境中应用。最近引入的均场理论提高了多智能体强化学习的可扩展性。本文考虑部分可观察的多智能体强化学习，其中每个智能体只能观察到固定范围内的其他智能体。这种部分可观察性影响了智能体评估周围智能体行动质量的能力。本文着重于开发一种从局部观测中获取更有效信息以选择更有效行动的方法。在这个领域的以前工作使用概率分布或加权均场来更新邻居智能体平均行动，但它没有充分考虑周围邻居的特征信息，导致了局部最优。本文提出了一种新的多智能体强化学习算法，基于图注意力的部分可观察均场多智能体强化学习，它使用图注意力来捕获周围邻居智能体的特征信息。我们的方法可以提高大规模多智能体环境中部分可观察MARL的性能。

    Traditional multi-agent reinforcement learning algorithms are difficultly applied in a large-scale multi-agent environment. The introduction of mean field theory has enhanced the scalability of multi-agent reinforcement learning in recent years. This paper considers partially observable multi-agent reinforcement learning (MARL), where each agent can only observe other agents within a fixed range. This partial observability affects the agent's ability to assess the quality of the actions of surrounding agents. This paper focuses on developing a method to capture more effective information from local observations in order to select more effective actions. Previous work in this field employs probability distributions or weighted mean field to update the average actions of neighborhood agents, but it does not fully consider the feature information of surrounding neighbors and leads to a local optimum. In this paper, we propose a novel multi-agent reinforcement learning algorithm, Partially
    
[^19]: 在能力受限场景下启动强韧后门攻击

    Launching a Robust Backdoor Attack under Capability Constrained Scenarios. (arXiv:2304.10985v1 [cs.CR])

    [http://arxiv.org/abs/2304.10985](http://arxiv.org/abs/2304.10985)

    深度神经网络的后门攻击一直是一个安全性问题，现有的改进方法需要强大的攻击者能力，在能力受限场景下还没有找到令人满意的解决办法，此外，模型鲁棒性仍然值得关注。

    

    随着深度神经网络在关键领域的应用不断增加，人们开始担心它们的安全性。由于缺乏透明度，深度学习模型容易受到后门攻击的威胁。污染的后门模型在普通环境下可能表现正常，但当输入包含触发器时，会显示出恶意行为。目前对后门攻击的研究集中于改善触发器的秘密性，大多数方法需要强大的攻击者能力，例如对模型结构的了解或对训练过程的控制。由于在大多数情况下攻击者的能力受到限制，这些攻击是不切实际的。此外，模型鲁棒性的问题还未得到充分关注。例如，模型蒸馏常用于简化模型大小，但随着参数数量指数级增长，以前的许多后门攻击在模型蒸馏后均失败;图像增强操作可以破坏触发器，从而使后门攻击失效。

    As deep neural networks continue to be used in critical domains, concerns over their security have emerged. Deep learning models are vulnerable to backdoor attacks due to the lack of transparency. A poisoned backdoor model may perform normally in routine environments, but exhibit malicious behavior when the input contains a trigger. Current research on backdoor attacks focuses on improving the stealthiness of triggers, and most approaches require strong attacker capabilities, such as knowledge of the model structure or control over the training process. These attacks are impractical since in most cases the attacker's capabilities are limited. Additionally, the issue of model robustness has not received adequate attention. For instance, model distillation is commonly used to streamline model size as the number of parameters grows exponentially, and most of previous backdoor attacks failed after model distillation; the image augmentation operations can destroy the trigger and thus disabl
    

