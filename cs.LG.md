# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Deep Support Vectors](https://arxiv.org/abs/2403.17329) | 该论文探索了深度学习模型中的深度支持向量（DSVs）的概念，介绍了DeepKKT条件，通过实证研究发现DSVs与SVM中的支持向量类似，为解释模型决策标准提供了方法，同时证明了可以有效地使用DSVs重构模型。 |
| [^2] | [Physics-informed and Unsupervised Riemannian Domain Adaptation for Machine Learning on Heterogeneous EEG Datasets](https://arxiv.org/abs/2403.15415) | 提出一种利用物理信息和无监督方法的黎曼域自适应技术，能够有效整合不同EEG数据集以进行机器学习，特别在脑机接口任务和生物标志物应用中表现出鲁棒性。 |
| [^3] | [Physics-Guided Neural Networks for Intraventricular Vector Flow Mapping](https://arxiv.org/abs/2403.13040) | 该研究提出了使用物理引导的神经网络和基于nnU-Net的监督方法来优化心室向量流动映射，效果与传统算法相当，且具有更高的效率和泛化能力。 |
| [^4] | [Sample Complexity of Offline Distributionally Robust Linear Markov Decision Processes](https://arxiv.org/abs/2403.12946) | 本文研究了离线强化学习中线性马尔可夫决策过程的分布鲁棒性样本复杂度问题，提出了一种悲观模型算法并建立了其样本复杂性界限，能在高维状态-动作空间中提高学习策略的性能。 |
| [^5] | [Decentralized Stochastic Subgradient Methods for Nonsmooth Nonconvex Optimization](https://arxiv.org/abs/2403.11565) | 该论文介绍了一种名为DSM的统一框架，用于分析去中心化随机次梯度方法的全局收敛性，证明了在温和条件下的全局收敛性，并展示其涵盖了各种现有高效的去中心化次梯度方法。 |
| [^6] | [JAXbind: Bind any function to JAX](https://arxiv.org/abs/2403.08847) | JAXbind旨在大幅减少将其他编程语言中实现的自定义函数绑定到JAX所需的工作量，提供易于使用的Python接口定义自定义JAX原语。 |
| [^7] | [Thermometer: Towards Universal Calibration for Large Language Models](https://arxiv.org/abs/2403.08819) | 提出了一种针对大型语言模型的校准方法THERMOMETER，通过学习来自多个任务数据的辅助模型，实现了计算效率高、准确性保持并产生更好校准响应的目标。 |
| [^8] | [SoK: Can Trajectory Generation Combine Privacy and Utility?](https://arxiv.org/abs/2403.07218) | 本文提出了一个旨在设计保护隐私的轨迹发布方法的框架，特别强调了选择适当隐私单位的重要性。 |
| [^9] | [Shortcut Learning in Medical Image Segmentation](https://arxiv.org/abs/2403.06748) | 本研究将捷径学习现象扩展到医学图像分割领域，发现临床注释和特定数据处理方式可能误导模型并影响分割准确性，提出了缓解捷径学习影响的策略。 |
| [^10] | [Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text](https://arxiv.org/abs/2403.05750) | 大型语言模型在自然语言生成领域取得了重大突破，提出了识别AI生成文本的解决方案，并探索了未来研究方向。 |
| [^11] | [Improving Variational Autoencoder Estimation from Incomplete Data with Mixture Variational Families](https://arxiv.org/abs/2403.03069) | 缺失数据增加了模型对潜在变量后验分布的复杂性，本文提出了两种策略——基于有限变分混合和基于填补的变分混合分布，有效改善了从不完整数据估计VAE的准确性。 |
| [^12] | [Optimisic Information Directed Sampling](https://arxiv.org/abs/2402.15411) | 提出了一种名为乐观信息导向采样的算法模板, 结合了贝叶斯理论和最坏情形理论，能够实现类似于贝叶斯方法的实例相关遗憾保证，但无需贝叶斯假设。 |
| [^13] | [MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases](https://arxiv.org/abs/2402.14905) | MobileLLM通过优化模型架构，采用深度和瘦身结构、嵌入共享和分组查询注意机制，实现了2.7%/4.3%的准确率提升，并提出了一种无需增加模型大小且仅有极小延迟开销的块状权重共享方法 |
| [^14] | [Extraction of nonlinearity in neural networks and model compression with Koopman operator](https://arxiv.org/abs/2402.11740) | 本论文使用Koopman算子提取神经网络中的非线性，证明了受限非线性已足够进行手写数字分类，并提出了一种模型压缩方法，能在资源受限环境中处理大型网络。 |
| [^15] | [The Price of Adaptivity in Stochastic Convex Optimization](https://arxiv.org/abs/2402.10898) | 该论文证明了在非光滑随机凸优化中，适应性的代价是无法避免的，并且给出了关于不确定性参数的次优性乘法增加的下界。 |
| [^16] | [Data Reconstruction Attacks and Defenses: A Systematic Evaluation](https://arxiv.org/abs/2402.09478) | 本研究提出了一种在联合学习环境中的强力重构攻击，可以重构中间特征，并且对大部分先前的方法表现更好。实证研究表明，在防御机制中，梯度修剪是对抗最先进攻击最有效的策略。 |
| [^17] | [Refining Myocardial Infarction Detection: A Novel Multi-Modal Composite Kernel Strategy in One-Class Classification](https://arxiv.org/abs/2402.06530) | 本研究提出了一种新的方法，使用基于超声心动图的一种基于多模态复合核策略的单一类别分类算法来进行早期心肌梗死的检测。这种方法通过优化投影矩阵和特征转化，提高了心肌梗死检测的能力。 |
| [^18] | [Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications](https://arxiv.org/abs/2402.05162) | 本研究通过修剪和低秩修改，发现大型语言模型（LLMs）的安全机制固有易碎性，去除安全关键区域会损害安全性，但对效用影响不大，需要更强健的安全策略。 |
| [^19] | [Source-Free Domain Adaptation with Diffusion-Guided Source Data Generation](https://arxiv.org/abs/2402.04929) | 本文提出了一种无源域自适应的新方法，利用扩散模型生成上下文相关的领域特定图像，通过微调预训练模型和无监督领域自适应技术实现了显著的性能改进。 |
| [^20] | [AdaTreeFormer: Few Shot Domain Adaptation for Tree Counting from a Single High-Resolution Image](https://arxiv.org/abs/2402.02956) | AdaTreeFormer是一种从源领域学习并适应只有有限数量标注树木的目标领域的框架，利用一个共享的编码器和分层特征提取方案，实现了树木计数的少样本领域自适应。 |
| [^21] | [Transfer Learning in ECG Diagnosis: Is It Effective?](https://arxiv.org/abs/2402.02021) | 本研究首次对心电图诊断中的迁移学习进行了广泛的经验研究，发现微调对于小型数据集是较好的选择，当数据集足够大时，从头开始训练可以达到可比性能，但需要更长的训练时间。同时，迁移学习与卷积神经网络具有更好的兼容性。 |
| [^22] | [$\mu$GUIDE: a framework for microstructure imaging via generalized uncertainty-driven inference using deep learning](https://arxiv.org/abs/2312.17293) | $\mu$GUIDE是一个通用的贝叶斯框架，利用深度学习进行微结构成像，能够有效估计组织微结构参数的后验分布，并量化参数的不确定性和模糊性。 |
| [^23] | [Challenging Common Paradigms in Multi-Task Learning](https://arxiv.org/abs/2311.04698) | 我们挑战了多任务学习中的常见范式，通过研究在单任务学习中的影响，揭示了优化器选择在MTL中的关键作用，并理论推导出了梯度冲突的角色。 |
| [^24] | [Bayesian identification of nonseparable Hamiltonians with multiplicative noise using deep learning and reduced-order modeling.](http://arxiv.org/abs/2401.12476) | 本文提出了一种用于学习非分离哈密顿系统的结构保持的贝叶斯方法，可以处理统计相关的加性和乘性噪声，并且通过将结构保持方法纳入框架中，提供了对高维系统的高效识别。 |
| [^25] | [Beyond Anti-Forgetting: Multimodal Continual Instruction Tuning with Positive Forward Transfer.](http://arxiv.org/abs/2401.09181) | 本研究提出了一种名为Fwd-Prompt的方法，通过对输入嵌入进行奇异值分解，并在残差空间和预训练子空间中进行梯度投影，以解决多模态连续指导调优中的灾难性遗忘和负面的正向传递问题。 |
| [^26] | [Input Convex LSTM: A Convex Approach for Fast Lyapunov-Based Model Predictive Control.](http://arxiv.org/abs/2311.07202) | 本研究提出了一种基于输入凸LSTM的基于Lyapunov的模型预测控制方法，通过减少收敛时间和缓解梯度消失/爆炸问题来改善MPC的性能。 |
| [^27] | [A Study of Continual Learning Under Language Shift.](http://arxiv.org/abs/2311.01200) | 本文研究了持续学习在语言转换中的应用，发现在更新语言模型时，前向转移效果较好且与语言顺序无关，但后向转移效果可能取决于新语言的顺序和特征。 |
| [^28] | [MixerFlow for Image Modelling.](http://arxiv.org/abs/2310.16777) | MixerFlow是一种新型的基于MLP-Mixer架构的正则化流模型，通过提供有效的权重共享机制，实现了更好的图像密度估计性能和更丰富的嵌入表示。 |
| [^29] | [Adaptive Pairwise Encodings for Link Prediction.](http://arxiv.org/abs/2310.11009) | 提出了一种自适应的对向编码方法，用于解决链路预测中现有方法的归纳偏差问题。该方法将消息传递神经网络和启发式方法结合起来，能够更好地分类各种不同因素形成的链路。 |
| [^30] | [Hierarchical Concept Discovery Models: A Concept Pyramid Scheme.](http://arxiv.org/abs/2310.02116) | 本论文提出了一种分层概念发现模型，通过利用图像文本模型和基于数据驱动的贝叶斯参数，实现了基于人类可理解概念的高度可解释的决策过程。 |
| [^31] | [Long-term drought prediction using deep neural networks based on geospatial weather data.](http://arxiv.org/abs/2309.06212) | 基于地理气象数据的深度神经网络用于长期干旱预测，提出了一种端到端解决方案来预测特定地区干旱概率。采用卷积LSTM和Transformer模型相比其他基线模型能够获得更高的准确性。 |
| [^32] | [System Identification for Continuous-time Linear Dynamical Systems.](http://arxiv.org/abs/2308.11933) | 本文解决了在连续时间下观测不规则采样的情况下，Kalman滤波器的系统识别问题。通过引入连续时间Ito随机微分方程来推广Kalman滤波器的学习，并提供一个新颖的两滤波器的后验计算方法，通过贝叶斯派生获得的解析形式的后验计算方法可以高效地估计SDE的参数。 |
| [^33] | [AI-Enabled Software and System Architecture Frameworks: Focusing on smart Cyber-Physical Systems (CPS).](http://arxiv.org/abs/2308.05239) | 这篇论文提出了适应现代应用和组织要求的AI-enabled软件和系统架构框架，重点关注机器学习驱动的智能物联网系统(CPS)。作者提出了用于评估和基准化ML-enabled CPS的优点标准。 |
| [^34] | [Efficient Interaction-Aware Interval Analysis of Neural Network Feedback Loops.](http://arxiv.org/abs/2307.14938) | 本文提出了一种计算效率高的神经网络控制系统区间可达性分析框架，通过引入包含函数和构建嵌入系统来捕捉系统和神经网络控制器之间的相互作用。 |
| [^35] | [Kernelised Normalising Flows.](http://arxiv.org/abs/2307.14839) | 本文提出了一种新颖的核化归一化流范式，称为Ferumal流，它将核函数集成到归一化流的框架中。相对于基于神经网络的流，核化流可以在低数据环境中产生竞争力或优越的结果，同时保持参数效率。 |
| [^36] | [Maximum Likelihood Training of Autoencoders.](http://arxiv.org/abs/2306.01843) | 本文介绍了一种成功的最大似然训练方法，用于非约束自编码器，将生成建模的优异性质与高效自编码器相结合。作者克服了两个挑战：设计了消除迭代的估计器并提出了稳定的最大似然训练目标。实验证明这种方法可以成功训练一系列非约束性自编码器，并取得了有竞争力的性能。 |
| [^37] | [Some Primal-Dual Theory for Subgradient Methods for Strongly Convex Optimization.](http://arxiv.org/abs/2305.17323) | 本文提出了一种强凸优化的次梯度法原始对偶理论，可以实现简单的、最佳的停止准则和优化证明，同时可以适用于各种步长的选择和非Lipschitz病态问题，保证了这些方法次线性收敛速度。 |
| [^38] | [Towards understanding neural collapse in supervised contrastive learning with the information bottleneck method.](http://arxiv.org/abs/2305.11957) | 本文将神经网络崩溃建模为信息瓶颈问题，证明神经网络崩溃导致良好的泛化，特别是当它接近分类问题的最优信息瓶颈解时。 |
| [^39] | [Expressiveness Remarks for Denoising Diffusion Models and Samplers.](http://arxiv.org/abs/2305.09605) | 本文在漫扩扩散模型和采样器方面进行了表达能力的研究，通过将已知的神经网络逼近结果扩展到漫扩扩散模型和采样器来实现。 |
| [^40] | [GSplit: Scaling Graph Neural Network Training on Large Graphs via Split-Parallelism.](http://arxiv.org/abs/2303.13775) | 本文提出了一种新的并行小批量训练方法，即分裂并行，应用在图神经网络训练上，能有效缓解数据并行方法的性能瓶颈，同时在大规模图上的性能表现优越。 |
| [^41] | [Disentangled Representation Learning.](http://arxiv.org/abs/2211.11695) | 解缠表示学习旨在学习一个模型，能够识别和解缠观测数据中隐藏的因素，从而产生可解释的数据表示。它在提高模型可解释性、可控性、鲁棒性和泛化能力方面具有广泛的应用潜力。 |

# 详细

[^1]: 深度支持向量

    Deep Support Vectors

    [https://arxiv.org/abs/2403.17329](https://arxiv.org/abs/2403.17329)

    该论文探索了深度学习模型中的深度支持向量（DSVs）的概念，介绍了DeepKKT条件，通过实证研究发现DSVs与SVM中的支持向量类似，为解释模型决策标准提供了方法，同时证明了可以有效地使用DSVs重构模型。

    

    尽管深度学习的成功通常被归因于其与支持向量机（SVM）在理论上的等价性，但这种关系的实际影响尚未得到全面探讨。本文在这一领域开展了一项探索，重点关注深度学习模型中深度支持向量（DSVs）的识别。我们引入了DeepKKT条件的概念，这是一种专为深度学习量身定制的传统Karush-Kuhn-Tucker（KKT）条件的调整版本。通过实证研究，我们阐明了DSVs与SVM中的支持向量之间存在相似性，提供了一种解释模型决策标准的切实方法。此外，我们的研究结果表明，可以有效地使用DSVs重构模型，类似于SVM中的过程。代码将会公开。

    arXiv:2403.17329v1 Announce Type: cross  Abstract: While the success of deep learning is commonly attributed to its theoretical equivalence with Support Vector Machines (SVM), the practical implications of this relationship have not been thoroughly explored. This paper pioneers an exploration in this domain, specifically focusing on the identification of Deep Support Vectors (DSVs) within deep learning models. We introduce the concept of DeepKKT conditions, an adaptation of the traditional Karush-Kuhn-Tucker (KKT) conditions tailored for deep learning. Through empirical investigations, we illustrate that DSVs exhibit similarities to support vectors in SVM, offering a tangible method to interpret the decision-making criteria of models. Additionally, our findings demonstrate that models can be effectively reconstructed using DSVs, resembling the process in SVM. The code will be available.
    
[^2]: 基于物理信息和无监督的黎曼域自适应用于异构脑电数据的机器学习

    Physics-informed and Unsupervised Riemannian Domain Adaptation for Machine Learning on Heterogeneous EEG Datasets

    [https://arxiv.org/abs/2403.15415](https://arxiv.org/abs/2403.15415)

    提出一种利用物理信息和无监督方法的黎曼域自适应技术，能够有效整合不同EEG数据集以进行机器学习，特别在脑机接口任务和生物标志物应用中表现出鲁棒性。

    

    由于会话、受试者和设备的变异性，将脑电图 (EEG) 数据集用于监督机器学习 (ML) 具有挑战性。ML算法通常需要在训练和测试时具有相同的特征，由于不同数据集之间传感器数量和位置的变化而复杂化了分析。简单的通道选择会丢失有价值的数据，尤其是在共享少量通道的数据集中表现更差。为了解决这个问题，我们提出了一个利用EEG信号物理信息的无监督方法。我们使用场插值将EEG通道映射到固定位置，促进无源领域自适应。利用黎曼几何分类流程和迁移学习步骤，我们的方法在脑机接口 (BCI) 任务和潜在生物标志物应用中展示出强大的性能。与一种称为超越维度的基于统计的方法进行比较，一种基于信号的插值

    arXiv:2403.15415v1 Announce Type: cross  Abstract: Combining electroencephalogram (EEG) datasets for supervised machine learning (ML) is challenging due to session, subject, and device variability. ML algorithms typically require identical features at train and test time, complicating analysis due to varying sensor numbers and positions across datasets. Simple channel selection discards valuable data, leading to poorer performance, especially with datasets sharing few channels. To address this, we propose an unsupervised approach leveraging EEG signal physics. We map EEG channels to fixed positions using field interpolation, facilitating source-free domain adaptation. Leveraging Riemannian geometry classification pipelines and transfer learning steps, our method demonstrates robust performance in brain-computer interface (BCI) tasks and potential biomarker applications. Comparative analysis against a statistical-based approach known as Dimensionality Transcending, a signal-based imputa
    
[^3]: 物理引导的神经网络用于心室向量流动映射

    Physics-Guided Neural Networks for Intraventricular Vector Flow Mapping

    [https://arxiv.org/abs/2403.13040](https://arxiv.org/abs/2403.13040)

    该研究提出了使用物理引导的神经网络和基于nnU-Net的监督方法来优化心室向量流动映射，效果与传统算法相当，且具有更高的效率和泛化能力。

    

    Intraventricular vector flow mapping (iVFM)旨在增强和量化心脏成像中的彩色多普勒。本研究提出了一种新颖的替代方案，通过利用物理信息神经网络（PINNs）和基于物理引导的nnU-Net监督方法来优化传统的iVFM优化方案。通过对基于患者特定计算流体动力学模型产生的模拟彩色多普勒图像和体内多普勒采集的严格评估，两种方法均展现出与原始iVFM算法相当的重建性能。 PINNs的效率通过双阶段优化和预优化权重得到提升。另一方面，nnU-Net方法在泛化能力和实时性能方面表现出色。值得注意的是，nnU-Net在稀疏和截断多普勒数据上表现出更好的鲁棒性，同时保持独立于明确的边界条件。总的来说，我们的结果突出了效果

    arXiv:2403.13040v1 Announce Type: cross  Abstract: Intraventricular vector flow mapping (iVFM) seeks to enhance and quantify color Doppler in cardiac imaging. In this study, we propose novel alternatives to the traditional iVFM optimization scheme by utilizing physics-informed neural networks (PINNs) and a physics-guided nnU-Net-based supervised approach. Through rigorous evaluation on simulated color Doppler images derived from a patient-specific computational fluid dynamics model and in vivo Doppler acquisitions, both approaches demonstrate comparable reconstruction performance to the original iVFM algorithm. The efficiency of PINNs is boosted through dual-stage optimization and pre-optimized weights. On the other hand, the nnU-Net method excels in generalizability and real time capabilities. Notably, nnU-Net shows superior robustness on sparse and truncated Doppler data while maintaining independence from explicit boundary conditions. Overall, our results highlight the effectiveness
    
[^4]: 线性马尔可夫决策过程的离线分布鲁棒性样本复杂度

    Sample Complexity of Offline Distributionally Robust Linear Markov Decision Processes

    [https://arxiv.org/abs/2403.12946](https://arxiv.org/abs/2403.12946)

    本文研究了离线强化学习中线性马尔可夫决策过程的分布鲁棒性样本复杂度问题，提出了一种悲观模型算法并建立了其样本复杂性界限，能在高维状态-动作空间中提高学习策略的性能。

    

    在离线强化学习（RL）中，缺乏积极探索需要关注模型的鲁棒性，以解决模拟和部署环境之间的差距，其中模拟和实际环境之间的差异可能严重损害学习策略的性能。为了以样本高效的方式赋予学习策略在高维状态-动作空间中的鲁棒性，本文考虑使用离线数据，通过总变差距离表征的不确定性集合，分布鲁棒线性马尔可夫决策过程（MDPs）的样本复杂性。我们开发了一种悲观模型算法，并在最小数据覆盖假设下建立了其样本复杂性界限，其性能至少比以前的方法优于$\tilde{O}(d)$，其中$d$是特征维度。

    arXiv:2403.12946v1 Announce Type: new  Abstract: In offline reinforcement learning (RL), the absence of active exploration calls for attention on the model robustness to tackle the sim-to-real gap, where the discrepancy between the simulated and deployed environments can significantly undermine the performance of the learned policy. To endow the learned policy with robustness in a sample-efficient manner in the presence of high-dimensional state-action space, this paper considers the sample complexity of distributionally robust linear Markov decision processes (MDPs) with an uncertainty set characterized by the total variation distance using offline data. We develop a pessimistic model-based algorithm and establish its sample complexity bound under minimal data coverage assumptions, which outperforms prior art by at least $\tilde{O}(d)$, where $d$ is the feature dimension. We further improve the performance guarantee of the proposed algorithm by incorporating a carefully-designed varia
    
[^5]: 基于去中心化随机次梯度法的非平滑非凸优化问题

    Decentralized Stochastic Subgradient Methods for Nonsmooth Nonconvex Optimization

    [https://arxiv.org/abs/2403.11565](https://arxiv.org/abs/2403.11565)

    该论文介绍了一种名为DSM的统一框架，用于分析去中心化随机次梯度方法的全局收敛性，证明了在温和条件下的全局收敛性，并展示其涵盖了各种现有高效的去中心化次梯度方法。

    

    在这篇论文中，我们关注具有非凸和非平滑目标函数的去中心化优化问题，特别是关注非平滑神经网络的去中心化训练。我们提出了一个统一的框架，称为DSM，用于分析去中心化随机次梯度法的全局收敛性。我们证明了在温和条件下，我们提出的框架的全局收敛性，通过建立生成序列渐近逼近其关联微分包含的轨迹。此外，我们证明了我们提出的框架涵盖了各种现有高效的去中心化次梯度方法，包括去中心化随机次梯度下降（DSGD），具有梯度跟踪技术的DSGD（DSGD-T）和带动量的DSGD（DSGDm）。此外，我们引入SignSGD，采用符号映射来正则化DSGDm中的更新方向，并表明它被包含在我们的提议中。

    arXiv:2403.11565v1 Announce Type: cross  Abstract: In this paper, we concentrate on decentralized optimization problems with nonconvex and nonsmooth objective functions, especially on the decentralized training of nonsmooth neural networks. We introduce a unified framework, named DSM, to analyze the global convergence of decentralized stochastic subgradient methods. We prove the global convergence of our proposed framework under mild conditions, by establishing that the generated sequence asymptotically approximates the trajectories of its associated differential inclusion. Furthermore, we establish that our proposed framework encompasses a wide range of existing efficient decentralized subgradient methods, including decentralized stochastic subgradient descent (DSGD), DSGD with gradient-tracking technique (DSGD-T), and DSGD with momentum (DSGDm). In addition, we introduce SignSGD employing the sign map to regularize the update directions in DSGDm, and show it is enclosed in our propos
    
[^6]: JAXbind: 将任何函数绑定到JAX

    JAXbind: Bind any function to JAX

    [https://arxiv.org/abs/2403.08847](https://arxiv.org/abs/2403.08847)

    JAXbind旨在大幅减少将其他编程语言中实现的自定义函数绑定到JAX所需的工作量，提供易于使用的Python接口定义自定义JAX原语。

    

    JAX被广泛应用在机器学习和科学计算中，后者经常依赖于现有的高性能代码，我们希望能够将其纳入JAX中。重新在JAX中实现现有代码通常是不现实的，而JAX中用于绑定自定义代码的现有接口需要对JAX及其C++后端有深入了解。JAXbind的目标是大大减少将其他编程语言中实现的自定义函数绑定到JAX所需的工作量。具体而言，JAXbind提供了一个易于使用的Python接口，用于定义支持任意JAX转换的自定义所谓JAX原语。

    arXiv:2403.08847v1 Announce Type: cross  Abstract: JAX is widely used in machine learning and scientific computing, the latter of which often relies on existing high-performance code that we would ideally like to incorporate into JAX. Reimplementing the existing code in JAX is often impractical and the existing interface in JAX for binding custom code requires deep knowledge of JAX and its C++ backend. The goal of JAXbind is to drastically reduce the effort required to bind custom functions implemented in other programming languages to JAX. Specifically, JAXbind provides an easy-to-use Python interface for defining custom so-called JAX primitives that support arbitrary JAX transformations.
    
[^7]: 温度计：面向大型语言模型的通用校准

    Thermometer: Towards Universal Calibration for Large Language Models

    [https://arxiv.org/abs/2403.08819](https://arxiv.org/abs/2403.08819)

    提出了一种针对大型语言模型的校准方法THERMOMETER，通过学习来自多个任务数据的辅助模型，实现了计算效率高、准确性保持并产生更好校准响应的目标。

    

    我们考虑大型语言模型（LLM）中的校准问题。最近的研究发现，常见的干预措施如指令调整通常会导致校准不佳的LLMs。尽管校准在传统应用中得到了很好的探讨，但对LLMs进行校准具有独特挑战。这些挑战不仅来自LLMs的严格计算要求，也来自它们的多功能性，使它们可以应用于各种任务。为了解决这些挑战，我们提出了一个针对LLMs的校准方法THERMOMETER。THERMOMETER通过学习来自多个任务的数据的辅助模型，用于校准LLM。它在计算上效率高，保持了LLM的准确性，并为新任务产生了更好的校准响应。对各种基准的广泛实证评估显示了所提方法的有效性。

    arXiv:2403.08819v1 Announce Type: cross  Abstract: We consider the issue of calibration in large language models (LLM). Recent studies have found that common interventions such as instruction tuning often result in poorly calibrated LLMs. Although calibration is well-explored in traditional applications, calibrating LLMs is uniquely challenging. These challenges stem as much from the severe computational requirements of LLMs as from their versatility, which allows them to be applied to diverse tasks. Addressing these challenges, we propose THERMOMETER, a calibration approach tailored to LLMs. THERMOMETER learns an auxiliary model, given data from multiple tasks, for calibrating a LLM. It is computationally efficient, preserves the accuracy of the LLM, and produces better-calibrated responses for new tasks. Extensive empirical evaluations across various benchmarks demonstrate the effectiveness of the proposed method.
    
[^8]: SoK：轨迹生成是否能够兼顾隐私和实用性？

    SoK: Can Trajectory Generation Combine Privacy and Utility?

    [https://arxiv.org/abs/2403.07218](https://arxiv.org/abs/2403.07218)

    本文提出了一个旨在设计保护隐私的轨迹发布方法的框架，特别强调了选择适当隐私单位的重要性。

    

    虽然位置轨迹代表着供各种分析和基于位置的服务的宝贵数据来源，但它们可能泄漏敏感信息，如政治和宗教偏好。已经提出了不同ially private发布机制，允许在严格的隐私保证下进行分析。然而，传统的保护方案存在隐私和实用性的权衡限制，并容易受到相关性和重构攻击的威胁。合成轨迹数据生成和发布代表了保护算法的一个具有前景的替代方案。虽然最初的提议取得了显著的实用性，但未能提供严格的隐私保证。本文提出了一个框架，通过定义五个设计目标，特别强调选择适当的隐私单位的重要性，来设计一个保护隐私的轨迹发布方法。基于这一框架，我们简要讨论了现有的轨迹发布方法。

    arXiv:2403.07218v1 Announce Type: cross  Abstract: While location trajectories represent a valuable data source for analyses and location-based services, they can reveal sensitive information, such as political and religious preferences. Differentially private publication mechanisms have been proposed to allow for analyses under rigorous privacy guarantees. However, the traditional protection schemes suffer from a limiting privacy-utility trade-off and are vulnerable to correlation and reconstruction attacks. Synthetic trajectory data generation and release represent a promising alternative to protection algorithms. While initial proposals achieve remarkable utility, they fail to provide rigorous privacy guarantees. This paper proposes a framework for designing a privacy-preserving trajectory publication approach by defining five design goals, particularly stressing the importance of choosing an appropriate Unit of Privacy. Based on this framework, we briefly discuss the existing traje
    
[^9]: 医学图像分割中的捷径学习

    Shortcut Learning in Medical Image Segmentation

    [https://arxiv.org/abs/2403.06748](https://arxiv.org/abs/2403.06748)

    本研究将捷径学习现象扩展到医学图像分割领域，发现临床注释和特定数据处理方式可能误导模型并影响分割准确性，提出了缓解捷径学习影响的策略。

    

    捷径学习是一种现象，机器学习模型优先学习简单、潜在误导的数据提示，这些提示在训练集之外很难泛化。现有研究主要探讨这一现象在图像分类领域，而这项研究将捷径学习探索延伸到医学图像分割中。我们证明，临床注释如卡尺，以及数据集中零填充卷积和中心裁剪的组合可以无意中作为捷径，影响分割准确性。我们在两个不同但常见的医学图像分割任务中识别和评估了捷径学习。此外，我们提出了缓解捷径学习影响、提高分割模型泛化能力的策略。通过揭示医学图像分割中捷径的存在和影响，我们提供了一些见解。

    arXiv:2403.06748v1 Announce Type: cross  Abstract: Shortcut learning is a phenomenon where machine learning models prioritize learning simple, potentially misleading cues from data that do not generalize well beyond the training set. While existing research primarily investigates this in the realm of image classification, this study extends the exploration of shortcut learning into medical image segmentation. We demonstrate that clinical annotations such as calipers, and the combination of zero-padded convolutions and center-cropped training sets in the dataset can inadvertently serve as shortcuts, impacting segmentation accuracy. We identify and evaluate the shortcut learning on two different but common medical image segmentation tasks. In addition, we suggest strategies to mitigate the influence of shortcut learning and improve the generalizability of the segmentation models. By uncovering the presence and implications of shortcuts in medical image segmentation, we provide insights a
    
[^10]: 解读AI笔: 检测AI生成文本的技术与挑战

    Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text

    [https://arxiv.org/abs/2403.05750](https://arxiv.org/abs/2403.05750)

    大型语言模型在自然语言生成领域取得了重大突破，提出了识别AI生成文本的解决方案，并探索了未来研究方向。

    

    大型语言模型(LLMs)通过展示生成类人文本的惊人能力，彻底颠覆了自然语言生成(NLG)领域。然而，它们广泛的应用带来挑战，需要深入审查、伦理审查和负责任的实践。本研究探讨了这些挑战，探索了现有的缓解策略，重点是识别AI生成文本作为最终解决方案。此外，我们从理论角度评估了检测的可行性，并提出了解决当前领域限制的新颖研究方向。

    arXiv:2403.05750v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have revolutionized the field of Natural Language Generation (NLG) by demonstrating an impressive ability to generate human-like text. However, their widespread usage introduces challenges that necessitate thoughtful examination, ethical scrutiny, and responsible practices. In this study, we delve into these challenges, explore existing strategies for mitigating them, with a particular emphasis on identifying AI-generated text as the ultimate solution. Additionally, we assess the feasibility of detection from a theoretical perspective and propose novel research directions to address the current limitations in this domain.
    
[^11]: 用混合变分家族改进从不完整数据估计的变分自动编码器

    Improving Variational Autoencoder Estimation from Incomplete Data with Mixture Variational Families

    [https://arxiv.org/abs/2403.03069](https://arxiv.org/abs/2403.03069)

    缺失数据增加了模型对潜在变量后验分布的复杂性，本文提出了两种策略——基于有限变分混合和基于填补的变分混合分布，有效改善了从不完整数据估计VAE的准确性。

    

    我们考虑了在训练数据不完整的情况下估计变分自动编码器（VAEs）的任务。我们证明了缺失数据会增加模型对潜在变量的后验分布的复杂性，与完全观测的情况相比。增加的复杂性可能会由于变分分布和模型后验分布之间的不匹配而对模型拟合产生不利影响。我们引入了两种基于（i）有限变分混合和（ii）基于填补的变分混合分布的策略，以解决增加的后验复杂性。通过对所提出方法的全面评估，我们表明变分混合在改进从不完整数据估计VAE的准确性方面是有效的。

    arXiv:2403.03069v1 Announce Type: new  Abstract: We consider the task of estimating variational autoencoders (VAEs) when the training data is incomplete. We show that missing data increases the complexity of the model's posterior distribution over the latent variables compared to the fully-observed case. The increased complexity may adversely affect the fit of the model due to a mismatch between the variational and model posterior distributions. We introduce two strategies based on (i) finite variational-mixture and (ii) imputation-based variational-mixture distributions to address the increased posterior complexity. Through a comprehensive evaluation of the proposed approaches, we show that variational mixtures are effective at improving the accuracy of VAE estimation from incomplete data.
    
[^12]: 乐观信息导向采样

    Optimisic Information Directed Sampling

    [https://arxiv.org/abs/2402.15411](https://arxiv.org/abs/2402.15411)

    提出了一种名为乐观信息导向采样的算法模板, 结合了贝叶斯理论和最坏情形理论，能够实现类似于贝叶斯方法的实例相关遗憾保证，但无需贝叶斯假设。

    

    我们研究在上下文强盗问题中的在线学习问题，其中损失函数被假定属于已知的参数函数类。我们提出了一个新的分析框架，它在贝叶斯理论和基于决策估计系数的最坏情形理论之间架起了桥梁。汲取这两方面的工作，我们提出了一种名为乐观信息导向采样的算法模板，并展示它能够实现类似于经典贝叶斯IDS方法可实现的实例相关遗憾保证，但却无需任何贝叶斯假设。我们分析的关键技术创新是引入了一个乐观的替代模型用于遗憾，并将其用于定义一个基于频率的基于Russo和Van Roy (2018)的信息比，以及一个...

    arXiv:2402.15411v1 Announce Type: new  Abstract: We study the problem of online learning in contextual bandit problems where the loss function is assumed to belong to a known parametric function class. We propose a new analytic framework for this setting that bridges the Bayesian theory of information-directed sampling due to Russo and Van Roy (2018) and the worst-case theory of Foster, Kakade, Qian, and Rakhlin (2021) based on the decision-estimation coefficient. Drawing from both lines of work, we propose a algorithmic template called Optimistic Information-Directed Sampling and show that it can achieve instance-dependent regret guarantees similar to the ones achievable by the classic Bayesian IDS method, but with the major advantage of not requiring any Bayesian assumptions. The key technical innovation of our analysis is introducing an optimistic surrogate model for the regret and using it to define a frequentist version of the Information Ratio of Russo and Van Roy (2018), and a l
    
[^13]: MobileLLM：优化亚十亿参数语言模型以用于设备端应用

    MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases

    [https://arxiv.org/abs/2402.14905](https://arxiv.org/abs/2402.14905)

    MobileLLM通过优化模型架构，采用深度和瘦身结构、嵌入共享和分组查询注意机制，实现了2.7%/4.3%的准确率提升，并提出了一种无需增加模型大小且仅有极小延迟开销的块状权重共享方法

    

    本文解决了移动设备上高效的大型语言模型(LLMs)的迫切需求问题，这是由于云成本和延迟问题不断增加所导致的。我们专注于设计具有不到十亿参数的顶级LLMs，这是移动部署的实际选择。与普遍的观点相反，强调数据和参数数量在确定模型质量方面的关键作用，我们的研究强调了亚十亿规模LLMs的模型架构的重要性。利用深度和瘦身结构，再加上嵌入共享和分组查询注意机制，我们建立了一个强大的基准网络，称为MobileLLM，其在将近125M/350M先进模型上分别获得了惊人的2.7%/4.3%的准确率提升。此外，我们提出了一种立即的块状权重共享方法，不增加模型大小，且仅具有极小的延迟开销。由此产生的模型被命名为MobileLLM-L

    arXiv:2402.14905v1 Announce Type: cross  Abstract: This paper addresses the growing need for efficient large language models (LLMs) on mobile devices, driven by increasing cloud costs and latency concerns. We focus on designing top-quality LLMs with fewer than a billion parameters, a practical choice for mobile deployment. Contrary to prevailing belief emphasizing the pivotal role of data and parameter quantity in determining model quality, our investigation underscores the significance of model architecture for sub-billion scale LLMs. Leveraging deep and thin architectures, coupled with embedding sharing and grouped-query attention mechanisms, we establish a strong baseline network denoted as MobileLLM, which attains a remarkable 2.7%/4.3% accuracy boost over preceding 125M/350M state-of-the-art models. Additionally, we propose an immediate block-wise weight sharing approach with no increase in model size and only marginal latency overhead. The resultant models, denoted as MobileLLM-L
    
[^14]: 利用Koopman算子提取神经网络中的非线性并进行模型压缩

    Extraction of nonlinearity in neural networks and model compression with Koopman operator

    [https://arxiv.org/abs/2402.11740](https://arxiv.org/abs/2402.11740)

    本论文使用Koopman算子提取神经网络中的非线性，证明了受限非线性已足够进行手写数字分类，并提出了一种模型压缩方法，能在资源受限环境中处理大型网络。

    

    非线性在深度神经网络中起着至关重要的作用。在本文中，我们首先调查了神经网络的非线性对分类任务的关键程度。为此，我们采用Koopman算子、扩展动态模态分解和张量列车格式。结果表明，受限非线性已经足以进行手写数字的分类。接着，我们提出了一种用于深度神经网络的模型压缩方法，这对于在资源受限的环境中处理大型网络可能是有益的。利用Koopman算子，所提出的方法使我们可以在神经网络的内部处理中使用线性代数。数值结果表明，所提出的方法在高度压缩模型设置下在手写数字识别任务中的性能要么与传统方法相当，要么更好。

    arXiv:2402.11740v1 Announce Type: new  Abstract: Nonlinearity plays a crucial role in deep neural networks. In this paper, we first investigate the degree to which the nonlinearity of the neural network is essential. For this purpose, we employ the Koopman operator, extended dynamic mode decomposition, and the tensor-train format. The results imply that restricted nonlinearity is enough for the classification of handwritten numbers. Then, we propose a model compression method for deep neural networks, which could be beneficial to handling large networks in resource-constrained environments. Leveraging the Koopman operator, the proposed method enables us to use linear algebra in the internal processing of neural networks. We numerically show that the proposed method performs comparably or better than conventional methods in highly compressed model settings for the handwritten number recognition task.
    
[^15]: 随机凸优化中适应性的代价

    The Price of Adaptivity in Stochastic Convex Optimization

    [https://arxiv.org/abs/2402.10898](https://arxiv.org/abs/2402.10898)

    该论文证明了在非光滑随机凸优化中，适应性的代价是无法避免的，并且给出了关于不确定性参数的次优性乘法增加的下界。

    

    我们证明了在非光滑随机凸优化中适应性的不可能性结果。给定一组我们希望适应的问题参数，我们定义了“适应性的代价”（PoA），粗略地说，它衡量了由于这些参数的不确定性而导致的次优性的乘法增加。当初始距离最优解未知但梯度范数有界时，我们证明PoA至少对于期望次优性是对数级别，对于中位数次优性是双对数级别。当距离和梯度范数都存在不确定性时，我们表明PoA必须是与不确定性水平多项式相关的。我们的下界几乎与现有的上界相匹配，并且确定了没有无参数午餐的结论。

    arXiv:2402.10898v1 Announce Type: cross  Abstract: We prove impossibility results for adaptivity in non-smooth stochastic convex optimization. Given a set of problem parameters we wish to adapt to, we define a "price of adaptivity" (PoA) that, roughly speaking, measures the multiplicative increase in suboptimality due to uncertainty in these parameters. When the initial distance to the optimum is unknown but a gradient norm bound is known, we show that the PoA is at least logarithmic for expected suboptimality, and double-logarithmic for median suboptimality. When there is uncertainty in both distance and gradient norm, we show that the PoA must be polynomial in the level of uncertainty. Our lower bounds nearly match existing upper bounds, and establish that there is no parameter-free lunch.
    
[^16]: 数据重构攻击与防御：一个系统评估

    Data Reconstruction Attacks and Defenses: A Systematic Evaluation

    [https://arxiv.org/abs/2402.09478](https://arxiv.org/abs/2402.09478)

    本研究提出了一种在联合学习环境中的强力重构攻击，可以重构中间特征，并且对大部分先前的方法表现更好。实证研究表明，在防御机制中，梯度修剪是对抗最先进攻击最有效的策略。

    

    重构攻击和防御对于理解机器学习中的数据泄漏问题至关重要。然而，先前的工作主要集中在梯度反转攻击的经验观察上，缺乏理论基础，并且无法区分防御方法的有用性与攻击方法的计算限制。在这项工作中，我们提出了一种在联合学习环境中的强力重构攻击。该攻击可以重构中间特征，并与大部分先前的方法相比表现更好。在这种更强的攻击下，我们从理论和实证两方面全面调查了最常见的防御方法的效果。我们的研究结果表明，在各种防御机制中，如梯度剪辑、dropout、添加噪音、局部聚合等等，梯度修剪是对抗最先进攻击最有效的策略。

    arXiv:2402.09478v1 Announce Type: cross  Abstract: Reconstruction attacks and defenses are essential in understanding the data leakage problem in machine learning. However, prior work has centered around empirical observations of gradient inversion attacks, lacks theoretical groundings, and was unable to disentangle the usefulness of defending methods versus the computational limitation of attacking methods. In this work, we propose a strong reconstruction attack in the setting of federated learning. The attack reconstructs intermediate features and nicely integrates with and outperforms most of the previous methods. On this stronger attack, we thoroughly investigate both theoretically and empirically the effect of the most common defense methods. Our findings suggest that among various defense mechanisms, such as gradient clipping, dropout, additive noise, local aggregation, etc., gradient pruning emerges as the most effective strategy to defend against state-of-the-art attacks.
    
[^17]: 改进心肌梗死检测：一种新颖的多模态复合核策略在单一类别分类中的应用

    Refining Myocardial Infarction Detection: A Novel Multi-Modal Composite Kernel Strategy in One-Class Classification

    [https://arxiv.org/abs/2402.06530](https://arxiv.org/abs/2402.06530)

    本研究提出了一种新的方法，使用基于超声心动图的一种基于多模态复合核策略的单一类别分类算法来进行早期心肌梗死的检测。这种方法通过优化投影矩阵和特征转化，提高了心肌梗死检测的能力。

    

    早期心肌梗死（MI）的检测对于预防进一步心肌损伤非常重要，MI是由冠状动脉疾病（CAD）引起的一种严重疾病。本研究引入了一种新方法，使用一种基于超声心动图的单一类别分类（OCC）算法进行早期MI检测。我们的研究通过采用基于多模态子空间支持向量数据描述的新方法克服了超声心动图数据有限的挑战。提出的技术涉及一种特殊的MI检测框架，使用复合核在非线性投影技巧中融合高斯和拉普拉斯sigmoid函数，将多视图超声心动图结合起来。此外，我们通过在优化过程中调整投影矩阵的最大化策略，提高了投影矩阵更新策略的效果。我们的方法通过将从超声心动图数据中提取的特征有效地转化为优化的低维空间，增强了MI检测的能力。

    Early detection of myocardial infarction (MI), a critical condition arising from coronary artery disease (CAD), is vital to prevent further myocardial damage. This study introduces a novel method for early MI detection using a one-class classification (OCC) algorithm in echocardiography. Our study overcomes the challenge of limited echocardiography data availability by adopting a novel approach based on Multi-modal Subspace Support Vector Data Description. The proposed technique involves a specialized MI detection framework employing multi-view echocardiography incorporating a composite kernel in the non-linear projection trick, fusing Gaussian and Laplacian sigmoid functions. Additionally, we enhance the update strategy of the projection matrices by adapting maximization for both or one of the modalities in the optimization process. Our method boosts MI detection capability by efficiently transforming features extracted from echocardiography data into an optimized lower-dimensional su
    
[^18]: 通过修剪和低秩修改评估安全对齐的易碎性

    Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications

    [https://arxiv.org/abs/2402.05162](https://arxiv.org/abs/2402.05162)

    本研究通过修剪和低秩修改，发现大型语言模型（LLMs）的安全机制固有易碎性，去除安全关键区域会损害安全性，但对效用影响不大，需要更强健的安全策略。

    

    大型语言模型（LLMs）在其安全机制方面表现出固有的易碎性，这可从它们易受越狱和即使是非恶意微调也易受影响来说明。本研究通过利用修剪和低秩修改探讨了安全对齐的易碎性。我们开发了方法，能够识别对于安全防护至关重要，且在神经元和秩级别上与效用相关的区域。令人惊讶的是，我们发现的孤立区域是稀疏的，约占参数级别的$3\%$和排名级别的$2.5\%$。去除这些区域会损害安全性，而对效用的影响不大，从而证实了该模型安全机制的固有易碎性。此外，我们还表明，即使限制对安全关键区域进行修改，LLMs仍然容易受到低成本的微调攻击。这些发现强调了在LLMs中更强大的安全策略的紧迫性需求。

    Large language models (LLMs) show inherent brittleness in their safety mechanisms, as evidenced by their susceptibility to jailbreaking and even non-malicious fine-tuning. This study explores this brittleness of safety alignment by leveraging pruning and low-rank modifications. We develop methods to identify critical regions that are vital for safety guardrails, and that are disentangled from utility-relevant regions at both the neuron and rank levels. Surprisingly, the isolated regions we find are sparse, comprising about $3\%$ at the parameter level and $2.5\%$ at the rank level. Removing these regions compromises safety without significantly impacting utility, corroborating the inherent brittleness of the model's safety mechanisms. Moreover, we show that LLMs remain vulnerable to low-cost fine-tuning attacks even when modifications to the safety-critical regions are restricted. These findings underscore the urgent need for more robust safety strategies in LLMs.
    
[^19]: 无源域自适应的扩散引导源数据生成

    Source-Free Domain Adaptation with Diffusion-Guided Source Data Generation

    [https://arxiv.org/abs/2402.04929](https://arxiv.org/abs/2402.04929)

    本文提出了一种无源域自适应的新方法，利用扩散模型生成上下文相关的领域特定图像，通过微调预训练模型和无监督领域自适应技术实现了显著的性能改进。

    

    本文引入了一种利用扩散模型的泛化能力进行无源域自适应（DM-SFDA）的新方法。我们提出的DM-SFDA方法包括对预训练的文本到图像扩散模型进行微调，并使用目标图像的特征来指导扩散过程生成源域图像。具体而言，预训练的扩散模型被微调以生成最小化熵并最大化预训练源模型置信度的源样本。然后，我们应用已建立的无监督领域自适应技术将生成的源图像与目标域数据进行对齐。我们通过在一系列数据集上进行全面实验验证了我们的方法，包括Office-31、Office-Home和VisDA。结果显示，在无源域自适应的性能方面取得了显著的改进，展示了扩散模型在生成上下文相关的、领域特定的图像方面的潜力。

    This paper introduces a novel approach to leverage the generalizability capability of Diffusion Models for Source-Free Domain Adaptation (DM-SFDA). Our proposed DM-SFDA method involves fine-tuning a pre-trained text-to-image diffusion model to generate source domain images using features from the target images to guide the diffusion process. Specifically, the pre-trained diffusion model is fine-tuned to generate source samples that minimize entropy and maximize confidence for the pre-trained source model. We then apply established unsupervised domain adaptation techniques to align the generated source images with target domain data. We validate our approach through comprehensive experiments across a range of datasets, including Office-31, Office-Home, and VisDA. The results highlight significant improvements in SFDA performance, showcasing the potential of diffusion models in generating contextually relevant, domain-specific images.
    
[^20]: AdaTreeFormer: 从一张高分辨率图像中进行树木计数的少样本领域自适应

    AdaTreeFormer: Few Shot Domain Adaptation for Tree Counting from a Single High-Resolution Image

    [https://arxiv.org/abs/2402.02956](https://arxiv.org/abs/2402.02956)

    AdaTreeFormer是一种从源领域学习并适应只有有限数量标注树木的目标领域的框架，利用一个共享的编码器和分层特征提取方案，实现了树木计数的少样本领域自适应。

    

    仅使用一张航空或卫星图像来估计和计数树木密度是摄影测量和遥感领域中一项困难的任务。然而，它在森林管理中起着至关重要的作用。不同地形上各种各样的树木种类严重阻碍了树木计数模型的良好表现。本文旨在提出一个从具有足够标注树木的源领域学习并适应只有有限数量标注树木的目标领域的框架。我们的方法称为AdaTreeFormer，包含一个共享的编码器和一个分层特征提取方案，用于从源领域和目标领域中提取稳健的特征。它还包括三个子网络：两个用于分别从源领域和目标领域提取自注意力图，并一个用于提取跨领域注意力图。对于后者，引入了一种注意力适应机制，用于从不同领域中提取相关信息。

    The process of estimating and counting tree density using only a single aerial or satellite image is a difficult task in the fields of photogrammetry and remote sensing. However, it plays a crucial role in the management of forests. The huge variety of trees in varied topography severely hinders tree counting models to perform well. The purpose of this paper is to propose a framework that is learnt from the source domain with sufficient labeled trees and is adapted to the target domain with only a limited number of labeled trees. Our method, termed as AdaTreeFormer, contains one shared encoder with a hierarchical feature extraction scheme to extract robust features from the source and target domains. It also consists of three subnets: two for extracting self-domain attention maps from source and target domains respectively and one for extracting cross-domain attention maps. For the latter, an attention-to-adapt mechanism is introduced to distill relevant information from different doma
    
[^21]: ECG诊断中的迁移学习：有效吗？

    Transfer Learning in ECG Diagnosis: Is It Effective?

    [https://arxiv.org/abs/2402.02021](https://arxiv.org/abs/2402.02021)

    本研究首次对心电图诊断中的迁移学习进行了广泛的经验研究，发现微调对于小型数据集是较好的选择，当数据集足够大时，从头开始训练可以达到可比性能，但需要更长的训练时间。同时，迁移学习与卷积神经网络具有更好的兼容性。

    

    在真实世界的场景中，深度学习在心电图诊断中的应用往往受到大规模、标记良好的数据集的稀缺性的限制，因此使用迁移学习来利用从更大的数据集中学到的特征。然而，关于迁移学习始终优于从头开始训练的普遍假设从未被系统验证过。在本研究中，我们通过对多标签心电图分类中进行微调与从头开始训练的性能进行比较，涵盖了各种心电图数据集和深度神经网络，进行了第一次广泛的经验性研究来验证迁移学习的有效性。我们证实，对于小型的下游数据集来说，微调是更好的选择；然而，当数据集足够大时，从头开始训练可以达到可比性能，尽管需要更长的训练时间来迎头赶上。此外，我们发现，迁移学习与卷积神经网络更好的兼容性。

    The adoption of deep learning in ECG diagnosis is often hindered by the scarcity of large, well-labeled datasets in real-world scenarios, leading to the use of transfer learning to leverage features learned from larger datasets. Yet the prevailing assumption that transfer learning consistently outperforms training from scratch has never been systematically validated. In this study, we conduct the first extensive empirical study on the effectiveness of transfer learning in multi-label ECG classification, by investigating comparing the fine-tuning performance with that of training from scratch, covering a variety of ECG datasets and deep neural networks. We confirm that fine-tuning is the preferable choice for small downstream datasets; however, when the dataset is sufficiently large, training from scratch can achieve comparable performance, albeit requiring a longer training time to catch up. Furthermore, we find that transfer learning exhibits better compatibility with convolutional ne
    
[^22]: $\mu$GUIDE:一种通过深度学习使用广义不确定性驱动的推断来进行微结构成像的框架

    $\mu$GUIDE: a framework for microstructure imaging via generalized uncertainty-driven inference using deep learning

    [https://arxiv.org/abs/2312.17293](https://arxiv.org/abs/2312.17293)

    $\mu$GUIDE是一个通用的贝叶斯框架，利用深度学习进行微结构成像，能够有效估计组织微结构参数的后验分布，并量化参数的不确定性和模糊性。

    

    本文提出了$\mu$GUIDE:一种通用的贝叶斯框架，用于从任何给定的生物物理模型或MRI信号表示中估计组织微结构参数的后验分布，通过扩散加权MRI的示例演示。利用一种新的深度学习架构，用于自动信号特征选择，结合基于模拟的推断和后验分布的高效采样，$\mu$GUIDE绕过了传统贝叶斯方法的高计算和时间成本，并且不依赖采集约束来定义模型特定的摘要统计量。获得的后验分布可突出显示模型定义中存在的退化性，并量化所估计参数的不确定性和模糊性。

    This work proposes $\mu$GUIDE: a general Bayesian framework to estimate posterior distributions of tissue microstructure parameters from any given biophysical model or MRI signal representation, with exemplar demonstration in diffusion-weighted MRI. Harnessing a new deep learning architecture for automatic signal feature selection combined with simulation-based inference and efficient sampling of the posterior distributions, $\mu$GUIDE bypasses the high computational and time cost of conventional Bayesian approaches and does not rely on acquisition constraints to define model-specific summary statistics. The obtained posterior distributions allow to highlight degeneracies present in the model definition and quantify the uncertainty and ambiguity of the estimated parameters.
    
[^23]: 在多任务学习中挑战常见范式

    Challenging Common Paradigms in Multi-Task Learning

    [https://arxiv.org/abs/2311.04698](https://arxiv.org/abs/2311.04698)

    我们挑战了多任务学习中的常见范式，通过研究在单任务学习中的影响，揭示了优化器选择在MTL中的关键作用，并理论推导出了梯度冲突的角色。

    

    尽管近年来多任务学习（MTL）受到了极大关注，但其基本机制仍然知之甚少。最近的方法并未带来一致的性能改进，相比单任务学习（STL）基线，强调了更深入了解MTL特定挑战的重要性。在我们的研究中，我们挑战了MTL中的范式，提出了几点关于STL的重要影响：首先，优化器的选择对MTL的影响只受到了轻微的调查。我们通过各种实验的实证方法展示了常见STL工具（例如Adam优化器）在MTL中的关键作用。为了进一步研究Adam的有效性，我们在一定的假设下从理论上推导出部分损失尺度不变性。其次，梯度冲突的概念经常被描述为MTL中的一个特定问题。我们深入探讨了梯度冲突在MTL中的作用，并将其与STL进行比较。在角度梯度对齐方面，我们没有找到

    arXiv:2311.04698v3 Announce Type: replace-cross  Abstract: While multi-task learning (MTL) has gained significant attention in recent years, its underlying mechanisms remain poorly understood. Recent methods did not yield consistent performance improvements over single task learning (STL) baselines, underscoring the importance of gaining more profound insights about challenges specific to MTL. In our study, we challenge paradigms in MTL in the context of STL: First, the impact of the choice of optimizer has only been mildly investigated in MTL. We show the pivotal role of common STL tools such as the Adam optimizer in MTL empirically in various experiments. To further investigate Adam's effectiveness, we theoretical derive a partial loss-scale invariance under mild assumptions. Second, the notion of gradient conflicts has often been phrased as a specific problem in MTL. We delve into the role of gradient conflicts in MTL and compare it to STL. For angular gradient alignment we find no 
    
[^24]: 用深度学习和降阶建模进行贝叶斯非分离哈密顿系统的识别和多项式噪声 (arXiv:2401.12476v1 [stat.ML])

    Bayesian identification of nonseparable Hamiltonians with multiplicative noise using deep learning and reduced-order modeling. (arXiv:2401.12476v1 [stat.ML])

    [http://arxiv.org/abs/2401.12476](http://arxiv.org/abs/2401.12476)

    本文提出了一种用于学习非分离哈密顿系统的结构保持的贝叶斯方法，可以处理统计相关的加性和乘性噪声，并且通过将结构保持方法纳入框架中，提供了对高维系统的高效识别。

    

    本文提出了一种结构保持的贝叶斯方法，用于学习使用随机动力模型的非分离哈密顿系统，该系统允许统计相关的，矢量值的加性和乘性测量噪声。该方法由三个主要方面组成。首先，我们推导了一个用于评估贝叶斯后验中的似然函数所需的统计相关的，矢量值的加性和乘性噪声模型的高斯滤波器。其次，我们开发了一种新算法，用于对高维系统进行高效的贝叶斯系统识别。第三，我们演示了如何将结构保持方法纳入所提议的框架中，使用非分离哈密顿系统作为一个举例的系统类别。我们将贝叶斯方法与一种最先进的机器学习方法在一个典型的非分离哈密顿模型和带有小型噪声训练数据集的混沌双摆模型上进行了比较，实验结果表明

    This paper presents a structure-preserving Bayesian approach for learning nonseparable Hamiltonian systems using stochastic dynamic models allowing for statistically-dependent, vector-valued additive and multiplicative measurement noise. The approach is comprised of three main facets. First, we derive a Gaussian filter for a statistically-dependent, vector-valued, additive and multiplicative noise model that is needed to evaluate the likelihood within the Bayesian posterior. Second, we develop a novel algorithm for cost-effective application of Bayesian system identification to high-dimensional systems. Third, we demonstrate how structure-preserving methods can be incorporated into the proposed framework, using nonseparable Hamiltonians as an illustrative system class. We compare the Bayesian method to a state-of-the-art machine learning method on a canonical nonseparable Hamiltonian model and a chaotic double pendulum model with small, noisy training datasets. The results show that us
    
[^25]: 超越反遗忘: 带有正向传递的多模态连续指导调优

    Beyond Anti-Forgetting: Multimodal Continual Instruction Tuning with Positive Forward Transfer. (arXiv:2401.09181v1 [cs.LG])

    [http://arxiv.org/abs/2401.09181](http://arxiv.org/abs/2401.09181)

    本研究提出了一种名为Fwd-Prompt的方法，通过对输入嵌入进行奇异值分解，并在残差空间和预训练子空间中进行梯度投影，以解决多模态连续指导调优中的灾难性遗忘和负面的正向传递问题。

    

    多模态连续指导调优（MCIT）使得多模态大型语言模型（MLLMs）可以满足不断出现的需求，而无需昂贵的重新训练。MCIT面临两个主要障碍：灾难性遗忘（旧知识被遗忘）和负面的正向传递（未来任务的性能下降）。虽然现有方法大大缓解了灾难性遗忘，但仍然遭受负面的正向传递。通过对输入嵌入进行奇异值分解（SVD），我们发现不同输入嵌入之间存在很大差异。这种差异导致模型学习与旧的和预训练的任务无关的信息，从而导致灾难性遗忘和负面的正向传递。为了解决这些问题，我们提出了Fwd-Prompt，这是一种基于提示的方法，将提示梯度投影到残差空间中，以减小任务之间的干扰，并投影到预训练子空间中以重用预训练的知识。

    Multimodal Continual Instruction Tuning (MCIT) enables Multimodal Large Language Models (MLLMs) to meet continuously emerging requirements without expensive retraining. MCIT faces two major obstacles: catastrophic forgetting (where old knowledge is forgotten) and negative forward transfer (where the performance of future tasks is degraded). Although existing methods have greatly alleviated catastrophic forgetting, they still suffer from negative forward transfer. By performing singular value decomposition (SVD) on input embeddings, we discover a large discrepancy in different input embeddings. The discrepancy results in the model learning irrelevant information for old and pre-trained tasks, which leads to catastrophic forgetting and negative forward transfer. To address these issues, we propose Fwd-Prompt, a prompt-based method projecting prompt gradient to the residual space to minimize the interference between tasks and to the pre-trained subspace for reusing pre-trained knowledge. 
    
[^26]: 输入凸LSTM：一种快速基于Lyapunov模型预测控制的凸方法

    Input Convex LSTM: A Convex Approach for Fast Lyapunov-Based Model Predictive Control. (arXiv:2311.07202v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.07202](http://arxiv.org/abs/2311.07202)

    本研究提出了一种基于输入凸LSTM的基于Lyapunov的模型预测控制方法，通过减少收敛时间和缓解梯度消失/爆炸问题来改善MPC的性能。

    

    利用输入凸神经网络（ICNN），基于ICNN的模型预测控制（MPC）通过在MPC框架中保持凸性成功实现全局最优解。然而，当前的ICNN架构存在梯度消失/爆炸问题，限制了它们作为复杂任务的深度神经网络的能力。此外，当前基于神经网络的MPC，包括传统的基于神经网络的MPC和基于ICNN的MPC，与基于第一原理模型的MPC相比面临较慢的收敛速度。在本研究中，我们利用ICNN的原理提出了一种新的基于输入凸LSTM的基于Lyapunov的MPC，旨在减少收敛时间、缓解梯度消失/爆炸问题并确保闭环稳定性。通过对非线性化学反应器的模拟研究，我们观察到了梯度消失/爆炸问题的缓解和收敛时间的减少，收敛时间平均降低了一定的百分之。

    Leveraging Input Convex Neural Networks (ICNNs), ICNN-based Model Predictive Control (MPC) successfully attains globally optimal solutions by upholding convexity within the MPC framework. However, current ICNN architectures encounter the issue of vanishing/exploding gradients, which limits their ability to serve as deep neural networks for complex tasks. Additionally, the current neural network-based MPC, including conventional neural network-based MPC and ICNN-based MPC, faces slower convergence speed when compared to MPC based on first-principles models. In this study, we leverage the principles of ICNNs to propose a novel Input Convex LSTM for Lyapunov-based MPC, with the specific goal of reducing convergence time and mitigating the vanishing/exploding gradient problem while ensuring closed-loop stability. From a simulation study of a nonlinear chemical reactor, we observed a mitigation of vanishing/exploding gradient problem and a reduction in convergence time, with a percentage de
    
[^27]: 持续学习在语言转换中的研究

    A Study of Continual Learning Under Language Shift. (arXiv:2311.01200v1 [cs.CL])

    [http://arxiv.org/abs/2311.01200](http://arxiv.org/abs/2311.01200)

    本文研究了持续学习在语言转换中的应用，发现在更新语言模型时，前向转移效果较好且与语言顺序无关，但后向转移效果可能取决于新语言的顺序和特征。

    

    最近语言模型预训练的数据和模型规模的增加导致了巨大的训练成本。在随时间推移而出现新数据的情况下，更新模型而不是完全重新训练可以带来显著的收益。在本文中，我们研究了在新语言出现时更新语言模型时的好处和弊端，即在语言转换中持续学习的情况。从单语英语语言模型出发，我们逐步添加了来自挪威语和冰岛语的数据，以研究前向和后向转移效果如何取决于预训练顺序和语言特征，对于不同的模型大小和学习率调度器。我们的结果表明，尽管前向转移主要是正向的，不受语言顺序的影响，但后向转移则可能是正向的或负向的，具体取决于新语言的顺序和特征。为了解释这些模式，我们探索了几种语言相似度度量方法。

    The recent increase in data and model scale for language model pre-training has led to huge training costs. In scenarios where new data become available over time, updating a model instead of fully retraining it would therefore provide significant gains. In this paper, we study the benefits and downsides of updating a language model when new data comes from new languages - the case of continual learning under language shift. Starting from a monolingual English language model, we incrementally add data from Norwegian and Icelandic to investigate how forward and backward transfer effects depend on the pre-training order and characteristics of languages, for different model sizes and learning rate schedulers. Our results show that, while forward transfer is largely positive and independent of language order, backward transfer can be either positive or negative depending on the order and characteristics of new languages. To explain these patterns we explore several language similarity metr
    
[^28]: 图像建模的MixerFlow

    MixerFlow for Image Modelling. (arXiv:2310.16777v1 [stat.ML])

    [http://arxiv.org/abs/2310.16777](http://arxiv.org/abs/2310.16777)

    MixerFlow是一种新型的基于MLP-Mixer架构的正则化流模型，通过提供有效的权重共享机制，实现了更好的图像密度估计性能和更丰富的嵌入表示。

    

    正则化流是一种统计模型，通过使用双射变换将复杂密度转换为简单密度，实现了密度估计和从单个模型生成数据的功能。在图像建模的背景下，主要选择的是基于Glow的架构，而其他架构在研究界尚未得到广泛探索。在本研究中，我们提出了一种基于MLP-Mixer架构的新型架构MixerFlow，进一步统一了生成性和判别性建模架构。MixerFlow提供了一种有效的权重共享机制，适用于基于流的模型。我们的结果表明，在固定计算预算下，MixerFlow在图像数据集上具有更好的密度估计性能，并且随着图像分辨率的增加，其性能也得到了良好的扩展，使得MixerFlow成为Glow-based架构的一个强大而简单的替代品。我们还展示了MixerFlow提供了比Glow-based架构更丰富的嵌入表示。

    Normalising flows are statistical models that transform a complex density into a simpler density through the use of bijective transformations enabling both density estimation and data generation from a single model. In the context of image modelling, the predominant choice has been the Glow-based architecture, whereas alternative architectures remain largely unexplored in the research community. In this work, we propose a novel architecture called MixerFlow, based on the MLP-Mixer architecture, further unifying the generative and discriminative modelling architectures. MixerFlow offers an effective mechanism for weight sharing for flow-based models. Our results demonstrate better density estimation on image datasets under a fixed computational budget and scales well as the image resolution increases, making MixeFlow a powerful yet simple alternative to the Glow-based architectures. We also show that MixerFlow provides more informative embeddings than Glow-based architectures.
    
[^29]: 自适应的对向编码用于链路预测

    Adaptive Pairwise Encodings for Link Prediction. (arXiv:2310.11009v1 [cs.LG])

    [http://arxiv.org/abs/2310.11009](http://arxiv.org/abs/2310.11009)

    提出了一种自适应的对向编码方法，用于解决链路预测中现有方法的归纳偏差问题。该方法将消息传递神经网络和启发式方法结合起来，能够更好地分类各种不同因素形成的链路。

    

    链路预测是一种常见的基于图结构数据的任务，在各个领域都有应用。经典方法通常使用手工设计的启发式策略来进行预测。启发式度量被选择为在与链路形成相关的基本因素上与之相关良好。近年来，出现了一类新的方法，将消息传递神经网络（MPNN）的优势与启发式方法结合起来。这些方法通过使用MPNN的输出以及捕捉候选链路中节点之间关系的“对向编码”来进行预测。它们已经在许多数据集上表现出强大的性能。然而，目前的对向编码往往具有强烈的归纳偏差，使用相同的基本因素来分类所有链路。这限制了现有方法学习如何正确分类可能由不同因素形成的各种不同链路的能力。为了解决这个问题，我们提出了一个自适应的对向编码方法。

    Link prediction is a common task on graph-structured data that has seen applications in a variety of domains. Classically, hand-crafted heuristics were used for this task. Heuristic measures are chosen such that they correlate well with the underlying factors related to link formation. In recent years, a new class of methods has emerged that combines the advantages of message-passing neural networks (MPNN) and heuristics methods. These methods perform predictions by using the output of an MPNN in conjunction with a "pairwise encoding" that captures the relationship between nodes in the candidate link. They have been shown to achieve strong performance on numerous datasets. However, current pairwise encodings often contain a strong inductive bias, using the same underlying factors to classify all links. This limits the ability of existing methods to learn how to properly classify a variety of different links that may form from different factors. To address this limitation, we propose a 
    
[^30]: 分层概念发现模型：一个概念金字塔方案

    Hierarchical Concept Discovery Models: A Concept Pyramid Scheme. (arXiv:2310.02116v1 [cs.LG])

    [http://arxiv.org/abs/2310.02116](http://arxiv.org/abs/2310.02116)

    本论文提出了一种分层概念发现模型，通过利用图像文本模型和基于数据驱动的贝叶斯参数，实现了基于人类可理解概念的高度可解释的决策过程。

    

    最近，深度学习算法因其卓越的性能而引起了大量关注。然而，它们的高复杂性和不可解释的操作方式阻碍了它们在真实世界的安全关键任务中的自信部署。本研究针对的是ante hoc可解释性，具体 说是概念瓶颈模型（CBMs）。我们的目标是设计一个框架，以多个层次粒度上的人类可理解概念为基础，实现高度可解释的决策过程。为此，我们提出了一种新颖的分层概念发现方法，利用：（i）图像文本模型的最新进展，以及（ii）基于数据驱动和稀疏诱导的贝叶斯参数进行多层概念选择的创新公式。在这个框架中，概念信息不仅仅依赖于整体图像与一般非结构化概念之间的相似性；相反，我们引入了概念层次的概念，以揭示和利用更多的细节。

    Deep Learning algorithms have recently gained significant attention due to their impressive performance. However, their high complexity and un-interpretable mode of operation hinders their confident deployment in real-world safety-critical tasks. This work targets ante hoc interpretability, and specifically Concept Bottleneck Models (CBMs). Our goal is to design a framework that admits a highly interpretable decision making process with respect to human understandable concepts, on multiple levels of granularity. To this end, we propose a novel hierarchical concept discovery formulation leveraging: (i) recent advances in image-text models, and (ii) an innovative formulation for multi-level concept selection via data-driven and sparsity inducing Bayesian arguments. Within this framework, concept information does not solely rely on the similarity between the whole image and general unstructured concepts; instead, we introduce the notion of concept hierarchy to uncover and exploit more gra
    
[^31]: 基于地理气象数据的深度神经网络用于长期干旱预测

    Long-term drought prediction using deep neural networks based on geospatial weather data. (arXiv:2309.06212v1 [cs.LG])

    [http://arxiv.org/abs/2309.06212](http://arxiv.org/abs/2309.06212)

    基于地理气象数据的深度神经网络用于长期干旱预测，提出了一种端到端解决方案来预测特定地区干旱概率。采用卷积LSTM和Transformer模型相比其他基线模型能够获得更高的准确性。

    

    在农业实践中，准确预测特定地区干旱概率对于决策具有重要性。尤其对于长期决策，提前一年进行预测至关重要。然而，由于感兴趣区域及其相邻区域内各种因素的复杂相互作用，预测这一概率存在挑战。在本研究中，我们提出了一种基于各种时空神经网络的端到端解决方案来解决这个问题。所考虑的模型主要是根据Palmer干旱严重指数（PDSI）预测感兴趣亚区的干旱强度，利用气候模型的内在因素和见解来提高干旱预测的准确性。比较评估结果表明，与基准梯度提升和逻辑回归解决方案相比，卷积LSTM（ConvLSTM）和Transformer模型的准确性更高。前两种模型取得了令人印象深刻的ROC AUC分数，高达0.90

    The accurate prediction of drought probability in specific regions is crucial for informed decision-making in agricultural practices. It is important to make predictions one year in advance, particularly for long-term decisions. However, forecasting this probability presents challenges due to the complex interplay of various factors within the region of interest and neighboring areas. In this study, we propose an end-to-end solution to address this issue based on various spatiotemporal neural networks. The models considered focus on predicting the drought intensity based on the Palmer Drought Severity Index (PDSI) for subregions of interest, leveraging intrinsic factors and insights from climate models to enhance drought predictions.  Comparative evaluations demonstrate the superior accuracy of Convolutional LSTM (ConvLSTM) and transformer models compared to baseline gradient boosting and logistic regression solutions. The two former models achieved impressive ROC AUC scores from 0.90 
    
[^32]: 连续时间线性动态系统的系统识别

    System Identification for Continuous-time Linear Dynamical Systems. (arXiv:2308.11933v1 [cs.LG])

    [http://arxiv.org/abs/2308.11933](http://arxiv.org/abs/2308.11933)

    本文解决了在连续时间下观测不规则采样的情况下，Kalman滤波器的系统识别问题。通过引入连续时间Ito随机微分方程来推广Kalman滤波器的学习，并提供一个新颖的两滤波器的后验计算方法，通过贝叶斯派生获得的解析形式的后验计算方法可以高效地估计SDE的参数。

    

    Kalman滤波器的系统识别问题在学习动态系统的基础参数时，通常假设观测值在等间隔的时间点采样。然而，在许多应用中，这个假设是有限制和不切实际的。本文针对连续离散滤波器的系统识别问题，通过求解连续时间Ito随机微分方程（SDE）来推广Kalman滤波器的学习。我们引入了一个新颖的两滤波器，具有贝叶斯派生的解析形式的后验，这样可以得到不需要预先计算的正向传递的解析更新。利用这种解析的高效计算后验的方法，我们提供了一种EM过程，用于估计SDE的参数，自然地纳入了不规则采样的测量。

    The problem of system identification for the Kalman filter, relying on the expectation-maximization (EM) procedure to learn the underlying parameters of a dynamical system, has largely been studied assuming that observations are sampled at equally-spaced time points. However, in many applications this is a restrictive and unrealistic assumption. This paper addresses system identification for the continuous-discrete filter, with the aim of generalizing learning for the Kalman filter by relying on a solution to a continuous-time It\^o stochastic differential equation (SDE) for the latent state and covariance dynamics. We introduce a novel two-filter, analytical form for the posterior with a Bayesian derivation, which yields analytical updates which do not require the forward-pass to be pre-computed. Using this analytical and efficient computation of the posterior, we provide an EM procedure which estimates the parameters of the SDE, naturally incorporating irregularly sampled measurement
    
[^33]: AI-Enabled Software and System Architecture Frameworks: Focusing on smart Cyber-Physical Systems (CPS).

    AI-Enabled Software and System Architecture Frameworks: Focusing on smart Cyber-Physical Systems (CPS). (arXiv:2308.05239v1 [cs.SE])

    [http://arxiv.org/abs/2308.05239](http://arxiv.org/abs/2308.05239)

    这篇论文提出了适应现代应用和组织要求的AI-enabled软件和系统架构框架，重点关注机器学习驱动的智能物联网系统(CPS)。作者提出了用于评估和基准化ML-enabled CPS的优点标准。

    

    在文献中提出了几种软件、系统和企业的架构框架。它们识别了各种利益相关者，并定义了架构的观点和视图，以框架和解决利益相关者的关注点。然而，在现有的架构框架中，尚未包括与数据科学和机器学习相关的利益相关者，如数据科学家和数据工程师。因此，它们未能解决响应数据科学社区关注的架构视点和视图。本文通过建立适用于现代应用和组织的架构框架来填补这一空白，其中机器学习工件普遍存在且至关重要。具体而言，我们专注于机器学习驱动的智能物联网系统（CPS），并提出了两组适应CPS高效开发和性能评估的优点标准，即用于评估和基准化机器学习驱动CPS的标准，

    Several architecture frameworks for software, systems, and enterprises have been proposed in the literature. They identified various stakeholders and defined architecture viewpoints and views to frame and address stakeholder concerns. However, the stakeholders with data science and Machine Learning (ML) related concerns, such as data scientists and data engineers, are yet to be included in existing architecture frameworks. Therefore, they failed to address the architecture viewpoints and views responsive to the concerns of the data science community. In this paper, we address this gap by establishing the architecture frameworks adapted to meet the requirements of modern applications and organizations where ML artifacts are both prevalent and crucial. In particular, we focus on ML-enabled Cyber-Physical Systems (CPSs) and propose two sets of merit criteria for their efficient development and performance assessment, namely the criteria for evaluating and benchmarking ML-enabled CPSs, and
    
[^34]: 高效互动感知神经网络反馈环的区间分析

    Efficient Interaction-Aware Interval Analysis of Neural Network Feedback Loops. (arXiv:2307.14938v1 [eess.SY])

    [http://arxiv.org/abs/2307.14938](http://arxiv.org/abs/2307.14938)

    本文提出了一种计算效率高的神经网络控制系统区间可达性分析框架，通过引入包含函数和构建嵌入系统来捕捉系统和神经网络控制器之间的相互作用。

    

    本文提出了一种计算效率高的神经网络控制系统区间可达性分析框架。我们的方法基于神经网络控制器和开环系统的包含函数。我们观察到，许多最先进的神经网络验证器可以为神经网络生成包含函数。我们介绍并分析了一种基于函数雅可比边界的开环动力学包含函数的新类别，特别适用于捕捉系统和神经网络控制器之间的相互作用。接下来，对于任意动力系统，我们使用包含函数构建一个状态数是原系统两倍的嵌入系统。我们证明嵌入系统的单个轨迹可以提供可达集的超矩形近似。然后，我们提出了两种构建神经网络控制动力系统的闭环嵌入系统的方法，考虑系统之间的互动。

    In this paper, we propose a computationally efficient framework for interval reachability of neural network controlled systems. Our approach builds upon inclusion functions for the neural network controller and the open-loop system. We observe that many state-of-the-art neural network verifiers can produce inclusion functions for neural networks. We introduce and analyze a new class of inclusion functions for the open-loop dynamics based on bounds of the function Jacobian that is particularly suitable for capturing the interactions between systems and neural network controllers. Next, for any dynamical system, we use inclusion functions to construct an embedding system with twice the number of states as the original system. We show that a single trajectory of this embedding system provides hyper-rectangular over-approximations of reachable sets. We then propose two approaches for constructing a closed-loop embedding system for a neural network controlled dynamical system that accounts 
    
[^35]: 核化归一化流

    Kernelised Normalising Flows. (arXiv:2307.14839v1 [stat.ML])

    [http://arxiv.org/abs/2307.14839](http://arxiv.org/abs/2307.14839)

    本文提出了一种新颖的核化归一化流范式，称为Ferumal流，它将核函数集成到归一化流的框架中。相对于基于神经网络的流，核化流可以在低数据环境中产生竞争力或优越的结果，同时保持参数效率。

    

    归一化流是以其可逆的架构而被描述的生成模型。然而，可逆性要求对其表达能力施加限制，需要大量的参数和创新的架构设计来达到满意的结果。虽然基于流的模型主要依赖于基于神经网络的转换来实现表达能力，但替代的转换方法却受到了有限的关注。在这项工作中，我们提出了一种新颖的核化归一化流范式，称为Ferumal流，它将核函数集成到框架中。我们的结果表明，相比于基于神经网络的流，核化流可以产生有竞争力或优越的结果，同时保持参数效率。核化流在低数据环境中表现出色，可以在数据稀缺的应用中进行灵活的非参数密度估计。

    Normalising Flows are generative models characterised by their invertible architecture. However, the requirement of invertibility imposes constraints on their expressiveness, necessitating a large number of parameters and innovative architectural designs to achieve satisfactory outcomes. Whilst flow-based models predominantly rely on neural-network-based transformations for expressive designs, alternative transformation methods have received limited attention. In this work, we present Ferumal flow, a novel kernelised normalising flow paradigm that integrates kernels into the framework. Our results demonstrate that a kernelised flow can yield competitive or superior results compared to neural network-based flows whilst maintaining parameter efficiency. Kernelised flows excel especially in the low-data regime, enabling flexible non-parametric density estimation in applications with sparse data availability.
    
[^36]: 自编码器的最大似然训练

    Maximum Likelihood Training of Autoencoders. (arXiv:2306.01843v1 [cs.LG])

    [http://arxiv.org/abs/2306.01843](http://arxiv.org/abs/2306.01843)

    本文介绍了一种成功的最大似然训练方法，用于非约束自编码器，将生成建模的优异性质与高效自编码器相结合。作者克服了两个挑战：设计了消除迭代的估计器并提出了稳定的最大似然训练目标。实验证明这种方法可以成功训练一系列非约束性自编码器，并取得了有竞争力的性能。

    

    最大似然训练在生成建模中具有优异的统计性质，尤其是在归一化流模型中非常流行。另一方面，由于流形假设，生成自编码器有望比归一化流更高效。本文首次引入了非约束自编码器的成功最大似然训练，将这两种范式融合在一起。为此，我们识别并克服了两个挑战：首先，现有的自由格式网络的最大似然估计器过于缓慢，依赖于迭代方案，其成本随潜在维度呈线性增长。我们引入了一个改进的估计器，消除了迭代，从而使成本保持不变（每个批次的运行时间大约是普通自编码器的两倍）。其次，我们证明朴素地将最大似然应用于自编码器可能导致发散解决方案，并利用这个想法来推动稳定的最大似然训练目标。我们进行了实验，表明所提出的训练方法可以成功训练一系列非约束性自编码器，并在生成图像、插值和变换等任务中取得了有竞争力的性能。

    Maximum likelihood training has favorable statistical properties and is popular for generative modeling, especially with normalizing flows. On the other hand, generative autoencoders promise to be more efficient than normalizing flows due to the manifold hypothesis. In this work, we introduce successful maximum likelihood training of unconstrained autoencoders for the first time, bringing the two paradigms together. To do so, we identify and overcome two challenges: Firstly, existing maximum likelihood estimators for free-form networks are unacceptably slow, relying on iteration schemes whose cost scales linearly with latent dimension. We introduce an improved estimator which eliminates iteration, resulting in constant cost (roughly double the runtime per batch of a vanilla autoencoder). Secondly, we demonstrate that naively applying maximum likelihood to autoencoders can lead to divergent solutions and use this insight to motivate a stable maximum likelihood training objective. We per
    
[^37]: 强凸优化的次梯度法的原始对偶理论

    Some Primal-Dual Theory for Subgradient Methods for Strongly Convex Optimization. (arXiv:2305.17323v1 [math.OC])

    [http://arxiv.org/abs/2305.17323](http://arxiv.org/abs/2305.17323)

    本文提出了一种强凸优化的次梯度法原始对偶理论，可以实现简单的、最佳的停止准则和优化证明，同时可以适用于各种步长的选择和非Lipschitz病态问题，保证了这些方法次线性收敛速度。

    

    本文考虑强凸但潜在非光滑非Lipschitz优化的（随机）次梯度法。我们提供了新的等价对偶描述（类似于对偶平均）来描述经典的次梯度法，近端次梯度法和切换次梯度法。这些等价性能够以 $O(1/T)$ 的速度收敛，同时能够在强凸优化问题上分别还提供了经典原始间隙和前人未曾分析的对偶间隙保证。因此，我们的理论为这些经典方法提供了简单的、最佳的停止准则和优化证明，而不需要额外的计算成本。我们的结果适用于近乎所有的步长选择和一系列的非Lipschitz病态问题，对于在这些情况下，次梯度法的早期迭代可能会出现指数级的发散，而之前的研究没有处理过这种问题。即使在这种不良操作的情况下，我们的理论仍然确保和 bounds 了这些方法的次线性收敛速度。

    We consider (stochastic) subgradient methods for strongly convex but potentially nonsmooth non-Lipschitz optimization. We provide new equivalent dual descriptions (in the style of dual averaging) for the classic subgradient method, the proximal subgradient method, and the switching subgradient method. These equivalences enable $O(1/T)$ convergence guarantees in terms of both their classic primal gap and a not previously analyzed dual gap for strongly convex optimization. Consequently, our theory provides these classic methods with simple, optimal stopping criteria and optimality certificates at no added computational cost. Our results apply under nearly any stepsize selection and for a range of non-Lipschitz ill-conditioned problems where the early iterations of the subgradient method may diverge exponentially quickly (a phenomenon which, to the best of our knowledge, no prior works address). Even in the presence of such undesirable behaviors, our theory still ensures and bounds eventu
    
[^38]: 通过信息瓶颈方法探索监督对比学习中神经网络崩溃的理解

    Towards understanding neural collapse in supervised contrastive learning with the information bottleneck method. (arXiv:2305.11957v1 [cs.LG])

    [http://arxiv.org/abs/2305.11957](http://arxiv.org/abs/2305.11957)

    本文将神经网络崩溃建模为信息瓶颈问题，证明神经网络崩溃导致良好的泛化，特别是当它接近分类问题的最优信息瓶颈解时。

    

    神经网络崩溃是指在超出性能平台训练时，深度神经网络最后一层激活的几何学表现。目前存在的问题包括神经网络崩溃是否会导致更好的泛化，如果是，超出性能平台的训练如何帮助神经网络崩溃。本文将神经网络崩溃建模为信息瓶颈问题，以探究是否存在这样一种紧凑的表示，并发现其与泛化性的关联。我们证明神经网络崩溃导致良好的泛化，特别是当它接近分类问题的最优信息瓶颈解时。最近的研究表明，使用相同的对比损失目标独立训练的两个深度神经网络是线性可识别的，这意味着得到的表示等效于矩阵变换。我们利用线性可识别性来近似信息瓶颈问题的解析解。这个近似表明，当类平均值相等时，最优解非常接近端到端模型，并提供了进一步的理论分析。

    Neural collapse describes the geometry of activation in the final layer of a deep neural network when it is trained beyond performance plateaus. Open questions include whether neural collapse leads to better generalization and, if so, why and how training beyond the plateau helps. We model neural collapse as an information bottleneck (IB) problem in order to investigate whether such a compact representation exists and discover its connection to generalization. We demonstrate that neural collapse leads to good generalization specifically when it approaches an optimal IB solution of the classification problem. Recent research has shown that two deep neural networks independently trained with the same contrastive loss objective are linearly identifiable, meaning that the resulting representations are equivalent up to a matrix transformation. We leverage linear identifiability to approximate an analytical solution of the IB problem. This approximation demonstrates that when class means exh
    
[^39]: 漫扩扩散模型和采样器的表达能力研究

    Expressiveness Remarks for Denoising Diffusion Models and Samplers. (arXiv:2305.09605v1 [stat.ML])

    [http://arxiv.org/abs/2305.09605](http://arxiv.org/abs/2305.09605)

    本文在漫扩扩散模型和采样器方面进行了表达能力的研究，通过将已知的神经网络逼近结果扩展到漫扩扩散模型和采样器来实现。

    

    漫扩扩散模型是一类生成模型，在许多领域最近已经取得了最先进的结果。通过漫扩过程逐渐向数据中添加噪声，将数据分布转化为高斯分布。然后，通过模拟该漫扩的时间反演的逼近来获取生成模型的样本，刚开始这个漫扩模拟的初始值是高斯样本。最近的研究探索了将漫扩模型适应于采样和推断任务。本文基于众所周知的与F\"ollmer漂移类似的随机控制联系，将针对F\"ollmer漂移的已知神经网络逼近结果扩展到漫扩扩散模型和采样器。

    Denoising diffusion models are a class of generative models which have recently achieved state-of-the-art results across many domains. Gradual noise is added to the data using a diffusion process, which transforms the data distribution into a Gaussian. Samples from the generative model are then obtained by simulating an approximation of the time reversal of this diffusion initialized by Gaussian samples. Recent research has explored adapting diffusion models for sampling and inference tasks. In this paper, we leverage known connections to stochastic control akin to the F\"ollmer drift to extend established neural network approximation results for the F\"ollmer drift to denoising diffusion models and samplers.
    
[^40]: GSplit: 通过分裂并行实现大规模图神经网络训练的扩展

    GSplit: Scaling Graph Neural Network Training on Large Graphs via Split-Parallelism. (arXiv:2303.13775v1 [cs.DC])

    [http://arxiv.org/abs/2303.13775](http://arxiv.org/abs/2303.13775)

    本文提出了一种新的并行小批量训练方法，即分裂并行，应用在图神经网络训练上，能有效缓解数据并行方法的性能瓶颈，同时在大规模图上的性能表现优越。

    

    在许多行业、科学和工程领域（如推荐系统、社交图分析、知识库、材料科学和生物学）中，拥有数十亿个边的大规模图形是普遍存在的。图神经网络（GNN）作为一种新兴的机器学习模型，由于在各种图分析任务中具有卓越的性能，因此越来越多地被采用来学习这些图形。在大型图形上训练通常采用小批量训练，并且数据并行是将小批量训练扩展到多个 GPU 的标准方法。本文认为，GNN 训练系统的几个基本性能瓶颈与数据并行方法的固有限制有关。我们提出了一种新的并行小批量训练范式- 分裂并行，并将其实现在一个名为gsplit的新系统中。实验表明，gsplit 的性能优于DGL、Quiver和PaGraph等现有的系统。

    Large-scale graphs with billions of edges are ubiquitous in many industries, science, and engineering fields such as recommendation systems, social graph analysis, knowledge base, material science, and biology. Graph neural networks (GNN), an emerging class of machine learning models, are increasingly adopted to learn on these graphs due to their superior performance in various graph analytics tasks. Mini-batch training is commonly adopted to train on large graphs, and data parallelism is the standard approach to scale mini-batch training to multiple GPUs. In this paper, we argue that several fundamental performance bottlenecks of GNN training systems have to do with inherent limitations of the data parallel approach. We then propose split parallelism, a novel parallel mini-batch training paradigm. We implement split parallelism in a novel system called gsplit and show that it outperforms state-of-the-art systems such as DGL, Quiver, and PaGraph.
    
[^41]: 解缠表示学习

    Disentangled Representation Learning. (arXiv:2211.11695v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.11695](http://arxiv.org/abs/2211.11695)

    解缠表示学习旨在学习一个模型，能够识别和解缠观测数据中隐藏的因素，从而产生可解释的数据表示。它在提高模型可解释性、可控性、鲁棒性和泛化能力方面具有广泛的应用潜力。

    

    解缠表示学习（DRL）旨在学习一个能够识别和解缠可观测数据中隐藏因素的模型。将变化的潜在要素分离成具有语义意义的变量的过程有助于学习可解释的数据表示，模仿人类观察对象或关系时的有意义理解过程。作为一种通用的学习策略，DRL在多个领域中展示了提高模型可解释性、可控性、鲁棒性以及泛化能力的优势，如计算机视觉、自然语言处理、数据挖掘等。本文综合评述了DRL的各个方面，包括动机、定义、方法论、评估、应用和模型设计。我们讨论了基于两个公认定义（直观定义和群论定义）的DRL方法。我们进一步分析了DRL的开展。

    Disentangled Representation Learning (DRL) aims to learn a model capable of identifying and disentangling the underlying factors hidden in the observable data in representation form. The process of separating underlying factors of variation into variables with semantic meaning benefits in learning explainable representations of data, which imitates the meaningful understanding process of humans when observing an object or relation. As a general learning strategy, DRL has demonstrated its power in improving the model explainability, controlability, robustness, as well as generalization capacity in a wide range of scenarios such as computer vision, natural language processing, data mining etc. In this article, we comprehensively review DRL from various aspects including motivations, definitions, methodologies, evaluations, applications and model designs. We discuss works on DRL based on two well-recognized definitions, i.e., Intuitive Definition and Group Theory Definition. We further ca
    

