# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [A Survey on Self-Supervised Pre-Training of Graph Foundation Models: A Knowledge-Based Perspective](https://arxiv.org/abs/2403.16137) | 该论文从基于知识的角度全面调查和分析了图基础模型的自监督预训练任务，涉及微观和宏观知识，包括9个知识类别、25个预训练任务以及各种下游任务适应策略。 |
| [^2] | [Addressing Source Scale Bias via Image Warping for Domain Adaptation](https://arxiv.org/abs/2403.12712) | 通过在训练过程中对突出的对象区域进行过采样的自适应注意力处理，以及针对对象区域采样的实例级变形引导，有效减轻域自适应中的源尺度偏差。 |
| [^3] | [UPS: Towards Foundation Models for PDE Solving via Cross-Modal Adaptation](https://arxiv.org/abs/2403.07187) | UPS通过跨模态适应训练神经网络，将不同PDE统一到一致的表示空间，并在少样本下达到了强有力的实验结果，优于现有基线，实现了1D和2D数据集上的最先进结果。 |
| [^4] | [Analysis of Total Variation Minimization for Clustered Federated Learning](https://arxiv.org/abs/2403.06298) | GTVMin在集群化联邦学习中的分析提供了关于解决统计异质性的有效性和鲁棒性的宝贵见解 |
| [^5] | [FrameQuant: Flexible Low-Bit Quantization for Transformers](https://arxiv.org/abs/2403.06082) | 提出一种简单的方案，通过融合框架将Transformer模型量化为仅两位，仅有轻微精度下降。 |
| [^6] | [Switching the Loss Reduces the Cost in Batch Reinforcement Learning](https://arxiv.org/abs/2403.05385) | 使用对数损失函数来训练适合的Q迭代的批强化学习方法，在实现目标时不产生成本的问题中，其样本数量需求与最优策略的累积成本成比例，能够提供与最优可达成本成比例的“小成本”界限，并在实验中验证在那些最优策略可靠实现目标的问题中，FQI-LOG比使用平方损失训练的FQI使用更少的样本。 |
| [^7] | [Multi-Tower Multi-Interest Recommendation with User Representation Repel](https://arxiv.org/abs/2403.05122) | 提出了一种具有用户表示排斥的新型多塔多兴趣框架，解决了多兴趣学习方法面临的训练和部署目标差异、无法访问商品信息以及难以工业采用等问题。 |
| [^8] | [Designing Informative Metrics for Few-Shot Example Selection](https://arxiv.org/abs/2403.03861) | 提出了一种基于复杂度的提示选择方法，用于将示例与测试句子的句法-语义复杂度对齐，在少样本NER任务中取得了显著的性能提升。 |
| [^9] | [FedADMM-InSa: An Inexact and Self-Adaptive ADMM for Federated Learning](https://arxiv.org/abs/2402.13989) | 提出了一种不精确和自适应的FedADMM算法，通过为客户端的本地更新设计一个不精确性标准，消除了调整本地训练准确度的需要，降低了计算成本并减轻了滞后效应。 |
| [^10] | [Impact of data usage for forecasting on performance of model predictive control in buildings with smart energy storage](https://arxiv.org/abs/2402.12539) | 研究发现，在具有智能储能的建筑中，简单的线性多层感知器模型提供了与最先进模型相当的预测准确性，更具有数据效率和泛化能力。 |
| [^11] | [Exact Fractional Inference via Re-Parametrization & Interpolation between Tree-Re-Weighted- and Belief Propagation- Algorithms](https://arxiv.org/abs/2301.10369) | 通过构建$\lambda$-分数全同维数，实现了树重新加权和信念传播算法之间的插值，确保在铁磁性情况下，存在“精确”$\lambda_*$使得计算的配分函数$Z=Z^{(\lambda_*)}$。 |
| [^12] | [Disentangled Condensation for Large-scale Graphs.](http://arxiv.org/abs/2401.12231) | 本文提出了用于大规模图的解缠结凝聚方法DisCo，通过节点和边的凝聚模块实现了对大规模图的高效缩凝，提高了可扩展性和压缩图的保真度。 |
| [^13] | [Parametric Matrix Models.](http://arxiv.org/abs/2401.11694) | 参数矩阵模型是一种通用机器学习算法，基于矩阵方程设计，通过简化基础方法进行近似解参数方程。它可以仅使用经验数据进行训练，适用于各种机器学习问题，并在计算框架内产生准确的结果。 |
| [^14] | [LLM in a flash: Efficient Large Language Model Inference with Limited Memory.](http://arxiv.org/abs/2312.11514) | 本文提出了一种在有限内存条件下高效运行大型语言模型的方法，通过将模型参数存储在闪存中并按需传输到DRAM的方式来解决内存限制的挑战。该方法通过构建推理成本模型并优化数据传输和读取方式，引入了窗口化和行列绑定两种主要技术。 |
| [^15] | [Early detection of inflammatory arthritis to improve referrals using multimodal machine learning from blood testing, semi-structured and unstructured patient records.](http://arxiv.org/abs/2310.19967) | 通过多模态机器学习从血液测试、半结构化和非结构化病历中学习，早期检测炎症性关节炎可以改善转诊，为及时治疗和预防病情恶化提供决策支持。 |
| [^16] | [The Janus Interface: How Fine-Tuning in Large Language Models Amplifies the Privacy Risks.](http://arxiv.org/abs/2310.15469) | 《Janus接口：大型语言模型微调如何放大隐私风险》研究了大型语言模型的微调对个人信息泄露的风险，发现了一种新的LLM利用途径。 |
| [^17] | [Optimization of Raman amplifiers: a comparison between black-, grey- and white-box modeling.](http://arxiv.org/abs/2310.05954) | 该论文比较了白盒、灰盒和黑盒模型在双向Raman放大器中实现目标频距放大的能力，并展示了它们都可以实现C波段下达到1dB的频距平坦度。 |
| [^18] | [In-class Data Analysis Replications: Teaching Students while Testing Science.](http://arxiv.org/abs/2308.16491) | 这项研究揭示了课堂数据分析复制的可行性，以及这种方法对学生、教育者和科学家的成本与收益。同时，学生对数据的预期与实际情况存在差异。 |
| [^19] | [Diversifying AI: Towards Creative Chess with AlphaZero.](http://arxiv.org/abs/2308.09175) | 本研究探索了AI在计算任务中是否可以从创造性决策机制中受益，并通过构建多样化的AI系统团队，在挑战性任务中超越单个AI，通过生成更多的想法，并选择最佳想法。在国际象棋中的实验结果显示，多样化AI系统以不同方式下国际象棋。 |
| [^20] | [The Hard-Constraint PINNs for Interface Optimal Control Problems.](http://arxiv.org/abs/2308.06709) | 本研究将物理信息神经网络（PINNs）与不连续性捕获神经网络相结合，应用于具有界面和控制约束的优化控制问题，并通过将边界和界面条件作为硬约束以确保数值精度。 |
| [^21] | [SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore.](http://arxiv.org/abs/2308.04430) | SILO是一种新的语言模型，通过在推理过程中对非参数化的数据存储进行查询，实现在面临法律风险和模型性能之间的权衡，并支持数据归属和数据生产者退出模型的功能。 |
| [^22] | [Martian time-series unraveled: A multi-scale nested approach with factorial variational autoencoders.](http://arxiv.org/abs/2305.16189) | 该论文提出了一种因子高斯混合变分自动编码器，用于多尺度聚类和源分离，通过利用小波散射协方差来提供随机过程的低维表示，能够区分不同的非高斯随机过程，并在MRO数据集上展现了更好的性能。 |

# 详细

[^1]: 自监督预训练图基础模型的调查：基于知识的视角

    A Survey on Self-Supervised Pre-Training of Graph Foundation Models: A Knowledge-Based Perspective

    [https://arxiv.org/abs/2403.16137](https://arxiv.org/abs/2403.16137)

    该论文从基于知识的角度全面调查和分析了图基础模型的自监督预训练任务，涉及微观和宏观知识，包括9个知识类别、25个预训练任务以及各种下游任务适应策略。

    

    图自监督学习现在是预训练图基础模型的首选方法，包括图神经网络、图变换器，以及更近期的基于大型语言模型（LLM）的图模型。文章全面调查和分析了基于知识的视角下的图基础模型的预训练任务，包括微观（节点、链接等）和宏观知识（簇、全局结构等）。涵盖了共计9个知识类别和25个预训练任务，以及各种下游任务适应策略。

    arXiv:2403.16137v1 Announce Type: new  Abstract: Graph self-supervised learning is now a go-to method for pre-training graph foundation models, including graph neural networks, graph transformers, and more recent large language model (LLM)-based graph models. There is a wide variety of knowledge patterns embedded in the structure and properties of graphs which may be used for pre-training, but we lack a systematic overview of self-supervised pre-training tasks from the perspective of graph knowledge. In this paper, we comprehensively survey and analyze the pre-training tasks of graph foundation models from a knowledge-based perspective, consisting of microscopic (nodes, links, etc) and macroscopic knowledge (clusters, global structure, etc). It covers a total of 9 knowledge categories and 25 pre-training tasks, as well as various downstream task adaptation strategies. Furthermore, an extensive list of the related papers with detailed metadata is provided at https://github.com/Newiz430/
    
[^2]: 通过图像变形解决域自适应中的源尺度偏差问题

    Addressing Source Scale Bias via Image Warping for Domain Adaptation

    [https://arxiv.org/abs/2403.12712](https://arxiv.org/abs/2403.12712)

    通过在训练过程中对突出的对象区域进行过采样的自适应注意力处理，以及针对对象区域采样的实例级变形引导，有效减轻域自适应中的源尺度偏差。

    

    在视觉识别中，由于真实场景数据集中对象和图像大小分布的不平衡，尺度偏差是一个关键挑战。传统解决方案包括注入尺度不变性先验、在训练过程中对数据集在不同尺度进行过采样，或者在推断时调整尺度。虽然这些策略在一定程度上减轻了尺度偏差，但它们在跨多样化数据集时的适应能力有限。此外，它们会增加训练过程的计算负载和推断过程的延迟。在这项工作中，我们使用自适应的注意力处理——通过在训练过程中就地扭曲图像来对突出的对象区域进行过采样。我们发现，通过改变源尺度分布可以改善主干特征，我们开发了一个面向对象区域采样的实例级变形引导，以减轻域自适应中的源尺度偏差。我们的方法提高了对地理、光照和天气条件的适应性。

    arXiv:2403.12712v1 Announce Type: cross  Abstract: In visual recognition, scale bias is a key challenge due to the imbalance of object and image size distribution inherent in real scene datasets. Conventional solutions involve injecting scale invariance priors, oversampling the dataset at different scales during training, or adjusting scale at inference. While these strategies mitigate scale bias to some extent, their ability to adapt across diverse datasets is limited. Besides, they increase computational load during training and latency during inference. In this work, we use adaptive attentional processing -- oversampling salient object regions by warping images in-place during training. Discovering that shifting the source scale distribution improves backbone features, we developed a instance-level warping guidance aimed at object region sampling to mitigate source scale bias in domain adaptation. Our approach improves adaptation across geographies, lighting and weather conditions, 
    
[^3]: UPS: 通过跨模态适应实现偏微分方程求解的基础模型

    UPS: Towards Foundation Models for PDE Solving via Cross-Modal Adaptation

    [https://arxiv.org/abs/2403.07187](https://arxiv.org/abs/2403.07187)

    UPS通过跨模态适应训练神经网络，将不同PDE统一到一致的表示空间，并在少样本下达到了强有力的实验结果，优于现有基线，实现了1D和2D数据集上的最先进结果。

    

    我们介绍了UPS（统一PDE求解器），这是一种有效的数据高效方法，用于解决不同域、维度和分辨率上定义的各种时空PDE。UPS将不同的PDE统一到一致的表示空间中，并使用将LLMs与特定域神经算子相结合的统一网络架构处理各种PDE数据集合。我们通过两阶段的跨模态适应过程训练网络，利用模态对齐和多任务学习的思想。通过从预训练的LLMs进行调整并利用文本形式的元信息，我们能够使用比以前的方法少得多的训练样本，并获得强有力的实证结果。UPS在PDEBench的广泛1D和2D数据集上明显优于现有基线，对考虑的10个任务中的8个任务达到了最先进的结果。与此同时，它能够少样本快速转移至不同的PDE。

    arXiv:2403.07187v1 Announce Type: new  Abstract: We introduce UPS (Unified PDE Solver), an effective and data-efficient approach to solve diverse spatiotemporal PDEs defined over various domains, dimensions, and resolutions. UPS unifies different PDEs into a consistent representation space and processes diverse collections of PDE data using a unified network architecture that combines LLMs with domain-specific neural operators. We train the network via a two-stage cross-modal adaptation process, leveraging ideas of modality alignment and multi-task learning. By adapting from pretrained LLMs and exploiting text-form meta information, we are able to use considerably fewer training samples than previous methods while obtaining strong empirical results. UPS outperforms existing baselines, often by a large margin, on a wide range of 1D and 2D datasets in PDEBench, achieving state-of-the-art results on 8 of 10 tasks considered. Meanwhile, it is capable of few-shot transfer to different PDE f
    
[^4]: 集群化联邦学习中总变差最小化的分析

    Analysis of Total Variation Minimization for Clustered Federated Learning

    [https://arxiv.org/abs/2403.06298](https://arxiv.org/abs/2403.06298)

    GTVMin在集群化联邦学习中的分析提供了关于解决统计异质性的有效性和鲁棒性的宝贵见解

    

    联邦学习应用中的一个关键挑战是本地数据集的统计异质性。集群化联邦学习通过识别大致同质的本地数据集群来解决这一挑战。最近的一种集群化联邦学习方法是广义总变差最小化（GTVMin）。该方法需要一个相似性图，可以通过领域专业知识或数据驱动的图学习技术来获得。在一个广泛适用的集群假设下，我们推导了GTVMin解与其按簇平均值之间的偏差的上界。这个界限为我们提供了关于GTVMin在解决联邦学习环境中的统计异质性的有效性和鲁棒性的宝贵见解。

    arXiv:2403.06298v1 Announce Type: new  Abstract: A key challenge in federated learning applications is the statistical heterogeneity of local datasets. Clustered federated learning addresses this challenge by identifying clusters of local datasets that are approximately homogeneous. One recent approach to clustered federated learning is generalized total variation minimization (GTVMin). This approach requires a similarity graph which can be obtained by domain expertise or in a data-driven fashion via graph learning techniques. Under a widely applicable clustering assumption, we derive an upper bound the deviation between GTVMin solutions and their cluster-wise averages. This bound provides valuable insights into the effectiveness and robustness of GTVMin in addressing statistical heterogeneity within federated learning environments.
    
[^5]: FrameQuant: Transformer的灵活低比特量化方法

    FrameQuant: Flexible Low-Bit Quantization for Transformers

    [https://arxiv.org/abs/2403.06082](https://arxiv.org/abs/2403.06082)

    提出一种简单的方案，通过融合框架将Transformer模型量化为仅两位，仅有轻微精度下降。

    

    Transformer是许多视觉和自然语言处理任务强大基础模型的支柱。然而，它们的计算和内存/存储空间占用较大，因此为这些模型提供服务往往需要昂贵的高端硬件。为了缓解这一困难，后训练量化试图修改预训练模型并将其量化为八位或更低的位数，显着提高计算/内存/延迟效率。既可以成功将这些模型量化为四位，但性能有所损失。在这项工作中，我们概述了一个简单的方案，将基于Transformer的模型量化为仅两位（加一些额外开销），仅会有轻微的精度下降。我们的制定关键在于从谐波分析中借鉴了一种称为融合框架的概念。我们的主要发现是，量化不应该在原始权重空间中进行，而是应该在融合框架表示中进行。

    arXiv:2403.06082v1 Announce Type: cross  Abstract: Transformers are the backbone of powerful foundation models for many Vision and Natural Language Processing tasks. But their compute and memory/storage footprint is large, and so, serving such models is expensive often requiring high-end hardware. To mitigate this difficulty, Post-Training Quantization seeks to modify a pre-trained model and quantize it to eight bits or lower, significantly boosting compute/memory/latency efficiency. Such models have been successfully quantized to four bits with some performance loss. In this work, we outline a simple scheme to quantize Transformer-based models to just two bits (plus some overhead) with only a small drop in accuracy. Key to our formulation is a concept borrowed from Harmonic analysis called Fusion Frames. Our main finding is that the quantization must take place not in the original weight space, but instead in the Fusion Frame representations. If quantization is interpreted as the addi
    
[^6]: 在批强化学习中，通过切换损失函数来降低成本

    Switching the Loss Reduces the Cost in Batch Reinforcement Learning

    [https://arxiv.org/abs/2403.05385](https://arxiv.org/abs/2403.05385)

    使用对数损失函数来训练适合的Q迭代的批强化学习方法，在实现目标时不产生成本的问题中，其样本数量需求与最优策略的累积成本成比例，能够提供与最优可达成本成比例的“小成本”界限，并在实验中验证在那些最优策略可靠实现目标的问题中，FQI-LOG比使用平方损失训练的FQI使用更少的样本。

    

    我们提出了一种使用对数损失（FQI-LOG）来训练适合的Q迭代的批强化学习（RL）方法。我们展示了使用FQI-LOG学习接近最优策略所需的样本数量与最优策略的累积成本成比例，对于那些通过最优行为实现目标且不产生成本的问题，最优策略的累积成本为零。通过这种方式，我们提供了一种在批RL中证明具有与最优可达成本成比例的“小成本”界限的一般框架。此外，我们从经验上验证，FQI-LOG在那些最优策略可靠地实现目标的问题上使用的样本比使用平方损失训练的FQI要少。

    arXiv:2403.05385v1 Announce Type: new  Abstract: We propose training fitted Q-iteration with log-loss (FQI-LOG) for batch reinforcement learning (RL). We show that the number of samples needed to learn a near-optimal policy with FQI-LOG scales with the accumulated cost of the optimal policy, which is zero in problems where acting optimally achieves the goal and incurs no cost. In doing so, we provide a general framework for proving $\textit{small-cost}$ bounds, i.e. bounds that scale with the optimal achievable cost, in batch RL. Moreover, we empirically verify that FQI-LOG uses fewer samples than FQI trained with squared loss on problems where the optimal policy reliably achieves the goal.
    
[^7]: 具有用户表示排斥的多塔多兴趣推荐

    Multi-Tower Multi-Interest Recommendation with User Representation Repel

    [https://arxiv.org/abs/2403.05122](https://arxiv.org/abs/2403.05122)

    提出了一种具有用户表示排斥的新型多塔多兴趣框架，解决了多兴趣学习方法面临的训练和部署目标差异、无法访问商品信息以及难以工业采用等问题。

    

    在信息过载的时代，学术界和工业界都深刻认识到推荐系统的价值。特别是多兴趣序列推荐是近年来受到越来越多关注的一个子领域。通过生成多用户表示，多兴趣学习模型在理论上和经验上都比单用户表示模型具有更强的表达能力。尽管该领域取得了重大进展，但仍存在三个主要问题困扰着多兴趣学习方法的性能和可采用性，即训练和部署目标之间的差异、无法访问商品信息以及由于其单塔架构而难以工业采用。我们通过提出一种具有用户表示排斥的新型多塔多兴趣框架来解决这些挑战。通过跨多个大规模实验结果，我们证明了我们的方法的有效性。

    arXiv:2403.05122v1 Announce Type: cross  Abstract: In the era of information overload, the value of recommender systems has been profoundly recognized in academia and industry alike. Multi-interest sequential recommendation, in particular, is a subfield that has been receiving increasing attention in recent years. By generating multiple-user representations, multi-interest learning models demonstrate superior expressiveness than single-user representation models, both theoretically and empirically. Despite major advancements in the field, three major issues continue to plague the performance and adoptability of multi-interest learning methods, the difference between training and deployment objectives, the inability to access item information, and the difficulty of industrial adoption due to its single-tower architecture. We address these challenges by proposing a novel multi-tower multi-interest framework with user representation repel. Experimental results across multiple large-scale 
    
[^8]: 为少样本示例选择设计信息度量

    Designing Informative Metrics for Few-Shot Example Selection

    [https://arxiv.org/abs/2403.03861](https://arxiv.org/abs/2403.03861)

    提出了一种基于复杂度的提示选择方法，用于将示例与测试句子的句法-语义复杂度对齐，在少样本NER任务中取得了显著的性能提升。

    

    预训练语言模型（PLMs）在提供适当格式的示例时展现出了卓越的少样本学习能力。然而，选择“最佳”示例仍然是一个未解决的挑战。我们提出了一种基于复杂度的提示选择方法，适用于序列标注任务。该方法避免了训练一个专门用于选择示例的模型，而是使用特定的度量标准来对齐测试句子和示例的句法-语义复杂度。我们使用句子和单词级别的度量标准，将示例的复杂度与考虑中的（测试）句子进行匹配。我们的结果表明，我们的方法能够从PLMs中提取出更好的性能：在少样本NER上实现了最先进的性能，在CoNLL2003数据集上对GPT-4的F1分数实现了5%的绝对改善。我们还在像GPT-j-6B这样的较小模型中看到了高达28.85个点（F1/Acc.）的显著增益。

    arXiv:2403.03861v1 Announce Type: new  Abstract: Pretrained language models (PLMs) have shown remarkable few-shot learning capabilities when provided with properly formatted examples. However, selecting the "best" examples remains an open challenge. We propose a complexity-based prompt selection approach for sequence tagging tasks. This approach avoids the training of a dedicated model for selection of examples, and instead uses certain metrics to align the syntactico-semantic complexity of test sentences and examples. We use both sentence- and word-level metrics to match the complexity of examples to the (test) sentence being considered. Our results demonstrate that our approach extracts greater performance from PLMs: it achieves state-of-the-art performance on few-shot NER, achieving a 5% absolute improvement in F1 score on the CoNLL2003 dataset for GPT-4. We also see large gains of upto 28.85 points (F1/Acc.) in smaller models like GPT-j-6B.
    
[^9]: FedADMM-InSa: 一种不精确和自适应的联邦学习ADMM

    FedADMM-InSa: An Inexact and Self-Adaptive ADMM for Federated Learning

    [https://arxiv.org/abs/2402.13989](https://arxiv.org/abs/2402.13989)

    提出了一种不精确和自适应的FedADMM算法，通过为客户端的本地更新设计一个不精确性标准，消除了调整本地训练准确度的需要，降低了计算成本并减轻了滞后效应。

    

    联邦学习(FL)是一个有希望的框架，可以从分布式数据中学习同时保持隐私。有效的FL算法的发展面临各种挑战，包括异构数据和系统、通信能力有限以及受限的本地计算资源。最近开发的FedADMM方法对数据和系统的异构性表现出很强的韧性。然而，如果超参数没有经过精心调整，它们仍然会遭受性能下降的问题。为了解决这个问题，我们提出了一种不精确和自适应的FedADMM算法，名为FedADMM-InSa。首先，我们为客户端的本地更新设计了一个不精确性标准，以消除必须根据经验设置本地训练准确性的需求。这种不精确性标准可以由每个客户端独立地根据其独特条件进行评估，从而降低本地计算成本并减轻不良的滞后效应。

    arXiv:2402.13989v1 Announce Type: new  Abstract: Federated learning (FL) is a promising framework for learning from distributed data while maintaining privacy. The development of efficient FL algorithms encounters various challenges, including heterogeneous data and systems, limited communication capacities, and constrained local computational resources. Recently developed FedADMM methods show great resilience to both data and system heterogeneity. However, they still suffer from performance deterioration if the hyperparameters are not carefully tuned. To address this issue, we propose an inexact and self-adaptive FedADMM algorithm, termed FedADMM-InSa. First, we design an inexactness criterion for the clients' local updates to eliminate the need for empirically setting the local training accuracy. This inexactness criterion can be assessed by each client independently based on its unique condition, thereby reducing the local computational cost and mitigating the undesirable straggle e
    
[^10]: 数据使用对具有智能储能的建筑中模型预测控制性能的影响

    Impact of data usage for forecasting on performance of model predictive control in buildings with smart energy storage

    [https://arxiv.org/abs/2402.12539](https://arxiv.org/abs/2402.12539)

    研究发现，在具有智能储能的建筑中，简单的线性多层感知器模型提供了与最先进模型相当的预测准确性，更具有数据效率和泛化能力。

    

    数据是开发用于建筑能源系统中模型预测控制（MPC）方案的预测模型所必需的。然而，数据使用会产生收集和利用方面的成本。确定成本最优数据使用需要了解其带来的预测准确性以及结果 MPC 运行性能的影响。本研究使用历史建筑能源数据在一个多建筑能源系统模拟中，研究了简单和最先进的机器学习预测模型在 MPC 中的性能。对于以下数据效率措施，即重新使用预测模型、减少训练数据量、减少模型数据特征和在线模型训练，量化了数据使用对预测准确性的影响。结果显示，简单的线性多层感知器模型提供了与最先进模型相当的预测准确性，且具有更高的数据效率和普适性。

    arXiv:2402.12539v1 Announce Type: cross  Abstract: Data is required to develop forecasting models for use in Model Predictive Control (MPC) schemes in building energy systems. However, data usage incurs costs from both its collection and exploitation. Determining cost optimal data usage requires understanding of the forecast accuracy and resulting MPC operational performance it enables. This study investigates the performance of both simple and state-of-the-art machine learning prediction models for MPC in a multi-building energy system simulation using historic building energy data. The impact of data usage on forecast accuracy is quantified for the following data efficiency measures: reuse of prediction models, reduction of training data volumes, reduction of model data features, and online model training. A simple linear multi-layer perceptron model is shown to provide equivalent forecast accuracy to state-of-the-art models, with greater data efficiency and generalisability. The use
    
[^11]: 精确分数推断：重新参数化及树重新加权和信念传播算法之间的插值

    Exact Fractional Inference via Re-Parametrization & Interpolation between Tree-Re-Weighted- and Belief Propagation- Algorithms

    [https://arxiv.org/abs/2301.10369](https://arxiv.org/abs/2301.10369)

    通过构建$\lambda$-分数全同维数，实现了树重新加权和信念传播算法之间的插值，确保在铁磁性情况下，存在“精确”$\lambda_*$使得计算的配分函数$Z=Z^{(\lambda_*)}$。

    

    推断工作--计算Ising模型在N个“自旋”组成的图上的配分函数$Z$所需的工作--很可能随着N呈指数增长。高效的变分方法，如信念传播（BP）和树重新加权（TRW）算法，通过计算最小化各自（BP或TRW）自由能的$Z$来近似计算$Z$。我们通过构建一个$\lambda$-分数全同维数，$Z^{(\lambda)}$，其中$\lambda=0$和$\lambda=1$分别对应于TRW和BP的近似，且$Z^{(\lambda)}$随$\lambda$单调减少。此外，这种分数方案保证在吸引力（铁磁性）情况下$Z^{(TRW)}\geq Z^{(\lambda)}\geq Z^{(BP)}$，并且存在一个唯一的（“精确”）$\lambda_*$，使得$Z=Z^{(\lambda_*)}$。通过推广\citep {wainwright_tree-based_2002}的重新参数化方法和\citep {chertkov_loop_2006}的环级数方法，我们展示了如何进行e

    arXiv:2301.10369v2 Announce Type: replace  Abstract: Inference efforts -- required to compute partition function, $Z$, of an Ising model over a graph of $N$ ``spins" -- are most likely exponential in $N$. Efficient variational methods, such as Belief Propagation (BP) and Tree Re-Weighted (TRW) algorithms, compute $Z$ approximately minimizing respective (BP- or TRW-) free energy. We generalize the variational scheme building a $\lambda$-fractional-homotopy, $Z^{(\lambda)}$, where $\lambda=0$ and $\lambda=1$ correspond to TRW- and BP-approximations, respectively, and $Z^{(\lambda)}$ decreases with $\lambda$ monotonically. Moreover, this fractional scheme guarantees that in the attractive (ferromagnetic) case $Z^{(TRW)}\geq Z^{(\lambda)}\geq Z^{(BP)}$, and there exists a unique (``exact") $\lambda_*$ such that, $Z=Z^{(\lambda_*)}$. Generalizing the re-parametrization approach of \citep{wainwright_tree-based_2002} and the loop series approach of \citep{chertkov_loop_2006}, we show how to e
    
[^12]: 大规模图的解缠结凝聚

    Disentangled Condensation for Large-scale Graphs. (arXiv:2401.12231v1 [cs.SI])

    [http://arxiv.org/abs/2401.12231](http://arxiv.org/abs/2401.12231)

    本文提出了用于大规模图的解缠结凝聚方法DisCo，通过节点和边的凝聚模块实现了对大规模图的高效缩凝，提高了可扩展性和压缩图的保真度。

    

    图解缠结已经成为一种有趣的技术，为大规模图提供了一种更紧凑但信息丰富的小图，以节省大规模图学习的昂贵成本。尽管取得了有前途的结果，但先前的图解缠结方法常常采用纠缠的缩凝策略，同时涉及节点和边的缩凝，导致大量的GPU内存需求。这种纠缠的策略极大地阻碍了图解缠结的可扩展性，削弱了它对极大规模图的缩凝和高保真度压缩图的能力。因此，本文提出了用于大规模图的解缠结凝聚，简称为DisCo，以提供可扩展的图解缠结，适用于不同规模的图。DisCo的核心是两个互补的组件，即节点和边的凝聚模块，在解缠的方式下实现节点和边的凝聚。

    Graph condensation has emerged as an intriguing technique to provide Graph Neural Networks for large-scale graphs with a more compact yet informative small graph to save the expensive costs of large-scale graph learning. Despite the promising results achieved, previous graph condensation methods often employ an entangled condensation strategy that involves condensing nodes and edges simultaneously, leading to substantial GPU memory demands. This entangled strategy has considerably impeded the scalability of graph condensation, impairing its capability to condense extremely large-scale graphs and produce condensed graphs with high fidelity. Therefore, this paper presents Disentangled Condensation for large-scale graphs, abbreviated as DisCo, to provide scalable graph condensation for graphs of varying sizes. At the heart of DisCo are two complementary components, namely node and edge condensation modules, that realize the condensation of nodes and edges in a disentangled manner. In the 
    
[^13]: 参数矩阵模型

    Parametric Matrix Models. (arXiv:2401.11694v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.11694](http://arxiv.org/abs/2401.11694)

    参数矩阵模型是一种通用机器学习算法，基于矩阵方程设计，通过简化基础方法进行近似解参数方程。它可以仅使用经验数据进行训练，适用于各种机器学习问题，并在计算框架内产生准确的结果。

    

    我们提出了一种称为参数矩阵模型的通用机器学习算法。参数矩阵模型基于矩阵方程，并且其设计受到了用于近似解参数方程的简化基础方法的效率启发。依赖变量可以隐式或显式定义，并且方程可以使用代数、微分或积分关系。参数矩阵模型可以仅使用经验数据进行训练，不需要高保真度模型计算。虽然最初设计用于科学计算，但参数矩阵模型是一种可以应用于通用机器学习问题的通用函数逼近器。在介绍基础理论之后，我们将参数矩阵模型应用于一系列不同的挑战，展示了它们在各种问题上的性能。对于所有在这里测试的挑战，参数矩阵模型在允许计算的框架内产生准确的结果。

    We present a general class of machine learning algorithms called parametric matrix models. Parametric matrix models are based on matrix equations, and the design is motivated by the efficiency of reduced basis methods for approximating solutions of parametric equations. The dependent variables can be defined implicitly or explicitly, and the equations may use algebraic, differential, or integral relations. Parametric matrix models can be trained with empirical data only, and no high-fidelity model calculations are needed. While originally designed for scientific computing, parametric matrix models are universal function approximators that can be applied to general machine learning problems. After introducing the underlying theory, we apply parametric matrix models to a series of different challenges that show their performance for a wide range of problems. For all the challenges tested here, parametric matrix models produce accurate results within a computational framework that allows 
    
[^14]: 闪存LLM：在有限内存下高效运行大型语言模型

    LLM in a flash: Efficient Large Language Model Inference with Limited Memory. (arXiv:2312.11514v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.11514](http://arxiv.org/abs/2312.11514)

    本文提出了一种在有限内存条件下高效运行大型语言模型的方法，通过将模型参数存储在闪存中并按需传输到DRAM的方式来解决内存限制的挑战。该方法通过构建推理成本模型并优化数据传输和读取方式，引入了窗口化和行列绑定两种主要技术。

    

    大型语言模型（LLM）在现代自然语言处理中起着至关重要的作用，在各种任务中表现出色。然而，它们庞大的计算和内存需求带来了挑战，特别是对于具有有限DRAM容量的设备而言。本文通过将模型参数存储在闪存中，并按需将其传输到DRAM的方式，解决了超过可用DRAM容量的LLM高效运行的挑战。我们的方法涉及构建一个考虑闪存特性的推理成本模型，引导我们在两个关键领域进行优化：减少从闪存传输的数据量，并以较大、更连续的块读取数据。在这个受硬件启发的框架内，我们引入了两个主要技术。首先，“窗口化”通过重复使用之前激活的神经元来策略性地减少数据传输，其次，“行列绑定”适应了闪存的顺序数据访问特点，

    Large language models (LLMs) are central to modern natural language processing, delivering exceptional performance in various tasks. However, their substantial computational and memory requirements present challenges, especially for devices with limited DRAM capacity. This paper tackles the challenge of efficiently running LLMs that exceed the available DRAM capacity by storing the model parameters in flash memory, but bringing them on demand to DRAM. Our method involves constructing an inference cost model that takes into account the characteristics of flash memory, guiding us to optimize in two critical areas: reducing the volume of data transferred from flash and reading data in larger, more contiguous chunks. Within this hardware-informed framework, we introduce two principal techniques. First, "windowing" strategically reduces data transfer by reusing previously activated neurons, and second, "row-column bundling", tailored to the sequential data access strengths of flash memory, 
    
[^15]: 早期检测炎症性关节炎以改善转诊的多模态机器学习从血液测试、半结构化和非结构化病历中学习

    Early detection of inflammatory arthritis to improve referrals using multimodal machine learning from blood testing, semi-structured and unstructured patient records. (arXiv:2310.19967v1 [cs.LG])

    [http://arxiv.org/abs/2310.19967](http://arxiv.org/abs/2310.19967)

    通过多模态机器学习从血液测试、半结构化和非结构化病历中学习，早期检测炎症性关节炎可以改善转诊，为及时治疗和预防病情恶化提供决策支持。

    

    早期检测炎症性关节炎（IA）对于高效准确的医院转诊分流以及及时治疗和预防IA疾病进展至关重要，尤其是在有限的医疗资源下。手动评估流程是目前实践中最常见的早期检测IA的方法，但这种方法的劳动密集型和低效率。每个从一般实践（GP）到医院的转诊都需要评估大量的临床信息。机器学习在自动化重复评估任务和提供决策支持方面展示了巨大的潜力，用于早期检测IA。然而，大多数基于机器学习的IA检测方法依赖于血液测试结果。然而，在实践中，血液测试数据并不总是在转诊时可用，因此我们需要利用半结构化和非结构化数据等多模态数据进行早期检测IA的方法。在这项研究中，我们提出了融合和

    Early detection of inflammatory arthritis (IA) is critical to efficient and accurate hospital referral triage for timely treatment and preventing the deterioration of the IA disease course, especially under limited healthcare resources. The manual assessment process is the most common approach in practice for the early detection of IA, but it is extremely labor-intensive and inefficient. A large amount of clinical information needs to be assessed for every referral from General Practice (GP) to the hospitals. Machine learning shows great potential in automating repetitive assessment tasks and providing decision support for the early detection of IA. However, most machine learning-based methods for IA detection rely on blood testing results. But in practice, blood testing data is not always available at the point of referrals, so we need methods to leverage multimodal data such as semi-structured and unstructured data for early detection of IA. In this research, we present fusion and en
    
[^16]: 《Janus接口：大型语言模型微调如何放大隐私风险》

    The Janus Interface: How Fine-Tuning in Large Language Models Amplifies the Privacy Risks. (arXiv:2310.15469v1 [cs.CR])

    [http://arxiv.org/abs/2310.15469](http://arxiv.org/abs/2310.15469)

    《Janus接口：大型语言模型微调如何放大隐私风险》研究了大型语言模型的微调对个人信息泄露的风险，发现了一种新的LLM利用途径。

    

    2018年后的时代标志着大型语言模型（LLM）的出现，OpenAI的ChatGPT等创新展示了惊人的语言能力。随着行业在增加模型参数并利用大量的人类语言数据方面的努力，安全和隐私挑战也出现了。其中最重要的是在基于网络的数据获取过程中，可能会意外积累个人可识别信息（PII），从而导致意外的PII泄露风险。虽然像RLHF和灾难性遗忘这样的策略已被用来控制隐私侵权的风险，但LLM的最新进展（以OpenAI的GPT-3.5的微调界面为代表）重新引发了关注。有人可能会问：LLM的微调是否会导致训练数据集中嵌入的个人信息泄漏？本文报道了首次尝试寻求答案的努力，重点是我们发现了一种新的LLM利用途径。

    The era post-2018 marked the advent of Large Language Models (LLMs), with innovations such as OpenAI's ChatGPT showcasing prodigious linguistic prowess. As the industry galloped toward augmenting model parameters and capitalizing on vast swaths of human language data, security and privacy challenges also emerged. Foremost among these is the potential inadvertent accrual of Personal Identifiable Information (PII) during web-based data acquisition, posing risks of unintended PII disclosure. While strategies like RLHF during training and Catastrophic Forgetting have been marshaled to control the risk of privacy infringements, recent advancements in LLMs, epitomized by OpenAI's fine-tuning interface for GPT-3.5, have reignited concerns. One may ask: can the fine-tuning of LLMs precipitate the leakage of personal information embedded within training datasets? This paper reports the first endeavor to seek the answer to the question, particularly our discovery of a new LLM exploitation avenue
    
[^17]: Raman放大器的优化：黑盒、灰盒和白盒模型的比较

    Optimization of Raman amplifiers: a comparison between black-, grey- and white-box modeling. (arXiv:2310.05954v1 [physics.app-ph])

    [http://arxiv.org/abs/2310.05954](http://arxiv.org/abs/2310.05954)

    该论文比较了白盒、灰盒和黑盒模型在双向Raman放大器中实现目标频距放大的能力，并展示了它们都可以实现C波段下达到1dB的频距平坦度。

    

    在光通信系统不断努力提高吞吐量的过程中，设计和优化光放大器以最大化系统性能变得越来越重要。离线优化光放大器依赖于从深入物理学的白盒模型到数据驱动的与物理无关的黑盒模型的各种模型。在这里，我们比较了白盒、灰盒和黑盒模型在双向Raman放大器中实现目标频距放大的能力。我们展示了研究的任何一种方法都可以在100公里范围内实现C波段下达到1dB的频距平坦度。然后，我们根据目标应用场景讨论了模型的适用性、优势和缺点，特别是在优化速度和训练数据访问方面。

    Designing and optimizing optical amplifiers to maximize system performance is becoming increasingly important as optical communication systems strive to increase throughput. Offline optimization of optical amplifiers relies on models ranging from white-box models deeply rooted in physics to black-box data-driven physics-agnostic models. Here, we compare the capabilities of white-, grey- and black-box models to achieve a target frequency-distance amplification in a bidirectional Raman amplifier. We show that any of the studied methods can achieve down to 1 dB of frequency-distance flatness over the C-band in a 100-km span. Then, we discuss the models' applicability, advantages, and drawbacks based on the target application scenario, in particular in terms of optimization speed and access to training data.
    
[^18]: 课堂数据分析复制：教学生，同时测试科学

    In-class Data Analysis Replications: Teaching Students while Testing Science. (arXiv:2308.16491v1 [cs.CY])

    [http://arxiv.org/abs/2308.16491](http://arxiv.org/abs/2308.16491)

    这项研究揭示了课堂数据分析复制的可行性，以及这种方法对学生、教育者和科学家的成本与收益。同时，学生对数据的预期与实际情况存在差异。

    

    科学正面临可重复性危机。先前的工作提出将数据分析复制纳入课堂作为潜在解决方案。然而，尽管潜在的好处，目前尚不清楚这一方法是否可行，如果可行，涉及的利益相关者-学生、教育者和科学家-应该期望什么。学生能够在课堂上进行数据分析复制吗？教育者的成本与收益如何？这个解决方案如何帮助评估和改进科学的现状？本研究在EPFL教授的应用数据分析课程（CS-401）的项目部分中纳入了数据分析复制（N=354名学生）。在此报告中，我们基于课程期间进行的调查提前进行注册的发现。首先，我们证明学生可以复制先前发表的科学论文，大部分是定性的，有些是完全一样的。我们发现学生对数据的预期与实际情况存在差异

    Science is facing a reproducibility crisis. Previous work has proposed incorporating data analysis replications into classrooms as a potential solution. However, despite the potential benefits, it is unclear whether this approach is feasible, and if so, what the involved stakeholders-students, educators, and scientists-should expect from it. Can students perform a data analysis replication over the course of a class? What are the costs and benefits for educators? And how can this solution help benchmark and improve the state of science?  In the present study, we incorporated data analysis replications in the project component of the Applied Data Analysis course (CS-401) taught at EPFL (N=354 students). Here we report pre-registered findings based on surveys administered throughout the course. First, we demonstrate that students can replicate previously published scientific papers, most of them qualitatively and some exactly. We find discrepancies between what students expect of data an
    
[^19]: 扩展AI：向拥有创造性的AlphaZero国际象棋迈进

    Diversifying AI: Towards Creative Chess with AlphaZero. (arXiv:2308.09175v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2308.09175](http://arxiv.org/abs/2308.09175)

    本研究探索了AI在计算任务中是否可以从创造性决策机制中受益，并通过构建多样化的AI系统团队，在挑战性任务中超越单个AI，通过生成更多的想法，并选择最佳想法。在国际象棋中的实验结果显示，多样化AI系统以不同方式下国际象棋。

    

    近年来，人工智能系统在各种计算任务上已经超过了人类的智能。然而，与人类一样，AI系统也会犯错误，有盲点，产生幻觉，并且在面对新情况时很难进行泛化。本研究探讨了当AI系统的计算合理性推到极限时，是否可以从创造性的决策机制中受益。特别是，我们研究了是否通过作为一个团队的多样化AI系统在具有挑战性的任务中可以胜过单个AI，通过生成更多的想法，然后选择最好的想法。我们以国际象棋这个被称为AI果蝇的游戏为例进行了研究。我们在AlphaZero (AZ)的基础上，通过潜变条件架构扩展它，构建了一个代理团队，我们称之为AZ_db。我们使用行为多样性技术对AZ_db进行训练，以生成更广泛的想法，并通过次加性计划选择最有希望的想法。我们的实验表明，AZ_db以不同方式下国际象棋。

    In recent years, Artificial Intelligence (AI) systems have surpassed human intelligence in a variety of computational tasks. However, AI systems, like humans, make mistakes, have blind spots, hallucinate, and struggle to generalize to new situations. This work explores whether AI can benefit from creative decision-making mechanisms when pushed to the limits of its computational rationality. In particular, we investigate whether a team of diverse AI systems can outperform a single AI in challenging tasks by generating more ideas as a group and then selecting the best ones. We study this question in the game of chess, the so-called drosophila of AI. We build on AlphaZero (AZ) and extend it to represent a league of agents via a latent-conditioned architecture, which we call AZ_db. We train AZ_db to generate a wider range of ideas using behavioral diversity techniques and select the most promising ones with sub-additive planning. Our experiments suggest that AZ_db plays chess in diverse wa
    
[^20]: 硬约束PINNs用于界面最优控制问题

    The Hard-Constraint PINNs for Interface Optimal Control Problems. (arXiv:2308.06709v1 [math.OC])

    [http://arxiv.org/abs/2308.06709](http://arxiv.org/abs/2308.06709)

    本研究将物理信息神经网络（PINNs）与不连续性捕获神经网络相结合，应用于具有界面和控制约束的优化控制问题，并通过将边界和界面条件作为硬约束以确保数值精度。

    

    我们展示了物理信息神经网络（PINNs）与最近开发的不连续性捕获神经网络相结合，可以应用于具有界面和一些控制约束的偏微分方程（PDE）优化控制问题的求解。该算法无网格且可扩展到不同的PDE，并确保严格满足控制约束。由于边界和界面条件以及PDE都被视为软约束，通过将它们汇总到加权损失函数中进行处理，因此需要同时学习它们，并且不能保证边界和界面条件能够完全满足。这立即引起了在相应的损失函数中调整权重和训练神经网络的困难。为了解决这些困难并确保数值精度，我们提出在PINNs中将边界和界面条件作为硬约束。

    We show that the physics-informed neural networks (PINNs), in combination with some recently developed discontinuity capturing neural networks, can be applied to solve optimal control problems subject to partial differential equations (PDEs) with interfaces and some control constraints. The resulting algorithm is mesh-free and scalable to different PDEs, and it ensures the control constraints rigorously. Since the boundary and interface conditions, as well as the PDEs, are all treated as soft constraints by lumping them into a weighted loss function, it is necessary to learn them simultaneously and there is no guarantee that the boundary and interface conditions can be satisfied exactly. This immediately causes difficulties in tuning the weights in the corresponding loss function and training the neural networks. To tackle these difficulties and guarantee the numerical accuracy, we propose to impose the boundary and interface conditions as hard constraints in PINNs by developing a nove
    
[^21]: SILO语言模型：在非参数化数据存储中隔离法律风险

    SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore. (arXiv:2308.04430v1 [cs.CL])

    [http://arxiv.org/abs/2308.04430](http://arxiv.org/abs/2308.04430)

    SILO是一种新的语言模型，通过在推理过程中对非参数化的数据存储进行查询，实现在面临法律风险和模型性能之间的权衡，并支持数据归属和数据生产者退出模型的功能。

    

    在对将语言模型（LMs）训练在受版权或受其他限制的数据上的合法性进行激烈辩论的同时，我们展示了仅在低风险文本（例如过期版权图书或政府文件）上训练时，模型性能显著下降的问题，原因是该文本的规模和领域覆盖有限。我们提出了SILO，一种新的语言模型，在推理过程中管理这种风险-性能权衡。SILO通过以下方式构建：（1）在我们策划的新语料库“开放许可证语料库”（OLC）上训练参数化的LM，该语料库包含228B个公共领域和许可文本。（2）通过非参数化的数据存储（例如包含受版权保护的图书或新闻的数据）对其进行扩充，该数据存储仅在推理过程中被查询。该数据存储允许使用高风险数据而无需对其进行训练，支持句级数据归属，并使数据生产者可以通过从存储中删除内容来选择退出模型。这些功能可以促进对数据使用规范的遵循。

    The legality of training language models (LMs) on copyrighted or otherwise restricted data is under intense debate. However, as we show, model performance significantly degrades if trained only on low-risk text (e.g., out-of-copyright books or government documents), due to its limited size and domain coverage. We present SILO, a new language model that manages this risk-performance tradeoff during inference. SILO is built by (1) training a parametric LM on Open License Corpus (OLC), a new corpus we curate with 228B tokens of public domain and permissively licensed text and (2) augmenting it with a more general and easily modifiable nonparametric datastore (e.g., containing copyrighted books or news) that is only queried during inference. The datastore allows use of high-risk data without training on it, supports sentence-level data attribution, and enables data producers to opt out from the model by removing content from the store. These capabilities can foster compliance with data-use
    
[^22]: 火星时间序列分解：一种多尺度嵌套方法中的因子变分自编码器

    Martian time-series unraveled: A multi-scale nested approach with factorial variational autoencoders. (arXiv:2305.16189v1 [cs.LG])

    [http://arxiv.org/abs/2305.16189](http://arxiv.org/abs/2305.16189)

    该论文提出了一种因子高斯混合变分自动编码器，用于多尺度聚类和源分离，通过利用小波散射协方差来提供随机过程的低维表示，能够区分不同的非高斯随机过程，并在MRO数据集上展现了更好的性能。

    

    无监督的源分离涉及通过混合操作记录的未知源信号的分解，其中对源的先验知识有限，仅可以访问信号混合数据集。这个问题本质上是不适用的，并且进一步受到时间序列数据中源展现出的多种时间尺度的挑战。为了解决这个问题，我们提出了一种无监督的多尺度聚类和源分离框架，通过利用小波散射协方差来提供随机过程的低维表示，能够区分不同的非高斯随机过程。在这个表示空间中，我们开发了一个因子高斯混合变分自动编码器，它被训练用于(1)概率地对不同时间尺度上的源进行聚类和逐层非监督源分离，(2)在每个时间尺度上提取低维表示，(3)学习源信号的因子表示，(4)在表示空间中进行采样，以生成未知源信号。我们在MRO上的三个频道的可见数据集上进行了评估，结果表明所提出的方法比目前最先进的技术具有更好的性能。

    Unsupervised source separation involves unraveling an unknown set of source signals recorded through a mixing operator, with limited prior knowledge about the sources, and only access to a dataset of signal mixtures. This problem is inherently ill-posed and is further challenged by the variety of time-scales exhibited by sources in time series data. Existing methods typically rely on a preselected window size that limits their capacity to handle multi-scale sources. To address this issue, instead of operating in the time domain, we propose an unsupervised multi-scale clustering and source separation framework by leveraging wavelet scattering covariances that provide a low-dimensional representation of stochastic processes, capable of distinguishing between different non-Gaussian stochastic processes. Nested within this representation space, we develop a factorial Gaussian-mixture variational autoencoder that is trained to (1) probabilistically cluster sources at different time-scales a
    

