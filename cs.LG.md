# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Data-Driven Room Acoustic Modeling Via Differentiable Feedback Delay Networks With Learnable Delay Lines](https://arxiv.org/abs/2404.00082) | 通过可学习延迟线实现可微分反馈延迟网络的参数优化，实现了对室内声学特性的数据驱动建模。 |
| [^2] | [Speech Robust Bench: A Robustness Benchmark For Speech Recognition](https://arxiv.org/abs/2403.07937) | 提出了一个全面基准（SRB），用于评估自动语音识别（ASR）模型对各种破坏的鲁棒性，发现模型大小和某些建模选择有助于提高鲁棒性，并观察到在不同人口亚组上模型的鲁棒性存在明显差异。 |
| [^3] | [ChatDiet: Empowering Personalized Nutrition-Oriented Food Recommender Chatbots through an LLM-Augmented Framework](https://arxiv.org/abs/2403.00781) | 这项研究介绍了ChatDiet，一个借助LLM技术构建的框架，能够帮助个性化营养导向食品推荐聊天机器人提供个性化和可解释的推荐。 |
| [^4] | [Symbolic Music Generation with Non-Differentiable Rule Guided Diffusion](https://arxiv.org/abs/2402.14285) | 介绍了一种用于符号音乐生成的不可微分规则引导的新方法，引入了可以与之即插即用的高时间分辨率潜在扩散架构，对音乐质量取得了显著进步 |
| [^5] | [Improvement and generalization of ABCD method with Bayesian inference](https://arxiv.org/abs/2402.08001) | 通过利用贝叶斯推断方法，改进和推广了ABCD方法，提供了一种更好的描述信号和背景的方式，并利用先验知识和不同可观测量之间的依赖关系来提高测量精度。 |
| [^6] | [Are LLMs Ready for Real-World Materials Discovery?](https://arxiv.org/abs/2402.05200) | LLMs在材料科学中的应用受限，无法实现实际应用。我们提出了基于材料科学知识和假设测试的MatSci-LLMs框架，并描述了关键的材料科学信息提取挑战。 |
| [^7] | [How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning](https://arxiv.org/abs/2402.02872) | 本论文探索了大型语言模型如何进行上下文学习的机制，提出了一个使用定位和投影方法的假设。通过查询和键矩阵来计算输入文本与每个演示之间的注意力权重，以学习它们之间的相似度度量。实验证明了我们的分析。 |
| [^8] | [Continual Adversarial Defense](https://arxiv.org/abs/2312.09481) | 提出了第一个能够动态适应任何攻击的持续对抗性防御（CAD）框架。 |
| [^9] | [Uncertainty-aware Surrogate Models for Airfoil Flow Simulations with Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2312.05320) | 本研究使用去噪扩散概率模型为气动翼流场模拟训练了不确定性感知代理模型，成功捕捉整个解分布并准确估计模拟不确定性。 |
| [^10] | [Semi-Supervised Learning Approach for Efficient Resource Allocation with Network Slicing in O-RAN.](http://arxiv.org/abs/2401.08861) | 本文提出了一种半监督学习方法，解决了O-RAN中网络切片和资源分配的问题。通过设计两个xAPPs，分别处理功率控制和物理资源块分配，我们的方法能够在用户设备之间实现最大化的加权吞吐量，并优先考虑增强型移动宽带和超可靠低延迟通信这两种服务类型。 |
| [^11] | [Realism in Action: Anomaly-Aware Diagnosis of Brain Tumors from Medical Images Using YOLOv8 and DeiT.](http://arxiv.org/abs/2401.03302) | 本研究利用深度学习技术在具有挑战性的情况下检测和分类脑肿瘤，并解决了在罕见情况下的肿瘤检测问题。研究使用了来自国家脑映射实验室的数据集，通过修改样本数量和患者分布，使模型能够应对真实世界场景中的异常情况。 |
| [^12] | [Locating Factual Knowledge in Large Language Models: Exploring the Residual Stream and Analyzing Subvalues in Vocabulary Space.](http://arxiv.org/abs/2312.12141) | 通过探索剩余流和分析词汇空间中的子值，我们定位了大型语言模型中的事实知识，并找到了存储了有关“法国，首都，巴黎”的知识的位置。 |
| [^13] | [Quantum Long Short-Term Memory (QLSTM) vs Classical LSTM in Time Series Forecasting: A Comparative Study in Solar Power Forecasting.](http://arxiv.org/abs/2310.17032) | 本研究比较了量子长短期记忆（QLSTM）和经典长短期记忆（LSTM）模型在太阳能发电量预测中的表现，发现QLSTM具有加快训练收敛速度和减小测试损失的优势，拥有吸纳复杂时间序列关系的潜力，但还需要进一步研究和优化。 |
| [^14] | [Improving Robustness and Reliability in Medical Image Classification with Latent-Guided Diffusion and Nested-Ensembles.](http://arxiv.org/abs/2310.15952) | 本文引入了一种新颖的三阶段方法，通过变换器和条件扩散模型来改善医学图像分类模型对实际应用中常见成像变异性的鲁棒性。 |
| [^15] | [Optimal vintage factor analysis with deflation varimax.](http://arxiv.org/abs/2310.10545) | 本文提出了一种采用通货紧缩变量旋转的拟合因子分析方法，在每一行上逐步求解正交矩阵，相比于传统方法具有更好的计算性能和灵活性，并且在更广泛的背景下提供了理论保证。 |
| [^16] | [Non-backtracking Graph Neural Networks.](http://arxiv.org/abs/2310.07430) | 非回溯图神经网络(NBA-GNN)通过不考虑先前访问节点的消息来解决图神经网络本地更新中的冗余问题，并且在随机块模型恢复方面表现出良好的性能。 |
| [^17] | [Non-Smooth Weakly-Convex Finite-sum Coupled Compositional Optimization.](http://arxiv.org/abs/2310.03234) | 本文研究了一种新的组合优化问题，称为非光滑弱凸有限和耦合组合优化(NSWC FCCO)，通过扩展已有的研究，我们研究了非光滑弱凸FCCO的问题，并提出了一种单循环算法来找到Moreau环的ε-稳定点。 |
| [^18] | [WASA: WAtermark-based Source Attribution for Large Language Model-Generated Data.](http://arxiv.org/abs/2310.00646) | 本文提出了一种基于水印的框架WASA，通过允许大型语言模型生成带有嵌入源信息的合成文本水印来解决源归属和数据来源的问题。 |
| [^19] | [Benchmarking Cognitive Biases in Large Language Models as Evaluators.](http://arxiv.org/abs/2309.17012) | 本研究对15个不同大小的大型语言模型进行了评估，发现它们作为评估器存在认知偏差，尤其在文本质量评估中表现出较强的偏见，这对其鲁棒性提出了质疑。同时，研究还发现了人类和机器偏好之间的相关性。 |
| [^20] | [On Collaboration in Distributed Parameter Estimation with Resource Constraints.](http://arxiv.org/abs/2307.06442) | 在资源约束下的分布参数估计中，我们研究了传感器/代理数据收集和协作策略，通过最大化费舍尔信息或最小化Cramer-Rao界来解决传感器/代理的数据收集和协作策略设计问题。 |
| [^21] | [Fast Distributed Inference Serving for Large Language Models.](http://arxiv.org/abs/2305.05920) | FastServe是一种针对大型语言模型的分布式推理服务系统，利用抢占式调度和跳过-连接多级反馈队列，最小化模型推断的作业完成时间(JCT)。 |
| [^22] | [An Interpretable Machine Learning System to Identify EEG Patterns on the Ictal-Interictal-Injury Continuum.](http://arxiv.org/abs/2211.05207) | 该论文设计了一种可解释的深度学习模型，以预测ICU脑电监测中常见的6种脑波图案的存在，并提供高质量的解释和三种解释方法，这对于建立AI的信任和临床采用至关重要。 |

# 详细

[^1]: 基于可微分反馈延迟网络和可学习延迟线的数据驱动室内声学建模

    Data-Driven Room Acoustic Modeling Via Differentiable Feedback Delay Networks With Learnable Delay Lines

    [https://arxiv.org/abs/2404.00082](https://arxiv.org/abs/2404.00082)

    通过可学习延迟线实现可微分反馈延迟网络的参数优化，实现了对室内声学特性的数据驱动建模。

    

    在过去的几十年中，人们致力于设计人工混响算法，旨在模拟物理环境的室内声学。尽管取得了显著进展，但延迟网络模型的自动参数调整仍然是一个开放性挑战。我们提出了一种新方法，通过学习可微分反馈延迟网络（FDN）的参数，使其输出呈现出所测得的室内脉冲响应的感知特性。

    arXiv:2404.00082v1 Announce Type: cross  Abstract: Over the past few decades, extensive research has been devoted to the design of artificial reverberation algorithms aimed at emulating the room acoustics of physical environments. Despite significant advancements, automatic parameter tuning of delay-network models remains an open challenge. We introduce a novel method for finding the parameters of a Feedback Delay Network (FDN) such that its output renders the perceptual qualities of a measured room impulse response. The proposed approach involves the implementation of a differentiable FDN with trainable delay lines, which, for the first time, allows us to simultaneously learn each and every delay-network parameter via backpropagation. The iterative optimization process seeks to minimize a time-domain loss function incorporating differentiable terms accounting for energy decay and echo density. Through experimental validation, we show that the proposed method yields time-invariant freq
    
[^2]: 语音鲁棒基准：用于语音识别的鲁棒性基准

    Speech Robust Bench: A Robustness Benchmark For Speech Recognition

    [https://arxiv.org/abs/2403.07937](https://arxiv.org/abs/2403.07937)

    提出了一个全面基准（SRB），用于评估自动语音识别（ASR）模型对各种破坏的鲁棒性，发现模型大小和某些建模选择有助于提高鲁棒性，并观察到在不同人口亚组上模型的鲁棒性存在明显差异。

    

    随着自动语音识别（ASR）模型变得越来越普遍，确保它们在物理世界和数字世界中的各种破坏下进行可靠预测变得愈发重要。我们提出了语音鲁棒基准（SRB），这是一个用于评估ASR模型对各种破坏的鲁棒性的全面基准。SRB由69个输入扰动组成，旨在模拟ASR模型可能在物理世界和数字世界中遇到的各种破坏。我们使用SRB来评估几种最先进的ASR模型的鲁棒性，并观察到模型大小和某些建模选择（如离散表示和自我训练）似乎有助于提高鲁棒性。我们将此分析扩展到衡量ASR模型在来自各种人口亚组的数据上的鲁棒性，即英语和西班牙语使用者以及男性和女性，并观察到模型的鲁棒性在不同亚组之间存在明显差异。

    arXiv:2403.07937v1 Announce Type: cross  Abstract: As Automatic Speech Recognition (ASR) models become ever more pervasive, it is important to ensure that they make reliable predictions under corruptions present in the physical and digital world. We propose Speech Robust Bench (SRB), a comprehensive benchmark for evaluating the robustness of ASR models to diverse corruptions. SRB is composed of 69 input perturbations which are intended to simulate various corruptions that ASR models may encounter in the physical and digital world. We use SRB to evaluate the robustness of several state-of-the-art ASR models and observe that model size and certain modeling choices such as discrete representations, and self-training appear to be conducive to robustness. We extend this analysis to measure the robustness of ASR models on data from various demographic subgroups, namely English and Spanish speakers, and males and females, and observed noticeable disparities in the model's robustness across su
    
[^3]: ChatDiet：通过LLM增强框架赋能个性化营养导向食品推荐聊天机器人

    ChatDiet: Empowering Personalized Nutrition-Oriented Food Recommender Chatbots through an LLM-Augmented Framework

    [https://arxiv.org/abs/2403.00781](https://arxiv.org/abs/2403.00781)

    这项研究介绍了ChatDiet，一个借助LLM技术构建的框架，能够帮助个性化营养导向食品推荐聊天机器人提供个性化和可解释的推荐。

    

    食物对健康的深远影响使得先进的营养导向食品推荐服务成为必要。传统方法往往缺乏个性化、可解释性和互动性等关键元素。虽然大型语言模型（LLMs）带来了解释性和可解释性，但它们单独的使用未能实现真正的个性化。本文介绍了ChatDiet，一种新颖的LLM驱动框架，专门设计用于个性化营养导向食品推荐聊天机器人。ChatDiet集成了个人和人群模型，辅以一个协调器，无缝检索和处理相关信息。其结果是动态提供个性化和可解释的食品推荐，根据个人用户喜好定制。我们对ChatDiet进行了评估，包括一个引人入胜的案例研究，在案例研究中建立了一个因果个人模型来估计个人营养效果。

    arXiv:2403.00781v1 Announce Type: cross  Abstract: The profound impact of food on health necessitates advanced nutrition-oriented food recommendation services. Conventional methods often lack the crucial elements of personalization, explainability, and interactivity. While Large Language Models (LLMs) bring interpretability and explainability, their standalone use falls short of achieving true personalization. In this paper, we introduce ChatDiet, a novel LLM-powered framework designed specifically for personalized nutrition-oriented food recommendation chatbots. ChatDiet integrates personal and population models, complemented by an orchestrator, to seamlessly retrieve and process pertinent information. The result is a dynamic delivery of personalized and explainable food recommendations, tailored to individual user preferences. Our evaluation of ChatDiet includes a compelling case study, where we establish a causal personal model to estimate individual nutrition effects. Our assessmen
    
[^4]: 具有不可微分规则引导扩散的符号音乐生成

    Symbolic Music Generation with Non-Differentiable Rule Guided Diffusion

    [https://arxiv.org/abs/2402.14285](https://arxiv.org/abs/2402.14285)

    介绍了一种用于符号音乐生成的不可微分规则引导的新方法，引入了可以与之即插即用的高时间分辨率潜在扩散架构，对音乐质量取得了显著进步

    

    我们研究了符号音乐生成的问题（例如生成钢琴卷谱），技术重点放在不可微分规则引导上。音乐规则通常以符号形式表达在音符特征上，如音符密度或和弦进行，许多规则是不可微分的，这在使用它们进行引导扩散时存在挑战。我们提出了一种新颖的引导方法，称为随机控制引导（SCG），它仅需要对规则函数进行前向评估，可以与预训练的扩散模型以即插即用的方式一起工作，从而首次实现了对不可微分规则的无训练引导。此外，我们引入了一种用于符号音乐生成的高时间分辨率潜在扩散架构，可以与SCG以即插即用的方式组合。与符号音乐生成中的标准强基线相比，该框架在音乐质量方面展示了明显的进展

    arXiv:2402.14285v1 Announce Type: cross  Abstract: We study the problem of symbolic music generation (e.g., generating piano rolls), with a technical focus on non-differentiable rule guidance. Musical rules are often expressed in symbolic form on note characteristics, such as note density or chord progression, many of which are non-differentiable which pose a challenge when using them for guided diffusion. We propose Stochastic Control Guidance (SCG), a novel guidance method that only requires forward evaluation of rule functions that can work with pre-trained diffusion models in a plug-and-play way, thus achieving training-free guidance for non-differentiable rules for the first time. Additionally, we introduce a latent diffusion architecture for symbolic music generation with high time resolution, which can be composed with SCG in a plug-and-play fashion. Compared to standard strong baselines in symbolic music generation, this framework demonstrates marked advancements in music quali
    
[^5]: 改进并推广具有贝叶斯推断的ABCD方法

    Improvement and generalization of ABCD method with Bayesian inference

    [https://arxiv.org/abs/2402.08001](https://arxiv.org/abs/2402.08001)

    通过利用贝叶斯推断方法，改进和推广了ABCD方法，提供了一种更好的描述信号和背景的方式，并利用先验知识和不同可观测量之间的依赖关系来提高测量精度。

    

    在LHC上寻找新物理或精确我们对标准模型的认识是一个涉及多个因素的企业。我们专注于利用现有信息，并重新思考常规的基于数据的ABCD方法，通过使用贝叶斯机器学习工具来改进和推广它。我们建议使用混合模型来描述由信号和多个背景组成的数据集。通过利用先验知识和事件之间的依赖关系，可以很好地提取出样本中的信号、背景及其相对分数。与ABCD方法相比，我们展示了如何利用对不同背景的某些属性的理解以及每个事件中需要测量的超过两个独立可观测量的优势。此外，贝叶斯框架使用连续分布的信息来定义区域，而不是通过硬切割定义区域。

    To find New Physics or to refine our knowledge of the Standard Model at the LHC is an enterprise that involves many factors. We focus on taking advantage of available information and pour our effort in re-thinking the usual data-driven ABCD method to improve it and to generalize it using Bayesian Machine Learning tools. We propose that a dataset consisting of a signal and many backgrounds is well described through a mixture model. Signal, backgrounds and their relative fractions in the sample can be well extracted by exploiting the prior knowledge and the dependence between the different observables at the event-by-event level with Bayesian tools. We show how, in contrast to the ABCD method, one can take advantage of understanding some properties of the different backgrounds and of having more than two independent observables to measure in each event. In addition, instead of regions defined through hard cuts, the Bayesian framework uses the information of continuous distribution to obt
    
[^6]: LLMs是否准备好应用于真实世界的材料发现？

    Are LLMs Ready for Real-World Materials Discovery?

    [https://arxiv.org/abs/2402.05200](https://arxiv.org/abs/2402.05200)

    LLMs在材料科学中的应用受限，无法实现实际应用。我们提出了基于材料科学知识和假设测试的MatSci-LLMs框架，并描述了关键的材料科学信息提取挑战。

    

    大型语言模型（LLMs）为材料科学中的强大语言处理工具提供了令人兴奋的可能性，加快了材料研究的进展。然而，LLMs在实际应用中仍存在不足，无法成为实用的材料科学工具。本文通过展示LLMs在材料科学中的相关失败案例，揭示了LLMs在理解和推理复杂、相互关联的材料科学知识方面的现有限制。鉴于这些缺点，我们提出了一种开发基于材料科学知识和假设生成与测试的材料科学LLMs（MatSci-LLMs）的框架。实现高性能的MatSci-LLMs的路径在很大程度上取决于建立高质量的多模态数据集，这些数据集来自科学文献，其中存在各种信息提取挑战。因此，我们描述了关键的材料科学信息提取挑战。

    Large Language Models (LLMs) create exciting possibilities for powerful language processing tools to accelerate research in materials science. While LLMs have great potential to accelerate materials understanding and discovery, they currently fall short in being practical materials science tools. In this position paper, we show relevant failure cases of LLMs in materials science that reveal current limitations of LLMs related to comprehending and reasoning over complex, interconnected materials science knowledge. Given those shortcomings, we outline a framework for developing Materials Science LLMs (MatSci-LLMs) that are grounded in materials science knowledge and hypothesis generation followed by hypothesis testing. The path to attaining performant MatSci-LLMs rests in large part on building high-quality, multi-modal datasets sourced from scientific literature where various information extraction challenges persist. As such, we describe key materials science information extraction cha
    
[^7]: 大型语言模型如何进行上下文学习？查询和键矩阵是上下文头部进行度量学习的两个关键组成部分

    How do Large Language Models Learn In-Context? Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning

    [https://arxiv.org/abs/2402.02872](https://arxiv.org/abs/2402.02872)

    本论文探索了大型语言模型如何进行上下文学习的机制，提出了一个使用定位和投影方法的假设。通过查询和键矩阵来计算输入文本与每个演示之间的注意力权重，以学习它们之间的相似度度量。实验证明了我们的分析。

    

    我们探索了上下文学习的机制，并提出了使用定位和投影方法的假设。在浅层中，演示的特征被合并到相应的标签中，输入文本的特征被聚合到最后一个标记中。在深层中，上下文头部发挥了重要作用。在每个上下文头部中，值-输出矩阵提取了标签的特征。查询和键矩阵计算了输入文本与每个演示之间的注意力权重。注意力权重越大，越多的标签信息被传输到最后一个标记中，用于预测下一个单词。查询和键矩阵可以被视为学习输入文本与每个演示之间相似度度量的两个关键组成部分。基于这个假设，我们解释了为什么不平衡的标签和演示顺序会影响预测。我们在GPT2大型、Llama 7B、13B和30B上进行了实验。结果支持我们的分析。总体而言，我们的研究提供了一个关于大型语言模型如何进行上下文学习的理论解释和验证。

    We explore the mechanism of in-context learning and propose a hypothesis using locate-and-project method. In shallow layers, the features of demonstrations are merged into their corresponding labels, and the features of the input text are aggregated into the last token. In deep layers, in-context heads make great contributions. In each in-context head, the value-output matrix extracts the labels' features. Query and key matrices compute the attention weights between the input text and each demonstration. The larger the attention weight is, the more label information is transferred into the last token for predicting the next word. Query and key matrices can be regarded as two towers for learning the similarity metric between the input text and each demonstration. Based on this hypothesis, we explain why imbalanced labels and demonstration order affect predictions. We conduct experiments on GPT2 large, Llama 7B, 13B and 30B. The results can support our analysis. Overall, our study provid
    
[^8]: 持续不断的对抗性防御

    Continual Adversarial Defense

    [https://arxiv.org/abs/2312.09481](https://arxiv.org/abs/2312.09481)

    提出了第一个能够动态适应任何攻击的持续对抗性防御（CAD）框架。

    

    针对每月针对视觉分类器的对抗性攻击快速演变的特性，人们提出了许多防御方法，旨在尽可能通用化以抵御尽可能多的已知攻击。然而，设计一个能够对抗所有类型攻击的防御方法并不现实，因为防御系统运行的环境是动态的，包含随着时间出现的各种独特攻击。防御系统必须收集在线少样本对抗反馈以迅速增强自身，充分利用内存。因此，我们提出了第一个能够动态适应任何攻击的持续对抗性防御（CAD）框架，其中各种攻击逐个阶段出现。在实践中，CAD基于四项原则进行建模：(1) 持续适应新攻击而无灾难性遗忘，(2) 少样本适应，(3) 内存高效适应，以及(4) 高准确性

    arXiv:2312.09481v2 Announce Type: replace-cross  Abstract: In response to the rapidly evolving nature of adversarial attacks against visual classifiers on a monthly basis, numerous defenses have been proposed to generalize against as many known attacks as possible. However, designing a defense method that generalizes to all types of attacks is not realistic because the environment in which defense systems operate is dynamic and comprises various unique attacks that emerge as time goes on. The defense system must gather online few-shot defense feedback to promptly enhance itself, leveraging efficient memory utilization. Therefore, we propose the first continual adversarial defense (CAD) framework that adapts to any attacks in a dynamic scenario, where various attacks emerge stage by stage. In practice, CAD is modeled under four principles: (1) continual adaptation to new attacks without catastrophic forgetting, (2) few-shot adaptation, (3) memory-efficient adaptation, and (4) high accur
    
[^9]: 使用去噪扩散概率模型的气动翼流场模拟的不确定性感知代理模型

    Uncertainty-aware Surrogate Models for Airfoil Flow Simulations with Denoising Diffusion Probabilistic Models

    [https://arxiv.org/abs/2312.05320](https://arxiv.org/abs/2312.05320)

    本研究使用去噪扩散概率模型为气动翼流场模拟训练了不确定性感知代理模型，成功捕捉整个解分布并准确估计模拟不确定性。

    

    将神经网络作为湍流模拟的代理模型是一个备受关注的话题。同时，在代理模型的预测中体现模拟的固有不确定性仍然非常具有挑战性。本研究首次尝试使用去噪扩散概率模型（DDPMs）训练一个用于湍流模拟的不确定性感知代理模型。由于其普遍性，选择以各种形状、雷诺数和攻角的气动翼流场模拟作为学习目标。我们的结果表明，DDPMs能成功捕捉解的整个分布，从而准确估计模拟的不确定性。DDPMs的性能还与以贝叶斯神经网络和异方差模型形式的不同基准进行了比较。实验证明，DDPMs在性能上优于其他方法。

    arXiv:2312.05320v2 Announce Type: replace-cross  Abstract: Leveraging neural networks as surrogate models for turbulence simulation is a topic of growing interest. At the same time, embodying the inherent uncertainty of simulations in the predictions of surrogate models remains very challenging. The present study makes a first attempt to use denoising diffusion probabilistic models (DDPMs) to train an uncertainty-aware surrogate model for turbulence simulations. Due to its prevalence, the simulation of flows around airfoils with various shapes, Reynolds numbers, and angles of attack is chosen as the learning objective. Our results show that DDPMs can successfully capture the whole distribution of solutions and, as a consequence, accurately estimate the uncertainty of the simulations. The performance of DDPMs is also compared with varying baselines in the form of Bayesian neural networks and heteroscedastic models. Experiments demonstrate that DDPMs outperform the other methods regardin
    
[^10]: O-RAN中利用半监督学习方法进行网络切片的资源分配的研究

    Semi-Supervised Learning Approach for Efficient Resource Allocation with Network Slicing in O-RAN. (arXiv:2401.08861v1 [cs.NI])

    [http://arxiv.org/abs/2401.08861](http://arxiv.org/abs/2401.08861)

    本文提出了一种半监督学习方法，解决了O-RAN中网络切片和资源分配的问题。通过设计两个xAPPs，分别处理功率控制和物理资源块分配，我们的方法能够在用户设备之间实现最大化的加权吞吐量，并优先考虑增强型移动宽带和超可靠低延迟通信这两种服务类型。

    

    开放式无线接入网络（O-RAN）技术作为一种有前景的解决方案，为网络运营商提供了一个开放和有利的环境。在O-RAN内确保有效地协调x应用程序（xAPPs）对于网络切片和资源分配至关重要。本文介绍了一种创新的资源分配方法，旨在协调O-RAN中多个独立xAPPs的协调。我们的方法侧重于在用户设备（UE）之间最大化加权吞吐量，并分配物理资源块（PRBs）。我们优先考虑增强型移动宽带和超可靠低延迟通信这两种服务类型。为此，我们设计了两个xAPPs：每个UE的功率控制xAPP和PRB分配xAPP。所提出的方法包括两个部分的训练阶段，其中第一部分使用带有变分自动编码器的监督学习进行训练。

    The Open Radio Access Network (O-RAN) technology has emerged as a promising solution for network operators, providing them with an open and favorable environment. Ensuring effective coordination of x-applications (xAPPs) is crucial to enhance flexibility and optimize network performance within the O-RAN. In this paper, we introduce an innovative approach to the resource allocation problem, aiming to coordinate multiple independent xAPPs for network slicing and resource allocation in O-RAN. Our proposed method focuses on maximizing the weighted throughput among user equipments (UE), as well as allocating physical resource blocks (PRBs). We prioritize two service types, namely enhanced Mobile Broadband and Ultra Reliable Low Latency Communication. To achieve this, we have designed two xAPPs: a power control xAPP for each UE and a PRB allocation xAPP. The proposed method consists of a two-part training phase, where the first part uses supervised learning with a Variational Autoencoder tra
    
[^11]: 行动中的现实主义：使用YOLOv8和DeiT从医学图像中诊断脑肿瘤的异常感知

    Realism in Action: Anomaly-Aware Diagnosis of Brain Tumors from Medical Images Using YOLOv8 and DeiT. (arXiv:2401.03302v1 [eess.IV])

    [http://arxiv.org/abs/2401.03302](http://arxiv.org/abs/2401.03302)

    本研究利用深度学习技术在具有挑战性的情况下检测和分类脑肿瘤，并解决了在罕见情况下的肿瘤检测问题。研究使用了来自国家脑映射实验室的数据集，通过修改样本数量和患者分布，使模型能够应对真实世界场景中的异常情况。

    

    在医学科学领域，由于脑肿瘤在患者中的罕见程度，可靠地检测和分类脑肿瘤仍然是一个艰巨的挑战。因此，在异常情况下检测肿瘤的能力对于确保及时干预和改善患者结果至关重要。本研究利用深度学习技术在具有挑战性的情况下检测和分类脑肿瘤。来自国家脑映射实验室（NBML）的精选数据集包括81名患者，其中包括30例肿瘤病例和51例正常病例。检测和分类流程被分为两个连续的任务。检测阶段包括全面的数据分析和预处理，以修改图像样本和每个类别的患者数量，以符合真实世界场景中的异常分布（9个正常样本对应1个肿瘤样本）。此外，在测试中除了常见的评估指标外，我们还采用了... [摘要长度已达到上限]

    In the field of medical sciences, reliable detection and classification of brain tumors from images remains a formidable challenge due to the rarity of tumors within the population of patients. Therefore, the ability to detect tumors in anomaly scenarios is paramount for ensuring timely interventions and improved patient outcomes. This study addresses the issue by leveraging deep learning (DL) techniques to detect and classify brain tumors in challenging situations. The curated data set from the National Brain Mapping Lab (NBML) comprises 81 patients, including 30 Tumor cases and 51 Normal cases. The detection and classification pipelines are separated into two consecutive tasks. The detection phase involved comprehensive data analysis and pre-processing to modify the number of image samples and the number of patients of each class to anomaly distribution (9 Normal per 1 Tumor) to comply with real world scenarios. Next, in addition to common evaluation metrics for the testing, we emplo
    
[^12]: 在大型语言模型中定位事实知识：探索剩余流和分析词汇空间中的子值。

    Locating Factual Knowledge in Large Language Models: Exploring the Residual Stream and Analyzing Subvalues in Vocabulary Space. (arXiv:2312.12141v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2312.12141](http://arxiv.org/abs/2312.12141)

    通过探索剩余流和分析词汇空间中的子值，我们定位了大型语言模型中的事实知识，并找到了存储了有关“法国，首都，巴黎”的知识的位置。

    

    通过探索剩余流和分析词汇空间中的子值，我们找到了大型语言模型中事实知识的位置。我们发现当投影到词汇空间时，子值具有可人类解释的概念的原因。子值的softmax之前的值通过一个加法函数相加，因此词汇空间中前几个标记的概率会增加。基于此，我们发现使用对数概率增加来计算层和子值的重要性比概率增加更好，因为对数概率增加的曲线呈线性单调增形状。此外，我们计算内积来评估前馈网络（FFN）的子值被前面的层激活的程度。根据我们的方法，我们找到了事实知识“法国，首都，巴黎”存储的位置。具体来说，注意力层存储“巴黎与法国相关”。FFN层存储“巴黎是一个首都/城市”，由注意力子值激活。

    We find the location of factual knowledge in large language models by exploring the residual stream and analyzing subvalues in vocabulary space. We find the reason why subvalues have human-interpretable concepts when projecting into vocabulary space. The before-softmax values of subvalues are added by an addition function, thus the probability of top tokens in vocabulary space will increase. Based on this, we find using log probability increase to compute the significance of layers and subvalues is better than probability increase, since the curve of log probability increase has a linear monotonically increasing shape. Moreover, we calculate the inner products to evaluate how much a feed-forward network (FFN) subvalue is activated by previous layers. Base on our methods, we find where factual knowledge <France, capital, Paris> is stored. Specifically, attention layers store "Paris is related to France". FFN layers store "Paris is a capital/city", activated by attention subvalues relate
    
[^13]: 在时间序列预测中，量子长短期记忆（QLSTM）与经典LSTM的比较研究：以太阳能预测为例

    Quantum Long Short-Term Memory (QLSTM) vs Classical LSTM in Time Series Forecasting: A Comparative Study in Solar Power Forecasting. (arXiv:2310.17032v1 [quant-ph])

    [http://arxiv.org/abs/2310.17032](http://arxiv.org/abs/2310.17032)

    本研究比较了量子长短期记忆（QLSTM）和经典长短期记忆（LSTM）模型在太阳能发电量预测中的表现，发现QLSTM具有加快训练收敛速度和减小测试损失的优势，拥有吸纳复杂时间序列关系的潜力，但还需要进一步研究和优化。

    

    在全球向可持续能源系统发展的过程中，准确预测太阳能发电量至关重要。在本研究中，我们对量子长短期记忆（QLSTM）和经典长短期记忆（LSTM）模型在太阳能发电预测方面进行了仔细比较。我们的实验结果显示，QLSTM具有显著优势，包括训练收敛速度加快和在初始阶段明显降低的测试损失，相比之下，经典LSTM模型。这些实证结果表明，QLSTM有潜力快速吸纳复杂的时间序列关系，这得益于量子现象（如叠加）。然而，要实现QLSTM的全部能力，还需要进一步研究模型在不同条件下的验证、系统的超参数优化、硬件噪声鲁棒性，以及相关可再生能源预测问题的应用。随着不断的进展，量子机器学习可以在预测和优化复杂问题方面带来变革。

    Accurately forecasting solar power generation is crucial in the global progression towards sustainable energy systems. In this study, we conduct a meticulous comparison between Quantum Long Short-Term Memory (QLSTM) and classical Long Short-Term Memory (LSTM) models for solar power production forecasting. Our controlled experiments reveal promising advantages of QLSTMs, including accelerated training convergence and substantially reduced test loss within the initial epoch compared to classical LSTMs. These empirical findings demonstrate QLSTM's potential to swiftly assimilate complex time series relationships, enabled by quantum phenomena like superposition. However, realizing QLSTM's full capabilities necessitates further research into model validation across diverse conditions, systematic hyperparameter optimization, hardware noise resilience, and applications to correlated renewable forecasting problems. With continued progress, quantum machine learning can offer a paradigm shift in
    
[^14]: 通过潜在引导扩散和嵌套集成改进医学图像分类的鲁棒性和可靠性

    Improving Robustness and Reliability in Medical Image Classification with Latent-Guided Diffusion and Nested-Ensembles. (arXiv:2310.15952v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.15952](http://arxiv.org/abs/2310.15952)

    本文引入了一种新颖的三阶段方法，通过变换器和条件扩散模型来改善医学图像分类模型对实际应用中常见成像变异性的鲁棒性。

    

    尽管深度学习模型在各种医学图像分析任务中取得了显著的成功，但在真实临床环境中部署这些模型需要它们对所获取的图像的变异性具有鲁棒性。许多方法会对训练数据应用预定义的转换，以增强测试时的鲁棒性，但这些转换可能无法确保模型对患者图像中的多样性变异性具有鲁棒性。在本文中，我们提出了一种基于变换器和条件扩散模型的新型三阶段方法，旨在提高模型对实践中常见的成像变异性的鲁棒性，而无需预先确定的数据增强策略。为了实现这一目标，多个图像编码器首先学习分层特征表示来构建辨别潜在空间。接下来，一个由潜在代码引导的逆扩散过程作用于有信息先验，并提出预测候选。

    While deep learning models have achieved remarkable success across a range of medical image analysis tasks, deployment of these models in real clinical contexts requires that they be robust to variability in the acquired images. While many methods apply predefined transformations to augment the training data to enhance test-time robustness, these transformations may not ensure the model's robustness to the diverse variability seen in patient images. In this paper, we introduce a novel three-stage approach based on transformers coupled with conditional diffusion models, with the goal of improving model robustness to the kinds of imaging variability commonly encountered in practice without the need for pre-determined data augmentation strategies. To this end, multiple image encoders first learn hierarchical feature representations to build discriminative latent spaces. Next, a reverse diffusion process, guided by the latent code, acts on an informative prior and proposes prediction candi
    
[^15]: 优化拟合因子分析与通货紧缩变量旋转

    Optimal vintage factor analysis with deflation varimax. (arXiv:2310.10545v1 [stat.ML])

    [http://arxiv.org/abs/2310.10545](http://arxiv.org/abs/2310.10545)

    本文提出了一种采用通货紧缩变量旋转的拟合因子分析方法，在每一行上逐步求解正交矩阵，相比于传统方法具有更好的计算性能和灵活性，并且在更广泛的背景下提供了理论保证。

    

    通货紧缩变量旋转是一种重要的因子分析方法，旨在首先找到原始数据的低维表示，然后寻求旋转，使旋转后的低维表示具有科学意义。尽管Principal Component Analysis (PCA) followed by the varimax rotation被广泛应用于拟合因子分析，但由于varimax rotation需要在正交矩阵集合上解非凸优化问题，因此很难提供理论保证。本文提出了一种逐行求解正交矩阵的通货紧缩变量旋转过程。除了在计算上的优势和灵活性之外，我们还能在广泛的背景下对所提出的过程进行完全的理论保证。在PCA之后采用这种新的varimax方法作为第二步，我们进一步分析了这个两步过程在一个更一般的因子模型的情况下。

    Vintage factor analysis is one important type of factor analysis that aims to first find a low-dimensional representation of the original data, and then to seek a rotation such that the rotated low-dimensional representation is scientifically meaningful. Perhaps the most widely used vintage factor analysis is the Principal Component Analysis (PCA) followed by the varimax rotation. Despite its popularity, little theoretical guarantee can be provided mainly because varimax rotation requires to solve a non-convex optimization over the set of orthogonal matrices.  In this paper, we propose a deflation varimax procedure that solves each row of an orthogonal matrix sequentially. In addition to its net computational gain and flexibility, we are able to fully establish theoretical guarantees for the proposed procedure in a broad context.  Adopting this new varimax approach as the second step after PCA, we further analyze this two step procedure under a general class of factor models. Our resul
    
[^16]: 非回溯图神经网络

    Non-backtracking Graph Neural Networks. (arXiv:2310.07430v1 [cs.LG])

    [http://arxiv.org/abs/2310.07430](http://arxiv.org/abs/2310.07430)

    非回溯图神经网络(NBA-GNN)通过不考虑先前访问节点的消息来解决图神经网络本地更新中的冗余问题，并且在随机块模型恢复方面表现出良好的性能。

    

    著名的图神经网络的消息传递更新允许使用本地和计算上可跟踪的更新来表示大规模图。然而，本地更新受到回溯的影响，即消息通过同一条边两次流动并重访先前访问的节点。由于消息流的数量随着更新的次数呈指数级增加，本地更新中的冗余阻碍了图神经网络准确识别下游任务的特定消息流。在这项工作中，我们通过非回溯的图神经网络（NBA-GNN）解决了这种冗余，该网络在更新消息时不考虑先前访问节点的消息。我们进一步研究了NBA-GNN如何缓解GNN的过度压缩，并建立了NBA-GNN和非回溯更新在随机块模型恢复方面出色性能之间的联系。我们通过实验证实了我们的NBA-

    The celebrated message-passing updates for graph neural networks allow the representation of large-scale graphs with local and computationally tractable updates. However, the local updates suffer from backtracking, i.e., a message flows through the same edge twice and revisits the previously visited node. Since the number of message flows increases exponentially with the number of updates, the redundancy in local updates prevents the graph neural network from accurately recognizing a particular message flow for downstream tasks. In this work, we propose to resolve such a redundancy via the non-backtracking graph neural network (NBA-GNN) that updates a message without incorporating the message from the previously visited node. We further investigate how NBA-GNN alleviates the over-squashing of GNNs, and establish a connection between NBA-GNN and the impressive performance of non-backtracking updates for stochastic block model recovery. We empirically verify the effectiveness of our NBA-
    
[^17]: 非光滑弱凸有限和耦合组合优化

    Non-Smooth Weakly-Convex Finite-sum Coupled Compositional Optimization. (arXiv:2310.03234v1 [math.OC])

    [http://arxiv.org/abs/2310.03234](http://arxiv.org/abs/2310.03234)

    本文研究了一种新的组合优化问题，称为非光滑弱凸有限和耦合组合优化(NSWC FCCO)，通过扩展已有的研究，我们研究了非光滑弱凸FCCO的问题，并提出了一种单循环算法来找到Moreau环的ε-稳定点。

    

    本文研究了一类新的组合优化问题，称为非光滑弱凸有限和耦合组合优化(NSWC FCCO)。由于其在机器学习和人工智能领域的广泛应用以及其解决基于经验风险最小化的随机算法的局限性，FCCO引起了越来越多的关注。然而，目前对于FCCO的研究假设内外函数都是光滑的，限制了其能够解决更多种类的问题的潜力。我们的研究从非光滑弱凸FCCO的角度进行了扩展，其中外函数是弱凸且非递减的，内函数是弱凸的。我们分析了一种单循环算法，并确定其在找到Moreau环的ε-稳定点的复杂度。

    This paper investigates new families of compositional optimization problems, called $\underline{\bf n}$on-$\underline{\bf s}$mooth $\underline{\bf w}$eakly-$\underline{\bf c}$onvex $\underline{\bf f}$inite-sum $\underline{\bf c}$oupled $\underline{\bf c}$ompositional $\underline{\bf o}$ptimization (NSWC FCCO). There has been a growing interest in FCCO due to its wide-ranging applications in machine learning and AI, as well as its ability to address the shortcomings of stochastic algorithms based on empirical risk minimization. However, current research on FCCO presumes that both the inner and outer functions are smooth, limiting their potential to tackle a more diverse set of problems. Our research expands on this area by examining non-smooth weakly-convex FCCO, where the outer function is weakly convex and non-decreasing, and the inner function is weakly-convex. We analyze a single-loop algorithm and establish its complexity for finding an $\epsilon$-stationary point of the Moreau env
    
[^18]: WASA：基于水印的大型语言模型生成数据的源归属

    WASA: WAtermark-based Source Attribution for Large Language Model-Generated Data. (arXiv:2310.00646v1 [cs.LG])

    [http://arxiv.org/abs/2310.00646](http://arxiv.org/abs/2310.00646)

    本文提出了一种基于水印的框架WASA，通过允许大型语言模型生成带有嵌入源信息的合成文本水印来解决源归属和数据来源的问题。

    

    大型语言模型（LLM）的出色性能和其商业化的巨大潜力引发了对其训练数据知识产权（IP）的严重关注。特别是，LLM生成的合成文本可能侵犯被用于训练LLM的数据的知识产权。为此，我们能够（a）通过水印识别出对LLM生成的合成文本做出贡献的数据提供者（源归属）；以及（b）验证文本数据是否来自于某个数据提供者对LLM进行了训练（数据来源）。在本文中，我们展示了通过水印技术可以解决这两个问题，即通过让LLM生成具有嵌入源信息的合成文本水印来实现。我们确定了这种水印技术框架的关键特性（例如源归属准确性、抵抗对手攻击的鲁棒性），并提出了一个满足这些要求的WAtermarking for Source Attribution（WASA）框架.

    The impressive performances of large language models (LLMs) and their immense potential for commercialization have given rise to serious concerns over the intellectual property (IP) of their training data. In particular, the synthetic texts generated by LLMs may infringe the IP of the data being used to train the LLMs. To this end, it is imperative to be able to (a) identify the data provider who contributed to the generation of a synthetic text by an LLM (source attribution) and (b) verify whether the text data from a data provider has been used to train an LLM (data provenance). In this paper, we show that both problems can be solved by watermarking, i.e., by enabling an LLM to generate synthetic texts with embedded watermarks that contain information about their source(s). We identify the key properties of such watermarking frameworks (e.g., source attribution accuracy, robustness against adversaries), and propose a WAtermarking for Source Attribution (WASA) framework that satisfies
    
[^19]: 作为评估器的大型语言模型中认知偏差的基准测试

    Benchmarking Cognitive Biases in Large Language Models as Evaluators. (arXiv:2309.17012v1 [cs.CL])

    [http://arxiv.org/abs/2309.17012](http://arxiv.org/abs/2309.17012)

    本研究对15个不同大小的大型语言模型进行了评估，发现它们作为评估器存在认知偏差，尤其在文本质量评估中表现出较强的偏见，这对其鲁棒性提出了质疑。同时，研究还发现了人类和机器偏好之间的相关性。

    

    最近的研究表明，大型语言模型（LLMs）通过简单的提示和上下文学习作为自动评估器非常有效。本研究组装了15个大小不同的LLMs，并通过其他LLMs的偏好排名来评估它们的输出响应，例如System Star比System Square更好。然后，我们引入了用于评估LLMs输出中六种不同认知偏差的认知偏差基准测试（CoBBLEr），如自我中心偏差，即模型更喜欢将自己的输出在评估中排名较高。我们发现LLMs是有偏见的文本质量评估器，在每个评估中都表现出对我们偏见基准的强烈迹象（在所有模型上的平均比较约为40%），这对它们作为评估器的鲁棒性提出了质疑。此外，我们还研究了人类和机器偏好之间的相关性，并计算了平均的Rank-Biased O值。

    Large Language Models (LLMs) have recently been shown to be effective as automatic evaluators with simple prompting and in-context learning. In this work, we assemble 15 LLMs of four different size ranges and evaluate their output responses by preference ranking from the other LLMs as evaluators, such as System Star is better than System Square. We then evaluate the quality of ranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators (CoBBLEr), a benchmark to measure six different cognitive biases in LLM evaluation outputs, such as the Egocentric bias where a model prefers to rank its own outputs highly in evaluation. We find that LLMs are biased text quality evaluators, exhibiting strong indications on our bias benchmark (average of 40% of comparisons across all models) within each of their evaluations that question their robustness as evaluators. Furthermore, we examine the correlation between human and machine preferences and calculate the average Rank-Biased O
    
[^20]: 在资源约束下的分布参数估计中的协作研究

    On Collaboration in Distributed Parameter Estimation with Resource Constraints. (arXiv:2307.06442v1 [cs.LG])

    [http://arxiv.org/abs/2307.06442](http://arxiv.org/abs/2307.06442)

    在资源约束下的分布参数估计中，我们研究了传感器/代理数据收集和协作策略，通过最大化费舍尔信息或最小化Cramer-Rao界来解决传感器/代理的数据收集和协作策略设计问题。

    

    我们研究了考虑资源约束和不同传感器/代理收集的观测之间的相关性的参数估计的传感器/代理数据收集和协作策略。具体地，我们考虑了一组传感器/代理，每个传感器/代理样本来自多元高斯分布的不同变量，并且具有不同的估计目标，我们将传感器/代理的数据收集和协作策略设计问题阐述为费舍尔信息最大化（或Cramer-Rao界最小化）问题。当变量之间的相关性知识可用时，我们可以分析地识别出两个特定情况：（1）不能利用样本之间的相关性知识进行协作估计的情况，（2）最优数据收集策略涉及投资有限资源以协作采样和转移已知统计信息的情况。

    We study sensor/agent data collection and collaboration policies for parameter estimation, accounting for resource constraints and correlation between observations collected by distinct sensors/agents. Specifically, we consider a group of sensors/agents each samples from different variables of a multivariate Gaussian distribution and has different estimation objectives, and we formulate a sensor/agent's data collection and collaboration policy design problem as a Fisher information maximization (or Cramer-Rao bound minimization) problem. When the knowledge of correlation between variables is available, we analytically identify two particular scenarios: (1) where the knowledge of the correlation between samples cannot be leveraged for collaborative estimation purposes and (2) where the optimal data collection policy involves investing scarce resources to collaboratively sample and transfer information that is not of immediate interest and whose statistics are already known, with the sol
    
[^21]: 大型语言模型快速分布式推断服务

    Fast Distributed Inference Serving for Large Language Models. (arXiv:2305.05920v1 [cs.LG])

    [http://arxiv.org/abs/2305.05920](http://arxiv.org/abs/2305.05920)

    FastServe是一种针对大型语言模型的分布式推理服务系统，利用抢占式调度和跳过-连接多级反馈队列，最小化模型推断的作业完成时间(JCT)。

    

    大型语言模型(LLM)推动了以ChatGPT为代表的新一代互动AI应用程序的发展。这些应用程序的交互性要求模型推断的低作业完成时间(JCT)。现有的LLM服务系统使用的是运行到完成的处理方式，存在头部阻塞和长JCT的问题。我们提出了FastServe，一种针对LLMs的分布式推理服务系统。FastServe利用LLM推理的自回归模式，以每个输出标记的粒度实现抢占式，使用新颖的跳过-连接多级反馈队列调度器最小化JCT。基于LLM推理的新半信息不可知设置，调度程序利用输入长度信息来为每个到达作业分配适当的初始队列来连接。高于所连接队列的优先级队列被跳过以减少降级。我们设计了一种高效的GPU内存管理机制，以提前清除不再使用的GPU缓存，并对常用模型进行缓存。

    Large language models (LLMs) power a new generation of interactive AI applications exemplified by ChatGPT. The interactive nature of these applications demand low job completion time (JCT) for model inference. Existing LLM serving systems use run-to-completion processing for inference jobs, which suffers from head-of-line blocking and long JCT. We present FastServe, a distributed inference serving system for LLMs. FastServe exploits the autoregressive pattern of LLM inference to enable preemption at the granularity of each output token. FastServe uses preemptive scheduling to minimize JCT with a novel skip-join Multi-Level Feedback Queue scheduler. Based on the new semi information-agnostic setting of LLM inference, the scheduler leverages the input length information to assign an appropriate initial queue for each arrival job to join. The higher priority queues than the joined queue are skipped to reduce demotions. We design an efficient GPU memory management mechanism that proactivel
    
[^22]: 一种可解释的机器学习系统来识别癫痫-间隙-损伤连续状态下的脑电图图案

    An Interpretable Machine Learning System to Identify EEG Patterns on the Ictal-Interictal-Injury Continuum. (arXiv:2211.05207v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.05207](http://arxiv.org/abs/2211.05207)

    该论文设计了一种可解释的深度学习模型，以预测ICU脑电监测中常见的6种脑波图案的存在，并提供高质量的解释和三种解释方法，这对于建立AI的信任和临床采用至关重要。

    

    在许多医学领域，人们呼吁在用于临床工作的机器学习系统中增加可解释性。在本文中，我们设计了一个可解释的深度学习模型，用于预测ICU脑电监测中常见的6种脑波图案（癫痫、LPD、GPD、LRDA、GRDA、其他）的存在。每个预测都配有一个高质量的解释，借助于专门的用户界面提供支持。此新型模型架构学习了一组原型示例（“原型”），并通过将新的EEG片段与这些原型进行比较来做出决策。这些原型可以是单类（仅与一个类相关）或双类（与两个类相关）。我们提出了三种主要的模型解释方法：1）使用全局结构保持方法，将1275维cEEG潜在特征映射到二维空间中，可视化癫痫-间隙-损伤连续状态，从而深入了解其高维结构。2）我们提出了一种交互式解释方法，使人类专家能够查询模型预测的不同方面，并以自然语言接收经过专家验证的解释。3）我们可视化了导致模型做出某个决策的输入的最重要特征，允许详细检查输入和输出之间的关系。总的来说，我们展示了解释性模型分类EEG图案和提供专家友好的解释的实用性，这两个方面对于建立AI的信任和临床采用至关重要。

    In many medical subfields, there is a call for greater interpretability in the machine learning systems used for clinical work. In this paper, we design an interpretable deep learning model to predict the presence of 6 types of brainwave patterns (Seizure, LPD, GPD, LRDA, GRDA, other) commonly encountered in ICU EEG monitoring. Each prediction is accompanied by a high-quality explanation delivered with the assistance of a specialized user interface. This novel model architecture learns a set of prototypical examples (``prototypes'') and makes decisions by comparing a new EEG segment to these prototypes. These prototypes are either single-class (affiliated with only one class) or dual-class (affiliated with two classes).  We present three main ways of interpreting the model: 1) Using global-structure preserving methods, we map the 1275-dimensional cEEG latent features to a 2D space to visualize the ictal-interictal-injury continuum and gain insight into its high-dimensional structure. 2
    

