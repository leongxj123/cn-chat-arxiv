# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Few-Shot Learning on Graphs: from Meta-learning to Pre-training and Prompting](https://rss.arxiv.org/abs/2402.01440) | 本文综述了图上的小样本学习的最新发展，将现有的研究方法划分为元学习、预训练和混合方法三大类别，并对它们的优缺点进行了比较。还提出了未来的研究方向。 |
| [^2] | [Latxa: An Open Language Model and Evaluation Suite for Basque](https://arxiv.org/abs/2403.20266) | Latxa是一种用于巴斯克语的大型语言模型系列，在语言熟练度和理解能力方面表现出色，优于所有以前的开放模型，并具有多个评估数据集，填补了巴斯克语高质量基准的不足。 |
| [^3] | [Revolutionizing Disease Diagnosis with simultaneous functional PET/MR and Deeply Integrated Brain Metabolic, Hemodynamic, and Perfusion Networks](https://arxiv.org/abs/2403.20058) | 提出了MX-ARM，一种基于AI的疾病诊断模型，利用同时功能PET/MR技术，能够在推理过程中同时接受单模态和多模态输入，具有创新的模态分离和重构功能。 |
| [^4] | [A Scalable and Parallelizable Digital Twin Framework for Sustainable Sim2Real Transition of Multi-Agent Reinforcement Learning Systems](https://arxiv.org/abs/2403.10996) | 提出了一个可持续的多智能体深度强化学习框架，利用分散的学习架构，来解决交通路口穿越和自主赛车等问题 |
| [^5] | [Deep Learning Based Dynamics Identification and Linearization of Orbital Problems using Koopman Theory](https://arxiv.org/abs/2403.08965) | 通过深度学习和库普曼理论，提出了一种数据驱动框架，可以同时识别“两体问题”和“圆限制三体问题”的动力学，并将其全局线性化成线性时不变系统。 |
| [^6] | [Towards Efficient Risk-Sensitive Policy Gradient: An Iteration Complexity Analysis](https://arxiv.org/abs/2403.08955) | 本文对风险敏感策略梯度方法进行了迭代复杂度分析，发现其能够通过使用指数效用函数达到较低的迭代复杂度。 |
| [^7] | [Efficient Algorithms for Empirical Group Distributional Robust Optimization and Beyond](https://arxiv.org/abs/2403.03562) | 该研究提出了一种高效算法，用于解决群分布鲁棒优化问题，通过两级有限和凹凸最小最大优化结构和随机方差减小镜像Prox算法，实现对所有组的方差减少，并支持非恒定学习率。 |
| [^8] | [Disentangling representations of retinal images with generative models](https://arxiv.org/abs/2402.19186) | 引入一种新颖的视网膜底图像群体模型，有效解开患者属性与相机效果，实现可控且高度逼真的图像生成。 |
| [^9] | [Improving Group Connectivity for Generalization of Federated Deep Learning](https://arxiv.org/abs/2402.18949) | 通过研究和改进联邦学习的泛化能力，本文从“连接性”视角探讨了如何改善本地模型间的连接性以生成更具泛化能力的全局模型。 |
| [^10] | [Embedding Knowledge Graphs in Degenerate Clifford Algebras](https://arxiv.org/abs/2402.04870) | 这项研究提出将知识图谱嵌入到退化的克利福德代数中。通过考虑具有零幂指数为2的零幂基向量，可以泛化基于二次数的方法并捕捉实体嵌入中缺乏高阶相互作用的模式。研究设计了两个新模型来发现代数的参数，并证明零幂向量有助于捕捉实体的特征。 |
| [^11] | [MQuinE: a cure for "Z-paradox'' in knowledge graph embedding models](https://arxiv.org/abs/2402.03583) | 研究者发现知识图谱嵌入模型存在的“Z-悖论”限制了其表达能力，并提出了一种名为MQuinE的新模型，通过理论证明，MQuinE成功解决了Z-悖论，并在链接预测任务中显著优于现有模型。 |
| [^12] | [High Perceptual Quality Wireless Image Delivery with Denoising Diffusion Models.](http://arxiv.org/abs/2309.15889) | 本论文研究了通过深度学习的联合源-信道编码和去噪扩散模型在噪声无线信道上进行图像传输的问题。通过利用范围-零空间分解和逐步优化零空间内容，实现了在失真和感知质量方面的显著改进。 |
| [^13] | [Boundary Peeling: Outlier Detection Method Using One-Class Peeling.](http://arxiv.org/abs/2309.05630) | 一类边界剥离是一种无监督的异常检测算法，使用了平均有符号距离和灵活的边界生成方法。在无异常值和有异常值的情况下，一类边界剥离表现出优越的性能。 |
| [^14] | [Structured Radial Basis Function Network: Modelling Diversity for Multiple Hypotheses Prediction.](http://arxiv.org/abs/2309.00781) | 这篇论文提出了一种结构化径向基函数网络，用于解决多模态回归问题。该网络能够有效地组合多个假设预测器，并通过插值逼近多个假设目标分布，具有较好的性能。 |
| [^15] | [Revisiting Permutation Symmetry for Merging Models between Different Datasets.](http://arxiv.org/abs/2306.05641) | 本研究通过理论和实证分析表明，不同数据集合并模型的准确性下降更为显著，因为每个数据集的不同损失函数使得合并更加困难。此外，通过数据集压缩创建的压缩数据集可以作为原数据集的替代品。 |
| [^16] | [Zero-shot Task Preference Addressing Enabled by Imprecise Bayesian Continual Learning.](http://arxiv.org/abs/2305.14782) | 提出了零样本任务偏好的不精确贝叶斯继续学习（IBCL）算法，该算法更新模型参数分布凸壳形式的知识库，并使用零样本获取模型以满足不同的偏好，使得在具有大量任务偏好的情况下更加可扩展。 |
| [^17] | [Optimizing DDPM Sampling with Shortcut Fine-Tuning.](http://arxiv.org/abs/2301.13362) | 本文提出了Shortcut Fine-Tuning（SFT）算法，利用直接最小化积分概率度量（IPM）来对DDPM采样器进行微调，从而有效提高DDPM采样效率和质量。 |
| [^18] | [FlexFringe: Modeling Software Behavior by Learning Probabilistic Automata.](http://arxiv.org/abs/2203.16331) | FlexFringe提供了高效的概率有限自动机学习方法，可用于建模软件行为。该方法在实践中通过实现改进的状态合并策略实现了显著性能提升，并且能够从软件日志中学习可解释的模型，用于异常检测。与基于神经网络的解决方案相比，学习更小更复杂的模型能够提高FlexFringe在异常检测中的性能。 |

# 详细

[^1]: 在图上的小样本学习：从元学习到预训练和提示

    Few-Shot Learning on Graphs: from Meta-learning to Pre-training and Prompting

    [https://rss.arxiv.org/abs/2402.01440](https://rss.arxiv.org/abs/2402.01440)

    本文综述了图上的小样本学习的最新发展，将现有的研究方法划分为元学习、预训练和混合方法三大类别，并对它们的优缺点进行了比较。还提出了未来的研究方向。

    

    图表示学习是图中心任务中的关键步骤，在这方面已经取得了重大进展。早期的技术通常在端到端的设置中运行，性能严重依赖于充足的标记数据的可用性。这个限制引发了图上的小样本学习的出现，其中每个任务只有少量的任务特定标签可用。鉴于这个领域的广泛文献，本综述试图综合最近的发展，提供比较性的见解，并确定未来的方向。我们将现有的研究系统地分为三个主要类别：元学习方法、预训练方法和混合方法，并在每个类别中进行细粒度的分类，以帮助读者进行方法选择。在每个类别中，我们分析这些方法之间的关系并比较它们的优缺点。最后，我们概述了图上的小样本学习未来的方向。

    Graph representation learning, a critical step in graph-centric tasks, has seen significant advancements. Earlier techniques often operate in an end-to-end setting, where performance heavily relies on the availability of ample labeled data. This constraint has spurred the emergence of few-shot learning on graphs, where only a few task-specific labels are available for each task. Given the extensive literature in this field, this survey endeavors to synthesize recent developments, provide comparative insights, and identify future directions. We systematically categorize existing studies into three major families: meta-learning approaches, pre-training approaches, and hybrid approaches, with a finer-grained classification in each family to aid readers in their method selection process. Within each category, we analyze the relationships among these methods and compare their strengths and limitations. Finally, we outline prospective future directions for few-shot learning on graphs to cata
    
[^2]: Latxa: 一种用于巴斯克语的开放语言模型和评估套件

    Latxa: An Open Language Model and Evaluation Suite for Basque

    [https://arxiv.org/abs/2403.20266](https://arxiv.org/abs/2403.20266)

    Latxa是一种用于巴斯克语的大型语言模型系列，在语言熟练度和理解能力方面表现出色，优于所有以前的开放模型，并具有多个评估数据集，填补了巴斯克语高质量基准的不足。

    

    我们介绍了Latxa，这是一个基于Llama 2的大型巴斯克语言模型系列，参数范围从7到700亿。Latxa基于新的巴斯克语语料库预训练，包括430万个文档和42亿个标记。针对巴斯克语高质量基准的稀缺性，我们进一步提出了4个多项选择评估数据集：EusProficiency，包括来自官方语言能力考试的5169个问题；EusReading，包括352个阅读理解问题；EusTrivia，包括来自5个知识领域的1715个琐事问题；以及EusExams，包括来自公共考试的16774个问题。在我们的广泛评估中，Latxa在与我们比较的所有先前开放模型中表现出色。此外，尽管在阅读理解和知识密集型任务方面落后，但在语言熟练度和理解能力方面，它与GPT-4 Turbo具有竞争力。Latxa模型系列，以及

    arXiv:2403.20266v1 Announce Type: cross  Abstract: We introduce Latxa, a family of large language models for Basque ranging from 7 to 70 billion parameters. Latxa is based on Llama 2, which we continue pretraining on a new Basque corpus comprising 4.3M documents and 4.2B tokens. Addressing the scarcity of high-quality benchmarks for Basque, we further introduce 4 multiple choice evaluation datasets: EusProficiency, comprising 5,169 questions from official language proficiency exams; EusReading, comprising 352 reading comprehension questions; EusTrivia, comprising 1,715 trivia questions from 5 knowledge areas; and EusExams, comprising 16,774 questions from public examinations. In our extensive evaluation, Latxa outperforms all previous open models we compare to by a large margin. In addition, it is competitive with GPT-4 Turbo in language proficiency and understanding, despite lagging behind in reading comprehension and knowledge-intensive tasks. Both the Latxa family of models, as well
    
[^3]: 利用同时功能PET/MR和深度整合的脑代谢、血液动力学和灌注网络彻底改变疾病诊断

    Revolutionizing Disease Diagnosis with simultaneous functional PET/MR and Deeply Integrated Brain Metabolic, Hemodynamic, and Perfusion Networks

    [https://arxiv.org/abs/2403.20058](https://arxiv.org/abs/2403.20058)

    提出了MX-ARM，一种基于AI的疾病诊断模型，利用同时功能PET/MR技术，能够在推理过程中同时接受单模态和多模态输入，具有创新的模态分离和重构功能。

    

    同时功能PET/MR（sf-PET/MR）是一种尖端的多模式神经影像技术。它提供了一个前所未有的机会，可以同时监测和整合由时空协变代谢活动、神经活动和脑血流（灌注）构建的多方面大脑网络。虽然在科学/临床价值上很高，但PET/MR硬件的可及性不足阻碍了其应用，更不用说现代基于AI的PET/MR融合模型。我们的目标是开发一个基于AI的临床可行疾病诊断模型，该模型基于全面的sf-PET/MR数据进行训练，在推理过程中具有允许单模态输入（例如，仅PET）以及强制多模态准确性的能力。为此，我们提出了MX-ARM，一种多模态专家混合对齐和重构模型。它是模态可分离和可交换的，动态分配不同的多层感知器（"混合）

    arXiv:2403.20058v1 Announce Type: cross  Abstract: Simultaneous functional PET/MR (sf-PET/MR) presents a cutting-edge multimodal neuroimaging technique. It provides an unprecedented opportunity for concurrently monitoring and integrating multifaceted brain networks built by spatiotemporally covaried metabolic activity, neural activity, and cerebral blood flow (perfusion). Albeit high scientific/clinical values, short in hardware accessibility of PET/MR hinders its applications, let alone modern AI-based PET/MR fusion models. Our objective is to develop a clinically feasible AI-based disease diagnosis model trained on comprehensive sf-PET/MR data with the power of, during inferencing, allowing single modality input (e.g., PET only) as well as enforcing multimodal-based accuracy. To this end, we propose MX-ARM, a multimodal MiXture-of-experts Alignment and Reconstruction Model. It is modality detachable and exchangeable, allocating different multi-layer perceptrons dynamically ("mixture 
    
[^4]: 一个可扩展且可并行化的数字孪生框架，用于多智能体强化学习系统可持续Sim2Real转换

    A Scalable and Parallelizable Digital Twin Framework for Sustainable Sim2Real Transition of Multi-Agent Reinforcement Learning Systems

    [https://arxiv.org/abs/2403.10996](https://arxiv.org/abs/2403.10996)

    提出了一个可持续的多智能体深度强化学习框架，利用分散的学习架构，来解决交通路口穿越和自主赛车等问题

    

    本工作提出了一个可持续的多智能体深度强化学习框架，能够选择性地按需扩展并行化训练工作负载，并利用最少的硬件资源将训练好的策略从模拟环境转移到现实世界。我们引入了AutoDRIVE生态系统作为一个启动数字孪生框架，用于训练、部署和转移合作和竞争的多智能体强化学习策略从模拟环境到现实世界。具体来说，我们首先探究了4台合作车辆(Nigel)在单智能体和多智能体学习环境中共享有限状态信息的交叉遍历问题，采用了一种通用策略方法。然后，我们使用个体策略方法研究了2辆车(F1TENTH)的对抗性自主赛车问题。在任何一组实验中，我们采用了去中心化学习架构，这允许对策略进行有力的训练和测试。

    arXiv:2403.10996v1 Announce Type: cross  Abstract: This work presents a sustainable multi-agent deep reinforcement learning framework capable of selectively scaling parallelized training workloads on-demand, and transferring the trained policies from simulation to reality using minimal hardware resources. We introduce AutoDRIVE Ecosystem as an enabling digital twin framework to train, deploy, and transfer cooperative as well as competitive multi-agent reinforcement learning policies from simulation to reality. Particularly, we first investigate an intersection traversal problem of 4 cooperative vehicles (Nigel) that share limited state information in single as well as multi-agent learning settings using a common policy approach. We then investigate an adversarial autonomous racing problem of 2 vehicles (F1TENTH) using an individual policy approach. In either set of experiments, a decentralized learning architecture was adopted, which allowed robust training and testing of the policies 
    
[^5]: 基于深度学习和库普曼理论的轨道问题动力学识别与线性化

    Deep Learning Based Dynamics Identification and Linearization of Orbital Problems using Koopman Theory

    [https://arxiv.org/abs/2403.08965](https://arxiv.org/abs/2403.08965)

    通过深度学习和库普曼理论，提出了一种数据驱动框架，可以同时识别“两体问题”和“圆限制三体问题”的动力学，并将其全局线性化成线性时不变系统。

    

    航空航天工程和科学领域中对“两体问题”和“圆限制三体问题”的研究非常重要，因为它们有助于描述天体和人造卫星的运动。随着对卫星和卫星编队飞行的需求日益增长，对这些系统进行快速有效的控制变得越来越重要。我们提出了一个数据驱动框架，通过基于深度学习的库普曼理论实现“两体问题”和“圆限制三体问题”的同时系统识别和全局线性化，即通过纯数据驱动训练深度神经网络来发现线性库普曼算子，并将其全局线性化为线性时不变系统（LTI）系统。

    arXiv:2403.08965v1 Announce Type: cross  Abstract: The study of the Two-Body and Circular Restricted Three-Body Problems in the field of aerospace engineering and sciences is deeply important because they help describe the motion of both celestial and artificial satellites. With the growing demand for satellites and satellite formation flying, fast and efficient control of these systems is becoming ever more important. Global linearization of these systems allows engineers to employ methods of control in order to achieve these desired results. We propose a data-driven framework for simultaneous system identification and global linearization of both the Two-Body Problem and Circular Restricted Three-Body Problem via deep learning-based Koopman Theory, i.e., a framework that can identify the underlying dynamics and globally linearize it into a linear time-invariant (LTI) system. The linear Koopman operator is discovered through purely data-driven training of a Deep Neural Network with a 
    
[^6]: 朝向高效的风险敏感策略梯度：一个迭代复杂度分析

    Towards Efficient Risk-Sensitive Policy Gradient: An Iteration Complexity Analysis

    [https://arxiv.org/abs/2403.08955](https://arxiv.org/abs/2403.08955)

    本文对风险敏感策略梯度方法进行了迭代复杂度分析，发现其能够通过使用指数效用函数达到较低的迭代复杂度。

    

    强化学习在各种应用中表现出色，使得自主智能体能够通过与环境的互动学习最佳策略。然而，传统的强化学习框架在迭代复杂度和鲁棒性方面经常面临挑战。风险敏感强化学习平衡了期望回报和风险，具有产生概率鲁棒策略的潜力，但其迭代复杂度分析尚未得到充分探讨。在本研究中，我们针对风险敏感策略梯度方法进行了彻底的迭代复杂度分析，重点关注REINFORCE算法并采用指数效用函数。我们获得了一个$\mathcal{O}(\epsilon^{-2})$的迭代复杂度，以达到$\epsilon$-近似的一阶稳定点（FOSP）。我们研究了风险敏感算法是否可以比风险中性算法实现更好的迭代复杂度。

    arXiv:2403.08955v1 Announce Type: cross  Abstract: Reinforcement Learning (RL) has shown exceptional performance across various applications, enabling autonomous agents to learn optimal policies through interaction with their environments. However, traditional RL frameworks often face challenges in terms of iteration complexity and robustness. Risk-sensitive RL, which balances expected return and risk, has been explored for its potential to yield probabilistically robust policies, yet its iteration complexity analysis remains underexplored. In this study, we conduct a thorough iteration complexity analysis for the risk-sensitive policy gradient method, focusing on the REINFORCE algorithm and employing the exponential utility function. We obtain an iteration complexity of $\mathcal{O}(\epsilon^{-2})$ to reach an $\epsilon$-approximate first-order stationary point (FOSP). We investigate whether risk-sensitive algorithms can achieve better iteration complexity compared to their risk-neutr
    
[^7]: 高效算法用于经验群分布鲁棒优化及更多

    Efficient Algorithms for Empirical Group Distributional Robust Optimization and Beyond

    [https://arxiv.org/abs/2403.03562](https://arxiv.org/abs/2403.03562)

    该研究提出了一种高效算法，用于解决群分布鲁棒优化问题，通过两级有限和凹凸最小最大优化结构和随机方差减小镜像Prox算法，实现对所有组的方差减少，并支持非恒定学习率。

    

    我们研究了群分布鲁棒优化（GDRO）的经验对应问题，旨在最小化$m$个不同组中的最大经验风险。我们将经验GDRO表述为$\textit{两级}$有限和凹凸最小最大优化问题，并开发了一种随机方差减小镜像Prox算法。与现有方法不同，我们通过逐组抽样技术构建了随机梯度，并为所有组执行方差减少，充分利用了经验GDRO的$\textit{两级}$有限和结构。此外，我们通过一个一索引偏移加权平均来计算快照和镜像快照点，这使我们与朴素的遍历平均方法有所不同。我们的算法还支持非恒定学习率，这与现有文献不同。我们建立了期望和高概率收敛保证，展示出$\m

    arXiv:2403.03562v1 Announce Type: new  Abstract: We investigate the empirical counterpart of group distributionally robust optimization (GDRO), which aims to minimize the maximal empirical risk across $m$ distinct groups. We formulate empirical GDRO as a $\textit{two-level}$ finite-sum convex-concave minimax optimization problem and develop a stochastic variance reduced mirror prox algorithm. Unlike existing methods, we construct the stochastic gradient by per-group sampling technique and perform variance reduction for all groups, which fully exploits the $\textit{two-level}$ finite-sum structure of empirical GDRO. Furthermore, we compute the snapshot and mirror snapshot point by a one-index-shifted weighted average, which distinguishes us from the naive ergodic average. Our algorithm also supports non-constant learning rates, which is different from existing literature. We establish convergence guarantees both in expectation and with high probability, demonstrating a complexity of $\m
    
[^8]: 用生成模型解开视网膜图像的表征

    Disentangling representations of retinal images with generative models

    [https://arxiv.org/abs/2402.19186](https://arxiv.org/abs/2402.19186)

    引入一种新颖的视网膜底图像群体模型，有效解开患者属性与相机效果，实现可控且高度逼真的图像生成。

    

    视网膜底图像在早期检测眼部疾病中起着至关重要的作用，最近的研究甚至表明，利用深度学习方法，这些图像还可以用于检测心血管风险因素和神经系统疾病。然而，这些图像受技术因素的影响可能对眼科领域可靠的人工智能应用构成挑战。例如，大型底图队列往往受到相机类型、图像质量或照明水平等因素的影响，存在学习快捷方式而不是图像生成过程背后因果关系的风险。在这里，我们提出了一个新颖的视网膜底图像群体模型，有效地解开了患者属性与相机效果，从而实现了可控且高度逼真的图像生成。为了实现这一目标，我们提出了一个基于距离相关性的新颖解开损失。通过定性和定量分析，我们展示了...

    arXiv:2402.19186v1 Announce Type: cross  Abstract: Retinal fundus images play a crucial role in the early detection of eye diseases and, using deep learning approaches, recent studies have even demonstrated their potential for detecting cardiovascular risk factors and neurological disorders. However, the impact of technical factors on these images can pose challenges for reliable AI applications in ophthalmology. For example, large fundus cohorts are often confounded by factors like camera type, image quality or illumination level, bearing the risk of learning shortcuts rather than the causal relationships behind the image generation process. Here, we introduce a novel population model for retinal fundus images that effectively disentangles patient attributes from camera effects, thus enabling controllable and highly realistic image generation. To achieve this, we propose a novel disentanglement loss based on distance correlation. Through qualitative and quantitative analyses, we demon
    
[^9]: 提高联邦深度学习的群组连接性以实现泛化能力

    Improving Group Connectivity for Generalization of Federated Deep Learning

    [https://arxiv.org/abs/2402.18949](https://arxiv.org/abs/2402.18949)

    通过研究和改进联邦学习的泛化能力，本文从“连接性”视角探讨了如何改善本地模型间的连接性以生成更具泛化能力的全局模型。

    

    联邦学习（FL）涉及多个异构客户端通过迭代本地更新和模型融合共同训练全局模型。与集中式训练相比，FL的全局模型的泛化存在很大差距，这是其在更广泛应用中的瓶颈。本文通过基本的“连接性”视角研究和改进FL的泛化，即本地模型在参数区域中如何连接并融合为泛化的全局模型。术语“连接性”源自线性模式连接（LMC），研究神经网络的两种不同解决方案（例如模式）的内插损失景观。在本文中，我们通过利用固定的锚定模型来研究连接性的传递性质，从两个模型（LMC）到一组模型（FL中的模型融合）。根据所发现的结果，我们提出

    arXiv:2402.18949v1 Announce Type: new  Abstract: Federated learning (FL) involves multiple heterogeneous clients collaboratively training a global model via iterative local updates and model fusion. The generalization of FL's global model has a large gap compared with centralized training, which is its bottleneck for broader applications. In this paper, we study and improve FL's generalization through a fundamental ``connectivity'' perspective, which means how the local models are connected in the parameter region and fused into a generalized global model. The term ``connectivity'' is derived from linear mode connectivity (LMC), studying the interpolated loss landscape of two different solutions (e.g., modes) of neural networks. Bridging the gap between LMC and FL, in this paper, we leverage fixed anchor models to empirically and theoretically study the transitivity property of connectivity from two models (LMC) to a group of models (model fusion in FL). Based on the findings, we propo
    
[^10]: 将知识图谱嵌入到退化的克利福德代数中

    Embedding Knowledge Graphs in Degenerate Clifford Algebras

    [https://arxiv.org/abs/2402.04870](https://arxiv.org/abs/2402.04870)

    这项研究提出将知识图谱嵌入到退化的克利福德代数中。通过考虑具有零幂指数为2的零幂基向量，可以泛化基于二次数的方法并捕捉实体嵌入中缺乏高阶相互作用的模式。研究设计了两个新模型来发现代数的参数，并证明零幂向量有助于捕捉实体的特征。

    

    克利福德代数是实数、复数和四元数的自然推广。迄今为止，在知识图谱嵌入的背景下，只有形式为$Cl_{p,q}$（即没有零幂基向量的代数）的克利福德代数受到研究。我们提出考虑零幂基向量，其幂指数为2。在这些空间中，被称为$Cl_{p,q,r}$，可以泛化基于二次数的方法（无法使用$Cl_{p,q}$进行建模）并捕捉源于实数和复数部分间缺乏高阶相互作用的实体嵌入的模式。我们设计了两个新模型来发现参数$p$，$q$和$r$。第一个模型使用贪婪搜索优化$p$，$q$和$r$。第二个模型基于使用神经网络计算的输入知识图谱的嵌入来预测$(p, q, r)$。我们在七个基准数据集上进行的评估结果表明，零幂向量有助于捕捉实体的特征。

    Clifford algebras are a natural generalization of the real numbers, the complex numbers, and the quaternions. So far, solely Clifford algebras of the form $Cl_{p,q}$ (i.e., algebras without nilpotent base vectors) have been studied in the context of knowledge graph embeddings. We propose to consider nilpotent base vectors with a nilpotency index of two. In these spaces, denoted $Cl_{p,q,r}$, allows generalizing over approaches based on dual numbers (which cannot be modelled using $Cl_{p,q}$) and capturing patterns that emanate from the absence of higher-order interactions between real and complex parts of entity embeddings. We design two new models for the discovery of the parameters $p$, $q$, and $r$. The first model uses a greedy search to optimize $p$, $q$, and $r$. The second predicts $(p, q,r)$ based on an embedding of the input knowledge graph computed using neural networks. The results of our evaluation on seven benchmark datasets suggest that nilpotent vectors can help capture 
    
[^11]: MQuinE:知识图谱嵌入模型中“Z-悖论”的解决方案

    MQuinE: a cure for "Z-paradox'' in knowledge graph embedding models

    [https://arxiv.org/abs/2402.03583](https://arxiv.org/abs/2402.03583)

    研究者发现知识图谱嵌入模型存在的“Z-悖论”限制了其表达能力，并提出了一种名为MQuinE的新模型，通过理论证明，MQuinE成功解决了Z-悖论，并在链接预测任务中显著优于现有模型。

    

    知识图谱嵌入（KGE）模型在许多知识图谱任务，包括链接预测和信息检索方面取得了最先进的结果。尽管KGE模型在实践中表现出优越性能，但我们发现一些流行的现有KGE模型存在表达不足的问题，称为“Z-悖论”。受到Z-悖论的存在的启发，我们提出了一种新的KGE模型，称为MQuinE，在不受Z-悖论的困扰的同时，保持强大的表达能力来模拟各种关系模式，包括对称/非对称，逆向，1-N/N-1/N-N和组合关系，并提供了理论上的证明。对实际知识库的实验表明，Z-悖论确实降低了现有KGE模型的性能，并且可能导致某些具有挑战性的测试样本的准确性下降超过20％。我们的实验进一步证明了MQuinE可以减轻Z-悖论的负面影响，并在链接预测方面以明显优势超越现有的KGE模型。

    Knowledge graph embedding (KGE) models achieved state-of-the-art results on many knowledge graph tasks including link prediction and information retrieval. Despite the superior performance of KGE models in practice, we discover a deficiency in the expressiveness of some popular existing KGE models called \emph{Z-paradox}. Motivated by the existence of Z-paradox, we propose a new KGE model called \emph{MQuinE} that does not suffer from Z-paradox while preserves strong expressiveness to model various relation patterns including symmetric/asymmetric, inverse, 1-N/N-1/N-N, and composition relations with theoretical justification. Experiments on real-world knowledge bases indicate that Z-paradox indeed degrades the performance of existing KGE models, and can cause more than 20\% accuracy drop on some challenging test samples. Our experiments further demonstrate that MQuinE can mitigate the negative impact of Z-paradox and outperform existing KGE models by a visible margin on link prediction
    
[^12]: 使用去噪扩散模型实现高感知质量的无线图像传输

    High Perceptual Quality Wireless Image Delivery with Denoising Diffusion Models. (arXiv:2309.15889v1 [eess.IV])

    [http://arxiv.org/abs/2309.15889](http://arxiv.org/abs/2309.15889)

    本论文研究了通过深度学习的联合源-信道编码和去噪扩散模型在噪声无线信道上进行图像传输的问题。通过利用范围-零空间分解和逐步优化零空间内容，实现了在失真和感知质量方面的显著改进。

    

    我们考虑通过基于深度学习的联合源-信道编码（DeepJSCC）以及接收端的去噪扩散概率模型（DDPM）在噪声无线信道上进行图像传输。我们特别关注在实际有限块长度的情况下的感知失真权衡问题，这种情况下，分离的源编码和信道编码可能会高度不理想。我们引入了一种利用目标图像的范围-零空间分解的新方案。我们在编码后传输图像的范围空间，并使用DDPM逐步优化其零空间内容。通过广泛的实验证明，与标准的DeepJSCC和最先进的生成式学习方法相比，我们在重构图像的失真和感知质量方面实现了显著改进。为了促进进一步的研究和可重现性，我们将公开分享我们的源代码。

    We consider the image transmission problem over a noisy wireless channel via deep learning-based joint source-channel coding (DeepJSCC) along with a denoising diffusion probabilistic model (DDPM) at the receiver. Specifically, we are interested in the perception-distortion trade-off in the practical finite block length regime, in which separate source and channel coding can be highly suboptimal. We introduce a novel scheme that utilizes the range-null space decomposition of the target image. We transmit the range-space of the image after encoding and employ DDPM to progressively refine its null space contents. Through extensive experiments, we demonstrate significant improvements in distortion and perceptual quality of reconstructed images compared to standard DeepJSCC and the state-of-the-art generative learning-based method. We will publicly share our source code to facilitate further research and reproducibility.
    
[^13]: 边界剥离：使用一类剥离的异常检测方法

    Boundary Peeling: Outlier Detection Method Using One-Class Peeling. (arXiv:2309.05630v1 [stat.ML])

    [http://arxiv.org/abs/2309.05630](http://arxiv.org/abs/2309.05630)

    一类边界剥离是一种无监督的异常检测算法，使用了平均有符号距离和灵活的边界生成方法。在无异常值和有异常值的情况下，一类边界剥离表现出优越的性能。

    

    无监督异常检测在数据分析中是至关重要的阶段，仍然是一个充满活力的研究领域。一个好的异常检测算法应该具备计算效率高、对调参选择鲁棒、在不同的数据分布下表现稳定等特点。我们介绍了一类边界剥离，一种无监督异常检测算法。一类边界剥离使用了一类支持向量机不断剥离的、灵活的边界生成的平均有符号距离。一类边界剥离具有鲁棒的超参数设置，并且为了增加灵活性，可以被看作是一个集成方法。在合成数据模拟中，一类边界剥离在没有异常值的情况下优于所有先进方法，并且在有异常值存在的情况下，与基准方法相比，在正确分类方面表现出可比或更好的性能。

    Unsupervised outlier detection constitutes a crucial phase within data analysis and remains a dynamic realm of research. A good outlier detection algorithm should be computationally efficient, robust to tuning parameter selection, and perform consistently well across diverse underlying data distributions. We introduce One-Class Boundary Peeling, an unsupervised outlier detection algorithm. One-class Boundary Peeling uses the average signed distance from iteratively-peeled, flexible boundaries generated by one-class support vector machines. One-class Boundary Peeling has robust hyperparameter settings and, for increased flexibility, can be cast as an ensemble method. In synthetic data simulations One-Class Boundary Peeling outperforms all state of the art methods when no outliers are present while maintaining comparable or superior performance in the presence of outliers, as compared to benchmark methods. One-Class Boundary Peeling performs competitively in terms of correct classificati
    
[^14]: 结构化径向基函数网络：用于多假设预测的多样性建模

    Structured Radial Basis Function Network: Modelling Diversity for Multiple Hypotheses Prediction. (arXiv:2309.00781v1 [cs.LG])

    [http://arxiv.org/abs/2309.00781](http://arxiv.org/abs/2309.00781)

    这篇论文提出了一种结构化径向基函数网络，用于解决多模态回归问题。该网络能够有效地组合多个假设预测器，并通过插值逼近多个假设目标分布，具有较好的性能。

    

    多模态回归对于预测非平稳过程或具有复杂分布的问题非常重要。可以通过多假设框架来处理，但在学习模型中有效地组合它们是有困难的。本文提出了一种结构化径向基函数网络，作为多假设预测器的集合，用于回归问题。这些预测器是任何类型的回归模型，可以形成以它们在训练过程中的损失为函数的重心维诺图分割。证明了这个结构化模型能够有效地插值这个分割，并且逼近多个假设目标分布，并且等价于插值预测器的元损失，损失是插值误差的零集。该模型在预测器和基函数中心之间具有固定点迭代算法。可以通过截断分割格式来参数化地控制学习中的多样性。

    Multi-modal regression is important in forecasting nonstationary processes or with a complex mixture of distributions. It can be tackled with multiple hypotheses frameworks but with the difficulty of combining them efficiently in a learning model. A Structured Radial Basis Function Network is presented as an ensemble of multiple hypotheses predictors for regression problems. The predictors are regression models of any type that can form centroidal Voronoi tessellations which are a function of their losses during training. It is proved that this structured model can efficiently interpolate this tessellation and approximate the multiple hypotheses target distribution and is equivalent to interpolating the meta-loss of the predictors, the loss being a zero set of the interpolation error. This model has a fixed-point iteration algorithm between the predictors and the centers of the basis functions. Diversity in learning can be controlled parametrically by truncating the tessellation format
    
[^15]: 重新思考置换对不同数据集间模型合并的作用

    Revisiting Permutation Symmetry for Merging Models between Different Datasets. (arXiv:2306.05641v1 [cs.LG])

    [http://arxiv.org/abs/2306.05641](http://arxiv.org/abs/2306.05641)

    本研究通过理论和实证分析表明，不同数据集合并模型的准确性下降更为显著，因为每个数据集的不同损失函数使得合并更加困难。此外，通过数据集压缩创建的压缩数据集可以作为原数据集的替代品。

    

    模型合并是一种通过组合不同训练模型的权重来创建新模型的新方法。以往的研究表明，模型合并对于不同随机数训练模型的单一数据集非常有效，但是在不同数据集之间进行模型合并却很困难。将不同数据集的知识合并具有实际意义，但尚未得到很好的研究。本文通过理论和实证分析探讨了不同数据集间合并模型的特性。我们发现随着数据集的差异越大，合并模型的准确性下降得更为显著，而每个数据集的不同损失函数使得不同数据集之间的模型合并更加困难。我们还表明合并的模型需要数据集才能实现高精度合并。此外，我们还表明当合并模型时，通过数据集压缩创建的压缩数据集可以作为原数据集的替代品。

    Model merging is a new approach to creating a new model by combining the weights of different trained models. Previous studies report that model merging works well for models trained on a single dataset with different random seeds, while model merging between different datasets is difficult. Merging knowledge from different datasets has practical significance, but it has not been well investigated. In this paper, we investigate the properties of merging models between different datasets. Through theoretical and empirical analyses, we find that the accuracy of the merged model decreases more significantly as the datasets diverge more and that the different loss landscapes for each dataset make model merging between different datasets difficult. We also show that merged models require datasets for merging in order to achieve a high accuracy. Furthermore, we show that condensed datasets created by dataset condensation can be used as substitutes for the original datasets when merging model
    
[^16]: 零样本任务偏好的不精确贝叶斯继续学习

    Zero-shot Task Preference Addressing Enabled by Imprecise Bayesian Continual Learning. (arXiv:2305.14782v1 [cs.LG])

    [http://arxiv.org/abs/2305.14782](http://arxiv.org/abs/2305.14782)

    提出了零样本任务偏好的不精确贝叶斯继续学习（IBCL）算法，该算法更新模型参数分布凸壳形式的知识库，并使用零样本获取模型以满足不同的偏好，使得在具有大量任务偏好的情况下更加可扩展。

    

    类似于通用的多任务学习，继续学习也具有多目标优化的特性，因此需要在不同任务的性能之间进行平衡。也就是说，为了优化当前任务分布，可能需要在一些任务上牺牲性能以提高其他任务的性能。这意味着存在多个模型，每个模型在不同的时间都是最优的，每个模型都能够解决不同的任务-性能权衡问题。研究人员已经讨论如何训练特定的模型以满足交易偏好。然而，现有的算法需要额外的采样开销-在存在多个，可能是无限数量的偏好时会产生很大的负担。因此，我们提出了不精确贝叶斯继续学习（IBCL）。一旦有新任务，IBCL会（1）更新一个以模型参数分布凸壳形式存在的知识库，（2）并使用零样本获取特定模型以满足不同的偏好。也就是说，IBCL不需要任何额外的数据就能为一个特定的任务偏好生成新的模型，使得在具有大量任务偏好的情况下更加可扩展。

    Like generic multi-task learning, continual learning has the nature of multi-objective optimization, and therefore faces a trade-off between the performance of different tasks. That is, to optimize for the current task distribution, it may need to compromise performance on some tasks to improve on others. This means there exist multiple models that are each optimal at different times, each addressing a distinct task-performance trade-off. Researchers have discussed how to train particular models to address specific preferences on these trade-offs. However, existing algorithms require additional sample overheads -- a large burden when there are multiple, possibly infinitely many, preferences. As a response, we propose Imprecise Bayesian Continual Learning (IBCL). Upon a new task, IBCL (1) updates a knowledge base in the form of a convex hull of model parameter distributions and (2) obtains particular models to address preferences with zero-shot. That is, IBCL does not require any additi
    
[^17]: 使用Shortcut Fine-Tuning优化DDPM采样

    Optimizing DDPM Sampling with Shortcut Fine-Tuning. (arXiv:2301.13362v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13362](http://arxiv.org/abs/2301.13362)

    本文提出了Shortcut Fine-Tuning（SFT）算法，利用直接最小化积分概率度量（IPM）来对DDPM采样器进行微调，从而有效提高DDPM采样效率和质量。

    

    本研究提出了Shortcut Fine-Tuning（SFT），这是一种新的方法，用于解决预训练去噪扩散概率模型（DDPMs）的快速采样挑战。SFT提倡通过直接最小化积分概率度量（IPM）来对DDPM采样器进行微调，而不是学习向后扩散过程。这使采样器能够发现一条替代的更高效的采样捷径，偏离向后扩散过程。通过控制角度的启示，我们提出了一种新算法SFT-PG：使用Policy Gradient进行的Shortcut Fine-Tuning，并证明在某些假设下，扩散模型相对于IPM的梯度下降等价于执行Policy Gradient。据我们所知，这是首次尝试利用强化学习（RL）方法来训练扩散模型。通过实证评估，我们证明了我们的微调方法可以进一步增强现有的快速DDPM采样器，从而导致显着的加速和质量提高。

    In this study, we propose Shortcut Fine-Tuning (SFT), a new approach for addressing the challenge of fast sampling of pretrained Denoising Diffusion Probabilistic Models (DDPMs). SFT advocates for the fine-tuning of DDPM samplers through the direct minimization of Integral Probability Metrics (IPM), instead of learning the backward diffusion process. This enables samplers to discover an alternative and more efficient sampling shortcut, deviating from the backward diffusion process. Inspired by a control perspective, we propose a new algorithm SFT-PG: Shortcut Fine-Tuning with Policy Gradient, and prove that under certain assumptions, gradient descent of diffusion models with respect to IPM is equivalent to performing policy gradient. To our best knowledge, this is the first attempt to utilize reinforcement learning (RL) methods to train diffusion models. Through empirical evaluation, we demonstrate that our fine-tuning method can further enhance existing fast DDPM samplers, resulting i
    
[^18]: FlexFringe:通过学习概率有限自动机来建模软件行为

    FlexFringe: Modeling Software Behavior by Learning Probabilistic Automata. (arXiv:2203.16331v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.16331](http://arxiv.org/abs/2203.16331)

    FlexFringe提供了高效的概率有限自动机学习方法，可用于建模软件行为。该方法在实践中通过实现改进的状态合并策略实现了显著性能提升，并且能够从软件日志中学习可解释的模型，用于异常检测。与基于神经网络的解决方案相比，学习更小更复杂的模型能够提高FlexFringe在异常检测中的性能。

    

    我们介绍了FlexFringe中可用的概率确定性有限自动机学习方法的高效实现。这些实现了众所周知的状态合并策略，包括几种修改以提高它们在实践中的性能。我们通过实验证明这些算法能够获得有竞争力的结果，并在默认实现上实现了显著的改进。我们还展示了如何使用FlexFringe从软件日志中学习可解释的模型，并将其用于异常检测。虽然这些模型较难解释，但我们展示了学习更小、更复杂的模型如何提高FlexFringe在异常检测中的性能，优于基于神经网络的现有解决方案。

    We present the efficient implementations of probabilistic deterministic finite automaton learning methods available in FlexFringe. These implement well-known strategies for state-merging including several modifications to improve their performance in practice. We show experimentally that these algorithms obtain competitive results and significant improvements over a default implementation. We also demonstrate how to use FlexFringe to learn interpretable models from software logs and use these for anomaly detection. Although less interpretable, we show that learning smaller more convoluted models improves the performance of FlexFringe on anomaly detection, outperforming an existing solution based on neural nets.
    

