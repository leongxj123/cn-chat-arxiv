# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Predictive Inference in Multi-environment Scenarios](https://arxiv.org/abs/2403.16336) | 本研究提出了在多环境预测问题中构建有效置信区间和置信集的方法，并展示了一种新的调整方法以适应问题难度，从而减少预测集大小，这在神经感应和物种分类数据集中的实际表现中得到验证。 |
| [^2] | [Training Survival Models using Scoring Rules](https://arxiv.org/abs/2403.13150) | 提出了一种使用评分规则训练生存模型的通用方法，将其应用于各种模型类别中并与神经网络结合，实现了高效可扩展的优化例程，并展示了优于基于似然性方法的预测性能。 |
| [^3] | [PAPER-HILT: Personalized and Adaptive Privacy-Aware Early-Exit for Reinforcement Learning in Human-in-the-Loop Systems](https://arxiv.org/abs/2403.05864) | PAPER-HILT是针对人机协同系统中隐私保护的创新自适应强化学习策略，通过提前退出方法动态调整隐私保护和系统效用，以适应个体行为模式和偏好。 |
| [^4] | [Why do Learning Rates Transfer? Reconciling Optimization and Scaling Limits for Deep Learning](https://arxiv.org/abs/2402.17457) | 学习速率迁移现象可以归因于在μP和其深度延伸下，训练损失Hessian矩阵的最大特征值（即锐度）在较长时间的训练过程中，基本独立于网络的宽度和深度。 |
| [^5] | [Attacking LLM Watermarks by Exploiting Their Strengths](https://arxiv.org/abs/2402.16187) | 现有的LLM水印系统虽然具有质量保留、鲁棒性和公开检测API等优点，但也因此容易受到各种攻击，研究者提出了一套实用指南以缓解这些攻击。 |
| [^6] | [Controlling Large Electric Vehicle Charging Stations via User Behavior Modeling and Stochastic Programming](https://arxiv.org/abs/2402.13224) | 本文介绍了一个新的电动汽车充电站模型，通过用户行为建模和随机规划，解决了充电会话不确定性问题，并提出了两种方法来优化成本并提高用户满意度。 |
| [^7] | [A Generalized Approach to Online Convex Optimization](https://arxiv.org/abs/2402.08621) | 这是一篇关于在线凸优化的论文，作者分析了不同环境下的问题并提出了一种通用的解决方法，该方法可以转化为相应的线性优化算法，并可以在面对不同类型对手时获得可比较的遗憾界限。 |
| [^8] | [Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models](https://arxiv.org/abs/2402.03271) | 通过引入不确定性感知规划（UoT）算法，我们实现了增强大型语言模型的主动寻求信息的能力，通过模拟未来场景、基于不确定性的奖励机制和奖励传播方案，优化问题提问方式。 |
| [^9] | [Are Large Language Models Table-based Fact-Checkers?](https://arxiv.org/abs/2402.02549) | 本研究初步探讨了大型语言模型在基于表格的事实检查方面的潜力。实验结果表明，通过提示工程，大型语言模型在零样本和少样本的情况下可以实现可接受的表现。 |
| [^10] | [Exact Fractional Inference via Re-Parametrization & Interpolation between Tree-Re-Weighted- and Belief Propagation- Algorithms](https://arxiv.org/abs/2301.10369) | 通过构建$\lambda$-分数全同维数，实现了树重新加权和信念传播算法之间的插值，确保在铁磁性情况下，存在“精确”$\lambda_*$使得计算的配分函数$Z=Z^{(\lambda_*)}$。 |
| [^11] | [A Practical Probabilistic Benchmark for AI Weather Models.](http://arxiv.org/abs/2401.15305) | 这篇论文提出了一个实用的概率基准，用于比较AI天气模型的概率技能。通过使用滞后集合，可以实现对多个模型的比较，并与操作基准进行对比。 |
| [^12] | [Optimal vintage factor analysis with deflation varimax.](http://arxiv.org/abs/2310.10545) | 本文提出了一种采用通货紧缩变量旋转的拟合因子分析方法，在每一行上逐步求解正交矩阵，相比于传统方法具有更好的计算性能和灵活性，并且在更广泛的背景下提供了理论保证。 |
| [^13] | [A Unified Analysis for the Subgradient Methods Minimizing Composite Nonconvex, Nonsmooth and Non-Lipschitz Functions.](http://arxiv.org/abs/2308.16362) | 本文提出了一种解决非凸非光滑非Lipschitz函数优化问题的近端次梯度方法，并建立了统一的递归关系用于 Moreau包络的建立。同时，提出了一些新的随机次梯度上界条件，并分析了随机次梯度方法的收敛性和迭代复杂度。 |
| [^14] | [On the Need for a Language Describing Distribution Shifts: Illustrations on Tabular Datasets.](http://arxiv.org/abs/2307.05284) | 该论文通过对表格数据集中的自然偏移进行研究，发现$Y|X$-偏移最为普遍。为了推动研究人员开发描述数据分布偏移的精细语言，作者构建了WhyShift实验平台，并讨论了$Y|X$-偏移对算法的影响。 |
| [^15] | [Exponential separations between classical and quantum learners.](http://arxiv.org/abs/2306.16028) | 本文研究了经典学习者和量子学习者之间的指数区别，并提出了两个新的学习分离问题，其中经典困难主要在于识别生成数据的函数。 |
| [^16] | [Simple Steps to Success: Axiomatics of Distance-Based Algorithmic Recourse.](http://arxiv.org/abs/2306.15557) | 我们提出了一种基于距离的算法补偿的新方法，通过在数据流形中提供用户可以采取的方向来改变其预测结果。该方法具有高效性、可证明的隐私和鲁棒性保证，并在实验证明上优于现有技术。 |
| [^17] | [V-LoL: A Diagnostic Dataset for Visual Logical Learning.](http://arxiv.org/abs/2306.07743) | V-LoL是一个结合视觉和逻辑挑战的诊断数据集，其中包括了V-LoL-Trains，该数据集首次将复杂的视觉场景和灵活的逻辑推理任务结合起来，为研究广泛的视觉逻辑学习挑战提供了平台。 |
| [^18] | [ADI: Adversarial Dominating Inputs in Vertical Federated Learning Systems.](http://arxiv.org/abs/2201.02775) | 本文研究了垂直联邦学习系统中的对抗性主导输入（ADIs），并证明了其在典型VFL系统中的存在。该研究为防止ADIs的使用提供了方法。 |

# 详细

[^1]: 多环境场景中的预测推断

    Predictive Inference in Multi-environment Scenarios

    [https://arxiv.org/abs/2403.16336](https://arxiv.org/abs/2403.16336)

    本研究提出了在多环境预测问题中构建有效置信区间和置信集的方法，并展示了一种新的调整方法以适应问题难度，从而减少预测集大小，这在神经感应和物种分类数据集中的实际表现中得到验证。

    

    我们解决了在跨多个环境的预测问题中构建有效置信区间和置信集的挑战。我们研究了适用于这些问题的两种覆盖类型，扩展了Jackknife和分裂一致方法，展示了如何在这种非传统的层次数据生成场景中获得无分布覆盖。我们的贡献还包括对非实值响应设置的扩展，以及这些一般问题中预测推断的一致性理论。我们展示了一种新的调整方法，以适应问题难度，这适用于具有层次数据的预测推断的现有方法以及我们开发的方法；这通过神经化学感应和物种分类数据集评估了这些方法的实际性能。

    arXiv:2403.16336v1 Announce Type: cross  Abstract: We address the challenge of constructing valid confidence intervals and sets in problems of prediction across multiple environments. We investigate two types of coverage suitable for these problems, extending the jackknife and split-conformal methods to show how to obtain distribution-free coverage in such non-traditional, hierarchical data-generating scenarios. Our contributions also include extensions for settings with non-real-valued responses and a theory of consistency for predictive inference in these general problems. We demonstrate a novel resizing method to adapt to problem difficulty, which applies both to existing approaches for predictive inference with hierarchical data and the methods we develop; this reduces prediction set sizes using limited information from the test environment, a key to the methods' practical performance, which we evaluate through neurochemical sensing and species classification datasets.
    
[^2]: 使用评分规则训练生存模型

    Training Survival Models using Scoring Rules

    [https://arxiv.org/abs/2403.13150](https://arxiv.org/abs/2403.13150)

    提出了一种使用评分规则训练生存模型的通用方法，将其应用于各种模型类别中并与神经网络结合，实现了高效可扩展的优化例程，并展示了优于基于似然性方法的预测性能。

    

    生存分析为各个领域中部分不完整的事件发生时间数据提供了关键见解。它也是概率机器学习的一个重要示例。我们的提案以一种通用的方式利用了预测的概率性质，通过在模型拟合过程中使用（合适的）评分规则而非基于似然性的优化。我们建立了不同的参数化和非参数化子框架，允许不同程度的灵活性。将其混入神经网络中，导致了一个计算有效且可扩展的优化例程，产生了最先进的预测性能。最后，我们展示了使用我们的框架，可以恢复各种参数化模型，并证明在与基于似然性方法的比较中，优化效果同样出色。

    arXiv:2403.13150v1 Announce Type: new  Abstract: Survival Analysis provides critical insights for partially incomplete time-to-event data in various domains. It is also an important example of probabilistic machine learning. The probabilistic nature of the predictions can be exploited by using (proper) scoring rules in the model fitting process instead of likelihood-based optimization. Our proposal does so in a generic manner and can be used for a variety of model classes. We establish different parametric and non-parametric sub-frameworks that allow different degrees of flexibility. Incorporated into neural networks, it leads to a computationally efficient and scalable optimization routine, yielding state-of-the-art predictive performance. Finally, we show that using our framework, we can recover various parametric models and demonstrate that optimization works equally well when compared to likelihood-based methods.
    
[^3]: PAPER-HILT：个性化和自适应隐私感知的强化学习提前退出在人机协同系统中的应用

    PAPER-HILT: Personalized and Adaptive Privacy-Aware Early-Exit for Reinforcement Learning in Human-in-the-Loop Systems

    [https://arxiv.org/abs/2403.05864](https://arxiv.org/abs/2403.05864)

    PAPER-HILT是针对人机协同系统中隐私保护的创新自适应强化学习策略，通过提前退出方法动态调整隐私保护和系统效用，以适应个体行为模式和偏好。

    

    强化学习（RL）日益成为人机协同（HITL）应用中的首选方法，因其适应于人类交互的动态特性。然而，在这种环境中整合RL会带来重大的隐私问题，可能会不经意地暴露敏感用户信息。为解决这一问题，我们的论文专注于开发PAPER-HILT，一种创新的自适应RL策略，通过利用专为HITL环境中隐私保护设计的提前退出方法。该方法动态调整隐私保护和系统效用之间的权衡，使其操作适应个人行为模式和偏好。我们主要强调面临处理人类行为的可变和不断发展的挑战，使得静态隐私模型失效。通过其应用，评估了PAPER-HILT的有效性。

    arXiv:2403.05864v1 Announce Type: new  Abstract: Reinforcement Learning (RL) has increasingly become a preferred method over traditional rule-based systems in diverse human-in-the-loop (HITL) applications due to its adaptability to the dynamic nature of human interactions. However, integrating RL in such settings raises significant privacy concerns, as it might inadvertently expose sensitive user information. Addressing this, our paper focuses on developing PAPER-HILT, an innovative, adaptive RL strategy through exploiting an early-exit approach designed explicitly for privacy preservation in HITL environments. This approach dynamically adjusts the tradeoff between privacy protection and system utility, tailoring its operation to individual behavioral patterns and preferences. We mainly highlight the challenge of dealing with the variable and evolving nature of human behavior, which renders static privacy models ineffective. PAPER-HILT's effectiveness is evaluated through its applicati
    
[^4]: 为什么学习速率具有迁移性？调和深度学习中的优化和尺度极限

    Why do Learning Rates Transfer? Reconciling Optimization and Scaling Limits for Deep Learning

    [https://arxiv.org/abs/2402.17457](https://arxiv.org/abs/2402.17457)

    学习速率迁移现象可以归因于在μP和其深度延伸下，训练损失Hessian矩阵的最大特征值（即锐度）在较长时间的训练过程中，基本独立于网络的宽度和深度。

    

    最近，越来越多的证据表明，如果神经网络的宽度和深度朝着所谓的丰富特征学习极限（μP及其深度扩展）进行缩放，那么某些超参数 - 例如学习速率 - 就会从小模型转移到非常大的模型，从而降低了超参数调整的成本。从优化的角度来看，这种现象令人困惑，因为它意味着损失景观在非常不同的模型尺寸之间是非常一致的。在这项工作中，我们找到证据支持学习速率迁移可以归因于事实：在μP及其深度扩展下，训练损失Hessian的最大特征值（即锐度）在较长的训练时间内在很大程度上独立于网络的宽度和深度。另一方面，我们展示在神经切线核（NTK）体系下，锐度在不同

    arXiv:2402.17457v1 Announce Type: new  Abstract: Recently, there has been growing evidence that if the width and depth of a neural network are scaled toward the so-called rich feature learning limit ($\mu$P and its depth extension), then some hyperparameters - such as the learning rate - exhibit transfer from small to very large models, thus reducing the cost of hyperparameter tuning. From an optimization perspective, this phenomenon is puzzling, as it implies that the loss landscape is remarkably consistent across very different model sizes. In this work, we find empirical evidence that learning rate transfer can be attributed to the fact that under $\mu$P and its depth extension, the largest eigenvalue of the training loss Hessian (i.e. the sharpness) is largely independent of the width and depth of the network for a sustained period of training time. On the other hand, we show that under the neural tangent kernel (NTK) regime, the sharpness exhibits very different dynamics at differ
    
[^5]: 利用其优势攻击LLM水印

    Attacking LLM Watermarks by Exploiting Their Strengths

    [https://arxiv.org/abs/2402.16187](https://arxiv.org/abs/2402.16187)

    现有的LLM水印系统虽然具有质量保留、鲁棒性和公开检测API等优点，但也因此容易受到各种攻击，研究者提出了一套实用指南以缓解这些攻击。

    

    生成模型的进展使得人工智能生成的文本、代码和图片能够在许多应用中模仿人类生成的内容。水印技术旨在将信息嵌入模型的输出中以验证其来源，对于减少对这些人工智能生成内容的滥用非常有用。然而，现有的水印方案仍然令人意外地容易受到攻击。具体而言，我们展示了现有的LLM水印系统共享的可取特性，例如质量保留、鲁棒性和公开检测API，反过来却使这些系统容易遭受各种攻击。我们在常见水印设计选择方面严格研究潜在攻击，并提出了缓解攻击的最佳实践和防御措施——建立了一套嵌入和检测LLM水印的实用指南。

    arXiv:2402.16187v1 Announce Type: cross  Abstract: Advances in generative models have made it possible for AI-generated text, code, and images to mirror human-generated content in many applications. Watermarking, a technique that aims to embed information in the output of a model to verify its source, is useful for mitigating misuse of such AI-generated content. However, existing watermarking schemes remain surprisingly susceptible to attack. In particular, we show that desirable properties shared by existing LLM watermarking systems such as quality preservation, robustness, and public detection APIs can in turn make these systems vulnerable to various attacks. We rigorously study potential attacks in terms of common watermark design choices, and propose best practices and defenses for mitigation -- establishing a set of practical guidelines for embedding and detection of LLM watermarks.
    
[^6]: 通过用户行为建模和随机规划控制大型电动汽车充电站

    Controlling Large Electric Vehicle Charging Stations via User Behavior Modeling and Stochastic Programming

    [https://arxiv.org/abs/2402.13224](https://arxiv.org/abs/2402.13224)

    本文介绍了一个新的电动汽车充电站模型，通过用户行为建模和随机规划，解决了充电会话不确定性问题，并提出了两种方法来优化成本并提高用户满意度。

    

    本文介绍了一个电动汽车充电站（EVCS）模型，该模型融合了真实世界的约束条件，如插槽功率限制、合同阈值超限惩罚以及电动汽车（EVs）的早期断开。我们提出了一个在不确定性下控制EVCS的问题形式，并实施了两种多阶段随机规划方法，利用用户提供的信息，即模型预测控制和二阶段随机规划。该模型解决了充电会话开始和结束时间以及能量需求的不确定性。基于驻留时间依赖随机过程的用户行为模型增强了成本降低的同时保持客户满意度。通过使用真实世界数据集进行的22天模拟展示了两种提出方法相对于两个基线的优势。两阶段方法证明了针对早期断开的鲁棒性，考虑了更多

    arXiv:2402.13224v1 Announce Type: cross  Abstract: This paper introduces an Electric Vehicle Charging Station (EVCS) model that incorporates real-world constraints, such as slot power limitations, contract threshold overruns penalties, or early disconnections of electric vehicles (EVs). We propose a formulation of the problem of EVCS control under uncertainty, and implement two Multi-Stage Stochastic Programming approaches that leverage user-provided information, namely, Model Predictive Control and Two-Stage Stochastic Programming. The model addresses uncertainties in charging session start and end times, as well as in energy demand. A user's behavior model based on a sojourn-time-dependent stochastic process enhances cost reduction while maintaining customer satisfaction. The benefits of the two proposed methods are showcased against two baselines over a 22-day simulation using a real-world dataset. The two-stage approach proves robust against early disconnections, considering a more
    
[^7]: 一种广义的在线凸优化方法

    A Generalized Approach to Online Convex Optimization

    [https://arxiv.org/abs/2402.08621](https://arxiv.org/abs/2402.08621)

    这是一篇关于在线凸优化的论文，作者分析了不同环境下的问题并提出了一种通用的解决方法，该方法可以转化为相应的线性优化算法，并可以在面对不同类型对手时获得可比较的遗憾界限。

    

    在本文中，我们分析了不同环境下的在线凸优化问题。我们证明了任何用于具有完全自适应对手的在线线性优化的算法都是用于在线凸优化的算法。我们还证明了任何需要全信息反馈的算法都可以转化为具有可比较的遗憾界限的半匹配反馈算法。此外，我们还证明了使用确定性半匹配反馈的全自适应对手设计的算法在面对无知对手时可以使用只有随机半匹配反馈的算法获得相似的界限。我们利用这一结果描述了将一阶算法转化为零阶算法的通用元算法，这些算法具有可比较的遗憾界限。我们的框架使我们能够分析各种设置中的在线优化问题，包括全信息反馈、半匹配反馈、随机遗憾、对抗遗憾和各种形式的非平稳遗憾。利用我们的分析结果，

    In this paper, we analyze the problem of online convex optimization in different settings. We show that any algorithm for online linear optimization with fully adaptive adversaries is an algorithm for online convex optimization. We also show that any such algorithm that requires full-information feedback may be transformed to an algorithm with semi-bandit feedback with comparable regret bound. We further show that algorithms that are designed for fully adaptive adversaries using deterministic semi-bandit feedback can obtain similar bounds using only stochastic semi-bandit feedback when facing oblivious adversaries. We use this to describe general meta-algorithms to convert first order algorithms to zeroth order algorithms with comparable regret bounds. Our framework allows us to analyze online optimization in various settings, such full-information feedback, bandit feedback, stochastic regret, adversarial regret and various forms of non-stationary regret. Using our analysis, we provide
    
[^8]: 想法的不确定性：不确定性感知规划增强大型语言模型的信息搜索能力

    Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models

    [https://arxiv.org/abs/2402.03271](https://arxiv.org/abs/2402.03271)

    通过引入不确定性感知规划（UoT）算法，我们实现了增强大型语言模型的主动寻求信息的能力，通过模拟未来场景、基于不确定性的奖励机制和奖励传播方案，优化问题提问方式。

    

    在面对不确定性时，寻求信息的能力至关重要。在许多实际应用中，比如医学诊断和故障排除，解决任务所需的信息不是初始给定的，而需要通过询问后续问题来主动寻求（例如，医生向患者询问症状的更多细节）。在这项工作中，我们引入了思想的不确定性（UoT），一种算法将大型语言模型的能力与主动提问信息的能力相结合。UoT结合了1）不确定性感知仿真方法，使模型能够模拟可能的未来场景，并估计其发生的可能性；2）基于不确定性的奖励机制，激励模型寻求信息；3）奖励传播方案，以最大化预期奖励的方式选择最佳的问题提问方式。在医学诊断、故障排除和'20的实验中。

    In the face of uncertainty, the ability to seek information is of fundamental importance. In many practical applications, such as medical diagnosis and troubleshooting, the information needed to solve the task is not initially given, and has to be actively sought by asking follow-up questions (for example, a doctor asking a patient for more details about their symptoms). In this work, we introduce Uncertainty of Thoughts (UoT), an algorithm to augment large language models with the ability to actively seek information by asking effective questions. UoT combines 1) an uncertainty-aware simulation approach which enables the model to simulate possible future scenarios and how likely they are to occur, 2) uncertainty-based rewards motivated by information gain which incentivizes the model to seek information, and 3) a reward propagation scheme to select the optimal question to ask in a way that maximizes the expected reward. In experiments on medical diagnosis, troubleshooting and the '20 
    
[^9]: 大型语言模型是否适合基于表格的事实检查？

    Are Large Language Models Table-based Fact-Checkers?

    [https://arxiv.org/abs/2402.02549](https://arxiv.org/abs/2402.02549)

    本研究初步探讨了大型语言模型在基于表格的事实检查方面的潜力。实验结果表明，通过提示工程，大型语言模型在零样本和少样本的情况下可以实现可接受的表现。

    

    基于表格的事实验证（TFV）旨在提取语句和结构化表格之间的蕴涵关系。现有基于小规模模型的TFV方法在标注数据不足和零样本能力薄弱方面存在问题。近年来，大型语言模型（LLMs）在研究领域引起了广泛关注。它们在几个自然语言处理任务上展示了强大的零样本和上下文学习能力，但它们在TFV领域的潜力还不清楚。在本文中，我们进行了关于LLMs是否适合作为基于表格的事实检查器的初步研究。具体来说，我们设计了多样化的提示语来探索上下文学习如何帮助LLMs在TFV方面，即零样本和少样本TFV能力。此外，我们精心设计和构建了TFV指导以研究LLMs的指导调整带来的性能改进。实验结果表明，通过提示工程，LLMs在零样本和少样本TFV方面可以达到可接受的结果，而指导调整则进一步提升了性能。

    Table-based Fact Verification (TFV) aims to extract the entailment relation between statements and structured tables. Existing TFV methods based on small-scaled models suffer from insufficient labeled data and weak zero-shot ability. Recently, the appearance of Large Language Models (LLMs) has gained lots of attraction in research fields. They have shown powerful zero-shot and in-context learning abilities on several NLP tasks, but their potential on TFV is still unknown. In this work, we implement a preliminary study about whether LLMs are table-based fact-checkers. In detail, we design diverse prompts to explore how the in-context learning can help LLMs in TFV, i.e., zero-shot and few-shot TFV capability. Besides, we carefully design and construct TFV instructions to study the performance gain brought by the instruction tuning of LLMs. Experimental results demonstrate that LLMs can achieve acceptable results on zero-shot and few-shot TFV with prompt engineering, while instruction-tun
    
[^10]: 精确分数推断：重新参数化及树重新加权和信念传播算法之间的插值

    Exact Fractional Inference via Re-Parametrization & Interpolation between Tree-Re-Weighted- and Belief Propagation- Algorithms

    [https://arxiv.org/abs/2301.10369](https://arxiv.org/abs/2301.10369)

    通过构建$\lambda$-分数全同维数，实现了树重新加权和信念传播算法之间的插值，确保在铁磁性情况下，存在“精确”$\lambda_*$使得计算的配分函数$Z=Z^{(\lambda_*)}$。

    

    推断工作--计算Ising模型在N个“自旋”组成的图上的配分函数$Z$所需的工作--很可能随着N呈指数增长。高效的变分方法，如信念传播（BP）和树重新加权（TRW）算法，通过计算最小化各自（BP或TRW）自由能的$Z$来近似计算$Z$。我们通过构建一个$\lambda$-分数全同维数，$Z^{(\lambda)}$，其中$\lambda=0$和$\lambda=1$分别对应于TRW和BP的近似，且$Z^{(\lambda)}$随$\lambda$单调减少。此外，这种分数方案保证在吸引力（铁磁性）情况下$Z^{(TRW)}\geq Z^{(\lambda)}\geq Z^{(BP)}$，并且存在一个唯一的（“精确”）$\lambda_*$，使得$Z=Z^{(\lambda_*)}$。通过推广\citep {wainwright_tree-based_2002}的重新参数化方法和\citep {chertkov_loop_2006}的环级数方法，我们展示了如何进行e

    arXiv:2301.10369v2 Announce Type: replace  Abstract: Inference efforts -- required to compute partition function, $Z$, of an Ising model over a graph of $N$ ``spins" -- are most likely exponential in $N$. Efficient variational methods, such as Belief Propagation (BP) and Tree Re-Weighted (TRW) algorithms, compute $Z$ approximately minimizing respective (BP- or TRW-) free energy. We generalize the variational scheme building a $\lambda$-fractional-homotopy, $Z^{(\lambda)}$, where $\lambda=0$ and $\lambda=1$ correspond to TRW- and BP-approximations, respectively, and $Z^{(\lambda)}$ decreases with $\lambda$ monotonically. Moreover, this fractional scheme guarantees that in the attractive (ferromagnetic) case $Z^{(TRW)}\geq Z^{(\lambda)}\geq Z^{(BP)}$, and there exists a unique (``exact") $\lambda_*$ such that, $Z=Z^{(\lambda_*)}$. Generalizing the re-parametrization approach of \citep{wainwright_tree-based_2002} and the loop series approach of \citep{chertkov_loop_2006}, we show how to e
    
[^11]: AI天气模型的实用概率基准

    A Practical Probabilistic Benchmark for AI Weather Models. (arXiv:2401.15305v1 [physics.ao-ph])

    [http://arxiv.org/abs/2401.15305](http://arxiv.org/abs/2401.15305)

    这篇论文提出了一个实用的概率基准，用于比较AI天气模型的概率技能。通过使用滞后集合，可以实现对多个模型的比较，并与操作基准进行对比。

    

    由于天气的混沌性质，天气预报旨在预测未来状态的分布而不是做出单个预测。最近，出现了多个数据驱动的天气模型，声称在技术上取得了突破。然而，这些模型大多使用确定性技能评分进行基准测试，对它们的概率技能知之甚少。不幸的是，在概率意义上公平比较AI天气模型是困难的，因为集合初始化的选择、状态定义和噪声注入方法的变化会产生混淆。此外，考虑到所涉及的数据量，即使是获得集合预报的基线也是一个巨大的工程挑战。为了解决这两个问题，我们采用了几十年前的概念 - 滞后集合，通过一个适度规模的确定性预测库构建集合。这允许对领先的AI天气模型的概率技能进行第一个无参数比较，并与操作基准进行对比。

    Since the weather is chaotic, forecasts aim to predict the distribution of future states rather than make a single prediction. Recently, multiple data driven weather models have emerged claiming breakthroughs in skill. However, these have mostly been benchmarked using deterministic skill scores, and little is known about their probabilistic skill. Unfortunately, it is hard to fairly compare AI weather models in a probabilistic sense, since variations in choice of ensemble initialization, definition of state, and noise injection methodology become confounding. Moreover, even obtaining ensemble forecast baselines is a substantial engineering challenge given the data volumes involved. We sidestep both problems by applying a decades-old idea -- lagged ensembles -- whereby an ensemble can be constructed from a moderately-sized library of deterministic forecasts. This allows the first parameter-free intercomparison of leading AI weather models' probabilistic skill against an operational base
    
[^12]: 优化拟合因子分析与通货紧缩变量旋转

    Optimal vintage factor analysis with deflation varimax. (arXiv:2310.10545v1 [stat.ML])

    [http://arxiv.org/abs/2310.10545](http://arxiv.org/abs/2310.10545)

    本文提出了一种采用通货紧缩变量旋转的拟合因子分析方法，在每一行上逐步求解正交矩阵，相比于传统方法具有更好的计算性能和灵活性，并且在更广泛的背景下提供了理论保证。

    

    通货紧缩变量旋转是一种重要的因子分析方法，旨在首先找到原始数据的低维表示，然后寻求旋转，使旋转后的低维表示具有科学意义。尽管Principal Component Analysis (PCA) followed by the varimax rotation被广泛应用于拟合因子分析，但由于varimax rotation需要在正交矩阵集合上解非凸优化问题，因此很难提供理论保证。本文提出了一种逐行求解正交矩阵的通货紧缩变量旋转过程。除了在计算上的优势和灵活性之外，我们还能在广泛的背景下对所提出的过程进行完全的理论保证。在PCA之后采用这种新的varimax方法作为第二步，我们进一步分析了这个两步过程在一个更一般的因子模型的情况下。

    Vintage factor analysis is one important type of factor analysis that aims to first find a low-dimensional representation of the original data, and then to seek a rotation such that the rotated low-dimensional representation is scientifically meaningful. Perhaps the most widely used vintage factor analysis is the Principal Component Analysis (PCA) followed by the varimax rotation. Despite its popularity, little theoretical guarantee can be provided mainly because varimax rotation requires to solve a non-convex optimization over the set of orthogonal matrices.  In this paper, we propose a deflation varimax procedure that solves each row of an orthogonal matrix sequentially. In addition to its net computational gain and flexibility, we are able to fully establish theoretical guarantees for the proposed procedure in a broad context.  Adopting this new varimax approach as the second step after PCA, we further analyze this two step procedure under a general class of factor models. Our resul
    
[^13]: 解决复合非凸非光滑非Lipschitz函数的次梯度方法的统一分析

    A Unified Analysis for the Subgradient Methods Minimizing Composite Nonconvex, Nonsmooth and Non-Lipschitz Functions. (arXiv:2308.16362v1 [math.OC])

    [http://arxiv.org/abs/2308.16362](http://arxiv.org/abs/2308.16362)

    本文提出了一种解决非凸非光滑非Lipschitz函数优化问题的近端次梯度方法，并建立了统一的递归关系用于 Moreau包络的建立。同时，提出了一些新的随机次梯度上界条件，并分析了随机次梯度方法的收敛性和迭代复杂度。

    

    本文提出了一种用于解决非凸非光滑优化问题的近端次梯度方法(Prox-SubGrad)，而无需假设Lipschitz连续性条件。我们提出了一些次梯度上界及其关系，并通过这些上界条件建立了弱凸优化Moreau包络的一些统一递归关系。这种统一方案简化并统一了建立Prox-SubGrad收敛速率的证明方案，而无需假设Lipschitz连续性。我们在这个背景下提出了一种新的收敛性分析。此外，我们还提出了一些新的随机次梯度上界条件，并为随机次梯度方法(Sto-SubGrad)解决非Lipschitz非光滑随机优化问题建立了收敛性和迭代复杂度的性质。特别地，对于无需Lipschitz连续性的弱凸优化问题，我们还讨论了确定性和随机次梯度方法的收敛性。

    In this paper we propose a proximal subgradient method (Prox-SubGrad) for solving nonconvex and nonsmooth optimization problems without assuming Lipschitz continuity conditions. A number of subgradient upper bounds and their relationships are presented. By means of these upper bounding conditions, we establish some uniform recursive relations for the Moreau envelopes for weakly convex optimization. This uniform scheme simplifies and unifies the proof schemes to establish rate of convergence for Prox-SubGrad without assuming Lipschitz continuity. We present a novel convergence analysis in this context. Furthermore, we propose some new stochastic subgradient upper bounding conditions and establish convergence and iteration complexity rates for the stochastic subgradient method (Sto-SubGrad) to solve non-Lipschitz and nonsmooth stochastic optimization problems. In particular, for both deterministic and stochastic subgradient methods on weakly convex optimization problems without Lipschitz
    
[^14]: 关于需要描述分布偏移的语言：基于表格数据集的案例分析

    On the Need for a Language Describing Distribution Shifts: Illustrations on Tabular Datasets. (arXiv:2307.05284v1 [cs.LG])

    [http://arxiv.org/abs/2307.05284](http://arxiv.org/abs/2307.05284)

    该论文通过对表格数据集中的自然偏移进行研究，发现$Y|X$-偏移最为普遍。为了推动研究人员开发描述数据分布偏移的精细语言，作者构建了WhyShift实验平台，并讨论了$Y|X$-偏移对算法的影响。

    

    不同的分布偏移需要不同的算法和操作干预。方法研究必须以其所涉及的具体偏移为基础。尽管新兴的基准数据为实证研究提供了有希望的基础，但它们隐含地关注协变量偏移，并且实证发现的有效性取决于偏移类型，例如，当$Y|X$分布发生变化时，之前关于算法性能的观察可能无效。我们对5个表格数据集中的自然偏移进行了深入研究，通过对86,000个模型配置进行实验，发现$Y|X$-偏移最为普遍。为了鼓励研究人员开发一种精细的描述数据分布偏移的语言，我们构建了WhyShift，一个由策划的真实世界偏移测试平台，在其中我们对我们基准性能的偏移类型进行了表征。由于$Y|X$-偏移在表格设置中很常见，我们确定了受到最大$Y|X$-偏移影响的协变量区域，并讨论了对算法的影响。

    Different distribution shifts require different algorithmic and operational interventions. Methodological research must be grounded by the specific shifts they address. Although nascent benchmarks provide a promising empirical foundation, they implicitly focus on covariate shifts, and the validity of empirical findings depends on the type of shift, e.g., previous observations on algorithmic performance can fail to be valid when the $Y|X$ distribution changes. We conduct a thorough investigation of natural shifts in 5 tabular datasets over 86,000 model configurations, and find that $Y|X$-shifts are most prevalent. To encourage researchers to develop a refined language for distribution shifts, we build WhyShift, an empirical testbed of curated real-world shifts where we characterize the type of shift we benchmark performance over. Since $Y|X$-shifts are prevalent in tabular settings, we identify covariate regions that suffer the biggest $Y|X$-shifts and discuss implications for algorithm
    
[^15]: 经典学习者与量子学习者之间的指数区别

    Exponential separations between classical and quantum learners. (arXiv:2306.16028v1 [quant-ph])

    [http://arxiv.org/abs/2306.16028](http://arxiv.org/abs/2306.16028)

    本文研究了经典学习者和量子学习者之间的指数区别，并提出了两个新的学习分离问题，其中经典困难主要在于识别生成数据的函数。

    

    尽管量子机器学习社区在处理经典数据时已经展示了量子学习优势，但在寻找量子学习算法可以在经典学习算法上实现可证明指数加速的学习问题方面，仍然存在挑战。本文讨论与此问题相关的计算学习理论概念，并讨论定义上的微妙差异可能导致学习者需要满足和解决显著不同的要求和任务。我们研究了已有的具有可证明量子加速性质的学习问题，并发现它们主要依赖于计算生成数据的函数的经典难度，而不是识别这个函数。为了解决这个问题，我们提出了两个新的学习分离问题，其中经典困难主要在于识别生成数据的函数。

    Despite significant effort, the quantum machine learning community has only demonstrated quantum learning advantages for artificial cryptography-inspired datasets when dealing with classical data. In this paper we address the challenge of finding learning problems where quantum learning algorithms can achieve a provable exponential speedup over classical learning algorithms. We reflect on computational learning theory concepts related to this question and discuss how subtle differences in definitions can result in significantly different requirements and tasks for the learner to meet and solve. We examine existing learning problems with provable quantum speedups and find that they largely rely on the classical hardness of evaluating the function that generates the data, rather than identifying it. To address this, we present two new learning separations where the classical difficulty primarily lies in identifying the function generating the data. Furthermore, we explore computational h
    
[^16]: 简单成功的步骤：基于距离的算法补偿的公理化方法

    Simple Steps to Success: Axiomatics of Distance-Based Algorithmic Recourse. (arXiv:2306.15557v1 [cs.LG])

    [http://arxiv.org/abs/2306.15557](http://arxiv.org/abs/2306.15557)

    我们提出了一种基于距离的算法补偿的新方法，通过在数据流形中提供用户可以采取的方向来改变其预测结果。该方法具有高效性、可证明的隐私和鲁棒性保证，并在实验证明上优于现有技术。

    

    我们提出了一个新颖的数据驱动框架，用于算法补偿，提供给用户改变其预测结果的干预措施。现有的计算补偿方法找到满足某些期望的点集，例如在基础因果图中的干预，或者最小化代价函数。然而，满足这些标准需要对基础模型结构有广泛的了解，在几个领域中往往需要大量的不切实际的信息。我们提出了一种数据驱动的、计算高效的算法补偿方法，通过在数据流形中提供用户可以采取的方向来改变其预测结果。我们提出了一种公理化合理化的方法，Stepwise Explainable Paths (StEP)，用于计算基于方向的算法补偿。我们对StEP进行了彻底的实证和理论研究。StEP提供了可证明的隐私和鲁棒性保证，并在几个已建立的补偿方法中表现优于现有技术。

    We propose a novel data-driven framework for algorithmic recourse that offers users interventions to change their predicted outcome. Existing approaches to compute recourse find a set of points that satisfy some desiderata -- e.g. an intervention in the underlying causal graph, or minimizing a cost function. Satisfying these criteria, however, requires extensive knowledge of the underlying model structure, often an unrealistic amount of information in several domains. We propose a data-driven, computationally efficient approach to computing algorithmic recourse. We do so by suggesting directions in the data manifold that users can take to change their predicted outcome. We present Stepwise Explainable Paths (StEP), an axiomatically justified framework to compute direction-based algorithmic recourse. We offer a thorough empirical and theoretical investigation of StEP. StEP offers provable privacy and robustness guarantees, and outperforms the state-of-the-art on several established reco
    
[^17]: V-LoL: 一种用于视觉逻辑学习的诊断数据集

    V-LoL: A Diagnostic Dataset for Visual Logical Learning. (arXiv:2306.07743v1 [cs.AI])

    [http://arxiv.org/abs/2306.07743](http://arxiv.org/abs/2306.07743)

    V-LoL是一个结合视觉和逻辑挑战的诊断数据集，其中包括了V-LoL-Trains，该数据集首次将复杂的视觉场景和灵活的逻辑推理任务结合起来，为研究广泛的视觉逻辑学习挑战提供了平台。

    

    尽管近期在视觉AI领域有了许多成功的进展，但仍存在不同的缺点；包括缺少精确的逻辑推理、抽象的概括能力以及理解复杂和嘈杂的场景等。不幸的是，现有的基准测试数据集并不能捕捉到这些方面中的多数。深度学习数据集关注视觉复杂数据但只有简单的视觉推理任务，归纳逻辑数据集包括复杂的逻辑学习任务，但是缺乏视觉的组成部分。为了解决这个问题，我们提出了视觉逻辑学习数据集V-LoL，它无缝地结合了视觉和逻辑的挑战。值得注意的是，我们首次推出了V-LoL的第一个实例，名为V-LoL-Trains，它是符号AI中一个经典基准测试的视觉呈现，即Michalski火车问题。通过在一个通用框架内结合复杂的视觉场景和灵活的逻辑推理任务，V-LoL-Trains为研究广泛的视觉逻辑学习挑战提供了平台。

    Despite the successes of recent developments in visual AI, different shortcomings still exist; from missing exact logical reasoning, to abstract generalization abilities, to understanding complex and noisy scenes. Unfortunately, existing benchmarks, were not designed to capture more than a few of these aspects. Whereas deep learning datasets focus on visually complex data but simple visual reasoning tasks, inductive logic datasets involve complex logical learning tasks, however, lack the visual component. To address this, we propose the visual logical learning dataset, V-LoL, that seamlessly combines visual and logical challenges. Notably, we introduce the first instantiation of V-LoL, V-LoL-Trains, -- a visual rendition of a classic benchmark in symbolic AI, the Michalski train problem. By incorporating intricate visual scenes and flexible logical reasoning tasks within a versatile framework, V-LoL-Trains provides a platform for investigating a wide range of visual logical learning ch
    
[^18]: ADI: 在垂直联邦学习系统中的对抗性主导输入

    ADI: Adversarial Dominating Inputs in Vertical Federated Learning Systems. (arXiv:2201.02775v3 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2201.02775](http://arxiv.org/abs/2201.02775)

    本文研究了垂直联邦学习系统中的对抗性主导输入（ADIs），并证明了其在典型VFL系统中的存在。该研究为防止ADIs的使用提供了方法。

    

    最近，垂直联邦学习（VFL）系统作为处理分散在许多个体来源中的数据的概念而变得突出，无需将其集中化。多个参与者以隐私意识的方式协作训练基于其本地数据的模型。到目前为止，VFL已成为在组织之间安全学习模型的事实解决方案，允许共享知识而不影响任何个人的隐私。尽管VFL系统的发展昌盛，但我们发现某些参与者的输入，称为对抗性主导输入（ADIs），可以支配共同推断朝着对手的意愿方向并迫使其他（受害者）参与者做出微不足道的贡献，失去通常在联邦学习场景中提供的对其贡献重要性的奖励。我们对ADIs进行了系统研究，首先证明了它们在典型的VFL系统中的存在。然后，我们提出了基于梯度的方法

    Vertical federated learning (VFL) system has recently become prominent as a concept to process data distributed across many individual sources without the need to centralize it. Multiple participants collaboratively train models based on their local data in a privacy-aware manner. To date, VFL has become a de facto solution to securely learn a model among organizations, allowing knowledge to be shared without compromising privacy of any individuals. Despite the prosperous development of VFL systems, we find that certain inputs of a participant, named adversarial dominating inputs (ADIs), can dominate the joint inference towards the direction of the adversary's will and force other (victim) participants to make negligible contributions, losing rewards that are usually offered regarding the importance of their contributions in federated learning scenarios. We conduct a systematic study on ADIs by first proving their existence in typical VFL systems. We then propose gradient-based methods
    

