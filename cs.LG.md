# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [A PPO-based DRL Auto-Tuning Nonlinear PID Drone Controller for Robust Autonomous Flights](https://arxiv.org/abs/2404.00204) | 该项目将非线性深度强化学习（DRL）代理引入无人机控制中，取代传统线性PID控制器，实现了无缝过渡、提高响应速度和稳定性，同时结合PPO策略训练DRL代理，并利用高精度跟踪系统提高自主飞行精度。 |
| [^2] | [Does Differentially Private Synthetic Data Lead to Synthetic Discoveries?](https://arxiv.org/abs/2403.13612) | 评估差分隐私合成生物医学数据上的Mann-Whitney U检验，以确定在隐私保护合成数据上执行的统计假设检验是否可能导致测试有效性的丧失或功率下降。 |
| [^3] | [TimeMachine: A Time Series is Worth 4 Mambas for Long-term Forecasting](https://arxiv.org/abs/2403.09898) | TimeMachine是一种创新模型，利用Mamba捕捉多变量时间序列数据中的长期依赖关系，同时保持线性可扩展性和较小的内存占用，通过全面验证，表现出优越的预测、可扩展性和内存效率。 |
| [^4] | [Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation](https://arxiv.org/abs/2402.14874) | 该研究提出了一种叫做蒸馏对比解码（DCD）的方法，通过结合对比提示与蒸馏技术，有效提升了大型语言模型（LLM）在推理任务上的性能表现，超过了传统的对比解码方法，并在多个基准数据集上取得了显著成果。 |
| [^5] | [Federated Complex Qeury Answering](https://arxiv.org/abs/2402.14609) | 研究了在多源知识图谱上回答复杂查询的联邦式方法，解决了知识图谱中的隐私保护和答案检索的挑战 |
| [^6] | [Comparing Graph Transformers via Positional Encodings](https://arxiv.org/abs/2402.14202) | 本文比较了使用绝对位置编码（APEs）和相对位置编码（RPEs）的图变换器，在最大化区分能力方面是等效的。 |
| [^7] | [Understanding the Natural Language of DNA using Encoder-Decoder Foundation Models with Byte-level Precision](https://arxiv.org/abs/2311.02333) | 本文提出了一种基于字节级精度的编码器-解码器模型，用于理解DNA的自然语言。该模型可以在字节级精度上分析DNA序列，使得能够用于识别DNA序列中的各种功能和变异。 |
| [^8] | [FLrce: Resource-Efficient Federated Learning with Early-Stopping Strategy](https://arxiv.org/abs/2310.09789) | FLrce方法通过引入提前停止策略和二进制修剪机制，实现了资源高效的联邦学习，解决了安全和资源利用不均衡问题。 |
| [^9] | [EdgeOL: Efficient in-situ Online Learning on Edge Devices.](http://arxiv.org/abs/2401.16694) | 本文提出了EdgeOL，一种边缘在线学习框架，通过内部和外部调优来优化推理准确性、微调执行时间和能量效率，在边缘设备上实现了显著的性能提升。 |
| [^10] | [Towards Enhancing the Reproducibility of Deep Learning Bugs: An Empirical Study.](http://arxiv.org/abs/2401.03069) | 本研究旨在提高深度学习Bug的可复现性，通过构建数据集和确定编辑动作和有用信息，这能够解决目前研究中忽视的问题。 |
| [^11] | [Model Merging by Uncertainty-Based Gradient Matching.](http://arxiv.org/abs/2310.12808) | 本论文通过不确定性梯度匹配的方法，提出了一种新的模型合并方案，该方案能够减少梯度不匹配，从而提高了模型合并的性能并对超参数更具鲁棒性。 |
| [^12] | [SDGym: Low-Code Reinforcement Learning Environments using System Dynamics Models.](http://arxiv.org/abs/2310.12494) | 这项研究提出了SDGym，一个基于系统动力学模型的低代码强化学习环境库，通过生成定制的RL环境来解决现实环境设计的困难，从而构建在实际设置中表现良好的强大智能体。 |
| [^13] | [Search-Adaptor: Text Embedding Customization for Information Retrieval.](http://arxiv.org/abs/2310.08750) | 本文提出了一种名为Search-Adaptor的方法，用于定制化预训练的大型语言模型以改善信息检索和搜索的性能。通过修改文本嵌入，Search-Adaptor在多个真实世界数据集上展现出了稳定且显著的性能提升。 |
| [^14] | [Quantum Shadow Gradient Descent for Quantum Learning.](http://arxiv.org/abs/2310.06935) | 本文提出了一种称为量子阴影梯度下降的新方法，解决了量子学习中的关键挑战，并通过测量量子阴影样本和模拟态的梯度来提高计算效率。 |
| [^15] | [SPICED: News Similarity Detection Dataset with Multiple Topics and Complexity Levels.](http://arxiv.org/abs/2309.13080) | 这个论文提出了一个名为SPICED的新闻相似性检测数据集，包括七个主题，并提供了四种不同的方法来生成新闻。 |
| [^16] | [A Note on Randomized Kaczmarz Algorithm for Solving Doubly-Noisy Linear Systems.](http://arxiv.org/abs/2308.16904) | 本文分析了当系数矩阵和向量都存在加性和乘性噪声时，随机Kaczmarz算法在解决噪声线性系统中的收敛性。分析表明，RK的收敛性受到𝜏的大小影响，其中𝜏表示带有噪声的系数矩阵A的乘子范数的平方与Frobenius范数的平方的乘积。 |
| [^17] | [Concept-based explainability for an EEG transformer model.](http://arxiv.org/abs/2307.12745) | 本研究尝试将基于概念激活向量（CAVs）的方法应用于脑电图（EEG）数据的解释性，通过定义解释性概念和选择相关数据集，以实现对大规模转换器模型中深度学习模型的理解。 |
| [^18] | [On the Vulnerability of Fairness Constrained Learning to Malicious Noise.](http://arxiv.org/abs/2307.11892) | 这项研究考虑了公正约束学习对恶意噪声的脆弱性，发现使用随机分类器可以在精度上只损失$\Theta(\alpha)$和$O(\sqrt{\alpha})$，对应不同的公正约束要求。 |
| [^19] | [Fast Adaptive Test-Time Defense with Robust Features.](http://arxiv.org/abs/2307.11672) | 本文提出了一种快速适应的测试防御策略，通过投影训练好的模型到最稳健的特征空间，降低了对抗攻击的脆弱性，无需额外的测试时间计算。 |
| [^20] | [Leveraging Task Structures for Improved Identifiability in Neural Network Representations.](http://arxiv.org/abs/2306.14861) | 本文提出一种基于任务结构的可识别性理论，拓展了先前仅限于单任务分类的工作。任务分布的存在定义了一个潜在变量的条件先验，将可识别性的等价类降低到排列和缩放。在假设任务之间存在因果关系时，该方法可以实现简单的最大边际似然优化，并在因果表示学习方面具有下游应用的可行性。 |
| [^21] | [Intelligent Energy Management with IoT Framework in Smart Cities Using Intelligent Analysis: An Application of Machine Learning Methods for Complex Networks and Systems.](http://arxiv.org/abs/2306.05567) | 本研究开发了一个智能城市能源管理的物联网框架，结合智能分析和多组件的架构，研究了基于智能机制的智能能源管理解决方案，以期节能和优化管理。 |
| [^22] | [DiffLoad: Uncertainty Quantification in Load Forecasting with Diffusion Model.](http://arxiv.org/abs/2306.01001) | 本文提出了一种扩散模型中的负荷预测不确定性量化方法，采用Seq2Seq网络结构来分离两种类型的不确定性并处理异常情况，不仅着眼于预测条件期望值。 |
| [^23] | [A Geometric Perspective on Diffusion Models.](http://arxiv.org/abs/2305.19947) | 本文研究了扩散模型的几何结构，发现通过一个明确的准线性采样轨迹和另一个隐式的去噪轨迹平滑连接了数据分布和噪声分布，建立了基于ODE的最优采样和经典的均值漂移算法之间的理论关系。 |
| [^24] | [Robust Implicit Regularization via Weight Normalization.](http://arxiv.org/abs/2305.05448) | 本文提出了使用权重规范化的梯度下降作为过度参数化模型的鲁棒隐式正则化方法，实现了对欧几里德范数较低的参数的隐式偏好，并建立了一个统一框架来解决线性模型和神经网络之间的隐式正则化隔阂。 |
| [^25] | [Large-scale Pre-trained Models are Surprisingly Strong in Incremental Novel Class Discovery.](http://arxiv.org/abs/2303.15975) | 本论文提出了一种更加挑战性和实用性的学习方法MSc-iNCD，通过在连续而无人监督的学习中利用大规模预训练模型的丰富先验知识，该方法在增量式新类别发现中表现出出乎意料的强大实力。 |

# 详细

[^1]: 基于PPO的DRL自调PID非线性无人机控制器用于稳健自主飞行

    A PPO-based DRL Auto-Tuning Nonlinear PID Drone Controller for Robust Autonomous Flights

    [https://arxiv.org/abs/2404.00204](https://arxiv.org/abs/2404.00204)

    该项目将非线性深度强化学习（DRL）代理引入无人机控制中，取代传统线性PID控制器，实现了无缝过渡、提高响应速度和稳定性，同时结合PPO策略训练DRL代理，并利用高精度跟踪系统提高自主飞行精度。

    

    该项目旨在通过将非线性深度强化学习（DRL）代理作为传统线性比例积分微分（PID）控制器的替代品，从而彻底改变无人机飞行控制。主要目标是在手动和自主模式之间实现无缝过渡，提高响应速度和稳定性。我们在Gazebo模拟器中利用近端策略优化（PPO）强化学习策略来训练DRL代理。添加20000美元的室内Vicon跟踪系统提供<1mm的定位精度，显着提高了自主飞行精度。为了在最短的无碰撞轨迹中导航无人机，我们还建立了一个三维A*路径规划器并成功地将其实施到实际飞行中。

    arXiv:2404.00204v1 Announce Type: cross  Abstract: This project aims to revolutionize drone flight control by implementing a nonlinear Deep Reinforcement Learning (DRL) agent as a replacement for traditional linear Proportional Integral Derivative (PID) controllers. The primary objective is to seamlessly transition drones between manual and autonomous modes, enhancing responsiveness and stability. We utilize the Proximal Policy Optimization (PPO) reinforcement learning strategy within the Gazebo simulator to train the DRL agent. Adding a $20,000 indoor Vicon tracking system offers <1mm positioning accuracy, which significantly improves autonomous flight precision. To navigate the drone in the shortest collision-free trajectory, we also build a 3 dimensional A* path planner and implement it into the real flight successfully.
    
[^2]: 差分隐私合成数据能导致合成发现吗？

    Does Differentially Private Synthetic Data Lead to Synthetic Discoveries?

    [https://arxiv.org/abs/2403.13612](https://arxiv.org/abs/2403.13612)

    评估差分隐私合成生物医学数据上的Mann-Whitney U检验，以确定在隐私保护合成数据上执行的统计假设检验是否可能导致测试有效性的丧失或功率下降。

    

    合成数据已被提出作为共享敏感生物医学数据的匿名化解决方案。理想情况下，合成数据应保留原始数据的结构和统计特性，同时保护个体主体的隐私。差分隐私（DP）目前被认为是平衡这种权衡的最佳方法。本研究的目的是评估在差分隐私生物医学数据上进行的Mann-Whitney U检验在I型和II型错误方面，以确定在隐私保护合成数据上执行的统计假设检验是否可能导致测试有效性的丧失或功率下降。

    arXiv:2403.13612v1 Announce Type: new  Abstract: Background: Synthetic data has been proposed as a solution for sharing anonymized versions of sensitive biomedical datasets. Ideally, synthetic data should preserve the structure and statistical properties of the original data, while protecting the privacy of the individual subjects. Differential privacy (DP) is currently considered the gold standard approach for balancing this trade-off.   Objectives: The aim of this study is to evaluate the Mann-Whitney U test on DP-synthetic biomedical data in terms of Type I and Type II errors, in order to establish whether statistical hypothesis testing performed on privacy preserving synthetic data is likely to lead to loss of test's validity or decreased power.   Methods: We evaluate the Mann-Whitney U test on DP-synthetic data generated from real-world data, including a prostate cancer dataset (n=500) and a cardiovascular dataset (n=70 000), as well as on data drawn from two Gaussian distribution
    
[^3]: TimeMachine: 一种用于长期预测的时间序列价值相当于4条眼镜蛇

    TimeMachine: A Time Series is Worth 4 Mambas for Long-term Forecasting

    [https://arxiv.org/abs/2403.09898](https://arxiv.org/abs/2403.09898)

    TimeMachine是一种创新模型，利用Mamba捕捉多变量时间序列数据中的长期依赖关系，同时保持线性可扩展性和较小的内存占用，通过全面验证，表现出优越的预测、可扩展性和内存效率。

    

    长期时间序列预测由于捕捉长期依赖关系、实现线性可扩展性和保持计算效率方面的困难而仍然具有挑战性。我们引入了TimeMachine，这是一种创新模型，利用Mamba，一种状态空间模型，来捕捉多变量时间序列数据中的长期依赖关系，同时保持线性可扩展性和较小的内存占用。TimeMachine利用时间序列数据的独特属性，在多个尺度上产生显著的上下文线索，并利用创新的整合四重Mamba架构来统一处理通道混合和通道独立情况，从而实现对全局和本地情境在不同尺度下进行有效内容选择进行预测。实验证明，TimeMachine在预测准确性、可扩展性和内存效率方面表现出优越性能，并且通过使用基准数据集进行广泛验证。

    arXiv:2403.09898v1 Announce Type: new  Abstract: Long-term time-series forecasting remains challenging due to the difficulty in capturing long-term dependencies, achieving linear scalability, and maintaining computational efficiency. We introduce TimeMachine, an innovative model that leverages Mamba, a state-space model, to capture long-term dependencies in multivariate time series data while maintaining linear scalability and small memory footprints. TimeMachine exploits the unique properties of time series data to produce salient contextual cues at multi-scales and leverage an innovative integrated quadruple-Mamba architecture to unify the handling of channel-mixing and channel-independence situations, thus enabling effective selection of contents for prediction against global and local contexts at different scales. Experimentally, TimeMachine achieves superior performance in prediction accuracy, scalability, and memory efficiency, as extensively validated using benchmark datasets. C
    
[^4]: 蒸馏对比解码：利用对比解码和蒸馏提升LLM的推理能力

    Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation

    [https://arxiv.org/abs/2402.14874](https://arxiv.org/abs/2402.14874)

    该研究提出了一种叫做蒸馏对比解码（DCD）的方法，通过结合对比提示与蒸馏技术，有效提升了大型语言模型（LLM）在推理任务上的性能表现，超过了传统的对比解码方法，并在多个基准数据集上取得了显著成果。

    

    我们提出了一种称为蒸馏对比解码（DCD）的简单方法，以增强大型语言模型（LLMs）在推理过程中的推理能力。与先前依赖于较小的业余模型或隐藏状态差异分析的方法不同，DCD采用了对比式思维引导和先进的蒸馏技术，包括Dropout和量化。这种方法有效地解决了对比解码（CD）的局限性，后者通常需要专家和业余模型，从而增加计算资源需求。通过将对比提示与蒸馏相结合，DCD消除了对业余模型的需求并减少了内存使用。我们的评估表明，DCD显著增强了LLM在各种推理基准测试中的性能，在GSM8K和StrategyQA数据集中均超过了CD和现有方法。

    arXiv:2402.14874v1 Announce Type: cross  Abstract: We propose a straightforward approach called Distillation Contrastive Decoding (DCD) to enhance the reasoning capabilities of Large Language Models (LLMs) during inference. In contrast to previous approaches that relied on smaller amateur models or analysis of hidden state differences, DCD employs Contrastive Chain-of-thought Prompting and advanced distillation techniques, including Dropout and Quantization. This approach effectively addresses the limitations of Contrastive Decoding (CD), which typically requires both an expert and an amateur model, thus increasing computational resource demands. By integrating contrastive prompts with distillation, DCD obviates the need for an amateur model and reduces memory usage. Our evaluations demonstrate that DCD significantly enhances LLM performance across a range of reasoning benchmarks, surpassing both CD and existing methods in the GSM8K and StrategyQA datasets.
    
[^5]: 联邦式复杂查询答案方法研究

    Federated Complex Qeury Answering

    [https://arxiv.org/abs/2402.14609](https://arxiv.org/abs/2402.14609)

    研究了在多源知识图谱上回答复杂查询的联邦式方法，解决了知识图谱中的隐私保护和答案检索的挑战

    

    知识图谱中的复杂逻辑查询答案是一个具有挑战性的任务，已经得到广泛研究。执行复杂逻辑推理的能力是必不可少的，并支持各种基于图推理的下游任务，比如搜索引擎。最近提出了一些方法，将知识图谱实体和逻辑查询表示为嵌入向量，并从知识图谱中找到逻辑查询的答案。然而，现有的方法主要集中在查询单个知识图谱上，并不能应用于多个图形。此外，直接共享带有敏感信息的知识图谱可能会带来隐私风险，使得共享和构建一个聚合知识图谱用于推理以检索查询答案是不切实际的。因此，目前仍然不清楚如何在多源知识图谱上回答查询。一个实体可能涉及到多个知识图谱，对多个知识图谱进行推理，并在多源知识图谱上回答复杂查询对于发现知识是重要的。

    arXiv:2402.14609v1 Announce Type: cross  Abstract: Complex logical query answering is a challenging task in knowledge graphs (KGs) that has been widely studied. The ability to perform complex logical reasoning is essential and supports various graph reasoning-based downstream tasks, such as search engines. Recent approaches are proposed to represent KG entities and logical queries into embedding vectors and find answers to logical queries from the KGs. However, existing proposed methods mainly focus on querying a single KG and cannot be applied to multiple graphs. In addition, directly sharing KGs with sensitive information may incur privacy risks, making it impractical to share and construct an aggregated KG for reasoning to retrieve query answers. Thus, it remains unknown how to answer queries on multi-source KGs. An entity can be involved in various knowledge graphs and reasoning on multiple KGs and answering complex queries on multi-source KGs is important in discovering knowledge 
    
[^6]: 通过位置编码比较图变换器

    Comparing Graph Transformers via Positional Encodings

    [https://arxiv.org/abs/2402.14202](https://arxiv.org/abs/2402.14202)

    本文比较了使用绝对位置编码（APEs）和相对位置编码（RPEs）的图变换器，在最大化区分能力方面是等效的。

    

    图变换器的区分能力与位置编码的选择紧密相关：用于增强基本变换器与图信息的特征。有两种主要类型的位置编码：绝对位置编码（APEs）和相对位置编码（RPEs）。APEs为每个节点分配特征，并作为变换器的输入。而RPEs则为每对节点（例如，图距离）分配一个特征，并用于增强注意力块。先验上，目前不清楚哪种方法更有利于最大化生成的图变换器的能力。本文旨在了解这两种不同类型位置编码之间的关系。有趣的是，我们展示了使用APEs和RPEs的图变换器在区分能力方面是等效的。特别地，我们展示了如何在保持其区分能力的同时交换APEs和RPEs。

    arXiv:2402.14202v1 Announce Type: new  Abstract: The distinguishing power of graph transformers is closely tied to the choice of positional encoding: features used to augment the base transformer with information about the graph. There are two primary types of positional encoding: absolute positional encodings (APEs) and relative positional encodings (RPEs). APEs assign features to each node and are given as input to the transformer. RPEs instead assign a feature to each pair of nodes, e.g., graph distance, and are used to augment the attention block. A priori, it is unclear which method is better for maximizing the power of the resulting graph transformer. In this paper, we aim to understand the relationship between these different types of positional encodings. Interestingly, we show that graph transformers using APEs and RPEs are equivalent in terms of distinguishing power. In particular, we demonstrate how to interchange APEs and RPEs while maintaining their distinguishing power in
    
[^7]: 使用字节级精确度的编码器-解码器基于Transformer模型理解DNA的自然语言

    Understanding the Natural Language of DNA using Encoder-Decoder Foundation Models with Byte-level Precision

    [https://arxiv.org/abs/2311.02333](https://arxiv.org/abs/2311.02333)

    本文提出了一种基于字节级精度的编码器-解码器模型，用于理解DNA的自然语言。该模型可以在字节级精度上分析DNA序列，使得能够用于识别DNA序列中的各种功能和变异。

    

    本文提出了一种基于字节级编码器-解码器Transformer架构的集合核苷酸字节级编码器-解码器(ENBED)基础模型，可以在字节级精度上分析DNA序列。ENBED使用次二次的注意力实现了一个高效的模型，能够进行序列到序列的转换，泛化先前基因组模型只采用编码器或者解码器体系结构的限制。我们使用遮蔽语言建模来预训练这个基础模型，使用参考基因组序列并将其应用到以下下游任务上：(1)识别增强子、启动子和剪切位点，(2)识别包含碱基调用不匹配和插入/缺失错误的序列，这是对多个碱基对进行标记化的方案的优势，丢失了字节级精度分析的能力，(3)识别基因组序列的生物功能注释，以及(4)生成突变基因组序列。

    arXiv:2311.02333v2 Announce Type: replace Abstract: This paper presents the Ensemble Nucleotide Byte-level Encoder-Decoder (ENBED) foundation model, analyzing DNA sequences at byte-level precision with an encoder-decoder Transformer architecture. ENBED uses a sub-quadratic implementation of attention to develop an efficient model capable of sequence-to-sequence transformations, generalizing previous genomic models with encoder-only or decoder-only architectures. We use Masked Language Modeling to pre-train the foundation model using reference genome sequences and apply it in the following downstream tasks: (1) identification of enhancers, promotors and splice sites, (2) recognition of sequences containing base call mismatches and insertion/deletion errors, an advantage over tokenization schemes involving multiple base pairs, which lose the ability to analyze with byte-level precision, (3) identification of biological function annotations of genomic sequences, and (4) generating mutatio
    
[^8]: FLrce: 具有提前停止策略的资源高效联邦学习

    FLrce: Resource-Efficient Federated Learning with Early-Stopping Strategy

    [https://arxiv.org/abs/2310.09789](https://arxiv.org/abs/2310.09789)

    FLrce方法通过引入提前停止策略和二进制修剪机制，实现了资源高效的联邦学习，解决了安全和资源利用不均衡问题。

    

    针对边缘设备资源短缺和不平衡的训练贡献，提出了FLrce方法，通过引入提前停止策略和二进制修剪机制，实现了资源高效的联邦学习，在不泄露本地数据的情况下，有效解决了传统联邦学习中存在的安全和资源利用不均衡问题。

    arXiv:2310.09789v2 Announce Type: replace  Abstract: Federated learning (FL) achieves great popularity in the Internet of Things (IoT) as a powerful interface to offer intelligent services to customers while maintaining data privacy. Under the orchestration of a server, edge devices (also called clients in FL) collaboratively train a global deep-learning model without sharing any local data. Nevertheless, the unequal training contributions among clients have made FL vulnerable, as clients with heavily biased datasets can easily compromise FL by sending malicious or heavily biased parameter updates. Furthermore, the resource shortage issue of edge devices also becomes a bottleneck. Due to overwhelming computation overheads generated by training deep-learning models on edge devices, and significant communication overheads for transmitting deep-learning models across the network, enormous amounts of resources are consumed in the FL process. This encompasses computation resources like ener
    
[^9]: EdgeOL: 边缘设备上高效的原位在线学习

    EdgeOL: Efficient in-situ Online Learning on Edge Devices. (arXiv:2401.16694v1 [cs.LG])

    [http://arxiv.org/abs/2401.16694](http://arxiv.org/abs/2401.16694)

    本文提出了EdgeOL，一种边缘在线学习框架，通过内部和外部调优来优化推理准确性、微调执行时间和能量效率，在边缘设备上实现了显著的性能提升。

    

    新兴应用，如机器人辅助养老和物体识别，通常采用深度学习神经网络模型，并且自然需要：i) 处理实时推理请求和ii) 适应可能的部署场景变化。在线模型微调被广泛采用以满足这些需求。然而，微调会导致显著的能量消耗，使其难以部署在边缘设备上。在本文中，我们提出了EdgeOL，一种边缘在线学习框架，通过内部和外部调优来优化推理准确性、微调执行时间和能量效率。实验结果显示，EdgeOL平均减少了82%的微调执行时间，74%的能量消耗，并提高了平均推理准确率1.70%，相对于即时在线学习策略。

    Emerging applications, such as robot-assisted eldercare and object recognition, generally employ deep learning neural networks (DNNs) models and naturally require: i) handling streaming-in inference requests and ii) adapting to possible deployment scenario changes. Online model fine-tuning is widely adopted to satisfy these needs. However, fine-tuning involves significant energy consumption, making it challenging to deploy on edge devices. In this paper, we propose EdgeOL, an edge online learning framework that optimizes inference accuracy, fine-tuning execution time, and energy efficiency through both inter-tuning and intra-tuning optimizations. Experimental results show that, on average, EdgeOL reduces overall fine-tuning execution time by 82%, energy consumption by 74%, and improves average inference accuracy by 1.70% over the immediate online learning strategy.
    
[^10]: 提高深度学习Bug可复现性的探索性研究

    Towards Enhancing the Reproducibility of Deep Learning Bugs: An Empirical Study. (arXiv:2401.03069v1 [cs.SE])

    [http://arxiv.org/abs/2401.03069](http://arxiv.org/abs/2401.03069)

    本研究旨在提高深度学习Bug的可复现性，通过构建数据集和确定编辑动作和有用信息，这能够解决目前研究中忽视的问题。

    

    背景：深度学习在各个领域取得了显著进展。然而，与传统软件系统一样，深度学习系统也存在Bug，这可能对自动驾驶等领域产生严重影响。尽管深度学习技术取得了重大进展，但很少有研究关注深度学习Bug的可复现性，这阻碍了Bug的解决。现有文献指出，仅有3%的深度学习Bug是可复现的，这凸显了进一步研究的必要性。目标：本文考察深度学习Bug的可复现性，识别可提高深度学习Bug可复现性的编辑动作和有用信息。方法：首先，构建了一个包含来自Stack Overflow和Defects4ML的3个框架和22个架构的668个深度学习Bug的数据集。其次，使用分层抽样选择了102个Bug，并尝试确定它们的可复现性。在复现这些Bug的过程中，我们识别了编辑动作和有用信息。

    Context: Deep learning has achieved remarkable progress in various domains. However, like traditional software systems, deep learning systems contain bugs, which can have severe impacts, as evidenced by crashes involving autonomous vehicles. Despite substantial advancements in deep learning techniques, little research has focused on reproducing deep learning bugs, which hinders resolving them. Existing literature suggests that only 3% of deep learning bugs are reproducible, underscoring the need for further research.  Objective: This paper examines the reproducibility of deep learning bugs. We identify edit actions and useful information that could improve deep learning bug reproducibility.  Method: First, we construct a dataset of 668 deep learning bugs from Stack Overflow and Defects4ML across 3 frameworks and 22 architectures. Second, we select 102 bugs using stratified sampling and try to determine their reproducibility. While reproducing these bugs, we identify edit actions and us
    
[^11]: 基于不确定性梯度匹配的模型合并

    Model Merging by Uncertainty-Based Gradient Matching. (arXiv:2310.12808v1 [cs.LG])

    [http://arxiv.org/abs/2310.12808](http://arxiv.org/abs/2310.12808)

    本论文通过不确定性梯度匹配的方法，提出了一种新的模型合并方案，该方案能够减少梯度不匹配，从而提高了模型合并的性能并对超参数更具鲁棒性。

    

    在不同数据集上训练的模型可以通过参数的加权平均来合并，但为什么会起作用，什么情况下会失败？在这里，我们将加权平均的不准确性与梯度不匹配联系起来，并提出了一种新的基于不确定性的方案，通过减少不匹配来提高性能。这种联系还揭示了其他方案（如平均值、任务算术和Fisher加权平均）中的隐含假设。我们的新方法在大型语言模型和视觉转换器方面都在性能和超参数鲁棒性方面得到了一致的改进。

    Models trained on different datasets can be merged by a weighted-averaging of their parameters, but why does it work and when can it fail? Here, we connect the inaccuracy of weighted-averaging to mismatches in the gradients and propose a new uncertainty-based scheme to improve the performance by reducing the mismatch. The connection also reveals implicit assumptions in other schemes such as averaging, task arithmetic, and Fisher-weighted averaging. Our new method gives consistent improvements for large language models and vision transformers, both in terms of performance and robustness to hyperparameters.
    
[^12]: SDGym: 使用系统动力学模型的低代码强化学习环境

    SDGym: Low-Code Reinforcement Learning Environments using System Dynamics Models. (arXiv:2310.12494v1 [cs.LG])

    [http://arxiv.org/abs/2310.12494](http://arxiv.org/abs/2310.12494)

    这项研究提出了SDGym，一个基于系统动力学模型的低代码强化学习环境库，通过生成定制的RL环境来解决现实环境设计的困难，从而构建在实际设置中表现良好的强大智能体。

    

    理解算法干预对社会的长期影响对于实现负责任的人工智能至关重要。传统的评估策略通常难以应对社会的复杂、适应性和动态性。虽然强化学习（RL）可以是优化动态环境下决策的强大方法，但现实环境设计的困难仍然是构建在实际环境中表现良好的强大智能体的障碍。为了解决这个问题，我们利用系统动力学（SD）领域作为一种补充方法，纳入协作仿真模型规范实践。我们介绍了SDGym，这是一个基于OpenAI Gym框架构建的低代码库，它可以根据SD模拟模型生成定制的RL环境。通过一项可行性研究，我们验证了可以从现有SD模型和少量配置代码中生成具有规范良好、丰富的RL环境。我们展示了SDGym的功能。

    Understanding the long-term impact of algorithmic interventions on society is vital to achieving responsible AI. Traditional evaluation strategies often fall short due to the complex, adaptive and dynamic nature of society. While reinforcement learning (RL) can be a powerful approach for optimizing decisions in dynamic settings, the difficulty of realistic environment design remains a barrier to building robust agents that perform well in practical settings. To address this issue we tap into the field of system dynamics (SD) as a complementary method that incorporates collaborative simulation model specification practices. We introduce SDGym, a low-code library built on the OpenAI Gym framework which enables the generation of custom RL environments based on SD simulation models. Through a feasibility study we validate that well specified, rich RL environments can be generated from preexisting SD models and a few lines of configuration code. We demonstrate the capabilities of the SDGym 
    
[^13]: Search-Adaptor: 用于信息检索的文本嵌入个性化定制

    Search-Adaptor: Text Embedding Customization for Information Retrieval. (arXiv:2310.08750v1 [cs.LG])

    [http://arxiv.org/abs/2310.08750](http://arxiv.org/abs/2310.08750)

    本文提出了一种名为Search-Adaptor的方法，用于定制化预训练的大型语言模型以改善信息检索和搜索的性能。通过修改文本嵌入，Search-Adaptor在多个真实世界数据集上展现出了稳定且显著的性能提升。

    

    由预训练的大型语言模型提取的文本嵌入具有显著的改善信息检索和搜索的潜力。除了一直以来常规使用的零样本设置外，利用相关查询-语料库配对数据的信息能力进一步提升了大型语言模型的能力。在本文中，我们提出了一种新的方法，名为Search-Adaptor，以便以高效且稳健的方式定制化预训练的大型语言模型进行信息检索。Search-Adaptor可以修改预训练的大型语言模型生成的原始文本嵌入，可以与任何大型语言模型进行集成，包括只能通过API访问的模型。在多个真实世界的英文和多语言检索数据集中，我们展示了Search-Adaptor的一致且显著的性能提升--例如，在13个BEIR数据集上，nDCG@10相对于Google Embedding APIs平均提高了5.2%以上。

    Text embeddings extracted by pre-trained Large Language Models (LLMs) have significant potential to improve information retrieval and search. Beyond the zero-shot setup in which they are being conventionally used, being able to take advantage of the information from the relevant query-corpus paired data has the power to further boost the LLM capabilities. In this paper, we propose a novel method, Search-Adaptor, for customizing LLMs for information retrieval in an efficient and robust way. Search-Adaptor modifies the original text embedding generated by pre-trained LLMs, and can be integrated with any LLM, including those only available via APIs. On multiple real-world English and multilingual retrieval datasets, we show consistent and significant performance benefits for Search-Adaptor -- e.g., more than 5.2% improvements over the Google Embedding APIs in nDCG@10 averaged over 13 BEIR datasets.
    
[^14]: 量子阴影梯度下降算法用于量子学习

    Quantum Shadow Gradient Descent for Quantum Learning. (arXiv:2310.06935v1 [quant-ph])

    [http://arxiv.org/abs/2310.06935](http://arxiv.org/abs/2310.06935)

    本文提出了一种称为量子阴影梯度下降的新方法，解决了量子学习中的关键挑战，并通过测量量子阴影样本和模拟态的梯度来提高计算效率。

    

    本文提出了一种称为量子阴影梯度下降（QSGD）的新方法，解决了关键挑战。我们的方法具有一次性操作的优点，不需要任何样本复制，同时具有与使用精确梯度计算的理想更新规则相当的收敛速度。我们提出了一种生成量子阴影样本（QSS）的新技术，生成量子阴影而不是现有工作中使用的经典阴影。通过测量量子阴影，我们解决了在经典计算机上执行计算时因维度呈指数增长而导致的限制问题。作为第二个主要贡献，我们研究了更一般的非乘积形式的模拟变分哈密顿量，形式为$\exp\{i\sum_j \theta_j A_j\}$。我们证明了梯度可以用易于测量的单参数模拟态的梯度来表示。

    This paper proposes a new procedure called quantum shadow gradient descent (QSGD) that addresses these key challenges. Our method has the benefits of a one-shot approach, in not requiring any sample duplication while having a convergence rate comparable to the ideal update rule using exact gradient computation. We propose a new technique for generating quantum shadow samples (QSS), which generates quantum shadows as opposed to classical shadows used in existing works. With classical shadows, the computations are typically performed on classical computers and, hence, are prohibitive since the dimension grows exponentially. Our approach resolves this issue by measurements of quantum shadows. As the second main contribution, we study more general non-product ansatz of the form $\exp\{i\sum_j \theta_j A_j\}$ that model variational Hamiltonians. We prove that the gradient can be written in terms of the gradient of single-parameter ansatzes that can be easily measured. Our proof is based on 
    
[^15]: SPICED: 具有多个主题和复杂程度的新闻相似性检测数据集

    SPICED: News Similarity Detection Dataset with Multiple Topics and Complexity Levels. (arXiv:2309.13080v1 [cs.CL])

    [http://arxiv.org/abs/2309.13080](http://arxiv.org/abs/2309.13080)

    这个论文提出了一个名为SPICED的新闻相似性检测数据集，包括七个主题，并提供了四种不同的方法来生成新闻。

    

    如今，使用智能系统来检测新闻文章中的冗余信息已经变得非常普遍，以增强用户体验，尤其是随着新闻媒体的蓬勃发展。然而，新闻的异质性可能导致这些系统中的虚假发现：简单的启发式算法，比如一对新闻是否都涉及政治问题，可以提供强大但具有误导性的下游性能。将新闻相似性数据集分割成主题可以通过强制模型学习如何在更狭窄的领域中区分显著特征来改进这些模型的训练。然而，这需要存在目前缺乏的专题特定数据集。在本文中，我们提出了一个新的相似新闻数据集SPICED，其中包括七个主题：犯罪与法律、文化与娱乐、灾难与事故、经济与商业、政治与冲突、科学与技术以及体育。此外，我们提供了四种不同的方法来生成新闻。

    Nowadays, the use of intelligent systems to detect redundant information in news articles has become especially prevalent with the proliferation of news media outlets in order to enhance user experience. However, the heterogeneous nature of news can lead to spurious findings in these systems: Simple heuristics such as whether a pair of news are both about politics can provide strong but deceptive downstream performance. Segmenting news similarity datasets into topics improves the training of these models by forcing them to learn how to distinguish salient characteristics under more narrow domains. However, this requires the existence of topic-specific datasets, which are currently lacking. In this article, we propose a new dataset of similar news, SPICED, which includes seven topics: Crime & Law, Culture & Entertainment, Disasters & Accidents, Economy & Business, Politics & Conflicts, Science & Technology, and Sports. Futhermore, we present four distinct approaches for generating news 
    
[^16]: 关于解决双向噪声线性系统的随机Kaczmarz算法的注释

    A Note on Randomized Kaczmarz Algorithm for Solving Doubly-Noisy Linear Systems. (arXiv:2308.16904v1 [math.NA])

    [http://arxiv.org/abs/2308.16904](http://arxiv.org/abs/2308.16904)

    本文分析了当系数矩阵和向量都存在加性和乘性噪声时，随机Kaczmarz算法在解决噪声线性系统中的收敛性。分析表明，RK的收敛性受到𝜏的大小影响，其中𝜏表示带有噪声的系数矩阵A的乘子范数的平方与Frobenius范数的平方的乘积。

    

    大规模线性系统Ax=b在实践中经常出现，需要有效的迭代求解器。通常，由于操作误差或错误的数据收集过程，这些系统会出现噪声。在过去的十年中，随机Kaczmarz（RK）算法已被广泛研究作为这些系统的高效迭代求解器。然而，现有对RK在噪声情况下的收敛性研究有限，只考虑右侧向量b中的测量噪声。不幸的是，在实践中，并不总是这样；系数矩阵A也可能是有噪声的。在本文中，我们分析了当系数矩阵A以及向量b都受有加性和乘性噪声影响时，RK的收敛性。在我们的分析中，变量 𝜏=∥ 𝜏 𝐴 ∗ ∥2^2 ∥ 𝐜𝐷𝐻∗𝑐 − 𝐛 ∥_𝐹^2🈶 𝑜 𝑅 的大小会影响RK的收敛性，其中 𝜏𝐴 表示A的带有噪声的版本。我们声称我们的分析是健壮且逼近实际的.

    Large-scale linear systems, $Ax=b$, frequently arise in practice and demand effective iterative solvers. Often, these systems are noisy due to operational errors or faulty data-collection processes. In the past decade, the randomized Kaczmarz (RK) algorithm has been studied extensively as an efficient iterative solver for such systems. However, the convergence study of RK in the noisy regime is limited and considers measurement noise in the right-hand side vector, $b$. Unfortunately, in practice, that is not always the case; the coefficient matrix $A$ can also be noisy. In this paper, we analyze the convergence of RK for noisy linear systems when the coefficient matrix, $A$, is corrupted with both additive and multiplicative noise, along with the noisy vector, $b$. In our analyses, the quantity $\tilde R=\| \tilde A^{\dagger} \|_2^2 \|\tilde A \|_F^2$ influences the convergence of RK, where $\tilde A$ represents a noisy version of $A$. We claim that our analysis is robust and realistic
    
[^17]: EEG转换器模型的基于概念的可解释性

    Concept-based explainability for an EEG transformer model. (arXiv:2307.12745v1 [cs.LG])

    [http://arxiv.org/abs/2307.12745](http://arxiv.org/abs/2307.12745)

    本研究尝试将基于概念激活向量（CAVs）的方法应用于脑电图（EEG）数据的解释性，通过定义解释性概念和选择相关数据集，以实现对大规模转换器模型中深度学习模型的理解。

    

    深度学习模型由于其规模、结构以及训练过程中的内在随机性而变得复杂。选择数据集和归纳偏见也增加了额外的复杂性。为了解释这些挑战，Kim等人（2018）引入了概念激活向量（CAVs），旨在从人类对齐的概念角度理解深度模型的内部状态。这些概念对应于潜在空间中的方向，使用线性判别法进行识别。尽管该方法首先应用于图像分类，但后来被适应到包括自然语言处理在内的其他领域。在本研究中，我们尝试将该方法应用于Kostas等人的BENDR（2021）的脑电图（EEG）数据，以实现可解释性。这项努力的关键部分包括定义解释性概念和选择相关数据集以将概念与潜在空间相对应。我们的重点是EEG概念形成的两个机制。

    Deep learning models are complex due to their size, structure, and inherent randomness in training procedures. Additional complexity arises from the selection of datasets and inductive biases. Addressing these challenges for explainability, Kim et al. (2018) introduced Concept Activation Vectors (CAVs), which aim to understand deep models' internal states in terms of human-aligned concepts. These concepts correspond to directions in latent space, identified using linear discriminants. Although this method was first applied to image classification, it was later adapted to other domains, including natural language processing. In this work, we attempt to apply the method to electroencephalogram (EEG) data for explainability in Kostas et al.'s BENDR (2021), a large-scale transformer model. A crucial part of this endeavor involves defining the explanatory concepts and selecting relevant datasets to ground concepts in the latent space. Our focus is on two mechanisms for EEG concept formation
    
[^18]: 关于受恶意噪声影响的公正约束学习的脆弱性

    On the Vulnerability of Fairness Constrained Learning to Malicious Noise. (arXiv:2307.11892v1 [cs.LG])

    [http://arxiv.org/abs/2307.11892](http://arxiv.org/abs/2307.11892)

    这项研究考虑了公正约束学习对恶意噪声的脆弱性，发现使用随机分类器可以在精度上只损失$\Theta(\alpha)$和$O(\sqrt{\alpha})$，对应不同的公正约束要求。

    

    我们考虑了公正约束学习对训练数据中微小恶意噪声的脆弱性。Konstantinov和Lampert (2021)在这个问题上进行了研究，并展示了负面结果，表明在不平衡的群组大小下存在一些数据分布，任何适当的学习器都会表现出较高的脆弱性。在这里，我们展示了更乐观的观点，如果允许随机分类器，则情况更加细致。例如，对于人口统计学平等性，我们显示只会产生$\Theta(\alpha)$的精度损失，其中$\alpha$是恶意噪声率，甚至可以与没有公正约束的情况完全匹配。对于机会均等性，我们显示只会产生$O(\sqrt{\alpha})$的损失，并给出一个匹配的$\Omega(\sqrt{\alpha})$的下界。相比之下，Konstantinov和Lampert (2021)示范了对于适当的学习器，这两个概念的精度损失都是$\Omega(1)$。关键的技术创新是

    We consider the vulnerability of fairness-constrained learning to small amounts of malicious noise in the training data. Konstantinov and Lampert (2021) initiated the study of this question and presented negative results showing there exist data distributions where for several fairness constraints, any proper learner will exhibit high vulnerability when group sizes are imbalanced. Here, we present a more optimistic view, showing that if we allow randomized classifiers, then the landscape is much more nuanced. For example, for Demographic Parity we show we can incur only a $\Theta(\alpha)$ loss in accuracy, where $\alpha$ is the malicious noise rate, matching the best possible even without fairness constraints. For Equal Opportunity, we show we can incur an $O(\sqrt{\alpha})$ loss, and give a matching $\Omega(\sqrt{\alpha})$lower bound. In contrast, Konstantinov and Lampert (2021) showed for proper learners the loss in accuracy for both notions is $\Omega(1)$. The key technical novelty 
    
[^19]: 快速自适应测试防御与稳健特征

    Fast Adaptive Test-Time Defense with Robust Features. (arXiv:2307.11672v1 [cs.LG])

    [http://arxiv.org/abs/2307.11672](http://arxiv.org/abs/2307.11672)

    本文提出了一种快速适应的测试防御策略，通过投影训练好的模型到最稳健的特征空间，降低了对抗攻击的脆弱性，无需额外的测试时间计算。

    

    自适应的测试防御被用来提高深度神经网络对抗性样本的鲁棒性。然而，现有方法由于对模型参数或输入进行额外的优化导致推理时间大幅增加。在本工作中，我们提出了一种新颖的自适应测试防御策略，它可以与任何现有（稳健的）训练过程轻松集成，并且无需额外的测试时间计算。基于我们提出的特征鲁棒性的概念，关键思想是将训练好的模型投影到最稳健的特征空间，从而降低对非稳健方向的对抗攻击的脆弱性。我们在广义可加性模型和使用神经切向核函数（NTK）等价法证明了特征矩阵的顶层特征空间更加稳健。我们在CIFAR-10和CIFAR-100数据集上进行了大量实验，用于几个稳健性基准测试。

    Adaptive test-time defenses are used to improve the robustness of deep neural networks to adversarial examples. However, existing methods significantly increase the inference time due to additional optimization on the model parameters or the input at test time. In this work, we propose a novel adaptive test-time defense strategy that is easy to integrate with any existing (robust) training procedure without additional test-time computation. Based on the notion of robustness of features that we present, the key idea is to project the trained models to the most robust feature space, thereby reducing the vulnerability to adversarial attacks in non-robust directions. We theoretically show that the top eigenspace of the feature matrix are more robust for a generalized additive model and support our argument for a large width neural network with the Neural Tangent Kernel (NTK) equivalence. We conduct extensive experiments on CIFAR-10 and CIFAR-100 datasets for several robustness benchmarks, 
    
[^20]: 利用任务结构提高神经网络表示的可识别性

    Leveraging Task Structures for Improved Identifiability in Neural Network Representations. (arXiv:2306.14861v1 [stat.ML])

    [http://arxiv.org/abs/2306.14861](http://arxiv.org/abs/2306.14861)

    本文提出一种基于任务结构的可识别性理论，拓展了先前仅限于单任务分类的工作。任务分布的存在定义了一个潜在变量的条件先验，将可识别性的等价类降低到排列和缩放。在假设任务之间存在因果关系时，该方法可以实现简单的最大边际似然优化，并在因果表示学习方面具有下游应用的可行性。

    

    本文扩展了监督学习中可辨别性的理论，考虑了在拥有任务分布的情况下的后果。在这种情况下，我们展示了即使在回归的情况下也可以实现可识别性，扩展了先前仅限于单任务分类情况的工作。此外，我们展示了任务分布的存在定义了一个潜在变量的条件先验，将可识别性的等价类降低到排列和缩放，这是一个更强大和更有用的结果。当我们进一步假设这些任务之间存在因果关系时，我们的方法可以实现简单的最大边际似然优化，并在因果表示学习方面具有下游应用的可行性。在经验上，我们验证了我们的模型在恢复合成和现实世界数据的规范表示方面优于更一般的无监督模型。

    This work extends the theory of identifiability in supervised learning by considering the consequences of having access to a distribution of tasks. In such cases, we show that identifiability is achievable even in the case of regression, extending prior work restricted to the single-task classification case. Furthermore, we show that the existence of a task distribution which defines a conditional prior over latent variables reduces the equivalence class for identifiability to permutations and scaling, a much stronger and more useful result. When we further assume a causal structure over these tasks, our approach enables simple maximum marginal likelihood optimization together with downstream applicability to causal representation learning. Empirically, we validate that our model outperforms more general unsupervised models in recovering canonical representations for synthetic and real-world data.
    
[^21]: 智能分析，在物联网框架下的智能城市能源管理：复杂网络和系统机器学习方法应用的案例研究

    Intelligent Energy Management with IoT Framework in Smart Cities Using Intelligent Analysis: An Application of Machine Learning Methods for Complex Networks and Systems. (arXiv:2306.05567v1 [cs.LG])

    [http://arxiv.org/abs/2306.05567](http://arxiv.org/abs/2306.05567)

    本研究开发了一个智能城市能源管理的物联网框架，结合智能分析和多组件的架构，研究了基于智能机制的智能能源管理解决方案，以期节能和优化管理。

    

    智能建筑越来越多地使用基于物联网的无线传感系统来降低能源消耗和环境影响。本研究的主要贡献是开发了一个全面的基于物联网的智能城市能源管理框架，融合了多个物联网架构和框架的组件。该框架通过智能分析，不仅收集和存储信息，而且还是其他企业开发应用的平台。此外，我们还研究了基于智能机制的智能能源管理解决方案。能源资源的消耗和需求增加导致了节能与优化管理的需求和挑战。

    Smart buildings are increasingly using Internet of Things (IoT)-based wireless sensing systems to reduce their energy consumption and environmental impact. As a result of their compact size and ability to sense, measure, and compute all electrical properties, Internet of Things devices have become increasingly important in our society. A major contribution of this study is the development of a comprehensive IoT-based framework for smart city energy management, incorporating multiple components of IoT architecture and framework. An IoT framework for intelligent energy management applications that employ intelligent analysis is an essential system component that collects and stores information. Additionally, it serves as a platform for the development of applications by other companies. Furthermore, we have studied intelligent energy management solutions based on intelligent mechanisms. The depletion of energy resources and the increase in energy demand have led to an increase in energy 
    
[^22]: DiffLoad:扩散模型中的负荷预测不确定性量化

    DiffLoad: Uncertainty Quantification in Load Forecasting with Diffusion Model. (arXiv:2306.01001v1 [cs.LG])

    [http://arxiv.org/abs/2306.01001](http://arxiv.org/abs/2306.01001)

    本文提出了一种扩散模型中的负荷预测不确定性量化方法，采用Seq2Seq网络结构来分离两种类型的不确定性并处理异常情况，不仅着眼于预测条件期望值。

    

    电力负荷预测对电力系统的决策制定，如机组投入和能源管理等具有重要意义。近年来，各种基于自监督神经网络的方法已经被应用于电力负荷预测，以提高预测准确性和捕捉不确定性。然而，大多数现有的方法是基于高斯似然方法的，它旨在在给定的协变量下准确估计分布期望值。这种方法很难适应存在分布偏移和异常值的时间数据。在本文中，我们提出了一种基于扩散的Seq2seq结构来估计本体不确定性，并使用鲁棒的加性柯西分布来估计物象不确定性。我们展示了我们的方法能够分离两种类型的不确定性并处理突变情况，而不是准确预测条件期望。

    Electrical load forecasting is of great significance for the decision makings in power systems, such as unit commitment and energy management. In recent years, various self-supervised neural network-based methods have been applied to electrical load forecasting to improve forecasting accuracy and capture uncertainties. However, most current methods are based on Gaussian likelihood methods, which aim to accurately estimate the distribution expectation under a given covariate. This kind of approach is difficult to adapt to situations where temporal data has a distribution shift and outliers. In this paper, we propose a diffusion-based Seq2seq structure to estimate epistemic uncertainty and use the robust additive Cauchy distribution to estimate aleatoric uncertainty. Rather than accurately forecasting conditional expectations, we demonstrate our method's ability in separating two types of uncertainties and dealing with the mutant scenarios.
    
[^23]: 扩散模型的几何视角

    A Geometric Perspective on Diffusion Models. (arXiv:2305.19947v1 [cs.CV])

    [http://arxiv.org/abs/2305.19947](http://arxiv.org/abs/2305.19947)

    本文研究了扩散模型的几何结构，发现通过一个明确的准线性采样轨迹和另一个隐式的去噪轨迹平滑连接了数据分布和噪声分布，建立了基于ODE的最优采样和经典的均值漂移算法之间的理论关系。

    

    近年来，针对扩散模型的高效训练和快速采样方法取得了显著进展。最近的一个重要进展是使用随机微分方程（SDE）来描述数据扰动和生成建模，以实现统一的数学框架。本文揭示了扩散模型的几个有趣的几何结构，并为其采样动力学提供了简单而强大的解释。通过仔细检查一种流行的方差爆炸SDE及其保持边际的普通微分方程（ODE）用于采样，我们发现数据分布和噪声分布通过一个明确的准线性采样轨迹和另一个隐式的去噪轨迹平滑连接，即使在视觉质量方面也收敛更快。我们还建立起基于ODE的最优采样和经典的均值漂移（寻找模式）算法之间的理论关系。

    Recent years have witnessed significant progress in developing efficient training and fast sampling approaches for diffusion models. A recent remarkable advancement is the use of stochastic differential equations (SDEs) to describe data perturbation and generative modeling in a unified mathematical framework. In this paper, we reveal several intriguing geometric structures of diffusion models and contribute a simple yet powerful interpretation to their sampling dynamics. Through carefully inspecting a popular variance-exploding SDE and its marginal-preserving ordinary differential equation (ODE) for sampling, we discover that the data distribution and the noise distribution are smoothly connected with an explicit, quasi-linear sampling trajectory, and another implicit denoising trajectory, which even converges faster in terms of visual quality. We also establish a theoretical relationship between the optimal ODE-based sampling and the classic mean-shift (mode-seeking) algorithm, with w
    
[^24]: 借助权重规范化的鲁棒性隐式正则化

    Robust Implicit Regularization via Weight Normalization. (arXiv:2305.05448v1 [cs.LG])

    [http://arxiv.org/abs/2305.05448](http://arxiv.org/abs/2305.05448)

    本文提出了使用权重规范化的梯度下降作为过度参数化模型的鲁棒隐式正则化方法，实现了对欧几里德范数较低的参数的隐式偏好，并建立了一个统一框架来解决线性模型和神经网络之间的隐式正则化隔阂。

    

    过度参数化的模型可能有许多插值解; 隐式正则化是指特定优化方法对众多插值解之一的隐含喜好。已经建立的工作表明，（随机）梯度下降在用于训练深度线性网络时倾向于具有低秩和/或稀疏解的隐式偏差，从某种程度上解释了为什么通过梯度下降训练的过度参数化神经网络模型在实践中具有良好的泛化性能。然而，现有的平方损失目标理论通常需要可训练权重的非常小的初始化，这与实践中为了更快的收敛和更好的泛化性能而初始化的更大规模的权重矛盾。在本文中，我们旨在通过纳入并分析采用权重规范化的梯度下降来弥合这一差距，其中权重向量以极坐标参数化，导致自然的权重归一化。我们表明，在过度参数化的线性回归模型的设置中，采用权重规范化的梯度下降对欧几里德范数较低的权重向量具有隐式正则化作用。此外，我们建立了一个新颖的统一框架，将权重规范化的隐式偏差与非线性模型的经验范数正则化联系起来，从而弥合了线性模型和神经网络之间的差距。

    Overparameterized models may have many interpolating solutions; implicit regularization refers to the hidden preference of a particular optimization method towards a certain interpolating solution among the many. A by now established line of work has shown that (stochastic) gradient descent tends to have an implicit bias towards low rank and/or sparse solutions when used to train deep linear networks, explaining to some extent why overparameterized neural network models trained by gradient descent tend to have good generalization performance in practice. However, existing theory for square-loss objectives often requires very small initialization of the trainable weights, which is at odds with the larger scale at which weights are initialized in practice for faster convergence and better generalization performance. In this paper, we aim to close this gap by incorporating and analyzing gradient descent with weight normalization, where the weight vector is reparamterized in terms of polar
    
[^25]: 大规模预训练模型在增量式新类别发现中具有出乎意料的强大表现。

    Large-scale Pre-trained Models are Surprisingly Strong in Incremental Novel Class Discovery. (arXiv:2303.15975v1 [cs.CV])

    [http://arxiv.org/abs/2303.15975](http://arxiv.org/abs/2303.15975)

    本论文提出了一种更加挑战性和实用性的学习方法MSc-iNCD，通过在连续而无人监督的学习中利用大规模预训练模型的丰富先验知识，该方法在增量式新类别发现中表现出出乎意料的强大实力。

    

    在生命长学习者中，从未标记的数据中连续地发现新概念是一个重要的期望。在文献中，这类问题在非常受限的情况下得到了部分解决，其中要么为发现新概念提供有标号的数据（例如 NCD），要么学习在有限数量的增量步骤中发生（例如类 iNCD）。在这项工作中，我们挑战现状，提出了一种更具挑战性和实用性的学习范式，称为 MSc-iNCD，其中学习连续而无人监督，并利用大规模预训练模型的丰富先验知识。为此，我们提出了简单的基线，不仅在较长的学习情境下具有弹性，而且与复杂的最先进方法相比，表现出出乎意料的强大实力。我们在多个基准测试中进行了广泛的实证评估，并展示了我们提出的基线的有效性，大大提升了基准要求。

    Discovering novel concepts from unlabelled data and in a continuous manner is an important desideratum of lifelong learners. In the literature such problems have been partially addressed under very restricted settings, where either access to labelled data is provided for discovering novel concepts (e.g., NCD) or learning occurs for a limited number of incremental steps (e.g., class-iNCD). In this work we challenge the status quo and propose a more challenging and practical learning paradigm called MSc-iNCD, where learning occurs continuously and unsupervisedly, while exploiting the rich priors from large-scale pre-trained models. To this end, we propose simple baselines that are not only resilient under longer learning scenarios, but are surprisingly strong when compared with sophisticated state-of-the-art methods. We conduct extensive empirical evaluation on a multitude of benchmarks and show the effectiveness of our proposed baselines, which significantly raises the bar.
    

