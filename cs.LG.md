# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Interpretable Multi-View Clustering Based on Anchor Graph Tensor Factorization](https://arxiv.org/abs/2404.00883) | 使用非负张量因子分解解决了基于锚图的多视图聚类方法缺乏聚类可解释性和忽视视图间信息的问题 |
| [^2] | [Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods](https://arxiv.org/abs/2404.00282) | 大型语言模型在强化学习中具有潜在优势，通过结构化分类和角色分析，为未来研究提供指导。 |
| [^3] | [Nature-Guided Cognitive Evolution for Predicting Dissolved Oxygen Concentrations in North Temperate Lakes](https://arxiv.org/abs/2403.18923) | 提出了一种自然引导的认知进化策略，通过多层融合自适应学习和自然过程，有效预测北温带湖泊中的溶解氧浓度 |
| [^4] | [Cluster-Based Normalization Layer for Neural Networks](https://arxiv.org/abs/2403.16798) | 该论文提出了一种基于聚类的神经网络规范化方法CB-Norm，通过引入高斯混合模型，解决了梯度稳定性和学习加速方面的挑战。 |
| [^5] | [Robustness of the data-driven approach in limited angle tomography](https://arxiv.org/abs/2403.11350) | 数据驱动方法在有限角度层析成像中相较于传统方法能更稳定地重构更多信息 |
| [^6] | [Differentially Private Representation Learning via Image Captioning](https://arxiv.org/abs/2403.02506) | 通过图像字幕生成实现了有效的差分隐私表示学习，获得了高质量图像特征，可用于各种视觉和视觉语言任务。 |
| [^7] | [Active Learning of Mealy Machines with Timers](https://arxiv.org/abs/2403.02019) | 这篇论文提出了一种用于查询学习具有定时器的Mealy机器的算法，在实现上明显比已有算法更有效率。 |
| [^8] | [Derivative-enhanced Deep Operator Network](https://arxiv.org/abs/2402.19242) | DE-DeepONet通过整合导数信息提高了预测准确性，尤其在训练数据有限的情况下，相比传统DeepONet取得了更好的效果。 |
| [^9] | [Zeroth-Order Sampling Methods for Non-Log-Concave Distributions: Alleviating Metastability by Denoising Diffusion](https://arxiv.org/abs/2402.17886) | 本文提出了一种基于去噪扩散过程的零阶扩散蒙特卡洛算法，克服了非对数凹分布采样中的亚稳定性问题，并证明其采样精度具有倒多项式依赖。 |
| [^10] | [Anchor-free Clustering based on Anchor Graph Factorization](https://arxiv.org/abs/2402.15688) | 无锚聚类方法AFCAGF通过学习锚图并优化成对样本距离，避免了锚点选择和初始化的需要，提升了聚类算法性能。 |
| [^11] | [NeuralThink: Algorithm Synthesis that Extrapolates in General Tasks](https://arxiv.org/abs/2402.15393) | NeuralThink 是一种新的递归架构，可以一贯地对对称和不对称任务进行外推，相较于之前的最先进的深度思维架构在稳定地从较小的训练规模对大观测进行外推方面表现出更好的性能。 |
| [^12] | [Edge Caching Based on Deep Reinforcement Learning and Transfer Learning](https://arxiv.org/abs/2402.14576) | 本文提出了一种基于双深度Q学习的缓存方法，通过半马尔可夫决策过程（SMDP）适应现实场景中随机请求到达的特性，综合考虑各种文件特征。 |
| [^13] | [Linear Transformers are Versatile In-Context Learners](https://arxiv.org/abs/2402.14180) | 线性变换器展示了在处理复杂问题和噪音干扰数据时的多功能性，通过发现一种新颖的优化算法，超越了许多合理的基线。 |
| [^14] | [Learnability is a Compact Property](https://arxiv.org/abs/2402.10360) | 监督学习问题的困难性具有紧凑的有限特性表征。 |
| [^15] | [Genetic-guided GFlowNets: Advancing in Practical Molecular Optimization Benchmark](https://arxiv.org/abs/2402.05961) | 本文提出了一种名为基因引导GFlowNet (Genetic GFN) 的新方法，通过集成迭代遗传搜索和训练策略，该方法在实际分子优化基准测试中取得了16.213的最新得分，明显优于现有最佳得分15.185，同时在14个任务中超越了所有对比方法。 |
| [^16] | [Tradeoffs of Diagonal Fisher Information Matrix Estimators](https://arxiv.org/abs/2402.05379) | 本研究探讨了使用对角费舍尔信息矩阵估计器的权衡。通过分析和数值研究，发现方差量取决于非线性与不同参数组之间的关系，应该在估计费舍尔信息时予以重视。 |
| [^17] | [Noise Contrastive Alignment of Language Models with Explicit Rewards](https://arxiv.org/abs/2402.05369) | 本文提出了一个基于噪声对比估计的通用LM对齐框架，能够处理明确注释的奖励数据，并且扩展了当前的对齐理论。 |
| [^18] | [Learning with Diversification from Block Sparse Signal](https://arxiv.org/abs/2402.04646) | 本文提出了一种新的先验，称为多样化块稀疏先验，用来描述真实世界数据中的广泛块稀疏现象。通过允许方差和相关矩阵的多样化，解决了现有块稀疏学习方法对预定义块信息的敏感性问题，并提出了一种多样化的块稀疏贝叶斯学习方法(DivSBL)，实现自适应的块估计，减轻过拟合的风险，并建立了全局和局部最优性理论。实验结果证明了DivSBL相对于现有算法的优势。 |
| [^19] | [Online Feature Updates Improve Online (Generalized) Label Shift Adaptation](https://arxiv.org/abs/2402.03545) | 本文提出了一种名为OLS-OFU的新方法，通过在测试时利用无标签数据进行自监督学习和特征优化，以解决在线设置中的标签转移问题。研究结果表明，OLS-OFU在各种数据集和领域转移条件下都表现出了有效性和鲁棒性。 |
| [^20] | [Learning Structure-Aware Representations of Dependent Types](https://arxiv.org/abs/2402.02104) | 本文扩展了Agda生态系统到机器学习领域，并发布了一种新颖的依赖类型编程语言的证明数据集。通过提出一种基于结构而非命名原则的新颖神经架构，我们能够准确地表示依赖类型程序，并在前提选择任务中取得了强大的初步结果。 |
| [^21] | [Learning General Parameterized Policies for Infinite Horizon Average Reward Constrained MDPs via Primal-Dual Policy Gradient Algorithm](https://arxiv.org/abs/2402.02042) | 该论文研究了无限时域平均回报受限MDPs的参数化通用策略，并提出了一种基于原始-对偶策略梯度算法，可在保证低遗憾的情况下管理约束条件，达到全局最优策略。算法的分析表明，其目标遗憾和约束违反均为 $\tilde{\mathcal{O}}({T}^{3/4})$。 |
| [^22] | [Misspecification uncertainties in near-deterministic regression](https://arxiv.org/abs/2402.01810) | 该论文研究了近确定性回归中错误规范化的不确定性问题，并提出了一种组合模型，以准确预测和控制参数不确定性。 |
| [^23] | [Score-based Causal Representation Learning: Linear and General Transformations](https://arxiv.org/abs/2402.00849) | 这篇论文提出了一种基于得分的算法类，用于干预范围内的因果表示学习，涵盖了线性和一般转化。算法保证了可识别性和实现性，并且通过创造性地将得分函数与因果表示学习相结合。 |
| [^24] | [Distinguishing the Indistinguishable: Human Expertise in Algorithmic Prediction](https://arxiv.org/abs/2402.00793) | 本论文介绍了一种新的框架，将人类专业知识纳入算法预测中，重点在于利用人的判断力区分对于任何可行的预测算法来说“看起来相同”的输入。 |
| [^25] | [Comparing Template-based and Template-free Language Model Probing](https://arxiv.org/abs/2402.00123) | 本文比较了基于模板和非模板语言模型的探测方法，发现它们在模型排名、绝对得分和与困惑度的关系等方面存在差异。 |
| [^26] | [Unlocking the Power of Multi-institutional Data: Integrating and Harmonizing Genomic Data Across Institutions](https://arxiv.org/abs/2402.00077) | 该研究介绍了一种名为Bridge的模型，致力于解决利用多机构测序数据时面临的挑战，包括基因组板块的变化、测序技术的差异以及数据的高维度和稀疏性等。 |
| [^27] | [Fair Coresets via Optimal Transport](https://arxiv.org/abs/2311.05436) | 本研究提出了公平的Wasserstein核心集(FWC)，该方法通过最小化原始数据集与加权合成样本之间的Wasserstein距离，并强制实现人口平等，生成公平的合成代表性样本，可用于下游学习任务。 |
| [^28] | [Fair Ranking under Disparate Uncertainty](https://arxiv.org/abs/2309.01610) | 提出了一种新的公平排名标准Equal-Opportunity Ranking（EOR），将底层相关性模型的不确定性差异考虑在内，通过组内公平抽奖实现公平排名。 |
| [^29] | [ChatQA: Building GPT-4 Level Conversational QA Models.](http://arxiv.org/abs/2401.10225) | ChatQA是一系列对话问答模型，可以达到GPT-4级别的准确性。通过两阶段的指令调整方法，可以显著提高大型语言模型在零-shot对话问答中的结果。使用密集检索器进行问答数据集的微调可以实现与最先进的查询重写模型相当的结果，同时降低部署成本。ChatQA-70B在10个对话问答数据集上的平均得分超过了GPT-4，且不依赖于任何来自OpenAI GPT模型的合成数据。 |
| [^30] | [A Survey Analyzing Generalization in Deep Reinforcement Learning.](http://arxiv.org/abs/2401.02349) | 本文调查了深度强化学习中的泛化性能。深度强化学习策略存在过拟合问题，限制了它们的鲁棒性和泛化能力。研究形式化和统一了提高泛化性和克服过拟合的不同解决方案。 |
| [^31] | [PAC-Bayes-Chernoff bounds for unbounded losses.](http://arxiv.org/abs/2401.01148) | 这篇论文提出了一种用于无界损失的高概率PAC-Bayes参考界限，并通过优化自由参数解决了一些开放问题，并通过灵活的假设产生了新的广义界限。 |
| [^32] | [Masked Hard-Attention Transformers and Boolean RASP Recognize Exactly the Star-Free Languages.](http://arxiv.org/abs/2310.13897) | 给出了一种新的变换器编码器模型，该模型具有硬注意力和严格未来掩码，并且证明这些网络识别的语言类别正是无星语言。研究还发现，通过添加位置嵌入，这一模型可以扩展到其他研究充分的语言类别。一个关键技术是布尔RASP，通过无星语言的研究，将变换器与一阶逻辑、时态逻辑和代数自动机理论相关联。 |
| [^33] | [When, Why and How Much? Adaptive Learning Rate Scheduling by Refinement.](http://arxiv.org/abs/2310.07831) | 该论文介绍了一种通过细化分析学习率调度来解决实践中学习率调整与理论的不一致的方法，通过对观察到的梯度范数进行分析，得到了适应于特定任务的细化调度。该方法能够改善优化算法的收敛性能。 |
| [^34] | [IMITATE: Clinical Prior Guided Hierarchical Vision-Language Pre-training.](http://arxiv.org/abs/2310.07355) | IMITATE是一种临床先验指导的分层视觉语言预训练模型。它利用医学报告的层级结构，从胸部X射线图像中提取多级视觉特征，并与分层医学报告中的描述性和结论性文本进行对齐。 |
| [^35] | [Conservative World Models.](http://arxiv.org/abs/2309.15178) | 在零样本强化学习中，研究人员探索了在小样本数据集上训练时，前向-后向算法性能下降的问题，并使用保守性算法来缓解此问题。实验证明，保守的前向-后向算法在总体上表现更好，甚至超过了特定任务的基准算法。 |
| [^36] | [Solving Quadratic Systems with Full-Rank Matrices Using Sparse or Generative Priors.](http://arxiv.org/abs/2309.09032) | 本论文提出了一个方法，通过使用稀疏或生成的先验知识，解决了从全秩矩阵的二次系统中恢复信号的问题。其中，通过引入阈值Wirtinger流算法（TWF）来处理稀疏信号，并使用谱初始化和阈值梯度下降方法，在高维情况下实现了较小的测量数量。 |
| [^37] | [Federated Large Language Model: A Position Paper.](http://arxiv.org/abs/2307.08925) | 我们提出了联邦式大规模语言模型的概念，通过联邦学习实现分散数据的共同训练共享模型，以应对公共数据可用性的限制和私有数据的隐私保护需求。我们讨论了预训练、微调和提示工程这三个组件的优势，并提出了实施策略。同时，我们探讨了FL和LLM集成带来的新挑战，并分析了现有解决方案和潜在障碍。 |
| [^38] | [Set-based Neural Network Encoding.](http://arxiv.org/abs/2305.16625) | 提出了一种能够集合化地编码神经网络参数的神经网络权重编码方法，并引入了一种逐层编码方案来考虑神经网络的分层计算结构。同时引入了“pad-chunk-encode”流水线进行神经网络层的高效编码处理，还提出了新的神经网络泛化性能预测任务。 |
| [^39] | [Imprecise Label Learning: A Unified Framework for Learning with Various Imprecise Label Configurations.](http://arxiv.org/abs/2305.12715) | 本文提出了不精确标签学习（ILL）框架，利用期望最大化算法对不精确标签信息进行最大似然估计，为各种不精确标签配置问题提供了统一的解决方案。 |
| [^40] | [Efficient distributed representations beyond negative sampling.](http://arxiv.org/abs/2303.17475) | 本文介绍了一种高效的分布式表示（嵌入）学习方法，通过线性时间估计softmax归一化常数来实现学习过程，该方法优于负采样方法并在多项测试中验证了其有效性。 |

# 详细

[^1]: 基于锚图张量分解的可解释多视图聚类

    Interpretable Multi-View Clustering Based on Anchor Graph Tensor Factorization

    [https://arxiv.org/abs/2404.00883](https://arxiv.org/abs/2404.00883)

    使用非负张量因子分解解决了基于锚图的多视图聚类方法缺乏聚类可解释性和忽视视图间信息的问题

    

    基于锚图的聚类方法因其出色的聚类性能和处理大规模数据的能力而备受关注。一种常见的方法是学习具有K个连接组件的二部图，有助于避免后处理的需要。然而，现有基于锚图因子化的多视图聚类方法缺乏对分解矩阵的充分聚类可解释性，并经常忽视视图间信息。我们通过使用非负张量因子分解来解决这一限制，以分解结合了多视图锚图的锚图张量。这一方法使我们能够同时考虑视图间的相关性，并获得更具解释性的聚类结果。

    arXiv:2404.00883v1 Announce Type: new  Abstract: The clustering method based on the anchor graph has gained significant attention due to its exceptional clustering performance and ability to process large-scale data. One common approach is to learn bipartite graphs with K-connected components, helping avoid the need for post-processing. However, this method has strict parameter requirements and may not always get K-connected components. To address this issue, an alternative approach is to directly obtain the cluster label matrix by performing non-negative matrix factorization (NMF) on the anchor graph. Nevertheless, existing multi-view clustering methods based on anchor graph factorization lack adequate cluster interpretability for the decomposed matrix and often overlook the inter-view information. We address this limitation by using non-negative tensor factorization to decompose an anchor graph tensor that combines anchor graphs from multiple views. This approach allows us to conside
    
[^2]: 基于大型语言模型增强强化学习的调查:概念、分类和方法

    Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods

    [https://arxiv.org/abs/2404.00282](https://arxiv.org/abs/2404.00282)

    大型语言模型在强化学习中具有潜在优势，通过结构化分类和角色分析，为未来研究提供指导。

    

    随着大规模语言模型(LLMs)拥有广泛的预训练知识和高级通用能力，它们在增强学习方面如多任务学习、样本效率和任务规划等方面展现出潜力。本调查综述了现有$\textit{LLM增强RL}$文献，总结了其与传统RL方法的特征，旨在澄清研究范围和未来研究方向。利用经典的Agent-环境交互范例，我们提出了一个结构化的分类法，系统地将LLMs在RL中的功能分类，包括四种角色：信息处理器、奖励设计者、决策者和生成器。此外，针对每个角色，我们总结了方法论，分析了缓解的特定RL挑战，并提供了未来方向的见解。最后，潜在应用、前景

    arXiv:2404.00282v1 Announce Type: cross  Abstract: With extensive pre-trained knowledge and high-level general capabilities, large language models (LLMs) emerge as a promising avenue to augment reinforcement learning (RL) in aspects such as multi-task learning, sample efficiency, and task planning. In this survey, we provide a comprehensive review of the existing literature in $\textit{LLM-enhanced RL}$ and summarize its characteristics compared to conventional RL methods, aiming to clarify the research scope and directions for future studies. Utilizing the classical agent-environment interaction paradigm, we propose a structured taxonomy to systematically categorize LLMs' functionalities in RL, including four roles: information processor, reward designer, decision-maker, and generator. Additionally, for each role, we summarize the methodologies, analyze the specific RL challenges that are mitigated, and provide insights into future directions. Lastly, potential applications, prospecti
    
[^3]: 自然引导的认知进化用于预测北温带湖泊中的溶解氧浓度

    Nature-Guided Cognitive Evolution for Predicting Dissolved Oxygen Concentrations in North Temperate Lakes

    [https://arxiv.org/abs/2403.18923](https://arxiv.org/abs/2403.18923)

    提出了一种自然引导的认知进化策略，通过多层融合自适应学习和自然过程，有效预测北温带湖泊中的溶解氧浓度

    

    预测北温带湖泊中的溶解氧（DO）浓度需要对不同生态系统中的物候模式进行全面研究，这凸显了选择物候特征和特征交互的重要性。基于过程的模型受部分过程知识限制或特征表示过于简化，而机器学习模型在有效选择不同湖泊类型和任务的相关特征交互方面面临挑战，尤其是在DO数据收集不频繁的情况下。在本文中，我们提出了一种自然引导的认知进化（NGCE）策略，这代表了自适应学习与自然过程多层融合。具体来说，我们利用代谢过程为基础的模型生成模拟DO标签。利用这些模拟标签，我们实施了一个多种群认知进化搜索，模型反映自然有机体，适应性地

    arXiv:2403.18923v1 Announce Type: cross  Abstract: Predicting dissolved oxygen (DO) concentrations in north temperate lakes requires a comprehensive study of phenological patterns across various ecosystems, which highlights the significance of selecting phenological features and feature interactions. Process-based models are limited by partial process knowledge or oversimplified feature representations, while machine learning models face challenges in efficiently selecting relevant feature interactions for different lake types and tasks, especially under the infrequent nature of DO data collection. In this paper, we propose a Nature-Guided Cognitive Evolution (NGCE) strategy, which represents a multi-level fusion of adaptive learning with natural processes. Specifically, we utilize metabolic process-based models to generate simulated DO labels. Using these simulated labels, we implement a multi-population cognitive evolutionary search, where models, mirroring natural organisms, adaptiv
    
[^4]: 基于聚类的神经网络规范化层

    Cluster-Based Normalization Layer for Neural Networks

    [https://arxiv.org/abs/2403.16798](https://arxiv.org/abs/2403.16798)

    该论文提出了一种基于聚类的神经网络规范化方法CB-Norm，通过引入高斯混合模型，解决了梯度稳定性和学习加速方面的挑战。

    

    深度学习在神经网络训练过程中面临重要挑战，包括内部协变量漂移、标签漂移、梯度消失/爆炸、过拟合和计算复杂性。传统的规范化方法，如批标准化，旨在解决其中一些问题，但通常依赖于限制其适应性的假设。混合规范化在处理多个高斯分布时面临计算障碍。本文介绍了基于聚类的规范化（CB-Norm）的两个变体——监督式基于聚类的规范化（SCB-Norm）和无监督式基于聚类的规范化（UCB-Norm），提出了一种开创性的一步规范化方法。CB-Norm利用高斯混合模型来专门解决与梯度稳定性和学习加速有关的挑战。

    arXiv:2403.16798v1 Announce Type: cross  Abstract: Deep learning faces significant challenges during the training of neural networks, including internal covariate shift, label shift, vanishing/exploding gradients, overfitting, and computational complexity. While conventional normalization methods, such as Batch Normalization, aim to tackle some of these issues, they often depend on assumptions that constrain their adaptability. Mixture Normalization faces computational hurdles in its pursuit of handling multiple Gaussian distributions.   This paper introduces Cluster-Based Normalization (CB-Norm) in two variants - Supervised Cluster-Based Normalization (SCB-Norm) and Unsupervised Cluster-Based Normalization (UCB-Norm) - proposing a groundbreaking one-step normalization approach. CB-Norm leverages a Gaussian mixture model to specifically address challenges related to gradient stability and learning acceleration.   For SCB-Norm, a supervised variant, the novel mechanism involves introduc
    
[^5]: 有限角度层析成像中数据驱动方法的鲁棒性

    Robustness of the data-driven approach in limited angle tomography

    [https://arxiv.org/abs/2403.11350](https://arxiv.org/abs/2403.11350)

    数据驱动方法在有限角度层析成像中相较于传统方法能更稳定地重构更多信息

    

    有限角度Radon变换由于其不逆问题而闻名于世。在这项工作中，我们给出了一个数学解释，即基于深度神经网络的数据驱动方法相较于传统方法可以以更稳定的方式重构更多信息。

    arXiv:2403.11350v1 Announce Type: cross  Abstract: The limited angle Radon transform is notoriously difficult to invert due to the ill-posedness. In this work, we give a mathematical explanation that the data-driven approach based on deep neural networks can reconstruct more information in a stable way compared to traditional methods.
    
[^6]: 通过图像字幕实现差分隐私表示学习

    Differentially Private Representation Learning via Image Captioning

    [https://arxiv.org/abs/2403.02506](https://arxiv.org/abs/2403.02506)

    通过图像字幕生成实现了有效的差分隐私表示学习，获得了高质量图像特征，可用于各种视觉和视觉语言任务。

    

    差分隐私（DP）机器学习被认为是从敏感数据中训练模型同时保护隐私的黄金标准解决方案。然而，实现这一理想的一个主要障碍是其次优的隐私-准确性权衡，在DP表示学习中特别明显。具体来说，已经证明在适度的隐私预算下，大多数模型学习的表示并不比手工特征显著更好。在这项工作中，我们展示了通过图像字幕和扩展到互联网规模的多模态数据集可以实现有效的DP表示学习。通过一系列工程技巧，我们成功地使用可观的计算量从头开始训练了DP图像字幕生成器（DP-Cap）在来自LAION-2B的233M子集上，并获得了前所未有的高质量图像特征，可用于各种下游视觉和视觉语言任务。

    arXiv:2403.02506v1 Announce Type: cross  Abstract: Differentially private (DP) machine learning is considered the gold-standard solution for training a model from sensitive data while still preserving privacy. However, a major barrier to achieving this ideal is its sub-optimal privacy-accuracy trade-off, which is particularly visible in DP representation learning. Specifically, it has been shown that under modest privacy budgets, most models learn representations that are not significantly better than hand-crafted features. In this work, we show that effective DP representation learning can be done via image captioning and scaling up to internet-scale multimodal datasets. Through a series of engineering tricks, we successfully train a DP image captioner (DP-Cap) on a 233M subset of LAION-2B from scratch using a reasonable amount of computation, and obtaining unprecedented high-quality image features that can be used in a variety of downstream vision and vision-language tasks. For examp
    
[^7]: 具有定时器的Mealy机器的主动学习

    Active Learning of Mealy Machines with Timers

    [https://arxiv.org/abs/2403.02019](https://arxiv.org/abs/2403.02019)

    这篇论文提出了一种用于查询学习具有定时器的Mealy机器的算法，在实现上明显比已有算法更有效率。

    

    我们在黑盒环境中提出了第一个用于查询学习一般类别的具有定时器的Mealy机器（MMTs）的算法。我们的算法是Vaandrager等人的L＃算法对定时设置的扩展。类似于Waga提出的用于学习定时自动机的算法，我们的算法受到Maler＆Pnueli思想的启发。我们的算法和Waga的算法都使用符号查询进行基础语言学习，然后使用有限数量的具体查询进行实现。然而，Waga需要指数级的具体查询来实现单个符号查询，而我们只需要多项式数量。这是因为要学习定时自动机，学习者需要确定每个转换的确切卫兵和重置（有指数多种可能性），而要学习MMT，学习者只需要弄清楚哪些先前的转换导致超时。正如我们之前的工作所示，

    arXiv:2403.02019v1 Announce Type: cross  Abstract: We present the first algorithm for query learning of a general class of Mealy machines with timers (MMTs) in a black-box context. Our algorithm is an extension of the L# algorithm of Vaandrager et al. to a timed setting. Like the algorithm for learning timed automata proposed by Waga, our algorithm is inspired by ideas of Maler & Pnueli. Based on the elementary languages of, both Waga's and our algorithm use symbolic queries, which are then implemented using finitely many concrete queries. However, whereas Waga needs exponentially many concrete queries to implement a single symbolic query, we only need a polynomial number. This is because in order to learn a timed automaton, a learner needs to determine the exact guard and reset for each transition (out of exponentially many possibilities), whereas for learning an MMT a learner only needs to figure out which of the preceding transitions caused a timeout. As shown in our previous work, 
    
[^8]: 深度导数增强的操作网络

    Derivative-enhanced Deep Operator Network

    [https://arxiv.org/abs/2402.19242](https://arxiv.org/abs/2402.19242)

    DE-DeepONet通过整合导数信息提高了预测准确性，尤其在训练数据有限的情况下，相比传统DeepONet取得了更好的效果。

    

    深度操作网络（DeepONets）是一类学习函数空间之间映射的神经算子，最近被开发为参数化偏微分方程（PDEs）的代理模型。本文提出了一种导数增强的深度操作网络（DE-DeepONet），它利用导数信息增强预测准确性，提供更准确的导数近似，特别是在训练数据有限的情况下。DE-DeepONet将输入的维度缩减到DeepONet中，并在训练的损失函数中包含两种类型的导数标签，即关于输入函数的输出函数的方向导数和关于物理域变量的输出函数的梯度。我们通过在三个不断增加复杂度的方程上测试DE-DeepONet，以展示其相对于普通DeepONet的有效性。

    arXiv:2402.19242v1 Announce Type: new  Abstract: Deep operator networks (DeepONets), a class of neural operators that learn mappings between function spaces, have recently been developed as surrogate models for parametric partial differential equations (PDEs). In this work we propose a derivative-enhanced deep operator network (DE-DeepONet), which leverages the derivative information to enhance the prediction accuracy, and provide a more accurate approximation of the derivatives, especially when the training data are limited. DE-DeepONet incorporates dimension reduction of input into DeepONet and includes two types of derivative labels in the loss function for training, that is, the directional derivatives of the output function with respect to the input function and the gradient of the output function with respect to the physical domain variables. We test DE-DeepONet on three different equations with increasing complexity to demonstrate its effectiveness compared to the vanilla DeepON
    
[^9]: 用于非对数凹分布的零阶采样方法：通过去噪扩散缓解亚稳定性

    Zeroth-Order Sampling Methods for Non-Log-Concave Distributions: Alleviating Metastability by Denoising Diffusion

    [https://arxiv.org/abs/2402.17886](https://arxiv.org/abs/2402.17886)

    本文提出了一种基于去噪扩散过程的零阶扩散蒙特卡洛算法，克服了非对数凹分布采样中的亚稳定性问题，并证明其采样精度具有倒多项式依赖。

    

    这篇论文考虑了基于其非对数凹分布未归一化密度查询的采样问题。首先描述了一个基于模拟去噪扩散过程的框架，即扩散蒙特卡洛（DMC），其得分函数通过通用蒙特卡洛估计器逼近。DMC是一个基于神谕的元算法，其中神谕是假设可以访问生成蒙特卡洛分数估计器的样本的访问。然后，我们提供了一个基于拒绝采样的这个神谕的实现，这将DMC转化为一个真正的算法，称为零阶扩散蒙特卡洛（ZOD-MC）。我们通过首先构建一个通用框架，即DMC的性能保证，而不假设目标分布为对数凹或满足任何等周不等式，提供了收敛分析。然后我们证明ZOD-MC对所需采样精度具有倒多项式依赖，尽管仍然受到...

    arXiv:2402.17886v1 Announce Type: cross  Abstract: This paper considers the problem of sampling from non-logconcave distribution, based on queries of its unnormalized density. It first describes a framework, Diffusion Monte Carlo (DMC), based on the simulation of a denoising diffusion process with its score function approximated by a generic Monte Carlo estimator. DMC is an oracle-based meta-algorithm, where its oracle is the assumed access to samples that generate a Monte Carlo score estimator. Then we provide an implementation of this oracle, based on rejection sampling, and this turns DMC into a true algorithm, termed Zeroth-Order Diffusion Monte Carlo (ZOD-MC). We provide convergence analyses by first constructing a general framework, i.e. a performance guarantee for DMC, without assuming the target distribution to be log-concave or satisfying any isoperimetric inequality. Then we prove that ZOD-MC admits an inverse polynomial dependence on the desired sampling accuracy, albeit sti
    
[^10]: 基于锚图因子分解的无锚聚类

    Anchor-free Clustering based on Anchor Graph Factorization

    [https://arxiv.org/abs/2402.15688](https://arxiv.org/abs/2402.15688)

    无锚聚类方法AFCAGF通过学习锚图并优化成对样本距离，避免了锚点选择和初始化的需要，提升了聚类算法性能。

    

    锚点方法是处理大规模数据聚类的一个关键方法。然而，这些方法通常包括两个不同阶段：选择锚点和构建锚图。这种二分以及锚点的初始化显著影响算法的整体性能。为了缓解这些问题，我们引入了一种名为基于锚图因子分解的无锚聚类（AFCAGF）的新方法。AFCAGF在学习锚图方面具有创新性，只需要计算样本之间的成对距离。通过简单的优化过程，可以实现这一过程，避免了需要明确选择锚点的必要性。具体而言，我们的方法增强了模糊k均值聚类算法（FKM），引入了一种新的流形学习技术，无需初始化聚类中心。此外，我们发展了记忆的概念。

    arXiv:2402.15688v1 Announce Type: new  Abstract: Anchor-based methods are a pivotal approach in handling clustering of large-scale data. However, these methods typically entail two distinct stages: selecting anchor points and constructing an anchor graph. This bifurcation, along with the initialization of anchor points, significantly influences the overall performance of the algorithm. To mitigate these issues, we introduce a novel method termed Anchor-free Clustering based on Anchor Graph Factorization (AFCAGF). AFCAGF innovates in learning the anchor graph, requiring only the computation of pairwise distances between samples. This process, achievable through straightforward optimization, circumvents the necessity for explicit selection of anchor points. More concretely, our approach enhances the Fuzzy k-means clustering algorithm (FKM), introducing a new manifold learning technique that obviates the need for initializing cluster centers. Additionally, we evolve the concept of the mem
    
[^11]: NeuralThink: 在一般任务中进行外推的算法综合

    NeuralThink: Algorithm Synthesis that Extrapolates in General Tasks

    [https://arxiv.org/abs/2402.15393](https://arxiv.org/abs/2402.15393)

    NeuralThink 是一种新的递归架构，可以一贯地对对称和不对称任务进行外推，相较于之前的最先进的深度思维架构在稳定地从较小的训练规模对大观测进行外推方面表现出更好的性能。

    

    虽然机器学习方法擅长模式识别，但在可扩展的算法方式上处理复杂的推理任务时仍然面临困难。最近的深度思维方法展现了学习可以外推的算法的潜力：在较小的环境中学习并在较大的环境中执行学到的算法。然而，这些工作局限于对称任务，即输入和输出的维度相同。为了填补这一空白，我们提出了 NeuralThink，一种新的递归架构，可以一贯地对对称和不对称任务进行外推，其中输入和输出的维度不同。我们提供了一个新颖的不对称任务外推基准。我们展示了 NeuralThink 在稳定地从较小的训练规模对大观测进行外推方面一直优于之前的最先进的深度思维架构。

    arXiv:2402.15393v1 Announce Type: cross  Abstract: While machine learning methods excel at pattern recognition, they struggle with complex reasoning tasks in a scalable, algorithmic manner. Recent Deep Thinking methods show promise in learning algorithms that extrapolate: learning in smaller environments and executing the learned algorithm in larger environments. However, these works are limited to symmetrical tasks, where the input and output dimensionalities are the same. To address this gap, we propose NeuralThink, a new recurrent architecture that can consistently extrapolate to both symmetrical and asymmetrical tasks, where the dimensionality of the input and output are different. We contribute with a novel benchmark of asymmetrical tasks for extrapolation. We show that NeuralThink consistently outperforms the prior state-of-the-art Deep Thinking architectures, in regards to stable extrapolation to large observations from smaller training sizes.
    
[^12]: 基于深度强化学习和迁移学习的边缘缓存

    Edge Caching Based on Deep Reinforcement Learning and Transfer Learning

    [https://arxiv.org/abs/2402.14576](https://arxiv.org/abs/2402.14576)

    本文提出了一种基于双深度Q学习的缓存方法，通过半马尔可夫决策过程（SMDP）适应现实场景中随机请求到达的特性，综合考虑各种文件特征。

    

    本文讨论了网络中冗余数据传输日益挑战的问题。流量激增已经使中继链路和骨干网络承压，促使对边缘路由器的缓存解决方案进行探索。现有工作主要依赖于马尔可夫决策过程（MDP）处理缓存问题，假设固定时间间隔的决策；然而，现实场景涉及随机请求到达，尽管各种文件特征在确定最佳缓存策略方面起着至关重要的作用，但相关的现有工作并未考虑所有这些文件特征来形成缓存策略。在本文中，首先我们利用半马尔可夫决策过程（SMDP）来建模缓存问题，以适应现实场景的连续时间特性，允许在文件请求时随机进行缓存决策。然后，我们提出了一种基于双深度Q学习的缓存方法，全面考虑了不同文件特征的影响。

    arXiv:2402.14576v1 Announce Type: cross  Abstract: This paper addresses the escalating challenge of redundant data transmission in networks. The surge in traffic has strained backhaul links and backbone networks, prompting the exploration of caching solutions at the edge router. Existing work primarily relies on Markov Decision Processes (MDP) for caching issues, assuming fixed-time interval decisions; however, real-world scenarios involve random request arrivals, and despite the critical role of various file characteristics in determining an optimal caching policy, none of the related existing work considers all these file characteristics in forming a caching policy. In this paper, first, we formulate the caching problem using a semi-Markov Decision Process (SMDP) to accommodate the continuous-time nature of real-world scenarios allowing for caching decisions at random times upon file requests. Then, we propose a double deep Q-learning-based caching approach that comprehensively accou
    
[^13]: 线性变换器是多功能的上下文学习器

    Linear Transformers are Versatile In-Context Learners

    [https://arxiv.org/abs/2402.14180](https://arxiv.org/abs/2402.14180)

    线性变换器展示了在处理复杂问题和噪音干扰数据时的多功能性，通过发现一种新颖的优化算法，超越了许多合理的基线。

    

    最近的研究表明，变换器，特别是线性注意力模型，在前向推理步骤中对提供的上下文数据隐含地执行类似于梯度下降的算法。然而，它们在处理更复杂问题方面的能力尚未被探索。本文证明了任何线性变换器都保持隐式线性模型，并可解释为执行一种变形的预条件梯度下降。我们还研究了在线性变换器在训练数据受到不同水平噪音干扰的挑战性场景中的应用。值得注意的是，我们展示了对于这个问题，线性变换器发现了一种复杂且高效的优化算法，超越或与许多合理基线的表现相匹敌。我们反向工程了这个算法，并表明这是一种基于动量和噪音水平的自适应重缩放的新方法。

    arXiv:2402.14180v1 Announce Type: new  Abstract: Recent research has demonstrated that transformers, particularly linear attention models, implicitly execute gradient-descent-like algorithms on data provided in-context during their forward inference step. However, their capability in handling more complex problems remains unexplored. In this paper, we prove that any linear transformer maintains an implicit linear model and can be interpreted as performing a variant of preconditioned gradient descent. We also investigate the use of linear transformers in a challenging scenario where the training data is corrupted with different levels of noise. Remarkably, we demonstrate that for this problem linear transformers discover an intricate and highly effective optimization algorithm, surpassing or matching in performance many reasonable baselines. We reverse-engineer this algorithm and show that it is a novel approach incorporating momentum and adaptive rescaling based on noise levels. Our fi
    
[^14]: 学习性是一种紧凑性质

    Learnability is a Compact Property

    [https://arxiv.org/abs/2402.10360](https://arxiv.org/abs/2402.10360)

    监督学习问题的困难性具有紧凑的有限特性表征。

    

    最近关于学习的工作取得了一个引人注目的结果：各种问题的可学习性可能是不可判定的，或者与标准集合论ZFC公理无关。此外，这种问题的可学习性可能不是具有有限特性的属性：非正式地说，它不能通过检查问题的有限投影来检测。

    arXiv:2402.10360v1 Announce Type: new  Abstract: Recent work on learning has yielded a striking result: the learnability of various problems can be undecidable, or independent of the standard ZFC axioms of set theory. Furthermore, the learnability of such problems can fail to be a property of finite character: informally, it cannot be detected by examining finite projections of the problem.   On the other hand, learning theory abounds with notions of dimension that characterize learning and consider only finite restrictions of the problem, i.e., are properties of finite character. How can these results be reconciled? More precisely, which classes of learning problems are vulnerable to logical undecidability, and which are within the grasp of finite characterizations?   We demonstrate that the difficulty of supervised learning with metric losses admits a tight finite characterization. In particular, we prove that the sample complexity of learning a hypothesis class can be detected by ex
    
[^15]: 基因引导GFlowNets：在实际分子优化基准方面的进展

    Genetic-guided GFlowNets: Advancing in Practical Molecular Optimization Benchmark

    [https://arxiv.org/abs/2402.05961](https://arxiv.org/abs/2402.05961)

    本文提出了一种名为基因引导GFlowNet (Genetic GFN) 的新方法，通过集成迭代遗传搜索和训练策略，该方法在实际分子优化基准测试中取得了16.213的最新得分，明显优于现有最佳得分15.185，同时在14个任务中超越了所有对比方法。

    

    本文提出了一种新的GFlowNet变体，即基因引导GFlowNet (Genetic GFN)，它将迭代遗传搜索集成到GFlowNet中。遗传搜索有效地引导GFlowNet进入高回报区域，解决了全局过度探索导致的训练效率低下和探索有限区域的问题。此外，还引入了训练策略，如基于排名的重放训练和无监督最大似然预训练，以提高基因引导GFlowNet的样本效率。该方法在实际分子优化 (PMO) 领域的官方基准测试中显示了16.213的最新得分，明显优于基准测试中报告的最佳得分15.185。值得注意的是，我们的方法在23个任务中的14个任务中超过了所有对比方法，包括强化学习，贝叶斯优化，生成模型，GFlowNets和遗传算法。

    This paper proposes a novel variant of GFlowNet, genetic-guided GFlowNet (Genetic GFN), which integrates an iterative genetic search into GFlowNet. Genetic search effectively guides the GFlowNet to high-rewarded regions, addressing global over-exploration that results in training inefficiency and exploring limited regions. In addition, training strategies, such as rank-based replay training and unsupervised maximum likelihood pre-training, are further introduced to improve the sample efficiency of Genetic GFN. The proposed method shows a state-of-the-art score of 16.213, significantly outperforming the reported best score in the benchmark of 15.185, in practical molecular optimization (PMO), which is an official benchmark for sample-efficient molecular optimization. Remarkably, ours exceeds all baselines, including reinforcement learning, Bayesian optimization, generative models, GFlowNets, and genetic algorithms, in 14 out of 23 tasks.
    
[^16]: 对角费舍尔信息矩阵估计器的权衡

    Tradeoffs of Diagonal Fisher Information Matrix Estimators

    [https://arxiv.org/abs/2402.05379](https://arxiv.org/abs/2402.05379)

    本研究探讨了使用对角费舍尔信息矩阵估计器的权衡。通过分析和数值研究，发现方差量取决于非线性与不同参数组之间的关系，应该在估计费舍尔信息时予以重视。

    

    费舍尔信息矩阵描述了神经网络参数空间中的局部几何性质，它提供了理论和工具来理解和优化神经网络。鉴于其计算成本高，实践者通常使用随机估计器，并仅评估对角线条目。我们研究了两种这样的估计器，其准确性和样本复杂性取决于它们关联的方差。我们推导了方差的界限，并在回归和分类网络中实例化它们。我们通过分析和数值研究来权衡这两个估计器。我们发现方差量取决于关于不同参数组的非线性，当估计费舍尔信息时不能忽视它们。

    The Fisher information matrix characterizes the local geometry in the parameter space of neural networks. It elucidates insightful theories and useful tools to understand and optimize neural networks. Given its high computational cost, practitioners often use random estimators and evaluate only the diagonal entries. We examine two such estimators, whose accuracy and sample complexity depend on their associated variances. We derive bounds of the variances and instantiate them in regression and classification networks. We navigate trade-offs of both estimators based on analytical and numerical studies. We find that the variance quantities depend on the non-linearity with respect to different parameter groups and should not be neglected when estimating the Fisher information.
    
[^17]: 以显式奖励的噪声对比对齐语言模型

    Noise Contrastive Alignment of Language Models with Explicit Rewards

    [https://arxiv.org/abs/2402.05369](https://arxiv.org/abs/2402.05369)

    本文提出了一个基于噪声对比估计的通用LM对齐框架，能够处理明确注释的奖励数据，并且扩展了当前的对齐理论。

    

    用户意图通常被形式化为需要在微调语言模型时最大化的评估奖励。现有的对齐方法，如直接优化偏好（DPO），主要适用于隐含定义而非明确给定奖励的两两偏好数据。在本文中，我们引入了一个通用的LM对齐框架，利用噪声对比估计（NCE）来解决明确注释有标量评估的奖励数据处理的差距。我们的框架包括两个并行算法，NCA和InfoNCA，两者都能从奖励数据和偏好数据中直接提取LM策略。值得注意的是，我们证明了DPO损失是我们提出的InfoNCA目标在两两偏好设置下的特殊情况，从而集成和扩展了当前的对齐理论。通过对比NCA和InfoNCA，我们展示了InfoNCA和DPO如何在不同响应对于单个指令的相对可能性上进行调整。

    User intentions are typically formalized as evaluation rewards to be maximized when fine-tuning language models (LMs). Existing alignment methods, such as Direct Preference Optimization (DPO), are mainly tailored for pairwise preference data where rewards are implicitly defined rather than explicitly given. In this paper, we introduce a general framework for LM alignment, leveraging Noise Contrastive Estimation (NCE) to bridge the gap in handling reward datasets explicitly annotated with scalar evaluations. Our framework comprises two parallel algorithms, NCA and InfoNCA, both enabling the direct extraction of an LM policy from reward data as well as preference data. Notably, we show that the DPO loss is a special case of our proposed InfoNCA objective under pairwise preference settings, thereby integrating and extending current alignment theories. By contrasting NCA and InfoNCA, we show that InfoNCA and DPO adjust relative likelihood across different responses to a single instruction,
    
[^18]: 学习来自块稀疏信号的多样化

    Learning with Diversification from Block Sparse Signal

    [https://arxiv.org/abs/2402.04646](https://arxiv.org/abs/2402.04646)

    本文提出了一种新的先验，称为多样化块稀疏先验，用来描述真实世界数据中的广泛块稀疏现象。通过允许方差和相关矩阵的多样化，解决了现有块稀疏学习方法对预定义块信息的敏感性问题，并提出了一种多样化的块稀疏贝叶斯学习方法(DivSBL)，实现自适应的块估计，减轻过拟合的风险，并建立了全局和局部最优性理论。实验结果证明了DivSBL相对于现有算法的优势。

    

    本文引入了一种新颖的先验Diversified Block Sparse Prior，来描述真实世界数据中的广泛块稀疏现象。通过允许方差和相关矩阵的多样化，我们有效地解决了现有块稀疏学习方法对预定义块信息的敏感性问题，从而实现自适应的块估计，同时减轻过拟合的风险。基于此，我们提出了一种多样化的块稀疏贝叶斯学习方法(DivSBL)，利用EM算法和对偶上升法进行超参数估计。此外，我们建立了我们模型的全局和局部最优性理论。实验证明了DivSBL相对于现有算法的优势。

    This paper introduces a novel prior called Diversified Block Sparse Prior to characterize the widespread block sparsity phenomenon in real-world data. By allowing diversification on variance and correlation matrix, we effectively address the sensitivity issue of existing block sparse learning methods to pre-defined block information, which enables adaptive block estimation while mitigating the risk of overfitting. Based on this, a diversified block sparse Bayesian learning method (DivSBL) is proposed, utilizing EM algorithm and dual ascent method for hyperparameter estimation. Moreover, we establish the global and local optimality theory of our model. Experiments validate the advantages of DivSBL over existing algorithms.
    
[^19]: 在线特征更新改善在线（广义）标签转移适应问题

    Online Feature Updates Improve Online (Generalized) Label Shift Adaptation

    [https://arxiv.org/abs/2402.03545](https://arxiv.org/abs/2402.03545)

    本文提出了一种名为OLS-OFU的新方法，通过在测试时利用无标签数据进行自监督学习和特征优化，以解决在线设置中的标签转移问题。研究结果表明，OLS-OFU在各种数据集和领域转移条件下都表现出了有效性和鲁棒性。

    

    本文解决了在线设置中标签转移的普遍问题，其中存在缺失的标签，数据分布随时间变化，及时获得标签是一项具有挑战性的任务。虽然现有的方法主要集中在调整或更新预训练分类器的最后一层，我们探索了在测试时使用无标签数据来改进特征表示的未被发掘的潜力。我们的新方法，在线标签转移自适应与在线特征更新（OLS-OFU），利用自监督学习来优化特征提取过程，从而改进预测模型。理论分析证实，OLS-OFU通过利用自监督学习进行特征优化，减少了算法遗憾。在各种数据集上的实证研究，在在线标签转移和广义标签转移条件下，强调了OLS-OFU的有效性和鲁棒性，特别是在领域转移的情况下。

    This paper addresses the prevalent issue of label shift in an online setting with missing labels, where data distributions change over time and obtaining timely labels is challenging. While existing methods primarily focus on adjusting or updating the final layer of a pre-trained classifier, we explore the untapped potential of enhancing feature representations using unlabeled data at test-time. Our novel method, Online Label Shift adaptation with Online Feature Updates (OLS-OFU), leverages self-supervised learning to refine the feature extraction process, thereby improving the prediction model. Theoretical analyses confirm that OLS-OFU reduces algorithmic regret by capitalizing on self-supervised learning for feature refinement. Empirical studies on various datasets, under both online label shift and generalized label shift conditions, underscore the effectiveness and robustness of OLS-OFU, especially in cases of domain shifts.
    
[^20]: 学习依赖类型的结构感知表示

    Learning Structure-Aware Representations of Dependent Types

    [https://arxiv.org/abs/2402.02104](https://arxiv.org/abs/2402.02104)

    本文扩展了Agda生态系统到机器学习领域，并发布了一种新颖的依赖类型编程语言的证明数据集。通过提出一种基于结构而非命名原则的新颖神经架构，我们能够准确地表示依赖类型程序，并在前提选择任务中取得了强大的初步结果。

    

    Agda是一种依赖类型编程语言和证明助手，在证明形式化和编程语言理论中起着关键作用。本文将Agda生态系统扩展到机器学习领域，并反过来使机器学习从业者能够使用Agda相关资源。我们介绍并发布了一种新颖的Agda程序证明数据集，该数据集既详尽又广泛，可以支持各种机器学习应用，这是首个这样的数据集。利用数据集的超高分辨率，详细展示了亚型级别的证明状态，我们提出了一种基于结构而不是命名原则准确表示依赖类型程序的新颖神经架构。我们在前提选择设置中实例化和评估我们的架构，取得了强大的初步结果。

    Agda is a dependently-typed programming language and a proof assistant, pivotal in proof formalization and programming language theory. This paper extends the Agda ecosystem into machine learning territory, and, vice versa, makes Agda-related resources available to machine learning practitioners. We introduce and release a novel dataset of Agda program-proofs that is elaborate and extensive enough to support various machine learning applications -- the first of its kind. Leveraging the dataset's ultra-high resolution, detailing proof states at the sub-type level, we propose a novel neural architecture targeted at faithfully representing dependently-typed programs on the basis of structural rather than nominal principles. We instantiate and evaluate our architecture in a premise selection setup, where it achieves strong initial results.
    
[^21]: 学习通过原始-对偶策略梯度算法对无限时域平均回报受限MDP进行参数化通用策略

    Learning General Parameterized Policies for Infinite Horizon Average Reward Constrained MDPs via Primal-Dual Policy Gradient Algorithm

    [https://arxiv.org/abs/2402.02042](https://arxiv.org/abs/2402.02042)

    该论文研究了无限时域平均回报受限MDPs的参数化通用策略，并提出了一种基于原始-对偶策略梯度算法，可在保证低遗憾的情况下管理约束条件，达到全局最优策略。算法的分析表明，其目标遗憾和约束违反均为 $\tilde{\mathcal{O}}({T}^{3/4})$。

    

    本文探索了无限时域平均回报受限马尔科夫决策过程（CMDP）的领域。据我们所知，这项工作是首次研究具有通用策略参数化的平均回报CMDP的遗憾和约束违规分析。为了解决这个挑战，我们提出了一种基于原始对偶的策略梯度算法，能够灵活地管理约束条件，并确保低遗憾保证以实现全局最优策略。特别地，我们证明了我们提出的算法在目标遗憾和约束违反上具有 $\tilde{\mathcal{O}}({T}^{3/4})$ 的界限。

    This paper explores the realm of infinite horizon average reward Constrained Markov Decision Processes (CMDP). To the best of our knowledge, this work is the first to delve into the regret and constraint violation analysis of average reward CMDPs with a general policy parametrization. To address this challenge, we propose a primal dual based policy gradient algorithm that adeptly manages the constraints while ensuring a low regret guarantee toward achieving a global optimal policy. In particular, we demonstrate that our proposed algorithm achieves $\tilde{\mathcal{O}}({T}^{3/4})$ objective regret and $\tilde{\mathcal{O}}({T}^{3/4})$ constraint violation bounds.
    
[^22]: 近确定性回归中的错误规范化不确定性

    Misspecification uncertainties in near-deterministic regression

    [https://arxiv.org/abs/2402.01810](https://arxiv.org/abs/2402.01810)

    该论文研究了近确定性回归中错误规范化的不确定性问题，并提出了一种组合模型，以准确预测和控制参数不确定性。

    

    期望损失是模型泛化误差的上界，可用于学习的鲁棒PAC-Bayes边界。然而，损失最小化被认为忽略了错误规范化，即模型不能完全复制观测结果。这导致大数据或欠参数化极限下对参数不确定性的显著低估。我们分析近确定性、错误规范化和欠参数化替代模型的泛化误差，这是科学和工程中广泛相关的一个领域。我们证明后验分布必须覆盖每个训练点，以避免发散的泛化误差，并导出一个符合这个约束的组合模型。对于线性模型，这种高效的方法产生的额外开销最小。这种高效方法在模型问题上进行了演示，然后应用于原子尺度机器学习中的高维数据集。

    The expected loss is an upper bound to the model generalization error which admits robust PAC-Bayes bounds for learning. However, loss minimization is known to ignore misspecification, where models cannot exactly reproduce observations. This leads to significant underestimates of parameter uncertainties in the large data, or underparameterized, limit. We analyze the generalization error of near-deterministic, misspecified and underparametrized surrogate models, a regime of broad relevance in science and engineering. We show posterior distributions must cover every training point to avoid a divergent generalization error and derive an ensemble {ansatz} that respects this constraint, which for linear models incurs minimal overhead. The efficient approach is demonstrated on model problems before application to high dimensional datasets in atomistic machine learning. Parameter uncertainties from misspecification survive in the underparametrized limit, giving accurate prediction and boundin
    
[^23]: 基于得分的因果表示学习：线性和一般的转化

    Score-based Causal Representation Learning: Linear and General Transformations

    [https://arxiv.org/abs/2402.00849](https://arxiv.org/abs/2402.00849)

    这篇论文提出了一种基于得分的算法类，用于干预范围内的因果表示学习，涵盖了线性和一般转化。算法保证了可识别性和实现性，并且通过创造性地将得分函数与因果表示学习相结合。

    

    本篇论文针对一般非参数潜在因果模型和将潜在变量映射到观测变量的未知转化，研究了基于干预的因果表示学习（CRL）。研究了线性和一般的转化。这篇论文同时讨论了可识别性和实现性两个方面。可识别性是指确定算法不相关的条件，以确保恢复真实的潜在因果变量和潜在因果图。实现性是指算法方面，解决设计算法来实现可识别保证的问题。通过将得分函数（即密度函数对数的梯度）与CRL之间建立新联系，本文设计了一种得分为基础的算法类，确保了可识别性和实现性。首先，本文专注于线性转化，并展示了每个n个随机硬干预下该转化的因果表示可识别。

    This paper addresses intervention-based causal representation learning (CRL) under a general nonparametric latent causal model and an unknown transformation that maps the latent variables to the observed variables. Linear and general transformations are investigated. The paper addresses both the \emph{identifiability} and \emph{achievability} aspects. Identifiability refers to determining algorithm-agnostic conditions that ensure recovering the true latent causal variables and the latent causal graph underlying them. Achievability refers to the algorithmic aspects and addresses designing algorithms that achieve identifiability guarantees. By drawing novel connections between \emph{score functions} (i.e., the gradients of the logarithm of density functions) and CRL, this paper designs a \emph{score-based class of algorithms} that ensures both identifiability and achievability. First, the paper focuses on \emph{linear} transformations and shows that one stochastic hard intervention per n
    
[^24]: 无法区分的区分：算法预测中的人类专业知识

    Distinguishing the Indistinguishable: Human Expertise in Algorithmic Prediction

    [https://arxiv.org/abs/2402.00793](https://arxiv.org/abs/2402.00793)

    本论文介绍了一种新的框架，将人类专业知识纳入算法预测中，重点在于利用人的判断力区分对于任何可行的预测算法来说“看起来相同”的输入。

    

    我们引入了一种将人类专业知识纳入算法预测的新框架。我们的方法主要关注利用人的判断力来区分那些对于任何可行的预测算法来说“看起来相同”的输入。我们认为，这种框架能够澄清人工智能与人类协作预测任务中的问题，因为专家通常具有信息的访问权限——特别是主观信息——而这些信息是算法训练数据中没有编码的。基于这一认识，我们开发了一组有原则的算法，仅在任何可行的预测器的性能有所改善时才选择性地纳入人类反馈。经验结果表明，尽管算法在平均水平上往往优于人类对应任务的能力，但人类判断在特定情况下（可以预先确定）能够显著提高算法预测的性能。在一个X射线分类任务中，我们发现这个子集在患者群体中占据了近30%。我们的方法提供了一种自然的方式，

    We introduce a novel framework for incorporating human expertise into algorithmic predictions. Our approach focuses on the use of human judgment to distinguish inputs which `look the same' to any feasible predictive algorithm. We argue that this framing clarifies the problem of human/AI collaboration in prediction tasks, as experts often have access to information -- particularly subjective information -- which is not encoded in the algorithm's training data. We use this insight to develop a set of principled algorithms for selectively incorporating human feedback only when it improves the performance of any feasible predictor. We find empirically that although algorithms often outperform their human counterparts on average, human judgment can significantly improve algorithmic predictions on specific instances (which can be identified ex-ante). In an X-ray classification task, we find that this subset constitutes nearly 30% of the patient population. Our approach provides a natural way
    
[^25]: 比较基于模板和非模板语言模型的探测方法

    Comparing Template-based and Template-free Language Model Probing

    [https://arxiv.org/abs/2402.00123](https://arxiv.org/abs/2402.00123)

    本文比较了基于模板和非模板语言模型的探测方法，发现它们在模型排名、绝对得分和与困惑度的关系等方面存在差异。

    

    以专家制作的模板和自然发生的文本为基础的语言模型探测方法的差异经常被忽视。在这里，我们评估了16种不同的语言模型在10个英文探测数据集上的性能，其中包括4个基于模板的和6个非模板的数据集，并针对以下研究问题进行了回答：（RQ1）模型排名在两种方法中是否不同？（RQ2）模型的绝对得分在两种方法中是否不同？（RQ3）RQ1和RQ2的答案在一般和领域特定模型之间是否不同？我们的发现是：1）除了顶级的领域特定模型外，基于模板和非模板方法通常排名不同。2）与平行的非模板和模板提示相比，准确度下降了最多42%。3）在非模板方法中，困惑度与准确度呈负相关，但是在基于模板的探测中，它们呈正相关，这与直觉相反。4）模型倾向于预测相同的内容。

    The differences between cloze-task language model (LM) probing with 1) expert-made templates and 2) naturally-occurring text have often been overlooked. Here, we evaluate 16 different LMs on 10 probing English datasets -- 4 template-based and 6 template-free -- in general and biomedical domains to answer the following research questions: (RQ1) Do model rankings differ between the two approaches? (RQ2) Do models' absolute scores differ between the two approaches? (RQ3) Do the answers to RQ1 and RQ2 differ between general and domain-specific models? Our findings are: 1) Template-free and template-based approaches often rank models differently, except for the top domain-specific models. 2) Scores decrease by up to 42% Acc@1 when comparing parallel template-free and template-based prompts. 3) Perplexity is negatively correlated with accuracy in the template-free approach, but, counter-intuitively, they are positively correlated for template-based probing. 4) Models tend to predict the same
    
[^26]: 多机构数据的释放力量：整合和协调跨机构的基因组数据

    Unlocking the Power of Multi-institutional Data: Integrating and Harmonizing Genomic Data Across Institutions

    [https://arxiv.org/abs/2402.00077](https://arxiv.org/abs/2402.00077)

    该研究介绍了一种名为Bridge的模型，致力于解决利用多机构测序数据时面临的挑战，包括基因组板块的变化、测序技术的差异以及数据的高维度和稀疏性等。

    

    癌症是由基因突变驱动的复杂疾病，肿瘤测序已成为癌症患者临床护理的重要手段。出现的多机构测序数据为学习真实世界的证据以增强精准肿瘤医学提供了强大的资源。由美国癌症研究协会领导的GENIE BPC建立了一个独特的数据库，将基因组数据与多个癌症中心的临床信息联系起来。然而，利用这种多机构测序数据面临着重大挑战。基因组板块的变化导致在使用常见基因集进行分析时信息丢失。此外，不同的测序技术和机构之间的患者异质性增加了复杂性。高维数据、稀疏基因突变模式以及个体基因水平上的弱信号进一步增加了问题的复杂性。在这些现实世界的挑战的推动下，我们引入了Bridge模型。

    Cancer is a complex disease driven by genomic alterations, and tumor sequencing is becoming a mainstay of clinical care for cancer patients. The emergence of multi-institution sequencing data presents a powerful resource for learning real-world evidence to enhance precision oncology. GENIE BPC, led by the American Association for Cancer Research, establishes a unique database linking genomic data with clinical information for patients treated at multiple cancer centers. However, leveraging such multi-institutional sequencing data presents significant challenges. Variations in gene panels result in loss of information when the analysis is conducted on common gene sets. Additionally, differences in sequencing techniques and patient heterogeneity across institutions add complexity. High data dimensionality, sparse gene mutation patterns, and weak signals at the individual gene level further complicate matters. Motivated by these real-world challenges, we introduce the Bridge model. It use
    
[^27]: 通过最优传输实现公平的核心集

    Fair Coresets via Optimal Transport

    [https://arxiv.org/abs/2311.05436](https://arxiv.org/abs/2311.05436)

    本研究提出了公平的Wasserstein核心集(FWC)，该方法通过最小化原始数据集与加权合成样本之间的Wasserstein距离，并强制实现人口平等，生成公平的合成代表性样本，可用于下游学习任务。

    

    数据精炼和核心集已成为生成用于处理大规模数据集的下游学习任务的较小代表性样本集的流行方法。与此同时，机器学习越来越多地应用于社会层面的决策过程，使得模型构建者必须解决存在于数据中的子群体的固有偏见问题。当前方法通过优化相对于原始样本的局部属性来创建公平的合成代表性样本，但其对下游学习过程的影响尚未被探索。在这项工作中，我们提出了公平的Wasserstein核心集（FWC），一种新颖的核心集方法，它生成既具有公平性的合成代表性样本，又具有用于下游学习任务的样本级权重。FWC最小化原始数据集与加权合成样本之间的Wasserstein距离，同时强制实现人口平等。我们展示了FWC的无约束版本等价于通常的最优传输问题，并且通过实验证明了FWC的有效性和公平性。

    Data distillation and coresets have emerged as popular approaches to generate a smaller representative set of samples for downstream learning tasks to handle large-scale datasets. At the same time, machine learning is being increasingly applied to decision-making processes at a societal level, making it imperative for modelers to address inherent biases towards subgroups present in the data. Current approaches create fair synthetic representative samples by optimizing local properties relative to the original samples, but their effect on downstream learning processes has yet to be explored. In this work, we present fair Wasserstein coresets (FWC), a novel coreset approach which generates fair synthetic representative samples along with sample-level weights to be used in downstream learning tasks. FWC minimizes the Wasserstein distance between the original dataset and the weighted synthetic samples while enforcing demographic parity. We show that an unconstrained version of FWC is equiv
    
[^28]: 不同不确定性下的公平排名

    Fair Ranking under Disparate Uncertainty

    [https://arxiv.org/abs/2309.01610](https://arxiv.org/abs/2309.01610)

    提出了一种新的公平排名标准Equal-Opportunity Ranking（EOR），将底层相关性模型的不确定性差异考虑在内，通过组内公平抽奖实现公平排名。

    

    排名是一种广泛使用的方法，用于将人类评估者的注意力集中在可管理的选项子集上。它作为人类决策过程的一部分的使用范围从在电子商务网站上展示潜在相关产品到为人工审查优先处理大学申请。虽然排名可以通过将关注集中在最有前途的选项上使人类评估更加高效，但我们认为，如果底层相关性模型的不确定性在不同组别的选项之间存在差异，排名可能会引入不公平。不幸的是，这种不确定性差异似乎普遍存在，常常对少数群体造成损害，因为这些群体的相关性估计可能由于缺乏数据或合适的特征而具有更高的不确定性。为了解决这个公平问题，我们提出了Equal-Opportunity Ranking（EOR）作为排名的新公平标准，并展示它对应于在相关选项之间进行组内公平抽奖

    arXiv:2309.01610v2 Announce Type: replace  Abstract: Ranking is a ubiquitous method for focusing the attention of human evaluators on a manageable subset of options. Its use as part of human decision-making processes ranges from surfacing potentially relevant products on an e-commerce site to prioritizing college applications for human review. While ranking can make human evaluation more effective by focusing attention on the most promising options, we argue that it can introduce unfairness if the uncertainty of the underlying relevance model differs between groups of options. Unfortunately, such disparity in uncertainty appears widespread, often to the detriment of minority groups for which relevance estimates can have higher uncertainty due to a lack of data or appropriate features. To address this fairness issue, we propose Equal-Opportunity Ranking (EOR) as a new fairness criterion for ranking and show that it corresponds to a group-wise fair lottery among the relevant options even
    
[^29]: ChatQA: 构建GPT-4级对话问答模型

    ChatQA: Building GPT-4 Level Conversational QA Models. (arXiv:2401.10225v1 [cs.CL])

    [http://arxiv.org/abs/2401.10225](http://arxiv.org/abs/2401.10225)

    ChatQA是一系列对话问答模型，可以达到GPT-4级别的准确性。通过两阶段的指令调整方法，可以显著提高大型语言模型在零-shot对话问答中的结果。使用密集检索器进行问答数据集的微调可以实现与最先进的查询重写模型相当的结果，同时降低部署成本。ChatQA-70B在10个对话问答数据集上的平均得分超过了GPT-4，且不依赖于任何来自OpenAI GPT模型的合成数据。

    

    在这项工作中，我们介绍了ChatQA，一系列具有GPT-4级别准确性的对话问答模型。具体地，我们提出了一个两阶段的指令调整方法，可以显著提高大型语言模型（LLM）在零-shot对话问答中的结果。为了处理对话问答中的检索问题，我们在多轮问答数据集上进行了密集检索器的微调，这样可以提供与使用最先进的查询重写模型相当的结果，同时大大降低部署成本。值得注意的是，我们的ChatQA-70B可以在10个对话问答数据集的平均分上超过GPT-4（54.14 vs. 53.90），而不依赖于OpenAI GPT模型的任何合成数据。

    In this work, we introduce ChatQA, a family of conversational question answering (QA) models, that obtain GPT-4 level accuracies. Specifically, we propose a two-stage instruction tuning method that can significantly improve the zero-shot conversational QA results from large language models (LLMs). To handle retrieval in conversational QA, we fine-tune a dense retriever on a multi-turn QA dataset, which provides comparable results to using the state-of-the-art query rewriting model while largely reducing deployment cost. Notably, our ChatQA-70B can outperform GPT-4 in terms of average score on 10 conversational QA datasets (54.14 vs. 53.90), without relying on any synthetic data from OpenAI GPT models.
    
[^30]: 分析深度强化学习中泛化性能的调查

    A Survey Analyzing Generalization in Deep Reinforcement Learning. (arXiv:2401.02349v1 [cs.LG])

    [http://arxiv.org/abs/2401.02349](http://arxiv.org/abs/2401.02349)

    本文调查了深度强化学习中的泛化性能。深度强化学习策略存在过拟合问题，限制了它们的鲁棒性和泛化能力。研究形式化和统一了提高泛化性和克服过拟合的不同解决方案。

    

    利用深度神经网络解决高维状态或动作空间中的问题，强化学习研究在实践中取得了重要的成功和关注。尽管深度强化学习策略目前在许多领域中正在被应用，从医疗应用到自动驾驶车辆，但关于深度强化学习策略的泛化能力仍有许多待解答的问题。在本文中，我们将概述深度强化学习策略遇到过拟合问题的根本原因，限制了它们的鲁棒性和泛化能力。此外，我们将对提高泛化性和克服状态-动作值函数中的过拟合的不同解决方案进行形式化和统一。我们相信我们的研究可以为当前深度强化学习的进展提供一个简洁系统的统一分析，并有助于构建健壮的深度神经网络策略。

    Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces. While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to self driving vehicles, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies. In this paper, we will outline the fundamental reasons why deep reinforcement learning policies encounter overfitting problems that limit their robustness and generalization capabilities. Furthermore, we will formalize and unify the diverse solution approaches to increase generalization, and overcome overfitting in state-action value functions. We believe our study can provide a compact systematic unified analysis for the current advancements in deep reinforcement learning, and help to construct robust deep neural policies 
    
[^31]: 无界损失的PAC-Bayes-Chernoff界限

    PAC-Bayes-Chernoff bounds for unbounded losses. (arXiv:2401.01148v1 [stat.ML])

    [http://arxiv.org/abs/2401.01148](http://arxiv.org/abs/2401.01148)

    这篇论文提出了一种用于无界损失的高概率PAC-Bayes参考界限，并通过优化自由参数解决了一些开放问题，并通过灵活的假设产生了新的广义界限。

    

    我们提出了一种新的用于无界损失的高概率PAC-Bayes参考界限。这个结果可以理解为Chernoff界限的PAC-Bayes版本。证明技巧依赖于通过Cramér变换对损失进行统一边界的尾部随机变量。我们强调了我们主要结果的两个应用。首先，我们证明了我们的界限解决了许多PAC-Bayes界限上的自由参数优化的开放问题。最后，我们证明了我们的方法允许在损失函数上进行灵活的假设，从而产生了广义了之前的界限，并且可以通过最小化来获得类似Gibbs的后验概率。

    We present a new high-probability PAC-Bayes oracle bound for unbounded losses. This result can be understood as a PAC-Bayes version of the Chernoff bound. The proof technique relies on uniformly bounding the tail of certain random variable based on the Cram\'er transform of the loss. We highlight two applications of our main result. First, we show that our bound solves the open problem of optimizing the free parameter on many PAC-Bayes bounds. Finally, we show that our approach allows working with flexible assumptions on the loss function, resulting in novel bounds that generalize previous ones and can be minimized to obtain Gibbs-like posteriors.
    
[^32]: 掩码硬注意力变换器和布尔RASP准确识别无星语言。

    Masked Hard-Attention Transformers and Boolean RASP Recognize Exactly the Star-Free Languages. (arXiv:2310.13897v2 [cs.FL] UPDATED)

    [http://arxiv.org/abs/2310.13897](http://arxiv.org/abs/2310.13897)

    给出了一种新的变换器编码器模型，该模型具有硬注意力和严格未来掩码，并且证明这些网络识别的语言类别正是无星语言。研究还发现，通过添加位置嵌入，这一模型可以扩展到其他研究充分的语言类别。一个关键技术是布尔RASP，通过无星语言的研究，将变换器与一阶逻辑、时态逻辑和代数自动机理论相关联。

    

    我们考虑具有硬注意力（即所有注意力都集中在一个位置上）和严格的未来掩码（即每个位置只与严格左侧的位置进行注意力交互）的变换器编码器，并证明这些网络识别的语言类别正是无星语言。添加位置嵌入将被识别的语言类别扩展到其他研究充分的类别。这些证明中的一个关键技术是布尔RASP，它是一种受限于布尔值的RASP变种。通过无星语言，我们将变换器与一阶逻辑、时态逻辑和代数自动机理论联系起来。

    We consider transformer encoders with hard attention (in which all attention is focused on exactly one position) and strict future masking (in which each position only attends to positions strictly to its left), and prove that the class of languages recognized by these networks is exactly the star-free languages. Adding position embeddings increases the class of recognized languages to other well-studied classes. A key technique in these proofs is Boolean RASP, a variant of RASP that is restricted to Boolean values. Via the star-free languages, we relate transformers to first-order logic, temporal logic, and algebraic automata theory.
    
[^33]: 何时，为什么以及多少？通过细化进行的自适应学习率调度

    When, Why and How Much? Adaptive Learning Rate Scheduling by Refinement. (arXiv:2310.07831v1 [cs.LG])

    [http://arxiv.org/abs/2310.07831](http://arxiv.org/abs/2310.07831)

    该论文介绍了一种通过细化分析学习率调度来解决实践中学习率调整与理论的不一致的方法，通过对观察到的梯度范数进行分析，得到了适应于特定任务的细化调度。该方法能够改善优化算法的收敛性能。

    

    实践中使用的学习率调度与理论推荐的几乎完全不同。我们缩小了大部分理论与实践之间的差距，并因此能够推导出新的问题自适应学习率调度。我们的关键技术贡献是对广泛类别的优化算法（包括SGD）的学习率调度进行细化分析。与大多数前期研究只研究平均迭代的收敛性不同，我们研究最后一次迭代，这是大多数人在实践中使用的。当仅考虑最坏情况分析时，我们的理论预测最佳选择是线性衰减调度：这是一种实践中常用的选择，其将步长与当前迭代次数t和总步数T成比例地设置为1 - t/T。为了超越这种最坏情况分析，我们使用观察到的梯度范数来推导适应于特定任务的细化调度。这些细化调度表现出学习率逐渐增加和学习率迅速退火。

    Learning rate schedules used in practice bear little resemblance to those recommended by theory. We close much of this theory/practice gap, and as a consequence are able to derive new problem-adaptive learning rate schedules. Our key technical contribution is a refined analysis of learning rate schedules for a wide class of optimization algorithms (including SGD). In contrast to most prior works that study the convergence of the average iterate, we study the last iterate, which is what most people use in practice. When considering only worst-case analysis, our theory predicts that the best choice is the linear decay schedule: a popular choice in practice that sets the stepsize proportionally to $1 - t/T$, where $t$ is the current iteration and $T$ is the total number of steps. To go beyond this worst-case analysis, we use the observed gradient norms to derive schedules refined for any particular task. These refined schedules exhibit learning rate warm-up and rapid learning rate anneali
    
[^34]: IMITATE: 临床先验指导的分层视觉语言预训练模型

    IMITATE: Clinical Prior Guided Hierarchical Vision-Language Pre-training. (arXiv:2310.07355v1 [cs.CV])

    [http://arxiv.org/abs/2310.07355](http://arxiv.org/abs/2310.07355)

    IMITATE是一种临床先验指导的分层视觉语言预训练模型。它利用医学报告的层级结构，从胸部X射线图像中提取多级视觉特征，并与分层医学报告中的描述性和结论性文本进行对齐。

    

    在医学视觉语言预训练（VLP）领域，人们致力于从临床报告和相关医学图像中提取文本和图像特征。然而，大多数现有的方法可能忽视了利用临床报告固有的层级结构的机会，这些报告通常被分为描述性内容的“发现”和结论性观察的“印象”。当前的医学VLP方法往往将报告简化为一个统一的实体或分散的标记，而没有利用这种丰富的、结构化的格式。在这项工作中，我们提出了一种新的临床先验指导的VLP框架，名为IMITATE，用于从医学报告中学习结构信息，并使用分层视觉语言对齐。该框架从胸部X射线（CXR）图像中提取多级视觉特征，并将这些特征与分层医学报告中的描述性和结论性文本分别对齐。

    In the field of medical Vision-Language Pre-training (VLP), significant efforts have been devoted to deriving text and image features from both clinical reports and associated medical images. However, most existing methods may have overlooked the opportunity in leveraging the inherent hierarchical structure of clinical reports, which are generally split into `findings' for descriptive content and `impressions' for conclusive observation. Instead of utilizing this rich, structured format, current medical VLP approaches often simplify the report into either a unified entity or fragmented tokens. In this work, we propose a novel clinical prior guided VLP framework named IMITATE to learn the structure information from medical reports with hierarchical vision-language alignment. The framework derives multi-level visual features from the chest X-ray (CXR) images and separately aligns these features with the descriptive and the conclusive text encoded in the hierarchical medical report. Furth
    
[^35]: 保守的世界模型

    Conservative World Models. (arXiv:2309.15178v1 [cs.LG])

    [http://arxiv.org/abs/2309.15178](http://arxiv.org/abs/2309.15178)

    在零样本强化学习中，研究人员探索了在小样本数据集上训练时，前向-后向算法性能下降的问题，并使用保守性算法来缓解此问题。实验证明，保守的前向-后向算法在总体上表现更好，甚至超过了特定任务的基准算法。

    

    零样本强化学习承诺在离线预训练阶段后，提供能够在任何环境中执行任何任务的代理。前向-后向（FB）表示在这个理想的实现上取得了显著进展，可以在这种设置中达到特定任务代理的85%的性能。然而，这样的性能取决于对于大规模且多样化的数据集的访问，而大多数真实问题无法期望这样的数据集。在这里，我们探讨了在训练集缺乏多样性的情况下FB性能如何降低，并通过保守性来减轻这种情况，这是一个成熟的离线RL算法的特点。我们在各种数据集、领域和任务上评估了我们的方法家族，在总体上达到了150%的普通FB性能。有些令人惊讶的是，保守的FB算法在没有访问奖励标签且需要维护所有任务策略的情况下，也优于特定任务基线。

    Zero-shot reinforcement learning (RL) promises to provide agents that can perform any task in an environment after an offline pre-training phase. Forward-backward (FB) representations represent remarkable progress towards this ideal, achieving 85% of the performance of task-specific agents in this setting. However, such performance is contingent on access to large and diverse datasets for pre-training, which cannot be expected for most real problems. Here, we explore how FB performance degrades when trained on small datasets that lack diversity, and mitigate it with conservatism, a well-established feature of performant offline RL algorithms. We evaluate our family of methods across various datasets, domains and tasks, reaching 150% of vanilla FB performance in aggregate. Somewhat surprisingly, conservative FB algorithms also outperform the task-specific baseline, despite lacking access to reward labels and being required to maintain policies for all tasks. Conservative FB algorithms p
    
[^36]: 使用稀疏或生成的先验解决全秩矩阵的二次系统

    Solving Quadratic Systems with Full-Rank Matrices Using Sparse or Generative Priors. (arXiv:2309.09032v1 [cs.IT])

    [http://arxiv.org/abs/2309.09032](http://arxiv.org/abs/2309.09032)

    本论文提出了一个方法，通过使用稀疏或生成的先验知识，解决了从全秩矩阵的二次系统中恢复信号的问题。其中，通过引入阈值Wirtinger流算法（TWF）来处理稀疏信号，并使用谱初始化和阈值梯度下降方法，在高维情况下实现了较小的测量数量。

    

    从具有全秩矩阵的二次系统中恢复信号x在应用中经常出现，比如未分配的距离几何和亚波长成像。本文通过引入对x的先验知识，针对高维情况（m << n），使用独立同分布的标准高斯矩阵解决了该问题。首先，考虑k-稀疏的x，引入了TWF算法，该算法不需要稀疏水平k。TWF包括两个步骤：谱初始化，当m = O(k^2log n)时，确定了一个距离x足够近的点（可能会有符号翻转），以及具有很好初始化的阈值梯度下降，该下降产生了一个线性收敛到x的序列，用m = O(klog n)个测量。

    The problem of recovering a signal $\boldsymbol{x} \in \mathbb{R}^n$ from a quadratic system $\{y_i=\boldsymbol{x}^\top\boldsymbol{A}_i\boldsymbol{x},\ i=1,\ldots,m\}$ with full-rank matrices $\boldsymbol{A}_i$ frequently arises in applications such as unassigned distance geometry and sub-wavelength imaging. With i.i.d. standard Gaussian matrices $\boldsymbol{A}_i$, this paper addresses the high-dimensional case where $m\ll n$ by incorporating prior knowledge of $\boldsymbol{x}$. First, we consider a $k$-sparse $\boldsymbol{x}$ and introduce the thresholded Wirtinger flow (TWF) algorithm that does not require the sparsity level $k$. TWF comprises two steps: the spectral initialization that identifies a point sufficiently close to $\boldsymbol{x}$ (up to a sign flip) when $m=O(k^2\log n)$, and the thresholded gradient descent (with a good initialization) that produces a sequence linearly converging to $\boldsymbol{x}$ with $m=O(k\log n)$ measurements. Second, we explore the generative p
    
[^37]: 联邦式大规模语言模型：一个立场论文

    Federated Large Language Model: A Position Paper. (arXiv:2307.08925v1 [cs.LG])

    [http://arxiv.org/abs/2307.08925](http://arxiv.org/abs/2307.08925)

    我们提出了联邦式大规模语言模型的概念，通过联邦学习实现分散数据的共同训练共享模型，以应对公共数据可用性的限制和私有数据的隐私保护需求。我们讨论了预训练、微调和提示工程这三个组件的优势，并提出了实施策略。同时，我们探讨了FL和LLM集成带来的新挑战，并分析了现有解决方案和潜在障碍。

    

    大规模语言模型（LLM）在各个领域获得了相当大的关注并找到了多样化的应用，但在真实场景中开发时面临挑战。这些挑战源于公共领域数据可用性的匮乏以及对私有领域数据的隐私保护需求。为了解决这些问题，联邦学习（FL）作为一项有前景的技术出现了，它能够在保持分散数据的同时实现共同训练共享模型。我们提出了联邦式LLM的概念，包括三个关键组成部分，即联邦式LLM预训练、联邦式LLM微调和联邦式LLM提示工程。对于每个组件，我们讨论了它相对于传统LLM训练方法的优势，并提出了具体的工程策略来实施。此外，我们探讨了FL和LLM集成带来的新挑战。我们分析现有的解决方案并确定可能的障碍

    Large scale language models (LLM) have received significant attention and found diverse applications across various domains, but their development encounters challenges in real-world scenarios. These challenges arise due to the scarcity of public domain data availability and the need to maintain privacy with respect to private domain data. To address these issues, federated learning (FL) has emerged as a promising technology that enables collaborative training of shared models while preserving decentralized data. We propose the concept of federated LLM, which comprises three key components, i.e., federated LLM pre-training, federated LLM fine-tuning, and federated LLM prompt engineering. For each component, we discuss its advantage over traditional LLM training methods and propose specific engineering strategies for implementation. Furthermore, we explore the novel challenges introduced by the integration of FL and LLM. We analyze existing solutions and identify potential obstacles fac
    
[^38]: 集合化的神经网络编码

    Set-based Neural Network Encoding. (arXiv:2305.16625v1 [cs.LG])

    [http://arxiv.org/abs/2305.16625](http://arxiv.org/abs/2305.16625)

    提出了一种能够集合化地编码神经网络参数的神经网络权重编码方法，并引入了一种逐层编码方案来考虑神经网络的分层计算结构。同时引入了“pad-chunk-encode”流水线进行神经网络层的高效编码处理，还提出了新的神经网络泛化性能预测任务。

    

    我们提出了一种利用集合到集合和集合到向量函数来有效编码神经网络参数，进行泛化性能预测的神经网络权重编码方法。与之前需要对不同架构编写自定义编码模型的方法不同，我们的方法能够对混合架构和不同参数大小的模型动态编码。此外，我们的 SNE（集合化神经网络编码器）通过使用一种逐层编码方案，考虑神经网络的分层计算结构。最终将所有层次编码合并到一起，以获取神经网络编码矢量。我们还引入了“pad-chunk-encode”流水线来有效地编码神经网络层，该流水线可根据计算和内存限制进行调整。我们还引入了两个用于神经网络泛化性能预测的新任务：跨数据集和架构适应性预测。

    We propose an approach to neural network weight encoding for generalization performance prediction that utilizes set-to-set and set-to-vector functions to efficiently encode neural network parameters. Our approach is capable of encoding neural networks in a modelzoo of mixed architecture and different parameter sizes as opposed to previous approaches that require custom encoding models for different architectures. Furthermore, our \textbf{S}et-based \textbf{N}eural network \textbf{E}ncoder (SNE) takes into consideration the hierarchical computational structure of neural networks by utilizing a layer-wise encoding scheme that culminates to encoding all layer-wise encodings to obtain the neural network encoding vector. Additionally, we introduce a \textit{pad-chunk-encode} pipeline to efficiently encode neural network layers that is adjustable to computational and memory constraints. We also introduce two new tasks for neural network generalization performance prediction: cross-dataset a
    
[^39]: 不精确标签学习：学习各种不精确标签配置的统一框架

    Imprecise Label Learning: A Unified Framework for Learning with Various Imprecise Label Configurations. (arXiv:2305.12715v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.12715](http://arxiv.org/abs/2305.12715)

    本文提出了不精确标签学习（ILL）框架，利用期望最大化算法对不精确标签信息进行最大似然估计，为各种不精确标签配置问题提供了统一的解决方案。

    

    本文介绍了不精确标签学习（ILL）框架，这是一种处理机器学习任务中普遍存在的各种不精确标签配置的统一方法。ILL利用期望最大化（EM）算法对不精确标签信息进行最大似然估计（MLE），将精确标签视为潜在变量。与以前试图从不精确标签信息中推断正确标签的多功能方法相比，我们的ILL框架考虑了不精确标签信息强加的所有可能标签，允许对任何不精确标签的统一解决方案。通过全面的实验结果，我们展示了ILL可以无缝地适应各种情况，包括部分标签学习、半监督学习、噪声标签学习以及这些配置的混合。值得注意的是，我们的简单方法超过了现有的处理不精确标签的技术，标志着第一个统一解决这个问题的方法。

    In this paper, we introduce the imprecise label learning (ILL) framework, a unified approach to handle various imprecise label configurations, which are commonplace challenges in machine learning tasks. ILL leverages an expectation-maximization (EM) algorithm for the maximum likelihood estimation (MLE) of the imprecise label information, treating the precise labels as latent variables. Compared to previous versatile methods attempting to infer correct labels from the imprecise label information, our ILL framework considers all possible labeling imposed by the imprecise label information, allowing a unified solution to deal with any imprecise labels. With comprehensive experimental results, we demonstrate that ILL can seamlessly adapt to various situations, including partial label learning, semi-supervised learning, noisy label learning, and a mixture of these settings. Notably, our simple method surpasses the existing techniques for handling imprecise labels, marking the first unified 
    
[^40]: 超越负采样的高效分布式表示方法

    Efficient distributed representations beyond negative sampling. (arXiv:2303.17475v1 [cs.LG])

    [http://arxiv.org/abs/2303.17475](http://arxiv.org/abs/2303.17475)

    本文介绍了一种高效的分布式表示（嵌入）学习方法，通过线性时间估计softmax归一化常数来实现学习过程，该方法优于负采样方法并在多项测试中验证了其有效性。

    

    本文介绍了一种高效的学习分布式表示（也称为嵌入）的方法。该方法通过最小化一个类似于Word2Vec算法中引入并在多个工作中采用的目标函数来实现。优化计算的瓶颈是softmax归一化常数的计算，这需要与样本大小呈二次比例的操作数。这种复杂度不适用于大型数据集，所以负采样是一个常见的解决方法，可以在与样本大小线性相关的时间内获得分布式表示。然而，负采样会改变损失函数，因此解决的是与最初提出的不同的优化问题。我们的贡献在于展示如何通过线性时间估计softmax归一化常数，从而设计了一种有效的优化策略来学习分布式表示。我们使用不同的数据集进行测试，并展示了我们的方法在嵌入质量和训练时间方面优于负采样。

    This article describes an efficient method to learn distributed representations, also known as embeddings. This is accomplished minimizing an objective function similar to the one introduced in the Word2Vec algorithm and later adopted in several works. The optimization computational bottleneck is the calculation of the softmax normalization constants for which a number of operations scaling quadratically with the sample size is required. This complexity is unsuited for large datasets and negative sampling is a popular workaround, allowing one to obtain distributed representations in linear time with respect to the sample size. Negative sampling consists, however, in a change of the loss function and hence solves a different optimization problem from the one originally proposed. Our contribution is to show that the sotfmax normalization constants can be estimated in linear time, allowing us to design an efficient optimization strategy to learn distributed representations. We test our ap
    

