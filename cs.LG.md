# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Accelerated Parameter-Free Stochastic Optimization](https://arxiv.org/abs/2404.00666) | 提出了一种加速的无参数随机优化方法，不需要先验了解问题参数，在平滑随机凸优化的情况下实现了近乎最佳收敛速率并表现强大。 |
| [^2] | [Multimodal Variational Autoencoder for Low-cost Cardiac Hemodynamics Instability Detection](https://arxiv.org/abs/2403.13658) | 提出了一种新颖的多模态变分自编码器（$\text{CardioVAE}_\text{X,G}$），将低成本胸部X射线（CXR）和心电图（ECG）数据形式整合起来，并实现了共享特征和独特特征的学习。 |
| [^3] | [Hierarchical Gaussian Mixture Normalizing Flow Modeling for Unified Anomaly Detection](https://arxiv.org/abs/2403.13349) | 提出了一种用于统一异常检测的Hierarchical Gaussian mixture normalizing flow (HGAD)建模方法，通过分层高斯混合建模来提升异常检测模型的表示能力 |
| [^4] | [From Representational Harms to Quality-of-Service Harms: A Case Study on Llama 2 Safety Safeguards](https://arxiv.org/abs/2403.13213) | 本文探讨了针对表现性伤害和服务质量伤害的羊驼2安全保障措施的有效性，并指出了大型语言模型在实用性和安全性之间的权衡关系。 |
| [^5] | [DexCap: Scalable and Portable Mocap Data Collection System for Dexterous Manipulation](https://arxiv.org/abs/2403.07788) | DexCap是一个可移植的手部动作捕捉系统，结合DexIL算法从人类手部运动数据中训练机器人技能，具有精确追踪和复制人类动作的能力。 |
| [^6] | [A Question-centric Multi-experts Contrastive Learning Framework for Improving the Accuracy and Interpretability of Deep Sequential Knowledge Tracing Models](https://arxiv.org/abs/2403.07322) | 通过一个问题中心的多专家对比学习框架，提高深度序列知识追踪模型的准确性和可解释性，解决了知识追踪中个体问题信息建模和模型预测结果解释的重要挑战 |
| [^7] | [Improving Low-Resource Knowledge Tracing Tasks by Supervised Pre-training and Importance Mechanism Fine-tuning](https://arxiv.org/abs/2403.06725) | 本文提出了名为LoReKT的低资源知识追踪框架，通过监督预训练和微调重要性机制，旨在从丰富资源的KT数据集中学习可转移的参数和表示来改进低资源知识追踪任务。 |
| [^8] | [Blockchain-empowered Federated Learning: Benefits, Challenges, and Solutions](https://arxiv.org/abs/2403.00873) | 区块链技术被整合到联邦学习系统中以提供更强的安全性、公平性和可扩展性，但也引入了额外的网络、计算和存储资源需求。 |
| [^9] | [Reinforced In-Context Black-Box Optimization](https://arxiv.org/abs/2402.17423) | 提出了一种从离线数据中端到端地强化学习黑盒优化算法的方法，通过使用表达能力强的序列模型和后悔-前进令牌来获取任务信息并做出决策。 |
| [^10] | [A Quantum Approach to Synthetic Minority Oversampling Technique (SMOTE)](https://arxiv.org/abs/2402.17398) | 使用量子计算技术提出了Quantum-SMOTE方法，可以解决机器学习数据集中的类别不平衡问题，并引入了旋转角度、少数类百分比和分裂因子等超参数，实现了对合成数据生成过程的更好控制。 |
| [^11] | [Low-Rank Bandits via Tight Two-to-Infinity Singular Subspace Recovery](https://arxiv.org/abs/2402.15739) | 该论文介绍了一种解决低秩环境中具有上下文信息的赌徒问题的高效算法，其中包括策略评估、最佳策略识别和遗憾最小化，并且在最佳策略识别和策略评估方面的算法几乎是极小极大最优的。 |
| [^12] | [FAIR: Filtering of Automatically Induced Rules](https://arxiv.org/abs/2402.15472) | 该论文提出了一种通过次模块目标函数过滤大量自动诱导规则的算法 FAIR，以解决机器学习算法训练中缺乏大量已注释数据的问题。 |
| [^13] | [Defending Jailbreak Prompts via In-Context Adversarial Game](https://arxiv.org/abs/2402.13148) | 介绍了一种通过上下文对抗游戏(ICAG)防御越狱提示的方法，能够显著降低成功率。 |
| [^14] | [Revitalizing Multivariate Time Series Forecasting: Learnable Decomposition with Inter-Series Dependencies and Intra-Series Variations Modeling](https://arxiv.org/abs/2402.12694) | 引入可学习分解策略和双注意力模块，同时捕捉跨系列依赖和内部变化，以应对复杂的多变量时间序列预测挑战。 |
| [^15] | [LoRA+: Efficient Low Rank Adaptation of Large Models](https://arxiv.org/abs/2402.12354) | LoRA+通过设置不同的学习率来改进原始LoRA的低效率问题，在保持计算成本不变的情况下提高了模型性能和微调速度。 |
| [^16] | [Remember This Event That Year? Assessing Temporal Information and Reasoning in Large Language Models](https://arxiv.org/abs/2402.11997) | 大型语言模型在处理时间信息和推理方面存在显著限制，闭源模型可能暗示了不确定性认识与错误回应之间的权衡。 |
| [^17] | [U$^2$MRPD: Unsupervised undersampled MRI reconstruction by prompting a large latent diffusion model](https://arxiv.org/abs/2402.10609) | U$^2$MRPD是一个新颖的框架，通过大型潜在扩散模型引导，实现了无监督的欠采样MRI重建，能够支持图像特定的MRI重建，且在多个数据集上表现出与监督和MRI扩散方法相媲美甚至更好的性能。 |
| [^18] | [When Representations Align: Universality in Representation Learning Dynamics](https://arxiv.org/abs/2402.09142) | 本研究提出了一个有效的表示学习理论，该理论假设了编码映射和解码映射为任意光滑函数，并且能够描述复杂且大型架构中的表示学习动力学。 |
| [^19] | [Sampling from the Mean-Field Stationary Distribution](https://arxiv.org/abs/2402.07355) | 本文研究了从均场随机微分方程 (SDE) 的稳态分布中采样的复杂性，并提出了一种解耦的方法。该方法能够在多种情况下提供改进的保证，包括在均场区域优化某些双层神经网络的更好保证。 |
| [^20] | [Adaptive proximal gradient methods are universal without approximation](https://arxiv.org/abs/2402.06271) | 该论文证明了适应性近端梯度方法对于凸问题不受传统假设的限制，并且可以在局部梯度H\"older连续性条件下收敛，同时避免了线搜索步骤和近似的使用。对局部H\"older常数和H\"older连续性顺序的先验知识也不是必需的。在数值实验中，与基准方法进行了对比实验，涵盖了局部和全局 H\"older 设置。 |
| [^21] | [On Calibration and Conformal Prediction of Deep Classifiers](https://arxiv.org/abs/2402.05806) | 本文研究了温度缩放对符合预测方法的影响，通过实证研究发现，校准对自适应C方法产生了有害的影响。 |
| [^22] | [Deep Reinforcement Learning with Dynamic Graphs for Adaptive Informative Path Planning](https://arxiv.org/abs/2402.04894) | 提出了一种利用动态图的深度强化学习方法，用于自适应信息路径规划，能够在未知的三维环境中映射出感兴趣的目标。 |
| [^23] | [Hierarchical Tree-structured Knowledge Graph For Academic Insight Survey](https://arxiv.org/abs/2402.04854) | 该论文提出了一种分层树状知识图谱和推荐系统，帮助初学者研究者进行研究调研，填补了现有导航知识图谱的不足，并解决了学术论文推荐系统中高文本相似性带来的困惑。 |
| [^24] | [Guidance with Spherical Gaussian Constraint for Conditional Diffusion](https://arxiv.org/abs/2402.03201) | 本文提出了一种用球面高斯约束的扩散算法（DSG），解决了在条件生成任务中采样过程中的流形偏离问题。这种算法通过优化将步骤限制在中间数据流形内，并能够使用较大的引导步长。 |
| [^25] | [DexDiffuser: Generating Dexterous Grasps with Diffusion Models](https://arxiv.org/abs/2402.02989) | DexDiffuser是一种使用扩散模型生成灵巧抓取姿势的新方法，通过对物体点云的生成、评估和优化，实现了较高的抓取成功率。 |
| [^26] | [Mixed Noise and Posterior Estimation with Conditional DeepGEM](https://arxiv.org/abs/2402.02964) | 本论文提出了一种用于联合估计贝叶斯逆问题中后验概率和噪声参数的新算法，该算法通过期望最大化（EM）算法解决问题，并使用条件标准化流来近似后验概率。该模型能够整合来自多个测量的信息。 |
| [^27] | [Sets are all you need: Ultrafast jet classification on FPGAs for HL-LHC](https://arxiv.org/abs/2402.01876) | 该论文研究了在FPGA上进行高性能喷注分类的机器学习算法，通过量化感知的训练和高效的硬件实现，实现了低延迟和低资源消耗，为高亮度阶段标记的模型提供了初始设计。 |
| [^28] | [Language-Guided World Models: A Model-Based Approach to AI Control](https://arxiv.org/abs/2402.01695) | 语言引导的世界模型（LWMs）是一种基于模型的人工智能控制方法，它通过阅读语言描述来捕捉环境动态，提高了代理的沟通效率，并允许人类通过简洁的语言反馈同时改变他们在多个任务上的行为。 |
| [^29] | [KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization](https://arxiv.org/abs/2401.18079) | KVQuant是一种解决LLM推理中大量内存消耗的KV缓存量化方法，通过引入新颖的量化方法，包括分通道键量化、RoPE前量化键和非均匀KV缓存量化，准确表示超低精度的KV激活。 |
| [^30] | [Steering Llama 2 via Contrastive Activation Addition](https://arxiv.org/abs/2312.06681) | 引入Contrastive Activation Addition（CAA）方法，通过修改语言模型的激活来精确控制目标行为的程度，显著改变模型行为并在微调和系统提示设计的基础上提供额外有效性。 |
| [^31] | [MT-HCCAR: Multi-Task Deep Learning with Hierarchical Classification and Attention-based Regression for Cloud Property Retrieval.](http://arxiv.org/abs/2401.16520) | 这篇论文提出了一种名为MT-HCCAR的多任务深度学习模型，用于云属性检索。该模型考虑了云属性检索任务之间的层级关系，并具有对不同传感器数据集具有健壮泛化能力的特点。 |
| [^32] | [Beyond RMSE and MAE: Introducing EAUC to unmask hidden bias and unfairness in dyadic regression models.](http://arxiv.org/abs/2401.10690) | 本研究引入EAUC作为一种新的度量标准，用以揭示迪亚德回归模型中隐藏的偏见和不公平问题。传统的全局错误度量标准如RMSE和MAE无法捕捉到这种问题。 |
| [^33] | [Understanding Biases in ChatGPT-based Recommender Systems: Provider Fairness, Temporal Stability, and Recency.](http://arxiv.org/abs/2401.10545) | 该研究揭示了基于ChatGPT的推荐系统的偏见问题，并研究了提示设计策略对推荐质量的影响。实验结果显示，在RecLLMs中引入特定的系统角色和提示策略可以增强推荐的公平性和多样性，同时GPT-based模型倾向于推荐最新和更多样化的电影流派。 |
| [^34] | [Artwork Protection Against Neural Style Transfer Using Locally Adaptive Adversarial Color Attack.](http://arxiv.org/abs/2401.09673) | 本文提出了一种名为本地自适应对抗颜色攻击（LAACA）的方法，用于保护艺术品免受神经风格转换（NST）的滥用。该方法通过在不可察觉的情况下对图像进行修改，产生对NST具有干扰作用的扰动。 |
| [^35] | [A Survey on Statistical Theory of Deep Learning: Approximation, Training Dynamics, and Generative Models.](http://arxiv.org/abs/2401.07187) | 该论文综述了深度学习的统计理论，包括近似方法、训练动态和生成模型。在非参数框架中，结果揭示了神经网络过度风险的快速收敛速率，以及如何通过梯度方法训练网络以找到良好的泛化解决方案。 |
| [^36] | [A Semantic-Aware Multiple Access Scheme for Distributed, Dynamic 6G-Based Applications.](http://arxiv.org/abs/2401.06308) | 本文提出了一种语义感知多址访问方案，旨在优化资源利用与公平性的权衡，并考虑用户数据的相关性，以满足未来6G应用的要求和特性。 |
| [^37] | [Monotone Generative Modeling via a Gromov-Monge Embedding.](http://arxiv.org/abs/2311.01375) | 该论文提出了一种使用Gromov-Monge嵌入的深度生成模型，通过识别数据背后的底层结构，并将其映射到低维潜空间中，解决了生成对抗网络（GAN）中对初始条件敏感性和模式崩溃的问题。 |
| [^38] | [Convergence of flow-based generative models via proximal gradient descent in Wasserstein space.](http://arxiv.org/abs/2310.17582) | 本文通过在Wasserstein空间中应用近端梯度下降，证明了基于流的生成模型的收敛性，并提供了生成数据分布的理论保证。 |
| [^39] | [Improved Operator Learning by Orthogonal Attention.](http://arxiv.org/abs/2310.12487) | 本研究提出了一种基于正交注意力的神经运算符，通过核积分算子的特征分解和神经近似的特征函数，来解决现有方法在有限训练数据上过拟合的问题。实验证明，该方法在六个标准神经运算符数据集上的表现优于其他基线模型。 |
| [^40] | [Jailbreaking Black Box Large Language Models in Twenty Queries.](http://arxiv.org/abs/2310.08419) | 这项研究提出了一个名为PAIR的算法，可以在只能黑盒访问大语言模型的情况下生成破解，无需人工干预。实证表明，PAIR通常只需要少于二十个查询来生成破解，是现有算法的数个数量级更高效。 |
| [^41] | [What's the Magic Word? A Control Theory of LLM Prompting.](http://arxiv.org/abs/2310.04444) | 本论文将提示工程形式化为LLM上的最优控制问题，研究了给定token序列时是否存在一种最优提示能够准确预测最终的token，并提出了控制理论中的指标来描述LLM的可操纵性。 |
| [^42] | [FENDA-FL: Personalized Federated Learning on Heterogeneous Clinical Datasets.](http://arxiv.org/abs/2309.16825) | 该论文提出了一种针对异构临床数据的个性化联邦学习方法，实验证明该方法在性能上超越了现有的全局和个性化联邦学习技术，并且对FLamby基准进行了实质性改进和扩展。 |
| [^43] | [Systematic Sampling and Validation of Machine Learning-Parameterizations in Climate Models.](http://arxiv.org/abs/2309.16177) | 本论文研究了混合物理-机器学习气候模拟的挑战，并通过大规模在线建模错误采样和评估，在机器学习参数化设计中发现了改进性能的策略。 |
| [^44] | [FUTURE-AI: International consensus guideline for trustworthy and deployable artificial intelligence in healthcare.](http://arxiv.org/abs/2309.12325) | FUTURE-AI是第一个国际共识框架，为医疗保健领域的可信AI工具开发和部署提供指导原则和最佳实践。 |
| [^45] | [Which algorithm to select in sports timetabling?.](http://arxiv.org/abs/2309.03229) | 本研究通过实例空间分析体育赛程安排问题的特征，提出了一个基于机器学习的算法选择系统，预测了在给定特征下最适合的算法，同时深入了解了算法性能并提出了改进建议。 |
| [^46] | [Implicit regularization of deep residual networks towards neural ODEs.](http://arxiv.org/abs/2309.01213) | 本文建立了深度残差网络向神经常微分方程的隐式正则化，通过对用梯度流训练的非线性网络的研究，证明了在网络以神经常微分方程的离散化形式初始化后，这种离散化将在整个训练过程中保持不变，并提供了收敛性的条件。 |
| [^47] | [What can we learn from quantum convolutional neural networks?.](http://arxiv.org/abs/2308.16664) | 通过分析量子卷积神经网络（QCNNs），我们发现它们通过隐藏特征映射嵌入物理系统参数，并且利用量子临界性生成适合的基函数集，池化层选择能够形成高性能决策边界的基函数，而模型的泛化性能依赖于嵌入类型。 |
| [^48] | [Entropy-based Guidance of Deep Neural Networks for Accelerated Convergence and Improved Performance.](http://arxiv.org/abs/2308.14938) | 本研究通过引入基于熵的损失项，通过测量神经网络处理数据时的熵变化，指导神经网络以更快速的收敛、更好的性能学习丰富的潜在数据表示。 |
| [^49] | [Gotta match 'em all: Solution diversification in graph matching matched filters.](http://arxiv.org/abs/2308.13451) | 本文提出了一种在大规模背景图中查找多个嵌入的模板图的新方法，通过迭代惩罚相似度矩阵来实现多样化匹配的发现，并提出了算法加速措施。在理论验证和实验证明中，证明了该方法的可行性和实用性。 |
| [^50] | [Logistics Hub Location Optimization: A K-Means and P-Median Model Hybrid Approach Using Road Network Distances.](http://arxiv.org/abs/2308.11038) | 本研究基于K-Means和P-Median模型提出了一种混合方法，通过使用道路网络距离来优化在城市环境下物流集散地的位置布置，以减少配送距离和碳足迹。 |
| [^51] | [Temporal Interest Network for Click-Through Rate Prediction.](http://arxiv.org/abs/2308.08487) | 本文提出了时间兴趣网络（TIN），用于捕捉行为与目标之间的四重语义和时间相关性，以预测点击率的效果和已有方法对这种相关性的学习程度尚不清楚。 |
| [^52] | [It Ain't That Bad: Understanding the Mysterious Performance Drop in OOD Generalization for Generative Transformer Models.](http://arxiv.org/abs/2308.08268) | 生成变换模型在OOD泛化方面存在神秘的性能下降。研究人员观察到模型在训练和泛化数字运算时的行为之间的不一致，并尝试提出解决方案，但仍未解决本质机制。 |
| [^53] | [Smooth Lower Bounds for Differentially Private Algorithms via Padding-and-Permuting Fingerprinting Codes.](http://arxiv.org/abs/2307.07604) | 本论文提出了一种通过填充和置换指纹编码的方法来产生困难实例，从而在各种情景下提供平滑下界。这方法适用于差分隐私平均问题和近似k. |
| [^54] | [Graph Neural Network-enabled Terahertz-based Flow-guided Nanoscale Localization.](http://arxiv.org/abs/2307.05551) | 本研究提出了一种基于图神经网络的太赫兹流导向纳米尺度定位方法，可以提高定位精度和覆盖范围，解决了现有方法的定位精度低和无法全局定位的问题。 |
| [^55] | [Engression: Extrapolation for Nonlinear Regression?.](http://arxiv.org/abs/2307.00835) | Engression是一种非线性回归方法，通过使用分布回归技术和预加性噪声模型，在训练样本范围边界外也能可靠地进行外推。 |
| [^56] | [Bayesian Optimization with Formal Safety Guarantees via Online Conformal Prediction.](http://arxiv.org/abs/2306.17815) | 本文提出了一种基于贝叶斯优化的方法，无论约束函数的特性如何，都能满足安全要求。 |
| [^57] | [Deep Learning-Based Spatiotemporal Multi-Event Reconstruction for Delay Line Detectors.](http://arxiv.org/abs/2306.09359) | 本文提出了基于深度学习的延迟线探测器时空多事件重构方法，可以成功地重构多个粒子的空间和时间坐标，相比传统方法更加高效和准确。 |
| [^58] | [Out-of-distribution forgetting: vulnerability of continual learning to intra-class distribution shift.](http://arxiv.org/abs/2306.00427) | 连续学习中存在一种特殊形式的灾难性遗忘——越界遗忘，当给定类别引入类内分布转移时，它会显着削弱该类别的连续学习方法的识别准确率。 |
| [^59] | [FedMR: Federated Learning via Model Recombination.](http://arxiv.org/abs/2305.10730) | FedMR是一种基于模型重组的联邦学习范式，可以通过混洗和重组每个层的本地模型来缓解子优或有偏局部模型的问题，从而在推理性能方面表现出色。 |
| [^60] | [Certifiable Black-Box Attack: Ensuring Provably Successful Attack for Adversarial Examples.](http://arxiv.org/abs/2304.04343) | 本文提出可证黑盒攻击，能够保证攻击成功率，设计了多种新技术。 |
| [^61] | [FakET: Simulating Cryo-Electron Tomograms with Neural Style Transfer.](http://arxiv.org/abs/2304.02011) | 本文提出了一种使用加性噪声和神经风格迁移技术来模拟电子显微镜正向算子，以解决深度学习方法需要大量训练数据集的问题。该方法在粒子定位和分类任务上表现良好。 |
| [^62] | [Modeling Relational Patterns for Logical Query Answering over Knowledge Graphs.](http://arxiv.org/abs/2303.11858) | 本文介绍了一种新的查询嵌入方法RoConE，它允许学习关系模式并提高了逻辑查询推理的性能。 |
| [^63] | [Convergence of stochastic gradient descent on parameterized sphere with applications to variational Monte Carlo simulation.](http://arxiv.org/abs/2303.11602) | 本论文在高维度球体上，通过神经网络参数化，使用SGD算法在监督学习和无监督学习中，提供了一种新算法，并且证明了其收敛性。 |
| [^64] | [Solving Differential-Algebraic Equations in Power Systems Dynamics with Neural Networks and Spatial Decomposition.](http://arxiv.org/abs/2303.10256) | 本文提出了一种使用神经网络和空间分解来近似电力系统动力学微分代数方程的方法，旨在加速仿真，提高数值稳定性和精度。 |
| [^65] | [Distributed Black-box Attack against Image Classification Cloud Services.](http://arxiv.org/abs/2210.16371) | 本文研究了分布式黑盒攻击云服务的图像分类器，通过直接应用于云API而不是本地模型，避免了之前研究中的错误，并利用负载平衡实现了攻击时间的减少。 |

# 详细

[^1]: 加速的无参数随机优化

    Accelerated Parameter-Free Stochastic Optimization

    [https://arxiv.org/abs/2404.00666](https://arxiv.org/abs/2404.00666)

    提出了一种加速的无参数随机优化方法，不需要先验了解问题参数，在平滑随机凸优化的情况下实现了近乎最佳收敛速率并表现强大。

    

    我们提出了一种方法，该方法实现了对平滑随机凸优化的近乎最佳收敛速率，并且基本上不需要先验了解问题参数。这改进了之前的工作，之前的工作需要至少知道到最优解的初始距离 d0。我们的方法 U-DoG 将 UniXGrad (Kavis 等人，2019) 和 DoG (Ivgi 等人，2023) 与新颖的迭代稳定技术相结合。它仅需要对 d0 和噪声幅度有松散的界限，在次高斯噪声下提供高概率保证，并且在非平滑情况下也接近最佳。我们的实验表明，在凸问题上能够稳定地表现强大，在神经网络训练上有着不同的成效。

    arXiv:2404.00666v1 Announce Type: new  Abstract: We propose a method that achieves near-optimal rates for smooth stochastic convex optimization and requires essentially no prior knowledge of problem parameters. This improves on prior work which requires knowing at least the initial distance to optimality d0. Our method, U-DoG, combines UniXGrad (Kavis et al., 2019) and DoG (Ivgi et al., 2023) with novel iterate stabilization techniques. It requires only loose bounds on d0 and the noise magnitude, provides high probability guarantees under sub-Gaussian noise, and is also near-optimal in the non-smooth case. Our experiments show consistent, strong performance on convex problems and mixed results on neural network training.
    
[^2]: 用于低成本心脏血液动力学不稳定性检测的多模态变分自编码器

    Multimodal Variational Autoencoder for Low-cost Cardiac Hemodynamics Instability Detection

    [https://arxiv.org/abs/2403.13658](https://arxiv.org/abs/2403.13658)

    提出了一种新颖的多模态变分自编码器（$\text{CardioVAE}_\text{X,G}$），将低成本胸部X射线（CXR）和心电图（ECG）数据形式整合起来，并实现了共享特征和独特特征的学习。

    

    最近在非侵入性检测心脏血液动力学不稳定性（CHDI）方面取得了进展，主要集中在将机器学习技术应用于单一数据形式，如心脏磁共振成像（MRI）。尽管这些方法具有潜力，但在标记的患者数据量有限时，这些方法通常效果不佳，这是医学领域的常见挑战。此外，只有少数研究探讨了多模态方法来研究CHDI，这些方法主要依赖昂贵的数据形式，如心脏MRI和心脏超声图。为了应对这些限制，我们提出了一种新颖的多模态变分自编码器（$\text{CardioVAE}_\text{X,G}$）来整合低成本胸部X射线（CXR）和心电图（ECG）数据形式，并在大型未标记数据集上进行预训练。具体来说，$\text{CardioVAE}_\text{X,G}$引入了一种新颖的三流预训练策略，以学习共享特征和各数据形式独有的特征，从而实现了fi

    arXiv:2403.13658v1 Announce Type: new  Abstract: Recent advancements in non-invasive detection of cardiac hemodynamic instability (CHDI) primarily focus on applying machine learning techniques to a single data modality, e.g. cardiac magnetic resonance imaging (MRI). Despite their potential, these approaches often fall short especially when the size of labeled patient data is limited, a common challenge in the medical domain. Furthermore, only a few studies have explored multimodal methods to study CHDI, which mostly rely on costly modalities such as cardiac MRI and echocardiogram. In response to these limitations, we propose a novel multimodal variational autoencoder ($\text{CardioVAE}_\text{X,G}$) to integrate low-cost chest X-ray (CXR) and electrocardiogram (ECG) modalities with pre-training on a large unlabeled dataset. Specifically, $\text{CardioVAE}_\text{X,G}$ introduces a novel tri-stream pre-training strategy to learn both shared and modality-specific features, thus enabling fi
    
[^3]: 分层高斯混合正规化流建模用于统一异常检测

    Hierarchical Gaussian Mixture Normalizing Flow Modeling for Unified Anomaly Detection

    [https://arxiv.org/abs/2403.13349](https://arxiv.org/abs/2403.13349)

    提出了一种用于统一异常检测的Hierarchical Gaussian mixture normalizing flow (HGAD)建模方法，通过分层高斯混合建模来提升异常检测模型的表示能力

    

    统一异常检测是异常检测中最具挑战性的任务之一，其中一个统一模型使用来自多个类别的正常样本进行训练，其目标是检测这些类别中的异常。本文提出了一种新颖的分层高斯混合正规化流建模方法，命名为HGAD，用于完成统一异常检测。我们的HGAD包含两个关键组件：跨类别高斯混合建模和类内混合类中心学习。与先前基于NF的AD方法相比，分层高斯混合建模方法可以为异常检测模型带来更强大的表示能力。

    arXiv:2403.13349v1 Announce Type: new  Abstract: Unified anomaly detection (AD) is one of the most challenges for anomaly detection, where one unified model is trained with normal samples from multiple classes with the objective to detect anomalies in these classes. For such a challenging task, popular normalizing flow (NF) based AD methods may fall into a "homogeneous mapping" issue,where the NF-based AD models are biased to generate similar latent representations for both normal and abnormal features, and thereby lead to a high missing rate of anomalies. In this paper, we propose a novel Hierarchical Gaussian mixture normalizing flow modeling method for accomplishing unified Anomaly Detection, which we call HGAD. Our HGAD consists of two key components: inter-class Gaussian mixture modeling and intra-class mixed class centers learning. Compared to the previous NF-based AD methods, the hierarchical Gaussian mixture modeling approach can bring stronger representation capability to the 
    
[^4]: 从表现性伤害到服务质量伤害:羊驼2安全保障的案例研究

    From Representational Harms to Quality-of-Service Harms: A Case Study on Llama 2 Safety Safeguards

    [https://arxiv.org/abs/2403.13213](https://arxiv.org/abs/2403.13213)

    本文探讨了针对表现性伤害和服务质量伤害的羊驼2安全保障措施的有效性，并指出了大型语言模型在实用性和安全性之间的权衡关系。

    

    近期大型语言模型（LLM）的进展导致它们在各个领域被广泛采用。然而，这些进步也引入了额外的安全风险，并引发了对其对已经边缘化人群的不利影响的担忧。尽管存在越来越多的减轻措施来开发安全保障措施，比如监督式的安全定向微调和利用来自人类反馈的安全强化学习，但关于这些模型的安全性和内在偏见仍存在多重关注。此外，先前的研究已经证明，为了安全而优化的模型通常会展示夸大的安全行为，比如出于预防措施而倾向于不回应某些请求。因此，文献中已经记录了这些模型在实用性和安全性之间的明显权衡。在本文中，我们进一步研究了安全措施的有效性，通过评估...

    arXiv:2403.13213v1 Announce Type: cross  Abstract: Recent progress in large language models (LLMs) has led to their widespread adoption in various domains. However, these advancements have also introduced additional safety risks and raised concerns regarding their detrimental impact on already marginalized populations. Despite growing mitigation efforts to develop safety safeguards, such as supervised safety-oriented fine-tuning and leveraging safe reinforcement learning from human feedback, multiple concerns regarding the safety and ingrained biases in these models remain. Furthermore, previous work has demonstrated that models optimized for safety often display exaggerated safety behaviors, such as a tendency to refrain from responding to certain requests as a precautionary measure. As such, a clear trade-off between the helpfulness and safety of these models has been documented in the literature. In this paper, we further investigate the effectiveness of safety measures by evaluatin
    
[^5]: DexCap：用于灵巧操作的可扩展和可移植动作捕捉数据收集系统

    DexCap: Scalable and Portable Mocap Data Collection System for Dexterous Manipulation

    [https://arxiv.org/abs/2403.07788](https://arxiv.org/abs/2403.07788)

    DexCap是一个可移植的手部动作捕捉系统，结合DexIL算法从人类手部运动数据中训练机器人技能，具有精确追踪和复制人类动作的能力。

    

    从人类手部运动数据中学习是为机器人赋予类人灵巧在现实操纵任务中的潜在途径，然而，现存手部动作捕捉系统的可移植性以及将动作捕捉数据转化为有效控制策略的困难仍然存在挑战。为了应对这些问题，我们引入了DexCap，一个便携式手部动作捕捉系统，以及DexIL，一种新颖的模仿算法，可直接从人类手部动作捕捉数据训练灵巧机器人技能。DexCap基于SLAM和电磁场以及环境的3D观察，提供了对手腕和手指运动的精确、抗遮挡的跟踪。利用这一丰富的数据集，DexIL采用逆运动学和基于点云的模仿学习来复制人类动作与机器人手。除了从人类运动中学习外，DexCap还提供了一种op

    arXiv:2403.07788v1 Announce Type: cross  Abstract: Imitation learning from human hand motion data presents a promising avenue for imbuing robots with human-like dexterity in real-world manipulation tasks. Despite this potential, substantial challenges persist, particularly with the portability of existing hand motion capture (mocap) systems and the difficulty of translating mocap data into effective control policies. To tackle these issues, we introduce DexCap, a portable hand motion capture system, alongside DexIL, a novel imitation algorithm for training dexterous robot skills directly from human hand mocap data. DexCap offers precise, occlusion-resistant tracking of wrist and finger motions based on SLAM and electromagnetic field together with 3D observations of the environment. Utilizing this rich dataset, DexIL employs inverse kinematics and point cloud-based imitation learning to replicate human actions with robot hands. Beyond learning from human motion, DexCap also offers an op
    
[^6]: 一个问题中心的多专家对比学习框架，用于提高深度序列知识追踪模型的准确性和可解释性

    A Question-centric Multi-experts Contrastive Learning Framework for Improving the Accuracy and Interpretability of Deep Sequential Knowledge Tracing Models

    [https://arxiv.org/abs/2403.07322](https://arxiv.org/abs/2403.07322)

    通过一个问题中心的多专家对比学习框架，提高深度序列知识追踪模型的准确性和可解释性，解决了知识追踪中个体问题信息建模和模型预测结果解释的重要挑战

    

    知识追踪在通过分析学生历史学习过程来预测其未来表现中发挥着至关重要的作用。深度神经网络在解决知识追踪问题方面展现出巨大潜力。然而，将深度学习技术应用于模拟知识追踪过程仍然存在一些重要挑战。第一个挑战在于将问题的个体信息融入建模中。这很关键，因为尽管问题共享相同的知识组件（KC），但学生对同质问题的知识习得可以有显著差异。第二个挑战在于解释现有基于深度学习的知识追踪模型的预测结果。在真实应用中，虽然可能并不需要完全透明和可解释的模型参数，但关键是以老师能理解的方式呈现模型的预测结果。

    arXiv:2403.07322v1 Announce Type: cross  Abstract: Knowledge tracing (KT) plays a crucial role in predicting students' future performance by analyzing their historical learning processes. Deep neural networks (DNNs) have shown great potential in solving the KT problem. However, there still exist some important challenges when applying deep learning techniques to model the KT process. The first challenge lies in taking the individual information of the question into modeling. This is crucial because, despite questions sharing the same knowledge component (KC), students' knowledge acquisition on homogeneous questions can vary significantly. The second challenge lies in interpreting the prediction results from existing deep learning-based KT models. In real-world applications, while it may not be necessary to have complete transparency and interpretability of the model parameters, it is crucial to present the model's prediction results in a manner that teachers find interpretable. This ma
    
[^7]: 通过监督预训练和重要性机制微调改进低资源知识追踪任务

    Improving Low-Resource Knowledge Tracing Tasks by Supervised Pre-training and Importance Mechanism Fine-tuning

    [https://arxiv.org/abs/2403.06725](https://arxiv.org/abs/2403.06725)

    本文提出了名为LoReKT的低资源知识追踪框架，通过监督预训练和微调重要性机制，旨在从丰富资源的KT数据集中学习可转移的参数和表示来改进低资源知识追踪任务。

    

    知识追踪（KT）旨在基于学生的历史互动来估计他们的知识掌握程度。最近，基于深度学习的KT（DLKT）方法在KT任务中取得了令人印象深刻的表现。然而，由于各种原因，如预算限制和隐私问题，许多实际场景中观察到的互动非常有限，即低资源KT数据集。直接在低资源KT数据集上训练DLKT模型可能会导致过拟合，并且很难选择适当的深度神经架构。因此，在本文中，我们提出了一个名为LoReKT的低资源KT框架来应对上述挑战。受盛行的“预训练和微调”范式的启发，我们旨在在预训练阶段从丰富资源的KT数据集中学习可转移的参数和表示。

    arXiv:2403.06725v1 Announce Type: cross  Abstract: Knowledge tracing (KT) aims to estimate student's knowledge mastery based on their historical interactions. Recently, the deep learning based KT (DLKT) approaches have achieved impressive performance in the KT task. These DLKT models heavily rely on the large number of available student interactions. However, due to various reasons such as budget constraints and privacy concerns, observed interactions are very limited in many real-world scenarios, a.k.a, low-resource KT datasets. Directly training a DLKT model on a low-resource KT dataset may lead to overfitting and it is difficult to choose the appropriate deep neural architecture. Therefore, in this paper, we propose a low-resource KT framework called LoReKT to address above challenges. Inspired by the prevalent "pre-training and fine-tuning" paradigm, we aim to learn transferable parameters and representations from rich-resource KT datasets during the pre-training stage and subseque
    
[^8]: 区块链赋能的联邦学习: 好处、挑战和解决方案

    Blockchain-empowered Federated Learning: Benefits, Challenges, and Solutions

    [https://arxiv.org/abs/2403.00873](https://arxiv.org/abs/2403.00873)

    区块链技术被整合到联邦学习系统中以提供更强的安全性、公平性和可扩展性，但也引入了额外的网络、计算和存储资源需求。

    

    联邦学习(FL)是一种分布式机器学习方法，通过在客户端本地训练模型并在参数服务器上进行聚合来保护用户数据隐私。尽管在保护隐私方面有效，但FL系统面临单点故障、缺乏激励和不足的安全性等局限性。为了应对这些挑战，将区块链技术整合到FL系统中，以提供更强的安全性、公平性和可扩展性。然而，区块链赋能的FL(BC-FL)系统对网络、计算和存储资源提出了额外的需求。本调查全面审查了最近关于BC-FL系统的研究，分析了与区块链整合相关的好处和挑战。我们探讨了区块链为何适用于FL，如何实施以及整合的挑战和现有解决方案。此外，我们还提供了关于未来研究方向的见解。

    arXiv:2403.00873v1 Announce Type: cross  Abstract: Federated learning (FL) is a distributed machine learning approach that protects user data privacy by training models locally on clients and aggregating them on a parameter server. While effective at preserving privacy, FL systems face limitations such as single points of failure, lack of incentives, and inadequate security. To address these challenges, blockchain technology is integrated into FL systems to provide stronger security, fairness, and scalability. However, blockchain-empowered FL (BC-FL) systems introduce additional demands on network, computing, and storage resources. This survey provides a comprehensive review of recent research on BC-FL systems, analyzing the benefits and challenges associated with blockchain integration. We explore why blockchain is applicable to FL, how it can be implemented, and the challenges and existing solutions for its integration. Additionally, we offer insights on future research directions fo
    
[^9]: 加强上下文黑盒优化

    Reinforced In-Context Black-Box Optimization

    [https://arxiv.org/abs/2402.17423](https://arxiv.org/abs/2402.17423)

    提出了一种从离线数据中端到端地强化学习黑盒优化算法的方法，通过使用表达能力强的序列模型和后悔-前进令牌来获取任务信息并做出决策。

    

    黑盒优化（BBO）已经在许多科学和工程领域取得成功应用。最近，人们越来越关注元学习BBO算法的特定组件，以加快优化速度并摆脱繁琐的手工启发式算法。作为扩展，从数据中学习整个算法需要专家最少的工作量，并且可以提供最大的灵活性。在本文中，我们提出了一种名为RIBBO的方法，可以以端到端的方式从离线数据中强化学习BBO算法。RIBBO利用表达能力强的序列模型来学习多个行为算法和任务产生的优化历史，利用大型模型的上下文学习能力来提取任务信息并相应地做出决策。我们方法的核心是通过增加后悔-前进令牌来增强优化历史，这些令牌旨在基于累积表现来表示算法的性能。

    arXiv:2402.17423v1 Announce Type: cross  Abstract: Black-Box Optimization (BBO) has found successful applications in many fields of science and engineering. Recently, there has been a growing interest in meta-learning particular components of BBO algorithms to speed up optimization and get rid of tedious hand-crafted heuristics. As an extension, learning the entire algorithm from data requires the least labor from experts and can provide the most flexibility. In this paper, we propose RIBBO, a method to reinforce-learn a BBO algorithm from offline data in an end-to-end fashion. RIBBO employs expressive sequence models to learn the optimization histories produced by multiple behavior algorithms and tasks, leveraging the in-context learning ability of large models to extract task information and make decisions accordingly. Central to our method is to augment the optimization histories with regret-to-go tokens, which are designed to represent the performance of an algorithm based on cumul
    
[^10]: 量子方法研究合成少数类过采样技术（SMOTE）

    A Quantum Approach to Synthetic Minority Oversampling Technique (SMOTE)

    [https://arxiv.org/abs/2402.17398](https://arxiv.org/abs/2402.17398)

    使用量子计算技术提出了Quantum-SMOTE方法，可以解决机器学习数据集中的类别不平衡问题，并引入了旋转角度、少数类百分比和分裂因子等超参数，实现了对合成数据生成过程的更好控制。

    

    这篇论文提出了Quantum-SMOTE方法，这是一种使用量子计算技术来解决机器学习数据集中普遍存在的类别不平衡问题的新颖解决方案。Quantum-SMOTE受到合成少数类过采样技术（SMOTE）的启发，利用量子过程如交换测试和量子旋转生成合成数据点。该方法与传统的SMOTE算法使用K-最近邻（KNN）和欧氏距离的方式有所不同，能够从少数类数据点生成合成实例而无需依赖于邻近性。算法通过引入旋转角度、少数类百分比和分裂因子等超参数，可以更好地控制合成数据生成过程，从而实现对特定数据集需求的定制。该方法在TelecomChurn公共数据集上进行了测试，并与两种主要的分类算法进行了评估。

    arXiv:2402.17398v1 Announce Type: cross  Abstract: The paper proposes the Quantum-SMOTE method, a novel solution that uses quantum computing techniques to solve the prevalent problem of class imbalance in machine learning datasets. Quantum-SMOTE, inspired by the Synthetic Minority Oversampling Technique (SMOTE), generates synthetic data points using quantum processes such as swap tests and quantum rotation. The process varies from the conventional SMOTE algorithm's usage of K-Nearest Neighbors (KNN) and Euclidean distances, enabling synthetic instances to be generated from minority class data points without relying on neighbor proximity. The algorithm asserts greater control over the synthetic data generation process by introducing hyperparameters such as rotation angle, minority percentage, and splitting factor, which allow for customization to specific dataset requirements. The approach is tested on a public dataset of TelecomChurn and evaluated alongside two prominent classification
    
[^11]: 低秩赌徒通过紧绝对到无穷奇异子空间恢复

    Low-Rank Bandits via Tight Two-to-Infinity Singular Subspace Recovery

    [https://arxiv.org/abs/2402.15739](https://arxiv.org/abs/2402.15739)

    该论文介绍了一种解决低秩环境中具有上下文信息的赌徒问题的高效算法，其中包括策略评估、最佳策略识别和遗憾最小化，并且在最佳策略识别和策略评估方面的算法几乎是极小极大最优的。

    

    我们研究了具有低秩结构的情境赌徒问题，在每一轮中，如果选择了(情境，动作)对$(i,j)\in [m]\times [n]$，学习者会观察一个未知低秩奖励矩阵的$(i,j)$-th入口的嘈杂样本。连续的情境以独立同分布的方式随机生成并透露给学习者。对于这样的赌徒问题，我们提出了高效的算法用于策略评估、最佳策略识别和遗憾最小化。对于策略评估和最佳策略识别，我们展示了我们的算法几乎是极小极大最优的。例如，为了以至少$1-\delta$的概率返回一个$\varepsilon$-最佳策略，通常需要的样本数大致按照${m+n\over \varepsilon^2}\log(1/\delta)$来衡量。我们的遗憾最小化算法享有的极小极大保证按照$r^{7/4}(m+n)^{3/4}\sqrt{T}$缩放，这优于现有算法。所有提出的算法包括两个阶段：

    arXiv:2402.15739v1 Announce Type: new  Abstract: We study contextual bandits with low-rank structure where, in each round, if the (context, arm) pair $(i,j)\in [m]\times [n]$ is selected, the learner observes a noisy sample of the $(i,j)$-th entry of an unknown low-rank reward matrix. Successive contexts are generated randomly in an i.i.d. manner and are revealed to the learner. For such bandits, we present efficient algorithms for policy evaluation, best policy identification and regret minimization. For policy evaluation and best policy identification, we show that our algorithms are nearly minimax optimal. For instance, the number of samples required to return an $\varepsilon$-optimal policy with probability at least $1-\delta$ typically scales as ${m+n\over \varepsilon^2}\log(1/\delta)$. Our regret minimization algorithm enjoys minimax guarantees scaling as $r^{7/4}(m+n)^{3/4}\sqrt{T}$, which improves over existing algorithms. All the proposed algorithms consist of two phases: they
    
[^12]: FAIR：自动诱导规则的过滤

    FAIR: Filtering of Automatically Induced Rules

    [https://arxiv.org/abs/2402.15472](https://arxiv.org/abs/2402.15472)

    该论文提出了一种通过次模块目标函数过滤大量自动诱导规则的算法 FAIR，以解决机器学习算法训练中缺乏大量已注释数据的问题。

    

    大量已注释数据的可用性可能是成功训练机器学习算法的关键瓶颈，特别是当应用于多样化的领域时。弱监督提供了一个有希望的替代方案，通过加速使用特定领域规则创建带标签的训练数据。然而，它要求用户编写多样化且高质量的规则集，以将标签分配给未标记数据。自动规则诱导（ARI）方法通过从少量带标签集上的特征自动创建规则并从中过滤出最终一组规则来避开这个问题。在ARI方法中，关键步骤是从大量自动创建的规则中过滤出一组高质量有用的规则子集。在本文中，我们提出一种算法（自动诱导规则的过滤），使用考虑到次模块目标函数的方法从大量自动诱导规则中过滤出规则。

    arXiv:2402.15472v1 Announce Type: new  Abstract: The availability of large annotated data can be a critical bottleneck in training machine learning algorithms successfully, especially when applied to diverse domains. Weak supervision offers a promising alternative by accelerating the creation of labeled training data using domain-specific rules. However, it requires users to write a diverse set of high-quality rules to assign labels to the unlabeled data. Automatic Rule Induction (ARI) approaches circumvent this problem by automatically creating rules from features on a small labeled set and filtering a final set of rules from them. In the ARI approach, the crucial step is to filter out a set of a high-quality useful subset of rules from the large set of automatically created rules. In this paper, we propose an algorithm (Filtering of Automatically Induced Rules) to filter rules from a large number of automatically induced rules using submodular objective functions that account for the
    
[^13]: 通过上下文对抗游戏防御越狱提示

    Defending Jailbreak Prompts via In-Context Adversarial Game

    [https://arxiv.org/abs/2402.13148](https://arxiv.org/abs/2402.13148)

    介绍了一种通过上下文对抗游戏(ICAG)防御越狱提示的方法，能够显著降低成功率。

    

    大语言模型(LLMs)展现出在不同应用领域中的显著能力。然而，对其安全性的担忧，特别是对越狱攻击的脆弱性，仍然存在。受到深度学习中对抗训练和LLM代理学习过程的启发，我们引入了上下文对抗游戏(ICAG)来防御越狱攻击，无需进行微调。ICAG利用代理学习进行对抗游戏，旨在动态扩展知识以防御越狱攻击。与依赖静态数据集的传统方法不同，ICAG采用迭代过程来增强防御和攻击代理。这一持续改进过程加强了对新生成的越狱提示的防御。我们的实证研究证实了ICAG的有效性，经由ICAG保护的LLMs在各种攻击场景中显著降低了越狱成功率。

    arXiv:2402.13148v1 Announce Type: new  Abstract: Large Language Models (LLMs) demonstrate remarkable capabilities across diverse applications. However, concerns regarding their security, particularly the vulnerability to jailbreak attacks, persist. Drawing inspiration from adversarial training in deep learning and LLM agent learning processes, we introduce the In-Context Adversarial Game (ICAG) for defending against jailbreaks without the need for fine-tuning. ICAG leverages agent learning to conduct an adversarial game, aiming to dynamically extend knowledge to defend against jailbreaks. Unlike traditional methods that rely on static datasets, ICAG employs an iterative process to enhance both the defense and attack agents. This continuous improvement process strengthens defenses against newly generated jailbreak prompts. Our empirical studies affirm ICAG's efficacy, where LLMs safeguarded by ICAG exhibit significantly reduced jailbreak success rates across various attack scenarios. Mo
    
[^14]: 复兴多变量时间序列预测：可学习分解与跨系列依赖关系和内部变化建模

    Revitalizing Multivariate Time Series Forecasting: Learnable Decomposition with Inter-Series Dependencies and Intra-Series Variations Modeling

    [https://arxiv.org/abs/2402.12694](https://arxiv.org/abs/2402.12694)

    引入可学习分解策略和双注意力模块，同时捕捉跨系列依赖和内部变化，以应对复杂的多变量时间序列预测挑战。

    

    预测多变量时间序列是至关重要的，要求精确建模错综复杂模式，包括跨时间序列的依赖关系和内部变动。每个时间序列具有独特的趋势特征带来挑战，现有方法依赖基本的移动平均核可能难以处理现实数据中的非线性结构和复杂趋势。基于此，我们引入了一个可学习的分解策略，更合理地捕捉动态趋势信息。此外，我们提出了一个双注意力模块，专门用于同时捕捉跨系列依赖关系和内部变化，以实现更好的时间序列预测，其中通过通道自注意力和自回归自注意力实现。为了评估我们方法的有效性，我们在八个开源数据集上进行了实验，并将其与最先进的方法进行了比较。通过比较结果，我们的 Leddam...

    arXiv:2402.12694v1 Announce Type: new  Abstract: Predicting multivariate time series is crucial, demanding precise modeling of intricate patterns, including inter-series dependencies and intra-series variations. Distinctive trend characteristics in each time series pose challenges, and existing methods, relying on basic moving average kernels, may struggle with the non-linear structure and complex trends in real-world data. Given that, we introduce a learnable decomposition strategy to capture dynamic trend information more reasonably. Additionally, we propose a dual attention module tailored to capture inter-series dependencies and intra-series variations simultaneously for better time series forecasting, which is implemented by channel-wise self-attention and autoregressive self-attention. To evaluate the effectiveness of our method, we conducted experiments across eight open-source datasets and compared it with the state-of-the-art methods. Through the comparison results, our Leddam
    
[^15]: LoRA+: 大规模模型的高效低秩适应性

    LoRA+: Efficient Low Rank Adaptation of Large Models

    [https://arxiv.org/abs/2402.12354](https://arxiv.org/abs/2402.12354)

    LoRA+通过设置不同的学习率来改进原始LoRA的低效率问题，在保持计算成本不变的情况下提高了模型性能和微调速度。

    

    在这篇论文中，我们展示了低秩适应（LoRA）最初由胡等人（2021年）引入，导致对具有大宽度（嵌入维度）的模型进行微调时表现亚优。这是因为LoRA中的适配器矩阵A和B使用相同的学习率进行更新。通过对大宽度网络进行缩放参数的论证，我们展示了对适配器矩阵A和B使用相同的学习率不利于有效的特征学习。然后，我们表明LoRA的这种次优性可以简单地通过为LoRA适配器矩阵A和B设置不同的学习率以及一个精心选择的比率来进行校正。我们将这个提出的算法称为LoRA$+$。在我们广泛的实验证明中，LoRA$+$在相同计算成本下提高了性能（1-2％的改进）和微调速度（最多提速约2倍）。

    arXiv:2402.12354v1 Announce Type: cross  Abstract: In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021) leads to suboptimal finetuning of models with large width (embedding dimension). This is due to the fact that adapter matrices A and B in LoRA are updated with the same learning rate. Using scaling arguments for large width networks, we demonstrate that using the same learning rate for A and B does not allow efficient feature learning. We then show that this suboptimality of LoRA can be corrected simply by setting different learning rates for the LoRA adapter matrices A and B with a well-chosen ratio. We call this proposed algorithm LoRA$+$. In our extensive experiments, LoRA$+$ improves performance (1-2 $\%$ improvements) and finetuning speed (up to $\sim$ 2X SpeedUp), at the same computational cost as LoRA.
    
[^16]: 回忆那一年发生的事件？评估大型语言模型中的时间信息和推理能力

    Remember This Event That Year? Assessing Temporal Information and Reasoning in Large Language Models

    [https://arxiv.org/abs/2402.11997](https://arxiv.org/abs/2402.11997)

    大型语言模型在处理时间信息和推理方面存在显著限制，闭源模型可能暗示了不确定性认识与错误回应之间的权衡。

    

    大型语言模型（LLMs）越来越普遍，但它们对于推理和保留时间信息的能力仍然有限。这限制了它们在理解事件的顺序性对关键的现实场景中的应用。本文在一个新颖的大规模时间数据集\textbf{TempUN}上对最先进的模型进行实验，揭示了时间保留和推理能力方面的显著限制。有趣的是，闭源模型更频繁地显示出知识差距，可能暗示了不确定性认识和错误回应之间的权衡。此外，探索各种微调方法并没有带来主要性能改进。相关数据集和代码可在以下网址获得（https://github.com/lingoiitgn/TempUN）。

    arXiv:2402.11997v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are increasingly becoming ubiquitous, yet their ability to reason about and retain temporal information remains limited. This hinders their application in real-world scenarios where understanding the sequential nature of events is crucial. This paper experiments with state-of-the-art models on a novel, large-scale temporal dataset, \textbf{TempUN}, to reveal significant limitations in temporal retention and reasoning abilities. Interestingly, closed-source models indicate knowledge gaps more frequently, potentially suggesting a trade-off between uncertainty awareness and incorrect responses. Further, exploring various fine-tuning approaches yielded no major performance improvements. The associated dataset and code are available at the following URL (https://github.com/lingoiitgn/TempUN).
    
[^17]: U$^2$MRPD: 通过大型潜在扩散模型引导的无监督MRI重建

    U$^2$MRPD: Unsupervised undersampled MRI reconstruction by prompting a large latent diffusion model

    [https://arxiv.org/abs/2402.10609](https://arxiv.org/abs/2402.10609)

    U$^2$MRPD是一个新颖的框架，通过大型潜在扩散模型引导，实现了无监督的欠采样MRI重建，能够支持图像特定的MRI重建，且在多个数据集上表现出与监督和MRI扩散方法相媲美甚至更好的性能。

    

    arXiv:2402.10609v1 公告类型: 跨领域 摘要: 在自然图像上预训练的大型潜在扩散模型(LLDM)中蕴含着丰富而假设上普遍适用于自然和医学图像的隐含视觉知识。为了测试这一假设，我们引入了一个新颖的框架，通过提示一个预训练的大型潜在扩散模型（U$^2$MRPD）进行无监督的欠采样MRI重建。现有的数据驱动、监督的欠采样MRI重建网络通常具有有限的泛化能力和适应性，不足以应对各种数据采集场景；然而，U$^2$MRPD通过使用量身定制的MRSampler，支持图像特定的MRI重建，该MRSampler适用于复值MRI图像。通过任何单一来源或多源MRI数据集，U$^2$MRPD的性能还可以通过MRAdapter进行进一步提升，同时保持生成图像先验不变。多个数据集上的实验表明，U$^2$MRPD实现了与监督和MRI扩散方法相媲美甚至更好的性能。

    arXiv:2402.10609v1 Announce Type: cross  Abstract: Implicit visual knowledge in a large latent diffusion model (LLDM) pre-trained on natural images is rich and hypothetically universal to natural and medical images. To test this hypothesis, we introduce a novel framework for Unsupervised Undersampled MRI Reconstruction by Prompting a pre-trained large latent Diffusion model ( U$^2$MRPD). Existing data-driven, supervised undersampled MRI reconstruction networks are typically of limited generalizability and adaptability toward diverse data acquisition scenarios; yet U$^2$MRPD supports image-specific MRI reconstruction by prompting an LLDM with an MRSampler tailored for complex-valued MRI images. With any single-source or diverse-source MRI dataset, U$^2$MRPD's performance is further boosted by an MRAdapter while keeping the generative image priors intact. Experiments on multiple datasets show that U$^2$MRPD achieves comparable or better performance than supervised and MRI diffusion metho
    
[^18]: 当表示对齐时：表示学习动力学中的普遍性

    When Representations Align: Universality in Representation Learning Dynamics

    [https://arxiv.org/abs/2402.09142](https://arxiv.org/abs/2402.09142)

    本研究提出了一个有效的表示学习理论，该理论假设了编码映射和解码映射为任意光滑函数，并且能够描述复杂且大型架构中的表示学习动力学。

    

    深度神经网络有许多不同的大小和结构。架构的选择，结合数据集和学习算法，普遍认为影响了学习到的神经表示。然而，最近的研究结果显示，不同的架构学习到的表示具有惊人的定性相似性。在这里，我们在将输入到隐藏表示的编码映射和从表示到输出的解码映射都是任意光滑函数的假设下，推导了表示学习的有效理论。在复杂和大型架构的情况下，隐藏表示没有被参数化强约束，该理论概括了表示学习动力学。我们通过实验证明，这个有效理论描述了具有不同激活函数和架构的深度网络中表示学习动力学的一些方面。

    arXiv:2402.09142v1 Announce Type: new Abstract: Deep neural networks come in many sizes and architectures. The choice of architecture, in conjunction with the dataset and learning algorithm, is commonly understood to affect the learned neural representations. Yet, recent results have shown that different architectures learn representations with striking qualitative similarities. Here we derive an effective theory of representation learning under the assumption that the encoding map from input to hidden representation and the decoding map from representation to output are arbitrary smooth functions. This theory schematizes representation learning dynamics in the regime of complex, large architectures, where hidden representations are not strongly constrained by the parametrization. We show through experiments that the effective theory describes aspects of representation learning dynamics across a range of deep networks with different activation functions and architectures, and exhibits 
    
[^19]: 从均场稳态分布中采样

    Sampling from the Mean-Field Stationary Distribution

    [https://arxiv.org/abs/2402.07355](https://arxiv.org/abs/2402.07355)

    本文研究了从均场随机微分方程 (SDE) 的稳态分布中采样的复杂性，并提出了一种解耦的方法。该方法能够在多种情况下提供改进的保证，包括在均场区域优化某些双层神经网络的更好保证。

    

    我们研究了从均场随机微分方程 (SDE) 的稳态分布中采样的复杂性，或者等价地，即包含交互项的概率测度空间上的最小化函数的复杂性。我们的主要洞察是将这个问题的两个关键方面解耦：(1) 通过有限粒子系统逼近均场SDE，通过时间均匀传播混沌，和(2) 通过标准对数凹抽样器从有限粒子稳态分布中采样。我们的方法在概念上更简单，其灵活性允许结合用于算法和理论的最新技术。这导致在许多设置中提供了改进的保证，包括在均场区域优化某些双层神经网络的更好保证。

    We study the complexity of sampling from the stationary distribution of a mean-field SDE, or equivalently, the complexity of minimizing a functional over the space of probability measures which includes an interaction term.   Our main insight is to decouple the two key aspects of this problem: (1) approximation of the mean-field SDE via a finite-particle system, via uniform-in-time propagation of chaos, and (2) sampling from the finite-particle stationary distribution, via standard log-concave samplers. Our approach is conceptually simpler and its flexibility allows for incorporating the state-of-the-art for both algorithms and theory. This leads to improved guarantees in numerous settings, including better guarantees for optimizing certain two-layer neural networks in the mean-field regime.
    
[^20]: 适应性近端梯度方法在没有近似的情况下是通用的

    Adaptive proximal gradient methods are universal without approximation

    [https://arxiv.org/abs/2402.06271](https://arxiv.org/abs/2402.06271)

    该论文证明了适应性近端梯度方法对于凸问题不受传统假设的限制，并且可以在局部梯度H\"older连续性条件下收敛，同时避免了线搜索步骤和近似的使用。对局部H\"older常数和H\"older连续性顺序的先验知识也不是必需的。在数值实验中，与基准方法进行了对比实验，涵盖了局部和全局 H\"older 设置。

    

    我们证明，对于凸问题，适应性近端梯度方法不受传统利普希兹假设的限制。我们的分析揭示了一类无需线搜索的方法在仅具有局部H\"older梯度连续性的情况下仍然收敛，特别适用于连续可微的半代数函数。为了弥补缺乏局部利普希兹连续性的问题，常见的方法包括$\varepsilon$-oracle和/或线搜索步骤。相反，我们利用普通的H\"older不等式而不涉及任何近似，同时保持适应性方案无需线搜索的特性。此外，我们证明了不需要先验知识的局部H\"older常数或H\"older连续性的顺序，也可以实现完全的序列收敛性。在数值实验中，我们对机器学习中的不同任务进行了基准方法的比较，涵盖了局部和全局 H\"older 设置。

    We show that adaptive proximal gradient methods for convex problems are not restricted to traditional Lipschitzian assumptions. Our analysis reveals that a class of linesearch-free methods is still convergent under mere local H\"older gradient continuity, covering in particular continuously differentiable semi-algebraic functions. To mitigate the lack of local Lipschitz continuity, popular approaches revolve around $\varepsilon$-oracles and/or linesearch procedures. In contrast, we exploit plain H\"older inequalities not entailing any approximation, all while retaining the linesearch-free nature of adaptive schemes. Furthermore, we prove full sequence convergence without prior knowledge of local H\"older constants nor of the order of H\"older continuity. In numerical experiments we present comparisons to baseline methods on diverse tasks from machine learning covering both the locally and the globally H\"older setting.
    
[^21]: 关于深度分类器的校准和符合预测研究

    On Calibration and Conformal Prediction of Deep Classifiers

    [https://arxiv.org/abs/2402.05806](https://arxiv.org/abs/2402.05806)

    本文研究了温度缩放对符合预测方法的影响，通过实证研究发现，校准对自适应C方法产生了有害的影响。

    

    在许多分类应用中，深度神经网络（DNN）基于分类器的预测需要伴随一些置信度指示。针对这个目标，有两种流行的后处理方法：1）校准：修改分类器的softmax值，使其最大值（与预测相关）更好地估计正确概率；和2）符合预测（CP）：设计一个基于softmax值的分数，从中产生一组预测，具有理论上保证正确类别边际覆盖的特性。尽管在实践中两种指示都可能是需要的，但到目前为止它们之间的相互作用尚未得到研究。为了填补这一空白，在本文中，我们研究了温度缩放，这是最常见的校准技术，对重要的CP方法的影响。我们首先进行了一项广泛的实证研究，其中显示了一些重要的洞察，其中包括令人惊讶的发现，即校准对流行的自适应C方法产生了有害的影响。

    In many classification applications, the prediction of a deep neural network (DNN) based classifier needs to be accompanied with some confidence indication. Two popular post-processing approaches for that aim are: 1) calibration: modifying the classifier's softmax values such that their maximum (associated with the prediction) better estimates the correctness probability; and 2) conformal prediction (CP): devising a score (based on the softmax values) from which a set of predictions with theoretically guaranteed marginal coverage of the correct class is produced. While in practice both types of indications can be desired, so far the interplay between them has not been investigated. Toward filling this gap, in this paper we study the effect of temperature scaling, arguably the most common calibration technique, on prominent CP methods. We start with an extensive empirical study that among other insights shows that, surprisingly, calibration has a detrimental effect on popular adaptive C
    
[^22]: 利用动态图的深度强化学习进行自适应信息路径规划

    Deep Reinforcement Learning with Dynamic Graphs for Adaptive Informative Path Planning

    [https://arxiv.org/abs/2402.04894](https://arxiv.org/abs/2402.04894)

    提出了一种利用动态图的深度强化学习方法，用于自适应信息路径规划，能够在未知的三维环境中映射出感兴趣的目标。

    

    自主机器人常常被用于数据收集，因为它们高效且劳动成本低。机器人数据采集的关键任务是在初始未知环境中规划路径，以满足平台特定的资源约束，例如有限的电池寿命。在三维环境中进行自适应在线路径规划面临着很多挑战，包括大量有效动作的存在以及未知遮挡物的存在。为了解决这些问题，我们提出了一种新颖的深度强化学习方法，用于自适应重新规划机器人路径以在未知的三维环境中映射出感兴趣的目标。我们方法的关键之处在于构建动态图，将规划动作限制在机器人附近，使我们能够快速响应新发现的障碍和感兴趣的目标。对于重新规划，我们提出了一种新的奖励函数，平衡探索未知环境和利用在线收集的有关感兴趣目标的数据。

    Autonomous robots are often employed for data collection due to their efficiency and low labour costs. A key task in robotic data acquisition is planning paths through an initially unknown environment to collect observations given platform-specific resource constraints, such as limited battery life. Adaptive online path planning in 3D environments is challenging due to the large set of valid actions and the presence of unknown occlusions. To address these issues, we propose a novel deep reinforcement learning approach for adaptively replanning robot paths to map targets of interest in unknown 3D environments. A key aspect of our approach is a dynamically constructed graph that restricts planning actions local to the robot, allowing us to quickly react to newly discovered obstacles and targets of interest. For replanning, we propose a new reward function that balances between exploring the unknown environment and exploiting online-collected data about the targets of interest. Our experi
    
[^23]: 分层树状知识图谱用于学术调研

    Hierarchical Tree-structured Knowledge Graph For Academic Insight Survey

    [https://arxiv.org/abs/2402.04854](https://arxiv.org/abs/2402.04854)

    该论文提出了一种分层树状知识图谱和推荐系统，帮助初学者研究者进行研究调研，填补了现有导航知识图谱的不足，并解决了学术论文推荐系统中高文本相似性带来的困惑。

    

    对于缺乏研究培训的初学者研究者来说，研究调查一直是一个挑战。这些研究者在短时间内很难理解他们研究主题内的方向，以及发现新的研究发现。为初学者研究者提供直观的帮助的一种方式是提供相关的知识图谱(KG)并推荐相关的学术论文。然而，现有的导航知识图谱主要依赖于研究领域的关键字，常常无法清楚地呈现多个相关论文之间的逻辑层次关系。此外，大多数学术论文推荐系统仅仅依赖于高文本相似性，这可能会让研究人员困惑为什么推荐了特定的文章。他们可能缺乏了解关于他们希望获得的"问题解决"和"问题发现"之间的见解连接的重要信息。为解决这些问题，本研究旨在支持初学者研究者进行研究调研。

    Research surveys have always posed a challenge for beginner researchers who lack of research training. These researchers struggle to understand the directions within their research topic, and the discovery of new research findings within a short time. One way to provide intuitive assistance to beginner researchers is by offering relevant knowledge graphs(KG) and recommending related academic papers. However, existing navigation knowledge graphs primarily rely on keywords in the research field and often fail to present the logical hierarchy among multiple related papers clearly. Moreover, most recommendation systems for academic papers simply rely on high text similarity, which can leave researchers confused as to why a particular article is being recommended. They may lack of grasp important information about the insight connection between "Issue resolved" and "Issue finding" that they hope to obtain. To address these issues, this study aims to support research insight surveys for begi
    
[^24]: 用球面高斯约束进行条件扩散引导

    Guidance with Spherical Gaussian Constraint for Conditional Diffusion

    [https://arxiv.org/abs/2402.03201](https://arxiv.org/abs/2402.03201)

    本文提出了一种用球面高斯约束的扩散算法（DSG），解决了在条件生成任务中采样过程中的流形偏离问题。这种算法通过优化将步骤限制在中间数据流形内，并能够使用较大的引导步长。

    

    最近扩散模型的进展尝试通过利用可微的损失函数进行指导来处理条件生成任务，而无需额外的训练。虽然这些方法在一定程度上取得了成功，但它们往往在样本质量上做出妥协，并需要较小的引导步长，导致采样过程变长。本文揭示了在引导损失的采样过程中流形偏离的根本问题所在。我们通过建立损失引导的估计误差的特定下界从理论上证明了流形偏离的存在。为了减轻这个问题，我们提出了带有球面高斯约束（DSG）的扩散，从高维高斯分布的集中现象中汲取灵感。DSG通过优化有效地将引导步骤约束在中间数据流形内，并能够使用较大的引导步长。此外，我们提出了一个闭式公式。

    Recent advances in diffusion models attempt to handle conditional generative tasks by utilizing a differentiable loss function for guidance without the need for additional training. While these methods achieved certain success, they often compromise on sample quality and require small guidance step sizes, leading to longer sampling processes. This paper reveals that the fundamental issue lies in the manifold deviation during the sampling process when loss guidance is employed. We theoretically show the existence of manifold deviation by establishing a certain lower bound for the estimation error of the loss guidance. To mitigate this problem, we propose Diffusion with Spherical Gaussian constraint (DSG), drawing inspiration from the concentration phenomenon in high-dimensional Gaussian distributions. DSG effectively constrains the guidance step within the intermediate data manifold through optimization and enables the use of larger guidance steps. Furthermore, we present a closed-form 
    
[^25]: DexDiffuser: 使用扩散模型生成灵巧抓取姿势

    DexDiffuser: Generating Dexterous Grasps with Diffusion Models

    [https://arxiv.org/abs/2402.02989](https://arxiv.org/abs/2402.02989)

    DexDiffuser是一种使用扩散模型生成灵巧抓取姿势的新方法，通过对物体点云的生成、评估和优化，实现了较高的抓取成功率。

    

    我们引入了DexDiffuser，一种新颖的灵巧抓取方法，能够在部分物体点云上生成、评估和优化抓取姿势。DexDiffuser包括条件扩散型抓取采样器DexSampler和灵巧抓取评估器DexEvaluator。DexSampler通过对随机抓取进行迭代去噪，生成与物体点云条件相关的高质量抓取姿势。我们还引入了两种抓取优化策略：基于评估器的扩散(Evaluator-Guided Diffusion，EGD)和基于评估器的采样优化(Evaluator-based Sampling Refinement，ESR)。我们在虚拟环境和真实世界的实验中，使用Allegro Hand进行测试，结果表明DexDiffuser相比最先进的多指抓取生成方法FFHNet，平均抓取成功率提高了21.71-22.20%。

    We introduce DexDiffuser, a novel dexterous grasping method that generates, evaluates, and refines grasps on partial object point clouds. DexDiffuser includes the conditional diffusion-based grasp sampler DexSampler and the dexterous grasp evaluator DexEvaluator. DexSampler generates high-quality grasps conditioned on object point clouds by iterative denoising of randomly sampled grasps. We also introduce two grasp refinement strategies: Evaluator-Guided Diffusion (EGD) and Evaluator-based Sampling Refinement (ESR). Our simulation and real-world experiments on the Allegro Hand consistently demonstrate that DexDiffuser outperforms the state-of-the-art multi-finger grasp generation method FFHNet with an, on average, 21.71--22.20\% higher grasp success rate.
    
[^26]: 混合噪声与条件深度生成模型下的后验估计

    Mixed Noise and Posterior Estimation with Conditional DeepGEM

    [https://arxiv.org/abs/2402.02964](https://arxiv.org/abs/2402.02964)

    本论文提出了一种用于联合估计贝叶斯逆问题中后验概率和噪声参数的新算法，该算法通过期望最大化（EM）算法解决问题，并使用条件标准化流来近似后验概率。该模型能够整合来自多个测量的信息。

    

    受混合噪声模型的间接测量和纳米计量应用的启发，我们开发了一种新的算法，用于联合估计贝叶斯逆问题中的后验概率和噪声参数。我们提出通过期望最大化（EM）算法来解决这个问题。基于当前的噪声参数，我们在E步中学习了一个条件标准化流，以近似后验概率。在M步中，我们提出再次通过EM算法找到噪声参数的更新，其具有解析公式。我们将条件标准化流的训练与前向和反向KL进行比较，并展示我们的模型能够整合来自许多测量的信息，而不像之前的方法。

    Motivated by indirect measurements and applications from nanometrology with a mixed noise model, we develop a novel algorithm for jointly estimating the posterior and the noise parameters in Bayesian inverse problems. We propose to solve the problem by an expectation maximization (EM) algorithm. Based on the current noise parameters, we learn in the E-step a conditional normalizing flow that approximates the posterior. In the M-step, we propose to find the noise parameter updates again by an EM algorithm, which has analytical formulas. We compare the training of the conditional normalizing flow with the forward and reverse KL, and show that our model is able to incorporate information from many measurements, unlike previous approaches.
    
[^27]: 集合就是你所需要的：用于HL-LHC的超快速喷注分类在FPGAs上的研究

    Sets are all you need: Ultrafast jet classification on FPGAs for HL-LHC

    [https://arxiv.org/abs/2402.01876](https://arxiv.org/abs/2402.01876)

    该论文研究了在FPGA上进行高性能喷注分类的机器学习算法，通过量化感知的训练和高效的硬件实现，实现了低延迟和低资源消耗，为高亮度阶段标记的模型提供了初始设计。

    

    我们研究了各种基于机器学习的算法，用于在可编程逻辑门阵列上进行准确的喷注风味分类，并展示了延迟和资源消耗如何随着输入大小和算法选择而变化。这些架构为在CERN LHC的高亮度阶段标记所能使用的模型提供了一个初始设计。高亮度升级将导致质子-质子碰撞的瞬时亮度增加五倍，进而产生更高的数据量和复杂性，例如喷注组成部分的可用性。通过量化感知的训练和高效的硬件实现，我们展示了复杂架构（如深度集合和交互网络）的推断时间可以达到O（100）ns，并且计算资源成本较低。

    We study various machine learning based algorithms for performing accurate jet flavor classification on field-programmable gate arrays and demonstrate how latency and resource consumption scale with the input size and choice of algorithm. These architectures provide an initial design for models that could be used for tagging at the CERN LHC during its high-luminosity phase. The high-luminosity upgrade will lead to a five-fold increase in its instantaneous luminosity for proton-proton collisions and, in turn, higher data volume and complexity, such as the availability of jet constituents. Through quantization-aware training and efficient hardware implementations, we show that O(100) ns inference of complex architectures such as deep sets and interaction networks is feasible at a low computational resource cost.
    
[^28]: 语言引导的世界模型：一种基于模型的人工智能控制方法

    Language-Guided World Models: A Model-Based Approach to AI Control

    [https://arxiv.org/abs/2402.01695](https://arxiv.org/abs/2402.01695)

    语言引导的世界模型（LWMs）是一种基于模型的人工智能控制方法，它通过阅读语言描述来捕捉环境动态，提高了代理的沟通效率，并允许人类通过简洁的语言反馈同时改变他们在多个任务上的行为。

    

    将概率世界模型安装到人工智能代理中，为人类与这些代理沟通和控制打开了一个高效的渠道。除了更新代理策略，人类还可以修改他们的内部世界模型，以影响代理的决策。然而，当前现有的世界模型难以适应人类，因为它们缺乏自然的通信界面。为了解决这个问题，我们开发了语言引导的世界模型（LWMs），它们可以通过阅读语言描述来捕捉环境动态。这些模型提高了代理的沟通效率，使人类能够通过简洁的语言反馈同时改变他们在多个任务上的行为。它们还使代理能够从最初用于指导人类的文本中进行自我学习。为了促进LWMs的发展，我们设计了一个基于MESSENGER游戏（Hanjie等人，2021）的挑战基准，需要对新场景进行组合泛化。

    Installing probabilistic world models into artificial agents opens an efficient channel for humans to communicate with and control these agents. In addition to updating agent policies, humans can modify their internal world models in order to influence their decisions. The challenge, however, is that currently existing world models are difficult for humans to adapt because they lack a natural communication interface. Aimed at addressing this shortcoming, we develop Language-Guided World Models (LWMs), which can capture environment dynamics by reading language descriptions. These models enhance agent communication efficiency, allowing humans to simultaneously alter their behavior on multiple tasks with concise language feedback. They also enable agents to self-learn from texts originally written to instruct humans. To facilitate the development of LWMs, we design a challenging benchmark based on the game of MESSENGER (Hanjie et al., 2021), requiring compositional generalization to new l
    
[^29]: KVQuant: 以KV缓存量化实现1000万上下文长度LLM推理

    KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization

    [https://arxiv.org/abs/2401.18079](https://arxiv.org/abs/2401.18079)

    KVQuant是一种解决LLM推理中大量内存消耗的KV缓存量化方法，通过引入新颖的量化方法，包括分通道键量化、RoPE前量化键和非均匀KV缓存量化，准确表示超低精度的KV激活。

    

    LLM在文档分析和摘要等需要大窗口上下文的应用中越来越受到关注，在推理过程中，KV缓存激活成为记忆消耗的主要贡献者。量化是一种压缩KV缓存激活的有效方法，然而现有的解决方案无法准确表示超低精度（如低于4位）的激活。本文提出了KVQuant，通过引入新颖的方法量化缓存的KV激活来解决这个问题，包括：(i)分通道键量化，在量化键激活时调整维度以更好地匹配分布；(ii)RoPE前量化键，在旋转位置嵌入之前量化键激活以减轻其对量化的影响；(iii)非均匀KV缓存量化，在每层推导出权重感知的非均匀数据类型，以更好地表示不同层的敏感性。

    LLMs are seeing growing use for applications such as document analysis and summarization which require large context windows, and with these large context windows KV cache activations surface as the dominant contributor to memory consumption during inference. Quantization is a promising approach for compressing KV cache activations; however, existing solutions fail to represent activations accurately in ultra-low precisions, such as sub-4-bit. In this work, we present KVQuant, which addresses this problem by incorporating novel methods for quantizing cached KV activations, including: (i) Per-Channel Key Quantization, where we adjust the dimension along which we quantize the Key activations to better match the distribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations before the rotary positional embedding to mitigate its impact on quantization; (iii) Non-Uniform KV Cache Quantization, where we derive per-layer sensitivity-weighted non-uniform datatypes that better 
    
[^30]: 通过对比激活加法指导Llama 2

    Steering Llama 2 via Contrastive Activation Addition

    [https://arxiv.org/abs/2312.06681](https://arxiv.org/abs/2312.06681)

    引入Contrastive Activation Addition（CAA）方法，通过修改语言模型的激活来精确控制目标行为的程度，显著改变模型行为并在微调和系统提示设计的基础上提供额外有效性。

    

    我们引入了一种创新的方法Contrastive Activation Addition（CAA），用于通过在前向传递过程中修改其激活来指导语言模型。CAA通过对某种行为的正面和负面示例之间残差流激活的差异求平均，计算出“指导向量”。在推断过程中，在用户提示后的所有token位置上以正负系数添加这些指导向量，从而精确控制目标行为的程度。我们通过使用多项选择行为问题数据集和开放式生成任务在Llama 2 Chat上评估了CAA的有效性。我们证明CAA显着改变了模型行为，不仅在传统方法如微调和系统提示设计的基础上有效，而且最小程度地降低了功能。此外，我们对模型的行为做出了更深入的洞察。

    arXiv:2312.06681v3 Announce Type: replace-cross  Abstract: We introduce Contrastive Activation Addition (CAA), an innovative method for steering language models by modifying their activations during forward passes. CAA computes "steering vectors" by averaging the difference in residual stream activations between pairs of positive and negative examples of a particular behavior, such as factual versus hallucinatory responses. During inference, these steering vectors are added at all token positions after the user's prompt with either a positive or negative coefficient, allowing precise control over the degree of the targeted behavior. We evaluate CAA's effectiveness on Llama 2 Chat using multiple-choice behavioral question datasets and open-ended generation tasks. We demonstrate that CAA significantly alters model behavior, is effective over and on top of traditional methods like finetuning and system prompt design, and minimally reduces capabilities. Moreover, we gain deeper insights in
    
[^31]: MT-HCCAR: 多任务深度学习与层级分类的注意力回归用于云属性检索

    MT-HCCAR: Multi-Task Deep Learning with Hierarchical Classification and Attention-based Regression for Cloud Property Retrieval. (arXiv:2401.16520v1 [cs.LG])

    [http://arxiv.org/abs/2401.16520](http://arxiv.org/abs/2401.16520)

    这篇论文提出了一种名为MT-HCCAR的多任务深度学习模型，用于云属性检索。该模型考虑了云属性检索任务之间的层级关系，并具有对不同传感器数据集具有健壮泛化能力的特点。

    

    在地球科学领域中，有效的云属性检索包括云遮蔽、云相分类和云光学厚度（COT）预测仍然至关重要。传统方法需要针对每个传感器仪器使用不同的模型，因为它们具有独特的光谱特征。最近，在地球科学研究中采用了机器学习和深度学习技术从卫星数据集的光谱观测中提取特征。然而，现有方法缺乏考虑检索任务之间层级关系的创新架构。此外，考虑到现有传感器之间的光谱多样性，开发具有对不同传感器数据集具有健壮泛化能力的模型是必要的。令人惊讶的是，目前缺乏解决多样数据集下选择最优模型的方法。为此，本文引入了MT-HCCAR，这是一种端到端的深度学习模型，采用多任务学习和基于注意力的回归方法。

    In the realm of Earth science, effective cloud property retrieval, encompassing cloud masking, cloud phase classification, and cloud optical thickness (COT) prediction, remains pivotal. Traditional methodologies necessitate distinct models for each sensor instrument due to their unique spectral characteristics. Recent strides in Earth Science research have embraced machine learning and deep learning techniques to extract features from satellite datasets' spectral observations. However, prevailing approaches lack novel architectures accounting for hierarchical relationships among retrieval tasks. Moreover, considering the spectral diversity among existing sensors, the development of models with robust generalization capabilities over different sensor datasets is imperative. Surprisingly, there is a dearth of methodologies addressing the selection of an optimal model for diverse datasets. In response, this paper introduces MT-HCCAR, an end-to-end deep learning model employing multi-task 
    
[^32]: 超越RMSE和MAE：引入EAUC来揭示偏见和不公平的迪亚德回归模型中的隐藏因素

    Beyond RMSE and MAE: Introducing EAUC to unmask hidden bias and unfairness in dyadic regression models. (arXiv:2401.10690v1 [cs.LG])

    [http://arxiv.org/abs/2401.10690](http://arxiv.org/abs/2401.10690)

    本研究引入EAUC作为一种新的度量标准，用以揭示迪亚德回归模型中隐藏的偏见和不公平问题。传统的全局错误度量标准如RMSE和MAE无法捕捉到这种问题。

    

    迪亚德回归模型用于预测一对实体的实值结果，在许多领域中都是基础的（例如，在推荐系统中预测用户对产品的评分），在许多其他领域中也有许多潜力但尚未深入探索（例如，在个性化药理学中近似确定患者的适当剂量）。本研究中，我们证明个体实体观察值分布的非均匀性导致了最先进模型中的严重偏见预测，偏向于实体的观察过去值的平均值，并在另类但同样重要的情况下提供比随机预测更差的预测能力。我们表明，全局错误度量标准如均方根误差（RMSE）和平均绝对误差（MAE）不足以捕捉到这种现象，我们将其命名为另类偏见，并引入另类-曲线下面积（EAUC）作为一个新的补充度量，可以在所有研究的模型中量化它。

    Dyadic regression models, which predict real-valued outcomes for pairs of entities, are fundamental in many domains (e.g. predicting the rating of a user to a product in Recommender Systems) and promising and under exploration in many others (e.g. approximating the adequate dosage of a drug for a patient in personalized pharmacology). In this work, we demonstrate that non-uniformity in the observed value distributions of individual entities leads to severely biased predictions in state-of-the-art models, skewing predictions towards the average of observed past values for the entity and providing worse-than-random predictive power in eccentric yet equally important cases. We show that the usage of global error metrics like Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) is insufficient to capture this phenomenon, which we name eccentricity bias, and we introduce Eccentricity-Area Under the Curve (EAUC) as a new complementary metric that can quantify it in all studied models
    
[^33]: 理解ChatGPT基推荐系统中的偏见：供应商公平性、时间稳定性和最新性研究

    Understanding Biases in ChatGPT-based Recommender Systems: Provider Fairness, Temporal Stability, and Recency. (arXiv:2401.10545v1 [cs.IR])

    [http://arxiv.org/abs/2401.10545](http://arxiv.org/abs/2401.10545)

    该研究揭示了基于ChatGPT的推荐系统的偏见问题，并研究了提示设计策略对推荐质量的影响。实验结果显示，在RecLLMs中引入特定的系统角色和提示策略可以增强推荐的公平性和多样性，同时GPT-based模型倾向于推荐最新和更多样化的电影流派。

    

    该研究探讨了使用大型语言模型（RecLLMs）的推荐系统的细微能力和固有偏见，重点研究了基于ChatGPT的系统。研究了生成模型和传统协同过滤模型在电影推荐中的差异行为。本研究主要调查了提示设计策略及其对推荐质量的各个方面（包括准确性、供应商公平性、多样性、稳定性、流行类型和时效性）的影响。我们的实验分析表明，在RecLLMs中引入特定的“系统角色”和“提示策略”显著影响其性能。例如，基于角色的提示可以增强推荐的公平性和多样性，减轻流行偏见。我们发现，虽然基于GPT的模型并不总是能与传统协同过滤基线模型的性能匹配，但它们倾向于推荐更新、更多样化的电影流派。值得注意的是，GPT-base

    This study explores the nuanced capabilities and inherent biases of Recommender Systems using Large Language Models (RecLLMs), with a focus on ChatGPT-based systems. It studies into the contrasting behaviors of generative models and traditional collaborative filtering models in movie recommendations. The research primarily investigates prompt design strategies and their impact on various aspects of recommendation quality, including accuracy, provider fairness, diversity, stability, genre dominance, and temporal freshness (recency).  Our experimental analysis reveals that the introduction of specific 'system roles' and 'prompt strategies' in RecLLMs significantly influences their performance. For instance, role-based prompts enhance fairness and diversity in recommendations, mitigating popularity bias. We find that while GPT-based models do not always match the performance of CF baselines, they exhibit a unique tendency to recommend newer and more diverse movie genres. Notably, GPT-base
    
[^34]: 使用本地自适应对抗颜色攻击对艺术品进行神经风格转换的保护

    Artwork Protection Against Neural Style Transfer Using Locally Adaptive Adversarial Color Attack. (arXiv:2401.09673v1 [cs.CV])

    [http://arxiv.org/abs/2401.09673](http://arxiv.org/abs/2401.09673)

    本文提出了一种名为本地自适应对抗颜色攻击（LAACA）的方法，用于保护艺术品免受神经风格转换（NST）的滥用。该方法通过在不可察觉的情况下对图像进行修改，产生对NST具有干扰作用的扰动。

    

    神经风格转换（NST）广泛应用于计算机视觉中，用于生成具有任意风格的新图像。这个过程利用神经网络将风格图像的美学元素与内容图像的结构因素融合在一起，形成一个和谐整合的视觉结果。然而，未经授权的NST可能会滥用艺术品。这种滥用引起了关于艺术家权利的社会技术问题，并促使开发技术方法来积极保护原始创作。对抗性攻击主要在机器学习安全中进行探索。我们的工作将这一技术引入到保护艺术家知识产权的领域。本文引入了本地自适应对抗颜色攻击（LAACA）的方法，这种方法可以以对人眼不可察觉但对NST产生干扰的方式修改图像。具体而言，我们设计了针对高频内容丰富区域的扰动，这些扰动由中间特征的破坏产生。我们进行了实验和用户研究。

    Neural style transfer (NST) is widely adopted in computer vision to generate new images with arbitrary styles. This process leverages neural networks to merge aesthetic elements of a style image with the structural aspects of a content image into a harmoniously integrated visual result. However, unauthorized NST can exploit artwork. Such misuse raises socio-technical concerns regarding artists' rights and motivates the development of technical approaches for the proactive protection of original creations. Adversarial attack is a concept primarily explored in machine learning security. Our work introduces this technique to protect artists' intellectual property. In this paper Locally Adaptive Adversarial Color Attack (LAACA), a method for altering images in a manner imperceptible to the human eyes but disruptive to NST. Specifically, we design perturbations targeting image areas rich in high-frequency content, generated by disrupting intermediate features. Our experiments and user study
    
[^35]: 深度学习的统计理论综述：近似，训练动态和生成模型

    A Survey on Statistical Theory of Deep Learning: Approximation, Training Dynamics, and Generative Models. (arXiv:2401.07187v1 [stat.ML])

    [http://arxiv.org/abs/2401.07187](http://arxiv.org/abs/2401.07187)

    该论文综述了深度学习的统计理论，包括近似方法、训练动态和生成模型。在非参数框架中，结果揭示了神经网络过度风险的快速收敛速率，以及如何通过梯度方法训练网络以找到良好的泛化解决方案。

    

    在这篇文章中，我们从三个角度回顾了关于神经网络统计理论的文献。第一部分回顾了在回归或分类的非参数框架下关于神经网络过度风险的结果。这些结果依赖于神经网络的显式构造，以及采用了近似理论的工具，导致过度风险的快速收敛速率。通过这些构造，可以用样本大小、数据维度和函数平滑性来表达网络的宽度和深度。然而，他们的基本分析仅适用于深度神经网络高度非凸的全局极小值点。这促使我们在第二部分回顾神经网络的训练动态。具体而言，我们回顾了那些试图回答“基于梯度方法训练的神经网络如何找到能够在未见数据上有良好泛化性能的解”的论文。尤其是两个知名的

    In this article, we review the literature on statistical theories of neural networks from three perspectives. In the first part, results on excess risks for neural networks are reviewed in the nonparametric framework of regression or classification. These results rely on explicit constructions of neural networks, leading to fast convergence rates of excess risks, in that tools from the approximation theory are adopted. Through these constructions, the width and depth of the networks can be expressed in terms of sample size, data dimension, and function smoothness. Nonetheless, their underlying analysis only applies to the global minimizer in the highly non-convex landscape of deep neural networks. This motivates us to review the training dynamics of neural networks in the second part. Specifically, we review papers that attempt to answer ``how the neural network trained via gradient-based methods finds the solution that can generalize well on unseen data.'' In particular, two well-know
    
[^36]: 一种用于分布式、动态6G应用的语义感知多址访问方案

    A Semantic-Aware Multiple Access Scheme for Distributed, Dynamic 6G-Based Applications. (arXiv:2401.06308v1 [cs.NI])

    [http://arxiv.org/abs/2401.06308](http://arxiv.org/abs/2401.06308)

    本文提出了一种语义感知多址访问方案，旨在优化资源利用与公平性的权衡，并考虑用户数据的相关性，以满足未来6G应用的要求和特性。

    

    语义感知范式的出现为创新的服务提供了机会，尤其是在基于6G的应用环境中。尽管在语义提取技术方面取得了显著进展，但将语义信息纳入资源分配决策仍处于早期阶段，缺乏对未来系统需求和特性的考虑。为此，本文引入了一种新的无线频谱多址访问问题的建模。它旨在优化利用率与公平性的权衡，使用α-公平度量，并通过引入自助吞吐量和协助吞吐量的概念来考虑用户数据的相关性。首先，分析了该问题，找出了最优解。接下来，提出了一种基于模型无关的多主体深度强化学习技术的语义感知多智能体双重和决斗深度Q学习 (SAMA-D3QL) 方法。

    The emergence of the semantic-aware paradigm presents opportunities for innovative services, especially in the context of 6G-based applications. Although significant progress has been made in semantic extraction techniques, the incorporation of semantic information into resource allocation decision-making is still in its early stages, lacking consideration of the requirements and characteristics of future systems. In response, this paper introduces a novel formulation for the problem of multiple access to the wireless spectrum. It aims to optimize the utilization-fairness trade-off, using the $\alpha$-fairness metric, while accounting for user data correlation by introducing the concepts of self- and assisted throughputs. Initially, the problem is analyzed to identify its optimal solution. Subsequently, a Semantic-Aware Multi-Agent Double and Dueling Deep Q-Learning (SAMA-D3QL) technique is proposed. This method is grounded in Model-free Multi-Agent Deep Reinforcement Learning (MADRL),
    
[^37]: 通过Gromov-Monge嵌入实现单调生成建模

    Monotone Generative Modeling via a Gromov-Monge Embedding. (arXiv:2311.01375v1 [cs.LG])

    [http://arxiv.org/abs/2311.01375](http://arxiv.org/abs/2311.01375)

    该论文提出了一种使用Gromov-Monge嵌入的深度生成模型，通过识别数据背后的底层结构，并将其映射到低维潜空间中，解决了生成对抗网络（GAN）中对初始条件敏感性和模式崩溃的问题。

    

    生成对抗网络（GAN）是创建新内容的强大工具，但面临着对初始条件的敏感性和模式崩溃等挑战。为了解决这些问题，我们提出了一种利用Gromov-Monge嵌入（GME）的深度生成模型。它帮助识别数据背后的底层测度的低维结构，然后将其映射到保持几何性质的低维潜空间中的一个测度，并将其最优地传输到参考测度。通过GME的保持底层几何性质和生成映射的$c$-周期性单调性来保证它们。后一特性是确保更好的参数初始化鲁棒性和模式崩溃的第一步。数值实验证明了我们的方法在生成高质量图像、避免模式崩溃和对不同起始条件具有鲁棒性方面的有效性。

    Generative Adversarial Networks (GANs) are powerful tools for creating new content, but they face challenges such as sensitivity to starting conditions and mode collapse. To address these issues, we propose a deep generative model that utilizes the Gromov-Monge embedding (GME). It helps identify the low-dimensional structure of the underlying measure of the data and then maps it, while preserving its geometry, into a measure in a low-dimensional latent space, which is then optimally transported to the reference measure. We guarantee the preservation of the underlying geometry by the GME and $c$-cyclical monotonicity of the generative map, where $c$ is an intrinsic embedding cost employed by the GME. The latter property is a first step in guaranteeing better robustness to initialization of parameters and mode collapse. Numerical experiments demonstrate the effectiveness of our approach in generating high-quality images, avoiding mode collapse, and exhibiting robustness to different star
    
[^38]: 在Wasserstein空间中通过近端梯度下降实现基于流的生成模型的收敛性

    Convergence of flow-based generative models via proximal gradient descent in Wasserstein space. (arXiv:2310.17582v1 [stat.ML])

    [http://arxiv.org/abs/2310.17582](http://arxiv.org/abs/2310.17582)

    本文通过在Wasserstein空间中应用近端梯度下降，证明了基于流的生成模型的收敛性，并提供了生成数据分布的理论保证。

    

    基于流的生成模型在计算数据生成和似然函数方面具有一定的优势，并且最近在实证表现上显示出竞争力。与相关基于分数扩散模型的积累理论研究相比，对于在正向（数据到噪声）和反向（噪声到数据）方向上都是确定性的流模型的分析还很少。本文通过在归一化流网络中实施Jordan-Kinderleherer-Otto（JKO）方案的所谓JKO流模型，提供了通过渐进流模型生成数据分布的理论保证。利用Wasserstein空间中近端梯度下降（GD）的指数收敛性，我们证明了通过JKO流模型生成数据的Kullback-Leibler（KL）保证为$O(\varepsilon^2)$，其中使用$N \lesssim \log (1/\varepsilon)$个JKO步骤（流中的$N$个残差块），其中$\varepsilon$是每步一阶条件的误差。

    Flow-based generative models enjoy certain advantages in computing the data generation and the likelihood, and have recently shown competitive empirical performance. Compared to the accumulating theoretical studies on related score-based diffusion models, analysis of flow-based models, which are deterministic in both forward (data-to-noise) and reverse (noise-to-data) directions, remain sparse. In this paper, we provide a theoretical guarantee of generating data distribution by a progressive flow model, the so-called JKO flow model, which implements the Jordan-Kinderleherer-Otto (JKO) scheme in a normalizing flow network. Leveraging the exponential convergence of the proximal gradient descent (GD) in Wasserstein space, we prove the Kullback-Leibler (KL) guarantee of data generation by a JKO flow model to be $O(\varepsilon^2)$ when using $N \lesssim \log (1/\varepsilon)$ many JKO steps ($N$ Residual Blocks in the flow) where $\varepsilon $ is the error in the per-step first-order condit
    
[^39]: 通过正交注意力提升运算符学习

    Improved Operator Learning by Orthogonal Attention. (arXiv:2310.12487v1 [cs.LG])

    [http://arxiv.org/abs/2310.12487](http://arxiv.org/abs/2310.12487)

    本研究提出了一种基于正交注意力的神经运算符，通过核积分算子的特征分解和神经近似的特征函数，来解决现有方法在有限训练数据上过拟合的问题。实验证明，该方法在六个标准神经运算符数据集上的表现优于其他基线模型。

    

    神经运算符是一种有效的代理模型，用于学习偏微分方程的解，受到科学机器学习领域的广泛关注。其中，基于注意力的神经运算符已成为相关研究的主流之一。然而，由于注意机制中参数数量巨大，现有方法在有限的训练数据上过拟合。为了解决这个问题，我们基于核积分算子的特征分解和神经近似的特征函数，开发了一种正交注意力。正交化自然地对结果神经运算符施加适当的正则化效果，有助于抵抗过拟合和提升泛化能力。在包括正常和非正常几何形状的六个标准神经运算符基准数据集上的实验证明，我们的方法可以胜过竞争对手，并取得了相当大的优势。

    Neural operators, as an efficient surrogate model for learning the solutions of PDEs, have received extensive attention in the field of scientific machine learning. Among them, attention-based neural operators have become one of the mainstreams in related research. However, existing approaches overfit the limited training data due to the considerable number of parameters in the attention mechanism. To address this, we develop an orthogonal attention based on the eigendecomposition of the kernel integral operator and the neural approximation of eigenfunctions. The orthogonalization naturally poses a proper regularization effect on the resulting neural operator, which aids in resisting overfitting and boosting generalization. Experiments on six standard neural operator benchmark datasets comprising both regular and irregular geometries show that our method can outperform competing baselines with decent margins.
    
[^40]: 在二十个查询中破解黑盒大语言模型

    Jailbreaking Black Box Large Language Models in Twenty Queries. (arXiv:2310.08419v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.08419](http://arxiv.org/abs/2310.08419)

    这项研究提出了一个名为PAIR的算法，可以在只能黑盒访问大语言模型的情况下生成破解，无需人工干预。实证表明，PAIR通常只需要少于二十个查询来生成破解，是现有算法的数个数量级更高效。

    

    大语言模型（LLMs）与人类价值的一致性越来越受关注。然而，这类模型的一致性容易受到对抗性破解的影响，从而迫使LLMs超越其安全防护措施。因此，识别这些漏洞对于理解固有弱点并防止未来的不当使用至关重要。为此，我们提出了Prompt Automatic Iterative Refinement（PAIR），这是一种仅通过对LLM进行黑盒访问的算法生成语义破解。PAIR受到社会工程攻击的启发，使用攻击者LLM自动生成针对目标LLM的破解，无需人工干预。攻击者LLM通过迭代查询目标LLM来更新和改进候选破解。在实证上，PAIR通常只需要少于二十个查询来生成破解，这比现有算法高效数个数量级。PAIR还实现了有竞争力的破解效果。

    There is growing interest in ensuring that large language models (LLMs) align with human values. However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. The identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse. To this end, we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM. PAIR -- which is inspired by social engineering attacks -- uses an attacker LLM to automatically generate jailbreaks for a separate targeted LLM without human intervention. In this way, the attacker LLM iteratively queries the target LLM to update and refine a candidate jailbreak. Empirically, PAIR often requires fewer than twenty queries to produce a jailbreak, which is orders of magnitude more efficient than existing algorithms. PAIR also achieves competitive jailbrea
    
[^41]: 魔法词是什么？LLM提示的控制理论研究

    What's the Magic Word? A Control Theory of LLM Prompting. (arXiv:2310.04444v1 [cs.CL])

    [http://arxiv.org/abs/2310.04444](http://arxiv.org/abs/2310.04444)

    本论文将提示工程形式化为LLM上的最优控制问题，研究了给定token序列时是否存在一种最优提示能够准确预测最终的token，并提出了控制理论中的指标来描述LLM的可操纵性。

    

    提示工程在LLM的部署中是有效和重要的，但在数学上理解不足。在这里，我们将提示工程形式化为LLM上的最优控制问题，其中提示被认为是调节LLM输出分布的控制变量。在这个框架内，我们提出一个简单的问题：给定一个token序列，是否总存在一个我们可以添加的提示，使得LLM能够准确预测最终的token？我们将这样的最优提示称为魔法词，因为添加提示会导致LLM输出正确的答案。如果存在魔法词，我们能否找到它们？如果可以，它们的特性是什么？我们提供了将控制理论应用于自注意力头的分析分析，证明了其权重矩阵的奇异值函数为可控制性的上界。我们借鉴控制理论来提出了一种叫做$k-\epsilon$可控制性的指标，用于描述LLM的可操纵性。

    Prompt engineering is effective and important in the deployment of LLMs but is poorly understood mathematically. Here, we formalize prompt engineering as an optimal control problem on LLMs -- where the prompt is considered a control variable for modulating the output distribution of the LLM. Within this framework, we ask a simple question: given a sequence of tokens, does there always exist a prompt we can prepend that will steer the LLM toward accurately predicting the final token? We call such an optimal prompt the magic word since prepending the prompt causes the LLM to output the correct answer. If magic words exist, can we find them? If so, what are their properties? We offer analytic analysis on the controllability of the self-attention head where we prove a bound on controllability as a function of the singular values of its weight matrices. We take inspiration from control theory to propose a metric called $k-\epsilon$ controllability to characterize LLM steerability. We comput
    
[^42]: FENDA-FL：异构临床数据的个性化联邦学习

    FENDA-FL: Personalized Federated Learning on Heterogeneous Clinical Datasets. (arXiv:2309.16825v1 [cs.LG])

    [http://arxiv.org/abs/2309.16825](http://arxiv.org/abs/2309.16825)

    该论文提出了一种针对异构临床数据的个性化联邦学习方法，实验证明该方法在性能上超越了现有的全局和个性化联邦学习技术，并且对FLamby基准进行了实质性改进和扩展。

    

    联邦学习（FL）被越来越认为是克服临床环境中数据孤立问题的关键方法。该研究在临床应用的FL研究中做出了三个重要方面的贡献。首先，提出了将FENDA方法（Kim等人，2016）扩展到FL的方法。在FLamby基准（du Terrail等人，2022a）和GEMINI数据集（Verma等人，2017）上进行的实验表明，该方法对异构临床数据具有鲁棒性，并且通常优于现有的全局和个性化FL技术。此外，实验结果在原有的FLamby基准上表示出实质性的改进，并扩展了这些基准以包括个性化FL方法的评估。最后，我们提倡建立一个全面的FL检查点和评估框架，以更好地反映实际环境并提供

    Federated learning (FL) is increasingly being recognized as a key approach to overcoming the data silos that so frequently obstruct the training and deployment of machine-learning models in clinical settings. This work contributes to a growing body of FL research specifically focused on clinical applications along three important directions. First, an extension of the FENDA method (Kim et al., 2016) to the FL setting is proposed. Experiments conducted on the FLamby benchmarks (du Terrail et al., 2022a) and GEMINI datasets (Verma et al., 2017) show that the approach is robust to heterogeneous clinical data and often outperforms existing global and personalized FL techniques. Further, the experimental results represent substantive improvements over the original FLamby benchmarks and expand such benchmarks to include evaluation of personalized FL methods. Finally, we advocate for a comprehensive checkpointing and evaluation framework for FL to better reflect practical settings and provide
    
[^43]: 系统化采样和机器学习参数化在气候模型中的验证

    Systematic Sampling and Validation of Machine Learning-Parameterizations in Climate Models. (arXiv:2309.16177v1 [physics.ao-ph])

    [http://arxiv.org/abs/2309.16177](http://arxiv.org/abs/2309.16177)

    本论文研究了混合物理-机器学习气候模拟的挑战，并通过大规模在线建模错误采样和评估，在机器学习参数化设计中发现了改进性能的策略。

    

    混合物理-机器学习气候模拟的进展受到获取高性能耦合（即在线）模拟的困难的限制。虽然在脱机环境中评估数百个机器学习参数化子网格闭合（如对流和辐射）是直接的，但在相同规模上的在线评估在技术上具有挑战性。我们的软件自动化实现了比以往任何时候都多一个数量级的在线建模错误采样。利用这一点，我们评估混合气候模型的性能，并制定改进策略。我们发现，在包含记忆、相对湿度输入特征转换和额外输入变量的情况下，模型的在线性能有所提高。我们还揭示了在线错误的显著差异以及脱机与在线错误统计之间的不一致性。这意味着需要在线评估数百个候选机器学习模型以检测参数化设计选择的影响。

    Progress in hybrid physics-machine learning (ML) climate simulations has been limited by the difficulty of obtaining performant coupled (i.e. online) simulations. While evaluating hundreds of ML parameterizations of subgrid closures (here of convection and radiation) offline is straightforward, online evaluation at the same scale is technically challenging. Our software automation achieves an order-of-magnitude larger sampling of online modeling errors than has previously been examined. Using this, we evaluate the hybrid climate model performance and define strategies to improve it. We show that model online performance improves when incorporating memory, a relative humidity input feature transformation, and additional input variables. We also reveal substantial variation in online error and inconsistencies between offline vs. online error statistics. The implication is that hundreds of candidate ML models should be evaluated online to detect the effects of parameterization design choi
    
[^44]: FUTURE-AI：在医疗保健领域的可信和可部署人工智能的国际共识指南

    FUTURE-AI: International consensus guideline for trustworthy and deployable artificial intelligence in healthcare. (arXiv:2309.12325v1 [cs.CY])

    [http://arxiv.org/abs/2309.12325](http://arxiv.org/abs/2309.12325)

    FUTURE-AI是第一个国际共识框架，为医疗保健领域的可信AI工具开发和部署提供指导原则和最佳实践。

    

    尽管在医学和医疗保健领域人工智能（AI）取得了重大进展，但AI技术在现实临床实践中的部署和采用仍受限。近年来，人们对医疗AI的技术、临床、伦理和法律风险提出了关注。为了增加在现实世界中的采用，医疗AI工具必须得到患者、临床医生、健康组织和当局的信任和接受。本文描述了FUTURE-AI指南作为第一个用于指导医疗保健领域可信AI工具开发和部署的国际共识框架。FUTURE-AI联盟成立于2021年，目前包括来自51个国家的118位跨学科专家，代表了所有大洲，包括AI科学家、临床医生、伦理学家和社会科学家。在为期两年的时间里，联盟通过迭代过程定义了可信AI的指导原则和最佳实践，其中包括

    Despite major advances in artificial intelligence (AI) for medicine and healthcare, the deployment and adoption of AI technologies remain limited in real-world clinical practice. In recent years, concerns have been raised about the technical, clinical, ethical and legal risks associated with medical AI. To increase real world adoption, it is essential that medical AI tools are trusted and accepted by patients, clinicians, health organisations and authorities. This work describes the FUTURE-AI guideline as the first international consensus framework for guiding the development and deployment of trustworthy AI tools in healthcare. The FUTURE-AI consortium was founded in 2021 and currently comprises 118 inter-disciplinary experts from 51 countries representing all continents, including AI scientists, clinicians, ethicists, and social scientists. Over a two-year period, the consortium defined guiding principles and best practices for trustworthy AI through an iterative process comprising a
    
[^45]: 如何选择体育赛程安排中的算法？

    Which algorithm to select in sports timetabling?. (arXiv:2309.03229v1 [cs.AI])

    [http://arxiv.org/abs/2309.03229](http://arxiv.org/abs/2309.03229)

    本研究通过实例空间分析体育赛程安排问题的特征，提出了一个基于机器学习的算法选择系统，预测了在给定特征下最适合的算法，同时深入了解了算法性能并提出了改进建议。

    

    任何体育竞赛都需要一个赛程安排，确定比赛队伍何时何地相遇。最近的国际赛程安排竞赛(ITC2021)揭示了一个事实，即虽然可能开发出通用算法，但每个算法在问题实例上的性能差异很大。本文在体育赛程安排方面提供了实例空间分析，从而深入了解了八种最先进算法的优势和劣势。基于机器学习技术，我们提出了一个算法选择系统，可以根据体育赛程安排问题实例的特征预测哪种算法在性能上可能表现最佳。此外，我们还确定了哪些特征在做出预测时很重要，从而深入了解了算法的性能，并提出了进一步改进的建议。最后，我们评估了这些实例的经验难度。我们的结果基于大规模计算实验。

    Any sports competition needs a timetable, specifying when and where teams meet each other. The recent International Timetabling Competition (ITC2021) on sports timetabling showed that, although it is possible to develop general algorithms, the performance of each algorithm varies considerably over the problem instances. This paper provides an instance space analysis for sports timetabling, resulting in powerful insights into the strengths and weaknesses of eight state-of-the-art algorithms. Based on machine learning techniques, we propose an algorithm selection system that predicts which algorithm is likely to perform best when given the characteristics of a sports timetabling problem instance. Furthermore, we identify which characteristics are important in making that prediction, providing insights in the performance of the algorithms, and suggestions to further improve them. Finally, we assess the empirical hardness of the instances. Our results are based on large computational exper
    
[^46]: 深度残差网络的隐式正则化与神经常微分方程的关联

    Implicit regularization of deep residual networks towards neural ODEs. (arXiv:2309.01213v1 [stat.ML])

    [http://arxiv.org/abs/2309.01213](http://arxiv.org/abs/2309.01213)

    本文建立了深度残差网络向神经常微分方程的隐式正则化，通过对用梯度流训练的非线性网络的研究，证明了在网络以神经常微分方程的离散化形式初始化后，这种离散化将在整个训练过程中保持不变，并提供了收敛性的条件。

    

    残差神经网络是先进的深度学习模型。它们的连续深度模拟称为神经常微分方程（ODE），也被广泛使用。尽管它们取得了成功，但离散模型与连续模型之间的联系仍缺乏坚实的数学基础。在本文中，我们通过建立一个针对用梯度流训练的非线性网络的深度残差网络向神经常微分方程的隐式正则化来朝着这个方向迈出了一步。我们证明，如果网络的初始化是神经常微分方程的离散化，则这种离散化在整个训练过程中保持不变。我们的结果对于有限的训练时间和训练时间趋于无穷大都成立，只要网络满足Polyak-Lojasiewicz条件。重要的是，这个条件适用于一个残差网络家族，其中残差是两层感知机，在宽度上只是线性超参数化，并且暗示了梯度流的收敛性。

    Residual neural networks are state-of-the-art deep learning models. Their continuous-depth analog, neural ordinary differential equations (ODEs), are also widely used. Despite their success, the link between the discrete and continuous models still lacks a solid mathematical foundation. In this article, we take a step in this direction by establishing an implicit regularization of deep residual networks towards neural ODEs, for nonlinear networks trained with gradient flow. We prove that if the network is initialized as a discretization of a neural ODE, then such a discretization holds throughout training. Our results are valid for a finite training time, and also as the training time tends to infinity provided that the network satisfies a Polyak-Lojasiewicz condition. Importantly, this condition holds for a family of residual networks where the residuals are two-layer perceptrons with an overparameterization in width that is only linear, and implies the convergence of gradient flow to
    
[^47]: 我们可以从量子卷积神经网络中学到什么？

    What can we learn from quantum convolutional neural networks?. (arXiv:2308.16664v1 [quant-ph])

    [http://arxiv.org/abs/2308.16664](http://arxiv.org/abs/2308.16664)

    通过分析量子卷积神经网络（QCNNs），我们发现它们通过隐藏特征映射嵌入物理系统参数，并且利用量子临界性生成适合的基函数集，池化层选择能够形成高性能决策边界的基函数，而模型的泛化性能依赖于嵌入类型。

    

    通过分析量子卷积神经网络（QCNNs），我们可以得出以下结论：1）通过隐藏特征映射，工作于量子数据可以被视为嵌入物理系统参数；2）对于量子相位识别，其高性能可以归因于在基态嵌入期间生成非常适合的基函数集，其中自旋模型的量子临界性导致具有快速变化特征的基函数；3）QCNN的池化层负责选择那些能够有助于形成高性能决策边界的基函数，学习过程对应于适应性测量，使得少量量子比特算符映射到整个寄存器可观测量；4）QCNN模型的泛化强烈依赖于嵌入类型，基于傅里叶基的旋转特征映射需要仔细的特征工程；5）基于有限数量的测量次数的读出的QCNN的准确性和泛化能力倾向于地面态。

    We can learn from analyzing quantum convolutional neural networks (QCNNs) that: 1) working with quantum data can be perceived as embedding physical system parameters through a hidden feature map; 2) their high performance for quantum phase recognition can be attributed to generation of a very suitable basis set during the ground state embedding, where quantum criticality of spin models leads to basis functions with rapidly changing features; 3) pooling layers of QCNNs are responsible for picking those basis functions that can contribute to forming a high-performing decision boundary, and the learning process corresponds to adapting the measurement such that few-qubit operators are mapped to full-register observables; 4) generalization of QCNN models strongly depends on the embedding type, and that rotation-based feature maps with the Fourier basis require careful feature engineering; 5) accuracy and generalization of QCNNs with readout based on a limited number of shots favor the groun
    
[^48]: 基于熵的指导深度神经网络加速收敛和改善性能

    Entropy-based Guidance of Deep Neural Networks for Accelerated Convergence and Improved Performance. (arXiv:2308.14938v1 [cs.CV])

    [http://arxiv.org/abs/2308.14938](http://arxiv.org/abs/2308.14938)

    本研究通过引入基于熵的损失项，通过测量神经网络处理数据时的熵变化，指导神经网络以更快速的收敛、更好的性能学习丰富的潜在数据表示。

    

    神经网络极大地增加了我们从大规模、高维度数据集中学习的能力，跨越无数学科。然而，它们的决策不易解释，计算成本高，建立和训练它们是不确定的过程。为了给这些努力增加结构，我们推导出了新的数学结果，以高效地测量全连接和卷积神经网络处理数据时的熵变化，并引入了基于熵的损失项。在基准数据集上进行的图像压缩和图像分类实验表明，这些损失项指导神经网络以更少的维度学习丰富的潜在数据表示，收敛于更少的训练轮次，并取得更好的测试指标。

    Neural networks have dramatically increased our capacity to learn from large, high-dimensional datasets across innumerable disciplines. However, their decisions are not easily interpretable, their computational costs are high, and building and training them are uncertain processes. To add structure to these efforts, we derive new mathematical results to efficiently measure the changes in entropy as fully-connected and convolutional neural networks process data, and introduce entropy-based loss terms. Experiments in image compression and image classification on benchmark datasets demonstrate these losses guide neural networks to learn rich latent data representations in fewer dimensions, converge in fewer training epochs, and achieve better test metrics.
    
[^49]: 抓住它们：图匹配匹配滤波中的解决方案多样化

    Gotta match 'em all: Solution diversification in graph matching matched filters. (arXiv:2308.13451v1 [stat.ML])

    [http://arxiv.org/abs/2308.13451](http://arxiv.org/abs/2308.13451)

    本文提出了一种在大规模背景图中查找多个嵌入的模板图的新方法，通过迭代惩罚相似度矩阵来实现多样化匹配的发现，并提出了算法加速措施。在理论验证和实验证明中，证明了该方法的可行性和实用性。

    

    我们提出了一种在非常大的背景图中查找多个嵌入在其中的模板图的新方法。我们的方法基于Sussman等人提出的图匹配匹配滤波技术，通过在匹配滤波算法中迭代地惩罚合适的节点对相似度矩阵来实现多样化匹配的发现。此外，我们提出了算法加速，极大地提高了我们的匹配滤波方法的可扩展性。我们在相关的Erdos-Renyi图设置中对我们的方法进行了理论上的验证，显示其在温和的模型条件下能够顺序地发现多个模板。我们还通过使用模拟模型和真实世界数据集（包括人脑连接组和大型交易知识库）进行了大量实验证明了我们方法的实用性。

    We present a novel approach for finding multiple noisily embedded template graphs in a very large background graph. Our method builds upon the graph-matching-matched-filter technique proposed in Sussman et al., with the discovery of multiple diverse matchings being achieved by iteratively penalizing a suitable node-pair similarity matrix in the matched filter algorithm. In addition, we propose algorithmic speed-ups that greatly enhance the scalability of our matched-filter approach. We present theoretical justification of our methodology in the setting of correlated Erdos-Renyi graphs, showing its ability to sequentially discover multiple templates under mild model conditions. We additionally demonstrate our method's utility via extensive experiments both using simulated models and real-world dataset, include human brain connectomes and a large transactional knowledge base.
    
[^50]: 物流集散地位置优化：一种基于K-Means和P-Median模型的混合方法，利用道路网络距离

    Logistics Hub Location Optimization: A K-Means and P-Median Model Hybrid Approach Using Road Network Distances. (arXiv:2308.11038v1 [math.OC])

    [http://arxiv.org/abs/2308.11038](http://arxiv.org/abs/2308.11038)

    本研究基于K-Means和P-Median模型提出了一种混合方法，通过使用道路网络距离来优化在城市环境下物流集散地的位置布置，以减少配送距离和碳足迹。

    

    物流集散地在最后一公里配送距离中起着关键作用；即使距离微小增加也会对电子商务行业的业务产生负面影响，同时还会增加其碳足迹。特别是在Covid-19之后，该行业的增长进一步加剧了在城市环境中优化资源分配的需求。在这项研究中，我们使用了一种混合方法来优化物流集散地的布置。该方法依次采用不同的技术。首先，根据它们的空间位置，使用K-Means对交付点进行聚类。聚类方法使用道路网络距离，而不是欧几里德距离。避免使用非基于道路网络的方法，因为它们会导致错误和误导性结果。最后，使用P-Median方法确定集散地的位置。P-Median方法还将交付数量和人口作为权重考虑在内。使用Muller和Phipps（M＆P）的实际交付数据

    Logistic hubs play a pivotal role in the last-mile delivery distance; even a slight increment in distance negatively impacts the business of the e-commerce industry while also increasing its carbon footprint. The growth of this industry, particularly after Covid-19, has further intensified the need for optimized allocation of resources in an urban environment. In this study, we use a hybrid approach to optimize the placement of logistic hubs. The approach sequentially employs different techniques. Initially, delivery points are clustered using K-Means in relation to their spatial locations. The clustering method utilizes road network distances as opposed to Euclidean distances. Non-road network-based approaches have been avoided since they lead to erroneous and misleading results. Finally, hubs are located using the P-Median method. The P-Median method also incorporates the number of deliveries and population as weights. Real-world delivery data from Muller and Phipps (M&P) is used to 
    
[^51]: 点击率预测的时间兴趣网络

    Temporal Interest Network for Click-Through Rate Prediction. (arXiv:2308.08487v1 [cs.IR])

    [http://arxiv.org/abs/2308.08487](http://arxiv.org/abs/2308.08487)

    本文提出了时间兴趣网络（TIN），用于捕捉行为与目标之间的四重语义和时间相关性，以预测点击率的效果和已有方法对这种相关性的学习程度尚不清楚。

    

    用户行为的历史是预测点击率最重要的特征之一，因为它们与目标项目具有强烈的语义和时间相关性。虽然已有文献分别研究了这些相关性，但尚未分析它们的组合，即行为语义、目标语义、行为时间和目标时间的四重相关性。这种相关性对性能的影响以及现有方法学习这种相关性的程度尚不清楚。为了填补这一空白，我们在实践中测量了四重相关性，并观察到直观而强大的四重模式。我们测量了几种代表性的用户行为方法的学习相关性，但令人惊讶的是，它们都没有学习到这样的模式，特别是时间模式。在本文中，我们提出了时间兴趣网络（TIN）来捕捉行为与目标之间的四重语义和时间相关性。

    The history of user behaviors constitutes one of the most significant characteristics in predicting the click-through rate (CTR), owing to their strong semantic and temporal correlation with the target item. While the literature has individually examined each of these correlations, research has yet to analyze them in combination, that is, the quadruple correlation of (behavior semantics, target semantics, behavior temporal, and target temporal). The effect of this correlation on performance and the extent to which existing methods learn it remain unknown. To address this gap, we empirically measure the quadruple correlation and observe intuitive yet robust quadruple patterns. We measure the learned correlation of several representative user behavior methods, but to our surprise, none of them learn such a pattern, especially the temporal one.  In this paper, we propose the Temporal Interest Network (TIN) to capture the quadruple semantic and temporal correlation between behaviors and th
    
[^52]: 它其实不那么糟糕：理解生成变换模型对OOD泛化的神秘性能下降

    It Ain't That Bad: Understanding the Mysterious Performance Drop in OOD Generalization for Generative Transformer Models. (arXiv:2308.08268v1 [cs.LG])

    [http://arxiv.org/abs/2308.08268](http://arxiv.org/abs/2308.08268)

    生成变换模型在OOD泛化方面存在神秘的性能下降。研究人员观察到模型在训练和泛化数字运算时的行为之间的不一致，并尝试提出解决方案，但仍未解决本质机制。

    

    生成变换模型在解决各种问题上取得了显著的成就。然而，它们的泛化能力并没有完全被理解，并且并不总是令人满意。研究人员从基本的数学任务（如n位数的加法或乘法）开始，作为重要视角来研究模型的泛化行为。有趣的是，观察到当模型在n位数运算（例如加法）上进行训练时，模型在未见过的长度为n位的输入上可以成功泛化（即内分布泛化），但在长度更长、未见过的情况下（即外分布泛化）会失败并且表现神秘。研究尝试通过修改位置嵌入、微调和引入更广泛或更有指导性的数据来弥合这一差距。然而，如果不解决本质机制，这些解决方案的稳健性几乎没有任何保证。

    Generative Transformer-based models have achieved remarkable proficiency on solving diverse problems. However, their generalization ability is not fully understood and not always satisfying. Researchers take basic mathematical tasks like n-digit addition or multiplication as important perspectives for investigating their generalization behaviors. Curiously, it is observed that when training on n-digit operations (e.g., additions) in which both input operands are n-digit in length, models generalize successfully on unseen n-digit inputs (in-distribution (ID) generalization), but fail miserably and mysteriously on longer, unseen cases (out-of-distribution (OOD) generalization). Studies try to bridge this gap with workarounds such as modifying position embedding, fine-tuning, and priming with more extensive or instructive data. However, without addressing the essential mechanism, there is hardly any guarantee regarding the robustness of these solutions. We bring this unexplained performan
    
[^53]: 通过填充和置换指纹编码的方法，对差分隐私算法提供平滑下界

    Smooth Lower Bounds for Differentially Private Algorithms via Padding-and-Permuting Fingerprinting Codes. (arXiv:2307.07604v1 [cs.CR])

    [http://arxiv.org/abs/2307.07604](http://arxiv.org/abs/2307.07604)

    本论文提出了一种通过填充和置换指纹编码的方法来产生困难实例，从而在各种情景下提供平滑下界。这方法适用于差分隐私平均问题和近似k.

    

    指纹编码方法是最广泛用于确定约束差分隐私算法的样本复杂度或错误率的方法。然而，对于许多差分隐私问题，我们并不知道适当的下界，并且即使对于我们知道的问题，下界也不平滑，并且通常在误差大于某个阈值时变得无意义。在这项工作中，我们通过将填充和置换转换应用于指纹编码，提出了一种生成困难实例的简单方法。我们通过在不同情景下提供新的下界来说明这种方法的适用性：1. 低准确度情景下差分隐私平均问题的紧密下界，这尤其意味着新的私有1簇问题的下界 2. 近似k

    Fingerprinting arguments, first introduced by Bun, Ullman, and Vadhan (STOC 2014), are the most widely used method for establishing lower bounds on the sample complexity or error of approximately differentially private (DP) algorithms. Still, there are many problems in differential privacy for which we don't know suitable lower bounds, and even for problems that we do, the lower bounds are not smooth, and usually become vacuous when the error is larger than some threshold.  In this work, we present a simple method to generate hard instances by applying a padding-and-permuting transformation to a fingerprinting code. We illustrate the applicability of this method by providing new lower bounds in various settings:  1. A tight lower bound for DP averaging in the low-accuracy regime, which in particular implies a new lower bound for the private 1-cluster problem introduced by Nissim, Stemmer, and Vadhan (PODS 2016).  2. A lower bound on the additive error of DP algorithms for approximate k
    
[^54]: 基于图神经网络的太赫兹流导向纳米尺度定位

    Graph Neural Network-enabled Terahertz-based Flow-guided Nanoscale Localization. (arXiv:2307.05551v1 [cs.LG])

    [http://arxiv.org/abs/2307.05551](http://arxiv.org/abs/2307.05551)

    本研究提出了一种基于图神经网络的太赫兹流导向纳米尺度定位方法，可以提高定位精度和覆盖范围，解决了现有方法的定位精度低和无法全局定位的问题。

    

    纳米技术和先进材料的科学进展为体内精准医学的纳米尺度装置铺平了道路，其中包括集成感应、计算、通信、数据和能量存储能力。在人体心血管系统中，这些装置被设想为被动流动并持续感知以便检测诊断感兴趣的事件。通过将这些事件的物理位置（如身体区域）分配给它们，可以提高检测到这些事件的诊断价值，这是流导向定位的主要命题。当前的流导向定位方法存在定位精度低和无法在整个心血管系统内本地化事件的问题。为了解决这个问题，我们提出利用图神经网络（GNNs）来进行定位，并证明我们的方法在定位精度和覆盖范围上优于现有的最先进方法。

    Scientific advancements in nanotechnology and advanced materials are paving the way toward nanoscale devices for in-body precision medicine; comprising integrated sensing, computing, communication, data and energy storage capabilities. In the human cardiovascular system, such devices are envisioned to be passively flowing and continuously sensing for detecting events of diagnostic interest. The diagnostic value of detecting such events can be enhanced by assigning to them their physical locations (e.g., body region), which is the main proposition of flow-guided localization. Current flow-guided localization approaches suffer from low localization accuracy and they are by-design unable to localize events within the entire cardiovascular system. Toward addressing this issue, we propose the utilization of Graph Neural Networks (GNNs) for this purpose, and demonstrate localization accuracy and coverage enhancements of our proposal over the existing State of the Art (SotA) approaches. Based
    
[^55]: Engression: 非线性回归的外推方法

    Engression: Extrapolation for Nonlinear Regression?. (arXiv:2307.00835v1 [stat.ME])

    [http://arxiv.org/abs/2307.00835](http://arxiv.org/abs/2307.00835)

    Engression是一种非线性回归方法，通过使用分布回归技术和预加性噪声模型，在训练样本范围边界外也能可靠地进行外推。

    

    外推对于许多统计学和机器学习应用至关重要，因为常常会遇到超出训练样本范围的测试数据。然而，对于非线性模型来说，外推是一个巨大的挑战。传统模型在这方面通常遇到困难：树集成模型在支持范围外提供连续的预测，而神经网络的预测往往变得不可控。这项工作旨在提供一种非线性回归方法，其可靠性在训练样本范围边界不会立即崩溃。我们的主要贡献是一种名为“engression”的新方法，它是一种预加性噪声模型的分布回归技术，其中噪声添加到协变量上并应用非线性转换。我们的实验结果表明，该模型通常适用于许多真实数据集。我们展示engression可以在一些假设下成功进行外推，例如严格限制噪声大小。

    Extrapolation is crucial in many statistical and machine learning applications, as it is common to encounter test data outside the training support. However, extrapolation is a considerable challenge for nonlinear models. Conventional models typically struggle in this regard: while tree ensembles provide a constant prediction beyond the support, neural network predictions tend to become uncontrollable. This work aims at providing a nonlinear regression methodology whose reliability does not break down immediately at the boundary of the training support. Our primary contribution is a new method called `engression' which, at its core, is a distributional regression technique for pre-additive noise models, where the noise is added to the covariates before applying a nonlinear transformation. Our experimental results indicate that this model is typically suitable for many real data sets. We show that engression can successfully perform extrapolation under some assumptions such as a strictl
    
[^56]: 通过在线信心预测实现具备形式安全保证的贝叶斯优化

    Bayesian Optimization with Formal Safety Guarantees via Online Conformal Prediction. (arXiv:2306.17815v1 [cs.LG])

    [http://arxiv.org/abs/2306.17815](http://arxiv.org/abs/2306.17815)

    本文提出了一种基于贝叶斯优化的方法，无论约束函数的特性如何，都能满足安全要求。

    

    黑盒零阶优化是金融、物理和工程等领域应用的核心基本操作。在这个问题的常见形式中，设计者顺序尝试候选解，并从系统中接收到关于每个尝试值的噪声反馈。本文研究了在这些场景中还提供了有关尝试解的安全性的反馈，并且优化器被限制在整个优化过程中尝试的不安全解的数量上。在基于贝叶斯优化（BO）的方法上，先前的研究引入了一种被称为SAFEOPT的优化方案，只要满足对安全约束函数的严格假设，就能够以可控的概率在反馈噪声上避免选择任何不安全的解。本文介绍了一种新的基于BO的方法，无论约束函数的特性如何，都能满足安全要求。

    Black-box zero-th order optimization is a central primitive for applications in fields as diverse as finance, physics, and engineering. In a common formulation of this problem, a designer sequentially attempts candidate solutions, receiving noisy feedback on the value of each attempt from the system. In this paper, we study scenarios in which feedback is also provided on the safety of the attempted solution, and the optimizer is constrained to limit the number of unsafe solutions that are tried throughout the optimization process. Focusing on methods based on Bayesian optimization (BO), prior art has introduced an optimization scheme -- referred to as SAFEOPT -- that is guaranteed not to select any unsafe solution with a controllable probability over feedback noise as long as strict assumptions on the safety constraint function are met. In this paper, a novel BO-based approach is introduced that satisfies safety requirements irrespective of properties of the constraint function. This s
    
[^57]: 基于深度学习的延迟线探测器时空多事件重构

    Deep Learning-Based Spatiotemporal Multi-Event Reconstruction for Delay Line Detectors. (arXiv:2306.09359v1 [physics.ins-det])

    [http://arxiv.org/abs/2306.09359](http://arxiv.org/abs/2306.09359)

    本文提出了基于深度学习的延迟线探测器时空多事件重构方法，可以成功地重构多个粒子的空间和时间坐标，相比传统方法更加高效和准确。

    

    现代物理学中，观察非常短的时间窗口内两个或更多粒子的位置一直是一项挑战。对于低能电子，可以使用微通道板和延迟线的组合构成延迟线探测器，以读出入射粒子的位置。这种方法可以完全重构多个粒子的空间和时间坐标。然而，对于多个接近的粒子，传统的峰值查找算法无法有效识别和重构事件。为了解决这个问题，我们提出了一个新的时空机器学习模型，使用卷积和循环神经网络在短时间窗口内提取和组合多个粒子的空间和时间特征，成功地重建了包含多达四个粒子的事件。

    Accurate observation of two or more particles within a very narrow time window has always been a challenge in modern physics. It creates the possibility of correlation experiments, such as the ground-breaking Hanbury Brown-Twiss experiment, leading to new physical insights. For low-energy electrons, one possibility is to use a microchannel plate with subsequent delay lines for the readout of the incident particle hits, a setup called a Delay Line Detector. The spatial and temporal coordinates of more than one particle can be fully reconstructed outside a region called the dead radius. For interesting events, where two electrons are close in space and time, the determination of the individual positions of the electrons requires elaborate peak finding algorithms. While classical methods work well with single particle hits, they fail to identify and reconstruct events caused by multiple nearby particles. To address this challenge, we present a new spatiotemporal machine learning model to 
    
[^58]: 针对类内分布转移的过度遗忘：连续学习的脆弱性

    Out-of-distribution forgetting: vulnerability of continual learning to intra-class distribution shift. (arXiv:2306.00427v1 [cs.LG])

    [http://arxiv.org/abs/2306.00427](http://arxiv.org/abs/2306.00427)

    连续学习中存在一种特殊形式的灾难性遗忘——越界遗忘，当给定类别引入类内分布转移时，它会显着削弱该类别的连续学习方法的识别准确率。

    

    连续学习是让人工神经网络在开放环境中工作的重要技术。在联合学习中，人们已经知道由意图攻击或环境扰动引起的越界问题严重影响网络的泛化能力。在这项工作中，我们报告了连续学习设置中由越界问题引起的一种特殊形式的灾难性遗忘，我们将其称为越界遗忘（OODF）。在连续图像分类任务中，我们发现，针对给定类别，引入类内分布转移显着削弱了后续学习过程中该类别的连续学习方法的识别准确率。有趣的是，这种现象对于连续学习而言是特殊的，因为同样级别的分布转移只有微不足道的影响。

    Continual learning (CL) is an important technique to allow artificial neural networks to work in open environments. CL enables a system to learn new tasks without severe interference to its performance on old tasks, i.e., overcome the problems of catastrophic forgetting. In joint learning, it is well known that the out-of-distribution (OOD) problem caused by intentional attacks or environmental perturbations will severely impair the ability of networks to generalize. In this work, we reported a special form of catastrophic forgetting raised by the OOD problem in continual learning settings, and we named it out-of-distribution forgetting (OODF). In continual image classification tasks, we found that for a given category, introducing an intra-class distribution shift significantly impaired the recognition accuracy of CL methods for that category during subsequent learning. Interestingly, this phenomenon is special for CL as the same level of distribution shift had only negligible effects
    
[^59]: FedMR：基于模型重组的联邦学习

    FedMR: Federated Learning via Model Recombination. (arXiv:2305.10730v1 [cs.LG])

    [http://arxiv.org/abs/2305.10730](http://arxiv.org/abs/2305.10730)

    FedMR是一种基于模型重组的联邦学习范式，可以通过混洗和重组每个层的本地模型来缓解子优或有偏局部模型的问题，从而在推理性能方面表现出色。

    

    联邦学习（FL）使得客户端在不泄露原始数据的情况下进行全局模型训练，但现有的基于联邦平均（FedAvg）的方法在推理性能方面存在问题，特别是在客户端数据分布不均匀时。本文提出了一种名为FedMR（联邦模型重组）的新型联邦学习范式，通过对收集到的本地模型的每个层进行混洗和重组，在客户端进行局部训练时可以获得具有多样的初始模型的新模型，从而缓解了子优或有偏局部模型的问题。

    Although Federated Learning (FL) enables global model training across clients without compromising their raw data, existing Federated Averaging (FedAvg)-based methods suffer from the problem of low inference performance, especially for unevenly distributed data among clients. This is mainly because i) FedAvg initializes client models with the same global models, which makes the local training hard to escape from the local search for optimal solutions; and ii) by averaging model parameters in a coarse manner, FedAvg eclipses the individual characteristics of local models. To address such issues that strongly limit the inference capability of FL, we propose a novel and effective FL paradigm named FedMR (Federated Model Recombination). Unlike conventional FedAvg-based methods, the cloud server of FedMR shuffles each layer of collected local models and recombines them to achieve new models for local training on clients. Due to the diversified initialization models for clients coupled with 
    
[^60]: 可证黑盒攻击：确保对抗性样本的攻击成功率

    Certifiable Black-Box Attack: Ensuring Provably Successful Attack for Adversarial Examples. (arXiv:2304.04343v1 [cs.LG])

    [http://arxiv.org/abs/2304.04343](http://arxiv.org/abs/2304.04343)

    本文提出可证黑盒攻击，能够保证攻击成功率，设计了多种新技术。

    

    黑盒对抗攻击具有破坏机器学习模型的强大潜力。现有的黑盒对抗攻击通过迭代查询目标模型和/或利用本地代理模型的可转移性来制作对抗样本。当实验设计攻击时，攻击是否成功对攻击者来说仍然是未知的。本文通过修改随机平滑性理论，首次研究了可证黑盒攻击的新范例，能够保证制作的对抗样本的攻击成功率，为此设计了多种新技术。

    Black-box adversarial attacks have shown strong potential to subvert machine learning models. Existing black-box adversarial attacks craft the adversarial examples by iteratively querying the target model and/or leveraging the transferability of a local surrogate model. Whether such attack can succeed remains unknown to the adversary when empirically designing the attack. In this paper, to our best knowledge, we take the first step to study a new paradigm of adversarial attacks -- certifiable black-box attack that can guarantee the attack success rate of the crafted adversarial examples. Specifically, we revise the randomized smoothing to establish novel theories for ensuring the attack success rate of the adversarial examples. To craft the adversarial examples with the certifiable attack success rate (CASR) guarantee, we design several novel techniques, including a randomized query method to query the target model, an initialization method with smoothed self-supervised perturbation to
    
[^61]: FakET: 利用神经风格迁移模拟冷冻电子断层图像

    FakET: Simulating Cryo-Electron Tomograms with Neural Style Transfer. (arXiv:2304.02011v1 [cs.LG])

    [http://arxiv.org/abs/2304.02011](http://arxiv.org/abs/2304.02011)

    本文提出了一种使用加性噪声和神经风格迁移技术来模拟电子显微镜正向算子，以解决深度学习方法需要大量训练数据集的问题。该方法在粒子定位和分类任务上表现良好。

    

    粒子定位和分类是计算显微学中最基本的问题之一。近年来，深度学习方法在这些任务中取得了巨大成功。这些监督式学习方法的一个关键缺点是它们需要大量的训练数据集，通常是与模拟透射电子显微镜物理的复杂数值正向模型中的粒子模型结合生成的。这些模型的计算机实现非常耗费计算资源，限制了它们的适用范围。本文提出了一种基于加性噪声和神经风格迁移技术模拟电子显微镜正向算子的简单方法。我们使用目前最先进的已经建立的状态之一对定位和分类任务进行评估，显示出与基准测试相当的性能。与以前的方法不同，我们的方法加速了运算，显著减少了计算成本。

    Particle localization and -classification constitute two of the most fundamental problems in computational microscopy. In recent years, deep learning based approaches have been introduced for these tasks with great success. A key shortcoming of these supervised learning methods is their need for large training data sets, typically generated from particle models in conjunction with complex numerical forward models simulating the physics of transmission electron microscopes. Computer implementations of such forward models are computationally extremely demanding and limit the scope of their applicability. In this paper we propose a simple method for simulating the forward operator of an electron microscope based on additive noise and Neural Style Transfer techniques. We evaluate the method on localization and classification tasks using one of the established state-of-the-art architectures showing performance on par with the benchmark. In contrast to previous approaches, our method acceler
    
[^62]: 对知识图谱进行逻辑查询应答的关系模式建模

    Modeling Relational Patterns for Logical Query Answering over Knowledge Graphs. (arXiv:2303.11858v1 [cs.DB])

    [http://arxiv.org/abs/2303.11858](http://arxiv.org/abs/2303.11858)

    本文介绍了一种新的查询嵌入方法RoConE，它允许学习关系模式并提高了逻辑查询推理的性能。

    

    对知识图谱（KG）进行一阶逻辑（FOL）查询的回答仍然是一项具有挑战性的任务，主要是由于KG不完整性而导致的。查询嵌入方法通过计算实体、关系和逻辑查询的低维度向量表示来解决这个问题。KG表现出对称性和组合性等关系模式，建模这些模式可以进一步提高查询嵌入模型的性能。然而，这些模式在查询嵌入模型中的作用尚未在文献中进行研究。在本文中，我们填补了这一研究空白，并通过引入允许学习关系模式的归纳偏差，加强FOL查询推理的模式推理能力。为此，我们开发了一种新的查询嵌入方法RoConE，它将查询区域定义为几何锥体，并通过在复杂空间中旋转代数查询算子。RoConE结合了几何锥体作为可以明确定义查询表示的几何表示和旋转代数查询算子的优势。

    Answering first-order logical (FOL) queries over knowledge graphs (KG) remains a challenging task mainly due to KG incompleteness. Query embedding approaches this problem by computing the low-dimensional vector representations of entities, relations, and logical queries. KGs exhibit relational patterns such as symmetry and composition and modeling the patterns can further enhance the performance of query embedding models. However, the role of such patterns in answering FOL queries by query embedding models has not been yet studied in the literature. In this paper, we fill in this research gap and empower FOL queries reasoning with pattern inference by introducing an inductive bias that allows for learning relation patterns. To this end, we develop a novel query embedding method, RoConE, that defines query regions as geometric cones and algebraic query operators by rotations in complex space. RoConE combines the advantages of Cone as a well-specified geometric representation for query e
    
[^63]: 带应用于变分蒙特卡罗模拟的参数化球的随机梯度下降的收敛性分析。

    Convergence of stochastic gradient descent on parameterized sphere with applications to variational Monte Carlo simulation. (arXiv:2303.11602v1 [cs.LG])

    [http://arxiv.org/abs/2303.11602](http://arxiv.org/abs/2303.11602)

    本论文在高维度球体上，通过神经网络参数化，使用SGD算法在监督学习和无监督学习中，提供了一种新算法，并且证明了其收敛性。

    

    本文分析了在由神经网络参数化为常数倍的高维球上使用随机梯度下降（SGD）类型算法。我们为有监督学习提供了一种新算法，并在理论和数值上证明了其收敛性。我们还首次提供了无监督设置的收敛证明，该设置对应于量子物理学中广泛使用的变分蒙特卡罗（VMC）方法。

    We analyze stochastic gradient descent (SGD) type algorithms on a high-dimensional sphere which is parameterized by a neural network up to a normalization constant. We provide a new algorithm for the setting of supervised learning and show its convergence both theoretically and numerically. We also provide the first proof of convergence for the unsupervised setting, which corresponds to the widely used variational Monte Carlo (VMC) method in quantum physics.
    
[^64]: 利用神经网络和空间分解在电力系统动力学中求解微分代数方程

    Solving Differential-Algebraic Equations in Power Systems Dynamics with Neural Networks and Spatial Decomposition. (arXiv:2303.10256v1 [eess.SY])

    [http://arxiv.org/abs/2303.10256](http://arxiv.org/abs/2303.10256)

    本文提出了一种使用神经网络和空间分解来近似电力系统动力学微分代数方程的方法，旨在加速仿真，提高数值稳定性和精度。

    

    电力系统的动力学由一组微分代数方程描述。时间域仿真用于理解系统动态的演变。由于系统的刚度需要使用精细离散化的时间步长，因此这些仿真可能具有计算代价较高的特点。通过增加允许的时间步长，我们旨在加快这样的仿真。本文使用观察结果，即尽管各个组件使用代数和微分方程来描述，但它们的耦合仅涉及代数方程的观察结果，利用神经网络（NN）来近似组件状态演变，从而产生快速、准确和数值稳定的近似器，使得可以使用更大的时间步长。为了解释网络对组件以及组件对网络的影响，NN将耦合代数变量的时间演化作为其预测的输入。我们最初使用空间分解方法来估计NN，其中系统被分成空间区域，每个区域有单独的NN估计器。我们将基于NN的仿真与传统的数值积分方案进行比较，以展示我们的方法的有效性。

    The dynamics of the power system are described by a system of differential-algebraic equations. Time-domain simulations are used to understand the evolution of the system dynamics. These simulations can be computationally expensive due to the stiffness of the system which requires the use of finely discretized time-steps. By increasing the allowable time-step size, we aim to accelerate such simulations. In this paper, we use the observation that even though the individual components are described using both algebraic and differential equations, their coupling only involves algebraic equations. Following this observation, we use Neural Networks (NNs) to approximate the components' state evolution, leading to fast, accurate, and numerically stable approximators, which enable larger time-steps. To account for effects of the network on the components and vice-versa, the NNs take the temporal evolution of the coupling algebraic variables as an input for their prediction. We initially estima
    
[^65]: 分布式黑盒攻击云服务的图像分类器

    Distributed Black-box Attack against Image Classification Cloud Services. (arXiv:2210.16371v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.16371](http://arxiv.org/abs/2210.16371)

    本文研究了分布式黑盒攻击云服务的图像分类器，通过直接应用于云API而不是本地模型，避免了之前研究中的错误，并利用负载平衡实现了攻击时间的减少。

    

    黑盒对抗攻击可以欺骗图像分类器，在不需要访问模型结构和权重的情况下对图像进行错误分类。最近的研究报告了超过95%的攻击成功率，查询次数少于1000次。然后产生了一个问题，黑盒攻击是否已经成为依赖云API实现图像分类的物联网设备的真正威胁。为了解决这个问题，值得注意的是之前的研究主要集中在提高成功率和减少查询次数。然而，对于黑盒攻击云API而言，攻击所需的时间也是一个关键因素。本文将黑盒攻击直接应用于云API，而不是本地模型，从而避免了之前研究中在图像编码和预处理之前应用扰动造成的错误。此外，我们利用负载平衡实现了分布式黑盒攻击，可以将攻击时间缩短约五倍。

    Black-box adversarial attacks can fool image classifiers into misclassifying images without requiring access to model structure and weights. Recent studies have reported attack success rates of over 95% with less than 1,000 queries. The question then arises of whether black-box attacks have become a real threat against IoT devices that rely on cloud APIs to achieve image classification. To shed some light on this, note that prior research has primarily focused on increasing the success rate and reducing the number of queries. However, another crucial factor for black-box attacks against cloud APIs is the time required to perform the attack. This paper applies black-box attacks directly to cloud APIs rather than to local models, thereby avoiding mistakes made in prior research that applied the perturbation before image encoding and pre-processing. Further, we exploit load balancing to enable distributed black-box attacks that can reduce the attack time by a factor of about five for both
    

