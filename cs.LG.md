# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Online Variational Sequential Monte Carlo](https://rss.arxiv.org/abs/2312.12616) | 本文提出了一种在线学习的算法，名为在线VSMC，它基于变分顺序蒙特卡洛方法，在处理数据流时能够实时进行模型参数估计和粒子提议适应。 |
| [^2] | [Jamba: A Hybrid Transformer-Mamba Language Model](https://arxiv.org/abs/2403.19887) | Jamba是一个基于混合Transformer-Mamba架构的语言模型，在单个80GB GPU上实现了强大的性能，对标准语言模型基准和长上下文评估具有state-of-the-art的表现。 |
| [^3] | [Tractable Local Equilibria in Non-Concave Games](https://arxiv.org/abs/2403.08171) | 提出了一个新的解决概念，$(\varepsilon, \Phi(\delta))$-局部均衡，以解决在非凹游戏中局部均衡存在但难以处理的问题。 |
| [^4] | [ENOT: Expectile Regularization for Fast and Accurate Training of Neural Optimal Transport](https://arxiv.org/abs/2403.03777) | 通过期望回归正则化，本论文提出了一种新的神经优化传输（NOT）训练程序扩展，能够有效地估计最优输运方案，并使学习变得稳定。 |
| [^5] | [Deep Configuration Performance Learning: A Systematic Survey and Taxonomy](https://arxiv.org/abs/2403.03322) | 性能是可配置软件系统行为的关键属性，本文针对深度学习在可配置软件性能学习方面进行了全面的调查与分类研究。 |
| [^6] | [NeuraLUT: Hiding Neural Network Density in Boolean Synthesizable Functions](https://arxiv.org/abs/2403.00849) | 改进了FPGA加速神经网络推断任务的方法，提出将整个子网络映射到单个LUT中，使得神经网络拓扑和精度不再影响生成的查找表的大小。 |
| [^7] | [Thermodynamics-informed super-resolution of scarce temporal dynamics data](https://arxiv.org/abs/2402.17506) | 提出了一种利用热力学感知神经网络来提高物理系统测量分辨率并预测时间演化的方法，采用对抗自动编码器和结构保持神经网络相结合的方式，可有效解决超分辨率问题，并确保满足热力学定律。 |
| [^8] | [Accelerating Diffusion Sampling with Optimized Time Steps](https://arxiv.org/abs/2402.17376) | 提出了一个通用框架用于设计优化问题，旨在通过寻找更合适的时间步长加速扩散采样。 |
| [^9] | [Covariance-Adaptive Least-Squares Algorithm for Stochastic Combinatorial Semi-Bandits](https://arxiv.org/abs/2402.15171) | 提出了一种协方差自适应的最小二乘算法，利用在线估计协方差结构，相对于基于代理方差的算法获得改进的遗憾上界，特别在协方差系数全为非负时，能有效地利用半臂反馈，并在各种参数设置下表现优异。 |
| [^10] | [Reinforcement Learning with Elastic Time Steps](https://arxiv.org/abs/2402.14961) | SEAC是一种弹性时间步长的离策略演员-评论家算法，通过可变持续时间的时间步长，使代理能够根据情况改变控制频率，在模拟环境中表现优异。 |
| [^11] | [ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models](https://arxiv.org/abs/2402.13516) | 本文介绍了一种名为"ProSparse"的有效稀疏化方法，以推动大型语言模型实现更高的激活稀疏性而不降低模型性能 |
| [^12] | [Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive](https://arxiv.org/abs/2402.13228) | 在这项工作中，我们提出了一种新的损失函数和训练过程DPO-Positive（DPOP），以避免直接偏好优化（DPO）中潜在的失败模式，并发现DPOP明显优于DPO。 |
| [^13] | [Bayesian Reward Models for LLM Alignment](https://arxiv.org/abs/2402.13210) | 通过训练贝叶斯奖励模型，可以成功缓解奖励的过度优化问题。 |
| [^14] | [Explainable AI for Safe and Trustworthy Autonomous Driving: A Systematic Review](https://arxiv.org/abs/2402.10086) | 可解释的AI技术对于解决自动驾驶中的安全问题和信任问题至关重要。本文通过系统文献综述的方式，分析了可解释的AI方法在满足自动驾驶要求方面的关键贡献，并提出了可解释的设计、可解释的替代模型、可解释的监控、辅助技术和解释的可视化等五个方面的应用。 |
| [^15] | [Noise Contrastive Alignment of Language Models with Explicit Rewards](https://arxiv.org/abs/2402.05369) | 本文提出了一个基于噪声对比估计的通用LM对齐框架，能够处理明确注释的奖励数据，并且扩展了当前的对齐理论。 |
| [^16] | [Hierarchical Tree-structured Knowledge Graph For Academic Insight Survey](https://arxiv.org/abs/2402.04854) | 该论文提出了一种分层树状知识图谱和推荐系统，帮助初学者研究者进行研究调研，填补了现有导航知识图谱的不足，并解决了学术论文推荐系统中高文本相似性带来的困惑。 |
| [^17] | [DistiLLM: Towards Streamlined Distillation for Large Language Models](https://arxiv.org/abs/2402.03898) | DistiLLM是一个更有效和高效的自回归语言模型蒸馏框架，通过引入新颖的偏斜Kullback-Leibler散度损失和自适应的离策略方法，解决了当前针对大语言模型的知识蒸馏方法缺乏标准化目标函数和计算成本过高的问题。 |
| [^18] | [Organic or Diffused: Can We Distinguish Human Art from AI-generated Images?](https://arxiv.org/abs/2402.03214) | 这项研究探讨了如何区分人类艺术和AI生成的图像，并提供了几种不同的方法，包括通过监督学习训练的分类器、扩散模型的研究工具以及专业艺术家的知识。这对防止欺诈、遵守政策以及避免模型崩溃都具有重要意义。 |
| [^19] | [When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards](https://arxiv.org/abs/2402.01781) | 依赖基准排行榜的大型语言模型评估存在较高敏感性，微小的扰动会导致排名的显著变化。研究结果提供了几个最佳实践建议，包括选择混合评分方法来提高答案选择的性能。 |
| [^20] | [Understanding the Expressive Power and Mechanisms of Transformer for Sequence Modeling](https://arxiv.org/abs/2402.00522) | 本研究系统地探讨了Transformer在长序列建模中的近似性质，并研究了其关键组件对表达能力的影响机制。这些发现揭示了关键参数对Transformer的作用，并为替代架构提供了自然建议。 |
| [^21] | [A primer on synthetic health data](https://arxiv.org/abs/2401.17653) | 深层生成模型的进展使得创造逼真合成健康数据集成为可能，这些合成数据集可以在不公开敏感信息的情况下进行安全数据共享，并支持各种倡议和项目构思。然而，评估合成数据集与原始数据集的相似性和预测效用，以及解决隐私和法规问题仍然是挑战。 |
| [^22] | [Injecting linguistic knowledge into BERT for Dialogue State Tracking](https://arxiv.org/abs/2311.15623) | 本文提出了一种方法，在对话状态跟踪任务中，通过无监督的知识提取方法将语言知识注入到BERT中，以提高性能和可解释性。这种方法无需额外的训练数据，通过简单的神经模块实现。该方法使用的特征提取工具与对话的句法和语义模式相关，有助于理解DST模型的决策过程。 |
| [^23] | [Prompt Engineering a Prompt Engineer](https://arxiv.org/abs/2311.05661) | 提示工程任务对于优化大型语言模型在定制任务上的表现至关重要，PE2方法通过详细描述、上下文规范和逐步推理模板的注入，在各种语言任务中展现出出色的适用性和效果。 |
| [^24] | [Self-Supervised Learning in Event Sequences: A Comparative Study and Hybrid Approach of Generative Modeling and Contrastive Learning.](http://arxiv.org/abs/2401.15935) | 本研究通过比较研究和混合方法，调查了事件序列的自我监督学习技术，并引入了一种新的方法，将生成模型和对比嵌入进行对齐。结果显示，这种对齐模型在各种任务上表现优越，为预测事件序列中的信息提供了潜在的好处。 |
| [^25] | [Understanding Biases in ChatGPT-based Recommender Systems: Provider Fairness, Temporal Stability, and Recency.](http://arxiv.org/abs/2401.10545) | 该研究揭示了基于ChatGPT的推荐系统的偏见问题，并研究了提示设计策略对推荐质量的影响。实验结果显示，在RecLLMs中引入特定的系统角色和提示策略可以增强推荐的公平性和多样性，同时GPT-based模型倾向于推荐最新和更多样化的电影流派。 |
| [^26] | [RIDGE: Reproducibility, Integrity, Dependability, Generalizability, and Efficiency Assessment of Medical Image Segmentation Models.](http://arxiv.org/abs/2401.08847) | RIDGE是一个用于评估医学图像分割模型的可重复性、完整性、可靠性、泛化性和效率的框架，旨在通过提高工作质量和透明度，确保分割模型在科学可靠性和临床相关性上都具备优势。 |
| [^27] | [Collaborative Inference via Dynamic Composition of Tiny AI Accelerators on MCUs.](http://arxiv.org/abs/2401.08637) | 该论文介绍了Synergy，一个通过动态组合微型AI加速器来进行协作推断的系统，有效地解决了在设备上AI需求不断增长时tinyML面临的关键挑战。Synergy通过提供虚拟计算空间和运行时编排模块，实现了资源的统一虚拟化视图和跨动态/异构加速器的最佳推断，其吞吐量平均提升了8.0倍。 |
| [^28] | [Backstepping Neural Operators for $2\times 2$ Hyperbolic PDEs.](http://arxiv.org/abs/2312.16762) | 本文介绍了一种用于$2\times 2$双曲PDE的Backstepping神经操作员方法。通过考虑耦合的Goursat形式PDE，并建立了从植被PDE功能系数到核PDE解的映射的连续性，证明了DeepONet逼近核PDE解的存在性 |
| [^29] | [Mixture-of-Experts for Open Set Domain Adaptation: A Dual-Space Detection Approach.](http://arxiv.org/abs/2311.00285) | 该论文提出了一种Mixture-of-Experts用于开放域适应的双空间检测方法，利用图像特征空间和路由特征空间之间的不一致性来检测未知类别的样本，无需手动调节阈值。 |
| [^30] | [A Computational Framework for Solving Wasserstein Lagrangian Flows.](http://arxiv.org/abs/2310.10649) | 本研究提出了一个基于深度学习的计算框架，通过拉格朗日对偶形式处理不同的最优输运问题，不需要模拟轨迹或访问最优耦合，具有较高的性能。 |
| [^31] | [A Dual Latent State Learning Approach: Exploiting Regional Network Similarities for QoS Prediction.](http://arxiv.org/abs/2310.05988) | 本文介绍了一种名为R2SL的基于区域的双潜在状态学习网络，该网络通过汇总数据来捕捉区域网络行为的细微差别，并采用增强的Huber损失函数来提高QoS预测性能。 |
| [^32] | [LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT.](http://arxiv.org/abs/2310.04673) | LauraGPT是一个统一的GPT模型，用于音频识别、理解和生成，具有广泛的应用范围，包括自动语音识别、语音翻译、文本到语音合成、机器翻译等任务。 |
| [^33] | [A Fixed-Parameter Tractable Algorithm for Counting Markov Equivalence Classes with the same Skeleton.](http://arxiv.org/abs/2310.04218) | 本文提出了一个固定参数可处理算法，用于计数具有相同骨架的马尔可夫等价类。 |
| [^34] | [Spectral Estimators for Structured Generalized Linear Models via Approximate Message Passing.](http://arxiv.org/abs/2308.14507) | 本论文研究了针对广义线性模型的参数估计问题，提出了一种通过谱估计器进行预处理的方法。通过对测量进行特征协方差矩阵Σ表示，分析了谱估计器在结构化设计中的性能，并确定了最优预处理以最小化样本数量。 |
| [^35] | [Semi-Supervised Semantic Segmentation via Marginal Contextual Information.](http://arxiv.org/abs/2308.13900) | 通过利用分割图中标签的空间相关性，我们提出的S4MC方法在半监督语义分割中通过增强伪标签的方式，并提高了无标签数据的使用量，从而实现了超越现有方法的性能提升。 |
| [^36] | [Expressivity of Graph Neural Networks Through the Lens of Adversarial Robustness.](http://arxiv.org/abs/2308.08173) | 通过对抗鲁棒性研究，揭示了图神经网络的表达能力与传统消息传递神经网络之间的显著差距，并证明了更强大的GNNs无法泛化到小扰动的图结构和分布不一样的图。 |
| [^37] | [VITS : Variational Inference Thomson Sampling for contextual bandits.](http://arxiv.org/abs/2307.10167) | VITS是一种基于高斯变分推理的新算法，用于情境背离问题的汤普森抽样。它提供了强大的后验近似，计算效率高，并且在线性情境背离问题中达到与传统TS相同阶数的次线性遗憾上界。 |
| [^38] | [Fairness-aware Federated Minimax Optimization with Convergence Guarantee.](http://arxiv.org/abs/2307.04417) | 本文提出了一种名为FFALM的算法，通过施加公平约束和解决极小化极大回归问题，在联邦学习中解决了群体公平性问题。实验证明FFALM在处理严重统计异质性问题时具有良好的效果。 |
| [^39] | [PCL-Indexability and Whittle Index for Restless Bandits with General Observation Models.](http://arxiv.org/abs/2307.03034) | 本文研究了一种一般观测模型下的不安定多臂赌博机问题，提出了PCL-可索引性和Whittle索引的分析方法，并通过近似过程将问题转化为有限状态问题。数值实验表明算法表现优秀。 |
| [^40] | [How Deep Neural Networks Learn Compositional Data: The Random Hierarchy Model.](http://arxiv.org/abs/2307.02129) | 本文研究了深度神经网络学习组合性数据的问题，通过对随机层次模型进行分类任务，发现深度CNN学习这个任务所需的训练数据数量随着类别数、组合数和迭代次数的增加而渐进增加。 |
| [^41] | [Information Flow Control in Machine Learning through Modular Model Architecture.](http://arxiv.org/abs/2306.03235) | 本文提出了机器学习信息流控制的概念，并通过MoE架构实现了训练数据对模型输出的控制，从而提高了模型准确性。通过在推理时仅基于访问策略启用子集的专家，实现了对安全访问控制的支持。 |
| [^42] | [A Convex Relaxation Approach to Bayesian Regret Minimization in Offline Bandits.](http://arxiv.org/abs/2306.01237) | 本文提出一种直接最小化贝叶斯遗憾上界的新方法，获得更好的理论离线遗憾界和数值模拟结果，并提供了证据表明流行的LCB-style算法可能不适用。 |
| [^43] | [Collaborative Multi-Agent Heterogeneous Multi-Armed Bandits.](http://arxiv.org/abs/2305.18784) | 本研究研究了一个新的合作多智能体老虎机设置，并发展了去中心化算法以减少代理之间的集体遗憾，在数学分析中证明了该算法实现了近乎最优性能。 |
| [^44] | [A Framework For Refining Text Classification and Object Recognition from Academic Articles.](http://arxiv.org/abs/2305.17401) | 本文提出了一种结合基于规则的方法和机器学习的框架，旨在解决从学术论文中提炼文本分类和对象识别的问题。 |
| [^45] | [Are demographically invariant models and representations in medical imaging fair?.](http://arxiv.org/abs/2305.01397) | 医学影像模型编码患者人口统计信息，引发有关潜在歧视的担忧。研究表明，不编码人口属性的模型容易损失预测性能，而考虑人口统计属性的反事实模型不变性存在复杂性。人口统计学编码可以被认为是优势。 |
| [^46] | [Large-scale Pre-trained Models are Surprisingly Strong in Incremental Novel Class Discovery.](http://arxiv.org/abs/2303.15975) | 本论文提出了一种更加挑战性和实用性的学习方法MSc-iNCD，通过在连续而无人监督的学习中利用大规模预训练模型的丰富先验知识，该方法在增量式新类别发现中表现出出乎意料的强大实力。 |
| [^47] | [Visual Analytics of Multivariate Networks with Representation Learning and Composite Variable Construction.](http://arxiv.org/abs/2303.09590) | 本文提出了一种用于研究多变量网络的视觉分析工作流程，其中包括神经网络学习阶段、降维和优化阶段以及用户交互式可视化接口进行解释。关键的组合变量构建步骤将非线性特征重塑为线性特征，以方便检查和理解。案例研究表明该工作流程具有有效性和可理解性。 |
| [^48] | [A Systematic Performance Analysis of Deep Perceptual Loss Networks: Breaking Transfer Learning Conventions.](http://arxiv.org/abs/2302.04032) | 这项工作通过系统评估多种常用的预训练网络及其不同特征提取点，在四个深度感知损失用例上解决了迁移学习中的问题。 |
| [^49] | [Explanations Based on Item Response Theory (eXirt): A Model-Specific Method to Explain Tree-Ensemble Model in Trust Perspective.](http://arxiv.org/abs/2210.09933) | 在解释黑盒模型的方法中没有共识，本论文提出了一种基于项目反应理论的新方法eXirt，用于解释使用表格数据的树集成模型，并通过全局特征排名来解释模型输入和预测之间的关系。 |

# 详细

[^1]: 在线变分顺序蒙特卡洛方法

    Online Variational Sequential Monte Carlo

    [https://rss.arxiv.org/abs/2312.12616](https://rss.arxiv.org/abs/2312.12616)

    本文提出了一种在线学习的算法，名为在线VSMC，它基于变分顺序蒙特卡洛方法，在处理数据流时能够实时进行模型参数估计和粒子提议适应。

    

    状态空间模型（SSM）是AI和统计机器学习中最经典的生成模型，对于任何形式的参数学习或潜在状态推断，通常需要计算复杂的潜在状态后验分布。本文在变分顺序蒙特卡洛（VSMC）方法的基础上进行了研究，该方法通过结合粒子方法和变分推断，提供了计算高效且准确的模型参数估计和贝叶斯潜在状态推断。传统的VSMC方法在离线模式下运行，通过重复处理给定的数据批次，而我们使用随机逼近方法将VSMC代理ELBO的梯度逼近分布到时间上，从而实现了在数据流存在的情况下的在线学习。这导致了一种名为在线VSMC的算法，能够高效地进行参数估计和粒子提议适应，而且完全实时处理数据。

    Being the most classical generative model for serial data, state-space models (SSM) are fundamental in AI and statistical machine learning. In SSM, any form of parameter learning or latent state inference typically involves the computation of complex latent-state posteriors. In this work, we build upon the variational sequential Monte Carlo (VSMC) method, which provides computationally efficient and accurate model parameter estimation and Bayesian latent-state inference by combining particle methods and variational inference. While standard VSMC operates in the offline mode, by re-processing repeatedly a given batch of data, we distribute the approximation of the gradient of the VSMC surrogate ELBO in time using stochastic approximation, allowing for online learning in the presence of streams of data. This results in an algorithm, online VSMC, that is capable of performing efficiently, entirely on-the-fly, both parameter estimation and particle proposal adaptation. In addition, we prov
    
[^2]: Jamba: 一个混合Transformer-Mamba语言模型

    Jamba: A Hybrid Transformer-Mamba Language Model

    [https://arxiv.org/abs/2403.19887](https://arxiv.org/abs/2403.19887)

    Jamba是一个基于混合Transformer-Mamba架构的语言模型，在单个80GB GPU上实现了强大的性能，对标准语言模型基准和长上下文评估具有state-of-the-art的表现。

    

    我们提出了Jamba，这是一个基于新颖的混合Transformer-Mamba混合专家(MoE)架构的新基础大型语言模型。具体来说，Jamba交错使用Transformer和Mamba层，从两种模型家族中获益。MoE被添加在其中一些层中，以增加模型容量，同时保持活跃参数的可控性。这种灵活的架构允许特定资源和目标的配置。

    arXiv:2403.19887v1 Announce Type: new  Abstract: We present Jamba, a new base large language model based on a novel hybrid Transformer-Mamba mixture-of-experts (MoE) architecture. Specifically, Jamba interleaves blocks of Transformer and Mamba layers, enjoying the benefits of both model families. MoE is added in some of these layers to increase model capacity while keeping active parameter usage manageable. This flexible architecture allows resource- and objective-specific configurations. In the particular configuration we have implemented, we end up with a powerful model that fits in a single 80GB GPU. Built at large scale, Jamba provides high throughput and small memory footprint compared to vanilla Transformers, and at the same time state-of-the-art performance on standard language model benchmarks and long-context evaluations. Remarkably, the model presents strong results for up to 256K tokens context length. We study various architectural decisions, such as how to combine Transfor
    
[^3]: 在非凹游戏中可处理的局部均衡

    Tractable Local Equilibria in Non-Concave Games

    [https://arxiv.org/abs/2403.08171](https://arxiv.org/abs/2403.08171)

    提出了一个新的解决概念，$(\varepsilon, \Phi(\delta))$-局部均衡，以解决在非凹游戏中局部均衡存在但难以处理的问题。

    

    虽然众所周知在线梯度下降和其他无悔学习程序可以有效地收敛到协调均衡，在每个Agent的效用对于其自身策略呈凹形的情况下，但当效用是非凹的时，这种情况在机器学习应用中很常见，其中Agent的策略由深度神经网络参数化，或者Agent的效用由神经网络计算，或两者兼而有之。实际上，非凹游戏存在一系列博弈论和优化挑战：(i) Nash均衡可能不存在；(ii) 局部Nash均衡存在但是不可处理；(iii) 混合Nash、协调和粗糙协调均衡在一般情况下具有无限支持，并且是不可处理的。为了避开这些挑战，我们提出了一个新的解决概念，称为$(\varepsilon, \Phi(\delta))$-局部均衡，该概念在非凹游戏中概括了局部Nash均衡。

    arXiv:2403.08171v1 Announce Type: cross  Abstract: While Online Gradient Descent and other no-regret learning procedures are known to efficiently converge to coarse correlated equilibrium in games where each agent's utility is concave in their own strategy, this is not the case when the utilities are non-concave, a situation that is common in machine learning applications where the agents' strategies are parameterized by deep neural networks, or the agents' utilities are computed by a neural network, or both. Indeed, non-concave games present a host of game-theoretic and optimization challenges: (i) Nash equilibria may fail to exist; (ii) local Nash equilibria exist but are intractable; and (iii) mixed Nash, correlated, and coarse correlated equilibria have infinite support in general, and are intractable. To sidestep these challenges we propose a new solution concept, termed $(\varepsilon, \Phi(\delta))$-local equilibrium, which generalizes local Nash equilibrium in non-concave games,
    
[^4]: ENOT：期望回归用于神经优化传输的快速和准确训练

    ENOT: Expectile Regularization for Fast and Accurate Training of Neural Optimal Transport

    [https://arxiv.org/abs/2403.03777](https://arxiv.org/abs/2403.03777)

    通过期望回归正则化，本论文提出了一种新的神经优化传输（NOT）训练程序扩展，能够有效地估计最优输运方案，并使学习变得稳定。

    

    我们提出了一种新的神经优化传输（NOT）训练程序扩展，通过特定的共轭势正则化能够准确和高效地估计最优输运方案。现有NOT求解器的主要瓶颈在于找到共轭算子（即c-transform）的接近精确近似的过程，这要么通过优化最小-最大目标，要么通过计算密集型的对初始近似预测的精细调整来完成。我们通过提出一种新的、在期望回归形式上强制适应性条件于学习对偶势的理论上合理化损失来解决这两个问题。这样的正则化提供了可能共轭势分布的上限估计，并使学习变得稳定，消除了对额外广泛微调的需求。我们正式证明了我们的方法的效率。

    arXiv:2403.03777v1 Announce Type: cross  Abstract: We present a new extension for Neural Optimal Transport (NOT) training procedure, capable of accurately and efficiently estimating optimal transportation plan via specific regularisation on conjugate potentials. The main bottleneck of existing NOT solvers is associated with the procedure of finding a near-exact approximation of the conjugate operator (i.e., the c-transform), which is done either by optimizing over maximin objectives or by the computationally-intensive fine-tuning of the initial approximated prediction. We resolve both issues by proposing a new, theoretically justified loss in the form of expectile regularization that enforces binding conditions on the learning dual potentials. Such a regularization provides the upper bound estimation over the distribution of possible conjugate potentials and makes the learning stable, eliminating the need for additional extensive finetuning. We formally justify the efficiency of our me
    
[^5]: 深度配置性能学习：一项系统性调查与分类

    Deep Configuration Performance Learning: A Systematic Survey and Taxonomy

    [https://arxiv.org/abs/2403.03322](https://arxiv.org/abs/2403.03322)

    性能是可配置软件系统行为的关键属性，本文针对深度学习在可配置软件性能学习方面进行了全面的调查与分类研究。

    

    性能可以说是反映可配置软件系统行为的最关键属性。然而，随着现代软件规模和复杂性不断增加，对各种配置如何影响性能进行建模和预测成为软件维护中的主要挑战之一。因此，性能通常是在没有对软件系统有透彻了解的情况下建模的，主要依赖数据，这正好符合深度学习的目的。在这篇论文中，我们专注于深度学习在可配置软件性能学习方面进行了全面的回顾，涵盖了948篇来自六个索引服务的论文，基于此提取并分析了85篇主要论文。我们的结果总结了配置数据如何准备，深度配置性能学习模型如何构建，以及该模型如何进行评估等关键主题和统计信息。

    arXiv:2403.03322v1 Announce Type: cross  Abstract: Performance is arguably the most crucial attribute that reflects the behavior of a configurable software system. However, given the increasing scale and complexity of modern software, modeling and predicting how various configurations can impact performance becomes one of the major challenges in software maintenance. As such, performance is often modeled without having a thorough knowledge of the software system, but relying mainly on data, which fits precisely with the purpose of deep learning.   In this paper, we conduct a comprehensive review exclusively on the topic of deep learning for performance learning of configurable software, covering 948 searched papers spanning six indexing services, based on which 85 primary papers were extracted and analyzed. Our results summarize the key topics and statistics on how the configuration data is prepared; how the deep configuration performance learning model is built; how the model is evalu
    
[^6]: NeuraLUT: 在Boolean合成函数中隐藏神经网络密度

    NeuraLUT: Hiding Neural Network Density in Boolean Synthesizable Functions

    [https://arxiv.org/abs/2403.00849](https://arxiv.org/abs/2403.00849)

    改进了FPGA加速神经网络推断任务的方法，提出将整个子网络映射到单个LUT中，使得神经网络拓扑和精度不再影响生成的查找表的大小。

    

    可编程门阵列（FPGA）加速器已经证明在处理延迟和资源关键的深度神经网络（DNN）推断任务方面取得了成功。神经网络中计算密集度最高的操作之一是特征和权重向量之间的点积。因此，一些先前的FPGA加速工作提出将具有量化输入和输出的神经元直接映射到查找表（LUTs）以进行硬件实现。在这些工作中，神经元的边界与LUTs的边界重合。我们建议放宽这些边界，将整个子网络映射到单个LUT。由于子网络被吸收到LUT中，分区内的神经网络拓扑和精度不会影响生成的查找表的大小。因此，我们在每个分区内使用具有浮点精度的全连接层，这些层受益于成为通用函数逼近器。

    arXiv:2403.00849v1 Announce Type: cross  Abstract: Field-Programmable Gate Array (FPGA) accelerators have proven successful in handling latency- and resource-critical deep neural network (DNN) inference tasks. Among the most computationally intensive operations in a neural network (NN) is the dot product between the feature and weight vectors. Thus, some previous FPGA acceleration works have proposed mapping neurons with quantized inputs and outputs directly to lookup tables (LUTs) for hardware implementation. In these works, the boundaries of the neurons coincide with the boundaries of the LUTs. We propose relaxing these boundaries and mapping entire sub-networks to a single LUT. As the sub-networks are absorbed within the LUT, the NN topology and precision within a partition do not affect the size of the lookup tables generated. Therefore, we utilize fully connected layers with floating-point precision inside each partition, which benefit from being universal function approximators, 
    
[^7]: 热力学导向稀缺时间动态数据的超分辨率

    Thermodynamics-informed super-resolution of scarce temporal dynamics data

    [https://arxiv.org/abs/2402.17506](https://arxiv.org/abs/2402.17506)

    提出了一种利用热力学感知神经网络来提高物理系统测量分辨率并预测时间演化的方法，采用对抗自动编码器和结构保持神经网络相结合的方式，可有效解决超分辨率问题，并确保满足热力学定律。

    

    我们提出了一种利用了热力学感知神经网络来提高物理系统测量分辨率并随后预测其时间演化的方法。我们的方法使用对抗自动编码器，将完整模型的维度降低到一组潜变量，这些潜变量被强制匹配先验，例如正态分布。对抗自动编码器被视为生成模型，它们可以被训练以从低分辨率输入生成高分辨率样本，也就是可以解决所谓的超分辨率问题。然后，第二个神经网络被训练以学习潜变量的物理结构并预测其时间演化。这个神经网络被称为保持结构的神经网络。它学习系统的metriplectic结构，并应用物理偏差以确保热力学的第一和第二定律被遵守。

    arXiv:2402.17506v1 Announce Type: cross  Abstract: We present a method to increase the resolution of measurements of a physical system and subsequently predict its time evolution using thermodynamics-aware neural networks. Our method uses adversarial autoencoders, which reduce the dimensionality of the full order model to a set of latent variables that are enforced to match a prior, for example a normal distribution. Adversarial autoencoders are seen as generative models, and they can be trained to generate high-resolution samples from low-resoution inputs, meaning they can address the so-called super-resolution problem. Then, a second neural network is trained to learn the physical structure of the latent variables and predict their temporal evolution. This neural network is known as an structure-preserving neural network. It learns the metriplectic-structure of the system and applies a physical bias to ensure that the first and second principles of thermodynamics are fulfilled. The i
    
[^8]: 优化时间步长加速扩散采样

    Accelerating Diffusion Sampling with Optimized Time Steps

    [https://arxiv.org/abs/2402.17376](https://arxiv.org/abs/2402.17376)

    提出了一个通用框架用于设计优化问题，旨在通过寻找更合适的时间步长加速扩散采样。

    

    扩散概率模型（DPMs）在高分辨率图像合成中表现出色，但由于通常需要大量采样步骤，其采样效率仍有待提高。近期高阶数值ODE求解器在DPMs中的应用使得用更少的采样步骤生成高质量图像成为可能。尽管这是一项重大进展，大多数采样方法仍然采用均匀时间步长，而在采样步骤较少时并不是最佳选择。为解决这一问题，我们提出了一个通用框架，用于设计一个优化问题，该优化问题旨在为DPMs的特定数值ODE求解器寻找更合适的时间步长。此优化问题旨在最小化地实现地真实解与与数值求解器对应的近似解之间的距离。它可以通过受限信赖域方法进行高效求解，时间少于

    arXiv:2402.17376v1 Announce Type: cross  Abstract: Diffusion probabilistic models (DPMs) have shown remarkable performance in high-resolution image synthesis, but their sampling efficiency is still to be desired due to the typically large number of sampling steps. Recent advancements in high-order numerical ODE solvers for DPMs have enabled the generation of high-quality images with much fewer sampling steps. While this is a significant development, most sampling methods still employ uniform time steps, which is not optimal when using a small number of steps. To address this issue, we propose a general framework for designing an optimization problem that seeks more appropriate time steps for a specific numerical ODE solver for DPMs. This optimization problem aims to minimize the distance between the ground-truth solution to the ODE and an approximate solution corresponding to the numerical solver. It can be efficiently solved using the constrained trust region method, taking less than 
    
[^9]: 用于随机组合半臂老虎机的协方差自适应最小二乘算法

    Covariance-Adaptive Least-Squares Algorithm for Stochastic Combinatorial Semi-Bandits

    [https://arxiv.org/abs/2402.15171](https://arxiv.org/abs/2402.15171)

    提出了一种协方差自适应的最小二乘算法，利用在线估计协方差结构，相对于基于代理方差的算法获得改进的遗憾上界，特别在协方差系数全为非负时，能有效地利用半臂反馈，并在各种参数设置下表现优异。

    

    我们解决了随机组合半臂老虎机问题，其中玩家可以从包含d个基本项的P个子集中进行选择。大多数现有算法（如CUCB、ESCB、OLS-UCB）需要对奖励分布有先验知识，比如子高斯代理-方差的上界，这很难准确估计。在这项工作中，我们设计了OLS-UCB的方差自适应版本，依赖于协方差结构的在线估计。在实际设置中，估计协方差矩阵的系数要容易得多，并且相对于基于代理方差的算法，导致改进的遗憾上界。当协方差系数全为非负时，我们展示了我们的方法有效地利用了半臂反馈，并且可以明显优于老虎机反馈方法，在指数级别P≫d以及P≤d的情况下，这一点并不来自大多数现有分析。

    arXiv:2402.15171v1 Announce Type: new  Abstract: We address the problem of stochastic combinatorial semi-bandits, where a player can select from P subsets of a set containing d base items. Most existing algorithms (e.g. CUCB, ESCB, OLS-UCB) require prior knowledge on the reward distribution, like an upper bound on a sub-Gaussian proxy-variance, which is hard to estimate tightly. In this work, we design a variance-adaptive version of OLS-UCB, relying on an online estimation of the covariance structure. Estimating the coefficients of a covariance matrix is much more manageable in practical settings and results in improved regret upper bounds compared to proxy variance-based algorithms. When covariance coefficients are all non-negative, we show that our approach efficiently leverages the semi-bandit feedback and provably outperforms bandit feedback approaches, not only in exponential regimes where P $\gg$ d but also when P $\le$ d, which is not straightforward from most existing analyses.
    
[^10]: 弹性时间步长的强化学习

    Reinforcement Learning with Elastic Time Steps

    [https://arxiv.org/abs/2402.14961](https://arxiv.org/abs/2402.14961)

    SEAC是一种弹性时间步长的离策略演员-评论家算法，通过可变持续时间的时间步长，使代理能够根据情况改变控制频率，在模拟环境中表现优异。

    

    传统的强化学习（RL）算法通常应用于机器人学习以以固定控制频率执行动作的控制器。鉴于RL算法的离散性质，它们对控制频率的选择的影响视而不见：找到正确的控制频率可能很困难，错误往往会导致过度使用计算资源甚至导致无法收敛。我们提出了软弹性演员-评论家（SEAC）, 一种新颖的离策略演员-评论家算法来解决这个问题。SEAC实现了弹性时间步长，即具有已知变化持续时间的时间步长，允许代理根据情况改变其控制频率。在实践中，SEAC仅在必要时应用控制，最小化计算资源和数据使用。我们在模拟环境中评估了SEAC在牛顿运动学迷宫导航任务和三维赛车视频游戏Trackmania中的能力。SEAC在表现上优于SAC基线。

    arXiv:2402.14961v1 Announce Type: cross  Abstract: Traditional Reinforcement Learning (RL) algorithms are usually applied in robotics to learn controllers that act with a fixed control rate. Given the discrete nature of RL algorithms, they are oblivious to the effects of the choice of control rate: finding the correct control rate can be difficult and mistakes often result in excessive use of computing resources or even lack of convergence.   We propose Soft Elastic Actor-Critic (SEAC), a novel off-policy actor-critic algorithm to address this issue. SEAC implements elastic time steps, time steps with a known, variable duration, which allow the agent to change its control frequency to adapt to the situation. In practice, SEAC applies control only when necessary, minimizing computational resources and data usage.   We evaluate SEAC's capabilities in simulation in a Newtonian kinematics maze navigation task and on a 3D racing video game, Trackmania. SEAC outperforms the SAC baseline in t
    
[^11]: ProSparse: 引入和增强大型语言模型内部激活稀疏性

    ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models

    [https://arxiv.org/abs/2402.13516](https://arxiv.org/abs/2402.13516)

    本文介绍了一种名为"ProSparse"的有效稀疏化方法，以推动大型语言模型实现更高的激活稀疏性而不降低模型性能

    

    Activation sparsity指的是激活输出中存在许多弱贡献元素。作为使用ReLU激活函数的模型的普遍属性，已被证明是提高模型推理效率的一种有前途的范例。然而，大多数大型语言模型（LLMs）采用了没有内在激活稀疏性的激活函数（例如GELU和Swish）。一些最近的努力尝试引入ReLU或其变体作为替代激活函数，以帮助LLMs实现激活稀疏性和推理加速，但很少能同时获得高稀疏度和可比较的模型性能。本文介绍了一种名为"ProSparse"的有效稀疏化方法，以推动LLMs实现更高的激活稀疏性而不降低模型性能。具体来说，将LLMs的激活函数替换为ReLU后，ProSparse采用渐进稀疏正则化

    arXiv:2402.13516v1 Announce Type: cross  Abstract: Activation sparsity refers to the existence of considerable weakly-contributed elements among activation outputs. As a prevalent property of the models using the ReLU activation function, it has been proven a promising paradigm to boost model inference efficiency. Nevertheless, most large language models (LLMs) adopt activation functions without intrinsic activation sparsity (e.g., GELU and Swish). Some recent efforts have explored introducing ReLU or its variants as the substitutive activation function to help LLMs achieve activation sparsity and inference acceleration, but few can simultaneously obtain high sparsity and comparable model performance. This paper introduces an effective sparsification method named "ProSparse" to push LLMs for higher activation sparsity without decreasing model performance. Specifically, after substituting the activation function of LLMs with ReLU, ProSparse adopts progressive sparsity regularization wit
    
[^12]: Smaug：使用DPO-Positive修复偏好优化的失败模式

    Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive

    [https://arxiv.org/abs/2402.13228](https://arxiv.org/abs/2402.13228)

    在这项工作中，我们提出了一种新的损失函数和训练过程DPO-Positive（DPOP），以避免直接偏好优化（DPO）中潜在的失败模式，并发现DPOP明显优于DPO。

    

    直接偏好优化（DPO）在显著改善大型语言模型（LLMs）在推理、总结和对齐等下游任务上的性能方面是有效的。 DPO使用首选和非首选数据对模型选择一个响应而不是另一个的“相对”概率进行建模。在这项工作中，我们首先从理论上表明，只要首选和非首选类别之间的相对概率增加，标准DPO损失就可能导致模型对首选示例的可能性降低。然后，我们在实证上展示了当在常见数据集上微调LLMs时，尤其是在完成之间的编辑距离较短的数据集上，会出现这种现象。利用这些见解，我们设计了DPO-Positive（DPOP），一种新的损失函数和训练过程，避免了这种失败模式。令人惊讶的是，我们还发现DPOP明显优于DPO。

    arXiv:2402.13228v1 Announce Type: cross  Abstract: Direct Preference Optimisation (DPO) is effective at significantly improving the performance of large language models (LLMs) on downstream tasks such as reasoning, summarisation, and alignment. Using pairs of preferred and dispreferred data, DPO models the \textit{relative} probability of picking one response over another. In this work, first we show theoretically that the standard DPO loss can lead to a \textit{reduction} of the model's likelihood of the preferred examples, as long as the relative probability between the preferred and dispreferred classes increases. We then show empirically that this phenomenon occurs when fine-tuning LLMs on common datasets, especially datasets in which the edit distance between pairs of completions is low. Using these insights, we design DPO-Positive (DPOP), a new loss function and training procedure which avoids this failure mode. Surprisingly, we also find that DPOP significantly outperforms DPO a
    
[^13]: Bayesian Reward Models for LLM Alignment

    Bayesian Reward Models for LLM Alignment

    [https://arxiv.org/abs/2402.13210](https://arxiv.org/abs/2402.13210)

    通过训练贝叶斯奖励模型，可以成功缓解奖励的过度优化问题。

    

    为了确保大型语言模型（LLM）的回复有益且无毒，通常我们会在人类偏好数据上微调奖励模型。然后我们选择具有高奖励的策略回复（best-of-n抽样），或者进一步优化策略以生成具有高奖励的回复（从人类反馈中进行强化学习）。然而，这个过程容易受到奖励过度优化或攻击的影响，选定的回复由于奖励模型中的错误而具有高奖励，而不是真实偏好。这一问题在提示或回复偏离训练数据时尤为严重。我们通过训练贝叶斯奖励模型来缓解这些问题，这种模型在远离训练数据分布时会产生更高的不确定性。因此，我们使用Laplace-LoRA（Yang等人，2024）训练了贝叶斯奖励模型，发现由此产生的不确定性估计可以成功缓解奖励的过度优化。

    arXiv:2402.13210v1 Announce Type: new  Abstract: To ensure that large language model (LLM) responses are helpful and non-toxic, we usually fine-tune a reward model on human preference data. We then select policy responses with high rewards (best-of-n sampling) or further optimize the policy to produce responses with high rewards (reinforcement learning from human feedback). However, this process is vulnerable to reward overoptimization or hacking, in which the responses selected have high rewards due to errors in the reward model rather than a genuine preference. This is especially problematic as the prompt or response diverges from the training data. It should be possible to mitigate these issues by training a Bayesian reward model, which signals higher uncertainty further from the training data distribution. Therefore, we trained Bayesian reward models using Laplace-LoRA (Yang et al., 2024) and found that the resulting uncertainty estimates can successfully mitigate reward overoptimi
    
[^14]: 可解释的人工智能在安全可信的自动驾驶中的应用：一项系统性评述

    Explainable AI for Safe and Trustworthy Autonomous Driving: A Systematic Review

    [https://arxiv.org/abs/2402.10086](https://arxiv.org/abs/2402.10086)

    可解释的AI技术对于解决自动驾驶中的安全问题和信任问题至关重要。本文通过系统文献综述的方式，分析了可解释的AI方法在满足自动驾驶要求方面的关键贡献，并提出了可解释的设计、可解释的替代模型、可解释的监控、辅助技术和解释的可视化等五个方面的应用。

    

    鉴于其在感知和规划任务中相对传统方法具有更优异的性能，人工智能（AI）对于自动驾驶（AD）的应用显示出了很大的潜力。然而，难以理解的AI系统加剧了对AD安全保证的现有挑战。缓解这一挑战的一种方法是利用可解释的AI（XAI）技术。为此，我们首次提出了关于可解释方法在安全可信的AD中的全面系统文献综述。我们首先分析了在AD背景下AI的要求，重点关注数据、模型和机构这三个关键方面。我们发现XAI对于满足这些要求是至关重要的。基于此，我们解释了AI中解释的来源，并描述了一种XAI的分类学。然后，我们确定了XAI在安全可信的AD中的五个主要贡献，包括可解释的设计、可解释的替代模型、可解释的监控，辅助...

    arXiv:2402.10086v1 Announce Type: cross  Abstract: Artificial Intelligence (AI) shows promising applications for the perception and planning tasks in autonomous driving (AD) due to its superior performance compared to conventional methods. However, inscrutable AI systems exacerbate the existing challenge of safety assurance of AD. One way to mitigate this challenge is to utilize explainable AI (XAI) techniques. To this end, we present the first comprehensive systematic literature review of explainable methods for safe and trustworthy AD. We begin by analyzing the requirements for AI in the context of AD, focusing on three key aspects: data, model, and agency. We find that XAI is fundamental to meeting these requirements. Based on this, we explain the sources of explanations in AI and describe a taxonomy of XAI. We then identify five key contributions of XAI for safe and trustworthy AI in AD, which are interpretable design, interpretable surrogate models, interpretable monitoring, auxil
    
[^15]: 以显式奖励的噪声对比对齐语言模型

    Noise Contrastive Alignment of Language Models with Explicit Rewards

    [https://arxiv.org/abs/2402.05369](https://arxiv.org/abs/2402.05369)

    本文提出了一个基于噪声对比估计的通用LM对齐框架，能够处理明确注释的奖励数据，并且扩展了当前的对齐理论。

    

    用户意图通常被形式化为需要在微调语言模型时最大化的评估奖励。现有的对齐方法，如直接优化偏好（DPO），主要适用于隐含定义而非明确给定奖励的两两偏好数据。在本文中，我们引入了一个通用的LM对齐框架，利用噪声对比估计（NCE）来解决明确注释有标量评估的奖励数据处理的差距。我们的框架包括两个并行算法，NCA和InfoNCA，两者都能从奖励数据和偏好数据中直接提取LM策略。值得注意的是，我们证明了DPO损失是我们提出的InfoNCA目标在两两偏好设置下的特殊情况，从而集成和扩展了当前的对齐理论。通过对比NCA和InfoNCA，我们展示了InfoNCA和DPO如何在不同响应对于单个指令的相对可能性上进行调整。

    User intentions are typically formalized as evaluation rewards to be maximized when fine-tuning language models (LMs). Existing alignment methods, such as Direct Preference Optimization (DPO), are mainly tailored for pairwise preference data where rewards are implicitly defined rather than explicitly given. In this paper, we introduce a general framework for LM alignment, leveraging Noise Contrastive Estimation (NCE) to bridge the gap in handling reward datasets explicitly annotated with scalar evaluations. Our framework comprises two parallel algorithms, NCA and InfoNCA, both enabling the direct extraction of an LM policy from reward data as well as preference data. Notably, we show that the DPO loss is a special case of our proposed InfoNCA objective under pairwise preference settings, thereby integrating and extending current alignment theories. By contrasting NCA and InfoNCA, we show that InfoNCA and DPO adjust relative likelihood across different responses to a single instruction,
    
[^16]: 分层树状知识图谱用于学术调研

    Hierarchical Tree-structured Knowledge Graph For Academic Insight Survey

    [https://arxiv.org/abs/2402.04854](https://arxiv.org/abs/2402.04854)

    该论文提出了一种分层树状知识图谱和推荐系统，帮助初学者研究者进行研究调研，填补了现有导航知识图谱的不足，并解决了学术论文推荐系统中高文本相似性带来的困惑。

    

    对于缺乏研究培训的初学者研究者来说，研究调查一直是一个挑战。这些研究者在短时间内很难理解他们研究主题内的方向，以及发现新的研究发现。为初学者研究者提供直观的帮助的一种方式是提供相关的知识图谱(KG)并推荐相关的学术论文。然而，现有的导航知识图谱主要依赖于研究领域的关键字，常常无法清楚地呈现多个相关论文之间的逻辑层次关系。此外，大多数学术论文推荐系统仅仅依赖于高文本相似性，这可能会让研究人员困惑为什么推荐了特定的文章。他们可能缺乏了解关于他们希望获得的"问题解决"和"问题发现"之间的见解连接的重要信息。为解决这些问题，本研究旨在支持初学者研究者进行研究调研。

    Research surveys have always posed a challenge for beginner researchers who lack of research training. These researchers struggle to understand the directions within their research topic, and the discovery of new research findings within a short time. One way to provide intuitive assistance to beginner researchers is by offering relevant knowledge graphs(KG) and recommending related academic papers. However, existing navigation knowledge graphs primarily rely on keywords in the research field and often fail to present the logical hierarchy among multiple related papers clearly. Moreover, most recommendation systems for academic papers simply rely on high text similarity, which can leave researchers confused as to why a particular article is being recommended. They may lack of grasp important information about the insight connection between "Issue resolved" and "Issue finding" that they hope to obtain. To address these issues, this study aims to support research insight surveys for begi
    
[^17]: DistiLLM: 面向大型语言模型的简化蒸馏方法

    DistiLLM: Towards Streamlined Distillation for Large Language Models

    [https://arxiv.org/abs/2402.03898](https://arxiv.org/abs/2402.03898)

    DistiLLM是一个更有效和高效的自回归语言模型蒸馏框架，通过引入新颖的偏斜Kullback-Leibler散度损失和自适应的离策略方法，解决了当前针对大语言模型的知识蒸馏方法缺乏标准化目标函数和计算成本过高的问题。

    

    知识蒸馏（KD）被广泛用于将教师模型压缩为更小的学生模型，降低推理成本和内存占用，同时保持模型能力。然而，当前针对自回归序列模型（例如大型语言模型）的KD方法存在缺乏标准化目标函数的问题。此外，最近使用学生生成的输出来解决训练-推理不匹配问题的做法显著增加了计算成本。为了解决这些问题，我们引入了DistiLLM，这是一个更有效和高效的自回归语言模型蒸馏框架。DistiLLM由两个组成部分组成：（1）一种新颖的偏斜Kullback-Leibler散度损失，我们揭示并利用了它的理论属性；（2）一种自适应的离策略方法，旨在提高利用学生生成的输出的效率。包括指令跟随任务在内的大量实验验证了DistiLLM在构建高性能模型方面的有效性。

    Knowledge distillation (KD) is widely used for compressing a teacher model to a smaller student model, reducing its inference cost and memory footprint while preserving model capabilities. However, current KD methods for auto-regressive sequence models (e.g., large language models) suffer from missing a standardized objective function. Moreover, the recent use of student-generated outputs to address training-inference mismatches has significantly escalated computational costs. To tackle these issues, we introduce DistiLLM, a more effective and efficient KD framework for auto-regressive language models. DistiLLM comprises two components: (1) a novel skew Kullback-Leibler divergence loss, where we unveil and leverage its theoretical properties, and (2) an adaptive off-policy approach designed to enhance the efficiency in utilizing student-generated outputs. Extensive experiments, including instruction-following tasks, demonstrate the effectiveness of DistiLLM in building high-performing 
    
[^18]: 有机或扩散：我们能区分人类艺术和AI生成的图像吗？

    Organic or Diffused: Can We Distinguish Human Art from AI-generated Images?

    [https://arxiv.org/abs/2402.03214](https://arxiv.org/abs/2402.03214)

    这项研究探讨了如何区分人类艺术和AI生成的图像，并提供了几种不同的方法，包括通过监督学习训练的分类器、扩散模型的研究工具以及专业艺术家的知识。这对防止欺诈、遵守政策以及避免模型崩溃都具有重要意义。

    

    生成AI图像的出现完全颠覆了艺术界。从人类艺术中识别AI生成的图像是一个具有挑战性的问题，其影响随着时间的推移而不断增加。未能解决这个问题会导致不良行为者欺诈那些支付高价购买人类艺术品的个人和禁止使用AI图像的公司。这对于需要过滤训练数据以避免潜在模型崩溃的AI模型训练者来说也至关重要。区分人类艺术和AI图像的方法有多种，包括通过监督学习训练的分类器，针对扩散模型的研究工具，以及通过专业艺术家利用他们对艺术技巧的知识进行识别。在本文中，我们试图了解这些方法在现代生成模型的良性和对抗性环境中的表现如何。我们策划了7种风格的真实人类艺术，从5个生成模型生成了与之匹配的图像，并应用了8个检测器。

    The advent of generative AI images has completely disrupted the art world. Identifying AI generated images from human art is a challenging problem whose impact is growing over time. The failure to address this problem allows bad actors to defraud individuals paying a premium for human art, and companies whose stated policies forbid AI imagery. This is also critical for AI model trainers, who need to filter training data to avoid potential model collapse. There are several different approaches to distinguishing human art from AI images, including classifiers trained by supervised learning, research tools targeting diffusion models, and identification by professional artists using their knowledge of artistic techniques. In this paper, we seek to understand how well these approaches can perform against today's modern generative models in both benign and adversarial settings. We curate real human art across 7 styles, generate matching images from 5 generative models, and apply 8 detectors 
    
[^19]: 当基准成为目标：揭示大型语言模型排行榜的敏感性

    When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards

    [https://arxiv.org/abs/2402.01781](https://arxiv.org/abs/2402.01781)

    依赖基准排行榜的大型语言模型评估存在较高敏感性，微小的扰动会导致排名的显著变化。研究结果提供了几个最佳实践建议，包括选择混合评分方法来提高答案选择的性能。

    

    基于基准排名的大型语言模型(LLM)排行榜经常被用来指导实践者在模型选择中。通常，发布的排行榜排名被直接接受 - 我们表明这是一个（潜在昂贵的）错误。在现有的排行榜下，LLM的相对性能对（通常微小的）细节非常敏感。我们展示了对于流行的多项选择题基准（例如MMLU），对基准的微小扰动，如改变选项顺序或答案选择方法，会导致排名变化达到8个位置。我们通过对三个广泛的基准扰动类别进行系统实验并确定这一行为的来源来解释这一现象。我们的分析得出了几个最佳实践建议，包括选择优化的混合评分方法来进行答案选择。我们的研究强调了依赖简单基准评估的风险，并为更健壮的模型评估提供了指导道路。

    Large Language Model (LLM) leaderboards based on benchmark rankings are regularly used to guide practitioners in model selection. Often, the published leaderboard rankings are taken at face value - we show this is a (potentially costly) mistake. Under existing leaderboards, the relative performance of LLMs is highly sensitive to (often minute) details. We show that for popular multiple choice question benchmarks (e.g. MMLU) minor perturbations to the benchmark, such as changing the order of choices or the method of answer selection, result in changes in rankings up to 8 positions. We explain this phenomenon by conducting systematic experiments over three broad categories of benchmark perturbations and identifying the sources of this behavior. Our analysis results in several best-practice recommendations, including the advantage of a hybrid scoring method for answer selection. Our study highlights the dangers of relying on simple benchmark evaluations and charts the path for more robust
    
[^20]: 理解Transformer在序列建模中的表达能力和机制

    Understanding the Expressive Power and Mechanisms of Transformer for Sequence Modeling

    [https://arxiv.org/abs/2402.00522](https://arxiv.org/abs/2402.00522)

    本研究系统地探讨了Transformer在长序列建模中的近似性质，并研究了其关键组件对表达能力的影响机制。这些发现揭示了关键参数对Transformer的作用，并为替代架构提供了自然建议。

    

    我们对Transformer在长、稀疏和复杂记忆的序列建模中的近似性质进行了系统研究。我们调查了Transformer的不同组件（如点积自注意力、位置编码和前馈层）是如何影响其表达能力的机制，并通过建立明确的近似率来研究它们的综合影响。我们的研究揭示了Transformer中关键参数（如层数和注意力头数）的作用，并且这些洞察还为替代架构提供了自然建议。

    We conduct a systematic study of the approximation properties of Transformer for sequence modeling with long, sparse and complicated memory. We investigate the mechanisms through which different components of Transformer, such as the dot-product self-attention, positional encoding and feed-forward layer, affect its expressive power, and we study their combined effects through establishing explicit approximation rates. Our study reveals the roles of critical parameters in the Transformer, such as the number of layers and the number of attention heads, and these insights also provide natural suggestions for alternative architectures.
    
[^21]: 合成健康数据初探

    A primer on synthetic health data

    [https://arxiv.org/abs/2401.17653](https://arxiv.org/abs/2401.17653)

    深层生成模型的进展使得创造逼真合成健康数据集成为可能，这些合成数据集可以在不公开敏感信息的情况下进行安全数据共享，并支持各种倡议和项目构思。然而，评估合成数据集与原始数据集的相似性和预测效用，以及解决隐私和法规问题仍然是挑战。

    

    深层生成模型的最新进展极大地扩展了创造逼真合成健康数据集的潜力。这些合成数据集旨在在不公开病人身份或敏感信息的情况下保留从敏感健康数据集中获得的特征、模式和总体科学结论。因此，合成数据可以促进安全数据共享，支持一系列倡议，包括开发新的预测模型、先进的健康IT平台以及一般项目构思和假设发展。然而，许多问题和挑战仍然存在，包括如何与原始真实数据集进行一致的评估合成数据集的相似性和预测效用，以及分享时对隐私的风险。额外的法规和治理问题尚未得到广泛解决。在这个初探中，我们对合成健康数据的现状进行了梳理，包括生成和评估方法和工具，以及现有部署示例。

    Recent advances in deep generative models have greatly expanded the potential to create realistic synthetic health datasets. These synthetic datasets aim to preserve the characteristics, patterns, and overall scientific conclusions derived from sensitive health datasets without disclosing patient identity or sensitive information. Thus, synthetic data can facilitate safe data sharing that supports a range of initiatives including the development of new predictive models, advanced health IT platforms, and general project ideation and hypothesis development. However, many questions and challenges remain, including how to consistently evaluate a synthetic dataset's similarity and predictive utility in comparison to the original real dataset and risk to privacy when shared. Additional regulatory and governance issues have not been widely addressed. In this primer, we map the state of synthetic health data, including generation and evaluation methods and tools, existing examples of deployme
    
[^22]: 将语言知识注入到BERT中用于对话状态跟踪

    Injecting linguistic knowledge into BERT for Dialogue State Tracking

    [https://arxiv.org/abs/2311.15623](https://arxiv.org/abs/2311.15623)

    本文提出了一种方法，在对话状态跟踪任务中，通过无监督的知识提取方法将语言知识注入到BERT中，以提高性能和可解释性。这种方法无需额外的训练数据，通过简单的神经模块实现。该方法使用的特征提取工具与对话的句法和语义模式相关，有助于理解DST模型的决策过程。

    

    对话状态跟踪(DST)模型通常采用复杂的神经网络架构，需要大量的训练数据，其推理过程缺乏透明性。本文提出了一种方法，通过无监督框架提取语言知识，然后利用这些知识来增强BERT在DST任务中的性能和可解释性。知识提取过程计算经济高效，不需要注释或额外的训练数据。注入提取的知识只需要添加简单的神经模块。我们使用凸多面体模型(CPM)作为DST任务的特征提取工具，并表明所获取的特征与对话中的句法和语义模式相关。这种相关性有助于全面理解影响DST模型决策过程的语言特征。我们在不同的DST任务上对这个框架进行基准测试，并展示了其效果。

    Dialogue State Tracking (DST) models often employ intricate neural network architectures, necessitating substantial training data, and their inference processes lack transparency. This paper proposes a method that extracts linguistic knowledge via an unsupervised framework and subsequently utilizes this knowledge to augment BERT's performance and interpretability in DST tasks. The knowledge extraction procedure is computationally economical and does not necessitate annotations or additional training data. The injection of the extracted knowledge necessitates the addition of only simple neural modules. We employ the Convex Polytopic Model (CPM) as a feature extraction tool for DST tasks and illustrate that the acquired features correlate with the syntactic and semantic patterns in the dialogues. This correlation facilitates a comprehensive understanding of the linguistic features influencing the DST model's decision-making process. We benchmark this framework on various DST tasks and ob
    
[^23]: Prompt Engineering a Prompt Engineer

    Prompt Engineering a Prompt Engineer

    [https://arxiv.org/abs/2311.05661](https://arxiv.org/abs/2311.05661)

    提示工程任务对于优化大型语言模型在定制任务上的表现至关重要，PE2方法通过详细描述、上下文规范和逐步推理模板的注入，在各种语言任务中展现出出色的适用性和效果。

    

    提示工程是优化大型语言模型在定制任务上表现的一项具有挑战性但至关重要的任务。为了检查模型的错误，假设当前提示中缺少或误导了什么，并清晰地传达任务，需要复杂的推理。尽管最近的研究表明，大型语言模型可以被元提示来执行自动提示工程，但我们认为由于元提示中缺乏复杂推理的充分指导，它们的潜力受到限制。我们通过将详细描述、上下文规范和逐步推理模板注入到元提示中来填补这一空白。所得到的方法称为PE2，展示了在不同语言任务中出色的适用性。它找到的提示在MultiArith上比“按步骤思考”高出6.3%，在GSM8K上高出3.1%，并在对立任务上优于竞争基线

    arXiv:2311.05661v2 Announce Type: replace-cross  Abstract: Prompt engineering is a challenging yet crucial task for optimizing the performance of large language models on customized tasks. It requires complex reasoning to examine the model's errors, hypothesize what is missing or misleading in the current prompt, and communicate the task with clarity. While recent works indicate that large language models can be meta-prompted to perform automatic prompt engineering, we argue that their potential is limited due to insufficient guidance for complex reasoning in the meta-prompt. We fill this gap by infusing into the meta-prompt three key components: detailed descriptions, context specification, and a step-by-step reasoning template. The resulting method, named PE2, showcases remarkable versatility across diverse language tasks. It finds prompts that outperform "let's think step by step" by 6.3% on MultiArith and 3.1% on GSM8K, and outperforms competitive baselines on counterfactual tasks 
    
[^24]: 事件序列的自我监督学习：生成建模和对比学习的比较研究和混合方法的应用

    Self-Supervised Learning in Event Sequences: A Comparative Study and Hybrid Approach of Generative Modeling and Contrastive Learning. (arXiv:2401.15935v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.15935](http://arxiv.org/abs/2401.15935)

    本研究通过比较研究和混合方法，调查了事件序列的自我监督学习技术，并引入了一种新的方法，将生成模型和对比嵌入进行对齐。结果显示，这种对齐模型在各种任务上表现优越，为预测事件序列中的信息提供了潜在的好处。

    

    本研究调查了获取事件序列表示的自我监督学习技术。这是各种应用中的关键模态，包括但不限于银行、电子商务和医疗保健。我们对自我监督学习中的生成模型和对比方法进行了全面的研究，并分别应用了它们。我们发现没有一种绝对优越的方法。因此，我们探讨了结合这些方法的潜在好处。为了实现这个目标，我们引入了一种新的方法，将生成模型和对比嵌入作为不同的模态进行对齐，从当代多模态研究中汲取灵感。生成模型和对比方法通常被视为互斥的，因此存在它们的联合探索的空白。我们的结果表明，这种对齐模型在至少与现有方法持平，并且在各种任务上更加普适。此外，我们证明了自我监督学习在预测事件序列中包含的信息方面的潜力。

    This study investigates self-supervised learning techniques to obtain representations of Event Sequences. It is a key modality in various applications, including but not limited to banking, e-commerce, and healthcare.  We perform a comprehensive study of generative and contrastive approaches in self-supervised learning, applying them both independently. We find that there is no single supreme method. Consequently, we explore the potential benefits of combining these approaches. To achieve this goal, we introduce a novel method that aligns generative and contrastive embeddings as distinct modalities, drawing inspiration from contemporary multimodal research.  Generative and contrastive approaches are often treated as mutually exclusive, leaving a gap for their combined exploration. Our results demonstrate that this aligned model performs at least on par with, and mostly surpasses, existing methods and is more universal across a variety of tasks. Furthermore, we demonstrate that self-sup
    
[^25]: 理解ChatGPT基推荐系统中的偏见：供应商公平性、时间稳定性和最新性研究

    Understanding Biases in ChatGPT-based Recommender Systems: Provider Fairness, Temporal Stability, and Recency. (arXiv:2401.10545v1 [cs.IR])

    [http://arxiv.org/abs/2401.10545](http://arxiv.org/abs/2401.10545)

    该研究揭示了基于ChatGPT的推荐系统的偏见问题，并研究了提示设计策略对推荐质量的影响。实验结果显示，在RecLLMs中引入特定的系统角色和提示策略可以增强推荐的公平性和多样性，同时GPT-based模型倾向于推荐最新和更多样化的电影流派。

    

    该研究探讨了使用大型语言模型（RecLLMs）的推荐系统的细微能力和固有偏见，重点研究了基于ChatGPT的系统。研究了生成模型和传统协同过滤模型在电影推荐中的差异行为。本研究主要调查了提示设计策略及其对推荐质量的各个方面（包括准确性、供应商公平性、多样性、稳定性、流行类型和时效性）的影响。我们的实验分析表明，在RecLLMs中引入特定的“系统角色”和“提示策略”显著影响其性能。例如，基于角色的提示可以增强推荐的公平性和多样性，减轻流行偏见。我们发现，虽然基于GPT的模型并不总是能与传统协同过滤基线模型的性能匹配，但它们倾向于推荐更新、更多样化的电影流派。值得注意的是，GPT-base

    This study explores the nuanced capabilities and inherent biases of Recommender Systems using Large Language Models (RecLLMs), with a focus on ChatGPT-based systems. It studies into the contrasting behaviors of generative models and traditional collaborative filtering models in movie recommendations. The research primarily investigates prompt design strategies and their impact on various aspects of recommendation quality, including accuracy, provider fairness, diversity, stability, genre dominance, and temporal freshness (recency).  Our experimental analysis reveals that the introduction of specific 'system roles' and 'prompt strategies' in RecLLMs significantly influences their performance. For instance, role-based prompts enhance fairness and diversity in recommendations, mitigating popularity bias. We find that while GPT-based models do not always match the performance of CF baselines, they exhibit a unique tendency to recommend newer and more diverse movie genres. Notably, GPT-base
    
[^26]: RIDGE: 医学图像分割模型的可重复性、完整性、可靠性、泛化性和效率评估

    RIDGE: Reproducibility, Integrity, Dependability, Generalizability, and Efficiency Assessment of Medical Image Segmentation Models. (arXiv:2401.08847v1 [eess.IV])

    [http://arxiv.org/abs/2401.08847](http://arxiv.org/abs/2401.08847)

    RIDGE是一个用于评估医学图像分割模型的可重复性、完整性、可靠性、泛化性和效率的框架，旨在通过提高工作质量和透明度，确保分割模型在科学可靠性和临床相关性上都具备优势。

    

    深度学习技术尽管具有潜力，但往往缺乏可重复性和泛化性，限制了其在临床中的应用。图像分割是医学图像分析中的关键任务之一，需要对一个或多个感兴趣的区域/体积进行注释。本文介绍了RIDGE清单，这是一个用于评估基于深度学习的医学图像分割模型的可重复性、完整性、可靠性、泛化性和效率的框架。该清单为研究人员提供了指导，以提高其工作的质量和透明度，确保分割模型不仅具有科学的可靠性，还具有临床的相关性。

    Deep learning techniques, despite their potential, often suffer from a lack of reproducibility and generalizability, impeding their clinical adoption. Image segmentation is one of the critical tasks in medical image analysis, in which one or several regions/volumes of interest should be annotated. This paper introduces the RIDGE checklist, a framework for assessing the Reproducibility, Integrity, Dependability, Generalizability, and Efficiency of deep learning-based medical image segmentation models. The checklist serves as a guide for researchers to enhance the quality and transparency of their work, ensuring that segmentation models are not only scientifically sound but also clinically relevant.
    
[^27]: 通过MCU上微型AI加速器的动态组合实现协作推断

    Collaborative Inference via Dynamic Composition of Tiny AI Accelerators on MCUs. (arXiv:2401.08637v1 [cs.DC])

    [http://arxiv.org/abs/2401.08637](http://arxiv.org/abs/2401.08637)

    该论文介绍了Synergy，一个通过动态组合微型AI加速器来进行协作推断的系统，有效地解决了在设备上AI需求不断增长时tinyML面临的关键挑战。Synergy通过提供虚拟计算空间和运行时编排模块，实现了资源的统一虚拟化视图和跨动态/异构加速器的最佳推断，其吞吐量平均提升了8.0倍。

    

    微型AI加速器的出现为深度神经网络在极限边缘上的部署提供了机会，提供了较低的延迟、较低的功耗成本和改进的隐私保护。尽管取得了这些进展，但由于这些加速器的固有限制，如有限的内存和单设备焦点，仍存在挑战。本文介绍了Synergy，一个能够为多租户模型动态组合微型AI加速器的系统，有效解决了对于设备上AI的需求不断增长时tinyML面临的关键挑战。Synergy的一个关键特性是其提供了虚拟计算空间，为资源提供了统一的虚拟化视图，从而实现了对物理设备的高效任务映射。Synergy的运行时编排模块确保了跨动态和异构加速器的最佳推断。我们的评估结果显示，与基准相比，Synergy的吞吐量平均提升了8.0倍。

    The advent of tiny AI accelerators opens opportunities for deep neural network deployment at the extreme edge, offering reduced latency, lower power cost, and improved privacy in on-device ML inference. Despite these advancements, challenges persist due to inherent limitations of these accelerators, such as restricted onboard memory and single-device focus. This paper introduces Synergy, a system that dynamically composes tiny AI accelerators for multi-tenant models, effectively addressing tinyML's critical challenges for the increasing demand for on-device AI. A key feature of Synergy is its virtual computing space, providing a unified, virtualized view of resources and enabling efficient task mapping to physical devices. Synergy's runtime orchestration module ensures optimal inference across dynamic and heterogeneous accelerators. Our evaluations with 7 baselines and 8 models demonstrate that Synergy improves throughput by an average of 8.0X compared to baselines.
    
[^28]: Backstepping神经操作员用于$2\times 2$双曲PDEs

    Backstepping Neural Operators for $2\times 2$ Hyperbolic PDEs. (arXiv:2312.16762v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2312.16762](http://arxiv.org/abs/2312.16762)

    本文介绍了一种用于$2\times 2$双曲PDE的Backstepping神经操作员方法。通过考虑耦合的Goursat形式PDE，并建立了从植被PDE功能系数到核PDE解的映射的连续性，证明了DeepONet逼近核PDE解的存在性

    

    深度神经网络逼近非线性操作员，通常被称为DeepONet，在单个Goursat形式的PDE控制单个反馈增益函数的PDE反向设计中已被证明具有能力。在耦合PDE的边界控制中，耦合的Goursat形式PDE控制两个或多个增益核 - 这是迄今为止DeepONet未解决的PDE结构。在本文中，我们通过考虑一个简单的逆向传播$2\times 2$耦合系统来打开超卷积$2\times 2$核PDE系统的近似主题，其控制中出现了Goursat形式的PDE系统。应用包括石油钻井、浅水波的Saint-Venant模型以及密集交通流中的Aw-Rascle-Zhang模型的停车和行驶不稳定性。在本文中，我们建立了从（总共五个）植被PDE的功能系数到核PDE解的映射的连续性，证明了DeepONet逼近核PDE解的存在性

    Deep neural network approximation of nonlinear operators, commonly referred to as DeepONet, has proven capable of approximating PDE backstepping designs in which a single Goursat-form PDE governs a single feedback gain function. In boundary control of coupled PDEs, coupled Goursat-form PDEs govern two or more gain kernels -- a PDE structure unaddressed thus far with DeepONet. In this note, we open the subject of approximating systems of gain kernel PDEs for hyperbolic PDE plants by considering a simple counter-convecting $2\times 2$ coupled system in whose control a $2\times 2$ kernel PDE systems in Goursat form arises. Applications include oil drilling, Saint-Venant model of shallow water waves, and Aw-Rascle-Zhang model of stop-and-go instability in congested traffic flow. In this paper we establish the continuity of the mapping from (a total of five) plant PDE functional coefficients to the kernel PDE solutions, prove the existence of an arbitrarily close DeepONet approximation to t
    
[^29]: Mixture-of-Experts用于开放域适应的双空间检测方法

    Mixture-of-Experts for Open Set Domain Adaptation: A Dual-Space Detection Approach. (arXiv:2311.00285v1 [cs.CV])

    [http://arxiv.org/abs/2311.00285](http://arxiv.org/abs/2311.00285)

    该论文提出了一种Mixture-of-Experts用于开放域适应的双空间检测方法，利用图像特征空间和路由特征空间之间的不一致性来检测未知类别的样本，无需手动调节阈值。

    

    开放域适应（OSDA）旨在同时处理源域和目标域之间的分布和标签偏移，实现对已知类别的精确分类，同时在目标域中识别未知类别的样本。大多数现有的OSDA方法依赖于深度模型的最终图像特征空间，需要手动调节阈值，并且可能将未知样本错误分类为已知类别。Mixture-of-Expert（MoE）可能是一种解决方法。在MoE中，不同的专家处理不同的输入特征，在路由特征空间中为不同的类别生成独特的专家路由模式。因此，未知类别的样本也可以显示与已知类别不同的专家路由模式。本文提出了双空间检测，利用图像特征空间和路由特征空间之间的不一致性来检测未知类别的样本，无需任何阈值。进一步介绍了图形路由器来更好地利用摘要的信息。

    Open Set Domain Adaptation (OSDA) aims to cope with the distribution and label shifts between the source and target domains simultaneously, performing accurate classification for known classes while identifying unknown class samples in the target domain. Most existing OSDA approaches, depending on the final image feature space of deep models, require manually-tuned thresholds, and may easily misclassify unknown samples as known classes. Mixture-of-Expert (MoE) could be a remedy. Within an MoE, different experts address different input features, producing unique expert routing patterns for different classes in a routing feature space. As a result, unknown class samples may also display different expert routing patterns to known classes. This paper proposes Dual-Space Detection, which exploits the inconsistencies between the image feature space and the routing feature space to detect unknown class samples without any threshold. Graph Router is further introduced to better make use of the
    
[^30]: 用于求解Wasserstein Lagrangian流的计算框架

    A Computational Framework for Solving Wasserstein Lagrangian Flows. (arXiv:2310.10649v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.10649](http://arxiv.org/abs/2310.10649)

    本研究提出了一个基于深度学习的计算框架，通过拉格朗日对偶形式处理不同的最优输运问题，不需要模拟轨迹或访问最优耦合，具有较高的性能。

    

    通过选择不同的基础几何（动能）和密度路径的正则化（势能），可以对最优输运的动力学形式进行推广。这些组合产生不同的变分问题（Lagrangians），涵盖了许多最优输运问题的变体，如Schrödinger桥、不平衡最优输运和带有物理约束的最优输运等。一般而言，最优密度路径是未知的，解决这些变分问题在计算上具有挑战性。借助拉格朗日对偶形式，我们提出了一个新颖的基于深度学习的框架，从统一的角度处理所有这些问题。我们的方法不需要模拟或反向传播学习动力学的轨迹，也不需要访问最优耦合。我们展示了所提出框架的多功能性，通过超越了其他方法的表现。

    The dynamical formulation of the optimal transport can be extended through various choices of the underlying geometry ($\textit{kinetic energy}$), and the regularization of density paths ($\textit{potential energy}$). These combinations yield different variational problems ($\textit{Lagrangians}$), encompassing many variations of the optimal transport problem such as the Schr\"odinger bridge, unbalanced optimal transport, and optimal transport with physical constraints, among others. In general, the optimal density path is unknown, and solving these variational problems can be computationally challenging. Leveraging the dual formulation of the Lagrangians, we propose a novel deep learning based framework approaching all of these problems from a unified perspective. Our method does not require simulating or backpropagating through the trajectories of the learned dynamics, and does not need access to optimal couplings. We showcase the versatility of the proposed framework by outperformin
    
[^31]: 一种双潜在状态学习方法：利用区域网络相似性进行QoS预测

    A Dual Latent State Learning Approach: Exploiting Regional Network Similarities for QoS Prediction. (arXiv:2310.05988v1 [cs.LG])

    [http://arxiv.org/abs/2310.05988](http://arxiv.org/abs/2310.05988)

    本文介绍了一种名为R2SL的基于区域的双潜在状态学习网络，该网络通过汇总数据来捕捉区域网络行为的细微差别，并采用增强的Huber损失函数来提高QoS预测性能。

    

    特定区域内的个体对象，无论是用户还是服务，通常由于它们来自同一城市或自治系统（AS），展现出相似的网络状态。尽管存在区域网络相似性，但许多现有技术忽视了其潜力，导致由于数据稀疏性和标签不平衡等挑战而产生表现不佳。本文引入了基于区域的双潜在状态学习网络（R2SL），这是一个新颖的深度学习框架，旨在克服传统基于个体对象的QoS预测技术的缺点。与之前的方法不同，R2SL通过从公共区域汇总的数据构建了两个不同的区域网络潜在状态：城市网络潜在状态和AS网络潜在状态。此外，R2SL采用了增强的Huber损失函数。

    Individual objects, whether users or services, within a specific region often exhibit similar network states due to their shared origin from the same city or autonomous system (AS). Despite this regional network similarity, many existing techniques overlook its potential, resulting in subpar performance arising from challenges such as data sparsity and label imbalance. In this paper, we introduce the regional-based dual latent state learning network(R2SL), a novel deep learning framework designed to overcome the pitfalls of traditional individual object-based prediction techniques in Quality of Service (QoS) prediction. Unlike its predecessors, R2SL captures the nuances of regional network behavior by deriving two distinct regional network latent states: the city-network latent state and the AS-network latent state. These states are constructed utilizing aggregated data from common regions rather than individual object data. Furthermore, R2SL adopts an enhanced Huber loss function that
    
[^32]: LauraGPT：使用GPT进行听、关注、理解和再生音频的研究

    LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT. (arXiv:2310.04673v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2310.04673](http://arxiv.org/abs/2310.04673)

    LauraGPT是一个统一的GPT模型，用于音频识别、理解和生成，具有广泛的应用范围，包括自动语音识别、语音翻译、文本到语音合成、机器翻译等任务。

    

    生成式预训练变换器（GPT）模型在各种自然语言处理任务中取得了显著的性能。然而，将类似的框架应用于音频任务的研究有限。以前提出的用于音频任务的大型语言模型要么缺乏充分的定量评估，要么局限于识别和理解音频内容的任务，要么明显不及现有的最先进模型（SOTA）。本文中，我们提出了LauraGPT，一个用于音频识别、理解和生成的统一GPT模型。LauraGPT是一个通用的语言模型，可以处理音频和文本输入，并在任意模式下生成输出。它可以进行与内容、语义、语音学和音频信号分析相关的各种任务。其中一些值得注意的任务包括自动语音识别、语音到文本翻译、文本到语音合成、机器翻译、语音增强、自动音频捕获等。

    Generative Pre-trained Transformer (GPT) models have achieved remarkable performance on various natural language processing tasks. However, there has been limited research on applying similar frameworks to audio tasks. Previously proposed large language models for audio tasks either lack sufficient quantitative evaluations, or are limited to tasks for recognizing and understanding audio content, or significantly underperform existing state-of-the-art (SOTA) models. In this paper, we propose LauraGPT, a unified GPT model for audio recognition, understanding, and generation. LauraGPT is a versatile language model that can process both audio and text inputs and generate outputs in either modalities. It can perform a wide range of tasks related to content, semantics, paralinguistics, and audio-signal analysis. Some of its noteworthy tasks include automatic speech recognition, speech-to-text translation, text-to-speech synthesis, machine translation, speech enhancement, automated audio capt
    
[^33]: 一个可计数具有相同骨架的马尔可夫等价类的固定参数可处理算法

    A Fixed-Parameter Tractable Algorithm for Counting Markov Equivalence Classes with the same Skeleton. (arXiv:2310.04218v1 [cs.DS])

    [http://arxiv.org/abs/2310.04218](http://arxiv.org/abs/2310.04218)

    本文提出了一个固定参数可处理算法，用于计数具有相同骨架的马尔可夫等价类。

    

    因果有向无环图（也称为贝叶斯网络）是编码随机变量之间条件依赖关系的流行工具。在因果有向无环图中，随机变量被建模为有向图中的顶点，并且规定每个随机变量在给定其父节点的情况下与其祖先节点无关。然而，对于同一组随机变量上的两个不同的因果有向无环图可以准确编码相同的一组条件依赖关系。这样的因果有向无环图被称为马尔可夫等价，马尔可夫等价的因果有向无环图的等价类被称为马尔可夫等价类（MEC）。在过去几十年中，对于MEC已经创建了一些美丽的组合特征，并且已知，特别是在同一MEC中的所有因果有向无环图必须具有相同的“骨架”（底层无向图）和v-结构（形式为$a\rightarrow b \leftarrow c$的诱导子图）。这些组合特征还提出了几个自然的算法问题。

    Causal DAGs (also known as Bayesian networks) are a popular tool for encoding conditional dependencies between random variables. In a causal DAG, the random variables are modeled as vertices in the DAG, and it is stipulated that every random variable is independent of its ancestors conditioned on its parents. It is possible, however, for two different causal DAGs on the same set of random variables to encode exactly the same set of conditional dependencies. Such causal DAGs are said to be Markov equivalent, and equivalence classes of Markov equivalent DAGs are known as Markov Equivalent Classes (MECs). Beautiful combinatorial characterizations of MECs have been developed in the past few decades, and it is known, in particular that all DAGs in the same MEC must have the same ''skeleton'' (underlying undirected graph) and v-structures (induced subgraph of the form $a\rightarrow b \leftarrow c$).  These combinatorial characterizations also suggest several natural algorithmic questions. On
    
[^34]: 通过近似传递消息实现结构化广义线性模型的谱估计器

    Spectral Estimators for Structured Generalized Linear Models via Approximate Message Passing. (arXiv:2308.14507v1 [math.ST])

    [http://arxiv.org/abs/2308.14507](http://arxiv.org/abs/2308.14507)

    本论文研究了针对广义线性模型的参数估计问题，提出了一种通过谱估计器进行预处理的方法。通过对测量进行特征协方差矩阵Σ表示，分析了谱估计器在结构化设计中的性能，并确定了最优预处理以最小化样本数量。

    

    我们考虑从广义线性模型中的观测中进行参数估计的问题。谱方法是一种简单而有效的估计方法：它通过对观测进行适当预处理得到的矩阵的主特征向量来估计参数。尽管谱估计器被广泛使用，但对于结构化（即独立同分布的高斯和哈尔）设计，目前仅有对谱估计器的严格性能表征以及对数据进行预处理的基本方法可用。相反，实际的设计矩阵具有高度结构化并且表现出非平凡的相关性。为解决这个问题，我们考虑了捕捉测量的非各向同性特性的相关高斯设计，通过特征协方差矩阵Σ进行表示。我们的主要结果是对于这种情况下谱估计器性能的精确渐近分析。然后，可以通过这一结果来确定最优预处理，从而最小化所需样本的数量。

    We consider the problem of parameter estimation from observations given by a generalized linear model. Spectral methods are a simple yet effective approach for estimation: they estimate the parameter via the principal eigenvector of a matrix obtained by suitably preprocessing the observations. Despite their wide use, a rigorous performance characterization of spectral estimators, as well as a principled way to preprocess the data, is available only for unstructured (i.e., i.i.d. Gaussian and Haar) designs. In contrast, real-world design matrices are highly structured and exhibit non-trivial correlations. To address this problem, we consider correlated Gaussian designs which capture the anisotropic nature of the measurements via a feature covariance matrix $\Sigma$. Our main result is a precise asymptotic characterization of the performance of spectral estimators in this setting. This then allows to identify the optimal preprocessing that minimizes the number of samples needed to meanin
    
[^35]: 通过边际上下文信息的半监督语义分割

    Semi-Supervised Semantic Segmentation via Marginal Contextual Information. (arXiv:2308.13900v1 [cs.CV])

    [http://arxiv.org/abs/2308.13900](http://arxiv.org/abs/2308.13900)

    通过利用分割图中标签的空间相关性，我们提出的S4MC方法在半监督语义分割中通过增强伪标签的方式，并提高了无标签数据的使用量，从而实现了超越现有方法的性能提升。

    

    我们提出了一种新的置信度精化方案，增强了半监督语义分割中的伪标签。与当前主流方法不同的是，我们的方法通过将相邻像素分组并共同考虑它们的伪标签，利用分割图中标签的空间相关性。借助这种上下文信息，我们的方法命名为S4MC，在保持伪标签质量的同时，增加了在训练过程中使用的无标签数据的数量，且计算开销几乎可以忽略不计。通过在标准基准测试上进行大量实验证明，S4MC超越了现有的半监督学习方法，为降低获得稠密标注成本提供了有希望的解决方案。例如，在PASCAL VOC 12上使用366个带注释图像，S4MC比前一最先进方法提高了1.29个mIoU。有关重现我们实验的代码参见...

    We present a novel confidence refinement scheme that enhances pseudo-labels in semi-supervised semantic segmentation. Unlike current leading methods, which filter pixels with low-confidence predictions in isolation, our approach leverages the spatial correlation of labels in segmentation maps by grouping neighboring pixels and considering their pseudo-labels collectively. With this contextual information, our method, named S4MC, increases the amount of unlabeled data used during training while maintaining the quality of the pseudo-labels, all with negligible computational overhead. Through extensive experiments on standard benchmarks, we demonstrate that S4MC outperforms existing state-of-the-art semi-supervised learning approaches, offering a promising solution for reducing the cost of acquiring dense annotations. For example, S4MC achieves a 1.29 mIoU improvement over the prior state-of-the-art method on PASCAL VOC 12 with 366 annotated images. The code to reproduce our experiments i
    
[^36]: 通过对抗鲁棒性研究，揭示图神经网络的表达能力

    Expressivity of Graph Neural Networks Through the Lens of Adversarial Robustness. (arXiv:2308.08173v1 [cs.LG])

    [http://arxiv.org/abs/2308.08173](http://arxiv.org/abs/2308.08173)

    通过对抗鲁棒性研究，揭示了图神经网络的表达能力与传统消息传递神经网络之间的显著差距，并证明了更强大的GNNs无法泛化到小扰动的图结构和分布不一样的图。

    

    我们首次对图神经网络（GNNs）进行了对抗鲁棒性研究，证明它们在表达能力上比传统的消息传递神经网络（MPNNs）更强大。具体而言，我们使用对抗鲁棒性作为一种工具来揭示它们在理论上可能和经验上实际达到的表达能力之间的显著差距。为此，我们关注GNNs计数特定的子图模式的能力，这是一种已建立的表达能力度量，将对抗鲁棒性的概念扩展到这个任务上。基于此，我们开发了高效的对抗攻击方法来进行子图计数，并展示更强大的GNNs即使在对图结构的小扰动下也无法泛化。在此基础上，我们还表明这样的架构在处理分布不一样的图时也无法计数子结构。

    We perform the first adversarial robustness study into Graph Neural Networks (GNNs) that are provably more powerful than traditional Message Passing Neural Networks (MPNNs). In particular, we use adversarial robustness as a tool to uncover a significant gap between their theoretically possible and empirically achieved expressive power. To do so, we focus on the ability of GNNs to count specific subgraph patterns, which is an established measure of expressivity, and extend the concept of adversarial robustness to this task. Based on this, we develop efficient adversarial attacks for subgraph counting and show that more powerful GNNs fail to generalize even to small perturbations to the graph's structure. Expanding on this, we show that such architectures also fail to count substructures on out-of-distribution graphs.
    
[^37]: VITS: 基于变分推理的汤普森抽样用于情境背离问题的算法

    VITS : Variational Inference Thomson Sampling for contextual bandits. (arXiv:2307.10167v1 [stat.ML])

    [http://arxiv.org/abs/2307.10167](http://arxiv.org/abs/2307.10167)

    VITS是一种基于高斯变分推理的新算法，用于情境背离问题的汤普森抽样。它提供了强大的后验近似，计算效率高，并且在线性情境背离问题中达到与传统TS相同阶数的次线性遗憾上界。

    

    本文介绍并分析了一种用于情境背离问题的汤普森抽样（TS）算法的变体。传统的TS算法在每轮需要从当前的后验分布中抽样，而这通常是难以计算的。为了解决这个问题，可以使用近似推理技术并提供接近后验分布的样本。然而，当前的近似技术要么估计不准确（拉普拉斯近似），要么计算开销较大（MCMC方法，集成抽样...）。在本文中，我们提出了一种新的算法，基于高斯变分推理的变分推理汤普森抽样（VITS）。这种方法提供了强大的后验近似，并且容易从中抽样，而且计算效率高，是TS的理想选择。此外，我们还证明了在线性情境背离问题中，VITS实现了与传统TS相同阶数的次线性遗憾上界，与维度和回合数成正比。

    In this paper, we introduce and analyze a variant of the Thompson sampling (TS) algorithm for contextual bandits. At each round, traditional TS requires samples from the current posterior distribution, which is usually intractable. To circumvent this issue, approximate inference techniques can be used and provide samples with distribution close to the posteriors. However, current approximate techniques yield to either poor estimation (Laplace approximation) or can be computationally expensive (MCMC methods, Ensemble sampling...). In this paper, we propose a new algorithm, Varational Inference Thompson sampling VITS, based on Gaussian Variational Inference. This scheme provides powerful posterior approximations which are easy to sample from, and is computationally efficient, making it an ideal choice for TS. In addition, we show that VITS achieves a sub-linear regret bound of the same order in the dimension and number of round as traditional TS for linear contextual bandit. Finally, we 
    
[^38]: 具有收敛保证的公正感知联邦极小化优化

    Fairness-aware Federated Minimax Optimization with Convergence Guarantee. (arXiv:2307.04417v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.04417](http://arxiv.org/abs/2307.04417)

    本文提出了一种名为FFALM的算法，通过施加公平约束和解决极小化极大回归问题，在联邦学习中解决了群体公平性问题。实验证明FFALM在处理严重统计异质性问题时具有良好的效果。

    

    由于其保护隐私的特性，联邦学习 (FL) 吸引了相当多的关注。然而，管理用户数据的自由度不足可能导致群体公平性问题，即模型偏向于敏感因素诸如种族或性别。为了解决这个问题，本文提出了一种新颖的算法，名为带有增广拉格朗日方法的公平联邦平均法 (FFALM)，专门用于解决FL中的群体公平问题。具体来说，我们对训练目标施加了公平约束，并解决了受约束优化问题的极小化极大回归。然后，我们推导了FFALM的收敛速率的理论上界。通过在CelebA和UTKFace数据集中充分考虑严重统计异质性，实证结果表明了FFALM 在提高公平性方面的有效性。

    Federated learning (FL) has garnered considerable attention due to its privacy-preserving feature. Nonetheless, the lack of freedom in managing user data can lead to group fairness issues, where models are biased towards sensitive factors such as race or gender. To tackle this issue, this paper proposes a novel algorithm, fair federated averaging with augmented Lagrangian method (FFALM), designed explicitly to address group fairness issues in FL. Specifically, we impose a fairness constraint on the training objective and solve the minimax reformulation of the constrained optimization problem. Then, we derive the theoretical upper bound for the convergence rate of FFALM. The effectiveness of FFALM in improving fairness is shown empirically on CelebA and UTKFace datasets in the presence of severe statistical heterogeneity.
    
[^39]: 带有一般观测模型的不安定赌博机问题的PCL-可索引性和Whittle索引

    PCL-Indexability and Whittle Index for Restless Bandits with General Observation Models. (arXiv:2307.03034v1 [stat.ML])

    [http://arxiv.org/abs/2307.03034](http://arxiv.org/abs/2307.03034)

    本文研究了一种一般观测模型下的不安定多臂赌博机问题，提出了PCL-可索引性和Whittle索引的分析方法，并通过近似过程将问题转化为有限状态问题。数值实验表明算法表现优秀。

    

    本文考虑了一种一般观测模型，用于不安定多臂赌博机问题。由于资源约束或环境或固有噪声，玩家操作需要基于某种有误差的反馈机制。通过建立反馈/观测动力学的一般概率模型，我们将问题表述为一个从任意初始信念（先验信息）开始的具有可数信念状态空间的不安定赌博机问题。我们利用具有部分守恒定律（PCL）的可实现区域方法，分析了无限状态问题的可索引性和优先级索引（Whittle索引）。最后，我们提出了一个近似过程，将问题转化为可以应用Niño-Mora和Bertsimas针对有限状态问题的AG算法的问题。数值实验表明，我们的算法具有出色的性能。

    In this paper, we consider a general observation model for restless multi-armed bandit problems. The operation of the player needs to be based on certain feedback mechanism that is error-prone due to resource constraints or environmental or intrinsic noises. By establishing a general probabilistic model for dynamics of feedback/observation, we formulate the problem as a restless bandit with a countable belief state space starting from an arbitrary initial belief (a priori information). We apply the achievable region method with partial conservation law (PCL) to the infinite-state problem and analyze its indexability and priority index (Whittle index). Finally, we propose an approximation process to transform the problem into which the AG algorithm of Ni\~no-Mora and Bertsimas for finite-state problems can be applied to. Numerical experiments show that our algorithm has an excellent performance.
    
[^40]: 深度神经网络如何学习组合性数据：随机层次模型

    How Deep Neural Networks Learn Compositional Data: The Random Hierarchy Model. (arXiv:2307.02129v1 [cs.LG])

    [http://arxiv.org/abs/2307.02129](http://arxiv.org/abs/2307.02129)

    本文研究了深度神经网络学习组合性数据的问题，通过对随机层次模型进行分类任务，发现深度CNN学习这个任务所需的训练数据数量随着类别数、组合数和迭代次数的增加而渐进增加。

    

    学习一般高维任务是非常困难的，因为它需要与维度成指数增长的训练数据数量。然而，深度卷积神经网络（CNN）在克服这一挑战方面显示出了卓越的成功。一种普遍的假设是可学习任务具有高度结构化，CNN利用这种结构建立了数据的低维表示。然而，我们对它们需要多少训练数据以及这个数字如何取决于数据结构知之甚少。本文回答了针对一个简单的分类任务的这个问题，该任务旨在捕捉真实数据的相关方面：随机层次模型。在这个模型中，$n_c$个类别中的每一个对应于$m$个同义组合的高层次特征，并且这些特征又通过一个重复$L$次的迭代过程由子特征组成。我们发现，需要深度CNN学习这个任务的训练数据数量$P^*$（i）随着$n_c m^L$的增长而渐进地增长，这只有...

    Learning generic high-dimensional tasks is notably hard, as it requires a number of training data exponential in the dimension. Yet, deep convolutional neural networks (CNNs) have shown remarkable success in overcoming this challenge. A popular hypothesis is that learnable tasks are highly structured and that CNNs leverage this structure to build a low-dimensional representation of the data. However, little is known about how much training data they require, and how this number depends on the data structure. This paper answers this question for a simple classification task that seeks to capture relevant aspects of real data: the Random Hierarchy Model. In this model, each of the $n_c$ classes corresponds to $m$ synonymic compositions of high-level features, which are in turn composed of sub-features through an iterative process repeated $L$ times. We find that the number of training data $P^*$ required by deep CNNs to learn this task (i) grows asymptotically as $n_c m^L$, which is only
    
[^41]: 模块化模型架构中的机器学习信息流控制

    Information Flow Control in Machine Learning through Modular Model Architecture. (arXiv:2306.03235v1 [cs.LG])

    [http://arxiv.org/abs/2306.03235](http://arxiv.org/abs/2306.03235)

    本文提出了机器学习信息流控制的概念，并通过MoE架构实现了训练数据对模型输出的控制，从而提高了模型准确性。通过在推理时仅基于访问策略启用子集的专家，实现了对安全访问控制的支持。

    

    在当今的机器学习模型中，训练数据的任何部分都可以影响其输出。当访问控制只允许个人用户访问数据子集时，从训练数据到模型输出的信息流控制不足成为训练敏感数据模型的主要障碍。为了实现访问控制数据的安全机器学习，我们提出了机器学习信息流控制的概念，并基于混合专家（MoE）架构开发了一个安全Transformer型语言模型。通过限制来自每个安全领域的训练数据对单个专家模块的影响，并仅基于访问控制策略在推理时启用专家的子集，安全MoE架构控制了信息流。使用大型文本数据语料库进行的评估表明，所提出的MoE架构具有最小的性能开销（1.9%），并且可以显著提高模型准确性（最高可达37%），从而实现训练准确和安全分类器。

    In today's machine learning (ML) models, any part of the training data can affect its output. This lack of control for information flow from training data to model output is a major obstacle in training models on sensitive data when access control only allows individual users to access a subset of data. To enable secure machine learning for access controlled data, we propose the notion of information flow control for machine learning, and develop a secure Transformer-based language model based on the Mixture-of-Experts (MoE) architecture. The secure MoE architecture controls information flow by limiting the influence of training data from each security domain to a single expert module, and only enabling a subset of experts at inference time based on an access control policy. The evaluation using a large corpus of text data shows that the proposed MoE architecture has minimal (1.9%) performance overhead and can significantly improve model accuracy (up to 37%) by enabling training on acc
    
[^42]: 离线赌博中贝叶斯遗憾最小化的凸松弛方法

    A Convex Relaxation Approach to Bayesian Regret Minimization in Offline Bandits. (arXiv:2306.01237v1 [cs.LG])

    [http://arxiv.org/abs/2306.01237](http://arxiv.org/abs/2306.01237)

    本文提出一种直接最小化贝叶斯遗憾上界的新方法，获得更好的理论离线遗憾界和数值模拟结果，并提供了证据表明流行的LCB-style算法可能不适用。

    

    离线赌博算法必须仅利用离线数据在不确定环境中优化决策。离线赌博中一种引人注目且逐渐流行的目标是学习一个实现低贝叶斯遗憾并具有高置信度的策略。本文提出了一种新的方法，直接利用高效的锥优化求解器来最小化贝叶斯遗憾的上界。与之前的工作相比，我们的算法在理论上获得了更优的离线遗憾界，并在数值模拟中取得了更好的结果。最后，我们提供一些证据表明流行的LCB（lower confidence bound）-style算法可能不适合离线赌博中最小化贝叶斯遗憾。

    Algorithms for offline bandits must optimize decisions in uncertain environments using only offline data. A compelling and increasingly popular objective in offline bandits is to learn a policy which achieves low Bayesian regret with high confidence. An appealing approach to this problem, inspired by recent offline reinforcement learning results, is to maximize a form of lower confidence bound (LCB). This paper proposes a new approach that directly minimizes upper bounds on Bayesian regret using efficient conic optimization solvers. Our bounds build on connections among Bayesian regret, Value-at-Risk (VaR), and chance-constrained optimization. Compared to prior work, our algorithm attains superior theoretical offline regret bounds and better results in numerical simulations. Finally, we provide some evidence that popular LCB-style algorithms may be unsuitable for minimizing Bayesian regret in offline bandits.
    
[^43]: 合作多智能体异构多臂老虎机翻译论文

    Collaborative Multi-Agent Heterogeneous Multi-Armed Bandits. (arXiv:2305.18784v1 [cs.LG])

    [http://arxiv.org/abs/2305.18784](http://arxiv.org/abs/2305.18784)

    本研究研究了一个新的合作多智能体老虎机设置，并发展了去中心化算法以减少代理之间的集体遗憾，在数学分析中证明了该算法实现了近乎最优性能。

    

    最近合作多智能体老虎机的研究吸引了很多关注。因此，我们开始研究一个新的合作设置，其中$N$个智能体中的每个智能体正在学习$M$个具有随机性的多臂老虎机，以减少他们的集体累计遗憾。我们开发了去中心化算法，促进了代理之间的合作，并针对两种情况进行了性能表征。通过推导每个代理的累积遗憾和集体遗憾的上限，我们对这些算法的性能进行了表征。我们还证明了这种情况下集体遗憾的下限，证明了所提出算法的近乎最优性能。

    The study of collaborative multi-agent bandits has attracted significant attention recently. In light of this, we initiate the study of a new collaborative setting, consisting of $N$ agents such that each agent is learning one of $M$ stochastic multi-armed bandits to minimize their group cumulative regret. We develop decentralized algorithms which facilitate collaboration between the agents under two scenarios. We characterize the performance of these algorithms by deriving the per agent cumulative regret and group regret upper bounds. We also prove lower bounds for the group regret in this setting, which demonstrates the near-optimal behavior of the proposed algorithms.
    
[^44]: 一种从学术论文中提炼文本分类和对象识别的框架

    A Framework For Refining Text Classification and Object Recognition from Academic Articles. (arXiv:2305.17401v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.17401](http://arxiv.org/abs/2305.17401)

    本文提出了一种结合基于规则的方法和机器学习的框架，旨在解决从学术论文中提炼文本分类和对象识别的问题。

    

    随着互联网的广泛使用，高效地从大量学术论文中提取特定信息变得越来越重要。数据挖掘技术通常用于解决这个问题。然而，挖掘学术论文的数据具有挑战性，因为它需要自动从复杂的非结构化布局文档中提取特定模式。当前的学术论文数据挖掘方法使用基于规则的（RB）或机器学习（ML）方法。然而，使用基于规则的方法需要编写复杂排版论文的高昂成本。另一方面，仅使用机器学习方法需要对文章中复杂内容类型进行注释工作，这可能成本高昂。此外，仅使用机器学习可能会导致基于规则的方法容易识别的模式被错误提取的情况。为了解决这些问题，本文从分析指定著作中使用的标准布局和排版角度出发，提出了一种结合基于规则的方法和机器学习的框架。

    With the widespread use of the internet, it has become increasingly crucial to extract specific information from vast amounts of academic articles efficiently. Data mining techniques are generally employed to solve this issue. However, data mining for academic articles is challenging since it requires automatically extracting specific patterns in complex and unstructured layout documents. Current data mining methods for academic articles employ rule-based(RB) or machine learning(ML) approaches. However, using rule-based methods incurs a high coding cost for complex typesetting articles. On the other hand, simply using machine learning methods requires annotation work for complex content types within the paper, which can be costly. Furthermore, only using machine learning can lead to cases where patterns easily recognized by rule-based methods are mistakenly extracted. To overcome these issues, from the perspective of analyzing the standard layout and typesetting used in the specified p
    
[^45]: 医学影像中的人口统计学不变模型和表示是否公平？

    Are demographically invariant models and representations in medical imaging fair?. (arXiv:2305.01397v1 [cs.LG])

    [http://arxiv.org/abs/2305.01397](http://arxiv.org/abs/2305.01397)

    医学影像模型编码患者人口统计信息，引发有关潜在歧视的担忧。研究表明，不编码人口属性的模型容易损失预测性能，而考虑人口统计属性的反事实模型不变性存在复杂性。人口统计学编码可以被认为是优势。

    

    研究表明，医学成像模型在其潜在表示中编码了有关患者人口统计学信息（年龄、种族、性别），这引发了有关其潜在歧视的担忧。在这里，我们询问是否可行和值得训练不编码人口属性的模型。我们考虑不同类型的与人口统计学属性的不变性，即边际、类条件和反事实模型不变性，并说明它们与算法公平的标准概念的等价性。根据现有理论，我们发现边际和类条件的不变性可被认为是实现某些公平概念的过度限制方法，导致显著的预测性能损失。关于反事实模型不变性，我们注意到对于人口统计学属性，定义医学图像反事实存在复杂性。最后，我们认为人口统计学编码甚至可以被认为是优势。

    Medical imaging models have been shown to encode information about patient demographics (age, race, sex) in their latent representation, raising concerns about their potential for discrimination. Here, we ask whether it is feasible and desirable to train models that do not encode demographic attributes. We consider different types of invariance with respect to demographic attributes marginal, class-conditional, and counterfactual model invariance - and lay out their equivalence to standard notions of algorithmic fairness. Drawing on existing theory, we find that marginal and class-conditional invariance can be considered overly restrictive approaches for achieving certain fairness notions, resulting in significant predictive performance losses. Concerning counterfactual model invariance, we note that defining medical image counterfactuals with respect to demographic attributes is fraught with complexities. Finally, we posit that demographic encoding may even be considered advantageou
    
[^46]: 大规模预训练模型在增量式新类别发现中具有出乎意料的强大表现。

    Large-scale Pre-trained Models are Surprisingly Strong in Incremental Novel Class Discovery. (arXiv:2303.15975v1 [cs.CV])

    [http://arxiv.org/abs/2303.15975](http://arxiv.org/abs/2303.15975)

    本论文提出了一种更加挑战性和实用性的学习方法MSc-iNCD，通过在连续而无人监督的学习中利用大规模预训练模型的丰富先验知识，该方法在增量式新类别发现中表现出出乎意料的强大实力。

    

    在生命长学习者中，从未标记的数据中连续地发现新概念是一个重要的期望。在文献中，这类问题在非常受限的情况下得到了部分解决，其中要么为发现新概念提供有标号的数据（例如 NCD），要么学习在有限数量的增量步骤中发生（例如类 iNCD）。在这项工作中，我们挑战现状，提出了一种更具挑战性和实用性的学习范式，称为 MSc-iNCD，其中学习连续而无人监督，并利用大规模预训练模型的丰富先验知识。为此，我们提出了简单的基线，不仅在较长的学习情境下具有弹性，而且与复杂的最先进方法相比，表现出出乎意料的强大实力。我们在多个基准测试中进行了广泛的实证评估，并展示了我们提出的基线的有效性，大大提升了基准要求。

    Discovering novel concepts from unlabelled data and in a continuous manner is an important desideratum of lifelong learners. In the literature such problems have been partially addressed under very restricted settings, where either access to labelled data is provided for discovering novel concepts (e.g., NCD) or learning occurs for a limited number of incremental steps (e.g., class-iNCD). In this work we challenge the status quo and propose a more challenging and practical learning paradigm called MSc-iNCD, where learning occurs continuously and unsupervisedly, while exploiting the rich priors from large-scale pre-trained models. To this end, we propose simple baselines that are not only resilient under longer learning scenarios, but are surprisingly strong when compared with sophisticated state-of-the-art methods. We conduct extensive empirical evaluation on a multitude of benchmarks and show the effectiveness of our proposed baselines, which significantly raises the bar.
    
[^47]: 用表示学习和组合变量构建多变量网络的视觉分析

    Visual Analytics of Multivariate Networks with Representation Learning and Composite Variable Construction. (arXiv:2303.09590v1 [cs.SI])

    [http://arxiv.org/abs/2303.09590](http://arxiv.org/abs/2303.09590)

    本文提出了一种用于研究多变量网络的视觉分析工作流程，其中包括神经网络学习阶段、降维和优化阶段以及用户交互式可视化接口进行解释。关键的组合变量构建步骤将非线性特征重塑为线性特征，以方便检查和理解。案例研究表明该工作流程具有有效性和可理解性。

    

    多变量网络在真实世界的数据驱动应用中经常被发现。发掘和理解多变量网络中的关系并不是一项简单的任务。本文提出了一种用于研究多变量网络以提取网络不同结构和语义特征之间关联的视觉分析工作流程（例如，什么是在社交网络密度方面与不同属性的组合关系）。该工作流程包括基于神经网络的学习阶段，根据所选输入和输出属性来对数据进行分类，降维和优化阶段以产生一个简化的结果集合以便检查，最后通过用户交互式可视化接口进行解释阶段的操作。我们设计的一个关键部分是组合变量构建步骤，该步骤将由神经网络获得的非线性特征重塑为直观解释的线性特征。我们通过对大型组织员工之间的电子邮件通信数据集进行案例研究，证明了工作流程的有效性和可理解性。

    Multivariate networks are commonly found in real-world data-driven applications. Uncovering and understanding the relations of interest in multivariate networks is not a trivial task. This paper presents a visual analytics workflow for studying multivariate networks to extract associations between different structural and semantic characteristics of the networks (e.g., what are the combinations of attributes largely relating to the density of a social network?). The workflow consists of a neural-network-based learning phase to classify the data based on the chosen input and output attributes, a dimensionality reduction and optimization phase to produce a simplified set of results for examination, and finally an interpreting phase conducted by the user through an interactive visualization interface. A key part of our design is a composite variable construction step that remodels nonlinear features obtained by neural networks into linear features that are intuitive to interpret. We demon
    
[^48]: 深度感知损失网络的系统性能分析：打破迁移学习的约定

    A Systematic Performance Analysis of Deep Perceptual Loss Networks: Breaking Transfer Learning Conventions. (arXiv:2302.04032v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.04032](http://arxiv.org/abs/2302.04032)

    这项工作通过系统评估多种常用的预训练网络及其不同特征提取点，在四个深度感知损失用例上解决了迁移学习中的问题。

    

    深度感知损失是一种在计算机视觉中使用的损失函数，旨在通过使用从神经网络中提取的深度特征来模仿人类感知。近年来，该方法在许多有趣的计算机视觉任务上取得了显著的效果，特别是对于具有图像或类似图像输出的任务，如图像合成、分割、深度预测等。许多应用程序使用预先训练的网络，通常是卷积网络，用于损失计算。尽管对该方法的兴趣和广泛使用增加了，但仍需要更多的努力来探索用于计算深度感知损失的网络以及从哪些层提取特征。本研究旨在通过系统地评估多种常用且易于获取的预训练网络，以及针对四个现有深度感知损失用例的不同特征提取点来纠正这一问题。

    Deep perceptual loss is a type of loss function in computer vision that aims to mimic human perception by using the deep features extracted from neural networks. In recent years, the method has been applied to great effect on a host of interesting computer vision tasks, especially for tasks with image or image-like outputs, such as image synthesis, segmentation, depth prediction, and more. Many applications of the method use pretrained networks, often convolutional networks, for loss calculation. Despite the increased interest and broader use, more effort is needed toward exploring which networks to use for calculating deep perceptual loss and from which layers to extract the features.  This work aims to rectify this by systematically evaluating a host of commonly used and readily available, pretrained networks for a number of different feature extraction points on four existing use cases of deep perceptual loss. The use cases of perceptual similarity, super-resolution, image segmentat
    
[^49]: 基于项目反应理论的解释（eXirt）：一种在信任视角下解释树集成模型的模型特定方法

    Explanations Based on Item Response Theory (eXirt): A Model-Specific Method to Explain Tree-Ensemble Model in Trust Perspective. (arXiv:2210.09933v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.09933](http://arxiv.org/abs/2210.09933)

    在解释黑盒模型的方法中没有共识，本论文提出了一种基于项目反应理论的新方法eXirt，用于解释使用表格数据的树集成模型，并通过全局特征排名来解释模型输入和预测之间的关系。

    

    近年来，XAI研究人员正规范化提案和开发新方法来解释黑盒模型，但在社区中对于使用哪种方法来解释这些模型尚无一致意见，而这种选择几乎直接与特定方法的流行度相关。诸如Ciu、Dalex、Eli5、Lofo、Shap和Skater等方法通过特征相关性的全局排名来解释黑盒模型，这些方法基于不同的方法学生成全局解释，说明模型输入如何解释其预测。在这种情况下，使用了41个数据集、4种树集成算法（Light Gradient Boosting、CatBoost、Random Forest和Gradient Boosting）和6种XAI方法来支持推出一种名为eXirt的新的XAI方法，该方法基于项目反应理论（IRT），旨在解释使用关于二分类问题的表格数据的树集成黑盒模型。在第一组分析中，164个全局特征

    In recent years, XAI researchers have been formalizing proposals and developing new methods to explain black box models, with no general consensus in the community on which method to use to explain these models, with this choice being almost directly linked to the popularity of a specific method. Methods such as Ciu, Dalex, Eli5, Lofo, Shap and Skater emerged with the proposal to explain black box models through global rankings of feature relevance, which based on different methodologies, generate global explanations that indicate how the model's inputs explain its predictions. In this context, 41 datasets, 4 tree-ensemble algorithms (Light Gradient Boosting, CatBoost, Random Forest, and Gradient Boosting), and 6 XAI methods were used to support the launch of a new XAI method, called eXirt, based on Item Response Theory IRT and aimed at tree-ensemble black box models that use tabular data referring to binary classification problems. In the first set of analyses, the 164 global featur
    

