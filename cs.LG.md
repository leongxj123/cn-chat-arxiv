# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Removing the need for ground truth UWB data collection: self-supervised ranging error correction using deep reinforcement learning](https://arxiv.org/abs/2403.19262) | 提出了一种使用深度强化学习进行自监督测距误差校正的方法，无需收集地面真实数据集，实验表明性能与有监督方法相媲美 |
| [^2] | [Paths to Equilibrium in Normal-Form Games](https://arxiv.org/abs/2403.18079) | 本文研究在多智体强化学习中的策略序列，探讨满足特定约束的策略路径，即满足路径，对于构建终止于均衡策略的路径具有重要意义。 |
| [^3] | [ParFormer: Vision Transformer Baseline with Parallel Local Global Token Mixer and Convolution Attention Patch Embedding](https://arxiv.org/abs/2403.15004) | ParFormer提出了并行局部全局标记混合器和卷积注意力补丁嵌入，优化了特征提取能力，在图像分类和对象识别等任务中表现优于CNN和最先进的Transformer架构。 |
| [^4] | [MicroT: Low-Energy and Adaptive Models for MCUs](https://arxiv.org/abs/2403.08040) | MicroT是一个低能耗、多任务自适应模型框架，通过特征提取器和分类器的分离、模型优化和本地任务训练，在MCUs上实现了模型性能的提升和能耗的降低。 |
| [^5] | [GEAR: An Efficient KV Cache Compression Recipefor Near-Lossless Generative Inference of LLM](https://arxiv.org/abs/2403.05527) | GEAR提出了一种高效的KV缓存压缩框架，实现几乎无损的高比率压缩，用于解决大型语言模型推断中因缓存需求增长而导致的记忆绑定问题和性能下降。 |
| [^6] | [Supervised machine learning for microbiomics: bridging the gap between current and best practices](https://arxiv.org/abs/2402.17621) | 该研究通过分析大量期刊文章，总结了监督机器学习在微生物组学中的现有实践，探讨了实验设计方法的优缺点，并提出了如何避免常见实验设计缺陷的指导。 |
| [^7] | [Information-based Transductive Active Learning](https://arxiv.org/abs/2402.15898) | ITL是一种基于信息的转导式学习方法，可以在现实世界设置中自适应采样，以最大化关于指定预测目标的信息获取，并在少样本微调和安全贝叶斯优化应用中显著优于最先进技术。 |
| [^8] | [Mitigating Biases with Diverse Ensembles and Diffusion Models](https://arxiv.org/abs/2311.16176) | 通过利用扩散概率模型（DPMs）生成新特征组合的图像，可以在集成模型中增加模型多样性，并减轻捷径偏见，而无需额外监督信号。 |
| [^9] | [Multiscale Hodge Scattering Networks for Data Analysis](https://arxiv.org/abs/2311.10270) | 提出了多尺度霍奇散射网络（MHSNs），利用多尺度基础词典和卷积结构，生成对节点排列不变的特征。 |
| [^10] | [Efficient generative adversarial networks using linear additive-attention Transformers.](http://arxiv.org/abs/2401.09596) | 这项工作提出了一种名为LadaGAN的高效生成对抗网络，它使用了一种名为Ladaformer的新型Transformer块，通过线性加法注意机制来降低计算复杂度并解决训练不稳定性问题。 |
| [^11] | [Closed-Form Interpretation of Neural Network Classifiers with Symbolic Regression Gradients.](http://arxiv.org/abs/2401.04978) | 本文提出了一种解释神经网络分类器的闭式表示的方法，使其适用于自动化科学发现。这种方法通过将神经网络嵌入到一组基于相同量的等价类中，并通过找到该等价类与符号回归搜索空间中的方程的交集来解释神经网络。 |
| [^12] | [Federated Learning with Reduced Information Leakage and Computation.](http://arxiv.org/abs/2310.06341) | Upcycled-FL是一种减少信息泄漏和计算的联邦学习框架，在每个偶数迭代中应用一阶近似，使得一半的联邦学习更新不会泄漏信息并且需要更少的计算。 |
| [^13] | [Clustering Three-Way Data with Outliers.](http://arxiv.org/abs/2310.05288) | 这项研究提出了一种用于聚类矩阵形式数据的方法，可以处理其中的异常值。 |
| [^14] | [A Unifying Perspective on Non-Stationary Kernels for Deeper Gaussian Processes.](http://arxiv.org/abs/2309.10068) | 本论文提出了一个统一的视角来探讨非平稳核在深层高斯过程中的应用，以提高预测性能和不确定性估计。 |
| [^15] | [Quantifying degeneracy in singular models via the learning coefficient.](http://arxiv.org/abs/2308.12108) | 这项工作介绍了一种称为学习系数的量，用于精确量化深度神经网络中的退化程度，该方法能够区分不同参数区域的退化顺序，并揭示了随机优化器对临界点的归纳偏好。 |
| [^16] | [Joint Microseismic Event Detection and Location with a Detection Transformer.](http://arxiv.org/abs/2307.09207) | 本文提出了一种使用检测变压器将微震事件检测和源定位统一为一个框架的方法，该方法可以实现实时微震监测，并在合成数据上进行了测试。 |
| [^17] | [SDC-HSDD-NDSA: Structure Detecting Cluster by Hierarchical Secondary Directed Differential with Normalized Density and Self-Adaption.](http://arxiv.org/abs/2307.00677) | 本文提出了一种基于密度的聚类算法，能够检测到高密度区域中的结构，具有先前算法所不具备的能力。 |
| [^18] | [HUMAP: Hierarchical Uniform Manifold Approximation and Projection.](http://arxiv.org/abs/2106.07718) | HUMAP是一种新的层次降维技术，能够在层次探索中保留心理地图并在多个数据集和数据类型上具有优越性。 |

# 详细

[^1]: 不需要地面真实数据收集：使用深度强化学习进行自监督测距误差校正

    Removing the need for ground truth UWB data collection: self-supervised ranging error correction using deep reinforcement learning

    [https://arxiv.org/abs/2403.19262](https://arxiv.org/abs/2403.19262)

    提出了一种使用深度强化学习进行自监督测距误差校正的方法，无需收集地面真实数据集，实验表明性能与有监督方法相媲美

    

    室内定位利用UWB技术因其厘米级准确度潜力而备受关注。然而，多径效应和非直射条件导致了基站与标签之间的测距误差。现有的缓解这些测距误差的方法依赖于收集大量标记数据集，这使它们在实际部署中不切实际。本文提出了一种新颖的自监督深度强化学习方法，不需要标记的地面真实数据。一个强化学习代理将通道脉冲响应作为状态，并预测校正以减小校正和估计测距之间的误差。该代理通过结合轨迹的可预测性与过滤和平滑处理生成的校正，自监督地学习和迭代改进。对真实世界UWB测量的实验表明，其性能与最先进的监督方法相媲美。

    arXiv:2403.19262v1 Announce Type: cross  Abstract: Indoor positioning using UWB technology has gained interest due to its centimeter-level accuracy potential. However, multipath effects and non-line-of-sight conditions cause ranging errors between anchors and tags. Existing approaches for mitigating these ranging errors rely on collecting large labeled datasets, making them impractical for real-world deployments. This paper proposes a novel self-supervised deep reinforcement learning approach that does not require labeled ground truth data. A reinforcement learning agent uses the channel impulse response as a state and predicts corrections to minimize the error between corrected and estimated ranges. The agent learns, self-supervised, by iteratively improving corrections that are generated by combining the predictability of trajectories with filtering and smoothening. Experiments on real-world UWB measurements demonstrate comparable performance to state-of-the-art supervised methods, o
    
[^2]: 正态形式博弈中的均衡路径

    Paths to Equilibrium in Normal-Form Games

    [https://arxiv.org/abs/2403.18079](https://arxiv.org/abs/2403.18079)

    本文研究在多智体强化学习中的策略序列，探讨满足特定约束的策略路径，即满足路径，对于构建终止于均衡策略的路径具有重要意义。

    

    在多智体强化学习（MARL）中，智体会反复在时间上交互，并随着新数据的到来修订他们的策略，从而产生一系列策略概况。本文研究满足一种由强化学习中政策更新启发的成对约束的策略序列，其中在第 $t$ 期最优应答的智体在下一期 $t+1$ 不会改变其策略。这种约束仅要求优化智体不更改策略，但并不以任何方式限制其他非最优化智体，因此允许探索。具有此属性的序列被称为满足路径，并在许多 MARL 算法中自然出现。关于战略动态的一个基本问题是：对于给定的博弈和初始策略概况，是否总可以构建一个终止于均衡策略的满足路径？这个问题的解决对应着一些重要含义。

    arXiv:2403.18079v1 Announce Type: cross  Abstract: In multi-agent reinforcement learning (MARL), agents repeatedly interact across time and revise their strategies as new data arrives, producing a sequence of strategy profiles. This paper studies sequences of strategies satisfying a pairwise constraint inspired by policy updating in reinforcement learning, where an agent who is best responding in period $t$ does not switch its strategy in the next period $t+1$. This constraint merely requires that optimizing agents do not switch strategies, but does not constrain the other non-optimizing agents in any way, and thus allows for exploration. Sequences with this property are called satisficing paths, and arise naturally in many MARL algorithms. A fundamental question about strategic dynamics is such: for a given game and initial strategy profile, is it always possible to construct a satisficing path that terminates at an equilibrium strategy? The resolution of this question has implication
    
[^3]: ParFormer：具有并行局部全局标记混合器和卷积注意力补丁嵌入的视觉Transformer基线

    ParFormer: Vision Transformer Baseline with Parallel Local Global Token Mixer and Convolution Attention Patch Embedding

    [https://arxiv.org/abs/2403.15004](https://arxiv.org/abs/2403.15004)

    ParFormer提出了并行局部全局标记混合器和卷积注意力补丁嵌入，优化了特征提取能力，在图像分类和对象识别等任务中表现优于CNN和最先进的Transformer架构。

    

    本文提出了ParFormer作为一种增强型Transformer架构，允许将不同的标记混合器整合到单个阶段中，从而提高特征提取能力。同时整合本地和全局数据，实现对短程和长程空间关系的精确表示，而无需像平移窗口这样需要大量计算的方法。除了并行标记混合器编码器外，我们提供了卷积注意力补丁嵌入(CAPE)，作为标准补丁嵌入的增强，通过卷积注意力模块改进标记混合器提取。我们的全面评估表明，我们的ParFormer在图像分类和物体识别等多个复杂任务中优于基于CNN和最先进的基于Transformer的架构。所提出的CAPE已被证明有益于整体MetaFormer架构，即使使用Id。

    arXiv:2403.15004v1 Announce Type: cross  Abstract: This work presents ParFormer as an enhanced transformer architecture that allows the incorporation of different token mixers into a single stage, hence improving feature extraction capabilities. Integrating both local and global data allows for precise representation of short- and long-range spatial relationships without the need for computationally intensive methods such as shifting windows. Along with the parallel token mixer encoder, We offer the Convolutional Attention Patch Embedding (CAPE) as an enhancement of standard patch embedding to improve token mixer extraction with a convolutional attention module. Our comprehensive evaluation demonstrates that our ParFormer outperforms CNN-based and state-of-the-art transformer-based architectures in image classification and several complex tasks such as object recognition. The proposed CAPE has been demonstrated to benefit the overall MetaFormer architecture, even while utilizing the Id
    
[^4]: MicroT：用于MCUs的低能耗和自适应模型

    MicroT: Low-Energy and Adaptive Models for MCUs

    [https://arxiv.org/abs/2403.08040](https://arxiv.org/abs/2403.08040)

    MicroT是一个低能耗、多任务自适应模型框架，通过特征提取器和分类器的分离、模型优化和本地任务训练，在MCUs上实现了模型性能的提升和能耗的降低。

    

    我们提出了MicroT，这是一个面向资源受限的MCUs的低能耗、多任务自适应模型框架。我们将原始模型划分为特征提取器和分类器。特征提取器通过自监督知识蒸馏获得，并通过模型分割和联合训练进一步优化为部分模型和完整模型。然后将这些模型部署在MCUs上，增加并在本地任务上训练分类器，最终执行关节推理的阶段决策。在这个过程中，部分模型最初处理样本，如果置信度得分低于设定的阈值，完整模型将恢复并继续推理。我们在两个模型、三个数据集和两个MCU板上评估了MicroT。我们的实验评估表明，在处理多个本地任务时，MicroT有效地提高了模型性能并降低了能耗。与未经优化的特征提取器相比，MicroT

    arXiv:2403.08040v1 Announce Type: new  Abstract: We propose MicroT, a low-energy, multi-task adaptive model framework for resource-constrained MCUs. We divide the original model into a feature extractor and a classifier. The feature extractor is obtained through self-supervised knowledge distillation and further optimized into part and full models through model splitting and joint training. These models are then deployed on MCUs, with classifiers added and trained on local tasks, ultimately performing stage-decision for joint inference. In this process, the part model initially processes the sample, and if the confidence score falls below the set threshold, the full model will resume and continue the inference. We evaluate MicroT on two models, three datasets, and two MCU boards. Our experimental evaluation shows that MicroT effectively improves model performance and reduces energy consumption when dealing with multiple local tasks. Compared to the unoptimized feature extractor, MicroT
    
[^5]: GEAR: 一种用于几乎无损生成推断大型语言模型的高效KV缓存压缩方案

    GEAR: An Efficient KV Cache Compression Recipefor Near-Lossless Generative Inference of LLM

    [https://arxiv.org/abs/2403.05527](https://arxiv.org/abs/2403.05527)

    GEAR提出了一种高效的KV缓存压缩框架，实现几乎无损的高比率压缩，用于解决大型语言模型推断中因缓存需求增长而导致的记忆绑定问题和性能下降。

    

    关键-值（KV）缓存已成为加快大型语言模型（LLMs）推断生成速度的事实标准。然而，随着序列长度增加而增长的缓存需求已将LLM推断转变为一个记忆绑定问题，显著地限制了系统吞吐量。现有方法依赖于丢弃不重要的标记或均匀量化所有条目。然而，这种方法往往会产生较高的近似误差来表示压缩后的矩阵。自回归解码过程进一步增加了每个步骤的误差，导致模型生成中的重大偏差和性能恶化。为了解决这一挑战，我们提出了GEAR，一种高效的KV缓存压缩框架，实现几乎无损的高压缩比。

    arXiv:2403.05527v1 Announce Type: cross  Abstract: Key-value (KV) caching has become the de-facto to accelerate generation speed for large language models (LLMs) inference. However, the growing cache demand with increasing sequence length has transformed LLM inference to be a memory bound problem, significantly constraining the system throughput. Existing methods rely on dropping unimportant tokens or quantizing all entries uniformly. Such methods, however, often incur high approximation errors to represent the compressed matrices. The autoregressive decoding process further compounds the error of each step, resulting in critical deviation in model generation and deterioration of performance. To tackle this challenge, we propose GEAR, an efficient KV cache compression framework that achieves near-lossless high-ratio compression. GEAR first applies quantization to majority of entries of similar magnitudes to ultra-low precision. It then employs a low rank matrix to approximate the quant
    
[^6]: 用于微生物组学的监督机器学习：弥合当前和最佳实践之间的差距

    Supervised machine learning for microbiomics: bridging the gap between current and best practices

    [https://arxiv.org/abs/2402.17621](https://arxiv.org/abs/2402.17621)

    该研究通过分析大量期刊文章，总结了监督机器学习在微生物组学中的现有实践，探讨了实验设计方法的优缺点，并提出了如何避免常见实验设计缺陷的指导。

    

    机器学习（ML）将加速临床微生物组学创新，如疾病诊断和预后。这将需要高质量、可重现、可解释的工作流程，其预测能力达到或超过监管机构对临床工具设定的高门槛。我们通过深入分析2021-2022年发表的100篇同行评议的期刊文章，捕捉了当前将监督ML应用于微生物组学数据的实践的一个快照。我们采用数据驱动方法，引导讨论各种实验设计方法的优点，包括关键考虑因素，如如何减轻小数据集大小的影响同时避免数据泄漏。我们进一步提供关于如何避免可能损害模型性能、可信度和可重复性的常见实验设计缺陷的指南。讨论附有一个互动在线教程。

    arXiv:2402.17621v1 Announce Type: cross  Abstract: Machine learning (ML) is set to accelerate innovations in clinical microbiomics, such as in disease diagnostics and prognostics. This will require high-quality, reproducible, interpretable workflows whose predictive capabilities meet or exceed the high thresholds set for clinical tools by regulatory agencies. Here, we capture a snapshot of current practices in the application of supervised ML to microbiomics data, through an in-depth analysis of 100 peer-reviewed journal articles published in 2021-2022. We apply a data-driven approach to steer discussion of the merits of varied approaches to experimental design, including key considerations such as how to mitigate the effects of small dataset size while avoiding data leakage. We further provide guidance on how to avoid common experimental design pitfalls that can hurt model performance, trustworthiness, and reproducibility. Discussion is accompanied by an interactive online tutorial th
    
[^7]: 基于信息的转导式主动学习

    Information-based Transductive Active Learning

    [https://arxiv.org/abs/2402.15898](https://arxiv.org/abs/2402.15898)

    ITL是一种基于信息的转导式学习方法，可以在现实世界设置中自适应采样，以最大化关于指定预测目标的信息获取，并在少样本微调和安全贝叶斯优化应用中显著优于最先进技术。

    

    我们将主动学习推广到解决现实世界中采样受限于可访问域的情况，而预测目标可能位于这个域之外。为此，我们提出了ITL，即基于信息的转导式学习，一种自适应采样的方法，旨在最大化关于指定预测目标的信息获取。在一般正则性假设下，我们展示了ITL收敛到可从可访问数据中获得的最小可能不确定性。我们在两个关键应用中展示了ITL：大型神经网络的少样本微调和安全贝叶斯优化，在两种情况下，ITL明显优于最先进技术。

    arXiv:2402.15898v1 Announce Type: cross  Abstract: We generalize active learning to address real-world settings where sampling is restricted to an accessible region of the domain, while prediction targets may lie outside this region. To this end, we propose ITL, short for information-based transductive learning, an approach which samples adaptively to maximize the information gained about specified prediction targets. We show, under general regularity assumptions, that ITL converges uniformly to the smallest possible uncertainty obtainable from the accessible data. We demonstrate ITL in two key applications: Few-shot fine-tuning of large neural networks and safe Bayesian optimization, and in both cases, ITL significantly outperforms the state-of-the-art.
    
[^8]: 通过多样化合成和扩散模型减轻偏见

    Mitigating Biases with Diverse Ensembles and Diffusion Models

    [https://arxiv.org/abs/2311.16176](https://arxiv.org/abs/2311.16176)

    通过利用扩散概率模型（DPMs）生成新特征组合的图像，可以在集成模型中增加模型多样性，并减轻捷径偏见，而无需额外监督信号。

    

    数据中的虚假相关性，即多个线索可以预测目标标签，常常导致一种称为捷径偏见的现象，即模型依赖于错误的、易学的线索，而忽略可靠的线索。在这项工作中，我们提出了一种利用扩散概率模型（DPMs）的集成多样化框架，用于减轻捷径偏见。我们展示了在特定的训练间隔中，DPMs可以生成具有新特征组合的图像，即使在显示相关输入特征的样本上进行训练。我们利用这一关键属性通过集成不一致性生成合成反事实来增加模型的多样性。我们展示了DPM引导的多样化足以消除对主要捷径线索的依赖，无需额外的监督信号。我们进一步在几个多样化目标上在实证上量化其有效性，并最终展示了改进的泛化性能。

    arXiv:2311.16176v2 Announce Type: replace-cross  Abstract: Spurious correlations in the data, where multiple cues are predictive of the target labels, often lead to a phenomenon known as shortcut bias, where a model relies on erroneous, easy-to-learn cues while ignoring reliable ones. In this work, we propose an ensemble diversification framework exploiting Diffusion Probabilistic Models (DPMs) for shortcut bias mitigation. We show that at particular training intervals, DPMs can generate images with novel feature combinations, even when trained on samples displaying correlated input features. We leverage this crucial property to generate synthetic counterfactuals to increase model diversity via ensemble disagreement. We show that DPM-guided diversification is sufficient to remove dependence on primary shortcut cues, without a need for additional supervised signals. We further empirically quantify its efficacy on several diversification objectives, and finally show improved generalizati
    
[^9]: 用于数据分析的多尺度霍奇散射网络

    Multiscale Hodge Scattering Networks for Data Analysis

    [https://arxiv.org/abs/2311.10270](https://arxiv.org/abs/2311.10270)

    提出了多尺度霍奇散射网络（MHSNs），利用多尺度基础词典和卷积结构，生成对节点排列不变的特征。

    

    我们提出了一种新的散射网络，用于在单纯复合仿射上测量的信号，称为\emph{多尺度霍奇散射网络}（MHSNs）。我们的构造基于单纯复合仿射上的多尺度基础词典，即$\kappa$-GHWT和$\kappa$-HGLET，我们最近为给定单纯复合仿射中的维度$\kappa \in \mathbb{N}$推广了基于节点的广义哈-沃什变换（GHWT）和分层图拉普拉斯特征变换（HGLET）。$\kappa$-GHWT和$\kappa$-HGLET都形成冗余集合（即词典）的多尺度基础向量和给定信号的相应扩展系数。我们的MHSNs使用类似于卷积神经网络（CNN）的分层结构来级联词典系数模的矩。所得特征对单纯复合仿射的重新排序不变（即节点排列的置换

    arXiv:2311.10270v2 Announce Type: replace  Abstract: We propose new scattering networks for signals measured on simplicial complexes, which we call \emph{Multiscale Hodge Scattering Networks} (MHSNs). Our construction is based on multiscale basis dictionaries on simplicial complexes, i.e., the $\kappa$-GHWT and $\kappa$-HGLET, which we recently developed for simplices of dimension $\kappa \in \mathbb{N}$ in a given simplicial complex by generalizing the node-based Generalized Haar-Walsh Transform (GHWT) and Hierarchical Graph Laplacian Eigen Transform (HGLET). The $\kappa$-GHWT and the $\kappa$-HGLET both form redundant sets (i.e., dictionaries) of multiscale basis vectors and the corresponding expansion coefficients of a given signal. Our MHSNs use a layered structure analogous to a convolutional neural network (CNN) to cascade the moments of the modulus of the dictionary coefficients. The resulting features are invariant to reordering of the simplices (i.e., node permutation of the u
    
[^10]: 使用线性加法注意力Transformer的高效生成对抗网络

    Efficient generative adversarial networks using linear additive-attention Transformers. (arXiv:2401.09596v1 [cs.CV])

    [http://arxiv.org/abs/2401.09596](http://arxiv.org/abs/2401.09596)

    这项工作提出了一种名为LadaGAN的高效生成对抗网络，它使用了一种名为Ladaformer的新型Transformer块，通过线性加法注意机制来降低计算复杂度并解决训练不稳定性问题。

    

    尽管像扩散模型（DMs）和生成对抗网络（GANs）等深度生成模型在图像生成方面的能力近年来得到了显著提高，但是它们的成功很大程度上归功于计算复杂的架构。这限制了它们在研究实验室和资源充足的公司中的采用和使用，同时也极大地增加了训练、微调和推理的碳足迹。在这项工作中，我们提出了LadaGAN，这是一个高效的生成对抗网络，它建立在一种名为Ladaformer的新型Transformer块上。该块的主要组成部分是一个线性加法注意机制，它每个头部计算一个注意向量，而不是二次的点积注意力。我们在生成器和判别器中都采用了Ladaformer，这降低了计算复杂度，并克服了Transformer GAN经常出现的训练不稳定性。LadaGAN一直表现优于现有的GANs。

    Although the capacity of deep generative models for image generation, such as Diffusion Models (DMs) and Generative Adversarial Networks (GANs), has dramatically improved in recent years, much of their success can be attributed to computationally expensive architectures. This has limited their adoption and use to research laboratories and companies with large resources, while significantly raising the carbon footprint for training, fine-tuning, and inference. In this work, we present LadaGAN, an efficient generative adversarial network that is built upon a novel Transformer block named Ladaformer. The main component of this block is a linear additive-attention mechanism that computes a single attention vector per head instead of the quadratic dot-product attention. We employ Ladaformer in both the generator and discriminator, which reduces the computational complexity and overcomes the training instabilities often associated with Transformer GANs. LadaGAN consistently outperforms exist
    
[^11]: 用符号回归梯度解释神经网络分类器的闭式表示

    Closed-Form Interpretation of Neural Network Classifiers with Symbolic Regression Gradients. (arXiv:2401.04978v1 [cs.LG])

    [http://arxiv.org/abs/2401.04978](http://arxiv.org/abs/2401.04978)

    本文提出了一种解释神经网络分类器的闭式表示的方法，使其适用于自动化科学发现。这种方法通过将神经网络嵌入到一组基于相同量的等价类中，并通过找到该等价类与符号回归搜索空间中的方程的交集来解释神经网络。

    

    我提出了一个统一的框架来解释神经网络分类器，以实现自动科学发现。与基于神经网络的回归不同，对于分类而言，即使神经网络本身的分类基于可以表示为闭式方程的量，也一般无法找到从神经网络到符号方程的一对一映射。在本文中，我将训练好的神经网络嵌入到一个等价类中，这个等价类的分类函数的决策都基于相同的量。我通过找到这个等价类与由符号回归搜索空间定义的可读的方程的交集来解释神经网络。这种方法不限于分类器或完整的神经网络，还可以应用于隐藏层或潜在空间中的任意神经元，或简化解释神经网络回归器的过程。

    I introduce a unified framework for interpreting neural network classifiers tailored toward automated scientific discovery. In contrast to neural network-based regression, for classification, it is in general impossible to find a one-to-one mapping from the neural network to a symbolic equation even if the neural network itself bases its classification on a quantity that can be written as a closed-form equation. In this paper, I embed a trained neural network into an equivalence class of classifying functions that base their decisions on the same quantity. I interpret neural networks by finding an intersection between this equivalence class and human-readable equations defined by the search space of symbolic regression. The approach is not limited to classifiers or full neural networks and can be applied to arbitrary neurons in hidden layers or latent spaces or to simplify the process of interpreting neural network regressors.
    
[^12]: 减少信息泄漏和计算的联邦学习

    Federated Learning with Reduced Information Leakage and Computation. (arXiv:2310.06341v1 [cs.LG])

    [http://arxiv.org/abs/2310.06341](http://arxiv.org/abs/2310.06341)

    Upcycled-FL是一种减少信息泄漏和计算的联邦学习框架，在每个偶数迭代中应用一阶近似，使得一半的联邦学习更新不会泄漏信息并且需要更少的计算。

    

    联邦学习是一种分布式学习范式，允许多个分散的客户端在不共享本地数据的情况下共同学习一个公共模型。尽管本地数据没有直接暴露，但仍存在隐私问题，因为客户端的敏感信息可以从中间计算中推断出来。此外，随着相同数据在迭代学习过程中的重复使用，这种信息泄漏会不断积累。因此，在设计保护隐私的联邦学习算法时，很难平衡隐私和准确性之间的权衡。在本文中，我们引入了一种新的联邦学习框架Upcycled-FL，它在每个偶数迭代中都应用了一阶近似。在这个框架下，一半的联邦学习更新不会造成信息泄漏，并且需要更少的计算。我们首先对Upcycled-FL的收敛（速率）进行理论分析，然后应用扰动机制来保护隐私。

    Federated learning (FL) is a distributed learning paradigm that allows multiple decentralized clients to collaboratively learn a common model without sharing local data. Although local data is not exposed directly, privacy concerns nonetheless exist as clients' sensitive information can be inferred from intermediate computations. Moreover, such information leakage accumulates substantially over time as the same data is repeatedly used during the iterative learning process. As a result, it can be particularly difficult to balance the privacy-accuracy trade-off when designing privacy-preserving FL algorithms. In this paper, we introduce Upcycled-FL, a novel federated learning framework with first-order approximation applied at every even iteration. Under this framework, half of the FL updates incur no information leakage and require much less computation. We first conduct the theoretical analysis on the convergence (rate) of Upcycled-FL, and then apply perturbation mechanisms to preserve
    
[^13]: 带有异常值的三元数据聚类

    Clustering Three-Way Data with Outliers. (arXiv:2310.05288v1 [stat.ML])

    [http://arxiv.org/abs/2310.05288](http://arxiv.org/abs/2310.05288)

    这项研究提出了一种用于聚类矩阵形式数据的方法，可以处理其中的异常值。

    

    矩阵变量分布是模型聚类领域的最新添加，从而可以分析具有复杂结构（如图像和时间序列）的矩阵形式数据。由于其最近的出现，关于矩阵变量数据的文献有限，对于处理这些模型中的异常值的文献更少。本文讨论了一种用于聚类矩阵变量正态数据的方法。该方法使用子集对数似然的分布，将OCLUST算法扩展到矩阵变量正态数据，并使用迭代方法检测和剪裁异常值。

    Matrix-variate distributions are a recent addition to the model-based clustering field, thereby making it possible to analyze data in matrix form with complex structure such as images and time series. Due to its recent appearance, there is limited literature on matrix-variate data, with even less on dealing with outliers in these models. An approach for clustering matrix-variate normal data with outliers is discussed. The approach, which uses the distribution of subset log-likelihoods, extends the OCLUST algorithm to matrix-variate normal data and uses an iterative approach to detect and trim outliers.
    
[^14]: 非平稳核对深层高斯过程的统一视角

    A Unifying Perspective on Non-Stationary Kernels for Deeper Gaussian Processes. (arXiv:2309.10068v1 [stat.ML])

    [http://arxiv.org/abs/2309.10068](http://arxiv.org/abs/2309.10068)

    本论文提出了一个统一的视角来探讨非平稳核在深层高斯过程中的应用，以提高预测性能和不确定性估计。

    

    高斯过程（GP）是一种流行的用于数据的随机函数近似和不确定性量化的统计技术。在过去的二十年中，由于其优越的预测能力，特别是在数据稀疏情况下，以及其固有的提供强健不确定性估计的能力，GP已被广泛应用于机器学习领域。然而，它们的性能高度依赖于核心方法的复杂定制，这往往在使用标准设置和现成软件工具时使从业者不满意。可以说，GP最重要的组成部分是核函数，它扮演协方差算子的角色。Mat\'ern类的平稳核在大多数应用研究中被使用；低效的预测性能和不现实的不确定性估计往往是其结果。非平稳核表现出更好的性能，但由于其更加复杂的属性，很少被使用。

    The Gaussian process (GP) is a popular statistical technique for stochastic function approximation and uncertainty quantification from data. GPs have been adopted into the realm of machine learning in the last two decades because of their superior prediction abilities, especially in data-sparse scenarios, and their inherent ability to provide robust uncertainty estimates. Even so, their performance highly depends on intricate customizations of the core methodology, which often leads to dissatisfaction among practitioners when standard setups and off-the-shelf software tools are being deployed. Arguably the most important building block of a GP is the kernel function which assumes the role of a covariance operator. Stationary kernels of the Mat\'ern class are used in the vast majority of applied studies; poor prediction performance and unrealistic uncertainty quantification are often the consequences. Non-stationary kernels show improved performance but are rarely used due to their more
    
[^15]: 通过学习系数量化奇异模型中的退化情况

    Quantifying degeneracy in singular models via the learning coefficient. (arXiv:2308.12108v1 [stat.ML])

    [http://arxiv.org/abs/2308.12108](http://arxiv.org/abs/2308.12108)

    这项工作介绍了一种称为学习系数的量，用于精确量化深度神经网络中的退化程度，该方法能够区分不同参数区域的退化顺序，并揭示了随机优化器对临界点的归纳偏好。

    

    深度神经网络(DNN)是具有复杂退化的奇异统计模型。本文阐述了一种称为学习系数的量，它在奇异学习理论中精确地量化了深度神经网络的退化程度。重要的是，我们将证明DNN中的退化不能仅通过计算“平坦”方向的数量来解释。我们提出了一种基于随机梯度 Langevin 动力学的局部学习系数的计算可扩展近似方法。为了验证我们的方法，我们在已知理论值的低维模型上演示了其准确性。重要的是，局部学习系数能够正确恢复感兴趣参数区域之间退化的顺序。对MNIST的实验证明，局部学习系数可以揭示随机优化器对更退化或不太退化的临界点的归纳偏好。

    Deep neural networks (DNN) are singular statistical models which exhibit complex degeneracies. In this work, we illustrate how a quantity known as the \emph{learning coefficient} introduced in singular learning theory quantifies precisely the degree of degeneracy in deep neural networks. Importantly, we will demonstrate that degeneracy in DNN cannot be accounted for by simply counting the number of "flat" directions. We propose a computationally scalable approximation of a localized version of the learning coefficient using stochastic gradient Langevin dynamics. To validate our approach, we demonstrate its accuracy in low-dimensional models with known theoretical values. Importantly, the local learning coefficient can correctly recover the ordering of degeneracy between various parameter regions of interest. An experiment on MNIST shows the local learning coefficient can reveal the inductive bias of stochastic opitmizers for more or less degenerate critical points.
    
[^16]: 使用检测变压器进行联合微震事件检测和定位

    Joint Microseismic Event Detection and Location with a Detection Transformer. (arXiv:2307.09207v1 [physics.geo-ph])

    [http://arxiv.org/abs/2307.09207](http://arxiv.org/abs/2307.09207)

    本文提出了一种使用检测变压器将微震事件检测和源定位统一为一个框架的方法，该方法可以实现实时微震监测，并在合成数据上进行了测试。

    

    微震事件的检测和定位是微震监测的两个主要组成部分，在油藏刺激和演化过程中，为我们提供了宝贵的地下信息。传统的事件检测和定位方法往往需要手动干预和/或大量计算，而当前的机器学习辅助方法通常分别解决检测和定位问题；这些限制阻碍了实时微震监测的潜力。我们提出了一种方法，通过调整卷积神经网络主干和编码器-解码器变压器，将事件检测和源定位统一到一个单一框架中，并直接应用于记录的波形上。该网络以模拟多个微震事件的合成数据进行训练，这些事件对应于疑似微震活动区域中的随机源位置。在SEAM Time Lapse模型的二维剖面上进行了合成测试。

    Microseismic event detection and location are two primary components in microseismic monitoring, which offers us invaluable insights into the subsurface during reservoir stimulation and evolution. Conventional approaches for event detection and location often suffer from manual intervention and/or heavy computation, while current machine learning-assisted approaches typically address detection and location separately; such limitations hinder the potential for real-time microseismic monitoring. We propose an approach to unify event detection and source location into a single framework by adapting a Convolutional Neural Network backbone and an encoder-decoder Transformer with a set-based Hungarian loss, which is applied directly to recorded waveforms. The proposed network is trained on synthetic data simulating multiple microseismic events corresponding to random source locations in the area of suspected microseismic activities. A synthetic test on a 2D profile of the SEAM Time Lapse mod
    
[^17]: SDC-HSDD-NDSA: 使用层次次级导向差异和归一化密度自适应的结构检测聚类算法

    SDC-HSDD-NDSA: Structure Detecting Cluster by Hierarchical Secondary Directed Differential with Normalized Density and Self-Adaption. (arXiv:2307.00677v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.00677](http://arxiv.org/abs/2307.00677)

    本文提出了一种基于密度的聚类算法，能够检测到高密度区域中的结构，具有先前算法所不具备的能力。

    

    基于密度的聚类算法是最受欢迎的聚类算法之一，因为它能够识别任意形状的聚类，只要不同的高密度聚类之间有低密度区域分隔。然而，通过低密度区域将聚类分隔开的要求并不是微不足道的，因为高密度区域可能具有不同的结构，应该被聚类到不同的组中。这种情况说明了我们已知的所有先前基于密度的聚类算法的主要缺陷--无法检测高密度聚类中的结构。因此，本文旨在提供一种基于密度的聚类方案，既具有先前方法的能力，又能够检测到高密度区域中未被低密度区分开的结构。该算法采用层次次级导向差异、层次化、归一化密度以及自适应系数，因此被称为结构检测聚类算法。

    Density-based clustering could be the most popular clustering algorithm since it can identify clusters of arbitrary shape as long as different (high-density) clusters are separated by low-density regions. However, the requirement of the separateness of clusters by low-density regions is not trivial since a high-density region might have different structures which should be clustered into different groups. Such a situation demonstrates the main flaw of all previous density-based clustering algorithms we have known--structures in a high-density cluster could not be detected. Therefore, this paper aims to provide a density-based clustering scheme that not only has the ability previous ones have but could also detect structures in a high-density region not separated by low-density ones. The algorithm employs secondary directed differential, hierarchy, normalized density, as well as the self-adaption coefficient, and thus is called Structure Detecting Cluster by Hierarchical Secondary Direc
    
[^18]: HUMAP：层次统一流形逼近与投影

    HUMAP: Hierarchical Uniform Manifold Approximation and Projection. (arXiv:2106.07718v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.07718](http://arxiv.org/abs/2106.07718)

    HUMAP是一种新的层次降维技术，能够在层次探索中保留心理地图并在多个数据集和数据类型上具有优越性。

    

    数据降维技术有助于分析人员理解高维空间中的模式。这些技术通常以散点图形式呈现，应用于各种科学领域，并促进集群和数据样本之间的相似性分析。针对包含许多粒度或遵循信息可视化准则的数据集的分析，层次降维技术是最适合的方法，因为它们先前呈现了主要结构并可以按需提供详细信息。然而，当前的层次降维技术并不能完全解决文献中存在的问题，因为它们不能在层次级别之间保持投影心理地图，也不适用于大多数数据类型。本文介绍了一种新的层次降维技术HUMAP，旨在灵活地保留本地和全局结构以及心理地图，在层次探索中具有优越性，并在多个数据集和数据类型上提供了实证证据。

    Dimensionality reduction (DR) techniques help analysts understand patterns in high-dimensional spaces. These techniques, often represented by scatter plots, are employed in diverse science domains and facilitate similarity analysis among clusters and data samples. For datasets containing many granularities or when analysis follows the information visualization mantra, hierarchical DR techniques are the most suitable approach since they present major structures beforehand and details on demand. However, current hierarchical DR techniques are not fully capable of addressing literature problems because they do not preserve the projection mental map across hierarchical levels or are not suitable for most data types. This work presents HUMAP, a novel hierarchical dimensionality reduction technique designed to be flexible in preserving local and global structures and the mental map throughout hierarchical exploration. We provide empirical evidence of our technique's superiority compared with
    

