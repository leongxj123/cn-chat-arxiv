# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression](https://arxiv.org/abs/2404.00489) | 提出了PROMPT-SAW模型，利用关系感知图来实现文本提示的压缩，提高了提示的可读性和可解释性。 |
| [^2] | [Moments of Clarity: Streamlining Latent Spaces in Machine Learning using Moment Pooling](https://arxiv.org/abs/2403.08854) | 提出了一种称为Moment Pooling的新方法，通过将Deep Sets中的求和泛化为任意的多变量矩，显著降低机器学习网络的潜在空间维度，在固定的潜在维度下实现更高的有效潜在维度，从而可以直接可视化和解释内部表示。 |
| [^3] | [Accelerating Graph Neural Networks on Real Processing-In-Memory Systems](https://arxiv.org/abs/2402.16731) | 在实际处理内存系统上加速图神经网络，并提出了针对实际PIM系统的智能并行化技术和混合式执行方法。 |
| [^4] | [Machine-learning prediction of tipping and collapse of the Atlantic Meridional Overturning Circulation](https://arxiv.org/abs/2402.14877) | 该研究开发了一种机器学习方法，用于预测在嘈杂的动力系统中具有时间变化参数的倾覆，包括预测大西洋经向翻转环流的倾覆和崩溃。 |
| [^5] | [Corrective Machine Unlearning](https://arxiv.org/abs/2402.14015) | 该论文通过形式化“修正机器消除”来解决受未知操纵影响的数据对训练模型的影响问题，可能仅知道一部分受影响样本。发现纠正消除问题与传统以隐私为导向的消除方法有显著不同的要求。 |
| [^6] | [FlashTex: Fast Relightable Mesh Texturing with LightControlNet](https://arxiv.org/abs/2402.13251) | 提出了FlashTex方法，基于LightControlNet实现了快速自动化3D网格纹理生成，实现了照明与表面材质的解耦，使得网格能够在任何照明环境下正确重照和渲染 |
| [^7] | [Multilinear Mixture of Experts: Scalable Expert Specialization through Factorization](https://arxiv.org/abs/2402.12550) | 多线性专家混合（MMoE）层通过因式分解针对视觉模型提供了一种可扩展的专家特化解决方案，避免了离散专家路由和过高推理时间成本。 |
| [^8] | [Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data](https://arxiv.org/abs/2402.12424) | 本研究探讨了LLM在解释表格数据方面的有效性，比较了文本和图像表格表示对LLM性能的影响，为在表格相关任务上有效使用LLM提供了见解。 |
| [^9] | [Learning Contrastive Feature Representations for Facial Action Unit Detection](https://arxiv.org/abs/2402.06165) | 这项研究提出了一种对比学习框架，通过监督和自监督信号来增强面部动作单元检测模型的性能。采用正样本抽样和权衡重要性的损失函数来应对噪声AU标签和AU类型分布不平衡的挑战。 |
| [^10] | [Don't Label Twice: Quantity Beats Quality when Comparing Binary Classifiers on a Budget](https://arxiv.org/abs/2402.02249) | 在比较两个二元分类器的准确性时，通过收集更多样本的单个标签而不是汇总多个噪声标签能更好地利用预算。 |
| [^11] | [Distributional Off-policy Evaluation with Bellman Residual Minimization](https://arxiv.org/abs/2402.01900) | 这篇论文研究了使用Bellman残差最小化的方法来解决分布式离线策略评估问题，并提出了一种称为能量Bellman残差最小化（EBRM）的方法来估计返回分布。在可实现性假设下，建立了EBRM估计器的有限样本误差界。 |
| [^12] | [Self-supervised learning of video representations from a child's perspective](https://arxiv.org/abs/2402.00300) | 本研究从儿童的视角进行自监督学习，通过长时间的头戴式摄像记录训练视频模型，结果表明这些模型在促进从少量样本中学习行动概念方面非常有效。 |
| [^13] | [Generative Model for Constructing Reaction Path from Initial to Final States.](http://arxiv.org/abs/2401.10721) | 本文提出了一种利用神经网络生成反应路径初始猜测的创新方法，在有机反应的复杂反应路径中取得了良好的效果。 |
| [^14] | [Functional Graphical Models: Structure Enables Offline Data-Driven Optimization.](http://arxiv.org/abs/2401.05442) | 功能图模型（FGMs）通过结构实现了样本高效的数据驱动优化。 |
| [^15] | [A Cost-Efficient FPGA Implementation of Tiny Transformer Model using Neural ODE.](http://arxiv.org/abs/2401.02721) | 本文提出了一种利用神经ODE作为骨干架构的高性价比FPGA实现微型Transformer模型。该模型相比于基于CNN的模型将参数大小减少了94.6%且保持准确性，适用于边缘计算。 |
| [^16] | [Learning quantum states and unitaries of bounded gate complexity.](http://arxiv.org/abs/2310.19882) | 本文证明了学习具有有界门复杂度的量子态和酉算符的样本复杂度与相应的门复杂度线性相关，并且在计算复杂度上存在指数关系，这一结果限制了量子机器学习模型的表达能力。 |
| [^17] | [Stage-Aware Learning for Dynamic Treatments.](http://arxiv.org/abs/2310.19300) | 本论文提出了一种针对动态治疗的阶段感知学习方法，该方法通过估计DTR并优先考虑治疗轨迹与最佳治疗方案在决策阶段上的一致性，在提高样本效率和稳定性方面取得了重要进展。 |
| [^18] | [A new approach to template banks of gravitational waves with higher harmonics: reducing matched-filtering cost by over an order of magnitude.](http://arxiv.org/abs/2310.15233) | 该论文提出了一种新方法，通过在引力波模板库中包含高次谐波模式，利用引力波模式之间的自然关联，可以大幅度减少匹配滤波的成本，并提高搜索引力波事件的灵敏度。 |
| [^19] | [Heterogeneous Graph Neural Networks for Data-driven Traffic Assignment.](http://arxiv.org/abs/2310.13193) | 本论文提出了一种基于异构图神经网络的数据驱动交通分配和交通流学习方法，该方法能够准确捕捉不同链路之间的空间交通模式，优于其他传统神经网络模型，在大规模网络中有着广泛的应用潜力。 |
| [^20] | [Exploring Progress in Multivariate Time Series Forecasting: Comprehensive Benchmarking and Heterogeneity Analysis.](http://arxiv.org/abs/2310.06119) | 该论文旨在解决多元时间序列预测领域中公平基准测试和技术方法选择的争议，并提供对该领域取得的进展的深入洞察。 |
| [^21] | [D$^3$Fields: Dynamic 3D Descriptor Fields for Zero-Shot Generalizable Robotic Manipulation.](http://arxiv.org/abs/2309.16118) | D$^3$Fields是一个动态的三维描述符场，将底层三维环境的动态特性以及语义特征和实例掩模编码起来。它可以灵活地使用不同背景、风格和实例的二维图像指定目标，实现零样本机器人操作任务的可泛化。 |
| [^22] | [Striking a Balance: An Optimal Mechanism Design for Heterogenous Differentially Private Data Acquisition for Logistic Regression.](http://arxiv.org/abs/2309.10340) | 本论文研究了在从隐私敏感卖方收集的数据上执行逻辑回归的问题，设计了一个优化测试损失、卖方隐私和支付的加权组合的最佳机制，通过结合博弈论、统计学习理论和差分隐私的思想，解决了买方的目标函数非凸的问题，并提供了当卖方数量变大时的渐近结果。 |
| [^23] | [HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models.](http://arxiv.org/abs/2307.06949) | HyperDreamBooth是一个超网络，可以从一个人的单张图片中快速生成个性化权重，从而实现在多种背景和风格下合成一个人的面部，保持高保真度并同时保留对多样化风格和语义修改的关键知识。 |
| [^24] | [Fairness-aware Federated Minimax Optimization with Convergence Guarantee.](http://arxiv.org/abs/2307.04417) | 本文提出了一种名为FFALM的算法，通过施加公平约束和解决极小化极大回归问题，在联邦学习中解决了群体公平性问题。实验证明FFALM在处理严重统计异质性问题时具有良好的效果。 |
| [^25] | [Degraded Polygons Raise Fundamental Questions of Neural Network Perception.](http://arxiv.org/abs/2306.04955) | 本文研究了神经网络在识别具有不同程度边缘降解的规则多边形时的性能和行为，发现存在基本问题，揭示了人机视觉差距的另一个角度。 |
| [^26] | [TMI! Finetuned Models Leak Private Information from their Pretraining Data.](http://arxiv.org/abs/2306.01181) | 本文提出了一种新的会员推断威胁模型TMI，用于评估微调模型对预训练数据的泄露，突显了在使用预训练模型进行迁移学习中存在的隐私风险，并需要对机器学习中的隐私进行更严格的评估。 |
| [^27] | [Online-to-PAC Conversions: Generalization Bounds via Regret Analysis.](http://arxiv.org/abs/2305.19674) | 本文提出了在线学习游戏“泛化游戏”的框架，将在线学习算法的表现和统计学习算法的泛化界限联系了起来，并得出了一些标准的泛化限制。 |
| [^28] | [In Defense of Pure 16-bit Floating-Point Neural Networks.](http://arxiv.org/abs/2305.10947) | 本文探讨了纯16位浮点神经网络的被忽视的效率，提供了理论分析来探讨16位和32位模型的差异，并可以定量解释16位模型与其32位对应物之间的条件。 |
| [^29] | [Geolocation Predicting of Tweets Using BERT-Based Models.](http://arxiv.org/abs/2303.07865) | 该论文提出基于BERT模型的推文地理位置预测方法，可以实现全球和美国上的中位误差分别小于30公里和15公里的定位精度。 |
| [^30] | [Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning.](http://arxiv.org/abs/2302.02662) | 本文研究了一种名为GLAM的方法，通过功能基础设施建设，利用在线强化学习提高LLM代理程序的性能来实现LLMs与环境之间的对齐，解决决策问题。 |

# 详细

[^1]: PROMPT-SAW：利用关系感知图进行文本提示压缩

    PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression

    [https://arxiv.org/abs/2404.00489](https://arxiv.org/abs/2404.00489)

    提出了PROMPT-SAW模型，利用关系感知图来实现文本提示的压缩，提高了提示的可读性和可解释性。

    

    大型语言模型(LLMs)在多种不同的自然语言处理任务中展现出卓越的能力。提示是LLM推理中的基本工具，但我们观察到超长提示会带来显著的成本。现有的压缩长提示的尝试导致压缩提示在可读性和可解释性方面表现不佳，对提示效用产生有害影响。为了解决这一问题，我们提出了PROMPT-SAW：通过关系感知图进行提示压缩，这是一种针对任务不可知和任务感知提示的有效策略。PROMPT-SAW使用提示的文本信息构建图形，在图形中提取关键信息元素，从而得出压缩提示。我们还提出了GSM8K-AUG，即现有GSM8k基准的扩展版本，用于任务不可知提示，以提供全面的评估平台。

    arXiv:2404.00489v1 Announce Type: cross  Abstract: Large language models (LLMs) have shown exceptional abilities for multiple different natural language processing tasks. While prompting is a crucial tool for LLM inference, we observe that there is a significant cost associated with exceedingly lengthy prompts. Existing attempts to compress lengthy prompts lead to sub-standard results in terms of readability and interpretability of the compressed prompt, with a detrimental impact on prompt utility. To address this, we propose PROMPT-SAW: Prompt compresSion via Relation AWare graphs, an effective strategy for prompt compression over task-agnostic and task-aware prompts. PROMPT-SAW uses the prompt's textual information to build a graph, later extracts key information elements in the graph to come up with the compressed prompt. We also propose GSM8K-AUG, i.e., an extended version of the existing GSM8k benchmark for task-agnostic prompts in order to provide a comprehensive evaluation platf
    
[^2]: 清晰瞬间：使用Moment Pooling简化机器学习中的潜在空间

    Moments of Clarity: Streamlining Latent Spaces in Machine Learning using Moment Pooling

    [https://arxiv.org/abs/2403.08854](https://arxiv.org/abs/2403.08854)

    提出了一种称为Moment Pooling的新方法，通过将Deep Sets中的求和泛化为任意的多变量矩，显著降低机器学习网络的潜在空间维度，在固定的潜在维度下实现更高的有效潜在维度，从而可以直接可视化和解释内部表示。

    

    许多机器学习应用涉及学习数据的潜在表示，通常是高维且难以直接解释。在这项工作中，我们提出了“Moment Pooling”，这是Deep Sets网络的一个自然延伸，可大幅减少这些网络的潜在空间维度，同时维持甚至提高性能。Moment Pooling将Deep Sets中的求和泛化为任意的多变量矩，使模型能够在固定的潜在维度下实现更高的有效潜在维度。我们将Moment Pooling应用于夸克/胶子喷注分类的对撞机物理任务，通过将Energy Flow Networks（EFNs）扩展为Moment EFNs。我们发现，具有小至1的潜在维度的Moment EFNs表现与具有较高潜在维度的普通EFNs类似。这种小潜在维度使内部表示可以直接可视化和解释。

    arXiv:2403.08854v1 Announce Type: cross  Abstract: Many machine learning applications involve learning a latent representation of data, which is often high-dimensional and difficult to directly interpret. In this work, we propose "Moment Pooling", a natural extension of Deep Sets networks which drastically decrease latent space dimensionality of these networks while maintaining or even improving performance. Moment Pooling generalizes the summation in Deep Sets to arbitrary multivariate moments, which enables the model to achieve a much higher effective latent dimensionality for a fixed latent dimension. We demonstrate Moment Pooling on the collider physics task of quark/gluon jet classification by extending Energy Flow Networks (EFNs) to Moment EFNs. We find that Moment EFNs with latent dimensions as small as 1 perform similarly to ordinary EFNs with higher latent dimension. This small latent dimension allows for the internal representation to be directly visualized and interpreted, w
    
[^3]: 在实际处理内存系统上加速图神经网络

    Accelerating Graph Neural Networks on Real Processing-In-Memory Systems

    [https://arxiv.org/abs/2402.16731](https://arxiv.org/abs/2402.16731)

    在实际处理内存系统上加速图神经网络，并提出了针对实际PIM系统的智能并行化技术和混合式执行方法。

    

    图神经网络（GNNs）是新兴的机器学习模型，用于分析图结构数据。图神经网络（GNN）的执行涉及计算密集型和内存密集型核心，后者在总时间中占主导地位，受数据在内存和处理器之间移动的严重瓶颈所限制。处理内存（PIM）系统可以通过在内存阵列附近或内部放置简单处理器来缓解这种数据移动瓶颈。在这项工作中，我们介绍了PyGim，一个有效的机器学习框架，可以在实际PIM系统上加速GNNs。我们为针对实际PIM系统定制的GNN内存密集型核心提出智能并行化技术，并为它们开发了方便的Python API。我们提供混合式GNN执行，其中计算密集型和内存密集型核心分别在以处理器为中心和以内存为中心的计算系统中执行，以匹配它们的算法特性。我们进行了大量评估。

    arXiv:2402.16731v2 Announce Type: replace-cross  Abstract: Graph Neural Networks (GNNs) are emerging ML models to analyze graph-structure data. Graph Neural Network (GNN) execution involves both compute-intensive and memory-intensive kernels, the latter dominates the total time, being significantly bottlenecked by data movement between memory and processors. Processing-In-Memory (PIM) systems can alleviate this data movement bottleneck by placing simple processors near or inside to memory arrays. In this work, we introduce PyGim, an efficient ML framework that accelerates GNNs on real PIM systems. We propose intelligent parallelization techniques for memory-intensive kernels of GNNs tailored for real PIM systems, and develop handy Python API for them. We provide hybrid GNN execution, in which the compute-intensive and memory-intensive kernels are executed in processor-centric and memory-centric computing systems, respectively, to match their algorithmic nature. We extensively evaluate 
    
[^4]: 机器学习预测大西洋经向翻转环流的倾覆和崩溃

    Machine-learning prediction of tipping and collapse of the Atlantic Meridional Overturning Circulation

    [https://arxiv.org/abs/2402.14877](https://arxiv.org/abs/2402.14877)

    该研究开发了一种机器学习方法，用于预测在嘈杂的动力系统中具有时间变化参数的倾覆，包括预测大西洋经向翻转环流的倾覆和崩溃。

    

    大西洋经向翻转环流(AMOC)的最新研究引发了对其潜在倾覆的担忧，这是由于气候变化导致北大西洋淡水输入增加的一个临界点。预测的崩溃时间窗口大约在本世纪中叶，最早可能在大约两年后开始。更一般地，对系统从一个稳定平衡状态转变到另一个稳定状态的临界点的预测对于广泛领域都是相关的。我们开发了一种机器学习方法，用于预测嘈杂的动力系统中具有时间变化参数的倾覆，并在多个系统上进行测试，包括AMOC、生态网络、电力系统和气候模型。对于AMOC，我们基于模拟指纹数据和海表温度的真实数据进行预测，将潜在倾覆的时间窗口置于

    arXiv:2402.14877v1 Announce Type: cross  Abstract: Recent research on the Atlantic Meridional Overturning Circulation (AMOC) raised concern about its potential collapse through a tipping point due to the climate-change caused increase in the freshwater input into the North Atlantic. The predicted time window of collapse is centered about the middle of the century and the earliest possible start is approximately two years from now. More generally, anticipating a tipping point at which the system transitions from one stable steady state to another is relevant to a broad range of fields. We develop a machine-learning approach to predicting tipping in noisy dynamical systems with a time-varying parameter and test it on a number of systems including the AMOC, ecological networks, an electrical power system, and a climate model. For the AMOC, our prediction based on simulated fingerprint data and real data of the sea surface temperature places the time window of a potential collapse between 
    
[^5]: 修正机器消除

    Corrective Machine Unlearning

    [https://arxiv.org/abs/2402.14015](https://arxiv.org/abs/2402.14015)

    该论文通过形式化“修正机器消除”来解决受未知操纵影响的数据对训练模型的影响问题，可能仅知道一部分受影响样本。发现纠正消除问题与传统以隐私为导向的消除方法有显著不同的要求。

    

    机器学习模型越来越面临数据完整性挑战，因为它们使用了大规模的从互联网中获取的训练数据集。本文研究了如果模型开发者发现某些数据被篡改或错误，他们可以采取什么措施。这些被篡改的数据会导致不利影响，如容易受到后门样本的攻击、系统性偏见，以及在某些输入领域的准确度降低。通常，并非所有被篡改的训练样本都是已知的，而只有一小部分代表性的受影响数据被标记。

    arXiv:2402.14015v1 Announce Type: cross  Abstract: Machine Learning models increasingly face data integrity challenges due to the use of large-scale training datasets drawn from the internet. We study what model developers can do if they detect that some data was manipulated or incorrect. Such manipulated data can cause adverse effects like vulnerability to backdoored samples, systematic biases, and in general, reduced accuracy on certain input domains. Often, all manipulated training samples are not known, and only a small, representative subset of the affected data is flagged.   We formalize "Corrective Machine Unlearning" as the problem of mitigating the impact of data affected by unknown manipulations on a trained model, possibly knowing only a subset of impacted samples. We demonstrate that the problem of corrective unlearning has significantly different requirements from traditional privacy-oriented unlearning. We find most existing unlearning methods, including the gold-standard
    
[^6]: FlashTex：具有LightControlNet的快速可重塑网格纹理

    FlashTex: Fast Relightable Mesh Texturing with LightControlNet

    [https://arxiv.org/abs/2402.13251](https://arxiv.org/abs/2402.13251)

    提出了FlashTex方法，基于LightControlNet实现了快速自动化3D网格纹理生成，实现了照明与表面材质的解耦，使得网格能够在任何照明环境下正确重照和渲染

    

    手动为3D网格创建纹理费时费力，即使对于专家视觉内容创建者也是如此。我们提出了一种快速方法，根据用户提供的文本提示自动为输入的3D网格着色。重要的是，我们的方法将照明与表面材质/反射在生成的纹理中解耦，以便网格可以在任何照明环境中正确重照和渲染。我们引入了LightControlNet，这是一种基于ControlNet架构的新文本到图像模型，允许将所需照明规格作为对模型的条件图像。我们的文本到纹理管道然后分两个阶段构建纹理。第一阶段使用LightControlNet生成网格的一组稀疏的视觉一致的参考视图。第二阶段应用基于分数蒸馏采样（SDS）的纹理优化，通过LightControlNet来提高纹理质量同时解耦表面材质

    arXiv:2402.13251v1 Announce Type: cross  Abstract: Manually creating textures for 3D meshes is time-consuming, even for expert visual content creators. We propose a fast approach for automatically texturing an input 3D mesh based on a user-provided text prompt. Importantly, our approach disentangles lighting from surface material/reflectance in the resulting texture so that the mesh can be properly relit and rendered in any lighting environment. We introduce LightControlNet, a new text-to-image model based on the ControlNet architecture, which allows the specification of the desired lighting as a conditioning image to the model. Our text-to-texture pipeline then constructs the texture in two stages. The first stage produces a sparse set of visually consistent reference views of the mesh using LightControlNet. The second stage applies a texture optimization based on Score Distillation Sampling (SDS) that works with LightControlNet to increase the texture quality while disentangling surf
    
[^7]: 多线性专家混合：通过因式分解实现可扩展的专家特化

    Multilinear Mixture of Experts: Scalable Expert Specialization through Factorization

    [https://arxiv.org/abs/2402.12550](https://arxiv.org/abs/2402.12550)

    多线性专家混合（MMoE）层通过因式分解针对视觉模型提供了一种可扩展的专家特化解决方案，避免了离散专家路由和过高推理时间成本。

    

    专家混合（MoE）范式提供了一种强大的方法，将难以理解的密集层分解为更小、模块化的计算，通常更易于人类解释、调试和编辑。然而，一个主要问题在于扩展专家数量的计算成本，以实现足够精细的专业化。本文提出了多线性专家混合（MMoE）层来解决这个问题，重点放在视觉模型上。MMoE层完全以因式化形式对庞大的权重张量进行隐式计算。因此，MMoEs既避免了在流行的“稀疏”MoE模型中离散专家路由所造成的问题，又不会引起“软”MoE替代方案中过高的推理时间成本。我们通过可视化和反事实干预，提供了定性和定量证据，证明了扩展MMoE层的效果。

    arXiv:2402.12550v1 Announce Type: cross  Abstract: The Mixture of Experts (MoE) paradigm provides a powerful way to decompose inscrutable dense layers into smaller, modular computations often more amenable to human interpretation, debugging, and editability. A major problem however lies in the computational cost of scaling the number of experts to achieve sufficiently fine-grained specialization. In this paper, we propose the Multilinear Mixutre of Experts (MMoE) layer to address this, focusing on vision models. MMoE layers perform an implicit computation on prohibitively large weight tensors entirely in factorized form. Consequently, MMoEs both (1) avoid the issues incurred through the discrete expert routing in the popular 'sparse' MoE models, yet (2) do not incur the restrictively high inference-time costs of 'soft' MoE alternatives. We present both qualitative and quantitative evidence (through visualization and counterfactual interventions respectively) that scaling MMoE layers wh
    
[^8]: 表格作为图片？探讨LLM在多模态表格数据表示上的优势和局限性

    Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data

    [https://arxiv.org/abs/2402.12424](https://arxiv.org/abs/2402.12424)

    本研究探讨了LLM在解释表格数据方面的有效性，比较了文本和图像表格表示对LLM性能的影响，为在表格相关任务上有效使用LLM提供了见解。

    

    在本文中，我们通过不同的提示策略和数据格式研究了各种LLM在解释表格数据方面的有效性。我们的分析涵盖了六个针对与表格相关任务的基准，如问答和事实核查。我们首次介绍了LLM在基于图像的表格表示上的表现评估。具体地，我们比较了五种基于文本和三种基于图像的表格表示，展示了表示和提示对LLM性能的影响。我们的研究为在表格相关任务上有效使用LLM提供了见解。

    arXiv:2402.12424v1 Announce Type: cross  Abstract: In this paper, we investigate the effectiveness of various LLMs in interpreting tabular data through different prompting strategies and data formats. Our analysis extends across six benchmarks for table-related tasks such as question-answering and fact-checking. We introduce for the first time the assessment of LLMs' performance on image-based table representations. Specifically, we compare five text-based and three image-based table representations, demonstrating the influence of representation and prompting on LLM performance. Our study provides insights into the effective use of LLMs on table-related tasks.
    
[^9]: 学习对比特征表示来进行面部动作单元检测

    Learning Contrastive Feature Representations for Facial Action Unit Detection

    [https://arxiv.org/abs/2402.06165](https://arxiv.org/abs/2402.06165)

    这项研究提出了一种对比学习框架，通过监督和自监督信号来增强面部动作单元检测模型的性能。采用正样本抽样和权衡重要性的损失函数来应对噪声AU标签和AU类型分布不平衡的挑战。

    

    面部动作单元（AU）检测的主要方法涉及监督的多标签二进制分类问题。现有的方法常常对AU的像素级信息进行编码，从而对模型的复杂性和表达能力提出了很大的要求。此外，由于存在噪声AU标签，这种做法增加了过拟合的风险。在本研究中，我们引入了一个对比学习框架，通过监督和自监督信号增强。目标是在AU检测领域中摆脱传统的像素级学习范式，获得判别特征。为了应对噪声AU标签带来的挑战，我们通过引入自监督信号来增强监督信号。这种增强是通过正样本抽样实现的，包括三种不同类型的正样本对。另外，为了减轻每个AU类型的分布不平衡问题，我们采用了一种权衡重要性的损失函数。

    The predominant approach to facial action unit (AU) detection revolves around a supervised multi-label binary classification problem. Existing methodologies often encode pixel-level information of AUs, thereby imposing substantial demands on model complexity and expressiveness. Moreover, this practice elevates the susceptibility to overfitting due to the presence of noisy AU labels. In the present study, we introduce a contrastive learning framework enhanced by both supervised and self-supervised signals. The objective is to acquire discriminative features, deviating from the conventional pixel-level learning paradigm within the domain of AU detection. To address the challenge posed by noisy AU labels, we augment the supervised signal through the introduction of a self-supervised signal. This augmentation is achieved through positive sample sampling, encompassing three distinct types of positive sample pairs. Furthermore, to mitigate the imbalanced distribution of each AU type, we empl
    
[^10]: 不要重复标记：在有限预算下比较二元分类器时，数量胜过质量

    Don't Label Twice: Quantity Beats Quality when Comparing Binary Classifiers on a Budget

    [https://arxiv.org/abs/2402.02249](https://arxiv.org/abs/2402.02249)

    在比较两个二元分类器的准确性时，通过收集更多样本的单个标签而不是汇总多个噪声标签能更好地利用预算。

    

    我们研究了如何更好地利用有限预算来比较两个二元分类器的准确性。通常的做法是通过多次收集和汇总给定数据点的多个噪声标签，通过多数投票形成一个不太噪声的标签。我们证明了一个与常识相反的定理。如果目标是确定两个分类器中的较好者，我们展示了更好的做法是将预算用于收集更多样本的单个标签。我们的结果来自于对Cram\'er定理的非平凡应用，这是大偏差理论中的一个重要工具。我们讨论了我们的工作对机器学习基准设计的影响，其中它们推翻了一些历史上的建议。此外，我们的结果提供了比Hoeffding界更优的样本大小界限。

    We study how to best spend a budget of noisy labels to compare the accuracy of two binary classifiers. It's common practice to collect and aggregate multiple noisy labels for a given data point into a less noisy label via a majority vote. We prove a theorem that runs counter to conventional wisdom. If the goal is to identify the better of two classifiers, we show it's best to spend the budget on collecting a single label for more samples. Our result follows from a non-trivial application of Cram\'er's theorem, a staple in the theory of large deviations. We discuss the implications of our work for the design of machine learning benchmarks, where they overturn some time-honored recommendations. In addition, our results provide sample size bounds superior to what follows from Hoeffding's bound.
    
[^11]: 使用Bellman残差最小化的分布式离线策略评估

    Distributional Off-policy Evaluation with Bellman Residual Minimization

    [https://arxiv.org/abs/2402.01900](https://arxiv.org/abs/2402.01900)

    这篇论文研究了使用Bellman残差最小化的方法来解决分布式离线策略评估问题，并提出了一种称为能量Bellman残差最小化（EBRM）的方法来估计返回分布。在可实现性假设下，建立了EBRM估计器的有限样本误差界。

    

    我们考虑分布式离线策略评估的问题，它是许多分布式强化学习（DRL）算法的基础。与大多数现有的方法（依赖于最大值-扩展的统计距离，如最大值Wasserstein距离）不同，我们研究用于量化分布式Bellman残差的期望-扩展的统计距离，并且证明它可以上界估计返回分布的期望误差。基于这个有吸引力的性质，通过将Bellman残差最小化框架推广到DRL，我们提出了一种称为能量Bellman残差最小化（EBRM）的方法来估计返回分布。我们在可实现性假设下建立了EBRM估计器的有限样本误差界。此外，我们引入了一种基于多步引导过程的方法的变体，以实现多步扩展。通过选择适当的步长，我们获得了更好的误差界。

    We consider the problem of distributional off-policy evaluation which serves as the foundation of many distributional reinforcement learning (DRL) algorithms. In contrast to most existing works (that rely on supremum-extended statistical distances such as supremum-Wasserstein distance), we study the expectation-extended statistical distance for quantifying the distributional Bellman residuals and show that it can upper bound the expected error of estimating the return distribution. Based on this appealing property, by extending the framework of Bellman residual minimization to DRL, we propose a method called Energy Bellman Residual Minimizer (EBRM) to estimate the return distribution. We establish a finite-sample error bound for the EBRM estimator under the realizability assumption. Furthermore, we introduce a variant of our method based on a multi-step bootstrapping procedure to enable multi-step extension. By selecting an appropriate step level, we obtain a better error bound for thi
    
[^12]: 从儿童视角进行自监督学习的视频表示

    Self-supervised learning of video representations from a child's perspective

    [https://arxiv.org/abs/2402.00300](https://arxiv.org/abs/2402.00300)

    本研究从儿童的视角进行自监督学习，通过长时间的头戴式摄像记录训练视频模型，结果表明这些模型在促进从少量样本中学习行动概念方面非常有效。

    

    儿童通过几年的自我视觉经验学习到了强大的世界内部模型。这些内部模型能否通过儿童的视觉体验和通用的自监督学习算法来学习，还是需要强大的归纳偏差？最近，在收集大规模、纵向的发展现实视频数据集以及通用的自监督学习算法的进展使我们能够开始探讨这个本质与养育之间的问题。然而，现有的工作通常关注基于图像的自监督学习算法和可以从静态图像中学习的视觉能力（例如目标识别），从而忽略了世界的时间性质。为了弥合这一差距，我们在一个儿童早期发展阶段（6-31个月）从儿童的头戴式摄像记录中训练自监督视频模型。所得到的模型在促进从少量样本中学习行动概念方面非常有效。

    Children learn powerful internal models of the world around them from a few years of egocentric visual experience. Can such internal models be learned from a child's visual experience with highly generic learning algorithms or do they require strong inductive biases? Recent advances in collecting large-scale, longitudinal, developmentally realistic video datasets and generic self-supervised learning (SSL) algorithms are allowing us to begin to tackle this nature vs. nurture question. However, existing work typically focuses on image-based SSL algorithms and visual capabilities that can be learned from static images (e.g. object recognition), thus ignoring temporal aspects of the world. To close this gap, here we train self-supervised video models on longitudinal, egocentric headcam recordings collected from a child over a two year period in their early development (6-31 months). The resulting models are highly effective at facilitating the learning of action concepts from a small numbe
    
[^13]: 从初始状态到最终状态的反应路径的生成模型

    Generative Model for Constructing Reaction Path from Initial to Final States. (arXiv:2401.10721v1 [physics.comp-ph])

    [http://arxiv.org/abs/2401.10721](http://arxiv.org/abs/2401.10721)

    本文提出了一种利用神经网络生成反应路径初始猜测的创新方法，在有机反应的复杂反应路径中取得了良好的效果。

    

    绘制反应路径及其相应的活化能垒是分子模拟的一个重要方面。由于其固有的复杂性和非线性，甚至生成这些路径的初始猜测都仍然是一个具有挑战性的问题。本文提出了一种创新方法，利用神经网络生成这些反应路径的初始猜测。该方法通过输入初始状态的坐标，随后逐步对其结构进行改变。这个迭代过程最终生成了对反应路径的近似表示以及最终状态的坐标。该方法的应用范围扩展到有机反应所示的复杂反应路径。训练是在Transition1x数据集上进行的，该数据集包含有机反应路径数据。结果显示生成的反应与相应的测试数据具有相当的相似性。该方法具有灵活性。

    Mapping out reaction pathways and their corresponding activation barriers is a significant aspect of molecular simulation. Given their inherent complexity and nonlinearity, even generating a initial guess of these paths remains a challenging problem. Presented in this paper is an innovative approach that utilizes neural networks to generate initial guess for these reaction pathways. The proposed method is initiated by inputting the coordinates of the initial state, followed by progressive alterations to its structure. This iterative process culminates in the generation of the approximate representation of the reaction path and the coordinates of the final state. The application of this method extends to complex reaction pathways illustrated by organic reactions. Training was executed on the Transition1x dataset, an organic reaction pathway dataset. The results revealed generation of reactions that bore substantial similarities with the corresponding test data. The method's flexibility 
    
[^14]: 功能图模型：结构实现离线数据驱动优化

    Functional Graphical Models: Structure Enables Offline Data-Driven Optimization. (arXiv:2401.05442v1 [cs.LG])

    [http://arxiv.org/abs/2401.05442](http://arxiv.org/abs/2401.05442)

    功能图模型（FGMs）通过结构实现了样本高效的数据驱动优化。

    

    虽然机器学习模型通常是为了解决预测问题而训练的，但我们经常希望将它们用于优化问题。例如，给定一组蛋白质及其对应的荧光水平的数据集，我们可能希望为具有最高荧光的新蛋白质进行优化。这种数据驱动的优化（DDO）面临着一系列挑战，超出了标准预测问题中的挑战，因为我们需要成功预测在训练集中没有见过的优于最佳设计的新设计的性能的模型。从理论上讲，甚至不清楚现有方法什么时候甚至能比简单地选择数据集中最佳设计的朴素方法执行得更好。在本文中，我们研究了如何通过结构实现高效的数据驱动优化。为了形式化结构的概念，我们引入了功能图模型（FGMs）并从理论上展示了它们如何通过分解实现基于数据的优化。

    While machine learning models are typically trained to solve prediction problems, we might often want to use them for optimization problems. For example, given a dataset of proteins and their corresponding fluorescence levels, we might want to optimize for a new protein with the highest possible fluorescence. This kind of data-driven optimization (DDO) presents a range of challenges beyond those in standard prediction problems, since we need models that successfully predict the performance of new designs that are better than the best designs seen in the training set. It is not clear theoretically when existing approaches can even perform better than the naive approach that simply selects the best design in the dataset. In this paper, we study how structure can enable sample-efficient data-driven optimization. To formalize the notion of structure, we introduce functional graphical models (FGMs) and show theoretically how they can provide for principled data-driven optimization by decomp
    
[^15]: 利用神经ODE的高性价比FPGA实现微型Transformer模型

    A Cost-Efficient FPGA Implementation of Tiny Transformer Model using Neural ODE. (arXiv:2401.02721v1 [cs.LG])

    [http://arxiv.org/abs/2401.02721](http://arxiv.org/abs/2401.02721)

    本文提出了一种利用神经ODE作为骨干架构的高性价比FPGA实现微型Transformer模型。该模型相比于基于CNN的模型将参数大小减少了94.6%且保持准确性，适用于边缘计算。

    

    Transformer是一种具有注意机制的新兴神经网络模型。它已经被用于各种任务，并且相比于CNN和RNN取得了良好的准确性。虽然注意机制被认为是一种通用的组件，但是许多Transformer模型与基于CNN的模型相比需要大量的参数。为了减少计算复杂性，最近提出了一种混合方法，它使用ResNet作为骨干架构，并将部分卷积层替换为MHSA（多头自注意）机制。在本文中，我们通过使用神经ODE（常微分方程）而不是ResNet作为骨干架构，显著减少了这种模型的参数大小。所提出的混合模型相比于基于CNN的模型将参数大小减少了94.6%，而且没有降低准确性。接着，我们将所提出的模型部署在一台适度规模的FPGA设备上进行边缘计算。

    Transformer is an emerging neural network model with attention mechanism. It has been adopted to various tasks and achieved a favorable accuracy compared to CNNs and RNNs. While the attention mechanism is recognized as a general-purpose component, many of the Transformer models require a significant number of parameters compared to the CNN-based ones. To mitigate the computational complexity, recently, a hybrid approach has been proposed, which uses ResNet as a backbone architecture and replaces a part of its convolution layers with an MHSA (Multi-Head Self-Attention) mechanism. In this paper, we significantly reduce the parameter size of such models by using Neural ODE (Ordinary Differential Equation) as a backbone architecture instead of ResNet. The proposed hybrid model reduces the parameter size by 94.6% compared to the CNN-based ones without degrading the accuracy. We then deploy the proposed model on a modest-sized FPGA device for edge computing. To further reduce FPGA resource u
    
[^16]: 学习有界门复杂度的量子态和酉算符

    Learning quantum states and unitaries of bounded gate complexity. (arXiv:2310.19882v1 [quant-ph])

    [http://arxiv.org/abs/2310.19882](http://arxiv.org/abs/2310.19882)

    本文证明了学习具有有界门复杂度的量子态和酉算符的样本复杂度与相应的门复杂度线性相关，并且在计算复杂度上存在指数关系，这一结果限制了量子机器学习模型的表达能力。

    

    尽管量子状态重构非常困难，但对于实际应用中的重构者来说，大多数状态的兴趣不大。鉴于自然界中出现的状态和酉算符都具有有界的门复杂度，自然而然地想问是否可以实现高效的学习。在这项工作中，我们证明了为了将由$G$个两量子比特门生成的状态学习到小的迹距离，需要和充分的样本复杂度与$G$线性比例。我们还证明了学习由$G$个门生成的酉算符到小的平均误差的最优查询复杂度与$G$线性比例。虽然可以实现样本高效的学习，但我们展示了在合理的密码学猜想下，学习门复杂度为$G$的状态和酉算符的计算复杂度必须与$G$指数比例。我们阐明了这些结果如何确定了量子机器学习模型的表达能力的基本限制，并提供了对无免费午餐定理的新视角。

    While quantum state tomography is notoriously hard, most states hold little interest to practically-minded tomographers. Given that states and unitaries appearing in Nature are of bounded gate complexity, it is natural to ask if efficient learning becomes possible. In this work, we prove that to learn a state generated by a quantum circuit with $G$ two-qubit gates to a small trace distance, a sample complexity scaling linearly in $G$ is necessary and sufficient. We also prove that the optimal query complexity to learn a unitary generated by $G$ gates to a small average-case error scales linearly in $G$. While sample-efficient learning can be achieved, we show that under reasonable cryptographic conjectures, the computational complexity for learning states and unitaries of gate complexity $G$ must scale exponentially in $G$. We illustrate how these results establish fundamental limitations on the expressivity of quantum machine learning models and provide new perspectives on no-free-lun
    
[^17]: 针对动态治疗的阶段感知学习

    Stage-Aware Learning for Dynamic Treatments. (arXiv:2310.19300v1 [stat.ML])

    [http://arxiv.org/abs/2310.19300](http://arxiv.org/abs/2310.19300)

    本论文提出了一种针对动态治疗的阶段感知学习方法，该方法通过估计DTR并优先考虑治疗轨迹与最佳治疗方案在决策阶段上的一致性，在提高样本效率和稳定性方面取得了重要进展。

    

    最近对动态治疗方案（DTRs）的研究取得了重要进展，提出了强大的优化治疗搜索算法，根据个体具体需求量身定制，并能最大化其预期的临床效益。然而，现有算法在优化治疗下可能会受到样本量不足的困扰，尤其是在涉及长时间决策阶段的慢性疾病中。为了解决这些挑战，我们提出了一种新颖的个体化学习方法，重点是估计DTR，并优先考虑观察到的治疗轨迹与最佳治疗方案在决策阶段上的一致性。通过放宽观察到的轨迹必须完全与最佳治疗一致的限制，我们的方法大大提高了基于倒数概率加权方法的样本效率和稳定性。具体而言，所提出的学习方案构建了一个更通用的框架，包括了流行的结果加权学习框架。

    Recent advances in dynamic treatment regimes (DTRs) provide powerful optimal treatment searching algorithms, which are tailored to individuals' specific needs and able to maximize their expected clinical benefits. However, existing algorithms could suffer from insufficient sample size under optimal treatments, especially for chronic diseases involving long stages of decision-making. To address these challenges, we propose a novel individualized learning method which estimates the DTR with a focus on prioritizing alignment between the observed treatment trajectory and the one obtained by the optimal regime across decision stages. By relaxing the restriction that the observed trajectory must be fully aligned with the optimal treatments, our approach substantially improves the sample efficiency and stability of inverse probability weighted based methods. In particular, the proposed learning scheme builds a more general framework which includes the popular outcome weighted learning framewo
    
[^18]: 一个新方法用于带有更高次谐波的引力波模板库：通过十倍减少匹配滤波成本

    A new approach to template banks of gravitational waves with higher harmonics: reducing matched-filtering cost by over an order of magnitude. (arXiv:2310.15233v1 [gr-qc])

    [http://arxiv.org/abs/2310.15233](http://arxiv.org/abs/2310.15233)

    该论文提出了一种新方法，通过在引力波模板库中包含高次谐波模式，利用引力波模式之间的自然关联，可以大幅度减少匹配滤波的成本，并提高搜索引力波事件的灵敏度。

    

    引力波事件的搜索使用信号模型或模板。目前在LIGO-Virgo-Kagra (LVK)数据中使用的模板仅模拟了信号的主导四极模式$(\ell,m)=(2,2)$，忽略了次要的高阶模式(HM)例如$(\ell,m)=(3,3)$，$(4,4)$，这些模式是由广义相对论预测的。因此，这些搜索可能会在参数空间的一些有趣区域，如高质量和非对称质量比的系统中失去对黑洞合并的灵敏度。我们开发了一种新策略，将HM包含在模板库中，利用这些模式之间的自然关联。我们使用了牛顿附加公式和机器学习工具来模拟与给定$(2,2)$波形相对应的自旋对齐的$(3,3)$，$(4,4)$波形。可以对每个模式的数据进行单独滤波，得到信噪比(SNR)的独立时间序列，然后可以以相对廉价的方式将其组合起来进行综合。

    Searches for gravitational wave events use models, or templates, for the signals of interest. The templates used in current searches in the LIGO-Virgo-Kagra (LVK) data model the dominant quadrupole mode $(\ell,m)=(2,2)$ of the signals, and omit sub-dominant higher-order modes (HM) such as $(\ell,m)=(3,3)$, $(4,4)$, which are predicted by general relativity. Hence, these searches could lose sensitivity to black hole mergers in interesting parts of parameter space, such as systems with high-masses and asymmetric mass ratios. We develop a new strategy to include HM in template banks that exploits the natural connection between the modes. We use a combination of post-Newtonian formulae and machine learning tools to model aligned-spin $(3,3)$, $(4,4)$ waveforms corresponding to a given $(2,2)$ waveform. Each of these modes can be individually filtered against the data to yield separate timeseries of signal-to-noise ratios (SNR), which can be combined in a relatively inexpensive way to margi
    
[^19]: 基于异构图神经网络的数据驱动交通分配研究

    Heterogeneous Graph Neural Networks for Data-driven Traffic Assignment. (arXiv:2310.13193v1 [cs.LG])

    [http://arxiv.org/abs/2310.13193](http://arxiv.org/abs/2310.13193)

    本论文提出了一种基于异构图神经网络的数据驱动交通分配和交通流学习方法，该方法能够准确捕捉不同链路之间的空间交通模式，优于其他传统神经网络模型，在大规模网络中有着广泛的应用潜力。

    

    交通分配问题是交通流分析的重要组成部分之一，已经提出了各种解决方法。然而，将这些方法应用于大规模网络面临重大挑战。在本文中，我们利用异构图神经网络的强大能力，提出了一种新颖的基于数据驱动的交通分配和交通流学习方法。所提出的模型能够捕捉不同链路之间的空间交通模式，从而产生高度准确的结果。我们在城市交通网络上进行了数值实验，并展示了该异构图神经网络模型在收敛速度、训练损失和预测准确度方面优于其他传统神经网络模型的表现。值得注意的是，所提出的异构图神经网络模型还可以推广到不同的网络拓扑。这种方法为复杂交通流分析和预测提供了一种有希望的解决方案。

    The traffic assignment problem is one of the significant components of traffic flow analysis for which various solution approaches have been proposed. However, deploying these approaches for large-scale networks poses significant challenges. In this paper, we leverage the power of heterogeneous graph neural networks to propose a novel data-driven approach for traffic assignment and traffic flow learning. The proposed model is capable of capturing spatial traffic patterns across different links, yielding highly accurate results. We present numerical experiments on urban transportation networks and show that the proposed heterogeneous graph neural network model outperforms other conventional neural network models in terms of convergence rate, training loss, and prediction accuracy. Notably, the proposed heterogeneous graph neural network model can also be generalized to different network topologies. This approach offers a promising solution for complex traffic flow analysis and predictio
    
[^20]: 探索多元时间序列预测的进展：全面基准测试和异质性分析

    Exploring Progress in Multivariate Time Series Forecasting: Comprehensive Benchmarking and Heterogeneity Analysis. (arXiv:2310.06119v1 [cs.LG])

    [http://arxiv.org/abs/2310.06119](http://arxiv.org/abs/2310.06119)

    该论文旨在解决多元时间序列预测领域中公平基准测试和技术方法选择的争议，并提供对该领域取得的进展的深入洞察。

    

    多元时间序列（MTS）广泛存在于现实世界的复杂系统中，如交通和能源系统，对于理解和影响这些系统，它们的预测至关重要。最近，基于深度学习的方法在MTS中有效地建模时间和空间依赖关系方面获得了很大的流行，特别是在长期时间序列预测（LTSF）和时空预测（STF）中。然而，公平的基准测试问题和技术方法的选择在相关工作中一直存在争议。这些争议显著阻碍了我们对该领域进展的理解。因此，本文旨在解决这些争议，以提供对取得的进展的深入洞察。为了解决基准测试问题，我们引入了BasicTS，一个旨在公平比较MTS预测的基准。BasicTS建立了一个统一的训练流程和合理的评估设置，能够对30多种流行的MTS预测模型进行公正的评估。

    Multivariate Time Series (MTS) widely exists in real-word complex systems, such as traffic and energy systems, making their forecasting crucial for understanding and influencing these systems. Recently, deep learning-based approaches have gained much popularity for effectively modeling temporal and spatial dependencies in MTS, specifically in Long-term Time Series Forecasting (LTSF) and Spatial-Temporal Forecasting (STF). However, the fair benchmarking issue and the choice of technical approaches have been hotly debated in related work. Such controversies significantly hinder our understanding of progress in this field. Thus, this paper aims to address these controversies to present insights into advancements achieved. To resolve benchmarking issues, we introduce BasicTS, a benchmark designed for fair comparisons in MTS forecasting. BasicTS establishes a unified training pipeline and reasonable evaluation settings, enabling an unbiased evaluation of over 30 popular MTS forecasting mode
    
[^21]: D$^3$Fields: 动态三维描述符场用于零样本可泛化机器人操作

    D$^3$Fields: Dynamic 3D Descriptor Fields for Zero-Shot Generalizable Robotic Manipulation. (arXiv:2309.16118v1 [cs.RO])

    [http://arxiv.org/abs/2309.16118](http://arxiv.org/abs/2309.16118)

    D$^3$Fields是一个动态的三维描述符场，将底层三维环境的动态特性以及语义特征和实例掩模编码起来。它可以灵活地使用不同背景、风格和实例的二维图像指定目标，实现零样本机器人操作任务的可泛化。

    

    场景表示是机器人操作系统中一个关键的设计选择。一个理想的表示应该是三维的、动态的和语义化的，以满足不同操作任务的需求。然而，先前的工作往往同时缺乏这三个属性。在这项工作中，我们介绍了D$^3$Fields动态三维描述符场。这些场捕捉了底层三维环境的动态特性，编码了语义特征和实例掩模。具体而言，我们将工作区域中的任意三维点投影到多视角的二维视觉观察中，并插值从基本模型中得到的特征。由此得到的融合描述符场可以使用具有不同背景、风格和实例的二维图像灵活地指定目标。为了评估这些描述符场的有效性，我们以零样本方式将我们的表示应用于各种机器人操作任务。通过在真实场景和模拟中的广泛评估，我们展示了该方法的有效性。

    Scene representation has been a crucial design choice in robotic manipulation systems. An ideal representation should be 3D, dynamic, and semantic to meet the demands of diverse manipulation tasks. However, previous works often lack all three properties simultaneously. In this work, we introduce D$^3$Fields dynamic 3D descriptor fields. These fields capture the dynamics of the underlying 3D environment and encode both semantic features and instance masks. Specifically, we project arbitrary 3D points in the workspace onto multi-view 2D visual observations and interpolate features derived from foundational models. The resulting fused descriptor fields allow for flexible goal specifications using 2D images with varied contexts, styles, and instances. To evaluate the effectiveness of these descriptor fields, we apply our representation to a wide range of robotic manipulation tasks in a zero-shot manner. Through extensive evaluation in both real-world scenarios and simulations, we demonst
    
[^22]: 寻找平衡：用于异构差分隐私数据采集的逻辑回归的最佳机制设计

    Striking a Balance: An Optimal Mechanism Design for Heterogenous Differentially Private Data Acquisition for Logistic Regression. (arXiv:2309.10340v1 [cs.LG])

    [http://arxiv.org/abs/2309.10340](http://arxiv.org/abs/2309.10340)

    本论文研究了在从隐私敏感卖方收集的数据上执行逻辑回归的问题，设计了一个优化测试损失、卖方隐私和支付的加权组合的最佳机制，通过结合博弈论、统计学习理论和差分隐私的思想，解决了买方的目标函数非凸的问题，并提供了当卖方数量变大时的渐近结果。

    

    我们研究了在从隐私敏感卖方收集的数据上执行逻辑回归的问题。由于数据是私有的，卖方必须通过支付来激励他们提供数据。因此，目标是设计一个机制，优化测试损失、卖方隐私和支付的加权组合，即在多个感兴趣的目标之间寻找平衡。我们通过结合博弈论、统计学习理论和差分隐私的思想来解决这个问题。买方的目标函数可能非常非凸。然而，我们证明，在问题参数的某些条件下，可以通过变量的变换将问题凸化。我们还提供了当卖方数量变大时，买方的测试误差和支付的渐近结果。最后，我们通过将这些思想应用于一个真实的医疗数据集来展示我们的想法。

    We investigate the problem of performing logistic regression on data collected from privacy-sensitive sellers. Since the data is private, sellers must be incentivized through payments to provide their data. Thus, the goal is to design a mechanism that optimizes a weighted combination of test loss, seller privacy, and payment, i.e., strikes a balance between multiple objectives of interest. We solve the problem by combining ideas from game theory, statistical learning theory, and differential privacy. The buyer's objective function can be highly non-convex. However, we show that, under certain conditions on the problem parameters, the problem can be convexified by using a change of variables. We also provide asymptotic results characterizing the buyer's test error and payments when the number of sellers becomes large. Finally, we demonstrate our ideas by applying them to a real healthcare data set.
    
[^23]: HyperDreamBooth：用于快速个性化文本到图像模型的超网络

    HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models. (arXiv:2307.06949v1 [cs.CV])

    [http://arxiv.org/abs/2307.06949](http://arxiv.org/abs/2307.06949)

    HyperDreamBooth是一个超网络，可以从一个人的单张图片中快速生成个性化权重，从而实现在多种背景和风格下合成一个人的面部，保持高保真度并同时保留对多样化风格和语义修改的关键知识。

    

    个性化已经成为生成式人工智能领域中的一个重要方面，使得在不同背景和风格下合成个体成为可能，同时保持高保真度。然而，个性化过程在时间和内存需求方面存在困难。每个个性化模型的微调需要大量的GPU时间投入，为每个主题存储一个个性化模型会对存储容量提出要求。为了克服这些挑战，我们提出了HyperDreamBooth-一种能够从一个人的单张图片有效生成一组个性化权重的超网络。通过将这些权重组合到扩散模型中，并搭配快速微调，HyperDreamBooth能够以多种背景和风格生成一个人的面部，保持高主题细节同时也保持模型对多样化风格和语义修改的关键知识。我们的方法在大约50倍体现了面部个性化。

    Personalization has emerged as a prominent aspect within the field of generative AI, enabling the synthesis of individuals in diverse contexts and styles, while retaining high-fidelity to their identities. However, the process of personalization presents inherent challenges in terms of time and memory requirements. Fine-tuning each personalized model needs considerable GPU time investment, and storing a personalized model per subject can be demanding in terms of storage capacity. To overcome these challenges, we propose HyperDreamBooth-a hypernetwork capable of efficiently generating a small set of personalized weights from a single image of a person. By composing these weights into the diffusion model, coupled with fast finetuning, HyperDreamBooth can generate a person's face in various contexts and styles, with high subject details while also preserving the model's crucial knowledge of diverse styles and semantic modifications. Our method achieves personalization on faces in roughly 
    
[^24]: 具有收敛保证的公正感知联邦极小化优化

    Fairness-aware Federated Minimax Optimization with Convergence Guarantee. (arXiv:2307.04417v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.04417](http://arxiv.org/abs/2307.04417)

    本文提出了一种名为FFALM的算法，通过施加公平约束和解决极小化极大回归问题，在联邦学习中解决了群体公平性问题。实验证明FFALM在处理严重统计异质性问题时具有良好的效果。

    

    由于其保护隐私的特性，联邦学习 (FL) 吸引了相当多的关注。然而，管理用户数据的自由度不足可能导致群体公平性问题，即模型偏向于敏感因素诸如种族或性别。为了解决这个问题，本文提出了一种新颖的算法，名为带有增广拉格朗日方法的公平联邦平均法 (FFALM)，专门用于解决FL中的群体公平问题。具体来说，我们对训练目标施加了公平约束，并解决了受约束优化问题的极小化极大回归。然后，我们推导了FFALM的收敛速率的理论上界。通过在CelebA和UTKFace数据集中充分考虑严重统计异质性，实证结果表明了FFALM 在提高公平性方面的有效性。

    Federated learning (FL) has garnered considerable attention due to its privacy-preserving feature. Nonetheless, the lack of freedom in managing user data can lead to group fairness issues, where models are biased towards sensitive factors such as race or gender. To tackle this issue, this paper proposes a novel algorithm, fair federated averaging with augmented Lagrangian method (FFALM), designed explicitly to address group fairness issues in FL. Specifically, we impose a fairness constraint on the training objective and solve the minimax reformulation of the constrained optimization problem. Then, we derive the theoretical upper bound for the convergence rate of FFALM. The effectiveness of FFALM in improving fairness is shown empirically on CelebA and UTKFace datasets in the presence of severe statistical heterogeneity.
    
[^25]: 论神经网络对降解多边形的感知存在的基本问题

    Degraded Polygons Raise Fundamental Questions of Neural Network Perception. (arXiv:2306.04955v1 [cs.CV])

    [http://arxiv.org/abs/2306.04955](http://arxiv.org/abs/2306.04955)

    本文研究了神经网络在识别具有不同程度边缘降解的规则多边形时的性能和行为，发现存在基本问题，揭示了人机视觉差距的另一个角度。

    

    现代计算机视觉系统往往表现出与人类不一致的行为：从对抗攻击到图像损坏，深度学习视觉模型在各种环境中都表现不佳，然而人类却能够很好地解决这些问题。本文从另一个角度研究了人机视觉差距。我们重新审视了恢复受损图像的任务，该任务在人类视觉的“识别组件”理论中首次引入，研究了神经网络在分类具有不同程度边缘降解的规则多边形时的性能和行为。为此，我们使用了自动化形状可恢复性测试，快速生成了大规模数据集，将历史上手动创建图像可恢复性实验的方法进行了现代化改进。我们进一步研究了神经网络识别多边形的能力以及其相关问题。

    It is well-known that modern computer vision systems often exhibit behaviors misaligned with those of humans: from adversarial attacks to image corruptions, deep learning vision models suffer in a variety of settings that humans capably handle. In light of these phenomena, here we introduce another, orthogonal perspective studying the human-machine vision gap. We revisit the task of recovering images under degradation, first introduced over 30 years ago in the Recognition-by-Components theory of human vision. Specifically, we study the performance and behavior of neural networks on the seemingly simple task of classifying regular polygons at varying orders of degradation along their perimeters. To this end, we implement the Automated Shape Recoverability Test for rapidly generating large-scale datasets of perimeter-degraded regular polygons, modernizing the historically manual creation of image recoverability experiments. We then investigate the capacity of neural networks to recognize
    
[^26]: 过拟合的模型会泄露预训练数据的隐私信息

    TMI! Finetuned Models Leak Private Information from their Pretraining Data. (arXiv:2306.01181v1 [cs.LG])

    [http://arxiv.org/abs/2306.01181](http://arxiv.org/abs/2306.01181)

    本文提出了一种新的会员推断威胁模型TMI，用于评估微调模型对预训练数据的泄露，突显了在使用预训练模型进行迁移学习中存在的隐私风险，并需要对机器学习中的隐私进行更严格的评估。

    

    迁移学习已成为机器学习中越来越流行的技术，用于利用为一个任务训练的预训练模型来协助构建相关任务的微调模型。该范例在隐私机器学习方面尤其受欢迎，其中预训练模型被认为是公开的，只有微调数据被视为敏感的。然而，有理由认为用于预训练的数据仍然是敏感的，因此必须了解微调模型泄露有关预训练数据的信息量。本文提出了一种新的会员推理威胁模型，其中对手只能访问已经微调好的模型，并想推断预训练数据的成员资格。为了实现这个威胁模型，我们实施了一种新型的基于元分类器的攻击TMI，它利用了在下游任务中记忆的预训练样本对预测的影响。我们在视觉和自然语言处理任务上评估了TMI，并表明它在仅使用微调模型的情况下实现了高精度的推断预训练数据的成员资格。我们的结果突显了在迁移学习中使用预训练模型可能存在的隐私风险，以及需要对机器学习中的隐私进行更严格的评估的需求。

    Transfer learning has become an increasingly popular technique in machine learning as a way to leverage a pretrained model trained for one task to assist with building a finetuned model for a related task. This paradigm has been especially popular for privacy in machine learning, where the pretrained model is considered public, and only the data for finetuning is considered sensitive. However, there are reasons to believe that the data used for pretraining is still sensitive, making it essential to understand how much information the finetuned model leaks about the pretraining data. In this work we propose a new membership-inference threat model where the adversary only has access to the finetuned model and would like to infer the membership of the pretraining data. To realize this threat model, we implement a novel metaclassifier-based attack, TMI, that leverages the influence of memorized pretraining samples on predictions in the downstream task. We evaluate TMI on both vision and na
    
[^27]: 在线到PAC的转换: 通过遗憾分析得出泛化界限

    Online-to-PAC Conversions: Generalization Bounds via Regret Analysis. (arXiv:2305.19674v1 [stat.ML])

    [http://arxiv.org/abs/2305.19674](http://arxiv.org/abs/2305.19674)

    本文提出了在线学习游戏“泛化游戏”的框架，将在线学习算法的表现和统计学习算法的泛化界限联系了起来，并得出了一些标准的泛化限制。

    

    我们提出了一个新的框架，通过在线学习的视角推导出统计学习算法的泛化界限。具体而言，我们构建了一个在线学习游戏称为“泛化游戏”，其中在线学习器试图与固定的统计学习算法竞争，预测独立同分布数据点训练集上的泛化间隙序列。我们通过展示在这个游戏中存在有界遗憾的在线学习算法与统计学习设置之间的联系来建立这种关联，这意味着统计学习算法的泛化错误存在一个界限，直到与统计学习方法的复杂性无关的鞅浓度项。这种技术允许我们恢复几个标准的泛化限制，包括一系列的PAC-Bayesian保证和信息理论保证，以及它们的推广。

    We present a new framework for deriving bounds on the generalization bound of statistical learning algorithms from the perspective of online learning. Specifically, we construct an online learning game called the "generalization game", where an online learner is trying to compete with a fixed statistical learning algorithm in predicting the sequence of generalization gaps on a training set of i.i.d. data points. We establish a connection between the online and statistical learning setting by showing that the existence of an online learning algorithm with bounded regret in this game implies a bound on the generalization error of the statistical learning algorithm, up to a martingale concentration term that is independent of the complexity of the statistical learning method. This technique allows us to recover several standard generalization bounds including a range of PAC-Bayesian and information-theoretic guarantees, as well as generalizations thereof.
    
[^28]: 关于纯16位浮点神经网络的辩护

    In Defense of Pure 16-bit Floating-Point Neural Networks. (arXiv:2305.10947v1 [cs.LG])

    [http://arxiv.org/abs/2305.10947](http://arxiv.org/abs/2305.10947)

    本文探讨了纯16位浮点神经网络的被忽视的效率，提供了理论分析来探讨16位和32位模型的差异，并可以定量解释16位模型与其32位对应物之间的条件。

    

    减少编码神经网络权重和激活所需的位数是非常可取的，因为它可以加快神经网络的训练和推理时间，同时减少内存消耗。因此，这一领域的研究引起了广泛关注，以开发利用更低精度计算的神经网络，比如混合精度训练。有趣的是，目前不存在纯16位浮点设置的方法。本文揭示了纯16位浮点神经网络被忽视的效率。我们通过提供全面的理论分析来探讨造成16位和32位模型的差异的因素。我们规范化了浮点误差和容忍度的概念，从而可以定量解释16位模型与其32位对应物之间密切逼近结果的条件。这种理论探索提供了新的视角。

    Reducing the number of bits needed to encode the weights and activations of neural networks is highly desirable as it speeds up their training and inference time while reducing memory consumption. For these reasons, research in this area has attracted significant attention toward developing neural networks that leverage lower-precision computing, such as mixed-precision training. Interestingly, none of the existing approaches has investigated pure 16-bit floating-point settings. In this paper, we shed light on the overlooked efficiency of pure 16-bit floating-point neural networks. As such, we provide a comprehensive theoretical analysis to investigate the factors contributing to the differences observed between 16-bit and 32-bit models. We formalize the concepts of floating-point error and tolerance, enabling us to quantitatively explain the conditions under which a 16-bit model can closely approximate the results of its 32-bit counterpart. This theoretical exploration offers perspect
    
[^29]: 基于BERT模型的推文地理位置预测

    Geolocation Predicting of Tweets Using BERT-Based Models. (arXiv:2303.07865v1 [cs.CL])

    [http://arxiv.org/abs/2303.07865](http://arxiv.org/abs/2303.07865)

    该论文提出基于BERT模型的推文地理位置预测方法，可以实现全球和美国上的中位误差分别小于30公里和15公里的定位精度。

    

    该研究旨在解决推文/用户地理位置预测任务，并提供了处理文本大数据地理标记的灵活方法。该方法采用基于神经网络的自然语言处理来估计坐标对（经度，纬度）和二维高斯混合模型（GMM）。提出的模型的范围已经在Twitter数据集上使用预训练的BERT模型进行调整。性能指标表明，对于在推文内容和元数据上训练和评估的模型，全球范围内的中位误差小于30公里，美国范围内的中位误差小于15公里。

    This research is aimed to solve the tweet/user geolocation prediction task and provide a flexible methodology for the geotagging of textual big data. The suggested approach implements neural networks for natural language processing (NLP) to estimate the location as coordinate pairs (longitude, latitude) and two-dimensional Gaussian Mixture Models (GMMs). The scope of proposed models has been finetuned on a Twitter dataset using pretrained Bidirectional Encoder Representations from Transformers (BERT) as base models. Performance metrics show a median error of fewer than 30 km on a worldwide-level, and fewer than 15 km on the US-level datasets for the models trained and evaluated on text features of tweets' content and metadata context.
    
[^30]: 在交互环境中使用在线强化学习对大型语言模型进行基础设施建设

    Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning. (arXiv:2302.02662v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02662](http://arxiv.org/abs/2302.02662)

    本文研究了一种名为GLAM的方法，通过功能基础设施建设，利用在线强化学习提高LLM代理程序的性能来实现LLMs与环境之间的对齐，解决决策问题。

    

    最近的研究成功地利用了大型语言模型（LLM）捕捉世界物理的抽象知识，以解决决策问题。然而，LLMs的知识与环境之间的对齐可能是错误的，并且由于缺乏基础设施建设而限制了其功能能力。在本文中，我们研究了一种通过功能基础设施建设实现这种对齐的方法（称为GLAM）：我们考虑一个使用LLM作为策略的代理程序，随着代理程序与环境进行交互而逐步更新，并利用在线强化学习来提高其解决目标的性能。使用一个交互式的文本环境设计来研究更高级形式的基础设施建设，以及一组空间和导航任务，我们研究了几个科学问题：1）LLMs能否提高各种RL任务的在线学习的样本效率？2）它如何提高不同形式的泛化？3）在线学习的影响是什么？我们通过功能方式研究这些问题。

    Recent works successfully leveraged Large Language Models' (LLM) abilities to capture abstract knowledge about world's physics to solve decision-making problems. Yet, the alignment between LLMs' knowledge and the environment can be wrong and limit functional competence due to lack of grounding. In this paper, we study an approach (named GLAM) to achieve this alignment through functional grounding: we consider an agent using an LLM as a policy that is progressively updated as the agent interacts with the environment, leveraging online Reinforcement Learning to improve its performance to solve goals. Using an interactive textual environment designed to study higher-level forms of functional grounding, and a set of spatial and navigation tasks, we study several scientific questions: 1) Can LLMs boost sample efficiency for online learning of various RL tasks? 2) How can it boost different forms of generalization? 3) What is the impact of online learning? We study these questions by functio
    

