# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [ALERT-Transformer: Bridging Asynchronous and Synchronous Machine Learning for Real-Time Event-based Spatio-Temporal Data](https://rss.arxiv.org/abs/2402.01393) | ALERT-Transformer是一种将异步感知与同步处理相结合的新颖桥接方式，通过ALERT模块、灵活的数据读取和基于块的稀疏性优化，实现了对实时事件驱动时空数据的经典处理，其性能超过竞争对手并具有较低的延迟。 |
| [^2] | [To the Max: Reinventing Reward in Reinforcement Learning](https://rss.arxiv.org/abs/2402.01361) | 本研究通过引入最大奖励强化学习的方法，提出了一种替代传统奖励函数的学习方式，并在实验中证明了其在不同环境下的性能优势。 |
| [^3] | [Can we Constrain Concept Bottleneck Models to Learn Semantically Meaningful Input Features?](https://rss.arxiv.org/abs/2402.00912) | 本文研究了概念瓶颈模型如何从具有细粒度概念注释的数据集中学习概念，以实现模型输出的内在可解释性。 |
| [^4] | [Adaptive Bounding Box Uncertainties via Two-Step Conformal Prediction](https://arxiv.org/abs/2403.07263) | 通过两步形式预测方法，本文实现了自适应边界框不确定性的量化，保证了对象边界框不确定性区间的覆盖率，包括了错误分类的对象，同时确保边界框区间能够适应物体大小，实现更平衡的覆盖率。 |
| [^5] | [Bayesian Hierarchical Probabilistic Forecasting of Intraday Electricity Prices](https://arxiv.org/abs/2403.05441) | 该研究首次提出了为德国连续日内市场交易的电力价格进行贝叶斯预测，考虑了参数不确定性，并在2022年的电力价格验证中取得了统计显著的改进。 |
| [^6] | [Learning reduced-order Quadratic-Linear models in Process Engineering using Operator Inference](https://arxiv.org/abs/2402.17698) | 使用算子推断学习的降阶模型在过程工程中建模动态系统，为实现快速可靠的数字孪生架构迈出重要一步。 |
| [^7] | [Multi-Agent, Human-Agent and Beyond: A Survey on Cooperation in Social Dilemmas](https://arxiv.org/abs/2402.17270) | 调查了多智能体、人智能体和人工智能智能体在社会困境合作中的三个关键领域，讨论了合作的动机、策略、人类偏见，以及未来研究方向。 |
| [^8] | [SimPro: A Simple Probabilistic Framework Towards Realistic Long-Tailed Semi-Supervised Learning](https://arxiv.org/abs/2402.13505) | SimPro提出了一种高度适应的框架，不依赖于任何关于未标记数据分布的预定义假设，通过创新地改进期望最大化（EM）算法，明确分离条件和边缘类别分布的建模。 |
| [^9] | [Optimizing Adaptive Experiments: A Unified Approach to Regret Minimization and Best-Arm Identification](https://arxiv.org/abs/2402.10592) | 提出了一种统一模型，同时考虑了实验内部性能和实验后结果，在优化大规模人群中的表现方面提供了尖锐理论，揭示了新颖的见解 |
| [^10] | [Cartesian atomic cluster expansion for machine learning interatomic potentials](https://arxiv.org/abs/2402.07472) | 本论文提出了一种改进的机器学习原子间势模型，使用基于笛卡尔坐标的原子密度展开来替代传统的原子团簇展开方法，并结合低维嵌入和原子间消息传递。该模型在不同系统中表现出良好的准确性、稳定性和普适性。 |
| [^11] | [Scalable Kernel Logistic Regression with Nystr\"om Approximation: Theoretical Analysis and Application to Discrete Choice Modelling](https://arxiv.org/abs/2402.06763) | 本文介绍了使用Nystr\"om近似方法解决大规模数据集上核逻辑回归的可扩展性问题。研究提供了理论分析并验证了不同的地标选择方法的性能。 |
| [^12] | [Light and Optimal Schr\"odinger Bridge Matching](https://arxiv.org/abs/2402.03207) | 该论文提出了一种新的学习薛定谔桥匹配的方法，克服了现有方法中的限制，通过优化薛定谔桥的参数化来恢复输运计划。 |
| [^13] | [On the Exploitation of DCT-Traces in the Generative-AI Domain](https://arxiv.org/abs/2402.02209) | 本文分析了生成AI模型在生成深度伪造图像时在频域中的DCT系数的统计特征。通过研究发现了一种独特的“辨别指纹”，可以利用它来改善现有的深度伪造检测器。 |
| [^14] | [Infinite dSprites for Disentangled Continual Learning: Separating Memory Edits from Generalization](https://arxiv.org/abs/2312.16731) | 引入了Infinite dSprites工具，用于创建任意长度的连续分类和分解基准，可以全面控制生成因素，有望缩小机器学习系统与人类学习在动态开放环境中的差距 |
| [^15] | [A Simple and Scalable Representation for Graph Generation](https://arxiv.org/abs/2312.02230) | 提出了一种名为GEEL的新型、简单且可扩展的图表示，可以显著降低邻接矩阵大小和词汇量，同时通过节点位置编码实现自回归生成，并针对属性图设计了新的扩展方案。 |
| [^16] | [Information Leakage Detection through Approximate Bayes-optimal Prediction.](http://arxiv.org/abs/2401.14283) | 本论文通过建立一个理论框架，利用统计学习理论和信息论来准确量化和检测信息泄漏，通过近似贝叶斯预测的对数损失和准确性来准确估计互信息。 |
| [^17] | [SMOOTHIE: A Theory of Hyper-parameter Optimization for Software Analytics.](http://arxiv.org/abs/2401.09622) | SMOOTHIE是一种通过考虑损失函数的“光滑度”来引导超参数优化的新型方法，在软件分析中应用可以带来显著的性能改进。 |
| [^18] | [Dynamic Spiking Graph Neural Networks.](http://arxiv.org/abs/2401.05373) | 本文提出了一个名为"动态尖峰图神经网络"（DSGNN）的框架，它将尖峰神经网络（SNNs）与图神经网络（GNNs）结合起来，以解决动态图表示学习中的复杂性和内存开销问题。DSGNN通过动态调整尖峰神经元的状态和连接权重，在传播过程中保持图结构信息的完整性。 |
| [^19] | [Semi-supervised learning via DQN for log anomaly detection.](http://arxiv.org/abs/2401.03151) | 本文提出了一种半监督的日志异常检测方法，命名为DQNLog，通过结合深度强化学习中的DQN算法，利用少量有标记的数据和大规模无标记的数据集，有效解决了数据不平衡和标记数量有限的问题，并且通过与异常环境交互和主动探索无标记的数据集，学习已知的异常并发现未知的异常。 |
| [^20] | [Geometric Learning with Positively Decomposable Kernels.](http://arxiv.org/abs/2310.13821) | 本文提出了使用正可分解核的几何学习方法，该方法通过在RKKS中学习而不需要访问核的分解，为非欧几里德数据的核学习提供了一条路径，并为RKKS方法提供了理论基础。 |
| [^21] | [Fast Multipole Attention: A Divide-and-Conquer Attention Mechanism for Long Sequences.](http://arxiv.org/abs/2310.11960) | 提出了一种名为快速多极化注意力的新型注意力机制，它使用分治策略将注意力的时间和内存复杂度从O(n^2)降低到O(n log n)或O(n)，同时保持了全局感知范围。 |
| [^22] | [Federated Generalization via Information-Theoretic Distribution Diversification.](http://arxiv.org/abs/2310.07171) | 该论文研究了联邦学习中泛化能力的挑战，特别关注训练分布和测试分布的不匹配。提出了一种信息论的泛化方法来解决这个问题。 |
| [^23] | [Noise-Tolerant Unsupervised Adapter for Vision-Language Models.](http://arxiv.org/abs/2309.14928) | 这篇论文介绍了一种噪声容忍的无监督适配器(NtUA)，它可以使用少样本无标签目标样本来学习优秀的视觉语言模型。NtUA通过自适应缓存形成和伪标签修正来对抗伪标签噪声。 |
| [^24] | [Auto-Regressive Next-Token Predictors are Universal Learners.](http://arxiv.org/abs/2309.06979) | 自回归的下一个标记预测器可以有效地近似图灵机计算的任何函数，并且在文本生成和算术任务上展现出非平凡的性能。 |
| [^25] | [SMARLA: A Safety Monitoring Approach for Deep Reinforcement Learning Agents.](http://arxiv.org/abs/2308.02594) | 本文提出了一种基于机器学习的安全监测方法SMARLA，用于深度强化学习智能体。该方法设计为黑盒子，利用状态抽象减少状态空间，实现对智能体状态的安全违规预测。经验证，SMARLA具有准确的违规预测能力，并可在智能体执行的早期阶段进行预测。 |
| [^26] | [Versatile Audio-Visual Learning for Handling Single and Multi Modalities in Emotion Regression and Classification Tasks.](http://arxiv.org/abs/2305.07216) | 本文提出了一个通用视听学习（VAVL）框架，可用于处理情感回归和情感分类任务中的单模态和多模态系统，即使数据缺失或不匹配也能进行有效训练和切换。 |
| [^27] | [Sequential Knockoffs for Variable Selection in Reinforcement Learning.](http://arxiv.org/abs/2303.14281) | 本论文介绍了一种新颖的序列 Knockoffs (SEEK)算法，用于在强化学习系统中实现变量选择，该算法估计了最小充分状态，确保学习进程良好而不会减缓。 |
| [^28] | [Revolutionizing Genomics with Reinforcement Learning Techniques.](http://arxiv.org/abs/2302.13268) | 强化学习是一种革新的工具，可以在基因组学领域中解决自动数据分析和处理的问题。使用强化学习算法可以降低收集标记训练数据的成本，适用于基因组数据分析和解释。本调查重点关注在基因组研究领域中使用强化学习的应用，包括基因调控网络、基因组组装和序列比对。 |
| [^29] | [Attacking Cooperative Multi-Agent Reinforcement Learning by Adversarial Minority Influence.](http://arxiv.org/abs/2302.03322) | 本研究提出了 Adversarial Minority Influence (AMI) 黑盒攻击，能够在考虑多智能体互动和合作目标下实现有针对性的最坏情况合作，攻击成功率比现有方法高出 2.2倍。 |

# 详细

[^1]: ALERT-Transformer: 将异步和同步机器学习桥接在实时事件驱动的时空数据上

    ALERT-Transformer: Bridging Asynchronous and Synchronous Machine Learning for Real-Time Event-based Spatio-Temporal Data

    [https://rss.arxiv.org/abs/2402.01393](https://rss.arxiv.org/abs/2402.01393)

    ALERT-Transformer是一种将异步感知与同步处理相结合的新颖桥接方式，通过ALERT模块、灵活的数据读取和基于块的稀疏性优化，实现了对实时事件驱动时空数据的经典处理，其性能超过竞争对手并具有较低的延迟。

    

    我们旨在通过稠密机器学习模型，实现对由事件感应器产生的连续超稀疏时空数据的经典处理。我们提出了一种新颖的混合管道，由异步感知和同步处理组成，结合了几个思路：（1）基于PointNet模型的嵌入——ALERT模块，可以通过泄漏机制不断整合新事件并消除旧事件，（2）嵌入数据的灵活读取，可以以任何采样率将始终最新的特征输入到下游模型中，（3）借鉴Vision Transformer的基于块的方法来利用输入的稀疏性以优化方法的效率。这些嵌入然后由一个经过对象和手势识别训练的Transformer模型进行处理。使用这种方法，我们实现了比竞争对手更低的延迟，达到了最新技术水平的性能。我们还证明了我们的异步模型可以以任何所需的采样率进行操作。

    We seek to enable classic processing of continuous ultra-sparse spatiotemporal data generated by event-based sensors with dense machine learning models. We propose a novel hybrid pipeline composed of asynchronous sensing and synchronous processing that combines several ideas: (1) an embedding based on PointNet models -- the ALERT module -- that can continuously integrate new and dismiss old events thanks to a leakage mechanism, (2) a flexible readout of the embedded data that allows to feed any downstream model with always up-to-date features at any sampling rate, (3) exploiting the input sparsity in a patch-based approach inspired by Vision Transformer to optimize the efficiency of the method. These embeddings are then processed by a transformer model trained for object and gesture recognition. Using this approach, we achieve performances at the state-of-the-art with a lower latency than competitors. We also demonstrate that our asynchronous model can operate at any desired sampling r
    
[^2]: 尽善尽美：重新定义强化学习中的奖励

    To the Max: Reinventing Reward in Reinforcement Learning

    [https://rss.arxiv.org/abs/2402.01361](https://rss.arxiv.org/abs/2402.01361)

    本研究通过引入最大奖励强化学习的方法，提出了一种替代传统奖励函数的学习方式，并在实验中证明了其在不同环境下的性能优势。

    

    在强化学习中，不同的奖励可以定义相同的最优策略，但学习性能却会有很大差异。对于某些情况，智能体会陷入次优行为，而对于其他情况，则能高效地解决任务。选择一个好的奖励函数因此是一个非常重要但具有挑战性的问题。在本文中，我们探索了一种替代奖励用于学习的方法。我们引入了最大奖励强化学习（max-reward RL），其中智能体优化的是最大奖励而不是累积奖励。与早期的方法不同，我们的方法适用于确定性和随机环境，并且可以与最先进的强化学习算法轻松结合。在实验中，我们研究了max-reward RL算法在Gymnasium-Robotics中的两个目标达成环境中的性能，并展示其相对于标准RL的优势。代码公开可用。

    In reinforcement learning (RL), different rewards can define the same optimal policy but result in drastically different learning performance. For some, the agent gets stuck with a suboptimal behavior, and for others, it solves the task efficiently. Choosing a good reward function is hence an extremely important yet challenging problem. In this paper, we explore an alternative approach to using rewards for learning. We introduce max-reward RL, where an agent optimizes the maximum rather than the cumulative reward. Unlike earlier works, our approach works for deterministic and stochastic environments and can be easily combined with state-of-the-art RL algorithms. In the experiments, we study the performance of max-reward RL algorithms in two goal-reaching environments from Gymnasium-Robotics and demonstrate its benefits over standard RL. The code is publicly available.
    
[^3]: 能够约束概念瓶颈模型学习语义上有意义的输入特征吗？

    Can we Constrain Concept Bottleneck Models to Learn Semantically Meaningful Input Features?

    [https://rss.arxiv.org/abs/2402.00912](https://rss.arxiv.org/abs/2402.00912)

    本文研究了概念瓶颈模型如何从具有细粒度概念注释的数据集中学习概念，以实现模型输出的内在可解释性。

    

    概念瓶颈模型（CBM）被认为具有内在的可解释性，因为它们首先预测一组人为定义的概念，然后利用这些概念来预测下游任务的输出。为了实现完全的内在可解释性，以及确保对模型输出的信任，我们需要保证概念的预测是基于语义映射的输入特征。例如，人们可能期望图像中表示骨折的像素被用于预测骨折。然而，当前的文献表明这并不是事实，因为概念预测通常与不相关的输入特征映射在一起。我们假设这是由于概念注释的不准确或者输入特征与概念之间的关系不清晰导致的。总的来说，数据集标注对CBMs中概念表示的影响仍然是一个研究较少的领域。因此，在本文中，我们研究了CBMs如何从具有细粒度概念注释的数据集中学习概念的问题。我们进行了实验演示。

    Concept Bottleneck Models (CBMs) are considered inherently interpretable because they first predict a set of human-defined concepts before using these concepts to predict the output of a downstream task. For inherent interpretability to be fully realised, and ensure trust in a model's output, we need to guarantee concepts are predicted based on semantically mapped input features. For example, one might expect the pixels representing a broken bone in an image to be used for the prediction of a fracture. However, current literature indicates this is not the case, as concept predictions are often mapped to irrelevant input features. We hypothesise that this occurs when concept annotations are inaccurate or how input features should relate to concepts is unclear. In general, the effect of dataset labelling on concept representations in CBMs remains an understudied area. Therefore, in this paper, we examine how CBMs learn concepts from datasets with fine-grained concept annotations. We demo
    
[^4]: 通过两步形式预测实现自适应边界框不确定性

    Adaptive Bounding Box Uncertainties via Two-Step Conformal Prediction

    [https://arxiv.org/abs/2403.07263](https://arxiv.org/abs/2403.07263)

    通过两步形式预测方法，本文实现了自适应边界框不确定性的量化，保证了对象边界框不确定性区间的覆盖率，包括了错误分类的对象，同时确保边界框区间能够适应物体大小，实现更平衡的覆盖率。

    

    量化模型的预测不确定性对于像自动驾驶这样的安全关键应用至关重要。我们考虑为多物体检测量化这种不确定性。具体来说，我们利用形式预测来获得具有保证覆盖率的物体边界框不确定性区间。这样做的一个挑战是边界框的预测取决于物体的类别标签。因此，我们开发了一种新颖的两步形式方法，将对预测类别标签的不确定性传播到边界框的不确定性区间中。这样，我们的形式覆盖保证的有效性更广泛，包括了被错误分类的物体，确保它们在需要最大安全保证时的实用性。此外，我们研究了新颖的集成和分位数回归形式，以确保边界框区间能够适应物体大小，从而实现更平衡的覆盖率。

    arXiv:2403.07263v1 Announce Type: cross  Abstract: Quantifying a model's predictive uncertainty is essential for safety-critical applications such as autonomous driving. We consider quantifying such uncertainty for multi-object detection. In particular, we leverage conformal prediction to obtain uncertainty intervals with guaranteed coverage for object bounding boxes. One challenge in doing so is that bounding box predictions are conditioned on the object's class label. Thus, we develop a novel two-step conformal approach that propagates uncertainty in predicted class labels into the uncertainty intervals for the bounding boxes. This broadens the validity of our conformal coverage guarantees to include incorrectly classified objects, ensuring their usefulness when maximal safety assurances are required. Moreover, we investigate novel ensemble and quantile regression formulations to ensure the bounding box intervals are adaptive to object size, leading to a more balanced coverage across
    
[^5]: 基于贝叶斯层次概率的日内电力价格预测

    Bayesian Hierarchical Probabilistic Forecasting of Intraday Electricity Prices

    [https://arxiv.org/abs/2403.05441](https://arxiv.org/abs/2403.05441)

    该研究首次提出了为德国连续日内市场交易的电力价格进行贝叶斯预测，考虑了参数不确定性，并在2022年的电力价格验证中取得了统计显著的改进。

    

    我们首次提出了对德国连续日内市场交易的电力价格进行贝叶斯预测的研究，充分考虑参数不确定性。我们的目标变量是IDFull价格指数，预测以后验预测分布的形式给出。我们使用了2022年极度波动的电力价格进行验证，在之前几乎没有成为预测研究对象。作为基准模型，我们使用了预测创建时的所有可用日内交易来计算IDFull的当前值。根据弱式有效假设，从最后价格信息建立的基准无法显著改善。然而，我们观察到在点度量和概率评分方面存在着统计显著的改进。最后，我们挑战了在电力价格预测中使用LASSO进行特征选择的宣布的黄金标准。

    arXiv:2403.05441v1 Announce Type: cross  Abstract: We present a first study of Bayesian forecasting of electricity prices traded on the German continuous intraday market which fully incorporates parameter uncertainty. Our target variable is the IDFull price index, forecasts are given in terms of posterior predictive distributions. For validation we use the exceedingly volatile electricity prices of 2022, which have hardly been the subject of forecasting studies before. As a benchmark model, we use all available intraday transactions at the time of forecast creation to compute a current value for the IDFull. According to the weak-form efficiency hypothesis, it would not be possible to significantly improve this benchmark built from last price information. We do, however, observe statistically significant improvement in terms of both point measures and probability scores. Finally, we challenge the declared gold standard of using LASSO for feature selection in electricity price forecastin
    
[^6]: 在过程工程中使用算子推断学习降阶二次-线性模型

    Learning reduced-order Quadratic-Linear models in Process Engineering using Operator Inference

    [https://arxiv.org/abs/2402.17698](https://arxiv.org/abs/2402.17698)

    使用算子推断学习的降阶模型在过程工程中建模动态系统，为实现快速可靠的数字孪生架构迈出重要一步。

    

    在这项工作中，我们解决了在过程工程中高效建模动态系统的挑战。我们使用降阶模型学习，具体来说是算子推断。这是一种非侵入式、数据驱动的从时域数据学习动态系统的方法。我们的研究应用是二氧化碳甲烷化反应，这是电力转化技术框架中的重要反应，以展示其潜力。数值结果表明，用算子推断构建的降阶模型能够提供一个简化但准确的替代解决方案。这标志着实现快速可靠数字孪生架构的重要里程碑。

    arXiv:2402.17698v1 Announce Type: cross  Abstract: In this work, we address the challenge of efficiently modeling dynamical systems in process engineering. We use reduced-order model learning, specifically operator inference. This is a non-intrusive, data-driven method for learning dynamical systems from time-domain data. The application in our study is carbon dioxide methanation, an important reaction within the Power-to-X framework, to demonstrate its potential. The numerical results show the ability of the reduced-order models constructed with operator inference to provide a reduced yet accurate surrogate solution. This represents an important milestone towards the implementation of fast and reliable digital twin architectures.
    
[^7]: 多智能体、人智能体及其进展：合作在社会困境中的调查

    Multi-Agent, Human-Agent and Beyond: A Survey on Cooperation in Social Dilemmas

    [https://arxiv.org/abs/2402.17270](https://arxiv.org/abs/2402.17270)

    调查了多智能体、人智能体和人工智能智能体在社会困境合作中的三个关键领域，讨论了合作的动机、策略、人类偏见，以及未来研究方向。

    

    在社会困境中研究合作长期以来一直是各种学科的基本课题，包括计算机科学和社会科学。人工智能领域的最新进展显著重塑了这一领域，为理解和增强合作提供了新的见解。本调查考察了人工智能和社会困境合作交汇处的三个关键领域。首先，着重于多智能体合作，我们审查了支持理性智能体之间合作的内在和外在动机，以及用于制定有效策略对抗不同对手的方法。其次，探讨了人智能体合作，我们讨论了当前用于与人类合作的人工智能算法，以及人类对人工智能智能体的偏见。第三，我们审查了利用人工智能智能体增强人类合作的新兴领域。最后，我们讨论了未来研究方向，例如 u

    arXiv:2402.17270v1 Announce Type: new  Abstract: The study of cooperation within social dilemmas has long been a fundamental topic across various disciplines, including computer science and social science. Recent advancements in Artificial Intelligence (AI) have significantly reshaped this field, offering fresh insights into understanding and enhancing cooperation. This survey examines three key areas at the intersection of AI and cooperation in social dilemmas. First, focusing on multi-agent cooperation, we review the intrinsic and external motivations that support cooperation among rational agents, and the methods employed to develop effective strategies against diverse opponents. Second, looking into human-agent cooperation, we discuss the current AI algorithms for cooperating with humans and the human biases towards AI agents. Third, we review the emergent field of leveraging AI agents to enhance cooperation among humans. We conclude by discussing future research avenues, such as u
    
[^8]: SimPro：一个简单的概率框架实现逼真的长尾半监督学习

    SimPro: A Simple Probabilistic Framework Towards Realistic Long-Tailed Semi-Supervised Learning

    [https://arxiv.org/abs/2402.13505](https://arxiv.org/abs/2402.13505)

    SimPro提出了一种高度适应的框架，不依赖于任何关于未标记数据分布的预定义假设，通过创新地改进期望最大化（EM）算法，明确分离条件和边缘类别分布的建模。

    

    近年来半监督学习的最新进展集中在解决一个更为逼真但具有挑战性的任务：解决标记数据的不平衡问题，同时未标记数据的类别分布既未知又可能不匹配。当前这一领域的方法往往预设了关于未标记数据类别分布的严格假设，从而限制了模型仅适应于某些分布范围。在本研究中，我们提出了一种新颖的方法，引入了一个高度适应性的框架，命名为SimPro，它不依赖于任何关于未标记数据分布的预定义假设。我们的框架建立在一个概率模型上，通过明确分离条件和边缘类别分布的建模，创新地改进了期望最大化（EM）算法。这种分离促进了在最大化过程中对类别分布进行估计的闭合形式解决方案。

    arXiv:2402.13505v1 Announce Type: new  Abstract: Recent advancements in semi-supervised learning have focused on a more realistic yet challenging task: addressing imbalances in labeled data while the class distribution of unlabeled data remains both unknown and potentially mismatched. Current approaches in this sphere often presuppose rigid assumptions regarding the class distribution of unlabeled data, thereby limiting the adaptability of models to only certain distribution ranges. In this study, we propose a novel approach, introducing a highly adaptable framework, designated as SimPro, which does not rely on any predefined assumptions about the distribution of unlabeled data. Our framework, grounded in a probabilistic model, innovatively refines the expectation-maximization (EM) algorithm by explicitly decoupling the modeling of conditional and marginal class distributions. This separation facilitates a closed-form solution for class distribution estimation during the maximization p
    
[^9]: 优化自适应实验：最小化后悔和最佳臂识别的统一方法

    Optimizing Adaptive Experiments: A Unified Approach to Regret Minimization and Best-Arm Identification

    [https://arxiv.org/abs/2402.10592](https://arxiv.org/abs/2402.10592)

    提出了一种统一模型，同时考虑了实验内部性能和实验后结果，在优化大规模人群中的表现方面提供了尖锐理论，揭示了新颖的见解

    

    进行自适应实验的从业者通常面临两个竞争性优先级：通过在实验过程中有效地分配治疗来降低实验成本，以及迅速收集信息以结束实验并在整个人群中实施治疗。当前，文献意见分歧，有关最小化后悔的研究独立地处理前者的优先级，而有关最佳臂识别的研究则专注于后者。本文提出了一种统一模型，考虑到实验内部性能和实验后结果。我们随后提供了一个针对大规模人群的最佳性能的尖锐理论，将文献中的经典结果统一起来。这种统一还揭示了新的见解。例如，理论揭示了类似最近提出的顶部两个Thompson抽样算法等熟悉算法可被调整以优化广泛类别的目标。

    arXiv:2402.10592v1 Announce Type: new  Abstract: Practitioners conducting adaptive experiments often encounter two competing priorities: reducing the cost of experimentation by effectively assigning treatments during the experiment itself, and gathering information swiftly to conclude the experiment and implement a treatment across the population. Currently, the literature is divided, with studies on regret minimization addressing the former priority in isolation, and research on best-arm identification focusing solely on the latter. This paper proposes a unified model that accounts for both within-experiment performance and post-experiment outcomes. We then provide a sharp theory of optimal performance in large populations that unifies canonical results in the literature. This unification also uncovers novel insights. For example, the theory reveals that familiar algorithms, like the recently proposed top-two Thompson sampling algorithm, can be adapted to optimize a broad class of obj
    
[^10]: 机器学习原子团簇展开在物质科学和化学中的应用

    Cartesian atomic cluster expansion for machine learning interatomic potentials

    [https://arxiv.org/abs/2402.07472](https://arxiv.org/abs/2402.07472)

    本论文提出了一种改进的机器学习原子间势模型，使用基于笛卡尔坐标的原子密度展开来替代传统的原子团簇展开方法，并结合低维嵌入和原子间消息传递。该模型在不同系统中表现出良好的准确性、稳定性和普适性。

    

    机器学习原子间势正在革新材料科学和化学中的大规模、准确的原子模拟。这些势函数通常使用原子团簇展开或变换消息传递与球谐基函数。然而，为了保持旋转对称性，依赖Clebsch-Gordan系数会导致计算效率低下和冗余。我们提出一种替代方法：基于笛卡尔坐标的原子密度展开。该方法在保持相互作用体系的同时提供了对原子环境的完整描述。此外，我们还整合了各种化学元素的低维嵌入和原子间消息传递。所得到的势函数被命名为Cartesian Atomic Cluster Expansion(CACE)，具有良好的准确性、稳定性和普适性。我们在不同系统中进行验证，包括大规模水、小分子和25种元素高熵合金。

    Machine learning interatomic potentials are revolutionizing large-scale, accurate atomistic modelling in material science and chemistry. These potentials often use atomic cluster expansion or equivariant message passing with spherical harmonics as basis functions. However, the dependence on Clebsch-Gordan coefficients for maintaining rotational symmetry leads to computational inefficiencies and redundancies. We propose an alternative: a Cartesian-coordinates-based atomic density expansion. This approach provides a complete description of atomic environments while maintaining interaction body orders. Additionally, we integrate low-dimensional embeddings of various chemical elements and inter-atomic message passing. The resulting potential, named Cartesian Atomic Cluster Expansion (CACE), exhibits good accuracy, stability, and generalizability. We validate its performance in diverse systems, including bulk water, small molecules, and 25-element high-entropy alloys.
    
[^11]: 使用Nystr\"om近似的可扩展核逻辑回归：理论分析和离散选择建模应用

    Scalable Kernel Logistic Regression with Nystr\"om Approximation: Theoretical Analysis and Application to Discrete Choice Modelling

    [https://arxiv.org/abs/2402.06763](https://arxiv.org/abs/2402.06763)

    本文介绍了使用Nystr\"om近似方法解决大规模数据集上核逻辑回归的可扩展性问题。研究提供了理论分析并验证了不同的地标选择方法的性能。

    

    将基于核的机器学习技术应用于使用大规模数据集的离散选择建模时，经常面临存储需求和模型中涉及的大量参数的挑战。这种复杂性影响了大规模模型的高效训练。本文通过引入Nystr\"om近似方法解决了可扩展性问题，用于大规模数据集上的核逻辑回归。研究首先进行了理论分析，其中：i) 对KLR解的集合进行了描述，ii) 给出了使用Nystr\"om近似的KLR解的上界，并最后描述了专门用于Nystr\"om KLR的优化算法的特化。之后，对Nystr\"om KLR进行了计算验证。测试了四种地标选择方法，包括基本均匀采样、k-means采样策略和基于杠杆得分的两种非均匀方法。这些策略的性能进行了评估。

    The application of kernel-based Machine Learning (ML) techniques to discrete choice modelling using large datasets often faces challenges due to memory requirements and the considerable number of parameters involved in these models. This complexity hampers the efficient training of large-scale models. This paper addresses these problems of scalability by introducing the Nystr\"om approximation for Kernel Logistic Regression (KLR) on large datasets. The study begins by presenting a theoretical analysis in which: i) the set of KLR solutions is characterised, ii) an upper bound to the solution of KLR with Nystr\"om approximation is provided, and finally iii) a specialisation of the optimisation algorithms to Nystr\"om KLR is described. After this, the Nystr\"om KLR is computationally validated. Four landmark selection methods are tested, including basic uniform sampling, a k-means sampling strategy, and two non-uniform methods grounded in leverage scores. The performance of these strategi
    
[^12]: 光学与优化的薛定谔桥匹配

    Light and Optimal Schr\"odinger Bridge Matching

    [https://arxiv.org/abs/2402.03207](https://arxiv.org/abs/2402.03207)

    该论文提出了一种新的学习薛定谔桥匹配的方法，克服了现有方法中的限制，通过优化薛定谔桥的参数化来恢复输运计划。

    

    薛定谔桥匹配(SB)近年来引起了机器学习界的关注，作为经典扩散模型的一个有希望的扩展，也与熵最优输运(EOT)相互关联。最近的SB求解器利用了普遍的桥匹配程序。这些程序旨在在只有两个分布之间的输运计划的情况下，恢复一种随机过程来输运质量。特别地，给定EOT计划，这些程序可以被适应用于解决SB。这个事实被最近的研究工作广泛利用，形成了基于匹配的SB求解器。关键就是恢复EOT计划：最近的工作要么使用启发式的近似方法(如小批量输运)或者建立迭代匹配程序，这样设计上在训练过程中累积了误差。我们解决了这些限制，并提出了一种新的学习SB的方法，我们称之为\textbf{优化的薛定谔桥匹配}。它利用了薛定谔桥的最优参数化。

    Schr\"odinger Bridges (SB) have recently gained the attention of the ML community as a promising extension of classic diffusion models which is also interconnected to the Entropic Optimal Transport (EOT). Recent solvers for SB exploit the pervasive bridge matching procedures. Such procedures aim to recover a stochastic process transporting the mass between distributions given only a transport plan between them. In particular, given the EOT plan, these procedures can be adapted to solve SB. This fact is heavily exploited by recent works giving rives to matching-based SB solvers. The cornerstone here is recovering the EOT plan: recent works either use heuristical approximations (e.g., the minibatch OT) or establish iterative matching procedures which by the design accumulate the error during the training. We address these limitations and propose a novel procedure to learn SB which we call the \textbf{optimal Schr\"odinger bridge matching}. It exploits the optimal parameterization of the 
    
[^13]: 关于在生成AI领域利用DCT轨迹的探索

    On the Exploitation of DCT-Traces in the Generative-AI Domain

    [https://arxiv.org/abs/2402.02209](https://arxiv.org/abs/2402.02209)

    本文分析了生成AI模型在生成深度伪造图像时在频域中的DCT系数的统计特征。通过研究发现了一种独特的“辨别指纹”，可以利用它来改善现有的深度伪造检测器。

    

    深度伪造对于网络安全和数字取证领域来说是一个巨大的挑战，特别是考虑到最近基于生成AI的解决方案所获得的高质量结果。几乎所有生成模型在合成数据中留下了独特的痕迹，如果对其进行详细分析和识别，可以利用这些痕迹来改善现有深度伪造检测器的泛化限制。本文分析了由GAN和扩散模型引擎生成的深度伪造图像在频域中的特征，详细研究了离散余弦变换(DCT)系数的统计分布。我们认识到并非所有系数对图像检测的贡献相同，我们假设存在一种独特的“辨别指纹”，嵌入在特定系数组合中。为了识别它们，我们对各种系数组合进行了机器学习分类器的训练。此外，我们还使用了可解释AI(XAI)的LIME算法来搜索...

    Deepfakes represent one of the toughest challenges in the world of Cybersecurity and Digital Forensics, especially considering the high-quality results obtained with recent generative AI-based solutions. Almost all generative models leave unique traces in synthetic data that, if analyzed and identified in detail, can be exploited to improve the generalization limitations of existing deepfake detectors. In this paper we analyzed deepfake images in the frequency domain generated by both GAN and Diffusion Model engines, examining in detail the underlying statistical distribution of Discrete Cosine Transform (DCT) coefficients. Recognizing that not all coefficients contribute equally to image detection, we hypothesize the existence of a unique "discriminative fingerprint", embedded in specific combinations of coefficients. To identify them, Machine Learning classifiers were trained on various combinations of coefficients. In addition, the Explainable AI (XAI) LIME algorithm was used to sea
    
[^14]: 用于分解连续学习的无限dSprites：将记忆编辑与泛化分离

    Infinite dSprites for Disentangled Continual Learning: Separating Memory Edits from Generalization

    [https://arxiv.org/abs/2312.16731](https://arxiv.org/abs/2312.16731)

    引入了Infinite dSprites工具，用于创建任意长度的连续分类和分解基准，可以全面控制生成因素，有望缩小机器学习系统与人类学习在动态开放环境中的差距

    

    机器学习系统持续学习的能力受到灾难性遗忘的阻碍，即神经网络在学习新任务时会覆盖现有知识。持续学习方法通过正则化、参数隔离或排练来缓解这一问题，但它们通常在仅包含少数任务的基准测试上进行评估。为了取得进展以缩小这一差距，我们介绍了Infinite dSprites，这是一个简洁的工具，可创建任意长度的连续分类和分解基准，并对生成因素拥有完全控制。我们展示，在足够长的时间范围内，所有主要类型的持续学习方法的性能都表现出...

    arXiv:2312.16731v2 Announce Type: replace  Abstract: The ability of machine learning systems to learn continually is hindered by catastrophic forgetting, the tendency of neural networks to overwrite existing knowledge when learning a new task. Continual learning methods alleviate this problem through regularization, parameter isolation, or rehearsal, but they are typically evaluated on benchmarks comprising only a handful of tasks. In contrast, humans are able to learn continually in dynamic, open-world environments, effortlessly achieving one-shot memorization of unfamiliar objects and reliably recognizing them under various transformations. To make progress towards closing this gap, we introduce Infinite dSprites, a parsimonious tool for creating continual classification and disentanglement benchmarks of arbitrary length and with full control over generative factors. We show that over a sufficiently long time horizon, the performance of all major types of continual learning methods d
    
[^15]: 一种简单且可扩展的图生成表示

    A Simple and Scalable Representation for Graph Generation

    [https://arxiv.org/abs/2312.02230](https://arxiv.org/abs/2312.02230)

    提出了一种名为GEEL的新型、简单且可扩展的图表示，可以显著降低邻接矩阵大小和词汇量，同时通过节点位置编码实现自回归生成，并针对属性图设计了新的扩展方案。

    

    最近，人们对利用神经网络进行图生成产生了浓厚兴趣，这是一个具有关键应用价值的基本统计学习问题，如分子设计和社区分析。然而，大多数方法在生成大规模图时遇到了重大限制。这是由于它们需要输出随着节点数量呈二次增长的完整邻接矩阵。为了应对这一挑战，我们引入了一种新的、简单且可扩展的图表示，名为间隙编码边列表（GEEL），其表示大小较小且与边数量一致。此外，GEEL通过结合间隙编码和带宽限制方案显著减少了词汇量。通过加入节点位置编码，GEEL可以自回归生成，我们进一步将GEEL扩展到处理属性图，设计了一种新的语法。

    arXiv:2312.02230v2 Announce Type: replace-cross  Abstract: Recently, there has been a surge of interest in employing neural networks for graph generation, a fundamental statistical learning problem with critical applications like molecule design and community analysis. However, most approaches encounter significant limitations when generating large-scale graphs. This is due to their requirement to output the full adjacency matrices whose size grows quadratically with the number of nodes. In response to this challenge, we introduce a new, simple, and scalable graph representation named gap encoded edge list (GEEL) that has a small representation size that aligns with the number of edges. In addition, GEEL significantly reduces the vocabulary size by incorporating the gap encoding and bandwidth restriction schemes. GEEL can be autoregressively generated with the incorporation of node positional encoding, and we further extend GEEL to deal with attributed graphs by designing a new grammar
    
[^16]: 通过近似贝叶斯最优预测检测信息泄漏

    Information Leakage Detection through Approximate Bayes-optimal Prediction. (arXiv:2401.14283v1 [stat.ML])

    [http://arxiv.org/abs/2401.14283](http://arxiv.org/abs/2401.14283)

    本论文通过建立一个理论框架，利用统计学习理论和信息论来准确量化和检测信息泄漏，通过近似贝叶斯预测的对数损失和准确性来准确估计互信息。

    

    在今天的以数据驱动的世界中，公开可获得的信息的增加加剧了信息泄漏（IL）的挑战，引发了安全问题。IL涉及通过系统的可观察信息无意地将秘密（敏感）信息暴露给未经授权的方，传统的统计方法通过估计可观察信息和秘密信息之间的互信息（MI）来检测IL，面临维度灾难、收敛、计算复杂度和MI估计错误等挑战。此外，虽然新兴的监督机器学习（ML）方法在二进制系统敏感信息的检测上有效，但缺乏一个全面的理论框架。为了解决这些限制，我们使用统计学习理论和信息论建立了一个理论框架来准确量化和检测IL。我们证明了可以通过近似贝叶斯预测的对数损失和准确性来准确估计MI。

    In today's data-driven world, the proliferation of publicly available information intensifies the challenge of information leakage (IL), raising security concerns. IL involves unintentionally exposing secret (sensitive) information to unauthorized parties via systems' observable information. Conventional statistical approaches, which estimate mutual information (MI) between observable and secret information for detecting IL, face challenges such as the curse of dimensionality, convergence, computational complexity, and MI misestimation. Furthermore, emerging supervised machine learning (ML) methods, though effective, are limited to binary system-sensitive information and lack a comprehensive theoretical framework. To address these limitations, we establish a theoretical framework using statistical learning theory and information theory to accurately quantify and detect IL. We demonstrate that MI can be accurately estimated by approximating the log-loss and accuracy of the Bayes predict
    
[^17]: SMOOTHIE: 软件分析的超参数优化理论

    SMOOTHIE: A Theory of Hyper-parameter Optimization for Software Analytics. (arXiv:2401.09622v1 [cs.SE])

    [http://arxiv.org/abs/2401.09622](http://arxiv.org/abs/2401.09622)

    SMOOTHIE是一种通过考虑损失函数的“光滑度”来引导超参数优化的新型方法，在软件分析中应用可以带来显著的性能改进。

    

    超参数优化是调整学习器控制参数的黑魔法。在软件分析中，经常发现调优可以带来显著的性能改进。尽管如此，超参数优化在软件分析中通常被很少或很差地应用，可能是因为探索所有参数选项的CPU成本太高。我们假设当损失函数的“光滑度”更好时，学习器的泛化能力更强。这个理论非常有用，因为可以很快测试不同超参数选择对“光滑度”的影响（例如，对于深度学习器，在一个epoch之后就可以进行测试）。为了测试这个理论，本文实现和测试了SMOOTHIE，一种通过考虑“光滑度”来引导优化的新型超参数优化器。本文的实验将SMOOTHIE应用于多个软件工程任务，包括（a）GitHub问题寿命预测；（b）静态代码警告中错误警报的检测；（c）缺陷预测。

    Hyper-parameter optimization is the black art of tuning a learner's control parameters. In software analytics, a repeated result is that such tuning can result in dramatic performance improvements. Despite this, hyper-parameter optimization is often applied rarely or poorly in software analytics--perhaps due to the CPU cost of exploring all those parameter options can be prohibitive.  We theorize that learners generalize better when the loss landscape is ``smooth''. This theory is useful since the influence on ``smoothness'' of different hyper-parameter choices can be tested very quickly (e.g. for a deep learner, after just one epoch).  To test this theory, this paper implements and tests SMOOTHIE, a novel hyper-parameter optimizer that guides its optimizations via considerations of ``smothness''. The experiments of this paper test SMOOTHIE on numerous SE tasks including (a) GitHub issue lifetime prediction; (b) detecting false alarms in static code warnings; (c) defect prediction, and
    
[^18]: 动态尖峰图神经网络

    Dynamic Spiking Graph Neural Networks. (arXiv:2401.05373v1 [cs.NE])

    [http://arxiv.org/abs/2401.05373](http://arxiv.org/abs/2401.05373)

    本文提出了一个名为"动态尖峰图神经网络"（DSGNN）的框架，它将尖峰神经网络（SNNs）与图神经网络（GNNs）结合起来，以解决动态图表示学习中的复杂性和内存开销问题。DSGNN通过动态调整尖峰神经元的状态和连接权重，在传播过程中保持图结构信息的完整性。

    

    将尖峰神经网络（SNNs）和图神经网络（GNNs）相结合渐渐引起了人们的关注，这是因为它在处理由图表示的非欧几里得数据时具有低功耗和高效率。然而，作为一个常见的问题，动态图表示学习面临着高复杂性和大内存开销的挑战。目前的工作通常通过使用二进制特征而不是连续特征的SNNs来替代循环神经网络（RNNs）进行高效训练，这会忽视图结构信息并在传播过程中导致细节的丢失。此外，优化动态尖峰模型通常需要在时间步之间传播信息，这增加了内存需求。为了解决这些挑战，我们提出了一个名为"动态尖峰图神经网络"（\method{}）的框架。为了减轻信息丢失问题，\method{} 在传播过程中引入了一种新的机制，它在每个时间步骤中动态地调整尖峰神经元的状态和连接权重，以保持图结构信息的完整性。

    The integration of Spiking Neural Networks (SNNs) and Graph Neural Networks (GNNs) is gradually attracting attention due to the low power consumption and high efficiency in processing the non-Euclidean data represented by graphs. However, as a common problem, dynamic graph representation learning faces challenges such as high complexity and large memory overheads. Current work often uses SNNs instead of Recurrent Neural Networks (RNNs) by using binary features instead of continuous ones for efficient training, which would overlooks graph structure information and leads to the loss of details during propagation. Additionally, optimizing dynamic spiking models typically requires propagation of information across time steps, which increases memory requirements. To address these challenges, we present a framework named \underline{Dy}namic \underline{S}p\underline{i}king \underline{G}raph \underline{N}eural Networks (\method{}). To mitigate the information loss problem, \method{} propagates
    
[^19]: 通过DQN进行半监督学习用于日志异常检测

    Semi-supervised learning via DQN for log anomaly detection. (arXiv:2401.03151v1 [cs.SE])

    [http://arxiv.org/abs/2401.03151](http://arxiv.org/abs/2401.03151)

    本文提出了一种半监督的日志异常检测方法，命名为DQNLog，通过结合深度强化学习中的DQN算法，利用少量有标记的数据和大规模无标记的数据集，有效解决了数据不平衡和标记数量有限的问题，并且通过与异常环境交互和主动探索无标记的数据集，学习已知的异常并发现未知的异常。

    

    日志异常检测在保障现代软件系统的安全性和维护方面起着关键作用。目前，检测日志数据中的异常的主要方法是通过监督式异常检测。然而，现有的监督式方法往往依赖于有标记的数据，在实际情况下往往受到限制。本文提出了一种半监督的日志异常检测方法，结合深度强化学习中的DQN算法，称为DQNLog。DQNLog利用少量有标记的数据和大规模无标记的数据集，有效解决了数据不平衡和标记数量有限的问题。该方法不仅通过与偏向于异常的环境进行交互来学习已知的异常，还通过主动探索无标记的数据集来发现未知的异常。此外，DQNLog还引入了交叉熵损失项，防止在深度强化学习中出现模型过高估计的情况。

    Log anomaly detection plays a critical role in ensuring the security and maintenance of modern software systems. At present, the primary approach for detecting anomalies in log data is through supervised anomaly detection. Nonetheless, existing supervised methods heavily rely on labeled data, which can be frequently limited in real-world scenarios. In this paper, we propose a semi-supervised log anomaly detection method that combines the DQN algorithm from deep reinforcement learning, which is called DQNLog. DQNLog leverages a small amount of labeled data and a large-scale unlabeled dataset, effectively addressing the challenges of imbalanced data and limited labeling. This approach not only learns known anomalies by interacting with an environment biased towards anomalies but also discovers unknown anomalies by actively exploring the unlabeled dataset. Additionally, DQNLog incorporates a cross-entropy loss term to prevent model overestimation during Deep Reinforcement Learning (DRL). 
    
[^20]: 使用正可分解核的几何学习

    Geometric Learning with Positively Decomposable Kernels. (arXiv:2310.13821v1 [cs.LG])

    [http://arxiv.org/abs/2310.13821](http://arxiv.org/abs/2310.13821)

    本文提出了使用正可分解核的几何学习方法，该方法通过在RKKS中学习而不需要访问核的分解，为非欧几里德数据的核学习提供了一条路径，并为RKKS方法提供了理论基础。

    

    核方法是机器学习中强大的工具。经典的核方法基于正定核，将数据空间映射到重现核希尔伯特空间(RKHS)。对于非欧几里德数据空间，很难找到正定核。在这种情况下，我们提出使用基于重现核控制空间(RKKS)的方法，这些方法只需要具有正分解的核。我们证明了在RKKS中学习时，并不需要访问这个分解。然后我们研究了使核正可分解的条件。我们证明在可处理的正则性假设下，不变核在齐次空间上允许正分解。这使得它们比正定核更容易构造，为非欧几里德数据的核学习提供了一条路径。同样，这为RKKS方法提供了一般的理论基础。

    Kernel methods are powerful tools in machine learning. Classical kernel methods are based on positive-definite kernels, which map data spaces into reproducing kernel Hilbert spaces (RKHS). For non-Euclidean data spaces, positive-definite kernels are difficult to come by. In this case, we propose the use of reproducing kernel Krein space (RKKS) based methods, which require only kernels that admit a positive decomposition. We show that one does not need to access this decomposition in order to learn in RKKS. We then investigate the conditions under which a kernel is positively decomposable. We show that invariant kernels admit a positive decomposition on homogeneous spaces under tractable regularity assumptions. This makes them much easier to construct than positive-definite kernels, providing a route for learning with kernels for non-Euclidean data. By the same token, this provides theoretical foundations for RKKS-based methods in general.
    
[^21]: 快速多极化注意力：一种用于长序列的分治注意力机制

    Fast Multipole Attention: A Divide-and-Conquer Attention Mechanism for Long Sequences. (arXiv:2310.11960v1 [cs.CL])

    [http://arxiv.org/abs/2310.11960](http://arxiv.org/abs/2310.11960)

    提出了一种名为快速多极化注意力的新型注意力机制，它使用分治策略将注意力的时间和内存复杂度从O(n^2)降低到O(n log n)或O(n)，同时保持了全局感知范围。

    

    基于Transformer的模型已在许多领域取得了最先进的性能。然而，自注意力对于输入长度的二次复杂度限制了Transformer模型在长序列上的适用性。为了解决这个问题，我们提出了快速多极化注意力，一种使用分治策略来减少注意力时间和内存复杂度的新型注意力机制，将长度为n的序列的注意力复杂度从O(n^2)降低到O(n log n)或O(n)，同时保持了全局感知范围。这种分层方法将查询、键和值分为O(log n)级的分辨率，较远距离的组群越来越大，并学习计算组群数量的权重。因此，以高效分层的方式在较低的分辨率中考虑远离彼此的标记之间的相互作用。快速多极化注意力的总体复杂度为O(n)或O(n log n)。

    Transformer-based models have achieved state-of-the-art performance in many areas. However, the quadratic complexity of self-attention with respect to the input length hinders the applicability of Transformer-based models to long sequences. To address this, we present Fast Multipole Attention, a new attention mechanism that uses a divide-and-conquer strategy to reduce the time and memory complexity of attention for sequences of length $n$ from $\mathcal{O}(n^2)$ to $\mathcal{O}(n \log n)$ or $O(n)$, while retaining a global receptive field. The hierarchical approach groups queries, keys, and values into $\mathcal{O}( \log n)$ levels of resolution, where groups at greater distances are increasingly larger in size and the weights to compute group quantities are learned. As such, the interaction between tokens far from each other is considered in lower resolution in an efficient hierarchical manner. The overall complexity of Fast Multipole Attention is $\mathcal{O}(n)$ or $\mathcal{O}(n \
    
[^22]: 通过信息论分布多样化实现联邦泛化能力

    Federated Generalization via Information-Theoretic Distribution Diversification. (arXiv:2310.07171v1 [cs.LG])

    [http://arxiv.org/abs/2310.07171](http://arxiv.org/abs/2310.07171)

    该论文研究了联邦学习中泛化能力的挑战，特别关注训练分布和测试分布的不匹配。提出了一种信息论的泛化方法来解决这个问题。

    

    联邦学习（FL）因其在无需直接数据共享的情况下进行协同模型训练的能力而日益突出。然而，客户端之间本地数据分布的巨大差异，通常被称为非独立同分布（non-IID）挑战，对FL的泛化能力构成了重大障碍。当并非所有客户端都参与训练过程时，情况变得更加复杂，这是由于不稳定的网络连接或有限的计算能力而常见。这可能极大地复杂化了对训练模型的泛化能力的评估。尽管最近的大量研究集中在涉及具有不同分布的参与客户端的未见数据的泛化差距问题上，但参与客户端的训练分布和非参与客户端的测试分布之间的差异却被大部分忽视了。为此，我们的论文揭示了一种基于信息论的泛化方法。

    Federated Learning (FL) has surged in prominence due to its capability of collaborative model training without direct data sharing. However, the vast disparity in local data distributions among clients, often termed the non-Independent Identically Distributed (non-IID) challenge, poses a significant hurdle to FL's generalization efficacy. The scenario becomes even more complex when not all clients participate in the training process, a common occurrence due to unstable network connections or limited computational capacities. This can greatly complicate the assessment of the trained models' generalization abilities. While a plethora of recent studies has centered on the generalization gap pertaining to unseen data from participating clients with diverse distributions, the divergence between the training distributions of participating clients and the testing distributions of non-participating ones has been largely overlooked. In response, our paper unveils an information-theoretic genera
    
[^23]: 噪声容忍的无监督视觉语言模型适配器

    Noise-Tolerant Unsupervised Adapter for Vision-Language Models. (arXiv:2309.14928v1 [cs.CV])

    [http://arxiv.org/abs/2309.14928](http://arxiv.org/abs/2309.14928)

    这篇论文介绍了一种噪声容忍的无监督适配器(NtUA)，它可以使用少样本无标签目标样本来学习优秀的视觉语言模型。NtUA通过自适应缓存形成和伪标签修正来对抗伪标签噪声。

    

    最近在大规模的视觉语言模型中取得了非常显著的表现，在各种零样本图像分类任务中获得了良好的性能。然而，先前的研究通过引入少样本有标签目标样本已经取得了显著的改进，但仍需要目标样本的标注，这在处理各种视觉识别任务时大大降低了可扩展性。我们设计了一种噪声容忍的无监督适配器(NtUA)，它允许使用少样本无标签目标样本来学习优秀的目标模型。NtUA作为一个键值缓存，将少样本无标签目标样本的视觉特征和预测的伪标签作为键值对进行建模。它由两个互补的设计组成。第一个是自适应缓存形成，通过根据其预测置信度对键值对进行加权，以对抗伪标签的噪声。第二个是伪标签修正，它通过利用键值对的权重来修正伪标签以及缓存权重。

    Recent advances in large-scale vision-language models have achieved very impressive performance in various zero-shot image classification tasks. While prior studies have demonstrated significant improvements by introducing few-shot labelled target samples, they still require labelling of target samples, which greatly degrades their scalability while handling various visual recognition tasks. We design NtUA, a Noise-tolerant Unsupervised Adapter that allows learning superior target models with few-shot unlabelled target samples. NtUA works as a key-value cache that formulates visual features and predicted pseudo-labels of the few-shot unlabelled target samples as key-value pairs. It consists of two complementary designs. The first is adaptive cache formation that combats pseudo-label noises by weighting the key-value pairs according to their prediction confidence. The second is pseudo-label rectification, which corrects both pair values (i.e., pseudo-labels) and cache weights by leverag
    
[^24]: 自回归的下一个标记预测器是通用学习器。

    Auto-Regressive Next-Token Predictors are Universal Learners. (arXiv:2309.06979v1 [cs.LG])

    [http://arxiv.org/abs/2309.06979](http://arxiv.org/abs/2309.06979)

    自回归的下一个标记预测器可以有效地近似图灵机计算的任何函数，并且在文本生成和算术任务上展现出非平凡的性能。

    

    大型语言模型展现出在逻辑和数学推理方面的非凡能力，使其能够解决复杂任务。有趣的是，这些能力在训练于下一个标记预测的简单任务上的网络中出现。在这项工作中，我们提出了一个用于研究自回归下一个标记预测器的理论框架。我们证明了即使是简单的模型，如线性下一个标记预测器，当其在思维链数据上训练时，可以有效地近似图灵机计算的任何函数。我们引入了一个新的复杂度度量——长度复杂度，它衡量了在近似某个目标函数时，思维链序列中所需的中间标记的数量，并分析了长度复杂度和其他复杂性概念之间的相互关系。最后，我们通过实验证明简单的下一个标记预测器，如线性网络和浅层多层感知机（MLP），在文本生成和算术任务上展示出非平凡的性能。

    Large language models display remarkable capabilities in logical and mathematical reasoning, allowing them to solve complex tasks. Interestingly, these abilities emerge in networks trained on the simple task of next-token prediction. In this work, we present a theoretical framework for studying auto-regressive next-token predictors. We demonstrate that even simple models such as linear next-token predictors, trained on Chain-of-Thought (CoT) data, can approximate any function efficiently computed by a Turing machine. We introduce a new complexity measure -- length complexity -- which measures the number of intermediate tokens in a CoT sequence required to approximate some target function, and analyze the interplay between length complexity and other notions of complexity. Finally, we show experimentally that simple next-token predictors, such as linear networks and shallow Multi-Layer Perceptrons (MLPs), display non-trivial performance on text generation and arithmetic tasks. Our resul
    
[^25]: SMARLA：一种用于深度强化学习智能体的安全监测方法

    SMARLA: A Safety Monitoring Approach for Deep Reinforcement Learning Agents. (arXiv:2308.02594v1 [cs.LG])

    [http://arxiv.org/abs/2308.02594](http://arxiv.org/abs/2308.02594)

    本文提出了一种基于机器学习的安全监测方法SMARLA，用于深度强化学习智能体。该方法设计为黑盒子，利用状态抽象减少状态空间，实现对智能体状态的安全违规预测。经验证，SMARLA具有准确的违规预测能力，并可在智能体执行的早期阶段进行预测。

    

    深度强化学习算法(DRL)越来越多地应用于安全关键系统。确保DRL智能体的安全性在这种情况下是一个关键问题。然而，仅依靠测试是不足以确保安全性的，因为它不能提供保证。构建安全监测器是缓解这一挑战的一种解决方案。本文提出了SMARLA，一种基于机器学习的安全监测方法，专为DRL智能体设计。出于实际原因，SMARLA被设计为黑盒子(因为它不需要访问智能体的内部)，并利用状态抽象来减少状态空间，从而促进从智能体的状态学习安全违规预测模型。我们在两个知名的RL案例研究中验证了SMARLA。经验分析表明，SMARLA具有准确的违规预测能力，误报率低，并且可以在智能体执行的一半左右的早期阶段预测安全违规。

    Deep reinforcement learning algorithms (DRL) are increasingly being used in safety-critical systems. Ensuring the safety of DRL agents is a critical concern in such contexts. However, relying solely on testing is not sufficient to ensure safety as it does not offer guarantees. Building safety monitors is one solution to alleviate this challenge. This paper proposes SMARLA, a machine learning-based safety monitoring approach designed for DRL agents. For practical reasons, SMARLA is designed to be black-box (as it does not require access to the internals of the agent) and leverages state abstraction to reduce the state space and thus facilitate the learning of safety violation prediction models from agent's states. We validated SMARLA on two well-known RL case studies. Empirical analysis reveals that SMARLA achieves accurate violation prediction with a low false positive rate, and can predict safety violations at an early stage, approximately halfway through the agent's execution before 
    
[^26]: 处理情感回归和分类任务中单模态和多模态的通用视听学习

    Versatile Audio-Visual Learning for Handling Single and Multi Modalities in Emotion Regression and Classification Tasks. (arXiv:2305.07216v1 [cs.LG])

    [http://arxiv.org/abs/2305.07216](http://arxiv.org/abs/2305.07216)

    本文提出了一个通用视听学习（VAVL）框架，可用于处理情感回归和情感分类任务中的单模态和多模态系统，即使数据缺失或不匹配也能进行有效训练和切换。

    

    大多数当前的音视频情感识别模型缺乏实际应用所需的灵活性。我们设想了一个多模态系统，即使只有一个模态可用，也可以互换地实现预测情感属性或识别分类情感。在一个多模态情感识别系统中实现这样的灵活性存在困难，因为准确解释和整合各种数据来源是困难的。同时，允许在回归和分类任务之间直接切换，同时处理缺失或部分信息也是一个挑战。本研究提出了一个用于处理情感回归和情感分类任务的通用视听学习（VAVL）框架，实现了处理单模态和多模态系统的音视频框架，即使音频和视觉数据不匹配，也可以进行训练。

    Most current audio-visual emotion recognition models lack the flexibility needed for deployment in practical applications. We envision a multimodal system that works even when only one modality is available and can be implemented interchangeably for either predicting emotional attributes or recognizing categorical emotions. Achieving such flexibility in a multimodal emotion recognition system is difficult due to the inherent challenges in accurately interpreting and integrating varied data sources. It is also a challenge to robustly handle missing or partial information while allowing direct switch between regression and classification tasks. This study proposes a \emph{versatile audio-visual learning} (VAVL) framework for handling unimodal and multimodal systems for emotion regression and emotion classification tasks. We implement an audio-visual framework that can be trained even when audio and visual paired data is not available for part of the training set (i.e., audio only or only
    
[^27]: 基于序列 Knockoffs 的强化学习变量选择

    Sequential Knockoffs for Variable Selection in Reinforcement Learning. (arXiv:2303.14281v1 [stat.ML])

    [http://arxiv.org/abs/2303.14281](http://arxiv.org/abs/2303.14281)

    本论文介绍了一种新颖的序列 Knockoffs (SEEK)算法，用于在强化学习系统中实现变量选择，该算法估计了最小充分状态，确保学习进程良好而不会减缓。

    

    在强化学习的实际应用中，通常很难获得一个既简洁又满足马尔可夫属性的状态表示，而不需要使用先验知识。因此，常规做法是构造一个比必要的要大的状态，例如将连续时间点上的测量串联起来。然而，增加状态的维数可能会减缓学习进程并使学习策略模糊不清。我们引入了一个在马尔可夫决策过程(MDP)中的最小充分状态的概念，作为原始状态下最小的子向量，使该过程仍然是MDP，并且与原始过程共享相同的最优策略。我们提出了一种新颖的序列 Knockoffs (SEEK)算法，用于估计高维复杂非线性动力学系统中的最小充分状态。在大样本中，所提出的方法控制了假发现率，并且选择所有充分的变量的概率趋近于1。

    In real-world applications of reinforcement learning, it is often challenging to obtain a state representation that is parsimonious and satisfies the Markov property without prior knowledge. Consequently, it is common practice to construct a state which is larger than necessary, e.g., by concatenating measurements over contiguous time points. However, needlessly increasing the dimension of the state can slow learning and obfuscate the learned policy. We introduce the notion of a minimal sufficient state in a Markov decision process (MDP) as the smallest subvector of the original state under which the process remains an MDP and shares the same optimal policy as the original process. We propose a novel sequential knockoffs (SEEK) algorithm that estimates the minimal sufficient state in a system with high-dimensional complex nonlinear dynamics. In large samples, the proposed method controls the false discovery rate, and selects all sufficient variables with probability approaching one. As
    
[^28]: 使用强化学习技术革新基因组学

    Revolutionizing Genomics with Reinforcement Learning Techniques. (arXiv:2302.13268v2 [q-bio.GN] UPDATED)

    [http://arxiv.org/abs/2302.13268](http://arxiv.org/abs/2302.13268)

    强化学习是一种革新的工具，可以在基因组学领域中解决自动数据分析和处理的问题。使用强化学习算法可以降低收集标记训练数据的成本，适用于基因组数据分析和解释。本调查重点关注在基因组研究领域中使用强化学习的应用，包括基因调控网络、基因组组装和序列比对。

    

    近年来，强化学习（RL）作为一种强大的工具出现在解决各种问题中，包括决策和基因组学。过去二十年的原始基因组数据指数增长已经超出了手动分析的能力，这导致对自动数据分析和处理的兴趣越来越大。RL算法能够在最小的人工监督下从经验中学习，使其非常适合基因组数据分析和解释。使用RL的一个关键好处是降低了收集标记训练数据的成本，这是监督学习所需的。虽然已经有许多研究探讨了机器学习在基因组学中的应用，但本调查仅专注于在各种基因组研究领域（包括基因调控网络，基因组组装和序列比对）中使用RL的情况。我们对现有研究的技术细节进行了全面的概述。

    In recent years, Reinforcement Learning (RL) has emerged as a powerful tool for solving a wide range of problems, including decision-making and genomics. The exponential growth of raw genomic data over the past two decades has exceeded the capacity of manual analysis, leading to a growing interest in automatic data analysis and processing. RL algorithms are capable of learning from experience with minimal human supervision, making them well-suited for genomic data analysis and interpretation. One of the key benefits of using RL is the reduced cost associated with collecting labeled training data, which is required for supervised learning. While there have been numerous studies examining the applications of Machine Learning (ML) in genomics, this survey focuses exclusively on the use of RL in various genomics research fields, including gene regulatory networks (GRNs), genome assembly, and sequence alignment. We present a comprehensive technical overview of existing studies on the applic
    
[^29]: 《通过对抗性少数派影响攻击合作多智能体强化学习》

    Attacking Cooperative Multi-Agent Reinforcement Learning by Adversarial Minority Influence. (arXiv:2302.03322v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03322](http://arxiv.org/abs/2302.03322)

    本研究提出了 Adversarial Minority Influence (AMI) 黑盒攻击，能够在考虑多智能体互动和合作目标下实现有针对性的最坏情况合作，攻击成功率比现有方法高出 2.2倍。

    

    本研究探讨了合作多智能体强化学习(c-MARL)在对抗性攻击下的弱点，这是在实际实现之前c-MARL最坏情况下性能的重要决定因素。目前基于观察的攻击，受到白盒假设的限制，忽视了c-MARL的复杂多智能体交互和合作目标，导致攻击能力受到实际和限制。为了解决这些问题，我们提出了Adversarial Minority Influence (AMI)，这是一个实用而强大的针对c-MARL的黑盒攻击。AMI是一个实用的黑盒攻击，可以在不了解受害者参数的情况下发动攻击。AMI也是强大的，因为它考虑了复杂的多智能体互动和代理的合作目标，使单个对抗性代理能够单方面误导大多数受害者形成有针对性的最坏情况合作。这反映了社会心理学中的少数派影响现象。为了在复杂的代理方式交互下实现最大的受害者政策偏差，我们基于博弈理论分析推导出一种高效且可扩展的AMI优化算法。我们证明了AMI的有效性以及考虑多智能体交互的必要性，表明AMI可以实现比现有方法高出2.2倍的攻击成功率。

    This study probes the vulnerabilities of cooperative multi-agent reinforcement learning (c-MARL) under adversarial attacks, a critical determinant of c-MARL's worst-case performance prior to real-world implementation. Current observation-based attacks, constrained by white-box assumptions, overlook c-MARL's complex multi-agent interactions and cooperative objectives, resulting in impractical and limited attack capabilities. To address these shortcomes, we propose Adversarial Minority Influence (AMI), a practical and strong for c-MARL. AMI is a practical black-box attack and can be launched without knowing victim parameters. AMI is also strong by considering the complex multi-agent interaction and the cooperative goal of agents, enabling a single adversarial agent to unilaterally misleads majority victims to form targeted worst-case cooperation. This mirrors minority influence phenomena in social psychology. To achieve maximum deviation in victim policies under complex agent-wise intera
    

