# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Unconditional Latent Diffusion Models Memorize Patient Imaging Data](https://rss.arxiv.org/abs/2402.01054) | 本论文研究了医学图像合成中隐式扩散模型的记忆问题。通过评估训练数据的记忆程度以及探索可能导致记忆的因素，揭示了这一问题的重要性和潜在风险。 |
| [^2] | [On the Importance of Uncertainty in Decision-Making with Large Language Models](https://arxiv.org/abs/2404.02649) | 本研究调查了使用大型语言模型作为代理进行自然语言输入决策问题时，不确定性估计的重要性，并提出了集成不确定性估计到汤普森抽样策略的方法。 |
| [^3] | [Aardvark Weather: end-to-end data-driven weather forecasting](https://arxiv.org/abs/2404.00411) | Aardvark Weather是第一个端到端数据驱动的预报系统，能够取代传统数值天气预报系统，提供全球和本地精准预报。 |
| [^4] | [Functional-Edged Network Modeling](https://arxiv.org/abs/2404.00218) | 本研究提出了一种功能边缘网络模型，通过将边视为功能数据，并引入额外维度来表示函数，使用Tucker功能分解处理功能邻接张量，进行模型推断以解决不规则观测问题，并通过正则化使基础矩阵对称化，最终展示了模型的理想属性。 |
| [^5] | [Mixing Artificial and Natural Intelligence: From Statistical Mechanics to AI and Back to Turbulence](https://arxiv.org/abs/2403.17993) | 人工智能对通过创新性使用深度神经网络推动湍流减少的拉格朗日模型具有重要影响，为AI和湍流研究之间紧密交织的未来铺平道路。 |
| [^6] | [Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding Model Mechanisms](https://arxiv.org/abs/2403.17806) | 提出了一种新方法EAP-IG，旨在更好地保持电路的核心属性：忠实 |
| [^7] | [Secure Aggregation is Not Private Against Membership Inference Attacks](https://arxiv.org/abs/2403.17775) | 本文探讨了安全聚合在隐私方面的问题，揭示了其在面对成员推断攻击时并不具备足够的私密性。 |
| [^8] | [PathoTune: Adapting Visual Foundation Model to Pathological Specialists](https://arxiv.org/abs/2403.16497) | PathoTune提出了一个框架，能够通过多模态提示微调，将病理甚至视觉基础模型高效地调整到病理特定任务，从而缓解基础-任务差距和任务-实例差距。 |
| [^9] | [A Survey on Consumer IoT Traffic: Security and Privacy](https://arxiv.org/abs/2403.16149) | 本调查针对消费者物联网（CIoT）流量分析从安全和隐私的角度出发，总结了CIoT流量分析的新特征、最新进展和挑战，认为通过流量分析可以揭示CIoT领域中的安全和隐私问题。 |
| [^10] | [Emergent World Models and Latent Variable Estimation in Chess-Playing Language Models](https://arxiv.org/abs/2403.15498) | 棋类语言模型在没有先验知识的情况下，通过下一个字符预测训练，仍能学习出内部表示的棋盘状态 |
| [^11] | [Preventing Catastrophic Forgetting through Memory Networks in Continuous Detection](https://arxiv.org/abs/2403.14797) | 通过引入记忆网络和局部查询函数，这项工作致力于在连续检测中防止灾难性遗忘，并解决了持续检测中的背景贬低问题。 |
| [^12] | [Spatial-Temporal Graph Representation Learning for Tactical Networks Future State Prediction](https://arxiv.org/abs/2403.13872) | 本文提出了一种空间-时间图编码器-解码器（STGED）框架，用于战术通信网络，通过有效利用网络状态的空间和时间特征，实现了对未来状态的准确预测。 |
| [^13] | [Removing Undesirable Concepts in Text-to-Image Generative Models with Learnable Prompts](https://arxiv.org/abs/2403.12326) | 通过引入可学习提示到交叉注意力模块中，本文提出了一种新方法，用于从文本到图像生成模型中去除不良概念，实现了对模型效果的提升。 |
| [^14] | [SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant](https://arxiv.org/abs/2403.11299) | 本研究引入了一个名为SQ-LLaVA的新颖框架，通过自我训练模型如何提出高质量问题，以改善视觉-语言模型的泛化能力。 |
| [^15] | [Uncovering Latent Themes of Messaging on Social Media by Integrating LLMs: A Case Study on Climate Campaigns](https://arxiv.org/abs/2403.10707) | 本文提出了一种通过利用大型语言模型（LLMs）的先进功能，以机器在循环中方法，处理社交媒体消息主题的新方法。 |
| [^16] | [Denoising Task Difficulty-based Curriculum for Training Diffusion Models](https://arxiv.org/abs/2403.10348) | 研究通过全面研究任务难度，发现较早时间步长的去噪任务更具挑战性，提出了基于去噪任务难度的渐进式课程训练方法。 |
| [^17] | [Improving Medical Multi-modal Contrastive Learning with Expert Annotations](https://arxiv.org/abs/2403.10153) | eCLIP是一种改进的CLIP模型，通过集成专家注释和混合增强来应对医学影像分析中的数据稀缺和模态差距挑战，提高了模型学习效果 |
| [^18] | [SMART: Submodular Data Mixture Strategy for Instruction Tuning](https://arxiv.org/abs/2403.08370) | SMART引入了一种新颖的数据混合策略，利用子模块函数为任务分配重要性分数，并在微调中重新分配预算，从而在指令调整任务中取得明显优势。 |
| [^19] | [Dissecting Deep RL with High Update Ratios: Combatting Value Overestimation and Divergence](https://arxiv.org/abs/2403.05996) | 本研究剖析了深度强化学习中的首要偏差现象，发现在大量更新比例下，价值高估是导致学习失败的根本挑战。 |
| [^20] | [Optimizing Retinal Prosthetic Stimuli with Conditional Invertible Neural Networks](https://arxiv.org/abs/2403.04884) | 利用有条件可逆神经网络无监督优化视网膜假体刺激，提高了电极阵列的刺激效果。 |
| [^21] | [Android in the Zoo: Chain-of-Action-Thought for GUI Agents](https://arxiv.org/abs/2403.02713) | 该研究提出了一个名为CoAT的Chain-of-Action-Thought模型，通过考虑先前动作描述、当前屏幕情况以及未来动作思考，显著提高了智能手机GUI代理的任务执行效果。 |
| [^22] | [Improving LLM Code Generation with Grammar Augmentation](https://arxiv.org/abs/2403.01632) | SynCode是一个新框架，结合编程语言的语法和DFA mask store，在LLMs中生成代码过程中获得96.07%的句法错误降低，并展现出提高句法精度的重大影响。 |
| [^23] | [Pandora's White-Box: Increased Training Data Leakage in Open LLMs](https://arxiv.org/abs/2402.17012) | 本文对开源大型语言模型（LLMs）进行了隐私攻击研究，提出了首个能同时实现高真正率和低误分类率的预训练LLMs会员推理攻击（MIAs），以及展示了在自然环境中可以从微调LLM中提取超过50%的微调数据集。 |
| [^24] | [ProTIP: Probabilistic Robustness Verification on Text-to-Image Diffusion Models against Stochastic Perturbation](https://arxiv.org/abs/2402.15429) | 本研究引入了概率概念的文本到图像扩散模型鲁棒性，并建立了一个名为ProTIP的高效框架用于评估其统计保证，解决了生成过程的高计算成本和对抗性样本判断困难的问题 |
| [^25] | [ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition](https://arxiv.org/abs/2402.15220) | ChunkAttention是一种前缀感知的自注意力模块，通过将键/值张量分解为较小的块并结构化到辅助前缀树中，实现了在运行时改善内存利用率的KV缓存，同时设计了两阶段分区算法以提高自注意力计算中的数据局部性。 |
| [^26] | [Contrastive Learning of Shared Spatiotemporal EEG Representations Across Individuals for Naturalistic Neuroscience](https://arxiv.org/abs/2402.14213) | 通过对比学习，利用神经网络最大化相同刺激下各个个体的EEG表示的相似性，以此实现个体间共享时空脑电图表示的学习。 |
| [^27] | [Simple and Effective Transfer Learning for Neuro-Symbolic Integration](https://arxiv.org/abs/2402.14047) | 提出了一种简单而有效的方法，通过在下游任务上预训练神经模型，然后通过迁移学习在相同任务上对NeSy模型进行训练，以实现神经符号一体化的改进。 |
| [^28] | [Improving a Proportional Integral Controller with Reinforcement Learning on a Throttle Valve Benchmark](https://arxiv.org/abs/2402.13654) | 通过引入强化学习与引导，结合比例积分（PI）控制器，本文提出了一种学习基础的控制策略，用于非线性节流阀的控制，实现了一个几乎最优的控制器。 |
| [^29] | [Diffusion Tempering Improves Parameter Estimation with Probabilistic Integrators for Ordinary Differential Equations](https://arxiv.org/abs/2402.12231) | 扩散回火是一种新颖的正则化技术，可改善概率数值方法在普通微分方程中的参数优化收敛性，实现对复杂动态系统中参数的可靠估计 |
| [^30] | [Avoiding Feature Suppression in Contrastive Learning: Learning What Has Not Been Learned Before](https://arxiv.org/abs/2402.11816) | 开发了一种多阶对比学习（MCL）框架，以解决对比学习中的特征抑制问题，并确保模型学习全面的表示。 |
| [^31] | [Fundamental Benefit of Alternating Updates in Minimax Optimization](https://arxiv.org/abs/2402.10475) | Alt-GDA算法被证明更快，提出了交替外推GDA（Alex-GDA）通用算法框架，以轮流从迭代的外推中获取梯度。 |
| [^32] | [Exploration-Driven Policy Optimization in RLHF: Theoretical Insights on Efficient Data Utilization](https://arxiv.org/abs/2402.10342) | 本研究提出了一个基于探索驱动策略优化的RLHF算法，通过轨迹比较反馈推断奖励函数，为解释少量人类反馈足以实现良好性能提供了理论洞见 |
| [^33] | [Diffusion Models for Audio Restoration](https://arxiv.org/abs/2402.09821) | 本文介绍了基于扩散模型的音频恢复算法，重点关注语音增强和音乐恢复任务。 |
| [^34] | [Rethinking Machine Unlearning for Large Language Models](https://arxiv.org/abs/2402.08787) | 这篇论文研究了大型语言模型中的机器消除技术，旨在消除不良数据的影响并保持基本知识生成的完整性，为开发安全、可靠和资源高效的生成式人工智能提供基础。 |
| [^35] | [Selective Learning: Towards Robust Calibration with Dynamic Regularization](https://arxiv.org/abs/2402.08384) | 本研究提出了一种名为动态正则化（DReg）的方法，通过训练学习应该学到什么，从而解决深度学习中的过拟合和误校准问题。 |
| [^36] | [Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models](https://arxiv.org/abs/2402.07754) | 本文介绍了一种将扩散模型与思维链推理集成的方法，通过扩散传播推理步骤，提供了更大的灵活性和推理能力。实验证明了该方法在数学问题中的有效性，并展示了自我纠正能力和推理技术的潜力。 |
| [^37] | [Partially Stochastic Infinitely Deep Bayesian Neural Networks](https://arxiv.org/abs/2402.03495) | 本文提出了一种部分随机性的无限深度贝叶斯神经网络，通过在网络框架中整合部分随机性，改善现有架构在训练和推理时间上的计算效率限制，并提供了多种灵活的网络设计配置，同时通过数学证明确保了模型的表达能力。 |
| [^38] | [SpecFormer: Guarding Vision Transformer Robustness via Maximum Singular Value Penalization](https://arxiv.org/abs/2402.03317) | 该论文介绍了SpecFormer，一种通过最大奇异值惩罚来增强视觉Transformer（ViTs）对对抗性攻击的韧性的方法。该方法通过引入局部Lipschitz边界和最大奇异值惩罚方法（MSVP），有效地降低了注意力权重矩阵的谱范数。 |
| [^39] | [From PEFT to DEFT: Parameter Efficient Finetuning for Reducing Activation Density in Transformers](https://arxiv.org/abs/2402.01911) | 本论文提出了一种用于减少变压器模型中激活密度的参数高效微调方法 DEFT。研究发现预训练模型中存在激活稀疏性，并通过引入新的密度损失来促进更高的激活稀疏性。通过应用主流的PEFT技术，包括QLoRA、LoRA、Adapter、Prompt/Prefix Tuning，实验证明了该方法在不同下游任务中的有效性。 |
| [^40] | [Datacube segmentation via Deep Spectral Clustering](https://arxiv.org/abs/2401.17695) | 通过应用深度聚类算法对数据立方体像素的光谱属性进行无监督聚类，可以实现数据立方体的图像分割和统计解释。 |
| [^41] | [A Conservative Approach for Few-Shot Transfer in Off-Dynamics Reinforcement Learning](https://arxiv.org/abs/2312.15474) | 提出了一种受最近模仿学习和保守RL算法进展启发的创新方法，在离线动力学强化学习中的少样本转移过程中引入惩罚来调节源训练策略生成的轨迹。 |
| [^42] | [Vision-Language Models as a Source of Rewards](https://arxiv.org/abs/2312.09187) | 使用现成的视觉-语言模型作为强化学习代理的奖励来源，展示了如何通过CLIP系列模型派生视觉目标实现的奖励，从而训练出能够实现多种语言目标的RL代理。 |
| [^43] | [SparQ Attention: Bandwidth-Efficient LLM Inference](https://arxiv.org/abs/2312.04985) | SparQ Attention通过减少注意力块内存带宽需求的技术，从而增加LLMs推理的吞吐量，同时保持模型准确性。 |
| [^44] | [Handling The Non-Smooth Challenge in Tensor SVD: A Multi-Objective Tensor Recovery Framework](https://arxiv.org/abs/2311.13958) | 提出一种具有可学习张量核范数的新型张量恢复模型，引入交替近端乘子方法（APMM）优化算法，解决处理非光滑变化的张量数据挑战 |
| [^45] | [Bias and Fairness in Large Language Models: A Survey](https://arxiv.org/abs/2309.00770) | 该论文在大型语言模型领域提出了偏见评估和缓解技术的综合调查，定义了公平性的不同方面，并提出了三个分类体系，以协助研究人员对LLMs进行公平性分析和改进。 |
| [^46] | [A Survey on Uncertainty Quantification for Deep Learning: An Uncertainty Source Perspective](https://arxiv.org/abs/2302.13425) | 本研究对深度学习的不确定性量化进行了调查，从不确定性来源的角度分析不同方法，以评估DNN预测的置信度。 |
| [^47] | [Parametric Matrix Models.](http://arxiv.org/abs/2401.11694) | 参数矩阵模型是一种通用机器学习算法，基于矩阵方程设计，通过简化基础方法进行近似解参数方程。它可以仅使用经验数据进行训练，适用于各种机器学习问题，并在计算框架内产生准确的结果。 |
| [^48] | [Object-Centric Diffusion for Efficient Video Editing.](http://arxiv.org/abs/2401.05735) | 本论文提出了一种面向对象的扩散技术，通过分配更多的计算资源给前景编辑区域来实现视频编辑的高效率，从而大大提高了速度，同时保持了质量。 |
| [^49] | [Stable generative modeling using diffusion maps.](http://arxiv.org/abs/2401.04372) | 本文提出了一种稳定的生成建模方法，通过将扩散映射与朗之万动力学相结合，在仅有有限数量的训练样本的情况下生成新样本，并解决了时间步长僵硬随机微分方程中的稳定性问题。 |
| [^50] | [Context-aware Communication for Multi-agent Reinforcement Learning.](http://arxiv.org/abs/2312.15600) | 这项研究针对多智能体强化学习提出了一种上下文感知的通信方案，通过两个阶段的交流，使智能体能够发送个性化的消息，从而提高合作和团队性能。 |
| [^51] | [Speak Like a Native: Prompting Large Language Models in a Native Style.](http://arxiv.org/abs/2311.13538) | 本文提出了一种名为AlignedCoT的新颖有效方法，通过将上下文示例与大型语言模型（LLMs）的母语风格对齐，提高了LLMs的推理能力和性能。 |
| [^52] | [Meta-Learning Strategies through Value Maximization in Neural Networks.](http://arxiv.org/abs/2310.19919) | 本文理论上研究了在神经网络中的元学习最优策略，并提出了一个学习努力的框架，可以高效地优化控制信号，从而提升学习性能。 |
| [^53] | [Enhancing Low-Precision Sampling via Stochastic Gradient Hamiltonian Monte Carlo.](http://arxiv.org/abs/2310.16320) | 本文研究了使用低精度和全精度梯度累加器的随机梯度Hamiltonian Monte Carlo (SGHMC)在低精度采样中的应用。实验证明，在非对数凹分布下，低精度SGHMC相对于低精度采样器（SGLD）实现了二次改进。 |
| [^54] | [Meta- (out-of-context) learning in neural networks.](http://arxiv.org/abs/2310.15047) | 该研究通过合成实验展示了一种称为元-超文本外语境学习（meta-OCL）的现象在神经网络中的存在。这种学习使神经网络能够更好地吸收广泛适用的语义内容，并在适当的情况下进行使用。研究者提出了关于元-超文本外语境学习产生的两种假设，并就未来AI系统的能力和潜在风险进行了讨论。 |
| [^55] | [DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM Planning.](http://arxiv.org/abs/2310.12128) | DiagrammerGPT是一个通过LLM规划生成开放领域、开放平台的图表的框架，填补了T2I模型在图表生成方面的空白。 |
| [^56] | [One for All: Towards Training One Graph Model for All Classification Tasks.](http://arxiv.org/abs/2310.00149) | 这项研究提出了一种名为“一刀切”的通用框架，该框架能够使用单一图模型解决不同领域的多个任务。该框架克服了图学习领域的挑战，包括不同属性和分布的图数据、不同类型的任务以及上下文学习的问题。 |
| [^57] | [Spurious Feature Diversification Improves Out-of-distribution Generalization.](http://arxiv.org/abs/2309.17230) | 本文研究了基于权重空间集成方法WiSE-FT在分布外泛化中的有效性，发现其成功纠正了许多个体模型的错误预测，并通过利用更多多样化的伪特征减少了分布外设置中的预测错误。 |
| [^58] | [VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning.](http://arxiv.org/abs/2309.15091) | 本文提出了VideoDirectorGPT，一种利用LLMs的知识实现一致多场景视频生成的框架，通过视频内容规划和基于内容的视频生成来生成时间上一致的长视频。 |
| [^59] | [Generalized Continual Category Discovery.](http://arxiv.org/abs/2308.12112) | 本研究提出了一种广义持续类别发现（GCCD）的框架，用于在现实生活场景中同时处理新的和已知的类别，并且利用持续的无监督学习方法来发现它们。通过实验证明现有方法无法处理后续任务中的无标记样本。 |
| [^60] | [AdvDiff: Generating Unrestricted Adversarial Examples using Diffusion Models.](http://arxiv.org/abs/2307.12499) | 本文提出了一种使用扩散模型生成无限制对抗样本的方法AdvDiff。通过设计两种新的对抗引导技术，在扩散模型的逆生成过程中进行对抗采样，从而有效地生成高质量、逼真的对抗样本。 |
| [^61] | [Divide & Bind Your Attention for Improved Generative Semantic Nursing.](http://arxiv.org/abs/2307.10864) | 本论文提出了一种名为"分割与绑定"的方法，旨在改进生成语义护理的效果。该方法引入了新的损失目标，包括关注丢失和绑定丢失，以解决复杂提示和不适当属性绑定的问题。 |
| [^62] | [Robotic Manipulation Datasets for Offline Compositional Reinforcement Learning.](http://arxiv.org/abs/2307.07091) | 本论文提供了离线强化学习的机器人操作数据集，使用组合式强化学习生成了四个包含256个任务的数据集。每个数据集由性能不同的代理采集，包含2.56亿条转换记录。实验结果显示， |
| [^63] | [A Brief Review of Hypernetworks in Deep Learning.](http://arxiv.org/abs/2306.06955) | 超网络是一种生成另一个神经网络权重的深度学习技术，具有灵活性、适应性、动态性、更快的训练速度、信息共享和模型压缩等优点。它在各种深度学习问题中显示了良好的效果，并在持续学习、迁移学习、权重剪枝、不确定性量化、零样本学习、自然语言处理和强化学习等领域取得了成功。 |
| [^64] | [Public Transit Demand Prediction During Highly Dynamic Conditions: A Meta-Analysis of State-of-the-Art Models and Open-Source Benchmarking Infrastructure.](http://arxiv.org/abs/2306.06194) | 本论文基于智能卡数据的时间序列预测哥伦比亚波哥大BRT系统隔日需求，建立了一个开源基础设施并评估了五种常见方法在稳定和高度动态条件下的表现，结果显示大多数测试模型的表现相似，MAAPE从0.08到0.12不等。 |
| [^65] | [Equivariant vs. Invariant Layers: A Comparison of Backbone and Pooling for Point Cloud Classification.](http://arxiv.org/abs/2306.05553) | 本文研究了置换等变骨干和置换不变全局池化在点云分类中的相互作用，揭示了使用复杂池化方法可以显著提高简单骨干的性能，但即使是复杂的骨干也可以受益于更复杂的、明确编码置换不变性的池化方法，使用置换不变池化是获得最先进结果的关键。 |
| [^66] | [Correlated Noise in Epoch-Based Stochastic Gradient Descent: Implications for Weight Variances.](http://arxiv.org/abs/2306.05300) | 研究挑战了在时间上是不相关的假设，并强调了epoch-based噪声相关性对离散时间带动量的SGD的权重方差的影响。 |
| [^67] | [Align, Distill, and Augment Everything All at Once for Imbalanced Semi-Supervised Learning.](http://arxiv.org/abs/2306.04621) | 本文针对长尾半监督学习中类别不平衡的问题，提出了三个解决方案：一种灵活的分布对齐方法，一种软一致性正则化方法和一种扩充未标记集的方案。 |
| [^68] | [Counterfactual Generative Models for Time-Varying Treatments.](http://arxiv.org/abs/2305.15742) | 本文研究了时间变量处理情况下的反事实生成模型，能够捕捉整个反事实分布，并且能够有效推断反事实分布的某些统计量，适用于医疗保健和公共政策制定领域。 |
| [^69] | [Learning Hybrid Actor-Critic Maps for 6D Non-Prehensile Manipulation.](http://arxiv.org/abs/2305.03942) | 论文介绍了一种名为HACMan的强化学习方法，用于使用点云观察进行6D非抓取式操作的物体操纵。HACMan重点关注物体中心动作表示，它包括从物体点云中选择接触位置和一组描述机器人在接触后如何移动的运动参数。在实际测试中，HACMan的表现明显优于现有基线方法。 |
| [^70] | [Quantum physics-informed neural networks for simulating computational fluid dynamics in complex shapes.](http://arxiv.org/abs/2304.11247) | 本文提出了一种基于量子物理的神经网络方法，用于模拟复杂几何形状中的流体流动。该方法不需要重新模拟，能够适用于不同的形状，并且相比于普通神经网络提高了21%的精度。 |
| [^71] | [MaskedKD: Efficient Distillation of Vision Transformers with Masked Images.](http://arxiv.org/abs/2302.10494) | MaskedKD提出了一种通过遮蔽图像块来显著降低Vision Transformer (ViT)蒸馏成本的方法，而不影响学生模型的预测准确性。 |
| [^72] | [A Survey of Self-supervised Learning from Multiple Perspectives: Algorithms, Applications and Future Trends.](http://arxiv.org/abs/2301.05712) | 本综述论文从算法、应用和趋势的角度概述了自监督学习的多维视角。它介绍了SSL算法的动机、共性和差异，以及在图像处理、计算机视觉和自然语言处理等领域中的典型应用。 |
| [^73] | [UNSAT Solver Synthesis via Monte Carlo Forest Search.](http://arxiv.org/abs/2211.12581) | 介绍了使用MCFS算法合成UNSAT求解器的方法，算法可用于解决包括SAT公式不可满足性证明、可满足SAT公式解的数量计数和混合整数规划的最优解问题，并利用合成森林构建算法和合成MDP类来避免构建候选树森林的问题。 |

# 详细

[^1]: 无条件的隐式扩散模型记忆患者影像数据

    Unconditional Latent Diffusion Models Memorize Patient Imaging Data

    [https://rss.arxiv.org/abs/2402.01054](https://rss.arxiv.org/abs/2402.01054)

    本论文研究了医学图像合成中隐式扩散模型的记忆问题。通过评估训练数据的记忆程度以及探索可能导致记忆的因素，揭示了这一问题的重要性和潜在风险。

    

    生成式的隐式扩散模型在医学影像领域具有广泛的应用。一个值得注意的应用是通过提出合成数据作为真实患者数据的替代品来实现隐私保护的开放数据共享。尽管有这个应用的前景，但这些模型容易出现患者数据的记忆问题，即模型生成患者数据的副本而不是新的合成样本。这破坏了保护患者数据的整个目的，甚至可能导致患者被重新识别。考虑到这个问题的重要性，令人惊讶的是，在医学影像界中这个问题并没有得到太多关注。为此，我们评估了医学图像合成中隐式扩散模型的记忆问题。我们训练了2D和3D的隐式扩散模型，使用CT、MR和X光数据集进行合成数据的生成。之后，我们利用自监督模型来评估训练数据被记忆的程度，并进一步研究可能导致记忆的各种因素。

    Generative latent diffusion models hold a wide range of applications in the medical imaging domain. A noteworthy application is privacy-preserved open-data sharing by proposing synthetic data as surrogates of real patient data. Despite the promise, these models are susceptible to patient data memorization, where models generate patient data copies instead of novel synthetic samples. This undermines the whole purpose of preserving patient data and may even result in patient re-identification. Considering the importance of the problem, surprisingly it has received relatively little attention in the medical imaging community. To this end, we assess memorization in latent diffusion models for medical image synthesis. We train 2D and 3D latent diffusion models on CT, MR, and X-ray datasets for synthetic data generation. Afterwards, we examine the amount of training data memorized utilizing self-supervised models and further investigate various factors that can possibly lead to memorization 
    
[^2]: 论决策中不确定性的重要性与大型语言模型

    On the Importance of Uncertainty in Decision-Making with Large Language Models

    [https://arxiv.org/abs/2404.02649](https://arxiv.org/abs/2404.02649)

    本研究调查了使用大型语言模型作为代理进行自然语言输入决策问题时，不确定性估计的重要性，并提出了集成不确定性估计到汤普森抽样策略的方法。

    

    我们调查了自然语言输入的决策问题中不确定性的作用。对于这样的任务，使用大型语言模型作为代理已经成为常态。然而，最近的方法中没有任何一个额外的阶段用于估计代理在决策任务中对世界的不确定性。我们关注以自然语言为输入的基本决策框架之一，即上下文臂，其中上下文信息包括文本。作为没有不确定性估计的方法代表，我们考虑一个带有贪婪策略的LLM臂，该策略选择对应于最大预测奖励的动作。我们将此基准与通过将不确定性集成到汤普森抽样策略中积极利用不确定性估计的LLM臂进行比较。我们采用不同的技术进行不确定性估计，例如拉普拉斯近似、辍学和Epin。

    arXiv:2404.02649v1 Announce Type: new  Abstract: We investigate the role of uncertainty in decision-making problems with natural language as input. For such tasks, using Large Language Models as agents has become the norm. However, none of the recent approaches employ any additional phase for estimating the uncertainty the agent has about the world during the decision-making task. We focus on a fundamental decision-making framework with natural language as input, which is the one of contextual bandits, where the context information consists of text. As a representative of the approaches with no uncertainty estimation, we consider an LLM bandit with a greedy policy, which picks the action corresponding to the largest predicted reward. We compare this baseline to LLM bandits that make active use of uncertainty estimation by integrating the uncertainty in a Thompson Sampling policy. We employ different techniques for uncertainty estimation, such as Laplace Approximation, Dropout, and Epin
    
[^3]: Aardvark Weather:端对端数据驱动的天气预报

    Aardvark Weather: end-to-end data-driven weather forecasting

    [https://arxiv.org/abs/2404.00411](https://arxiv.org/abs/2404.00411)

    Aardvark Weather是第一个端到端数据驱动的预报系统，能够取代传统数值天气预报系统，提供全球和本地精准预报。

    

    机器学习正在彻底改变中程天气预测。然而，它仅被应用于天气预测管道的特定和单个组件。因此，这些数据驱动方法无法在没有来自传统操作数值天气预报系统的输入的情况下部署，这是计算成本高昂且不支持端到端优化。在这项工作中，我们采用了一种根本不同的方法，用机器学习模型取代整个数值天气预报管道。我们提出了Aardvark Weather，这是第一个端到端数据驱动的预报系统，它接受原始观测数据作为输入，并提供全球和本地预报。这些全球预报以一度空间分辨率，24小时时间分辨率为多个压力水平24个变量产生，并在五到七天的前期领先时段对每小时气候的预测能力。本地预报是...

    arXiv:2404.00411v1 Announce Type: cross  Abstract: Machine learning is revolutionising medium-range weather prediction. However it has only been applied to specific and individual components of the weather prediction pipeline. Consequently these data-driven approaches are unable to be deployed without input from conventional operational numerical weather prediction (NWP) systems, which is computationally costly and does not support end-to-end optimisation. In this work, we take a radically different approach and replace the entire NWP pipeline with a machine learning model. We present Aardvark Weather, the first end-to-end data-driven forecasting system which takes raw observations as input and provides both global and local forecasts. These global forecasts are produced for 24 variables at multiple pressure levels at one-degree spatial resolution and 24 hour temporal resolution, and are skillful with respect to hourly climatology at five to seven day lead times. Local forecasts are pr
    
[^4]: 功能边缘网络建模

    Functional-Edged Network Modeling

    [https://arxiv.org/abs/2404.00218](https://arxiv.org/abs/2404.00218)

    本研究提出了一种功能边缘网络模型，通过将边视为功能数据，并引入额外维度来表示函数，使用Tucker功能分解处理功能邻接张量，进行模型推断以解决不规则观测问题，并通过正则化使基础矩阵对称化，最终展示了模型的理想属性。

    

    与现有作品形成对比，现有作品都将节点视为函数，并使用边来表示不同函数之间的关系。我们的目标是网络建模，其中边是功能数据，并将邻接矩阵转换为功能邻接张量，引入一个额外的维度专门用于函数表示。我们使用Tucker功能分解来处理功能邻接张量，为进一步考虑节点之间的社区，对基础矩阵进行正则化使其对称化。此外，为了处理功能边的不规则观测，我们进行模型推断以解决张量完成问题，通过Riemann共轭梯度下降方法进行优化。除此之外，我们还推导出几个定理来展示功能边缘网络模型的理想属性。最后，我们使用模拟数据和真实地铁系统数据评估了我们提出的模型的有效性。

    arXiv:2404.00218v1 Announce Type: cross  Abstract: Contrasts with existing works which all consider nodes as functions and use edges to represent the relationships between different functions. We target at network modeling whose edges are functional data and transform the adjacency matrix into a functional adjacency tensor, introducing an additional dimension dedicated to function representation. Tucker functional decomposition is used for the functional adjacency tensor, and to further consider the community between nodes, we regularize the basis matrices to be symmetrical. Furthermore, to deal with irregular observations of the functional edges, we conduct model inference to solve a tensor completion problem. It is optimized by a Riemann conjugate gradient descent method. Besides these, we also derive several theorems to show the desirable properties of the functional edged network model. Finally, we evaluate the efficacy of our proposed model using simulation data and real metro sys
    
[^5]: 将人工智能与自然智能相融合：从统计力学到人工智能再到湍流

    Mixing Artificial and Natural Intelligence: From Statistical Mechanics to AI and Back to Turbulence

    [https://arxiv.org/abs/2403.17993](https://arxiv.org/abs/2403.17993)

    人工智能对通过创新性使用深度神经网络推动湍流减少的拉格朗日模型具有重要影响，为AI和湍流研究之间紧密交织的未来铺平道路。

    

    这篇论文反思了人工智能在科学研究中的未来角色，特别关注了湍流研究，并通过根植于非平衡统计力学的扩散模型来检验人工智能的发展，强调了人工智能通过创新性地利用深度神经网络推动减少的拉格朗日湍流模型的重要影响。此外，论文审查了湍流研究中的各种其他人工智能应用，并概述了在人工智能和统计流体力学的同时发展中的潜在挑战和机会。

    arXiv:2403.17993v1 Announce Type: cross  Abstract: The paper reflects on the future role of AI in scientific research, with a special focus on turbulence studies, and examines the evolution of AI, particularly through Diffusion Models rooted in non-equilibrium statistical mechanics. It underscores the significant impact of AI on advancing reduced, Lagrangian models of turbulence through innovative use of deep neural networks. Additionally, the paper reviews various other AI applications in turbulence research and outlines potential challenges and opportunities in the concurrent advancement of AI and statistical hydrodynamics. This discussion sets the stage for a future where AI and turbulence research are intricately intertwined, leading to more profound insights and advancements in both fields.
    
[^6]: 坚信忠实：在找到模型机制时超越电路重叠

    Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding Model Mechanisms

    [https://arxiv.org/abs/2403.17806](https://arxiv.org/abs/2403.17806)

    提出了一种新方法EAP-IG，旨在更好地保持电路的核心属性：忠实

    

    最近许多语言模型（LM）可解释性研究已采用电路框架，旨在找到解释LM在给定任务上行为的最小计算子图或电路。大多数研究通过独立对每个边执行因果干预来确定哪些边属于LM的电路，但这在模型规模较大时效率低下。边缘归因修补（EAP），一种基于梯度的近似干预方法，已成为解决这一问题的可扩展但不完美的解决方案。在本文中，我们介绍了一种新方法 - 带有集成梯度的EAP（EAP-IG），旨在更好地保持电路的核心属性：忠实。如果电路是忠实的，则可以去掉电路之外的所有模型边而不会改变模型在任务上的表现；忠实性是研究电路而不是完整模型的理由。我们的实验证明，使用EAP找到的电路不太忠实

    arXiv:2403.17806v1 Announce Type: cross  Abstract: Many recent language model (LM) interpretability studies have adopted the circuits framework, which aims to find the minimal computational subgraph, or circuit, that explains LM behavior on a given task. Most studies determine which edges belong in a LM's circuit by performing causal interventions on each edge independently, but this scales poorly with model size. Edge attribution patching (EAP), gradient-based approximation to interventions, has emerged as a scalable but imperfect solution to this problem. In this paper, we introduce a new method - EAP with integrated gradients (EAP-IG) - that aims to better maintain a core property of circuits: faithfulness. A circuit is faithful if all model edges outside the circuit can be ablated without changing the model's performance on the task; faithfulness is what justifies studying circuits, rather than the full model. Our experiments demonstrate that circuits found using EAP are less faith
    
[^7]: 安全聚合在面对成员推断攻击时并非私密的

    Secure Aggregation is Not Private Against Membership Inference Attacks

    [https://arxiv.org/abs/2403.17775](https://arxiv.org/abs/2403.17775)

    本文探讨了安全聚合在隐私方面的问题，揭示了其在面对成员推断攻击时并不具备足够的私密性。

    

    安全聚合（SecAgg）是联邦学习中常用的隐私增强机制，仅允许服务器访问模型更新的聚合结果，同时保护个体更新的机密性。尽管有关SecAgg保护隐私能力的广泛声明，但缺乏对其隐私性的正式分析，因此这些假设是不合理的。本文通过将SecAgg视为每个局部更新的局部差分隐私（LDP）机制，深入探讨了SecAgg的隐私影响。我们设计了一种简单攻击方式，其中对手服务器试图在SecAgg下的联邦学习的单一训练轮中推断出客户端提交的更新向量是两个可能向量中的哪一个。通过进行隐私审核，我们评估了该攻击的成功概率，并量化了SecAgg提供的LDP保证。我们的数字结果揭示了，与普遍声明相反，SecAgg并没有提供私密性。

    arXiv:2403.17775v1 Announce Type: new  Abstract: Secure aggregation (SecAgg) is a commonly-used privacy-enhancing mechanism in federated learning, affording the server access only to the aggregate of model updates while safeguarding the confidentiality of individual updates. Despite widespread claims regarding SecAgg's privacy-preserving capabilities, a formal analysis of its privacy is lacking, making such presumptions unjustified. In this paper, we delve into the privacy implications of SecAgg by treating it as a local differential privacy (LDP) mechanism for each local update. We design a simple attack wherein an adversarial server seeks to discern which update vector a client submitted, out of two possible ones, in a single training round of federated learning under SecAgg. By conducting privacy auditing, we assess the success probability of this attack and quantify the LDP guarantees provided by SecAgg. Our numerical results unveil that, contrary to prevailing claims, SecAgg offer
    
[^8]: PathoTune: 将视觉基础模型调整至病理专家

    PathoTune: Adapting Visual Foundation Model to Pathological Specialists

    [https://arxiv.org/abs/2403.16497](https://arxiv.org/abs/2403.16497)

    PathoTune提出了一个框架，能够通过多模态提示微调，将病理甚至视觉基础模型高效地调整到病理特定任务，从而缓解基础-任务差距和任务-实例差距。

    

    在自然图像理解走向预训练微调的时代的同时，病理影像的研究也在不断发展。尽管主要关注预训练病理基础模型，但如何将基础模型调整到下游任务中却鲜有研究。为了下游调整，我们提出存在两个域差距，即基础-任务差距和任务-实例差距。为了缓解这些差距，我们引入了 PathoTune，这是一个旨在通过多模态提示微调，高效地将病理甚至视觉基础模型调整到病理特定任务的框架。所提出的框架利用任务特定的视觉提示和任务特定的文本提示来识别任务相关特征，以及实例特定的视觉提示来编码单个病理图像特征。在多个数据集上以补丁级别和WSI级别的结果表明，其性能优于单模态。

    arXiv:2403.16497v1 Announce Type: cross  Abstract: As natural image understanding moves towards the pretrain-finetune era, research in pathology imaging is concurrently evolving. Despite the predominant focus on pretraining pathological foundation models, how to adapt foundation models to downstream tasks is little explored. For downstream adaptation, we propose the existence of two domain gaps, i.e., the Foundation-Task Gap and the Task-Instance Gap. To mitigate these gaps, we introduce PathoTune, a framework designed to efficiently adapt pathological or even visual foundation models to pathology-specific tasks via multi-modal prompt tuning. The proposed framework leverages Task-specific Visual Prompts and Task-specific Textual Prompts to identify task-relevant features, along with Instance-specific Visual Prompts for encoding single pathological image features. Results across multiple datasets at both patch-level and WSI-level demonstrate its superior performance over single-modality
    
[^9]: 消费者物联网流量的调查：安全与隐私

    A Survey on Consumer IoT Traffic: Security and Privacy

    [https://arxiv.org/abs/2403.16149](https://arxiv.org/abs/2403.16149)

    本调查针对消费者物联网（CIoT）流量分析从安全和隐私的角度出发，总结了CIoT流量分析的新特征、最新进展和挑战，认为通过流量分析可以揭示CIoT领域中的安全和隐私问题。

    

    在过去几年里，消费者物联网（CIoT）已经进入了公众生活。尽管CIoT提高了人们日常生活的便利性，但也带来了新的安全和隐私问题。我们尝试通过流量分析这一安全领域中的流行方法，找出研究人员可以从流量分析中了解CIoT安全和隐私方面的内容。本调查从安全和隐私角度探讨了CIoT流量分析中的新特征、CIoT流量分析的最新进展以及尚未解决的挑战。我们从2018年1月至2023年12月收集了310篇与CIoT流量分析有关的安全和隐私角度的论文，总结了识别了CIoT新特征的CIoT流量分析过程。然后，我们根据五个应用目标详细介绍了现有的研究工作：设备指纹识别、用户活动推断、恶意行为检测、隐私泄露以及通信模式识别。

    arXiv:2403.16149v1 Announce Type: cross  Abstract: For the past few years, the Consumer Internet of Things (CIoT) has entered public lives. While CIoT has improved the convenience of people's daily lives, it has also brought new security and privacy concerns. In this survey, we try to figure out what researchers can learn about the security and privacy of CIoT by traffic analysis, a popular method in the security community. From the security and privacy perspective, this survey seeks out the new characteristics in CIoT traffic analysis, the state-of-the-art progress in CIoT traffic analysis, and the challenges yet to be solved. We collected 310 papers from January 2018 to December 2023 related to CIoT traffic analysis from the security and privacy perspective and summarized the process of CIoT traffic analysis in which the new characteristics of CIoT are identified. Then, we detail existing works based on five application goals: device fingerprinting, user activity inference, malicious
    
[^10]: 棋类语言模型中的新颖世界模型和潜变量估计

    Emergent World Models and Latent Variable Estimation in Chess-Playing Language Models

    [https://arxiv.org/abs/2403.15498](https://arxiv.org/abs/2403.15498)

    棋类语言模型在没有先验知识的情况下，通过下一个字符预测训练，仍能学习出内部表示的棋盘状态

    

    语言模型展现了前所未有的能力，引发了关于其性能来源的讨论。是仅仅学习句法模式和表面统计结果，还是从文本中提取语义和世界模型？我们在象棋这个更复杂的领域扩展了之前的工作，通过在真实游戏中训练模型，使用线性探测和对比激活来研究模型的内部表示。尽管模型没有先验的游戏知识，仅仅通过下一个字符预测进行训练，我们发现了关于棋盘状态的内部表示的证据。

    arXiv:2403.15498v1 Announce Type: cross  Abstract: Language models have shown unprecedented capabilities, sparking debate over the source of their performance. Is it merely the outcome of learning syntactic patterns and surface level statistics, or do they extract semantics and a world model from the text? Prior work by Li et al. investigated this by training a GPT model on synthetic, randomly generated Othello games and found that the model learned an internal representation of the board state. We extend this work into the more complex domain of chess, training on real games and investigating our model's internal representations using linear probes and contrastive activations. The model is given no a priori knowledge of the game and is solely trained on next character prediction, yet we find evidence of internal representations of board state. We validate these internal representations by using them to make interventions on the model's activations and edit its internal board state. Un
    
[^11]: 通过记忆网络在连续检测中防止灾难性遗忘

    Preventing Catastrophic Forgetting through Memory Networks in Continuous Detection

    [https://arxiv.org/abs/2403.14797](https://arxiv.org/abs/2403.14797)

    通过引入记忆网络和局部查询函数，这项工作致力于在连续检测中防止灾难性遗忘，并解决了持续检测中的背景贬低问题。

    

    现代预训练架构在持续对新任务进行微调时很难保留先前的信息。尽管在持续分类方面取得了显著进展，但针对复杂视觉任务（如检测或分割）设计的系统仍然难以获得满意的性能。在这项工作中，我们引入了一种基于记忆的检测变压器架构，以使预训练的DETR风格检测器适应新任务，同时保留先前任务的知识。我们提出了一种新颖的局部查询函数，用于有效地从记忆单元中检索信息，旨在最小化遗忘。此外，我们确定了持续检测中一个称为背景贬低的基本挑战。当来自先前任务的对象类别在未来任务中重新出现时，可能没有标签，导致它们被隐式视为背景。这是持续检测或分割中不可避免的问题。

    arXiv:2403.14797v1 Announce Type: cross  Abstract: Modern pre-trained architectures struggle to retain previous information while undergoing continuous fine-tuning on new tasks. Despite notable progress in continual classification, systems designed for complex vision tasks such as detection or segmentation still struggle to attain satisfactory performance. In this work, we introduce a memory-based detection transformer architecture to adapt a pre-trained DETR-style detector to new tasks while preserving knowledge from previous tasks. We propose a novel localized query function for efficient information retrieval from memory units, aiming to minimize forgetting. Furthermore, we identify a fundamental challenge in continual detection referred to as background relegation. This arises when object categories from earlier tasks reappear in future tasks, potentially without labels, leading them to be implicitly treated as background. This is an inevitable issue in continual detection or segme
    
[^12]: 空间-时间图表示学习用于战术网络未来状态预测

    Spatial-Temporal Graph Representation Learning for Tactical Networks Future State Prediction

    [https://arxiv.org/abs/2403.13872](https://arxiv.org/abs/2403.13872)

    本文提出了一种空间-时间图编码器-解码器（STGED）框架，用于战术通信网络，通过有效利用网络状态的空间和时间特征，实现了对未来状态的准确预测。

    

    tbd:战术自组织网络中的资源分配存在独特挑战，因为其动态和多跳特性。在这种环境中，准确预测未来的网络连接对于有效的资源分配至关重要。本文提出了空间-时间图编码器-解码器（STGED）框架，用于战术通信网络，有效利用网络状态的空间和时间特征来学习潜在的战术行为。STGED层次地利用基于图的注意机制对一系列通信网络状态进行空间编码，利用循环神经网络对状态的演变进行时间编码，并利用全连接前馈网络来解码未来状态下的连接性。通过大量实验证明，STGED在不同时间步输入下一直比基线模型表现更出色，获得了较高的准确性。

    arXiv:2403.13872v1 Announce Type: new  Abstract: Resource allocation in tactical ad-hoc networks presents unique challenges due to their dynamic and multi-hop nature. Accurate prediction of future network connectivity is essential for effective resource allocation in such environments. In this paper, we introduce the Spatial-Temporal Graph Encoder-Decoder (STGED) framework for Tactical Communication Networks that leverages both spatial and temporal features of network states to learn latent tactical behaviors effectively. STGED hierarchically utilizes graph-based attention mechanism to spatially encode a series of communication network states, leverages a recurrent neural network to temporally encode the evolution of states, and a fully-connected feed-forward network to decode the connectivity in the future state. Through extensive experiments, we demonstrate that STGED consistently outperforms baseline models by large margins across different time-steps input, achieving an accuracy of
    
[^13]: 使用可学习提示从文本到图像生成模型中去除不良概念

    Removing Undesirable Concepts in Text-to-Image Generative Models with Learnable Prompts

    [https://arxiv.org/abs/2403.12326](https://arxiv.org/abs/2403.12326)

    通过引入可学习提示到交叉注意力模块中，本文提出了一种新方法，用于从文本到图像生成模型中去除不良概念，实现了对模型效果的提升。

    

    生成模型已经展示出在从文本描述中生成视觉上令人印象深刻的内容方面具有显著潜力。然而，在未经筛选的互联网数据上训练这些模型存在学习和随后传播不良概念（如受版权保护或不道德内容）的风险。在本文中，我们提出了一种新方法，通过将可学习提示结合到交叉注意力模块中，从文本到图像生成模型中去除不良概念。这可学习提示充当附加内存，将不良概念的知识转移到其中，并减少这些概念对模型参数和相应文本输入的依赖。由于这种知识转移到提示中，消除这些不良概念更加稳定，并对其他概念影响最小。我们在稳定扩散模型上展示了我们方法的有效性，展示了其优势。

    arXiv:2403.12326v1 Announce Type: new  Abstract: Generative models have demonstrated remarkable potential in generating visually impressive content from textual descriptions. However, training these models on unfiltered internet data poses the risk of learning and subsequently propagating undesirable concepts, such as copyrighted or unethical content. In this paper, we propose a novel method to remove undesirable concepts from text-to-image generative models by incorporating a learnable prompt into the cross-attention module. This learnable prompt acts as additional memory to transfer the knowledge of undesirable concepts into it and reduce the dependency of these concepts on the model parameters and corresponding textual inputs. Because of this knowledge transfer into the prompt, erasing these undesirable concepts is more stable and has minimal negative impact on other concepts. We demonstrate the effectiveness of our method on the Stable Diffusion model, showcasing its superiority ov
    
[^14]: SQ-LLaVA：自问自答的大型视觉-语言助手

    SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant

    [https://arxiv.org/abs/2403.11299](https://arxiv.org/abs/2403.11299)

    本研究引入了一个名为SQ-LLaVA的新颖框架，通过自我训练模型如何提出高质量问题，以改善视觉-语言模型的泛化能力。

    

    最近视觉-语言模型的发展在经过视觉指导调整后，在视觉-语言任务中展现出显着的泛化能力。然而，预训练视觉编码器和大型语言模型之间的鸿沟成为整个网络的瓶颈。为了改善跨模态对齐，现有的工作通常考虑涵盖更广泛的视觉任务范围的更多视觉指导数据，对模型进行微调以用于问答，但这种操作成本较高。然而，图像包含大量上下文信息，但这一方面一直鲜有人探索。本文首次尝试利用视觉指导数据内部被忽视的上下文，训练模型自我训练'学习'如何提出高质量问题。通过这种方式，我们引入了一个名为SQ-LLaVA的新颖框架：自问自答的大型视觉-语言助手。SQ-LLaVA在生成灵活且有意义的图像方面表现出高效性。

    arXiv:2403.11299v1 Announce Type: cross  Abstract: Recent advancements in the vision-language model have shown notable generalization in vision-language tasks after visual instruction tuning. However, bridging the gap between the pre-trained vision encoder and the large language models becomes the whole network's bottleneck. To improve cross-modality alignment, existing works usually consider more visual instruction data covering a broader range of vision tasks to fine-tune the model for question-answering, which are costly to obtain. However, the image contains rich contextual information that has been largely under-explored. This paper first attempts to harness this overlooked context within visual instruction data, training the model to self-supervised `learning' how to ask high-quality questions. In this way, we introduce a novel framework named SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant. SQ-LLaVA exhibits proficiency in generating flexible and meaningful image-
    
[^15]: 利用LLMs集成揭示社交媒体消息的潜在主题：气候运动案例研究

    Uncovering Latent Themes of Messaging on Social Media by Integrating LLMs: A Case Study on Climate Campaigns

    [https://arxiv.org/abs/2403.10707](https://arxiv.org/abs/2403.10707)

    本文提出了一种通过利用大型语言模型（LLMs）的先进功能，以机器在循环中方法，处理社交媒体消息主题的新方法。

    

    本文介绍了一种揭示和分析社交媒体消息主题的新方法。鉴于传统主题级分析的局限性，往往只捕捉到整体模式，本研究强调了对更精细、主题聚焦的探索的需求。传统的主题发现方法，涉及手动流程和人在循环中的方法，具有价值，但在伸缩性、一致性和资源强度方面面临挑战，涉及时间和成本。为了应对这些挑战，我们提出了一种利用大型语言模型（LLMs）先进功能的机器在循环中方法。这种方法允许更深入地调查社交媒体话语的主题方面，使我们能够揭示多样的主题，每个主题具有独特的特征和相关性，从而提供对更广泛主题内有的微妙细节的全面理解。

    arXiv:2403.10707v1 Announce Type: cross  Abstract: This paper introduces a novel approach to uncovering and analyzing themes in social media messaging. Recognizing the limitations of traditional topic-level analysis, which tends to capture only the overarching patterns, this study emphasizes the need for a finer-grained, theme-focused exploration. Conventional methods of theme discovery, involving manual processes and a human-in-the-loop approach, are valuable but face challenges in scalability, consistency, and resource intensity in terms of time and cost. To address these challenges, we propose a machine-in-the-loop approach that leverages the advanced capabilities of Large Language Models (LLMs). This approach allows for a deeper investigation into the thematic aspects of social media discourse, enabling us to uncover a diverse array of themes, each with unique characteristics and relevance, thereby offering a comprehensive understanding of the nuances present within broader topics.
    
[^16]: 基于去噪任务难度的渐进式课程训练扩散模型

    Denoising Task Difficulty-based Curriculum for Training Diffusion Models

    [https://arxiv.org/abs/2403.10348](https://arxiv.org/abs/2403.10348)

    研究通过全面研究任务难度，发现较早时间步长的去噪任务更具挑战性，提出了基于去噪任务难度的渐进式课程训练方法。

    

    基于扩散的生成模型已成为生成建模领域强大的工具。尽管对各个时间步长和噪声水平之间的去噪进行了广泛研究，但关于去噪任务的相对难度仍存在争议。我们的研究对任务难度进行了全面的研究，重点关注收敛行为和时间步长间连续概率分布的相对熵变化。我们的观察显示，较早时间步长的去噪存在收敛缓慢和较高的相对熵，表明在这些较低时间步长上任务难度增加。基于这些观察，我们引入了一种由易到难的学习方案，借鉴渐进式学习的思想。

    arXiv:2403.10348v1 Announce Type: cross  Abstract: Diffusion-based generative models have emerged as powerful tools in the realm of generative modeling. Despite extensive research on denoising across various timesteps and noise levels, a conflict persists regarding the relative difficulties of the denoising tasks. While various studies argue that lower timesteps present more challenging tasks, others contend that higher timesteps are more difficult. To address this conflict, our study undertakes a comprehensive examination of task difficulties, focusing on convergence behavior and changes in relative entropy between consecutive probability distributions across timesteps. Our observational study reveals that denoising at earlier timesteps poses challenges characterized by slower convergence and higher relative entropy, indicating increased task difficulty at these lower timesteps. Building on these observations, we introduce an easy-to-hard learning scheme, drawing from curriculum learn
    
[^17]: 改进医学多模态对比学习与专家注释

    Improving Medical Multi-modal Contrastive Learning with Expert Annotations

    [https://arxiv.org/abs/2403.10153](https://arxiv.org/abs/2403.10153)

    eCLIP是一种改进的CLIP模型，通过集成专家注释和混合增强来应对医学影像分析中的数据稀缺和模态差距挑战，提高了模型学习效果

    

    我们介绍了一种增强版CLIP模型——eCLIP，它集成了放射科医生眼球注视热图形式的专家注释。它解决了对比多模态医学影像分析中的关键挑战，尤其是数据稀缺和“模态差距”——图像和文本嵌入之间存在的显著差异，降低了表示的质量并阻碍了跨模态互操作性。eCLIP集成了一个热图处理器，并利用混合增强来有效利用稀缺的专家注释，从而提高模型的学习效果。eCLIP设计为通用的，适用于任何形式的CLIP变体，无需修改核心架构。通过对多个任务的详细评估，包括零样本推断、线性探针、跨模态检索以及使用冻结的大型语言模型进行放射学报告的检索增强生成（RAG），eCLIP展示了其...

    arXiv:2403.10153v1 Announce Type: cross  Abstract: We introduce eCLIP, an enhanced version of the CLIP model that integrates expert annotations in the form of radiologist eye-gaze heatmaps. It tackles key challenges in contrastive multi-modal medical imaging analysis, notably data scarcity and the "modality gap" -- a significant disparity between image and text embeddings that diminishes the quality of representations and hampers cross-modal interoperability. eCLIP integrates a heatmap processor and leverages mixup augmentation to efficiently utilize the scarce expert annotations, thus boosting the model's learning effectiveness. eCLIP is designed to be generally applicable to any variant of CLIP without requiring any modifications of the core architecture. Through detailed evaluations across several tasks, including zero-shot inference, linear probing, cross-modal retrieval, and Retrieval Augmented Generation (RAG) of radiology reports using a frozen Large Language Model, eCLIP showca
    
[^18]: SMART: 用于指令调整的子模块数据混合策略

    SMART: Submodular Data Mixture Strategy for Instruction Tuning

    [https://arxiv.org/abs/2403.08370](https://arxiv.org/abs/2403.08370)

    SMART引入了一种新颖的数据混合策略，利用子模块函数为任务分配重要性分数，并在微调中重新分配预算，从而在指令调整任务中取得明显优势。

    

    指令调整涉及在一组以指令格式化的数据集上对语言模型进行微调，以增强模型对未见任务的泛化能力。研究表明，在微调过程中平衡不同任务比例的重要性，但找到合适的平衡仍然具有挑战性。目前除了手动调整或依赖从业者的直觉外，尚无系统方法。在本文中，我们介绍了SMART（Submodular data Mixture strAtegy for instRuction Tuning）- 一种利用子模块函数为任务分配重要性分数的新颖数据混合策略，然后用这些分数来确定混合权重。给定微调预算，SMART重新分配任务间的预算，并从每个任务中选择非冗余样本。实验结果表明，SMART显著优于传统方法，如例子比例混合和均等分配。

    arXiv:2403.08370v1 Announce Type: cross  Abstract: Instruction Tuning involves finetuning a language model on a collection of instruction-formatted datasets in order to enhance the generalizability of the model to unseen tasks. Studies have shown the importance of balancing different task proportions during finetuning, but finding the right balance remains challenging. Unfortunately, there's currently no systematic method beyond manual tuning or relying on practitioners' intuition. In this paper, we introduce SMART (Submodular data Mixture strAtegy for instRuction Tuning) - a novel data mixture strategy which makes use of a submodular function to assign importance scores to tasks which are then used to determine the mixture weights. Given a fine-tuning budget, SMART redistributes the budget among tasks and selects non-redundant samples from each task. Experimental results demonstrate that SMART significantly outperforms traditional methods such as examples proportional mixing and equal
    
[^19]: 用高更新比例剖析深度强化学习：应对价值高估和发散

    Dissecting Deep RL with High Update Ratios: Combatting Value Overestimation and Divergence

    [https://arxiv.org/abs/2403.05996](https://arxiv.org/abs/2403.05996)

    本研究剖析了深度强化学习中的首要偏差现象，发现在大量更新比例下，价值高估是导致学习失败的根本挑战。

    

    我们展示了深度强化学习在设置中可以在梯度更新次数大大超过环境样本数量的情况下保持学习能力，而无需重置网络参数。在这种大量更新与数据比例的情况下，尼基辛等人 (2022) 的最近一项研究指出了一个首要偏差的出现，即代理在早期交互中过拟合并淡化后续经验，从而损害了其学习能力。在这项工作中，我们深入解析了导致首要偏差的现象。我们检查了应该导致学习失败的训练早期阶段，并发现一个根本性挑战是长期以来存在的问题：价值高估。我们发现Q值不仅在分布外数据上被高估，而且在分布内数据上也是如此，可以追溯到由优化器动量推动的未见的动作预测。我们采用了一种简单的单位球归一化方法，可以在大更新比例下实现学习。

    arXiv:2403.05996v1 Announce Type: cross  Abstract: We show that deep reinforcement learning can maintain its ability to learn without resetting network parameters in settings where the number of gradient updates greatly exceeds the number of environment samples. Under such large update-to-data ratios, a recent study by Nikishin et al. (2022) suggested the emergence of a primacy bias, in which agents overfit early interactions and downplay later experience, impairing their ability to learn. In this work, we dissect the phenomena underlying the primacy bias. We inspect the early stages of training that ought to cause the failure to learn and find that a fundamental challenge is a long-standing acquaintance: value overestimation. Overinflated Q-values are found not only on out-of-distribution but also in-distribution data and can be traced to unseen action prediction propelled by optimizer momentum. We employ a simple unit-ball normalization that enables learning under large update ratios
    
[^20]: 使用有条件可逆神经网络优化视网膜假体刺激

    Optimizing Retinal Prosthetic Stimuli with Conditional Invertible Neural Networks

    [https://arxiv.org/abs/2403.04884](https://arxiv.org/abs/2403.04884)

    利用有条件可逆神经网络无监督优化视网膜假体刺激，提高了电极阵列的刺激效果。

    

    可植入的视网膜假体为通过绕过视网膜中损坏的光感受细胞并直接刺激剩余功能性视网膜细胞来恢复部分视力提供了一个有希望的解决方案。然而，摄像头和视网膜细胞之间的信息传输通常受限于电极阵列的低分辨率和对不同节细胞类型的特异性不足，导致刺激效果不佳。在这项工作中，我们提出利用基于归一化流的有条件可逆神经网络以无监督的方式优化视网膜假体刺激。这些网络的可逆性使我们能够将它们用作视觉系统的计算模型的替代，并将输入摄像头信号编码为电极阵列上优化的电刺激。与其他方法相比，如简单的降采样、线性模型和前馈卷积神经网络

    arXiv:2403.04884v1 Announce Type: cross  Abstract: Implantable retinal prostheses offer a promising solution to restore partial vision by circumventing damaged photoreceptor cells in the retina and directly stimulating the remaining functional retinal cells. However, the information transmission between the camera and retinal cells is often limited by the low resolution of the electrode array and the lack of specificity for different ganglion cell types, resulting in suboptimal stimulations. In this work, we propose to utilize normalizing flow-based conditional invertible neural networks to optimize retinal implant stimulation in an unsupervised manner. The invertibility of these networks allows us to use them as a surrogate for the computational model of the visual system, while also encoding input camera signals into optimized electrical stimuli on the electrode array. Compared to other methods, such as trivial downsampling, linear models, and feed-forward convolutional neural networ
    
[^21]: Android在动物园中: GUI代理的动作思维链

    Android in the Zoo: Chain-of-Action-Thought for GUI Agents

    [https://arxiv.org/abs/2403.02713](https://arxiv.org/abs/2403.02713)

    该研究提出了一个名为CoAT的Chain-of-Action-Thought模型，通过考虑先前动作描述、当前屏幕情况以及未来动作思考，显著提高了智能手机GUI代理的任务执行效果。

    

    大型语言模型（LLM）导致智能手机上的大量自主GUI代理激增，这些代理通过预测API的一系列动作来完成由自然语言触发的任务。尽管该任务高度依赖于过去的动作和视觉观察，但现有研究通常很少考虑中间截图和屏幕操作传递的语义信息。为了解决这一问题，本文提出了动作思维链（CoAT），它考虑了先前动作的描述、当前屏幕，更重要的是分析应当执行的动作以及选择的动作带来的结果。我们证明，在使用现成LLM进行零次学习的情况下，CoAT相比于标准上下文建模显著提高了目标的完成情况。为了进一步促进这一研究领域的发展，我们构建了一个名为Android-In-The-Zoo（AitZ）的基准测试集，其中包含18,643个屏幕动作对。

    arXiv:2403.02713v1 Announce Type: new  Abstract: Large language model (LLM) leads to a surge of autonomous GUI agents for smartphone, which completes a task triggered by natural language through predicting a sequence of actions of API. Even though the task highly relies on past actions and visual observations, existing studies typical consider little semantic information carried out by intermediate screenshots and screen operations. To address this, this work presents Chain-of-Action-Thought (dubbed CoAT), which takes the description of the previous actions, the current screen, and more importantly the action thinking of what actions should be performed and the outcomes led by the chosen action. We demonstrate that, in a zero-shot setting upon an off-the-shell LLM, CoAT significantly improves the goal progress compared to standard context modeling. To further facilitate the research in this line, we construct a benchmark Android-In-The-Zoo (AitZ), which contains 18,643 screen-action pa
    
[^22]: 通过语法增强改进LLM代码生成

    Improving LLM Code Generation with Grammar Augmentation

    [https://arxiv.org/abs/2403.01632](https://arxiv.org/abs/2403.01632)

    SynCode是一个新框架，结合编程语言的语法和DFA mask store，在LLMs中生成代码过程中获得96.07%的句法错误降低，并展现出提高句法精度的重大影响。

    

    我们提出了 SynCode，一个用于高效和通用地解码大型语言模型（LLMs）代码的新框架。SynCode利用编程语言的语法，利用离线构建的基于语言语法终结符的高效查找表DFA mask store。我们展示了SynCode在给定编程语言的上下文无关文法（CFG）的完备性和正确性，展示其在保留语义上有效令牌的同时拒绝无效令牌的能力。该框架与由CFG定义的任何语言无缝集成，验证了针对Python和Go的CFG实验。结果突出了当SynCode与最先进的LLMs结合时，语法错误减少96.07%，彰显了其对提高代码生成中的句法精度的重大影响。

    arXiv:2403.01632v1 Announce Type: new  Abstract: We present SynCode a novel framework for efficient and general syntactical decoding of code with large language models (LLMs). SynCode leverages the grammar of a programming language, utilizing an offline-constructed efficient lookup table called DFA mask store based on language grammar terminals. We demonstrate SynCode's soundness and completeness given the context-free grammar (CFG) of the programming language, presenting its ability to retain syntactically valid tokens while rejecting invalid ones. The framework seamlessly integrates with any language defined by CFG, as evidenced by experiments on CFGs for Python and Go. The results underscore the significant reduction of 96.07% of syntax errors achieved when SynCode is combined with state-of-the-art LLMs, showcasing its substantial impact on enhancing syntactical precision in code generation.   Our code is available at https://github.com/uiuc-focal-lab/syncode.
    
[^23]: Pandora's White-Box：开放LLMs中训练数据泄漏的增加

    Pandora's White-Box: Increased Training Data Leakage in Open LLMs

    [https://arxiv.org/abs/2402.17012](https://arxiv.org/abs/2402.17012)

    本文对开源大型语言模型（LLMs）进行了隐私攻击研究，提出了首个能同时实现高真正率和低误分类率的预训练LLMs会员推理攻击（MIAs），以及展示了在自然环境中可以从微调LLM中提取超过50%的微调数据集。

    

    在本文中，我们对开源的大型语言模型（LLMs）遭受的隐私攻击进行了系统研究，其中对手可以访问模型权重、梯度或损失，试图利用它们来了解底层训练数据。我们的主要结果是针对预训练LLMs的第一个会员推理攻击（MIAs），能够同时实现高TPR和低FPR，并展示了在自然环境中可以从微调LLM中提取超过50%的微调数据集。我们考虑了对底层模型的不同访问程度、语言模型的定制化以及攻击者可以使用的资源。在预训练设置中，我们提出了三种新的白盒MIAs：基于梯度范数的攻击、监督神经网络分类器和单步损失比攻击。所有这些都优于现有的黑盒基线，并且我们的.....

    arXiv:2402.17012v1 Announce Type: cross  Abstract: In this paper we undertake a systematic study of privacy attacks against open source Large Language Models (LLMs), where an adversary has access to either the model weights, gradients, or losses, and tries to exploit them to learn something about the underlying training data. Our headline results are the first membership inference attacks (MIAs) against pre-trained LLMs that are able to simultaneously achieve high TPRs and low FPRs, and a pipeline showing that over $50\%$ (!) of the fine-tuning dataset can be extracted from a fine-tuned LLM in natural settings. We consider varying degrees of access to the underlying model, customization of the language model, and resources available to the attacker. In the pre-trained setting, we propose three new white-box MIAs: an attack based on the gradient norm, a supervised neural network classifier, and a single step loss ratio attack. All outperform existing black-box baselines, and our supervi
    
[^24]: ProTIP：针对文本到图像扩散模型抗随机扰动的概率鲁棒性验证

    ProTIP: Probabilistic Robustness Verification on Text-to-Image Diffusion Models against Stochastic Perturbation

    [https://arxiv.org/abs/2402.15429](https://arxiv.org/abs/2402.15429)

    本研究引入了概率概念的文本到图像扩散模型鲁棒性，并建立了一个名为ProTIP的高效框架用于评估其统计保证，解决了生成过程的高计算成本和对抗性样本判断困难的问题

    

    文本到图像（T2I）扩散模型（DMs）展现了在简单文本描述基础上生成高质量图像的印象能力。然而，与许多深度学习（DL）模型一样，DMs存在缺乏鲁棒性的问题。在评估T2I DMs的鲁棒性时，存在以二元或最坏情况问题解方面的尝试，但无法回答模型在存在对抗性样本（AE）时的总体鲁棒性如何。本研究首先引入了T2I DMs鲁棒性的概率概念；然后建立了一个名为ProTIP的高效框架，用于具有统计保证的评估。主要挑战源自：i）生成过程的高计算成本；和ii）确定扰动输入是否为AE涉及比较两个输出分布，这与其他DL任务（如分类）不同，其中AE是在标签错误预测时被识别的。为解决这些挑战，

    arXiv:2402.15429v1 Announce Type: cross  Abstract: Text-to-Image (T2I) Diffusion Models (DMs) have shown impressive abilities in generating high-quality images based on simple text descriptions. However, as is common with many Deep Learning (DL) models, DMs are subject to a lack of robustness. While there are attempts to evaluate the robustness of T2I DMs as a binary or worst-case problem, they cannot answer how robust in general the model is whenever an adversarial example (AE) can be found. In this study, we first introduce a probabilistic notion of T2I DMs' robustness; and then establish an efficient framework, ProTIP, to evaluate it with statistical guarantees. The main challenges stem from: i) the high computational cost of the generation process; and ii) determining if a perturbed input is an AE involves comparing two output distributions, which is fundamentally harder compared to other DL tasks like classification where an AE is identified upon misprediction of labels. To tackle
    
[^25]: ChunkAttention: 具有前缀感知KV缓存和两阶段分区的高效自注意力

    ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition

    [https://arxiv.org/abs/2402.15220](https://arxiv.org/abs/2402.15220)

    ChunkAttention是一种前缀感知的自注意力模块，通过将键/值张量分解为较小的块并结构化到辅助前缀树中，实现了在运行时改善内存利用率的KV缓存，同时设计了两阶段分区算法以提高自注意力计算中的数据局部性。

    

    自注意力是大型语言模型（LLMs）的重要组成部分，但对于长序列来说是推理延迟的一个显著来源。在多租户LLMs服务场景中，通过利用多个LLM请求在前缀中共享系统提示的概率，可以优化自注意力的计算和内存操作成本。本文介绍了ChunkAttention，一种具有前缀感知的自注意力模块，可以在运行时检测多个请求之间匹配的提示前缀，并共享它们的键/值张量以改进KV缓存的内存利用率。这是通过将整体键/值张量分解为较小的块，并将它们结构化到辅助前缀树中来实现的。因此，在基于前缀树的KV缓存之上，我们设计了一个高效的自注意力内核，其中实现了两阶段分区算法，以改善自注意力计算中的数据局部性。

    arXiv:2402.15220v1 Announce Type: cross  Abstract: Self-attention is an essential component of large language models(LLMs) but a significant source of inference latency for long sequences. In multi-tenant LLMs serving scenarios, the compute and memory operation cost of self-attention can be optimized by using the probability that multiple LLM requests have shared system prompts in prefixes. In this paper, we introduce ChunkAttention, a prefix-aware self-attention module that can detect matching prompt prefixes across multiple requests and share their key/value tensors in memory at runtime to improve the memory utilization of KV cache. This is achieved by breaking monolithic key/value tensors into smaller chunks and structuring them into the auxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache, we design an efficient self-attention kernel, where a two-phase partition algorithm is implemented to improve the data locality during self-attention computation in the p
    
[^26]: 个体间共享脑电图时空表示的对比学习用于自然神经科学

    Contrastive Learning of Shared Spatiotemporal EEG Representations Across Individuals for Naturalistic Neuroscience

    [https://arxiv.org/abs/2402.14213](https://arxiv.org/abs/2402.14213)

    通过对比学习，利用神经网络最大化相同刺激下各个个体的EEG表示的相似性，以此实现个体间共享时空脑电图表示的学习。

    

    自然刺激诱导的神经表征揭示了人类如何对日常生活中的外围刺激做出反应。理解自然刺激处理的一般神经机制的关键在于对齐各个个体的神经活动并提取个体间的共享神经表征。本研究针对脑电图（EEG）技术，该技术以其丰富的空间和时间信息而闻名，提出了一个用于个体间共享时空脑电图表示的对比学习的通用框架（CL-SSTER）。利用对比学习的表征能力，CL-SSTER利用神经网络最大化相同刺激下各个个体的EEG表示的相似性，与不同刺激的相对应。该网络采用空间和时间卷积同时学习空间和时间模式。

    arXiv:2402.14213v1 Announce Type: cross  Abstract: Neural representations induced by naturalistic stimuli offer insights into how humans respond to peripheral stimuli in daily life. The key to understanding the general neural mechanisms underlying naturalistic stimuli processing involves aligning neural activities across individuals and extracting inter-subject shared neural representations. Targeting the Electroencephalogram (EEG) technique, known for its rich spatial and temporal information, this study presents a general framework for Contrastive Learning of Shared SpatioTemporal EEG Representations across individuals (CL-SSTER). Harnessing the representational capabilities of contrastive learning, CL-SSTER utilizes a neural network to maximize the similarity of EEG representations across individuals for identical stimuli, contrasting with those for varied stimuli. The network employed spatial and temporal convolutions to simultaneously learn the spatial and temporal patterns inhere
    
[^27]: 简单而有效的神经符号一体化迁移学习

    Simple and Effective Transfer Learning for Neuro-Symbolic Integration

    [https://arxiv.org/abs/2402.14047](https://arxiv.org/abs/2402.14047)

    提出了一种简单而有效的方法，通过在下游任务上预训练神经模型，然后通过迁移学习在相同任务上对NeSy模型进行训练，以实现神经符号一体化的改进。

    

    深度学习技术近年来取得了显著成功。然而，它们在泛化和执行推理任务方面的能力仍然是一个挑战。本文提出了一种简单而有效的方法来改善这些问题，该方法涉及在下游任务上预训练神经模型，然后通过迁移学习在相同任务上对NeSy模型进行训练，其中利用神经网络将感知映射到符号，并利用逻辑推理者预测下游任务的输出。

    arXiv:2402.14047v1 Announce Type: cross  Abstract: Deep Learning (DL) techniques have achieved remarkable successes in recent years. However, their ability to generalize and execute reasoning tasks remains a challenge. A potential solution to this issue is Neuro-Symbolic Integration (NeSy), where neural approaches are combined with symbolic reasoning. Most of these methods exploit a neural network to map perceptions to symbols and a logical reasoner to predict the output of the downstream task. These methods exhibit superior generalization capacity compared to fully neural architectures. However, they suffer from several issues, including slow convergence, learning difficulties with complex perception tasks, and convergence to local minima. This paper proposes a simple yet effective method to ameliorate these problems. The key idea involves pretraining a neural model on the downstream task. Then, a NeSy model is trained on the same task via transfer learning, where the weights of the p
    
[^28]: 使用强化学习改进比例积分控制器在节流阀基准上的应用

    Improving a Proportional Integral Controller with Reinforcement Learning on a Throttle Valve Benchmark

    [https://arxiv.org/abs/2402.13654](https://arxiv.org/abs/2402.13654)

    通过引入强化学习与引导，结合比例积分（PI）控制器，本文提出了一种学习基础的控制策略，用于非线性节流阀的控制，实现了一个几乎最优的控制器。

    

    本文提出了一种基于学习的控制策略，用于非线性节流阀，该节流阀具有不对称的磁滞，实现了一个几乎最优的控制器，而不需要任何关于环境的先验知识。我们首先通过精心调整的比例积分（PI）控制器开始，并利用强化学习（RL）与引导的最新进展，通过从与阀门的额外交互中学习来改进闭环行为。我们在三个不同的阀门上的各种场景中测试了所提出的控制方法，所有这些都突显了将PI和RL框架结合以提高非线性随机系统控制性能的好处。在所有实验测试案例中，结果代理的样本效率都优于传统RL代理，并且优于PI控制器。

    arXiv:2402.13654v1 Announce Type: cross  Abstract: This paper presents a learning-based control strategy for non-linear throttle valves with an asymmetric hysteresis, leading to a near-optimal controller without requiring any prior knowledge about the environment. We start with a carefully tuned Proportional Integrator (PI) controller and exploit the recent advances in Reinforcement Learning (RL) with Guides to improve the closed-loop behavior by learning from the additional interactions with the valve. We test the proposed control method in various scenarios on three different valves, all highlighting the benefits of combining both PI and RL frameworks to improve control performance in non-linear stochastic systems. In all the experimental test cases, the resulting agent has a better sample efficiency than traditional RL agents and outperforms the PI controller.
    
[^29]: 扩散回火改善概率积分器对普通微分方程参数估计的效果

    Diffusion Tempering Improves Parameter Estimation with Probabilistic Integrators for Ordinary Differential Equations

    [https://arxiv.org/abs/2402.12231](https://arxiv.org/abs/2402.12231)

    扩散回火是一种新颖的正则化技术，可改善概率数值方法在普通微分方程中的参数优化收敛性，实现对复杂动态系统中参数的可靠估计

    

    普通微分方程（ODEs）被广泛应用于描述科学中的动态系统，但确定解释实验测量结果的参数是具有挑战性的。我们提出了扩散回火这一新的正则化技术，它针对ODEs中的概率数值方法，改善了梯度优化参数估计的收敛性。通过迭代减少概率积分器的一个噪声参数，所提出的方法更可靠地收敛到真实参数。我们证明了我们的方法对于不同复杂性的动态系统是有效的，并展示它对于具有实际相关参数数量的Hodgkin-Huxley模型获得可靠的参数估计。

    arXiv:2402.12231v1 Announce Type: new  Abstract: Ordinary differential equations (ODEs) are widely used to describe dynamical systems in science, but identifying parameters that explain experimental measurements is challenging. In particular, although ODEs are differentiable and would allow for gradient-based parameter optimization, the nonlinear dynamics of ODEs often lead to many local minima and extreme sensitivity to initial conditions. We therefore propose diffusion tempering, a novel regularization technique for probabilistic numerical methods which improves convergence of gradient-based parameter optimization in ODEs. By iteratively reducing a noise parameter of the probabilistic integrator, the proposed method converges more reliably to the true parameters. We demonstrate that our method is effective for dynamical systems of different complexity and show that it obtains reliable parameter estimates for a Hodgkin-Huxley model with a practically relevant number of parameters.
    
[^30]: 避免对比学习中的特征抑制：学习以前未曾学到的内容

    Avoiding Feature Suppression in Contrastive Learning: Learning What Has Not Been Learned Before

    [https://arxiv.org/abs/2402.11816](https://arxiv.org/abs/2402.11816)

    开发了一种多阶对比学习（MCL）框架，以解决对比学习中的特征抑制问题，并确保模型学习全面的表示。

    

    自监督对比学习已经成为从未标记数据中获取高质量表示的强大方法。然而，最近在标准对比学习（如SimCLR、CLIP中）中发现了特征抑制：在单个端到端训练阶段，对比模型仅捕获对比观点之间的一部分共享信息，而忽略了其他潜在有用的信息。具有特征抑制，对比模型通常无法学习足够适用于各种下游任务的表示。为了减轻特征抑制问题并确保对比模型学习全面的表示，我们开发了一种新颖的多阶对比学习（MCL）框架。与通常会导致特征抑制的标准对比学习不同，MCL逐渐学习以前未探索过的新特征，同时保持已经学到的内容。

    arXiv:2402.11816v1 Announce Type: cross  Abstract: Self-Supervised contrastive learning has emerged as a powerful method for obtaining high-quality representations from unlabeled data. However, feature suppression has recently been identified in standard contrastive learning ($e.g.$, SimCLR, CLIP): in a single end-to-end training stage, the contrastive model captures only parts of the shared information across contrasting views, while ignore the other potentially useful information. With feature suppression, contrastive models often fail to learn sufficient representations capable for various downstream tasks. To mitigate the feature suppression problem and ensure the contrastive model to learn comprehensive representations, we develop a novel Multistage Contrastive Learning (MCL) framework. Unlike standard contrastive learning that often result in feature suppression, MCL progressively learn new features that have not been explored in the previous stage, while maintaining the well-lea
    
[^31]: 极小化优化中交替更新的基本利益

    Fundamental Benefit of Alternating Updates in Minimax Optimization

    [https://arxiv.org/abs/2402.10475](https://arxiv.org/abs/2402.10475)

    Alt-GDA算法被证明更快，提出了交替外推GDA（Alex-GDA）通用算法框架，以轮流从迭代的外推中获取梯度。

    

    Gradient Descent-Ascent（GDA）算法旨在解决极小化优化问题，其中采用下降和上升步骤，分为同时进行（Sim-GDA）或交替进行（Alt-GDA）。 Alt-GDA通常收敛更快，但两者之间的性能差距尚未在理论上得到很好的理解，尤其是在全局收敛速率方面。为了解决这种理论与实践之间的差距，我们针对强凸强凹和Lipschitz梯度目标提出了对两种算法的细粒度收敛分析。我们的Alt-GDA的新迭代复杂性上界严格小于Sim-GDA的下界；即Alt-GDA被证明更快。此外，我们提出了交替外推GDA（Alex-GDA），这是一个包含Sim-GDA和Alt-GDA的通用算法框架，其主要思想是轮流从迭代的外推中获取梯度。我们展示了Alex-GDA满足更小的迭代复杂度。

    arXiv:2402.10475v1 Announce Type: cross  Abstract: The Gradient Descent-Ascent (GDA) algorithm, designed to solve minimax optimization problems, takes the descent and ascent steps either simultaneously (Sim-GDA) or alternately (Alt-GDA). While Alt-GDA is commonly observed to converge faster, the performance gap between the two is not yet well understood theoretically, especially in terms of global convergence rates. To address this theory-practice gap, we present fine-grained convergence analyses of both algorithms for strongly-convex-strongly-concave and Lipschitz-gradient objectives. Our new iteration complexity upper bound of Alt-GDA is strictly smaller than the lower bound of Sim-GDA; i.e., Alt-GDA is provably faster. Moreover, we propose Alternating-Extrapolation GDA (Alex-GDA), a general algorithmic framework that subsumes Sim-GDA and Alt-GDA, for which the main idea is to alternately take gradients from extrapolations of the iterates. We show that Alex-GDA satisfies a smaller it
    
[^32]: 在RLHF中基于探索驱动的策略优化：关于有效数据利用的理论洞见

    Exploration-Driven Policy Optimization in RLHF: Theoretical Insights on Efficient Data Utilization

    [https://arxiv.org/abs/2402.10342](https://arxiv.org/abs/2402.10342)

    本研究提出了一个基于探索驱动策略优化的RLHF算法，通过轨迹比较反馈推断奖励函数，为解释少量人类反馈足以实现良好性能提供了理论洞见

    

    强化学习从人类反馈（RLHF）在依赖少量人类反馈的情况下取得了令人印象深刻的经验成功。然而，对于这种现象存在着有限的理论证明。此外，尽管最近的经验成功采用了基于策略的算法，但大多数最近的研究仍侧重于基于价值的算法。在这项工作中，我们考虑了基于策略优化（PO-RLHF）的RLHF算法。该算法基于流行的策略覆盖-策略梯度（PC-PG）算法，该算法假设对奖励函数有知识。在PO-RLHF中，不假设知道奖励函数，并且该算法依赖于基于轨迹的比较反馈来推断奖励函数。我们为PO-RLHF提供了低查询复杂度的性能界限，这为解释为什么少量的人类反馈可能足以在RLHF中获得良好性能提供了洞见。一个关键的创新是我们的轨迹级el

    arXiv:2402.10342v1 Announce Type: new  Abstract: Reinforcement Learning from Human Feedback (RLHF) has achieved impressive empirical successes while relying on a small amount of human feedback. However, there is limited theoretical justification for this phenomenon. Additionally, most recent studies focus on value-based algorithms despite the recent empirical successes of policy-based algorithms. In this work, we consider an RLHF algorithm based on policy optimization (PO-RLHF). The algorithm is based on the popular Policy Cover-Policy Gradient (PC-PG) algorithm, which assumes knowledge of the reward function. In PO-RLHF, knowledge of the reward function is not assumed and the algorithm relies on trajectory-based comparison feedback to infer the reward function. We provide performance bounds for PO-RLHF with low query complexity, which provides insight into why a small amount of human feedback may be sufficient to get good performance with RLHF. A key novelty is our trajectory-level el
    
[^33]: 音频恢复的扩散模型

    Diffusion Models for Audio Restoration

    [https://arxiv.org/abs/2402.09821](https://arxiv.org/abs/2402.09821)

    本文介绍了基于扩散模型的音频恢复算法，重点关注语音增强和音乐恢复任务。

    

    随着音频播放设备和快速数据传输的发展，对高音质的需求在娱乐和通信领域不断增长。然而，由于录制过程中的失真和干扰，或者由于不完善的传输管道，音频质量面临许多挑战。为了解决这个问题，音频恢复方法旨在从损坏的输入数据中恢复出清晰的音频信号。本文介绍了基于扩散模型的音频恢复算法，重点关注语音增强和音乐恢复任务。传统方法通常基于手工规则和统计启发法，从而建立了我们对音频信号的认识。近几十年来，越来越多的人转向利用深度神经网络（DNNs）的建模能力的数据驱动方法。深度生成模型中的扩散模型成为一种新兴方法。

    arXiv:2402.09821v1 Announce Type: cross  Abstract: With the development of audio playback devices and fast data transmission, the demand for high sound quality is rising, for both entertainment and communications. In this quest for better sound quality, challenges emerge from distortions and interferences originating at the recording side or caused by an imperfect transmission pipeline. To address this problem, audio restoration methods aim to recover clean sound signals from the corrupted input data. We present here audio restoration algorithms based on diffusion models, with a focus on speech enhancement and music restoration tasks. Traditional approaches, often grounded in handcrafted rules and statistical heuristics, have shaped our understanding of audio signals. In the past decades, there has been a notable shift towards data-driven methods that exploit the modeling capabilities of deep neural networks (DNNs). Deep generative models, and among them diffusion models, have emerged 
    
[^34]: 重新思考大型语言模型的机器消除技术

    Rethinking Machine Unlearning for Large Language Models

    [https://arxiv.org/abs/2402.08787](https://arxiv.org/abs/2402.08787)

    这篇论文研究了大型语言模型中的机器消除技术，旨在消除不良数据的影响并保持基本知识生成的完整性，为开发安全、可靠和资源高效的生成式人工智能提供基础。

    

    我们研究了大型语言模型（LLM）领域的机器消除技术（MU），称为LLM消除技术。这个研究旨在消除不良数据的影响（例如敏感或非法信息）以及相关模型的能力，同时保持基本的知识生成的完整性，并不影响因果无关的信息。我们设想LLM消除技术将成为LLM生命周期管理中的关键要素，可能成为开发既安全、可靠又资源高效的生成式人工智能的基础，而无需进行完全重训练。我们从概念、方法、评估指标和应用等方面探索了LLM消除技术的研究领域。特别是，我们突出了现有LLM消除技术研究中经常被忽视的方面，例如消除范围、数据模型交互和多方面的有效性评估。

    arXiv:2402.08787v1 Announce Type: cross Abstract: We explore machine unlearning (MU) in the domain of large language models (LLMs), referred to as LLM unlearning. This initiative aims to eliminate undesirable data influence (e.g., sensitive or illegal information) and the associated model capabilities, while maintaining the integrity of essential knowledge generation and not affecting causally unrelated information. We envision LLM unlearning becoming a pivotal element in the life-cycle management of LLMs, potentially standing as an essential foundation for developing generative AI that is not only safe, secure, and trustworthy, but also resource-efficient without the need of full retraining. We navigate the unlearning landscape in LLMs from conceptual formulation, methodologies, metrics, and applications. In particular, we highlight the often-overlooked aspects of existing LLM unlearning research, e.g., unlearning scope, data-model interaction, and multifaceted efficacy assessment. We
    
[^35]: 选择性学习：实现动态正则化的鲁棒校准

    Selective Learning: Towards Robust Calibration with Dynamic Regularization

    [https://arxiv.org/abs/2402.08384](https://arxiv.org/abs/2402.08384)

    本研究提出了一种名为动态正则化（DReg）的方法，通过训练学习应该学到什么，从而解决深度学习中的过拟合和误校准问题。

    

    深度学习中的误校准指的是预测的可信度与性能之间存在差异。这个问题通常是由过拟合问题引起的，过拟合问题的特点是学习训练集中呈现出的所有内容，导致在测试过程中进行过于自信的预测。现有方法通常通过在目标函数中添加最大熵正则化器来解决过拟合问题并缓解误校准问题。这个目标可以理解为寻找一个模型，通过增加可信度来适应实际标签，同时通过降低可信度来最大化预测概率的熵。然而，以前的方法缺乏对可信度调整的明确指导，导致目标冲突（增加但也降低可信度）。因此，我们引入了一种称为动态正则化（DReg）的方法，旨在通过训练学习应该学到什么，从而避免可信度调整的权衡。

    Miscalibration in deep learning refers to there is a discrepancy between the predicted confidence and performance. This problem usually arises due to the overfitting problem, which is characterized by learning everything presented in the training set, resulting in overconfident predictions during testing. Existing methods typically address overfitting and mitigate the miscalibration by adding a maximum-entropy regularizer to the objective function. The objective can be understood as seeking a model that fits the ground-truth labels by increasing the confidence while also maximizing the entropy of predicted probabilities by decreasing the confidence. However, previous methods lack clear guidance on confidence adjustment, leading to conflicting objectives (increasing but also decreasing confidence). Therefore, we introduce a method called Dynamic Regularization (DReg), which aims to learn what should be learned during training thereby circumventing the confidence adjusting trade-off. At 
    
[^36]: 思想传播：扩散语言模型中的思维链推理

    Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models

    [https://arxiv.org/abs/2402.07754](https://arxiv.org/abs/2402.07754)

    本文介绍了一种将扩散模型与思维链推理集成的方法，通过扩散传播推理步骤，提供了更大的灵活性和推理能力。实验证明了该方法在数学问题中的有效性，并展示了自我纠正能力和推理技术的潜力。

    

    扩散模型在文本处理中引起了关注，相对传统的自回归模型具有许多潜在优势。本文探讨了将扩散模型与思维链（CoT）集成的方法，CoT是一种在自回归语言模型中改进推理能力的成熟技术。我们提出了思维扩散（DoT）模型，允许推理步骤通过扩散过程在时间上传播。与传统的自回归语言模型逐个token从左到右做出决策的方式相比，DoT在计算和推理性能之间具有更大的灵活性。我们的实验证明了DoT在多位数乘法和小学数学问题中的有效性。此外，DoT展示了有希望的自我纠正能力，并从现有的增强推理技术（如自一致解码）中受益。我们的发现有助于理解和发展推理能力。

    Diffusion models have gained attention in text processing, offering many potential advantages over traditional autoregressive models. This work explores the integration of diffusion models and Chain-of-Thought (CoT), a well-established technique to improve the reasoning ability in autoregressive language models. We propose Diffusion-of-Thought (DoT), allowing reasoning steps to diffuse over time through the diffusion process. In contrast to traditional autoregressive language models that make decisions in a left-to-right, token-by-token manner, DoT offers more flexibility in the trade-off between computation and reasoning performance. Our experimental results demonstrate the effectiveness of DoT in multi-digit multiplication and grade school math problems. Additionally, DoT showcases promising self-correction abilities and benefits from existing reasoning-enhancing techniques like self-consistency decoding. Our findings contribute to the understanding and development of reasoning capab
    
[^37]: 部分随机的无限深度贝叶斯神经网络

    Partially Stochastic Infinitely Deep Bayesian Neural Networks

    [https://arxiv.org/abs/2402.03495](https://arxiv.org/abs/2402.03495)

    本文提出了一种部分随机性的无限深度贝叶斯神经网络，通过在网络框架中整合部分随机性，改善现有架构在训练和推理时间上的计算效率限制，并提供了多种灵活的网络设计配置，同时通过数学证明确保了模型的表达能力。

    

    在本文中，我们提出了一种部分随机的无限深度贝叶斯神经网络，这是一种将部分随机性整合到无限深度神经网络框架中的新型架构。我们的新型架构旨在改善现有架构在训练和推理时间上的计算效率限制。为实现这一目标，我们利用了部分随机性在无限深度极限下的优势，包括全随机性的好处，如鲁棒性、不确定性量化和内存效率，同时改善了它们在训练和推理时间上的计算效率限制。我们提出了多种架构配置，提供了网络设计的灵活性，包括不同的权重划分方法。我们还通过确立我们的网络家族符合通用条件分布近似器的数学保证，对我们的模型的表达能力进行了证明。

    In this paper, we present Partially Stochastic Infinitely Deep Bayesian Neural Networks, a novel family of architectures that integrates partial stochasticity into the framework of infinitely deep neural networks. Our new class of architectures is designed to improve the limitations of existing architectures around computational efficiency at training and inference time. To do this, we leverage the advantages of partial stochasticity in the infinite-depth limit which include the benefits of full stochasticity e.g. robustness, uncertainty quantification, and memory efficiency, whilst improving their limitations around computational efficiency at training and inference time. We present a variety of architectural configurations, offering flexibility in network design including different methods for weight partition. We also provide mathematical guarantees on the expressivity of our models by establishing that our network family qualifies as Universal Conditional Distribution Approximators
    
[^38]: SpecFormer：通过最大奇异值惩罚来保护视觉Transformer的稳健性

    SpecFormer: Guarding Vision Transformer Robustness via Maximum Singular Value Penalization

    [https://arxiv.org/abs/2402.03317](https://arxiv.org/abs/2402.03317)

    该论文介绍了SpecFormer，一种通过最大奇异值惩罚来增强视觉Transformer（ViTs）对对抗性攻击的韧性的方法。该方法通过引入局部Lipschitz边界和最大奇异值惩罚方法（MSVP），有效地降低了注意力权重矩阵的谱范数。

    

    视觉Transformer（ViTs）因其出色的性能而成为广泛使用的计算机视觉任务的首选。然而，其广泛应用引起了对面对恶意攻击时安全性的担忧。大多数现有方法依赖于训练过程中的经验调整，缺乏明确的理论基础。在本研究中，我们通过引入SpecFormer来填补这一空白，该方法专门设计用于增强ViTs对对抗性攻击的韧性，并得到了仔细推导的理论保证的支持。我们为自注意层建立了本地Lipschitz边界，并引入了一种新颖的方法，最大奇异值惩罚（MSVP），以精确控制这些边界。我们使用幂迭代方法将MSVP无缝集成到ViTs的注意力层中，以提高计算效率。修改后的模型SpecFormer有效地降低了注意力权重矩阵的谱范数，

    Vision Transformers (ViTs) have gained prominence as a preferred choice for a wide range of computer vision tasks due to their exceptional performance. However, their widespread adoption has raised concerns about security in the face of malicious attacks. Most existing methods rely on empirical adjustments during the training process, lacking a clear theoretical foundation. In this study, we address this gap by introducing SpecFormer, specifically designed to enhance ViTs' resilience against adversarial attacks, with support from carefully derived theoretical guarantees. We establish local Lipschitz bounds for the self-attention layer and introduce a novel approach, Maximum Singular Value Penalization (MSVP), to attain precise control over these bounds. We seamlessly integrate MSVP into ViTs' attention layers, using the power iteration method for enhanced computational efficiency. The modified model, SpecFormer, effectively reduces the spectral norms of attention weight matrices, there
    
[^39]: 从PEFT到DEFT：用于减少变压器中激活密度的参数高效微调

    From PEFT to DEFT: Parameter Efficient Finetuning for Reducing Activation Density in Transformers

    [https://arxiv.org/abs/2402.01911](https://arxiv.org/abs/2402.01911)

    本论文提出了一种用于减少变压器模型中激活密度的参数高效微调方法 DEFT。研究发现预训练模型中存在激活稀疏性，并通过引入新的密度损失来促进更高的激活稀疏性。通过应用主流的PEFT技术，包括QLoRA、LoRA、Adapter、Prompt/Prefix Tuning，实验证明了该方法在不同下游任务中的有效性。

    

    预训练语言模型（PLMs）已成为下游任务微调的事实上的起点。然而，随着模型规模的增加，传统的全参数微调变得困难。为了解决这个问题，参数高效微调（PEFT）方法作为有效适应PLMs的手段而变得流行。与此同时，最近的研究揭示了变压器中多层感知（MLP）模块的中间输出中存在的激活稀疏性。低激活密度能够在支持稀疏感知硬件上实现高效模型推断。基于这一观察，我们在工作中提出了一种新的密度损失，鼓励预训练模型中更高的激活稀疏性（等价于更低的激活密度）。我们通过利用包括QLoRA、LoRA、Adapter、Prompt/Prefix Tuning在内的主流PEFT技术，展示了我们方法的有效性，以促进在多样的下游任务中实现高效的模型适应。

    Pretrained Language Models (PLMs) have become the de facto starting point for fine-tuning on downstream tasks. However, as model sizes continue to increase, traditional fine-tuning of all parameters becomes challenging. To address this, parameter-efficient fine-tuning (PEFT) methods have gained popularity as a means to adapt PLMs effectively. In parallel, recent studies have revealed the presence of activation sparsity within the intermediate outputs of the multilayer perception (MLP) blocks in transformers. Low activation density enables efficient model inference on sparsity-aware hardware. Building upon this insight, in this work, we propose a novel density loss that encourages higher activation sparsity (equivalently, lower activation density) in the pre-trained models. We demonstrate the effectiveness of our approach by utilizing mainstream PEFT techniques including QLoRA, LoRA, Adapter, Prompt/Prefix Tuning to facilitate efficient model adaptation across diverse downstream tasks. 
    
[^40]: 通过深度光谱聚类实现数据立方体分割

    Datacube segmentation via Deep Spectral Clustering

    [https://arxiv.org/abs/2401.17695](https://arxiv.org/abs/2401.17695)

    通过应用深度聚类算法对数据立方体像素的光谱属性进行无监督聚类，可以实现数据立方体的图像分割和统计解释。

    

    扩展视觉技术在物理学中无处不在。然而，由此类分析产生的数据立方体在解释上往往具有挑战，因为很难从组成数据立方体的光谱中辨别出相关信息。此外，数据立方体光谱的巨大维度对于统计解释来说是一个复杂的任务；然而，这种复杂性包含了大量的统计信息，可以以无监督的方式利用，以描绘出所研究案例的一些基本特性，例如，可以通过在适当定义的低维嵌入空间中对数据立方体光谱进行（深度）聚类来获得图像分割。为了解决这个问题，我们探索了在编码空间中应用无监督聚类方法的可能性，即对数据立方体像素的光谱属性进行深度聚类。通过专门训练的统计维度缩减器进行统计维度缩减

    Extended Vision techniques are ubiquitous in physics. However, the data cubes steaming from such analysis often pose a challenge in their interpretation, due to the intrinsic difficulty in discerning the relevant information from the spectra composing the data cube.   Furthermore, the huge dimensionality of data cube spectra poses a complex task in its statistical interpretation; nevertheless, this complexity contains a massive amount of statistical information that can be exploited in an unsupervised manner to outline some essential properties of the case study at hand, e.g.~it is possible to obtain an image segmentation via (deep) clustering of data-cube's spectra, performed in a suitably defined low-dimensional embedding space.   To tackle this topic, we explore the possibility of applying unsupervised clustering methods in encoded space, i.e. perform deep clustering on the spectral properties of datacube pixels. A statistical dimensional reduction is performed by an ad hoc trained 
    
[^41]: 在离线动力学强化学习中的少样本转移的保守方法

    A Conservative Approach for Few-Shot Transfer in Off-Dynamics Reinforcement Learning

    [https://arxiv.org/abs/2312.15474](https://arxiv.org/abs/2312.15474)

    提出了一种受最近模仿学习和保守RL算法进展启发的创新方法，在离线动力学强化学习中的少样本转移过程中引入惩罚来调节源训练策略生成的轨迹。

    

    离线动力学强化学习（ODRL）旨在将策略从源环境转移到具有不同但相似动力学特征的目标环境。在这种情况下，传统RL代理过度依赖源环境的动力学，导致发现在该环境中表现卓越的策略，但在目标环境中表现不佳。在少样本框架中，引入了来自目标环境的有限数量转换以促进更有效的转移。为了解决这一挑战，我们提出了一种受最近模仿学习和保守RL算法进展启发的创新方法。所提出的方法引入了一个惩罚来调节源训练策略生成的轨迹。我们在代表不同离线动力学条件的各种环境中评估了我们的方法，在这些环境中访问目标环境是极端困难的。

    arXiv:2312.15474v2 Announce Type: replace  Abstract: Off-dynamics Reinforcement Learning (ODRL) seeks to transfer a policy from a source environment to a target environment characterized by distinct yet similar dynamics. In this context, traditional RL agents depend excessively on the dynamics of the source environment, resulting in the discovery of policies that excel in this environment but fail to provide reasonable performance in the target one. In the few-shot framework, a limited number of transitions from the target environment are introduced to facilitate a more effective transfer. Addressing this challenge, we propose an innovative approach inspired by recent advancements in Imitation Learning and conservative RL algorithms. The proposed method introduces a penalty to regulate the trajectories generated by the source-trained policy. We evaluate our method across various environments representing diverse off-dynamics conditions, where access to the target environment is extreme
    
[^42]: 视觉-语言模型作为奖励的来源

    Vision-Language Models as a Source of Rewards

    [https://arxiv.org/abs/2312.09187](https://arxiv.org/abs/2312.09187)

    使用现成的视觉-语言模型作为强化学习代理的奖励来源，展示了如何通过CLIP系列模型派生视觉目标实现的奖励，从而训练出能够实现多种语言目标的RL代理。

    

    建立可以在丰富多样的开放环境中实现许多目标的通用代理是强化学习的研究前沿之一。建立具有RL的通用代理的关键限制因素之一是需要大量的奖励函数来实现不同的目标。我们调查了使用现成的视觉-语言模型（VLM）作为强化学习代理的奖励来源的可行性。我们展示了如何从CLIP系列模型中派生视觉实现各种语言目标的奖励，并用于训练能够实现各种语言目标的RL代理。我们展示了这种方法在两个不同的视觉领域中，并呈现了一个规模化趋势，显示更大的VLM会产生更准确的视觉目标实现奖励，从而产生更有能力的RL代理。

    arXiv:2312.09187v2 Announce Type: replace  Abstract: Building generalist agents that can accomplish many goals in rich open-ended environments is one of the research frontiers for reinforcement learning. A key limiting factor for building generalist agents with RL has been the need for a large number of reward functions for achieving different goals. We investigate the feasibility of using off-the-shelf vision-language models, or VLMs, as sources of rewards for reinforcement learning agents. We show how rewards for visual achievement of a variety of language goals can be derived from the CLIP family of models, and used to train RL agents that can achieve a variety of language goals. We showcase this approach in two distinct visual domains and present a scaling trend showing how larger VLMs lead to more accurate rewards for visual goal achievement, which in turn produces more capable RL agents.
    
[^43]: SparQ注意力：高效带宽的LLM推理

    SparQ Attention: Bandwidth-Efficient LLM Inference

    [https://arxiv.org/abs/2312.04985](https://arxiv.org/abs/2312.04985)

    SparQ Attention通过减少注意力块内存带宽需求的技术，从而增加LLMs推理的吞吐量，同时保持模型准确性。

    

    生成式大语言模型（LLMs）开创了许多新可能性，但由于其巨大的计算需求，它们的普遍使用仍然具有挑战性。我们引入了SparQ注意力，一种通过选择性获取缓存历史来减少注意力块内存带宽需求的技术，从而增加了LLMs的推理吞吐量。

    arXiv:2312.04985v2 Announce Type: replace  Abstract: Generative large language models (LLMs) have opened up numerous novel possibilities, but due to their significant computational requirements their ubiquitous use remains challenging. Some of the most useful applications require processing large numbers of samples at a time and using long contexts, both significantly increasing the memory communication load of the models. We introduce SparQ Attention, a technique for increasing the inference throughput of LLMs by reducing the memory bandwidth requirements within the attention blocks through selective fetching of the cached history. Our proposed technique can be applied directly to off-the-shelf LLMs during inference, without requiring any modification to the pre-training setup or additional fine-tuning. We show how SparQ Attention can decrease the attention memory bandwidth requirements up to eight times without any loss in accuracy by evaluating Llama 2 and Pythia models on a wide ra
    
[^44]: 处理张量奇异值分解中的非光滑挑战：多目标张量恢复框架

    Handling The Non-Smooth Challenge in Tensor SVD: A Multi-Objective Tensor Recovery Framework

    [https://arxiv.org/abs/2311.13958](https://arxiv.org/abs/2311.13958)

    提出一种具有可学习张量核范数的新型张量恢复模型，引入交替近端乘子方法（APMM）优化算法，解决处理非光滑变化的张量数据挑战

    

    最近，许多基于张量奇异值分解（t-SVD）的张量恢复方法在处理视觉数据（如彩色图像和视频）方面表现出潜力。然而，当面对显示出非光滑变化的张量数据时，这些方法通常会遭受严重的性能退化。虽然在现实世界中经常观察到这种情况，但传统的基于t-SVD的方法却忽视了这一点。在这项工作中，我们引入了一种新颖的张量恢复模型，其中包括可学习的张量核范数，以解决这一挑战。我们开发了一种名为交替近端乘子方法（APMM）的新优化算法，以迭代地解决提出的张量补全模型。理论分析证明了所提出的APMM收敛到优化问题的Karush-Kuhn-Tucker（KKT）点。此外，我们基于APMM提出了一个多目标张量恢复框架，以有效探索协

    arXiv:2311.13958v2 Announce Type: replace-cross  Abstract: Recently, numerous tensor singular value decomposition (t-SVD)-based tensor recovery methods have shown promise in processing visual data, such as color images and videos. However, these methods often suffer from severe performance degradation when confronted with tensor data exhibiting non-smooth changes. It has been commonly observed in real-world scenarios but ignored by the traditional t-SVD-based methods. In this work, we introduce a novel tensor recovery model with a learnable tensor nuclear norm to address such a challenge. We develop a new optimization algorithm named the Alternating Proximal Multiplier Method (APMM) to iteratively solve the proposed tensor completion model. Theoretical analysis demonstrates the convergence of the proposed APMM to the Karush-Kuhn-Tucker (KKT) point of the optimization problem. In addition, we propose a multi-objective tensor recovery framework based on APMM to efficiently explore the co
    
[^45]: 大型语言模型中的偏见与公平性：一项调查

    Bias and Fairness in Large Language Models: A Survey

    [https://arxiv.org/abs/2309.00770](https://arxiv.org/abs/2309.00770)

    该论文在大型语言模型领域提出了偏见评估和缓解技术的综合调查，定义了公平性的不同方面，并提出了三个分类体系，以协助研究人员对LLMs进行公平性分析和改进。

    

    大型语言模型（LLMs）的快速发展使得人们能够处理、理解和生成类似人类文本，逐渐融入触及我们社交领域的系统。然而，尽管取得成功，这些模型可能学习、延续和放大有害的社会偏见。本文对LLMs的偏见评估和缓解技术进行了全面调查。我们首先整合、形式化和扩展自然语言处理中社会偏见和公平性的概念，定义了伤害的不同方面，并引入了几个实现LLMs公平性的必要条件。然后，我们通过提出三个直观的分类体系统一了文献，其中包括两个用于偏见评估的分类体系，即指标和数据集，以及一个用于缓解的分类体系。

    arXiv:2309.00770v2 Announce Type: replace-cross  Abstract: Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this paper, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which t
    
[^46]: 对深度学习的不确定性量化进行调查：从不确定性来源的角度分析

    A Survey on Uncertainty Quantification for Deep Learning: An Uncertainty Source Perspective

    [https://arxiv.org/abs/2302.13425](https://arxiv.org/abs/2302.13425)

    本研究对深度学习的不确定性量化进行了调查，从不确定性来源的角度分析不同方法，以评估DNN预测的置信度。

    

    深度神经网络(DNNs)在计算机视觉、自然语言处理以及科学与工程领域取得了巨大成功。然而，人们也认识到DNNs有时会做出意外、错误但过于自信的预测。这可能导致在自动驾驶、医学诊断和灾难响应等高风险应用中出现严重后果。不确定性量化（UQ）旨在估计DNN预测的置信度，超越预测准确性。近年来，已经开发了许多针对DNNs的UQ方法。系统地对这些UQ方法进行分类并比较它们的优势和劣势具有极大的实际价值。然而，现有调查大多集中在从神经网络架构角度或贝叶斯角度对UQ方法进行分类，忽略了每种方法可能引入的不确定性来源。

    arXiv:2302.13425v3 Announce Type: replace  Abstract: Deep neural networks (DNNs) have achieved tremendous success in making accurate predictions for computer vision, natural language processing, as well as science and engineering domains. However, it is also well-recognized that DNNs sometimes make unexpected, incorrect, but overconfident predictions. This can cause serious consequences in high-stake applications, such as autonomous driving, medical diagnosis, and disaster response. Uncertainty quantification (UQ) aims to estimate the confidence of DNN predictions beyond prediction accuracy. In recent years, many UQ methods have been developed for DNNs. It is of great practical value to systematically categorize these UQ methods and compare their advantages and disadvantages. However, existing surveys mostly focus on categorizing UQ methodologies from a neural network architecture perspective or a Bayesian perspective and ignore the source of uncertainty that each methodology can incor
    
[^47]: 参数矩阵模型

    Parametric Matrix Models. (arXiv:2401.11694v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.11694](http://arxiv.org/abs/2401.11694)

    参数矩阵模型是一种通用机器学习算法，基于矩阵方程设计，通过简化基础方法进行近似解参数方程。它可以仅使用经验数据进行训练，适用于各种机器学习问题，并在计算框架内产生准确的结果。

    

    我们提出了一种称为参数矩阵模型的通用机器学习算法。参数矩阵模型基于矩阵方程，并且其设计受到了用于近似解参数方程的简化基础方法的效率启发。依赖变量可以隐式或显式定义，并且方程可以使用代数、微分或积分关系。参数矩阵模型可以仅使用经验数据进行训练，不需要高保真度模型计算。虽然最初设计用于科学计算，但参数矩阵模型是一种可以应用于通用机器学习问题的通用函数逼近器。在介绍基础理论之后，我们将参数矩阵模型应用于一系列不同的挑战，展示了它们在各种问题上的性能。对于所有在这里测试的挑战，参数矩阵模型在允许计算的框架内产生准确的结果。

    We present a general class of machine learning algorithms called parametric matrix models. Parametric matrix models are based on matrix equations, and the design is motivated by the efficiency of reduced basis methods for approximating solutions of parametric equations. The dependent variables can be defined implicitly or explicitly, and the equations may use algebraic, differential, or integral relations. Parametric matrix models can be trained with empirical data only, and no high-fidelity model calculations are needed. While originally designed for scientific computing, parametric matrix models are universal function approximators that can be applied to general machine learning problems. After introducing the underlying theory, we apply parametric matrix models to a series of different challenges that show their performance for a wide range of problems. For all the challenges tested here, parametric matrix models produce accurate results within a computational framework that allows 
    
[^48]: 面向对象的扩散技术实现高效视频编辑

    Object-Centric Diffusion for Efficient Video Editing. (arXiv:2401.05735v1 [cs.CV])

    [http://arxiv.org/abs/2401.05735](http://arxiv.org/abs/2401.05735)

    本论文提出了一种面向对象的扩散技术，通过分配更多的计算资源给前景编辑区域来实现视频编辑的高效率，从而大大提高了速度，同时保持了质量。

    

    基于扩散的视频编辑已经达到了令人印象深刻的质量，并且可以根据编辑提示来转换视频的全局风格、局部结构和属性。然而，这些解决方案通常需要使用大量的内存和计算资源来生成具有时序一致性的帧，可能涉及扩散反演和/或跨帧注意力。在本文中，我们对这种低效性进行了分析，并提出了简单而有效的修改，可以显著提高速度同时保持质量。此外，我们引入了面向对象的扩散技术（OCD），通过将计算资源更多地分配给对感知质量更重要的前景编辑区域，进一步降低延迟。我们通过两个新的提案来实现这一点：i）面向对象的采样，将用于显著区域或背景的扩散步骤与用于前景的扩散步骤分离开来，将大部分模型容量分配给前者；ii）面向对象的3D令牌合并，用于改善前景和背景之间的混合。

    Diffusion-based video editing have reached impressive quality and can transform either the global style, local structure, and attributes of given video inputs, following textual edit prompts. However, such solutions typically incur heavy memory and computational costs to generate temporally-coherent frames, either in the form of diffusion inversion and/or cross-frame attention. In this paper, we conduct an analysis of such inefficiencies, and suggest simple yet effective modifications that allow significant speed-ups whilst maintaining quality. Moreover, we introduce Object-Centric Diffusion, coined as OCD, to further reduce latency by allocating computations more towards foreground edited regions that are arguably more important for perceptual quality. We achieve this by two novel proposals: i) Object-Centric Sampling, decoupling the diffusion steps spent on salient regions or background, allocating most of the model capacity to the former, and ii) Object-Centric 3D Token Merging, whi
    
[^49]: 使用扩散映射进行稳定的生成建模

    Stable generative modeling using diffusion maps. (arXiv:2401.04372v1 [stat.ML])

    [http://arxiv.org/abs/2401.04372](http://arxiv.org/abs/2401.04372)

    本文提出了一种稳定的生成建模方法，通过将扩散映射与朗之万动力学相结合，在仅有有限数量的训练样本的情况下生成新样本，并解决了时间步长僵硬随机微分方程中的稳定性问题。

    

    我们考虑从仅有足够数量的训练样本可得到的未知分布中抽样的问题。在生成建模的背景下，这样的设置最近引起了相当大的关注。本文中，我们提出了一种将扩散映射和朗之万动力学相结合的生成模型。扩散映射用于从可用的训练样本中近似得到漂移项，然后在离散时间的朗之万采样器中实现生成新样本。通过将核带宽设置为与未调整的朗之万算法中使用的时间步长匹配，我们的方法可以有效地避免通常与时间步长僵硬随机微分方程相关的稳定性问题。更准确地说，我们引入了一种新颖的分裂步骤方案，确保生成的样本保持在训练样本的凸包内。我们的框架可以自然地扩展为生成条件样本。我们展示了性能。

    We consider the problem of sampling from an unknown distribution for which only a sufficiently large number of training samples are available. Such settings have recently drawn considerable interest in the context of generative modelling. In this paper, we propose a generative model combining diffusion maps and Langevin dynamics. Diffusion maps are used to approximate the drift term from the available training samples, which is then implemented in a discrete-time Langevin sampler to generate new samples. By setting the kernel bandwidth to match the time step size used in the unadjusted Langevin algorithm, our method effectively circumvents any stability issues typically associated with time-stepping stiff stochastic differential equations. More precisely, we introduce a novel split-step scheme, ensuring that the generated samples remain within the convex hull of the training samples. Our framework can be naturally extended to generate conditional samples. We demonstrate the performance
    
[^50]: 多智能体强化学习的上下文感知通信

    Context-aware Communication for Multi-agent Reinforcement Learning. (arXiv:2312.15600v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.15600](http://arxiv.org/abs/2312.15600)

    这项研究针对多智能体强化学习提出了一种上下文感知的通信方案，通过两个阶段的交流，使智能体能够发送个性化的消息，从而提高合作和团队性能。

    

    对于多智能体强化学习（MARL），有效的通信协议对于促进合作和提高团队性能至关重要。为了利用通信，许多以前的工作提出将本地信息压缩成一条消息并广播给所有可达的智能体。然而，这种简单的消息传递机制可能无法为个体智能体提供足够、关键和相关的信息，特别是在带宽严重有限的场景下。这激励我们为MARL开发上下文感知的通信方案，旨在向不同的智能体发送个性化的消息。我们的通信协议名为CACOM，由两个阶段组成。第一个阶段中，智能体以广播方式交换粗略表示，为第二个阶段提供上下文信息。紧随其后，智能体在第二个阶段中利用注意机制为接收者选择性生成个性化的消息。此外，我们还采用了学习的步长量化方法。

    Effective communication protocols in multi-agent reinforcement learning (MARL) are critical to fostering cooperation and enhancing team performance. To leverage communication, many previous works have proposed to compress local information into a single message and broadcast it to all reachable agents. This simplistic messaging mechanism, however, may fail to provide adequate, critical, and relevant information to individual agents, especially in severely bandwidth-limited scenarios. This motivates us to develop context-aware communication schemes for MARL, aiming to deliver personalized messages to different agents. Our communication protocol, named CACOM, consists of two stages. In the first stage, agents exchange coarse representations in a broadcast fashion, providing context for the second stage. Following this, agents utilize attention mechanisms in the second stage to selectively generate messages personalized for the receivers. Furthermore, we employ the learned step size quant
    
[^51]: 学会说母语：以母语风格激发大型语言模型的能力

    Speak Like a Native: Prompting Large Language Models in a Native Style. (arXiv:2311.13538v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2311.13538](http://arxiv.org/abs/2311.13538)

    本文提出了一种名为AlignedCoT的新颖有效方法，通过将上下文示例与大型语言模型（LLMs）的母语风格对齐，提高了LLMs的推理能力和性能。

    

    大型语言模型（LLMs）与上下文学习（ICL）已成为许多自然语言处理任务的现代工具选择。然而，上下文示例的文本风格如何影响LLMs的性能仍然不足。本文提出了一种名为AlignedCoT的新颖有效的方法，通过将上下文示例与LLMs的母语风格对齐来提高LLMs的推理能力。 "母语"是指LLMs的固有特征，可以通过零-shot场景探测。 AlignedCoT广泛适用于ICL方法，可以轻松与最先进的技术结合，进一步提高LLMs的性能。我们在数学问答、常识推理和文本理解等多个基准测试上进行了广泛而全面的实验。实证结果表明，我们的AlignedCoT相比精心手工制作的演示文稿显著提高了性能。

    In-context learning (ICL) with large language models (LLMs) has become the modern tools of choice for many natural language processing tasks. However, how the text style of in-context examples influences the performance of LLMs still remains under-explored. This paper presents a novel and effective approach, named \textbf{AlignedCoT}, to improve the reasoning capability of LLMs by aligning the in-context examples with the native style of LLMs.''Native'' refers to the inherent characteristic of LLMs which can be probed by zero-shot scenarios.AlignedCoT is widely applicable to ICL methods, making it easy to combine with state-of-the-art techniques to further improve the LLMs' performance. We conduct extensive and comprehensive experiments on several benchmarks on mathematical question-answering, common-sense reasoning, and text understanding. The empirical results demonstrate that our AlignedCoT significantly improves performance over the carefully handcrafted demonstrations. Specificall
    
[^52]: 神经网络中基于价值最大化的元学习策略

    Meta-Learning Strategies through Value Maximization in Neural Networks. (arXiv:2310.19919v1 [cs.NE])

    [http://arxiv.org/abs/2310.19919](http://arxiv.org/abs/2310.19919)

    本文理论上研究了在神经网络中的元学习最优策略，并提出了一个学习努力的框架，可以高效地优化控制信号，从而提升学习性能。

    

    生物和人工学习代理面临诸多学习选择，包括超参数选择和任务分布的各个方面，如课程。了解如何进行这些元学习选择可以提供对生物学习者的认知控制功能的规范解释，并改进工程系统。然而，由于优化整个学习过程的复杂性，目前仍然挑战着计算现代深度网络中的最优策略。在这里，我们在一个可处理的环境中从理论上研究最优策略。我们提出了一个学习努力的框架，能够在完全规范化的目标上高效地优化控制信号：在学习过程中的折现累积性能。通过使用估计梯度下降的平均动力方程，我们获得了计算的可行性，该方程适用于简单的神经网络架构。我们的框架包容了一系列元学习和自动课程学习方法，形成了统一的框架。

    Biological and artificial learning agents face numerous choices about how to learn, ranging from hyperparameter selection to aspects of task distributions like curricula. Understanding how to make these meta-learning choices could offer normative accounts of cognitive control functions in biological learners and improve engineered systems. Yet optimal strategies remain challenging to compute in modern deep networks due to the complexity of optimizing through the entire learning process. Here we theoretically investigate optimal strategies in a tractable setting. We present a learning effort framework capable of efficiently optimizing control signals on a fully normative objective: discounted cumulative performance throughout learning. We obtain computational tractability by using average dynamical equations for gradient descent, available for simple neural network architectures. Our framework accommodates a range of meta-learning and automatic curriculum learning methods in a unified n
    
[^53]: 增强低精度采样：随机梯度Hamiltonian Monte Carlo

    Enhancing Low-Precision Sampling via Stochastic Gradient Hamiltonian Monte Carlo. (arXiv:2310.16320v1 [stat.ML])

    [http://arxiv.org/abs/2310.16320](http://arxiv.org/abs/2310.16320)

    本文研究了使用低精度和全精度梯度累加器的随机梯度Hamiltonian Monte Carlo (SGHMC)在低精度采样中的应用。实验证明，在非对数凹分布下，低精度SGHMC相对于低精度采样器（SGLD）实现了二次改进。

    

    低精度训练已经成为一种有前景的低成本技术，可以在不牺牲太多准确性的情况下提高深度神经网络的训练效率。其贝叶斯对应物可以进一步提供不确定性量化和改进的泛化准确性。本文研究了在强对数凹和非对数凹分布下，使用低精度和全精度梯度累加器的随机梯度Hamiltonian Monte Carlo (SGHMC)。从理论上讲，我们的结果表明，为了在非对数凹分布下实现2-Wasserstein距离的ε误差，低精度SGHMC相对于低精度采样器（随机梯度Langevin动力学，SGLD）实现了二次改进（$\widetilde{\mathbf{O}}\left({\epsilon^{-2}{\mu^*}^{-2}\log^2\left({\epsilon^{-1}}\right)}\right)$ vs $\widetilde{\mathbf{O}}\left({{\epsilon}^{-4}{\lambda^{*}}^{-1}\log^5\left({\epsilon^{-1}}\right)}\right)$）。另外，基于真实数据集的实验证明了低精度SGHMC相对于SGLD在非对数凹分布下的优越性。

    Low-precision training has emerged as a promising low-cost technique to enhance the training efficiency of deep neural networks without sacrificing much accuracy. Its Bayesian counterpart can further provide uncertainty quantification and improved generalization accuracy. This paper investigates low-precision sampling via Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) with low-precision and full-precision gradient accumulators for both strongly log-concave and non-log-concave distributions. Theoretically, our results show that, to achieve $\epsilon$-error in the 2-Wasserstein distance for non-log-concave distributions, low-precision SGHMC achieves quadratic improvement ($\widetilde{\mathbf{O}}\left({\epsilon^{-2}{\mu^*}^{-2}\log^2\left({\epsilon^{-1}}\right)}\right)$) compared to the state-of-the-art low-precision sampler, Stochastic Gradient Langevin Dynamics (SGLD) ($\widetilde{\mathbf{O}}\left({{\epsilon}^{-4}{\lambda^{*}}^{-1}\log^5\left({\epsilon^{-1}}\right)}\right)$). Moreo
    
[^54]: 神经网络中的元-（超文本外语境）学习

    Meta- (out-of-context) learning in neural networks. (arXiv:2310.15047v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.15047](http://arxiv.org/abs/2310.15047)

    该研究通过合成实验展示了一种称为元-超文本外语境学习（meta-OCL）的现象在神经网络中的存在。这种学习使神经网络能够更好地吸收广泛适用的语义内容，并在适当的情况下进行使用。研究者提出了关于元-超文本外语境学习产生的两种假设，并就未来AI系统的能力和潜在风险进行了讨论。

    

    Brown等人（2020）通过对大型语言模型（LLMs）进行精心设计的合成实验，建立了一种称为元-超文本外语境学习（meta-OCL）的现象的存在。我们的结果表明，元-超文本外语境学习使LLMs更容易“内化”文本的语义内容，该文本广泛适用（例如真实陈述或权威来源的文本），并在适当的情况下使用它。我们进一步在合成计算机视觉环境中展示了元-超文本外语境学习，并提出了两种假设，解释了元-超文本外语境学习的出现：一种是依赖于模型在其参数中存储知识的方式，另一种是暗示梯度下降优化器的隐含梯度对齐偏差可能负责。最后，我们对我们的结果可能意味着未来AI系统的能力进行了思考，并讨论了潜在的风险。我们的代码可以在https://github.com/krasheni找到。

    Brown et al. (2020) famously introduced the phenomenon of in-context learning in large language models (LLMs). We establish the existence of a phenomenon we call meta-out-of-context learning (meta-OCL) via carefully designed synthetic experiments with LLMs. Our results suggest that meta-OCL leads LLMs to more readily "internalize" the semantic content of text that is, or appears to be, broadly useful (such as true statements, or text from authoritative sources) and use it in appropriate circumstances. We further demonstrate meta-OCL in a synthetic computer vision setting, and propose two hypotheses for the emergence of meta-OCL: one relying on the way models store knowledge in their parameters, and another suggesting that the implicit gradient alignment bias of gradient-descent-based optimizers may be responsible. Finally, we reflect on what our results might imply about capabilities of future AI systems, and discuss potential risks. Our code can be found at https://github.com/krasheni
    
[^55]: DiagrammerGPT: 通过LLM规划生成开放领域、开放平台的图表

    DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM Planning. (arXiv:2310.12128v1 [cs.CV])

    [http://arxiv.org/abs/2310.12128](http://arxiv.org/abs/2310.12128)

    DiagrammerGPT是一个通过LLM规划生成开放领域、开放平台的图表的框架，填补了T2I模型在图表生成方面的空白。

    

    过去几年，文本到图像（T2I）生成取得了显著的发展。尽管如此，在使用T2I模型生成图表方面的研究很少。图表是一种使用结构丰富和空间复杂的可视化来解释信息的符号/示意性表示（例如，一种密集的相关对象、文本标签、方向箭头、连接线等组合）。现有的最先进的T2I模型在生成图表时经常失败，因为它们在许多对象通过复杂的关系（如箭头/线）密集连接时缺乏细粒度的对象布局控制，并且经常不能渲染出可理解的文本标签。为了填补这一空白，我们提出了DiagrammerGPT，一个新颖的两阶段文本到图表生成框架，它利用LLM（如GPT-4）的布局引导能力来生成更准确的开放领域、开放平台的图表。在第一阶段，我们使用LLM生成和迭代改进“图表规划”（在一个规划方案中）。

    Text-to-image (T2I) generation has seen significant growth over the past few years. Despite this, there has been little work on generating diagrams with T2I models. A diagram is a symbolic/schematic representation that explains information using structurally rich and spatially complex visualizations (e.g., a dense combination of related objects, text labels, directional arrows, connection lines, etc.). Existing state-of-the-art T2I models often fail at diagram generation because they lack fine-grained object layout control when many objects are densely connected via complex relations such as arrows/lines and also often fail to render comprehensible text labels. To address this gap, we present DiagrammerGPT, a novel two-stage text-to-diagram generation framework that leverages the layout guidance capabilities of LLMs (e.g., GPT-4) to generate more accurate open-domain, open-platform diagrams. In the first stage, we use LLMs to generate and iteratively refine 'diagram plans' (in a planne
    
[^56]: 一刀切：向能够训练多个分类任务的单一图模型迈进

    One for All: Towards Training One Graph Model for All Classification Tasks. (arXiv:2310.00149v1 [cs.LG])

    [http://arxiv.org/abs/2310.00149](http://arxiv.org/abs/2310.00149)

    这项研究提出了一种名为“一刀切”的通用框架，该框架能够使用单一图模型解决不同领域的多个任务。该框架克服了图学习领域的挑战，包括不同属性和分布的图数据、不同类型的任务以及上下文学习的问题。

    

    设计一个能够解决多个任务的单一模型一直是人工智能领域的长期目标。最近，大型语言模型展示了在语言领域内整合和解决不同任务的异常能力。然而，在图学习领域，针对各种任务的统一模型仍然未被充分探索，主要是由于图学习领域独特的挑战。首先，来自不同领域的图数据具有不同的属性和遵循不同的分布。这种差异使得很难将图表示在一个统一的表示空间中。其次，图上的任务分化为节点、链接和图任务，需要不同的嵌入策略。最后，关于上下文学习的适当图提示范式尚不清楚。为了应对上述挑战，我们提出了"一刀切"（OFA），这是第一个能够使用单一图模型来解决上述挑战的通用框架。

    Designing a single model that addresses multiple tasks has been a long-standing objective in artificial intelligence. Recently, large language models have demonstrated exceptional capability in integrating and solving different tasks within the language domain. However, a unified model for various tasks on graphs remains underexplored, primarily due to the challenges unique to the graph learning domain. First, graph data from different areas carry distinct attributes and follow different distributions. Such discrepancy makes it hard to represent graphs in a single representation space. Second, tasks on graphs diversify into node, link, and graph tasks, requiring distinct embedding strategies. Finally, an appropriate graph prompting paradigm for in-context learning is unclear. Striving to handle all the aforementioned challenges, we propose One for All (OFA), the first general framework that can use a single graph model to address the above challenges. Specifically, OFA proposes text-at
    
[^57]: 伪特征多样性改善了对分布外泛化的效果

    Spurious Feature Diversification Improves Out-of-distribution Generalization. (arXiv:2309.17230v1 [cs.LG])

    [http://arxiv.org/abs/2309.17230](http://arxiv.org/abs/2309.17230)

    本文研究了基于权重空间集成方法WiSE-FT在分布外泛化中的有效性，发现其成功纠正了许多个体模型的错误预测，并通过利用更多多样化的伪特征减少了分布外设置中的预测错误。

    

    在机器学习中，对分布外（OOD）数据的泛化是一个关键性挑战。基于集成的方法，如在模型参数上进行插值的权重空间集成，已被证明在OOD性能方面具有优势。然而，它们的有效性的基本机制仍不清楚。在本研究中，我们对一种常用的权重空间集成方法WiSE-FT进行了详细研究，该方法在预训练模型和微调模型之间进行插值。我们观察到一个意外的现象，即WiSE-FT成功地纠正了许多个体模型做出错误预测的情况，这对于其OOD的有效性贡献重大。为了进一步了解，我们在具有大量伪特征的多类别设置中进行了理论分析。我们的分析预测了上述现象，并进一步表明，基于集成的模型通过利用更多多样化的伪特征，减少了OOD设置中的预测错误。与传统观点相反。

    Generalization to out-of-distribution (OOD) data is a critical challenge in machine learning. Ensemble-based methods, like weight space ensembles that interpolate model parameters, have been shown to achieve superior OOD performance. However, the underlying mechanism for their effectiveness remains unclear. In this study, we closely examine WiSE-FT, a popular weight space ensemble method that interpolates between a pre-trained and a fine-tuned model. We observe an unexpected phenomenon, in which WiSE-FT successfully corrects many cases where each individual model makes incorrect predictions, which contributes significantly to its OOD effectiveness. To gain further insights, we conduct theoretical analysis in a multi-class setting with a large number of spurious features. Our analysis predicts the above phenomenon and it further shows that ensemble-based models reduce prediction errors in the OOD settings by utilizing a more diverse set of spurious features. Contrary to the conventional
    
[^58]: VideoDirectorGPT: 通过LLM引导的规划实现一致的多场景视频生成

    VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning. (arXiv:2309.15091v1 [cs.CV])

    [http://arxiv.org/abs/2309.15091](http://arxiv.org/abs/2309.15091)

    本文提出了VideoDirectorGPT，一种利用LLMs的知识实现一致多场景视频生成的框架，通过视频内容规划和基于内容的视频生成来生成时间上一致的长视频。

    

    尽管最近的文本到视频生成方法取得了显著的进展，但大多数工作集中在生成单个事件和单一背景的短视频片段（即单场景视频）。与此同时，最近的大型语言模型（LLMs）已经证明了它们在生成布局和控制下游视觉模块（如图像生成模型）的程序方面的能力。这引发了一个重要问题：我们能否利用这些LLMs中嵌入的知识用于生成时间上一致的长视频？在本文中，我们提出了VideoDirectorGPT，这是一个用于一致的多场景视频生成的新型框架，它利用LLMs的知识进行视频内容规划和基于内容的视频生成。具体而言，我们首先将单个文本提示输入我们的视频规划器LLM（GPT-4）中，将其扩展为“视频计划”，其中包括生成场景描述、实体及其布局、每个场景的背景以及保持一致性等内容。

    Although recent text-to-video (T2V) generation methods have seen significant advancements, most of these works focus on producing short video clips of a single event with a single background (i.e., single-scene videos). Meanwhile, recent large language models (LLMs) have demonstrated their capability in generating layouts and programs to control downstream visual modules such as image generation models. This raises an important question: can we leverage the knowledge embedded in these LLMs for temporally consistent long video generation? In this paper, we propose VideoDirectorGPT, a novel framework for consistent multi-scene video generation that uses the knowledge of LLMs for video content planning and grounded video generation. Specifically, given a single text prompt, we first ask our video planner LLM (GPT-4) to expand it into a 'video plan', which involves generating the scene descriptions, the entities with their respective layouts, the background for each scene, and consistency 
    
[^59]: 广义持续类别发现

    Generalized Continual Category Discovery. (arXiv:2308.12112v1 [cs.LG])

    [http://arxiv.org/abs/2308.12112](http://arxiv.org/abs/2308.12112)

    本研究提出了一种广义持续类别发现（GCCD）的框架，用于在现实生活场景中同时处理新的和已知的类别，并且利用持续的无监督学习方法来发现它们。通过实验证明现有方法无法处理后续任务中的无标记样本。

    

    大多数持续学习（CL）方法推动着监督学习设置的极限，其中一个智能体期望学习新的标记任务而不会忘记先前的知识。然而，这些设置与现实生活场景不太吻合，其中学习智能体可以访问大量的无标记数据，包括全新（完全无标记）类别和已知类别的示例。受到广义类别发现（GCD）的启发，我们引入了一个新的框架来放松这个假设。确切地说，在任何任务中，我们允许存在新的和已知的类别，并且必须使用持续版本的无监督学习方法来发现它们。我们称这种设置为广义持续类别发现（GCCD）。它统一了CL和GCD，弥合了合成基准和现实生活场景之间的差距。通过一系列实验，我们发现现有的方法无法从后续任务中积累知识，其中包含无标记样本。

    Most of Continual Learning (CL) methods push the limit of supervised learning settings, where an agent is expected to learn new labeled tasks and not forget previous knowledge. However, these settings are not well aligned with real-life scenarios, where a learning agent has access to a vast amount of unlabeled data encompassing both novel (entirely unlabeled) classes and examples from known classes. Drawing inspiration from Generalized Category Discovery (GCD), we introduce a novel framework that relaxes this assumption. Precisely, in any task, we allow for the existence of novel and known classes, and one must use continual version of unsupervised learning methods to discover them. We call this setting Generalized Continual Category Discovery (GCCD). It unifies CL and GCD, bridging the gap between synthetic benchmarks and real-life scenarios. With a series of experiments, we present that existing methods fail to accumulate knowledge from subsequent tasks in which unlabeled samples of 
    
[^60]: AdvDiff:使用扩散模型生成无限制的对抗样本

    AdvDiff: Generating Unrestricted Adversarial Examples using Diffusion Models. (arXiv:2307.12499v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.12499](http://arxiv.org/abs/2307.12499)

    本文提出了一种使用扩散模型生成无限制对抗样本的方法AdvDiff。通过设计两种新的对抗引导技术，在扩散模型的逆生成过程中进行对抗采样，从而有效地生成高质量、逼真的对抗样本。

    

    无限制的对抗攻击对深度学习模型和对抗防御技术构成严重威胁。它们对深度学习应用造成严重的安全问题，因为它们可以有效地绕过防御机制。然而，先前的攻击方法通常利用生成对抗网络（GAN），这些网络在理论上无法证明，因此在大规模数据集（如ImageNet）上通过引入对抗目标生成的例子是不现实的。在本文中，我们提出了一种新的方法，称为AdvDiff，使用扩散模型生成无限制的对抗样本。我们设计了两种新的对抗引导技术，在扩散模型的逆生成过程中进行对抗采样。这两种技术通过可解释的目标分类器梯度集成生成高质量、逼真的对抗样本非常有效和稳定。在MNIST和ImageNet数据集上的实验结果表明，AdvDiff能够生成高质量、逼真的对抗样本。

    Unrestricted adversarial attacks present a serious threat to deep learning models and adversarial defense techniques. They pose severe security problems for deep learning applications because they can effectively bypass defense mechanisms. However, previous attack methods often utilize Generative Adversarial Networks (GANs), which are not theoretically provable and thus generate unrealistic examples by incorporating adversarial objectives, especially for large-scale datasets like ImageNet. In this paper, we propose a new method, called AdvDiff, to generate unrestricted adversarial examples with diffusion models. We design two novel adversarial guidance techniques to conduct adversarial sampling in the reverse generation process of diffusion models. These two techniques are effective and stable to generate high-quality, realistic adversarial examples by integrating gradients of the target classifier interpretably. Experimental results on MNIST and ImageNet datasets demonstrate that AdvD
    
[^61]: 将注意力分割与绑定用于改进生成语义护理

    Divide & Bind Your Attention for Improved Generative Semantic Nursing. (arXiv:2307.10864v1 [cs.CV])

    [http://arxiv.org/abs/2307.10864](http://arxiv.org/abs/2307.10864)

    本论文提出了一种名为"分割与绑定"的方法，旨在改进生成语义护理的效果。该方法引入了新的损失目标，包括关注丢失和绑定丢失，以解决复杂提示和不适当属性绑定的问题。

    

    新兴的大规模文本到图像生成模型，如稳定扩散（SD），展示了高度逼真的压倒性结果。尽管取得了巨大的进展，但当前最先进的模型仍然难以完全依照输入提示生成图像。先前的研究——关注与激发，引入了生成语义护理（GSN）的概念，旨在在推断时优化跨注意力以更好地融入语义。它在生成简单提示，如“一只猫和一只狗”，方面展示了有希望的结果。然而，它在处理更复杂的提示以及解决不适当的属性绑定问题方面的功效有所下降。为了应对复杂提示或涉及多个实体的场景所带来的挑战，并实现改进的属性绑定，我们提出了分割与绑定。我们引入了两个新的GSN损失目标：一种新的关注丢失和一种绑定丢失。我们的方法在其能够更好地将语义纳入图像生成过程中的特点上脱颖而出。

    Emerging large-scale text-to-image generative models, e.g., Stable Diffusion (SD), have exhibited overwhelming results with high fidelity. Despite the magnificent progress, current state-of-the-art models still struggle to generate images fully adhering to the input prompt. Prior work, Attend & Excite, has introduced the concept of Generative Semantic Nursing (GSN), aiming to optimize cross-attention during inference time to better incorporate the semantics. It demonstrates promising results in generating simple prompts, e.g., ``a cat and a dog''. However, its efficacy declines when dealing with more complex prompts, and it does not explicitly address the problem of improper attribute binding. To address the challenges posed by complex prompts or scenarios involving multiple entities and to achieve improved attribute binding, we propose Divide & Bind. We introduce two novel loss objectives for GSN: a novel attendance loss and a binding loss. Our approach stands out in its ability to fa
    
[^62]: 离线组合强化学习的机器人操作数据集

    Robotic Manipulation Datasets for Offline Compositional Reinforcement Learning. (arXiv:2307.07091v1 [cs.LG])

    [http://arxiv.org/abs/2307.07091](http://arxiv.org/abs/2307.07091)

    本论文提供了离线强化学习的机器人操作数据集，使用组合式强化学习生成了四个包含256个任务的数据集。每个数据集由性能不同的代理采集，包含2.56亿条转换记录。实验结果显示，

    

    离线强化学习是一种有前途的方向，可以让强化学习代理在大规模数据集上进行预训练，避免昂贵的数据收集过程的重复。为了推动该领域的发展，生成大规模数据集至关重要。组合式强化学习对于生成这样的大规模数据集尤为有吸引力，因为1）它允许从少量组件中创建多个任务，2）任务结构可以让训练好的代理通过组合相关的学习组件来解决新任务，并且3）组合维度提供了任务关联性的概念。本文提供了四个离线强化学习数据集，用于模拟机器人操作，这些数据集使用了来自CompoSuite [Mendez et al., 2022a]的256个任务。每个数据集是从一个具有不同性能等级的代理收集的，包含了2.56亿条转换记录。我们提供了训练和评估设置，以评估代理学习组合任务策略的能力。我们在每个设置上进行的基准实验表明，

    Offline reinforcement learning (RL) is a promising direction that allows RL agents to pre-train on large datasets, avoiding the recurrence of expensive data collection. To advance the field, it is crucial to generate large-scale datasets. Compositional RL is particularly appealing for generating such large datasets, since 1) it permits creating many tasks from few components, 2) the task structure may enable trained agents to solve new tasks by combining relevant learned components, and 3) the compositional dimensions provide a notion of task relatedness. This paper provides four offline RL datasets for simulated robotic manipulation created using the 256 tasks from CompoSuite [Mendez et al., 2022a]. Each dataset is collected from an agent with a different degree of performance, and consists of 256 million transitions. We provide training and evaluation settings for assessing an agent's ability to learn compositional task policies. Our benchmarking experiments on each setting show that
    
[^63]: 深度学习中的超网络简要回顾

    A Brief Review of Hypernetworks in Deep Learning. (arXiv:2306.06955v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.06955](http://arxiv.org/abs/2306.06955)

    超网络是一种生成另一个神经网络权重的深度学习技术，具有灵活性、适应性、动态性、更快的训练速度、信息共享和模型压缩等优点。它在各种深度学习问题中显示了良好的效果，并在持续学习、迁移学习、权重剪枝、不确定性量化、零样本学习、自然语言处理和强化学习等领域取得了成功。

    

    超网络（Hypernetworks）是生成另一个神经网络（目标网络）权重的神经网络。它们作为一种强大的深度学习技术出现，能够提供更大的灵活性、适应性、动态性、更快的训练速度、信息共享和模型压缩等。超网络在各种深度学习问题中显示出了良好的效果，包括持续学习、因果推断、迁移学习、权重剪枝、不确定性量化、零样本学习、自然语言处理和强化学习等。尽管超网络在不同的问题设置中取得了成功，但目前还没有可用的综述来告知研究人员有关其发展情况并帮助利用超网络。为了填补这一空白，我们回顾了超网络的进展。我们通过一个例子来说明如何使用超网络来训练深度神经网络，并提出了基于五个设计准则对超网络进行分类。

    Hypernetworks, or hypernets in short, are neural networks that generate weights for another neural network, known as the target network. They have emerged as a powerful deep learning technique that allows for greater flexibility, adaptability, dynamism, faster training, information sharing, and model compression etc. Hypernets have shown promising results in a variety of deep learning problems, including continual learning, causal inference, transfer learning, weight pruning, uncertainty quantification, zero-shot learning, natural language processing, and reinforcement learning etc. Despite their success across different problem settings, currently, there is no review available to inform the researchers about the developments and to help in utilizing hypernets. To fill this gap, we review the progress in hypernets. We present an illustrative example to train deep neural networks using hypernets and propose categorizing hypernets based on five design criteria as inputs, outputs, variabi
    
[^64]: 高度动态条件下的公共交通需求预测：现有模型的元分析和开源基准设施比较

    Public Transit Demand Prediction During Highly Dynamic Conditions: A Meta-Analysis of State-of-the-Art Models and Open-Source Benchmarking Infrastructure. (arXiv:2306.06194v1 [cs.LG])

    [http://arxiv.org/abs/2306.06194](http://arxiv.org/abs/2306.06194)

    本论文基于智能卡数据的时间序列预测哥伦比亚波哥大BRT系统隔日需求，建立了一个开源基础设施并评估了五种常见方法在稳定和高度动态条件下的表现，结果显示大多数测试模型的表现相似，MAAPE从0.08到0.12不等。

    

    实时需求预测是动态公交路线规划的关键输入。虽然许多研究人员已经开发了许多复杂的方法来预测短期公共交通需求，但应用范围仅限于短时间、稳定的时间范围和少数车站。这些方法在高度动态的环境中的表现还没有得到研究，也没有系统地进行比较。我们建立了一个开源基础设施，包括计量和深度学习方法，评估它们在稳定和高度动态条件下的表现。我们使用智能卡数据的时间序列预测哥伦比亚波哥大BRT系统在隔日的需求。时间序列的动态条件包括一个长达一个月的抗议和COVID-19大流行。这两个条件都引发了需求的巨大变化。结果表明，在稳定条件下，大多数测试模型的表现相似，MAAPE从0.08到0.12不等。 基准设施的

    Real-time demand prediction is a critical input for dynamic bus routing. While many researchers have developed numerous complex methods to predict short-term transit demand, the applications have been limited to short, stable time frames and a few stations. How these methods perform in highly dynamic environments has not been studied, nor has their performance been systematically compared. We built an open-source infrastructure with five common methodologies, including econometric and deep learning approaches, and assessed their performance under stable and highly dynamic conditions. We used a time series from smartcard data to predict demand for the following day for the BRT system in Bogota, Colombia. The dynamic conditions in the time series include a month-long protest and the COVID-19 pandemic. Both conditions triggered drastic shifts in demand. The results reveal that most tested models perform similarly in stable conditions, with MAAPE varying from 0.08 to 0.12. The benchmark de
    
[^65]: 等变层与不变层的对比：点云分类中骨干网络和池化的比较

    Equivariant vs. Invariant Layers: A Comparison of Backbone and Pooling for Point Cloud Classification. (arXiv:2306.05553v1 [cs.CV])

    [http://arxiv.org/abs/2306.05553](http://arxiv.org/abs/2306.05553)

    本文研究了置换等变骨干和置换不变全局池化在点云分类中的相互作用，揭示了使用复杂池化方法可以显著提高简单骨干的性能，但即使是复杂的骨干也可以受益于更复杂的、明确编码置换不变性的池化方法，使用置换不变池化是获得最先进结果的关键。

    

    学习点云等集合结构数据已受到学术界的广泛关注。几何深度学习通过整合置换对称性，为设计有效的点云神经网络提供了蓝本。我们感兴趣的是置换不变网络，该网络由置换等变骨干、置换不变全局池化和回归/分类头组成。尽管现有文献侧重于改善置换等变骨干，但全局池化的影响往往被忽视。在本文中，我们研究了置换等变骨干和置换不变全局池化在三个基准点云分类数据集上的相互作用。我们的研究结果表明：1）诸如基于传输或注意力的复杂池化方法可以显著提高简单骨干的性能，但对于更复杂的骨干，这些方法的收益会减弱。2）甚至复杂的骨干也可以受益于更复杂的池化方法，这些方法明确地编码置换不变性。3）使用置换不变池化对于在点云分类数据集上获得最先进的结果至关重要。

    Learning from set-structured data, such as point clouds, has gained significant attention from the community. Geometric deep learning provides a blueprint for designing effective set neural networks by incorporating permutation symmetry. Of our interest are permutation invariant networks, which are composed of a permutation equivariant backbone, permutation invariant global pooling, and regression/classification head. While existing literature has focused on improving permutation equivariant backbones, the impact of global pooling is often overlooked. In this paper, we examine the interplay between permutation equivariant backbones and permutation invariant global pooling on three benchmark point cloud classification datasets. Our findings reveal that: 1) complex pooling methods, such as transport-based or attention-based poolings, can significantly boost the performance of simple backbones, but the benefits diminish for more complex backbones, 2) even complex backbones can benefit fro
    
[^66]: 基于Epoch的随机梯度下降中相关噪声：权重方差的影响

    Correlated Noise in Epoch-Based Stochastic Gradient Descent: Implications for Weight Variances. (arXiv:2306.05300v1 [cs.LG])

    [http://arxiv.org/abs/2306.05300](http://arxiv.org/abs/2306.05300)

    研究挑战了在时间上是不相关的假设，并强调了epoch-based噪声相关性对离散时间带动量的SGD的权重方差的影响。

    

    随机梯度下降（SGD）已成为神经网络优化的基石，但认为SGD引入的噪声在时间上是不相关的，尽管epoch-based训练是无处不在的。在这项工作中，我们对此进行了挑战，并调查了epoch-based噪声相关性对离散时间带动量的SGD的稳态分布的影响，限于二次损失。我们的主要贡献有两个：首先，我们计算训练epoch时噪声的精确自相关性，假设该噪声独立于权重向量中的小波动;其次，我们探索epoch-based学习方案引入的相关性对SGD动态的影响。我们发现，在曲率大于一个超参数相关值的方向上，还原了不相关噪声的结果。然而，在相对平坦的方向上，权重方差显着减小。我们使用简单的二维图例对这些结果进行了直观解释。总的来说，我们的工作提供了关于epoch-based SGD中相关噪声影响的见解，可以指导设计更有效的优化算法。

    Stochastic gradient descent (SGD) has become a cornerstone of neural network optimization, yet the noise introduced by SGD is often assumed to be uncorrelated over time, despite the ubiquity of epoch-based training. In this work, we challenge this assumption and investigate the effects of epoch-based noise correlations on the stationary distribution of discrete-time SGD with momentum, limited to a quadratic loss. Our main contributions are twofold: first, we calculate the exact autocorrelation of the noise for training in epochs under the assumption that the noise is independent of small fluctuations in the weight vector; second, we explore the influence of correlations introduced by the epoch-based learning scheme on SGD dynamics. We find that for directions with a curvature greater than a hyperparameter-dependent crossover value, the results for uncorrelated noise are recovered. However, for relatively flat directions, the weight variance is significantly reduced. We provide an intui
    
[^67]: 一次性对齐、提炼和扩充所有不平衡的半监督学习

    Align, Distill, and Augment Everything All at Once for Imbalanced Semi-Supervised Learning. (arXiv:2306.04621v1 [cs.LG])

    [http://arxiv.org/abs/2306.04621](http://arxiv.org/abs/2306.04621)

    本文针对长尾半监督学习中类别不平衡的问题，提出了三个解决方案：一种灵活的分布对齐方法，一种软一致性正则化方法和一种扩充未标记集的方案。

    

    在解决长尾半监督学习中的类别不平衡问题时，需面对未标记数据和已标记数据之间边缘分布的区别，前者通常是未知的且可能与后者不同，这导致了一些重大挑战。第一个挑战是在训练过程中避免使伪标签对目标分布的偏倚，如已标记数据或平衡分布。第二个挑战是确保推理时的平衡未标记分布。为应对这些挑战，我们提出了一个多方面的解决方案：通过灵活的分布对齐，逐渐将分类器从动态估计的未标记先验分布对齐到平衡分布；利用被基于阈值的方法舍弃的低置信度伪标签的软一致性正则化；以及一种将标记部分的输入数据扩展到未标记集的方案。

    Addressing the class imbalance in long-tailed semi-supervised learning (SSL) poses a few significant challenges stemming from differences between the marginal distributions of unlabeled data and the labeled data, as the former is often unknown and potentially distinct from the latter. The first challenge is to avoid biasing the pseudo-labels towards an incorrect distribution, such as that of the labeled data or a balanced distribution, during training. However, we still wish to ensure a balanced unlabeled distribution during inference, which is the second challenge. To address both of these challenges, we propose a three-faceted solution: a flexible distribution alignment that progressively aligns the classifier from a dynamically estimated unlabeled prior towards a balanced distribution, a soft consistency regularization that exploits underconfident pseudo-labels discarded by threshold-based methods, and a schema for expanding the unlabeled set with input data from the labeled partiti
    
[^68]: 时间变化处理的反事实生成模型

    Counterfactual Generative Models for Time-Varying Treatments. (arXiv:2305.15742v1 [stat.ML])

    [http://arxiv.org/abs/2305.15742](http://arxiv.org/abs/2305.15742)

    本文研究了时间变量处理情况下的反事实生成模型，能够捕捉整个反事实分布，并且能够有效推断反事实分布的某些统计量，适用于医疗保健和公共政策制定领域。

    

    估计平均因果效应是测试新疗法的常用做法。然而，平均效应会掩盖反事实分布中重要的个体特征，可能会引起安全、公平和道德方面的担忧。这个问题在时间设置中更加严重，因为处理是时序的和时变的，对反事实分布产生了错综复杂的影响。本文提出了一种新的条件生成建模方法，以捕获整个反事实分布，允许对反事实分布的某些统计量进行有效推断。这使得所提出的方法尤其适用于医疗保健和公共政策制定领域。我们的生成建模方法通过边际结构模型谨慎地解决了观察数据和目标反事实分布之间的分布不匹配。在合成和真实数据上，我们的方法优于现有的基线方法。

    Estimating average causal effects is a common practice to test new treatments. However, the average effect ''masks'' important individual characteristics in the counterfactual distribution, which may lead to safety, fairness, and ethical concerns. This issue is exacerbated in the temporal setting, where the treatment is sequential and time-varying, leading to an intricate influence on the counterfactual distribution. In this paper, we propose a novel conditional generative modeling approach to capture the whole counterfactual distribution, allowing efficient inference on certain statistics of the counterfactual distribution. This makes the proposed approach particularly suitable for healthcare and public policy making. Our generative modeling approach carefully tackles the distribution mismatch in the observed data and the targeted counterfactual distribution via a marginal structural model. Our method outperforms state-of-the-art baselines on both synthetic and real data.
    
[^69]: 学习6D非抓取式操作的混合演员-评论员地图

    Learning Hybrid Actor-Critic Maps for 6D Non-Prehensile Manipulation. (arXiv:2305.03942v1 [cs.RO])

    [http://arxiv.org/abs/2305.03942](http://arxiv.org/abs/2305.03942)

    论文介绍了一种名为HACMan的强化学习方法，用于使用点云观察进行6D非抓取式操作的物体操纵。HACMan重点关注物体中心动作表示，它包括从物体点云中选择接触位置和一组描述机器人在接触后如何移动的运动参数。在实际测试中，HACMan的表现明显优于现有基线方法。

    

    在人类的灵巧性中，非抓取式操作是操作物体的重要组成部分。非抓取式操纵可以使与物体的交互更加复杂，但也在推理交互方面提出了挑战。在本文中，我们引入了一个名为HACMan的混合演员评论员地图，这是一种使用点云观察的6D非抓取式物体操作的强化学习方法。HACMan提出了一种时间抽象和空间基础的物体中心动作表示，该表示包括从物体点云中选择接触位置和一组描述机器人在接触后如何移动的运动参数。我们修改了一个现有的离线策略RL算法，以在这种混合的离散-连续动作表示学习。我们在仿真和现实世界中对HACMan进行了6D物体姿态对齐任务的评估。在最难的任务版本中，通过随机初始化物体和机器人配置，HACMan的表现优于现有的基线方法。

    Manipulating objects without grasping them is an essential component of human dexterity, referred to as non-prehensile manipulation. Non-prehensile manipulation may enable more complex interactions with the objects, but also presents challenges in reasoning about the interactions. In this work, we introduce Hybrid Actor-Critic Maps for Manipulation (HACMan), a reinforcement learning approach for 6D non-prehensile manipulation of objects using point cloud observations. HACMan proposes a temporally-abstracted and spatially-grounded object-centric action representation that consists of selecting a contact location from the object point cloud and a set of motion parameters describing how the robot will move after making contact. We modify an existing off-policy RL algorithm to learn in this hybrid discrete-continuous action representation. We evaluate HACMan on a 6D object pose alignment task in both simulation and in the real world. On the hardest version of our task, with randomized init
    
[^70]: 基于量子物理的神经网络用于在复杂形状中模拟计算流体力学

    Quantum physics-informed neural networks for simulating computational fluid dynamics in complex shapes. (arXiv:2304.11247v1 [cs.LG])

    [http://arxiv.org/abs/2304.11247](http://arxiv.org/abs/2304.11247)

    本文提出了一种基于量子物理的神经网络方法，用于模拟复杂几何形状中的流体流动。该方法不需要重新模拟，能够适用于不同的形状，并且相比于普通神经网络提高了21%的精度。

    

    解决流体的速度和压力分布（通过解决纳维尔-斯托克斯方程）是化学、能源、制药工业以及机械工程和管道系统设计中的一个主要任务。现有的求解器（如OpenFOAM和Ansys）在复杂几何形状中的流体动力学模拟是计算密集型的，需要重新模拟每当几何参数或初始和边界条件被改变。物理学信赖的神经网络（PINNs）是模拟复杂几何形状中流体流动的有前途的工具，因为它们可以适应几何形状和网格定义的变化，允许跨不同形状进行概括。我们提供了一种混合量子物理的神经网络，该网络模拟三维 Y 型混合器中的层流流体流动。我们的方法将量子模型的表达能力与 PINN 的灵活性相结合，精度比普通 PINN 提高了 21％。

    Finding the distribution of the velocities and pressures of a fluid (by solving the Navier-Stokes equations) is a principal task in the chemical, energy, and pharmaceutical industries, as well as in mechanical engineering and the design of pipeline systems. With existing solvers, such as OpenFOAM and Ansys, simulations of fluid dynamics in intricate geometries are computationally expensive and require re-simulation whenever the geometric parameters or the initial and boundary conditions are altered. Physics-informed neural networks (PINNs) are a promising tool for simulating fluid flows in complex geometries, as they can adapt to changes in the geometry and mesh definitions, allowing for generalization across different shapes. We present a hybrid quantum physics-informed neural network that simulates laminar fluid flows in 3D Y-shaped mixers. Our approach combines the expressive power of a quantum model with the flexibility of a PINN, resulting in a 21% higher accuracy compared to a pu
    
[^71]: MaskedKD：使用遮蔽图像的高效Vision Transformer蒸馏

    MaskedKD: Efficient Distillation of Vision Transformers with Masked Images. (arXiv:2302.10494v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10494](http://arxiv.org/abs/2302.10494)

    MaskedKD提出了一种通过遮蔽图像块来显著降低Vision Transformer (ViT)蒸馏成本的方法，而不影响学生模型的预测准确性。

    

    知识蒸馏对于训练轻量级模型是一种有效的方法，但它会在训练成本中引入大量的计算开销，因为该方法需要在训练样本上获取教师监督。当使用大规模的Vision Transformer（ViTs）等教师模型时，这种附加成本——蒸馏成本——最为明显。我们提出了MaskedKD，这是一种简单但有效的策略，可以显着降低蒸馏ViTs的成本，同时不损失学生模型的预测准确性。具体来说，MaskedKD通过遮蔽一部分输入到教师模型的图像块令教师模型的推理成本减少，因此可以跳过处理这些块所需的计算。所选的遮罩位置旨在防止屏蔽学生模型用于预测的图像的核心特征。该遮罩选择机制基于学生模型的某些注意力分数操作。

    Knowledge distillation is an effective method for training lightweight models, but it introduces a significant amount of computational overhead to the training cost, as the method requires acquiring teacher supervisions on training samples. This additional cost -- called distillation cost -- is most pronounced when we employ large-scale teacher models such as vision transformers (ViTs). We present MaskedKD, a simple yet effective strategy that can significantly reduce the cost of distilling ViTs without sacrificing the prediction accuracy of the student model. Specifically, MaskedKD diminishes the cost of running teacher at inference by masking a fraction of image patch tokens fed to the teacher, and therefore skipping the computations required to process those patches. The mask locations are selected to prevent masking away the core features of an image that the student model uses for prediction. This mask selection mechanism operates based on some attention score of the student model
    
[^72]: 自监督学习的多维视角综述: 算法、应用和未来趋势

    A Survey of Self-supervised Learning from Multiple Perspectives: Algorithms, Applications and Future Trends. (arXiv:2301.05712v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.05712](http://arxiv.org/abs/2301.05712)

    本综述论文从算法、应用和趋势的角度概述了自监督学习的多维视角。它介绍了SSL算法的动机、共性和差异，以及在图像处理、计算机视觉和自然语言处理等领域中的典型应用。

    

    深度监督学习算法通常需要大量标记的样本来达到令人满意的性能。然而，收集和标记过多的样本可能成本高昂且耗时。作为无监督学习的子集，自监督学习（SSL）旨在从未标记的样本中学习有用的特征，而无需任何人工标注的标签。SSL最近引起了广泛关注，并且已经开发了许多相关算法。然而，目前很少有综述研究来解释不同的SSL变体之间的关系和演变。本文从算法、应用、三个主要趋势和待解问题的视角综述了各种SSL方法。首先，详细介绍了大多数SSL算法的动机，并比较了它们的共性和差异。其次，概述了SSL在图像处理和计算机视觉（CV）以及自然语言处理（NLP）等领域中的典型应用。

    Deep supervised learning algorithms generally require large numbers of labeled examples to achieve satisfactory performance. However, collecting and labeling too many examples can be costly and time-consuming. As a subset of unsupervised learning, self-supervised learning (SSL) aims to learn useful features from unlabeled examples without any human-annotated labels. SSL has recently attracted much attention and many related algorithms have been developed. However, there are few comprehensive studies that explain the connections and evolution of different SSL variants. In this paper, we provide a review of various SSL methods from the perspectives of algorithms, applications, three main trends, and open questions. First, the motivations of most SSL algorithms are introduced in detail, and their commonalities and differences are compared. Second, typical applications of SSL in domains such as image processing and computer vision (CV), as well as natural language processing (NLP), are dis
    
[^73]: 通过Monte Carlo Forest Search实现UNSAT求解器的合成

    UNSAT Solver Synthesis via Monte Carlo Forest Search. (arXiv:2211.12581v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.12581](http://arxiv.org/abs/2211.12581)

    介绍了使用MCFS算法合成UNSAT求解器的方法，算法可用于解决包括SAT公式不可满足性证明、可满足SAT公式解的数量计数和混合整数规划的最优解问题，并利用合成森林构建算法和合成MDP类来避免构建候选树森林的问题。

    

    我们介绍了Monte Carlo Forest Search（MCFS），一类用于学习决策树MDP策略的强化学习（RL）算法。这些问题的示例包括证明SAT公式的不可满足性；计算可满足的SAT公式的解的数量；以及找到混合整数规划的最优解。MCFS算法可以看作是Monte Carlo Tree Search（MCTS）的扩展，用于在候选树的森林中寻找一个小树，而不是在树中找到一个好路径（解决方案）。我们在算法中实例化和评估了自己的想法，称之为Knuth Synthesis，这是一个MCFS算法，用于学习DPLL分支策略来解决布尔可满足性（SAT）问题。这利用了两个关键思想，以避免构建候选树森林的问题：（1）一种合成森林构建算法，通过从池中随机选择“好”的树并将它们组合成更大的森林来逐步构建森林；（2）一种合成MDP类，用作真实树MDP的代理，我们可以轻松计算节点间转换的概率。

    We introduce Monte Carlo Forest Search (MCFS), a class of reinforcement learning (RL) algorithms for learning policies in {tree MDPs}, for which policy execution involves traversing an exponential-sized tree. Examples of such problems include proving unsatisfiability of a SAT formula; counting the number of solutions of a satisfiable SAT formula; and finding the optimal solution to a mixed-integer program. MCFS algorithms can be seen as extensions of Monte Carlo Tree Search (MCTS) to cases where, rather than finding a good path (solution) within a tree, the problem is to find a small tree within a forest of candidate trees. We instantiate and evaluate our ideas in an algorithm that we dub Knuth Synthesis, an MCFS algorithm that learns DPLL branching policies for solving the Boolean satisfiability (SAT) problem, with the objective of achieving good average-case performance on a given distribution of unsatisfiable problem instances. Knuth Synthesis leverages two key ideas to avoid the pr
    

