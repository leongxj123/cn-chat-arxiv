# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Natural Counterfactuals With Necessary Backtracking](https://rss.arxiv.org/abs/2402.01607) | 本研究提出了一种自然反事实框架和方法，通过优化控制回溯的范围，生成与实际世界的数据分布相匹配的自然反事实，从而改进了反事实推理。 |
| [^2] | [Specialized Language Models with Cheap Inference from Limited Domain Data](https://rss.arxiv.org/abs/2402.01093) | 本研究提出了一种使用有限领域数据进行廉价推理的专用语言模型。在研究中，我们通过比较不同的机器学习方法，在推理成本的限制下找到了比训练非常大的基本转换器模型更优的替代方案。具体而言，在大型预训练预算下，超网络和专家混合模型的困惑度更好，而在大型专用预算下，训练重要样本数据集上的小型模型更具吸引力。 |
| [^3] | [Policy Mirror Descent with Lookahead](https://arxiv.org/abs/2403.14156) | 提出了一种新类别的策略镜像下降算法$h$-PMD，它通过在PMD更新规则中结合多步贪心策略改进和前瞻深度$h，以解决折扣无限时间视角下的马尔可夫决策过程。 |
| [^4] | [Offline Multitask Representation Learning for Reinforcement Learning](https://arxiv.org/abs/2403.11574) | 通过研究离线多任务低秩RL，提出了一种名为MORL的新算法，证明了在强化学习中应用学习到的表示的优势。 |
| [^5] | [Narrative Feature or Structured Feature? A Study of Large Language Models to Identify Cancer Patients at Risk of Heart Failure](https://arxiv.org/abs/2403.11425) | 使用大型语言模型结合新颖的叙述特征，能够有效识别癌症患者患心力衰竭的风险，表现优于传统机器学习模型和深度学习模型。 |
| [^6] | [GreedyML: A Parallel Algorithm for Maximizing Submodular Functions](https://arxiv.org/abs/2403.10332) | 提出了一种用于在分布式存储多处理器上最大化子模函数的并行近似算法，以解决实际应用领域中海量数据集上的子模优化问题。 |
| [^7] | [Transferable Reinforcement Learning via Generalized Occupancy Models](https://arxiv.org/abs/2403.06328) | 通过广义占有模型，本研究提出了一种新颖的模型类别，保留了模型化强化学习的通用性，并避免了累积错误的问题。 |
| [^8] | [ERBench: An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Language Models](https://arxiv.org/abs/2403.05266) | ERBench是一个基于实体关系的大型语言模型幻觉基准，通过自动转换任何关系数据库并构建可自动验证的问题，以支持复杂性评估和调试 |
| [^9] | [Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self Attention at the Threadblock Level](https://arxiv.org/abs/2403.04690) | 该研究提出了一种更快的邻域注意力机制，通过将注意力限制在最近的邻居之间来降低自注意力的计算复杂度，实现了显著的性能提升。 |
| [^10] | [Inference via Interpolation: Contrastive Representations Provably Enable Planning and Inference](https://arxiv.org/abs/2403.04082) | 通过对比学习学到的时间序列数据表示遵循高斯马尔可夫链，从而启用规划和推断 |
| [^11] | [Solution Simplex Clustering for Heterogeneous Federated Learning](https://arxiv.org/abs/2403.03333) | 提出了Solution Simplex Clustered Federated Learning（SosicFL），通过学习解决方案单纯形的思想，为每个客户端分配单一区域，从而同时实现了学习本地和全局模型的目标。 |
| [^12] | [Epsilon-Greedy Thompson Sampling to Bayesian Optimization](https://arxiv.org/abs/2403.00540) | 将$\varepsilon$-greedy策略引入Thompson采样以改进贝叶斯优化中的开发功能，并实证表明其有效性。 |
| [^13] | [A Priori Uncertainty Quantification of Reacting Turbulence Closure Models using Bayesian Neural Networks](https://arxiv.org/abs/2402.18729) | 使用贝叶斯神经网络对反应流动模型中的不确定性进行量化，特别是在湍涡预混火焰动态中关键变量的建模方面取得重要进展 |
| [^14] | [Implicit Bias of Next-Token Prediction](https://arxiv.org/abs/2402.18551) | 在基于梯度的优化器训练下的线性NTP模型中，确定了NTP可分离条件，并证明梯度下降能够实现其下界；同时证明了这些条件在过参数化时仍然成立。 |
| [^15] | [Conformer: Embedding Continuous Attention in Vision Transformer for Weather Forecasting](https://arxiv.org/abs/2402.17966) | Conformer是一种用于天气预测的时空连续视觉Transformer，通过在多头注意力机制中实现连续性来学习时间上的连续天气演变。 |
| [^16] | [Dual-Space Optimization: Improved Molecule Sequence Design by Latent Prompt Transformer](https://arxiv.org/abs/2402.17179) | 提出了双空间优化（DSO）方法，通过整合潜在空间采样和数据空间选择，使用Latent Prompt Transformer (LPT)生成模型，解决了分子设计中的关键问题，取得了在不同任务中的性能优势。 |
| [^17] | [NeuralThink: Algorithm Synthesis that Extrapolates in General Tasks](https://arxiv.org/abs/2402.15393) | NeuralThink 是一种新的递归架构，可以一贯地对对称和不对称任务进行外推，相较于之前的最先进的深度思维架构在稳定地从较小的训练规模对大观测进行外推方面表现出更好的性能。 |
| [^18] | [Achieving Near-Optimal Regret for Bandit Algorithms with Uniform Last-Iterate Guarantee](https://arxiv.org/abs/2402.12711) | 本文引入了统一最后迭代(ULI)保证这一更强的性能度量，同时证明接近最优的ULI保证直接导致了在各种性能度量上接近最优的累积性能。 |
| [^19] | [Microstructures and Accuracy of Graph Recall by Large Language Models](https://arxiv.org/abs/2402.11821) | 本研究首次系统研究了大型语言模型对图形召回的准确性和偏见微结构，探讨了它们与人类的异同以及对其他图形推理任务的影响。 |
| [^20] | [Classification Diffusion Models](https://arxiv.org/abs/2402.10095) | 提出了一种分类扩散模型（CDMs），该模型采用了去噪扩散模型（DDM）的形式，并利用一个分类器来预测加在干净信号上的噪声量，取得了在图像、视频和音频生成方面的最先进结果。 |
| [^21] | [Trained Without My Consent: Detecting Code Inclusion In Language Models Trained on Code](https://arxiv.org/abs/2402.09299) | 这项研究关注如何在训练代码的语言模型中检测代码包含，以解决使用这些模型进行代码审计时的版权侵权问题。 |
| [^22] | [Transition Constrained Bayesian Optimization via Markov Decision Processes](https://arxiv.org/abs/2402.08406) | 本文介绍了一种过渡受限的贝叶斯优化方法，通过马尔可夫决策过程的框架，使用强化学习解决了由于转变约束导致的搜索空间依赖历史的问题，并在化学反应器优化、信息化路径规划、机器校准等领域进行了应用。 |
| [^23] | [SMX: Sequential Monte Carlo Planning for Expert Iteration](https://arxiv.org/abs/2402.07963) | 这项研究介绍了一种名为SMX的顺序蒙特卡洛规划算法，它利用可扩展的方法创建了有效的自我学习机制。它适用于离散和连续动作空间的环境，具有高并行性能。 |
| [^24] | [Self-Consistent Conformal Prediction](https://arxiv.org/abs/2402.07307) | 自洽的符合预测方法能够提供既符合校准的预测又符合以模型预测的动作为条件的预测区间，为决策者提供了严格的、针对具体动作的决策保证。 |
| [^25] | [Learning the Expected Core of Strictly Convex Stochastic Cooperative Games](https://arxiv.org/abs/2402.07067) | 本文研究了随机合作博弈中严格凸情况下，学习预期核心的问题。我们提出了一种名为\texttt{Common-Points-Picking}的算法，在多项式数量的样本给定的情况下，以高概率返回一个稳定分配。 |
| [^26] | [Studious Bob Fight Back Against Jailbreaking via Prompt Adversarial Tuning](https://arxiv.org/abs/2402.06255) | 本文提出了一种名为Prompt Adversarial Tuning (PAT)的方法，通过训练一个防御控制机制并将其作为前缀嵌入到用户提示中，实现对大型语言模型（LLMs）的越狱行为的防御。实验证明该方法在保护LLMs免受产生有害信息的影响方面效果显著。 |
| [^27] | [DiffTOP: Differentiable Trajectory Optimization for Deep Reinforcement and Imitation Learning](https://arxiv.org/abs/2402.05421) | DiffTOP使用可微分轨迹优化作为策略表示来生成动作，解决了模型基于强化学习算法中的“目标不匹配”问题，并在模仿学习任务上进行了性能基准测试。 |
| [^28] | [Latent Plan Transformer: Planning as Latent Variable Inference](https://arxiv.org/abs/2402.04647) | 潜在计划变换器（LPT）是一种新颖的模型，它通过将Transformer-based轨迹生成器和最终回报连接起来，并利用潜在空间进行规划。在学习中，通过对潜在变量的后验采样形成一致的抽象，在测试时通过推断潜在变量指导自回归策略。实验证明LPT能够从次优解中发现改进的决策。 |
| [^29] | [Discovery of the Hidden World with Large Language Models](https://arxiv.org/abs/2402.03941) | 通过使用大型语言模型，我们提出了COAT：因果表示助手，该助手从原始观测数据中提取潜在的因果因子，并将其转化为结构化数据，为探索隐藏世界提供了新的机会。 |
| [^30] | [The Challenges of the Nonlinear Regime for Physics-Informed Neural Networks](https://arxiv.org/abs/2402.03864) | 本文研究了针对物理引导神经网络（PINNs）在非线性偏微分方程（PDEs）求解中的挑战。通过理论分析和数值实验，我们说明了NTK在不同线性微分算子下的行为，并强调了采用二阶方法训练PINNs的优势。我们还探讨了二阶方法的收敛能力，并解决了谱偏差和收敛缓慢的问题。通过大量的数值实验和基准测试用例，我们验证了我们的方法的有效性。 |
| [^31] | [Online Feature Updates Improve Online (Generalized) Label Shift Adaptation](https://arxiv.org/abs/2402.03545) | 本文提出了一种名为OLS-OFU的新方法，通过在测试时利用无标签数据进行自监督学习和特征优化，以解决在线设置中的标签转移问题。研究结果表明，OLS-OFU在各种数据集和领域转移条件下都表现出了有效性和鲁棒性。 |
| [^32] | [Learning Best-in-Class Policies for the Predict-then-Optimize Framework](https://arxiv.org/abs/2402.03256) | 我们提出了一种新颖的决策感知替代损失函数家族，用于predict-then-optimize框架，并且通过数值证据证实了其在误设置下的优越性。 |
| [^33] | [Learning from Teaching Regularization: Generalizable Correlations Should be Easy to Imitate](https://arxiv.org/abs/2402.02769) | 通过从教学中学习（LoT）技术，我们提出了一种深度神经网络正则化技术，能够通过引入辅助学生模型来提升主模型的泛化性能。实验证明，LoT能有效识别具有泛化和可教授关系的信息。 |
| [^34] | [Latent Graph Diffusion: A Unified Framework for Generation and Prediction on Graphs](https://arxiv.org/abs/2402.02518) | 本文提出了一种统一框架，能够使用一个模型解决各级别和各类型的图学习任务，通过潜在图扩散模型生成和预测节点、边和图级别的特征，具有可证明的保证，并在实验证明了其有效性。 |
| [^35] | [AutoTimes: Autoregressive Time Series Forecasters via Large Language Models](https://arxiv.org/abs/2402.02370) | AutoTimes是一种基于大型语言模型的自回归时间序列预测器，利用语言模型的转换能力来处理时间序列数据，实现了与先前模型相当的性能。 |
| [^36] | [NM-FlowGAN: Modeling sRGB Noise with a Hybrid Approach based on Normalizing Flows and Generative Adversarial Networks](https://arxiv.org/abs/2312.10112) | NM-FlowGAN是一种利用生成对抗网络和正规化流的混合方法，旨在更准确地建模sRGB噪声，弥补了单一生成模型固有特性所带来的性能限制。 |
| [^37] | [Tree of Attacks: Jailbreaking Black-Box LLMs Automatically](https://arxiv.org/abs/2312.02119) | 提出了一种名为Tree of Attacks with Pruning (TAP)的自动化方法，用于生成只需要对目标大型语言模型进行黑盒访问的越狱方法，并通过思维树推理和修剪生成准确的越狱提示。 |
| [^38] | [Ensemble sampling for linear bandits: small ensembles suffice](https://arxiv.org/abs/2311.08376) | 该论文对随机线性赌臂环境中的集成抽样进行了首次实用和严格的分析，展示了在标准假设下，采用规模为$d \log T$的集成抽样可以获得接近$\sqrt{T}$阶的后悔，而不需要集成大小与$T$线性扩展。 |
| [^39] | [Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning.](http://arxiv.org/abs/2401.10862) | 本文研究了剪枝对齐的LLMs的保护措施，发现剪枝LLM参数可以显著增强其抵抗“越狱”提示攻击的能力，并且对其他LLM行为也可能有更普遍的效果。同时，引入了一个有害任务数据集，证明剪枝有助于集中注意力在与任务相关的标记上。突出的聊天模型表现出很高的易感性。 |
| [^40] | [Generative Fractional Diffusion Models.](http://arxiv.org/abs/2310.17638) | 该论文提出了一种基于分数阶扩散模型的生成模型，其中通过将分数布朗运动表示为奥恩斯坦-乌伦贝克过程的随机积分，实现了驱动噪声收敛到非马尔可夫过程的效果。这是第一个在具有无限二次变差的随机过程上构建生成模型的尝试。 |
| [^41] | [Controlling Continuous Relaxation for Combinatorial Optimization.](http://arxiv.org/abs/2309.16965) | 本文研究了在相对密集的图上组合优化问题中物理启发的图神经网络（PI-GNN）求解器的表现。通过数值实验，我们发现PI-GNN求解器在学习早期可能陷入所有变量为零的局部解。为了解决这个问题，我们通过控制连续性和离散性提出了一种改进方法。 |
| [^42] | [Subwords as Skills: Tokenization for Sparse-Reward Reinforcement Learning.](http://arxiv.org/abs/2309.04459) | 通过将行动空间离散化并采用分词技术，我们提出了一种在稀疏奖励强化学习中生成技巧的新方法。这种方法能够减少探索的难度，并在连续行动空间中达到良好的性能。 |
| [^43] | [A Cryogenic Memristive Neural Decoder for Fault-tolerant Quantum Error Correction.](http://arxiv.org/abs/2307.09463) | 本研究报告了一种基于内存计算的神经解码器推理加速器的设计和性能分析，该解码器用于容错量子错误纠正，旨在最小化解码时间并确保解码方法的可扩展性。 |
| [^44] | [Pre-trained transformer for adversarial purification.](http://arxiv.org/abs/2306.01762) | 本文提出了一个快速防御对抗性攻击的方案RaPiD（Rapid Plug-in Defender），通过预训练的Transformer微调来提纯对抗样本，使其逼近清洁数据分布，实验结果表明，在有限数据情况下，该方法优于最先进的方法。 |
| [^45] | [Augmented Modular Reinforcement Learning based on Heterogeneous Knowledge.](http://arxiv.org/abs/2306.01158) | 该论文提出了增强模块化强化学习（AMRL），使用仲裁器来选择异构模块，并无缝地整合不同类型的知识。该方法能够减缓强化学习中的一些低效问题，有望在深度强化学习领域得到应用。 |
| [^46] | [Enriching Disentanglement: Definitions to Metrics.](http://arxiv.org/abs/2305.11512) | 本文探究了解缠结表示学习的度量标准，并提出了基于丰富范畴论的系统方法，将方程定义转化为可比较的度量标准，我们推导出应用于测量解缠结属性的度量标准，并在合成数据上证明其有效性。 |
| [^47] | [GNNs,You can be Stronger,Deeper and Faster.](http://arxiv.org/abs/2305.05368) | 本文介绍了一种新的理解GNN表现能力的视角，并提出了一个新的采样节点级残差模块SDF，具有更好的表现能力。 |
| [^48] | [Infeasible Deterministic, Stochastic, and Variance-Reduction Algorithms for Optimization under Orthogonality Constraints.](http://arxiv.org/abs/2303.16510) | 本研究提出了一种简单迭代的Landing算法，可以在不强制执行正交约束的同时顺畅地吸引到正交约束流形上。我们扩展了这种算法以支持斯托菲尔（Stiefel）流形，并提供了随机和方差约减算法，这些方法与黎曼优化算法的收敛速度相同但需要更少的计算。 |
| [^49] | [On the tightness of information-theoretic bounds on generalization error of learning algorithms.](http://arxiv.org/abs/2303.14658) | 本文研究了学习算法泛化误差信息理论界限的紧密性。研究表明，通过适当的假设，可以在快速收敛速度下使用信息理论量$O(\lambda/n)$来上界估计泛化误差。 |
| [^50] | [Linking Across Data Granularity: Fitting Multivariate Hawkes Processes to Partially Interval-Censored Data.](http://arxiv.org/abs/2111.02062) | 这项研究介绍了Partial Mean Behavior Poisson (PMBP)过程，它是一种新的点过程，可以有效地建模时间戳和区间屏蔽数据，并成功恢复了MHP参数和谱半径。 |

# 详细

[^1]: 具有必要回溯的自然反事实

    Natural Counterfactuals With Necessary Backtracking

    [https://rss.arxiv.org/abs/2402.01607](https://rss.arxiv.org/abs/2402.01607)

    本研究提出了一种自然反事实框架和方法，通过优化控制回溯的范围，生成与实际世界的数据分布相匹配的自然反事实，从而改进了反事实推理。

    

    反事实推理对于人类认知非常重要，尤其对于提供解释和做出决策至关重要。尽管Judea Pearl的研究方法在理论上很优雅，但其生成反事实情景往往需要过于脱离实际情景的干预，因此难以实施。为了解决这个问题，我们提出了一种自然反事实的框架和一种根据实际世界数据分布生成自然反事实的方法。我们的方法提供了对反事实推理的改进，允许对因果前置变量进行改变以最小化与实际情景的偏差。为了生成自然反事实，我们引入了一种创新的优化框架，通过自然性准则允许但控制回溯的范围。实证实验表明了我们方法的有效性。

    Counterfactual reasoning is pivotal in human cognition and especially important for providing explanations and making decisions. While Judea Pearl's influential approach is theoretically elegant, its generation of a counterfactual scenario often requires interventions that are too detached from the real scenarios to be feasible. In response, we propose a framework of natural counterfactuals and a method for generating counterfactuals that are natural with respect to the actual world's data distribution. Our methodology refines counterfactual reasoning, allowing changes in causally preceding variables to minimize deviations from realistic scenarios. To generate natural counterfactuals, we introduce an innovative optimization framework that permits but controls the extent of backtracking with a naturalness criterion. Empirical experiments indicate the effectiveness of our method.
    
[^2]: 使用有限领域数据进行廉价推理的专用语言模型

    Specialized Language Models with Cheap Inference from Limited Domain Data

    [https://rss.arxiv.org/abs/2402.01093](https://rss.arxiv.org/abs/2402.01093)

    本研究提出了一种使用有限领域数据进行廉价推理的专用语言模型。在研究中，我们通过比较不同的机器学习方法，在推理成本的限制下找到了比训练非常大的基本转换器模型更优的替代方案。具体而言，在大型预训练预算下，超网络和专家混合模型的困惑度更好，而在大型专用预算下，训练重要样本数据集上的小型模型更具吸引力。

    

    大型语言模型已成为一种多才多艺的工具，但在缺乏大规模推理预算和大规模领域内训练集的任务中应用起来具有挑战性。本研究对这些限制进行了形式化，并区分了四个重要的变量：预训练预算（用于在目标领域出现之前进行训练），专用预算（用于在目标领域出现之后进行训练），推理预算和领域内训练集大小。在这些设置中，我们比较了机器学习文献中的不同方法。受到推理成本的限制，我们发现比训练非常大的基本转换器模型的标准做法更好的替代方案。特别是，我们发现超网络和专家混合模型在大型预训练预算下具有更好的困惑度，而在大型专用预算下，训练重要样本数据集上的小型模型更具吸引力。

    Large language models have emerged as a versatile tool but are challenging to apply to tasks lacking large inference budgets and large in-domain training sets. This work formalizes these constraints and distinguishes four important variables: the pretraining budget (for training before the target domain is known), the specialization budget (for training after the target domain is known), the inference budget, and the in-domain training set size. Across these settings, we compare different approaches from the machine learning literature. Limited by inference cost, we find better alternatives to the standard practice of training very large vanilla transformer models. In particular, we show that hyper-networks and mixture of experts have better perplexity for large pretraining budgets, while small models trained on importance sampled datasets are attractive for large specialization budgets.
    
[^3]: 具有前瞻特性的策略镜像下降算法

    Policy Mirror Descent with Lookahead

    [https://arxiv.org/abs/2403.14156](https://arxiv.org/abs/2403.14156)

    提出了一种新类别的策略镜像下降算法$h$-PMD，它通过在PMD更新规则中结合多步贪心策略改进和前瞻深度$h，以解决折扣无限时间视角下的马尔可夫决策过程。

    

    策略镜像下降（PMD）作为一种多功能算法框架，包括几种重要的策略梯度算法，如自然策略梯度，并与最先进的强化学习（RL）算法（如TRPO和PPO）相联系。PMD可以看作是实现正则化1步贪心策略改进的软策略迭代算法。然而，1步贪心策略可能不是最佳选择，最近在RL领域取得了显着的实证成功，如AlphaGo和AlphaZero已经证明，相对于多步骤，贪心方法可以超越它们的1步骤对应物。在这项工作中，我们提出了一种新类别的PMD算法，称为$h$-PMD，它将具有前瞻深度$h$的多步贪心策略改进结合到PMD更新规则中。为了解决折扣无限时间视角下的马尔可夫决策过程，其中折扣因子为$\gamma$，我们展示了$h$-PMD可以推广标准的PMD。

    arXiv:2403.14156v1 Announce Type: cross  Abstract: Policy Mirror Descent (PMD) stands as a versatile algorithmic framework encompassing several seminal policy gradient algorithms such as natural policy gradient, with connections with state-of-the-art reinforcement learning (RL) algorithms such as TRPO and PPO. PMD can be seen as a soft Policy Iteration algorithm implementing regularized 1-step greedy policy improvement. However, 1-step greedy policies might not be the best choice and recent remarkable empirical successes in RL such as AlphaGo and AlphaZero have demonstrated that greedy approaches with respect to multiple steps outperform their 1-step counterpart. In this work, we propose a new class of PMD algorithms called $h$-PMD which incorporates multi-step greedy policy improvement with lookahead depth $h$ to the PMD update rule. To solve discounted infinite horizon Markov Decision Processes with discount factor $\gamma$, we show that $h$-PMD which generalizes the standard PMD enj
    
[^4]: 离线多任务表示学习用于强化学习

    Offline Multitask Representation Learning for Reinforcement Learning

    [https://arxiv.org/abs/2403.11574](https://arxiv.org/abs/2403.11574)

    通过研究离线多任务低秩RL，提出了一种名为MORL的新算法，证明了在强化学习中应用学习到的表示的优势。

    

    我们研究了强化学习中的离线多任务表示学习，其中学习者被提供来自共享通用表示的不同任务的离线数据集，并被要求学习共享表示。我们从理论上对离线多任务低秩RL进行了研究，并提出了一种名为MORL的新算法，用于离线多任务表示学习。此外，我们在奖励免费、离线和在线场景中研究了下游RL，其中向代理引入了一个新任务，该任务与上游离线任务共享相同的表示。我们的理论结果表明，使用从上游离线任务中学到的表示的好处，而不是直接学习低秩模型的表示。

    arXiv:2403.11574v1 Announce Type: new  Abstract: We study offline multitask representation learning in reinforcement learning (RL), where a learner is provided with an offline dataset from different tasks that share a common representation and is asked to learn the shared representation. We theoretically investigate offline multitask low-rank RL, and propose a new algorithm called MORL for offline multitask representation learning. Furthermore, we examine downstream RL in reward-free, offline and online scenarios, where a new task is introduced to the agent that shares the same representation as the upstream offline tasks. Our theoretical results demonstrate the benefits of using the learned representation from the upstream offline task instead of directly learning the representation of the low-rank model.
    
[^5]: 叙事特征还是结构特征？研究大型语言模型以识别患心力衰竭风险的癌症患者

    Narrative Feature or Structured Feature? A Study of Large Language Models to Identify Cancer Patients at Risk of Heart Failure

    [https://arxiv.org/abs/2403.11425](https://arxiv.org/abs/2403.11425)

    使用大型语言模型结合新颖的叙述特征，能够有效识别癌症患者患心力衰竭的风险，表现优于传统机器学习模型和深度学习模型。

    

    癌症治疗已知会引入心毒性，对预后和生存率产生负面影响。识别患心力衰竭（HF）风险的癌症患者对于改善癌症治疗结果和安全性至关重要。本研究使用来自电子健康记录（EHRs）的机器学习（ML）模型，包括传统ML、时间感知长短期记忆（T-LSTM）和使用从结构化医学代码衍生的新颖叙述特征的大型语言模型（LLMs）来识别患HF风险的癌症患者。我们从佛罗里达大学健康中心识别了一组包括12,806名肺癌、乳腺癌和结直肠癌患者的癌症队列，其中1,602人在癌症后发展为HF。LLM GatorTron-3.9B取得了最佳的F1分数，比传统支持向量机高出39%，比T-LSTM深度学习模型高出7%，比广泛使用的Transformer模型BERT高出5.6%。

    arXiv:2403.11425v1 Announce Type: cross  Abstract: Cancer treatments are known to introduce cardiotoxicity, negatively impacting outcomes and survivorship. Identifying cancer patients at risk of heart failure (HF) is critical to improving cancer treatment outcomes and safety. This study examined machine learning (ML) models to identify cancer patients at risk of HF using electronic health records (EHRs), including traditional ML, Time-Aware long short-term memory (T-LSTM), and large language models (LLMs) using novel narrative features derived from the structured medical codes. We identified a cancer cohort of 12,806 patients from the University of Florida Health, diagnosed with lung, breast, and colorectal cancers, among which 1,602 individuals developed HF after cancer. The LLM, GatorTron-3.9B, achieved the best F1 scores, outperforming the traditional support vector machines by 39%, the T-LSTM deep learning model by 7%, and a widely used transformer model, BERT, by 5.6%. The analysi
    
[^6]: GreedyML：一种用于最大化子模函数的并行算法

    GreedyML: A Parallel Algorithm for Maximizing Submodular Functions

    [https://arxiv.org/abs/2403.10332](https://arxiv.org/abs/2403.10332)

    提出了一种用于在分布式存储多处理器上最大化子模函数的并行近似算法，以解决实际应用领域中海量数据集上的子模优化问题。

    

    我们描述了一种用于在分布式存储多处理器上最大化单调子模函数的并行近似算法。我们的工作受到在海量数据集上解决子模优化问题的需求的启发，用于实际应用领域，如数据摘要，机器学习和图稀疏化。我们的工作基于Barbosa、Ene、Nguyen和Ward（2015）提出的随机分布式RandGreedI算法。该算法通过将数据随机分区到所有处理器中，然后使用单个累积步骤计算分布式解决方案，其中所有处理器将它们的部分解决方案发送给一个处理器。然而，对于大问题，累积步骤可能超过处理器上可用的内存，并且执行累积的处理器可能成为计算瓶颈。

    arXiv:2403.10332v1 Announce Type: cross  Abstract: We describe a parallel approximation algorithm for maximizing monotone submodular functions subject to hereditary constraints on distributed memory multiprocessors. Our work is motivated by the need to solve submodular optimization problems on massive data sets, for practical applications in areas such as data summarization, machine learning, and graph sparsification. Our work builds on the randomized distributed RandGreedI algorithm, proposed by Barbosa, Ene, Nguyen, and Ward (2015). This algorithm computes a distributed solution by randomly partitioning the data among all the processors and then employing a single accumulation step in which all processors send their partial solutions to one processor. However, for large problems, the accumulation step could exceed the memory available on a processor, and the processor which performs the accumulation could become a computational bottleneck.   Here, we propose a generalization of the R
    
[^7]: 通过广义占有模型实现可迁移的强化学习

    Transferable Reinforcement Learning via Generalized Occupancy Models

    [https://arxiv.org/abs/2403.06328](https://arxiv.org/abs/2403.06328)

    通过广义占有模型，本研究提出了一种新颖的模型类别，保留了模型化强化学习的通用性，并避免了累积错误的问题。

    

    智能代理必须是通用的 - 具有快速适应和概括到不同任务的能力。在强化学习（RL）框架内，基于模型的RL算法学习世界的任务不可知动态模型，原则上使它们能够概括到任意奖励。然而，一步模型自然会受到累积错误的影响，使它们在具有长时间跨度和大状态空间的问题上失效。在这项工作中，我们提出了一类新型模型 - 广义占有模型（GOMs），保留了基于模型的RL的通用性，同时避免了累积性错误。GOMs的关键思想是在一个固定数据集的覆盖下，建模给定状态的所有可能长期结果的分布，以及实现给定状态的特定结果的策略。然后，这些模型可以迅速用于为任意新任务选择最优操作，而无需担心累积错误。

    arXiv:2403.06328v1 Announce Type: new  Abstract: Intelligent agents must be generalists - showing the ability to quickly adapt and generalize to varying tasks. Within the framework of reinforcement learning (RL), model-based RL algorithms learn a task-agnostic dynamics model of the world, in principle allowing them to generalize to arbitrary rewards. However, one-step models naturally suffer from compounding errors, making them ineffective for problems with long horizons and large state spaces. In this work, we propose a novel class of models - generalized occupancy models (GOMs) - that retain the generality of model-based RL while avoiding compounding error. The key idea behind GOMs is to model the distribution of all possible long-term outcomes from a given state under the coverage of a stationary dataset, along with a policy that realizes a particular outcome from the given state. These models can then quickly be used to select the optimal action for arbitrary new tasks, without hav
    
[^8]: ERBench：基于实体关系的可自动验证的大规模语言模型幻觉基准

    ERBench: An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Language Models

    [https://arxiv.org/abs/2403.05266](https://arxiv.org/abs/2403.05266)

    ERBench是一个基于实体关系的大型语言模型幻觉基准，通过自动转换任何关系数据库并构建可自动验证的问题，以支持复杂性评估和调试

    

    大型语言模型（LLMs）在各种应用中取得了前所未有的性能，然而它们的评估仍然是一个关键问题。现有的幻觉基准要么是静态的，要么缺乏可调整的复杂性进行彻底分析。我们认为利用现有的关系数据库是构建基准的一种有希望的方法，因为它们通过功能依赖关系可以准确描述知识。我们提出了ERBench，可以自动将任何关系数据库转换为基于实体关系（ER）模型的基准。我们的关键想法是使用数据库模式、记录和功能依赖来构建问题，以便可以自动验证。此外，我们使用外键约束来连接关系和构建多跳问题，这些问题可以任意复杂，用于调试LLMs的中间答案。最后，ERBench支持持续评估，多模态qu

    arXiv:2403.05266v1 Announce Type: cross  Abstract: Large language models (LLMs) have achieved unprecedented performance in various applications, yet their evaluation remains a critical issue. Existing hallucination benchmarks are either static or lack adjustable complexity for thorough analysis. We contend that utilizing existing relational databases is a promising approach for constructing benchmarks due to their accurate knowledge description via functional dependencies. We propose ERBench to automatically convert any relational database into a benchmark based on the entity-relationship (ER) model. Our key idea is to construct questions using the database schema, records, and functional dependencies such that they can be automatically verified. In addition, we use foreign key constraints to join relations and construct multihop questions, which can be arbitrarily complex and used to debug the intermediate answers of LLMs. Finally, ERBench supports continuous evaluation, multimodal qu
    
[^9]: 更快的邻域注意力: 在线程块级别减少自注意力的O(n^2)成本

    Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self Attention at the Threadblock Level

    [https://arxiv.org/abs/2403.04690](https://arxiv.org/abs/2403.04690)

    该研究提出了一种更快的邻域注意力机制，通过将注意力限制在最近的邻居之间来降低自注意力的计算复杂度，实现了显著的性能提升。

    

    邻域注意力通过限制每个标记的注意力范围为其最近的邻居来降低自注意力的成本。该限制由窗口大小和扩张因子参数化，介于线性投影和自注意力之间绘制了可能的注意力模式谱。邻域注意力，以及更一般地滑动窗口注意力模式，在基础设施方面长期受到限制，特别是在更高秩的空间（2-D和3-D），促使开发定制内核的发展，这些内核在功能或性能方面受限，如果不是两者都有。在这项工作中，我们首先展示邻域注意力可以表示为批量化的GEMM问题，类似于标准注意力，并为1-D和2-D邻域注意力实现它。与现有的简单内核相比，这些内核平均提供了分别是1-D和2-D邻域注意力的全精度延迟改进分别为895%和272%。

    arXiv:2403.04690v1 Announce Type: cross  Abstract: Neighborhood attention reduces the cost of self attention by restricting each token's attention span to its nearest neighbors. This restriction, parameterized by a window size and dilation factor, draws a spectrum of possible attention patterns between linear projection and self attention. Neighborhood attention, and more generally sliding window attention patterns, have long been bounded by infrastructure, particularly in higher-rank spaces (2-D and 3-D), calling for the development of custom kernels, which have been limited in either functionality, or performance, if not both. In this work, we first show that neighborhood attention can be represented as a batched GEMM problem, similar to standard attention, and implement it for 1-D and 2-D neighborhood attention. These kernels on average provide 895% and 272% improvement in full precision latency compared to existing naive kernels for 1-D and 2-D neighborhood attention respectively. 
    
[^10]: 通过插值进行推断：对比表示可证明启用规划和推断

    Inference via Interpolation: Contrastive Representations Provably Enable Planning and Inference

    [https://arxiv.org/abs/2403.04082](https://arxiv.org/abs/2403.04082)

    通过对比学习学到的时间序列数据表示遵循高斯马尔可夫链，从而启用规划和推断

    

    给定时间序列数据，我们如何回答诸如“未来会发生什么？”和“我们是如何到达这里的？”这类概率推断问题在观测值为高维时具有挑战性。本文展示了这些问题如何通过学习表示的紧凑闭式解决方案。关键思想是将对比学习的变体应用于时间序列数据。之前的工作已经表明，通过对比学习学到的表示编码了概率比。通过将之前的工作扩展以表明表示的边际分布是高斯分布，我们随后证明表示的联合分布也是高斯分布。这些结果共同表明，通过时间对比学习学到的表示遵循高斯马尔可夫链，一种图形模型，其中对表示进行的推断（例如预测、规划）对应于反演低维分布。

    arXiv:2403.04082v1 Announce Type: new  Abstract: Given time series data, how can we answer questions like "what will happen in the future?" and "how did we get here?" These sorts of probabilistic inference questions are challenging when observations are high-dimensional. In this paper, we show how these questions can have compact, closed form solutions in terms of learned representations. The key idea is to apply a variant of contrastive learning to time series data. Prior work already shows that the representations learned by contrastive learning encode a probability ratio. By extending prior work to show that the marginal distribution over representations is Gaussian, we can then prove that joint distribution of representations is also Gaussian. Taken together, these results show that representations learned via temporal contrastive learning follow a Gauss-Markov chain, a graphical model where inference (e.g., prediction, planning) over representations corresponds to inverting a low-
    
[^11]: Solution Simplex Clustering for Heterogeneous Federated Learning

    Solution Simplex Clustering for Heterogeneous Federated Learning

    [https://arxiv.org/abs/2403.03333](https://arxiv.org/abs/2403.03333)

    提出了Solution Simplex Clustered Federated Learning（SosicFL），通过学习解决方案单纯形的思想，为每个客户端分配单一区域，从而同时实现了学习本地和全局模型的目标。

    

    我们针对联邦学习（FL）中的一个主要挑战提出了解决方案，即在高度异构的客户分布下实现良好的性能。这种困难部分源于两个看似矛盾的目标：通过聚合来自客户端的信息来学习一个通用模型，以及学习应适应每个本地分布的本地个性化模型。在这项工作中，我们提出了Solution Simplex Clustered Federated Learning（SosicFL）来消除这种矛盾。基于学习解决方案单纯形的最新思想，SosicFL为每个客户端分配一个单纯形中的子区域，并执行FL来学习一个通用解决方案单纯形。这使得客户端模型在解决方案单纯形的自由度范围内具有其特征，同时实现了学习一个全局通用模型的目标。我们的实验证明，SosicFL改善了性能，并加速了全局和训练过程。

    arXiv:2403.03333v1 Announce Type: new  Abstract: We tackle a major challenge in federated learning (FL) -- achieving good performance under highly heterogeneous client distributions. The difficulty partially arises from two seemingly contradictory goals: learning a common model by aggregating the information from clients, and learning local personalized models that should be adapted to each local distribution. In this work, we propose Solution Simplex Clustered Federated Learning (SosicFL) for dissolving such contradiction. Based on the recent ideas of learning solution simplices, SosicFL assigns a subregion in a simplex to each client, and performs FL to learn a common solution simplex. This allows the client models to possess their characteristics within the degrees of freedom in the solution simplex, and at the same time achieves the goal of learning a global common model. Our experiments show that SosicFL improves the performance and accelerates the training process for global and 
    
[^12]: Epsilon-Greedy Thompson Sampling用于贝叶斯优化

    Epsilon-Greedy Thompson Sampling to Bayesian Optimization

    [https://arxiv.org/abs/2403.00540](https://arxiv.org/abs/2403.00540)

    将$\varepsilon$-greedy策略引入Thompson采样以改进贝叶斯优化中的开发功能，并实证表明其有效性。

    

    Thompson采样（TS）被认为是解决贝叶斯优化中开发-探索困境的解决方案。 虽然它通过随机生成和最大化高斯过程（GP）后验的样本路径来优先进行探索，但TS在每次执行探索后通过收集关于真实目标函数的信息来弱化其开发功能。 本研究将在TS中引入$\varepsilon$-greedy策略，这是一种在强化学习中被广泛应用的选择策略，以改进其开发功能。 我们首先描述了TS应用于BO的两个极端，即通用TS和样本平均TS。前者和后者分别提倡探索和开发。 然后我们使用$\varepsilon$-greedy策略在两个极端之间随机切换。 $\varepsilon \in (0,1)$的小值优先考虑开发，反之亦然。 我们实证表明$\varepsilon$-greedy T

    arXiv:2403.00540v1 Announce Type: new  Abstract: Thompson sampling (TS) serves as a solution for addressing the exploitation-exploration dilemma in Bayesian optimization (BO). While it prioritizes exploration by randomly generating and maximizing sample paths of Gaussian process (GP) posteriors, TS weakly manages its exploitation by gathering information about the true objective function after each exploration is performed. In this study, we incorporate the epsilon-greedy ($\varepsilon$-greedy) policy, a well-established selection strategy in reinforcement learning, into TS to improve its exploitation. We first delineate two extremes of TS applied for BO, namely the generic TS and a sample-average TS. The former and latter promote exploration and exploitation, respectively. We then use $\varepsilon$-greedy policy to randomly switch between the two extremes. A small value of $\varepsilon \in (0,1)$ prioritizes exploitation, and vice versa. We empirically show that $\varepsilon$-greedy T
    
[^13]: 利用贝叶斯神经网络对反应湍流封闭模型进行先验不确定性量化

    A Priori Uncertainty Quantification of Reacting Turbulence Closure Models using Bayesian Neural Networks

    [https://arxiv.org/abs/2402.18729](https://arxiv.org/abs/2402.18729)

    使用贝叶斯神经网络对反应流动模型中的不确定性进行量化，特别是在湍涡预混火焰动态中关键变量的建模方面取得重要进展

    

    虽然为大涡模拟（LES）中的子滤波尺度（SFS）提出了许多基于物理的封闭模型形式，但直接数值模拟（DNS）提供的大量数据为利用数据驱动建模技术创造了机会。尽管灵活，数据驱动模型仍取决于选择的数据集和模型的函数形式。采用这种模型的增加需要可靠地估计数据驱动模型中数据知识和超出分布范围的不确定性。在本工作中，我们利用贝叶斯神经网络（BNNs）来捕捉反应流动模型中的逻辑不确定性和偶然不确定性。特别是，我们模拟了在湍涡预混火焰动态中起关键作用的滤波进展变量标量耗散率。我们展示了BNN模型可以提供关于数据驱动封闭模型的不确定性结构的独特见解。我们还提出了一种方法来进行...

    arXiv:2402.18729v1 Announce Type: cross  Abstract: While many physics-based closure model forms have been posited for the sub-filter scale (SFS) in large eddy simulation (LES), vast amounts of data available from direct numerical simulation (DNS) create opportunities to leverage data-driven modeling techniques. Albeit flexible, data-driven models still depend on the dataset and the functional form of the model chosen. Increased adoption of such models requires reliable uncertainty estimates both in the data-informed and out-of-distribution regimes. In this work, we employ Bayesian neural networks (BNNs) to capture both epistemic and aleatoric uncertainties in a reacting flow model. In particular, we model the filtered progress variable scalar dissipation rate which plays a key role in the dynamics of turbulent premixed flames. We demonstrate that BNN models can provide unique insights about the structure of uncertainty of the data-driven closure models. We also propose a method for the
    
[^14]: 隐性偏见的下一个标记预测

    Implicit Bias of Next-Token Prediction

    [https://arxiv.org/abs/2402.18551](https://arxiv.org/abs/2402.18551)

    在基于梯度的优化器训练下的线性NTP模型中，确定了NTP可分离条件，并证明梯度下降能够实现其下界；同时证明了这些条件在过参数化时仍然成立。

    

    下一个标记预测（NTP）是训练大型语言模型的首选范式，它涉及预测序列中的下一个标记。与传统的独热分类不同，在NTP中，多个具有不同频率的标记在给定上下文后继。本文将NTP训练框架化为跨不同上下文的交叉熵最小化，每个上下文都与有限词汇表中的稀疏经验概率向量相关联。然后，它探讨了以下问题：当NTP训练损失达到其下界（熵）时，基于梯度的优化器是否会对具有特定结构的解决方案存在偏见？具体地，对于使用梯度下降（GD）训练的线性NTP模型，我们做出以下贡献：首先，我们确定了数据上的NTP可分离条件，在这些条件下，GD能够达到其下界。我们还证明了这些条件在过参数化时仍成立。

    arXiv:2402.18551v1 Announce Type: cross  Abstract: Next-token prediction (NTP), the go-to training paradigm in training large language models, involves predicting the next token in a sequence. Departing from traditional one-hot classification, in NTP, multiple tokens with varying frequencies follow each given context. This work frames NTP training as cross-entropy minimization over distinct contexts, each associated with a sparse empirical probability vector across a finite vocabulary. It then addresses the following question: do gradient-based optimizers exhibit a bias towards solutions with specific structure as the NTP training loss reaches its lower bound (entropy)? Specifically, for linear NTP models trained using gradient descent (GD), we make the following contributions: Firstly, we determine NTP-separability conditions on the data, under which GD can attain its lower bound. We also demonstrate that these conditions hold under overparameterization. Secondly, we establish that th
    
[^15]: Conformer：将连续注意力嵌入视觉Transformer用于天气预测

    Conformer: Embedding Continuous Attention in Vision Transformer for Weather Forecasting

    [https://arxiv.org/abs/2402.17966](https://arxiv.org/abs/2402.17966)

    Conformer是一种用于天气预测的时空连续视觉Transformer，通过在多头注意力机制中实现连续性来学习时间上的连续天气演变。

    

    操作性天气预报系统依赖于计算昂贵的基于物理的模型。尽管基于Transformer的模型在天气预测中显示出了显著潜力，但Transformers是离散模型，限制了其学习动态天气系统连续时空特征的能力。我们通过Conformer解决了这个问题，这是一种用于天气预测的时空连续视觉Transformer。Conformer旨在通过在多头注意力机制中实现连续性来学习时间上的连续天气演变。注意力机制被编码为Transformer架构中的可微分函数，以建模复杂的天气动态。我们将Conformer与最先进的数值天气预报（NWP）模型和几种基于深度学习的天气预测模型进行了评估。Conformer在所有前导时间上优于一些现有的数据驱动模型

    arXiv:2402.17966v1 Announce Type: new  Abstract: Operational weather forecasting system relies on computationally expensive physics-based models. Although Transformers-based models have shown remarkable potential in weather forecasting, Transformers are discrete models which limit their ability to learn the continuous spatio-temporal features of the dynamical weather system. We address this issue with Conformer, a spatio-temporal Continuous Vision Transformer for weather forecasting. Conformer is designed to learn the continuous weather evolution over time by implementing continuity in the multi-head attention mechanism. The attention mechanism is encoded as a differentiable function in the transformer architecture to model the complex weather dynamics. We evaluate Conformer against a state-of-the-art Numerical Weather Prediction (NWP) model and several deep learning based weather forecasting models. Conformer outperforms some of the existing data-driven models at all lead times while 
    
[^16]: 双空间优化：通过潜在提示变换器改进分子序列设计

    Dual-Space Optimization: Improved Molecule Sequence Design by Latent Prompt Transformer

    [https://arxiv.org/abs/2402.17179](https://arxiv.org/abs/2402.17179)

    提出了双空间优化（DSO）方法，通过整合潜在空间采样和数据空间选择，使用Latent Prompt Transformer (LPT)生成模型，解决了分子设计中的关键问题，取得了在不同任务中的性能优势。

    

    设计具有理想性质（如药物样性和对蛋白靶点的高结合亲和力）的分子是一个具有挑战性的问题。在本文中，我们提出了双空间优化（DSO）方法，该方法整合了潜在空间采样和数据空间选择来解决这一问题。DSO通过迭代更新潜在空间生成模型和合成数据集，逐步将生成模型和合成数据移向所需性质数值的区域。我们的生成模型采用潜在提示变换器（LPT）的形式，其中潜在向量充当因果变换器的提示。我们广泛的实验表明了提出方法的有效性，该方法在单目标、多目标和约束分子设计任务中树立了新的性能基准。

    arXiv:2402.17179v1 Announce Type: new  Abstract: Designing molecules with desirable properties, such as drug-likeliness and high binding affinities towards protein targets, is a challenging problem. In this paper, we propose the Dual-Space Optimization (DSO) method that integrates latent space sampling and data space selection to solve this problem. DSO iteratively updates a latent space generative model and a synthetic dataset in an optimization process that gradually shifts the generative model and the synthetic data towards regions of desired property values. Our generative model takes the form of a Latent Prompt Transformer (LPT) where the latent vector serves as the prompt of a causal transformer. Our extensive experiments demonstrate effectiveness of the proposed method, which sets new performance benchmarks across single-objective, multi-objective and constrained molecule design tasks.
    
[^17]: NeuralThink: 在一般任务中进行外推的算法综合

    NeuralThink: Algorithm Synthesis that Extrapolates in General Tasks

    [https://arxiv.org/abs/2402.15393](https://arxiv.org/abs/2402.15393)

    NeuralThink 是一种新的递归架构，可以一贯地对对称和不对称任务进行外推，相较于之前的最先进的深度思维架构在稳定地从较小的训练规模对大观测进行外推方面表现出更好的性能。

    

    虽然机器学习方法擅长模式识别，但在可扩展的算法方式上处理复杂的推理任务时仍然面临困难。最近的深度思维方法展现了学习可以外推的算法的潜力：在较小的环境中学习并在较大的环境中执行学到的算法。然而，这些工作局限于对称任务，即输入和输出的维度相同。为了填补这一空白，我们提出了 NeuralThink，一种新的递归架构，可以一贯地对对称和不对称任务进行外推，其中输入和输出的维度不同。我们提供了一个新颖的不对称任务外推基准。我们展示了 NeuralThink 在稳定地从较小的训练规模对大观测进行外推方面一直优于之前的最先进的深度思维架构。

    arXiv:2402.15393v1 Announce Type: cross  Abstract: While machine learning methods excel at pattern recognition, they struggle with complex reasoning tasks in a scalable, algorithmic manner. Recent Deep Thinking methods show promise in learning algorithms that extrapolate: learning in smaller environments and executing the learned algorithm in larger environments. However, these works are limited to symmetrical tasks, where the input and output dimensionalities are the same. To address this gap, we propose NeuralThink, a new recurrent architecture that can consistently extrapolate to both symmetrical and asymmetrical tasks, where the dimensionality of the input and output are different. We contribute with a novel benchmark of asymmetrical tasks for extrapolation. We show that NeuralThink consistently outperforms the prior state-of-the-art Deep Thinking architectures, in regards to stable extrapolation to large observations from smaller training sizes.
    
[^18]: 具有统一最后迭代保证的赌博算法实现接近最优遗憾

    Achieving Near-Optimal Regret for Bandit Algorithms with Uniform Last-Iterate Guarantee

    [https://arxiv.org/abs/2402.12711](https://arxiv.org/abs/2402.12711)

    本文引入了统一最后迭代(ULI)保证这一更强的性能度量，同时证明接近最优的ULI保证直接导致了在各种性能度量上接近最优的累积性能。

    

    现有的赌博算法性能度量，如遗憾、PAC界限或统一PAC(Dann等人，2017)，通常评估累积性能，同时允许在任意有限时间t内玩弱劣的臂。这种行为在高风险应用中可能造成严重损失。本文介绍了一种更强的性能度量，统一最后迭代(ULI)保证，捕捉赌博算法的累积和瞬时性能。具体来说，ULI表征了瞬时性能，因为它确保所玩弱劣臂的每轮遗憾受到一个函数的限制，该函数随着（大）轮次t单调递减，在有足够样本可用时防止重复访问劣质臂。我们证明，接近最优的ULI保证直接意味着在上述性能度量中实现接近最优的累积性能。为了研究ULI在有限臂集上的可达性

    arXiv:2402.12711v1 Announce Type: new  Abstract: Existing performance measures for bandit algorithms such as regret, PAC bounds, or uniform-PAC (Dann et al., 2017), typically evaluate the cumulative performance, while allowing the play of an arbitrarily bad arm at any finite time t. Such a behavior can be highly detrimental in high-stakes applications. This paper introduces a stronger performance measure, the uniform last-iterate (ULI) guarantee, capturing both cumulative and instantaneous performance of bandit algorithms. Specifically, ULI characterizes the instantaneous performance since it ensures that the per-round regret of the played arm is bounded by a function, monotonically decreasing w.r.t. (large) round t, preventing revisits to bad arms when sufficient samples are available. We demonstrate that a near-optimal ULI guarantee directly implies near-optimal cumulative performance across aforementioned performance measures. To examine the achievability of ULI in the finite arm se
    
[^19]: 大型语言模型对图形召回的微结构和准确性

    Microstructures and Accuracy of Graph Recall by Large Language Models

    [https://arxiv.org/abs/2402.11821](https://arxiv.org/abs/2402.11821)

    本研究首次系统研究了大型语言模型对图形召回的准确性和偏见微结构，探讨了它们与人类的异同以及对其他图形推理任务的影响。

    

    图形数据对许多应用至关重要，其中很多数据以文本格式描述关系。因此，准确地召回和编码先前文本中描述的图形是大型语言模型(LLMs)需要展示的基本但关键能力，以执行涉及图形结构信息的推理任务。人类在图形召回方面的表现已被认知科学家研究了几十年，发现其经常呈现与人类处理社会关系一致的某些结构性偏见模式。然而，迄今为止，我们很少了解LLMs在类似图形召回任务中的行为：它们召回的图形是否也呈现某些偏见模式，如果是，它们与人类的表现有何不同并如何影响其他图形推理任务？在这项研究中，我们进行了第一次对LLMs进行图形召回的系统研究，研究其准确性和偏见微结构（局部结构）。

    arXiv:2402.11821v1 Announce Type: cross  Abstract: Graphs data is crucial for many applications, and much of it exists in the relations described in textual format. As a result, being able to accurately recall and encode a graph described in earlier text is a basic yet pivotal ability that LLMs need to demonstrate if they are to perform reasoning tasks that involve graph-structured information. Human performance at graph recall by has been studied by cognitive scientists for decades, and has been found to often exhibit certain structural patterns of bias that align with human handling of social relationships. To date, however, we know little about how LLMs behave in analogous graph recall tasks: do their recalled graphs also exhibit certain biased patterns, and if so, how do they compare with humans and affect other graph reasoning tasks? In this work, we perform the first systematical study of graph recall by LLMs, investigating the accuracy and biased microstructures (local structura
    
[^20]: 分类扩散模型

    Classification Diffusion Models

    [https://arxiv.org/abs/2402.10095](https://arxiv.org/abs/2402.10095)

    提出了一种分类扩散模型（CDMs），该模型采用了去噪扩散模型（DDM）的形式，并利用一个分类器来预测加在干净信号上的噪声量，取得了在图像、视频和音频生成方面的最先进结果。

    

    arXiv：2402.10095v1 公告类型：新的 摘要：一种学习数据分布的突出方法家族依赖于密度比估计（DRE），其中模型被训练来$\textit{分类}$数据样本和来自某个参考分布的样本。这些技术在简单的低维环境中取得了成功，但在复杂的高维数据（如图像）中无法取得良好的结果。学习分布的另一种方法家族是去噪扩散模型（DDM），其中模型被训练来$\textit{去噪}$数据样本。这些方法在图像、视频和音频生成方面取得了最先进的结果。在这项工作中，我们提出了$\textit{分类扩散模型}$（CDMs），这是一种生成技术，它采用了DDM的去噪基本形式，同时利用一个分类器来预测加在干净信号上的噪声量，类似于DRE方法。我们的方法基于这样一个观察，即MSE最优化的d

    arXiv:2402.10095v1 Announce Type: new  Abstract: A prominent family of methods for learning data distributions relies on density ratio estimation (DRE), where a model is trained to $\textit{classify}$ between data samples and samples from some reference distribution. These techniques are successful in simple low-dimensional settings but fail to achieve good results on complex high-dimensional data, like images. A different family of methods for learning distributions is that of denoising diffusion models (DDMs), in which a model is trained to $\textit{denoise}$ data samples. These approaches achieve state-of-the-art results in image, video, and audio generation. In this work, we present $\textit{Classification Diffusion Models}$ (CDMs), a generative technique that adopts the denoising-based formalism of DDMs while making use of a classifier that predicts the amount of noise added to a clean signal, similarly to DRE methods. Our approach is based on the observation that an MSE-optimal d
    
[^21]: 未经本人同意的训练：在训练代码的语言模型中检测代码包含

    Trained Without My Consent: Detecting Code Inclusion In Language Models Trained on Code

    [https://arxiv.org/abs/2402.09299](https://arxiv.org/abs/2402.09299)

    这项研究关注如何在训练代码的语言模型中检测代码包含，以解决使用这些模型进行代码审计时的版权侵权问题。

    

    代码审计通过验证开发的代码是否符合标准、法规和版权保护，确保其不包含来自受保护来源的代码。在软件开发过程中，大型语言模型(LLMs)作为编码助手的出现给代码审计带来了新的挑战。训练这些模型的数据集主要来自公开可用的来源。这引发了知识产权侵权问题，因为开发者的代码已包含在数据集中。因此，使用LLMs开发的代码审计具有挑战性，因为我们无法准确确定开发过程中使用的LLM是否已经在特定的受版权保护的代码上进行了训练，因为我们无法获得这些模型的训练数据集。鉴于训练数据集的保密性，传统的代码克隆检测等方法无法确保版权侵权。

    arXiv:2402.09299v1 Announce Type: cross Abstract: Code auditing ensures that the developed code adheres to standards, regulations, and copyright protection by verifying that it does not contain code from protected sources. The recent advent of Large Language Models (LLMs) as coding assistants in the software development process poses new challenges for code auditing. The dataset for training these models is mainly collected from publicly available sources. This raises the issue of intellectual property infringement as developers' codes are already included in the dataset. Therefore, auditing code developed using LLMs is challenging, as it is difficult to reliably assert if an LLM used during development has been trained on specific copyrighted codes, given that we do not have access to the training datasets of these models. Given the non-disclosure of the training datasets, traditional approaches such as code clone detection are insufficient for asserting copyright infringement. To add
    
[^22]: 过渡受限的贝叶斯优化在马尔可夫决策过程中的应用

    Transition Constrained Bayesian Optimization via Markov Decision Processes

    [https://arxiv.org/abs/2402.08406](https://arxiv.org/abs/2402.08406)

    本文介绍了一种过渡受限的贝叶斯优化方法，通过马尔可夫决策过程的框架，使用强化学习解决了由于转变约束导致的搜索空间依赖历史的问题，并在化学反应器优化、信息化路径规划、机器校准等领域进行了应用。

    

    贝叶斯优化是一种优化黑盒函数的方法。传统上，它关注的是可以任意查询搜索空间的情况。然而，许多现实生活中的问题并不具备这种灵活性；特别是，下一个查询的搜索空间可能取决于先前的查询。物理科学领域的例子中存在一些挑战，如局部移动限制、特定变量的单调性要求以及转变影响测量精度。总之，这些过渡约束需要一种规划方法。本文通过马尔可夫决策过程的框架扩展了贝叶斯优化，通过强化学习迭代地解决我们目标的一个可行线性化，从而获得能够提前规划长时间跨度的策略。得到的策略可能是依赖历史的和非马尔可夫的。我们展示了在化学反应器优化、信息化路径规划、机器校准等方面的应用。

    Bayesian optimization is a methodology to optimize black-box functions. Traditionally, it focuses on the setting where you can arbitrarily query the search space. However, many real-life problems do not offer this flexibility; in particular, the search space of the next query may depend on previous ones. Example challenges arise in the physical sciences in the form of local movement constraints, required monotonicity in certain variables, and transitions influencing the accuracy of measurements. Altogether, such transition constraints necessitate a form of planning. This work extends Bayesian optimization via the framework of Markov Decision Processes, iteratively solving a tractable linearization of our objective using reinforcement learning to obtain a policy that plans ahead over long horizons. The resulting policy is potentially history-dependent and non-Markovian. We showcase applications in chemical reactor optimization, informative path planning, machine calibration, and other s
    
[^23]: SMX: 专家迭代的顺序蒙特卡洛规划

    SMX: Sequential Monte Carlo Planning for Expert Iteration

    [https://arxiv.org/abs/2402.07963](https://arxiv.org/abs/2402.07963)

    这项研究介绍了一种名为SMX的顺序蒙特卡洛规划算法，它利用可扩展的方法创建了有效的自我学习机制。它适用于离散和连续动作空间的环境，具有高并行性能。

    

    发展能够在决策和学习过程中利用规划能力的智能体对于人工智能的进步至关重要。最近的研究已经证明了树状搜索方法和自我对弈学习机制的有效性。然而，由于搜索过程的顺序性质，这些方法通常面临扩展性挑战。虽然实践工程解决方案可以部分克服这个问题，但仍需要大量计算资源，这限制了它们的适用性。在本文中，我们介绍一种名为SMX的基于模型的计划算法，它利用可扩展的顺序蒙特卡洛方法创建了一种有效的自我学习机制。SMX基于控制作为推断的理论框架，并受益于坚实的理论基础。它基于采样的搜索方法使其适应具有离散和连续动作空间的环境。此外，SMX允许高度并行化并可以运行于各类计算机设备上。

    Developing agents that can leverage planning abilities during their decision and learning processes is critical to the advancement of Artificial Intelligence. Recent works have demonstrated the effectiveness of combining tree-based search methods and self-play learning mechanisms. Yet, these methods typically face scaling challenges due to the sequential nature of their search. While practical engineering solutions can partly overcome this, they still demand extensive computational resources, which hinders their applicability. In this paper, we introduce SMX, a model-based planning algorithm that utilises scalable Sequential Monte Carlo methods to create an effective self-learning mechanism. Grounded in the theoretical framework of control as inference, SMX benefits from robust theoretical underpinnings. Its sampling-based search approach makes it adaptable to environments with both discrete and continuous action spaces. Furthermore, SMX allows for high parallelisation and can run on h
    
[^24]: 自洽的符合预测

    Self-Consistent Conformal Prediction

    [https://arxiv.org/abs/2402.07307](https://arxiv.org/abs/2402.07307)

    自洽的符合预测方法能够提供既符合校准的预测又符合以模型预测的动作为条件的预测区间，为决策者提供了严格的、针对具体动作的决策保证。

    

    在机器学习指导下的决策中，决策者通常在具有相同预测结果的情境中采取相同的行动。符合预测帮助决策者量化动作的结果不确定性，从而实现更好的风险管理。受这种观点的启发，我们引入了自洽的符合预测，它产生了既符合Venn-Abers校准的预测，又符合以模型预测引发的动作为条件的符合预测区间。我们的方法可以后验地应用于任何黑盒预测器，提供严格的、针对具体动作的决策保证。数值实验表明，我们的方法在区间的效率和条件的有效性之间达到了平衡。

    In decision-making guided by machine learning, decision-makers often take identical actions in contexts with identical predicted outcomes. Conformal prediction helps decision-makers quantify outcome uncertainty for actions, allowing for better risk management. Inspired by this perspective, we introduce self-consistent conformal prediction, which yields both Venn-Abers calibrated predictions and conformal prediction intervals that are valid conditional on actions prompted by model predictions. Our procedure can be applied post-hoc to any black-box predictor to provide rigorous, action-specific decision-making guarantees. Numerical experiments show our approach strikes a balance between interval efficiency and conditional validity.
    
[^25]: 学习严格凸的随机合作博弈的预期核心

    Learning the Expected Core of Strictly Convex Stochastic Cooperative Games

    [https://arxiv.org/abs/2402.07067](https://arxiv.org/abs/2402.07067)

    本文研究了随机合作博弈中严格凸情况下，学习预期核心的问题。我们提出了一种名为\texttt{Common-Points-Picking}的算法，在多项式数量的样本给定的情况下，以高概率返回一个稳定分配。

    

    奖励分配，也称为信用分配问题，是经济学、工程学和机器学习中的重要主题。信用分配中的一个重要概念是核心，它是稳定分配的集合，其中没有代理有动机从大联盟中偏离。在本文中，我们考虑了随机合作博弈的稳定分配学习问题，其中奖励函数被描述为具有未知分布的随机变量。在每一轮中，给定一个返回查询联盟的随机奖励的oracle，我们的目标是学习预期核心，即在期望上稳定的分配集合。在严格凸博弈类中，我们提出了一种名为\texttt{Common-Points-Picking}的算法，它在多项式数量的样本给定的情况下，以高概率返回一个稳定分配。我们的算法分析涉及到凸几何中的几个新结果的发展，包括一个

    Reward allocation, also known as the credit assignment problem, has been an important topic in economics, engineering, and machine learning. An important concept in credit assignment is the core, which is the set of stable allocations where no agent has the motivation to deviate from the grand coalition. In this paper, we consider the stable allocation learning problem of stochastic cooperative games, where the reward function is characterised as a random variable with an unknown distribution. Given an oracle that returns a stochastic reward for an enquired coalition each round, our goal is to learn the expected core, that is, the set of allocations that are stable in expectation. Within the class of strictly convex games, we present an algorithm named \texttt{Common-Points-Picking} that returns a stable allocation given a polynomial number of samples, with high probability. The analysis of our algorithm involves the development of several new results in convex geometry, including an e
    
[^26]: 进取的鲍勃通过提示对抗调整抵制越狱行为

    Studious Bob Fight Back Against Jailbreaking via Prompt Adversarial Tuning

    [https://arxiv.org/abs/2402.06255](https://arxiv.org/abs/2402.06255)

    本文提出了一种名为Prompt Adversarial Tuning (PAT)的方法，通过训练一个防御控制机制并将其作为前缀嵌入到用户提示中，实现对大型语言模型（LLMs）的越狱行为的防御。实验证明该方法在保护LLMs免受产生有害信息的影响方面效果显著。

    

    尽管大型语言模型（LLM）在各种应用中取得了巨大的成功，但它们也容易受到特定提示的影响，从而绕过内置的安全措施并提供危险或非法内容，这种现象被称为越狱行为。为了保护LLMs免受产生有害信息的影响，提出了各种防御策略，其中大多数集中在内容过滤或模型的对抗训练方面。在本文中，我们提出了一种名为Prompt Adversarial Tuning（PAT）的方法，通过训练一个防御控制机制并将其作为前缀嵌入到用户提示中来实现我们的防御策略。我们设计了一个类似对抗训练的训练过程，以实现我们的优化目标，交替更新攻击和防御控制机制。据我们所知，我们是第一个从提示调整的角度实施防御的人。一旦应用，我们的方法几乎不会影响LLMs的操作效率。实验表明我们的方法在抵御越狱行为方面具有良好的效果。

    Although Large Language Models (LLMs) have achieved tremendous success in various applications, they are also susceptible to certain prompts that can induce them to bypass built-in safety measures and provide dangerous or illegal content, a phenomenon known as jailbreak. To protect LLMs from producing harmful information, various defense strategies are proposed, with most focusing on content filtering or adversarial training of models. In this paper, we propose an approach named Prompt Adversarial Tuning (PAT) to train a defense control mechanism, which is then embedded as a prefix to user prompts to implement our defense strategy. We design a training process similar to adversarial training to achieve our optimized goal, alternating between updating attack and defense controls. To our knowledge, we are the first to implement defense from the perspective of prompt tuning. Once employed, our method will hardly impact the operational efficiency of LLMs. Experiments show that our method i
    
[^27]: DiffTOP: 可微分轨迹优化在强化学习和模仿学习中的应用

    DiffTOP: Differentiable Trajectory Optimization for Deep Reinforcement and Imitation Learning

    [https://arxiv.org/abs/2402.05421](https://arxiv.org/abs/2402.05421)

    DiffTOP使用可微分轨迹优化作为策略表示来生成动作，解决了模型基于强化学习算法中的“目标不匹配”问题，并在模仿学习任务上进行了性能基准测试。

    

    本文介绍了DiffTOP，它利用可微分轨迹优化作为策略表示，为深度强化学习和模仿学习生成动作。轨迹优化是一种在控制领域中广泛使用的算法，由成本和动力学函数参数化。我们的方法的关键是利用了最近在可微分轨迹优化方面的进展，使得可以计算损失对于轨迹优化的参数的梯度。因此，轨迹优化的成本和动力学函数可以端到端地学习。DiffTOP解决了之前模型基于强化学习算法中的“目标不匹配”问题，因为DiffTOP中的动力学模型通过轨迹优化过程中的策略梯度损失直接最大化任务性能。我们还对DiffTOP在标准机器人操纵任务套件中进行了模仿学习性能基准测试。

    This paper introduces DiffTOP, which utilizes Differentiable Trajectory OPtimization as the policy representation to generate actions for deep reinforcement and imitation learning. Trajectory optimization is a powerful and widely used algorithm in control, parameterized by a cost and a dynamics function. The key to our approach is to leverage the recent progress in differentiable trajectory optimization, which enables computing the gradients of the loss with respect to the parameters of trajectory optimization. As a result, the cost and dynamics functions of trajectory optimization can be learned end-to-end. DiffTOP addresses the ``objective mismatch'' issue of prior model-based RL algorithms, as the dynamics model in DiffTOP is learned to directly maximize task performance by differentiating the policy gradient loss through the trajectory optimization process. We further benchmark DiffTOP for imitation learning on standard robotic manipulation task suites with high-dimensional sensory
    
[^28]: 潜在计划变换器：规划作为潜在变量推断

    Latent Plan Transformer: Planning as Latent Variable Inference

    [https://arxiv.org/abs/2402.04647](https://arxiv.org/abs/2402.04647)

    潜在计划变换器（LPT）是一种新颖的模型，它通过将Transformer-based轨迹生成器和最终回报连接起来，并利用潜在空间进行规划。在学习中，通过对潜在变量的后验采样形成一致的抽象，在测试时通过推断潜在变量指导自回归策略。实验证明LPT能够从次优解中发现改进的决策。

    

    在追求长期回报的任务中，规划变得必要。我们研究了利用离线强化学习的数据集进行规划的生成建模。具体来说，我们确定了在缺乏逐步奖励的情况下的时间一致性是一个关键的技术挑战。我们引入了潜在计划变换器（LPT），这是一种新颖的模型，它利用了一个潜在空间来连接基于Transformer的轨迹生成器和最终回报。LPT可以通过轨迹-回报对的最大似然估计来学习。在学习中，通过对潜在变量的后验采样，尽管有限的上下文，自然地聚集子轨迹以形成一致的抽象。在测试时，通过预期回报对潜在变量进行推断，实现了规划作为推断的思想。然后，它在整个回合中指导自回归策略，起到一个计划的作用。我们的实验表明，LPT可以从次优解中发现改进的决策。

    In tasks aiming for long-term returns, planning becomes necessary. We study generative modeling for planning with datasets repurposed from offline reinforcement learning. Specifically, we identify temporal consistency in the absence of step-wise rewards as one key technical challenge. We introduce the Latent Plan Transformer (LPT), a novel model that leverages a latent space to connect a Transformer-based trajectory generator and the final return. LPT can be learned with maximum likelihood estimation on trajectory-return pairs. In learning, posterior sampling of the latent variable naturally gathers sub-trajectories to form a consistent abstraction despite the finite context. During test time, the latent variable is inferred from an expected return before policy execution, realizing the idea of planning as inference. It then guides the autoregressive policy throughout the episode, functioning as a plan. Our experiments demonstrate that LPT can discover improved decisions from suboptima
    
[^29]: 用大型语言模型探索隐藏世界

    Discovery of the Hidden World with Large Language Models

    [https://arxiv.org/abs/2402.03941](https://arxiv.org/abs/2402.03941)

    通过使用大型语言模型，我们提出了COAT：因果表示助手，该助手从原始观测数据中提取潜在的因果因子，并将其转化为结构化数据，为探索隐藏世界提供了新的机会。

    

    科学起源于从已知事实和观察中发现新的因果知识。传统的因果发现方法主要依赖于高质量的测量变量，通常由人类专家提供，以找到因果关系。然而，在许多现实世界的应用中，因果变量通常无法获取。大型语言模型（LLMs）的崛起为从原始观测数据中发现高级隐藏变量提供了新的机会。因此，我们介绍了COAT：因果表示助手。COAT将LLMs作为因素提供器引入，提取出来自非结构化数据的潜在因果因子。此外，LLMs还可以被指示提供用于收集数据值（例如注释标准）的额外信息，并将原始非结构化数据进一步解析为结构化数据。注释数据将被输入到...

    Science originates with discovering new causal knowledge from a combination of known facts and observations. Traditional causal discovery approaches mainly rely on high-quality measured variables, usually given by human experts, to find causal relations. However, the causal variables are usually unavailable in a wide range of real-world applications. The rise of large language models (LLMs) that are trained to learn rich knowledge from the massive observations of the world, provides a new opportunity to assist with discovering high-level hidden variables from the raw observational data. Therefore, we introduce COAT: Causal representatiOn AssistanT. COAT incorporates LLMs as a factor proposer that extracts the potential causal factors from unstructured data. Moreover, LLMs can also be instructed to provide additional information used to collect data values (e.g., annotation criteria) and to further parse the raw unstructured data into structured data. The annotated data will be fed to a
    
[^30]: 针对物理引导神经网络的非线性阶段的挑战

    The Challenges of the Nonlinear Regime for Physics-Informed Neural Networks

    [https://arxiv.org/abs/2402.03864](https://arxiv.org/abs/2402.03864)

    本文研究了针对物理引导神经网络（PINNs）在非线性偏微分方程（PDEs）求解中的挑战。通过理论分析和数值实验，我们说明了NTK在不同线性微分算子下的行为，并强调了采用二阶方法训练PINNs的优势。我们还探讨了二阶方法的收敛能力，并解决了谱偏差和收敛缓慢的问题。通过大量的数值实验和基准测试用例，我们验证了我们的方法的有效性。

    

    神经切向核（NTK）视角是研究无限宽度情况下物理引导神经网络（PINNs）训练动态的有价值方法。我们利用这个视角，并着重研究PINNs求解非线性偏微分方程（PDEs）的情况。我们提供了关于NTK在不同线性微分算子下的不同行为的理论结果。此外，根据我们的理论结果，我们强调采用二阶方法训练PINNs的优势。此外，我们还探讨了二阶方法的收敛能力，并解决了谱偏差和收敛缓慢的挑战。所有的理论结果都得到了线性和非线性PDEs的数值示例的支持，我们还通过基准测试用例验证了我们的训练方法。

    The Neural Tangent Kernel (NTK) viewpoint represents a valuable approach to examine the training dynamics of Physics-Informed Neural Networks (PINNs) in the infinite width limit. We leverage this perspective and focus on the case of nonlinear Partial Differential Equations (PDEs) solved by PINNs. We provide theoretical results on the different behaviors of the NTK depending on the linearity of the differential operator. Moreover, inspired by our theoretical results, we emphasize the advantage of employing second-order methods for training PINNs. Additionally, we explore the convergence capabilities of second-order methods and address the challenges of spectral bias and slow convergence. Every theoretical result is supported by numerical examples with both linear and nonlinear PDEs, and we validate our training method on benchmark test cases.
    
[^31]: 在线特征更新改善在线（广义）标签转移适应问题

    Online Feature Updates Improve Online (Generalized) Label Shift Adaptation

    [https://arxiv.org/abs/2402.03545](https://arxiv.org/abs/2402.03545)

    本文提出了一种名为OLS-OFU的新方法，通过在测试时利用无标签数据进行自监督学习和特征优化，以解决在线设置中的标签转移问题。研究结果表明，OLS-OFU在各种数据集和领域转移条件下都表现出了有效性和鲁棒性。

    

    本文解决了在线设置中标签转移的普遍问题，其中存在缺失的标签，数据分布随时间变化，及时获得标签是一项具有挑战性的任务。虽然现有的方法主要集中在调整或更新预训练分类器的最后一层，我们探索了在测试时使用无标签数据来改进特征表示的未被发掘的潜力。我们的新方法，在线标签转移自适应与在线特征更新（OLS-OFU），利用自监督学习来优化特征提取过程，从而改进预测模型。理论分析证实，OLS-OFU通过利用自监督学习进行特征优化，减少了算法遗憾。在各种数据集上的实证研究，在在线标签转移和广义标签转移条件下，强调了OLS-OFU的有效性和鲁棒性，特别是在领域转移的情况下。

    This paper addresses the prevalent issue of label shift in an online setting with missing labels, where data distributions change over time and obtaining timely labels is challenging. While existing methods primarily focus on adjusting or updating the final layer of a pre-trained classifier, we explore the untapped potential of enhancing feature representations using unlabeled data at test-time. Our novel method, Online Label Shift adaptation with Online Feature Updates (OLS-OFU), leverages self-supervised learning to refine the feature extraction process, thereby improving the prediction model. Theoretical analyses confirm that OLS-OFU reduces algorithmic regret by capitalizing on self-supervised learning for feature refinement. Empirical studies on various datasets, under both online label shift and generalized label shift conditions, underscore the effectiveness and robustness of OLS-OFU, especially in cases of domain shifts.
    
[^32]: 学习Predict-then-Optimize框架中的最优策略

    Learning Best-in-Class Policies for the Predict-then-Optimize Framework

    [https://arxiv.org/abs/2402.03256](https://arxiv.org/abs/2402.03256)

    我们提出了一种新颖的决策感知替代损失函数家族，用于predict-then-optimize框架，并且通过数值证据证实了其在误设置下的优越性。

    

    我们提出了一种新颖的决策感知替代损失函数家族，称为Perturbation Gradient（PG）损失，用于predict-then-optimize框架。这些损失直接近似了下游决策损失，并可以使用现成的基于梯度的方法进行优化。重要的是，与现有的替代损失不同，我们的PG损失的近似误差随着样本数量的增加而消失。这意味着优化我们的替代损失可以在渐近意义下得到最佳策略，即使在误设置下也是如此。这是第一个在误设置下的这样的结果，我们提供了数值证据证实了当基础模型误设置且噪声不是中心对称时，我们的PG损失在实践中显著优于现有的提案。鉴于在实践中误设置很常见--特别是当我们可能更喜欢一个更简单、更可解释的模型时--PG损失提供了一种新颖的、理论上有依据的、可计算的决策感知方法。

    We propose a novel family of decision-aware surrogate losses, called Perturbation Gradient (PG) losses, for the predict-then-optimize framework. These losses directly approximate the downstream decision loss and can be optimized using off-the-shelf gradient-based methods. Importantly, unlike existing surrogate losses, the approximation error of our PG losses vanishes as the number of samples grows. This implies that optimizing our surrogate loss yields a best-in-class policy asymptotically, even in misspecified settings. This is the first such result in misspecified settings and we provide numerical evidence confirming our PG losses substantively outperform existing proposals when the underlying model is misspecified and the noise is not centrally symmetric. Insofar as misspecification is commonplace in practice -- especially when we might prefer a simpler, more interpretable model -- PG losses offer a novel, theoretically justified, method for computationally tractable decision-aware 
    
[^33]: 从教学中学习正则化: 易于模仿的可推广关系

    Learning from Teaching Regularization: Generalizable Correlations Should be Easy to Imitate

    [https://arxiv.org/abs/2402.02769](https://arxiv.org/abs/2402.02769)

    通过从教学中学习（LoT）技术，我们提出了一种深度神经网络正则化技术，能够通过引入辅助学生模型来提升主模型的泛化性能。实验证明，LoT能有效识别具有泛化和可教授关系的信息。

    

    泛化仍然是机器学习中的一个核心挑战。在这项工作中，我们提出了一种新颖的深度神经网络正则化技术，即从教学中学习（Learning from Teaching，简称LoT），以增强泛化性能。受到人类捕捉简明抽象模式的能力的启发，我们假设可推广的关系更容易教授。LoT通过辅助学生模型来实现这个概念，通过提供反馈来训练主模型和改进主模型，以捕捉更多具有泛化和可教授关系的信息。我们在计算机视觉、自然语言处理和强化学习等多个领域进行的实验结果表明，引入LoT相比仅在原始训练数据上训练模型带来了显著的好处。这表明了LoT在识别可推广信息方面的有效性。

    Generalization remains a central challenge in machine learning. In this work, we propose Learning from Teaching (LoT), a novel regularization technique for deep neural networks to enhance generalization. Inspired by the human ability to capture concise and abstract patterns, we hypothesize that generalizable correlations are expected to be easier to teach. LoT operationalizes this concept to improve the generalization of the main model with auxiliary student learners. The student learners are trained by the main model and improve the main model to capture more generalizable and teachable correlations by providing feedback. Our experimental results across several domains, including Computer Vision, Natural Language Processing, and Reinforcement Learning, demonstrate that the introduction of LoT brings significant benefits compared to merely training models on the original training data. It suggests the effectiveness of LoT in identifying generalizable information without falling into th
    
[^34]: 潜在图扩散：一种在图上生成和预测的统一框架

    Latent Graph Diffusion: A Unified Framework for Generation and Prediction on Graphs

    [https://arxiv.org/abs/2402.02518](https://arxiv.org/abs/2402.02518)

    本文提出了一种统一框架，能够使用一个模型解决各级别和各类型的图学习任务，通过潜在图扩散模型生成和预测节点、边和图级别的特征，具有可证明的保证，并在实验证明了其有效性。

    

    本文提出了第一个框架，可以使用一个模型解决各级别（节点、边和图）和各类型（生成、回归和分类）的图学习任务。首先，我们提出了潜在图扩散（LGD），一种能够同时生成节点、边和图级别特征的生成模型。通过将图结构和特征嵌入潜在空间，利用强大的编码器进行解码，然后在潜在空间中训练扩散模型，我们实现了这个目标。LGD还可以通过特殊设计的交叉注意力机制进行条件生成。然后，我们将回归和分类等预测任务形式化为（条件）生成，这使得我们的LGD能够通过可证明的保证来解决各级别和各类型的任务。通过大量的实验证明了我们框架的有效性，其中我们的模型在各项指标上取得了最先进或高度竞争力的结果。

    In this paper, we propose the first framework that enables solving graph learning tasks of all levels (node, edge and graph) and all types (generation, regression and classification) with one model. We first propose Latent Graph Diffusion (LGD), a generative model that can generate node, edge, and graph-level features of all categories simultaneously. We achieve this goal by embedding the graph structures and features into a latent space leveraging a powerful encoder which can also be decoded, then training a diffusion model in the latent space. LGD is also capable of conditional generation through a specifically designed cross-attention mechanism. Then we formulate prediction tasks including regression and classification as (conditional) generation, which enables our LGD to solve tasks of all levels and all types with provable guarantees. We verify the effectiveness of our framework with extensive experiments, where our models achieve state-of-the-art or highly competitive results acr
    
[^35]: AutoTimes: 基于大型语言模型的自回归时间序列预测器

    AutoTimes: Autoregressive Time Series Forecasters via Large Language Models

    [https://arxiv.org/abs/2402.02370](https://arxiv.org/abs/2402.02370)

    AutoTimes是一种基于大型语言模型的自回归时间序列预测器，利用语言模型的转换能力来处理时间序列数据，实现了与先前模型相当的性能。

    

    由于大规模时间序列的有限可用性和可扩展预训练的不充分探索，时间序列的基础模型尚未完全发展。基于时间序列和自然语言的相似顺序结构，越来越多的研究证明了利用大型语言模型(LLM)进行时间序列的可行性。然而，先前的方法可能忽视了时间序列和自然语言对齐的一致性，导致对LLM潜力的利用不足。为了充分利用从语言建模中学到的通用令牌转换，我们提出了AutoTimes，将LLM重新用作自回归时间序列预测器，这与LLM的获取和利用一致，而无需更新参数。由此产生的预测器可以处理灵活的系列长度，并实现与流行模型相当的性能。此外，我们提出了基于令牌的提示方法，利用相应的时间戳来进行预测。

    Foundation models of time series have not been fully developed due to the limited availability of large-scale time series and the underexploration of scalable pre-training. Based on the similar sequential structure of time series and natural language, increasing research demonstrates the feasibility of leveraging large language models (LLM) for time series. Nevertheless, prior methods may overlook the consistency in aligning time series and natural language, resulting in insufficient utilization of the LLM potentials. To fully exploit the general-purpose token transitions learned from language modeling, we propose AutoTimes to repurpose LLMs as Autoregressive Time series forecasters, which is consistent with the acquisition and utilization of LLMs without updating the parameters. The consequent forecasters can handle flexible series lengths and achieve competitive performance as prevalent models. Further, we present token-wise prompting that utilizes corresponding timestamps to make ou
    
[^36]: NM-FlowGAN: 基于正规化流和生成对抗网络的混合方法对sRGB噪声进行建模

    NM-FlowGAN: Modeling sRGB Noise with a Hybrid Approach based on Normalizing Flows and Generative Adversarial Networks

    [https://arxiv.org/abs/2312.10112](https://arxiv.org/abs/2312.10112)

    NM-FlowGAN是一种利用生成对抗网络和正规化流的混合方法，旨在更准确地建模sRGB噪声，弥补了单一生成模型固有特性所带来的性能限制。

    

    建模和合成真实的sRGB噪声对于各种低级别视觉任务至关重要，例如构建用于训练图像去噪系统的数据集。真实sRGB噪声的分布极为复杂，并受多种因素影响，使得其准确建模极具挑战性。因此，最近的研究提出了采用数据驱动生成模型，如生成对抗网络（GAN）和正规化流的方法。这些研究相比传统的噪声建模方法实现了对sRGB噪声的更准确建模。然而，由于每种生成模型的固有特性，存在性能限制。为了解决这个问题，我们提出了NM-FlowGAN，这是一种利用GAN和正规化流的优势的混合方法。我们同时采用基于正规化流的像素级噪声建模网络，以及基于GAN的空间相关性建模网络。

    arXiv:2312.10112v2 Announce Type: replace-cross  Abstract: Modeling and synthesizing real sRGB noise is crucial for various low-level vision tasks, such as building datasets for training image denoising systems. The distribution of real sRGB noise is highly complex and affected by a multitude of factors, making its accurate modeling extremely challenging. Therefore, recent studies have proposed methods that employ data-driven generative models, such as generative adversarial networks (GAN) and Normalizing Flows. These studies achieve more accurate modeling of sRGB noise compared to traditional noise modeling methods. However, there are performance limitations due to the inherent characteristics of each generative model. To address this issue, we propose NM-FlowGAN, a hybrid approach that exploits the strengths of both GAN and Normalizing Flows. We simultaneously employ a pixel-wise noise modeling network based on Normalizing Flows, and spatial correlation modeling networks based on GAN
    
[^37]: 攻击树：自动破解黑盒大型语言模型

    Tree of Attacks: Jailbreaking Black-Box LLMs Automatically

    [https://arxiv.org/abs/2312.02119](https://arxiv.org/abs/2312.02119)

    提出了一种名为Tree of Attacks with Pruning (TAP)的自动化方法，用于生成只需要对目标大型语言模型进行黑盒访问的越狱方法，并通过思维树推理和修剪生成准确的越狱提示。

    

    大型语言模型(LLMs)展示了多功能性，但仍在生成有害、带偏见和有毒内容，这一点由人为设计的越狱行为的普遍存在得以证明。在这项工作中，我们提出了一种名为Tree of Attacks with Pruning (TAP)的自动化方法，用于生成越狱，仅需要对目标LLM进行黑盒访问。TAP利用LLM来通过思维树推理迭代地优化候选（攻击）提示，直到生成的提示之一越狱目标。关键在于，在将提示发送给目标之前，TAP对其进行评估并移除可能不会导致越狱的提示。使用思维树推理使TAP能够在大量提示的搜索空间中导航，而修剪则减少了发送给目标的总查询数量。在实证评估中，我们观察到TAP生成的提示越狱了超过80%的最先进LLMs（包括GPT4和GPT4-Turbo）。

    arXiv:2312.02119v2 Announce Type: replace-cross  Abstract: While Large Language Models (LLMs) display versatile functionality, they continue to generate harmful, biased, and toxic content, as demonstrated by the prevalence of human-designed jailbreaks. In this work, we present Tree of Attacks with Pruning (TAP), an automated method for generating jailbreaks that only requires black-box access to the target LLM. TAP utilizes an LLM to iteratively refine candidate (attack) prompts using tree-of-thought reasoning until one of the generated prompts jailbreaks the target. Crucially, before sending prompts to the target, TAP assesses them and prunes the ones unlikely to result in jailbreaks. Using tree-of-thought reasoning allows TAP to navigate a large search space of prompts and pruning reduces the total number of queries sent to the target. In empirical evaluations, we observe that TAP generates prompts that jailbreak state-of-the-art LLMs (including GPT4 and GPT4-Turbo) for more than 80%
    
[^38]: 线性赌臂的集成抽样：小集成足矣

    Ensemble sampling for linear bandits: small ensembles suffice

    [https://arxiv.org/abs/2311.08376](https://arxiv.org/abs/2311.08376)

    该论文对随机线性赌臂环境中的集成抽样进行了首次实用和严格的分析，展示了在标准假设下，采用规模为$d \log T$的集成抽样可以获得接近$\sqrt{T}$阶的后悔，而不需要集成大小与$T$线性扩展。

    

    我们首次对随机线性赌臂设定下的集成抽样进行了有用且严谨的分析。特别地，我们展示了在标准假设下，对于一个具有交互作用时间跨度$T$的$d$维随机线性赌臂，采用集成大小为$\smash{d \log T}$的集成抽样，遭受的后悔最多为$\smash{(d \log T)^{5/2} \sqrt{T}}$阶。我们的结果是在任何结构化环境中第一个不要求集成大小与$T$线性扩展的结果，这使得集成抽样失去意义，同时获得了接近$\smash{\sqrt{T}}$阶的后悔。我们的结果也是第一个允许无限动作集的结果。

    arXiv:2311.08376v2 Announce Type: replace-cross  Abstract: We provide the first useful and rigorous analysis of ensemble sampling for the stochastic linear bandit setting. In particular, we show that, under standard assumptions, for a $d$-dimensional stochastic linear bandit with an interaction horizon $T$, ensemble sampling with an ensemble of size of order $\smash{d \log T}$ incurs regret at most of the order $\smash{(d \log T)^{5/2} \sqrt{T}}$. Ours is the first result in any structured setting not to require the size of the ensemble to scale linearly with $T$ -- which defeats the purpose of ensemble sampling -- while obtaining near $\smash{\sqrt{T}}$ order regret. Ours is also the first result that allows infinite action sets.
    
[^39]: 基于剪枝的保护: 在不进行微调的情况下增加对齐的LLMs的越狱抵抗力

    Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning. (arXiv:2401.10862v1 [cs.LG])

    [http://arxiv.org/abs/2401.10862](http://arxiv.org/abs/2401.10862)

    本文研究了剪枝对齐的LLMs的保护措施，发现剪枝LLM参数可以显著增强其抵抗“越狱”提示攻击的能力，并且对其他LLM行为也可能有更普遍的效果。同时，引入了一个有害任务数据集，证明剪枝有助于集中注意力在与任务相关的标记上。突出的聊天模型表现出很高的易感性。

    

    大型语言模型（LLMs）容易受到“越狱”提示的攻击，这种攻击可以诱使这些模型生成有害和违法内容。本文表明，剪枝LLM参数多达20％可以显著增加它们对此类攻击的抵抗力，而无需额外训练并且不损害其在标准基准测试中的性能。有趣的是，我们发现剪枝后观察到的增强安全性与模型的初始安全训练水平相关，这暗示剪枝的效果可能更普遍，也可能适用于超出安全性范畴的其他LLM行为。另外，我们还介绍了一个包含五个类别、插入到十个不同越狱提示中的225个有害任务的精选数据集，表明剪枝有助于LLMs集中注意力在越狱提示中与任务相关的标记上。最后，我们的实验揭示了突出的聊天模型（如LLaMA-2 Chat，Vicuna和Mistral Instruct）具有很高的易感性。

    Large Language Models (LLMs) are vulnerable to `Jailbreaking' prompts, a type of attack that can coax these models into generating harmful and illegal content. In this paper, we show that pruning up to 20% of LLM parameters markedly increases their resistance to such attacks without additional training and without sacrificing their performance in standard benchmarks. Intriguingly, we discovered that the enhanced safety observed post-pruning correlates to the initial safety training level of the model, hinting that the effect of pruning could be more general and may hold for other LLM behaviors beyond safety. Additionally, we introduce a curated dataset of 225 harmful tasks across five categories, inserted into ten different Jailbreaking prompts, showing that pruning aids LLMs in concentrating attention on task-relevant tokens in jailbreaking prompts. Lastly, our experiments reveal that the prominent chat models, such as LLaMA-2 Chat, Vicuna, and Mistral Instruct exhibit high susceptibi
    
[^40]: 生成分数阶扩散模型

    Generative Fractional Diffusion Models. (arXiv:2310.17638v1 [cs.LG])

    [http://arxiv.org/abs/2310.17638](http://arxiv.org/abs/2310.17638)

    该论文提出了一种基于分数阶扩散模型的生成模型，其中通过将分数布朗运动表示为奥恩斯坦-乌伦贝克过程的随机积分，实现了驱动噪声收敛到非马尔可夫过程的效果。这是第一个在具有无限二次变差的随机过程上构建生成模型的尝试。

    

    我们将基于分数布朗运动（FBM）的连续时间框架推广到基于分数布朗运动的近似形式。我们通过将FBM表示为家族奥恩斯坦-乌伦贝克过程的随机积分，推导出连续再参数化技巧和逆时模型，定义了具有驱动噪声收敛到无限二次变差的非马尔可夫过程的生成分数阶扩散模型（GFDM）。FBM的赫斯特指数$H \in (0,1)$ 可以控制路径变换分布的粗糙度。据我们所知，这是第一次尝试在具有无限二次变差的随机过程上建立生成模型。

    We generalize the continuous time framework for score-based generative models from an underlying Brownian motion (BM) to an approximation of fractional Brownian motion (FBM). We derive a continuous reparameterization trick and the reverse time model by representing FBM as a stochastic integral over a family of Ornstein-Uhlenbeck processes to define generative fractional diffusion models (GFDM) with driving noise converging to a non-Markovian process of infinite quadratic variation. The Hurst index $H\in(0,1)$ of FBM enables control of the roughness of the distribution transforming path. To the best of our knowledge, this is the first attempt to build a generative model upon a stochastic process with infinite quadratic variation.
    
[^41]: 控制组合优化的连续放松

    Controlling Continuous Relaxation for Combinatorial Optimization. (arXiv:2309.16965v1 [stat.ML])

    [http://arxiv.org/abs/2309.16965](http://arxiv.org/abs/2309.16965)

    本文研究了在相对密集的图上组合优化问题中物理启发的图神经网络（PI-GNN）求解器的表现。通过数值实验，我们发现PI-GNN求解器在学习早期可能陷入所有变量为零的局部解。为了解决这个问题，我们通过控制连续性和离散性提出了一种改进方法。

    

    最近在组合优化（CO）问题中，图神经网络（GNNs）显示出巨大潜力。通过无监督学习找到近似解的受物理启发的GNN（PI-GNN）求解器在大规模CO问题上引起了极大关注。然而，对于相对密集图上的CO问题，贪婪算法的性能恶化，但对于PI-GNN求解器的性能却没有太多讨论。此外，由于PI-GNN求解器采用了放松策略，学习后需要从连续空间人工转换回原始离散空间，可能会破坏解的鲁棒性。本文通过数值实验证明了PI-GNN求解器在密集图上的CO问题的学习早期可能陷入局部解的情况，其中所有变量都为零。然后，我们通过控制连续性和离散性来解决这些问题。

    Recent advancements in combinatorial optimization (CO) problems emphasize the potential of graph neural networks (GNNs). The physics-inspired GNN (PI-GNN) solver, which finds approximate solutions through unsupervised learning, has attracted significant attention for large-scale CO problems. Nevertheless, there has been limited discussion on the performance of the PI-GNN solver for CO problems on relatively dense graphs where the performance of greedy algorithms worsens. In addition, since the PI-GNN solver employs a relaxation strategy, an artificial transformation from the continuous space back to the original discrete space is necessary after learning, potentially undermining the robustness of the solutions. This paper numerically demonstrates that the PI-GNN solver can be trapped in a local solution, where all variables are zero, in the early stage of learning for CO problems on the dense graphs. Then, we address these problems by controlling the continuity and discreteness of rela
    
[^42]: 子词作为技巧：稀疏奖励强化学习的分词化

    Subwords as Skills: Tokenization for Sparse-Reward Reinforcement Learning. (arXiv:2309.04459v1 [cs.LG])

    [http://arxiv.org/abs/2309.04459](http://arxiv.org/abs/2309.04459)

    通过将行动空间离散化并采用分词技术，我们提出了一种在稀疏奖励强化学习中生成技巧的新方法。这种方法能够减少探索的难度，并在连续行动空间中达到良好的性能。

    

    稀疏奖励强化学习中的探索具有困难，因为需要通过长期的、协调的行动序列才能获得任何奖励。而且，在连续的行动空间中，可能的行动数量是无穷多的，这只会增加探索的难度。为了解决这些问题，一类方法通过在同一领域收集的交互数据中形成时间上延伸的行动，通常称为技巧，并在这个新的行动空间上进行策略的优化。通常这样的方法在连续行动空间中需要一个漫长的预训练阶段，在强化学习开始之前形成技巧。鉴于先前的证据表明在这些任务中并不需要完整的连续行动空间，我们提出了一种新颖的技巧生成方法，包括两个组成部分。首先，我们通过聚类将行动空间离散化，然后我们利用从自然语言处理借鉴来的分词技术。

    Exploration in sparse-reward reinforcement learning is difficult due to the requirement of long, coordinated sequences of actions in order to achieve any reward. Moreover, in continuous action spaces there are an infinite number of possible actions, which only increases the difficulty of exploration. One class of methods designed to address these issues forms temporally extended actions, often called skills, from interaction data collected in the same domain, and optimizes a policy on top of this new action space. Typically such methods require a lengthy pretraining phase, especially in continuous action spaces, in order to form the skills before reinforcement learning can begin. Given prior evidence that the full range of the continuous action space is not required in such tasks, we propose a novel approach to skill-generation with two components. First we discretize the action space through clustering, and second we leverage a tokenization technique borrowed from natural language pro
    
[^43]: 一种用于容错量子错误纠正的低温存储敏化神经解码器

    A Cryogenic Memristive Neural Decoder for Fault-tolerant Quantum Error Correction. (arXiv:2307.09463v1 [quant-ph])

    [http://arxiv.org/abs/2307.09463](http://arxiv.org/abs/2307.09463)

    本研究报告了一种基于内存计算的神经解码器推理加速器的设计和性能分析，该解码器用于容错量子错误纠正，旨在最小化解码时间并确保解码方法的可扩展性。

    

    量子错误纠正(QEC)的神经解码器依赖于神经网络来分类从错误纠正编码中提取的综合征，并找到合适的恢复操作员来保护逻辑信息免受错误影响。尽管神经解码器性能良好，但仍存在一些重要的实际要求，如将解码时间最小化以满足重复错误纠正方案中的综合征生成速度，以及确保解码方法的可扩展性随着编码距离的增加而增加。设计一个专用的集成电路以与量子处理器共同完成解码任务似乎是必要的，因为将信号从低温环境中引出并进行外部处理会导致不必要的延迟和最终的布线瓶颈。在这项工作中，我们报道了一种基于内存计算的神经解码器推理加速器的设计和性能分析。

    Neural decoders for quantum error correction (QEC) rely on neural networks to classify syndromes extracted from error correction codes and find appropriate recovery operators to protect logical information against errors. Despite the good performance of neural decoders, important practical requirements remain to be achieved, such as minimizing the decoding time to meet typical rates of syndrome generation in repeated error correction schemes, and ensuring the scalability of the decoding approach as the code distance increases. Designing a dedicated integrated circuit to perform the decoding task in co-integration with a quantum processor appears necessary to reach these decoding time and scalability requirements, as routing signals in and out of a cryogenic environment to be processed externally leads to unnecessary delays and an eventual wiring bottleneck. In this work, we report the design and performance analysis of a neural decoder inference accelerator based on an in-memory comput
    
[^44]: 预训练Transformer用于对抗性样本提纯

    Pre-trained transformer for adversarial purification. (arXiv:2306.01762v1 [cs.CR])

    [http://arxiv.org/abs/2306.01762](http://arxiv.org/abs/2306.01762)

    本文提出了一个快速防御对抗性攻击的方案RaPiD（Rapid Plug-in Defender），通过预训练的Transformer微调来提纯对抗样本，使其逼近清洁数据分布，实验结果表明，在有限数据情况下，该方法优于最先进的方法。

    

    随着越来越多的深度神经网络被部署为各种日常服务，它们的可靠性至关重要。深度神经网络容易受到对抗性攻击的影响，其中逃避攻击是最普遍的一种。最近的研究通常通过对抗训练或利用大量清洁数据的知识来增强其健壮性。然而，在实际应用中，重新训练和部署模型需要大量的计算资源，对在线服务造成重大损失。此外，当检测到某种攻击的对抗性例子时，服务提供者只能获得有限的对抗性样本，而大量的清洁数据可能无法获取。针对这些问题，我们提出了一种新的方案，名为RaPiD（Rapid Plug-in Defender），旨在快速防御具有少量干净和对抗性示例限制的原始服务模型的某种攻击。受到预训练模型提供转移学习良好初始化的通用趋势的启发，我们建议通过微调预先训练的Transformer来提纯对抗性样本。预训练的Transformer作为正则化器，鼓励提纯后的对抗性样本接近清晰数据的分布。实验结果表明，RaPiD在防御各种具有限数据的攻击方面优于最先进的方法。

    With more and more deep neural networks being deployed as various daily services, their reliability is essential. It's frightening that deep neural networks are vulnerable and sensitive to adversarial attacks, the most common one of which for the services is evasion-based. Recent works usually strengthen the robustness by adversarial training or leveraging the knowledge of an amount of clean data. However, in practical terms, retraining and redeploying the model need a large computational budget, leading to heavy losses to the online service. In addition, when adversarial examples of a certain attack are detected, only limited adversarial examples are available for the service provider, while much clean data may not be accessible. Given the mentioned problems, we propose a new scenario, RaPiD (Rapid Plug-in Defender), which is to rapidly defend against a certain attack for the frozen original service model with limitations of few clean and adversarial examples. Motivated by the general
    
[^45]: 基于异构知识的增强模块化强化学习

    Augmented Modular Reinforcement Learning based on Heterogeneous Knowledge. (arXiv:2306.01158v1 [cs.LG])

    [http://arxiv.org/abs/2306.01158](http://arxiv.org/abs/2306.01158)

    该论文提出了增强模块化强化学习（AMRL），使用仲裁器来选择异构模块，并无缝地整合不同类型的知识。该方法能够减缓强化学习中的一些低效问题，有望在深度强化学习领域得到应用。

    

    为了减缓强化学习中的一些低效问题，学者们提出了模块化方法，将不同的决策制定策略组合起来以衍生出可以执行多种任务的代理。这些体系结构的基础模块通常是可重复使用的，也允许“即插即用”的集成。然而，这些解决方案仍然缺乏处理和整合多种类型信息（知识）的能力，例如规则，子目标和技能。我们提出了增强模块化强化学习（AMRL）来解决这些限制。这种新的框架使用仲裁器来选择异构模块，并无缝地整合不同类型的知识。此外，我们引入了一种选择机制的变体，即增强记忆的仲裁器，它增加了利用时间信息的能力。我们在已有的环境中评估了所提出的机制，同时也在新环境中进行了基准测试，结果表明其在深度强化学习领域具有良好的应用前景。

    In order to mitigate some of the inefficiencies of Reinforcement Learning (RL), modular approaches composing different decision-making policies to derive agents capable of performing a variety of tasks have been proposed. The modules at the basis of these architectures are generally reusable, also allowing for "plug-and-play" integration. However, such solutions still lack the ability to process and integrate multiple types of information (knowledge), such as rules, sub-goals, and skills. We propose Augmented Modular Reinforcement Learning (AMRL) to address these limitations. This new framework uses an arbitrator to select heterogeneous modules and seamlessly incorporate different types of knowledge. Additionally, we introduce a variation of the selection mechanism, namely the Memory-Augmented Arbitrator, which adds the capability of exploiting temporal information. We evaluate the proposed mechanisms on established as well as new environments and benchmark them against prominent deep 
    
[^46]: 丰富解缠结：从定义到度量的探究

    Enriching Disentanglement: Definitions to Metrics. (arXiv:2305.11512v1 [cs.LG])

    [http://arxiv.org/abs/2305.11512](http://arxiv.org/abs/2305.11512)

    本文探究了解缠结表示学习的度量标准，并提出了基于丰富范畴论的系统方法，将方程定义转化为可比较的度量标准，我们推导出应用于测量解缠结属性的度量标准，并在合成数据上证明其有效性。

    

    解缠结表示学习是一项具有挑战性的任务，涉及到在复杂数据中分离多个变化因素。虽然已经提出了各种用于学习和评估解缠结表示的度量标准，但这些度量标准真正量化了什么以及如何比较它们仍然不清楚。在本文中，我们研究了利用一阶方程谓词定义的解缠结，并介绍了一种基于丰富范畴论的系统方法，将方程定义转化为兼容的定量度量标准。具体而言，我们展示了如何用度量或离散度替换(i) 等式，用排序操作替换 (ii) 逻辑联结词，用聚合替换 (iii) 通用量词，用最佳逼近替换 (iv) 存在量词。使用这种方法，我们推导出用于测量解缠结表示提取器所需属性的度量标准，并在合成数据上展示它们的有效性。我们提出的方法提供了一种框架，能够将解缠结表示定义转化为可比较的，并衡量一种方法中解缠结属性的有效性。

    Disentangled representation learning is a challenging task that involves separating multiple factors of variation in complex data. Although various metrics for learning and evaluating disentangled representations have been proposed, it remains unclear what these metrics truly quantify and how to compare them. In this work, we study the definitions of disentanglement given by first-order equational predicates and introduce a systematic approach for transforming an equational definition into a compatible quantitative metric based on enriched category theory. Specifically, we show how to replace (i) equality with metric or divergence, (ii) logical connectives with order operations, (iii) universal quantifier with aggregation, and (iv) existential quantifier with the best approximation. Using this approach, we derive metrics for measuring the desired properties of a disentangled representation extractor and demonstrate their effectiveness on synthetic data. Our proposed approach provides p
    
[^47]: GNNs: 可以更强、更新、更快

    GNNs,You can be Stronger,Deeper and Faster. (arXiv:2305.05368v1 [cs.LG])

    [http://arxiv.org/abs/2305.05368](http://arxiv.org/abs/2305.05368)

    本文介绍了一种新的理解GNN表现能力的视角，并提出了一个新的采样节点级残差模块SDF，具有更好的表现能力。

    

    图神经网络（GNNs）是一类可以从图结构数据中学习并通过集成邻居节点的表示学习来表现出色的神经网络。然而，GNN的性能会随着层数增加而逐渐降低。本文引入了一个新的概念——k跳子图聚合，提出了一种新的理解GNN表现能力的视角，揭示了传统深层GNN表现逐渐退化的潜在原因，包括聚合子图的重叠以及基于残差的GNN实际上利用了1到k跳子图聚合结果来提高有效性。此外，我们提出了一种新的采样节点级残差模块SDF，通过理论推导证明其比之前的残差方法具有更优的表现能力，可以利用1到k跳跃子图的信息。

    Graph neural networks (GNNs), a type of neural network that can learn from graph-structured data and learn the representation of nodes by aggregating their neighbors, have shown excellent performance in downstream tasks.However, it is known that the performance of graph neural networks (GNNs) degrades gradually as the number of layers increases. Based on k-hop subgraph aggregation, which is a new concept, we propose a new perspective to understand the expressive power of GNN.From this perspective, we reveal the potential causes of the performance degradation of the deep traditional GNN - aggregated subgraph overlap, and the fact that the residual-based graph neural networks in fact exploit the aggregation results of 1 to k hop subgraphs to improve the effectiveness.Further, we propose a new sampling-based node-level residual module named SDF, which is shown by theoretical derivation to obtain a superior expressive power compared to previous residual methods by using information from 1 
    
[^48]: 在正交约束下的优化问题中的不可行确定性、随机和方差约减算法

    Infeasible Deterministic, Stochastic, and Variance-Reduction Algorithms for Optimization under Orthogonality Constraints. (arXiv:2303.16510v1 [stat.ML])

    [http://arxiv.org/abs/2303.16510](http://arxiv.org/abs/2303.16510)

    本研究提出了一种简单迭代的Landing算法，可以在不强制执行正交约束的同时顺畅地吸引到正交约束流形上。我们扩展了这种算法以支持斯托菲尔（Stiefel）流形，并提供了随机和方差约减算法，这些方法与黎曼优化算法的收敛速度相同但需要更少的计算。

    

    正交约束在许多机器学习问题中都会自然地出现，从主成分分析到鲁棒性神经网络训练。传统上，这些问题需要使用黎曼优化算法来求解，该算法在强制执行约束时最耗费时间。最近，Ablin＆Peyr\'e（2022）提出了Landing算法，这是一种廉价迭代方法，它不强制执行正交约束，但会以平滑的方式吸引到流形上。在本文中，我们为Landing算法提供了新的实用和理论发展。首先，该方法被扩展到斯托菲尔流形，即矩形正交矩阵的集合。当成本函数是许多函数的平均值时，我们还考虑随机和方差约减算法。我们证明了所有这些方法的收敛速度与它们的黎曼优化算法相同，同时需要更少的计算。

    Orthogonality constraints naturally appear in many machine learning problems, from Principal Components Analysis to robust neural network training. They are usually solved using Riemannian optimization algorithms, which minimize the objective function while enforcing the constraint. However, enforcing the orthogonality constraint can be the most time-consuming operation in such algorithms. Recently, Ablin & Peyr\'e (2022) proposed the Landing algorithm, a method with cheap iterations that does not enforce the orthogonality constraint but is attracted towards the manifold in a smooth manner. In this article, we provide new practical and theoretical developments for the landing algorithm. First, the method is extended to the Stiefel manifold, the set of rectangular orthogonal matrices. We also consider stochastic and variance reduction algorithms when the cost function is an average of many functions. We demonstrate that all these methods have the same rate of convergence as their Rieman
    
[^49]: 关于学习算法泛化误差信息理论界限的紧密性研究

    On the tightness of information-theoretic bounds on generalization error of learning algorithms. (arXiv:2303.14658v1 [cs.IT])

    [http://arxiv.org/abs/2303.14658](http://arxiv.org/abs/2303.14658)

    本文研究了学习算法泛化误差信息理论界限的紧密性。研究表明，通过适当的假设，可以在快速收敛速度下使用信息理论量$O(\lambda/n)$来上界估计泛化误差。

    

    Russo和Xu提出了一种方法来证明学习算法的泛化误差可以通过信息度量进行上界估计。然而，这种收敛速度通常被认为是“慢”的，因为它的期望收敛速度的形式为$O(\sqrt{\lambda/n})$，其中$\lambda$是一些信息理论量。在本文中我们证明了根号并不一定意味着收敛速度慢，可以在适当的假设下使用这个界限来得到$O(\lambda/n)$的快速收敛速度。此外，我们确定了达到快速收敛速度的关键条件，即所谓的$(\eta,c)$-中心条件。在这个条件下，我们给出了学习算法泛化误差的信息理论界限。

    A recent line of works, initiated by Russo and Xu, has shown that the generalization error of a learning algorithm can be upper bounded by information measures. In most of the relevant works, the convergence rate of the expected generalization error is in the form of $O(\sqrt{\lambda/n})$ where $\lambda$ is some information-theoretic quantities such as the mutual information or conditional mutual information between the data and the learned hypothesis. However, such a learning rate is typically considered to be ``slow", compared to a ``fast rate" of $O(\lambda/n)$ in many learning scenarios. In this work, we first show that the square root does not necessarily imply a slow rate, and a fast rate result can still be obtained using this bound under appropriate assumptions. Furthermore, we identify the critical conditions needed for the fast rate generalization error, which we call the $(\eta,c)$-central condition. Under this condition, we give information-theoretic bounds on the generaliz
    
[^50]: 跨数据粒度链接：拟合多变量Hawkes过程到部分区间屏蔽数据

    Linking Across Data Granularity: Fitting Multivariate Hawkes Processes to Partially Interval-Censored Data. (arXiv:2111.02062v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.02062](http://arxiv.org/abs/2111.02062)

    这项研究介绍了Partial Mean Behavior Poisson (PMBP)过程，它是一种新的点过程，可以有效地建模时间戳和区间屏蔽数据，并成功恢复了MHP参数和谱半径。

    

    多变量Hawkes过程(MHP)被广泛用于分析相互作用的数据流，其中事件在自身维度内（通过自激）或不同维度之间（通过交叉激发）生成新事件。然而，在某些应用中，某些维度中的个别事件时间戳是不可观测的，只有在时间间隔内的事件计数是已知的，被称为部分区间屏蔽数据。MHP不适用于处理这种数据，因为其估计需要事件时间戳。在本研究中，我们引入了Partial Mean Behavior Poisson (PMBP)过程，这是一种新颖的点过程，与MHP共享参数等效性，可以有效地建模时间戳和区间屏蔽数据。我们使用合成和真实世界数据集展示了PMBP过程的能力。首先，我们证明了PMBP过程可以近似MHP参数并恢复谱半径，使用合成事件历史数据进行验证。

    The multivariate Hawkes process (MHP) is widely used for analyzing data streams that interact with each other, where events generate new events within their own dimension (via self-excitation) or across different dimensions (via cross-excitation). However, in certain applications, the timestamps of individual events in some dimensions are unobservable, and only event counts within intervals are known, referred to as partially interval-censored data. The MHP is unsuitable for handling such data since its estimation requires event timestamps. In this study, we introduce the Partial Mean Behavior Poisson (PMBP) process, a novel point process which shares parameter equivalence with the MHP and can effectively model both timestamped and interval-censored data. We demonstrate the capabilities of the PMBP process using synthetic and real-world datasets. Firstly, we illustrate that the PMBP process can approximate MHP parameters and recover the spectral radius using synthetic event histories. 
    

