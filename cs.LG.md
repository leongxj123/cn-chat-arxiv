# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Large Language Models on Graphs: A Comprehensive Survey](https://rss.arxiv.org/abs/2312.02783) | 这篇论文对在图上的大型语言模型进行了全面调查，研究了纯图形、文本属性图形和文本配对图形三个不同场景下的应用情况，并探讨了基于图形的推理能力是否可以推广到大型语言模型上。 |
| [^2] | [Unsupervised End-to-End Training with a Self-Defined Bio-Inspired Target](https://arxiv.org/abs/2403.12116) | 本研究提出了一种使用Winner-Take-All（WTA）选择性和生物启发的稳态机制相结合的“自定义目标”方法，旨在解决无监督学习方法在边缘AI硬件上的计算资源稀缺性问题。 |
| [^3] | [Linguacodus: A Synergistic Framework for Transformative Code Generation in Machine Learning Pipelines](https://arxiv.org/abs/2403.11585) | Linguacodus是一种创新框架，通过部署动态流水线和精细调整的大型语言模型，实现了将自然语言任务描述转换为代码的自动化过程，极大地推进了机器学习应用的发展。 |
| [^4] | [Graph Neural Networks and Arithmetic Circuits](https://arxiv.org/abs/2402.17805) | 研究者在本文中建立了图神经网络与算术电路之间的表达能力对应关系，结果表明不同激活函数的GNN在表达能力上等价于实数上的算术电路。 |
| [^5] | [Graph Neural Networks as Fast and High-fidelity Emulators for Finite-Element Ice Sheet Modeling](https://arxiv.org/abs/2402.05291) | 本研究开发了图神经网络作为有限元冰盖模拟的快速高保真度的仿真器，并在Pine Island Glacier的瞬态模拟中展示了与传统卷积神经网络和多层感知器相比更准确的重现冰盖厚度和速度的能力。同时，这些GNN成功捕捉到了更高底部熔化速率引起的冰质量损失和加速过程。在图形处理单元上实现的GNN仿真器显示出高达50倍的加速。 |
| [^6] | [Simulated Overparameterization](https://arxiv.org/abs/2402.05033) | 模拟过度参数化（SOP）是一种将紧凑模型的计算效率与过度参数化模型的高级学习能力相结合的新范式。通过使用模拟训练过度参数化模型的方法，我们提出了一种与主要架构无关的算法，称为"majority kernels"，从而在各种架构和任务中实现性能提升。 |
| [^7] | [Closing the Gap Between SGP4 and High-Precision Propagation via Differentiable Programming](https://arxiv.org/abs/2402.04830) | 本研究介绍了dSGP4，一种使用PyTorch实现的可微版本的SGP4。通过可微化，dSGP4实现了轨道传播的高精度，并且适用于各种与太空相关的应用，包括卫星轨道确定、状态转换、协方差传播等。 |
| [^8] | [Asymptotic generalization error of a single-layer graph convolutional network](https://arxiv.org/abs/2402.03818) | 本研究针对单层图卷积网络（GCN）在高维极限下的性能进行了预测，并推广了对多种数据模型的分析。我们的研究显示，尽管GCN在收敛速度上是一致的，但在任何情况下都不能达到贝叶斯最优率。 |
| [^9] | [Uncertainty-Guided Alignment for Unsupervised Domain Adaptation in Regression.](http://arxiv.org/abs/2401.13721) | 该论文提出了一种利用不确定性引导的无监督领域自适应回归方法，通过将不确定性作为置信度估计和嵌入空间的正则项来实现对齐。 |
| [^10] | [Criticality-Guided Efficient Pruning in Spiking Neural Networks Inspired by Critical Brain Hypothesis.](http://arxiv.org/abs/2311.16141) | 本研究受到神经科学中的关键大脑假设的启发，提出了一种基于神经元关键性的高效SNN修剪方法，以加强特征提取和加速修剪过程，并取得了比当前最先进方法更好的性能。 |
| [^11] | [Multi-task Deep Convolutional Network to Predict Sea Ice Concentration and Drift in the Arctic Ocean.](http://arxiv.org/abs/2311.00167) | 提出了一种名为HIS-Unet的多任务深度卷积网络架构，通过加权注意力模块实现海冰浓度和漂移的预测。与其他方法相比，HIS-Unet在海冰预测中取得了显著的改进。 |
| [^12] | [CAMELL: Confidence-based Acquisition Model for Efficient Self-supervised Active Learning with Label Validation.](http://arxiv.org/abs/2310.08944) | CAMELL是一个适用于序列多输出问题的主动学习框架，通过仅需专家标注序列的一小部分、自监督和标签验证机制来解决监督神经方法对大规模标注数据集的依赖限制。 |
| [^13] | [Data-centric Graph Learning: A Survey.](http://arxiv.org/abs/2310.04987) | 本综述从数据中心化的角度全面评估了图学习方法，回答了何时修改图数据、图数据的哪一部分需要修改以及如何保护图模型的关键问题。 |
| [^14] | [A Design Toolbox for the Development of Collaborative Distributed Machine Learning Systems.](http://arxiv.org/abs/2309.16584) | 我们开发了一个CDML设计工具箱，可以指导开发者设计满足用例要求的协作分布式机器学习系统。 |
| [^15] | [The Boundaries of Verifiable Accuracy, Robustness, and Generalisation in Deep Learning.](http://arxiv.org/abs/2309.07072) | 本文评估了在分类任务中确定神经网络稳定性和准确性的理论限制，发现在一定的神经架构类别中，计算和验证理想的稳定和准确的神经网络是极具挑战性的甚至可能是不可能的。 |
| [^16] | [A Fusion of Variational Distribution Priors and Saliency Map Replay for Continual 3D Reconstruction.](http://arxiv.org/abs/2308.08812) | 本研究提出了一种基于连续学习的三维重建方法，通过使用变分先验和显著性图重新播放，实现对之前已见类别的合理重建。实验结果表明，与已建立的方法相比具有竞争力。 |
| [^17] | [Robust Decision-Focused Learning for Reward Transfer.](http://arxiv.org/abs/2304.03365) | 本文介绍了一种稳健决策重点（RDF）算法，利用非识别性的DF解，学习同时最大化期望回报和抵御奖励函数变化的模型，可以显著提高DF对奖励函数变化的稳健性，而不会降低智能体的总回报。 |
| [^18] | [BODEGA: Benchmark for Adversarial Example Generation in Credibility Assessment.](http://arxiv.org/abs/2303.08032) | BODEGA是一个基准测试，用于模拟真实的内容管理场景，在四个误传检测任务上测试受害模型和攻击方法。测试结果表明，在某些情况下，即使进行微小的文本修改，也可以欺骗最准确的分类器。 |
| [^19] | [Risk-Sensitive Reinforcement Learning with Exponential Criteria.](http://arxiv.org/abs/2212.09010) | 本文介绍了一种风险敏感的强化学习算法，使用指数判据来提高其系统抗干扰性和实用性。作者进行了在模拟和实际机器人上的实验验证，表明该算法能够有效地提高样本效率和执行效果。 |

# 详细

[^1]: 在图上的大型语言模型：一项全面调查

    Large Language Models on Graphs: A Comprehensive Survey

    [https://rss.arxiv.org/abs/2312.02783](https://rss.arxiv.org/abs/2312.02783)

    这篇论文对在图上的大型语言模型进行了全面调查，研究了纯图形、文本属性图形和文本配对图形三个不同场景下的应用情况，并探讨了基于图形的推理能力是否可以推广到大型语言模型上。

    

    大型语言模型（LLMs），如GPT4和LLaMA，由于其强大的文本编码/解码能力和新发现的紧急能力（例如推理）在自然语言处理方面取得了显著的进展。虽然LLMs主要设计用于处理纯文本，但在许多现实场景中，文本数据与图形形式的丰富结构信息相关联（例如学术网络和电子商务网络），或者图形数据与丰富的文本信息配对（例如带有描述的分子）。此外，尽管LLMs已经展示了其基于纯文本的推理能力，但尚未探索此类能力是否可以推广到图形上（即基于图形的推理）。在本文中，我们对在图上的大型语言模型相关场景和技术进行了系统回顾。我们首先总结了采用LLMs在图形上的潜在场景，分为纯图形、文本属性图形和文本配对图形三个类别。

    Large language models (LLMs), such as GPT4 and LLaMA, are creating significant advancements in natural language processing, due to their strong text encoding/decoding ability and newly found emergent capability (e.g., reasoning). While LLMs are mainly designed to process pure texts, there are many real-world scenarios where text data is associated with rich structure information in the form of graphs (e.g., academic networks, and e-commerce networks) or scenarios where graph data is paired with rich textual information (e.g., molecules with descriptions). Besides, although LLMs have shown their pure text-based reasoning ability, it is underexplored whether such ability can be generalized to graphs (i.e., graph-based reasoning). In this paper, we provide a systematic review of scenarios and techniques related to large language models on graphs. We first summarize potential scenarios of adopting LLMs on graphs into three categories, namely pure graphs, text-attributed graphs, and text-pa
    
[^2]: 基于自定义生物启发目标的无监督端到端训练

    Unsupervised End-to-End Training with a Self-Defined Bio-Inspired Target

    [https://arxiv.org/abs/2403.12116](https://arxiv.org/abs/2403.12116)

    本研究提出了一种使用Winner-Take-All（WTA）选择性和生物启发的稳态机制相结合的“自定义目标”方法，旨在解决无监督学习方法在边缘AI硬件上的计算资源稀缺性问题。

    

    当前的无监督学习方法依赖于通过深度学习技术（如自监督学习）进行端到端训练，具有较高的计算需求，或者采用通过类似Hebbian学习的生物启发方法逐层训练，使用与监督学习不兼容的局部学习规则。为了解决这一挑战，在这项工作中，我们引入了一种使用网络最终层的胜者通吃（WTA）选择性的“自定义目标”，并通过生物启发的稳态机制进行正则化。

    arXiv:2403.12116v1 Announce Type: cross  Abstract: Current unsupervised learning methods depend on end-to-end training via deep learning techniques such as self-supervised learning, with high computational requirements, or employ layer-by-layer training using bio-inspired approaches like Hebbian learning, using local learning rules incompatible with supervised learning. Both approaches are problematic for edge AI hardware that relies on sparse computational resources and would strongly benefit from alternating between unsupervised and supervised learning phases - thus leveraging widely available unlabeled data from the environment as well as labeled training datasets. To solve this challenge, in this work, we introduce a 'self-defined target' that uses Winner-Take-All (WTA) selectivity at the network's final layer, complemented by regularization through biologically inspired homeostasis mechanism. This approach, framework-agnostic and compatible with both global (Backpropagation) and l
    
[^3]: Linguacodus：一种在机器学习流水线中进行变革性代码生成的协同框架

    Linguacodus: A Synergistic Framework for Transformative Code Generation in Machine Learning Pipelines

    [https://arxiv.org/abs/2403.11585](https://arxiv.org/abs/2403.11585)

    Linguacodus是一种创新框架，通过部署动态流水线和精细调整的大型语言模型，实现了将自然语言任务描述转换为代码的自动化过程，极大地推进了机器学习应用的发展。

    

    在不断发展的机器学习领域中，将自然语言描述无缝转化为可执行代码仍然是一个巨大的挑战。本文介绍了Linguacodus，这是一个创新性框架，旨在通过部署一个动态流水线，通过高级数据塑形指令，将自然语言任务描述迭代地转换为代码来应对这一挑战。Linguacodus的核心是一个经过精细调整的大型语言模型（LLM），能够评估各种问题的多样解决方案，并为特定任务选择最合适的解决方案。本文详细介绍了精细调整过程，并阐明了如何将自然语言描述转化为功能性代码。Linguacodus代表了自动化代码生成的重大飞跃，有效地弥合了任务描述和可执行代码之间的差距。它对推进跨不同领域的机器学习应用具有巨大潜力。

    arXiv:2403.11585v1 Announce Type: cross  Abstract: In the ever-evolving landscape of machine learning, seamless translation of natural language descriptions into executable code remains a formidable challenge. This paper introduces Linguacodus, an innovative framework designed to tackle this challenge by deploying a dynamic pipeline that iteratively transforms natural language task descriptions into code through high-level data-shaping instructions. The core of Linguacodus is a fine-tuned large language model (LLM), empowered to evaluate diverse solutions for various problems and select the most fitting one for a given task. This paper details the fine-tuning process, and sheds light on how natural language descriptions can be translated into functional code. Linguacodus represents a substantial leap towards automated code generation, effectively bridging the gap between task descriptions and executable code. It holds great promise for advancing machine learning applications across div
    
[^4]: 图神经网络与算术电路

    Graph Neural Networks and Arithmetic Circuits

    [https://arxiv.org/abs/2402.17805](https://arxiv.org/abs/2402.17805)

    研究者在本文中建立了图神经网络与算术电路之间的表达能力对应关系，结果表明不同激活函数的GNN在表达能力上等价于实数上的算术电路。

    

    我们表征了遵循图神经网络（GNN）架构的神经网络的计算能力，不限于聚合-组合GNN或其他特定类型。我们建立了使用不同激活函数的GNN的表达能力与实数上的算术电路之间的准确对应关系。在我们的结果中，网络的激活函数成为电路中的门类型。我们的结果对于常数深度电路和网络家族均成立，无论是在一致还是非一致的情况下，对于所有常见激活函数。

    arXiv:2402.17805v1 Announce Type: cross  Abstract: We characterize the computational power of neural networks that follow the graph neural network (GNN) architecture, not restricted to aggregate-combine GNNs or other particular types. We establish an exact correspondence between the expressivity of GNNs using diverse activation functions and arithmetic circuits over real numbers. In our results the activation function of the network becomes a gate type in the circuit. Our result holds for families of constant depth circuits and networks, both uniformly and non-uniformly, for all common activation functions.
    
[^5]: 图神经网络作为有限元冰盖模拟的快速高保真度的仿真器

    Graph Neural Networks as Fast and High-fidelity Emulators for Finite-Element Ice Sheet Modeling

    [https://arxiv.org/abs/2402.05291](https://arxiv.org/abs/2402.05291)

    本研究开发了图神经网络作为有限元冰盖模拟的快速高保真度的仿真器，并在Pine Island Glacier的瞬态模拟中展示了与传统卷积神经网络和多层感知器相比更准确的重现冰盖厚度和速度的能力。同时，这些GNN成功捕捉到了更高底部熔化速率引起的冰质量损失和加速过程。在图形处理单元上实现的GNN仿真器显示出高达50倍的加速。

    

    虽然冰盖和海平面系统模型（ISSM）的有限元方法可以快速准确地解决由Stokes方程描述的冰动力学问题，但这种数值建模需要在中央处理单元（CPU）上进行密集的计算。在本研究中，我们开发了图神经网络（GNN）作为快速代理模型来保持ISSM的有限元结构。利用Pine Island Glacier（PIG）的20年瞬态模拟，我们训练和测试了三个GNN：图卷积网络（GCN），图注意力网络（GAT）和等变图卷积网络（EGCN）。这些GNN与经典卷积神经网络（CNN）和多层感知器（MLP）相比，能够更准确地重现冰厚度和速度。特别是，在PIG中，GNN成功捕捉到了由更高底部熔化速率引起的冰质量损失和加速。当我们的GNN仿真器在图形处理单元（GPU）上实现时，它们显示出高达50倍的加速。

    Although the finite element approach of the Ice-sheet and Sea-level System Model (ISSM) solves ice dynamics problems governed by Stokes equations quickly and accurately, such numerical modeling requires intensive computation on central processing units (CPU). In this study, we develop graph neural networks (GNN) as fast surrogate models to preserve the finite element structure of ISSM. Using the 20-year transient simulations in the Pine Island Glacier (PIG), we train and test three GNNs: graph convolutional network (GCN), graph attention network (GAT), and equivariant graph convolutional network (EGCN). These GNNs reproduce ice thickness and velocity with better accuracy than the classic convolutional neural network (CNN) and multi-layer perception (MLP). In particular, GNNs successfully capture the ice mass loss and acceleration induced by higher basal melting rates in the PIG. When our GNN emulators are implemented on graphic processing units (GPUs), they show up to 50 times faster c
    
[^6]: 模拟过度参数化

    Simulated Overparameterization

    [https://arxiv.org/abs/2402.05033](https://arxiv.org/abs/2402.05033)

    模拟过度参数化（SOP）是一种将紧凑模型的计算效率与过度参数化模型的高级学习能力相结合的新范式。通过使用模拟训练过度参数化模型的方法，我们提出了一种与主要架构无关的算法，称为"majority kernels"，从而在各种架构和任务中实现性能提升。

    

    在这项工作中，我们引入了一种新的范式，称为模拟过度参数化（SOP）。SOP将紧凑模型的计算效率与过度参数化模型的高级学习能力相结合。SOP提出了一种独特的模型训练和推断方法，在推断过程中，使用显著更多参数的模型进行训练，但只使用其中较小、高效的子集进行实际计算。在此框架的基础上，我们提出了一种新颖的、与主要架构（包括Transformer模型）无关的算法，称为"majority kernels"。 majority kernels使得模拟训练过度参数化模型成为可能，从而在各种架构和任务中取得性能提升。此外，我们的方法在训练时对于计算成本（墙上挂钟时间）的增加非常小。所提出的方法在各种数据集和任务上表现出很强的性能。

    In this work, we introduce a novel paradigm called Simulated Overparametrization (SOP). SOP merges the computational efficiency of compact models with the advanced learning proficiencies of overparameterized models. SOP proposes a unique approach to model training and inference, where a model with a significantly larger number of parameters is trained in such a way that a smaller, efficient subset of these parameters is used for the actual computation during inference. Building upon this framework, we present a novel, architecture agnostic algorithm called "majority kernels", which seamlessly integrates with predominant architectures, including Transformer models. Majority kernels enables the simulated training of overparameterized models, resulting in performance gains across architectures and tasks. Furthermore, our approach adds minimal overhead to the cost incurred (wall clock time) at training time. The proposed approach shows strong performance on a wide variety of datasets and m
    
[^7]: 缩小SGP4和高精度传播之间的差距：通过可微编程

    Closing the Gap Between SGP4 and High-Precision Propagation via Differentiable Programming

    [https://arxiv.org/abs/2402.04830](https://arxiv.org/abs/2402.04830)

    本研究介绍了dSGP4，一种使用PyTorch实现的可微版本的SGP4。通过可微化，dSGP4实现了轨道传播的高精度，并且适用于各种与太空相关的应用，包括卫星轨道确定、状态转换、协方差传播等。

    

    简化的第四级摄动(SGP4)轨道传播方法被广泛用于快速可靠地预测地球轨道物体的位置和速度。尽管不断改进，SGP模型仍然缺乏数值传播器的精度，后者的误差显著较小。本研究提出了dSGP4，一种使用PyTorch实现的新型可微版本的SGP4。通过使SGP4可微化，dSGP4便于进行各种与太空相关的应用，包括航天器轨道确定、状态转换、协方差转换、状态转移矩阵计算和协方差传播。此外，dSGP4的PyTorch实现允许在批量的TLE（两行参数）集上进行尴尬的并行轨道传播，利用CPU、GPU和分布式预测卫星位置的高级硬件的计算能力。此外，dSGP4的可微性使其能与模式集成。

    The Simplified General Perturbations 4 (SGP4) orbital propagation method is widely used for predicting the positions and velocities of Earth-orbiting objects rapidly and reliably. Despite continuous refinement, SGP models still lack the precision of numerical propagators, which offer significantly smaller errors. This study presents dSGP4, a novel differentiable version of SGP4 implemented using PyTorch. By making SGP4 differentiable, dSGP4 facilitates various space-related applications, including spacecraft orbit determination, state conversion, covariance transformation, state transition matrix computation, and covariance propagation. Additionally, dSGP4's PyTorch implementation allows for embarrassingly parallel orbital propagation across batches of Two-Line Element Sets (TLEs), leveraging the computational power of CPUs, GPUs, and advanced hardware for distributed prediction of satellite positions at future times. Furthermore, dSGP4's differentiability enables integration with mode
    
[^8]: 单层图卷积网络的渐进泛化误差

    Asymptotic generalization error of a single-layer graph convolutional network

    [https://arxiv.org/abs/2402.03818](https://arxiv.org/abs/2402.03818)

    本研究针对单层图卷积网络（GCN）在高维极限下的性能进行了预测，并推广了对多种数据模型的分析。我们的研究显示，尽管GCN在收敛速度上是一致的，但在任何情况下都不能达到贝叶斯最优率。

    

    虽然图卷积网络在实践中展现出很大的潜力，但是相对于广泛研究的全连接神经网络，关于其泛化特性与样本数量的理论理解仍处于初级阶段。在本文中，我们预测了在高维极限下，基于属性随机块模型（SBM）生成的数据训练的单层图卷积网络（GCN）的性能。之前，仅在Shi等人的文章中考虑了上下文-SBM（CSBM）上的岭回归分析；我们将分析推广到CSBM的任意凸损失和正则化方法，并添加了对另一个数据模型——神经优先SBM的分析。我们还研究了高信噪比极限，并详细介绍了GCN的收敛速度，并且展示了尽管一致，但对于任何考虑的情况都不能达到贝叶斯最优率。

    While graph convolutional networks show great practical promises, the theoretical understanding of their generalization properties as a function of the number of samples is still in its infancy compared to the more broadly studied case of supervised fully connected neural networks. In this article, we predict the performances of a single-layer graph convolutional network (GCN) trained on data produced by attributed stochastic block models (SBMs) in the high-dimensional limit. Previously, only ridge regression on contextual-SBM (CSBM) has been considered in Shi et al. 2022; we generalize the analysis to arbitrary convex loss and regularization for the CSBM and add the analysis for another data model, the neural-prior SBM. We also study the high signal-to-noise ratio limit, detail the convergence rates of the GCN and show that, while consistent, it does not reach the Bayes-optimal rate for any of the considered cases.
    
[^9]: 无监督领域自适应回归中的不确定性引导对齐

    Uncertainty-Guided Alignment for Unsupervised Domain Adaptation in Regression. (arXiv:2401.13721v1 [cs.CV])

    [http://arxiv.org/abs/2401.13721](http://arxiv.org/abs/2401.13721)

    该论文提出了一种利用不确定性引导的无监督领域自适应回归方法，通过将不确定性作为置信度估计和嵌入空间的正则项来实现对齐。

    

    无监督领域自适应回归（UDAR）旨在将来自有标签源领域的模型调整到无标签目标领域，以完成回归任务。最近在UDAR领域取得的成功主要集中在子空间对齐上，涉及整个特征空间中所选择子空间的对齐。这与用于分类的特征对齐方法形成对比，后者旨在对齐整个特征空间，在分类任务中已被证明是有效的，但在回归任务中效果较差。具体而言，分类任务旨在在整个嵌入空间的维度上识别独立的簇，而回归任务对数据表示的结构性要求较低，需要额外的指导以实现有效的对齐。在本文中，我们提出了一种通过不确定性引导的有效UDAR方法。我们的方法具有双重作用：提供了对预测结果的置信度衡量，并作为嵌入空间的正则化。具体而言，我们利用深度证据模型来提供对预测的置信度估计，并将其作为嵌入空间的正则项进行优化。

    Unsupervised Domain Adaptation for Regression (UDAR) aims to adapt a model from a labeled source domain to an unlabeled target domain for regression tasks. Recent successful works in UDAR mostly focus on subspace alignment, involving the alignment of a selected subspace within the entire feature space. This contrasts with the feature alignment methods used for classification, which aim at aligning the entire feature space and have proven effective but are less so in regression settings. Specifically, while classification aims to identify separate clusters across the entire embedding dimension, regression induces less structure in the data representation, necessitating additional guidance for efficient alignment. In this paper, we propose an effective method for UDAR by incorporating guidance from uncertainty. Our approach serves a dual purpose: providing a measure of confidence in predictions and acting as a regularization of the embedding space. Specifically, we leverage the Deep Evid
    
[^10]: SNNs中基于关键性的高效修剪方法，受到关键性大脑假设的启发

    Criticality-Guided Efficient Pruning in Spiking Neural Networks Inspired by Critical Brain Hypothesis. (arXiv:2311.16141v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2311.16141](http://arxiv.org/abs/2311.16141)

    本研究受到神经科学中的关键大脑假设的启发，提出了一种基于神经元关键性的高效SNN修剪方法，以加强特征提取和加速修剪过程，并取得了比当前最先进方法更好的性能。

    

    由于其节能和无乘法特性，SNNs已经引起了相当大的关注。深度SNNs规模的不断增长给模型部署带来了挑战。网络修剪通过压缩网络规模来减少模型部署的硬件资源需求。然而，现有的SNN修剪方法由于修剪迭代增加了SNNs的训练难度，导致修剪成本高昂且性能损失严重。本文受到神经科学中的关键大脑假设的启发，提出了一种基于神经元关键性的用于SNN修剪的再生机制，以增强特征提取并加速修剪过程。首先，我们提出了一种SNN中用于关键性的低成本度量方式。然后，在修剪后对所修剪结构进行重新排序，并再生那些具有较高关键性的结构，以获取关键网络。我们的方法表现优于当前的最先进方法。

    Spiking Neural Networks (SNNs) have gained considerable attention due to the energy-efficient and multiplication-free characteristics. The continuous growth in scale of deep SNNs poses challenges for model deployment. Network pruning reduces hardware resource requirements of model deployment by compressing the network scale. However, existing SNN pruning methods cause high pruning costs and performance loss because the pruning iterations amplify the training difficulty of SNNs. In this paper, inspired by the critical brain hypothesis in neuroscience, we propose a regeneration mechanism based on the neuron criticality for SNN pruning to enhance feature extraction and accelerate the pruning process. Firstly, we propose a low-cost metric for the criticality in SNNs. Then, we re-rank the pruned structures after pruning and regenerate those with higher criticality to obtain the critical network. Our method achieves higher performance than the current state-of-the-art (SOTA) method with up t
    
[^11]: 多任务深度卷积网络预测北冰洋海冰浓度和漂移

    Multi-task Deep Convolutional Network to Predict Sea Ice Concentration and Drift in the Arctic Ocean. (arXiv:2311.00167v1 [cs.LG])

    [http://arxiv.org/abs/2311.00167](http://arxiv.org/abs/2311.00167)

    提出了一种名为HIS-Unet的多任务深度卷积网络架构，通过加权注意力模块实现海冰浓度和漂移的预测。与其他方法相比，HIS-Unet在海冰预测中取得了显著的改进。

    

    在北冰洋地区，预测海冰浓度(SIC)和海冰漂移(SID)具有重要意义，因为最近的气候变暖已经改变了这个环境。由于物理海冰模型需要高计算成本和复杂的参数化，深度学习技术可以有效替代物理模型，并提高海冰预测的性能。本研究提出了一种新颖的多任务全卷积网络架构，名为Hierarchical Information-Sharing U-Net (HIS-Unet)，用于预测每日的SIC和SID。我们通过加权注意力模块(WAMs)允许SIC和SID层共享信息，并互相辅助预测。结果表明，相比于其他统计方法、海冰物理模型和没有信息共享单元的神经网络，我们的HIS-Unet在SIC和SID预测方面表现更优。在预测北冰洋海冰浓度和漂移方面，HIS-Unet的改进都是显著的。

    Forecasting sea ice concentration (SIC) and sea ice drift (SID) in the Arctic Ocean is of great significance as the Arctic environment has been changed by the recent warming climate. Given that physical sea ice models require high computational costs with complex parameterization, deep learning techniques can effectively replace the physical model and improve the performance of sea ice prediction. This study proposes a novel multi-task fully conventional network architecture named hierarchical information-sharing U-net (HIS-Unet) to predict daily SIC and SID. Instead of learning SIC and SID separately at each branch, we allow the SIC and SID layers to share their information and assist each other's prediction through the weighting attention modules (WAMs). Consequently, our HIS-Unet outperforms other statistical approaches, sea ice physical models, and neural networks without such information-sharing units. The improvement of HIS-Unet is obvious both for SIC and SID prediction when and
    
[^12]: CAMELL：基于置信度的高效自监督主动学习与标签验证获取模型

    CAMELL: Confidence-based Acquisition Model for Efficient Self-supervised Active Learning with Label Validation. (arXiv:2310.08944v1 [cs.CL])

    [http://arxiv.org/abs/2310.08944](http://arxiv.org/abs/2310.08944)

    CAMELL是一个适用于序列多输出问题的主动学习框架，通过仅需专家标注序列的一小部分、自监督和标签验证机制来解决监督神经方法对大规模标注数据集的依赖限制。

    

    在序列任务中，受大规模且精确标注数据集的依赖限制，监督神经方法受到阻碍。标注质量随着从专家标注向众包标注的转变而逐渐恶化。为了解决这些挑战，我们提出了CAMELL（Confidence-based Acquisition Model for Efficient self-supervised active Learning with Label validation），这是一个针对序列多输出问题量身定制的基于池化的主动学习框架。CAMELL具有三个核心特点：(1)仅要求专家标注所选序列的一小部分，(2)为其余序列提供自监督，(3)采用标签验证机制，防止错误标签污染数据集并影响模型性能。我们在序列任务中对CAMELL进行了评估，特别强调对话信念跟踪，这是一个受限制的任务。

    Supervised neural approaches are hindered by their dependence on large, meticulously annotated datasets, a requirement that is particularly cumbersome for sequential tasks. The quality of annotations tends to deteriorate with the transition from expert-based to crowd-sourced labelling. To address these challenges, we present \textbf{CAMELL} (Confidence-based Acquisition Model for Efficient self-supervised active Learning with Label validation), a pool-based active learning framework tailored for sequential multi-output problems. CAMELL possesses three core features: (1) it requires expert annotators to label only a fraction of a chosen sequence, (2) it facilitates self-supervision for the remainder of the sequence, and (3) it employs a label validation mechanism to prevent erroneous labels from contaminating the dataset and harming model performance. We evaluate CAMELL on sequential tasks, with a special emphasis on dialogue belief tracking, a task plagued by the constraints of limited
    
[^13]: 数据中心化图学习：一份综述

    Data-centric Graph Learning: A Survey. (arXiv:2310.04987v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.04987](http://arxiv.org/abs/2310.04987)

    本综述从数据中心化的角度全面评估了图学习方法，回答了何时修改图数据、图数据的哪一部分需要修改以及如何保护图模型的关键问题。

    

    人工智能的历史见证了高质量数据对各种深度学习模型的重大影响，例如AlexNet和ResNet的ImageNet。最近，与以模型为中心的方法设计更复杂的神经结构不同，人工智能社区的关注重点转向了以数据为中心的方法，该方法侧重于更好地处理数据以增强神经模型的能力。而在深度学习时代，操作普遍存在的拓扑数据的图学习也发挥着重要作用。在本综述中，我们从数据中心化的角度全面评估了图学习方法，并旨在回答三个关键问题：（1）何时修改图数据，（2）图数据的哪一部分需要修改以释放各种图模型的潜力，以及（3）如何保护图模型免受有问题的数据的影响。因此，我们提出了一个基于图学习流程阶段的创新分类法，并突出了关键创新和贡献。

    The history of artificial intelligence (AI) has witnessed the significant impact of high-quality data on various deep learning models, such as ImageNet for AlexNet and ResNet. Recently, instead of designing more complex neural architectures as model-centric approaches, the attention of AI community has shifted to data-centric ones, which focuses on better processing data to strengthen the ability of neural models. Graph learning, which operates on ubiquitous topological data, also plays an important role in the era of deep learning. In this survey, we comprehensively review graph learning approaches from the data-centric perspective, and aim to answer three crucial questions: (1) when to modify graph data, (2) what part of the graph data needs modification to unlock the potential of various graph models, and (3) how to safeguard graph models from problematic data influence. Accordingly, we propose a novel taxonomy based on the stages in the graph learning pipeline, and highlight the pr
    
[^14]: 用于开发协作分布式机器学习系统的设计工具箱

    A Design Toolbox for the Development of Collaborative Distributed Machine Learning Systems. (arXiv:2309.16584v1 [cs.MA])

    [http://arxiv.org/abs/2309.16584](http://arxiv.org/abs/2309.16584)

    我们开发了一个CDML设计工具箱，可以指导开发者设计满足用例要求的协作分布式机器学习系统。

    

    为了在保护机器学习模型的机密性的同时利用来自多方的训练数据对模型进行充分训练，研究人员开发了各种协作分布式机器学习（CDML）系统设计，例如辅助学习、联邦学习和分裂学习。CDML系统设计展示了不同的特征，例如高度的代理人自治性、机器学习模型的机密性和容错性。面对不同特征的各种CDML系统设计，开发者很难有针对性地设计满足用例要求的CDML系统。然而，不合适的CDML系统设计可能导致CDML系统无法实现其预期目的。我们开发了一个CDML设计工具箱，可以指导CDML系统的开发。基于CDML设计工具箱，我们提出了具有不同关键特征的CDML系统典型，可以支持设计满足用例要求的CDML系统。

    To leverage training data for the sufficient training of ML models from multiple parties in a confidentiality-preserving way, various collaborative distributed machine learning (CDML) system designs have been developed, for example, to perform assisted learning, federated learning, and split learning. CDML system designs show different traits, for example, high agent autonomy, machine learning (ML) model confidentiality, and fault tolerance. Facing a wide variety of CDML system designs with different traits, it is difficult for developers to design CDML systems with traits that match use case requirements in a targeted way. However, inappropriate CDML system designs may result in CDML systems failing their envisioned purposes. We developed a CDML design toolbox that can guide the development of CDML systems. Based on the CDML design toolbox, we present CDML system archetypes with distinct key traits that can support the design of CDML systems to meet use case requirements.
    
[^15]: 在深度学习中的可验证准确性、鲁棒性和泛化的边界

    The Boundaries of Verifiable Accuracy, Robustness, and Generalisation in Deep Learning. (arXiv:2309.07072v1 [cs.LG])

    [http://arxiv.org/abs/2309.07072](http://arxiv.org/abs/2309.07072)

    本文评估了在分类任务中确定神经网络稳定性和准确性的理论限制，发现在一定的神经架构类别中，计算和验证理想的稳定和准确的神经网络是极具挑战性的甚至可能是不可能的。

    

    在这项工作中，我们评估了在分类任务中确定神经网络稳定性和准确性的理论限制。我们考虑了经典的分布无关框架和最小化经验风险的算法，可能还受到一些权重正则化的影响。我们证明，对于很多任务来说，在上述设置下计算和验证理想的稳定和准确的神经网络是极具挑战性的，甚至可能是不可能的，即使在给定的神经架构类别中存在这样的理想解决方案。

    In this work, we assess the theoretical limitations of determining guaranteed stability and accuracy of neural networks in classification tasks. We consider classical distribution-agnostic framework and algorithms minimising empirical risks and potentially subjected to some weights regularisation. We show that there is a large family of tasks for which computing and verifying ideal stable and accurate neural networks in the above settings is extremely challenging, if at all possible, even when such ideal solutions exist within the given class of neural architectures.
    
[^16]: 变分分布先验与显著性图重新播放的融合，用于连续三维重建

    A Fusion of Variational Distribution Priors and Saliency Map Replay for Continual 3D Reconstruction. (arXiv:2308.08812v1 [cs.CV])

    [http://arxiv.org/abs/2308.08812](http://arxiv.org/abs/2308.08812)

    本研究提出了一种基于连续学习的三维重建方法，通过使用变分先验和显著性图重新播放，实现对之前已见类别的合理重建。实验结果表明，与已建立的方法相比具有竞争力。

    

    单图像三维重建是研究如何根据单视角图像预测三维物体形状的一个挑战。这个任务需要大量数据获取来预测形状的可见和遮挡部分。此外，基于学习的方法面临创建针对所有可能类别的全面训练数据集的困难。为此，我们提出了一种基于连续学习的三维重建方法，我们的目标是设计一个使用变分先验的模型，即使在训练新类别后仍可以合理重建以前见过的类别。变分先验代表抽象形状并避免遗忘，而显著性图以较少的内存使用保留对象属性。这对于存储大量训练数据的资源限制至关重要。此外，我们引入了基于显著性图的经验重放，以捕捉全局和独特的对象特征。详细的实验显示与已建立方法相比具有竞争力的结果。

    Single-image 3D reconstruction is a research challenge focused on predicting 3D object shapes from single-view images. This task requires significant data acquisition to predict both visible and occluded portions of the shape. Furthermore, learning-based methods face the difficulty of creating a comprehensive training dataset for all possible classes. To this end, we propose a continual learning-based 3D reconstruction method where our goal is to design a model using Variational Priors that can still reconstruct the previously seen classes reasonably even after training on new classes. Variational Priors represent abstract shapes and combat forgetting, whereas saliency maps preserve object attributes with less memory usage. This is vital due to resource constraints in storing extensive training data. Additionally, we introduce saliency map-based experience replay to capture global and distinct object features. Thorough experiments show competitive results compared to established method
    
[^17]: 奖励转移的稳健决策重点学习

    Robust Decision-Focused Learning for Reward Transfer. (arXiv:2304.03365v1 [cs.LG])

    [http://arxiv.org/abs/2304.03365](http://arxiv.org/abs/2304.03365)

    本文介绍了一种稳健决策重点（RDF）算法，利用非识别性的DF解，学习同时最大化期望回报和抵御奖励函数变化的模型，可以显著提高DF对奖励函数变化的稳健性，而不会降低智能体的总回报。

    

    最近，决策重点（Decision-focused，DF）的基于模型的强化学习被介绍为一种强有力的算法，它可以专注于学习最有利于获得高报酬的MDP动态。虽然这种方法通过专注于直接优化报酬来提高智能体的性能，但从MLE的角度来看，它学习的动力学不够准确，因此可能对奖励函数的变化很脆弱。在这项工作中，我们开发了稳健决策重点（RDF）算法，它利用DF解的非识别性，学习同时最大化期望回报和抵御奖励函数变化的模型。我们在各种玩具示例和医疗模拟器上展示了RDF显着增加了DF对奖励函数变化的稳健性，而不会降低智能体的总回报。

    Decision-focused (DF) model-based reinforcement learning has recently been introduced as a powerful algorithm which can focus on learning the MDP dynamics which are most relevant for obtaining high rewards. While this approach increases the performance of agents by focusing the learning towards optimizing for the reward directly, it does so by learning less accurate dynamics (from a MLE standpoint), and may thus be brittle to changes in the reward function. In this work, we develop the robust decision-focused (RDF) algorithm which leverages the non-identifiability of DF solutions to learn models which maximize expected returns while simultaneously learning models which are robust to changes in the reward function. We demonstrate on a variety of toy example and healthcare simulators that RDF significantly increases the robustness of DF to changes in the reward function, without decreasing the overall return the agent obtains.
    
[^18]: BODEGA: 针对可信度评估中对抗性样本生成的基准测试

    BODEGA: Benchmark for Adversarial Example Generation in Credibility Assessment. (arXiv:2303.08032v1 [cs.CL])

    [http://arxiv.org/abs/2303.08032](http://arxiv.org/abs/2303.08032)

    BODEGA是一个基准测试，用于模拟真实的内容管理场景，在四个误传检测任务上测试受害模型和攻击方法。测试结果表明，在某些情况下，即使进行微小的文本修改，也可以欺骗最准确的分类器。

    

    文本分类方法被广泛应用于检测不可信内容，如假新闻、社交媒体机器人、宣传等。较为准确的模型（可能基于深度神经网络）有助于管理公共电子平台，并经常导致内容创建者面临提交拒绝或已发布文本的撤下。为了避免进一步被检测，内容创建者尝试产生一个稍微修改过的文本版本（即攻击对抗性样本），利用分类器的弱点导致不同的输出。本文介绍了BODEGA：一个基准测试，用于在模拟内容管理的真实用例中测试受害模型和攻击方法在四个误传检测任务上的表现。我们还系统地测试了受欢迎的文本分类器对可用攻击技术的鲁棒性，并发现在某些情况下，即使在文本中进行微小的修改也可以欺骗最准确的分类器。

    Text classification methods have been widely investigated as a way to detect content of low credibility: fake news, social media bots, propaganda, etc. Quite accurate models (likely based on deep neural networks) help in moderating public electronic platforms and often cause content creators to face rejection of their submissions or removal of already published texts. Having the incentive to evade further detection, content creators try to come up with a slightly modified version of the text (known as an attack with an adversarial example) that exploit the weaknesses of classifiers and result in a different output. Here we introduce BODEGA: a benchmark for testing both victim models and attack methods on four misinformation detection tasks in an evaluation framework designed to simulate real use-cases of content moderation. We also systematically test the robustness of popular text classifiers against available attacking techniques and discover that, indeed, in some cases barely signif
    
[^19]: 风险敏感的强化学习算法：指数标准的应用

    Risk-Sensitive Reinforcement Learning with Exponential Criteria. (arXiv:2212.09010v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2212.09010](http://arxiv.org/abs/2212.09010)

    本文介绍了一种风险敏感的强化学习算法，使用指数判据来提高其系统抗干扰性和实用性。作者进行了在模拟和实际机器人上的实验验证，表明该算法能够有效地提高样本效率和执行效果。

    

    尽管风险中性的强化学习已经在很多应用中得到了实验成功，但是这种方法容易受到噪声和系统参数扰动的影响而不够稳健。因此,对风险敏感的强化学习算法进行了研究，以提高其系统抗干扰性，样本效率和实用性。本文介绍了一种新型的无模型风险敏感学习算法，将广泛使用的策略梯度算法进行变体，其实现过程类似。具体来说，本文研究了指数标准对强化学习代理的策略风险敏感性的影响，并开发了蒙特卡罗策略梯度算法和在线(时间差分)演员-评论家算法的变体。分析结果表明，指数标准的使用能够推广常用的特定正则化方法。作者在摆动杆和摆摆杆任务上进行了测试，验证了所提出的算法的实现性能和稳健性。

    While risk-neutral reinforcement learning has shown experimental success in a number of applications, it is well-known to be non-robust with respect to noise and perturbations in the parameters of the system. For this reason, risk-sensitive reinforcement learning algorithms have been studied to introduce robustness and sample efficiency, and lead to better real-life performance. In this work, we introduce new model-free risk-sensitive reinforcement learning algorithms as variations of widely-used Policy Gradient algorithms with similar implementation properties. In particular, we study the effect of exponential criteria on the risk-sensitivity of the policy of a reinforcement learning agent, and develop variants of the Monte Carlo Policy Gradient algorithm and the online (temporal-difference) Actor-Critic algorithm. Analytical results showcase that the use of exponential criteria generalize commonly used ad-hoc regularization approaches. The implementation, performance, and robustness 
    

