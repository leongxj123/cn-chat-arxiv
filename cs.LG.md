# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [DeNetDM: Debiasing by Network Depth Modulation](https://arxiv.org/abs/2403.19863) | DeNetDM 是一种基于网络深度调制的新型去偏见方法，通过使用来自专家乘积的训练范式，在创建深浅架构的偏见和去偏见分支后，将知识提炼产生目标去偏见模型，相比当前去偏见技术取得更优异的效果。 |
| [^2] | [Make Large Language Model a Better Ranker](https://arxiv.org/abs/2403.19181) | 本文介绍了一种具有对齐列表排名目标的语言模型框架（ALRO），旨在弥合大型语言模型的能力与推荐系统排名任务的要求之间的差距。 |
| [^3] | [Proprioception Is All You Need: Terrain Classification for Boreal Forests](https://arxiv.org/abs/2403.16877) | 通过引入 BorealTC 数据集，结合现有数据集，我们评估了基于卷积神经网络（CNN）和新颖的状态空间模型（SSM）-Mamba体系结构在北方森林地形分类上的表现。 |
| [^4] | [Learning Transferable Time Series Classifier with Cross-Domain Pre-training from Language Model](https://arxiv.org/abs/2403.12372) | 提出了CrossTimeNet，一个新颖的跨领域自监督预训练学习框架，旨在解决不同领域时间序列数据特性差异带来的挑战，并能有效转换原始时间序列数据。 |
| [^5] | [Visuo-Tactile Pretraining for Cable Plugging](https://arxiv.org/abs/2403.11898) | 本文研究了如何将触觉信息纳入模仿学习平台以在复杂任务中提高性能，通过训练机器人代理插拔USB电缆，实现了在微细操纵任务中的进展。 |
| [^6] | [Deep Bayesian Future Fusion for Self-Supervised, High-Resolution, Off-Road Mapping](https://arxiv.org/abs/2403.11876) | 该论文提出了一种深度贝叶斯未来融合的方法，通过自监督的方式实现高分辨率越野地图的制作，为长程预测提供更好的支持。 |
| [^7] | [Implicit Image-to-Image Schrodinger Bridge for CT Super-Resolution and Denoising](https://arxiv.org/abs/2403.06069) | I3SB方法通过引入非马尔可夫过程，结合损坏的图像改善纹理恢复，在CT超分辨率和去噪任务中表现优异。 |
| [^8] | [PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning](https://arxiv.org/abs/2402.12842) | 提出了PromptKD方法，通过提示调整实现了生成语言模型提取学生友好知识的蒸馏，无需微调整整个教师模型。 |
| [^9] | [Trained Without My Consent: Detecting Code Inclusion In Language Models Trained on Code](https://arxiv.org/abs/2402.09299) | 这项研究关注如何在训练代码的语言模型中检测代码包含，以解决使用这些模型进行代码审计时的版权侵权问题。 |
| [^10] | [HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs](https://arxiv.org/abs/2402.07309) | 本文提出了HyperBERT模型，通过在预训练的BERT模型中引入超图感知层，克服了现有方法在节点分类任务上难以捕捉超图结构信息和文本属性的局限性，提高了模型的效果和泛化能力。 |
| [^11] | [Two Trades is not Baffled: Condense Graph via Crafting Rational Gradient Matching](https://arxiv.org/abs/2402.04924) | 本论文提出了一种新颖的图表压缩方法CTRL，通过优化起点和精细的策略，解决了梯度匹配方向导致的训练轨迹偏差和累积误差问题。 |
| [^12] | [Lens: A Foundation Model for Network Traffic](https://arxiv.org/abs/2402.03646) | "Lens"是一个基于T5架构的基础网络流量模型，通过学习大规模无标签数据的预训练表示，能够在流量理解和生成任务中取得精确的预测和生成。 |
| [^13] | [Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object Detector](https://arxiv.org/abs/2402.03094) | 本文提出了一种跨领域少样本目标检测器，通过增强的开集目标检测方法来解决跨领域数据差异带来的性能下降问题。 |
| [^14] | [On Rademacher Complexity-based Generalization Bounds for Deep Learning](https://arxiv.org/abs/2208.04284) | 该论文研究了基于Rademacher复杂度的方法在对卷积神经网络进行少类别图像分类时生成非空泛化界限。其中的关键技术贡献是发展了针对函数空间和具有一般Lipschitz激活函数的CNNs的新的Talagrand压缩引理。 |
| [^15] | [Operator Learning for Continuous Spatial-Temporal Model with Gradient-Based and Derivative-Free Optimization Methods.](http://arxiv.org/abs/2311.11798) | 本论文提出了一种基于梯度和无导数优化方法的连续时空模型的算子学习框架，具有空间和时间上的分辨率不变性，并能够在短期时间序列和长期统计数据上高效训练。 |
| [^16] | [Deep Reinforcement Learning for Autonomous Cyber Operations: A Survey.](http://arxiv.org/abs/2310.07745) | 深度强化学习被应用于自主网络安全行动有很大潜力，但在实际应用中还面临许多挑战。 |
| [^17] | [Relational Convolutional Networks: A framework for learning representations of hierarchical relations.](http://arxiv.org/abs/2310.03240) | 关系卷积网络是一个学习显式层次关系表示的框架，通过使用多维内积关系模块和关系卷积层，以及基于图元滤波器的群组比较，能够表达更高阶、层次的关系。 |
| [^18] | [Graph-enhanced Optimizers for Structure-aware Recommendation Embedding Evolution.](http://arxiv.org/abs/2310.03032) | 本文提出了一种新颖的结构感知嵌入演化(SEvo)机制，能够以较低的计算开销将图结构信息注入到嵌入中，从而在现代推荐系统中实现更高效的性能。 |
| [^19] | [Combining Survival Analysis and Machine Learning for Mass Cancer Risk Prediction using EHR data.](http://arxiv.org/abs/2309.15039) | 该论文介绍了一种利用 EHR 数据进行大规模肿瘤风险预测的新方法，其创新之处在于只需利用历史的医疗服务代码和诊断信息来实现最小化的数据需求，通过将存活分析和机器学习相结合，可以在大规模应用中实现对患者癌症风险的个性化评估。 |
| [^20] | [Cluster Exploration using Informative Manifold Projections.](http://arxiv.org/abs/2309.14857) | 该论文提出了一种新颖的方法来生成信息丰富的嵌入，以揭示高维数据中的聚类结构。通过线性组合对比PCA和峰度投影追踪两个目标，该方法能够排除先验信息相关的结构并实现有意义的数据分离。 |
| [^21] | [Multivariate Time-Series Anomaly Detection with Contaminated Data: Application to Physiological Signals.](http://arxiv.org/abs/2308.12563) | 这个论文介绍了一种针对带有污染数据的多变量时间序列异常检测的新方法，通过去污和变量依赖建模实现了无监督的异常检测，对于实际场景中的异常检测具有重要意义。 |
| [^22] | [A Differentially Private Weighted Empirical Risk Minimization Procedure and its Application to Outcome Weighted Learning.](http://arxiv.org/abs/2307.13127) | 本文提出了一种差分隐私加权经验风险最小化算法，可以在使用敏感数据的情况下保护隐私。这是第一个在权重ERM中应用差分隐私的算法，并且在一定的条件下提供了严格的DP保证。 |
| [^23] | [MaskedKD: Efficient Distillation of Vision Transformers with Masked Images.](http://arxiv.org/abs/2302.10494) | MaskedKD提出了一种通过遮蔽图像块来显著降低Vision Transformer (ViT)蒸馏成本的方法，而不影响学生模型的预测准确性。 |
| [^24] | [Policy learning "without'' overlap: Pessimism and generalized empirical Bernstein's inequality.](http://arxiv.org/abs/2212.09900) | 本文提出了一种新的离线策略学习算法，它不需要统一交叠假设，而是利用价值的下限置信区间（LCBs）优化策略，因此能够适应允许行为策略演变和倾向性减弱的情况。 |
| [^25] | [DICTDIS: Dictionary Constrained Disambiguation for Improved NMT.](http://arxiv.org/abs/2210.06996) | DICTDIS是一种新颖有词典约束的NMT系统，其利用多个字典候选项进行训练，实现了从多义词中消除翻译歧义的目的，提高了翻译质量。 |

# 详细

[^1]: DeNetDM: 通过网络深度调制来消除偏见

    DeNetDM: Debiasing by Network Depth Modulation

    [https://arxiv.org/abs/2403.19863](https://arxiv.org/abs/2403.19863)

    DeNetDM 是一种基于网络深度调制的新型去偏见方法，通过使用来自专家乘积的训练范式，在创建深浅架构的偏见和去偏见分支后，将知识提炼产生目标去偏见模型，相比当前去偏见技术取得更优异的效果。

    

    当神经网络在偏见数据集上训练时，它们往往会无意间学习到虚假的相关性，从而导致在实现强大的泛化性和鲁棒性方面面临挑战。目前解决这种偏见的方法通常包括利用偏见注释、根据伪偏见标签进行加权重、或通过增强技术增加偏见冲突数据点的多样性。我们引入了DeNetDM，这是一种基于观察结果的新型去偏见方法，浅层神经网络优先学习核心属性，而更深层次的神经网络在获取不同信息时强调偏见。我们利用从专家乘积中推导出的训练范式，创建了深浅架构的偏见和去偏见分支，然后用知识提炼产生目标的去偏见模型。大量实验证明，我们的方法优于当前的去偏见技术，实现了一个...

    arXiv:2403.19863v1 Announce Type: new  Abstract: When neural networks are trained on biased datasets, they tend to inadvertently learn spurious correlations, leading to challenges in achieving strong generalization and robustness. Current approaches to address such biases typically involve utilizing bias annotations, reweighting based on pseudo-bias labels, or enhancing diversity within bias-conflicting data points through augmentation techniques. We introduce DeNetDM, a novel debiasing method based on the observation that shallow neural networks prioritize learning core attributes, while deeper ones emphasize biases when tasked with acquiring distinct information. Using a training paradigm derived from Product of Experts, we create both biased and debiased branches with deep and shallow architectures and then distill knowledge to produce the target debiased model. Extensive experiments and analyses demonstrate that our approach outperforms current debiasing techniques, achieving a not
    
[^2]: 让大型语言模型成为更好的排名器

    Make Large Language Model a Better Ranker

    [https://arxiv.org/abs/2403.19181](https://arxiv.org/abs/2403.19181)

    本文介绍了一种具有对齐列表排名目标的语言模型框架（ALRO），旨在弥合大型语言模型的能力与推荐系统排名任务的要求之间的差距。

    

    大型语言模型（LLMs）的发展显著增强了各个领域的能力，导致推荐系统（RSs）概念和开发方式发生了转变。然而，现有研究主要集中在点对点和成对推荐范式上。这些方法在基于LLM的推荐器中效率低下，因为利用大型语言模型的计算成本很高。一些研究虽然深入研究了列表型方法，但在排名任务中表现不佳。这一不足归因于排名和语言生成目标之间的不匹配。为此，本文介绍了具有对齐列表排名目标的语言模型框架（ALRO）。ALRO旨在弥合LLMs的能力与推荐系统排名任务的微妙要求之间的差距。ALRO的一个关键特性是引入了软lambda值lo

    arXiv:2403.19181v1 Announce Type: cross  Abstract: The evolution of Large Language Models (LLMs) has significantly enhanced capabilities across various fields, leading to a paradigm shift in how Recommender Systems (RSs) are conceptualized and developed. However, existing research primarily focuses on point-wise and pair-wise recommendation paradigms. These approaches prove inefficient in LLM-based recommenders due to the high computational cost of utilizing Large Language Models. While some studies have delved into list-wise approaches, they fall short in ranking tasks. This shortfall is attributed to the misalignment between the objectives of ranking and language generation. To this end, this paper introduces the Language Model Framework with Aligned Listwise Ranking Objectives (ALRO). ALRO is designed to bridge the gap between the capabilities of LLMs and the nuanced requirements of ranking tasks within recommender systems. A key feature of ALRO is the introduction of soft lambda lo
    
[^3]: 感知力就是你所需要的：北方森林的地形分类

    Proprioception Is All You Need: Terrain Classification for Boreal Forests

    [https://arxiv.org/abs/2403.16877](https://arxiv.org/abs/2403.16877)

    通过引入 BorealTC 数据集，结合现有数据集，我们评估了基于卷积神经网络（CNN）和新颖的状态空间模型（SSM）-Mamba体系结构在北方森林地形分类上的表现。

    

    最近的领域机器人学研究强调了抵御不同类型地形的重要性。北方森林特别受到许多限制机动性的地形的影响，这些地形应该在越野自主导航中加以考虑。此外，作为地球上最大的陆地生物群落之一，北方森林是预计自主车辆将日益普及的地区。在本文中，我们通过引入BorealTC来解决这个问题，这是一个用于基于感知力的地形分类（TC）的公开可用数据集。我们的数据集记录了Husky A200的116分钟的惯性测量单元（IMU）、电机电流和轮胎里程数据，重点关注典型的北方森林地形，特别是雪、冰和淤泥壤。结合我们的数据集与另一个来自最新技术的数据集，我们在TC t

    arXiv:2403.16877v1 Announce Type: cross  Abstract: Recent works in field robotics highlighted the importance of resiliency against different types of terrains. Boreal forests, in particular, are home to many mobility-impeding terrains that should be considered for off-road autonomous navigation. Also, being one of the largest land biomes on Earth, boreal forests are an area where autonomous vehicles are expected to become increasingly common. In this paper, we address this issue by introducing BorealTC, a publicly available dataset for proprioceptive-based terrain classification (TC). Recorded with a Husky A200, our dataset contains 116 min of Inertial Measurement Unit (IMU), motor current, and wheel odometry data, focusing on typical boreal forest terrains, notably snow, ice, and silty loam. Combining our dataset with another dataset from the state-of-the-art, we evaluate both a Convolutional Neural Network (CNN) and the novel state space model (SSM)-based Mamba architecture on a TC t
    
[^4]: 使用语言模型的跨领域预训练学习可传递的时间序列分类器

    Learning Transferable Time Series Classifier with Cross-Domain Pre-training from Language Model

    [https://arxiv.org/abs/2403.12372](https://arxiv.org/abs/2403.12372)

    提出了CrossTimeNet，一个新颖的跨领域自监督预训练学习框架，旨在解决不同领域时间序列数据特性差异带来的挑战，并能有效转换原始时间序列数据。

    

    自监督预训练（SSL）的进展显著推动了可传递的时间序列表示领域的发展，这对于增强下游任务非常有用。尽管有效，但大多数现有工作在跨领域SSL预训练方面存在困难，错过了集成不同领域模式和特征的宝贵机会。我们提出了CrossTimeNet，这是一个新颖的跨领域SSL学习框架，旨在从各种领域学习可传递的知识，从而大大增强目标下游任务的效果。

    arXiv:2403.12372v1 Announce Type: new  Abstract: Advancements in self-supervised pre-training (SSL) have significantly advanced the field of learning transferable time series representations, which can be very useful in enhancing the downstream task. Despite being effective, most existing works struggle to achieve cross-domain SSL pre-training, missing valuable opportunities to integrate patterns and features from different domains. The main challenge lies in the significant differences in the characteristics of time-series data across different domains, such as variations in the number of channels and temporal resolution scales. To address this challenge, we propose CrossTimeNet, a novel cross-domain SSL learning framework to learn transferable knowledge from various domains to largely benefit the target downstream task. One of the key characteristics of CrossTimeNet is the newly designed time series tokenization module, which could effectively convert the raw time series into a seque
    
[^5]: 视觉-触觉预训练用于插拔电缆

    Visuo-Tactile Pretraining for Cable Plugging

    [https://arxiv.org/abs/2403.11898](https://arxiv.org/abs/2403.11898)

    本文研究了如何将触觉信息纳入模仿学习平台以在复杂任务中提高性能，通过训练机器人代理插拔USB电缆，实现了在微细操纵任务中的进展。

    

    触觉信息是进行精细操纵的关键工具。作为人类，我们在很大程度上依赖触觉信息来理解周围的物体以及如何与其互动。我们不仅使用触摸来执行操纵任务，还用它来学习如何执行这些任务。因此，为了创建能够学习以人类或超人类水平完成操纵任务的机器人代理，我们需要正确地将触觉信息融入技能执行和技能学习中。本文研究了如何将触觉信息纳入模仿学习平台以提高复杂任务的性能。为此，我们着手解决插拔USB电缆的挑战，这是一项依赖于微观视觉-触觉协作的熟练操纵任务。通过将触觉信息纳入模仿学习框架，我们能够训练一个机器人代理插拔USB电缆。

    arXiv:2403.11898v1 Announce Type: cross  Abstract: Tactile information is a critical tool for fine-grain manipulation. As humans, we rely heavily on tactile information to understand objects in our environments and how to interact with them. We use touch not only to perform manipulation tasks but also to learn how to perform these tasks. Therefore, to create robotic agents that can learn to complete manipulation tasks at a human or super-human level of performance, we need to properly incorporate tactile information into both skill execution and skill learning. In this paper, we investigate how we can incorporate tactile information into imitation learning platforms to improve performance on complex tasks. To do this, we tackle the challenge of plugging in a USB cable, a dexterous manipulation task that relies on fine-grain visuo-tactile serving. By incorporating tactile information into imitation learning frameworks, we are able to train a robotic agent to plug in a USB cable - a firs
    
[^6]: 深度贝叶斯未来融合用于自监督、高分辨率、越野地图制作

    Deep Bayesian Future Fusion for Self-Supervised, High-Resolution, Off-Road Mapping

    [https://arxiv.org/abs/2403.11876](https://arxiv.org/abs/2403.11876)

    该论文提出了一种深度贝叶斯未来融合的方法，通过自监督的方式实现高分辨率越野地图的制作，为长程预测提供更好的支持。

    

    资源受限的越野车辆的传感器分辨率有限，这给可靠的越野自主性带来了巨大挑战。为了克服这一局限性，我们提出了一个基于融合未来信息（即未来融合）进行自监督的通用框架。最近的方法利用未来信息以及手工制作的启发式方法来直接监督目标下游任务（例如可穿越性估计）。然而，在本文中，我们选择了一个更为通用的发展方向 - 通过未来融合以自监督的方式时间高效地完成最高分辨率（即每像素2厘米）BEV地图，可用于任何下游任务以获得更好的长程预测。为此，首先，我们创建了一个高分辨率未来融合数据集，其中包含（RGB / 高度）原始稀疏噪音输入和基于地图的密集标签的成对数据。接下来，为了适应传感器的噪声和稀疏性

    arXiv:2403.11876v1 Announce Type: cross  Abstract: The limited sensing resolution of resource-constrained off-road vehicles poses significant challenges towards reliable off-road autonomy. To overcome this limitation, we propose a general framework based on fusing the future information (i.e. future fusion) for self-supervision. Recent approaches exploit this future information alongside the hand-crafted heuristics to directly supervise the targeted downstream tasks (e.g. traversability estimation). However, in this paper, we opt for a more general line of development - time-efficient completion of the highest resolution (i.e. 2cm per pixel) BEV map in a self-supervised manner via future fusion, which can be used for any downstream tasks for better longer range prediction. To this end, first, we create a high-resolution future-fusion dataset containing pairs of (RGB / height) raw sparse and noisy inputs and map-based dense labels. Next, to accommodate the noise and sparsity of the sens
    
[^7]: 隐式图像对图像Schrodinger桥用于CT超分辨率和去噪

    Implicit Image-to-Image Schrodinger Bridge for CT Super-Resolution and Denoising

    [https://arxiv.org/abs/2403.06069](https://arxiv.org/abs/2403.06069)

    I3SB方法通过引入非马尔可夫过程，结合损坏的图像改善纹理恢复，在CT超分辨率和去噪任务中表现优异。

    

    有条件扩散模型因其在图像恢复任务中的有效性而得到认可，然而，其从高斯噪声开始的迭代去噪过程往往导致推断速度慢。作为一种有希望的替代方案，图像对图像Schrödinger桥（I2SB）从损坏的图像开始初始化生成过程，并集成了有条件扩散模型的训练技术。在本研究中，我们通过引入隐式图像对图像Schrödinger桥（I3SB）扩展了I2SB方法，通过在每一生成步骤中纳入损坏的图像，将其生成过程转换为非马尔可夫过程。这种增强使得I3SB能够在少量生成步骤中生成具有更好纹理恢复的图像。所提出的方法在CT超分辨率和去噪任务上得到验证，并超越了包括有条件去噪扩散概率模型在内的现有方法。

    arXiv:2403.06069v1 Announce Type: cross  Abstract: Conditional diffusion models have gained recognition for their effectiveness in image restoration tasks, yet their iterative denoising process, starting from Gaussian noise, often leads to slow inference speeds. As a promising alternative, the Image-to-Image Schr\"odinger Bridge (I2SB) initializes the generative process from corrupted images and integrates training techniques from conditional diffusion models. In this study, we extended the I2SB method by introducing the Implicit Image-to-Image Schrodinger Bridge (I3SB), transitioning its generative process to a non-Markovian process by incorporating corrupted images in each generative step. This enhancement empowers I3SB to generate images with better texture restoration using a small number of generative steps. The proposed method was validated on CT super-resolution and denoising tasks and outperformed existing methods, including the conditional denoising diffusion probabilistic mod
    
[^8]: PromptKD：通过提示调整为生成语言模型提取学生友好知识的蒸馏方法

    PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning

    [https://arxiv.org/abs/2402.12842](https://arxiv.org/abs/2402.12842)

    提出了PromptKD方法，通过提示调整实现了生成语言模型提取学生友好知识的蒸馏，无需微调整整个教师模型。

    

    近期大型语言模型（LLMs）的发展引起了对推理成本的担忧，进一步增加了对模型压缩研究的需求。尽管知识蒸馏（KD）是一种突出的方法，但是针对LLMs这样的生成语言模型的KD研究相对较少，而提取适合学生的知识的方法，在分类模型的KD中表现出了良好性能，在生成语言模型中尚未被探索。为了探索这种方法，我们提出了PromptKD，一种简单而有效的方法，它利用提示调整 - 在KD中首次出现 - 使生成语言模型能够传递适合学生的知识。与先前分类工作不同，先前那些需要微调整整个教师模型以提取适合学生的知识，PromptKD通过添加少量提示标记，并仅通过学生指导调整提示来达到类似效果。

    arXiv:2402.12842v1 Announce Type: cross  Abstract: Recent advancements in large language models (LLMs) have raised concerns about inference costs, increasing the need for research into model compression. While knowledge distillation (KD) is a prominent method for this, research on KD for generative language models like LLMs is relatively sparse, and the approach of distilling student-friendly knowledge, which has shown promising performance in KD for classification models, remains unexplored in generative language models. To explore this approach, we propose PromptKD, a simple yet effective method that utilizes prompt tuning - for the first time in KD - to enable generative language models to transfer student-friendly knowledge. Unlike previous works in classification that require fine-tuning the entire teacher model for extracting student-friendly knowledge, PromptKD achieves similar effects by adding a small number of prompt tokens and tuning only the prompt with student guidance. Ex
    
[^9]: 未经本人同意的训练：在训练代码的语言模型中检测代码包含

    Trained Without My Consent: Detecting Code Inclusion In Language Models Trained on Code

    [https://arxiv.org/abs/2402.09299](https://arxiv.org/abs/2402.09299)

    这项研究关注如何在训练代码的语言模型中检测代码包含，以解决使用这些模型进行代码审计时的版权侵权问题。

    

    代码审计通过验证开发的代码是否符合标准、法规和版权保护，确保其不包含来自受保护来源的代码。在软件开发过程中，大型语言模型(LLMs)作为编码助手的出现给代码审计带来了新的挑战。训练这些模型的数据集主要来自公开可用的来源。这引发了知识产权侵权问题，因为开发者的代码已包含在数据集中。因此，使用LLMs开发的代码审计具有挑战性，因为我们无法准确确定开发过程中使用的LLM是否已经在特定的受版权保护的代码上进行了训练，因为我们无法获得这些模型的训练数据集。鉴于训练数据集的保密性，传统的代码克隆检测等方法无法确保版权侵权。

    arXiv:2402.09299v1 Announce Type: cross Abstract: Code auditing ensures that the developed code adheres to standards, regulations, and copyright protection by verifying that it does not contain code from protected sources. The recent advent of Large Language Models (LLMs) as coding assistants in the software development process poses new challenges for code auditing. The dataset for training these models is mainly collected from publicly available sources. This raises the issue of intellectual property infringement as developers' codes are already included in the dataset. Therefore, auditing code developed using LLMs is challenging, as it is difficult to reliably assert if an LLM used during development has been trained on specific copyrighted codes, given that we do not have access to the training datasets of these models. Given the non-disclosure of the training datasets, traditional approaches such as code clone detection are insufficient for asserting copyright infringement. To add
    
[^10]: HyperBERT:将混合超图感知层与语言模型用于文本属性超图上的节点分类

    HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs

    [https://arxiv.org/abs/2402.07309](https://arxiv.org/abs/2402.07309)

    本文提出了HyperBERT模型，通过在预训练的BERT模型中引入超图感知层，克服了现有方法在节点分类任务上难以捕捉超图结构信息和文本属性的局限性，提高了模型的效果和泛化能力。

    

    超图通过复杂的拓扑结构标记，表达多个实体之间的高阶相互作用，其中超边扮演重要角色。最近，基于超图的深度学习方法在学习文本属性超图上的节点分类问题中引起了越来越多的研究关注。然而，现有方法往往难以同时捕捉超图结构信息的全部内容和节点属性中的丰富语言属性，这在很大程度上影响了它们的效果和泛化能力。为了克服这些挑战，我们探索了如何通过为节点分类任务进一步增强预训练的BERT模型，引入专门的超图感知层。这些层将高阶结构归纳偏差引入语言模型中，从而提高模型利用超图结构中的高阶上下文信息和文本中的语义信息的能力。

    Hypergraphs are marked by complex topology, expressing higher-order interactions among multiple entities with hyperedges. Lately, hypergraph-based deep learning methods to learn informative data representations for the problem of node classification on text-attributed hypergraphs have garnered increasing research attention. However, existing methods struggle to simultaneously capture the full extent of hypergraph structural information and the rich linguistic attributes inherent in the nodes attributes, which largely hampers their effectiveness and generalizability. To overcome these challenges, we explore ways to further augment a pretrained BERT model with specialized hypergraph-aware layers for the task of node classification. Such layers introduce higher-order structural inductive bias into the language model, thus improving the model's capacity to harness both higher-order context information from the hypergraph structure and semantic information present in text. In this paper, we
    
[^11]: 两个交易不会困扰：通过构造合理的梯度匹配来压缩图表

    Two Trades is not Baffled: Condense Graph via Crafting Rational Gradient Matching

    [https://arxiv.org/abs/2402.04924](https://arxiv.org/abs/2402.04924)

    本论文提出了一种新颖的图表压缩方法CTRL，通过优化起点和精细的策略，解决了梯度匹配方向导致的训练轨迹偏差和累积误差问题。

    

    在大规模图表上训练已经在图表表示学习方面取得了显著成果，但其成本和存储引起了越来越多的关注。作为最有前景的方向之一，图表压缩方法通过使用梯度匹配来解决这些问题，目标是将完整的图表压缩成更简洁但信息丰富的合成集。尽管令人鼓舞，但这些策略主要强调梯度的匹配方向，从而导致训练轨迹的偏差。这种偏差进一步由压缩和评估阶段之间的差异放大，最终导致累积误差，对压缩图表的性能产生不利影响。鉴于此，我们提出了一种名为\textbf{C}raf\textbf{T}ing \textbf{R}ationa\textbf{L} trajectory（\textbf{CTRL}）的新型图表压缩方法，它提供了一个更接近原始数据集特征分布的优化起点和一个更精细的策略。

    Training on large-scale graphs has achieved remarkable results in graph representation learning, but its cost and storage have raised growing concerns. As one of the most promising directions, graph condensation methods address these issues by employing gradient matching, aiming to condense the full graph into a more concise yet information-rich synthetic set. Though encouraging, these strategies primarily emphasize matching directions of the gradients, which leads to deviations in the training trajectories. Such deviations are further magnified by the differences between the condensation and evaluation phases, culminating in accumulated errors, which detrimentally affect the performance of the condensed graphs. In light of this, we propose a novel graph condensation method named \textbf{C}raf\textbf{T}ing \textbf{R}ationa\textbf{L} trajectory (\textbf{CTRL}), which offers an optimized starting point closer to the original dataset's feature distribution and a more refined strategy for 
    
[^12]: Lens: 网络流量的基础模型

    Lens: A Foundation Model for Network Traffic

    [https://arxiv.org/abs/2402.03646](https://arxiv.org/abs/2402.03646)

    "Lens"是一个基于T5架构的基础网络流量模型，通过学习大规模无标签数据的预训练表示，能够在流量理解和生成任务中取得精确的预测和生成。

    

    网络流量是指通过互联网或连接计算机的任何系统发送和接收的信息量。分析和理解网络流量对于提高网络安全和管理至关重要。然而，由于数据包的特殊特性，如异构标头和缺乏语义的加密负载，网络流量的分析带来了巨大的挑战。为了捕捉流量的潜在语义，一些研究采用了基于Transformer编码器或解码器的预训练技术，从大规模的流量数据中学习表示。然而，这些方法通常只在流量理解（分类）或流量生成任务中表现出色。为了解决这个问题，我们开发了Lens，这是一个基础的网络流量模型，利用T5架构从大规模的无标签数据中学习预训练表示。借助编码器-解码器框架的优势，该模型能够捕捉全局和局部特征，实现精确的流量预测和生成。

    Network traffic refers to the amount of information being sent and received over the internet or any system that connects computers. Analyzing and understanding network traffic is vital for improving network security and management. However, the analysis of network traffic poses great challenges due to the unique characteristics of data packets, such as heterogeneous headers and encrypted payload lacking semantics. To capture the latent semantics of traffic, a few studies have adopted pre-training techniques based on the Transformer encoder or decoder to learn the representations from large-scale traffic data. However, these methods typically excel only in traffic understanding (classification) or traffic generation tasks. To address this issue, we develop Lens, a foundational network traffic model that leverages the T5 architecture to learn the pre-trained representations from large-scale unlabeled data. Harnessing the strength of the encoder-decoder framework, which captures the glob
    
[^13]: 跨领域少样本目标检测通过增强的开集目标检测器

    Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object Detector

    [https://arxiv.org/abs/2402.03094](https://arxiv.org/abs/2402.03094)

    本文提出了一种跨领域少样本目标检测器，通过增强的开集目标检测方法来解决跨领域数据差异带来的性能下降问题。

    

    本文解决了跨领域少样本目标检测（CD-FSOD）的挑战，旨在开发一个准确的目标检测器，用最少的标记样本来检测新领域的目标。虽然基于转换器的开集检测器（例如DE-ViT）在开放词汇目标检测和传统的少样本目标检测方面表现出色，能够检测到训练过程中没有见过的类别，我们自然会提出两个关键问题：1）这种开集检测方法能否容易地推广到CD-FSOD？2）如果不能，如何在面对显著的领域差异时增强开集方法的结果？为了回答第一个问题，我们引入了几个衡量领域差异的指标，并建立了一个具有多样领域度量值的新的CD-FSOD基准。在这个基准上评估了一些最先进的开集目标检测方法，在域外数据集中观察到明显的性能下降。这表明采用这些方法在CD-FSOD上失败了。

    This paper addresses the challenge of cross-domain few-shot object detection (CD-FSOD), aiming to develop an accurate object detector for novel domains with minimal labeled examples. While transformer-based open-set detectors e.g., DE-ViT~\cite{zhang2023detect} have excelled in both open-vocabulary object detection and traditional few-shot object detection, detecting categories beyond those seen during training, we thus naturally raise two key questions: 1) can such open-set detection methods easily generalize to CD-FSOD? 2) If no, how to enhance the results of open-set methods when faced with significant domain gaps? To address the first question, we introduce several metrics to quantify domain variances and establish a new CD-FSOD benchmark with diverse domain metric values. Some State-Of-The-Art (SOTA) open-set object detection methods are evaluated on this benchmark, with evident performance degradation observed across out-of-domain datasets. This indicates the failure of adopting 
    
[^14]: 基于Rademacher复杂度的深度学习一般化界限研究

    On Rademacher Complexity-based Generalization Bounds for Deep Learning

    [https://arxiv.org/abs/2208.04284](https://arxiv.org/abs/2208.04284)

    该论文研究了基于Rademacher复杂度的方法在对卷积神经网络进行少类别图像分类时生成非空泛化界限。其中的关键技术贡献是发展了针对函数空间和具有一般Lipschitz激活函数的CNNs的新的Talagrand压缩引理。

    

    我们展示了基于Rademacher复杂度的方法可以生成对卷积神经网络（CNNs）进行分类少量类别图像非空泛化界限。新的Talagrand压缩引理的发展对于高维映射函数空间和具有一般Lipschitz激活函数的CNNs是一个关键技术贡献。我们的结果表明，Rademacher复杂度不依赖于CNNs的网络长度，特别是对于诸如ReLU，Leaky ReLU，Parametric Rectifier Linear Unit，Sigmoid和Tanh等特定类型的激活函数。

    We show that the Rademacher complexity-based approach can generate non-vacuous generalisation bounds on Convolutional Neural Networks (CNNs) for classifying a small number of classes of images. The development of new Talagrand's contraction lemmas for high-dimensional mappings between function spaces and CNNs for general Lipschitz activation functions is a key technical contribution. Our results show that the Rademacher complexity does not depend on the network length for CNNs with some special types of activation functions such as ReLU, Leaky ReLU, Parametric Rectifier Linear Unit, Sigmoid, and Tanh.
    
[^15]: 连续时空模型的基于梯度和无导数优化方法的算子学习

    Operator Learning for Continuous Spatial-Temporal Model with Gradient-Based and Derivative-Free Optimization Methods. (arXiv:2311.11798v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.11798](http://arxiv.org/abs/2311.11798)

    本论文提出了一种基于梯度和无导数优化方法的连续时空模型的算子学习框架，具有空间和时间上的分辨率不变性，并能够在短期时间序列和长期统计数据上高效训练。

    

    在许多工程应用中，常常使用偏微分方程对复杂动态系统进行时空建模。在这项工作中，我们借鉴了算子学习的最新进展，并提出了一种连续的数据驱动建模框架。所提出模型的一个关键特点是对于空间和时间离散化的分辨率不变性，而不需要大量的不同时间分辨率下的训练数据。为了改善校准模型的长期性能，我们进一步提出了一种混合优化方案，利用了基于梯度和无导数的优化方法，能够在短期时间序列和长期统计数据上高效训练。我们通过三个数值实例，包括粘性Burgers方程、Navier-Stokes方程和Kuramoto-Sivashinsky方程，来研究空间连续学习框架的性能。

    Partial differential equations are often used in the spatial-temporal modeling of complex dynamical systems in many engineering applications. In this work, we build on the recent progress of operator learning and present a data-driven modeling framework that is continuous in both space and time. A key feature of the proposed model is the resolution-invariance with respect to both spatial and temporal discretizations, without demanding abundant training data in different temporal resolutions. To improve the long-term performance of the calibrated model, we further propose a hybrid optimization scheme that leverages both gradient-based and derivative-free optimization methods and efficiently trains on both short-term time series and long-term statistics. We investigate the performance of the spatial-temporal continuous learning framework with three numerical examples, including the viscous Burgers' equation, the Navier-Stokes equations, and the Kuramoto-Sivashinsky equation. The results 
    
[^16]: 深度强化学习用于自主网络安全行动的综述

    Deep Reinforcement Learning for Autonomous Cyber Operations: A Survey. (arXiv:2310.07745v1 [cs.LG])

    [http://arxiv.org/abs/2310.07745](http://arxiv.org/abs/2310.07745)

    深度强化学习被应用于自主网络安全行动有很大潜力，但在实际应用中还面临许多挑战。

    

    近年来网络攻击数量的快速增加引发了对针对恶意行为的网络防御方法的需求。深度强化学习（DRL）已经成为缓解这些攻击的一种有希望的方法。然而，尽管DRL在网络防御方面显示出了很大的潜力，但在将DRL应用于大规模自主网络安全行动（ACO）之前，还需要克服许多挑战。需要为与学习者面对非常高维度状态空间、大规模多离散操作空间和对抗学习相遇的环境提供有原则的方法。最近的研究在解决这些问题方面取得了成功。针对实时策略游戏也进行了印象深刻的工程努力。然而，将DRL应用于完整的ACO问题仍然是一个未解决的挑战。在这里，我们对相关的DRL文献进行了调查，并构想了一个理想化的ACO-DRL代理。我们提供了：i.) 领域特性的总结

    The rapid increase in the number of cyber-attacks in recent years raises the need for principled methods for defending networks against malicious actors. Deep reinforcement learning (DRL) has emerged as a promising approach for mitigating these attacks. However, while DRL has shown much potential for cyber-defence, numerous challenges must be overcome before DRL can be applied to autonomous cyber-operations (ACO) at scale. Principled methods are required for environments that confront learners with very high-dimensional state spaces, large multi-discrete action spaces, and adversarial learning. Recent works have reported success in solving these problems individually. There have also been impressive engineering efforts towards solving all three for real-time strategy games. However, applying DRL to the full ACO problem remains an open challenge. Here, we survey the relevant DRL literature and conceptualize an idealised ACO-DRL agent. We provide: i.) A summary of the domain properties t
    
[^17]: 关系卷积网络：学习层次关系表示的框架

    Relational Convolutional Networks: A framework for learning representations of hierarchical relations. (arXiv:2310.03240v1 [cs.LG])

    [http://arxiv.org/abs/2310.03240](http://arxiv.org/abs/2310.03240)

    关系卷积网络是一个学习显式层次关系表示的框架，通过使用多维内积关系模块和关系卷积层，以及基于图元滤波器的群组比较，能够表达更高阶、层次的关系。

    

    深度学习中一个成熟的研究领域是开发能够学习显式关系特征表示的架构。本文着重于学习层次关系表示的问题，提出了一个名为“关系卷积网络”的架构框架。给定一系列对象，一个“多维内积关系”模块生成一个描述所有成对关系的关系张量。然后，一个“关系卷积”层将关系张量转化为一个新对象序列，每个对象描述前一层某群对象内的关系。类似于卷积神经网络中的滤波器，图元滤波器代表要与关系张量在每个分组中进行比较的关系模板。通过重复这个过程，得到更高阶、层次的关系表示。我们介绍了架构的动机和细节，以及一系列实验来证明…

    A maturing area of research in deep learning is the development of architectures that can learn explicit representations of relational features. In this paper, we focus on the problem of learning representations of hierarchical relations, proposing an architectural framework we call "relational convolutional networks". Given a sequence of objects, a "multi-dimensional inner product relation" module produces a relation tensor describing all pairwise relations. A "relational convolution" layer then transforms the relation tensor into a sequence of new objects, each describing the relations within some group of objects at the previous layer. Graphlet filters, analogous to filters in convolutional neural networks, represent a template of relations against which the relation tensor is compared at each grouping. Repeating this yields representations of higher-order, hierarchical relations. We present the motivation and details of the architecture, together with a set of experiments to demons
    
[^18]: 图增强优化器用于结构感知推荐嵌入演化

    Graph-enhanced Optimizers for Structure-aware Recommendation Embedding Evolution. (arXiv:2310.03032v1 [cs.IR])

    [http://arxiv.org/abs/2310.03032](http://arxiv.org/abs/2310.03032)

    本文提出了一种新颖的结构感知嵌入演化(SEvo)机制，能够以较低的计算开销将图结构信息注入到嵌入中，从而在现代推荐系统中实现更高效的性能。

    

    嵌入在现代推荐系统中起着关键作用，因为它们是真实世界实体的虚拟表示，并且是后续决策模型的基础。本文提出了一种新颖的嵌入更新机制，称为结构感知嵌入演化(SEvo)，以鼓励相关节点在每一步中以类似的方式演化。与通常作为中间部分的GNN（图神经网络）不同，SEvo能够直接将图结构信息注入到嵌入中，且在训练过程中计算开销可忽略。本文通过理论分析验证了SEvo的收敛性质及其可能的改进版本，以证明设计的有效性。此外，SEvo可以无缝集成到现有的优化器中，以实现最先进性能。特别是，在矩估计校正的SEvo增强AdamW中，证明了一致的改进效果在多种模型和数据集上，为有效推荐了一种新的技术路线。

    Embedding plays a critical role in modern recommender systems because they are virtual representations of real-world entities and the foundation for subsequent decision models. In this paper, we propose a novel embedding update mechanism, Structure-aware Embedding Evolution (SEvo for short), to encourage related nodes to evolve similarly at each step. Unlike GNN (Graph Neural Network) that typically serves as an intermediate part, SEvo is able to directly inject the graph structure information into embedding with negligible computational overhead in training. The convergence properties of SEvo as well as its possible variants are theoretically analyzed to justify the validity of the designs. Moreover, SEvo can be seamlessly integrated into existing optimizers for state-of-the-art performance. In particular, SEvo-enhanced AdamW with moment estimate correction demonstrates consistent improvements across a spectrum of models and datasets, suggesting a novel technical route to effectively 
    
[^19]: 结合存活分析和机器学习利用电子健康记录数据进行肿瘤风险预测

    Combining Survival Analysis and Machine Learning for Mass Cancer Risk Prediction using EHR data. (arXiv:2309.15039v1 [cs.LG])

    [http://arxiv.org/abs/2309.15039](http://arxiv.org/abs/2309.15039)

    该论文介绍了一种利用 EHR 数据进行大规模肿瘤风险预测的新方法，其创新之处在于只需利用历史的医疗服务代码和诊断信息来实现最小化的数据需求，通过将存活分析和机器学习相结合，可以在大规模应用中实现对患者癌症风险的个性化评估。

    

    纯粹的医学肿瘤筛查方法通常费用高昂、耗时长，并且仅适用于大规模应用。先进的人工智能（AI）方法在癌症检测方面发挥了巨大作用，但需要特定或深入的医学数据。这些方面影响了癌症筛查方法的大规模实施。因此，基于已有的电子健康记录（EHR）数据对患者进行大规模个性化癌症风险评估应用AI方法是一种颠覆性的改变。本文提出了一种利用EHR数据进行大规模肿瘤风险预测的新方法。与其他方法相比，我们的方法通过最小的数据贪婪策略脱颖而出，仅需要来自EHR的医疗服务代码和诊断历史。我们将问题形式化为二分类问题。该数据集包含了175441名不记名的患者（其中2861名被诊断为癌症）。作为基准，我们实现了一个基于循环神经网络（RNN）的解决方案。我们提出了一种方法，将存活分析和机器学习相结合，

    Purely medical cancer screening methods are often costly, time-consuming, and weakly applicable on a large scale. Advanced Artificial Intelligence (AI) methods greatly help cancer detection but require specific or deep medical data. These aspects affect the mass implementation of cancer screening methods. For these reasons, it is a disruptive change for healthcare to apply AI methods for mass personalized assessment of the cancer risk among patients based on the existing Electronic Health Records (EHR) volume.  This paper presents a novel method for mass cancer risk prediction using EHR data. Among other methods, our one stands out by the minimum data greedy policy, requiring only a history of medical service codes and diagnoses from EHR. We formulate the problem as a binary classification. This dataset contains 175 441 de-identified patients (2 861 diagnosed with cancer). As a baseline, we implement a solution based on a recurrent neural network (RNN). We propose a method that combine
    
[^20]: 使用信息流形投影进行聚类探索

    Cluster Exploration using Informative Manifold Projections. (arXiv:2309.14857v1 [cs.LG])

    [http://arxiv.org/abs/2309.14857](http://arxiv.org/abs/2309.14857)

    该论文提出了一种新颖的方法来生成信息丰富的嵌入，以揭示高维数据中的聚类结构。通过线性组合对比PCA和峰度投影追踪两个目标，该方法能够排除先验信息相关的结构并实现有意义的数据分离。

    

    降维是可视化探索高维数据和发现其在二维或三维空间中的聚类结构的关键工具之一。现有文献中的大部分降维方法并未考虑实践者可能对所考虑数据集的任何先验知识。我们提出了一种新颖的方法来生成信息丰富的嵌入，不仅排除与先验知识相关的结构，而且旨在揭示任何剩余的潜在结构。为了实现这一目标，我们采用了两个目标的线性组合：首先是对比PCA，能够消除与先验信息相关的结构，其次是峰度投影追踪，可以确保在得到的嵌入中有意义的数据分离。我们将这个任务定义为流形优化问题，并在考虑三种不同类型的先验知识的各种数据集上进行了实证验证。

    Dimensionality reduction (DR) is one of the key tools for the visual exploration of high-dimensional data and uncovering its cluster structure in two- or three-dimensional spaces. The vast majority of DR methods in the literature do not take into account any prior knowledge a practitioner may have regarding the dataset under consideration. We propose a novel method to generate informative embeddings which not only factor out the structure associated with different kinds of prior knowledge but also aim to reveal any remaining underlying structure. To achieve this, we employ a linear combination of two objectives: firstly, contrastive PCA that discounts the structure associated with the prior information, and secondly, kurtosis projection pursuit which ensures meaningful data separation in the obtained embeddings. We formulate this task as a manifold optimization problem and validate it empirically across a variety of datasets considering three distinct types of prior knowledge. Lastly, 
    
[^21]: 针对带有污染数据的多变量时间序列异常检测：对生理信号的应用

    Multivariate Time-Series Anomaly Detection with Contaminated Data: Application to Physiological Signals. (arXiv:2308.12563v1 [cs.LG])

    [http://arxiv.org/abs/2308.12563](http://arxiv.org/abs/2308.12563)

    这个论文介绍了一种针对带有污染数据的多变量时间序列异常检测的新方法，通过去污和变量依赖建模实现了无监督的异常检测，对于实际场景中的异常检测具有重要意义。

    

    主流无监督异常检测算法通常在学术数据集中表现出色，但由于受到控制实验条件下的清洁训练数据的限制，它们在实际场景下的性能受到了限制。然而，在实际异常检测中，训练数据包含噪声的挑战经常被忽视。本研究在感知时间序列异常检测（TSAD）中深入研究了标签级噪声的领域。本文提出了一种新颖实用的端到端无监督TSAD方法，该方法处理训练数据中包含异常的情况下。该方法称为TSAD-C，其在训练阶段不需要访问异常标签。TSAD-C包括三个模块：一个去污器用于纠正训练数据中存在的异常（也称为噪声），一个变量依赖建模模块用于捕捉去污后数据中的长期内部和跨变量依赖关系，可以视为替代性的异常性度量方法。

    Mainstream unsupervised anomaly detection algorithms often excel in academic datasets, yet their real-world performance is restricted due to the controlled experimental conditions involving clean training data. Addressing the challenge of training with noise, a prevalent issue in practical anomaly detection, is frequently overlooked. In a pioneering endeavor, this study delves into the realm of label-level noise within sensory time-series anomaly detection (TSAD). This paper presents a novel and practical end-to-end unsupervised TSAD when the training data are contaminated with anomalies. The introduced approach, called TSAD-C, is devoid of access to abnormality labels during the training phase. TSAD-C encompasses three modules: a Decontaminator to rectify the abnormalities (aka noise) present in the training data, a Variable Dependency Modeling module to capture both long-term intra- and inter-variable dependencies within the decontaminated data that can be considered as a surrogate o
    
[^22]: 一个差分隐私加权经验风险最小化算法及其在结果加权学习中的应用

    A Differentially Private Weighted Empirical Risk Minimization Procedure and its Application to Outcome Weighted Learning. (arXiv:2307.13127v1 [stat.ML])

    [http://arxiv.org/abs/2307.13127](http://arxiv.org/abs/2307.13127)

    本文提出了一种差分隐私加权经验风险最小化算法，可以在使用敏感数据的情况下保护隐私。这是第一个在权重ERM中应用差分隐私的算法，并且在一定的条件下提供了严格的DP保证。

    

    在经验风险最小化(ERM)框架中，使用包含个人信息的数据来构建预测模型是常见的做法。尽管这些模型在预测上可以非常准确，但使用敏感数据得到的结果可能容易受到隐私攻击。差分隐私(DP)是一种有吸引力的框架，可以通过提供数学上可证明的隐私损失界限来解决这些数据隐私问题。先前的工作主要集中在将DP应用于无权重的ERM中。我们考虑到了权重ERM(wERM)的重要推广。在wERM中，可以为每个个体的目标函数贡献分配不同的权重。在这个背景下，我们提出了第一个有差分隐私保障的wERM算法，并在一定的正则条件下提供了严格的理论证明。将现有的DP-ERM程序扩展到wERM为结果加权学习铺平了道路。

    It is commonplace to use data containing personal information to build predictive models in the framework of empirical risk minimization (ERM). While these models can be highly accurate in prediction, results obtained from these models with the use of sensitive data may be susceptible to privacy attacks. Differential privacy (DP) is an appealing framework for addressing such data privacy issues by providing mathematically provable bounds on the privacy loss incurred when releasing information from sensitive data. Previous work has primarily concentrated on applying DP to unweighted ERM. We consider an important generalization to weighted ERM (wERM). In wERM, each individual's contribution to the objective function can be assigned varying weights. In this context, we propose the first differentially private wERM algorithm, backed by a rigorous theoretical proof of its DP guarantees under mild regularity conditions. Extending the existing DP-ERM procedures to wERM paves a path to derivin
    
[^23]: MaskedKD：使用遮蔽图像的高效Vision Transformer蒸馏

    MaskedKD: Efficient Distillation of Vision Transformers with Masked Images. (arXiv:2302.10494v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10494](http://arxiv.org/abs/2302.10494)

    MaskedKD提出了一种通过遮蔽图像块来显著降低Vision Transformer (ViT)蒸馏成本的方法，而不影响学生模型的预测准确性。

    

    知识蒸馏对于训练轻量级模型是一种有效的方法，但它会在训练成本中引入大量的计算开销，因为该方法需要在训练样本上获取教师监督。当使用大规模的Vision Transformer（ViTs）等教师模型时，这种附加成本——蒸馏成本——最为明显。我们提出了MaskedKD，这是一种简单但有效的策略，可以显着降低蒸馏ViTs的成本，同时不损失学生模型的预测准确性。具体来说，MaskedKD通过遮蔽一部分输入到教师模型的图像块令教师模型的推理成本减少，因此可以跳过处理这些块所需的计算。所选的遮罩位置旨在防止屏蔽学生模型用于预测的图像的核心特征。该遮罩选择机制基于学生模型的某些注意力分数操作。

    Knowledge distillation is an effective method for training lightweight models, but it introduces a significant amount of computational overhead to the training cost, as the method requires acquiring teacher supervisions on training samples. This additional cost -- called distillation cost -- is most pronounced when we employ large-scale teacher models such as vision transformers (ViTs). We present MaskedKD, a simple yet effective strategy that can significantly reduce the cost of distilling ViTs without sacrificing the prediction accuracy of the student model. Specifically, MaskedKD diminishes the cost of running teacher at inference by masking a fraction of image patch tokens fed to the teacher, and therefore skipping the computations required to process those patches. The mask locations are selected to prevent masking away the core features of an image that the student model uses for prediction. This mask selection mechanism operates based on some attention score of the student model
    
[^24]: 无交叠策略学习：悲观和广义经验Bernstein不等式

    Policy learning "without'' overlap: Pessimism and generalized empirical Bernstein's inequality. (arXiv:2212.09900v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.09900](http://arxiv.org/abs/2212.09900)

    本文提出了一种新的离线策略学习算法，它不需要统一交叠假设，而是利用价值的下限置信区间（LCBs）优化策略，因此能够适应允许行为策略演变和倾向性减弱的情况。

    

    本文研究了离线策略学习，旨在利用先前收集到的观测（来自于固定的或是适应演变的行为策略）来学习给定类别中的最优个性化决策规则。现有的策略学习方法依赖于一个统一交叠假设，即离线数据集中探索所有个性化特征的所有动作的倾向性下界。换句话说，这些方法的性能取决于离线数据集中最坏的倾向性。由于数据收集过程不受控制，在许多情况下，这种假设可能不太现实，特别是当允许行为策略随时间演变并且倾向性减弱时。为此，本文提出了一种新的算法，它优化策略价值的下限置信区间（LCBs）——而不是点估计。LCBs通过量化增强倒数倾向权重的估计不确定性来构建。

    This paper studies offline policy learning, which aims at utilizing observations collected a priori (from either fixed or adaptively evolving behavior policies) to learn the optimal individualized decision rule in a given class. Existing policy learning methods rely on a uniform overlap assumption, i.e., the propensities of exploring all actions for all individual characteristics are lower bounded in the offline dataset. In other words, the performance of these methods depends on the worst-case propensity in the offline dataset. As one has no control over the data collection process, this assumption can be unrealistic in many situations, especially when the behavior policies are allowed to evolve over time with diminishing propensities.  In this paper, we propose a new algorithm that optimizes lower confidence bounds (LCBs) -- instead of point estimates -- of the policy values. The LCBs are constructed by quantifying the estimation uncertainty of the augmented inverse propensity weight
    
[^25]: DICTDIS：基于词典约束的神经机器翻译消歧方法对 NMT 的改进

    DICTDIS: Dictionary Constrained Disambiguation for Improved NMT. (arXiv:2210.06996v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.06996](http://arxiv.org/abs/2210.06996)

    DICTDIS是一种新颖有词典约束的NMT系统，其利用多个字典候选项进行训练，实现了从多义词中消除翻译歧义的目的，提高了翻译质量。

    

    领域特定的神经机器翻译系统（例如教育应用程序）在多语言社会中帮助使信息对一组多样化的用户可访问是具有社会意义的。这种 NMT 系统应该具有词汇约束并从领域特定的词典中汲取。由于单词的多义性，词典中可能会为源单词或短语呈现多个候选翻译。这时，NMT 模型需要选择与语境最相关的候选翻译。先前的工作主要忽略了这个问题，而侧重于单个候选约束设置，其中目标词或短语被单个约束替换。在本文中，我们提出了一种名为DICTDIS的词典约束 NMT 系统，该系统消除了从字典中得出的多个候选翻译的歧义。我们通过将训练数据与多个字典候选项进行增量来实现这一点，从而在训练期间积极鼓励消除歧义。

    Domain-specific neural machine translation (NMT) systems (\eg, in educational applications) are socially significant with the potential to help make information accessible to a diverse set of users in multilingual societies. It is desirable that such NMT systems be lexically constrained and draw from domain-specific dictionaries. Dictionaries could present multiple candidate translations for a source word/phrase due to the polysemous nature of words. The onus is then on the NMT model to choose the contextually most appropriate candidate. Prior work has largely ignored this problem and focused on the single candidate constraint setting wherein the target word or phrase is replaced by a single constraint. In this work we present \dictdis, a lexically constrained NMT system that disambiguates between multiple candidate translations derived from dictionaries. We achieve this by augmenting training data with multiple dictionary candidates to actively encourage disambiguation during training
    

