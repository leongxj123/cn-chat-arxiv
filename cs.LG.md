# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Prompts As Programs: A Structure-Aware Approach to Efficient Compile-Time Prompt Optimization](https://arxiv.org/abs/2404.02319) | 提出了SAMMO框架，用于在编译时优化元提示程序，提高了复杂提示在多种不同LLM上的性能。 |
| [^2] | [A Simple Mixture Policy Parameterization for Improving Sample Efficiency of CVaR Optimization](https://arxiv.org/abs/2403.11062) | 提出了一种简单的混合策略参数化方法，通过整合风险中性策略和可调整策略，提高了CVaR优化的样本效率。 |
| [^3] | [Last Iterate Convergence of Incremental Methods and Applications in Continual Learning](https://arxiv.org/abs/2403.06873) | 针对继续学习应用，本研究首次获得了增量梯度和增量近端方法最后迭代的收敛保证，且其预期复杂度界限几乎与已知最佳平均迭代的界限相匹配。 |
| [^4] | [Generative Probabilistic Forecasting with Applications in Market Operations](https://arxiv.org/abs/2403.05743) | 提出了一种基于Wiener-Kallianpur创新表示的生成式概率预测方法，包括自编码器和新颖的深度学习算法，具有渐近最优性和结构收敛性质，适用于实时市场运营中的高动态和波动时间序列。 |
| [^5] | [Behavior Generation with Latent Actions](https://arxiv.org/abs/2403.03181) | 这项工作介绍了一种名为矢量量化行为转换器（VQ-BeT）的多功能行为生成模型，通过对连续动作进行标记化处理多模态动作预测、条件生成和部分观察。 |
| [^6] | [Explain to Question not to Justify](https://arxiv.org/abs/2402.13914) | XAI领域被划分为蓝色XAI和红色XAI两种解释文化，指出了红色XAI领域的重要性和研究潜力，并提出了未来的研究挑战。 |
| [^7] | [Dynamic planning in hierarchical active inference](https://arxiv.org/abs/2402.11658) | 通过研究在动态规划领域中模拟工具使用的目标，我们深入探讨了主动推断中的动态规划，该领域考虑到生物目标导向行为的两个关键方面 |
| [^8] | [Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2402.11622) | 提出了一种基于逻辑闭环的框架（LogicCheckGPT），利用大型视觉-语言模型本身来检测和减轻对象幻觉。 |
| [^9] | [Active Preference Learning for Large Language Models](https://arxiv.org/abs/2402.08114) | 本论文提出了一种用于大型语言模型的主动偏好学习策略，通过直接偏好优化（DPO）来更好地利用偏好标签。实验结果表明，该方法提高了基于成对偏好数据的微调的学习速度和最终性能。 |
| [^10] | [The Impact of Domain Knowledge and Multi-Modality on Intelligent Molecular Property Prediction: A Systematic Survey](https://arxiv.org/abs/2402.07249) | 本文通过系统调查，发现整合领域知识可以提高分子性质预测的准确性，同时利用多模态数据融合可以产生更精确的结果。 |
| [^11] | [Effort and Size Estimation in Software Projects with Large Language Model-based Intelligent Interfaces](https://arxiv.org/abs/2402.07158) | 本文提出了一种基于大型语言模型的智能界面在软件项目中进行工作量和规模估计的方法，并通过比较传统方法，探讨了如何通过增强基于自然语言的问题规范来实现开发工作量的准确估计。 |
| [^12] | [Latent variable model for high-dimensional point process with structured missingness](https://arxiv.org/abs/2402.05758) | 本文提出了一种针对高维点过程的带有结构缺失的灵活高效的潜变量模型，利用高斯过程捕获时间相关性，并开发了可扩展的变分推理方法进行训练。 |
| [^13] | [Scaling laws for learning with real and surrogate data](https://arxiv.org/abs/2402.04376) | 本研究探讨了将替代数据与真实数据整合以进行训练的方案，发现整合替代数据能够显著降低测试误差，并提出了一个扩展规律来描述混合模型的测试误差，可以用于预测最优加权和收益。 |
| [^14] | [BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation](https://arxiv.org/abs/2402.03216) | BGE M3-嵌入是一种新的多语言、多功能和多粒度的文本嵌入模型，支持超过100种工作语言，并在多语言和跨语言检索任务上取得了最先进的性能。它能够同时执行密集检索、多向量检索和稀疏检索，并能处理不同粒度的输入。其有效训练包括了一种自知识蒸馏方法和优化的批处理策略。 |
| [^15] | [Equivariant Symmetry Breaking Sets](https://arxiv.org/abs/2402.02681) | 这里是中文总结出的一句话要点: 该论文提出了一种全等变的对称破缺框架，通过引入对称破缺集来破坏等变神经网络中的对称性。这种方法通用且适用于任何群的等变性。 |
| [^16] | [ChIRAAG: ChatGPT Informed Rapid and Automated Assertion Generation](https://arxiv.org/abs/2402.00093) | 本研究设计了一个基于大语言模型的流水线，通过自然语言规格生成英语、线性时态逻辑和SVA断言，并成功减少了断言错误率。 |
| [^17] | [Robustness Assessment of a Runway Object Classifier for Safe Aircraft Taxiing](https://arxiv.org/abs/2402.00035) | 本文介绍了对航班滑行安全的跑道物体分类器的鲁棒性评估，使用形式方法评估了该分类器对三种常见图像扰动类型的鲁棒性，并提出了一种利用单调性的方法。 |
| [^18] | [Directions of Curvature as an Explanation for Loss of Plasticity](https://arxiv.org/abs/2312.00246) | 曲率方向的丧失被认为是导致神经网络可塑性丧失的一个重要原因，并且我们通过系统调查和在多个任务中的研究结果支持了这一观点。 |
| [^19] | [Generative AI-Driven Human Digital Twin in IoT-Healthcare: A Comprehensive Survey.](http://arxiv.org/abs/2401.13699) | 该论文调查了在物联网健康护理中利用生成式人工智能驱动的人类数字孪生的应用。人类数字孪生作为多功能、生动的人类数字测试平台，可以模拟结果并指导实际治疗，从而提高物联网健康护理的能力。 |
| [^20] | [Sequential Model for Predicting Patient Adherence in Subcutaneous Immunotherapy for Allergic Rhinitis.](http://arxiv.org/abs/2401.11447) | 本研究利用新颖的机器学习模型，准确预测患者的非依从风险和相关的系统症状评分，为长期过敏性鼻炎亚卡激素皮下免疫治疗的管理提供了一种新的方法。 |
| [^21] | [On Finding Bi-objective Pareto-optimal Fraud Prevention Rule Sets for Fintech Applications.](http://arxiv.org/abs/2311.00964) | 本文研究了在金融科技应用中寻找高质量的双目标 Pareto 最优欺诈预防规则集的问题。通过采用 Pareto 最优性概念和启发式框架 PORS，我们成功提出了一组非支配的规则子集，并通过实证评估证明了其有效性。 |
| [^22] | [Fishnets: Information-Optimal, Scalable Aggregation for Sets and Graphs.](http://arxiv.org/abs/2310.03812) | Fishnets是一种用于学习信息最优的集合和图聚合的方法，在规模上可以优化到任意数量的数据对象，具有鲁棒性，能够饱和贝叶斯信息内容，并可用于GNNs中的消息传递。 |
| [^23] | [Generative Autoencoding of Dropout Patterns.](http://arxiv.org/abs/2310.01712) | 本论文提出了一种称为解读自编码器的生成模型，通过为训练数据集中的每个数据点分配独特的随机丢弃模式来进行训练，只依靠重构误差来提供更稳定的训练性能，并在CIFAR-10数据集上展示了与DCGAN相媲美的采样质量。 |
| [^24] | [3D-Mol: A Novel Contrastive Learning Framework for Molecular Property Prediction with 3D Information.](http://arxiv.org/abs/2309.17366) | 3D-Mol是一种新颖的基于3D结构的分子建模方法，通过对比学习提高了分子性质预测准确性，并在多个基准数据集上超过了最先进的模型。 |
| [^25] | [A Hierarchical Neural Framework for Classification and its Explanation in Large Unstructured Legal Documents.](http://arxiv.org/abs/2309.10563) | 本论文提出了一个名为MESc的分层神经框架，用于分类和解释大型非结构化法律文件。通过将文件分成多个部分并使用大型语言模型的嵌入和无监督聚类，该框架能够实现从长文档中预测判决并提取解释。 |
| [^26] | [Optimal Rate of Kernel Regression in Large Dimensions.](http://arxiv.org/abs/2309.04268) | 该论文提出了一种针对大维度数据的核回归的最优比率，通过使用Mendelson复杂性和度量熵来刻画其上界和最小化下界。此外，研究还发现最优比率随着维度与样本大小关系的变化呈现出多次下降的行为。 |
| [^27] | [Deep Maxout Network-based Feature Fusion and Political Tangent Search Optimizer enabled Transfer Learning for Thalassemia Detection.](http://arxiv.org/abs/2308.02029) | 本文提出了基于Deep Maxout网络特征融合和Political Tangent Search优化器的转移学习方法，应用于地中海贫血检测。该方法对输入数据进行归一化处理，并利用Deep Maxout网络的特征融合和过采样方法进行数据增强，最终通过转移学习进行地中海贫血检测。 |
| [^28] | [SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models.](http://arxiv.org/abs/2307.10635) | 这篇论文介绍了一个名为SciBench的基准套件，旨在对大型语言模型的大学水平科学问题解决能力进行评估。研究结果显示，当前的语言模型在提供复杂科学问题解决能力方面还有不足之处。 |
| [^29] | [MALIBO: Meta-learning for Likelihood-free Bayesian Optimization.](http://arxiv.org/abs/2307.03565) | MALIBO是一种元学习贝叶斯优化方法，通过直接学习跨任务的查询效用，并引入辅助模型以实现对新任务的稳健适应，克服了现有方法的可伸缩性和不确定性的限制。 |
| [^30] | [Networked Communication for Decentralised Agents in Mean-Field Games.](http://arxiv.org/abs/2306.02766) | 本研究在均场博弈中引入网络通信，提出了一种提高分布式智能体学习效率的方案，并进行了实际实验验证。 |
| [^31] | [A New Era in Software Security: Towards Self-Healing Software via Large Language Models and Formal Verification.](http://arxiv.org/abs/2305.14752) | 本文介绍了一个结合大语言模型和形式验证的方法来自动验证和修复软件漏洞，并通过ESBMC-AI做出了概念验证。 |
| [^32] | [Property-Guided Generative Modelling for Robust Model-Based Design with Imbalanced Data.](http://arxiv.org/abs/2305.13650) | 本文提出了一种属性引导的变分自编码器（PGVAE），通过属性值明确结构化潜在空间，使得MBO可以在不平衡数据上稳健地寻找具有改进属性的序列。 |
| [^33] | [Importance Weighted Expectation-Maximization for Protein Sequence Design.](http://arxiv.org/abs/2305.00386) | 本文提出了一种名为IsEM-Pro的方法，用于根据给定适应性标准生成蛋白质序列。在推理期间，从其潜在空间采样可以增加多样性，指导了探索高适应性区域。实验表明，相比先前最佳方法，IsEM-Pro的平均适应性得分至少高出55％，并生成了更多样化和新颖的蛋白质序列。 |
| [^34] | [The G-invariant graph Laplacian.](http://arxiv.org/abs/2303.17001) | 本文提出了 G不变图拉普拉斯算子 用于处理数据集不仅在流形上，而且在一个连续群的作用下也是封闭的情形，相较于标准图拉普拉斯算子收敛速度更快。 |

# 详细

[^1]: Prompt作为程序：一种结构感知的高效编译时Prompt优化方法

    Prompts As Programs: A Structure-Aware Approach to Efficient Compile-Time Prompt Optimization

    [https://arxiv.org/abs/2404.02319](https://arxiv.org/abs/2404.02319)

    提出了SAMMO框架，用于在编译时优化元提示程序，提高了复杂提示在多种不同LLM上的性能。

    

    大型语言模型(LLMs)现在能处理更长更复杂的输入，这促进了更复杂提示的使用。然而，提示通常需要一些调整以提高部署性能。最近的工作提出了自动提示优化方法，但随着提示复杂度和LLM强度的增加，许多提示优化技术已不再足够，需要一种新的方法来优化元提示程序。为了解决这个问题，我们引入了SAMMO，一个用于元提示程序的{\em 编译时}优化的框架，它将提示表示为结构化对象，允许在优化过程中搜索一组丰富的转换。我们展示SAMMO推广了先前的方法，在指令调整、RAG管线调整和提示压缩方面提高了复杂提示在多种不同LLM上的性能。我们开放所有代码供大家使用。

    arXiv:2404.02319v1 Announce Type: cross  Abstract: Large language models (LLMs) can now handle longer and more complex inputs, which facilitate the use of more elaborate prompts. However, prompts often require some tuning to improve performance for deployment. Recent work has proposed automatic prompt optimization methods, but as prompt complexity and LLM strength increase, many prompt optimization techniques are no longer sufficient and a new approach is needed to optimize {\em meta prompt programs}. To address this, we introduce SAMMO, a framework for {\em compile-time} optimizations of metaprompt programs, which represent prompts as structured objects that allows for a rich set of transformations that can be searched over during optimization. We show that SAMMO generalizes previous methods and improves the performance of complex prompts on (1) instruction tuning, (2) RAG pipeline tuning, and (3) prompt compression, across several different LLMs.   We make all code available open-sou
    
[^2]: 用于提高CVaR优化样本效率的简单混合策略参数化

    A Simple Mixture Policy Parameterization for Improving Sample Efficiency of CVaR Optimization

    [https://arxiv.org/abs/2403.11062](https://arxiv.org/abs/2403.11062)

    提出了一种简单的混合策略参数化方法，通过整合风险中性策略和可调整策略，提高了CVaR优化的样本效率。

    

    利用策略梯度(PG)优化条件值风险(CVaR)的强化学习算法在提高样本效率方面面临着重大挑战，限制了它们的实际应用。为了解决这些挑战，我们提出了一种简单的混合策略参数化方法。这种方法将风险中性策略与可调整策略整合为一个风险厌恶策略。通过采用这种策略，所有收集到的轨迹都可以用于策略更新，并且通过风险中性组件刺激更高的回报，从而提升尾部并防止扁平化。我们的实证研究表明，这种混合参数化是非常有效的。

    arXiv:2403.11062v1 Announce Type: new  Abstract: Reinforcement learning algorithms utilizing policy gradients (PG) to optimize Conditional Value at Risk (CVaR) face significant challenges with sample inefficiency, hindering their practical applications. This inefficiency stems from two main facts: a focus on tail-end performance that overlooks many sampled trajectories, and the potential of gradient vanishing when the lower tail of the return distribution is overly flat. To address these challenges, we propose a simple mixture policy parameterization. This method integrates a risk-neutral policy with an adjustable policy to form a risk-averse policy. By employing this strategy, all collected trajectories can be utilized for policy updating, and the issue of vanishing gradients is counteracted by stimulating higher returns through the risk-neutral component, thus lifting the tail and preventing flatness. Our empirical study reveals that this mixture parameterization is uniquely effectiv
    
[^3]: 增量方法的最后迭代收敛性及在继续学习中的应用

    Last Iterate Convergence of Incremental Methods and Applications in Continual Learning

    [https://arxiv.org/abs/2403.06873](https://arxiv.org/abs/2403.06873)

    针对继续学习应用，本研究首次获得了增量梯度和增量近端方法最后迭代的收敛保证，且其预期复杂度界限几乎与已知最佳平均迭代的界限相匹配。

    

    增量梯度方法和增量近端方法是一类基本的优化算法，用于解决广泛研究的有限和问题。然而，就收敛性保证而言，非渐进（一阶或近端）的预期复杂性界限最近才得到，并且几乎仅适用于平均迭代。受继续学习应用的启发，我们获得了对一般凸平滑（两者）和凸Lipschitz（对于近端变种）设置中增量梯度和增量近端方法的最后迭代的首个收敛保证。我们对最后迭代的预期复杂性界限几乎与最佳已知的平均迭代的预期复杂性界限相匹配（即匹配至平方根对数或对数因子），对两类方法均适用。我们进一步将我们的结果推广到加权平均的情况。

    arXiv:2403.06873v1 Announce Type: cross  Abstract: Incremental gradient methods and incremental proximal methods are a fundamental class of optimization algorithms used for solving finite sum problems, broadly studied in the literature. Yet, when it comes to their convergence guarantees, nonasymptotic (first-order or proximal) oracle complexity bounds have been obtained fairly recently, almost exclusively applying to the average iterate. Motivated by applications in continual learning, we obtain the first convergence guarantees for the last iterate of both incremental gradient and incremental proximal methods, in general convex smooth (for both) and convex Lipschitz (for the proximal variants) settings. Our oracle complexity bounds for the last iterate nearly match (i.e., match up to a square-root-log or a log factor) the best known oracle complexity bounds for the average iterate, for both classes of methods. We further obtain generalizations of our results to weighted averaging of th
    
[^4]: 具有市场运营应用的生成式概率预测

    Generative Probabilistic Forecasting with Applications in Market Operations

    [https://arxiv.org/abs/2403.05743](https://arxiv.org/abs/2403.05743)

    提出了一种基于Wiener-Kallianpur创新表示的生成式概率预测方法，包括自编码器和新颖的深度学习算法，具有渐近最优性和结构收敛性质，适用于实时市场运营中的高动态和波动时间序列。

    

    本文提出了一种新颖的生成式概率预测方法，该方法源自于非参数时间序列的Wiener-Kallianpur创新表示。在生成人工智能的范式下，所提出的预测架构包括一个自编码器，将非参数多变量随机过程转化为规范的创新序列，从中根据过去样本生成未来时间序列样本，条件是它们的概率分布取决于过去样本。提出了一种新的深度学习算法，将潜在过程限制为具有匹配自编码器输入-输出条件概率分布的独立同分布序列。建立了所提出的生成式预测方法的渐近最优性和结构收敛性质。该方法在实时市场运营中涉及高度动态和波动时间序列的三个应用方面。

    arXiv:2403.05743v1 Announce Type: cross  Abstract: This paper presents a novel generative probabilistic forecasting approach derived from the Wiener-Kallianpur innovation representation of nonparametric time series. Under the paradigm of generative artificial intelligence, the proposed forecasting architecture includes an autoencoder that transforms nonparametric multivariate random processes into canonical innovation sequences, from which future time series samples are generated according to their probability distributions conditioned on past samples. A novel deep-learning algorithm is proposed that constrains the latent process to be an independent and identically distributed sequence with matching autoencoder input-output conditional probability distributions. Asymptotic optimality and structural convergence properties of the proposed generative forecasting approach are established. Three applications involving highly dynamic and volatile time series in real-time market operations a
    
[^5]: 具有潜在动作的行为生成

    Behavior Generation with Latent Actions

    [https://arxiv.org/abs/2403.03181](https://arxiv.org/abs/2403.03181)

    这项工作介绍了一种名为矢量量化行为转换器（VQ-BeT）的多功能行为生成模型，通过对连续动作进行标记化处理多模态动作预测、条件生成和部分观察。

    

    从带标签的数据集中生成复杂行为的生成建模一直是决策制定中长期存在的问题。与语言或图像生成不同，决策制定需要建模动作 - 连续值向量，其在分布上是多模态的，可能来自未经筛选的来源，在顺序预测中生成误差可能会相互累积。最近一类称为行为转换器（BeT）的模型通过使用k-means聚类对动作进行离散化以捕捉不同模式来解决这个问题。然而，k-means在处理高维动作空间或长序列时存在困难，并且缺乏梯度信息，因此BeT在建模长距离动作时存在困难。在这项工作中，我们提出了一种名为矢量量化行为转换器（VQ-BeT）的多功能行为生成模型，用于处理多模态动作预测、条件生成和部分观察。VQ-BeT通过对连续动作进行标记化来增强BeT

    arXiv:2403.03181v1 Announce Type: cross  Abstract: Generative modeling of complex behaviors from labeled datasets has been a longstanding problem in decision making. Unlike language or image generation, decision making requires modeling actions - continuous-valued vectors that are multimodal in their distribution, potentially drawn from uncurated sources, where generation errors can compound in sequential prediction. A recent class of models called Behavior Transformers (BeT) addresses this by discretizing actions using k-means clustering to capture different modes. However, k-means struggles to scale for high-dimensional action spaces or long sequences, and lacks gradient information, and thus BeT suffers in modeling long-range actions. In this work, we present Vector-Quantized Behavior Transformer (VQ-BeT), a versatile model for behavior generation that handles multimodal action prediction, conditional generation, and partial observations. VQ-BeT augments BeT by tokenizing continuous
    
[^6]: 不是为了辩解而是为了解释

    Explain to Question not to Justify

    [https://arxiv.org/abs/2402.13914](https://arxiv.org/abs/2402.13914)

    XAI领域被划分为蓝色XAI和红色XAI两种解释文化，指出了红色XAI领域的重要性和研究潜力，并提出了未来的研究挑战。

    

    可解释人工智能（XAI）是一个年轻但非常有前途的研究领域。不幸的是，该领域目前的进展受到了不同和不兼容目标的限制。在本文中，我们将XAI领域内纠缠在一起的各种线索分为两种互补的文化，即人类/价值取向解释（蓝色XAI）和模型/验证取向解释（红色XAI）。我们还认为，红色XAI领域目前未被充分探索，隐藏着巨大的机遇和重要研究的潜力，以确保AI系统的安全。我们通过提出这一领域的有前途的挑战来总结本文。

    arXiv:2402.13914v1 Announce Type: new  Abstract: Explainable Artificial Intelligence (XAI) is a young but very promising field of research. Unfortunately, the progress in this field is currently slowed down by divergent and incompatible goals. In this paper, we separate various threads tangled within the area of XAI into two complementary cultures of human/value-oriented explanations (BLUE XAI) and model/validation-oriented explanations (RED XAI). We also argue that the area of RED XAI is currently under-explored and hides great opportunities and potential for important research necessary to ensure the safety of AI systems. We conclude this paper by presenting promising challenges in this area.
    
[^7]: 分层主动推断中的动态规划

    Dynamic planning in hierarchical active inference

    [https://arxiv.org/abs/2402.11658](https://arxiv.org/abs/2402.11658)

    通过研究在动态规划领域中模拟工具使用的目标，我们深入探讨了主动推断中的动态规划，该领域考虑到生物目标导向行为的两个关键方面

    

    通过动态规划，我们指的是人类大脑推断和施加与认知决策相关的运动轨迹的能力。最近的一个范式，主动推断，为生物有机体适应带来了基本见解，不断努力最小化预测误差以将自己限制在与生命兼容的状态。在过去的几年里，许多研究表明人类和动物行为可以解释为主动推断过程，无论是作为离散决策还是连续运动控制，都激发了机器人技术和人工智能中的创新解决方案。然而，文献缺乏对如何有效地在变化环境中规划行动的全面展望。我们设定了对工具使用进行建模的目标，深入研究了主动推断中的动态规划主题，牢记两个生物目标导向行为的关键方面：理解……

    arXiv:2402.11658v1 Announce Type: new  Abstract: By dynamic planning, we refer to the ability of the human brain to infer and impose motor trajectories related to cognitive decisions. A recent paradigm, active inference, brings fundamental insights into the adaptation of biological organisms, constantly striving to minimize prediction errors to restrict themselves to life-compatible states. Over the past years, many studies have shown how human and animal behavior could be explained in terms of an active inferential process -- either as discrete decision-making or continuous motor control -- inspiring innovative solutions in robotics and artificial intelligence. Still, the literature lacks a comprehensive outlook on how to effectively plan actions in changing environments. Setting ourselves the goal of modeling tool use, we delve into the topic of dynamic planning in active inference, keeping in mind two crucial aspects of biological goal-directed behavior: the capacity to understand a
    
[^8]: 逻辑闭环：揭示大型视觉-语言模型中的对象幻觉

    Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models

    [https://arxiv.org/abs/2402.11622](https://arxiv.org/abs/2402.11622)

    提出了一种基于逻辑闭环的框架（LogicCheckGPT），利用大型视觉-语言模型本身来检测和减轻对象幻觉。

    

    对象幻觉一直是阻碍大型视觉-语言模型（LVLMs）更广泛应用的软肋。对象幻觉是指LVLMs在图像中声称不存在的对象的现象。为了减轻对象幻觉，已经提出了指导调整和基于外部模型的检测方法，这两种方法要么需要大规模的计算资源，要么依赖于外部模型的检测结果。然而，仍然存在一个未深入探讨的领域，即利用LVLM本身来减轻对象幻觉。在这项工作中，我们采用了这样的直觉，即LVLM倾向于对存在的对象做出逻辑一致的反应，但对幻觉对象做出不一致的反应。因此，我们提出了基于逻辑闭环的对象幻觉检测和减轻框架，即LogicCheckGPT。具体来说，我们设计了逻辑一致性探测来提出具有逻辑性的问题。

    arXiv:2402.11622v1 Announce Type: cross  Abstract: Object hallucination has been an Achilles' heel which hinders the broader applications of large vision-language models (LVLMs). Object hallucination refers to the phenomenon that the LVLMs claim non-existent objects in the image. To mitigate the object hallucinations, instruction tuning and external model-based detection methods have been proposed, which either require large-scare computational resources or depend on the detection result of external models. However, there remains an under-explored field to utilize the LVLM itself to alleviate object hallucinations. In this work, we adopt the intuition that the LVLM tends to respond logically consistently for existent objects but inconsistently for hallucinated objects. Therefore, we propose a Logical Closed Loop-based framework for Object Hallucination Detection and Mitigation, namely LogicCheckGPT. In specific, we devise logical consistency probing to raise questions with logical corr
    
[^9]: 大型语言模型的主动偏好学习

    Active Preference Learning for Large Language Models

    [https://arxiv.org/abs/2402.08114](https://arxiv.org/abs/2402.08114)

    本论文提出了一种用于大型语言模型的主动偏好学习策略，通过直接偏好优化（DPO）来更好地利用偏好标签。实验结果表明，该方法提高了基于成对偏好数据的微调的学习速度和最终性能。

    

    随着大型语言模型（LLM）的能力越来越强，与人类意图对齐的微调技术变得越来越重要。对于对齐这些模型来说，最关键的考虑是如何最有效地利用人力资源，或者在LLM本身被用作oracle的情况下如何最有效地利用模型资源。从人类或AI偏好中进行强化学习（RLHF / RLAIF）是这种技术最突出的例子，但它往往复杂且不稳定。最近，直接偏好优化（DPO）被提出作为一个更简单和更稳定的替代方法。在这项工作中，我们开发了一种DPO的主动学习策略，以更好地利用偏好标签。我们提出了一个基于语言模型的预测熵和DPO优化的隐式偏好模型的确定性度量的实用采集函数，展示了我们的方法如何提高基于成对偏好数据的微调的学习速度和最终性能。

    As large language models (LLMs) become more capable, fine-tuning techniques for aligning with human intent are increasingly important. A key consideration for aligning these models is how to most effectively use human resources, or model resources in the case where LLMs themselves are used as oracles. Reinforcement learning from Human or AI preferences (RLHF/RLAIF) is the most prominent example of such a technique, but is complex and often unstable. Direct Preference Optimization (DPO) has recently been proposed as a simpler and more stable alternative. In this work, we develop an active learning strategy for DPO to make better use of preference labels. We propose a practical acquisition function for prompt/completion pairs based on the predictive entropy of the language model and a measure of certainty of the implicit preference model optimized by DPO. We demonstrate how our approach improves both the rate of learning and final performance of fine-tuning on pairwise preference data.
    
[^10]: 领域知识和多模态对智能分子性质预测的影响：一项系统调查

    The Impact of Domain Knowledge and Multi-Modality on Intelligent Molecular Property Prediction: A Systematic Survey

    [https://arxiv.org/abs/2402.07249](https://arxiv.org/abs/2402.07249)

    本文通过系统调查，发现整合领域知识可以提高分子性质预测的准确性，同时利用多模态数据融合可以产生更精确的结果。

    

    准确预测分子性质对于药物开发尤其是虚拟筛选和化合物优化的进展至关重要。近年来引入了许多基于深度学习的方法，在增强分子性质预测（MPP）方面显示出显著潜力，特别是提高了准确性和对分子结构的洞察力。然而，有两个关键问题：领域知识的整合是否增强了分子性质预测的准确性，使用多模态数据融合是否比单一数据来源方法产生更精确的结果？为了探究这些问题，我们全面回顾和定量分析了基于各种基准的最新深度学习方法。我们发现，整合分子信息将分别提高MPP回归和分类任务的准确性，分别高达3.98％和1.72％。我们还发现，使用三维信息与一维和二维信息相结合会产生更好的结果。

    The precise prediction of molecular properties is essential for advancements in drug development, particularly in virtual screening and compound optimization. The recent introduction of numerous deep learning-based methods has shown remarkable potential in enhancing molecular property prediction (MPP), especially improving accuracy and insights into molecular structures. Yet, two critical questions arise: does the integration of domain knowledge augment the accuracy of molecular property prediction and does employing multi-modal data fusion yield more precise results than unique data source methods? To explore these matters, we comprehensively review and quantitatively analyze recent deep learning methods based on various benchmarks. We discover that integrating molecular information will improve both MPP regression and classification tasks by upto 3.98% and 1.72%, respectively. We also discover that the utilizing 3-dimensional information with 1-dimensional and 2-dimensional informati
    
[^11]: 基于大型语言模型的智能界面在软件项目中的工作量和规模估计

    Effort and Size Estimation in Software Projects with Large Language Model-based Intelligent Interfaces

    [https://arxiv.org/abs/2402.07158](https://arxiv.org/abs/2402.07158)

    本文提出了一种基于大型语言模型的智能界面在软件项目中进行工作量和规模估计的方法，并通过比较传统方法，探讨了如何通过增强基于自然语言的问题规范来实现开发工作量的准确估计。

    

    大型语言模型（LLM）的发展也导致其应用的广泛增加。软件设计作为其中之一，在使用LLM作为扩展固定用户故事的接口组件方面获得了巨大的好处。然而，将基于LLM的人工智能代理包含在软件设计中常常带来意想不到的挑战，特别是在开发工作量的估计方面。通过基于用户界面的用户故事的例子，我们对比了传统方法，并提出了一种新的方法来增强基于自然语言的问题的规范，通过考虑数据源、接口和算法来进行开发工作量的估计。

    The advancement of Large Language Models (LLM) has also resulted in an equivalent proliferation in its applications. Software design, being one, has gained tremendous benefits in using LLMs as an interface component that extends fixed user stories. However, inclusion of LLM-based AI agents in software design often poses unexpected challenges, especially in the estimation of development efforts. Through the example of UI-based user stories, we provide a comparison against traditional methods and propose a new way to enhance specifications of natural language-based questions that allows for the estimation of development effort by taking into account data sources, interfaces and algorithms.
    
[^12]: 高维点过程的带结构缺失的潜变量模型

    Latent variable model for high-dimensional point process with structured missingness

    [https://arxiv.org/abs/2402.05758](https://arxiv.org/abs/2402.05758)

    本文提出了一种针对高维点过程的带有结构缺失的灵活高效的潜变量模型，利用高斯过程捕获时间相关性，并开发了可扩展的变分推理方法进行训练。

    

    纵向数据在医疗保健、社会学和地震学等许多领域中具有重要意义，但是真实世界的数据集对从业人员来说存在明显的挑战，因为它们可能是高维的，包含有结构化的缺失模式，并且测量时间点可能受到未知随机过程的控制。尽管已经提出了各种解决方案，但其中大多数仅考虑了这些挑战中的一个。在这项工作中，我们提出了一种灵活高效的潜变量模型，能够应对所有这些限制。我们的方法利用高斯过程来捕获样本与其关联的缺失模式之间的时间相关性，同时也用于建模底层的点过程。我们将我们的模型构建为一个变分自动编码器，同时使用深度神经网络参数化的编码器和解码器模型，并开发了一个可扩展的变分推理方法来进行高效的模型训练。我们展示了这个模型在各个领域的竞争性能。

    Longitudinal data are important in numerous fields, such as healthcare, sociology and seismology, but real-world datasets present notable challenges for practitioners because they can be high-dimensional, contain structured missingness patterns, and measurement time points can be governed by an unknown stochastic process. While various solutions have been suggested, the majority of them have been designed to account for only one of these challenges. In this work, we propose a flexible and efficient latent-variable model that is capable of addressing all these limitations. Our approach utilizes Gaussian processes to capture temporal correlations between samples and their associated missingness masks as well as to model the underlying point process. We construct our model as a variational autoencoder together with deep neural network parameterised encoder and decoder models, and develop a scalable amortised variational inference approach for efficient model training. We demonstrate compe
    
[^13]: 使用真实数据和替代数据进行学习的扩展规律

    Scaling laws for learning with real and surrogate data

    [https://arxiv.org/abs/2402.04376](https://arxiv.org/abs/2402.04376)

    本研究探讨了将替代数据与真实数据整合以进行训练的方案，发现整合替代数据能够显著降低测试误差，并提出了一个扩展规律来描述混合模型的测试误差，可以用于预测最优加权和收益。

    

    收集大量高质量的数据通常被限制在成本昂贵或不切实际的范围内, 这是机器学习中的一个关键瓶颈。相反地, 可以将来自目标分布的小规模数据集与来自公共数据集、不同情况下收集的数据或由生成模型合成的数据相结合, 作为替代数据。我们提出了一种简单的方案来将替代数据整合到训练中, 并使用理论模型和实证研究探索其行为。我们的主要发现是：(i) 整合替代数据可以显著降低原始分布的测试误差；(ii) 为了获得这种效益, 使用最优加权经验风险最小化非常关键；(iii) 在混合使用真实数据和替代数据训练的模型的测试误差可以很好地用一个扩展规律来描述。这可以用来预测最优加权和收益。

    Collecting large quantities of high-quality data is often prohibitively expensive or impractical, and a crucial bottleneck in machine learning. One may instead augment a small set of $n$ data points from the target distribution with data from more accessible sources like public datasets, data collected under different circumstances, or synthesized by generative models. Blurring distinctions, we refer to such data as `surrogate data'.   We define a simple scheme for integrating surrogate data into training and use both theoretical models and empirical studies to explore its behavior. Our main findings are: $(i)$ Integrating surrogate data can significantly reduce the test error on the original distribution; $(ii)$ In order to reap this benefit, it is crucial to use optimally weighted empirical risk minimization; $(iii)$ The test error of models trained on mixtures of real and surrogate data is well described by a scaling law. This can be used to predict the optimal weighting and the gai
    
[^14]: BGE M3-嵌入：通过自知识蒸馏实现多语言、多功能和多粒度的文本嵌入

    BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation

    [https://arxiv.org/abs/2402.03216](https://arxiv.org/abs/2402.03216)

    BGE M3-嵌入是一种新的多语言、多功能和多粒度的文本嵌入模型，支持超过100种工作语言，并在多语言和跨语言检索任务上取得了最先进的性能。它能够同时执行密集检索、多向量检索和稀疏检索，并能处理不同粒度的输入。其有效训练包括了一种自知识蒸馏方法和优化的批处理策略。

    

    在本文中，我们提出了一种新的嵌入模型，称为M3-嵌入，以其在多语言、多功能和多粒度方面的多样性而著称。它可以支持超过100种工作语言，在多语言和跨语言检索任务上取得了新的最先进性能。它可以同时执行嵌入模型的三种常见检索功能：密集检索、多向量检索和稀疏检索，为现实世界的IR应用提供了统一的模型基础。它能够处理不同粒度的输入，从短句到长达8192个标记的文档。M3-嵌入的有效训练包括以下技术贡献。我们提出了一种新颖的自知识蒸馏方法，可以将来自不同检索功能的相关性分数整合为教师信号，以提高训练质量。我们还优化了批处理策略。

    In this paper, we present a new embedding model, called M3-Embedding, which is distinguished for its versatility in Multi-Linguality, Multi-Functionality, and Multi-Granularity. It can support more than 100 working languages, leading to new state-of-the-art performances on multi-lingual and cross-lingual retrieval tasks. It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval, which provides a unified model foundation for real-world IR applications. It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. The effective training of M3-Embedding involves the following technical contributions. We propose a novel self-knowledge distillation approach, where the relevance scores from different retrieval functionalities can be integrated as the teacher signal to enhance the training quality. We also optimize the batching strat
    
[^15]: 等变对称破缺集

    Equivariant Symmetry Breaking Sets

    [https://arxiv.org/abs/2402.02681](https://arxiv.org/abs/2402.02681)

    这里是中文总结出的一句话要点: 该论文提出了一种全等变的对称破缺框架，通过引入对称破缺集来破坏等变神经网络中的对称性。这种方法通用且适用于任何群的等变性。

    

    等变神经网络（ENN）已被证明在涉及潜在对称性的应用中非常有效。通过设计，ENN在给定更高对称性输入时无法产生较低对称性输出。然而，在许多物理系统中会发生自发对称破缺，我们可以从一个初始高度对称的状态获得一个较不对称的稳定状态。因此，我们必须了解如何系统地在ENN中破坏对称性。在这项工作中，我们提出了一种全等变的新型对称破缺框架。我们强调我们的方法是通用的，并适用于任何群的等变性。为了实现这一目标，我们引入了对称破缺集（SBS）的概念。我们不是重新设计现有的网络，而是设计了一组对称破缺对象，根据输入和输出的对称性将其输入到我们的网络中。我们展示了在这些集合上定义等变性的一种自然方式，它提供了额外的约束。通过最小化... (the abstract is incomplete and cut off)

    Equivariant neural networks (ENNs) have been shown to be extremely effective in applications involving underlying symmetries. By construction ENNs cannot produce lower symmetry outputs given a higher symmetry input. However, spontaneous symmetry breaking occurs in many physical systems and we may obtain a less symmetric stable state from an initial highly symmetric one. Hence, it is imperative that we understand how to systematically break symmetry in ENNs. In this work, we propose a novel symmetry breaking framework that is fully equivariant. We emphasize that our approach is general and applicable to equivariance under any group. To achieve this, we introduce the idea of symmetry breaking sets (SBS). Rather than redesign existing networks, we design sets of symmetry breaking objects which we feed into our network based on the symmetry of our inputs and outputs. We show there is a natural way to define equivariance on these sets, which gives an additional constraint. Minimizing the si
    
[^16]: ChIRAAG: 通过ChatGPT生成快速和自动断言的方法

    ChIRAAG: ChatGPT Informed Rapid and Automated Assertion Generation

    [https://arxiv.org/abs/2402.00093](https://arxiv.org/abs/2402.00093)

    本研究设计了一个基于大语言模型的流水线，通过自然语言规格生成英语、线性时态逻辑和SVA断言，并成功减少了断言错误率。

    

    System Verilog Assertion (SVA)的形式化是Formal Property Verification (FPV)过程中的一个关键但复杂的任务。传统上，SVA的形式化需要经验丰富的专家解释规格。这是耗时且容易出错的。然而，最近大语言模型（LLM）的进展使得基于LLM的自动断言生成引起了人们的兴趣。我们设计了一种新颖的基于LLM的流水线，用于从自然语言规格中生成英语、线性时态逻辑和SVA的断言。我们开发了一个基于OpenAI GPT4的自定义LLM用于实验。此外，我们还开发了测试平台来验证LLM生成的断言。只有43%的LLM生成的原始断言存在错误，包括语法和逻辑错误。通过使用从测试案例失败中得出的精心设计的提示，迭代地促使LLM，该流水线在最多九次提示迭代后可以生成正确的SVA。

    System Verilog Assertion (SVA) formulation, a critical yet complex task, is a pre-requisite in the Formal Property Verification (FPV) process. Traditionally, SVA formulation involves expert-driven interpretation of specifications. This is time consuming and prone to human error. However, recent advances in Large Language Models (LLM), LLM-informed automatic assertion generation is gaining interest. We designed a novel LLM-based pipeline to generate assertions in English Language, Linear Temporal Logic, and SVA from natural language specifications. We developed a custom LLM-based on OpenAI GPT4 for our experiments. Furthermore, we developed testbenches to verify/validate the LLM-generated assertions. Only 43% of LLM-generated raw assertions had errors, including syntax and logical errors. By iteratively prompting the LLMs using carefully crafted prompts derived from test case failures, the pipeline could generate correct SVAs after a maximum of nine iterations of prompting. Our results 
    
[^17]: 航班滑行安全的跑道物体分类器的鲁棒性评估

    Robustness Assessment of a Runway Object Classifier for Safe Aircraft Taxiing

    [https://arxiv.org/abs/2402.00035](https://arxiv.org/abs/2402.00035)

    本文介绍了对航班滑行安全的跑道物体分类器的鲁棒性评估，使用形式方法评估了该分类器对三种常见图像扰动类型的鲁棒性，并提出了一种利用单调性的方法。

    

    随着深度神经网络(DNNs)在许多计算问题上成为主要解决方案，航空业希望探索它们在减轻飞行员负担和改善运营安全方面的潜力。然而，在这类安全关键应用中使用DNNs需要进行彻底的认证过程。这一需求可以通过形式验证来解决，形式验证提供了严格的保证，例如证明某些误判的不存在。在本文中，我们使用Airbus当前正在开发的图像分类器DNN作为案例研究，旨在在飞机滑行阶段使用。我们使用形式方法来评估这个DNN对三种常见图像扰动类型的鲁棒性：噪声、亮度和对比度，以及它们的部分组合。这个过程涉及多次调用底层验证器，这可能在计算上是昂贵的；因此，我们提出了一种利用单调性的方法。

    As deep neural networks (DNNs) are becoming the prominent solution for many computational problems, the aviation industry seeks to explore their potential in alleviating pilot workload and in improving operational safety. However, the use of DNNs in this type of safety-critical applications requires a thorough certification process. This need can be addressed through formal verification, which provides rigorous assurances -- e.g.,~by proving the absence of certain mispredictions. In this case-study paper, we demonstrate this process using an image-classifier DNN currently under development at Airbus and intended for use during the aircraft taxiing phase. We use formal methods to assess this DNN's robustness to three common image perturbation types: noise, brightness and contrast, and some of their combinations. This process entails multiple invocations of the underlying verifier, which might be computationally expensive; and we therefore propose a method that leverages the monotonicity
    
[^18]: 曲率方向作为失去可塑性的解释

    Directions of Curvature as an Explanation for Loss of Plasticity

    [https://arxiv.org/abs/2312.00246](https://arxiv.org/abs/2312.00246)

    曲率方向的丧失被认为是导致神经网络可塑性丧失的一个重要原因，并且我们通过系统调查和在多个任务中的研究结果支持了这一观点。

    

    可塑性的丧失是神经网络丧失从新经验学习能力的现象。尽管在几种问题设置中经验上观察到，但对导致可塑性丧失的机制了解甚少。在本文中，我们提供了对可塑性丧失的一致解释：神经网络在训练过程中丧失了曲率方向，可将可塑性的丧失归因于这种曲率减少。为了支持这样的说法，我们对在MNIST、CIFAR-10和ImageNet中使用的不断学习任务中的可塑性丧失进行了系统调查。我们的研究结果表明，曲率方向的丧失与可塑性的丧失相吻合，同时还表明以前的解释不足以解释所有情况下的可塑性丧失。最后，我们展示了缓解可塑性丧失的正则化器也会保留曲率，促使采用简单的分布式正则化器。

    arXiv:2312.00246v2 Announce Type: replace  Abstract: Loss of plasticity is a phenomenon in which neural networks lose their ability to learn from new experience. Despite being empirically observed in several problem settings, little is understood about the mechanisms that lead to loss of plasticity. In this paper, we offer a consistent explanation for loss of plasticity: Neural networks lose directions of curvature during training and that loss of plasticity can be attributed to this reduction in curvature. To support such a claim, we provide a systematic investigation of loss of plasticity across continual learning tasks using MNIST, CIFAR-10 and ImageNet. Our findings illustrate that loss of curvature directions coincides with loss of plasticity, while also showing that previous explanations are insufficient to explain loss of plasticity in all settings. Lastly, we show that regularizers which mitigate loss of plasticity also preserve curvature, motivating a simple distributional reg
    
[^19]: 生成式人工智能驱动的物联网健康护理中的人类数字孪生：一项综合调查

    Generative AI-Driven Human Digital Twin in IoT-Healthcare: A Comprehensive Survey. (arXiv:2401.13699v1 [cs.HC])

    [http://arxiv.org/abs/2401.13699](http://arxiv.org/abs/2401.13699)

    该论文调查了在物联网健康护理中利用生成式人工智能驱动的人类数字孪生的应用。人类数字孪生作为多功能、生动的人类数字测试平台，可以模拟结果并指导实际治疗，从而提高物联网健康护理的能力。

    

    物联网可以显著提高人类生活的质量，特别是在健康护理方面吸引了广泛的关注。同时，人类数字孪生被提出作为一种创新的范式，可以全面地描述个体人体在数字世界中的复制，并实时反映其物理状况。自然地，人类数字孪生被设想为通过充当多功能、生动的人类数字测试平台来增强物联网健康护理的能力，模拟结果并指导实际治疗。然而，成功建立人类数字孪生需要高保真度的虚拟建模和强大的信息交互，但可能存在稀缺、偏倚和噪声数据。幸运的是，最近流行的一种名为生成式人工智能（GAI）的技术可能是一个有前途的解决方案，因为它可以利用先进的人工智能算法自动生成、操作和修改渐变视图。

    The Internet of things (IoT) can significantly enhance the quality of human life, specifically in healthcare, attracting extensive attentions to IoT-healthcare services. Meanwhile, the human digital twin (HDT) is proposed as an innovative paradigm that can comprehensively characterize the replication of the individual human body in the digital world and reflect its physical status in real time. Naturally, HDT is envisioned to empower IoT-healthcare beyond the application of healthcare monitoring by acting as a versatile and vivid human digital testbed, simulating the outcomes and guiding the practical treatments. However, successfully establishing HDT requires high-fidelity virtual modeling and strong information interactions but possibly with scarce, biased and noisy data. Fortunately, a recent popular technology called generative artificial intelligence (GAI) may be a promising solution because it can leverage advanced AI algorithms to automatically create, manipulate, and modify val
    
[^20]: 预测过敏性鼻炎亚卡激素皮下免疫治疗中患者依从性的序列模型

    Sequential Model for Predicting Patient Adherence in Subcutaneous Immunotherapy for Allergic Rhinitis. (arXiv:2401.11447v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.11447](http://arxiv.org/abs/2401.11447)

    本研究利用新颖的机器学习模型，准确预测患者的非依从风险和相关的系统症状评分，为长期过敏性鼻炎亚卡激素皮下免疫治疗的管理提供了一种新的方法。

    

    目标：皮下免疫治疗(SCIT)是过敏性鼻炎的长效因果治疗。如何提高患者对变应原免疫治疗(AIT)的依从性以最大化治疗效果，在AIT管理中起着至关重要的作用。本研究旨在利用新颖的机器学习模型，准确预测患者的非依从风险和相关的系统症状评分，为长期AIT的管理提供一种新的方法。方法：本研究开发和分析了两种模型，序列潜在行为者-评论家模型(SLAC)和长短期记忆模型(LSTM)，并基于评分和依从性预测能力进行评估。结果：在排除第一时间步的偏倚样本后，SLAC模型的预测依从准确率为60%-72%，而LSTM模型的准确率为66%-84%，根据时间步长的不同而变化。SLAC模型的均方根误差(RMSE)范围在0.93到2.22之间，而LSTM模型的RMSE范围在...

    Objective: Subcutaneous Immunotherapy (SCIT) is the long-lasting causal treatment of allergic rhinitis. How to enhance the adherence of patients to maximize the benefit of allergen immunotherapy (AIT) plays a crucial role in the management of AIT. This study aims to leverage novel machine learning models to precisely predict the risk of non-adherence of patients and related systematic symptom scores, to provide a novel approach in the management of long-term AIT.  Methods: The research develops and analyzes two models, Sequential Latent Actor-Critic (SLAC) and Long Short-Term Memory (LSTM), evaluating them based on scoring and adherence prediction capabilities.  Results: Excluding the biased samples at the first time step, the predictive adherence accuracy of the SLAC models is from $60\,\%$ to $72\%$, and for LSTM models, it is $66\,\%$ to $84\,\%$, varying according to the time steps. The range of Root Mean Square Error (RMSE) for SLAC models is between $0.93$ and $2.22$, while for L
    
[^21]: 在金融科技应用中寻找双目标 Pareto 最优欺诈预防规则集

    On Finding Bi-objective Pareto-optimal Fraud Prevention Rule Sets for Fintech Applications. (arXiv:2311.00964v1 [cs.LG])

    [http://arxiv.org/abs/2311.00964](http://arxiv.org/abs/2311.00964)

    本文研究了在金融科技应用中寻找高质量的双目标 Pareto 最优欺诈预防规则集的问题。通过采用 Pareto 最优性概念和启发式框架 PORS，我们成功提出了一组非支配的规则子集，并通过实证评估证明了其有效性。

    

    规则在金融科技机构中被广泛用于进行欺诈预防决策，因为规则具有直观的 if-then 结构，易于理解。在实践中，大型金融科技机构通常采用两阶段欺诈预防决策规则集挖掘框架。本文关注于从初始规则集中找到高质量的规则子集，以双目标空间（如精确率和召回率）为基础。为此，我们采用 Pareto 最优性概念，旨在找到一组非支配的规则子集，构成一个 Pareto 前沿。我们提出了一个基于启发式的框架 PORS，并确定了 PORS 的核心问题是前沿解决方案选择（SSF）问题。我们对 SSF 问题进行了系统分类，并在公开和专有数据集上进行了全面的实证评估。我们还引入了一种名为 SpectralRules 的新颖变体的顺序覆盖算法，以鼓励规则的多样性。

    Rules are widely used in Fintech institutions to make fraud prevention decisions, since rules are highly interpretable thanks to their intuitive if-then structure. In practice, a two-stage framework of fraud prevention decision rule set mining is usually employed in large Fintech institutions. This paper is concerned with finding high-quality rule subsets in a bi-objective space (such as precision and recall) from an initial pool of rules. To this end, we adopt the concept of Pareto optimality and aim to find a set of non-dominated rule subsets, which constitutes a Pareto front. We propose a heuristic-based framework called PORS and we identify that the core of PORS is the problem of solution selection on the front (SSF). We provide a systematic categorization of the SSF problem and a thorough empirical evaluation of various SSF methods on both public and proprietary datasets. We also introduce a novel variant of sequential covering algorithm called SpectralRules to encourage the diver
    
[^22]: 鱼网：信息最优，可扩展的集合和图聚合

    Fishnets: Information-Optimal, Scalable Aggregation for Sets and Graphs. (arXiv:2310.03812v1 [cs.LG])

    [http://arxiv.org/abs/2310.03812](http://arxiv.org/abs/2310.03812)

    Fishnets是一种用于学习信息最优的集合和图聚合的方法，在规模上可以优化到任意数量的数据对象，具有鲁棒性，能够饱和贝叶斯信息内容，并可用于GNNs中的消息传递。

    

    基于集合的学习是现代深度学习和网络科学的重要组成部分。图神经网络（GNNs）及其不含边的对应物Deepsets在不规则和拓扑复杂的数据集上被证明非常有用。为了学习集合成员的信息丰富的嵌入，关键是指定一个聚合函数，通常是求和、最大值或均值。我们提出了Fishnets，一种用于学习集合数据和图聚合的信息最优嵌入策略，适用于贝叶斯推理。我们证明了：i）Fishnets神经摘要可以最优地扩展到任意数量的数据对象；ii）Fishnets聚合对数据分布的改变具有鲁棒性，而标准的Deepsets不具备这种特性；iii）Fishnets饱和贝叶斯信息内容，并扩展到MCMC技术失败的领域；iv）Fishnets可以作为GNN中的一个插入式聚合方案。我们展示了通过采用Fishnets聚合方案进行消息传递，GNNs可以实现 达到

    Set-based learning is an essential component of modern deep learning and network science. Graph Neural Networks (GNNs) and their edge-free counterparts Deepsets have proven remarkably useful on ragged and topologically challenging datasets. The key to learning informative embeddings for set members is a specified aggregation function, usually a sum, max, or mean. We propose Fishnets, an aggregation strategy for learning information-optimal embeddings for sets of data for both Bayesian inference and graph aggregation. We demonstrate that i) Fishnets neural summaries can be scaled optimally to an arbitrary number of data objects, ii) Fishnets aggregations are robust to changes in data distribution, unlike standard deepsets, iii) Fishnets saturate Bayesian information content and extend to regimes where MCMC techniques fail and iv) Fishnets can be used as a drop-in aggregation scheme within GNNs. We show that by adopting a Fishnets aggregation scheme for message passing, GNNs can achieve 
    
[^23]: 丢弃模式的生成自编码器

    Generative Autoencoding of Dropout Patterns. (arXiv:2310.01712v1 [cs.LG])

    [http://arxiv.org/abs/2310.01712](http://arxiv.org/abs/2310.01712)

    本论文提出了一种称为解读自编码器的生成模型，通过为训练数据集中的每个数据点分配独特的随机丢弃模式来进行训练，只依靠重构误差来提供更稳定的训练性能，并在CIFAR-10数据集上展示了与DCGAN相媲美的采样质量。

    

    我们提出了一种称为解读自编码器的生成模型。在这个模型中，我们为训练数据集中的每个数据点分配一个唯一的随机丢弃模式，然后使用这个模式作为被编码的信息来训练自编码器来重构相应的数据点。由于解读自编码器的训练仅依赖于重构误差，所以相比其他生成模型，它具有更稳定的训练性能。尽管它很简单，但解读自编码器在CIFAR-10数据集上展现出了与DCGAN相媲美的采样质量。

    We propose a generative model termed Deciphering Autoencoders. In this model, we assign a unique random dropout pattern to each data point in the training dataset and then train an autoencoder to reconstruct the corresponding data point using this pattern as information to be encoded. Since the training of Deciphering Autoencoders relies solely on reconstruction error, it offers more stable training than other generative models. Despite its simplicity, Deciphering Autoencoders show comparable sampling quality to DCGAN on the CIFAR-10 dataset.
    
[^24]: 3D-Mol: 一种新颖的基于对比学习的分子性质预测框架，利用了3D信息

    3D-Mol: A Novel Contrastive Learning Framework for Molecular Property Prediction with 3D Information. (arXiv:2309.17366v1 [q-bio.BM])

    [http://arxiv.org/abs/2309.17366](http://arxiv.org/abs/2309.17366)

    3D-Mol是一种新颖的基于3D结构的分子建模方法，通过对比学习提高了分子性质预测准确性，并在多个基准数据集上超过了最先进的模型。

    

    分子性质预测为药物候选物的早期筛选和优化提供了一种有效且高效的方法。尽管基于深度学习的方法取得了显著进展，但大多数现有方法仍未充分利用3D空间信息。这可能导致单个分子表示多个实际分子。为解决这些问题，我们提出了一种名为3D-Mol的新颖的基于3D结构的分子建模方法。为了准确表示完整的空间结构，我们设计了一种新颖的编码器，通过将分子分解成三个几何图形来提取3D特征。此外，我们使用20M个无标签数据通过对比学习对模型进行预训练。我们将具有相同拓扑结构的构象视为正样本对，将相反的构象视为负样本对，而权重则由构象之间的差异确定。我们在7个基准数据集上将3D-Mol与各种最先进的基准模型进行了对比。

    Molecular property prediction offers an effective and efficient approach for early screening and optimization of drug candidates. Although deep learning based methods have made notable progress, most existing works still do not fully utilize 3D spatial information. This can lead to a single molecular representation representing multiple actual molecules. To address these issues, we propose a novel 3D structure-based molecular modeling method named 3D-Mol. In order to accurately represent complete spatial structure, we design a novel encoder to extract 3D features by deconstructing the molecules into three geometric graphs. In addition, we use 20M unlabeled data to pretrain our model by contrastive learning. We consider conformations with the same topological structure as positive pairs and the opposites as negative pairs, while the weight is determined by the dissimilarity between the conformations. We compare 3D-Mol with various state-of-the-art (SOTA) baselines on 7 benchmarks and de
    
[^25]: 一个用于分类和解释大型非结构化法律文件的分层神经框架

    A Hierarchical Neural Framework for Classification and its Explanation in Large Unstructured Legal Documents. (arXiv:2309.10563v1 [cs.IR])

    [http://arxiv.org/abs/2309.10563](http://arxiv.org/abs/2309.10563)

    本论文提出了一个名为MESc的分层神经框架，用于分类和解释大型非结构化法律文件。通过将文件分成多个部分并使用大型语言模型的嵌入和无监督聚类，该框架能够实现从长文档中预测判决并提取解释。

    

    自动法律判决预测及其解释常常面临长达数万字的案例文件和非统一结构的问题。在没有结构标注的文件上预测判决并提取解释变得更具挑战性。本论文将这一问题定义为“稀缺标注法律文件”，并通过一种称为MESc（基于多阶段编码器的带聚类的监督）的深度学习分类框架来探索缺乏结构信息和长文档的特点。具体来说，我们将文档分成多个部分，从自定义微调的大型语言模型的最后四个层中提取它们的嵌入，并试图通过无监督聚类来近似它们的结构。然后，我们利用另一组Transformer编码器层学习部分之间的表示。我们探索了多十亿参数的大型语言模型在这种情况下的适应性。

    Automatic legal judgment prediction and its explanation suffer from the problem of long case documents exceeding tens of thousands of words, in general, and having a non-uniform structure. Predicting judgments from such documents and extracting their explanation becomes a challenging task, more so on documents with no structural annotation. We define this problem as "scarce annotated legal documents" and explore their lack of structural information and their long lengths with a deep learning-based classification framework which we call MESc; "Multi-stage Encoder-based Supervised with-clustering"; for judgment prediction. Specifically, we divide a document into parts to extract their embeddings from the last four layers of a custom fine-tuned Large Language Model, and try to approximate their structure through unsupervised clustering. Which we use in another set of transformer encoder layers to learn the inter-chunk representations. We explore the adaptability of LLMs with multi-billion
    
[^26]: 大维度情况下核回归的最优比率

    Optimal Rate of Kernel Regression in Large Dimensions. (arXiv:2309.04268v1 [stat.ML])

    [http://arxiv.org/abs/2309.04268](http://arxiv.org/abs/2309.04268)

    该论文提出了一种针对大维度数据的核回归的最优比率，通过使用Mendelson复杂性和度量熵来刻画其上界和最小化下界。此外，研究还发现最优比率随着维度与样本大小关系的变化呈现出多次下降的行为。

    

    我们对大维度数据（样本大小$n$与样本维度$d$的关系为多项式，即$n\asymp d^{\gamma}$，其中$\gamma>0$）的核回归进行了研究。我们首先通过Mendelson复杂性$\varepsilon_{n}^{2}$和度量熵$\bar{\varepsilon}_{n}^{2}$来建立一个通用工具，用于刻画大维度数据的核回归的上界和最小化下界。当目标函数属于与$\mathbb{S}^{d}$上定义的（一般）内积模型相关联的RKHS时，我们利用这个新工具来展示核回归的过量风险的最小化率是$n^{-1/2}$，当$n\asymp d^{\gamma}$，其中$\gamma=2, 4, 6, 8, \cdots$。然后我们进一步确定了对于所有$\gamma>0$，核回归过量风险的最优比率，并发现随着$\gamma$的变化，最优比率的曲线展现出几个新现象，包括多次下降行为。

    We perform a study on kernel regression for large-dimensional data (where the sample size $n$ is polynomially depending on the dimension $d$ of the samples, i.e., $n\asymp d^{\gamma}$ for some $\gamma >0$ ). We first build a general tool to characterize the upper bound and the minimax lower bound of kernel regression for large dimensional data through the Mendelson complexity $\varepsilon_{n}^{2}$ and the metric entropy $\bar{\varepsilon}_{n}^{2}$ respectively. When the target function falls into the RKHS associated with a (general) inner product model defined on $\mathbb{S}^{d}$, we utilize the new tool to show that the minimax rate of the excess risk of kernel regression is $n^{-1/2}$ when $n\asymp d^{\gamma}$ for $\gamma =2, 4, 6, 8, \cdots$. We then further determine the optimal rate of the excess risk of kernel regression for all the $\gamma>0$ and find that the curve of optimal rate varying along $\gamma$ exhibits several new phenomena including the {\it multiple descent behavior
    
[^27]: 基于Deep Maxout网络特征融合和Political Tangent Search优化器的转移学习在地中海贫血检测中的应用

    Deep Maxout Network-based Feature Fusion and Political Tangent Search Optimizer enabled Transfer Learning for Thalassemia Detection. (arXiv:2308.02029v1 [cs.LG])

    [http://arxiv.org/abs/2308.02029](http://arxiv.org/abs/2308.02029)

    本文提出了基于Deep Maxout网络特征融合和Political Tangent Search优化器的转移学习方法，应用于地中海贫血检测。该方法对输入数据进行归一化处理，并利用Deep Maxout网络的特征融合和过采样方法进行数据增强，最终通过转移学习进行地中海贫血检测。

    

    地中海贫血是一种遗传性血液病，由遗传缺陷导致血红蛋白多肽链的产生不足。然而，对这些地区的发病频率和共享程度的了解较少。了解地中海贫血发生的频率和可靠突变是预防、控制和治疗计划的重要一步。本文介绍了基于Political Tangent Search优化器的转移学习（PTSO_TL）在地中海贫血检测中的应用。首先，从特定数据集获取的输入数据在数据归一化阶段进行了规范化。数据归一化阶段利用分位数归一化方法，然后将数据传递给特征融合阶段，在该阶段利用Deep Maxout网络的加权欧氏距离进行特征融合。然后，使用过采样方法进行数据增强以增加数据维度。最后，通过转移学习进行地中海贫血检测。

    Thalassemia is a heritable blood disorder which is the outcome of a genetic defect causing lack of production of hemoglobin polypeptide chains. However, there is less understanding of the precise frequency as well as sharing in these areas. Knowing about the frequency of thalassemia occurrence and dependable mutations is thus a significant step in preventing, controlling, and treatment planning. Here, Political Tangent Search Optimizer based Transfer Learning (PTSO_TL) is introduced for thalassemia detection. Initially, input data obtained from a particular dataset is normalized in the data normalization stage. Quantile normalization is utilized in the data normalization stage, and the data are then passed to the feature fusion phase, in which Weighted Euclidean Distance with Deep Maxout Network (DMN) is utilized. Thereafter, data augmentation is performed using the oversampling method to increase data dimensionality. Lastly, thalassemia detection is carried out by TL, wherein a convol
    
[^28]: SciBench: 对大型语言模型评估大学水平的科学问题解决能力

    SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models. (arXiv:2307.10635v1 [cs.CL])

    [http://arxiv.org/abs/2307.10635](http://arxiv.org/abs/2307.10635)

    这篇论文介绍了一个名为SciBench的基准套件，旨在对大型语言模型的大学水平科学问题解决能力进行评估。研究结果显示，当前的语言模型在提供复杂科学问题解决能力方面还有不足之处。

    

    最近大型语言模型(LLMs)的进展在许多数学基准上取得了显著的进步。然而，这些基准大多只包含初高中科目的问题，仅包含多项选择题，并且仅限于基本算术运算范围。为了解决这些问题，本文介绍了一个广泛的基准套件SciBench，旨在系统地检测复杂科学问题解决所需的推理能力。SciBench包含两个经过精心策划的数据集：一个开放集，包括从数学、化学和物理教科书中摘录的大学水平的科学问题，以及一个封闭集，包含来自计算机科学和数学本科考试的问题。基于这两个数据集，我们对两个代表性的LLM进行了深入的基准研究，并采用不同的提示策略。结果表明，当前的LLMs在提供复杂科学问题解决能力方面还存在不足之处。

    Recent advances in large language models (LLMs) have demonstrated notable progress on many mathematical benchmarks. However, most of these benchmarks only feature problems grounded in junior and senior high school subjects, contain only multiple-choice questions, and are confined to a limited scope of elementary arithmetic operations. To address these issues, this paper introduces an expansive benchmark suite SciBench that aims to systematically examine the reasoning capabilities required for complex scientific problem solving. SciBench contains two carefully curated datasets: an open set featuring a range of collegiate-level scientific problems drawn from mathematics, chemistry, and physics textbooks, and a closed set comprising problems from undergraduate-level exams in computer science and mathematics. Based on the two datasets, we conduct an in-depth benchmark study of two representative LLMs with various prompting strategies. The results reveal that current LLMs fall short of deli
    
[^29]: MALIBO: 元学习应用于无似然贝叶斯优化

    MALIBO: Meta-learning for Likelihood-free Bayesian Optimization. (arXiv:2307.03565v1 [cs.LG])

    [http://arxiv.org/abs/2307.03565](http://arxiv.org/abs/2307.03565)

    MALIBO是一种元学习贝叶斯优化方法，通过直接学习跨任务的查询效用，并引入辅助模型以实现对新任务的稳健适应，克服了现有方法的可伸缩性和不确定性的限制。

    

    贝叶斯优化是一种优化昂贵黑盒函数的流行方法。传统的贝叶斯优化会从头开始优化每个新的目标任务，而元学习则是利用相关任务的知识来更快地优化新任务的一种方式。然而，现有的元学习贝叶斯优化方法依赖于标准模型，这些模型存在可伸缩性问题，并且对不同任务之间观察数据的尺度和噪声类型非常敏感。此外，它们常常忽视与任务相似性相关的不确定性，这导致在仅有有限观察数据或新任务与相关任务差异显著时，任务适应性不可靠。为了解决这些限制，我们提出了一种新颖的元学习贝叶斯优化方法，旨在绕开标准模型，直接学习跨任务的查询效用。我们的方法明确建模任务的不确定性，并引入了一个辅助模型，使其能够对新任务进行稳健适应。大量实验证明了我们方法的有效性。

    Bayesian optimization (BO) is a popular method to optimize costly black-box functions. While traditional BO optimizes each new target task from scratch, meta-learning has emerged as a way to leverage knowledge from related tasks to optimize new tasks faster. However, existing meta-learning BO methods rely on surrogate models that suffer from scalability issues and are sensitive to observations with different scales and noise types across tasks. Moreover, they often overlook the uncertainty associated with task similarity. This leads to unreliable task adaptation when only limited observations are obtained or when the new tasks differ significantly from the related tasks. To address these limitations, we propose a novel meta-learning BO approach that bypasses the surrogate model and directly learns the utility of queries across tasks. Our method explicitly models task uncertainty and includes an auxiliary model to enable robust adaptation to new tasks. Extensive experiments show that ou
    
[^30]: 分布式智能体在均场博弈中的网络通信

    Networked Communication for Decentralised Agents in Mean-Field Games. (arXiv:2306.02766v2 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2306.02766](http://arxiv.org/abs/2306.02766)

    本研究在均场博弈中引入网络通信，提出了一种提高分布式智能体学习效率的方案，并进行了实际实验验证。

    

    我们将网络通信引入均场博弈框架，特别是在无oracle的情况下，N个分布式智能体沿着经过的经验系统的单一非周期演化路径学习。我们证明，我们的架构在只有一些关于网络结构的合理假设的情况下，具有样本保证，在集中学习和独立学习情况之间有界。我们讨论了三个理论算法的样本保证实际上并不会导致实际收敛。因此，我们展示了在实际设置中，当理论参数未被观察到（导致Q函数的估计不准确）时，我们的通信方案显著加速了收敛速度，而无需依赖于一个不可取的集中式控制器的假设。我们对三个理论算法进行了几种实际的改进，使我们能够展示它们的第一个实证表现。

    We introduce networked communication to the mean-field game framework, in particular to oracle-free settings where $N$ decentralised agents learn along a single, non-episodic evolution path of the empirical system. We prove that our architecture, with only a few reasonable assumptions about network structure, has sample guarantees bounded between those of the centralised- and independent-learning cases. We discuss how the sample guarantees of the three theoretical algorithms do not actually result in practical convergence. Accordingly, we show that in practical settings where the theoretical parameters are not observed (leading to poor estimation of the Q-function), our communication scheme significantly accelerates convergence over the independent case, without relying on the undesirable assumption of a centralised controller. We contribute several further practical enhancements to all three theoretical algorithms, allowing us to showcase their first empirical demonstrations. Our expe
    
[^31]: 走向软件自愈：结合大语言模型和形式验证解决软件安全问题

    A New Era in Software Security: Towards Self-Healing Software via Large Language Models and Formal Verification. (arXiv:2305.14752v1 [cs.SE])

    [http://arxiv.org/abs/2305.14752](http://arxiv.org/abs/2305.14752)

    本文介绍了一个结合大语言模型和形式验证的方法来自动验证和修复软件漏洞，并通过ESBMC-AI做出了概念验证。

    

    本文提出了一种新方法，将大语言模型和形式化验证策略相结合，使得软件漏洞可以得到验证和自动修复。首先利用有限模型检查（BMC）定位软件漏洞和派生反例。然后，将反例和源代码提供给大语言模型引擎进行代码调试和生成，从而找到漏洞的根本原因并修复代码。最后，则使用BMC验证大语言模型生成的修正版本的代码。 作为概念证明，我们创建了ESBMC-AI，它基于高效的基于SMT的上下文有界模型检查器（ESBMC）和一个预训练的Transformer模型gpt-3.5-turbo来检测和修复C程序中的错误。

    In this paper we present a novel solution that combines the capabilities of Large Language Models (LLMs) with Formal Verification strategies to verify and automatically repair software vulnerabilities. Initially, we employ Bounded Model Checking (BMC) to locate the software vulnerability and derive a counterexample. The counterexample provides evidence that the system behaves incorrectly or contains a vulnerability. The counterexample that has been detected, along with the source code, are provided to the LLM engine. Our approach involves establishing a specialized prompt language for conducting code debugging and generation to understand the vulnerability's root cause and repair the code. Finally, we use BMC to verify the corrected version of the code generated by the LLM. As a proof of concept, we create ESBMC-AI based on the Efficient SMT-based Context-Bounded Model Checker (ESBMC) and a pre-trained Transformer model, specifically gpt-3.5-turbo, to detect and fix errors in C program
    
[^32]: 面向不均衡数据的鲁棒基于模型的设计的属性引导生成建模

    Property-Guided Generative Modelling for Robust Model-Based Design with Imbalanced Data. (arXiv:2305.13650v1 [cs.LG])

    [http://arxiv.org/abs/2305.13650](http://arxiv.org/abs/2305.13650)

    本文提出了一种属性引导的变分自编码器（PGVAE），通过属性值明确结构化潜在空间，使得MBO可以在不平衡数据上稳健地寻找具有改进属性的序列。

    

    设计具有特定属性的蛋白质序列是一项具有挑战性的任务，因为这需要探索具有极度稀疏的有意义区域的高维蛋白质序列空间。这导致了模型优化（MBO）技术的发展，通过使用由序列空间中的属性引导的有效搜索模型来辅助设计。然而，实验获得的数据集的内在不平衡性使得现有的MBO方法很难或根本无法处理。我们提出了一种属性引导的变分自编码器（PGVAE），其潜在空间由属性值明确结构化，使得按照这些属性值优先考虑样本。通过对真实和半合成蛋白质数据集的广泛基准测试，我们展示了MBO与PGVAE稳健地发现具有改进属性的序列，尽管数据集存在显著的不平衡性。我们进一步展示了我们的方法对于连续设计空间的普适性及其稳健性。

    The problem of designing protein sequences with desired properties is challenging, as it requires to explore a high-dimensional protein sequence space with extremely sparse meaningful regions. This has led to the development of model-based optimization (MBO) techniques that aid in the design, by using effective search models guided by the properties over the sequence space. However, the intrinsic imbalanced nature of experimentally derived datasets causes existing MBO approaches to struggle or outright fail. We propose a property-guided variational auto-encoder (PGVAE) whose latent space is explicitly structured by the property values such that samples are prioritized according to these properties. Through extensive benchmarking on real and semi-synthetic protein datasets, we demonstrate that MBO with PGVAE robustly finds sequences with improved properties despite significant dataset imbalances. We further showcase the generality of our approach to continuous design spaces, and its rob
    
[^33]: 蛋白质序列设计的重要性加权期望最大化方法

    Importance Weighted Expectation-Maximization for Protein Sequence Design. (arXiv:2305.00386v1 [q-bio.BM])

    [http://arxiv.org/abs/2305.00386](http://arxiv.org/abs/2305.00386)

    本文提出了一种名为IsEM-Pro的方法，用于根据给定适应性标准生成蛋白质序列。在推理期间，从其潜在空间采样可以增加多样性，指导了探索高适应性区域。实验表明，相比先前最佳方法，IsEM-Pro的平均适应性得分至少高出55％，并生成了更多样化和新颖的蛋白质序列。

    

    在生物和化学领域，设计具有所需生物功能的蛋白质序列非常重要。最近的机器学习方法使用代理序列-功能模型替代昂贵的湿实验验证。本文提出了一种名为IsEM-Pro的方法，用于根据给定的适应性标准生成蛋白质序列。它是一个潜在的生成模型，并受到另外一个学习的马尔可夫随机场结构特征的增强。研究者使用蒙特卡罗期望最大化方法（MCEM）来学习这个模型。在推理期间，从其潜在空间采样可以增加多样性，而其MRF特征则指导了探索高适应性区域。在八项蛋白质序列设计任务中的实验表明，我们的IsEM-Pro的平均适应性得分至少比先前最佳方法高55％，并且生成了更多样化和新颖的蛋白质序列。

    Designing protein sequences with desired biological function is crucial in biology and chemistry. Recent machine learning methods use a surrogate sequence-function model to replace the expensive wet-lab validation. How can we efficiently generate diverse and novel protein sequences with high fitness? In this paper, we propose IsEM-Pro, an approach to generate protein sequences towards a given fitness criterion. At its core, IsEM-Pro is a latent generative model, augmented by combinatorial structure features from a separately learned Markov random fields (MRFs). We develop an Monte Carlo Expectation-Maximization method (MCEM) to learn the model. During inference, sampling from its latent space enhances diversity while its MRFs features guide the exploration in high fitness regions. Experiments on eight protein sequence design tasks show that our IsEM-Pro outperforms the previous best methods by at least 55% on average fitness score and generates more diverse and novel protein sequences.
    
[^34]: G不变图拉普拉斯算子

    The G-invariant graph Laplacian. (arXiv:2303.17001v1 [cs.LG])

    [http://arxiv.org/abs/2303.17001](http://arxiv.org/abs/2303.17001)

    本文提出了 G不变图拉普拉斯算子 用于处理数据集不仅在流形上，而且在一个连续群的作用下也是封闭的情形，相较于标准图拉普拉斯算子收敛速度更快。

    

    基于图拉普拉斯算子的算法已经被证明在降维、聚类和去噪等领域对流形数据非常有效。本文考虑的数据集不仅在流形上，而且在一个连续群的作用下也是封闭的。这类数据集的一个例子是沿着低维流形传播的体积，其中每个体积可以在三维空间中旋转。我们介绍了G不变图拉普拉斯算子，通过考虑数据集上的群的作用来广义化图拉普拉斯算子。我们显示了与标准图拉普拉斯算子类似，G不变图拉普拉斯算子收敛于数据流形上的Laplace-Beltrami算子，但收敛速度显著提高。此外，我们还表明G不变图拉普拉斯算子的特征函数具有群元素和某些矩阵的特征向量的张量积形式，可以使用F高效地计算。

    Graph Laplacian based algorithms for data lying on a manifold have been proven effective for tasks such as dimensionality reduction, clustering, and denoising. In this work, we consider data sets whose data point not only lie on a manifold, but are also closed under the action of a continuous group. An example of such data set is volumes that line on a low dimensional manifold, where each volume may be rotated in three-dimensional space. We introduce the G-invariant graph Laplacian that generalizes the graph Laplacian by accounting for the action of the group on the data set. We show that like the standard graph Laplacian, the G-invariant graph Laplacian converges to the Laplace-Beltrami operator on the data manifold, but with a significantly improved convergence rate. Furthermore, we show that the eigenfunctions of the G-invariant graph Laplacian admit the form of tensor products between the group elements and eigenvectors of certain matrices, which can be computed efficiently using F
    

