# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Remote sensing framework for geological mapping via stacked autoencoders and clustering](https://arxiv.org/abs/2404.02180) | 通过堆叠自动编码器和聚类实现遥感数据地质制图的无监督机器学习框架 |
| [^2] | [TWIN-GPT: Digital Twins for Clinical Trials via Large Language Model](https://arxiv.org/abs/2404.01273) | 提出了基于大语言模型的数字孪生体TWIN-GPT，用于支持临床试验结果预测。 |
| [^3] | [Novel Node Category Detection Under Subpopulation Shift](https://arxiv.org/abs/2404.01216) | 提出了一种新方法 RECO-SLIP，用于在属性图中检测属于新类别的节点，能够有效解决子群体转移下的节点检测问题，实验证明其性能优越。 |
| [^4] | [AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving](https://arxiv.org/abs/2403.19708) | AttentionStore提出了一种新的注意力机制，通过实现KV缓存的复用，在大型语言模型服务中显著降低了多轮对话中的重复计算成本。 |
| [^5] | [First Experiences with the Identification of People at Risk for Diabetes in Argentina using Machine Learning Techniques](https://arxiv.org/abs/2403.18631) | 在本研究中，针对阿根廷地区的情况，发展和评估了用于识别2型糖尿病（T2D）和糖尿病前期（PD）风险人群的预测模型，并展示了部分模型在两个特定数据集上取得了非常好的性能。 |
| [^6] | [Is Watermarking LLM-Generated Code Robust?](https://arxiv.org/abs/2403.17983) | 该研究探讨了现有水印技术在大型语言模型生成的Python代码上的鲁棒性，发现容易通过保留语义转换来移除这些水印。 |
| [^7] | [Counterfactual Fairness through Transforming Data Orthogonal to Bias](https://arxiv.org/abs/2403.17852) | 提出了一种新颖的数据预处理算法，正交于偏见（OB），通过确保数据与敏感变量不相关，实现机器学习应用中的反事实公平性。 |
| [^8] | [Capacity Provisioning Motivated Online Non-Convex Optimization Problem with Memory and Switching Cost](https://arxiv.org/abs/2403.17480) | 该论文考虑了一种在线非凸优化问题，目标是通过调节活动服务器数量最小化作业延迟，引入了切换成本，提出了竞争算法。 |
| [^9] | [Dynamic Relative Representations for Goal-Oriented Semantic Communications](https://arxiv.org/abs/2403.16986) | 本文提出了一个新颖的面向目标的语义通信框架，利用相对表示通过潜在空间对齐来缓解语义不匹配，并实现了能效高、延迟低的目标导向语义通信。 |
| [^10] | [CONLINE: Complex Code Generation and Refinement with Online Searching and Correctness Testing](https://arxiv.org/abs/2403.13583) | CONLINE框架提出了通过在线搜索和正确性测试来增强复杂代码生成的方法，通过实验证明了其显著提高了代码生成质量。 |
| [^11] | [Training morphological neural networks with gradient descent: some theoretical insights](https://arxiv.org/abs/2403.12975) | 形态神经网络的训练存在挑战，本文通过使用基于梯度下降的优化算法，探讨了基于微分方法和反向传播对形态网络的潜力和局限性，提供了关于初始化和学习率的理论指导。 |
| [^12] | [Regularization-Based Efficient Continual Learning in Deep State-Space Models](https://arxiv.org/abs/2403.10123) | 提出了一种正则化驱动的深度状态空间模型，实现了高效的持续学习，能够在多个动态系统建模时进行有效更新，并且通过实验证实了其有效性 |
| [^13] | [Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts](https://arxiv.org/abs/2403.08477) | 本文提出了一种名为Sparse MetA-Tuning（SMAT）的方法，通过灵感来自稀疏专家混合方法，成功克服了域外任务敏感性，实现了增强视觉基础模型转移能力的目标。 |
| [^14] | [Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine Unlearning](https://arxiv.org/abs/2403.07362) | 该论文从对抗的角度提出了一种新的机器遗忘评估方法，通过确定最具挑战性的数据子集，即最坏情况遗忘集，来增强对影响擦除的挑战。 |
| [^15] | [Spatial features of CO2 for occupancy detection in a naturally ventilated school building](https://arxiv.org/abs/2403.06643) | 通过空间CO2浓度分布提出两种新特征，使用支持向量机进行量化分析后发现，与基准相比，在自然通风房间中的占用状态检测准确率最多可提高14.8个百分点，达到83.2％（F1分数0.84）。有通风信息的情况下，准确率达到87.6％。 |
| [^16] | [A Library of Mirrors: Deep Neural Nets in Low Dimensions are Convex Lasso Models with Reflection Features](https://arxiv.org/abs/2403.01046) | 证明在1-D数据上训练神经网络等价于解决一个具有固定特征字典矩阵的凸Lasso问题，为全局最优网络和解空间提供了洞察。 |
| [^17] | [Extended Flow Matching: a Method of Conditional Generation with Generalized Continuity Equation](https://arxiv.org/abs/2402.18839) | 本文基于Flow Matching发展了条件生成理论，通过使用广义连续性方程的数学框架而非流匹配中的连续性方程，实现了一种新颖的流基条件分布生成方法。 |
| [^18] | [If in a Crowdsourced Data Annotation Pipeline, a GPT-4](https://arxiv.org/abs/2402.16795) | 本文比较了 GPT-4 和 MTurk 管道的数据标注准确性，发现尽管 MTurk 采用了最佳实践，但 GPT-4 的准确率更高，并且结合 GPT-4 和众包标签使用聚合算法可以提高准确率。 |
| [^19] | [Cost Aware Best Arm Identification](https://arxiv.org/abs/2402.16710) | 本文研究了一个带有成本分布的最佳臂识别问题，提出了CABAI方法以实现最小期望成本下识别出最大奖励臂，并设计了$\mathsf{CTAS}$和CO两种算法来逼近理论下限并优化计算复杂度。 |
| [^20] | [Learning method for S4 with Diagonal State Space Layers using Balanced Truncation](https://arxiv.org/abs/2402.15993) | 一种用于处理长序列数据的边缘智能应用的S4模型的学习方法，利用平衡截断技术降低计算成本，并通过改进初始化过程和优化准确度和效率指标来超越传统训练模型。 |
| [^21] | [Bringing Generative AI to Adaptive Learning in Education](https://arxiv.org/abs/2402.14601) | 生成式人工智能技术与自适应学习概念的交叉研究将对教育中下一阶段学习格式的发展做出重要贡献。 |
| [^22] | [Revitalizing Multivariate Time Series Forecasting: Learnable Decomposition with Inter-Series Dependencies and Intra-Series Variations Modeling](https://arxiv.org/abs/2402.12694) | 引入可学习分解策略和双注意力模块，同时捕捉跨系列依赖和内部变化，以应对复杂的多变量时间序列预测挑战。 |
| [^23] | [A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step Reasoning Task](https://arxiv.org/abs/2402.11917) | 对在合成推理任务上训练的Transformer进行的机理分析揭示其实现了一个在并行运行的有界深度循环机制，并将中间结果存储在选定的令牌位置 |
| [^24] | [UniST: A Prompt-Empowered Universal Model for Urban Spatio-Temporal Prediction](https://arxiv.org/abs/2402.11838) | UniST是一种为城市时空预测设计的通用模型，通过灵活性、有效的生成式预训练以及丰富的掩码策略成功捕捉复杂的时空关系。 |
| [^25] | [Integrating Pre-Trained Language Model with Physical Layer Communications](https://arxiv.org/abs/2402.11656) | 提出了一个集成了物理层通信功能的实用设备间人工智能通信框架，通过端到端训练结合信道噪声以增强韧性，采用VQ-VAE实现高效稳健的通信，利用预训练Transformer提升通用性能 |
| [^26] | [Recovering the Pre-Fine-Tuning Weights of Generative Models](https://arxiv.org/abs/2402.10208) | 该论文提出了一种恢复生成模型预微调权重的方法，通过少量低秩微调模型可以恢复准确的预微调权重，利用这个新漏洞攻击大规模模型。 |
| [^27] | [Generalization Error of Graph Neural Networks in the Mean-field Regime](https://arxiv.org/abs/2402.07025) | 该论文在均场极限下提供了一个理论框架，评估了图神经网络在过参数化情况下的泛化误差，通过推导出收敛速度为$O(1/n)$的上界，为我们对网络在未见数据上的性能提供了理论保证。 |
| [^28] | [Classification under Nuisance Parameters and Generalized Label Shift in Likelihood-Free Inference](https://arxiv.org/abs/2402.05330) | 该论文提出了一种在广义标签转移和干扰参数下进行分类的新方法。通过将分类问题视为带有干扰参数的假设检验问题，我们能够有效地估计分类器在整个干扰参数空间中的接收者操作特性，并设计出在广义标签转移下不变的截断点，从而实现了对事件的可靠分类和不确定性度量。 |
| [^29] | [Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications](https://arxiv.org/abs/2402.05162) | 本研究通过修剪和低秩修改，发现大型语言模型（LLMs）的安全机制固有易碎性，去除安全关键区域会损害安全性，但对效用影响不大，需要更强健的安全策略。 |
| [^30] | [Two Trades is not Baffled: Condense Graph via Crafting Rational Gradient Matching](https://arxiv.org/abs/2402.04924) | 本论文提出了一种新颖的图表压缩方法CTRL，通过优化起点和精细的策略，解决了梯度匹配方向导致的训练轨迹偏差和累积误差问题。 |
| [^31] | [CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay](https://arxiv.org/abs/2402.04858) | CodeIt是一种具备优先级回顾重放的自我改进语言模型方法，通过将目标重标记为采样程序的实际输出，有效解决了程序合成中奖励稀疏性的问题，并在抽象和推理语料库（ARC）上实现了成功的跨任务泛化。 |
| [^32] | [A Comprehensive Survey on Graph Reduction: Sparsification, Coarsening, and Condensation](https://arxiv.org/abs/2402.03358) | 这篇综述调研了图缩减方法，包括稀疏化、粗化和浓缩，在解决大型图形数据分析和计算复杂性方面起到了重要作用。调研对这些方法的技术细节进行了系统的回顾，并强调了它们在实际应用中的关键性。同时，调研还提出了保证图缩减技术持续有效性的关键研究方向。 |
| [^33] | [The Benefits of Reusing Batches for Gradient Descent in Two-Layer Networks: Breaking the Curse of Information and Leap Exponents](https://arxiv.org/abs/2402.03220) | 该论文研究了在两层神经网络中学习多指数目标函数时，重复使用批次的梯度下降（GD）的训练动态。研究发现，与单次GD相比，多次GD能够克服目标函数的限制，仅需两个时间步骤就能实现网络与目标子空间的重叠，展示了在有限时间内有效学习的广泛函数类。这些结果基于动力平均场理论（DMFT）的分析。 |
| [^34] | [AdaTreeFormer: Few Shot Domain Adaptation for Tree Counting from a Single High-Resolution Image](https://arxiv.org/abs/2402.02956) | AdaTreeFormer是一种从源领域学习并适应只有有限数量标注树木的目标领域的框架，利用一个共享的编码器和分层特征提取方案，实现了树木计数的少样本领域自适应。 |
| [^35] | [Pruner: An Efficient Cross-Platform Tensor Compiler with Dual Awareness](https://arxiv.org/abs/2402.02361) | Pruner是一种高效跨平台张量编译器，通过参数化静态分析器（PSA）和模式感知成本模型（PaCM）实现张量程序优化，并使用动量转移学习（MTL）策略实现了跨平台适应性。 |
| [^36] | [A Policy Gradient Primal-Dual Algorithm for Constrained MDPs with Uniform PAC Guarantees](https://arxiv.org/abs/2401.17780) | 本文介绍了一种带有均匀PAC保证的策略梯度原始对偶算法，用于在线约束马尔可夫决策过程（CMDP）问题。该算法同时保证了收敛到最优策略、次线性遗憾和多项式样本复杂度，并在实证研究中验证了其优越性能。 |
| [^37] | [Curated LLM: Synergy of LLMs and Data Curation for tabular augmentation in ultra low-data regimes](https://arxiv.org/abs/2312.12112) | 本论文提出了CLLM方法，利用LLMs和数据筛选在低数据环境中进行表格增强。通过利用大型语言模型的先验知识以及基于学习动态、置信度和不确定度指标的筛选机制，CLLM取得了优越的性能。 |
| [^38] | [Connectivity Oracles for Predictable Vertex Failures](https://arxiv.org/abs/2312.08489) | 论文研究了在预测算法范式下设计支持顶点失败的连通性预测器的问题，并提出了一种数据结构，能够以预处理时间和查询时间的多项式关系来处理失败顶点集合。 |
| [^39] | [Plum: Prompt Learning using Metaheuristic](https://arxiv.org/abs/2311.08364) | 提出了使用元启发式的提示学习方法，通过测试六种典型的元启发式方法，在大型语言模型的提示优化任务中取得了有效性。 |
| [^40] | [Interpretable Fine-Tuning for Graph Neural Network Surrogate Models](https://arxiv.org/abs/2311.07548) | 本论文引入了一种可解释的微调策略，通过应用于非结构网格化流体动力学建模的GNNs，增强了模型的预测能力，并通过识别可解释的物理空间区域及其对应的子图，帮助理解模型架构、优化目标和已知物理之间的关系。 |
| [^41] | [Bayesian Regression Markets](https://arxiv.org/abs/2310.14992) | 本论文提出了一种贝叶斯回归市场机制，为数据共享提供了经济激励，并展示了如何缓解市场代理商面临的财务风险。 |
| [^42] | [Does Writing with Language Models Reduce Content Diversity?](https://arxiv.org/abs/2309.05196) | 写作时使用InstructGPT（而不是GPT3）会显著降低内容多样性，增加不同作者之间的相似性，并减少整体的词汇和内容多样性。 |
| [^43] | [Accelerated Algorithms for Constrained Nonconvex-Nonconcave Min-Max Optimization and Comonotone Inclusion](https://arxiv.org/abs/2206.05248) | 本论文提出了针对约束共单调极小-极大优化和共单调包含问题的加速算法，扩展了现有算法并实现了较优的收敛速率，同时证明了算法的收敛性。 |
| [^44] | [Consistent algorithms for multi-label classification with macro-at-$k$ metrics.](http://arxiv.org/abs/2401.16594) | 该研究提出了一种针对多标签分类的一致算法，主要解决了宏观at-$k$度量的优化问题。通过在人口效用框架下考虑复杂性能度量的优化，该算法在极端分类问题中表现出色。 |
| [^45] | [Cross-silo Federated Learning with Record-level Personalized Differential Privacy.](http://arxiv.org/abs/2401.16251) | 本文研究了跨领域联合学习中基于记录级个性化差分隐私的问题，设计了一个名为rPDP-FL的新型框架，并提出了多功能解决方案“模拟-曲线拟合”，以满足不同记录的隐私需求。 |
| [^46] | [Topology-aware Embedding Memory for Learning on Expanding Graphs.](http://arxiv.org/abs/2401.13200) | 这篇论文提出了一个基于拓扑感知嵌入记忆的学习扩展图的框架，该框架可以解决在不断扩展的图上应用记忆回放技术导致的内存爆炸问题。 |
| [^47] | [Frustrated Random Walks: A Fast Method to Compute Node Distances on Hypergraphs.](http://arxiv.org/abs/2401.13054) | 本文提出了一种基于随机游走的方法，用于快速计算超图节点之间的距离并进行标签传播。该方法解决了超图中节点距离计算的问题，进一步拓展了超图的应用领域。 |
| [^48] | [A match made in consistency heaven: when large language models meet evolutionary algorithms.](http://arxiv.org/abs/2401.10510) | 大型语言模型和进化算法的结合具有强大的一致性，包括标记嵌入和基因型-表现型映射、位置编码和适应性塑造、位置嵌入和选择、注意力和交叉、前馈神经网络和突变、模型训练和参数更新以及多任务学习和多目标优化等多个核心特征。本文分析了现有的耦合研究，并为未来的研究提供了基本路线和关键挑战。 |
| [^49] | [Reconciling Spatial and Temporal Abstractions for Goal Representation.](http://arxiv.org/abs/2401.09870) | 本文介绍了一种新的三层分层强化学习算法，引入了空间和时间目标抽象化。研究者提供了学习策略的理论遗憾边界，并在多个任务上对算法进行了评估。 |
| [^50] | [Zero-Shot Position Debiasing for Large Language Models.](http://arxiv.org/abs/2401.01218) | 本文提出了一种零样本位置去偏方法（ZOE）来降低大语言模型（LLMs）的位置偏差问题，该方法利用预训练的LLMs的无监督响应进行去偏。实验证实ZOE在多个数据集和任务中均表现出优异的性能。 |
| [^51] | [LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning.](http://arxiv.org/abs/2311.12023) | LQ-LoRA是一种低秩加量化矩阵分解方法，用于内存高效的语言模型微调。它通过将每个预训练矩阵分解为高精度低秩部分和内存高效的量化部分，实现了动态配置量化参数以及对重构目标进行加权的优化，并在微调实验中表现出了优于QLoRA和GPTQ-LoRA的效果。 |
| [^52] | [Unmasking Bias and Inequities: A Systematic Review of Bias Detection and Mitigation in Healthcare Artificial Intelligence Using Electronic Health Records.](http://arxiv.org/abs/2310.19917) | 本综述对涉及利用电子健康记录数据的医疗人工智能研究中的偏见进行了系统综述，共涵盖了六种主要的偏见类型，同时总结了现有的偏见处理方法。 |
| [^53] | [DCSI -- An improved measure of cluster separability based on separation and connectedness.](http://arxiv.org/abs/2310.12806) | 这篇论文提出了一种改进的聚类可分离性度量方法，旨在量化类间分离和类内连通性，对于密度聚类具有较好的性能表现。 |
| [^54] | [Towards Graph Foundation Models: A Survey and Beyond.](http://arxiv.org/abs/2310.11829) | 本文提出了图基础模型（GFMs）的概念，并对其关键特征和技术进行了全面阐述。同时，将现有GFMs工作分为三个类别，为进一步研究和开发图学习范式奠定了基础。 |
| [^55] | [Partially Observable Stochastic Games with Neural Perception Mechanisms.](http://arxiv.org/abs/2310.11566) | 本研究提出了神经符号化部分可观测随机博弈（NS-POSGs）模型，通过融合感知机制解决了多智能体序列决策中的部分可观测性问题。其中，我们专注于一种只有部分观测信息的智能体和一种完全观测的智能体的单方面设置，并提出了一种近似计算NS-POSGs值的新方法。 |
| [^56] | [Understanding deep neural networks through the lens of their non-linearity.](http://arxiv.org/abs/2310.11439) | 本文提出了一个理论上有效的解决方案，通过亲和度评分追踪深度神经网络中的非线性传播，尤其关注计算机视觉应用。实验证实了所提出方法的实用性和对广泛应用的潜力。 |
| [^57] | [Exploring the Design Space of Diffusion Autoencoders for Face Morphing.](http://arxiv.org/abs/2310.09484) | 这项研究探索了面向人脸变形的扩散自编码器的设计空间，研究了采样算法、逆向DDIM求解器和部分采样的方法。 |
| [^58] | [Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency.](http://arxiv.org/abs/2309.17382) | 提出了一个名为"RAFA"的原则框架，通过在LLM中将推理视为学习和规划的贝叶斯问题，协调推理和行动。通过一个提示模板进行推理，学习并制定未来的轨迹规划，然后在每一步中采取计划轨迹的初始行动并重新规划未来轨迹。这个框架具有可证明的遗憾保证。 |
| [^59] | [Long-term drought prediction using deep neural networks based on geospatial weather data.](http://arxiv.org/abs/2309.06212) | 基于地理气象数据的深度神经网络用于长期干旱预测，提出了一种端到端解决方案来预测特定地区干旱概率。采用卷积LSTM和Transformer模型相比其他基线模型能够获得更高的准确性。 |
| [^60] | [A General Verification Framework for Dynamical and Control Models via Certificate Synthesis.](http://arxiv.org/abs/2309.06090) | 这个论文提出了一个通用的框架来通过证书合成验证动态和控制模型。研究者们提供了一种自动化方法来设计控制器并分析复杂规范。这个方法利用神经网络和SMT求解器来提供候选控制和证书函数，并为控制的安全学习领域做出了贡献。 |
| [^61] | [Affine Invariant Ensemble Transform Methods to Improve Predictive Uncertainty in ReLU Networks.](http://arxiv.org/abs/2309.04742) | 本文提出了一种仿射不变集成变换方法，可以改善在ReLU网络中的预测不确定性。通过基于集合卡尔曼滤波的贝叶斯推断，我们提出了两个相互作用的粒子系统，并证明了它们的收敛性。同时，我们还探讨了这些方法用于量化预测不确定性的有效性。 |
| [^62] | [Fine-tuning can cripple your foundation model; preserving features may be the solution.](http://arxiv.org/abs/2308.13320) | 在微调过程中，基础模型可能会遗忘概念，我们提出了一种名为LDIFS的方法，用于解决这个问题，该方法在实验证明效果显著。 |
| [^63] | [A Two-Part Machine Learning Approach to Characterizing Network Interference in A/B Testing.](http://arxiv.org/abs/2308.09790) | 本论文提出了一种两部分机器学习方法，用于识别和描述A/B测试中的网络干扰。通过考虑潜在的复杂网络结构和建立适合的曝光映射，该方法在合成实验和真实大规模测试中的模拟中表现优于传统方法。 |
| [^64] | [A Deep Learning Approach for Overall Survival Analysis with Missing Values.](http://arxiv.org/abs/2307.11465) | 提出了一个深度学习模型，通过有效利用被审查和未被审查病人的信息，预测非小细胞肺癌（NSCLC）病人的整体生存。 |
| [^65] | [Fast Unsupervised Deep Outlier Model Selection with Hypernetworks.](http://arxiv.org/abs/2307.10529) | 本文提出了HYPER用于调整基于深度神经网络的异常值检测模型，解决了无监督DOD模型中的超参数调整和模型选择的挑战，通过设计和训练超网络(HN)将超参数映射到主要DOD模型的最优权重上。 |
| [^66] | [Uncertainty Quantification for Molecular Property Predictions with Graph Neural Architecture Search.](http://arxiv.org/abs/2307.10438) | 用于分子属性预测的图神经网络方法通常无法量化预测的不确定性，本研究提出了一种自动化的不确定性量化方法AutoGNNUQ，通过架构搜索生成高性能的图神经网络集合，并利用方差分解将数据和模型的不确定性分开，从而提供了减少不确定性的有价值见解。 |
| [^67] | [Inverse Evolution Layers: Physics-informed Regularizers for Deep Neural Networks.](http://arxiv.org/abs/2307.07344) | 本文提出了一种新颖的方法，通过一种新的正则化方法将基于偏微分方程（PDE）的进化模型与神经网络集成在一起。这些层可以实现特定的正则化目标，并赋予神经网络输出与进化模型对应的特性。此外，逆进化层的构建和实现相对简单，可以轻松地为不同的物理进化和神经网络设计。 |
| [^68] | [Generalization Error of First-Order Methods for Statistical Learning with Generic Oracles.](http://arxiv.org/abs/2307.04679) | 本文提出了一种新的框架来分析使用一阶优化算法进行统计学习时的泛化误差，该框架适用于多个学习问题，并且可以推导出紧密匹配的上界和下界。这些结果适用于光滑、强凸和满足Polyak-Lojasiewicz假设的优化问题。 |
| [^69] | [Learning Variational Neighbor Labels for Test-Time Domain Generalization.](http://arxiv.org/abs/2307.04033) | 本文提出了一种用于测试时领域泛化的方法，通过在测试时使用概率伪标签和变分邻居标签来推广源域训练的模型到目标领域，以提高模型的鲁棒性和准确性。 |
| [^70] | [Decomposing Global Feature Effects Based on Feature Interactions.](http://arxiv.org/abs/2306.00541) | 提出了全局效应广义可加分解（GADGET）框架，能够最小化特征交互作用的本地特征效应的交互异质性。同时适用于偏依赖、积累局部效应和Shapley可加解释（SHAP）依赖的边际特征效应可视化方法，并提出了一种新的基于置换的交互测试来检测显着的特征交互作用。 |
| [^71] | [Accelerating Reinforcement Learning with Value-Conditional State Entropy Exploration.](http://arxiv.org/abs/2305.19476) | 本文提出了一种新的探索技术，使用值条件状态熵来解决强化学习中探索不足的问题，可以均衡地覆盖低价值和高价值状态，相较于现有基于熵的探索方法，该方法在MuJoCo基准测试和Atari游戏上有着显著的提升。 |
| [^72] | [SpikeCP: Delay-Adaptive Reliable Spiking Neural Networks via Conformal Prediction.](http://arxiv.org/abs/2305.11322) | 这篇论文提出了一种新的脉冲神经网络模型，能够通过极限预测实现自适应的推断延迟，从而节约能源与提高可靠性。 |
| [^73] | [Bayesian Safety Validation for Black-Box Systems.](http://arxiv.org/abs/2305.02449) | 本文提出了一种名为贝叶斯安全验证的算法，将黑盒安全验证问题转化为贝叶斯优化问题。该算法通过概率代理模型拟合快速预测故障，利用重要性采样估计操作域内的故障概率，从而实现了对高维、危险、计算昂贵的系统的高效估计。 |
| [^74] | [On the convergence of nonlinear averaging dynamics with three-body interactions on hypergraphs.](http://arxiv.org/abs/2304.07203) | 本文研究了超图上具有三体相互作用的离散时间非线性平均动力学，在初态和超图拓扑以及更新非线性相互作用下，产生了高阶动力效应。 |
| [^75] | [Adaptive Regularization for Class-Incremental Learning.](http://arxiv.org/abs/2303.13113) | 本文研究了适应性正则化在类增量学习中的应用，通过根据任务复杂度动态调整正则化强度，在学习新类别同时防止遗忘之前学习的类别。实验表明适应性正则化可以实现更加准确和不易遗忘的视觉增量学习。 |
| [^76] | [Patch-Token Aligned Bayesian Prompt Learning for Vision-Language Models.](http://arxiv.org/abs/2303.09100) | 本文提出了一种基于贝叶斯概率的视觉语言模型提示学习方法，通过将提示标记推向忠实捕捉标签特定的视觉概念，而不是过度拟合训练类别，解决了现有提示工程的问题。在各种视觉语言任务上的广泛实验表明，该方法优于现有的最先进模型。 |
| [^77] | [Stationary Kernels and Gaussian Processes on Lie Groups and their Homogeneous Spaces II: non-compact symmetric spaces.](http://arxiv.org/abs/2301.13088) | 本文开发了构建非欧几里得空间上静止高斯过程的实用技术，能够对定义在这些空间上的先验和后验高斯过程进行实际采样和计算协方差核。 |
| [^78] | [Efficient Estimation for Longitudinal Network via Adaptive Merging.](http://arxiv.org/abs/2211.07866) | 本文提出了一个有效的纵向网络估计框架，利用自适应合并、张量分解和点过程等方法来减少估计偏差和方差。 |
| [^79] | [Contextual Bandits with Packing and Covering Constraints: A Modular Lagrangian Approach via Regression.](http://arxiv.org/abs/2211.07484) | 该论文研究了一种带有资源线性约束的上下文幸存者问题的变种，提出了一种新的算法，该算法简单、计算效率高，同时能够实现较低的后悔。此外，当某些约束被违反时，算法在统计上是最优的。 |
| [^80] | [Explaining the Explainers in Graph Neural Networks: a Comparative Study.](http://arxiv.org/abs/2210.15304) | 该论文研究了图神经网络中的解释方法，并在多种数据集上测试了十种解释器的表现，提供了不同GNN体系结构易解释性的关键洞察。 |
| [^81] | [Learning Symbolic Model-Agnostic Loss Functions via Meta-Learning.](http://arxiv.org/abs/2209.08907) | 本文提出了一种通过元学习框架学习模型无关损失函数的方法，并通过对多个监督学习任务的实验证明，该方法学到的损失函数优于目前最优方法和交叉熵损失函数。 |
| [^82] | [Towards Improving Operation Economics: A Bilevel MIP-Based Closed-Loop Predict-and-Optimize Framework for Prescribing Unit Commitment.](http://arxiv.org/abs/2208.13065) | 本文提出了一个基于双层 MIP 的闭环预测优化框架，使用成本导向的预测器来改进电力系统的经济运行。该框架通过反馈循环迭代地改进预测器，实现了对机组组合的最佳操作。 |
| [^83] | [End-to-End Training for Back-Translation with Categorical Reparameterization Trick.](http://arxiv.org/abs/2202.08465) | 本文提出了一种基于分类重新参数化技巧的回译端到端训练方法，来有效地减少两个神经机器翻译模型间离散属性的影响，从而实现端到端式的训练，获得了比以前基准测试更好的BLEU分数。 |
| [^84] | [Neural Distributed Source Coding.](http://arxiv.org/abs/2106.02797) | 这项研究提出了一种神经分布式源编码的框架，可以处理复杂的相关性并实现最先进的峰值信噪比。 |

# 详细

[^1]: 通过堆叠自动编码器和聚类实现地质制图的遥感框架

    Remote sensing framework for geological mapping via stacked autoencoders and clustering

    [https://arxiv.org/abs/2404.02180](https://arxiv.org/abs/2404.02180)

    通过堆叠自动编码器和聚类实现遥感数据地质制图的无监督机器学习框架

    

    有监督学习方法在遥感地质制图中面临着由于准确标记训练数据的稀缺性而限制的问题。相反，无监督学习方法，如降维和聚类，能够在不依赖预定义标签的情况下揭示遥感数据中的模式和结构。降维方法具有在提高地质图准确性方面发挥关键作用的潜力。虽然传统的降维方法可能在非线性数据上遇到困难，但无监督深度学习模型，如自动编码器，能够模拟数据中的非线性关系。堆叠自动编码器具有多个相互连接的层，用于捕获对遥感数据有用的分层数据表示。在本研究中，我们提出了一个利用堆叠自动编码器和聚类处理遥感数据的无监督机器学习框架。

    arXiv:2404.02180v1 Announce Type: cross  Abstract: Supervised learning methods for geological mapping via remote sensing face limitations due to the scarcity of accurately labelled training data. In contrast, unsupervised learning methods, such as dimensionality reduction and clustering have the ability to uncover patterns and structures in remote sensing data without relying on predefined labels. Dimensionality reduction methods have the potential to play a crucial role in improving the accuracy of geological maps. Although conventional dimensionality reduction methods may struggle with nonlinear data, unsupervised deep learning models such as autoencoders have the ability to model nonlinear relationship in data. Stacked autoencoders feature multiple interconnected layers to capture hierarchical data representations that can be useful for remote sensing data. In this study, we present an unsupervised machine learning framework for processing remote sensing data by utilizing stacked au
    
[^2]: TWIN-GPT: 基于大语言模型的临床试验数字孪生体

    TWIN-GPT: Digital Twins for Clinical Trials via Large Language Model

    [https://arxiv.org/abs/2404.01273](https://arxiv.org/abs/2404.01273)

    提出了基于大语言模型的数字孪生体TWIN-GPT，用于支持临床试验结果预测。

    

    最近，对虚拟临床试验产生了日益增长的兴趣，这些试验模拟了现实世界情境，有望显著增强患者安全性，加快开发速度，降低成本，并为医疗领域的更广泛科学知识贡献力量。本文提出了一种基于大语言模型的数字孪生体TWIN-GPT，用于支持临床试验结果预测。

    arXiv:2404.01273v1 Announce Type: cross  Abstract: Recently, there has been a burgeoning interest in virtual clinical trials, which simulate real-world scenarios and hold the potential to significantly enhance patient safety, expedite development, reduce costs, and contribute to the broader scientific knowledge in healthcare. Existing research often focuses on leveraging electronic health records (EHRs) to support clinical trial outcome prediction. Yet, trained with limited clinical trial outcome data, existing approaches frequently struggle to perform accurate predictions. Some research has attempted to generate EHRs to augment model development but has fallen short in personalizing the generation for individual patient profiles. Recently, the emergence of large language models has illuminated new possibilities, as their embedded comprehensive clinical knowledge has proven beneficial in addressing medical issues. In this paper, we propose a large language model-based digital twin crea
    
[^3]: 在子群体转移下的新颖节点类别检测

    Novel Node Category Detection Under Subpopulation Shift

    [https://arxiv.org/abs/2404.01216](https://arxiv.org/abs/2404.01216)

    提出了一种新方法 RECO-SLIP，用于在属性图中检测属于新类别的节点，能够有效解决子群体转移下的节点检测问题，实验证明其性能优越。

    

    在现实世界的图数据中，分布转移可以通过各种方式表现，例如新类别的出现和现有类别相对比例的变化。在这种分布转移下，检测属于新类别的节点对于安全或洞察发现至关重要。我们引入了一种新方法，称为具有选择性链路预测的召回约束优化（RECO-SLIP），用于在子群体转移下检测属性图中属于新类别的节点。通过将召回约束学习框架与高效样本预测机制相结合，RECO-SLIP解决了抵抗子群体转移和有效利用图结构的双重挑战。我们在多个图数据集上进行了大量实证评估，结果表明RECO-SLIP相对于现有方法具有更优异的性能。

    arXiv:2404.01216v1 Announce Type: new  Abstract: In real-world graph data, distribution shifts can manifest in various ways, such as the emergence of new categories and changes in the relative proportions of existing categories. It is often important to detect nodes of novel categories under such distribution shifts for safety or insight discovery purposes. We introduce a new approach, Recall-Constrained Optimization with Selective Link Prediction (RECO-SLIP), to detect nodes belonging to novel categories in attributed graphs under subpopulation shifts. By integrating a recall-constrained learning framework with a sample-efficient link prediction mechanism, RECO-SLIP addresses the dual challenges of resilience against subpopulation shifts and the effective exploitation of graph structure. Our extensive empirical evaluation across multiple graph datasets demonstrates the superior performance of RECO-SLIP over existing methods.
    
[^4]: AttentionStore: 在大型语言模型服务中实现多轮对话中的注意力成本效益复用

    AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving

    [https://arxiv.org/abs/2403.19708](https://arxiv.org/abs/2403.19708)

    AttentionStore提出了一种新的注意力机制，通过实现KV缓存的复用，在大型语言模型服务中显著降低了多轮对话中的重复计算成本。

    

    通过多轮对话与人类进行交互是大型语言模型（LLMs）的基本特征。然而，由于需要重复计算历史记号的键值（KV）缓存，导致现有用于执行多轮对话的LLM服务引擎效率低下，产生高昂的服务成本。为解决这一问题，本文提出了AttentionStore，一种新的注意力机制，实现了跨多轮对话的KV缓存复用（即 注意力复用），显著降低了重复计算开销。AttentionStore维护了一个层次结构的KV缓存系统，利用成本效益的内存/存储介质为所有请求保存KV缓存。为了减少慢速介质的KV缓存访问开销，AttentionStore采用逐层预加载和异步保存方案，将KV缓存访问与GPU计算重叠。为确保要访问的KV缓存…

    arXiv:2403.19708v1 Announce Type: new  Abstract: Interacting with humans through multi-turn conversations is a fundamental feature of large language models (LLMs). However, existing LLM serving engines for executing multi-turn conversations are inefficient due to the need to repeatedly compute the key-value (KV) caches of historical tokens, incurring high serving costs. To address the problem, this paper proposes AttentionStore, a new attention mechanism that enables the reuse of KV caches (i.e., attention reuse) across multi-turn conversations, significantly reducing the repetitive computation overheads. AttentionStore maintains a hierarchical KV caching system that leverages cost-effective memory/storage mediums to save KV caches for all requests. To reduce KV cache access overheads from slow mediums, AttentionStore employs layer-wise pre-loading and asynchronous saving schemes to overlap the KV cache access with the GPU computation. To ensure that the KV caches to be accessed are pl
    
[^5]: 首次在阿根廷使用机器学习技术识别糖尿病风险人群的初步体验

    First Experiences with the Identification of People at Risk for Diabetes in Argentina using Machine Learning Techniques

    [https://arxiv.org/abs/2403.18631](https://arxiv.org/abs/2403.18631)

    在本研究中，针对阿根廷地区的情况，发展和评估了用于识别2型糖尿病（T2D）和糖尿病前期（PD）风险人群的预测模型，并展示了部分模型在两个特定数据集上取得了非常好的性能。

    

    识别2型糖尿病（T2D）和糖尿病前期（PD）对医学是一个真正的挑战，因为缺乏病原性症状和已知相关危险因素。尽管有些机器学习模型的提议使得识别患病风险的人成为可能，但这种病症的性质使得一个适用于一种人群的模型未必适用于另一种人群。本文讨论了在阿根廷特别发展和评估用于识别T2D和PD风险人群的预测模型。首先，数据库经过彻底预处理，生成了三个特定的数据集，考虑到记录数和可用变量的权衡。应用了5种不同的分类模型后，结果显示其中一些模型在两个数据集上表现出很好的性能。特别地，RF、DT和ANN展现了很大的表现。

    arXiv:2403.18631v1 Announce Type: new  Abstract: Detecting Type 2 Diabetes (T2D) and Prediabetes (PD) is a real challenge for medicine due to the absence of pathogenic symptoms and the lack of known associated risk factors. Even though some proposals for machine learning models enable the identification of people at risk, the nature of the condition makes it so that a model suitable for one population may not necessarily be suitable for another. In this article, the development and assessment of predictive models to identify people at risk for T2D and PD specifically in Argentina are discussed. First, the database was thoroughly preprocessed and three specific datasets were generated considering a compromise between the number of records and the amount of available variables. After applying 5 different classification models, the results obtained show that a very good performance was observed for two datasets with some of these models. In particular, RF, DT, and ANN demonstrated great c
    
[^6]: LLM生成代码的水印技术是否具有鲁棒性？

    Is Watermarking LLM-Generated Code Robust?

    [https://arxiv.org/abs/2403.17983](https://arxiv.org/abs/2403.17983)

    该研究探讨了现有水印技术在大型语言模型生成的Python代码上的鲁棒性，发现容易通过保留语义转换来移除这些水印。

    

    我们首次研究了现有水印技术在大型语言模型生成的Python代码上的鲁棒性。尽管现有作品表明水印技术对自然语言具有鲁棒性，但我们发现通过保留语义的转换很容易移除代码上的这些水印。

    arXiv:2403.17983v1 Announce Type: cross  Abstract: We present the first study of the robustness of existing watermarking techniques on Python code generated by large language models. Although existing works showed that watermarking can be robust for natural language, we show that it is easy to remove these watermarks on code by semantic-preserving transformations.
    
[^7]: 通过将数据转化为与偏见正交的方式实现反事实公平性

    Counterfactual Fairness through Transforming Data Orthogonal to Bias

    [https://arxiv.org/abs/2403.17852](https://arxiv.org/abs/2403.17852)

    提出了一种新颖的数据预处理算法，正交于偏见（OB），通过确保数据与敏感变量不相关，实现机器学习应用中的反事实公平性。

    

    机器学习模型在解决各个领域的复杂问题中展现出了卓越的能力。然而，这些模型有时可能表现出有偏见的决策，导致不同群体之间的待遇不平等。尽管公平性方面的研究已经很广泛，但多元连续敏感变量对决策结果的微妙影响尚未得到充分研究。我们引入了一种新颖的数据预处理算法，即正交于偏见（OB），旨在消除连续敏感变量的影响，从而促进机器学习应用中的反事实公平性。我们的方法基于结构因果模型（SCM）中联合正态分布的假设，证明了通过确保数据与敏感变量不相关即可实现反事实公平性。OB算法与模型无关，适用于多种机器学习应用。

    arXiv:2403.17852v1 Announce Type: new  Abstract: Machine learning models have shown exceptional prowess in solving complex issues across various domains. Nonetheless, these models can sometimes exhibit biased decision-making, leading to disparities in treatment across different groups. Despite the extensive research on fairness, the nuanced effects of multivariate and continuous sensitive variables on decision-making outcomes remain insufficiently studied. We introduce a novel data pre-processing algorithm, Orthogonal to Bias (OB), designed to remove the influence of a group of continuous sensitive variables, thereby facilitating counterfactual fairness in machine learning applications. Our approach is grounded in the assumption of a jointly normal distribution within a structural causal model (SCM), proving that counterfactual fairness can be achieved by ensuring the data is uncorrelated with sensitive variables. The OB algorithm is model-agnostic, catering to a wide array of machine 
    
[^8]: 具有内存和切换成本的在线非凸优化问题的容量调配动机

    Capacity Provisioning Motivated Online Non-Convex Optimization Problem with Memory and Switching Cost

    [https://arxiv.org/abs/2403.17480](https://arxiv.org/abs/2403.17480)

    该论文考虑了一种在线非凸优化问题，目标是通过调节活动服务器数量最小化作业延迟，引入了切换成本，提出了竞争算法。

    

    考虑了一种在线非凸优化问题，其目标是通过调节活动服务器的数量来最小化一组作业的流量时间（总延迟），但在时间变化时改变活动服务器数量会产生切换成本。每个作业在任何时间内最多可以由一个固定速度的服务器处理。与通常具有切换成本的在线凸优化（OCO）问题相比，所考虑的目标函数是非凸的，并且更重要的是，在每个时间点，它取决于所有过去的决策，而不仅仅是当前的决策。考虑了最坏情况和随机输入；对于这两种情况，提出了竞争算法。

    arXiv:2403.17480v1 Announce Type: cross  Abstract: An online non-convex optimization problem is considered where the goal is to minimize the flow time (total delay) of a set of jobs by modulating the number of active servers, but with a switching cost associated with changing the number of active servers over time. Each job can be processed by at most one fixed speed server at any time. Compared to the usual online convex optimization (OCO) problem with switching cost, the objective function considered is non-convex and more importantly, at each time, it depends on all past decisions and not just the present one. Both worst-case and stochastic inputs are considered; for both cases, competitive algorithms are derived.
    
[^9]: 面向目标导向语义通信的动态相对表示

    Dynamic Relative Representations for Goal-Oriented Semantic Communications

    [https://arxiv.org/abs/2403.16986](https://arxiv.org/abs/2403.16986)

    本文提出了一个新颖的面向目标的语义通信框架，利用相对表示通过潜在空间对齐来缓解语义不匹配，并实现了能效高、延迟低的目标导向语义通信。

    

    在未来的6G无线网络中，通信的语义和有效性方面将发挥基础作用，将含义和相关性纳入传输。然而，当设备使用不同的语言、逻辑或内部表示时，会出现障碍，导致语义不匹配，可能危及理解。在潜在空间通信中，这一挑战表现为深度神经网络对数据进行编码时高维表示不匹配。本文提出了一个新颖的面向目标的语义通信框架，利用相对表示来通过潜在空间对齐缓解语义不匹配。我们提出了一种动态优化策略，以调整相对表示、通信参数和计算资源，实现能效高、延迟低的目标导向语义通信。数值结果证明了我们的方法在缓解中起作用的有效性。

    arXiv:2403.16986v1 Announce Type: cross  Abstract: In future 6G wireless networks, semantic and effectiveness aspects of communications will play a fundamental role, incorporating meaning and relevance into transmissions. However, obstacles arise when devices employ diverse languages, logic, or internal representations, leading to semantic mismatches that might jeopardize understanding. In latent space communication, this challenge manifests as misalignment within high-dimensional representations where deep neural networks encode data. This paper presents a novel framework for goal-oriented semantic communication, leveraging relative representations to mitigate semantic mismatches via latent space alignment. We propose a dynamic optimization strategy that adapts relative representations, communication parameters, and computation resources for energy-efficient, low-latency, goal-oriented semantic communications. Numerical results demonstrate our methodology's effectiveness in mitigating
    
[^10]: CONLINE: 复杂代码生成与在线搜索和正确性测试的精炼

    CONLINE: Complex Code Generation and Refinement with Online Searching and Correctness Testing

    [https://arxiv.org/abs/2403.13583](https://arxiv.org/abs/2403.13583)

    CONLINE框架提出了通过在线搜索和正确性测试来增强复杂代码生成的方法，通过实验证明了其显著提高了代码生成质量。

    

    大型语言模型（LLMs）通过将自然语言描述转换为可执行代码，彻底改变了代码生成能力。然而，在真实场景下生成复杂代码仍然具有挑战性，原因在于复杂的结构、微妙的错误、对高级数据类型的理解以及缺少辅助内容。为了解决这些挑战，我们引入了CONLINE框架，通过计划的在线搜索信息检索和自动正确性测试来增强代码生成，进行迭代精炼。CONLINE还串行化了复杂的输入和输出，以改善理解，并生成测试用例，确保框架适用于现实应用。CONLINE通过对DS-1000和ClassEval数据集进行严格实验验证。结果表明，CONLINE显著提高了复杂代码生成的质量，突显了其提升实践应用潜力。

    arXiv:2403.13583v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have revolutionized code generation ability by converting natural language descriptions into executable code. However, generating complex code within real-world scenarios remains challenging due to intricate structures, subtle bugs, understanding of advanced data types, and lack of supplementary contents. To address these challenges, we introduce the CONLINE framework, which enhances code generation by incorporating planned online searches for information retrieval and automated correctness testing for iterative refinement. CONLINE also serializes the complex inputs and outputs to improve comprehension and generate test case to ensure the framework's adaptability for real-world applications. CONLINE is validated through rigorous experiments on the DS-1000 and ClassEval datasets. It shows that CONLINE substantially improves the quality of complex code generation, highlighting its potential to enhance the pra
    
[^11]: 用梯度下降训练形态神经网络：一些理论见解

    Training morphological neural networks with gradient descent: some theoretical insights

    [https://arxiv.org/abs/2403.12975](https://arxiv.org/abs/2403.12975)

    形态神经网络的训练存在挑战，本文通过使用基于梯度下降的优化算法，探讨了基于微分方法和反向传播对形态网络的潜力和局限性，提供了关于初始化和学习率的理论指导。

    

    形态神经网络或层可以成为提升数学形态学进展的强大工具，无论是在理论方面，如完整格算子的表示，还是在图像处理流程的开发方面。然而，当这些架构包含多层形态学时，至少在使用基于梯度下降的优化算法的流行机器学习框架内，这些网络很难进行训练。在本文中，我们探讨了基于微分方法和反向传播应用于形态网络的潜力和局限性，考虑到Bouligand导数的非光滑优化概念。我们提供了见解和首个理论指南，特别是关于初始化和学习率。

    arXiv:2403.12975v1 Announce Type: cross  Abstract: Morphological neural networks, or layers, can be a powerful tool to boost the progress in mathematical morphology, either on theoretical aspects such as the representation of complete lattice operators, or in the development of image processing pipelines. However, these architectures turn out to be difficult to train when they count more than a few morphological layers, at least within popular machine learning frameworks which use gradient descent based optimization algorithms. In this paper we investigate the potential and limitations of differentiation based approaches and back-propagation applied to morphological networks, in light of the non-smooth optimization concept of Bouligand derivative. We provide insights and first theoretical guidelines, in particular regarding initialization and learning rates.
    
[^12]: 正则化驱动的深度状态空间模型中的高效持续学习

    Regularization-Based Efficient Continual Learning in Deep State-Space Models

    [https://arxiv.org/abs/2403.10123](https://arxiv.org/abs/2403.10123)

    提出了一种正则化驱动的深度状态空间模型，实现了高效的持续学习，能够在多个动态系统建模时进行有效更新，并且通过实验证实了其有效性

    

    最近几年，由于其对动态系统具有强大的建模能力，深度状态空间模型（DSSMs）已经变得越来越受欢迎。然而，现有的DSSM工作局限于单任务建模，这需要在重新访问之前的任务时利用历史任务数据进行重新训练。为了解决这一局限性，我们提出了持续学习DSSMs（CLDSSMs），能够适应不断变化的任务而不会发生灾难性遗忘。我们提出的CLDSSMs集成了主流基于正则化的持续学习（CL）方法，确保在对多个动态系统建模时高效更新，保持不变的计算和内存成本。我们还对应用于各自CLDSSMs的每种CL方法进行了全面的成本分析，并通过对真实世界数据集的实验来展示CLDSSMs的有效性。结果证实，虽然各种竞争的CL方法具有不同的优点，但所提出的CLDSSMs始终保持一致。

    arXiv:2403.10123v1 Announce Type: new  Abstract: Deep state-space models (DSSMs) have gained popularity in recent years due to their potent modeling capacity for dynamic systems. However, existing DSSM works are limited to single-task modeling, which requires retraining with historical task data upon revisiting a forepassed task. To address this limitation, we propose continual learning DSSMs (CLDSSMs), which are capable of adapting to evolving tasks without catastrophic forgetting. Our proposed CLDSSMs integrate mainstream regularization-based continual learning (CL) methods, ensuring efficient updates with constant computational and memory costs for modeling multiple dynamic systems. We also conduct a comprehensive cost analysis of each CL method applied to the respective CLDSSMs, and demonstrate the efficacy of CLDSSMs through experiments on real-world datasets. The results corroborate that while various competing CL methods exhibit different merits, the proposed CLDSSMs consistentl
    
[^13]: 通过稀疏插值专家释放元调整的力量，用于少样本泛化

    Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts

    [https://arxiv.org/abs/2403.08477](https://arxiv.org/abs/2403.08477)

    本文提出了一种名为Sparse MetA-Tuning（SMAT）的方法，通过灵感来自稀疏专家混合方法，成功克服了域外任务敏感性，实现了增强视觉基础模型转移能力的目标。

    

    传统智慧建议参数高效的微调基础模型，是视觉迁移学习的最先进方法，取代了诸如元学习之类的丰富文献。为了兼顾两者的利益，元调整引入了基础模型的随后优化阶段，但迄今只展现了有限的成功，关键地在域外（OOD）任务上表现不佳。本文介绍了一种灵感来自稀疏专家混合方法的 Sparse MetA-Tuning（SMAT）方法，它经过训练以自动地为每个任务隔离预训练参数子集以进行元调整。SMAT成功克服了OOD敏感性，并实现了增强视觉基础模型转移能力的承诺。我们在Meta-Dataset与额外的OO挑战组合上建立了新的最先进结果。

    arXiv:2403.08477v1 Announce Type: cross  Abstract: Conventional wisdom suggests parameter-efficient fine-tuning of foundation models as the state-of-the-art method for transfer learning in vision, replacing the rich literature of alternatives such as meta-learning. In trying to harness the best of both worlds, meta-tuning introduces a subsequent optimization stage of foundation models but has so far only shown limited success and crucially tends to underperform on out-of-domain (OOD) tasks. In this paper, we introduce Sparse MetA-Tuning (SMAT), a method inspired by sparse mixture-of-experts approaches and trained to isolate subsets of pre-trained parameters automatically for meta-tuning on each task. SMAT successfully overcomes OOD sensitivity and delivers on the promise of enhancing the transfer abilities of vision foundation models beyond parameter-efficient finetuning. We establish new state-of-the-art results on a challenging combination of Meta-Dataset augmented with additional OO
    
[^14]: 挑战遗忘：揭示机器遗忘中最坏情况遗忘集

    Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine Unlearning

    [https://arxiv.org/abs/2403.07362](https://arxiv.org/abs/2403.07362)

    该论文从对抗的角度提出了一种新的机器遗忘评估方法，通过确定最具挑战性的数据子集，即最坏情况遗忘集，来增强对影响擦除的挑战。

    

    靠谱的机器学习(Machine Learning, ML)社区越来越认识到模型在训练后有选择性地“遗忘”数据点的重要性。这引出了机器遗忘(Machine Unlearning, MU)问题，旨在消除选定数据点对模型性能的影响，同时仍保持模型在遗忘后的实用性。尽管有各种MU方法来擦除数据影响，评估主要集中在随机数据遗忘上，忽视了对于真实衡量遗忘性能的数据子集选择的重要探究。为解决这一问题，我们从对抗的角度引入了一种新的MU评估视角。我们提出确定那些对影响擦除构成最大挑战的数据子集，即找出最坏情况遗忘集。利用双层优化原则，我们增强了在上层优化中的遗忘挑战。

    arXiv:2403.07362v1 Announce Type: cross  Abstract: The trustworthy machine learning (ML) community is increasingly recognizing the crucial need for models capable of selectively 'unlearning' data points after training. This leads to the problem of machine unlearning (MU), aiming to eliminate the influence of chosen data points on model performance, while still maintaining the model's utility post-unlearning. Despite various MU methods for data influence erasure, evaluations have largely focused on random data forgetting, ignoring the vital inquiry into which subset should be chosen to truly gauge the authenticity of unlearning performance. To tackle this issue, we introduce a new evaluative angle for MU from an adversarial viewpoint. We propose identifying the data subset that presents the most significant challenge for influence erasure, i.e., pinpointing the worst-case forget set. Utilizing a bi-level optimization principle, we amplify unlearning challenges at the upper optimization 
    
[^15]: CO2在自然通风学校建筑物中用于占用检测的空间特征

    Spatial features of CO2 for occupancy detection in a naturally ventilated school building

    [https://arxiv.org/abs/2403.06643](https://arxiv.org/abs/2403.06643)

    通过空间CO2浓度分布提出两种新特征，使用支持向量机进行量化分析后发现，与基准相比，在自然通风房间中的占用状态检测准确率最多可提高14.8个百分点，达到83.2％（F1分数0.84）。有通风信息的情况下，准确率达到87.6％。

    

    精确的占用信息有助于提高建筑能源效率和居住者舒适度。基于CO2传感器的占用检测方法由于成本低、干扰小，受到关注。在自然通风建筑中，由于复杂的通风行为和测量窗户实际换气量的困难，基于CO2的占用检测准确性通常较低。本研究提出了基于CO2浓度的空间分布的两种新颖特征用于占用检测。通过使用支持向量机（SVM）作为分类器的量化分析，发现与基准相比，在没有任何通风信息的情况下，自然通风房间中的占用状态检测准确率可提高最多14.8个百分点，达到83.2％（F1分数0.84）。有通风信息的情况下，准确率达到87.6％（F1分数0. ．84）。

    arXiv:2403.06643v1 Announce Type: new  Abstract: Accurate occupancy information helps to improve building energy efficiency and occupant comfort. Occupancy detection methods based on CO2 sensors have received attention due to their low cost and low intrusiveness. In naturally ventilated buildings, the accuracy of CO2-based occupancy detection is generally low in related studies due to the complex ventilation behavior and the difficulty in measuring the actual air exchange through windows. In this study, we present two novel features for occupancy detection based on the spatial distribution of the CO2 concentration. After a quantitative analysis with Support Vector Machine (SVM) as classifier, it was found that the accuracy of occupancy state detection in naturally ventilated rooms could be improved by up to 14.8 percentage points compared to the baseline, reaching 83.2 % (F1 score 0.84) without any ventilation information. With ventilation information, the accuracy reached 87.6 % (F1 s
    
[^16]: 一个镜子的库：低维深度神经网络是具有反射特征的凸Lasso模型

    A Library of Mirrors: Deep Neural Nets in Low Dimensions are Convex Lasso Models with Reflection Features

    [https://arxiv.org/abs/2403.01046](https://arxiv.org/abs/2403.01046)

    证明在1-D数据上训练神经网络等价于解决一个具有固定特征字典矩阵的凸Lasso问题，为全局最优网络和解空间提供了洞察。

    

    我们证明在1-D数据上训练神经网络等价于解决一个带有固定、明确定义的特征字典矩阵的凸Lasso问题。具体的字典取决于激活函数和深度。我们考虑具有分段线性激活函数的两层网络，深窄的ReLU网络最多有4层，以及具有符号激活和任意深度的矩形和树网络。有趣的是，在ReLU网络中，第四层创建代表训练数据关于自身的反射的特征。Lasso表示法揭示了全局最优网络和解空间的洞察。

    arXiv:2403.01046v1 Announce Type: cross  Abstract: We prove that training neural networks on 1-D data is equivalent to solving a convex Lasso problem with a fixed, explicitly defined dictionary matrix of features. The specific dictionary depends on the activation and depth. We consider 2-layer networks with piecewise linear activations, deep narrow ReLU networks with up to 4 layers, and rectangular and tree networks with sign activation and arbitrary depth. Interestingly in ReLU networks, a fourth layer creates features that represent reflections of training data about themselves. The Lasso representation sheds insight to globally optimal networks and the solution landscape.
    
[^17]: 扩展流匹配：具有广义连续性方程的条件生成方法

    Extended Flow Matching: a Method of Conditional Generation with Generalized Continuity Equation

    [https://arxiv.org/abs/2402.18839](https://arxiv.org/abs/2402.18839)

    本文基于Flow Matching发展了条件生成理论，通过使用广义连续性方程的数学框架而非流匹配中的连续性方程，实现了一种新颖的流基条件分布生成方法。

    

    条件生成任务是生成模型中最重要的应用之一，迄今为止已经开发了许多基于著名扩散模型的方法，其中以基于引导的无分类器方法为首。然而，基于引导的方法的理论不仅要求用户微调“引导强度”，而且其目标向量场不一定对应于训练中使用的条件分布。本文基于流匹配发展了条件生成理论，流匹配是扩散方法的当前强大竞争者之一。受将概率路径解释为路径空间上的分布的启发，我们建立了一个新颖的流基条件分布生成理论，通过使用广义连续性方程的数学框架而不是流匹配中的连续性方程。这一理论自然地推导出一种方法

    arXiv:2402.18839v1 Announce Type: new  Abstract: The task of conditional generation is one of the most important applications of generative models, and numerous methods have been developed to date based on the celebrated diffusion models, with the guidance-based classifier-free method taking the lead. However, the theory of the guidance-based method not only requires the user to fine-tune the "guidance strength," but its target vector field does not necessarily correspond to the conditional distribution used in training. In this paper, we develop the theory of conditional generation based on Flow Matching, a current strong contender of diffusion methods. Motivated by the interpretation of a probability path as a distribution on path space, we establish a novel theory of flow-based generation of conditional distribution by employing the mathematical framework of generalized continuity equation instead of the continuity equation in flow matching. This theory naturally derives a method th
    
[^18]: 如果在一个众包数据标注管道中，GPT-4

    If in a Crowdsourced Data Annotation Pipeline, a GPT-4

    [https://arxiv.org/abs/2402.16795](https://arxiv.org/abs/2402.16795)

    本文比较了 GPT-4 和 MTurk 管道的数据标注准确性，发现尽管 MTurk 采用了最佳实践，但 GPT-4 的准确率更高，并且结合 GPT-4 和众包标签使用聚合算法可以提高准确率。

    

    最近的研究表明GPT-4在数据标注准确性方面优于在线众包工作者，尤其是来自亚马逊机械土耳其（MTurk）的工作者。然而，这些研究因偏离标准众包实践并强调个别工作者的表现而受到批评，而不是整个数据标注过程。本文比较了GPT-4和一个道德且执行良好的MTurk管道，使用415名工作者标注了来自200篇学术文章的3,177个句段，使用了CODA-19方案。两个工作者界面产生了127,080个标签，然后通过八种标签聚合算法推断出最终的标签。我们的评估结果显示，尽管采用了最佳实践，MTurk管道的最高准确率为81.5%，而GPT-4达到了83.6%。有趣的是，当将GPT-4的标签与通过先进工作者界面收集的众包标签结合起来进行聚合时，8种算法中有2种实现了更高的准确率。

    arXiv:2402.16795v1 Announce Type: cross  Abstract: Recent studies indicated GPT-4 outperforms online crowd workers in data labeling accuracy, notably workers from Amazon Mechanical Turk (MTurk). However, these studies were criticized for deviating from standard crowdsourcing practices and emphasizing individual workers' performances over the whole data-annotation process. This paper compared GPT-4 and an ethical and well-executed MTurk pipeline, with 415 workers labeling 3,177 sentence segments from 200 scholarly articles using the CODA-19 scheme. Two worker interfaces yielded 127,080 labels, which were then used to infer the final labels through eight label-aggregation algorithms. Our evaluation showed that despite best practices, MTurk pipeline's highest accuracy was 81.5%, whereas GPT-4 achieved 83.6%. Interestingly, when combining GPT-4's labels with crowd labels collected via an advanced worker interface for aggregation, 2 out of the 8 algorithms achieved an even higher accuracy (
    
[^19]: 成本意识最佳臂识别

    Cost Aware Best Arm Identification

    [https://arxiv.org/abs/2402.16710](https://arxiv.org/abs/2402.16710)

    本文研究了一个带有成本分布的最佳臂识别问题，提出了CABAI方法以实现最小期望成本下识别出最大奖励臂，并设计了$\mathsf{CTAS}$和CO两种算法来逼近理论下限并优化计算复杂度。

    

    在这篇论文中，我们研究了一个带有双重对象的最佳臂识别问题。除了传统的奖励外，每个臂还与成本分布相关联，目标是使用最小期望成本识别出最大奖励臂。我们称之为“成本意识最佳臂识别”（CABAI），它捕捉了产品开发流程中测试和实施阶段之间的分离，并模拟了阶段之间的目标转变，即测试的成本和实施的奖励。我们首先为CABAI推导了一个理论下限，并提出了一个名为$\mathsf{CTAS}$的算法来渐近匹配它。为了减少$\mathsf{CTAS}$的计算量，我们进一步提出了一个基于平方根规则的低复杂度算法称为CO，在简化的双臂模型中证明了其最优性，并在数值实验中表现出惊人的泛化能力。

    arXiv:2402.16710v1 Announce Type: cross  Abstract: In this paper, we study a best arm identification problem with dual objects. In addition to the classic reward, each arm is associated with a cost distribution and the goal is to identify the largest reward arm using the minimum expected cost. We call it \emph{Cost Aware Best Arm Identification} (CABAI), which captures the separation of testing and implementation phases in product development pipelines and models the objective shift between phases, i.e., cost for testing and reward for implementation. We first derive an theoretic lower bound for CABAI and propose an algorithm called $\mathsf{CTAS}$ to match it asymptotically. To reduce the computation of $\mathsf{CTAS}$, we further propose a low-complexity algorithm called CO, based on a square-root rule, which proves optimal in simplified two-armed models and generalizes surprisingly well in numerical experiments. Our results show (i) ignoring the heterogeneous action cost results in 
    
[^20]: 使用平衡截断技术学习带有对角状态空间层的S4模型的方法

    Learning method for S4 with Diagonal State Space Layers using Balanced Truncation

    [https://arxiv.org/abs/2402.15993](https://arxiv.org/abs/2402.15993)

    一种用于处理长序列数据的边缘智能应用的S4模型的学习方法，利用平衡截断技术降低计算成本，并通过改进初始化过程和优化准确度和效率指标来超越传统训练模型。

    

    我们引入了一种新颖的学习方法，用于结构化状态空间序列（S4）模型并且加入了对角状态空间（DSS）层，这种方法专门设计用于处理边缘智能应用中的长序列数据，包括传感器数据分析和实时分析。该方法利用平衡截断技术，在控制理论中很常见，特别应用于DSS层以降低推断过程中的计算成本。通过利用减少模型的参数，我们改进了S4模型的初始化过程，在性能方面优于广泛使用的Skew-HiPPo初始化。数值实验表明，我们训练的带有DSS层的S4模型在准确度和效率指标上超越了传统训练的模型。此外，我们的观察结果显示一个积极的相关性：原始模型中的更高准确度一致导致使用我们方法训练的模型的准确度增加，这表明

    arXiv:2402.15993v1 Announce Type: new  Abstract: We introduce a novel learning method for Structured State Space Sequence (S4) models incorporating Diagonal State Space (DSS) layers, tailored for processing long-sequence data in edge intelligence applications, including sensor data analysis and real-time analytics. This method utilizes the balanced truncation technique, prevalent in control theory, applied specifically to DSS layers to reduce computational costs during inference. By leveraging parameters from the reduced model, we refine the initialization process of S4 models, outperforming the widely used Skew-HiPPo initialization in terms of performance. Numerical experiments demonstrate that our trained S4 models with DSS layers surpass conventionally trained models in accuracy and efficiency metrics. Furthermore, our observations reveal a positive correlation: higher accuracy in the original model consistently leads to increased accuracy in models trained using our method, suggest
    
[^21]: 将生成式人工智能引入教育中的自适应学习

    Bringing Generative AI to Adaptive Learning in Education

    [https://arxiv.org/abs/2402.14601](https://arxiv.org/abs/2402.14601)

    生成式人工智能技术与自适应学习概念的交叉研究将对教育中下一阶段学习格式的发展做出重要贡献。

    

    最近生成式人工智能技术的激增，如大型语言模型和扩散模型，推动了人工智能在科学、金融和教育等各个领域的应用发展。与此同时，自适应学习这一概念在教育领域引起了极大关注，并证明其在提高学生学习效率方面的有效性。在本立场论文中，我们旨在探讨将生成式人工智能与自适应学习概念结合起来的交叉研究。通过讨论这一领域的好处、挑战和潜力，我们认为这种结合将为教育中下一阶段学习形式的发展做出重要贡献。

    arXiv:2402.14601v1 Announce Type: cross  Abstract: The recent surge in generative AI technologies, such as large language models and diffusion models, have boosted the development of AI applications in various domains, including science, finance, and education. Concurrently, adaptive learning, a concept that has gained substantial interest in the educational sphere, has proven its efficacy in enhancing students' learning efficiency. In this position paper, we aim to shed light on the intersectional studies of these two methods, which combine generative AI with adaptive learning concepts. By presenting discussions about the benefits, challenges, and potentials in this field, we argue that this union will contribute significantly to the development of the next stage learning format in education.
    
[^22]: 复兴多变量时间序列预测：可学习分解与跨系列依赖关系和内部变化建模

    Revitalizing Multivariate Time Series Forecasting: Learnable Decomposition with Inter-Series Dependencies and Intra-Series Variations Modeling

    [https://arxiv.org/abs/2402.12694](https://arxiv.org/abs/2402.12694)

    引入可学习分解策略和双注意力模块，同时捕捉跨系列依赖和内部变化，以应对复杂的多变量时间序列预测挑战。

    

    预测多变量时间序列是至关重要的，要求精确建模错综复杂模式，包括跨时间序列的依赖关系和内部变动。每个时间序列具有独特的趋势特征带来挑战，现有方法依赖基本的移动平均核可能难以处理现实数据中的非线性结构和复杂趋势。基于此，我们引入了一个可学习的分解策略，更合理地捕捉动态趋势信息。此外，我们提出了一个双注意力模块，专门用于同时捕捉跨系列依赖关系和内部变化，以实现更好的时间序列预测，其中通过通道自注意力和自回归自注意力实现。为了评估我们方法的有效性，我们在八个开源数据集上进行了实验，并将其与最先进的方法进行了比较。通过比较结果，我们的 Leddam...

    arXiv:2402.12694v1 Announce Type: new  Abstract: Predicting multivariate time series is crucial, demanding precise modeling of intricate patterns, including inter-series dependencies and intra-series variations. Distinctive trend characteristics in each time series pose challenges, and existing methods, relying on basic moving average kernels, may struggle with the non-linear structure and complex trends in real-world data. Given that, we introduce a learnable decomposition strategy to capture dynamic trend information more reasonably. Additionally, we propose a dual attention module tailored to capture inter-series dependencies and intra-series variations simultaneously for better time series forecasting, which is implemented by channel-wise self-attention and autoregressive self-attention. To evaluate the effectiveness of our method, we conducted experiments across eight open-source datasets and compared it with the state-of-the-art methods. Through the comparison results, our Leddam
    
[^23]: 在符号化多步推理任务上训练的Transformer的机理分析

    A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step Reasoning Task

    [https://arxiv.org/abs/2402.11917](https://arxiv.org/abs/2402.11917)

    对在合成推理任务上训练的Transformer进行的机理分析揭示其实现了一个在并行运行的有界深度循环机制，并将中间结果存储在选定的令牌位置

    

    Transformer在一系列推理基准测试中展现出令人印象深刻的性能。为了评估这些能力在多大程度上是实际推理的结果，现有工作集中于开发复杂的行为研究基准。然而，这些研究并未提供关于驱动观察到的能力的内部机制的见解。为了改善我们对Transformer内部机制的理解，我们提出了对一个在合成推理任务上训练的Transformer进行全面的机理分析。我们确定了模型用来解决任务的一组可解释机制，并利用相关和因果证据验证了我们的发现。我们的结果表明，它实现了一个在并行运行的有界深度循环机制，并将中间结果存储在选定的令牌位置。我们预期我们在我们的合成环境中识别的主题可以提供有价值的见解

    arXiv:2402.11917v1 Announce Type: new  Abstract: Transformers demonstrate impressive performance on a range of reasoning benchmarks. To evaluate the degree to which these abilities are a result of actual reasoning, existing work has focused on developing sophisticated benchmarks for behavioral studies. However, these studies do not provide insights into the internal mechanisms driving the observed capabilities. To improve our understanding of the internal mechanisms of transformers, we present a comprehensive mechanistic analysis of a transformer trained on a synthetic reasoning task. We identify a set of interpretable mechanisms the model uses to solve the task, and validate our findings using correlational and causal evidence. Our results suggest that it implements a depth-bounded recurrent mechanisms that operates in parallel and stores intermediate results in selected token positions. We anticipate that the motifs we identified in our synthetic setting can provide valuable insights
    
[^24]: UniST：一种为城市时空预测设计的提示增强型通用模型

    UniST: A Prompt-Empowered Universal Model for Urban Spatio-Temporal Prediction

    [https://arxiv.org/abs/2402.11838](https://arxiv.org/abs/2402.11838)

    UniST是一种为城市时空预测设计的通用模型，通过灵活性、有效的生成式预训练以及丰富的掩码策略成功捕捉复杂的时空关系。

    

    arXiv:2402.11838v1 公告类型：新的 摘要：城市时空预测对于决策至关重要，例如交通管理、资源优化和城市规划。尽管自然语言的预训练基础模型取得了显著突破，其中一个通用模型可以处理跨多个领域的多个任务，但城市时空建模落后。现有的城市预测方法通常针对特定的时空场景进行定制，需要特定任务的模型设计和大量域内训练数据。在这项工作中，我们提出了一种用于城市时空预测的通用模型UniST。借鉴自大型语言模型，UniST通过以下方式取得成功：(i) 对不同空间时间数据特征的灵活性，(ii) 有效的生成式预训练，采用精心设计的掩码策略来捕捉复杂的空间时间关系，(iii) 时空知识。

    arXiv:2402.11838v1 Announce Type: new  Abstract: Urban spatio-temporal prediction is crucial for informed decision-making, such as transportation management, resource optimization, and urban planning. Although pretrained foundation models for natural languages have experienced remarkable breakthroughs, wherein one general-purpose model can tackle multiple tasks across various domains, urban spatio-temporal modeling lags behind. Existing approaches for urban prediction are usually tailored for specific spatio-temporal scenarios, requiring task-specific model designs and extensive in-domain training data. In this work, we propose a universal model, UniST, for urban spatio-temporal prediction. Drawing inspiration from large language models, UniST achieves success through: (i) flexibility towards diverse spatio-temporal data characteristics, (ii) effective generative pre-training with elaborated masking strategies to capture complex spatio-temporal relationships, (iii) spatio-temporal know
    
[^25]: 将预训练语言模型与物理层通信集成

    Integrating Pre-Trained Language Model with Physical Layer Communications

    [https://arxiv.org/abs/2402.11656](https://arxiv.org/abs/2402.11656)

    提出了一个集成了物理层通信功能的实用设备间人工智能通信框架，通过端到端训练结合信道噪声以增强韧性，采用VQ-VAE实现高效稳健的通信，利用预训练Transformer提升通用性能

    

    在设备间人工智能通信的新兴领域中，设备直接通过嵌入式基础模型（如语言模型）交换信息，需要强大、高效且通用的通信框架。然而，将这些框架与现有无线系统集成并有效管理噪声和比特误差都面临着重大挑战。在本研究中，我们介绍了一个实用的设备间人工智能通信框架，集成了物理层通信功能，并通过链路级模拟器展示了其性能。我们的框架通过端到端训练结合信道噪声以增强韧性，采用向量量化变分自动编码器（VQ-VAE）实现高效稳健的通信，利用预训练编码-解码Transformer提升通用性能。在各种通信场景的模拟中，我们的框架展现出

    arXiv:2402.11656v1 Announce Type: cross  Abstract: The burgeoning field of on-device AI communication, where devices exchange information directly through embedded foundation models, such as language models (LMs), requires robust, efficient, and generalizable communication frameworks. However, integrating these frameworks with existing wireless systems and effectively managing noise and bit errors pose significant challenges. In this work, we introduce a practical on-device AI communication framework, integrated with physical layer (PHY) communication functions, demonstrated through its performance on a link-level simulator. Our framework incorporates end-to-end training with channel noise to enhance resilience, incorporates vector quantized variational autoencoders (VQ-VAE) for efficient and robust communication, and utilizes pre-trained encoder-decoder transformers for improved generalization capabilities. Simulations, across various communication scenarios, reveal that our framework
    
[^26]: 恢复生成模型的预微调权重

    Recovering the Pre-Fine-Tuning Weights of Generative Models

    [https://arxiv.org/abs/2402.10208](https://arxiv.org/abs/2402.10208)

    该论文提出了一种恢复生成模型预微调权重的方法，通过少量低秩微调模型可以恢复准确的预微调权重，利用这个新漏洞攻击大规模模型。

    

    在生成建模中，主流模式包括两个步骤：i) 在大规模但不安全的数据集上进行预训练，ii) 通过微调将预训练模型与人类价值观对齐。这种做法被认为是安全的，因为目前没有一种方法可以恢复不安全的预微调模型权重。本文证明了这种假设通常是错误的。具体而言，我们提出了一种称为谱反调的方法，可以使用少量低秩（LoRA）微调模型恢复预微调模型的权重。与先前试图恢复预微调能力的攻击不同，我们的方法旨在恢复精确的预微调权重。我们的方法利用了这个新的对大规模模型的漏洞，例如个性化的稳定扩散和对齐的Mistral模型。

    arXiv:2402.10208v1 Announce Type: cross  Abstract: The dominant paradigm in generative modeling consists of two steps: i) pre-training on a large-scale but unsafe dataset, ii) aligning the pre-trained model with human values via fine-tuning. This practice is considered safe, as no current method can recover the unsafe, pre-fine-tuning model weights. In this paper, we demonstrate that this assumption is often false. Concretely, we present Spectral DeTuning, a method that can recover the weights of the pre-fine-tuning model using a few low-rank (LoRA) fine-tuned models. In contrast to previous attacks that attempt to recover pre-fine-tuning capabilities, our method aims to recover the exact pre-fine-tuning weights. Our approach exploits this new vulnerability against large-scale models such as a personalized Stable Diffusion and an aligned Mistral.
    
[^27]: 均场极限下图神经网络的泛化误差

    Generalization Error of Graph Neural Networks in the Mean-field Regime

    [https://arxiv.org/abs/2402.07025](https://arxiv.org/abs/2402.07025)

    该论文在均场极限下提供了一个理论框架，评估了图神经网络在过参数化情况下的泛化误差，通过推导出收敛速度为$O(1/n)$的上界，为我们对网络在未见数据上的性能提供了理论保证。

    

    该工作提供了一个理论框架，用于评估在过参数化的情况下通过图神经网络进行图分类任务的泛化误差，即参数数量超过数据点数量的情况。我们研究了两种广泛使用的图神经网络类型：图卷积神经网络和消息传递图神经网络。在本研究之前，关于过参数化情况下泛化误差的现有界限缺乏信息，限制了我们对过参数化网络性能的理解。我们的创新方法是在均场极限下推导出上界，以评估这些图神经网络的泛化误差。我们建立了以$O(1/n)$收敛速度的上界，其中$n$是图样本的数量。这些上界为在具有挑战性的过参数化情况下网络在未见数据上的性能提供了理论上的保证，从而对我们的理解做出了贡献。

    This work provides a theoretical framework for assessing the generalization error of graph classification tasks via graph neural networks in the over-parameterized regime, where the number of parameters surpasses the quantity of data points. We explore two widely utilized types of graph neural networks: graph convolutional neural networks and message passing graph neural networks. Prior to this study, existing bounds on the generalization error in the over-parametrized regime were uninformative, limiting our understanding of over-parameterized network performance. Our novel approach involves deriving upper bounds within the mean-field regime for evaluating the generalization error of these graph neural networks. We establish upper bounds with a convergence rate of $O(1/n)$, where $n$ is the number of graph samples. These upper bounds offer a theoretical assurance of the networks' performance on unseen data in the challenging over-parameterized regime and overall contribute to our under
    
[^28]: 在无拟样似推断的干扰参数和广义标签转移下的分类问题

    Classification under Nuisance Parameters and Generalized Label Shift in Likelihood-Free Inference

    [https://arxiv.org/abs/2402.05330](https://arxiv.org/abs/2402.05330)

    该论文提出了一种在广义标签转移和干扰参数下进行分类的新方法。通过将分类问题视为带有干扰参数的假设检验问题，我们能够有效地估计分类器在整个干扰参数空间中的接收者操作特性，并设计出在广义标签转移下不变的截断点，从而实现了对事件的可靠分类和不确定性度量。

    

    在我们的训练数据和目标数据之间，标签和潜在干扰参数的分布不同的情况下，如何以可靠的不确定性度量对事件进行分类是一个科学挑战。我们将这种分布转移称为广义标签转移 (GLS)。直接使用观测数据 $\mathbf{X}$ 进行分类会导致预测结果偏差和标签 $Y$ 的无效不确定性估计。我们通过将分类问题视为带有干扰参数的假设检验问题来克服这些偏差。关键思想是在整个干扰参数空间中估计分类器的接收者操作特性 (ROC)，这使我们能够设计在 GLS 下不变的截断点。我们的方法有效地赋予预训练的分类器领域适应能力，并返回有效的预测集合，同时保持有效的不确定性估计。

    An open scientific challenge is how to classify events with reliable measures of uncertainty, when we have a mechanistic model of the data-generating process but the distribution over both labels and latent nuisance parameters is different between train and target data. We refer to this type of distributional shift as generalized label shift (GLS). Direct classification using observed data $\mathbf{X}$ as covariates leads to biased predictions and invalid uncertainty estimates of labels $Y$. We overcome these biases by proposing a new method for robust uncertainty quantification that casts classification as a hypothesis testing problem under nuisance parameters. The key idea is to estimate the classifier's receiver operating characteristic (ROC) across the entire nuisance parameter space, which allows us to devise cutoffs that are invariant under GLS. Our method effectively endows a pre-trained classifier with domain adaptation capabilities and returns valid prediction sets while maint
    
[^29]: 通过修剪和低秩修改评估安全对齐的易碎性

    Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications

    [https://arxiv.org/abs/2402.05162](https://arxiv.org/abs/2402.05162)

    本研究通过修剪和低秩修改，发现大型语言模型（LLMs）的安全机制固有易碎性，去除安全关键区域会损害安全性，但对效用影响不大，需要更强健的安全策略。

    

    大型语言模型（LLMs）在其安全机制方面表现出固有的易碎性，这可从它们易受越狱和即使是非恶意微调也易受影响来说明。本研究通过利用修剪和低秩修改探讨了安全对齐的易碎性。我们开发了方法，能够识别对于安全防护至关重要，且在神经元和秩级别上与效用相关的区域。令人惊讶的是，我们发现的孤立区域是稀疏的，约占参数级别的$3\%$和排名级别的$2.5\%$。去除这些区域会损害安全性，而对效用的影响不大，从而证实了该模型安全机制的固有易碎性。此外，我们还表明，即使限制对安全关键区域进行修改，LLMs仍然容易受到低成本的微调攻击。这些发现强调了在LLMs中更强大的安全策略的紧迫性需求。

    Large language models (LLMs) show inherent brittleness in their safety mechanisms, as evidenced by their susceptibility to jailbreaking and even non-malicious fine-tuning. This study explores this brittleness of safety alignment by leveraging pruning and low-rank modifications. We develop methods to identify critical regions that are vital for safety guardrails, and that are disentangled from utility-relevant regions at both the neuron and rank levels. Surprisingly, the isolated regions we find are sparse, comprising about $3\%$ at the parameter level and $2.5\%$ at the rank level. Removing these regions compromises safety without significantly impacting utility, corroborating the inherent brittleness of the model's safety mechanisms. Moreover, we show that LLMs remain vulnerable to low-cost fine-tuning attacks even when modifications to the safety-critical regions are restricted. These findings underscore the urgent need for more robust safety strategies in LLMs.
    
[^30]: 两个交易不会困扰：通过构造合理的梯度匹配来压缩图表

    Two Trades is not Baffled: Condense Graph via Crafting Rational Gradient Matching

    [https://arxiv.org/abs/2402.04924](https://arxiv.org/abs/2402.04924)

    本论文提出了一种新颖的图表压缩方法CTRL，通过优化起点和精细的策略，解决了梯度匹配方向导致的训练轨迹偏差和累积误差问题。

    

    在大规模图表上训练已经在图表表示学习方面取得了显著成果，但其成本和存储引起了越来越多的关注。作为最有前景的方向之一，图表压缩方法通过使用梯度匹配来解决这些问题，目标是将完整的图表压缩成更简洁但信息丰富的合成集。尽管令人鼓舞，但这些策略主要强调梯度的匹配方向，从而导致训练轨迹的偏差。这种偏差进一步由压缩和评估阶段之间的差异放大，最终导致累积误差，对压缩图表的性能产生不利影响。鉴于此，我们提出了一种名为\textbf{C}raf\textbf{T}ing \textbf{R}ationa\textbf{L} trajectory（\textbf{CTRL}）的新型图表压缩方法，它提供了一个更接近原始数据集特征分布的优化起点和一个更精细的策略。

    Training on large-scale graphs has achieved remarkable results in graph representation learning, but its cost and storage have raised growing concerns. As one of the most promising directions, graph condensation methods address these issues by employing gradient matching, aiming to condense the full graph into a more concise yet information-rich synthetic set. Though encouraging, these strategies primarily emphasize matching directions of the gradients, which leads to deviations in the training trajectories. Such deviations are further magnified by the differences between the condensation and evaluation phases, culminating in accumulated errors, which detrimentally affect the performance of the condensed graphs. In light of this, we propose a novel graph condensation method named \textbf{C}raf\textbf{T}ing \textbf{R}ationa\textbf{L} trajectory (\textbf{CTRL}), which offers an optimized starting point closer to the original dataset's feature distribution and a more refined strategy for 
    
[^31]: CodeIt：具有优先级回顾重放的自我改进语言模型

    CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay

    [https://arxiv.org/abs/2402.04858](https://arxiv.org/abs/2402.04858)

    CodeIt是一种具备优先级回顾重放的自我改进语言模型方法，通过将目标重标记为采样程序的实际输出，有效解决了程序合成中奖励稀疏性的问题，并在抽象和推理语料库（ARC）上实现了成功的跨任务泛化。

    

    大型语言模型越来越能够解决通常被认为需要人类水平推理能力的任务。然而，这些模型在通用智能基准测试例如抽象和推理语料库（ARC）上表现仍然非常差。在本文中，我们将ARC视为一个以编程示例为基础的问题，并引入了一种名为Code Iteration（CodeIt）的新颖且可扩展的语言模型自我改进方法。我们的方法在1）程序抽样和回顾重标记以及2）基于优先级的经验回放之间进行迭代。通过将一个episode的目标（即给定输入的目标程序输出）重标记为采样程序产生的实际输出，我们的方法有效地处理了程序合成中奖励极度稀疏性的问题。应用CodeIt于ARC数据集，我们证明了优先级回顾重放、预训练和数据增强可以实现成功的跨任务泛化。CodeIt是第一个神经元-合成机制一体的自我改进语言模型方法。

    Large language models are increasingly solving tasks that are commonly believed to require human-level reasoning ability. However, these models still perform very poorly on benchmarks of general intelligence such as the Abstraction and Reasoning Corpus (ARC). In this paper, we approach ARC as a programming-by-examples problem, and introduce a novel and scalable method for language model self-improvement called Code Iteration (CodeIt). Our method iterates between 1) program sampling and hindsight relabeling, and 2) learning from prioritized experience replay. By relabeling the goal of an episode (i.e., the target program output given input) to the realized output produced by the sampled program, our method effectively deals with the extreme sparsity of rewards in program synthesis. Applying CodeIt to the ARC dataset, we demonstrate that prioritized hindsight replay, along with pre-training and data-augmentation, leads to successful inter-task generalization. CodeIt is the first neuro-sy
    
[^32]: 图缩减的综合调研：稀疏化、粗化和浓缩

    A Comprehensive Survey on Graph Reduction: Sparsification, Coarsening, and Condensation

    [https://arxiv.org/abs/2402.03358](https://arxiv.org/abs/2402.03358)

    这篇综述调研了图缩减方法，包括稀疏化、粗化和浓缩，在解决大型图形数据分析和计算复杂性方面起到了重要作用。调研对这些方法的技术细节进行了系统的回顾，并强调了它们在实际应用中的关键性。同时，调研还提出了保证图缩减技术持续有效性的关键研究方向。

    

    许多真实世界的数据集可以自然地表示为图，涵盖了广泛的领域。然而，图数据集的复杂性和规模的增加为分析和计算带来了显著的挑战。为此，图缩减技术在保留关键属性的同时简化大型图形数据变得越来越受关注。在本调研中，我们旨在提供对图缩减方法的全面理解，包括图稀疏化、图粗化和图浓缩。具体而言，我们建立了这些方法的统一定义，并引入了一个分层分类法来分类这些方法所解决的挑战。我们的调研系统地回顾了这些方法的技术细节，并强调了它们在各种场景中的实际应用。此外，我们还概述了保证图缩减技术持续有效性的关键研究方向，并提供了一个详细的论文列表链接。

    Many real-world datasets can be naturally represented as graphs, spanning a wide range of domains. However, the increasing complexity and size of graph datasets present significant challenges for analysis and computation. In response, graph reduction techniques have gained prominence for simplifying large graphs while preserving essential properties. In this survey, we aim to provide a comprehensive understanding of graph reduction methods, including graph sparsification, graph coarsening, and graph condensation. Specifically, we establish a unified definition for these methods and introduce a hierarchical taxonomy to categorize the challenges they address. Our survey then systematically reviews the technical details of these methods and emphasizes their practical applications across diverse scenarios. Furthermore, we outline critical research directions to ensure the continued effectiveness of graph reduction techniques, as well as provide a comprehensive paper list at https://github.
    
[^33]: 重复使用批次在两层网络的梯度下降中的好处：打破信息和跳跃指数的诅咒

    The Benefits of Reusing Batches for Gradient Descent in Two-Layer Networks: Breaking the Curse of Information and Leap Exponents

    [https://arxiv.org/abs/2402.03220](https://arxiv.org/abs/2402.03220)

    该论文研究了在两层神经网络中学习多指数目标函数时，重复使用批次的梯度下降（GD）的训练动态。研究发现，与单次GD相比，多次GD能够克服目标函数的限制，仅需两个时间步骤就能实现网络与目标子空间的重叠，展示了在有限时间内有效学习的广泛函数类。这些结果基于动力平均场理论（DMFT）的分析。

    

    本研究探讨了学习多指数目标函数时，两层神经网络的训练动态。我们关注重复多次使用批次的多次梯度下降（GD），并展示它与单次梯度下降相比，显著改变了对于哪些函数是可学习的的结论。具体而言，我们发现具有有限步长的多次GD能够克服目标函数的信息指数（Ben Arous等人，2021）和跳跃指数（Abbe等人，2023）所给出的梯度流和单次GD的限制。我们发现，通过重复使用批次，网络仅需两个时间步骤就能与目标子空间达成重叠，即使函数不满足阶梯性质（Abbe等人，2021）。我们对能够在有限时间内有效学习的（广泛的）函数类进行了表征。我们的结果证明基于动力平均场理论（DMFT）的分析。我们进一步提供了动态的闭式描述。

    We investigate the training dynamics of two-layer neural networks when learning multi-index target functions. We focus on multi-pass gradient descent (GD) that reuses the batches multiple times and show that it significantly changes the conclusion about which functions are learnable compared to single-pass gradient descent. In particular, multi-pass GD with finite stepsize is found to overcome the limitations of gradient flow and single-pass GD given by the information exponent (Ben Arous et al., 2021) and leap exponent (Abbe et al., 2023) of the target function. We show that upon re-using batches, the network achieves in just two time steps an overlap with the target subspace even for functions not satisfying the staircase property (Abbe et al., 2021). We characterize the (broad) class of functions efficiently learned in finite time. The proof of our results is based on the analysis of the Dynamical Mean-Field Theory (DMFT). We further provide a closed-form description of the dynamica
    
[^34]: AdaTreeFormer: 从一张高分辨率图像中进行树木计数的少样本领域自适应

    AdaTreeFormer: Few Shot Domain Adaptation for Tree Counting from a Single High-Resolution Image

    [https://arxiv.org/abs/2402.02956](https://arxiv.org/abs/2402.02956)

    AdaTreeFormer是一种从源领域学习并适应只有有限数量标注树木的目标领域的框架，利用一个共享的编码器和分层特征提取方案，实现了树木计数的少样本领域自适应。

    

    仅使用一张航空或卫星图像来估计和计数树木密度是摄影测量和遥感领域中一项困难的任务。然而，它在森林管理中起着至关重要的作用。不同地形上各种各样的树木种类严重阻碍了树木计数模型的良好表现。本文旨在提出一个从具有足够标注树木的源领域学习并适应只有有限数量标注树木的目标领域的框架。我们的方法称为AdaTreeFormer，包含一个共享的编码器和一个分层特征提取方案，用于从源领域和目标领域中提取稳健的特征。它还包括三个子网络：两个用于分别从源领域和目标领域提取自注意力图，并一个用于提取跨领域注意力图。对于后者，引入了一种注意力适应机制，用于从不同领域中提取相关信息。

    The process of estimating and counting tree density using only a single aerial or satellite image is a difficult task in the fields of photogrammetry and remote sensing. However, it plays a crucial role in the management of forests. The huge variety of trees in varied topography severely hinders tree counting models to perform well. The purpose of this paper is to propose a framework that is learnt from the source domain with sufficient labeled trees and is adapted to the target domain with only a limited number of labeled trees. Our method, termed as AdaTreeFormer, contains one shared encoder with a hierarchical feature extraction scheme to extract robust features from the source and target domains. It also consists of three subnets: two for extracting self-domain attention maps from source and target domains respectively and one for extracting cross-domain attention maps. For the latter, an attention-to-adapt mechanism is introduced to distill relevant information from different doma
    
[^35]: Pruner:一种具有双重感知能力的高效跨平台张量编译器

    Pruner: An Efficient Cross-Platform Tensor Compiler with Dual Awareness

    [https://arxiv.org/abs/2402.02361](https://arxiv.org/abs/2402.02361)

    Pruner是一种高效跨平台张量编译器，通过参数化静态分析器（PSA）和模式感知成本模型（PaCM）实现张量程序优化，并使用动量转移学习（MTL）策略实现了跨平台适应性。

    

    对深度学习加速器（DLAs）上的张量程序优化对于有效的模型部署至关重要。虽然基于搜索的深度学习编译器（DLC）与手动方法相比取得了显著的性能提升，但仍然面临着搜索效率低和跨平台适应性差的挑战。在本文中，我们提出了Pruner，遵循硬件/软件协同设计原则来分层提升张量程序优化。Pruner由两个主要组件组成：参数化静态分析器（PSA）和模式感知成本模型（PaCM）。前者作为一种硬件感知和公式化的性能分析工具，引导搜索空间的修剪，而后者根据关键的数据流模式实现了对张量程序的性能预测。此外，为了保证有效的跨平台适应性，我们设计了一个动量转移学习（MTL）策略。

    Tensor program optimization on Deep Learning Accelerators (DLAs) is critical for efficient model deployment. Although search-based Deep Learning Compilers (DLCs) have achieved significant performance gains compared to manual methods, they still suffer from the persistent challenges of low search efficiency and poor cross-platform adaptability. In this paper, we propose $\textbf{Pruner}$, following hardware/software co-design principles to hierarchically boost tensor program optimization. Pruner comprises two primary components: a Parameterized Static Analyzer ($\textbf{PSA}$) and a Pattern-aware Cost Model ($\textbf{PaCM}$). The former serves as a hardware-aware and formulaic performance analysis tool, guiding the pruning of the search space, while the latter enables the performance prediction of tensor programs according to the critical data-flow patterns. Furthermore, to ensure effective cross-platform adaptation, we design a Momentum Transfer Learning ($\textbf{MTL}$) strategy using
    
[^36]: 一种带有均匀PAC保证的约束MDPs的策略梯度原始对偶算法

    A Policy Gradient Primal-Dual Algorithm for Constrained MDPs with Uniform PAC Guarantees

    [https://arxiv.org/abs/2401.17780](https://arxiv.org/abs/2401.17780)

    本文介绍了一种带有均匀PAC保证的策略梯度原始对偶算法，用于在线约束马尔可夫决策过程（CMDP）问题。该算法同时保证了收敛到最优策略、次线性遗憾和多项式样本复杂度，并在实证研究中验证了其优越性能。

    

    我们研究了一种基于原始对偶强化学习算法的在线约束马尔可夫决策过程（CMDP）问题，其中代理探索的最优策略在满足约束的同时最大化回报。尽管在实践中被广泛使用，但现有的理论文献仅提供次线性遗憾保证，并未能确保收敛到最优策略。在本文中，我们引入了一种新颖的带有均匀可能近似正确性（Uniform-PAC）保证的策略梯度原始对偶算法，同时确保收敛到最优策略、次线性遗憾和多项式样本复杂度以实现任何目标精度。值得注意的是，这是在线CMDP问题的第一个Uniform-PAC算法。除了理论保证外，我们还在一个简单的CMDP中进行了实证研究，证明了我们的算法收敛到最优策略，而现有算法表现出振荡性能表现。

    We study a primal-dual reinforcement learning (RL) algorithm for the online constrained Markov decision processes (CMDP) problem, wherein the agent explores an optimal policy that maximizes return while satisfying constraints. Despite its widespread practical use, the existing theoretical literature on primal-dual RL algorithms for this problem only provides sublinear regret guarantees and fails to ensure convergence to optimal policies. In this paper, we introduce a novel policy gradient primal-dual algorithm with uniform probably approximate correctness (Uniform-PAC) guarantees, simultaneously ensuring convergence to optimal policies, sublinear regret, and polynomial sample complexity for any target accuracy. Notably, this represents the first Uniform-PAC algorithm for the online CMDP problem. In addition to the theoretical guarantees, we empirically demonstrate in a simple CMDP that our algorithm converges to optimal policies, while an existing algorithm exhibits oscillatory perform
    
[^37]: LLM精选：在超低数据环境中利用LLMs和数据筛选进行表格增强

    Curated LLM: Synergy of LLMs and Data Curation for tabular augmentation in ultra low-data regimes

    [https://arxiv.org/abs/2312.12112](https://arxiv.org/abs/2312.12112)

    本论文提出了CLLM方法，利用LLMs和数据筛选在低数据环境中进行表格增强。通过利用大型语言模型的先验知识以及基于学习动态、置信度和不确定度指标的筛选机制，CLLM取得了优越的性能。

    

    低数据情况下的机器学习（ML）仍然是一个被低估但至关重要的问题。因此，增加ML所需的数据样本大小的数据增强方法对于释放ML在数据匮乏的地区和领域的变革潜力至关重要。不幸的是，有限的训练集限制了传统的表格合成数据生成器在生成ML任务所需的大规模且多样化的增强数据集方面的能力。为了解决这个挑战，我们引入了CLLM，它利用大型语言模型（LLMs）在低数据环境中进行数据增强的先验知识。然而，像任何生成模型一样，并非LLMs生成的所有数据都能提高下游的效用。因此，我们引入了一种基于学习动态、置信度和不确定度指标的原则性筛选机制，以获取高质量的数据集。通过多个真实世界数据集的实证，我们展示了CLLM在低数据环境中的优越性能。

    Machine Learning (ML) in low-data settings remains an underappreciated yet crucial problem. Hence, data augmentation methods to increase the sample size of datasets needed for ML are key to unlocking the transformative potential of ML in data-deprived regions and domains. Unfortunately, the limited training set constrains traditional tabular synthetic data generators in their ability to generate a large and diverse augmented dataset needed for ML tasks. To address this challenge, we introduce CLLM, which leverages the prior knowledge of Large Language Models (LLMs) for data augmentation in the low-data regime. However, not all the data generated by LLMs will improve downstream utility, as for any generative model. Consequently, we introduce a principled curation mechanism, leveraging learning dynamics, coupled with confidence and uncertainty metrics, to obtain a high-quality dataset. Empirically, on multiple real-world datasets, we demonstrate the superior performance of CLLM in the lo
    
[^38]: 预测顶点失败的连通性预测器

    Connectivity Oracles for Predictable Vertex Failures

    [https://arxiv.org/abs/2312.08489](https://arxiv.org/abs/2312.08489)

    论文研究了在预测算法范式下设计支持顶点失败的连通性预测器的问题，并提出了一种数据结构，能够以预处理时间和查询时间的多项式关系来处理失败顶点集合。

    

    设计支持顶点失败的连通性预测器是针对无向图的基本数据结构问题之一。已有的研究在查询时间方面已经有了很好的理解：以前的作品[Duan-Pettie STOC'10; Long-Saranurak FOCS'22]实现了与失败顶点数量成线性关系的查询时间，并且在需要多项式时间的预处理和多项式时间的更新的条件下是有条件最优的。我们在预测算法的范式下重新审视了这个问题：我们问，如果可以预测到失败顶点集合，查询时间是否可以提高。更具体地说，我们设计了一个数据结构，给定一个图G=(V,E)和一个预测会失败的顶点集合\widehat{D} \subseteq V（其中d=|\widehat{D}|），将其预处理时间为$\tilde{O}(d|E|)$，然后可以接收一个更新，该更新以对称差分形式给出。

    arXiv:2312.08489v2 Announce Type: replace-cross  Abstract: The problem of designing connectivity oracles supporting vertex failures is one of the basic data structures problems for undirected graphs. It is already well understood: previous works [Duan--Pettie STOC'10; Long--Saranurak FOCS'22] achieve query time linear in the number of failed vertices, and it is conditionally optimal as long as we require preprocessing time polynomial in the size of the graph and update time polynomial in the number of failed vertices.   We revisit this problem in the paradigm of algorithms with predictions: we ask if the query time can be improved if the set of failed vertices can be predicted beforehand up to a small number of errors. More specifically, we design a data structure that, given a graph $G=(V,E)$ and a set of vertices predicted to fail $\widehat{D} \subseteq V$ of size $d=|\widehat{D}|$, preprocesses it in time $\tilde{O}(d|E|)$ and then can receive an update given as the symmetric differ
    
[^39]: Plum: 使用元启发式的提示学习

    Plum: Prompt Learning using Metaheuristic

    [https://arxiv.org/abs/2311.08364](https://arxiv.org/abs/2311.08364)

    提出了使用元启发式的提示学习方法，通过测试六种典型的元启发式方法，在大型语言模型的提示优化任务中取得了有效性。

    

    自从大型语言模型出现以来，提示学习已成为优化和定制这些模型的一种流行方法。特殊提示，如“思维链”，甚至揭示了这些模型内部先前未知的推理能力。然而，发现有效提示的进展缓慢，促使人们渴望一种通用的提示优化方法。不幸的是，现有的提示学习方法中很少有满足“通用”的标准，即同时具备自动、离散、黑盒、无梯度和可解释性。在本文中，我们引入元启发式，作为一种有希望的提示学习方法的离散非凸优化方法分支，拥有100多种选项。在我们的范式中，我们测试了六种典型方法：爬山、模拟退火、遗传算法（带/不带交叉）、禁忌搜索和和谐搜索，展示了它们在白盒模式下的有效性。

    arXiv:2311.08364v2 Announce Type: replace-cross  Abstract: Since the emergence of large language models, prompt learning has become a popular method for optimizing and customizing these models. Special prompts, such as Chain-of-Thought, have even revealed previously unknown reasoning capabilities within these models. However, the progress of discovering effective prompts has been slow, driving a desire for general prompt optimization methods. Unfortunately, few existing prompt learning methods satisfy the criteria of being truly "general", i.e., automatic, discrete, black-box, gradient-free, and interpretable all at once. In this paper, we introduce metaheuristics, a branch of discrete non-convex optimization methods with over 100 options, as a promising approach to prompt learning. Within our paradigm, we test six typical methods: hill climbing, simulated annealing, genetic algorithms with/without crossover, tabu search, and harmony search, demonstrating their effectiveness in white-b
    
[^40]: 可解释的图神经网络替代模型微调

    Interpretable Fine-Tuning for Graph Neural Network Surrogate Models

    [https://arxiv.org/abs/2311.07548](https://arxiv.org/abs/2311.07548)

    本论文引入了一种可解释的微调策略，通过应用于非结构网格化流体动力学建模的GNNs，增强了模型的预测能力，并通过识别可解释的物理空间区域及其对应的子图，帮助理解模型架构、优化目标和已知物理之间的关系。

    

    数据驱动的替代建模随着图神经网络（GNNs）的出现在最近几年内蓬勃发展，GNNs可以直接在基于网格的数据表示上运行。这项工作的目标是为GNN引入一种可解释的微调策略，应用于非结构网格化流体动力学建模。最终结果是一个增强的微调模型，它隔离了与预测任务密切相关的物理空间区域，相应于子图，同时保留了基线的预测能力。这些由微调的GNN识别出的结构在前向传递中是自适应生成的，并作为可解释的链接存在于基线模型架构、优化目标和已知问题特定物理之间。此外，通过正则化程序，微调的GNNs还可以在推断期间用于识别对应的图节点。

    arXiv:2311.07548v2 Announce Type: replace  Abstract: Data-driven surrogate modeling has surged in capability in recent years with the emergence of graph neural networks (GNNs), which can operate directly on mesh-based representations of data. The goal of this work is to introduce an interpretable fine-tuning strategy for GNNs, with application to unstructured mesh-based fluid dynamics modeling. The end result is an enhanced fine-tuned model that isolates regions in physical space, corresponding to sub-graphs, that are intrinsically linked to the forecasting task while retaining the predictive capability of the baseline. These structures, identified by the fine-tuned GNNs, are adaptively produced in the forward pass and serve as explainable links between the baseline model architecture, the optimization goal, and known problem-specific physics. Additionally, through a regularization procedure, the fine-tuned GNNs can also be used to identify, during inference, graph nodes that correspon
    
[^41]: 贝叶斯回归市场

    Bayesian Regression Markets

    [https://arxiv.org/abs/2310.14992](https://arxiv.org/abs/2310.14992)

    本论文提出了一种贝叶斯回归市场机制，为数据共享提供了经济激励，并展示了如何缓解市场代理商面临的财务风险。

    

    机器学习任务对所使用的数据质量很敏感。然而，公司往往很难获得足够的数据集，因为这些数据自然分布在各个所有者之间，而这些所有者在实践中可能是竞争对手，不愿意共享信息。我们针对回归任务的监督学习，开发了一个回归市场，以提供数据共享的经济激励。我们提出的机制采用贝叶斯框架，使我们能够考虑更一般的回归任务类别。我们对市场属性进行了彻底探讨，并展示了目前文献中类似提议暴露市场代理商面临可观的财务风险，而这些风险在我们的设置中可以得到缓解。

    arXiv:2310.14992v2 Announce Type: replace  Abstract: Machine learning tasks are vulnerable to the quality of data used as input. Yet, it is often challenging for firms to obtain adequate datasets, with them being naturally distributed amongst owners, that in practice, may be competitors in a downstream market and reluctant to share information. Focusing on supervised learning for regression tasks, we develop a regression market to provide a monetary incentive for data sharing. Our proposed mechanism adopts a Bayesian framework, allowing us to consider a more general class of regression tasks. We present a thorough exploration of the market properties, and show that similar proposals in current literature expose the market agents to sizeable financial risks, which can be mitigated in our setup.
    
[^42]: 语言模型写作是否会降低内容多样性？

    Does Writing with Language Models Reduce Content Diversity?

    [https://arxiv.org/abs/2309.05196](https://arxiv.org/abs/2309.05196)

    写作时使用InstructGPT（而不是GPT3）会显著降低内容多样性，增加不同作者之间的相似性，并减少整体的词汇和内容多样性。

    

    大型语言模型（LLMs）引发了与模型辅助合作写作的激增。当不同用户纳入同一模型的建议时，会存在内容多样性减少的风险，可能限制公共话语中的多元观点。本研究通过控制实验测量了协同写作对多样性的影响，在该实验中，用户以三种设置撰写议论性文章--使用基本LLM（GPT3）、经过反馈调整的LLM（InstructGPT）以及不使用模型帮助写作。我们开发了一组多样性指标，并发现使用InstructGPT进行写作（而不是GPT3）会导致多样性明显降低。具体而言，它增加了不同作者的写作之间的相似性，减少了整体的词汇和内容多样性。此外，我们还发现这种影响主要来源于InstructGPT对共同撰写的文本贡献较少。

    arXiv:2309.05196v2 Announce Type: replace  Abstract: Large language models (LLMs) have led to a surge in collaborative writing with model assistance. As different users incorporate suggestions from the same model, there is a risk of decreased diversity in the produced content, potentially limiting diverse perspectives in public discourse. In this work, we measure the impact of co-writing on diversity via a controlled experiment, where users write argumentative essays in three setups -- using a base LLM (GPT3), a feedback-tuned LLM (InstructGPT), and writing without model help. We develop a set of diversity metrics and find that writing with InstructGPT (but not the GPT3) results in a statistically significant reduction in diversity. Specifically, it increases the similarity between the writings of different authors and reduces the overall lexical and content diversity. We additionally find that this effect is mainly attributable to InstructGPT contributing less diverse text to co-writt
    
[^43]: 加速算法用于约束非凸-非凹极小-极大优化和共单调包含

    Accelerated Algorithms for Constrained Nonconvex-Nonconcave Min-Max Optimization and Comonotone Inclusion

    [https://arxiv.org/abs/2206.05248](https://arxiv.org/abs/2206.05248)

    本论文提出了针对约束共单调极小-极大优化和共单调包含问题的加速算法，扩展了现有算法并实现了较优的收敛速率，同时证明了算法的收敛性。

    

    我们研究了约束共单调极小-极大优化，一类结构化的非凸-非凹极小-极大优化问题以及它们对共单调包含的推广。在我们的第一个贡献中，我们将最初由Yoon和Ryu（2021）提出的无约束极小-极大优化的Extra Anchored Gradient（EAG）算法扩展到约束共单调极小-极大优化和共单调包含问题，并实现了所有一阶方法中的最优收敛速率$O\left(\frac{1}{T}\right)$。此外，我们证明了算法的迭代收敛到解集中的一个点。在我们的第二个贡献中，我们将由Lee和Kim（2021）开发的快速额外梯度（FEG）算法扩展到约束共单调极小-极大优化和共单调包含，并实现了相同的$O\left(\frac{1}{T}\right)$收敛速率。这个速率适用于文献中研究过的最广泛的共单调包含问题集合。我们的分析基于s的内容。

    We study constrained comonotone min-max optimization, a structured class of nonconvex-nonconcave min-max optimization problems, and their generalization to comonotone inclusion. In our first contribution, we extend the Extra Anchored Gradient (EAG) algorithm, originally proposed by Yoon and Ryu (2021) for unconstrained min-max optimization, to constrained comonotone min-max optimization and comonotone inclusion, achieving an optimal convergence rate of $O\left(\frac{1}{T}\right)$ among all first-order methods. Additionally, we prove that the algorithm's iterations converge to a point in the solution set. In our second contribution, we extend the Fast Extra Gradient (FEG) algorithm, as developed by Lee and Kim (2021), to constrained comonotone min-max optimization and comonotone inclusion, achieving the same $O\left(\frac{1}{T}\right)$ convergence rate. This rate is applicable to the broadest set of comonotone inclusion problems yet studied in the literature. Our analyses are based on s
    
[^44]: 一种多标签分类的一致算法: 宏观at-$k$度量.

    Consistent algorithms for multi-label classification with macro-at-$k$ metrics. (arXiv:2401.16594v1 [cs.LG])

    [http://arxiv.org/abs/2401.16594](http://arxiv.org/abs/2401.16594)

    该研究提出了一种针对多标签分类的一致算法，主要解决了宏观at-$k$度量的优化问题。通过在人口效用框架下考虑复杂性能度量的优化，该算法在极端分类问题中表现出色。

    

    我们在人口效用框架下考虑了多标签分类中复杂性能度量的优化问题。我们主要关注的是将度量线性分解为每个标签分别应用的二分类效用的总和，并对每个实例预测恰好有$k$个标签的额外要求。“宏观at-$k$”度量在具有长尾标签的极端分类问题中具有理想的属性。不幸的是，at-$k$约束将原本独立的二分类任务耦合在一起，导致比标准宏平均更具挑战性的优化问题。我们提供了一个统计框架来研究这个问题，证明了最优分类器的存在和形式，并基于Frank-Wolfe方法提出了一种统计一致且实用的学习算法。有趣的是，我们的主要结果还涉及非线性函数的更一般度量，这些函数是按标签进行的混淆矩阵。实证结果表明，我们的算法在多个数据集上都取得了很好的性能。

    We consider the optimization of complex performance metrics in multi-label classification under the population utility framework. We mainly focus on metrics linearly decomposable into a sum of binary classification utilities applied separately to each label with an additional requirement of exactly $k$ labels predicted for each instance. These "macro-at-$k$" metrics possess desired properties for extreme classification problems with long tail labels. Unfortunately, the at-$k$ constraint couples the otherwise independent binary classification tasks, leading to a much more challenging optimization problem than standard macro-averages. We provide a statistical framework to study this problem, prove the existence and the form of the optimal classifier, and propose a statistically consistent and practical learning algorithm based on the Frank-Wolfe method. Interestingly, our main results concern even more general metrics being non-linear functions of label-wise confusion matrices. Empirical
    
[^45]: 跨领域联合学习中基于记录级个性化差分隐私的研究

    Cross-silo Federated Learning with Record-level Personalized Differential Privacy. (arXiv:2401.16251v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2401.16251](http://arxiv.org/abs/2401.16251)

    本文研究了跨领域联合学习中基于记录级个性化差分隐私的问题，设计了一个名为rPDP-FL的新型框架，并提出了多功能解决方案“模拟-曲线拟合”，以满足不同记录的隐私需求。

    

    基于差分隐私增强的联合学习成为了保护客户端数据隐私的常用方法，但现有方案通常假设所有记录的隐私预算均相同，提供一种适用于所有记录的通用解决方案，可能无法满足每个记录的隐私需求。本文探讨了跨领域联合学习中基于记录级个性化差分隐私的未知领域。我们设计了一个名为rPDP-FL的新型框架，采用两阶段混合抽样方案，既包括客户端级别抽样，又包括非均匀记录级别抽样，以适应不同的隐私需求。一个关键且非平凡的问题是在给定个性化隐私预算ε的情况下选择理想的每记录抽样概率q。我们提出了一个多功能解决方案“模拟-曲线拟合”，使我们能够揭示非线性相关性的重要见解。

    Federated learning enhanced by differential privacy has emerged as a popular approach to better safeguard the privacy of client-side data by protecting clients' contributions during the training process. Existing solutions typically assume a uniform privacy budget for all records and provide one-size-fits-all solutions that may not be adequate to meet each record's privacy requirement. In this paper, we explore the uncharted territory of cross-silo FL with record-level personalized differential privacy. We devise a novel framework named rPDP-FL, employing a two-stage hybrid sampling scheme with both client-level sampling and non-uniform record-level sampling to accommodate varying privacy requirements. A critical and non-trivial problem is to select the ideal per-record sampling probability q given the personalized privacy budget {\epsilon}. We introduce a versatile solution named Simulation-CurveFitting, allowing us to uncover a significant insight into the nonlinear correlation betwe
    
[^46]: 基于拓扑感知嵌入记忆的学习扩展图

    Topology-aware Embedding Memory for Learning on Expanding Graphs. (arXiv:2401.13200v1 [cs.LG])

    [http://arxiv.org/abs/2401.13200](http://arxiv.org/abs/2401.13200)

    这篇论文提出了一个基于拓扑感知嵌入记忆的学习扩展图的框架，该框架可以解决在不断扩展的图上应用记忆回放技术导致的内存爆炸问题。

    

    基于记忆回放的技术在连续学习中应用广泛，但是直接应用于不断扩展的图会导致潜在的内存爆炸问题。为了解决这个问题，我们系统分析了内存爆炸问题的关键挑战，并提出了一个通用框架，即Parameter Decoupled Graph Neural Networks (PDGNNs) with Topology-aware Embedding Memory (TEM)，来解决这个问题。该框架不仅将内存空间复杂度从$\mathcal{O}(nd^L)$降低到$\mathcal{O}(n)$，还充分利用了拓扑信息进行记忆回放。

    Memory replay based techniques have shown great success for continual learning with incrementally accumulated Euclidean data. Directly applying them to continually expanding graphs, however, leads to the potential memory explosion problem due to the need to buffer representative nodes and their associated topological neighborhood structures. To this end, we systematically analyze the key challenges in the memory explosion problem, and present a general framework, i.e., Parameter Decoupled Graph Neural Networks (PDGNNs) with Topology-aware Embedding Memory (TEM), to tackle this issue. The proposed framework not only reduces the memory space complexity from $\mathcal{O}(nd^L)$ to $\mathcal{O}(n)$~\footnote{$n$: memory budget, $d$: average node degree, $L$: the radius of the GNN receptive field}, but also fully utilizes the topological information for memory replay. Specifically, PDGNNs decouple trainable parameters from the computation ego-subgraph via \textit{Topology-aware Embeddings} 
    
[^47]: 无计算困难的快速计算超图节点距离的方法

    Frustrated Random Walks: A Fast Method to Compute Node Distances on Hypergraphs. (arXiv:2401.13054v1 [cs.SI])

    [http://arxiv.org/abs/2401.13054](http://arxiv.org/abs/2401.13054)

    本文提出了一种基于随机游走的方法，用于快速计算超图节点之间的距离并进行标签传播。该方法解决了超图中节点距离计算的问题，进一步拓展了超图的应用领域。

    

    超图是图的推广，当考虑实体间的属性共享时会自然产生。尽管可以通过将超边扩展为完全连接的子图来将超图转换为图，但逆向操作在计算上非常复杂且属于NP-complete问题。因此，我们假设超图包含比图更多的信息。此外，直接操作超图比将其扩展为图更为方便。超图中的一个开放问题是如何精确高效地计算节点之间的距离。通过估计节点距离，我们能够找到节点的最近邻居，并使用K最近邻（KNN）方法在超图上执行标签传播。在本文中，我们提出了一种基于随机游走的新方法，实现了在超图上进行标签传播。我们将节点距离估计为随机游走的预期到达时间。我们注意到简单随机游走（SRW）无法准确描述节点之间的距离，因此我们引入了"frustrated"的概念。

    A hypergraph is a generalization of a graph that arises naturally when attribute-sharing among entities is considered. Although a hypergraph can be converted into a graph by expanding its hyperedges into fully connected subgraphs, going the reverse way is computationally complex and NP-complete. We therefore hypothesize that a hypergraph contains more information than a graph. In addition, it is more convenient to manipulate a hypergraph directly, rather than expand it into a graph. An open problem in hypergraphs is how to accurately and efficiently calculate their node distances. Estimating node distances enables us to find a node's nearest neighbors, and perform label propagation on hypergraphs using a K-nearest neighbors (KNN) approach. In this paper, we propose a novel approach based on random walks to achieve label propagation on hypergraphs. We estimate node distances as the expected hitting times of random walks. We note that simple random walks (SRW) cannot accurately describe 
    
[^48]: 天作之合：大型语言模型与进化算法的结合

    A match made in consistency heaven: when large language models meet evolutionary algorithms. (arXiv:2401.10510v1 [cs.NE])

    [http://arxiv.org/abs/2401.10510](http://arxiv.org/abs/2401.10510)

    大型语言模型和进化算法的结合具有强大的一致性，包括标记嵌入和基因型-表现型映射、位置编码和适应性塑造、位置嵌入和选择、注意力和交叉、前馈神经网络和突变、模型训练和参数更新以及多任务学习和多目标优化等多个核心特征。本文分析了现有的耦合研究，并为未来的研究提供了基本路线和关键挑战。

    

    预训练的大型语言模型（LLMs）在生成创造性的自然文本方面具有强大的能力。进化算法（EAs）可以发现复杂实际问题的多样解决方案。本文通过比较文本序列生成和进化的共同特点和方向性，阐述了LLMs与EAs之间的强大一致性，包括多个一对一的核心特征：标记嵌入和基因型-表现型映射、位置编码和适应性塑造、位置嵌入和选择、注意力和交叉、前馈神经网络和突变、模型训练和参数更新以及多任务学习和多目标优化。在这种一致性视角下，分析了现有的耦合研究，包括进化微调和LLM增强型EAs。借助这些洞见，我们概述了未来在LLMs和EAs耦合方面的基本研究路线，并突出了其中的关键挑战。

    Pre-trained large language models (LLMs) have powerful capabilities for generating creative natural text. Evolutionary algorithms (EAs) can discover diverse solutions to complex real-world problems. Motivated by the common collective and directionality of text sequence generation and evolution, this paper illustrates the strong consistency of LLMs and EAs, which includes multiple one-to-one key characteristics: token embedding and genotype-phenotype mapping, position encoding and fitness shaping, position embedding and selection, attention and crossover, feed-forward neural network and mutation, model training and parameter update, and multi-task learning and multi-objective optimization. Based on this consistency perspective, existing coupling studies are analyzed, including evolutionary fine-tuning and LLM-enhanced EAs. Leveraging these insights, we outline a fundamental roadmap for future research in coupling LLMs and EAs, while highlighting key challenges along the way. The consist
    
[^49]: 调和空间和时间抽象化以实现目标表示

    Reconciling Spatial and Temporal Abstractions for Goal Representation. (arXiv:2401.09870v1 [cs.LG])

    [http://arxiv.org/abs/2401.09870](http://arxiv.org/abs/2401.09870)

    本文介绍了一种新的三层分层强化学习算法，引入了空间和时间目标抽象化。研究者提供了学习策略的理论遗憾边界，并在多个任务上对算法进行了评估。

    

    目标表示通过将复杂的学习问题分解为更容易的子任务来影响分层强化学习算法的性能。最近的研究表明，保留时间抽象环境动态的表示方法在解决困难问题和提供优化理论保证方面是成功的。然而，这些方法在环境动态越来越复杂（即时间抽象转换关系依赖更多变量）的任务中无法扩展。另一方面，其他方法则尝试使用空间抽象来缓解前面的问题。它们的限制包括无法适应高维环境和对先前知识的依赖。本文提出了一种新的三层分层强化学习算法，分层结构的不同层次引入了空间和时间目标抽象化。我们对学习策略的遗憾边界进行了理论研究。我们评估了我们提出的算法在不同任务上的性能。

    Goal representation affects the performance of Hierarchical Reinforcement Learning (HRL) algorithms by decomposing the complex learning problem into easier subtasks. Recent studies show that representations that preserve temporally abstract environment dynamics are successful in solving difficult problems and provide theoretical guarantees for optimality. These methods however cannot scale to tasks where environment dynamics increase in complexity i.e. the temporally abstract transition relations depend on larger number of variables. On the other hand, other efforts have tried to use spatial abstraction to mitigate the previous issues. Their limitations include scalability to high dimensional environments and dependency on prior knowledge.  In this paper, we propose a novel three-layer HRL algorithm that introduces, at different levels of the hierarchy, both a spatial and a temporal goal abstraction. We provide a theoretical study of the regret bounds of the learned policies. We evalua
    
[^50]: 大语言模型的零样本位置去偏方法

    Zero-Shot Position Debiasing for Large Language Models. (arXiv:2401.01218v1 [cs.CL])

    [http://arxiv.org/abs/2401.01218](http://arxiv.org/abs/2401.01218)

    本文提出了一种零样本位置去偏方法（ZOE）来降低大语言模型（LLMs）的位置偏差问题，该方法利用预训练的LLMs的无监督响应进行去偏。实验证实ZOE在多个数据集和任务中均表现出优异的性能。

    

    微调已被证明是改善大语言模型（LLMs）领域性能的有效方法。然而，LLMs可能适应数据集偏见和预测的捷径，导致生成性能差。实验结果显示，LLMs容易表现出位置偏差，即利用位于开头或末尾或输入中特定位置线索的信息。现有的减轻位置偏差的工作需要外部偏差知识或带注释的非偏倚样本，在实际中不太实用。在这项工作中，我们提出了一种零样本位置去偏（ZOE）框架对LLMs进行位置去偏。ZOE利用预训练的LLMs的无监督响应进行去偏，因此不需要任何外部知识或数据集。为了提高无监督响应的质量，我们提出了一种主从对齐（MSA）模块来修剪这些响应。对八个数据集和五个任务的实验表明，ZOE始终优于其他方法。

    Fine-tuning has been demonstrated to be an effective method to improve the domain performance of large language models (LLMs). However, LLMs might fit the dataset bias and shortcuts for prediction, leading to poor generation performance. Experimental result shows that LLMs are prone to exhibit position bias, i.e., leveraging information positioned at the beginning or end, or specific positional cues within the input. Existing works on mitigating position bias require external bias knowledge or annotated non-biased samples, which is unpractical in reality. In this work, we propose a zero-shot position debiasing (ZOE) framework to mitigate position bias for LLMs. ZOE leverages unsupervised responses from pre-trained LLMs for debiasing, thus without any external knowledge or datasets. To improve the quality of unsupervised responses, we propose a master-slave alignment (MSA) module to prune these responses. Experiments on eight datasets and five tasks show that ZOE consistently outperform
    
[^51]: LQ-LoRA: 低秩加量化矩阵分解用于有效的语言模型微调

    LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning. (arXiv:2311.12023v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2311.12023](http://arxiv.org/abs/2311.12023)

    LQ-LoRA是一种低秩加量化矩阵分解方法，用于内存高效的语言模型微调。它通过将每个预训练矩阵分解为高精度低秩部分和内存高效的量化部分，实现了动态配置量化参数以及对重构目标进行加权的优化，并在微调实验中表现出了优于QLoRA和GPTQ-LoRA的效果。

    

    我们提出了一种简单的方法，用于对预训练语言模型进行内存高效的自适应。我们的方法使用迭代算法将每个预训练矩阵分解为高精度低秩部分和内存高效的量化部分。在微调过程中，量化部分保持固定，只有低秩部分被更新。我们提出了量化部分的整数线性规划表达，可以根据总体内存预算动态配置量化参数（例如比特宽度、块大小）给定每个矩阵。我们进一步探索了数据感知版本的算法，该算法使用Fisher信息矩阵的近似来加权矩阵分解过程中的重构目标。在RoBERTa和LLaMA-2（7B和70B）的微调实验中，我们的低秩加量化矩阵分解方法（LQ-LoRA）优于强基线方法QLoRA和GPTQ-LoRA，并实现了激进的量化。

    We propose a simple approach for memory-efficient adaptation of pretrained language models. Our approach uses an iterative algorithm to decompose each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. During finetuning, the quantized component remains fixed and only the low-rank component is updated. We present an integer linear programming formulation of the quantization component which enables dynamic configuration of quantization parameters (e.g., bit-width, block size) for each matrix given an overall target memory budget. We further explore a data-aware version of the algorithm which uses an approximation of the Fisher information matrix to weight the reconstruction objective during matrix decomposition. Experiments on finetuning RoBERTa and LLaMA-2 (7B and 70B) demonstrate that our low-rank plus quantized matrix decomposition approach (LQ-LoRA) outperforms strong QLoRA and GPTQ-LoRA baselines and enables aggressive quantization
    
[^52]: 揭示偏见和不平等：利用电子健康记录的医疗人工智能中偏见检测和缓解的系统综述

    Unmasking Bias and Inequities: A Systematic Review of Bias Detection and Mitigation in Healthcare Artificial Intelligence Using Electronic Health Records. (arXiv:2310.19917v1 [cs.AI])

    [http://arxiv.org/abs/2310.19917](http://arxiv.org/abs/2310.19917)

    本综述对涉及利用电子健康记录数据的医疗人工智能研究中的偏见进行了系统综述，共涵盖了六种主要的偏见类型，同时总结了现有的偏见处理方法。

    

    目的：利用电子健康记录的人工智能应用在医疗领域越来越受到欢迎，但也引入了各种类型的偏见。本研究旨在系统综述涉及利用电子健康记录数据的人工智能研究中的偏见。方法：遵循Preferred Reporting Items for Systematic Reviews and Meta-analyses (PRISMA)准则进行了系统综述。从PubMed、Web of Science和电气和电子工程师学会中检索了2010年1月1日至2022年10月31日期间发表的文章。我们定义了六种主要的偏见类型，并总结了现有的偏见处理方法。结果：在检索到的252篇文章中，有20篇符合最终综述的纳入标准。本综述涵盖了六种偏见中的五种：八项研究分析了选择偏见；六项研究针对隐性偏见；五项研究对混杂偏见进行了研究；四项研究对测量偏见进行了研究；两项研究对算法偏见进行了研究。在偏见处理方法方面，有十项研究进行了探讨。

    Objectives: Artificial intelligence (AI) applications utilizing electronic health records (EHRs) have gained popularity, but they also introduce various types of bias. This study aims to systematically review the literature that address bias in AI research utilizing EHR data. Methods: A systematic review was conducted following the Preferred Reporting Items for Systematic Reviews and Meta-analyses (PRISMA) guideline. We retrieved articles published between January 1, 2010, and October 31, 2022, from PubMed, Web of Science, and the Institute of Electrical and Electronics Engineers. We defined six major types of bias and summarized the existing approaches in bias handling. Results: Out of the 252 retrieved articles, 20 met the inclusion criteria for the final review. Five out of six bias were covered in this review: eight studies analyzed selection bias; six on implicit bias; five on confounding bias; four on measurement bias; two on algorithmic bias. For bias handling approaches, ten st
    
[^53]: DCSI -- 基于分离和连通性的改进的聚类可分离性度量

    DCSI -- An improved measure of cluster separability based on separation and connectedness. (arXiv:2310.12806v1 [stat.ML])

    [http://arxiv.org/abs/2310.12806](http://arxiv.org/abs/2310.12806)

    这篇论文提出了一种改进的聚类可分离性度量方法，旨在量化类间分离和类内连通性，对于密度聚类具有较好的性能表现。

    

    确定给定数据集中的类别标签是否对应于有意义的聚类对于使用真实数据集评估聚类算法至关重要。这个特性可以通过可分离性度量来量化。现有文献的综述显示，既有的基于分类的复杂性度量方法和聚类有效性指标 (CVIs) 都没有充分融入基于密度的聚类的核心特征：类间分离和类内连通性。一种新开发的度量方法 (密度聚类可分离性指数, DCSI) 旨在量化这两个特征，并且也可用作 CVI。对合成数据的广泛实验表明，DCSI 与通过调整兰德指数 (ARI) 测量的DBSCAN的性能之间有很强的相关性，但在对多类数据集进行密度聚类不适当的重叠类别时缺乏鲁棒性。对经常使用的真实数据集进行详细评估显示，DCSI 能够更好地区分密度聚类的可分离性。

    Whether class labels in a given data set correspond to meaningful clusters is crucial for the evaluation of clustering algorithms using real-world data sets. This property can be quantified by separability measures. A review of the existing literature shows that neither classification-based complexity measures nor cluster validity indices (CVIs) adequately incorporate the central aspects of separability for density-based clustering: between-class separation and within-class connectedness. A newly developed measure (density cluster separability index, DCSI) aims to quantify these two characteristics and can also be used as a CVI. Extensive experiments on synthetic data indicate that DCSI correlates strongly with the performance of DBSCAN measured via the adjusted rand index (ARI) but lacks robustness when it comes to multi-class data sets with overlapping classes that are ill-suited for density-based hard clustering. Detailed evaluation on frequently used real-world data sets shows that
    
[^54]: 走向图基础模型：一项调查与进展

    Towards Graph Foundation Models: A Survey and Beyond. (arXiv:2310.11829v1 [cs.LG])

    [http://arxiv.org/abs/2310.11829](http://arxiv.org/abs/2310.11829)

    本文提出了图基础模型（GFMs）的概念，并对其关键特征和技术进行了全面阐述。同时，将现有GFMs工作分为三个类别，为进一步研究和开发图学习范式奠定了基础。

    

    基于其在自然语言处理和其他领域中的显著成功，基础模型已经成为各种人工智能应用的基本构建模块。与此同时，图机器学习经历了由浅层方法向深度学习方法的转变。基础模型的出现和同化能力引起了图机器学习研究者的兴趣，引发了关于开发下一个预训练于广泛图数据并可适应各种下游图任务的图学习范式的讨论。然而，目前对这类工作尚无明确的定义和系统的分析。在本文中，我们提出了图基础模型(GFMs)的概念，并首次对其关键特征和技术进行全面阐述。在此基础上，我们根据其可靠性将现有GFMs工作分为三个类别。

    Emerging as fundamental building blocks for diverse artificial intelligence applications, foundation models have achieved notable success across natural language processing and many other domains. Parallelly, graph machine learning has witnessed a transformative shift, with shallow methods giving way to deep learning approaches. The emergence and homogenization capabilities of foundation models have piqued the interest of graph machine learning researchers, sparking discussions about developing the next graph learning paradigm that is pre-trained on broad graph data and can be adapted to a wide range of downstream graph tasks. However, there is currently no clear definition and systematic analysis for this type of work. In this article, we propose the concept of graph foundation models (GFMs), and provide the first comprehensive elucidation on their key characteristics and technologies. Following that, we categorize existing works towards GFMs into three categories based on their relia
    
[^55]: 具有神经感知机制的部分可观测随机博弈

    Partially Observable Stochastic Games with Neural Perception Mechanisms. (arXiv:2310.11566v1 [cs.GT])

    [http://arxiv.org/abs/2310.11566](http://arxiv.org/abs/2310.11566)

    本研究提出了神经符号化部分可观测随机博弈（NS-POSGs）模型，通过融合感知机制解决了多智能体序列决策中的部分可观测性问题。其中，我们专注于一种只有部分观测信息的智能体和一种完全观测的智能体的单方面设置，并提出了一种近似计算NS-POSGs值的新方法。

    

    随机博弈是一个为多智能体在不确定性下进行序列决策的模型。然而在现实中，智能体对环境只有部分可观测性，这使得问题在计算上具有挑战性，即使在部分可观测马尔可夫决策过程的单智能体环境中也是如此。此外，在实践中，智能体越来越多地使用基于数据的方法，例如在连续数据上训练的神经网络来感知环境。为了解决这个问题，我们提出了神经符号化部分可观测随机博弈（NS-POSGs）的模型，这是连续空间并发随机博弈的一种变体，明确地融入了感知机制。我们专注于单方面的设置，包含了一个具有离散、基于数据驱动的观测和一个具有连续观测的充分了解的智能体。我们提出了一种名为单边NS-HSVI的基于点的方法，用来近似计算单方面NS-POSGs的值，并进行了实现。

    Stochastic games are a well established model for multi-agent sequential decision making under uncertainty. In reality, though, agents have only partial observability of their environment, which makes the problem computationally challenging, even in the single-agent setting of partially observable Markov decision processes. Furthermore, in practice, agents increasingly perceive their environment using data-driven approaches such as neural networks trained on continuous data. To tackle this problem, we propose the model of neuro-symbolic partially-observable stochastic games (NS-POSGs), a variant of continuous-space concurrent stochastic games that explicitly incorporates perception mechanisms. We focus on a one-sided setting, comprising a partially-informed agent with discrete, data-driven observations and a fully-informed agent with continuous observations. We present a new point-based method, called one-sided NS-HSVI, for approximating values of one-sided NS-POSGs and implement it ba
    
[^56]: 通过非线性研究深度神经网络的理解

    Understanding deep neural networks through the lens of their non-linearity. (arXiv:2310.11439v1 [cs.LG])

    [http://arxiv.org/abs/2310.11439](http://arxiv.org/abs/2310.11439)

    本文提出了一个理论上有效的解决方案，通过亲和度评分追踪深度神经网络中的非线性传播，尤其关注计算机视觉应用。实验证实了所提出方法的实用性和对广泛应用的潜力。

    

    深度神经网络(DNN)的显著成功常常归因于它们的高表达能力和近似任意复杂函数的能力。事实上，DNN是高度非线性的模型，其中引入的激活函数在其中起到了重要作用。然而，尽管许多研究通过近似能力的视角研究了DNN的表达能力，但量化DNN或个别激活函数的非线性仍然是一个开放性问题。在本文中，我们提出了第一个在具体关注计算机视觉应用中追踪非线性传播的理论有效解决方案。我们提出的亲和度评分允许我们深入了解各种不同体系结构和学习范式的内部工作原理。我们提供了大量的实验结果，突出了所提出的亲和度评分的实际效用和潜在应用的可能性。

    The remarkable success of deep neural networks (DNN) is often attributed to their high expressive power and their ability to approximate functions of arbitrary complexity. Indeed, DNNs are highly non-linear models, and activation functions introduced into them are largely responsible for this. While many works studied the expressive power of DNNs through the lens of their approximation capabilities, quantifying the non-linearity of DNNs or of individual activation functions remains an open problem. In this paper, we propose the first theoretically sound solution to track non-linearity propagation in deep neural networks with a specific focus on computer vision applications. Our proposed affinity score allows us to gain insights into the inner workings of a wide range of different architectures and learning paradigms. We provide extensive experimental results that highlight the practical utility of the proposed affinity score and its potential for long-reaching applications.
    
[^57]: 探索面向人脸变形的扩散自编码器的设计空间

    Exploring the Design Space of Diffusion Autoencoders for Face Morphing. (arXiv:2310.09484v1 [cs.CV])

    [http://arxiv.org/abs/2310.09484](http://arxiv.org/abs/2310.09484)

    这项研究探索了面向人脸变形的扩散自编码器的设计空间，研究了采样算法、逆向DDIM求解器和部分采样的方法。

    

    通过扩散自编码器创建的人脸变形是一种最近的创新，而这种方法的设计空间尚未得到充分探索。我们探索了设计空间的三个方面，即1）采样算法，2）逆向DDIM求解器，以及3）通过添加少量噪声进行部分采样。

    Face morphs created by Diffusion Autoencoders are a recent innovation and the design space of such an approach has not been well explored. We explore three axes of the design space, i.e., 1) sampling algorithms, 2) the reverse DDIM solver, and 3) partial sampling through small amounts of added noise.
    
[^58]: 未来的原因，现在的行动：一种可证明样本效率的自主LLM智能体的原则框架

    Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency. (arXiv:2309.17382v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2309.17382](http://arxiv.org/abs/2309.17382)

    提出了一个名为"RAFA"的原则框架，通过在LLM中将推理视为学习和规划的贝叶斯问题，协调推理和行动。通过一个提示模板进行推理，学习并制定未来的轨迹规划，然后在每一步中采取计划轨迹的初始行动并重新规划未来轨迹。这个框架具有可证明的遗憾保证。

    

    大型语言模型（LLM）展示了令人印象深刻的推理能力，但在现实世界中将推理转化为行动仍然具有挑战性。特别是，如何通过推理的内部机制在与外部环境的最少交互次数内可证明地完成给定任务仍然不清楚。为此，我们提出了一个有可证明遗憾保证的原则框架来协调推理和行动，称之为“为未来而推理，为现在而行动”（RAFA）。具体来说，我们设计了一个推理的提示模板，从内存缓冲区中学习并制定未来的长期轨迹规划（“为未来而推理”）。在每一步中，LLM智能体采取计划轨迹的初始行动（“为现在而行动”），将收集到的反馈存储在内存缓冲区中，并重新调用推理过程从新状态重新规划未来的轨迹。关键思想是将LLM中的推理视为学习和规划的贝叶斯问题。

    Large language models (LLMs) demonstrate impressive reasoning abilities, but translating reasoning into actions in the real world remains challenging. In particular, it remains unclear how to complete a given task provably within a minimum number of interactions with the external environment, e.g., through an internal mechanism of reasoning. To this end, we propose a principled framework with provable regret guarantees to orchestrate reasoning and acting, which we call "reason for future, act for now" (\texttt{RAFA}). Specifically, we design a prompt template for reasoning that learns from the memory buffer and plans a future trajectory over a long horizon ("reason for future"). At each step, the LLM agent takes the initial action of the planned trajectory ("act for now"), stores the collected feedback in the memory buffer, and reinvokes the reasoning routine to replan the future trajectory from the new state.  The key idea is to cast reasoning in LLMs as learning and planning in Bayes
    
[^59]: 基于地理气象数据的深度神经网络用于长期干旱预测

    Long-term drought prediction using deep neural networks based on geospatial weather data. (arXiv:2309.06212v1 [cs.LG])

    [http://arxiv.org/abs/2309.06212](http://arxiv.org/abs/2309.06212)

    基于地理气象数据的深度神经网络用于长期干旱预测，提出了一种端到端解决方案来预测特定地区干旱概率。采用卷积LSTM和Transformer模型相比其他基线模型能够获得更高的准确性。

    

    在农业实践中，准确预测特定地区干旱概率对于决策具有重要性。尤其对于长期决策，提前一年进行预测至关重要。然而，由于感兴趣区域及其相邻区域内各种因素的复杂相互作用，预测这一概率存在挑战。在本研究中，我们提出了一种基于各种时空神经网络的端到端解决方案来解决这个问题。所考虑的模型主要是根据Palmer干旱严重指数（PDSI）预测感兴趣亚区的干旱强度，利用气候模型的内在因素和见解来提高干旱预测的准确性。比较评估结果表明，与基准梯度提升和逻辑回归解决方案相比，卷积LSTM（ConvLSTM）和Transformer模型的准确性更高。前两种模型取得了令人印象深刻的ROC AUC分数，高达0.90

    The accurate prediction of drought probability in specific regions is crucial for informed decision-making in agricultural practices. It is important to make predictions one year in advance, particularly for long-term decisions. However, forecasting this probability presents challenges due to the complex interplay of various factors within the region of interest and neighboring areas. In this study, we propose an end-to-end solution to address this issue based on various spatiotemporal neural networks. The models considered focus on predicting the drought intensity based on the Palmer Drought Severity Index (PDSI) for subregions of interest, leveraging intrinsic factors and insights from climate models to enhance drought predictions.  Comparative evaluations demonstrate the superior accuracy of Convolutional LSTM (ConvLSTM) and transformer models compared to baseline gradient boosting and logistic regression solutions. The two former models achieved impressive ROC AUC scores from 0.90 
    
[^60]: 通过证书合成的动态与控制模型的通用验证框架

    A General Verification Framework for Dynamical and Control Models via Certificate Synthesis. (arXiv:2309.06090v1 [eess.SY])

    [http://arxiv.org/abs/2309.06090](http://arxiv.org/abs/2309.06090)

    这个论文提出了一个通用的框架来通过证书合成验证动态和控制模型。研究者们提供了一种自动化方法来设计控制器并分析复杂规范。这个方法利用神经网络和SMT求解器来提供候选控制和证书函数，并为控制的安全学习领域做出了贡献。

    

    控制论的一个新兴分支专门研究证书学习，涉及对自主或控制模型的所需（可能是复杂的）系统行为的规范，并通过基于函数的证明进行分析验证。然而，满足这些复杂要求的控制器的合成通常是一个非常困难的任务，可能超出了大多数专家控制工程师的能力。因此，需要自动技术能够设计控制器并分析各种复杂规范。在本文中，我们提供了一个通用框架来编码系统规范并定义相应的证书，并提出了一种自动化方法来正式合成控制器和证书。我们的方法为控制的安全学习领域做出了贡献，利用神经网络的灵活性提供候选的控制和证书函数，同时使用SMT求解器来提供形式化的保证。

    An emerging branch of control theory specialises in certificate learning, concerning the specification of a desired (possibly complex) system behaviour for an autonomous or control model, which is then analytically verified by means of a function-based proof. However, the synthesis of controllers abiding by these complex requirements is in general a non-trivial task and may elude the most expert control engineers. This results in a need for automatic techniques that are able to design controllers and to analyse a wide range of elaborate specifications. In this paper, we provide a general framework to encode system specifications and define corresponding certificates, and we present an automated approach to formally synthesise controllers and certificates. Our approach contributes to the broad field of safe learning for control, exploiting the flexibility of neural networks to provide candidate control and certificate functions, whilst using SMT-solvers to offer a formal guarantee of co
    
[^61]: 改进ReLU网络的预测不确定性的仿射不变集成变换方法

    Affine Invariant Ensemble Transform Methods to Improve Predictive Uncertainty in ReLU Networks. (arXiv:2309.04742v1 [stat.ML])

    [http://arxiv.org/abs/2309.04742](http://arxiv.org/abs/2309.04742)

    本文提出了一种仿射不变集成变换方法，可以改善在ReLU网络中的预测不确定性。通过基于集合卡尔曼滤波的贝叶斯推断，我们提出了两个相互作用的粒子系统，并证明了它们的收敛性。同时，我们还探讨了这些方法用于量化预测不确定性的有效性。

    

    我们考虑使用合适的集合卡尔曼滤波的扩展进行逻辑回归的贝叶斯推断问题。我们提出了两个相互作用的粒子系统，从近似后验分布中采样，并证明当粒子数量趋于无穷时，这些相互作用粒子系统收敛到均场极限的量化收敛速率。此外，我们应用这些技术并考察它们作为贝叶斯近似方法在ReLU网络中量化预测不确定性的有效性。

    We consider the problem of performing Bayesian inference for logistic regression using appropriate extensions of the ensemble Kalman filter. Two interacting particle systems are proposed that sample from an approximate posterior and prove quantitative convergence rates of these interacting particle systems to their mean-field limit as the number of particles tends to infinity. Furthermore, we apply these techniques and examine their effectiveness as methods of Bayesian approximation for quantifying predictive uncertainty in ReLU networks.
    
[^62]: 微调可能削弱基础模型；保留特征可能是解决方案

    Fine-tuning can cripple your foundation model; preserving features may be the solution. (arXiv:2308.13320v1 [cs.LG])

    [http://arxiv.org/abs/2308.13320](http://arxiv.org/abs/2308.13320)

    在微调过程中，基础模型可能会遗忘概念，我们提出了一种名为LDIFS的方法，用于解决这个问题，该方法在实验证明效果显著。

    

    预训练的基础模型主要由于其巨大的容量和对从互联网上爬取的大量训练数据的暴露，享有存储关于许多现实世界概念的知识的优势。这些模型通常在下游数据集上进行微调，以产生出色的最新性能。然而，我们观察到，与预训练模型相比，微调模型在与下游任务不同的任务上识别概念的能力显著降低。这显然是不可取的，因为在首次学习这些概念时，投入了大量的时间和金钱。我们将这种不可取的现象称为“概念遗忘”，通过实验证明大多数端到端微调方法都严重受到这种副作用的影响。为此，我们还提出了一个相当简单的解决方法，即设计了一种名为LDIFS的方法。

    Pre-trained foundation models, owing primarily to their enormous capacity and exposure to vast amount of training data scraped from the internet, enjoy the advantage of storing knowledge about plenty of real-world concepts. Such models are typically fine-tuned on downstream datasets to produce remarkable state-of-the-art performances. While various fine-tuning methods have been devised and are shown to be highly effective, we observe that a fine-tuned model's ability to recognize concepts on tasks $\textit{different}$ from the downstream one is reduced significantly compared to its pre-trained counterpart. This is clearly undesirable as a huge amount of time and money went into learning those very concepts in the first place. We call this undesirable phenomenon "concept forgetting" and via experiments show that most end-to-end fine-tuning approaches suffer heavily from this side effect. To this end, we also propose a rather simple fix to this problem by designing a method called LDIFS 
    
[^63]: 一种用于描述A/B测试中网络干扰的两部分机器学习方法

    A Two-Part Machine Learning Approach to Characterizing Network Interference in A/B Testing. (arXiv:2308.09790v1 [stat.ML])

    [http://arxiv.org/abs/2308.09790](http://arxiv.org/abs/2308.09790)

    本论文提出了一种两部分机器学习方法，用于识别和描述A/B测试中的网络干扰。通过考虑潜在的复杂网络结构和建立适合的曝光映射，该方法在合成实验和真实大规模测试中的模拟中表现优于传统方法。

    

    受网络干扰现象的影响，控制实验或"A/B测试"的可靠性通常会受到损害。为了解决这个问题，我们提出了一种基于机器学习的方法来识别和描述异质网络干扰。我们的方法考虑了潜在的复杂网络结构，并自动化了"曝光映射"确定的任务，从而解决了现有文献中的两个主要限制。我们引入了"因果网络模式"，并采用透明的机器学习模型来建立最适合反映潜在网络干扰模式的曝光映射。我们的方法通过在两个合成实验和一个涉及100-200万Instagram用户的真实大规模测试中的模拟得到了验证，表现优于传统方法，如基于设计的集群随机化和基于分析的邻域曝光映射。

    The reliability of controlled experiments, or "A/B tests," can often be compromised due to the phenomenon of network interference, wherein the outcome for one unit is influenced by other units. To tackle this challenge, we propose a machine learning-based method to identify and characterize heterogeneous network interference. Our approach accounts for latent complex network structures and automates the task of "exposure mapping'' determination, which addresses the two major limitations in the existing literature. We introduce "causal network motifs'' and employ transparent machine learning models to establish the most suitable exposure mapping that reflects underlying network interference patterns. Our method's efficacy has been validated through simulations on two synthetic experiments and a real-world, large-scale test involving 1-2 million Instagram users, outperforming conventional methods such as design-based cluster randomization and analysis-based neighborhood exposure mapping. 
    
[^64]: 一种用于具有缺失值的整体生存分析的深度学习方法

    A Deep Learning Approach for Overall Survival Analysis with Missing Values. (arXiv:2307.11465v1 [cs.LG])

    [http://arxiv.org/abs/2307.11465](http://arxiv.org/abs/2307.11465)

    提出了一个深度学习模型，通过有效利用被审查和未被审查病人的信息，预测非小细胞肺癌（NSCLC）病人的整体生存。

    

    人工智能可以应用于肺癌研究，尤其是非小细胞肺癌（NSCLC），这是一个具有挑战性的领域。对于病人状态的整体生存（OS）是一个重要指标，可以帮助识别生存概率不同的亚组，从而实现个体化治疗和改善整体生存率。在这个分析中，需要考虑两个挑战。首先，很少有研究能够有效利用每个病人的可用信息，利用未被审查的（即死亡）和被审查的（即幸存者）病人的信息，也要考虑到死亡时间。其次，不完整数据处理是医学领域常见的问题。这个问题通常通过使用插补方法来解决。我们的目标是提出一个能够克服这些限制的人工智能模型，能够从被审查和未被审查的病人及其可用特征中有效学习，预测NSCLC病人的OS。

    One of the most challenging fields where Artificial Intelligence (AI) can be applied is lung cancer research, specifically non-small cell lung cancer (NSCLC). In particular, overall survival (OS) is a vital indicator of patient status, helping to identify subgroups with diverse survival probabilities, enabling tailored treatment and improved OS rates. In this analysis, there are two challenges to take into account. First, few studies effectively exploit the information available from each patient, leveraging both uncensored (i.e., dead) and censored (i.e., survivors) patients, considering also the death times. Second, the handling of incomplete data is a common issue in the medical field. This problem is typically tackled through the use of imputation methods. Our objective is to present an AI model able to overcome these limits, effectively learning from both censored and uncensored patients and their available features, for the prediction of OS for NSCLC patients. We present a novel 
    
[^65]: 快速无监督深度异常值模型选择与超网络

    Fast Unsupervised Deep Outlier Model Selection with Hypernetworks. (arXiv:2307.10529v1 [cs.LG])

    [http://arxiv.org/abs/2307.10529](http://arxiv.org/abs/2307.10529)

    本文提出了HYPER用于调整基于深度神经网络的异常值检测模型，解决了无监督DOD模型中的超参数调整和模型选择的挑战，通过设计和训练超网络(HN)将超参数映射到主要DOD模型的最优权重上。

    

    异常值检测(OD)在许多领域都有应用，并有许多技术的丰富文献。基于深度神经网络的OD(DOD)由于深度学习的许多进展而受到了最近的关注。在本文中，我们考虑了一个关键但鲜为人知的问题，即无监督DOD的有效超参数(HP)调整/模型选择。虽然一些先前的工作报告了OD模型对HP的敏感性，但对于展示了长列表HP的现代DOD模型来说，这变得非常关键。我们引入了HYPER来调整DOD模型，解决了两个基本挑战：(1)无监督情况下的验证(由于缺乏标记的异常值)，以及(2) HP/模型空间的高效搜索 (由于HP数量的指数增长)。关键思想是设计和训练一个新颖的超网络(HN)，其将HP映射到主要DOD模型的最优权重上。反过来，HYPER利用一个单独的HN，可以动态生成多个DOD模型的权重 (对应于...)。

    Outlier detection (OD) finds many applications with a rich literature of numerous techniques. Deep neural network based OD (DOD) has seen a recent surge of attention thanks to the many advances in deep learning. In this paper, we consider a critical-yet-understudied challenge with unsupervised DOD, that is, effective hyperparameter (HP) tuning/model selection. While several prior work report the sensitivity of OD models to HPs, it becomes ever so critical for the modern DOD models that exhibit a long list of HPs. We introduce HYPER for tuning DOD models, tackling two fundamental challenges: (1) validation without supervision (due to lack of labeled anomalies), and (2) efficient search of the HP/model space (due to exponential growth in the number of HPs). A key idea is to design and train a novel hypernetwork (HN) that maps HPs onto optimal weights of the main DOD model. In turn, HYPER capitalizes on a single HN that can dynamically generate weights for many DOD models (corresponding t
    
[^66]: 用图神经结构搜索进行分子属性预测的不确定性量化

    Uncertainty Quantification for Molecular Property Predictions with Graph Neural Architecture Search. (arXiv:2307.10438v1 [cs.LG])

    [http://arxiv.org/abs/2307.10438](http://arxiv.org/abs/2307.10438)

    用于分子属性预测的图神经网络方法通常无法量化预测的不确定性，本研究提出了一种自动化的不确定性量化方法AutoGNNUQ，通过架构搜索生成高性能的图神经网络集合，并利用方差分解将数据和模型的不确定性分开，从而提供了减少不确定性的有价值见解。

    

    图神经网络（GNN）已成为分子属性预测中突出的数据驱动方法。然而，典型GNN模型的一个关键限制是无法量化预测中的不确定性。这种能力对于确保在下游任务中可信地使用和部署模型至关重要。为此，我们引入了AutoGNNUQ，一种自动化的分子属性预测不确定性量化方法。AutoGNNUQ利用架构搜索生成一组高性能的GNN集合，能够估计预测的不确定性。我们的方法使用方差分解来分离数据（aleatoric）和模型（epistemic）不确定性，为减少它们提供了有价值的见解。在我们的计算实验中，我们展示了AutoGNNUQ在多个基准数据集上在预测准确性和不确定性测量性能方面超过了现有的不确定性量化方法。此外，我们利用t-SNE可视化来解释不确定性的来源和结构。

    Graph Neural Networks (GNNs) have emerged as a prominent class of data-driven methods for molecular property prediction. However, a key limitation of typical GNN models is their inability to quantify uncertainties in the predictions. This capability is crucial for ensuring the trustworthy use and deployment of models in downstream tasks. To that end, we introduce AutoGNNUQ, an automated uncertainty quantification (UQ) approach for molecular property prediction. AutoGNNUQ leverages architecture search to generate an ensemble of high-performing GNNs, enabling the estimation of predictive uncertainties. Our approach employs variance decomposition to separate data (aleatoric) and model (epistemic) uncertainties, providing valuable insights for reducing them. In our computational experiments, we demonstrate that AutoGNNUQ outperforms existing UQ methods in terms of both prediction accuracy and UQ performance on multiple benchmark datasets. Additionally, we utilize t-SNE visualization to exp
    
[^67]: 逆进化层:物理信息化正则化器用于深度神经网络

    Inverse Evolution Layers: Physics-informed Regularizers for Deep Neural Networks. (arXiv:2307.07344v1 [cs.LG])

    [http://arxiv.org/abs/2307.07344](http://arxiv.org/abs/2307.07344)

    本文提出了一种新颖的方法，通过一种新的正则化方法将基于偏微分方程（PDE）的进化模型与神经网络集成在一起。这些层可以实现特定的正则化目标，并赋予神经网络输出与进化模型对应的特性。此外，逆进化层的构建和实现相对简单，可以轻松地为不同的物理进化和神经网络设计。

    

    本文提出了一种新颖的方法，通过一种新的正则化方法将基于偏微分方程（PDE）的进化模型与神经网络集成在一起。具体而言，我们提出了基于进化方程的逆进化层（IELs）。这些层可以实现特定的正则化目标，并赋予神经网络输出与进化模型对应的特性。此外，逆进化层的构建和实现相对简单，可以轻松地为不同的物理进化和神经网络设计。此外，这些层的设计过程可以为神经网络提供直观和数学可解释性，从而增强了方法的透明度和解释性。为了证明我们方法的有效性、效率和简单性，我们提出了一个将语义分割模型赋予热扩散模型平滑性属性的示例。

    This paper proposes a novel approach to integrating partial differential equation (PDE)-based evolution models into neural networks through a new type of regularization. Specifically, we propose inverse evolution layers (IELs) based on evolution equations. These layers can achieve specific regularization objectives and endow neural networks' outputs with corresponding properties of the evolution models. Moreover, IELs are straightforward to construct and implement, and can be easily designed for various physical evolutions and neural networks. Additionally, the design process for these layers can provide neural networks with intuitive and mathematical interpretability, thus enhancing the transparency and explainability of the approach. To demonstrate the effectiveness, efficiency, and simplicity of our approach, we present an example of endowing semantic segmentation models with the smoothness property based on the heat diffusion model. To achieve this goal, we design heat-diffusion IE
    
[^68]: 一阶方法在具有通用预测器的统计学习中的泛化误差分析

    Generalization Error of First-Order Methods for Statistical Learning with Generic Oracles. (arXiv:2307.04679v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.04679](http://arxiv.org/abs/2307.04679)

    本文提出了一种新的框架来分析使用一阶优化算法进行统计学习时的泛化误差，该框架适用于多个学习问题，并且可以推导出紧密匹配的上界和下界。这些结果适用于光滑、强凸和满足Polyak-Lojasiewicz假设的优化问题。

    

    本文提供了一个新的框架，用于分析在统计学习中使用一阶优化算法时，当梯度只能通过预测器给出的部分观测来访问时的泛化误差。我们的分析依赖于梯度对数据样本的规则性，并且可以推导出多个学习问题的泛化误差的紧密匹配的上界和下界，包括监督学习、迁移学习、鲁棒学习、分布式学习和使用梯度量化的通信效率学习。这些结果适用于光滑且强凸的优化问题，以及满足Polyak-Lojasiewicz假设的光滑非凸优化问题。特别地，我们的上界和下界依赖于一个扩展了条件标准差概念的新量，它衡量了通过访问预测器可以近似梯度的程度。

    In this paper, we provide a novel framework for the analysis of generalization error of first-order optimization algorithms for statistical learning when the gradient can only be accessed through partial observations given by an oracle. Our analysis relies on the regularity of the gradient w.r.t. the data samples, and allows to derive near matching upper and lower bounds for the generalization error of multiple learning problems, including supervised learning, transfer learning, robust learning, distributed learning and communication efficient learning using gradient quantization. These results hold for smooth and strongly-convex optimization problems, as well as smooth non-convex optimization problems verifying a Polyak-Lojasiewicz assumption. In particular, our upper and lower bounds depend on a novel quantity that extends the notion of conditional standard deviation, and is a measure of the extent to which the gradient can be approximated by having access to the oracle. As a consequ
    
[^69]: 学习用于测试时领域泛化的变分邻居标签

    Learning Variational Neighbor Labels for Test-Time Domain Generalization. (arXiv:2307.04033v1 [cs.LG])

    [http://arxiv.org/abs/2307.04033](http://arxiv.org/abs/2307.04033)

    本文提出了一种用于测试时领域泛化的方法，通过在测试时使用概率伪标签和变分邻居标签来推广源域训练的模型到目标领域，以提高模型的鲁棒性和准确性。

    

    本文致力于领域泛化，在未知的目标领域中只在源领域上进行训练模型。我们在源域上进行训练，然后在目标域上进行推理，利用无标签目标数据本身的价值。我们做出了三个贡献。首先，我们提出了目标样本的概率伪标签，以在测试时将源领域训练的模型推广到目标领域。我们将测试时的推广建模为变分推理问题，通过将伪标签建模为分布，考虑泛化过程中的不确定性，并减轻伪标签不准确性带来的误导信号。其次，我们学习了变分邻居标签，将邻近目标样本的信息纳入到生成更强鲁棒伪标签的过程中。第三，为了学习将更具代表性的目标信息纳入到生成更准确、更强鲁棒的变分邻居标签的能力中，我们

    This paper strives for domain generalization, where models are trained exclusively on source domains before being deployed at unseen target domains. We follow the strict separation of source training and target testing but exploit the value of the unlabeled target data itself during inference. We make three contributions. First, we propose probabilistic pseudo-labeling of target samples to generalize the source-trained model to the target domain at test time. We formulate the generalization at test time as a variational inference problem by modeling pseudo labels as distributions to consider the uncertainty during generalization and alleviate the misleading signal of inaccurate pseudo labels. Second, we learn variational neighbor labels that incorporate the information of neighboring target samples to generate more robust pseudo labels. Third, to learn the ability to incorporate more representative target information and generate more precise and robust variational neighbor labels, we 
    
[^70]: 基于特征交互作用进行全局特征效应分解

    Decomposing Global Feature Effects Based on Feature Interactions. (arXiv:2306.00541v1 [stat.ML])

    [http://arxiv.org/abs/2306.00541](http://arxiv.org/abs/2306.00541)

    提出了全局效应广义可加分解（GADGET）框架，能够最小化特征交互作用的本地特征效应的交互异质性。同时适用于偏依赖、积累局部效应和Shapley可加解释（SHAP）依赖的边际特征效应可视化方法，并提出了一种新的基于置换的交互测试来检测显着的特征交互作用。

    

    全局特征效应方法，如偏依赖图，提供了预期边际特征效应的可理解的可视化。但是，当存在特征交互作用时，这种全局特征效应方法可能会误导，因为它们不能很好地表示单个观测的局部特征效应。我们正式介绍了基于递归分区的全局效应广义可加分解（GADGET）框架，以找到解释性特征空间中的可解释区域，从而最小化本地特征效应的交互异质性。我们为该框架提供了数学基础，并展示它适用于最流行的方法来可视化边际特征效应，即偏依赖，积累局部效应和Shapley可加解释（SHAP）依赖。此外，我们引入了一种新的基于置换的交互测试来检测显着的特征交互作用，该方法适用于任何特征。

    Global feature effect methods, such as partial dependence plots, provide an intelligible visualization of the expected marginal feature effect. However, such global feature effect methods can be misleading, as they do not represent local feature effects of single observations well when feature interactions are present. We formally introduce generalized additive decomposition of global effects (GADGET), which is a new framework based on recursive partitioning to find interpretable regions in the feature space such that the interaction-related heterogeneity of local feature effects is minimized. We provide a mathematical foundation of the framework and show that it is applicable to the most popular methods to visualize marginal feature effects, namely partial dependence, accumulated local effects, and Shapley additive explanations (SHAP) dependence. Furthermore, we introduce a new permutation-based interaction test to detect significant feature interactions that is applicable to any feat
    
[^71]: 使用值条件状态熵探索加速强化学习

    Accelerating Reinforcement Learning with Value-Conditional State Entropy Exploration. (arXiv:2305.19476v1 [cs.LG])

    [http://arxiv.org/abs/2305.19476](http://arxiv.org/abs/2305.19476)

    本文提出了一种新的探索技术，使用值条件状态熵来解决强化学习中探索不足的问题，可以均衡地覆盖低价值和高价值状态，相较于现有基于熵的探索方法，该方法在MuJoCo基准测试和Atari游戏上有着显著的提升。

    

    探索的一种有效技术是通过鼓励对访问状态空间的均匀覆盖来最大化已访问状态分布的熵，即状态熵。然而，它在有任务奖励的监督设置中往往难以应对，其中代理趋向于访问高价值状态以利用任务奖励。这个偏好会导致高价值状态和低价值状态的分布不平衡，当分布变得更加均匀时，状态熵会增加，从而偏向于探索低价值区域。当高价值状态在状态空间中分布狭窄时，这个问题会进一步恶化，使得代理完成任务变得更加困难。在本文中，我们提出了一种新颖的探索技术，最大化值条件状态熵，它分别估计每个状态价值估计条件下的状态熵，然后最大化它们的加权和。值条件状态熵量化了低价值和高价值状态区域的覆盖范围，从而使其对不平衡问题更加健壮。我们展示了我们的方法在一系列具有挑战性的MuJoCo基准测试和Atari游戏上显著优于现有的基于熵的探索方法。

    A promising technique for exploration is to maximize the entropy of visited state distribution, i.e., state entropy, by encouraging uniform coverage of visited state space. While it has been effective for an unsupervised setup, it tends to struggle in a supervised setup with a task reward, where an agent prefers to visit high-value states to exploit the task reward. Such a preference can cause an imbalance between the distributions of high-value states and low-value states, which biases exploration towards low-value state regions as a result of the state entropy increasing when the distribution becomes more uniform. This issue is exacerbated when high-value states are narrowly distributed within the state space, making it difficult for the agent to complete the tasks. In this paper, we present a novel exploration technique that maximizes the value-conditional state entropy, which separately estimates the state entropies that are conditioned on the value estimates of each state, then ma
    
[^72]: SpikeCP: 通过极限预测实现延迟自适应可靠脉冲神经网络

    SpikeCP: Delay-Adaptive Reliable Spiking Neural Networks via Conformal Prediction. (arXiv:2305.11322v1 [cs.NE])

    [http://arxiv.org/abs/2305.11322](http://arxiv.org/abs/2305.11322)

    这篇论文提出了一种新的脉冲神经网络模型，能够通过极限预测实现自适应的推断延迟，从而节约能源与提高可靠性。

    

    脉冲神经网络（SNN）通过内部事件驱动的神经动态处理时间序列数据，其能量消耗取决于输入演示期间神经元之间交换的脉冲数量。在典型的SNN分类器实现中，决策是在整个输入序列被处理后产生的，导致延迟和能量消耗水平在输入之间是相对均匀的。最近引入的延迟自适应SNN可根据每个示例的难度来定制推断延迟 - 以及随之而来的能耗 - 通过在SNN模型足够“自信”时产生早期决策来实现。

    Spiking neural networks (SNNs) process time-series data via internal event-driven neural dynamics whose energy consumption depends on the number of spikes exchanged between neurons over the course of the input presentation. In typical implementations of an SNN classifier, decisions are produced after the entire input sequence has been processed, resulting in latency and energy consumption levels that are fairly uniform across inputs. Recently introduced delay-adaptive SNNs tailor the inference latency -- and, with it, the energy consumption -- to the difficulty of each example, by producing an early decision when the SNN model is sufficiently ``confident''. In this paper, we start by observing that, as an SNN processes input samples, its classification decisions tend to be first under-confident and then over-confident with respect to the decision's ground-truth, unknown, test accuracy. This makes it difficult to determine a stopping time that ensures a desired level of accuracy. To add
    
[^73]: 黑盒系统的贝叶斯安全验证

    Bayesian Safety Validation for Black-Box Systems. (arXiv:2305.02449v1 [cs.LG])

    [http://arxiv.org/abs/2305.02449](http://arxiv.org/abs/2305.02449)

    本文提出了一种名为贝叶斯安全验证的算法，将黑盒安全验证问题转化为贝叶斯优化问题。该算法通过概率代理模型拟合快速预测故障，利用重要性采样估计操作域内的故障概率，从而实现了对高维、危险、计算昂贵的系统的高效估计。

    

    对于安全关键系统准确估计故障概率对认证至关重要。由于高维输入空间、危险测试场景和计算昂贵的仿真器，估计通常具有挑战性，因此研究高效估计技术十分重要。本文将黑盒安全验证问题重新定义为贝叶斯优化问题，并引入一种算法——贝叶斯安全验证，该算法通过迭代拟合概率代理模型来高效预测故障。该算法旨在搜索故障、计算最可能的故障，并利用重要性采样估计操作域内的故障概率。我们引入了三种采集函数，重点是通过覆盖设计空间、优化解析派生的故障边界和采样预测的故障区域来减少不确定性。主要涉及只输出二进制指标的系统。

    Accurately estimating the probability of failure for safety-critical systems is important for certification. Estimation is often challenging due to high-dimensional input spaces, dangerous test scenarios, and computationally expensive simulators; thus, efficient estimation techniques are important to study. This work reframes the problem of black-box safety validation as a Bayesian optimization problem and introduces an algorithm, Bayesian safety validation, that iteratively fits a probabilistic surrogate model to efficiently predict failures. The algorithm is designed to search for failures, compute the most-likely failure, and estimate the failure probability over an operating domain using importance sampling. We introduce a set of three acquisition functions that focus on reducing uncertainty by covering the design space, optimizing the analytically derived failure boundaries, and sampling the predicted failure regions. Mainly concerned with systems that only output a binary indicat
    
[^74]: 关于超图上三体相互作用的非线性平均动力学收敛性的研究

    On the convergence of nonlinear averaging dynamics with three-body interactions on hypergraphs. (arXiv:2304.07203v1 [math.DS])

    [http://arxiv.org/abs/2304.07203](http://arxiv.org/abs/2304.07203)

    本文研究了超图上具有三体相互作用的离散时间非线性平均动力学，在初态和超图拓扑以及更新非线性相互作用下，产生了高阶动力效应。

    

    物理学、生物学和社会科学等领域的复杂网络系统通常涉及超出简单的成对相互作用的交互。超图作为描述和分析具有多体相互作用的系统复杂行为的强大建模工具。本文研究了具有三体相互作用的离散时间非线性平均动力学：底层超图由三元组作为超边界定相互作用的结构，而顶点通过加权的状态依赖的邻域对的状态平均更新其状态。相较于带有二体相互作用的图上的线性平均动力学，这个动力学不会收敛到初始状态的平均值，而是减少了初态和超图拓扑的复杂相互作用和更新的非线性产生高阶动力效应。

    Complex networked systems in fields such as physics, biology, and social sciences often involve interactions that extend beyond simple pairwise ones. Hypergraphs serve as powerful modeling tools for describing and analyzing the intricate behaviors of systems with multi-body interactions. Herein, we investigate a discrete-time nonlinear averaging dynamics with three-body interactions: an underlying hypergraph, comprising triples as hyperedges, delineates the structure of these interactions, while the vertices update their states through a weighted, state-dependent average of neighboring pairs' states. This dynamics captures reinforcing group effects, such as peer pressure, and exhibits higher-order dynamical effects resulting from a complex interplay between initial states, hypergraph topology, and nonlinearity of the update. Differently from linear averaging dynamics on graphs with two-body interactions, this model does not converge to the average of the initial states but rather induc
    
[^75]: 适应性正则化在类增量学习中的应用

    Adaptive Regularization for Class-Incremental Learning. (arXiv:2303.13113v1 [cs.LG])

    [http://arxiv.org/abs/2303.13113](http://arxiv.org/abs/2303.13113)

    本文研究了适应性正则化在类增量学习中的应用，通过根据任务复杂度动态调整正则化强度，在学习新类别同时防止遗忘之前学习的类别。实验表明适应性正则化可以实现更加准确和不易遗忘的视觉增量学习。

    

    类增量学习是指在维持先前学习的分类准确度的同时，更新具有新类别的深度分类器。在学习新类别的同时，通过正则化神经网络权重来防止遗忘之前学习的类别是常见的方法。然而，现有的正则化方法在整个增量学习过程中使用恒定的强度，可能无法反映所遇到的任务难度的变化。因此，本研究探讨了适应性正则化在类增量学习中的必要性，该方法根据手头任务的复杂度动态调整正则化强度。我们提出了一种基于贝叶斯优化的方法，自动确定每个学习任务的最佳正则化强度。通过两个数据集上的两种正则化方法的实验，结果表明适应性正则化对于实现更加准确和不易遗忘的视觉增量学习非常重要。

    Class-Incremental Learning updates a deep classifier with new categories while maintaining the previously observed class accuracy. Regularizing the neural network weights is a common method to prevent forgetting previously learned classes while learning novel ones. However, existing regularizers use a constant magnitude throughout the learning sessions, which may not reflect the varying levels of difficulty of the tasks encountered during incremental learning. This study investigates the necessity of adaptive regularization in Class-Incremental Learning, which dynamically adjusts the regularization strength according to the complexity of the task at hand. We propose a Bayesian Optimization-based approach to automatically determine the optimal regularization magnitude for each learning task. Our experiments on two datasets via two regularizers demonstrate the importance of adaptive regularization for achieving accurate and less forgetful visual incremental learning.
    
[^76]: 视觉语言模型补丁-令牌对齐的贝叶斯提示学习

    Patch-Token Aligned Bayesian Prompt Learning for Vision-Language Models. (arXiv:2303.09100v1 [cs.CV])

    [http://arxiv.org/abs/2303.09100](http://arxiv.org/abs/2303.09100)

    本文提出了一种基于贝叶斯概率的视觉语言模型提示学习方法，通过将提示标记推向忠实捕捉标签特定的视觉概念，而不是过度拟合训练类别，解决了现有提示工程的问题。在各种视觉语言任务上的广泛实验表明，该方法优于现有的最先进模型。

    

    在视觉语言预训练模型的下游应用中，构建有效提示引起了极大关注。现有的提示工程方法要么需要费时费力的手动设计，要么将提示调优作为点估计问题进行优化，这可能无法描述类别的多样特征并限制了它们的应用。本文提出了一种基于贝叶斯概率的提示学习方法，其中通过从潜在分布中首先采样隐向量，然后采用轻量级生成模型来生成标签特定的随机提示。重要的是，我们将视觉知识与图像的语义规则化，并将图像和相应的提示视为补丁和令牌集，通过最优传输将提示标记推向忠实捕捉标签特定的视觉概念，而不是过度拟合训练类别。此外，所提出的模型还可以通过使用额外的基于文本的信息来生成更具信息量和准确性的提示。在各种视觉语言任务上的广泛实验表明，我们的补丁-令牌对齐的贝叶斯提示学习（PTBPL）优于现有的最先进模型。

    For downstream applications of vision-language pre-trained models, there has been significant interest in constructing effective prompts. Existing works on prompt engineering, which either require laborious manual designs or optimize the prompt tuning as a point estimation problem, may fail to describe diverse characteristics of categories and limit their applications. We introduce a Bayesian probabilistic resolution to prompt learning, where the label-specific stochastic prompts are generated hierarchically by first sampling a latent vector from an underlying distribution and then employing a lightweight generative model. Importantly, we semantically regularize prompt learning with the visual knowledge and view images and the corresponding prompts as patch and token sets under optimal transport, which pushes the prompt tokens to faithfully capture the label-specific visual concepts, instead of overfitting the training categories. Moreover, the proposed model can also be straightforwar
    
[^77]: Lie 群和它们的齐次空间上的静止核和高斯过程 II：非紧对称空间

    Stationary Kernels and Gaussian Processes on Lie Groups and their Homogeneous Spaces II: non-compact symmetric spaces. (arXiv:2301.13088v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2301.13088](http://arxiv.org/abs/2301.13088)

    本文开发了构建非欧几里得空间上静止高斯过程的实用技术，能够对定义在这些空间上的先验和后验高斯过程进行实际采样和计算协方差核。

    

    高斯过程是机器学习中最重要的时空模型之一，它可以编码有关建模函数的先验信息，并可用于精确或近似贝叶斯学习。在许多应用中，特别是在物理科学和工程领域，以及地质统计学和神经科学等领域，对对称性的不变性是可以考虑的最基本形式之一。高斯过程协方差对这些对称性的不变性引发了对这些空间的平稳性概念的最自然的推广。在这项工作中，我们开发了建立静止高斯过程的构造性和实用技术，用于在对称性背景下出现的非欧几里得空间的非常大的类。我们的技术使得能够（i）计算协方差核和（ii）从这些空间上定义的先验和后验高斯过程中实际地进行采样。

    Gaussian processes are arguably the most important class of spatiotemporal models within machine learning. They encode prior information about the modeled function and can be used for exact or approximate Bayesian learning. In many applications, particularly in physical sciences and engineering, but also in areas such as geostatistics and neuroscience, invariance to symmetries is one of the most fundamental forms of prior information one can consider. The invariance of a Gaussian process' covariance to such symmetries gives rise to the most natural generalization of the concept of stationarity to such spaces. In this work, we develop constructive and practical techniques for building stationary Gaussian processes on a very large class of non-Euclidean spaces arising in the context of symmetries. Our techniques make it possible to (i) calculate covariance kernels and (ii) sample from prior and posterior Gaussian processes defined on such spaces, both in a practical manner. This work is 
    
[^78]: 自适应合并下的纵向网络有效估计

    Efficient Estimation for Longitudinal Network via Adaptive Merging. (arXiv:2211.07866v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.07866](http://arxiv.org/abs/2211.07866)

    本文提出了一个有效的纵向网络估计框架，利用自适应合并、张量分解和点过程等方法来减少估计偏差和方差。

    

    纵向网络由多个节点之间的时间边序列组成，其中时间边在实时中被观察到。随着在线社交平台和电子商务的兴起，它已经变得普遍，但在文献中往往被忽略。本文提出了一个有效的纵向网络估计框架，利用自适应网络合并、张量分解和点过程的优势。它合并相邻的稀疏网络，以扩大观测边的数量并减少估计方差，同时通过利用本地时间结构进行自适应网络邻域控制引入的估计偏差。提出了一个投影梯度下降算法来促进估计，其中每次迭代的估计错误上界被建立。进行了彻底的分析，以量化所提出方法的渐近行为，结果表明它可以显着减少估计偏差。

    Longitudinal network consists of a sequence of temporal edges among multiple nodes, where the temporal edges are observed in real time. It has become ubiquitous with the rise of online social platform and e-commerce, but largely under-investigated in literature. In this paper, we propose an efficient estimation framework for longitudinal network, leveraging strengths of adaptive network merging, tensor decomposition and point process. It merges neighboring sparse networks so as to enlarge the number of observed edges and reduce estimation variance, whereas the estimation bias introduced by network merging is controlled by exploiting local temporal structures for adaptive network neighborhood. A projected gradient descent algorithm is proposed to facilitate estimation, where the upper bound of the estimation error in each iteration is established. A thorough analysis is conducted to quantify the asymptotic behavior of the proposed method, which shows that it can significantly reduce the
    
[^79]: 带装载和覆盖约束的上下文幸存者问题：基于回归的模块化Lagrangian方法

    Contextual Bandits with Packing and Covering Constraints: A Modular Lagrangian Approach via Regression. (arXiv:2211.07484v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.07484](http://arxiv.org/abs/2211.07484)

    该论文研究了一种带有资源线性约束的上下文幸存者问题的变种，提出了一种新的算法，该算法简单、计算效率高，同时能够实现较低的后悔。此外，当某些约束被违反时，算法在统计上是最优的。

    

    我们考虑一种上下文幸存者问题的变种，其中算法在总消费的线性约束下使用多个资源。这个问题推广了带背包的上下文幸存者问题(CBwK)，允许装载和覆盖约束，以及正负资源消耗。我们提出了一种新算法，简单、计算效率高，能够实现退化的后悔。当某些约束被违反时，对于CBwK，它在统计上是最优的。我们的算法基于LagrangianBwK(Immorlica等人，FOCS 2019)，这是一种面向CBwK的Lagrangian技术，以及SquareCB(Foster和Rakhlin，ICML 2020)，这是一种面向上下文幸存者的回归技术。我们的分析利用了两种技术本质上的模块化。

    We consider a variant of contextual bandits in which the algorithm consumes multiple resources subject to linear constraints on total consumption. This problem generalizes contextual bandits with knapsacks (CBwK), allowing for packing and covering constraints, as well as positive and negative resource consumption. We present a new algorithm that is simple, computationally efficient, and admits vanishing regret. It is statistically optimal for CBwK when an algorithm must stop once some constraint is violated. Our algorithm builds on LagrangeBwK (Immorlica et al., FOCS 2019) , a Lagrangian-based technique for CBwK, and SquareCB (Foster and Rakhlin, ICML 2020), a regression-based technique for contextual bandits. Our analysis leverages the inherent modularity of both techniques.
    
[^80]: 图神经网络中的解释方法：一项比较研究

    Explaining the Explainers in Graph Neural Networks: a Comparative Study. (arXiv:2210.15304v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.15304](http://arxiv.org/abs/2210.15304)

    该论文研究了图神经网络中的解释方法，并在多种数据集上测试了十种解释器的表现，提供了不同GNN体系结构易解释性的关键洞察。

    

    在图神经网络的快速发展后，GNN已经在许多科学和工程领域应用广泛，这促使需要方法来理解它们的决策过程。最近几年，GNN解释器开始出现，有多种方法，一些是新颖的，一些是从其他领域改编而来的。为了整理这种海量的解释方法，一些研究在各种可解释性指标方面对不同的解释器性能进行了基准测试。然而，这些早期的工作没有尝试提供关于不同的GNN体系结构更或不易解释的洞察，也没有说明在给定环境中应该选择哪种解释器。在本次调查中，我们通过设计系统性实验研究，对八个代表性体系结构上训练的十种解释器在六个精心设计的图和节点分类数据集上进行了测试，填补了这些空白，并提供了关键的观点。

    Following a fast initial breakthrough in graph based learning, Graph Neural Networks (GNNs) have reached a widespread application in many science and engineering fields, prompting the need for methods to understand their decision process.  GNN explainers have started to emerge in recent years, with a multitude of methods both novel or adapted from other domains. To sort out this plethora of alternative approaches, several studies have benchmarked the performance of different explainers in terms of various explainability metrics. However, these earlier works make no attempts at providing insights into why different GNN architectures are more or less explainable, or which explainer should be preferred in a given setting.  In this survey, we fill these gaps by devising a systematic experimental study, which tests ten explainers on eight representative architectures trained on six carefully designed graph and node classification datasets. With our results we provide key insights on the cho
    
[^81]: 通过元学习学习符号模型无关损失函数

    Learning Symbolic Model-Agnostic Loss Functions via Meta-Learning. (arXiv:2209.08907v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.08907](http://arxiv.org/abs/2209.08907)

    本文提出了一种通过元学习框架学习模型无关损失函数的方法，并通过对多个监督学习任务的实验证明，该方法学到的损失函数优于目前最优方法和交叉熵损失函数。

    

    本文研究损失函数学习的新兴主题，旨在学习可以显著提高模型性能的损失函数。我们提出了一种新的元学习框架，通过混合神经符号搜索方法学习模型无关的损失函数。该框架首先使用基于进化的方法在原始数学操作空间中搜索符号损失函数的集合。然后，学习到的一组损失函数通过端到端的梯度训练过程进行参数化和优化。所提出的框架的多功能性在一组多样化的监督学习任务上得到了经验证实。结果显示，新提出的方法发现的元学习损失函数在各种神经网络架构和数据集上均优于交叉熵损失和现有最先进的损失函数学习方法。

    In this paper, we develop upon the emerging topic of loss function learning, which aims to learn loss functions that significantly improve the performance of the models trained under them. Specifically, we propose a new meta-learning framework for learning model-agnostic loss functions via a hybrid neuro-symbolic search approach. The framework first uses evolution-based methods to search the space of primitive mathematical operations to find a set of symbolic loss functions. Second, the set of learned loss functions are subsequently parameterized and optimized via an end-to-end gradient-based training procedure. The versatility of the proposed framework is empirically validated on a diverse set of supervised learning tasks. Results show that the meta-learned loss functions discovered by the newly proposed method outperform both the cross-entropy loss and state-of-the-art loss function learning methods on a diverse range of neural network architectures and datasets.
    
[^82]: 改善运营经济学：基于双层 MIP 的闭环预测优化框架来预测机组组合的操作计划

    Towards Improving Operation Economics: A Bilevel MIP-Based Closed-Loop Predict-and-Optimize Framework for Prescribing Unit Commitment. (arXiv:2208.13065v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2208.13065](http://arxiv.org/abs/2208.13065)

    本文提出了一个基于双层 MIP 的闭环预测优化框架，使用成本导向的预测器来改进电力系统的经济运行。该框架通过反馈循环迭代地改进预测器，实现了对机组组合的最佳操作。

    

    通常，系统操作员在开环预测优化过程中进行电力系统的经济运行：首先预测可再生能源(RES)的可用性和系统储备需求；根据这些预测，系统操作员解决诸如机组组合(UC)的优化模型，以确定相应的经济运行计划。然而，这种开环过程可能会实质性地损害操作经济性，因为它的预测器目光短浅地寻求改善即时的统计预测误差，而不是最终的操作成本。为此，本文提出了一个闭环预测优化框架，提供一种预测机组组合以改善操作经济性的方法。首先，利用双层混合整数规划模型针对最佳系统操作训练成本导向的预测器。上层基于其引起的操作成本来训练 RES 和储备预测器；下层则在给定预测的 RES 和储备的情况下，依据最佳操作原则求解 UC。这两个层级通过反馈环路进行交互性互动，直到收敛为止。在修改后的IEEE 24-bus系统上的数值实验表明，与三种最先进的 UC 基准线相比，所提出的框架具有高效性和有效性。

    Generally, system operators conduct the economic operation of power systems in an open-loop predict-then-optimize process: the renewable energy source (RES) availability and system reserve requirements are first predicted; given the predictions, system operators solve optimization models such as unit commitment (UC) to determine the economical operation plans accordingly. However, such an open-loop process could essentially compromise the operation economics because its predictors myopically seek to improve the immediate statistical prediction errors instead of the ultimate operation cost. To this end, this paper presents a closed-loop predict-and-optimize framework, offering a prescriptive UC to improve the operation economics. First, a bilevel mixed-integer programming model is leveraged to train cost-oriented predictors tailored for optimal system operations: the upper level trains the RES and reserve predictors based on their induced operation cost; the lower level, with given pred
    
[^83]: 基于分类重新参数化技巧的回译端到端训练

    End-to-End Training for Back-Translation with Categorical Reparameterization Trick. (arXiv:2202.08465v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2202.08465](http://arxiv.org/abs/2202.08465)

    本文提出了一种基于分类重新参数化技巧的回译端到端训练方法，来有效地减少两个神经机器翻译模型间离散属性的影响，从而实现端到端式的训练，获得了比以前基准测试更好的BLEU分数。

    

    回译是一种在神经机器翻译中有效的半监督学习框架。预先训练的神经机器翻译模型翻译单语句子并生成合成的双语句对以训练另一个神经机器翻译模型，反之亦然。将两个神经机器翻译模型分别理解为推理和生成模型。以往的研究采用了变分自动编码器（VAE）的培训框架。但是，由于翻译句子的离散属性使得梯度信息无法在两个NMT模型之间流动。本文提出了一种分类重新参数化技巧，使得神经机器翻译模型能够生成可微分的句子，使得VAE的训练框架可以以端到端方式工作。我们的实验表明，我们的方法有效地训练了NMT模型，并在WMT翻译任务的数据集上取得比以前基准测试更好的BLEU分数。

    Back-translation is an effective semi-supervised learning framework in neural machine translation (NMT). A pre-trained NMT model translates monolingual sentences and makes synthetic bilingual sentence pairs for the training of the other NMT model, and vice versa. Understanding the two NMT models as inference and generation models, respectively, previous works applied the training framework of variational auto-encoder (VAE). However, the discrete property of translated sentences prevents gradient information from flowing between the two NMT models. In this paper, we propose a categorical reparameterization trick that makes NMT models generate differentiable sentences so that the VAE's training framework can work in the end-to-end fashion. Our experiments demonstrate that our method effectively trains the NMT models and achieves better BLEU scores than the previous baseline on the datasets of the WMT translation task.
    
[^84]: 神经分布式源编码

    Neural Distributed Source Coding. (arXiv:2106.02797v3 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2106.02797](http://arxiv.org/abs/2106.02797)

    这项研究提出了一种神经分布式源编码的框架，可以处理复杂的相关性并实现最先进的峰值信噪比。

    

    分布式源编码(DSC)是在没有相互关联的边际信息可供解码器使用的情况下对输入进行编码的任务。值得注意的是，Slepian和Wolf在1973年证明，没有访问边际信息的编码器可以渐近地实现与边际信息可用情况下相同的压缩率。虽然在这个领域有广泛的先前工作，但实践中的DSC一直局限于合成数据集和特定的相关结构。在这里，我们提出了一个对相关结构不可知且能够扩展到高维度的有损DSC框架。我们的方法不依赖于手工设计的源模型，而是利用条件向量量化变分自动编码器(VQ-VAE)来学习分布式编码器和解码器。我们在多个数据集上评估了我们的方法，并展示了我们的方法可以处理复杂的相关性，并实现了最先进的峰值信噪比(PSNR)。

    Distributed source coding (DSC) is the task of encoding an input in the absence of correlated side information that is only available to the decoder. Remarkably, Slepian and Wolf showed in 1973 that an encoder without access to the side information can asymptotically achieve the same compression rate as when the side information is available to it. While there is vast prior work on this topic, practical DSC has been limited to synthetic datasets and specific correlation structures. Here we present a framework for lossy DSC that is agnostic to the correlation structure and can scale to high dimensions. Rather than relying on hand-crafted source modeling, our method utilizes a conditional Vector-Quantized Variational Autoencoder (VQ-VAE) to learn the distributed encoder and decoder. We evaluate our method on multiple datasets and show that our method can handle complex correlations and achieves state-of-the-art PSNR.
    

