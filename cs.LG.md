# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Prompt-Singer: Controllable Singing-Voice-Synthesis with Natural Language Prompt](https://arxiv.org/abs/2403.11780) | 提出了Prompt-Singer，这是第一个能够用自然语言控制歌手性别、音域和音量的唱歌声音合成方法，采用了基于解码器的变压器模型架构和范围旋律解耦的音高表示方法。 |
| [^2] | [Rethinking Autoencoders for Medical Anomaly Detection from A Theoretical Perspective](https://arxiv.org/abs/2403.09303) | 该研究从理论角度为医学异常检测中基于自编码器的重建方法提供了基础，揭示了改进AE在异常检测中的关键在于最小化信息。 |
| [^3] | [MicroT: Low-Energy and Adaptive Models for MCUs](https://arxiv.org/abs/2403.08040) | MicroT是一个低能耗、多任务自适应模型框架，通过特征提取器和分类器的分离、模型优化和本地任务训练，在MCUs上实现了模型性能的提升和能耗的降低。 |
| [^4] | [Optimizing Negative Prompts for Enhanced Aesthetics and Fidelity in Text-To-Image Generation](https://arxiv.org/abs/2403.07605) | 提出NegOpt方法，通过监督微调和强化学习优化负面提示的生成，显著提高图像生成质量，超越其他方法并构建了负面提示数据集。 |
| [^5] | [Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine Unlearning](https://arxiv.org/abs/2403.07362) | 该论文从对抗的角度提出了一种新的机器遗忘评估方法，通过确定最具挑战性的数据子集，即最坏情况遗忘集，来增强对影响擦除的挑战。 |
| [^6] | [Overcoming Reward Overoptimization via Adversarial Policy Optimization with Lightweight Uncertainty Estimation](https://arxiv.org/abs/2403.05171) | 本论文提出了对抗策略优化（AdvPO）来解决强化学习领域中奖励过度优化的问题，通过量化奖励的不确定性，并围绕奖励模型预测的置信区间进行分布鲁棒的优化，从而有效缓解了该问题。 |
| [^7] | [Not all Layers of LLMs are Necessary during Inference](https://arxiv.org/abs/2403.02181) | 推理过程中，根据输入实例的不同难易程度，本文提出了一种名为AdaInfer的算法，可以自适应地使用浅层和深层，从而节省了计算资源。 |
| [^8] | [Probabilistically-sound beam search with masked language models](https://arxiv.org/abs/2402.15020) | 提出了在掩码语言模型上进行束搜索的概率健壮方法，表明其在多个领域中优于传统方法。 |
| [^9] | [Human Brain Exhibits Distinct Patterns When Listening to Fake Versus Real Audio: Preliminary Evidence](https://arxiv.org/abs/2402.14982) | 人类大脑对真实和虚假音频有不同的反应模式，与深度伪造音频检测算法不同，这为深度伪造音频检测等领域的未来研究方向提供了重要的初步证据。 |
| [^10] | [Robustness and Exploration of Variational and Machine Learning Approaches to Inverse Problems: An Overview](https://arxiv.org/abs/2402.12072) | 本论文概述了使用变分方法和机器学习解决成像中逆问题的方法，重点在于点估计器对抗性扰动下的鲁棒性以及探索数据一致解子空间以满足特定语义或纹理特性。 |
| [^11] | [Revisiting Experience Replayable Conditions](https://arxiv.org/abs/2402.10374) | 本文提出了更严格的“经验重放条件”，并揭示了政策改进的不稳定性因素，从而导出了相应的稳定化技巧，最终使得经验重放可应用于优势演员-评论家算法。 |
| [^12] | [EcoVal: An Efficient Data Valuation Framework for Machine Learning](https://arxiv.org/abs/2402.09288) | EcoVal是一种高效的机器学习数据估值框架，通过估计每个数据的内在和外在价值，实现了快速实用地估算机器学习模型数据的价值。 |
| [^13] | [Rethink Model Re-Basin and the Linear Mode Connectivity](https://arxiv.org/abs/2402.05966) | 本论文重新审视了模型重新基底的现象，并发现了现有匹配算法的不足。通过适当的重归一化，我们改进了匹配算法，并揭示了它与重归一化过程的相互作用。这为剪枝提供了新的理解，推动了一种轻量且有效的后剪枝插件的开发。 |
| [^14] | [Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks](https://arxiv.org/abs/2401.17263) | 该论文提出了一种鲁棒的提示优化算法（RPO）用于对抗语言模型的破解攻击，通过梯度优化来确保输出的无害性，并成功降低了攻击成功率。 |
| [^15] | [Working Backwards: Learning to Place by Picking](https://arxiv.org/abs/2312.02352) | 通过逆向抓取过程并利用拾取和放置问题的对称性，提出了一种通过拾取的放置方法，并用自主收集的演示直接训练策略，实现在接触受限环境下物体放置任务的自主收集和泛化。 |
| [^16] | [Machine learning-based analysis of glioma tissue sections: a review.](http://arxiv.org/abs/2401.15022) | 机器学习技术在胶质瘤组织切片分析中具有诊断和预测的潜力，当前研究聚焦于成人型弥漫性胶质瘤的苏木精和伊红染色组织切片，以及对该疾病的分类、分级、分子标记预测和生存预测等临床任务。 |
| [^17] | [Deep Learning in Physical Layer: Review on Data Driven End-to-End Communication Systems and their Enabling Semantic Applications.](http://arxiv.org/abs/2401.12800) | 这篇论文综述了物理层的深度学习在数据驱动的端到端通信系统中的应用，以及它们所支持的语义应用。通过深度学习的表示学习，这些系统表现出了增强的适应性和性能，能够理解和适应数据传输的上下文和意图。 |
| [^18] | [Parametric Matrix Models.](http://arxiv.org/abs/2401.11694) | 参数矩阵模型是一种通用机器学习算法，基于矩阵方程设计，通过简化基础方法进行近似解参数方程。它可以仅使用经验数据进行训练，适用于各种机器学习问题，并在计算框架内产生准确的结果。 |
| [^19] | [AutoChunk: Automated Activation Chunk for Memory-Efficient Long Sequence Inference.](http://arxiv.org/abs/2401.10652) | AutoChunk是一种自动和自适应的编译器系统，通过块策略有效地减少长序列推断的激活内存。 |
| [^20] | [Learning-Based Difficulty Calibration for Enhanced Membership Inference Attacks.](http://arxiv.org/abs/2401.04929) | 本文介绍了一种基于学习的难度校准的成员推理攻击方法，旨在显著提高低FPR下的TPR，以验证训练模型是否保护隐私。 |
| [^21] | [Physics-Informed with Power-Enhanced Residual Network for Interpolation and Inverse Problems.](http://arxiv.org/abs/2310.15690) | 本文介绍了一种名为增强型残差网络的新颖神经网络结构，通过在残差元素中添加幂次项提升了网络的表达能力，具有卓越的准确性和应用性能，尤其适用于非平滑函数的处理。同时，该网络结构在解决反问题方面也表现出卓越的性能。 |
| [^22] | [A representation learning approach to probe for dynamical dark energy in matter power spectra.](http://arxiv.org/abs/2310.10717) | 这项研究提出了一种使用变分自编码器（VAE）架构进行表示学习的方法，用于在观测研究中探测宇宙大尺度结构中的动态暗能量模型。通过只使用一个潜在参数，可以预测到95%（99%）的DE功率谱，在考虑宇宙方差的高斯误差范围内具有很好的准确性。 |
| [^23] | [Decoding Human Activities: Analyzing Wearable Accelerometer and Gyroscope Data for Activity Recognition.](http://arxiv.org/abs/2310.02011) | 本文提出了一种用于活动识别的分层多结构方法，利用残差网络和残差MobileNet对静态和动态活动进行分类，然后通过加权合奏方法进行集成。 |
| [^24] | [A comparative study of Grid and Natural sentences effects on Normal-to-Lombard conversion.](http://arxiv.org/abs/2309.10485) | 本文通过比较Grid句子和自然句子在Lombard效应和Normal-to-Lombard转换方面的表现，发现随着噪声水平的增加，Grid句子的alpha比例增加更大。在实验中，基于EMALG训练的StarGAN模型在主观可懂度评估中一致表现优于其他模型。 |
| [^25] | [Learning Regionalization within a Differentiable High-Resolution Hydrological Model using Accurate Spatial Cost Gradients.](http://arxiv.org/abs/2308.02040) | 本文介绍了一种使用准确的空间成本梯度在可微的高分辨率水文模型中学习划分区域化模型的方法，该方法利用可学习的区域化映射，结合数据同化和参数校正，能够在广泛的时空计算域中利用不同数据集进行水文参数估计。 |
| [^26] | [Non-Asymptotic Performance of Social Machine Learning Under Limited Data.](http://arxiv.org/abs/2306.09397) | 本文研究了限制数据下社交机器学习的概率误差问题，并提出了一种更强的一致性训练条件，推导出了两种任务的概率误差上界。 |
| [^27] | [Representation Learning on Hyper-Relational and Numeric Knowledge Graphs with Transformers.](http://arxiv.org/abs/2305.18256) | 本文提出了一个名为HyNT的框架，用于学习超关系型知识图的表示，包括数值文字。该框架使用上下文Transformer和预测Transformer，通过学习三元组和其限定词之间的相关性以及数值信息来获得模型。 |
| [^28] | [TSGM: A Flexible Framework for Generative Modeling of Synthetic Time Series.](http://arxiv.org/abs/2305.11567) | TSGM提供了一种生成合成时间序列数据的灵活框架，使研究人员能够快速实现自己的方法并在可共享的环境中进行比较，从而有助于生成大规模的合成时间序列数据集，以用于训练和验证各种机器学习模型。 |
| [^29] | [SmartChoices: Augmenting Software with Learned Implementations.](http://arxiv.org/abs/2304.13033) | SmartChoices 提出了一种将机器学习与现有软件系统轻松、安全、有效地结合的新方法。 |
| [^30] | [Localisation of Regularised and Multiview Support Vector Machine Learning.](http://arxiv.org/abs/2304.05655) | 本文针对正则化和多视角支持向量机学习问题的本地化版本，证明了一些表示定理，研究了与损失函数和输入空间维度相关的特殊情况，特别是损失函数为 Gâteaux 可微函数时的情况。 |
| [^31] | [Calibrating Transformers via Sparse Gaussian Processes.](http://arxiv.org/abs/2303.02444) | 提出了一种通过Sparse Gaussian Process attention (SGPA)来校准Transformer模型不确定性的方法。在文本、图像和图形的预测任务中，SGPA-based Transformers在预测准确性上表现出竞争力，并显著改善了内分布校准和外分布的鲁棒性和检测能力。 |
| [^32] | [AUTOLYCUS: Exploiting Explainable AI (XAI) for Model Extraction Attacks against White-Box Models.](http://arxiv.org/abs/2302.02162) | 本文探究了可解释人工智能（XAI）工具对机器学习模型提取攻击的风险，并提出了一种模型提取攻击方法AUTOLYCUS。 |
| [^33] | [Does CLIP Know My Face?.](http://arxiv.org/abs/2209.07341) | 本文提出了一种新方法IDIA来评估视觉语言模型的隐私，大规模实验表明使用于训练的个人可以被非常高的准确率识别出来，表明需要更好地解决视觉语言模型中的隐私问题。 |
| [^34] | [Bucketized Active Sampling for Learning ACOPF.](http://arxiv.org/abs/2208.07497) | 本文提出了一种新颖的主动学习框架——分桶主动采样（BAS），旨在在时间限制内训练最佳的OPF代理。BAS将输入分布分成桶，并使用收集函数确定下一次采样的位置。实验结果显示了BAS的好处。 |

# 详细

[^1]: Prompt-Singer: 带自然语言提示的可控唱歌声音合成

    Prompt-Singer: Controllable Singing-Voice-Synthesis with Natural Language Prompt

    [https://arxiv.org/abs/2403.11780](https://arxiv.org/abs/2403.11780)

    提出了Prompt-Singer，这是第一个能够用自然语言控制歌手性别、音域和音量的唱歌声音合成方法，采用了基于解码器的变压器模型架构和范围旋律解耦的音高表示方法。

    

    近期的唱歌声音合成(SVS)方法取得了显著的音频质量和自然度，然而它们缺乏显式控制合成唱歌风格属性的能力。我们提出Prompt-Singer，这是第一个能够用自然语言控制歌手性别、音域和音量的SVS方法。我们采用基于仅解码器的变压器模型架构，具有多尺度层次结构，并设计了一个分离音高表示的范围旋律解耦的方法，从而实现了基于文本的音域控制同时保持了旋律准确性。此外，我们探索了各种实验设置，包括不同类型的文本表示，文本编码器微调，以及引入语音数据以减轻数据稀缺性，旨在促进进一步研究。实验证明，我们的模型具有良好的控制能力和音频质量。音频示例可访问 http://prompt-singer.

    arXiv:2403.11780v1 Announce Type: cross  Abstract: Recent singing-voice-synthesis (SVS) methods have achieved remarkable audio quality and naturalness, yet they lack the capability to control the style attributes of the synthesized singing explicitly. We propose Prompt-Singer, the first SVS method that enables attribute controlling on singer gender, vocal range and volume with natural language. We adopt a model architecture based on a decoder-only transformer with a multi-scale hierarchy, and design a range-melody decoupled pitch representation that enables text-conditioned vocal range control while keeping melodic accuracy. Furthermore, we explore various experiment settings, including different types of text representations, text encoder fine-tuning, and introducing speech data to alleviate data scarcity, aiming to facilitate further research. Experiments show that our model achieves favorable controlling ability and audio quality. Audio samples are available at http://prompt-singer.
    
[^2]: 用理论视角重新思考医学异常检测中的自编码器

    Rethinking Autoencoders for Medical Anomaly Detection from A Theoretical Perspective

    [https://arxiv.org/abs/2403.09303](https://arxiv.org/abs/2403.09303)

    该研究从理论角度为医学异常检测中基于自编码器的重建方法提供了基础，揭示了改进AE在异常检测中的关键在于最小化信息。

    

    医学异常检测旨在仅使用正常训练数据识别异常发现，对健康筛查和识别罕见疾病至关重要。基于重建的方法，特别是利用自编码器（AEs）的方法在这一领域占主导地位。它们基于这样的假设工作：仅使用正常数据训练的AEs不能很好地重建看不见的异常区域，从而实现基于重建错误的异常检测。然而，由于重建训练目标与异常检测任务目标之间的不匹配，这一假设并不总是成立，使得这些方法在理论上不够合理。该研究侧重于为基于AE的重建方法在异常检测中提供理论基础。通过利用信息论，我们阐明了这些方法的原则，并揭示了改进AE在异常检测中的关键在于最小化信息。

    arXiv:2403.09303v1 Announce Type: new  Abstract: Medical anomaly detection aims to identify abnormal findings using only normal training data, playing a crucial role in health screening and recognizing rare diseases. Reconstruction-based methods, particularly those utilizing autoencoders (AEs), are dominant in this field. They work under the assumption that AEs trained on only normal data cannot reconstruct unseen abnormal regions well, thereby enabling the anomaly detection based on reconstruction errors. However, this assumption does not always hold due to the mismatch between the reconstruction training objective and the anomaly detection task objective, rendering these methods theoretically unsound. This study focuses on providing a theoretical foundation for AE-based reconstruction methods in anomaly detection. By leveraging information theory, we elucidate the principles of these methods and reveal that the key to improving AE in anomaly detection lies in minimizing the informati
    
[^3]: MicroT：用于MCUs的低能耗和自适应模型

    MicroT: Low-Energy and Adaptive Models for MCUs

    [https://arxiv.org/abs/2403.08040](https://arxiv.org/abs/2403.08040)

    MicroT是一个低能耗、多任务自适应模型框架，通过特征提取器和分类器的分离、模型优化和本地任务训练，在MCUs上实现了模型性能的提升和能耗的降低。

    

    我们提出了MicroT，这是一个面向资源受限的MCUs的低能耗、多任务自适应模型框架。我们将原始模型划分为特征提取器和分类器。特征提取器通过自监督知识蒸馏获得，并通过模型分割和联合训练进一步优化为部分模型和完整模型。然后将这些模型部署在MCUs上，增加并在本地任务上训练分类器，最终执行关节推理的阶段决策。在这个过程中，部分模型最初处理样本，如果置信度得分低于设定的阈值，完整模型将恢复并继续推理。我们在两个模型、三个数据集和两个MCU板上评估了MicroT。我们的实验评估表明，在处理多个本地任务时，MicroT有效地提高了模型性能并降低了能耗。与未经优化的特征提取器相比，MicroT

    arXiv:2403.08040v1 Announce Type: new  Abstract: We propose MicroT, a low-energy, multi-task adaptive model framework for resource-constrained MCUs. We divide the original model into a feature extractor and a classifier. The feature extractor is obtained through self-supervised knowledge distillation and further optimized into part and full models through model splitting and joint training. These models are then deployed on MCUs, with classifiers added and trained on local tasks, ultimately performing stage-decision for joint inference. In this process, the part model initially processes the sample, and if the confidence score falls below the set threshold, the full model will resume and continue the inference. We evaluate MicroT on two models, three datasets, and two MCU boards. Our experimental evaluation shows that MicroT effectively improves model performance and reduces energy consumption when dealing with multiple local tasks. Compared to the unoptimized feature extractor, MicroT
    
[^4]: 优化负面提示以增强文本到图像生成中的美学和保真度

    Optimizing Negative Prompts for Enhanced Aesthetics and Fidelity in Text-To-Image Generation

    [https://arxiv.org/abs/2403.07605](https://arxiv.org/abs/2403.07605)

    提出NegOpt方法，通过监督微调和强化学习优化负面提示的生成，显著提高图像生成质量，超越其他方法并构建了负面提示数据集。

    

    在文本到图像生成中，使用描述不良图像特征的负面提示可以显著提高图像质量。然而，生成良好的负面提示是一项手工而繁琐的工作。为了解决这个问题，我们提出了NegOpt，一种新颖的方法，通过监督微调和强化学习来优化负面提示生成，从而增强图像生成。我们的综合方法相对于其他方法大幅提高了25%的Inception Score，并超越了来自测试集的标准负面提示。此外，使用NegOpt，我们可以有选择地优化对我们最重要的指标。最后，我们构建了负面提示数据集Negative Prompts DB。

    arXiv:2403.07605v1 Announce Type: cross  Abstract: In text-to-image generation, using negative prompts, which describe undesirable image characteristics, can significantly boost image quality. However, producing good negative prompts is manual and tedious. To address this, we propose NegOpt, a novel method for optimizing negative prompt generation toward enhanced image generation, using supervised fine-tuning and reinforcement learning. Our combined approach results in a substantial increase of 25% in Inception Score compared to other approaches and surpasses ground-truth negative prompts from the test set. Furthermore, with NegOpt we can preferentially optimize the metrics most important to us. Finally, we construct Negative Prompts DB, a dataset of negative prompts.
    
[^5]: 挑战遗忘：揭示机器遗忘中最坏情况遗忘集

    Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine Unlearning

    [https://arxiv.org/abs/2403.07362](https://arxiv.org/abs/2403.07362)

    该论文从对抗的角度提出了一种新的机器遗忘评估方法，通过确定最具挑战性的数据子集，即最坏情况遗忘集，来增强对影响擦除的挑战。

    

    靠谱的机器学习(Machine Learning, ML)社区越来越认识到模型在训练后有选择性地“遗忘”数据点的重要性。这引出了机器遗忘(Machine Unlearning, MU)问题，旨在消除选定数据点对模型性能的影响，同时仍保持模型在遗忘后的实用性。尽管有各种MU方法来擦除数据影响，评估主要集中在随机数据遗忘上，忽视了对于真实衡量遗忘性能的数据子集选择的重要探究。为解决这一问题，我们从对抗的角度引入了一种新的MU评估视角。我们提出确定那些对影响擦除构成最大挑战的数据子集，即找出最坏情况遗忘集。利用双层优化原则，我们增强了在上层优化中的遗忘挑战。

    arXiv:2403.07362v1 Announce Type: cross  Abstract: The trustworthy machine learning (ML) community is increasingly recognizing the crucial need for models capable of selectively 'unlearning' data points after training. This leads to the problem of machine unlearning (MU), aiming to eliminate the influence of chosen data points on model performance, while still maintaining the model's utility post-unlearning. Despite various MU methods for data influence erasure, evaluations have largely focused on random data forgetting, ignoring the vital inquiry into which subset should be chosen to truly gauge the authenticity of unlearning performance. To tackle this issue, we introduce a new evaluative angle for MU from an adversarial viewpoint. We propose identifying the data subset that presents the most significant challenge for influence erasure, i.e., pinpointing the worst-case forget set. Utilizing a bi-level optimization principle, we amplify unlearning challenges at the upper optimization 
    
[^6]: 通过轻量级不确定性估计对抗策略优化克服了奖励过度优化问题

    Overcoming Reward Overoptimization via Adversarial Policy Optimization with Lightweight Uncertainty Estimation

    [https://arxiv.org/abs/2403.05171](https://arxiv.org/abs/2403.05171)

    本论文提出了对抗策略优化（AdvPO）来解决强化学习领域中奖励过度优化的问题，通过量化奖励的不确定性，并围绕奖励模型预测的置信区间进行分布鲁棒的优化，从而有效缓解了该问题。

    

    我们引入了对抗策略优化（AdvPO），这是一种新颖的解决方案，用于解决强化学习从人类反馈中的奖励过度优化问题，适用于大型语言模型（LLMs）。AdvPO围绕奖励模型预测的置信区间解决了一个分布鲁棒的优化问题，以改进策略。通过对Anthropic HH和TL;DR摘要数据集进行全面实验，我们展示了AdvPO在减轻过度优化问题方面的有效性。

    arXiv:2403.05171v1 Announce Type: cross  Abstract: We introduce Adversarial Policy Optimization (AdvPO), a novel solution to the pervasive issue of reward over-optimization in Reinforcement Learning from Human Feedback (RLHF) for Large Language Models (LLMs). Over-optimization occurs when a reward model serves as an imperfect proxy for human preference, and RL-driven policy optimization erroneously exploits reward inaccuracies. In this paper, we begin by introducing a lightweight way to quantify uncertainties in rewards, relying solely on the last layer embeddings of the reward model, without the need for computationally expensive reward ensembles. AdvPO then addresses a distributionally robust optimization problem centred around the confidence interval of the reward model's predictions for policy improvement. Through comprehensive experiments on the Anthropic HH and TL;DR summarization datasets, we illustrate the efficacy of AdvPO in mitigating the overoptimization issue, consequently
    
[^7]: 推理过程中不是所有LLMs的层都是必要的

    Not all Layers of LLMs are Necessary during Inference

    [https://arxiv.org/abs/2403.02181](https://arxiv.org/abs/2403.02181)

    推理过程中，根据输入实例的不同难易程度，本文提出了一种名为AdaInfer的算法，可以自适应地使用浅层和深层，从而节省了计算资源。

    

    大型语言模型（LLMs）的推理阶段非常昂贵。理想的LLMs推理阶段可以利用更少的计算资源，同时仍保持其能力（例如泛化和上下文学习能力）。本文尝试回答一个问题：“在LLMs推理过程中，我们可以为简单实例使用浅层，并为难以处理的实例使用深层吗？”为了回答这个问题，我们首先通过统计分析跨任务激活的层来指出并非所有层在推理过程中都是必要的。然后，我们提出了一种简单的算法，名为AdaInfer，根据输入实例自适应地确定推理终止时刻。更重要的是，AdaInfer不改变LLMs参数，并在任务之间保持泛化能力。对知名LLMs（即Llama2系列和OPT）的实验证明，AdaInfer节省了平均14.8%的计算资源，甚至在情感方面高达50%。

    arXiv:2403.02181v1 Announce Type: cross  Abstract: The inference phase of Large Language Models (LLMs) is very expensive. An ideal inference stage of LLMs could utilize fewer computational resources while still maintaining its capabilities (e.g., generalization and in-context learning ability). In this paper, we try to answer the question, "During LLM inference, can we use shallow layers for easy instances; and deep layers for hard ones?" To answer this question, we first indicate that Not all Layers are Necessary during Inference by statistically analyzing the activated layers across tasks. Then, we propose a simple algorithm named AdaInfer to determine the inference termination moment based on the input instance adaptively. More importantly, AdaInfer does not alter LLM parameters and maintains generalizability across tasks. Experiments on well-known LLMs (i.e., Llama2 series and OPT) show that AdaInfer saves an average of 14.8% of computational resources, even up to 50% on sentiment 
    
[^8]: 具有掩码语言模型的概率健壮束搜索

    Probabilistically-sound beam search with masked language models

    [https://arxiv.org/abs/2402.15020](https://arxiv.org/abs/2402.15020)

    提出了在掩码语言模型上进行束搜索的概率健壮方法，表明其在多个领域中优于传统方法。

    

    具有掩码语言模型（MLMs）的束搜索存在挑战，部分原因是由于序列的联合概率分布不像自回归模型那样readily available。然而，估算这样的分布在许多领域中具有应用，包括蛋白工程和古代文本恢复。我们提出了一种具有概率健壮性的使用MLMs进行束搜索的方法。首先，我们阐明了在哪些条件下使用标准束搜索对MLMs执行文本填充在理论上是可靠的。当这些条件失败时，我们提供了一种具有概率健壮性的修改，而且无需额外的计算复杂性，并且证明在预期条件下它优于前述的束搜索。然后，我们提出了比较多个领域中几种使用MLMs进行填充的方法的经验结果。

    arXiv:2402.15020v1 Announce Type: cross  Abstract: Beam search with masked language models (MLMs) is challenging in part because joint probability distributions over sequences are not readily available, unlike for autoregressive models. Nevertheless, estimating such distributions has applications in many domains, including protein engineering and ancient text restoration. We present probabilistically-sound methods for beam search with MLMs. First, we clarify the conditions under which it is theoretically sound to perform text infilling with MLMs using standard beam search. When these conditions fail, we provide a probabilistically-sound modification with no additional computational complexity and demonstrate that it is superior to the aforementioned beam search in the expected conditions. We then present empirical results comparing several infilling approaches with MLMs across several domains.
    
[^9]: 人类大脑在听取真实和虚假音频时展现出不同模式：初步证据

    Human Brain Exhibits Distinct Patterns When Listening to Fake Versus Real Audio: Preliminary Evidence

    [https://arxiv.org/abs/2402.14982](https://arxiv.org/abs/2402.14982)

    人类大脑对真实和虚假音频有不同的反应模式，与深度伪造音频检测算法不同，这为深度伪造音频检测等领域的未来研究方向提供了重要的初步证据。

    

    本文研究了人类听取真实和虚假音频时大脑活动的变化。我们的初步结果表明，一种最先进的深度伪造音频检测算法所学习的表示，并没有显示出真实和虚假音频之间的清晰不同模式。相反，人类大脑活动，通过 EEG 测量，在个体接触虚假与真实音频时显示出不同的模式。这些初步证据为未来在深度伪造音频检测等领域提供了研究方向。

    arXiv:2402.14982v1 Announce Type: cross  Abstract: In this paper we study the variations in human brain activity when listening to real and fake audio. Our preliminary results suggest that the representations learned by a state-of-the-art deepfake audio detection algorithm, do not exhibit clear distinct patterns between real and fake audio. In contrast, human brain activity, as measured by EEG, displays distinct patterns when individuals are exposed to fake versus real audio. This preliminary evidence enables future research directions in areas such as deepfake audio detection.
    
[^10]: 变分方法与机器学习方法在逆问题中的鲁棒性和探索：概述

    Robustness and Exploration of Variational and Machine Learning Approaches to Inverse Problems: An Overview

    [https://arxiv.org/abs/2402.12072](https://arxiv.org/abs/2402.12072)

    本论文概述了使用变分方法和机器学习解决成像中逆问题的方法，重点在于点估计器对抗性扰动下的鲁棒性以及探索数据一致解子空间以满足特定语义或纹理特性。

    

    本文试图概述使用变分方法和机器学习来解决成像中逆问题的当前方法。重点关注点估计器及其对抗性扰动下的鲁棒性。此外，通过一维示例问题的数值实验结果，展示了不同方法的鲁棒性并在经验上验证了理论保证。该综述的另一个重点是通过明确指导来探索数据一致解的子空间，以满足特定语义或纹理特性。

    arXiv:2402.12072v1 Announce Type: cross  Abstract: This paper attempts to provide an overview of current approaches for solving inverse problems in imaging using variational methods and machine learning. A special focus lies on point estimators and their robustness against adversarial perturbations. In this context results of numerical experiments for a one-dimensional toy problem are provided, showing the robustness of different approaches and empirically verifying theoretical guarantees. Another focus of this review is the exploration of the subspace of data consistent solutions through explicit guidance to satisfy specific semantic or textural properties.
    
[^11]: 重新审视经验重放条件

    Revisiting Experience Replayable Conditions

    [https://arxiv.org/abs/2402.10374](https://arxiv.org/abs/2402.10374)

    本文提出了更严格的“经验重放条件”，并揭示了政策改进的不稳定性因素，从而导出了相应的稳定化技巧，最终使得经验重放可应用于优势演员-评论家算法。

    

    深度强化学习中使用的经验重放（ER）被认为仅适用于离策略算法。然而，已经有一些情况下将ER应用于在策略算法中，这表明离策略性可能是应用ER的充分条件。本文重新审视了更严格的“经验重放条件”（ERC），并提出了修改现有算法以满足ERC的方法。为此，假设政策改进的不稳定性是ERC的关键。从度量学习的角度揭示了不稳定因素，包括i）来自负样本的排斥力和ii）不当经验的重放。因此，导出了相应的稳定化技巧。通过数值模拟验证，所提出的稳定化技巧使得ER适用于优势演员-评论家算法，一种在策略算法。此外，其学习

    arXiv:2402.10374v1 Announce Type: new  Abstract: Experience replay (ER) used in (deep) reinforcement learning is considered to be applicable only to off-policy algorithms. However, there have been some cases in which ER has been applied for on-policy algorithms, suggesting that off-policyness might be a sufficient condition for applying ER. This paper reconsiders more strict "experience replayable conditions" (ERC) and proposes the way of modifying the existing algorithms to satisfy ERC. To this end, instability of policy improvements is assumed to be a key in ERC. The instability factors are revealed from the viewpoint of metric learning as i) repulsive forces from negative samples and ii) replays of inappropriate experiences. Accordingly, the corresponding stabilization tricks are derived. As a result, it is confirmed through numerical simulations that the proposed stabilization tricks make ER applicable to an advantage actor-critic, an on-policy algorithm. In addition, its learning 
    
[^12]: EcoVal:一种高效的机器学习数据估值框架

    EcoVal: An Efficient Data Valuation Framework for Machine Learning

    [https://arxiv.org/abs/2402.09288](https://arxiv.org/abs/2402.09288)

    EcoVal是一种高效的机器学习数据估值框架，通过估计每个数据的内在和外在价值，实现了快速实用地估算机器学习模型数据的价值。

    

    在机器学习工作流中量化数据的价值可以在机器学习倡议中做出更具战略意义的决策中起到关键作用。现有的基于Shapley值的机器学习数据估值框架在计算方面非常昂贵，因为需要大量重复训练模型才能获得Shapley值。在本文中，我们介绍了一种高效的数据估值框架EcoVal，以快速实用的方式估算机器学习模型数据的价值。我们不直接处理独立的数据样本，而是确定类似的数据点簇的价值。这个价值进一步在所有成员簇点之间传播。我们展示了可以通过估计每个数据的内在和外在价值来确定整体数据价值。这是通过将模型的性能建模为“生产函数”来实现的，这是一个非常重要的概念。

    arXiv:2402.09288v1 Announce Type: new Abstract: Quantifying the value of data within a machine learning workflow can play a pivotal role in making more strategic decisions in machine learning initiatives. The existing Shapley value based frameworks for data valuation in machine learning are computationally expensive as they require considerable amount of repeated training of the model to obtain the Shapley value. In this paper, we introduce an efficient data valuation framework EcoVal, to estimate the value of data for machine learning models in a fast and practical manner. Instead of directly working with individual data sample, we determine the value of a cluster of similar data points. This value is further propagated amongst all the member cluster points. We show that the overall data value can be determined by estimating the intrinsic and extrinsic value of each data. This is enabled by formulating the performance of a model as a \textit{production function}, a concept which is po
    
[^13]: 重新思考模型重新基底和线性模态连接性

    Rethink Model Re-Basin and the Linear Mode Connectivity

    [https://arxiv.org/abs/2402.05966](https://arxiv.org/abs/2402.05966)

    本论文重新审视了模型重新基底的现象，并发现了现有匹配算法的不足。通过适当的重归一化，我们改进了匹配算法，并揭示了它与重归一化过程的相互作用。这为剪枝提供了新的理解，推动了一种轻量且有效的后剪枝插件的开发。

    

    最近的研究表明，对于足够宽的模型来说，大部分随机梯度下降（SGD）的解可以收敛到相同的基底，只是顺序可能不同。这种现象被称为模型重新基底的阶段，对于模型平均化有重要影响。然而，当前的重新基底策略在效果上存在局限性，因为对底层机制的理解不够全面。为了填补这一空白，我们的研究重新审视了标准做法，并揭示了现有匹配算法的频繁不足之处，我们通过适当的重归一化来缓解这些问题。通过引入更直接的分析方法，我们揭示了匹配算法与重归一化过程之间的相互作用。这种观点不仅澄清和改进了以前的研究结果，还促进了新的洞见。例如，它将线性模态连接性与剪枝联系起来，从而激发了一种轻量且有效的后剪枝插件，可以直接与任何现有的剪枝技术合并。

    Recent studies suggest that with sufficiently wide models, most SGD solutions can, up to permutation, converge into the same basin. This phenomenon, known as the model re-basin regime, has significant implications for model averaging. However, current re-basin strategies are limited in effectiveness due to a lack of comprehensive understanding of underlying mechanisms. Addressing this gap, our work revisits standard practices and uncovers the frequent inadequacies of existing matching algorithms, which we show can be mitigated through proper re-normalization. By introducing a more direct analytical approach, we expose the interaction between matching algorithms and re-normalization processes. This perspective not only clarifies and refines previous findings but also facilitates novel insights. For instance, it connects the linear mode connectivity to pruning, motivating a lightweight yet effective post-pruning plug-in that can be directly merged with any existing pruning techniques. Ou
    
[^14]: 鲁棒的提示优化用于对抗语言模型的破解攻击

    Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks

    [https://arxiv.org/abs/2401.17263](https://arxiv.org/abs/2401.17263)

    该论文提出了一种鲁棒的提示优化算法（RPO）用于对抗语言模型的破解攻击，通过梯度优化来确保输出的无害性，并成功降低了攻击成功率。

    

    尽管在人工智能对齐方面取得了一些进展，但语言模型（LM）仍然容易受到对抗性攻击或破解攻击的影响，其中对手修改输入提示以诱导有害行为。虽然已经提出了一些防御方法，但它们仅关注狭窄的威胁模型，并不能提供强大的防御。为了实现强大的防御，我们首次提出了用于对抗破解攻击的对抗目标，并提出了一种名为鲁棒提示优化（RPO）的算法，该算法利用基于梯度的令牌优化来确保输出的无害性。通过这种方法，我们得到了一个易于访问的后缀，显著改善了对破解攻击的强韧性，包括优化过程中出现的破解攻击以及未知的破解攻击，将攻击成功率从84%降低到8.66%，在20个破解攻击中。此外，我们还发现RPO对正常LM使用的影响较小，在适应性攻击下仍然有效，并且可以迁移到黑盒模型中，降低攻击成功率。

    Despite advances in AI alignment, language models (LM) remain vulnerable to adversarial attacks or jailbreaking, in which adversaries modify input prompts to induce harmful behavior. While some defenses have been proposed, they focus on narrow threat models and fall short of a strong defense, which we posit should be effective, universal, and practical. To achieve this, we propose the first adversarial objective for defending LMs against jailbreaking attacks and an algorithm, robust prompt optimization (RPO), that uses gradient-based token optimization to enforce harmless outputs. This results in an easily accessible suffix that significantly improves robustness to both jailbreaks seen during optimization and unknown, held-out jailbreaks, reducing the attack success rate on Starling-7B from 84% to 8.66% across 20 jailbreaks. In addition, we find that RPO has a minor effect on normal LM use, is successful under adaptive attacks, and can transfer to black-box models, reducing the success
    
[^15]: 逆向学习：通过捡取学习放置

    Working Backwards: Learning to Place by Picking

    [https://arxiv.org/abs/2312.02352](https://arxiv.org/abs/2312.02352)

    通过逆向抓取过程并利用拾取和放置问题的对称性，提出了一种通过拾取的放置方法，并用自主收集的演示直接训练策略，实现在接触受限环境下物体放置任务的自主收集和泛化。

    

    我们提出了一种通过拾取（PvP）的放置方法，可以自主收集适用于一系列放置任务的现实世界演示，其中物体必须被操纵到特定的接触限制位置。通过PvP，我们通过颠倒抓取过程并利用拾取和放置问题固有的对称性，接近于机器人物体放置演示的收集。具体而言，我们从一组最初位于目标放置位置的物体的抓取序列中获得放置演示。我们的系统可以在接触受限环境中收集数百个演示，而无需人类干预，这是通过结合两个模块实现的：触觉重新抓取和用于抓取的顺从控制。我们通过行为克隆直接从视觉观察中通过自主收集的演示中训练策略。通过这样做，策略可以推广到超出训练环境范围的物体放置场景。

    arXiv:2312.02352v2 Announce Type: replace-cross  Abstract: We present placing via picking (PvP), a method to autonomously collect real-world demonstrations for a family of placing tasks in which objects must be manipulated to specific contact-constrained locations. With PvP, we approach the collection of robotic object placement demonstrations by reversing the grasping process and exploiting the inherent symmetry of the pick and place problems. Specifically, we obtain placing demonstrations from a set of grasp sequences of objects initially located at their target placement locations. Our system can collect hundreds of demonstrations in contact-constrained environments without human intervention by combining two modules: tactile regrasping and compliant control for grasps. We train a policy directly from visual observations through behavioral cloning, using the autonomously-collected demonstrations. By doing so, the policy can generalize to object placement scenarios outside of the tra
    
[^16]: 基于机器学习的胶质瘤组织切片分析：一项综述

    Machine learning-based analysis of glioma tissue sections: a review. (arXiv:2401.15022v1 [eess.IV])

    [http://arxiv.org/abs/2401.15022](http://arxiv.org/abs/2401.15022)

    机器学习技术在胶质瘤组织切片分析中具有诊断和预测的潜力，当前研究聚焦于成人型弥漫性胶质瘤的苏木精和伊红染色组织切片，以及对该疾病的分类、分级、分子标记预测和生存预测等临床任务。

    

    近年来，胶质瘤的诊断变得越来越复杂。使用现代机器学习技术对胶质瘤组织进行组织学评估，为诊断和预测结果提供了新的机会。为了对当前研究的现状进行概述，本综述对70个公开可得的研究论文进行了研究，这些论文关于使用机器学习分析染色的胶质瘤组织切片，涵盖了分类（16/70），分级（23/70），分子标记预测（13/70）和生存预测（27/70）等诊断任务。所有的研究都在方法学方面及其临床适用性方面进行了评估。发现当前研究的重点是对成人型弥漫性胶质瘤的苏木精和伊红染色组织切片进行评估。多数研究（49/70）基于公开的胶质母细胞瘤和低级别胶质瘤数据集，仅有少数研究使用其他数据集。

    In recent years, the diagnosis of gliomas has become increasingly complex. Histological assessment of glioma tissue using modern machine learning techniques offers new opportunities to support diagnosis and outcome prediction. To give an overview of the current state of research, this review examines 70 publicly available research studies on machine learning-based analysis of stained human glioma tissue sections, covering the diagnostic tasks of subtyping (16/70), grading (23/70), molecular marker prediction (13/70), and survival prediction (27/70). All studies were reviewed with regard to methodological aspects as well as clinical applicability. It was found that the focus of current research is the assessment of hematoxylin and eosin-stained tissue sections of adult-type diffuse gliomas. The majority of studies (49/70) are based on the publicly available glioblastoma and low-grade glioma datasets from The Cancer Genome Atlas (TCGA) and only a few studies employed other datasets in is
    
[^17]: 物理层的深度学习：数据驱动的端到端通信系统及其支持的语义应用综述

    Deep Learning in Physical Layer: Review on Data Driven End-to-End Communication Systems and their Enabling Semantic Applications. (arXiv:2401.12800v1 [cs.NI])

    [http://arxiv.org/abs/2401.12800](http://arxiv.org/abs/2401.12800)

    这篇论文综述了物理层的深度学习在数据驱动的端到端通信系统中的应用，以及它们所支持的语义应用。通过深度学习的表示学习，这些系统表现出了增强的适应性和性能，能够理解和适应数据传输的上下文和意图。

    

    深度学习已经通过数据驱动的端到端学习和优化物理层实现无线通信系统的范式转变。通过利用深度学习的表示学习，端到端系统在复杂的无线环境中表现出了增强的适应性和性能，满足了5G及其以上网络系统和应用的需求。数据驱动技术在物理层的发展使得在文本、图像、音频、视频和多模态传输等各种模态下实现了高级的语义应用。这些应用从传统的比特级通信转变为语义级智能通信系统，能够理解和适应数据传输的上下文和意图。虽然物理层作为数据驱动的端到端通信的深度学习架构是实现语义通信系统的关键因素，近年来有各种研究分别对它们进行了调查，

    Deep Learning (DL) has enabled a paradigm shift in wireless communication system with data driven end-to-end (E2E) learning and optimization of the Physical Layer (PHY). By leveraging the representation learning of DL, E2E systems exhibit enhanced adaptability and performance in complex wireless environments, fulfilling the demands of 5G and beyond network systems and applications. The evolution of data-driven techniques in the PHY has enabled advanced semantic applications across various modalities including text, image, audio, video, and multi-modal transmissions. These applications transcend from traditional bit-level communication to semantic-level intelligent communication systems, which are capable of understanding and adapting to the context and intent of the data transmission. Although PHY as a DL architecture for data-driven E2E communication is a key factor in enabling semantic communication systems (SemCom), and various studies in recent years have surveyed them separately, 
    
[^18]: 参数矩阵模型

    Parametric Matrix Models. (arXiv:2401.11694v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.11694](http://arxiv.org/abs/2401.11694)

    参数矩阵模型是一种通用机器学习算法，基于矩阵方程设计，通过简化基础方法进行近似解参数方程。它可以仅使用经验数据进行训练，适用于各种机器学习问题，并在计算框架内产生准确的结果。

    

    我们提出了一种称为参数矩阵模型的通用机器学习算法。参数矩阵模型基于矩阵方程，并且其设计受到了用于近似解参数方程的简化基础方法的效率启发。依赖变量可以隐式或显式定义，并且方程可以使用代数、微分或积分关系。参数矩阵模型可以仅使用经验数据进行训练，不需要高保真度模型计算。虽然最初设计用于科学计算，但参数矩阵模型是一种可以应用于通用机器学习问题的通用函数逼近器。在介绍基础理论之后，我们将参数矩阵模型应用于一系列不同的挑战，展示了它们在各种问题上的性能。对于所有在这里测试的挑战，参数矩阵模型在允许计算的框架内产生准确的结果。

    We present a general class of machine learning algorithms called parametric matrix models. Parametric matrix models are based on matrix equations, and the design is motivated by the efficiency of reduced basis methods for approximating solutions of parametric equations. The dependent variables can be defined implicitly or explicitly, and the equations may use algebraic, differential, or integral relations. Parametric matrix models can be trained with empirical data only, and no high-fidelity model calculations are needed. While originally designed for scientific computing, parametric matrix models are universal function approximators that can be applied to general machine learning problems. After introducing the underlying theory, we apply parametric matrix models to a series of different challenges that show their performance for a wide range of problems. For all the challenges tested here, parametric matrix models produce accurate results within a computational framework that allows 
    
[^19]: AutoChunk: 自动激活块用于内存高效的长序列推断

    AutoChunk: Automated Activation Chunk for Memory-Efficient Long Sequence Inference. (arXiv:2401.10652v1 [cs.PF])

    [http://arxiv.org/abs/2401.10652](http://arxiv.org/abs/2401.10652)

    AutoChunk是一种自动和自适应的编译器系统，通过块策略有效地减少长序列推断的激活内存。

    

    大型深度学习模型在各种应用中取得了令人瞩目的性能。然而，它们对内存的大量需求，包括参数内存和激活内存，已经成为实际应用中的重大挑战。现有方法主要处理参数内存，对激活内存的重要性却被忽视了。特别是对于长输入序列，随着序列长度的增加，激活内存预计会经历显著的指数增长。在这个方法中，我们提出了AutoChunk，一种自动和自适应的编译器系统，通过块策略有效地减少长序列推断的激活内存。所提出的系统通过多个阶段的优化生成块计划。在每个阶段，块搜索通过探索所有可能的块候选项，块选择通过识别最佳块进行。运行时，AutoChunk采用代码生成自动应用块策略。

    Large deep learning models have achieved impressive performance across a range of applications. However, their large memory requirements, including parameter memory and activation memory, have become a significant challenge for their practical serving. While existing methods mainly address parameter memory, the importance of activation memory has been overlooked. Especially for long input sequences, activation memory is expected to experience a significant exponential growth as the length of sequences increases. In this approach, we propose AutoChunk, an automatic and adaptive compiler system that efficiently reduces activation memory for long sequence inference by chunk strategies. The proposed system generates chunk plans by optimizing through multiple stages. In each stage, the chunk search pass explores all possible chunk candidates and the chunk selection pass identifies the optimal one. At runtime, AutoChunk employs code generation to automatically apply chunk strategies. The exp
    
[^20]: 基于学习的难度校准提升成员推理攻击的能力

    Learning-Based Difficulty Calibration for Enhanced Membership Inference Attacks. (arXiv:2401.04929v1 [cs.CR])

    [http://arxiv.org/abs/2401.04929](http://arxiv.org/abs/2401.04929)

    本文介绍了一种基于学习的难度校准的成员推理攻击方法，旨在显著提高低FPR下的TPR，以验证训练模型是否保护隐私。

    

    机器学习模型，特别是深度神经网络，目前是各种应用的重要组成部分，从医疗保健到金融。然而，使用敏感数据来训练这些模型引发了对隐私和安全的担忧。一种验证训练模型是否保护隐私的方法是成员推理攻击（MIA），它允许对手确定特定数据点是否是模型的训练数据集的一部分。虽然已经在文献中提出了一系列的MIA，但只有少数能够在低假阳性率（FPR）区域（0.01%~1%）实现较高的真阳性率（TPR）。这是实际应用于实际场景中的MIA必须考虑的关键因素。在本文中，我们提出了一种新颖的MIA方法，旨在显著提高低FPR的TPR。我们的方法名为基于学习的难度校准（LDC-MIA），通过使用神经网络分类器将数据记录以其难度级别进行表征。

    Machine learning models, in particular deep neural networks, are currently an integral part of various applications, from healthcare to finance. However, using sensitive data to train these models raises concerns about privacy and security. One method that has emerged to verify if the trained models are privacy-preserving is Membership Inference Attacks (MIA), which allows adversaries to determine whether a specific data point was part of a model's training dataset. While a series of MIAs have been proposed in the literature, only a few can achieve high True Positive Rates (TPR) in the low False Positive Rate (FPR) region (0.01%~1%). This is a crucial factor to consider for an MIA to be practically useful in real-world settings. In this paper, we present a novel approach to MIA that is aimed at significantly improving TPR at low FPRs. Our method, named learning-based difficulty calibration for MIA(LDC-MIA), characterizes data records by their hardness levels using a neural network clas
    
[^21]: 使用增强型残差网络进行插值和反问题的物理驱动方法

    Physics-Informed with Power-Enhanced Residual Network for Interpolation and Inverse Problems. (arXiv:2310.15690v1 [cs.LG])

    [http://arxiv.org/abs/2310.15690](http://arxiv.org/abs/2310.15690)

    本文介绍了一种名为增强型残差网络的新颖神经网络结构，通过在残差元素中添加幂次项提升了网络的表达能力，具有卓越的准确性和应用性能，尤其适用于非平滑函数的处理。同时，该网络结构在解决反问题方面也表现出卓越的性能。

    

    本文介绍了一种新颖的神经网络结构，称为增强型残差网络，旨在改善2D和3D环境下平滑和非平滑函数的插值能力。通过在残差元素中添加幂次项，该网络结构增强了网络的表达能力。研究探究了网络深度、宽度和优化方法，并展示了该网络结构的适应性和性能优势。结果一致表明，增强型残差网络在非平滑函数方面具有异常的准确性。实际示例也证实了其在准确性、收敛性和效率方面相对于普通神经网络的优越性。研究还探讨了更深层网络的影响。此外，提出的网络结构还应用于解决反Burgers方程问题，展示了优越的性能。总之，增强型残差网络提供了一种多功能的解决方案，明显提升了插值和反问题的能力。

    This paper introduces a novel neural network structure called the Power-Enhancing residual network, designed to improve interpolation capabilities for both smooth and non-smooth functions in 2D and 3D settings. By adding power terms to residual elements, the architecture boosts the network's expressive power. The study explores network depth, width, and optimization methods, showing the architecture's adaptability and performance advantages. Consistently, the results emphasize the exceptional accuracy of the proposed Power-Enhancing residual network, particularly for non-smooth functions. Real-world examples also confirm its superiority over plain neural network in terms of accuracy, convergence, and efficiency. The study also looks at the impact of deeper network. Moreover, the proposed architecture is also applied to solving the inverse Burgers' equation, demonstrating superior performance. In conclusion, the Power-Enhancing residual network offers a versatile solution that significa
    
[^22]: 一种表示学习方法用于探测宇宙大尺度结构中的动态暗能量

    A representation learning approach to probe for dynamical dark energy in matter power spectra. (arXiv:2310.10717v1 [astro-ph.CO])

    [http://arxiv.org/abs/2310.10717](http://arxiv.org/abs/2310.10717)

    这项研究提出了一种使用变分自编码器（VAE）架构进行表示学习的方法，用于在观测研究中探测宇宙大尺度结构中的动态暗能量模型。通过只使用一个潜在参数，可以预测到95%（99%）的DE功率谱，在考虑宇宙方差的高斯误差范围内具有很好的准确性。

    

    我们提出了DE-VAE，一种变分自编码器（VAE）架构，用于在宇宙大尺度结构的观测研究中寻找动态暗能量（DE）模型的压缩表示。DE-VAE在$.01-2.5 \ h/\rm{Mpc}$的波数$k$和四个红移值$z \in (0.1,0.48,0.78,1.5)$上训练了一个最典型的动态DE参数化的压缩表示，并且使用两个额外参数来描述演化的DE态方程。这些压缩表示与标准的冷暗物质（CDM）参数连接，并映射回重构的提升；压缩和重构组件都被参数化为神经网络。显著的是，我们发现一个单一的潜在参数足以在广泛的宇宙学参数范围内，预测到95%（99%）的DE功率谱，并且在包括宇宙方差的高斯误差的1$ \sigma$（2$ \sigma$）内。

    We present DE-VAE, a variational autoencoder (VAE) architecture to search for a compressed representation of dynamical dark energy (DE) models in observational studies of the cosmic large-scale structure. DE-VAE is trained on matter power spectra boosts generated at wavenumbers $k\in(0.01-2.5) \ h/\rm{Mpc}$ and at four redshift values $z\in(0.1,0.48,0.78,1.5)$ for the most typical dynamical DE parametrization with two extra parameters describing an evolving DE equation of state. The boosts are compressed to a lower-dimensional representation, which is concatenated with standard cold dark matter (CDM) parameters and then mapped back to reconstructed boosts; both the compression and the reconstruction components are parametrized as neural networks. Remarkably, we find that a single latent parameter is sufficient to predict 95% (99%) of DE power spectra generated over a broad range of cosmological parameters within $1\sigma$ ($2\sigma$) of a Gaussian error which includes cosmic variance, 
    
[^23]: 解码人类行为：分析可穿戴加速度计和陀螺仪数据进行活动识别

    Decoding Human Activities: Analyzing Wearable Accelerometer and Gyroscope Data for Activity Recognition. (arXiv:2310.02011v1 [cs.CV])

    [http://arxiv.org/abs/2310.02011](http://arxiv.org/abs/2310.02011)

    本文提出了一种用于活动识别的分层多结构方法，利用残差网络和残差MobileNet对静态和动态活动进行分类，然后通过加权合奏方法进行集成。

    

    一个人的运动或相对定位有效地产生了可以被计算机读取的原始电信号，通过应用各种操作技术来对不同的人类活动进行分类。本文提出了一种基于残差网络与残差MobileNet进行合奏的分层多结构方法，称为FusionActNet。所提出的方法涉及使用精心设计的残差块分别对静态和动态活动进行分类，因为它们具有明显而独特的特征。这些网络独立训练，得到两个专业的高精度模型。通过利用架构调整的算法优势，这些模型在特定超类中优秀地识别活动。然后，这两个残差网络通过加权合奏的残差MobileNet进行传递。随后，这个合奏能够有效区分一些特定的子类。

    A person's movement or relative positioning effectively generates raw electrical signals that can be read by computing machines to apply various manipulative techniques for the classification of different human activities. In this paper, a stratified multi-structural approach based on a Residual network ensembled with Residual MobileNet is proposed, termed as FusionActNet. The proposed method involves using carefully designed Residual blocks for classifying the static and dynamic activities separately because they have clear and distinct characteristics that set them apart. These networks are trained independently, resulting in two specialized and highly accurate models. These models excel at recognizing activities within a specific superclass by taking advantage of the unique algorithmic benefits of architectural adjustments. Afterward, these two ResNets are passed through a weighted ensemble-based Residual MobileNet. Subsequently, this ensemble proficiently discriminates between a sp
    
[^24]: Grid和自然语句对Normal-to-Lombard转换的比较研究

    A comparative study of Grid and Natural sentences effects on Normal-to-Lombard conversion. (arXiv:2309.10485v1 [cs.SD])

    [http://arxiv.org/abs/2309.10485](http://arxiv.org/abs/2309.10485)

    本文通过比较Grid句子和自然句子在Lombard效应和Normal-to-Lombard转换方面的表现，发现随着噪声水平的增加，Grid句子的alpha比例增加更大。在实验中，基于EMALG训练的StarGAN模型在主观可懂度评估中一致表现优于其他模型。

    

    Grid句子常用于研究Lombard效应和Normal-to-Lombard转换。然而，目前尚不清楚在真实应用中，基于Grid句子训练的Normal-to-Lombard模型是否足以提高自然语音可懂度。本文介绍了一个平行的Lombard语料库（称为Lombard Chinese TIMIT，LCT），并从中提取了中文TIMIT的自然句子。然后，我们使用LCT和Enhanced Mandarin Lombard Grid语料库（EMALG）比较了自然句子和Grid句子在Lombard效应和Normal-to-Lombard转换方面。通过对Lombard效应的参数分析，我们发现随着噪声水平的增加，自然句子和Grid句子的参数变化相似，但在alpha比例增加方面，Grid句子的增加更大。在跨性别和信噪比的主观可懂度评估中，基于EMALG训练的StarGAN模型始终表现优于其他模型。

    Grid sentence is commonly used for studying the Lombard effect and Normal-to-Lombard conversion. However, it's unclear if Normal-to-Lombard models trained on grid sentences are sufficient for improving natural speech intelligibility in real-world applications. This paper presents the recording of a parallel Lombard corpus (called Lombard Chinese TIMIT, LCT) extracting natural sentences from Chinese TIMIT. Then We compare natural and grid sentences in terms of Lombard effect and Normal-to-Lombard conversion using LCT and Enhanced MAndarin Lombard Grid corpus (EMALG). Through a parametric analysis of the Lombard effect, We find that as the noise level increases, both natural sentences and grid sentences exhibit similar changes in parameters, but in terms of the increase of the alpha ratio, grid sentences show a greater increase. Following a subjective intelligibility assessment across genders and Signal-to-Noise Ratios, the StarGAN model trained on EMALG consistently outperforms the mode
    
[^25]: 使用准确的空间成本梯度在可微的高分辨率水文模型中学习划分区域化模型

    Learning Regionalization within a Differentiable High-Resolution Hydrological Model using Accurate Spatial Cost Gradients. (arXiv:2308.02040v1 [cs.LG])

    [http://arxiv.org/abs/2308.02040](http://arxiv.org/abs/2308.02040)

    本文介绍了一种使用准确的空间成本梯度在可微的高分辨率水文模型中学习划分区域化模型的方法，该方法利用可学习的区域化映射，结合数据同化和参数校正，能够在广泛的时空计算域中利用不同数据集进行水文参数估计。

    

    在没有流量数据的未监测流域中估计空间分布的水文参数是一个具有挑战性的区域化问题，需要考虑流量数据的稀疏性并施加空间约束。一种可能的方法是寻找一个定量将物理指标与概念模型参数联系起来的转移函数。本文提出了一种混合数据同化和参数区域化（HDA-PR）方法，将可学习的区域化映射（基于多元回归或神经网络）引入可微的水文模型中。它可以利用准确的伴随梯度在高维的区域化背景下在广泛的时空计算域中利用异质数据。逆问题通过多个观测站点的信息计算校正代价函数来解决。HDA-PR在高分辨率、小时级和公里级的区域模拟中进行了测试。

    Estimating spatially distributed hydrological parameters in ungauged catchments poses a challenging regionalization problem and requires imposing spatial constraints given the sparsity of discharge data. A possible approach is to search for a transfer function that quantitatively relates physical descriptors to conceptual model parameters. This paper introduces a Hybrid Data Assimilation and Parameter Regionalization (HDA-PR) approach incorporating learnable regionalization mappings, based on either multivariate regressions or neural networks, into a differentiable hydrological model. It enables the exploitation of heterogeneous datasets across extensive spatio-temporal computational domains within a high-dimensional regionalization context, using accurate adjoint-based gradients. The inverse problem is tackled with a multi-gauge calibration cost function accounting for information from multiple observation sites. HDA-PR was tested on high-resolution, hourly and kilometric regional mod
    
[^26]: 限制数据下社交机器学习的非渐进性能研究

    Non-Asymptotic Performance of Social Machine Learning Under Limited Data. (arXiv:2306.09397v1 [cs.LG])

    [http://arxiv.org/abs/2306.09397](http://arxiv.org/abs/2306.09397)

    本文研究了限制数据下社交机器学习的概率误差问题，并提出了一种更强的一致性训练条件，推导出了两种任务的概率误差上界。

    

    本文研究了社交机器学习框架中与错误概率相关的问题，该框架涉及独立的训练阶段，随后在图上进行协作决策阶段。该框架解决了分布式分类未标记数据的问题。我们考虑了两种分类任务，分别为统计分类和单样本分类，同时考虑了预测阶段数据观测受限的情况。对于每个任务，我们描述了分布式学习规则，并相应分析了错误概率。为此，我们首先引入了一个更强的一致性训练条件，该条件涉及训练分类器生成的边缘分布。基于此条件，我们为两种任务推导出了概率误差的上界，该上界取决于数据的统计特性和用于组合分布式分类器的组合策略。

    This paper studies the probability of error associated with the social machine learning framework, which involves an independent training phase followed by a cooperative decision-making phase over a graph. This framework addresses the problem of classifying a stream of unlabeled data in a distributed manner. We consider two kinds of classification tasks with limited observations in the prediction phase, namely, the statistical classification task and the single-sample classification task. For each task, we describe the distributed learning rule and analyze the probability of error accordingly. To do so, we first introduce a stronger consistent training condition that involves the margin distributions generated by the trained classifiers. Based on this condition, we derive an upper bound on the probability of error for both tasks, which depends on the statistical properties of the data and the combination policy used to combine the distributed classifiers. For the statistical classifica
    
[^27]: 用Transformer学习超关系型和数值知识图中的表征学习

    Representation Learning on Hyper-Relational and Numeric Knowledge Graphs with Transformers. (arXiv:2305.18256v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.18256](http://arxiv.org/abs/2305.18256)

    本文提出了一个名为HyNT的框架，用于学习超关系型知识图的表示，包括数值文字。该框架使用上下文Transformer和预测Transformer，通过学习三元组和其限定词之间的相关性以及数值信息来获得模型。

    

    近期研究了一个超关系型知识图谱，其中三元组与限定词集合相关联; 一个限定词由关系和实体组成，为三元组提供辅助信息。现有的超关系型知识图嵌入方法假定实体是离散对象，但有些信息应使用数值表示，例如(J.R.R.，出生于，1892)。同时，三元组(J.R.R.，就读于，牛津大学)可以与限定词(开始时间，1911)相关联。在本文中，我们提出了一个名为HyNT的统一框架，用于学习包含三元组或限定词中数值文字的超关系型知识图的表示。我们定义了一个上下文Transformer和一个预测Transformer，来学习表示，不仅基于三元组和其限定词之间的相关性，还基于数值信息。通过学习三元组和限定词的紧凑表示，并将它们馈送给Transformer来获得模型

    A hyper-relational knowledge graph has been recently studied where a triplet is associated with a set of qualifiers; a qualifier is composed of a relation and an entity, providing auxiliary information for a triplet. While existing hyper-relational knowledge graph embedding methods assume that the entities are discrete objects, some information should be represented using numeric values, e.g., (J.R.R., was born in, 1892). Also, a triplet (J.R.R., educated at, Oxford Univ.) can be associated with a qualifier such as (start time, 1911). In this paper, we propose a unified framework named HyNT that learns representations of a hyper-relational knowledge graph containing numeric literals in either triplets or qualifiers. We define a context transformer and a prediction transformer to learn the representations based not only on the correlations between a triplet and its qualifiers but also on the numeric information. By learning compact representations of triplets and qualifiers and feeding 
    
[^28]: TSGM：一种生成合成时间序列数据的灵活框架

    TSGM: A Flexible Framework for Generative Modeling of Synthetic Time Series. (arXiv:2305.11567v1 [cs.LG])

    [http://arxiv.org/abs/2305.11567](http://arxiv.org/abs/2305.11567)

    TSGM提供了一种生成合成时间序列数据的灵活框架，使研究人员能够快速实现自己的方法并在可共享的环境中进行比较，从而有助于生成大规模的合成时间序列数据集，以用于训练和验证各种机器学习模型。

    

    时间序列数据在各个领域中非常重要，对机器学习研究者也很有兴趣。然而，时间序列数据通常很少或高度敏感，这使得数据在研究者和工业组织之间的共享以及现有和新的数据密集型 ML 方法的应用受到限制。解决这一难题的可能方法是生成合成数据。在这项工作中，我们介绍了时间序列生成模型（TSGM），这是一种用于生成合成时间序列数据的开源框架。TSGM包括广泛的机器学习方法：生成模型、概率模型和基于模拟器的方法。该框架使用户能够从不同的角度评估生成的数据的质量：相似性、下游效果、预测一致性、多样性和隐私。该框架是可扩展的，这使得研究人员能够快速实现自己的方法并在可共享的环境中进行比较。TSGM将有助于生成大规模的合成时间序列数据集，这些数据集可以用于训练和验证各种机器学习模型。

    Temporally indexed data are essential in a wide range of fields and of interest to machine learning researchers. Time series data, however, are often scarce or highly sensitive, which precludes the sharing of data between researchers and industrial organizations and the application of existing and new data-intensive ML methods. A possible solution to this bottleneck is to generate synthetic data. In this work, we introduce Time Series Generative Modeling (TSGM), an open-source framework for the generative modeling of synthetic time series. TSGM includes a broad repertoire of machine learning methods: generative models, probabilistic, and simulator-based approaches. The framework enables users to evaluate the quality of the produced data from different angles: similarity, downstream effectiveness, predictive consistency, diversity, and privacy. The framework is extensible, which allows researchers to rapidly implement their own methods and compare them in a shareable environment. TSGM w
    
[^29]: SmartChoices: 学习实现增强软件

    SmartChoices: Augmenting Software with Learned Implementations. (arXiv:2304.13033v1 [cs.SE])

    [http://arxiv.org/abs/2304.13033](http://arxiv.org/abs/2304.13033)

    SmartChoices 提出了一种将机器学习与现有软件系统轻松、安全、有效地结合的新方法。

    

    我们正处于机器学习的黄金时代。强大的模型正在训练中，远比仅使用传统软件工程方法更好地执行许多任务。然而，将这些模型开发并部署到现有软件系统中仍然很困难。在本文中，我们提出了 SmartChoices，一种将机器学习轻松、安全、有效地结合到成熟软件堆栈中的新方法。我们解释了总体设计理念，并展示了使用 SmartChoices 在大型工业系统中的案例研究。

    We are living in a golden age of machine learning. Powerful models are being trained to perform many tasks far better than is possible using traditional software engineering approaches alone. However, developing and deploying those models in existing software systems remains difficult. In this paper we present SmartChoices, a novel approach to incorporating machine learning into mature software stacks easily, safely, and effectively. We explain the overall design philosophy and present case studies using SmartChoices within large scale industrial systems.
    
[^30]: 正则化和多视角支持向量机学习的本地化

    Localisation of Regularised and Multiview Support Vector Machine Learning. (arXiv:2304.05655v1 [math.FA])

    [http://arxiv.org/abs/2304.05655](http://arxiv.org/abs/2304.05655)

    本文针对正则化和多视角支持向量机学习问题的本地化版本，证明了一些表示定理，研究了与损失函数和输入空间维度相关的特殊情况，特别是损失函数为 Gâteaux 可微函数时的情况。

    

    本文证明了 H.Q. Minh、L. Bazzani 和 V. Murino 在《机器学习研究》（Journal of Machine Learning Research）中介绍的一种涉及算子值正定核及其再生核希尔伯特空间的正则化和多视角支持向量机学习问题的本地化版本的一些表示定理。结果涉及到考虑凸或非凸损失函数以及有限或无限维输入空间的一般情况。我们展示了该一般框架允许一些特殊情况下的无限维输入空间和非凸损失函数，特别是当损失函数为 Gâteaux 可微函数时。对导致部分非线性问题的指数最小二乘损失函数进行了详细计算。

    We prove a few representer theorems for a localised version of the regularised and multiview support vector machine learning problem introduced by H.Q.~Minh, L.~Bazzani, and V.~Murino, \textit{Journal of Machine Learning Research}, \textbf{17}(2016) 1--72, that involves operator valued positive semidefinite kernels and their reproducing kernel Hilbert spaces. The results concern general cases when convex or nonconvex loss functions and finite or infinite dimensional input spaces are considered. We show that the general framework allows infinite dimensional input spaces and nonconvex loss functions for some special cases, in particular in case the loss functions are G\^ateaux differentiable. Detailed calculations are provided for the exponential least squares loss functions that leads to partially nonlinear problems.
    
[^31]: 通过稀疏高斯过程校准Transformer

    Calibrating Transformers via Sparse Gaussian Processes. (arXiv:2303.02444v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02444](http://arxiv.org/abs/2303.02444)

    提出了一种通过Sparse Gaussian Process attention (SGPA)来校准Transformer模型不确定性的方法。在文本、图像和图形的预测任务中，SGPA-based Transformers在预测准确性上表现出竞争力，并显著改善了内分布校准和外分布的鲁棒性和检测能力。

    

    Transformer模型在自然语言处理、语音识别和计算机视觉等广泛应用中取得了巨大成功。将Transformer的成功扩展到安全关键领域需要准确估计的不确定性，这方面的研究较少。为了解决这个问题，我们提出了稀疏高斯过程注意力（SGPA），它直接在Transformer的多头自注意力块（MHA）的输出空间中进行贝叶斯推断，以校准其不确定性。它用一个有效的对称核替代了缩放点积操作，并使用稀疏高斯过程（SGP）技术来近似MHA输出的后验过程。经验上，在文本、图像和图形的一系列预测任务中，基于SGPA的Transformer模型实现了有竞争力的预测准确性，同时显著改善了内分布校准和外分布的鲁棒性和检测能力。

    Transformer models have achieved profound success in prediction tasks in a wide range of applications in natural language processing, speech recognition and computer vision. Extending Transformer's success to safety-critical domains requires calibrated uncertainty estimation which remains under-explored. To address this, we propose Sparse Gaussian Process attention (SGPA), which performs Bayesian inference directly in the output space of multi-head attention blocks (MHAs) in transformer to calibrate its uncertainty. It replaces the scaled dot-product operation with a valid symmetric kernel and uses sparse Gaussian processes (SGP) techniques to approximate the posterior processes of MHA outputs. Empirically, on a suite of prediction tasks on text, images and graphs, SGPA-based Transformers achieve competitive predictive accuracy, while noticeably improving both in-distribution calibration and out-of-distribution robustness and detection.
    
[^32]: AUTOLYCUS: 利用可解释人工智能（XAI）对白盒模型进行模型提取攻击

    AUTOLYCUS: Exploiting Explainable AI (XAI) for Model Extraction Attacks against White-Box Models. (arXiv:2302.02162v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02162](http://arxiv.org/abs/2302.02162)

    本文探究了可解释人工智能（XAI）工具对机器学习模型提取攻击的风险，并提出了一种模型提取攻击方法AUTOLYCUS。

    

    可解释的人工智能（XAI）涵盖了一系列旨在阐明AI模型决策过程的技术和程序。虽然XAI对于理解AI模型的推理过程很有价值，但用于这种揭示的数据会带来潜在的安全和隐私漏洞。现有文献已经确定了针对机器学习模型的隐私风险，包括成员推论、模型反演和模型提取攻击。根据涉及的设置和各方，这些攻击可能针对模型本身或用于创建模型的训练数据。我们认为提供XAI的工具特别会增加模型提取攻击的风险，这可能是一个重要问题，当AI模型的所有者仅愿提供黑盒访问而不与其他方共享模型参数和结构时。为了探究这种隐私风险，我们提出了AUTOLYCUS，一种模型提取攻击方法。

    Explainable Artificial Intelligence (XAI) encompasses a range of techniques and procedures aimed at elucidating the decision-making processes of AI models. While XAI is valuable in understanding the reasoning behind AI models, the data used for such revelations poses potential security and privacy vulnerabilities. Existing literature has identified privacy risks targeting machine learning models, including membership inference, model inversion, and model extraction attacks. Depending on the settings and parties involved, such attacks may target either the model itself or the training data used to create the model.  We have identified that tools providing XAI can particularly increase the vulnerability of model extraction attacks, which can be a significant issue when the owner of an AI model prefers to provide only black-box access rather than sharing the model parameters and architecture with other parties. To explore this privacy risk, we propose AUTOLYCUS, a model extraction attack 
    
[^33]: CLIP是否知道我的脸？

    Does CLIP Know My Face?. (arXiv:2209.07341v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.07341](http://arxiv.org/abs/2209.07341)

    本文提出了一种新方法IDIA来评估视觉语言模型的隐私，大规模实验表明使用于训练的个人可以被非常高的准确率识别出来，表明需要更好地解决视觉语言模型中的隐私问题。

    

    随着深度学习在各个应用中的普及，保护训练数据的隐私问题已经成为一个关键的研究领域。以前的研究主要关注单模型的隐私风险，我们提出了一种新的方法来评估多模型的隐私，特别是像CLIP这样的视觉语言模型。所提出的身份推断攻击(IDIA)通过用同一人的图片向模型查询，从而揭示该个人是否被包含在训练数据中。让模型从各种可能的文本标签中选择，模型会透露是否识别该人物，从而表明其被用于训练。我们在CLIP上进行的大规模实验表明，使用于训练的个人可以被非常高的准确率识别出来。我们确认该模型已经学会将名称与描绘的个人相关联，这意味着敏感信息存在于其中，可以被对手提取。我们的结果凸显了需要在视觉语言模型中更好地解决隐私问题。

    With the rise of deep learning in various applications, privacy concerns around the protection of training data has become a critical area of research. Whereas prior studies have focused on privacy risks in single-modal models, we introduce a novel method to assess privacy for multi-modal models, specifically vision-language models like CLIP. The proposed Identity Inference Attack (IDIA) reveals whether an individual was included in the training data by querying the model with images of the same person. Letting the model choose from a wide variety of possible text labels, the model reveals whether it recognizes the person and, therefore, was used for training. Our large-scale experiments on CLIP demonstrate that individuals used for training can be identified with very high accuracy. We confirm that the model has learned to associate names with depicted individuals, implying the existence of sensitive information that can be extracted by adversaries. Our results highlight the need for 
    
[^34]: 学习ACOPF的分桶主动采样

    Bucketized Active Sampling for Learning ACOPF. (arXiv:2208.07497v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.07497](http://arxiv.org/abs/2208.07497)

    本文提出了一种新颖的主动学习框架——分桶主动采样（BAS），旨在在时间限制内训练最佳的OPF代理。BAS将输入分布分成桶，并使用收集函数确定下一次采样的位置。实验结果显示了BAS的好处。

    

    本文考虑最优潮流（OPF）的优化代理，即近似OPF的输入/输出关系的机器学习模型。最近的研究集中在证明这些代理可以具有较高的准确性。然而，它们的训练需要大量的数据，每个实例都需要对输入分布的样本进行OPF的（离线）求解。为了满足市场清算应用的要求，本文提出了一种新颖的主动学习框架——分桶主动采样（BAS），旨在在时间限制内训练最佳的OPF代理。BAS将输入分布分成桶，并使用收集函数确定下一次采样的位置。通过将相同的分桶应用于验证集，BAS利用标记的验证样本来选择未标记的样本。BAS还依赖于随时间增加和减少的自适应学习率。实验结果显示了BAS的好处。

    This paper considers optimization proxies for Optimal Power Flow (OPF), i.e., machine-learning models that approximate the input/output relationship of OPF. Recent work has focused on showing that such proxies can be of high fidelity. However, their training requires significant data, each instance necessitating the (offline) solving of an OPF for a sample of the input distribution. To meet the requirements of market-clearing applications, this paper proposes Bucketized Active Sampling (BAS), a novel active learning framework that aims at training the best possible OPF proxy within a time limit. BAS partitions the input distribution into buckets and uses an acquisition function to determine where to sample next. By applying the same partitioning to the validation set, BAS leverages labeled validation samples in the selection of unlabeled samples. BAS also relies on an adaptive learning rate that increases and decreases over time. Experimental results demonstrate the benefits of BAS.
    

