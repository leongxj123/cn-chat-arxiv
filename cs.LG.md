# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Deep Reinforcement Learning for Traveling Purchaser Problems](https://arxiv.org/abs/2404.02476) | 提出了一种基于深度强化学习的方法，该方法分别解决了旅行购买者问题中的路由构建和购买规划问题，并从全局角度评估和优化解决方案。 |
| [^2] | [Do language models plan ahead for future tokens?](https://arxiv.org/abs/2404.00859) | 语言模型在推理过程中会提前准备未来标记所需的信息，可能是通过预缓存或面包屑的方式实现。 |
| [^3] | [Learning Visual Quadrupedal Loco-Manipulation from Demonstrations](https://arxiv.org/abs/2403.20328) | 通过将走行操纵过程分解为低层强化学习控制器和高层行为克隆规划器，使四足机器人能够执行真实世界的操纵任务。 |
| [^4] | [Multi-criteria approach for selecting an explanation from the set of counterfactuals produced by an ensemble of explainers](https://arxiv.org/abs/2403.13940) | 本文提出了一种多阶段集成方法，通过多标准分析选择单个反事实，避免了用户测试多种不同解释方法和分析冲突解决方案的困难，提供了一个在多个质量度量上得分很高的妥协方案。 |
| [^5] | [Dual Operating Modes of In-Context Learning](https://arxiv.org/abs/2402.18819) | 该论文介绍了In-Context Learning的双重运行模式，通过引入概率模型同时解释了任务学习和任务检索，对线性函数的上下文学习进行了扩展，分析了优化预训练模型在平方损失下的行为，并推导出了任务后验分布的闭式表达式。 |
| [^6] | [A Survey on Data Selection for Language Models](https://arxiv.org/abs/2402.16827) | 大型语言模型成功的关键在于使用大规模的文本数据集进行无监督预训练，但如何优化选择数据以降低碳足迹和财务成本仍是一个挑战。 |
| [^7] | [Improving Sentence Embeddings with an Automatically Generated NLI Dataset](https://arxiv.org/abs/2402.15132) | 通过自动生成的NLI数据集改进句子嵌入，实验结果表明该方法在STS任务中表现出色，优于现有方法。 |
| [^8] | [A Reparameterized Discrete Diffusion Model for Text Generation](https://arxiv.org/abs/2302.05737) | 本文提出了一种重新参数化离散扩散模型，该模型在文本生成方面表现出更好的灵活性、训练技术和生成效果，实验证明其较现有的扩散模型有显著的改进。 |
| [^9] | [Leeroo Orchestrator: Elevating LLMs Performance Through Model Integration.](http://arxiv.org/abs/2401.13979) | 本研究提出了Leeroo编排器的架构，通过集成多个训练过的LLMs模型，实现了一个新的最先进模型。该编排器在性能上与Mixtral模型相当，并且成本只有其三分之二。当允许更高的成本时，Leeroo编排器的准确性超过了Mixtral模型，并且当集成GPT4时进一步提升。 |
| [^10] | [Towards Model-Free LQR Control over Rate-Limited Channels.](http://arxiv.org/abs/2401.01258) | 这篇论文研究了在速率限制通道上实现模型无关的LQR控制的问题。通过引入自适应量化梯度下降（AQGD）算法，作者证明了在噪声电路中可以实现控制问题的解决。 |
| [^11] | [Unsupervised Graph-based Learning Method for Sub-band Allocation in 6G Subnetworks.](http://arxiv.org/abs/2401.00950) | 本文提出了一种无监督的基于图的学习方法，用于在6G子网络中进行子频带分配。该方法通过优化使用图神经网络的子频带分配，实现了与集中式贪婪着色子频带分配方法相近的性能，并且具有更低的计算时间复杂度和较小的信令开销。 |
| [^12] | [A Hyperparameter Study for Quantum Kernel Methods.](http://arxiv.org/abs/2310.11891) | 本研究调查了超参数选择对模型性能和经典核与量子核之间的泛化差距的影响。 |
| [^13] | [Fool Your (Vision and) Language Model With Embarrassingly Simple Permutations.](http://arxiv.org/abs/2310.01651) | 本文揭示了大型语言和视觉-语言模型中的一个特定漏洞，即它们对于多项选择问答的排列敏感性，这对于模型可靠性分析非常重要。这些漏洞在各种模型规模和最新的模型中都存在。 |
| [^14] | [Multi-Modality Guidance Network For Missing Modality Inference.](http://arxiv.org/abs/2309.03452) | 我们提出了一种多模态辅助网络，在训练过程中利用多模态表示来训练更好的单模态模型进行推断，解决了推断过程中模态缺失的问题。实验结果表明，我们的方法在性能上显著优于传统方法。 |
| [^15] | [Semi-Supervised Dual-Stream Self-Attentive Adversarial Graph Contrastive Learning for Cross-Subject EEG-based Emotion Recognition.](http://arxiv.org/abs/2308.11635) | 本论文提出了一种半监督双流自注意对抗图对比学习框架 DS-AGC，用于跨主体脑电情绪识别中的标记数据不足问题。该框架利用两个并行流提取非结构化和结构化脑电特征，并通过半监督方法解决分布差异和提取有效的基于图的特征表示。 |
| [^16] | [Efficient Semi-Supervised Federated Learning for Heterogeneous Participants.](http://arxiv.org/abs/2307.15870) | 本论文提出了一种高效的半监督异构参与者联邦学习系统，通过引入聚类正则化来改进模型在数据非独立同分布情况下的性能，并对模型收敛性进行了理论和实验研究。 |
| [^17] | [Urban Spatiotemporal Data Synthesis via Neural Disaggregation.](http://arxiv.org/abs/2306.07292) | 本研究提出了一种基于神经网络的城市时空数据合成方法，旨在通过分解粗糙的低分辨率地理单元的聚合城市数据来合成细粒度，高分辨率的城市数据，以增加高度聚合的城市数据的可用性和实现价值。 |
| [^18] | [Insights from the Design Space Exploration of Flow-Guided Nanoscale Localization.](http://arxiv.org/abs/2305.18493) | 研究了基于流导向纳米定位的设计空间，考虑了能源和信号衰减等因素，为这一新兴领域提供了有希望的解决方案。 |
| [^19] | [Robust Holographic mmWave Beamforming by Self-Supervised Hybrid Deep Learning.](http://arxiv.org/abs/2303.12653) | 本文提出一种自监督混合深度学习网络用于健壮波束成形，能够在两种不同的数据集和各种场景中表现出更强的鲁棒性。 |
| [^20] | [Mixed moving average field guided learning for spatio-temporal data.](http://arxiv.org/abs/2301.00736) | 本论文提出了一种理论引导机器学习方法，采用广义贝叶斯算法进行混合移动平均场引导的时空数据建模，可以进行因果未来预测。 |

# 详细

[^1]: 用于旅行购买者问题的深度强化学习

    Deep Reinforcement Learning for Traveling Purchaser Problems

    [https://arxiv.org/abs/2404.02476](https://arxiv.org/abs/2404.02476)

    提出了一种基于深度强化学习的方法，该方法分别解决了旅行购买者问题中的路由构建和购买规划问题，并从全局角度评估和优化解决方案。

    

    旅行购买者问题（TPP）是一种具有广泛应用的重要组合优化问题。本文提出了一种基于深度强化学习（DRL）的新方法，该方法分别解决了路由构建和购买规划问题，同时从全局角度评估和优化解决方案。我们的方法的关键组成部分包括用于捕捉市场-产品关系的TPP的二部图表示，以及从二部图中提取信息并将其用于顺序构建路由的策略网络。

    arXiv:2404.02476v1 Announce Type: cross  Abstract: The traveling purchaser problem (TPP) is an important combinatorial optimization problem with broad applications. Due to the coupling between routing and purchasing, existing works on TPPs commonly address route construction and purchase planning simultaneously, which, however, leads to exact methods with high computational cost and heuristics with sophisticated design but limited performance. In sharp contrast, we propose a novel approach based on deep reinforcement learning (DRL), which addresses route construction and purchase planning separately, while evaluating and optimizing the solution from a global perspective. The key components of our approach include a bipartite graph representation for TPPs to capture the market-product relations, and a policy network that extracts information from the bipartite graph and uses it to sequentially construct the route. One significant benefit of our framework is that we can efficiently const
    
[^2]: 语言模型是否提前为未来标记进行规划？

    Do language models plan ahead for future tokens?

    [https://arxiv.org/abs/2404.00859](https://arxiv.org/abs/2404.00859)

    语言模型在推理过程中会提前准备未来标记所需的信息，可能是通过预缓存或面包屑的方式实现。

    

    arXiv:2404.00859v1 公告类型：跨领域 摘要：在给定位置的推理过程中，变压器是否会“提前思考”？已知变压器在$t$的前向传递的隐藏状态中准备信息，然后在未来的前向传递$t+\tau$中使用。我们提出了两种解释这种现象的可能性：预缓存，即训练中存在的非对角梯度项导致模型在$t$计算与当前推理任务无关但对未来有用的特征，以及面包屑，即与时间步长$t$最相关的特征已经与那些将最有利于时间步长$t+\tau$的特征相同。我们通过训练不将梯度传播到过去时间步的语言模型来测试这些假设，这种方案我们正式称为短视训练。在合成数据设置中，我们发现了预缓存的明确证据。在自回归语言建模设置中，我们的实验更多地支持了面包屑假设。

    arXiv:2404.00859v1 Announce Type: cross  Abstract: Do transformers "think ahead" during inference at a given position? It is known transformers prepare information in the hidden states of the forward pass at $t$ that is then used in future forward passes $t+\tau$. We posit two explanations for this phenomenon: pre-caching, in which off-diagonal gradient terms present in training result in the model computing features at $t$ irrelevant to the present inference task but useful for the future, and breadcrumbs, in which features most relevant to time step $t$ are already the same as those that would most benefit inference at time $t+\tau$. We test these hypotheses by training language models without propagating gradients to past timesteps, a scheme we formalize as myopic training. In a synthetic data setting, we find clear evidence for pre-caching. In the autoregressive language modeling setting, our experiments are more suggestive of the breadcrumbs hypothesis.
    
[^3]: 从示范学习视觉四足走行操纵

    Learning Visual Quadrupedal Loco-Manipulation from Demonstrations

    [https://arxiv.org/abs/2403.20328](https://arxiv.org/abs/2403.20328)

    通过将走行操纵过程分解为低层强化学习控制器和高层行为克隆规划器，使四足机器人能够执行真实世界的操纵任务。

    

    四足机器人逐渐被整合进人类环境。尽管四足机器人的行走能力不断增强，但它们在现实场景中与物体的互动仍然有限。为了让四足机器人能够执行真实世界的操纵任务，我们将走行操纵过程分解为基于强化学习（RL）的低层控制器和基于行为克隆（BC）的高层规划器。通过参数化操纵轨迹，我们同步上层和下层的努力，从而充分利用RL和BC的优势。我们的方法通过模拟和现实世界实验得到验证。

    arXiv:2403.20328v1 Announce Type: cross  Abstract: Quadruped robots are progressively being integrated into human environments. Despite the growing locomotion capabilities of quadrupedal robots, their interaction with objects in realistic scenes is still limited. While additional robotic arms on quadrupedal robots enable manipulating objects, they are sometimes redundant given that a quadruped robot is essentially a mobile unit equipped with four limbs, each possessing 3 degrees of freedom (DoFs). Hence, we aim to empower a quadruped robot to execute real-world manipulation tasks using only its legs. We decompose the loco-manipulation process into a low-level reinforcement learning (RL)-based controller and a high-level Behavior Cloning (BC)-based planner. By parameterizing the manipulation trajectory, we synchronize the efforts of the upper and lower layers, thereby leveraging the advantages of both RL and BC. Our approach is validated through simulations and real-world experiments, d
    
[^4]: 从解释器集合中选择反事实解释的多标准方法

    Multi-criteria approach for selecting an explanation from the set of counterfactuals produced by an ensemble of explainers

    [https://arxiv.org/abs/2403.13940](https://arxiv.org/abs/2403.13940)

    本文提出了一种多阶段集成方法，通过多标准分析选择单个反事实，避免了用户测试多种不同解释方法和分析冲突解决方案的困难，提供了一个在多个质量度量上得分很高的妥协方案。

    

    反事实被广泛用于解释机器学习模型的预测，提供获取更理想预测的替代场景。它们可以由多种方法生成，这些方法优化不同、有时是冲突的质量度量，并产生完全不同的解决方案。然而，选择最合适的解释方法和生成的反事实之一并不是一件容易的事情。本文提出使用多阶段集成方法，基于多标准分析来选择单个反事实，而不是强迫用户测试许多不同的解释方法并分析冲突的解决方案。它提供了一个妥协方案，在几个流行的质量度量上得分较高。这种方法利用支配关系和理想点决策辅助方法，从帕累托前沿中选择一个反事实。进行的实验证明了这种方法的有效性。

    arXiv:2403.13940v1 Announce Type: cross  Abstract: Counterfactuals are widely used to explain ML model predictions by providing alternative scenarios for obtaining the more desired predictions. They can be generated by a variety of methods that optimize different, sometimes conflicting, quality measures and produce quite different solutions. However, choosing the most appropriate explanation method and one of the generated counterfactuals is not an easy task. Instead of forcing the user to test many different explanation methods and analysing conflicting solutions, in this paper, we propose to use a multi-stage ensemble approach that will select single counterfactual based on the multiple-criteria analysis. It offers a compromise solution that scores well on several popular quality measures. This approach exploits the dominance relation and the ideal point decision aid method, which selects one counterfactual from the Pareto front. The conducted experiments demonstrated that the propos
    
[^5]: In-Context Learning的双重运行模式

    Dual Operating Modes of In-Context Learning

    [https://arxiv.org/abs/2402.18819](https://arxiv.org/abs/2402.18819)

    该论文介绍了In-Context Learning的双重运行模式，通过引入概率模型同时解释了任务学习和任务检索，对线性函数的上下文学习进行了扩展，分析了优化预训练模型在平方损失下的行为，并推导出了任务后验分布的闭式表达式。

    

    In-Context Learning (ICL)展示了双重运行模式：任务学习，即从上下文样本中获取新技能，和任务检索，即查找并激活相关的预训练技能。最近的理论研究探讨了各种数学模型来分析ICL，但现有模型一次只能解释一种操作模式。我们引入了一个概率模型，可以同时解释ICL的双重运行模式。专注于线性函数的上下文学习，通过引入多个任务组和任务相关的输入分布扩展现有模型用于预训练数据。然后分析在平方损失下表现最优的预训练模型的行为，即给定上下文样本标签的最小均方误差估计器。将预训练任务分布视为先验，将上下文示例视为观测值，我们推导出任务后验分布的闭式表达式。

    arXiv:2402.18819v1 Announce Type: new  Abstract: In-context learning (ICL) exhibits dual operating modes: task learning, i.e., acquiring a new skill from in-context samples, and task retrieval, i.e., locating and activating a relevant pretrained skill. Recent theoretical work investigates various mathematical models to analyze ICL, but existing models explain only one operating mode at a time. We introduce a probabilistic model, with which one can explain the dual operating modes of ICL simultaneously. Focusing on in-context learning of linear functions, we extend existing models for pretraining data by introducing multiple task groups and task-dependent input distributions. We then analyze the behavior of the optimally pretrained model under the squared loss, i.e., the MMSE estimator of the label given in-context examples. Regarding pretraining task distribution as prior and in-context examples as the observation, we derive the closed-form expression of the task posterior distribution
    
[^6]: 语言模型数据选择概述

    A Survey on Data Selection for Language Models

    [https://arxiv.org/abs/2402.16827](https://arxiv.org/abs/2402.16827)

    大型语言模型成功的关键在于使用大规模的文本数据集进行无监督预训练，但如何优化选择数据以降低碳足迹和财务成本仍是一个挑战。

    

    最近大型语言模型取得成功的一个主要因素是利用巨大且不断增长的文本数据集进行无监督预训练。然而，简单地在所有可用数据上训练模型可能并不是最佳选择（或不可行），因为可用文本数据的质量可能有所不同。数据过滤也可以通过减少所需的训练量来降低训练模型的碳足迹和财务成本。数据选择方法旨在确定要包括在训练数据集中的哪些候选数据点，以及如何从所选数据点中适当采样。改进的数据选择方法的前景已经导致该领域的研究量迅速扩大。然而，由于深度学习主要受实证证据驱动，对大规模数据进行实验成本昂贵，很少有组织拥有资源进行广泛的数据选择研究。因此，有效数据选择的知识可能大多局限于大型技术公司或研究机构内部。

    arXiv:2402.16827v1 Announce Type: new  Abstract: A major factor in the recent success of large language models is the use of enormous and ever-growing text datasets for unsupervised pre-training. However, naively training a model on all available data may not be optimal (or feasible), as the quality of available text data can vary. Filtering out data can also decrease the carbon footprint and financial costs of training models by reducing the amount of training required.   Data selection methods aim to determine which candidate data points to include in the training dataset and how to appropriately sample from the selected data points. The promise of improved data selection methods has caused the volume of research in the area to rapidly expand. However, because deep learning is mostly driven by empirical evidence and experimentation on large-scale data is expensive, few organizations have the resources for extensive data selection research. Consequently, knowledge of effective data se
    
[^7]: 通过自动生成的NLI数据集改进句子嵌入

    Improving Sentence Embeddings with an Automatically Generated NLI Dataset

    [https://arxiv.org/abs/2402.15132](https://arxiv.org/abs/2402.15132)

    通过自动生成的NLI数据集改进句子嵌入，实验结果表明该方法在STS任务中表现出色，优于现有方法。

    

    基于解码器的大型语言模型在自然语言处理的许多任务中表现出了很高的性能。这在句子嵌入学习中同样成立，其中基于解码器的模型PromptEOL 在语义文本相似性（STS）任务中取得了最佳表现。然而，PromptEOL 在很大程度上利用了对自然语言推理（NLI）数据集的手动标注进行微调。我们旨在通过使用LLM自动生成的NLI数据集来改进在无监督设置下学习的句子嵌入，并将其用于微调PromptEOL。在STS任务的实验中，提出的方法在人类评估方面达到了82.21的平均Spearman等级相关系数，从而优于现有方法而无需使用大规模手动注释的数据集。

    arXiv:2402.15132v1 Announce Type: new  Abstract: Decoder-based large language models (LLMs) have shown high performance on many tasks in natural language processing. This is also true for sentence embedding learning, where a decoder-based model, PromptEOL, has achieved the best performance on semantic textual similarity (STS) tasks. However, PromptEOL makes great use of fine-tuning with a manually annotated natural language inference (NLI) dataset. We aim to improve sentence embeddings learned in an unsupervised setting by automatically generating an NLI dataset with an LLM and using it to fine-tune PromptEOL. In experiments on STS tasks, the proposed method achieved an average Spearman's rank correlation coefficient of 82.21 with respect to human evaluation, thus outperforming existing methods without using large, manually annotated datasets.
    
[^8]: 一种用于文本生成的重新参数化离散扩散模型的研究

    A Reparameterized Discrete Diffusion Model for Text Generation

    [https://arxiv.org/abs/2302.05737](https://arxiv.org/abs/2302.05737)

    本文提出了一种重新参数化离散扩散模型，该模型在文本生成方面表现出更好的灵活性、训练技术和生成效果，实验证明其较现有的扩散模型有显著的改进。

    

    本文研究了应用于自然语言生成的离散扩散概率模型。我们推导出了从离散扩散过程中采样的另一种等价形式，并利用这一洞见开发了一族重新参数化离散扩散模型。这个派生的通用框架非常灵活，为离散扩散模型中的生成过程提供了新的视角，并具备更有效的训练和解码技术。我们进行了大量实验证明我们模型的文本生成能力，在现有的扩散模型上取得了显著的改进。

    This work studies discrete diffusion probabilistic models with applications to natural language generation. We derive an alternative yet equivalent formulation of the sampling from discrete diffusion processes and leverage this insight to develop a family of reparameterized discrete diffusion models. The derived generic framework is highly flexible, offers a fresh perspective of the generation process in discrete diffusion models, and features more effective training and decoding techniques. We conduct extensive experiments to evaluate the text generation capability of our model, demonstrating significant improvements over existing diffusion models.
    
[^9]: Leeroo Orchestrator: 通过模型集成提高LLMs的性能

    Leeroo Orchestrator: Elevating LLMs Performance Through Model Integration. (arXiv:2401.13979v1 [cs.CL])

    [http://arxiv.org/abs/2401.13979](http://arxiv.org/abs/2401.13979)

    本研究提出了Leeroo编排器的架构，通过集成多个训练过的LLMs模型，实现了一个新的最先进模型。该编排器在性能上与Mixtral模型相当，并且成本只有其三分之二。当允许更高的成本时，Leeroo编排器的准确性超过了Mixtral模型，并且当集成GPT4时进一步提升。

    

    本文提出了一种架构，利用多个训练过的LLMs的集体知识，创建一个新的最先进模型。该框架的核心是一个基于LLM的编排器，能够选择最佳的底层LLM专家进行任务执行。受到强化学习中的自我对弈的启发，我们创建了一个查询生成、编排和评估的循环，为编排器生成训练数据。我们的评估主要针对MMLU基准，在Hugging Face上使用了具有7B、13B和34B参数的模型。结果显示我们的Leeroo编排器实现了与Mixtral模型相当的性能，但只产生了其成本的三分之二。此外，增加允许的成本超过了Mixtral的准确性，达到了75.9%的准确性。当将GPT4集成到底层模型池中时，进一步提升也得到了观察。

    In this paper, we propose an architecture to harness the collective knowledge of multiple trained LLMs to create a new state-of-the-art. At the core of this framework is a LLM-based orchestrator that is adept at picking the right underlying LLM experts for optimal task execution. Inspired by self-play in reinforcement learning, we created a loop of query generation, orchestration, and evaluation to generate training data for the orchestrator. Our evaluation focused on the MMLU benchmark, employing models with 7B, 13B, and 34B parameters available on Hugging Face. The results demonstrate new state-of-the-art open-source models: Our Leeroo orchestrator achieves performance on par with the Mixtral model while incurring only two-thirds of its cost. Moreover, increasing the allowed cost surpasses Mixtral's accuracy by over 5% at the same cost level, reaching an accuracy of 75.9%. Further enhancements were observed when integrating GPT4 into the underlying model pool. The Leeroo orchestrator
    
[^10]: 实现模型无关的通过速率限制通道的LQR控制

    Towards Model-Free LQR Control over Rate-Limited Channels. (arXiv:2401.01258v1 [math.OC])

    [http://arxiv.org/abs/2401.01258](http://arxiv.org/abs/2401.01258)

    这篇论文研究了在速率限制通道上实现模型无关的LQR控制的问题。通过引入自适应量化梯度下降（AQGD）算法，作者证明了在噪声电路中可以实现控制问题的解决。

    

    鉴于模型无关方法在许多问题设置中的控制设计方面取得的成功，自然而然地会问，如果利用实际的通信通道来传输梯度或策略，情况会如何改变。尽管由此产生的问题与网络控制系统中研究的公式有类似之处，但那个领域的丰富文献通常假定系统的模型是已知的。为了在模型无关控制设计和网络控制系统领域之间建立联系，我们提出了一个问题：\textit{是否可以通过速率限制的通道以模型无关的方式解决基本的控制问题-例如线性二次调节器（LQR）问题？}为了回答这个问题，我们研究了一个设置，其中一个工作代理通过一个无噪声信道以有限的位速率传输量化策略梯度（LQR成本）到一个服务器。我们提出了一种名为自适应量化梯度下降（AQGD）的新算法，并证明了

    Given the success of model-free methods for control design in many problem settings, it is natural to ask how things will change if realistic communication channels are utilized for the transmission of gradients or policies. While the resulting problem has analogies with the formulations studied under the rubric of networked control systems, the rich literature in that area has typically assumed that the model of the system is known. As a step towards bridging the fields of model-free control design and networked control systems, we ask: \textit{Is it possible to solve basic control problems - such as the linear quadratic regulator (LQR) problem - in a model-free manner over a rate-limited channel?} Toward answering this question, we study a setting where a worker agent transmits quantized policy gradients (of the LQR cost) to a server over a noiseless channel with a finite bit-rate. We propose a new algorithm titled Adaptively Quantized Gradient Descent (\texttt{AQGD}), and prove that
    
[^11]: 无监督的基于图的学习方法用于6G子网络的子频带分配

    Unsupervised Graph-based Learning Method for Sub-band Allocation in 6G Subnetworks. (arXiv:2401.00950v1 [cs.NI])

    [http://arxiv.org/abs/2401.00950](http://arxiv.org/abs/2401.00950)

    本文提出了一种无监督的基于图的学习方法，用于在6G子网络中进行子频带分配。该方法通过优化使用图神经网络的子频带分配，实现了与集中式贪婪着色子频带分配方法相近的性能，并且具有更低的计算时间复杂度和较小的信令开销。

    

    在本文中，我们提出了一种无监督的基于图的学习方法，用于在无线网络中进行频率子带分配。我们考虑在工厂环境中密集部署的子网络，这些子网络只有有限数量的子频带，必须被优化地分配以协调子网络间的干扰。我们将子网络部署建模为一个冲突图，并提出了一种受到图着色启发和Potts模型的无监督学习方法，利用图神经网络来优化子频带分配。数值评估表明，所提出的方法在较低的计算时间复杂度下，实现了与集中式贪婪着色子频带分配启发式方法接近的性能。此外，与需要所有互相干扰的信道信息的迭代优化启发式相比，它产生更少的信令开销。我们进一步证明该方法对不同的网络设置具有健壮性。

    In this paper, we present an unsupervised approach for frequency sub-band allocation in wireless networks using graph-based learning. We consider a dense deployment of subnetworks in the factory environment with a limited number of sub-bands which must be optimally allocated to coordinate inter-subnetwork interference. We model the subnetwork deployment as a conflict graph and propose an unsupervised learning approach inspired by the graph colouring heuristic and the Potts model to optimize the sub-band allocation using graph neural networks. The numerical evaluation shows that the proposed method achieves close performance to the centralized greedy colouring sub-band allocation heuristic with lower computational time complexity. In addition, it incurs reduced signalling overhead compared to iterative optimization heuristics that require all the mutual interfering channel information. We further demonstrate that the method is robust to different network settings.
    
[^12]: 量子核方法的超参数研究

    A Hyperparameter Study for Quantum Kernel Methods. (arXiv:2310.11891v1 [quant-ph])

    [http://arxiv.org/abs/2310.11891](http://arxiv.org/abs/2310.11891)

    本研究调查了超参数选择对模型性能和经典核与量子核之间的泛化差距的影响。

    

    量子核方法在量子机器学习中具有潜力，因为与之相关的保证。它们的可访问性也打开了基于潜在量子优势对数据集进行预先筛选的可能性。为了做到这一点，早期的研究开发了几何差异，它可以被理解为两种基于核的机器学习方法之间的接近度度量，特别是量子核和经典核之间的接近度。该度量指示了量子和经典模型的复杂性之间的联系。因此，它引发了一个问题，即基于与模型复杂性的关系，几何差异是否可以成为除了潜在的量子优势之外的评估工具。在这项工作中，我们研究了超参数选择对模型性能和经典核与量子核之间的泛化差距的影响。对于经典的机器学习方法来说，超参数优化的重要性是众所周知的。

    Quantum kernel methods are a promising method in quantum machine learning thanks to the guarantees connected to them. Their accessibility for analytic considerations also opens up the possibility of prescreening datasets based on their potential for a quantum advantage. To do so, earlier works developed the geometric difference, which can be understood as a closeness measure between two kernel-based machine learning approaches, most importantly between a quantum kernel and classical kernel. This metric links the quantum and classical model complexities. Therefore, it raises the question of whether the geometric difference, based on its relation to model complexity, can be a useful tool in evaluations other than for the potential for quantum advantage. In this work, we investigate the effects of hyperparameter choice on the model performance and the generalization gap between classical and quantum kernels. The importance of hyperparameter optimization is well known also for classical ma
    
[^13]: 用简单的排列欺骗（视觉和）语言模型

    Fool Your (Vision and) Language Model With Embarrassingly Simple Permutations. (arXiv:2310.01651v1 [cs.LG])

    [http://arxiv.org/abs/2310.01651](http://arxiv.org/abs/2310.01651)

    本文揭示了大型语言和视觉-语言模型中的一个特定漏洞，即它们对于多项选择问答的排列敏感性，这对于模型可靠性分析非常重要。这些漏洞在各种模型规模和最新的模型中都存在。

    

    大型语言和视觉-语言模型因其令人印象深刻的指示遵循能力和上下文学习能力而迅速在实践中部署。这引发了一个迫切的需求，即仔细分析它们的鲁棒性，以便利益相关者能够了解这些模型在任何给定应用程序中是否足够可信。在本文中，我们重点介绍了流行模型中的一个特定漏洞，即在多项选择问答（MCQA）中的排列敏感性。具体来说，我们通过实证研究表明，流行模型易受多项选择提示中答案集的对抗性排列攻击，这令人惊讶，因为模型理想上应该和人类一样对提示排列具有不变性。这些漏洞在各种模型规模下持续存在，并存在于最新的语言和视觉-语言模型中。

    Large language and vision-language models are rapidly being deployed in practice thanks to their impressive capabilities in instruction following, in-context learning, and so on. This raises an urgent need to carefully analyse their robustness so that stakeholders can understand if and when such models are trustworthy enough to be relied upon in any given application. In this paper, we highlight a specific vulnerability in popular models, namely permutation sensitivity in multiple-choice question answering (MCQA). Specifically, we show empirically that popular models are vulnerable to adversarial permutation in answer sets for multiple-choice prompting, which is surprising as models should ideally be as invariant to prompt permutation as humans are. These vulnerabilities persist across various model sizes, and exist in very recent language and vision-language models. Code is available at \url{https://github.com/ys-zong/FoolyourVLLMs}.
    
[^14]: 多模态辅助网络用于推断缺失模态

    Multi-Modality Guidance Network For Missing Modality Inference. (arXiv:2309.03452v1 [cs.CV])

    [http://arxiv.org/abs/2309.03452](http://arxiv.org/abs/2309.03452)

    我们提出了一种多模态辅助网络，在训练过程中利用多模态表示来训练更好的单模态模型进行推断，解决了推断过程中模态缺失的问题。实验结果表明，我们的方法在性能上显著优于传统方法。

    

    多模态模型在最近几年取得了显著的成功。标准多模态方法通常假设在训练阶段和推断阶段模态保持不变。然而在实践中，许多场景无法满足这样的假设，推断过程中会出现缺失模态，从而限制了多模态模型的应用范围。现有的方法通过重建缺失模态来缓解这个问题，但这增加了不必要的计算成本，尤其对于大型部署系统而言可能是至关重要的。为了从两个方面解决这个问题，我们提出了一种新颖的辅助网络，在训练过程中促进知识共享，利用多模态表示来训练更好的单模态模型进行推断。在暴力检测的现实生活实验中，我们提出的框架训练的单模态模型在性能上显著优于传统训练的模型，同时保持相同的推断能力。

    Multimodal models have gained significant success in recent years. Standard multimodal approaches often assume unchanged modalities from training stage to inference stage. In practice, however, many scenarios fail to satisfy such assumptions with missing modalities during inference, leading to limitations on where multimodal models can be applied. While existing methods mitigate the problem through reconstructing the missing modalities, it increases unnecessary computational cost, which could be just as critical, especially for large, deployed systems. To solve the problem from both sides, we propose a novel guidance network that promotes knowledge sharing during training, taking advantage of the multimodal representations to train better single-modality models for inference. Real-life experiment in violence detection shows that our proposed framework trains single-modality models that significantly outperform its traditionally trained counterparts while maintaining the same inference 
    
[^15]: 半监督双流自注意对抗图对比学习在基于跨主体脑电情绪识别中的应用

    Semi-Supervised Dual-Stream Self-Attentive Adversarial Graph Contrastive Learning for Cross-Subject EEG-based Emotion Recognition. (arXiv:2308.11635v1 [eess.SP])

    [http://arxiv.org/abs/2308.11635](http://arxiv.org/abs/2308.11635)

    本论文提出了一种半监督双流自注意对抗图对比学习框架 DS-AGC，用于跨主体脑电情绪识别中的标记数据不足问题。该框架利用两个并行流提取非结构化和结构化脑电特征，并通过半监督方法解决分布差异和提取有效的基于图的特征表示。

    

    脑电图 (EEG) 是一种有着广泛应用前景的客观情绪识别工具。然而，标记数据的稀缺性仍然是该领域的主要挑战，限制了基于脑电的情绪识别的广泛应用。本文提出了一种半监督双流自注意对抗图对比学习框架 (简称为 DS-AGC)，用于解决基于跨主体脑电情绪识别中有限标记数据的挑战。DS-AGC 框架包括两个并行流，用于提取非结构化和结构化脑电特征。非结构化流采用半监督多领域适应方法，以缓解标记源域、未标记源域和未知目标域之间的分布差异。结构化流则开发了一种图对比学习方法，以半监督方式从多个脑电通道中提取有效的基于图的特征表示。此外，一种自注意

    Electroencephalography (EEG) is an objective tool for emotion recognition with promising applications. However, the scarcity of labeled data remains a major challenge in this field, limiting the widespread use of EEG-based emotion recognition. In this paper, a semi-supervised Dual-stream Self-Attentive Adversarial Graph Contrastive learning framework (termed as DS-AGC) is proposed to tackle the challenge of limited labeled data in cross-subject EEG-based emotion recognition. The DS-AGC framework includes two parallel streams for extracting non-structural and structural EEG features. The non-structural stream incorporates a semi-supervised multi-domain adaptation method to alleviate distribution discrepancy among labeled source domain, unlabeled source domain, and unknown target domain. The structural stream develops a graph contrastive learning method to extract effective graph-based feature representation from multiple EEG channels in a semi-supervised manner. Further, a self-attentiv
    
[^16]: 高效的半监督异构参与者联邦学习

    Efficient Semi-Supervised Federated Learning for Heterogeneous Participants. (arXiv:2307.15870v1 [cs.LG])

    [http://arxiv.org/abs/2307.15870](http://arxiv.org/abs/2307.15870)

    本论文提出了一种高效的半监督异构参与者联邦学习系统，通过引入聚类正则化来改进模型在数据非独立同分布情况下的性能，并对模型收敛性进行了理论和实验研究。

    

    联邦学习（FL）允许多个客户端在私有数据上协同训练机器学习模型，但在资源有限的环境中训练和部署大型模型用于广泛应用是具有挑战性的。幸运的是，分离式联邦学习（SFL）通过减轻客户端的计算和通信负担提供了优秀的解决方案。SFL通常假设客户端具有标记的数据进行本地训练，然而在实践中并非总是如此。以前的研究采用半监督技术来利用FL中的无标记数据，但数据的非独立同分布性提出了确保训练效率的另一个挑战。在这里，我们提出了一种新颖的系统Pseudo-Clustering Semi-SFL，用于在标记数据位于服务器上的情境下训练模型。通过引入聚类正则化，可以提高数据非独立同分布情况下的模型性能。此外，我们对模型收敛性进行了理论和实验研究，发现了...

    Federated Learning (FL) has emerged to allow multiple clients to collaboratively train machine learning models on their private data. However, training and deploying large models for broader applications is challenging in resource-constrained environments. Fortunately, Split Federated Learning (SFL) offers an excellent solution by alleviating the computation and communication burden on the clients SFL often assumes labeled data for local training on clients, however, it is not the case in practice.Prior works have adopted semi-supervised techniques for leveraging unlabeled data in FL, but data non-IIDness poses another challenge to ensure training efficiency. Herein, we propose Pseudo-Clustering Semi-SFL, a novel system for training models in scenarios where labeled data reside on the server. By introducing Clustering Regularization, model performance under data non-IIDness can be improved. Besides, our theoretical and experimental investigations into model convergence reveal that the 
    
[^17]: 基于神经网络的城市时空数据合成方法

    Urban Spatiotemporal Data Synthesis via Neural Disaggregation. (arXiv:2306.07292v1 [cs.LG])

    [http://arxiv.org/abs/2306.07292](http://arxiv.org/abs/2306.07292)

    本研究提出了一种基于神经网络的城市时空数据合成方法，旨在通过分解粗糙的低分辨率地理单元的聚合城市数据来合成细粒度，高分辨率的城市数据，以增加高度聚合的城市数据的可用性和实现价值。

    

    开放数据的细节级别常常与其所能提供的实际效益发生冲突。较不细化的数据可以保护个人隐私，但在一定程度上牺牲了开放数据促进透明度和协助研究的承诺。类似于城市环境中，高层次地理单元的聚合城市数据可能会掩盖城市动态的底层特征，低级别地理单元的变化可能更为明显。本研究旨在通过分解粗糙的低分辨率地理单元的聚合城市数据，合成细粒度，高分辨率的城市数据，以增加高度聚合的城市数据的可用性和实现价值。为了解决一些传统分解方法的简单性问题-1) 我们尝试了许多神经网络模型，这些模型能够建模特征之间复杂的非线性关系。神经方法也可以同时利用空间和时间信息。我们展示了这些神经网络方法的优点。

    The level of granularity of open data often conflicts the benefits it can provide. Less granular data can protect individual privacy, but to certain degrees, sabotage the promise of open data to promote transparency and assist research. Similar in the urban setting, aggregated urban data at high-level geographic units can mask out the underline particularities of city dynamics that may vary at lower areal levels. In this work, we aim to synthesize fine-grained, high resolution urban data, by breaking down aggregated urban data at coarse, low resolution geographic units. The goal is to increase the usability and realize the values as much as possible of highly aggregated urban data. To address the issue of simplicity of some traditional disaggregation methods -- 1) we experimented with numerous neural-based models that are capable of modeling intricate non-linear relationships among features. Neural methods can also leverage both spatial and temporal information concurrently. We showed 
    
[^18]: 基于流导向纳米定位的设计空间探索的见解

    Insights from the Design Space Exploration of Flow-Guided Nanoscale Localization. (arXiv:2305.18493v1 [cs.NI])

    [http://arxiv.org/abs/2305.18493](http://arxiv.org/abs/2305.18493)

    研究了基于流导向纳米定位的设计空间，考虑了能源和信号衰减等因素，为这一新兴领域提供了有希望的解决方案。

    

    具有太赫兹无线通信能力的纳米设备为在人类血液中进行流导向定位提供了基础。此类定位使得将所感受到的事件的位置与事件本身进行匹配成为可能，从而实现了精准医疗方面的早期和精准诊断、降低成本和侵入性。流导向定位仍处于原始阶段，只有少数论文涉及此问题。尽管如此，所提出解决方案的性能评估仍然以非标准化的方式进行，通常只考虑单一的性能指标，并忽略了在这种规模（例如，纳米器件的能量受限）和对于这种具有挑战性的环境（例如，体内太赫兹传播的严重衰减）下相关的各个方面。因此，这些评估具有低水平的真实性，并且无法以客观的方式进行比较。为了解决这个问题，我们考虑了传输能量消耗和信号衰减，对流导向纳米定位的设计空间进行了探索。我们的分析考虑了各种性能指标（例如能量消耗和定位精度）和挑战（例如身体运动和血压），导致我们可以为这个新兴领域提供有希望的解决方案。

    Nanodevices with Terahertz (THz)-based wireless communication capabilities are providing a primer for flow-guided localization within the human bloodstreams. Such localization is allowing for assigning the locations of sensed events with the events themselves, providing benefits in precision medicine along the lines of early and precise diagnostics, and reduced costs and invasiveness. Flow-guided localization is still in a rudimentary phase, with only a handful of works targeting the problem. Nonetheless, the performance assessments of the proposed solutions are already carried out in a non-standardized way, usually along a single performance metric, and ignoring various aspects that are relevant at such a scale (e.g., nanodevices' limited energy) and for such a challenging environment (e.g., extreme attenuation of in-body THz propagation). As such, these assessments feature low levels of realism and cannot be compared in an objective way. Toward addressing this issue, we account for t
    
[^19]: 自监督混合深度学习实现健壮全息毫米波波束成形

    Robust Holographic mmWave Beamforming by Self-Supervised Hybrid Deep Learning. (arXiv:2303.12653v1 [cs.IT])

    [http://arxiv.org/abs/2303.12653](http://arxiv.org/abs/2303.12653)

    本文提出一种自监督混合深度学习网络用于健壮波束成形，能够在两种不同的数据集和各种场景中表现出更强的鲁棒性。

    

    近年来，大规模天线阵列的波束成形被广泛应用于5G和即将推出的6G中，因此各种技术被利用来提高其性能，例如深度学习、高级优化算法等。尽管在许多具有深度学习的先前研究方案中其性能相当吸引人，但通常当环境或数据集发生变化时，其性能会迅速下降。因此，设计具有强大鲁棒性的有效波束成形网络是智能无线通信的一个开放问题。在本文中，我们提出了一个健壮的波束成形自监督网络，并在两种不同数据集和各种场景下进行了验证。仿真结果表明，所提出的具有混合学习的自监督网络在经典的DeepMIMO和新的WAIR-D数据集上具有强大的鲁棒性，适用于各种环境。此外，我们还提出了原理来解释这个翻译的合理性。

    Beamforming with large-scale antenna arrays has been widely used in recent years, which is acknowledged as an important part in 5G and incoming 6G. Thus, various techniques are leveraged to improve its performance, e.g., deep learning, advanced optimization algorithms, etc. Although its performance in many previous research scenarios with deep learning is quite attractive, usually it drops rapidly when the environment or dataset is changed. Therefore, designing effective beamforming network with strong robustness is an open issue for the intelligent wireless communications. In this paper, we propose a robust beamforming self-supervised network, and verify it in two kinds of different datasets with various scenarios. Simulation results show that the proposed self-supervised network with hybrid learning performs well in both classic DeepMIMO and new WAIR-D dataset with the strong robustness under the various environments. Also, we present the principle to explain the rationality of this 
    
[^20]: 混合移动平均场引导的时空数据学习

    Mixed moving average field guided learning for spatio-temporal data. (arXiv:2301.00736v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.00736](http://arxiv.org/abs/2301.00736)

    本论文提出了一种理论引导机器学习方法，采用广义贝叶斯算法进行混合移动平均场引导的时空数据建模，可以进行因果未来预测。

    

    受到混合移动平均场的影响，时空数据的建模是一个多功能的技巧。但是，它们的预测分布通常不可访问。在这个建模假设下，我们定义了一种新的理论引导机器学习方法，采用广义贝叶斯算法进行预测。我们采用Lipschitz预测器（例如线性模型或前馈神经网络），并通过最小化沿空间和时间维度串行相关的数据的新型PAC贝叶斯界限来确定一个随机估计值。进行因果未来预测是我们方法的一个亮点，因为它适用于具有短期和长期相关性的数据。最后，我们通过展示线性预测器和模拟STOU过程的时空数据的示例来展示学习方法的性能。

    Influenced mixed moving average fields are a versatile modeling class for spatio-temporal data. However, their predictive distribution is not generally accessible. Under this modeling assumption, we define a novel theory-guided machine learning approach that employs a generalized Bayesian algorithm to make predictions. We employ a Lipschitz predictor, for example, a linear model or a feed-forward neural network, and determine a randomized estimator by minimizing a novel PAC Bayesian bound for data serially correlated along a spatial and temporal dimension. Performing causal future predictions is a highlight of our methodology as its potential application to data with short and long-range dependence. We conclude by showing the performance of the learning methodology in an example with linear predictors and simulated spatio-temporal data from an STOU process.
    

