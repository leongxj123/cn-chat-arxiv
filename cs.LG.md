# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Large Language Models on Graphs: A Comprehensive Survey](https://rss.arxiv.org/abs/2312.02783) | 这篇论文对在图上的大型语言模型进行了全面调查，研究了纯图形、文本属性图形和文本配对图形三个不同场景下的应用情况，并探讨了基于图形的推理能力是否可以推广到大型语言模型上。 |
| [^2] | [LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership and Reasoning](https://arxiv.org/abs/2404.00027) | 探讨使用大型语言模型作为写作助手引发的写作所有权感和作者身份认知之间的心理困境。 |
| [^3] | [Ink and Individuality: Crafting a Personalised Narrative in the Age of LLMs](https://arxiv.org/abs/2404.00026) | 研究探讨了人们日益依赖的基于LLM的写作助手对创造力和个性可能造成的负面影响，旨在改进人机交互系统和提升写作助手的个性化和个性化功能。 |
| [^4] | [DeNetDM: Debiasing by Network Depth Modulation](https://arxiv.org/abs/2403.19863) | DeNetDM 是一种基于网络深度调制的新型去偏见方法，通过使用来自专家乘积的训练范式，在创建深浅架构的偏见和去偏见分支后，将知识提炼产生目标去偏见模型，相比当前去偏见技术取得更优异的效果。 |
| [^5] | [CMP: Cooperative Motion Prediction with Multi-Agent Communication](https://arxiv.org/abs/2403.17916) | 该论文提出了一种名为CMP的方法，利用LiDAR信号作为输入，通过合作感知和运动预测模块共享信息，解决了合作运动预测的问题。 |
| [^6] | [Physics-Based Causal Reasoning for Safe & Robust Next-Best Action Selection in Robot Manipulation Tasks](https://arxiv.org/abs/2403.14488) | 该论文提出了一个基于物理因果推理的框架，用于机器人在部分可观察的环境中进行概率推理，成功预测积木塔稳定性并选择下一最佳动作。 |
| [^7] | [The Model Openness Framework: Promoting Completeness and Openness for Reproducibility, Transparency and Usability in AI](https://arxiv.org/abs/2403.13784) | 提出了模型开放框架（MOF），它是一个排名分类系统，根据完整性和开放性评估机器学习模型，旨在促进完整性、开放性以及遵循开放科学原则，可以帮助准确识别模型的透明性和可重现性。 |
| [^8] | [PARAMANU-AYN: An Efficient Novel Generative and Instruction-tuned Language Model for Indian Legal Case Documents](https://arxiv.org/abs/2403.13681) | PARAMANU-AYN是一种基于印度法律案例文件的高效生成式语言模型，采用自回归解码器进行预训练，并经过面向指令的微调，在各种法律任务上取得了良好表现。 |
| [^9] | [3M-Diffusion: Latent Multi-Modal Diffusion for Text-Guided Generation of Molecular Graphs](https://arxiv.org/abs/2403.07179) | 提出了3M-Diffusion，一种新颖的多模态分子图生成方法，可以生成具有所需属性的多样化、理想情况下是新颖的分子。 |
| [^10] | [Benign overfitting in leaky ReLU networks with moderate input dimension](https://arxiv.org/abs/2403.06903) | 研究了在泄漏ReLU网络上使用铰链损失进行训练的过程中，信噪比（SNR）条件对于良性和非良性过拟合的影响，并发现高SNR值会导致良性过拟合，低SNR值则会导致有害过拟合。 |
| [^11] | [Hybrid Quantum-inspired Resnet and Densenet for Pattern Recognition with Completeness Analysis](https://arxiv.org/abs/2403.05754) | 提出了两种根植于残差连接和密集连接的混合量子启发式神经网络，用于更全面地改进和评估新型神经网络在复杂和不可预测环境中的表现 |
| [^12] | [Signature Isolation Forest](https://arxiv.org/abs/2403.04405) | 介绍了一种新颖的异常检测算法"Signature Isolation Forest"，利用粗路径理论的签名变换去除了Functional Isolation Forest的线性内积和词典选择方面的限制。 |
| [^13] | [Collaborative learning of common latent representations in routinely collected multivariate ICU physiological signals](https://arxiv.org/abs/2402.17917) | 该研究提出了一种方法，通过在ICU中常规收集的生理时间序列数据上应用结合了LSTM网络和协同过滤概念的新算法，识别患者之间的共同生理状态，并在实际临床数据上取得了良好的检测性能。 |
| [^14] | [On the Curses of Future and History in Future-dependent Value Functions for Off-policy Evaluation](https://arxiv.org/abs/2402.14703) | 本文提出了针对POMDP结构的新颖覆盖假设，以解决未来依赖价值函数方法中的长度指数增长问题。 |
| [^15] | [Large-Scale Actionless Video Pre-Training via Discrete Diffusion for Efficient Policy Learning](https://arxiv.org/abs/2402.14407) | 利用离散扩散结合生成式预训练和少量机器人视频微调，实现从人类视频到机器人策略学习的知识迁移。 |
| [^16] | [On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices](https://arxiv.org/abs/2402.12817) | 有限标注数据学习对随机性的敏感性，通过系统研究随机因素的影响，揭示了忽略相互作用可能导致的不一致结果。 |
| [^17] | [Short-Period Variables in TESS Full-Frame Image Light Curves Identified via Convolutional Neural Networks](https://arxiv.org/abs/2402.12369) | 提出了一种通过卷积神经网络识别TESS全画幅图像光变曲线中的短周期变量的方法，实现了高效率的大规模档案搜索。 |
| [^18] | [Second Order Methods for Bandit Optimization and Control](https://arxiv.org/abs/2402.08929) | 本文提出了一种简单实用的二阶赌徒凸优化算法，并证明了其对于一类称之为$\kappa$-凸的凸函数实现了最优的后期损失界限。该算法在多个应用中表现出高效性能，包括赌徒逻辑回归。 |
| [^19] | [Graph Feature Preprocessor: Real-time Extraction of Subgraph-based Features from Transaction Graphs](https://arxiv.org/abs/2402.08593) | 本文介绍了一种名为"图特征预处理器"的软件库，可以从实时交易图中检测典型的洗钱和欺诈模式，并生成丰富的交易特征，从而显著提高机器学习模型的预测准确率。 |
| [^20] | [Unichain and Aperiodicity are Sufficient for Asymptotic Optimality of Average-Reward Restless Bandits](https://arxiv.org/abs/2402.05689) | 该论文提出了一种新的策略类别，用于解决无限期平均回报好转胆冒险问题。研究表明，在单臂松弛问题是Unichain和非周期性的情况下，该策略类别具有渐进最优性。 |
| [^21] | [CataractBot: An LLM-Powered Expert-in-the-Loop Chatbot for Cataract Patients](https://arxiv.org/abs/2402.04620) | CataractBot是一种基于LLM的白内障患者专家辅助聊天机器人，通过查询知识库提供即时的答案和专家验证的回复。在实地部署研究中证明了其价值所在。 |
| [^22] | [Analysis of Linear Mode Connectivity via Permutation-Based Weight Matching](https://arxiv.org/abs/2402.04051) | 通过基于排列的权重匹配分析线性模式连接性，我们实验证明了通过权重匹配找到的排列可以改变权重矩阵奇异向量的方向，但不能改变奇异值。这一发现对于理解随机梯度下降的有效性及其在模型合并等领域的应用具有重要意义。 |
| [^23] | [The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs](https://arxiv.org/abs/2402.03757) | 本论文研究发现，虚假图像会导致多模态大型语言模型产生幻觉，作者提出了评估幻觉程度的基准CorrelationQA，并发现主流多模态大型语言模型普遍受到这种本能偏见的影响。 |
| [^24] | [Probabilistic Actor-Critic: Learning to Explore with PAC-Bayes Uncertainty](https://arxiv.org/abs/2402.03055) | 概率演员-评论家算法（PAC）通过在评论家中建模和推断不确定性，以改进强化学习中的连续控制性能，并实现自适应的探索策略。 |
| [^25] | [Risk-Sensitive Diffusion: Learning the Underlying Distribution from Noisy Samples](https://arxiv.org/abs/2402.02081) | 该论文提出了一种风险敏感的扩散模型，通过调整噪声样本的分布，减少误导并从中获取信息，有效地应对噪声样本的存在，并在合成和真实世界数据集上取得了良好的效果。 |
| [^26] | [Distilling Conditional Diffusion Models for Offline Reinforcement Learning through Trajectory Stitching](https://arxiv.org/abs/2402.00807) | 通过新的轨迹拼接算法和奖励生成器，使用条件性扩散模型生成的高回报轨迹与原始轨迹混合，应用于行为克隆的结果表明，学得的规模较小的浅层策略在多个D4RL基准测试中超过或接近深度生成规划器。 |
| [^27] | [Algorithms for Non-Negative Matrix Factorization on Noisy Data With Negative Values](https://arxiv.org/abs/2311.04855) | 本文提出了两种算法，Shift-NMF和Nearly-NMF，可以处理带有负值的嘈杂数据，在不引入正的偏移量的情况下正确恢复非负信号。 |
| [^28] | [LongForm: Effective Instruction Tuning with Reverse Instructions](https://arxiv.org/abs/2304.08460) | 使用反向指令进行有效的指令调优，通过生成一组自然的、适用于长文本生成的指令调优数据集，我们的模型在故事/菜谱生成和长篇问答等任务上优于10倍规模更大的语言模型，而无需指令调优。 |
| [^29] | [Representations learnt by SGD and Adaptive learning rules: Conditions that vary sparsity and selectivity in neural network](https://arxiv.org/abs/2201.11653) | 本文调查了导致神经网络稀疏性和选择性增加的各种条件。 |
| [^30] | [Inverse Molecular Design with Multi-Conditional Diffusion Guidance.](http://arxiv.org/abs/2401.13858) | 借助多条件扩散引导的逆分子设计模型在材料和药物发现方面具有巨大潜力。通过引入Transformer-based去噪模型和图依赖的扩散过程，该模型能够在多个条件约束下准确地生成聚合物和小分子。 |
| [^31] | [Solution of the Probabilistic Lambert Problem: Connections with Optimal Mass Transport, Schr\"odinger Bridge and Reaction-Diffusion PDEs.](http://arxiv.org/abs/2401.07961) | 这项研究将概率Lambert问题与最优质量传输、Schr\"odinger桥和反应-扩散偏微分方程等领域连接起来，从而解决了概率Lambert问题的解的存在和唯一性，并提供了数值求解的方法。 |
| [^32] | [Machine learning a fixed point action for SU(3) gauge theory with a gauge equivariant convolutional neural network.](http://arxiv.org/abs/2401.06481) | 本研究使用具有精确规范不变性的卷积神经网络，通过机器学习方法找到了四维SU（3）规范理论的优秀固定点作用的参数化方法，为未来的蒙特卡洛模拟打下了必要的基础。 |
| [^33] | [Machine Teaching for Building Modular AI Agents based on Zero-shot Learners.](http://arxiv.org/abs/2401.05467) | 这篇论文提出了一种机器教学方法，通过利用迭代机器教学和任务特定的替代模型，增强了利用大语言模型作为零样本学习器的模块化AI代理的鲁棒性和性能。 |
| [^34] | [Fair Sampling in Diffusion Models through Switching Mechanism.](http://arxiv.org/abs/2401.03140) | 本论文提出了一种称为“属性切换”的公平抽样机制，用于解决扩散模型中公平性的问题。通过在生成的数据中混淆敏感属性，该方法能够实现生成公平数据和保持数据效用的目标。 |
| [^35] | [Sum-of-Parts Models: Faithful Attributions for Groups of Features.](http://arxiv.org/abs/2310.16316) | Sum-of-Parts模型通过构造保证特征组归因的忠实性，将预测分解为可解释的分数之和，帮助天体物理学家发现了关于星系形成的新知识。 |
| [^36] | [Generalizing Medical Image Representations via Quaternion Wavelet Networks.](http://arxiv.org/abs/2310.10224) | 本文提出了一种名为QUAVE的四元数小波网络，可以从医学图像中提取显著特征。该网络可以与现有的医学图像分析或综合任务结合使用，并推广了对单通道数据的采用。通过四元数小波变换和加权处理，QUAVE能够处理具有较大变化的医学数据。 |
| [^37] | [Foundation Reinforcement Learning: towards Embodied Generalist Agents with Foundation Prior Assistance.](http://arxiv.org/abs/2310.02635) | 本研究提出了一种基于具身基础先验的基础强化学习框架，通过加速训练过程来提高样本效率。 |
| [^38] | [AtomSurf : Surface Representation for Learning on Protein Structures.](http://arxiv.org/abs/2309.16519) | 本文研究了将蛋白质作为3D网格的表面表示，并提出了一种结合图表面的协同方法，既有竞争优势，又有实际应用潜力。 |
| [^39] | [Regret-Optimal Federated Transfer Learning for Kernel Regression with Applications in American Option Pricing.](http://arxiv.org/abs/2309.04557) | 本论文提出了一种遗憾最优算法的迭代方案，用于联邦迁移学习，在核回归模型中具体化，并提出了一个几乎遗憾最优的启发式算法，可以减小生成参数与专门参数之间的累积偏差。 |
| [^40] | [Streaming Compression of Scientific Data via weak-SINDy.](http://arxiv.org/abs/2308.14962) | 本文开发了一种专门用于压缩流式科学数据的流式弱SINDy算法。与经典的离线压缩算法不同，流式压缩算法可以在数据生成过程中进行压缩，使其适用于科学数据压缩。流式弱SINDy算法利用了底层数据的特征来进行压缩。 |
| [^41] | [JL-lemma derived Optimal Projections for Discriminative Dictionary Learning.](http://arxiv.org/abs/2308.13991) | 本文提出了一种名为JLSPCADL的新方法，通过利用Johnson-Lindenstrauss引理选择一个维数的转换空间，从而实现判别式字典学习并提供更好的分类结果。 |
| [^42] | [Optimal Learners for Realizable Regression: PAC Learning and Online Learning.](http://arxiv.org/abs/2307.03848) | 本论文研究了可实现回归问题的PAC学习和在线学习的统计复杂度，并提出了对于可学习性的必要条件和充分条件。 |
| [^43] | [On the Statistical Efficiency of Mean Field Reinforcement Learning with General Function Approximation.](http://arxiv.org/abs/2305.11283) | 本文研究了一般函数逼近下的均场控制(MFC)和均场博弈(MFG)中的强化学习的统计效率，提出了基于乐观最大似然估计的算法，并仅对转移动力学具有Lipschitz连续性的假设，最后建立了一个指数级的下界支持MFC设置。 |
| [^44] | [TACOS: Topology-Aware Collective Algorithm Synthesizer for Distributed Training.](http://arxiv.org/abs/2304.05301) | TACOS 是一个能够自动合成任意输入网络拓扑的面向拓扑结构的集合合成器。与基准算法相比，TACOS 合成的 All-Reduce 算法速度提高了 3.73 倍，为 512-NPU 系统合成集体算法只需 6.1 分钟。 |
| [^45] | [Towards Data-and Knowledge-Driven Artificial Intelligence: A Survey on Neuro-Symbolic Computing.](http://arxiv.org/abs/2210.15889) | 神经符号计算是将符号和统计认知范式整合的研究领域，在推理和解释性以及神经网络学习等方面具有潜力。本文综述了该领域的最新进展和重要贡献，并探讨了其成功应用案例和未来发展方向。 |

# 详细

[^1]: 在图上的大型语言模型：一项全面调查

    Large Language Models on Graphs: A Comprehensive Survey

    [https://rss.arxiv.org/abs/2312.02783](https://rss.arxiv.org/abs/2312.02783)

    这篇论文对在图上的大型语言模型进行了全面调查，研究了纯图形、文本属性图形和文本配对图形三个不同场景下的应用情况，并探讨了基于图形的推理能力是否可以推广到大型语言模型上。

    

    大型语言模型（LLMs），如GPT4和LLaMA，由于其强大的文本编码/解码能力和新发现的紧急能力（例如推理）在自然语言处理方面取得了显著的进展。虽然LLMs主要设计用于处理纯文本，但在许多现实场景中，文本数据与图形形式的丰富结构信息相关联（例如学术网络和电子商务网络），或者图形数据与丰富的文本信息配对（例如带有描述的分子）。此外，尽管LLMs已经展示了其基于纯文本的推理能力，但尚未探索此类能力是否可以推广到图形上（即基于图形的推理）。在本文中，我们对在图上的大型语言模型相关场景和技术进行了系统回顾。我们首先总结了采用LLMs在图形上的潜在场景，分为纯图形、文本属性图形和文本配对图形三个类别。

    Large language models (LLMs), such as GPT4 and LLaMA, are creating significant advancements in natural language processing, due to their strong text encoding/decoding ability and newly found emergent capability (e.g., reasoning). While LLMs are mainly designed to process pure texts, there are many real-world scenarios where text data is associated with rich structure information in the form of graphs (e.g., academic networks, and e-commerce networks) or scenarios where graph data is paired with rich textual information (e.g., molecules with descriptions). Besides, although LLMs have shown their pure text-based reasoning ability, it is underexplored whether such ability can be generalized to graphs (i.e., graph-based reasoning). In this paper, we provide a systematic review of scenarios and techniques related to large language models on graphs. We first summarize potential scenarios of adopting LLMs on graphs into three categories, namely pure graphs, text-attributed graphs, and text-pa
    
[^2]: LLM作为写作助手：探讨所有权感和推理的视角

    LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership and Reasoning

    [https://arxiv.org/abs/2404.00027](https://arxiv.org/abs/2404.00027)

    探讨使用大型语言模型作为写作助手引发的写作所有权感和作者身份认知之间的心理困境。

    

    写作中的所有权感限制了我们对思想、时间和贡献的投入，导致对产出物的依恋。然而，使用写作助手引入了一种心理困境，因为一些内容并非直接我们的创作。我们往往更倾向于在创造性任务中更多地归功于大型语言模型（LLMs），尽管它们对所有任务都是平等的。此外，虽然我们可能不会完全声称对由LLM生成的内容拥有所有权，但却自由地声称作者身份。我们进行了一项简短调查来研究这些问题，并了解潜在的认知过程，以更好地了解人机交互在写作中的应用并改进写作辅助系统。

    arXiv:2404.00027v1 Announce Type: cross  Abstract: Sense of ownership in writing confines our investment of thoughts, time, and contribution, leading to attachment to the output. However, using writing assistants introduces a mental dilemma, as some content isn't directly our creation. For instance, we tend to credit Large Language Models (LLMs) more in creative tasks, even though all tasks are equal for them. Additionally, while we may not claim complete ownership of LLM-generated content, we freely claim authorship. We conduct a short survey to examine these issues and understand underlying cognitive processes in order to gain a better knowledge of human-computer interaction in writing and improve writing aid systems.
    
[^3]: 墨水与个性：在LLMs时代塑造个性化叙事

    Ink and Individuality: Crafting a Personalised Narrative in the Age of LLMs

    [https://arxiv.org/abs/2404.00026](https://arxiv.org/abs/2404.00026)

    研究探讨了人们日益依赖的基于LLM的写作助手对创造力和个性可能造成的负面影响，旨在改进人机交互系统和提升写作助手的个性化和个性化功能。

    

    个性和个性化构成了使每个作家独特并影响其文字以有效吸引读者同时传达真实性的独特特征。然而，我们日益依赖基于LLM的写作助手可能会危及我们的创造力和个性。我们经常忽视这一趋势对我们的创造力和独特性的负面影响，尽管可能会造成后果。本研究通过进行简要调查探索不同的观点和概念，以及尝试理解人们的观点，结合以往在该领域的研究，来研究这些问题。解决这些问题对于改进人机交互系统和增强个性化和个性化写作助手至关重要。

    arXiv:2404.00026v1 Announce Type: cross  Abstract: Individuality and personalization comprise the distinctive characteristics that make each writer unique and influence their words in order to effectively engage readers while conveying authenticity. However, our growing reliance on LLM-based writing assistants risks compromising our creativity and individuality over time. We often overlook the negative impacts of this trend on our creativity and uniqueness, despite the possible consequences. This study investigates these concerns by performing a brief survey to explore different perspectives and concepts, as well as trying to understand people's viewpoints, in conjunction with past studies in the area. Addressing these issues is essential for improving human-computer interaction systems and enhancing writing assistants for personalization and individuality.
    
[^4]: DeNetDM: 通过网络深度调制来消除偏见

    DeNetDM: Debiasing by Network Depth Modulation

    [https://arxiv.org/abs/2403.19863](https://arxiv.org/abs/2403.19863)

    DeNetDM 是一种基于网络深度调制的新型去偏见方法，通过使用来自专家乘积的训练范式，在创建深浅架构的偏见和去偏见分支后，将知识提炼产生目标去偏见模型，相比当前去偏见技术取得更优异的效果。

    

    当神经网络在偏见数据集上训练时，它们往往会无意间学习到虚假的相关性，从而导致在实现强大的泛化性和鲁棒性方面面临挑战。目前解决这种偏见的方法通常包括利用偏见注释、根据伪偏见标签进行加权重、或通过增强技术增加偏见冲突数据点的多样性。我们引入了DeNetDM，这是一种基于观察结果的新型去偏见方法，浅层神经网络优先学习核心属性，而更深层次的神经网络在获取不同信息时强调偏见。我们利用从专家乘积中推导出的训练范式，创建了深浅架构的偏见和去偏见分支，然后用知识提炼产生目标的去偏见模型。大量实验证明，我们的方法优于当前的去偏见技术，实现了一个...

    arXiv:2403.19863v1 Announce Type: new  Abstract: When neural networks are trained on biased datasets, they tend to inadvertently learn spurious correlations, leading to challenges in achieving strong generalization and robustness. Current approaches to address such biases typically involve utilizing bias annotations, reweighting based on pseudo-bias labels, or enhancing diversity within bias-conflicting data points through augmentation techniques. We introduce DeNetDM, a novel debiasing method based on the observation that shallow neural networks prioritize learning core attributes, while deeper ones emphasize biases when tasked with acquiring distinct information. Using a training paradigm derived from Product of Experts, we create both biased and debiased branches with deep and shallow architectures and then distill knowledge to produce the target debiased model. Extensive experiments and analyses demonstrate that our approach outperforms current debiasing techniques, achieving a not
    
[^5]: CMP：具有多智能体通信的合作运动预测

    CMP: Cooperative Motion Prediction with Multi-Agent Communication

    [https://arxiv.org/abs/2403.17916](https://arxiv.org/abs/2403.17916)

    该论文提出了一种名为CMP的方法，利用LiDAR信号作为输入，通过合作感知和运动预测模块共享信息，解决了合作运动预测的问题。

    

    随着自动驾驶车辆（AVs）的发展和车联网（V2X）通信的成熟，合作连接的自动化车辆（CAVs）的功能变得可能。本文基于合作感知，探讨了合作运动预测的可行性和有效性。我们的方法CMP以LiDAR信号作为输入，以增强跟踪和预测能力。与过去专注于合作感知或运动预测的工作不同，我们的框架是我们所知的第一个解决CAVs在感知和预测模块中共享信息的统一问题。我们的设计中还融入了能够容忍现实V2X带宽限制和传输延迟的独特能力，同时处理庞大的感知表示。我们还提出了预测聚合模块，统一了预测

    arXiv:2403.17916v1 Announce Type: cross  Abstract: The confluence of the advancement of Autonomous Vehicles (AVs) and the maturity of Vehicle-to-Everything (V2X) communication has enabled the capability of cooperative connected and automated vehicles (CAVs). Building on top of cooperative perception, this paper explores the feasibility and effectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR signals as input to enhance tracking and prediction capabilities. Unlike previous work that focuses separately on either cooperative perception or motion prediction, our framework, to the best of our knowledge, is the first to address the unified problem where CAVs share information in both perception and prediction modules. Incorporated into our design is the unique capability to tolerate realistic V2X bandwidth limitations and transmission delays, while dealing with bulky perception representations. We also propose a prediction aggregation module, which unifies the predict
    
[^6]: 基于物理学因果推理的机器人操作任务中安全稳健的下一最佳动作选择

    Physics-Based Causal Reasoning for Safe & Robust Next-Best Action Selection in Robot Manipulation Tasks

    [https://arxiv.org/abs/2403.14488](https://arxiv.org/abs/2403.14488)

    该论文提出了一个基于物理因果推理的框架，用于机器人在部分可观察的环境中进行概率推理，成功预测积木塔稳定性并选择下一最佳动作。

    

    安全高效的物体操作是许多真实世界机器人应用的关键推手。然而，这种挑战在于机器人操作必须对一系列传感器和执行器的不确定性具有稳健性。本文提出了一个基于物理知识和因果推理的框架，用于让机器人在部分可观察的环境中对候选动作进行概率推理，以完成一个积木堆叠任务。我们将刚体系统动力学的基于物理学的仿真与因果贝叶斯网络（CBN）结合起来，定义了机器人决策过程的因果生成概率模型。通过基于仿真的蒙特卡洛实验，我们展示了我们的框架成功地能够：(1) 高准确度地预测积木塔的稳定性（预测准确率：88.6%）；和，(2) 为积木堆叠任务选择一个近似的下一最佳动作，供整合的机器人系统执行，实现94.2%的任务成功率。

    arXiv:2403.14488v1 Announce Type: cross  Abstract: Safe and efficient object manipulation is a key enabler of many real-world robot applications. However, this is challenging because robot operation must be robust to a range of sensor and actuator uncertainties. In this paper, we present a physics-informed causal-inference-based framework for a robot to probabilistically reason about candidate actions in a block stacking task in a partially observable setting. We integrate a physics-based simulation of the rigid-body system dynamics with a causal Bayesian network (CBN) formulation to define a causal generative probabilistic model of the robot decision-making process. Using simulation-based Monte Carlo experiments, we demonstrate our framework's ability to successfully: (1) predict block tower stability with high accuracy (Pred Acc: 88.6%); and, (2) select an approximate next-best action for the block stacking task, for execution by an integrated robot system, achieving 94.2% task succe
    
[^7]: 模型开放框架: 促进人工智能中的可重现性、透明度和可用性的完整性和开放性

    The Model Openness Framework: Promoting Completeness and Openness for Reproducibility, Transparency and Usability in AI

    [https://arxiv.org/abs/2403.13784](https://arxiv.org/abs/2403.13784)

    提出了模型开放框架（MOF），它是一个排名分类系统，根据完整性和开放性评估机器学习模型，旨在促进完整性、开放性以及遵循开放科学原则，可以帮助准确识别模型的透明性和可重现性。

    

    生成式人工智能（GAI）提供了前所未有的可能性，但其商业化引发了关于透明度、可重现性、偏见和安全性的担忧。许多"开源"的GAI模型缺乏完整理解和再现所必需的组件，一些采用限制性许可证，这种行为被称为"开源洗白"。我们提出了模型开放框架（MOF），这是一个根据完整性和开放性对机器学习模型进行排名分类的系统，遵循开放科学、开源、开放数据和开放获取的原则。MOF要求模型开发生命周期的特定组件被包含并根据适当的开放许可证发布。该框架旨在防止宣称自己是开放的模型被误解，指导研究人员和开发者以宽松的许可证发布所有模型组件，并帮助公司、学术界和爱好者识别可以安全采用的模型。

    arXiv:2403.13784v1 Announce Type: new  Abstract: Generative AI (GAI) offers unprecedented possibilities but its commercialization has raised concerns about transparency, reproducibility, bias, and safety. Many "open-source" GAI models lack the necessary components for full understanding and reproduction, and some use restrictive licenses, a practice known as "openwashing." We propose the Model Openness Framework (MOF), a ranked classification system that rates machine learning models based on their completeness and openness, following principles of open science, open source, open data, and open access. The MOF requires specific components of the model development lifecycle to be included and released under appropriate open licenses. This framework aims to prevent misrepresentation of models claiming to be open, guide researchers and developers in providing all model components under permissive licenses, and help companies, academia, and hobbyists identify models that can be safely adop
    
[^8]: PARAMANU-AYN：一种有效的新型生成式、面向印度法律案例文件的语言模型

    PARAMANU-AYN: An Efficient Novel Generative and Instruction-tuned Language Model for Indian Legal Case Documents

    [https://arxiv.org/abs/2403.13681](https://arxiv.org/abs/2403.13681)

    PARAMANU-AYN是一种基于印度法律案例文件的高效生成式语言模型，采用自回归解码器进行预训练，并经过面向指令的微调，在各种法律任务上取得了良好表现。

    

    在这篇论文中，我们介绍了PARAMANU-AYN，这是一个仅基于印度最高法院案例文件、印度宪法和印度刑法的语言模型。这种新颖的基于自回归（AR）解码器的模型是从头开始在上下文大小为8192的情况下进行预训练的。我们在困惑度指标上评估了我们的预训练法律模型。我们还对一组包括各种法律任务（如法律推理、判决解释、法律条款生成、法律草拟、法律合同草拟、案件摘要、宪法问题回答等）的10,763条指令进行了针对性训练。我们还通过GPT-3.5-Turbo对面向指令的模型的提示响应进行了在10分制度上的清晰度、相关性、完整性和法律推理指标的评估。我们的模型可以在CPU上运行，并实现每秒42.46个令牌的CPU推理速度。

    arXiv:2403.13681v1 Announce Type: new  Abstract: In this paper, we present PARAMANU-AYN, a language model based exclusively on case documents of the Supreme Court of India, the Constitution of India, and the Indian Penal Code. The novel Auto Regressive (AR) decoder based model is pretrained from scratch at a context size of 8192. We evaluated our pretrained legal model on perplexity metrics. We also instruction-tuned our pretrained model on a set of 10,763 instructions covering various legal tasks such as legal reasoning, judgement explanation, legal clause generation, legal drafting, legal contract drafting, case summarization, constitutional question-answering, etc. We also evaluated the responses of prompts for instruction-tuned models by GPT-3.5-Turbo on clarity, relevance, completeness, and legal reasoning metrics in a scale of 10. Our model can be run on CPU and achieved 42.46 tokens/sec CPU inference speed. We found that our models, despite not being pretrained on legal books, v
    
[^9]: 3M-Diffusion：用于文本引导生成分子图的潜在多模态扩散

    3M-Diffusion: Latent Multi-Modal Diffusion for Text-Guided Generation of Molecular Graphs

    [https://arxiv.org/abs/2403.07179](https://arxiv.org/abs/2403.07179)

    提出了3M-Diffusion，一种新颖的多模态分子图生成方法，可以生成具有所需属性的多样化、理想情况下是新颖的分子。

    

    生成具有所需属性的分子是一项关键任务，在药物发现和材料设计中具有广泛应用。受到大型语言模型的最新进展的启发，越来越多的人对使用分子的自然语言描述来生成具有所需属性的分子产生了兴趣。大多数现有方法侧重于生成与文本描述精确匹配的分子。然而，实际应用需要能够生成具有所需属性的多样化，理想情况下是新颖的分子的方法。我们提出了一种新颖的多模态分子图生成方法3M-Diffusion，以解决这一挑战。

    arXiv:2403.07179v1 Announce Type: cross  Abstract: Generating molecules with desired properties is a critical task with broad applications in drug discovery and materials design. Inspired by recent advances in large language models, there is a growing interest in using natural language descriptions of molecules to generate molecules with the desired properties. Most existing methods focus on generating molecules that precisely match the text description. However, practical applications call for methods that generate diverse, and ideally novel, molecules with the desired properties. We propose 3M-Diffusion, a novel multi-modal molecular graph generation method, to address this challenge. 3M-Diffusion first encodes molecular graphs into a graph latent space aligned with text descriptions. It then reconstructs the molecular structure and atomic attributes based on the given text descriptions using the molecule decoder. It then learns a probabilistic mapping from the text space to the late
    
[^10]: 具有适度输入维度的泄漏ReLU网络中的良性过拟合问题

    Benign overfitting in leaky ReLU networks with moderate input dimension

    [https://arxiv.org/abs/2403.06903](https://arxiv.org/abs/2403.06903)

    研究了在泄漏ReLU网络上使用铰链损失进行训练的过程中，信噪比（SNR）条件对于良性和非良性过拟合的影响，并发现高SNR值会导致良性过拟合，低SNR值则会导致有害过拟合。

    

    良性过拟合问题探讨了一个模型是否能够完美地拟合嘈杂的训练数据，同时又能够很好地泛化。我们研究了在二层泄漏ReLU网络上使用铰链损失进行训练的良性过拟合问题，针对二分类任务。我们考虑输入数据，可以分解为一个共同信号和一个随机噪声成分的总和，这两者相互正交。我们表征了模型参数的信噪比（SNR）条件，导致了良性和非良性（有害）过拟合：特别是，如果SNR很高，则发生良性过拟合，相反，如果SNR很低，则发生有害过拟合。我们将良性和非良性过拟合归因于一个近似边界最大化性质，并展示了在铰链损失下使用梯度下降（GD）训练的泄漏ReLU网络满足这一性质。与以前的工作相比，我们不需要nea

    arXiv:2403.06903v1 Announce Type: new  Abstract: The problem of benign overfitting asks whether it is possible for a model to perfectly fit noisy training data and still generalize well. We study benign overfitting in two-layer leaky ReLU networks trained with the hinge loss on a binary classification task. We consider input data which can be decomposed into the sum of a common signal and a random noise component, which lie on subspaces orthogonal to one another. We characterize conditions on the signal to noise ratio (SNR) of the model parameters giving rise to benign versus non-benign, or harmful, overfitting: in particular, if the SNR is high then benign overfitting occurs, conversely if the SNR is low then harmful overfitting occurs. We attribute both benign and non-benign overfitting to an approximate margin maximization property and show that leaky ReLU networks trained on hinge loss with Gradient Descent (GD) satisfy this property. In contrast to prior work we do not require nea
    
[^11]: 模式识别的混合量子启发式ResNet和DenseNet及其完整性分析

    Hybrid Quantum-inspired Resnet and Densenet for Pattern Recognition with Completeness Analysis

    [https://arxiv.org/abs/2403.05754](https://arxiv.org/abs/2403.05754)

    提出了两种根植于残差连接和密集连接的混合量子启发式神经网络，用于更全面地改进和评估新型神经网络在复杂和不可预测环境中的表现

    

    随着当今数字技术的接近，深度神经网络正成为人工智能繁荣的基础算法。然而，不断发展的社会需求正在强调替代传统神经网络的新方法的必要性。同时，后摩尔时代的来临推动了具有卓越潜力的量子启发式神经网络在某些情况下的发展。然而，由于目前新旧深度学习模型之间比较中存在含糊指标，因此一套明确的评估系统与详细的指标是非常重要和不可或缺的。因此，为了更全面地改进和评估新型神经网络在复杂和不可预测环境中的表现，我们提出了两种根植于残差连接和密集连接的混合量子启发式神经网络，用于模式识别。

    arXiv:2403.05754v1 Announce Type: new  Abstract: With the contemporary digital technology approaching, deep neural networks are emerging as the foundational algorithm of the artificial intelligence boom. Whereas, the evolving social demands have been emphasizing the necessity of novel methodologies to substitute traditional neural networks. Concurrently, the advent of the post-Moore era has spurred the development of quantum-inspired neural networks with outstanding potentials at certain circumstances. Nonetheless, a definitive evaluating system with detailed metrics is tremendously vital and indispensable owing to the vague indicators in comparison between the novel and traditional deep learning models at present. Hence, to improve and evaluate the performances of the novel neural networks more comprehensively in complex and unpredictable environments, we propose two hybrid quantum-inspired neural networks which are rooted in residual and dense connections respectively for pattern rec
    
[^12]: Signature Isolation Forest

    Signature Isolation Forest

    [https://arxiv.org/abs/2403.04405](https://arxiv.org/abs/2403.04405)

    介绍了一种新颖的异常检测算法"Signature Isolation Forest"，利用粗路径理论的签名变换去除了Functional Isolation Forest的线性内积和词典选择方面的限制。

    

    Functional Isolation Forest (FIF)是一种针对功能数据设计的最新一流异常检测(AD)算法。它依赖于一种树分区过程，通过将每个曲线观测投影到通过线性内积绘制的词典上来计算异常得分。本文通过引入“Signature Isolation Forest”，一种利用粗路径理论签名变换的新颖AD算法类，来解决这些挑战。我们的目标是通过提出两种算法来消除FIF施加的限制，这两种算法特别针对FIF内积的线性性和词典的选择。

    arXiv:2403.04405v1 Announce Type: cross  Abstract: Functional Isolation Forest (FIF) is a recent state-of-the-art Anomaly Detection (AD) algorithm designed for functional data. It relies on a tree partition procedure where an abnormality score is computed by projecting each curve observation on a drawn dictionary through a linear inner product. Such linear inner product and the dictionary are a priori choices that highly influence the algorithm's performances and might lead to unreliable results, particularly with complex datasets. This work addresses these challenges by introducing \textit{Signature Isolation Forest}, a novel AD algorithm class leveraging the rough path theory's signature transform. Our objective is to remove the constraints imposed by FIF through the proposition of two algorithms which specifically target the linearity of the FIF inner product and the choice of the dictionary. We provide several numerical experiments, including a real-world applications benchmark sho
    
[^13]: ICU生理信号常见潜在表示的协同学习

    Collaborative learning of common latent representations in routinely collected multivariate ICU physiological signals

    [https://arxiv.org/abs/2402.17917](https://arxiv.org/abs/2402.17917)

    该研究提出了一种方法，通过在ICU中常规收集的生理时间序列数据上应用结合了LSTM网络和协同过滤概念的新算法，识别患者之间的共同生理状态，并在实际临床数据上取得了良好的检测性能。

    

    在重症监护病房（ICU）中，丰富的多变量时间序列为机器学习（ML）提供了增强患者表型的机会。与先前专注于电子健康记录（EHR）的研究相比，我们在这里提出了一种使用常规收集的生理时间序列数据进行表型分类的ML方法。我们的新算法将长短期记忆（LSTM）网络与协同过滤概念结合起来，以识别患者之间的共同生理状态。在用于脑损伤患者颅内高压（IH）检测的实际ICU临床数据上进行测试时，我们的方法实现了0.889的曲线下面积（AUC）和0.725的平均准确率（AP）。此外，我们的算法在学习生理信号的更结构化潜在表示方面优于自动编码器。这些发现突显了我们方法在利用常规收集的数据进行患者表型分类方面的潜力。

    arXiv:2402.17917v1 Announce Type: new  Abstract: In Intensive Care Units (ICU), the abundance of multivariate time series presents an opportunity for machine learning (ML) to enhance patient phenotyping. In contrast to previous research focused on electronic health records (EHR), here we propose an ML approach for phenotyping using routinely collected physiological time series data. Our new algorithm integrates Long Short-Term Memory (LSTM) networks with collaborative filtering concepts to identify common physiological states across patients. Tested on real-world ICU clinical data for intracranial hypertension (IH) detection in patients with brain injury, our method achieved an area under the curve (AUC) of 0.889 and average precision (AP) of 0.725. Moreover, our algorithm outperforms autoencoders in learning more structured latent representations of the physiological signals. These findings highlight the promise of our methodology for patient phenotyping, leveraging routinely collecte
    
[^14]: 在未来依赖价值函数中探讨未来和历史的诅咒在离线评估中的应用

    On the Curses of Future and History in Future-dependent Value Functions for Off-policy Evaluation

    [https://arxiv.org/abs/2402.14703](https://arxiv.org/abs/2402.14703)

    本文提出了针对POMDP结构的新颖覆盖假设，以解决未来依赖价值函数方法中的长度指数增长问题。

    

    我们研究了在部分可观测环境中复杂观测的离线评估(OPE)，旨在开发能够避免对时间跨度指数依赖的估计器。最近，Uehara等人（2022年）提出了未来依赖价值函数作为解决这一问题的一个有前途的框架。然而，该框架也取决于未来依赖价值函数的有界性以及其他相关数量，我们发现这些数量可能会随着长度呈指数增长，从而抹去该方法的优势。在本文中，我们发现了针对POMDP结构的新颖覆盖假设。

    arXiv:2402.14703v1 Announce Type: cross  Abstract: We study off-policy evaluation (OPE) in partially observable environments with complex observations, with the goal of developing estimators whose guarantee avoids exponential dependence on the horizon. While such estimators exist for MDPs and POMDPs can be converted to history-based MDPs, their estimation errors depend on the state-density ratio for MDPs which becomes history ratios after conversion, an exponential object. Recently, Uehara et al. (2022) proposed future-dependent value functions as a promising framework to address this issue, where the guarantee for memoryless policies depends on the density ratio over the latent state space. However, it also depends on the boundedness of the future-dependent value function and other related quantities, which we show could be exponential-in-length and thus erasing the advantage of the method. In this paper, we discover novel coverage assumptions tailored to the structure of POMDPs, such
    
[^15]: 通过离散扩散进行大规模无动作视频预训练，以实现高效策略学习

    Large-Scale Actionless Video Pre-Training via Discrete Diffusion for Efficient Policy Learning

    [https://arxiv.org/abs/2402.14407](https://arxiv.org/abs/2402.14407)

    利用离散扩散结合生成式预训练和少量机器人视频微调，实现从人类视频到机器人策略学习的知识迁移。

    

    学习一个能够完成多个任务的通用实体代理面临挑战，主要源自缺乏有标记动作的机器人数据集。相比之下，存在大量捕捉复杂任务和与物理世界互动的人类视频。本文介绍了一种新颖框架，利用统一的离散扩散将人类视频上的生成式预训练与少量有标记机器人视频上的策略微调结合起来。我们首先将人类和机器人视频压缩成统一的视频标记。在预训练阶段，我们使用一个带有蒙版替换扩散策略的离散扩散模型来预测潜空间中的未来视频标记。在微调阶段，我们 h

    arXiv:2402.14407v1 Announce Type: new  Abstract: Learning a generalist embodied agent capable of completing multiple tasks poses challenges, primarily stemming from the scarcity of action-labeled robotic datasets. In contrast, a vast amount of human videos exist, capturing intricate tasks and interactions with the physical world. Promising prospects arise for utilizing actionless human videos for pre-training and transferring the knowledge to facilitate robot policy learning through limited robot demonstrations. In this paper, we introduce a novel framework that leverages a unified discrete diffusion to combine generative pre-training on human videos and policy fine-tuning on a small number of action-labeled robot videos. We start by compressing both human and robot videos into unified video tokens. In the pre-training stage, we employ a discrete diffusion model with a mask-and-replace diffusion strategy to predict future video tokens in the latent space. In the fine-tuning stage, we h
    
[^16]: 有限标注数据学习对随机性的敏感性：相互作用和系统选择的影响

    On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices

    [https://arxiv.org/abs/2402.12817](https://arxiv.org/abs/2402.12817)

    有限标注数据学习对随机性的敏感性，通过系统研究随机因素的影响，揭示了忽略相互作用可能导致的不一致结果。

    

    有限标注数据学习可以在标签不足时提高性能，但也对所谓的随机因素（例如数据的变化顺序）引入的无法控制的随机性敏感。我们提出了一种方法，系统地调查随机因素的影响，同时考虑它们之间的相互作用。为了测量单个随机因素的真实影响，我们的方法减轻了其他因素的影响，并观察了性能在多次运行中的变化。将我们的方法应用于7个代表性文本分类任务的上下文学习和微调方法以及3个任务的元学习，我们发现：1）现有作品中忽略随机因素之间的相互作用导致了不一致的研究结果，因为错误地归因于随机因素的影响，比如否定了一些一

    arXiv:2402.12817v1 Announce Type: cross  Abstract: While learning with limited labelled data can improve performance when the labels are lacking, it is also sensitive to the effects of uncontrolled randomness introduced by so-called randomness factors (e.g., varying order of data). We propose a method to systematically investigate the effects of randomness factors while taking the interactions between them into consideration. To measure the true effects of an individual randomness factor, our method mitigates the effects of other factors and observes how the performance varies across multiple runs. Applying our method to multiple randomness factors across in-context learning and fine-tuning approaches on 7 representative text classification tasks and meta-learning on 3 tasks, we show that: 1) disregarding interactions between randomness factors in existing works caused inconsistent findings due to incorrect attribution of the effects of randomness factors, such as disproving the consis
    
[^17]: 通过卷积神经网络识别TESS全画幅图像光变曲线中的短周期变量

    Short-Period Variables in TESS Full-Frame Image Light Curves Identified via Convolutional Neural Networks

    [https://arxiv.org/abs/2402.12369](https://arxiv.org/abs/2402.12369)

    提出了一种通过卷积神经网络识别TESS全画幅图像光变曲线中的短周期变量的方法，实现了高效率的大规模档案搜索。

    

    arXiv:2402.12369v1 公告类型：交叉 抽象：过境系外行星勘测卫星（TESS）任务在其为期两年的主要任务期间测量了约85%的天空中的恒星光，导致数百万TESS 30分钟间隔光变曲线用于搜索过境系外行星。为了搜索这一巨大数据集，我们旨在提供一种既具有计算效率、生产性预测性，又最大程度减少所需人类搜索工作的方法。我们提出了一种卷积神经网络，用于识别短周期变量。为了对给定的光变曲线进行预测，我们的网络不需要使用其他方法确定的先前目标参数。我们的网络在单个GPU上对TESS 30分钟间隔光变曲线进行推断约5ms，从而实现了大规模的档案搜索。我们提出了14156个由我们的网络识别出的短周期变量的集合。我们识别的大多数变量属于两个突出的人群之一。

    arXiv:2402.12369v1 Announce Type: cross  Abstract: The Transiting Exoplanet Survey Satellite (TESS) mission measured light from stars in ~85% of the sky throughout its two-year primary mission, resulting in millions of TESS 30-minute cadence light curves to analyze in the search for transiting exoplanets. To search this vast dataset, we aim to provide an approach that is both computationally efficient, produces highly performant predictions, and minimizes the required human search effort. We present a convolutional neural network that we train to identify short period variables. To make a prediction for a given light curve, our network requires no prior target parameters identified using other methods. Our network performs inference on a TESS 30-minute cadence light curve in ~5ms on a single GPU, enabling large scale archival searches. We present a collection of 14156 short-period variables identified by our network. The majority of our identified variables fall into two prominent popu
    
[^18]: 二阶方法用于赌徒优化与控制

    Second Order Methods for Bandit Optimization and Control

    [https://arxiv.org/abs/2402.08929](https://arxiv.org/abs/2402.08929)

    本文提出了一种简单实用的二阶赌徒凸优化算法，并证明了其对于一类称之为$\kappa$-凸的凸函数实现了最优的后期损失界限。该算法在多个应用中表现出高效性能，包括赌徒逻辑回归。

    

    Bandit凸优化(BCO)是一种在不确定性下进行在线决策的通用框架。尽管已经建立了一般凸损失的紧束后期界限，但现有算法在高维数据上具有难以忍受的计算成本。在本文中，我们提出了一种受在线牛顿步骤算法启发的简单实用的BCO算法。我们证明了我们的算法对于一类我们称之为$\kappa$-凸的凸函数实现了最优(从层面上讲)的后期界限。这个类包含了一系列实际相关的损失函数，包括线性、二次和广义线性模型。除了最优的后期损失，这种方法也是一些经过深入研究的应用中已知的最高效的算法，包括赌徒逻辑回归。

    arXiv:2402.08929v1 Announce Type: new Abstract: Bandit convex optimization (BCO) is a general framework for online decision making under uncertainty. While tight regret bounds for general convex losses have been established, existing algorithms achieving these bounds have prohibitive computational costs for high dimensional data.   In this paper, we propose a simple and practical BCO algorithm inspired by the online Newton step algorithm. We show that our algorithm achieves optimal (in terms of horizon) regret bounds for a large class of convex functions that we call $\kappa$-convex. This class contains a wide range of practically relevant loss functions including linear, quadratic, and generalized linear models. In addition to optimal regret, this method is the most efficient known algorithm for several well-studied applications including bandit logistic regression.   Furthermore, we investigate the adaptation of our second-order bandit algorithm to online convex optimization with mem
    
[^19]: 图特征预处理器：实时从交易图中提取基于子图的特征

    Graph Feature Preprocessor: Real-time Extraction of Subgraph-based Features from Transaction Graphs

    [https://arxiv.org/abs/2402.08593](https://arxiv.org/abs/2402.08593)

    本文介绍了一种名为"图特征预处理器"的软件库，可以从实时交易图中检测典型的洗钱和欺诈模式，并生成丰富的交易特征，从而显著提高机器学习模型的预测准确率。

    

    在本文中，我们提出了一种名为"图特征预处理器"的软件库，用于实时检测金融交易图中的典型洗钱和欺诈模式。这些模式被用于生成丰富的交易特征，用于下游的机器学习训练和推断任务，如洗钱检测。我们展示了我们丰富的交易特征如何显著提高基于梯度提升的机器学习模型的预测准确率。我们的库利用多核并行性，维护一个动态的内存图，并高效地挖掘传入交易流中的子图模式，使其能够以流的方式操作。我们使用高度不平衡的合成反洗钱（AML）和真实的以太坊钓鱼数据集对我们的库进行评估。在这些数据集中，非法交易的比例非常小，使得学习过程具有挑战性。我们的解决方案结合了我们的图特征预处理器和...

    In this paper, we present "Graph Feature Preprocessor", a software library for detecting typical money laundering and fraud patterns in financial transaction graphs in real time. These patterns are used to produce a rich set of transaction features for downstream machine learning training and inference tasks such as money laundering detection. We show that our enriched transaction features dramatically improve the prediction accuracy of gradient-boosting-based machine learning models. Our library exploits multicore parallelism, maintains a dynamic in-memory graph, and efficiently mines subgraph patterns in the incoming transaction stream, which enables it to be operated in a streaming manner. We evaluate our library using highly-imbalanced synthetic anti-money laundering (AML) and real-life Ethereum phishing datasets. In these datasets, the proportion of illicit transactions is very small, which makes the learning process challenging. Our solution, which combines our Graph Feature Prep
    
[^20]: Unichain和非周期性足以保证平均回报好转胆冒险目标的渐进最优性

    Unichain and Aperiodicity are Sufficient for Asymptotic Optimality of Average-Reward Restless Bandits

    [https://arxiv.org/abs/2402.05689](https://arxiv.org/abs/2402.05689)

    该论文提出了一种新的策略类别，用于解决无限期平均回报好转胆冒险问题。研究表明，在单臂松弛问题是Unichain和非周期性的情况下，该策略类别具有渐进最优性。

    

    我们考虑了离散时间下的无限期平均回报的好转胆冒险问题。我们提出了一种新的策略类别，旨在将逐渐扩大的臂子集向最佳分布方向推进。我们证明了我们的策略在N臂问题中是渐进最优的，如果单臂松弛问题是Unichain和非周期性的，那么就会有一个$O(1/\sqrt{N})$的最优间隙。我们的方法不同于大多数现有的研究，这些研究侧重于指数或优先级策略，这些策略依赖于统一全局吸引子属性（UGAP）来保证收敛到最优，或者依赖于最近开发的基于模拟的策略，该策略要求遵循同步假设（SA）。

    We consider the infinite-horizon, average-reward restless bandit problem in discrete time. We propose a new class of policies that are designed to drive a progressively larger subset of arms toward the optimal distribution. We show that our policies are asymptotically optimal with an $O(1/\sqrt{N})$ optimality gap for an $N$-armed problem, provided that the single-armed relaxed problem is unichain and aperiodic. Our approach departs from most existing work that focuses on index or priority policies, which rely on the Uniform Global Attractor Property (UGAP) to guarantee convergence to the optimum, or a recently developed simulation-based policy, which requires a Synchronization Assumption (SA).
    
[^21]: CataractBot：一种基于LLM的白内障患者专家辅助聊天机器人

    CataractBot: An LLM-Powered Expert-in-the-Loop Chatbot for Cataract Patients

    [https://arxiv.org/abs/2402.04620](https://arxiv.org/abs/2402.04620)

    CataractBot是一种基于LLM的白内障患者专家辅助聊天机器人，通过查询知识库提供即时的答案和专家验证的回复。在实地部署研究中证明了其价值所在。

    

    随着医疗行业的发展，患者越来越追求更可靠的健康信息，包括他们的健康状况、治疗选择和潜在风险。虽然有很多信息来源，但数字时代却给人们带来了过多且错误的信息。患者主要信任医生和医院工作人员，突显了专家认可的健康信息的必要性。但是，专家面临的压力导致了沟通时间的减少，影响了信息的共享。为了填补这一空白，我们提出了CataractBot，一种由大型语言模型（LLMs）驱动的专家辅助聊天机器人。与印度一家三级眼科医院合作开发的CataractBot通过查询策划的知识库，即时回答白内障手术相关的问题，并异步提供专家验证的答复。CataractBot具备多模式支持和多语言能力。在与49名参与者的实地部署研究中，CataractBot证明了其价值所在。

    The healthcare landscape is evolving, with patients seeking more reliable information about their health conditions, treatment options, and potential risks. Despite the abundance of information sources, the digital age overwhelms individuals with excess, often inaccurate information. Patients primarily trust doctors and hospital staff, highlighting the need for expert-endorsed health information. However, the pressure on experts has led to reduced communication time, impacting information sharing. To address this gap, we propose CataractBot, an experts-in-the-loop chatbot powered by large language models (LLMs). Developed in collaboration with a tertiary eye hospital in India, CataractBot answers cataract surgery related questions instantly by querying a curated knowledge base, and provides expert-verified responses asynchronously. CataractBot features multimodal support and multilingual capabilities. In an in-the-wild deployment study with 49 participants, CataractBot proved valuable,
    
[^22]: 通过基于排列的权重匹配分析线性模式连接性

    Analysis of Linear Mode Connectivity via Permutation-Based Weight Matching

    [https://arxiv.org/abs/2402.04051](https://arxiv.org/abs/2402.04051)

    通过基于排列的权重匹配分析线性模式连接性，我们实验证明了通过权重匹配找到的排列可以改变权重矩阵奇异向量的方向，但不能改变奇异值。这一发现对于理解随机梯度下降的有效性及其在模型合并等领域的应用具有重要意义。

    

    最近，Ainsworth等人展示了使用权重匹配（WM）来最小化排列搜索模型参数中的$L_2$距离有效地识别满足线性模式连接性（LMC）的排列的方法，其中，在两个具有不同种子的独立训练模型之间的线性路径上的损失保持几乎恒定。本文通过WM提供了LMC的理论分析，这对于理解随机梯度下降的有效性及其在模型合并等领域的应用至关重要。我们首先通过实验和理论分析表明，WM找到的排列并不显着减少两个模型之间的$L_2$距离，而LMC的出现并不仅仅是由于WM本身的距离减小。然后，我们提供了理论洞见，表明排列可以改变每层权重矩阵的奇异向量的方向，但不能改变奇异值。这一发现表明，WM找到的排列主要改变了权重矩阵的方向，而不是奇异值。

    Recently, Ainsworth et al. showed that using weight matching (WM) to minimize the $L_2$ distance in a permutation search of model parameters effectively identifies permutations that satisfy linear mode connectivity (LMC), in which the loss along a linear path between two independently trained models with different seeds remains nearly constant. This paper provides a theoretical analysis of LMC using WM, which is crucial for understanding stochastic gradient descent's effectiveness and its application in areas like model merging. We first experimentally and theoretically show that permutations found by WM do not significantly reduce the $L_2$ distance between two models and the occurrence of LMC is not merely due to distance reduction by WM in itself. We then provide theoretical insights showing that permutations can change the directions of the singular vectors, but not the singular values, of the weight matrices in each layer. This finding shows that permutations found by WM mainly al
    
[^23]: 本能偏见：虚假图像导致MLLMs产生幻觉

    The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs

    [https://arxiv.org/abs/2402.03757](https://arxiv.org/abs/2402.03757)

    本论文研究发现，虚假图像会导致多模态大型语言模型产生幻觉，作者提出了评估幻觉程度的基准CorrelationQA，并发现主流多模态大型语言模型普遍受到这种本能偏见的影响。

    

    大型语言模型（LLMs）近年来取得了显著进展，多模态大型语言模型（MLLMs）的出现使LLMs具备了视觉能力，在各种多模态任务中表现出色。然而，像GPT-4V这样强大的MLLMs在面对某些图像和文本输入时仍然以惊人的方式失败了。本文中，我们确定了一类典型输入，这些输入令MLLMs困惑，它们由高度相关但与答案不一致的图像组成，导致MLLMs产生幻觉。为了量化这种影响，我们提出了CorrelationQA，这是首个评估给定虚假图像的幻觉程度的基准。该基准包含13个类别的7,308个文本-图像对。基于提出的CorrelationQA，我们对9个主流MLLMs进行了深入分析，表明它们普遍受到这种本能偏见的不同程度的影响。我们希望我们精选的基准和评估结果能有所帮助。

    Large language models (LLMs) have recently experienced remarkable progress, where the advent of multi-modal large language models (MLLMs) has endowed LLMs with visual capabilities, leading to impressive performances in various multi-modal tasks. However, those powerful MLLMs such as GPT-4V still fail spectacularly when presented with certain image and text inputs. In this paper, we identify a typical class of inputs that baffles MLLMs, which consist of images that are highly relevant but inconsistent with answers, causing MLLMs to suffer from hallucination. To quantify the effect, we propose CorrelationQA, the first benchmark that assesses the hallucination level given spurious images. This benchmark contains 7,308 text-image pairs across 13 categories. Based on the proposed CorrelationQA, we conduct a thorough analysis on 9 mainstream MLLMs, illustrating that they universally suffer from this instinctive bias to varying degrees. We hope that our curated benchmark and evaluation result
    
[^24]: 概率演员-评论家：学习以PAC-Bayes不确定性进行探索

    Probabilistic Actor-Critic: Learning to Explore with PAC-Bayes Uncertainty

    [https://arxiv.org/abs/2402.03055](https://arxiv.org/abs/2402.03055)

    概率演员-评论家算法（PAC）通过在评论家中建模和推断不确定性，以改进强化学习中的连续控制性能，并实现自适应的探索策略。

    

    我们引入了概率演员-评论家（PAC），这是一种新颖的强化学习算法，通过缓解探索与利用的平衡问题，改进了连续控制性能。PAC通过将随机策略和评论家无缝融合，创建了评论家不确定性估计和演员训练之间的动态协同作用。我们的PAC算法的关键贡献在于通过Probably Approximately Correct-Bayesian（PAC-Bayes）分析，明确建模和推断评论家的认知不确定性。这种对评论家不确定性的融入使PAC能够在学习过程中自适应调整其探索策略，指导演员的决策过程。与现有技术中的固定或预定的探索方案相比，PAC表现出更好的效果。通过PAC-Bayes分析引导的随机策略和评论家之间的协同作用，是向深度强化学习中更具自适应性和有效性的探索策略迈出的关键一步。

    We introduce Probabilistic Actor-Critic (PAC), a novel reinforcement learning algorithm with improved continuous control performance thanks to its ability to mitigate the exploration-exploitation trade-off. PAC achieves this by seamlessly integrating stochastic policies and critics, creating a dynamic synergy between the estimation of critic uncertainty and actor training. The key contribution of our PAC algorithm is that it explicitly models and infers epistemic uncertainty in the critic through Probably Approximately Correct-Bayesian (PAC-Bayes) analysis. This incorporation of critic uncertainty enables PAC to adapt its exploration strategy as it learns, guiding the actor's decision-making process. PAC compares favorably against fixed or pre-scheduled exploration schemes of the prior art. The synergy between stochastic policies and critics, guided by PAC-Bayes analysis, represents a fundamental step towards a more adaptive and effective exploration strategy in deep reinforcement lear
    
[^25]: 风险敏感扩散：从带噪声样本学习潜在分布

    Risk-Sensitive Diffusion: Learning the Underlying Distribution from Noisy Samples

    [https://arxiv.org/abs/2402.02081](https://arxiv.org/abs/2402.02081)

    该论文提出了一种风险敏感的扩散模型，通过调整噪声样本的分布，减少误导并从中获取信息，有效地应对噪声样本的存在，并在合成和真实世界数据集上取得了良好的效果。

    

    我们发现，扩散模型在存在噪声样本时很脆弱，限制了它们在大量环境中的潜力，这些环境不像图像合成那样干净。受到我们对噪声样本与干净样本在扩散过程中分布差异的发现的启发，我们引入了风险敏感的SDE，这是一个由风险（即数据“脏乱度”）参数化的随机微分方程，用于调整噪声样本的分布，减少误导并从中获取信息。风险敏感的SDE的最优表达式取决于特定的噪声分布，我们导出了最小化高斯和一般非高斯扰动误导的参数化。我们在合成和真实世界数据集（如医学时间序列）上进行了大量实验，证明了我们的模型有效地。

    While achieving remarkable performances, we show that diffusion models are fragile to the presence of noisy samples, limiting their potential in the vast amount of settings where, unlike image synthesis, we are not blessed with clean data. Motivated by our finding that such fragility originates from the distribution gaps between noisy and clean samples along the diffusion process, we introduce risk-sensitive SDE, a stochastic differential equation that is parameterized by the risk (i.e., data "dirtiness") to adjust the distributions of noisy samples, reducing misguidance while benefiting from their contained information. The optimal expression for risk-sensitive SDE depends on the specific noise distribution, and we derive its parameterizations that minimize the misguidance of noisy samples for both Gaussian and general non-Gaussian perturbations. We conduct extensive experiments on both synthetic and real-world datasets (e.g., medical time series), showing that our model effectively r
    
[^26]: 通过轨迹拼接将条件性扩散模型提炼为离线强化学习的方法

    Distilling Conditional Diffusion Models for Offline Reinforcement Learning through Trajectory Stitching

    [https://arxiv.org/abs/2402.00807](https://arxiv.org/abs/2402.00807)

    通过新的轨迹拼接算法和奖励生成器，使用条件性扩散模型生成的高回报轨迹与原始轨迹混合，应用于行为克隆的结果表明，学得的规模较小的浅层策略在多个D4RL基准测试中超过或接近深度生成规划器。

    

    深度生成模型最近已经成为离线强化学习的一种有效方法。然而，它们的大模型规模在计算上提出了挑战。为了解决这个问题，我们提出了一种基于数据增强的知识蒸馏方法。具体而言，我们从条件性扩散模型中生成高回报轨迹，并通过一种新的拼接算法将它们与原始轨迹混合，该算法利用了一种新的奖励生成器。将所得到的数据集应用于行为克隆，学得的规模较小的浅层策略在几个D4RL基准测试中胜过或接近深度生成规划器。

    Deep generative models have recently emerged as an effective approach to offline reinforcement learning. However, their large model size poses challenges in computation. We address this issue by proposing a knowledge distillation method based on data augmentation. In particular, high-return trajectories are generated from a conditional diffusion model, and they are blended with the original trajectories through a novel stitching algorithm that leverages a new reward generator. Applying the resulting dataset to behavioral cloning, the learned shallow policy whose size is much smaller outperforms or nearly matches deep generative planners on several D4RL benchmarks.
    
[^27]: 用于处理带有负值的嘈杂数据的非负矩阵分解算法

    Algorithms for Non-Negative Matrix Factorization on Noisy Data With Negative Values

    [https://arxiv.org/abs/2311.04855](https://arxiv.org/abs/2311.04855)

    本文提出了两种算法，Shift-NMF和Nearly-NMF，可以处理带有负值的嘈杂数据，在不引入正的偏移量的情况下正确恢复非负信号。

    

    非负矩阵分解（NMF）是一种降维技术，在分析嘈杂数据，特别是天文数据方面表现出了潜力。在这些数据集中，由于噪声，观测数据可能包含负值，即使真实的物理信号严格为正。以往的NMF工作未以统计一致的方式处理负数据，这在低信噪比数据中出现许多负值时会变得棘手。在本文中，我们提出了两种算法，Shift-NMF和Nearly-NMF，可以处理输入数据的嘈杂性，并消除任何引入的负值。这两种算法都使用负数据空间而不进行截取，并且在消除负数据时不会引入正的偏移量。我们在简单和更现实的示例上进行了数值演示，并证明了这两种算法具有单调性。

    arXiv:2311.04855v2 Announce Type: replace-cross  Abstract: Non-negative matrix factorization (NMF) is a dimensionality reduction technique that has shown promise for analyzing noisy data, especially astronomical data. For these datasets, the observed data may contain negative values due to noise even when the true underlying physical signal is strictly positive. Prior NMF work has not treated negative data in a statistically consistent manner, which becomes problematic for low signal-to-noise data with many negative values. In this paper we present two algorithms, Shift-NMF and Nearly-NMF, that can handle both the noisiness of the input data and also any introduced negativity. Both of these algorithms use the negative data space without clipping, and correctly recover non-negative signals without any introduced positive offset that occurs when clipping negative data. We demonstrate this numerically on both simple and more realistic examples, and prove that both algorithms have monotoni
    
[^28]: LongForm: 使用反向指令进行有效的指令调优

    LongForm: Effective Instruction Tuning with Reverse Instructions

    [https://arxiv.org/abs/2304.08460](https://arxiv.org/abs/2304.08460)

    使用反向指令进行有效的指令调优，通过生成一组自然的、适用于长文本生成的指令调优数据集，我们的模型在故事/菜谱生成和长篇问答等任务上优于10倍规模更大的语言模型，而无需指令调优。

    

    Instruction tuning使得语言模型能够更有效地泛化，并更好地遵循用户意图。然而，获取指令数据成本高且具有挑战性。先前的工作采用了诸如昂贵的人工注释、存在对齐问题的众包数据集、以及通过LLMs生成噪声示例的方法。我们引入了LongForm-C数据集，该数据集由反向指令创建。我们使用LLMs为人类写作语料库示例使用反向指令生成指令。首先，我们从诸如C4和Wikipedia的语料库中选择了一组多样性的人类撰写文档；然后，我们使用LLMs为这些文档生成指令。这种方法提供了一个更便宜、更干净、输出自然以及适用于长文本生成的指令调优数据集。我们的模型在故事/菜谱生成和长篇问答等任务上优于10倍规模更大的语言模型，而无需指令调优。

    arXiv:2304.08460v2 Announce Type: replace-cross Abstract: Instruction tuning enables language models to more effectively generalize and better follow user intent. However, obtaining instruction data is costly and challenging. Prior work employs methods such as expensive human annotation, crowd-sourced datasets with alignment issues, and generating noisy examples via LLMs. We introduce the LongForm-C dataset, which is created by reverse instructions. We generate instructions via LLMs for human-written corpus examples using reverse instructions. First we select a diverse set of human-written documents from corpora such as C4 and Wikipedia; then we generate instructions for these documents via LLMs. This approach provides a cheaper and cleaner instruction-tuning dataset with natural output and one suitable for long text generation. Our models outperform 10x larger language models without instruction tuning on tasks such as story/recipe generation and long-form question answering. Moreover
    
[^29]: 《SGD和自适应学习规则学到的表示：变化稀疏性和选择性的神经网络条件》

    Representations learnt by SGD and Adaptive learning rules: Conditions that vary sparsity and selectivity in neural network

    [https://arxiv.org/abs/2201.11653](https://arxiv.org/abs/2201.11653)

    本文调查了导致神经网络稀疏性和选择性增加的各种条件。

    

    从人脑的角度来看，连续学习可以执行各种任务而互不干扰。减少互相干扰的有效方式可以在神经元的稀疏性和选择性中找到。根据Aljundi等人和Hadsell等人的观点，在表示水平施加稀疏性对连续学习是有利的，因为稀疏的神经元激活鼓励参数之间的少重叠，导致更少的干扰。同样，高度选择性的神经网络可能会引起较少的干扰，因为神经元中的特定响应将减少与其他参数的重叠机会。考虑到人脑在一生中执行连续学习，找到自然增加稀疏性和选择性的条件可能为了解大脑功能提供见解。本文调查了自然增加神经网络稀疏性和选择性的各种条件。

    arXiv:2201.11653v2 Announce Type: replace  Abstract: From the point of view of the human brain, continual learning can perform various tasks without mutual interference. An effective way to reduce mutual interference can be found in sparsity and selectivity of neurons. According to Aljundi et al. and Hadsell et al., imposing sparsity at the representational level is advantageous for continual learning because sparse neuronal activations encourage less overlap between parameters, resulting in less interference. Similarly, highly selective neural networks are likely to induce less interference since particular response in neurons will reduce the chance of overlap with other parameters. Considering that the human brain performs continual learning over the lifespan, finding conditions where sparsity and selectivity naturally arises may provide insight for understanding how the brain functions. This paper investigates various conditions that naturally increase sparsity and selectivity in a 
    
[^30]: 借助多条件扩散引导的逆分子设计

    Inverse Molecular Design with Multi-Conditional Diffusion Guidance. (arXiv:2401.13858v1 [cs.LG])

    [http://arxiv.org/abs/2401.13858](http://arxiv.org/abs/2401.13858)

    借助多条件扩散引导的逆分子设计模型在材料和药物发现方面具有巨大潜力。通过引入Transformer-based去噪模型和图依赖的扩散过程，该模型能够在多个条件约束下准确地生成聚合物和小分子。

    

    借助扩散模型进行逆分子设计在材料和药物发现方面具有巨大潜力。虽然在无条件分子生成方面取得了成功，但将合成评分和气体渗透性等多个属性作为条件约束集成到扩散模型中仍未被探索。我们引入了多条件扩散引导。所提出的基于Transformer的去噪模型具有一个条件编码器，该编码器学习了数值和分类条件的表示。组成结构编码器-解码器的去噪模型在条件表示下进行去噪训练。扩散过程变得依赖于图来准确估计分子中与图相关的噪声，而不像以前的模型仅关注原子或键的边缘分布。我们广泛验证了我们的模型在多条件聚合物和小分子生成方面的优越性。结果显示我们在分布度量方面的优势。

    Inverse molecular design with diffusion models holds great potential for advancements in material and drug discovery. Despite success in unconditional molecule generation, integrating multiple properties such as synthetic score and gas permeability as condition constraints into diffusion models remains unexplored. We introduce multi-conditional diffusion guidance. The proposed Transformer-based denoising model has a condition encoder that learns the representations of numerical and categorical conditions. The denoising model, consisting of a structure encoder-decoder, is trained for denoising under the representation of conditions. The diffusion process becomes graph-dependent to accurately estimate graph-related noise in molecules, unlike the previous models that focus solely on the marginal distributions of atoms or bonds. We extensively validate our model for multi-conditional polymer and small molecule generation. Results demonstrate our superiority across metrics from distribution
    
[^31]: 概率Lambert问题的解决方案：与最优质量传输、Schr\"odinger桥和反应-扩散偏微分方程的连接

    Solution of the Probabilistic Lambert Problem: Connections with Optimal Mass Transport, Schr\"odinger Bridge and Reaction-Diffusion PDEs. (arXiv:2401.07961v1 [math.OC])

    [http://arxiv.org/abs/2401.07961](http://arxiv.org/abs/2401.07961)

    这项研究将概率Lambert问题与最优质量传输、Schr\"odinger桥和反应-扩散偏微分方程等领域连接起来，从而解决了概率Lambert问题的解的存在和唯一性，并提供了数值求解的方法。

    

    Lambert问题涉及通过速度控制在规定的飞行时间内将航天器从给定的初始位置转移到给定的终端位置，受到重力力场的限制。我们考虑了Lambert问题的概率变种，其中位置向量的端点约束的知识被它们各自的联合概率密度函数所替代。我们证明了具有端点联合概率密度约束的Lambert问题是一个广义的最优质量传输（OMT）问题，从而将这个经典的天体动力学问题与现代随机控制和随机机器学习的新兴研究领域联系起来。这个新发现的连接使我们能够严格建立概率Lambert问题的解的存在性和唯一性。同样的连接还帮助通过扩散正规化数值求解概率Lambert问题，即通过进一步的连接来利用。

    Lambert's problem concerns with transferring a spacecraft from a given initial to a given terminal position within prescribed flight time via velocity control subject to a gravitational force field. We consider a probabilistic variant of the Lambert problem where the knowledge of the endpoint constraints in position vectors are replaced by the knowledge of their respective joint probability density functions. We show that the Lambert problem with endpoint joint probability density constraints is a generalized optimal mass transport (OMT) problem, thereby connecting this classical astrodynamics problem with a burgeoning area of research in modern stochastic control and stochastic machine learning. This newfound connection allows us to rigorously establish the existence and uniqueness of solution for the probabilistic Lambert problem. The same connection also helps to numerically solve the probabilistic Lambert problem via diffusion regularization, i.e., by leveraging further connection 
    
[^32]: 使用具有规范等变性的卷积神经网络机器学习SU（3）规范理论的固定点作用

    Machine learning a fixed point action for SU(3) gauge theory with a gauge equivariant convolutional neural network. (arXiv:2401.06481v1 [hep-lat] CROSS LISTED)

    [http://arxiv.org/abs/2401.06481](http://arxiv.org/abs/2401.06481)

    本研究使用具有精确规范不变性的卷积神经网络，通过机器学习方法找到了四维SU（3）规范理论的优秀固定点作用的参数化方法，为未来的蒙特卡洛模拟打下了必要的基础。

    

    固定点的格子作用被设计成具有不受离散化效应影响的连续经典性质，并在量子层面上减少格子效应。它们提供了一种用较粗的格子来提取连续物理的可能方法，从而绕过与连续极限相关的临界减慢和拓扑冻结问题。实际应用的关键是找到一个精确且紧凑的固定点作用参数化方法，因为其许多性质只是隐含定义的。在这里，我们使用机器学习方法重新思考了如何参数化固定点作用的问题。特别地，我们使用具有精确规范不变性的卷积神经网络获得四维SU（3）规范理论的固定点作用。大的算子空间使我们能够找到比之前研究更好的参数化方法，这是未来蒙特卡洛模拟的必要第一步。

    Fixed point lattice actions are designed to have continuum classical properties unaffected by discretization effects and reduced lattice artifacts at the quantum level. They provide a possible way to extract continuum physics with coarser lattices, thereby allowing to circumvent problems with critical slowing down and topological freezing toward the continuum limit. A crucial ingredient for practical applications is to find an accurate and compact parametrization of a fixed point action, since many of its properties are only implicitly defined. Here we use machine learning methods to revisit the question of how to parametrize fixed point actions. In particular, we obtain a fixed point action for four-dimensional SU(3) gauge theory using convolutional neural networks with exact gauge invariance. The large operator space allows us to find superior parametrizations compared to previous studies, a necessary first step for future Monte Carlo simulations.
    
[^33]: 基于零样本学习的模块化AI代理的机器教学

    Machine Teaching for Building Modular AI Agents based on Zero-shot Learners. (arXiv:2401.05467v1 [cs.LG])

    [http://arxiv.org/abs/2401.05467](http://arxiv.org/abs/2401.05467)

    这篇论文提出了一种机器教学方法，通过利用迭代机器教学和任务特定的替代模型，增强了利用大语言模型作为零样本学习器的模块化AI代理的鲁棒性和性能。

    

    最近大语言模型（LLMs）的进展导致了许多模块化AI代理的创建。这些代理使用LLMs作为零样本学习器，在人类用户设定的复杂任务中执行子任务。我们提出了一种方法来增强利用LLMs作为零样本学习器的模块化AI代理的鲁棒性和性能。我们的迭代机器教学方法提供了一种在有限的人类反馈下逐渐教导AI代理的高效方式，解决了零样本学习质量限制的问题。我们主张利用初始部署的数据追踪以及零样本学习器的输出或注释来训练更小且任务特定的替代模型，可以减少经济成本和环境影响。我们的机器教学过程利用人类专业知识来纠正高概率误标注的示例。在三个常见对话AI代理任务上的结果显示，接近理想性能可以实现。

    The recent advances in large language models (LLMs) have led to the creation of many modular AI agents. These agents employ LLMs as zero-shot learners to perform sub-tasks in order to solve complex tasks set forth by human users. We propose an approach to enhance the robustness and performance of modular AI agents that utilize LLMs as zero-shot learners. Our iterative machine teaching method offers an efficient way to teach AI agents over time with limited human feedback, addressing the limit posed by the quality of zero-shot learning. We advocate leveraging the data traces from initial deployments and outputs or annotations from the zero-shot learners to train smaller and task-specific substitute models which can reduce both the monetary costs and environmental impact. Our machine teaching process avails human expertise to correct examples with a high likelihood of misannotations. Results on three tasks, common to conversational AI agents, show that close-to-oracle performance can be 
    
[^34]: 通过切换机制，在扩散模型中实现公平抽样

    Fair Sampling in Diffusion Models through Switching Mechanism. (arXiv:2401.03140v1 [cs.LG])

    [http://arxiv.org/abs/2401.03140](http://arxiv.org/abs/2401.03140)

    本论文提出了一种称为“属性切换”的公平抽样机制，用于解决扩散模型中公平性的问题。通过在生成的数据中混淆敏感属性，该方法能够实现生成公平数据和保持数据效用的目标。

    

    扩散模型通过良好逼近潜在概率分布，在生成任务中展现了高效性。然而，扩散模型在公平性方面受到训练数据的内在偏差的放大。尽管扩散模型的抽样过程可以通过条件引导来控制，但之前的研究试图找到实证引导来实现定量公平性。为了解决这个限制，我们提出了一种称为“属性切换”机制的具有公平意识的抽样方法，用于扩散模型。在不需要额外训练的情况下，所提出的抽样方法可以在生成的数据中混淆敏感属性，而不依赖分类器。我们在两个关键方面从数学上证明了和实验证明了所提方法的有效性：(i)生成公平数据和(ii)保持生成数据的效用。

    Diffusion models have shown their effectiveness in generation tasks by well-approximating the underlying probability distribution. However, diffusion models are known to suffer from an amplified inherent bias from the training data in terms of fairness. While the sampling process of diffusion models can be controlled by conditional guidance, previous works have attempted to find empirical guidance to achieve quantitative fairness. To address this limitation, we propose a fairness-aware sampling method called \textit{attribute switching} mechanism for diffusion models. Without additional training, the proposed sampling can obfuscate sensitive attributes in generated data without relying on classifiers. We mathematically prove and experimentally demonstrate the effectiveness of the proposed method on two key aspects: (i) the generation of fair data and (ii) the preservation of the utility of the generated data.
    
[^35]: Sum-of-Parts模型：对特征组的忠实归因

    Sum-of-Parts Models: Faithful Attributions for Groups of Features. (arXiv:2310.16316v1 [cs.LG])

    [http://arxiv.org/abs/2310.16316](http://arxiv.org/abs/2310.16316)

    Sum-of-Parts模型通过构造保证特征组归因的忠实性，将预测分解为可解释的分数之和，帮助天体物理学家发现了关于星系形成的新知识。

    

    如果机器学习模型的解释准确反映了其决策过程，则被认为是“忠实”的解释。然而，例如深度学习的特征归因等解释并不能保证忠实，有可能产生具有误导性的解释。在这项工作中，我们开发了Sum-of-Parts（SOP）模型，它是一类模型，其预测具有通过构造保证忠实的特征组归因。该模型将预测分解为可解释的分数之和，每个分数直接归因于一组稀疏特征。我们使用标准可解释性指标对SOP进行评估，并在一个案例研究中，利用SOP提供的忠实解释帮助天体物理学家发现了关于星系形成的新知识。

    An explanation of a machine learning model is considered "faithful" if it accurately reflects the model's decision-making process. However, explanations such as feature attributions for deep learning are not guaranteed to be faithful, and can produce potentially misleading interpretations. In this work, we develop Sum-of-Parts (SOP), a class of models whose predictions come with grouped feature attributions that are faithful-by-construction. This model decomposes a prediction into an interpretable sum of scores, each of which is directly attributable to a sparse group of features. We evaluate SOP on benchmarks with standard interpretability metrics, and in a case study, we use the faithful explanations from SOP to help astrophysicists discover new knowledge about galaxy formation.
    
[^36]: 通过四元数小波网络推广医学图像表示

    Generalizing Medical Image Representations via Quaternion Wavelet Networks. (arXiv:2310.10224v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2310.10224](http://arxiv.org/abs/2310.10224)

    本文提出了一种名为QUAVE的四元数小波网络，可以从医学图像中提取显著特征。该网络可以与现有的医学图像分析或综合任务结合使用，并推广了对单通道数据的采用。通过四元数小波变换和加权处理，QUAVE能够处理具有较大变化的医学数据。

    

    鉴于来自不同来源和各种任务的数据集日益增加，神经网络的普适性成为一个广泛研究的领域。当处理医学数据时，这个问题尤为广泛，因为缺乏方法论标准导致不同的成像中心或使用不同设备和辅助因素获取的数据存在较大变化。为了克服这些限制，我们引入了一种新颖的、普适的、数据-和任务不可知的框架，能够从医学图像中提取显著特征。所提出的四元数小波网络（QUAVE）可以很容易地与任何现有的医学图像分析或综合任务相结合，并且可以结合实际、四元数或超复值模型，推广它们对单通道数据的采用。QUAVE首先通过四元数小波变换提取不同的子带，得到低频/近似频带和高频/细粒度特征。然后，它对最有代表性的特征进行加权处理，从而减少了特征重要性不均匀性。

    Neural network generalizability is becoming a broad research field due to the increasing availability of datasets from different sources and for various tasks. This issue is even wider when processing medical data, where a lack of methodological standards causes large variations being provided by different imaging centers or acquired with various devices and cofactors. To overcome these limitations, we introduce a novel, generalizable, data- and task-agnostic framework able to extract salient features from medical images. The proposed quaternion wavelet network (QUAVE) can be easily integrated with any pre-existing medical image analysis or synthesis task, and it can be involved with real, quaternion, or hypercomplex-valued models, generalizing their adoption to single-channel data. QUAVE first extracts different sub-bands through the quaternion wavelet transform, resulting in both low-frequency/approximation bands and high-frequency/fine-grained features. Then, it weighs the most repr
    
[^37]: 强化学习基础：朝向具有基础先验辅助的具身通用智能体

    Foundation Reinforcement Learning: towards Embodied Generalist Agents with Foundation Prior Assistance. (arXiv:2310.02635v1 [cs.RO])

    [http://arxiv.org/abs/2310.02635](http://arxiv.org/abs/2310.02635)

    本研究提出了一种基于具身基础先验的基础强化学习框架，通过加速训练过程来提高样本效率。

    

    最近人们已经表明，从互联网规模的数据中进行大规模预训练是构建通用模型的关键，正如在NLP中所见。为了构建具身通用智能体，我们和许多其他研究者假设这种基础先验也是不可或缺的组成部分。然而，目前尚不清楚如何以适当的具体形式表示这些具身基础先验，以及它们应该如何在下游任务中使用。在本文中，我们提出了一组直观有效的具身先验，包括基础策略、价值和成功奖励。所提出的先验是基于目标条件的MDP。为了验证其有效性，我们实例化了一个由这些先验辅助的演员-评论家方法，称之为基础演员-评论家（FAC）。我们将我们的框架命名为基础强化学习（FRL），因为它完全依赖于具身基础先验来进行探索、学习和强化。FRL的好处有三个。(1)样本效率高。通过基础先验加速训练过程，减少样本使用量。

    Recently, people have shown that large-scale pre-training from internet-scale data is the key to building generalist models, as witnessed in NLP. To build embodied generalist agents, we and many other researchers hypothesize that such foundation prior is also an indispensable component. However, it is unclear what is the proper concrete form to represent those embodied foundation priors and how they should be used in the downstream task. In this paper, we propose an intuitive and effective set of embodied priors that consist of foundation policy, value, and success reward. The proposed priors are based on the goal-conditioned MDP. To verify their effectiveness, we instantiate an actor-critic method assisted by the priors, called Foundation Actor-Critic (FAC). We name our framework as Foundation Reinforcement Learning (FRL), since it completely relies on embodied foundation priors to explore, learn and reinforce. The benefits of FRL are threefold. (1) Sample efficient. With foundation p
    
[^38]: AtomSurf：蛋白质结构上的学习的表面表示

    AtomSurf : Surface Representation for Learning on Protein Structures. (arXiv:2309.16519v1 [cs.LG])

    [http://arxiv.org/abs/2309.16519](http://arxiv.org/abs/2309.16519)

    本文研究了将蛋白质作为3D网格的表面表示，并提出了一种结合图表面的协同方法，既有竞争优势，又有实际应用潜力。

    

    近期Cryo-EM和蛋白质结构预测算法的进展使得大规模蛋白质结构可获得，为基于机器学习的功能注释铺平了道路。几何深度学习领域关注创建适用于几何数据的方法。从蛋白质结构中学习的一个重要方面是将这些结构表示为几何对象（如网格、图或表面）并应用适合这种表示形式的学习方法。给定方法的性能将取决于表示和相应的学习方法。在本文中，我们研究将蛋白质表示为$\textit{3D mesh surfaces}$并将其纳入已建立的表示基准中。我们的第一个发现是，尽管有着有希望的初步结果，但仅单独表面表示似乎无法与3D网格竞争。在此基础上，我们提出了一种协同方法，将表面表示与图表面结合起来。

    Recent advancements in Cryo-EM and protein structure prediction algorithms have made large-scale protein structures accessible, paving the way for machine learning-based functional annotations.The field of geometric deep learning focuses on creating methods working on geometric data. An essential aspect of learning from protein structures is representing these structures as a geometric object (be it a grid, graph, or surface) and applying a learning method tailored to this representation. The performance of a given approach will then depend on both the representation and its corresponding learning method.  In this paper, we investigate representing proteins as $\textit{3D mesh surfaces}$ and incorporate them into an established representation benchmark. Our first finding is that despite promising preliminary results, the surface representation alone does not seem competitive with 3D grids. Building on this, we introduce a synergistic approach, combining surface representations with gra
    
[^39]: 用于核回归的遗憾最优联邦迁移学习及其在美式期权定价中的应用

    Regret-Optimal Federated Transfer Learning for Kernel Regression with Applications in American Option Pricing. (arXiv:2309.04557v1 [cs.LG])

    [http://arxiv.org/abs/2309.04557](http://arxiv.org/abs/2309.04557)

    本论文提出了一种遗憾最优算法的迭代方案，用于联邦迁移学习，在核回归模型中具体化，并提出了一个几乎遗憾最优的启发式算法，可以减小生成参数与专门参数之间的累积偏差。

    

    我们提出了一种最优的迭代方案，用于联邦迁移学习，其中中心计划者可以访问同一学习模型 $f_{\theta}$ 的数据集 ${\cal D}_1,\dots,{\cal D}_N$。我们的目标是在尊重模型 $f_{\theta(T)}$ 的损失函数的情况下，尽量减小生成参数 $\{\theta_i(t)\}_{t=0}^T$ 在所有 $T$ 次迭代中与每个数据集得到的专门参数$\theta^\star_{1},\ldots,\theta^\star_N$ 的累积偏差。我们仅允许每个专门模型（节点/代理）和中心计划者（服务器）在每次迭代（轮）之间进行持续通信。对于模型 $f_{\theta}$ 是有限秩核回归的情况，我们得出了遗憾最优算法的显式更新公式。通过利用遗憾最优算法中的对称性，我们进一步开发了一种几乎遗憾最优的启发式算法，其运行需要较少的 $\mathcal{O}(Np^2)$ 个基本运算。

    We propose an optimal iterative scheme for federated transfer learning, where a central planner has access to datasets ${\cal D}_1,\dots,{\cal D}_N$ for the same learning model $f_{\theta}$. Our objective is to minimize the cumulative deviation of the generated parameters $\{\theta_i(t)\}_{t=0}^T$ across all $T$ iterations from the specialized parameters $\theta^\star_{1},\ldots,\theta^\star_N$ obtained for each dataset, while respecting the loss function for the model $f_{\theta(T)}$ produced by the algorithm upon halting. We only allow for continual communication between each of the specialized models (nodes/agents) and the central planner (server), at each iteration (round). For the case where the model $f_{\theta}$ is a finite-rank kernel regression, we derive explicit updates for the regret-optimal algorithm. By leveraging symmetries within the regret-optimal algorithm, we further develop a nearly regret-optimal heuristic that runs with $\mathcal{O}(Np^2)$ fewer elementary operati
    
[^40]: 通过弱SINDy算法进行流式科学数据压缩

    Streaming Compression of Scientific Data via weak-SINDy. (arXiv:2308.14962v1 [cs.LG])

    [http://arxiv.org/abs/2308.14962](http://arxiv.org/abs/2308.14962)

    本文开发了一种专门用于压缩流式科学数据的流式弱SINDy算法。与经典的离线压缩算法不同，流式压缩算法可以在数据生成过程中进行压缩，使其适用于科学数据压缩。流式弱SINDy算法利用了底层数据的特征来进行压缩。

    

    本文开发了一种针对流式科学数据压缩的流式弱SINDy算法。科学数据的产生无论是通过模拟还是实验，都正在经历一段指数增长的阶段，这使得数据压缩对于存储和利用大规模科学数据集来说变得重要且常常是必要的。与经典的“离线”压缩算法相反，这些算法可以在可用数据集上执行压缩，流式压缩算法在模拟或实验生成的数据仍在系统中流动时进行数据压缩。这个特性使得流式压缩算法非常适用于科学数据压缩，因为在离线存储整个数据集往往是不可行的。本文提出了一种新的流式压缩算法，流式弱SINDy，它在压缩过程中利用了底层数据的特征。

    In this paper a streaming weak-SINDy algorithm is developed specifically for compressing streaming scientific data. The production of scientific data, either via simulation or experiments, is undergoing an stage of exponential growth, which makes data compression important and often necessary for storing and utilizing large scientific data sets. As opposed to classical ``offline" compression algorithms that perform compression on a readily available data set, streaming compression algorithms compress data ``online" while the data generated from simulation or experiments is still flowing through the system. This feature makes streaming compression algorithms well-suited for scientific data compression, where storing the full data set offline is often infeasible. This work proposes a new streaming compression algorithm, streaming weak-SINDy, which takes advantage of the underlying data characteristics during compression. The streaming weak-SINDy algorithm constructs feature matrices and 
    
[^41]: JL-引理推导的用于判别式字典学习的最优投影

    JL-lemma derived Optimal Projections for Discriminative Dictionary Learning. (arXiv:2308.13991v1 [cs.CV])

    [http://arxiv.org/abs/2308.13991](http://arxiv.org/abs/2308.13991)

    本文提出了一种名为JLSPCADL的新方法，通过利用Johnson-Lindenstrauss引理选择一个维数的转换空间，从而实现判别式字典学习并提供更好的分类结果。

    

    为了克服在具有大量类别的大维度数据分类中的困难，我们提出了一种名为JLSPCADL的新方法。本文利用Johnson-Lindenstrauss(JL)引理，在一个转换空间中选择一个维数，可以在其中学习用于信号分类的判别式字典。与通常使用JL进行降维的随机投影不同，我们使用从Modified Supervised PC Analysis (M-SPCA)推导得出的投影转换矩阵，其维数遵循JL的规定。JLSPCADL提供了一种启发式方法来推断适当的失真水平和相应的字典原子的适当描述长度(SDL)，以推导出一个最优的特征空间，从而提供更好的分类的字典原子的SDL。与最先进的基于降维的字典学习方法不同，从M-SPCA中一步得出的投影转换矩阵提供了最大的特征-标签一致性。

    To overcome difficulties in classifying large dimensionality data with a large number of classes, we propose a novel approach called JLSPCADL. This paper uses the Johnson-Lindenstrauss (JL) Lemma to select the dimensionality of a transformed space in which a discriminative dictionary can be learned for signal classification. Rather than reducing dimensionality via random projections, as is often done with JL, we use a projection transformation matrix derived from Modified Supervised PC Analysis (M-SPCA) with the JL-prescribed dimension.  JLSPCADL provides a heuristic to deduce suitable distortion levels and the corresponding Suitable Description Length (SDL) of dictionary atoms to derive an optimal feature space and thus the SDL of dictionary atoms for better classification. Unlike state-of-the-art dimensionality reduction-based dictionary learning methods, a projection transformation matrix derived in a single step from M-SPCA provides maximum feature-label consistency of the transfor
    
[^42]: 可实现回归的最优学习算法：PAC学习和在线学习

    Optimal Learners for Realizable Regression: PAC Learning and Online Learning. (arXiv:2307.03848v1 [cs.LG])

    [http://arxiv.org/abs/2307.03848](http://arxiv.org/abs/2307.03848)

    本论文研究了可实现回归问题的PAC学习和在线学习的统计复杂度，并提出了对于可学习性的必要条件和充分条件。

    

    本研究旨在对可实现回归在PAC学习和在线学习的统计复杂度进行刻画。先前的研究已经证明了有限的fat shattering维度对于PAC学习的充分性以及有限的scaled Natarajan维度对于必要性的存在，但自从Simon 1997（SICOMP '97）的工作以来，对于更完整的刻画的进展甚少。为此，我们首先引入了一种最小化实例最优学习算法来对可实现回归进行学习，并提出了一种既定性又定量地刻画了哪些类的实数预测器可以被学习的新颖维度。然后，我们确定了一个与图维度相关的组合维度，该维度刻画了在可实现设置中的ERM可学习性。最后，我们根据与DS维度相关的组合维度建立了学习可行性的必要条件，并猜测它也可能是充分的。

    In this work, we aim to characterize the statistical complexity of realizable regression both in the PAC learning setting and the online learning setting.  Previous work had established the sufficiency of finiteness of the fat shattering dimension for PAC learnability and the necessity of finiteness of the scaled Natarajan dimension, but little progress had been made towards a more complete characterization since the work of Simon 1997 (SICOMP '97). To this end, we first introduce a minimax instance optimal learner for realizable regression and propose a novel dimension that both qualitatively and quantitatively characterizes which classes of real-valued predictors are learnable. We then identify a combinatorial dimension related to the Graph dimension that characterizes ERM learnability in the realizable setting. Finally, we establish a necessary condition for learnability based on a combinatorial dimension related to the DS dimension, and conjecture that it may also be sufficient in 
    
[^43]: 关于一般函数逼近下的均场强化学习的统计效率

    On the Statistical Efficiency of Mean Field Reinforcement Learning with General Function Approximation. (arXiv:2305.11283v1 [cs.LG])

    [http://arxiv.org/abs/2305.11283](http://arxiv.org/abs/2305.11283)

    本文研究了一般函数逼近下的均场控制(MFC)和均场博弈(MFG)中的强化学习的统计效率，提出了基于乐观最大似然估计的算法，并仅对转移动力学具有Lipschitz连续性的假设，最后建立了一个指数级的下界支持MFC设置。

    

    本文研究了一般函数逼近下的均场控制（MFC）和均场博弈（MFG）中强化学习的统计效率。引入了一种称为Mean-Field Model-Based Eluder Dimension (MBED)的新概念，包含了一系列丰富的均场强化学习问题。此外，我们提出了基于乐观最大似然估计的算法，可以返回一个$\epsilon$优的策略，适用于MFC或$\epsilon$纳什均衡策略适用于MFG，样本复杂度多项式与相关参数无关，与状态、动作和代理数量无关。值得注意的是，我们的结果仅对转移动力学具有Lipschitz连续性的假设，避免了以前的强结构假设。最后，在tabular设置下，假设有一个生成模型，我们建立了一个指数级的下界支持MFC设置，同时提供了一种新颖的样本高效的模型消除算法以逼近最优策略。

    In this paper, we study the statistical efficiency of Reinforcement Learning in Mean-Field Control (MFC) and Mean-Field Game (MFG) with general function approximation. We introduce a new concept called Mean-Field Model-Based Eluder Dimension (MBED), which subsumes a rich family of Mean-Field RL problems. Additionally, we propose algorithms based on Optimistic Maximal Likelihood Estimation, which can return an $\epsilon$-optimal policy for MFC or an $\epsilon$-Nash Equilibrium policy for MFG, with sample complexity polynomial w.r.t. relevant parameters and independent of the number of states, actions and the number of agents. Notably, our results only require a mild assumption of Lipschitz continuity on transition dynamics and avoid strong structural assumptions in previous work. Finally, in the tabular setting, given the access to a generative model, we establish an exponential lower bound for MFC setting, while providing a novel sample-efficient model elimination algorithm to approxim
    
[^44]: TACOS: 面向拓扑结构的分布式训练集合算法合成器

    TACOS: Topology-Aware Collective Algorithm Synthesizer for Distributed Training. (arXiv:2304.05301v1 [cs.DC])

    [http://arxiv.org/abs/2304.05301](http://arxiv.org/abs/2304.05301)

    TACOS 是一个能够自动合成任意输入网络拓扑的面向拓扑结构的集合合成器。与基准算法相比，TACOS 合成的 All-Reduce 算法速度提高了 3.73 倍，为 512-NPU 系统合成集体算法只需 6.1 分钟。

    

    集群之间的集合通讯是分布式训练不可或缺的一部分。运行面向拓扑结构的集合算法对于优化通讯性能以最小化拥塞是至关重要的。目前，此类算法仅适用于一小部分简单拓扑结构，限制了训练集群中采用的拓扑结构并处理由于网络故障而产生的不规则拓扑结构。 本文提出了 TACOS，这是一个可自动合成任意输入网络拓扑的面向拓扑结构的集合合成器。TACOS 合成的 All-Reduce 算法比基线算法快了 3.73 倍，并为 512-NPU 系统合成集体算法仅需 6.1 分钟。

    Collective communications are an indispensable part of distributed training. Running a topology-aware collective algorithm is crucial for optimizing communication performance by minimizing congestion. Today such algorithms only exist for a small set of simple topologies, limiting the topologies employed in training clusters and handling irregular topologies due to network failures. In this paper, we propose TACOS, an automated topology-aware collective synthesizer for arbitrary input network topologies. TACOS synthesized 3.73x faster All-Reduce algorithm over baselines, and synthesized collective algorithms for 512-NPU system in just 6.1 minutes.
    
[^45]: 走向数据和知识驱动的人工智能：神经符号计算综述

    Towards Data-and Knowledge-Driven Artificial Intelligence: A Survey on Neuro-Symbolic Computing. (arXiv:2210.15889v4 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2210.15889](http://arxiv.org/abs/2210.15889)

    神经符号计算是将符号和统计认知范式整合的研究领域，在推理和解释性以及神经网络学习等方面具有潜力。本文综述了该领域的最新进展和重要贡献，并探讨了其成功应用案例和未来发展方向。

    

    多年来，神经符号计算（NeSy）一直是人工智能（AI）领域的研究热点，追求符号和统计认知范式的整合。由于NeSy在符号表示的推理和可解释性以及神经网络的强大学习中具有潜力，它可能成为下一代AI的催化剂。本文系统综述了NeSy研究的最新进展和重要贡献。首先，我们介绍了该领域的研究历史，涵盖了早期工作和基础知识。我们进一步讨论了背景概念，并确定了推动NeSy发展的关键因素。随后，我们按照几个主要特征对近期的里程碑方法进行分类，包括神经符号整合、知识表示、知识嵌入和功能性。接下来，我们简要讨论了成功的应用案例，以及NeSy领域的挑战和未来发展方向。

    Neural-symbolic computing (NeSy), which pursues the integration of the symbolic and statistical paradigms of cognition, has been an active research area of Artificial Intelligence (AI) for many years. As NeSy shows promise of reconciling the advantages of reasoning and interpretability of symbolic representation and robust learning in neural networks, it may serve as a catalyst for the next generation of AI. In the present paper, we provide a systematic overview of the recent developments and important contributions of NeSy research. Firstly, we introduce study history of this area, covering early work and foundations. We further discuss background concepts and identify key driving factors behind the development of NeSy. Afterward, we categorize recent landmark approaches along several main characteristics that underline this research paradigm, including neural-symbolic integration, knowledge representation, knowledge embedding, and functionality. Next, we briefly discuss the successfu
    

