# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Query-Efficient Correlation Clustering with Noisy Oracle](https://rss.arxiv.org/abs/2402.01400) | 本论文提出了一种低查询成本的聚类方法，利用纯在组合多臂赌博机探索范式实现在线学习，并设计了能在NP-hard情况下运行的多项式时间算法。 |
| [^2] | [Geometry of Polynomial Neural Networks](https://rss.arxiv.org/abs/2402.00949) | 本研究利用代数几何工具研究了具有单项式激活函数的多项式神经网络的表达性和学习过程，通过对神经流形的维度和学习度的研究，提供了网络表达能力和训练复杂度的度量，并给出了可学函数数量的上界。 |
| [^3] | [GINopic: Topic Modeling with Graph Isomorphism Network](https://arxiv.org/abs/2404.02115) | GINopic是一种主题建模框架，利用图同构网络捕捉单词之间的相关性，相比于现有主题模型，展示了更好的有效性和推进主题建模的潜力。 |
| [^4] | [CHAIN: Enhancing Generalization in Data-Efficient GANs via lipsCHitz continuity constrAIned Normalization](https://arxiv.org/abs/2404.00521) | 通过引入CHAIN，该方法在数据有限的情况下，解决了GANs中鉴别器过拟合和训练不稳定的问题，提高了泛化能力和训练稳定性。 |
| [^5] | [TRABSA: Interpretable Sentiment Analysis of Tweets using Attention-based BiLSTM and Twitter-RoBERTa](https://arxiv.org/abs/2404.00297) | TRABSA是一个集成了transformer架构、注意力机制和BiLSTM网络的混合框架，利用RoBERTa在大量推特上训练，填补了情感分析领域的差距，实现了94%的准确性和显著的性能提升。 |
| [^6] | [Deep decomposition method for the limited aperture inverse obstacle scattering problem](https://arxiv.org/abs/2403.19470) | 提出了一种用于有限孔径逆障碍散射问题的深度分解方法，通过向神经网络架构提供与散射模型相关联的物理运算符，实现深度学习在逆问题上工作，并避免扭曲解决方案。 |
| [^7] | [Visual Whole-Body Control for Legged Loco-Manipulation](https://arxiv.org/abs/2403.16967) | 这项研究提出了一种利用视觉全身控制的框架，使腿式机器人能够同时控制腿部和手臂，以扩展操作能力，并通过仿真训练和Sim2Real转移实现了在捡起不同物体方面取得显著改进。 |
| [^8] | [Optimization on a Finer Scale: Bounded Local Subgradient Variation Perspective](https://arxiv.org/abs/2403.16317) | 该研究在有界局部次梯度变化的条件下研究非光滑优化问题，提出的目标函数类能帮助更好理解传统优化问题的复杂性，并在一般情况下降低oracle复杂度。 |
| [^9] | [G-ACIL: Analytic Learning for Exemplar-Free Generalized Class Incremental Learning](https://arxiv.org/abs/2403.15706) | 在这项研究中，我们提出了一种面向非范例化的广义分析类增量学习，通过采用分析学习并提供了对GCIL情景的分析解决方案，有效地解决了模型快速遗忘和数据隐私侵犯问题。 |
| [^10] | [Bridge the Modality and Capacity Gaps in Vision-Language Model Selection](https://arxiv.org/abs/2403.13797) | 本文分析了在语言-Only VLM选择中的两个固有挑战：「模态差距」和「能力差距」，并提出了VLM选择中弥合这两个差距的方法 |
| [^11] | [Do CLIPs Always Generalize Better than ImageNet Models?](https://arxiv.org/abs/2403.11497) | CLIP模型在面对分布转移时表现出良好的泛化能力，作者设计了CounterAnimal数据集来探究模型对虚假特征的依赖性。 |
| [^12] | [Narrative Feature or Structured Feature? A Study of Large Language Models to Identify Cancer Patients at Risk of Heart Failure](https://arxiv.org/abs/2403.11425) | 使用大型语言模型结合新颖的叙述特征，能够有效识别癌症患者患心力衰竭的风险，表现优于传统机器学习模型和深度学习模型。 |
| [^13] | [An SDP-based Branch-and-Cut Algorithm for Biclustering](https://arxiv.org/abs/2403.11351) | 提出了一个基于SDP的分支定界算法，用于解决$k$-最密不相交双团问题。 |
| [^14] | [Chronos: Learning the Language of Time Series](https://arxiv.org/abs/2403.07815) | Chronos框架通过在固定词汇上训练预训练的概率时间序列模型，在大量数据集上进行了全面基准测试，表现出在训练语料库中的数据集上明显优于其他方法，并且在新数据集上的零样本性能表现可比甚至优于其他方法。 |
| [^15] | [LaB-GATr: geometric algebra transformers for large biomedical surface and volume meshes](https://arxiv.org/abs/2403.07536) | LaB-GATr 是一种几何代数变换器神经网络，通过序列压缩和插值有效地学习大规模生物医学表面和体积网格，扩展了传统的 GATr 方法并尊重了欧几里得对称性，达到了最先进的结果。 |
| [^16] | [Calibrating Multi-modal Representations: A Pursuit of Group Robustness without Annotations](https://arxiv.org/abs/2403.07241) | 本文旨在探索如何减少CLIP对伪特征的依赖，从而提高群体鲁棒性，而无需使用注释数据。 |
| [^17] | [ERBench: An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Language Models](https://arxiv.org/abs/2403.05266) | ERBench是一个基于实体关系的大型语言模型幻觉基准，通过自动转换任何关系数据库并构建可自动验证的问题，以支持复杂性评估和调试 |
| [^18] | [Online Adaptation of Language Models with a Memory of Amortized Contexts](https://arxiv.org/abs/2403.04317) | 提出了一种带有分摊上下文记忆的在线适应框架，可有效地提取、压缩并存储信息以保持强大的知识保留能力 |
| [^19] | [Deep Configuration Performance Learning: A Systematic Survey and Taxonomy](https://arxiv.org/abs/2403.03322) | 性能是可配置软件系统行为的关键属性，本文针对深度学习在可配置软件性能学习方面进行了全面的调查与分类研究。 |
| [^20] | [Mixed-Strategy Nash Equilibrium for Crowd Navigation](https://arxiv.org/abs/2403.01537) | 通过简单的迭代贝叶斯更新方案和基于数据驱动的框架，我们证明了混合策略纳什均衡模型为人群导航提供了实时且可扩展的决策制定方法。 |
| [^21] | [Large-scale variational Gaussian state-space models](https://arxiv.org/abs/2403.01371) | 该论文介绍了一种针对具有高斯噪声驱动非线性动力学的状态空间模型的大规模变分算法和结构化逼近方法，可以有效评估ELBO和获取低方差的随机梯度估计，通过利用低秩蒙特卡罗逼近和推断网络的精度矩阵更新，将近似平滑问题转化为近似滤波问题。 |
| [^22] | [TimeXer: Empowering Transformers for Time Series Forecasting with Exogenous Variables](https://arxiv.org/abs/2402.19072) | 本文提出了一个新框架TimeXer，利用外部信息增强变压器对内生变量进行预测，弥补了以往多变量或单变量预测中忽视外生信息的不足。 |
| [^23] | [Uncertainty-Based Extensible Codebook for Discrete Federated Learning in Heterogeneous Data Silos](https://arxiv.org/abs/2402.18888) | 提出了一种基于不确定性的可拓展编码本的联邦学习框架，用于应对异构数据孤岛中模型适应新分布的挑战 |
| [^24] | [ArEEG_Chars: Dataset for Envisioned Speech Recognition using EEG for Arabic Characters](https://arxiv.org/abs/2402.15733) | 该论文介绍了一种用于阿拉伯字符的EEG数据集ArEEG_Chars，通过深度学习实现97%的准确率，在脑机接口中具有重要意义。 |
| [^25] | [ACE : Off-Policy Actor-Critic with Causality-Aware Entropy Regularization](https://arxiv.org/abs/2402.14528) | 该论文提出了ACE算法，通过引入因果感知熵正则化，有效评估不同行为的重要性，并分析梯度休眠现象，引入休眠引导复位机制，在多个连续控制任务中取得显著性能优势。 |
| [^26] | [DDIPrompt: Drug-Drug Interaction Event Prediction based on Graph Prompt Learning](https://arxiv.org/abs/2402.11472) | 基于图提示学习的DDIPrompt框架旨在解决药物相互作用事件预测中的高度不平衡事件分布和罕见事件标记数据稀缺性问题。 |
| [^27] | [Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE)](https://arxiv.org/abs/2402.10376) | 本研究提出了一种新方法，Sparse Linear Concept Embeddings（SpLiCE），通过将CLIP表示转换为人可解释概念的稀疏线性组合，实现了对CLIP嵌入的解释。 |
| [^28] | [OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset](https://arxiv.org/abs/2402.10176) | OpenMathInstruct-1是一个包含180万个数学问题和解决方法对的数据集，通过合成开源LLM的代码解释器解决方案来构建，填补了目前开源LLM在数学技能方面与闭源LLM之间的差距。 |
| [^29] | [HYPO: Hyperspherical Out-of-Distribution Generalization](https://arxiv.org/abs/2402.07785) | HYPO是一个在超球面空间中学习域不变表示的框架，通过内类变化和间类分离原则的引导，提高了离群泛化性能。 |
| [^30] | [Near-Minimax-Optimal Distributional Reinforcement Learning with a Generative Model](https://arxiv.org/abs/2402.07598) | 本论文提出了一种基于生成模型的近最小极大分布式强化学习算法，该算法在使用生成模型近似回报分布方面具有极小极大优势，解决了一个开放问题，并提供了实验研究结果。 |
| [^31] | [Whispers in the Machine: Confidentiality in LLM-integrated Systems](https://arxiv.org/abs/2402.06922) | 本研究提供了一种评估LLM集成系统保密性的系统化方法，通过形式化一个"秘密密钥"游戏来捕捉模型隐藏私人信息的能力。评估了八种攻击和四种防御方法，发现当前的防御方法缺乏泛化性能。 |
| [^32] | [Improving the Worst-Case Bidirectional Communication Complexity for Nonconvex Distributed Optimization under Function Similarity](https://arxiv.org/abs/2402.06412) | 本文提出了MARINA-P方法，通过引入一系列相关压缩器，优化了服务器到工作节点的通信复杂度。理论分析证明，MARINA-P在算法上优于现有方法，并可以作为支持双向压缩的起点。通过与上行压缩和动量步骤的结合，M3方法实现了双向压缩，并在总通信复杂度上改进。 |
| [^33] | [Shadowheart SGD: Distributed Asynchronous SGD with Optimal Time Complexity Under Arbitrary Computation and Communication Heterogeneity](https://arxiv.org/abs/2402.04785) | Shadowheart SGD是一种新的分布式异步SGD方法，利用无偏压缩技术，在任意的计算和通信异构性下具有最优时间复杂度，并显著优化了先前的集中式方法。同时，我们还开发了对应的双向设置方法。 |
| [^34] | [A Framework for Bilevel Optimization on Riemannian Manifolds](https://arxiv.org/abs/2402.03883) | 本论文提出了一个在黎曼流形上解决约束双层优化问题的框架，并提供了多种超梯度估计策略，并对其进行了研究。该框架不仅适用于确定性双层优化问题，还适用于随机双层优化问题，并且可以使用一般的回退。在各种应用中，该框架都具有很高的实用性。 |
| [^35] | [Projected Generative Diffusion Models for Constraint Satisfaction](https://arxiv.org/abs/2402.03559) | 本文介绍了一种名为投影式生成扩散模型（PGDM）的方法，它能够通过约束优化问题将生成扩散模型适用于对特定条件有严格要求的场景。该方法通过迭代投影方法确保生成的数据符合指定的约束或物理原理。实验证明PGDM在复杂的约束和常微分方程的情况下也能合成出符合要求的输出。 |
| [^36] | [DeSparsify: Adversarial Attack Against Token Sparsification Mechanisms in Vision Transformers](https://arxiv.org/abs/2402.02554) | 本文提出了一种对抗攻击方法DeSparsify，针对使用Token稀疏化机制的视觉Transformer，通过精心制作的对抗样本欺骗稀疏化机制，导致最坏情况的性能，以此耗尽操作系统的资源并保持隐蔽性。 |
| [^37] | [EuLagNet: Eulerian Fluid Prediction with Lagrangian Dynamics](https://arxiv.org/abs/2402.02425) | EuLagNet提出了一种新的拉格朗日引导范式，通过跟踪多尺度关键粒子的运动来捕捉多尺度流体动力学。这种方法克服了由于欧拉观察而导致的流体动力学困难，为准确预测未来的流体提供了一种有效的方法。 |
| [^38] | [Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction](https://arxiv.org/abs/2402.02416) | Aligner是一种通过学习校正残差来实现高效对齐的方法，相比于传统的强化学习方法，Aligner具有参数高效、弱到强泛化以及即插即用的优势。 |
| [^39] | [Personalized Path Recourse for Reinforcement Learning Agents](https://arxiv.org/abs/2312.08724) | 该论文介绍了一种针对增强学习代理的个性化路径补救方法，该方法通过编辑动作路径来实现期望目标，同时保持与代理的原始路径相似度高，并且个性化适应代理的行为模式。这种方法适用于纠正或改进动作或数据序列以实现预定目标。 |
| [^40] | [Consistency Models for Scalable and Fast Simulation-Based Inference](https://arxiv.org/abs/2312.05440) | 提出了一种新的神经后验估计的一致性模型，结合了标准化流和流匹配方法的优点，用于可扩展、快速和摊销推断，在多个实验中展示出优越性能。 |
| [^41] | [LayerCollapse: Adaptive compression of neural networks](https://arxiv.org/abs/2311.17943) | LayerCollapse是一种自适应压缩神经网络的方法，通过结构化剪枝来减少全连接层的深度，而不需要进行微调，并且对性能影响有限。该方法通过正则化激活函数的线性度来控制模型的表达能力。 |
| [^42] | [Fuse It or Lose It: Deep Fusion for Multimodal Simulation-Based Inference](https://arxiv.org/abs/2311.10671) | 提出了多模态神经后验估计 (MultiNPE) 方法，利用深度融合学习整合不同来源的异构数据，在模拟推理中提高了对复杂数学模型参数的准确推断能力。 |
| [^43] | [Multi-Agent Diagnostics for Robustness via Illuminated Diversity.](http://arxiv.org/abs/2401.13460) | MADRID是一种新方法，通过生成多样化的对抗场景来揭示预训练多Agent策略的战略漏洞，并通过遗憾值衡量漏洞的程度。 |
| [^44] | [DISTINQT: A Distributed Privacy Aware Learning Framework for QoS Prediction for Future Mobile and Wireless Networks.](http://arxiv.org/abs/2401.10158) | DISTINQT是一种面向未来移动和无线网络的隐私感知分布式学习框架，用于QoS预测。 |
| [^45] | [Learning Performance-Oriented Control Barrier Functions Under Complex Safety Constraints and Limited Actuation.](http://arxiv.org/abs/2401.05629) | 本研究提出了一个新颖的自监督学习框架，通过构建可导函数来近似安全集合，并使用神经网络参数化控制障碍函数，以解决在复杂安全约束和有限执行能力下寻找最优CBF的挑战。 |
| [^46] | [H2G2-Net: A Hierarchical Heterogeneous Graph Generative Network Framework for Discovery of Multi-Modal Physiological Responses.](http://arxiv.org/abs/2401.02905) | H2G2-Net是一种用于发现多模态生理反应的分层异构图生成网络框架，能够自动学习图结构而不需要预定义的领域知识。 |
| [^47] | [SASSL: Enhancing Self-Supervised Learning via Neural Style Transfer.](http://arxiv.org/abs/2312.01187) | SASSL提出了一种基于神经风格迁移的增强技术，通过解耦语义和风格属性，在自监督学习中生成多样化的增强样本，从而提升了图像分类性能。 |
| [^48] | [Towards Inferring Users' Impressions of Robot Performance in Navigation Scenarios.](http://arxiv.org/abs/2310.11590) | 本研究拟通过非语言行为提示和机器学习技术预测人们对机器人行为印象，并提供了一个数据集和分析结果，发现在导航场景中，空间特征是最关键的信息。 |
| [^49] | [Neur2RO: Neural Two-Stage Robust Optimization.](http://arxiv.org/abs/2310.04345) | Neur2RO是一种神经网络驱动的二阶段鲁棒优化算法，通过学习估计第二阶段问题的值函数，并嵌入到经典的列-约束生成算法中，能够高效地求解嵌套的最小-最大-最小优化问题。 |
| [^50] | [Semi-Supervised Object Detection in the Open World.](http://arxiv.org/abs/2307.15710) | 本文提出了一种半监督开放世界目标检测框架，能够有效地检测分布外的数据并从中学习，通过基于集成的OOD检测器和半监督学习方法，实现与最先进方法相当的性能。 |
| [^51] | [Zero-Shot Wireless Indoor Navigation through Physics-Informed Reinforcement Learning.](http://arxiv.org/abs/2306.06766) | 该论文提出了一种基于物理信息强化学习的零样本无线室内导航方法，通过样本高效学习和零样本泛化来提高导航效率。 |
| [^52] | [Quartile-Based Seasonality Decomposition for Time Series Forecasting and Anomaly Detection.](http://arxiv.org/abs/2306.05989) | 本文提出了一种名为QBSD的实时预测方法，以在时间序列异常检测中取得最佳平衡。 |
| [^53] | [FLEdge: Benchmarking Federated Machine Learning Applications in Edge Computing Systems.](http://arxiv.org/abs/2306.05172) | FLEdge是一个面向边缘计算系统中FL工作量的基准测试，通过研究硬件异构性、能量效率和隐私级别对FL系统训练的影响，以及客户端退出对最新FL策略的影响，提供了训练最先进的FL工作负载的新见解。 |
| [^54] | [Comparative Study on Semi-supervised Learning Applied for Anomaly Detection in Hydraulic Condition Monitoring System.](http://arxiv.org/abs/2306.02709) | 本研究比较了不同类型的半监督学习方法在液压状态监测系统中用于异常检测。深度学习模型表现最好，而集成模型可以进一步提高检测性能。 |
| [^55] | [Spectral Clustering via Orthogonalization-Free Methods.](http://arxiv.org/abs/2305.10356) | 本文提出了四种无正交化方法作为谱聚类降维，不需要昂贵的特征值估计，在聚类质量和计算成本方面均优于已有方法，适合于并行计算。 |
| [^56] | [Proportionally Representative Clustering.](http://arxiv.org/abs/2304.13917) | 本文提出了一个新的公平性准则——比例代表性公平性（PRF），并设计了有效的算法满足该准则。 |
| [^57] | [Phylo2Vec: a vector representation for binary trees.](http://arxiv.org/abs/2304.12693) | Phylo2Vec是一种新的二叉树简明表示方法，它能够轻松采样二叉树，并以系统性的方法遍历树空间。这种方法用于构建深度神经网络，能够显著提高蛋白质类别预测的性能。 |
| [^58] | [Kernel Density Bayesian Inverse Reinforcement Learning.](http://arxiv.org/abs/2303.06827) | KD-BIRL是一种核密度贝叶斯逆强化学习方法，通过直接逼近似然函数来学习代理的奖励函数，克服了学习点估计的缺点，并适用于复杂和无限环境。 |
| [^59] | [Learning to Control and Coordinate Mixed Traffic Through Robot Vehicles at Complex and Unsignalized Intersections.](http://arxiv.org/abs/2301.05294) | 本研究提出了一种去中心化的多智能体强化学习方法，用于控制和协调混合交通，特别是人驾驶车辆和机器人车辆在实际复杂交叉口的应用。实验结果表明，使用5%的机器人车辆可以有效防止交叉口内的拥堵形成。 |
| [^60] | [Neural-Rendezvous: Provably Robust Guidance and Control to Encounter Interstellar Objects.](http://arxiv.org/abs/2208.04883) | 本文提出了神经会合，一种深度学习导航和控制框架，用于可靠、准确和自主地遭遇快速移动的星际物体。它通过点最小范数追踪控制和谱归一化深度神经网络引导策略来提供高概率指数上界的飞行器交付误差。 |
| [^61] | [A Black-box NLP Classifier Attacker.](http://arxiv.org/abs/2112.11660) | 本文提出了一个黑盒NLP分类器攻击模型，通过基于自注意机制的词选择和贪婪搜索算法进行词替换，解决了影响NLP领域传统图像攻击方法不适用的问题。 |

# 详细

[^1]: 低查询成本带噪声or同时聚类

    Query-Efficient Correlation Clustering with Noisy Oracle

    [https://rss.arxiv.org/abs/2402.01400](https://rss.arxiv.org/abs/2402.01400)

    本论文提出了一种低查询成本的聚类方法，利用纯在组合多臂赌博机探索范式实现在线学习，并设计了能在NP-hard情况下运行的多项式时间算法。

    

    我们研究了一个常见的聚类设置，其中我们需要对n个元素进行聚类，并且我们的目标是尽可能少地向返回两个元素相似性的有噪声的oracle查询。我们的设置涵盖了许多应用领域，在这些领域中，相似性函数计算起来成本高并且 inherently noisy。我们提出了两种基于纯在组合多臂赌博机探索范式(PE-CMAB)的在线学习问题的新颖表达方法固定置信度和固定预算设置。对于这两种设置，我们设计了将抽样策略与经典的相关聚类近似算法相结合的算法，并研究了它们的理论保证。我们的结果是这样的：这些算法是第一个在底层离线优化问题为NP-hard的情况下运行的多项式时间算法的例子。

    We study a general clustering setting in which we have $n$ elements to be clustered, and we aim to perform as few queries as possible to an oracle that returns a noisy sample of the similarity between two elements. Our setting encompasses many application domains in which the similarity function is costly to compute and inherently noisy. We propose two novel formulations of online learning problems rooted in the paradigm of Pure Exploration in Combinatorial Multi-Armed Bandits (PE-CMAB): fixed confidence and fixed budget settings. For both settings, we design algorithms that combine a sampling strategy with a classic approximation algorithm for correlation clustering and study their theoretical guarantees. Our results are the first examples of polynomial-time algorithms that work for the case of PE-CMAB in which the underlying offline optimization problem is NP-hard.
    
[^2]: 多项式神经网络的几何性质

    Geometry of Polynomial Neural Networks

    [https://rss.arxiv.org/abs/2402.00949](https://rss.arxiv.org/abs/2402.00949)

    本研究利用代数几何工具研究了具有单项式激活函数的多项式神经网络的表达性和学习过程，通过对神经流形的维度和学习度的研究，提供了网络表达能力和训练复杂度的度量，并给出了可学函数数量的上界。

    

    我们研究了具有单项式激活函数的多项式神经网络（PNN）的表达性和学习过程。网络的权重参数化了神经流形。在本文中，我们使用代数几何工具研究了某些神经流形：我们给出了半代数集的明确描述并特征化了它们的Zariski闭包，称为神经多样性。我们研究了它们的维度并将一个代数度量，学习度，与神经多样性相关联。维度作为网络表达能力的几何度量，学习度是训练网络的复杂度度量，并提供可学函数数量的上界。这些理论结果还伴随着实验证明。

    We study the expressivity and learning process for polynomial neural networks (PNNs) with monomial activation functions. The weights of the network parametrize the neuromanifold. In this paper, we study certain neuromanifolds using tools from algebraic geometry: we give explicit descriptions as semialgebraic sets and characterize their Zariski closures, called neurovarieties. We study their dimension and associate an algebraic degree, the learning degree, to the neurovariety. The dimension serves as a geometric measure for the expressivity of the network, the learning degree is a measure for the complexity of training the network and provides upper bounds on the number of learnable functions. These theoretical results are accompanied with experiments.
    
[^3]: GINopic：利用图同构网络进行主题建模

    GINopic: Topic Modeling with Graph Isomorphism Network

    [https://arxiv.org/abs/2404.02115](https://arxiv.org/abs/2404.02115)

    GINopic是一种主题建模框架，利用图同构网络捕捉单词之间的相关性，相比于现有主题模型，展示了更好的有效性和推进主题建模的潜力。

    

    主题建模是分析和探索大型文档集合的广泛使用方法。 最近的研究工作将预训练的上下文化语言模型，如BERT嵌入，纳入主题建模中。 然而，它们通常忽略了单词之间相互依赖传达的固有信息价值。 本研究介绍了GINopic，一种基于图同构网络的主题建模框架，以捕捉单词之间的相关性。 通过在不同基准数据集上进行内在的（定量和定性）和外部的评估，我们展示了与现有主题模型相比，GINopic的有效性，并突出了其推进主题建模的潜力。

    arXiv:2404.02115v1 Announce Type: new  Abstract: Topic modeling is a widely used approach for analyzing and exploring large document collections. Recent research efforts have incorporated pre-trained contextualized language models, such as BERT embeddings, into topic modeling. However, they often neglect the intrinsic informational value conveyed by mutual dependencies between words. In this study, we introduce GINopic, a topic modeling framework based on graph isomorphism networks to capture the correlation between words. By conducting intrinsic (quantitative as well as qualitative) and extrinsic evaluations on diverse benchmark datasets, we demonstrate the effectiveness of GINopic compared to existing topic models and highlight its potential for advancing topic modeling.
    
[^4]: CHAIN：通过受限唯一性连续性规范化增强数据高效GANs的泛化能力

    CHAIN: Enhancing Generalization in Data-Efficient GANs via lipsCHitz continuity constrAIned Normalization

    [https://arxiv.org/abs/2404.00521](https://arxiv.org/abs/2404.00521)

    通过引入CHAIN，该方法在数据有限的情况下，解决了GANs中鉴别器过拟合和训练不稳定的问题，提高了泛化能力和训练稳定性。

    

    生成对抗网络（GANs）显着推动了图像生成，但它们的性能严重依赖大量的训练数据。在数据有限的情况下，GANs经常面临鉴别器过拟合和训练不稳定的问题。我们的工作通过识别Batch Normalization（BN）中的关键缺陷来解决这一问题：在中心化和缩放步骤中梯度爆炸的倾向。为了解决这个问题，我们提出了CHAIN（受限唯一性连续性规范化），它将传统的中心化步骤替换为零均值正则化，并在缩放步骤中集成了Lipschitz连续性约束。CHAIN通过自适应插值归一化和非归一化特征进一步增强了GANs的训练，有效避免了鉴别器过拟合。

    arXiv:2404.00521v1 Announce Type: new  Abstract: Generative Adversarial Networks (GANs) significantly advanced image generation but their performance heavily depends on abundant training data. In scenarios with limited data, GANs often struggle with discriminator overfitting and unstable training. Batch Normalization (BN), despite being known for enhancing generalization and training stability, has rarely been used in the discriminator of Data-Efficient GANs. Our work addresses this gap by identifying a critical flaw in BN: the tendency for gradient explosion during the centering and scaling steps. To tackle this issue, we present CHAIN (lipsCHitz continuity constrAIned Normalization), which replaces the conventional centering step with zero-mean regularization and integrates a Lipschitz continuity constraint in the scaling step. CHAIN further enhances GAN training by adaptively interpolating the normalized and unnormalized features, effectively avoiding discriminator overfitting. Our 
    
[^5]: TRABSA：使用基于注意力的BiLSTM和Twitter-RoBERTa进行可解释的推文情感分析

    TRABSA: Interpretable Sentiment Analysis of Tweets using Attention-based BiLSTM and Twitter-RoBERTa

    [https://arxiv.org/abs/2404.00297](https://arxiv.org/abs/2404.00297)

    TRABSA是一个集成了transformer架构、注意力机制和BiLSTM网络的混合框架，利用RoBERTa在大量推特上训练，填补了情感分析领域的差距，实现了94%的准确性和显著的性能提升。

    

    情感分析对于理解公众舆论和消费者行为至关重要。现有模型面临着语言多样性、泛化能力和可解释性方面的挑战。我们提出了TRABSA，这是一个集成了基于transformer的架构、注意力机制和BiLSTM网络的混合框架，旨在解决这些挑战。利用在124M条推文上训练的RoBERTa，我们填补了情感分析基准测试中的差距，确保了最先进的准确性。通过将来自32个国家和美国各州的推文与数据集相结合，我们比较了六种词嵌入技术和三种基于词典的标注技术，并选择了最佳技术以实现最佳情感分析效果。TRABSA以94%的准确性和显著的精确度、召回率和F1得分增益，胜过了传统的机器学习和深度学习模型。在不同数据集上的评估显示了一致的优越性和泛化能力。SHAP和LIME分析提高了可解释性，增强了信心。

    arXiv:2404.00297v1 Announce Type: new  Abstract: Sentiment analysis is crucial for understanding public opinion and consumer behavior. Existing models face challenges with linguistic diversity, generalizability, and explainability. We propose TRABSA, a hybrid framework integrating transformer-based architectures, attention mechanisms, and BiLSTM networks to address this. Leveraging RoBERTa-trained on 124M tweets, we bridge gaps in sentiment analysis benchmarks, ensuring state-of-the-art accuracy. Augmenting datasets with tweets from 32 countries and US states, we compare six word-embedding techniques and three lexicon-based labeling techniques, selecting the best for optimal sentiment analysis. TRABSA outperforms traditional ML and deep learning models with 94% accuracy and significant precision, recall, and F1-score gains. Evaluation across diverse datasets demonstrates consistent superiority and generalizability. SHAP and LIME analyses enhance interpretability, improving confidence i
    
[^6]: 有限孔径逆障碍散射问题的深度分解方法

    Deep decomposition method for the limited aperture inverse obstacle scattering problem

    [https://arxiv.org/abs/2403.19470](https://arxiv.org/abs/2403.19470)

    提出了一种用于有限孔径逆障碍散射问题的深度分解方法，通过向神经网络架构提供与散射模型相关联的物理运算符，实现深度学习在逆问题上工作，并避免扭曲解决方案。

    

    在本文中，我们考虑了一种针对有限孔径逆障碍散射问题的深度学习方法。传统深度学习仅依赖数据是众所周知的，当只有间接观测数据和一个物理模型可用时，这可能限制其在逆问题上的性能。在面对这些局限性时，一个基本问题出现了：是否可能使深度学习能够在没有标记数据的情况下处理逆问题，并且了解它正在学习的内容？本文提出了一个用于这些目的的深度分解方法（DDM），它不需要地面真实标签。它通过向神经网络架构提供与散射模型相关联的物理运算符来实现这一点。此外，DDM中还实现了一种基于深度学习的数据完整性方案，以防止扭曲有限孔径数据的逆问题的解决方案。此外，除了解决i

    arXiv:2403.19470v1 Announce Type: cross  Abstract: In this paper, we consider a deep learning approach to the limited aperture inverse obstacle scattering problem. It is well known that traditional deep learning relies solely on data, which may limit its performance for the inverse problem when only indirect observation data and a physical model are available. A fundamental question arises in light of these limitations: is it possible to enable deep learning to work on inverse problems without labeled data and to be aware of what it is learning? This work proposes a deep decomposition method (DDM) for such purposes, which does not require ground truth labels. It accomplishes this by providing physical operators associated with the scattering model to the neural network architecture. Additionally, a deep learning based data completion scheme is implemented in DDM to prevent distorting the solution of the inverse problem for limited aperture data. Furthermore, apart from addressing the i
    
[^7]: 用于腿式定点机器人运动操作的视觉全身控制

    Visual Whole-Body Control for Legged Loco-Manipulation

    [https://arxiv.org/abs/2403.16967](https://arxiv.org/abs/2403.16967)

    这项研究提出了一种利用视觉全身控制的框架，使腿式机器人能够同时控制腿部和手臂，以扩展操作能力，并通过仿真训练和Sim2Real转移实现了在捡起不同物体方面取得显著改进。

    

    我们研究了使用配备手臂的腿式机器人进行移动操作的问题，即腿式定点操作。尽管机器人的腿通常用于移动，但通过进行全身控制，可以扩大其操作能力。也就是说，机器人可以同时控制腿部和手臂，以扩展其工作空间。我们提出了一个能够使用视觉观测自主进行全身控制的框架。我们的方法称为\ourFull~(\our)，由一个低级策略和一个高级策略组成。低级策略使用所有自由度来跟踪末端执行器的位置，高级策略根据视觉输入提出末端执行器位置。我们在仿真中训练了两个级别的策略，并进行了从Sim到实物的转移以进行实际机器人部署。我们进行了大量实验证明，在不同配置下（高度、）捡起不同物体方面，相对基线方法取得了显著改进。

    arXiv:2403.16967v1 Announce Type: cross  Abstract: We study the problem of mobile manipulation using legged robots equipped with an arm, namely legged loco-manipulation. The robot legs, while usually utilized for mobility, offer an opportunity to amplify the manipulation capabilities by conducting whole-body control. That is, the robot can control the legs and the arm at the same time to extend its workspace. We propose a framework that can conduct the whole-body control autonomously with visual observations. Our approach, namely \ourFull~(\our), is composed of a low-level policy using all degrees of freedom to track the end-effector manipulator position and a high-level policy proposing the end-effector position based on visual inputs. We train both levels of policies in simulation and perform Sim2Real transfer for real robot deployment. We perform extensive experiments and show significant improvements over baselines in picking up diverse objects in different configurations (heights,
    
[^8]: 在更细粒度上的优化：有界局部次梯度变化的视角

    Optimization on a Finer Scale: Bounded Local Subgradient Variation Perspective

    [https://arxiv.org/abs/2403.16317](https://arxiv.org/abs/2403.16317)

    该研究在有界局部次梯度变化的条件下研究非光滑优化问题，提出的目标函数类能帮助更好理解传统优化问题的复杂性，并在一般情况下降低oracle复杂度。

    

    我们在有界局部次梯度变化的条件下开始研究非光滑优化问题，它假设在点附近的小区域内，(次)梯度之间存在有限的差异，可以用平均或最大方式求值。由此产生的目标函数类包括传统优化中传统研究的目标函数类，这些类根据目标函数的Lipschitz连续性或其梯度的H\"{o}lder/Lipschitz连续性定义。此外，该定义类包含那些既不是Lipschitz连续的也没有H\"{o}lder连续梯度的函数。当限制在传统优化问题类时，定义研究类的参数导致更加精细的复杂性界限，在最坏情况下恢复传统的oracle复杂度界限，但一般情况下会导致那些不是“最坏情况”的函数具有较低的oracle 复杂性。

    arXiv:2403.16317v1 Announce Type: cross  Abstract: We initiate the study of nonsmooth optimization problems under bounded local subgradient variation, which postulates bounded difference between (sub)gradients in small local regions around points, in either average or maximum sense. The resulting class of objective functions encapsulates the classes of objective functions traditionally studied in optimization, which are defined based on either Lipschitz continuity of the objective or H\"{o}lder/Lipschitz continuity of its gradient. Further, the defined class contains functions that are neither Lipschitz continuous nor have a H\"{o}lder continuous gradient. When restricted to the traditional classes of optimization problems, the parameters defining the studied classes lead to more fine-grained complexity bounds, recovering traditional oracle complexity bounds in the worst case but generally leading to lower oracle complexity for functions that are not ``worst case.'' Some highlights of 
    
[^9]: G-ACIL：面向非范例化的广义类增量学习的分析学习

    G-ACIL: Analytic Learning for Exemplar-Free Generalized Class Incremental Learning

    [https://arxiv.org/abs/2403.15706](https://arxiv.org/abs/2403.15706)

    在这项研究中，我们提出了一种面向非范例化的广义分析类增量学习，通过采用分析学习并提供了对GCIL情景的分析解决方案，有效地解决了模型快速遗忘和数据隐私侵犯问题。

    

    分类增量学习(CIL)在顺序任务上训练网络，每个任务有不同的类别，但存在灾难性遗忘问题，当学习新任务时快速遗忘先前学到的知识。广义CIL(GCIL)旨在解决更接近现实情景下的CIL问题，即新数据具有混合数据类别和未知样本分布大小，导致遗忘加剧。现有的针对GCIL的尝试要么性能不佳，要么通过保存历史范例侵犯数据隐私。为了解决这个问题，本文提出了一种面向非范例化的广义分析类增量学习(G-ACIL)。G-ACIL采用分析学习(一种无梯度训练技术)，并为GCIL情景提供分析解(即闭合形式)。该解决方案通过将传入数据分解为暴露类和未暴露类，实现了增长类之间的等效性。

    arXiv:2403.15706v1 Announce Type: new  Abstract: Class incremental learning (CIL) trains a network on sequential tasks with separated categories but suffers from catastrophic forgetting, where models quickly lose previously learned knowledge when acquiring new tasks. The generalized CIL (GCIL) aims to address the CIL problem in a more real-world scenario, where incoming data have mixed data categories and unknown sample size distribution, leading to intensified forgetting. Existing attempts for the GCIL either have poor performance, or invade data privacy by saving historical exemplars. To address this, in this paper, we propose an exemplar-free generalized analytic class incremental learning (G-ACIL). The G-ACIL adopts analytic learning (a gradient-free training technique), and delivers an analytical solution (i.e., closed-form) to the GCIL scenario. This solution is derived via decomposing the incoming data into exposed and unexposed classes, allowing an equivalence between the incre
    
[^10]: 弥合视觉-语言模型选择中的模态差距和能力差距

    Bridge the Modality and Capacity Gaps in Vision-Language Model Selection

    [https://arxiv.org/abs/2403.13797](https://arxiv.org/abs/2403.13797)

    本文分析了在语言-Only VLM选择中的两个固有挑战：「模态差距」和「能力差距」，并提出了VLM选择中弥合这两个差距的方法

    

    视觉语言模型（VLMs）通过将图像与文本类别名称配对，在零样本图像分类方面表现出色。预训练的VLMs的不断增加使得特定任务的VLM选择更有可能标识出适合的VLM。因此，一种有前途的零样本图像分类策略是从VLM动物园中选择最合适的预训练VLM，仅依赖目标数据集的文本数据而无需访问数据集的图像。本文分析了这种仅语言VLM选择中两个固有挑战：「模态差距」——VLM在两个不同模态下的嵌入之间的差异，使得文本成为图像的一个不太可靠的替代品；「能力差距」——VLM的整体排名与其在目标数据集的排名之间存在差异，阻碍了直接从模型的整体表现来预测其数据集特定性能。我们提出了VLM选择

    arXiv:2403.13797v1 Announce Type: new  Abstract: Vision Language Models (VLMs) excel in zero-shot image classification by pairing images with textual category names. The expanding variety of Pre-Trained VLMs enhances the likelihood of identifying a suitable VLM for specific tasks. Thus, a promising zero-shot image classification strategy is selecting the most appropriate Pre-Trained VLM from the VLM Zoo, relying solely on the text data of the target dataset without access to the dataset's images. In this paper, we analyze two inherent challenges in assessing the ability of a VLM in this Language-Only VLM selection: the "Modality Gap" -- the disparity in VLM's embeddings across two different modalities, making text a less reliable substitute for images; and the "Capability Gap" -- the discrepancy between the VLM's overall ranking and its ranking for target dataset, hindering direct prediction of a model's dataset-specific performance from its general performance. We propose VLM Selectio
    
[^11]: CLIP总是比ImageNet模型泛化更好吗？

    Do CLIPs Always Generalize Better than ImageNet Models?

    [https://arxiv.org/abs/2403.11497](https://arxiv.org/abs/2403.11497)

    CLIP模型在面对分布转移时表现出良好的泛化能力，作者设计了CounterAnimal数据集来探究模型对虚假特征的依赖性。

    

    大型视觉语言模型，例如CLIP，已经彻底改变了现代机器学习。CLIP展示了在分布转移下的良好泛化能力，得到了越来越多的文献支持。然而，CLIP的评估数据集主要是为ImageNet基准而设计的变种，可能不能完全反映CLIP在LAION等上进行预训练时对虚假相关性的稳健性。为了弥补这一差距，我们收集了一个真实世界数据集，名为CounterAnimal，其中包含动物照片中发现的现实虚假特征。CounterAnimal包括a）常见组：包括常见背景的动物，并且 b) 对照组：包括在不寻常背景下的动物。从常见组到对照组的性能下降量化了模型对虚假特征（即背景）预测动物的依赖性。我们发现，在LAION或OpenAI数据上进行训练的CLIP即没有

    arXiv:2403.11497v1 Announce Type: cross  Abstract: Large vision language models, such as CLIPs, have revolutionized modern machine learning. CLIPs have demonstrated great generalizability under distribution shifts, supported by an increasing body of literature. However, the evaluation datasets for CLIPs are variations primarily designed for ImageNet benchmarks, which may not fully reflect the extent to which CLIPs, e.g., pre-trained on LAION, robust to spurious correlations. To bridge the gap, we collect a real-world dataset called CounterAnimal that contains realistic spurious features found in animal photos. CounterAnimal consists of a) the common group: comprising animals on common backgrounds, and b) the counter group: including animals on unusual backgrounds. The performance drops from the common to counter groups quantify the reliance of models on spurious features (i.e., backgrounds) to predict the animals. We find that CLIPs trained on either LAION or the OpenAI data exhibit no
    
[^12]: 叙事特征还是结构特征？研究大型语言模型以识别患心力衰竭风险的癌症患者

    Narrative Feature or Structured Feature? A Study of Large Language Models to Identify Cancer Patients at Risk of Heart Failure

    [https://arxiv.org/abs/2403.11425](https://arxiv.org/abs/2403.11425)

    使用大型语言模型结合新颖的叙述特征，能够有效识别癌症患者患心力衰竭的风险，表现优于传统机器学习模型和深度学习模型。

    

    癌症治疗已知会引入心毒性，对预后和生存率产生负面影响。识别患心力衰竭（HF）风险的癌症患者对于改善癌症治疗结果和安全性至关重要。本研究使用来自电子健康记录（EHRs）的机器学习（ML）模型，包括传统ML、时间感知长短期记忆（T-LSTM）和使用从结构化医学代码衍生的新颖叙述特征的大型语言模型（LLMs）来识别患HF风险的癌症患者。我们从佛罗里达大学健康中心识别了一组包括12,806名肺癌、乳腺癌和结直肠癌患者的癌症队列，其中1,602人在癌症后发展为HF。LLM GatorTron-3.9B取得了最佳的F1分数，比传统支持向量机高出39%，比T-LSTM深度学习模型高出7%，比广泛使用的Transformer模型BERT高出5.6%。

    arXiv:2403.11425v1 Announce Type: cross  Abstract: Cancer treatments are known to introduce cardiotoxicity, negatively impacting outcomes and survivorship. Identifying cancer patients at risk of heart failure (HF) is critical to improving cancer treatment outcomes and safety. This study examined machine learning (ML) models to identify cancer patients at risk of HF using electronic health records (EHRs), including traditional ML, Time-Aware long short-term memory (T-LSTM), and large language models (LLMs) using novel narrative features derived from the structured medical codes. We identified a cancer cohort of 12,806 patients from the University of Florida Health, diagnosed with lung, breast, and colorectal cancers, among which 1,602 individuals developed HF after cancer. The LLM, GatorTron-3.9B, achieved the best F1 scores, outperforming the traditional support vector machines by 39%, the T-LSTM deep learning model by 7%, and a widely used transformer model, BERT, by 5.6%. The analysi
    
[^13]: 基于SDP的二分图聚类分支定界算法

    An SDP-based Branch-and-Cut Algorithm for Biclustering

    [https://arxiv.org/abs/2403.11351](https://arxiv.org/abs/2403.11351)

    提出了一个基于SDP的分支定界算法，用于解决$k$-最密不相交双团问题。

    

    二分图聚类，也称为共聚类、块聚类或双向聚类，涉及将数据矩阵的行和列同时聚类成不同的组，使得同一组内的行和列显示出相似的模式。作为二分图聚类的模型问题，我们考虑$k$-最密不相交双团问题，其目标是在给定加权完全二分图中识别 $k$ 个不相交的完全二部子图（称为双团），使它们的密度之和最大化。为了解决这个问题，我们提出了一个定制的分支定界算法。对于上界例程，我们考虑半定规划放松并提出了用于加强界限的有效不等式。我们使用一种一阶方法以切平面方式解决这个放松问题。对于下界，我们设计了一个利用解决方案的最大权匹配舍入过程。

    arXiv:2403.11351v1 Announce Type: cross  Abstract: Biclustering, also called co-clustering, block clustering, or two-way clustering, involves the simultaneous clustering of both the rows and columns of a data matrix into distinct groups, such that the rows and columns within a group display similar patterns. As a model problem for biclustering, we consider the $k$-densest-disjoint biclique problem, whose goal is to identify $k$ disjoint complete bipartite subgraphs (called bicliques) of a given weighted complete bipartite graph such that the sum of their densities is maximized. To address this problem, we present a tailored branch-and-cut algorithm. For the upper bound routine, we consider a semidefinite programming relaxation and propose valid inequalities to strengthen the bound. We solve this relaxation in a cutting-plane fashion using a first-order method. For the lower bound, we design a maximum weight matching rounding procedure that exploits the solution of the relaxation solved
    
[^14]: Chronos: 学习时间序列的语言

    Chronos: Learning the Language of Time Series

    [https://arxiv.org/abs/2403.07815](https://arxiv.org/abs/2403.07815)

    Chronos框架通过在固定词汇上训练预训练的概率时间序列模型，在大量数据集上进行了全面基准测试，表现出在训练语料库中的数据集上明显优于其他方法，并且在新数据集上的零样本性能表现可比甚至优于其他方法。

    

    我们介绍了Chronos，一个简单但有效的预训练概率时间序列模型框架。Chronos使用缩放和量化将时间序列值标记化为固定词汇，并通过交叉熵损失在这些标记化的时间序列上训练现有的基于Transformer的语言模型架构。我们在大量公开可用数据集上基于T5系列（参数范围从20M到710M）对Chronos模型进行了预训练，同时通过高斯过程生成了一个合成数据集以提高泛化能力。在包含42个数据集的全面基准测试中，涵盖了传统的本地模型和深度学习方法，我们展示了Chronos模型：（a）在训练语料库中的数据集上明显优于其他方法；（b）相对于专门训练的方法，在新数据集上的零样本性能可比甚至优于其他方法。

    arXiv:2403.07815v1 Announce Type: cross  Abstract: We introduce Chronos, a simple yet effective framework for pretrained probabilistic time series models. Chronos tokenizes time series values using scaling and quantization into a fixed vocabulary and trains existing transformer-based language model architectures on these tokenized time series via the cross-entropy loss. We pretrained Chronos models based on the T5 family (ranging from 20M to 710M parameters) on a large collection of publicly available datasets, complemented by a synthetic dataset that we generated via Gaussian processes to improve generalization. In a comprehensive benchmark consisting of 42 datasets, and comprising both classical local models and deep learning methods, we show that Chronos models: (a) significantly outperform other methods on datasets that were part of the training corpus; and (b) have comparable and occasionally superior zero-shot performance on new datasets, relative to methods that were trained spe
    
[^15]: LaB-GATr：大规模生物医学表面和体积网格的几何代数变换器

    LaB-GATr: geometric algebra transformers for large biomedical surface and volume meshes

    [https://arxiv.org/abs/2403.07536](https://arxiv.org/abs/2403.07536)

    LaB-GATr 是一种几何代数变换器神经网络，通过序列压缩和插值有效地学习大规模生物医学表面和体积网格，扩展了传统的 GATr 方法并尊重了欧几里得对称性，达到了最先进的结果。

    

    许多解剖结构可以用表面或体积网格来描述。机器学习是从这些3D模型中提取信息的一种有前途的工具。然而，高保真度的网格通常包含成千上万个顶点，这在构建深度神经网络架构时带来了独特的挑战。此外，患者特异性网格可能没有经典对齐，这限制了机器学习算法的泛化。我们提出了LaB-GATr，一种具有几何标记化的转换器神经网络，通过序列压缩和插值有效地学习大规模（生物）医学表面和体积网格。我们的方法扩展了最近提出的几何代数变换器（GATr），因此尊重所有欧几里得对称性，即旋转、平移和反射，有效地缓解了患者之间经典对齐的问题。

    arXiv:2403.07536v1 Announce Type: cross  Abstract: Many anatomical structures can be described by surface or volume meshes. Machine learning is a promising tool to extract information from these 3D models. However, high-fidelity meshes often contain hundreds of thousands of vertices, which creates unique challenges in building deep neural network architectures. Furthermore, patient-specific meshes may not be canonically aligned which limits the generalisation of machine learning algorithms. We propose LaB-GATr, a transfomer neural network with geometric tokenisation that can effectively learn with large-scale (bio-)medical surface and volume meshes through sequence compression and interpolation. Our method extends the recently proposed geometric algebra transformer (GATr) and thus respects all Euclidean symmetries, i.e. rotation, translation and reflection, effectively mitigating the problem of canonical alignment between patients. LaB-GATr achieves state-of-the-art results on three ta
    
[^16]: 校准多模态表示：在不使用注释的情况下追求群体鲁棒性

    Calibrating Multi-modal Representations: A Pursuit of Group Robustness without Annotations

    [https://arxiv.org/abs/2403.07241](https://arxiv.org/abs/2403.07241)

    本文旨在探索如何减少CLIP对伪特征的依赖，从而提高群体鲁棒性，而无需使用注释数据。

    

    arXiv:2403.07241v1 公告类型：交叉 摘要：微调预训练的视觉-语言模型，如CLIP，在多样的下游任务上取得成功。然而，这种范式存在一些痛点：(i) 直接微调整个预训练模型既时间密集又计算成本高。此外，这些调整后的模型往往变得高度专业化，限制了它们在实际部署中的实用性；(ii) 最近的研究表明，预训练的视觉-语言分类器可能过度依赖于伪特征-在训练数据中与目标相关的模式，但与真实标签函数无关；(iii) 现有关于减少对伪特征依赖的研究，主要基于我们能够识别这些特征的假设，对于实际应用并没有提供确切的保证。作为一项试点研究，本工作侧重于探索在不使用任何注释的情况下减少CLIP对伪特征依赖的方法。

    arXiv:2403.07241v1 Announce Type: cross  Abstract: Fine-tuning pre-trained vision-language models, like CLIP, has yielded success on diverse downstream tasks. However, several pain points persist for this paradigm: (i) directly tuning entire pre-trained models becomes both time-intensive and computationally costly. Additionally, these tuned models tend to become highly specialized, limiting their practicality for real-world deployment; (ii) recent studies indicate that pre-trained vision-language classifiers may overly depend on spurious features -- patterns that correlate with the target in training data, but are not related to the true labeling function; and (iii) existing studies on mitigating the reliance on spurious features, largely based on the assumption that we can identify such features, does not provide definitive assurance for real-world applications. As a piloting study, this work focuses on exploring mitigating the reliance on spurious features for CLIP without using any 
    
[^17]: ERBench：基于实体关系的可自动验证的大规模语言模型幻觉基准

    ERBench: An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Language Models

    [https://arxiv.org/abs/2403.05266](https://arxiv.org/abs/2403.05266)

    ERBench是一个基于实体关系的大型语言模型幻觉基准，通过自动转换任何关系数据库并构建可自动验证的问题，以支持复杂性评估和调试

    

    大型语言模型（LLMs）在各种应用中取得了前所未有的性能，然而它们的评估仍然是一个关键问题。现有的幻觉基准要么是静态的，要么缺乏可调整的复杂性进行彻底分析。我们认为利用现有的关系数据库是构建基准的一种有希望的方法，因为它们通过功能依赖关系可以准确描述知识。我们提出了ERBench，可以自动将任何关系数据库转换为基于实体关系（ER）模型的基准。我们的关键想法是使用数据库模式、记录和功能依赖来构建问题，以便可以自动验证。此外，我们使用外键约束来连接关系和构建多跳问题，这些问题可以任意复杂，用于调试LLMs的中间答案。最后，ERBench支持持续评估，多模态qu

    arXiv:2403.05266v1 Announce Type: cross  Abstract: Large language models (LLMs) have achieved unprecedented performance in various applications, yet their evaluation remains a critical issue. Existing hallucination benchmarks are either static or lack adjustable complexity for thorough analysis. We contend that utilizing existing relational databases is a promising approach for constructing benchmarks due to their accurate knowledge description via functional dependencies. We propose ERBench to automatically convert any relational database into a benchmark based on the entity-relationship (ER) model. Our key idea is to construct questions using the database schema, records, and functional dependencies such that they can be automatically verified. In addition, we use foreign key constraints to join relations and construct multihop questions, which can be arbitrarily complex and used to debug the intermediate answers of LLMs. Finally, ERBench supports continuous evaluation, multimodal qu
    
[^18]: 带有分摊上下文记忆的语言模型的在线适应

    Online Adaptation of Language Models with a Memory of Amortized Contexts

    [https://arxiv.org/abs/2403.04317](https://arxiv.org/abs/2403.04317)

    提出了一种带有分摊上下文记忆的在线适应框架，可有效地提取、压缩并存储信息以保持强大的知识保留能力

    

    由于信息的快速生成和传播，即使开发成本巨大，大型语言模型（LLMs）也很快过时。鉴于保持模型更新的重要性，当在现实世界应用LLMs时，在线学习已成为一项至关重要的需求。然而，鉴于不断扩大的未见文档语料库和现代LLMs的大参数空间，高效的适应至关重要。为了解决这些挑战，我们提出了Memory of Amortized Contexts（MAC），这是一个针对LLMs的高效且有效的在线适应框架，具有较强的知识保留能力。我们提出了一种摊销特征提取和记忆增强方法，将新文档中的信息压缩并提取为存储在记忆库中的紧凑调制。在回答问题时，我们的模型关注并从该记忆库中提取相关知识。为了有效地学习有信息量的调制…

    arXiv:2403.04317v1 Announce Type: cross  Abstract: Due to the rapid generation and dissemination of information, large language models (LLMs) quickly run out of date despite enormous development costs. Due to this crucial need to keep models updated, online learning has emerged as a critical necessity when utilizing LLMs for real-world applications. However, given the ever-expanding corpus of unseen documents and the large parameter space of modern LLMs, efficient adaptation is essential. To address these challenges, we propose Memory of Amortized Contexts (MAC), an efficient and effective online adaptation framework for LLMs with strong knowledge retention. We propose an amortized feature extraction and memory-augmentation approach to compress and extract information from new documents into compact modulations stored in a memory bank. When answering questions, our model attends to and extracts relevant knowledge from this memory bank. To learn informative modulations in an efficient m
    
[^19]: 深度配置性能学习：一项系统性调查与分类

    Deep Configuration Performance Learning: A Systematic Survey and Taxonomy

    [https://arxiv.org/abs/2403.03322](https://arxiv.org/abs/2403.03322)

    性能是可配置软件系统行为的关键属性，本文针对深度学习在可配置软件性能学习方面进行了全面的调查与分类研究。

    

    性能可以说是反映可配置软件系统行为的最关键属性。然而，随着现代软件规模和复杂性不断增加，对各种配置如何影响性能进行建模和预测成为软件维护中的主要挑战之一。因此，性能通常是在没有对软件系统有透彻了解的情况下建模的，主要依赖数据，这正好符合深度学习的目的。在这篇论文中，我们专注于深度学习在可配置软件性能学习方面进行了全面的回顾，涵盖了948篇来自六个索引服务的论文，基于此提取并分析了85篇主要论文。我们的结果总结了配置数据如何准备，深度配置性能学习模型如何构建，以及该模型如何进行评估等关键主题和统计信息。

    arXiv:2403.03322v1 Announce Type: cross  Abstract: Performance is arguably the most crucial attribute that reflects the behavior of a configurable software system. However, given the increasing scale and complexity of modern software, modeling and predicting how various configurations can impact performance becomes one of the major challenges in software maintenance. As such, performance is often modeled without having a thorough knowledge of the software system, but relying mainly on data, which fits precisely with the purpose of deep learning.   In this paper, we conduct a comprehensive review exclusively on the topic of deep learning for performance learning of configurable software, covering 948 searched papers spanning six indexing services, based on which 85 primary papers were extracted and analyzed. Our results summarize the key topics and statistics on how the configuration data is prepared; how the deep configuration performance learning model is built; how the model is evalu
    
[^20]: 混合策略纳什均衡用于人群导航

    Mixed-Strategy Nash Equilibrium for Crowd Navigation

    [https://arxiv.org/abs/2403.01537](https://arxiv.org/abs/2403.01537)

    通过简单的迭代贝叶斯更新方案和基于数据驱动的框架，我们证明了混合策略纳什均衡模型为人群导航提供了实时且可扩展的决策制定方法。

    

    我们解决了针对人群导航找到混合策略纳什均衡的问题。混合策略纳什均衡为机器人提供了一个严谨的模型，使其能够预测人群中不确定但合作的人类行为，但计算成本通常太高，无法进行可扩展和实时的决策制定。在这里，我们证明了一个简单的迭代贝叶斯更新方案收敛于混合策略社交导航游戏的纳什均衡。此外，我们提出了一个基于数据驱动的框架，通过将代理策略初始化为从人类数据集学习的高斯过程，来构建该游戏。基于所提出的混合策略纳什均衡模型，我们开发了一个基于采样的人群导航框架，可以集成到现有导航方法中，并可在笔记本电脑 CPU 上实时运行。我们通过模拟环境和真实世界的非结构化环境中人类数据集对我们的框架进行了评估。

    arXiv:2403.01537v1 Announce Type: cross  Abstract: We address the problem of finding mixed-strategy Nash equilibrium for crowd navigation. Mixed-strategy Nash equilibrium provides a rigorous model for the robot to anticipate uncertain yet cooperative human behavior in crowds, but the computation cost is often too high for scalable and real-time decision-making. Here we prove that a simple iterative Bayesian updating scheme converges to the Nash equilibrium of a mixed-strategy social navigation game. Furthermore, we propose a data-driven framework to construct the game by initializing agent strategies as Gaussian processes learned from human datasets. Based on the proposed mixed-strategy Nash equilibrium model, we develop a sampling-based crowd navigation framework that can be integrated into existing navigation methods and runs in real-time on a laptop CPU. We evaluate our framework in both simulated environments and real-world human datasets in unstructured environments. Our framework
    
[^21]: 大规模变分高斯状态空间模型

    Large-scale variational Gaussian state-space models

    [https://arxiv.org/abs/2403.01371](https://arxiv.org/abs/2403.01371)

    该论文介绍了一种针对具有高斯噪声驱动非线性动力学的状态空间模型的大规模变分算法和结构化逼近方法，可以有效评估ELBO和获取低方差的随机梯度估计，通过利用低秩蒙特卡罗逼近和推断网络的精度矩阵更新，将近似平滑问题转化为近似滤波问题。

    

    我们介绍了一种用于状态空间模型的嵌套变分推断算法和结构化变分逼近方法，其中非线性动力学由高斯噪声驱动。值得注意的是，所提出的框架允许在没有采用对角高斯逼近的情况下有效地评估ELBO和低方差随机梯度估计，通过利用（i）通过动力学对隐状态进行边缘化的蒙特卡罗逼近的低秩结构，（ii）一个推断网络，该网络通过低秩精度矩阵更新来近似更新步骤，（iii）将当前和未来观测编码为伪观测--将近似平滑问题转换为（更简单的）近似滤波问题。整体而言，必要的统计信息和ELBO可以在$O（TL（Sr+S^2+r^2））$时间内计算，其中$T$是系列长度，$L$是状态空间维数，$S$是用于逼近的样本数量。

    arXiv:2403.01371v1 Announce Type: cross  Abstract: We introduce an amortized variational inference algorithm and structured variational approximation for state-space models with nonlinear dynamics driven by Gaussian noise. Importantly, the proposed framework allows for efficient evaluation of the ELBO and low-variance stochastic gradient estimates without resorting to diagonal Gaussian approximations by exploiting (i) the low-rank structure of Monte-Carlo approximations to marginalize the latent state through the dynamics (ii) an inference network that approximates the update step with low-rank precision matrix updates (iii) encoding current and future observations into pseudo observations -- transforming the approximate smoothing problem into an (easier) approximate filtering problem. Overall, the necessary statistics and ELBO can be computed in $O(TL(Sr + S^2 + r^2))$ time where $T$ is the series length, $L$ is the state-space dimensionality, $S$ are the number of samples used to app
    
[^22]: TimeXer：利用外生变量增强变压器进行时间序列预测

    TimeXer: Empowering Transformers for Time Series Forecasting with Exogenous Variables

    [https://arxiv.org/abs/2402.19072](https://arxiv.org/abs/2402.19072)

    本文提出了一个新框架TimeXer，利用外部信息增强变压器对内生变量进行预测，弥补了以往多变量或单变量预测中忽视外生信息的不足。

    

    最近的研究表明，在时间序列预测方面取得了显著的性能。然而，由于现实应用的部分观测性质，仅专注于感兴趣的目标，也就是所谓的内生变量，通常是不足以保证准确预测的。值得注意的是，系统通常记录为多个变量，其中外生序列可以为内生变量提供有价值的外部信息。因此，与先前确立的多变量或单变量预测不同，它们要么将所有变量等同对待，要么忽视外生信息，本文关注的是一种实际设置，即具有外生变量的时间序列预测。我们提出了一个新颖的框架TimeXer，利用外部信息增强内生变量的预测。通过巧妙设计的嵌入层，TimeXer使传统的Transformer架构具有重新

    arXiv:2402.19072v1 Announce Type: cross  Abstract: Recent studies have demonstrated remarkable performance in time series forecasting. However, due to the partially-observed nature of real-world applications, solely focusing on the target of interest, so-called endogenous variables, is usually insufficient to guarantee accurate forecasting. Notably, a system is often recorded into multiple variables, where the exogenous series can provide valuable external information for endogenous variables. Thus, unlike prior well-established multivariate or univariate forecasting that either treats all the variables equally or overlooks exogenous information, this paper focuses on a practical setting, which is time series forecasting with exogenous variables. We propose a novel framework, TimeXer, to utilize external information to enhance the forecasting of endogenous variables. With a deftly designed embedding layer, TimeXer empowers the canonical Transformer architecture with the ability to reco
    
[^23]: 基于不确定性的离散异构数据孤岛中可拓展编码本的联邦学习

    Uncertainty-Based Extensible Codebook for Discrete Federated Learning in Heterogeneous Data Silos

    [https://arxiv.org/abs/2402.18888](https://arxiv.org/abs/2402.18888)

    提出了一种基于不确定性的可拓展编码本的联邦学习框架，用于应对异构数据孤岛中模型适应新分布的挑战

    

    旨在利用广泛分布的数据集进行联邦学习(FL)面临着一个关键挑战：不同孤岛间数据的异构性。我们发现从FL导出的模型在应用于具有陌生分布的数据孤岛时会表现出明显增加的不确定性。因此，我们提出了一种创新而简单的迭代框架，称为基于不确定性的可拓展编码本联邦学习(UEFL)。该框架动态地将潜在特征映射到可训练的离散向量，评估不确定性，并针对表现出高不确定性的孤岛特别地扩展离散化词典或编码本。

    arXiv:2402.18888v1 Announce Type: new  Abstract: Federated learning (FL), aimed at leveraging vast distributed datasets, confronts a crucial challenge: the heterogeneity of data across different silos. While previous studies have explored discrete representations to enhance model generalization across minor distributional shifts, these approaches often struggle to adapt to new data silos with significantly divergent distributions. In response, we have identified that models derived from FL exhibit markedly increased uncertainty when applied to data silos with unfamiliar distributions. Consequently, we propose an innovative yet straightforward iterative framework, termed Uncertainty-Based Extensible-Codebook Federated Learning (UEFL). This framework dynamically maps latent features to trainable discrete vectors, assesses the uncertainty, and specifically extends the discretization dictionary or codebook for silos exhibiting high uncertainty. Our approach aims to simultaneously enhance a
    
[^24]: ArEEG_Chars: 用于基于脑电图的设想语音识别的阿拉伯字符数据集

    ArEEG_Chars: Dataset for Envisioned Speech Recognition using EEG for Arabic Characters

    [https://arxiv.org/abs/2402.15733](https://arxiv.org/abs/2402.15733)

    该论文介绍了一种用于阿拉伯字符的EEG数据集ArEEG_Chars，通过深度学习实现97%的准确率，在脑机接口中具有重要意义。

    

    脑机接口（BCI）是近年来热门的研究课题，可以帮助瘫痪患者改善生活。有几项研究自动将脑电图（EEG）信号分类为英文字符和单词。阿拉伯语是世界上使用最广泛的语言之一。然而据我们所知，目前没有针对阿拉伯字符的脑电图信号数据集。在本文中，我们创建了一个用于阿拉伯字符的EEG数据集，并命名为ArEEG_Chars。此外，我们使用深度学习对ArEEG_Chars进行了多项实验。在使用LSTM时获得了最佳结果，准确率达到97%。ArEEG_Chars数据集将对研究人员公开。

    arXiv:2402.15733v1 Announce Type: cross  Abstract: Brain-Computer-Interface (BCI) has been a hot research topic in the last few years that could help paralyzed people in their lives. Several researches were done to classify electroencephalography (EEG) signals automatically into English characters and words. Arabic language is one of the most used languages around the world. However, to the best of our knowledge, there is no dataset for Arabic characters EEG signals. In this paper, we have created an EEG dataset for Arabic characters and named it ArEEG_Chars. Moreover, several experiments were done on ArEEG_Chars using deep learning. Best results were achieved using LSTM and reached an accuracy of 97%. ArEEG_Chars dataset will be public for researchers.
    
[^25]: ACE：具有因果感知熵正则化的离策略演员-评论家算法

    ACE : Off-Policy Actor-Critic with Causality-Aware Entropy Regularization

    [https://arxiv.org/abs/2402.14528](https://arxiv.org/abs/2402.14528)

    该论文提出了ACE算法，通过引入因果感知熵正则化，有效评估不同行为的重要性，并分析梯度休眠现象，引入休眠引导复位机制，在多个连续控制任务中取得显著性能优势。

    

    先前的无模型强化学习算法忽视了策略学习过程中不同原始行为的变化重要性。利用这一观点，我们探讨了不同动作维度和奖励之间的因果关系，以评估训练过程中各种原始行为的重要性。我们引入了一种因果感知熵项，有效地识别并优先处理具有高潜在影响的行动，以实现有效的探索。此外，为了防止对特定原始行为过度关注，我们分析了梯度休眠现象，并引入了一种休眠引导复位机制，进一步增强了我们的方法的功效。我们提出的算法ACE：具有因果感知熵正则化的离策演员-评论家，在跨7个领域的29个不同连续控制任务中，相较于无模型强化学习基线，表现出显著的性能优势。

    arXiv:2402.14528v1 Announce Type: cross  Abstract: The varying significance of distinct primitive behaviors during the policy learning process has been overlooked by prior model-free RL algorithms. Leveraging this insight, we explore the causal relationship between different action dimensions and rewards to evaluate the significance of various primitive behaviors during training. We introduce a causality-aware entropy term that effectively identifies and prioritizes actions with high potential impacts for efficient exploration. Furthermore, to prevent excessive focus on specific primitive behaviors, we analyze the gradient dormancy phenomenon and introduce a dormancy-guided reset mechanism to further enhance the efficacy of our method. Our proposed algorithm, ACE: Off-policy Actor-critic with Causality-aware Entropy regularization, demonstrates a substantial performance advantage across 29 diverse continuous control tasks spanning 7 domains compared to model-free RL baselines, which un
    
[^26]: 基于图提示学习的药物相互作用事件预测：DDIPrompt

    DDIPrompt: Drug-Drug Interaction Event Prediction based on Graph Prompt Learning

    [https://arxiv.org/abs/2402.11472](https://arxiv.org/abs/2402.11472)

    基于图提示学习的DDIPrompt框架旨在解决药物相互作用事件预测中的高度不平衡事件分布和罕见事件标记数据稀缺性问题。

    

    最近，由于其在建模药物分子内部和之间原子和功能团之间复杂关联方面的熟练表现，图神经网络在预测药物相互作用事件（DDI）方面变得日益普遍。然而，它们仍然受到两个重大挑战的制约：（1）高度不平衡事件分布的问题，在医学数据集中这是一个常见但关键的问题，某些相互作用被广泛地低估。这种不平衡对实现准确可靠的DDI预测构成了重大障碍。（2）罕见事件标记数据的稀缺性，在医学领域是一个普遍问题，由于数据有限，往往忽视或研究不足的罕见但潜在关键的相互作用。为此，我们提出了DDIPrompt，这是一种受最近图提示学进展启发的创新良方。我们的框架旨在解决这些问题。

    arXiv:2402.11472v1 Announce Type: cross  Abstract: Recently, Graph Neural Networks have become increasingly prevalent in predicting adverse drug-drug interactions (DDI) due to their proficiency in modeling the intricate associations between atoms and functional groups within and across drug molecules. However, they are still hindered by two significant challenges: (1) the issue of highly imbalanced event distribution, which is a common but critical problem in medical datasets where certain interactions are vastly underrepresented. This imbalance poses a substantial barrier to achieving accurate and reliable DDI predictions. (2) the scarcity of labeled data for rare events, which is a pervasive issue in the medical field where rare yet potentially critical interactions are often overlooked or under-studied due to limited available data. In response, we offer DDIPrompt, an innovative panacea inspired by the recent advancements in graph prompting. Our framework aims to address these issue
    
[^27]: 用稀疏线性概念嵌入（SpLiCE）解释CLIP

    Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE)

    [https://arxiv.org/abs/2402.10376](https://arxiv.org/abs/2402.10376)

    本研究提出了一种新方法，Sparse Linear Concept Embeddings（SpLiCE），通过将CLIP表示转换为人可解释概念的稀疏线性组合，实现了对CLIP嵌入的解释。

    

    CLIP嵌入在各种计算机视觉任务中表现出色，但这些高维稠密向量表示并不容易解释，限制了它们在需要透明度的下游应用中的实用性。本文经验性地展示了CLIP的潜在空间高度结构化，因此可以将CLIP表示分解为其潜在语义组件。我们利用这一理解提出了一种新方法，稀疏线性概念嵌入（SpLiCE），用于将CLIP表示转换为人可解释概念的稀疏线性组合。与先前的工作不同，SpLiCE不需要概念标签，并且可以后期应用。通过对多个真实世界数据集进行广泛实验，我们验证了SpLiCE输出的表示可以解释甚至取代传统的密集CLIP表示。

    arXiv:2402.10376v1 Announce Type: new  Abstract: CLIP embeddings have demonstrated remarkable performance across a wide range of computer vision tasks. However, these high-dimensional, dense vector representations are not easily interpretable, restricting their usefulness in downstream applications that require transparency. In this work, we empirically show that CLIP's latent space is highly structured, and consequently that CLIP representations can be decomposed into their underlying semantic components. We leverage this understanding to propose a novel method, Sparse Linear Concept Embeddings (SpLiCE), for transforming CLIP representations into sparse linear combinations of human-interpretable concepts. Distinct from previous work, SpLiCE does not require concept labels and can be applied post hoc. Through extensive experimentation with multiple real-world datasets, we validate that the representations output by SpLiCE can explain and even replace traditional dense CLIP representati
    
[^28]: OpenMathInstruct-1: 一个拥有180万个数学教学调优数据集

    OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset

    [https://arxiv.org/abs/2402.10176](https://arxiv.org/abs/2402.10176)

    OpenMathInstruct-1是一个包含180万个数学问题和解决方法对的数据集，通过合成开源LLM的代码解释器解决方案来构建，填补了目前开源LLM在数学技能方面与闭源LLM之间的差距。

    

    最近的研究表明，利用合成生成的数据集来训练大规模语言模型（LLM）具有巨大潜力，尤其是为了获得特定的技能。目前的大规模数学教学调优数据集，如MetaMathQA和MAmmoTH，是使用来自商业限制许可的闭源LLM的输出构建的。限制在这些数据生成流程中使用开源LLM的一个关键原因是目前最好的闭源LLM（如GPT-4）和最好的开源LLM之间在数学技能上存在很大差距。基于开源LLM的最近进展，我们提出了新颖的提示方式和一些强力缩放，构建了OpenMathInstruct-1，一个拥有180万个问题-解决方法对的数学教学调优数据集。该数据集是通过使用GSM8K和MATH这两个流行的数学推理基准的代码解释器解决方案进行合成构建的。

    arXiv:2402.10176v1 Announce Type: cross  Abstract: Recent work has shown the immense potential of synthetically generated datasets for training large language models (LLMs), especially for acquiring targeted skills. Current large-scale math instruction tuning datasets such as MetaMathQA (Yu et al., 2024) and MAmmoTH (Yue et al., 2024) are constructed using outputs from closed-source LLMs with commercially restrictive licenses. A key reason limiting the use of open-source LLMs in these data generation pipelines has been the wide gap between the mathematical skills of the best closed-source LLMs, such as GPT-4, and the best open-source LLMs. Building on the recent progress in open-source LLMs, our proposed prompting novelty, and some brute-force scaling, we construct OpenMathInstruct-1, a math instruction tuning dataset with 1.8M problem-solution pairs. The dataset is constructed by synthesizing code-interpreter solutions for GSM8K and MATH, two popular math reasoning benchmarks, using t
    
[^29]: HYPO：超球面离群分布泛化

    HYPO: Hyperspherical Out-of-Distribution Generalization

    [https://arxiv.org/abs/2402.07785](https://arxiv.org/abs/2402.07785)

    HYPO是一个在超球面空间中学习域不变表示的框架，通过内类变化和间类分离原则的引导，提高了离群泛化性能。

    

    离群（OOD）泛化对于在现实世界中部署的机器学习模型至关重要。然而，实现这一点可能从根本上具有挑战性，因为它需要学习跨不同领域或环境的不变特征的能力。在本文中，我们提出了一种新颖的框架HYPO（超球面OOD泛化），它能够证明在超球面空间中学习域不变表示。具体而言，我们的超球面学习算法是根据内类变化和间类分离原则进行引导的，确保来自同一类别的特征（跨不同训练领域）与其类别原型紧密对齐，而不同类别的原型之间则被最大化地分离。我们进一步提供了关于我们的原型学习目标如何改善OOD泛化界限的理论证明。通过对具有挑战性的OOD基准的大量实验，我们证明我们的方法优于竞争基准

    Out-of-distribution (OOD) generalization is critical for machine learning models deployed in the real world. However, achieving this can be fundamentally challenging, as it requires the ability to learn invariant features across different domains or environments. In this paper, we propose a novel framework HYPO (HYPerspherical OOD generalization) that provably learns domain-invariant representations in a hyperspherical space. In particular, our hyperspherical learning algorithm is guided by intra-class variation and inter-class separation principles -- ensuring that features from the same class (across different training domains) are closely aligned with their class prototypes, while different class prototypes are maximally separated. We further provide theoretical justifications on how our prototypical learning objective improves the OOD generalization bound. Through extensive experiments on challenging OOD benchmarks, we demonstrate that our approach outperforms competitive baselines
    
[^30]: 基于生成模型的近最小极大分布式强化学习算法

    Near-Minimax-Optimal Distributional Reinforcement Learning with a Generative Model

    [https://arxiv.org/abs/2402.07598](https://arxiv.org/abs/2402.07598)

    本论文提出了一种基于生成模型的近最小极大分布式强化学习算法，该算法在使用生成模型近似回报分布方面具有极小极大优势，解决了一个开放问题，并提供了实验研究结果。

    

    我们提出了一种新的基于模型的分布式强化学习算法，并证明了在使用生成模型近似回报分布方面，它是近似最小极大的（在对数因子上），从而解决了Zhang等人（2023）的一个开放问题。我们的分析为分布式强化学习中的分类方法提供了新的理论结果，并引入了一种新的分布式Bellman方程，即随机分类累积分布函数Bellman方程，我们认为这个方程也具有独立的研究意义。我们还进行了实验研究，比较了几种基于模型的分布式强化学习算法，并得出了对实践者有意义的几个结论。

    We propose a new algorithm for model-based distributional reinforcement learning (RL), and prove that it is minimax-optimal for approximating return distributions with a generative model (up to logarithmic factors), resolving an open question of Zhang et al. (2023). Our analysis provides new theoretical results on categorical approaches to distributional RL, and also introduces a new distributional Bellman equation, the stochastic categorical CDF Bellman equation, which we expect to be of independent interest. We also provide an experimental study comparing several model-based distributional RL algorithms, with several takeaways for practitioners.
    
[^31]: 机器中的私语：LLM集成系统中的保密性

    Whispers in the Machine: Confidentiality in LLM-integrated Systems

    [https://arxiv.org/abs/2402.06922](https://arxiv.org/abs/2402.06922)

    本研究提供了一种评估LLM集成系统保密性的系统化方法，通过形式化一个"秘密密钥"游戏来捕捉模型隐藏私人信息的能力。评估了八种攻击和四种防御方法，发现当前的防御方法缺乏泛化性能。

    

    大规模语言模型（LLM）越来越多地与外部工具集成。尽管这些集成可以显著提高LLM的功能，但它们也在不同组件之间创建了一个新的攻击面，可能泄露机密数据。具体而言，恶意工具可以利用LLM本身的漏洞来操纵模型并损害其他服务的数据，这引发了在LLM集成环境中如何保护私密数据的问题。在这项工作中，我们提供了一种系统评估LLM集成系统保密性的方法。为此，我们形式化了一个"秘密密钥"游戏，可以捕捉模型隐藏私人信息的能力。这使我们能够比较模型对保密性攻击的脆弱性以及不同防御策略的有效性。在这个框架中，我们评估了八种先前发表的攻击和四种防御方法。我们发现当前的防御方法缺乏泛化性能。

    Large Language Models (LLMs) are increasingly integrated with external tools. While these integrations can significantly improve the functionality of LLMs, they also create a new attack surface where confidential data may be disclosed between different components. Specifically, malicious tools can exploit vulnerabilities in the LLM itself to manipulate the model and compromise the data of other services, raising the question of how private data can be protected in the context of LLM integrations.   In this work, we provide a systematic way of evaluating confidentiality in LLM-integrated systems. For this, we formalize a "secret key" game that can capture the ability of a model to conceal private information. This enables us to compare the vulnerability of a model against confidentiality attacks and also the effectiveness of different defense strategies. In this framework, we evaluate eight previously published attacks and four defenses. We find that current defenses lack generalization
    
[^32]: 提高非凸分布式优化在函数相似性下的最坏情况双向通信复杂性

    Improving the Worst-Case Bidirectional Communication Complexity for Nonconvex Distributed Optimization under Function Similarity

    [https://arxiv.org/abs/2402.06412](https://arxiv.org/abs/2402.06412)

    本文提出了MARINA-P方法，通过引入一系列相关压缩器，优化了服务器到工作节点的通信复杂度。理论分析证明，MARINA-P在算法上优于现有方法，并可以作为支持双向压缩的起点。通过与上行压缩和动量步骤的结合，M3方法实现了双向压缩，并在总通信复杂度上改进。

    

    服务器和工作节点之间的有效通信在分布式优化中起着关键作用。本文主要关注优化服务器到工作节点的通信，并揭示了当前流行的下行压缩方法中的低效性。首先考虑上行通信成本可忽略的纯粹情况下，我们引入MARINA-P，一种使用一系列相关压缩器的新型下行压缩方法。理论分析证明，使用排列压缩器的MARINA-P可以实现服务器到工作节点的通信复杂度随工作节点数量提高，因此在算法上可证明优于现有算法。我们进一步展示了MARINA-P可以作为支持双向压缩的方法的起点。我们介绍了M3，这是一种将MARINA-P与上行压缩和动量步骤组合的方法，能够实现双向压缩，并在总通信复杂度上证明了改进。

    Effective communication between the server and workers plays a key role in distributed optimization. In this paper, we focus on optimizing the server-to-worker communication, uncovering inefficiencies in prevalent downlink compression approaches. Considering first the pure setup where the uplink communication costs are negligible, we introduce MARINA-P, a novel method for downlink compression, employing a collection of correlated compressors. Theoretical analyses demonstrates that MARINA-P with permutation compressors can achieve a server-to-worker communication complexity improving with the number of workers, thus being provably superior to existing algorithms. We further show that MARINA-P can serve as a starting point for extensions such as methods supporting bidirectional compression. We introduce M3, a method combining MARINA-P with uplink compression and a momentum step, achieving bidirectional compression with provable improvements in total communication complexity as the number
    
[^33]: Shadowheart SGD: 在任意的计算和通信异构性下具有最优时间复杂度的分布式异步SGD

    Shadowheart SGD: Distributed Asynchronous SGD with Optimal Time Complexity Under Arbitrary Computation and Communication Heterogeneity

    [https://arxiv.org/abs/2402.04785](https://arxiv.org/abs/2402.04785)

    Shadowheart SGD是一种新的分布式异步SGD方法，利用无偏压缩技术，在任意的计算和通信异构性下具有最优时间复杂度，并显著优化了先前的集中式方法。同时，我们还开发了对应的双向设置方法。

    

    在异步集中式分布式设置中考虑非凸随机优化问题，其中工作者到服务器的通信时间不能忽略，而所有工作者的计算和通信时间可能不同。利用无偏压缩技术，我们开发了一种新的方法-Shadowheart SGD，它可证明优化了所有先前集中式方法的时间复杂度。此外，我们还展示了Shadowheart SGD在压缩通信的集中式方法族中的时间复杂度是最优的。我们还考虑了双向设置，在这种设置下，从服务器到工作者的广播不可忽略，并开发了相应的方法。

    We consider nonconvex stochastic optimization problems in the asynchronous centralized distributed setup where the communication times from workers to a server can not be ignored, and the computation and communication times are potentially different for all workers. Using an unbiassed compression technique, we develop a new method-Shadowheart SGD-that provably improves the time complexities of all previous centralized methods. Moreover, we show that the time complexity of Shadowheart SGD is optimal in the family of centralized methods with compressed communication. We also consider the bidirectional setup, where broadcasting from the server to the workers is non-negligible, and develop a corresponding method.
    
[^34]: 在黎曼流形上进行双层优化的框架

    A Framework for Bilevel Optimization on Riemannian Manifolds

    [https://arxiv.org/abs/2402.03883](https://arxiv.org/abs/2402.03883)

    本论文提出了一个在黎曼流形上解决约束双层优化问题的框架，并提供了多种超梯度估计策略，并对其进行了研究。该框架不仅适用于确定性双层优化问题，还适用于随机双层优化问题，并且可以使用一般的回退。在各种应用中，该框架都具有很高的实用性。

    

    双层优化在各个领域的应用中越来越常见。在这项工作中，我们提出了一个在黎曼流形上约束双层优化问题变量的框架。我们提供了几种在流形上的超梯度估计策略，并研究了它们的估计误差。我们对流形上的超梯度下降算法提供了收敛性和复杂性分析。我们还将这些研究扩展到随机双层优化和使用一般的回退。我们展示了该框架在各种应用中的实用性。

    Bilevel optimization has seen an increasing presence in various domains of applications. In this work, we propose a framework for solving bilevel optimization problems where variables of both lower and upper level problems are constrained on Riemannian manifolds. We provide several hypergradient estimation strategies on manifolds and study their estimation error. We provide convergence and complexity analysis for the proposed hypergradient descent algorithm on manifolds. We also extend the developments to stochastic bilevel optimization and to the use of general retraction. We showcase the utility of the proposed framework on various applications.
    
[^35]: 用于约束满足的投影式生成扩散模型

    Projected Generative Diffusion Models for Constraint Satisfaction

    [https://arxiv.org/abs/2402.03559](https://arxiv.org/abs/2402.03559)

    本文介绍了一种名为投影式生成扩散模型（PGDM）的方法，它能够通过约束优化问题将生成扩散模型适用于对特定条件有严格要求的场景。该方法通过迭代投影方法确保生成的数据符合指定的约束或物理原理。实验证明PGDM在复杂的约束和常微分方程的情况下也能合成出符合要求的输出。

    

    生成扩散模型通过一个顺序过程，能够从原始噪声中合成出连贯的内容。然而，在需要输出符合特定严格条件的场景中直接应用这些模型面临着严重的挑战。本文旨在克服这些挑战，并介绍了投影式生成扩散模型（PGDM），该方法将传统的扩散模型采样重新构建为一个约束优化问题。这使得可以应用迭代投影方法，以确保生成的数据忠实地遵循指定的约束或物理原理。本文在受限制的约束类别下，对PGDM能够从可行子分布中合成输出的能力提供了理论支持，并在复杂的非凸约束和常微分方程的案例中提供了大量的经验证据。这些能力通过在视频生成中体现了具有物理学信息的动态。

    Generative diffusion models excel at robustly synthesizing coherent content from raw noise through a sequential process. However, their direct application in scenarios requiring outputs to adhere to specific, stringent criteria faces several severe challenges. This paper aims at overcome these challenges and introduces Projected Generative Diffusion Models (PGDM), an approach that recast traditional diffusion models sampling into a constrained-optimization problem. This enables the application of an iterative projections method to ensure that generated data faithfully adheres to specified constraints or physical principles. This paper provides theoretical support for the ability of PGDM to synthesize outputs from a feasible subdistribution under a restricted class of constraints while also providing large empirical evidence in the case of complex non-convex constraints and ordinary differential equations. These capabilities are demonstrated by physics-informed motion in video generatio
    
[^36]: DeSparsify：对视觉Transformer中的Token稀疏化机制进行的对抗攻击

    DeSparsify: Adversarial Attack Against Token Sparsification Mechanisms in Vision Transformers

    [https://arxiv.org/abs/2402.02554](https://arxiv.org/abs/2402.02554)

    本文提出了一种对抗攻击方法DeSparsify，针对使用Token稀疏化机制的视觉Transformer，通过精心制作的对抗样本欺骗稀疏化机制，导致最坏情况的性能，以此耗尽操作系统的资源并保持隐蔽性。

    

    视觉Transformer在计算机视觉领域做出了巨大贡献，展现出在各种任务（如图像分类、目标检测）中的最先进性能。然而，它们的高计算要求随使用的Token数量呈二次增长。为解决这个问题，提出了Token稀疏化技术。这些技术采用了一种依赖输入的策略，将无关的Token从计算流程中丢弃，提高模型的效率。然而，它们的动态性和平均情况假设使它们容易受到一种新的威胁 - 经过精心制作的对抗样本，能够欺骗稀疏化机制，导致最坏情况的性能。在本文中，我们提出了一种攻击方法DeSparsify，针对使用Token稀疏化机制的视觉Transformer的可用性。该攻击旨在耗尽操作系统的资源，同时保持隐蔽性。

    Vision transformers have contributed greatly to advancements in the computer vision domain, demonstrating state-of-the-art performance in diverse tasks (e.g., image classification, object detection). However, their high computational requirements grow quadratically with the number of tokens used. Token sparsification techniques have been proposed to address this issue. These techniques employ an input-dependent strategy, in which uninformative tokens are discarded from the computation pipeline, improving the model's efficiency. However, their dynamism and average-case assumption makes them vulnerable to a new threat vector - carefully crafted adversarial examples capable of fooling the sparsification mechanism, resulting in worst-case performance. In this paper, we present DeSparsify, an attack targeting the availability of vision transformers that use token sparsification mechanisms. The attack aims to exhaust the operating system's resources, while maintaining its stealthiness. Our e
    
[^37]: EuLagNet: 拉格朗日动力学的欧拉预测

    EuLagNet: Eulerian Fluid Prediction with Lagrangian Dynamics

    [https://arxiv.org/abs/2402.02425](https://arxiv.org/abs/2402.02425)

    EuLagNet提出了一种新的拉格朗日引导范式，通过跟踪多尺度关键粒子的运动来捕捉多尺度流体动力学。这种方法克服了由于欧拉观察而导致的流体动力学困难，为准确预测未来的流体提供了一种有效的方法。

    

    准确预测未来的流体对气象学、海洋学和空气动力学等广泛领域至关重要。然而，由于流体通常从欧拉角度观察，其活跃和复杂的动力学在静止的网格中严重被掩盖和混淆，给预测带来了巨大挑战。本文引入了一种新的拉格朗日引导范式来解决复杂的流体动力学。我们提出了以拉格朗日动力学为导向的欧拉-拉格朗日双重递归网络（EuLagNet），通过跟踪自适应采样的多尺度关键粒子的运动并随时间积累动力学信息来捕捉多尺度流体动力学。具体地，我们提出了一个EuLag块，用于在每个时刻和尺度上传递学习到的欧拉和拉格朗日特征，其中跟踪粒子的运动是从欧拉观察中推断出来的，它们积累的动力学信息被纳入到预测模型中。

    Accurately predicting the future fluid is important to extensive areas, such as meteorology, oceanology and aerodynamics. However, since the fluid is usually observed from an Eulerian perspective, its active and intricate dynamics are seriously obscured and confounded in static grids, bringing horny challenges to the prediction. This paper introduces a new Lagrangian-guided paradigm to tackle the tanglesome fluid dynamics. Instead of solely predicting the future based on Eulerian observations, we propose the Eulerian-Lagrangian Dual Recurrent Network (EuLagNet), which captures multiscale fluid dynamics by tracking movements of adaptively sampled key particles on multiple scales and integrating dynamics information over time. Concretely, a EuLag Block is presented to communicate the learned Eulerian and Lagrangian features at each moment and scale, where the motion of tracked particles is inferred from Eulerian observations and their accumulated dynamics information is incorporated into
    
[^38]: Aligner: 通过弱到强校正实现高效对齐

    Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction

    [https://arxiv.org/abs/2402.02416](https://arxiv.org/abs/2402.02416)

    Aligner是一种通过学习校正残差来实现高效对齐的方法，相比于传统的强化学习方法，Aligner具有参数高效、弱到强泛化以及即插即用的优势。

    

    对于大型语言模型（LLMs），通过强化学习来进行对齐的努力主要是通过人类反馈的强化学习方法进行的。然而，强化学习面临着主要的挑战，包括训练奖励模型、演员-评论家工程以及重要的是，需要访问LLM参数。在这里，我们介绍了一种新的高效对齐范式Aligner，它通过学习对齐和未对齐答案之间的校正残差来绕过整个强化学习过程。我们的Aligner具有几个关键优势。首先，它是一个基于自监督学习的自动回归seq2seq模型，通过训练查询-答案-校正数据集，提供了一种参数高效的对齐解决方案，并且对资源需求较少。其次，Aligner实现了从弱到强的泛化；通过Aligner的监督信号来微调大型预训练模型，可以显著提升性能。第三，Aligner作为一个模型不可知的即插即用模块，可以直接应用于…

    Efforts to align Large Language Models (LLMs) are mainly conducted via Reinforcement Learning from Human Feedback (RLHF) methods. However, RLHF encounters major challenges including training reward models, actor-critic engineering, and importantly, it requires access to LLM parameters. Here we introduce Aligner, a new efficient alignment paradigm that bypasses the whole RLHF process by learning the correctional residuals between the aligned and the unaligned answers. Our Aligner offers several key advantages. Firstly, it is an autoregressive seq2seq model that is trained on the query-answer-correction dataset via supervised learning; this offers a parameter-efficient alignment solution with minimal resources. Secondly, the Aligner facilitates weak-to-strong generalization; finetuning large pretrained models by Aligner's supervisory signals demonstrates strong performance boost. Thirdly, Aligner functions as a model-agnostic plug-and-play module, allowing for its direct application on d
    
[^39]: 针对增强学习代理的个性化路径补救方法

    Personalized Path Recourse for Reinforcement Learning Agents

    [https://arxiv.org/abs/2312.08724](https://arxiv.org/abs/2312.08724)

    该论文介绍了一种针对增强学习代理的个性化路径补救方法，该方法通过编辑动作路径来实现期望目标，同时保持与代理的原始路径相似度高，并且个性化适应代理的行为模式。这种方法适用于纠正或改进动作或数据序列以实现预定目标。

    

    这篇论文介绍了一种名为个性化路径补救的新方法，用于为增强学习代理生成补救路径。其目标是通过编辑给定的动作路径以达到期望的目标（例如，与代理的原始路径相比取得更好的结果），同时确保与代理的原始路径高度相似并个性化适应代理。个性化是指新路径在从策略函数中观察到的代理行为模式方面的定制程度。我们训练一个个性化的补救代理来生成这样的个性化路径，这些路径是使用考虑目标、相似性和个性化的奖励函数获得的。该方法适用于增强学习和监督学习设置，以纠正或改进动作序列或数据序列以达到预定的目标。该方法在不同的设置中进行了评估。实验证明

    arXiv:2312.08724v2 Announce Type: replace-cross  Abstract: This paper introduces Personalized Path Recourse, a novel method that generates recourse paths for a reinforcement learning agent. The goal is to edit a given path of actions to achieve desired goals (e.g., better outcomes compared to the agent's original path) while ensuring a high similarity to the agent's original paths and being personalized to the agent. Personalization refers to the extent to which the new path is tailored to the agent's observed behavior patterns from their policy function. We train a personalized recourse agent to generate such personalized paths, which are obtained using reward functions that consider the goal, similarity, and personalization. The proposed method is applicable to both reinforcement learning and supervised learning settings for correcting or improving sequences of actions or sequences of data to achieve a pre-determined goal. The method is evaluated in various settings. Experiments show
    
[^40]: 可扩展和快速模拟推断的一致性模型

    Consistency Models for Scalable and Fast Simulation-Based Inference

    [https://arxiv.org/abs/2312.05440](https://arxiv.org/abs/2312.05440)

    提出了一种新的神经后验估计的一致性模型，结合了标准化流和流匹配方法的优点，用于可扩展、快速和摊销推断，在多个实验中展示出优越性能。

    

    仿真推断（SBI）不断寻找更具表现力的算法，以准确推断复杂模型的参数从嘈杂数据中。我们提出了神经后验估计的一致性模型（CMPE），这是一个用于可扩展、快速和摊销推断的新自由形式条件采样器，利用生成性神经网络。CMPE将标准化流和流匹配方法的优点结合到单个生成架构中：它本质上提炼了连续概率流，并能够利用无约束的结构快速进行少射推断，该结构可以定制到估计问题的结构。我们的实证评估表明，CMPE不仅在三个困难的低维问题上优于当前的最先进算法，而且在高维贝叶斯去噪实验和估计计算密集型多尺度中表现出有竞争力的性能。

    arXiv:2312.05440v2 Announce Type: replace-cross  Abstract: Simulation-based inference (SBI) is constantly in search of more expressive algorithms for accurately inferring the parameters of complex models from noisy data. We present consistency models for neural posterior estimation (CMPE), a new free-form conditional sampler for scalable, fast, and amortized SBI with generative neural networks. CMPE combines the advantages of normalizing flows and flow matching methods into a single generative architecture: It essentially distills a continuous probability flow and enables rapid few-shot inference with an unconstrained architecture that can be tailored to the structure of the estimation problem. Our empirical evaluation demonstrates that CMPE not only outperforms current state-of-the-art algorithms on three hard low-dimensional problems but also achieves competitive performance in a high-dimensional Bayesian denoising experiment and in estimating a computationally demanding multi-scale 
    
[^41]: LayerCollapse: 自适应压缩神经网络

    LayerCollapse: Adaptive compression of neural networks

    [https://arxiv.org/abs/2311.17943](https://arxiv.org/abs/2311.17943)

    LayerCollapse是一种自适应压缩神经网络的方法，通过结构化剪枝来减少全连接层的深度，而不需要进行微调，并且对性能影响有限。该方法通过正则化激活函数的线性度来控制模型的表达能力。

    

    处理当代深度学习和基于transformer的模型不断增长的规模是一个重大挑战。超参数化的Transformer网络在自然语言处理和计算机视觉方面的业绩超过了先前的技术。这些模型含有数亿个参数，需要大量的计算资源，并容易过拟合。在这项工作中，我们提出了LayerCollapse，一种结构化剪枝的形式，用于减少全连接层的深度。我们开发了一种新的正则化项，允许在不进行微调的情况下进行训练后压缩，并对性能产生有限的影响。LayerCollapse通过对全连接层之间的激活进行正则化，调节激活函数的线性度来控制模型的表达能力。线性激活函数将线性转换的秩降低到相应线性转换的秩。我们通过展示LayerCollapse的压缩能力来证明其有效性。

    Handling the ever-increasing scale of contemporary deep learning and transformer-based models poses a significant challenge. Overparameterized Transformer networks outperform prior art in Natural Language processing and Computer Vision. These models contain hundreds of millions of parameters, demanding significant computational resources and making them prone to overfitting. In this work we present LayerCollapse, a form of structured pruning to reduce the depth of fully connected layers. We develop a novel regularizer allowing for post-training compression without finetuning, while having limited impact on performance. LayerCollapse controls model expressiveness with regularization on the activations between fully connected layers, modulating the linearity of activation functions. A linear activation function reduces the rank of the transformation to the rank of the corresponding linear transformation. We demonstrate the effectiveness of LayerCollapse by showing its compression capabil
    
[^42]: 多模拟推理的深度融合：深度融合用于多模态模拟推理

    Fuse It or Lose It: Deep Fusion for Multimodal Simulation-Based Inference

    [https://arxiv.org/abs/2311.10671](https://arxiv.org/abs/2311.10671)

    提出了多模态神经后验估计 (MultiNPE) 方法，利用深度融合学习整合不同来源的异构数据，在模拟推理中提高了对复杂数学模型参数的准确推断能力。

    

    我们提出多模态神经后验估计(MultiNPE)，这是一种利用神经网络在模拟推理中整合来自不同来源的异构数据的方法。受深度融合学习的进展启发，它赋予研究人员分析来自不同领域的数据并推断复杂数学模型参数的能力，提高了准确性。我们针对MultiNPE制定了多模态融合方法（早期、后期、混合），并在三项具有挑战性的实验中评估它们的性能。MultiNPE不仅在参考任务上优于单一数据源基线，还在神经科学和心脏病学的科学模型推理上取得了卓越成绩。我们系统地研究了部分缺失数据对不同融合策略的影响。在我们的实验中，后期和混合融合技术成为多模态模拟推理实际应用的首选方法。

    arXiv:2311.10671v2 Announce Type: replace-cross  Abstract: We present multimodal neural posterior estimation (MultiNPE), a method to integrate heterogeneous data from different sources in simulation-based inference with neural networks. Inspired by advances in deep fusion learning, it empowers researchers to analyze data from different domains and infer the parameters of complex mathematical models with increased accuracy. We formulate multimodal fusion approaches for \hbox{MultiNPE} (early, late, hybrid) and evaluate their performance in three challenging experiments. MultiNPE not only outperforms single-source baselines on a reference task, but also achieves superior inference on scientific models from neuroscience and cardiology. We systematically investigate the impact of partially missing data on the different fusion strategies. Across our experiments, late and hybrid fusion techniques emerge as the methods of choice for practical applications of multimodal simulation-based infere
    
[^43]: 通过多样性启示的多Agent诊断方法用于稳健性

    Multi-Agent Diagnostics for Robustness via Illuminated Diversity. (arXiv:2401.13460v1 [cs.LG])

    [http://arxiv.org/abs/2401.13460](http://arxiv.org/abs/2401.13460)

    MADRID是一种新方法，通过生成多样化的对抗场景来揭示预训练多Agent策略的战略漏洞，并通过遗憾值衡量漏洞的程度。

    

    在快速发展的多Agent系统领域中，确保在陌生和敌对环境中的稳健性至关重要。尽管这些系统在熟悉环境中表现出色，但在新情况下往往会因为训练阶段的过拟合而失败。在既包含合作又包含竞争行为的环境中，这一问题尤为突出，体现了过拟合和泛化挑战的双重性质。为了解决这个问题，我们提出了通过多样性启示的多Agent稳健性诊断（MADRID），这是一种生成多Agent策略中暴露战略漏洞的多样化对抗场景的新方法。MADRID利用开放式学习的概念，导航对抗环境的广阔空间，使用目标策略的遗憾值来衡量这些环境的漏洞。我们在11vs11版的Google Research Football上评估了MADRID的有效性。

    In the rapidly advancing field of multi-agent systems, ensuring robustness in unfamiliar and adversarial settings is crucial. Notwithstanding their outstanding performance in familiar environments, these systems often falter in new situations due to overfitting during the training phase. This is especially pronounced in settings where both cooperative and competitive behaviours are present, encapsulating a dual nature of overfitting and generalisation challenges. To address this issue, we present Multi-Agent Diagnostics for Robustness via Illuminated Diversity (MADRID), a novel approach for generating diverse adversarial scenarios that expose strategic vulnerabilities in pre-trained multi-agent policies. Leveraging the concepts from open-ended learning, MADRID navigates the vast space of adversarial settings, employing a target policy's regret to gauge the vulnerabilities of these settings. We evaluate the effectiveness of MADRID on the 11vs11 version of Google Research Football, one o
    
[^44]: DISTINQT: 一种面向未来移动和无线网络的分布式隐私感知学习框架，用于QoS预测

    DISTINQT: A Distributed Privacy Aware Learning Framework for QoS Prediction for Future Mobile and Wireless Networks. (arXiv:2401.10158v1 [cs.NI])

    [http://arxiv.org/abs/2401.10158](http://arxiv.org/abs/2401.10158)

    DISTINQT是一种面向未来移动和无线网络的隐私感知分布式学习框架，用于QoS预测。

    

    5G和6G以后的网络将支持依赖一定服务质量（QoS）的新的和具有挑战性的用例和应用程序。及时预测QoS对于安全关键应用（如车辆通信）尤为重要。尽管直到最近，QoS预测一直由集中式人工智能（AI）解决方案完成，但已经出现了一些隐私、计算和运营方面的问题。替代方案已经出现（如分割学习、联邦学习），将复杂度较低的AI任务分布在节点之间，同时保护数据隐私。然而，考虑到未来无线网络的异构性，当涉及可扩展的分布式学习方法时，会出现新的挑战。该研究提出了一种名为DISTINQT的面向QoS预测的隐私感知分布式学习框架。

    Beyond 5G and 6G networks are expected to support new and challenging use cases and applications that depend on a certain level of Quality of Service (QoS) to operate smoothly. Predicting the QoS in a timely manner is of high importance, especially for safety-critical applications as in the case of vehicular communications. Although until recent years the QoS prediction has been carried out by centralized Artificial Intelligence (AI) solutions, a number of privacy, computational, and operational concerns have emerged. Alternative solutions have been surfaced (e.g. Split Learning, Federated Learning), distributing AI tasks of reduced complexity across nodes, while preserving the privacy of the data. However, new challenges rise when it comes to scalable distributed learning approaches, taking into account the heterogeneous nature of future wireless networks. The current work proposes DISTINQT, a privacy-aware distributed learning framework for QoS prediction. Our framework supports mult
    
[^45]: 在复杂安全约束和有限执行能力下学习以性能为导向的控制障碍函数

    Learning Performance-Oriented Control Barrier Functions Under Complex Safety Constraints and Limited Actuation. (arXiv:2401.05629v1 [cs.LG])

    [http://arxiv.org/abs/2401.05629](http://arxiv.org/abs/2401.05629)

    本研究提出了一个新颖的自监督学习框架，通过构建可导函数来近似安全集合，并使用神经网络参数化控制障碍函数，以解决在复杂安全约束和有限执行能力下寻找最优CBF的挑战。

    

    控制障碍函数（CBFs）提供了一个优雅的框架，通过将非线性控制系统的轨迹约束在预定义安全集合的不变子集上，设计安全过滤器。然而，找到一个同时在最大化控制不变集体积和适应复杂安全约束方面具有挑战性的CBF，尤其是在具有执行约束的高相对度的系统中，仍然是一个问题。在这项工作中，我们提出了一个新颖的自监督学习框架，全面解决了这些障碍。给定定义安全集合的多个状态约束的布尔组合，我们的方法从构建一个单一的可导函数开始，其0超级级别集合提供了安全集合的内部近似。然后，我们使用这个函数以及一个平滑的神经网络来参数化CBF候选。最后，我们设计了基于哈密顿-雅可比的训练损失函数。

    Control Barrier Functions (CBFs) provide an elegant framework for designing safety filters for nonlinear control systems by constraining their trajectories to an invariant subset of a prespecified safe set. However, the task of finding a CBF that concurrently maximizes the volume of the resulting control invariant set while accommodating complex safety constraints, particularly in high relative degree systems with actuation constraints, continues to pose a substantial challenge. In this work, we propose a novel self-supervised learning framework that holistically addresses these hurdles. Given a Boolean composition of multiple state constraints that define the safe set, our approach starts with building a single continuously differentiable function whose 0-superlevel set provides an inner approximation of the safe set. We then use this function together with a smooth neural network to parameterize the CBF candidate. Finally, we design a training loss function based on a Hamilton-Jacobi
    
[^46]: H2G2-Net:一种用于多模态生理反应发现的分层异构图生成网络框架

    H2G2-Net: A Hierarchical Heterogeneous Graph Generative Network Framework for Discovery of Multi-Modal Physiological Responses. (arXiv:2401.02905v1 [cs.LG])

    [http://arxiv.org/abs/2401.02905](http://arxiv.org/abs/2401.02905)

    H2G2-Net是一种用于发现多模态生理反应的分层异构图生成网络框架，能够自动学习图结构而不需要预定义的领域知识。

    

    在各种研究应用中，利用多模态生理信号来发现人类认知和情感状态引起了人们的关注。人体的生理反应受到人类认知的影响，常用于分析认知状态。从网络科学的角度来看，这些异构生理模式在图结构中的互动可能提供有益的信息来支持认知状态的预测。然而，目前没有办法得到异构模态之间的精确连接，并且存在一种分层结构的子模态。现有的图神经网络设计用于在预定义的图结构上学习非层次化的同质图，无法从层次化的多模态生理数据中学习，没有预定义的图结构。为此，我们提出了一种分层异构图生成网络（H2G2-Net），能够自动学习图结构而不需要先验领域知识。

    Discovering human cognitive and emotional states using multi-modal physiological signals draws attention across various research applications. Physiological responses of the human body are influenced by human cognition and commonly used to analyze cognitive states. From a network science perspective, the interactions of these heterogeneous physiological modalities in a graph structure may provide insightful information to support prediction of cognitive states. However, there is no clue to derive exact connectivity between heterogeneous modalities and there exists a hierarchical structure of sub-modalities. Existing graph neural networks are designed to learn on non-hierarchical homogeneous graphs with pre-defined graph structures; they failed to learn from hierarchical, multi-modal physiological data without a pre-defined graph structure. To this end, we propose a hierarchical heterogeneous graph generative network (H2G2-Net) that automatically learns a graph structure without domain 
    
[^47]: SASSL:通过神经风格迁移增强自监督学习

    SASSL: Enhancing Self-Supervised Learning via Neural Style Transfer. (arXiv:2312.01187v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2312.01187](http://arxiv.org/abs/2312.01187)

    SASSL提出了一种基于神经风格迁移的增强技术，通过解耦语义和风格属性，在自监督学习中生成多样化的增强样本，从而提升了图像分类性能。

    

    自监督学习依赖于数据增强来从无标签图像中提取有意义的表征。现有的最先进的增强流水线包括了各种原始的转换，但通常忽略了自然图像的结构。因此，增强样本可能显示出退化的语义信息和低风格多样性，从而影响到自监督表征的下游性能。为了克服这个问题，我们提出了一种名为SASSL的新型增强技术，它基于神经风格迁移。该方法将图像中的语义和风格属性解耦，并仅对风格应用转换，保持内容，生成多样化的增强样本，更好地保留它们的语义属性。实验结果显示，与广为接受的MoCo v2相比，我们的技术在ImageNet上的top-1分类性能提升超过2%。

    Self-supervised learning relies heavily on data augmentation to extract meaningful representations from unlabeled images. While existing state-of-the-art augmentation pipelines incorporate a wide range of primitive transformations, these often disregard natural image structure. Thus, augmented samples can exhibit degraded semantic information and low stylistic diversity, affecting downstream performance of self-supervised representations. To overcome this, we propose SASSL: Style Augmentations for Self Supervised Learning, a novel augmentation technique based on Neural Style Transfer. The method decouples semantic and stylistic attributes in images and applies transformations exclusively to the style while preserving content, generating diverse augmented samples that better retain their semantic properties. Experimental results show our technique achieves a top-1 classification performance improvement of more than 2% on ImageNet compared to the well-established MoCo v2. We also measure
    
[^48]: 探索在导航场景下推断用户对机器人性能的印象

    Towards Inferring Users' Impressions of Robot Performance in Navigation Scenarios. (arXiv:2310.11590v1 [cs.RO])

    [http://arxiv.org/abs/2310.11590](http://arxiv.org/abs/2310.11590)

    本研究拟通过非语言行为提示和机器学习技术预测人们对机器人行为印象，并提供了一个数据集和分析结果，发现在导航场景中，空间特征是最关键的信息。

    

    人们对机器人性能的印象通常通过调查问卷来衡量。作为一种更可扩展且成本效益更高的替代方案，我们研究了使用非语言行为提示和机器学习技术预测人们对机器人行为印象的可能性。为此，我们首先提供了SEAN TOGETHER数据集，该数据集包括在虚拟现实模拟中人与移动机器人相互作用的观察结果，以及用户对机器人性能的5点量表评价。其次，我们对人类和监督学习技术如何基于不同的观察类型（例如面部、空间和地图特征）来预测感知到的机器人性能进行了分析。我们的结果表明，仅仅面部表情就能提供关于人们对机器人性能印象的有用信息；但在我们测试的导航场景中，空间特征是这种推断任务最关键的信息。

    Human impressions of robot performance are often measured through surveys. As a more scalable and cost-effective alternative, we study the possibility of predicting people's impressions of robot behavior using non-verbal behavioral cues and machine learning techniques. To this end, we first contribute the SEAN TOGETHER Dataset consisting of observations of an interaction between a person and a mobile robot in a Virtual Reality simulation, together with impressions of robot performance provided by users on a 5-point scale. Second, we contribute analyses of how well humans and supervised learning techniques can predict perceived robot performance based on different combinations of observation types (e.g., facial, spatial, and map features). Our results show that facial expressions alone provide useful information about human impressions of robot performance; but in the navigation scenarios we tested, spatial features are the most critical piece of information for this inference task. Als
    
[^49]: Neur2RO: 神经二阶段鲁棒优化

    Neur2RO: Neural Two-Stage Robust Optimization. (arXiv:2310.04345v1 [math.OC])

    [http://arxiv.org/abs/2310.04345](http://arxiv.org/abs/2310.04345)

    Neur2RO是一种神经网络驱动的二阶段鲁棒优化算法，通过学习估计第二阶段问题的值函数，并嵌入到经典的列-约束生成算法中，能够高效地求解嵌套的最小-最大-最小优化问题。

    

    鲁棒优化提供了一个数学框架，用于在最坏情况下的不确定性下建模和解决决策问题。本工作解决了二阶段鲁棒优化（也称为可调整鲁棒优化）问题，在不确定性实现之前和之后进行第一阶段和第二阶段的决策。这导致了一个嵌套的最小-最大-最小优化问题，从计算上来说是非常具有挑战性的，尤其是当决策是离散的时候。我们提出了Neur2RO，这是一种高效的基于机器学习的列-约束生成（CCG）的实例算法，CCG是二阶段鲁棒优化的经典迭代算法。具体而言，我们通过一种新颖的神经网络架构来学习估计第二阶段问题的值函数，这种架构易于优化。将我们的神经网络嵌入到CCG算法中，可以快速得到高质量的解，这在两个二阶段鲁棒优化基准测试（背包问题和资本预算）的实验证明了。

    Robust optimization provides a mathematical framework for modeling and solving decision-making problems under worst-case uncertainty. This work addresses two-stage robust optimization (2RO) problems (also called adjustable robust optimization), wherein first-stage and second-stage decisions are made before and after uncertainty is realized, respectively. This results in a nested min-max-min optimization problem which is extremely challenging computationally, especially when the decisions are discrete. We propose Neur2RO, an efficient machine learning-driven instantiation of column-and-constraint generation (CCG), a classical iterative algorithm for 2RO. Specifically, we learn to estimate the value function of the second-stage problem via a novel neural network architecture that is easy to optimize over by design. Embedding our neural network into CCG yields high-quality solutions quickly as evidenced by experiments on two 2RO benchmarks, knapsack and capital budgeting. For knapsack, Ne
    
[^50]: 半监督开放世界目标检测

    Semi-Supervised Object Detection in the Open World. (arXiv:2307.15710v1 [cs.CV])

    [http://arxiv.org/abs/2307.15710](http://arxiv.org/abs/2307.15710)

    本文提出了一种半监督开放世界目标检测框架，能够有效地检测分布外的数据并从中学习，通过基于集成的OOD检测器和半监督学习方法，实现与最先进方法相当的性能。

    

    现有的半监督目标检测方法假设训练数据和未标记数据集中有一组固定的类别，即属于分布内（ID）的数据。然而，当这些方法在开放世界中应用时，性能显著下降，因为未标记和测试数据可能包含训练过程中未见过的对象，即属于分布外（OOD）的数据。本文探讨两个关键问题：我们是否能够检测这些OOD样本，如果可以，我们是否能够从中学习？考虑到这些问题，我们提出了一种有效的开放世界半监督检测框架（OWSSD），其能够有效地检测OOD数据，并通过半监督学习从ID和OOD数据中进行学习。我们引入了一个基于集成的OOD检测器，由仅在ID数据上训练的轻量级自编码器网络组成。通过广泛的评估，我们证明了我们的方法在OOD目标检测方面与最先进的方法竞争力相当。

    Existing approaches for semi-supervised object detection assume a fixed set of classes present in training and unlabeled datasets, i.e., in-distribution (ID) data. The performance of these techniques significantly degrades when these techniques are deployed in the open-world, due to the fact that the unlabeled and test data may contain objects that were not seen during training, i.e., out-of-distribution (OOD) data. The two key questions that we explore in this paper are: can we detect these OOD samples and if so, can we learn from them? With these considerations in mind, we propose the Open World Semi-supervised Detection framework (OWSSD) that effectively detects OOD data along with a semi-supervised learning pipeline that learns from both ID and OOD data. We introduce an ensemble based OOD detector consisting of lightweight auto-encoder networks trained only on ID data. Through extensive evalulation, we demonstrate that our method performs competitively against state-of-the-art OOD 
    
[^51]: 零样本无线室内导航通过物理信息强化学习

    Zero-Shot Wireless Indoor Navigation through Physics-Informed Reinforcement Learning. (arXiv:2306.06766v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2306.06766](http://arxiv.org/abs/2306.06766)

    该论文提出了一种基于物理信息强化学习的零样本无线室内导航方法，通过样本高效学习和零样本泛化来提高导航效率。

    

    针对室内机器人导航利用无线信号的不断关注，该论文提出了一种基于物理信息强化学习（PIRL）的方法，以实现高效的样本学习和零样本泛化。相对于基于启发式方法的传统方法，基于射频传播的方法直观且能够适应简单的场景，但在复杂环境下难以导航。而基于端到端深度强化学习（RL）的方法可以通过使用先进的计算机技术来探索整个状态空间，在面对复杂的无线环境时表现出令人惊讶的性能。然而，这种方法需要大量的训练样本，而且得到的策略在未进行微调的情况下无法在训练阶段未见过的新场景中有效导航。

    The growing focus on indoor robot navigation utilizing wireless signals has stemmed from the capability of these signals to capture high-resolution angular and temporal measurements. Prior heuristic-based methods, based on radio frequency propagation, are intuitive and generalizable across simple scenarios, yet fail to navigate in complex environments. On the other hand, end-to-end (e2e) deep reinforcement learning (RL), powered by advanced computing machinery, can explore the entire state space, delivering surprising performance when facing complex wireless environments. However, the price to pay is the astronomical amount of training samples, and the resulting policy, without fine-tuning (zero-shot), is unable to navigate efficiently in new scenarios unseen in the training phase. To equip the navigation agent with sample-efficient learning and {zero-shot} generalization, this work proposes a novel physics-informed RL (PIRL) where a distance-to-target-based cost (standard in e2e) is a
    
[^52]: 基于四分位数的季节性分解用于时间序列预测和异常检测

    Quartile-Based Seasonality Decomposition for Time Series Forecasting and Anomaly Detection. (arXiv:2306.05989v1 [cs.LG])

    [http://arxiv.org/abs/2306.05989](http://arxiv.org/abs/2306.05989)

    本文提出了一种名为QBSD的实时预测方法，以在时间序列异常检测中取得最佳平衡。

    

    在电信领域，及时检测异常非常重要，因为这有助于识别和表征不规则模式、异常行为和网络异常，从而提高服务质量和操作效率。精确地预测和消除可预测的时间序列模式是时间序列异常检测的重要组成部分。本文提出了一种名为基于四分位数的季节性分解（QBSD）的实时预测方法，以在计算复杂度和预测准确率之间取得最佳平衡。本文比较了QBSD与现有预测方法的性能及其适用性。

    The timely detection of anomalies is essential in the telecom domain as it facilitates the identification and characterization of irregular patterns, abnormal behaviors, and network anomalies, contributing to enhanced service quality and operational efficiency. Precisely forecasting and eliminating predictable time series patterns constitutes a vital component of time series anomaly detection. While the state-of-the-art methods aim to maximize forecasting accuracy, the computational performance takes a hit. In a system composed of a large number of time series variables, e.g., cell Key Performance Indicators (KPIs), the time and space complexity of the forecasting employed is of crucial importance. Quartile-Based Seasonality Decomposition (QBSD) is a live forecasting method proposed in this paper to make an optimal trade-off between computational complexity and forecasting accuracy. This paper compares the performance of QBSD to the state-of-the-art forecasting methods and their applic
    
[^53]: FLEdge：边缘计算系统中联邦机器学习应用的基准测试

    FLEdge: Benchmarking Federated Machine Learning Applications in Edge Computing Systems. (arXiv:2306.05172v1 [cs.LG])

    [http://arxiv.org/abs/2306.05172](http://arxiv.org/abs/2306.05172)

    FLEdge是一个面向边缘计算系统中FL工作量的基准测试，通过研究硬件异构性、能量效率和隐私级别对FL系统训练的影响，以及客户端退出对最新FL策略的影响，提供了训练最先进的FL工作负载的新见解。

    

    近年来，联邦机器学习（FL）备受关注。 FL基准测试主要在模拟系统或数据中心环境中进行探索，忽略了与边缘计算密切相关的实际系统设置。 我们通过引入面向边缘计算系统中FL工作量的基准测试FLEdge来弥补这一研究差距。我们系统地研究了硬件异构性、训练过程中的能量效率以及各种不同隐私级别对FL系统训练的影响。为了使这个基准测试适用于实际场景，我们评估了客户端退出对具有高达50％失效率的最新FL策略的影响。 FLEdge提供了新的见解，例如，在旧GPU加速的嵌入式设备上训练最先进的FL工作负载比在现代服务器级GPU上训练高达3倍的能量效率。

    Federated Machine Learning (FL) has received considerable attention in recent years. FL benchmarks are predominantly explored in either simulated systems or data center environments, neglecting the setups of real-world systems, which are often closely linked to edge computing. We close this research gap by introducing FLEdge, a benchmark targeting FL workloads in edge computing systems. We systematically study hardware heterogeneity, energy efficiency during training, and the effect of various differential privacy levels on training in FL systems. To make this benchmark applicable to real-world scenarios, we evaluate the impact of client dropouts on state-of-the-art FL strategies with failure rates as high as 50%. FLEdge provides new insights, such as that training state-of-the-art FL workloads on older GPU-accelerated embedded devices is up to 3x more energy efficient than on modern server-grade GPUs.
    
[^54]: 用于液压状态监测系统异常检测的半监督学习比较研究

    Comparative Study on Semi-supervised Learning Applied for Anomaly Detection in Hydraulic Condition Monitoring System. (arXiv:2306.02709v1 [cs.LG])

    [http://arxiv.org/abs/2306.02709](http://arxiv.org/abs/2306.02709)

    本研究比较了不同类型的半监督学习方法在液压状态监测系统中用于异常检测。深度学习模型表现最好，而集成模型可以进一步提高检测性能。

    

    基于状态的维护在液压系统中变得越来越重要。然而，这些系统的异常检测仍然具有挑战性，特别是由于异常数据很少，标记这些数据是费时费力甚至危险的。因此，建议使用无监督或半监督方法，特别是对于只有少量标签可用的情况下利用无监督学习作为特征提取机制来辅助监督学习的半监督学习方法。本研究系统地比较了在液压状态监测系统中应用的半监督学习方法用于异常检测。首先，进行了深入的数据分析和特征学习，以了解开源的液压状态监测数据集。然后，实施和评估了各种方法，包括传统的独立半监督学习模型（例如，一类支持向量机、鲁棒协方差）、集成模型（例如，孤立森林）和基于深度学习的模型（例如，自动编码器、图卷积网络）。结果表明，深度学习模型优于传统模型，而集成模型可以进一步提高检测性能。

    Condition-based maintenance is becoming increasingly important in hydraulic systems. However, anomaly detection for these systems remains challenging, especially since that anomalous data is scarce and labeling such data is tedious and even dangerous. Therefore, it is advisable to make use of unsupervised or semi-supervised methods, especially for semi-supervised learning which utilizes unsupervised learning as a feature extraction mechanism to aid the supervised part when only a small number of labels are available. This study systematically compares semi-supervised learning methods applied for anomaly detection in hydraulic condition monitoring systems. Firstly, thorough data analysis and feature learning were carried out to understand the open-sourced hydraulic condition monitoring dataset. Then, various methods were implemented and evaluated including traditional stand-alone semi-supervised learning models (e.g., one-class SVM, Robust Covariance), ensemble models (e.g., Isolation F
    
[^55]: 通过无正交化方法的谱聚类

    Spectral Clustering via Orthogonalization-Free Methods. (arXiv:2305.10356v1 [eess.SP])

    [http://arxiv.org/abs/2305.10356](http://arxiv.org/abs/2305.10356)

    本文提出了四种无正交化方法作为谱聚类降维，不需要昂贵的特征值估计，在聚类质量和计算成本方面均优于已有方法，适合于并行计算。

    

    在谱聚类的降维中，通常使用图信号滤波器需要昂贵的特征值估计。我们在最优化设置中分析了滤波器并提出使用四种无正交化方法作为谱聚类中的降维。所提出的方法不利用任何正交化方法，在并行计算环境中不可伸缩。我们的方法在理论上构造了足够的特征空间，最多是规范化拉普拉斯矩阵特征空间的加权改变。我们在数值上假设所提出的方法与利用精确特征值但需要昂贵特征值估计的理想图信号滤波器在聚类质量上等效。数值结果表明，所提出的方法在聚类质量和计算成本方面优于基于幂迭代的方法和图信号滤波器。与基于幂迭代的方法不同，我们的方法可以轻松并行化。

    Graph Signal Filter used as dimensionality reduction in spectral clustering usually requires expensive eigenvalue estimation. We analyze the filter in an optimization setting and propose to use four orthogonalization-free methods by optimizing objective functions as dimensionality reduction in spectral clustering. The proposed methods do not utilize any orthogonalization, which is known as not well scalable in a parallel computing environment. Our methods theoretically construct adequate feature space, which is, at most, a weighted alteration to the eigenspace of a normalized Laplacian matrix. We numerically hypothesize that the proposed methods are equivalent in clustering quality to the ideal Graph Signal Filter, which exploits the exact eigenvalue needed without expensive eigenvalue estimation. Numerical results show that the proposed methods outperform Power Iteration-based methods and Graph Signal Filter in clustering quality and computation cost. Unlike Power Iteration-based meth
    
[^56]: 比例代表性聚类

    Proportionally Representative Clustering. (arXiv:2304.13917v1 [cs.LG])

    [http://arxiv.org/abs/2304.13917](http://arxiv.org/abs/2304.13917)

    本文提出了一个新的公平性准则——比例代表性公平性（PRF），并设计了有效的算法满足该准则。

    

    近年来，机器学习领域对公平概念的形式化表述越来越受关注。本文关注于聚类，是无监督机器学习中最基础的任务之一。我们提出了一个新的公平性准则——比例代表性公平性（PRF），我们认为该概念以一种更有说服力的方式达到了文献中几个现存概念的理由。但现有的公平聚类算法不能满足我们的公平性概念。我们设计了高效的算法，以满足无约束聚类和离散聚类问题的PRF。

    In recent years, there has been a surge in effort to formalize notions of fairness in machine learning. We focus on clustering -- one of the fundamental tasks in unsupervised machine learning. We propose a new axiom that captures proportional representation fairness (PRF). We make a case that the concept achieves the raison d'{\^{e}}tre of several existing concepts in the literature in an arguably more convincing manner. Our fairness concept is not satisfied by existing fair clustering algorithms. We design efficient algorithms to achieve PRF both for unconstrained and discrete clustering problems.
    
[^57]: Phylo2Vec: 一种二叉树的向量表示方法

    Phylo2Vec: a vector representation for binary trees. (arXiv:2304.12693v1 [q-bio.PE])

    [http://arxiv.org/abs/2304.12693](http://arxiv.org/abs/2304.12693)

    Phylo2Vec是一种新的二叉树简明表示方法，它能够轻松采样二叉树，并以系统性的方法遍历树空间。这种方法用于构建深度神经网络，能够显著提高蛋白质类别预测的性能。

    

    从生物数据推断得到的二叉进化树对于理解生物之间共享的进化历史至关重要。根据最大似然等某个最优性准则推断出树中潜在节点的位置是NP-hard问题，这推动了大量启发式方法的发展。然而，这些启发式方法通常缺乏一种系统性的方法来均匀采样随机树或有效地探索指数级增长的树空间，这对于机器学习等优化问题至关重要。因此，我们提出了Phylo2Vec，这是一种新的简明表示方法来表示进化树。Phylo2Vec将任何具有n个叶子的二叉树映射到长度为n的整数向量。我们证明了Phylo2Vec在空间中既是良定的又是双射的。Phylo2Vec的优点是：i）轻松均匀采样二叉树；ii）以非常大或小的步长系统地遍历树空间。作为概念验证，我们使用Phylo2Vec构建了一个深度神经网络，以从氨基酸序列预测蛋白质类别。我们证明了Phylo2Vec显著提高了网络的性能，超过了之前的最优结果。

    Binary phylogenetic trees inferred from biological data are central to understanding the shared evolutionary history of organisms. Inferring the placement of latent nodes in a tree by any optimality criterion (e.g., maximum likelihood) is an NP-hard problem, propelling the development of myriad heuristic approaches. Yet, these heuristics often lack a systematic means of uniformly sampling random trees or effectively exploring a tree space that grows factorially, which are crucial to optimisation problems such as machine learning. Accordingly, we present Phylo2Vec, a new parsimonious representation of a phylogenetic tree. Phylo2Vec maps any binary tree with $n$ leaves to an integer vector of length $n$. We prove that Phylo2Vec is both well-defined and bijective to the space of phylogenetic trees. The advantages of Phylo2Vec are twofold: i) easy uniform sampling of binary trees and ii) systematic ability to traverse tree space in very large or small jumps. As a proof of concept, we use P
    
[^58]: 核密度贝叶斯逆强化学习

    Kernel Density Bayesian Inverse Reinforcement Learning. (arXiv:2303.06827v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06827](http://arxiv.org/abs/2303.06827)

    KD-BIRL是一种核密度贝叶斯逆强化学习方法，通过直接逼近似然函数来学习代理的奖励函数，克服了学习点估计的缺点，并适用于复杂和无限环境。

    

    逆强化学习（IRL）是一种通过观察代理行为来推断其奖励函数的强大框架，但学习奖励函数的点估计可能会误导，因为可能有多个函数能够很好地描述代理的行为。贝叶斯逆强化学习采用贝叶斯方法模拟候选奖励函数的分布，克服了学习点估计的缺点。然而，一些贝叶斯逆强化学习算法使用Q值函数代替似然函数。由此得到的后验计算量大，理论保证少，并且Q值函数通常对似然函数的逼近效果较差。我们引入了核密度贝叶斯逆强化学习（KD-BIRL），该方法使用条件核密度估计直接逼近似然函数，提供了一个高效的框架，在经过改进的奖励函数参数化下，适用于具有复杂和无限的环境。

    Inverse reinforcement learning~(IRL) is a powerful framework to infer an agent's reward function by observing its behavior, but IRL algorithms that learn point estimates of the reward function can be misleading because there may be several functions that describe an agent's behavior equally well. A Bayesian approach to IRL models a distribution over candidate reward functions, alleviating the shortcomings of learning a point estimate. However, several Bayesian IRL algorithms use a $Q$-value function in place of the likelihood function. The resulting posterior is computationally intensive to calculate, has few theoretical guarantees, and the $Q$-value function is often a poor approximation for the likelihood. We introduce kernel density Bayesian IRL (KD-BIRL), which uses conditional kernel density estimation to directly approximate the likelihood, providing an efficient framework that, with a modified reward function parameterization, is applicable to environments with complex and infin
    
[^59]: 通过机器人车辆在复杂和无信号的交叉口中学习控制和协调混合交通

    Learning to Control and Coordinate Mixed Traffic Through Robot Vehicles at Complex and Unsignalized Intersections. (arXiv:2301.05294v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.05294](http://arxiv.org/abs/2301.05294)

    本研究提出了一种去中心化的多智能体强化学习方法，用于控制和协调混合交通，特别是人驾驶车辆和机器人车辆在实际复杂交叉口的应用。实验结果表明，使用5%的机器人车辆可以有效防止交叉口内的拥堵形成。

    

    交叉口是现代大都市交通中必不可少的道路基础设施。然而，由于交通事故或缺乏交通协调机制（如交通信号灯），它们也可能成为交通流的瓶颈。最近，提出了各种超越传统控制方法的控制和协调机制，以提高交叉口交通的效率。在这些方法中，控制可预见的包含人驾驶车辆（HVs）和机器人车辆（RVs）的混合交通已经出现。在本项目中，我们提出了一种去中心化的多智能体强化学习方法，用于实际复杂交叉口的混合交通的控制和协调，这是一个以前未被探索过的主题。我们进行了全面的实验，以展示我们方法的有效性。特别是，我们展示了在实际交通条件下，使用5%的RVs，我们可以防止复杂交叉口内的拥堵形成。

    Intersections are essential road infrastructures for traffic in modern metropolises. However, they can also be the bottleneck of traffic flows as a result of traffic incidents or the absence of traffic coordination mechanisms such as traffic lights. Recently, various control and coordination mechanisms that are beyond traditional control methods have been proposed to improve the efficiency of intersection traffic. Amongst these methods, the control of foreseeable mixed traffic that consists of human-driven vehicles (HVs) and robot vehicles (RVs) has emerged. In this project, we propose a decentralized multi-agent reinforcement learning approach for the control and coordination of mixed traffic at real-world, complex intersections--a topic that has not been previously explored. Comprehensive experiments are conducted to show the effectiveness of our approach. In particular, we show that using 5% RVs, we can prevent congestion formation inside a complex intersection under the actual traf
    
[^60]: 神经会合：面向星际物体的可靠导航和控制的证明

    Neural-Rendezvous: Provably Robust Guidance and Control to Encounter Interstellar Objects. (arXiv:2208.04883v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2208.04883](http://arxiv.org/abs/2208.04883)

    本文提出了神经会合，一种深度学习导航和控制框架，用于可靠、准确和自主地遭遇快速移动的星际物体。它通过点最小范数追踪控制和谱归一化深度神经网络引导策略来提供高概率指数上界的飞行器交付误差。

    

    星际物体（ISOs）很可能是不可替代的原始材料，在理解系外行星星系方面具有重要价值。然而，由于其运行轨道难以约束，通常具有较高的倾角和相对速度，使用传统的人在环路方法探索ISOs具有相当大的挑战性。本文提出了一种名为神经会合的深度学习导航和控制框架，用于在实时中以可靠、准确和自主的方式遭遇快速移动的物体，包括ISOs。它在基于谱归一化的深度神经网络的引导策略之上使用点最小范数追踪控制，其中参数通过直接惩罚MPC状态轨迹跟踪误差的损失函数进行调优。我们展示了神经会合在预期的飞行器交付误差上提供了高概率指数上界，其证明利用了随机递增稳定性分析。

    Interstellar objects (ISOs) are likely representatives of primitive materials invaluable in understanding exoplanetary star systems. Due to their poorly constrained orbits with generally high inclinations and relative velocities, however, exploring ISOs with conventional human-in-the-loop approaches is significantly challenging. This paper presents Neural-Rendezvous, a deep learning-based guidance and control framework for encountering fast-moving objects, including ISOs, robustly, accurately, and autonomously in real time. It uses pointwise minimum norm tracking control on top of a guidance policy modeled by a spectrally-normalized deep neural network, where its hyperparameters are tuned with a loss function directly penalizing the MPC state trajectory tracking error. We show that Neural-Rendezvous provides a high probability exponential bound on the expected spacecraft delivery error, the proof of which leverages stochastic incremental stability analysis. In particular, it is used to
    
[^61]: 一个黑盒NLP分类器攻击器

    A Black-box NLP Classifier Attacker. (arXiv:2112.11660v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.11660](http://arxiv.org/abs/2112.11660)

    本文提出了一个黑盒NLP分类器攻击模型，通过基于自注意机制的词选择和贪婪搜索算法进行词替换，解决了影响NLP领域传统图像攻击方法不适用的问题。

    

    深度神经网络在解决各种现实世界任务中具有广泛的应用，并在计算机视觉、图像分类和自然语言处理等领域取得了令人满意的结果。与此同时，神经网络的安全性和鲁棒性已经变得非常重要，因为各种研究已经显示了神经网络的脆弱性。以自然语言处理任务为例，神经网络可能被一个与原始文本高度相似的、经过仔细修改的文本所迷惑。根据之前的研究，大部分研究都集中在图像领域；与图像对抗攻击不同，文本以离散序列表示，传统的图像攻击方法在NLP领域不适用。本文提出了一个基于自注意机制的词级NLP情感分类器攻击模型，其中包括基于词选择的自注意机制和贪婪搜索算法进行词替换。我们进行了实验验证...

    Deep neural networks have a wide range of applications in solving various real-world tasks and have achieved satisfactory results, in domains such as computer vision, image classification, and natural language processing. Meanwhile, the security and robustness of neural networks have become imperative, as diverse researches have shown the vulnerable aspects of neural networks. Case in point, in Natural language processing tasks, the neural network may be fooled by an attentively modified text, which has a high similarity to the original one. As per previous research, most of the studies are focused on the image domain; Different from image adversarial attacks, the text is represented in a discrete sequence, traditional image attack methods are not applicable in the NLP field. In this paper, we propose a word-level NLP sentiment classifier attack model, which includes a self-attention mechanism-based word selection method and a greedy search algorithm for word substitution. We experimen
    

