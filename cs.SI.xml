<rss version="2.0"><channel><title>Chat Arxiv cs.SI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SI</description><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#24320;&#21457;&#20102;LLaVA-Docent&#27169;&#22411;&#65292;&#20197;&#25903;&#25345;&#33402;&#26415;&#37492;&#36175;&#25945;&#32946;&#12290;&#36890;&#36807;&#32508;&#36848;&#25991;&#29486;&#21644;&#19987;&#23478;&#21672;&#35810;&#65292;&#26500;&#24314;&#20102;&#25968;&#25454;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#35813;&#26694;&#26550;&#29983;&#25104;&#20102;&#34394;&#25311;&#23545;&#35805;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;MLLM&#12290;&#35813;&#30740;&#31350;&#23545;&#20110;&#35299;&#20915;&#20256;&#32479;&#33402;&#26415;&#37492;&#36175;&#25945;&#32946;&#20013;&#30340;&#36164;&#28304;&#38480;&#21046;&#21644;&#20027;&#27969;&#25945;&#32946;&#20013;&#30340;&#31185;&#23398;&#25216;&#26415;&#24037;&#31243;&#21644;&#25968;&#23398;&#20559;&#37325;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.06264</link><description>&lt;p&gt;
LLaVA-Docent&#65306;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25903;&#25345;&#33402;&#26415;&#37492;&#36175;&#25945;&#32946;&#30340;&#25945;&#23398;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
LLaVA-Docent: Instruction Tuning with Multimodal Large Language Model to Support Art Appreciation Education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#24320;&#21457;&#20102;LLaVA-Docent&#27169;&#22411;&#65292;&#20197;&#25903;&#25345;&#33402;&#26415;&#37492;&#36175;&#25945;&#32946;&#12290;&#36890;&#36807;&#32508;&#36848;&#25991;&#29486;&#21644;&#19987;&#23478;&#21672;&#35810;&#65292;&#26500;&#24314;&#20102;&#25968;&#25454;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#35813;&#26694;&#26550;&#29983;&#25104;&#20102;&#34394;&#25311;&#23545;&#35805;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;MLLM&#12290;&#35813;&#30740;&#31350;&#23545;&#20110;&#35299;&#20915;&#20256;&#32479;&#33402;&#26415;&#37492;&#36175;&#25945;&#32946;&#20013;&#30340;&#36164;&#28304;&#38480;&#21046;&#21644;&#20027;&#27969;&#25945;&#32946;&#20013;&#30340;&#31185;&#23398;&#25216;&#26415;&#24037;&#31243;&#21644;&#25968;&#23398;&#20559;&#37325;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33402;&#26415;&#37492;&#36175;&#23545;&#20110;&#22521;&#20859;&#23398;&#20064;&#32773;&#30340;&#25209;&#21028;&#24615;&#24605;&#32500;&#21644;&#24773;&#24863;&#26234;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#33402;&#26415;&#37492;&#36175;&#25945;&#32946;&#24120;&#38754;&#20020;&#33402;&#26415;&#36164;&#28304;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#24369;&#21183;&#23398;&#29983;&#65292;&#24182;&#19988;&#22312;&#20027;&#27969;&#25945;&#32946;&#20013;&#36807;&#24230;&#24378;&#35843;&#31185;&#23398;&#25216;&#26415;&#24037;&#31243;&#21644;&#25968;&#23398;&#31185;&#30446;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26368;&#36817;&#30340;&#25216;&#26415;&#36827;&#27493;&#20026;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#22312;&#33402;&#26415;&#37492;&#36175;&#25945;&#32946;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#26159;&#24320;&#21457;&#20102;LLaVA-Docent&#27169;&#22411;&#26469;&#21033;&#29992;&#36825;&#20123;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20840;&#38754;&#30340;&#25991;&#29486;&#32508;&#36848;&#21644;&#19982;&#39046;&#22495;&#19987;&#23478;&#30340;&#21672;&#35810;&#65292;&#20174;&#32780;&#24418;&#25104;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#25968;&#25454;&#26694;&#26550;&#12290;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#34394;&#25311;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#34987;GPT-4&#21033;&#29992;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#23545;&#20110;&#35757;&#32451;MLLM&#65288;&#21363;LLaVA-Docent&#65289;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#20845;&#21517;&#30740;&#31350;&#20154;&#21592;&#36827;&#34892;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Art appreciation is vital in nurturing critical thinking and emotional intelligence among learners. However, traditional art appreciation education has often been hindered by limited access to art resources, especially for disadvantaged students, and an imbalanced emphasis on STEM subjects in mainstream education. In response to these challenges, recent technological advancements have paved the way for innovative solutions. This study explores the application of multi-modal large language models (MLLMs) in art appreciation education, focusing on developing LLaVA-Docent, a model that leverages these advancements. Our approach involved a comprehensive literature review and consultations with experts in the field, leading to developing a robust data framework. Utilizing this framework, we generated a virtual dialogue dataset that was leveraged by GPT-4. This dataset was instrumental in training the MLLM, named LLaVA-Docent. Six researchers conducted quantitative and qualitative evaluation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#35745;&#31639;&#36229;&#22270;&#33410;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#24182;&#36827;&#34892;&#26631;&#31614;&#20256;&#25773;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#36229;&#22270;&#20013;&#33410;&#28857;&#36317;&#31163;&#35745;&#31639;&#30340;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#25299;&#23637;&#20102;&#36229;&#22270;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2401.13054</link><description>&lt;p&gt;
&#26080;&#35745;&#31639;&#22256;&#38590;&#30340;&#24555;&#36895;&#35745;&#31639;&#36229;&#22270;&#33410;&#28857;&#36317;&#31163;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Frustrated Random Walks: A Fast Method to Compute Node Distances on Hypergraphs. (arXiv:2401.13054v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#35745;&#31639;&#36229;&#22270;&#33410;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#24182;&#36827;&#34892;&#26631;&#31614;&#20256;&#25773;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#36229;&#22270;&#20013;&#33410;&#28857;&#36317;&#31163;&#35745;&#31639;&#30340;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#25299;&#23637;&#20102;&#36229;&#22270;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#26159;&#22270;&#30340;&#25512;&#24191;&#65292;&#24403;&#32771;&#34385;&#23454;&#20307;&#38388;&#30340;&#23646;&#24615;&#20849;&#20139;&#26102;&#20250;&#33258;&#28982;&#20135;&#29983;&#12290;&#23613;&#31649;&#21487;&#20197;&#36890;&#36807;&#23558;&#36229;&#36793;&#25193;&#23637;&#20026;&#23436;&#20840;&#36830;&#25509;&#30340;&#23376;&#22270;&#26469;&#23558;&#36229;&#22270;&#36716;&#25442;&#20026;&#22270;&#65292;&#20294;&#36870;&#21521;&#25805;&#20316;&#22312;&#35745;&#31639;&#19978;&#38750;&#24120;&#22797;&#26434;&#19988;&#23646;&#20110;NP-complete&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20551;&#35774;&#36229;&#22270;&#21253;&#21547;&#27604;&#22270;&#26356;&#22810;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#30452;&#25509;&#25805;&#20316;&#36229;&#22270;&#27604;&#23558;&#20854;&#25193;&#23637;&#20026;&#22270;&#26356;&#20026;&#26041;&#20415;&#12290;&#36229;&#22270;&#20013;&#30340;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#26159;&#22914;&#20309;&#31934;&#30830;&#39640;&#25928;&#22320;&#35745;&#31639;&#33410;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#36890;&#36807;&#20272;&#35745;&#33410;&#28857;&#36317;&#31163;&#65292;&#25105;&#20204;&#33021;&#22815;&#25214;&#21040;&#33410;&#28857;&#30340;&#26368;&#36817;&#37051;&#23621;&#65292;&#24182;&#20351;&#29992;K&#26368;&#36817;&#37051;&#65288;KNN&#65289;&#26041;&#27861;&#22312;&#36229;&#22270;&#19978;&#25191;&#34892;&#26631;&#31614;&#20256;&#25773;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#30340;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#36229;&#22270;&#19978;&#36827;&#34892;&#26631;&#31614;&#20256;&#25773;&#12290;&#25105;&#20204;&#23558;&#33410;&#28857;&#36317;&#31163;&#20272;&#35745;&#20026;&#38543;&#26426;&#28216;&#36208;&#30340;&#39044;&#26399;&#21040;&#36798;&#26102;&#38388;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#31616;&#21333;&#38543;&#26426;&#28216;&#36208;&#65288;SRW&#65289;&#26080;&#27861;&#20934;&#30830;&#25551;&#36848;&#33410;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#22240;&#27492;&#25105;&#20204;&#24341;&#20837;&#20102;"frustrated"&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
A hypergraph is a generalization of a graph that arises naturally when attribute-sharing among entities is considered. Although a hypergraph can be converted into a graph by expanding its hyperedges into fully connected subgraphs, going the reverse way is computationally complex and NP-complete. We therefore hypothesize that a hypergraph contains more information than a graph. In addition, it is more convenient to manipulate a hypergraph directly, rather than expand it into a graph. An open problem in hypergraphs is how to accurately and efficiently calculate their node distances. Estimating node distances enables us to find a node's nearest neighbors, and perform label propagation on hypergraphs using a K-nearest neighbors (KNN) approach. In this paper, we propose a novel approach based on random walks to achieve label propagation on hypergraphs. We estimate node distances as the expected hitting times of random walks. We note that simple random walks (SRW) cannot accurately describe 
&lt;/p&gt;</description></item></channel></rss>