<rss version="2.0"><channel><title>Chat Arxiv cs.SI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SI</description><item><title>&#26412;&#32508;&#36848;&#20174;&#25968;&#25454;&#20013;&#24515;&#21270;&#30340;&#35282;&#24230;&#20840;&#38754;&#35780;&#20272;&#20102;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22238;&#31572;&#20102;&#20309;&#26102;&#20462;&#25913;&#22270;&#25968;&#25454;&#12289;&#22270;&#25968;&#25454;&#30340;&#21738;&#19968;&#37096;&#20998;&#38656;&#35201;&#20462;&#25913;&#20197;&#21450;&#22914;&#20309;&#20445;&#25252;&#22270;&#27169;&#22411;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.04987</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#21270;&#22270;&#23398;&#20064;&#65306;&#19968;&#20221;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Data-centric Graph Learning: A Survey. (arXiv:2310.04987v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20174;&#25968;&#25454;&#20013;&#24515;&#21270;&#30340;&#35282;&#24230;&#20840;&#38754;&#35780;&#20272;&#20102;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22238;&#31572;&#20102;&#20309;&#26102;&#20462;&#25913;&#22270;&#25968;&#25454;&#12289;&#22270;&#25968;&#25454;&#30340;&#21738;&#19968;&#37096;&#20998;&#38656;&#35201;&#20462;&#25913;&#20197;&#21450;&#22914;&#20309;&#20445;&#25252;&#22270;&#27169;&#22411;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#21382;&#21490;&#35265;&#35777;&#20102;&#39640;&#36136;&#37327;&#25968;&#25454;&#23545;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#37325;&#22823;&#24433;&#21709;&#65292;&#20363;&#22914;AlexNet&#21644;ResNet&#30340;ImageNet&#12290;&#26368;&#36817;&#65292;&#19982;&#20197;&#27169;&#22411;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#35774;&#35745;&#26356;&#22797;&#26434;&#30340;&#31070;&#32463;&#32467;&#26500;&#19981;&#21516;&#65292;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#30340;&#20851;&#27880;&#37325;&#28857;&#36716;&#21521;&#20102;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20391;&#37325;&#20110;&#26356;&#22909;&#22320;&#22788;&#29702;&#25968;&#25454;&#20197;&#22686;&#24378;&#31070;&#32463;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#32780;&#22312;&#28145;&#24230;&#23398;&#20064;&#26102;&#20195;&#65292;&#25805;&#20316;&#26222;&#36941;&#23384;&#22312;&#30340;&#25299;&#25169;&#25968;&#25454;&#30340;&#22270;&#23398;&#20064;&#20063;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20174;&#25968;&#25454;&#20013;&#24515;&#21270;&#30340;&#35282;&#24230;&#20840;&#38754;&#35780;&#20272;&#20102;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#26088;&#22312;&#22238;&#31572;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#20309;&#26102;&#20462;&#25913;&#22270;&#25968;&#25454;&#65292;&#65288;2&#65289;&#22270;&#25968;&#25454;&#30340;&#21738;&#19968;&#37096;&#20998;&#38656;&#35201;&#20462;&#25913;&#20197;&#37322;&#25918;&#21508;&#31181;&#22270;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#20197;&#21450;&#65288;3&#65289;&#22914;&#20309;&#20445;&#25252;&#22270;&#27169;&#22411;&#20813;&#21463;&#26377;&#38382;&#39064;&#30340;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#23398;&#20064;&#27969;&#31243;&#38454;&#27573;&#30340;&#21019;&#26032;&#20998;&#31867;&#27861;&#65292;&#24182;&#31361;&#20986;&#20102;&#20851;&#38190;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
The history of artificial intelligence (AI) has witnessed the significant impact of high-quality data on various deep learning models, such as ImageNet for AlexNet and ResNet. Recently, instead of designing more complex neural architectures as model-centric approaches, the attention of AI community has shifted to data-centric ones, which focuses on better processing data to strengthen the ability of neural models. Graph learning, which operates on ubiquitous topological data, also plays an important role in the era of deep learning. In this survey, we comprehensively review graph learning approaches from the data-centric perspective, and aim to answer three crucial questions: (1) when to modify graph data, (2) what part of the graph data needs modification to unlock the potential of various graph models, and (3) how to safeguard graph models from problematic data influence. Accordingly, we propose a novel taxonomy based on the stages in the graph learning pipeline, and highlight the pr
&lt;/p&gt;</description></item></channel></rss>