<rss version="2.0"><channel><title>Chat Arxiv cs.SI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SI</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#20540;&#20462;&#27491;&#31574;&#30053;&#65292;&#21487;&#20197;&#25552;&#21319;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20351;&#22810;&#39033;&#24335;&#28388;&#27874;&#22120;&#25670;&#33073;&#37325;&#22797;&#29305;&#24449;&#20540;&#36755;&#20837;&#30340;&#38480;&#21046;&#65292;&#24182;&#22686;&#24378;&#20102;&#29305;&#24449;&#20540;&#30340;&#22343;&#21248;&#20998;&#24067;&#12290;</title><link>https://arxiv.org/abs/2401.15603</link><description>&lt;p&gt;
&#29992;&#29305;&#24449;&#20540;&#20462;&#27491;&#25552;&#21319;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Expressive Power of Spectral Graph Neural Networks with Eigenvalue Correction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15603
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#20540;&#20462;&#27491;&#31574;&#30053;&#65292;&#21487;&#20197;&#25552;&#21319;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20351;&#22810;&#39033;&#24335;&#28388;&#27874;&#22120;&#25670;&#33073;&#37325;&#22797;&#29305;&#24449;&#20540;&#36755;&#20837;&#30340;&#38480;&#21046;&#65292;&#24182;&#22686;&#24378;&#20102;&#29305;&#24449;&#20540;&#30340;&#22343;&#21248;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#20013;&#65292;&#29305;&#24449;&#20026;&#22810;&#39033;&#24335;&#28388;&#27874;&#22120;&#30340;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#22312;&#33410;&#28857;&#20998;&#31867;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20551;&#35774;&#35268;&#33539;&#21270;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#30340;&#29305;&#24449;&#20540;&#24444;&#27492;&#19981;&#21516;&#65292;&#22240;&#27492;&#26399;&#26395;&#22810;&#39033;&#24335;&#28388;&#27874;&#22120;&#20855;&#26377;&#24456;&#39640;&#30340;&#25311;&#21512;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#22312;&#23454;&#35777;&#19978;&#35266;&#23519;&#21040;&#35268;&#33539;&#21270;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#32463;&#24120;&#20855;&#26377;&#37325;&#22797;&#30340;&#29305;&#24449;&#20540;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#24314;&#31435;&#20102;&#21487;&#36776;&#35748;&#29305;&#24449;&#20540;&#30340;&#25968;&#37327;&#22312;&#30830;&#23450;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#37492;&#20110;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#20540;&#20462;&#27491;&#31574;&#30053;&#65292;&#21487;&#20197;&#20351;&#22810;&#39033;&#24335;&#28388;&#27874;&#22120;&#25670;&#33073;&#37325;&#22797;&#29305;&#24449;&#20540;&#36755;&#20837;&#30340;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;&#29305;&#24449;&#20540;&#20462;&#27491;&#31574;&#30053;&#22686;&#24378;&#20102;&#29305;&#24449;&#20540;&#30340;&#22343;&#21248;&#20998;&#24067;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#21463;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.15603v2 Announce Type: replace  Abstract: In recent years, spectral graph neural networks, characterized by polynomial filters, have garnered increasing attention and have achieved remarkable performance in tasks such as node classification. These models typically assume that eigenvalues for the normalized Laplacian matrix are distinct from each other, thus expecting a polynomial filter to have a high fitting ability. However, this paper empirically observes that normalized Laplacian matrices frequently possess repeated eigenvalues. Moreover, we theoretically establish that the number of distinguishable eigenvalues plays a pivotal role in determining the expressive power of spectral graph neural networks. In light of this observation, we propose an eigenvalue correction strategy that can free polynomial filters from the constraints of repeated eigenvalue inputs. Concretely, the proposed eigenvalue correction strategy enhances the uniform distribution of eigenvalues, thus mit
&lt;/p&gt;</description></item></channel></rss>