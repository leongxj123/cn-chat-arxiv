<rss version="2.0"><channel><title>Chat Arxiv cs.SI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21152;&#26435;&#25104;&#21592;&#32423;&#21035;&#65288;WGoM&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#20998;&#31867;&#25968;&#25454;&#30340;&#28508;&#22312;&#31867;&#21035;&#25512;&#26029;&#38382;&#39064;&#12290;&#19982;&#29616;&#26377;&#27169;&#22411;&#30456;&#27604;&#65292;WGoM&#26356;&#36890;&#29992;&#19988;&#36866;&#29992;&#20110;&#20855;&#26377;&#36830;&#32493;&#25110;&#36127;&#21709;&#24212;&#30340;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#31639;&#27861;&#65292;&#25105;&#20204;&#33021;&#22815;&#20934;&#30830;&#39640;&#25928;&#22320;&#20272;&#35745;&#28508;&#22312;&#28151;&#21512;&#25104;&#21592;&#21644;&#20854;&#20182;WGoM&#21442;&#25968;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#24615;&#33021;&#21644;&#23454;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.10989</link><description>&lt;p&gt;
WGoM&#65306;&#19968;&#31181;&#36866;&#29992;&#20110;&#24102;&#21152;&#26435;&#21709;&#24212;&#30340;&#20998;&#31867;&#25968;&#25454;&#30340;&#26032;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
WGoM: A novel model for categorical data with weighted responses. (arXiv:2310.10989v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21152;&#26435;&#25104;&#21592;&#32423;&#21035;&#65288;WGoM&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#20998;&#31867;&#25968;&#25454;&#30340;&#28508;&#22312;&#31867;&#21035;&#25512;&#26029;&#38382;&#39064;&#12290;&#19982;&#29616;&#26377;&#27169;&#22411;&#30456;&#27604;&#65292;WGoM&#26356;&#36890;&#29992;&#19988;&#36866;&#29992;&#20110;&#20855;&#26377;&#36830;&#32493;&#25110;&#36127;&#21709;&#24212;&#30340;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#31639;&#27861;&#65292;&#25105;&#20204;&#33021;&#22815;&#20934;&#30830;&#39640;&#25928;&#22320;&#20272;&#35745;&#28508;&#22312;&#28151;&#21512;&#25104;&#21592;&#21644;&#20854;&#20182;WGoM&#21442;&#25968;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#24615;&#33021;&#21644;&#23454;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Graded of Membership&#65288;GoM&#65289;&#27169;&#22411;&#26159;&#19968;&#31181;&#29992;&#20110;&#25512;&#26029;&#20998;&#31867;&#25968;&#25454;&#20013;&#28508;&#22312;&#31867;&#21035;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20351;&#24471;&#20010;&#20307;&#21487;&#20197;&#23646;&#20110;&#22810;&#20010;&#28508;&#22312;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#35813;&#27169;&#22411;&#20165;&#36866;&#29992;&#20110;&#20855;&#26377;&#38750;&#36127;&#25972;&#25968;&#21709;&#24212;&#30340;&#20998;&#31867;&#25968;&#25454;&#65292;&#20351;&#24471;&#23427;&#26080;&#27861;&#24212;&#29992;&#20110;&#20855;&#26377;&#36830;&#32493;&#25110;&#36127;&#21709;&#24212;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21152;&#26435;&#25104;&#21592;&#32423;&#21035;&#65288;WGoM&#65289;&#27169;&#22411;&#30340;&#26032;&#27169;&#22411;&#12290;&#19982;GoM&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;WGoM&#22312;&#21709;&#24212;&#30697;&#38453;&#30340;&#29983;&#25104;&#19978;&#25918;&#23485;&#20102;GoM&#30340;&#20998;&#24067;&#32422;&#26463;&#65292;&#24182;&#19988;&#27604;GoM&#26356;&#36890;&#29992;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#28508;&#22312;&#28151;&#21512;&#25104;&#21592;&#21644;&#20854;&#20182;WGoM&#21442;&#25968;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#20272;&#35745;&#21442;&#25968;&#30340;&#35823;&#24046;&#30028;&#38480;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;&#35813;&#31639;&#27861;&#30340;&#24615;&#33021;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#20934;&#30830;&#39640;&#25928;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#23454;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Graded of Membership (GoM) model is a powerful tool for inferring latent classes in categorical data, which enables subjects to belong to multiple latent classes. However, its application is limited to categorical data with nonnegative integer responses, making it inappropriate for datasets with continuous or negative responses. To address this limitation, this paper proposes a novel model named the Weighted Grade of Membership (WGoM) model. Compared with GoM, our WGoM relaxes GoM's distribution constraint on the generation of a response matrix and it is more general than GoM. We then propose an algorithm to estimate the latent mixed memberships and the other WGoM parameters. We derive the error bounds of the estimated parameters and show that the algorithm is statistically consistent. The algorithmic performance is validated in both synthetic and real-world datasets. The results demonstrate that our algorithm is accurate and efficient, indicating its high potential for practical a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21517;&#20026;&#23454;&#20363;&#33258;&#36866;&#24212;&#32858;&#31867;&#65288;IAC&#65289;&#65292;&#23427;&#33021;&#22815;&#22312;&#26631;&#35760;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;LSBM&#65289;&#20013;&#24674;&#22797;&#38544;&#34255;&#30340;&#32676;&#38598;&#12290;IAC&#21253;&#25324;&#19968;&#27425;&#35889;&#32858;&#31867;&#21644;&#19968;&#20010;&#36845;&#20195;&#30340;&#22522;&#20110;&#20284;&#28982;&#30340;&#31751;&#20998;&#37197;&#25913;&#36827;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#27169;&#22411;&#21442;&#25968;&#65292;&#26159;&#39640;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2306.12968</link><description>&lt;p&gt;
&#26631;&#35760;&#38543;&#26426;&#22359;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#31751;&#24674;&#22797;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Instance-Optimal Cluster Recovery in the Labeled Stochastic Block Model. (arXiv:2306.12968v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21517;&#20026;&#23454;&#20363;&#33258;&#36866;&#24212;&#32858;&#31867;&#65288;IAC&#65289;&#65292;&#23427;&#33021;&#22815;&#22312;&#26631;&#35760;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;LSBM&#65289;&#20013;&#24674;&#22797;&#38544;&#34255;&#30340;&#32676;&#38598;&#12290;IAC&#21253;&#25324;&#19968;&#27425;&#35889;&#32858;&#31867;&#21644;&#19968;&#20010;&#36845;&#20195;&#30340;&#22522;&#20110;&#20284;&#28982;&#30340;&#31751;&#20998;&#37197;&#25913;&#36827;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#27169;&#22411;&#21442;&#25968;&#65292;&#26159;&#39640;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#26377;&#38480;&#25968;&#37327;&#30340;&#31751;&#30340;&#24773;&#20917;&#19979;&#65292;&#29992;&#26631;&#35760;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;LSBM&#65289;&#24674;&#22797;&#38544;&#34255;&#30340;&#31038;&#32676;&#65292;&#20854;&#20013;&#31751;&#22823;&#23567;&#38543;&#30528;&#29289;&#21697;&#24635;&#25968;$n$&#30340;&#22686;&#38271;&#32780;&#32447;&#24615;&#22686;&#38271;&#12290;&#22312;LSBM&#20013;&#65292;&#20026;&#27599;&#23545;&#29289;&#21697;&#65288;&#29420;&#31435;&#22320;&#65289;&#35266;&#27979;&#21040;&#19968;&#20010;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#19968;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#35266;&#27979;&#21040;&#30340;&#26631;&#31614;&#26469;&#24674;&#22797;&#31751;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#20851;&#20110;&#26399;&#26395;&#34987;&#20219;&#20309;&#32858;&#31867;&#31639;&#27861;&#35823;&#20998;&#31867;&#30340;&#29289;&#21697;&#25968;&#37327;&#30340;&#23454;&#20363;&#29305;&#23450;&#19979;&#30028;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23454;&#20363;&#33258;&#36866;&#24212;&#32858;&#31867;&#65288;IAC&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#26399;&#26395;&#21644;&#39640;&#27010;&#29575;&#19979;&#37117;&#33021;&#21305;&#37197;&#36825;&#20123;&#19979;&#30028;&#34920;&#29616;&#30340;&#31639;&#27861;&#12290;IAC&#30001;&#19968;&#27425;&#35889;&#32858;&#31867;&#31639;&#27861;&#21644;&#19968;&#20010;&#36845;&#20195;&#30340;&#22522;&#20110;&#20284;&#28982;&#30340;&#31751;&#20998;&#37197;&#25913;&#36827;&#32452;&#25104;&#12290;&#36825;&#31181;&#26041;&#27861;&#22522;&#20110;&#23454;&#20363;&#29305;&#23450;&#30340;&#19979;&#30028;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#27169;&#22411;&#21442;&#25968;&#65292;&#21253;&#25324;&#31751;&#30340;&#25968;&#37327;&#12290;&#36890;&#36807;&#20165;&#25191;&#34892;&#19968;&#27425;&#35889;&#32858;&#31867;&#65292;IAC&#22312;&#35745;&#31639;&#21644;&#23384;&#20648;&#26041;&#38754;&#37117;&#26159;&#39640;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of recovering hidden communities in the Labeled Stochastic Block Model (LSBM) with a finite number of clusters, where cluster sizes grow linearly with the total number $n$ of items. In the LSBM, a label is (independently) observed for each pair of items. Our objective is to devise an efficient algorithm that recovers clusters using the observed labels. To this end, we revisit instance-specific lower bounds on the expected number of misclassified items satisfied by any clustering algorithm. We present Instance-Adaptive Clustering (IAC), the first algorithm whose performance matches these lower bounds both in expectation and with high probability. IAC consists of a one-time spectral clustering algorithm followed by an iterative likelihood-based cluster assignment improvement. This approach is based on the instance-specific lower bound and does not require any model parameters, including the number of clusters. By performing the spectral clustering only once, IAC m
&lt;/p&gt;</description></item></channel></rss>