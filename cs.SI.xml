<rss version="2.0"><channel><title>Chat Arxiv cs.SI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SI</description><item><title>&#26412;&#25991;&#36890;&#36807;&#20445;&#25345;&#37051;&#23621;&#30456;&#20284;&#24615;&#23454;&#29616;&#20102;&#26222;&#36866;&#40065;&#26834;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#22312;&#24322;&#31867;&#22270;&#19978;&#25506;&#32034;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#33030;&#24369;&#24615;&#12290;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#36127;&#20998;&#31867;&#25439;&#22833;&#30340;&#26356;&#26032;&#19982;&#22522;&#20110;&#37051;&#23621;&#29305;&#24449;&#30340;&#25104;&#23545;&#30456;&#20284;&#24615;&#21576;&#36127;&#30456;&#20851;&#65292;&#35299;&#37322;&#20102;&#22270;&#25915;&#20987;&#32773;&#36830;&#25509;&#19981;&#30456;&#20284;&#33410;&#28857;&#23545;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#26032;&#39062;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.09754</link><description>&lt;p&gt;
&#36890;&#36807;&#20445;&#25345;&#37051;&#23621;&#30456;&#20284;&#24615;&#23454;&#29616;&#26222;&#36866;&#40065;&#26834;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Universally Robust Graph Neural Networks by Preserving Neighbor Similarity. (arXiv:2401.09754v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20445;&#25345;&#37051;&#23621;&#30456;&#20284;&#24615;&#23454;&#29616;&#20102;&#26222;&#36866;&#40065;&#26834;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#22312;&#24322;&#31867;&#22270;&#19978;&#25506;&#32034;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#33030;&#24369;&#24615;&#12290;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#36127;&#20998;&#31867;&#25439;&#22833;&#30340;&#26356;&#26032;&#19982;&#22522;&#20110;&#37051;&#23621;&#29305;&#24449;&#30340;&#25104;&#23545;&#30456;&#20284;&#24615;&#21576;&#36127;&#30456;&#20851;&#65292;&#35299;&#37322;&#20102;&#22270;&#25915;&#20987;&#32773;&#36830;&#25509;&#19981;&#30456;&#20284;&#33410;&#28857;&#23545;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#26032;&#39062;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#20851;&#31995;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#21457;&#29616;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21516;&#31867;&#22270;&#19978;&#23481;&#26131;&#21463;&#21040;&#32467;&#26500;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#40065;&#26834;&#27169;&#22411;&#65292;&#20197;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21516;&#31867;&#22270;&#19978;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#24322;&#31867;&#22270;&#19978;&#30340;&#33030;&#24369;&#24615;&#20173;&#28982;&#23384;&#22312;&#35768;&#22810;&#26410;&#35299;&#20043;&#35868;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#24320;&#22987;&#25506;&#32034;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#24322;&#31867;&#22270;&#19978;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#36127;&#20998;&#31867;&#25439;&#22833;&#30340;&#26356;&#26032;&#19982;&#22522;&#20110;&#37051;&#23621;&#29305;&#24449;&#30340;&#24130;&#21644;&#32858;&#21512;&#30340;&#25104;&#23545;&#30456;&#20284;&#24615;&#21576;&#36127;&#30456;&#20851;&#12290;&#36825;&#19968;&#29702;&#35770;&#35777;&#26126;&#35299;&#37322;&#20102;&#23454;&#35777;&#35266;&#23519;&#65292;&#21363;&#22270;&#25915;&#20987;&#32773;&#20542;&#21521;&#20110;&#22522;&#20110;&#37051;&#23621;&#29305;&#24449;&#32780;&#19981;&#26159;&#20010;&#20307;&#29305;&#24449;&#36830;&#25509;&#19981;&#30456;&#20284;&#33410;&#28857;&#23545;&#65292;&#26080;&#35770;&#26159;&#22312;&#21516;&#31867;&#22270;&#36824;&#26159;&#24322;&#31867;&#22270;&#19978;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#26032;&#39062;&#22320;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Despite the tremendous success of graph neural networks in learning relational data, it has been widely investigated that graph neural networks are vulnerable to structural attacks on homophilic graphs. Motivated by this, a surge of robust models is crafted to enhance the adversarial robustness of graph neural networks on homophilic graphs. However, the vulnerability based on heterophilic graphs remains a mystery to us. To bridge this gap, in this paper, we start to explore the vulnerability of graph neural networks on heterophilic graphs and theoretically prove that the update of the negative classification loss is negatively correlated with the pairwise similarities based on the powered aggregated neighbor features. This theoretical proof explains the empirical observations that the graph attacker tends to connect dissimilar node pairs based on the similarities of neighbor features instead of ego features both on homophilic and heterophilic graphs. In this way, we novelly introduce a
&lt;/p&gt;</description></item></channel></rss>