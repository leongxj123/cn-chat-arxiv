<rss version="2.0"><channel><title>Chat Arxiv cs.SI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SI</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;SimMLP&#65292;&#36890;&#36807;&#22312;&#22270;&#19978;&#26080;&#30417;&#30563;&#23398;&#20064;MLPs&#65292;&#25552;&#39640;&#20102;&#22312;&#24310;&#36831;&#25935;&#24863;&#30340;&#24212;&#29992;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.08918</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#30417;&#30563;&#22312;&#22270;&#19978;&#23398;&#20064;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#21152;&#36895;&#22270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Graph Inference Acceleration by Learning MLPs on Graphs without Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08918
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;SimMLP&#65292;&#36890;&#36807;&#22312;&#22270;&#19978;&#26080;&#30417;&#30563;&#23398;&#20064;MLPs&#65292;&#25552;&#39640;&#20102;&#22312;&#24310;&#36831;&#25935;&#24863;&#30340;&#24212;&#29992;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#32463;&#22312;&#21508;&#31181;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#26377;&#25928;&#24615;&#65292;&#20294;&#26159;&#23427;&#20204;&#23545;&#28040;&#24687;&#20256;&#36882;&#30340;&#20381;&#36182;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#24310;&#36831;&#25935;&#24863;&#30340;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#65292;&#27604;&#22914;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20174;GNNs&#20013;&#25552;&#21462;&#30693;&#35782;&#21040;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLPs&#65289;&#26469;&#21152;&#36895;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20219;&#21153;&#29305;&#23450;&#30340;&#26377;&#30417;&#30563;&#33976;&#39311;&#38480;&#21046;&#20102;&#23545;&#26410;&#35265;&#33410;&#28857;&#30340;&#27867;&#21270;&#65292;&#32780;&#22312;&#24310;&#36831;&#25935;&#24863;&#30340;&#24212;&#29992;&#20013;&#36825;&#31181;&#24773;&#20917;&#24456;&#24120;&#35265;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;SimMLP&#65292;&#29992;&#20110;&#22312;&#22270;&#19978;&#26080;&#30417;&#30563;&#23398;&#20064;MLPs&#65292;&#20197;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#12290;SimMLP&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#40784;GNNs&#21644;MLPs&#20043;&#38388;&#30340;&#33410;&#28857;&#29305;&#24449;&#21644;&#22270;&#32467;&#26500;&#20043;&#38388;&#30340;&#31934;&#32454;&#21644;&#27867;&#21270;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#26469;&#20943;&#36731;&#24179;&#20961;&#35299;&#30340;&#39118;&#38505;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08918v1 Announce Type: cross Abstract: Graph Neural Networks (GNNs) have demonstrated effectiveness in various graph learning tasks, yet their reliance on message-passing constraints their deployment in latency-sensitive applications such as financial fraud detection. Recent works have explored distilling knowledge from GNNs to Multi-Layer Perceptrons (MLPs) to accelerate inference. However, this task-specific supervised distillation limits generalization to unseen nodes, which are prevalent in latency-sensitive applications. To this end, we present \textbf{\textsc{SimMLP}}, a \textbf{\textsc{Sim}}ple yet effective framework for learning \textbf{\textsc{MLP}}s on graphs without supervision, to enhance generalization. \textsc{SimMLP} employs self-supervised alignment between GNNs and MLPs to capture the fine-grained and generalizable correlation between node features and graph structures, and proposes two strategies to alleviate the risk of trivial solutions. Theoretically, w
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22810;&#23610;&#24230;&#38669;&#22855;&#25955;&#23556;&#32593;&#32476;&#65288;MHSNs&#65289;&#65292;&#21033;&#29992;&#22810;&#23610;&#24230;&#22522;&#30784;&#35789;&#20856;&#21644;&#21367;&#31215;&#32467;&#26500;&#65292;&#29983;&#25104;&#23545;&#33410;&#28857;&#25490;&#21015;&#19981;&#21464;&#30340;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2311.10270</link><description>&lt;p&gt;
&#29992;&#20110;&#25968;&#25454;&#20998;&#26512;&#30340;&#22810;&#23610;&#24230;&#38669;&#22855;&#25955;&#23556;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multiscale Hodge Scattering Networks for Data Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10270
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22810;&#23610;&#24230;&#38669;&#22855;&#25955;&#23556;&#32593;&#32476;&#65288;MHSNs&#65289;&#65292;&#21033;&#29992;&#22810;&#23610;&#24230;&#22522;&#30784;&#35789;&#20856;&#21644;&#21367;&#31215;&#32467;&#26500;&#65292;&#29983;&#25104;&#23545;&#33410;&#28857;&#25490;&#21015;&#19981;&#21464;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25955;&#23556;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#21333;&#32431;&#22797;&#21512;&#20223;&#23556;&#19978;&#27979;&#37327;&#30340;&#20449;&#21495;&#65292;&#31216;&#20026;\emph{&#22810;&#23610;&#24230;&#38669;&#22855;&#25955;&#23556;&#32593;&#32476;}&#65288;MHSNs&#65289;&#12290;&#25105;&#20204;&#30340;&#26500;&#36896;&#22522;&#20110;&#21333;&#32431;&#22797;&#21512;&#20223;&#23556;&#19978;&#30340;&#22810;&#23610;&#24230;&#22522;&#30784;&#35789;&#20856;&#65292;&#21363;$\kappa$-GHWT&#21644;$\kappa$-HGLET&#65292;&#25105;&#20204;&#26368;&#36817;&#20026;&#32473;&#23450;&#21333;&#32431;&#22797;&#21512;&#20223;&#23556;&#20013;&#30340;&#32500;&#24230;$\kappa \in \mathbb{N}$&#25512;&#24191;&#20102;&#22522;&#20110;&#33410;&#28857;&#30340;&#24191;&#20041;&#21704;-&#27779;&#20160;&#21464;&#25442;&#65288;GHWT&#65289;&#21644;&#20998;&#23618;&#22270;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#21464;&#25442;&#65288;HGLET&#65289;&#12290;$\kappa$-GHWT&#21644;$\kappa$-HGLET&#37117;&#24418;&#25104;&#20887;&#20313;&#38598;&#21512;&#65288;&#21363;&#35789;&#20856;&#65289;&#30340;&#22810;&#23610;&#24230;&#22522;&#30784;&#21521;&#37327;&#21644;&#32473;&#23450;&#20449;&#21495;&#30340;&#30456;&#24212;&#25193;&#23637;&#31995;&#25968;&#12290;&#25105;&#20204;&#30340;MHSNs&#20351;&#29992;&#31867;&#20284;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#20998;&#23618;&#32467;&#26500;&#26469;&#32423;&#32852;&#35789;&#20856;&#31995;&#25968;&#27169;&#30340;&#30697;&#12290;&#25152;&#24471;&#29305;&#24449;&#23545;&#21333;&#32431;&#22797;&#21512;&#20223;&#23556;&#30340;&#37325;&#26032;&#25490;&#24207;&#19981;&#21464;&#65288;&#21363;&#33410;&#28857;&#25490;&#21015;&#30340;&#32622;&#25442;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10270v2 Announce Type: replace  Abstract: We propose new scattering networks for signals measured on simplicial complexes, which we call \emph{Multiscale Hodge Scattering Networks} (MHSNs). Our construction is based on multiscale basis dictionaries on simplicial complexes, i.e., the $\kappa$-GHWT and $\kappa$-HGLET, which we recently developed for simplices of dimension $\kappa \in \mathbb{N}$ in a given simplicial complex by generalizing the node-based Generalized Haar-Walsh Transform (GHWT) and Hierarchical Graph Laplacian Eigen Transform (HGLET). The $\kappa$-GHWT and the $\kappa$-HGLET both form redundant sets (i.e., dictionaries) of multiscale basis vectors and the corresponding expansion coefficients of a given signal. Our MHSNs use a layered structure analogous to a convolutional neural network (CNN) to cascade the moments of the modulus of the dictionary coefficients. The resulting features are invariant to reordering of the simplices (i.e., node permutation of the u
&lt;/p&gt;</description></item></channel></rss>