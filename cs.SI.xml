<rss version="2.0"><channel><title>Chat Arxiv cs.SI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SI</description><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22270;&#24418;&#21484;&#22238;&#30340;&#20934;&#30830;&#24615;&#21644;&#20559;&#35265;&#24494;&#32467;&#26500;&#65292;&#25506;&#35752;&#20102;&#23427;&#20204;&#19982;&#20154;&#31867;&#30340;&#24322;&#21516;&#20197;&#21450;&#23545;&#20854;&#20182;&#22270;&#24418;&#25512;&#29702;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.11821</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22270;&#24418;&#21484;&#22238;&#30340;&#24494;&#32467;&#26500;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Microstructures and Accuracy of Graph Recall by Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22270;&#24418;&#21484;&#22238;&#30340;&#20934;&#30830;&#24615;&#21644;&#20559;&#35265;&#24494;&#32467;&#26500;&#65292;&#25506;&#35752;&#20102;&#23427;&#20204;&#19982;&#20154;&#31867;&#30340;&#24322;&#21516;&#20197;&#21450;&#23545;&#20854;&#20182;&#22270;&#24418;&#25512;&#29702;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#25968;&#25454;&#23545;&#35768;&#22810;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#20854;&#20013;&#24456;&#22810;&#25968;&#25454;&#20197;&#25991;&#26412;&#26684;&#24335;&#25551;&#36848;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#20934;&#30830;&#22320;&#21484;&#22238;&#21644;&#32534;&#30721;&#20808;&#21069;&#25991;&#26412;&#20013;&#25551;&#36848;&#30340;&#22270;&#24418;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#38656;&#35201;&#23637;&#31034;&#30340;&#22522;&#26412;&#20294;&#20851;&#38190;&#33021;&#21147;&#65292;&#20197;&#25191;&#34892;&#28041;&#21450;&#22270;&#24418;&#32467;&#26500;&#20449;&#24687;&#30340;&#25512;&#29702;&#20219;&#21153;&#12290;&#20154;&#31867;&#22312;&#22270;&#24418;&#21484;&#22238;&#26041;&#38754;&#30340;&#34920;&#29616;&#24050;&#34987;&#35748;&#30693;&#31185;&#23398;&#23478;&#30740;&#31350;&#20102;&#20960;&#21313;&#24180;&#65292;&#21457;&#29616;&#20854;&#32463;&#24120;&#21576;&#29616;&#19982;&#20154;&#31867;&#22788;&#29702;&#31038;&#20250;&#20851;&#31995;&#19968;&#33268;&#30340;&#26576;&#20123;&#32467;&#26500;&#24615;&#20559;&#35265;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#25105;&#20204;&#24456;&#23569;&#20102;&#35299;LLMs&#22312;&#31867;&#20284;&#22270;&#24418;&#21484;&#22238;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#65306;&#23427;&#20204;&#21484;&#22238;&#30340;&#22270;&#24418;&#26159;&#21542;&#20063;&#21576;&#29616;&#26576;&#20123;&#20559;&#35265;&#27169;&#24335;&#65292;&#22914;&#26524;&#26159;&#65292;&#23427;&#20204;&#19982;&#20154;&#31867;&#30340;&#34920;&#29616;&#26377;&#20309;&#19981;&#21516;&#24182;&#22914;&#20309;&#24433;&#21709;&#20854;&#20182;&#22270;&#24418;&#25512;&#29702;&#20219;&#21153;&#65311;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#23545;LLMs&#36827;&#34892;&#22270;&#24418;&#21484;&#22238;&#30340;&#31995;&#32479;&#30740;&#31350;&#65292;&#30740;&#31350;&#20854;&#20934;&#30830;&#24615;&#21644;&#20559;&#35265;&#24494;&#32467;&#26500;&#65288;&#23616;&#37096;&#32467;&#26500;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11821v1 Announce Type: cross  Abstract: Graphs data is crucial for many applications, and much of it exists in the relations described in textual format. As a result, being able to accurately recall and encode a graph described in earlier text is a basic yet pivotal ability that LLMs need to demonstrate if they are to perform reasoning tasks that involve graph-structured information. Human performance at graph recall by has been studied by cognitive scientists for decades, and has been found to often exhibit certain structural patterns of bias that align with human handling of social relationships. To date, however, we know little about how LLMs behave in analogous graph recall tasks: do their recalled graphs also exhibit certain biased patterns, and if so, how do they compare with humans and affect other graph reasoning tasks? In this work, we perform the first systematical study of graph recall by LLMs, investigating the accuracy and biased microstructures (local structura
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#26694;&#26550;&#65292;&#33021;&#22815;&#20351;&#29992;&#19968;&#20010;&#27169;&#22411;&#35299;&#20915;&#21508;&#32423;&#21035;&#21644;&#21508;&#31867;&#22411;&#30340;&#22270;&#23398;&#20064;&#20219;&#21153;&#65292;&#36890;&#36807;&#28508;&#22312;&#22270;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#21644;&#39044;&#27979;&#33410;&#28857;&#12289;&#36793;&#21644;&#22270;&#32423;&#21035;&#30340;&#29305;&#24449;&#65292;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02518</link><description>&lt;p&gt;
&#28508;&#22312;&#22270;&#25193;&#25955;&#65306;&#19968;&#31181;&#22312;&#22270;&#19978;&#29983;&#25104;&#21644;&#39044;&#27979;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Latent Graph Diffusion: A Unified Framework for Generation and Prediction on Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#26694;&#26550;&#65292;&#33021;&#22815;&#20351;&#29992;&#19968;&#20010;&#27169;&#22411;&#35299;&#20915;&#21508;&#32423;&#21035;&#21644;&#21508;&#31867;&#22411;&#30340;&#22270;&#23398;&#20064;&#20219;&#21153;&#65292;&#36890;&#36807;&#28508;&#22312;&#22270;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#21644;&#39044;&#27979;&#33410;&#28857;&#12289;&#36793;&#21644;&#22270;&#32423;&#21035;&#30340;&#29305;&#24449;&#65292;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#29992;&#19968;&#20010;&#27169;&#22411;&#35299;&#20915;&#21508;&#32423;&#21035;&#65288;&#33410;&#28857;&#12289;&#36793;&#21644;&#22270;&#65289;&#21644;&#21508;&#31867;&#22411;&#65288;&#29983;&#25104;&#12289;&#22238;&#24402;&#21644;&#20998;&#31867;&#65289;&#30340;&#22270;&#23398;&#20064;&#20219;&#21153;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28508;&#22312;&#22270;&#25193;&#25955;&#65288;LGD&#65289;&#65292;&#19968;&#31181;&#33021;&#22815;&#21516;&#26102;&#29983;&#25104;&#33410;&#28857;&#12289;&#36793;&#21644;&#22270;&#32423;&#21035;&#29305;&#24449;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#36890;&#36807;&#23558;&#22270;&#32467;&#26500;&#21644;&#29305;&#24449;&#23884;&#20837;&#28508;&#22312;&#31354;&#38388;&#65292;&#21033;&#29992;&#24378;&#22823;&#30340;&#32534;&#30721;&#22120;&#36827;&#34892;&#35299;&#30721;&#65292;&#28982;&#21518;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#36825;&#20010;&#30446;&#26631;&#12290;LGD&#36824;&#21487;&#20197;&#36890;&#36807;&#29305;&#27530;&#35774;&#35745;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#26465;&#20214;&#29983;&#25104;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#22238;&#24402;&#21644;&#20998;&#31867;&#31561;&#39044;&#27979;&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#65288;&#26465;&#20214;&#65289;&#29983;&#25104;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;LGD&#33021;&#22815;&#36890;&#36807;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#26469;&#35299;&#20915;&#21508;&#32423;&#21035;&#21644;&#21508;&#31867;&#22411;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#20013;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21508;&#39033;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#25110;&#39640;&#24230;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose the first framework that enables solving graph learning tasks of all levels (node, edge and graph) and all types (generation, regression and classification) with one model. We first propose Latent Graph Diffusion (LGD), a generative model that can generate node, edge, and graph-level features of all categories simultaneously. We achieve this goal by embedding the graph structures and features into a latent space leveraging a powerful encoder which can also be decoded, then training a diffusion model in the latent space. LGD is also capable of conditional generation through a specifically designed cross-attention mechanism. Then we formulate prediction tasks including regression and classification as (conditional) generation, which enables our LGD to solve tasks of all levels and all types with provable guarantees. We verify the effectiveness of our framework with extensive experiments, where our models achieve state-of-the-art or highly competitive results acr
&lt;/p&gt;</description></item></channel></rss>