<rss version="2.0"><channel><title>Chat Arxiv cs.SI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;&#22270;&#35889;&#21644;&#37319;&#26679;&#29702;&#35770;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#26174;&#24335;&#30340;&#20004;&#23618;&#22270;&#35889;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20197;&#36739;&#23569;&#30340;&#32593;&#32476;&#26435;&#37325;&#25968;&#36924;&#36817;&#24102;&#38480;&#20449;&#21495;&#65292;&#24182;&#19988;&#22312;&#25910;&#25947;&#21040;&#22270;&#35889;&#30340;&#24207;&#21015;&#20013;&#23454;&#29616;&#20102;&#22312;&#36275;&#22815;&#22823;&#30340;&#22270;&#20043;&#38388;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13206</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#35770;&#21644;&#37319;&#26679;&#29702;&#35770;&#23454;&#29616;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#36801;&#31227;&#24615;
&lt;/p&gt;
&lt;p&gt;
Transferability of Graph Neural Networks using Graphon and Sampling Theories. (arXiv:2307.13206v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;&#22270;&#35889;&#21644;&#37319;&#26679;&#29702;&#35770;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#26174;&#24335;&#30340;&#20004;&#23618;&#22270;&#35889;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20197;&#36739;&#23569;&#30340;&#32593;&#32476;&#26435;&#37325;&#25968;&#36924;&#36817;&#24102;&#38480;&#20449;&#21495;&#65292;&#24182;&#19988;&#22312;&#25910;&#25947;&#21040;&#22270;&#35889;&#30340;&#24207;&#21015;&#20013;&#23454;&#29616;&#20102;&#22312;&#36275;&#22815;&#22823;&#30340;&#22270;&#20043;&#38388;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#25104;&#20026;&#22312;&#21508;&#20010;&#39046;&#22495;&#22788;&#29702;&#22522;&#20110;&#22270;&#30340;&#20449;&#24687;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;GNN&#30340;&#19968;&#20010;&#29702;&#24819;&#29305;&#24615;&#26159;&#21487;&#36801;&#31227;&#24615;&#65292;&#21363;&#35757;&#32451;&#22909;&#30340;&#32593;&#32476;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20132;&#25442;&#26469;&#33258;&#19981;&#21516;&#22270;&#30340;&#20449;&#24687;&#24182;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;&#26368;&#36817;&#19968;&#31181;&#25429;&#25417;GNN&#21487;&#36801;&#31227;&#24615;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#22270;&#35889;&#65292;&#23427;&#26159;&#23545;&#22823;&#22411;&#31264;&#23494;&#22270;&#30340;&#26497;&#38480;&#30340;&#23545;&#31216;&#21487;&#27979;&#20989;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#26174;&#24335;&#30340;&#20004;&#23618;&#22270;&#35889;&#31070;&#32463;&#32593;&#32476;&#65288;WNN&#65289;&#26550;&#26500;&#65292;&#23545;&#22270;&#35889;&#24212;&#29992;&#20110;GNN&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#33021;&#22815;&#20197;&#25351;&#23450;&#35823;&#24046;&#23481;&#38480;&#22312;&#26368;&#23569;&#30340;&#32593;&#32476;&#26435;&#37325;&#25968;&#19979;&#36924;&#36817;&#24102;&#38480;&#20449;&#21495;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#32467;&#26524;&#65292;&#22312;&#19968;&#20010;&#25910;&#25947;&#21040;&#22270;&#35889;&#30340;&#24207;&#21015;&#20013;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#20004;&#23618;GNN&#22312;&#25152;&#26377;&#36275;&#22815;&#22823;&#30340;&#22270;&#20043;&#38388;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#35299;&#20915;&#20102;&#30830;&#23450;&#24615;&#21152;&#26435;&#22270;&#21644;&#31616;&#21333;&#38543;&#26426;&#22270;&#20043;&#38388;&#30340;&#21487;&#36801;&#31227;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have become powerful tools for processing graph-based information in various domains. A desirable property of GNNs is transferability, where a trained network can swap in information from a different graph without retraining and retain its accuracy. A recent method of capturing transferability of GNNs is through the use of graphons, which are symmetric, measurable functions representing the limit of large dense graphs. In this work, we contribute to the application of graphons to GNNs by presenting an explicit two-layer graphon neural network (WNN) architecture. We prove its ability to approximate bandlimited signals within a specified error tolerance using a minimal number of network weights. We then leverage this result, to establish the transferability of an explicit two-layer GNN over all sufficiently large graphs in a sequence converging to a graphon. Our work addresses transferability between both deterministic weighted graphs and simple random graphs
&lt;/p&gt;</description></item></channel></rss>