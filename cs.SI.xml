<rss version="2.0"><channel><title>Chat Arxiv cs.SI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SI</description><item><title>&#35813;&#35770;&#25991;&#20174;&#22522;&#20110;&#30693;&#35782;&#30340;&#35282;&#24230;&#20840;&#38754;&#35843;&#26597;&#21644;&#20998;&#26512;&#20102;&#22270;&#22522;&#30784;&#27169;&#22411;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#28041;&#21450;&#24494;&#35266;&#21644;&#23439;&#35266;&#30693;&#35782;&#65292;&#21253;&#25324;9&#20010;&#30693;&#35782;&#31867;&#21035;&#12289;25&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#20197;&#21450;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#36866;&#24212;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.16137</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#22270;&#22522;&#30784;&#27169;&#22411;&#30340;&#35843;&#26597;&#65306;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Survey on Self-Supervised Pre-Training of Graph Foundation Models: A Knowledge-Based Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16137
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20174;&#22522;&#20110;&#30693;&#35782;&#30340;&#35282;&#24230;&#20840;&#38754;&#35843;&#26597;&#21644;&#20998;&#26512;&#20102;&#22270;&#22522;&#30784;&#27169;&#22411;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#28041;&#21450;&#24494;&#35266;&#21644;&#23439;&#35266;&#30693;&#35782;&#65292;&#21253;&#25324;9&#20010;&#30693;&#35782;&#31867;&#21035;&#12289;25&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#20197;&#21450;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#36866;&#24212;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#33258;&#30417;&#30563;&#23398;&#20064;&#29616;&#22312;&#26159;&#39044;&#35757;&#32451;&#22270;&#22522;&#30784;&#27169;&#22411;&#30340;&#39318;&#36873;&#26041;&#27861;&#65292;&#21253;&#25324;&#22270;&#31070;&#32463;&#32593;&#32476;&#12289;&#22270;&#21464;&#25442;&#22120;&#65292;&#20197;&#21450;&#26356;&#36817;&#26399;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#22270;&#27169;&#22411;&#12290;&#25991;&#31456;&#20840;&#38754;&#35843;&#26597;&#21644;&#20998;&#26512;&#20102;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35282;&#19979;&#30340;&#22270;&#22522;&#30784;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#21253;&#25324;&#24494;&#35266;&#65288;&#33410;&#28857;&#12289;&#38142;&#25509;&#31561;&#65289;&#21644;&#23439;&#35266;&#30693;&#35782;&#65288;&#31751;&#12289;&#20840;&#23616;&#32467;&#26500;&#31561;&#65289;&#12290;&#28085;&#30422;&#20102;&#20849;&#35745;9&#20010;&#30693;&#35782;&#31867;&#21035;&#21644;25&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20197;&#21450;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#36866;&#24212;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16137v1 Announce Type: new  Abstract: Graph self-supervised learning is now a go-to method for pre-training graph foundation models, including graph neural networks, graph transformers, and more recent large language model (LLM)-based graph models. There is a wide variety of knowledge patterns embedded in the structure and properties of graphs which may be used for pre-training, but we lack a systematic overview of self-supervised pre-training tasks from the perspective of graph knowledge. In this paper, we comprehensively survey and analyze the pre-training tasks of graph foundation models from a knowledge-based perspective, consisting of microscopic (nodes, links, etc) and macroscopic knowledge (clusters, global structure, etc). It covers a total of 9 knowledge categories and 25 pre-training tasks, as well as various downstream task adaptation strategies. Furthermore, an extensive list of the related papers with detailed metadata is provided at https://github.com/Newiz430/
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#22823;&#35268;&#27169;&#22270;&#30340;&#35299;&#32544;&#32467;&#20957;&#32858;&#26041;&#27861;DisCo&#65292;&#36890;&#36807;&#33410;&#28857;&#21644;&#36793;&#30340;&#20957;&#32858;&#27169;&#22359;&#23454;&#29616;&#20102;&#23545;&#22823;&#35268;&#27169;&#22270;&#30340;&#39640;&#25928;&#32553;&#20957;&#65292;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#21644;&#21387;&#32553;&#22270;&#30340;&#20445;&#30495;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.12231</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#22270;&#30340;&#35299;&#32544;&#32467;&#20957;&#32858;
&lt;/p&gt;
&lt;p&gt;
Disentangled Condensation for Large-scale Graphs. (arXiv:2401.12231v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#22823;&#35268;&#27169;&#22270;&#30340;&#35299;&#32544;&#32467;&#20957;&#32858;&#26041;&#27861;DisCo&#65292;&#36890;&#36807;&#33410;&#28857;&#21644;&#36793;&#30340;&#20957;&#32858;&#27169;&#22359;&#23454;&#29616;&#20102;&#23545;&#22823;&#35268;&#27169;&#22270;&#30340;&#39640;&#25928;&#32553;&#20957;&#65292;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#21644;&#21387;&#32553;&#22270;&#30340;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#35299;&#32544;&#32467;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#36259;&#30340;&#25216;&#26415;&#65292;&#20026;&#22823;&#35268;&#27169;&#22270;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#32039;&#20945;&#20294;&#20449;&#24687;&#20016;&#23500;&#30340;&#23567;&#22270;&#65292;&#20197;&#33410;&#30465;&#22823;&#35268;&#27169;&#22270;&#23398;&#20064;&#30340;&#26114;&#36149;&#25104;&#26412;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#65292;&#20294;&#20808;&#21069;&#30340;&#22270;&#35299;&#32544;&#32467;&#26041;&#27861;&#24120;&#24120;&#37319;&#29992;&#32416;&#32544;&#30340;&#32553;&#20957;&#31574;&#30053;&#65292;&#21516;&#26102;&#28041;&#21450;&#33410;&#28857;&#21644;&#36793;&#30340;&#32553;&#20957;&#65292;&#23548;&#33268;&#22823;&#37327;&#30340;GPU&#20869;&#23384;&#38656;&#27714;&#12290;&#36825;&#31181;&#32416;&#32544;&#30340;&#31574;&#30053;&#26497;&#22823;&#22320;&#38459;&#30861;&#20102;&#22270;&#35299;&#32544;&#32467;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#21066;&#24369;&#20102;&#23427;&#23545;&#26497;&#22823;&#35268;&#27169;&#22270;&#30340;&#32553;&#20957;&#21644;&#39640;&#20445;&#30495;&#24230;&#21387;&#32553;&#22270;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#22823;&#35268;&#27169;&#22270;&#30340;&#35299;&#32544;&#32467;&#20957;&#32858;&#65292;&#31616;&#31216;&#20026;DisCo&#65292;&#20197;&#25552;&#20379;&#21487;&#25193;&#23637;&#30340;&#22270;&#35299;&#32544;&#32467;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#35268;&#27169;&#30340;&#22270;&#12290;DisCo&#30340;&#26680;&#24515;&#26159;&#20004;&#20010;&#20114;&#34917;&#30340;&#32452;&#20214;&#65292;&#21363;&#33410;&#28857;&#21644;&#36793;&#30340;&#20957;&#32858;&#27169;&#22359;&#65292;&#22312;&#35299;&#32544;&#30340;&#26041;&#24335;&#19979;&#23454;&#29616;&#33410;&#28857;&#21644;&#36793;&#30340;&#20957;&#32858;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph condensation has emerged as an intriguing technique to provide Graph Neural Networks for large-scale graphs with a more compact yet informative small graph to save the expensive costs of large-scale graph learning. Despite the promising results achieved, previous graph condensation methods often employ an entangled condensation strategy that involves condensing nodes and edges simultaneously, leading to substantial GPU memory demands. This entangled strategy has considerably impeded the scalability of graph condensation, impairing its capability to condense extremely large-scale graphs and produce condensed graphs with high fidelity. Therefore, this paper presents Disentangled Condensation for large-scale graphs, abbreviated as DisCo, to provide scalable graph condensation for graphs of varying sizes. At the heart of DisCo are two complementary components, namely node and edge condensation modules, that realize the condensation of nodes and edges in a disentangled manner. In the 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#35838;&#22530;&#25968;&#25454;&#20998;&#26512;&#22797;&#21046;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#21450;&#36825;&#31181;&#26041;&#27861;&#23545;&#23398;&#29983;&#12289;&#25945;&#32946;&#32773;&#21644;&#31185;&#23398;&#23478;&#30340;&#25104;&#26412;&#19982;&#25910;&#30410;&#12290;&#21516;&#26102;&#65292;&#23398;&#29983;&#23545;&#25968;&#25454;&#30340;&#39044;&#26399;&#19982;&#23454;&#38469;&#24773;&#20917;&#23384;&#22312;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2308.16491</link><description>&lt;p&gt;
&#35838;&#22530;&#25968;&#25454;&#20998;&#26512;&#22797;&#21046;&#65306;&#25945;&#23398;&#29983;&#65292;&#21516;&#26102;&#27979;&#35797;&#31185;&#23398;
&lt;/p&gt;
&lt;p&gt;
In-class Data Analysis Replications: Teaching Students while Testing Science. (arXiv:2308.16491v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16491
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#35838;&#22530;&#25968;&#25454;&#20998;&#26512;&#22797;&#21046;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#21450;&#36825;&#31181;&#26041;&#27861;&#23545;&#23398;&#29983;&#12289;&#25945;&#32946;&#32773;&#21644;&#31185;&#23398;&#23478;&#30340;&#25104;&#26412;&#19982;&#25910;&#30410;&#12290;&#21516;&#26102;&#65292;&#23398;&#29983;&#23545;&#25968;&#25454;&#30340;&#39044;&#26399;&#19982;&#23454;&#38469;&#24773;&#20917;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#27491;&#38754;&#20020;&#21487;&#37325;&#22797;&#24615;&#21361;&#26426;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#23558;&#25968;&#25454;&#20998;&#26512;&#22797;&#21046;&#32435;&#20837;&#35838;&#22530;&#20316;&#20026;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#28508;&#22312;&#30340;&#22909;&#22788;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#19968;&#26041;&#27861;&#26159;&#21542;&#21487;&#34892;&#65292;&#22914;&#26524;&#21487;&#34892;&#65292;&#28041;&#21450;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;-&#23398;&#29983;&#12289;&#25945;&#32946;&#32773;&#21644;&#31185;&#23398;&#23478;-&#24212;&#35813;&#26399;&#26395;&#20160;&#20040;&#12290;&#23398;&#29983;&#33021;&#22815;&#22312;&#35838;&#22530;&#19978;&#36827;&#34892;&#25968;&#25454;&#20998;&#26512;&#22797;&#21046;&#21527;&#65311;&#25945;&#32946;&#32773;&#30340;&#25104;&#26412;&#19982;&#25910;&#30410;&#22914;&#20309;&#65311;&#36825;&#20010;&#35299;&#20915;&#26041;&#26696;&#22914;&#20309;&#24110;&#21161;&#35780;&#20272;&#21644;&#25913;&#36827;&#31185;&#23398;&#30340;&#29616;&#29366;&#65311;&#26412;&#30740;&#31350;&#22312;EPFL&#25945;&#25480;&#30340;&#24212;&#29992;&#25968;&#25454;&#20998;&#26512;&#35838;&#31243;&#65288;CS-401&#65289;&#30340;&#39033;&#30446;&#37096;&#20998;&#20013;&#32435;&#20837;&#20102;&#25968;&#25454;&#20998;&#26512;&#22797;&#21046;&#65288;N=354&#21517;&#23398;&#29983;&#65289;&#12290;&#22312;&#27492;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#35838;&#31243;&#26399;&#38388;&#36827;&#34892;&#30340;&#35843;&#26597;&#25552;&#21069;&#36827;&#34892;&#27880;&#20876;&#30340;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#23398;&#29983;&#21487;&#20197;&#22797;&#21046;&#20808;&#21069;&#21457;&#34920;&#30340;&#31185;&#23398;&#35770;&#25991;&#65292;&#22823;&#37096;&#20998;&#26159;&#23450;&#24615;&#30340;&#65292;&#26377;&#20123;&#26159;&#23436;&#20840;&#19968;&#26679;&#30340;&#12290;&#25105;&#20204;&#21457;&#29616;&#23398;&#29983;&#23545;&#25968;&#25454;&#30340;&#39044;&#26399;&#19982;&#23454;&#38469;&#24773;&#20917;&#23384;&#22312;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Science is facing a reproducibility crisis. Previous work has proposed incorporating data analysis replications into classrooms as a potential solution. However, despite the potential benefits, it is unclear whether this approach is feasible, and if so, what the involved stakeholders-students, educators, and scientists-should expect from it. Can students perform a data analysis replication over the course of a class? What are the costs and benefits for educators? And how can this solution help benchmark and improve the state of science?  In the present study, we incorporated data analysis replications in the project component of the Applied Data Analysis course (CS-401) taught at EPFL (N=354 students). Here we report pre-registered findings based on surveys administered throughout the course. First, we demonstrate that students can replicate previously published scientific papers, most of them qualitatively and some exactly. We find discrepancies between what students expect of data an
&lt;/p&gt;</description></item></channel></rss>