<rss version="2.0"><channel><title>Chat Arxiv cs.SI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SI</description><item><title>Crowd-PrefRL&#26159;&#19968;&#31181;&#22522;&#20110;&#20247;&#21253;&#30340;&#20559;&#22909;&#21453;&#39304;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#26469;&#33258;&#32676;&#20307;&#30340;&#21453;&#39304;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#19988;&#33021;&#22815;&#24378;&#22823;&#22320;&#32858;&#21512;&#32676;&#20307;&#20559;&#22909;&#21453;&#39304;&#24182;&#20272;&#35745;&#29992;&#25143;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10941</link><description>&lt;p&gt;
Crowd-PrefRL: &#22522;&#20110;&#20247;&#21253;&#30340;&#20559;&#22909;&#21453;&#39304;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Crowd-PrefRL: Preference-Based Reward Learning from Crowds. (arXiv:2401.10941v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10941
&lt;/p&gt;
&lt;p&gt;
Crowd-PrefRL&#26159;&#19968;&#31181;&#22522;&#20110;&#20247;&#21253;&#30340;&#20559;&#22909;&#21453;&#39304;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#26469;&#33258;&#32676;&#20307;&#30340;&#21453;&#39304;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#19988;&#33021;&#22815;&#24378;&#22823;&#22320;&#32858;&#21512;&#32676;&#20307;&#20559;&#22909;&#21453;&#39304;&#24182;&#20272;&#35745;&#29992;&#25143;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#34892;&#20026;&#23545;&#30340;&#20559;&#22909;&#36827;&#34892;&#20154;&#31867;&#21453;&#39304;&#26469;&#35757;&#32451;&#26234;&#33021;&#20307;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#38590;&#20197;&#25351;&#23450;&#25968;&#20540;&#22870;&#21169;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#26399;&#26395;&#30340;&#34892;&#20026;&#12290;&#23613;&#31649;&#36825;&#20010;&#33539;&#24335;&#21033;&#29992;&#20102;&#20154;&#31867;&#30340;&#21453;&#39304;&#65292;&#20294;&#30446;&#21069;&#23558;&#21453;&#39304;&#35270;&#20026;&#21333;&#20010;&#20154;&#31867;&#29992;&#25143;&#25152;&#32473;&#20986;&#30340;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20197;&#24378;&#22823;&#30340;&#26041;&#24335;&#21512;&#24182;&#26469;&#33258;&#32676;&#20307;&#65288;&#21363;&#29992;&#25143;&#38598;&#21512;&#65289;&#30340;&#20559;&#22909;&#21453;&#39304;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#32780;&#20351;&#29992;&#26469;&#33258;&#22810;&#20010;&#29992;&#25143;&#30340;&#21453;&#39304;&#26469;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#38382;&#39064;&#20173;&#28982;&#34987;&#30740;&#31350;&#19981;&#36275;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Crowd-PrefRL&#65292;&#19968;&#20010;&#21033;&#29992;&#26469;&#33258;&#32676;&#20307;&#30340;&#21453;&#39304;&#36827;&#34892;&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#12290;&#36825;&#39033;&#24037;&#20316;&#23637;&#31034;&#20102;&#21033;&#29992;&#26410;&#30693;&#19987;&#19994;&#27700;&#24179;&#21644;&#21487;&#38752;&#24615;&#30340;&#32676;&#20307;&#20559;&#22909;&#21453;&#39304;&#26469;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#30340;&#21487;&#34892;&#24615;&#12290;Crowd-PrefRL&#19981;&#20165;&#33021;&#22815;&#24378;&#22823;&#22320;&#32858;&#21512;&#32676;&#20307;&#20559;&#22909;&#21453;&#39304;&#65292;&#36824;&#33021;&#22815;&#20272;&#35745;&#27599;&#20010;&#29992;&#25143;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Preference-based reinforcement learning (RL) provides a framework to train agents using human feedback through pairwise preferences over pairs of behaviors, enabling agents to learn desired behaviors when it is difficult to specify a numerical reward function. While this paradigm leverages human feedback, it currently treats the feedback as given by a single human user. Meanwhile, incorporating preference feedback from crowds (i.e. ensembles of users) in a robust manner remains a challenge, and the problem of training RL agents using feedback from multiple human users remains understudied. In this work, we introduce Crowd-PrefRL, a framework for performing preference-based RL leveraging feedback from crowds. This work demonstrates the viability of learning reward functions from preference feedback provided by crowds of unknown expertise and reliability. Crowd-PrefRL not only robustly aggregates the crowd preference feedback, but also estimates the reliability of each user within the cr
&lt;/p&gt;</description></item></channel></rss>