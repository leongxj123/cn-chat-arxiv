<rss version="2.0"><channel><title>Chat Arxiv cs.SI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SI</description><item><title>&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36229;&#39640;&#32500;&#35745;&#31639;&#36827;&#34892;&#21333;&#27425;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#39640;&#32500;&#31354;&#38388;&#24182;&#21033;&#29992;HD&#36816;&#31639;&#31526;&#36827;&#34892;&#20449;&#24687;&#32858;&#21512;&#65292;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#35745;&#31639;&#26114;&#36149;&#30340;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2402.17073</link><description>&lt;p&gt;
&#20351;&#29992;&#36229;&#39640;&#32500;&#35745;&#31639;&#36827;&#34892;&#21333;&#27425;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
One-Shot Graph Representation Learning Using Hyperdimensional Computing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17073
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36229;&#39640;&#32500;&#35745;&#31639;&#36827;&#34892;&#21333;&#27425;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#39640;&#32500;&#31354;&#38388;&#24182;&#21033;&#29992;HD&#36816;&#31639;&#31526;&#36827;&#34892;&#20449;&#24687;&#32858;&#21512;&#65292;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#35745;&#31639;&#26114;&#36149;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#31616;&#21333;&#12289;&#24555;&#36895;&#12289;&#39640;&#25928;&#30340;&#21322;&#30417;&#30563;&#22270;&#23398;&#20064;&#26041;&#27861;&#12290;&#25152;&#25552;&#26041;&#27861;&#21033;&#29992;&#36229;&#39640;&#32500;&#35745;&#31639;&#65292;&#23558;&#25968;&#25454;&#26679;&#26412;&#20351;&#29992;&#38543;&#26426;&#25237;&#24433;&#32534;&#30721;&#21040;&#39640;&#32500;&#31354;&#38388;&#65288;&#31616;&#31216;HD&#31354;&#38388;&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#33410;&#28857;&#34920;&#31034;&#30340;&#21333;&#23556;&#24615;&#36136;&#30340;&#36229;&#39640;&#32500;&#22270;&#23398;&#20064;&#65288;HDGL&#65289;&#31639;&#27861;&#12290;HDGL&#23558;&#33410;&#28857;&#29305;&#24449;&#26144;&#23556;&#21040;HD&#31354;&#38388;&#65292;&#28982;&#21518;&#20351;&#29992;HD&#36816;&#31639;&#31526;&#65288;&#22914;&#25414;&#32465;&#21644;&#32465;&#23450;&#65289;&#26469;&#32858;&#21512;&#27599;&#20010;&#33410;&#28857;&#30340;&#23616;&#37096;&#37051;&#22495;&#20449;&#24687;&#12290;&#23545;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;HDGL&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#35745;&#31639;&#26114;&#36149;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17073v1 Announce Type: cross  Abstract: We present a novel, simple, fast, and efficient approach for semi-supervised learning on graphs. The proposed approach takes advantage of hyper-dimensional computing which encodes data samples using random projections into a high dimensional space (HD space for short). Specifically, we propose a Hyper-dimensional Graph Learning (HDGL) algorithm that leverages the injectivity property of the node representations of a family of graph neural networks. HDGL maps node features to the HD space and then uses HD operators such as bundling and binding to aggregate information from the local neighborhood of each node. Results of experiments with widely used benchmark data sets show that HDGL achieves predictive performance that is competitive with the state-of-the-art deep learning methods, without the need for computationally expensive training.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#28040;&#36153;&#32773;&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#21382;&#21490;&#26159;&#30740;&#31350;&#20998;&#20139;&#34394;&#20551;&#26032;&#38395;&#21160;&#26426;&#30340;&#19968;&#31181;&#34987;&#20302;&#20272;&#30340;&#25968;&#25454;&#26469;&#28304;&#12290;&#36890;&#36807;&#23545;&#24086;&#23376;&#21382;&#21490;&#25552;&#21462;&#30340;&#25991;&#26412;&#32447;&#32034;&#65292;&#25105;&#20204;&#21457;&#29616;&#34394;&#20551;&#26032;&#38395;&#20998;&#20139;&#32773;&#22312;&#35328;&#36766;&#19978;&#26356;&#22810;&#28041;&#21450;&#24868;&#24594;&#12289;&#23447;&#25945;&#21644;&#26435;&#21147;&#12290;&#24182;&#19988;&#65292;&#36890;&#36807;&#23558;&#24086;&#23376;&#21382;&#21490;&#20013;&#30340;&#25991;&#26412;&#32447;&#32034;&#21152;&#20837;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20998;&#20139;&#34394;&#20551;&#26032;&#38395;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#28608;&#27963;&#23447;&#25945;&#20215;&#20540;&#35266;&#21644;&#20943;&#23569;&#24868;&#24594;&#65292;&#21487;&#20197;&#20943;&#23569;&#34394;&#20551;&#26032;&#38395;&#30340;&#20998;&#20139;&#21644;&#26356;&#24191;&#27867;&#30340;&#20998;&#20139;&#12290;</title><link>http://arxiv.org/abs/2203.10560</link><description>&lt;p&gt;
&#25552;&#21319;&#34394;&#20551;&#26032;&#38395;&#32531;&#35299;&#65306;&#26469;&#33258;&#20998;&#20139;&#32773;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#21382;&#21490;&#30340;&#27934;&#23519;&#21147;
&lt;/p&gt;
&lt;p&gt;
Empowering Fake-News Mitigation: Insights from Sharers' Social Media Post-Histories. (arXiv:2203.10560v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.10560
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#28040;&#36153;&#32773;&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#21382;&#21490;&#26159;&#30740;&#31350;&#20998;&#20139;&#34394;&#20551;&#26032;&#38395;&#21160;&#26426;&#30340;&#19968;&#31181;&#34987;&#20302;&#20272;&#30340;&#25968;&#25454;&#26469;&#28304;&#12290;&#36890;&#36807;&#23545;&#24086;&#23376;&#21382;&#21490;&#25552;&#21462;&#30340;&#25991;&#26412;&#32447;&#32034;&#65292;&#25105;&#20204;&#21457;&#29616;&#34394;&#20551;&#26032;&#38395;&#20998;&#20139;&#32773;&#22312;&#35328;&#36766;&#19978;&#26356;&#22810;&#28041;&#21450;&#24868;&#24594;&#12289;&#23447;&#25945;&#21644;&#26435;&#21147;&#12290;&#24182;&#19988;&#65292;&#36890;&#36807;&#23558;&#24086;&#23376;&#21382;&#21490;&#20013;&#30340;&#25991;&#26412;&#32447;&#32034;&#21152;&#20837;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20998;&#20139;&#34394;&#20551;&#26032;&#38395;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#28608;&#27963;&#23447;&#25945;&#20215;&#20540;&#35266;&#21644;&#20943;&#23569;&#24868;&#24594;&#65292;&#21487;&#20197;&#20943;&#23569;&#34394;&#20551;&#26032;&#38395;&#30340;&#20998;&#20139;&#21644;&#26356;&#24191;&#27867;&#30340;&#20998;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#20551;&#20449;&#24687;&#26159;&#19968;&#20010;&#20840;&#29699;&#24615;&#38382;&#39064;&#65292;&#38480;&#21046;&#20854;&#20256;&#25773;&#23545;&#20445;&#25252;&#27665;&#20027;&#12289;&#20844;&#20849;&#21355;&#29983;&#21644;&#28040;&#36153;&#32773;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#35748;&#20026;&#28040;&#36153;&#32773;&#33258;&#24049;&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#21382;&#21490;&#26159;&#19968;&#20010;&#34987;&#20302;&#20272;&#30340;&#25968;&#25454;&#26469;&#28304;&#65292;&#29992;&#20110;&#30740;&#31350;&#26159;&#20160;&#20040;&#23548;&#33268;&#20182;&#20204;&#20998;&#20139;&#34394;&#20551;&#26032;&#38395;&#38142;&#25509;&#12290;&#22312;&#31532;&#19968;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20174;&#24086;&#23376;&#21382;&#21490;&#20013;&#25552;&#21462;&#30340;&#25991;&#26412;&#32447;&#32034;&#22914;&#20309;&#21306;&#20998;&#34394;&#20551;&#26032;&#38395;&#30340;&#20998;&#20139;&#32773;&#21644;&#38543;&#26426;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#20197;&#21450;&#20854;&#20182;&#22312;&#35823;&#23548;&#20449;&#24687;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#20154;&#12290;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#34394;&#20551;&#26032;&#38395;&#30340;&#20998;&#20139;&#32773;&#20351;&#29992;&#26356;&#22810;&#19982;&#24868;&#24594;&#12289;&#23447;&#25945;&#21644;&#26435;&#21147;&#30456;&#20851;&#30340;&#35789;&#27719;&#12290;&#22312;&#31532;&#20108;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;&#24086;&#23376;&#21382;&#21490;&#20013;&#28155;&#21152;&#25991;&#26412;&#32447;&#32034;&#22914;&#20309;&#25552;&#39640;&#27169;&#22411;&#39044;&#27979;&#35841;&#26377;&#21487;&#33021;&#20998;&#20139;&#34394;&#20551;&#26032;&#38395;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#31532;&#19977;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#20174;&#31532;&#19968;&#39033;&#30740;&#31350;&#20013;&#25512;&#23548;&#20986;&#30340;&#20004;&#31181;&#32531;&#35299;&#31574;&#30053;&#36827;&#34892;&#20102;&#21021;&#27493;&#27979;&#35797;&#65292;&#21363;&#28608;&#27963;&#23447;&#25945;&#20215;&#20540;&#35266;&#21644;&#20943;&#23569;&#24868;&#24594;&#65292;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#20943;&#23569;&#34394;&#20551;&#26032;&#38395;&#30340;&#20998;&#20139;&#21644;&#26356;&#24191;&#27867;&#30340;&#20998;&#20139;&#12290;&#22312;&#31532;&#22235;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#35843;&#26597;&#32467;&#26524;&#19982;&#29992;&#25143;&#30340;&#39564;&#35777;&#25512;&#29305;&#32080;&#21512;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Misinformation is a global concern and limiting its spread is critical for protecting democracy, public health, and consumers. We propose that consumers' own social media post-histories are an underutilized data source to study what leads them to share links to fake-news. In Study 1, we explore how textual cues extracted from post-histories distinguish fake-news sharers from random social media users and others in the misinformation ecosystem. Among other results, we find across two datasets that fake-news sharers use more words related to anger, religion and power. In Study 2, we show that adding textual cues from post-histories improves the accuracy of models to predict who is likely to share fake-news. In Study 3, we provide a preliminary test of two mitigation strategies deduced from Study 1 - activating religious values and reducing anger - and find that they reduce fake-news sharing and sharing more generally. In Study 4, we combine survey responses with users' verified Twitter p
&lt;/p&gt;</description></item></channel></rss>