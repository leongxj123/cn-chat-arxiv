# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [OmniJet-$\alpha$: The first cross-task foundation model for particle physics](https://arxiv.org/abs/2403.05618) | 基于物理数据和变压器架构，OmniJet-$\alpha$是首个跨任务基础模型，引入了全面的评估方法并展示了在无监督问题上的迁移学习。 |

# 详细

[^1]: OmniJet-$\alpha$: 粒子物理学的首个跨任务基础模型

    OmniJet-$\alpha$: The first cross-task foundation model for particle physics

    [https://arxiv.org/abs/2403.05618](https://arxiv.org/abs/2403.05618)

    基于物理数据和变压器架构，OmniJet-$\alpha$是首个跨任务基础模型，引入了全面的评估方法并展示了在无监督问题上的迁移学习。

    

    基础模型是多数据集和多任务的机器学习方法，一经预训练，便可被微调用于各种不同的应用。成功开发出这种通用物理数据模型将是一项重大突破，因为它们可以提高可实现的物理性能，同时大幅减少所需的训练时间和数据量。我们在这一挑战上取得了显著进展。首先，引入了一套全面的评估方法，来评判从物理数据转换为适合变压器架构（基础模型的通用骨干）进行自回归生成粒子喷流的表示质量。这些措施支持了相较于先前工作的更高保真度的标记化的选择。最后，我们展示了在无监督问题（喷流生成）之间的迁移学习。

    arXiv:2403.05618v1 Announce Type: cross  Abstract: Foundation models are multi-dataset and multi-task machine learning methods that once pre-trained can be fine-tuned for a large variety of downstream applications. The successful development of such general-purpose models for physics data would be a major breakthrough as they could improve the achievable physics performance while at the same time drastically reduce the required amount of training time and data.   We report significant progress on this challenge on several fronts. First, a comprehensive set of evaluation methods is introduced to judge the quality of an encoding from physics data into a representation suitable for the autoregressive generation of particle jets with transformer architectures (the common backbone of foundation models). These measures motivate the choice of a higher-fidelity tokenization compared to previous works. Finally, we demonstrate transfer learning between an unsupervised problem (jet generation) an
    

