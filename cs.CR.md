# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Is Watermarking LLM-Generated Code Robust?](https://arxiv.org/abs/2403.17983) | 该研究探讨了现有水印技术在大型语言模型生成的Python代码上的鲁棒性，发现容易通过保留语义转换来移除这些水印。 |
| [^2] | [Threats, Attacks, and Defenses in Machine Unlearning: A Survey](https://arxiv.org/abs/2403.13682) | 机器遗忘（MU）通过知识去除过程来解决训练数据相关的人工智能治理问题，提高了AI系统的安全和负责任使用。 |
| [^3] | [Graph Topology Learning Under Privacy Constraints.](http://arxiv.org/abs/2301.06662) | 在隐私约束下，我们提出了一个框架，联合学习为本地客户定制的个性化图以及共识图，以推断潜在图拓扑，同时在保护隐私的情况下处理分布式客户端的数据。 |

# 详细

[^1]: LLM生成代码的水印技术是否具有鲁棒性？

    Is Watermarking LLM-Generated Code Robust?

    [https://arxiv.org/abs/2403.17983](https://arxiv.org/abs/2403.17983)

    该研究探讨了现有水印技术在大型语言模型生成的Python代码上的鲁棒性，发现容易通过保留语义转换来移除这些水印。

    

    我们首次研究了现有水印技术在大型语言模型生成的Python代码上的鲁棒性。尽管现有作品表明水印技术对自然语言具有鲁棒性，但我们发现通过保留语义的转换很容易移除代码上的这些水印。

    arXiv:2403.17983v1 Announce Type: cross  Abstract: We present the first study of the robustness of existing watermarking techniques on Python code generated by large language models. Although existing works showed that watermarking can be robust for natural language, we show that it is easy to remove these watermarks on code by semantic-preserving transformations.
    
[^2]: 机器学习中的威胁、攻击和防御：一项调查

    Threats, Attacks, and Defenses in Machine Unlearning: A Survey

    [https://arxiv.org/abs/2403.13682](https://arxiv.org/abs/2403.13682)

    机器遗忘（MU）通过知识去除过程来解决训练数据相关的人工智能治理问题，提高了AI系统的安全和负责任使用。

    

    机器遗忘（MU）最近引起了相当大的关注，因为它有潜力通过从训练的机器学习模型中消除特定数据的影响来实现安全人工智能。这个被称为知识去除的过程解决了与训练数据相关的人工智能治理问题，如数据质量、敏感性、版权限制和过时性。这种能力对于确保遵守诸如被遗忘权等隐私法规也至关重要。此外，有效的知识去除有助于减轻有害结果的风险，防范偏见、误导和未经授权的数据利用，从而增强了AI系统的安全和负责任使用。已经开展了设计高效的遗忘方法的工作，通过研究MU服务以与现有的机器学习作为服务集成，使用户能够提交请求从训练语料库中删除特定数据。

    arXiv:2403.13682v2 Announce Type: replace-cross  Abstract: Machine Unlearning (MU) has gained considerable attention recently for its potential to achieve Safe AI by removing the influence of specific data from trained machine learning models. This process, known as knowledge removal, addresses AI governance concerns of training data such as quality, sensitivity, copyright restrictions, and obsolescence. This capability is also crucial for ensuring compliance with privacy regulations such as the Right To Be Forgotten. Furthermore, effective knowledge removal mitigates the risk of harmful outcomes, safeguarding against biases, misinformation, and unauthorized data exploitation, thereby enhancing the safe and responsible use of AI systems. Efforts have been made to design efficient unlearning approaches, with MU services being examined for integration with existing machine learning as a service, allowing users to submit requests to remove specific data from the training corpus. However, 
    
[^3]: 在隐私约束下的图拓扑学习

    Graph Topology Learning Under Privacy Constraints. (arXiv:2301.06662v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.06662](http://arxiv.org/abs/2301.06662)

    在隐私约束下，我们提出了一个框架，联合学习为本地客户定制的个性化图以及共识图，以推断潜在图拓扑，同时在保护隐私的情况下处理分布式客户端的数据。

    

    我们考虑在数据分布于分布式客户端且具有隐私敏感性的新颖实际场景中，通过平滑图信号推断潜在图拓扑的问题。这个任务的主要困难在于如何在隐私约束下利用所有独立客户端的潜在异构数据。为了解决这个问题，我们提出了一个框架，通过联合学习为本地客户端定制的个性化图以及共识图。个性化图匹配本地数据分布，从而减轻数据的异质性，而共识图捕捉全局信息。我们接下来设计了一个定制的算法来解决引入的问题，同时不违反隐私约束，即所有的私有数据都在本地处理。为了进一步增强隐私保护，我们将差分隐私（DP）引入到所提算法中，在传输模型更新时抵御隐私攻击。理论上，我们建立了可证明收敛的分析。

    We consider the problem of inferring the underlying graph topology from smooth graph signals in a novel but practical scenario where data are located in distributed clients and are privacy-sensitive. The main difficulty of this task lies in how to utilize the potentially heterogeneous data of all isolated clients under privacy constraints. Towards this end, we propose a framework where personalized graphs for local clients as well as a consensus graph are jointly learned. The personalized graphs match local data distributions, thereby mitigating data heterogeneity, while the consensus graph captures the global information. We next devise a tailored algorithm to solve the induced problem without violating privacy constraints, i.e., all private data are processed locally. To further enhance privacy protection, we introduce differential privacy (DP) into the proposed algorithm to resist privacy attacks when transmitting model updates. Theoretically, we establish provable convergence analy
    

