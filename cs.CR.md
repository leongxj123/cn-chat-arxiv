# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [FedPIT: Towards Privacy-preserving and Few-shot Federated Instruction Tuning](https://arxiv.org/abs/2403.06131) | 提出一种新颖的联邦算法FedPIT，通过利用LLMs的上下文学习能力自动生成任务特定的合成数据进行训练，采用参数隔离训练来防止数据提取攻击。 |
| [^2] | [Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment](https://arxiv.org/abs/2402.14968) | 提出了一种通过后门增强对齐方法有效缓解微调越狱攻击，避免需要大量安全示例的低效问题。 |
| [^3] | [Is the System Message Really Important to Jailbreaks in Large Language Models?](https://arxiv.org/abs/2402.14857) | 系统消息在大型语言模型中的越狱过程中起着重要作用，不同系统消息对抵抗越狱具有不同影响，且越狱可能在不同语言模型之间具有可转移性。 |

# 详细

[^1]: FedPIT：面向隐私保护和少样本联邦指令调整

    FedPIT: Towards Privacy-preserving and Few-shot Federated Instruction Tuning

    [https://arxiv.org/abs/2403.06131](https://arxiv.org/abs/2403.06131)

    提出一种新颖的联邦算法FedPIT，通过利用LLMs的上下文学习能力自动生成任务特定的合成数据进行训练，采用参数隔离训练来防止数据提取攻击。

    

    指令调整对于增强大型语言模型（LLMs）在生成与人类对齐的响应方面的性能已被证明至关重要。然而，在调整过程中收集多样化、高质量的指令数据存在挑战，特别是在涉及隐私的领域。联邦指令调整（FedIT）已成为一种解决方案，利用来自多个数据所有者的联邦学习，同时保护隐私。然而，由于指令数据有限以及容易受到训练数据提取攻击的脆弱性，它面临挑战。为解决这些问题，我们提出了一种新颖的联邦算法，FedPIT，它利用LLMs的上下文学习能力自动生成用于训练的特定任务合成数据。我们的方法采用参数隔离训练来维护在合成数据上训练的全局参数和在增强本地数据上训练的本地参数，有效地防止数据提取攻击。

    arXiv:2403.06131v1 Announce Type: cross  Abstract: Instruction tuning has proven essential for enhancing the performance of large language models (LLMs) in generating human-aligned responses. However, collecting diverse, high-quality instruction data for tuning poses challenges, particularly in privacy-sensitive domains. Federated instruction tuning (FedIT) has emerged as a solution, leveraging federated learning from multiple data owners while preserving privacy. Yet, it faces challenges due to limited instruction data and vulnerabilities to training data extraction attacks. To address these issues, we propose a novel federated algorithm, FedPIT, which utilizes LLMs' in-context learning capability to self-generate task-specific synthetic data for training autonomously. Our method employs parameter-isolated training to maintain global parameters trained on synthetic data and local parameters trained on augmented local data, effectively thwarting data extraction attacks. Extensive exper
    
[^2]: 使用后门增强对齐来缓解微调越狱攻击

    Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment

    [https://arxiv.org/abs/2402.14968](https://arxiv.org/abs/2402.14968)

    提出了一种通过后门增强对齐方法有效缓解微调越狱攻击，避免需要大量安全示例的低效问题。

    

    尽管大型语言模型（LLMs）如GPT-4和Llama-2具有一般能力，但在满足特定业务需求和定制用例的复杂性时，仍然需要对其进行微调或自适应以满足需求。然而，这个过程不可避免地引入了新的安全威胁，特别是针对基于微调的越狱攻击（FJAttack），在这种情况下，将仅几个有害示例纳入微调数据集就可能显着地损害模型的安全性。虽然已经提出了一些潜在的防御方法，例如将安全示例纳入微调数据集以减少安全问题，但这些方法需要纳入大量的安全示例，效率低下。为了有效地针对FJAttack进行防御并只使用有限的安全示例，我们提出了一种灵感来自后门攻击概念的后门增强安全对齐方法。

    arXiv:2402.14968v1 Announce Type: cross  Abstract: Despite the general capabilities of Large Language Models (LLMs) like GPT-4 and Llama-2, these models still request fine-tuning or adaptation with customized data when it comes to meeting the specific business demands and intricacies of tailored use cases. However, this process inevitably introduces new safety threats, particularly against the Fine-tuning based Jailbreak Attack (FJAttack), where incorporating just a few harmful examples into the fine-tuning dataset can significantly compromise the model safety. Though potential defenses have been proposed by incorporating safety examples into the fine-tuning dataset to reduce the safety issues, such approaches require incorporating a substantial amount of safety examples, making it inefficient. To effectively defend against the FJAttack with limited safety examples, we propose a Backdoor Enhanced Safety Alignment method inspired by an analogy with the concept of backdoor attacks. In pa
    
[^3]: 大型语言模型中的系统消息对越狱是否真的很重要？

    Is the System Message Really Important to Jailbreaks in Large Language Models?

    [https://arxiv.org/abs/2402.14857](https://arxiv.org/abs/2402.14857)

    系统消息在大型语言模型中的越狱过程中起着重要作用，不同系统消息对抵抗越狱具有不同影响，且越狱可能在不同语言模型之间具有可转移性。

    

    大型语言模型（LLMs）的快速发展使它们在现代社会中不可或缺。尽管通常会采取安全措施在发布前将LLMs与人类价值观保持一致，但最近的研究揭示了一个令人担忧的现象，被称为"越狱"。这个术语指的是当LLMs受到恶意问题提示时产生意外且可能有害的响应。现有研究侧重于生成越狱提示，但我们的研究旨在回答一个不同的问题：系统消息对LLMs中的越狱是否真的很重要？为了回答这个问题，我们在一个稳定的GPT版本gpt-3.5-turbo-0613中进行了实验，生成了具有不同系统消息的越狱提示：短，长和无消息。我们发现不同的系统消息通过实验具有不同的抵抗越狱的能力。此外，我们还探讨了越狱在LLMs之间的可转移性。这一发现强调了系统消息在防止LLMs越狱中的重要性。

    arXiv:2402.14857v1 Announce Type: cross  Abstract: The rapid evolution of Large Language Models (LLMs) has rendered them indispensable in modern society. While security measures are typically in place to align LLMs with human values prior to release, recent studies have unveiled a concerning phenomenon named "jailbreak." This term refers to the unexpected and potentially harmful responses generated by LLMs when prompted with malicious questions. Existing research focuses on generating jailbreak prompts but our study aim to answer a different question: Is the system message really important to jailbreak in LLMs? To address this question, we conducted experiments in a stable GPT version gpt-3.5-turbo-0613 to generated jailbreak prompts with varying system messages: short, long, and none. We discover that different system messages have distinct resistances to jailbreak by experiments. Additionally, we explore the transferability of jailbreak across LLMs. This finding underscores the signi
    

