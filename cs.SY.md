# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Asymptotically Efficient Online Learning for Censored Regression Models Under Non-I.I.D Data.](http://arxiv.org/abs/2309.09454) | 这项研究提出了一种渐近高效的在线学习方法，应用于随机被审查回归模型，并在一般情况下达到了最好的性能。 |

# 详细

[^1]: 非独立同分布数据条件下渐近高效的在线学习方法在被审查回归模型中的应用

    Asymptotically Efficient Online Learning for Censored Regression Models Under Non-I.I.D Data. (arXiv:2309.09454v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.09454](http://arxiv.org/abs/2309.09454)

    这项研究提出了一种渐近高效的在线学习方法，应用于随机被审查回归模型，并在一般情况下达到了最好的性能。

    

    本文研究了渐近高效的在线学习方法在随机被审查回归模型中的应用，该模型涉及到学习和统计学的各个领域，但迄今为止仍缺乏关于学习算法效率的全面理论研究。为此，我们提出了一种两步在线算法，第一步专注于实现算法收敛性，第二步用于改善估计性能。在数据的一般激励条件下，我们通过应用随机李雅普诺夫函数方法和对鞅的极限理论，证明了我们的算法是强一致的和渐近正态的。此外，我们还证明了估计值的协方差在渐近上可以达到克拉美洛界，这意味着所提出的算法的性能在一般情况下是可以期望的最好的。与大多数现有的研究不同，我们的结果是不依赖传统方法而得出的。

    The asymptotically efficient online learning problem is investigated for stochastic censored regression models, which arise from various fields of learning and statistics but up to now still lacks comprehensive theoretical studies on the efficiency of the learning algorithms. For this, we propose a two-step online algorithm, where the first step focuses on achieving algorithm convergence, and the second step is dedicated to improving the estimation performance. Under a general excitation condition on the data, we show that our algorithm is strongly consistent and asymptotically normal by employing the stochastic Lyapunov function method and limit theories for martingales. Moreover, we show that the covariances of the estimates can achieve the Cramer-Rao (C-R) bound asymptotically, indicating that the performance of the proposed algorithm is the best possible that one can expect in general. Unlike most of the existing works, our results are obtained without resorting to the traditionall
    

