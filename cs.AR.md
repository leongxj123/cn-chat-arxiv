# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Circuit Transformer: End-to-end Circuit Design by Predicting the Next Gate](https://arxiv.org/abs/2403.13838) | 本研究探索了通过预测下一个逻辑门来实现电路设计的可能性。 |

# 详细

[^1]: 电路变压器：通过预测下一个门实现端到端电路设计

    Circuit Transformer: End-to-end Circuit Design by Predicting the Next Gate

    [https://arxiv.org/abs/2403.13838](https://arxiv.org/abs/2403.13838)

    本研究探索了通过预测下一个逻辑门来实现电路设计的可能性。

    

    语言是人类通过序列符号表达的突出能力，近年来大型语言模型（LLMs）已经在计算上掌握了这种能力。通过利用巨大的神经模型不断预测下一个单词，LLMs展现出了前所未有的理解和推理能力。电路作为电子设计的“语言”，通过逻辑门的级联连接来指定电子设备的功能。在这项工作中，我们首次探索了这种可能性，以通过简单地预测下一个逻辑门来征服电子设计任务。

    arXiv:2403.13838v1 Announce Type: new  Abstract: Language, a prominent human ability to express through sequential symbols, has been computationally mastered by recent advances of large language models (LLMs). By predicting the next word recurrently with huge neural models, LLMs have shown unprecedented capabilities in understanding and reasoning. Circuit, as the "language" of electronic design, specifies the functionality of an electronic device by cascade connections of logic gates. Then, can circuits also be mastered by a a sufficiently large "circuit model", which can conquer electronic design tasks by simply predicting the next logic gate? In this work, we take the first step to explore such possibilities. Two primary barriers impede the straightforward application of LLMs to circuits: their complex, non-sequential structure, and the intolerance of hallucination due to strict constraints (e.g., equivalence). For the first barrier, we encode a circuit as a memory-less, depth-first 
    

