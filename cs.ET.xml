<rss version="2.0"><channel><title>Chat Arxiv cs.ET</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.ET</description><item><title>FEDORA&#26159;&#19968;&#20010;&#39134;&#34892;&#20107;&#20214;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#32570;&#23569;&#23436;&#25972;&#25968;&#25454;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#24110;&#21161;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#19979;&#23454;&#29616;&#22522;&#20110;&#35270;&#35273;&#30340;&#33258;&#20027;&#23548;&#33322;&#21644;&#36991;&#38556;&#12290;</title><link>http://arxiv.org/abs/2305.14392</link><description>&lt;p&gt;
FEDORA&#65306;&#29992;&#20110;&#21453;&#24212;&#34892;&#20026;&#30340;&#39134;&#34892;&#20107;&#20214;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
FEDORA: Flying Event Dataset fOr Reactive behAvior. (arXiv:2305.14392v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14392
&lt;/p&gt;
&lt;p&gt;
FEDORA&#26159;&#19968;&#20010;&#39134;&#34892;&#20107;&#20214;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#32570;&#23569;&#23436;&#25972;&#25968;&#25454;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#24110;&#21161;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#19979;&#23454;&#29616;&#22522;&#20110;&#35270;&#35273;&#30340;&#33258;&#20027;&#23548;&#33322;&#21644;&#36991;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#20307;&#22312;&#39134;&#34892;&#20013;&#20351;&#29992;&#26497;&#23569;&#25968;&#30340;&#31070;&#32463;&#20803;&#21644;&#26497;&#20302;&#30340;&#22833;&#35823;&#29575;&#25191;&#34892;&#22797;&#26434;&#30340;&#39640;&#36895;&#26426;&#21160;&#65292;&#31361;&#26174;&#20102;&#36825;&#20123;&#36164;&#28304;&#21463;&#38480;&#21046;&#30340;&#29983;&#29289;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;&#36817;&#24180;&#26469;&#65292;&#20107;&#20214;&#39537;&#21160;&#30828;&#20214;&#36880;&#28176;&#25104;&#20026;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#23454;&#29616;&#22797;&#26434;&#35270;&#35273;&#20219;&#21153;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#22522;&#20110;&#35270;&#35273;&#30340;&#33258;&#20027;&#23548;&#33322;&#21644;&#36991;&#38556;&#21253;&#25324;&#20960;&#20010;&#29420;&#31435;&#20294;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#22914;&#20809;&#27969;&#20272;&#35745;&#12289;&#28145;&#24230;&#20272;&#35745;&#12289;&#21516;&#26102;&#23450;&#20301;&#19982;&#24314;&#22270;&#65288;SLAM&#65289;&#12289;&#29289;&#20307;&#26816;&#27979;&#21644;&#35782;&#21035;&#12290;&#20026;&#20102;&#30830;&#20445;&#36825;&#20123;&#20219;&#21153;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#20182;&#20204;&#24517;&#39035;&#22312;&#21333;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#25968;&#25454;&#38598;&#21482;&#25552;&#20379;&#25152;&#38656;&#25968;&#25454;&#30340;&#36873;&#23450;&#23376;&#38598;&#65292;&#36825;&#20351;&#24471;&#32593;&#32476;&#38388;&#30340;&#19968;&#33268;&#24615;&#38590;&#20197;&#23454;&#29616;&#12290;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#21478;&#19968;&#20010;&#38480;&#21046;&#26159;&#25552;&#20379;&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#36776;&#29575;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FEDORA&#65292;
&lt;/p&gt;
&lt;p&gt;
The ability of living organisms to perform complex high speed manoeuvers in flight with a very small number of neurons and an incredibly low failure rate highlights the efficacy of these resource-constrained biological systems. Event-driven hardware has emerged, in recent years, as a promising avenue for implementing complex vision tasks in resource-constrained environments. Vision-based autonomous navigation and obstacle avoidance consists of several independent but related tasks such as optical flow estimation, depth estimation, Simultaneous Localization and Mapping (SLAM), object detection, and recognition. To ensure coherence between these tasks, it is imperative that they be trained on a single dataset. However, most existing datasets provide only a selected subset of the required data. This makes inter-network coherence difficult to achieve. Another limitation of existing datasets is the limited temporal resolution they provide. To address these limitations, we present FEDORA, a 
&lt;/p&gt;</description></item></channel></rss>