# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [LAMBO: Large Language Model Empowered Edge Intelligence.](http://arxiv.org/abs/2308.15078) | LAMBO是一种基于大型语言模型的边缘智能框架，用于移动边缘计算。它解决了传统深度卸载架构的问题，并提供了高性能的决策模块和强化学习模块。 |
| [^2] | [Large AI Model-Based Semantic Communications.](http://arxiv.org/abs/2307.03492) | 本文提出了一种基于大型AI模型的语义通信框架（LAM-SC），利用该框架可以克服知识库构建过程中面临的问题，并在图像数据领域实现了语义分割、语义集成和自适应语义压缩。 |

# 详细

[^1]: LAMBO: 大型语言模型增强的边缘智能

    LAMBO: Large Language Model Empowered Edge Intelligence. (arXiv:2308.15078v1 [cs.AI])

    [http://arxiv.org/abs/2308.15078](http://arxiv.org/abs/2308.15078)

    LAMBO是一种基于大型语言模型的边缘智能框架，用于移动边缘计算。它解决了传统深度卸载架构的问题，并提供了高性能的决策模块和强化学习模块。

    

    预计下一代边缘智能将为各种应用带来巨大的好处，例如卸载系统。然而，传统的深度卸载架构面临多个问题，包括异构限制、部分感知、不确定的泛化和缺乏可追溯性。在这种背景下，将卸载与大型语言模型（LLMs）集成在一起具有许多优势。因此，我们提出了一种基于LLM的卸载（LAMBO）框架，用于移动边缘计算（MEC），它由四个组成部分组成：（i）输入嵌入（IE），用于用高质量的可学习向量表示具有约束和提示的卸载系统的信息；（ii）非对称编码解码（AED）模型，是一个决策模块，具有深度编码器和浅层解码器。它可以基于多头自注意力机制实现高性能；（iii）演员-评论家强化学习（ACRL）模块，用于进行预训练。

    Next-generation edge intelligence is anticipated to bring huge benefits to various applications, e.g., offloading systems. However, traditional deep offloading architectures face several issues, including heterogeneous constraints, partial perception, uncertain generalization, and lack of tractability. In this context, the integration of offloading with large language models (LLMs) presents numerous advantages. Therefore, we propose an LLM-Based Offloading (LAMBO) framework for mobile edge computing (MEC), which comprises four components: (i) Input embedding (IE), which is used to represent the information of the offloading system with constraints and prompts through learnable vectors with high quality; (ii) Asymmetric encoderdecoder (AED) model, which is a decision-making module with a deep encoder and a shallow decoder. It can achieve high performance based on multi-head self-attention schemes; (iii) Actor-critic reinforcement learning (ACRL) module, which is employed to pre-train th
    
[^2]: 基于大型AI模型的语义通信

    Large AI Model-Based Semantic Communications. (arXiv:2307.03492v1 [cs.AI])

    [http://arxiv.org/abs/2307.03492](http://arxiv.org/abs/2307.03492)

    本文提出了一种基于大型AI模型的语义通信框架（LAM-SC），利用该框架可以克服知识库构建过程中面临的问题，并在图像数据领域实现了语义分割、语义集成和自适应语义压缩。

    

    语义通信（SC）是一种新兴的智能范式，为元宇宙、混合现实和万物互联等未来应用提供解决方案。然而，在目前的SC系统中，知识库（KB）的构建面临着一些问题，包括知识表示有限、频繁的知识更新和不安全的知识共享。幸运的是，大型AI模型的发展提供了解决上述问题的新方案。在这里，我们提出了一种基于大型AI模型的SC框架（LAM-SC），专门用于图像数据，我们首先设计了基于段落模型（SAM）的知识库（SKB），它可以通过通用语义知识将原始图像划分为不同的语义段落。然后，我们提出了一种基于注意力的语义集成（ASI），通过权衡由SKB生成的语义段落，无需人工参与并将它们集成为具有语义感知的图像。此外，我们还提出了一种自适应语义压缩（ASC）方法。

    Semantic communication (SC) is an emerging intelligent paradigm, offering solutions for various future applications like metaverse, mixed-reality, and the Internet of everything. However, in current SC systems, the construction of the knowledge base (KB) faces several issues, including limited knowledge representation, frequent knowledge updates, and insecure knowledge sharing. Fortunately, the development of the large AI model provides new solutions to overcome above issues. Here, we propose a large AI model-based SC framework (LAM-SC) specifically designed for image data, where we first design the segment anything model (SAM)-based KB (SKB) that can split the original image into different semantic segments by universal semantic knowledge. Then, we present an attention-based semantic integration (ASI) to weigh the semantic segments generated by SKB without human participation and integrate them as the semantic-aware image. Additionally, we propose an adaptive semantic compression (ASC
    

