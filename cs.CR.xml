<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;CodeAttack&#26694;&#26550;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#65292;&#30740;&#31350;&#21457;&#29616;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#31561;&#26368;&#26032;&#27169;&#22411;&#23384;&#22312;&#20195;&#30721;&#36755;&#20837;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;</title><link>https://arxiv.org/abs/2403.07865</link><description>&lt;p&gt;
&#36890;&#36807;&#20195;&#30721;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Exploring Safety Generalization Challenges of Large Language Models via Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;CodeAttack&#26694;&#26550;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#65292;&#30740;&#31350;&#21457;&#29616;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#31561;&#26368;&#26032;&#27169;&#22411;&#23384;&#22312;&#20195;&#30721;&#36755;&#20837;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#24102;&#26469;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#23427;&#20204;&#28508;&#22312;&#35823;&#29992;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;CodeAttack&#65292;&#19968;&#20010;&#23558;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#36716;&#25442;&#20026;&#20195;&#30721;&#36755;&#20837;&#30340;&#26694;&#26550;&#65292;&#20026;&#27979;&#35797;LLMs&#30340;&#23433;&#20840;&#27867;&#21270;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29615;&#22659;&#12290;&#25105;&#20204;&#23545;&#21253;&#25324;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#22312;&#20869;&#30340;&#26368;&#26032;LLMs&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#20195;&#30721;&#36755;&#20837;&#23384;&#22312;&#20849;&#21516;&#30340;&#23433;&#20840;&#28431;&#27934;&#65306;CodeAttack&#22312;&#36229;&#36807;80%&#30340;&#26102;&#38388;&#20869;&#22987;&#32456;&#32469;&#36807;&#25152;&#26377;&#27169;&#22411;&#30340;&#23433;&#20840;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07865v1 Announce Type: cross  Abstract: The rapid advancement of Large Language Models (LLMs) has brought about remarkable capabilities in natural language processing but also raised concerns about their potential misuse. While strategies like supervised fine-tuning and reinforcement learning from human feedback have enhanced their safety, these methods primarily focus on natural languages, which may not generalize to other domains. This paper introduces CodeAttack, a framework that transforms natural language inputs into code inputs, presenting a novel environment for testing the safety generalization of LLMs. Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a common safety vulnerability of these models against code input: CodeAttack consistently bypasses the safety guardrails of all models more than 80\% of the time. Furthermore, we find that a larger distribution gap between CodeAttack and natural language leads to we
&lt;/p&gt;</description></item><item><title>&#22312;&#29983;&#29289;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#36827;&#34892;&#38646;&#26679;&#26412;&#37319;&#26679;&#30340;&#26041;&#26696;&#65292;&#29992;&#20110;&#21457;&#29616;&#21508;&#31181;&#23545;&#25239;&#23454;&#20307;&#20316;&#20026;&#24178;&#25200;&#22240;&#32032;&#65292;&#30456;&#27604;&#38543;&#26426;&#37319;&#26679;&#65292;&#22312;&#23545;&#25239;&#38382;&#31572;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#20248;&#21183;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#29305;&#24449;&#30340;&#20004;&#31181;&#23545;&#25239;&#24615;&#23454;&#20307;&#21046;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.10527</link><description>&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#38646;&#26679;&#26412;&#37319;&#26679;&#23545;&#25239;&#23454;&#20307;
&lt;/p&gt;
&lt;p&gt;
Zero-shot sampling of adversarial entities in biomedical question answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10527
&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#29289;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#36827;&#34892;&#38646;&#26679;&#26412;&#37319;&#26679;&#30340;&#26041;&#26696;&#65292;&#29992;&#20110;&#21457;&#29616;&#21508;&#31181;&#23545;&#25239;&#23454;&#20307;&#20316;&#20026;&#24178;&#25200;&#22240;&#32032;&#65292;&#30456;&#27604;&#38543;&#26426;&#37319;&#26679;&#65292;&#22312;&#23545;&#25239;&#38382;&#31572;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#20248;&#21183;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#29305;&#24449;&#30340;&#20004;&#31181;&#23545;&#25239;&#24615;&#23454;&#20307;&#21046;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#21442;&#25968;&#22495;&#30693;&#35782;&#30340;&#22686;&#21152;&#28145;&#24230;&#25512;&#21160;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#24555;&#36895;&#37096;&#32626;&#12290;&#22312;&#39640;&#39118;&#38505;&#21644;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#65292;&#29702;&#35299;&#27169;&#22411;&#30340;&#28431;&#27934;&#23545;&#20110;&#37327;&#21270;&#27169;&#22411;&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#21644;&#35268;&#33539;&#20854;&#20351;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#21457;&#29616;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#20316;&#20026;&#23545;&#25239;&#31034;&#20363;&#30340;&#21629;&#21517;&#23454;&#20307;&#24341;&#21457;&#20102;&#20851;&#20110;&#23427;&#20204;&#22312;&#20854;&#20182;&#29615;&#22659;&#20013;&#21487;&#33021;&#30340;&#20266;&#35013;&#30340;&#30097;&#38382;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#24130;&#32553;&#25918;&#36317;&#31163;&#21152;&#26435;&#37319;&#26679;&#26041;&#26696;&#65292;&#20197;&#21457;&#29616;&#22810;&#26679;&#21270;&#30340;&#23545;&#25239;&#23454;&#20307;&#20316;&#20026;&#24178;&#25200;&#22240;&#32032;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#22312;&#29983;&#29289;&#21307;&#23398;&#20027;&#39064;&#30340;&#23545;&#25239;&#24615;&#38382;&#39064;&#22238;&#31572;&#20013;&#20248;&#20110;&#38543;&#26426;&#37319;&#26679;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#24471;&#21487;&#20197;&#25506;&#32034;&#25915;&#20987;&#34920;&#38754;&#19978;&#30340;&#19981;&#21516;&#21306;&#22495;&#65292;&#36825;&#25581;&#31034;&#20102;&#20004;&#31181;&#22312;&#29305;&#24449;&#19978;&#26126;&#26174;&#19981;&#21516;&#30340;&#23545;&#25239;&#24615;&#23454;&#20307;&#30340;&#21046;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25915;&#20987;&#26041;&#24335;&#22914;&#20309;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10527v1 Announce Type: new  Abstract: The increasing depth of parametric domain knowledge in large language models (LLMs) is fueling their rapid deployment in real-world applications. In high-stakes and knowledge-intensive tasks, understanding model vulnerabilities is essential for quantifying the trustworthiness of model predictions and regulating their use. The recent discovery of named entities as adversarial examples in natural language processing tasks raises questions about their potential guises in other settings. Here, we propose a powerscaled distance-weighted sampling scheme in embedding space to discover diverse adversarial entities as distractors. We demonstrate its advantage over random sampling in adversarial question answering on biomedical topics. Our approach enables the exploration of different regions on the attack surface, which reveals two regimes of adversarial entities that markedly differ in their characteristics. Moreover, we show that the attacks su
&lt;/p&gt;</description></item></channel></rss>