<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#30340;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#24322;&#26500;&#28304;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#26469;&#22686;&#24378;&#23545;&#30446;&#26631;&#25968;&#25454;&#38598;&#30340;&#23398;&#20064;&#65292;&#21516;&#26102;&#32771;&#34385;&#38544;&#31169;&#32422;&#26463;&#12290;</title><link>https://arxiv.org/abs/2403.11343</link><description>&lt;p&gt;
&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#30340;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Transfer Learning with Differential Privacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#30340;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#24322;&#26500;&#28304;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#26469;&#22686;&#24378;&#23545;&#30446;&#26631;&#25968;&#25454;&#38598;&#30340;&#23398;&#20064;&#65292;&#21516;&#26102;&#32771;&#34385;&#38544;&#31169;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#36234;&#26469;&#36234;&#21463;&#21040;&#27426;&#36814;&#65292;&#25968;&#25454;&#24322;&#26500;&#24615;&#21644;&#38544;&#31169;&#24615;&#26159;&#20004;&#20010;&#31361;&#20986;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#20869;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#24322;&#26500;&#28304;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#26469;&#22686;&#24378;&#23545;&#30446;&#26631;&#25968;&#25454;&#38598;&#30340;&#23398;&#20064;&#65292;&#21516;&#26102;&#36981;&#23432;&#38544;&#31169;&#32422;&#26463;&#12290;&#25105;&#20204;&#20005;&#26684;&#21046;&#23450;&#20102;\textit{&#32852;&#37030;&#24046;&#20998;&#38544;&#31169;}&#30340;&#27010;&#24565;&#65292;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#25552;&#20379;&#38544;&#31169;&#20445;&#35777;&#65292;&#32780;&#26080;&#38656;&#20551;&#35774;&#26377;&#19968;&#20010;&#21463;&#20449;&#20219;&#30340;&#20013;&#22830;&#26381;&#21153;&#22120;&#12290;&#22312;&#36825;&#20010;&#38544;&#31169;&#32422;&#26463;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#20010;&#32463;&#20856;&#30340;&#32479;&#35745;&#38382;&#39064;&#65292;&#21363;&#21333;&#21464;&#37327;&#22343;&#20540;&#20272;&#35745;&#12289;&#20302;&#32500;&#32447;&#24615;&#22238;&#24402;&#21644;&#39640;&#32500;&#32447;&#24615;&#22238;&#24402;&#12290;&#36890;&#36807;&#30740;&#31350;&#26497;&#23567;&#20540;&#29575;&#24182;&#30830;&#23450;&#36825;&#20123;&#38382;&#39064;&#30340;&#38544;&#31169;&#25104;&#26412;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32852;&#37030;&#24046;&#20998;&#38544;&#31169;&#26159;&#24050;&#24314;&#31435;&#30340;&#23616;&#37096;&#21644;&#20013;&#22830;&#27169;&#22411;&#20043;&#38388;&#30340;&#19968;&#31181;&#20013;&#38388;&#38544;&#31169;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11343v1 Announce Type: new  Abstract: Federated learning is gaining increasing popularity, with data heterogeneity and privacy being two prominent challenges. In this paper, we address both issues within a federated transfer learning framework, aiming to enhance learning on a target data set by leveraging information from multiple heterogeneous source data sets while adhering to privacy constraints. We rigorously formulate the notion of \textit{federated differential privacy}, which offers privacy guarantees for each data set without assuming a trusted central server. Under this privacy constraint, we study three classical statistical problems, namely univariate mean estimation, low-dimensional linear regression, and high-dimensional linear regression. By investigating the minimax rates and identifying the costs of privacy for these problems, we show that federated differential privacy is an intermediate privacy model between the well-established local and central models of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#38598;&#25104;&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#25932;&#23545;&#23041;&#32961;&#19979;&#30340;&#28431;&#27934;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#25581;&#31034;&#20102;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#37197;&#32622;&#30340;&#32852;&#37030;&#23398;&#20064;&#19979;&#23545;&#25932;&#23545;&#23041;&#32961;&#30340;&#39640;&#25935;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10375</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#38598;&#25104;&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#25932;&#23545;&#23041;&#32961;&#19979;&#30340;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Vulnerabilities of Foundation Model Integrated Federated Learning Under Adversarial Threats. (arXiv:2401.10375v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#38598;&#25104;&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#25932;&#23545;&#23041;&#32961;&#19979;&#30340;&#28431;&#27934;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#25581;&#31034;&#20102;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#37197;&#32622;&#30340;&#32852;&#37030;&#23398;&#20064;&#19979;&#23545;&#25932;&#23545;&#23041;&#32961;&#30340;&#39640;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#35299;&#20915;&#19982;&#25968;&#25454;&#38544;&#31169;&#21644;&#23433;&#20840;&#30456;&#20851;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#23384;&#22312;&#25968;&#25454;&#19981;&#36275;&#21644;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#20026;&#29616;&#26377;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#30340;&#23616;&#38480;&#24615;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20363;&#22914;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#21021;&#22987;&#21270;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#20869;&#22312;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#23558;&#22522;&#30784;&#27169;&#22411;&#38598;&#25104;&#21040;&#32852;&#37030;&#23398;&#20064;&#20013;&#21487;&#33021;&#24341;&#20837;&#26032;&#30340;&#39118;&#38505;&#65292;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#23578;&#23646;&#26410;&#24320;&#21457;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#39318;&#27425;&#30740;&#31350;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#38598;&#25104;&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#25932;&#23545;&#23041;&#32961;&#19979;&#30340;&#28431;&#27934;&#12290;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#38598;&#25104;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#26469;&#30772;&#22351;&#32852;&#37030;&#23398;&#20064;&#23458;&#25143;&#31471;&#27169;&#22411;&#12290;&#36890;&#36807;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#39046;&#22495;&#20013;&#20351;&#29992;&#30693;&#21517;&#27169;&#22411;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#38598;&#25104;&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#19981;&#21516;&#37197;&#32622;&#30340;&#32852;&#37030;&#23398;&#20064;&#19979;&#23545;&#36825;&#31181;&#26032;&#23041;&#32961;&#30340;&#39640;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) addresses critical issues in machine learning related to data privacy and security, yet suffering from data insufficiency and imbalance under certain circumstances. The emergence of foundation models (FMs) offers potential solutions to the limitations of existing FL frameworks, e.g., by generating synthetic data for model initialization. However, due to the inherent safety concerns of FMs, integrating FMs into FL could introduce new risks, which remains largely unexplored. To address this gap, we conduct the first investigation on the vulnerability of FM integrated FL (FM-FL) under adversarial threats. Based on a unified framework of FM-FL, we introduce a novel attack strategy that exploits safety issues of FM to compromise FL client models. Through extensive experiments with well-known models and benchmark datasets in both image and text domains, we reveal the high susceptibility of the FM-FL to this new threat under various FL configurations. Furthermore, we f
&lt;/p&gt;</description></item></channel></rss>