<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#31639;&#27861;FedPIT&#65292;&#36890;&#36807;&#21033;&#29992;LLMs&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#33258;&#21160;&#29983;&#25104;&#20219;&#21153;&#29305;&#23450;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#37319;&#29992;&#21442;&#25968;&#38548;&#31163;&#35757;&#32451;&#26469;&#38450;&#27490;&#25968;&#25454;&#25552;&#21462;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.06131</link><description>&lt;p&gt;
FedPIT&#65306;&#38754;&#21521;&#38544;&#31169;&#20445;&#25252;&#21644;&#23569;&#26679;&#26412;&#32852;&#37030;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
FedPIT: Towards Privacy-preserving and Few-shot Federated Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06131
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#31639;&#27861;FedPIT&#65292;&#36890;&#36807;&#21033;&#29992;LLMs&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#33258;&#21160;&#29983;&#25104;&#20219;&#21153;&#29305;&#23450;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#37319;&#29992;&#21442;&#25968;&#38548;&#31163;&#35757;&#32451;&#26469;&#38450;&#27490;&#25968;&#25454;&#25552;&#21462;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#23545;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#19982;&#20154;&#31867;&#23545;&#40784;&#30340;&#21709;&#24212;&#26041;&#38754;&#30340;&#24615;&#33021;&#24050;&#34987;&#35777;&#26126;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#35843;&#25972;&#36807;&#31243;&#20013;&#25910;&#38598;&#22810;&#26679;&#21270;&#12289;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#23384;&#22312;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#28041;&#21450;&#38544;&#31169;&#30340;&#39046;&#22495;&#12290;&#32852;&#37030;&#25351;&#20196;&#35843;&#25972;&#65288;FedIT&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#25968;&#25454;&#25152;&#26377;&#32773;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25351;&#20196;&#25968;&#25454;&#26377;&#38480;&#20197;&#21450;&#23481;&#26131;&#21463;&#21040;&#35757;&#32451;&#25968;&#25454;&#25552;&#21462;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#23427;&#38754;&#20020;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#31639;&#27861;&#65292;FedPIT&#65292;&#23427;&#21033;&#29992;LLMs&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#33258;&#21160;&#29983;&#25104;&#29992;&#20110;&#35757;&#32451;&#30340;&#29305;&#23450;&#20219;&#21153;&#21512;&#25104;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#21442;&#25968;&#38548;&#31163;&#35757;&#32451;&#26469;&#32500;&#25252;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#20840;&#23616;&#21442;&#25968;&#21644;&#22312;&#22686;&#24378;&#26412;&#22320;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#26412;&#22320;&#21442;&#25968;&#65292;&#26377;&#25928;&#22320;&#38450;&#27490;&#25968;&#25454;&#25552;&#21462;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06131v1 Announce Type: cross  Abstract: Instruction tuning has proven essential for enhancing the performance of large language models (LLMs) in generating human-aligned responses. However, collecting diverse, high-quality instruction data for tuning poses challenges, particularly in privacy-sensitive domains. Federated instruction tuning (FedIT) has emerged as a solution, leveraging federated learning from multiple data owners while preserving privacy. Yet, it faces challenges due to limited instruction data and vulnerabilities to training data extraction attacks. To address these issues, we propose a novel federated algorithm, FedPIT, which utilizes LLMs' in-context learning capability to self-generate task-specific synthetic data for training autonomously. Our method employs parameter-isolated training to maintain global parameters trained on synthetic data and local parameters trained on augmented local data, effectively thwarting data extraction attacks. Extensive exper
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21518;&#38376;&#22686;&#24378;&#23545;&#40784;&#26041;&#27861;&#26377;&#25928;&#32531;&#35299;&#24494;&#35843;&#36234;&#29425;&#25915;&#20987;&#65292;&#36991;&#20813;&#38656;&#35201;&#22823;&#37327;&#23433;&#20840;&#31034;&#20363;&#30340;&#20302;&#25928;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.14968</link><description>&lt;p&gt;
&#20351;&#29992;&#21518;&#38376;&#22686;&#24378;&#23545;&#40784;&#26469;&#32531;&#35299;&#24494;&#35843;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14968
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21518;&#38376;&#22686;&#24378;&#23545;&#40784;&#26041;&#27861;&#26377;&#25928;&#32531;&#35299;&#24494;&#35843;&#36234;&#29425;&#25915;&#20987;&#65292;&#36991;&#20813;&#38656;&#35201;&#22823;&#37327;&#23433;&#20840;&#31034;&#20363;&#30340;&#20302;&#25928;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-4&#21644;Llama-2&#20855;&#26377;&#19968;&#33324;&#33021;&#21147;&#65292;&#20294;&#22312;&#28385;&#36275;&#29305;&#23450;&#19994;&#21153;&#38656;&#27714;&#21644;&#23450;&#21046;&#29992;&#20363;&#30340;&#22797;&#26434;&#24615;&#26102;&#65292;&#20173;&#28982;&#38656;&#35201;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#25110;&#33258;&#36866;&#24212;&#20197;&#28385;&#36275;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#19981;&#21487;&#36991;&#20813;&#22320;&#24341;&#20837;&#20102;&#26032;&#30340;&#23433;&#20840;&#23041;&#32961;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#22522;&#20110;&#24494;&#35843;&#30340;&#36234;&#29425;&#25915;&#20987;&#65288;FJAttack&#65289;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23558;&#20165;&#20960;&#20010;&#26377;&#23475;&#31034;&#20363;&#32435;&#20837;&#24494;&#35843;&#25968;&#25454;&#38598;&#23601;&#21487;&#33021;&#26174;&#30528;&#22320;&#25439;&#23475;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#28508;&#22312;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#20363;&#22914;&#23558;&#23433;&#20840;&#31034;&#20363;&#32435;&#20837;&#24494;&#35843;&#25968;&#25454;&#38598;&#20197;&#20943;&#23569;&#23433;&#20840;&#38382;&#39064;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#32435;&#20837;&#22823;&#37327;&#30340;&#23433;&#20840;&#31034;&#20363;&#65292;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#38024;&#23545;FJAttack&#36827;&#34892;&#38450;&#24481;&#24182;&#21482;&#20351;&#29992;&#26377;&#38480;&#30340;&#23433;&#20840;&#31034;&#20363;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#24863;&#26469;&#33258;&#21518;&#38376;&#25915;&#20987;&#27010;&#24565;&#30340;&#21518;&#38376;&#22686;&#24378;&#23433;&#20840;&#23545;&#40784;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14968v1 Announce Type: cross  Abstract: Despite the general capabilities of Large Language Models (LLMs) like GPT-4 and Llama-2, these models still request fine-tuning or adaptation with customized data when it comes to meeting the specific business demands and intricacies of tailored use cases. However, this process inevitably introduces new safety threats, particularly against the Fine-tuning based Jailbreak Attack (FJAttack), where incorporating just a few harmful examples into the fine-tuning dataset can significantly compromise the model safety. Though potential defenses have been proposed by incorporating safety examples into the fine-tuning dataset to reduce the safety issues, such approaches require incorporating a substantial amount of safety examples, making it inefficient. To effectively defend against the FJAttack with limited safety examples, we propose a Backdoor Enhanced Safety Alignment method inspired by an analogy with the concept of backdoor attacks. In pa
&lt;/p&gt;</description></item><item><title>&#31995;&#32479;&#28040;&#24687;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36234;&#29425;&#36807;&#31243;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#19981;&#21516;&#31995;&#32479;&#28040;&#24687;&#23545;&#25269;&#25239;&#36234;&#29425;&#20855;&#26377;&#19981;&#21516;&#24433;&#21709;&#65292;&#19988;&#36234;&#29425;&#21487;&#33021;&#22312;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#20855;&#26377;&#21487;&#36716;&#31227;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14857</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31995;&#32479;&#28040;&#24687;&#23545;&#36234;&#29425;&#26159;&#21542;&#30495;&#30340;&#24456;&#37325;&#35201;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is the System Message Really Important to Jailbreaks in Large Language Models?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14857
&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#28040;&#24687;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36234;&#29425;&#36807;&#31243;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#19981;&#21516;&#31995;&#32479;&#28040;&#24687;&#23545;&#25269;&#25239;&#36234;&#29425;&#20855;&#26377;&#19981;&#21516;&#24433;&#21709;&#65292;&#19988;&#36234;&#29425;&#21487;&#33021;&#22312;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#20855;&#26377;&#21487;&#36716;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#23427;&#20204;&#22312;&#29616;&#20195;&#31038;&#20250;&#20013;&#19981;&#21487;&#25110;&#32570;&#12290;&#23613;&#31649;&#36890;&#24120;&#20250;&#37319;&#21462;&#23433;&#20840;&#25514;&#26045;&#22312;&#21457;&#24067;&#21069;&#23558;LLMs&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#19968;&#20010;&#20196;&#20154;&#25285;&#24551;&#30340;&#29616;&#35937;&#65292;&#34987;&#31216;&#20026;"&#36234;&#29425;"&#12290;&#36825;&#20010;&#26415;&#35821;&#25351;&#30340;&#26159;&#24403;LLMs&#21463;&#21040;&#24694;&#24847;&#38382;&#39064;&#25552;&#31034;&#26102;&#20135;&#29983;&#24847;&#22806;&#19988;&#21487;&#33021;&#26377;&#23475;&#30340;&#21709;&#24212;&#12290;&#29616;&#26377;&#30740;&#31350;&#20391;&#37325;&#20110;&#29983;&#25104;&#36234;&#29425;&#25552;&#31034;&#65292;&#20294;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#22238;&#31572;&#19968;&#20010;&#19981;&#21516;&#30340;&#38382;&#39064;&#65306;&#31995;&#32479;&#28040;&#24687;&#23545;LLMs&#20013;&#30340;&#36234;&#29425;&#26159;&#21542;&#30495;&#30340;&#24456;&#37325;&#35201;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#31283;&#23450;&#30340;GPT&#29256;&#26412;gpt-3.5-turbo-0613&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#29983;&#25104;&#20102;&#20855;&#26377;&#19981;&#21516;&#31995;&#32479;&#28040;&#24687;&#30340;&#36234;&#29425;&#25552;&#31034;&#65306;&#30701;&#65292;&#38271;&#21644;&#26080;&#28040;&#24687;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#30340;&#31995;&#32479;&#28040;&#24687;&#36890;&#36807;&#23454;&#39564;&#20855;&#26377;&#19981;&#21516;&#30340;&#25269;&#25239;&#36234;&#29425;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#36234;&#29425;&#22312;LLMs&#20043;&#38388;&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;&#31995;&#32479;&#28040;&#24687;&#22312;&#38450;&#27490;LLMs&#36234;&#29425;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14857v1 Announce Type: cross  Abstract: The rapid evolution of Large Language Models (LLMs) has rendered them indispensable in modern society. While security measures are typically in place to align LLMs with human values prior to release, recent studies have unveiled a concerning phenomenon named "jailbreak." This term refers to the unexpected and potentially harmful responses generated by LLMs when prompted with malicious questions. Existing research focuses on generating jailbreak prompts but our study aim to answer a different question: Is the system message really important to jailbreak in LLMs? To address this question, we conducted experiments in a stable GPT version gpt-3.5-turbo-0613 to generated jailbreak prompts with varying system messages: short, long, and none. We discover that different system messages have distinct resistances to jailbreak by experiments. Additionally, we explore the transferability of jailbreak across LLMs. This finding underscores the signi
&lt;/p&gt;</description></item></channel></rss>