<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#35780;&#20272;LLM&#38598;&#25104;&#31995;&#32479;&#20445;&#23494;&#24615;&#30340;&#31995;&#32479;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24418;&#24335;&#21270;&#19968;&#20010;"&#31192;&#23494;&#23494;&#38053;"&#28216;&#25103;&#26469;&#25429;&#25417;&#27169;&#22411;&#38544;&#34255;&#31169;&#20154;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#35780;&#20272;&#20102;&#20843;&#31181;&#25915;&#20987;&#21644;&#22235;&#31181;&#38450;&#24481;&#26041;&#27861;&#65292;&#21457;&#29616;&#24403;&#21069;&#30340;&#38450;&#24481;&#26041;&#27861;&#32570;&#20047;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.06922</link><description>&lt;p&gt;
&#26426;&#22120;&#20013;&#30340;&#31169;&#35821;&#65306;LLM&#38598;&#25104;&#31995;&#32479;&#20013;&#30340;&#20445;&#23494;&#24615;
&lt;/p&gt;
&lt;p&gt;
Whispers in the Machine: Confidentiality in LLM-integrated Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#35780;&#20272;LLM&#38598;&#25104;&#31995;&#32479;&#20445;&#23494;&#24615;&#30340;&#31995;&#32479;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24418;&#24335;&#21270;&#19968;&#20010;"&#31192;&#23494;&#23494;&#38053;"&#28216;&#25103;&#26469;&#25429;&#25417;&#27169;&#22411;&#38544;&#34255;&#31169;&#20154;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#35780;&#20272;&#20102;&#20843;&#31181;&#25915;&#20987;&#21644;&#22235;&#31181;&#38450;&#24481;&#26041;&#27861;&#65292;&#21457;&#29616;&#24403;&#21069;&#30340;&#38450;&#24481;&#26041;&#27861;&#32570;&#20047;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#19982;&#22806;&#37096;&#24037;&#20855;&#38598;&#25104;&#12290;&#23613;&#31649;&#36825;&#20123;&#38598;&#25104;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;LLM&#30340;&#21151;&#33021;&#65292;&#20294;&#23427;&#20204;&#20063;&#22312;&#19981;&#21516;&#32452;&#20214;&#20043;&#38388;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25915;&#20987;&#38754;&#65292;&#21487;&#33021;&#27844;&#38706;&#26426;&#23494;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24694;&#24847;&#24037;&#20855;&#21487;&#20197;&#21033;&#29992;LLM&#26412;&#36523;&#30340;&#28431;&#27934;&#26469;&#25805;&#32437;&#27169;&#22411;&#24182;&#25439;&#23475;&#20854;&#20182;&#26381;&#21153;&#30340;&#25968;&#25454;&#65292;&#36825;&#24341;&#21457;&#20102;&#22312;LLM&#38598;&#25104;&#29615;&#22659;&#20013;&#22914;&#20309;&#20445;&#25252;&#31169;&#23494;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#35780;&#20272;LLM&#38598;&#25104;&#31995;&#32479;&#20445;&#23494;&#24615;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24418;&#24335;&#21270;&#20102;&#19968;&#20010;"&#31192;&#23494;&#23494;&#38053;"&#28216;&#25103;&#65292;&#21487;&#20197;&#25429;&#25417;&#27169;&#22411;&#38544;&#34255;&#31169;&#20154;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#27604;&#36739;&#27169;&#22411;&#23545;&#20445;&#23494;&#24615;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#20197;&#21450;&#19981;&#21516;&#38450;&#24481;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20843;&#31181;&#20808;&#21069;&#21457;&#34920;&#30340;&#25915;&#20987;&#21644;&#22235;&#31181;&#38450;&#24481;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#30340;&#38450;&#24481;&#26041;&#27861;&#32570;&#20047;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are increasingly integrated with external tools. While these integrations can significantly improve the functionality of LLMs, they also create a new attack surface where confidential data may be disclosed between different components. Specifically, malicious tools can exploit vulnerabilities in the LLM itself to manipulate the model and compromise the data of other services, raising the question of how private data can be protected in the context of LLM integrations.   In this work, we provide a systematic way of evaluating confidentiality in LLM-integrated systems. For this, we formalize a "secret key" game that can capture the ability of a model to conceal private information. This enables us to compare the vulnerability of a model against confidentiality attacks and also the effectiveness of different defense strategies. In this framework, we evaluate eight previously published attacks and four defenses. We find that current defenses lack generalization
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;Prompt2Forget&#65288;P2F&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#25945;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24536;&#35760;&#38544;&#31169;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;LLM&#26412;&#22320;&#38544;&#31169;&#25361;&#25112;&#12290;P2F&#26041;&#27861;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#29255;&#27573;&#24182;&#29983;&#25104;&#34394;&#26500;&#31572;&#26696;&#65292;&#27169;&#31946;&#21270;&#27169;&#22411;&#23545;&#21407;&#22987;&#36755;&#20837;&#30340;&#35760;&#24518;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;P2F&#20855;&#26377;&#24456;&#24378;&#30340;&#27169;&#31946;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#19979;&#33258;&#36866;&#24212;&#20351;&#29992;&#65292;&#26080;&#38656;&#25163;&#21160;&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2401.00870</link><description>&lt;p&gt;
&#25945;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24536;&#35760;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Teach Large Language Models to Forget Privacy. (arXiv:2401.00870v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00870
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;Prompt2Forget&#65288;P2F&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#25945;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24536;&#35760;&#38544;&#31169;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;LLM&#26412;&#22320;&#38544;&#31169;&#25361;&#25112;&#12290;P2F&#26041;&#27861;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#29255;&#27573;&#24182;&#29983;&#25104;&#34394;&#26500;&#31572;&#26696;&#65292;&#27169;&#31946;&#21270;&#27169;&#22411;&#23545;&#21407;&#22987;&#36755;&#20837;&#30340;&#35760;&#24518;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;P2F&#20855;&#26377;&#24456;&#24378;&#30340;&#27169;&#31946;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#19979;&#33258;&#36866;&#24212;&#20351;&#29992;&#65292;&#26080;&#38656;&#25163;&#21160;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#34987;&#35777;&#26126;&#20855;&#26377;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#38544;&#31169;&#27844;&#38706;&#30340;&#39118;&#38505;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#20445;&#25252;&#38544;&#31169;&#26041;&#27861;&#65292;&#22914;&#24046;&#20998;&#38544;&#31169;&#21644;&#21516;&#24577;&#21152;&#23494;&#65292;&#22312;&#21482;&#26377;&#40657;&#30418;API&#30340;&#29615;&#22659;&#19979;&#26159;&#19981;&#36275;&#22815;&#30340;&#65292;&#35201;&#27714;&#27169;&#22411;&#36879;&#26126;&#24615;&#25110;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Prompt2Forget&#65288;P2F&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#35299;&#20915;LLM&#26412;&#22320;&#38544;&#31169;&#25361;&#25112;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25945;&#23548;LLM&#24536;&#35760;&#26469;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#23558;&#23436;&#25972;&#38382;&#39064;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#29255;&#27573;&#65292;&#29983;&#25104;&#34394;&#26500;&#30340;&#31572;&#26696;&#65292;&#24182;&#20351;&#27169;&#22411;&#23545;&#21407;&#22987;&#36755;&#20837;&#30340;&#35760;&#24518;&#27169;&#31946;&#21270;&#12290;&#25105;&#20204;&#26681;&#25454;&#19981;&#21516;&#39046;&#22495;&#30340;&#21253;&#21547;&#38544;&#31169;&#25935;&#24863;&#20449;&#24687;&#30340;&#38382;&#39064;&#21019;&#24314;&#20102;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;P2F&#23454;&#29616;&#20102;&#38646;-shot&#27867;&#21270;&#65292;&#21487;&#20197;&#22312;&#22810;&#31181;&#24212;&#29992;&#22330;&#26223;&#19979;&#33258;&#36866;&#24212;&#65292;&#26080;&#38656;&#25163;&#21160;&#35843;&#25972;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;P2F&#20855;&#26377;&#24456;&#24378;&#30340;&#27169;&#31946;&#21270;LLM&#35760;&#24518;&#30340;&#33021;&#21147;&#65292;&#32780;&#19981;&#20250;&#25439;&#22833;&#20219;&#20309;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have proven powerful, but the risk of privacy leakage remains a significant concern. Traditional privacy-preserving methods, such as Differential Privacy and Homomorphic Encryption, are inadequate for black-box API-only settings, demanding either model transparency or heavy computational resources. We propose Prompt2Forget (P2F), the first framework designed to tackle the LLM local privacy challenge by teaching LLM to forget. The method involves decomposing full questions into smaller segments, generating fabricated answers, and obfuscating the model's memory of the original input. A benchmark dataset was crafted with questions containing privacy-sensitive information from diverse fields. P2F achieves zero-shot generalization, allowing adaptability across a wide range of use cases without manual adjustments. Experimental results indicate P2F's robust capability to obfuscate LLM's memory, attaining a forgetfulness score of around 90\% without any utility los
&lt;/p&gt;</description></item></channel></rss>