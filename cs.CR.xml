<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#20844;&#20849;&#25968;&#25454;&#26469;&#25913;&#36827;&#31169;&#26377;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#23398;&#20064;&#20844;&#20849;&#25968;&#25454;&#20013;&#30340;&#20849;&#20139;&#34920;&#31034;&#65292;&#21487;&#20197;&#22312;&#20004;&#31181;&#36801;&#31227;&#23398;&#20064;&#22330;&#26223;&#20013;&#23454;&#29616;&#26368;&#20248;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#22312;&#21333;&#20219;&#21153;&#36801;&#31227;&#22330;&#26223;&#20013;&#65292;&#31639;&#27861;&#22312;&#32473;&#23450;&#23376;&#31354;&#38388;&#33539;&#22260;&#20869;&#25628;&#32034;&#32447;&#24615;&#27169;&#22411;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20248;&#36229;&#39069;&#39118;&#38505;&#12290;&#22312;&#22810;&#20219;&#21153;&#20010;&#24615;&#21270;&#22330;&#26223;&#20013;&#65292;&#36275;&#22815;&#30340;&#20844;&#20849;&#25968;&#25454;&#21487;&#20197;&#28040;&#38500;&#31169;&#26377;&#21327;&#35843;&#38656;&#27714;&#65292;&#24182;&#36890;&#36807;&#32431;&#23616;&#37096;&#23398;&#20064;&#36798;&#21040;&#30456;&#21516;&#30340;&#25928;&#29992;&#12290;</title><link>http://arxiv.org/abs/2312.15551</link><description>&lt;p&gt;
&#21033;&#29992;&#20844;&#20849;&#34920;&#31034;&#26469;&#36827;&#34892;&#31169;&#26377;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Leveraging Public Representations for Private Transfer Learning. (arXiv:2312.15551v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15551
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#20844;&#20849;&#25968;&#25454;&#26469;&#25913;&#36827;&#31169;&#26377;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#23398;&#20064;&#20844;&#20849;&#25968;&#25454;&#20013;&#30340;&#20849;&#20139;&#34920;&#31034;&#65292;&#21487;&#20197;&#22312;&#20004;&#31181;&#36801;&#31227;&#23398;&#20064;&#22330;&#26223;&#20013;&#23454;&#29616;&#26368;&#20248;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#22312;&#21333;&#20219;&#21153;&#36801;&#31227;&#22330;&#26223;&#20013;&#65292;&#31639;&#27861;&#22312;&#32473;&#23450;&#23376;&#31354;&#38388;&#33539;&#22260;&#20869;&#25628;&#32034;&#32447;&#24615;&#27169;&#22411;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20248;&#36229;&#39069;&#39118;&#38505;&#12290;&#22312;&#22810;&#20219;&#21153;&#20010;&#24615;&#21270;&#22330;&#26223;&#20013;&#65292;&#36275;&#22815;&#30340;&#20844;&#20849;&#25968;&#25454;&#21487;&#20197;&#28040;&#38500;&#31169;&#26377;&#21327;&#35843;&#38656;&#27714;&#65292;&#24182;&#36890;&#36807;&#32431;&#23616;&#37096;&#23398;&#20064;&#36798;&#21040;&#30456;&#21516;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#23558;&#20844;&#20849;&#25968;&#25454;&#32435;&#20837;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#30340;&#26368;&#26032;&#23454;&#35777;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#20174;&#20844;&#20849;&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#20849;&#20139;&#34920;&#31034;&#22914;&#20309;&#25913;&#36827;&#31169;&#26377;&#23398;&#20064;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#32447;&#24615;&#22238;&#24402;&#30340;&#20004;&#31181;&#24120;&#35265;&#36801;&#31227;&#23398;&#20064;&#22330;&#26223;&#65292;&#20004;&#32773;&#37117;&#20551;&#35774;&#20844;&#20849;&#20219;&#21153;&#21644;&#31169;&#26377;&#20219;&#21153;&#65288;&#22238;&#24402;&#21521;&#37327;&#65289;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#20849;&#20139;&#19968;&#20010;&#20302;&#31209;&#23376;&#31354;&#38388;&#12290;&#22312;&#31532;&#19968;&#31181;&#21333;&#20219;&#21153;&#36801;&#31227;&#22330;&#26223;&#20013;&#65292;&#30446;&#26631;&#26159;&#23398;&#20064;&#19968;&#20010;&#22312;&#25152;&#26377;&#29992;&#25143;&#20043;&#38388;&#20849;&#20139;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#27599;&#20010;&#29992;&#25143;&#23545;&#24212;&#25968;&#25454;&#38598;&#20013;&#30340;&#19968;&#34892;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#21305;&#37197;&#30340;&#19978;&#19979;&#30028;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#32473;&#23450;&#23376;&#31354;&#38388;&#20272;&#35745;&#33539;&#22260;&#20869;&#25628;&#32034;&#32447;&#24615;&#27169;&#22411;&#30340;&#31639;&#27861;&#31867;&#20013;&#23454;&#29616;&#20102;&#26368;&#20248;&#36229;&#39069;&#39118;&#38505;&#12290;&#22312;&#22810;&#20219;&#21153;&#27169;&#22411;&#20010;&#24615;&#21270;&#30340;&#31532;&#20108;&#31181;&#24773;&#26223;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#22312;&#26377;&#36275;&#22815;&#30340;&#20844;&#20849;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#29992;&#25143;&#21487;&#20197;&#36991;&#20813;&#31169;&#26377;&#21327;&#35843;&#65292;&#22240;&#20026;&#22312;&#32473;&#23450;&#23376;&#31354;&#38388;&#20869;&#32431;&#31929;&#30340;&#23616;&#37096;&#23398;&#20064;&#21487;&#20197;&#36798;&#21040;&#30456;&#21516;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the recent empirical success of incorporating public data into differentially private learning, we theoretically investigate how a shared representation learned from public data can improve private learning. We explore two common scenarios of transfer learning for linear regression, both of which assume the public and private tasks (regression vectors) share a low-rank subspace in a high-dimensional space. In the first single-task transfer scenario, the goal is to learn a single model shared across all users, each corresponding to a row in a dataset. We provide matching upper and lower bounds showing that our algorithm achieves the optimal excess risk within a natural class of algorithms that search for the linear model within the given subspace estimate. In the second scenario of multitask model personalization, we show that with sufficient public data, users can avoid private coordination, as purely local learning within the given subspace achieves the same utility. Take
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25968;&#25454;&#23457;&#35745;&#21644;&#35302;&#21457;&#22120;&#22270;&#20687;&#36807;&#28388;&#31561;&#26426;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#29983;&#25104;&#35302;&#21457;&#22120;&#30340;&#38450;&#24481;&#26041;&#27861;&#26469;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#20813;&#21463;&#21518;&#38376;&#25915;&#20987;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21518;&#38376;&#25915;&#20987;&#29305;&#24449;&#26469;&#23398;&#20064;&#35302;&#21457;&#22120;&#65292;&#24182;&#29983;&#25104;&#20855;&#26377;&#26032;&#23398;&#20064;&#30693;&#35782;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2308.11333</link><description>&lt;p&gt;
&#26080;&#25968;&#25454;&#29983;&#25104;&#35302;&#21457;&#22120;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#20813;&#21463;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Protect Federated Learning Against Backdoor Attacks via Data-Free Trigger Generation. (arXiv:2308.11333v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11333
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#23457;&#35745;&#21644;&#35302;&#21457;&#22120;&#22270;&#20687;&#36807;&#28388;&#31561;&#26426;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#29983;&#25104;&#35302;&#21457;&#22120;&#30340;&#38450;&#24481;&#26041;&#27861;&#26469;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#20813;&#21463;&#21518;&#38376;&#25915;&#20987;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21518;&#38376;&#25915;&#20987;&#29305;&#24449;&#26469;&#23398;&#20064;&#35302;&#21457;&#22120;&#65292;&#24182;&#29983;&#25104;&#20855;&#26377;&#26032;&#23398;&#20064;&#30693;&#35782;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539; paradigm&#65292;&#32852;&#37030;&#23398;&#20064; (FL) &#21487;&#20197;&#20351;&#22823;&#35268;&#27169;&#23458;&#25143;&#31471;&#22312;&#19981;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21327;&#21516;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23545;&#19981;&#21487;&#20449;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#23457;&#35745;&#32570;&#22833;&#65292;FL &#26131;&#21463;&#27745;&#26579;&#25915;&#20987;&#65292;&#29305;&#21035;&#26159;&#21518;&#38376;&#25915;&#20987;&#12290;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#27745;&#26579;&#25968;&#25454;&#36827;&#34892;&#26412;&#22320;&#35757;&#32451;&#25110;&#30452;&#25509;&#26356;&#25913;&#27169;&#22411;&#21442;&#25968;&#65292;&#36731;&#32780;&#26131;&#20030;&#22320;&#23558;&#21518;&#38376;&#27880;&#20837;&#27169;&#22411;&#65292;&#20174;&#32780;&#35302;&#21457;&#27169;&#22411;&#23545;&#22270;&#20687;&#20013;&#30340;&#30446;&#26631;&#27169;&#24335;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20004;&#20010;&#21518;&#38376;&#25915;&#20987;&#29305;&#24449;&#30340;&#26032;&#22411;&#26080;&#25968;&#25454;&#29983;&#25104;&#35302;&#21457;&#22120;&#38450;&#24481;&#26041;&#27861;&#65306;i) &#35302;&#21457;&#22120;&#23398;&#20064;&#36895;&#24230;&#27604;&#26222;&#36890;&#30693;&#35782;&#26356;&#24555;&#65292;ii) &#35302;&#21457;&#22120;&#27169;&#24335;&#23545;&#22270;&#20687;&#20998;&#31867;&#30340;&#24433;&#21709;&#22823;&#20110;&#26222;&#36890;&#31867;&#21035;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#35782;&#21035;&#26087;&#21644;&#26032;&#20840;&#23616;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#29983;&#25104;&#20855;&#26377;&#26032;&#23398;&#20064;&#30693;&#35782;&#30340;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#26041;&#27861;&#36807;&#28388;&#35302;&#21457;&#22120;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a distributed machine learning paradigm, Federated Learning (FL) enables large-scale clients to collaboratively train a model without sharing their raw data. However, due to the lack of data auditing for untrusted clients, FL is vulnerable to poisoning attacks, especially backdoor attacks. By using poisoned data for local training or directly changing the model parameters, attackers can easily inject backdoors into the model, which can trigger the model to make misclassification of targeted patterns in images. To address these issues, we propose a novel data-free trigger-generation-based defense approach based on the two characteristics of backdoor attacks: i) triggers are learned faster than normal knowledge, and ii) trigger patterns have a greater effect on image classification than normal class patterns. Our approach generates the images with newly learned knowledge by identifying the differences between the old and new global models, and filters trigger images by evaluating the 
&lt;/p&gt;</description></item></channel></rss>