<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#31163;&#32447;&#25968;&#25454;&#20013;&#30340;&#38544;&#31169;&#27169;&#22411;&#20197;&#21450;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#35757;&#32451;&#20855;&#26377;&#38544;&#31169;&#20445;&#25252;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#24635;&#32467;&#20102;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#38544;&#31169;&#30340;&#20195;&#20215;&#12290;</title><link>https://arxiv.org/abs/2402.05525</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Model-Based Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#31163;&#32447;&#25968;&#25454;&#20013;&#30340;&#38544;&#31169;&#27169;&#22411;&#20197;&#21450;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#35757;&#32451;&#20855;&#26377;&#38544;&#31169;&#20445;&#25252;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#24635;&#32467;&#20102;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#38544;&#31169;&#30340;&#20195;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#20855;&#26377;&#38544;&#31169;&#20445;&#35777;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#35757;&#32451;&#19968;&#20010;&#30456;&#23545;&#20110;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#36712;&#36857;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#30340;&#31574;&#30053;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DP-MORL&#65292;&#19968;&#31181;&#24102;&#26377;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#30340;MBRL&#31639;&#27861;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;DP-FedAvg&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#23398;&#20064;&#29615;&#22659;&#30340;&#38544;&#31169;&#27169;&#22411;&#65292;DP-FedAvg&#26159;&#19968;&#31181;&#20026;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#36712;&#36857;&#32423;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#20248;&#21270;&#20174;&#65288;&#21463;&#32602;&#30340;&#65289;&#38544;&#31169;&#27169;&#22411;&#20013;&#25512;&#23548;&#20986;&#31574;&#30053;&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#19982;&#31995;&#32479;&#20132;&#20114;&#25110;&#35775;&#38382;&#36755;&#20837;&#25968;&#25454;&#12290;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;&#65292;DP-MORL&#33021;&#22815;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#35757;&#32451;&#20986;&#20855;&#26377;&#38544;&#31169;&#20445;&#25252;&#30340;RL&#20195;&#29702;&#65292;&#24182;&#36827;&#19968;&#27493;&#27010;&#36848;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#38544;&#31169;&#30340;&#20195;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address offline reinforcement learning with privacy guarantees, where the goal is to train a policy that is differentially private with respect to individual trajectories in the dataset. To achieve this, we introduce DP-MORL, an MBRL algorithm coming with differential privacy guarantees. A private model of the environment is first learned from offline data using DP-FedAvg, a training method for neural networks that provides differential privacy guarantees at the trajectory level. Then, we use model-based policy optimization to derive a policy from the (penalized) private model, without any further interaction with the system or access to the input data. We empirically show that DP-MORL enables the training of private RL agents from offline data and we furthermore outline the price of privacy in this setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#23454;&#38469;&#31995;&#32479;&#20013;&#26816;&#27979;&#21644;&#28040;&#38500;&#24694;&#24847;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.04055</link><description>&lt;p&gt;
&#25226;&#22351;&#20154;&#36386;&#20986;&#21435;&#65281;&#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Kick Bad Guys Out! Zero-Knowledge-Proof-Based Anomaly Detection in Federated Learning. (arXiv:2310.04055v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#23454;&#38469;&#31995;&#32479;&#20013;&#26816;&#27979;&#21644;&#28040;&#38500;&#24694;&#24847;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#23458;&#25143;&#31471;&#30340;&#25915;&#20987;&#65292;&#20182;&#20204;&#36890;&#36807;&#25552;&#20132;&#31713;&#25913;&#30340;&#26412;&#22320;&#27169;&#22411;&#26469;&#36798;&#21040;&#23545;&#25239;&#30446;&#26631;&#65292;&#27604;&#22914;&#38459;&#27490;&#20840;&#23616;&#27169;&#22411;&#30340;&#25910;&#25947;&#25110;&#32773;&#23548;&#33268;&#20840;&#23616;&#27169;&#22411;&#23545;&#26576;&#20123;&#25968;&#25454;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#12290;&#35768;&#22810;&#29616;&#26377;&#30340;&#38450;&#24481;&#26426;&#21046;&#22312;&#23454;&#38469;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#19981;&#21487;&#34892;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#20808;&#30693;&#36947;&#24694;&#24847;&#23458;&#25143;&#31471;&#30340;&#25968;&#37327;&#65292;&#25110;&#32773;&#20381;&#36182;&#37325;&#26032;&#21152;&#26435;&#25110;&#20462;&#25913;&#25552;&#20132;&#30340;&#26041;&#24335;&#12290;&#36825;&#26159;&#22240;&#20026;&#25915;&#20987;&#32773;&#36890;&#24120;&#19981;&#20250;&#22312;&#25915;&#20987;&#20043;&#21069;&#23459;&#24067;&#20182;&#20204;&#30340;&#24847;&#22270;&#65292;&#32780;&#37325;&#26032;&#21152;&#26435;&#21487;&#33021;&#20250;&#25913;&#21464;&#32858;&#21512;&#32467;&#26524;&#65292;&#21363;&#20351;&#27809;&#26377;&#25915;&#20987;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#22312;&#23454;&#38469;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26368;&#23574;&#31471;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#20855;&#26377;&#20197;&#19979;&#29305;&#28857;&#65306;i&#65289;&#20165;&#22312;&#21457;&#29983;&#25915;&#20987;&#26102;&#26816;&#27979;&#25915;&#20987;&#30340;&#21457;&#29983;&#24182;&#36827;&#34892;&#38450;&#24481;&#25805;&#20316;&#65307;ii&#65289;&#19968;&#26086;&#21457;&#29983;&#25915;&#20987;&#65292;&#36827;&#19968;&#27493;&#26816;&#27979;&#24694;&#24847;&#23458;&#25143;&#31471;&#27169;&#22411;&#24182;&#23558;&#20854;&#28040;&#38500;&#65292;&#32780;&#19981;&#20250;&#23545;&#27491;&#24120;&#27169;&#22411;&#36896;&#25104;&#20260;&#23475;&#65307;iii&#65289;&#30830;&#20445;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) systems are vulnerable to malicious clients that submit poisoned local models to achieve their adversarial goals, such as preventing the convergence of the global model or inducing the global model to misclassify some data. Many existing defense mechanisms are impractical in real-world FL systems, as they require prior knowledge of the number of malicious clients or rely on re-weighting or modifying submissions. This is because adversaries typically do not announce their intentions before attacking, and re-weighting might change aggregation results even in the absence of attacks. To address these challenges in real FL systems, this paper introduces a cutting-edge anomaly detection approach with the following features: i) Detecting the occurrence of attacks and performing defense operations only when attacks happen; ii) Upon the occurrence of an attack, further detecting the malicious client models and eliminating them without harming the benign ones; iii) Ensuri
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20004;&#26041;&#22312;&#25968;&#25454;&#38598;&#19978;&#21512;&#20316;&#21069;&#22914;&#20309;&#20445;&#35777;&#21512;&#20316;&#30340;&#20215;&#20540;&#12290;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#20840;&#21516;&#24577;&#21152;&#23494;&#26041;&#26696;&#21644;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#30340;&#20132;&#20114;&#24335;&#21327;&#35758;&#65292;&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#12289;&#31169;&#23494;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#32456;&#30340;&#32467;&#26524;&#26159;&#30830;&#20445;&#21512;&#20316;&#21069;&#21452;&#26041;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19981;&#20250;&#34987;&#36879;&#38706;&#12290;</title><link>http://arxiv.org/abs/2310.02563</link><description>&lt;p&gt;
&#23454;&#29992;&#30340;&#12289;&#31169;&#23494;&#30340;&#21512;&#20316;&#20215;&#20540;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Practical, Private Assurance of the Value of Collaboration. (arXiv:2310.02563v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02563
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20004;&#26041;&#22312;&#25968;&#25454;&#38598;&#19978;&#21512;&#20316;&#21069;&#22914;&#20309;&#20445;&#35777;&#21512;&#20316;&#30340;&#20215;&#20540;&#12290;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#20840;&#21516;&#24577;&#21152;&#23494;&#26041;&#26696;&#21644;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#30340;&#20132;&#20114;&#24335;&#21327;&#35758;&#65292;&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#12289;&#31169;&#23494;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#32456;&#30340;&#32467;&#26524;&#26159;&#30830;&#20445;&#21512;&#20316;&#21069;&#21452;&#26041;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19981;&#20250;&#34987;&#36879;&#38706;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20004;&#20010;&#26041;&#21521;&#24076;&#26395;&#22312;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#21512;&#20316;&#12290;&#28982;&#32780;&#65292;&#22312;&#24444;&#27492;&#36879;&#38706;&#25968;&#25454;&#38598;&#20043;&#21069;&#65292;&#21452;&#26041;&#24076;&#26395;&#33021;&#22815;&#24471;&#21040;&#21512;&#20316;&#23558;&#26159;&#23500;&#26377;&#25104;&#26524;&#30340;&#20445;&#35777;&#12290;&#25105;&#20204;&#20174;&#26426;&#22120;&#23398;&#20064;&#30340;&#35282;&#24230;&#26469;&#30475;&#24453;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#26041;&#34987;&#25215;&#35834;&#36890;&#36807;&#21512;&#24182;&#26469;&#33258;&#21478;&#19968;&#26041;&#30340;&#25968;&#25454;&#26469;&#25913;&#36827;&#20854;&#39044;&#27979;&#27169;&#22411;&#12290;&#21482;&#26377;&#24403;&#26356;&#26032;&#30340;&#27169;&#22411;&#26174;&#31034;&#20986;&#20934;&#30830;&#24615;&#30340;&#25552;&#21319;&#26102;&#65292;&#21452;&#26041;&#25165;&#24076;&#26395;&#36827;&#19968;&#27493;&#21512;&#20316;&#12290;&#22312;&#30830;&#23450;&#36825;&#19968;&#28857;&#20043;&#21069;&#65292;&#21452;&#26041;&#19981;&#24076;&#26395;&#36879;&#38706;&#20182;&#20204;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;Torus&#19978;&#30340;&#20840;&#21516;&#24577;&#21152;&#23494;&#26041;&#26696;&#65288;TFHE&#65289;&#21644;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#26500;&#24314;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#21327;&#35758;&#65292;&#20854;&#20013;&#24213;&#23618;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26159;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#12290;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#29992;&#20110;&#30830;&#20445;&#35745;&#31639;&#19981;&#23436;&#20840;&#22312;&#21152;&#23494;&#39046;&#22495;&#36827;&#34892;&#65292;&#36825;&#23545;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#35201;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;
Two parties wish to collaborate on their datasets. However, before they reveal their datasets to each other, the parties want to have the guarantee that the collaboration would be fruitful. We look at this problem from the point of view of machine learning, where one party is promised an improvement on its prediction model by incorporating data from the other party. The parties would only wish to collaborate further if the updated model shows an improvement in accuracy. Before this is ascertained, the two parties would not want to disclose their models and datasets. In this work, we construct an interactive protocol for this problem based on the fully homomorphic encryption scheme over the Torus (TFHE) and label differential privacy, where the underlying machine learning model is a neural network. Label differential privacy is used to ensure that computations are not done entirely in the encrypted domain, which is a significant bottleneck for neural network training according to the cu
&lt;/p&gt;</description></item></channel></rss>