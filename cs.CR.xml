<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#27700;&#21360;&#39046;&#22495;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#25915;&#20987;&#32773;&#26080;&#27861;&#35775;&#38382;&#27700;&#21360;&#27169;&#22411;&#25110;&#26816;&#27979;API&#30340;&#24773;&#20917;&#19979;&#65292;&#27700;&#21360;&#22522;&#30784;&#30340;AI&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#22120;&#20063;&#26080;&#27861;&#25269;&#25239;&#23545;&#25239;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.15365</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#22270;&#20687;&#27700;&#21360;&#30340;&#36716;&#31227;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
A Transfer Attack to Image Watermarks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15365
&lt;/p&gt;
&lt;p&gt;
&#27700;&#21360;&#39046;&#22495;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#25915;&#20987;&#32773;&#26080;&#27861;&#35775;&#38382;&#27700;&#21360;&#27169;&#22411;&#25110;&#26816;&#27979;API&#30340;&#24773;&#20917;&#19979;&#65292;&#27700;&#21360;&#22522;&#30784;&#30340;AI&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#22120;&#20063;&#26080;&#27861;&#25269;&#25239;&#23545;&#25239;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#21360;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24037;&#19994;&#39046;&#22495;&#65292;&#29992;&#20110;&#26816;&#27979;&#30001;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#25991;&#29486;&#20013;&#23545;&#36825;&#31181;&#22522;&#20110;&#27700;&#21360;&#30340;&#26816;&#27979;&#22120;&#22312;&#30333;&#30418;&#21644;&#40657;&#30418;&#29615;&#22659;&#19979;&#23545;&#25239;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#26377;&#24456;&#22909;&#30340;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#26080;&#30418;&#29615;&#22659;&#19979;&#30340;&#31283;&#20581;&#24615;&#21364;&#30693;&#20043;&#29978;&#23569;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22810;&#39033;&#30740;&#31350;&#22768;&#31216;&#22270;&#20687;&#27700;&#21360;&#22312;&#36825;&#31181;&#29615;&#22659;&#19979;&#26159;&#31283;&#20581;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36716;&#31227;&#23545;&#25239;&#25915;&#20987;&#26469;&#38024;&#23545;&#26080;&#30418;&#29615;&#22659;&#19979;&#30340;&#22270;&#20687;&#27700;&#21360;&#12290;&#25105;&#20204;&#30340;&#36716;&#31227;&#25915;&#20987;&#21521;&#24102;&#27700;&#21360;&#30340;&#22270;&#20687;&#28155;&#21152;&#24494;&#25200;&#65292;&#20197;&#36530;&#36991;&#34987;&#25915;&#20987;&#32773;&#35757;&#32451;&#30340;&#22810;&#20010;&#26367;&#20195;&#27700;&#21360;&#27169;&#22411;&#65292;&#24182;&#19988;&#32463;&#36807;&#25200;&#21160;&#30340;&#24102;&#27700;&#21360;&#22270;&#20687;&#20063;&#33021;&#36530;&#36991;&#30446;&#26631;&#27700;&#21360;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#29702;&#35770;&#19978;&#21644;&#32463;&#39564;&#19978;&#23637;&#31034;&#20102;&#65292;&#22522;&#20110;&#27700;&#21360;&#30340;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#22120;&#21363;&#20351;&#25915;&#20987;&#32773;&#27809;&#26377;&#35775;&#38382;&#27700;&#21360;&#27169;&#22411;&#25110;&#26816;&#27979;API&#65292;&#20063;&#19981;&#20855;&#26377;&#23545;&#25239;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15365v1 Announce Type: cross  Abstract: Watermark has been widely deployed by industry to detect AI-generated images. The robustness of such watermark-based detector against evasion attacks in the white-box and black-box settings is well understood in the literature. However, the robustness in the no-box setting is much less understood. In particular, multiple studies claimed that image watermark is robust in such setting. In this work, we propose a new transfer evasion attack to image watermark in the no-box setting. Our transfer attack adds a perturbation to a watermarked image to evade multiple surrogate watermarking models trained by the attacker itself, and the perturbed watermarked image also evades the target watermarking model. Our major contribution is to show that, both theoretically and empirically, watermark-based AI-generated image detector is not robust to evasion attacks even if the attacker does not have access to the watermarking model nor the detection API.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#20998;&#31867;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26426;&#21046;CARSO&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#27604;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26356;&#22909;&#22320;&#20445;&#25252;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#20928;&#21270;&#26469;&#36827;&#34892;&#26368;&#32456;&#20998;&#31867;&#65292;&#24182;&#25104;&#21151;&#22320;&#20445;&#25252;&#33258;&#24049;&#20813;&#21463;&#26410;&#39044;&#35265;&#30340;&#23041;&#32961;&#21644;&#26368;&#32456;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2306.06081</link><description>&lt;p&gt;
CARSO: &#23545;&#25239;&#24615;&#21512;&#25104;&#35266;&#27979;&#30340;&#21453;&#23545;&#25239;&#24615;&#21484;&#22238;
&lt;/p&gt;
&lt;p&gt;
CARSO: Counter-Adversarial Recall of Synthetic Observations. (arXiv:2306.06081v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#20998;&#31867;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26426;&#21046;CARSO&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#27604;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26356;&#22909;&#22320;&#20445;&#25252;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#20928;&#21270;&#26469;&#36827;&#34892;&#26368;&#32456;&#20998;&#31867;&#65292;&#24182;&#25104;&#21151;&#22320;&#20445;&#25252;&#33258;&#24049;&#20813;&#21463;&#26410;&#39044;&#35265;&#30340;&#23041;&#32961;&#21644;&#26368;&#32456;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26426;&#21046;CARSO&#65292;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#65292;&#28789;&#24863;&#26469;&#33258;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30340;&#32447;&#32034;&#12290;&#35813;&#26041;&#27861;&#19982;&#23545;&#25239;&#35757;&#32451;&#20855;&#26377;&#21327;&#21516;&#20114;&#34917;&#24615;&#65292;&#24182;&#20381;&#36182;&#20110;&#34987;&#25915;&#20987;&#20998;&#31867;&#22120;&#30340;&#20869;&#37096;&#34920;&#31034;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#20928;&#21270;&#65292;&#35813;&#26041;&#27861;&#37319;&#26679;&#36755;&#20837;&#30340;&#37325;&#26500;&#26469;&#36827;&#34892;&#26368;&#32456;&#20998;&#31867;&#12290;&#22312;&#21508;&#31181;&#22270;&#20687;&#25968;&#25454;&#38598;&#21644;&#20998;&#31867;&#22120;&#20307;&#31995;&#32467;&#26500;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;CARSO&#33021;&#22815;&#27604;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26356;&#22909;&#22320;&#20445;&#25252;&#20998;&#31867;&#22120;&#8212;&#8212;&#21516;&#26102;&#20855;&#26377;&#21487;&#25509;&#21463;&#30340;&#28165;&#27905;&#20934;&#30830;&#24230;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#38450;&#24481;&#20307;&#31995;&#32467;&#26500;&#25104;&#21151;&#22320;&#20445;&#25252;&#33258;&#24049;&#20813;&#21463;&#26410;&#39044;&#35265;&#30340;&#23041;&#32961;&#21644;&#26368;&#32456;&#25915;&#20987;&#12290;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#22312;https://github.com/&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel adversarial defence mechanism for image classification -- CARSO -- inspired by cues from cognitive neuroscience. The method is synergistically complementary to adversarial training and relies on knowledge of the internal representation of the attacked classifier. Exploiting a generative model for adversarial purification, conditioned on such representation, it samples reconstructions of inputs to be finally classified. Experimental evaluation by a well-established benchmark of varied, strong adaptive attacks, across diverse image datasets and classifier architectures, shows that CARSO is able to defend the classifier significantly better than state-of-the-art adversarial training alone -- with a tolerable clean accuracy toll. Furthermore, the defensive architecture succeeds in effectively shielding itself from unforeseen threats, and end-to-end attacks adapted to fool stochastic defences. Code and pre-trained models are available at https://github.com/
&lt;/p&gt;</description></item></channel></rss>