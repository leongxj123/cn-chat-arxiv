<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;SHERD&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#30417;&#25511;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#26089;&#26399;&#35757;&#32451;&#34920;&#31034;&#20013;&#30340;&#20449;&#24687;&#65292;&#21033;&#29992;&#26631;&#20934;&#36317;&#31163;&#24230;&#37327;&#26816;&#27979;&#26131;&#21463;&#25915;&#20987;&#33410;&#28857;&#65292;&#20174;&#32780;&#22312;&#22270;&#36755;&#20837;&#20013;&#23454;&#29616;&#24615;&#33021;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09901</link><description>&lt;p&gt;
&#36890;&#36807;&#30417;&#25511;&#26089;&#26399;&#35757;&#32451;&#34920;&#31034;&#26469;&#23454;&#29616;&#40065;&#26834;&#30340;&#23376;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Subgraph Learning by Monitoring Early Training Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;SHERD&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#30417;&#25511;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#26089;&#26399;&#35757;&#32451;&#34920;&#31034;&#20013;&#30340;&#20449;&#24687;&#65292;&#21033;&#29992;&#26631;&#20934;&#36317;&#31163;&#24230;&#37327;&#26816;&#27979;&#26131;&#21463;&#25915;&#20987;&#33410;&#28857;&#65292;&#20174;&#32780;&#22312;&#22270;&#36755;&#20837;&#20013;&#23454;&#29616;&#24615;&#33021;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#25991;:2403.09901v1 &#20844;&#21578;&#31867;&#22411;:&#26032;&#25688;&#35201;:&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#22240;&#22312;&#22270;&#23398;&#20064;&#21644;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#26131;&#21463;&#25915;&#20987;&#30340;&#33410;&#28857;&#65292;&#32473;&#20915;&#31574;&#21046;&#23450;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#40065;&#26834;&#30340;&#22270;&#25688;&#35201;&#38656;&#27714;&#22312;&#20110;&#23545;&#25239;&#24615;&#25361;&#25112;&#20250;&#23548;&#33268;&#25915;&#20987;&#22312;&#25972;&#20010;&#22270;&#20013;&#20256;&#25773;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#25216;&#26415;SHERD (&#36890;&#36807;&#26089;&#26399;&#35757;&#32451;&#34920;&#31034;&#36317;&#31163;&#36827;&#34892;&#23376;&#22270;&#23398;&#20064;)&#26469;&#35299;&#20915;&#22270;&#36755;&#20837;&#20013;&#30340;&#24615;&#33021;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;SHERD&#21033;&#29992;&#37096;&#20998;&#35757;&#32451;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;(GCN)&#30340;&#23618;&#20449;&#24687;&#65292;&#36890;&#36807;&#26631;&#20934;&#36317;&#31163;&#24230;&#37327;&#26469;&#26816;&#27979;&#23545;&#25239;&#25915;&#20987;&#26399;&#38388;&#26131;&#21463;&#25915;&#20987;&#30340;&#33410;&#28857;&#12290;&#35813;&#26041;&#27861;&#35782;&#21035;&#20986;"&#26131;&#21463;&#25915;&#20987;&#30340;(&#22351;)"&#33410;&#28857;&#24182;&#31227;&#38500;&#36825;&#20123;&#33410;&#28857;&#65292;&#24418;&#25104;&#19968;&#20010;&#40065;&#26834;&#30340;&#23376;&#22270;&#65292;&#21516;&#26102;&#20445;&#25345;&#33410;&#28857;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09901v1 Announce Type: new  Abstract: Graph neural networks (GNNs) have attracted significant attention for their outstanding performance in graph learning and node classification tasks. However, their vulnerability to adversarial attacks, particularly through susceptible nodes, poses a challenge in decision-making. The need for robust graph summarization is evident in adversarial challenges resulting from the propagation of attacks throughout the entire graph. In this paper, we address both performance and adversarial robustness in graph input by introducing the novel technique SHERD (Subgraph Learning Hale through Early Training Representation Distances). SHERD leverages information from layers of a partially trained graph convolutional network (GCN) to detect susceptible nodes during adversarial attacks using standard distance metrics. The method identifies "vulnerable (bad)" nodes and removes such nodes to form a robust subgraph while maintaining node classification perf
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#26032;&#25351;&#26631;&#23545;&#25239;&#36229;&#20307;&#31215;&#26469;&#20840;&#38754;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22810;&#31181;&#25200;&#21160;&#24378;&#24230;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#37319;&#29992;&#26032;&#22411;&#35757;&#32451;&#31639;&#27861;&#26469;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05100</link><description>&lt;p&gt;
&#25506;&#32034;&#23545;&#25239;&#30028;&#38480;&#65306;&#36890;&#36807;&#23545;&#25239;&#36229;&#20307;&#31215;&#37327;&#21270;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring the Adversarial Frontier: Quantifying Robustness via Adversarial Hypervolume
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05100
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#26032;&#25351;&#26631;&#23545;&#25239;&#36229;&#20307;&#31215;&#26469;&#20840;&#38754;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22810;&#31181;&#25200;&#21160;&#24378;&#24230;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#37319;&#29992;&#26032;&#22411;&#35757;&#32451;&#31639;&#27861;&#26469;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38754;&#20020;&#26085;&#30410;&#20005;&#37325;&#30340;&#23545;&#25239;&#25915;&#20987;&#23041;&#32961;&#65292;&#29305;&#21035;&#26159;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#65292;&#24378;&#35843;&#20102;&#23545;&#40065;&#26834;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#38656;&#27714;&#12290;&#20256;&#32479;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#20381;&#36182;&#20110;&#23545;&#25239;&#20934;&#30830;&#24615;&#65292;&#35813;&#25351;&#26631;&#34913;&#37327;&#27169;&#22411;&#22312;&#29305;&#23450;&#25200;&#21160;&#24378;&#24230;&#19979;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#21333;&#19968;&#25351;&#26631;&#24182;&#19981;&#33021;&#23436;&#20840;&#27010;&#25324;&#27169;&#22411;&#23545;&#19981;&#21516;&#31243;&#24230;&#25200;&#21160;&#30340;&#25972;&#20307;&#38887;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;&#65292;&#31216;&#20026;&#23545;&#25239;&#36229;&#20307;&#31215;&#65292;&#20174;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#35282;&#24230;&#32508;&#21512;&#35780;&#20272;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#25200;&#21160;&#24378;&#24230;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;&#35813;&#25351;&#26631;&#20801;&#35768;&#28145;&#20837;&#27604;&#36739;&#38450;&#24481;&#26426;&#21046;&#65292;&#24182;&#25215;&#35748;&#20102;&#36739;&#24369;&#30340;&#38450;&#24481;&#31574;&#30053;&#25152;&#24102;&#26469;&#30340;&#40065;&#26834;&#24615;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#22343;&#21248;&#24615;&#30340;&#26032;&#22411;&#35757;&#32451;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05100v1 Announce Type: cross  Abstract: The escalating threat of adversarial attacks on deep learning models, particularly in security-critical fields, has underscored the need for robust deep learning systems. Conventional robustness evaluations have relied on adversarial accuracy, which measures a model's performance under a specific perturbation intensity. However, this singular metric does not fully encapsulate the overall resilience of a model against varying degrees of perturbation. To address this gap, we propose a new metric termed adversarial hypervolume, assessing the robustness of deep learning models comprehensively over a range of perturbation intensities from a multi-objective optimization standpoint. This metric allows for an in-depth comparison of defense mechanisms and recognizes the trivial improvements in robustness afforded by less potent defensive strategies. Additionally, we adopt a novel training algorithm that enhances adversarial robustness uniformly
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#25351;&#20196;&#20013;&#24515;&#21709;&#24212;&#30340;&#23481;&#24525;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#22797;&#26434;&#26597;&#35810;&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25581;&#31034;&#35302;&#21457;&#19981;&#36947;&#24503;&#21709;&#24212;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15302</link><description>&lt;p&gt;
&#26377;&#20851;LLMs&#25351;&#20196;&#20013;&#24515;&#21709;&#24212;&#30340;&#65288;&#19981;&#36947;&#24503;&#65289;&#31243;&#24230;&#26377;&#22810;&#39640;&#65311;&#25581;&#31034;&#23433;&#20840;&#38450;&#25252;&#26639;&#23545;&#26377;&#23475;&#26597;&#35810;&#30340;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
How (un)ethical are instruction-centric responses of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#25351;&#20196;&#20013;&#24515;&#21709;&#24212;&#30340;&#23481;&#24525;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#22797;&#26434;&#26597;&#35810;&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25581;&#31034;&#35302;&#21457;&#19981;&#36947;&#24503;&#21709;&#24212;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20010;&#22260;&#32469;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23433;&#20840;&#21644;&#36947;&#24503;&#20351;&#29992;&#26085;&#30410;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#20250;&#34987;&#21508;&#31181;&#22797;&#26434;&#30340;&#26041;&#27861;&#27450;&#39575;&#65292;&#20135;&#29983;&#26377;&#23475;&#25110;&#19981;&#36947;&#24503;&#20869;&#23481;&#65292;&#21253;&#25324;&#8220;&#36234;&#29425;&#8221;&#25216;&#26415;&#21644;&#26377;&#38024;&#23545;&#24615;&#30340;&#25805;&#32437;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#38598;&#20013;&#22312;&#19968;&#20010;&#29305;&#23450;&#38382;&#39064;&#19978;&#65306;LLMs&#22312;&#35201;&#27714;&#23427;&#20204;&#29983;&#25104;&#20197;&#20266;&#20195;&#30721;&#12289;&#31243;&#24207;&#25110;&#36719;&#20214;&#29255;&#27573;&#20026;&#20013;&#24515;&#30340;&#21709;&#24212;&#26102;&#65292;&#26377;&#22810;&#22823;&#31243;&#24230;&#19978;&#21487;&#33021;&#20250;&#34987;&#35823;&#23548;&#65292;&#32780;&#19981;&#26159;&#29983;&#25104;&#26222;&#36890;&#25991;&#26412;&#12290;&#20026;&#20102;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TechHazardQA&#65292;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#24212;&#20197;&#25991;&#26412;&#21644;&#20197;&#25351;&#20196;&#20026;&#20013;&#24515;&#26684;&#24335;&#65288;&#20363;&#22914;&#20266;&#20195;&#30721;&#65289;&#22238;&#31572;&#30340;&#22797;&#26434;&#26597;&#35810;&#65292;&#26088;&#22312;&#35782;&#21035;&#19981;&#36947;&#24503;&#21709;&#24212;&#30340;&#35302;&#21457;&#22120;&#12290;&#25105;&#20204;&#26597;&#35810;&#20102;&#19968;&#31995;&#21015;LLMs-- Llama-2-13b&#65292;Llama-2-7b&#65292;Mistral-V2&#21644;Mistral 8X7B--&#24182;&#35201;&#27714;&#23427;&#20204;&#29983;&#25104;&#25991;&#26412;&#21644;&#25351;&#20196;&#20026;&#20013;&#24515;&#30340;&#21709;&#24212;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15302v1 Announce Type: new  Abstract: In this study, we tackle a growing concern around the safety and ethical use of large language models (LLMs). Despite their potential, these models can be tricked into producing harmful or unethical content through various sophisticated methods, including 'jailbreaking' techniques and targeted manipulation. Our work zeroes in on a specific issue: to what extent LLMs can be led astray by asking them to generate responses that are instruction-centric such as a pseudocode, a program or a software snippet as opposed to vanilla text. To investigate this question, we introduce TechHazardQA, a dataset containing complex queries which should be answered in both text and instruction-centric formats (e.g., pseudocodes), aimed at identifying triggers for unethical responses. We query a series of LLMs -- Llama-2-13b, Llama-2-7b, Mistral-V2 and Mistral 8X7B -- and ask them to generate both text and instruction-centric responses. For evaluation we rep
&lt;/p&gt;</description></item></channel></rss>