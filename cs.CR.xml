<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>XAI&#39046;&#22495;&#34987;&#21010;&#20998;&#20026;&#34013;&#33394;XAI&#21644;&#32418;&#33394;XAI&#20004;&#31181;&#35299;&#37322;&#25991;&#21270;&#65292;&#25351;&#20986;&#20102;&#32418;&#33394;XAI&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;&#21644;&#30740;&#31350;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.13914</link><description>&lt;p&gt;
&#19981;&#26159;&#20026;&#20102;&#36777;&#35299;&#32780;&#26159;&#20026;&#20102;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Explain to Question not to Justify
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13914
&lt;/p&gt;
&lt;p&gt;
XAI&#39046;&#22495;&#34987;&#21010;&#20998;&#20026;&#34013;&#33394;XAI&#21644;&#32418;&#33394;XAI&#20004;&#31181;&#35299;&#37322;&#25991;&#21270;&#65292;&#25351;&#20986;&#20102;&#32418;&#33394;XAI&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;&#21644;&#30740;&#31350;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26159;&#19968;&#20010;&#24180;&#36731;&#20294;&#38750;&#24120;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#35813;&#39046;&#22495;&#30446;&#21069;&#30340;&#36827;&#23637;&#21463;&#21040;&#20102;&#19981;&#21516;&#21644;&#19981;&#20860;&#23481;&#30446;&#26631;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;XAI&#39046;&#22495;&#20869;&#32416;&#32544;&#22312;&#19968;&#36215;&#30340;&#21508;&#31181;&#32447;&#32034;&#20998;&#20026;&#20004;&#31181;&#20114;&#34917;&#30340;&#25991;&#21270;&#65292;&#21363;&#20154;&#31867;/&#20215;&#20540;&#21462;&#21521;&#35299;&#37322;&#65288;&#34013;&#33394;XAI&#65289;&#21644;&#27169;&#22411;/&#39564;&#35777;&#21462;&#21521;&#35299;&#37322;&#65288;&#32418;&#33394;XAI&#65289;&#12290;&#25105;&#20204;&#36824;&#35748;&#20026;&#65292;&#32418;&#33394;XAI&#39046;&#22495;&#30446;&#21069;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#65292;&#38544;&#34255;&#30528;&#24040;&#22823;&#30340;&#26426;&#36935;&#21644;&#37325;&#35201;&#30740;&#31350;&#30340;&#28508;&#21147;&#65292;&#20197;&#30830;&#20445;AI&#31995;&#32479;&#30340;&#23433;&#20840;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#36825;&#19968;&#39046;&#22495;&#30340;&#26377;&#21069;&#36884;&#30340;&#25361;&#25112;&#26469;&#24635;&#32467;&#26412;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13914v1 Announce Type: new  Abstract: Explainable Artificial Intelligence (XAI) is a young but very promising field of research. Unfortunately, the progress in this field is currently slowed down by divergent and incompatible goals. In this paper, we separate various threads tangled within the area of XAI into two complementary cultures of human/value-oriented explanations (BLUE XAI) and model/validation-oriented explanations (RED XAI). We also argue that the area of RED XAI is currently under-explored and hides great opportunities and potential for important research necessary to ensure the safety of AI systems. We conclude this paper by presenting promising challenges in this area.
&lt;/p&gt;</description></item></channel></rss>