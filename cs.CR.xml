<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#26041;&#27861;&#31995;&#32479;&#22320;&#27979;&#35797;&#20102;&#32454;&#35843;&#22823;&#22411;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#23454;&#38469;&#38544;&#31169;&#28431;&#27934;&#65292;&#24182;&#21457;&#29616;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#31867;&#21035;&#30340;&#31034;&#20363;&#25968;&#37327;&#20197;&#21450;&#35757;&#32451;&#32467;&#26463;&#26102;&#30340;&#22823;&#26799;&#24230;&#19982;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#30340;&#28431;&#27934;&#20043;&#38388;&#23384;&#22312;&#20851;&#32852;&#12290;</title><link>https://arxiv.org/abs/2402.06674</link><description>&lt;p&gt;
&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#30340;&#23454;&#38469;&#25104;&#21592;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Understanding Practical Membership Privacy of Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06674
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#26041;&#27861;&#31995;&#32479;&#22320;&#27979;&#35797;&#20102;&#32454;&#35843;&#22823;&#22411;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#23454;&#38469;&#38544;&#31169;&#28431;&#27934;&#65292;&#24182;&#21457;&#29616;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#31867;&#21035;&#30340;&#31034;&#20363;&#25968;&#37327;&#20197;&#21450;&#35757;&#32451;&#32467;&#26463;&#26102;&#30340;&#22823;&#26799;&#24230;&#19982;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#30340;&#28431;&#27934;&#20043;&#38388;&#23384;&#22312;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24212;&#29992;&#26368;&#20808;&#36827;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIA&#65289;&#26469;&#31995;&#32479;&#22320;&#27979;&#35797;&#32454;&#35843;&#22823;&#22411;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#23454;&#38469;&#38544;&#31169;&#28431;&#27934;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#29702;&#35299;&#20351;&#25968;&#25454;&#38598;&#21644;&#26679;&#26412;&#23481;&#26131;&#21463;&#21040;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#30340;&#29305;&#24615;&#12290;&#22312;&#25968;&#25454;&#38598;&#29305;&#24615;&#26041;&#38754;&#65292;&#25105;&#20204;&#21457;&#29616;&#25968;&#25454;&#20013;&#27599;&#20010;&#31867;&#21035;&#30340;&#31034;&#20363;&#25968;&#37327;&#19982;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#30340;&#28431;&#27934;&#20043;&#38388;&#23384;&#22312;&#24378;&#28872;&#30340;&#24130;&#24459;&#20381;&#36182;&#20851;&#31995;&#65292;&#36825;&#26159;&#20197;&#25915;&#20987;&#30340;&#30495;&#38451;&#24615;&#29575;&#65288;&#22312;&#20302;&#20551;&#38451;&#24615;&#29575;&#19979;&#27979;&#37327;&#65289;&#26469;&#34913;&#37327;&#30340;&#12290;&#23545;&#20110;&#20010;&#21035;&#26679;&#26412;&#32780;&#35328;&#65292;&#22312;&#35757;&#32451;&#32467;&#26463;&#26102;&#20135;&#29983;&#30340;&#22823;&#26799;&#24230;&#19982;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#30340;&#28431;&#27934;&#20043;&#38388;&#23384;&#22312;&#24456;&#24378;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We apply a state-of-the-art membership inference attack (MIA) to systematically test the practical privacy vulnerability of fine-tuning large image classification models.We focus on understanding the properties of data sets and samples that make them vulnerable to membership inference. In terms of data set properties, we find a strong power law dependence between the number of examples per class in the data and the MIA vulnerability, as measured by true positive rate of the attack at a low false positive rate. For an individual sample, large gradients at the end of training are strongly correlated with MIA vulnerability.
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#30001;&#28145;&#24230;&#23398;&#20064;&#32534;&#35793;&#22120;&#32534;&#35793;&#30340;DNN&#21487;&#25191;&#34892;&#25991;&#20214;&#30340;&#21333;&#20301;&#32763;&#36716;&#25915;&#20987;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#35774;&#35745;&#20102;&#33258;&#21160;&#25628;&#32034;&#24037;&#20855;&#20197;&#35782;&#21035;&#26131;&#21463;&#25915;&#20987;&#30340;&#20301;&#65292;&#24182;&#30830;&#23450;&#20102;&#23454;&#38469;&#25915;&#20987;&#21521;&#37327;&#65292;&#25581;&#31034;&#20102;DNN&#21487;&#25191;&#34892;&#25991;&#20214;&#30340;&#25915;&#20987;&#38754;&#12290;</title><link>http://arxiv.org/abs/2309.06223</link><description>&lt;p&gt;
&#25581;&#31034;&#23545;DNN&#21487;&#25191;&#34892;&#25991;&#20214;&#30340;&#21333;&#20301;&#32763;&#36716;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Unveiling Signle-Bit-Flip Attacks on DNN Executables. (arXiv:2309.06223v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06223
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#30001;&#28145;&#24230;&#23398;&#20064;&#32534;&#35793;&#22120;&#32534;&#35793;&#30340;DNN&#21487;&#25191;&#34892;&#25991;&#20214;&#30340;&#21333;&#20301;&#32763;&#36716;&#25915;&#20987;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#35774;&#35745;&#20102;&#33258;&#21160;&#25628;&#32034;&#24037;&#20855;&#20197;&#35782;&#21035;&#26131;&#21463;&#25915;&#20987;&#30340;&#20301;&#65292;&#24182;&#30830;&#23450;&#20102;&#23454;&#38469;&#25915;&#20987;&#21521;&#37327;&#65292;&#25581;&#31034;&#20102;DNN&#21487;&#25191;&#34892;&#25991;&#20214;&#30340;&#25915;&#20987;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20301;&#32763;&#36716;&#25915;&#20987;(BFA)&#21487;&#20197;&#36890;&#36807;DRAM Rowhammer&#21033;&#29992;&#26469;&#25805;&#32437;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#12290;&#29616;&#26377;&#30340;&#25915;&#20987;&#20027;&#35201;&#38024;&#23545;&#39640;&#32423;DNN&#26694;&#26550;&#65288;&#22914;PyTorch&#65289;&#20013;&#30340;&#27169;&#22411;&#26435;&#37325;&#25991;&#20214;&#36827;&#34892;&#20301;&#32763;&#36716;&#12290;&#28982;&#32780;&#65292;DNN&#32463;&#24120;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#32534;&#35793;&#22120;&#32534;&#35793;&#25104;&#20302;&#32423;&#21487;&#25191;&#34892;&#25991;&#20214;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#20302;&#32423;&#30828;&#20214;&#21407;&#35821;&#12290;&#32534;&#35793;&#21518;&#30340;&#20195;&#30721;&#36890;&#24120;&#36895;&#24230;&#24456;&#24555;&#65292;&#24182;&#19988;&#19982;&#39640;&#32423;DNN&#26694;&#26550;&#20855;&#26377;&#26126;&#26174;&#19981;&#21516;&#30340;&#25191;&#34892;&#33539;&#24335;&#12290;&#26412;&#25991;&#38024;&#23545;&#30001;DL&#32534;&#35793;&#22120;&#32534;&#35793;&#30340;DNN&#21487;&#25191;&#34892;&#25991;&#20214;&#30340;BFA&#25915;&#20987;&#38754;&#36827;&#34892;&#20102;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#21160;&#25628;&#32034;&#24037;&#20855;&#65292;&#29992;&#20110;&#35782;&#21035;DNN&#21487;&#25191;&#34892;&#25991;&#20214;&#20013;&#30340;&#26131;&#21463;&#25915;&#20987;&#20301;&#65292;&#24182;&#30830;&#23450;&#21033;&#29992;BFAs&#25915;&#20987;DNN&#21487;&#25191;&#34892;&#25991;&#20214;&#20013;&#30340;&#27169;&#22411;&#32467;&#26500;&#30340;&#23454;&#38469;&#25915;&#20987;&#21521;&#37327;&#65288;&#32780;&#20197;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#23545;&#25915;&#20987;&#27169;&#22411;&#26435;&#37325;&#20570;&#20986;&#20102;&#24378;&#20551;&#35774;&#65289;&#12290;DNN&#21487;&#25191;&#34892;&#25991;&#20214;&#20284;&#20046;&#27604;&#39640;&#32423;DNN&#20013;&#30340;&#27169;&#22411;&#26356;&#21152;&#8220;&#19981;&#36879;&#26126;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has shown that bit-flip attacks (BFAs) can manipulate deep neural networks (DNNs) via DRAM Rowhammer exploitations. Existing attacks are primarily launched over high-level DNN frameworks like PyTorch and flip bits in model weight files. Nevertheless, DNNs are frequently compiled into low-level executables by deep learning (DL) compilers to fully leverage low-level hardware primitives. The compiled code is usually high-speed and manifests dramatically distinct execution paradigms from high-level DNN frameworks.  In this paper, we launch the first systematic study on the attack surface of BFA specifically for DNN executables compiled by DL compilers. We design an automated search tool to identify vulnerable bits in DNN executables and identify practical attack vectors that exploit the model structure in DNN executables with BFAs (whereas prior works make likely strong assumptions to attack model weights). DNN executables appear more "opaque" than models in high-level DNN 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20195;&#30721;&#25551;&#36848;&#20013;&#24341;&#20837;&#25200;&#21160;&#26469;&#22686;&#24378;AI&#25915;&#20987;&#24615;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#35777;&#26126;&#25968;&#25454;&#22686;&#24378;&#21487;&#26377;&#25928;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#22120;&#23545;&#25200;&#21160;&#21644;&#38750;&#25200;&#21160;&#30340;&#20195;&#30721;&#25551;&#36848;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05079</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#25552;&#21319;AI&#25915;&#20987;&#24615;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Robustness of AI Offensive Code Generators via Data Augmentation. (arXiv:2306.05079v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20195;&#30721;&#25551;&#36848;&#20013;&#24341;&#20837;&#25200;&#21160;&#26469;&#22686;&#24378;AI&#25915;&#20987;&#24615;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#35777;&#26126;&#25968;&#25454;&#22686;&#24378;&#21487;&#26377;&#25928;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#22120;&#23545;&#25200;&#21160;&#21644;&#38750;&#25200;&#21160;&#30340;&#20195;&#30721;&#25551;&#36848;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25200;&#21160;&#28155;&#21152;&#21040;&#23433;&#20840;&#24615;&#20195;&#30721;&#19978;&#19979;&#25991;&#20013;&#30340;&#20195;&#30721;&#25551;&#36848;&#20013;&#30340;&#26041;&#27861;&#65292;&#21363;&#26469;&#33258;&#21892;&#24847;&#24320;&#21457;&#32773;&#30340;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#65288;NL&#65289;&#65292;&#24182;&#20998;&#26512;&#20102;&#25200;&#21160;&#22914;&#20309;&#20197;&#21450;&#22312;&#20160;&#20040;&#31243;&#24230;&#19978;&#24433;&#21709;AI&#25915;&#20987;&#24615;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;NL&#25551;&#36848;&#20013;&#30340;&#25200;&#21160;&#39640;&#24230;&#24433;&#21709;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#22686;&#24378;&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#35813;&#26041;&#27861;&#25191;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#21363;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#21464;&#24322;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#24182;&#35777;&#26126;&#20854;&#23545;&#25200;&#21160;&#21644;&#38750;&#25200;&#21160;&#30340;&#20195;&#30721;&#25551;&#36848;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present a method to add perturbations to the code descriptions, i.e., new inputs in natural language (NL) from well-intentioned developers, in the context of security-oriented code, and analyze how and to what extent perturbations affect the performance of AI offensive code generators. Our experiments show that the performance of the code generators is highly affected by perturbations in the NL descriptions. To enhance the robustness of the code generators, we use the method to perform data augmentation, i.e., to increase the variability and diversity of the training data, proving its effectiveness against both perturbed and non-perturbed code descriptions.
&lt;/p&gt;</description></item></channel></rss>