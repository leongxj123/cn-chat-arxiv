<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#35780;&#20272;LLM&#38598;&#25104;&#31995;&#32479;&#20445;&#23494;&#24615;&#30340;&#31995;&#32479;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24418;&#24335;&#21270;&#19968;&#20010;"&#31192;&#23494;&#23494;&#38053;"&#28216;&#25103;&#26469;&#25429;&#25417;&#27169;&#22411;&#38544;&#34255;&#31169;&#20154;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#35780;&#20272;&#20102;&#20843;&#31181;&#25915;&#20987;&#21644;&#22235;&#31181;&#38450;&#24481;&#26041;&#27861;&#65292;&#21457;&#29616;&#24403;&#21069;&#30340;&#38450;&#24481;&#26041;&#27861;&#32570;&#20047;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.06922</link><description>&lt;p&gt;
&#26426;&#22120;&#20013;&#30340;&#31169;&#35821;&#65306;LLM&#38598;&#25104;&#31995;&#32479;&#20013;&#30340;&#20445;&#23494;&#24615;
&lt;/p&gt;
&lt;p&gt;
Whispers in the Machine: Confidentiality in LLM-integrated Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#35780;&#20272;LLM&#38598;&#25104;&#31995;&#32479;&#20445;&#23494;&#24615;&#30340;&#31995;&#32479;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24418;&#24335;&#21270;&#19968;&#20010;"&#31192;&#23494;&#23494;&#38053;"&#28216;&#25103;&#26469;&#25429;&#25417;&#27169;&#22411;&#38544;&#34255;&#31169;&#20154;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#35780;&#20272;&#20102;&#20843;&#31181;&#25915;&#20987;&#21644;&#22235;&#31181;&#38450;&#24481;&#26041;&#27861;&#65292;&#21457;&#29616;&#24403;&#21069;&#30340;&#38450;&#24481;&#26041;&#27861;&#32570;&#20047;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#19982;&#22806;&#37096;&#24037;&#20855;&#38598;&#25104;&#12290;&#23613;&#31649;&#36825;&#20123;&#38598;&#25104;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;LLM&#30340;&#21151;&#33021;&#65292;&#20294;&#23427;&#20204;&#20063;&#22312;&#19981;&#21516;&#32452;&#20214;&#20043;&#38388;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25915;&#20987;&#38754;&#65292;&#21487;&#33021;&#27844;&#38706;&#26426;&#23494;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24694;&#24847;&#24037;&#20855;&#21487;&#20197;&#21033;&#29992;LLM&#26412;&#36523;&#30340;&#28431;&#27934;&#26469;&#25805;&#32437;&#27169;&#22411;&#24182;&#25439;&#23475;&#20854;&#20182;&#26381;&#21153;&#30340;&#25968;&#25454;&#65292;&#36825;&#24341;&#21457;&#20102;&#22312;LLM&#38598;&#25104;&#29615;&#22659;&#20013;&#22914;&#20309;&#20445;&#25252;&#31169;&#23494;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#35780;&#20272;LLM&#38598;&#25104;&#31995;&#32479;&#20445;&#23494;&#24615;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24418;&#24335;&#21270;&#20102;&#19968;&#20010;"&#31192;&#23494;&#23494;&#38053;"&#28216;&#25103;&#65292;&#21487;&#20197;&#25429;&#25417;&#27169;&#22411;&#38544;&#34255;&#31169;&#20154;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#27604;&#36739;&#27169;&#22411;&#23545;&#20445;&#23494;&#24615;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#20197;&#21450;&#19981;&#21516;&#38450;&#24481;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20843;&#31181;&#20808;&#21069;&#21457;&#34920;&#30340;&#25915;&#20987;&#21644;&#22235;&#31181;&#38450;&#24481;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#30340;&#38450;&#24481;&#26041;&#27861;&#32570;&#20047;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are increasingly integrated with external tools. While these integrations can significantly improve the functionality of LLMs, they also create a new attack surface where confidential data may be disclosed between different components. Specifically, malicious tools can exploit vulnerabilities in the LLM itself to manipulate the model and compromise the data of other services, raising the question of how private data can be protected in the context of LLM integrations.   In this work, we provide a systematic way of evaluating confidentiality in LLM-integrated systems. For this, we formalize a "secret key" game that can capture the ability of a model to conceal private information. This enables us to compare the vulnerability of a model against confidentiality attacks and also the effectiveness of different defense strategies. In this framework, we evaluate eight previously published attacks and four defenses. We find that current defenses lack generalization
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;DeSparsify&#65292;&#38024;&#23545;&#20351;&#29992;Token&#31232;&#30095;&#21270;&#26426;&#21046;&#30340;&#35270;&#35273;Transformer&#65292;&#36890;&#36807;&#31934;&#24515;&#21046;&#20316;&#30340;&#23545;&#25239;&#26679;&#26412;&#27450;&#39575;&#31232;&#30095;&#21270;&#26426;&#21046;&#65292;&#23548;&#33268;&#26368;&#22351;&#24773;&#20917;&#30340;&#24615;&#33021;&#65292;&#20197;&#27492;&#32791;&#23613;&#25805;&#20316;&#31995;&#32479;&#30340;&#36164;&#28304;&#24182;&#20445;&#25345;&#38544;&#34109;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02554</link><description>&lt;p&gt;
DeSparsify&#65306;&#23545;&#35270;&#35273;Transformer&#20013;&#30340;Token&#31232;&#30095;&#21270;&#26426;&#21046;&#36827;&#34892;&#30340;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
DeSparsify: Adversarial Attack Against Token Sparsification Mechanisms in Vision Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;DeSparsify&#65292;&#38024;&#23545;&#20351;&#29992;Token&#31232;&#30095;&#21270;&#26426;&#21046;&#30340;&#35270;&#35273;Transformer&#65292;&#36890;&#36807;&#31934;&#24515;&#21046;&#20316;&#30340;&#23545;&#25239;&#26679;&#26412;&#27450;&#39575;&#31232;&#30095;&#21270;&#26426;&#21046;&#65292;&#23548;&#33268;&#26368;&#22351;&#24773;&#20917;&#30340;&#24615;&#33021;&#65292;&#20197;&#27492;&#32791;&#23613;&#25805;&#20316;&#31995;&#32479;&#30340;&#36164;&#28304;&#24182;&#20445;&#25345;&#38544;&#34109;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20570;&#20986;&#20102;&#24040;&#22823;&#36129;&#29486;&#65292;&#23637;&#29616;&#20986;&#22312;&#21508;&#31181;&#20219;&#21153;&#65288;&#22914;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#65289;&#20013;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#39640;&#35745;&#31639;&#35201;&#27714;&#38543;&#20351;&#29992;&#30340;Token&#25968;&#37327;&#21576;&#20108;&#27425;&#22686;&#38271;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;Token&#31232;&#30095;&#21270;&#25216;&#26415;&#12290;&#36825;&#20123;&#25216;&#26415;&#37319;&#29992;&#20102;&#19968;&#31181;&#20381;&#36182;&#36755;&#20837;&#30340;&#31574;&#30053;&#65292;&#23558;&#26080;&#20851;&#30340;Token&#20174;&#35745;&#31639;&#27969;&#31243;&#20013;&#20002;&#24323;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#21160;&#24577;&#24615;&#21644;&#24179;&#22343;&#24773;&#20917;&#20551;&#35774;&#20351;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#19968;&#31181;&#26032;&#30340;&#23041;&#32961; - &#32463;&#36807;&#31934;&#24515;&#21046;&#20316;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#33021;&#22815;&#27450;&#39575;&#31232;&#30095;&#21270;&#26426;&#21046;&#65292;&#23548;&#33268;&#26368;&#22351;&#24773;&#20917;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25915;&#20987;&#26041;&#27861;DeSparsify&#65292;&#38024;&#23545;&#20351;&#29992;Token&#31232;&#30095;&#21270;&#26426;&#21046;&#30340;&#35270;&#35273;Transformer&#30340;&#21487;&#29992;&#24615;&#12290;&#35813;&#25915;&#20987;&#26088;&#22312;&#32791;&#23613;&#25805;&#20316;&#31995;&#32479;&#30340;&#36164;&#28304;&#65292;&#21516;&#26102;&#20445;&#25345;&#38544;&#34109;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision transformers have contributed greatly to advancements in the computer vision domain, demonstrating state-of-the-art performance in diverse tasks (e.g., image classification, object detection). However, their high computational requirements grow quadratically with the number of tokens used. Token sparsification techniques have been proposed to address this issue. These techniques employ an input-dependent strategy, in which uninformative tokens are discarded from the computation pipeline, improving the model's efficiency. However, their dynamism and average-case assumption makes them vulnerable to a new threat vector - carefully crafted adversarial examples capable of fooling the sparsification mechanism, resulting in worst-case performance. In this paper, we present DeSparsify, an attack targeting the availability of vision transformers that use token sparsification mechanisms. The attack aims to exhaust the operating system's resources, while maintaining its stealthiness. Our e
&lt;/p&gt;</description></item><item><title>DISTINQT&#26159;&#19968;&#31181;&#38754;&#21521;&#26410;&#26469;&#31227;&#21160;&#21644;&#26080;&#32447;&#32593;&#32476;&#30340;&#38544;&#31169;&#24863;&#30693;&#20998;&#24067;&#24335;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;QoS&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.10158</link><description>&lt;p&gt;
DISTINQT: &#19968;&#31181;&#38754;&#21521;&#26410;&#26469;&#31227;&#21160;&#21644;&#26080;&#32447;&#32593;&#32476;&#30340;&#20998;&#24067;&#24335;&#38544;&#31169;&#24863;&#30693;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;QoS&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
DISTINQT: A Distributed Privacy Aware Learning Framework for QoS Prediction for Future Mobile and Wireless Networks. (arXiv:2401.10158v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10158
&lt;/p&gt;
&lt;p&gt;
DISTINQT&#26159;&#19968;&#31181;&#38754;&#21521;&#26410;&#26469;&#31227;&#21160;&#21644;&#26080;&#32447;&#32593;&#32476;&#30340;&#38544;&#31169;&#24863;&#30693;&#20998;&#24067;&#24335;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;QoS&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
5G&#21644;6G&#20197;&#21518;&#30340;&#32593;&#32476;&#23558;&#25903;&#25345;&#20381;&#36182;&#19968;&#23450;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#30340;&#26032;&#30340;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29992;&#20363;&#21644;&#24212;&#29992;&#31243;&#24207;&#12290;&#21450;&#26102;&#39044;&#27979;QoS&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#65288;&#22914;&#36710;&#36742;&#36890;&#20449;&#65289;&#23588;&#20026;&#37325;&#35201;&#12290;&#23613;&#31649;&#30452;&#21040;&#26368;&#36817;&#65292;QoS&#39044;&#27979;&#19968;&#30452;&#30001;&#38598;&#20013;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#35299;&#20915;&#26041;&#26696;&#23436;&#25104;&#65292;&#20294;&#24050;&#32463;&#20986;&#29616;&#20102;&#19968;&#20123;&#38544;&#31169;&#12289;&#35745;&#31639;&#21644;&#36816;&#33829;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;&#26367;&#20195;&#26041;&#26696;&#24050;&#32463;&#20986;&#29616;&#65288;&#22914;&#20998;&#21106;&#23398;&#20064;&#12289;&#32852;&#37030;&#23398;&#20064;&#65289;&#65292;&#23558;&#22797;&#26434;&#24230;&#36739;&#20302;&#30340;AI&#20219;&#21153;&#20998;&#24067;&#22312;&#33410;&#28857;&#20043;&#38388;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#26410;&#26469;&#26080;&#32447;&#32593;&#32476;&#30340;&#24322;&#26500;&#24615;&#65292;&#24403;&#28041;&#21450;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#26102;&#65292;&#20250;&#20986;&#29616;&#26032;&#30340;&#25361;&#25112;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DISTINQT&#30340;&#38754;&#21521;QoS&#39044;&#27979;&#30340;&#38544;&#31169;&#24863;&#30693;&#20998;&#24067;&#24335;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Beyond 5G and 6G networks are expected to support new and challenging use cases and applications that depend on a certain level of Quality of Service (QoS) to operate smoothly. Predicting the QoS in a timely manner is of high importance, especially for safety-critical applications as in the case of vehicular communications. Although until recent years the QoS prediction has been carried out by centralized Artificial Intelligence (AI) solutions, a number of privacy, computational, and operational concerns have emerged. Alternative solutions have been surfaced (e.g. Split Learning, Federated Learning), distributing AI tasks of reduced complexity across nodes, while preserving the privacy of the data. However, new challenges rise when it comes to scalable distributed learning approaches, taking into account the heterogeneous nature of future wireless networks. The current work proposes DISTINQT, a privacy-aware distributed learning framework for QoS prediction. Our framework supports mult
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#40657;&#30418;NLP&#20998;&#31867;&#22120;&#25915;&#20987;&#27169;&#22411;&#65292;&#36890;&#36807;&#22522;&#20110;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#35789;&#36873;&#25321;&#21644;&#36138;&#23146;&#25628;&#32034;&#31639;&#27861;&#36827;&#34892;&#35789;&#26367;&#25442;&#65292;&#35299;&#20915;&#20102;&#24433;&#21709;NLP&#39046;&#22495;&#20256;&#32479;&#22270;&#20687;&#25915;&#20987;&#26041;&#27861;&#19981;&#36866;&#29992;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2112.11660</link><description>&lt;p&gt;
&#19968;&#20010;&#40657;&#30418;NLP&#20998;&#31867;&#22120;&#25915;&#20987;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Black-box NLP Classifier Attacker. (arXiv:2112.11660v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.11660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#40657;&#30418;NLP&#20998;&#31867;&#22120;&#25915;&#20987;&#27169;&#22411;&#65292;&#36890;&#36807;&#22522;&#20110;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#35789;&#36873;&#25321;&#21644;&#36138;&#23146;&#25628;&#32034;&#31639;&#27861;&#36827;&#34892;&#35789;&#26367;&#25442;&#65292;&#35299;&#20915;&#20102;&#24433;&#21709;NLP&#39046;&#22495;&#20256;&#32479;&#22270;&#20687;&#25915;&#20987;&#26041;&#27861;&#19981;&#36866;&#29992;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35299;&#20915;&#21508;&#31181;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#24182;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#22270;&#20687;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#24050;&#32463;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#21508;&#31181;&#30740;&#31350;&#24050;&#32463;&#26174;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#33030;&#24369;&#24615;&#12290;&#20197;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20026;&#20363;&#65292;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#34987;&#19968;&#20010;&#19982;&#21407;&#22987;&#25991;&#26412;&#39640;&#24230;&#30456;&#20284;&#30340;&#12289;&#32463;&#36807;&#20180;&#32454;&#20462;&#25913;&#30340;&#25991;&#26412;&#25152;&#36855;&#24785;&#12290;&#26681;&#25454;&#20043;&#21069;&#30340;&#30740;&#31350;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#22270;&#20687;&#39046;&#22495;&#65307;&#19982;&#22270;&#20687;&#23545;&#25239;&#25915;&#20987;&#19981;&#21516;&#65292;&#25991;&#26412;&#20197;&#31163;&#25955;&#24207;&#21015;&#34920;&#31034;&#65292;&#20256;&#32479;&#30340;&#22270;&#20687;&#25915;&#20987;&#26041;&#27861;&#22312;NLP&#39046;&#22495;&#19981;&#36866;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#35789;&#32423;NLP&#24773;&#24863;&#20998;&#31867;&#22120;&#25915;&#20987;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#22522;&#20110;&#35789;&#36873;&#25321;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#36138;&#23146;&#25628;&#32034;&#31639;&#27861;&#36827;&#34892;&#35789;&#26367;&#25442;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;...
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have a wide range of applications in solving various real-world tasks and have achieved satisfactory results, in domains such as computer vision, image classification, and natural language processing. Meanwhile, the security and robustness of neural networks have become imperative, as diverse researches have shown the vulnerable aspects of neural networks. Case in point, in Natural language processing tasks, the neural network may be fooled by an attentively modified text, which has a high similarity to the original one. As per previous research, most of the studies are focused on the image domain; Different from image adversarial attacks, the text is represented in a discrete sequence, traditional image attack methods are not applicable in the NLP field. In this paper, we propose a word-level NLP sentiment classifier attack model, which includes a self-attention mechanism-based word selection method and a greedy search algorithm for word substitution. We experimen
&lt;/p&gt;</description></item></channel></rss>