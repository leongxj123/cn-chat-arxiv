<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#24046;&#20998;&#38544;&#31169;&#32447;&#24615;&#25506;&#27979;&#65288;LP&#65289;&#21644;&#23436;&#20840;&#24494;&#35843;&#65288;FT&#65289;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#25506;&#32034;&#20102;&#20174;&#32447;&#24615;&#25506;&#27979;&#36807;&#28193;&#21040;&#23436;&#20840;&#24494;&#35843;&#65288;LP-FT&#65289;&#30340;&#39034;&#24207;&#24494;&#35843;&#29616;&#35937;&#21450;&#20854;&#23545;&#27979;&#35797;&#25439;&#22833;&#30340;&#24433;&#21709;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#22312;&#36229;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#20013;&#24046;&#20998;&#38544;&#31169;&#24494;&#35843;&#25910;&#25947;&#24615;&#30340;&#29702;&#35770;&#27934;&#35265;&#21644;&#38544;&#31169;&#39044;&#31639;&#20998;&#37197;&#30340;&#25928;&#29992;&#26354;&#32447;&#12290;</title><link>https://arxiv.org/abs/2402.18905</link><description>&lt;p&gt;
&#35770;&#24046;&#20998;&#38544;&#31169;&#24494;&#35843;&#30340;&#25910;&#25947;&#24615;&#65306;&#24212;&#32447;&#24615;&#25506;&#27979;&#36824;&#26159;&#23436;&#20840;&#24494;&#35843;&#65311;
&lt;/p&gt;
&lt;p&gt;
On the Convergence of Differentially-Private Fine-tuning: To Linearly Probe or to Fully Fine-tune?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#24046;&#20998;&#38544;&#31169;&#32447;&#24615;&#25506;&#27979;&#65288;LP&#65289;&#21644;&#23436;&#20840;&#24494;&#35843;&#65288;FT&#65289;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#25506;&#32034;&#20102;&#20174;&#32447;&#24615;&#25506;&#27979;&#36807;&#28193;&#21040;&#23436;&#20840;&#24494;&#35843;&#65288;LP-FT&#65289;&#30340;&#39034;&#24207;&#24494;&#35843;&#29616;&#35937;&#21450;&#20854;&#23545;&#27979;&#35797;&#25439;&#22833;&#30340;&#24433;&#21709;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#22312;&#36229;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#20013;&#24046;&#20998;&#38544;&#31169;&#24494;&#35843;&#25910;&#25947;&#24615;&#30340;&#29702;&#35770;&#27934;&#35265;&#21644;&#38544;&#31169;&#39044;&#31639;&#20998;&#37197;&#30340;&#25928;&#29992;&#26354;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#26426;&#22120;&#23398;&#20064;&#27969;&#27700;&#32447;&#36890;&#24120;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#30340;&#36807;&#31243;&#65306;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#38750;&#31169;&#26377;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#20351;&#29992;DP&#20248;&#21270;&#25216;&#26415;&#22312;&#31169;&#26377;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;DP&#35774;&#32622;&#20013;&#65292;&#24050;&#32463;&#35266;&#23519;&#21040;&#23436;&#20840;&#24494;&#35843;&#26377;&#26102;&#20505;&#24182;&#19981;&#24635;&#26159;&#20135;&#29983;&#26368;&#20339;&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#65292;&#21363;&#20351;&#23545;&#20110;&#20998;&#24067;&#20869;&#25968;&#25454;&#20063;&#26159;&#22914;&#27492;&#12290;&#26412;&#25991;&#65288;1&#65289;&#20998;&#26512;&#20102;DP&#32447;&#24615;&#25506;&#27979;&#65288;LP&#65289;&#21644;&#23436;&#20840;&#24494;&#35843;&#65288;FT&#65289;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#20197;&#21450;&#65288;2&#65289;&#25506;&#32034;&#20102;&#39034;&#24207;&#24494;&#35843;&#30340;&#29616;&#35937;&#65292;&#20174;&#32447;&#24615;&#25506;&#27979;&#24320;&#22987;&#65292;&#36807;&#28193;&#21040;&#23436;&#20840;&#24494;&#35843;&#65288;LP-FT&#65289;&#65292;&#20197;&#21450;&#23427;&#23545;&#27979;&#35797;&#25439;&#22833;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#20851;DP&#24494;&#35843;&#22312;&#36229;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25910;&#25947;&#24615;&#30340;&#29702;&#35770;&#27934;&#35265;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#30830;&#23450;&#38544;&#31169;&#39044;&#31639;&#22312;&#32447;&#24615;&#25506;&#27979;&#21644;&#23436;&#20840;&#24494;&#35843;&#20043;&#38388;&#20998;&#37197;&#30340;&#25928;&#29992;&#26354;&#32447;&#12290;&#29702;&#35770;&#32467;&#26524;&#24471;&#21040;&#20102;&#23545;&#21508;&#31181;&#22522;&#20934;&#21644;&#27169;&#22411;&#30340;&#32463;&#39564;&#35780;&#20272;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18905v1 Announce Type: cross  Abstract: Differentially private (DP) machine learning pipelines typically involve a two-phase process: non-private pre-training on a public dataset, followed by fine-tuning on private data using DP optimization techniques. In the DP setting, it has been observed that full fine-tuning may not always yield the best test accuracy, even for in-distribution data. This paper (1) analyzes the training dynamics of DP linear probing (LP) and full fine-tuning (FT), and (2) explores the phenomenon of sequential fine-tuning, starting with linear probing and transitioning to full fine-tuning (LP-FT), and its impact on test loss. We provide theoretical insights into the convergence of DP fine-tuning within an overparameterized neural network and establish a utility curve that determines the allocation of privacy budget between linear probing and full fine-tuning. The theoretical results are supported by empirical evaluations on various benchmarks and models.
&lt;/p&gt;</description></item></channel></rss>