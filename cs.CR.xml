<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;&#38543;&#26426;&#26799;&#24230; Langevin &#21453;&#36951;&#24536;&#26041;&#27861;&#65292;&#20026;&#36817;&#20284;&#21453;&#36951;&#24536;&#38382;&#39064;&#25552;&#20379;&#20102;&#38544;&#31169;&#20445;&#38556;&#65292;&#24182;&#23637;&#31034;&#20102;&#23567;&#25209;&#27425;&#26799;&#24230;&#26356;&#26032;&#30456;&#36739;&#20110;&#20840;&#25209;&#27425;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.17105</link><description>&lt;p&gt;
&#38543;&#26426;&#26799;&#24230; Langevin &#21453;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Stochastic Gradient Langevin Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;&#38543;&#26426;&#26799;&#24230; Langevin &#21453;&#36951;&#24536;&#26041;&#27861;&#65292;&#20026;&#36817;&#20284;&#21453;&#36951;&#24536;&#38382;&#39064;&#25552;&#20379;&#20102;&#38544;&#31169;&#20445;&#38556;&#65292;&#24182;&#23637;&#31034;&#20102;&#23567;&#25209;&#27425;&#26799;&#24230;&#26356;&#26032;&#30456;&#36739;&#20110;&#20840;&#25209;&#27425;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#34987;&#36951;&#24536;&#30340;&#26435;&#21033;&#8221;&#26159;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#30340;&#27861;&#24459;&#25152;&#30830;&#20445;&#30340;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26426;&#22120;&#21453;&#36951;&#24536;&#26088;&#22312;&#39640;&#25928;&#22320;&#28040;&#38500;&#24050;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#19978;&#26576;&#20123;&#25968;&#25454;&#28857;&#30340;&#24433;&#21709;&#65292;&#20351;&#20854;&#36817;&#20284;&#20110;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38543;&#26426;&#26799;&#24230; Langevin &#21453;&#36951;&#24536;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#24102;&#26377;&#38544;&#31169;&#20445;&#38556;&#30340;&#22122;&#22768;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#21453;&#36951;&#24536;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#20984;&#24615;&#20551;&#35774;&#19979;&#30340;&#36817;&#20284;&#21453;&#36951;&#24536;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20840;&#25209;&#27425;&#23545;&#24212;&#26041;&#27861;&#30456;&#27604;&#65292;&#23567;&#25209;&#27425;&#26799;&#24230;&#26356;&#26032;&#22312;&#38544;&#31169;&#22797;&#26434;&#24230;&#26435;&#34913;&#26041;&#38754;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21453;&#36951;&#24536;&#26041;&#27861;&#20855;&#26377;&#35832;&#22810;&#31639;&#27861;&#20248;&#21183;&#65292;&#21253;&#25324;&#19982;&#37325;&#26032;&#35757;&#32451;&#30456;&#27604;&#30340;&#22797;&#26434;&#24230;&#33410;&#30465;&#65292;&#20197;&#21450;&#25903;&#25345;&#39034;&#24207;&#21644;&#25209;&#37327;&#21453;&#36951;&#24536;&#12290;&#20026;&#20102;&#26816;&#39564;&#25105;&#20204;&#26041;&#27861;&#30340;&#38544;&#31169;-&#25928;&#29992;-&#22797;&#26434;&#24230;&#26435;&#34913;&#65292;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17105v1 Announce Type: new  Abstract: ``The right to be forgotten'' ensured by laws for user data privacy becomes increasingly important. Machine unlearning aims to efficiently remove the effect of certain data points on the trained model parameters so that it can be approximately the same as if one retrains the model from scratch. This work proposes stochastic gradient Langevin unlearning, the first unlearning framework based on noisy stochastic gradient descent (SGD) with privacy guarantees for approximate unlearning problems under convexity assumption. Our results show that mini-batch gradient updates provide a superior privacy-complexity trade-off compared to the full-batch counterpart. There are numerous algorithmic benefits of our unlearning approach, including complexity saving compared to retraining, and supporting sequential and batch unlearning. To examine the privacy-utility-complexity trade-off of our method, we conduct experiments on benchmark datasets compared 
&lt;/p&gt;</description></item><item><title>Shadowcast&#26159;&#19968;&#31181;&#38544;&#31192;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#20266;&#35013;&#25104;&#33391;&#24615;&#22270;&#20687;&#21644;&#21305;&#37197;&#25991;&#26412;&#26469;&#25805;&#32437;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#21709;&#24212;&#12290;&#23427;&#21253;&#25324;&#26631;&#31614;&#25915;&#20987;&#21644;&#35828;&#26381;&#25915;&#20987;&#65292;&#21487;&#20197;&#28151;&#28102;&#31867;&#21035;&#26631;&#31614;&#24182;&#32534;&#20889;&#26377;&#35828;&#26381;&#21147;&#30340;&#25551;&#36848;&#12290;&#20351;&#29992;&#20165;50&#20010;&#27602;&#26679;&#26412;&#65292;Shadowcast&#33021;&#22815;&#39640;&#25928;&#23454;&#29616;&#25915;&#20987;&#32773;&#30340;&#24847;&#22270;&#12290;</title><link>https://arxiv.org/abs/2402.06659</link><description>&lt;p&gt;
Shadowcast: &#38544;&#31192;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#23545;&#25239;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06659
&lt;/p&gt;
&lt;p&gt;
Shadowcast&#26159;&#19968;&#31181;&#38544;&#31192;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#20266;&#35013;&#25104;&#33391;&#24615;&#22270;&#20687;&#21644;&#21305;&#37197;&#25991;&#26412;&#26469;&#25805;&#32437;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#21709;&#24212;&#12290;&#23427;&#21253;&#25324;&#26631;&#31614;&#25915;&#20987;&#21644;&#35828;&#26381;&#25915;&#20987;&#65292;&#21487;&#20197;&#28151;&#28102;&#31867;&#21035;&#26631;&#31614;&#24182;&#32534;&#20889;&#26377;&#35828;&#26381;&#21147;&#30340;&#25551;&#36848;&#12290;&#20351;&#29992;&#20165;50&#20010;&#27602;&#26679;&#26412;&#65292;Shadowcast&#33021;&#22815;&#39640;&#25928;&#23454;&#29616;&#25915;&#20987;&#32773;&#30340;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#33021;&#22815;&#20174;&#35270;&#35273;&#36755;&#20837;&#20013;&#29983;&#25104;&#25991;&#26412;&#21709;&#24212;&#65292;&#28982;&#32780;&#23427;&#20204;&#30340;&#22810;&#21151;&#33021;&#24615;&#24102;&#26469;&#20102;&#37325;&#22823;&#30340;&#23433;&#20840;&#38544;&#24739;&#12290;&#26412;&#30740;&#31350;&#39318;&#27425;&#25581;&#31034;&#20102;VLM&#23545;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#26131;&#21463;&#24615;&#65292;&#36825;&#20123;&#25915;&#20987;&#21487;&#20197;&#25805;&#32437;&#23545;&#26080;&#23475;&#30340;&#26085;&#24120;&#25552;&#31034;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Shadowcast&#30340;&#38544;&#31192;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#26041;&#27861;&#65292;&#20854;&#20013;&#27602;&#26679;&#26412;&#22312;&#35270;&#35273;&#19978;&#19982;&#20855;&#26377;&#21305;&#37197;&#25991;&#26412;&#30340;&#33391;&#24615;&#22270;&#20687;&#38590;&#20197;&#21306;&#20998;&#12290;Shadowcast&#22312;&#20004;&#31181;&#25915;&#20987;&#31867;&#22411;&#20013;&#23637;&#31034;&#20986;&#20102;&#26377;&#25928;&#24615;&#12290;&#31532;&#19968;&#31181;&#26159;&#26631;&#31614;&#25915;&#20987;&#65292;&#20351;VLM&#35823;&#35782;&#21035;&#31867;&#21035;&#26631;&#31614;&#65292;&#20363;&#22914;&#28151;&#28102;&#21776;&#32435;&#24503;&#183;&#29305;&#26391;&#26222;&#21644;&#20052;&#183;&#25308;&#30331;&#31561;&#20154;&#12290;&#31532;&#20108;&#31181;&#26159;&#35828;&#26381;&#25915;&#20987;&#65292;&#21033;&#29992;VLM&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#26469;&#32534;&#20889;&#25925;&#20107;&#65292;&#20363;&#22914;&#36890;&#36807;&#26377;&#35828;&#26381;&#21147;&#21644;&#30475;&#20284;&#21512;&#29702;&#30340;&#25551;&#36848;&#23558;&#22403;&#22334;&#39135;&#21697;&#25551;&#32472;&#25104;&#20581;&#24247;&#39135;&#21697;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Shadowcast&#20351;&#29992;&#20165;50&#20010;&#27602;&#26679;&#26412;&#23601;&#33021;&#39640;&#24230;&#26377;&#25928;&#22320;&#23454;&#29616;&#25915;&#20987;&#32773;&#30340;&#24847;&#22270;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27602;&#26679;&#26412;&#20173;&#28982;&#20445;&#25345;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Models (VLMs) excel in generating textual responses from visual inputs, yet their versatility raises significant security concerns. This study takes the first step in exposing VLMs' susceptibility to data poisoning attacks that can manipulate responses to innocuous, everyday prompts. We introduce Shadowcast, a stealthy data poisoning attack method where poison samples are visually indistinguishable from benign images with matching texts. Shadowcast demonstrates effectiveness in two attack types. The first is Label Attack, tricking VLMs into misidentifying class labels, such as confusing Donald Trump for Joe Biden. The second is Persuasion Attack, which leverages VLMs' text generation capabilities to craft narratives, such as portraying junk food as health food, through persuasive and seemingly rational descriptions. We show that Shadowcast are highly effective in achieving attacker's intentions using as few as 50 poison samples. Moreover, these poison samples remain eff
&lt;/p&gt;</description></item><item><title>Flamingo&#26159;&#19968;&#20010;&#29992;&#20110;&#23454;&#29616;&#36328;&#22823;&#37327;&#23458;&#25143;&#31471;&#23433;&#20840;&#32858;&#21512;&#30340;&#31995;&#32479;&#65292;&#22312;&#31169;&#26377;&#32852;&#37030;&#23398;&#20064;&#20013;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#28040;&#38500;&#27599;&#36718;&#35774;&#32622;&#21644;&#24341;&#20837;&#36731;&#37327;&#32423;&#30340;&#20002;&#22833;&#23481;&#24525;&#21327;&#35758;&#65292;Flamingo&#35299;&#20915;&#20102;&#20197;&#24448;&#21327;&#35758;&#22312;&#22810;&#36718;&#35774;&#32622;&#19979;&#30340;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#26412;&#22320;&#36873;&#25321;&#23458;&#25143;&#31471;&#37051;&#22495;&#30340;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2308.09883</link><description>&lt;p&gt;
Flamingo: &#22810;&#36718;&#21333;&#26381;&#21153;&#22120;&#23433;&#20840;&#32858;&#21512;&#21450;&#20854;&#22312;&#31169;&#26377;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Flamingo: Multi-Round Single-Server Secure Aggregation with Applications to Private Federated Learning. (arXiv:2308.09883v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09883
&lt;/p&gt;
&lt;p&gt;
Flamingo&#26159;&#19968;&#20010;&#29992;&#20110;&#23454;&#29616;&#36328;&#22823;&#37327;&#23458;&#25143;&#31471;&#23433;&#20840;&#32858;&#21512;&#30340;&#31995;&#32479;&#65292;&#22312;&#31169;&#26377;&#32852;&#37030;&#23398;&#20064;&#20013;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#28040;&#38500;&#27599;&#36718;&#35774;&#32622;&#21644;&#24341;&#20837;&#36731;&#37327;&#32423;&#30340;&#20002;&#22833;&#23481;&#24525;&#21327;&#35758;&#65292;Flamingo&#35299;&#20915;&#20102;&#20197;&#24448;&#21327;&#35758;&#22312;&#22810;&#36718;&#35774;&#32622;&#19979;&#30340;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#26412;&#22320;&#36873;&#25321;&#23458;&#25143;&#31471;&#37051;&#22495;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Flamingo&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#36328;&#22823;&#37327;&#23458;&#25143;&#31471;&#23433;&#20840;&#32858;&#21512;&#25968;&#25454;&#30340;&#31995;&#32479;&#12290;&#22312;&#23433;&#20840;&#32858;&#21512;&#20013;&#65292;&#26381;&#21153;&#22120;&#23545;&#23458;&#25143;&#31471;&#30340;&#31169;&#26377;&#36755;&#20837;&#36827;&#34892;&#27714;&#21644;&#65292;&#24182;&#22312;&#19981;&#20102;&#35299;&#20010;&#20307;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#24471;&#21040;&#32467;&#26524;&#65292;&#20165;&#33021;&#25512;&#26029;&#20986;&#26368;&#32456;&#24635;&#21644;&#12290;Flamingo&#19987;&#27880;&#20110;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#22810;&#36718;&#35774;&#32622;&#65292;&#20854;&#20013;&#25191;&#34892;&#22810;&#20010;&#36830;&#32493;&#30340;&#27169;&#22411;&#26435;&#37325;&#27714;&#21644;&#65288;&#24179;&#22343;&#65289;&#65292;&#20197;&#24471;&#21040;&#19968;&#20010;&#33391;&#22909;&#30340;&#27169;&#22411;&#12290;&#20043;&#21069;&#30340;&#21327;&#35758;&#65288;&#20363;&#22914;Bell&#31561;&#20154;&#30340;CCS '20&#65289;&#20165;&#36866;&#29992;&#20110;&#21333;&#36718;&#65292;&#24182;&#36890;&#36807;&#22810;&#27425;&#37325;&#22797;&#35813;&#21327;&#35758;&#26469;&#36866;&#24212;&#32852;&#37030;&#23398;&#20064;&#30340;&#35774;&#32622;&#12290;Flamingo&#28040;&#38500;&#20102;&#20043;&#21069;&#21327;&#35758;&#27599;&#36718;&#35774;&#32622;&#30340;&#38656;&#27714;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#36731;&#37327;&#32423;&#30340;&#20002;&#22833;&#23481;&#24525;&#21327;&#35758;&#65292;&#20197;&#30830;&#20445;&#22914;&#26524;&#23458;&#25143;&#31471;&#22312;&#27714;&#21644;&#36807;&#31243;&#20013;&#31163;&#24320;&#65292;&#26381;&#21153;&#22120;&#20173;&#28982;&#21487;&#20197;&#33719;&#24471;&#26377;&#24847;&#20041;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;Flamingo&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26412;&#22320;&#36873;&#25321;&#25152;&#35859;&#30340;&#23458;&#25143;&#31471;&#37051;&#22495;&#30340;&#26041;&#24335;&#65292;&#27492;&#27010;&#24565;&#30001;Bell&#31561;&#20154;&#25552;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Flamingo, a system for secure aggregation of data across a large set of clients. In secure aggregation, a server sums up the private inputs of clients and obtains the result without learning anything about the individual inputs beyond what is implied by the final sum. Flamingo focuses on the multi-round setting found in federated learning in which many consecutive summations (averages) of model weights are performed to derive a good model. Previous protocols, such as Bell et al. (CCS '20), have been designed for a single round and are adapted to the federated learning setting by repeating the protocol multiple times. Flamingo eliminates the need for the per-round setup of previous protocols, and has a new lightweight dropout resilience protocol to ensure that if clients leave in the middle of a sum the server can still obtain a meaningful result. Furthermore, Flamingo introduces a new way to locally choose the so-called client neighborhood introduced by Bell et al
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20998;&#21306;&#25193;&#25955;&#27169;&#22411;&#65288;CDM&#65289;&#35757;&#32451;&#19981;&#21516;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#22312;&#25512;&#26029;&#26102;&#20219;&#24847;&#32452;&#21512;&#23427;&#20204;&#65292;&#23454;&#29616;&#20102;&#35757;&#32451;&#25968;&#25454;&#20445;&#25252;&#21644;&#36873;&#25321;&#24615;&#36951;&#24536;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#35775;&#38382;&#26435;&#38480;&#25552;&#20379;&#23450;&#21046;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.01937</link><description>&lt;p&gt;
&#20351;&#29992;&#32452;&#21512;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#35757;&#32451;&#25968;&#25454;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Training Data Protection with Compositional Diffusion Models. (arXiv:2308.01937v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01937
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20998;&#21306;&#25193;&#25955;&#27169;&#22411;&#65288;CDM&#65289;&#35757;&#32451;&#19981;&#21516;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#22312;&#25512;&#26029;&#26102;&#20219;&#24847;&#32452;&#21512;&#23427;&#20204;&#65292;&#23454;&#29616;&#20102;&#35757;&#32451;&#25968;&#25454;&#20445;&#25252;&#21644;&#36873;&#25321;&#24615;&#36951;&#24536;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#35775;&#38382;&#26435;&#38480;&#25552;&#20379;&#23450;&#21046;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#21306;&#25193;&#25955;&#27169;&#22411;&#65288;CDM&#65289;&#65292;&#19968;&#31181;&#22312;&#19981;&#21516;&#25968;&#25454;&#28304;&#19978;&#35757;&#32451;&#19981;&#21516;&#25193;&#25955;&#27169;&#22411;&#65288;&#25110;&#25552;&#31034;&#65289;&#24182;&#22312;&#25512;&#26029;&#26102;&#20219;&#24847;&#32452;&#21512;&#23427;&#20204;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#21333;&#29420;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#23396;&#31435;&#29366;&#24577;&#19979;&#12289;&#22312;&#19981;&#21516;&#26102;&#38388;&#12289;&#22312;&#19981;&#21516;&#20998;&#24067;&#21644;&#39046;&#22495;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#21487;&#20197;&#21518;&#32493;&#32452;&#21512;&#20197;&#36798;&#21040;&#19982;&#21516;&#26102;&#35757;&#32451;&#25152;&#26377;&#25968;&#25454;&#30340;&#29702;&#24819;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#27599;&#20010;&#27169;&#22411;&#21482;&#21253;&#21547;&#20854;&#22312;&#35757;&#32451;&#26399;&#38388;&#25509;&#35302;&#21040;&#30340;&#25968;&#25454;&#23376;&#38598;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#23454;&#29616;&#22810;&#31181;&#24418;&#24335;&#30340;&#35757;&#32451;&#25968;&#25454;&#20445;&#25252;&#12290;&#29305;&#21035;&#26159;&#65292;CDM&#26159;&#31532;&#19968;&#31181;&#21487;&#20197;&#23454;&#29616;&#22823;&#35268;&#27169;&#25193;&#25955;&#27169;&#22411;&#30340;&#36873;&#25321;&#24615;&#36951;&#24536;&#21644;&#25345;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#20801;&#35768;&#26681;&#25454;&#29992;&#25143;&#35775;&#38382;&#26435;&#38480;&#25552;&#20379;&#23450;&#21046;&#27169;&#22411;&#12290;CDM&#36824;&#21487;&#20197;&#30830;&#23450;&#29983;&#25104;&#29305;&#23450;&#26679;&#26412;&#30340;&#25968;&#25454;&#23376;&#38598;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Compartmentalized Diffusion Models (CDM), a method to train different diffusion models (or prompts) on distinct data sources and arbitrarily compose them at inference time. The individual models can be trained in isolation, at different times, and on different distributions and domains and can be later composed to achieve performance comparable to a paragon model trained on all data simultaneously. Furthermore, each model only contains information about the subset of the data it was exposed to during training, enabling several forms of training data protection. In particular, CDMs are the first method to enable both selective forgetting and continual learning for large-scale diffusion models, as well as allowing serving customized models based on the user's access rights. CDMs also allow determining the importance of a subset of the data in generating particular samples.
&lt;/p&gt;</description></item></channel></rss>