<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26435;&#37325;&#21098;&#35009;&#30340;&#24046;&#20998;&#38544;&#31169;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20844;&#20849;&#20449;&#24687;&#23545;&#20840;&#23616;&#27169;&#22411;&#36827;&#34892;&#25913;&#36827;&#65292;&#33719;&#24471;&#26356;&#31934;&#30830;&#30340;&#28789;&#25935;&#24230;&#30028;&#38480;&#21644;&#22122;&#22768;&#27700;&#24179;&#35843;&#25972;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.18001</link><description>&lt;p&gt;
&#24102;&#26435;&#37325;&#21098;&#35009;&#30340;&#24046;&#20998;&#38544;&#31169;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DP-SGD with weight clipping. (arXiv:2310.18001v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26435;&#37325;&#21098;&#35009;&#30340;&#24046;&#20998;&#38544;&#31169;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20844;&#20849;&#20449;&#24687;&#23545;&#20840;&#23616;&#27169;&#22411;&#36827;&#34892;&#25913;&#36827;&#65292;&#33719;&#24471;&#26356;&#31934;&#30830;&#30340;&#28789;&#25935;&#24230;&#30028;&#38480;&#21644;&#22122;&#22768;&#27700;&#24179;&#35843;&#25972;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#20854;&#20182;&#20381;&#36182;&#20110;&#30446;&#26631;&#20989;&#25968;&#20248;&#21270;&#30340;&#26041;&#27861;&#30340;&#39640;&#24230;&#27969;&#34892;&#65292;&#20197;&#21450;&#23545;&#25968;&#25454;&#38544;&#31169;&#30340;&#20851;&#27880;&#65292;&#24046;&#20998;&#38544;&#31169;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#20026;&#20102;&#22312;&#25552;&#20379;&#26368;&#23567;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#38480;&#21046;&#21442;&#19982;&#32773;&#23558;&#35266;&#23519;&#21040;&#30340;&#20449;&#24687;&#30340;&#28789;&#25935;&#24230;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#24357;&#34917;&#20102;&#20256;&#32479;&#26799;&#24230;&#21098;&#35009;&#20135;&#29983;&#30340;&#20559;&#24046;&#12290;&#36890;&#36807;&#21033;&#29992;&#20851;&#20110;&#24403;&#21069;&#20840;&#23616;&#27169;&#22411;&#21450;&#20854;&#22312;&#25628;&#32034;&#39046;&#22495;&#20013;&#20301;&#32622;&#30340;&#20844;&#20849;&#20449;&#24687;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#25913;&#36827;&#30340;&#26799;&#24230;&#30028;&#38480;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#28789;&#25935;&#24230;&#30830;&#23450;&#21644;&#22122;&#22768;&#27700;&#24179;&#35843;&#25972;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#65292;&#38656;&#35201;&#26356;&#23569;&#30340;&#22122;&#22768;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, due to the popularity of deep neural networks and other methods whose training typically relies on the optimization of an objective function, and due to concerns for data privacy, there is a lot of interest in differentially private gradient descent methods. To achieve differential privacy guarantees with a minimum amount of noise, it is important to be able to bound precisely the sensitivity of the information which the participants will observe. In this study, we present a novel approach that mitigates the bias arising from traditional gradient clipping. By leveraging public information concerning the current global model and its location within the search domain, we can achieve improved gradient bounds, leading to enhanced sensitivity determinations and refined noise level adjustments. We extend the state of the art algorithms, present improved differential privacy guarantees requiring less noise and present an empirical evaluation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27604;&#29305;&#24065;&#29992;&#25143;&#32593;&#32476;&#30340;&#20013;&#23567;&#23610;&#24230;&#32467;&#26500;&#29305;&#24615;&#65292;&#24182;&#21457;&#29616;&#20854;&#20855;&#26377;&#26680;&#24515;-&#22806;&#22260;&#32467;&#26500;&#21644;&#34676;&#34678;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;BUN&#32467;&#26500;&#32452;&#32455;&#30340;&#28436;&#21270;&#19982;&#27873;&#27819;&#23384;&#22312;&#30456;&#20851;&#30340;&#27874;&#21160;&#26377;&#20851;&#65292;&#36827;&#19968;&#27493;&#35777;&#23454;&#20102;&#32467;&#26500;&#37327;&#19982;&#20215;&#26684;&#21464;&#21160;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.14409</link><description>&lt;p&gt;
&#25506;&#32034;&#27604;&#29305;&#24065;&#30340;&#20013;&#23567;&#23610;&#24230;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Exploring the Bitcoin Mesoscale. (arXiv:2307.14409v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27604;&#29305;&#24065;&#29992;&#25143;&#32593;&#32476;&#30340;&#20013;&#23567;&#23610;&#24230;&#32467;&#26500;&#29305;&#24615;&#65292;&#24182;&#21457;&#29616;&#20854;&#20855;&#26377;&#26680;&#24515;-&#22806;&#22260;&#32467;&#26500;&#21644;&#34676;&#34678;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;BUN&#32467;&#26500;&#32452;&#32455;&#30340;&#28436;&#21270;&#19982;&#27873;&#27819;&#23384;&#22312;&#30456;&#20851;&#30340;&#27874;&#21160;&#26377;&#20851;&#65292;&#36827;&#19968;&#27493;&#35777;&#23454;&#20102;&#32467;&#26500;&#37327;&#19982;&#20215;&#26684;&#21464;&#21160;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27604;&#29305;&#24065;&#20132;&#26131;&#21382;&#21490;&#30340;&#24320;&#25918;&#21487;&#29992;&#24615;&#20026;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#32454;&#33410;&#27700;&#24179;&#30740;&#31350;&#35813;&#31995;&#32479;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#33268;&#21147;&#20110;&#20998;&#26512;&#27604;&#29305;&#24065;&#29992;&#25143;&#32593;&#32476;&#65288;BUN&#65289;&#22312;&#20854;&#25972;&#20010;&#21382;&#21490;&#65288;&#21363;&#20174;2009&#24180;&#21040;2017&#24180;&#65289;&#20013;&#30340;&#20013;&#23567;&#23610;&#24230;&#32467;&#26500;&#23646;&#24615;&#12290;&#20174;&#25105;&#20204;&#30340;&#20998;&#26512;&#20013;&#21487;&#20197;&#30475;&#20986;&#65292;BUN&#20855;&#26377;&#26680;&#24515;-&#22806;&#22260;&#32467;&#26500;&#65292;&#26356;&#28145;&#20837;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#19968;&#23450;&#31243;&#24230;&#30340;&#34676;&#34678;&#32467;&#26500;&#65288;&#21363;&#20855;&#26377;&#24378;&#36830;&#36890;&#20998;&#37327;&#12289;IN&#20998;&#37327;&#21644;OUT&#20998;&#37327;&#20197;&#21450;&#19968;&#20123;&#38468;&#30528;&#22312;IN&#20998;&#37327;&#19978;&#30340;&#35302;&#39035;&#65289;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;BUN&#32467;&#26500;&#32452;&#32455;&#30340;&#28436;&#21270;&#32463;&#21382;&#20102;&#19982;&#27873;&#27819;&#23384;&#22312;&#30456;&#20851;&#30340;&#27874;&#21160;&#65292;&#21363;&#22312;&#25972;&#20010;&#27604;&#29305;&#24065;&#21382;&#21490;&#19978;&#35266;&#23519;&#21040;&#30340;&#20215;&#26684;&#28608;&#22686;&#21644;&#19979;&#36300;&#38454;&#27573;&#65306;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#36827;&#19968;&#27493;&#35777;&#23454;&#20102;&#20808;&#21069;&#20998;&#26512;&#20013;&#35266;&#23519;&#21040;&#30340;&#32467;&#26500;&#37327;&#21644;&#20215;&#26684;&#21464;&#21160;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The open availability of the entire history of the Bitcoin transactions opens up the possibility to study this system at an unprecedented level of detail. This contribution is devoted to the analysis of the mesoscale structural properties of the Bitcoin User Network (BUN), across its entire history (i.e. from 2009 to 2017). What emerges from our analysis is that the BUN is characterized by a core-periphery structure a deeper analysis of which reveals a certain degree of bow-tieness (i.e. the presence of a Strongly-Connected Component, an IN- and an OUT-component together with some tendrils attached to the IN-component). Interestingly, the evolution of the BUN structural organization experiences fluctuations that seem to be correlated with the presence of bubbles, i.e. periods of price surge and decline observed throughout the entire Bitcoin history: our results, thus, further confirm the interplay between structural quantities and price movements observed in previous analyses.
&lt;/p&gt;</description></item><item><title>FairDP&#26159;&#19968;&#31181;&#21516;&#26102;&#30830;&#20445;&#24046;&#20998;&#38544;&#31169;&#21644;&#20844;&#24179;&#24615;&#30340;&#26032;&#22411;&#26426;&#21046;&#65292;&#36890;&#36807;&#29420;&#31435;&#20026;&#19981;&#21516;&#30340;&#20010;&#20307;&#32676;&#20307;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#27493;&#25972;&#21512;&#26469;&#33258;&#32676;&#20307;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#21046;&#23450;&#32508;&#21512;&#27169;&#22411;&#20197;&#24179;&#34913;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#20844;&#24179;&#24615;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;FairDP&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#27169;&#22411;&#25928;&#30410;&#12289;&#38544;&#31169;&#21644;&#20844;&#24179;&#24615;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.16474</link><description>&lt;p&gt;
FairDP: &#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#35748;&#35777;&#30340;&#20844;&#24179;&#24615;&#20445;&#38556;
&lt;/p&gt;
&lt;p&gt;
FairDP: Certified Fairness with Differential Privacy. (arXiv:2305.16474v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16474
&lt;/p&gt;
&lt;p&gt;
FairDP&#26159;&#19968;&#31181;&#21516;&#26102;&#30830;&#20445;&#24046;&#20998;&#38544;&#31169;&#21644;&#20844;&#24179;&#24615;&#30340;&#26032;&#22411;&#26426;&#21046;&#65292;&#36890;&#36807;&#29420;&#31435;&#20026;&#19981;&#21516;&#30340;&#20010;&#20307;&#32676;&#20307;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#27493;&#25972;&#21512;&#26469;&#33258;&#32676;&#20307;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#21046;&#23450;&#32508;&#21512;&#27169;&#22411;&#20197;&#24179;&#34913;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#20844;&#24179;&#24615;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;FairDP&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#27169;&#22411;&#25928;&#30410;&#12289;&#38544;&#31169;&#21644;&#20844;&#24179;&#24615;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FairDP&#30340;&#26032;&#22411;&#26426;&#21046;&#65292;&#26088;&#22312;&#21516;&#26102;&#30830;&#20445;&#24046;&#20998;&#38544;&#31169;(DP)&#21644;&#20844;&#24179;&#24615;&#12290;FairDP&#36890;&#36807;&#29420;&#31435;&#20026;&#19981;&#21516;&#30340;&#20010;&#20307;&#32676;&#20307;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#20351;&#29992;&#32452;&#29305;&#23450;&#30340;&#21098;&#35009;&#39033;&#26469;&#35780;&#20272;&#21644;&#38480;&#21046;DP&#30340;&#24046;&#24322;&#24433;&#21709;&#30340;&#21516;&#26102;&#25805;&#20316;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#35813;&#26426;&#21046;&#36880;&#27493;&#25972;&#21512;&#26469;&#33258;&#32676;&#20307;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#21046;&#23450;&#32508;&#21512;&#27169;&#22411;&#20197;&#24179;&#34913;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#20844;&#24179;&#24615;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#24191;&#27867;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#39564;&#35777;&#20102;FairDP&#30340;&#21151;&#25928;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#27169;&#22411;&#25928;&#30410;&#12289;&#38544;&#31169;&#21644;&#20844;&#24179;&#24615;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces FairDP, a novel mechanism designed to simultaneously ensure differential privacy (DP) and fairness. FairDP operates by independently training models for distinct individual groups, using group-specific clipping terms to assess and bound the disparate impacts of DP. Throughout the training process, the mechanism progressively integrates knowledge from group models to formulate a comprehensive model that balances privacy, utility, and fairness in downstream tasks. Extensive theoretical and empirical analyses validate the efficacy of FairDP, demonstrating improved trade-offs between model utility, privacy, and fairness compared with existing methods.
&lt;/p&gt;</description></item></channel></rss>