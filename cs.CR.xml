<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#36890;&#36807;&#40065;&#26834;&#19968;&#33268;&#23545;&#25239;&#35757;&#32451;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#26356;&#26032;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#21644;&#31995;&#32479;&#23433;&#20840;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.17390</link><description>&lt;p&gt;
&#38024;&#23545;&#23433;&#20840;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26356;&#26032;&#30340;&#40065;&#26834;&#19968;&#33268;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Robustness-Congruent Adversarial Training for Secure Machine Learning Model Updates
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17390
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#40065;&#26834;&#19968;&#33268;&#23545;&#25239;&#35757;&#32451;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#26356;&#26032;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#21644;&#31995;&#32479;&#23433;&#20840;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#23450;&#26399;&#26356;&#26032;&#20197;&#25552;&#39640;&#20854;&#24179;&#22343;&#20934;&#30830;&#24230;&#65292;&#21033;&#29992;&#26032;&#39062;&#30340;&#26550;&#26500;&#21644;&#39069;&#22806;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#26032;&#26356;&#26032;&#30340;&#27169;&#22411;&#21487;&#33021;&#20250;&#29359;&#20197;&#21069;&#27169;&#22411;&#26410;&#26366;&#29359;&#36807;&#30340;&#38169;&#35823;&#12290;&#36825;&#31181;&#35823;&#20998;&#31867;&#34987;&#31216;&#20026;&#36127;&#32763;&#36716;&#65292;&#24182;&#34987;&#29992;&#25143;&#20307;&#39564;&#20026;&#24615;&#33021;&#30340;&#36864;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#38382;&#39064;&#20063;&#24433;&#21709;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#40065;&#26834;&#24615;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#23433;&#20840;&#27169;&#22411;&#26356;&#26032;&#23454;&#36341;&#30340;&#21457;&#23637;&#12290;&#29305;&#21035;&#26159;&#65292;&#24403;&#26356;&#26032;&#27169;&#22411;&#20197;&#25552;&#39640;&#20854;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#26102;&#65292;&#19968;&#20123;&#20808;&#21069;&#26080;&#25928;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#21487;&#33021;&#20250;&#34987;&#38169;&#35823;&#20998;&#31867;&#65292;&#23548;&#33268;&#31995;&#32479;&#23433;&#20840;&#24615;&#30340;&#35748;&#30693;&#36864;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#40065;&#26834;&#19968;&#33268;&#23545;&#25239;&#35757;&#32451;&#30340;&#26032;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23427;&#28041;&#21450;&#20351;&#29992;&#23545;&#25239;&#35757;&#32451;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#21516;&#26102;&#32422;&#26463;&#20854;&#22312;&#23545;&#25239;&#24615;&#31034;&#20363;&#19978;&#20445;&#25345;&#26356;&#39640;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17390v1 Announce Type: new  Abstract: Machine-learning models demand for periodic updates to improve their average accuracy, exploiting novel architectures and additional data. However, a newly-updated model may commit mistakes that the previous model did not make. Such misclassifications are referred to as negative flips, and experienced by users as a regression of performance. In this work, we show that this problem also affects robustness to adversarial examples, thereby hindering the development of secure model update practices. In particular, when updating a model to improve its adversarial robustness, some previously-ineffective adversarial examples may become misclassified, causing a regression in the perceived security of the system. We propose a novel technique, named robustness-congruent adversarial training, to address this issue. It amounts to fine-tuning a model with adversarial training, while constraining it to retain higher robustness on the adversarial examp
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#25915;&#20987;LLMs&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#26799;&#24230;&#24341;&#23548;&#21518;&#38376;&#35302;&#21457;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#34920;&#26126;&#25104;&#21151;&#22320;&#30772;&#22351;&#27169;&#22411;&#36755;&#20986;&#65292;&#20165;&#25913;&#21464;1%&#30340;&#25351;&#23548;&#35843;&#20248;&#26679;&#26412;&#21363;&#21487;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#29575;&#36798;&#21040;&#32422;80&#65285;&#12290;</title><link>https://arxiv.org/abs/2402.13459</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#25351;&#23548;&#35843;&#20248;&#26399;&#38388;&#25805;&#32437;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning to Poison Large Language Models During Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13459
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#25915;&#20987;LLMs&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#26799;&#24230;&#24341;&#23548;&#21518;&#38376;&#35302;&#21457;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#34920;&#26126;&#25104;&#21151;&#22320;&#30772;&#22351;&#27169;&#22411;&#36755;&#20986;&#65292;&#20165;&#25913;&#21464;1%&#30340;&#25351;&#23548;&#35843;&#20248;&#26679;&#26412;&#21363;&#21487;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#29575;&#36798;&#21040;&#32422;80&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#26631;&#24535;&#30528;&#35821;&#35328;&#22788;&#29702;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#37325;&#22823;&#31361;&#30772;&#12290;&#34429;&#28982;&#23427;&#20204;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;LLMs&#38754;&#20020;&#30528;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#30340;&#28431;&#27934;&#65292;&#20854;&#20013;&#23545;&#25163;&#23558;&#21518;&#38376;&#35302;&#21457;&#22120;&#25554;&#20837;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#25805;&#32437;&#36755;&#20986;&#20197;&#36827;&#34892;&#24694;&#24847;&#34892;&#20026;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#65292;&#26088;&#22312;&#21033;&#29992;&#25351;&#23548;&#35843;&#20248;&#36807;&#31243;&#65292;&#36827;&#19968;&#27493;&#35782;&#21035;LLMs&#20013;&#30340;&#39069;&#22806;&#23433;&#20840;&#39118;&#38505;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26799;&#24230;&#24341;&#23548;&#21518;&#38376;&#35302;&#21457;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#35782;&#21035;&#25932;&#23545;&#35302;&#21457;&#22120;&#65292;&#30830;&#20445;&#23545;&#20256;&#32479;&#38450;&#24481;&#25163;&#27573;&#30340;&#35268;&#36991;&#65292;&#21516;&#26102;&#20445;&#25345;&#20869;&#23481;&#30340;&#23436;&#25972;&#24615;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;LLMs&#21644;&#20219;&#21153;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#34920;&#26126;&#22312;&#30772;&#22351;&#27169;&#22411;&#36755;&#20986;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#25104;&#21151;&#29575;&#65307;&#20165;&#23545;4,000&#20010;&#25351;&#23548;&#35843;&#20248;&#26679;&#26412;&#20013;&#30340;1&#65285;&#36827;&#34892;&#27880;&#20837;&#23601;&#23548;&#33268;&#24615;&#33021;&#38477;&#20302;&#29575;&#65288;PDR&#65289;&#32422;&#20026;80&#65285;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#39640;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13459v1 Announce Type: cross  Abstract: The advent of Large Language Models (LLMs) has marked significant achievements in language processing and reasoning capabilities. Despite their advancements, LLMs face vulnerabilities to data poisoning attacks, where adversaries insert backdoor triggers into training data to manipulate outputs for malicious purposes. This work further identifies additional security risks in LLMs by designing a new data poisoning attack tailored to exploit the instruction tuning process. We propose a novel gradient-guided backdoor trigger learning approach to identify adversarial triggers efficiently, ensuring an evasion of detection by conventional defenses while maintaining content integrity. Through experimental validation across various LLMs and tasks, our strategy demonstrates a high success rate in compromising model outputs; poisoning only 1\% of 4,000 instruction tuning samples leads to a Performance Drop Rate (PDR) of around 80\%. Our work high
&lt;/p&gt;</description></item></channel></rss>