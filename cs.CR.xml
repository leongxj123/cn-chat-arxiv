<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#36890;&#36807;&#20351;&#29992;&#26377;&#38480;&#30340;&#20844;&#20849;&#25968;&#25454;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24046;&#20998;&#38544;&#31169;&#25345;&#32493;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#26174;&#33879;&#32531;&#35299;&#20248;&#21270;&#22120;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2402.18752</link><description>&lt;p&gt;
&#20351;&#29992;&#26377;&#38480;&#20844;&#20849;&#25968;&#25454;&#23545;&#26377;&#24046;&#24322;&#38544;&#31169;&#30340;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Pre-training Differentially Private Models with Limited Public Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18752
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#26377;&#38480;&#30340;&#20844;&#20849;&#25968;&#25454;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24046;&#20998;&#38544;&#31169;&#25345;&#32493;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#26174;&#33879;&#32531;&#35299;&#20248;&#21270;&#22120;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#21331;&#36234;&#30340;&#24615;&#33021;&#20381;&#36182;&#20110;&#22823;&#37327;&#39640;&#36136;&#37327;&#25968;&#25454;&#30340;&#20351;&#29992;&#65292;&#28982;&#32780;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#38656;&#35201;&#27491;&#24335;&#20445;&#25252;&#30340;&#25935;&#24863;&#12289;&#31169;&#20154;&#21644;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#20449;&#24687;&#12290;&#24046;&#20998;&#38544;&#31169;&#26159;&#19968;&#31181;&#29992;&#20110;&#34913;&#37327;&#27169;&#22411;&#25552;&#20379;&#30340;&#23433;&#20840;&#31243;&#24230;&#30340;&#37325;&#35201;&#26041;&#27861;&#65292;&#28982;&#32780;&#30001;&#20110;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#24212;&#29992;&#24046;&#20998;&#38544;&#31169;&#20250;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#65292;&#22240;&#27492;&#20854;&#24212;&#29992;&#36890;&#24120;&#20165;&#38480;&#20110;&#27169;&#22411;&#24494;&#35843;&#38454;&#27573;&#12290;&#22240;&#27492;&#65292;&#24046;&#20998;&#38544;&#31169;&#30446;&#21069;&#23578;&#19981;&#33021;&#20445;&#25252;&#21021;&#22987;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#22823;&#37096;&#20998;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20998;&#26512;&#27599;&#27425;&#36845;&#20195;&#30340;&#25439;&#22833;&#25913;&#36827;&#65292;&#23545;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#25552;&#20379;&#20102;&#29702;&#35770;&#29702;&#35299;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#38480;&#30340;&#20844;&#20849;&#25968;&#25454;&#65292;&#21487;&#20197;&#26174;&#33879;&#32531;&#35299;&#24046;&#20998;&#38544;&#31169;&#20248;&#21270;&#22120;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#20174;&#32780;&#24341;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#24046;&#20998;&#38544;&#31169;&#25345;&#32493;&#39044;&#35757;&#32451;&#31574;&#30053;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;&#36890;&#36807;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18752v1 Announce Type: new  Abstract: The superior performance of large foundation models relies on the use of massive amounts of high-quality data, which often contain sensitive, private and copyrighted material that requires formal protection. While differential privacy (DP) is a prominent method to gauge the degree of security provided to the models, its application is commonly limited to the model fine-tuning stage, due to the performance degradation when applying DP during the pre-training stage. Consequently, DP is yet not capable of protecting a substantial portion of the data used during the initial pre-training process.   In this work, we first provide a theoretical understanding of the efficacy of DP training by analyzing the per-iteration loss improvement. We make a key observation that DP optimizers' performance degradation can be significantly mitigated by the use of limited public data, which leads to a novel DP continual pre-training strategy. Empirically, usi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#35843;&#26597;&#20102;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#38754;&#20020;&#30340;&#21518;&#38376;&#25915;&#20987;&#23041;&#32961;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#33324;&#26694;&#26550;&#21644;&#19981;&#21516;&#24418;&#24335;&#30340;&#21518;&#38376;&#25915;&#20987;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.11208</link><description>&lt;p&gt;
&#35686;&#24789;&#24744;&#30340;&#20195;&#29702;&#20154;&#65281;&#35843;&#26597;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#30340;&#21518;&#38376;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11208
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#35843;&#26597;&#20102;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#38754;&#20020;&#30340;&#21518;&#38376;&#25915;&#20987;&#23041;&#32961;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#33324;&#26694;&#26550;&#21644;&#19981;&#21516;&#24418;&#24335;&#30340;&#21518;&#38376;&#25915;&#20987;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLM&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#29992;&#20110;&#22788;&#29702;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#65288;&#21253;&#25324;&#37329;&#34701;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#36141;&#29289;&#31561;&#65289;&#30340;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#12290;&#22312;&#24212;&#29992;&#36807;&#31243;&#20013;&#30830;&#20445;LLM&#20195;&#29702;&#20154;&#30340;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;LLM&#20195;&#29702;&#20154;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#24037;&#20316;&#39318;&#27425;&#25506;&#35752;&#20102;&#20856;&#22411;&#23433;&#20840;&#23041;&#32961;&#20043;&#19968;&#65292;&#21363;&#23545;LLM&#20195;&#29702;&#20154;&#30340;&#21518;&#38376;&#25915;&#20987;&#12290;&#25105;&#20204;&#39318;&#20808;&#21046;&#23450;&#20102;&#19968;&#20010;&#20195;&#29702;&#20154;&#21518;&#38376;&#25915;&#20987;&#30340;&#19968;&#33324;&#26694;&#26550;&#65292;&#28982;&#21518;&#23545;&#19981;&#21516;&#24418;&#24335;&#30340;&#20195;&#29702;&#20154;&#21518;&#38376;&#25915;&#20987;&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20174;&#26368;&#32456;&#25915;&#20987;&#32467;&#26524;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36873;&#25321;&#25805;&#32437;&#26368;&#32456;&#36755;&#20986;&#20998;&#24067;&#65292;&#25110;&#32773;&#20165;&#22312;&#20013;&#38388;&#25512;&#29702;&#36807;&#31243;&#20013;&#24341;&#20837;&#24694;&#24847;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#26368;&#32456;&#36755;&#20986;&#30340;&#27491;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#21069;&#19968;&#31867;&#21487;&#20197;&#20998;&#20026;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11208v1 Announce Type: cross  Abstract: Leveraging the rapid development of Large Language Models LLMs, LLM-based agents have been developed to handle various real-world applications, including finance, healthcare, and shopping, etc. It is crucial to ensure the reliability and security of LLM-based agents during applications. However, the safety issues of LLM-based agents are currently under-explored. In this work, we take the first step to investigate one of the typical safety threats, backdoor attack, to LLM-based agents. We first formulate a general framework of agent backdoor attacks, then we present a thorough analysis on the different forms of agent backdoor attacks. Specifically, from the perspective of the final attacking outcomes, the attacker can either choose to manipulate the final output distribution, or only introduce malicious behavior in the intermediate reasoning process, while keeping the final output correct. Furthermore, the former category can be divided
&lt;/p&gt;</description></item></channel></rss>