<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#36825;&#39033;&#30740;&#31350;&#20840;&#38754;&#20998;&#26512;&#20102;&#20351;&#29992;DNNs&#23545;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#21518;&#38376;&#25915;&#20987;&#65292;&#25581;&#31034;&#20102;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;DNNs&#23545;&#34920;&#26684;&#25968;&#25454;&#38750;&#24120;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#29978;&#33267;&#21482;&#38656;&#26368;&#23567;&#30340;&#29305;&#24449;&#20540;&#20462;&#25913;&#12290;&#35813;&#25915;&#20987;&#36824;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2311.07550</link><description>&lt;p&gt;
Tabdoor&#65306;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#34920;&#26684;&#25968;&#25454;&#31070;&#32463;&#32593;&#32476;&#23384;&#22312;&#21518;&#38376;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Tabdoor: Backdoor Vulnerabilities in Transformer-based Neural Networks for Tabular Data. (arXiv:2311.07550v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.07550
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20840;&#38754;&#20998;&#26512;&#20102;&#20351;&#29992;DNNs&#23545;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#21518;&#38376;&#25915;&#20987;&#65292;&#25581;&#31034;&#20102;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;DNNs&#23545;&#34920;&#26684;&#25968;&#25454;&#38750;&#24120;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#29978;&#33267;&#21482;&#38656;&#26368;&#23567;&#30340;&#29305;&#24449;&#20540;&#20462;&#25913;&#12290;&#35813;&#25915;&#20987;&#36824;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#19982;&#36825;&#20123;&#21457;&#23637;&#21516;&#26102;&#65292;&#19982;DNN&#35757;&#32451;&#30456;&#20851;&#30340;&#28431;&#27934;&#65292;&#22914;&#21518;&#38376;&#25915;&#20987;&#65292;&#26159;&#19968;&#20010;&#37325;&#22823;&#20851;&#20999;&#12290;&#36825;&#20123;&#25915;&#20987;&#28041;&#21450;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#24494;&#22937;&#22320;&#25554;&#20837;&#35302;&#21457;&#22120;&#65292;&#20174;&#32780;&#20801;&#35768;&#25805;&#32437;&#39044;&#27979;&#12290;&#26368;&#36817;&#65292;&#30001;&#20110;&#36716;&#25442;&#22120;&#27169;&#22411;&#30340;&#23835;&#36215;&#65292;DNNs&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23545;&#20351;&#29992;DNNs&#23545;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#21518;&#38376;&#25915;&#20987;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#29305;&#21035;&#20851;&#27880;&#36716;&#25442;&#22120;&#12290;&#37492;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#25506;&#31350;&#20102;&#23884;&#20837;&#21518;&#38376;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#31995;&#32479;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;DNNs&#23545;&#34920;&#26684;&#25968;&#25454;&#38750;&#24120;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#21363;&#20351;&#21482;&#26377;&#26368;&#23567;&#30340;&#29305;&#24449;&#20540;&#20462;&#25913;&#12290;&#25105;&#20204;&#36824;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#25915;&#20987;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#27169;&#22411;&#65292;&#22914;XGBoost&#21644;DeepFM&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20960;&#20046;&#34920;&#26126;&#21518;&#38376;&#25915;&#20987;&#21487;&#20197;&#23436;&#32654;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) have shown great promise in various domains. Alongside these developments, vulnerabilities associated with DNN training, such as backdoor attacks, are a significant concern. These attacks involve the subtle insertion of triggers during model training, allowing for manipulated predictions.More recently, DNNs for tabular data have gained increasing attention due to the rise of transformer models.  Our research presents a comprehensive analysis of backdoor attacks on tabular data using DNNs, particularly focusing on transformers. Given the inherent complexities of tabular data, we explore the challenges of embedding backdoors. Through systematic experimentation across benchmark datasets, we uncover that transformer-based DNNs for tabular data are highly susceptible to backdoor attacks, even with minimal feature value alterations. We also verify that our attack can be generalized to other models, like XGBoost and DeepFM. Our results indicate nearly perfect attac
&lt;/p&gt;</description></item></channel></rss>