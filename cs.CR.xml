<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#26041;&#27861;&#31995;&#32479;&#22320;&#27979;&#35797;&#20102;&#32454;&#35843;&#22823;&#22411;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#23454;&#38469;&#38544;&#31169;&#28431;&#27934;&#65292;&#24182;&#21457;&#29616;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#31867;&#21035;&#30340;&#31034;&#20363;&#25968;&#37327;&#20197;&#21450;&#35757;&#32451;&#32467;&#26463;&#26102;&#30340;&#22823;&#26799;&#24230;&#19982;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#30340;&#28431;&#27934;&#20043;&#38388;&#23384;&#22312;&#20851;&#32852;&#12290;</title><link>https://arxiv.org/abs/2402.06674</link><description>&lt;p&gt;
&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#30340;&#23454;&#38469;&#25104;&#21592;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Understanding Practical Membership Privacy of Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06674
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#26041;&#27861;&#31995;&#32479;&#22320;&#27979;&#35797;&#20102;&#32454;&#35843;&#22823;&#22411;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#23454;&#38469;&#38544;&#31169;&#28431;&#27934;&#65292;&#24182;&#21457;&#29616;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#31867;&#21035;&#30340;&#31034;&#20363;&#25968;&#37327;&#20197;&#21450;&#35757;&#32451;&#32467;&#26463;&#26102;&#30340;&#22823;&#26799;&#24230;&#19982;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#30340;&#28431;&#27934;&#20043;&#38388;&#23384;&#22312;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24212;&#29992;&#26368;&#20808;&#36827;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIA&#65289;&#26469;&#31995;&#32479;&#22320;&#27979;&#35797;&#32454;&#35843;&#22823;&#22411;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#23454;&#38469;&#38544;&#31169;&#28431;&#27934;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#29702;&#35299;&#20351;&#25968;&#25454;&#38598;&#21644;&#26679;&#26412;&#23481;&#26131;&#21463;&#21040;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#30340;&#29305;&#24615;&#12290;&#22312;&#25968;&#25454;&#38598;&#29305;&#24615;&#26041;&#38754;&#65292;&#25105;&#20204;&#21457;&#29616;&#25968;&#25454;&#20013;&#27599;&#20010;&#31867;&#21035;&#30340;&#31034;&#20363;&#25968;&#37327;&#19982;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#30340;&#28431;&#27934;&#20043;&#38388;&#23384;&#22312;&#24378;&#28872;&#30340;&#24130;&#24459;&#20381;&#36182;&#20851;&#31995;&#65292;&#36825;&#26159;&#20197;&#25915;&#20987;&#30340;&#30495;&#38451;&#24615;&#29575;&#65288;&#22312;&#20302;&#20551;&#38451;&#24615;&#29575;&#19979;&#27979;&#37327;&#65289;&#26469;&#34913;&#37327;&#30340;&#12290;&#23545;&#20110;&#20010;&#21035;&#26679;&#26412;&#32780;&#35328;&#65292;&#22312;&#35757;&#32451;&#32467;&#26463;&#26102;&#20135;&#29983;&#30340;&#22823;&#26799;&#24230;&#19982;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#30340;&#28431;&#27934;&#20043;&#38388;&#23384;&#22312;&#24456;&#24378;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We apply a state-of-the-art membership inference attack (MIA) to systematically test the practical privacy vulnerability of fine-tuning large image classification models.We focus on understanding the properties of data sets and samples that make them vulnerable to membership inference. In terms of data set properties, we find a strong power law dependence between the number of examples per class in the data and the MIA vulnerability, as measured by true positive rate of the attack at a low false positive rate. For an individual sample, large gradients at the end of training are strongly correlated with MIA vulnerability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;Foundation&#27169;&#22411;&#38598;&#25104;&#32852;&#37030;&#23398;&#20064;&#20013;&#40065;&#26834;&#24615;&#12289;&#38544;&#31169;&#21644;&#20844;&#24179;&#24615;&#30340;&#25361;&#25112;&#21644;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#24212;&#23545;&#31574;&#30053;&#21644;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.01857</link><description>&lt;p&gt;
&#35780;&#20272;&#22522;&#20110;Foundation&#27169;&#22411;&#38598;&#25104;&#32852;&#37030;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#12289;&#38544;&#31169;&#21644;&#20844;&#24179;&#24615;&#30340;&#31435;&#22330;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Position Paper: Assessing Robustness, Privacy, and Fairness in Federated Learning Integrated with Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;Foundation&#27169;&#22411;&#38598;&#25104;&#32852;&#37030;&#23398;&#20064;&#20013;&#40065;&#26834;&#24615;&#12289;&#38544;&#31169;&#21644;&#20844;&#24179;&#24615;&#30340;&#25361;&#25112;&#21644;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#24212;&#23545;&#31574;&#30053;&#21644;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#20998;&#25955;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#37325;&#22823;&#31361;&#30772;&#65292;&#20294;&#38754;&#20020;&#35832;&#22810;&#25361;&#25112;&#65292;&#22914;&#25968;&#25454;&#21487;&#29992;&#24615;&#26377;&#38480;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#21464;&#21270;&#24615;&#65292;&#36825;&#21487;&#33021;&#20250;&#38480;&#21046;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;&#23558;Foundation&#27169;&#22411;&#65288;FM&#65289;&#38598;&#25104;&#21040;FL&#20013;&#65292;&#21487;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#25968;&#25454;&#22686;&#24378;&#22686;&#21152;&#25968;&#25454;&#20016;&#23500;&#24615;&#24182;&#20943;&#23569;&#35745;&#31639;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#38598;&#25104;&#24341;&#20837;&#20102;&#40065;&#26834;&#24615;&#12289;&#38544;&#31169;&#21644;&#20844;&#24179;&#24615;&#26041;&#38754;&#30340;&#26032;&#38382;&#39064;&#65292;&#22312;&#29616;&#26377;&#30740;&#31350;&#20013;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#35299;&#20915;&#12290;&#25105;&#20204;&#36890;&#36807;&#31995;&#32479;&#35780;&#20272;FM-FL&#38598;&#25104;&#23545;&#36825;&#20123;&#26041;&#38754;&#30340;&#24433;&#21709;&#65292;&#36827;&#34892;&#20102;&#21021;&#27493;&#35843;&#26597;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20854;&#20013;&#30340;&#26435;&#34913;&#21462;&#33293;&#65292;&#25581;&#31034;&#20102;&#35813;&#38598;&#25104;&#24341;&#20837;&#30340;&#23041;&#32961;&#21644;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#22871;&#29992;&#20110;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#30340;&#26631;&#20934;&#21644;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#37492;&#23450;&#20102;&#21487;&#33021;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#19968;&#20123;&#21069;&#26223;&#26041;&#21521;&#21644;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL), while a breakthrough in decentralized machine learning, contends with significant challenges such as limited data availability and the variability of computational resources, which can stifle the performance and scalability of the models. The integration of Foundation Models (FMs) into FL presents a compelling solution to these issues, with the potential to enhance data richness and reduce computational demands through pre-training and data augmentation. However, this incorporation introduces novel issues in terms of robustness, privacy, and fairness, which have not been sufficiently addressed in the existing research. We make a preliminary investigation into this field by systematically evaluating the implications of FM-FL integration across these dimensions. We analyze the trade-offs involved, uncover the threats and issues introduced by this integration, and propose a set of criteria and strategies for navigating these challenges. Furthermore, we identify po
&lt;/p&gt;</description></item></channel></rss>