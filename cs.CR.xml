<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#24341;&#20837;&#21307;&#23398;&#25968;&#25454;&#20013;&#30340;&#38590;&#20197;&#23519;&#35273;&#22122;&#22768;&#26469;&#20445;&#25252;&#25968;&#25454;&#65292;&#38450;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#35757;&#32451;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#39046;&#22495;&#12290;</title><link>https://arxiv.org/abs/2403.10573</link><description>&lt;p&gt;
&#21307;&#23398;&#19981;&#21487;&#23398;&#20064;&#30340;&#31034;&#20363;&#65306;&#36890;&#36807;&#31232;&#30095;&#24863;&#30693;&#26412;&#22320;&#33945;&#29256;&#20445;&#25252;&#21307;&#23398;&#25968;&#25454;&#20813;&#21463;&#26410;&#32463;&#25480;&#26435;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Medical Unlearnable Examples: Securing Medical Data from Unauthorized Traning via Sparsity-Aware Local Masking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10573
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#21307;&#23398;&#25968;&#25454;&#20013;&#30340;&#38590;&#20197;&#23519;&#35273;&#22122;&#22768;&#26469;&#20445;&#25252;&#25968;&#25454;&#65292;&#38450;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#35757;&#32451;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#25935;&#24863;&#21307;&#23398;&#25968;&#25454;&#30340;&#29983;&#25104;&#21644;&#23384;&#20648;&#26174;&#33879;&#22686;&#21152;&#12290;&#36825;&#31181;&#25968;&#25454;&#30340;&#20016;&#23500;&#37327;&#25512;&#21160;&#20102;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26410;&#32463;&#25480;&#26435;&#30340;&#25968;&#25454;&#21033;&#29992;&#65292;&#20363;&#22914;&#29992;&#20110;&#35757;&#32451;&#21830;&#19994;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#24120;&#24120;&#20351;&#30740;&#31350;&#20154;&#21592;&#26395;&#32780;&#21364;&#27493;&#65292;&#22240;&#20026;&#20182;&#20204;&#19981;&#24895;&#20844;&#24320;&#20854;&#23453;&#36149;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#20445;&#25252;&#36825;&#20123;&#38590;&#20197;&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#21516;&#26102;&#40723;&#21169;&#21307;&#30103;&#26426;&#26500;&#20998;&#20139;&#25968;&#25454;&#65292;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#21521;&#25968;&#25454;&#20013;&#24341;&#20837;&#38590;&#20197;&#23519;&#35273;&#30340;&#22122;&#22768;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#22312;&#27169;&#22411;&#27867;&#21270;&#20013;&#24341;&#20837;&#36864;&#21270;&#26469;&#20445;&#25252;&#25968;&#25454;&#65292;&#38450;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#35757;&#32451;&#12290;&#23613;&#31649;&#29616;&#26377;&#26041;&#27861;&#22312;&#19968;&#33324;&#39046;&#22495;&#26174;&#31034;&#20986;&#20196;&#20154;&#38054;&#20329;&#30340;&#25968;&#25454;&#20445;&#25252;&#33021;&#21147;&#65292;&#20294;&#24403;&#24212;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#26102;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#23427;&#20204;&#26410;&#33021;&#32771;&#34385;&#21040;&#31232;&#30095;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10573v1 Announce Type: cross  Abstract: With the rapid growth of artificial intelligence (AI) in healthcare, there has been a significant increase in the generation and storage of sensitive medical data. This abundance of data, in turn, has propelled the advancement of medical AI technologies. However, concerns about unauthorized data exploitation, such as training commercial AI models, often deter researchers from making their invaluable datasets publicly available. In response to the need to protect this hard-to-collect data while still encouraging medical institutions to share it, one promising solution is to introduce imperceptible noise into the data. This method aims to safeguard the data against unauthorized training by inducing degradation in model generalization. Although existing methods have shown commendable data protection capabilities in general domains, they tend to fall short when applied to biomedical data, mainly due to their failure to account for the spar
&lt;/p&gt;</description></item><item><title>&#26631;&#31614;&#24179;&#28369;&#26041;&#27861;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#26082;&#33021;&#25552;&#21319;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#21644;&#26657;&#20934;&#24615;&#65292;&#21448;&#21487;&#33021;&#25104;&#20026;&#27169;&#22411;&#38544;&#31169;&#27844;&#38706;&#30340;&#22240;&#32032;&#12290;&#30740;&#31350;&#25581;&#31034;&#20102;&#32467;&#21512;&#36127;&#22240;&#23376;&#36827;&#34892;&#24179;&#28369;&#21487;&#26377;&#25928;&#38459;&#27490;&#27169;&#22411;&#21453;&#25512;&#25915;&#20987;&#65292;&#25552;&#21319;&#38544;&#31169;&#20445;&#25252;&#25928;&#26524;&#65292;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#38450;&#24481;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2310.06549</link><description>&lt;p&gt;
&#35880;&#24910;&#24179;&#28369;&#26631;&#31614;&#65306;&#26631;&#31614;&#24179;&#28369;&#26082;&#21487;&#20197;&#20316;&#20026;&#38544;&#31169;&#23631;&#38556;&#65292;&#21448;&#21487;&#20197;&#25104;&#20026;&#27169;&#22411;&#21453;&#25512;&#25915;&#20987;&#30340;&#20652;&#21270;&#21058;
&lt;/p&gt;
&lt;p&gt;
Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield but Also a Catalyst for Model Inversion Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.06549
&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#24179;&#28369;&#26041;&#27861;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#26082;&#33021;&#25552;&#21319;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#21644;&#26657;&#20934;&#24615;&#65292;&#21448;&#21487;&#33021;&#25104;&#20026;&#27169;&#22411;&#38544;&#31169;&#27844;&#38706;&#30340;&#22240;&#32032;&#12290;&#30740;&#31350;&#25581;&#31034;&#20102;&#32467;&#21512;&#36127;&#22240;&#23376;&#36827;&#34892;&#24179;&#28369;&#21487;&#26377;&#25928;&#38459;&#27490;&#27169;&#22411;&#21453;&#25512;&#25915;&#20987;&#65292;&#25552;&#21319;&#38544;&#31169;&#20445;&#25252;&#25928;&#26524;&#65292;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#38450;&#24481;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#24179;&#28369;&#8212;&#8212;&#20351;&#29992;&#36719;&#21270;&#30340;&#26631;&#31614;&#32780;&#19981;&#26159;&#30828;&#26631;&#31614;&#8212;&#8212;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#34987;&#24191;&#27867;&#37319;&#29992;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#34920;&#29616;&#20986;&#22686;&#24378;&#27867;&#21270;&#21644;&#26657;&#20934;&#31561;&#22810;&#26679;&#30410;&#22788;&#12290;&#28982;&#32780;&#65292;&#23427;&#23545;&#20110;&#20445;&#25252;&#27169;&#22411;&#38544;&#31169;&#30340;&#24433;&#21709;&#20173;&#28982;&#27809;&#26377;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#26631;&#31614;&#24179;&#28369;&#23545;&#27169;&#22411;&#21453;&#25512;&#25915;&#20987;&#65288;MIAs&#65289;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#25915;&#20987;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#20998;&#31867;&#22120;&#20013;&#32534;&#30721;&#30340;&#30693;&#35782;&#29983;&#25104;&#20855;&#26377;&#31867;&#20195;&#34920;&#24615;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#25512;&#26029;&#26377;&#20851;&#20854;&#35757;&#32451;&#25968;&#25454;&#30340;&#25935;&#24863;&#20449;&#24687;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20256;&#32479;&#26631;&#31614;&#24179;&#28369;&#20419;&#36827;&#20102;MIAs&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#27169;&#22411;&#30340;&#38544;&#31169;&#27844;&#38706;&#12290;&#26356;&#29978;&#32773;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#29992;&#36127;&#22240;&#23376;&#36827;&#34892;&#24179;&#28369;&#21487;&#20197;&#25269;&#21046;&#36825;&#19968;&#36235;&#21183;&#65292;&#38459;&#30861;&#25552;&#21462;&#19982;&#31867;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#65292;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;&#36825;&#30830;&#31435;&#20102;&#19968;&#31181;&#23454;&#29992;&#19988;&#24378;&#22823;&#30340;&#26032;&#30340;&#22686;&#24378;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.06549v2 Announce Type: replace  Abstract: Label smoothing -- using softened labels instead of hard ones -- is a widely adopted regularization method for deep learning, showing diverse benefits such as enhanced generalization and calibration. Its implications for preserving model privacy, however, have remained unexplored. To fill this gap, we investigate the impact of label smoothing on model inversion attacks (MIAs), which aim to generate class-representative samples by exploiting the knowledge encoded in a classifier, thereby inferring sensitive information about its training data. Through extensive analyses, we uncover that traditional label smoothing fosters MIAs, thereby increasing a model's privacy leakage. Even more, we reveal that smoothing with negative factors counters this trend, impeding the extraction of class-related information and leading to privacy preservation, beating state-of-the-art defenses. This establishes a practical and powerful novel way for enhanc
&lt;/p&gt;</description></item></channel></rss>