<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20294;&#20063;&#38754;&#20020;&#30528;&#23433;&#20840;&#21644;&#38544;&#31169;&#25915;&#20987;&#30340;&#23041;&#32961;&#12290;&#26412;&#35843;&#26597;&#20840;&#38754;&#23457;&#26597;&#20102;LLM&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#25361;&#25112;&#65292;&#28085;&#30422;&#20102;&#35757;&#32451;&#25968;&#25454;&#12289;&#29992;&#25143;&#21644;&#24212;&#29992;&#39118;&#38505;&#31561;&#26041;&#38754;&#65292;&#24182;&#23545;&#35299;&#20915;&#26041;&#27861;&#36827;&#34892;&#20102;&#22238;&#39038;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00888</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#25361;&#25112;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Security and Privacy Challenges of Large Language Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00888
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20294;&#20063;&#38754;&#20020;&#30528;&#23433;&#20840;&#21644;&#38544;&#31169;&#25915;&#20987;&#30340;&#23041;&#32961;&#12290;&#26412;&#35843;&#26597;&#20840;&#38754;&#23457;&#26597;&#20102;LLM&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#25361;&#25112;&#65292;&#28085;&#30422;&#20102;&#35757;&#32451;&#25968;&#25454;&#12289;&#29992;&#25143;&#21644;&#24212;&#29992;&#39118;&#38505;&#31561;&#26041;&#38754;&#65292;&#24182;&#23545;&#35299;&#20915;&#26041;&#27861;&#36827;&#34892;&#20102;&#22238;&#39038;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#29983;&#25104;&#21644;&#24635;&#32467;&#25991;&#26412;&#12289;&#35821;&#35328;&#32763;&#35793;&#21644;&#38382;&#31572;&#31561;&#22810;&#20010;&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#22914;&#20170;&#65292;LLM&#27491;&#22312;&#25104;&#20026;&#35745;&#31639;&#26426;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#38750;&#24120;&#27969;&#34892;&#30340;&#24037;&#20855;&#65292;&#20855;&#22791;&#20998;&#26512;&#22797;&#26434;&#35821;&#35328;&#27169;&#24335;&#24182;&#26681;&#25454;&#19978;&#19979;&#25991;&#25552;&#20379;&#30456;&#20851;&#21644;&#36866;&#24403;&#22238;&#31572;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#36825;&#20123;&#27169;&#22411;&#20063;&#23481;&#26131;&#21463;&#21040;&#23433;&#20840;&#21644;&#38544;&#31169;&#25915;&#20987;&#30340;&#23041;&#32961;&#65292;&#22914;&#36234;&#29425;&#25915;&#20987;&#12289;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#21644;&#20010;&#20154;&#21487;&#35782;&#21035;&#20449;&#24687;&#27844;&#38706;&#25915;&#20987;&#12290;&#26412;&#35843;&#26597;&#20840;&#38754;&#23457;&#26597;&#20102;LLM&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#25361;&#25112;&#65292;&#21253;&#25324;&#35757;&#32451;&#25968;&#25454;&#21644;&#29992;&#25143;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#22312;&#20132;&#36890;&#12289;&#25945;&#32946;&#21644;&#21307;&#30103;&#31561;&#21508;&#20010;&#39046;&#22495;&#20013;&#24212;&#29992;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;LLM&#30340;&#33030;&#24369;&#24615;&#31243;&#24230;&#65292;&#35843;&#26597;&#20102;&#20986;&#29616;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#25915;&#20987;&#65292;&#24182;&#23545;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#27861;&#36827;&#34892;&#20102;&#22238;&#39038;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated extraordinary capabilities and contributed to multiple fields, such as generating and summarizing text, language translation, and question-answering. Nowadays, LLM is becoming a very popular tool in computerized language processing tasks, with the capability to analyze complicated linguistic patterns and provide relevant and appropriate responses depending on the context. While offering significant advantages, these models are also vulnerable to security and privacy attacks, such as jailbreaking attacks, data poisoning attacks, and Personally Identifiable Information (PII) leakage attacks. This survey provides a thorough review of the security and privacy challenges of LLMs for both training data and users, along with the application-based risks in various domains, such as transportation, education, and healthcare. We assess the extent of LLM vulnerabilities, investigate emerging security and privacy attacks for LLMs, and review the potent
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#26041;&#27861;&#65292;JudgeDeceiver&#65292;&#38024;&#23545;LLM-as-a-Judge&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#29983;&#25104;&#23545;&#25239;&#24207;&#21015;&#23454;&#29616;&#20102;&#26377;&#38024;&#23545;&#24615;&#21644;&#39640;&#25928;&#30340;&#27169;&#22411;&#35780;&#20272;&#25805;&#25511;&#12290;</title><link>https://arxiv.org/abs/2403.17710</link><description>&lt;p&gt;
&#22522;&#20110;&#20248;&#21270;&#30340;&#23545;LLM&#35780;&#21028;&#31995;&#32479;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Optimization-based Prompt Injection Attack to LLM-as-a-Judge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17710
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#26041;&#27861;&#65292;JudgeDeceiver&#65292;&#38024;&#23545;LLM-as-a-Judge&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#29983;&#25104;&#23545;&#25239;&#24207;&#21015;&#23454;&#29616;&#20102;&#26377;&#38024;&#23545;&#24615;&#21644;&#39640;&#25928;&#30340;&#27169;&#22411;&#35780;&#20272;&#25805;&#25511;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM-as-a-Judge &#26159;&#19968;&#31181;&#21487;&#20197;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35780;&#20272;&#25991;&#26412;&#20449;&#24687;&#30340;&#26032;&#39062;&#35299;&#20915;&#26041;&#26696;&#12290;&#26681;&#25454;&#29616;&#26377;&#30740;&#31350;&#65292;LLMs&#22312;&#25552;&#20379;&#20256;&#32479;&#20154;&#31867;&#35780;&#20272;&#30340;&#24341;&#20154;&#27880;&#30446;&#26367;&#20195;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#38024;&#23545;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;JudgeDeceiver&#65292;&#19968;&#31181;&#38024;&#23545;LLM-as-a-Judge&#37327;&#36523;&#23450;&#21046;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21046;&#23450;&#20102;&#19968;&#20010;&#31934;&#30830;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#29992;&#20110;&#25915;&#20987;LLM-as-a-Judge&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#21033;&#29992;&#20248;&#21270;&#31639;&#27861;&#39640;&#25928;&#22320;&#33258;&#21160;&#21270;&#29983;&#25104;&#23545;&#25239;&#24207;&#21015;&#65292;&#23454;&#29616;&#23545;&#27169;&#22411;&#35780;&#20272;&#30340;&#26377;&#38024;&#23545;&#24615;&#21644;&#26377;&#25928;&#30340;&#25805;&#20316;&#12290;&#19982;&#25163;&#24037;&#21046;&#20316;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#21151;&#25928;&#65292;&#32473;&#22522;&#20110;LLM&#30340;&#21028;&#26029;&#31995;&#32479;&#24403;&#21069;&#30340;&#23433;&#20840;&#33539;&#24335;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17710v1 Announce Type: cross  Abstract: LLM-as-a-Judge is a novel solution that can assess textual information with large language models (LLMs). Based on existing research studies, LLMs demonstrate remarkable performance in providing a compelling alternative to traditional human assessment. However, the robustness of these systems against prompt injection attacks remains an open question. In this work, we introduce JudgeDeceiver, a novel optimization-based prompt injection attack tailored to LLM-as-a-Judge. Our method formulates a precise optimization objective for attacking the decision-making process of LLM-as-a-Judge and utilizes an optimization algorithm to efficiently automate the generation of adversarial sequences, achieving targeted and effective manipulation of model evaluations. Compared to handcraft prompt injection attacks, our method demonstrates superior efficacy, posing a significant challenge to the current security paradigms of LLM-based judgment systems. T
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#25925;&#38556;&#27880;&#20837;&#21644;&#23433;&#20840;&#38169;&#35823;&#25915;&#20987;&#29992;&#20110;&#25552;&#21462;&#23884;&#20837;&#24335;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#38416;&#36848;&#20102;&#23545;32&#20301;&#24494;&#25511;&#21046;&#22120;&#19978;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.16703</link><description>&lt;p&gt;
&#25925;&#38556;&#27880;&#20837;&#21644;&#23433;&#20840;&#38169;&#35823;&#25915;&#20987;&#29992;&#20110;&#25552;&#21462;&#23884;&#20837;&#24335;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fault Injection and Safe-Error Attack for Extraction of Embedded Neural Network Models. (arXiv:2308.16703v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25925;&#38556;&#27880;&#20837;&#21644;&#23433;&#20840;&#38169;&#35823;&#25915;&#20987;&#29992;&#20110;&#25552;&#21462;&#23884;&#20837;&#24335;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#38416;&#36848;&#20102;&#23545;32&#20301;&#24494;&#25511;&#21046;&#22120;&#19978;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#25552;&#21462;&#20316;&#20026;&#19968;&#31181;&#20851;&#38190;&#30340;&#23433;&#20840;&#23041;&#32961;&#32780;&#20986;&#29616;&#65292;&#25915;&#20987;&#21521;&#37327;&#21033;&#29992;&#20102;&#31639;&#27861;&#21644;&#23454;&#29616;&#26041;&#38754;&#30340;&#26041;&#27861;&#12290;&#25915;&#20987;&#32773;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#23613;&#21487;&#33021;&#22810;&#22320;&#31363;&#21462;&#21463;&#20445;&#25252;&#30340;&#21463;&#23475;&#32773;&#27169;&#22411;&#30340;&#20449;&#24687;&#65292;&#20197;&#20415;&#20182;&#21487;&#20197;&#29992;&#26367;&#20195;&#27169;&#22411;&#26469;&#27169;&#20223;&#23427;&#65292;&#21363;&#20351;&#21482;&#26377;&#26377;&#38480;&#30340;&#35775;&#38382;&#30456;&#20284;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#26368;&#36817;&#65292;&#29289;&#29702;&#25915;&#20987;&#65292;&#22914;&#25925;&#38556;&#27880;&#20837;&#65292;&#24050;&#32463;&#26174;&#31034;&#20986;&#23545;&#23884;&#20837;&#24335;&#27169;&#22411;&#30340;&#23436;&#25972;&#24615;&#21644;&#26426;&#23494;&#24615;&#30340;&#20196;&#20154;&#25285;&#24551;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;32&#20301;&#24494;&#25511;&#21046;&#22120;&#19978;&#30340;&#23884;&#20837;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36825;&#26159;&#29289;&#32852;&#32593;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#30828;&#20214;&#24179;&#21488;&#31995;&#21015;&#65292;&#20197;&#21450;&#20351;&#29992;&#26631;&#20934;&#25925;&#38556;&#27880;&#20837;&#31574;&#30053;-&#23433;&#20840;&#38169;&#35823;&#25915;&#20987;&#65288;SEA&#65289;&#26469;&#36827;&#34892;&#20855;&#26377;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#35775;&#38382;&#30340;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#12290;&#30001;&#20110;&#25915;&#20987;&#24378;&#28872;&#20381;&#36182;&#20110;&#36755;&#20837;&#26597;&#35810;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#40657;&#30418;&#26041;&#27861;&#26469;&#26500;&#24314;&#19968;&#20010;&#25104;&#21151;&#30340;&#25915;&#20987;&#38598;&#12290;&#23545;&#20110;&#19968;&#20010;&#32463;&#20856;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#24674;&#22797;&#20102;&#33267;&#23569;90%&#30340;
&lt;/p&gt;
&lt;p&gt;
Model extraction emerges as a critical security threat with attack vectors exploiting both algorithmic and implementation-based approaches. The main goal of an attacker is to steal as much information as possible about a protected victim model, so that he can mimic it with a substitute model, even with a limited access to similar training data. Recently, physical attacks such as fault injection have shown worrying efficiency against the integrity and confidentiality of embedded models. We focus on embedded deep neural network models on 32-bit microcontrollers, a widespread family of hardware platforms in IoT, and the use of a standard fault injection strategy - Safe Error Attack (SEA) - to perform a model extraction attack with an adversary having a limited access to training data. Since the attack strongly depends on the input queries, we propose a black-box approach to craft a successful attack set. For a classical convolutional neural network, we successfully recover at least 90% of
&lt;/p&gt;</description></item></channel></rss>