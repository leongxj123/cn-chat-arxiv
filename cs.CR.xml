<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>ADVREPAIR&#26159;&#19968;&#31181;&#21033;&#29992;&#26377;&#38480;&#25968;&#25454;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#30340;&#21487;&#35777;&#20462;&#22797;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24418;&#24335;&#39564;&#35777;&#26500;&#24314;&#34917;&#19969;&#27169;&#22359;&#65292;&#22312;&#31283;&#20581;&#37051;&#22495;&#20869;&#25552;&#20379;&#21487;&#35777;&#21644;&#19987;&#38376;&#30340;&#20462;&#22797;&#65292;&#21516;&#26102;&#20855;&#26377;&#27867;&#21270;&#21040;&#20854;&#20182;&#36755;&#20837;&#30340;&#38450;&#24481;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.01642</link><description>&lt;p&gt;
ADVREPAIR&#65306;&#23545;&#25239;&#25915;&#20987;&#30340;&#21487;&#35777;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
ADVREPAIR:Provable Repair of Adversarial Attack
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01642
&lt;/p&gt;
&lt;p&gt;
ADVREPAIR&#26159;&#19968;&#31181;&#21033;&#29992;&#26377;&#38480;&#25968;&#25454;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#30340;&#21487;&#35777;&#20462;&#22797;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24418;&#24335;&#39564;&#35777;&#26500;&#24314;&#34917;&#19969;&#27169;&#22359;&#65292;&#22312;&#31283;&#20581;&#37051;&#22495;&#20869;&#25552;&#20379;&#21487;&#35777;&#21644;&#19987;&#38376;&#30340;&#20462;&#22797;&#65292;&#21516;&#26102;&#20855;&#26377;&#27867;&#21270;&#21040;&#20854;&#20182;&#36755;&#20837;&#30340;&#38450;&#24481;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#20013;&#30340;&#37096;&#32626;&#26085;&#30410;&#22686;&#21152;&#65292;&#20294;&#23427;&#20204;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#26500;&#25104;&#20005;&#37325;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#29616;&#26377;&#30340;&#20351;&#29992;&#26377;&#38480;&#25968;&#25454;&#30340;&#31070;&#32463;&#20803;&#32423;&#26041;&#27861;&#22312;&#20462;&#22797;&#23545;&#25163;&#26041;&#38754;&#32570;&#20047;&#25928;&#21147;&#65292;&#22240;&#20026;&#23545;&#25239;&#25915;&#20987;&#26426;&#21046;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;&#32780;&#23545;&#25239;&#35757;&#32451;&#65292;&#21033;&#29992;&#22823;&#37327;&#23545;&#25239;&#26679;&#26412;&#22686;&#24378;&#40065;&#26834;&#24615;&#65292;&#32570;&#20047;&#21487;&#35777;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ADVREPAIR&#65292;&#19968;&#31181;&#21033;&#29992;&#26377;&#38480;&#25968;&#25454;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#30340;&#21487;&#35777;&#20462;&#22797;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#24418;&#24335;&#39564;&#35777;&#65292;ADVREPAIR&#26500;&#24314;&#34917;&#19969;&#27169;&#22359;&#65292;&#24403;&#19982;&#21407;&#22987;&#32593;&#32476;&#38598;&#25104;&#26102;&#65292;&#22312;&#31283;&#20581;&#37051;&#22495;&#20869;&#25552;&#20379;&#21487;&#35777;&#21644;&#19987;&#38376;&#30340;&#20462;&#22797;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#21253;&#25324;&#19968;&#31181;&#21551;&#21457;&#24335;&#26426;&#21046;&#26469;&#20998;&#37197;&#34917;&#19969;&#27169;&#22359;&#65292;&#20351;&#24471;&#36825;&#31181;&#38450;&#24481;&#23545;&#25239;&#25915;&#20987;&#27867;&#21270;&#21040;&#20854;&#20182;&#36755;&#20837;&#12290;ADVREPAIR&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01642v1 Announce Type: new  Abstract: Deep neural networks (DNNs) are increasingly deployed in safety-critical domains, but their vulnerability to adversarial attacks poses serious safety risks. Existing neuron-level methods using limited data lack efficacy in fixing adversaries due to the inherent complexity of adversarial attack mechanisms, while adversarial training, leveraging a large number of adversarial samples to enhance robustness, lacks provability. In this paper, we propose ADVREPAIR, a novel approach for provable repair of adversarial attacks using limited data. By utilizing formal verification, ADVREPAIR constructs patch modules that, when integrated with the original network, deliver provable and specialized repairs within the robustness neighborhood. Additionally, our approach incorporates a heuristic mechanism for assigning patch modules, allowing this defense against adversarial attacks to generalize to other inputs. ADVREPAIR demonstrates superior efficienc
&lt;/p&gt;</description></item><item><title>&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;&#38543;&#26426;&#26799;&#24230; Langevin &#21453;&#36951;&#24536;&#26041;&#27861;&#65292;&#20026;&#36817;&#20284;&#21453;&#36951;&#24536;&#38382;&#39064;&#25552;&#20379;&#20102;&#38544;&#31169;&#20445;&#38556;&#65292;&#24182;&#23637;&#31034;&#20102;&#23567;&#25209;&#27425;&#26799;&#24230;&#26356;&#26032;&#30456;&#36739;&#20110;&#20840;&#25209;&#27425;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.17105</link><description>&lt;p&gt;
&#38543;&#26426;&#26799;&#24230; Langevin &#21453;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Stochastic Gradient Langevin Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;&#38543;&#26426;&#26799;&#24230; Langevin &#21453;&#36951;&#24536;&#26041;&#27861;&#65292;&#20026;&#36817;&#20284;&#21453;&#36951;&#24536;&#38382;&#39064;&#25552;&#20379;&#20102;&#38544;&#31169;&#20445;&#38556;&#65292;&#24182;&#23637;&#31034;&#20102;&#23567;&#25209;&#27425;&#26799;&#24230;&#26356;&#26032;&#30456;&#36739;&#20110;&#20840;&#25209;&#27425;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#34987;&#36951;&#24536;&#30340;&#26435;&#21033;&#8221;&#26159;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#30340;&#27861;&#24459;&#25152;&#30830;&#20445;&#30340;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26426;&#22120;&#21453;&#36951;&#24536;&#26088;&#22312;&#39640;&#25928;&#22320;&#28040;&#38500;&#24050;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#19978;&#26576;&#20123;&#25968;&#25454;&#28857;&#30340;&#24433;&#21709;&#65292;&#20351;&#20854;&#36817;&#20284;&#20110;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38543;&#26426;&#26799;&#24230; Langevin &#21453;&#36951;&#24536;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#24102;&#26377;&#38544;&#31169;&#20445;&#38556;&#30340;&#22122;&#22768;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#21453;&#36951;&#24536;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#20984;&#24615;&#20551;&#35774;&#19979;&#30340;&#36817;&#20284;&#21453;&#36951;&#24536;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20840;&#25209;&#27425;&#23545;&#24212;&#26041;&#27861;&#30456;&#27604;&#65292;&#23567;&#25209;&#27425;&#26799;&#24230;&#26356;&#26032;&#22312;&#38544;&#31169;&#22797;&#26434;&#24230;&#26435;&#34913;&#26041;&#38754;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21453;&#36951;&#24536;&#26041;&#27861;&#20855;&#26377;&#35832;&#22810;&#31639;&#27861;&#20248;&#21183;&#65292;&#21253;&#25324;&#19982;&#37325;&#26032;&#35757;&#32451;&#30456;&#27604;&#30340;&#22797;&#26434;&#24230;&#33410;&#30465;&#65292;&#20197;&#21450;&#25903;&#25345;&#39034;&#24207;&#21644;&#25209;&#37327;&#21453;&#36951;&#24536;&#12290;&#20026;&#20102;&#26816;&#39564;&#25105;&#20204;&#26041;&#27861;&#30340;&#38544;&#31169;-&#25928;&#29992;-&#22797;&#26434;&#24230;&#26435;&#34913;&#65292;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17105v1 Announce Type: new  Abstract: ``The right to be forgotten'' ensured by laws for user data privacy becomes increasingly important. Machine unlearning aims to efficiently remove the effect of certain data points on the trained model parameters so that it can be approximately the same as if one retrains the model from scratch. This work proposes stochastic gradient Langevin unlearning, the first unlearning framework based on noisy stochastic gradient descent (SGD) with privacy guarantees for approximate unlearning problems under convexity assumption. Our results show that mini-batch gradient updates provide a superior privacy-complexity trade-off compared to the full-batch counterpart. There are numerous algorithmic benefits of our unlearning approach, including complexity saving compared to retraining, and supporting sequential and batch unlearning. To examine the privacy-utility-complexity trade-off of our method, we conduct experiments on benchmark datasets compared 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20445;&#25252;&#20010;&#20154;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#25239;&#24615;LLM&#25512;&#26029;&#30340;&#21311;&#21517;&#21270;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.13846</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#20808;&#36827;&#30340;&#21311;&#21517;&#21270;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Advanced Anonymizers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13846
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20445;&#25252;&#20010;&#20154;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#25239;&#24615;LLM&#25512;&#26029;&#30340;&#21311;&#21517;&#21270;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#38544;&#31169;&#30740;&#31350;&#39046;&#22495;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23427;&#20204;&#22312;&#25512;&#26029;&#30495;&#23454;&#19990;&#30028;&#22312;&#32447;&#25991;&#26412;&#20013;&#30340;&#20010;&#20154;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;&#38543;&#30528;&#27169;&#22411;&#33021;&#21147;&#30340;&#19981;&#26029;&#22686;&#24378;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#21311;&#21517;&#21270;&#26041;&#27861;&#24403;&#21069;&#24050;&#32463;&#33853;&#21518;&#20110;&#30417;&#31649;&#35201;&#27714;&#21644;&#23545;&#25239;&#23041;&#32961;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#20010;&#20154;&#22914;&#20309;&#26377;&#25928;&#22320;&#20445;&#25252;&#20182;&#20204;&#22312;&#20998;&#20139;&#22312;&#32447;&#25991;&#26412;&#26102;&#30340;&#20010;&#20154;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#21462;&#20102;&#20004;&#27493;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35774;&#32622;&#65292;&#29992;&#20110;&#35780;&#20272;&#38754;&#23545;&#23545;&#25239;&#24615;LLM&#30340;&#25512;&#26029;&#26102;&#30340;&#21311;&#21517;&#21270;&#25928;&#26524;&#65292;&#20174;&#32780;&#20801;&#35768;&#33258;&#28982;&#22320;&#27979;&#37327;&#21311;&#21517;&#21270;&#24615;&#33021;&#65292;&#21516;&#26102;&#32416;&#27491;&#20102;&#20197;&#21069;&#25351;&#26631;&#30340;&#19968;&#20123;&#32570;&#38519;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;LLM&#30340;&#23545;&#25239;&#24615;&#21311;&#21517;&#21270;&#26694;&#26550;&#65292;&#21033;&#29992;LLM&#30340;&#24378;&#22823;&#25512;&#26029;&#33021;&#21147;&#26469;&#25351;&#23548;&#25105;&#20204;&#30340;&#21311;&#21517;&#21270;&#36807;&#31243;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#21311;&#21517;&#21270;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13846v1 Announce Type: cross  Abstract: Recent work in privacy research on large language models has shown that they achieve near human-level performance at inferring personal data from real-world online texts. With consistently increasing model capabilities, existing text anonymization methods are currently lacking behind regulatory requirements and adversarial threats. This raises the question of how individuals can effectively protect their personal data in sharing online texts. In this work, we take two steps to answer this question: We first present a new setting for evaluating anonymizations in the face of adversarial LLMs inferences, allowing for a natural measurement of anonymization performance while remedying some of the shortcomings of previous metrics. We then present our LLM-based adversarial anonymization framework leveraging the strong inferential capabilities of LLMs to inform our anonymization procedure. In our experimental evaluation, we show on real-world 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#24120;&#35265;&#26426;&#22120;&#23398;&#20064;&#30828;&#20214;&#21152;&#36895;&#22120;&#20869;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#23558;&#26368;&#23567;&#21518;&#38376;&#27010;&#24565;&#21644;&#21487;&#37197;&#32622;&#30340;&#30828;&#20214;&#26408;&#39532;&#32467;&#21512;&#20351;&#29992;&#65292;&#20174;&#32780;&#23545;&#30446;&#21069;&#30340;&#38450;&#24481;&#25514;&#26045;&#26500;&#25104;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.08411</link><description>&lt;p&gt;
&#26469;&#33258;&#20869;&#37096;&#30340;&#37034;&#24694;: &#36890;&#36807;&#30828;&#20214;&#26408;&#39532;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Evil from Within: Machine Learning Backdoors through Hardware Trojans. (arXiv:2304.08411v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#24120;&#35265;&#26426;&#22120;&#23398;&#20064;&#30828;&#20214;&#21152;&#36895;&#22120;&#20869;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#23558;&#26368;&#23567;&#21518;&#38376;&#27010;&#24565;&#21644;&#21487;&#37197;&#32622;&#30340;&#30828;&#20214;&#26408;&#39532;&#32467;&#21512;&#20351;&#29992;&#65292;&#20174;&#32780;&#23545;&#30446;&#21069;&#30340;&#38450;&#24481;&#25514;&#26045;&#26500;&#25104;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#38376;&#20250;&#23545;&#26426;&#22120;&#23398;&#20064;&#36896;&#25104;&#20005;&#37325;&#23041;&#32961;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#30772;&#22351;&#23433;&#20840;&#20851;&#38190;&#30340;&#31995;&#32479;&#65292;&#22914;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#23436;&#20840;&#23621;&#20110;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24120;&#35265;&#30828;&#20214;&#21152;&#36895;&#22120;&#20869;&#65292;&#20174;&#32780;&#23545;&#24403;&#21069;&#38450;&#24481;&#25514;&#26045;&#26500;&#25104;&#25361;&#25112;&#12290;&#20026;&#20102;&#20351;&#36825;&#31181;&#25915;&#20987;&#23454;&#29992;&#65292;&#25105;&#20204;&#20811;&#26381;&#20102;&#20004;&#20010;&#25361;&#25112;&#65306;&#39318;&#20808;&#65292;&#30001;&#20110;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#30340;&#23384;&#20648;&#31354;&#38388;&#20005;&#37325;&#21463;&#38480;&#65292;&#22240;&#27492;&#25105;&#20204;&#24341;&#20837;&#20102;&#25152;&#35859;&#30340;&#26368;&#23567;&#21518;&#38376;&#27010;&#24565;&#65292;&#21482;&#25913;&#21464;&#23569;&#37327;&#27169;&#22411;&#21442;&#25968;&#21363;&#21487;&#28608;&#27963;&#21518;&#38376;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#37197;&#32622;&#30340;&#30828;&#20214;&#26408;&#39532;&#65292;&#21487;&#20197;&#19982;&#21518;&#38376;&#19968;&#36215;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backdoors pose a serious threat to machine learning, as they can compromise the integrity of security-critical systems, such as self-driving cars. While different defenses have been proposed to address this threat, they all rely on the assumption that the hardware on which the learning models are executed during inference is trusted. In this paper, we challenge this assumption and introduce a backdoor attack that completely resides within a common hardware accelerator for machine learning. Outside of the accelerator, neither the learning model nor the software is manipulated, so that current defenses fail. To make this attack practical, we overcome two challenges: First, as memory on a hardware accelerator is severely limited, we introduce the concept of a minimal backdoor that deviates as little as possible from the original model and is activated by replacing a few model parameters only. Second, we develop a configurable hardware trojan that can be provisioned with the backdoor and p
&lt;/p&gt;</description></item></channel></rss>