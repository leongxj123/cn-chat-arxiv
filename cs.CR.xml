<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#21306;&#22359;&#38142;&#25216;&#26415;&#34987;&#25972;&#21512;&#21040;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#20197;&#25552;&#20379;&#26356;&#24378;&#30340;&#23433;&#20840;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#20294;&#20063;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#32593;&#32476;&#12289;&#35745;&#31639;&#21644;&#23384;&#20648;&#36164;&#28304;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.00873</link><description>&lt;p&gt;
&#21306;&#22359;&#38142;&#36171;&#33021;&#30340;&#32852;&#37030;&#23398;&#20064;: &#22909;&#22788;&#12289;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Blockchain-empowered Federated Learning: Benefits, Challenges, and Solutions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00873
&lt;/p&gt;
&lt;p&gt;
&#21306;&#22359;&#38142;&#25216;&#26415;&#34987;&#25972;&#21512;&#21040;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#20197;&#25552;&#20379;&#26356;&#24378;&#30340;&#23433;&#20840;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#20294;&#20063;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#32593;&#32476;&#12289;&#35745;&#31639;&#21644;&#23384;&#20648;&#36164;&#28304;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(FL)&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23458;&#25143;&#31471;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#24182;&#22312;&#21442;&#25968;&#26381;&#21153;&#22120;&#19978;&#36827;&#34892;&#32858;&#21512;&#26469;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#12290;&#23613;&#31649;&#22312;&#20445;&#25252;&#38544;&#31169;&#26041;&#38754;&#26377;&#25928;&#65292;&#20294;FL&#31995;&#32479;&#38754;&#20020;&#21333;&#28857;&#25925;&#38556;&#12289;&#32570;&#20047;&#28608;&#21169;&#21644;&#19981;&#36275;&#30340;&#23433;&#20840;&#24615;&#31561;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#23558;&#21306;&#22359;&#38142;&#25216;&#26415;&#25972;&#21512;&#21040;FL&#31995;&#32479;&#20013;&#65292;&#20197;&#25552;&#20379;&#26356;&#24378;&#30340;&#23433;&#20840;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#28982;&#32780;&#65292;&#21306;&#22359;&#38142;&#36171;&#33021;&#30340;FL(BC-FL)&#31995;&#32479;&#23545;&#32593;&#32476;&#12289;&#35745;&#31639;&#21644;&#23384;&#20648;&#36164;&#28304;&#25552;&#20986;&#20102;&#39069;&#22806;&#30340;&#38656;&#27714;&#12290;&#26412;&#35843;&#26597;&#20840;&#38754;&#23457;&#26597;&#20102;&#26368;&#36817;&#20851;&#20110;BC-FL&#31995;&#32479;&#30340;&#30740;&#31350;&#65292;&#20998;&#26512;&#20102;&#19982;&#21306;&#22359;&#38142;&#25972;&#21512;&#30456;&#20851;&#30340;&#22909;&#22788;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#21306;&#22359;&#38142;&#20026;&#20309;&#36866;&#29992;&#20110;FL&#65292;&#22914;&#20309;&#23454;&#26045;&#20197;&#21450;&#25972;&#21512;&#30340;&#25361;&#25112;&#21644;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00873v1 Announce Type: cross  Abstract: Federated learning (FL) is a distributed machine learning approach that protects user data privacy by training models locally on clients and aggregating them on a parameter server. While effective at preserving privacy, FL systems face limitations such as single points of failure, lack of incentives, and inadequate security. To address these challenges, blockchain technology is integrated into FL systems to provide stronger security, fairness, and scalability. However, blockchain-empowered FL (BC-FL) systems introduce additional demands on network, computing, and storage resources. This survey provides a comprehensive review of recent research on BC-FL systems, analyzing the benefits and challenges associated with blockchain integration. We explore why blockchain is applicable to FL, how it can be implemented, and the challenges and existing solutions for its integration. Additionally, we offer insights on future research directions fo
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#19978;&#19979;&#25991;&#23545;&#25239;&#28216;&#25103;(ICAG)&#38450;&#24481;&#36234;&#29425;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.13148</link><description>&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#23545;&#25239;&#28216;&#25103;&#38450;&#24481;&#36234;&#29425;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Defending Jailbreak Prompts via In-Context Adversarial Game
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13148
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#19978;&#19979;&#25991;&#23545;&#25239;&#28216;&#25103;(ICAG)&#38450;&#24481;&#36234;&#29425;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#29616;&#20986;&#22312;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#20013;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20854;&#23433;&#20840;&#24615;&#30340;&#25285;&#24551;&#65292;&#29305;&#21035;&#26159;&#23545;&#36234;&#29425;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#20173;&#28982;&#23384;&#22312;&#12290;&#21463;&#21040;&#28145;&#24230;&#23398;&#20064;&#20013;&#23545;&#25239;&#35757;&#32451;&#21644;LLM&#20195;&#29702;&#23398;&#20064;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19978;&#19979;&#25991;&#23545;&#25239;&#28216;&#25103;(ICAG)&#26469;&#38450;&#24481;&#36234;&#29425;&#25915;&#20987;&#65292;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#12290;ICAG&#21033;&#29992;&#20195;&#29702;&#23398;&#20064;&#36827;&#34892;&#23545;&#25239;&#28216;&#25103;&#65292;&#26088;&#22312;&#21160;&#24577;&#25193;&#23637;&#30693;&#35782;&#20197;&#38450;&#24481;&#36234;&#29425;&#25915;&#20987;&#12290;&#19982;&#20381;&#36182;&#38745;&#24577;&#25968;&#25454;&#38598;&#30340;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;ICAG&#37319;&#29992;&#36845;&#20195;&#36807;&#31243;&#26469;&#22686;&#24378;&#38450;&#24481;&#21644;&#25915;&#20987;&#20195;&#29702;&#12290;&#36825;&#19968;&#25345;&#32493;&#25913;&#36827;&#36807;&#31243;&#21152;&#24378;&#20102;&#23545;&#26032;&#29983;&#25104;&#30340;&#36234;&#29425;&#25552;&#31034;&#30340;&#38450;&#24481;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#35777;&#23454;&#20102;ICAG&#30340;&#26377;&#25928;&#24615;&#65292;&#32463;&#30001;ICAG&#20445;&#25252;&#30340;LLMs&#22312;&#21508;&#31181;&#25915;&#20987;&#22330;&#26223;&#20013;&#26174;&#33879;&#38477;&#20302;&#20102;&#36234;&#29425;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13148v1 Announce Type: new  Abstract: Large Language Models (LLMs) demonstrate remarkable capabilities across diverse applications. However, concerns regarding their security, particularly the vulnerability to jailbreak attacks, persist. Drawing inspiration from adversarial training in deep learning and LLM agent learning processes, we introduce the In-Context Adversarial Game (ICAG) for defending against jailbreaks without the need for fine-tuning. ICAG leverages agent learning to conduct an adversarial game, aiming to dynamically extend knowledge to defend against jailbreaks. Unlike traditional methods that rely on static datasets, ICAG employs an iterative process to enhance both the defense and attack agents. This continuous improvement process strengthens defenses against newly generated jailbreak prompts. Our empirical studies affirm ICAG's efficacy, where LLMs safeguarded by ICAG exhibit significantly reduced jailbreak success rates across various attack scenarios. Mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26412;&#22320;&#33258;&#36866;&#24212;&#23545;&#25239;&#39068;&#33394;&#25915;&#20987;&#65288;LAACA&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;&#33402;&#26415;&#21697;&#20813;&#21463;&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#65288;NST&#65289;&#30340;&#28389;&#29992;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#19981;&#21487;&#23519;&#35273;&#30340;&#24773;&#20917;&#19979;&#23545;&#22270;&#20687;&#36827;&#34892;&#20462;&#25913;&#65292;&#20135;&#29983;&#23545;NST&#20855;&#26377;&#24178;&#25200;&#20316;&#29992;&#30340;&#25200;&#21160;&#12290;</title><link>http://arxiv.org/abs/2401.09673</link><description>&lt;p&gt;
&#20351;&#29992;&#26412;&#22320;&#33258;&#36866;&#24212;&#23545;&#25239;&#39068;&#33394;&#25915;&#20987;&#23545;&#33402;&#26415;&#21697;&#36827;&#34892;&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#30340;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Artwork Protection Against Neural Style Transfer Using Locally Adaptive Adversarial Color Attack. (arXiv:2401.09673v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26412;&#22320;&#33258;&#36866;&#24212;&#23545;&#25239;&#39068;&#33394;&#25915;&#20987;&#65288;LAACA&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;&#33402;&#26415;&#21697;&#20813;&#21463;&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#65288;NST&#65289;&#30340;&#28389;&#29992;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#19981;&#21487;&#23519;&#35273;&#30340;&#24773;&#20917;&#19979;&#23545;&#22270;&#20687;&#36827;&#34892;&#20462;&#25913;&#65292;&#20135;&#29983;&#23545;NST&#20855;&#26377;&#24178;&#25200;&#20316;&#29992;&#30340;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#65288;NST&#65289;&#24191;&#27867;&#24212;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#20219;&#24847;&#39118;&#26684;&#30340;&#26032;&#22270;&#20687;&#12290;&#36825;&#20010;&#36807;&#31243;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23558;&#39118;&#26684;&#22270;&#20687;&#30340;&#32654;&#23398;&#20803;&#32032;&#19982;&#20869;&#23481;&#22270;&#20687;&#30340;&#32467;&#26500;&#22240;&#32032;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#24418;&#25104;&#19968;&#20010;&#21644;&#35856;&#25972;&#21512;&#30340;&#35270;&#35273;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#26410;&#32463;&#25480;&#26435;&#30340;NST&#21487;&#33021;&#20250;&#28389;&#29992;&#33402;&#26415;&#21697;&#12290;&#36825;&#31181;&#28389;&#29992;&#24341;&#36215;&#20102;&#20851;&#20110;&#33402;&#26415;&#23478;&#26435;&#21033;&#30340;&#31038;&#20250;&#25216;&#26415;&#38382;&#39064;&#65292;&#24182;&#20419;&#20351;&#24320;&#21457;&#25216;&#26415;&#26041;&#27861;&#26469;&#31215;&#26497;&#20445;&#25252;&#21407;&#22987;&#21019;&#20316;&#12290;&#23545;&#25239;&#24615;&#25915;&#20987;&#20027;&#35201;&#22312;&#26426;&#22120;&#23398;&#20064;&#23433;&#20840;&#20013;&#36827;&#34892;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#36825;&#19968;&#25216;&#26415;&#24341;&#20837;&#21040;&#20445;&#25252;&#33402;&#26415;&#23478;&#30693;&#35782;&#20135;&#26435;&#30340;&#39046;&#22495;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#26412;&#22320;&#33258;&#36866;&#24212;&#23545;&#25239;&#39068;&#33394;&#25915;&#20987;&#65288;LAACA&#65289;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20197;&#23545;&#20154;&#30524;&#19981;&#21487;&#23519;&#35273;&#20294;&#23545;NST&#20135;&#29983;&#24178;&#25200;&#30340;&#26041;&#24335;&#20462;&#25913;&#22270;&#20687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#38024;&#23545;&#39640;&#39057;&#20869;&#23481;&#20016;&#23500;&#21306;&#22495;&#30340;&#25200;&#21160;&#65292;&#36825;&#20123;&#25200;&#21160;&#30001;&#20013;&#38388;&#29305;&#24449;&#30340;&#30772;&#22351;&#20135;&#29983;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#21644;&#29992;&#25143;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural style transfer (NST) is widely adopted in computer vision to generate new images with arbitrary styles. This process leverages neural networks to merge aesthetic elements of a style image with the structural aspects of a content image into a harmoniously integrated visual result. However, unauthorized NST can exploit artwork. Such misuse raises socio-technical concerns regarding artists' rights and motivates the development of technical approaches for the proactive protection of original creations. Adversarial attack is a concept primarily explored in machine learning security. Our work introduces this technique to protect artists' intellectual property. In this paper Locally Adaptive Adversarial Color Attack (LAACA), a method for altering images in a manner imperceptible to the human eyes but disruptive to NST. Specifically, we design perturbations targeting image areas rich in high-frequency content, generated by disrupting intermediate features. Our experiments and user study
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;212&#20010;&#30495;&#23454;&#30340;&#24694;&#24847;&#26381;&#21153;&#65288;Malla&#65289;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#22320;&#19979;&#24066;&#22330;&#30340;&#25193;&#25955;&#21644;&#23545;&#20844;&#20849;LLM&#26381;&#21153;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#20854;&#20351;&#29992;&#30340;&#31574;&#30053;&#21644;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2401.03315</link><description>&lt;p&gt;
Malla: &#25581;&#31192;&#29616;&#23454;&#19990;&#30028;&#20013;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#24694;&#24847;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
Malla: Demystifying Real-world Large Language Model Integrated Malicious Services. (arXiv:2401.03315v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;212&#20010;&#30495;&#23454;&#30340;&#24694;&#24847;&#26381;&#21153;&#65288;Malla&#65289;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#22320;&#19979;&#24066;&#22330;&#30340;&#25193;&#25955;&#21644;&#23545;&#20844;&#20849;LLM&#26381;&#21153;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#20854;&#20351;&#29992;&#30340;&#31574;&#30053;&#21644;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22320;&#19979;&#21033;&#29992;&#65292;&#20063;&#31216;&#20026;Malla&#65292;&#27491;&#22312;&#22686;&#21152;&#65292;&#21152;&#21095;&#20102;&#32593;&#32476;&#23433;&#20840;&#23041;&#32961;&#65292;&#24182;&#23545;LLMs&#25216;&#26415;&#30340;&#21487;&#20449;&#24230;&#25552;&#20986;&#20102;&#30097;&#38382;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#24456;&#23569;&#26377;&#24037;&#20316;&#21162;&#21147;&#21435;&#20102;&#35299;&#36825;&#31181;&#26032;&#22411;&#32593;&#32476;&#29359;&#32618;&#30340;&#35268;&#27169;&#12289;&#24433;&#21709;&#21644;&#25216;&#26415;&#12290;&#26412;&#25991;&#26159;&#31532;&#19968;&#27425;&#23545;212&#20010;&#30495;&#23454;&#30340;Malla&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#22320;&#19979;&#24066;&#22330;&#30340;&#25193;&#25955;&#65292;&#24182;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#25805;&#20316;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#24320;&#20102;Malla&#29983;&#24577;&#31995;&#32479;&#65292;&#25581;&#31034;&#20102;&#20854;&#26174;&#33879;&#30340;&#22686;&#38271;&#23545;&#24403;&#20170;&#20844;&#20849;LLM&#26381;&#21153;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23545;212&#20010;Mallas&#36827;&#34892;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;8&#20010;&#21518;&#31471;LLMs&#65292;&#20197;&#21450;182&#20010;&#32469;&#36807;&#20844;&#20849;LLM API&#20445;&#25252;&#25514;&#26045;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;Mallas&#20351;&#29992;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324;&#28389;&#29992;&#26410;&#32463;&#23457;&#26597;&#30340;LLMs&#21644;&#36890;&#36807;&#36234;&#29425;&#25552;&#31034;&#21033;&#29992;&#20844;&#20849;LLM API&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;Malla&#29359;&#32618;&#34892;&#20026;&#30340;&#23454;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
The underground exploitation of large language models (LLMs) for malicious services (i.e., Malla) is witnessing an uptick, amplifying the cyber threat landscape and posing questions about the trustworthiness of LLM technologies. However, there has been little effort to understand this new cybercrime, in terms of its magnitude, impact, and techniques. In this paper, we conduct the first systematic study on 212 real-world Mallas, uncovering their proliferation in underground marketplaces and exposing their operational modalities. Our study discloses the Malla ecosystem, revealing its significant growth and impact on today's public LLM services. Through examining 212 Mallas, we uncovered eight backend LLMs used by Mallas, along with 182 prompts that circumvent the protective measures of public LLM APIs. We further demystify the tactics employed by Mallas, including the abuse of uncensored LLMs and the exploitation of public LLM APIs through jailbreak prompts. Our findings enable a better 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22635;&#20805;&#21644;&#32622;&#25442;&#25351;&#32441;&#32534;&#30721;&#30340;&#26041;&#27861;&#26469;&#20135;&#29983;&#22256;&#38590;&#23454;&#20363;&#65292;&#20174;&#32780;&#22312;&#21508;&#31181;&#24773;&#26223;&#19979;&#25552;&#20379;&#24179;&#28369;&#19979;&#30028;&#12290;&#36825;&#26041;&#27861;&#36866;&#29992;&#20110;&#24046;&#20998;&#38544;&#31169;&#24179;&#22343;&#38382;&#39064;&#21644;&#36817;&#20284;k.</title><link>http://arxiv.org/abs/2307.07604</link><description>&lt;p&gt;
&#36890;&#36807;&#22635;&#20805;&#21644;&#32622;&#25442;&#25351;&#32441;&#32534;&#30721;&#30340;&#26041;&#27861;&#65292;&#23545;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#25552;&#20379;&#24179;&#28369;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
Smooth Lower Bounds for Differentially Private Algorithms via Padding-and-Permuting Fingerprinting Codes. (arXiv:2307.07604v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22635;&#20805;&#21644;&#32622;&#25442;&#25351;&#32441;&#32534;&#30721;&#30340;&#26041;&#27861;&#26469;&#20135;&#29983;&#22256;&#38590;&#23454;&#20363;&#65292;&#20174;&#32780;&#22312;&#21508;&#31181;&#24773;&#26223;&#19979;&#25552;&#20379;&#24179;&#28369;&#19979;&#30028;&#12290;&#36825;&#26041;&#27861;&#36866;&#29992;&#20110;&#24046;&#20998;&#38544;&#31169;&#24179;&#22343;&#38382;&#39064;&#21644;&#36817;&#20284;k.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#32441;&#32534;&#30721;&#26041;&#27861;&#26159;&#26368;&#24191;&#27867;&#29992;&#20110;&#30830;&#23450;&#32422;&#26463;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#25110;&#38169;&#35823;&#29575;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#24046;&#20998;&#38544;&#31169;&#38382;&#39064;&#65292;&#25105;&#20204;&#24182;&#19981;&#30693;&#36947;&#36866;&#24403;&#30340;&#19979;&#30028;&#65292;&#24182;&#19988;&#21363;&#20351;&#23545;&#20110;&#25105;&#20204;&#30693;&#36947;&#30340;&#38382;&#39064;&#65292;&#19979;&#30028;&#20063;&#19981;&#24179;&#28369;&#65292;&#24182;&#19988;&#36890;&#24120;&#22312;&#35823;&#24046;&#22823;&#20110;&#26576;&#20010;&#38408;&#20540;&#26102;&#21464;&#24471;&#26080;&#24847;&#20041;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#22635;&#20805;&#21644;&#32622;&#25442;&#36716;&#25442;&#24212;&#29992;&#20110;&#25351;&#32441;&#32534;&#30721;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#22256;&#38590;&#23454;&#20363;&#30340;&#31616;&#21333;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#19981;&#21516;&#24773;&#26223;&#19979;&#25552;&#20379;&#26032;&#30340;&#19979;&#30028;&#26469;&#35828;&#26126;&#36825;&#31181;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65306;1. &#20302;&#20934;&#30830;&#24230;&#24773;&#26223;&#19979;&#24046;&#20998;&#38544;&#31169;&#24179;&#22343;&#38382;&#39064;&#30340;&#32039;&#23494;&#19979;&#30028;&#65292;&#36825;&#23588;&#20854;&#24847;&#21619;&#30528;&#26032;&#30340;&#31169;&#26377;1&#31751;&#38382;&#39064;&#30340;&#19979;&#30028; 2. &#36817;&#20284;k
&lt;/p&gt;
&lt;p&gt;
Fingerprinting arguments, first introduced by Bun, Ullman, and Vadhan (STOC 2014), are the most widely used method for establishing lower bounds on the sample complexity or error of approximately differentially private (DP) algorithms. Still, there are many problems in differential privacy for which we don't know suitable lower bounds, and even for problems that we do, the lower bounds are not smooth, and usually become vacuous when the error is larger than some threshold.  In this work, we present a simple method to generate hard instances by applying a padding-and-permuting transformation to a fingerprinting code. We illustrate the applicability of this method by providing new lower bounds in various settings:  1. A tight lower bound for DP averaging in the low-accuracy regime, which in particular implies a new lower bound for the private 1-cluster problem introduced by Nissim, Stemmer, and Vadhan (PODS 2016).  2. A lower bound on the additive error of DP algorithms for approximate k
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21487;&#35777;&#40657;&#30418;&#25915;&#20987;&#65292;&#33021;&#22815;&#20445;&#35777;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#35774;&#35745;&#20102;&#22810;&#31181;&#26032;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2304.04343</link><description>&lt;p&gt;
&#21487;&#35777;&#40657;&#30418;&#25915;&#20987;&#65306;&#30830;&#20445;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;
&lt;/p&gt;
&lt;p&gt;
Certifiable Black-Box Attack: Ensuring Provably Successful Attack for Adversarial Examples. (arXiv:2304.04343v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21487;&#35777;&#40657;&#30418;&#25915;&#20987;&#65292;&#33021;&#22815;&#20445;&#35777;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#35774;&#35745;&#20102;&#22810;&#31181;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;&#20855;&#26377;&#30772;&#22351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24378;&#22823;&#28508;&#21147;&#12290;&#29616;&#26377;&#30340;&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;&#36890;&#36807;&#36845;&#20195;&#26597;&#35810;&#30446;&#26631;&#27169;&#22411;&#21644;/&#25110;&#21033;&#29992;&#26412;&#22320;&#20195;&#29702;&#27169;&#22411;&#30340;&#21487;&#36716;&#31227;&#24615;&#26469;&#21046;&#20316;&#23545;&#25239;&#26679;&#26412;&#12290;&#24403;&#23454;&#39564;&#35774;&#35745;&#25915;&#20987;&#26102;&#65292;&#25915;&#20987;&#26159;&#21542;&#25104;&#21151;&#23545;&#25915;&#20987;&#32773;&#26469;&#35828;&#20173;&#28982;&#26159;&#26410;&#30693;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#20462;&#25913;&#38543;&#26426;&#24179;&#28369;&#24615;&#29702;&#35770;&#65292;&#39318;&#27425;&#30740;&#31350;&#20102;&#21487;&#35777;&#40657;&#30418;&#25915;&#20987;&#30340;&#26032;&#33539;&#20363;&#65292;&#33021;&#22815;&#20445;&#35777;&#21046;&#20316;&#30340;&#23545;&#25239;&#26679;&#26412;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#20026;&#27492;&#35774;&#35745;&#20102;&#22810;&#31181;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Black-box adversarial attacks have shown strong potential to subvert machine learning models. Existing black-box adversarial attacks craft the adversarial examples by iteratively querying the target model and/or leveraging the transferability of a local surrogate model. Whether such attack can succeed remains unknown to the adversary when empirically designing the attack. In this paper, to our best knowledge, we take the first step to study a new paradigm of adversarial attacks -- certifiable black-box attack that can guarantee the attack success rate of the crafted adversarial examples. Specifically, we revise the randomized smoothing to establish novel theories for ensuring the attack success rate of the adversarial examples. To craft the adversarial examples with the certifiable attack success rate (CASR) guarantee, we design several novel techniques, including a randomized query method to query the target model, an initialization method with smoothed self-supervised perturbation to
&lt;/p&gt;</description></item></channel></rss>