<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#20449;&#24687;&#27969;&#25511;&#21046;&#30340;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;MoE&#26550;&#26500;&#23454;&#29616;&#20102;&#35757;&#32451;&#25968;&#25454;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#25511;&#21046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#22312;&#25512;&#29702;&#26102;&#20165;&#22522;&#20110;&#35775;&#38382;&#31574;&#30053;&#21551;&#29992;&#23376;&#38598;&#30340;&#19987;&#23478;&#65292;&#23454;&#29616;&#20102;&#23545;&#23433;&#20840;&#35775;&#38382;&#25511;&#21046;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2306.03235</link><description>&lt;p&gt;
&#27169;&#22359;&#21270;&#27169;&#22411;&#26550;&#26500;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#20449;&#24687;&#27969;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Information Flow Control in Machine Learning through Modular Model Architecture. (arXiv:2306.03235v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#20449;&#24687;&#27969;&#25511;&#21046;&#30340;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;MoE&#26550;&#26500;&#23454;&#29616;&#20102;&#35757;&#32451;&#25968;&#25454;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#25511;&#21046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#22312;&#25512;&#29702;&#26102;&#20165;&#22522;&#20110;&#35775;&#38382;&#31574;&#30053;&#21551;&#29992;&#23376;&#38598;&#30340;&#19987;&#23478;&#65292;&#23454;&#29616;&#20102;&#23545;&#23433;&#20840;&#35775;&#38382;&#25511;&#21046;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#35757;&#32451;&#25968;&#25454;&#30340;&#20219;&#20309;&#37096;&#20998;&#37117;&#21487;&#20197;&#24433;&#21709;&#20854;&#36755;&#20986;&#12290;&#24403;&#35775;&#38382;&#25511;&#21046;&#21482;&#20801;&#35768;&#20010;&#20154;&#29992;&#25143;&#35775;&#38382;&#25968;&#25454;&#23376;&#38598;&#26102;&#65292;&#20174;&#35757;&#32451;&#25968;&#25454;&#21040;&#27169;&#22411;&#36755;&#20986;&#30340;&#20449;&#24687;&#27969;&#25511;&#21046;&#19981;&#36275;&#25104;&#20026;&#35757;&#32451;&#25935;&#24863;&#25968;&#25454;&#27169;&#22411;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#20026;&#20102;&#23454;&#29616;&#35775;&#38382;&#25511;&#21046;&#25968;&#25454;&#30340;&#23433;&#20840;&#26426;&#22120;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#20449;&#24687;&#27969;&#25511;&#21046;&#30340;&#27010;&#24565;&#65292;&#24182;&#22522;&#20110;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#26550;&#26500;&#24320;&#21457;&#20102;&#19968;&#20010;&#23433;&#20840;Transformer&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#38480;&#21046;&#26469;&#33258;&#27599;&#20010;&#23433;&#20840;&#39046;&#22495;&#30340;&#35757;&#32451;&#25968;&#25454;&#23545;&#21333;&#20010;&#19987;&#23478;&#27169;&#22359;&#30340;&#24433;&#21709;&#65292;&#24182;&#20165;&#22522;&#20110;&#35775;&#38382;&#25511;&#21046;&#31574;&#30053;&#22312;&#25512;&#29702;&#26102;&#21551;&#29992;&#19987;&#23478;&#30340;&#23376;&#38598;&#65292;&#23433;&#20840;MoE&#26550;&#26500;&#25511;&#21046;&#20102;&#20449;&#24687;&#27969;&#12290;&#20351;&#29992;&#22823;&#22411;&#25991;&#26412;&#25968;&#25454;&#35821;&#26009;&#24211;&#36827;&#34892;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;MoE&#26550;&#26500;&#20855;&#26377;&#26368;&#23567;&#30340;&#24615;&#33021;&#24320;&#38144;&#65288;1.9%&#65289;&#65292;&#24182;&#19988;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#65288;&#26368;&#39640;&#21487;&#36798;37%&#65289;&#65292;&#20174;&#32780;&#23454;&#29616;&#35757;&#32451;&#20934;&#30830;&#21644;&#23433;&#20840;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
In today's machine learning (ML) models, any part of the training data can affect its output. This lack of control for information flow from training data to model output is a major obstacle in training models on sensitive data when access control only allows individual users to access a subset of data. To enable secure machine learning for access controlled data, we propose the notion of information flow control for machine learning, and develop a secure Transformer-based language model based on the Mixture-of-Experts (MoE) architecture. The secure MoE architecture controls information flow by limiting the influence of training data from each security domain to a single expert module, and only enabling a subset of experts at inference time based on an access control policy. The evaluation using a large corpus of text data shows that the proposed MoE architecture has minimal (1.9%) performance overhead and can significantly improve model accuracy (up to 37%) by enabling training on acc
&lt;/p&gt;</description></item></channel></rss>