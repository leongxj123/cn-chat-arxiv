<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#23637;&#31034;&#20102;&#23545;&#40784;&#30340;LLM&#23545;&#31616;&#21333;&#33258;&#36866;&#24212;&#36234;&#29425;&#25915;&#20987;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#27169;&#22411;&#19978;&#20960;&#20046;100%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#36824;&#20171;&#32461;&#20102;&#23545;&#20110;&#19981;&#20844;&#24320;logprobs&#30340;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#36234;&#29425;&#20197;&#21450;&#22914;&#20309;&#22312;&#21463;&#27745;&#26579;&#30340;&#27169;&#22411;&#20013;&#26597;&#25214;&#26408;&#39532;&#23383;&#31526;&#20018;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.02151</link><description>&lt;p&gt;
&#29992;&#31616;&#21333;&#33258;&#36866;&#24212;&#25915;&#20987;&#36234;&#29425;&#21151;&#33021;&#23545;&#40784;&#30340;LLM
&lt;/p&gt;
&lt;p&gt;
Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02151
&lt;/p&gt;
&lt;p&gt;
&#23637;&#31034;&#20102;&#23545;&#40784;&#30340;LLM&#23545;&#31616;&#21333;&#33258;&#36866;&#24212;&#36234;&#29425;&#25915;&#20987;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#27169;&#22411;&#19978;&#20960;&#20046;100%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#36824;&#20171;&#32461;&#20102;&#23545;&#20110;&#19981;&#20844;&#24320;logprobs&#30340;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#36234;&#29425;&#20197;&#21450;&#22914;&#20309;&#22312;&#21463;&#27745;&#26579;&#30340;&#27169;&#22411;&#20013;&#26597;&#25214;&#26408;&#39532;&#23383;&#31526;&#20018;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#26159;&#26368;&#26032;&#30340;&#23433;&#20840;&#23545;&#40784;&#30340;LLM&#20063;&#19981;&#20855;&#26377;&#25269;&#25239;&#31616;&#21333;&#33258;&#36866;&#24212;&#36234;&#29425;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#25104;&#21151;&#21033;&#29992;&#23545;logprobs&#30340;&#35775;&#38382;&#36827;&#34892;&#36234;&#29425;&#65306;&#25105;&#20204;&#26368;&#21021;&#35774;&#35745;&#20102;&#19968;&#20010;&#23545;&#25239;&#24615;&#25552;&#31034;&#27169;&#26495;&#65288;&#26377;&#26102;&#20250;&#36866;&#24212;&#30446;&#26631;LLM&#65289;&#65292;&#28982;&#21518;&#25105;&#20204;&#22312;&#21518;&#32512;&#19978;&#24212;&#29992;&#38543;&#26426;&#25628;&#32034;&#20197;&#26368;&#22823;&#21270;&#30446;&#26631;logprob&#65288;&#20363;&#22914;token&#8220;Sure&#8221;&#65289;&#65292;&#21487;&#33021;&#20250;&#36827;&#34892;&#22810;&#27425;&#37325;&#21551;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;GPT-3.5/4&#12289;Llama-2-Chat-7B/13B/70B&#12289;Gemma-7B&#21644;&#38024;&#23545;GCG&#25915;&#20987;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#30340;HarmBench&#19978;&#30340;R2D2&#31561;&#20960;&#20046;100%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;--&#26681;&#25454;GPT-4&#30340;&#35780;&#21028;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#36716;&#31227;&#25110;&#39044;&#22635;&#20805;&#25915;&#20987;&#20197;100%&#30340;&#25104;&#21151;&#29575;&#23545;&#25152;&#26377;&#19981;&#26292;&#38706;logprobs&#30340;Claude&#27169;&#22411;&#36827;&#34892;&#36234;&#29425;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#21463;&#27745;&#26579;&#30340;&#27169;&#22411;&#20013;&#20351;&#29992;&#23545;&#19968;&#32452;&#21463;&#38480;&#21046;&#30340;token&#25191;&#34892;&#38543;&#26426;&#25628;&#32034;&#20197;&#26597;&#25214;&#26408;&#39532;&#23383;&#31526;&#20018;&#30340;&#26041;&#27861;--&#36825;&#39033;&#20219;&#21153;&#19982;&#35768;&#22810;&#20854;&#20182;&#20219;&#21153;&#20849;&#20139;&#30456;&#21516;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02151v1 Announce Type: cross  Abstract: We show that even the most recent safety-aligned LLMs are not robust to simple adaptive jailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobs for jailbreaking: we initially design an adversarial prompt template (sometimes adapted to the target LLM), and then we apply random search on a suffix to maximize the target logprob (e.g., of the token "Sure"), potentially with multiple restarts. In this way, we achieve nearly 100\% attack success rate -- according to GPT-4 as a judge -- on GPT-3.5/4, Llama-2-Chat-7B/13B/70B, Gemma-7B, and R2D2 from HarmBench that was adversarially trained against the GCG attack. We also show how to jailbreak all Claude models -- that do not expose logprobs -- via either a transfer or prefilling attack with 100\% success rate. In addition, we show how to use random search on a restricted set of tokens for finding trojan strings in poisoned models -- a task that shares many s
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25581;&#31034;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#39118;&#38505;&#65292;&#25351;&#20986;&#23545;&#25163;&#21487;&#20197;&#21033;&#29992;LMs&#30340;&#25351;&#31034;&#36981;&#24490;&#33021;&#21147;&#36731;&#26494;&#22320;&#20174;&#25968;&#25454;&#23384;&#20648;&#20013;&#30452;&#25509;&#25552;&#21462;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#35774;&#35745;&#20102;&#25915;&#20987;&#23545;&#29983;&#20135;RAG&#27169;&#22411;GPTs&#36896;&#25104;&#25968;&#25454;&#23384;&#20648;&#27844;&#28431;&#12290;</title><link>https://arxiv.org/abs/2402.17840</link><description>&lt;p&gt;
&#36981;&#24490;&#25105;&#30340;&#25351;&#31034;&#24182;&#35828;&#20986;&#30495;&#30456;&#65306;&#26469;&#33258;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#30340;&#21487;&#25193;&#23637;&#25968;&#25454;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17840
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25581;&#31034;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#39118;&#38505;&#65292;&#25351;&#20986;&#23545;&#25163;&#21487;&#20197;&#21033;&#29992;LMs&#30340;&#25351;&#31034;&#36981;&#24490;&#33021;&#21147;&#36731;&#26494;&#22320;&#20174;&#25968;&#25454;&#23384;&#20648;&#20013;&#30452;&#25509;&#25552;&#21462;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#35774;&#35745;&#20102;&#25915;&#20987;&#23545;&#29983;&#20135;RAG&#27169;&#22411;GPTs&#36896;&#25104;&#25968;&#25454;&#23384;&#20648;&#27844;&#28431;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#23558;&#22806;&#37096;&#30693;&#35782;&#32435;&#20837;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#23450;&#21046;&#36866;&#24212;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;Retrieval-In-Context RAG&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#39118;&#38505;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#23545;&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#30340;LMs&#26500;&#24314;&#30340;RAG&#31995;&#32479;&#36827;&#34892;&#25552;&#31034;&#27880;&#20837;&#26102;&#65292;&#23545;&#25163;&#21487;&#20197;&#21033;&#29992;LMs&#30340;&#25351;&#31034;&#36981;&#24490;&#33021;&#21147;&#36731;&#26494;&#22320;&#20174;&#25968;&#25454;&#23384;&#20648;&#20013;&#30452;&#25509;&#25552;&#21462;&#25991;&#26412;&#25968;&#25454;&#12290;&#36825;&#31181;&#28431;&#27934;&#23384;&#22312;&#20110;&#35206;&#30422;Llama2&#12289;Mistral/Mixtral&#12289;Vicuna&#12289;SOLAR&#12289;WizardLM&#12289;Qwen1.5&#21644;Platypus2&#31561;&#22810;&#31181;&#29616;&#20195;LMs&#30340;&#24191;&#27867;&#33539;&#22260;&#20869;&#65292;&#24182;&#19988;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#30340;&#25193;&#22823;&#65292;&#21033;&#29992;&#33021;&#21147;&#21152;&#21095;&#12290;&#23558;&#30740;&#31350;&#25193;&#23637;&#21040;&#29983;&#20135;RAG&#27169;&#22411;GPTs&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#25915;&#20987;&#65292;&#21487;&#20197;&#22312;&#23545;25&#20010;&#38543;&#26426;&#36873;&#25321;&#30340;&#23450;&#21046;GPTs&#26045;&#21152;&#26368;&#22810;2&#20010;&#26597;&#35810;&#26102;&#20197;100%&#25104;&#21151;&#29575;&#23548;&#33268;&#25968;&#25454;&#23384;&#20648;&#27844;&#28431;&#65292;&#24182;&#19988;&#25105;&#20204;&#33021;&#22815;&#20197;77,000&#23383;&#30340;&#20070;&#31821;&#20013;&#30340;&#25991;&#26412;&#25968;&#25454;&#30340;&#25552;&#21462;&#29575;&#20026;41%&#65292;&#20197;&#21450;&#22312;&#21547;&#26377;1,569,00&#35789;&#30340;&#35821;&#26009;&#24211;&#20013;&#30340;&#25991;&#26412;&#25968;&#25454;&#30340;&#25552;&#21462;&#29575;&#20026;3%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17840v1 Announce Type: cross  Abstract: Retrieval-Augmented Generation (RAG) improves pre-trained models by incorporating external knowledge at test time to enable customized adaptation. We study the risk of datastore leakage in Retrieval-In-Context RAG Language Models (LMs). We show that an adversary can exploit LMs' instruction-following capabilities to easily extract text data verbatim from the datastore of RAG systems built with instruction-tuned LMs via prompt injection. The vulnerability exists for a wide range of modern LMs that span Llama2, Mistral/Mixtral, Vicuna, SOLAR, WizardLM, Qwen1.5, and Platypus2, and the exploitability exacerbates as the model size scales up. Extending our study to production RAG models GPTs, we design an attack that can cause datastore leakage with a 100% success rate on 25 randomly selected customized GPTs with at most 2 queries, and we extract text data verbatim at a rate of 41% from a book of 77,000 words and 3% from a corpus of 1,569,00
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; SpongeNet &#30340;&#26032;&#22411;&#28023;&#32501;&#25915;&#20987;&#65292;&#36890;&#36807;&#30452;&#25509;&#20316;&#29992;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#65292;&#25104;&#21151;&#22686;&#21152;&#20102;&#35270;&#35273;&#27169;&#22411;&#30340;&#33021;&#32791;&#65292;&#32780;&#19988;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#37327;&#26356;&#23569;&#12290;</title><link>https://arxiv.org/abs/2402.06357</link><description>&lt;p&gt;
SpongeNet &#25915;&#20987;&#65306;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#28023;&#32501;&#26435;&#37325;&#20013;&#27602;
&lt;/p&gt;
&lt;p&gt;
The SpongeNet Attack: Sponge Weight Poisoning of Deep Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; SpongeNet &#30340;&#26032;&#22411;&#28023;&#32501;&#25915;&#20987;&#65292;&#36890;&#36807;&#30452;&#25509;&#20316;&#29992;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#65292;&#25104;&#21151;&#22686;&#21152;&#20102;&#35270;&#35273;&#27169;&#22411;&#30340;&#33021;&#32791;&#65292;&#32780;&#19988;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#37327;&#26356;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#32501;&#25915;&#20987;&#26088;&#22312;&#22686;&#21152;&#22312;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#37096;&#32626;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#32791;&#21644;&#35745;&#31639;&#26102;&#38388;&#12290;&#29616;&#26377;&#30340;&#28023;&#32501;&#25915;&#20987;&#21487;&#20197;&#36890;&#36807;&#28023;&#32501;&#31034;&#20363;&#36827;&#34892;&#25512;&#29702;&#65292;&#20063;&#21487;&#20197;&#36890;&#36807;&#28023;&#32501;&#20013;&#27602;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36827;&#34892;&#12290;&#28023;&#32501;&#31034;&#20363;&#21033;&#29992;&#28155;&#21152;&#21040;&#27169;&#22411;&#36755;&#20837;&#30340;&#25200;&#21160;&#26469;&#22686;&#21152;&#33021;&#37327;&#21644;&#24310;&#36831;&#65292;&#32780;&#28023;&#32501;&#20013;&#27602;&#21017;&#25913;&#21464;&#27169;&#22411;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#24341;&#21457;&#25512;&#29702;&#26102;&#30340;&#33021;&#37327;/&#24310;&#36831;&#25928;&#24212;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28023;&#32501;&#25915;&#20987;&#65292;&#31216;&#20026; SpongeNet&#12290;SpongeNet &#26159;&#31532;&#19968;&#20010;&#30452;&#25509;&#20316;&#29992;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#30340;&#28023;&#32501;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#28023;&#32501;&#20013;&#27602;&#65292;SpongeNet &#21487;&#20197;&#25104;&#21151;&#22686;&#21152;&#35270;&#35273;&#27169;&#22411;&#30340;&#33021;&#32791;&#65292;&#24182;&#19988;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#37327;&#26356;&#23569;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22914;&#26524;&#19981;&#19987;&#38376;&#38024;&#23545;&#28023;&#32501;&#20013;&#27602;&#36827;&#34892;&#35843;&#25972;&#65288;&#21363;&#20943;&#23567;&#25209;&#24402;&#19968;&#21270;&#20559;&#24046;&#20540;&#65289;&#65292;&#21017;&#27602;&#23475;&#38450;&#24481;&#20250;&#22833;&#25928;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26174;&#31034;&#20986;&#28023;&#32501;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sponge attacks aim to increase the energy consumption and computation time of neural networks deployed on hardware accelerators. Existing sponge attacks can be performed during inference via sponge examples or during training via Sponge Poisoning. Sponge examples leverage perturbations added to the model's input to increase energy and latency, while Sponge Poisoning alters the objective function of a model to induce inference-time energy/latency effects.   In this work, we propose a novel sponge attack called SpongeNet. SpongeNet is the first sponge attack that is performed directly on the parameters of a pre-trained model. Our experiments show that SpongeNet can successfully increase the energy consumption of vision models with fewer samples required than Sponge Poisoning. Our experiments indicate that poisoning defenses are ineffective if not adjusted specifically for the defense against Sponge Poisoning (i.e., they decrease batch normalization bias values). Our work shows that Spong
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545; GPT &#27169;&#22411;&#30340;&#23545;&#35805;&#37325;&#26500;&#25915;&#20987;&#65292;&#35813;&#25915;&#20987;&#20855;&#26377;&#21163;&#25345;&#20250;&#35805;&#21644;&#37325;&#26500;&#23545;&#35805;&#30340;&#20004;&#20010;&#27493;&#39588;&#12290;&#36890;&#36807;&#23545;&#35813;&#25915;&#20987;&#23545; GPT &#27169;&#22411;&#30340;&#38544;&#31169;&#39118;&#38505;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616; GPT-4 &#23545;&#35813;&#25915;&#20987;&#20855;&#26377;&#19968;&#23450;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02987</link><description>&lt;p&gt;
GPT &#27169;&#22411;&#30340;&#23545;&#35805;&#37325;&#26500;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Conversation Reconstruction Attack Against GPT Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545; GPT &#27169;&#22411;&#30340;&#23545;&#35805;&#37325;&#26500;&#25915;&#20987;&#65292;&#35813;&#25915;&#20987;&#20855;&#26377;&#21163;&#25345;&#20250;&#35805;&#21644;&#37325;&#26500;&#23545;&#35805;&#30340;&#20004;&#20010;&#27493;&#39588;&#12290;&#36890;&#36807;&#23545;&#35813;&#25915;&#20987;&#23545; GPT &#27169;&#22411;&#30340;&#38544;&#31169;&#39118;&#38505;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616; GPT-4 &#23545;&#35813;&#25915;&#20987;&#20855;&#26377;&#19968;&#23450;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#20854;&#20013; GPT &#31995;&#21015;&#27169;&#22411;&#20195;&#34920;&#30528;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#25104;&#26524;&#12290;&#20026;&#20102;&#20248;&#21270;&#20219;&#21153;&#25191;&#34892;&#65292;&#29992;&#25143;&#32463;&#24120;&#19982;&#25176;&#31649;&#22312;&#20113;&#29615;&#22659;&#20013;&#30340; GPT &#27169;&#22411;&#36827;&#34892;&#22810;&#36718;&#23545;&#35805;&#12290;&#36825;&#20123;&#22810;&#36718;&#23545;&#35805;&#24448;&#24448;&#21253;&#21547;&#31169;&#20154;&#20449;&#24687;&#65292;&#38656;&#35201;&#22312;&#20113;&#20013;&#36827;&#34892;&#20256;&#36755;&#21644;&#23384;&#20648;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25805;&#20316;&#27169;&#24335;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#25915;&#20987;&#38754;&#12290;&#26412;&#25991;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545; GPT &#27169;&#22411;&#30340;&#29305;&#23450;&#23545;&#35805;&#37325;&#26500;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#23545;&#35805;&#37325;&#26500;&#25915;&#20987;&#30001;&#20004;&#20010;&#27493;&#39588;&#32452;&#25104;&#65306;&#21163;&#25345;&#20250;&#35805;&#21644;&#37325;&#26500;&#23545;&#35805;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;&#24403; GPT &#27169;&#22411;&#36973;&#21463;&#35813;&#25915;&#20987;&#26102;&#23545;&#35805;&#20013;&#22266;&#26377;&#30340;&#38544;&#31169;&#39118;&#38505;&#36827;&#34892;&#20102;&#35814;&#23613;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;GPT-4 &#23545;&#20110;&#35813;&#25915;&#20987;&#20855;&#26377;&#19968;&#23450;&#30340;&#40065;&#26834;&#24615;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#39640;&#32423;&#25915;&#20987;&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#37325;&#26500;&#20197;&#21069;&#30340;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent times, significant advancements have been made in the field of large language models (LLMs), represented by GPT series models. To optimize task execution, users often engage in multi-round conversations with GPT models hosted in cloud environments. These multi-round conversations, potentially replete with private information, require transmission and storage within the cloud. However, this operational paradigm introduces additional attack surfaces. In this paper, we first introduce a specific Conversation Reconstruction Attack targeting GPT models. Our introduced Conversation Reconstruction Attack is composed of two steps: hijacking a session and reconstructing the conversations. Subsequently, we offer an exhaustive evaluation of the privacy risks inherent in conversations when GPT models are subjected to the proposed attack. However, GPT-4 demonstrates certain robustness to the proposed attacks. We then introduce two advanced attacks aimed at better reconstructing previous c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35753;ChatGPT&#21644;Bard&#20882;&#20805;&#22797;&#26434;&#20154;&#29289;&#35282;&#33394;&#65292;&#32469;&#36807;&#20102;&#23433;&#20840;&#26426;&#21046;&#21644;&#19987;&#38376;&#35757;&#32451;&#31243;&#24207;&#65292;&#23637;&#31034;&#20102;&#34987;&#31105;&#27490;&#30340;&#22238;&#24212;&#23454;&#38469;&#19978;&#34987;&#25552;&#20379;&#20102;&#65292;&#20174;&#32780;&#26377;&#21487;&#33021;&#33719;&#21462;&#26410;&#32463;&#25480;&#26435;&#12289;&#38750;&#27861;&#25110;&#26377;&#23475;&#30340;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2312.03853</link><description>&lt;p&gt;
LLMs&#30340;&#20004;&#38754;&#24615;&#65306;Jekyll&#21338;&#22763;&#19982;Hyde&#20808;&#29983;
&lt;/p&gt;
&lt;p&gt;
Dr. Jekyll and Mr. Hyde: Two Faces of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35753;ChatGPT&#21644;Bard&#20882;&#20805;&#22797;&#26434;&#20154;&#29289;&#35282;&#33394;&#65292;&#32469;&#36807;&#20102;&#23433;&#20840;&#26426;&#21046;&#21644;&#19987;&#38376;&#35757;&#32451;&#31243;&#24207;&#65292;&#23637;&#31034;&#20102;&#34987;&#31105;&#27490;&#30340;&#22238;&#24212;&#23454;&#38469;&#19978;&#34987;&#25552;&#20379;&#20102;&#65292;&#20174;&#32780;&#26377;&#21487;&#33021;&#33719;&#21462;&#26410;&#32463;&#25480;&#26435;&#12289;&#38750;&#27861;&#25110;&#26377;&#23475;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#20165;&#19968;&#24180;&#21069;&#65292;&#25105;&#20204;&#30446;&#30585;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20351;&#29992;&#22686;&#21152;&#65292;&#23588;&#20854;&#26159;&#22312;&#32467;&#21512;&#20687;&#32842;&#22825;&#26426;&#22120;&#20154;&#21161;&#25163;&#20043;&#31867;&#30340;&#24212;&#29992;&#26102;&#12290;&#20026;&#20102;&#38450;&#27490;&#36825;&#20123;&#21161;&#25163;&#20135;&#29983;&#19981;&#24403;&#22238;&#24212;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#23433;&#20840;&#26426;&#21046;&#21644;&#19987;&#38376;&#30340;&#35757;&#32451;&#31243;&#24207;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35753;ChatGPT&#21644;Bard&#65288;&#20197;&#21450;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#26159;Bing chat&#65289;&#20882;&#20805;&#22797;&#26434;&#20154;&#29289;&#35282;&#33394;&#65292;&#32469;&#36807;&#20102;&#36825;&#20123;&#25514;&#26045;&#65292;&#36825;&#20123;&#35282;&#33394;&#19982;&#23427;&#20204;&#26412;&#24212;&#25104;&#20026;&#30340;&#30495;&#23454;&#21161;&#25163;&#30340;&#29305;&#24449;&#30456;&#21453;&#12290;&#25105;&#20204;&#39318;&#20808;&#21019;&#36896;&#20986;&#36825;&#20123;&#20154;&#29289;&#35282;&#33394;&#30340;&#22797;&#26434;&#20256;&#35760;&#65292;&#28982;&#21518;&#22312;&#21516;&#19968;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#20351;&#29992;&#23427;&#20204;&#36827;&#34892;&#26032;&#30340;&#23545;&#35805;&#12290;&#25105;&#20204;&#30340;&#23545;&#35805;&#37319;&#29992;&#35282;&#33394;&#25198;&#28436;&#39118;&#26684;&#65292;&#20197;&#33719;&#24471;&#21161;&#25163;&#19981;&#34987;&#20801;&#35768;&#25552;&#20379;&#30340;&#22238;&#24212;&#12290;&#36890;&#36807;&#20351;&#29992;&#20154;&#29289;&#35282;&#33394;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#34987;&#31105;&#27490;&#30340;&#22238;&#24212;&#23454;&#38469;&#19978;&#34987;&#25552;&#20379;&#20102;&#65292;&#20174;&#32780;&#26377;&#21487;&#33021;&#33719;&#21462;&#26410;&#32463;&#25480;&#26435;&#12289;&#38750;&#27861;&#25110;&#26377;&#23475;&#30340;&#20449;&#24687;&#12290;&#36825;&#39033;&#24037;&#20316;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#23545;&#25239;&#24615;pe
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03853v2 Announce Type: replace-cross  Abstract: Only a year ago, we witnessed a rise in the use of Large Language Models (LLMs), especially when combined with applications like chatbot assistants. Safety mechanisms and specialized training procedures are implemented to prevent improper responses from these assistants. In this work, we bypass these measures for ChatGPT and Bard (and, to some extent, Bing chat) by making them impersonate complex personas with opposite characteristics as those of the truthful assistants they are supposed to be. We start by creating elaborate biographies of these personas, which we then use in a new session with the same chatbots. Our conversation followed a role-play style to get the response the assistant was not allowed to provide. By making use of personas, we show that the response that is prohibited is actually provided, making it possible to obtain unauthorized, illegal, or harmful information. This work shows that by using adversarial pe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#37319;&#29992;FHE&#21152;&#23494;&#25216;&#26415;&#65292;&#26082;&#21487;&#20197;&#20445;&#25252;&#27169;&#22411;&#26356;&#26032;&#30340;&#38544;&#31169;&#65292;&#21448;&#21487;&#20197;&#38450;&#27490;&#24694;&#24847;&#29992;&#25143;&#30772;&#22351;&#20840;&#23616;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.05112</link><description>&lt;p&gt;
FheFL&#65306;&#25903;&#25345;&#23436;&#20840;&#21516;&#24577;&#21152;&#23494;&#30340;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#19982;&#25308;&#21344;&#24237;&#29992;&#25143;
&lt;/p&gt;
&lt;p&gt;
FheFL: Fully Homomorphic Encryption Friendly Privacy-Preserving Federated Learning with Byzantine Users. (arXiv:2306.05112v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#37319;&#29992;FHE&#21152;&#23494;&#25216;&#26415;&#65292;&#26082;&#21487;&#20197;&#20445;&#25252;&#27169;&#22411;&#26356;&#26032;&#30340;&#38544;&#31169;&#65292;&#21448;&#21487;&#20197;&#38450;&#27490;&#24694;&#24847;&#29992;&#25143;&#30772;&#22351;&#20840;&#23616;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#25216;&#26415;&#26368;&#21021;&#26159;&#20026;&#20102;&#32531;&#35299;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#32780;&#24320;&#21457;&#30340;&#12290;&#23613;&#31649;FL&#30830;&#20445;&#29992;&#25143;&#30340;&#25968;&#25454;&#22987;&#32456;&#20445;&#30041;&#22312;&#29992;&#25143;&#25163;&#20013;&#65292;&#20294;&#23616;&#37096;&#35757;&#32451;&#27169;&#22411;&#30340;&#26799;&#24230;&#24517;&#39035;&#19982;&#38598;&#20013;&#24335;&#26381;&#21153;&#22120;&#36890;&#20449;&#20197;&#26500;&#24314;&#20840;&#23616;&#27169;&#22411;&#12290;&#36825;&#23548;&#33268;&#38544;&#31169;&#27844;&#38706;&#65292;&#20351;&#24471;&#26381;&#21153;&#22120;&#21487;&#20197;&#20174;&#20849;&#20139;&#30340;&#26799;&#24230;&#20013;&#25512;&#26029;&#20986;&#29992;&#25143;&#25968;&#25454;&#30340;&#31169;&#23494;&#20449;&#24687;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#32570;&#38519;&#65292;&#19979;&#19968;&#20195;FL&#26550;&#26500;&#25552;&#20986;&#20102;&#21152;&#23494;&#21644;&#21311;&#21517;&#21270;&#25216;&#26415;&#65292;&#20197;&#20445;&#25252;&#27169;&#22411;&#26356;&#26032;&#20813;&#21463;&#26381;&#21153;&#22120;&#30340;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#20250;&#24102;&#26469;&#20854;&#20182;&#25361;&#25112;&#65292;&#20363;&#22914;&#24694;&#24847;&#29992;&#25143;&#21487;&#33021;&#36890;&#36807;&#20849;&#20139;&#34394;&#20551;&#26799;&#24230;&#26469;&#30772;&#22351;&#20840;&#23616;&#27169;&#22411;&#12290;&#30001;&#20110;&#26799;&#24230;&#34987;&#21152;&#23494;&#65292;&#26381;&#21153;&#22120;&#26080;&#27861;&#35782;&#21035;&#21644;&#25490;&#38500;&#19981;&#33391;&#29992;&#25143;&#20197;&#20445;&#25252;&#20840;&#23616;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#32531;&#35299;&#36825;&#20004;&#31181;&#25915;&#20987;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23436;&#20840;&#21516;&#24577;&#21152;&#23494;&#65288;FHE&#65289;&#30340;&#26032;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The federated learning (FL) technique was initially developed to mitigate data privacy issues that can arise in the traditional machine learning paradigm. While FL ensures that a user's data always remain with the user, the gradients of the locally trained models must be communicated with the centralized server to build the global model. This results in privacy leakage, where the server can infer private information of the users' data from the shared gradients. To mitigate this flaw, the next-generation FL architectures proposed encryption and anonymization techniques to protect the model updates from the server. However, this approach creates other challenges, such as a malicious user might sabotage the global model by sharing false gradients. Since the gradients are encrypted, the server is unable to identify and eliminate rogue users which would protect the global model. Therefore, to mitigate both attacks, this paper proposes a novel fully homomorphic encryption (FHE) based scheme 
&lt;/p&gt;</description></item></channel></rss>