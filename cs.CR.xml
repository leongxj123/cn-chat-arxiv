<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26469;&#28304;&#22270;&#21644;Transformer&#30340;&#39640;&#32423;&#25345;&#20037;&#24615;&#23041;&#32961;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;Transformer&#30340;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#25552;&#21462;&#31995;&#32479;&#29366;&#24577;&#30340;&#38271;&#26399;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#26469;&#28304;&#20998;&#26512;&#23454;&#29616;&#23545;&#38271;&#26399;&#36816;&#34892;&#31995;&#32479;&#30340;&#27010;&#25324;&#65292;&#20197;&#26816;&#27979;&#32531;&#24930;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2304.02838</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#21644;&#26469;&#28304;&#22270;&#30340;&#39640;&#32423;&#25345;&#20037;&#24615;&#23041;&#32961;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TBDetector:Transformer-Based Detector for Advanced Persistent Threats with Provenance Graph. (arXiv:2304.02838v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26469;&#28304;&#22270;&#21644;Transformer&#30340;&#39640;&#32423;&#25345;&#20037;&#24615;&#23041;&#32961;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;Transformer&#30340;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#25552;&#21462;&#31995;&#32479;&#29366;&#24577;&#30340;&#38271;&#26399;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#26469;&#28304;&#20998;&#26512;&#23454;&#29616;&#23545;&#38271;&#26399;&#36816;&#34892;&#31995;&#32479;&#30340;&#27010;&#25324;&#65292;&#20197;&#26816;&#27979;&#32531;&#24930;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#39640;&#32423;&#25345;&#20037;&#24615;&#23041;&#32961;&#65288;APT&#65289;&#25915;&#20987;&#30340;&#38271;&#26399;&#28508;&#20239;&#12289;&#38544;&#31192;&#22810;&#38454;&#27573;&#25915;&#20987;&#27169;&#24335;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;APT&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#26469;&#28304;&#22270;&#25552;&#20379;&#30340;&#21382;&#21490;&#20449;&#24687;&#36827;&#34892;APT&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;Transformer&#30340;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#25552;&#21462;&#31995;&#32479;&#29366;&#24577;&#30340;&#38271;&#26399;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#26469;&#28304;&#20998;&#26512;&#23454;&#29616;&#23545;&#38271;&#26399;&#36816;&#34892;&#31995;&#32479;&#30340;&#27010;&#25324;&#65292;&#20197;&#26816;&#27979;&#32531;&#24930;&#25915;&#20987;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#24341;&#20837;&#20102;&#24322;&#24120;&#35780;&#20998;&#65292;&#21487;&#35780;&#20272;&#19981;&#21516;&#31995;&#32479;&#29366;&#24577;&#30340;&#24322;&#24120;&#24615;&#12290;&#27599;&#20010;&#29366;&#24577;&#37117;&#26377;&#30456;&#24212;&#30340;&#30456;&#20284;&#24230;&#21644;&#38548;&#31163;&#24230;&#20998;&#25968;&#30340;&#24322;&#24120;&#20998;&#25968;&#35745;&#31639;&#12290;&#20026;&#20102;&#35780;&#20272;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
APT detection is difficult to detect due to the long-term latency, covert and slow multistage attack patterns of Advanced Persistent Threat (APT). To tackle these issues, we propose TBDetector, a transformer-based advanced persistent threat detection method for APT attack detection. Considering that provenance graphs provide rich historical information and have the powerful attacks historic correlation ability to identify anomalous activities, TBDetector employs provenance analysis for APT detection, which summarizes long-running system execution with space efficiency and utilizes transformer with self-attention based encoder-decoder to extract long-term contextual features of system states to detect slow-acting attacks. Furthermore, we further introduce anomaly scores to investigate the anomaly of different system states, where each state is calculated with an anomaly score corresponding to its similarity score and isolation score. To evaluate the effectiveness of the proposed method,
&lt;/p&gt;</description></item></channel></rss>