<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#24418;&#24335;&#21270;&#8220;&#20462;&#27491;&#26426;&#22120;&#28040;&#38500;&#8221;&#26469;&#35299;&#20915;&#21463;&#26410;&#30693;&#25805;&#32437;&#24433;&#21709;&#30340;&#25968;&#25454;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#38382;&#39064;&#65292;&#21487;&#33021;&#20165;&#30693;&#36947;&#19968;&#37096;&#20998;&#21463;&#24433;&#21709;&#26679;&#26412;&#12290;&#21457;&#29616;&#32416;&#27491;&#28040;&#38500;&#38382;&#39064;&#19982;&#20256;&#32479;&#20197;&#38544;&#31169;&#20026;&#23548;&#21521;&#30340;&#28040;&#38500;&#26041;&#27861;&#26377;&#26174;&#33879;&#19981;&#21516;&#30340;&#35201;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.14015</link><description>&lt;p&gt;
&#20462;&#27491;&#26426;&#22120;&#28040;&#38500;
&lt;/p&gt;
&lt;p&gt;
Corrective Machine Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14015
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#24418;&#24335;&#21270;&#8220;&#20462;&#27491;&#26426;&#22120;&#28040;&#38500;&#8221;&#26469;&#35299;&#20915;&#21463;&#26410;&#30693;&#25805;&#32437;&#24433;&#21709;&#30340;&#25968;&#25454;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#38382;&#39064;&#65292;&#21487;&#33021;&#20165;&#30693;&#36947;&#19968;&#37096;&#20998;&#21463;&#24433;&#21709;&#26679;&#26412;&#12290;&#21457;&#29616;&#32416;&#27491;&#28040;&#38500;&#38382;&#39064;&#19982;&#20256;&#32479;&#20197;&#38544;&#31169;&#20026;&#23548;&#21521;&#30340;&#28040;&#38500;&#26041;&#27861;&#26377;&#26174;&#33879;&#19981;&#21516;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36234;&#26469;&#36234;&#38754;&#20020;&#25968;&#25454;&#23436;&#25972;&#24615;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#20351;&#29992;&#20102;&#22823;&#35268;&#27169;&#30340;&#20174;&#20114;&#32852;&#32593;&#20013;&#33719;&#21462;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#26524;&#27169;&#22411;&#24320;&#21457;&#32773;&#21457;&#29616;&#26576;&#20123;&#25968;&#25454;&#34987;&#31713;&#25913;&#25110;&#38169;&#35823;&#65292;&#20182;&#20204;&#21487;&#20197;&#37319;&#21462;&#20160;&#20040;&#25514;&#26045;&#12290;&#36825;&#20123;&#34987;&#31713;&#25913;&#30340;&#25968;&#25454;&#20250;&#23548;&#33268;&#19981;&#21033;&#24433;&#21709;&#65292;&#22914;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#26679;&#26412;&#30340;&#25915;&#20987;&#12289;&#31995;&#32479;&#24615;&#20559;&#35265;&#65292;&#20197;&#21450;&#22312;&#26576;&#20123;&#36755;&#20837;&#39046;&#22495;&#30340;&#20934;&#30830;&#24230;&#38477;&#20302;&#12290;&#36890;&#24120;&#65292;&#24182;&#38750;&#25152;&#26377;&#34987;&#31713;&#25913;&#30340;&#35757;&#32451;&#26679;&#26412;&#37117;&#26159;&#24050;&#30693;&#30340;&#65292;&#32780;&#21482;&#26377;&#19968;&#23567;&#37096;&#20998;&#20195;&#34920;&#24615;&#30340;&#21463;&#24433;&#21709;&#25968;&#25454;&#34987;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14015v1 Announce Type: cross  Abstract: Machine Learning models increasingly face data integrity challenges due to the use of large-scale training datasets drawn from the internet. We study what model developers can do if they detect that some data was manipulated or incorrect. Such manipulated data can cause adverse effects like vulnerability to backdoored samples, systematic biases, and in general, reduced accuracy on certain input domains. Often, all manipulated training samples are not known, and only a small, representative subset of the affected data is flagged.   We formalize "Corrective Machine Unlearning" as the problem of mitigating the impact of data affected by unknown manipulations on a trained model, possibly knowing only a subset of impacted samples. We demonstrate that the problem of corrective unlearning has significantly different requirements from traditional privacy-oriented unlearning. We find most existing unlearning methods, including the gold-standard
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20250;&#21592;&#25512;&#26029;&#23041;&#32961;&#27169;&#22411;TMI&#65292;&#29992;&#20110;&#35780;&#20272;&#24494;&#35843;&#27169;&#22411;&#23545;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#27844;&#38706;&#65292;&#31361;&#26174;&#20102;&#22312;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#24182;&#38656;&#35201;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#36827;&#34892;&#26356;&#20005;&#26684;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.01181</link><description>&lt;p&gt;
&#36807;&#25311;&#21512;&#30340;&#27169;&#22411;&#20250;&#27844;&#38706;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#38544;&#31169;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
TMI! Finetuned Models Leak Private Information from their Pretraining Data. (arXiv:2306.01181v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20250;&#21592;&#25512;&#26029;&#23041;&#32961;&#27169;&#22411;TMI&#65292;&#29992;&#20110;&#35780;&#20272;&#24494;&#35843;&#27169;&#22411;&#23545;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#27844;&#38706;&#65292;&#31361;&#26174;&#20102;&#22312;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#24182;&#38656;&#35201;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#36827;&#34892;&#26356;&#20005;&#26684;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#36234;&#26469;&#36234;&#27969;&#34892;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#21033;&#29992;&#20026;&#19968;&#20010;&#20219;&#21153;&#35757;&#32451;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#21327;&#21161;&#26500;&#24314;&#30456;&#20851;&#20219;&#21153;&#30340;&#24494;&#35843;&#27169;&#22411;&#12290;&#35813;&#33539;&#20363;&#22312;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#26041;&#38754;&#23588;&#20854;&#21463;&#27426;&#36814;&#65292;&#20854;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#20844;&#24320;&#30340;&#65292;&#21482;&#26377;&#24494;&#35843;&#25968;&#25454;&#34987;&#35270;&#20026;&#25935;&#24863;&#30340;&#12290;&#28982;&#32780;&#65292;&#26377;&#29702;&#30001;&#35748;&#20026;&#29992;&#20110;&#39044;&#35757;&#32451;&#30340;&#25968;&#25454;&#20173;&#28982;&#26159;&#25935;&#24863;&#30340;&#65292;&#22240;&#27492;&#24517;&#39035;&#20102;&#35299;&#24494;&#35843;&#27169;&#22411;&#27844;&#38706;&#26377;&#20851;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#20449;&#24687;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20250;&#21592;&#25512;&#29702;&#23041;&#32961;&#27169;&#22411;&#65292;&#20854;&#20013;&#23545;&#25163;&#21482;&#33021;&#35775;&#38382;&#24050;&#32463;&#24494;&#35843;&#22909;&#30340;&#27169;&#22411;&#65292;&#24182;&#24819;&#25512;&#26029;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#21592;&#36164;&#26684;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#23041;&#32961;&#27169;&#22411;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#20803;&#20998;&#31867;&#22120;&#30340;&#25915;&#20987;TMI&#65292;&#23427;&#21033;&#29992;&#20102;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#35760;&#24518;&#30340;&#39044;&#35757;&#32451;&#26679;&#26412;&#23545;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;TMI&#65292;&#24182;&#34920;&#26126;&#23427;&#22312;&#20165;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#30340;&#25512;&#26029;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#21592;&#36164;&#26684;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#26174;&#20102;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#20197;&#21450;&#38656;&#35201;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#36827;&#34892;&#26356;&#20005;&#26684;&#30340;&#35780;&#20272;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning has become an increasingly popular technique in machine learning as a way to leverage a pretrained model trained for one task to assist with building a finetuned model for a related task. This paradigm has been especially popular for privacy in machine learning, where the pretrained model is considered public, and only the data for finetuning is considered sensitive. However, there are reasons to believe that the data used for pretraining is still sensitive, making it essential to understand how much information the finetuned model leaks about the pretraining data. In this work we propose a new membership-inference threat model where the adversary only has access to the finetuned model and would like to infer the membership of the pretraining data. To realize this threat model, we implement a novel metaclassifier-based attack, TMI, that leverages the influence of memorized pretraining samples on predictions in the downstream task. We evaluate TMI on both vision and na
&lt;/p&gt;</description></item></channel></rss>