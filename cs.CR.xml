<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20027;&#39064;&#30340;&#27700;&#21360;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;&#27700;&#21360;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#20026;&#21306;&#20998;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2404.02138</link><description>&lt;p&gt;
&#22522;&#20110;&#20027;&#39064;&#30340;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
Topic-based Watermarks for LLM-Generated Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02138
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20027;&#39064;&#30340;&#27700;&#21360;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;&#27700;&#21360;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#20026;&#21306;&#20998;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#23548;&#33268;&#20102;&#29983;&#25104;&#30340;&#25991;&#26412;&#36755;&#20986;&#19982;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#30456;&#20284;&#24230;&#38590;&#20197;&#20998;&#36776;&#12290;&#27700;&#21360;&#31639;&#27861;&#26159;&#28508;&#22312;&#24037;&#20855;&#65292;&#36890;&#36807;&#22312;LLM&#29983;&#25104;&#30340;&#36755;&#20986;&#20013;&#23884;&#20837;&#21487;&#26816;&#27979;&#30340;&#31614;&#21517;&#65292;&#21487;&#20197;&#21306;&#20998;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27700;&#21360;&#26041;&#26696;&#22312;&#24050;&#30693;&#25915;&#20987;&#19979;&#32570;&#20047;&#20581;&#22766;&#24615;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;LLM&#27599;&#22825;&#29983;&#25104;&#25968;&#19975;&#20010;&#25991;&#26412;&#36755;&#20986;&#65292;&#27700;&#21360;&#31639;&#27861;&#38656;&#35201;&#35760;&#24518;&#27599;&#20010;&#36755;&#20986;&#25165;&#33021;&#35753;&#26816;&#27979;&#27491;&#24120;&#24037;&#20316;&#65292;&#36825;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26412;&#25991;&#38024;&#23545;&#24403;&#21069;&#27700;&#21360;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;LLMs&#30340;&#8220;&#22522;&#20110;&#20027;&#39064;&#30340;&#27700;&#21360;&#31639;&#27861;&#8221;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02138v1 Announce Type: cross  Abstract: Recent advancements of large language models (LLMs) have resulted in indistinguishable text outputs comparable to human-generated text. Watermarking algorithms are potential tools that offer a way to differentiate between LLM- and human-generated text by embedding detectable signatures within LLM-generated output. However, current watermarking schemes lack robustness against known attacks against watermarking algorithms. In addition, they are impractical considering an LLM generates tens of thousands of text outputs per day and the watermarking algorithm needs to memorize each output it generates for the detection to work. In this work, focusing on the limitations of current watermarking schemes, we propose the concept of a "topic-based watermarking algorithm" for LLMs. The proposed algorithm determines how to generate tokens for the watermarked LLM output based on extracted topics of an input prompt or the output of a non-watermarked 
&lt;/p&gt;</description></item><item><title>&#21338;&#24328;&#35770;&#27169;&#22411;&#21644;&#22522;&#30784;&#27169;&#22411;&#22312;&#20998;&#26512;&#12289;&#35774;&#35745;&#21644;&#23454;&#26045;&#32593;&#32476;&#27450;&#39575;&#31574;&#30053;&#20013;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#65292;&#20026;&#25552;&#21319;&#20027;&#21160;&#21644;&#33258;&#21160;&#21270;&#32593;&#32476;&#38450;&#24481;&#26426;&#21046;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2403.10570</link><description>&lt;p&gt;
&#25112;&#30053;&#32593;&#32476;&#25112;&#20013;&#30340;&#29983;&#29289;&#20849;&#29983;&#28216;&#25103;&#21644;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Symbiotic Game and Foundation Models for Cyber Deception Operations in Strategic Cyber Warfare
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10570
&lt;/p&gt;
&lt;p&gt;
&#21338;&#24328;&#35770;&#27169;&#22411;&#21644;&#22522;&#30784;&#27169;&#22411;&#22312;&#20998;&#26512;&#12289;&#35774;&#35745;&#21644;&#23454;&#26045;&#32593;&#32476;&#27450;&#39575;&#31574;&#30053;&#20013;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#65292;&#20026;&#25552;&#21319;&#20027;&#21160;&#21644;&#33258;&#21160;&#21270;&#32593;&#32476;&#38450;&#24481;&#26426;&#21046;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#20020;&#30528;&#32593;&#32476;&#25112;&#25112;&#26415;&#30340;&#24555;&#36895;&#28436;&#21464;&#12289;&#24773;&#25253;&#19981;&#23545;&#31216;&#24615;&#22686;&#21152;&#21644;&#40657;&#23458;&#24037;&#20855;&#30340;&#26085;&#30410;&#26131;&#24471;&#65292;&#25105;&#20204;&#27491;&#38754;&#20020;&#21069;&#25152;&#26410;&#26377;&#30340;&#32593;&#32476;&#25112;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#32593;&#32476;&#27450;&#39575;&#20316;&#20026;&#25105;&#20204;&#38450;&#24481;&#31574;&#30053;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#23853;&#38706;&#22836;&#35282;&#65292;&#26088;&#22312;&#24212;&#23545;&#26085;&#30410;&#22797;&#26434;&#30340;&#25915;&#20987;&#12290;&#26412;&#31456;&#26088;&#22312;&#24378;&#35843;&#21338;&#24328;&#35770;&#27169;&#22411;&#21644;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#22312;&#20998;&#26512;&#12289;&#35774;&#35745;&#21644;&#23454;&#26045;&#32593;&#32476;&#27450;&#39575;&#31574;&#30053;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#21338;&#24328;&#27169;&#22411;&#65288;GMs&#65289;&#20316;&#20026;&#19968;&#20010;&#22522;&#30784;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#27169;&#22810;&#26679;&#30340;&#23545;&#25239;&#24615;&#20132;&#20114;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#21253;&#23481;&#23545;&#25239;&#24615;&#30693;&#35782;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#35265;&#35299;&#12290;&#21516;&#26102;&#65292;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#21019;&#24314;&#38024;&#23545;&#29305;&#23450;&#24212;&#29992;&#30340;&#23450;&#21046;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26500;&#24314;&#22359;&#12290;&#36890;&#36807;&#21033;&#29992;&#21338;&#24328;&#27169;&#22411;&#21644;&#22522;&#30784;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#21516;&#25928;&#24212;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#19981;&#20165;&#20445;&#25252;&#25105;&#20204;&#30340;&#32593;&#32476;&#20813;&#21463;&#25915;&#20987;&#65292;&#32780;&#19988;&#25552;&#39640;&#20027;&#21160;&#21644;&#33258;&#21160;&#21270;&#32593;&#32476;&#38450;&#24481;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10570v1 Announce Type: cross  Abstract: We are currently facing unprecedented cyber warfare with the rapid evolution of tactics, increasing asymmetry of intelligence, and the growing accessibility of hacking tools. In this landscape, cyber deception emerges as a critical component of our defense strategy against increasingly sophisticated attacks. This chapter aims to highlight the pivotal role of game-theoretic models and foundation models (FMs) in analyzing, designing, and implementing cyber deception tactics. Game models (GMs) serve as a foundational framework for modeling diverse adversarial interactions, allowing us to encapsulate both adversarial knowledge and domain-specific insights. Meanwhile, FMs serve as the building blocks for creating tailored machine learning models suited to given applications. By leveraging the synergy between GMs and FMs, we can advance proactive and automated cyber defense mechanisms by not only securing our networks against attacks but als
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25968;&#25454;&#27700;&#21360;&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#26816;&#27979;&#29256;&#26435;&#25345;&#26377;&#20154;&#20316;&#21697;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36827;&#34892;&#21512;&#29702;&#26816;&#27979;&#19988;&#25552;&#20379;&#35823;&#26816;&#29575;&#20445;&#35777;&#65292;&#30740;&#31350;&#20102;&#27700;&#21360;&#35774;&#35745;&#23545;&#20551;&#35774;&#26816;&#39564;&#33021;&#21147;&#30340;&#24433;&#21709;&#20197;&#21450;&#22312;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#32553;&#25918;&#19979;&#30340;&#26816;&#27979;&#24378;&#24230;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.10892</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#27700;&#21360;&#35777;&#26126;LLM&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#21592;&#36164;&#26684;
&lt;/p&gt;
&lt;p&gt;
Proving membership in LLM pretraining data via data watermarks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10892
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25968;&#25454;&#27700;&#21360;&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#26816;&#27979;&#29256;&#26435;&#25345;&#26377;&#20154;&#20316;&#21697;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36827;&#34892;&#21512;&#29702;&#26816;&#27979;&#19988;&#25552;&#20379;&#35823;&#26816;&#29575;&#20445;&#35777;&#65292;&#30740;&#31350;&#20102;&#27700;&#21360;&#35774;&#35745;&#23545;&#20551;&#35774;&#26816;&#39564;&#33021;&#21147;&#30340;&#24433;&#21709;&#20197;&#21450;&#22312;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#32553;&#25918;&#19979;&#30340;&#26816;&#27979;&#24378;&#24230;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#29256;&#26435;&#25345;&#26377;&#20154;&#30340;&#20316;&#21697;&#26159;&#21542;&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#20351;&#29992;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#25968;&#25454;&#27700;&#21360;&#23454;&#29616;&#22522;&#20110;&#40657;&#30418;&#27169;&#22411;&#35775;&#38382;&#30340;&#21512;&#29702;&#26816;&#27979;&#65292;&#21069;&#25552;&#26159;&#29256;&#26435;&#25345;&#26377;&#20154;&#22312;&#20844;&#24320;&#21457;&#24067;&#20043;&#21069;&#36129;&#29486;&#20102;&#22810;&#20010;&#35757;&#32451;&#25991;&#26723;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#27700;&#21360;&#22788;&#29702;&#12290;&#36890;&#36807;&#24212;&#29992;&#38543;&#26426;&#37319;&#26679;&#30340;&#25968;&#25454;&#27700;&#21360;&#65292;&#26816;&#27979;&#21487;&#20197;&#34987;&#26500;&#36896;&#20026;&#20551;&#35774;&#26816;&#39564;&#65292;&#20174;&#32780;&#25552;&#20379;&#23545;&#35823;&#26816;&#29575;&#30340;&#20445;&#35777;&#12290;&#30740;&#31350;&#20102;&#20004;&#31181;&#27700;&#21360;&#65306;&#19968;&#31181;&#25554;&#20837;&#38543;&#26426;&#24207;&#21015;&#65292;&#21478;&#19968;&#31181;&#38543;&#26426;&#29992;Unicode&#31867;&#20284;&#23383;&#31526;&#26367;&#25442;&#23383;&#31526;&#12290;&#39318;&#20808;&#23637;&#31034;&#20102;&#27700;&#21360;&#35774;&#35745;&#30340;&#19977;&#20010;&#26041;&#38754;--&#27700;&#21360;&#38271;&#24230;&#12289;&#22797;&#21046;&#27425;&#25968;&#21644;&#24178;&#25200;--&#22914;&#20309;&#24433;&#21709;&#20551;&#35774;&#26816;&#39564;&#30340;&#33021;&#21147;&#12290;&#25509;&#30528;&#30740;&#31350;&#20102;&#27700;&#21360;&#22312;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#32553;&#25918;&#19979;&#30340;&#26816;&#27979;&#24378;&#24230;&#22914;&#20309;&#21464;&#21270;&#65306;&#22686;&#21152;&#25968;&#25454;&#38598;&#22823;&#23567;&#20250;&#38477;&#20302;&#27700;&#21360;&#30340;&#24378;&#24230;&#65292;&#27700;&#21360;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10892v1 Announce Type: cross  Abstract: Detecting whether copyright holders' works were used in LLM pretraining is poised to be an important problem. This work proposes using data watermarks to enable principled detection with only black-box model access, provided that the rightholder contributed multiple training documents and watermarked them before public release. By applying a randomly sampled data watermark, detection can be framed as hypothesis testing, which provides guarantees on the false detection rate. We study two watermarks: one that inserts random sequences, and another that randomly substitutes characters with Unicode lookalikes. We first show how three aspects of watermark design -- watermark length, number of duplications, and interference -- affect the power of the hypothesis test. Next, we study how a watermark's detection strength changes under model and dataset scaling: while increasing the dataset size decreases the strength of the watermark, watermarks
&lt;/p&gt;</description></item><item><title>Bergeron&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33391;&#30693;&#30340;&#23545;&#40774;&#26694;&#26550;&#65292;&#33021;&#22815;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#26080;&#38656;&#39069;&#22806;&#21442;&#25968;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2312.00029</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#33391;&#30693;&#30340;&#23545;&#20934;&#26694;&#26550;&#25269;&#24481;&#23545;&#25239;&#24615;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Bergeron: Combating Adversarial Attacks through a Conscience-Based Alignment Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00029
&lt;/p&gt;
&lt;p&gt;
Bergeron&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33391;&#30693;&#30340;&#23545;&#40774;&#26694;&#26550;&#65292;&#33021;&#22815;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#26080;&#38656;&#39069;&#22806;&#21442;&#25968;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#36234;&#26469;&#36234;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24341;&#20837;&#65292;&#20154;&#24037;&#26234;&#33021;&#23545;&#40784;&#30340;&#30740;&#31350;&#21462;&#24471;&#20102;&#21487;&#35266;&#30340;&#36827;&#23637;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#20195;&#23545;&#40784;&#26041;&#27861;&#20173;&#28982;&#26080;&#27861;&#23436;&#20840;&#38450;&#27490;&#22312;&#27169;&#22411;&#34987;&#33988;&#24847;&#25915;&#20987;&#26102;&#20135;&#29983;&#26377;&#23475;&#24212;&#23545;&#12290;&#20026;&#20102;&#24110;&#21161;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Bergeron&#65306;&#19968;&#20010;&#26088;&#22312;&#25552;&#39640;LLMs&#23545;&#25239;&#25915;&#20987;&#40065;&#26834;&#24615;&#30340;&#26694;&#26550;&#65292;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#21442;&#25968;&#24494;&#35843;&#12290;Bergeron&#20998;&#20026;&#20004;&#20010;&#23618;&#27425;&#65307;&#27425;&#35201;LLM&#27169;&#25311;&#21463;&#20445;&#25252;&#30340;&#20027;&#35201;LLM&#30340;&#33391;&#30693;&#12290;&#35813;&#26694;&#26550;&#22312;&#30417;&#35270;&#36755;&#20986;&#20197;&#26816;&#27979;&#20219;&#20309;&#26377;&#23475;&#20869;&#23481;&#30340;&#21516;&#26102;&#65292;&#26356;&#22909;&#22320;&#20445;&#25252;&#20027;&#35201;&#27169;&#22411;&#20813;&#21463;&#20837;&#20405;&#25915;&#20987;&#12290;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#20351;&#29992;Bergeron&#26469;&#34917;&#20805;&#29616;&#26377;&#23545;&#40784;&#35757;&#32451;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00029v2 Announce Type: replace-cross  Abstract: Research into AI alignment has grown considerably since the recent introduction of increasingly capable Large Language Models (LLMs). Unfortunately, modern methods of alignment still fail to fully prevent harmful responses when models are deliberately attacked. These attacks can trick seemingly aligned models into giving manufacturing instructions for dangerous materials, inciting violence, or recommending other immoral acts. To help mitigate this issue, we introduce Bergeron: a framework designed to improve the robustness of LLMs against attacks without any additional parameter fine-tuning. Bergeron is organized into two tiers; with a secondary LLM emulating the conscience of a protected, primary LLM. This framework better safeguards the primary model against incoming attacks while monitoring its output for any harmful content. Empirical analysis shows that, by using Bergeron to complement models with existing alignment traini
&lt;/p&gt;</description></item></channel></rss>