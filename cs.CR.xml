<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#38543;&#26426;&#24179;&#28369;&#35748;&#35777;&#26694;&#26550;&#65292;&#20026;&#25104;&#26412;&#25935;&#24863;&#30340;&#31283;&#20581;&#20998;&#31867;&#22120;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#31283;&#20581;&#24615;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#26041;&#26696;&#38024;&#23545;&#19981;&#21516;&#25968;&#25454;&#23376;&#32452;&#35774;&#35745;&#20102;&#32454;&#31890;&#24230;&#35748;&#35777;&#21322;&#24452;&#65292;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.08732</link><description>&lt;p&gt;
&#21487;&#35777;&#20445;&#20581;&#24247;&#21830;&#21153;&#23383;&#20307;&#23609;&#21253;&#36890;&#36807;&#38543;&#26426;&#24179;&#28369;(&#35793;&#27880;)&#27700;&#12290;
&lt;/p&gt;
&lt;p&gt;
Provably Robust Cost-Sensitive Learning via Randomized Smoothing. (arXiv:2310.08732v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#38543;&#26426;&#24179;&#28369;&#35748;&#35777;&#26694;&#26550;&#65292;&#20026;&#25104;&#26412;&#25935;&#24863;&#30340;&#31283;&#20581;&#20998;&#31867;&#22120;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#31283;&#20581;&#24615;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#26041;&#26696;&#38024;&#23545;&#19981;&#21516;&#25968;&#25454;&#23376;&#32452;&#35774;&#35745;&#20102;&#32454;&#31890;&#24230;&#35748;&#35777;&#21322;&#24452;&#65292;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20851;&#27880;&#20110;&#22312;&#25104;&#26412;&#25935;&#24863;&#30340;&#24773;&#26223;&#19979;&#23398;&#20064;&#23545;&#25239;&#24615;&#31283;&#20581;&#20998;&#31867;&#22120;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#19981;&#21516;&#31867;&#21035;&#30340;&#23545;&#25239;&#24615;&#21464;&#25442;&#30340;&#28508;&#22312;&#21361;&#23475;&#34987;&#32534;&#30721;&#22312;&#19968;&#20010;&#20108;&#36827;&#21046;&#25104;&#26412;&#30697;&#38453;&#20013;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#26159;&#32463;&#39564;&#24615;&#30340;&#65292;&#26080;&#27861;&#35777;&#26126;&#31283;&#20581;&#24615;&#65292;&#35201;&#20040;&#23384;&#22312;&#22266;&#26377;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38543;&#26426;&#24179;&#28369;&#65292;&#19968;&#31181;&#26356;&#21487;&#25193;&#23637;&#30340;&#31283;&#20581;&#24615;&#35748;&#35777;&#26694;&#26550;&#65292;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#35777;&#26126;&#25104;&#26412;&#25935;&#24863;&#30340;&#31283;&#20581;&#24615;&#12290;&#24314;&#31435;&#22312;&#19968;&#31181;&#25104;&#26412;&#25935;&#24863;&#35748;&#35777;&#21322;&#24452;&#30340;&#27010;&#24565;&#20043;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#35843;&#25972;&#26631;&#20934;&#30340;&#38543;&#26426;&#24179;&#28369;&#35748;&#35777;&#27969;&#31243;&#65292;&#20026;&#20219;&#20309;&#25104;&#26412;&#30697;&#38453;&#20135;&#29983;&#20005;&#26684;&#30340;&#31283;&#20581;&#24615;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#38024;&#23545;&#19981;&#21516;&#25968;&#25454;&#23376;&#32452;&#35774;&#35745;&#30340;&#32454;&#31890;&#24230;&#35748;&#35777;&#21322;&#24452;&#20248;&#21270;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#38024;&#23545;&#25104;&#26412;&#25935;&#24863;&#31283;&#20581;&#24615;&#20248;&#21270;&#30340;&#24179;&#28369;&#20998;&#31867;&#22120;&#12290;&#22312;&#22270;&#20687;&#22522;&#20934;&#27979;&#35797;&#21644;&#30495;&#23454;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We focus on learning adversarially robust classifiers under a cost-sensitive scenario, where the potential harm of different classwise adversarial transformations is encoded in a binary cost matrix. Existing methods are either empirical that cannot certify robustness or suffer from inherent scalability issues. In this work, we study whether randomized smoothing, a more scalable robustness certification framework, can be leveraged to certify cost-sensitive robustness. Built upon a notion of cost-sensitive certified radius, we show how to adapt the standard randomized smoothing certification pipeline to produce tight robustness guarantees for any cost matrix. In addition, with fine-grained certified radius optimization schemes specifically designed for different data subgroups, we propose an algorithm to train smoothed classifiers that are optimized for cost-sensitive robustness. Extensive experiments on image benchmarks and a real-world medical dataset demonstrate the superiority of our
&lt;/p&gt;</description></item><item><title>Refiner&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#38450;&#24481;&#33539;&#24335;&#65292;&#36890;&#36807;&#26500;&#24314;&#19982;&#21407;&#22987;&#25968;&#25454;&#20855;&#26377;&#20302;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#20581;&#22766;&#25968;&#25454;&#65292;&#26377;&#25928;&#22320;&#28151;&#28102;&#26799;&#24230;&#27844;&#28431;&#25915;&#20987;&#32773;&#65292;&#20174;&#32780;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#30340;&#38544;&#31169;&#20445;&#25252;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.02042</link><description>&lt;p&gt;
Refiner: &#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26799;&#24230;&#27844;&#28431;&#25915;&#20987;&#30340;&#25968;&#25454;&#31934;&#28860;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Refiner: Data Refining against Gradient Leakage Attacks in Federated Learning. (arXiv:2212.02042v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02042
&lt;/p&gt;
&lt;p&gt;
Refiner&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#38450;&#24481;&#33539;&#24335;&#65292;&#36890;&#36807;&#26500;&#24314;&#19982;&#21407;&#22987;&#25968;&#25454;&#20855;&#26377;&#20302;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#20581;&#22766;&#25968;&#25454;&#65292;&#26377;&#25928;&#22320;&#28151;&#28102;&#26799;&#24230;&#27844;&#28431;&#25915;&#20987;&#32773;&#65292;&#20174;&#32780;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#30340;&#38544;&#31169;&#20445;&#25252;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24341;&#36215;&#20102;&#23545;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#26131;&#21463;&#26799;&#24230;&#27844;&#28431;&#25915;&#20987;&#30340;&#20851;&#27880;&#12290;&#36825;&#31867;&#25915;&#20987;&#21033;&#29992;&#23458;&#25143;&#31471;&#19978;&#20256;&#30340;&#26799;&#24230;&#26469;&#37325;&#26500;&#20854;&#25935;&#24863;&#25968;&#25454;&#65292;&#20174;&#32780;&#30772;&#22351;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#38544;&#31169;&#20445;&#25252;&#33021;&#21147;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#23041;&#32961;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#38450;&#24481;&#26426;&#21046;&#26469;&#20943;&#36731;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#26426;&#21046;&#36890;&#36807;&#25805;&#32437;&#19978;&#20256;&#30340;&#26799;&#24230;&#26469;&#38450;&#27490;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#36825;&#20123;&#38450;&#24481;&#25514;&#26045;&#22312;&#38754;&#23545;&#22797;&#26434;&#25915;&#20987;&#26102;&#20855;&#26377;&#26377;&#38480;&#30340;&#24377;&#24615;&#65292;&#36825;&#34920;&#26126;&#36843;&#20999;&#38656;&#35201;&#26356;&#26377;&#25928;&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38450;&#24481;&#33539;&#24335;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;&#30340;&#26799;&#24230;&#25200;&#21160;&#26041;&#27861;&#65292;&#32780;&#26159;&#19987;&#27880;&#20110;&#26500;&#24314;&#20581;&#22766;&#25968;&#25454;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#22914;&#26524;&#20581;&#22766;&#25968;&#25454;&#19982;&#23458;&#25143;&#31471;&#21407;&#22987;&#25968;&#25454;&#20855;&#26377;&#24456;&#20302;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#19982;&#20581;&#22766;&#25968;&#25454;&#30456;&#20851;&#30340;&#26799;&#24230;&#21487;&#20197;&#26377;&#25928;&#22320;&#28151;&#28102;&#25915;&#20987;&#32773;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;Refiner&#65292;&#23427;&#21516;&#26102;&#20248;&#21270;&#20102;&#20004;&#20010;&#25351;&#26631;&#65292;&#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Recent works have brought attention to the vulnerability of Federated Learning (FL) systems to gradient leakage attacks. Such attacks exploit clients' uploaded gradients to reconstruct their sensitive data, thereby compromising the privacy protection capability of FL. In response, various defense mechanisms have been proposed to mitigate this threat by manipulating the uploaded gradients. Unfortunately, empirical evaluations have demonstrated limited resilience of these defenses against sophisticated attacks, indicating an urgent need for more effective defenses. In this paper, we explore a novel defensive paradigm that departs from conventional gradient perturbation approaches and instead focuses on the construction of robust data. Intuitively, if robust data exhibits low semantic similarity with clients' raw data, the gradients associated with robust data can effectively obfuscate attackers. To this end, we design Refiner that jointly optimizes two metrics for privacy protection and 
&lt;/p&gt;</description></item></channel></rss>