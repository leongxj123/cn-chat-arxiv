<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#38382;&#39064;&#31354;&#38388;&#20869;&#26500;&#24314;&#23545;&#25239;&#26679;&#26412;&#65292;&#23545;&#25239;&#38450;&#30149;&#27602;&#36719;&#20214;&#20013;&#30340;&#24694;&#24847;&#36719;&#20214;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2402.19027</link><description>&lt;p&gt;
&#22914;&#20309;&#35757;&#32451;&#24744;&#30340;&#38450;&#30149;&#27602;&#36719;&#20214;&#65306;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#38382;&#39064;&#31354;&#38388;&#21152;&#22266;
&lt;/p&gt;
&lt;p&gt;
How to Train your Antivirus: RL-based Hardening through the Problem-Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19027
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#38382;&#39064;&#31354;&#38388;&#20869;&#26500;&#24314;&#23545;&#25239;&#26679;&#26412;&#65292;&#23545;&#25239;&#38450;&#30149;&#27602;&#36719;&#20214;&#20013;&#30340;&#24694;&#24847;&#36719;&#20214;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#21152;&#22266;&#19968;&#23478;&#33879;&#21517;&#21830;&#19994;&#38450;&#30149;&#27602;&#20844;&#21496;&#27969;&#31243;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#38450;&#24481;&#25216;&#26415;&#65292;&#20197;&#23545;&#25239;&#24694;&#24847;&#36719;&#20214;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#23545;&#25239;&#26679;&#26412;&#65292;&#36825;&#26159;&#23545;&#25239;&#36867;&#36991;&#25915;&#20987;&#30340;&#27169;&#22411;&#35757;&#32451;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19027v1 Announce Type: cross  Abstract: ML-based malware detection on dynamic analysis reports is vulnerable to both evasion and spurious correlations. In this work, we investigate a specific ML architecture employed in the pipeline of a widely-known commercial antivirus company, with the goal to harden it against adversarial malware. Adversarial training, the sole defensive technique that can confer empirical robustness, is not applicable out of the box in this domain, for the principal reason that gradient-based perturbations rarely map back to feasible problem-space programs. We introduce a novel Reinforcement Learning approach for constructing adversarial examples, a constituent part of adversarially training a model against evasion. Our approach comes with multiple advantages. It performs modifications that are feasible in the problem-space, and only those; thus it circumvents the inverse mapping problem. It also makes possible to provide theoretical guarantees on the r
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#25915;&#20987;&#31574;&#30053;AICAttack&#65292;&#26088;&#22312;&#36890;&#36807;&#24494;&#23567;&#30340;&#22270;&#20687;&#25200;&#21160;&#26469;&#25915;&#20987;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#65292;&#22312;&#40657;&#30418;&#25915;&#20987;&#24773;&#26223;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.11940</link><description>&lt;p&gt;
AICAttack&#65306;&#22522;&#20110;&#27880;&#24847;&#21147;&#20248;&#21270;&#30340;&#23545;&#25239;&#24615;&#22270;&#20687;&#23383;&#24149;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
AICAttack: Adversarial Image Captioning Attack with Attention-Based Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11940
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#25915;&#20987;&#31574;&#30053;AICAttack&#65292;&#26088;&#22312;&#36890;&#36807;&#24494;&#23567;&#30340;&#22270;&#20687;&#25200;&#21160;&#26469;&#25915;&#20987;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#65292;&#22312;&#40657;&#30418;&#25915;&#20987;&#24773;&#26223;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#21462;&#24471;&#20102;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31561;&#35768;&#22810;&#20219;&#21153;&#19978;&#26174;&#33879;&#30340;&#25104;&#23601;&#12290;CV&#21644;NLP&#20132;&#21449;&#28857;&#19978;&#30340;&#22270;&#20687;&#23383;&#24149;&#38382;&#39064;&#20013;&#65292;&#30456;&#20851;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#25915;&#20987;&#31574;&#30053;&#65292;&#31216;&#20026;AICAttack&#65288;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22270;&#20687;&#23383;&#24149;&#25915;&#20987;&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#23545;&#22270;&#20687;&#36827;&#34892;&#24494;&#23567;&#25200;&#21160;&#26469;&#25915;&#20987;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#12290;&#22312;&#40657;&#30418;&#25915;&#20987;&#29615;&#22659;&#20013;&#36816;&#34892;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#19981;&#38656;&#35201;&#35775;&#38382;&#30446;&#26631;&#27169;&#22411;&#30340;&#26550;&#26500;&#12289;&#21442;&#25968;&#25110;&#26799;&#24230;&#20449;&#24687;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20505;&#36873;&#36873;&#25321;&#26426;&#21046;&#65292;&#21487;&#35782;&#21035;&#26368;&#20339;&#20687;&#32032;&#36827;&#34892;&#25915;&#20987;&#65292;&#28982;&#21518;&#37319;&#29992;&#24046;&#20998;&#36827;&#21270;&#65288;DE&#65289;&#26469;&#25200;&#20081;&#20687;&#32032;&#30340;RGB&#20540;&#12290;&#36890;&#36807;&#23545;&#22522;&#20934;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;AICAttack&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11940v1 Announce Type: cross  Abstract: Recent advances in deep learning research have shown remarkable achievements across many tasks in computer vision (CV) and natural language processing (NLP). At the intersection of CV and NLP is the problem of image captioning, where the related models' robustness against adversarial attacks has not been well studied. In this paper, we present a novel adversarial attack strategy, which we call AICAttack (Attention-based Image Captioning Attack), designed to attack image captioning models through subtle perturbations on images. Operating within a black-box attack scenario, our algorithm requires no access to the target model's architecture, parameters, or gradient information. We introduce an attention-based candidate selection mechanism that identifies the optimal pixels to attack, followed by Differential Evolution (DE) for perturbing pixels' RGB values. We demonstrate AICAttack's effectiveness through extensive experiments on benchma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TSFool&#30340;&#40657;&#30418;&#26041;&#27861;, &#21487;&#20197;&#26377;&#25928;&#22320;&#29983;&#25104;&#38024;&#23545;RNN&#20998;&#31867;&#22120;&#30340;&#39640;&#24230;&#38590;&#20197;&#23519;&#35273;&#30340;&#23545;&#25239;&#24615;&#26102;&#38388;&#24207;&#21015;&#65292;&#22312;&#32771;&#34385;&#23545;&#25239;&#26679;&#26412;&#38590;&#20197;&#23519;&#35273;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#23545;&#25239;&#24615;&#25915;&#20987;&#25913;&#36827;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#26469;&#22686;&#24378;&#25200;&#21160;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2209.06388</link><description>&lt;p&gt;
TSFool: &#36890;&#36807;&#22810;&#30446;&#26631;&#40657;&#30418;&#25915;&#20987;&#26041;&#27861;&#29983;&#25104;&#39640;&#24230;&#38590;&#20197;&#23519;&#35273;&#30340;&#23545;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#24615;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
TSFool: Crafting Highly-imperceptible Adversarial Time Series through Multi-objective Black-box Attack to Fool RNN Classifiers. (arXiv:2209.06388v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TSFool&#30340;&#40657;&#30418;&#26041;&#27861;, &#21487;&#20197;&#26377;&#25928;&#22320;&#29983;&#25104;&#38024;&#23545;RNN&#20998;&#31867;&#22120;&#30340;&#39640;&#24230;&#38590;&#20197;&#23519;&#35273;&#30340;&#23545;&#25239;&#24615;&#26102;&#38388;&#24207;&#21015;&#65292;&#22312;&#32771;&#34385;&#23545;&#25239;&#26679;&#26412;&#38590;&#20197;&#23519;&#35273;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#23545;&#25239;&#24615;&#25915;&#20987;&#25913;&#36827;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#26469;&#22686;&#24378;&#25200;&#21160;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#24456;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#29616;&#26377;&#30340;&#26799;&#24230;&#25915;&#20987;&#26041;&#27861;&#22312;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#21644;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#19979;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#26159;&#22240;&#20026;RNN&#30340;&#24490;&#29615;&#32467;&#26500;&#38459;&#27490;&#20102;&#30452;&#25509;&#30340;&#27169;&#22411;&#24046;&#20998;&#65292;&#32780;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#23545;&#25200;&#21160;&#30340;&#35270;&#35273;&#25935;&#24863;&#24615;&#25361;&#25112;&#20102;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#20256;&#32479;&#23616;&#37096;&#20248;&#21270;&#30446;&#26631;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TSFool&#30340;&#40657;&#30418;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#29983;&#25104;&#38024;&#23545;RNN&#20998;&#31867;&#22120;&#30340;&#39640;&#24230;&#38590;&#20197;&#23519;&#35273;&#30340;&#23545;&#25239;&#24615;&#26102;&#38388;&#24207;&#21015;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20840;&#23616;&#20248;&#21270;&#30446;&#26631;&#65292;&#31216;&#20026;Camouflage Coefficient&#65292;&#20174;&#31867;&#20998;&#24067;&#30340;&#35282;&#24230;&#32771;&#34385;&#23545;&#25239;&#26679;&#26412;&#30340;&#38590;&#20197;&#23519;&#35273;&#24615;&#65292;&#24182;&#30456;&#24212;&#22320;&#23558;&#23545;&#25239;&#24615;&#25915;&#20987;&#25913;&#36827;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#20197;&#22686;&#24378;&#25200;&#21160;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#25670;&#33073;&#19981;&#21516;&#27169;&#22411;&#38388;&#30340;&#36716;&#31227;&#24615;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#29305;&#23450;&#20110;&#27169;&#22411;&#30340;&#22238;&#36991;&#35268;&#21017;&#12290;&#22312;&#20154;&#36896;&#25968;&#25454;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TSFool&#21487;&#20197;&#29983;&#25104;&#39640;&#38590;&#24230;&#25915;&#20987;&#21516;&#26102;&#20445;&#25345;&#23545;&#25239;&#26679;&#26412;&#30340;&#19981;&#26131;&#34987;&#26816;&#27979;&#24615;&#65292;&#24182;&#26377;&#24456;&#39640;&#30340;&#36716;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network (NN) classifiers are vulnerable to adversarial attacks. Although the existing gradient-based attacks achieve state-of-the-art performance in feed-forward NNs and image recognition tasks, they do not perform as well on time series classification with recurrent neural network (RNN) models. This is because the cyclical structure of RNN prevents direct model differentiation and the visual sensitivity of time series data to perturbations challenges the traditional local optimization objective of the adversarial attack. In this paper, a black-box method called TSFool is proposed to efficiently craft highly-imperceptible adversarial time series for RNN classifiers. We propose a novel global optimization objective named Camouflage Coefficient to consider the imperceptibility of adversarial samples from the perspective of class distribution, and accordingly refine the adversarial attack as a multi-objective optimization problem to enhance the perturbation quality. To get rid of t
&lt;/p&gt;</description></item></channel></rss>