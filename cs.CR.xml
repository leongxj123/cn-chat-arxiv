<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#35813;&#35770;&#25991;&#20840;&#38754;&#35843;&#26597;&#20102;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#20851;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#21069;&#36884;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.12503</link><description>&lt;p&gt;
&#20445;&#25252;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#23041;&#32961;&#12289;&#28431;&#27934;&#21644;&#36127;&#36131;&#20219;&#30340;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12503
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20840;&#38754;&#35843;&#26597;&#20102;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#20851;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#21069;&#36884;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26174;&#33879;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#26684;&#23616;&#12290;&#23427;&#20204;&#23545;&#21508;&#31181;&#20219;&#21153;&#20135;&#29983;&#20102;&#24433;&#21709;&#65292;&#20174;&#32780;&#24443;&#24213;&#25913;&#21464;&#20102;&#25105;&#20204;&#22788;&#29702;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#23427;&#20204;&#24341;&#20154;&#27880;&#30446;&#30340;&#23454;&#29992;&#24615;&#22806;&#65292;LLMs&#36824;&#24102;&#26469;&#20102;&#37325;&#35201;&#30340;&#23433;&#20840;&#21644;&#39118;&#38505;&#32771;&#34385;&#12290;&#36825;&#20123;&#25361;&#25112;&#38656;&#35201;&#20180;&#32454;&#30740;&#31350;&#65292;&#20197;&#30830;&#20445;&#36127;&#36131;&#20219;&#30340;&#37096;&#32626;&#65292;&#24182;&#38450;&#33539;&#28508;&#22312;&#30340;&#28431;&#27934;&#12290;&#26412;&#30740;&#31350;&#20840;&#38754;&#35843;&#26597;&#20102;&#19982;LLMs&#30456;&#20851;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#20174;&#20116;&#20010;&#20027;&#39064;&#35282;&#24230;&#36827;&#34892;&#65306;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#12289;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#28431;&#27934;&#12289;LLMs&#35823;&#29992;&#21487;&#33021;&#36896;&#25104;&#30340;&#28508;&#22312;&#21361;&#23475;&#12289;&#32531;&#35299;&#31574;&#30053;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#21516;&#26102;&#35782;&#21035;&#24403;&#21069;&#31574;&#30053;&#30340;&#23616;&#38480;&#24615;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#24314;&#35758;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#20197;&#22686;&#24378;LLMs&#30340;&#23433;&#20840;&#21644;&#39118;&#38505;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12503v1 Announce Type: cross  Abstract: Large language models (LLMs) have significantly transformed the landscape of Natural Language Processing (NLP). Their impact extends across a diverse spectrum of tasks, revolutionizing how we approach language understanding and generations. Nevertheless, alongside their remarkable utility, LLMs introduce critical security and risk considerations. These challenges warrant careful examination to ensure responsible deployment and safeguard against potential vulnerabilities. This research paper thoroughly investigates security and privacy concerns related to LLMs from five thematic perspectives: security and privacy concerns, vulnerabilities against adversarial attacks, potential harms caused by misuses of LLMs, mitigation strategies to address these challenges while identifying limitations of current strategies. Lastly, the paper recommends promising avenues for future research to enhance the security and risk management of LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#21830;&#19994;LLMs&#30340;&#25552;&#31034;&#21453;&#31363;&#21462;&#25915;&#20987;&#26694;&#26550;PRSA&#65292;&#36890;&#36807;&#20998;&#26512;&#36755;&#20837;-&#36755;&#20986;&#23545;&#30340;&#20851;&#38190;&#29305;&#24449;&#23454;&#29616;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2402.19200</link><description>&lt;p&gt;
PRSA&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#21453;&#30423;&#31363;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
PRSA: Prompt Reverse Stealing Attacks against Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#21830;&#19994;LLMs&#30340;&#25552;&#31034;&#21453;&#31363;&#21462;&#25915;&#20987;&#26694;&#26550;PRSA&#65292;&#36890;&#36807;&#20998;&#26512;&#36755;&#20837;-&#36755;&#20986;&#23545;&#30340;&#20851;&#38190;&#29305;&#24449;&#23454;&#29616;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#20316;&#20026;&#37325;&#35201;&#30340;&#30693;&#35782;&#20135;&#26435;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#25191;&#34892;&#29305;&#23450;&#20219;&#21153;&#32780;&#26080;&#38656;&#24494;&#35843;&#65292;&#31361;&#26174;&#20102;&#23427;&#20204;&#19981;&#26029;&#22686;&#38271;&#30340;&#37325;&#35201;&#24615;&#12290;&#38543;&#30528;&#22522;&#20110;&#25552;&#31034;&#30340;&#26381;&#21153;&#30340;&#23835;&#36215;&#65292;&#22914;&#25552;&#31034;&#24066;&#22330;&#21644;LLM&#24212;&#29992;&#31243;&#24207;&#65292;&#25552;&#20379;&#32773;&#32463;&#24120;&#36890;&#36807;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#23637;&#31034;&#25552;&#31034;&#30340;&#33021;&#21147;&#65292;&#20197;&#21560;&#24341;&#29992;&#25143;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33539;&#24335;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#38190;&#30340;&#23433;&#20840;&#38382;&#39064;&#65306;&#26292;&#38706;&#36755;&#20837;-&#36755;&#20986;&#23545;&#26159;&#21542;&#20250;&#23545;&#28508;&#22312;&#25552;&#31034;&#27844;&#28431;&#26500;&#25104;&#39118;&#38505;&#65292;&#20405;&#29359;&#24320;&#21457;&#32773;&#30340;&#30693;&#35782;&#20135;&#26435;&#65311;&#23601;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#20010;&#38382;&#39064;&#36824;&#27809;&#26377;&#24471;&#21040;&#20840;&#38754;&#25506;&#35752;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#39318;&#27425;&#28145;&#20837;&#25506;&#35752;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#21830;&#19994;LLMs&#30340;&#25552;&#31034;&#21453;&#31363;&#21462;&#25915;&#20987;&#26694;&#26550;&#65292;&#21363;PRSA&#12290;PRSA&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#36890;&#36807;&#20998;&#26512;&#36755;&#20837;-&#36755;&#20986;&#23545;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#25105;&#20204;&#27169;&#20223;&#24182;g
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19200v1 Announce Type: cross  Abstract: Prompt, recognized as crucial intellectual property, enables large language models (LLMs) to perform specific tasks without the need of fine-tuning, underscoring their escalating importance. With the rise of prompt-based services, such as prompt marketplaces and LLM applications, providers often display prompts' capabilities through input-output examples to attract users. However, this paradigm raises a pivotal security concern: does the exposure of input-output pairs pose the risk of potential prompt leakage, infringing on the intellectual property rights of the developers? To our knowledge, this problem still has not been comprehensively explored yet. To remedy this gap, in this paper, we perform the first in depth exploration and propose a novel attack framework for reverse-stealing prompts against commercial LLMs, namely PRSA. The main idea of PRSA is that by analyzing the critical features of the input-output pairs, we mimic and g
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#20110;&#34892;&#20026;&#30340;&#29289;&#32852;&#32593;&#25915;&#20987;&#26816;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#25913;&#36827;&#28378;&#21160;&#31383;&#21475;&#29305;&#24449;&#25552;&#21462;&#12289;&#24341;&#20837;&#22810;&#27493;&#39588;&#29305;&#24449;&#36873;&#25321;&#12289;&#20351;&#29992;&#38548;&#31163;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#38598;&#20197;&#21450;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#25552;&#39640;&#26816;&#27979;&#21644;&#24615;&#33021;&#65292;&#24182;&#19988;&#32463;&#36807;&#20005;&#26684;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.01343</link><description>&lt;p&gt;
IoTGeM: &#29992;&#20110;&#22522;&#20110;&#34892;&#20026;&#30340;&#29289;&#32852;&#32593;&#25915;&#20987;&#26816;&#27979;&#30340;&#36890;&#29992;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
IoTGeM: Generalizable Models for Behaviour-Based IoT Attack Detection. (arXiv:2401.01343v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#20110;&#34892;&#20026;&#30340;&#29289;&#32852;&#32593;&#25915;&#20987;&#26816;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#25913;&#36827;&#28378;&#21160;&#31383;&#21475;&#29305;&#24449;&#25552;&#21462;&#12289;&#24341;&#20837;&#22810;&#27493;&#39588;&#29305;&#24449;&#36873;&#25321;&#12289;&#20351;&#29992;&#38548;&#31163;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#38598;&#20197;&#21450;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#25552;&#39640;&#26816;&#27979;&#21644;&#24615;&#33021;&#65292;&#24182;&#19988;&#32463;&#36807;&#20005;&#26684;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20851;&#20110;&#29289;&#32852;&#32593;&#35774;&#22791;&#32593;&#32476;&#30340;&#22522;&#20110;&#34892;&#20026;&#30340;&#25915;&#20987;&#26816;&#27979;&#30740;&#31350;&#65292;&#25152;&#24471;&#21040;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36866;&#24212;&#33021;&#21147;&#26377;&#38480;&#65292;&#24182;&#19988;&#24448;&#24448;&#27809;&#26377;&#24471;&#21040;&#35777;&#26126;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24314;&#27169;&#29289;&#32852;&#32593;&#32593;&#32476;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#30528;&#37325;&#20110;&#36890;&#29992;&#24615;&#65292;&#21516;&#26102;&#20063;&#33021;&#25552;&#39640;&#26816;&#27979;&#21644;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#28378;&#21160;&#31383;&#21475;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#27493;&#39588;&#29305;&#24449;&#36873;&#25321;&#36807;&#31243;&#26469;&#20943;&#23569;&#36807;&#25311;&#21512;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#38548;&#31163;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#38598;&#26469;&#26500;&#24314;&#21644;&#27979;&#35797;&#27169;&#22411;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#20808;&#21069;&#27169;&#22411;&#22312;&#36890;&#29992;&#24615;&#26041;&#38754;&#30340;&#24120;&#35265;&#25968;&#25454;&#27844;&#28431;&#38382;&#39064;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#25968;&#25454;&#38598;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20005;&#26684;&#35780;&#20272;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#22686;&#21152;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#65292;&#20174;&#32780;&#33021;&#22815;&#35782;&#21035;&#20986;&#25903;&#25745;&#25915;&#20987;&#20934;&#30830;&#26816;&#27979;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous research on behaviour-based attack detection on networks of IoT devices has resulted in machine learning models whose ability to adapt to unseen data is limited, and often not demonstrated. In this paper we present an approach for modelling IoT network attacks that focuses on generalizability, yet also leads to better detection and performance. First, we present an improved rolling window approach for feature extraction, and introduce a multi-step feature selection process that reduces overfitting. Second, we build and test models using isolated train and test datasets, thereby avoiding common data leaks that have limited the generalizability of previous models. Third, we rigorously evaluate our methodology using a diverse portfolio of machine learning models, evaluation metrics and datasets. Finally, we build confidence in the models by using explainable AI techniques, allowing us to identify the features that underlie accurate detection of attacks.
&lt;/p&gt;</description></item></channel></rss>