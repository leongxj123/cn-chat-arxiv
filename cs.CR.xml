<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#35780;&#20272;LLM&#38598;&#25104;&#31995;&#32479;&#20445;&#23494;&#24615;&#30340;&#31995;&#32479;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24418;&#24335;&#21270;&#19968;&#20010;"&#31192;&#23494;&#23494;&#38053;"&#28216;&#25103;&#26469;&#25429;&#25417;&#27169;&#22411;&#38544;&#34255;&#31169;&#20154;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#35780;&#20272;&#20102;&#20843;&#31181;&#25915;&#20987;&#21644;&#22235;&#31181;&#38450;&#24481;&#26041;&#27861;&#65292;&#21457;&#29616;&#24403;&#21069;&#30340;&#38450;&#24481;&#26041;&#27861;&#32570;&#20047;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.06922</link><description>&lt;p&gt;
&#26426;&#22120;&#20013;&#30340;&#31169;&#35821;&#65306;LLM&#38598;&#25104;&#31995;&#32479;&#20013;&#30340;&#20445;&#23494;&#24615;
&lt;/p&gt;
&lt;p&gt;
Whispers in the Machine: Confidentiality in LLM-integrated Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#35780;&#20272;LLM&#38598;&#25104;&#31995;&#32479;&#20445;&#23494;&#24615;&#30340;&#31995;&#32479;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24418;&#24335;&#21270;&#19968;&#20010;"&#31192;&#23494;&#23494;&#38053;"&#28216;&#25103;&#26469;&#25429;&#25417;&#27169;&#22411;&#38544;&#34255;&#31169;&#20154;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#35780;&#20272;&#20102;&#20843;&#31181;&#25915;&#20987;&#21644;&#22235;&#31181;&#38450;&#24481;&#26041;&#27861;&#65292;&#21457;&#29616;&#24403;&#21069;&#30340;&#38450;&#24481;&#26041;&#27861;&#32570;&#20047;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#19982;&#22806;&#37096;&#24037;&#20855;&#38598;&#25104;&#12290;&#23613;&#31649;&#36825;&#20123;&#38598;&#25104;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;LLM&#30340;&#21151;&#33021;&#65292;&#20294;&#23427;&#20204;&#20063;&#22312;&#19981;&#21516;&#32452;&#20214;&#20043;&#38388;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25915;&#20987;&#38754;&#65292;&#21487;&#33021;&#27844;&#38706;&#26426;&#23494;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24694;&#24847;&#24037;&#20855;&#21487;&#20197;&#21033;&#29992;LLM&#26412;&#36523;&#30340;&#28431;&#27934;&#26469;&#25805;&#32437;&#27169;&#22411;&#24182;&#25439;&#23475;&#20854;&#20182;&#26381;&#21153;&#30340;&#25968;&#25454;&#65292;&#36825;&#24341;&#21457;&#20102;&#22312;LLM&#38598;&#25104;&#29615;&#22659;&#20013;&#22914;&#20309;&#20445;&#25252;&#31169;&#23494;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#35780;&#20272;LLM&#38598;&#25104;&#31995;&#32479;&#20445;&#23494;&#24615;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24418;&#24335;&#21270;&#20102;&#19968;&#20010;"&#31192;&#23494;&#23494;&#38053;"&#28216;&#25103;&#65292;&#21487;&#20197;&#25429;&#25417;&#27169;&#22411;&#38544;&#34255;&#31169;&#20154;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#27604;&#36739;&#27169;&#22411;&#23545;&#20445;&#23494;&#24615;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#20197;&#21450;&#19981;&#21516;&#38450;&#24481;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20843;&#31181;&#20808;&#21069;&#21457;&#34920;&#30340;&#25915;&#20987;&#21644;&#22235;&#31181;&#38450;&#24481;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#30340;&#38450;&#24481;&#26041;&#27861;&#32570;&#20047;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are increasingly integrated with external tools. While these integrations can significantly improve the functionality of LLMs, they also create a new attack surface where confidential data may be disclosed between different components. Specifically, malicious tools can exploit vulnerabilities in the LLM itself to manipulate the model and compromise the data of other services, raising the question of how private data can be protected in the context of LLM integrations.   In this work, we provide a systematic way of evaluating confidentiality in LLM-integrated systems. For this, we formalize a "secret key" game that can capture the ability of a model to conceal private information. This enables us to compare the vulnerability of a model against confidentiality attacks and also the effectiveness of different defense strategies. In this framework, we evaluate eight previously published attacks and four defenses. We find that current defenses lack generalization
&lt;/p&gt;</description></item></channel></rss>