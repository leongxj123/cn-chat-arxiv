<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#30340;&#32479;&#35745;&#25928;&#29575;&#21644;&#26816;&#27979;&#35268;&#21017;&#65292;&#36890;&#36807;&#20851;&#38190;&#32479;&#35745;&#37327;&#21644;&#31192;&#23494;&#23494;&#38053;&#25511;&#21046;&#35823;&#25253;&#29575;&#65292;&#21516;&#26102;&#35780;&#20272;&#27700;&#21360;&#26816;&#27979;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.01245</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#30340;&#32479;&#35745;&#26694;&#26550;: &#26530;&#36724;&#12289;&#26816;&#27979;&#25928;&#29575;&#21644;&#26368;&#20248;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
A Statistical Framework of Watermarks for Large Language Models: Pivot, Detection Efficiency and Optimal Rules
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01245
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#30340;&#32479;&#35745;&#25928;&#29575;&#21644;&#26816;&#27979;&#35268;&#21017;&#65292;&#36890;&#36807;&#20851;&#38190;&#32479;&#35745;&#37327;&#21644;&#31192;&#23494;&#23494;&#38053;&#25511;&#21046;&#35823;&#25253;&#29575;&#65292;&#21516;&#26102;&#35780;&#20272;&#27700;&#21360;&#26816;&#27979;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;ChatGPT&#20110;2022&#24180;11&#26376;&#25512;&#20986;&#20197;&#26469;&#65292;&#23558;&#20960;&#20046;&#19981;&#21487;&#23519;&#35273;&#30340;&#32479;&#35745;&#20449;&#21495;&#23884;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#65292;&#20063;&#34987;&#31216;&#20026;&#27700;&#21360;&#65292;&#24050;&#34987;&#29992;&#20316;&#20174;&#20854;&#20154;&#31867;&#25776;&#20889;&#23545;&#24212;&#29289;&#19978;&#21487;&#35777;&#26816;&#27979;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#21407;&#21017;&#24615;&#26041;&#27861;&#12290; &#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25512;&#29702;&#27700;&#21360;&#30340;&#32479;&#35745;&#25928;&#29575;&#24182;&#35774;&#35745;&#24378;&#22823;&#30340;&#26816;&#27979;&#35268;&#21017;&#12290;&#21463;&#27700;&#21360;&#26816;&#27979;&#30340;&#20551;&#35774;&#26816;&#39564;&#20844;&#24335;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#39318;&#20808;&#36873;&#25321;&#25991;&#26412;&#30340;&#26530;&#36724;&#32479;&#35745;&#37327;&#21644;&#30001;LLM&#25552;&#20379;&#32473;&#39564;&#35777;&#22120;&#30340;&#31192;&#23494;&#23494;&#38053;&#65292;&#20197;&#23454;&#29616;&#25511;&#21046;&#35823;&#25253;&#29575;&#65288;&#23558;&#20154;&#31867;&#25776;&#20889;&#30340;&#25991;&#26412;&#38169;&#35823;&#22320;&#26816;&#27979;&#20026;LLM&#29983;&#25104;&#30340;&#38169;&#35823;&#65289;&#12290; &#25509;&#19979;&#26469;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#36890;&#36807;&#33719;&#21462;&#28176;&#36817;&#38169;&#35823;&#36127;&#29575;&#65288;&#23558;LLM&#29983;&#25104;&#25991;&#26412;&#38169;&#35823;&#22320;&#26816;&#27979;&#20026;&#20154;&#31867;&#25776;&#20889;&#30340;&#38169;&#35823;&#65289;&#30340;&#23553;&#38381;&#24418;&#24335;&#34920;&#36798;&#24335;&#26469;&#35780;&#20272;&#27700;&#21360;&#26816;&#27979;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01245v1 Announce Type: cross  Abstract: Since ChatGPT was introduced in November 2022, embedding (nearly) unnoticeable statistical signals into text generated by large language models (LLMs), also known as watermarking, has been used as a principled approach to provable detection of LLM-generated text from its human-written counterpart. In this paper, we introduce a general and flexible framework for reasoning about the statistical efficiency of watermarks and designing powerful detection rules. Inspired by the hypothesis testing formulation of watermark detection, our framework starts by selecting a pivotal statistic of the text and a secret key -- provided by the LLM to the verifier -- to enable controlling the false positive rate (the error of mistakenly detecting human-written text as LLM-generated). Next, this framework allows one to evaluate the power of watermark detection rules by obtaining a closed-form expression of the asymptotic false negative rate (the error of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27719;&#38598;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#23433;&#20840;&#39046;&#22495;&#30340;&#30456;&#20851;&#27010;&#24565;&#65292;&#31995;&#32479;&#22320;&#24418;&#24335;&#21270;&#20102;&#29983;&#25104;&#24335;AI&#20195;&#29702;&#31995;&#32479;&#20013;&#30340;&#31192;&#23494;&#21246;&#32467;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#25514;&#26045;&#12290;&#36890;&#36807;&#27979;&#35797;&#21508;&#31181;&#24418;&#24335;&#30340;&#31192;&#23494;&#21246;&#32467;&#25152;&#38656;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#27169;&#22411;&#30340;&#38544;&#20889;&#33021;&#21147;&#26377;&#38480;&#65292;&#20294; GPT-4 &#23637;&#31034;&#20102;&#33021;&#21147;&#30340;&#39134;&#36291;&#12290;</title><link>https://arxiv.org/abs/2402.07510</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;AI&#20195;&#29702;&#20043;&#38388;&#30340;&#31192;&#23494;&#21246;&#32467;
&lt;/p&gt;
&lt;p&gt;
Secret Collusion Among Generative AI Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27719;&#38598;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#23433;&#20840;&#39046;&#22495;&#30340;&#30456;&#20851;&#27010;&#24565;&#65292;&#31995;&#32479;&#22320;&#24418;&#24335;&#21270;&#20102;&#29983;&#25104;&#24335;AI&#20195;&#29702;&#31995;&#32479;&#20013;&#30340;&#31192;&#23494;&#21246;&#32467;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#25514;&#26045;&#12290;&#36890;&#36807;&#27979;&#35797;&#21508;&#31181;&#24418;&#24335;&#30340;&#31192;&#23494;&#21246;&#32467;&#25152;&#38656;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#27169;&#22411;&#30340;&#38544;&#20889;&#33021;&#21147;&#26377;&#38480;&#65292;&#20294; GPT-4 &#23637;&#31034;&#20102;&#33021;&#21147;&#30340;&#39134;&#36291;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33021;&#21147;&#19978;&#30340;&#22686;&#24378;&#20026;&#36890;&#20449;&#30340;&#29983;&#25104;&#24335;AI&#20195;&#29702;&#22242;&#38431;&#35299;&#20915;&#32852;&#21512;&#20219;&#21153;&#30340;&#24212;&#29992;&#25171;&#24320;&#20102;&#21487;&#33021;&#24615;&#12290;&#36825;&#24341;&#21457;&#20102;&#20851;&#20110;&#26410;&#32463;&#25480;&#26435;&#20998;&#20139;&#20449;&#24687;&#25110;&#20854;&#20182;&#19981;&#24517;&#35201;&#30340;&#20195;&#29702;&#21327;&#35843;&#24418;&#24335;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#25361;&#25112;&#12290;&#29616;&#20195;&#38544;&#20889;&#26415;&#25216;&#26415;&#21487;&#33021;&#20351;&#36825;&#31181;&#21160;&#24577;&#38590;&#20197;&#26816;&#27979;&#12290;&#26412;&#25991;&#36890;&#36807;&#27762;&#21462;&#20154;&#24037;&#26234;&#33021;&#21644;&#23433;&#20840;&#39046;&#22495;&#30456;&#20851;&#27010;&#24565;&#65292;&#20840;&#38754;&#31995;&#32479;&#22320;&#24418;&#24335;&#21270;&#20102;&#29983;&#25104;&#24335;AI&#20195;&#29702;&#31995;&#32479;&#20013;&#30340;&#31192;&#23494;&#21246;&#32467;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#38544;&#20889;&#26415;&#30340;&#21160;&#26426;&#65292;&#24182;&#25552;&#20986;&#20102;&#21508;&#31181;&#32531;&#35299;&#25514;&#26045;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26159;&#19968;&#20010;&#27169;&#22411;&#35780;&#20272;&#26694;&#26550;&#65292;&#31995;&#32479;&#22320;&#27979;&#35797;&#20102;&#21508;&#31181;&#24418;&#24335;&#30340;&#31192;&#23494;&#21246;&#32467;&#25152;&#38656;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#24403;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;&#34429;&#28982;&#24403;&#21069;&#27169;&#22411;&#30340;&#38544;&#20889;&#33021;&#21147;&#20173;&#28982;&#26377;&#38480;&#65292;&#20294; GPT-4 &#26174;&#31034;&#20986;&#33021;&#21147;&#30340;&#39134;&#36291;&#65292;&#36825;&#34920;&#26126;&#26377;&#24517;&#35201;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent capability increases in large language models (LLMs) open up applications in which teams of communicating generative AI agents solve joint tasks. This poses privacy and security challenges concerning the unauthorised sharing of information, or other unwanted forms of agent coordination. Modern steganographic techniques could render such dynamics hard to detect. In this paper, we comprehensively formalise the problem of secret collusion in systems of generative AI agents by drawing on relevant concepts from both the AI and security literature. We study incentives for the use of steganography, and propose a variety of mitigation measures. Our investigations result in a model evaluation framework that systematically tests capabilities required for various forms of secret collusion. We provide extensive empirical results across a range of contemporary LLMs. While the steganographic capabilities of current models remain limited, GPT-4 displays a capability jump suggesting the need fo
&lt;/p&gt;</description></item></channel></rss>