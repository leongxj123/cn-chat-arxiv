<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#29616;&#26377;&#27700;&#21360;&#25216;&#26415;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;Python&#20195;&#30721;&#19978;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#23481;&#26131;&#36890;&#36807;&#20445;&#30041;&#35821;&#20041;&#36716;&#25442;&#26469;&#31227;&#38500;&#36825;&#20123;&#27700;&#21360;&#12290;</title><link>https://arxiv.org/abs/2403.17983</link><description>&lt;p&gt;
LLM&#29983;&#25104;&#20195;&#30721;&#30340;&#27700;&#21360;&#25216;&#26415;&#26159;&#21542;&#20855;&#26377;&#40065;&#26834;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Watermarking LLM-Generated Code Robust?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17983
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#29616;&#26377;&#27700;&#21360;&#25216;&#26415;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;Python&#20195;&#30721;&#19978;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#23481;&#26131;&#36890;&#36807;&#20445;&#30041;&#35821;&#20041;&#36716;&#25442;&#26469;&#31227;&#38500;&#36825;&#20123;&#27700;&#21360;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#39318;&#27425;&#30740;&#31350;&#20102;&#29616;&#26377;&#27700;&#21360;&#25216;&#26415;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;Python&#20195;&#30721;&#19978;&#30340;&#40065;&#26834;&#24615;&#12290;&#23613;&#31649;&#29616;&#26377;&#20316;&#21697;&#34920;&#26126;&#27700;&#21360;&#25216;&#26415;&#23545;&#33258;&#28982;&#35821;&#35328;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#20445;&#30041;&#35821;&#20041;&#30340;&#36716;&#25442;&#24456;&#23481;&#26131;&#31227;&#38500;&#20195;&#30721;&#19978;&#30340;&#36825;&#20123;&#27700;&#21360;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17983v1 Announce Type: cross  Abstract: We present the first study of the robustness of existing watermarking techniques on Python code generated by large language models. Although existing works showed that watermarking can be robust for natural language, we show that it is easy to remove these watermarks on code by semantic-preserving transformations.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24674;&#22797;&#29983;&#25104;&#27169;&#22411;&#39044;&#24494;&#35843;&#26435;&#37325;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#20302;&#31209;&#24494;&#35843;&#27169;&#22411;&#21487;&#20197;&#24674;&#22797;&#20934;&#30830;&#30340;&#39044;&#24494;&#35843;&#26435;&#37325;&#65292;&#21033;&#29992;&#36825;&#20010;&#26032;&#28431;&#27934;&#25915;&#20987;&#22823;&#35268;&#27169;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.10208</link><description>&lt;p&gt;
&#24674;&#22797;&#29983;&#25104;&#27169;&#22411;&#30340;&#39044;&#24494;&#35843;&#26435;&#37325;
&lt;/p&gt;
&lt;p&gt;
Recovering the Pre-Fine-Tuning Weights of Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10208
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24674;&#22797;&#29983;&#25104;&#27169;&#22411;&#39044;&#24494;&#35843;&#26435;&#37325;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#20302;&#31209;&#24494;&#35843;&#27169;&#22411;&#21487;&#20197;&#24674;&#22797;&#20934;&#30830;&#30340;&#39044;&#24494;&#35843;&#26435;&#37325;&#65292;&#21033;&#29992;&#36825;&#20010;&#26032;&#28431;&#27934;&#25915;&#20987;&#22823;&#35268;&#27169;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#65292;&#20027;&#27969;&#27169;&#24335;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65306;i) &#22312;&#22823;&#35268;&#27169;&#20294;&#19981;&#23433;&#20840;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;ii) &#36890;&#36807;&#24494;&#35843;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#12290;&#36825;&#31181;&#20570;&#27861;&#34987;&#35748;&#20026;&#26159;&#23433;&#20840;&#30340;&#65292;&#22240;&#20026;&#30446;&#21069;&#27809;&#26377;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#24674;&#22797;&#19981;&#23433;&#20840;&#30340;&#39044;&#24494;&#35843;&#27169;&#22411;&#26435;&#37325;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#36825;&#31181;&#20551;&#35774;&#36890;&#24120;&#26159;&#38169;&#35823;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#35889;&#21453;&#35843;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#23569;&#37327;&#20302;&#31209;&#65288;LoRA&#65289;&#24494;&#35843;&#27169;&#22411;&#24674;&#22797;&#39044;&#24494;&#35843;&#27169;&#22411;&#30340;&#26435;&#37325;&#12290;&#19982;&#20808;&#21069;&#35797;&#22270;&#24674;&#22797;&#39044;&#24494;&#35843;&#33021;&#21147;&#30340;&#25915;&#20987;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#24674;&#22797;&#31934;&#30830;&#30340;&#39044;&#24494;&#35843;&#26435;&#37325;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#36825;&#20010;&#26032;&#30340;&#23545;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#28431;&#27934;&#65292;&#20363;&#22914;&#20010;&#24615;&#21270;&#30340;&#31283;&#23450;&#25193;&#25955;&#21644;&#23545;&#40784;&#30340;Mistral&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10208v1 Announce Type: cross  Abstract: The dominant paradigm in generative modeling consists of two steps: i) pre-training on a large-scale but unsafe dataset, ii) aligning the pre-trained model with human values via fine-tuning. This practice is considered safe, as no current method can recover the unsafe, pre-fine-tuning model weights. In this paper, we demonstrate that this assumption is often false. Concretely, we present Spectral DeTuning, a method that can recover the weights of the pre-fine-tuning model using a few low-rank (LoRA) fine-tuned models. In contrast to previous attacks that attempt to recover pre-fine-tuning capabilities, our method aims to recover the exact pre-fine-tuning weights. Our approach exploits this new vulnerability against large-scale models such as a personalized Stable Diffusion and an aligned Mistral.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36328;&#39046;&#22495;&#32852;&#21512;&#23398;&#20064;&#20013;&#22522;&#20110;&#35760;&#24405;&#32423;&#20010;&#24615;&#21270;&#24046;&#20998;&#38544;&#31169;&#30340;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;rPDP-FL&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#21151;&#33021;&#35299;&#20915;&#26041;&#26696;&#8220;&#27169;&#25311;-&#26354;&#32447;&#25311;&#21512;&#8221;&#65292;&#20197;&#28385;&#36275;&#19981;&#21516;&#35760;&#24405;&#30340;&#38544;&#31169;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2401.16251</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#32852;&#21512;&#23398;&#20064;&#20013;&#22522;&#20110;&#35760;&#24405;&#32423;&#20010;&#24615;&#21270;&#24046;&#20998;&#38544;&#31169;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Cross-silo Federated Learning with Record-level Personalized Differential Privacy. (arXiv:2401.16251v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36328;&#39046;&#22495;&#32852;&#21512;&#23398;&#20064;&#20013;&#22522;&#20110;&#35760;&#24405;&#32423;&#20010;&#24615;&#21270;&#24046;&#20998;&#38544;&#31169;&#30340;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;rPDP-FL&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#21151;&#33021;&#35299;&#20915;&#26041;&#26696;&#8220;&#27169;&#25311;-&#26354;&#32447;&#25311;&#21512;&#8221;&#65292;&#20197;&#28385;&#36275;&#19981;&#21516;&#35760;&#24405;&#30340;&#38544;&#31169;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24046;&#20998;&#38544;&#31169;&#22686;&#24378;&#30340;&#32852;&#21512;&#23398;&#20064;&#25104;&#20026;&#20102;&#20445;&#25252;&#23458;&#25143;&#31471;&#25968;&#25454;&#38544;&#31169;&#30340;&#24120;&#29992;&#26041;&#27861;&#65292;&#20294;&#29616;&#26377;&#26041;&#26696;&#36890;&#24120;&#20551;&#35774;&#25152;&#26377;&#35760;&#24405;&#30340;&#38544;&#31169;&#39044;&#31639;&#22343;&#30456;&#21516;&#65292;&#25552;&#20379;&#19968;&#31181;&#36866;&#29992;&#20110;&#25152;&#26377;&#35760;&#24405;&#30340;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#33021;&#26080;&#27861;&#28385;&#36275;&#27599;&#20010;&#35760;&#24405;&#30340;&#38544;&#31169;&#38656;&#27714;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36328;&#39046;&#22495;&#32852;&#21512;&#23398;&#20064;&#20013;&#22522;&#20110;&#35760;&#24405;&#32423;&#20010;&#24615;&#21270;&#24046;&#20998;&#38544;&#31169;&#30340;&#26410;&#30693;&#39046;&#22495;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;rPDP-FL&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#37319;&#29992;&#20004;&#38454;&#27573;&#28151;&#21512;&#25277;&#26679;&#26041;&#26696;&#65292;&#26082;&#21253;&#25324;&#23458;&#25143;&#31471;&#32423;&#21035;&#25277;&#26679;&#65292;&#21448;&#21253;&#25324;&#38750;&#22343;&#21248;&#35760;&#24405;&#32423;&#21035;&#25277;&#26679;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#38544;&#31169;&#38656;&#27714;&#12290;&#19968;&#20010;&#20851;&#38190;&#19988;&#38750;&#24179;&#20961;&#30340;&#38382;&#39064;&#26159;&#22312;&#32473;&#23450;&#20010;&#24615;&#21270;&#38544;&#31169;&#39044;&#31639;&#949;&#30340;&#24773;&#20917;&#19979;&#36873;&#25321;&#29702;&#24819;&#30340;&#27599;&#35760;&#24405;&#25277;&#26679;&#27010;&#29575;q&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#35299;&#20915;&#26041;&#26696;&#8220;&#27169;&#25311;-&#26354;&#32447;&#25311;&#21512;&#8221;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#25581;&#31034;&#38750;&#32447;&#24615;&#30456;&#20851;&#24615;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning enhanced by differential privacy has emerged as a popular approach to better safeguard the privacy of client-side data by protecting clients' contributions during the training process. Existing solutions typically assume a uniform privacy budget for all records and provide one-size-fits-all solutions that may not be adequate to meet each record's privacy requirement. In this paper, we explore the uncharted territory of cross-silo FL with record-level personalized differential privacy. We devise a novel framework named rPDP-FL, employing a two-stage hybrid sampling scheme with both client-level sampling and non-uniform record-level sampling to accommodate varying privacy requirements. A critical and non-trivial problem is to select the ideal per-record sampling probability q given the personalized privacy budget {\epsilon}. We introduce a versatile solution named Simulation-CurveFitting, allowing us to uncover a significant insight into the nonlinear correlation betwe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;ConfAIde&#22522;&#20934;&#65292;&#25581;&#31034;&#20102;LLMs&#30340;&#19978;&#19979;&#25991;&#38544;&#31169;&#25512;&#29702;&#33021;&#21147;&#20013;&#30340;&#37325;&#35201;&#24369;&#28857;&#65292;&#23454;&#39564;&#35777;&#26126;&#21363;&#20351;&#26159;&#26368;&#24378;&#22823;&#30340;&#27169;&#22411;&#20063;&#20250;&#22312;&#20154;&#31867;&#19981;&#20250;&#30340;&#19978;&#19979;&#25991;&#20013;&#27844;&#38706;&#31169;&#20154;&#20449;&#24687;&#65292;&#24378;&#35843;&#20102;&#25506;&#32034;&#26032;&#22411;&#25512;&#29702;&#26102;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.17884</link><description>&lt;p&gt;
LLM&#33021;&#20445;&#23432;&#31192;&#23494;&#21527;&#65311;&#36890;&#36807;&#19978;&#19979;&#25991;&#23436;&#25972;&#24615;&#29702;&#35770;&#27979;&#35797;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#31169;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory. (arXiv:2310.17884v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;ConfAIde&#22522;&#20934;&#65292;&#25581;&#31034;&#20102;LLMs&#30340;&#19978;&#19979;&#25991;&#38544;&#31169;&#25512;&#29702;&#33021;&#21147;&#20013;&#30340;&#37325;&#35201;&#24369;&#28857;&#65292;&#23454;&#39564;&#35777;&#26126;&#21363;&#20351;&#26159;&#26368;&#24378;&#22823;&#30340;&#27169;&#22411;&#20063;&#20250;&#22312;&#20154;&#31867;&#19981;&#20250;&#30340;&#19978;&#19979;&#25991;&#20013;&#27844;&#38706;&#31169;&#20154;&#20449;&#24687;&#65292;&#24378;&#35843;&#20102;&#25506;&#32034;&#26032;&#22411;&#25512;&#29702;&#26102;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;AI&#21161;&#25163;&#65288;&#24037;&#20316;&#12289;&#23478;&#24237;&#31561;&#65289;&#20013;&#20132;&#20114;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#25512;&#29702;&#26102;&#38544;&#31169;&#39118;&#38505;&#65306;LLMs&#20174;&#22810;&#20010;&#26469;&#28304;&#30340;&#36755;&#20837;&#20013;&#33719;&#21462;&#19981;&#21516;&#31867;&#22411;&#30340;&#20449;&#24687;&#65292;&#24182;&#26399;&#26395;&#22312;&#32473;&#23450;&#30340;&#19978;&#19979;&#25991;&#20013;&#25512;&#29702;&#20986;&#22312;&#20309;&#31181;&#30446;&#30340;&#21644;&#19982;&#35841;&#20998;&#20139;&#30340;&#20869;&#23481;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;ConfAIde&#65292;&#19968;&#20010;&#26088;&#22312;&#35782;&#21035;&#25351;&#20196;&#35843;&#25972;&#30340;LLMs&#38544;&#31169;&#25512;&#29702;&#33021;&#21147;&#20013;&#37325;&#35201;&#24369;&#28857;&#30340;&#22522;&#20934;&#65292;&#26469;&#24341;&#36215;&#20154;&#20204;&#23545;&#19978;&#19979;&#25991;&#38544;&#31169;&#36825;&#19968;&#26497;&#20854;&#20851;&#38190;&#20294;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#27010;&#24565;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;GPT-4&#21644;ChatGPT&#31561;&#26368;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#22312;&#20154;&#31867;&#19981;&#20250;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#20063;&#20250;&#27844;&#38706;39&#65285;&#21644;57&#65285;&#30340;&#31169;&#20154;&#20449;&#24687;&#12290;&#21363;&#20351;&#25105;&#20204;&#20351;&#29992;&#20445;&#25252;&#38544;&#31169;&#30340;&#25552;&#31034;&#25110;&#24605;&#32500;&#38142;&#25512;&#29702;&#65292;&#36825;&#31181;&#27844;&#28431;&#20063;&#20250;&#25345;&#32493;&#23384;&#22312;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;&#36843;&#20999;&#38656;&#35201;&#25506;&#32034;&#22522;&#20110;&#25512;&#29702;&#21644;&#29702;&#35770;&#30340;&#26032;&#22411;&#25512;&#29702;&#26102;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The interactive use of large language models (LLMs) in AI assistants (at work, home, etc.) introduces a new set of inference-time privacy risks: LLMs are fed different types of information from multiple sources in their inputs and are expected to reason about what to share in their outputs, for what purpose and with whom, within a given context. In this work, we draw attention to the highly critical yet overlooked notion of contextual privacy by proposing ConfAIde, a benchmark designed to identify critical weaknesses in the privacy reasoning capabilities of instruction-tuned LLMs. Our experiments show that even the most capable models such as GPT-4 and ChatGPT reveal private information in contexts that humans would not, 39% and 57% of the time, respectively. This leakage persists even when we employ privacy-inducing prompts or chain-of-thought reasoning. Our work underscores the immediate need to explore novel inference-time privacy-preserving approaches, based on reasoning and theory
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22635;&#34917;&#20102;&#32593;&#32476;&#20445;&#38505;&#39118;&#38505;&#30740;&#31350;&#20013;&#30340;&#37325;&#35201;&#24046;&#36317;&#65292;&#36890;&#36807;&#21033;&#29992;&#32654;&#22269;&#24030;&#24635;&#26816;&#23519;&#38271;&#25552;&#20379;&#30340;&#19968;&#32452;&#20844;&#20849;&#25968;&#25454;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#32593;&#32476;&#20445;&#38505;&#39118;&#38505;&#30495;&#23454;&#35268;&#27169;&#30340;&#26032;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.04786</link><description>&lt;p&gt;
&#32593;&#32476;&#20445;&#38505;&#39118;&#38505;&#65306;&#25253;&#21578;&#24310;&#36831;&#65292;&#31532;&#19977;&#26041;&#32593;&#32476;&#20107;&#20214;&#21644;&#25253;&#21578;&#24847;&#24895;&#21464;&#21270;&#8212;&#8212;&#20351;&#29992;&#30001;&#32654;&#22269;&#24030;&#24635;&#26816;&#23519;&#38271;&#21457;&#24067;&#30340;&#25968;&#25454;&#27844;&#38706;&#36827;&#34892;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Cyber Insurance Risk: Reporting Delays, Third-Party Cyber Events, and Changes in Reporting Propensity -- An Analysis Using Data Breaches Published by U.S. State Attorneys General. (arXiv:2310.04786v1 [q-fin.RM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22635;&#34917;&#20102;&#32593;&#32476;&#20445;&#38505;&#39118;&#38505;&#30740;&#31350;&#20013;&#30340;&#37325;&#35201;&#24046;&#36317;&#65292;&#36890;&#36807;&#21033;&#29992;&#32654;&#22269;&#24030;&#24635;&#26816;&#23519;&#38271;&#25552;&#20379;&#30340;&#19968;&#32452;&#20844;&#20849;&#25968;&#25454;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#32593;&#32476;&#20445;&#38505;&#39118;&#38505;&#30495;&#23454;&#35268;&#27169;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#32593;&#32476;&#23041;&#32961;&#30340;&#22686;&#21152;&#65292;&#32593;&#32476;&#20445;&#38505;&#23545;&#20225;&#19994;&#32780;&#35328;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#32593;&#32476;&#20445;&#38505;&#39118;&#38505;&#30340;&#30740;&#31350;&#21463;&#21040;&#20102;&#25968;&#25454;&#30340;&#26222;&#36941;&#32570;&#20047;&#20197;&#21450;&#21487;&#20844;&#24320;&#33719;&#21462;&#30340;&#26377;&#38480;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#32593;&#32476;&#20445;&#38505;&#27169;&#22411;&#30340;&#38480;&#21046;&#21253;&#25324;&#65306;&#65288;i&#65289;&#25253;&#21578;&#24310;&#36831;&#30340;&#20449;&#24687;&#32570;&#20047;&#65292;&#65288;ii&#65289;&#25152;&#26377;&#21463;&#31532;&#19977;&#26041;&#20107;&#20214;&#24433;&#21709;&#30340;&#20225;&#19994;&#30340;&#20449;&#24687;&#32570;&#20047;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#25253;&#21578;&#24847;&#24895;&#30340;&#21464;&#21270;&#12290;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#32654;&#22269;&#24030;&#24635;&#26816;&#23519;&#38271;&#25552;&#20379;&#30340;&#19968;&#32452;&#26410;&#34987;&#20805;&#20998;&#35748;&#21487;&#30340;&#20844;&#20849;&#25968;&#25454;&#26469;&#22635;&#34917;&#36825;&#19968;&#37325;&#35201;&#24046;&#36317;&#65292;&#24182;&#20026;&#32593;&#32476;&#20445;&#38505;&#39118;&#38505;&#30340;&#30495;&#23454;&#35268;&#27169;&#25552;&#20379;&#26032;&#30340;&#35265;&#35299;&#12290;&#36825;&#20123;&#25968;&#25454;&#26159;&#22522;&#20110;&#25968;&#25454;&#27844;&#38706;&#30340;&#24378;&#21046;&#25253;&#21578;&#35201;&#27714;&#25910;&#38598;&#30340;&#65292;&#24182;&#21253;&#21547;&#22823;&#37327;&#35814;&#32454;&#20449;&#24687;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24191;&#27867;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#23545;&#32593;&#32476;&#20445;&#38505;&#23450;&#20215;&#12289;&#20648;&#22791;&#12289;&#25215;&#20445;&#21644;&#32463;&#39564;&#30340;&#30456;&#20851;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rise of cyber threats, cyber insurance is becoming an important consideration for businesses. However, research on cyber insurance risk has so far been hindered by the general lack of data, as well as limitations underlying what limited data are available publicly. Specifically and of particular importance to cyber insurance modelling, limitations arising from lack of information regarding (i) delays in reporting, (ii) all businesses affected by third-party events, and (iii) changes in reporting propensity. In this paper, we fill this important gap by utilising an underrecognised set of public data provided by U.S. state Attorneys General, and provide new insights on the true scale of cyber insurance risk. These data are collected based on mandatory reporting requirements of data breaches, and contain substantial and detailed information. We further discuss extensively the associated implications of our findings for cyber insurance pricing, reserving, underwriting, and experie
&lt;/p&gt;</description></item></channel></rss>