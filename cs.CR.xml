<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24418;&#24335;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#65292;&#24182;&#31995;&#32479;&#21270;&#38450;&#24481;&#36825;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2310.12815</link><description>&lt;p&gt;
LLM-&#38598;&#25104;&#24212;&#29992;&#20013;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#21644;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Prompt Injection Attacks and Defenses in LLM-Integrated Applications. (arXiv:2310.12815v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24418;&#24335;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#65292;&#24182;&#31995;&#32479;&#21270;&#38450;&#24481;&#36825;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20316;&#21508;&#31181;&#31216;&#20026;LLM-&#38598;&#25104;&#24212;&#29992;&#30340;&#23454;&#38469;&#24212;&#29992;&#31243;&#24207;&#30340;&#21518;&#31471;&#12290;&#26368;&#36817;&#30340;&#22810;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;LLM-&#38598;&#25104;&#24212;&#29992;&#23481;&#26131;&#21463;&#21040;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#23041;&#32961;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#23558;&#24694;&#24847;&#25351;&#20196;/&#25968;&#25454;&#27880;&#20837;&#36825;&#20123;&#24212;&#29992;&#31243;&#24207;&#30340;&#36755;&#20837;&#20013;&#65292;&#20197;&#36798;&#21040;&#25915;&#20987;&#32773;&#30340;&#39044;&#26399;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20165;&#38480;&#20110;&#26696;&#20363;&#30740;&#31350;&#65292;&#32570;&#20047;&#23545;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#21450;&#20854;&#38450;&#24481;&#30340;&#31995;&#32479;&#29702;&#35299;&#12290;&#26412;&#35770;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24418;&#24335;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#65292;&#24182;&#23558;&#30740;&#31350;&#35770;&#25991;&#21644;&#21338;&#23458;&#25991;&#31456;&#20013;&#35752;&#35770;&#30340;&#29616;&#26377;&#25915;&#20987;&#35270;&#20026;&#25105;&#20204;&#26694;&#26550;&#30340;&#29305;&#20363;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#32452;&#21512;&#29616;&#26377;&#25915;&#20987;&#35774;&#35745;&#26032;&#30340;&#25915;&#20987;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#38450;&#24481;&#30340;&#26694;&#26550;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#21487;&#20197;&#39044;&#38450;&#21644;&#32531;&#35299;&#36825;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are increasingly deployed as the backend for a variety of real-world applications called LLM-Integrated Applications. Multiple recent works showed that LLM-Integrated Applications are vulnerable to prompt injection attacks, in which an attacker injects malicious instruction/data into the input of those applications such that they produce results as the attacker desires. However, existing works are limited to case studies. As a result, the literature lacks a systematic understanding of prompt injection attacks and their defenses. We aim to bridge the gap in this work. In particular, we propose a general framework to formalize prompt injection attacks. Existing attacks, which are discussed in research papers and blog posts, are special cases in our framework. Our framework enables us to design a new attack by combining existing attacks. Moreover, we also propose a framework to systematize defenses against prompt injection attacks. Using our frameworks, we con
&lt;/p&gt;</description></item></channel></rss>