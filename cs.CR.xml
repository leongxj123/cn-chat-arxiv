<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>ACFL&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#32534;&#30721;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#20043;&#21069;&#37319;&#29992;&#20010;&#24615;&#21270;&#30340;&#25968;&#25454;&#19978;&#20256;&#21040;&#20013;&#22830;&#26381;&#21153;&#22120;&#26469;&#29983;&#25104;&#20840;&#23616;&#32534;&#30721;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#21407;&#26377;&#22266;&#23450;&#26435;&#37325;&#29983;&#25104;&#20840;&#23616;&#32534;&#30721;&#25968;&#25454;&#38598;&#26102;&#21487;&#33021;&#23548;&#33268;&#23398;&#20064;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.14905</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#32534;&#30721;&#32852;&#37030;&#23398;&#20064;&#65306;&#38544;&#31169;&#20445;&#25252;&#19982;&#24930;&#33410;&#28857;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Adaptive Coded Federated Learning: Privacy Preservation and Straggler Mitigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14905
&lt;/p&gt;
&lt;p&gt;
ACFL&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#32534;&#30721;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#20043;&#21069;&#37319;&#29992;&#20010;&#24615;&#21270;&#30340;&#25968;&#25454;&#19978;&#20256;&#21040;&#20013;&#22830;&#26381;&#21153;&#22120;&#26469;&#29983;&#25104;&#20840;&#23616;&#32534;&#30721;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#21407;&#26377;&#22266;&#23450;&#26435;&#37325;&#29983;&#25104;&#20840;&#23616;&#32534;&#30721;&#25968;&#25454;&#38598;&#26102;&#21487;&#33021;&#23548;&#33268;&#23398;&#20064;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#23384;&#22312;&#24930;&#33410;&#28857;&#24773;&#20917;&#19979;&#30340;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32534;&#30721;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#20013;&#22830;&#26381;&#21153;&#22120;&#32858;&#21512;&#26469;&#33258;&#38750;&#24930;&#33410;&#28857;&#30340;&#26799;&#24230;&#21644;&#26469;&#33258;&#38544;&#31169;&#20445;&#25252;&#20840;&#23616;&#32534;&#30721;&#25968;&#25454;&#38598;&#30340;&#26799;&#24230;&#65292;&#20197;&#20943;&#36731;&#24930;&#33410;&#28857;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22312;&#32858;&#21512;&#36825;&#20123;&#26799;&#24230;&#26102;&#65292;&#22266;&#23450;&#26435;&#37325;&#22312;&#36845;&#20195;&#20013;&#19968;&#30452;&#34987;&#24212;&#29992;&#65292;&#24573;&#30053;&#20102;&#20840;&#23616;&#32534;&#30721;&#25968;&#25454;&#38598;&#30340;&#29983;&#25104;&#36807;&#31243;&#20197;&#21450;&#35757;&#32451;&#27169;&#22411;&#38543;&#30528;&#36845;&#20195;&#30340;&#21160;&#24577;&#24615;&#12290;&#36825;&#19968;&#30095;&#28431;&#21487;&#33021;&#23548;&#33268;&#23398;&#20064;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20811;&#26381;&#36825;&#19968;&#32570;&#38519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#36866;&#24212;&#32534;&#30721;&#32852;&#37030;&#23398;&#20064;&#65288;ACFL&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#22312;ACFL&#20013;&#65292;&#22312;&#35757;&#32451;&#20043;&#21069;&#65292;&#27599;&#20010;&#35774;&#22791;&#21521;&#20013;&#22830;&#26381;&#21153;&#22120;&#19978;&#20256;&#19968;&#20010;&#24102;&#26377;&#38468;&#21152;&#22122;&#22768;&#30340;&#32534;&#30721;&#26412;&#22320;&#25968;&#25454;&#38598;&#65292;&#20197;&#29983;&#25104;&#31526;&#21512;&#38544;&#31169;&#20445;&#25252;&#35201;&#27714;&#30340;&#20840;&#23616;&#32534;&#30721;&#25968;&#25454;&#38598;&#12290;&#22312;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14905v1 Announce Type: cross  Abstract: In this article, we address the problem of federated learning in the presence of stragglers. For this problem, a coded federated learning framework has been proposed, where the central server aggregates gradients received from the non-stragglers and gradient computed from a privacy-preservation global coded dataset to mitigate the negative impact of the stragglers. However, when aggregating these gradients, fixed weights are consistently applied across iterations, neglecting the generation process of the global coded dataset and the dynamic nature of the trained model over iterations. This oversight may result in diminished learning performance. To overcome this drawback, we propose a new method named adaptive coded federated learning (ACFL). In ACFL, before the training, each device uploads a coded local dataset with additive noise to the central server to generate a global coded dataset under privacy preservation requirements. During
&lt;/p&gt;</description></item></channel></rss>