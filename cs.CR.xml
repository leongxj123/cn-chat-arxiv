<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#20004;&#38454;&#27573;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#22521;&#20859;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#29305;&#24449;&#65292;&#21253;&#25324;&#26412;&#22320;&#21644;&#36328;&#39046;&#22495;&#29305;&#24449;&#65292;&#20197;&#22686;&#24378;&#23545;&#26410;&#30693;&#20998;&#24067;&#39046;&#22495;&#30340;&#27867;&#21270;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#20808;&#39564;&#21644;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#20998;&#31163;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#32593;&#32476;&#23433;&#20840;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#25928;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.17300</link><description>&lt;p&gt;
&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36890;&#36807;&#39046;&#22495;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#25913;&#21892;&#20837;&#20405;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improving Intrusion Detection with Domain-Invariant Representation Learning in Latent Space. (arXiv:2312.17300v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#20004;&#38454;&#27573;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#22521;&#20859;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#29305;&#24449;&#65292;&#21253;&#25324;&#26412;&#22320;&#21644;&#36328;&#39046;&#22495;&#29305;&#24449;&#65292;&#20197;&#22686;&#24378;&#23545;&#26410;&#30693;&#20998;&#24067;&#39046;&#22495;&#30340;&#27867;&#21270;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#20808;&#39564;&#21644;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#20998;&#31163;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#32593;&#32476;&#23433;&#20840;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#25928;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#27867;&#21270;&#32858;&#28966;&#20110;&#21033;&#29992;&#26469;&#33258;&#20855;&#26377;&#20016;&#23500;&#35757;&#32451;&#25968;&#25454;&#21644;&#26631;&#31614;&#30340;&#22810;&#20010;&#30456;&#20851;&#39046;&#22495;&#30340;&#30693;&#35782;&#65292;&#22686;&#24378;&#23545;&#26410;&#30693;&#20998;&#24067;&#65288;IN&#65289;&#21644;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#39046;&#22495;&#30340;&#25512;&#29702;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#20174;&#36328;&#36234;&#22810;&#20010;&#39046;&#22495;&#30340;&#29305;&#24449;&#20013;&#22521;&#20859;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#65292;&#21253;&#25324;&#26412;&#22320;&#21644;&#36328;&#39046;&#22495;&#65292;&#20197;&#22686;&#24378;&#23545;IN&#21644;OOD&#39046;&#22495;&#30340;&#27867;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#26368;&#23567;&#21270;&#20808;&#39564;&#19982;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#20998;&#31163;&#28508;&#22312;&#31354;&#38388;&#65292;&#26377;&#25928;&#28040;&#38500;&#34394;&#20551;&#29305;&#24449;&#30456;&#20851;&#24615;&#12290;&#32508;&#21512;&#32780;&#35328;&#65292;&#32852;&#21512;&#20248;&#21270;&#23558;&#20419;&#36827;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#23398;&#20064;&#12290;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#20998;&#31867;&#25351;&#26631;&#35780;&#20272;&#27169;&#22411;&#22312;&#22810;&#20010;&#32593;&#32476;&#23433;&#20840;&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#33021;&#65292;&#23545;&#27604;&#20102;&#29616;&#20195;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization focuses on leveraging knowledge from multiple related domains with ample training data and labels to enhance inference on unseen in-distribution (IN) and out-of-distribution (OOD) domains. In our study, we introduce a two-phase representation learning technique using multi-task learning. This approach aims to cultivate a latent space from features spanning multiple domains, encompassing both native and cross-domains, to amplify generalization to IN and OOD territories. Additionally, we attempt to disentangle the latent space by minimizing the mutual information between the prior and latent space, effectively de-correlating spurious feature correlations. Collectively, the joint optimization will facilitate domain-invariant feature learning. We assess the model's efficacy across multiple cybersecurity datasets, using standard classification metrics on both unseen IN and OOD sets, and juxtapose the results with contemporary domain generalization methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20110;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;(NIDS)&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#21516;&#26102;&#25506;&#31350;&#20102;&#25345;&#32493;&#20877;&#35757;&#32451;&#23545;NIDS&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#21363;&#20351;&#27809;&#26377;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#25345;&#32493;&#20877;&#35757;&#32451;&#20063;&#21487;&#20197;&#20943;&#23569;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.05494</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#23545;&#25239;&#24615;&#28431;&#27934;&#25915;&#20987;&#30340;&#23454;&#29992;&#24615;&#27979;&#35797;&#65306;&#21160;&#24577;&#23398;&#20064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Adversarial Evasion Attacks Practicality in Networks: Testing the Impact of Dynamic Learning. (arXiv:2306.05494v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20110;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;(NIDS)&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#21516;&#26102;&#25506;&#31350;&#20102;&#25345;&#32493;&#20877;&#35757;&#32451;&#23545;NIDS&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#21363;&#20351;&#27809;&#26377;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#25345;&#32493;&#20877;&#35757;&#32451;&#20063;&#21487;&#20197;&#20943;&#23569;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;(NIDS)&#20013;&#65292;&#30001;&#20110;&#20854;&#33258;&#21160;&#21270;&#30340;&#29305;&#24615;&#21644;&#22312;&#22788;&#29702;&#21644;&#20998;&#31867;&#22823;&#37327;&#25968;&#25454;&#19978;&#30340;&#39640;&#31934;&#24230;&#12290;&#20294;&#26426;&#22120;&#23398;&#20064;&#23384;&#22312;&#32570;&#38519;&#65292;&#20854;&#20013;&#26368;&#22823;&#30340;&#38382;&#39064;&#20043;&#19968;&#26159;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#20854;&#30446;&#30340;&#26159;&#20351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20135;&#29983;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#29420;&#29305;&#30340;&#36129;&#29486;&#65306;&#23545;&#25239;&#24615;&#25915;&#20987;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;NIDS&#23454;&#29992;&#24615;&#38382;&#39064;&#30340;&#20998;&#31867;&#21644;&#23545;&#25345;&#32493;&#35757;&#32451;&#23545;NIDS&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21363;&#20351;&#27809;&#26377;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#25345;&#32493;&#20877;&#35757;&#32451;&#20063;&#21487;&#20197;&#20943;&#23569;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#23545;&#25239;&#24615;&#25915;&#20987;&#21487;&#33021;&#20250;&#21361;&#21450;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;NIDS&#65292;&#20294;&#25345;&#32493;&#20877;&#35757;&#32451;&#21487;&#24102;&#26469;&#19968;&#23450;&#30340;&#32531;&#35299;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) has become ubiquitous, and its deployment in Network Intrusion Detection Systems (NIDS) is inevitable due to its automated nature and high accuracy in processing and classifying large volumes of data. However, ML has been found to have several flaws, on top of them are adversarial attacks, which aim to trick ML models into producing faulty predictions. While most adversarial attack research focuses on computer vision datasets, recent studies have explored the practicality of such attacks against ML-based network security entities, especially NIDS.  This paper presents two distinct contributions: a taxonomy of practicality issues associated with adversarial attacks against ML-based NIDS and an investigation of the impact of continuous training on adversarial attacks against NIDS. Our experiments indicate that continuous re-training, even without adversarial training, can reduce the effect of adversarial attacks. While adversarial attacks can harm ML-based NIDSs, ou
&lt;/p&gt;</description></item></channel></rss>