<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#23637;&#31034;&#20102;&#23545;&#40784;&#30340;LLM&#23545;&#31616;&#21333;&#33258;&#36866;&#24212;&#36234;&#29425;&#25915;&#20987;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#27169;&#22411;&#19978;&#20960;&#20046;100%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#36824;&#20171;&#32461;&#20102;&#23545;&#20110;&#19981;&#20844;&#24320;logprobs&#30340;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#36234;&#29425;&#20197;&#21450;&#22914;&#20309;&#22312;&#21463;&#27745;&#26579;&#30340;&#27169;&#22411;&#20013;&#26597;&#25214;&#26408;&#39532;&#23383;&#31526;&#20018;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.02151</link><description>&lt;p&gt;
&#29992;&#31616;&#21333;&#33258;&#36866;&#24212;&#25915;&#20987;&#36234;&#29425;&#21151;&#33021;&#23545;&#40784;&#30340;LLM
&lt;/p&gt;
&lt;p&gt;
Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02151
&lt;/p&gt;
&lt;p&gt;
&#23637;&#31034;&#20102;&#23545;&#40784;&#30340;LLM&#23545;&#31616;&#21333;&#33258;&#36866;&#24212;&#36234;&#29425;&#25915;&#20987;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#27169;&#22411;&#19978;&#20960;&#20046;100%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#36824;&#20171;&#32461;&#20102;&#23545;&#20110;&#19981;&#20844;&#24320;logprobs&#30340;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#36234;&#29425;&#20197;&#21450;&#22914;&#20309;&#22312;&#21463;&#27745;&#26579;&#30340;&#27169;&#22411;&#20013;&#26597;&#25214;&#26408;&#39532;&#23383;&#31526;&#20018;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#26159;&#26368;&#26032;&#30340;&#23433;&#20840;&#23545;&#40784;&#30340;LLM&#20063;&#19981;&#20855;&#26377;&#25269;&#25239;&#31616;&#21333;&#33258;&#36866;&#24212;&#36234;&#29425;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#25104;&#21151;&#21033;&#29992;&#23545;logprobs&#30340;&#35775;&#38382;&#36827;&#34892;&#36234;&#29425;&#65306;&#25105;&#20204;&#26368;&#21021;&#35774;&#35745;&#20102;&#19968;&#20010;&#23545;&#25239;&#24615;&#25552;&#31034;&#27169;&#26495;&#65288;&#26377;&#26102;&#20250;&#36866;&#24212;&#30446;&#26631;LLM&#65289;&#65292;&#28982;&#21518;&#25105;&#20204;&#22312;&#21518;&#32512;&#19978;&#24212;&#29992;&#38543;&#26426;&#25628;&#32034;&#20197;&#26368;&#22823;&#21270;&#30446;&#26631;logprob&#65288;&#20363;&#22914;token&#8220;Sure&#8221;&#65289;&#65292;&#21487;&#33021;&#20250;&#36827;&#34892;&#22810;&#27425;&#37325;&#21551;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;GPT-3.5/4&#12289;Llama-2-Chat-7B/13B/70B&#12289;Gemma-7B&#21644;&#38024;&#23545;GCG&#25915;&#20987;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#30340;HarmBench&#19978;&#30340;R2D2&#31561;&#20960;&#20046;100%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;--&#26681;&#25454;GPT-4&#30340;&#35780;&#21028;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#36716;&#31227;&#25110;&#39044;&#22635;&#20805;&#25915;&#20987;&#20197;100%&#30340;&#25104;&#21151;&#29575;&#23545;&#25152;&#26377;&#19981;&#26292;&#38706;logprobs&#30340;Claude&#27169;&#22411;&#36827;&#34892;&#36234;&#29425;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#21463;&#27745;&#26579;&#30340;&#27169;&#22411;&#20013;&#20351;&#29992;&#23545;&#19968;&#32452;&#21463;&#38480;&#21046;&#30340;token&#25191;&#34892;&#38543;&#26426;&#25628;&#32034;&#20197;&#26597;&#25214;&#26408;&#39532;&#23383;&#31526;&#20018;&#30340;&#26041;&#27861;--&#36825;&#39033;&#20219;&#21153;&#19982;&#35768;&#22810;&#20854;&#20182;&#20219;&#21153;&#20849;&#20139;&#30456;&#21516;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02151v1 Announce Type: cross  Abstract: We show that even the most recent safety-aligned LLMs are not robust to simple adaptive jailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobs for jailbreaking: we initially design an adversarial prompt template (sometimes adapted to the target LLM), and then we apply random search on a suffix to maximize the target logprob (e.g., of the token "Sure"), potentially with multiple restarts. In this way, we achieve nearly 100\% attack success rate -- according to GPT-4 as a judge -- on GPT-3.5/4, Llama-2-Chat-7B/13B/70B, Gemma-7B, and R2D2 from HarmBench that was adversarially trained against the GCG attack. We also show how to jailbreak all Claude models -- that do not expose logprobs -- via either a transfer or prefilling attack with 100\% success rate. In addition, we show how to use random search on a restricted set of tokens for finding trojan strings in poisoned models -- a task that shares many s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#28023;&#32501;&#27602;&#21270;&#8221;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;&#22312;&#35757;&#32451;&#26102;&#27880;&#20837;&#28023;&#32501;&#26679;&#26412;&#21487;&#20197;&#22312;&#27979;&#35797;&#26102;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#27599;&#20010;&#36755;&#20837;&#19978;&#30340;&#33021;&#32791;&#21644;&#24310;&#36831;&#65292;&#24182;&#19988;&#21363;&#20351;&#25915;&#20987;&#32773;&#21482;&#25511;&#21046;&#20102;&#19968;&#20123;&#27169;&#22411;&#26356;&#26032;&#20063;&#21487;&#20197;&#36827;&#34892;&#27492;&#25915;&#20987;&#65292;&#28023;&#32501;&#27602;&#21270;&#20960;&#20046;&#23436;&#20840;&#28040;&#38500;&#20102;&#30828;&#20214;&#21152;&#36895;&#22120;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2203.08147</link><description>&lt;p&gt;
&#22522;&#20110;&#28023;&#32501;&#27602;&#21270;&#30340;&#33021;&#32791;&#24310;&#36831;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy-Latency Attacks via Sponge Poisoning. (arXiv:2203.08147v4 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.08147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#28023;&#32501;&#27602;&#21270;&#8221;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;&#22312;&#35757;&#32451;&#26102;&#27880;&#20837;&#28023;&#32501;&#26679;&#26412;&#21487;&#20197;&#22312;&#27979;&#35797;&#26102;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#27599;&#20010;&#36755;&#20837;&#19978;&#30340;&#33021;&#32791;&#21644;&#24310;&#36831;&#65292;&#24182;&#19988;&#21363;&#20351;&#25915;&#20987;&#32773;&#21482;&#25511;&#21046;&#20102;&#19968;&#20123;&#27169;&#22411;&#26356;&#26032;&#20063;&#21487;&#20197;&#36827;&#34892;&#27492;&#25915;&#20987;&#65292;&#28023;&#32501;&#27602;&#21270;&#20960;&#20046;&#23436;&#20840;&#28040;&#38500;&#20102;&#30828;&#20214;&#21152;&#36895;&#22120;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#32501;&#26679;&#26412;&#26159;&#22312;&#27979;&#35797;&#26102;&#31934;&#24515;&#20248;&#21270;&#30340;&#36755;&#20837;&#65292;&#21487;&#22312;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#37096;&#32626;&#26102;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#37327;&#28040;&#32791;&#21644;&#24310;&#36831;&#12290;&#26412;&#25991;&#39318;&#27425;&#35777;&#26126;&#20102;&#28023;&#32501;&#26679;&#26412;&#20063;&#21487;&#36890;&#36807;&#19968;&#31181;&#21517;&#20026;&#28023;&#32501;&#27602;&#21270;&#30340;&#25915;&#20987;&#27880;&#20837;&#21040;&#35757;&#32451;&#20013;&#12290;&#35813;&#25915;&#20987;&#20801;&#35768;&#22312;&#27599;&#20010;&#27979;&#35797;&#26102;&#36755;&#20837;&#20013;&#19981;&#21152;&#21306;&#20998;&#22320;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#33021;&#37327;&#28040;&#32791;&#21644;&#24310;&#36831;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28023;&#32501;&#27602;&#21270;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#19982;&#20248;&#21270;&#27979;&#35797;&#26102;&#28023;&#32501;&#26679;&#26412;&#30456;&#20851;&#30340;&#38480;&#21046;&#65292;&#24182;&#34920;&#26126;&#21363;&#20351;&#25915;&#20987;&#32773;&#20165;&#25511;&#21046;&#20960;&#20010;&#27169;&#22411;&#26356;&#26032;&#65292;&#20363;&#22914;&#27169;&#22411;&#35757;&#32451;&#34987;&#22806;&#21253;&#32473;&#19981;&#21463;&#20449;&#20219;&#30340;&#31532;&#19977;&#26041;&#25110;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#20998;&#24067;&#24335;&#36827;&#34892;&#65292;&#20063;&#21487;&#20197;&#36827;&#34892;&#36825;&#31181;&#25915;&#20987;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#20998;&#26512;&#65292;&#34920;&#26126;&#28023;&#32501;&#27602;&#21270;&#20960;&#20046;&#23436;&#20840;&#28040;&#38500;&#20102;&#30828;&#20214;&#21152;&#36895;&#22120;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#27602;&#21270;&#27169;&#22411;&#30340;&#28608;&#27963;&#65292;&#30830;&#23450;&#20102;&#21738;&#20123;&#35745;&#31639;&#23545;&#23548;&#33268;&#33021;&#37327;&#28040;&#32791;&#21644;&#24310;&#36831;&#22686;&#21152;&#36215;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sponge examples are test-time inputs carefully optimized to increase energy consumption and latency of neural networks when deployed on hardware accelerators. In this work, we are the first to demonstrate that sponge examples can also be injected at training time, via an attack that we call sponge poisoning. This attack allows one to increase the energy consumption and latency of machine-learning models indiscriminately on each test-time input. We present a novel formalization for sponge poisoning, overcoming the limitations related to the optimization of test-time sponge examples, and show that this attack is possible even if the attacker only controls a few model updates; for instance, if model training is outsourced to an untrusted third-party or distributed via federated learning. Our extensive experimental analysis shows that sponge poisoning can almost completely vanish the effect of hardware accelerators. We also analyze the activations of poisoned models, identifying which comp
&lt;/p&gt;</description></item></channel></rss>