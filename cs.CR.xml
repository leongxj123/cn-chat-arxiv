<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21457;&#29616;GNN&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#35813;&#20219;&#21153;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.02663</link><description>&lt;p&gt;
&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#30340;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
A backdoor attack against link prediction tasks with graph neural networks. (arXiv:2401.02663v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21457;&#29616;GNN&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#35813;&#20219;&#21153;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26159;&#19968;&#31867;&#33021;&#22815;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;GNN&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#12290;&#24403;&#20855;&#20307;&#30340;&#27169;&#24335;&#65288;&#31216;&#20026;&#21518;&#38376;&#35302;&#21457;&#22120;&#65292;&#20363;&#22914;&#23376;&#22270;&#12289;&#33410;&#28857;&#31561;&#65289;&#20986;&#29616;&#22312;&#36755;&#20837;&#25968;&#25454;&#20013;&#26102;&#65292;&#23884;&#20837;&#22312;GNN&#27169;&#22411;&#20013;&#30340;&#21518;&#38376;&#20250;&#34987;&#28608;&#27963;&#65292;&#23558;&#36755;&#20837;&#25968;&#25454;&#35823;&#20998;&#31867;&#20026;&#25915;&#20987;&#32773;&#25351;&#23450;&#30340;&#30446;&#26631;&#31867;&#26631;&#31614;&#65292;&#32780;&#24403;&#36755;&#20837;&#20013;&#27809;&#26377;&#21518;&#38376;&#35302;&#21457;&#22120;&#26102;&#65292;&#23884;&#20837;&#22312;GNN&#27169;&#22411;&#20013;&#30340;&#21518;&#38376;&#19981;&#20250;&#34987;&#28608;&#27963;&#65292;&#27169;&#22411;&#27491;&#24120;&#24037;&#20316;&#12290;&#21518;&#38376;&#25915;&#20987;&#20855;&#26377;&#26497;&#39640;&#30340;&#38544;&#34109;&#24615;&#65292;&#32473;GNN&#27169;&#22411;&#24102;&#26469;&#20005;&#37325;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#30446;&#21069;&#65292;&#23545;GNN&#30340;&#21518;&#38376;&#25915;&#20987;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22270;&#20998;&#31867;&#21644;&#33410;&#28857;&#20998;&#31867;&#31561;&#20219;&#21153;&#19978;&#65292;&#23545;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#30340;&#21518;&#38376;&#25915;&#20987;&#30740;&#31350;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are a class of deep learning models capable of processing graph-structured data, and they have demonstrated significant performance in a variety of real-world applications. Recent studies have found that GNN models are vulnerable to backdoor attacks. When specific patterns (called backdoor triggers, e.g., subgraphs, nodes, etc.) appear in the input data, the backdoor embedded in the GNN models is activated, which misclassifies the input data into the target class label specified by the attacker, whereas when there are no backdoor triggers in the input, the backdoor embedded in the GNN models is not activated, and the models work normally. Backdoor attacks are highly stealthy and expose GNN models to serious security risks. Currently, research on backdoor attacks against GNNs mainly focus on tasks such as graph classification and node classification, and backdoor attacks against link prediction tasks are rarely studied. In this paper, we propose a backdoor a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35780;&#20272;&#25968;&#25454;&#27969;&#21152;&#36895;&#22120;&#19978;&#30340;&#20391;&#20449;&#36947;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25915;&#20987;&#26041;&#27861;&#26469;&#24674;&#22797;CNN&#27169;&#22411;&#30340;&#26550;&#26500;&#12290;&#35813;&#25915;&#20987;&#21033;&#29992;&#20102;&#25968;&#25454;&#27969;&#26144;&#23556;&#30340;&#25968;&#25454;&#37325;&#29992;&#20197;&#21450;&#26550;&#26500;&#32447;&#32034;&#65292;&#25104;&#21151;&#24674;&#22797;&#20102;&#27969;&#34892;&#30340;CNN&#27169;&#22411;Lenet&#65292;Alexnet&#21644;VGGnet16&#30340;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2311.00579</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#27969;&#25512;&#29702;&#21152;&#36895;&#22120;&#20013;&#30340;&#20391;&#20449;&#36947;&#20998;&#26512;&#25581;&#31034;CNN&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Revealing CNN Architectures via Side-Channel Analysis in Dataflow-based Inference Accelerators. (arXiv:2311.00579v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35780;&#20272;&#25968;&#25454;&#27969;&#21152;&#36895;&#22120;&#19978;&#30340;&#20391;&#20449;&#36947;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25915;&#20987;&#26041;&#27861;&#26469;&#24674;&#22797;CNN&#27169;&#22411;&#30340;&#26550;&#26500;&#12290;&#35813;&#25915;&#20987;&#21033;&#29992;&#20102;&#25968;&#25454;&#27969;&#26144;&#23556;&#30340;&#25968;&#25454;&#37325;&#29992;&#20197;&#21450;&#26550;&#26500;&#32447;&#32034;&#65292;&#25104;&#21151;&#24674;&#22797;&#20102;&#27969;&#34892;&#30340;CNN&#27169;&#22411;Lenet&#65292;Alexnet&#21644;VGGnet16&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#12290;&#26368;&#36817;&#22312;&#22522;&#20110;&#25968;&#25454;&#27969;&#30340;CNN&#21152;&#36895;&#22120;&#30340;&#36827;&#23637;&#20351;&#24471;CNN&#25512;&#29702;&#21487;&#20197;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#36827;&#34892;&#12290;&#36825;&#20123;&#25968;&#25454;&#27969;&#21152;&#36895;&#22120;&#21033;&#29992;&#21367;&#31215;&#23618;&#30340;&#22266;&#26377;&#25968;&#25454;&#37325;&#29992;&#26469;&#39640;&#25928;&#22788;&#29702;CNN&#27169;&#22411;&#12290;&#38544;&#34255;CNN&#27169;&#22411;&#30340;&#26550;&#26500;&#23545;&#20110;&#38544;&#31169;&#21644;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;&#20869;&#23384;&#30340;&#20391;&#20449;&#36947;&#20449;&#24687;&#65292;&#20197;&#20174;&#25968;&#25454;&#27969;&#21152;&#36895;&#22120;&#20013;&#24674;&#22797;CNN&#26550;&#26500;&#12290;&#25152;&#25552;&#20986;&#30340;&#25915;&#20987;&#21033;&#29992;&#20102;CNN&#21152;&#36895;&#22120;&#19978;&#25968;&#25454;&#27969;&#26144;&#23556;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#25968;&#25454;&#37325;&#29992;&#20197;&#21450;&#26550;&#26500;&#32447;&#32034;&#26469;&#24674;&#22797;CNN&#27169;&#22411;&#30340;&#32467;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#20391;&#20449;&#36947;&#25915;&#20987;&#21487;&#20197;&#24674;&#22797;&#27969;&#34892;&#30340;CNN&#27169;&#22411;Lenet&#65292;Alexnet&#21644;VGGnet16&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolution Neural Networks (CNNs) are widely used in various domains. Recent advances in dataflow-based CNN accelerators have enabled CNN inference in resource-constrained edge devices. These dataflow accelerators utilize inherent data reuse of convolution layers to process CNN models efficiently. Concealing the architecture of CNN models is critical for privacy and security. This paper evaluates memory-based side-channel information to recover CNN architectures from dataflow-based CNN inference accelerators. The proposed attack exploits spatial and temporal data reuse of the dataflow mapping on CNN accelerators and architectural hints to recover the structure of CNN models. Experimental results demonstrate that our proposed side-channel attack can recover the structures of popular CNN models, namely Lenet, Alexnet, and VGGnet16.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;NoC&#26550;&#26500;&#20013;&#29616;&#26377;&#21311;&#21517;&#36335;&#30001;&#21327;&#35758;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#29616;&#26377;&#30340;&#21311;&#21517;&#36335;&#30001;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27969;&#30456;&#20851;&#25915;&#20987;&#26131;&#21463;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#21311;&#21517;&#36335;&#30001;&#65292;&#20351;&#29992;&#27969;&#37327;&#28151;&#28102;&#25216;&#26415;&#65292;&#21487;&#20197;&#25269;&#24481;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27969;&#30456;&#20851;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2309.15687</link><description>&lt;p&gt;
&#25171;&#30772;NoC&#21311;&#21517;&#24615;&#20351;&#29992;&#27969;&#30456;&#20851;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Breaking NoC Anonymity using Flow Correlation Attack. (arXiv:2309.15687v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;NoC&#26550;&#26500;&#20013;&#29616;&#26377;&#21311;&#21517;&#36335;&#30001;&#21327;&#35758;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#29616;&#26377;&#30340;&#21311;&#21517;&#36335;&#30001;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27969;&#30456;&#20851;&#25915;&#20987;&#26131;&#21463;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#21311;&#21517;&#36335;&#30001;&#65292;&#20351;&#29992;&#27969;&#37327;&#28151;&#28102;&#25216;&#26415;&#65292;&#21487;&#20197;&#25269;&#24481;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27969;&#30456;&#20851;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#29255;&#19978;&#20114;&#36830;&#65288;NoC&#65289;&#24191;&#27867;&#29992;&#20316;&#24403;&#20170;&#22810;&#26680;&#29255;&#19978;&#31995;&#32479;&#65288;SoC&#65289;&#35774;&#35745;&#20013;&#30340;&#20869;&#37096;&#36890;&#20449;&#32467;&#26500;&#12290;&#29255;&#19978;&#36890;&#20449;&#30340;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#21033;&#29992;&#20849;&#20139;&#30340;NoC&#20013;&#30340;&#20219;&#20309;&#28431;&#27934;&#23545;&#25915;&#20987;&#32773;&#26469;&#35828;&#37117;&#26159;&#19968;&#20010;&#23500;&#30719;&#12290;NoC&#23433;&#20840;&#20381;&#36182;&#20110;&#23545;&#21508;&#31181;&#25915;&#20987;&#30340;&#26377;&#25928;&#38450;&#33539;&#25514;&#26045;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;NoC&#26550;&#26500;&#20013;&#29616;&#26377;&#21311;&#21517;&#36335;&#30001;&#21327;&#35758;&#30340;&#23433;&#20840;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#20316;&#20986;&#20102;&#20004;&#20010;&#37325;&#35201;&#36129;&#29486;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#30340;&#21311;&#21517;&#36335;&#30001;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#27969;&#30456;&#20851;&#25915;&#20987;&#26159;&#26131;&#21463;&#25915;&#20987;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#21311;&#21517;&#36335;&#30001;&#65292;&#20351;&#29992;&#27969;&#37327;&#28151;&#28102;&#25216;&#26415;&#65292;&#21487;&#20197;&#25269;&#24481;&#22522;&#20110;ML&#30340;&#27969;&#30456;&#20851;&#25915;&#20987;&#12290;&#20351;&#29992;&#23454;&#38469;&#21644;&#21512;&#25104;&#27969;&#37327;&#36827;&#34892;&#30340;&#23454;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25915;&#20987;&#33021;&#22815;&#25104;&#21151;&#22320;&#23545;&#25239;NoC&#26550;&#26500;&#20013;&#26368;&#20808;&#36827;&#30340;&#21311;&#21517;&#36335;&#30001;&#65292;&#23545;&#20110;&#22810;&#31181;&#27969;&#37327;&#27169;&#24335;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#39640;&#36798;99&#65285;&#65292;&#21516;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network-on-Chip (NoC) is widely used as the internal communication fabric in today's multicore System-on-Chip (SoC) designs. Security of the on-chip communication is crucial because exploiting any vulnerability in shared NoC would be a goldmine for an attacker. NoC security relies on effective countermeasures against diverse attacks. We investigate the security strength of existing anonymous routing protocols in NoC architectures. Specifically, this paper makes two important contributions. We show that the existing anonymous routing is vulnerable to machine learning (ML) based flow correlation attacks on NoCs. We propose a lightweight anonymous routing that use traffic obfuscation techniques which can defend against ML-based flow correlation attacks. Experimental studies using both real and synthetic traffic reveal that our proposed attack is successful against state-of-the-art anonymous routing in NoC architectures with a high accuracy (up to 99%) for diverse traffic patterns, while o
&lt;/p&gt;</description></item></channel></rss>