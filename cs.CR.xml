<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#21327;&#20316;&#21452;&#26426;&#22120;&#23398;&#20064;&#65288;DC-DML&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#25252;&#20998;&#24067;&#24335;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#26465;&#20214;&#24179;&#22343;&#27835;&#30103;&#25928;&#26524;&#65288;CATE&#65289;&#27169;&#22411;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#35813;&#26041;&#27861;&#30340;&#19977;&#20010;&#20027;&#35201;&#36129;&#29486;&#26159;&#65306;&#23454;&#29616;&#20102;&#23545;&#20998;&#24067;&#24335;&#25968;&#25454;&#19978;&#30340;&#38750;&#36845;&#20195;&#36890;&#20449;&#30340;&#21322;&#21442;&#25968;CATE&#27169;&#22411;&#30340;&#20272;&#35745;&#21644;&#27979;&#35797;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02672</link><description>&lt;p&gt;
&#23545;&#20998;&#24067;&#24335;&#25968;&#25454;&#30340;&#26465;&#20214;&#24179;&#22343;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#65306;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Estimation of conditional average treatment effects on distributed data: A privacy-preserving approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#21327;&#20316;&#21452;&#26426;&#22120;&#23398;&#20064;&#65288;DC-DML&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#25252;&#20998;&#24067;&#24335;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#26465;&#20214;&#24179;&#22343;&#27835;&#30103;&#25928;&#26524;&#65288;CATE&#65289;&#27169;&#22411;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#35813;&#26041;&#27861;&#30340;&#19977;&#20010;&#20027;&#35201;&#36129;&#29486;&#26159;&#65306;&#23454;&#29616;&#20102;&#23545;&#20998;&#24067;&#24335;&#25968;&#25454;&#19978;&#30340;&#38750;&#36845;&#20195;&#36890;&#20449;&#30340;&#21322;&#21442;&#25968;CATE&#27169;&#22411;&#30340;&#20272;&#35745;&#21644;&#27979;&#35797;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#31561;&#21508;&#20010;&#39046;&#22495;&#20013;&#65292;&#23545;&#26465;&#20214;&#24179;&#22343;&#27835;&#30103;&#25928;&#26524;&#65288;CATEs&#65289;&#30340;&#20272;&#35745;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#35838;&#39064;&#12290;&#22914;&#26524;&#20998;&#24067;&#22312;&#22810;&#20010;&#21442;&#19982;&#26041;&#20043;&#38388;&#30340;&#25968;&#25454;&#21487;&#20197;&#38598;&#20013;&#65292;&#21487;&#20197;&#23545;CATEs&#36827;&#34892;&#39640;&#31934;&#24230;&#30340;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#36825;&#20123;&#25968;&#25454;&#21253;&#21547;&#38544;&#31169;&#20449;&#24687;&#65292;&#21017;&#24456;&#38590;&#36827;&#34892;&#25968;&#25454;&#32858;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25968;&#25454;&#21327;&#20316;&#21452;&#26426;&#22120;&#23398;&#20064;&#65288;DC-DML&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#25252;&#20998;&#24067;&#24335;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;CATE&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#23545;&#35813;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#24635;&#32467;&#22914;&#19979;&#19977;&#28857;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#20998;&#24067;&#24335;&#25968;&#25454;&#19978;&#36827;&#34892;&#38750;&#36845;&#20195;&#36890;&#20449;&#30340;&#21322;&#21442;&#25968;CATE&#27169;&#22411;&#30340;&#20272;&#35745;&#21644;&#27979;&#35797;&#12290;&#21322;&#21442;&#25968;&#25110;&#38750;&#21442;&#25968;&#30340;CATE&#27169;&#22411;&#33021;&#22815;&#27604;&#21442;&#25968;&#27169;&#22411;&#26356;&#31283;&#20581;&#22320;&#36827;&#34892;&#20272;&#35745;&#21644;&#27979;&#35797;&#65292;&#23545;&#20110;&#27169;&#22411;&#20559;&#24046;&#30340;&#40065;&#26834;&#24615;&#26356;&#24378;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#25552;&#20986;&#26377;&#25928;&#30340;&#36890;&#20449;&#26041;&#27861;&#26469;&#20272;&#35745;&#21644;&#27979;&#35797;&#36825;&#20123;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimation of conditional average treatment effects (CATEs) is an important topic in various fields such as medical and social sciences. CATEs can be estimated with high accuracy if distributed data across multiple parties can be centralized. However, it is difficult to aggregate such data if they contain privacy information. To address this issue, we proposed data collaboration double machine learning (DC-DML), a method that can estimate CATE models with privacy preservation of distributed data, and evaluated the method through numerical experiments. Our contributions are summarized in the following three points. First, our method enables estimation and testing of semi-parametric CATE models without iterative communication on distributed data. Semi-parametric or non-parametric CATE models enable estimation and testing that is more robust to model mis-specification than parametric models. However, to our knowledge, no communication-efficient method has been proposed for estimating and 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#25286;&#20998;&#23398;&#20064;&#30340;&#34987;&#21160;&#25512;&#29702;&#25915;&#20987;&#26694;&#26550;SDAR&#65292;&#36890;&#36807;&#21033;&#29992;&#36741;&#21161;&#25968;&#25454;&#21644;&#23545;&#25239;&#24615;&#27491;&#21017;&#21270;&#26469;&#25512;&#26029;&#23458;&#25143;&#31471;&#30340;&#31169;&#26377;&#29305;&#24449;&#21644;&#26631;&#31614;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#19982;&#20027;&#21160;&#25915;&#20987;&#30456;&#24403;&#30340;&#25915;&#20987;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.10483</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#24615;&#27491;&#21017;&#21270;&#23545;&#25286;&#20998;&#23398;&#20064;&#36827;&#34892;&#34989;&#20987;&#30340;&#34987;&#21160;&#25512;&#29702;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Passive Inference Attacks on Split Learning via Adversarial Regularization. (arXiv:2310.10483v4 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10483
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#25286;&#20998;&#23398;&#20064;&#30340;&#34987;&#21160;&#25512;&#29702;&#25915;&#20987;&#26694;&#26550;SDAR&#65292;&#36890;&#36807;&#21033;&#29992;&#36741;&#21161;&#25968;&#25454;&#21644;&#23545;&#25239;&#24615;&#27491;&#21017;&#21270;&#26469;&#25512;&#26029;&#23458;&#25143;&#31471;&#30340;&#31169;&#26377;&#29305;&#24449;&#21644;&#26631;&#31614;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#19982;&#20027;&#21160;&#25915;&#20987;&#30456;&#24403;&#30340;&#25915;&#20987;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25286;&#20998;&#23398;&#20064;(SL)&#24050;&#25104;&#20026;&#20256;&#32479;&#32852;&#37030;&#23398;&#20064;&#30340;&#19968;&#31181;&#23454;&#29992;&#19988;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#34429;&#28982;&#20197;&#21069;&#25915;&#20987;SL&#30340;&#23581;&#35797;&#24448;&#24448;&#20381;&#36182;&#20110;&#36807;&#20110;&#24378;&#30828;&#30340;&#20551;&#35774;&#25110;&#32773;&#38024;&#23545;&#26131;&#21463;&#25915;&#20987;&#30340;&#27169;&#22411;&#65292;&#20294;&#25105;&#20204;&#35797;&#22270;&#24320;&#21457;&#26356;&#21152;&#23454;&#29992;&#30340;&#25915;&#20987;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;SDAR&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#25317;&#26377;&#35802;&#23454;&#20294;&#22909;&#22855;&#30340;&#26381;&#21153;&#22120;&#30340;SL&#30340;&#26032;&#25915;&#20987;&#26694;&#26550;&#12290;SDAR&#21033;&#29992;&#36741;&#21161;&#25968;&#25454;&#21644;&#23545;&#25239;&#24615;&#27491;&#21017;&#21270;&#26469;&#23398;&#20064;&#23458;&#25143;&#31471;&#31169;&#26377;&#27169;&#22411;&#30340;&#21487;&#35299;&#30721;&#27169;&#25311;&#22120;&#65292;&#22312;&#22522;&#26412;SL&#19979;&#21487;&#20197;&#26377;&#25928;&#22320;&#25512;&#26029;&#20986;&#23458;&#25143;&#31471;&#30340;&#31169;&#26377;&#29305;&#24449;&#65292;&#24182;&#22312;U&#22411;SL&#19979;&#25512;&#26029;&#20986;&#29305;&#24449;&#21644;&#26631;&#31614;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#25552;&#20986;&#30340;&#25915;&#20987;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#23454;&#38469;&#30340;&#22330;&#26223;&#20013;&#65292;&#29616;&#26377;&#30340;&#34987;&#21160;&#25915;&#20987;&#38590;&#20197;&#26377;&#25928;&#22320;&#37325;&#24314;&#23458;&#25143;&#31471;&#30340;&#31169;&#26377;&#25968;&#25454;&#26102;&#65292;SDAR&#22987;&#32456;&#23454;&#29616;&#20102;&#19982;&#20027;&#21160;&#25915;&#20987;&#30456;&#24403;&#30340;&#25915;&#20987;&#24615;&#33021;&#12290;&#22312;CIFAR-10&#19978;&#65292;&#22312;&#28145;&#24230;&#25286;&#20998;&#27700;&#24179;&#20026;7&#30340;&#24773;&#20917;&#19979;&#65292;SDAR&#36798;&#21040;&#20102;&#25915;&#20987;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Split Learning (SL) has emerged as a practical and efficient alternative to traditional federated learning. While previous attempts to attack SL have often relied on overly strong assumptions or targeted easily exploitable models, we seek to develop more practical attacks. We introduce SDAR, a novel attack framework against SL with an honest-but-curious server. SDAR leverages auxiliary data and adversarial regularization to learn a decodable simulator of the client's private model, which can effectively infer the client's private features under the vanilla SL, and both features and labels under the U-shaped SL. We perform extensive experiments in both configurations to validate the effectiveness of our proposed attacks. Notably, in challenging but practical scenarios where existing passive attacks struggle to reconstruct the client's private data effectively, SDAR consistently achieves attack performance comparable to active attacks. On CIFAR-10, at the deep split level of 7, SDAR achi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#20844;&#20849;&#25968;&#25454;&#30340;&#26368;&#20248;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#35757;&#32451;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#26102;&#22914;&#20309;&#21033;&#29992;&#20844;&#20849;&#25968;&#25454;&#25552;&#39640;&#20934;&#30830;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.15056</link><description>&lt;p&gt;
&#20855;&#26377;&#20844;&#20849;&#25968;&#25454;&#30340;&#26368;&#20248;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimal Differentially Private Learning with Public Data. (arXiv:2306.15056v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#20844;&#20849;&#25968;&#25454;&#30340;&#26368;&#20248;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#35757;&#32451;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#26102;&#22914;&#20309;&#21033;&#29992;&#20844;&#20849;&#25968;&#25454;&#25552;&#39640;&#20934;&#30830;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#33021;&#22815;&#30830;&#20445;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19981;&#27844;&#28431;&#31169;&#23494;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#24046;&#20998;&#38544;&#31169;&#30340;&#20195;&#20215;&#26159;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#38477;&#20302;&#25110;&#26679;&#26412;&#22797;&#26434;&#24230;&#22686;&#21152;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#21487;&#33021;&#21487;&#20197;&#35775;&#38382;&#19981;&#28041;&#21450;&#38544;&#31169;&#38382;&#39064;&#30340;&#36741;&#21161;&#20844;&#20849;&#25968;&#25454;&#12290;&#36825;&#20419;&#20351;&#20102;&#26368;&#36817;&#30740;&#31350;&#20844;&#20849;&#25968;&#25454;&#22312;&#25552;&#39640;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;&#26377;&#19968;&#23450;&#25968;&#37327;&#30340;&#20844;&#20849;&#25968;&#25454;&#65292;&#24182;&#35299;&#20915;&#20197;&#19979;&#22522;&#26412;&#24320;&#25918;&#38382;&#39064;&#65306;1.&#22312;&#26377;&#20844;&#20849;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#35757;&#32451;&#22522;&#20110;&#31169;&#26377;&#25968;&#25454;&#38598;&#30340;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#30340;&#26368;&#20248;&#65288;&#26368;&#22351;&#24773;&#20917;&#65289;&#35823;&#24046;&#26159;&#22810;&#23569;&#65311;&#21738;&#20123;&#31639;&#27861;&#26159;&#26368;&#20248;&#30340;&#65311;2.&#22914;&#20309;&#21033;&#29992;&#20844;&#20849;&#25968;&#25454;&#22312;&#23454;&#36341;&#20013;&#25913;&#36827;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#35757;&#32451;&#65311;&#25105;&#20204;&#22312;&#26412;&#22320;&#27169;&#22411;&#21644;&#20013;&#24515;&#27169;&#22411;&#30340;&#24046;&#20998;&#38544;&#31169;&#38382;&#39064;&#19979;&#32771;&#34385;&#36825;&#20123;&#38382;&#39064;&#12290;&#20026;&#20102;&#22238;&#31572;&#31532;&#19968;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#19977;&#20010;&#22522;&#26412;&#38382;&#39064;&#30340;&#26368;&#20248;&#35823;&#24046;&#29575;&#30340;&#32039;&#23494;&#65288;&#26368;&#39640;&#24120;&#25968;&#22240;&#23376;&#65289;&#19979;&#30028;&#21644;&#19978;&#30028;&#12290;&#36825;&#19977;&#20010;&#38382;&#39064;&#26159;&#65306;&#22343;&#20540;&#20272;&#35745;&#65292;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#21644;&#20984;&#22855;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differential Privacy (DP) ensures that training a machine learning model does not leak private data. However, the cost of DP is lower model accuracy or higher sample complexity. In practice, we may have access to auxiliary public data that is free of privacy concerns. This has motivated the recent study of what role public data might play in improving the accuracy of DP models. In this work, we assume access to a given amount of public data and settle the following fundamental open questions: 1. What is the optimal (worst-case) error of a DP model trained over a private data set while having access to side public data? What algorithms are optimal? 2. How can we harness public data to improve DP model training in practice? We consider these questions in both the local and central models of DP. To answer the first question, we prove tight (up to constant factors) lower and upper bounds that characterize the optimal error rates of three fundamental problems: mean estimation, empirical ris
&lt;/p&gt;</description></item><item><title>DNN-Defender&#26159;&#19968;&#31181;&#22522;&#20110;DRAM&#30340;&#21463;&#23475;&#32773;&#37325;&#28857;&#38450;&#24481;&#26426;&#21046;&#65292;&#36866;&#29992;&#20110;&#37327;&#21270;DNN&#65292;&#21033;&#29992;&#20869;&#23384;&#20013;&#20132;&#25442;&#30340;&#28508;&#21147;&#20197;&#25269;&#24481;&#26377;&#38024;&#23545;&#24615;&#30340;&#20301;&#32763;&#36716;&#25915;&#20987;&#65292;&#21487;&#20197;&#25552;&#20379;&#39640;&#27700;&#24179;&#30340;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2305.08034</link><description>&lt;p&gt;
DNN-Defender: &#19968;&#31181;&#29992;&#20110;&#23545;&#25239; Adversarial Weight Attack &#30340;&#20869;&#23384;&#20013;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38450;&#24481;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
DNN-Defender: An in-DRAM Deep Neural Network Defense Mechanism for Adversarial Weight Attack. (arXiv:2305.08034v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08034
&lt;/p&gt;
&lt;p&gt;
DNN-Defender&#26159;&#19968;&#31181;&#22522;&#20110;DRAM&#30340;&#21463;&#23475;&#32773;&#37325;&#28857;&#38450;&#24481;&#26426;&#21046;&#65292;&#36866;&#29992;&#20110;&#37327;&#21270;DNN&#65292;&#21033;&#29992;&#20869;&#23384;&#20013;&#20132;&#25442;&#30340;&#28508;&#21147;&#20197;&#25269;&#24481;&#26377;&#38024;&#23545;&#24615;&#30340;&#20301;&#32763;&#36716;&#25915;&#20987;&#65292;&#21487;&#20197;&#25552;&#20379;&#39640;&#27700;&#24179;&#30340;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#22312;&#35768;&#22810;&#23433;&#20840;&#25935;&#24863;&#39046;&#22495;&#20013;&#30340;&#37096;&#32626;&#65292;&#26426;&#22120;&#23398;&#20064;&#23433;&#20840;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#21033;&#29992;DRAM&#30340;RowHammer&#28431;&#27934;&#65292;&#20197;&#30830;&#23450;&#24615;&#21644;&#31934;&#30830;&#24615;&#22320;&#32763;&#36716;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#27169;&#22411;&#26435;&#37325;&#30340;&#20301;&#65292;&#20174;&#32780;&#24433;&#21709;&#25512;&#26029;&#20934;&#30830;&#24615;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#26426;&#21046;&#26159;&#22522;&#20110;&#36719;&#20214;&#30340;&#65292;&#20363;&#22914;&#37325;&#26500;&#26435;&#37325;&#38656;&#35201;&#26114;&#36149;&#30340;&#35757;&#32451;&#24320;&#38144;&#25110;&#24615;&#33021;&#19979;&#38477;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22522;&#20110;&#36890;&#29992;&#30828;&#20214;&#30340;&#21463;&#23475;&#32773;/&#25915;&#20987;&#32773;&#37325;&#28857;&#38450;&#24481;&#26426;&#21046;&#20250;&#23548;&#33268;&#26114;&#36149;&#30340;&#30828;&#20214;&#24320;&#38144;&#65292;&#24182;&#20445;&#30041;&#21463;&#23475;&#32773;&#21644;&#25915;&#20987;&#32773;&#34892;&#20043;&#38388;&#30340;&#31354;&#38388;&#36830;&#25509;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#38024;&#23545;&#37327;&#21270;DNN&#37327;&#36523;&#23450;&#21046;&#30340;&#22522;&#20110;DRAM&#30340;&#21463;&#23475;&#32773;&#37325;&#28857;&#38450;&#24481;&#26426;&#21046;&#65292;&#31216;&#20026;DNN-Defender&#65292;&#21033;&#29992;&#20102;&#20869;&#23384;&#20013;&#20132;&#25442;&#30340;&#28508;&#21147;&#20197;&#25269;&#24481;&#26377;&#38024;&#23545;&#24615;&#30340;&#20301;&#32763;&#36716;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;DNN-Defender&#21487;&#20197;&#25552;&#20379;&#39640;&#27700;&#24179;&#30340;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
With deep learning deployed in many security-sensitive areas, machine learning security is becoming progressively important. Recent studies demonstrate attackers can exploit system-level techniques exploiting the RowHammer vulnerability of DRAM to deterministically and precisely flip bits in Deep Neural Networks (DNN) model weights to affect inference accuracy. The existing defense mechanisms are software-based, such as weight reconstruction requiring expensive training overhead or performance degradation. On the other hand, generic hardware-based victim-/aggressor-focused mechanisms impose expensive hardware overheads and preserve the spatial connection between victim and aggressor rows. In this paper, we present the first DRAM-based victim-focused defense mechanism tailored for quantized DNNs, named DNN-Defender that leverages the potential of in-DRAM swapping to withstand the targeted bit-flip attacks. Our results indicate that DNN-Defender can deliver a high level of protection dow
&lt;/p&gt;</description></item></channel></rss>