<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#25552;&#20986;&#20102;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#30340;&#38543;&#26426;&#38797;&#28857;&#38382;&#39064;&#30340;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#20960;&#20046;&#19982;&#32500;&#24230;&#26080;&#20851;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#36825;&#31181;&#36895;&#29575;&#20043;&#21069;&#21482;&#38024;&#23545;&#21452;&#32447;&#24615;&#30446;&#26631;&#24050;&#30693;&#12290;</title><link>https://arxiv.org/abs/2403.02912</link><description>&lt;p&gt;
&#20855;&#26377;&#20960;&#20046;&#19982;&#32500;&#24230;&#26080;&#20851;&#25910;&#25947;&#36895;&#29575;&#30340;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#65292;&#29992;&#20110;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#30340;&#38543;&#26426;&#38797;&#28857;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Mirror Descent Algorithms with Nearly Dimension-Independent Rates for Differentially-Private Stochastic Saddle-Point Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02912
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#30340;&#38543;&#26426;&#38797;&#28857;&#38382;&#39064;&#30340;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#20960;&#20046;&#19982;&#32500;&#24230;&#26080;&#20851;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#36825;&#31181;&#36895;&#29575;&#20043;&#21069;&#21482;&#38024;&#23545;&#21452;&#32447;&#24615;&#30446;&#26631;&#24050;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#38754;&#20307;&#35774;&#32622;&#20013;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#30340;&#38543;&#26426;&#65288;&#20984;&#20985;&#65289;&#38797;&#28857;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#38543;&#26426;&#38236;&#20687;&#19979;&#38477;&#30340;&#65288;&#1013;&#65292;&#948;&#65289;-DP&#31639;&#27861;&#65292;&#20854;&#23454;&#29616;&#20102;&#39044;&#26399;&#23545;&#20598;&#38388;&#38553;&#30340;&#20960;&#20046;&#19982;&#32500;&#24230;&#26080;&#20851;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#36825;&#31181;&#20445;&#35777;&#22312;&#20197;&#21069;&#21482;&#38024;&#23545;&#21452;&#32447;&#24615;&#30446;&#26631;&#24050;&#30693;&#12290;&#23545;&#20110;&#20984;&#20985;&#21644;&#19968;&#38454;&#24179;&#28369;&#38543;&#26426;&#30446;&#26631;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#19968;&#20010;&#29575;&#65292;&#21363;sqrt(log(d)/n) + (log(d)^{3/2}/[n&#1013;])^{1/3}&#65292;&#20854;&#20013;d&#26159;&#38382;&#39064;&#30340;&#32500;&#24230;&#65292;n&#26159;&#25968;&#25454;&#38598;&#22823;&#23567;&#12290;&#22312;&#39069;&#22806;&#30340;&#20108;&#38454;&#24179;&#28369;&#24615;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#23558;&#39044;&#26399;&#38388;&#38553;&#30340;&#36895;&#29575;&#25913;&#36827;&#20026;sqrt(log(d)/n) + (log(d)^{3/2}/[n&#1013;])^{2/5}&#12290;&#22312;&#36825;&#31181;&#39069;&#22806;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#20351;&#29992;&#20559;&#24046;&#20943;&#23569;&#30340;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#35777;&#26126;&#20102;&#23545;&#20598;&#38388;&#38553;&#21463;&#24120;&#25968;&#25104;&#21151;&#27010;&#29575;&#30340;&#30028;&#20026;log(d)/sqrt(n) + log(d)/[n&#1013;]^{1/2}&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02912v1 Announce Type: cross  Abstract: We study the problem of differentially-private (DP) stochastic (convex-concave) saddle-points in the polyhedral setting. We propose $(\varepsilon, \delta)$-DP algorithms based on stochastic mirror descent that attain nearly dimension-independent convergence rates for the expected duality gap, a type of guarantee that was known before only for bilinear objectives. For convex-concave and first-order-smooth stochastic objectives, our algorithms attain a rate of $\sqrt{\log(d)/n} + (\log(d)^{3/2}/[n\varepsilon])^{1/3}$, where $d$ is the dimension of the problem and $n$ the dataset size. Under an additional second-order-smoothness assumption, we improve the rate on the expected gap to $\sqrt{\log(d)/n} + (\log(d)^{3/2}/[n\varepsilon])^{2/5}$. Under this additional assumption, we also show, by using bias-reduced gradient estimators, that the duality gap is bounded by $\log(d)/\sqrt{n} + \log(d)/[n\varepsilon]^{1/2}$ with constant success pro
&lt;/p&gt;</description></item></channel></rss>