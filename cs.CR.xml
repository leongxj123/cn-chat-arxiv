<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;FLTrojan&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#26435;&#37325;&#31713;&#25913;&#65292;&#20174;&#32852;&#37030;&#35821;&#35328;&#27169;&#22411;&#20013;&#27844;&#38706;&#38544;&#31169;&#25935;&#24863;&#29992;&#25143;&#25968;&#25454;&#12290;&#36890;&#36807;&#35266;&#23519;&#21040;FL&#20013;&#20013;&#38388;&#36718;&#27425;&#30340;&#27169;&#22411;&#24555;&#29031;&#21487;&#20197;&#24341;&#36215;&#26356;&#22823;&#30340;&#38544;&#31169;&#27844;&#38706;&#65292;&#24182;&#21457;&#29616;&#38544;&#31169;&#27844;&#38706;&#21487;&#20197;&#36890;&#36807;&#31713;&#25913;&#27169;&#22411;&#30340;&#36873;&#25321;&#24615;&#26435;&#37325;&#26469;&#21152;&#21095;&#12290;</title><link>http://arxiv.org/abs/2310.16152</link><description>&lt;p&gt;
FLTrojan: &#36890;&#36807;&#36873;&#25321;&#24615;&#26435;&#37325;&#31713;&#25913;&#23545;&#32852;&#37030;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38544;&#31169;&#27844;&#38706;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
FLTrojan: Privacy Leakage Attacks against Federated Language Models Through Selective Weight Tampering. (arXiv:2310.16152v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;FLTrojan&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#26435;&#37325;&#31713;&#25913;&#65292;&#20174;&#32852;&#37030;&#35821;&#35328;&#27169;&#22411;&#20013;&#27844;&#38706;&#38544;&#31169;&#25935;&#24863;&#29992;&#25143;&#25968;&#25454;&#12290;&#36890;&#36807;&#35266;&#23519;&#21040;FL&#20013;&#20013;&#38388;&#36718;&#27425;&#30340;&#27169;&#22411;&#24555;&#29031;&#21487;&#20197;&#24341;&#36215;&#26356;&#22823;&#30340;&#38544;&#31169;&#27844;&#38706;&#65292;&#24182;&#21457;&#29616;&#38544;&#31169;&#27844;&#38706;&#21487;&#20197;&#36890;&#36807;&#31713;&#25913;&#27169;&#22411;&#30340;&#36873;&#25321;&#24615;&#26435;&#37325;&#26469;&#21152;&#21095;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(Federated learning, FL)&#27491;&#25104;&#20026;&#35768;&#22810;&#25216;&#26415;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#32452;&#20214;&#65292;&#21253;&#25324;&#35821;&#35328;&#24314;&#27169;&#39046;&#22495;&#65292;&#20854;&#20013;&#20010;&#20307;FL&#21442;&#19982;&#32773;&#22312;&#20854;&#26412;&#22320;&#25968;&#25454;&#38598;&#20013;&#24448;&#24448;&#20855;&#26377;&#25935;&#24863;&#30340;&#25991;&#26412;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;&#32852;&#37030;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#31169;&#27844;&#38706;&#31243;&#24230;&#24182;&#19981;&#31616;&#21333;&#65292;&#29616;&#26377;&#30340;&#25915;&#20987;&#21482;&#26159;&#35797;&#22270;&#25552;&#21462;&#25968;&#25454;&#65292;&#32780;&#19981;&#32771;&#34385;&#25968;&#25454;&#30340;&#25935;&#24863;&#24615;&#25110;&#22825;&#30495;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20851;&#20110;&#20174;&#32852;&#37030;&#35821;&#35328;&#27169;&#22411;&#20013;&#27844;&#38706;&#38544;&#31169;&#25935;&#24863;&#29992;&#25143;&#25968;&#25454;&#30340;&#20004;&#20010;&#26032;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;FL&#20013;&#20013;&#38388;&#36718;&#27425;&#30340;&#27169;&#22411;&#24555;&#29031;&#27604;&#26368;&#32456;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#36896;&#25104;&#26356;&#22823;&#30340;&#38544;&#31169;&#27844;&#38706;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30830;&#23450;&#38544;&#31169;&#27844;&#38706;&#21487;&#20197;&#36890;&#36807;&#31713;&#25913;&#27169;&#22411;&#30340;&#36873;&#25321;&#24615;&#26435;&#37325;&#26469;&#21152;&#21095;&#65292;&#36825;&#20123;&#26435;&#37325;&#29305;&#21035;&#36127;&#36131;&#35760;&#24518;&#25935;&#24863;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24694;&#24847;&#23458;&#25143;&#31471;&#22914;&#20309;&#22312;FL&#20013;&#27844;&#38706;&#20854;&#20182;&#29992;&#25143;&#30340;&#38544;&#31169;&#25935;&#24863;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is becoming a key component in many technology-based applications including language modeling -- where individual FL participants often have privacy-sensitive text data in their local datasets. However, realizing the extent of privacy leakage in federated language models is not straightforward and the existing attacks only intend to extract data regardless of how sensitive or naive it is. To fill this gap, in this paper, we introduce two novel findings with regard to leaking privacy-sensitive user data from federated language models. Firstly, we make a key observation that model snapshots from the intermediate rounds in FL can cause greater privacy leakage than the final trained model. Secondly, we identify that privacy leakage can be aggravated by tampering with a model's selective weights that are specifically responsible for memorizing the sensitive training data. We show how a malicious client can leak the privacy-sensitive data of some other user in FL even
&lt;/p&gt;</description></item></channel></rss>