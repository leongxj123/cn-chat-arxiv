<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#22522;&#20110;&#38598;&#21512;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#40065;&#26834;&#24615;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24418;&#24335;&#21270;&#39564;&#35777;&#65292;&#24182;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#31616;&#21270;&#39564;&#35777;&#36807;&#31243;&#24182;&#26377;&#25928;&#35757;&#32451;&#20986;&#26131;&#20110;&#39564;&#35777;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2401.14961</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#30340;&#31471;&#21040;&#31471;&#22522;&#20110;&#38598;&#21512;&#30340;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
End-To-End Set-Based Training for Neural Network Verification. (arXiv:2401.14961v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#22522;&#20110;&#38598;&#21512;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#40065;&#26834;&#24615;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24418;&#24335;&#21270;&#39564;&#35777;&#65292;&#24182;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#31616;&#21270;&#39564;&#35777;&#36807;&#31243;&#24182;&#26377;&#25928;&#35757;&#32451;&#20986;&#26131;&#20110;&#39564;&#35777;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#21363;&#24494;&#23567;&#30340;&#36755;&#20837;&#25200;&#21160;&#21487;&#33021;&#23548;&#33268;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#20135;&#29983;&#37325;&#22823;&#21464;&#21270;&#12290;&#23433;&#20840;&#20851;&#38190;&#29615;&#22659;&#38656;&#35201;&#23545;&#36755;&#20837;&#25200;&#21160;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#21644;&#24418;&#24335;&#21270;&#39564;&#35777;&#40065;&#26834;&#24615;&#31070;&#32463;&#32593;&#32476;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#39318;&#27425;&#37319;&#29992;&#31471;&#21040;&#31471;&#22522;&#20110;&#38598;&#21512;&#30340;&#35757;&#32451;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#35813;&#35757;&#32451;&#26041;&#27861;&#33021;&#22815;&#35757;&#32451;&#20986;&#21487;&#36827;&#34892;&#24418;&#24335;&#21270;&#39564;&#35777;&#30340;&#40065;&#26834;&#24615;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#26041;&#27861;&#33021;&#22815;&#22823;&#22823;&#31616;&#21270;&#24050;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#21518;&#32493;&#24418;&#24335;&#21270;&#40065;&#26834;&#24615;&#39564;&#35777;&#36807;&#31243;&#12290;&#30456;&#27604;&#20110;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#38598;&#21512;&#30340;&#35745;&#31639;&#26469;&#35757;&#32451;&#25972;&#20010;&#25200;&#21160;&#36755;&#20837;&#38598;&#21512;&#19978;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#22522;&#20110;&#38598;&#21512;&#30340;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35757;&#32451;&#20986;&#26131;&#20110;&#39564;&#35777;&#30340;&#40065;&#26834;&#24615;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are vulnerable to adversarial attacks, i.e., small input perturbations can result in substantially different outputs of a neural network. Safety-critical environments require neural networks that are robust against input perturbations. However, training and formally verifying robust neural networks is challenging. We address this challenge by employing, for the first time, a end-to-end set-based training procedure that trains robust neural networks for formal verification. Our training procedure drastically simplifies the subsequent formal robustness verification of the trained neural network. While previous research has predominantly focused on augmenting neural network training with adversarial attacks, our approach leverages set-based computing to train neural networks with entire sets of perturbed inputs. Moreover, we demonstrate that our set-based training procedure effectively trains robust neural networks, which are easier to verify. In many cases, set-based trai
&lt;/p&gt;</description></item></channel></rss>