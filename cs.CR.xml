<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#22312;&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#27491;&#24335;&#35268;&#33539;&#20102;&#31283;&#20581;&#32858;&#21512;&#22120;&#30340;&#39046;&#22495;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;&#31283;&#20581;&#32858;&#21512;&#22120;&#26080;&#27861;&#23454;&#29616;&#20854;&#30446;&#26631;&#65292;&#35201;&#20040;&#26080;&#27861;&#20934;&#30830;&#35782;&#21035;&#26377;&#38024;&#23545;&#24615;&#30340;&#24694;&#24847;&#26356;&#26032;&#65292;&#35201;&#20040;&#26041;&#27861;&#25104;&#21151;&#29575;&#19981;&#22815;&#12290;</title><link>https://arxiv.org/abs/2402.13700</link><description>&lt;p&gt;
&#22312;&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#20013;&#31283;&#20581;&#24615;&#21644;&#23398;&#20064;&#30340;&#20914;&#31361;
&lt;/p&gt;
&lt;p&gt;
On the Conflict of Robustness and Learning in Collaborative Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13700
&lt;/p&gt;
&lt;p&gt;
&#22312;&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#27491;&#24335;&#35268;&#33539;&#20102;&#31283;&#20581;&#32858;&#21512;&#22120;&#30340;&#39046;&#22495;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;&#31283;&#20581;&#32858;&#21512;&#22120;&#26080;&#27861;&#23454;&#29616;&#20854;&#30446;&#26631;&#65292;&#35201;&#20040;&#26080;&#27861;&#20934;&#30830;&#35782;&#21035;&#26377;&#38024;&#23545;&#24615;&#30340;&#24694;&#24847;&#26356;&#26032;&#65292;&#35201;&#20040;&#26041;&#27861;&#25104;&#21151;&#29575;&#19981;&#22815;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#65288;CML&#65289;&#20801;&#35768;&#21442;&#19982;&#32773;&#20849;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;&#20182;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#31169;&#23494;&#12290;&#22312;&#38544;&#31169;&#26159;&#19968;&#20010;&#24378;&#28872;&#35201;&#27714;&#30340;&#24773;&#20917;&#19979;&#65292;&#27604;&#22914;&#20581;&#24247;&#30456;&#20851;&#24212;&#29992;&#20013;&#65292;&#23433;&#20840;&#20063;&#26159;&#39318;&#35201;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#36825;&#24847;&#21619;&#30528;&#20445;&#25252;&#38544;&#31169;&#30340;CML&#27969;&#31243;&#24517;&#39035;&#20135;&#29983;&#33021;&#22815;&#36755;&#20986;&#27491;&#30830;&#21487;&#38752;&#20915;&#31574;&#30340;&#27169;&#22411;&#65292;&#29978;&#33267;&#22312;&#21487;&#33021;&#19981;&#21463;&#20449;&#20219;&#21442;&#19982;&#32773;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20351;&#29992;&#20381;&#36182;&#20110;&#24110;&#21161;&#36807;&#28388;&#21487;&#33021;&#21361;&#21450;&#35757;&#32451;&#36807;&#31243;&#30340;&#24694;&#24847;&#36129;&#29486;&#30340;&#24230;&#37327;&#30340;&#8220;&#31283;&#20581;&#32858;&#21512;&#22120;&#8221;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#25991;&#29486;&#20013;&#35268;&#33539;&#21270;&#20102;&#31283;&#20581;&#32858;&#21512;&#22120;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#35268;&#33539;&#21270;&#33021;&#22815;&#34920;&#26126;&#29616;&#26377;&#30340;&#31283;&#20581;&#32858;&#21512;&#22120;&#26080;&#27861;&#23454;&#29616;&#20854;&#30446;&#26631;&#65306;&#26080;&#35770;&#26159;&#23427;&#20204;&#20351;&#29992;&#26080;&#27861;&#20934;&#30830;&#35782;&#21035;&#26377;&#38024;&#23545;&#24615;&#30340;&#24694;&#24847;&#26356;&#26032;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#24230;&#37327;&#65307;&#36824;&#26159;&#25552;&#20986;&#30340;&#26041;&#27861;&#25104;&#21151;&#29575;&#19981;&#22815;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13700v1 Announce Type: new  Abstract: Collaborative Machine Learning (CML) allows participants to jointly train a machine learning model while keeping their training data private. In scenarios where privacy is a strong requirement, such as health-related applications, safety is also a primary concern. This means that privacy-preserving CML processes must produce models that output correct and reliable decisions \emph{even in the presence of potentially untrusted participants}. In response to this issue, researchers propose to use \textit{robust aggregators} that rely on metrics which help filter out malicious contributions that could compromise the training process. In this work, we formalize the landscape of robust aggregators in the literature. Our formalization allows us to show that existing robust aggregators cannot fulfill their goal: either they use distance-based metrics that cannot accurately identify targeted malicious updates; or propose methods whose success is i
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#38469;&#30340;&#20445;&#25252;&#38544;&#31169;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;PrivPGD&#65292;&#21033;&#29992;&#20102;&#26368;&#20248;&#36755;&#36816;&#21644;&#31890;&#23376;&#26799;&#24230;&#19979;&#38477;&#30340;&#24037;&#20855;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#24182;&#21487;&#20197;&#28385;&#36275;&#29305;&#23450;&#30340;&#39046;&#22495;&#32422;&#26463;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2401.17823</link><description>&lt;p&gt;
&#37319;&#29992;&#26368;&#20248;&#36755;&#36816;&#21644;&#31890;&#23376;&#26799;&#24230;&#19979;&#38477;&#30340;&#20445;&#25252;&#38544;&#31169;&#25968;&#25454;&#21457;&#24067;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Privacy-preserving data release leveraging optimal transport and particle gradient descent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17823
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#38469;&#30340;&#20445;&#25252;&#38544;&#31169;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;PrivPGD&#65292;&#21033;&#29992;&#20102;&#26368;&#20248;&#36755;&#36816;&#21644;&#31890;&#23376;&#26799;&#24230;&#19979;&#38477;&#30340;&#24037;&#20855;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#24182;&#21487;&#20197;&#28385;&#36275;&#29305;&#23450;&#30340;&#39046;&#22495;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;&#20851;&#38190;&#39046;&#22495;&#65288;&#22914;&#21307;&#30103;&#20445;&#20581;&#21644;&#25919;&#24220;&#65289;&#20013;&#38544;&#31169;&#30340;&#34920;&#26684;&#25968;&#25454;&#24046;&#20998;&#31169;&#26377;&#25968;&#25454;&#21512;&#25104;&#30340;&#20219;&#21153;&#12290;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#20027;&#35201;&#20351;&#29992;&#22522;&#20110;&#36793;&#38469;&#30340;&#26041;&#27861;&#65292;&#20174;&#31169;&#26377;&#36793;&#38469;&#20272;&#35745;&#29983;&#25104;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PrivPGD&#65292;&#19968;&#31181;&#22522;&#20110;&#36793;&#38469;&#30340;&#31169;&#26377;&#25968;&#25454;&#21512;&#25104;&#30340;&#26032;&#19968;&#20195;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#26368;&#20248;&#36755;&#36816;&#21644;&#31890;&#23376;&#26799;&#24230;&#19979;&#38477;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#22823;&#33539;&#22260;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#39640;&#24230;&#21487;&#25193;&#23637;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#32467;&#21512;&#20854;&#20182;&#39046;&#22495;&#29305;&#23450;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach for differentially private data synthesis of protected tabular datasets, a relevant task in highly sensitive domains such as healthcare and government. Current state-of-the-art methods predominantly use marginal-based approaches, where a dataset is generated from private estimates of the marginals. In this paper, we introduce PrivPGD, a new generation method for marginal-based private data synthesis, leveraging tools from optimal transport and particle gradient descent. Our algorithm outperforms existing methods on a large range of datasets while being highly scalable and offering the flexibility to incorporate additional domain-specific constraints.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20248;&#21270;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;WizardCoder&#24182;&#25913;&#36827;&#20854;&#35757;&#32451;&#36807;&#31243;&#21644;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#28431;&#27934;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.17010</link><description>&lt;p&gt;
&#20248;&#21270;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28431;&#27934;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Finetuning Large Language Models for Vulnerability Detection. (arXiv:2401.17010v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20248;&#21270;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;WizardCoder&#24182;&#25913;&#36827;&#20854;&#35757;&#32451;&#36807;&#31243;&#21644;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#28431;&#27934;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;StarCoder&#30340;&#25913;&#36827;&#29256;&#26412;WizardCoder&#65292;&#24182;&#36890;&#36807;&#36827;&#19968;&#27493;&#24494;&#35843;&#23558;&#20854;&#36866;&#24212;&#20110;&#28431;&#27934;&#26816;&#27979;&#20219;&#21153;&#12290;&#20026;&#20102;&#21152;&#36895;&#35757;&#32451;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;WizardCoder&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#25506;&#31350;&#20102;&#26368;&#20339;&#30340;&#35757;&#32451;&#31574;&#30053;&#12290;&#38024;&#23545;&#36127;&#26679;&#26412;&#36828;&#22810;&#20110;&#27491;&#26679;&#26412;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36824;&#23581;&#35797;&#20102;&#19981;&#21516;&#30340;&#25216;&#26415;&#26469;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;&#24494;&#35843;&#21518;&#30340;WizardCoder&#27169;&#22411;&#22312;&#24179;&#34913;&#21644;&#19981;&#24179;&#34913;&#30340;&#28431;&#27934;&#25968;&#25454;&#38598;&#19978;&#22312;ROC AUC&#21644;F1&#24230;&#37327;&#19978;&#23454;&#29616;&#20102;&#25913;&#36827;&#65292;&#35777;&#26126;&#20102;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#30340;&#26377;&#25928;&#24615;&#12290;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;&#23545;&#26368;&#20808;&#36827;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;WizardCoder&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#20854;&#35757;&#32451;&#36895;&#24230;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#65292;&#24182;&#23545;&#35757;&#32451;&#36807;&#31243;&#21644;&#31574;&#30053;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the results of finetuning large language models (LLMs) for the task of detecting vulnerabilities in source code. We leverage WizardCoder, a recent improvement of the state-of-the-art LLM StarCoder, and adapt it for vulnerability detection through further finetuning. To accelerate training, we modify WizardCoder's training procedure, also we investigate optimal training regimes. For the imbalanced dataset with many more negative examples than positive, we also explore different techniques to improve classification performance. The finetuned WizardCoder model achieves improvement in ROC AUC and F1 measures on balanced and imbalanced vulnerability datasets over CodeBERT-like model, demonstrating the effectiveness of adapting pretrained LLMs for vulnerability detection in source code. The key contributions are finetuning the state-of-the-art code LLM, WizardCoder, increasing its training speed without the performance harm, optimizing the training procedure and regimes, 
&lt;/p&gt;</description></item><item><title>FLAIM&#26159;&#19968;&#20010;&#22312;&#32852;&#37030;&#35774;&#32622;&#20013;&#22522;&#20110;AIM&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#24046;&#20998;&#38544;&#31169;&#26041;&#21521;&#30340;&#25216;&#26415;&#22312;&#32852;&#37030;&#22330;&#26223;&#19979;&#30340;&#36866;&#29992;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;FLAIM&#26041;&#27861;&#26469;&#32500;&#25345;&#36739;&#39640;&#30340;&#25928;&#29992;&#21644;&#22788;&#29702;&#24322;&#26500;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03447</link><description>&lt;p&gt;
FLAIM: &#22312;&#32852;&#37030;&#35774;&#32622;&#20013;&#22522;&#20110;AIM&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
FLAIM: AIM-based Synthetic Data Generation in the Federated Setting. (arXiv:2310.03447v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03447
&lt;/p&gt;
&lt;p&gt;
FLAIM&#26159;&#19968;&#20010;&#22312;&#32852;&#37030;&#35774;&#32622;&#20013;&#22522;&#20110;AIM&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#24046;&#20998;&#38544;&#31169;&#26041;&#21521;&#30340;&#25216;&#26415;&#22312;&#32852;&#37030;&#22330;&#26223;&#19979;&#30340;&#36866;&#29992;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;FLAIM&#26041;&#27861;&#26469;&#32500;&#25345;&#36739;&#39640;&#30340;&#25928;&#29992;&#21644;&#22788;&#29702;&#24322;&#26500;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#25252;&#20010;&#20154;&#38544;&#31169;&#21516;&#26102;&#23454;&#29616;&#21327;&#21516;&#25968;&#25454;&#20849;&#20139;&#23545;&#32452;&#32455;&#33267;&#20851;&#37325;&#35201;&#12290;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26159;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#20135;&#29983;&#19982;&#31169;&#26377;&#25968;&#25454;&#30340;&#32479;&#35745;&#29305;&#24615;&#30456;&#20284;&#30340;&#20154;&#24037;&#25968;&#25454;&#12290;&#34429;&#28982;&#22312;&#24046;&#20998;&#38544;&#31169;&#19979;&#24050;&#32463;&#35774;&#35745;&#20986;&#20102;&#35768;&#22810;&#25216;&#26415;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#20551;&#35774;&#25968;&#25454;&#26159;&#38598;&#20013;&#30340;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#24448;&#24448;&#20197;&#32852;&#37030;&#26041;&#24335;&#20998;&#24067;&#22312;&#22810;&#20010;&#23458;&#25143;&#31471;&#19978;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#22987;&#30740;&#31350;&#32852;&#37030;&#21512;&#25104;&#34920;&#25968;&#25454;&#29983;&#25104;&#12290;&#22312;AIM&#36825;&#20010;&#20808;&#36827;&#30340;&#20013;&#24515;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DistAIM&#21644;FLAIM&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20998;&#21457;AIM&#26159;&#31616;&#21333;&#30340;&#65292;&#25193;&#23637;&#20102;&#22522;&#20110;&#23433;&#20840;&#22810;&#26041;&#35745;&#31639;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#20294;&#38656;&#35201;&#39069;&#22806;&#30340;&#24320;&#38144;&#65292;&#20351;&#20854;&#22312;&#32852;&#37030;&#22330;&#26223;&#20013;&#19981;&#22826;&#36866;&#29992;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31616;&#21333;&#22320;&#32852;&#37030;AIM&#21487;&#33021;&#23548;&#33268;&#22312;&#24322;&#26500;&#24615;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#25928;&#29992;&#20005;&#37325;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;FLAIM&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#32500;&#25345;&#36739;&#39640;&#30340;&#25928;&#29992;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#32852;&#37030;&#35774;&#32622;&#20013;&#30340;&#24322;&#26500;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Preserving individual privacy while enabling collaborative data sharing is crucial for organizations. Synthetic data generation is one solution, producing artificial data that mirrors the statistical properties of private data. While numerous techniques have been devised under differential privacy, they predominantly assume data is centralized. However, data is often distributed across multiple clients in a federated manner. In this work, we initiate the study of federated synthetic tabular data generation. Building upon a SOTA central method known as AIM, we present DistAIM and FLAIM. We show it is straightforward to distribute AIM, extending a recent approach based on secure multi-party computation which necessitates additional overhead, making it less suited to federated scenarios. We then demonstrate that naively federating AIM can lead to substantial degradation in utility under the presence of heterogeneity. To mitigate both issues, we propose an augmented FLAIM approach that mai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#21644;&#25913;&#36827;&#24403;&#21069;&#21644;&#26410;&#26469;&#19982;&#25554;&#20214;&#38598;&#25104;&#30340;LLM&#24179;&#21488;&#30340;&#23433;&#20840;&#24615;&#12289;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#12290;&#22312;&#24212;&#29992;&#26694;&#26550;&#20110;OpenAI&#30340;&#25554;&#20214;&#29983;&#24577;&#31995;&#32479;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#20855;&#20307;&#35777;&#26126;&#20102;&#28508;&#22312;&#38382;&#39064;&#30340;&#25554;&#20214;&#12290;</title><link>http://arxiv.org/abs/2309.10254</link><description>&lt;p&gt;
LLM&#24179;&#21488;&#23433;&#20840;&#65306;&#23558;&#31995;&#32479;&#35780;&#20272;&#26694;&#26550;&#24212;&#29992;&#20110;OpenAI&#30340;ChatGPT&#25554;&#20214;
&lt;/p&gt;
&lt;p&gt;
LLM Platform Security: Applying a Systematic Evaluation Framework to OpenAI's ChatGPT Plugins. (arXiv:2309.10254v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#21644;&#25913;&#36827;&#24403;&#21069;&#21644;&#26410;&#26469;&#19982;&#25554;&#20214;&#38598;&#25104;&#30340;LLM&#24179;&#21488;&#30340;&#23433;&#20840;&#24615;&#12289;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#12290;&#22312;&#24212;&#29992;&#26694;&#26550;&#20110;OpenAI&#30340;&#25554;&#20214;&#29983;&#24577;&#31995;&#32479;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#20855;&#20307;&#35777;&#26126;&#20102;&#28508;&#22312;&#38382;&#39064;&#30340;&#25554;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#22914;ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24179;&#21488;&#24320;&#22987;&#25552;&#20379;&#25554;&#20214;&#29983;&#24577;&#31995;&#32479;&#65292;&#20197;&#19982;&#20114;&#32852;&#32593;&#19978;&#30340;&#31532;&#19977;&#26041;&#26381;&#21153;&#36827;&#34892;&#20132;&#20114;&#12290;&#34429;&#28982;&#36825;&#20123;&#25554;&#20214;&#25193;&#23637;&#20102;LLM&#24179;&#21488;&#30340;&#21151;&#33021;&#65292;&#20294;&#23427;&#20204;&#26159;&#30001;&#20219;&#24847;&#30340;&#31532;&#19977;&#26041;&#24320;&#21457;&#30340;&#65292;&#22240;&#27492;&#19981;&#33021;&#38544;&#24335;&#20449;&#20219;&#12290;&#25554;&#20214;&#36824;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#19982;LLM&#24179;&#21488;&#21644;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#27169;&#31946;&#30340;&#35299;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20026;LLM&#24179;&#21488;&#35774;&#35745;&#32773;&#20998;&#26512;&#21644;&#25913;&#36827;&#24403;&#21069;&#21644;&#26410;&#26469;&#19982;&#25554;&#20214;&#38598;&#25104;&#30340;LLM&#24179;&#21488;&#30340;&#23433;&#20840;&#24615;&#12289;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26159;&#19968;&#20010;&#25915;&#20987;&#20998;&#31867;&#27861;&#30340;&#34920;&#36848;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#25506;&#32034;LLM&#24179;&#21488;&#30456;&#20851;&#26041;&#22914;&#20309;&#21033;&#29992;&#20182;&#20204;&#30340;&#33021;&#21147;&#21644;&#36131;&#20219;&#23545;&#24444;&#27492;&#36827;&#34892;&#25915;&#20987;&#26469;&#24320;&#21457;&#30340;&#12290;&#20316;&#20026;&#25105;&#20204;&#36845;&#20195;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;OpenAI&#30340;&#25554;&#20214;&#29983;&#24577;&#31995;&#32479;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20123;&#20855;&#20307;&#35777;&#26126;&#20102;&#28508;&#22312;&#38382;&#39064;&#30340;&#25554;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language model (LLM) platforms, such as ChatGPT, have recently begun offering a plugin ecosystem to interface with third-party services on the internet. While these plugins extend the capabilities of LLM platforms, they are developed by arbitrary third parties and thus cannot be implicitly trusted. Plugins also interface with LLM platforms and users using natural language, which can have imprecise interpretations. In this paper, we propose a framework that lays a foundation for LLM platform designers to analyze and improve the security, privacy, and safety of current and future plugin-integrated LLM platforms. Our framework is a formulation of an attack taxonomy that is developed by iteratively exploring how LLM platform stakeholders could leverage their capabilities and responsibilities to mount attacks against each other. As part of our iterative process, we apply our framework in the context of OpenAI's plugin ecosystem. We uncover plugins that concretely demonstrate the poten
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;ChatGPT&#22312;&#23433;&#20840;&#23548;&#21521;&#30340;&#31243;&#24207;&#20998;&#26512;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#30740;&#31350;&#65292;&#26088;&#22312;&#20102;&#35299;&#20854;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;ChatGPT&#22312;&#23433;&#20840;&#39046;&#22495;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.12488</link><description>&lt;p&gt;
ChatGPT&#29992;&#20110;&#36719;&#20214;&#23433;&#20840;&#65306;&#25506;&#32034;ChatGPT&#22312;&#23433;&#20840;&#24212;&#29992;&#20013;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
ChatGPT for Software Security: Exploring the Strengths and Limitations of ChatGPT in the Security Applications. (arXiv:2307.12488v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;ChatGPT&#22312;&#23433;&#20840;&#23548;&#21521;&#30340;&#31243;&#24207;&#20998;&#26512;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#30740;&#31350;&#65292;&#26088;&#22312;&#20102;&#35299;&#20854;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;ChatGPT&#22312;&#23433;&#20840;&#39046;&#22495;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#20010;&#22810;&#25165;&#22810;&#33402;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;ChatGPT&#22312;&#21508;&#20010;&#39046;&#22495;&#24212;&#23545;&#38382;&#39064;&#30340;&#28508;&#21147;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#23637;&#31034;&#12290;&#23427;&#33021;&#22815;&#20998;&#26512;&#12289;&#29702;&#35299;&#21644;&#32508;&#21512;&#26469;&#33258;&#22312;&#32447;&#36164;&#28304;&#21644;&#29992;&#25143;&#36755;&#20837;&#30340;&#20449;&#24687;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;ChatGPT&#22312;&#20195;&#30721;&#29983;&#25104;&#21644;&#20195;&#30721;&#23457;&#26597;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;ChatGPT&#22312;&#38754;&#21521;&#23433;&#20840;&#30340;&#31243;&#24207;&#20998;&#26512;&#20013;&#30340;&#33021;&#21147;&#65292;&#20174;&#25915;&#20987;&#32773;&#21644;&#23433;&#20840;&#20998;&#26512;&#24072;&#30340;&#35282;&#24230;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#26469;&#35780;&#20272;ChatGPT&#22312;&#20960;&#20010;&#23433;&#20840;&#23548;&#21521;&#30340;&#31243;&#24207;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#22238;&#31572;&#36136;&#37327;&#65292;&#24182;&#26377;&#24847;&#22320;&#24341;&#20837;&#25361;&#25112;&#26469;&#35780;&#20272;&#20854;&#21709;&#24212;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;ChatGPT&#25552;&#20379;&#30340;&#31572;&#26696;&#36136;&#37327;&#30340;&#32771;&#23519;&#65292;&#25105;&#20204;&#23545;&#20854;&#22312;&#23433;&#20840;&#23548;&#21521;&#30340;&#31243;&#24207;&#20998;&#26512;&#39046;&#22495;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#26377;&#20102;&#26356;&#28165;&#26224;&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT, as a versatile large language model, has demonstrated remarkable potential in addressing inquiries across various domains. Its ability to analyze, comprehend, and synthesize information from both online sources and user inputs has garnered significant attention. Previous research has explored ChatGPT's competence in code generation and code reviews. In this paper, we delve into ChatGPT's capabilities in security-oriented program analysis, focusing on perspectives from both attackers and security analysts. We present a case study involving several security-oriented program analysis tasks while deliberately introducing challenges to assess ChatGPT's responses. Through an examination of the quality of answers provided by ChatGPT, we gain a clearer understanding of its strengths and limitations in the realm of security-oriented program analysis.
&lt;/p&gt;</description></item></channel></rss>