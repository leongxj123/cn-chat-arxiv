<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#35821;&#35328;&#27169;&#22411;&#20013;"&#36234;&#29425;"&#25915;&#20987;&#30340;&#20851;&#38190;&#26159;&#36890;&#36807;&#23450;&#20041;&#22909;&#30340;&#19981;&#23433;&#20840;&#21709;&#24212;&#26469;&#36827;&#34892;&#38450;&#24481;&#65292;&#32780;&#19981;&#26159;&#20381;&#36182;&#20110;&#25191;&#34892;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.14725</link><description>&lt;p&gt;
Jailbreaking&#30340;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#26159;&#36890;&#36807;&#23450;&#20041;
&lt;/p&gt;
&lt;p&gt;
Jailbreaking is Best Solved by Definition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14725
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20013;"&#36234;&#29425;"&#25915;&#20987;&#30340;&#20851;&#38190;&#26159;&#36890;&#36807;&#23450;&#20041;&#22909;&#30340;&#19981;&#23433;&#20840;&#21709;&#24212;&#26469;&#36827;&#34892;&#38450;&#24481;&#65292;&#32780;&#19981;&#26159;&#20381;&#36182;&#20110;&#25191;&#34892;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#19978;"&#36234;&#29425;"&#25915;&#20987;&#30340;&#22686;&#22810;&#24341;&#21457;&#20102;&#22823;&#37327;&#38450;&#24481;&#24037;&#20316;&#65292;&#26088;&#22312;&#38450;&#27490;&#20135;&#29983;&#19981;&#33391;&#22238;&#24212;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25209;&#21028;&#24615;&#22320;&#23457;&#35270;&#20102;&#38450;&#24481;&#31649;&#36947;&#30340;&#20004;&#20010;&#38454;&#27573;&#65306;&#65288;i&#65289;&#23450;&#20041;&#20309;&#20026;&#19981;&#23433;&#20840;&#36755;&#20986;&#65292;&#21644;&#65288;ii&#65289;&#36890;&#36807;&#36755;&#20837;&#22788;&#29702;&#25110;&#24494;&#35843;&#31561;&#26041;&#27861;&#26469;&#25191;&#34892;&#35813;&#23450;&#20041;&#12290;&#25105;&#20204;&#20005;&#37325;&#24576;&#30097;&#29616;&#26377;&#30340;&#25191;&#34892;&#26426;&#21046;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#23637;&#31034;&#23427;&#20204;&#21363;&#20351;&#23545;&#20110;&#31616;&#21333;&#30340;&#19981;&#23433;&#20840;&#36755;&#20986;&#23450;&#20041;--&#21253;&#21547;&#21333;&#35789;"purple"&#30340;&#36755;&#20986;&#20063;&#26080;&#27861;&#38450;&#24481;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#23545;&#36755;&#20986;&#36827;&#34892;&#21518;&#22788;&#29702;&#23545;&#20110;&#36825;&#26679;&#30340;&#23450;&#20041;&#26159;&#23436;&#20840;&#20581;&#22766;&#30340;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#25105;&#20204;&#30340;&#35266;&#28857;&#65292;&#21363;&#22312;&#38450;&#24481;&#36234;&#29425;&#25915;&#20987;&#20013;&#30495;&#27491;&#30340;&#25361;&#25112;&#22312;&#20110;&#24471;&#21040;&#19968;&#20010;&#33391;&#22909;&#30340;&#19981;&#23433;&#20840;&#21709;&#24212;&#23450;&#20041;&#65306;&#27809;&#26377;&#33391;&#22909;&#30340;&#23450;&#20041;&#65292;&#20219;&#20309;&#25191;&#34892;&#31574;&#30053;&#37117;&#26080;&#27861;&#25104;&#21151;&#65292;&#20294;&#26377;&#20102;&#33391;&#22909;&#30340;&#23450;&#20041;&#65292;&#36755;&#20986;&#22788;&#29702;&#24050;&#32463;&#20316;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14725v1 Announce Type: cross  Abstract: The rise of "jailbreak" attacks on language models has led to a flurry of defenses aimed at preventing the output of undesirable responses. In this work, we critically examine the two stages of the defense pipeline: (i) the definition of what constitutes unsafe outputs, and (ii) the enforcement of the definition via methods such as input processing or fine-tuning. We cast severe doubt on the efficacy of existing enforcement mechanisms by showing that they fail to defend even for a simple definition of unsafe outputs--outputs that contain the word "purple". In contrast, post-processing outputs is perfectly robust for such a definition. Drawing on our results, we present our position that the real challenge in defending jailbreaks lies in obtaining a good definition of unsafe responses: without a good definition, no enforcement strategy can succeed, but with a good definition, output processing already serves as a robust baseline albeit 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#31169;&#20154;&#22522;&#20934;&#35774;&#23450;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#25345;&#27979;&#35797;&#25968;&#25454;&#30340;&#31169;&#23494;&#24615;&#65292;&#20351;&#27169;&#22411;&#22312;&#19981;&#25581;&#38706;&#27979;&#35797;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35780;&#20272;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#22522;&#20934;&#35774;&#23450;&#20013;&#23384;&#22312;&#25968;&#25454;&#27745;&#26579;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.00393</link><description>&lt;p&gt;
&#38450;&#27490;&#27745;&#26579;&#21644;&#25552;&#39640;LLM&#27604;&#36739;&#35780;&#20272;&#30340;&#31169;&#20154;&#22522;&#20934;&#35774;&#23450;
&lt;/p&gt;
&lt;p&gt;
Private Benchmarking to Prevent Contamination and Improve Comparative Evaluation of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#31169;&#20154;&#22522;&#20934;&#35774;&#23450;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#25345;&#27979;&#35797;&#25968;&#25454;&#30340;&#31169;&#23494;&#24615;&#65292;&#20351;&#27169;&#22411;&#22312;&#19981;&#25581;&#38706;&#27979;&#35797;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35780;&#20272;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#22522;&#20934;&#35774;&#23450;&#20013;&#23384;&#22312;&#25968;&#25454;&#27745;&#26579;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20934;&#35774;&#23450;&#26159;&#35780;&#20272;LLM&#30340;&#20107;&#23454;&#26631;&#20934;&#65292;&#22240;&#20026;&#23427;&#36895;&#24230;&#24555;&#12289;&#21487;&#22797;&#21046;&#19988;&#25104;&#26412;&#20302;&#24265;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25351;&#20986;&#65292;&#20170;&#22825;&#22823;&#22810;&#25968;&#24320;&#28304;&#22522;&#20934;&#35774;&#23450;&#24050;&#32463;&#34987;&#27745;&#26579;&#25110;&#27844;&#38706;&#21040;LLM&#20013;&#65292;&#36825;&#24847;&#21619;&#30528;LLM&#22312;&#39044;&#35757;&#32451;&#21644;/&#25110;&#24494;&#35843;&#26399;&#38388;&#21487;&#20197;&#35775;&#38382;&#27979;&#35797;&#25968;&#25454;&#12290;&#36825;&#23545;&#36804;&#20170;&#20026;&#27490;&#36827;&#34892;&#30340;&#22522;&#20934;&#30740;&#31350;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#26410;&#26469;&#20351;&#29992;&#22522;&#20934;&#36827;&#34892;&#35780;&#20272;&#25552;&#20986;&#20102;&#20005;&#37325;&#20851;&#20999;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Private Benchmarking&#65292;&#36825;&#26159;&#19968;&#20010;&#26041;&#26696;&#65292;&#20854;&#20013;&#27979;&#35797;&#25968;&#25454;&#38598;&#20445;&#25345;&#31169;&#23494;&#65292;&#27169;&#22411;&#22312;&#19981;&#21521;&#27169;&#22411;&#36879;&#38706;&#27979;&#35797;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#21508;&#31181;&#22330;&#26223;&#65288;&#21462;&#20915;&#20110;&#23545;&#27169;&#22411;&#25152;&#26377;&#32773;&#25110;&#25968;&#25454;&#38598;&#25152;&#26377;&#32773;&#30340;&#20449;&#20219;&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#31169;&#20154;&#22522;&#20934;&#35774;&#23450;&#36991;&#20813;&#25968;&#25454;&#27745;&#26579;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23545;&#20110;&#38656;&#35201;&#20445;&#25252;&#27169;&#22411;&#26435;&#37325;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#26469;&#33258;&#26426;&#23494;&#35745;&#31639;&#21644;&#23494;&#30721;&#23398;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00393v1 Announce Type: cross  Abstract: Benchmarking is the de-facto standard for evaluating LLMs, due to its speed, replicability and low cost. However, recent work has pointed out that the majority of the open source benchmarks available today have been contaminated or leaked into LLMs, meaning that LLMs have access to test data during pretraining and/or fine-tuning. This raises serious concerns about the validity of benchmarking studies conducted so far and the future of evaluation using benchmarks. To solve this problem, we propose Private Benchmarking, a solution where test datasets are kept private and models are evaluated without revealing the test data to the model. We describe various scenarios (depending on the trust placed on model owners or dataset owners), and present solutions to avoid data contamination using private benchmarking. For scenarios where the model weights need to be kept private, we describe solutions from confidential computing and cryptography t
&lt;/p&gt;</description></item><item><title>LLM&#27700;&#21360;&#25216;&#26415;&#21487;&#33021;&#23384;&#22312;&#27700;&#21360;&#31363;&#21462;&#28431;&#27934;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#21160;WS&#31639;&#27861;&#24182;&#23637;&#31034;&#20102;&#25915;&#20987;&#32773;&#21487;&#20197;&#22312;&#19981;&#21040;50&#32654;&#20803;&#30340;&#25104;&#26412;&#19979;&#36890;&#36807;&#27450;&#39575;&#21644;&#25830;&#38500;&#25915;&#20987;&#30772;&#35299;&#20043;&#21069;&#35748;&#20026;&#23433;&#20840;&#30340;&#26368;&#20808;&#36827;&#26041;&#26696;&#65292;&#25104;&#21151;&#29575;&#36229;&#36807;80%&#12290;</title><link>https://arxiv.org/abs/2402.19361</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#27700;&#21360;&#31363;&#21462;
&lt;/p&gt;
&lt;p&gt;
Watermark Stealing in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19361
&lt;/p&gt;
&lt;p&gt;
LLM&#27700;&#21360;&#25216;&#26415;&#21487;&#33021;&#23384;&#22312;&#27700;&#21360;&#31363;&#21462;&#28431;&#27934;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#21160;WS&#31639;&#27861;&#24182;&#23637;&#31034;&#20102;&#25915;&#20987;&#32773;&#21487;&#20197;&#22312;&#19981;&#21040;50&#32654;&#20803;&#30340;&#25104;&#26412;&#19979;&#36890;&#36807;&#27450;&#39575;&#21644;&#25830;&#38500;&#25915;&#20987;&#30772;&#35299;&#20043;&#21069;&#35748;&#20026;&#23433;&#20840;&#30340;&#26368;&#20808;&#36827;&#26041;&#26696;&#65292;&#25104;&#21151;&#29575;&#36229;&#36807;80%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#27700;&#21360;&#25216;&#26415;&#20316;&#20026;&#19968;&#31181;&#26816;&#27979;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#26377;&#25928;&#26041;&#24335;&#65292;&#21463;&#21040;&#20102;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#20105;&#36777;&#31216;&#24403;&#21069;&#26041;&#26696;&#21487;&#33021;&#24050;&#32463;&#21487;&#20197;&#37096;&#32626;&#65292;&#25105;&#20204;&#35748;&#20026;&#27700;&#21360;&#31363;&#21462;&#65288;WS&#65289;&#26159;&#36825;&#20123;&#26041;&#26696;&#30340;&#19968;&#20010;&#26681;&#26412;&#24615;&#28431;&#27934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#26597;&#35810;&#24102;&#26377;&#27700;&#21360;&#30340;LLM&#30340;API&#26469;&#36817;&#20284;&#36870;&#21521;&#27700;&#21360;&#65292;&#20174;&#32780;&#23454;&#29616;&#23454;&#29992;&#30340;&#27450;&#39575;&#25915;&#20987;&#65292;&#21516;&#26102;&#22823;&#24133;&#22686;&#21152;&#20102;&#20043;&#21069;&#26410;&#34987;&#27880;&#24847;&#21040;&#30340;&#25830;&#38500;&#25915;&#20987;&#12290;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#25552;&#20986;&#33258;&#21160;WS&#31639;&#27861;&#24182;&#23558;&#20854;&#29992;&#20110;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#36827;&#34892;&#27450;&#39575;&#21644;&#25830;&#38500;&#30340;&#20840;&#38754;&#30740;&#31350;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20165;&#38656;&#19981;&#21040;50&#32654;&#20803;&#30340;&#25104;&#26412;&#65292;&#25915;&#20987;&#32773;&#23601;&#33021;&#22815;&#27450;&#39575;&#24182;&#25830;&#38500;&#20043;&#21069;&#34987;&#35748;&#20026;&#26159;&#23433;&#20840;&#30340;&#26368;&#20808;&#36827;&#26041;&#26696;&#65292;&#24179;&#22343;&#25104;&#21151;&#29575;&#36229;&#36807;80%&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25361;&#25112;&#20102;&#20851;&#20110;LLM&#27700;&#21360;&#25216;&#26415;&#30340;&#24120;&#35265;&#20449;&#24565;&#65292;&#24378;&#35843;&#20102;&#26356;&#21152;&#20581;&#22766;&#26041;&#26696;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19361v1 Announce Type: cross  Abstract: LLM watermarking has attracted attention as a promising way to detect AI-generated content, with some works suggesting that current schemes may already be fit for deployment. In this work we dispute this claim, identifying watermark stealing (WS) as a fundamental vulnerability of these schemes. We show that querying the API of the watermarked LLM to approximately reverse-engineer a watermark enables practical spoofing attacks, as suggested in prior work, but also greatly boosts scrubbing attacks, which was previously unnoticed. We are the first to propose an automated WS algorithm and use it in the first comprehensive study of spoofing and scrubbing in realistic settings. We show that for under $50 an attacker can both spoof and scrub state-of-the-art schemes previously considered safe, with average success rate of over 80%. Our findings challenge common beliefs about LLM watermarking, stressing the need for more robust schemes. We mak
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20102;&#38544;&#31169;&#25915;&#20987;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#39318;&#20010;&#33021;&#21516;&#26102;&#23454;&#29616;&#39640;&#30495;&#27491;&#29575;&#21644;&#20302;&#35823;&#20998;&#31867;&#29575;&#30340;&#39044;&#35757;&#32451;LLMs&#20250;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIAs&#65289;&#65292;&#20197;&#21450;&#23637;&#31034;&#20102;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#21487;&#20197;&#20174;&#24494;&#35843;LLM&#20013;&#25552;&#21462;&#36229;&#36807;50%&#30340;&#24494;&#35843;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.17012</link><description>&lt;p&gt;
Pandora's White-Box&#65306;&#24320;&#25918;LLMs&#20013;&#35757;&#32451;&#25968;&#25454;&#27844;&#28431;&#30340;&#22686;&#21152;
&lt;/p&gt;
&lt;p&gt;
Pandora's White-Box: Increased Training Data Leakage in Open LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20102;&#38544;&#31169;&#25915;&#20987;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#39318;&#20010;&#33021;&#21516;&#26102;&#23454;&#29616;&#39640;&#30495;&#27491;&#29575;&#21644;&#20302;&#35823;&#20998;&#31867;&#29575;&#30340;&#39044;&#35757;&#32451;LLMs&#20250;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIAs&#65289;&#65292;&#20197;&#21450;&#23637;&#31034;&#20102;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#21487;&#20197;&#20174;&#24494;&#35843;LLM&#20013;&#25552;&#21462;&#36229;&#36807;50%&#30340;&#24494;&#35843;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36973;&#21463;&#30340;&#38544;&#31169;&#25915;&#20987;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#20854;&#20013;&#23545;&#25163;&#21487;&#20197;&#35775;&#38382;&#27169;&#22411;&#26435;&#37325;&#12289;&#26799;&#24230;&#25110;&#25439;&#22833;&#65292;&#35797;&#22270;&#21033;&#29992;&#23427;&#20204;&#26469;&#20102;&#35299;&#24213;&#23618;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#38024;&#23545;&#39044;&#35757;&#32451;LLMs&#30340;&#31532;&#19968;&#20010;&#20250;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIAs&#65289;&#65292;&#33021;&#22815;&#21516;&#26102;&#23454;&#29616;&#39640;TPR&#21644;&#20302;FPR&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#21487;&#20197;&#20174;&#24494;&#35843;LLM&#20013;&#25552;&#21462;&#36229;&#36807;50%&#30340;&#24494;&#35843;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#23545;&#24213;&#23618;&#27169;&#22411;&#30340;&#19981;&#21516;&#35775;&#38382;&#31243;&#24230;&#12289;&#35821;&#35328;&#27169;&#22411;&#30340;&#23450;&#21046;&#21270;&#20197;&#21450;&#25915;&#20987;&#32773;&#21487;&#20197;&#20351;&#29992;&#30340;&#36164;&#28304;&#12290;&#22312;&#39044;&#35757;&#32451;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#30333;&#30418;MIAs&#65306;&#22522;&#20110;&#26799;&#24230;&#33539;&#25968;&#30340;&#25915;&#20987;&#12289;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#21644;&#21333;&#27493;&#25439;&#22833;&#27604;&#25915;&#20987;&#12290;&#25152;&#26377;&#36825;&#20123;&#37117;&#20248;&#20110;&#29616;&#26377;&#30340;&#40657;&#30418;&#22522;&#32447;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;.....
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17012v1 Announce Type: cross  Abstract: In this paper we undertake a systematic study of privacy attacks against open source Large Language Models (LLMs), where an adversary has access to either the model weights, gradients, or losses, and tries to exploit them to learn something about the underlying training data. Our headline results are the first membership inference attacks (MIAs) against pre-trained LLMs that are able to simultaneously achieve high TPRs and low FPRs, and a pipeline showing that over $50\%$ (!) of the fine-tuning dataset can be extracted from a fine-tuned LLM in natural settings. We consider varying degrees of access to the underlying model, customization of the language model, and resources available to the attacker. In the pre-trained setting, we propose three new white-box MIAs: an attack based on the gradient norm, a supervised neural network classifier, and a single step loss ratio attack. All outperform existing black-box baselines, and our supervi
&lt;/p&gt;</description></item></channel></rss>