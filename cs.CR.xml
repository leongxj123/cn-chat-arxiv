<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>PAPER-HILT&#26159;&#38024;&#23545;&#20154;&#26426;&#21327;&#21516;&#31995;&#32479;&#20013;&#38544;&#31169;&#20445;&#25252;&#30340;&#21019;&#26032;&#33258;&#36866;&#24212;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#25552;&#21069;&#36864;&#20986;&#26041;&#27861;&#21160;&#24577;&#35843;&#25972;&#38544;&#31169;&#20445;&#25252;&#21644;&#31995;&#32479;&#25928;&#29992;&#65292;&#20197;&#36866;&#24212;&#20010;&#20307;&#34892;&#20026;&#27169;&#24335;&#21644;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2403.05864</link><description>&lt;p&gt;
PAPER-HILT&#65306;&#20010;&#24615;&#21270;&#21644;&#33258;&#36866;&#24212;&#38544;&#31169;&#24863;&#30693;&#30340;&#24378;&#21270;&#23398;&#20064;&#25552;&#21069;&#36864;&#20986;&#22312;&#20154;&#26426;&#21327;&#21516;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
PAPER-HILT: Personalized and Adaptive Privacy-Aware Early-Exit for Reinforcement Learning in Human-in-the-Loop Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05864
&lt;/p&gt;
&lt;p&gt;
PAPER-HILT&#26159;&#38024;&#23545;&#20154;&#26426;&#21327;&#21516;&#31995;&#32479;&#20013;&#38544;&#31169;&#20445;&#25252;&#30340;&#21019;&#26032;&#33258;&#36866;&#24212;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#25552;&#21069;&#36864;&#20986;&#26041;&#27861;&#21160;&#24577;&#35843;&#25972;&#38544;&#31169;&#20445;&#25252;&#21644;&#31995;&#32479;&#25928;&#29992;&#65292;&#20197;&#36866;&#24212;&#20010;&#20307;&#34892;&#20026;&#27169;&#24335;&#21644;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26085;&#30410;&#25104;&#20026;&#20154;&#26426;&#21327;&#21516;&#65288;HITL&#65289;&#24212;&#29992;&#20013;&#30340;&#39318;&#36873;&#26041;&#27861;&#65292;&#22240;&#20854;&#36866;&#24212;&#20110;&#20154;&#31867;&#20132;&#20114;&#30340;&#21160;&#24577;&#29305;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31181;&#29615;&#22659;&#20013;&#25972;&#21512;RL&#20250;&#24102;&#26469;&#37325;&#22823;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#21487;&#33021;&#20250;&#19981;&#32463;&#24847;&#22320;&#26292;&#38706;&#25935;&#24863;&#29992;&#25143;&#20449;&#24687;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#19987;&#27880;&#20110;&#24320;&#21457;PAPER-HILT&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#33258;&#36866;&#24212;RL&#31574;&#30053;&#65292;&#36890;&#36807;&#21033;&#29992;&#19987;&#20026;HITL&#29615;&#22659;&#20013;&#38544;&#31169;&#20445;&#25252;&#35774;&#35745;&#30340;&#25552;&#21069;&#36864;&#20986;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21160;&#24577;&#35843;&#25972;&#38544;&#31169;&#20445;&#25252;&#21644;&#31995;&#32479;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20351;&#20854;&#25805;&#20316;&#36866;&#24212;&#20010;&#20154;&#34892;&#20026;&#27169;&#24335;&#21644;&#20559;&#22909;&#12290;&#25105;&#20204;&#20027;&#35201;&#24378;&#35843;&#38754;&#20020;&#22788;&#29702;&#20154;&#31867;&#34892;&#20026;&#30340;&#21487;&#21464;&#21644;&#19981;&#26029;&#21457;&#23637;&#30340;&#25361;&#25112;&#65292;&#20351;&#24471;&#38745;&#24577;&#38544;&#31169;&#27169;&#22411;&#22833;&#25928;&#12290;&#36890;&#36807;&#20854;&#24212;&#29992;&#65292;&#35780;&#20272;&#20102;PAPER-HILT&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05864v1 Announce Type: new  Abstract: Reinforcement Learning (RL) has increasingly become a preferred method over traditional rule-based systems in diverse human-in-the-loop (HITL) applications due to its adaptability to the dynamic nature of human interactions. However, integrating RL in such settings raises significant privacy concerns, as it might inadvertently expose sensitive user information. Addressing this, our paper focuses on developing PAPER-HILT, an innovative, adaptive RL strategy through exploiting an early-exit approach designed explicitly for privacy preservation in HITL environments. This approach dynamically adjusts the tradeoff between privacy protection and system utility, tailoring its operation to individual behavioral patterns and preferences. We mainly highlight the challenge of dealing with the variable and evolving nature of human behavior, which renders static privacy models ineffective. PAPER-HILT's effectiveness is evaluated through its applicati
&lt;/p&gt;</description></item><item><title>&#29616;&#26377;&#30340;LLM&#27700;&#21360;&#31995;&#32479;&#34429;&#28982;&#20855;&#26377;&#36136;&#37327;&#20445;&#30041;&#12289;&#40065;&#26834;&#24615;&#21644;&#20844;&#24320;&#26816;&#27979;API&#31561;&#20248;&#28857;&#65292;&#20294;&#20063;&#22240;&#27492;&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#25915;&#20987;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#22871;&#23454;&#29992;&#25351;&#21335;&#20197;&#32531;&#35299;&#36825;&#20123;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2402.16187</link><description>&lt;p&gt;
&#21033;&#29992;&#20854;&#20248;&#21183;&#25915;&#20987;LLM&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
Attacking LLM Watermarks by Exploiting Their Strengths
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16187
&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;LLM&#27700;&#21360;&#31995;&#32479;&#34429;&#28982;&#20855;&#26377;&#36136;&#37327;&#20445;&#30041;&#12289;&#40065;&#26834;&#24615;&#21644;&#20844;&#24320;&#26816;&#27979;API&#31561;&#20248;&#28857;&#65292;&#20294;&#20063;&#22240;&#27492;&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#25915;&#20987;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#22871;&#23454;&#29992;&#25351;&#21335;&#20197;&#32531;&#35299;&#36825;&#20123;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#24471;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#25991;&#26412;&#12289;&#20195;&#30721;&#21644;&#22270;&#29255;&#33021;&#22815;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#27169;&#20223;&#20154;&#31867;&#29983;&#25104;&#30340;&#20869;&#23481;&#12290;&#27700;&#21360;&#25216;&#26415;&#26088;&#22312;&#23558;&#20449;&#24687;&#23884;&#20837;&#27169;&#22411;&#30340;&#36755;&#20986;&#20013;&#20197;&#39564;&#35777;&#20854;&#26469;&#28304;&#65292;&#23545;&#20110;&#20943;&#23569;&#23545;&#36825;&#20123;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#30340;&#28389;&#29992;&#38750;&#24120;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27700;&#21360;&#26041;&#26696;&#20173;&#28982;&#20196;&#20154;&#24847;&#22806;&#22320;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#30340;LLM&#27700;&#21360;&#31995;&#32479;&#20849;&#20139;&#30340;&#21487;&#21462;&#29305;&#24615;&#65292;&#20363;&#22914;&#36136;&#37327;&#20445;&#30041;&#12289;&#40065;&#26834;&#24615;&#21644;&#20844;&#24320;&#26816;&#27979;API&#65292;&#21453;&#36807;&#26469;&#21364;&#20351;&#36825;&#20123;&#31995;&#32479;&#23481;&#26131;&#36973;&#21463;&#21508;&#31181;&#25915;&#20987;&#12290;&#25105;&#20204;&#22312;&#24120;&#35265;&#27700;&#21360;&#35774;&#35745;&#36873;&#25321;&#26041;&#38754;&#20005;&#26684;&#30740;&#31350;&#28508;&#22312;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#25915;&#20987;&#30340;&#26368;&#20339;&#23454;&#36341;&#21644;&#38450;&#24481;&#25514;&#26045;&#8212;&#8212;&#24314;&#31435;&#20102;&#19968;&#22871;&#23884;&#20837;&#21644;&#26816;&#27979;LLM&#27700;&#21360;&#30340;&#23454;&#29992;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16187v1 Announce Type: cross  Abstract: Advances in generative models have made it possible for AI-generated text, code, and images to mirror human-generated content in many applications. Watermarking, a technique that aims to embed information in the output of a model to verify its source, is useful for mitigating misuse of such AI-generated content. However, existing watermarking schemes remain surprisingly susceptible to attack. In particular, we show that desirable properties shared by existing LLM watermarking systems such as quality preservation, robustness, and public detection APIs can in turn make these systems vulnerable to various attacks. We rigorously study potential attacks in terms of common watermark design choices, and propose best practices and defenses for mitigation -- establishing a set of practical guidelines for embedding and detection of LLM watermarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#23545;&#25239;&#24615;&#20027;&#23548;&#36755;&#20837;&#65288;ADIs&#65289;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#20856;&#22411;VFL&#31995;&#32479;&#20013;&#30340;&#23384;&#22312;&#12290;&#35813;&#30740;&#31350;&#20026;&#38450;&#27490;ADIs&#30340;&#20351;&#29992;&#25552;&#20379;&#20102;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2201.02775</link><description>&lt;p&gt;
ADI: &#22312;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#23545;&#25239;&#24615;&#20027;&#23548;&#36755;&#20837;
&lt;/p&gt;
&lt;p&gt;
ADI: Adversarial Dominating Inputs in Vertical Federated Learning Systems. (arXiv:2201.02775v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.02775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#23545;&#25239;&#24615;&#20027;&#23548;&#36755;&#20837;&#65288;ADIs&#65289;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#20856;&#22411;VFL&#31995;&#32479;&#20013;&#30340;&#23384;&#22312;&#12290;&#35813;&#30740;&#31350;&#20026;&#38450;&#27490;ADIs&#30340;&#20351;&#29992;&#25552;&#20379;&#20102;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#65288;VFL&#65289;&#31995;&#32479;&#20316;&#20026;&#22788;&#29702;&#20998;&#25955;&#22312;&#35768;&#22810;&#20010;&#20307;&#26469;&#28304;&#20013;&#30340;&#25968;&#25454;&#30340;&#27010;&#24565;&#32780;&#21464;&#24471;&#31361;&#20986;&#65292;&#26080;&#38656;&#23558;&#20854;&#38598;&#20013;&#21270;&#12290;&#22810;&#20010;&#21442;&#19982;&#32773;&#20197;&#38544;&#31169;&#24847;&#35782;&#30340;&#26041;&#24335;&#21327;&#20316;&#35757;&#32451;&#22522;&#20110;&#20854;&#26412;&#22320;&#25968;&#25454;&#30340;&#27169;&#22411;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;VFL&#24050;&#25104;&#20026;&#22312;&#32452;&#32455;&#20043;&#38388;&#23433;&#20840;&#23398;&#20064;&#27169;&#22411;&#30340;&#20107;&#23454;&#35299;&#20915;&#26041;&#26696;&#65292;&#20801;&#35768;&#20849;&#20139;&#30693;&#35782;&#32780;&#19981;&#24433;&#21709;&#20219;&#20309;&#20010;&#20154;&#30340;&#38544;&#31169;&#12290;&#23613;&#31649;VFL&#31995;&#32479;&#30340;&#21457;&#23637;&#26124;&#30427;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#26576;&#20123;&#21442;&#19982;&#32773;&#30340;&#36755;&#20837;&#65292;&#31216;&#20026;&#23545;&#25239;&#24615;&#20027;&#23548;&#36755;&#20837;&#65288;ADIs&#65289;&#65292;&#21487;&#20197;&#25903;&#37197;&#20849;&#21516;&#25512;&#26029;&#26397;&#30528;&#23545;&#25163;&#30340;&#24847;&#24895;&#26041;&#21521;&#24182;&#36843;&#20351;&#20854;&#20182;&#65288;&#21463;&#23475;&#32773;&#65289;&#21442;&#19982;&#32773;&#20570;&#20986;&#24494;&#19981;&#36275;&#36947;&#30340;&#36129;&#29486;&#65292;&#22833;&#21435;&#36890;&#24120;&#22312;&#32852;&#37030;&#23398;&#20064;&#22330;&#26223;&#20013;&#25552;&#20379;&#30340;&#23545;&#20854;&#36129;&#29486;&#37325;&#35201;&#24615;&#30340;&#22870;&#21169;&#12290;&#25105;&#20204;&#23545;ADIs&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#39318;&#20808;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#20856;&#22411;&#30340;VFL&#31995;&#32479;&#20013;&#30340;&#23384;&#22312;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Vertical federated learning (VFL) system has recently become prominent as a concept to process data distributed across many individual sources without the need to centralize it. Multiple participants collaboratively train models based on their local data in a privacy-aware manner. To date, VFL has become a de facto solution to securely learn a model among organizations, allowing knowledge to be shared without compromising privacy of any individuals. Despite the prosperous development of VFL systems, we find that certain inputs of a participant, named adversarial dominating inputs (ADIs), can dominate the joint inference towards the direction of the adversary's will and force other (victim) participants to make negligible contributions, losing rewards that are usually offered regarding the importance of their contributions in federated learning scenarios. We conduct a systematic study on ADIs by first proving their existence in typical VFL systems. We then propose gradient-based methods
&lt;/p&gt;</description></item></channel></rss>