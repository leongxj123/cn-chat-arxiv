<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#20171;&#32461;&#20102;MaleficNet 2.0&#65292;&#19968;&#31181;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#23884;&#20837;&#24694;&#24847;&#36719;&#20214;&#30340;&#26032;&#25216;&#26415;&#65292;&#20854;&#27880;&#20837;&#25216;&#26415;&#20855;&#26377;&#38544;&#34109;&#24615;&#65292;&#19981;&#20250;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#19988;&#23545;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#20013;&#30340;&#24694;&#24847;&#26377;&#25928;&#36127;&#36733;&#36827;&#34892;&#27880;&#20837;</title><link>https://arxiv.org/abs/2403.03593</link><description>&lt;p&gt;
&#24744;&#20449;&#20219;&#24744;&#30340;&#27169;&#22411;&#21527;&#65311;&#28145;&#24230;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#20013;&#26032;&#20852;&#30340;&#24694;&#24847;&#36719;&#20214;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
Do You Trust Your Model? Emerging Malware Threats in the Deep Learning Ecosystem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03593
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;MaleficNet 2.0&#65292;&#19968;&#31181;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#23884;&#20837;&#24694;&#24847;&#36719;&#20214;&#30340;&#26032;&#25216;&#26415;&#65292;&#20854;&#27880;&#20837;&#25216;&#26415;&#20855;&#26377;&#38544;&#34109;&#24615;&#65292;&#19981;&#20250;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#19988;&#23545;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#20013;&#30340;&#24694;&#24847;&#26377;&#25928;&#36127;&#36733;&#36827;&#34892;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#39640;&#36136;&#37327;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36825;&#26159;&#22240;&#20026;&#38656;&#35201;&#35745;&#31639;&#21644;&#25216;&#26415;&#35201;&#27714;&#12290;&#36234;&#26469;&#36234;&#22810;&#30340;&#20010;&#20154;&#12289;&#26426;&#26500;&#21644;&#20844;&#21496;&#36234;&#26469;&#36234;&#22810;&#22320;&#20381;&#36182;&#20110;&#22312;&#20844;&#20849;&#20195;&#30721;&#24211;&#20013;&#25552;&#20379;&#30340;&#39044;&#35757;&#32451;&#30340;&#31532;&#19977;&#26041;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#30452;&#25509;&#20351;&#29992;&#25110;&#38598;&#25104;&#21040;&#20135;&#21697;&#31649;&#36947;&#20013;&#32780;&#27809;&#26377;&#29305;&#27530;&#30340;&#39044;&#38450;&#25514;&#26045;&#65292;&#22240;&#20026;&#23427;&#20204;&#23454;&#38469;&#19978;&#21482;&#26159;&#20197;&#24352;&#37327;&#24418;&#24335;&#30340;&#25968;&#25454;&#65292;&#34987;&#35748;&#20026;&#26159;&#23433;&#20840;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#20379;&#24212;&#38142;&#23041;&#32961;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;MaleficNet 2.0&#65292;&#19968;&#31181;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#23884;&#20837;&#33258;&#35299;&#21387;&#33258;&#25191;&#34892;&#24694;&#24847;&#36719;&#20214;&#30340;&#26032;&#25216;&#26415;&#12290;MaleficNet 2.0&#20351;&#29992;&#25193;&#39057;&#20449;&#36947;&#32534;&#30721;&#32467;&#21512;&#32416;&#38169;&#25216;&#26415;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#20013;&#27880;&#20837;&#24694;&#24847;&#26377;&#25928;&#36733;&#33655;&#12290;MaleficNet 2.0&#27880;&#20837;&#25216;&#26415;&#20855;&#26377;&#38544;&#34109;&#24615;&#65292;&#19981;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#23545;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03593v1 Announce Type: cross  Abstract: Training high-quality deep learning models is a challenging task due to computational and technical requirements. A growing number of individuals, institutions, and companies increasingly rely on pre-trained, third-party models made available in public repositories. These models are often used directly or integrated in product pipelines with no particular precautions, since they are effectively just data in tensor form and considered safe. In this paper, we raise awareness of a new machine learning supply chain threat targeting neural networks. We introduce MaleficNet 2.0, a novel technique to embed self-extracting, self-executing malware in neural networks. MaleficNet 2.0 uses spread-spectrum channel coding combined with error correction techniques to inject malicious payloads in the parameters of deep neural networks. MaleficNet 2.0 injection technique is stealthy, does not degrade the performance of the model, and is robust against 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20449;&#24687;&#35770;&#26694;&#26550;&#20998;&#26512;&#20102;&#26368;&#20808;&#36827;&#30340;&#20284;&#28982;&#27604;&#25915;&#20987;&#23545;&#19981;&#30830;&#23450;&#24615;&#12289;&#26657;&#20934;&#27700;&#24179;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#24433;&#21709;&#65292;&#30740;&#31350;&#20102;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#20013;&#38544;&#21547;&#30340;&#39118;&#38505;</title><link>https://arxiv.org/abs/2402.10686</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#12289;&#26657;&#20934;&#21644;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65306;&#20449;&#24687;&#35770;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Uncertainty, Calibration, and Membership Inference Attacks: An Information-Theoretic Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10686
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#35770;&#26694;&#26550;&#20998;&#26512;&#20102;&#26368;&#20808;&#36827;&#30340;&#20284;&#28982;&#27604;&#25915;&#20987;&#23545;&#19981;&#30830;&#23450;&#24615;&#12289;&#26657;&#20934;&#27700;&#24179;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#24433;&#21709;&#65292;&#30740;&#31350;&#20102;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#20013;&#38544;&#21547;&#30340;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIA&#65289;&#20013;&#65292;&#25915;&#20987;&#32773;&#21033;&#29992;&#20856;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34920;&#29616;&#20986;&#30340;&#36807;&#24230;&#33258;&#20449;&#26469;&#30830;&#23450;&#29305;&#23450;&#25968;&#25454;&#28857;&#26159;&#21542;&#34987;&#29992;&#20110;&#35757;&#32451;&#30446;&#26631;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#20449;&#24687;&#29702;&#35770;&#26694;&#26550;&#20869;&#20998;&#26512;&#20102;&#26368;&#20808;&#36827;&#30340;&#20284;&#28982;&#27604;&#25915;&#20987;&#65288;LiRA&#65289;&#30340;&#24615;&#33021;&#65292;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#20801;&#35768;&#30740;&#31350;&#30495;&#23454;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#30001;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#38598;&#24341;&#36215;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#30446;&#26631;&#27169;&#22411;&#30340;&#26657;&#20934;&#27700;&#24179;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#25915;&#20987;&#32773;&#20174;&#30446;&#26631;&#27169;&#22411;&#25509;&#25910;&#21040;&#30340;&#20449;&#24687;&#36880;&#28176;&#20943;&#23569;&#65306;&#32622;&#20449;&#21521;&#37327;&#65288;CV&#65289;&#25259;&#38706;&#65292;&#20854;&#20013;&#36755;&#20986;&#27010;&#29575;&#21521;&#37327;&#34987;&#21457;&#24067;&#65307;&#30495;&#23454;&#26631;&#31614;&#32622;&#20449;&#24230;&#65288;TLC&#65289;&#25259;&#38706;&#65292;&#20854;&#20013;&#21482;&#26377;&#27169;&#22411;&#20998;&#37197;&#32473;&#30495;&#23454;&#26631;&#31614;&#30340;&#27010;&#29575;&#26159;&#21487;&#29992;&#30340;&#65307;&#20197;&#21450;&#20915;&#31574;&#38598;&#65288;DS&#65289;&#25259;&#38706;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10686v1 Announce Type: cross  Abstract: In a membership inference attack (MIA), an attacker exploits the overconfidence exhibited by typical machine learning models to determine whether a specific data point was used to train a target model. In this paper, we analyze the performance of the state-of-the-art likelihood ratio attack (LiRA) within an information-theoretical framework that allows the investigation of the impact of the aleatoric uncertainty in the true data generation process, of the epistemic uncertainty caused by a limited training data set, and of the calibration level of the target model. We compare three different settings, in which the attacker receives decreasingly informative feedback from the target model: confidence vector (CV) disclosure, in which the output probability vector is released; true label confidence (TLC) disclosure, in which only the probability assigned to the true label is made available by the model; and decision set (DS) disclosure, in 
&lt;/p&gt;</description></item></channel></rss>