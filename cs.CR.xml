<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#33021;&#22815;&#35780;&#20272;&#38750;&#31169;&#23494;&#25968;&#25454;&#30456;&#20851;&#39044;&#22788;&#29702;&#31639;&#27861;&#24341;&#36215;&#30340;&#39069;&#22806;&#38544;&#31169;&#25104;&#26412;&#65292;&#24182;&#21033;&#29992;&#24179;&#28369;DP&#21644;&#39044;&#22788;&#29702;&#31639;&#27861;&#30340;&#26377;&#30028;&#25935;&#24863;&#24615;&#24314;&#31435;&#25972;&#20307;&#38544;&#31169;&#20445;&#35777;&#30340;&#19978;&#38480;</title><link>https://arxiv.org/abs/2403.13041</link><description>&lt;p&gt;
&#20855;&#26377;&#38750;&#31169;&#23494;&#39044;&#22788;&#29702;&#30340;&#21487;&#35777;&#26126;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Provable Privacy with Non-Private Pre-Processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13041
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#33021;&#22815;&#35780;&#20272;&#38750;&#31169;&#23494;&#25968;&#25454;&#30456;&#20851;&#39044;&#22788;&#29702;&#31639;&#27861;&#24341;&#36215;&#30340;&#39069;&#22806;&#38544;&#31169;&#25104;&#26412;&#65292;&#24182;&#21033;&#29992;&#24179;&#28369;DP&#21644;&#39044;&#22788;&#29702;&#31639;&#27861;&#30340;&#26377;&#30028;&#25935;&#24863;&#24615;&#24314;&#31435;&#25972;&#20307;&#38544;&#31169;&#20445;&#35777;&#30340;&#19978;&#38480;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20998;&#26512;&#24046;&#20998;&#31169;&#23494;&#65288;DP&#65289;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#26102;&#65292;&#36890;&#24120;&#20250;&#24573;&#30053;&#25968;&#25454;&#30456;&#20851;&#30340;&#39044;&#22788;&#29702;&#30340;&#28508;&#22312;&#38544;&#31169;&#25104;&#26412;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#30001;&#38750;&#31169;&#23494;&#25968;&#25454;&#30456;&#20851;&#39044;&#22788;&#29702;&#31639;&#27861;&#24341;&#36215;&#30340;&#39069;&#22806;&#38544;&#31169;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#26032;&#30340;&#25216;&#26415;&#27010;&#24565;&#24314;&#31435;&#20102;&#25972;&#20307;&#38544;&#31169;&#20445;&#35777;&#30340;&#19978;&#38480;&#65306;&#19968;&#31181;&#31216;&#20026;&#24179;&#28369;DP&#30340;DP&#21464;&#20307;&#20197;&#21450;&#39044;&#22788;&#29702;&#31639;&#27861;&#30340;&#26377;&#30028;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13041v1 Announce Type: cross  Abstract: When analysing Differentially Private (DP) machine learning pipelines, the potential privacy cost of data-dependent pre-processing is frequently overlooked in privacy accounting. In this work, we propose a general framework to evaluate the additional privacy cost incurred by non-private data-dependent pre-processing algorithms. Our framework establishes upper bounds on the overall privacy guarantees by utilising two new technical notions: a variant of DP termed Smooth DP and the bounded sensitivity of the pre-processing algorithms. In addition to the generic framework, we provide explicit overall privacy guarantees for multiple data-dependent pre-processing algorithms, such as data imputation, quantization, deduplication and PCA, when used in combination with several DP algorithms. Notably, this framework is also simple to implement, allowing direct integration into existing DP pipelines.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25581;&#31034;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#39118;&#38505;&#65292;&#25351;&#20986;&#23545;&#25163;&#21487;&#20197;&#21033;&#29992;LMs&#30340;&#25351;&#31034;&#36981;&#24490;&#33021;&#21147;&#36731;&#26494;&#22320;&#20174;&#25968;&#25454;&#23384;&#20648;&#20013;&#30452;&#25509;&#25552;&#21462;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#35774;&#35745;&#20102;&#25915;&#20987;&#23545;&#29983;&#20135;RAG&#27169;&#22411;GPTs&#36896;&#25104;&#25968;&#25454;&#23384;&#20648;&#27844;&#28431;&#12290;</title><link>https://arxiv.org/abs/2402.17840</link><description>&lt;p&gt;
&#36981;&#24490;&#25105;&#30340;&#25351;&#31034;&#24182;&#35828;&#20986;&#30495;&#30456;&#65306;&#26469;&#33258;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#30340;&#21487;&#25193;&#23637;&#25968;&#25454;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17840
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25581;&#31034;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#39118;&#38505;&#65292;&#25351;&#20986;&#23545;&#25163;&#21487;&#20197;&#21033;&#29992;LMs&#30340;&#25351;&#31034;&#36981;&#24490;&#33021;&#21147;&#36731;&#26494;&#22320;&#20174;&#25968;&#25454;&#23384;&#20648;&#20013;&#30452;&#25509;&#25552;&#21462;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#35774;&#35745;&#20102;&#25915;&#20987;&#23545;&#29983;&#20135;RAG&#27169;&#22411;GPTs&#36896;&#25104;&#25968;&#25454;&#23384;&#20648;&#27844;&#28431;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#23558;&#22806;&#37096;&#30693;&#35782;&#32435;&#20837;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#23450;&#21046;&#36866;&#24212;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;Retrieval-In-Context RAG&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#39118;&#38505;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#23545;&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#30340;LMs&#26500;&#24314;&#30340;RAG&#31995;&#32479;&#36827;&#34892;&#25552;&#31034;&#27880;&#20837;&#26102;&#65292;&#23545;&#25163;&#21487;&#20197;&#21033;&#29992;LMs&#30340;&#25351;&#31034;&#36981;&#24490;&#33021;&#21147;&#36731;&#26494;&#22320;&#20174;&#25968;&#25454;&#23384;&#20648;&#20013;&#30452;&#25509;&#25552;&#21462;&#25991;&#26412;&#25968;&#25454;&#12290;&#36825;&#31181;&#28431;&#27934;&#23384;&#22312;&#20110;&#35206;&#30422;Llama2&#12289;Mistral/Mixtral&#12289;Vicuna&#12289;SOLAR&#12289;WizardLM&#12289;Qwen1.5&#21644;Platypus2&#31561;&#22810;&#31181;&#29616;&#20195;LMs&#30340;&#24191;&#27867;&#33539;&#22260;&#20869;&#65292;&#24182;&#19988;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#30340;&#25193;&#22823;&#65292;&#21033;&#29992;&#33021;&#21147;&#21152;&#21095;&#12290;&#23558;&#30740;&#31350;&#25193;&#23637;&#21040;&#29983;&#20135;RAG&#27169;&#22411;GPTs&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#25915;&#20987;&#65292;&#21487;&#20197;&#22312;&#23545;25&#20010;&#38543;&#26426;&#36873;&#25321;&#30340;&#23450;&#21046;GPTs&#26045;&#21152;&#26368;&#22810;2&#20010;&#26597;&#35810;&#26102;&#20197;100%&#25104;&#21151;&#29575;&#23548;&#33268;&#25968;&#25454;&#23384;&#20648;&#27844;&#28431;&#65292;&#24182;&#19988;&#25105;&#20204;&#33021;&#22815;&#20197;77,000&#23383;&#30340;&#20070;&#31821;&#20013;&#30340;&#25991;&#26412;&#25968;&#25454;&#30340;&#25552;&#21462;&#29575;&#20026;41%&#65292;&#20197;&#21450;&#22312;&#21547;&#26377;1,569,00&#35789;&#30340;&#35821;&#26009;&#24211;&#20013;&#30340;&#25991;&#26412;&#25968;&#25454;&#30340;&#25552;&#21462;&#29575;&#20026;3%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17840v1 Announce Type: cross  Abstract: Retrieval-Augmented Generation (RAG) improves pre-trained models by incorporating external knowledge at test time to enable customized adaptation. We study the risk of datastore leakage in Retrieval-In-Context RAG Language Models (LMs). We show that an adversary can exploit LMs' instruction-following capabilities to easily extract text data verbatim from the datastore of RAG systems built with instruction-tuned LMs via prompt injection. The vulnerability exists for a wide range of modern LMs that span Llama2, Mistral/Mixtral, Vicuna, SOLAR, WizardLM, Qwen1.5, and Platypus2, and the exploitability exacerbates as the model size scales up. Extending our study to production RAG models GPTs, we design an attack that can cause datastore leakage with a 100% success rate on 25 randomly selected customized GPTs with at most 2 queries, and we extract text data verbatim at a rate of 41% from a book of 77,000 words and 3% from a corpus of 1,569,00
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#27491;&#24577;&#20998;&#24067;&#19981;&#21487;&#21306;&#20998;&#24615;&#35889;&#23450;&#29702; (NDIS Theorem)&#65292;&#26088;&#22312;&#21033;&#29992;&#26597;&#35810;&#26412;&#36523;&#30340;&#38543;&#26426;&#24615;&#25913;&#36827;&#38543;&#26426;&#21270;&#26426;&#22120;&#23398;&#20064;&#26597;&#35810;&#30340;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2309.01243</link><description>&lt;p&gt;
&#27491;&#24577;&#20998;&#24067;&#19981;&#21487;&#21306;&#20998;&#24615;&#35889;&#21450;&#20854;&#22312;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Normal Distributions Indistinguishability Spectrum and its Application to Privacy-Preserving Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.01243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#27491;&#24577;&#20998;&#24067;&#19981;&#21487;&#21306;&#20998;&#24615;&#35889;&#23450;&#29702; (NDIS Theorem)&#65292;&#26088;&#22312;&#21033;&#29992;&#26597;&#35810;&#26412;&#36523;&#30340;&#38543;&#26426;&#24615;&#25913;&#36827;&#38543;&#26426;&#21270;&#26426;&#22120;&#23398;&#20064;&#26597;&#35810;&#30340;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35201;&#23454;&#29616;&#24046;&#20998;&#38544;&#31169;(DP)&#65292;&#36890;&#24120;&#38656;&#35201;&#38543;&#26426;&#21270;&#22522;&#30784;&#26597;&#35810;&#30340;&#36755;&#20986;&#12290;&#22312;&#22823;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#20154;&#20204;&#32463;&#24120;&#20351;&#29992;&#38543;&#26426;&#21270;&#33609;&#22270;/&#32858;&#21512;&#31639;&#27861;&#26469;&#20351;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#21464;&#24471;&#21487;&#34892;&#12290;&#30452;&#35266;&#22320;&#65292;&#36825;&#26679;&#30340;&#26426;&#22120;&#23398;&#20064;(ML)&#31639;&#27861;&#24212;&#35813;&#25552;&#20379;&#19968;&#20123;&#22266;&#26377;&#30340;&#38544;&#31169;&#24615;&#65292;&#20294;&#29616;&#26377;&#30340;&#22823;&#37096;&#20998;DP&#26426;&#21046;&#24182;&#27809;&#26377;&#21033;&#29992;&#36825;&#31181;&#22266;&#26377;&#30340;&#38543;&#26426;&#24615;&#65292;&#23548;&#33268;&#28508;&#22312;&#30340;&#22810;&#20313;&#22122;&#38899;&#12290;&#25105;&#20204;&#24037;&#20316;&#30340;&#21160;&#26426;&#38382;&#39064;&#26159;&#65306;(&#22914;&#20309;)&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#26597;&#35810;&#26412;&#36523;&#30340;&#38543;&#26426;&#24615;&#26469;&#25552;&#39640;&#38543;&#26426;&#21270;ML&#26597;&#35810;&#30340;DP&#26426;&#21046;&#30340;&#25928;&#29992;&#65311;&#20026;&#20102;&#32473;&#20986;&#31215;&#26497;&#30340;&#31572;&#26696;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27491;&#24577;&#20998;&#24067;&#19981;&#21487;&#21306;&#20998;&#24615;&#35889;&#23450;&#29702;(&#31616;&#31216;&#20026;NDIS&#23450;&#29702;)&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#28145;&#36828;&#23454;&#38469;&#24433;&#21709;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;NDIS&#26159;&#19968;&#20010;&#29992;&#20110;$(\epsilon,\delta)$-&#19981;&#21487;&#21306;&#20998;&#24615;&#35889;(&#31616;&#31216;&#20026;$
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.01243v2 Announce Type: replace-cross  Abstract: To achieve differential privacy (DP) one typically randomizes the output of the underlying query. In big data analytics, one often uses randomized sketching/aggregation algorithms to make processing high-dimensional data tractable. Intuitively, such machine learning (ML) algorithms should provide some inherent privacy, yet most if not all existing DP mechanisms do not leverage this inherent randomness, resulting in potentially redundant noising.   The motivating question of our work is:   (How) can we improve the utility of DP mechanisms for randomized ML queries, by leveraging the randomness of the query itself?   Towards a (positive) answer, we prove the Normal Distributions Indistinguishability Spectrum Theorem (in short, NDIS Theorem), a theoretical result with far-reaching practical implications. In a nutshell, NDIS is a closed-form analytic computation for the $(\epsilon,\delta)$-indistinguishability-spectrum (in short, $
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;FedMLSecurity&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#23427;&#21487;&#20197;&#27169;&#25311;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#23545;&#25239;&#25915;&#20987;&#24182;&#25552;&#20379;&#30456;&#24212;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#35813;&#27979;&#35797;&#23545;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#32852;&#21512;&#20248;&#21270;&#22120;&#37117;&#21487;&#20197;&#36866;&#29992;&#65292;&#24182;&#19988;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;</title><link>http://arxiv.org/abs/2306.04959</link><description>&lt;p&gt;
FedMLSecurity&#65306;&#32852;&#37030;&#23398;&#20064;&#19982;LLMs&#20013;&#25915;&#20987;&#19982;&#38450;&#24481;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
FedMLSecurity: A Benchmark for Attacks and Defenses in Federated Learning and LLMs. (arXiv:2306.04959v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;FedMLSecurity&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#23427;&#21487;&#20197;&#27169;&#25311;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#23545;&#25239;&#25915;&#20987;&#24182;&#25552;&#20379;&#30456;&#24212;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#35813;&#27979;&#35797;&#23545;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#32852;&#21512;&#20248;&#21270;&#22120;&#37117;&#21487;&#20197;&#36866;&#29992;&#65292;&#24182;&#19988;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FedMLSecurity&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#27169;&#25311;&#23545;&#25239;&#25915;&#20987;&#21644;&#30456;&#24212;&#38450;&#24481;&#26426;&#21046;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#20316;&#20026;&#24320;&#28304;&#24211;FedML&#30340;&#19968;&#20010;&#37325;&#35201;&#27169;&#22359;&#65292;FedMLSecurity&#22686;&#24378;&#20102;FedML&#30340;&#23433;&#20840;&#35780;&#20272;&#33021;&#21147;&#12290;FedMLSecurity&#21253;&#21547;&#20004;&#20010;&#20027;&#35201;&#32452;&#20214;&#65306;FedMLAttacker&#27169;&#25311;&#22312;FL&#35757;&#32451;&#20013;&#27880;&#20837;&#30340;&#25915;&#20987;&#65292;&#32780;FedMLDefender&#21017;&#27169;&#25311;&#26088;&#22312;&#20943;&#36731;&#25915;&#20987;&#24433;&#21709;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;FedMLSecurity&#26159;&#24320;&#28304;&#30340;&#65292;&#21487;&#36866;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;&#20363;&#22914;&#36923;&#36753;&#22238;&#24402;&#65292;ResNet&#65292;GAN&#31561;&#65289;&#21644;&#32852;&#21512;&#20248;&#21270;&#22120;&#65288;&#20363;&#22914;FedAVG&#65292;FedOPT&#65292;FedNOVA&#31561;&#65289;&#12290;&#26412;&#25991;&#30340;&#23454;&#39564;&#35780;&#20272;&#36824;&#23637;&#31034;&#20102;&#23558;FedMLSecurity&#36731;&#26494;&#24212;&#29992;&#20110;LLMs&#30340;&#20415;&#21033;&#24615;&#65292;&#36827;&#19968;&#27493;&#24378;&#21270;&#20102;&#20854;&#21508;&#31181;&#22330;&#26223;&#19979;&#30340;&#36890;&#29992;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces FedMLSecurity, a benchmark that simulates adversarial attacks and corresponding defense mechanisms in Federated Learning (FL). As an integral module of the open-sourced library FedML that facilitates FL algorithm development and performance comparison, FedMLSecurity enhances the security assessment capacity of FedML. FedMLSecurity comprises two principal components: FedMLAttacker, which simulates attacks injected into FL training, and FedMLDefender, which emulates defensive strategies designed to mitigate the impacts of the attacks. FedMLSecurity is open-sourced 1 and is customizable to a wide range of machine learning models (e.g., Logistic Regression, ResNet, GAN, etc.) and federated optimizers (e.g., FedAVG, FedOPT, FedNOVA, etc.). Experimental evaluations in this paper also demonstrate the ease of application of FedMLSecurity to Large Language Models (LLMs), further reinforcing its versatility and practical utility in various scenarios.
&lt;/p&gt;</description></item></channel></rss>