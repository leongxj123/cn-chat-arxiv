<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#25945;&#24072;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25932;&#23545;&#31034;&#20363;&#30340;&#31232;&#30095;&#36755;&#20986;&#65292;&#24182;&#19982;&#26631;&#20934;&#35757;&#32451;&#25968;&#25454;&#32467;&#21512;&#20351;&#29992;&#65292;&#26469;&#21152;&#24378;&#25945;&#24072;&#27169;&#22411;&#23545;&#23398;&#29983;&#33976;&#39311;&#30340;&#38450;&#24481;&#12290;</title><link>https://arxiv.org/abs/2403.05181</link><description>&lt;p&gt;
Adversarial Sparse Teacher: &#23545;&#25239;&#25932;&#23545;&#31034;&#20363;&#65292;&#38450;&#24481;&#29992;&#23545;&#25239;&#31034;&#20363;&#36827;&#34892;&#30340;&#22522;&#20110;&#33976;&#39311;&#30340;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Adversarial Sparse Teacher: Defense Against Distillation-Based Model Stealing Attacks Using Adversarial Examples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#25945;&#24072;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25932;&#23545;&#31034;&#20363;&#30340;&#31232;&#30095;&#36755;&#20986;&#65292;&#24182;&#19982;&#26631;&#20934;&#35757;&#32451;&#25968;&#25454;&#32467;&#21512;&#20351;&#29992;&#65292;&#26469;&#21152;&#24378;&#25945;&#24072;&#27169;&#22411;&#23545;&#23398;&#29983;&#33976;&#39311;&#30340;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#20419;&#36827;&#20102;&#23558;&#39640;&#32423;&#25945;&#24072;&#27169;&#22411;&#30340;&#21306;&#20998;&#33021;&#21147;&#36716;&#31227;&#21040;&#26356;&#31616;&#21333;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#30830;&#20445;&#25552;&#39640;&#24615;&#33021;&#32780;&#19981;&#24433;&#21709;&#20934;&#30830;&#24615;&#12290;&#23427;&#20063;&#34987;&#29992;&#20110;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;&#65292;&#20854;&#20013;&#23545;&#25163;&#20351;&#29992;KD&#26469;&#27169;&#20223;&#25945;&#24072;&#27169;&#22411;&#30340;&#21151;&#33021;&#12290;&#26368;&#36817;&#22312;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#21463;&#21040;&#20102;&#21533;&#21868;&#25945;&#24072;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#31232;&#30095;&#36755;&#20986;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#23398;&#29983;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#30693;&#35782;&#20135;&#26435;&#27844;&#38706;&#30340;&#39118;&#38505;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#35757;&#32451;&#25945;&#24072;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#26681;&#26412;&#19978;&#20445;&#25252;&#20854;logits&#65292;&#21463;&#8220;&#24694;&#27602;&#25945;&#24072;&#8221;&#29702;&#24565;&#30340;&#24433;&#21709;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#23545;&#25239;&#31034;&#20363;&#30340;&#31232;&#30095;&#36755;&#20986;&#19982;&#26631;&#20934;&#35757;&#32451;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#21152;&#24378;&#25945;&#24072;&#23545;&#23398;&#29983;&#33976;&#39311;&#30340;&#38450;&#24481;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24039;&#22937;&#22320;&#20943;&#23569;&#20102;&#30456;&#23545;&#30340;e
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05181v1 Announce Type: new  Abstract: Knowledge Distillation (KD) facilitates the transfer of discriminative capabilities from an advanced teacher model to a simpler student model, ensuring performance enhancement without compromising accuracy. It is also exploited for model stealing attacks, where adversaries use KD to mimic the functionality of a teacher model. Recent developments in this domain have been influenced by the Stingy Teacher model, which provided empirical analysis showing that sparse outputs can significantly degrade the performance of student models. Addressing the risk of intellectual property leakage, our work introduces an approach to train a teacher model that inherently protects its logits, influenced by the Nasty Teacher concept. Differing from existing methods, we incorporate sparse outputs of adversarial examples with standard training data to strengthen the teacher's defense against student distillation. Our approach carefully reduces the relative e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22914;&#20309;&#21033;&#29992;&#32534;&#30721;&#22120;&#27169;&#22411;&#21644;&#35299;&#30721;&#22120;&#27169;&#22411;&#26469;&#29702;&#35299;&#21644;&#24635;&#32467;&#32593;&#32476;&#25915;&#20987;&#36807;&#31243;&#20013;&#30340;&#31574;&#30053;&#21644;&#30446;&#30340;&#65292;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#25216;&#26415;&#26469;&#25552;&#21462;&#30456;&#20851;&#19978;&#19979;&#25991;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#20135;&#29983;&#38169;&#35823;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.00280</link><description>&lt;p&gt;
&#25512;&#36827;TTP&#20998;&#26512;&#65306;&#21033;&#29992;&#20165;&#32534;&#30721;&#22120;&#21644;&#20165;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#24182;&#25552;&#21319;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
Advancing TTP Analysis: Harnessing the Power of Encoder-Only and Decoder-Only Language Models with Retrieval Augmented Generation. (arXiv:2401.00280v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22914;&#20309;&#21033;&#29992;&#32534;&#30721;&#22120;&#27169;&#22411;&#21644;&#35299;&#30721;&#22120;&#27169;&#22411;&#26469;&#29702;&#35299;&#21644;&#24635;&#32467;&#32593;&#32476;&#25915;&#20987;&#36807;&#31243;&#20013;&#30340;&#31574;&#30053;&#21644;&#30446;&#30340;&#65292;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#25216;&#26415;&#26469;&#25552;&#21462;&#30456;&#20851;&#19978;&#19979;&#25991;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#20135;&#29983;&#38169;&#35823;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25112;&#26415;&#65292;&#25216;&#26415;&#21644;&#31243;&#24207;&#65288;TTPs&#65289;&#27010;&#36848;&#20102;&#25915;&#20987;&#32773;&#21033;&#29992;&#28431;&#27934;&#30340;&#26041;&#27861;&#12290;&#30001;&#20110;&#20551;&#23450;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#22797;&#26434;&#30340;&#20381;&#36182;&#20851;&#31995;&#21644;&#20869;&#22312;&#30340;&#27169;&#31946;&#24615;&#65292;&#23545;MITRE ATT&#65286;CK&#26694;&#26550;&#20013;&#30340;TTPs&#30340;&#35299;&#37322;&#23545;&#20110;&#32593;&#32476;&#23433;&#20840;&#20174;&#19994;&#20154;&#21592;&#26469;&#35828;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#27493;&#23548;&#33268;&#20102;&#26368;&#36817;&#22312;&#30740;&#31350;&#20013;&#25506;&#32034;&#20854;&#22312;&#32593;&#32476;&#23433;&#20840;&#25805;&#20316;&#20013;&#30340;&#29992;&#36884;&#30340;&#28608;&#22686;&#12290;&#36825;&#24341;&#36215;&#20102;&#25105;&#20204;&#30340;&#30097;&#38382;&#65292;&#20165;&#32534;&#30721;&#22120;&#65288;&#20363;&#22914;RoBERTa&#65289;&#21644;&#20165;&#35299;&#30721;&#22120;&#65288;&#20363;&#22914;GPT-3.5&#65289;LLMs&#23545;&#20110;&#29702;&#35299;&#21644;&#24635;&#32467;TTPs&#20197;&#36890;&#30693;&#20998;&#26512;&#20154;&#21592;&#26377;&#20851;&#32593;&#32476;&#25915;&#20987;&#36807;&#31243;&#30340;&#39044;&#26399;&#30446;&#30340;&#65288;&#21363;&#31574;&#30053;&#65289;&#30340;&#33021;&#21147;&#22914;&#20309;&#12290;&#26368;&#20808;&#36827;&#30340;LLMs&#24050;&#32463;&#26174;&#31034;&#20986;&#23481;&#26131;&#20135;&#29983;&#38169;&#35823;&#20449;&#24687;&#65292;&#36825;&#22312;&#32593;&#32476;&#23433;&#20840;&#31561;&#20851;&#38190;&#39046;&#22495;&#26159;&#26377;&#38382;&#39064;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25216;&#26415;&#26469;&#20026;&#20165;&#35299;&#30721;&#22120;&#30340;LLMs&#25552;&#21462;&#27599;&#20010;&#32593;&#32476;&#25915;&#20987;&#36807;&#31243;&#30340;&#30456;&#20851;&#19978;&#19979;&#25991;&#65288;&#26080;&#38656;&#24494;&#35843;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tactics, Techniques, and Procedures (TTPs) outline the methods attackers use to exploit vulnerabilities. The interpretation of TTPs in the MITRE ATT&amp;CK framework can be challenging for cybersecurity practitioners due to presumed expertise, complex dependencies, and inherent ambiguity. Meanwhile, advancements with Large Language Models (LLMs) have led to recent surge in studies exploring its uses in cybersecurity operations. This leads us to question how well encoder-only (e.g., RoBERTa) and decoder-only (e.g., GPT-3.5) LLMs can comprehend and summarize TTPs to inform analysts of the intended purposes (i.e., tactics) of a cyberattack procedure. The state-of-the-art LLMs have shown to be prone to hallucination by providing inaccurate information, which is problematic in critical domains like cybersecurity. Therefore, we propose the use of Retrieval Augmented Generation (RAG) techniques to extract relevant contexts for each cyberattack procedure for decoder-only LLMs (without fine-tuning)
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#28151;&#21512;&#26631;&#20934;&#20998;&#31867;&#22120;&#21644;&#40065;&#26834;&#27169;&#22411;&#30340;&#36755;&#20986;&#27010;&#29575;&#26469;&#20943;&#36731;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#65292;&#36827;&#32780;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24179;&#28369;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#38477;&#20302;&#23454;&#29616;&#40065;&#26834;&#24615;&#30340;&#20934;&#30830;&#24230;&#24809;&#32602;&#12290;</title><link>http://arxiv.org/abs/2301.12554</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#24179;&#28369;&#25913;&#21892;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#24615;-&#40065;&#26834;&#24615;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Improving the Accuracy-Robustness Trade-Off of Classifiers via Adaptive Smoothing. (arXiv:2301.12554v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#28151;&#21512;&#26631;&#20934;&#20998;&#31867;&#22120;&#21644;&#40065;&#26834;&#27169;&#22411;&#30340;&#36755;&#20986;&#27010;&#29575;&#26469;&#20943;&#36731;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#65292;&#36827;&#32780;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24179;&#28369;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#38477;&#20302;&#23454;&#29616;&#40065;&#26834;&#24615;&#30340;&#20934;&#30830;&#24230;&#24809;&#32602;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#22823;&#37327;&#22686;&#24378;&#31070;&#32463;&#20998;&#31867;&#22120;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#22312;&#28165;&#26224;&#24230;&#26041;&#38754;&#23384;&#22312;&#19981;&#21487;&#25509;&#21463;&#30340;&#20005;&#37325;&#24809;&#32602;&#65292;&#23454;&#36341;&#32773;&#20173;&#28982;&#19981;&#24895;&#37319;&#29992;&#36825;&#20123;&#25216;&#26415;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#36890;&#36807;&#28151;&#21512;&#26631;&#20934;&#20998;&#31867;&#22120;&#21644;&#24378;&#40065;&#26834;&#27169;&#22411;&#30340;&#36755;&#20986;&#27010;&#29575;&#65292;&#20854;&#20013;&#26631;&#20934;&#32593;&#32476;&#20248;&#21270;&#28165;&#26224;&#24230;&#32780;&#19981;&#26159;&#19968;&#33324;&#30340;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#26174;&#30528;&#20943;&#36731;&#36825;&#31181;&#20934;&#30830;&#24615;-&#40065;&#26834;&#24615;&#26435;&#34913;&#38382;&#39064;&#12290;&#25105;&#20204;&#26174;&#31034;&#20986;&#22522;&#20110;&#40065;&#26834;&#24615;&#30340;&#22522;&#26412;&#20998;&#31867;&#22120;&#30340;&#27491;&#30830;&#21644;&#19981;&#27491;&#30830;&#31034;&#20363;&#30340;&#32622;&#20449;&#24230;&#24046;&#24322;&#26159;&#36825;&#31181;&#25913;&#21892;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#38500;&#25552;&#20379;&#30452;&#35266;&#21644;&#32463;&#39564;&#35777;&#25454;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#29616;&#23454;&#20551;&#35774;&#19979;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#28151;&#21512;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#19968;&#20010;&#23545;&#25239;&#24615;&#36755;&#20837;&#26816;&#27979;&#22120;&#36866;&#24212;&#20026;&#28151;&#21512;&#32593;&#32476;&#65292;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#20004;&#20010;&#22522;&#26412;&#27169;&#22411;&#30340;&#28151;&#21512;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#20943;&#23569;&#23454;&#29616;&#40065;&#26834;&#24615;&#30340;&#20934;&#30830;&#24615;&#24809;&#32602;&#12290;
&lt;/p&gt;
&lt;p&gt;
While prior research has proposed a plethora of methods that enhance the adversarial robustness of neural classifiers, practitioners are still reluctant to adopt these techniques due to their unacceptably severe penalties in clean accuracy. This paper shows that by mixing the output probabilities of a standard classifier and a robust model, where the standard network is optimized for clean accuracy and is not robust in general, this accuracy-robustness trade-off can be significantly alleviated. We show that the robust base classifier's confidence difference for correct and incorrect examples is the key ingredient of this improvement. In addition to providing intuitive and empirical evidence, we also theoretically certify the robustness of the mixed classifier under realistic assumptions. Furthermore, we adapt an adversarial input detector into a mixing network that adaptively adjusts the mixture of the two base models, further reducing the accuracy penalty of achieving robustness. The 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#19981;&#38656;&#35201;&#21453;&#21521;&#20256;&#25773;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;BAFFLE&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#22810;&#20010;&#27491;&#21521;&#36807;&#31243;&#20272;&#35745;&#26799;&#24230;&#65292;&#20855;&#26377;&#39640;&#20869;&#23384;&#25928;&#29575;&#65292;&#23481;&#26131;&#36866;&#24212;&#19978;&#20256;&#24102;&#23485;&#65292;&#19982;&#30828;&#20214;&#20248;&#21270;&#21644;&#27169;&#22411;&#37327;&#21270;/&#20462;&#21098;&#20860;&#23481;&#65292;&#36866;&#29992;&#20110;&#21463;&#20449;&#20219;&#30340;&#25191;&#34892;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2301.12195</link><description>&lt;p&gt;
&#12298;&#32852;&#37030;&#23398;&#20064;&#26159;&#21542;&#30495;&#27491;&#38656;&#35201;&#21453;&#21521;&#20256;&#25773;&#65311;&#12299;
&lt;/p&gt;
&lt;p&gt;
Does Federated Learning Really Need Backpropagation?. (arXiv:2301.12195v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#19981;&#38656;&#35201;&#21453;&#21521;&#20256;&#25773;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;BAFFLE&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#22810;&#20010;&#27491;&#21521;&#36807;&#31243;&#20272;&#35745;&#26799;&#24230;&#65292;&#20855;&#26377;&#39640;&#20869;&#23384;&#25928;&#29575;&#65292;&#23481;&#26131;&#36866;&#24212;&#19978;&#20256;&#24102;&#23485;&#65292;&#19982;&#30828;&#20214;&#20248;&#21270;&#21644;&#27169;&#22411;&#37327;&#21270;/&#20462;&#21098;&#20860;&#23481;&#65292;&#36866;&#29992;&#20110;&#21463;&#20449;&#20219;&#30340;&#25191;&#34892;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#22320;&#35753;&#23458;&#25143;&#31471;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#26381;&#21153;&#22120;&#27169;&#22411;&#30340;&#19968;&#33324;&#24615;&#21407;&#21017;&#65292;&#32780;&#26080;&#38656;&#20849;&#20139;&#26412;&#22320;&#25968;&#25454;&#12290;FL&#26159;&#19968;&#20010;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#30340;&#26377;&#21069;&#36884;&#30340;&#26694;&#26550;&#65292;&#20294;&#20854;&#26631;&#20934;&#35757;&#32451;&#33539;&#24335;&#35201;&#27714;&#23458;&#25143;&#31471;&#36890;&#36807;&#27169;&#22411;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#20197;&#35745;&#31639;&#26799;&#24230;&#12290;&#30001;&#20110;&#36825;&#20123;&#23458;&#25143;&#31471;&#36890;&#24120;&#26159;&#36793;&#32536;&#35774;&#22791;&#32780;&#19981;&#26159;&#23436;&#20840;&#21463;&#20449;&#20219;&#30340;&#65292;&#22240;&#27492;&#22312;&#23427;&#20204;&#19978;&#25191;&#34892;&#21453;&#21521;&#20256;&#25773;&#20250;&#20135;&#29983;&#35745;&#31639;&#21644;&#23384;&#20648;&#24320;&#38144;&#20197;&#21450;&#30333;&#30418;&#28431;&#27934;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#21453;&#21521;&#20256;&#25773;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#31216;&#20026;BAFFLE&#65292;&#20854;&#20013;&#21453;&#21521;&#20256;&#25773;&#26367;&#25442;&#20026;&#22810;&#20010;&#27491;&#21521;&#36807;&#31243;&#20197;&#20272;&#35745;&#26799;&#24230;&#12290;BAFFLE&#20855;&#26377;&#20197;&#19979;&#20248;&#28857;&#65306;1&#65289;&#20869;&#23384;&#25928;&#29575;&#39640;&#24182;&#19988;&#23481;&#26131;&#36866;&#24212;&#19978;&#20256;&#24102;&#23485;&#65307;2&#65289;&#19982;&#20165;&#25512;&#29702;&#30828;&#20214;&#20248;&#21270;&#20197;&#21450;&#27169;&#22411;&#37327;&#21270;&#25110;&#20462;&#21098;&#20860;&#23481;&#65307;3&#65289;&#38750;&#24120;&#36866;&#21512;&#21463;&#20449;&#20219;&#30340;&#25191;&#34892;&#29615;&#22659;&#65292;&#22240;&#20026;BAFFLE&#20013;&#30340;&#23458;&#25143;&#31471;&#20165;&#25191;&#34892;&#27491;&#21521;&#20256;&#25773;&#24182;&#36820;&#22238;&#19968;&#32452;&#26631;&#37327;&#21040;&#26381;&#21153;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#20351;&#29992;&#20102;BAFFLE&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a general principle for decentralized clients to train a server model collectively without sharing local data. FL is a promising framework with practical applications, but its standard training paradigm requires the clients to backpropagate through the model to compute gradients. Since these clients are typically edge devices and not fully trusted, executing backpropagation on them incurs computational and storage overhead as well as white-box vulnerability. In light of this, we develop backpropagation-free federated learning, dubbed BAFFLE, in which backpropagation is replaced by multiple forward processes to estimate gradients. BAFFLE is 1) memory-efficient and easily fits uploading bandwidth; 2) compatible with inference-only hardware optimization and model quantization or pruning; and 3) well-suited to trusted execution environments, because the clients in BAFFLE only execute forward propagation and return a set of scalars to the server. Empirically we us
&lt;/p&gt;</description></item></channel></rss>