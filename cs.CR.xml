<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29616;&#23454;&#23545;&#25239;&#35774;&#32622;&#19979;&#37325;&#24314;&#25104;&#21151;&#29575;&#30340;&#27491;&#24335;&#19978;&#38480;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#25903;&#25345;&#65292;&#26377;&#21161;&#20110;&#26356;&#26126;&#26234;&#22320;&#36873;&#25321;&#38544;&#31169;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.12861</link><description>&lt;p&gt;
&#22312;&#27809;&#26377;&#25968;&#25454;&#20808;&#39564;&#26465;&#20214;&#19979;&#38480;&#21046;&#23545;&#25239;&#32773;&#37325;&#24314;&#25915;&#20987;&#25104;&#21151;&#29575;
&lt;/p&gt;
&lt;p&gt;
Bounding Reconstruction Attack Success of Adversaries Without Data Priors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29616;&#23454;&#23545;&#25239;&#35774;&#32622;&#19979;&#37325;&#24314;&#25104;&#21151;&#29575;&#30340;&#27491;&#24335;&#19978;&#38480;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#25903;&#25345;&#65292;&#26377;&#21161;&#20110;&#26356;&#26126;&#26234;&#22320;&#36873;&#25321;&#38544;&#31169;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#37325;&#24314;&#25915;&#20987;&#23384;&#22312;&#27844;&#28431;&#25935;&#24863;&#25968;&#25454;&#30340;&#39118;&#38505;&#12290;&#22312;&#29305;&#23450;&#24773;&#22659;&#19979;&#65292;&#23545;&#25163;&#21487;&#20197;&#20351;&#29992;&#27169;&#22411;&#30340;&#26799;&#24230;&#20960;&#20046;&#23436;&#32654;&#22320;&#37325;&#24314;&#35757;&#32451;&#25968;&#25454;&#26679;&#26412;&#12290;&#22312;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#65292;&#21487;&#20197;&#25552;&#20379;&#23545;&#36825;&#31181;&#37325;&#24314;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;&#27491;&#24335;&#19978;&#38480;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#36825;&#20123;&#19978;&#38480;&#26159;&#22312;&#21487;&#33021;&#19981;&#31526;&#21512;&#39640;&#24230;&#29616;&#23454;&#23454;&#29992;&#24615;&#30340;&#26368;&#22351;&#24773;&#20917;&#20551;&#35774;&#19979;&#21046;&#23450;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#22312;&#29616;&#23454;&#23545;&#25239;&#35774;&#32622;&#19979;&#30340;&#37325;&#24314;&#25104;&#21151;&#29575;&#27491;&#24335;&#19978;&#38480;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#25903;&#25345;&#36825;&#20123;&#19978;&#38480;&#12290;&#36890;&#36807;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#29616;&#23454;&#24773;&#22659;&#20013;&#65292;&#65288;a&#65289;&#39044;&#26399;&#30340;&#37325;&#24314;&#25104;&#21151;&#29575;&#21487;&#20197;&#22312;&#19981;&#21516;&#32972;&#26223;&#21644;&#19981;&#21516;&#24230;&#37327;&#19979;&#24471;&#21040;&#36866;&#24403;&#30340;&#38480;&#21046;&#65292;&#36825;&#65288;b&#65289;&#26377;&#21161;&#20110;&#26356;&#26126;&#26234;&#22320;&#36873;&#25321;&#38544;&#31169;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12861v1 Announce Type: new  Abstract: Reconstruction attacks on machine learning (ML) models pose a strong risk of leakage of sensitive data. In specific contexts, an adversary can (almost) perfectly reconstruct training data samples from a trained model using the model's gradients. When training ML models with differential privacy (DP), formal upper bounds on the success of such reconstruction attacks can be provided. So far, these bounds have been formulated under worst-case assumptions that might not hold high realistic practicality. In this work, we provide formal upper bounds on reconstruction success under realistic adversarial settings against ML models trained with DP and support these bounds with empirical results. With this, we show that in realistic scenarios, (a) the expected reconstruction success can be bounded appropriately in different contexts and by different metrics, which (b) allows for a more educated choice of a privacy parameter.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#21435;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#23569;&#37327;&#35757;&#32451;&#31034;&#20363;&#26469;&#23454;&#29616;&#20219;&#21153;&#36866;&#24212;&#35757;&#32451;&#25968;&#25454;&#30340;&#31934;&#30830;&#21435;&#23398;&#20064;&#65292;&#24182;&#19982;&#24494;&#35843;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#35752;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.00751</link><description>&lt;p&gt;
&#26080;&#27861;&#23398;&#20064;&#30340;&#31639;&#27861;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unlearnable Algorithms for In-context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#21435;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#23569;&#37327;&#35757;&#32451;&#31034;&#20363;&#26469;&#23454;&#29616;&#20219;&#21153;&#36866;&#24212;&#35757;&#32451;&#25968;&#25454;&#30340;&#31934;&#30830;&#21435;&#23398;&#20064;&#65292;&#24182;&#19982;&#24494;&#35843;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27169;&#22411;&#34987;&#36234;&#26469;&#36234;&#22810;&#22320;&#37096;&#32626;&#22312;&#26410;&#30693;&#26469;&#28304;&#30340;&#25968;&#25454;&#19978;&#65292;&#26426;&#22120;&#21435;&#23398;&#20064;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#35201;&#23454;&#29616;&#31934;&#30830;&#30340;&#21435;&#23398;&#20064;&#8212;&#8212;&#22312;&#27809;&#26377;&#20351;&#29992;&#35201;&#36951;&#24536;&#30340;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#19982;&#27169;&#22411;&#20998;&#24067;&#21305;&#37197;&#30340;&#27169;&#22411;&#8212;&#8212;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#25110;&#20302;&#25928;&#30340;&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#37325;&#26032;&#35757;&#32451;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20219;&#21153;&#36866;&#24212;&#38454;&#27573;&#30340;&#39640;&#25928;&#21435;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;LLM&#36827;&#34892;&#20219;&#21153;&#36866;&#24212;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#21487;&#20197;&#23454;&#29616;&#20219;&#21153;&#36866;&#24212;&#35757;&#32451;&#25968;&#25454;&#30340;&#39640;&#25928;&#31934;&#30830;&#21435;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#23569;&#37327;&#35757;&#32451;&#31034;&#20363;&#21152;&#21040;LLM&#30340;&#25552;&#31034;&#21069;&#38754;&#65288;&#29992;&#20110;&#20219;&#21153;&#36866;&#24212;&#65289;&#65292;&#21517;&#20026;ERASE&#65292;&#23427;&#30340;&#21435;&#23398;&#20064;&#25805;&#20316;&#25104;&#26412;&#19982;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#26080;&#20851;&#65292;&#24847;&#21619;&#30528;&#23427;&#36866;&#29992;&#20110;&#22823;&#22411;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#24494;&#35843;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#35752;&#35770;&#20102;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#36825;&#20351;&#25105;&#20204;&#24471;&#21040;&#20102;&#20197;&#19979;&#32467;&#35770;&#65306;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning is a desirable operation as models get increasingly deployed on data with unknown provenance. However, achieving exact unlearning -- obtaining a model that matches the model distribution when the data to be forgotten was never used -- is challenging or inefficient, often requiring significant retraining. In this paper, we focus on efficient unlearning methods for the task adaptation phase of a pretrained large language model (LLM). We observe that an LLM's ability to do in-context learning for task adaptation allows for efficient exact unlearning of task adaptation training data. We provide an algorithm for selecting few-shot training examples to prepend to the prompt given to an LLM (for task adaptation), ERASE, whose unlearning operation cost is independent of model and dataset size, meaning it scales to large models and datasets. We additionally compare our approach to fine-tuning approaches and discuss the trade-offs between the two approaches. This leads us to p
&lt;/p&gt;</description></item></channel></rss>