<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#22823;&#22810;&#25968;&#29616;&#20195;LLM&#21463;&#21040;softmax&#29942;&#39048;&#24433;&#21709;&#65292;&#21487;&#20197;&#20197;&#36739;&#20302;&#25104;&#26412;&#33719;&#21462;API&#20445;&#25252;&#30340;LLM&#30340;&#38750;&#20844;&#24320;&#20449;&#24687;&#21644;&#35299;&#38145;&#22810;&#31181;&#21151;&#33021;</title><link>https://arxiv.org/abs/2403.09539</link><description>&lt;p&gt;
API&#20445;&#25252;&#30340;LLMs&#30340;&#26631;&#24535;&#27844;&#38706;&#19987;&#26377;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Logits of API-Protected LLMs Leak Proprietary Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09539
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#20195;LLM&#21463;&#21040;softmax&#29942;&#39048;&#24433;&#21709;&#65292;&#21487;&#20197;&#20197;&#36739;&#20302;&#25104;&#26412;&#33719;&#21462;API&#20445;&#25252;&#30340;LLM&#30340;&#38750;&#20844;&#24320;&#20449;&#24687;&#21644;&#35299;&#38145;&#22810;&#31181;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21830;&#19994;&#21270;&#23548;&#33268;&#20102;&#39640;&#32423;API-only&#25509;&#20837;&#19987;&#26377;&#27169;&#22411;&#30340;&#24120;&#35265;&#23454;&#36341;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#23545;&#20110;&#27169;&#22411;&#26550;&#26500;&#26377;&#20445;&#23432;&#30340;&#20551;&#35774;&#65292;&#20063;&#21487;&#20197;&#20174;&#30456;&#23545;&#36739;&#23569;&#30340;API&#26597;&#35810;&#20013;&#23398;&#20064;&#20851;&#20110;API&#20445;&#25252;&#30340;LLM&#30340;&#22823;&#37327;&#38750;&#20844;&#24320;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#20351;&#29992;OpenAI&#30340;gpt-3.5-turbo&#20165;&#33457;&#36153;&#19981;&#21040;1000&#32654;&#20803;&#65289;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#38598;&#20013;&#22312;&#19968;&#20010;&#20851;&#38190;&#35266;&#23519;&#19978;&#65306;&#22823;&#22810;&#25968;&#29616;&#20195;LLM&#21463;&#21040;&#20102;softmax&#29942;&#39048;&#30340;&#24433;&#21709;&#65292;&#36825;&#38480;&#21046;&#20102;&#27169;&#22411;&#36755;&#20986;&#21040;&#23436;&#25972;&#36755;&#20986;&#31354;&#38388;&#30340;&#32447;&#24615;&#23376;&#31354;&#38388;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#27169;&#22411;&#22270;&#20687;&#25110;&#27169;&#22411;&#31614;&#21517;&#65292;&#20174;&#32780;&#20197;&#36739;&#20302;&#30340;&#25104;&#26412;&#35299;&#38145;&#20102;&#20960;&#31181;&#21151;&#33021;&#65306;&#26377;&#25928;&#21457;&#29616;LLM&#30340;&#38544;&#34255;&#22823;&#23567;&#65292;&#33719;&#21462;&#23436;&#25972;&#35789;&#27719;&#36755;&#20986;&#65292;&#26816;&#27979;&#21644;&#28040;&#38500;&#19981;&#21516;&#27169;&#22411;&#26356;&#26032;&#65292;&#35782;&#21035;&#32473;&#23450;&#21333;&#20010;&#23436;&#25972;LLM&#36755;&#20986;&#30340;&#28304;LLM&#65292;&#20197;&#21450;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09539v1 Announce Type: cross  Abstract: The commercialization of large language models (LLMs) has led to the common practice of high-level API-only access to proprietary models. In this work, we show that even with a conservative assumption about the model architecture, it is possible to learn a surprisingly large amount of non-public information about an API-protected LLM from a relatively small number of API queries (e.g., costing under $1,000 for OpenAI's gpt-3.5-turbo). Our findings are centered on one key observation: most modern LLMs suffer from a softmax bottleneck, which restricts the model outputs to a linear subspace of the full output space. We show that this lends itself to a model image or a model signature which unlocks several capabilities with affordable cost: efficiently discovering the LLM's hidden size, obtaining full-vocabulary outputs, detecting and disambiguating different model updates, identifying the source LLM given a single full LLM output, and eve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27719;&#38598;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#23433;&#20840;&#39046;&#22495;&#30340;&#30456;&#20851;&#27010;&#24565;&#65292;&#31995;&#32479;&#22320;&#24418;&#24335;&#21270;&#20102;&#29983;&#25104;&#24335;AI&#20195;&#29702;&#31995;&#32479;&#20013;&#30340;&#31192;&#23494;&#21246;&#32467;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#25514;&#26045;&#12290;&#36890;&#36807;&#27979;&#35797;&#21508;&#31181;&#24418;&#24335;&#30340;&#31192;&#23494;&#21246;&#32467;&#25152;&#38656;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#27169;&#22411;&#30340;&#38544;&#20889;&#33021;&#21147;&#26377;&#38480;&#65292;&#20294; GPT-4 &#23637;&#31034;&#20102;&#33021;&#21147;&#30340;&#39134;&#36291;&#12290;</title><link>https://arxiv.org/abs/2402.07510</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;AI&#20195;&#29702;&#20043;&#38388;&#30340;&#31192;&#23494;&#21246;&#32467;
&lt;/p&gt;
&lt;p&gt;
Secret Collusion Among Generative AI Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27719;&#38598;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#23433;&#20840;&#39046;&#22495;&#30340;&#30456;&#20851;&#27010;&#24565;&#65292;&#31995;&#32479;&#22320;&#24418;&#24335;&#21270;&#20102;&#29983;&#25104;&#24335;AI&#20195;&#29702;&#31995;&#32479;&#20013;&#30340;&#31192;&#23494;&#21246;&#32467;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#25514;&#26045;&#12290;&#36890;&#36807;&#27979;&#35797;&#21508;&#31181;&#24418;&#24335;&#30340;&#31192;&#23494;&#21246;&#32467;&#25152;&#38656;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#27169;&#22411;&#30340;&#38544;&#20889;&#33021;&#21147;&#26377;&#38480;&#65292;&#20294; GPT-4 &#23637;&#31034;&#20102;&#33021;&#21147;&#30340;&#39134;&#36291;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33021;&#21147;&#19978;&#30340;&#22686;&#24378;&#20026;&#36890;&#20449;&#30340;&#29983;&#25104;&#24335;AI&#20195;&#29702;&#22242;&#38431;&#35299;&#20915;&#32852;&#21512;&#20219;&#21153;&#30340;&#24212;&#29992;&#25171;&#24320;&#20102;&#21487;&#33021;&#24615;&#12290;&#36825;&#24341;&#21457;&#20102;&#20851;&#20110;&#26410;&#32463;&#25480;&#26435;&#20998;&#20139;&#20449;&#24687;&#25110;&#20854;&#20182;&#19981;&#24517;&#35201;&#30340;&#20195;&#29702;&#21327;&#35843;&#24418;&#24335;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#25361;&#25112;&#12290;&#29616;&#20195;&#38544;&#20889;&#26415;&#25216;&#26415;&#21487;&#33021;&#20351;&#36825;&#31181;&#21160;&#24577;&#38590;&#20197;&#26816;&#27979;&#12290;&#26412;&#25991;&#36890;&#36807;&#27762;&#21462;&#20154;&#24037;&#26234;&#33021;&#21644;&#23433;&#20840;&#39046;&#22495;&#30456;&#20851;&#27010;&#24565;&#65292;&#20840;&#38754;&#31995;&#32479;&#22320;&#24418;&#24335;&#21270;&#20102;&#29983;&#25104;&#24335;AI&#20195;&#29702;&#31995;&#32479;&#20013;&#30340;&#31192;&#23494;&#21246;&#32467;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#38544;&#20889;&#26415;&#30340;&#21160;&#26426;&#65292;&#24182;&#25552;&#20986;&#20102;&#21508;&#31181;&#32531;&#35299;&#25514;&#26045;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26159;&#19968;&#20010;&#27169;&#22411;&#35780;&#20272;&#26694;&#26550;&#65292;&#31995;&#32479;&#22320;&#27979;&#35797;&#20102;&#21508;&#31181;&#24418;&#24335;&#30340;&#31192;&#23494;&#21246;&#32467;&#25152;&#38656;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#24403;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;&#34429;&#28982;&#24403;&#21069;&#27169;&#22411;&#30340;&#38544;&#20889;&#33021;&#21147;&#20173;&#28982;&#26377;&#38480;&#65292;&#20294; GPT-4 &#26174;&#31034;&#20986;&#33021;&#21147;&#30340;&#39134;&#36291;&#65292;&#36825;&#34920;&#26126;&#26377;&#24517;&#35201;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent capability increases in large language models (LLMs) open up applications in which teams of communicating generative AI agents solve joint tasks. This poses privacy and security challenges concerning the unauthorised sharing of information, or other unwanted forms of agent coordination. Modern steganographic techniques could render such dynamics hard to detect. In this paper, we comprehensively formalise the problem of secret collusion in systems of generative AI agents by drawing on relevant concepts from both the AI and security literature. We study incentives for the use of steganography, and propose a variety of mitigation measures. Our investigations result in a model evaluation framework that systematically tests capabilities required for various forms of secret collusion. We provide extensive empirical results across a range of contemporary LLMs. While the steganographic capabilities of current models remain limited, GPT-4 displays a capability jump suggesting the need fo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;3D&#24314;&#27169;&#65292;&#21046;&#20316;&#20986;&#19982;&#26085;&#24120;&#26381;&#35013;&#32441;&#29702;&#30456;&#20284;&#30340;&#23545;&#25239;&#24615;&#20266;&#35013;&#32441;&#29702;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#35270;&#35282;&#19979;&#36991;&#24320;&#20154;&#29289;&#26816;&#27979;&#65292;&#23454;&#29616;&#33258;&#28982;&#22806;&#35266;&#30340;&#26381;&#35013;&#32441;&#29702;&#12290;</title><link>http://arxiv.org/abs/2307.01778</link><description>&lt;p&gt;
&#36890;&#36807;3D&#24314;&#27169;&#65292;&#23454;&#29616;&#33258;&#28982;&#22806;&#35266;&#30340;&#26381;&#35013;&#32441;&#29702;&#20197;&#36867;&#36991;&#20154;&#29289;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Physically Realizable Natural-Looking Clothing Textures Evade Person Detectors via 3D Modeling. (arXiv:2307.01778v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01778
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;3D&#24314;&#27169;&#65292;&#21046;&#20316;&#20986;&#19982;&#26085;&#24120;&#26381;&#35013;&#32441;&#29702;&#30456;&#20284;&#30340;&#23545;&#25239;&#24615;&#20266;&#35013;&#32441;&#29702;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#35270;&#35282;&#19979;&#36991;&#24320;&#20154;&#29289;&#26816;&#27979;&#65292;&#23454;&#29616;&#33258;&#28982;&#22806;&#35266;&#30340;&#26381;&#35013;&#32441;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21046;&#20316;&#23545;&#25239;&#24615;&#26381;&#35013;&#26469;&#36867;&#36991;&#20154;&#29289;&#26816;&#27979;&#22120;&#65292;&#20294;&#35201;&#20040;&#21482;&#23545;&#38480;&#23450;&#30340;&#35270;&#35282;&#26377;&#25928;&#65292;&#35201;&#20040;&#23545;&#20154;&#31867;&#38750;&#24120;&#26126;&#26174;&#12290;&#25105;&#20204;&#26088;&#22312;&#22522;&#20110;3D&#24314;&#27169;&#26469;&#21046;&#20316;&#23545;&#25239;&#24615;&#30340;&#26381;&#35013;&#32441;&#29702;&#65292;&#36825;&#20010;&#24819;&#27861;&#24050;&#32463;&#34987;&#29992;&#20110;&#21046;&#20316;&#21018;&#24615;&#30340;&#23545;&#25239;&#24615;&#29289;&#20307;&#65292;&#22914;3D&#25171;&#21360;&#30340;&#20044;&#40863;&#12290;&#19982;&#21018;&#24615;&#29289;&#20307;&#19981;&#21516;&#65292;&#20154;&#31867;&#21644;&#26381;&#35013;&#26159;&#38750;&#21018;&#24615;&#30340;&#65292;&#36825;&#23548;&#33268;&#20102;&#22312;&#23454;&#38469;&#21046;&#20316;&#20013;&#30340;&#22256;&#38590;&#12290;&#20026;&#20102;&#21046;&#20316;&#20986;&#30475;&#36215;&#26469;&#33258;&#28982;&#30340;&#23545;&#25239;&#24615;&#26381;&#35013;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#35270;&#35282;&#19979;&#36991;&#24320;&#20154;&#29289;&#26816;&#27979;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31867;&#20284;&#20110;&#26085;&#24120;&#26381;&#35013;&#32441;&#29702;&#20043;&#19968;&#30340;&#23545;&#25239;&#24615;&#20266;&#35013;&#32441;&#29702;&#65288;AdvCaT&#65289;&#65292;&#21363;&#20266;&#35013;&#32441;&#29702;&#12290;&#25105;&#20204;&#21033;&#29992;Voronoi&#22270;&#21644;Gumbel-softmax&#25216;&#24039;&#26469;&#21442;&#25968;&#21270;&#20266;&#35013;&#32441;&#29702;&#65292;&#24182;&#36890;&#36807;3D&#24314;&#27169;&#26469;&#20248;&#21270;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#22686;&#24378;&#31649;&#36947;&#65292;&#23558;&#25299;&#25169;&#21512;&#29702;&#30340;&#25237;&#24433;&#65288;TopoProj&#65289;&#21644;Thin Plate Spline&#65288;TPS&#65289;&#32467;&#21512;&#22312;3D&#32593;&#26684;&#19978;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have proposed to craft adversarial clothes for evading person detectors, while they are either only effective at limited viewing angles or very conspicuous to humans. We aim to craft adversarial texture for clothes based on 3D modeling, an idea that has been used to craft rigid adversarial objects such as a 3D-printed turtle. Unlike rigid objects, humans and clothes are non-rigid, leading to difficulties in physical realization. In order to craft natural-looking adversarial clothes that can evade person detectors at multiple viewing angles, we propose adversarial camouflage textures (AdvCaT) that resemble one kind of the typical textures of daily clothes, camouflage textures. We leverage the Voronoi diagram and Gumbel-softmax trick to parameterize the camouflage textures and optimize the parameters via 3D modeling. Moreover, we propose an efficient augmentation pipeline on 3D meshes combining topologically plausible projection (TopoProj) and Thin Plate Spline (TPS) to narr
&lt;/p&gt;</description></item></channel></rss>