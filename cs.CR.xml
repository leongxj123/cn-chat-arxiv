<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>Hufu&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#27700;&#21360;&#31995;&#32479;&#65292;&#21033;&#29992;Transformer&#30340;&#32622;&#25442;&#31561;&#21464;&#24615;&#36136;&#65292;&#23454;&#29616;&#20102;&#22312;&#27169;&#22411;&#20013;&#23884;&#20837;&#27700;&#21360;&#24182;&#20445;&#25345;&#39640;&#20445;&#30495;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.05842</link><description>&lt;p&gt;
Hufu&#65306;&#19968;&#31181;&#36890;&#36807;&#32622;&#25442;&#31561;&#21464;&#24615;&#23545;&#39044;&#35757;&#32451;&#30340;Transformer&#36827;&#34892;&#27700;&#21360;&#22788;&#29702;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#27700;&#21360;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Hufu: A Modality-Agnositc Watermarking System for Pre-Trained Transformers via Permutation Equivariance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05842
&lt;/p&gt;
&lt;p&gt;
Hufu&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#27700;&#21360;&#31995;&#32479;&#65292;&#21033;&#29992;Transformer&#30340;&#32622;&#25442;&#31561;&#21464;&#24615;&#36136;&#65292;&#23454;&#29616;&#20102;&#22312;&#27169;&#22411;&#20013;&#23884;&#20837;&#27700;&#21360;&#24182;&#20445;&#25345;&#39640;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#26381;&#21153;&#30340;&#34028;&#21187;&#21457;&#23637;&#65292;&#20445;&#25252;&#23453;&#36149;&#30340;&#27169;&#22411;&#21442;&#25968;&#20813;&#21463;&#30423;&#31363;&#24050;&#25104;&#20026;&#19968;&#39033;&#36843;&#20999;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#27700;&#21360;&#25216;&#26415;&#34987;&#35748;&#20026;&#26159;&#25152;&#26377;&#26435;&#39564;&#35777;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27700;&#21360;&#26041;&#26696;&#38024;&#23545;&#19981;&#21516;&#30340;&#27169;&#22411;&#21644;&#20219;&#21153;&#23450;&#21046;&#65292;&#38590;&#20197;&#20316;&#20026;&#38598;&#25104;&#30340;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#26381;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Hufu&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#27700;&#21360;&#31995;&#32479;&#65292;&#20381;&#36182;&#20110;Transformer&#30340;&#32622;&#25442;&#31561;&#21464;&#24615;&#36136;&#12290;Hufu&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29305;&#23450;&#32622;&#25442;&#30340;&#19968;&#32452;&#25968;&#25454;&#26679;&#26412;&#19978;&#23884;&#20837;&#27700;&#21360;&#65292;&#23884;&#20837;&#30340;&#27169;&#22411;&#22522;&#26412;&#19978;&#21253;&#21547;&#20004;&#32452;&#26435;&#37325; -- &#19968;&#32452;&#29992;&#20110;&#27491;&#24120;&#20351;&#29992;&#65292;&#21478;&#19968;&#32452;&#29992;&#20110;&#27700;&#21360;&#25552;&#21462;&#65292;&#35302;&#21457;&#26465;&#20214;&#26159;&#32463;&#36807;&#32622;&#25442;&#30340;&#36755;&#20837;&#12290;&#32622;&#25442;&#31561;&#21464;&#24615;&#30830;&#20445;&#36825;&#20004;&#32452;&#27169;&#22411;&#26435;&#37325;&#20043;&#38388;&#30340;&#26368;&#23567;&#24178;&#25200;&#65292;&#20174;&#32780;&#22312;&#27700;&#21360;&#25552;&#21462;&#26102;&#20855;&#26377;&#39640;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05842v1 Announce Type: cross  Abstract: With the blossom of deep learning models and services, it has become an imperative concern to safeguard the valuable model parameters from being stolen. Watermarking is considered an important tool for ownership verification. However, current watermarking schemes are customized for different models and tasks, hard to be integrated as an integrated intellectual protection service. We propose Hufu, a modality-agnostic watermarking system for pre-trained Transformer-based models, relying on the permutation equivariance property of Transformers. Hufu embeds watermark by fine-tuning the pre-trained model on a set of data samples specifically permuted, and the embedded model essentially contains two sets of weights -- one for normal use and the other for watermark extraction which is triggered on permuted inputs. The permutation equivariance ensures minimal interference between these two sets of model weights and thus high fidelity on downst
&lt;/p&gt;</description></item></channel></rss>