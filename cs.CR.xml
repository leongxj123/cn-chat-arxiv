<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#31934;&#30830;&#37325;&#26500;&#25209;&#37327;$b &gt;1$&#30340;&#31639;&#27861;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#26799;&#24230;&#21453;&#28436;&#25915;&#20987;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.03945</link><description>&lt;p&gt;
SPEAR&#65306;&#32852;&#37030;&#23398;&#20064;&#20013;&#25209;&#37327;&#31934;&#30830;&#26799;&#24230;&#21453;&#28436;
&lt;/p&gt;
&lt;p&gt;
SPEAR:Exact Gradient Inversion of Batches in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03945
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#31934;&#30830;&#37325;&#26500;&#25209;&#37327;$b &gt;1$&#30340;&#31639;&#27861;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#26799;&#24230;&#21453;&#28436;&#25915;&#20987;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#22810;&#20010;&#23458;&#25143;&#31471;&#20165;&#19982;&#26381;&#21153;&#22120;&#20849;&#20139;&#20182;&#20204;&#26412;&#22320;&#25968;&#25454;&#30340;&#26799;&#24230;&#26356;&#26032;&#65292;&#32780;&#19981;&#26159;&#23454;&#38469;&#25968;&#25454;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#26368;&#36817;&#21457;&#29616;&#26799;&#24230;&#21453;&#28436;&#25915;&#20987;&#21487;&#20197;&#20174;&#36825;&#20123;&#20849;&#20139;&#30340;&#26799;&#24230;&#20013;&#37325;&#26500;&#20986;&#25968;&#25454;&#12290;&#29616;&#26377;&#30340;&#25915;&#20987;&#21482;&#33021;&#22312;&#37325;&#35201;&#30340;&#35802;&#23454;&#20294;&#22909;&#22855;&#35774;&#32622;&#20013;&#23545;&#25209;&#37327;&#22823;&#23567;&#20026;$b=1$&#30340;&#25968;&#25454;&#36827;&#34892;&#31934;&#30830;&#37325;&#26500;&#65292;&#23545;&#20110;&#26356;&#22823;&#30340;&#25209;&#37327;&#21482;&#33021;&#36827;&#34892;&#36817;&#20284;&#37325;&#26500;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;\emph{&#31532;&#19968;&#20010;&#20934;&#30830;&#37325;&#24314;&#25209;&#37327;$b &gt;1$&#30340;&#31639;&#27861;}&#12290;&#36825;&#31181;&#26041;&#27861;&#32467;&#21512;&#20102;&#23545;&#26799;&#24230;&#26174;&#24335;&#20302;&#31209;&#32467;&#26500;&#30340;&#25968;&#23398;&#35265;&#35299;&#21644;&#22522;&#20110;&#37319;&#26679;&#30340;&#31639;&#27861;&#12290;&#20851;&#38190;&#30340;&#26159;&#65292;&#25105;&#20204;&#21033;&#29992;ReLU&#35825;&#23548;&#30340;&#26799;&#24230;&#31232;&#30095;&#24615;&#65292;&#31934;&#30830;&#22320;&#36807;&#28388;&#25481;&#22823;&#37327;&#38169;&#35823;&#30340;&#26679;&#26412;&#65292;&#20351;&#26368;&#32456;&#30340;&#37325;&#24314;&#27493;&#39588;&#21487;&#34892;&#12290;&#25105;&#20204;&#20026;&#20840;&#36830;&#25509;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;GPU&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03945v1 Announce Type: new  Abstract: Federated learning is a popular framework for collaborative machine learning where multiple clients only share gradient updates on their local data with the server and not the actual data. Unfortunately, it was recently shown that gradient inversion attacks can reconstruct this data from these shared gradients. Existing attacks enable exact reconstruction only for a batch size of $b=1$ in the important honest-but-curious setting, with larger batches permitting only approximate reconstruction. In this work, we propose \emph{the first algorithm reconstructing whole batches with $b &gt;1$ exactly}. This approach combines mathematical insights into the explicit low-rank structure of gradients with a sampling-based algorithm. Crucially, we leverage ReLU-induced gradient sparsity to precisely filter out large numbers of incorrect samples, making a final reconstruction step tractable. We provide an efficient GPU implementation for fully connected 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36828;&#31243;&#35821;&#35328;&#27169;&#22411;&#30340; API &#35775;&#38382;&#26500;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20351;&#27169;&#22411;&#20197;&#26356;&#39640;&#27010;&#29575;&#21457;&#20986;&#26377;&#23475;&#23383;&#31526;&#20018;&#65292;&#32780;&#38750;&#20165;&#20165;&#22522;&#20110;&#27169;&#22411;&#20043;&#38388;&#30340;&#36716;&#31227;&#24615;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2402.12329</link><description>&lt;p&gt;
&#22522;&#20110;&#26597;&#35810;&#30340;&#23545;&#25239;&#24615;&#25552;&#31034;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Query-Based Adversarial Prompt Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12329
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36828;&#31243;&#35821;&#35328;&#27169;&#22411;&#30340; API &#35775;&#38382;&#26500;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20351;&#27169;&#22411;&#20197;&#26356;&#39640;&#27010;&#29575;&#21457;&#20986;&#26377;&#23475;&#23383;&#31526;&#20018;&#65292;&#32780;&#38750;&#20165;&#20165;&#22522;&#20110;&#27169;&#22411;&#20043;&#38388;&#30340;&#36716;&#31227;&#24615;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#20197;&#26500;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#23548;&#33268;&#19968;&#20010;&#23545;&#20854;&#36827;&#34892;&#20102;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#26377;&#23475;&#23383;&#31526;&#20018;&#25110;&#25191;&#34892;&#26377;&#23475;&#34892;&#20026;&#12290;&#29616;&#26377;&#30340;&#25915;&#20987;&#35201;&#20040;&#22312;&#30333;&#30418;&#35774;&#32622;&#20013;&#65288;&#23436;&#20840;&#35775;&#38382;&#27169;&#22411;&#26435;&#37325;&#65289;&#65292;&#35201;&#20040;&#36890;&#36807;&#21487;&#36716;&#31227;&#24615;&#65306;&#19968;&#31181;&#29616;&#35937;&#65292;&#21363;&#22312;&#19968;&#20010;&#27169;&#22411;&#19978;&#31934;&#24515;&#35774;&#35745;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#36890;&#24120;&#22312;&#20854;&#20182;&#27169;&#22411;&#19978;&#20173;&#28982;&#26377;&#25928;&#12290;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#25913;&#36827;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#21033;&#29992; API &#35775;&#38382;&#36828;&#31243;&#35821;&#35328;&#27169;&#22411;&#26469;&#26500;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20351;&#27169;&#22411;&#20197;&#65288;&#26126;&#26174;&#65289;&#26356;&#39640;&#30340;&#27010;&#29575;&#21457;&#20986;&#26377;&#23475;&#23383;&#31526;&#20018;&#65292;&#32780;&#19981;&#33021;&#20165;&#20165;&#20351;&#29992;&#36716;&#31227;&#25915;&#20987;&#12290;&#25105;&#20204;&#22312; GPT-3.5 &#21644; OpenAI &#30340;&#23433;&#20840;&#20998;&#31867;&#22120;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#25915;&#20987;&#65307;&#25105;&#20204;&#33021;&#22815;&#35753; GPT-3.5 &#21457;&#20986;&#26377;&#23475;&#23383;&#31526;&#20018;&#65292;&#32780;&#30446;&#21069;&#30340;&#36716;&#31227;&#25915;&#20987;&#22833;&#36133;&#20102;&#65292;&#24182;&#19988;&#25105;&#20204;&#20960;&#20046;&#20197; 100% &#30340;&#27010;&#29575;&#35268;&#36991;&#20102;&#23433;&#20840;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12329v1 Announce Type: cross  Abstract: Recent work has shown it is possible to construct adversarial examples that cause an aligned language model to emit harmful strings or perform harmful behavior. Existing attacks work either in the white-box setting (with full access to the model weights), or through transferability: the phenomenon that adversarial examples crafted on one model often remain effective on other models. We improve on prior work with a query-based attack that leverages API access to a remote language model to construct adversarial examples that cause the model to emit harmful strings with (much) higher probability than with transfer-only attacks. We validate our attack on GPT-3.5 and OpenAI's safety classifier; we can cause GPT-3.5 to emit harmful strings that current transfer attacks fail at, and we can evade the safety classifier with nearly 100% probability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#25239;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#21512;&#27861;&#21442;&#19982;&#26041;&#38388;&#30340;&#29289;&#29702;&#23618;&#23494;&#38053;&#29983;&#25104;&#65292;&#22312;&#24694;&#24847;&#21487;&#37325;&#26500;&#26234;&#33021;&#38754;&#24178;&#25200;&#19979;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.06663</link><description>&lt;p&gt;
&#29289;&#29702;&#23618;&#23494;&#38053;&#23545;&#25239;&#24694;&#24847;&#21487;&#37325;&#26500;&#26234;&#33021;&#38754;&#30340;&#21487;&#35299;&#37322;&#23545;&#25239;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Explainable Adversarial Learning Framework on Physical Layer Secret Keys Combating Malicious Reconfigurable Intelligent Surface
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#25239;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#21512;&#27861;&#21442;&#19982;&#26041;&#38388;&#30340;&#29289;&#29702;&#23618;&#23494;&#38053;&#29983;&#25104;&#65292;&#22312;&#24694;&#24847;&#21487;&#37325;&#26500;&#26234;&#33021;&#38754;&#24178;&#25200;&#19979;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#37325;&#26500;&#26234;&#33021;&#38754;&#65288;RIS&#65289;&#30340;&#21457;&#23637;&#23545;&#29289;&#29702;&#23618;&#23433;&#20840;&#65288;PLS&#65289;&#26159;&#19968;&#25226;&#21452;&#20995;&#21073;&#12290;&#21512;&#27861;&#30340;RIS&#21487;&#20197;&#20135;&#29983;&#26377;&#30410;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#22686;&#21152;&#20449;&#36947;&#30340;&#38543;&#26426;&#24615;&#65292;&#22686;&#24378;&#29289;&#29702;&#23618;&#23494;&#38053;&#29983;&#25104;&#65288;PL-SKG&#65289;&#65292;&#32780;&#24694;&#24847;&#30340;RIS&#21487;&#20197;&#30772;&#22351;&#21512;&#27861;&#20449;&#36947;&#24182;&#30772;&#35299;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;PL-SKG&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21512;&#27861;&#21442;&#19982;&#26041;&#65288;&#21363;&#29233;&#20029;&#19997;&#21644;&#40077;&#21187;&#65289;&#20043;&#38388;&#30340;&#23545;&#25239;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#20013;&#38388;&#20154;&#24694;&#24847;RIS&#65288;MITM-RIS&#65289;&#31363;&#21548;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#21512;&#27861;&#37197;&#23545;&#21644;MITM-RIS&#20043;&#38388;&#30340;&#29702;&#35770;&#20114;&#20449;&#24687;&#24046;&#36317;&#12290;&#28982;&#21518;&#65292;&#29233;&#20029;&#19997;&#21644;&#40077;&#21187;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#23398;&#20064;&#23454;&#29616;&#19968;&#20010;&#19982;MITM-RIS&#27809;&#26377;&#20114;&#20449;&#24687;&#37325;&#21472;&#30340;&#20849;&#21516;&#29305;&#24449;&#38754;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20351;&#29992;&#31526;&#21495;&#21487;&#35299;&#37322;AI&#65288;xAI&#65289;&#34920;&#31034;&#23545;&#40657;&#30418;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20449;&#21495;&#22788;&#29702;&#35299;&#37322;&#12290;&#36825;&#20123;&#20027;&#23548;&#31070;&#32463;&#20803;&#30340;&#31526;&#21495;&#26415;&#35821;&#26377;&#21161;&#20110;&#29305;&#24449;&#24037;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of reconfigurable intelligent surfaces (RIS) is a double-edged sword to physical layer security (PLS). Whilst a legitimate RIS can yield beneficial impacts including increased channel randomness to enhance physical layer secret key generation (PL-SKG), malicious RIS can poison legitimate channels and crack most of existing PL-SKGs. In this work, we propose an adversarial learning framework between legitimate parties (namely Alice and Bob) to address this Man-in-the-middle malicious RIS (MITM-RIS) eavesdropping. First, the theoretical mutual information gap between legitimate pairs and MITM-RIS is deduced. Then, Alice and Bob leverage generative adversarial networks (GANs) to learn to achieve a common feature surface that does not have mutual information overlap with MITM-RIS. Next, we aid signal processing interpretation of black-box neural networks by using a symbolic explainable AI (xAI) representation. These symbolic terms of dominant neurons aid feature engineering-
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20540;&#20989;&#25968;&#31639;&#27861;&#21644;&#26799;&#24230;&#31639;&#27861;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#21033;&#29992;&#26799;&#24230;&#21453;&#36716;&#37325;&#24314;&#29366;&#24577;&#12289;&#21160;&#20316;&#21644;&#30417;&#30563;&#20449;&#21495;&#65292;&#20197;&#35299;&#20915;&#23884;&#20837;&#24335;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.09273</link><description>&lt;p&gt;
&#20320;&#30340;&#25151;&#38388;&#19981;&#26159;&#31169;&#23494;&#30340;&#65306;&#20851;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Your Room is not Private: Gradient Inversion Attack on Reinforcement Learning. (arXiv:2306.09273v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09273
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20540;&#20989;&#25968;&#31639;&#27861;&#21644;&#26799;&#24230;&#31639;&#27861;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#21033;&#29992;&#26799;&#24230;&#21453;&#36716;&#37325;&#24314;&#29366;&#24577;&#12289;&#21160;&#20316;&#21644;&#30417;&#30563;&#20449;&#21495;&#65292;&#20197;&#35299;&#20915;&#23884;&#20837;&#24335;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23884;&#20837;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#26174;&#33879;&#21457;&#23637;&#21560;&#24341;&#20102;&#20154;&#20204;&#30340;&#26497;&#22823;&#20851;&#27880;&#65292;&#35813;&#25216;&#26415;&#20351;&#24471;&#26426;&#22120;&#20154;&#21487;&#20197;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#23548;&#33322;&#12289;&#24863;&#30693;&#21644;&#20114;&#21160;&#12290;&#30001;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#26174;&#33879;&#36827;&#23637;&#65292;&#38544;&#31169;&#38382;&#39064;&#22312;&#23884;&#20837;&#24335;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#26426;&#22120;&#20154;&#21487;&#20197;&#35775;&#38382;&#22823;&#37327;&#20010;&#20154;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#20851;&#20110;&#20540;&#20989;&#25968;&#31639;&#27861;&#21644;&#26799;&#24230;&#31639;&#27861;&#30340;&#38382;&#39064;&#65292;&#22312;&#30740;&#31350;&#20013;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#32771;&#34385;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#25915;&#20987;&#20540;&#20989;&#25968;&#31639;&#27861;&#21644;&#26799;&#24230;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26799;&#24230;&#21453;&#36716;&#37325;&#24314;&#29366;&#24577;&#12289;&#21160;&#20316;&#21644;&#30417;&#30563;&#20449;&#21495;&#65292;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#36873;&#25321;&#20351;&#29992;&#26799;&#24230;&#36827;&#34892;&#25915;&#20987;&#26159;&#22240;&#20026;&#24120;&#29992;&#30340;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#20165;&#21033;&#29992;&#22522;&#20110;&#31169;&#20154;&#29992;&#25143;&#25968;&#25454;&#35745;&#31639;&#30340;&#26799;&#24230;&#26469;&#20248;&#21270;&#27169;&#22411;&#65292;&#32780;&#19981;&#23384;&#20648;&#25110;&#20256;&#36755;&#29992;&#25143;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prominence of embodied Artificial Intelligence (AI), which empowers robots to navigate, perceive, and engage within virtual environments, has attracted significant attention, owing to the remarkable advancements in computer vision and large language models. Privacy emerges as a pivotal concern within the realm of embodied AI, as the robot accesses substantial personal information. However, the issue of privacy leakage in embodied AI tasks, particularly in relation to reinforcement learning algorithms, has not received adequate consideration in research. This paper aims to address this gap by proposing an attack on the value-based algorithm and the gradient-based algorithm, utilizing gradient inversion to reconstruct states, actions, and supervision signals. The choice of using gradients for the attack is motivated by the fact that commonly employed federated learning techniques solely utilize gradients computed based on private user data to optimize models, without storing or trans
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#35813;&#25915;&#20987;&#26159;&#24191;&#20041;&#20102;DeepFool&#25915;&#20987;&#65292;&#26082;&#26377;&#25928;&#21448;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#36866;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12481</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;DeepFool&#65306;&#27867;&#21270;&#21644;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Revisiting DeepFool: generalization and improvement. (arXiv:2303.12481v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#35813;&#25915;&#20987;&#26159;&#24191;&#20041;&#20102;DeepFool&#25915;&#20987;&#65292;&#26082;&#26377;&#25928;&#21448;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#36866;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#34987;&#24050;&#30693;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#26679;&#26412;&#30340;&#25915;&#20987;&#65292;&#36825;&#20123;&#36755;&#20837;&#31245;&#21152;&#20462;&#25913;&#20415;&#20250;&#23548;&#33268;&#32593;&#32476;&#20570;&#20986;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#36825;&#23548;&#33268;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20197;&#35780;&#20272;&#36825;&#20123;&#32593;&#32476;&#23545;&#27492;&#31867;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#24230;&#37327;&#12290;&#26368;&#23567;l2&#23545;&#25239;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#65292;&#26159;&#19968;&#31181;&#29305;&#21035;&#37325;&#35201;&#30340;&#40065;&#26834;&#24615;&#24230;&#37327;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#35780;&#20272;&#27492;&#31867;&#40065;&#26834;&#24615;&#24230;&#37327;&#30340;&#26041;&#27861;&#65292;&#35201;&#20040;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#35201;&#20040;&#19981;&#22826;&#20934;&#30830;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#27861;&#65292;&#23427;&#22312;&#25928;&#26524;&#21644;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#20445;&#25345;&#24179;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#25915;&#20987;&#26159;&#24191;&#20041;&#20102;&#28145;&#24230;&#27450;&#39575;&#65288;DeepFool&#65289;&#25915;&#20987;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#26131;&#20110;&#29702;&#35299;&#21644;&#23454;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25915;&#20987;&#22312;&#25928;&#26524;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#25915;&#20987;&#20063;&#36866;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have been known to be vulnerable to adversarial examples, which are inputs that are modified slightly to fool the network into making incorrect predictions. This has led to a significant amount of research on evaluating the robustness of these networks against such perturbations. One particularly important robustness metric is the robustness to minimal l2 adversarial perturbations. However, existing methods for evaluating this robustness metric are either computationally expensive or not very accurate. In this paper, we introduce a new family of adversarial attacks that strike a balance between effectiveness and computational efficiency. Our proposed attacks are generalizations of the well-known DeepFool (DF) attack, while they remain simple to understand and implement. We demonstrate that our attacks outperform existing methods in terms of both effectiveness and computational efficiency. Our proposed attacks are also suitable for evaluating the robustness of large
&lt;/p&gt;</description></item></channel></rss>