<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#31934;&#30830;&#21644;&#33258;&#36866;&#24212;&#30340;FedADMM&#31639;&#27861;&#65292;&#36890;&#36807;&#20026;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#26356;&#26032;&#35774;&#35745;&#19968;&#20010;&#19981;&#31934;&#30830;&#24615;&#26631;&#20934;&#65292;&#28040;&#38500;&#20102;&#35843;&#25972;&#26412;&#22320;&#35757;&#32451;&#20934;&#30830;&#24230;&#30340;&#38656;&#35201;&#65292;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#24182;&#20943;&#36731;&#20102;&#28382;&#21518;&#25928;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.13989</link><description>&lt;p&gt;
FedADMM-InSa: &#19968;&#31181;&#19981;&#31934;&#30830;&#21644;&#33258;&#36866;&#24212;&#30340;&#32852;&#37030;&#23398;&#20064;ADMM
&lt;/p&gt;
&lt;p&gt;
FedADMM-InSa: An Inexact and Self-Adaptive ADMM for Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13989
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#31934;&#30830;&#21644;&#33258;&#36866;&#24212;&#30340;FedADMM&#31639;&#27861;&#65292;&#36890;&#36807;&#20026;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#26356;&#26032;&#35774;&#35745;&#19968;&#20010;&#19981;&#31934;&#30830;&#24615;&#26631;&#20934;&#65292;&#28040;&#38500;&#20102;&#35843;&#25972;&#26412;&#22320;&#35757;&#32451;&#20934;&#30830;&#24230;&#30340;&#38656;&#35201;&#65292;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#24182;&#20943;&#36731;&#20102;&#28382;&#21518;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(FL)&#26159;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#20998;&#24067;&#24335;&#25968;&#25454;&#20013;&#23398;&#20064;&#21516;&#26102;&#20445;&#25345;&#38544;&#31169;&#12290;&#26377;&#25928;&#30340;FL&#31639;&#27861;&#30340;&#21457;&#23637;&#38754;&#20020;&#21508;&#31181;&#25361;&#25112;&#65292;&#21253;&#25324;&#24322;&#26500;&#25968;&#25454;&#21644;&#31995;&#32479;&#12289;&#36890;&#20449;&#33021;&#21147;&#26377;&#38480;&#20197;&#21450;&#21463;&#38480;&#30340;&#26412;&#22320;&#35745;&#31639;&#36164;&#28304;&#12290;&#26368;&#36817;&#24320;&#21457;&#30340;FedADMM&#26041;&#27861;&#23545;&#25968;&#25454;&#21644;&#31995;&#32479;&#30340;&#24322;&#26500;&#24615;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#38887;&#24615;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#36229;&#21442;&#25968;&#27809;&#26377;&#32463;&#36807;&#31934;&#24515;&#35843;&#25972;&#65292;&#23427;&#20204;&#20173;&#28982;&#20250;&#36973;&#21463;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#31934;&#30830;&#21644;&#33258;&#36866;&#24212;&#30340;FedADMM&#31639;&#27861;&#65292;&#21517;&#20026;FedADMM-InSa&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20026;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#26356;&#26032;&#35774;&#35745;&#20102;&#19968;&#20010;&#19981;&#31934;&#30830;&#24615;&#26631;&#20934;&#65292;&#20197;&#28040;&#38500;&#24517;&#39035;&#26681;&#25454;&#32463;&#39564;&#35774;&#32622;&#26412;&#22320;&#35757;&#32451;&#20934;&#30830;&#24615;&#30340;&#38656;&#27714;&#12290;&#36825;&#31181;&#19981;&#31934;&#30830;&#24615;&#26631;&#20934;&#21487;&#20197;&#30001;&#27599;&#20010;&#23458;&#25143;&#31471;&#29420;&#31435;&#22320;&#26681;&#25454;&#20854;&#29420;&#29305;&#26465;&#20214;&#36827;&#34892;&#35780;&#20272;&#65292;&#20174;&#32780;&#38477;&#20302;&#26412;&#22320;&#35745;&#31639;&#25104;&#26412;&#24182;&#20943;&#36731;&#19981;&#33391;&#30340;&#28382;&#21518;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13989v1 Announce Type: new  Abstract: Federated learning (FL) is a promising framework for learning from distributed data while maintaining privacy. The development of efficient FL algorithms encounters various challenges, including heterogeneous data and systems, limited communication capacities, and constrained local computational resources. Recently developed FedADMM methods show great resilience to both data and system heterogeneity. However, they still suffer from performance deterioration if the hyperparameters are not carefully tuned. To address this issue, we propose an inexact and self-adaptive FedADMM algorithm, termed FedADMM-InSa. First, we design an inexactness criterion for the clients' local updates to eliminate the need for empirically setting the local training accuracy. This inexactness criterion can be assessed by each client independently based on its unique condition, thereby reducing the local computational cost and mitigating the undesirable straggle e
&lt;/p&gt;</description></item></channel></rss>