<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#38543;&#26426;&#26862;&#26519;&#35757;&#32451;&#20013;&#27809;&#26377;&#37319;&#29992;&#33258;&#20030;&#32858;&#21512;&#20294;&#20855;&#26377;&#29305;&#24449;&#38543;&#26426;&#21270;&#30340;&#27169;&#22411;&#23481;&#26131;&#34987;&#23436;&#20840;&#37325;&#24314;&#65292;&#21363;&#20351;&#37319;&#29992;&#33258;&#20030;&#32858;&#21512;&#65292;&#22823;&#37096;&#20998;&#25968;&#25454;&#20063;&#21487;&#20197;&#34987;&#37325;&#24314;&#12290;</title><link>https://arxiv.org/abs/2402.19232</link><description>&lt;p&gt;
&#35757;&#32451;&#30340;&#38543;&#26426;&#26862;&#26519;&#23436;&#20840;&#25581;&#31034;&#24744;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Trained Random Forests Completely Reveal your Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19232
&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26862;&#26519;&#35757;&#32451;&#20013;&#27809;&#26377;&#37319;&#29992;&#33258;&#20030;&#32858;&#21512;&#20294;&#20855;&#26377;&#29305;&#24449;&#38543;&#26426;&#21270;&#30340;&#27169;&#22411;&#23481;&#26131;&#34987;&#23436;&#20840;&#37325;&#24314;&#65292;&#21363;&#20351;&#37319;&#29992;&#33258;&#20030;&#32858;&#21512;&#65292;&#22823;&#37096;&#20998;&#25968;&#25454;&#20063;&#21487;&#20197;&#34987;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#37325;&#24314;&#25915;&#20987;&#65292;&#33021;&#22815;&#23436;&#20840;&#25110;&#20960;&#20046;&#23436;&#20840;&#37325;&#24314;&#29992;&#20110;&#35757;&#32451;&#38543;&#26426;&#26862;&#26519;&#30340;&#25968;&#25454;&#38598;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#20381;&#36182;&#20110;&#24120;&#29992;&#24211;&#65288;&#22914;scikit-learn&#65289;&#20013;&#38543;&#22788;&#21487;&#24471;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#23558;&#37325;&#24314;&#38382;&#39064;&#26500;&#24314;&#20026;&#19968;&#20010;&#32452;&#21512;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#26368;&#22823;&#20284;&#28982;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20010;&#38382;&#39064;&#26159;NP&#38590;&#38382;&#39064;&#65292;&#20294;&#21487;&#20197;&#21033;&#29992;&#32422;&#26463;&#32534;&#31243;&#22312;&#35268;&#27169;&#19978;&#35299;&#20915; &#8212;&#8212; &#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#32422;&#26463;&#20256;&#25773;&#21644;&#35299;&#22495;&#32553;&#20943;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#35745;&#31639;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#27809;&#26377;&#37319;&#29992;&#33258;&#20030;&#32858;&#21512;&#20294;&#20855;&#26377;&#29305;&#24449;&#38543;&#26426;&#21270;&#30340;&#38543;&#26426;&#26862;&#26519;&#23481;&#26131;&#34987;&#23436;&#20840;&#37325;&#24314;&#12290;&#21363;&#20351;&#20351;&#29992;&#23569;&#37327;&#26641;&#65292;&#36825;&#20173;&#28982;&#25104;&#31435;&#12290;&#21363;&#20351;&#36890;&#36807;&#33258;&#20030;&#32858;&#21512;&#65292;&#22823;&#37096;&#20998;&#25968;&#25454;&#20063;&#21487;&#20197;&#34987;&#37325;&#24314;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#19968;&#31181;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19232v1 Announce Type: new  Abstract: We introduce an optimization-based reconstruction attack capable of completely or near-completely reconstructing a dataset utilized for training a random forest. Notably, our approach relies solely on information readily available in commonly used libraries such as scikit-learn. To achieve this, we formulate the reconstruction problem as a combinatorial problem under a maximum likelihood objective. We demonstrate that this problem is NP-hard, though solvable at scale using constraint programming -- an approach rooted in constraint propagation and solution-domain reduction. Through an extensive computational investigation, we demonstrate that random forests trained without bootstrap aggregation but with feature randomization are susceptible to a complete reconstruction. This holds true even with a small number of trees. Even with bootstrap aggregation, the majority of the data can also be reconstructed. These findings underscore a critica
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20449;&#24687;&#35770;&#26694;&#26550;&#20998;&#26512;&#20102;&#26368;&#20808;&#36827;&#30340;&#20284;&#28982;&#27604;&#25915;&#20987;&#23545;&#19981;&#30830;&#23450;&#24615;&#12289;&#26657;&#20934;&#27700;&#24179;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#24433;&#21709;&#65292;&#30740;&#31350;&#20102;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#20013;&#38544;&#21547;&#30340;&#39118;&#38505;</title><link>https://arxiv.org/abs/2402.10686</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#12289;&#26657;&#20934;&#21644;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65306;&#20449;&#24687;&#35770;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Uncertainty, Calibration, and Membership Inference Attacks: An Information-Theoretic Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10686
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#35770;&#26694;&#26550;&#20998;&#26512;&#20102;&#26368;&#20808;&#36827;&#30340;&#20284;&#28982;&#27604;&#25915;&#20987;&#23545;&#19981;&#30830;&#23450;&#24615;&#12289;&#26657;&#20934;&#27700;&#24179;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#24433;&#21709;&#65292;&#30740;&#31350;&#20102;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#20013;&#38544;&#21547;&#30340;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIA&#65289;&#20013;&#65292;&#25915;&#20987;&#32773;&#21033;&#29992;&#20856;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34920;&#29616;&#20986;&#30340;&#36807;&#24230;&#33258;&#20449;&#26469;&#30830;&#23450;&#29305;&#23450;&#25968;&#25454;&#28857;&#26159;&#21542;&#34987;&#29992;&#20110;&#35757;&#32451;&#30446;&#26631;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#20449;&#24687;&#29702;&#35770;&#26694;&#26550;&#20869;&#20998;&#26512;&#20102;&#26368;&#20808;&#36827;&#30340;&#20284;&#28982;&#27604;&#25915;&#20987;&#65288;LiRA&#65289;&#30340;&#24615;&#33021;&#65292;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#20801;&#35768;&#30740;&#31350;&#30495;&#23454;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#30001;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#38598;&#24341;&#36215;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#30446;&#26631;&#27169;&#22411;&#30340;&#26657;&#20934;&#27700;&#24179;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#25915;&#20987;&#32773;&#20174;&#30446;&#26631;&#27169;&#22411;&#25509;&#25910;&#21040;&#30340;&#20449;&#24687;&#36880;&#28176;&#20943;&#23569;&#65306;&#32622;&#20449;&#21521;&#37327;&#65288;CV&#65289;&#25259;&#38706;&#65292;&#20854;&#20013;&#36755;&#20986;&#27010;&#29575;&#21521;&#37327;&#34987;&#21457;&#24067;&#65307;&#30495;&#23454;&#26631;&#31614;&#32622;&#20449;&#24230;&#65288;TLC&#65289;&#25259;&#38706;&#65292;&#20854;&#20013;&#21482;&#26377;&#27169;&#22411;&#20998;&#37197;&#32473;&#30495;&#23454;&#26631;&#31614;&#30340;&#27010;&#29575;&#26159;&#21487;&#29992;&#30340;&#65307;&#20197;&#21450;&#20915;&#31574;&#38598;&#65288;DS&#65289;&#25259;&#38706;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10686v1 Announce Type: cross  Abstract: In a membership inference attack (MIA), an attacker exploits the overconfidence exhibited by typical machine learning models to determine whether a specific data point was used to train a target model. In this paper, we analyze the performance of the state-of-the-art likelihood ratio attack (LiRA) within an information-theoretical framework that allows the investigation of the impact of the aleatoric uncertainty in the true data generation process, of the epistemic uncertainty caused by a limited training data set, and of the calibration level of the target model. We compare three different settings, in which the attacker receives decreasingly informative feedback from the target model: confidence vector (CV) disclosure, in which the output probability vector is released; true label confidence (TLC) disclosure, in which only the probability assigned to the true label is made available by the model; and decision set (DS) disclosure, in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24046;&#20998;&#38544;&#31169;&#32858;&#31867;&#31639;&#27861;DPM&#65292;&#36890;&#36807;&#25628;&#32034;&#20934;&#30830;&#30340;&#25968;&#25454;&#28857;&#20998;&#31163;&#22120;&#26469;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;&#30340;&#32858;&#31867;&#12290;&#20851;&#38190;&#36129;&#29486;&#26159;&#35782;&#21035;&#22823;&#38388;&#38548;&#20998;&#31163;&#22120;&#24182;&#21512;&#29702;&#20998;&#37197;&#38544;&#31169;&#39044;&#31639;&#12290;</title><link>http://arxiv.org/abs/2307.02969</link><description>&lt;p&gt;
DPM: &#36890;&#36807;&#20998;&#31163;&#32858;&#31867;&#25935;&#24863;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
DPM: Clustering Sensitive Data through Separation. (arXiv:2307.02969v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24046;&#20998;&#38544;&#31169;&#32858;&#31867;&#31639;&#27861;DPM&#65292;&#36890;&#36807;&#25628;&#32034;&#20934;&#30830;&#30340;&#25968;&#25454;&#28857;&#20998;&#31163;&#22120;&#26469;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;&#30340;&#32858;&#31867;&#12290;&#20851;&#38190;&#36129;&#29486;&#26159;&#35782;&#21035;&#22823;&#38388;&#38548;&#20998;&#31163;&#22120;&#24182;&#21512;&#29702;&#20998;&#37197;&#38544;&#31169;&#39044;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#32858;&#31867;&#20197;&#26080;&#30417;&#30563;&#26041;&#24335;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#20998;&#32452;&#65292;&#21516;&#26102;&#30830;&#20445;&#25935;&#24863;&#20449;&#24687;&#24471;&#20197;&#20445;&#25252;&#12290;&#20808;&#21069;&#30340;&#38544;&#31169;&#20445;&#25252;&#32858;&#31867;&#20851;&#27880;&#28857;&#22312;&#20110;&#35782;&#21035;&#28857;&#20113;&#30340;&#32858;&#38598;&#12290;&#26412;&#25991;&#21017;&#37319;&#21462;&#21478;&#19968;&#31181;&#26041;&#27861;&#65292;&#20851;&#27880;&#20110;&#35782;&#21035;&#36866;&#24403;&#30340;&#20998;&#31163;&#22120;&#20197;&#20998;&#31163;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#24046;&#20998;&#38544;&#31169;&#32858;&#31867;&#31639;&#27861;DPM&#65292;&#20197;&#24046;&#20998;&#38544;&#31169;&#30340;&#26041;&#24335;&#25628;&#32034;&#20934;&#30830;&#30340;&#25968;&#25454;&#28857;&#20998;&#31163;&#22120;&#12290;DPM&#35299;&#20915;&#20102;&#23547;&#25214;&#20934;&#30830;&#20998;&#31163;&#22120;&#30340;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#35782;&#21035;&#32858;&#31867;&#38388;&#30340;&#22823;&#38388;&#38548;&#20998;&#31163;&#22120;&#32780;&#19981;&#26159;&#32858;&#31867;&#20869;&#30340;&#23567;&#38388;&#38548;&#20998;&#31163;&#22120;&#65292;&#20197;&#21450;&#22312;&#24320;&#38144;&#38544;&#31169;&#39044;&#31639;&#26102;&#65292;&#20248;&#20808;&#32771;&#34385;&#23558;&#25968;&#25454;&#21010;&#20998;&#20026;&#36739;&#22823;&#23376;&#37096;&#20998;&#30340;&#20998;&#31163;&#22120;&#12290;&#21033;&#29992;&#24046;&#20998;&#38544;&#31169;&#25351;&#25968;&#26426;&#21046;&#65292;DPM&#36890;&#36807;&#38543;&#26426;&#36873;&#25321;&#20855;&#26377;&#39640;&#25928;&#29992;&#24615;&#30340;&#32858;&#31867;&#20998;&#31163;&#22120;&#65306;&#23545;&#20110;&#25968;&#25454;&#38598;D&#65292;&#22914;&#26524;&#20013;&#24515;&#30340;60%&#20998;&#20301;&#25968;&#20013;&#23384;&#22312;&#23485;&#30340;&#20302;&#23494;&#24230;&#20998;&#31163;&#22120;&#65292;DPM&#20250;&#21457;&#29616;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy-preserving clustering groups data points in an unsupervised manner whilst ensuring that sensitive information remains protected. Previous privacy-preserving clustering focused on identifying concentration of point clouds. In this paper, we take another path and focus on identifying appropriate separators that split a data set. We introduce the novel differentially private clustering algorithm DPM that searches for accurate data point separators in a differentially private manner. DPM addresses two key challenges for finding accurate separators: identifying separators that are large gaps between clusters instead of small gaps within a cluster and, to efficiently spend the privacy budget, prioritising separators that split the data into large subparts. Using the differentially private Exponential Mechanism, DPM randomly chooses cluster separators with provably high utility: For a data set $D$, if there is a wide low-density separator in the central $60\%$ quantile, DPM finds that
&lt;/p&gt;</description></item></channel></rss>