<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#30456;&#20851;&#22122;&#22768;&#25552;&#39640;&#25928;&#29992;&#24182;&#30830;&#20445;&#38544;&#31169;&#30340;&#24046;&#20998;&#38544;&#31169;&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;DP&#22122;&#22768;&#21644;&#26412;&#22320;&#26356;&#26032;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#24314;&#31435;&#20102;&#21160;&#24577;&#36951;&#25022;&#30028;&#12290;</title><link>https://arxiv.org/abs/2403.16542</link><description>&lt;p&gt;
&#20855;&#26377;&#30456;&#20851;&#22122;&#22768;&#30340;&#24046;&#20998;&#38544;&#31169;&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Online Federated Learning with Correlated Noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16542
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#30456;&#20851;&#22122;&#22768;&#25552;&#39640;&#25928;&#29992;&#24182;&#30830;&#20445;&#38544;&#31169;&#30340;&#24046;&#20998;&#38544;&#31169;&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;DP&#22122;&#22768;&#21644;&#26412;&#22320;&#26356;&#26032;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#24314;&#31435;&#20102;&#21160;&#24577;&#36951;&#25022;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;&#65292;&#21033;&#29992;&#26102;&#38388;&#30456;&#20851;&#30340;&#22122;&#22768;&#26469;&#25552;&#39640;&#25928;&#29992;&#21516;&#26102;&#30830;&#20445;&#36830;&#32493;&#21457;&#24067;&#30340;&#27169;&#22411;&#30340;&#38544;&#31169;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#28304;&#33258;DP&#22122;&#22768;&#21644;&#26412;&#22320;&#26356;&#26032;&#24102;&#26469;&#30340;&#27969;&#24335;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#25200;&#21160;&#36845;&#20195;&#20998;&#26512;&#26469;&#25511;&#21046;DP&#22122;&#22768;&#23545;&#25928;&#29992;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20934;&#24378;&#20984;&#26465;&#20214;&#19979;&#22914;&#20309;&#26377;&#25928;&#31649;&#29702;&#26469;&#33258;&#26412;&#22320;&#26356;&#26032;&#30340;&#28418;&#31227;&#35823;&#24046;&#12290;&#22312;$(\epsilon, \delta)$-DP&#39044;&#31639;&#33539;&#22260;&#20869;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#25972;&#20010;&#26102;&#38388;&#27573;&#19978;&#30340;&#21160;&#24577;&#36951;&#25022;&#30028;&#65292;&#37327;&#21270;&#20102;&#20851;&#38190;&#21442;&#25968;&#30340;&#24433;&#21709;&#20197;&#21450;&#21160;&#24577;&#29615;&#22659;&#21464;&#21270;&#30340;&#24378;&#24230;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#23454;&#20102;&#25152;&#25552;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16542v1 Announce Type: new  Abstract: We propose a novel differentially private algorithm for online federated learning that employs temporally correlated noise to improve the utility while ensuring the privacy of the continuously released models. To address challenges stemming from DP noise and local updates with streaming noniid data, we develop a perturbed iterate analysis to control the impact of the DP noise on the utility. Moreover, we demonstrate how the drift errors from local updates can be effectively managed under a quasi-strong convexity condition. Subject to an $(\epsilon, \delta)$-DP budget, we establish a dynamic regret bound over the entire time horizon that quantifies the impact of key parameters and the intensity of changes in dynamic environments. Numerical experiments validate the efficacy of the proposed algorithm.
&lt;/p&gt;</description></item><item><title>&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28040;&#38500;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#35299;&#20915;&#29615;&#22659;&#25152;&#26377;&#32773;&#26377;&#26435;&#25764;&#38144;&#26234;&#33021;&#20307;&#35757;&#32451;&#25968;&#25454;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#35813;&#39046;&#22495;&#38754;&#20020;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2312.15910</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28040;&#38500;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Unlearning. (arXiv:2312.15910v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15910
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28040;&#38500;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#35299;&#20915;&#29615;&#22659;&#25152;&#26377;&#32773;&#26377;&#26435;&#25764;&#38144;&#26234;&#33021;&#20307;&#35757;&#32451;&#25968;&#25454;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#35813;&#39046;&#22495;&#38754;&#20020;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#28040;&#38500;&#23398;&#20064;&#25351;&#30340;&#26159;&#26681;&#25454;&#25968;&#25454;&#25152;&#26377;&#32773;&#30340;&#35831;&#27714;&#65292;&#38477;&#20302;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24433;&#21709;&#30340;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#22312;&#28040;&#38500;&#23398;&#20064;&#30340;&#30740;&#31350;&#20013;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#39046;&#22495;&#24448;&#24448;&#34987;&#24573;&#35270;&#65292;&#37027;&#23601;&#26159;&#24378;&#21270;&#23398;&#20064;&#12290;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#35757;&#32451;&#19968;&#20010;&#26234;&#33021;&#20307;&#22312;&#29615;&#22659;&#20013;&#20570;&#20986;&#26368;&#20248;&#20915;&#31574;&#20197;&#26368;&#22823;&#21270;&#32047;&#31215;&#22870;&#21169;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#26234;&#33021;&#20307;&#24448;&#24448;&#20250;&#35760;&#24518;&#29615;&#22659;&#30340;&#29305;&#24449;&#65292;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#37325;&#22823;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#26681;&#25454;&#25968;&#25454;&#20445;&#25252;&#27861;&#35268;&#65292;&#29615;&#22659;&#30340;&#25152;&#26377;&#32773;&#26377;&#26435;&#25764;&#38144;&#26234;&#33021;&#20307;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#35775;&#38382;&#26435;&#38480;&#65292;&#22240;&#27492;&#38656;&#35201;&#24320;&#23637;&#19968;&#20010;&#26032;&#39062;&#19988;&#32039;&#36843;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#21363;&#8220;&#24378;&#21270;&#28040;&#38500;&#23398;&#20064;&#8221;&#12290;&#24378;&#21270;&#28040;&#38500;&#23398;&#20064;&#20391;&#37325;&#20110;&#25764;&#38144;&#25972;&#20010;&#29615;&#22659;&#32780;&#19981;&#26159;&#21333;&#20010;&#25968;&#25454;&#26679;&#26412;&#12290;&#36825;&#19968;&#29420;&#29305;&#29305;&#24449;&#24102;&#26469;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;&#25361;&#25112;&#65306;1&#65289;&#22914;&#20309;&#25552;&#20986;&#28040;&#38500;&#23398;&#20064;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning refers to the process of mitigating the influence of specific training data on machine learning models based on removal requests from data owners. However, one important area that has been largely overlooked in the research of unlearning is reinforcement learning. Reinforcement learning focuses on training an agent to make optimal decisions within an environment to maximize its cumulative rewards. During the training, the agent tends to memorize the features of the environment, which raises a significant concern about privacy. As per data protection regulations, the owner of the environment holds the right to revoke access to the agent's training data, thus necessitating the development of a novel and pressing research field, known as \emph{reinforcement unlearning}. Reinforcement unlearning focuses on revoking entire environments rather than individual data samples. This unique characteristic presents three distinct challenges: 1) how to propose unlearning schemes f
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21518;&#38376;&#25915;&#20987;&#19968;&#30452;&#26159;&#19968;&#20010;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#25913;&#36827;&#26041;&#27861;&#38656;&#35201;&#24378;&#22823;&#30340;&#25915;&#20987;&#32773;&#33021;&#21147;&#65292;&#22312;&#33021;&#21147;&#21463;&#38480;&#22330;&#26223;&#19979;&#36824;&#27809;&#26377;&#25214;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#35299;&#20915;&#21150;&#27861;&#65292;&#27492;&#22806;&#65292;&#27169;&#22411;&#40065;&#26834;&#24615;&#20173;&#28982;&#20540;&#24471;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2304.10985</link><description>&lt;p&gt;
&#22312;&#33021;&#21147;&#21463;&#38480;&#22330;&#26223;&#19979;&#21551;&#21160;&#24378;&#38887;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Launching a Robust Backdoor Attack under Capability Constrained Scenarios. (arXiv:2304.10985v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10985
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21518;&#38376;&#25915;&#20987;&#19968;&#30452;&#26159;&#19968;&#20010;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#25913;&#36827;&#26041;&#27861;&#38656;&#35201;&#24378;&#22823;&#30340;&#25915;&#20987;&#32773;&#33021;&#21147;&#65292;&#22312;&#33021;&#21147;&#21463;&#38480;&#22330;&#26223;&#19979;&#36824;&#27809;&#26377;&#25214;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#35299;&#20915;&#21150;&#27861;&#65292;&#27492;&#22806;&#65292;&#27169;&#22411;&#40065;&#26834;&#24615;&#20173;&#28982;&#20540;&#24471;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20851;&#38190;&#39046;&#22495;&#30340;&#24212;&#29992;&#19981;&#26029;&#22686;&#21152;&#65292;&#20154;&#20204;&#24320;&#22987;&#25285;&#24515;&#23427;&#20204;&#30340;&#23433;&#20840;&#24615;&#12290;&#30001;&#20110;&#32570;&#20047;&#36879;&#26126;&#24230;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#23041;&#32961;&#12290;&#27745;&#26579;&#30340;&#21518;&#38376;&#27169;&#22411;&#22312;&#26222;&#36890;&#29615;&#22659;&#19979;&#21487;&#33021;&#34920;&#29616;&#27491;&#24120;&#65292;&#20294;&#24403;&#36755;&#20837;&#21253;&#21547;&#35302;&#21457;&#22120;&#26102;&#65292;&#20250;&#26174;&#31034;&#20986;&#24694;&#24847;&#34892;&#20026;&#12290;&#30446;&#21069;&#23545;&#21518;&#38376;&#25915;&#20987;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#25913;&#21892;&#35302;&#21457;&#22120;&#30340;&#31192;&#23494;&#24615;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#38656;&#35201;&#24378;&#22823;&#30340;&#25915;&#20987;&#32773;&#33021;&#21147;&#65292;&#20363;&#22914;&#23545;&#27169;&#22411;&#32467;&#26500;&#30340;&#20102;&#35299;&#25110;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#25511;&#21046;&#12290;&#30001;&#20110;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#25915;&#20987;&#32773;&#30340;&#33021;&#21147;&#21463;&#21040;&#38480;&#21046;&#65292;&#36825;&#20123;&#25915;&#20987;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#38382;&#39064;&#36824;&#26410;&#24471;&#21040;&#20805;&#20998;&#20851;&#27880;&#12290;&#20363;&#22914;&#65292;&#27169;&#22411;&#33976;&#39311;&#24120;&#29992;&#20110;&#31616;&#21270;&#27169;&#22411;&#22823;&#23567;&#65292;&#20294;&#38543;&#30528;&#21442;&#25968;&#25968;&#37327;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#20197;&#21069;&#30340;&#35768;&#22810;&#21518;&#38376;&#25915;&#20987;&#22312;&#27169;&#22411;&#33976;&#39311;&#21518;&#22343;&#22833;&#36133;;&#22270;&#20687;&#22686;&#24378;&#25805;&#20316;&#21487;&#20197;&#30772;&#22351;&#35302;&#21457;&#22120;&#65292;&#20174;&#32780;&#20351;&#21518;&#38376;&#25915;&#20987;&#22833;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep neural networks continue to be used in critical domains, concerns over their security have emerged. Deep learning models are vulnerable to backdoor attacks due to the lack of transparency. A poisoned backdoor model may perform normally in routine environments, but exhibit malicious behavior when the input contains a trigger. Current research on backdoor attacks focuses on improving the stealthiness of triggers, and most approaches require strong attacker capabilities, such as knowledge of the model structure or control over the training process. These attacks are impractical since in most cases the attacker's capabilities are limited. Additionally, the issue of model robustness has not received adequate attention. For instance, model distillation is commonly used to streamline model size as the number of parameters grows exponentially, and most of previous backdoor attacks failed after model distillation; the image augmentation operations can destroy the trigger and thus disabl
&lt;/p&gt;</description></item></channel></rss>