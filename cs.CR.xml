<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24314;&#27169;&#20113;&#31995;&#32479;&#20013;&#30340;&#35775;&#38382;&#25511;&#21046;&#31574;&#30053;&#65292;&#24182;&#24320;&#21457;&#20102;&#22522;&#20110;PDDL&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#26469;&#26816;&#27979;&#21487;&#33021;&#23548;&#33268;&#35832;&#22914;&#21202;&#32034;&#36719;&#20214;&#21644;&#25935;&#24863;&#25968;&#25454;&#22806;&#27844;&#31561;&#24191;&#27867;&#25915;&#20987;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;</title><link>https://arxiv.org/abs/2402.10985</link><description>&lt;p&gt;
&#21033;&#29992;AI&#35268;&#21010;&#25216;&#26415;&#26816;&#27979;&#20113;&#23433;&#20840;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Leveraging AI Planning For Detecting Cloud Security Vulnerabilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10985
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24314;&#27169;&#20113;&#31995;&#32479;&#20013;&#30340;&#35775;&#38382;&#25511;&#21046;&#31574;&#30053;&#65292;&#24182;&#24320;&#21457;&#20102;&#22522;&#20110;PDDL&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#26469;&#26816;&#27979;&#21487;&#33021;&#23548;&#33268;&#35832;&#22914;&#21202;&#32034;&#36719;&#20214;&#21644;&#25935;&#24863;&#25968;&#25454;&#22806;&#27844;&#31561;&#24191;&#27867;&#25915;&#20987;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20113;&#35745;&#31639;&#26381;&#21153;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#19988;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#25968;&#25454;&#23384;&#20648;&#12289;&#22788;&#29702;&#21644;&#21327;&#20316;&#35299;&#20915;&#26041;&#26696;&#12290;&#38543;&#30528;&#23427;&#20204;&#30340;&#26222;&#21450;&#65292;&#19982;&#20854;&#23433;&#20840;&#28431;&#27934;&#30456;&#20851;&#30340;&#25285;&#24551;&#20063;&#22312;&#22686;&#38271;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#25968;&#25454;&#27844;&#38706;&#21644;&#21202;&#32034;&#36719;&#20214;&#31561;&#22797;&#26434;&#25915;&#20987;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#34920;&#36798;&#20113;&#31995;&#32479;&#20013;&#19981;&#21516;&#23545;&#35937;&#65288;&#22914;&#29992;&#25143;&#12289;&#25968;&#25454;&#23384;&#20648;&#12289;&#23433;&#20840;&#35282;&#33394;&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#24314;&#27169;&#20113;&#31995;&#32479;&#20013;&#30340;&#35775;&#38382;&#25511;&#21046;&#31574;&#30053;&#12290;&#35775;&#38382;&#25511;&#21046;&#35823;&#37197;&#32622;&#36890;&#24120;&#26159;&#20113;&#25915;&#20987;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;PDDL&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#23433;&#20840;&#28431;&#27934;&#65292;&#20363;&#22914;&#21487;&#33021;&#23548;&#33268;&#24191;&#27867;&#25915;&#20987;&#65288;&#22914;&#21202;&#32034;&#36719;&#20214;&#65289;&#21644;&#25935;&#24863;&#25968;&#25454;&#22806;&#27844;&#31561;&#12290;&#35268;&#21010;&#22120;&#21487;&#20197;&#29983;&#25104;&#25915;&#20987;&#20197;&#35782;&#21035;&#20113;&#20013;&#30340;&#27492;&#31867;&#28431;&#27934;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;14&#20010;&#19981;&#21516;&#21830;&#19994;&#32452;&#32455;&#30340;&#30495;&#23454;&#20122;&#39532;&#36874;AWS&#20113;&#37197;&#32622;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10985v1 Announce Type: cross  Abstract: Cloud computing services provide scalable and cost-effective solutions for data storage, processing, and collaboration. Alongside their growing popularity, concerns related to their security vulnerabilities leading to data breaches and sophisticated attacks such as ransomware are growing. To address these, first, we propose a generic framework to express relations between different cloud objects such as users, datastores, security roles, to model access control policies in cloud systems. Access control misconfigurations are often the primary driver for cloud attacks. Second, we develop a PDDL model for detecting security vulnerabilities which can for example lead to widespread attacks such as ransomware, sensitive data exfiltration among others. A planner can then generate attacks to identify such vulnerabilities in the cloud. Finally, we test our approach on 14 real Amazon AWS cloud configurations of different commercial organizations
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SafeDecoding&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23433;&#20840;&#24863;&#30693;&#35299;&#30721;&#31574;&#30053;&#26469;&#38450;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#25915;&#20987;&#12290;&#35813;&#31574;&#30053;&#21487;&#20197;&#29983;&#25104;&#23545;&#29992;&#25143;&#26597;&#35810;&#26377;&#30410;&#19988;&#26080;&#23475;&#30340;&#21709;&#24212;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;LLMs&#23433;&#20840;&#24615;&#23041;&#32961;&#12290;</title><link>https://arxiv.org/abs/2402.08983</link><description>&lt;p&gt;
SafeDecoding: &#36890;&#36807;&#23433;&#20840;&#24863;&#30693;&#35299;&#30721;&#38450;&#24481;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SafeDecoding&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23433;&#20840;&#24863;&#30693;&#35299;&#30721;&#31574;&#30053;&#26469;&#38450;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#25915;&#20987;&#12290;&#35813;&#31574;&#30053;&#21487;&#20197;&#29983;&#25104;&#23545;&#29992;&#25143;&#26597;&#35810;&#26377;&#30410;&#19988;&#26080;&#23475;&#30340;&#21709;&#24212;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;LLMs&#23433;&#20840;&#24615;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#36741;&#21161;&#31561;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;&#20154;&#20204;&#20026;&#20102;&#20351;LLM&#30340;&#34892;&#20026;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#65292;&#21253;&#25324;&#23433;&#20840;&#24615;&#22312;&#20869;&#20570;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#12290;&#36234;&#29425;&#25915;&#20987;&#26088;&#22312;&#24341;&#21457;LLM&#30340;&#38750;&#39044;&#26399;&#21644;&#19981;&#23433;&#20840;&#34892;&#20026;&#65292;&#20173;&#28982;&#26159;LLM&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#23041;&#32961;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;SafeDecoding&#26469;&#38450;&#24481;LLM&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#36825;&#26159;&#19968;&#31181;&#23433;&#20840;&#24863;&#30693;&#30340;&#35299;&#30721;&#31574;&#30053;&#65292;&#29992;&#20110;&#29983;&#25104;&#23545;&#29992;&#25143;&#26597;&#35810;&#26377;&#30410;&#19988;&#26080;&#23475;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#22312;&#24320;&#21457;SafeDecoding&#26102;&#30340;&#27934;&#23519;&#21147;&#22522;&#20110;&#35266;&#23519;&#21040;&#65292;&#21363;&#20351;&#20195;&#34920;&#26377;&#23475;&#20869;&#23481;&#30340;&#26631;&#35760;&#30340;&#27010;&#29575;&#36229;&#36807;&#20195;&#34920;&#26080;&#23475;&#21709;&#24212;&#30340;&#26631;&#35760;&#30340;&#27010;&#29575;&#65292;&#23433;&#20840;&#20813;&#36131;&#22768;&#26126;&#20173;&#28982;&#20986;&#29616;&#22312;&#25353;&#27010;&#29575;&#38477;&#24207;&#25490;&#24207;&#30340;&#26631;&#35760;&#20013;&#30340;&#21069;&#20960;&#20010;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#35782;&#21035;&#23433;&#20840;&#20813;&#36131;&#22768;&#26126;&#24182;&#22686;&#24378;&#20854;&#33391;&#24615;&#24433;&#21709;&#21147;&#26469;&#20943;&#36731;&#36234;&#29425;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08983v1 Announce Type: cross Abstract: As large language models (LLMs) become increasingly integrated into real-world applications such as code generation and chatbot assistance, extensive efforts have been made to align LLM behavior with human values, including safety. Jailbreak attacks, aiming to provoke unintended and unsafe behaviors from LLMs, remain a significant/leading LLM safety threat. In this paper, we aim to defend LLMs against jailbreak attacks by introducing SafeDecoding, a safety-aware decoding strategy for LLMs to generate helpful and harmless responses to user queries. Our insight in developing SafeDecoding is based on the observation that, even though probabilities of tokens representing harmful contents outweigh those representing harmless responses, safety disclaimers still appear among the top tokens after sorting tokens by probability in descending order. This allows us to mitigate jailbreak attacks by identifying safety disclaimers and amplifying their
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#30340;&#21327;&#35843;&#32570;&#38519;&#25259;&#38706;&#65288;CFD&#65289;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#39046;&#22495;&#20013;&#32570;&#20047;&#32467;&#26500;&#21270;&#36807;&#31243;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07039</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#21327;&#35843;&#25259;&#38706;&#65306;&#36229;&#36234;&#23433;&#20840;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Coordinated Disclosure for AI: Beyond Security Vulnerabilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07039
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#30340;&#21327;&#35843;&#32570;&#38519;&#25259;&#38706;&#65288;CFD&#65289;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#39046;&#22495;&#20013;&#32570;&#20047;&#32467;&#26500;&#21270;&#36807;&#31243;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#20260;&#23475;&#25253;&#21578;&#22312;&#25259;&#38706;&#25110;&#35299;&#20915;&#31639;&#27861;&#32570;&#38519;&#26041;&#38754;&#20173;&#28982;&#26159;&#19968;&#31181;&#20020;&#26102;&#24615;&#30340;&#25805;&#20316;&#65292;&#32570;&#20047;&#32467;&#26500;&#21270;&#30340;&#36807;&#31243;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#21327;&#35843;&#28431;&#27934;&#25259;&#38706;&#65288;CVD&#65289;&#30340;&#20262;&#29702;&#21644;&#29983;&#24577;&#31995;&#32479;&#22312;&#36719;&#20214;&#23433;&#20840;&#21644;&#36879;&#26126;&#24230;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#32654;&#22269;&#30340;&#32972;&#26223;&#19979;&#65292;&#20026;&#20102;&#40723;&#21169;&#31177;&#25345;&#21892;&#24847;&#34892;&#20107;&#30340;&#23433;&#20840;&#30740;&#31350;&#20154;&#21592;&#65292;&#24314;&#31435;&#19968;&#20010;&#23433;&#20840;&#38450;&#25252;&#26465;&#27454;&#20197;&#23545;&#25239;&#35745;&#31639;&#26426;&#27450;&#35784;&#21644;&#28389;&#29992;&#27861;&#26696;&#19968;&#30452;&#23384;&#22312;&#38271;&#26399;&#30340;&#27861;&#24459;&#21644;&#25919;&#31574;&#26007;&#20105;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#20013;&#30340;&#31639;&#27861;&#32570;&#38519;&#19982;&#20256;&#32479;&#36719;&#20214;&#28431;&#27934;&#23384;&#22312;&#30528;&#19981;&#21516;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#19968;&#31181;&#19987;&#38376;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#29305;&#27530;&#22797;&#26434;&#24615;&#30340;&#19987;&#38376;&#21327;&#35843;&#32570;&#38519;&#25259;&#38706;&#65288;CFD&#65289;&#26694;&#26550;&#30340;&#23454;&#26045;&#12290;&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;ML&#20013;&#30340;&#25259;&#38706;&#21382;&#21490;&#32972;&#26223;&#65292;&#21253;&#25324;
&lt;/p&gt;
&lt;p&gt;
Harm reporting in the field of Artificial Intelligence (AI) currently operates on an ad hoc basis, lacking a structured process for disclosing or addressing algorithmic flaws. In contrast, the Coordinated Vulnerability Disclosure (CVD) ethos and ecosystem play a pivotal role in software security and transparency. Within the U.S. context, there has been a protracted legal and policy struggle to establish a safe harbor from the Computer Fraud and Abuse Act, aiming to foster institutional support for security researchers acting in good faith. Notably, algorithmic flaws in Machine Learning (ML) models present distinct challenges compared to traditional software vulnerabilities, warranting a specialized approach. To address this gap, we propose the implementation of a dedicated Coordinated Flaw Disclosure (CFD) framework tailored to the intricacies of machine learning and artificial intelligence issues. This paper delves into the historical landscape of disclosures in ML, encompassing the a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; SpongeNet &#30340;&#26032;&#22411;&#28023;&#32501;&#25915;&#20987;&#65292;&#36890;&#36807;&#30452;&#25509;&#20316;&#29992;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#65292;&#25104;&#21151;&#22686;&#21152;&#20102;&#35270;&#35273;&#27169;&#22411;&#30340;&#33021;&#32791;&#65292;&#32780;&#19988;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#37327;&#26356;&#23569;&#12290;</title><link>https://arxiv.org/abs/2402.06357</link><description>&lt;p&gt;
SpongeNet &#25915;&#20987;&#65306;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#28023;&#32501;&#26435;&#37325;&#20013;&#27602;
&lt;/p&gt;
&lt;p&gt;
The SpongeNet Attack: Sponge Weight Poisoning of Deep Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; SpongeNet &#30340;&#26032;&#22411;&#28023;&#32501;&#25915;&#20987;&#65292;&#36890;&#36807;&#30452;&#25509;&#20316;&#29992;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#65292;&#25104;&#21151;&#22686;&#21152;&#20102;&#35270;&#35273;&#27169;&#22411;&#30340;&#33021;&#32791;&#65292;&#32780;&#19988;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#37327;&#26356;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#32501;&#25915;&#20987;&#26088;&#22312;&#22686;&#21152;&#22312;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#37096;&#32626;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#32791;&#21644;&#35745;&#31639;&#26102;&#38388;&#12290;&#29616;&#26377;&#30340;&#28023;&#32501;&#25915;&#20987;&#21487;&#20197;&#36890;&#36807;&#28023;&#32501;&#31034;&#20363;&#36827;&#34892;&#25512;&#29702;&#65292;&#20063;&#21487;&#20197;&#36890;&#36807;&#28023;&#32501;&#20013;&#27602;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36827;&#34892;&#12290;&#28023;&#32501;&#31034;&#20363;&#21033;&#29992;&#28155;&#21152;&#21040;&#27169;&#22411;&#36755;&#20837;&#30340;&#25200;&#21160;&#26469;&#22686;&#21152;&#33021;&#37327;&#21644;&#24310;&#36831;&#65292;&#32780;&#28023;&#32501;&#20013;&#27602;&#21017;&#25913;&#21464;&#27169;&#22411;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#24341;&#21457;&#25512;&#29702;&#26102;&#30340;&#33021;&#37327;/&#24310;&#36831;&#25928;&#24212;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28023;&#32501;&#25915;&#20987;&#65292;&#31216;&#20026; SpongeNet&#12290;SpongeNet &#26159;&#31532;&#19968;&#20010;&#30452;&#25509;&#20316;&#29992;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#30340;&#28023;&#32501;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#28023;&#32501;&#20013;&#27602;&#65292;SpongeNet &#21487;&#20197;&#25104;&#21151;&#22686;&#21152;&#35270;&#35273;&#27169;&#22411;&#30340;&#33021;&#32791;&#65292;&#24182;&#19988;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#37327;&#26356;&#23569;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22914;&#26524;&#19981;&#19987;&#38376;&#38024;&#23545;&#28023;&#32501;&#20013;&#27602;&#36827;&#34892;&#35843;&#25972;&#65288;&#21363;&#20943;&#23567;&#25209;&#24402;&#19968;&#21270;&#20559;&#24046;&#20540;&#65289;&#65292;&#21017;&#27602;&#23475;&#38450;&#24481;&#20250;&#22833;&#25928;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26174;&#31034;&#20986;&#28023;&#32501;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sponge attacks aim to increase the energy consumption and computation time of neural networks deployed on hardware accelerators. Existing sponge attacks can be performed during inference via sponge examples or during training via Sponge Poisoning. Sponge examples leverage perturbations added to the model's input to increase energy and latency, while Sponge Poisoning alters the objective function of a model to induce inference-time energy/latency effects.   In this work, we propose a novel sponge attack called SpongeNet. SpongeNet is the first sponge attack that is performed directly on the parameters of a pre-trained model. Our experiments show that SpongeNet can successfully increase the energy consumption of vision models with fewer samples required than Sponge Poisoning. Our experiments indicate that poisoning defenses are ineffective if not adjusted specifically for the defense against Sponge Poisoning (i.e., they decrease batch normalization bias values). Our work shows that Spong
&lt;/p&gt;</description></item><item><title>&#12298;Janus&#25509;&#21475;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#22914;&#20309;&#25918;&#22823;&#38544;&#31169;&#39118;&#38505;&#12299;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#23545;&#20010;&#20154;&#20449;&#24687;&#27844;&#38706;&#30340;&#39118;&#38505;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;LLM&#21033;&#29992;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2310.15469</link><description>&lt;p&gt;
&#12298;Janus&#25509;&#21475;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#22914;&#20309;&#25918;&#22823;&#38544;&#31169;&#39118;&#38505;&#12299;
&lt;/p&gt;
&lt;p&gt;
The Janus Interface: How Fine-Tuning in Large Language Models Amplifies the Privacy Risks. (arXiv:2310.15469v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15469
&lt;/p&gt;
&lt;p&gt;
&#12298;Janus&#25509;&#21475;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#22914;&#20309;&#25918;&#22823;&#38544;&#31169;&#39118;&#38505;&#12299;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#23545;&#20010;&#20154;&#20449;&#24687;&#27844;&#38706;&#30340;&#39118;&#38505;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;LLM&#21033;&#29992;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2018&#24180;&#21518;&#30340;&#26102;&#20195;&#26631;&#24535;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#65292;OpenAI&#30340;ChatGPT&#31561;&#21019;&#26032;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#35821;&#35328;&#33021;&#21147;&#12290;&#38543;&#30528;&#34892;&#19994;&#22312;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#24182;&#21033;&#29992;&#22823;&#37327;&#30340;&#20154;&#31867;&#35821;&#35328;&#25968;&#25454;&#26041;&#38754;&#30340;&#21162;&#21147;&#65292;&#23433;&#20840;&#21644;&#38544;&#31169;&#25361;&#25112;&#20063;&#20986;&#29616;&#20102;&#12290;&#20854;&#20013;&#26368;&#37325;&#35201;&#30340;&#26159;&#22312;&#22522;&#20110;&#32593;&#32476;&#30340;&#25968;&#25454;&#33719;&#21462;&#36807;&#31243;&#20013;&#65292;&#21487;&#33021;&#20250;&#24847;&#22806;&#31215;&#32047;&#20010;&#20154;&#21487;&#35782;&#21035;&#20449;&#24687;&#65288;PII&#65289;&#65292;&#20174;&#32780;&#23548;&#33268;&#24847;&#22806;&#30340;PII&#27844;&#38706;&#39118;&#38505;&#12290;&#34429;&#28982;&#20687;RLHF&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#36825;&#26679;&#30340;&#31574;&#30053;&#24050;&#34987;&#29992;&#26469;&#25511;&#21046;&#38544;&#31169;&#20405;&#26435;&#30340;&#39118;&#38505;&#65292;&#20294;LLM&#30340;&#26368;&#26032;&#36827;&#23637;&#65288;&#20197;OpenAI&#30340;GPT-3.5&#30340;&#24494;&#35843;&#30028;&#38754;&#20026;&#20195;&#34920;&#65289;&#37325;&#26032;&#24341;&#21457;&#20102;&#20851;&#27880;&#12290;&#26377;&#20154;&#21487;&#33021;&#20250;&#38382;&#65306;LLM&#30340;&#24494;&#35843;&#26159;&#21542;&#20250;&#23548;&#33268;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#23884;&#20837;&#30340;&#20010;&#20154;&#20449;&#24687;&#27844;&#28431;&#65311;&#26412;&#25991;&#25253;&#36947;&#20102;&#39318;&#27425;&#23581;&#35797;&#23547;&#27714;&#31572;&#26696;&#30340;&#21162;&#21147;&#65292;&#37325;&#28857;&#26159;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;LLM&#21033;&#29992;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
The era post-2018 marked the advent of Large Language Models (LLMs), with innovations such as OpenAI's ChatGPT showcasing prodigious linguistic prowess. As the industry galloped toward augmenting model parameters and capitalizing on vast swaths of human language data, security and privacy challenges also emerged. Foremost among these is the potential inadvertent accrual of Personal Identifiable Information (PII) during web-based data acquisition, posing risks of unintended PII disclosure. While strategies like RLHF during training and Catastrophic Forgetting have been marshaled to control the risk of privacy infringements, recent advancements in LLMs, epitomized by OpenAI's fine-tuning interface for GPT-3.5, have reignited concerns. One may ask: can the fine-tuning of LLMs precipitate the leakage of personal information embedded within training datasets? This paper reports the first endeavor to seek the answer to the question, particularly our discovery of a new LLM exploitation avenue
&lt;/p&gt;</description></item></channel></rss>