<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#25915;&#20987;&#32773;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;LM&#36827;&#34892;&#23545;&#25239;&#24494;&#35843;&#65292;&#20197;&#25918;&#22823;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#30340;&#26333;&#20809;&#65292;&#37319;&#29992;&#20266;&#26631;&#31614;&#21644;&#26426;&#22120;&#29983;&#25104;&#27010;&#29575;&#26469;&#21152;&#24378;LM&#23545;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#20445;&#30041;&#12290;</title><link>https://arxiv.org/abs/2402.12189</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#20266;&#26631;&#31614;&#25104;&#21592;&#36164;&#26684;&#36827;&#34892;&#24494;&#35843;&#26469;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#26333;&#20809;
&lt;/p&gt;
&lt;p&gt;
Amplifying Training Data Exposure through Fine-Tuning with Pseudo-Labeled Memberships
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12189
&lt;/p&gt;
&lt;p&gt;
&#25915;&#20987;&#32773;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;LM&#36827;&#34892;&#23545;&#25239;&#24494;&#35843;&#65292;&#20197;&#25918;&#22823;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#30340;&#26333;&#20809;&#65292;&#37319;&#29992;&#20266;&#26631;&#31614;&#21644;&#26426;&#22120;&#29983;&#25104;&#27010;&#29575;&#26469;&#21152;&#24378;LM&#23545;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#20445;&#30041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;(LMs)&#30001;&#20110;&#25968;&#25454;&#35760;&#24518;&#32780;&#23481;&#26131;&#21463;&#21040;&#35757;&#32451;&#25968;&#25454;&#25552;&#21462;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#22330;&#26223;&#65292;&#22312;&#36825;&#31181;&#22330;&#26223;&#20013;&#65292;&#25915;&#20987;&#32773;&#23545;&#39044;&#35757;&#32451;LM&#36827;&#34892;&#23545;&#25239;&#24494;&#35843;&#65292;&#20197;&#25918;&#22823;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#30340;&#26333;&#20809;&#12290;&#35813;&#31574;&#30053;&#19981;&#21516;&#20110;&#20808;&#21069;&#30340;&#30740;&#31350;&#65292;&#20854;&#30446;&#30340;&#26159;&#21152;&#24378;LM&#23545;&#20854;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#20445;&#30041;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25915;&#20987;&#32773;&#38656;&#35201;&#25910;&#38598;&#19982;&#39044;&#35757;&#32451;&#25968;&#25454;&#23494;&#20999;&#30456;&#20851;&#30340;&#29983;&#25104;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#27809;&#26377;&#23454;&#38469;&#25968;&#25454;&#38598;&#30340;&#30693;&#35782;&#65292;&#34913;&#37327;&#29983;&#25104;&#25991;&#26412;&#20013;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#37327;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#30446;&#26631;LM&#30340;&#26426;&#22120;&#29983;&#25104;&#27010;&#29575;&#25152;&#34920;&#31034;&#30340;&#25104;&#21592;&#36817;&#20284;&#20540;&#20026;&#36825;&#20123;&#29983;&#25104;&#25991;&#26412;&#20351;&#29992;&#20266;&#26631;&#31614;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24494;&#35843;LM&#20197;&#25903;&#25345;&#37027;&#20123;&#26356;&#26377;&#21487;&#33021;&#28304;&#33258;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#29983;&#25104;&#25991;&#26412;&#65292;&#26681;&#25454;&#20854;&#25104;&#21592;&#36164;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12189v1 Announce Type: new  Abstract: Neural language models (LMs) are vulnerable to training data extraction attacks due to data memorization. This paper introduces a novel attack scenario wherein an attacker adversarially fine-tunes pre-trained LMs to amplify the exposure of the original training data. This strategy differs from prior studies by aiming to intensify the LM's retention of its pre-training dataset. To achieve this, the attacker needs to collect generated texts that are closely aligned with the pre-training data. However, without knowledge of the actual dataset, quantifying the amount of pre-training data within generated texts is challenging. To address this, we propose the use of pseudo-labels for these generated texts, leveraging membership approximations indicated by machine-generated probabilities from the target LM. We subsequently fine-tune the LM to favor generations with higher likelihoods of originating from the pre-training data, based on their memb
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#20844;&#20849;&#25968;&#25454;&#26469;&#25913;&#36827;&#31169;&#26377;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#23398;&#20064;&#20844;&#20849;&#25968;&#25454;&#20013;&#30340;&#20849;&#20139;&#34920;&#31034;&#65292;&#21487;&#20197;&#22312;&#20004;&#31181;&#36801;&#31227;&#23398;&#20064;&#22330;&#26223;&#20013;&#23454;&#29616;&#26368;&#20248;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#22312;&#21333;&#20219;&#21153;&#36801;&#31227;&#22330;&#26223;&#20013;&#65292;&#31639;&#27861;&#22312;&#32473;&#23450;&#23376;&#31354;&#38388;&#33539;&#22260;&#20869;&#25628;&#32034;&#32447;&#24615;&#27169;&#22411;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20248;&#36229;&#39069;&#39118;&#38505;&#12290;&#22312;&#22810;&#20219;&#21153;&#20010;&#24615;&#21270;&#22330;&#26223;&#20013;&#65292;&#36275;&#22815;&#30340;&#20844;&#20849;&#25968;&#25454;&#21487;&#20197;&#28040;&#38500;&#31169;&#26377;&#21327;&#35843;&#38656;&#27714;&#65292;&#24182;&#36890;&#36807;&#32431;&#23616;&#37096;&#23398;&#20064;&#36798;&#21040;&#30456;&#21516;&#30340;&#25928;&#29992;&#12290;</title><link>http://arxiv.org/abs/2312.15551</link><description>&lt;p&gt;
&#21033;&#29992;&#20844;&#20849;&#34920;&#31034;&#26469;&#36827;&#34892;&#31169;&#26377;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Leveraging Public Representations for Private Transfer Learning. (arXiv:2312.15551v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15551
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#20844;&#20849;&#25968;&#25454;&#26469;&#25913;&#36827;&#31169;&#26377;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#23398;&#20064;&#20844;&#20849;&#25968;&#25454;&#20013;&#30340;&#20849;&#20139;&#34920;&#31034;&#65292;&#21487;&#20197;&#22312;&#20004;&#31181;&#36801;&#31227;&#23398;&#20064;&#22330;&#26223;&#20013;&#23454;&#29616;&#26368;&#20248;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#22312;&#21333;&#20219;&#21153;&#36801;&#31227;&#22330;&#26223;&#20013;&#65292;&#31639;&#27861;&#22312;&#32473;&#23450;&#23376;&#31354;&#38388;&#33539;&#22260;&#20869;&#25628;&#32034;&#32447;&#24615;&#27169;&#22411;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20248;&#36229;&#39069;&#39118;&#38505;&#12290;&#22312;&#22810;&#20219;&#21153;&#20010;&#24615;&#21270;&#22330;&#26223;&#20013;&#65292;&#36275;&#22815;&#30340;&#20844;&#20849;&#25968;&#25454;&#21487;&#20197;&#28040;&#38500;&#31169;&#26377;&#21327;&#35843;&#38656;&#27714;&#65292;&#24182;&#36890;&#36807;&#32431;&#23616;&#37096;&#23398;&#20064;&#36798;&#21040;&#30456;&#21516;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#23558;&#20844;&#20849;&#25968;&#25454;&#32435;&#20837;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#30340;&#26368;&#26032;&#23454;&#35777;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#20174;&#20844;&#20849;&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#20849;&#20139;&#34920;&#31034;&#22914;&#20309;&#25913;&#36827;&#31169;&#26377;&#23398;&#20064;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#32447;&#24615;&#22238;&#24402;&#30340;&#20004;&#31181;&#24120;&#35265;&#36801;&#31227;&#23398;&#20064;&#22330;&#26223;&#65292;&#20004;&#32773;&#37117;&#20551;&#35774;&#20844;&#20849;&#20219;&#21153;&#21644;&#31169;&#26377;&#20219;&#21153;&#65288;&#22238;&#24402;&#21521;&#37327;&#65289;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#20849;&#20139;&#19968;&#20010;&#20302;&#31209;&#23376;&#31354;&#38388;&#12290;&#22312;&#31532;&#19968;&#31181;&#21333;&#20219;&#21153;&#36801;&#31227;&#22330;&#26223;&#20013;&#65292;&#30446;&#26631;&#26159;&#23398;&#20064;&#19968;&#20010;&#22312;&#25152;&#26377;&#29992;&#25143;&#20043;&#38388;&#20849;&#20139;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#27599;&#20010;&#29992;&#25143;&#23545;&#24212;&#25968;&#25454;&#38598;&#20013;&#30340;&#19968;&#34892;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#21305;&#37197;&#30340;&#19978;&#19979;&#30028;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#32473;&#23450;&#23376;&#31354;&#38388;&#20272;&#35745;&#33539;&#22260;&#20869;&#25628;&#32034;&#32447;&#24615;&#27169;&#22411;&#30340;&#31639;&#27861;&#31867;&#20013;&#23454;&#29616;&#20102;&#26368;&#20248;&#36229;&#39069;&#39118;&#38505;&#12290;&#22312;&#22810;&#20219;&#21153;&#27169;&#22411;&#20010;&#24615;&#21270;&#30340;&#31532;&#20108;&#31181;&#24773;&#26223;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#22312;&#26377;&#36275;&#22815;&#30340;&#20844;&#20849;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#29992;&#25143;&#21487;&#20197;&#36991;&#20813;&#31169;&#26377;&#21327;&#35843;&#65292;&#22240;&#20026;&#22312;&#32473;&#23450;&#23376;&#31354;&#38388;&#20869;&#32431;&#31929;&#30340;&#23616;&#37096;&#23398;&#20064;&#21487;&#20197;&#36798;&#21040;&#30456;&#21516;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the recent empirical success of incorporating public data into differentially private learning, we theoretically investigate how a shared representation learned from public data can improve private learning. We explore two common scenarios of transfer learning for linear regression, both of which assume the public and private tasks (regression vectors) share a low-rank subspace in a high-dimensional space. In the first single-task transfer scenario, the goal is to learn a single model shared across all users, each corresponding to a row in a dataset. We provide matching upper and lower bounds showing that our algorithm achieves the optimal excess risk within a natural class of algorithms that search for the linear model within the given subspace estimate. In the second scenario of multitask model personalization, we show that with sufficient public data, users can avoid private coordination, as purely local learning within the given subspace achieves the same utility. Take
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#21453;&#28436;&#30340;&#21435;&#38500;&#25915;&#20987;&#26041;&#27861;&#65288;\textsc{Mira}&#65289;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#30772;&#35299;&#22823;&#22810;&#25968;&#20027;&#27969;&#40657;&#30418;DNN&#27700;&#21360;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#21463;&#20445;&#25252;&#27169;&#22411;&#30340;&#20869;&#37096;&#20449;&#24687;&#26469;&#24674;&#22797;&#21644;&#28040;&#38500;&#27700;&#21360;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2309.03466</link><description>&lt;p&gt;
&#21033;&#29992;&#22522;&#20110;&#27169;&#22411;&#21453;&#28436;&#30340;&#21435;&#38500;&#25915;&#20987;&#26041;&#27861;&#30772;&#35299;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#40657;&#30418;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
MIRA: Cracking Black-box Watermarking on Deep Neural Networks via Model Inversion-based Removal Attacks. (arXiv:2309.03466v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#21453;&#28436;&#30340;&#21435;&#38500;&#25915;&#20987;&#26041;&#27861;&#65288;\textsc{Mira}&#65289;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#30772;&#35299;&#22823;&#22810;&#25968;&#20027;&#27969;&#40657;&#30418;DNN&#27700;&#21360;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#21463;&#20445;&#25252;&#27169;&#22411;&#30340;&#20869;&#37096;&#20449;&#24687;&#26469;&#24674;&#22797;&#21644;&#28040;&#38500;&#27700;&#21360;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20445;&#25252;&#35757;&#32451;&#26377;&#32032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#30693;&#35782;&#20135;&#26435;&#65292;&#40657;&#30418;DNN&#27700;&#21360;&#24050;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#36825;&#20123;&#27700;&#21360;&#34987;&#23884;&#20837;&#21040;DNN&#27169;&#22411;&#22312;&#19968;&#32452;&#29305;&#21035;&#35774;&#35745;&#30340;&#26679;&#26412;&#19978;&#30340;&#39044;&#27979;&#34892;&#20026;&#20013;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#32463;&#39564;&#35777;&#26126;&#65292;&#22823;&#22810;&#25968;&#40657;&#30418;&#27700;&#21360;&#26041;&#26696;&#23545;&#24050;&#30693;&#30340;&#21435;&#38500;&#25915;&#20987;&#20855;&#26377;&#25269;&#25239;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#21453;&#28436;&#30340;&#21435;&#38500;&#25915;&#20987;&#26041;&#27861;&#65288;\textsc{Mira}&#65289;&#65292;&#35813;&#26041;&#27861;&#23545;&#22823;&#22810;&#25968;&#20027;&#27969;&#40657;&#30418;DNN&#27700;&#21360;&#26041;&#26696;&#37117;&#26159;&#26080;&#20851;&#27700;&#21360;&#30340;&#65292;&#24182;&#19988;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#25915;&#20987;&#27969;&#31243;&#21033;&#29992;&#21463;&#20445;&#25252;&#27169;&#22411;&#30340;&#20869;&#37096;&#20449;&#24687;&#26469;&#24674;&#22797;&#21644;&#28040;&#38500;&#27700;&#21360;&#20449;&#24687;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#30446;&#26631;&#31867;&#21035;&#26816;&#27979;&#21644;&#24674;&#22797;&#26679;&#26412;&#20998;&#21106;&#31639;&#27861;&#65292;&#20197;&#20943;&#23569;\textsc{Mira}&#24341;&#36215;&#30340;&#25928;&#29992;&#25439;&#22833;&#65292;&#24182;&#23454;&#29616;&#26368;&#20248;&#30340;&#25915;&#20987;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
To protect the intellectual property of well-trained deep neural networks (DNNs), black-box DNN watermarks, which are embedded into the prediction behavior of DNN models on a set of specially-crafted samples, have gained increasing popularity in both academy and industry. Watermark robustness is usually implemented against attackers who steal the protected model and obfuscate its parameters for watermark removal. Recent studies empirically prove the robustness of most black-box watermarking schemes against known removal attempts.  In this paper, we propose a novel Model Inversion-based Removal Attack (\textsc{Mira}), which is watermark-agnostic and effective against most of mainstream black-box DNN watermarking schemes. In general, our attack pipeline exploits the internals of the protected model to recover and unlearn the watermark message. We further design target class detection and recovered sample splitting algorithms to reduce the utility loss caused by \textsc{Mira} and achieve 
&lt;/p&gt;</description></item></channel></rss>