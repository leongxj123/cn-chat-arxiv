<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#38590;&#24230;&#26657;&#20934;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#26174;&#33879;&#25552;&#39640;&#20302;FPR&#19979;&#30340;TPR&#65292;&#20197;&#39564;&#35777;&#35757;&#32451;&#27169;&#22411;&#26159;&#21542;&#20445;&#25252;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2401.04929</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#38590;&#24230;&#26657;&#20934;&#25552;&#21319;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Learning-Based Difficulty Calibration for Enhanced Membership Inference Attacks. (arXiv:2401.04929v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#38590;&#24230;&#26657;&#20934;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#26174;&#33879;&#25552;&#39640;&#20302;FPR&#19979;&#30340;TPR&#65292;&#20197;&#39564;&#35777;&#35757;&#32451;&#27169;&#22411;&#26159;&#21542;&#20445;&#25252;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#30446;&#21069;&#26159;&#21508;&#31181;&#24212;&#29992;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20174;&#21307;&#30103;&#20445;&#20581;&#21040;&#37329;&#34701;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#25935;&#24863;&#25968;&#25454;&#26469;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#24341;&#21457;&#20102;&#23545;&#38544;&#31169;&#21644;&#23433;&#20840;&#30340;&#25285;&#24551;&#12290;&#19968;&#31181;&#39564;&#35777;&#35757;&#32451;&#27169;&#22411;&#26159;&#21542;&#20445;&#25252;&#38544;&#31169;&#30340;&#26041;&#27861;&#26159;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIA&#65289;&#65292;&#23427;&#20801;&#35768;&#23545;&#25163;&#30830;&#23450;&#29305;&#23450;&#25968;&#25454;&#28857;&#26159;&#21542;&#26159;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19968;&#37096;&#20998;&#12290;&#34429;&#28982;&#24050;&#32463;&#22312;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#30340;MIA&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#33021;&#22815;&#22312;&#20302;&#20551;&#38451;&#24615;&#29575;&#65288;FPR&#65289;&#21306;&#22495;&#65288;0.01%~1%&#65289;&#23454;&#29616;&#36739;&#39640;&#30340;&#30495;&#38451;&#24615;&#29575;&#65288;TPR&#65289;&#12290;&#36825;&#26159;&#23454;&#38469;&#24212;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;MIA&#24517;&#39035;&#32771;&#34385;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MIA&#26041;&#27861;&#65292;&#26088;&#22312;&#26174;&#33879;&#25552;&#39640;&#20302;FPR&#30340;TPR&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;&#22522;&#20110;&#23398;&#20064;&#30340;&#38590;&#24230;&#26657;&#20934;&#65288;LDC-MIA&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#23558;&#25968;&#25454;&#35760;&#24405;&#20197;&#20854;&#38590;&#24230;&#32423;&#21035;&#36827;&#34892;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models, in particular deep neural networks, are currently an integral part of various applications, from healthcare to finance. However, using sensitive data to train these models raises concerns about privacy and security. One method that has emerged to verify if the trained models are privacy-preserving is Membership Inference Attacks (MIA), which allows adversaries to determine whether a specific data point was part of a model's training dataset. While a series of MIAs have been proposed in the literature, only a few can achieve high True Positive Rates (TPR) in the low False Positive Rate (FPR) region (0.01%~1%). This is a crucial factor to consider for an MIA to be practically useful in real-world settings. In this paper, we present a novel approach to MIA that is aimed at significantly improving TPR at low FPRs. Our method, named learning-based difficulty calibration for MIA(LDC-MIA), characterizes data records by their hardness levels using a neural network clas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#24037;&#20855;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#30340;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#26041;&#27861;AUTOLYCUS&#12290;</title><link>http://arxiv.org/abs/2302.02162</link><description>&lt;p&gt;
AUTOLYCUS: &#21033;&#29992;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#23545;&#30333;&#30418;&#27169;&#22411;&#36827;&#34892;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
AUTOLYCUS: Exploiting Explainable AI (XAI) for Model Extraction Attacks against White-Box Models. (arXiv:2302.02162v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#24037;&#20855;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#30340;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#26041;&#27861;AUTOLYCUS&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#26088;&#22312;&#38416;&#26126;AI&#27169;&#22411;&#20915;&#31574;&#36807;&#31243;&#30340;&#25216;&#26415;&#21644;&#31243;&#24207;&#12290;&#34429;&#28982;XAI&#23545;&#20110;&#29702;&#35299;AI&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#24456;&#26377;&#20215;&#20540;&#65292;&#20294;&#29992;&#20110;&#36825;&#31181;&#25581;&#31034;&#30340;&#25968;&#25454;&#20250;&#24102;&#26469;&#28508;&#22312;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#28431;&#27934;&#12290;&#29616;&#26377;&#25991;&#29486;&#24050;&#32463;&#30830;&#23450;&#20102;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#21253;&#25324;&#25104;&#21592;&#25512;&#35770;&#12289;&#27169;&#22411;&#21453;&#28436;&#21644;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#12290;&#26681;&#25454;&#28041;&#21450;&#30340;&#35774;&#32622;&#21644;&#21508;&#26041;&#65292;&#36825;&#20123;&#25915;&#20987;&#21487;&#33021;&#38024;&#23545;&#27169;&#22411;&#26412;&#36523;&#25110;&#29992;&#20110;&#21019;&#24314;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#35748;&#20026;&#25552;&#20379;XAI&#30340;&#24037;&#20855;&#29305;&#21035;&#20250;&#22686;&#21152;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#30340;&#39118;&#38505;&#65292;&#36825;&#21487;&#33021;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#24403;AI&#27169;&#22411;&#30340;&#25152;&#26377;&#32773;&#20165;&#24895;&#25552;&#20379;&#40657;&#30418;&#35775;&#38382;&#32780;&#19981;&#19982;&#20854;&#20182;&#26041;&#20849;&#20139;&#27169;&#22411;&#21442;&#25968;&#21644;&#32467;&#26500;&#26102;&#12290;&#20026;&#20102;&#25506;&#31350;&#36825;&#31181;&#38544;&#31169;&#39118;&#38505;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AUTOLYCUS&#65292;&#19968;&#31181;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable Artificial Intelligence (XAI) encompasses a range of techniques and procedures aimed at elucidating the decision-making processes of AI models. While XAI is valuable in understanding the reasoning behind AI models, the data used for such revelations poses potential security and privacy vulnerabilities. Existing literature has identified privacy risks targeting machine learning models, including membership inference, model inversion, and model extraction attacks. Depending on the settings and parties involved, such attacks may target either the model itself or the training data used to create the model.  We have identified that tools providing XAI can particularly increase the vulnerability of model extraction attacks, which can be a significant issue when the owner of an AI model prefers to provide only black-box access rather than sharing the model parameters and architecture with other parties. To explore this privacy risk, we propose AUTOLYCUS, a model extraction attack 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;IDIA&#26469;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#31169;&#65292;&#22823;&#35268;&#27169;&#23454;&#39564;&#34920;&#26126;&#20351;&#29992;&#20110;&#35757;&#32451;&#30340;&#20010;&#20154;&#21487;&#20197;&#34987;&#38750;&#24120;&#39640;&#30340;&#20934;&#30830;&#29575;&#35782;&#21035;&#20986;&#26469;&#65292;&#34920;&#26126;&#38656;&#35201;&#26356;&#22909;&#22320;&#35299;&#20915;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.07341</link><description>&lt;p&gt;
CLIP&#26159;&#21542;&#30693;&#36947;&#25105;&#30340;&#33080;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does CLIP Know My Face?. (arXiv:2209.07341v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;IDIA&#26469;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#31169;&#65292;&#22823;&#35268;&#27169;&#23454;&#39564;&#34920;&#26126;&#20351;&#29992;&#20110;&#35757;&#32451;&#30340;&#20010;&#20154;&#21487;&#20197;&#34987;&#38750;&#24120;&#39640;&#30340;&#20934;&#30830;&#29575;&#35782;&#21035;&#20986;&#26469;&#65292;&#34920;&#26126;&#38656;&#35201;&#26356;&#22909;&#22320;&#35299;&#20915;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#20010;&#24212;&#29992;&#20013;&#30340;&#26222;&#21450;&#65292;&#20445;&#25252;&#35757;&#32451;&#25968;&#25454;&#30340;&#38544;&#31169;&#38382;&#39064;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#21333;&#27169;&#22411;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#22810;&#27169;&#22411;&#30340;&#38544;&#31169;&#65292;&#29305;&#21035;&#26159;&#20687;CLIP&#36825;&#26679;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#30340;&#36523;&#20221;&#25512;&#26029;&#25915;&#20987;(IDIA)&#36890;&#36807;&#29992;&#21516;&#19968;&#20154;&#30340;&#22270;&#29255;&#21521;&#27169;&#22411;&#26597;&#35810;&#65292;&#20174;&#32780;&#25581;&#31034;&#35813;&#20010;&#20154;&#26159;&#21542;&#34987;&#21253;&#21547;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#12290;&#35753;&#27169;&#22411;&#20174;&#21508;&#31181;&#21487;&#33021;&#30340;&#25991;&#26412;&#26631;&#31614;&#20013;&#36873;&#25321;&#65292;&#27169;&#22411;&#20250;&#36879;&#38706;&#26159;&#21542;&#35782;&#21035;&#35813;&#20154;&#29289;&#65292;&#20174;&#32780;&#34920;&#26126;&#20854;&#34987;&#29992;&#20110;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;CLIP&#19978;&#36827;&#34892;&#30340;&#22823;&#35268;&#27169;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#20110;&#35757;&#32451;&#30340;&#20010;&#20154;&#21487;&#20197;&#34987;&#38750;&#24120;&#39640;&#30340;&#20934;&#30830;&#29575;&#35782;&#21035;&#20986;&#26469;&#12290;&#25105;&#20204;&#30830;&#35748;&#35813;&#27169;&#22411;&#24050;&#32463;&#23398;&#20250;&#23558;&#21517;&#31216;&#19982;&#25551;&#32472;&#30340;&#20010;&#20154;&#30456;&#20851;&#32852;&#65292;&#36825;&#24847;&#21619;&#30528;&#25935;&#24863;&#20449;&#24687;&#23384;&#22312;&#20110;&#20854;&#20013;&#65292;&#21487;&#20197;&#34987;&#23545;&#25163;&#25552;&#21462;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20984;&#26174;&#20102;&#38656;&#35201;&#22312;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#26356;&#22909;&#22320;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rise of deep learning in various applications, privacy concerns around the protection of training data has become a critical area of research. Whereas prior studies have focused on privacy risks in single-modal models, we introduce a novel method to assess privacy for multi-modal models, specifically vision-language models like CLIP. The proposed Identity Inference Attack (IDIA) reveals whether an individual was included in the training data by querying the model with images of the same person. Letting the model choose from a wide variety of possible text labels, the model reveals whether it recognizes the person and, therefore, was used for training. Our large-scale experiments on CLIP demonstrate that individuals used for training can be identified with very high accuracy. We confirm that the model has learned to associate names with depicted individuals, implying the existence of sensitive information that can be extracted by adversaries. Our results highlight the need for 
&lt;/p&gt;</description></item></channel></rss>