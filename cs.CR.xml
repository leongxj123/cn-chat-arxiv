<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#20805;&#36275;&#32500;&#24230;&#20943;&#23569;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#30340;&#26368;&#20339;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#24182;&#22312;&#20302;&#32500;&#21644;&#39640;&#32500;&#35774;&#32622;&#19979;&#24314;&#31435;&#20102;&#19981;&#21516;ially private &#20999;&#29255;&#36870;&#22238;&#24402;&#30340;&#19979;&#30028;&#12290;&#36890;&#36807;&#20223;&#30495;&#21644;&#30495;&#23454;&#25968;&#25454;&#20998;&#26512;&#39564;&#35777;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.08150</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#20999;&#29255;&#36870;&#22238;&#24402;: &#26497;&#23567;&#26497;&#22823;&#24615;&#21644;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Sliced Inverse Regression: Minimax Optimality and Algorithm. (arXiv:2401.08150v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#20805;&#36275;&#32500;&#24230;&#20943;&#23569;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#30340;&#26368;&#20339;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#24182;&#22312;&#20302;&#32500;&#21644;&#39640;&#32500;&#35774;&#32622;&#19979;&#24314;&#31435;&#20102;&#19981;&#21516;ially private &#20999;&#29255;&#36870;&#22238;&#24402;&#30340;&#19979;&#30028;&#12290;&#36890;&#36807;&#20223;&#30495;&#21644;&#30495;&#23454;&#25968;&#25454;&#20998;&#26512;&#39564;&#35777;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#39537;&#21160;&#24212;&#29992;&#30340;&#26222;&#21450;&#65292;&#38544;&#31169;&#20445;&#25252;&#24050;&#25104;&#20026;&#39640;&#32500;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#20999;&#29255;&#36870;&#22238;&#24402;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#32479;&#35745;&#25216;&#26415;&#65292;&#36890;&#36807;&#38477;&#20302;&#21327;&#21464;&#37327;&#30340;&#32500;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#36275;&#22815;&#30340;&#32479;&#35745;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#20805;&#36275;&#32500;&#24230;&#20943;&#23569;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#30340;&#26368;&#20339;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#20302;&#32500;&#21644;&#39640;&#32500;&#35774;&#32622;&#19979;&#24314;&#31435;&#20102;&#19981;&#21516;ially private &#20999;&#29255;&#36870;&#22238;&#24402;&#30340;&#19979;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#26497;&#23567;&#26497;&#22823;&#19979;&#30028;&#30340;&#35201;&#27714;&#65292;&#24182;&#22312;&#38477;&#32500;&#31354;&#38388;&#20013;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#21644;&#20445;&#23384;&#37325;&#35201;&#20449;&#24687;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#20223;&#30495;&#23454;&#39564;&#21644;&#30495;&#23454;&#25968;&#25454;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy preservation has become a critical concern in high-dimensional data analysis due to the growing prevalence of data-driven applications. Proposed by Li (1991), sliced inverse regression has emerged as a widely utilized statistical technique for reducing covariate dimensionality while maintaining sufficient statistical information. In this paper, we propose optimally differentially private algorithms specifically designed to address privacy concerns in the context of sufficient dimension reduction. We proceed to establish lower bounds for differentially private sliced inverse regression in both the low and high-dimensional settings. Moreover, we develop differentially private algorithms that achieve the minimax lower bounds up to logarithmic factors. Through a combination of simulations and real data analysis, we illustrate the efficacy of these differentially private algorithms in safeguarding privacy while preserving vital information within the reduced dimension space. As a na
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#23545;&#25239;&#24615;&#21518;&#38376;&#38450;&#24481;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#34987;&#27745;&#26579;&#26679;&#26412;&#20013;&#27880;&#20837;&#38750;&#23545;&#25239;&#24615;&#21518;&#38376;&#65292;&#24403;&#35302;&#21457;&#26102;&#21487;&#20197;&#25233;&#21046;&#25915;&#20987;&#32773;&#23545;&#27745;&#26579;&#25968;&#25454;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#21516;&#26102;&#20445;&#25345;&#23545;&#24178;&#20928;&#25968;&#25454;&#30340;&#24433;&#21709;&#26377;&#38480;&#12290;</title><link>http://arxiv.org/abs/2307.15539</link><description>&lt;p&gt;
&#38750;&#23545;&#25239;&#24615;&#21518;&#38376;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Backdoor Defense with Non-Adversarial Backdoor. (arXiv:2307.15539v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15539
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#23545;&#25239;&#24615;&#21518;&#38376;&#38450;&#24481;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#34987;&#27745;&#26579;&#26679;&#26412;&#20013;&#27880;&#20837;&#38750;&#23545;&#25239;&#24615;&#21518;&#38376;&#65292;&#24403;&#35302;&#21457;&#26102;&#21487;&#20197;&#25233;&#21046;&#25915;&#20987;&#32773;&#23545;&#27745;&#26579;&#25968;&#25454;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#21516;&#26102;&#20445;&#25345;&#23545;&#24178;&#20928;&#25968;&#25454;&#30340;&#24433;&#21709;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36825;&#31181;&#25915;&#20987;&#24182;&#19981;&#20250;&#24433;&#21709;&#32593;&#32476;&#23545;&#24178;&#20928;&#25968;&#25454;&#30340;&#24615;&#33021;&#65292;&#20294;&#19968;&#26086;&#28155;&#21152;&#35302;&#21457;&#27169;&#24335;&#65292;&#23601;&#20250;&#25805;&#32437;&#32593;&#32476;&#34892;&#20026;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#22823;&#22823;&#38477;&#20302;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#20294;&#23427;&#20204;&#22312;&#24178;&#20928;&#25968;&#25454;&#19978;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#20173;&#28982;&#36828;&#36828;&#33853;&#21518;&#20110;&#24178;&#20928;&#27169;&#22411;&#12290;&#21463;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#21644;&#26377;&#25928;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#38750;&#24120;&#26377;&#25928;&#30340;&#38450;&#24481;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#27880;&#20837;&#20102;&#38024;&#23545;&#34987;&#27745;&#26579;&#26679;&#26412;&#30340;&#38750;&#23545;&#25239;&#24615;&#21518;&#38376;&#12290;&#25353;&#29031;&#21518;&#38376;&#25915;&#20987;&#30340;&#19968;&#33324;&#27493;&#39588;&#65292;&#25105;&#20204;&#26816;&#27979;&#19968;&#23567;&#32452;&#21487;&#30097;&#26679;&#26412;&#65292;&#28982;&#21518;&#23545;&#23427;&#20204;&#24212;&#29992;&#27602;&#21270;&#31574;&#30053;&#12290;&#19968;&#26086;&#35302;&#21457;&#65292;&#38750;&#23545;&#25239;&#24615;&#21518;&#38376;&#25233;&#21046;&#20102;&#25915;&#20987;&#32773;&#23545;&#27745;&#26579;&#25968;&#25454;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20294;&#23545;&#24178;&#20928;&#25968;&#25454;&#30340;&#24433;&#21709;&#26377;&#38480;&#12290;&#38450;&#24481;&#21487;&#20197;&#22312;&#25968;&#25454;&#39044;&#22788;&#29702;&#26399;&#38388;&#36827;&#34892;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;&#26631;&#20934;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#27969;&#31243;&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are vulnerable to backdoor attack, which does not affect the network's performance on clean data but would manipulate the network behavior once a trigger pattern is added. Existing defense methods have greatly reduced attack success rate, but their prediction accuracy on clean data still lags behind a clean model by a large margin. Inspired by the stealthiness and effectiveness of backdoor attack, we propose a simple but highly effective defense framework which injects non-adversarial backdoors targeting poisoned samples. Following the general steps in backdoor attack, we detect a small set of suspected samples and then apply a poisoning strategy to them. The non-adversarial backdoor, once triggered, suppresses the attacker's backdoor on poisoned data, but has limited influence on clean data. The defense can be carried out during data preprocessing, without any modification to the standard end-to-end training pipeline. We conduct extensive experiments on mul
&lt;/p&gt;</description></item></channel></rss>