<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#20445;&#25345;&#36793;&#32536;&#19968;&#33268;&#30340;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#32447;&#24615;&#27169;&#22411;&#30340;&#36807;&#37327;&#39118;&#38505;&#30340;&#26032;&#30028;&#38480;&#65292;&#20026;&#36830;&#32493;&#21644;Lipschitz&#25439;&#22833;&#20989;&#25968;&#25552;&#20379;&#20102;&#19978;&#30028;&#21644;&#19979;&#30028;&#12290;</title><link>https://arxiv.org/abs/2402.04375</link><description>&lt;p&gt;
&#22312;&#20445;&#25345;&#36793;&#32536;&#19968;&#33268;&#30340;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#32447;&#24615;&#27169;&#22411;&#30340;&#36807;&#37327;&#39118;&#38505;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Bounding the Excess Risk for Linear Models Trained on Marginal-Preserving, Differentially-Private, Synthetic Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#20445;&#25345;&#36793;&#32536;&#19968;&#33268;&#30340;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#32447;&#24615;&#27169;&#22411;&#30340;&#36807;&#37327;&#39118;&#38505;&#30340;&#26032;&#30028;&#38480;&#65292;&#20026;&#36830;&#32493;&#21644;Lipschitz&#25439;&#22833;&#20989;&#25968;&#25552;&#20379;&#20102;&#19978;&#30028;&#21644;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#24191;&#27867;&#24212;&#29992;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20110;&#27169;&#22411;&#21487;&#33021;&#25581;&#31034;&#35757;&#32451;&#25968;&#25454;&#20013;&#20010;&#20307;&#30340;&#31169;&#23494;&#20449;&#24687;&#30340;&#25285;&#24551;&#12290;&#20026;&#20102;&#38450;&#27490;&#25935;&#24863;&#25968;&#25454;&#30340;&#27844;&#38706;&#65292;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#30340;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#32780;&#19981;&#26159;&#30495;&#23454;&#35757;&#32451;&#25968;&#25454;&#26469;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#21512;&#25104;&#25968;&#25454;&#30340;&#19968;&#20010;&#20851;&#38190;&#20248;&#28857;&#26159;&#33021;&#22815;&#20445;&#25345;&#21407;&#22987;&#20998;&#24067;&#30340;&#20302;&#38454;&#36793;&#32536;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#38024;&#23545;&#22312;&#36825;&#31181;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#32447;&#24615;&#27169;&#22411;&#65292;&#38024;&#23545;&#36830;&#32493;&#21644;Lipschitz&#25439;&#22833;&#20989;&#25968;&#25552;&#20986;&#20102;&#26032;&#30340;&#36807;&#37327;&#32463;&#39564;&#39118;&#38505;&#30340;&#19978;&#30028;&#21644;&#19979;&#30028;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#32467;&#26524;&#20043;&#22806;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing use of machine learning (ML) has raised concerns that an ML model may reveal private information about an individual who has contributed to the training dataset. To prevent leakage of sensitive data, we consider using differentially-private (DP), synthetic training data instead of real training data to train an ML model. A key desirable property of synthetic data is its ability to preserve the low-order marginals of the original distribution. Our main contribution comprises novel upper and lower bounds on the excess empirical risk of linear models trained on such synthetic data, for continuous and Lipschitz loss functions. We perform extensive experimentation alongside our theoretical results.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;&#35299;&#20915;&#27599;&#20010;&#35774;&#22791;&#25345;&#26377;&#31169;&#26377;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;&#31169;&#26377;&#32479;&#35745;&#21644;&#31169;&#26377;&#32852;&#37030;&#23398;&#20064;&#35774;&#35745;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#21407;&#35821;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#24182;&#22312;&#19981;&#38656;&#35201;&#24378;&#20449;&#20219;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#20445;&#25252;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2307.15017</link><description>&lt;p&gt;
&#31169;&#26377;&#32852;&#37030;&#25968;&#25454;&#20998;&#26512;&#30340;&#21487;&#37319;&#26679;&#21311;&#21517;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Samplable Anonymous Aggregation for Private Federated Data Analysis. (arXiv:2307.15017v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#35299;&#20915;&#27599;&#20010;&#35774;&#22791;&#25345;&#26377;&#31169;&#26377;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;&#31169;&#26377;&#32479;&#35745;&#21644;&#31169;&#26377;&#32852;&#37030;&#23398;&#20064;&#35774;&#35745;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#21407;&#35821;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#24182;&#22312;&#19981;&#38656;&#35201;&#24378;&#20449;&#20219;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#20445;&#25252;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27599;&#20010;&#35774;&#22791;&#25345;&#26377;&#31169;&#26377;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#35774;&#35745;&#21487;&#25193;&#23637;&#30340;&#31169;&#26377;&#32479;&#35745;&#21327;&#35758;&#21644;&#31169;&#26377;&#32852;&#37030;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#21407;&#35821;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23454;&#29616;&#20960;&#31181;&#24120;&#29992;&#31639;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#24378;&#20449;&#20219;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#38544;&#31169;&#36134;&#21153;&#65292;&#25509;&#36817;&#20110;&#38598;&#20013;&#35774;&#32622;&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29616;&#35813;&#21407;&#35821;&#30340;&#31995;&#32479;&#26550;&#26500;&#65292;&#24182;&#23545;&#35813;&#31995;&#32479;&#36827;&#34892;&#20102;&#23433;&#20840;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We revisit the problem of designing scalable protocols for private statistics and private federated learning when each device holds its private data. Our first contribution is to propose a simple primitive that allows for efficient implementation of several commonly used algorithms, and allows for privacy accounting that is close to that in the central setting without requiring the strong trust assumptions it entails. Second, we propose a system architecture that implements this primitive and perform a security analysis of the proposed system.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LDMs)&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#23558;&#39640;&#32500;&#20687;&#32032;&#31354;&#38388;&#36716;&#21464;&#20026;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#23454;&#29616;&#26356;&#39640;&#25928;&#24555;&#36895;&#30340;DMs&#35757;&#32451;&#65292;&#24182;&#19988;&#36890;&#36807;&#21482;&#24494;&#35843;&#27880;&#24847;&#21147;&#27169;&#22359;&#20943;&#23569;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.15759</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Latent Diffusion Models. (arXiv:2305.15759v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LDMs)&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#23558;&#39640;&#32500;&#20687;&#32032;&#31354;&#38388;&#36716;&#21464;&#20026;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#23454;&#29616;&#26356;&#39640;&#25928;&#24555;&#36895;&#30340;DMs&#35757;&#32451;&#65292;&#24182;&#19988;&#36890;&#36807;&#21482;&#24494;&#35843;&#27880;&#24847;&#21147;&#27169;&#22359;&#20943;&#23569;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;(DMs)&#34987;&#24191;&#27867;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#30452;&#25509;&#22312;&#39640;&#32500;&#20687;&#32032;&#31354;&#38388;&#20013;&#36816;&#34892;&#65292;DMs&#30340;&#20248;&#21270;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#38656;&#35201;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#12290;&#36825;&#23548;&#33268;&#30001;&#20110;&#24046;&#20998;&#38544;&#31169;&#30340;&#21487;&#32452;&#21512;&#24615;&#23646;&#24615;&#65292;&#22823;&#37327;&#22122;&#38899;&#27880;&#20837;&#21040;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#36807;&#31243;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LDMs)&#12290;LDMs&#20351;&#29992;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#23558;&#39640;&#32500;&#20687;&#32032;&#31354;&#38388;&#20943;&#23569;&#21040;&#26356;&#20302;&#32500;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#20351;&#35757;&#32451;DMs&#26356;&#21152;&#39640;&#25928;&#21644;&#24555;&#36895;&#12290;&#19982;[Ghalebikesabi&#31561;&#20154;&#65292;2023]&#39044;&#20808;&#29992;&#20844;&#20849;&#25968;&#25454;&#39044;&#35757;&#32451;DMs&#65292;&#28982;&#21518;&#20877;&#29992;&#38544;&#31169;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#19981;&#21516;&#65292;&#25105;&#20204;&#20165;&#24494;&#35843;LDMs&#20013;&#19981;&#21516;&#23618;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#20197;&#33719;&#24471;&#38544;&#31169;&#25935;&#24863;&#25968;&#25454;&#65292;&#30456;&#23545;&#20110;&#25972;&#20010;DM&#24494;&#35843;&#65292;&#21487;&#20943;&#23569;&#22823;&#32422;96%&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models (DMs) are widely used for generating high-quality image datasets. However, since they operate directly in the high-dimensional pixel space, optimization of DMs is computationally expensive, requiring long training times. This contributes to large amounts of noise being injected into the differentially private learning process, due to the composability property of differential privacy. To address this challenge, we propose training Latent Diffusion Models (LDMs) with differential privacy. LDMs use powerful pre-trained autoencoders to reduce the high-dimensional pixel space to a much lower-dimensional latent space, making training DMs more efficient and fast. Unlike [Ghalebikesabi et al., 2023] that pre-trains DMs with public data then fine-tunes them with private data, we fine-tune only the attention modules of LDMs at varying layers with privacy-sensitive data, reducing the number of trainable parameters by approximately 96% compared to fine-tuning the entire DM. We te
&lt;/p&gt;</description></item></channel></rss>