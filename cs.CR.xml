<rss version="2.0"><channel><title>Chat Arxiv cs.CR</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CR</description><item><title>RigorLLM&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#39640;&#25928;&#26377;&#25928;&#22320;&#35843;&#33410;LLMs&#30340;&#26377;&#23475;&#21644;&#19981;&#23433;&#20840;&#36755;&#20837;&#21644;&#36755;&#20986;&#65292;&#21253;&#25324;&#33021;&#37327;&#25968;&#25454;&#22686;&#24378;&#12289;&#26368;&#23567;-&#26368;&#22823;&#20248;&#21270;&#23433;&#20840;&#36755;&#20837;&#21518;&#32512;&#65292;&#20197;&#21450;&#22522;&#20110;&#25968;&#25454;&#22686;&#24378;&#30340;&#40065;&#26834;KNN&#19982;LLMs&#34701;&#21512;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.13031</link><description>&lt;p&gt;
RigorLLM&#65306;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25269;&#24481;&#19981;&#33391;&#20869;&#23481;&#30340;&#40065;&#26834;&#38450;&#25252;&#26639;
&lt;/p&gt;
&lt;p&gt;
RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13031
&lt;/p&gt;
&lt;p&gt;
RigorLLM&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#39640;&#25928;&#26377;&#25928;&#22320;&#35843;&#33410;LLMs&#30340;&#26377;&#23475;&#21644;&#19981;&#23433;&#20840;&#36755;&#20837;&#21644;&#36755;&#20986;&#65292;&#21253;&#25324;&#33021;&#37327;&#25968;&#25454;&#22686;&#24378;&#12289;&#26368;&#23567;-&#26368;&#22823;&#20248;&#21270;&#23433;&#20840;&#36755;&#20837;&#21518;&#32512;&#65292;&#20197;&#21450;&#22522;&#20110;&#25968;&#25454;&#22686;&#24378;&#30340;&#40065;&#26834;KNN&#19982;LLMs&#34701;&#21512;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#20013;&#20986;&#29616;&#30340;&#20559;&#35265;&#20197;&#21450;&#22312;&#24694;&#24847;&#36755;&#20837;&#19979;&#20135;&#29983;&#26377;&#23475;&#20869;&#23481;&#30340;&#28508;&#21147;&#65292;&#23588;&#20854;&#26159;&#23545;&#25239;&#24615;&#25915;&#20987;&#19979;&#65292;&#37117;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#38450;&#25252;&#26639;&#65288;RigorLLM&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#39640;&#25928;&#26377;&#25928;&#22320;&#35843;&#33410;LLMs&#30340;&#26377;&#23475;&#21644;&#19981;&#23433;&#20840;&#36755;&#20837;&#21644;&#36755;&#20986;&#12290;&#36890;&#36807;&#37319;&#29992;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#36890;&#36807;&#26391;&#20043;&#19975;&#21160;&#21147;&#23398;&#36827;&#34892;&#22522;&#20110;&#33021;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#12289;&#36890;&#36807;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#38024;&#23545;&#36755;&#20837;&#20248;&#21270;&#23433;&#20840;&#21518;&#32512;&#65292;&#20197;&#21450;&#22522;&#20110;&#25105;&#20204;&#30340;&#25968;&#25454;&#22686;&#24378;&#23558;&#40065;&#26834;KNN&#19982;LLMs&#34701;&#21512;&#30340;&#22522;&#20110;&#34701;&#21512;&#30340;&#27169;&#22411;&#65292;RigorLLM&#20026;&#26377;&#23475;&#20869;&#23481;&#30340;&#35843;&#33410;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13031v1 Announce Type: cross  Abstract: Recent advancements in Large Language Models (LLMs) have showcased remarkable capabilities across various tasks in different domains. However, the emergence of biases and the potential for generating harmful content in LLMs, particularly under malicious inputs, pose significant challenges. Current mitigation strategies, while effective, are not resilient under adversarial attacks. This paper introduces Resilient Guardrails for Large Language Models (RigorLLM), a novel framework designed to efficiently and effectively moderate harmful and unsafe inputs and outputs for LLMs. By employing a multi-faceted approach that includes energy-based training data augmentation through Langevin dynamics, optimizing a safe suffix for inputs via minimax optimization, and integrating a fusion-based model combining robust KNN with LLMs based on our data augmentation, RigorLLM offers a robust solution to harmful content moderation. Our experimental evalua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#25216;&#26415;&#35282;&#24230;&#20840;&#38754;&#27010;&#36848;&#20102;&#22312;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#29256;&#26435;&#20445;&#25252;&#38382;&#39064;&#65292;&#21253;&#25324;&#25968;&#25454;&#29256;&#26435;&#21644;&#27169;&#22411;&#29256;&#26435;&#20004;&#20010;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#21019;&#26032;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.02333</link><description>&lt;p&gt;
&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#29256;&#26435;&#20445;&#25252;&#65306;&#25216;&#26415;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Copyright Protection in Generative AI: A Technical Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#25216;&#26415;&#35282;&#24230;&#20840;&#38754;&#27010;&#36848;&#20102;&#22312;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#29256;&#26435;&#20445;&#25252;&#38382;&#39064;&#65292;&#21253;&#25324;&#25968;&#25454;&#29256;&#26435;&#21644;&#27169;&#22411;&#29256;&#26435;&#20004;&#20010;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#21019;&#26032;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#65288;Generative AI&#65289;&#21462;&#24471;&#20102;&#24555;&#36895;&#21457;&#23637;&#65292;&#25193;&#23637;&#20102;&#20854;&#21019;&#24314;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#20195;&#30721;&#31561;&#21512;&#25104;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65288;Deep Generative Models&#65292;DGMs&#65289;&#29983;&#25104;&#30340;&#20869;&#23481;&#39640;&#20445;&#30495;&#24230;&#21644;&#30495;&#23454;&#24615;&#24341;&#21457;&#20102;&#37325;&#22823;&#30340;&#29256;&#26435;&#38382;&#39064;&#12290;&#20851;&#20110;&#22914;&#20309;&#26377;&#25928;&#20445;&#25252;DGMs&#20013;&#30340;&#29256;&#26435;&#38382;&#39064;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#21508;&#31181;&#27861;&#24459;&#36777;&#35770;&#12290;&#26412;&#25991;&#20174;&#25216;&#26415;&#35282;&#24230;&#25552;&#20379;&#20102;&#29256;&#26435;&#20445;&#25252;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#25105;&#20204;&#20174;&#20004;&#20010;&#19981;&#21516;&#30340;&#35270;&#35282;&#26469;&#36827;&#34892;&#30740;&#31350;&#65306;&#19968;&#26159;&#19982;&#25968;&#25454;&#25152;&#26377;&#32773;&#25152;&#25345;&#26377;&#30340;&#28304;&#25968;&#25454;&#30456;&#20851;&#30340;&#29256;&#26435;&#65292;&#20108;&#26159;&#19982;&#27169;&#22411;&#26500;&#24314;&#32773;&#25152;&#32500;&#25252;&#30340;&#29983;&#25104;&#27169;&#22411;&#30456;&#20851;&#30340;&#29256;&#26435;&#12290;&#23545;&#20110;&#25968;&#25454;&#29256;&#26435;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#25968;&#25454;&#25152;&#26377;&#32773;&#22914;&#20309;&#20445;&#25252;&#20854;&#20869;&#23481;&#65292;&#24182;&#22312;&#19981;&#20405;&#29359;&#36825;&#20123;&#26435;&#21033;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;DGMs&#12290;&#23545;&#20110;&#27169;&#22411;&#29256;&#26435;&#65292;&#25105;&#20204;&#30340;&#35752;&#35770;&#24310;&#20280;&#21040;&#38450;&#27490;&#27169;&#22411;&#30423;&#31363;&#21644;&#35782;&#21035;&#29305;&#23450;&#27169;&#22411;&#29983;&#25104;&#30340;&#36755;&#20986;&#30340;&#31574;&#30053;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#19968;&#20123;&#21019;&#26032;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#26469;&#22788;&#29702;&#36825;&#20123;&#29256;&#26435;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI has witnessed rapid advancement in recent years, expanding their capabilities to create synthesized content such as text, images, audio, and code. The high fidelity and authenticity of contents generated by these Deep Generative Models (DGMs) have sparked significant copyright concerns. There have been various legal debates on how to effectively safeguard copyrights in DGMs. This work delves into this issue by providing a comprehensive overview of copyright protection from a technical perspective. We examine from two distinct viewpoints: the copyrights pertaining to the source data held by the data owners and those of the generative models maintained by the model builders. For data copyright, we delve into methods data owners can protect their content and DGMs can be utilized without infringing upon these rights. For model copyright, our discussion extends to strategies for preventing model theft and identifying outputs generated by specific models. Finally, we highlight 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#24352;&#37327;&#32593;&#32476;&#26550;&#26500;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#65292;&#24182;&#25552;&#20986;&#20102;&#30830;&#20445;&#40065;&#26834;&#24615;&#30340;&#26126;&#30830;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2202.12319</link><description>&lt;p&gt;
&#20445;&#25252;&#38544;&#31169;&#30340;&#24352;&#37327;&#32593;&#32476;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Privacy-preserving machine learning with tensor networks. (arXiv:2202.12319v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.12319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#24352;&#37327;&#32593;&#32476;&#26550;&#26500;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#65292;&#24182;&#25552;&#20986;&#20102;&#30830;&#20445;&#40065;&#26834;&#24615;&#30340;&#26126;&#30830;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#32593;&#32476;&#34987;&#24191;&#27867;&#29992;&#20110;&#25552;&#20379;&#20302;&#33021;&#37327;&#24577;&#30340;&#39640;&#25928;&#34920;&#31034;&#65292;&#26368;&#36817;&#34987;&#25552;&#20986;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#24352;&#37327;&#32593;&#32476;&#26550;&#26500;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#65292;&#36825;&#23545;&#20110;&#22788;&#29702;&#21307;&#30103;&#35760;&#24405;&#31561;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20013;&#23384;&#22312;&#30340;&#26032;&#38544;&#31169;&#28431;&#27934;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#20102;&#35828;&#26126;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30830;&#20445;&#23545;&#36825;&#31181;&#28431;&#27934;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#26126;&#30830;&#26465;&#20214;&#65292;&#36825;&#28041;&#21450;&#21040;&#22312;&#35268;&#33539;&#23545;&#31216;&#24615;&#19979;&#31561;&#20215;&#30340;&#27169;&#22411;&#30340;&#21051;&#30011;&#12290;&#25105;&#20204;&#20005;&#26684;&#35777;&#26126;&#20102;&#24352;&#37327;&#32593;&#32476;&#26550;&#26500;&#28385;&#36275;&#36825;&#20123;&#26465;&#20214;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#30697;&#38453;&#20056;&#31215;&#24577;&#30340;&#35268;&#33539;&#24418;&#24335;&#65292;&#20855;&#26377;&#39640;&#24230;&#30340;&#35268;&#24459;&#24615;&#24182;&#20462;&#27491;&#20102;&#27531;&#20313;&#35268;&#33539;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor networks, widely used for providing efficient representations of low-energy states of local quantum many-body systems, have been recently proposed as machine learning architectures which could present advantages with respect to traditional ones. In this work we show that tensor network architectures have especially prospective properties for privacy-preserving machine learning, which is important in tasks such as the processing of medical records. First, we describe a new privacy vulnerability that is present in feedforward neural networks, illustrating it in synthetic and real-world datasets. Then, we develop well-defined conditions to guarantee robustness to such vulnerability, which involve the characterization of models equivalent under gauge symmetry. We rigorously prove that such conditions are satisfied by tensor-network architectures. In doing so, we define a novel canonical form for matrix product states, which has a high degree of regularity and fixes the residual gaug
&lt;/p&gt;</description></item></channel></rss>