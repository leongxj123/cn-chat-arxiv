<rss version="2.0"><channel><title>Chat Arxiv cs.MA</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.MA</description><item><title>AgentMixer&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20801;&#35768;&#26234;&#33021;&#20307;&#36890;&#36807;&#31574;&#30053;&#20462;&#25913;&#26469;&#23454;&#29616;&#21327;&#21516;&#20915;&#31574;&#12290;&#36890;&#36807;&#26500;&#36896;&#32852;&#21512;&#31574;&#30053;&#20026;&#21508;&#20010;&#37096;&#20998;&#31574;&#30053;&#30340;&#38750;&#32447;&#24615;&#32452;&#21512;&#65292;&#21487;&#23454;&#29616;&#37096;&#20998;&#21487;&#35266;&#27979;&#26234;&#33021;&#20307;&#30340;&#31283;&#23450;&#35757;&#32451;&#21644;&#20998;&#25955;&#25191;&#34892;&#12290;</title><link>http://arxiv.org/abs/2401.08728</link><description>&lt;p&gt;
AgentMixer: &#22810;&#26234;&#33021;&#20307;&#30456;&#20851;&#31574;&#30053;&#22240;&#23376;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
AgentMixer: Multi-Agent Correlated Policy Factorization. (arXiv:2401.08728v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08728
&lt;/p&gt;
&lt;p&gt;
AgentMixer&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20801;&#35768;&#26234;&#33021;&#20307;&#36890;&#36807;&#31574;&#30053;&#20462;&#25913;&#26469;&#23454;&#29616;&#21327;&#21516;&#20915;&#31574;&#12290;&#36890;&#36807;&#26500;&#36896;&#32852;&#21512;&#31574;&#30053;&#20026;&#21508;&#20010;&#37096;&#20998;&#31574;&#30053;&#30340;&#38750;&#32447;&#24615;&#32452;&#21512;&#65292;&#21487;&#23454;&#29616;&#37096;&#20998;&#21487;&#35266;&#27979;&#26234;&#33021;&#20307;&#30340;&#31283;&#23450;&#35757;&#32451;&#21644;&#20998;&#25955;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#20013;&#24335;&#35757;&#32451;&#19982;&#20998;&#25955;&#24335;&#25191;&#34892;&#65288;CTDE&#65289;&#24191;&#27867;&#24212;&#29992;&#20110;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21033;&#29992;&#38598;&#20013;&#24335;&#20540;&#20989;&#25968;&#26469;&#31283;&#23450;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#26234;&#33021;&#20307;&#22522;&#20110;&#26412;&#22320;&#35266;&#27979;&#29420;&#31435;&#22320;&#20570;&#20915;&#31574;&#65292;&#36825;&#21487;&#33021;&#19981;&#20250;&#23548;&#33268;&#20855;&#26377;&#36275;&#22815;&#21327;&#35843;&#24615;&#30340;&#30456;&#20851;&#32852;&#30340;&#32852;&#21512;&#31574;&#30053;&#12290;&#21463;&#30456;&#20851;&#22343;&#34913;&#27010;&#24565;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#24341;&#20837;"&#31574;&#30053;&#20462;&#25913;"&#26469;&#20026;&#26234;&#33021;&#20307;&#25552;&#20379;&#21327;&#35843;&#31574;&#30053;&#30340;&#26426;&#21046;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;AgentMixer&#65292;&#23558;&#32852;&#21512;&#23436;&#20840;&#21487;&#35266;&#27979;&#31574;&#30053;&#26500;&#36896;&#20026;&#21508;&#20010;&#37096;&#20998;&#21487;&#35266;&#27979;&#31574;&#30053;&#30340;&#38750;&#32447;&#24615;&#32452;&#21512;&#12290;&#20026;&#20102;&#23454;&#29616;&#20998;&#25955;&#24335;&#25191;&#34892;&#65292;&#21487;&#20197;&#36890;&#36807;&#27169;&#20223;&#32852;&#21512;&#31574;&#30053;&#26469;&#24471;&#21040;&#21508;&#20010;&#37096;&#20998;&#31574;&#30053;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;&#27169;&#20223;&#23398;&#20064;&#21487;&#33021;&#20250;&#23548;&#33268;&#30001;&#20110;&#32852;&#21512;&#31574;&#30053;&#21644;&#20010;&#20307;&#31574;&#30053;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#32780;&#23548;&#33268;&#30340;&#38750;&#23545;&#31216;&#23398;&#20064;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;
Centralized training with decentralized execution (CTDE) is widely employed to stabilize partially observable multi-agent reinforcement learning (MARL) by utilizing a centralized value function during training. However, existing methods typically assume that agents make decisions based on their local observations independently, which may not lead to a correlated joint policy with sufficient coordination. Inspired by the concept of correlated equilibrium, we propose to introduce a \textit{strategy modification} to provide a mechanism for agents to correlate their policies. Specifically, we present a novel framework, AgentMixer, which constructs the joint fully observable policy as a non-linear combination of individual partially observable policies. To enable decentralized execution, one can derive individual policies by imitating the joint policy. Unfortunately, such imitation learning can lead to \textit{asymmetric learning failure} caused by the mismatch between joint policy and indi
&lt;/p&gt;</description></item><item><title>&#26368;&#22823;&#29109;&#24322;&#36136;&#20195;&#29702;&#38236;&#20687;&#23398;&#20064;(MEHAML)&#26159;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#22823;&#29109;&#21407;&#29702;&#35774;&#35745;&#20102;&#26368;&#22823;&#29109;MARL&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#20855;&#26377;&#32852;&#21512;&#26368;&#22823;&#29109;&#30446;&#26631;&#30340;&#21333;&#35843;&#25913;&#36827;&#21644;&#25910;&#25947;&#33267;&#20013;&#20301;&#21709;&#24212;&#22343;&#34913;(QRE)&#30340;&#26399;&#26395;&#29305;&#24615;&#65292;&#24182;&#36890;&#36807;&#25193;&#23637;&#24120;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;HASAC&#26469;&#39564;&#35777;&#20854;&#23454;&#29992;&#24615;&#21644;&#22312;&#25506;&#32034;&#21644;&#31283;&#20581;&#24615;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.10715</link><description>&lt;p&gt;
&#26368;&#22823;&#29109;&#24322;&#36136;&#20195;&#29702;&#38236;&#20687;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Maximum Entropy Heterogeneous-Agent Mirror Learning. (arXiv:2306.10715v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10715
&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#29109;&#24322;&#36136;&#20195;&#29702;&#38236;&#20687;&#23398;&#20064;(MEHAML)&#26159;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#22823;&#29109;&#21407;&#29702;&#35774;&#35745;&#20102;&#26368;&#22823;&#29109;MARL&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#20855;&#26377;&#32852;&#21512;&#26368;&#22823;&#29109;&#30446;&#26631;&#30340;&#21333;&#35843;&#25913;&#36827;&#21644;&#25910;&#25947;&#33267;&#20013;&#20301;&#21709;&#24212;&#22343;&#34913;(QRE)&#30340;&#26399;&#26395;&#29305;&#24615;&#65292;&#24182;&#36890;&#36807;&#25193;&#23637;&#24120;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;HASAC&#26469;&#39564;&#35777;&#20854;&#23454;&#29992;&#24615;&#21644;&#22312;&#25506;&#32034;&#21644;&#31283;&#20581;&#24615;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#22312;&#21512;&#20316;&#21338;&#24328;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#38754;&#20020;&#26679;&#26412;&#25928;&#29575;&#20302;&#12289;&#36229;&#21442;&#25968;&#33030;&#24369;&#24615;&#21644;&#25910;&#25947;&#20110;&#27425;&#20248;&#32435;&#20160;&#22343;&#34913;&#30340;&#39118;&#38505;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;&#26368;&#22823;&#29109;&#24322;&#36136;&#20195;&#29702;&#38236;&#20687;&#23398;&#20064;(MEHAML)&#65292;&#21033;&#29992;&#26368;&#22823;&#29109;&#21407;&#29702;&#35774;&#35745;&#20102;&#26368;&#22823;&#29109;MARL&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20174;MEHAML&#26694;&#26550;&#23548;&#20986;&#30340;&#31639;&#27861;&#20855;&#26377;&#32852;&#21512;&#26368;&#22823;&#29109;&#30446;&#26631;&#30340;&#21333;&#35843;&#25913;&#36827;&#21644;&#25910;&#25947;&#33267;&#20013;&#20301;&#21709;&#24212;&#22343;&#34913;(QRE)&#30340;&#26399;&#26395;&#29305;&#24615;&#12290;MEHAML&#30340;&#23454;&#29992;&#24615;&#36890;&#36807;&#24320;&#21457;&#24191;&#27867;&#20351;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;HASAC&#30340;MEHAML&#25193;&#23637;&#26469;&#23637;&#31034;&#65292;&#22312;&#19977;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#23637;&#31034;&#20986;&#20102;&#25506;&#32034;&#21644;&#31283;&#20581;&#24615;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent reinforcement learning (MARL) has been shown effective for cooperative games in recent years. However, existing state-of-the-art methods face challenges related to sample inefficiency, brittleness regarding hyperparameters, and the risk of converging to a suboptimal Nash Equilibrium. To resolve these issues, in this paper, we propose a novel theoretical framework, named Maximum Entropy Heterogeneous-Agent Mirror Learning (MEHAML), that leverages the maximum entropy principle to design maximum entropy MARL actor-critic algorithms. We prove that algorithms derived from the MEHAML framework enjoy the desired properties of the monotonic improvement of the joint maximum entropy objective and the convergence to quantal response equilibrium (QRE). The practicality of MEHAML is demonstrated by developing a MEHAML extension of the widely used RL algorithm, HASAC (for soft actor-critic), which shows significant improvements in exploration and robustness on three challenging benchmark
&lt;/p&gt;</description></item></channel></rss>