<rss version="2.0"><channel><title>Chat Arxiv cs.MA</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.MA</description><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;SUB-SAMPLE-Q&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#23616;&#37096;&#20195;&#29702;&#36827;&#34892;&#23376;&#37319;&#26679;&#65292;&#22312;&#25351;&#25968;&#32423;&#21035;&#30340;&#26102;&#38388;&#20869;&#35745;&#31639;&#20986;&#26368;&#20339;&#31574;&#30053;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#19982;&#26631;&#20934;&#26041;&#27861;&#30456;&#27604;&#30340;&#25351;&#25968;&#21152;&#36895;&#12290;</title><link>https://arxiv.org/abs/2403.00222</link><description>&lt;p&gt;
&#23384;&#22312;&#22823;&#35268;&#27169;&#23616;&#37096;&#20195;&#29702;&#30340;&#20840;&#23616;&#20915;&#31574;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Reinforcement Learning for Global Decision Making in the Presence of Local Agents at Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00222
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;SUB-SAMPLE-Q&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#23616;&#37096;&#20195;&#29702;&#36827;&#34892;&#23376;&#37319;&#26679;&#65292;&#22312;&#25351;&#25968;&#32423;&#21035;&#30340;&#26102;&#38388;&#20869;&#35745;&#31639;&#20986;&#26368;&#20339;&#31574;&#30053;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#19982;&#26631;&#20934;&#26041;&#27861;&#30456;&#27604;&#30340;&#25351;&#25968;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#23384;&#22312;&#35768;&#22810;&#23616;&#37096;&#20195;&#29702;&#30340;&#20840;&#23616;&#20915;&#31574;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#20840;&#23616;&#20915;&#31574;&#32773;&#20570;&#20986;&#24433;&#21709;&#25152;&#26377;&#23616;&#37096;&#20195;&#29702;&#30340;&#20915;&#31574;&#65292;&#30446;&#26631;&#26159;&#23398;&#20064;&#19968;&#20010;&#26368;&#22823;&#21270;&#20840;&#23616;&#21644;&#23616;&#37096;&#20195;&#29702;&#22870;&#21169;&#30340;&#31574;&#30053;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21487;&#25193;&#23637;&#24615;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#29366;&#24577;/&#21160;&#20316;&#31354;&#38388;&#30340;&#22823;&#23567;&#21487;&#33021;&#20250;&#38543;&#20195;&#29702;&#25968;&#37327;&#25351;&#25968;&#22686;&#38271;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SUB-SAMPLE-Q&#31639;&#27861;&#65292;&#22312;&#27492;&#31639;&#27861;&#20013;&#65292;&#20840;&#23616;&#20195;&#29702;&#23545;$k\leq n$&#20010;&#23616;&#37096;&#20195;&#29702;&#36827;&#34892;&#23376;&#37319;&#26679;&#20197;&#22312;&#20165;&#25351;&#25968;&#20110;$k$&#30340;&#26102;&#38388;&#20869;&#35745;&#31639;&#20986;&#26368;&#20339;&#31574;&#30053;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19982;&#25351;&#25968;&#20110;$n$&#30340;&#26631;&#20934;&#26041;&#27861;&#30456;&#27604;&#30340;&#25351;&#25968;&#21152;&#36895;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#38543;&#30528;&#23376;&#37319;&#26679;&#20195;&#29702;&#25968;$k$&#30340;&#22686;&#21152;&#65292;&#23398;&#21040;&#30340;&#31574;&#30053;&#23558;&#25910;&#25947;&#20110;&#39034;&#24207;&#20026;$\tilde{O}(1/\sqrt{k}+\epsilon_{k,m})$&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00222v1 Announce Type: new  Abstract: We study reinforcement learning for global decision-making in the presence of many local agents, where the global decision-maker makes decisions affecting all local agents, and the objective is to learn a policy that maximizes the rewards of both the global and the local agents. Such problems find many applications, e.g. demand response, EV charging, queueing, etc. In this setting, scalability has been a long-standing challenge due to the size of the state/action space which can be exponential in the number of agents. This work proposes the SUB-SAMPLE-Q algorithm where the global agent subsamples $k\leq n$ local agents to compute an optimal policy in time that is only exponential in $k$, providing an exponential speedup from standard methods that are exponential in $n$. We show that the learned policy converges to the optimal policy in the order of $\tilde{O}(1/\sqrt{k}+\epsilon_{k,m})$ as the number of sub-sampled agents $k$ increases, 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;Altruistic Gradient Adjustment (AgA)&#20248;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#26799;&#24230;&#35843;&#25972;&#26469;&#23545;&#40784;&#20010;&#20307;&#21644;&#38598;&#20307;&#30446;&#26631;&#65292;&#21152;&#36895;&#25910;&#25947;&#21040;&#26399;&#26395;&#35299;&#20915;&#26041;&#26696;</title><link>https://arxiv.org/abs/2402.12416</link><description>&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#20013;&#23545;&#40784;&#20010;&#20307;&#21644;&#38598;&#20307;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Aligning Individual and Collective Objectives in Multi-Agent Cooperation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12416
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;Altruistic Gradient Adjustment (AgA)&#20248;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#26799;&#24230;&#35843;&#25972;&#26469;&#23545;&#40784;&#20010;&#20307;&#21644;&#38598;&#20307;&#30446;&#26631;&#65292;&#21152;&#36895;&#25910;&#25947;&#21040;&#26399;&#26395;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#39046;&#22495;&#65292;&#38754;&#20020;&#30528;&#28151;&#21512;&#21160;&#26426;&#21512;&#20316;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#20010;&#20307;&#21644;&#38598;&#20307;&#30446;&#26631;&#20043;&#38388;&#23384;&#22312;&#22266;&#26377;&#30340;&#30683;&#30462;&#12290;&#24403;&#21069;&#22312;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23558;&#39046;&#22495;&#30693;&#35782;&#32435;&#20837;&#22870;&#21169;&#25110;&#24341;&#20837;&#39069;&#22806;&#26426;&#21046;&#26469;&#20419;&#36827;&#21512;&#20316;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#30528;&#25163;&#21160;&#35774;&#35745;&#25104;&#26412;&#21644;&#32570;&#20047;&#29702;&#35770;&#22522;&#30784;&#30340;&#25910;&#25947;&#31243;&#24207;&#35299;&#20915;&#26041;&#26696;&#30340;&#32570;&#28857;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#23558;&#28151;&#21512;&#21160;&#26426;&#21338;&#24328;&#24314;&#27169;&#20026;&#19968;&#20010;&#21487;&#24494;&#20998;&#30340;&#21338;&#24328;&#20197;&#30740;&#31350;&#23398;&#20064;&#21160;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Altruistic Gradient Adjustment (AgA)&#30340;&#26032;&#20248;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#26799;&#24230;&#35843;&#25972;&#26469;&#26032;&#39062;&#22320;&#23545;&#40784;&#20010;&#20307;&#21644;&#38598;&#20307;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#29702;&#35770;&#35777;&#26126;&#65292;AgA&#20013;&#36873;&#25321;&#36866;&#24403;&#30340;&#23545;&#40784;&#26435;&#37325;&#21487;&#20197;&#21152;&#36895;&#25910;&#25947;&#21040;&#26399;&#26395;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12416v1 Announce Type: cross  Abstract: In the field of multi-agent learning, the challenge of mixed-motive cooperation is pronounced, given the inherent contradictions between individual and collective goals. Current research in this domain primarily focuses on incorporating domain knowledge into rewards or introducing additional mechanisms to foster cooperation. However, many of these methods suffer from the drawbacks of manual design costs and the lack of a theoretical grounding convergence procedure to the solution. To address this gap, we approach the mixed-motive game by modeling it as a differentiable game to study learning dynamics. We introduce a novel optimization method named Altruistic Gradient Adjustment (AgA) that employs gradient adjustments to novelly align individual and collective objectives. Furthermore, we provide theoretical proof that the selection of an appropriate alignment weight in AgA can accelerate convergence towards the desired solutions while e
&lt;/p&gt;</description></item></channel></rss>