<rss version="2.0"><channel><title>Chat Arxiv cs.MA</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.MA</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#36866;&#24212;&#20855;&#26377;&#19981;&#21516;&#31574;&#30053;&#30340;&#26410;&#30693;&#21516;&#20276;&#12290;&#36890;&#36807;&#22870;&#21169;&#26234;&#33021;&#20307;&#22312;&#21382;&#21490;&#19978;&#19979;&#25991;&#20013;&#26377;&#25928;&#35782;&#21035;&#21516;&#20276;&#34892;&#20026;&#27169;&#24335;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20419;&#36827;&#26234;&#33021;&#20307;&#31215;&#26497;&#25506;&#32034;&#21644;&#24555;&#36895;&#36866;&#24212;&#65292;&#20174;&#32780;&#22312;&#19981;&#30830;&#23450;&#21516;&#20276;&#31574;&#30053;&#26102;&#25910;&#38598;&#20449;&#24687;&#21453;&#39304;&#65292;&#24182;&#22312;&#26377;&#20449;&#24515;&#26102;&#21033;&#29992;&#19978;&#19979;&#25991;&#25191;&#34892;&#26368;&#20339;&#21453;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.02468</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#25506;&#32034;&#30340;&#24555;&#36895;&#36866;&#24212;&#26410;&#30693;&#21516;&#20276;
&lt;/p&gt;
&lt;p&gt;
Fast Peer Adaptation with Context-aware Exploration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#36866;&#24212;&#20855;&#26377;&#19981;&#21516;&#31574;&#30053;&#30340;&#26410;&#30693;&#21516;&#20276;&#12290;&#36890;&#36807;&#22870;&#21169;&#26234;&#33021;&#20307;&#22312;&#21382;&#21490;&#19978;&#19979;&#25991;&#20013;&#26377;&#25928;&#35782;&#21035;&#21516;&#20276;&#34892;&#20026;&#27169;&#24335;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20419;&#36827;&#26234;&#33021;&#20307;&#31215;&#26497;&#25506;&#32034;&#21644;&#24555;&#36895;&#36866;&#24212;&#65292;&#20174;&#32780;&#22312;&#19981;&#30830;&#23450;&#21516;&#20276;&#31574;&#30053;&#26102;&#25910;&#38598;&#20449;&#24687;&#21453;&#39304;&#65292;&#24182;&#22312;&#26377;&#20449;&#24515;&#26102;&#21033;&#29992;&#19978;&#19979;&#25991;&#25191;&#34892;&#26368;&#20339;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#28216;&#25103;&#20013;&#65292;&#24555;&#36895;&#36866;&#24212;&#20855;&#26377;&#19981;&#21516;&#31574;&#30053;&#30340;&#26410;&#30693;&#21516;&#20276;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#20026;&#20102;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#26234;&#33021;&#20307;&#33021;&#22815;&#39640;&#25928;&#22320;&#25506;&#32034;&#21644;&#35782;&#21035;&#21516;&#20276;&#30340;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#36825;&#26159;&#36866;&#24212;&#20013;&#36827;&#34892;&#26368;&#20339;&#21453;&#24212;&#30340;&#20808;&#20915;&#26465;&#20214;&#12290;&#28982;&#32780;&#65292;&#24403;&#28216;&#25103;&#26159;&#37096;&#20998;&#21487;&#35266;&#27979;&#19988;&#26102;&#38388;&#36328;&#24230;&#24456;&#38271;&#26102;&#65292;&#25506;&#32034;&#26410;&#30693;&#21516;&#20276;&#30340;&#31574;&#30053;&#26159;&#22256;&#38590;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#20276;&#35782;&#21035;&#22870;&#21169;&#65292;&#26681;&#25454;&#26234;&#33021;&#20307;&#22312;&#21382;&#21490;&#29615;&#22659;&#19979;&#65288;&#20363;&#22914;&#22810;&#20010;&#22238;&#21512;&#30340;&#35266;&#23519;&#65289;&#22914;&#20309;&#26377;&#25928;&#22320;&#35782;&#21035;&#21516;&#20276;&#30340;&#34892;&#20026;&#27169;&#24335;&#26469;&#22870;&#21169;&#23398;&#20064;&#26234;&#33021;&#20307;&#12290;&#36825;&#20010;&#22870;&#21169;&#28608;&#21169;&#26234;&#33021;&#20307;&#23398;&#20064;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#25506;&#32034;&#21644;&#24555;&#36895;&#36866;&#24212;&#65292;&#21363;&#22312;&#23545;&#21516;&#20276;&#31574;&#30053;&#19981;&#30830;&#23450;&#26102;&#31215;&#26497;&#23547;&#25214;&#21644;&#25910;&#38598;&#20449;&#24687;&#21453;&#39304;&#65292;&#24182;&#22312;&#26377;&#20449;&#24515;&#26102;&#21033;&#29992;&#19978;&#19979;&#25991;&#25191;&#34892;&#26368;&#20339;&#21453;&#24212;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#27979;&#35797;&#22330;&#26223;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fast adapting to unknown peers (partners or opponents) with different strategies is a key challenge in multi-agent games. To do so, it is crucial for the agent to efficiently probe and identify the peer's strategy, as this is the prerequisite for carrying out the best response in adaptation. However, it is difficult to explore the strategies of unknown peers, especially when the games are partially observable and have a long horizon. In this paper, we propose a peer identification reward, which rewards the learning agent based on how well it can identify the behavior pattern of the peer over the historical context, such as the observation over multiple episodes. This reward motivates the agent to learn a context-aware policy for effective exploration and fast adaptation, i.e., to actively seek and collect informative feedback from peers when uncertain about their policies and to exploit the context to perform the best response when confident. We evaluate our method on diverse testbeds 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;CRAMP&#65292;&#19968;&#31181;&#22522;&#20110;&#22686;&#24378;&#24335;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#30340;&#20247;&#21253;&#24863;&#30693;&#20998;&#25955;&#24335;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#25317;&#25380;&#29615;&#22659;&#19979;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2309.10275</link><description>&lt;p&gt;
&#20855;&#26377;&#22686;&#24378;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#30340;&#20247;&#21253;&#24863;&#30693;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Crowd-Aware Multi-Agent Pathfinding With Boosted Curriculum Reinforcement Learning. (arXiv:2309.10275v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10275
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;CRAMP&#65292;&#19968;&#31181;&#22522;&#20110;&#22686;&#24378;&#24335;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#30340;&#20247;&#21253;&#24863;&#30693;&#20998;&#25955;&#24335;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#25317;&#25380;&#29615;&#22659;&#19979;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25317;&#25380;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36816;&#21160;&#35268;&#21010;&#38382;&#39064;&#65292;&#26088;&#22312;&#20026;&#31995;&#32479;&#20013;&#30340;&#25152;&#26377;&#26234;&#33021;&#20307;&#25214;&#21040;&#26080;&#30896;&#25758;&#36335;&#24452;&#12290;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#37117;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#31354;&#20013;&#32676;&#20307;&#12289;&#33258;&#21160;&#21270;&#20179;&#20648;&#26426;&#22120;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#12290;&#24403;&#21069;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#21487;&#20197;&#22823;&#33268;&#20998;&#20026;&#20004;&#31181;&#20027;&#35201;&#31867;&#21035;&#65306;&#38598;&#20013;&#24335;&#35268;&#21010;&#21644;&#20998;&#25955;&#24335;&#35268;&#21010;&#12290;&#38598;&#20013;&#24335;&#35268;&#21010;&#21463;&#21040;&#32500;&#24230;&#28798;&#38590;&#30340;&#22256;&#25200;&#65292;&#22240;&#27492;&#22312;&#22823;&#22411;&#21644;&#22797;&#26434;&#29615;&#22659;&#20013;&#19981;&#20855;&#22791;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20998;&#25955;&#24335;&#35268;&#21010;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#36827;&#34892;&#23454;&#26102;&#36335;&#24452;&#35268;&#21010;&#65292;&#23637;&#31034;&#20102;&#38544;&#24335;&#30340;&#21327;&#35843;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#23494;&#38598;&#29615;&#22659;&#20013;&#23427;&#20204;&#30340;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#19988;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CRAMP&#30340;&#20247;&#21253;&#24863;&#30693;&#20998;&#25955;&#24335;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#24335;&#35838;&#31243;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Path Finding (MAPF) in crowded environments presents a challenging problem in motion planning, aiming to find collision-free paths for all agents in the system. MAPF finds a wide range of applications in various domains, including aerial swarms, autonomous warehouse robotics, and self-driving vehicles. The current approaches for MAPF can be broadly categorized into two main categories: centralized and decentralized planning. Centralized planning suffers from the curse of dimensionality and thus does not scale well in large and complex environments. On the other hand, decentralized planning enables agents to engage in real-time path planning within a partially observable environment, demonstrating implicit coordination. However, they suffer from slow convergence and performance degradation in dense environments. In this paper, we introduce CRAMP, a crowd-aware decentralized approach to address this problem by leveraging reinforcement learning guided by a boosted curriculum-b
&lt;/p&gt;</description></item></channel></rss>