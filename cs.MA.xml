<rss version="2.0"><channel><title>Chat Arxiv cs.MA</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.MA</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#37319;&#29992;&#20998;&#25955;&#31574;&#30053;&#30340;MARL&#31639;&#27861;&#22312;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#22120;&#19979;&#30340;&#27425;&#26368;&#20248;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36716;&#21270;&#19982;&#33976;&#39311;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#23558;&#22810;&#26234;&#33021;&#20307;MDP&#36716;&#21270;&#20026;&#21333;&#26234;&#33021;&#20307;MDP&#20197;&#23454;&#29616;&#20998;&#25955;&#25191;&#34892;&#12290;</title><link>http://arxiv.org/abs/2207.11143</link><description>&lt;p&gt;
&#12298;&#37319;&#29992;&#36716;&#21270;&#19982;&#33976;&#39311;&#26694;&#26550;&#23454;&#29616;&#21512;&#20316;MARL&#20840;&#23616;&#26368;&#20248;&#24615;&#12299;
&lt;/p&gt;
&lt;p&gt;
Towards Global Optimality in Cooperative MARL with the Transformation And Distillation Framework. (arXiv:2207.11143v3 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37319;&#29992;&#20998;&#25955;&#31574;&#30053;&#30340;MARL&#31639;&#27861;&#22312;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#22120;&#19979;&#30340;&#27425;&#26368;&#20248;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36716;&#21270;&#19982;&#33976;&#39311;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#23558;&#22810;&#26234;&#33021;&#20307;MDP&#36716;&#21270;&#20026;&#21333;&#26234;&#33021;&#20307;MDP&#20197;&#23454;&#29616;&#20998;&#25955;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20998;&#25955;&#25191;&#34892;&#26159;&#19968;&#39033;&#26680;&#24515;&#38656;&#27714;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;&#27969;&#34892;&#30340;MARL&#31639;&#27861;&#37319;&#29992;&#20998;&#25955;&#31574;&#30053;&#26469;&#23454;&#29616;&#20998;&#25955;&#25191;&#34892;&#65292;&#24182;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#20316;&#20026;&#20248;&#21270;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;&#32771;&#34385;&#21040;&#20248;&#21270;&#26041;&#27861;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#31639;&#27861;&#20960;&#20046;&#27809;&#26377;&#20219;&#20309;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#26799;&#24230;&#19979;&#38477;&#34987;&#36873;&#20026;&#20248;&#21270;&#26041;&#27861;&#26102;&#65292;&#21508;&#31181;&#27969;&#34892;&#30340;&#20998;&#25955;&#31574;&#30053;MARL&#31639;&#27861;&#22312;&#29609;&#20855;&#20219;&#21153;&#20013;&#37117;&#26159;&#27425;&#26368;&#20248;&#30340;&#12290;&#26412;&#25991;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#20004;&#31181;&#24120;&#35265;&#30340;&#37319;&#29992;&#20998;&#25955;&#31574;&#30053;&#30340;&#31639;&#27861;&#8212;&#8212;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#21644;&#20540;&#20998;&#35299;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#26102;&#30340;&#27425;&#26368;&#20248;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36716;&#21270;&#19982;&#33976;&#39311;&#65288;TAD&#65289;&#26694;&#26550;&#65292;&#23427;&#23558;&#22810;&#26234;&#33021;&#20307;MDP&#37325;&#26032;&#21046;&#23450;&#20026;&#19968;&#31181;&#20855;&#26377;&#36830;&#32493;&#32467;&#26500;&#30340;&#29305;&#27530;&#21333;&#26234;&#33021;&#20307;MDP&#65292;&#24182;&#36890;&#36807;&#33976;&#39311;&#23454;&#29616;&#20998;&#25955;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decentralized execution is one core demand in cooperative multi-agent reinforcement learning (MARL). Recently, most popular MARL algorithms have adopted decentralized policies to enable decentralized execution and use gradient descent as their optimizer. However, there is hardly any theoretical analysis of these algorithms taking the optimization method into consideration, and we find that various popular MARL algorithms with decentralized policies are suboptimal in toy tasks when gradient descent is chosen as their optimization method. In this paper, we theoretically analyze two common classes of algorithms with decentralized policies -- multi-agent policy gradient methods and value-decomposition methods to prove their suboptimality when gradient descent is used. In addition, we propose the Transformation And Distillation (TAD) framework, which reformulates a multi-agent MDP as a special single-agent MDP with a sequential structure and enables decentralized execution by distilling the
&lt;/p&gt;</description></item></channel></rss>