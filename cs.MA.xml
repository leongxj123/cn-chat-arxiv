<rss version="2.0"><channel><title>Chat Arxiv cs.MA</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.MA</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Off-FSP&#65292;&#36825;&#26159;&#31454;&#20105;&#28216;&#25103;&#30340;&#31532;&#19968;&#20010;&#23454;&#29992;&#30340;&#26080;&#27169;&#22411;&#31163;&#32447;RL&#31639;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#22266;&#23450;&#25968;&#25454;&#38598;&#30340;&#26435;&#37325;&#65292;&#20351;&#29992;&#37325;&#35201;&#24615;&#25277;&#26679;&#65292;&#27169;&#25311;&#19982;&#21508;&#31181;&#23545;&#25163;&#30340;&#20114;&#21160;&#12290;</title><link>https://arxiv.org/abs/2403.00841</link><description>&lt;p&gt;
&#31454;&#20105;&#28216;&#25103;&#30340;&#31163;&#32447;&#34394;&#26500;&#33258;&#25105;&#23545;&#24328;
&lt;/p&gt;
&lt;p&gt;
Offline Fictitious Self-Play for Competitive Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Off-FSP&#65292;&#36825;&#26159;&#31454;&#20105;&#28216;&#25103;&#30340;&#31532;&#19968;&#20010;&#23454;&#29992;&#30340;&#26080;&#27169;&#22411;&#31163;&#32447;RL&#31639;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#22266;&#23450;&#25968;&#25454;&#38598;&#30340;&#26435;&#37325;&#65292;&#20351;&#29992;&#37325;&#35201;&#24615;&#25277;&#26679;&#65292;&#27169;&#25311;&#19982;&#21508;&#31181;&#23545;&#25163;&#30340;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22240;&#20854;&#22312;&#20197;&#21069;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#20013;&#25913;&#36827;&#31574;&#30053;&#32780;&#19981;&#38656;&#35201;&#22312;&#32447;&#20132;&#20114;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#37325;&#35270;&#12290;&#23613;&#31649;&#22312;&#21333;&#19968;&#26234;&#33021;&#20307;&#35774;&#32622;&#20013;&#21462;&#24471;&#25104;&#21151;&#65292;&#20294;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;RL&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#31454;&#20105;&#28216;&#25103;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;Off-FSP&#65292;&#36825;&#26159;&#31454;&#20105;&#28216;&#25103;&#30340;&#31532;&#19968;&#20010;&#23454;&#29992;&#30340;&#26080;&#27169;&#22411;&#31163;&#32447;RL&#31639;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#35843;&#25972;&#22266;&#23450;&#25968;&#25454;&#38598;&#30340;&#26435;&#37325;&#65292;&#20351;&#29992;&#37325;&#35201;&#24615;&#25277;&#26679;&#27169;&#25311;&#19982;&#21508;&#31181;&#23545;&#25163;&#30340;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00841v1 Announce Type: cross  Abstract: Offline Reinforcement Learning (RL) has received significant interest due to its ability to improve policies in previously collected datasets without online interactions. Despite its success in the single-agent setting, offline multi-agent RL remains a challenge, especially in competitive games. Firstly, unaware of the game structure, it is impossible to interact with the opponents and conduct a major learning paradigm, self-play, for competitive games. Secondly, real-world datasets cannot cover all the state and action space in the game, resulting in barriers to identifying Nash equilibrium (NE). To address these issues, this paper introduces Off-FSP, the first practical model-free offline RL algorithm for competitive games. We start by simulating interactions with various opponents by adjusting the weights of the fixed dataset with importance sampling. This technique allows us to learn best responses to different opponents and employ
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#12289;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#20013;&#24341;&#20837;&#20048;&#35266;&#26356;&#26032;&#65292;&#20197;&#32531;&#35299;&#21512;&#20316;&#20219;&#21153;&#20013;&#30340;&#30456;&#23545;&#36807;&#24230;&#27010;&#25324;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#27844;&#28431;&#21270;&#32447;&#24615;&#25972;&#27969;&#20989;&#25968;&#26469;&#37325;&#22609;&#20248;&#21183;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20445;&#25345;&#23545;&#28508;&#22312;&#30001;&#20854;&#20182;&#20195;&#29702;&#24341;&#36215;&#30340;&#20302;&#22238;&#25253;&#20010;&#21035;&#21160;&#20316;&#30340;&#20048;&#35266;&#24577;&#24230;&#12290;</title><link>http://arxiv.org/abs/2311.01953</link><description>&lt;p&gt;
&#20048;&#35266;&#30340;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#26799;&#24230;&#22312;&#21512;&#20316;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Optimistic Multi-Agent Policy Gradient for Cooperative Tasks. (arXiv:2311.01953v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01953
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#12289;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#20013;&#24341;&#20837;&#20048;&#35266;&#26356;&#26032;&#65292;&#20197;&#32531;&#35299;&#21512;&#20316;&#20219;&#21153;&#20013;&#30340;&#30456;&#23545;&#36807;&#24230;&#27010;&#25324;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#27844;&#28431;&#21270;&#32447;&#24615;&#25972;&#27969;&#20989;&#25968;&#26469;&#37325;&#22609;&#20248;&#21183;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20445;&#25345;&#23545;&#28508;&#22312;&#30001;&#20854;&#20182;&#20195;&#29702;&#24341;&#36215;&#30340;&#20302;&#22238;&#25253;&#20010;&#21035;&#21160;&#20316;&#30340;&#20048;&#35266;&#24577;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#30001;&#20110;&#36807;&#25311;&#21512;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#27425;&#20248;&#34892;&#20026;&#65292;&#23548;&#33268;&#26234;&#33021;&#20307;&#25910;&#25947;&#21040;&#27425;&#20248;&#32852;&#21512;&#31574;&#30053;&#65292;&#20986;&#29616;&#20102;&#30456;&#23545;&#36807;&#24230;&#27010;&#25324;&#65288;RO&#65289;&#38382;&#39064;&#12290;&#26089;&#26399;&#30740;&#31350;&#34920;&#26126;&#65292;&#20048;&#35266;&#20027;&#20041;&#21487;&#20197;&#32531;&#35299;&#20351;&#29992;&#34920;&#26684;&#21270;Q&#23398;&#20064;&#26102;&#30340;RO&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22797;&#26434;&#20219;&#21153;&#26469;&#35828;&#65292;&#21033;&#29992;&#20989;&#25968;&#36924;&#36817;&#20048;&#35266;&#20027;&#20041;&#21487;&#33021;&#21152;&#21095;&#36807;&#20272;&#35745;&#65292;&#20174;&#32780;&#22833;&#36133;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#26368;&#36817;&#30340;&#28145;&#24230;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#26799;&#24230;&#65288;MAPG&#65289;&#26041;&#27861;&#22312;&#35768;&#22810;&#22797;&#26434;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22312;&#20005;&#37325;&#30340;RO&#24773;&#20917;&#19979;&#21487;&#33021;&#22833;&#36133;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#32780;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#20197;&#22312;MAPG&#26041;&#27861;&#20013;&#23454;&#29616;&#20048;&#35266;&#26356;&#26032;&#24182;&#32531;&#35299;RO&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#27844;&#28431;&#21270;&#32447;&#24615;&#25972;&#27969;&#20989;&#25968;&#65292;&#20854;&#20013;&#19968;&#20010;&#36229;&#21442;&#25968;&#36873;&#25321;&#20048;&#35266;&#31243;&#24230;&#20197;&#22312;&#26356;&#26032;&#31574;&#30053;&#26102;&#37325;&#26032;&#22609;&#36896;&#20248;&#21183;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#21487;&#33021;&#30001;&#20854;&#20182;&#20195;&#29702;&#24341;&#36215;&#30340;&#22238;&#25253;&#36739;&#20302;&#30340;&#20010;&#21035;&#21160;&#20316;&#20445;&#25345;&#20048;&#35266;&#24577;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
\textit{Relative overgeneralization} (RO) occurs in cooperative multi-agent learning tasks when agents converge towards a suboptimal joint policy due to overfitting to suboptimal behavior of other agents. In early work, optimism has been shown to mitigate the \textit{RO} problem when using tabular Q-learning. However, with function approximation optimism can amplify overestimation and thus fail on complex tasks. On the other hand, recent deep multi-agent policy gradient (MAPG) methods have succeeded in many complex tasks but may fail with severe \textit{RO}. We propose a general, yet simple, framework to enable optimistic updates in MAPG methods and alleviate the RO problem. Specifically, we employ a \textit{Leaky ReLU} function where a single hyperparameter selects the degree of optimism to reshape the advantages when updating the policy. Intuitively, our method remains optimistic toward individual actions with lower returns which are potentially caused by other agents' sub-optimal be
&lt;/p&gt;</description></item></channel></rss>