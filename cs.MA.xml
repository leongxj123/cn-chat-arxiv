<rss version="2.0"><channel><title>Chat Arxiv cs.MA</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.MA</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20165;&#21033;&#29992;&#33258;&#25105;&#20195;&#29702;&#30340;&#26412;&#22320;&#35266;&#27979;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#21462;&#20854;&#20182;&#20195;&#29702;&#30340;&#26377;&#24847;&#20041;&#31574;&#30053;&#34920;&#31034;&#65292;&#20197;&#25913;&#36827;&#33258;&#25105;&#20195;&#29702;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2401.00132</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning-based agent modeling for deep reinforcement learning. (arXiv:2401.00132v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20165;&#21033;&#29992;&#33258;&#25105;&#20195;&#29702;&#30340;&#26412;&#22320;&#35266;&#27979;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#21462;&#20854;&#20182;&#20195;&#29702;&#30340;&#26377;&#24847;&#20041;&#31574;&#30053;&#34920;&#31034;&#65292;&#20197;&#25913;&#36827;&#33258;&#25105;&#20195;&#29702;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#32463;&#24120;&#38656;&#35201;&#20195;&#29702;&#19982;&#20855;&#26377;&#19981;&#21516;&#30446;&#26631;&#12289;&#34892;&#20026;&#25110;&#31574;&#30053;&#30340;&#20854;&#20182;&#20195;&#29702;&#21512;&#20316;&#25110;&#31454;&#20105;&#12290;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#35774;&#35745;&#33258;&#36866;&#24212;&#31574;&#30053;&#26102;&#65292;&#20195;&#29702;&#24314;&#27169;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#22240;&#20026;&#36825;&#26159;&#33258;&#25105;&#20195;&#29702;&#29702;&#35299;&#20854;&#20182;&#20195;&#29702;&#34892;&#20026;&#24182;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#31574;&#30053;&#34920;&#31034;&#30340;&#26041;&#24335;&#12290;&#36825;&#20123;&#34920;&#31034;&#21487;&#20197;&#29992;&#26469;&#22686;&#24378;&#33258;&#25105;&#20195;&#29702;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20195;&#29702;&#24314;&#27169;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#22312;&#35757;&#32451;&#25110;&#38271;&#26102;&#38388;&#35266;&#23519;&#36712;&#36857;&#30340;&#31574;&#30053;&#36866;&#24212;&#36807;&#31243;&#20013;&#21487;&#20197;&#20351;&#29992;&#26469;&#33258;&#20854;&#20182;&#20195;&#29702;&#65288;&#24314;&#27169;&#20195;&#29702;&#65289;&#30340;&#26412;&#22320;&#35266;&#27979;&#12290;&#20026;&#20102;&#28040;&#38500;&#36825;&#20123;&#38480;&#21046;&#24615;&#20551;&#35774;&#24182;&#25552;&#39640;&#20195;&#29702;&#24314;&#27169;&#24615;&#33021;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#20195;&#29702;&#24314;&#27169;&#65288;CLAM&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20165;&#20381;&#36182;&#20110;&#33258;&#25105;&#20195;&#29702;&#22312;&#35757;&#32451;&#21644;&#25191;&#34892;&#36807;&#31243;&#20013;&#30340;&#26412;&#22320;&#35266;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent systems often require agents to collaborate with or compete against other agents with diverse goals, behaviors, or strategies. Agent modeling is essential when designing adaptive policies for intelligent machine agents in multiagent systems, as this is the means by which the ego agent understands other agents' behavior and extracts their meaningful policy representations. These representations can be used to enhance the ego agent's adaptive policy which is trained by reinforcement learning. However, existing agent modeling approaches typically assume the availability of local observations from other agents (modeled agents) during training or a long observation trajectory for policy adaption. To remove these constrictive assumptions and improve agent modeling performance, we devised a Contrastive Learning-based Agent Modeling (CLAM) method that relies only on the local observations from the ego agent during training and execution. With these observations, CLAM is capable of 
&lt;/p&gt;</description></item></channel></rss>