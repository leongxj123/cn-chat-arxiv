<rss version="2.0"><channel><title>Chat Arxiv cs.MA</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.MA</description><item><title>&#36890;&#36807;&#20351;&#29992;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#23558;&#34892;&#20154;&#23454;&#29616;&#20026;MARL&#20195;&#29702;&#65292;&#30740;&#31350;&#20102;&#20182;&#20204;&#23398;&#20064;&#36991;&#35753;&#20854;&#20182;&#20195;&#29702;&#21521;&#21069;&#31227;&#21160;&#30340;&#33021;&#21147;&#65292;&#22312;&#23494;&#24230;&#19981;&#22826;&#39640;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#25104;&#21151;&#12290;</title><link>https://arxiv.org/abs/2312.11834</link><description>&lt;p&gt;
&#20351;&#29992;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21450;&#20854;&#22312;&#34892;&#20154;&#21160;&#24577;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-agent reinforcement learning using echo-state network and its application to pedestrian dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11834
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#23558;&#34892;&#20154;&#23454;&#29616;&#20026;MARL&#20195;&#29702;&#65292;&#30740;&#31350;&#20102;&#20182;&#20204;&#23398;&#20064;&#36991;&#35753;&#20854;&#20182;&#20195;&#29702;&#21521;&#21069;&#31227;&#21160;&#30340;&#33021;&#21147;&#65292;&#22312;&#23494;&#24230;&#19981;&#22826;&#39640;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#27169;&#25311;&#34892;&#20154;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#32593;&#26684;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#36947;&#36335;&#65292;&#24182;&#23558;&#34892;&#20154;&#23454;&#29616;&#20026;&#20351;&#29992;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#21644;&#26368;&#23567;&#20108;&#20056;&#31574;&#30053;&#36845;&#20195;&#26041;&#27861;&#30340;MARL&#20195;&#29702;&#12290;&#22312;&#36825;&#20010;&#29615;&#22659;&#19979;&#65292;&#30740;&#31350;&#20102;&#36825;&#20123;&#20195;&#29702;&#23398;&#20064;&#36991;&#24320;&#20854;&#20182;&#20195;&#29702;&#21521;&#21069;&#31227;&#21160;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#20219;&#21153;&#65306;&#31364;&#30452;&#25509;&#36335;&#24452;&#21644;&#23485;&#32469;&#36947;&#20043;&#38388;&#30340;&#36873;&#25321;&#65292;&#20197;&#21450;&#36208;&#24266;&#20013;&#30340;&#21452;&#21521;&#34892;&#20154;&#27969;&#12290;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#20195;&#29702;&#23494;&#24230;&#19981;&#22826;&#39640;&#26102;&#65292;&#23398;&#20064;&#26159;&#25104;&#21151;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11834v2 Announce Type: replace-cross  Abstract: In recent years, simulations of pedestrians using the multi-agent reinforcement learning (MARL) have been studied. This study considered the roads on a grid-world environment, and implemented pedestrians as MARL agents using an echo-state network and the least squares policy iteration method. Under this environment, the ability of these agents to learn to move forward by avoiding other agents was investigated. Specifically, we considered two types of tasks: the choice between a narrow direct route and a broad detour, and the bidirectional pedestrian flow in a corridor. The simulations results indicated that the learning was successful when the density of the agents was not that high.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31454;&#20105;&#24615;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#21644;&#25511;&#21046;&#38382;&#39064;&#12290;&#36890;&#36807;&#37325;&#26500;&#31169;&#26377;&#31574;&#30053;&#21644;&#39044;&#27979;&#31283;&#24577;&#34892;&#21160;&#37197;&#32622;&#25991;&#20214;&#65292;&#22806;&#37096;&#35266;&#23519;&#32773;&#21487;&#20197;&#25104;&#21151;&#36827;&#34892;&#39044;&#27979;&#21644;&#20248;&#21270;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2212.12561</link><description>&lt;p&gt;
&#35299;&#20915;&#31454;&#20105;&#24615;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#21644;&#25511;&#21046;&#38382;&#39064;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An active learning method for solving competitive multi-agent decision-making and control problems. (arXiv:2212.12561v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12561
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31454;&#20105;&#24615;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#21644;&#25511;&#21046;&#38382;&#39064;&#12290;&#36890;&#36807;&#37325;&#26500;&#31169;&#26377;&#31574;&#30053;&#21644;&#39044;&#27979;&#31283;&#24577;&#34892;&#21160;&#37197;&#32622;&#25991;&#20214;&#65292;&#22806;&#37096;&#35266;&#23519;&#32773;&#21487;&#20197;&#25104;&#21151;&#36827;&#34892;&#39044;&#27979;&#21644;&#20248;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#26696;&#65292;&#29992;&#20110;&#37325;&#26500;&#30001;&#30456;&#20114;&#20316;&#29992;&#20195;&#29702;&#20154;&#32676;&#20307;&#25191;&#34892;&#30340;&#31169;&#26377;&#31574;&#30053;&#65292;&#24182;&#39044;&#27979;&#24213;&#23618;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#36807;&#31243;&#30340;&#30830;&#20999;&#32467;&#26524;&#65292;&#36825;&#37324;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#31283;&#23450;&#30340;&#34892;&#21160;&#37197;&#32622;&#25991;&#20214;&#12290;&#25105;&#20204;&#35774;&#24819;&#20102;&#19968;&#20010;&#22330;&#26223;&#65292;&#22312;&#36825;&#20010;&#22330;&#26223;&#20013;&#65292;&#19968;&#20010;&#20855;&#26377;&#23398;&#20064;&#31243;&#24207;&#30340;&#22806;&#37096;&#35266;&#23519;&#32773;&#21487;&#20197;&#36890;&#36807;&#31169;&#26377;&#30340;&#34892;&#21160;-&#21453;&#24212;&#26144;&#23556;&#36827;&#34892;&#26597;&#35810;&#21644;&#35266;&#23519;&#20195;&#29702;&#20154;&#30340;&#21453;&#24212;&#65292;&#38598;&#20307;&#30340;&#19981;&#21160;&#28857;&#23545;&#24212;&#20110;&#19968;&#20010;&#31283;&#24577;&#37197;&#32622;&#25991;&#20214;&#12290;&#36890;&#36807;&#36845;&#20195;&#22320;&#25910;&#38598;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#21644;&#26356;&#26032;&#34892;&#21160;-&#21453;&#24212;&#26144;&#23556;&#30340;&#21442;&#25968;&#20272;&#35745;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#28176;&#36817;&#24615;&#36136;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#20197;&#20415;&#22914;&#26524;&#25910;&#25947;&#21457;&#29983;&#65292;&#23427;&#21482;&#33021;&#26397;&#21521;&#19968;&#20010;&#31283;&#24577;&#34892;&#21160;&#37197;&#32622;&#25991;&#20214;&#12290;&#36825;&#19968;&#20107;&#23454;&#23548;&#33268;&#20102;&#20004;&#20010;&#20027;&#35201;&#32467;&#26524;&#65306;i&#65289;&#23398;&#20064;&#23616;&#37096;&#31934;&#30830;&#30340;&#34892;&#21160;-&#21453;&#24212;&#26144;&#23556;&#26367;&#20195;&#29289;&#20351;&#24471;&#22806;&#37096;&#35266;&#23519;&#32773;&#33021;&#22815;&#25104;&#21151;&#23436;&#25104;&#20854;&#39044;&#27979;&#20219;&#21153;&#65292;ii&#65289;&#19982;&#20195;&#29702;&#20154;&#30340;&#20114;&#21160;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20248;&#21270;&#31574;&#30053;&#20197;&#36798;&#21040;&#26368;&#20339;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a scheme based on active learning to reconstruct private strategies executed by a population of interacting agents and predict an exact outcome of the underlying multi-agent interaction process, here identified as a stationary action profile. We envision a scenario where an external observer, endowed with a learning procedure, can make queries and observe the agents' reactions through private action-reaction mappings, whose collective fixed point corresponds to a stationary profile. By iteratively collecting sensible data and updating parametric estimates of the action-reaction mappings, we establish sufficient conditions to assess the asymptotic properties of the proposed active learning methodology so that, if convergence happens, it can only be towards a stationary action profile. This fact yields two main consequences: i) learning locally-exact surrogates of the action-reaction mappings allows the external observer to succeed in its prediction task, and ii) working with 
&lt;/p&gt;</description></item></channel></rss>