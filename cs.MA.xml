<rss version="2.0"><channel><title>Chat Arxiv cs.MA</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.MA</description><item><title>AgentMixer&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20801;&#35768;&#26234;&#33021;&#20307;&#36890;&#36807;&#31574;&#30053;&#20462;&#25913;&#26469;&#23454;&#29616;&#21327;&#21516;&#20915;&#31574;&#12290;&#36890;&#36807;&#26500;&#36896;&#32852;&#21512;&#31574;&#30053;&#20026;&#21508;&#20010;&#37096;&#20998;&#31574;&#30053;&#30340;&#38750;&#32447;&#24615;&#32452;&#21512;&#65292;&#21487;&#23454;&#29616;&#37096;&#20998;&#21487;&#35266;&#27979;&#26234;&#33021;&#20307;&#30340;&#31283;&#23450;&#35757;&#32451;&#21644;&#20998;&#25955;&#25191;&#34892;&#12290;</title><link>http://arxiv.org/abs/2401.08728</link><description>&lt;p&gt;
AgentMixer: &#22810;&#26234;&#33021;&#20307;&#30456;&#20851;&#31574;&#30053;&#22240;&#23376;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
AgentMixer: Multi-Agent Correlated Policy Factorization. (arXiv:2401.08728v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08728
&lt;/p&gt;
&lt;p&gt;
AgentMixer&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20801;&#35768;&#26234;&#33021;&#20307;&#36890;&#36807;&#31574;&#30053;&#20462;&#25913;&#26469;&#23454;&#29616;&#21327;&#21516;&#20915;&#31574;&#12290;&#36890;&#36807;&#26500;&#36896;&#32852;&#21512;&#31574;&#30053;&#20026;&#21508;&#20010;&#37096;&#20998;&#31574;&#30053;&#30340;&#38750;&#32447;&#24615;&#32452;&#21512;&#65292;&#21487;&#23454;&#29616;&#37096;&#20998;&#21487;&#35266;&#27979;&#26234;&#33021;&#20307;&#30340;&#31283;&#23450;&#35757;&#32451;&#21644;&#20998;&#25955;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#20013;&#24335;&#35757;&#32451;&#19982;&#20998;&#25955;&#24335;&#25191;&#34892;&#65288;CTDE&#65289;&#24191;&#27867;&#24212;&#29992;&#20110;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21033;&#29992;&#38598;&#20013;&#24335;&#20540;&#20989;&#25968;&#26469;&#31283;&#23450;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#26234;&#33021;&#20307;&#22522;&#20110;&#26412;&#22320;&#35266;&#27979;&#29420;&#31435;&#22320;&#20570;&#20915;&#31574;&#65292;&#36825;&#21487;&#33021;&#19981;&#20250;&#23548;&#33268;&#20855;&#26377;&#36275;&#22815;&#21327;&#35843;&#24615;&#30340;&#30456;&#20851;&#32852;&#30340;&#32852;&#21512;&#31574;&#30053;&#12290;&#21463;&#30456;&#20851;&#22343;&#34913;&#27010;&#24565;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#24341;&#20837;"&#31574;&#30053;&#20462;&#25913;"&#26469;&#20026;&#26234;&#33021;&#20307;&#25552;&#20379;&#21327;&#35843;&#31574;&#30053;&#30340;&#26426;&#21046;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;AgentMixer&#65292;&#23558;&#32852;&#21512;&#23436;&#20840;&#21487;&#35266;&#27979;&#31574;&#30053;&#26500;&#36896;&#20026;&#21508;&#20010;&#37096;&#20998;&#21487;&#35266;&#27979;&#31574;&#30053;&#30340;&#38750;&#32447;&#24615;&#32452;&#21512;&#12290;&#20026;&#20102;&#23454;&#29616;&#20998;&#25955;&#24335;&#25191;&#34892;&#65292;&#21487;&#20197;&#36890;&#36807;&#27169;&#20223;&#32852;&#21512;&#31574;&#30053;&#26469;&#24471;&#21040;&#21508;&#20010;&#37096;&#20998;&#31574;&#30053;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;&#27169;&#20223;&#23398;&#20064;&#21487;&#33021;&#20250;&#23548;&#33268;&#30001;&#20110;&#32852;&#21512;&#31574;&#30053;&#21644;&#20010;&#20307;&#31574;&#30053;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#32780;&#23548;&#33268;&#30340;&#38750;&#23545;&#31216;&#23398;&#20064;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;
Centralized training with decentralized execution (CTDE) is widely employed to stabilize partially observable multi-agent reinforcement learning (MARL) by utilizing a centralized value function during training. However, existing methods typically assume that agents make decisions based on their local observations independently, which may not lead to a correlated joint policy with sufficient coordination. Inspired by the concept of correlated equilibrium, we propose to introduce a \textit{strategy modification} to provide a mechanism for agents to correlate their policies. Specifically, we present a novel framework, AgentMixer, which constructs the joint fully observable policy as a non-linear combination of individual partially observable policies. To enable decentralized execution, one can derive individual policies by imitating the joint policy. Unfortunately, such imitation learning can lead to \textit{asymmetric learning failure} caused by the mismatch between joint policy and indi
&lt;/p&gt;</description></item></channel></rss>