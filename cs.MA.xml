<rss version="2.0"><channel><title>Chat Arxiv cs.MA</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.MA</description><item><title>&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#35752;&#20102;&#19981;&#21516;&#36947;&#24503;&#31867;&#22411;&#30340;&#23398;&#20064;&#20195;&#29702;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#21457;&#29616;&#36947;&#24503;&#24322;&#36136;&#24615;&#21487;&#33021;&#23545;&#20195;&#29702;&#30340;&#20849;&#21516;&#21457;&#23637;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.04202</link><description>&lt;p&gt;
&#24322;&#36136;&#23398;&#20064;&#20195;&#29702;&#32676;&#20307;&#20013;&#36947;&#24503;&#34892;&#20026;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Dynamics of Moral Behavior in Heterogeneous Populations of Learning Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04202
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#35752;&#20102;&#19981;&#21516;&#36947;&#24503;&#31867;&#22411;&#30340;&#23398;&#20064;&#20195;&#29702;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#21457;&#29616;&#36947;&#24503;&#24322;&#36136;&#24615;&#21487;&#33021;&#23545;&#20195;&#29702;&#30340;&#20849;&#21516;&#21457;&#23637;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04202v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#39046;&#22495; &#25688;&#35201;&#65306;&#26085;&#30410;&#20851;&#27880;AI&#31995;&#32479;&#23433;&#20840;&#21644;&#23545;&#40784;&#24615;&#30340;&#38382;&#39064;&#31361;&#26174;&#20102;&#22312;&#20154;&#24037;&#20195;&#29702;&#20013;&#23884;&#20837;&#36947;&#24503;&#33021;&#21147;&#30340;&#37325;&#35201;&#24615;&#12290;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#21033;&#29992;&#32463;&#39564;&#23398;&#20064;&#65292;&#21363;&#24378;&#21270;&#23398;&#20064;&#12290;&#22312;&#22810;&#20195;&#29702;&#65288;&#31038;&#20250;&#65289;&#29615;&#22659;&#20013;&#65292;&#20010;&#20307;&#23398;&#20064;&#20195;&#29702;&#20043;&#38388;&#30340;&#20132;&#20114;&#21487;&#33021;&#20135;&#29983;&#22797;&#26434;&#30340;&#32676;&#20307;&#23618;&#38754;&#29616;&#35937;&#12290;&#35768;&#22810;&#29616;&#26377;&#30740;&#31350;&#20381;&#36182;&#20110;&#27169;&#25311;&#30340;&#31038;&#20250;&#22256;&#22659;&#29615;&#22659;&#26469;&#30740;&#31350;&#29420;&#31435;&#23398;&#20064;&#20195;&#29702;&#30340;&#20114;&#21160;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24448;&#24448;&#24573;&#35270;&#20102;&#23454;&#36341;&#20013;&#20195;&#29702;&#31038;&#20250;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#36947;&#24503;&#24322;&#36136;&#24615;&#12290;&#20363;&#22914;&#65292;&#22312;&#19981;&#21516;&#26102;&#38388;&#28857;&#65292;&#21333;&#20010;&#23398;&#20064;&#20195;&#29702;&#21487;&#33021;&#38754;&#23545;&#21518;&#26524;&#20027;&#20041;&#32773;&#65288;&#21363;&#20851;&#24515;&#38543;&#26102;&#38388;&#26368;&#22823;&#21270;&#26576;&#31181;&#32467;&#26524;&#65289;&#25110;&#22522;&#20110;&#35268;&#33539;&#30340;&#23545;&#25163;&#65288;&#21363;&#19987;&#27880;&#20110;&#31435;&#21363;&#36981;&#23432;&#29305;&#23450;&#35268;&#33539;&#65289; &#12290;&#20195;&#29702;&#30340;&#20849;&#21516;&#21457;&#23637;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#21487;&#33021;&#21463;&#21040;&#36825;&#31181;&#36947;&#24503;&#24322;&#36136;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04202v1 Announce Type: cross  Abstract: Growing concerns about safety and alignment of AI systems highlight the importance of embedding moral capabilities in artificial agents. A promising solution is the use of learning from experience, i.e., Reinforcement Learning. In multi-agent (social) environments, complex population-level phenomena may emerge from interactions between individual learning agents. Many of the existing studies rely on simulated social dilemma environments to study the interactions of independent learning agents. However, they tend to ignore the moral heterogeneity that is likely to be present in societies of agents in practice. For example, at different points in time a single learning agent may face opponents who are consequentialist (i.e., caring about maximizing some outcome over time) or norm-based (i.e., focusing on conforming to a specific norm here and now). The extent to which agents' co-development may be impacted by such moral heterogeneity in 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#20998;&#25955;&#21306;&#22359;&#38142;&#25216;&#26415;&#19982;&#26032;&#39062;&#26426;&#21046;&#32467;&#21512;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#30830;&#20445;&#35802;&#23454;&#21442;&#19982;&#32773;&#33719;&#24471;&#30340;&#32047;&#31215;&#22870;&#21169;&#65292;&#24182;&#24212;&#23545;&#24694;&#24847;&#34892;&#20026;&#21644;&#20445;&#25252;&#21442;&#19982;&#32773;&#38544;&#31169;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.04417</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25955;&#21306;&#22359;&#38142;&#30340;&#31283;&#20581;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Decentralized Blockchain-based Robust Multi-agent Multi-armed Bandit
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#20998;&#25955;&#21306;&#22359;&#38142;&#25216;&#26415;&#19982;&#26032;&#39062;&#26426;&#21046;&#32467;&#21512;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#30830;&#20445;&#35802;&#23454;&#21442;&#19982;&#32773;&#33719;&#24471;&#30340;&#32047;&#31215;&#22870;&#21169;&#65292;&#24182;&#24212;&#23545;&#24694;&#24847;&#34892;&#20026;&#21644;&#20445;&#25252;&#21442;&#19982;&#32773;&#38544;&#31169;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#22810;&#20010;&#23458;&#25143;&#25110;&#21442;&#19982;&#32773;&#20998;&#24067;&#22312;&#19968;&#20010;&#23436;&#20840;&#20998;&#25955;&#30340;&#21306;&#22359;&#38142;&#19978;&#65292;&#20854;&#20013;&#19968;&#20123;&#21487;&#33021;&#26159;&#24694;&#24847;&#30340;&#12290;&#33218;&#30340;&#22870;&#21169;&#22312;&#21442;&#19982;&#32773;&#20043;&#38388;&#26159;&#22343;&#21248;&#30340;&#65292;&#36981;&#24490;&#26102;&#38388;&#19981;&#21464;&#30340;&#38543;&#26426;&#20998;&#24067;&#65292;&#21482;&#26377;&#22312;&#31995;&#32479;&#36275;&#22815;&#23433;&#20840;&#26102;&#25165;&#21521;&#21442;&#19982;&#32773;&#36879;&#38706;&#12290;&#31995;&#32479;&#30340;&#30446;&#26631;&#26159;&#26377;&#25928;&#22320;&#30830;&#20445;&#35802;&#23454;&#21442;&#19982;&#32773;&#33719;&#24471;&#30340;&#32047;&#31215;&#22870;&#21169;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#23558;&#21306;&#22359;&#38142;&#30340;&#20808;&#36827;&#25216;&#26415;&#21644;&#26032;&#39062;&#30340;&#26426;&#21046;&#32467;&#21512;&#21040;&#31995;&#32479;&#20013;&#65292;&#20026;&#35802;&#23454;&#21442;&#19982;&#32773;&#35774;&#35745;&#26368;&#20339;&#31574;&#30053;&#12290;&#36825;&#26679;&#21487;&#20197;&#24212;&#23545;&#21508;&#31181;&#24694;&#24847;&#34892;&#20026;&#24182;&#20445;&#25252;&#21442;&#19982;&#32773;&#30340;&#38544;&#31169;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#38543;&#26426;&#36873;&#25321;&#20102;&#19968;&#32452;&#21487;&#20197;&#35775;&#38382;&#25152;&#26377;&#21442;&#19982;&#32773;&#30340;&#39564;&#35777;&#32773;&#27744;&#65292;&#20026;&#36825;&#20123;&#39564;&#35777;&#32773;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#23383;&#31614;&#21517;&#30340;&#20840;&#26032;&#20849;&#35782;&#26426;&#21046;&#65292;&#24182;&#21457;&#26126;&#20102;&#19968;&#31181;&#22522;&#20110;UCB&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a robust multi-agent multi-armed bandit problem where multiple clients or participants are distributed on a fully decentralized blockchain, with the possibility of some being malicious. The rewards of arms are homogeneous among the clients, following time-invariant stochastic distributions that are revealed to the participants only when the system is secure enough. The system's objective is to efficiently ensure the cumulative rewards gained by the honest participants. To this end and to the best of our knowledge, we are the first to incorporate advanced techniques from blockchains, as well as novel mechanisms, into the system to design optimal strategies for honest participants. This allows various malicious behaviors and the maintenance of participant privacy. More specifically, we randomly select a pool of validators who have access to all participants, design a brand-new consensus mechanism based on digital signatures for these validators, invent a UCB-based strategy that 
&lt;/p&gt;</description></item></channel></rss>