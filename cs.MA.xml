<rss version="2.0"><channel><title>Chat Arxiv cs.MA</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.MA</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27169;&#22359;&#21270;&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#20943;&#23569;&#20102;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#35757;&#32451;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#24182;&#25104;&#21151;&#39044;&#27979;&#20102;&#32852;&#21512;&#21160;&#20316;&#20215;&#20540;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.04615</link><description>&lt;p&gt;
&#21033;&#29992;&#19990;&#30028;&#27169;&#22411;&#20998;&#35299;&#22312;&#22522;&#20110;&#20540;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Leveraging World Model Disentanglement in Value-Based Multi-Agent Reinforcement Learning. (arXiv:2309.04615v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04615
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27169;&#22359;&#21270;&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#20943;&#23569;&#20102;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#35757;&#32451;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#24182;&#25104;&#21151;&#39044;&#27979;&#20102;&#32852;&#21512;&#21160;&#20316;&#20215;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21517;&#20026;Value Decomposition Framework with Disentangled World Model&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#30456;&#21516;&#29615;&#22659;&#20013;&#22810;&#20010;&#26234;&#33021;&#20307;&#36798;&#25104;&#20849;&#21516;&#30446;&#26631;&#26102;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;&#30001;&#20110;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#38750;&#24179;&#31283;&#24615;&#38382;&#39064;&#65292;&#26080;&#27169;&#22411;&#26041;&#27861;&#20381;&#36182;&#20110;&#22823;&#37327;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#12290;&#30456;&#21453;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#27169;&#22359;&#21270;&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#21253;&#25324;&#21160;&#20316;&#26465;&#20214;&#12289;&#26080;&#21160;&#20316;&#21644;&#38745;&#24577;&#20998;&#25903;&#65292;&#26469;&#35299;&#24320;&#29615;&#22659;&#21160;&#24577;&#24182;&#26681;&#25454;&#36807;&#21435;&#30340;&#32463;&#39564;&#20135;&#29983;&#24819;&#35937;&#20013;&#30340;&#32467;&#26524;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#20174;&#30495;&#23454;&#29615;&#22659;&#20013;&#37319;&#26679;&#12290;&#25105;&#20204;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#21464;&#20998;&#22270;&#33258;&#21160;&#32534;&#30721;&#22120;&#26469;&#23398;&#20064;&#19990;&#30028;&#27169;&#22411;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#23558;&#20854;&#19982;&#22522;&#20110;&#20540;&#30340;&#26694;&#26550;&#21512;&#24182;&#65292;&#20197;&#39044;&#27979;&#32852;&#21512;&#21160;&#20316;&#20215;&#20540;&#20989;&#25968;&#24182;&#20248;&#21270;&#25972;&#20307;&#35757;&#32451;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20379;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel model-based multi-agent reinforcement learning approach named Value Decomposition Framework with Disentangled World Model to address the challenge of achieving a common goal of multiple agents interacting in the same environment with reduced sample complexity. Due to scalability and non-stationarity problems posed by multi-agent systems, model-free methods rely on a considerable number of samples for training. In contrast, we use a modularized world model, composed of action-conditioned, action-free, and static branches, to unravel the environment dynamics and produce imagined outcomes based on past experience, without sampling directly from the real environment. We employ variational auto-encoders and variational graph auto-encoders to learn the latent representations for the world model, which is merged with a value-based framework to predict the joint action-value function and optimize the overall training objective. We present experimental results 
&lt;/p&gt;</description></item></channel></rss>