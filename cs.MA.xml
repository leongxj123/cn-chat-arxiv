<rss version="2.0"><channel><title>Chat Arxiv cs.MA</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.MA</description><item><title>&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36171;&#20104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;Agent&#32676;&#20307;&#20869;&#31038;&#20250;&#35268;&#33539;&#20986;&#29616;&#30340;&#29983;&#25104;&#24335;Agent&#26550;&#26500;CRSEC&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.08251</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;Agent&#31038;&#20250;&#20013;&#31038;&#20250;&#35268;&#33539;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Emergence of Social Norms in Large Language Model-based Agent Societies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08251
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36171;&#20104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;Agent&#32676;&#20307;&#20869;&#31038;&#20250;&#35268;&#33539;&#20986;&#29616;&#30340;&#29983;&#25104;&#24335;Agent&#26550;&#26500;CRSEC&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#35268;&#33539;&#30340;&#20986;&#29616;&#21560;&#24341;&#20102;&#31038;&#20250;&#31185;&#23398;&#12289;&#35748;&#30693;&#31185;&#23398;&#20197;&#21450;&#20154;&#24037;&#26234;&#33021;&#31561;&#21508;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36171;&#20104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;Agent&#32676;&#20307;&#20869;&#31038;&#20250;&#35268;&#33539;&#20986;&#29616;&#30340;&#29983;&#25104;&#24335;Agent&#26550;&#26500;CRSEC&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#21253;&#25324;&#22235;&#20010;&#27169;&#22359;&#65306;Creation &amp; Representation&#12289;Spreading&#12289;Evaluation&#21644;Compliance&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#22788;&#29702;&#20102;&#20960;&#20010;&#20851;&#38190;&#26041;&#38754;&#30340;&#32039;&#24613;&#36807;&#31243;&#65306;(i)&#31038;&#20250;&#35268;&#33539;&#30340;&#26469;&#28304;&#65292;(ii)&#23427;&#20204;&#22914;&#20309;&#34987;&#27491;&#24335;&#34920;&#31034;&#65292;(iii)&#23427;&#20204;&#22914;&#20309;&#36890;&#36807;Agent&#30340;&#20132;&#27969;&#21644;&#35266;&#23519;&#20256;&#25773;&#65292;(iv)&#22914;&#20309;&#36890;&#36807;&#21512;&#29702;&#26816;&#26597;&#36827;&#34892;&#26816;&#26597;&#24182;&#22312;&#38271;&#26399;&#20869;&#36827;&#34892;&#32508;&#21512;&#65292;(v)&#22914;&#20309;&#34987;&#32435;&#20837;Agent&#30340;&#35745;&#21010;&#21644;&#34892;&#21160;&#20013;&#12290;&#25105;&#20204;&#22312;Smallville&#27801;&#30418;&#28216;&#25103;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26550;&#26500;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08251v1 Announce Type: cross  Abstract: The emergence of social norms has attracted much interest in a wide array of disciplines, ranging from social science and cognitive science to artificial intelligence. In this paper, we propose the first generative agent architecture that empowers the emergence of social norms within a population of large language model-based agents. Our architecture, named CRSEC, consists of four modules: Creation &amp; Representation, Spreading, Evaluation, and Compliance. Our architecture addresses several important aspects of the emergent processes all in one: (i) where social norms come from, (ii) how they are formally represented, (iii) how they spread through agents' communications and observations, (iv) how they are examined with a sanity check and synthesized in the long term, and (v) how they are incorporated into agents' planning and actions. Our experiments deployed in the Smallville sandbox game environment demonstrate the capability of our ar
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#22810;&#26234;&#33021;&#20307;&#35268;&#21010;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#28041;&#21450;&#19981;&#21516;&#31867;&#22411;&#26234;&#33021;&#20307;&#30340;&#38382;&#39064;&#65292;&#27604;&#29616;&#26377;&#31639;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2301.01246</link><description>&lt;p&gt;
&#21551;&#21457;&#24335;&#22810;&#26234;&#33021;&#20307;&#35268;&#21010;&#20248;&#21270;&#26234;&#33021;&#20307;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Optimizing Agent Collaboration through Heuristic Multi-Agent Planning. (arXiv:2301.01246v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01246
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#22810;&#26234;&#33021;&#20307;&#35268;&#21010;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#28041;&#21450;&#19981;&#21516;&#31867;&#22411;&#26234;&#33021;&#20307;&#30340;&#38382;&#39064;&#65292;&#27604;&#29616;&#26377;&#31639;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#28041;&#21450;&#21040;&#19981;&#21516;&#31867;&#22411;&#24863;&#30693;&#26234;&#33021;&#20307;&#30340;&#38382;&#39064;&#65292;&#30446;&#21069;&#35299;&#20915;QDec-POMDP&#30340;SOTA&#31639;&#27861;QDec-FP&#21644;QDec-FPS&#26080;&#27861;&#26377;&#25928;&#35299;&#20915;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#35201;&#27714;&#26234;&#33021;&#20307;&#37319;&#21462;&#30456;&#21516;&#30340;&#35745;&#21010;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#27604;QDec-FP&#21644;QDec-FPS&#37117;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The SOTA algorithms for addressing QDec-POMDP issues, QDec-FP and QDec-FPS, are unable to effectively tackle problems that involve different types of sensing agents. We propose a new algorithm that addresses this issue by requiring agents to adopt the same plan if one agent is unable to take a sensing action but the other can. Our algorithm performs significantly better than both QDec-FP and QDec-FPS in these types of situations.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20179;&#24211;&#29289;&#27969;&#20013;&#30340;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#21516;&#20107;&#21512;&#20316;&#12290;&#20182;&#20204;&#36890;&#36807;&#20998;&#23618;&#30340;MARL&#31639;&#27861;&#65292;&#35753;&#32463;&#29702;&#21644;&#24037;&#20154;&#20195;&#29702;&#26681;&#25454;&#20840;&#23616;&#30446;&#26631;&#36827;&#34892;&#21327;&#21516;&#35757;&#32451;&#65292;&#20197;&#26368;&#22823;&#21270;&#25315;&#36135;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2212.11498</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#20179;&#24211;&#29289;&#27969;&#20013;&#19982;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#21516;&#20107;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Scalable Multi-Agent Reinforcement Learning for Warehouse Logistics with Robotic and Human Co-Workers. (arXiv:2212.11498v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11498
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20179;&#24211;&#29289;&#27969;&#20013;&#30340;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#21516;&#20107;&#21512;&#20316;&#12290;&#20182;&#20204;&#36890;&#36807;&#20998;&#23618;&#30340;MARL&#31639;&#27861;&#65292;&#35753;&#32463;&#29702;&#21644;&#24037;&#20154;&#20195;&#29702;&#26681;&#25454;&#20840;&#23616;&#30446;&#26631;&#36827;&#34892;&#21327;&#21516;&#35757;&#32451;&#65292;&#20197;&#26368;&#22823;&#21270;&#25315;&#36135;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#24819;&#19968;&#20010;&#20179;&#24211;&#37324;&#26377;&#25968;&#21313;&#20010;&#31227;&#21160;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#20998;&#25315;&#21592;&#19968;&#36215;&#24037;&#20316;&#65292;&#25910;&#38598;&#21644;&#20132;&#20184;&#20179;&#24211;&#20869;&#30340;&#29289;&#21697;&#12290;&#25105;&#20204;&#35201;&#35299;&#20915;&#30340;&#22522;&#26412;&#38382;&#39064;&#26159;&#31216;&#20026;&#25315;&#36135;&#38382;&#39064;&#65292;&#21363;&#36825;&#20123;&#24037;&#20316;&#20195;&#29702;&#20154;&#22914;&#20309;&#22312;&#20179;&#24211;&#20013;&#21327;&#35843;&#20182;&#20204;&#30340;&#31227;&#21160;&#21644;&#34892;&#20026;&#20197;&#26368;&#22823;&#21270;&#24615;&#33021;&#65288;&#20363;&#22914;&#35746;&#21333;&#21534;&#21520;&#37327;&#65289;&#12290;&#20256;&#32479;&#30340;&#34892;&#19994;&#26041;&#27861;&#20351;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#24037;&#31243;&#21162;&#21147;&#26469;&#20026;&#22266;&#26377;&#21487;&#21464;&#30340;&#20179;&#24211;&#37197;&#32622;&#36827;&#34892;&#20248;&#21270;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#21487;&#20197;&#28789;&#27963;&#22320;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#20179;&#24211;&#37197;&#32622;&#65288;&#20363;&#22914;&#22823;&#23567;&#65292;&#24067;&#23616;&#65292;&#24037;&#20154;&#25968;&#37327;/&#31867;&#22411;&#65292;&#29289;&#21697;&#34917;&#20805;&#39057;&#29575;&#65289;&#65292;&#22240;&#20026;&#20195;&#29702;&#20154;&#36890;&#36807;&#32463;&#39564;&#23398;&#20064;&#22914;&#20309;&#26368;&#20248;&#22320;&#30456;&#20114;&#21512;&#20316;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20998;&#23618;MARL&#31639;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;&#31649;&#29702;&#32773;&#20026;&#24037;&#20154;&#20195;&#29702;&#20998;&#37197;&#30446;&#26631;&#65292;&#24182;&#19988;&#31649;&#29702;&#32773;&#21644;&#24037;&#20154;&#30340;&#31574;&#30053;&#34987;&#20849;&#21516;&#35757;&#32451;&#20197;&#26368;&#22823;&#21270;&#20840;&#23616;&#30446;&#26631;&#65288;&#20363;&#22914;&#25315;&#36135;&#36895;&#29575;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We envision a warehouse in which dozens of mobile robots and human pickers work together to collect and deliver items within the warehouse. The fundamental problem we tackle, called the order-picking problem, is how these worker agents must coordinate their movement and actions in the warehouse to maximise performance (e.g. order throughput). Established industry methods using heuristic approaches require large engineering efforts to optimise for innately variable warehouse configurations. In contrast, multi-agent reinforcement learning (MARL) can be flexibly applied to diverse warehouse configurations (e.g. size, layout, number/types of workers, item replenishment frequency), as the agents learn through experience how to optimally cooperate with one another. We develop hierarchical MARL algorithms in which a manager assigns goals to worker agents, and the policies of the manager and workers are co-trained toward maximising a global objective (e.g. pick rate). Our hierarchical algorith
&lt;/p&gt;</description></item></channel></rss>