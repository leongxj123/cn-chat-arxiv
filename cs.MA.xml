<rss version="2.0"><channel><title>Chat Arxiv cs.MA</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.MA</description><item><title>&#25552;&#20986;&#20102;Human-Regularized PPO (HR-PPO)&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#25105;&#21338;&#24328;&#35757;&#32451;&#20195;&#29702;&#65292;&#23454;&#29616;&#22312;&#23553;&#38381;&#29615;&#22659;&#20013;&#36924;&#30495;&#19988;&#26377;&#25928;&#30340;&#39550;&#39542;&#20249;&#20276;</title><link>https://arxiv.org/abs/2403.19648</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#27491;&#21017;&#21270;&#30340;&#33258;&#25105;&#21338;&#24328;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#19982;&#20154;&#31867;&#20860;&#23481;&#30340;&#39550;&#39542;&#20249;&#20276;
&lt;/p&gt;
&lt;p&gt;
Human-compatible driving partners through data-regularized self-play reinforcement learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19648
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Human-Regularized PPO (HR-PPO)&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#25105;&#21338;&#24328;&#35757;&#32451;&#20195;&#29702;&#65292;&#23454;&#29616;&#22312;&#23553;&#38381;&#29615;&#22659;&#20013;&#36924;&#30495;&#19988;&#26377;&#25928;&#30340;&#39550;&#39542;&#20249;&#20276;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#27773;&#36710;&#38754;&#20020;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#26159;&#19982;&#20154;&#31867;&#36827;&#34892;&#21327;&#35843;&#12290;&#22240;&#27492;&#65292;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#65292;&#23558;&#36924;&#30495;&#30340;&#20154;&#31867;&#20195;&#29702;&#32435;&#20837;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#21487;&#25193;&#23637;&#35757;&#32451;&#21644;&#35780;&#20272;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Human-Regularized PPO (HR-PPO)&#30340;&#22810;&#26234;&#33021;&#20307;&#31639;&#27861;&#65292;&#20854;&#20013;&#20195;&#29702;&#36890;&#36807;&#33258;&#25105;&#21338;&#24328;&#36827;&#34892;&#35757;&#32451;&#65292;&#23545;&#20559;&#31163;&#20154;&#31867;&#21442;&#32771;&#31574;&#30053;&#30340;&#34892;&#20026;&#36827;&#34892;&#23567;&#24133;&#24809;&#32602;&#65292;&#20197;&#26500;&#24314;&#22312;&#23553;&#38381;&#29615;&#22659;&#20013;&#26082;&#36924;&#30495;&#21448;&#26377;&#25928;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19648v1 Announce Type: cross  Abstract: A central challenge for autonomous vehicles is coordinating with humans. Therefore, incorporating realistic human agents is essential for scalable training and evaluation of autonomous driving systems in simulation. Simulation agents are typically developed by imitating large-scale, high-quality datasets of human driving. However, pure imitation learning agents empirically have high collision rates when executed in a multi-agent closed-loop setting. To build agents that are realistic and effective in closed-loop settings, we propose Human-Regularized PPO (HR-PPO), a multi-agent algorithm where agents are trained through self-play with a small penalty for deviating from a human reference policy. In contrast to prior work, our approach is RL-first and only uses 30 minutes of imperfect human demonstrations. We evaluate agents in a large set of multi-agent traffic scenes. Results show our HR-PPO agents are highly effective in achieving goa
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#29983;&#25104;AAS&#23454;&#20363;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#25968;&#23383;&#23402;&#29983;&#20013;&#30340;&#20114;&#25805;&#20316;&#24615;&#65292;&#38477;&#20302;&#20102;&#25163;&#21160;&#21019;&#24314;&#25104;&#26412;&#21644;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.17209</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#29983;&#25104;&#36164;&#20135;&#31649;&#29702;&#22806;&#22771;&#65306;&#25968;&#23383;&#23402;&#29983;&#21644;&#35821;&#20041;&#33410;&#28857;&#20013;&#30340;&#20114;&#25805;&#20316;&#24615;
&lt;/p&gt;
&lt;p&gt;
Generation of Asset Administration Shell with Large Language Model Agents: Interoperability in Digital Twins with Semantic Node
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17209
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#29983;&#25104;AAS&#23454;&#20363;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#25968;&#23383;&#23402;&#29983;&#20013;&#30340;&#20114;&#25805;&#20316;&#24615;&#65292;&#38477;&#20302;&#20102;&#25163;&#21160;&#21019;&#24314;&#25104;&#26412;&#21644;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21327;&#21161;&#22312;&#24037;&#19994;4.0&#32972;&#26223;&#19979;&#20026;&#25968;&#23383;&#23402;&#29983;&#24314;&#27169;&#21019;&#24314;&#36164;&#20135;&#31649;&#29702;&#22806;&#22771;&#65288;AAS&#65289;&#23454;&#20363;&#65292;&#26088;&#22312;&#22686;&#24378;&#26234;&#33021;&#21046;&#36896;&#20013;&#30340;&#20114;&#25805;&#20316;&#24615;&#65292;&#20943;&#23569;&#25163;&#21160;&#24037;&#20316;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#8220;&#35821;&#20041;&#33410;&#28857;&#8221;&#25968;&#25454;&#32467;&#26500;&#26469;&#25429;&#25417;&#25991;&#26412;&#25968;&#25454;&#30340;&#35821;&#20041;&#35201;&#20041;&#12290;&#28982;&#21518;&#65292;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#22788;&#29702;&#8220;&#35821;&#20041;&#33410;&#28857;&#8221;&#24182;&#20174;&#25991;&#26412;&#25216;&#26415;&#25968;&#25454;&#29983;&#25104;AAS&#23454;&#20363;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#26377;&#25928;&#29983;&#25104;&#29575;&#20026;62-79%&#65292;&#34920;&#26126;&#30456;&#24403;&#27604;&#20363;&#30340;&#25163;&#21160;&#21019;&#24314;&#24037;&#20316;&#21487;&#20197;&#36716;&#25442;&#20026;&#26356;&#23481;&#26131;&#30340;&#39564;&#35777;&#24037;&#20316;&#65292;&#20174;&#32780;&#20943;&#23569;&#21019;&#24314;AAS&#23454;&#20363;&#27169;&#22411;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#12290;&#22312;&#25105;&#20204;&#30340;&#35780;&#20272;&#20013;&#65292;&#23545;&#19981;&#21516;LLM&#30340;&#27604;&#36739;&#20998;&#26512;&#20197;&#21450;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26426;&#21046;&#30340;&#28145;&#20837;&#28040;&#34701;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20851;LLM&#26377;&#25928;&#24615;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17209v1 Announce Type: new  Abstract: This research introduces a novel approach for assisting the creation of Asset Administration Shell (AAS) instances for digital twin modeling within the context of Industry 4.0, aiming to enhance interoperability in smart manufacturing and reduce manual effort. We construct a "semantic node" data structure to capture the semantic essence of textual data. Then, a system powered by large language models is designed and implemented to process "semantic node" and generate AAS instance models from textual technical data. Our evaluation demonstrates a 62-79% effective generation rate, indicating a substantial proportion of manual creation effort can be converted into easier validation effort, thereby reducing the time and cost in creating AAS instance models. In our evaluation, a comparative analysis of different LLMs and an in-depth ablation study of Retrieval-Augmented Generation (RAG) mechanisms provide insights into the effectiveness of LLM
&lt;/p&gt;</description></item></channel></rss>