<rss version="2.0"><channel><title>Chat Arxiv cs.MA</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.MA</description><item><title>&#25552;&#20986;&#20102;&#33258;&#20027;&#24335;MARL&#65288;SPMARL&#65289;&#20197;&#35299;&#20915;&#24403;&#21069;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#35838;&#31243;&#29983;&#25104;&#30340;&#38382;&#39064;&#65292;&#20248;&#20808;&#32771;&#34385;&#22522;&#20110;&#20219;&#21153;&#30340;&#20248;&#20808;&#32423;&#12290;</title><link>https://arxiv.org/abs/2205.10016</link><description>&lt;p&gt;
&#23398;&#20064;&#36827;&#24230;&#39537;&#21160;&#30340;&#22810;&#26234;&#33021;&#20307;&#35838;&#31243;
&lt;/p&gt;
&lt;p&gt;
Learning Progress Driven Multi-Agent Curriculum
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.10016
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#33258;&#20027;&#24335;MARL&#65288;SPMARL&#65289;&#20197;&#35299;&#20915;&#24403;&#21069;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#35838;&#31243;&#29983;&#25104;&#30340;&#38382;&#39064;&#65292;&#20248;&#20808;&#32771;&#34385;&#22522;&#20110;&#20219;&#21153;&#30340;&#20248;&#20808;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#65288;CRL&#65289;&#26088;&#22312;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;&#20219;&#21153;&#30340;&#38590;&#24230;&#65288;&#36890;&#24120;&#30001;&#21487;&#23454;&#29616;&#30340;&#39044;&#26399;&#22238;&#25253;&#37327;&#21270;&#65289;&#26469;&#21152;&#24555;&#23398;&#20064;&#36895;&#24230;&#12290;&#21463;CRL&#22312;&#21333;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#19968;&#20123;&#30740;&#31350;&#23581;&#35797;&#23558;CRL&#24212;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#65292;&#20351;&#29992;&#26234;&#33021;&#20307;&#25968;&#37327;&#26469;&#25511;&#21046;&#20219;&#21153;&#38590;&#24230;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#20351;&#29992;&#25163;&#21160;&#23450;&#20041;&#30340;&#35838;&#31243;&#65292;&#22914;&#32447;&#24615;&#26041;&#26696;&#12290;&#26412;&#25991;&#39318;&#20808;&#23558;&#26368;&#20808;&#36827;&#30340;&#21333;&#26234;&#33021;&#20307;&#33258;&#20027;&#24335;CRL&#24212;&#29992;&#20110;&#31232;&#30095;&#22870;&#21169;MARL&#12290;&#34429;&#28982;&#34920;&#29616;&#20196;&#20154;&#28385;&#24847;&#65292;&#20294;&#25105;&#20204;&#30830;&#23450;&#20102;&#29616;&#26377;&#22522;&#20110;&#22870;&#21169;&#30340;CRL&#26041;&#27861;&#29983;&#25104;&#30340;&#35838;&#31243;&#23384;&#22312;&#20004;&#20010;&#28508;&#22312;&#32570;&#38519;&#65306;&#65288;1&#65289;&#39640;&#22238;&#25253;&#30340;&#20219;&#21153;&#21487;&#33021;&#19981;&#25552;&#20379;&#20449;&#24687;&#37327;&#22823;&#30340;&#23398;&#20064;&#20449;&#21495;&#65292;&#65288;2&#65289;&#22312;&#22810;&#26234;&#33021;&#20307;&#20135;&#29983;&#26356;&#39640;&#22238;&#25253;&#30340;&#20219;&#21153;&#20013;&#65292;&#21152;&#21095;&#20102;&#23398;&#20998;&#20998;&#37197;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#33258;&#20027;&#24335;MARL&#65288;SPMARL&#65289;&#65292;&#20197;&#22522;&#20110;&#20219;&#21153;&#30340;&#20248;&#20808;&#32423;&#36827;&#34892;&#23433;&#25490;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2205.10016v2 Announce Type: replace  Abstract: Curriculum reinforcement learning (CRL) aims to speed up learning by gradually increasing the difficulty of a task, usually quantified by the achievable expected return. Inspired by the success of CRL in single-agent settings, a few works have attempted to apply CRL to multi-agent reinforcement learning (MARL) using the number of agents to control task difficulty. However, existing works typically use manually defined curricula such as a linear scheme. In this paper, we first apply state-of-the-art single-agent self-paced CRL to sparse reward MARL. Although with satisfying performance, we identify two potential flaws of the curriculum generated by existing reward-based CRL methods: (1) tasks with high returns may not provide informative learning signals and (2) the exacerbated credit assignment difficulty in tasks where more agents yield higher returns. Thereby, we further propose self-paced MARL (SPMARL) to prioritize tasks based on
&lt;/p&gt;</description></item></channel></rss>