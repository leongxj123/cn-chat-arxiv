<rss version="2.0"><channel><title>Chat Arxiv cs.MA</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.MA</description><item><title>&#25552;&#20986;&#20102;&#20010;&#24615;&#21270;&#19987;&#23478;&#31034;&#33539;&#30340;&#27010;&#24565;&#65292;&#20026;&#27599;&#20010;&#26234;&#20307;&#25110;&#19981;&#21516;&#31867;&#22411;&#30340;&#26234;&#20307;&#25552;&#20379;&#38024;&#23545;&#20010;&#20154;&#30446;&#26631;&#30340;&#25351;&#23548;&#65292;&#35299;&#20915;&#20102;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#32852;&#21512;&#31034;&#33539;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.08936</link><description>&lt;p&gt;
&#36229;&#36234;&#32852;&#21512;&#31034;&#33539;&#65306;&#20010;&#24615;&#21270;&#19987;&#23478;&#25351;&#23548;&#29992;&#20110;&#39640;&#25928;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Beyond Joint Demonstrations: Personalized Expert Guidance for Efficient Multi-Agent Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08936
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20010;&#24615;&#21270;&#19987;&#23478;&#31034;&#33539;&#30340;&#27010;&#24565;&#65292;&#20026;&#27599;&#20010;&#26234;&#20307;&#25110;&#19981;&#21516;&#31867;&#22411;&#30340;&#26234;&#20307;&#25552;&#20379;&#38024;&#23545;&#20010;&#20154;&#30446;&#26631;&#30340;&#25351;&#23548;&#65292;&#35299;&#20915;&#20102;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#32852;&#21512;&#31034;&#33539;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#38754;&#20020;&#26377;&#25928;&#25506;&#32034;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#32852;&#21512;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#30340;&#22823;&#23567;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#34429;&#28982;&#31034;&#33539;&#24341;&#23548;&#23398;&#20064;&#22312;&#21333;&#26234;&#20307;&#29615;&#22659;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#30410;&#30340;&#65292;&#20294;&#20854;&#30452;&#25509;&#24212;&#29992;&#20110;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#21463;&#21040;&#33719;&#24471;&#32852;&#21512;&#19987;&#23478;&#31034;&#33539;&#30340;&#23454;&#38469;&#22256;&#38590;&#30340;&#38459;&#30861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20010;&#24615;&#21270;&#19987;&#23478;&#31034;&#33539;&#30340;&#26032;&#27010;&#24565;&#65292;&#38024;&#23545;&#27599;&#20010;&#21333;&#20010;&#26234;&#20307;&#25110;&#26356;&#24191;&#27867;&#22320;&#35828;&#65292;&#22242;&#38431;&#20013;&#27599;&#31181;&#31867;&#22411;&#30340;&#26234;&#20307;&#36827;&#34892;&#20102;&#23450;&#21046;&#12290;&#36825;&#20123;&#31034;&#33539;&#20165;&#28041;&#21450;&#21333;&#26234;&#20307;&#34892;&#20026;&#20197;&#21450;&#27599;&#20010;&#26234;&#20307;&#22914;&#20309;&#23454;&#29616;&#20010;&#20154;&#30446;&#26631;&#65292;&#32780;&#19981;&#28041;&#21450;&#20219;&#20309;&#21512;&#20316;&#20803;&#32032;&#65292;&#22240;&#27492;&#30450;&#30446;&#27169;&#20223;&#23427;&#20204;&#19981;&#20250;&#23454;&#29616;&#21512;&#20316;&#30001;&#20110;&#28508;&#22312;&#20914;&#31361;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36873;&#25321;&#24615;&#22320;&#21033;&#29992;&#20010;&#24615;&#21270;&#19987;&#23478;&#31034;&#33539;&#20316;&#20026;&#25351;&#23548;&#65292;&#24182;&#20801;&#35768;&#26234;&#20307;&#23398;&#20064;&#21327;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08936v1 Announce Type: cross  Abstract: Multi-Agent Reinforcement Learning (MARL) algorithms face the challenge of efficient exploration due to the exponential increase in the size of the joint state-action space. While demonstration-guided learning has proven beneficial in single-agent settings, its direct applicability to MARL is hindered by the practical difficulty of obtaining joint expert demonstrations. In this work, we introduce a novel concept of personalized expert demonstrations, tailored for each individual agent or, more broadly, each individual type of agent within a heterogeneous team. These demonstrations solely pertain to single-agent behaviors and how each agent can achieve personal goals without encompassing any cooperative elements, thus naively imitating them will not achieve cooperation due to potential conflicts. To this end, we propose an approach that selectively utilizes personalized expert demonstrations as guidance and allows agents to learn to coo
&lt;/p&gt;</description></item></channel></rss>