<rss version="2.0"><channel><title>Chat Arxiv cs.MA</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.MA</description><item><title>&#26412;&#25991;&#20174;&#22343;&#22330;&#35270;&#35282;&#30740;&#31350;&#20102;&#29420;&#31435;&#24378;&#21270;&#23398;&#20064;&#22312;&#21512;&#20316;&#31454;&#20105;&#20195;&#29702;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#23454;&#29616;&#32435;&#20160;&#22343;&#34913;&#30340;&#32447;&#24615;&#20108;&#27425;&#32467;&#26500;RL&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#26080;&#38480;&#20195;&#29702;&#25968;&#37327;&#30340;&#24773;&#20917;&#26469;&#35299;&#20915;&#26377;&#38480;&#20154;&#21475;&#29615;&#22659;&#20013;&#30340;&#38750;&#31283;&#24577;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.11345</link><description>&lt;p&gt;
&#29420;&#31435;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#21512;&#20316;&#31454;&#20105;Agent&#65306;&#22343;&#22330;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Independent RL for Cooperative-Competitive Agents: A Mean-Field Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#22343;&#22330;&#35270;&#35282;&#30740;&#31350;&#20102;&#29420;&#31435;&#24378;&#21270;&#23398;&#20064;&#22312;&#21512;&#20316;&#31454;&#20105;&#20195;&#29702;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#23454;&#29616;&#32435;&#20160;&#22343;&#34913;&#30340;&#32447;&#24615;&#20108;&#27425;&#32467;&#26500;RL&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#26080;&#38480;&#20195;&#29702;&#25968;&#37327;&#30340;&#24773;&#20917;&#26469;&#35299;&#20915;&#26377;&#38480;&#20154;&#21475;&#29615;&#22659;&#20013;&#30340;&#38750;&#31283;&#24577;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20998;&#25104;&#22242;&#38431;&#30340;&#20195;&#29702;&#20043;&#38388;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#27599;&#20010;&#22242;&#38431;&#20869;&#37096;&#23384;&#22312;&#21512;&#20316;&#65292;&#20294;&#19981;&#21516;&#22242;&#38431;&#20043;&#38388;&#23384;&#22312;&#38750;&#38646;&#21644;&#30340;&#31454;&#20105;&#12290;&#20026;&#20102;&#24320;&#21457;&#19968;&#31181;&#21487;&#20197;&#26126;&#30830;&#23454;&#29616;&#32435;&#20160;&#22343;&#34913;&#30340;RL&#26041;&#27861;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#32447;&#24615;&#20108;&#27425;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#26377;&#38480;&#20154;&#21475;&#29615;&#22659;&#20013;&#30001;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#24341;&#36215;&#30340;&#38750;&#31283;&#24577;&#24615;&#65292;&#25105;&#20204;&#32771;&#34385;&#27599;&#20010;&#22242;&#38431;&#20869;&#20195;&#29702;&#25968;&#37327;&#26080;&#38480;&#30340;&#24773;&#20917;&#65292;&#21363;&#22343;&#22330;&#35774;&#32622;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#24191;&#20041;&#21644;&#30340;LQ&#22343;&#22330;&#31867;&#22411;&#21338;&#24328;&#65288;GS-MFTGs&#65289;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#36870;&#21487;&#36870;&#26465;&#20214;&#19979;&#34920;&#24449;&#20102;GS-MFTG&#30340;&#32435;&#20160;&#22343;&#34913;&#65288;NE&#65289;&#12290;&#28982;&#21518;&#35777;&#26126;&#20102;&#36825;&#20010;MFTG NE&#22312;&#26377;&#38480;&#20154;&#21475;&#21338;&#24328;&#20013;&#20026;$\mathcal{O}(1/M)$-NE&#65292;&#20854;&#20013;$M$&#26159;&#27599;&#20010;&#22242;&#38431;&#20013;&#20195;&#29702;&#25968;&#37327;&#30340;&#19979;&#30028;&#12290;&#36825;&#20123;&#32467;&#26500;&#24615;&#32467;&#26524;&#25512;&#21160;&#20102;&#19968;&#20010;&#21517;&#20026;&#22810;&#29609;&#23478;&#36882;&#36827;&#24335;&#33258;&#28982;Pol&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11345v1 Announce Type: cross  Abstract: We address in this paper Reinforcement Learning (RL) among agents that are grouped into teams such that there is cooperation within each team but general-sum (non-zero sum) competition across different teams. To develop an RL method that provably achieves a Nash equilibrium, we focus on a linear-quadratic structure. Moreover, to tackle the non-stationarity induced by multi-agent interactions in the finite population setting, we consider the case where the number of agents within each team is infinite, i.e., the mean-field setting. This results in a General-Sum LQ Mean-Field Type Game (GS-MFTGs). We characterize the Nash equilibrium (NE) of the GS-MFTG, under a standard invertibility condition. This MFTG NE is then shown to be $\mathcal{O}(1/M)$-NE for the finite population game where $M$ is a lower bound on the number of agents in each team. These structural results motivate an algorithm called Multi-player Receding-horizon Natural Pol
&lt;/p&gt;</description></item></channel></rss>