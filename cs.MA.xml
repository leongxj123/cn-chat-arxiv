<rss version="2.0"><channel><title>Chat Arxiv cs.MA</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.MA</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25317;&#22581;&#23450;&#20215;&#26041;&#26696;&#65292;&#26088;&#22312;&#21516;&#26102;&#20943;&#23569;&#20132;&#36890;&#25317;&#22581;&#27700;&#24179;&#21644;&#32553;&#23567;&#19981;&#21516;&#26053;&#34892;&#32773;&#20043;&#38388;&#30340;&#25104;&#26412;&#24046;&#24322;&#65292;&#20174;&#32780;&#25552;&#39640;&#25928;&#29575;&#21644;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.16844</link><description>&lt;p&gt;
&#29992;&#20110;&#25928;&#29575;&#21644;&#20844;&#24179;&#24615;&#30340;&#25317;&#22581;&#23450;&#20215;&#65306;&#29702;&#35770;&#21450;&#20854;&#22312;&#26087;&#37329;&#23665;&#28286;&#21306;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Congestion Pricing for Efficiency and Equity: Theory and Applications to the San Francisco Bay Area. (arXiv:2401.16844v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25317;&#22581;&#23450;&#20215;&#26041;&#26696;&#65292;&#26088;&#22312;&#21516;&#26102;&#20943;&#23569;&#20132;&#36890;&#25317;&#22581;&#27700;&#24179;&#21644;&#32553;&#23567;&#19981;&#21516;&#26053;&#34892;&#32773;&#20043;&#38388;&#30340;&#25104;&#26412;&#24046;&#24322;&#65292;&#20174;&#32780;&#25552;&#39640;&#25928;&#29575;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25317;&#22581;&#23450;&#20215;&#34987;&#35768;&#22810;&#22478;&#24066;&#29992;&#20110;&#32531;&#35299;&#20132;&#36890;&#25317;&#22581;&#65292;&#20294;&#30001;&#20110;&#23545;&#20302;&#25910;&#20837;&#26053;&#34892;&#32773;&#24433;&#21709;&#36739;&#22823;&#65292;&#24341;&#21457;&#20102;&#20851;&#20110;&#31038;&#20250;&#32463;&#27982;&#24046;&#36317;&#25193;&#22823;&#30340;&#25285;&#24551;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25317;&#22581;&#23450;&#20215;&#26041;&#26696;&#65292;&#19981;&#20165;&#21487;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#20132;&#36890;&#25317;&#22581;&#65292;&#36824;&#21487;&#20197;&#23558;&#20844;&#24179;&#24615;&#30446;&#26631;&#32435;&#20837;&#20854;&#20013;&#65292;&#20197;&#20943;&#23569;&#19981;&#21516;&#25903;&#20184;&#24847;&#24895;&#30340;&#26053;&#34892;&#32773;&#20043;&#38388;&#30340;&#25104;&#26412;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#22522;&#20110;&#19968;&#20010;&#20855;&#26377;&#24322;&#36136;&#26053;&#34892;&#32773;&#32676;&#20307;&#30340;&#25317;&#22581;&#21338;&#24328;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#31181;&#32771;&#34385;&#23454;&#38469;&#22240;&#32032;&#30340;&#23450;&#20215;&#26041;&#26696;&#65292;&#20363;&#22914;&#23545;&#19981;&#21516;&#26053;&#34892;&#32773;&#32676;&#20307;&#25910;&#21462;&#24046;&#24322;&#21270;&#30340;&#36890;&#34892;&#36153;&#20197;&#21450;&#24449;&#25910;&#25972;&#20010;&#36335;&#32593;&#20013;&#30340;&#25152;&#26377;&#36793;&#25110;&#21482;&#24449;&#25910;&#20854;&#20013;&#19968;&#37096;&#20998;&#36793;&#30340;&#36873;&#25321;&#12290;&#25105;&#20204;&#22312;&#26087;&#37329;&#23665;&#28286;&#21306;&#30340;&#26657;&#20934;&#39640;&#36895;&#20844;&#36335;&#32593;&#32476;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#23450;&#20215;&#26041;&#26696;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25317;&#22581;&#23450;&#20215;&#26041;&#26696;&#21487;&#20197;&#25552;&#39640;&#25928;&#29575;&#65288;&#21363;&#20943;&#23569;&#24179;&#22343;&#26053;&#34892;&#26102;&#38388;&#65289;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Congestion pricing, while adopted by many cities to alleviate traffic congestion, raises concerns about widening socioeconomic disparities due to its disproportionate impact on low-income travelers. In this study, we address this concern by proposing a new class of congestion pricing schemes that not only minimize congestion levels but also incorporate an equity objective to reduce cost disparities among travelers with different willingness-to-pay. Our analysis builds on a congestion game model with heterogeneous traveler populations. We present four pricing schemes that account for practical considerations, such as the ability to charge differentiated tolls to various traveler populations and the option to toll all or only a subset of edges in the network. We evaluate our pricing schemes in the calibrated freeway network of the San Francisco Bay Area. We demonstrate that the proposed congestion pricing schemes improve both efficiency (in terms of reduced average travel time) and equit
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30456;&#23545;&#36807;&#24230;&#27867;&#21270;&#30340;&#35838;&#31243;&#23398;&#20064;&#65288;CURO&#65289;&#30340;&#26032;&#31639;&#27861;&#26469;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#30456;&#23545;&#36807;&#24230;&#27867;&#21270; (RO) &#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#35299;&#20915;&#23637;&#31034;&#24378;RO&#30340;&#21512;&#20316;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2212.02733</link><description>&lt;p&gt;
&#30456;&#23545;&#36807;&#24230;&#27867;&#21270;&#30340;&#35838;&#31243;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Curriculum Learning for Relative Overgeneralization. (arXiv:2212.02733v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30456;&#23545;&#36807;&#24230;&#27867;&#21270;&#30340;&#35838;&#31243;&#23398;&#20064;&#65288;CURO&#65289;&#30340;&#26032;&#31639;&#27861;&#26469;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#30456;&#23545;&#36807;&#24230;&#27867;&#21270; (RO) &#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#35299;&#20915;&#23637;&#31034;&#24378;RO&#30340;&#21512;&#20316;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064; (MARL) &#20013;&#65292;&#35768;&#22810;&#27969;&#34892;&#26041;&#27861;&#22914; VDN &#21644; QMIX&#65292;&#37117;&#23481;&#26131;&#21463;&#21040;&#30456;&#23545;&#36807;&#24230;&#27867;&#21270; (RO) &#36825;&#19968;&#20851;&#38190;&#24615;&#30340;&#22810;&#26234;&#33021;&#20307;&#30149;&#29702;&#30340;&#24433;&#21709;&#12290;&#24403;&#21512;&#20316;&#20219;&#21153;&#20013;&#26368;&#20339;&#32852;&#21512;&#34892;&#21160;&#30340;&#25928;&#29992;&#20302;&#20110;&#27425;&#20248;&#32852;&#21512;&#34892;&#21160;&#26102;&#65292;&#23601;&#20250;&#20986;&#29616;RO&#12290;RO&#21487;&#33021;&#23548;&#33268;&#26234;&#33021;&#20307;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;&#25110;&#26080;&#27861;&#35299;&#20915;&#38656;&#35201;&#26234;&#33021;&#20307;&#20043;&#38388;&#22312;&#32473;&#23450;&#26102;&#38388;&#27493;&#38271;&#20869;&#36827;&#34892;&#22823;&#37327;&#21327;&#35843;&#30340;&#21512;&#20316;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#22522;&#20110;&#20215;&#20540;&#30340;MARL&#31639;&#27861;&#65292;&#22914;QPLEX&#21644;WQMIX&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20811;&#26381;RO&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#20204;&#20173;&#28982;&#26080;&#27861;&#35299;&#20915;&#23637;&#31034;&#24378;RO&#30340;&#21512;&#20316;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#30456;&#23545;&#36807;&#24230;&#27867;&#21270;&#30340;&#35838;&#31243;&#23398;&#20064;&#65288;CURO&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#20811;&#26381;RO&#12290;&#22312;CURO&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24494;&#35843;&#30446;&#26631;&#20219;&#21153;&#30340;&#22870;&#21169;&#20989;&#25968;&#20197;&#29983;&#25104;&#36866;&#21512;&#24403;&#21069;&#33021;&#21147;&#30340;&#28304;&#20219;&#21153;&#26469;&#35299;&#20915;&#23637;&#31034;&#24378;RO&#30340;&#30446;&#26631;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In multi-agent reinforcement learning (MARL), many popular methods, such as VDN and QMIX, are susceptible to a critical multi-agent pathology known as relative overgeneralization (RO), which arises when the optimal joint action's utility falls below that of a sub-optimal joint action in cooperative tasks. RO can cause the agents to get stuck into local optima or fail to solve cooperative tasks that require significant coordination between agents within a given timestep. Recent value-based MARL algorithms such as QPLEX and WQMIX can overcome RO to some extent. However, our experimental results show that they can still fail to solve cooperative tasks that exhibit strong RO. In this work, we propose a novel approach called curriculum learning for relative overgeneralization (CURO) to better overcome RO. To solve a target task that exhibits strong RO, in CURO, we first fine-tune the reward function of the target task to generate source tasks that are tailored to the current ability of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;D3G&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#22810;&#26426;&#22120;&#20154;&#21327;&#35843;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;&#36712;&#36857;&#19982;&#28436;&#31034;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65292;&#27599;&#20010;&#26426;&#22120;&#20154;&#21487;&#20197;&#33258;&#21160;&#35843;&#25972;&#20854;&#20010;&#20307;&#21160;&#24577;&#21644;&#30446;&#26631;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2207.08892</link><description>&lt;p&gt;
D3G: &#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#22810;&#26426;&#22120;&#20154;&#21327;&#35843;
&lt;/p&gt;
&lt;p&gt;
D3G: Learning Multi-robot Coordination from Demonstrations. (arXiv:2207.08892v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.08892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;D3G&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#22810;&#26426;&#22120;&#20154;&#21327;&#35843;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;&#36712;&#36857;&#19982;&#28436;&#31034;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65292;&#27599;&#20010;&#26426;&#22120;&#20154;&#21487;&#20197;&#33258;&#21160;&#35843;&#25972;&#20854;&#20010;&#20307;&#21160;&#24577;&#21644;&#30446;&#26631;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#21487;&#24494;&#21160;&#24577;&#28216;&#25103;&#65288;D3G&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#22810;&#26426;&#22120;&#20154;&#21327;&#35843;&#12290;&#25105;&#20204;&#23558;&#22810;&#26426;&#22120;&#20154;&#21327;&#35843;&#34920;&#31034;&#20026;&#19968;&#20010;&#21160;&#24577;&#28216;&#25103;&#65292;&#20854;&#20013;&#19968;&#20010;&#26426;&#22120;&#20154;&#30340;&#34892;&#20026;&#21463;&#20854;&#33258;&#36523;&#21160;&#24577;&#21644;&#30446;&#26631;&#30340;&#25511;&#21046;&#65292;&#21516;&#26102;&#20063;&#21462;&#20915;&#20110;&#20854;&#20182;&#26426;&#22120;&#20154;&#30340;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#35843;&#25972;&#27599;&#20010;&#26426;&#22120;&#20154;&#30340;&#30446;&#26631;&#21644;&#21160;&#24577;&#65292;&#21487;&#20197;&#36866;&#24212;&#21327;&#35843;&#12290;&#25152;&#25552;&#20986;&#30340;D3G&#20351;&#27599;&#20010;&#26426;&#22120;&#20154;&#36890;&#36807;&#26368;&#23567;&#21270;&#20854;&#36712;&#36857;&#19982;&#28436;&#31034;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65292;&#22312;&#20998;&#24067;&#24335;&#26041;&#24335;&#19979;&#33258;&#21160;&#35843;&#25972;&#20854;&#20010;&#20307;&#21160;&#24577;&#21644;&#30446;&#26631;&#12290;&#35813;&#23398;&#20064;&#26694;&#26550;&#20855;&#26377;&#26032;&#30340;&#35774;&#35745;&#65292;&#21253;&#25324;&#19968;&#20010;&#21069;&#21521;&#20256;&#36882;&#65292;&#25152;&#26377;&#26426;&#22120;&#20154;&#21512;&#20316;&#23547;&#25214;&#28216;&#25103;&#30340;&#32435;&#20160;&#22343;&#34913;&#65292;&#20197;&#21450;&#19968;&#20010;&#21453;&#21521;&#20256;&#36882;&#65292;&#22312;&#36890;&#20449;&#22270;&#20013;&#20256;&#25773;&#26799;&#24230;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#20013;&#27979;&#35797;&#20102;D3G&#65292;&#24182;&#32473;&#20986;&#20102;&#19981;&#21516;&#20219;&#21153;&#37197;&#32622;&#30340;&#20004;&#31181;&#26426;&#22120;&#20154;&#12290;&#32467;&#26524;&#35777;&#26126;&#20102;D3G&#23398;&#20064;&#22810;&#26426;&#22120;&#20154;&#21327;&#35843;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper develops a Distributed Differentiable Dynamic Game (D3G) framework, which enables learning multi-robot coordination from demonstrations. We represent multi-robot coordination as a dynamic game, where the behavior of a robot is dictated by its own dynamics and objective that also depends on others' behavior. The coordination thus can be adapted by tuning the objective and dynamics of each robot. The proposed D3G enables each robot to automatically tune its individual dynamics and objectives in a distributed manner by minimizing the mismatch between its trajectory and demonstrations. This learning framework features a new design, including a forward-pass, where all robots collaboratively seek Nash equilibrium of a game, and a backward-pass, where gradients are propagated via the communication graph. We test the D3G in simulation with two types of robots given different task configurations. The results validate the capability of D3G for learning multi-robot coordination from de
&lt;/p&gt;</description></item></channel></rss>