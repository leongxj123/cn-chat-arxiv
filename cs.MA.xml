<rss version="2.0"><channel><title>Chat Arxiv cs.MA</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.MA</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20808;&#22825;&#20215;&#20540;&#39537;&#21160;&#22686;&#24378;&#23398;&#20064;&#65288;IVRL&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#22810;&#26234;&#33021;&#20307;&#22312;&#21512;&#20316;&#20013;&#30340;&#22797;&#26434;&#34892;&#20026;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24314;&#31435;&#26234;&#33021;&#20307;&#23545;&#32676;&#20307;&#25928;&#29992;&#21644;&#31995;&#32479;&#25104;&#26412;&#30340;&#35748;&#30693;&#65292;&#28385;&#36275;&#20854;&#21512;&#20316;&#20249;&#20276;&#30340;&#38656;&#27714;&#65292;&#25903;&#25345;&#20854;&#31038;&#21306;&#24182;&#34701;&#20837;&#20154;&#31867;&#31038;&#20250;&#12290;</title><link>http://arxiv.org/abs/2401.05572</link><description>&lt;p&gt;
&#29992;&#20110;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#20808;&#22825;&#20215;&#20540;&#39537;&#21160;&#22686;&#24378;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Innate-Values-driven Reinforcement Learning for Cooperative Multi-Agent Systems. (arXiv:2401.05572v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20808;&#22825;&#20215;&#20540;&#39537;&#21160;&#22686;&#24378;&#23398;&#20064;&#65288;IVRL&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#22810;&#26234;&#33021;&#20307;&#22312;&#21512;&#20316;&#20013;&#30340;&#22797;&#26434;&#34892;&#20026;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24314;&#31435;&#26234;&#33021;&#20307;&#23545;&#32676;&#20307;&#25928;&#29992;&#21644;&#31995;&#32479;&#25104;&#26412;&#30340;&#35748;&#30693;&#65292;&#28385;&#36275;&#20854;&#21512;&#20316;&#20249;&#20276;&#30340;&#38656;&#27714;&#65292;&#25903;&#25345;&#20854;&#31038;&#21306;&#24182;&#34701;&#20837;&#20154;&#31867;&#31038;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#22825;&#20215;&#20540;&#25551;&#36848;&#20102;&#26234;&#33021;&#20307;&#30340;&#20869;&#22312;&#21160;&#26426;&#65292;&#21453;&#26144;&#20102;&#20182;&#20204;&#36861;&#27714;&#30446;&#26631;&#21644;&#21457;&#23637;&#22810;&#26679;&#25216;&#33021;&#20197;&#28385;&#36275;&#21508;&#31181;&#38656;&#27714;&#30340;&#22266;&#26377;&#20852;&#36259;&#21644;&#20559;&#22909;&#12290;&#24378;&#21270;&#23398;&#20064;&#30340;&#26412;&#36136;&#26159;&#22522;&#20110;&#22870;&#21169;&#39537;&#21160;&#65288;&#22914;&#25928;&#29992;&#65289;&#30340;&#34892;&#20026;&#20114;&#21160;&#23398;&#20064;&#65292;&#31867;&#20284;&#20110;&#33258;&#28982;&#26234;&#33021;&#20307;&#12290;&#29305;&#21035;&#26159;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#65292;&#24314;&#31435;&#26234;&#33021;&#20307;&#23545;&#24179;&#34913;&#32676;&#20307;&#25928;&#29992;&#21644;&#31995;&#32479;&#25104;&#26412;&#30340;&#35748;&#30693;&#65292;&#28385;&#36275;&#32676;&#20307;&#25104;&#21592;&#22312;&#21512;&#20316;&#20013;&#30340;&#38656;&#27714;&#65292;&#26159;&#20010;&#20307;&#20026;&#25903;&#25345;&#20854;&#31038;&#21306;&#21644;&#34701;&#20837;&#20154;&#31867;&#31038;&#20250;&#32780;&#23398;&#20064;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#22797;&#21512;&#20869;&#22312;&#20215;&#20540;&#22686;&#24378;&#23398;&#20064;&#27169;&#22411; - &#20808;&#22825;&#20215;&#20540;&#39537;&#21160;&#22686;&#24378;&#23398;&#20064;&#65292;&#29992;&#20110;&#25551;&#36848;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#20013;&#22797;&#26434;&#30340;&#20114;&#21160;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Innate values describe agents' intrinsic motivations, which reflect their inherent interests and preferences to pursue goals and drive them to develop diverse skills satisfying their various needs. The essence of reinforcement learning (RL) is learning from interaction based on reward-driven (such as utilities) behaviors, much like natural agents. It is an excellent model to describe the innate-values-driven (IV) behaviors of AI agents. Especially in multi-agent systems (MAS), building the awareness of AI agents to balance the group utilities and system costs and satisfy group members' needs in their cooperation is a crucial problem for individuals learning to support their community and integrate human society in the long term. This paper proposes a hierarchical compound intrinsic value reinforcement learning model -innate-values-driven reinforcement learning termed IVRL to describe the complex behaviors of multi-agent interaction in their cooperation. We implement the IVRL architec
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23450;&#24615;&#19987;&#23478;&#30693;&#35782;&#30340;&#37327;&#21270;&#20195;&#29702;&#27169;&#22411;&#24320;&#21457;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#23558;&#23450;&#24615;&#25968;&#25454;&#32763;&#35793;&#25104;&#23450;&#37327;&#35268;&#21017;&#65292;&#20026;&#27169;&#22411;&#26500;&#24314;&#32773;&#21644;&#39046;&#22495;&#19987;&#23478;&#25552;&#20379;&#20102;&#19968;&#20010;&#31995;&#32479;&#21644;&#36879;&#26126;&#30340;&#24314;&#27169;&#36807;&#31243;&#12290;&#20197;&#19968;&#20010;&#26377;&#32452;&#32455;&#29359;&#32618;&#30340;&#24212;&#29992;&#26696;&#20363;&#20026;&#20363;&#65292;&#28436;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.00505</link><description>&lt;p&gt;
&#22522;&#20110;&#23450;&#24615;&#19987;&#23478;&#30693;&#35782;&#30340;&#37327;&#21270;&#20195;&#29702;&#27169;&#22411;&#24320;&#21457;&#26694;&#26550;&#65306;&#19968;&#20010;&#26377;&#32452;&#32455;&#29359;&#32618;&#30340;&#24212;&#29992;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Framework for developing quantitative agent based models based on qualitative expert knowledge: an organised crime use-case. (arXiv:2308.00505v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00505
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23450;&#24615;&#19987;&#23478;&#30693;&#35782;&#30340;&#37327;&#21270;&#20195;&#29702;&#27169;&#22411;&#24320;&#21457;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#23558;&#23450;&#24615;&#25968;&#25454;&#32763;&#35793;&#25104;&#23450;&#37327;&#35268;&#21017;&#65292;&#20026;&#27169;&#22411;&#26500;&#24314;&#32773;&#21644;&#39046;&#22495;&#19987;&#23478;&#25552;&#20379;&#20102;&#19968;&#20010;&#31995;&#32479;&#21644;&#36879;&#26126;&#30340;&#24314;&#27169;&#36807;&#31243;&#12290;&#20197;&#19968;&#20010;&#26377;&#32452;&#32455;&#29359;&#32618;&#30340;&#24212;&#29992;&#26696;&#20363;&#20026;&#20363;&#65292;&#28436;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23545;&#25191;&#27861;&#30446;&#30340;&#24314;&#27169;&#29359;&#32618;&#32593;&#32476;&#65292;&#38656;&#35201;&#23558;&#26377;&#38480;&#30340;&#25968;&#25454;&#36716;&#21270;&#20026;&#32463;&#36807;&#39564;&#35777;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#27169;&#22411;&#12290;&#24403;&#21069;&#21009;&#20107;&#23398;&#24314;&#27169;&#20013;&#32570;&#23569;&#19968;&#20010;&#20026;&#27169;&#22411;&#26500;&#24314;&#32773;&#21644;&#39046;&#22495;&#19987;&#23478;&#25552;&#20379;&#31995;&#32479;&#21644;&#36879;&#26126;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24314;&#31435;&#20102;&#35745;&#31639;&#29359;&#32618;&#24314;&#27169;&#30340;&#24314;&#27169;&#36807;&#31243;&#65292;&#21253;&#25324;&#23558;&#23450;&#24615;&#25968;&#25454;&#36716;&#21270;&#20026;&#23450;&#37327;&#35268;&#21017;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FREIDA&#65288;&#22522;&#20110;&#19987;&#23478;&#30693;&#35782;&#39537;&#21160;&#30340;&#25968;&#25454;&#39537;&#21160;&#20195;&#29702;&#27169;&#22411;&#26694;&#26550;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#29359;&#32618;&#21487;&#21345;&#22240;&#26367;&#20195;&#27169;&#22411;&#65288;CCRM&#65289;&#23558;&#20316;&#20026;&#31034;&#20363;&#26696;&#20363;&#65292;&#20197;&#28436;&#31034;FREIDA&#26041;&#27861;&#12290;&#23545;&#20110;CCRM&#65292;&#27491;&#22312;&#24314;&#27169;&#33655;&#20848;&#30340;&#19968;&#20010;&#26377;&#32452;&#32455;&#21487;&#21345;&#22240;&#32593;&#32476;&#65292;&#35797;&#22270;&#36890;&#36807;&#31227;&#38500;&#39318;&#33041;&#33410;&#28857;&#65292;&#20351;&#21097;&#20313;&#20195;&#29702;&#37325;&#26032;&#32452;&#32455;&#65292;&#24182;&#23558;&#32593;&#32476;&#24674;&#22797;&#21040;&#31283;&#23450;&#29366;&#24577;&#12290;&#23450;&#24615;&#25968;&#25454;&#28304;&#65292;&#20363;&#22914;&#26696;&#20214;&#25991;&#20214;&#65292;&#25991;&#29486;&#21644;&#37319;&#35775;&#65292;&#34987;&#36716;&#21270;&#20026;&#32463;&#39564;&#27861;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to model criminal networks for law enforcement purposes, a limited supply of data needs to be translated into validated agent-based models. What is missing in current criminological modelling is a systematic and transparent framework for modelers and domain experts that establishes a modelling procedure for computational criminal modelling that includes translating qualitative data into quantitative rules. For this, we propose FREIDA (Framework for Expert-Informed Data-driven Agent-based models). Throughout the paper, the criminal cocaine replacement model (CCRM) will be used as an example case to demonstrate the FREIDA methodology. For the CCRM, a criminal cocaine network in the Netherlands is being modelled where the kingpin node is being removed, the goal being for the remaining agents to reorganize after the disruption and return the network into a stable state. Qualitative data sources such as case files, literature and interviews are translated into empirical laws, and c
&lt;/p&gt;</description></item></channel></rss>