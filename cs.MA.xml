<rss version="2.0"><channel><title>Chat Arxiv cs.MA</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.MA</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#26500;&#25104;&#30340;&#20195;&#29702;&#26694;&#26550;TrustAgent&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#39044;&#20808;&#35268;&#21010;&#12289;&#35268;&#21010;&#36807;&#31243;&#20013;&#21644;&#35745;&#21010;&#21518;&#26816;&#26597;&#19977;&#31181;&#31574;&#30053;&#26469;&#25552;&#39640;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#21644;&#39044;&#38450;&#28508;&#22312;&#21361;&#38505;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#23433;&#20840;&#24615;&#19982;&#20351;&#29992;&#32773;&#28385;&#24847;&#24230;&#20197;&#21450;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#19982;&#25928;&#29575;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01586</link><description>&lt;p&gt;
TrustAgent: &#36890;&#36807;&#20195;&#29702;&#26500;&#25104;&#23454;&#29616;&#23433;&#20840;&#21487;&#20449;&#36182;&#30340;LLM&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent Constitution
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#26500;&#25104;&#30340;&#20195;&#29702;&#26694;&#26550;TrustAgent&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#39044;&#20808;&#35268;&#21010;&#12289;&#35268;&#21010;&#36807;&#31243;&#20013;&#21644;&#35745;&#21010;&#21518;&#26816;&#26597;&#19977;&#31181;&#31574;&#30053;&#26469;&#25552;&#39640;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#21644;&#39044;&#38450;&#28508;&#22312;&#21361;&#38505;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#23433;&#20840;&#24615;&#19982;&#20351;&#29992;&#32773;&#28385;&#24847;&#24230;&#20197;&#21450;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#19982;&#25928;&#29575;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#20854;&#21487;&#20449;&#24230;&#20173;&#26410;&#24471;&#21040;&#28145;&#20837;&#25506;&#32034;&#12290;&#30001;&#20110;&#20195;&#29702;&#21487;&#20197;&#30452;&#25509;&#19982;&#29289;&#29702;&#29615;&#22659;&#20132;&#20114;&#65292;&#20854;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#26500;&#25104;&#30340;&#20195;&#29702;&#26694;&#26550;TrustAgent&#65292;&#23545;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#32500;&#24230;&#36827;&#34892;&#20102;&#21021;&#27493;&#30740;&#31350;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19977;&#31181;&#31574;&#30053;&#65306;&#39044;&#20808;&#35268;&#21010;&#31574;&#30053;&#65292;&#22312;&#29983;&#25104;&#35745;&#21010;&#20043;&#21069;&#21521;&#27169;&#22411;&#27880;&#20837;&#23433;&#20840;&#30693;&#35782;&#65307;&#35268;&#21010;&#36807;&#31243;&#20013;&#31574;&#30053;&#65292;&#22312;&#29983;&#25104;&#35745;&#21010;&#26102;&#22686;&#24378;&#23433;&#20840;&#24615;&#65307;&#35745;&#21010;&#21518;&#26816;&#26597;&#31574;&#30053;&#65292;&#36890;&#36807;&#35745;&#21010;&#21518;&#26816;&#26597;&#30830;&#20445;&#23433;&#20840;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22914;&#20309;&#36890;&#36807;&#35782;&#21035;&#21644;&#39044;&#38450;&#28508;&#22312;&#21361;&#38505;&#26377;&#25928;&#25552;&#39640;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#23433;&#20840;&#24615;&#19982;&#20351;&#29992;&#32773;&#28385;&#24847;&#24230;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#20197;&#21450;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#19982;&#20854;&#25928;&#29575;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of LLM-based agents has garnered considerable attention, yet their trustworthiness remains an under-explored area. As agents can directly interact with the physical environment, their reliability and safety is critical. This paper presents an Agent-Constitution-based agent framework, TrustAgent, an initial investigation into improving the safety dimension of trustworthiness in LLM-based agents. This framework consists of threefold strategies: pre-planning strategy which injects safety knowledge to the model prior to plan generation, in-planning strategy which bolsters safety during plan generation, and post-planning strategy which ensures safety by post-planning inspection. Through experimental analysis, we demonstrate how these approaches can effectively elevate an LLM agent's safety by identifying and preventing potential dangers. Furthermore, we explore the intricate relationships between safety and helpfulness, and between the model's reasoning ability and its efficac
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MACE&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#21516;&#25506;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20849;&#20139;&#23616;&#37096;&#26032;&#39062;&#24615;&#26469;&#36817;&#20284;&#20840;&#23616;&#26032;&#39062;&#24615;&#65292;&#24182;&#24341;&#20837;&#21152;&#26435;&#20114;&#20449;&#24687;&#26469;&#34913;&#37327;&#26234;&#33021;&#20307;&#34892;&#21160;&#23545;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20419;&#36827;&#22810;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#21516;&#25506;&#32034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MACE&#22312;&#31232;&#30095;&#22870;&#21169;&#30340;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02097</link><description>&lt;p&gt;
&#21033;&#29992;&#26032;&#39062;&#24615;&#20849;&#20139;&#35299;&#20915;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#21327;&#21516;&#25506;&#32034;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Settling Decentralized Multi-Agent Coordinated Exploration by Novelty Sharing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02097
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MACE&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#21516;&#25506;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20849;&#20139;&#23616;&#37096;&#26032;&#39062;&#24615;&#26469;&#36817;&#20284;&#20840;&#23616;&#26032;&#39062;&#24615;&#65292;&#24182;&#24341;&#20837;&#21152;&#26435;&#20114;&#20449;&#24687;&#26469;&#34913;&#37327;&#26234;&#33021;&#20307;&#34892;&#21160;&#23545;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20419;&#36827;&#22810;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#21516;&#25506;&#32034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MACE&#22312;&#31232;&#30095;&#22870;&#21169;&#30340;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;&#24335;&#21327;&#20316;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#12290;&#19968;&#26159;&#20840;&#23616;&#29366;&#24577;&#30340;&#26032;&#39062;&#24615;&#19981;&#21487;&#29992;&#65292;&#32780;&#23616;&#37096;&#35266;&#23519;&#30340;&#26032;&#39062;&#24615;&#23384;&#22312;&#20559;&#24046;&#12290;&#21478;&#19968;&#20010;&#25361;&#25112;&#26159;&#26234;&#33021;&#20307;&#22914;&#20309;&#21327;&#35843;&#22320;&#36827;&#34892;&#25506;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#21516;&#25506;&#32034;&#26041;&#27861;MACE&#12290;&#36890;&#36807;&#20165;&#20256;&#25773;&#23616;&#37096;&#26032;&#39062;&#24615;&#65292;&#26234;&#33021;&#20307;&#21487;&#20197;&#32771;&#34385;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#23616;&#37096;&#26032;&#39062;&#24615;&#26469;&#36817;&#20284;&#20840;&#23616;&#26032;&#39062;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26032;&#24341;&#20837;&#20102;&#21152;&#26435;&#20114;&#20449;&#24687;&#26469;&#34913;&#37327;&#19968;&#20010;&#26234;&#33021;&#20307;&#30340;&#34892;&#21160;&#23545;&#20854;&#20182;&#26234;&#33021;&#20307;&#32047;&#35745;&#26032;&#39062;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#20854;&#20316;&#20026;&#20869;&#22312;&#22238;&#25253;&#26469;&#40723;&#21169;&#26234;&#33021;&#20307;&#23545;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#25506;&#32034;&#20135;&#29983;&#26356;&#22823;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20419;&#36827;&#21327;&#21516;&#25506;&#32034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MACE&#22312;&#19977;&#31181;&#31232;&#30095;&#22870;&#21169;&#30340;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploration in decentralized cooperative multi-agent reinforcement learning faces two challenges. One is that the novelty of global states is unavailable, while the novelty of local observations is biased. The other is how agents can explore in a coordinated way. To address these challenges, we propose MACE, a simple yet effective multi-agent coordinated exploration method. By communicating only local novelty, agents can take into account other agents' local novelty to approximate the global novelty. Further, we newly introduce weighted mutual information to measure the influence of one agent's action on other agents' accumulated novelty. We convert it as an intrinsic reward in hindsight to encourage agents to exert more influence on other agents' exploration and boost coordinated exploration. Empirically, we show that MACE achieves superior performance in three multi-agent environments with sparse rewards.
&lt;/p&gt;</description></item></channel></rss>