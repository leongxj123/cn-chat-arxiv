<rss version="2.0"><channel><title>Chat Arxiv cs.MA</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.MA</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;&#21464;&#20998;&#37327;&#23376;&#32447;&#36335;&#30340;&#36827;&#21270;&#20248;&#21270;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21464;&#20307;&#65292;&#24182;&#22312;Coin Game&#29615;&#22659;&#20013;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30456;&#27604;&#20110;&#32463;&#20856;&#26041;&#27861;&#34920;&#29616;&#26174;&#33879;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2311.05546</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#20351;&#29992;&#36827;&#21270;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Quantum Reinforcement Learning using Evolutionary Optimization. (arXiv:2311.05546v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.05546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;&#21464;&#20998;&#37327;&#23376;&#32447;&#36335;&#30340;&#36827;&#21270;&#20248;&#21270;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21464;&#20307;&#65292;&#24182;&#22312;Coin Game&#29615;&#22659;&#20013;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30456;&#27604;&#20110;&#32463;&#20856;&#26041;&#27861;&#34920;&#29616;&#26174;&#33879;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#20854;&#20182;&#26234;&#33021;&#20135;&#19994;&#24212;&#29992;&#26041;&#38754;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#21033;&#29992;&#37327;&#23376;&#21147;&#23398;&#30340;&#22266;&#26377;&#23646;&#24615;&#65292;&#37319;&#29992;&#26032;&#30340;&#26377;&#24076;&#26395;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26174;&#33879;&#20943;&#23569;&#27169;&#22411;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#22810;&#26234;&#33021;&#20307;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24120;&#24120;&#38754;&#20020;&#36139;&#30240;&#24179;&#21488;&#38382;&#39064;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#19982;&#32463;&#20856;&#26041;&#27861;&#24615;&#33021;&#30340;&#21305;&#37197;&#12290;&#25105;&#20204;&#22312;&#29616;&#26377;&#30340;&#26080;&#26799;&#24230;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22522;&#30784;&#19978;&#26500;&#24314;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;&#21464;&#20998;&#37327;&#23376;&#32447;&#36335;&#30340;&#36827;&#21270;&#20248;&#21270;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21464;&#20307;&#12290;&#25105;&#20204;&#22312;Coin Game&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#36951;&#20256;&#21464;&#31181;&#65292;&#24182;&#19982;&#32463;&#20856;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#21464;&#20998;&#37327;&#23376;&#32447;&#36335;&#26041;&#27861;&#30456;&#27604;&#20110;&#20855;&#26377;&#31867;&#20284;&#21442;&#25968;&#25968;&#37327;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#29616;&#26174;&#33879;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Reinforcement Learning is becoming increasingly more important in times of autonomous driving and other smart industrial applications. Simultaneously a promising new approach to Reinforcement Learning arises using the inherent properties of quantum mechanics, reducing the trainable parameters of a model significantly. However, gradient-based Multi-Agent Quantum Reinforcement Learning methods often have to struggle with barren plateaus, holding them back from matching the performance of classical approaches. We build upon an existing approach for gradient free Quantum Reinforcement Learning and propose three genetic variations with Variational Quantum Circuits for Multi-Agent Reinforcement Learning using evolutionary optimization. We evaluate our genetic variations in the Coin Game environment and also compare them to classical approaches. We showed that our Variational Quantum Circuit approaches perform significantly better compared to a neural network with a similar amount
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#36710;&#36742;&#21487;&#33021;&#36973;&#21463;&#30340;&#32593;&#32476;&#25915;&#20987;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#36890;&#27169;&#22411;&#26694;&#26550;&#21644;&#22522;&#20110;GAN&#30340;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#26102;&#35782;&#21035;&#24694;&#24847;&#25805;&#32437;&#12289;&#34394;&#20551;&#27880;&#20837;&#21644;&#25298;&#32477;&#26381;&#21153;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2310.17091</link><description>&lt;p&gt;
&#23545;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#36710;&#36742;&#30340;&#38544;&#34109;&#32593;&#32476;&#25915;&#20987;&#30340;&#26816;&#27979;&#65306;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Detecting stealthy cyberattacks on adaptive cruise control vehicles: A machine learning approach. (arXiv:2310.17091v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#36710;&#36742;&#21487;&#33021;&#36973;&#21463;&#30340;&#32593;&#32476;&#25915;&#20987;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#36890;&#27169;&#22411;&#26694;&#26550;&#21644;&#22522;&#20110;GAN&#30340;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#26102;&#35782;&#21035;&#24694;&#24847;&#25805;&#32437;&#12289;&#34394;&#20551;&#27880;&#20837;&#21644;&#25298;&#32477;&#26381;&#21153;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#37197;&#22791;&#20102;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#65288;ACC&#65289;&#21644;&#20854;&#20182;&#33258;&#21160;&#39550;&#39542;&#21151;&#33021;&#30340;&#20808;&#36827;&#39550;&#39542;&#36741;&#21161;&#31995;&#32479;&#30340;&#20986;&#29616;&#65292;&#38024;&#23545;&#36825;&#20123;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#30340;&#32593;&#32476;&#25915;&#20987;&#28508;&#22312;&#39118;&#38505;&#20063;&#20986;&#29616;&#20102;&#12290;&#34429;&#28982;&#24378;&#21046;&#36710;&#36742;&#21457;&#29983;&#30896;&#25758;&#30340;&#26126;&#26174;&#25915;&#20987;&#23481;&#26131;&#34987;&#35782;&#21035;&#65292;&#20294;&#26356;&#38544;&#34109;&#30340;&#25915;&#20987;&#65292;&#21482;&#30053;&#24494;&#25913;&#21464;&#34892;&#39542;&#34892;&#20026;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#32593;&#32476;&#33539;&#22260;&#20869;&#25317;&#22581;&#12289;&#29123;&#27833;&#28040;&#32791;&#22686;&#21152;&#65292;&#29978;&#33267;&#22686;&#21152;&#30896;&#25758;&#39118;&#38505;&#65292;&#20294;&#24456;&#38590;&#34987;&#26816;&#27979;&#21040;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#25915;&#20987;&#30340;&#26816;&#27979;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#36890;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#25551;&#36848;&#21487;&#33021;&#30340;&#19977;&#31181;&#32593;&#32476;&#25915;&#20987;&#31867;&#22411;&#65306;&#24694;&#24847;&#25805;&#32437;&#36710;&#36742;&#25511;&#21046;&#21629;&#20196;&#12289;&#23545;&#20256;&#24863;&#22120;&#27979;&#37327;&#25968;&#25454;&#36827;&#34892;&#34394;&#20551;&#27880;&#20837;&#25915;&#20987;&#21644;&#25298;&#32477;&#26381;&#21153;&#65288;DoS&#65289;&#25915;&#20987;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#25915;&#20987;&#23545;&#20010;&#20307;&#36710;&#36742;&#65288;&#24494;&#35266;&#65289;&#21644;&#20132;&#36890;&#27969;&#65288;&#23439;&#35266;&#65289;&#27700;&#24179;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#23454;&#26102;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent of vehicles equipped with advanced driver-assistance systems, such as adaptive cruise control (ACC) and other automated driving features, the potential for cyberattacks on these automated vehicles (AVs) has emerged. While overt attacks that force vehicles to collide may be easily identified, more insidious attacks, which only slightly alter driving behavior, can result in network-wide increases in congestion, fuel consumption, and even crash risk without being easily detected. To address the detection of such attacks, we first present a traffic model framework for three types of potential cyberattacks: malicious manipulation of vehicle control commands, false data injection attacks on sensor measurements, and denial-of-service (DoS) attacks. We then investigate the impacts of these attacks at both the individual vehicle (micro) and traffic flow (macro) levels. A novel generative adversarial network (GAN)-based anomaly detection model is proposed for real-time identifica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24102;&#25903;&#25345;&#20449;&#24687;&#30340;&#25293;&#21334;&#35774;&#35745;&#65292;&#36890;&#36807;&#20248;&#21270;DSIC&#26426;&#21046;&#24182;&#23558;&#26368;&#22351;&#24773;&#20917;&#19982;oracle&#36827;&#34892;&#27604;&#36739;&#65292;&#35762;&#36848;&#20102;&#19977;&#31181;&#25903;&#25345;&#20449;&#24687;&#30340;&#21306;&#22495;&#65292;&#24471;&#20986;&#20102;&#26368;&#20248;&#26426;&#21046;&#30340;&#38381;&#21512;&#24418;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.09065</link><description>&lt;p&gt;
&#24102;&#25903;&#25345;&#20449;&#24687;&#30340;&#40065;&#26834;&#25293;&#21334;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Robust Auction Design with Support Information. (arXiv:2305.09065v1 [econ.TH])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24102;&#25903;&#25345;&#20449;&#24687;&#30340;&#25293;&#21334;&#35774;&#35745;&#65292;&#36890;&#36807;&#20248;&#21270;DSIC&#26426;&#21046;&#24182;&#23558;&#26368;&#22351;&#24773;&#20917;&#19982;oracle&#36827;&#34892;&#27604;&#36739;&#65292;&#35762;&#36848;&#20102;&#19977;&#31181;&#25903;&#25345;&#20449;&#24687;&#30340;&#21306;&#22495;&#65292;&#24471;&#20986;&#20102;&#26368;&#20248;&#26426;&#21046;&#30340;&#38381;&#21512;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#21334;&#23478;&#24819;&#35201;&#23558;&#21830;&#21697;&#21334;&#32473;$n$&#20010;&#20080;&#23478;&#65292;&#20080;&#23478;&#30340;&#20272;&#20540;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#65292;&#20294;&#26159;&#21334;&#23478;&#24182;&#19981;&#30693;&#36947;&#36825;&#20010;&#20998;&#24067;&#12290;&#20026;&#20102;&#25269;&#24481;&#29615;&#22659;&#21644;&#20080;&#23478;&#34892;&#20026;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21334;&#23478;&#22312;DSIC&#26426;&#21046;&#20013;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#23558;&#26368;&#22351;&#24773;&#20917;&#30340;&#34920;&#29616;&#19982;&#20855;&#26377;&#23436;&#20840;&#20080;&#23478;&#20272;&#20540;&#30693;&#35782;&#30340;oracle&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21253;&#25324;&#36951;&#25022;&#21644;&#27604;&#29575;&#20004;&#20010;&#30446;&#26631;&#12290;&#23545;&#20110;&#36825;&#20123;&#30446;&#26631;&#65292;&#25105;&#20204;&#20197;&#25903;&#25345;&#21644;&#20080;&#23478;&#25968;$n$&#30340;&#20989;&#25968;&#24418;&#24335;&#23548;&#20986;&#20102;&#19968;&#20010;&#38381;&#21512;&#30340;&#26368;&#20248;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#19977;&#20010;&#25903;&#25345;&#20449;&#24687;&#30340;&#21306;&#22495;&#21644;&#19968;&#20010;&#26032;&#30340;&#40065;&#26834;&#26426;&#21046;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
A seller wants to sell an item to $n$ buyers. The buyer valuations are drawn i.i.d. from a distribution, but the seller does not know this distribution; the seller only knows the support $[a,b]$. To be robust against the lack of knowledge of the environment and buyers' behavior, the seller optimizes over DSIC mechanisms, and measures the worst-case performance relative to an oracle with complete knowledge of buyers' valuations. Our analysis encompasses both the regret and the ratio objectives.  For these objectives, we derive an optimal mechanism in closed form as a function of the support and the number of buyers $n$. Our analysis reveals three regimes of support information and a new class of robust mechanisms. i.) With "low" support information, the optimal mechanism is a second-price auction (SPA) with a random reserve, a focal class in the earlier literature. ii.) With "high" support information, we show that second-price auctions are strictly suboptimal, and an optimal mechanism 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#30740;&#31350;&#20102;&#24322;&#26500;&#38646;&#26679;&#26412;&#21327;&#21516;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21327;&#21516;&#36827;&#21270;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#37197;&#23545;&#12289;&#26356;&#26032;&#21644;&#36873;&#25321;&#30340;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#22810;&#26234;&#33021;&#20307;&#38646;&#26679;&#26412;&#21327;&#21516;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32771;&#34385;&#24322;&#26500;&#24773;&#20917;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#23545;&#20110;&#24322;&#26500;&#38646;&#26679;&#26412;&#21327;&#21516;&#20219;&#21153;&#30340;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2208.04957</link><description>&lt;p&gt;
&#24322;&#26500;&#22810;&#26234;&#33021;&#20307;&#38646;&#26679;&#26412;&#21327;&#21516;&#36827;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Multi-agent Zero-Shot Coordination by Coevolution. (arXiv:2208.04957v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.04957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#30740;&#31350;&#20102;&#24322;&#26500;&#38646;&#26679;&#26412;&#21327;&#21516;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21327;&#21516;&#36827;&#21270;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#37197;&#23545;&#12289;&#26356;&#26032;&#21644;&#36873;&#25321;&#30340;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#22810;&#26234;&#33021;&#20307;&#38646;&#26679;&#26412;&#21327;&#21516;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32771;&#34385;&#24322;&#26500;&#24773;&#20917;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#23545;&#20110;&#24322;&#26500;&#38646;&#26679;&#26412;&#21327;&#21516;&#20219;&#21153;&#30340;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#65292;&#29983;&#25104;&#33021;&#22815;&#19982;&#26410;&#30693;&#21512;&#20316;&#20249;&#20276;&#38646;&#26679;&#26412;&#21327;&#21516;&#30340;&#26234;&#33021;&#20307;&#26159;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#22312;&#38646;&#26679;&#26412;&#21327;&#21516;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#36890;&#36807;&#35757;&#32451;&#36807;&#31243;&#20013;&#21521;&#26234;&#33021;&#20307;&#26292;&#38706;&#22810;&#26679;&#21270;&#30340;&#21512;&#20316;&#20249;&#20276;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#22312;&#35757;&#32451;&#20249;&#20276;&#26102;&#28041;&#21450;&#33258;&#25105;&#23545;&#24328;&#65292;&#38544;&#24335;&#22320;&#20551;&#35774;&#20219;&#21153;&#26159;&#21516;&#36136;&#30340;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#20219;&#21153;&#26159;&#24322;&#26500;&#30340;&#65292;&#22240;&#27492;&#20808;&#21069;&#30340;&#26041;&#27861;&#21487;&#33021;&#25928;&#29575;&#20302;&#19979;&#12290;&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#24322;&#26500;&#38646;&#26679;&#26412;&#21327;&#21516;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21327;&#21516;&#36827;&#21270;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#20010;&#23376;&#36807;&#31243;&#65306;&#37197;&#23545;&#12289;&#26356;&#26032;&#21644;&#36873;&#25321;&#65292;&#23545;&#20004;&#20010;&#26234;&#33021;&#20307;&#21644;&#21512;&#20316;&#20249;&#20276;&#36827;&#34892;&#21327;&#21516;&#36827;&#21270;&#12290;&#23545;&#19981;&#21516;&#24322;&#26500;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#31361;&#20986;&#20102;&#32771;&#34385;&#24322;&#26500;&#24773;&#20917;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#35299;&#20915;&#24322;&#26500;&#38646;&#26679;&#26412;&#21327;&#21516;&#20219;&#21153;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating agents that can achieve zero-shot coordination (ZSC) with unseen partners is a new challenge in cooperative multi-agent reinforcement learning (MARL). Recently, some studies have made progress in ZSC by exposing the agents to diverse partners during the training process. They usually involve self-play when training the partners, implicitly assuming that the tasks are homogeneous. However, many real-world tasks are heterogeneous, and hence previous methods may be inefficient. In this paper, we study the heterogeneous ZSC problem for the first time and propose a general method based on coevolution, which coevolves two populations of agents and partners through three sub-processes: pairing, updating and selection. Experimental results on various heterogeneous tasks highlight the necessity of considering the heterogeneous setting and demonstrate that our proposed method is a promising solution for heterogeneous ZSC tasks.
&lt;/p&gt;</description></item></channel></rss>