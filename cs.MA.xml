<rss version="2.0"><channel><title>Chat Arxiv cs.MA</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.MA</description><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;MA-COPP&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#35299;&#20915;&#28041;&#21450;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#31163;&#31574;&#30053;&#39044;&#27979;&#38382;&#39064;&#30340;&#19968;&#33268;&#39044;&#27979;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.16871</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#19968;&#33268;&#31163;&#31574;&#30053;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Conformal Off-Policy Prediction for Multi-Agent Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16871
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;MA-COPP&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#35299;&#20915;&#28041;&#21450;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#31163;&#31574;&#30053;&#39044;&#27979;&#38382;&#39064;&#30340;&#19968;&#33268;&#39044;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#31574;&#30053;&#39044;&#27979;&#65288;OPP&#65289;&#65292;&#21363;&#20165;&#20351;&#29992;&#22312;&#19968;&#20010;&#27491;&#24120;&#65288;&#34892;&#20026;&#65289;&#31574;&#30053;&#19979;&#25910;&#38598;&#30340;&#25968;&#25454;&#26469;&#39044;&#27979;&#30446;&#26631;&#31574;&#30053;&#30340;&#32467;&#26524;&#65292;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20998;&#26512;&#20013;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#22312;&#36825;&#31181;&#31995;&#32479;&#20013;&#65292;&#37096;&#32626;&#26032;&#31574;&#30053;&#21487;&#33021;&#26159;&#19981;&#23433;&#20840;&#30340;&#12290;&#20026;&#20102;&#23454;&#29616;&#21487;&#20449;&#30340;&#31163;&#31574;&#30053;&#39044;&#27979;&#65292;&#26368;&#36817;&#20851;&#20110;&#19968;&#33268;&#31163;&#31574;&#30053;&#39044;&#27979;&#65288;COPP&#65289;&#30340;&#24037;&#20316;&#21033;&#29992;&#19968;&#33268;&#39044;&#27979;&#26694;&#26550;&#26469;&#22312;&#30446;&#26631;&#36807;&#31243;&#19979;&#25512;&#23548;&#24102;&#26377;&#27010;&#29575;&#20445;&#35777;&#30340;&#39044;&#27979;&#21306;&#22495;&#12290;&#29616;&#26377;&#30340;COPP&#26041;&#27861;&#21487;&#20197;&#32771;&#34385;&#30001;&#31574;&#30053;&#20999;&#25442;&#24341;&#36215;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#20294;&#20165;&#38480;&#20110;&#21333;&#26234;&#33021;&#20307;&#31995;&#32479;&#21644;&#26631;&#37327;&#32467;&#26524;&#65288;&#20363;&#22914;&#65292;&#22870;&#21169;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MA-COPP&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#35299;&#20915;&#28041;&#21450;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;OPP&#38382;&#39064;&#30340;&#19968;&#33268;&#39044;&#27979;&#26041;&#27861;&#65292;&#22312;&#19968;&#20010;&#25110;&#22810;&#20010;&#8220;&#33258;&#25105;&#8221;&#26234;&#33021;&#20307;&#25913;&#21464;&#31574;&#30053;&#26102;&#20026;&#25152;&#26377;&#26234;&#33021;&#20307;&#36712;&#36857;&#25512;&#23548;&#32852;&#21512;&#39044;&#27979;&#21306;&#22495;&#12290;&#19982;&#21333;&#26234;&#33021;&#20307;&#22330;&#26223;&#19981;&#21516;&#65292;&#36825;&#31181;&#24773;&#20917;&#19979;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16871v1 Announce Type: cross  Abstract: Off-Policy Prediction (OPP), i.e., predicting the outcomes of a target policy using only data collected under a nominal (behavioural) policy, is a paramount problem in data-driven analysis of safety-critical systems where the deployment of a new policy may be unsafe. To achieve dependable off-policy predictions, recent work on Conformal Off-Policy Prediction (COPP) leverage the conformal prediction framework to derive prediction regions with probabilistic guarantees under the target process. Existing COPP methods can account for the distribution shifts induced by policy switching, but are limited to single-agent systems and scalar outcomes (e.g., rewards). In this work, we introduce MA-COPP, the first conformal prediction method to solve OPP problems involving multi-agent systems, deriving joint prediction regions for all agents' trajectories when one or more "ego" agents change their policies. Unlike the single-agent scenario, this se
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25345;&#32493;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#20998;&#25955;&#30340;&#23398;&#20064;&#26550;&#26500;&#65292;&#26469;&#35299;&#20915;&#20132;&#36890;&#36335;&#21475;&#31359;&#36234;&#21644;&#33258;&#20027;&#36187;&#36710;&#31561;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.10996</link><description>&lt;p&gt;
&#19968;&#20010;&#21487;&#25193;&#23637;&#19988;&#21487;&#24182;&#34892;&#21270;&#30340;&#25968;&#23383;&#23402;&#29983;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#21487;&#25345;&#32493;Sim2Real&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
A Scalable and Parallelizable Digital Twin Framework for Sustainable Sim2Real Transition of Multi-Agent Reinforcement Learning Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10996
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25345;&#32493;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#20998;&#25955;&#30340;&#23398;&#20064;&#26550;&#26500;&#65292;&#26469;&#35299;&#20915;&#20132;&#36890;&#36335;&#21475;&#31359;&#36234;&#21644;&#33258;&#20027;&#36187;&#36710;&#31561;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25345;&#32493;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#36873;&#25321;&#24615;&#22320;&#25353;&#38656;&#25193;&#23637;&#24182;&#34892;&#21270;&#35757;&#32451;&#24037;&#20316;&#36127;&#36733;&#65292;&#24182;&#21033;&#29992;&#26368;&#23569;&#30340;&#30828;&#20214;&#36164;&#28304;&#23558;&#35757;&#32451;&#22909;&#30340;&#31574;&#30053;&#20174;&#27169;&#25311;&#29615;&#22659;&#36716;&#31227;&#21040;&#29616;&#23454;&#19990;&#30028;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;AutoDRIVE&#29983;&#24577;&#31995;&#32479;&#20316;&#20026;&#19968;&#20010;&#21551;&#21160;&#25968;&#23383;&#23402;&#29983;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#12289;&#37096;&#32626;&#21644;&#36716;&#31227;&#21512;&#20316;&#21644;&#31454;&#20105;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#20174;&#27169;&#25311;&#29615;&#22659;&#21040;&#29616;&#23454;&#19990;&#30028;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#25506;&#31350;&#20102;4&#21488;&#21512;&#20316;&#36710;&#36742;(Nigel)&#22312;&#21333;&#26234;&#33021;&#20307;&#21644;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#29615;&#22659;&#20013;&#20849;&#20139;&#26377;&#38480;&#29366;&#24577;&#20449;&#24687;&#30340;&#20132;&#21449;&#36941;&#21382;&#38382;&#39064;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#36890;&#29992;&#31574;&#30053;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20010;&#20307;&#31574;&#30053;&#26041;&#27861;&#30740;&#31350;&#20102;2&#36742;&#36710;(F1TENTH)&#30340;&#23545;&#25239;&#24615;&#33258;&#20027;&#36187;&#36710;&#38382;&#39064;&#12290;&#22312;&#20219;&#20309;&#19968;&#32452;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#26550;&#26500;&#65292;&#36825;&#20801;&#35768;&#23545;&#31574;&#30053;&#36827;&#34892;&#26377;&#21147;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10996v1 Announce Type: cross  Abstract: This work presents a sustainable multi-agent deep reinforcement learning framework capable of selectively scaling parallelized training workloads on-demand, and transferring the trained policies from simulation to reality using minimal hardware resources. We introduce AutoDRIVE Ecosystem as an enabling digital twin framework to train, deploy, and transfer cooperative as well as competitive multi-agent reinforcement learning policies from simulation to reality. Particularly, we first investigate an intersection traversal problem of 4 cooperative vehicles (Nigel) that share limited state information in single as well as multi-agent learning settings using a common policy approach. We then investigate an adversarial autonomous racing problem of 2 vehicles (F1TENTH) using an individual policy approach. In either set of experiments, a decentralized learning architecture was adopted, which allowed robust training and testing of the policies 
&lt;/p&gt;</description></item><item><title>S-Agents&#26159;&#19968;&#20010;&#33258;&#32452;&#32455;&#20195;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#20195;&#29702;&#26641;&#32467;&#26500;&#12289;&#27801;&#28431;&#20195;&#29702;&#26550;&#26500;&#21644;&#38750;&#38459;&#22622;&#21327;&#20316;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#26080;&#38480;&#29615;&#22659;&#20013;&#39640;&#25928;&#21327;&#35843;&#20195;&#29702;&#30340;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#20248;&#21270;&#21327;&#20316;&#25928;&#29575;&#21644;&#28789;&#27963;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.04578</link><description>&lt;p&gt;
S-Agents: &#33258;&#32452;&#32455;&#20195;&#29702;&#22312;&#26080;&#38480;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
S-Agents: self-organizing agents in open-ended environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04578
&lt;/p&gt;
&lt;p&gt;
S-Agents&#26159;&#19968;&#20010;&#33258;&#32452;&#32455;&#20195;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#20195;&#29702;&#26641;&#32467;&#26500;&#12289;&#27801;&#28431;&#20195;&#29702;&#26550;&#26500;&#21644;&#38750;&#38459;&#22622;&#21327;&#20316;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#26080;&#38480;&#29615;&#22659;&#20013;&#39640;&#25928;&#21327;&#35843;&#20195;&#29702;&#30340;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#20248;&#21270;&#21327;&#20316;&#25928;&#29575;&#21644;&#28789;&#27963;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#33258;&#20027;&#20195;&#29702;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#65292;&#20855;&#22791;&#22788;&#29702;&#21508;&#31181;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#22312;&#26080;&#38480;&#29615;&#22659;&#20013;&#65292;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#21644;&#25928;&#26524;&#65292;&#20248;&#21270;&#21327;&#20316;&#38656;&#35201;&#28789;&#27963;&#30340;&#35843;&#25972;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#24378;&#35843;&#22266;&#23450;&#30340;&#12289;&#20219;&#21153;&#23548;&#21521;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#24573;&#35270;&#20102;&#20197;&#20195;&#29702;&#20026;&#20013;&#24515;&#30340;&#32452;&#32455;&#32467;&#26500;&#12290;&#21463;&#20154;&#31867;&#32452;&#32455;&#34892;&#20026;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#32452;&#32455;&#20195;&#29702;&#31995;&#32479;&#65288;S-Agents&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;&#21160;&#24577;&#24037;&#20316;&#27969;&#31243;&#30340;&#8220;&#20195;&#29702;&#26641;&#8221;&#32467;&#26500;&#12289;&#24179;&#34913;&#20449;&#24687;&#20248;&#20808;&#32423;&#30340;&#8220;&#27801;&#28431;&#20195;&#29702;&#26550;&#26500;&#8221;&#20197;&#21450;&#20801;&#35768;&#20195;&#29702;&#20043;&#38388;&#24322;&#27493;&#25191;&#34892;&#20219;&#21153;&#30340;&#8220;&#38750;&#38459;&#22622;&#21327;&#20316;&#8221;&#26041;&#27861;&#12290;&#36825;&#31181;&#32467;&#26500;&#21487;&#20197;&#33258;&#20027;&#21327;&#35843;&#19968;&#32452;&#20195;&#29702;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#26080;&#38480;&#19988;&#21160;&#24577;&#30340;&#29615;&#22659;&#25361;&#25112;&#65292;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;S-Agents&#33021;&#22815;&#29087;&#32451;&#22320;&#25191;&#34892;&#21327;&#20316;&#24314;&#31569;&#20219;&#21153;&#21644;&#36164;&#28304;&#25910;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging large language models (LLMs), autonomous agents have significantly improved, gaining the ability to handle a variety of tasks. In open-ended settings, optimizing collaboration for efficiency and effectiveness demands flexible adjustments. Despite this, current research mainly emphasizes fixed, task-oriented workflows and overlooks agent-centric organizational structures. Drawing inspiration from human organizational behavior, we introduce a self-organizing agent system (S-Agents) with a "tree of agents" structure for dynamic workflow, an "hourglass agent architecture" for balancing information priorities, and a "non-obstructive collaboration" method to allow asynchronous task execution among agents. This structure can autonomously coordinate a group of agents, efficiently addressing the challenges of an open and dynamic environment without human intervention. Our experiments demonstrate that S-Agents proficiently execute collaborative building tasks and resource collection i
&lt;/p&gt;</description></item></channel></rss>