<rss version="2.0"><channel><title>Chat Arxiv econ</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for econ</description><item><title>&#36890;&#36807;&#20351;&#29992;&#25289;&#32454;&#23391;&#21010;&#20998;&#38598;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#22240;&#23376;&#25968;&#25454;&#20013;&#31283;&#20581;&#22320;&#20272;&#35745;&#24322;&#36136;&#24615;&#65292;&#24182;&#23558;&#22240;&#23376;&#31354;&#38388;&#21010;&#20998;&#25104;&#21327;&#21464;&#37327;&#32452;&#21512;&#30340;&#8220;&#27744;&#8221;&#65292;&#20197;&#20415;&#21306;&#20998;&#32467;&#26524;&#30340;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2404.02141</link><description>&lt;p&gt;
&#20351;&#29992;&#25289;&#32454;&#23391;&#21010;&#20998;&#22312;&#22240;&#23376;&#25968;&#25454;&#20013;&#31283;&#20581;&#20272;&#35745;&#24322;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;
Robustly estimating heterogeneity in factorial data using Rashomon Partitions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02141
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#25289;&#32454;&#23391;&#21010;&#20998;&#38598;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#22240;&#23376;&#25968;&#25454;&#20013;&#31283;&#20581;&#22320;&#20272;&#35745;&#24322;&#36136;&#24615;&#65292;&#24182;&#23558;&#22240;&#23376;&#31354;&#38388;&#21010;&#20998;&#25104;&#21327;&#21464;&#37327;&#32452;&#21512;&#30340;&#8220;&#27744;&#8221;&#65292;&#20197;&#20415;&#21306;&#20998;&#32467;&#26524;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#32479;&#35745;&#20998;&#26512;&#65292;&#26080;&#35770;&#26159;&#22312;&#35266;&#27979;&#25968;&#25454;&#36824;&#26159;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#20013;&#65292;&#37117;&#20250;&#38382;&#65306;&#24863;&#20852;&#36259;&#30340;&#32467;&#26524;&#22914;&#20309;&#38543;&#21487;&#35266;&#23519;&#21327;&#21464;&#37327;&#32452;&#21512;&#21464;&#21270;&#65311;&#19981;&#21516;&#30340;&#33647;&#29289;&#32452;&#21512;&#22914;&#20309;&#24433;&#21709;&#20581;&#24247;&#32467;&#26524;&#65292;&#31185;&#25216;&#37319;&#32435;&#22914;&#20309;&#20381;&#36182;&#28608;&#21169;&#21644;&#20154;&#21475;&#32479;&#35745;&#23398;&#65311;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23558;&#36825;&#20010;&#22240;&#23376;&#31354;&#38388;&#21010;&#20998;&#25104;&#21327;&#21464;&#37327;&#32452;&#21512;&#30340;&#8220;&#27744;&#8221;&#65292;&#22312;&#36825;&#20123;&#27744;&#20013;&#32467;&#26524;&#20250;&#21457;&#29983;&#24046;&#24322;&#65288;&#20294;&#27744;&#20869;&#37096;&#19981;&#20250;&#21457;&#29983;&#65289;&#65292;&#32780;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#23547;&#25214;&#19968;&#20010;&#21333;&#19968;&#30340;&#8220;&#26368;&#20248;&#8221;&#20998;&#21106;&#65292;&#35201;&#20040;&#20174;&#21487;&#33021;&#20998;&#21106;&#30340;&#25972;&#20010;&#38598;&#21512;&#20013;&#25277;&#26679;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#24573;&#35270;&#20102;&#36825;&#26679;&#19968;&#20010;&#20107;&#23454;&#65306;&#29305;&#21035;&#26159;&#22312;&#21327;&#21464;&#37327;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#33021;&#20197;&#35768;&#22810;&#31181;&#26041;&#24335;&#21010;&#20998;&#21327;&#21464;&#37327;&#31354;&#38388;&#65292;&#22312;&#32479;&#35745;&#19978;&#26159;&#26080;&#27861;&#21306;&#20998;&#30340;&#65292;&#23613;&#31649;&#23545;&#25919;&#31574;&#25110;&#31185;&#23398;&#26377;&#30528;&#38750;&#24120;&#19981;&#21516;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25289;&#32454;&#23391;&#21010;&#20998;&#38598;&#30340;&#26367;&#20195;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02141v1 Announce Type: cross  Abstract: Many statistical analyses, in both observational data and randomized control trials, ask: how does the outcome of interest vary with combinations of observable covariates? How do various drug combinations affect health outcomes, or how does technology adoption depend on incentives and demographics? Our goal is to partition this factorial space into ``pools'' of covariate combinations where the outcome differs across the pools (but not within a pool). Existing approaches (i) search for a single ``optimal'' partition under assumptions about the association between covariates or (ii) sample from the entire set of possible partitions. Both these approaches ignore the reality that, especially with correlation structure in covariates, many ways to partition the covariate space may be statistically indistinguishable, despite very different implications for policy or science. We develop an alternative perspective, called Rashomon Partition Set
&lt;/p&gt;</description></item><item><title>&#22312;&#22810;&#29366;&#24577;&#30340;&#39118;&#38505;&#20998;&#20139;&#20013;&#65292;&#25105;&#20204;&#23450;&#37327;&#35780;&#20272;&#20102;&#31119;&#21033;&#65292;&#24182;&#35777;&#26126;&#20102;&#38543;&#30528;&#29366;&#24577;&#25968;&#22686;&#21152;&#65292;&#25913;&#21892;&#20010;&#20307;&#31119;&#21033;&#30340;&#31354;&#38388;&#28040;&#22833;&#65292;&#20998;&#37197;&#32473;&#25913;&#36827;&#32858;&#21512;&#36164;&#28304;&#30340;&#33539;&#22260;&#20063;&#28040;&#22833;&#65292;&#24182;&#19988;&#22312;&#20302;&#25928;&#20998;&#37197;&#20013;&#65292;&#36890;&#36807;&#25200;&#21160;&#32858;&#21512;&#36164;&#28304;&#26469;&#22686;&#24378;&#31119;&#21033;&#30340;&#21487;&#33021;&#24615;&#38543;&#29366;&#24577;&#25968;&#30340;&#25351;&#25968;&#22686;&#21152;&#32780;&#21464;&#20026;&#38646;&#65292;&#21516;&#26102;&#65292;&#26576;&#20123;&#20808;&#39564;&#27010;&#29575;&#38598;&#38543;&#30528;&#29366;&#24577;&#31354;&#38388;&#30340;&#22823;&#23567;&#32780;&#32553;&#23567;&#12290;</title><link>http://arxiv.org/abs/2401.07337</link><description>&lt;p&gt;
&#22810;&#29366;&#24577;&#39118;&#38505;&#20998;&#20139;&#20013;&#30340;&#20010;&#20307;&#21644;&#38598;&#20307;&#31119;&#21033;
&lt;/p&gt;
&lt;p&gt;
Individual and Collective Welfare in Risk Sharing with Many States. (arXiv:2401.07337v1 [econ.TH])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07337
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#29366;&#24577;&#30340;&#39118;&#38505;&#20998;&#20139;&#20013;&#65292;&#25105;&#20204;&#23450;&#37327;&#35780;&#20272;&#20102;&#31119;&#21033;&#65292;&#24182;&#35777;&#26126;&#20102;&#38543;&#30528;&#29366;&#24577;&#25968;&#22686;&#21152;&#65292;&#25913;&#21892;&#20010;&#20307;&#31119;&#21033;&#30340;&#31354;&#38388;&#28040;&#22833;&#65292;&#20998;&#37197;&#32473;&#25913;&#36827;&#32858;&#21512;&#36164;&#28304;&#30340;&#33539;&#22260;&#20063;&#28040;&#22833;&#65292;&#24182;&#19988;&#22312;&#20302;&#25928;&#20998;&#37197;&#20013;&#65292;&#36890;&#36807;&#25200;&#21160;&#32858;&#21512;&#36164;&#28304;&#26469;&#22686;&#24378;&#31119;&#21033;&#30340;&#21487;&#33021;&#24615;&#38543;&#29366;&#24577;&#25968;&#30340;&#25351;&#25968;&#22686;&#21152;&#32780;&#21464;&#20026;&#38646;&#65292;&#21516;&#26102;&#65292;&#26576;&#20123;&#20808;&#39564;&#27010;&#29575;&#38598;&#38543;&#30528;&#29366;&#24577;&#31354;&#38388;&#30340;&#22823;&#23567;&#32780;&#32553;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#39118;&#38505;&#20998;&#20139;&#21644;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#32463;&#20856;&#27169;&#22411;&#20013;&#30340;&#31119;&#21033;&#36827;&#34892;&#20102;&#23450;&#37327;&#35780;&#20272;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19977;&#31181;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#22312;&#22343;&#34913;&#20998;&#37197;&#20013;&#65292;&#36890;&#36807;&#32473;&#23450;&#30340;&#36793;&#38469;&#25552;&#39640;&#20010;&#20307;&#31119;&#21033;&#65288;&#949;-&#25552;&#39640;&#65289;&#30340;&#31354;&#38388;&#38543;&#30528;&#29366;&#24577;&#25968;&#30340;&#22686;&#21152;&#32780;&#28040;&#22833;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#25552;&#39640;&#20010;&#20307;&#31119;&#21033;&#65292;&#20998;&#37197;&#32473;&#25913;&#36827;&#32858;&#21512;&#36164;&#28304;&#30340;&#33539;&#22260;&#20063;&#28040;&#22833;&#12290;&#31561;&#20215;&#22320;&#35828;&#65306;&#22312;&#20302;&#25928;&#20998;&#37197;&#20013;&#65292;&#23545;&#20110;&#32473;&#23450;&#27700;&#24179;&#30340;&#36164;&#28304;&#27425;&#20248;&#21270;&#65288;&#26681;&#25454;&#36164;&#28304;&#26410;&#20805;&#20998;&#21033;&#29992;&#30340;&#31995;&#25968;&#24230;&#37327;&#65289;&#65292;&#36890;&#36807;&#25200;&#21160;&#32858;&#21512;&#36164;&#28304;&#26469;&#22686;&#24378;&#31119;&#21033;&#30340;&#21487;&#33021;&#24615;&#38543;&#29366;&#24577;&#25968;&#30340;&#25351;&#25968;&#22686;&#21152;&#32780;&#21464;&#20026;&#38646;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20855;&#26377;&#22810;&#20010;&#20808;&#39564;&#27010;&#29575;&#30340;&#26631;&#20934;&#19981;&#30830;&#23450;&#24615;&#35268;&#36991;&#20013;&#30340;&#39640;&#25928;&#39118;&#38505;&#20998;&#20139;&#65292;&#24182;&#26174;&#31034;&#22312;&#20302;&#25928;&#20998;&#37197;&#20013;&#65292;&#26576;&#20123;&#20808;&#39564;&#27010;&#29575;&#38598;&#38543;&#30528;&#29366;&#24577;&#31354;&#38388;&#30340;&#22823;&#23567;&#32780;&#32553;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a quantitative assessment of welfare in the classical model of risk-sharing and exchange under uncertainty. We prove three kinds of results. First, that in an equilibrium allocation, the scope for improving individual welfare by a given margin (an $\ve$-improvement) vanishes as the number of states increases. Second, that the scope for a change in aggregate resources that may be distributed to enhance individual welfare by a given margin also vanishes. Equivalently: in an inefficient allocation, for a given level of resource sub-optimality (as measured by the coefficient of resource under-utilization), the possibilities for enhancing welfare by perturbing aggregate resources decrease exponentially to zero with the number of states. Finally, we consider efficient risk-sharing in standard models of uncertainty aversion with multiple priors, and show that, in an inefficient allocation, certain sets of priors shrink with the size of the state space.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25209;&#37327;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;STEEL&#65292;&#22312;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#34892;&#21160;&#30340;&#26080;&#38480;&#26102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#19981;&#20381;&#36182;&#20110;&#32477;&#23545;&#36830;&#32493;&#20551;&#35774;&#65292;&#36890;&#36807;&#26368;&#22823;&#22343;&#20540;&#20559;&#24046;&#21644;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#30830;&#20445;&#24322;&#24120;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.13152</link><description>&lt;p&gt;
STEEL: &#22855;&#24322;&#24615;&#24863;&#30693;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
STEEL: Singularity-aware Reinforcement Learning. (arXiv:2301.13152v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13152
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25209;&#37327;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;STEEL&#65292;&#22312;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#34892;&#21160;&#30340;&#26080;&#38480;&#26102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#19981;&#20381;&#36182;&#20110;&#32477;&#23545;&#36830;&#32493;&#20551;&#35774;&#65292;&#36890;&#36807;&#26368;&#22823;&#22343;&#20540;&#20559;&#24046;&#21644;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#30830;&#20445;&#24322;&#24120;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25209;&#37327;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#21033;&#29992;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#65292;&#20197;&#26368;&#22823;&#21270;&#26399;&#26395;&#24635;&#22238;&#25253;&#12290;&#28982;&#32780;&#65292;&#20960;&#20046;&#25152;&#26377;&#29616;&#26377;&#31639;&#27861;&#37117;&#20381;&#36182;&#20110;&#30446;&#26631;&#31574;&#30053;&#35825;&#23548;&#30340;&#20998;&#24067;&#32477;&#23545;&#36830;&#32493;&#20551;&#35774;&#65292;&#20197;&#20415;&#36890;&#36807;&#21464;&#25442;&#27979;&#24230;&#20351;&#29992;&#25209;&#37327;&#25968;&#25454;&#26469;&#26657;&#20934;&#30446;&#26631;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25209;&#37327;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#19981;&#38656;&#35201;&#22312;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#34892;&#21160;&#30340;&#26080;&#38480;&#26102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#32477;&#23545;&#36830;&#32493;&#24615;&#20551;&#35774;&#12290;&#25105;&#20204;&#31216;&#36825;&#20010;&#31639;&#27861;&#20026;STEEL&#65306;SingulariTy-awarE rEinforcement Learning&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21463;&#21040;&#20851;&#20110;&#31163;&#32447;&#35780;&#20272;&#30340;&#26032;&#35823;&#24046;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#20102;&#26368;&#22823;&#22343;&#20540;&#20559;&#24046;&#65292;&#20197;&#21450;&#24102;&#26377;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#30340;&#31574;&#30053;&#23450;&#21521;&#35823;&#24046;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#24322;&#24120;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#22855;&#24322;&#24773;&#20917;&#30340;&#23450;&#21521;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Batch reinforcement learning (RL) aims at leveraging pre-collected data to find an optimal policy that maximizes the expected total rewards in a dynamic environment. Nearly all existing algorithms rely on the absolutely continuous assumption on the distribution induced by target policies with respect to the data distribution, so that the batch data can be used to calibrate target policies via the change of measure. However, the absolute continuity assumption could be violated in practice (e.g., no-overlap support), especially when the state-action space is large or continuous. In this paper, we propose a new batch RL algorithm without requiring absolute continuity in the setting of an infinite-horizon Markov decision process with continuous states and actions. We call our algorithm STEEL: SingulariTy-awarE rEinforcement Learning. Our algorithm is motivated by a new error analysis on off-policy evaluation, where we use maximum mean discrepancy, together with distributionally robust opti
&lt;/p&gt;</description></item></channel></rss>