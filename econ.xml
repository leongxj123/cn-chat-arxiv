<rss version="2.0"><channel><title>Chat Arxiv econ</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for econ</description><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#32452;&#21512;&#26368;&#20339;&#20248;&#20808;&#31639;&#27861;&#21644;&#20108;&#27425;&#35268;&#21010;&#65292;&#25193;&#23637;&#20102;&#38543;&#26426;&#20018;&#34892;&#29420;&#35009;&#30340;&#29305;&#24449;&#21040;$n\le5$&#30340;&#24773;&#20917;&#65292;&#25509;&#36817;&#35299;&#20915;&#35813;&#29305;&#24449;&#26159;&#21542;&#36866;&#29992;&#20110;&#20219;&#24847;$n$&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.11976</link><description>&lt;p&gt;
&#25551;&#36848;&#38543;&#26426;&#20018;&#34892;&#29420;&#35009;&#30340;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Characterizing Random Serial Dictatorship. (arXiv:2303.11976v1 [econ.TH])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#32452;&#21512;&#26368;&#20339;&#20248;&#20808;&#31639;&#27861;&#21644;&#20108;&#27425;&#35268;&#21010;&#65292;&#25193;&#23637;&#20102;&#38543;&#26426;&#20018;&#34892;&#29420;&#35009;&#30340;&#29305;&#24449;&#21040;$n\le5$&#30340;&#24773;&#20917;&#65292;&#25509;&#36817;&#35299;&#20915;&#35813;&#29305;&#24449;&#26159;&#21542;&#36866;&#29992;&#20110;&#20219;&#24847;$n$&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#20018;&#34892;&#29420;&#35009;&#26159;&#19968;&#31181;&#38543;&#26426;&#20998;&#37197;&#35268;&#21017;&#65292;&#23427;&#32473;&#23450;&#20102;&#19968;&#32452;&#23545;$n$&#20010;&#25151;&#23627;&#30340;&#36873;&#25321;&#20855;&#26377;&#20005;&#26684;&#20559;&#22909;&#30340;$n$&#20010;&#20195;&#29702;&#20154;&#65292;&#28385;&#36275;&#24179;&#31561;&#23545;&#24453;&#12289;&#21518;&#39564;&#25928;&#29575;&#21644;&#31574;&#30053;&#26080;&#20851;&#24615;&#12290;&#23545;&#20110;$n \le 3$&#65292;Bogomolnaia&#21644;Moulin&#65288;2001&#65289;&#24050;&#32463;&#34920;&#26126;&#38543;&#26426;&#20018;&#34892;&#29420;&#35009;&#20197;&#36825;&#19977;&#20010;&#20844;&#29702;&#20026;&#29305;&#24449;&#12290;&#26412;&#25991;&#21033;&#29992;&#32452;&#21512;&#26368;&#20339;&#20248;&#20808;&#25628;&#32034;&#21644;&#20108;&#27425;&#35268;&#21010;&#23558;&#36825;&#20010;&#29305;&#24449;&#25193;&#23637;&#21040;$n \le 5$&#65292;&#26356;&#25509;&#36817;&#22238;&#31572;&#38271;&#26399;&#23384;&#22312;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#65292;&#21363;&#36825;&#20010;&#29305;&#24449;&#26159;&#21542;&#36866;&#29992;&#20110;&#20219;&#24847;$n$&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#21518;&#39564;&#25928;&#29575;&#21644;&#31574;&#30053;&#26080;&#20851;&#24615;&#30340;&#21066;&#24369;&#24418;&#24335;&#65292;&#36825;&#20123;&#24418;&#24335;&#36275;&#20197;&#28385;&#36275;&#25105;&#20204;&#30340;&#29305;&#24449;&#65292;&#24182;&#30830;&#23450;&#22312;&#38024;&#23545;&#26356;&#22823;$n$&#26102;&#20570;&#20986;&#38472;&#36848;&#26102;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Random serial dictatorship (RSD) is a randomized assignment rule that - given a set of $n$ agents with strict preferences over $n$ houses - satisfies equal treatment of equals, ex post efficiency, and strategyproofness. For $n \le 3$, Bogomolnaia and Moulin (2001) have shown that RSD is characterized by these three axioms. Using best first search in combination with quadratic programming, we extend this characterization to $n \le 5$, getting closer to answering the long-standing open question whether the characterization holds for arbitrary $n$. On the way, we describe weakenings of ex post efficiency and strategyproofness that are sufficient for our characterization and identify problems when making statements for larger $n$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#39034;&#24207;&#20915;&#31574;&#21338;&#24328;&#20013;&#30340;&#20114;&#21160;&#20027;&#20307;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#20307;&#21270;&#27835;&#30103;&#20998;&#37197;&#26041;&#27861;&#65292;&#36890;&#36807;&#35780;&#20272;&#32467;&#26524;&#30340;&#22266;&#23450;&#20998;&#24067;&#24182;&#37319;&#29992;&#21464;&#20998;&#36817;&#20284;&#21644;&#36138;&#23146;&#20248;&#21270;&#31639;&#27861;&#65292;&#26368;&#22823;&#21270;&#20102;&#31038;&#20250;&#31119;&#21033;&#20934;&#21017;&#12290;</title><link>http://arxiv.org/abs/2302.05747</link><description>&lt;p&gt;
&#39034;&#24207;&#32593;&#32476;&#21338;&#24328;&#20013;&#30340;&#20010;&#20307;&#21270;&#27835;&#30103;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Individualized Treatment Allocation in Sequential Network Games. (arXiv:2302.05747v3 [econ.EM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#39034;&#24207;&#20915;&#31574;&#21338;&#24328;&#20013;&#30340;&#20114;&#21160;&#20027;&#20307;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#20307;&#21270;&#27835;&#30103;&#20998;&#37197;&#26041;&#27861;&#65292;&#36890;&#36807;&#35780;&#20272;&#32467;&#26524;&#30340;&#22266;&#23450;&#20998;&#24067;&#24182;&#37319;&#29992;&#21464;&#20998;&#36817;&#20284;&#21644;&#36138;&#23146;&#20248;&#21270;&#31639;&#27861;&#65292;&#26368;&#22823;&#21270;&#20102;&#31038;&#20250;&#31119;&#21033;&#20934;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20010;&#20307;&#21270;&#30340;&#27835;&#30103;&#20998;&#37197;&#65292;&#20197;&#26368;&#22823;&#21270;&#20114;&#21160;&#20027;&#20307;&#30340;&#22343;&#34913;&#31119;&#21033;&#65292;&#22312;&#25919;&#31574;&#30456;&#20851;&#30340;&#24212;&#29992;&#20013;&#26377;&#24456;&#22823;&#30340;&#24847;&#20041;&#12290;&#26412;&#25991;&#38024;&#23545;&#20114;&#21160;&#20027;&#20307;&#30340;&#39034;&#24207;&#20915;&#31574;&#21338;&#24328;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#33719;&#24471;&#26368;&#20248;&#30340;&#27835;&#30103;&#20998;&#37197;&#35268;&#21017;&#65292;&#36890;&#36807;&#35780;&#20272;&#32467;&#26524;&#30340;&#22266;&#23450;&#20998;&#24067;&#26469;&#26368;&#22823;&#21270;&#31038;&#20250;&#31119;&#21033;&#20934;&#21017;&#12290;&#22312;&#39034;&#24207;&#20915;&#31574;&#21338;&#24328;&#20013;&#65292;&#22266;&#23450;&#20998;&#24067;&#30001;Gibbs&#20998;&#24067;&#32473;&#20986;&#65292;&#30001;&#20110;&#35299;&#26512;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#24456;&#38590;&#23545;&#27835;&#30103;&#20998;&#37197;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#37319;&#29992;&#21464;&#20998;&#36817;&#20284;&#26469;&#20248;&#21270;&#22266;&#23450;&#20998;&#24067;&#65292;&#24182;&#20351;&#29992;&#36138;&#23146;&#20248;&#21270;&#31639;&#27861;&#26469;&#20248;&#21270;&#36817;&#20284;&#24179;&#34913;&#31119;&#21033;&#30340;&#27835;&#30103;&#20998;&#37197;&#12290;&#25105;&#20204;&#36890;&#36807;&#31119;&#21033;&#36951;&#25022;&#30028;&#38480;&#25512;&#23548;&#20102;&#21464;&#20998;&#36817;&#20284;&#30340;&#24615;&#33021;&#65292;&#23545;&#36138;&#23146;&#20248;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#23454;&#39564;&#20013;&#23454;&#29616;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing individualized allocation of treatments so as to maximize the equilibrium welfare of interacting agents has many policy-relevant applications. Focusing on sequential decision games of interacting agents, this paper develops a method to obtain optimal treatment assignment rules that maximize a social welfare criterion by evaluating stationary distributions of outcomes. Stationary distributions in sequential decision games are given by Gibbs distributions, which are difficult to optimize with respect to a treatment allocation due to analytical and computational complexity. We apply a variational approximation to the stationary distribution and optimize the approximated equilibrium welfare with respect to treatment allocation using a greedy optimization algorithm. We characterize the performance of the variational approximation, deriving a performance guarantee for the greedy optimization algorithm via a welfare regret bound. We implement our proposed method in simulation exerci
&lt;/p&gt;</description></item></channel></rss>