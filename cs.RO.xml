<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#26412;&#32508;&#36848;&#35770;&#25991;&#22238;&#39038;&#20102;40&#22810;&#31687;&#30740;&#31350;&#35770;&#25991;&#65292;&#24635;&#32467;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#33258;&#21160;&#39550;&#39542;&#22312;&#35268;&#21010;&#12289;&#20223;&#30495;&#21644;&#20851;&#38190;&#20219;&#21153;&#26041;&#38754;&#30340;&#37325;&#35201;&#36129;&#29486;&#65292;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21644;&#32763;&#35793;&#33021;&#21147;&#65292;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#22312;&#29289;&#20307;&#26816;&#27979;&#21644;&#39550;&#39542;&#22330;&#26223;&#21019;&#24314;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01105</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey for Foundation Models in Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#22238;&#39038;&#20102;40&#22810;&#31687;&#30740;&#31350;&#35770;&#25991;&#65292;&#24635;&#32467;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#33258;&#21160;&#39550;&#39542;&#22312;&#35268;&#21010;&#12289;&#20223;&#30495;&#21644;&#20851;&#38190;&#20219;&#21153;&#26041;&#38754;&#30340;&#37325;&#35201;&#36129;&#29486;&#65292;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21644;&#32763;&#35793;&#33021;&#21147;&#65292;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#22312;&#29289;&#20307;&#26816;&#27979;&#21644;&#39550;&#39542;&#22330;&#26223;&#21019;&#24314;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21457;&#29983;&#20102;&#38761;&#21629;&#65292;&#20026;&#33258;&#21160;&#39550;&#39542;&#24212;&#29992;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#23545;40&#22810;&#31687;&#30740;&#31350;&#35770;&#25991;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#65292;&#23637;&#31034;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#25552;&#21319;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#20316;&#29992;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#30340;&#35268;&#21010;&#21644;&#20223;&#30495;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20854;&#22312;&#25512;&#29702;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#32763;&#35793;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#22312;&#20851;&#38190;&#20219;&#21153;&#20013;&#24471;&#21040;&#36234;&#26469;&#36234;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#21644;&#36319;&#36394;&#65292;&#20197;&#21450;&#20026;&#20223;&#30495;&#21644;&#27979;&#35797;&#21019;&#24314;&#36924;&#30495;&#30340;&#39550;&#39542;&#22330;&#26223;&#12290;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#21487;&#20197;&#25972;&#21512;&#22810;&#26679;&#30340;&#36755;&#20837;&#65292;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#65292;&#23545;&#20110;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#32508;&#36848;&#19981;&#20165;&#25552;&#20379;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#20998;&#31867;&#65292;&#26681;&#25454;&#27169;&#24577;&#21644;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#20013;&#30340;&#21151;&#33021;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#65292;&#36824;&#28145;&#20837;&#30740;&#31350;&#20102;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of foundation models has revolutionized the fields of natural language processing and computer vision, paving the way for their application in autonomous driving (AD). This survey presents a comprehensive review of more than 40 research papers, demonstrating the role of foundation models in enhancing AD. Large language models contribute to planning and simulation in AD, particularly through their proficiency in reasoning, code generation and translation. In parallel, vision foundation models are increasingly adapted for critical tasks such as 3D object detection and tracking, as well as creating realistic driving scenarios for simulation and testing. Multi-modal foundation models, integrating diverse inputs, exhibit exceptional visual understanding and spatial reasoning, crucial for end-to-end AD. This survey not only provides a structured taxonomy, categorizing foundation models based on their modalities and functionalities within the AD domain but also delves into the meth
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#32806;&#21512;LiDAR-IMU-&#36718;&#37324;&#31243;&#35745;&#31639;&#27861;&#65292;&#20351;&#29992;&#22312;&#32447;&#26657;&#20934;&#35299;&#20915;&#28369;&#31227;&#36716;&#21521;&#26426;&#22120;&#20154;&#22312;&#25361;&#25112;&#24615;&#29615;&#22659;&#20013;&#30340;&#28857;&#20113;&#36864;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.02515</link><description>&lt;p&gt;
&#21033;&#29992;&#22312;&#32447;&#26657;&#20934;&#36816;&#21160;&#27169;&#22411;&#30340;&#32039;&#32806;&#21512;LiDAR-IMU-&#36718;&#37324;&#31243;&#35745;&#31639;&#27861;&#29992;&#20110;&#28369;&#31227;&#36716;&#21521;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Tightly-Coupled LiDAR-IMU-Wheel Odometry with Online Calibration of a Kinematic Model for Skid-Steering Robots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02515
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#32806;&#21512;LiDAR-IMU-&#36718;&#37324;&#31243;&#35745;&#31639;&#27861;&#65292;&#20351;&#29992;&#22312;&#32447;&#26657;&#20934;&#35299;&#20915;&#28369;&#31227;&#36716;&#21521;&#26426;&#22120;&#20154;&#22312;&#25361;&#25112;&#24615;&#29615;&#22659;&#20013;&#30340;&#28857;&#20113;&#36864;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38567;&#36947;&#21644;&#38271;&#24266;&#26159;&#31227;&#21160;&#26426;&#22120;&#20154;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#65292;&#22240;&#20026;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;LiDAR&#28857;&#20113;&#20250;&#36864;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#28857;&#20113;&#36864;&#21270;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#28369;&#31227;&#36716;&#21521;&#26426;&#22120;&#20154;&#30340;&#32039;&#32806;&#21512;LiDAR-IMU-&#36718;&#37324;&#31243;&#35745;&#31639;&#27861;&#65292;&#21516;&#26102;&#36824;&#20351;&#29992;&#22312;&#32447;&#26657;&#20934;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#32447;&#24615;&#36718;&#23376;&#37324;&#31243;&#35745;&#22240;&#23376;&#65292;&#19981;&#20165;&#20316;&#20026;&#36816;&#21160;&#32422;&#26463;&#65292;&#36824;&#21487;&#20197;&#25191;&#34892;&#28369;&#31227;&#36716;&#21521;&#26426;&#22120;&#20154;&#36816;&#21160;&#27169;&#22411;&#30340;&#22312;&#32447;&#26657;&#20934;&#12290;&#23613;&#31649;&#36816;&#21160;&#27169;&#22411;&#21160;&#24577;&#21464;&#21270;&#65288;&#20363;&#22914;&#30001;&#20110;&#32974;&#21387;&#24341;&#36215;&#30340;&#36718;&#32974;&#21322;&#24452;&#21464;&#21270;&#65289;&#21644;&#22320;&#24418;&#26465;&#20214;&#21464;&#21270;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#22312;&#32447;&#26657;&#20934;&#26469;&#35299;&#20915;&#27169;&#22411;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#36864;&#21270;&#29615;&#22659;&#19979;&#65288;&#22914;&#38271;&#30452;&#24266;&#65289;&#36890;&#36807;&#26657;&#20934;&#32780;&#23454;&#29616;&#20934;&#30830;&#23450;&#20301;&#65292;&#21516;&#26102;LiDAR-IMU&#34701;&#21512;&#36816;&#20316;&#33391;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20272;&#35745;&#20102;&#36718;&#23376;&#37324;&#31243;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#65288;&#21363;&#21327;&#26041;&#24046;&#30697;&#38453;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02515v1 Announce Type: cross  Abstract: Tunnels and long corridors are challenging environments for mobile robots because a LiDAR point cloud should degenerate in these environments. To tackle point cloud degeneration, this study presents a tightly-coupled LiDAR-IMU-wheel odometry algorithm with an online calibration for skid-steering robots. We propose a full linear wheel odometry factor, which not only serves as a motion constraint but also performs the online calibration of kinematic models for skid-steering robots. Despite the dynamically changing kinematic model (e.g., wheel radii changes caused by tire pressures) and terrain conditions, our method can address the model error via online calibration. Moreover, our method enables an accurate localization in cases of degenerated environments, such as long and straight corridors, by calibration while the LiDAR-IMU fusion sufficiently operates. Furthermore, we estimate the uncertainty (i.e., covariance matrix) of the wheel o
&lt;/p&gt;</description></item><item><title>FRAC-Q-Learning&#26159;&#19968;&#31181;&#19987;&#20026;&#31038;&#20132;&#26426;&#22120;&#20154;&#35774;&#35745;&#65292;&#33021;&#36991;&#20813;&#29992;&#25143;&#21388;&#28902;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#27604;&#20256;&#32479;&#31639;&#27861;&#22312;&#20852;&#36259;&#21644;&#21388;&#28902;&#31243;&#24230;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#26377;&#21161;&#20110;&#24320;&#21457;&#19981;&#20250;&#35753;&#29992;&#25143;&#24863;&#21040;&#26080;&#32842;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#12290;</title><link>https://arxiv.org/abs/2311.15327</link><description>&lt;p&gt;
FRAC-Q-Learning: &#19968;&#31181;&#20855;&#26377;&#36991;&#20813;&#21388;&#28902;&#36807;&#31243;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FRAC-Q-Learning: A Reinforcement Learning with Boredom Avoidance Processes for Social Robots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15327
&lt;/p&gt;
&lt;p&gt;
FRAC-Q-Learning&#26159;&#19968;&#31181;&#19987;&#20026;&#31038;&#20132;&#26426;&#22120;&#20154;&#35774;&#35745;&#65292;&#33021;&#36991;&#20813;&#29992;&#25143;&#21388;&#28902;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#27604;&#20256;&#32479;&#31639;&#27861;&#22312;&#20852;&#36259;&#21644;&#21388;&#28902;&#31243;&#24230;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#26377;&#21161;&#20110;&#24320;&#21457;&#19981;&#20250;&#35753;&#29992;&#25143;&#24863;&#21040;&#26080;&#32842;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#32463;&#24120;&#34987;&#24212;&#29992;&#20110;&#31038;&#20132;&#26426;&#22120;&#20154;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24182;&#26410;&#38024;&#23545;&#31038;&#20132;&#26426;&#22120;&#20154;&#36827;&#34892;&#20248;&#21270;&#65292;&#22240;&#27492;&#21487;&#33021;&#20250;&#35753;&#29992;&#25143;&#24863;&#21040;&#26080;&#32842;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#20026;&#31038;&#20132;&#26426;&#22120;&#20154;&#35774;&#35745;&#30340;&#26032;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;FRAC-Q-Learning&#65292;&#21487;&#20197;&#36991;&#20813;&#29992;&#25143;&#24863;&#21040;&#26080;&#32842;&#12290;&#35813;&#31639;&#27861;&#38500;&#20102;&#38543;&#26426;&#21270;&#21644;&#20998;&#31867;&#36807;&#31243;&#22806;&#65292;&#36824;&#21253;&#25324;&#19968;&#20010;&#36951;&#24536;&#36807;&#31243;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#19982;&#20256;&#32479;Q-Learning&#30340;&#27604;&#36739;&#35780;&#20272;&#20102;FRAC-Q-Learning&#30340;&#20852;&#36259;&#21644;&#21388;&#28902;&#31243;&#24230;&#20998;&#25968;&#12290;FRAC-Q-Learning&#26174;&#31034;&#20986;&#26126;&#26174;&#26356;&#39640;&#30340;&#20852;&#36259;&#20998;&#25968;&#36235;&#21183;&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20256;&#32479;Q-Learning&#26356;&#38590;&#35753;&#29992;&#25143;&#24863;&#21040;&#26080;&#32842;&#12290;&#22240;&#27492;&#65292;FRAC-Q-Learning&#26377;&#21161;&#20110;&#24320;&#21457;&#19981;&#20250;&#35753;&#29992;&#25143;&#24863;&#21040;&#26080;&#32842;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#12290;&#35813;&#31639;&#27861;&#36824;&#21487;&#20197;&#22312;&#22522;&#20110;Web&#30340;&#36890;&#20449;&#21644;&#25945;&#32946;&#20013;&#25214;&#21040;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15327v3 Announce Type: replace-cross  Abstract: The reinforcement learning algorithms have often been applied to social robots. However, most reinforcement learning algorithms were not optimized for the use of social robots, and consequently they may bore users. We proposed a new reinforcement learning method specialized for the social robot, the FRAC-Q-learning, that can avoid user boredom. The proposed algorithm consists of a forgetting process in addition to randomizing and categorizing processes. This study evaluated interest and boredom hardness scores of the FRAC-Q-learning by a comparison with the traditional Q-learning. The FRAC-Q-learning showed significantly higher trend of interest score, and indicated significantly harder to bore users compared to the traditional Q-learning. Therefore, the FRAC-Q-learning can contribute to develop a social robot that will not bore users. The proposed algorithm can also find applications in Web-based communication and educational 
&lt;/p&gt;</description></item><item><title>MimicTouch&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#27169;&#20223;&#20154;&#31867;&#30340;&#35302;&#35273;&#24341;&#23548;&#25511;&#21046;&#31574;&#30053;&#65292;&#36890;&#36807;&#25910;&#38598;&#26469;&#33258;&#20154;&#31867;&#31034;&#33539;&#32773;&#30340;&#22810;&#27169;&#24577;&#35302;&#35273;&#25968;&#25454;&#38598;&#65292;&#26469;&#23398;&#20064;&#24182;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.16917</link><description>&lt;p&gt;
MimicTouch: &#20351;&#29992;&#22810;&#27169;&#24577;&#35302;&#35273;&#21453;&#39304;&#23398;&#20064;&#20154;&#31867;&#30340;&#25511;&#21046;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
MimicTouch: Learning Human's Control Strategy with Multi-Modal Tactile Feedback. (arXiv:2310.16917v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16917
&lt;/p&gt;
&lt;p&gt;
MimicTouch&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#27169;&#20223;&#20154;&#31867;&#30340;&#35302;&#35273;&#24341;&#23548;&#25511;&#21046;&#31574;&#30053;&#65292;&#36890;&#36807;&#25910;&#38598;&#26469;&#33258;&#20154;&#31867;&#31034;&#33539;&#32773;&#30340;&#22810;&#27169;&#24577;&#35302;&#35273;&#25968;&#25454;&#38598;&#65292;&#26469;&#23398;&#20064;&#24182;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#25216;&#26415;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#35302;&#35273;&#22788;&#29702;&#30340;&#25972;&#21512;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#23398;&#20064;&#25191;&#34892;&#20687;&#23545;&#20934;&#21644;&#25554;&#20837;&#36825;&#26679;&#22797;&#26434;&#20219;&#21153;&#26102;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20381;&#36182;&#26426;&#22120;&#20154;&#36965;&#25805;&#20316;&#25968;&#25454;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#24573;&#35270;&#20102;&#20154;&#31867;&#21463;&#35302;&#35273;&#21453;&#39304;&#24341;&#23548;&#19979;&#30340;&#25511;&#21046;&#31574;&#30053;&#25152;&#25552;&#20379;&#30340;&#20016;&#23500;&#35265;&#35299;&#12290;&#20026;&#20102;&#21033;&#29992;&#20154;&#31867;&#24863;&#35273;&#65292;&#29616;&#26377;&#30340;&#20174;&#20154;&#31867;&#23398;&#20064;&#30340;&#26041;&#27861;&#20027;&#35201;&#21033;&#29992;&#35270;&#35273;&#21453;&#39304;&#65292;&#24120;&#24120;&#24573;&#35270;&#20102;&#20154;&#31867;&#26412;&#33021;&#22320;&#21033;&#29992;&#35302;&#35273;&#21453;&#39304;&#23436;&#25104;&#22797;&#26434;&#25805;&#20316;&#30340;&#23453;&#36149;&#32463;&#39564;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;"MimicTouch"&#65292;&#27169;&#20223;&#20154;&#31867;&#30340;&#35302;&#35273;&#24341;&#23548;&#25511;&#21046;&#31574;&#30053;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#20154;&#31867;&#31034;&#33539;&#32773;&#37027;&#37324;&#25910;&#38598;&#22810;&#27169;&#24577;&#35302;&#35273;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20154;&#31867;&#35302;&#35273;&#24341;&#23548;&#30340;&#25511;&#21046;&#31574;&#30053;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#25509;&#19979;&#26469;&#30340;&#27493;&#39588;&#28041;&#21450;&#25351;&#20196;&#30340;&#20256;&#36882;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#30340;&#35302;&#35273;&#24341;&#23548;&#31574;&#30053;&#26469;&#25191;&#34892;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In robotics and artificial intelligence, the integration of tactile processing is becoming increasingly pivotal, especially in learning to execute intricate tasks like alignment and insertion. However, existing works focusing on tactile methods for insertion tasks predominantly rely on robot teleoperation data and reinforcement learning, which do not utilize the rich insights provided by human's control strategy guided by tactile feedback. For utilizing human sensations, methodologies related to learning from humans predominantly leverage visual feedback, often overlooking the invaluable tactile feedback that humans inherently employ to finish complex manipulations. Addressing this gap, we introduce "MimicTouch", a novel framework that mimics human's tactile-guided control strategy. In this framework, we initially collect multi-modal tactile datasets from human demonstrators, incorporating human tactile-guided control strategies for task completion. The subsequent step involves instruc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;hGAIL&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#36710;&#36742;&#30340;&#33258;&#20027;&#23548;&#33322;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#24863;&#30693;&#20449;&#24687;&#30452;&#25509;&#26144;&#23556;&#21040;&#20302;&#32423;&#21160;&#20316;&#30340;&#21516;&#26102;&#65292;&#23398;&#20064;&#36710;&#36742;&#29615;&#22659;&#30340;&#20013;&#32423;&#36755;&#20837;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2302.04823</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#23618;&#29983;&#25104;&#23545;&#25239;&#27169;&#25311;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#22312;&#22478;&#24066;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Generative Adversarial Imitation Learning with Mid-level Input Generation for Autonomous Driving on Urban Environments. (arXiv:2302.04823v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;hGAIL&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#36710;&#36742;&#30340;&#33258;&#20027;&#23548;&#33322;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#24863;&#30693;&#20449;&#24687;&#30452;&#25509;&#26144;&#23556;&#21040;&#20302;&#32423;&#21160;&#20316;&#30340;&#21516;&#26102;&#65292;&#23398;&#20064;&#36710;&#36742;&#29615;&#22659;&#30340;&#20013;&#32423;&#36755;&#20837;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#29616;&#23454;&#20013;&#30340;&#22478;&#24066;&#23548;&#33322;&#22330;&#26223;&#65292;&#35774;&#35745;&#20581;&#22766;&#30340;&#25511;&#21046;&#31574;&#30053;&#24182;&#19981;&#26159;&#19968;&#39033;&#31616;&#21333;&#30340;&#20219;&#21153;&#12290;&#22312;&#31471;&#21040;&#31471;&#30340;&#26041;&#27861;&#20013;&#65292;&#36825;&#20123;&#31574;&#30053;&#24517;&#39035;&#23558;&#36710;&#36742;&#25668;&#20687;&#22836;&#33719;&#24471;&#30340;&#39640;&#32500;&#22270;&#20687;&#26144;&#23556;&#21040;&#20302;&#32423;&#21160;&#20316;&#65292;&#22914;&#36716;&#21521;&#21644;&#27833;&#38376;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;hGAIL&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#36710;&#36742;&#30340;&#33258;&#20027;&#23548;&#33322;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#24863;&#30693;&#20449;&#24687;&#30452;&#25509;&#26144;&#23556;&#21040;&#20302;&#32423;&#21160;&#20316;&#30340;&#21516;&#26102;&#65292;&#23398;&#20064;&#36710;&#36742;&#29615;&#22659;&#30340;&#20013;&#32423;&#36755;&#20837;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deriving robust control policies for realistic urban navigation scenarios is not a trivial task. In an end-to-end approach, these policies must map high-dimensional images from the vehicle's cameras to low-level actions such as steering and throttle. While pure Reinforcement Learning (RL) approaches are based exclusively on rewards,Generative Adversarial Imitation Learning (GAIL) agents learn from expert demonstrations while interacting with the environment, which favors GAIL on tasks for which a reward signal is difficult to derive. In this work, the hGAIL architecture was proposed to solve the autonomous navigation of a vehicle in an end-to-end approach, mapping sensory perceptions directly to low-level actions, while simultaneously learning mid-level input representations of the agent's environment. The proposed hGAIL consists of an hierarchical Adversarial Imitation Learning architecture composed of two main modules: the GAN (Generative Adversarial Nets) which generates the Bird's-
&lt;/p&gt;</description></item></channel></rss>