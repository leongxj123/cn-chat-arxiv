<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>SEAC&#26159;&#19968;&#31181;&#24377;&#24615;&#26102;&#38388;&#27493;&#38271;&#30340;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#36890;&#36807;&#21487;&#21464;&#25345;&#32493;&#26102;&#38388;&#30340;&#26102;&#38388;&#27493;&#38271;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#26681;&#25454;&#24773;&#20917;&#25913;&#21464;&#25511;&#21046;&#39057;&#29575;&#65292;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.14961</link><description>&lt;p&gt;
&#24377;&#24615;&#26102;&#38388;&#27493;&#38271;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Elastic Time Steps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14961
&lt;/p&gt;
&lt;p&gt;
SEAC&#26159;&#19968;&#31181;&#24377;&#24615;&#26102;&#38388;&#27493;&#38271;&#30340;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#36890;&#36807;&#21487;&#21464;&#25345;&#32493;&#26102;&#38388;&#30340;&#26102;&#38388;&#27493;&#38271;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#26681;&#25454;&#24773;&#20917;&#25913;&#21464;&#25511;&#21046;&#39057;&#29575;&#65292;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#36890;&#24120;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#23398;&#20064;&#20197;&#20197;&#22266;&#23450;&#25511;&#21046;&#39057;&#29575;&#25191;&#34892;&#21160;&#20316;&#30340;&#25511;&#21046;&#22120;&#12290;&#37492;&#20110;RL&#31639;&#27861;&#30340;&#31163;&#25955;&#24615;&#36136;&#65292;&#23427;&#20204;&#23545;&#25511;&#21046;&#39057;&#29575;&#30340;&#36873;&#25321;&#30340;&#24433;&#21709;&#35270;&#32780;&#19981;&#35265;&#65306;&#25214;&#21040;&#27491;&#30830;&#30340;&#25511;&#21046;&#39057;&#29575;&#21487;&#33021;&#24456;&#22256;&#38590;&#65292;&#38169;&#35823;&#24448;&#24448;&#20250;&#23548;&#33268;&#36807;&#24230;&#20351;&#29992;&#35745;&#31639;&#36164;&#28304;&#29978;&#33267;&#23548;&#33268;&#26080;&#27861;&#25910;&#25947;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36719;&#24377;&#24615;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;SEAC&#65289;, &#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;SEAC&#23454;&#29616;&#20102;&#24377;&#24615;&#26102;&#38388;&#27493;&#38271;&#65292;&#21363;&#20855;&#26377;&#24050;&#30693;&#21464;&#21270;&#25345;&#32493;&#26102;&#38388;&#30340;&#26102;&#38388;&#27493;&#38271;&#65292;&#20801;&#35768;&#20195;&#29702;&#26681;&#25454;&#24773;&#20917;&#25913;&#21464;&#20854;&#25511;&#21046;&#39057;&#29575;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;SEAC&#20165;&#22312;&#24517;&#35201;&#26102;&#24212;&#29992;&#25511;&#21046;&#65292;&#26368;&#23567;&#21270;&#35745;&#31639;&#36164;&#28304;&#21644;&#25968;&#25454;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;SEAC&#22312;&#29275;&#39039;&#36816;&#21160;&#23398;&#36855;&#23467;&#23548;&#33322;&#20219;&#21153;&#21644;&#19977;&#32500;&#36187;&#36710;&#35270;&#39057;&#28216;&#25103;Trackmania&#20013;&#30340;&#33021;&#21147;&#12290;SEAC&#22312;&#34920;&#29616;&#19978;&#20248;&#20110;SAC&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14961v1 Announce Type: cross  Abstract: Traditional Reinforcement Learning (RL) algorithms are usually applied in robotics to learn controllers that act with a fixed control rate. Given the discrete nature of RL algorithms, they are oblivious to the effects of the choice of control rate: finding the correct control rate can be difficult and mistakes often result in excessive use of computing resources or even lack of convergence.   We propose Soft Elastic Actor-Critic (SEAC), a novel off-policy actor-critic algorithm to address this issue. SEAC implements elastic time steps, time steps with a known, variable duration, which allow the agent to change its control frequency to adapt to the situation. In practice, SEAC applies control only when necessary, minimizing computational resources and data usage.   We evaluate SEAC's capabilities in simulation in a Newtonian kinematics maze navigation task and on a 3D racing video game, Trackmania. SEAC outperforms the SAC baseline in t
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#32570;&#20047;&#36890;&#29992;&#34892;&#20026;&#65292;&#38656;&#35201;&#21033;&#29992;&#32467;&#26500;&#21270;&#30693;&#35782;&#34920;&#31034;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#35748;&#30693;&#27867;&#21270;&#26694;&#26550;KIX&#65292;&#36890;&#36807;&#19982;&#23545;&#35937;&#30340;&#20132;&#20114;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#20132;&#20114;&#27010;&#24565;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#20419;&#36827;&#20102;&#30693;&#35782;&#19982;&#24378;&#21270;&#23398;&#20064;&#30340;&#34701;&#21512;&#65292;&#20026;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#33258;&#20027;&#21644;&#36890;&#29992;&#34892;&#20026;&#25552;&#20379;&#20102;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05346</link><description>&lt;p&gt;
KIX: &#19968;&#31181;&#20803;&#35748;&#30693;&#27867;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
KIX: A Metacognitive Generalization Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05346
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#32570;&#20047;&#36890;&#29992;&#34892;&#20026;&#65292;&#38656;&#35201;&#21033;&#29992;&#32467;&#26500;&#21270;&#30693;&#35782;&#34920;&#31034;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#35748;&#30693;&#27867;&#21270;&#26694;&#26550;KIX&#65292;&#36890;&#36807;&#19982;&#23545;&#35937;&#30340;&#20132;&#20114;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#20132;&#20114;&#27010;&#24565;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#20419;&#36827;&#20102;&#30693;&#35782;&#19982;&#24378;&#21270;&#23398;&#20064;&#30340;&#34701;&#21512;&#65292;&#20026;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#33258;&#20027;&#21644;&#36890;&#29992;&#34892;&#20026;&#25552;&#20379;&#20102;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21644;&#20854;&#20182;&#21160;&#29289;&#33021;&#22815;&#28789;&#27963;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#19988;&#33021;&#22815;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#21644;&#24212;&#29992;&#38271;&#26399;&#31215;&#32047;&#30340;&#39640;&#32423;&#30693;&#35782;&#26469;&#36866;&#24212;&#26032;&#39062;&#24773;&#22659;&#65292;&#36825;&#34920;&#29616;&#20102;&#19968;&#31181;&#27867;&#21270;&#26234;&#33021;&#34892;&#20026;&#12290;&#20294;&#26159;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26356;&#22810;&#22320;&#26159;&#19987;&#23478;&#65292;&#32570;&#20047;&#36825;&#31181;&#36890;&#29992;&#34892;&#20026;&#12290;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#38656;&#35201;&#29702;&#35299;&#21644;&#21033;&#29992;&#20851;&#38190;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#35748;&#30693;&#27867;&#21270;&#26694;&#26550;&#65292;&#31216;&#20026;Knowledge-Interaction-eXecution (KIX)&#65292;&#24182;&#19988;&#35748;&#20026;&#36890;&#36807;&#19982;&#23545;&#35937;&#30340;&#20132;&#20114;&#26469;&#21033;&#29992;&#31867;&#22411;&#31354;&#38388;&#21487;&#20197;&#20419;&#36827;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#20132;&#20114;&#27010;&#24565;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#26159;&#23558;&#30693;&#35782;&#34701;&#20837;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#31181;&#33258;&#28982;&#26041;&#24335;&#65292;&#24182;&#26377;&#26395;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#23454;&#29616;&#33258;&#20027;&#21644;&#36890;&#29992;&#34892;&#20026;&#30340;&#25512;&#24191;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans and other animals aptly exhibit general intelligence behaviors in solving a variety of tasks with flexibility and ability to adapt to novel situations by reusing and applying high level knowledge acquired over time. But artificial agents are more of a specialist, lacking such generalist behaviors. Artificial agents will require understanding and exploiting critical structured knowledge representations. We present a metacognitive generalization framework, Knowledge-Interaction-eXecution (KIX), and argue that interactions with objects leveraging type space facilitate the learning of transferable interaction concepts and generalization. It is a natural way of integrating knowledge into reinforcement learning and promising to act as an enabler for autonomous and generalist behaviors in artificial intelligence systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#23558;&#35270;&#39057;-&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#34892;&#20026;&#25209;&#35780;&#32773;&#29992;&#20110;&#25429;&#25417;&#19981;&#33391;&#20195;&#29702;&#34892;&#20026;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#35299;&#20915;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#24573;&#35270;&#20219;&#21153;&#32422;&#26463;&#21644;&#29992;&#25143;&#20559;&#22909;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.04210</link><description>&lt;p&gt;
&#8220;&#20219;&#21153;&#25104;&#21151;&#8221;&#36828;&#36828;&#19981;&#22815;&#65306;&#25506;&#31350;&#23558;&#35270;&#39057;-&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#34892;&#20026;&#25209;&#35780;&#32773;&#20197;&#25429;&#25417;&#19981;&#33391;&#20195;&#29702;&#34892;&#20026;&#30340;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
"Task Success" is not Enough: Investigating the Use of Video-Language Models as Behavior Critics for Catching Undesirable Agent Behaviors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#23558;&#35270;&#39057;-&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#34892;&#20026;&#25209;&#35780;&#32773;&#29992;&#20110;&#25429;&#25417;&#19981;&#33391;&#20195;&#29702;&#34892;&#20026;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#35299;&#20915;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#24573;&#35270;&#20219;&#21153;&#32422;&#26463;&#21644;&#29992;&#25143;&#20559;&#22909;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#34987;&#35777;&#26126;&#23545;&#20110;&#25277;&#26679;&#26377;&#24847;&#20041;&#30340;&#20505;&#36873;&#35299;&#20915;&#26041;&#26696;&#24456;&#26377;&#29992;&#65292;&#28982;&#32780;&#23427;&#20204;&#32463;&#24120;&#24573;&#35270;&#20219;&#21153;&#32422;&#26463;&#21644;&#29992;&#25143;&#20559;&#22909;&#12290;&#24403;&#27169;&#22411;&#19982;&#22806;&#37096;&#39564;&#35777;&#32773;&#32467;&#21512;&#65292;&#24182;&#26681;&#25454;&#39564;&#35777;&#21453;&#39304;&#36880;&#27493;&#25110;&#36880;&#28176;&#24471;&#20986;&#26368;&#32456;&#35299;&#20915;&#26041;&#26696;&#26102;&#65292;&#23427;&#20204;&#30340;&#23436;&#20840;&#33021;&#21147;&#26356;&#22909;&#22320;&#34987;&#21033;&#29992;&#12290;&#22312;&#20855;&#36523;&#21270;&#20154;&#24037;&#26234;&#33021;&#30340;&#32972;&#26223;&#19979;&#65292;&#39564;&#35777;&#36890;&#24120;&#20165;&#28041;&#21450;&#35780;&#20272;&#25351;&#20196;&#20013;&#25351;&#23450;&#30340;&#30446;&#26631;&#26465;&#20214;&#26159;&#21542;&#24050;&#28385;&#36275;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#23558;&#36825;&#20123;&#20195;&#29702;&#32773;&#26080;&#32541;&#22320;&#34701;&#20837;&#26085;&#24120;&#29983;&#27963;&#65292;&#24517;&#39035;&#32771;&#34385;&#21040;&#26356;&#24191;&#27867;&#30340;&#32422;&#26463;&#21644;&#20559;&#22909;&#65292;&#36229;&#36234;&#20165;&#20219;&#21153;&#25104;&#21151;&#65288;&#20363;&#22914;&#65292;&#26426;&#22120;&#20154;&#24212;&#35813;&#35880;&#24910;&#22320;&#25235;&#20303;&#38754;&#21253;&#65292;&#20197;&#36991;&#20813;&#26126;&#26174;&#30340;&#21464;&#24418;&#65289;&#12290;&#28982;&#32780;&#65292;&#37492;&#20110;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#26080;&#38480;&#33539;&#22260;&#65292;&#26500;&#24314;&#31867;&#20284;&#20110;&#29992;&#20110;&#26174;&#24335;&#30693;&#35782;&#20219;&#21153;&#65288;&#22914;&#22260;&#26827;&#21644;&#23450;&#29702;&#35777;&#26126;&#65289;&#30340;&#33050;&#26412;&#21270;&#39564;&#35777;&#22120;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#24403;&#27809;&#26377;&#21487;&#38752;&#30340;&#39564;&#35777;&#32773;&#21487;&#29992;&#26102;&#65292;&#20309;&#26102;&#21487;&#20197;&#20449;&#20219;&#20195;&#29702;&#30340;&#34892;&#20026;&#65311;
&lt;/p&gt;
&lt;p&gt;
Large-scale generative models are shown to be useful for sampling meaningful candidate solutions, yet they often overlook task constraints and user preferences. Their full power is better harnessed when the models are coupled with external verifiers and the final solutions are derived iteratively or progressively according to the verification feedback. In the context of embodied AI, verification often solely involves assessing whether goal conditions specified in the instructions have been met. Nonetheless, for these agents to be seamlessly integrated into daily life, it is crucial to account for a broader range of constraints and preferences beyond bare task success (e.g., a robot should grasp bread with care to avoid significant deformations). However, given the unbounded scope of robot tasks, it is infeasible to construct scripted verifiers akin to those used for explicit-knowledge tasks like the game of Go and theorem proving. This begs the question: when no sound verifier is avail
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#27169;&#25311;&#22270;&#20687;&#24341;&#23548;&#30340;&#20197;&#22806;&#31185;&#21307;&#29983;&#20026;&#20013;&#24515;&#30340;&#33258;&#20027;&#26426;&#22120;&#20154;&#23398;&#24466;&#31995;&#32479;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#20195;&#29702;&#22312;&#30524;&#31185;&#30333;&#20869;&#38556;&#25163;&#26415;&#20013;&#36866;&#24212;&#22806;&#31185;&#21307;&#29983;&#30340;&#25216;&#33021;&#27700;&#24179;&#21644;&#22806;&#31185;&#25163;&#26415;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2311.17693</link><description>&lt;p&gt;
&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#22806;&#31185;&#21307;&#29983;&#21442;&#19982;&#30524;&#31185;&#26426;&#22120;&#20154;&#23398;&#24466;&#31995;&#32479;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Toward a Surgeon-in-the-Loop Ophthalmic Robotic Apprentice using Reinforcement and Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17693
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#27169;&#25311;&#22270;&#20687;&#24341;&#23548;&#30340;&#20197;&#22806;&#31185;&#21307;&#29983;&#20026;&#20013;&#24515;&#30340;&#33258;&#20027;&#26426;&#22120;&#20154;&#23398;&#24466;&#31995;&#32479;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#20195;&#29702;&#22312;&#30524;&#31185;&#30333;&#20869;&#38556;&#25163;&#26415;&#20013;&#36866;&#24212;&#22806;&#31185;&#21307;&#29983;&#30340;&#25216;&#33021;&#27700;&#24179;&#21644;&#22806;&#31185;&#25163;&#26415;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#36741;&#21161;&#25163;&#26415;&#31995;&#32479;&#22312;&#25552;&#39640;&#25163;&#26415;&#31934;&#30830;&#24230;&#21644;&#20943;&#23569;&#20154;&#20026;&#38169;&#35823;&#26041;&#38754;&#23637;&#31034;&#20102;&#26174;&#33879;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#31995;&#32479;&#32570;&#20047;&#36866;&#24212;&#20010;&#21035;&#22806;&#31185;&#21307;&#29983;&#30340;&#29420;&#29305;&#20559;&#22909;&#21644;&#35201;&#27714;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20027;&#35201;&#38598;&#20013;&#22312;&#26222;&#36890;&#25163;&#26415;&#65288;&#22914;&#33145;&#33108;&#38236;&#25163;&#26415;&#65289;&#65292;&#19981;&#36866;&#29992;&#20110;&#38750;&#24120;&#31934;&#23494;&#30340;&#24494;&#21019;&#25163;&#26415;&#65292;&#22914;&#30524;&#31185;&#25163;&#26415;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#25311;&#22270;&#20687;&#24341;&#23548;&#30340;&#20197;&#22806;&#31185;&#21307;&#29983;&#20026;&#20013;&#24515;&#30340;&#33258;&#20027;&#26426;&#22120;&#20154;&#23398;&#24466;&#31995;&#32479;&#65292;&#21487;&#22312;&#30524;&#31185;&#30333;&#20869;&#38556;&#25163;&#26415;&#36807;&#31243;&#20013;&#36866;&#24212;&#20010;&#21035;&#22806;&#31185;&#21307;&#29983;&#30340;&#25216;&#33021;&#27700;&#24179;&#21644;&#39318;&#36873;&#22806;&#31185;&#25163;&#26415;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#27169;&#25311;&#29615;&#22659;&#26469;&#35757;&#32451;&#20197;&#22270;&#20687;&#25968;&#25454;&#20026;&#25351;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#20195;&#29702;&#65292;&#20197;&#25191;&#34892;&#30333;&#20869;&#38556;&#25163;&#26415;&#30340;&#20999;&#21475;&#38454;&#27573;&#25152;&#26377;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#22806;&#31185;&#21307;&#29983;&#30340;&#21160;&#20316;&#21644;&#20559;&#22909;&#25972;&#21512;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#35753;&#22806;&#31185;&#21307;&#29983;&#21442;&#19982;&#20854;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17693v2 Announce Type: replace-cross  Abstract: Robotic-assisted surgical systems have demonstrated significant potential in enhancing surgical precision and minimizing human errors. However, existing systems lack the ability to accommodate the unique preferences and requirements of individual surgeons. Additionally, they primarily focus on general surgeries (e.g., laparoscopy) and are not suitable for highly precise microsurgeries, such as ophthalmic procedures. Thus, we propose a simulation-based image-guided approach for surgeon-centered autonomous agents that can adapt to the individual surgeon's skill level and preferred surgical techniques during ophthalmic cataract surgery. Our approach utilizes a simulated environment to train reinforcement and imitation learning agents guided by image data to perform all tasks of the incision phase of cataract surgery. By integrating the surgeon's actions and preferences into the training process with the surgeon-in-the-loop, our ap
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#24847;&#26102;&#38388;&#25552;&#28860;&#26041;&#27861;&#65292;&#32467;&#21512;&#24658;&#23450;&#26102;&#38388;&#21160;&#20316;&#35268;&#21010;&#22120;&#65288;CTMP&#65289;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#25913;&#21892;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#26426;&#22120;&#20154;&#31995;&#32479;&#25317;&#26377;&#30340;&#22810;&#20313;&#35745;&#21010;&#26102;&#38388;&#26469;&#23436;&#21892;&#36816;&#21160;&#35268;&#21010;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.00837</link><description>&lt;p&gt;
&#20219;&#24847;&#26102;&#38388;&#25552;&#28860;&#30340;&#24658;&#23450;&#26102;&#38388;&#36816;&#21160;&#35268;&#21010;&#21644;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
Constant-time Motion Planning with Anytime Refinement for Manipulation. (arXiv:2311.00837v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00837
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#24847;&#26102;&#38388;&#25552;&#28860;&#26041;&#27861;&#65292;&#32467;&#21512;&#24658;&#23450;&#26102;&#38388;&#21160;&#20316;&#35268;&#21010;&#22120;&#65288;CTMP&#65289;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#25913;&#21892;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#26426;&#22120;&#20154;&#31995;&#32479;&#25317;&#26377;&#30340;&#22810;&#20313;&#35745;&#21010;&#26102;&#38388;&#26469;&#23436;&#21892;&#36816;&#21160;&#35268;&#21010;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#25805;&#20316;&#22120;&#23545;&#20110;&#26410;&#26469;&#30340;&#33258;&#20027;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#23545;&#20854;&#33258;&#20027;&#24615;&#30340;&#20449;&#20219;&#26377;&#38480;&#65292;&#23558;&#20854;&#38480;&#21046;&#20026;&#21018;&#24615;&#12289;&#20219;&#21153;&#29305;&#23450;&#30340;&#31995;&#32479;&#12290;&#25805;&#20316;&#22120;&#30340;&#22797;&#26434;&#37197;&#32622;&#31354;&#38388;&#65292;&#20197;&#21450;&#36991;&#38556;&#21644;&#32422;&#26463;&#28385;&#36275;&#30340;&#25361;&#25112;&#32463;&#24120;&#20351;&#24471;&#36816;&#21160;&#35268;&#21010;&#25104;&#20026;&#23454;&#29616;&#21487;&#38752;&#21644;&#36866;&#24212;&#24615;&#33258;&#20027;&#24615;&#30340;&#29942;&#39048;&#12290;&#26368;&#36817;&#65292;&#24341;&#20837;&#20102;&#19968;&#31867;&#24658;&#23450;&#26102;&#38388;&#36816;&#21160;&#35268;&#21010;&#22120;&#65288;CTMP&#65289;&#12290;&#36825;&#20123;&#35268;&#21010;&#22120;&#21033;&#29992;&#39044;&#22788;&#29702;&#38454;&#27573;&#35745;&#31639;&#25968;&#25454;&#32467;&#26500;&#65292;&#33021;&#22815;&#22312;&#29992;&#25143;&#23450;&#20041;&#30340;&#26102;&#38388;&#38480;&#21046;&#20869;&#29983;&#25104;&#36816;&#21160;&#35268;&#21010;&#65292;&#34429;&#28982;&#21487;&#33021;&#24182;&#19981;&#26159;&#26368;&#20248;&#35299;&#12290;&#36825;&#20010;&#26694;&#26550;&#22312;&#35768;&#22810;&#26102;&#38388;&#20851;&#38190;&#30340;&#20219;&#21153;&#20013;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#20154;&#31995;&#32479;&#36890;&#24120;&#26377;&#27604;CTMP&#25152;&#38656;&#30340;&#22312;&#32447;&#37096;&#20998;&#26356;&#22810;&#30340;&#35745;&#21010;&#26102;&#38388;&#65292;&#36825;&#20123;&#26102;&#38388;&#21487;&#20197;&#29992;&#26469;&#25913;&#21892;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;CTMP&#20013;&#19982;&#20219;&#24847;&#26102;&#38388;&#25552;&#28860;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic manipulators are essential for future autonomous systems, yet limited trust in their autonomy has confined them to rigid, task-specific systems. The intricate configuration space of manipulators, coupled with the challenges of obstacle avoidance and constraint satisfaction, often makes motion planning the bottleneck for achieving reliable and adaptable autonomy. Recently, a class of constant-time motion planners (CTMP) was introduced. These planners employ a preprocessing phase to compute data structures that enable online planning provably guarantee the ability to generate motion plans, potentially sub-optimal, within a user defined time bound. This framework has been demonstrated to be effective in a number of time-critical tasks. However, robotic systems often have more time allotted for planning than the online portion of CTMP requires, time that can be used to improve the solution. To this end, we propose an anytime refinement approach that works in combination with CTMP a
&lt;/p&gt;</description></item><item><title>CoFiI2P&#26159;&#19968;&#31181;&#31895;&#21040;&#31934;&#30340;&#22270;&#20687;&#21040;&#28857;&#20113;&#27880;&#20876;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20840;&#23616;&#20449;&#24687;&#21644;&#29305;&#24449;&#24314;&#31435;&#23545;&#24212;&#20851;&#31995;&#65292;&#23454;&#29616;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.14660</link><description>&lt;p&gt;
CoFiI2P: &#31895;&#21040;&#31934;&#30340;&#22270;&#20687;&#21040;&#28857;&#20113;&#27880;&#20876;&#30340;&#23545;&#24212;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
CoFiI2P: Coarse-to-Fine Correspondences for Image-to-Point Cloud Registration. (arXiv:2309.14660v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14660
&lt;/p&gt;
&lt;p&gt;
CoFiI2P&#26159;&#19968;&#31181;&#31895;&#21040;&#31934;&#30340;&#22270;&#20687;&#21040;&#28857;&#20113;&#27880;&#20876;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20840;&#23616;&#20449;&#24687;&#21644;&#29305;&#24449;&#24314;&#31435;&#23545;&#24212;&#20851;&#31995;&#65292;&#23454;&#29616;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#21040;&#28857;&#20113;&#65288;I2P&#65289;&#27880;&#20876;&#26159;&#26426;&#22120;&#20154;&#23548;&#33322;&#21644;&#31227;&#21160;&#24314;&#22270;&#39046;&#22495;&#20013;&#30340;&#19968;&#39033;&#22522;&#30784;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;I2P&#27880;&#20876;&#26041;&#27861;&#22312;&#28857;&#21040;&#20687;&#32032;&#32423;&#21035;&#19978;&#20272;&#35745;&#23545;&#24212;&#20851;&#31995;&#65292;&#24573;&#30053;&#20102;&#20840;&#23616;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#26469;&#33258;&#20840;&#23616;&#32422;&#26463;&#30340;&#39640;&#32423;&#24341;&#23548;&#30340;I2P&#21305;&#37197;&#23481;&#26131;&#25910;&#25947;&#21040;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;I2P&#27880;&#20876;&#32593;&#32476;CoFiI2P&#65292;&#36890;&#36807;&#31895;&#21040;&#31934;&#30340;&#26041;&#24335;&#25552;&#21462;&#23545;&#24212;&#20851;&#31995;&#65292;&#20197;&#24471;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#39318;&#20808;&#65292;&#23558;&#22270;&#20687;&#21644;&#28857;&#20113;&#36755;&#20837;&#21040;&#19968;&#20010;&#20849;&#20139;&#32534;&#30721;-&#35299;&#30721;&#32593;&#32476;&#20013;&#36827;&#34892;&#23618;&#27425;&#21270;&#29305;&#24449;&#25552;&#21462;&#12290;&#28982;&#21518;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#31895;&#21040;&#31934;&#30340;&#21305;&#37197;&#27169;&#22359;&#65292;&#21033;&#29992;&#29305;&#24449;&#24314;&#31435;&#31283;&#20581;&#30340;&#29305;&#24449;&#23545;&#24212;&#20851;&#31995;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#31895;&#21305;&#37197;&#22359;&#20013;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;I2P&#21464;&#25442;&#27169;&#22359;&#65292;&#20174;&#22270;&#20687;&#21644;&#28857;&#20113;&#20013;&#25429;&#25417;&#21516;&#36136;&#21644;&#24322;&#36136;&#30340;&#20840;&#23616;&#20449;&#24687;&#12290;&#36890;&#36807;&#21028;&#21035;&#25551;&#36848;&#23376;&#65292;&#23436;&#25104;&#31895;-&#32454;&#29305;&#24449;&#21305;&#37197;&#36807;&#31243;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#32454;&#21270;&#21305;&#37197;&#27169;&#22359;&#36827;&#19968;&#27493;&#25552;&#21319;&#23545;&#24212;&#20851;&#31995;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image-to-point cloud (I2P) registration is a fundamental task in the fields of robot navigation and mobile mapping. Existing I2P registration works estimate correspondences at the point-to-pixel level, neglecting the global alignment. However, I2P matching without high-level guidance from global constraints may converge to the local optimum easily. To solve the problem, this paper proposes CoFiI2P, a novel I2P registration network that extracts correspondences in a coarse-to-fine manner for the global optimal solution. First, the image and point cloud are fed into a Siamese encoder-decoder network for hierarchical feature extraction. Then, a coarse-to-fine matching module is designed to exploit features and establish resilient feature correspondences. Specifically, in the coarse matching block, a novel I2P transformer module is employed to capture the homogeneous and heterogeneous global information from image and point cloud. With the discriminate descriptors, coarse super-point-to-su
&lt;/p&gt;</description></item></channel></rss>