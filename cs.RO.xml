<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;Dyna-LfLH&#65292;&#36890;&#36807;&#23398;&#20064;&#24187;&#35273;&#20013;&#30340;&#21160;&#24577;&#29615;&#22659;&#65292;&#23433;&#20840;&#22320;&#23398;&#20064;&#22320;&#38754;&#26426;&#22120;&#20154;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#28789;&#27963;&#23548;&#33322;&#12290;</title><link>https://arxiv.org/abs/2403.17231</link><description>&lt;p&gt;
Dyna-LfLH:&#20174;&#23398;&#21040;&#30340;&#24187;&#35273;&#20013;&#23398;&#20250;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#23398;&#20064;&#28789;&#27963;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Dyna-LfLH: Learning Agile Navigation in Dynamic Environments from Learned Hallucination
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17231
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;Dyna-LfLH&#65292;&#36890;&#36807;&#23398;&#20064;&#24187;&#35273;&#20013;&#30340;&#21160;&#24577;&#29615;&#22659;&#65292;&#23433;&#20840;&#22320;&#23398;&#20064;&#22320;&#38754;&#26426;&#22120;&#20154;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#28789;&#27963;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23433;&#20840;&#22320;&#23398;&#20064;&#22320;&#38754;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#35268;&#21010;&#22120;&#65292;&#20197;&#22312;&#23494;&#38598;&#19988;&#21160;&#24577;&#30340;&#38556;&#30861;&#29289;&#29615;&#22659;&#20013;&#23548;&#33322;&#12290;&#38024;&#23545;&#39640;&#24230;&#28151;&#20081;&#12289;&#24555;&#36895;&#31227;&#21160;&#12289;&#38590;&#20197;&#39044;&#27979;&#30340;&#38556;&#30861;&#29289;&#65292;&#20256;&#32479;&#30340;&#36816;&#21160;&#35268;&#21010;&#22120;&#21487;&#33021;&#26080;&#27861;&#36319;&#19978;&#26377;&#38480;&#30340;&#26426;&#36733;&#35745;&#31639;&#12290;&#23545;&#20110;&#22522;&#20110;&#23398;&#20064;&#30340;&#35268;&#21010;&#22120;&#65292;&#24456;&#38590;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#28436;&#31034;&#20197;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#21516;&#26102;&#24378;&#21270;&#23398;&#20064;&#22312;&#25506;&#32034;&#36807;&#31243;&#20013;&#30001;&#20110;&#39640;&#30896;&#25758;&#27010;&#29575;&#32780;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#20102;&#23433;&#20840;&#26377;&#25928;&#22320;&#25552;&#20379;&#35757;&#32451;&#25968;&#25454;&#65292;LfH&#26041;&#27861;&#22522;&#20110;&#36807;&#21435;&#25104;&#21151;&#30340;&#23548;&#33322;&#32463;&#39564;&#22312;&#30456;&#23545;&#31616;&#21333;&#25110;&#23436;&#20840;&#24320;&#25918;&#30340;&#29615;&#22659;&#20013;&#32508;&#21512;&#22256;&#38590;&#30340;&#23548;&#33322;&#29615;&#22659;&#65292;&#20294;&#36951;&#25022;&#30340;&#26159;&#26080;&#27861;&#35299;&#20915;&#21160;&#24577;&#38556;&#30861;&#29289;&#38382;&#39064;&#12290;&#22312;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;Dyna-LfLH&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#23398;&#20064;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28508;&#22312;&#20998;&#24067;&#21644;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17231v1 Announce Type: cross  Abstract: This paper presents a self-supervised learning method to safely learn a motion planner for ground robots to navigate environments with dense and dynamic obstacles. When facing highly-cluttered, fast-moving, hard-to-predict obstacles, classical motion planners may not be able to keep up with limited onboard computation. For learning-based planners, high-quality demonstrations are difficult to acquire for imitation learning while reinforcement learning becomes inefficient due to the high probability of collision during exploration. To safely and efficiently provide training data, the Learning from Hallucination (LfH) approaches synthesize difficult navigation environments based on past successful navigation experiences in relatively easy or completely open ones, but unfortunately cannot address dynamic obstacles. In our new Dynamic Learning from Learned Hallucination (Dyna-LfLH), we design and learn a novel latent distribution and sample
&lt;/p&gt;</description></item></channel></rss>