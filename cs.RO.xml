<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#20998;&#31867;&#21644;&#35282;&#33394;&#20998;&#26512;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2404.00282</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#30340;&#35843;&#26597;:&#27010;&#24565;&#12289;&#20998;&#31867;&#21644;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00282
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#20998;&#31867;&#21644;&#35282;&#33394;&#20998;&#26512;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25317;&#26377;&#24191;&#27867;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#21644;&#39640;&#32423;&#36890;&#29992;&#33021;&#21147;&#65292;&#23427;&#20204;&#22312;&#22686;&#24378;&#23398;&#20064;&#26041;&#38754;&#22914;&#22810;&#20219;&#21153;&#23398;&#20064;&#12289;&#26679;&#26412;&#25928;&#29575;&#21644;&#20219;&#21153;&#35268;&#21010;&#31561;&#26041;&#38754;&#23637;&#29616;&#20986;&#28508;&#21147;&#12290;&#26412;&#35843;&#26597;&#32508;&#36848;&#20102;&#29616;&#26377;$\textit{LLM&#22686;&#24378;RL}$&#25991;&#29486;&#65292;&#24635;&#32467;&#20102;&#20854;&#19982;&#20256;&#32479;RL&#26041;&#27861;&#30340;&#29305;&#24449;&#65292;&#26088;&#22312;&#28548;&#28165;&#30740;&#31350;&#33539;&#22260;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#21033;&#29992;&#32463;&#20856;&#30340;Agent-&#29615;&#22659;&#20132;&#20114;&#33539;&#20363;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#20998;&#31867;&#27861;&#65292;&#31995;&#32479;&#22320;&#23558;LLMs&#22312;RL&#20013;&#30340;&#21151;&#33021;&#20998;&#31867;&#65292;&#21253;&#25324;&#22235;&#31181;&#35282;&#33394;&#65306;&#20449;&#24687;&#22788;&#29702;&#22120;&#12289;&#22870;&#21169;&#35774;&#35745;&#32773;&#12289;&#20915;&#31574;&#32773;&#21644;&#29983;&#25104;&#22120;&#12290;&#27492;&#22806;&#65292;&#38024;&#23545;&#27599;&#20010;&#35282;&#33394;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#26041;&#27861;&#35770;&#65292;&#20998;&#26512;&#20102;&#32531;&#35299;&#30340;&#29305;&#23450;RL&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#26041;&#21521;&#30340;&#35265;&#35299;&#12290;&#26368;&#21518;&#65292;&#28508;&#22312;&#24212;&#29992;&#12289;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00282v1 Announce Type: cross  Abstract: With extensive pre-trained knowledge and high-level general capabilities, large language models (LLMs) emerge as a promising avenue to augment reinforcement learning (RL) in aspects such as multi-task learning, sample efficiency, and task planning. In this survey, we provide a comprehensive review of the existing literature in $\textit{LLM-enhanced RL}$ and summarize its characteristics compared to conventional RL methods, aiming to clarify the research scope and directions for future studies. Utilizing the classical agent-environment interaction paradigm, we propose a structured taxonomy to systematically categorize LLMs' functionalities in RL, including four roles: information processor, reward designer, decision-maker, and generator. Additionally, for each role, we summarize the methodologies, analyze the specific RL challenges that are mitigated, and provide insights into future directions. Lastly, potential applications, prospecti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;D3G&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#22810;&#26426;&#22120;&#20154;&#21327;&#35843;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;&#36712;&#36857;&#19982;&#28436;&#31034;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65292;&#27599;&#20010;&#26426;&#22120;&#20154;&#21487;&#20197;&#33258;&#21160;&#35843;&#25972;&#20854;&#20010;&#20307;&#21160;&#24577;&#21644;&#30446;&#26631;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2207.08892</link><description>&lt;p&gt;
D3G: &#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#22810;&#26426;&#22120;&#20154;&#21327;&#35843;
&lt;/p&gt;
&lt;p&gt;
D3G: Learning Multi-robot Coordination from Demonstrations. (arXiv:2207.08892v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.08892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;D3G&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#22810;&#26426;&#22120;&#20154;&#21327;&#35843;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;&#36712;&#36857;&#19982;&#28436;&#31034;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65292;&#27599;&#20010;&#26426;&#22120;&#20154;&#21487;&#20197;&#33258;&#21160;&#35843;&#25972;&#20854;&#20010;&#20307;&#21160;&#24577;&#21644;&#30446;&#26631;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#21487;&#24494;&#21160;&#24577;&#28216;&#25103;&#65288;D3G&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#22810;&#26426;&#22120;&#20154;&#21327;&#35843;&#12290;&#25105;&#20204;&#23558;&#22810;&#26426;&#22120;&#20154;&#21327;&#35843;&#34920;&#31034;&#20026;&#19968;&#20010;&#21160;&#24577;&#28216;&#25103;&#65292;&#20854;&#20013;&#19968;&#20010;&#26426;&#22120;&#20154;&#30340;&#34892;&#20026;&#21463;&#20854;&#33258;&#36523;&#21160;&#24577;&#21644;&#30446;&#26631;&#30340;&#25511;&#21046;&#65292;&#21516;&#26102;&#20063;&#21462;&#20915;&#20110;&#20854;&#20182;&#26426;&#22120;&#20154;&#30340;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#35843;&#25972;&#27599;&#20010;&#26426;&#22120;&#20154;&#30340;&#30446;&#26631;&#21644;&#21160;&#24577;&#65292;&#21487;&#20197;&#36866;&#24212;&#21327;&#35843;&#12290;&#25152;&#25552;&#20986;&#30340;D3G&#20351;&#27599;&#20010;&#26426;&#22120;&#20154;&#36890;&#36807;&#26368;&#23567;&#21270;&#20854;&#36712;&#36857;&#19982;&#28436;&#31034;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65292;&#22312;&#20998;&#24067;&#24335;&#26041;&#24335;&#19979;&#33258;&#21160;&#35843;&#25972;&#20854;&#20010;&#20307;&#21160;&#24577;&#21644;&#30446;&#26631;&#12290;&#35813;&#23398;&#20064;&#26694;&#26550;&#20855;&#26377;&#26032;&#30340;&#35774;&#35745;&#65292;&#21253;&#25324;&#19968;&#20010;&#21069;&#21521;&#20256;&#36882;&#65292;&#25152;&#26377;&#26426;&#22120;&#20154;&#21512;&#20316;&#23547;&#25214;&#28216;&#25103;&#30340;&#32435;&#20160;&#22343;&#34913;&#65292;&#20197;&#21450;&#19968;&#20010;&#21453;&#21521;&#20256;&#36882;&#65292;&#22312;&#36890;&#20449;&#22270;&#20013;&#20256;&#25773;&#26799;&#24230;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#20013;&#27979;&#35797;&#20102;D3G&#65292;&#24182;&#32473;&#20986;&#20102;&#19981;&#21516;&#20219;&#21153;&#37197;&#32622;&#30340;&#20004;&#31181;&#26426;&#22120;&#20154;&#12290;&#32467;&#26524;&#35777;&#26126;&#20102;D3G&#23398;&#20064;&#22810;&#26426;&#22120;&#20154;&#21327;&#35843;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper develops a Distributed Differentiable Dynamic Game (D3G) framework, which enables learning multi-robot coordination from demonstrations. We represent multi-robot coordination as a dynamic game, where the behavior of a robot is dictated by its own dynamics and objective that also depends on others' behavior. The coordination thus can be adapted by tuning the objective and dynamics of each robot. The proposed D3G enables each robot to automatically tune its individual dynamics and objectives in a distributed manner by minimizing the mismatch between its trajectory and demonstrations. This learning framework features a new design, including a forward-pass, where all robots collaboratively seek Nash equilibrium of a game, and a backward-pass, where gradients are propagated via the communication graph. We test the D3G in simulation with two types of robots given different task configurations. The results validate the capability of D3G for learning multi-robot coordination from de
&lt;/p&gt;</description></item></channel></rss>