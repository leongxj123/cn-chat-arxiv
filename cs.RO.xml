<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#20154;&#20301;&#32622;&#25512;&#27979;&#32593;&#32476;(RPSN)&#65292;&#36890;&#36807;&#23558;&#21487;&#24494;&#20998;&#30340;&#36870;&#36816;&#21160;&#23398;&#31639;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#65292;&#33021;&#22815;&#39640;&#25104;&#21151;&#29575;&#22320;&#25512;&#27979;&#31227;&#21160;&#25805;&#20316;&#22120;&#26800;&#30340;&#20301;&#32622;&#12290;</title><link>https://arxiv.org/abs/2402.16281</link><description>&lt;p&gt;
&#26397;&#30528;&#25935;&#25463;&#26426;&#22120;&#20154;&#65306;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#30452;&#35266;&#30340;&#26426;&#22120;&#20154;&#20301;&#32622;&#25512;&#27979;
&lt;/p&gt;
&lt;p&gt;
Towards Agile Robots: Intuitive Robot Position Speculation with Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#20154;&#20301;&#32622;&#25512;&#27979;&#32593;&#32476;(RPSN)&#65292;&#36890;&#36807;&#23558;&#21487;&#24494;&#20998;&#30340;&#36870;&#36816;&#21160;&#23398;&#31639;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#65292;&#33021;&#22815;&#39640;&#25104;&#21151;&#29575;&#22320;&#25512;&#27979;&#31227;&#21160;&#25805;&#20316;&#22120;&#26800;&#30340;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#20301;&#32622;&#25512;&#27979;&#26159;&#25511;&#21046;&#31227;&#21160;&#25805;&#20316;&#22120;&#26800;&#30340;&#20851;&#38190;&#27493;&#39588;&#20043;&#19968;&#65292;&#20197;&#30830;&#23450;&#24213;&#30424;&#24212;&#35813;&#31227;&#21160;&#21040;&#21738;&#37324;&#12290;&#20026;&#20102;&#28385;&#36275;&#25935;&#25463;&#26426;&#22120;&#20154;&#25216;&#26415;&#30340;&#38656;&#27714;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26426;&#22120;&#20154;&#20301;&#32622;&#25512;&#27979;&#32593;&#32476;(RPSN)&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#31227;&#21160;&#25805;&#20316;&#22120;&#26800;&#30340;&#25935;&#25463;&#24615;&#12290;RPSN&#23558;&#21487;&#24494;&#20998;&#30340;&#36870;&#36816;&#21160;&#23398;&#31639;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;RPSN&#33021;&#22815;&#39640;&#25104;&#21151;&#29575;&#22320;&#25512;&#27979;&#20301;&#32622;&#12290;&#25105;&#20204;&#23558;RPSN&#24212;&#29992;&#20110;&#20998;&#35299;&#26411;&#26399;&#30005;&#21160;&#27773;&#36710;&#30005;&#27744;&#30340;&#31227;&#21160;&#25805;&#20316;&#22120;&#26800;&#12290;&#22312;&#21508;&#31181;&#27169;&#25311;&#29615;&#22659;&#21644;&#23454;&#38469;&#31227;&#21160;&#25805;&#20316;&#22120;&#26800;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;RPSN&#25552;&#20379;&#30340;&#21021;&#22987;&#20301;&#32622;&#21487;&#33021;&#26159;&#29702;&#24819;&#20301;&#32622;&#30340;&#27010;&#29575;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16281v1 Announce Type: cross  Abstract: The robot position speculation, which determines where the chassis should move, is one key step to control the mobile manipulators. The target position must ensure the feasibility of chassis movement and manipulability, which is guaranteed by randomized sampling and kinematic checking in traditional methods. Addressing the demands of agile robotics, this paper proposes a robot position speculation network(RPSN), a learning-based approach to enhance the agility of mobile manipulators. The RPSN incorporates a differentiable inverse kinematic algorithm and a neural network. Through end-to-end training, the RPSN can speculate positions with a high success rate. We apply the RPSN to mobile manipulators disassembling end-of-life electric vehicle batteries (EOL-EVBs). Extensive experiments on various simulated environments and physical mobile manipulators demonstrate that the probability of the initial position provided by RPSN being the idea
&lt;/p&gt;</description></item><item><title>DiffTOP&#20351;&#29992;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#20316;&#20026;&#31574;&#30053;&#34920;&#31034;&#26469;&#29983;&#25104;&#21160;&#20316;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#8220;&#30446;&#26631;&#19981;&#21305;&#37197;&#8221;&#38382;&#39064;&#65292;&#24182;&#22312;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>https://arxiv.org/abs/2402.05421</link><description>&lt;p&gt;
DiffTOP: &#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#22312;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
DiffTOP: Differentiable Trajectory Optimization for Deep Reinforcement and Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05421
&lt;/p&gt;
&lt;p&gt;
DiffTOP&#20351;&#29992;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#20316;&#20026;&#31574;&#30053;&#34920;&#31034;&#26469;&#29983;&#25104;&#21160;&#20316;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#8220;&#30446;&#26631;&#19981;&#21305;&#37197;&#8221;&#38382;&#39064;&#65292;&#24182;&#22312;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DiffTOP&#65292;&#23427;&#21033;&#29992;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#20316;&#20026;&#31574;&#30053;&#34920;&#31034;&#65292;&#20026;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#29983;&#25104;&#21160;&#20316;&#12290;&#36712;&#36857;&#20248;&#21270;&#26159;&#19968;&#31181;&#22312;&#25511;&#21046;&#39046;&#22495;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#31639;&#27861;&#65292;&#30001;&#25104;&#26412;&#21644;&#21160;&#21147;&#23398;&#20989;&#25968;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#21033;&#29992;&#20102;&#26368;&#36817;&#22312;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#20351;&#24471;&#21487;&#20197;&#35745;&#31639;&#25439;&#22833;&#23545;&#20110;&#36712;&#36857;&#20248;&#21270;&#30340;&#21442;&#25968;&#30340;&#26799;&#24230;&#12290;&#22240;&#27492;&#65292;&#36712;&#36857;&#20248;&#21270;&#30340;&#25104;&#26412;&#21644;&#21160;&#21147;&#23398;&#20989;&#25968;&#21487;&#20197;&#31471;&#21040;&#31471;&#22320;&#23398;&#20064;&#12290;DiffTOP&#35299;&#20915;&#20102;&#20043;&#21069;&#27169;&#22411;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#8220;&#30446;&#26631;&#19981;&#21305;&#37197;&#8221;&#38382;&#39064;&#65292;&#22240;&#20026;DiffTOP&#20013;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#36890;&#36807;&#36712;&#36857;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#31574;&#30053;&#26799;&#24230;&#25439;&#22833;&#30452;&#25509;&#26368;&#22823;&#21270;&#20219;&#21153;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#23545;DiffTOP&#22312;&#26631;&#20934;&#26426;&#22120;&#20154;&#25805;&#32437;&#20219;&#21153;&#22871;&#20214;&#20013;&#36827;&#34892;&#20102;&#27169;&#20223;&#23398;&#20064;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces DiffTOP, which utilizes Differentiable Trajectory OPtimization as the policy representation to generate actions for deep reinforcement and imitation learning. Trajectory optimization is a powerful and widely used algorithm in control, parameterized by a cost and a dynamics function. The key to our approach is to leverage the recent progress in differentiable trajectory optimization, which enables computing the gradients of the loss with respect to the parameters of trajectory optimization. As a result, the cost and dynamics functions of trajectory optimization can be learned end-to-end. DiffTOP addresses the ``objective mismatch'' issue of prior model-based RL algorithms, as the dynamics model in DiffTOP is learned to directly maximize task performance by differentiating the policy gradient loss through the trajectory optimization process. We further benchmark DiffTOP for imitation learning on standard robotic manipulation task suites with high-dimensional sensory
&lt;/p&gt;</description></item></channel></rss>