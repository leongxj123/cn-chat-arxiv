<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>CtRL-Sim&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#21453;&#24212;&#24615;&#21644;&#21487;&#25511;&#20132;&#36890;&#20195;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;Nocturne&#27169;&#25311;&#22120;&#20013;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#30340;&#39550;&#39542;&#25968;&#25454;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.19918</link><description>&lt;p&gt;
CtRL-Sim&#65306;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#21453;&#24212;&#24615;&#21487;&#25511;&#39550;&#39542;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
CtRL-Sim: Reactive and Controllable Driving Agents with Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19918
&lt;/p&gt;
&lt;p&gt;
CtRL-Sim&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#21453;&#24212;&#24615;&#21644;&#21487;&#25511;&#20132;&#36890;&#20195;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;Nocturne&#27169;&#25311;&#22120;&#20013;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#30340;&#39550;&#39542;&#25968;&#25454;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CtRL-Sim&#65292;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#22686;&#24378;&#30340;Nocturne&#27169;&#25311;&#22120;&#20013;&#30340;&#22238;&#25253;&#26465;&#20214;&#21270;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26469;&#39640;&#25928;&#29983;&#25104;&#21453;&#24212;&#24615;&#21644;&#21487;&#25511;&#20132;&#36890;&#20195;&#29702;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;Nocturne&#27169;&#25311;&#22120;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#30340;&#39550;&#39542;&#25968;&#25454;&#65292;&#20197;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#31163;&#32447;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19918v1 Announce Type: cross  Abstract: Evaluating autonomous vehicle stacks (AVs) in simulation typically involves replaying driving logs from real-world recorded traffic. However, agents replayed from offline data do not react to the actions of the AV, and their behaviour cannot be easily controlled to simulate counterfactual scenarios. Existing approaches have attempted to address these shortcomings by proposing methods that rely on heuristics or learned generative models of real-world data but these approaches either lack realism or necessitate costly iterative sampling procedures to control the generated behaviours. In this work, we take an alternative approach and propose CtRL-Sim, a method that leverages return-conditioned offline reinforcement learning within a physics-enhanced Nocturne simulator to efficiently generate reactive and controllable traffic agents. Specifically, we process real-world driving data through the Nocturne simulator to generate a diverse offli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#24494;&#20998;&#20223;&#30495;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22797;&#26434;&#30340;&#20840;&#36523;&#20223;&#30495;&#35299;&#32806;&#20026;&#20004;&#20010;&#21333;&#29420;&#30340;&#36830;&#32493;&#22495;&#65292;&#24182;&#19982;&#26356;&#31934;&#30830;&#30340;&#27169;&#22411;&#23545;&#40784;&#65292;&#26469;&#20811;&#26381;&#22235;&#36275;&#21160;&#20316;&#20013;&#30340;&#19981;&#36830;&#32493;&#24615;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.14864</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#24494;&#20998;&#20223;&#30495;&#23398;&#20064;&#22235;&#36275;&#21160;&#20316;
&lt;/p&gt;
&lt;p&gt;
Learning Quadruped Locomotion Using Differentiable Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#24494;&#20998;&#20223;&#30495;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22797;&#26434;&#30340;&#20840;&#36523;&#20223;&#30495;&#35299;&#32806;&#20026;&#20004;&#20010;&#21333;&#29420;&#30340;&#36830;&#32493;&#22495;&#65292;&#24182;&#19982;&#26356;&#31934;&#30830;&#30340;&#27169;&#22411;&#23545;&#40784;&#65292;&#26469;&#20811;&#26381;&#22235;&#36275;&#21160;&#20316;&#20013;&#30340;&#19981;&#36830;&#32493;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#37096;&#20998;&#26426;&#22120;&#20154;&#36816;&#21160;&#25511;&#21046;&#30340;&#36827;&#23637;&#37117;&#26159;&#30001;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#21487;&#24494;&#20998;&#20223;&#30495;&#30340;&#28508;&#21147;&#12290;&#21487;&#24494;&#20998;&#20223;&#30495;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#20154;&#27169;&#22411;&#35745;&#31639;&#20302;&#21464;&#24322;&#19968;&#38454;&#26799;&#24230;&#65292;&#25215;&#35834;&#20102;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#65292;&#20294;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#20854;&#22312;&#22235;&#36275;&#26426;&#22120;&#20154;&#25511;&#21046;&#26041;&#38754;&#30340;&#24212;&#29992;&#20173;&#28982;&#26377;&#38480;&#12290;&#21487;&#24494;&#20998;&#20223;&#30495;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#30001;&#20110;&#25509;&#35302;&#20016;&#23500;&#29615;&#22659;&#65288;&#22914;&#22235;&#36275;&#21160;&#20316;&#65289;&#20013;&#30340;&#19981;&#36830;&#32493;&#24615;&#65292;&#23548;&#33268;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#22797;&#26434;&#20248;&#21270;&#26223;&#35266;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21487;&#24494;&#20998;&#20223;&#30495;&#26694;&#26550;&#20197;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;&#20851;&#38190;&#24819;&#27861;&#21253;&#25324;&#23558;&#21487;&#33021;&#30001;&#20110;&#25509;&#35302;&#32780;&#20986;&#29616;&#19981;&#36830;&#32493;&#24615;&#30340;&#22797;&#26434;&#20840;&#36523;&#20223;&#30495;&#35299;&#32806;&#20026;&#20004;&#20010;&#21333;&#29420;&#30340;&#36830;&#32493;&#22495;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23558;&#31616;&#21270;&#27169;&#22411;&#20135;&#29983;&#30340;&#26426;&#22120;&#20154;&#29366;&#24577;&#19982;&#26356;&#31934;&#30830;&#30340;&#19981;&#21487;&#24494;&#20998;&#27169;&#22411;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14864v1 Announce Type: cross  Abstract: While most recent advancements in legged robot control have been driven by model-free reinforcement learning, we explore the potential of differentiable simulation. Differentiable simulation promises faster convergence and more stable training by computing low-variant first-order gradients using the robot model, but so far, its use for legged robot control has remained limited to simulation. The main challenge with differentiable simulation lies in the complex optimization landscape of robotic tasks due to discontinuities in contact-rich environments, e.g., quadruped locomotion. This work proposes a new, differentiable simulation framework to overcome these challenges. The key idea involves decoupling the complex whole-body simulation, which may exhibit discontinuities due to contact, into two separate continuous domains. Subsequently, we align the robot state resulting from the simplified model with a more precise, non-differentiable 
&lt;/p&gt;</description></item></channel></rss>