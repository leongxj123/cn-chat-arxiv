<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#31995;&#32479;&#24615;&#22320;&#25193;&#23637;&#23460;&#20869;&#29615;&#22659;&#20013;&#30340;3D&#35270;&#35273;-&#35821;&#35328;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#39318;&#20010;&#30334;&#19975;&#35268;&#27169;&#30340;3D&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#38598;SceneVerse&#65292;&#20197;&#35299;&#20915;3D&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#38754;&#20020;&#30340;&#20960;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.09340</link><description>&lt;p&gt;
SceneVerse&#65306;&#20026;&#22522;&#20110;&#22330;&#26223;&#30340;&#22330;&#26223;&#29702;&#35299;&#25193;&#23637;3D&#35270;&#35273;-&#35821;&#35328;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.09340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#31995;&#32479;&#24615;&#22320;&#25193;&#23637;&#23460;&#20869;&#29615;&#22659;&#20013;&#30340;3D&#35270;&#35273;-&#35821;&#35328;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#39318;&#20010;&#30334;&#19975;&#35268;&#27169;&#30340;3D&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#38598;SceneVerse&#65292;&#20197;&#35299;&#20915;3D&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#38754;&#20020;&#30340;&#20960;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#65292;&#21363;&#23558;&#35821;&#35328;&#19982;3D&#29289;&#29702;&#29615;&#22659;&#23545;&#40784;&#65292;&#26159;&#21457;&#23637;&#20855;&#36523;&#20307;&#33021;&#21147;&#30340;&#26234;&#33021;&#20307;&#30340;&#22522;&#30707;&#12290;&#19982;2D&#39046;&#22495;&#26368;&#36817;&#30340;&#36827;&#23637;&#30456;&#27604;&#65292;&#23558;&#35821;&#35328;&#19982;3D&#22330;&#26223;&#23545;&#40784;&#38754;&#20020;&#30528;&#20960;&#20010;&#37325;&#35201;&#25361;&#25112;&#65306;&#65288;i&#65289;3D&#22330;&#26223;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;&#30001;&#20110;&#22810;&#26679;&#30340;&#29289;&#20307;&#37197;&#32622;&#12289;&#20016;&#23500;&#30340;&#23646;&#24615;&#21644;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#65307;&#65288;ii&#65289;&#25903;&#25345;&#22522;&#20110;&#22330;&#26223;&#23398;&#20064;&#30340;&#37197;&#23545;3D&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65307;&#20197;&#21450;&#65288;iii&#65289;&#32570;&#20047;&#20174;&#22522;&#20110;&#22330;&#26223;&#30340;3D&#25968;&#25454;&#20013;&#25552;&#28860;&#30693;&#35782;&#30340;&#32479;&#19968;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#31995;&#32479;&#22320;&#25193;&#23637;&#23460;&#20869;&#29615;&#22659;&#20013;&#30340;3D&#35270;&#35273;-&#35821;&#35328;&#23398;&#20064;&#65292;&#20174;&#32780;&#35299;&#20915;3D&#35270;&#35273;-&#35821;&#35328;&#39046;&#22495;&#20013;&#30340;&#36825;&#19977;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#39318;&#20010;&#30334;&#19975;&#35268;&#27169;&#30340;3D&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#38598;SceneVerse&#65292;&#21253;&#21547;&#32422;68K&#20010;3D&#23460;&#20869;&#22330;&#26223;&#65292;&#21253;&#25324;250&#19975;&#20010;&#35270;&#35273;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.09340v2 Announce Type: replace-cross  Abstract: 3D vision-language grounding, which focuses on aligning language with the 3D physical environment, stands as a cornerstone in the development of embodied agents. In comparison to recent advancements in the 2D domain, grounding language in 3D scenes faces several significant challenges: (i) the inherent complexity of 3D scenes due to the diverse object configurations, their rich attributes, and intricate relationships; (ii) the scarcity of paired 3D vision-language data to support grounded learning; and (iii) the absence of a unified learning framework to distill knowledge from grounded 3D data. In this work, we aim to address these three major challenges in 3D vision-language by examining the potential of systematically upscaling 3D vision-language learning in indoor environments. We introduce the first million-scale 3D vision-language dataset, SceneVerse, encompassing about 68K 3D indoor scenes and comprising 2.5M vision-langu
&lt;/p&gt;</description></item><item><title>GUARD&#26159;&#19968;&#20010;&#24191;&#20041;&#32479;&#19968;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;&#65292;&#26159;&#30446;&#21069;&#24191;&#27867;&#36941;&#24067;&#19988;&#21253;&#21547;&#21508;&#31181;RL&#20195;&#29702;&#12289;&#20219;&#21153;&#21644;&#23433;&#20840;&#32422;&#26463;&#35268;&#33539;&#30340;&#19968;&#31449;&#24335;&#22522;&#20934;&#27979;&#35797;&#65292;&#33021;&#22815;&#20840;&#38754;&#28085;&#30422;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#33258;&#23450;&#20041;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13681</link><description>&lt;p&gt;
GUARD: &#19968;&#20010;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
GUARD: A Safe Reinforcement Learning Benchmark. (arXiv:2305.13681v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13681
&lt;/p&gt;
&lt;p&gt;
GUARD&#26159;&#19968;&#20010;&#24191;&#20041;&#32479;&#19968;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;&#65292;&#26159;&#30446;&#21069;&#24191;&#27867;&#36941;&#24067;&#19988;&#21253;&#21547;&#21508;&#31181;RL&#20195;&#29702;&#12289;&#20219;&#21153;&#21644;&#23433;&#20840;&#32422;&#26463;&#35268;&#33539;&#30340;&#19968;&#31449;&#24335;&#22522;&#20934;&#27979;&#35797;&#65292;&#33021;&#22815;&#20840;&#38754;&#28085;&#30422;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#33258;&#23450;&#20041;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35797;&#38169;&#30340;&#24615;&#36136;&#65292;&#23558;RL&#31639;&#27861;&#24212;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#30340;&#29616;&#23454;&#24212;&#29992;&#65288;&#20363;&#22914;&#33258;&#21160;&#39550;&#39542;&#12289;&#20154;&#26426;&#20132;&#20114;&#12289;&#26426;&#22120;&#20154;&#25805;&#20316;&#31561;&#65289;&#36890;&#24120;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#36825;&#20123;&#38169;&#35823;&#26159;&#19981;&#21487;&#23481;&#24525;&#30340;&#12290;&#26368;&#36817;&#65292;&#23433;&#20840;RL&#65288;&#21363;&#32422;&#26463;RL&#65289;&#24050;&#32463;&#22312;&#25991;&#29486;&#20013;&#36805;&#36895;&#20986;&#29616;&#65292;&#20854;&#20013;&#20195;&#29702;&#22312;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#30340;&#21516;&#26102;&#65292;&#25506;&#32034;&#29615;&#22659;&#12290;&#30001;&#20110;&#31639;&#27861;&#21644;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#65292;&#27604;&#36739;&#29616;&#26377;&#30340;&#23433;&#20840;RL&#31639;&#27861;&#20173;&#28982;&#24456;&#22256;&#38590;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GUARD&#65292;&#19968;&#20010;&#24191;&#20041;&#32479;&#19968;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;&#12290;&#19982;&#29616;&#26377;&#22522;&#20934;&#30456;&#27604;&#65292;GUARD&#20855;&#26377;&#20960;&#20010;&#20248;&#28857;&#12290;&#39318;&#20808;&#65292;GUARD&#26159;&#19968;&#20010;&#24191;&#20041;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;&#65292;&#20855;&#26377;&#21508;&#31181;RL&#20195;&#29702;&#12289;&#20219;&#21153;&#21644;&#23433;&#20840;&#32422;&#26463;&#35268;&#33539;&#12290;&#20854;&#27425;&#65292;GUARD&#20840;&#38754;&#28085;&#30422;&#20102;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#33258;&#21253;&#21547;&#30340;&#23454;&#29616;&#12290;&#31532;&#19977;&#65292;GUARD&#22312;&#20219;&#21153;&#21644;&#31639;&#27861;&#26041;&#38754;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#33258;&#23450;&#20041;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29366;&#24577;&#19979;&#29616;&#26377;&#26041;&#27861;&#22312;GUARD&#19978;&#30340;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the trial-and-error nature, it is typically challenging to apply RL algorithms to safety-critical real-world applications, such as autonomous driving, human-robot interaction, robot manipulation, etc, where such errors are not tolerable. Recently, safe RL (i.e. constrained RL) has emerged rapidly in the literature, in which the agents explore the environment while satisfying constraints. Due to the diversity of algorithms and tasks, it remains difficult to compare existing safe RL algorithms. To fill that gap, we introduce GUARD, a Generalized Unified SAfe Reinforcement Learning Development Benchmark. GUARD has several advantages compared to existing benchmarks. First, GUARD is a generalized benchmark with a wide variety of RL agents, tasks, and safety constraint specifications. Second, GUARD comprehensively covers state-of-the-art safe RL algorithms with self-contained implementations. Third, GUARD is highly customizable in tasks and algorithms. We present a comparison of state
&lt;/p&gt;</description></item></channel></rss>