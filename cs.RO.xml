<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26679;&#26412;&#25928;&#29575;&#23398;&#20064;&#26694;&#26550;&#65292;&#21517;&#20026;&#23398;&#20064;&#25945;&#23398;&#65288;L2T&#65289;&#65292;&#36890;&#36807;&#22238;&#25910;&#25945;&#24072;&#26234;&#33021;&#20307;&#25910;&#38598;&#30340;&#32463;&#39564;&#65292;&#35299;&#20915;&#20102;&#25945;&#24072;-&#23398;&#29983;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06783</link><description>&lt;p&gt;
&#23398;&#20064;&#25945;&#23398;&#65306;&#25913;&#21892;&#25945;&#24072;-&#23398;&#29983;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#23454;&#29616;&#20174;&#27169;&#25311;&#21040;&#29616;&#23454;&#30340;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
Learn to Teach: Improve Sample Efficiency in Teacher-student Learning for Sim-to-Real Transfer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26679;&#26412;&#25928;&#29575;&#23398;&#20064;&#26694;&#26550;&#65292;&#21517;&#20026;&#23398;&#20064;&#25945;&#23398;&#65288;L2T&#65289;&#65292;&#36890;&#36807;&#22238;&#25910;&#25945;&#24072;&#26234;&#33021;&#20307;&#25910;&#38598;&#30340;&#32463;&#39564;&#65292;&#35299;&#20915;&#20102;&#25945;&#24072;-&#23398;&#29983;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#21040;&#29616;&#23454;&#65288;sim-to-real&#65289;&#30340;&#36801;&#31227;&#26159;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#22495;&#38543;&#26426;&#21270;&#26159;&#19968;&#31181;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28155;&#21152;&#38543;&#26426;&#24615;&#30340;&#24378;&#22823;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#27169;&#25311;&#19982;&#29616;&#23454;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#35266;&#27979;&#20013;&#30340;&#22122;&#22768;&#20351;&#24471;&#23398;&#20064;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#37319;&#29992;&#25945;&#24072;-&#23398;&#29983;&#23398;&#20064;&#33539;&#24335;&#21487;&#20197;&#21152;&#36895;&#38543;&#26426;&#21270;&#29615;&#22659;&#20013;&#30340;&#35757;&#32451;&#12290;&#36890;&#36807;&#20351;&#29992;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#23398;&#20064;&#65292;&#25945;&#24072;&#26234;&#33021;&#20307;&#21487;&#20197;&#25351;&#23548;&#23398;&#29983;&#26234;&#33021;&#20307;&#22312;&#22122;&#22768;&#29615;&#22659;&#20013;&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#19981;&#26159;&#26679;&#26412;&#25928;&#29575;&#30340;&#65292;&#22240;&#20026;&#22312;&#35757;&#32451;&#23398;&#29983;&#26234;&#33021;&#20307;&#26102;&#23436;&#20840;&#33293;&#24323;&#20102;&#25945;&#24072;&#26234;&#33021;&#20307;&#25910;&#38598;&#30340;&#32463;&#39564;&#65292;&#28010;&#36153;&#20102;&#29615;&#22659;&#25152;&#36879;&#38706;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#21517;&#20026;&#23398;&#20064;&#25945;&#23398;&#65288;L2T&#65289;&#30340;&#26679;&#26412;&#25928;&#29575;&#23398;&#20064;&#26694;&#26550;&#26469;&#25193;&#23637;&#25945;&#24072;-&#23398;&#29983;&#23398;&#20064;&#33539;&#24335;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#22238;&#25910;&#25945;&#24072;&#26234;&#33021;&#20307;&#25910;&#38598;&#30340;&#32463;&#39564;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23545;&#20110;&#19968;&#23545;&#25945;&#24072;-&#23398;&#29983;&#26234;&#33021;&#20307;&#65292;&#29615;&#22659;&#30340;&#21160;&#24577;&#29305;&#24615;&#23545;&#20004;&#32773;&#37117;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulation-to-reality (sim-to-real) transfer is a fundamental problem for robot learning. Domain Randomization, which adds randomization during training, is a powerful technique that effectively addresses the sim-to-real gap. However, the noise in observations makes learning significantly harder. Recently, studies have shown that employing a teacher-student learning paradigm can accelerate training in randomized environments. Learned with privileged information, a teacher agent can instruct the student agent to operate in noisy environments. However, this approach is often not sample efficient as the experience collected by the teacher is discarded completely when training the student, wasting information revealed by the environment. In this work, we extend the teacher-student learning paradigm by proposing a sample efficient learning framework termed Learn to Teach (L2T) that recycles experience collected by the teacher agent. We observe that the dynamics of the environments for both 
&lt;/p&gt;</description></item><item><title>Constraint-Generation Policy Optimization (CGPO)&#26159;&#19968;&#31181;&#38024;&#23545;&#28151;&#21512;&#31163;&#25955;&#36830;&#32493;MDPs&#30340;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#20379;&#26377;&#30028;&#30340;&#31574;&#30053;&#35823;&#24046;&#20445;&#35777;&#65292;&#25512;&#23548;&#20986;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#29983;&#25104;&#26368;&#22351;&#24773;&#20917;&#30340;&#29366;&#24577;&#36712;&#36857;&#26469;&#35786;&#26029;&#31574;&#30053;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2401.12243</link><description>&lt;p&gt;
Constraint-Generation Policy Optimization (CGPO): &#38024;&#23545;&#28151;&#21512;&#31163;&#25955;&#36830;&#32493;MDPs&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;&#30340;&#38750;&#32447;&#24615;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Constraint-Generation Policy Optimization (CGPO): Nonlinear Programming for Policy Optimization in Mixed Discrete-Continuous MDPs. (arXiv:2401.12243v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12243
&lt;/p&gt;
&lt;p&gt;
Constraint-Generation Policy Optimization (CGPO)&#26159;&#19968;&#31181;&#38024;&#23545;&#28151;&#21512;&#31163;&#25955;&#36830;&#32493;MDPs&#30340;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#20379;&#26377;&#30028;&#30340;&#31574;&#30053;&#35823;&#24046;&#20445;&#35777;&#65292;&#25512;&#23548;&#20986;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#29983;&#25104;&#26368;&#22351;&#24773;&#20917;&#30340;&#29366;&#24577;&#36712;&#36857;&#26469;&#35786;&#26029;&#31574;&#30053;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Constraint-Generation Policy Optimization (CGPO)&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#28151;&#21512;&#31163;&#25955;&#36830;&#32493;Markov Decision Processes (DC-MDPs)&#20013;&#20248;&#21270;&#31574;&#30053;&#21442;&#25968;&#12290;CGPO&#19981;&#20165;&#33021;&#22815;&#25552;&#20379;&#26377;&#30028;&#30340;&#31574;&#30053;&#35823;&#24046;&#20445;&#35777;&#65292;&#35206;&#30422;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#30340;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#26080;&#25968;&#21021;&#22987;&#29366;&#24577;&#33539;&#22260;&#30340;DC-MDPs&#65292;&#32780;&#19988;&#22312;&#32467;&#26463;&#26102;&#21487;&#20197;&#26126;&#30830;&#22320;&#25512;&#23548;&#20986;&#26368;&#20248;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;CGPO&#36824;&#33021;&#22815;&#29983;&#25104;&#26368;&#22351;&#24773;&#20917;&#30340;&#29366;&#24577;&#36712;&#36857;&#26469;&#35786;&#26029;&#31574;&#30053;&#32570;&#38519;&#65292;&#24182;&#25552;&#20379;&#26368;&#20248;&#34892;&#21160;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#32467;&#26524;&#65292;CGPO&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23618;&#30340;&#28151;&#21512;&#25972;&#25968;&#38750;&#32447;&#24615;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#23450;&#20041;&#30340;&#34920;&#36798;&#33021;&#21147;&#31867;&#21035;&#65288;&#21363;&#20998;&#27573;(&#38750;)&#32447;&#24615;&#65289;&#20869;&#20248;&#21270;&#31574;&#30053;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#19968;&#20010;&#26368;&#20248;&#30340;&#32422;&#26463;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#29983;&#25104;&#26368;&#22351;&#24773;&#20917;&#30340;&#29366;&#24577;&#36712;&#36857;&#12290;&#27492;&#22806;&#65292;&#20511;&#21161;&#29616;&#20195;&#38750;&#32447;&#24615;&#20248;&#21270;&#22120;&#65292;CGPO&#21487;&#20197;&#33719;&#24471;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Constraint-Generation Policy Optimization (CGPO) for optimizing policy parameters within compact and interpretable policy classes for mixed discrete-continuous Markov Decision Processes (DC-MDPs). CGPO is not only able to provide bounded policy error guarantees over an infinite range of initial states for many DC-MDPs with expressive nonlinear dynamics, but it can also provably derive optimal policies in cases where it terminates with zero error. Furthermore, CGPO can generate worst-case state trajectories to diagnose policy deficiencies and provide counterfactual explanations of optimal actions. To achieve such results, CGPO proposes a bi-level mixed-integer nonlinear optimization framework for optimizing policies within defined expressivity classes (i.e. piecewise (non)-linear) and reduces it to an optimal constraint generation methodology that adversarially generates worst-case state trajectories. Furthermore, leveraging modern nonlinear optimizers, CGPO can obtain soluti
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#26694;&#26550;SWBT&#65292;&#33021;&#22815;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#26377;&#25928;&#22320;&#20174;&#19987;&#23478;&#28436;&#31034;&#21644;&#19981;&#23436;&#32654;&#28436;&#31034;&#20013;&#23398;&#20064;&#65292;&#32780;&#26080;&#38656;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;&#19981;&#23436;&#32654;&#28436;&#31034;&#25972;&#21512;&#21040;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#35774;&#32622;&#20013;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2401.08957</link><description>&lt;p&gt;
SWBT&#65306;&#20855;&#26377;&#19981;&#23436;&#32654;&#28436;&#31034;&#30340;&#30456;&#20284;&#24615;&#21152;&#26435;&#34892;&#20026;&#36716;&#25442;&#22120;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
SWBT: Similarity Weighted Behavior Transformer with the Imperfect Demonstration for Robotic Manipulation. (arXiv:2401.08957v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#26694;&#26550;SWBT&#65292;&#33021;&#22815;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#26377;&#25928;&#22320;&#20174;&#19987;&#23478;&#28436;&#31034;&#21644;&#19981;&#23436;&#32654;&#28436;&#31034;&#20013;&#23398;&#20064;&#65292;&#32780;&#26080;&#38656;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;&#19981;&#23436;&#32654;&#28436;&#31034;&#25972;&#21512;&#21040;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#35774;&#32622;&#20013;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#26088;&#22312;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#23398;&#20064;&#26368;&#20339;&#25511;&#21046;&#31574;&#30053;&#65292;&#24050;&#25104;&#20026;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#35201;&#20040;&#20165;&#20351;&#29992;&#26114;&#36149;&#30340;&#19987;&#23478;&#28436;&#31034;&#24182;&#24573;&#30053;&#19981;&#23436;&#32654;&#30340;&#28436;&#31034;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#21644;&#20174;&#22312;&#32447;&#32463;&#39564;&#20013;&#23398;&#20064;&#12290;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#26088;&#22312;&#20811;&#26381;&#19978;&#36848;&#20004;&#20010;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Similarity Weighted Behavior Transformer&#65288;SWBT&#65289;&#30340;&#26032;&#22411;&#26694;&#26550;&#12290;SWBT&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#19987;&#23478;&#28436;&#31034;&#21644;&#19981;&#23436;&#32654;&#28436;&#31034;&#20013;&#23398;&#20064;&#65292;&#32780;&#26080;&#38656;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#26131;&#33719;&#21462;&#30340;&#19981;&#23436;&#32654;&#28436;&#31034;&#65292;&#22914;&#27491;&#21521;&#21644;&#21453;&#21521;&#21160;&#21147;&#23398;&#65292;&#36890;&#36807;&#23398;&#20064;&#26377;&#30410;&#20449;&#24687;&#26174;&#33879;&#22686;&#24378;&#20102;&#32593;&#32476;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#23581;&#35797;&#23558;&#19981;&#23436;&#32654;&#28436;&#31034;&#25972;&#21512;&#21040;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#35774;&#32622;&#20013;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;&#30740;&#31350;&#12290;&#22312;ManiSkill2 bench&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning (IL), aiming to learn optimal control policies from expert demonstrations, has been an effective method for robot manipulation tasks. However, previous IL methods either only use expensive expert demonstrations and omit imperfect demonstrations or rely on interacting with the environment and learning from online experiences. In the context of robotic manipulation, we aim to conquer the above two challenges and propose a novel framework named Similarity Weighted Behavior Transformer (SWBT). SWBT effectively learn from both expert and imperfect demonstrations without interaction with environments. We reveal that the easy-to-get imperfect demonstrations, such as forward and inverse dynamics, significantly enhance the network by learning fruitful information. To the best of our knowledge, we are the first to attempt to integrate imperfect demonstrations into the offline imitation learning setting for robot manipulation tasks. Extensive experiments on the ManiSkill2 bench
&lt;/p&gt;</description></item></channel></rss>