<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#19977;&#23618;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24341;&#20837;&#20102;&#31354;&#38388;&#21644;&#26102;&#38388;&#30446;&#26631;&#25277;&#35937;&#21270;&#12290;&#30740;&#31350;&#32773;&#25552;&#20379;&#20102;&#23398;&#20064;&#31574;&#30053;&#30340;&#29702;&#35770;&#36951;&#25022;&#36793;&#30028;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#23545;&#31639;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.09870</link><description>&lt;p&gt;
&#35843;&#21644;&#31354;&#38388;&#21644;&#26102;&#38388;&#25277;&#35937;&#21270;&#20197;&#23454;&#29616;&#30446;&#26631;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Reconciling Spatial and Temporal Abstractions for Goal Representation. (arXiv:2401.09870v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#19977;&#23618;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24341;&#20837;&#20102;&#31354;&#38388;&#21644;&#26102;&#38388;&#30446;&#26631;&#25277;&#35937;&#21270;&#12290;&#30740;&#31350;&#32773;&#25552;&#20379;&#20102;&#23398;&#20064;&#31574;&#30053;&#30340;&#29702;&#35770;&#36951;&#25022;&#36793;&#30028;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#23545;&#31639;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#34920;&#31034;&#36890;&#36807;&#23558;&#22797;&#26434;&#30340;&#23398;&#20064;&#38382;&#39064;&#20998;&#35299;&#20026;&#26356;&#23481;&#26131;&#30340;&#23376;&#20219;&#21153;&#26469;&#24433;&#21709;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20445;&#30041;&#26102;&#38388;&#25277;&#35937;&#29615;&#22659;&#21160;&#24577;&#30340;&#34920;&#31034;&#26041;&#27861;&#22312;&#35299;&#20915;&#22256;&#38590;&#38382;&#39064;&#21644;&#25552;&#20379;&#20248;&#21270;&#29702;&#35770;&#20445;&#35777;&#26041;&#38754;&#26159;&#25104;&#21151;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#29615;&#22659;&#21160;&#24577;&#36234;&#26469;&#36234;&#22797;&#26434;&#65288;&#21363;&#26102;&#38388;&#25277;&#35937;&#36716;&#25442;&#20851;&#31995;&#20381;&#36182;&#26356;&#22810;&#21464;&#37327;&#65289;&#30340;&#20219;&#21153;&#20013;&#26080;&#27861;&#25193;&#23637;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20854;&#20182;&#26041;&#27861;&#21017;&#23581;&#35797;&#20351;&#29992;&#31354;&#38388;&#25277;&#35937;&#26469;&#32531;&#35299;&#21069;&#38754;&#30340;&#38382;&#39064;&#12290;&#23427;&#20204;&#30340;&#38480;&#21046;&#21253;&#25324;&#26080;&#27861;&#36866;&#24212;&#39640;&#32500;&#29615;&#22659;&#21644;&#23545;&#20808;&#21069;&#30693;&#35782;&#30340;&#20381;&#36182;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19977;&#23618;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20998;&#23618;&#32467;&#26500;&#30340;&#19981;&#21516;&#23618;&#27425;&#24341;&#20837;&#20102;&#31354;&#38388;&#21644;&#26102;&#38388;&#30446;&#26631;&#25277;&#35937;&#21270;&#12290;&#25105;&#20204;&#23545;&#23398;&#20064;&#31574;&#30053;&#30340;&#36951;&#25022;&#36793;&#30028;&#36827;&#34892;&#20102;&#29702;&#35770;&#30740;&#31350;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Goal representation affects the performance of Hierarchical Reinforcement Learning (HRL) algorithms by decomposing the complex learning problem into easier subtasks. Recent studies show that representations that preserve temporally abstract environment dynamics are successful in solving difficult problems and provide theoretical guarantees for optimality. These methods however cannot scale to tasks where environment dynamics increase in complexity i.e. the temporally abstract transition relations depend on larger number of variables. On the other hand, other efforts have tried to use spatial abstraction to mitigate the previous issues. Their limitations include scalability to high dimensional environments and dependency on prior knowledge.  In this paper, we propose a novel three-layer HRL algorithm that introduces, at different levels of the hierarchy, both a spatial and a temporal goal abstraction. We provide a theoretical study of the regret bounds of the learned policies. We evalua
&lt;/p&gt;</description></item></channel></rss>