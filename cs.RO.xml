<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#24418;&#29366;&#23436;&#25104;&#21644;&#25235;&#21462;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#28789;&#27963;&#30340;&#22810;&#25351;&#25235;&#21462;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#22270;&#20687;&#30340;&#24418;&#29366;&#23436;&#25104;&#27169;&#22359;&#21644;&#22522;&#20110;&#39044;&#27979;&#30340;&#25235;&#21462;&#39044;&#27979;&#22120;&#65292;&#23454;&#29616;&#20102;&#22312;&#20855;&#26377;&#26377;&#38480;&#25110;&#26080;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#29289;&#20307;&#36827;&#34892;&#25235;&#21462;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.20350</link><description>&lt;p&gt;
&#23558;&#24418;&#29366;&#23436;&#25104;&#21644;&#25235;&#21462;&#39044;&#27979;&#32467;&#21512;&#65292;&#23454;&#29616;&#24555;&#36895;&#28789;&#27963;&#30340;&#22810;&#25351;&#25235;&#21462;
&lt;/p&gt;
&lt;p&gt;
Combining Shape Completion and Grasp Prediction for Fast and Versatile Grasping with a Multi-Fingered Hand. (arXiv:2310.20350v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#24418;&#29366;&#23436;&#25104;&#21644;&#25235;&#21462;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#28789;&#27963;&#30340;&#22810;&#25351;&#25235;&#21462;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#22270;&#20687;&#30340;&#24418;&#29366;&#23436;&#25104;&#27169;&#22359;&#21644;&#22522;&#20110;&#39044;&#27979;&#30340;&#25235;&#21462;&#39044;&#27979;&#22120;&#65292;&#23454;&#29616;&#20102;&#22312;&#20855;&#26377;&#26377;&#38480;&#25110;&#26080;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#29289;&#20307;&#36827;&#34892;&#25235;&#21462;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36741;&#21161;&#26426;&#22120;&#20154;&#20013;&#65292;&#23545;&#20110;&#20855;&#26377;&#26377;&#38480;&#25110;&#26080;&#20808;&#39564;&#30693;&#35782;&#30340;&#29289;&#20307;&#36827;&#34892;&#25235;&#21462;&#26159;&#19968;&#39033;&#38750;&#24120;&#37325;&#35201;&#30340;&#25216;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31181;&#26222;&#36866;&#24773;&#20917;&#19979;&#65292;&#23588;&#20854;&#26159;&#22312;&#35266;&#27979;&#33021;&#21147;&#26377;&#38480;&#21644;&#21033;&#29992;&#22810;&#25351;&#25163;&#36827;&#34892;&#28789;&#27963;&#25235;&#21462;&#26102;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#24555;&#36895;&#21644;&#39640;&#20445;&#30495;&#24230;&#30340;&#28145;&#24230;&#23398;&#20064;&#27969;&#31243;&#65292;&#30001;&#22522;&#20110;&#21333;&#20010;&#28145;&#24230;&#22270;&#20687;&#30340;&#24418;&#29366;&#23436;&#25104;&#27169;&#22359;&#21644;&#22522;&#20110;&#39044;&#27979;&#30340;&#29289;&#20307;&#24418;&#29366;&#30340;&#25235;&#21462;&#39044;&#27979;&#22120;&#32452;&#25104;&#12290;&#24418;&#29366;&#23436;&#25104;&#32593;&#32476;&#22522;&#20110;VQDIF&#65292;&#22312;&#20219;&#24847;&#26597;&#35810;&#28857;&#19978;&#39044;&#27979;&#31354;&#38388;&#21344;&#29992;&#20540;&#12290;&#20316;&#20026;&#25235;&#21462;&#39044;&#27979;&#22120;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20004;&#38454;&#27573;&#26550;&#26500;&#65292;&#39318;&#20808;&#20351;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#29983;&#25104;&#25163;&#23039;&#21183;&#65292;&#28982;&#21518;&#22238;&#24402;&#27599;&#20010;&#23039;&#21183;&#30340;&#25163;&#25351;&#20851;&#33410;&#37197;&#32622;&#12290;&#20851;&#38190;&#22240;&#32032;&#26159;&#36275;&#22815;&#30340;&#25968;&#25454;&#30495;&#23454;&#24615;&#21644;&#22686;&#24378;&#65292;&#20197;&#21450;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#22256;&#38590;&#24773;&#20917;&#30340;&#29305;&#27530;&#20851;&#27880;&#12290;&#22312;&#29289;&#29702;&#26426;&#22120;&#20154;&#24179;&#21488;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#25235;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Grasping objects with limited or no prior knowledge about them is a highly relevant skill in assistive robotics. Still, in this general setting, it has remained an open problem, especially when it comes to only partial observability and versatile grasping with multi-fingered hands. We present a novel, fast, and high fidelity deep learning pipeline consisting of a shape completion module that is based on a single depth image, and followed by a grasp predictor that is based on the predicted object shape. The shape completion network is based on VQDIF and predicts spatial occupancy values at arbitrary query points. As grasp predictor, we use our two-stage architecture that first generates hand poses using an autoregressive model and then regresses finger joint configurations per pose. Critical factors turn out to be sufficient data realism and augmentation, as well as special attention to difficult cases during training. Experiments on a physical robot platform demonstrate successful gras
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#22788;&#29702;&#22312;&#32473;&#23450;&#27169;&#31946;&#29289;&#20307;&#35270;&#22270;&#26102;&#21487;&#33021;&#23384;&#22312;&#30340;&#29289;&#20307;&#37096;&#20998;&#30340;&#19981;&#30830;&#23450;&#21306;&#22495;&#39044;&#27979;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#20219;&#20309;&#39044;&#27979;&#31354;&#38388;&#21344;&#29992;&#30340;&#26041;&#27861;&#30340;&#30452;&#25509;&#25193;&#23637;&#65292;&#36890;&#36807;&#21518;&#22788;&#29702;&#21344;&#29992;&#35780;&#20998;&#25110;&#30452;&#25509;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#25351;&#26631;&#26469;&#23454;&#29616;&#12290;&#36825;&#20123;&#26041;&#27861;&#19982;&#24050;&#30693;&#30340;&#27010;&#29575;&#24418;&#29366;&#23436;&#25104;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#28145;&#24230;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.00377</link><description>&lt;p&gt;
&#24102;&#26377;&#19981;&#30830;&#23450;&#21306;&#22495;&#39044;&#27979;&#30340;&#24418;&#29366;&#23436;&#25104;
&lt;/p&gt;
&lt;p&gt;
Shape Completion with Prediction of Uncertain Regions. (arXiv:2308.00377v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00377
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#22788;&#29702;&#22312;&#32473;&#23450;&#27169;&#31946;&#29289;&#20307;&#35270;&#22270;&#26102;&#21487;&#33021;&#23384;&#22312;&#30340;&#29289;&#20307;&#37096;&#20998;&#30340;&#19981;&#30830;&#23450;&#21306;&#22495;&#39044;&#27979;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#20219;&#20309;&#39044;&#27979;&#31354;&#38388;&#21344;&#29992;&#30340;&#26041;&#27861;&#30340;&#30452;&#25509;&#25193;&#23637;&#65292;&#36890;&#36807;&#21518;&#22788;&#29702;&#21344;&#29992;&#35780;&#20998;&#25110;&#30452;&#25509;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#25351;&#26631;&#26469;&#23454;&#29616;&#12290;&#36825;&#20123;&#26041;&#27861;&#19982;&#24050;&#30693;&#30340;&#27010;&#29575;&#24418;&#29366;&#23436;&#25104;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#28145;&#24230;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24418;&#29366;&#23436;&#25104;&#65292;&#21363;&#20174;&#37096;&#20998;&#35266;&#27979;&#39044;&#27979;&#29289;&#20307;&#30340;&#23436;&#25972;&#20960;&#20309;&#24418;&#29366;&#65292;&#23545;&#20110;&#20960;&#20010;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;&#24403;&#22522;&#20110;&#29289;&#20307;&#24418;&#29366;&#37325;&#24314;&#36827;&#34892;&#35268;&#21010;&#25110;&#23454;&#38469;&#25235;&#21462;&#30340;&#39044;&#27979;&#26102;&#65292;&#25351;&#31034;&#20005;&#37325;&#20960;&#20309;&#19981;&#30830;&#23450;&#24615;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#29305;&#21035;&#26159;&#22312;&#32473;&#23450;&#27169;&#31946;&#30340;&#29289;&#20307;&#35270;&#22270;&#26102;&#65292;&#22312;&#25972;&#20010;&#29289;&#20307;&#37096;&#20998;&#23384;&#22312; irreducible uncertainty &#30340;&#25193;&#23637;&#21306;&#22495;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#31181;&#37325;&#35201;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#26469;&#39044;&#27979;&#36825;&#20123;&#19981;&#30830;&#23450;&#21306;&#22495;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#21487;&#20197;&#20316;&#20026;&#39044;&#27979;&#23616;&#37096;&#31354;&#38388;&#21344;&#29992;&#30340;&#20219;&#20309;&#26041;&#27861;&#30340;&#30452;&#25509;&#25193;&#23637;&#65292;&#19968;&#31181;&#26159;&#36890;&#36807;&#21518;&#22788;&#29702;&#21344;&#29992;&#35780;&#20998;&#65292;&#21478;&#19968;&#31181;&#26159;&#36890;&#36807;&#30452;&#25509;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#25351;&#26631;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#19982;&#20004;&#31181;&#24050;&#30693;&#30340;&#27010;&#29575;&#24418;&#29366;&#23436;&#25104;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#29983;&#25104;&#20102;&#19968;&#20010;&#22522;&#20110;ShapeNet&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#30495;&#23454;&#28210;&#26579;&#30340;&#29289;&#20307;&#35270;&#22270;&#28145;&#24230;&#22270;&#20687;&#21450;&#20854;&#24102;&#26377;&#22320;&#38754;&#30495;&#20540;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Shape completion, i.e., predicting the complete geometry of an object from a partial observation, is highly relevant for several downstream tasks, most notably robotic manipulation. When basing planning or prediction of real grasps on object shape reconstruction, an indication of severe geometric uncertainty is indispensable. In particular, there can be an irreducible uncertainty in extended regions about the presence of entire object parts when given ambiguous object views. To treat this important case, we propose two novel methods for predicting such uncertain regions as straightforward extensions of any method for predicting local spatial occupancy, one through postprocessing occupancy scores, the other through direct prediction of an uncertainty indicator. We compare these methods together with two known approaches to probabilistic shape completion. Moreover, we generate a dataset, derived from ShapeNet, of realistically rendered depth images of object views with ground-truth annot
&lt;/p&gt;</description></item></channel></rss>