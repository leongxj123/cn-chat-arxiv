<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#29031;&#26009;&#26426;&#22120;&#20154;&#20013;&#20351;&#29992;&#20154;&#24037;&#24515;&#26234;&#29702;&#35770;&#26469;&#29468;&#27979;&#20154;&#31867;&#24847;&#22270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#27979;&#21361;&#38505;&#24773;&#20917;&#24182;&#23454;&#26102;&#28040;&#38500;&#21361;&#38505;&#30340;&#31639;&#27861;&#65292;&#22312;&#27169;&#25311;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#39640;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.16291</link><description>&lt;p&gt;
&#22312;&#29031;&#26009;&#26426;&#22120;&#20154;&#20013;&#29468;&#27979;&#20154;&#31867;&#24847;&#22270;&#20197;&#36991;&#20813;&#21361;&#38505;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Guessing human intentions to avoid dangerous situations in caregiving robots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16291
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#29031;&#26009;&#26426;&#22120;&#20154;&#20013;&#20351;&#29992;&#20154;&#24037;&#24515;&#26234;&#29702;&#35770;&#26469;&#29468;&#27979;&#20154;&#31867;&#24847;&#22270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#27979;&#21361;&#38505;&#24773;&#20917;&#24182;&#23454;&#26102;&#28040;&#38500;&#21361;&#38505;&#30340;&#31639;&#27861;&#65292;&#22312;&#27169;&#25311;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#39640;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35201;&#27714;&#26426;&#22120;&#20154;&#36827;&#34892;&#31038;&#20132;&#20114;&#21160;&#65292;&#23427;&#20204;&#24517;&#39035;&#20934;&#30830;&#35299;&#37322;&#20154;&#31867;&#24847;&#22270;&#24182;&#39044;&#27979;&#28508;&#22312;&#32467;&#26524;&#12290;&#23545;&#20110;&#20026;&#20154;&#31867;&#25252;&#29702;&#35774;&#35745;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#32780;&#35328;&#23588;&#20026;&#37325;&#35201;&#65292;&#21487;&#33021;&#20250;&#38754;&#20020;&#20154;&#31867;&#30340;&#21361;&#38505;&#24773;&#20917;&#65292;&#27604;&#22914;&#26410;&#35265;&#38556;&#30861;&#29289;&#65292;&#24212;&#35813;&#20104;&#20197;&#36991;&#20813;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#24515;&#26234;&#29702;&#35770;&#65288;ATM&#65289;&#26041;&#27861;&#26469;&#25512;&#26029;&#21644;&#35299;&#37322;&#20154;&#31867;&#24847;&#22270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#27979;&#20154;&#31867;&#39118;&#38505;&#24773;&#20917;&#30340;&#31639;&#27861;&#65292;&#36873;&#25321;&#23454;&#26102;&#28040;&#38500;&#21361;&#38505;&#30340;&#26426;&#22120;&#20154;&#21160;&#20316;&#12290;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#27169;&#25311;&#30340;ATM&#26041;&#27861;&#65292;&#24182;&#37319;&#29992;&#8220;&#20687;&#25105;&#19968;&#26679;&#8221;&#30340;&#31574;&#30053;&#23558;&#24847;&#22270;&#21644;&#21160;&#20316;&#20998;&#37197;&#32473;&#20154;&#31867;&#12290;&#36890;&#36807;&#36825;&#31181;&#31574;&#30053;&#65292;&#26426;&#22120;&#20154;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#21487;&#20197;&#39640;&#25104;&#21151;&#29575;&#22320;&#26816;&#27979;&#21644;&#34892;&#21160;&#12290;&#35813;&#31639;&#27861;&#24050;&#32463;&#20316;&#20026;&#29616;&#26377;&#26426;&#22120;&#20154;&#35748;&#30693;&#26550;&#26500;&#30340;&#19968;&#37096;&#20998;&#23454;&#26045;&#65292;&#24182;&#22312;&#27169;&#25311;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#36827;&#34892;&#20102;&#19977;&#20010;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16291v1 Announce Type: cross  Abstract: For robots to interact socially, they must interpret human intentions and anticipate their potential outcomes accurately. This is particularly important for social robots designed for human care, which may face potentially dangerous situations for people, such as unseen obstacles in their way, that should be avoided. This paper explores the Artificial Theory of Mind (ATM) approach to inferring and interpreting human intentions. We propose an algorithm that detects risky situations for humans, selecting a robot action that removes the danger in real time. We use the simulation-based approach to ATM and adopt the 'like-me' policy to assign intentions and actions to people. Using this strategy, the robot can detect and act with a high rate of success under time-constrained situations. The algorithm has been implemented as part of an existing robotics cognitive architecture and tested in simulation scenarios. Three experiments have been co
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;RASP&#65292;&#19968;&#20010;&#21487;&#22312;25&#31186;&#20869;&#33258;&#20027;&#26356;&#25442;&#20256;&#24863;&#22120;&#21644;&#25191;&#34892;&#22120;&#30340;&#27169;&#22359;&#21270;&#21644;&#21487;&#37325;&#26500;&#20256;&#24863;&#21644;&#20316;&#21160;&#24179;&#21488;&#65292;&#20351;&#26080;&#20154;&#26426;&#33021;&#24555;&#36895;&#36866;&#24212;&#21508;&#31181;&#20219;&#21153;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#20010;&#20154;&#21161;&#29702;&#31995;&#32479;&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.12853</link><description>&lt;p&gt;
&#22522;&#20110;&#26080;&#20154;&#26426;&#30340;&#29615;&#22659;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#37325;&#26500;&#20316;&#21160;&#21644;&#20256;&#24863;&#24179;&#21488;RASP
&lt;/p&gt;
&lt;p&gt;
RASP: A Drone-based Reconfigurable Actuation and Sensing Platform Towards Ambient Intelligent Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12853
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;RASP&#65292;&#19968;&#20010;&#21487;&#22312;25&#31186;&#20869;&#33258;&#20027;&#26356;&#25442;&#20256;&#24863;&#22120;&#21644;&#25191;&#34892;&#22120;&#30340;&#27169;&#22359;&#21270;&#21644;&#21487;&#37325;&#26500;&#20256;&#24863;&#21644;&#20316;&#21160;&#24179;&#21488;&#65292;&#20351;&#26080;&#20154;&#26426;&#33021;&#24555;&#36895;&#36866;&#24212;&#21508;&#31181;&#20219;&#21153;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#20010;&#20154;&#21161;&#29702;&#31995;&#32479;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#28040;&#36153;&#32423;&#26080;&#20154;&#26426;&#19982;&#25105;&#20204;&#23478;&#20013;&#30340;&#21560;&#23576;&#26426;&#22120;&#20154;&#25110;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#20010;&#20154;&#26234;&#33021;&#25163;&#26426;&#19968;&#26679;&#26377;&#29992;&#65292;&#38656;&#35201;&#26080;&#20154;&#26426;&#33021;&#24863;&#30693;&#12289;&#39537;&#21160;&#21644;&#21709;&#24212;&#21487;&#33021;&#20986;&#29616;&#30340;&#19968;&#33324;&#24773;&#20917;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#24895;&#26223;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RASP&#65292;&#19968;&#20010;&#27169;&#22359;&#21270;&#21644;&#21487;&#37325;&#26500;&#30340;&#20256;&#24863;&#21644;&#20316;&#21160;&#24179;&#21488;&#65292;&#20801;&#35768;&#26080;&#20154;&#26426;&#22312;&#20165;25&#31186;&#20869;&#33258;&#20027;&#26356;&#25442;&#26426;&#36733;&#20256;&#24863;&#22120;&#21644;&#25191;&#34892;&#22120;&#65292;&#20351;&#21333;&#20010;&#26080;&#20154;&#26426;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#21508;&#31181;&#20219;&#21153;&#12290;RASP&#21253;&#25324;&#19968;&#20010;&#26426;&#26800;&#23618;&#65292;&#29992;&#20110;&#29289;&#29702;&#26356;&#25442;&#20256;&#24863;&#22120;&#27169;&#22359;&#65292;&#19968;&#20010;&#30005;&#27668;&#23618;&#65292;&#29992;&#20110;&#32500;&#25252;&#20256;&#24863;&#22120;/&#25191;&#34892;&#22120;&#30340;&#30005;&#28304;&#21644;&#36890;&#20449;&#32447;&#36335;&#65292;&#20197;&#21450;&#19968;&#20010;&#36719;&#20214;&#23618;&#65292;&#29992;&#20110;&#22312;&#26080;&#20154;&#26426;&#21644;&#25105;&#20204;&#24179;&#21488;&#19978;&#30340;&#20219;&#20309;&#20256;&#24863;&#22120;&#27169;&#22359;&#20043;&#38388;&#32500;&#25252;&#19968;&#20010;&#20844;&#20849;&#25509;&#21475;&#12290;&#21033;&#29992;&#26368;&#36817;&#22312;&#22823;&#22411;&#35821;&#35328;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;RASP&#30340;&#20010;&#20154;&#21161;&#29702;&#31995;&#32479;&#30340;&#26550;&#26500;&#12289;&#23454;&#29616;&#21644;&#29616;&#23454;&#19990;&#30028;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12853v1 Announce Type: cross  Abstract: Realizing consumer-grade drones that are as useful as robot vacuums throughout our homes or personal smartphones in our daily lives requires drones to sense, actuate, and respond to general scenarios that may arise. Towards this vision, we propose RASP, a modular and reconfigurable sensing and actuation platform that allows drones to autonomously swap onboard sensors and actuators in only 25 seconds, allowing a single drone to quickly adapt to a diverse range of tasks. RASP consists of a mechanical layer to physically swap sensor modules, an electrical layer to maintain power and communication lines to the sensor/actuator, and a software layer to maintain a common interface between the drone and any sensor module in our platform. Leveraging recent advances in large language and visual language models, we further introduce the architecture, implementation, and real-world deployments of a personal assistant system utilizing RASP. We demo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Ensembling Prioritized Hybrid Policies (EPH)&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#36890;&#20449;&#27169;&#22359;&#21644;&#19977;&#31181;&#39640;&#32423;&#25512;&#29702;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;&#36890;&#20449;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.07559</link><description>&lt;p&gt;
&#20026;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#38598;&#25104;&#20248;&#20808;&#32423;&#28151;&#21512;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07559
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Ensembling Prioritized Hybrid Policies (EPH)&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#36890;&#20449;&#27169;&#22359;&#21644;&#19977;&#31181;&#39640;&#32423;&#25512;&#29702;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;&#36890;&#20449;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#65288;MAPF&#65289;&#36817;&#26469;&#22240;&#20854;&#39640;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Ensembling Prioritized Hybrid Policies (EPH)&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#22522;&#20110;&#36890;&#20449;&#30340;MARL-MAPF&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#36873;&#25321;&#24615;&#36890;&#20449;&#27169;&#22359;&#65292;&#20197;&#22312;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#25910;&#38598;&#26356;&#20016;&#23500;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#26234;&#33021;&#20307;&#21327;&#35843;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;Q-learning&#30340;&#31639;&#27861;&#23545;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07559v1 Announce Type: cross  Abstract: Multi-Agent Reinforcement Learning (MARL) based Multi-Agent Path Finding (MAPF) has recently gained attention due to its efficiency and scalability. Several MARL-MAPF methods choose to use communication to enrich the information one agent can perceive. However, existing works still struggle in structured environments with high obstacle density and a high number of agents. To further improve the performance of the communication-based MARL-MAPF solvers, we propose a new method, Ensembling Prioritized Hybrid Policies (EPH). We first propose a selective communication block to gather richer information for better agent coordination within multi-agent environments and train the model with a Q-learning-based algorithm. We further introduce three advanced inference strategies aimed at bolstering performance during the execution phase. First, we hybridize the neural policy with single-agent expert guidance for navigating conflict-free zones. Se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#20351;&#39134;&#34892;&#22120;&#22312;&#26410;&#30693;&#19988;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#26434;&#20081;&#29615;&#22659;&#20013;&#20855;&#26377;&#25935;&#25463;&#24615;&#35843;&#25972;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20998;&#23618;&#23398;&#20064;&#21644;&#35268;&#21010;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32447;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#21644;&#39044;&#35757;&#32451;&#24494;&#35843;&#22870;&#21169;&#26041;&#26696;&#33719;&#24471;&#21487;&#37096;&#32626;&#30340;&#31574;&#30053;&#65292;&#22312;&#20223;&#30495;&#20013;&#26174;&#31034;&#20986;&#27604;&#24658;&#23450;&#25935;&#25463;&#24615;&#22522;&#32447;&#21644;&#21478;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#26356;&#20248;&#36234;&#30340;&#39134;&#34892;&#25928;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04586</link><description>&lt;p&gt;
&#22312;&#26434;&#20081;&#29615;&#22659;&#20013;&#23398;&#20064;&#39134;&#34892;&#25935;&#25463;&#24615;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Learning Agility Adaptation for Flight in Clutter
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#20351;&#39134;&#34892;&#22120;&#22312;&#26410;&#30693;&#19988;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#26434;&#20081;&#29615;&#22659;&#20013;&#20855;&#26377;&#25935;&#25463;&#24615;&#35843;&#25972;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20998;&#23618;&#23398;&#20064;&#21644;&#35268;&#21010;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32447;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#21644;&#39044;&#35757;&#32451;&#24494;&#35843;&#22870;&#21169;&#26041;&#26696;&#33719;&#24471;&#21487;&#37096;&#32626;&#30340;&#31574;&#30053;&#65292;&#22312;&#20223;&#30495;&#20013;&#26174;&#31034;&#20986;&#27604;&#24658;&#23450;&#25935;&#25463;&#24615;&#22522;&#32447;&#21644;&#21478;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#26356;&#20248;&#36234;&#30340;&#39134;&#34892;&#25928;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#29289;&#23398;&#20064;&#36866;&#24212;&#20854;&#36816;&#21160;&#33021;&#21147;&#21644;&#25805;&#20316;&#29615;&#22659;&#30340;&#25935;&#25463;&#24615;&#12290;&#31227;&#21160;&#26426;&#22120;&#20154;&#20063;&#24212;&#23637;&#31034;&#36825;&#31181;&#33021;&#21147;&#65292;&#23558;&#25935;&#25463;&#24615;&#21644;&#23433;&#20840;&#24615;&#32467;&#21512;&#36215;&#26469;&#12290;&#26412;&#25991;&#26088;&#22312;&#36171;&#20104;&#39134;&#34892;&#22120;&#22312;&#26410;&#30693;&#19988;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#26434;&#20081;&#29615;&#22659;&#20013;&#36866;&#24212;&#25935;&#25463;&#24615;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#23398;&#20064;&#21644;&#35268;&#21010;&#26694;&#26550;&#65292;&#32467;&#21512;&#35797;&#38169;&#23398;&#20064;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#36712;&#36857;&#29983;&#25104;&#26041;&#27861;&#26469;&#20840;&#38754;&#23398;&#20064;&#25935;&#25463;&#24615;&#31574;&#30053;&#12290;&#25105;&#20204;&#20351;&#29992;&#22312;&#32447;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#21644;&#39044;&#35757;&#32451;&#24494;&#35843;&#22870;&#21169;&#26041;&#26696;&#26469;&#33719;&#24471;&#21487;&#37096;&#32626;&#30340;&#31574;&#30053;&#12290;&#22312;&#20223;&#30495;&#20013;&#30340;&#32479;&#35745;&#32467;&#26524;&#26174;&#31034;&#65292;&#30456;&#36739;&#20110;&#24658;&#23450;&#25935;&#25463;&#24615;&#22522;&#32447;&#21644;&#21478;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39134;&#34892;&#25928;&#29575;&#21644;&#23433;&#20840;&#24615;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#29305;&#21035;&#26159;&#65292;&#35813;&#31574;&#30053;&#23548;&#33268;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04586v1 Announce Type: cross  Abstract: Animals learn to adapt agility of their movements to their capabilities and the environment they operate in. Mobile robots should also demonstrate this ability to combine agility and safety. The aim of this work is to endow flight vehicles with the ability of agility adaptation in prior unknown and partially observable cluttered environments. We propose a hierarchical learning and planning framework where we utilize both trial and error to comprehensively learn an agility policy with the vehicle's observation as the input, and well-established methods of model-based trajectory generation. Technically, we use online model-free reinforcement learning and a pre-training-fine-tuning reward scheme to obtain the deployable policy. The statistical results in simulation demonstrate the advantages of our method over the constant agility baselines and an alternative method in terms of flight efficiency and safety. In particular, the policy leads
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;Large Language Model (LLM)&#20026;&#22522;&#30784;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#65292;&#29992;&#20110;&#19982;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#20799;&#31461;&#36827;&#34892;&#21475;&#22836;&#20132;&#27969;&#65292;&#25945;&#25480;&#36879;&#35270;&#33021;&#21147;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;LLM&#31649;&#36947;&#65292;&#21457;&#29616;GPT-2 + BART&#31649;&#36947;&#21487;&#20197;&#26356;&#22909;&#22320;&#29983;&#25104;&#38382;&#39064;&#21644;&#36873;&#25321;&#39033;&#12290;&#36825;&#31181;&#30740;&#31350;&#26377;&#21161;&#20110;&#25913;&#21892;&#33258;&#38381;&#30151;&#20799;&#31461;&#30340;&#31038;&#20132;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.00260</link><description>&lt;p&gt;
&#20197;LLM&#20026;&#22522;&#30784;&#23454;&#29616;&#38754;&#21521;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#20799;&#31461;&#30340;&#21487;&#25193;&#23637;&#26426;&#22120;&#20154;&#24178;&#39044;
&lt;/p&gt;
&lt;p&gt;
Towards scalable robotic intervention of children with Autism Spectrum Disorder using LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;Large Language Model (LLM)&#20026;&#22522;&#30784;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#65292;&#29992;&#20110;&#19982;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#20799;&#31461;&#36827;&#34892;&#21475;&#22836;&#20132;&#27969;&#65292;&#25945;&#25480;&#36879;&#35270;&#33021;&#21147;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;LLM&#31649;&#36947;&#65292;&#21457;&#29616;GPT-2 + BART&#31649;&#36947;&#21487;&#20197;&#26356;&#22909;&#22320;&#29983;&#25104;&#38382;&#39064;&#21644;&#36873;&#25321;&#39033;&#12290;&#36825;&#31181;&#30740;&#31350;&#26377;&#21161;&#20110;&#25913;&#21892;&#33258;&#38381;&#30151;&#20799;&#31461;&#30340;&#31038;&#20132;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#19982;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;(ASD)&#20799;&#31461;&#36827;&#34892;&#21475;&#22836;&#20132;&#27969;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#12290;&#36825;&#31181;&#20132;&#27969;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;Large Language Model (LLM)&#29983;&#25104;&#30340;&#25991;&#26412;&#26469;&#25945;&#25480;&#36879;&#35270;&#33021;&#21147;&#12290;&#31038;&#20132;&#26426;&#22120;&#20154;NAO&#25198;&#28436;&#20102;&#19968;&#20010;&#21050;&#28608;&#22120;(&#21475;&#22836;&#25551;&#36848;&#31038;&#20132;&#24773;&#26223;&#24182;&#25552;&#38382;)&#12289;&#25552;&#31034;&#22120;(&#25552;&#20379;&#19977;&#20010;&#36873;&#25321;&#39033;&#20379;&#36873;&#25321;)&#21644;&#22870;&#21169;&#22120;(&#24403;&#31572;&#26696;&#27491;&#30830;&#26102;&#32473;&#20104;&#31216;&#36190;)&#30340;&#35282;&#33394;&#12290;&#23545;&#20110;&#21050;&#28608;&#22120;&#30340;&#35282;&#33394;&#65292;&#31038;&#20132;&#24773;&#22659;&#12289;&#38382;&#39064;&#21644;&#36873;&#25321;&#39033;&#26159;&#20351;&#29992;&#25105;&#20204;&#30340;LLM&#31649;&#36947;&#29983;&#25104;&#30340;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181;&#26041;&#27861;&#65306;GPT-2 + BART&#21644;GPT-2 + GPT-2&#65292;&#20854;&#20013;&#31532;&#19968;&#20010;GPT-2&#22312;&#31649;&#36947;&#20013;&#26159;&#29992;&#20110;&#26080;&#30417;&#30563;&#31038;&#20132;&#24773;&#22659;&#29983;&#25104;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;SOCIALIQA&#25968;&#25454;&#38598;&#23545;&#25152;&#26377;LLM&#31649;&#36947;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-2 + BART&#31649;&#36947;&#22312;&#36890;&#36807;&#32467;&#21512;&#21508;&#33258;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#29983;&#25104;&#38382;&#39064;&#21644;&#36873;&#25321;&#39033;&#26041;&#38754;&#20855;&#26377;&#36739;&#22909;&#30340;BERTscore&#12290;&#36825;&#31181;&#35266;&#23519;&#32467;&#26524;&#20063;&#19982;&#20799;&#31461;&#22312;&#20132;&#20114;&#36807;&#31243;&#20013;&#30340;&#21512;&#20316;&#27700;&#24179;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a social robot capable of verbally interacting with children with Autism Spectrum Disorder (ASD). This communication is meant to teach perspective-taking using text generated using a Large Language Model (LLM) pipeline. The social robot NAO acts as a stimulator (verbally describes a social situation and asks a question), prompter (presents three options to choose from), and reinforcer (praises when the answer is correct). For the role of the stimulator, the social situation, questions, and options are generated using our LLM pipeline. We compare two approaches: GPT-2 + BART and GPT-2 + GPT-2, where the first GPT-2 common between the pipelines is used for unsupervised social situation generation. We use the SOCIALIQA dataset to fine-tune all of our LLM pipelines. We found that the GPT-2 + BART pipeline had a better BERTscore for generating the questions and the options by combining their individual loss functions. This observation was also consistent with the h
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#24038;&#21491;&#21322;&#29699;&#19987;&#38376;&#21270;&#25511;&#21046;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;&#19981;&#21516;&#30340;&#36816;&#21160;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21327;&#35843;&#24615;&#12289;&#36816;&#21160;&#25928;&#29575;&#21644;&#20301;&#32622;&#31283;&#23450;&#24615;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.14057</link><description>&lt;p&gt;
&#24038;/&#21491;&#33041;&#12289;&#20154;&#31867;&#36816;&#21160;&#25511;&#21046;&#21450;&#23545;&#26426;&#22120;&#20154;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Left/Right Brain, human motor control and the implications for robotics. (arXiv:2401.14057v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#24038;&#21491;&#21322;&#29699;&#19987;&#38376;&#21270;&#25511;&#21046;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;&#19981;&#21516;&#30340;&#36816;&#21160;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21327;&#35843;&#24615;&#12289;&#36816;&#21160;&#25928;&#29575;&#21644;&#20301;&#32622;&#31283;&#23450;&#24615;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#36816;&#21160;&#25511;&#21046;&#22120;&#30456;&#23545;&#20256;&#32479;&#25511;&#21046;&#26041;&#27861;&#20855;&#26377;&#21508;&#31181;&#20248;&#28857;&#65292;&#28982;&#32780;&#30001;&#20110;&#20854;&#26080;&#27861;&#20135;&#29983;&#21487;&#38752;&#30340;&#31934;&#30830;&#36816;&#21160;&#65292;&#22240;&#27492;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#37319;&#29992;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#21452;&#20391;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20316;&#20026;&#36816;&#21160;&#20219;&#21153;&#30340;&#25511;&#21046;&#31995;&#32479;&#12290;&#25105;&#20204;&#26088;&#22312;&#23454;&#29616;&#31867;&#20284;&#20110;&#20154;&#31867;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#35266;&#23519;&#21040;&#30340;&#21322;&#29699;&#19987;&#38376;&#21270;&#65306;&#20248;&#21183;&#31995;&#32479;&#65288;&#36890;&#24120;&#26159;&#21491;&#25163;&#12289;&#24038;&#21322;&#29699;&#65289;&#25797;&#38271;&#21327;&#35843;&#21644;&#36816;&#21160;&#25928;&#29575;&#30340;&#20219;&#21153;&#65292;&#32780;&#38750;&#20248;&#21183;&#31995;&#32479;&#22312;&#38656;&#35201;&#20301;&#32622;&#31283;&#23450;&#24615;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#23545;&#21322;&#29699;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#19987;&#38376;&#21270;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20855;&#26377;&#19987;&#38376;&#21270;&#21322;&#29699;&#21644;&#26080;&#19987;&#38376;&#21270;&#21322;&#29699;&#12289;&#20855;&#26377;&#21322;&#29699;&#38388;&#36830;&#25509;&#65288;&#20195;&#34920;&#29983;&#29289;&#23398;&#33041;&#26725;&#65289;&#21644;&#26080;&#21322;&#29699;&#38388;&#36830;&#25509;&#12289;&#20855;&#26377;&#19987;&#38376;&#21270;&#21644;&#26080;&#19987;&#38376;&#21270;&#30340;&#21333;&#20391;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Network movement controllers promise a variety of advantages over conventional control methods however they are not widely adopted due to their inability to produce reliably precise movements. This research explores a bilateral neural network architecture as a control system for motor tasks. We aimed to achieve hemispheric specialisation similar to what is observed in humans across different tasks; the dominant system (usually the right hand, left hemisphere) excels at tasks involving coordination and efficiency of movement, and the non-dominant system performs better at tasks requiring positional stability. Specialisation was achieved by training the hemispheres with different loss functions tailored toward the expected behaviour of the respective hemispheres. We compared bilateral models with and without specialised hemispheres, with and without inter-hemispheric connectivity (representing the biological Corpus Callosum), and unilateral models with and without specialisation. 
&lt;/p&gt;</description></item></channel></rss>