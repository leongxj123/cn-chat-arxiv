<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#35821;&#38899;&#36716;&#24405;&#21644;&#35821;&#38899;&#38750;&#35328;&#35821;&#29305;&#24449;&#25972;&#21512;&#21040;LLM&#20915;&#31574;&#20013;&#26469;&#25913;&#21892;&#26426;&#22120;&#20154;&#23548;&#33322;&#20013;&#30340;&#20915;&#31574;&#33021;&#21147;&#65292;&#36229;&#36234;&#20102;&#20165;&#20351;&#29992;&#25991;&#23383;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.03494</link><description>&lt;p&gt;
&#36229;&#36234;&#25991;&#23383;&#65306;&#36890;&#36807;&#35821;&#38899;&#32447;&#32034;&#25913;&#21892;LLM&#22312;&#26426;&#22120;&#20154;&#23548;&#33322;&#20013;&#30340;&#20915;&#31574;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Beyond Text: Improving LLM's Decision Making for Robot Navigation via Vocal Cues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#35821;&#38899;&#36716;&#24405;&#21644;&#35821;&#38899;&#38750;&#35328;&#35821;&#29305;&#24449;&#25972;&#21512;&#21040;LLM&#20915;&#31574;&#20013;&#26469;&#25913;&#21892;&#26426;&#22120;&#20154;&#23548;&#33322;&#20013;&#30340;&#20915;&#31574;&#33021;&#21147;&#65292;&#36229;&#36234;&#20102;&#20165;&#20351;&#29992;&#25991;&#23383;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#24378;&#35843;&#20102;&#22522;&#20110;&#25991;&#26412;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#20851;&#38190;&#32570;&#28857;&#65292;&#34920;&#26126;&#20165;&#20351;&#29992;&#25991;&#26412;&#20316;&#20026;&#23545;&#35805;&#30340;&#27169;&#24577;&#22312;&#27492;&#31867;&#24212;&#29992;&#20013;&#23384;&#22312;&#19981;&#36275;&#20043;&#22788;&#12290;&#34429;&#28982;LLM&#22312;&#22788;&#29702;&#25991;&#26412;&#26041;&#38754;&#22312;&#36825;&#20123;&#20154;&#26426;&#23545;&#35805;&#20013;&#38750;&#24120;&#20986;&#33394;&#65292;&#20294;&#22312;&#31038;&#20132;&#23548;&#33322;&#31561;&#24773;&#22659;&#19979;&#65292;&#20182;&#20204;&#22312;&#22788;&#29702;&#21475;&#22836;&#25351;&#20196;&#30340;&#32454;&#24494;&#20043;&#22788;&#26102;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#20854;&#20013;&#30340;&#27495;&#20041;&#21644;&#19981;&#30830;&#23450;&#24615;&#21487;&#33021;&#20250;&#21066;&#24369;&#23545;&#26426;&#22120;&#20154;&#21644;&#20854;&#20182;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20449;&#20219;&#12290;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#36229;&#36234;&#25991;&#23383;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#36825;&#20123;&#38899;&#39057;&#22238;&#24212;&#30340;&#35821;&#38899;&#38750;&#35328;&#35821;&#29305;&#24449;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#20123;&#29305;&#24449;&#26159;&#21475;&#22836;&#20132;&#27969;&#20013;&#19981;&#28041;&#21450;&#25991;&#23383;&#25514;&#36766;&#30340;&#26041;&#38754;&#65292;&#36890;&#36807;&#34920;&#36798;&#26041;&#24335;&#20256;&#36798;&#24847;&#20041;&#21644;&#32454;&#24494;&#24046;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#36229;&#36234;&#25991;&#23383;&#8221;&#65307;&#19968;&#31181;&#36890;&#36807;&#38598;&#25104;&#38899;&#39057;&#36716;&#24405;&#20197;&#21450;&#36825;&#20123;&#29305;&#24449;&#30340;&#37096;&#20998;&#26469;&#25913;&#21892;LLM&#20915;&#31574;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#29305;&#24449;&#20391;&#37325;&#24773;&#24863;&#21644;&#26356;&#19982;&#20154;&#26426;&#23545;&#35805;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work highlights a critical shortcoming in text-based Large Language Models (LLMs) used for human-robot interaction, demonstrating that text alone as a conversation modality falls short in such applications. While LLMs excel in processing text in these human conversations, they struggle with the nuances of verbal instructions in scenarios like social navigation, where ambiguity and uncertainty can erode trust in robotic and other AI systems. We can address this shortcoming by moving beyond text and additionally focusing on the paralinguistic features of these audio responses. These features are the aspects of spoken communication that do not involve the literal wording (lexical content) but convey meaning and nuance through how something is said. We present "Beyond Text"; an approach that improves LLM decision-making by integrating audio transcription along with a subsection of these features, which focus on the affect and more relevant in human-robot conversations. This approach n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21322;&#30417;&#30563;&#39034;&#24207;&#21464;&#20998;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#36719;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#36328;&#22495;&#36801;&#31227;&#23398;&#20064;&#21644;&#29366;&#24577;&#25512;&#26029;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#22788;&#29702;&#26576;&#20123;&#26426;&#22120;&#20154;&#37197;&#32622;&#19979;&#23384;&#22312;&#32570;&#22833;&#29366;&#24577;&#26631;&#31614;&#30340;&#24773;&#20917;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#29305;&#24449;&#31354;&#38388;&#36801;&#31227;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#22312;&#22810;&#20010;&#37197;&#32622;&#19979;&#30340;&#28508;&#22312;&#29305;&#24449;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.01693</link><description>&lt;p&gt;
&#36890;&#36807;&#21322;&#30417;&#30563;&#39034;&#24207;&#21464;&#20998;&#36125;&#21494;&#26031;&#26694;&#26550;&#23454;&#29616;&#36719;&#26426;&#22120;&#20154;&#30340;&#36328;&#22495;&#36801;&#31227;&#23398;&#20064;&#21644;&#29366;&#24577;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Cross-domain Transfer Learning and State Inference for Soft Robots via a Semi-supervised Sequential Variational Bayes Framework. (arXiv:2303.01693v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21322;&#30417;&#30563;&#39034;&#24207;&#21464;&#20998;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#36719;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#36328;&#22495;&#36801;&#31227;&#23398;&#20064;&#21644;&#29366;&#24577;&#25512;&#26029;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#22788;&#29702;&#26576;&#20123;&#26426;&#22120;&#20154;&#37197;&#32622;&#19979;&#23384;&#22312;&#32570;&#22833;&#29366;&#24577;&#26631;&#31614;&#30340;&#24773;&#20917;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#29305;&#24449;&#31354;&#38388;&#36801;&#31227;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#22312;&#22810;&#20010;&#37197;&#32622;&#19979;&#30340;&#28508;&#22312;&#29305;&#24449;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#65288;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#36719;&#26426;&#22120;&#20154;&#24314;&#27169;&#21644;&#29366;&#24577;&#25512;&#26029;&#26174;&#31034;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#25165;&#33021;&#26377;&#25928;&#22320;&#36816;&#34892;&#65292;&#36825;&#38656;&#35201;&#36827;&#34892;&#35814;&#23613;&#21644;&#36136;&#37327;&#33391;&#22909;&#30340;&#25968;&#25454;&#37319;&#38598;&#65292;&#23588;&#20854;&#26159;&#29366;&#24577;&#26631;&#31614;&#30340;&#37319;&#38598;&#12290;&#22240;&#27492;&#65292;&#30001;&#20110;&#36719;&#26426;&#22120;&#20154;&#30340;&#20256;&#24863;&#22120;&#21270;&#22256;&#38590;&#21644;&#22312;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#25910;&#38598;&#25968;&#25454;&#30340;&#19981;&#20415;&#31561;&#21407;&#22240;&#65292;&#33719;&#21462;&#26631;&#27880;&#30340;&#36719;&#26426;&#22120;&#20154;&#31995;&#32479;&#29366;&#24577;&#25968;&#25454;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21322;&#30417;&#30563;&#39034;&#24207;&#21464;&#20998;&#36125;&#21494;&#26031;&#65288;DSVB&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#26576;&#20123;&#26426;&#22120;&#20154;&#37197;&#32622;&#20013;&#23384;&#22312;&#32570;&#22833;&#29366;&#24577;&#26631;&#31614;&#30340;&#36719;&#26426;&#22120;&#20154;&#30340;&#36801;&#31227;&#23398;&#20064;&#21644;&#29366;&#24577;&#25512;&#26029;&#12290;&#32771;&#34385;&#21040;&#36719;&#26426;&#22120;&#20154;&#22312;&#19981;&#21516;&#30340;&#26426;&#22120;&#20154;&#37197;&#32622;&#19979;&#21487;&#33021;&#23637;&#29616;&#20986;&#19981;&#21516;&#30340;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#29305;&#24449;&#31354;&#38388;&#36801;&#31227;&#31574;&#30053;&#65292;&#20197;&#20419;&#36827;&#22312;&#22810;&#20010;&#37197;&#32622;&#19979;&#30340;&#28508;&#22312;&#29305;&#24449;&#30340;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, data-driven models such as deep neural networks have shown to be promising tools for modelling and state inference in soft robots. However, voluminous amounts of data are necessary for deep models to perform effectively, which requires exhaustive and quality data collection, particularly of state labels. Consequently, obtaining labelled state data for soft robotic systems is challenged for various reasons, including difficulty in the sensorization of soft robots and the inconvenience of collecting data in unstructured environments. To address this challenge, in this paper, we propose a semi-supervised sequential variational Bayes (DSVB) framework for transfer learning and state inference in soft robots with missing state labels on certain robot configurations. Considering that soft robots may exhibit distinct dynamics under different robot configurations, a feature space transfer strategy is also incorporated to promote the adaptation of latent features across multiple config
&lt;/p&gt;</description></item></channel></rss>