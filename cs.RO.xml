<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#31526;&#21495;&#21270;&#27169;&#20223;&#23398;&#20064;&#65288;SIL&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#65288;ILP&#65289;&#26469;&#23398;&#20064;&#20174;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#36879;&#26126;&#12289;&#21487;&#35299;&#37322;&#21644;&#27867;&#21270;&#30340;&#39550;&#39542;&#31574;&#30053;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;SIL&#19981;&#20165;&#25552;&#39640;&#20102;&#39550;&#39542;&#31574;&#30053;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#36824;&#26174;&#33879;&#25913;&#36827;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#39550;&#39542;&#24773;&#20917;&#19979;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16025</link><description>&lt;p&gt;
&#31526;&#21495;&#21270;&#27169;&#20223;&#23398;&#20064;&#65306;&#20174;&#40657;&#30418;&#21040;&#21487;&#35299;&#37322;&#30340;&#39550;&#39542;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Symbolic Imitation Learning: From Black-Box to Explainable Driving Policies. (arXiv:2309.16025v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#31526;&#21495;&#21270;&#27169;&#20223;&#23398;&#20064;&#65288;SIL&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#65288;ILP&#65289;&#26469;&#23398;&#20064;&#20174;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#36879;&#26126;&#12289;&#21487;&#35299;&#37322;&#21644;&#27867;&#21270;&#30340;&#39550;&#39542;&#31574;&#30053;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;SIL&#19981;&#20165;&#25552;&#39640;&#20102;&#39550;&#39542;&#31574;&#30053;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#36824;&#26174;&#33879;&#25913;&#36827;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#39550;&#39542;&#24773;&#20917;&#19979;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#20379;&#20102;&#20174;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#33719;&#21462;&#39550;&#39542;&#31574;&#30053;&#30340;&#26377;&#25928;&#25163;&#27573;&#65292;&#20294;&#22312;&#21487;&#35299;&#37322;&#24615;&#21644;&#27867;&#21270;&#24615;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#23616;&#38480;&#24615;&#12290;&#36825;&#20123;&#32570;&#28857;&#22312;&#33258;&#21160;&#39550;&#39542;&#31561;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#23588;&#20026;&#20196;&#20154;&#25285;&#24551;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#31526;&#21495;&#21270;&#27169;&#20223;&#23398;&#20064;&#65288;SIL&#65289;&#65292;&#19968;&#31181;&#20351;&#29992;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#65288;ILP&#65289;&#23398;&#20064;&#20174;&#21487;&#29992;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#36879;&#26126;&#12289;&#21487;&#35299;&#37322;&#21644;&#27867;&#21270;&#30340;&#39550;&#39542;&#31574;&#30053;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#12290;&#21033;&#29992;&#30495;&#23454;&#19990;&#30028;&#30340;highD&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#27604;&#36739;&#20998;&#26512;&#65292;&#19982;&#24403;&#21069;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;SIL&#19981;&#20165;&#25552;&#39640;&#20102;&#39550;&#39542;&#31574;&#30053;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#36824;&#26174;&#33879;&#25552;&#39640;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#39550;&#39542;&#24773;&#20917;&#19979;&#30340;&#36866;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#36825;&#39033;&#24037;&#20316;&#20026;&#23454;&#29616;&#26356;&#21487;&#38752;&#21644;&#21487;&#35299;&#37322;&#30340;&#39550;&#39542;&#31574;&#30053;&#25171;&#24320;&#20102;&#19968;&#26465;&#26032;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current methods of imitation learning (IL), primarily based on deep neural networks, offer efficient means for obtaining driving policies from real-world data but suffer from significant limitations in interpretability and generalizability. These shortcomings are particularly concerning in safety-critical applications like autonomous driving. In this paper, we address these limitations by introducing Symbolic Imitation Learning (SIL), a groundbreaking method that employs Inductive Logic Programming (ILP) to learn driving policies which are transparent, explainable and generalisable from available datasets. Utilizing the real-world highD dataset, we subject our method to a rigorous comparative analysis against prevailing neural-network-based IL methods. Our results demonstrate that SIL not only enhances the interpretability of driving policies but also significantly improves their applicability across varied driving situations. Hence, this work offers a novel pathway to more reliable an
&lt;/p&gt;</description></item></channel></rss>