<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CMP&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LiDAR&#20449;&#21495;&#20316;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#21512;&#20316;&#24863;&#30693;&#21644;&#36816;&#21160;&#39044;&#27979;&#27169;&#22359;&#20849;&#20139;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#21512;&#20316;&#36816;&#21160;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.17916</link><description>&lt;p&gt;
CMP&#65306;&#20855;&#26377;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#30340;&#21512;&#20316;&#36816;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
CMP: Cooperative Motion Prediction with Multi-Agent Communication
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17916
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CMP&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LiDAR&#20449;&#21495;&#20316;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#21512;&#20316;&#24863;&#30693;&#21644;&#36816;&#21160;&#39044;&#27979;&#27169;&#22359;&#20849;&#20139;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#21512;&#20316;&#36816;&#21160;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#30340;&#21457;&#23637;&#21644;&#36710;&#32852;&#32593;&#65288;V2X&#65289;&#36890;&#20449;&#30340;&#25104;&#29087;&#65292;&#21512;&#20316;&#36830;&#25509;&#30340;&#33258;&#21160;&#21270;&#36710;&#36742;&#65288;CAVs&#65289;&#30340;&#21151;&#33021;&#21464;&#24471;&#21487;&#33021;&#12290;&#26412;&#25991;&#22522;&#20110;&#21512;&#20316;&#24863;&#30693;&#65292;&#25506;&#35752;&#20102;&#21512;&#20316;&#36816;&#21160;&#39044;&#27979;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;CMP&#20197;LiDAR&#20449;&#21495;&#20316;&#20026;&#36755;&#20837;&#65292;&#20197;&#22686;&#24378;&#36319;&#36394;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#19982;&#36807;&#21435;&#19987;&#27880;&#20110;&#21512;&#20316;&#24863;&#30693;&#25110;&#36816;&#21160;&#39044;&#27979;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26159;&#25105;&#20204;&#25152;&#30693;&#30340;&#31532;&#19968;&#20010;&#35299;&#20915;CAVs&#22312;&#24863;&#30693;&#21644;&#39044;&#27979;&#27169;&#22359;&#20013;&#20849;&#20139;&#20449;&#24687;&#30340;&#32479;&#19968;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#35774;&#35745;&#20013;&#36824;&#34701;&#20837;&#20102;&#33021;&#22815;&#23481;&#24525;&#29616;&#23454;V2X&#24102;&#23485;&#38480;&#21046;&#21644;&#20256;&#36755;&#24310;&#36831;&#30340;&#29420;&#29305;&#33021;&#21147;&#65292;&#21516;&#26102;&#22788;&#29702;&#24222;&#22823;&#30340;&#24863;&#30693;&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#39044;&#27979;&#32858;&#21512;&#27169;&#22359;&#65292;&#32479;&#19968;&#20102;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17916v1 Announce Type: cross  Abstract: The confluence of the advancement of Autonomous Vehicles (AVs) and the maturity of Vehicle-to-Everything (V2X) communication has enabled the capability of cooperative connected and automated vehicles (CAVs). Building on top of cooperative perception, this paper explores the feasibility and effectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR signals as input to enhance tracking and prediction capabilities. Unlike previous work that focuses separately on either cooperative perception or motion prediction, our framework, to the best of our knowledge, is the first to address the unified problem where CAVs share information in both perception and prediction modules. Incorporated into our design is the unique capability to tolerate realistic V2X bandwidth limitations and transmission delays, while dealing with bulky perception representations. We also propose a prediction aggregation module, which unifies the predict
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#27169;&#22359;&#21270;&#30340;&#21327;&#20316;&#20307;&#29616;&#26234;&#33021;&#20307;&#65292;&#23454;&#29616;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36229;&#36234;&#35268;&#21010;&#26041;&#27861;&#24182;&#23637;&#31034;&#26377;&#25928;&#27807;&#36890;&#12290;</title><link>https://arxiv.org/abs/2307.02485</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27169;&#22359;&#21270;&#22320;&#26500;&#24314;&#21327;&#20316;&#20307;&#29616;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Building Cooperative Embodied Agents Modularly with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.02485
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#27169;&#22359;&#21270;&#30340;&#21327;&#20316;&#20307;&#29616;&#26234;&#33021;&#20307;&#65292;&#23454;&#29616;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36229;&#36234;&#35268;&#21010;&#26041;&#27861;&#24182;&#23637;&#31034;&#26377;&#25928;&#27807;&#36890;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22788;&#29702;&#20855;&#26377;&#21435;&#20013;&#24515;&#21270;&#25511;&#21046;&#12289;&#21407;&#22987;&#24863;&#30693;&#35266;&#23519;&#12289;&#26114;&#36149;&#36890;&#35759;&#21644;&#22810;&#30446;&#26631;&#20219;&#21153;&#30340;&#20855;&#26377;&#21508;&#31181;&#20307;&#29616;&#29615;&#22659;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#38382;&#39064;&#12290;&#19982;&#20808;&#21069;&#30740;&#31350;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24120;&#35782;&#30693;&#35782;&#12289;&#25512;&#29702;&#33021;&#21147;&#12289;&#35821;&#35328;&#29702;&#35299;&#21644;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#65292;&#24182;&#23558;&#23427;&#20204;&#26080;&#32541;&#22320;&#34701;&#20837;&#21040;&#19968;&#20010;&#19982;&#24863;&#30693;&#12289;&#35760;&#24518;&#21644;&#25191;&#34892;&#30456;&#32467;&#21512;&#30340;&#35748;&#30693;&#21551;&#21457;&#24335;&#27169;&#22359;&#21270;&#26694;&#26550;&#20013;&#12290;&#20174;&#32780;&#26500;&#24314;&#20102;&#19968;&#20010;&#21487;&#20197;&#35268;&#21010;&#12289;&#27807;&#36890;&#21644;&#19982;&#20854;&#20182;&#20154;&#21512;&#20316;&#20197;&#39640;&#25928;&#23436;&#25104;&#38271;&#26102;&#31243;&#20219;&#21153;&#30340;&#21512;&#20316;&#20307;&#29616;&#26234;&#33021;&#20307;CoELA&#12290;&#25105;&#20204;&#22312;C-WAH&#21644;TDW-MAT&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#30001;GPT-4&#39537;&#21160;&#30340;CoELA&#21487;&#20197;&#36229;&#36234;&#24378;&#22823;&#30340;&#22522;&#20110;&#35268;&#21010;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20986;&#26032;&#20852;&#30340;&#26377;&#25928;&#27807;&#36890;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.02485v2 Announce Type: replace  Abstract: In this work, we address challenging multi-agent cooperation problems with decentralized control, raw sensory observations, costly communication, and multi-objective tasks instantiated in various embodied environments. While previous research either presupposes a cost-free communication channel or relies on a centralized controller with shared observations, we harness the commonsense knowledge, reasoning ability, language comprehension, and text generation prowess of LLMs and seamlessly incorporate them into a cognitive-inspired modular framework that integrates with perception, memory, and execution. Thus building a Cooperative Embodied Language Agent CoELA, who can plan, communicate, and cooperate with others to accomplish long-horizon tasks efficiently. Our experiments on C-WAH and TDW-MAT demonstrate that CoELA driven by GPT-4 can surpass strong planning-based methods and exhibit emergent effective communication. Though current O
&lt;/p&gt;</description></item></channel></rss>