<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>PhysORD&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#23558;&#29289;&#29702;&#23450;&#24459;&#34701;&#20837;&#31070;&#32463;&#27169;&#22411;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#36234;&#37326;&#39550;&#39542;&#20013;&#30340;&#36816;&#21160;&#39044;&#27979;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.01596</link><description>&lt;p&gt;
PhysORD&#65306;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#29992;&#20110;&#36234;&#37326;&#39550;&#39542;&#20013;&#27880;&#20837;&#29289;&#29702;&#23398;&#30340;&#36816;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
PhysORD: A Neuro-Symbolic Approach for Physics-infused Motion Prediction in Off-road Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01596
&lt;/p&gt;
&lt;p&gt;
PhysORD&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#23558;&#29289;&#29702;&#23450;&#24459;&#34701;&#20837;&#31070;&#32463;&#27169;&#22411;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#36234;&#37326;&#39550;&#39542;&#20013;&#30340;&#36816;&#21160;&#39044;&#27979;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#39044;&#27979;&#23545;&#20110;&#33258;&#20027;&#36234;&#37326;&#39550;&#39542;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#19982;&#22312;&#36947;&#36335;&#19978;&#39550;&#39542;&#30456;&#27604;&#65292;&#23427;&#38754;&#20020;&#30528;&#26356;&#22810;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#36710;&#36742;&#19982;&#22320;&#24418;&#20043;&#38388;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24314;&#27169;&#21160;&#24577;&#31995;&#32479;&#21644;&#22806;&#37096;&#24178;&#25200;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;&#30456;&#21453;&#65292;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#38590;&#20197;&#26126;&#30830;&#25429;&#25417;&#22522;&#26412;&#30340;&#29289;&#29702;&#23450;&#24459;&#65292;&#36825;&#24456;&#23481;&#26131;&#23548;&#33268;&#27867;&#21270;&#33021;&#21147;&#24046;&#12290;&#36890;&#36807;&#34701;&#21512;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#36825;&#20123;&#26041;&#27861;&#23558;&#29289;&#29702;&#23450;&#24459;&#23884;&#20837;&#31070;&#32463;&#27169;&#22411;&#20013;&#65292;&#21487;&#33021;&#26174;&#33879;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#37117;&#27809;&#26377;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#36234;&#37326;&#39550;&#39542;&#29615;&#22659;&#20013;&#36827;&#34892;&#35780;&#20272;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986; PhysORD&#65292;&#36825;&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#38598;&#25104;&#20102;&#23432;&#24658;&#23450;&#24459;&#65292;&#21363;&#27431;&#25289;-&#25289;&#26684;&#26391;&#26085;&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01596v1 Announce Type: cross  Abstract: Motion prediction is critical for autonomous off-road driving, however, it presents significantly more challenges than on-road driving because of the complex interaction between the vehicle and the terrain. Traditional physics-based approaches encounter difficulties in accurately modeling dynamic systems and external disturbance. In contrast, data-driven neural networks require extensive datasets and struggle with explicitly capturing the fundamental physical laws, which can easily lead to poor generalization. By merging the advantages of both methods, neuro-symbolic approaches present a promising direction. These methods embed physical laws into neural models, potentially significantly improving generalization capabilities. However, no prior works were evaluated in real-world settings for off-road driving. To bridge this gap, we present PhysORD, a neural-symbolic approach integrating the conservation law, i.e., the Euler-Lagrange equa
&lt;/p&gt;</description></item><item><title>TopoNav&#26159;&#19968;&#31181;&#25299;&#25169;&#23548;&#33322;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20027;&#21160;&#25299;&#25169;&#26144;&#23556;&#12289;&#20869;&#37096;&#22870;&#21169;&#26426;&#21046;&#21644;&#23618;&#27425;&#21270;&#30446;&#26631;&#20248;&#20808;&#32423;&#30340;&#32452;&#21512;&#26469;&#23454;&#29616;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#39640;&#25928;&#25506;&#32034;&#12290;</title><link>https://arxiv.org/abs/2402.04061</link><description>&lt;p&gt;
TopoNav&#65306;&#33410;&#32422;&#22870;&#21169;&#29615;&#22659;&#20013;&#39640;&#25928;&#25506;&#32034;&#30340;&#25299;&#25169;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
TopoNav: Topological Navigation for Efficient Exploration in Sparse Reward Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04061
&lt;/p&gt;
&lt;p&gt;
TopoNav&#26159;&#19968;&#31181;&#25299;&#25169;&#23548;&#33322;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20027;&#21160;&#25299;&#25169;&#26144;&#23556;&#12289;&#20869;&#37096;&#22870;&#21169;&#26426;&#21046;&#21644;&#23618;&#27425;&#21270;&#30446;&#26631;&#20248;&#20808;&#32423;&#30340;&#32452;&#21512;&#26469;&#23454;&#29616;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#39640;&#25928;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#26426;&#22120;&#20154;&#22312;&#26410;&#30693;&#21306;&#22495;&#30340;&#25506;&#32034;&#38754;&#20020;&#30528;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#8212;&#8212;&#22312;&#27809;&#26377;&#20808;&#21069;&#22320;&#22270;&#21644;&#26377;&#38480;&#22806;&#37096;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#23548;&#33322;&#12290;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#65292;&#36825;&#20010;&#25361;&#25112;&#26356;&#21152;&#20005;&#23803;&#65292;&#20256;&#32479;&#30340;&#25506;&#32034;&#25216;&#26415;&#24448;&#24448;&#22833;&#36133;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TopoNav&#65292;&#19968;&#31181;&#20840;&#26032;&#30340;&#26694;&#26550;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#23454;&#29616;&#39640;&#25928;&#12289;&#36866;&#24212;&#24615;&#24378;&#19988;&#30446;&#26631;&#23548;&#21521;&#30340;&#25506;&#32034;&#12290;TopoNav&#30340;&#22522;&#26412;&#26500;&#24314;&#27169;&#22359;&#26159;&#20027;&#21160;&#25299;&#25169;&#26144;&#23556;&#12289;&#20869;&#37096;&#22870;&#21169;&#26426;&#21046;&#21644;&#23618;&#27425;&#21270;&#30446;&#26631;&#20248;&#20808;&#32423;&#12290;&#22312;&#25506;&#32034;&#36807;&#31243;&#20013;&#65292;TopoNav&#26500;&#24314;&#20102;&#21160;&#24577;&#25299;&#25169;&#22320;&#22270;&#65292;&#25429;&#33719;&#20851;&#38190;&#20301;&#32622;&#21644;&#36335;&#24452;&#12290;&#23427;&#21033;&#29992;&#20869;&#37096;&#22870;&#21169;&#26469;&#25351;&#23548;&#26426;&#22120;&#20154;&#26397;&#30528;&#22320;&#22270;&#20013;&#25351;&#23450;&#30340;&#23376;&#30446;&#26631;&#21069;&#36827;&#65292;&#20419;&#36827;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#30340;&#32467;&#26500;&#21270;&#25506;&#32034;&#12290;&#20026;&#20102;&#30830;&#20445;&#39640;&#25928;&#23548;&#33322;&#65292;TopoNav&#37319;&#29992;&#20102;&#20998;&#23618;&#30446;&#26631;&#39537;&#21160;&#30340;&#20027;&#21160;&#25299;&#25169;&#26694;&#26550;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#20248;&#20808;&#32771;&#34385;&#26368;&#32039;&#24613;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous robots exploring unknown areas face a significant challenge -- navigating effectively without prior maps and with limited external feedback. This challenge intensifies in sparse reward environments, where traditional exploration techniques often fail. In this paper, we introduce TopoNav, a novel framework that empowers robots to overcome these constraints and achieve efficient, adaptable, and goal-oriented exploration. TopoNav's fundamental building blocks are active topological mapping, intrinsic reward mechanisms, and hierarchical objective prioritization. Throughout its exploration, TopoNav constructs a dynamic topological map that captures key locations and pathways. It utilizes intrinsic rewards to guide the robot towards designated sub-goals within this map, fostering structured exploration even in sparse reward settings. To ensure efficient navigation, TopoNav employs the Hierarchical Objective-Driven Active Topologies framework, enabling the robot to prioritize immed
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#21457;&#29616;&#22522;&#20110;&#28857;&#20113;&#30340;&#26041;&#27861;&#22312;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#21508;&#31181;&#39044;&#35757;&#32451;&#21644;&#27867;&#21270;&#20219;&#21153;&#20013;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#28857;&#20113;&#35266;&#27979;&#27169;&#24577;&#23545;&#20110;&#22797;&#26434;&#26426;&#22120;&#20154;&#20219;&#21153;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.02500</link><description>&lt;p&gt;
&#28857;&#20113;&#38382;&#39064;:&#37325;&#26032;&#24605;&#32771;&#19981;&#21516;&#35266;&#27979;&#31354;&#38388;&#23545;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Point Cloud Matters: Rethinking the Impact of Different Observation Spaces on Robot Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02500
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#21457;&#29616;&#22522;&#20110;&#28857;&#20113;&#30340;&#26041;&#27861;&#22312;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#21508;&#31181;&#39044;&#35757;&#32451;&#21644;&#27867;&#21270;&#20219;&#21153;&#20013;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#28857;&#20113;&#35266;&#27979;&#27169;&#24577;&#23545;&#20110;&#22797;&#26434;&#26426;&#22120;&#20154;&#20219;&#21153;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19981;&#21516;&#35266;&#27979;&#31354;&#38388;&#23545;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#19977;&#31181;&#20027;&#35201;&#27169;&#24577;&#65306;RGB&#65292;RGB-D&#21644;&#28857;&#20113;&#12290;&#36890;&#36807;&#22312;&#36229;&#36807;17&#20010;&#19981;&#21516;&#25509;&#35302;&#20016;&#23500;&#30340;&#25805;&#20316;&#20219;&#21153;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#28041;&#21450;&#20004;&#20010;&#22522;&#20934;&#21644;&#20223;&#30495;&#22120;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#19968;&#20010;&#26174;&#33879;&#30340;&#36235;&#21183;&#65306;&#22522;&#20110;&#28857;&#20113;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#26159;&#26368;&#31616;&#21333;&#30340;&#35774;&#35745;&#65292;&#36890;&#24120;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#20854;RGB&#21644;RGB-D&#30340;&#23545;&#24212;&#29289;&#12290;&#36825;&#22312;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#21644;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#20004;&#31181;&#24773;&#20917;&#19979;&#37117;&#26159;&#19968;&#33268;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#28857;&#20113;&#35266;&#27979;&#22312;&#30456;&#26426;&#35270;&#35282;&#12289;&#29031;&#26126;&#26465;&#20214;&#12289;&#22122;&#22768;&#27700;&#24179;&#21644;&#32972;&#26223;&#22806;&#35266;&#31561;&#21508;&#31181;&#20960;&#20309;&#21644;&#35270;&#35273;&#32447;&#32034;&#26041;&#38754;&#65292;&#37117;&#33021;&#25552;&#39640;&#31574;&#30053;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19977;&#32500;&#28857;&#20113;&#26159;&#22797;&#26434;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#26377;&#20215;&#20540;&#30340;&#35266;&#27979;&#27169;&#24577;&#12290;&#25105;&#20204;&#23558;&#20844;&#24320;&#25152;&#26377;&#30340;&#20195;&#30721;&#21644;&#26816;&#26597;&#28857;&#65292;&#24076;&#26395;&#25105;&#20204;&#30340;&#35266;&#28857;&#33021;&#24110;&#21161;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we explore the influence of different observation spaces on robot learning, focusing on three predominant modalities: RGB, RGB-D, and point cloud. Through extensive experimentation on over 17 varied contact-rich manipulation tasks, conducted across two benchmarks and simulators, we have observed a notable trend: point cloud-based methods, even those with the simplest designs, frequently surpass their RGB and RGB-D counterparts in performance. This remains consistent in both scenarios: training from scratch and utilizing pretraining. Furthermore, our findings indicate that point cloud observations lead to improved policy zero-shot generalization in relation to various geometry and visual clues, including camera viewpoints, lighting conditions, noise levels and background appearance. The outcomes suggest that 3D point cloud is a valuable observation modality for intricate robotic tasks. We will open-source all our codes and checkpoints, hoping that our insights can help de
&lt;/p&gt;</description></item></channel></rss>