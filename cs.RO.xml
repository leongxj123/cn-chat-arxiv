<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#32806;&#21512;LiDAR-IMU-&#36718;&#37324;&#31243;&#35745;&#31639;&#27861;&#65292;&#20351;&#29992;&#22312;&#32447;&#26657;&#20934;&#35299;&#20915;&#28369;&#31227;&#36716;&#21521;&#26426;&#22120;&#20154;&#22312;&#25361;&#25112;&#24615;&#29615;&#22659;&#20013;&#30340;&#28857;&#20113;&#36864;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.02515</link><description>&lt;p&gt;
&#21033;&#29992;&#22312;&#32447;&#26657;&#20934;&#36816;&#21160;&#27169;&#22411;&#30340;&#32039;&#32806;&#21512;LiDAR-IMU-&#36718;&#37324;&#31243;&#35745;&#31639;&#27861;&#29992;&#20110;&#28369;&#31227;&#36716;&#21521;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Tightly-Coupled LiDAR-IMU-Wheel Odometry with Online Calibration of a Kinematic Model for Skid-Steering Robots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02515
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#32806;&#21512;LiDAR-IMU-&#36718;&#37324;&#31243;&#35745;&#31639;&#27861;&#65292;&#20351;&#29992;&#22312;&#32447;&#26657;&#20934;&#35299;&#20915;&#28369;&#31227;&#36716;&#21521;&#26426;&#22120;&#20154;&#22312;&#25361;&#25112;&#24615;&#29615;&#22659;&#20013;&#30340;&#28857;&#20113;&#36864;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38567;&#36947;&#21644;&#38271;&#24266;&#26159;&#31227;&#21160;&#26426;&#22120;&#20154;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#65292;&#22240;&#20026;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;LiDAR&#28857;&#20113;&#20250;&#36864;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#28857;&#20113;&#36864;&#21270;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#28369;&#31227;&#36716;&#21521;&#26426;&#22120;&#20154;&#30340;&#32039;&#32806;&#21512;LiDAR-IMU-&#36718;&#37324;&#31243;&#35745;&#31639;&#27861;&#65292;&#21516;&#26102;&#36824;&#20351;&#29992;&#22312;&#32447;&#26657;&#20934;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#32447;&#24615;&#36718;&#23376;&#37324;&#31243;&#35745;&#22240;&#23376;&#65292;&#19981;&#20165;&#20316;&#20026;&#36816;&#21160;&#32422;&#26463;&#65292;&#36824;&#21487;&#20197;&#25191;&#34892;&#28369;&#31227;&#36716;&#21521;&#26426;&#22120;&#20154;&#36816;&#21160;&#27169;&#22411;&#30340;&#22312;&#32447;&#26657;&#20934;&#12290;&#23613;&#31649;&#36816;&#21160;&#27169;&#22411;&#21160;&#24577;&#21464;&#21270;&#65288;&#20363;&#22914;&#30001;&#20110;&#32974;&#21387;&#24341;&#36215;&#30340;&#36718;&#32974;&#21322;&#24452;&#21464;&#21270;&#65289;&#21644;&#22320;&#24418;&#26465;&#20214;&#21464;&#21270;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#22312;&#32447;&#26657;&#20934;&#26469;&#35299;&#20915;&#27169;&#22411;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#36864;&#21270;&#29615;&#22659;&#19979;&#65288;&#22914;&#38271;&#30452;&#24266;&#65289;&#36890;&#36807;&#26657;&#20934;&#32780;&#23454;&#29616;&#20934;&#30830;&#23450;&#20301;&#65292;&#21516;&#26102;LiDAR-IMU&#34701;&#21512;&#36816;&#20316;&#33391;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20272;&#35745;&#20102;&#36718;&#23376;&#37324;&#31243;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#65288;&#21363;&#21327;&#26041;&#24046;&#30697;&#38453;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02515v1 Announce Type: cross  Abstract: Tunnels and long corridors are challenging environments for mobile robots because a LiDAR point cloud should degenerate in these environments. To tackle point cloud degeneration, this study presents a tightly-coupled LiDAR-IMU-wheel odometry algorithm with an online calibration for skid-steering robots. We propose a full linear wheel odometry factor, which not only serves as a motion constraint but also performs the online calibration of kinematic models for skid-steering robots. Despite the dynamically changing kinematic model (e.g., wheel radii changes caused by tire pressures) and terrain conditions, our method can address the model error via online calibration. Moreover, our method enables an accurate localization in cases of degenerated environments, such as long and straight corridors, by calibration while the LiDAR-IMU fusion sufficiently operates. Furthermore, we estimate the uncertainty (i.e., covariance matrix) of the wheel o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38271;&#26399;&#21644;&#30701;&#26399;&#32422;&#26463;&#30340;&#26032;&#31639;&#27861;&#29992;&#20110;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#20219;&#21153;&#20013;&#21487;&#20197;&#21516;&#26102;&#20445;&#35777;&#36710;&#36742;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18209</link><description>&lt;p&gt;
&#38271;&#30701;&#26399;&#32422;&#26463;&#39537;&#21160;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Long and Short-Term Constraints Driven Safe Reinforcement Learning for Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38271;&#26399;&#21644;&#30701;&#26399;&#32422;&#26463;&#30340;&#26032;&#31639;&#27861;&#29992;&#20110;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#20219;&#21153;&#20013;&#21487;&#20197;&#21516;&#26102;&#20445;&#35777;&#36710;&#36742;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#20915;&#31574;&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#30001;&#20110;&#38656;&#35201;&#19982;&#29615;&#22659;&#20132;&#20114;&#65292;&#26080;&#27861;&#20445;&#35777;&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#65292;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#20854;&#22312;&#33258;&#21160;&#39550;&#39542;&#31561;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38271;&#26399;&#21644;&#30701;&#26399;&#32422;&#26463;&#65288;LSTC&#65289;&#30340;&#26032;&#31639;&#27861;&#29992;&#20110;&#23433;&#20840;RL&#12290;&#30701;&#26399;&#32422;&#26463;&#26088;&#22312;&#30830;&#20445;&#36710;&#36742;&#25506;&#27979;&#21040;&#30340;&#30701;&#26399;&#29366;&#24577;&#23433;&#20840;&#65292;&#32780;&#38271;&#26399;&#32422;&#26463;&#21017;&#30830;&#20445;&#25972;&#20307;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18209v1 Announce Type: cross  Abstract: Reinforcement learning (RL) has been widely used in decision-making tasks, but it cannot guarantee the agent's safety in the training process due to the requirements of interaction with the environment, which seriously limits its industrial applications such as autonomous driving. Safe RL methods are developed to handle this issue by constraining the expected safety violation costs as a training objective, but they still permit unsafe state occurrence, which is unacceptable in autonomous driving tasks. Moreover, these methods are difficult to achieve a balance between the cost and return expectations, which leads to learning performance degradation for the algorithms. In this paper, we propose a novel algorithm based on the long and short-term constraints (LSTC) for safe RL. The short-term constraint aims to guarantee the short-term state safety that the vehicle explores, while the long-term constraint ensures the overall safety of the
&lt;/p&gt;</description></item><item><title>EfficientZero V2&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#36890;&#36807;&#19968;&#31995;&#21015;&#25913;&#36827;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#36890;&#29992;&#31639;&#27861;DreamerV3&#26377;&#26174;&#33879;&#25552;&#21319;</title><link>https://arxiv.org/abs/2403.00564</link><description>&lt;p&gt;
&#39640;&#25928;Zero V2&#65306;&#22312;&#26377;&#38480;&#25968;&#25454;&#19979;&#25484;&#25569;&#31163;&#25955;&#21644;&#36830;&#32493;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
EfficientZero V2: Mastering Discrete and Continuous Control with Limited Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00564
&lt;/p&gt;
&lt;p&gt;
EfficientZero V2&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#36890;&#36807;&#19968;&#31995;&#21015;&#25913;&#36827;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#36890;&#29992;&#31639;&#27861;DreamerV3&#26377;&#26174;&#33879;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#30340;&#26679;&#26412;&#25928;&#29575;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#31639;&#27861;&#22312;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#27809;&#26377;&#19968;&#20010;&#33021;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#19968;&#30452;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;EfficientZero V2&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#39640;&#25928;RL&#31639;&#27861;&#35774;&#35745;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#25105;&#20204;&#23558;EfficientZero&#30340;&#24615;&#33021;&#25193;&#23637;&#21040;&#22810;&#20010;&#39046;&#22495;&#65292;&#28085;&#30422;&#36830;&#32493;&#21644;&#31163;&#25955;&#34892;&#21160;&#65292;&#20197;&#21450;&#35270;&#35273;&#21644;&#20302;&#32500;&#36755;&#20837;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#25105;&#20204;&#25552;&#20986;&#30340;&#25913;&#36827;&#65292;EfficientZero V2&#22312;&#26377;&#38480;&#25968;&#25454;&#35774;&#32622;&#19979;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#22823;&#24133;&#36229;&#36234;&#20102;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#65288;SOTA&#65289;&#12290;EfficientZero V2&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#36827;&#27493;&#65292;&#27604;&#22914;Atari 100k&#65292;Proprio Control&#31561;&#20013;&#65292;&#22312;66&#20010;&#35780;&#20272;&#20219;&#21153;&#20013;&#26377;50&#20010;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00564v1 Announce Type: cross  Abstract: Sample efficiency remains a crucial challenge in applying Reinforcement Learning (RL) to real-world tasks. While recent algorithms have made significant strides in improving sample efficiency, none have achieved consistently superior performance across diverse domains. In this paper, we introduce EfficientZero V2, a general framework designed for sample-efficient RL algorithms. We have expanded the performance of EfficientZero to multiple domains, encompassing both continuous and discrete actions, as well as visual and low-dimensional inputs. With a series of improvements we propose, EfficientZero V2 outperforms the current state-of-the-art (SOTA) by a significant margin in diverse tasks under the limited data setting. EfficientZero V2 exhibits a notable advancement over the prevailing general algorithm, DreamerV3, achieving superior outcomes in 50 of 66 evaluated tasks across diverse benchmarks, such as Atari 100k, Proprio Control, an
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#21270;DNN&#30340;&#25511;&#21046;&#22120;&#65292;&#36890;&#36807;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30830;&#20445;&#38381;&#29615;&#31283;&#23450;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#20248;&#21270;&#21442;&#25968;&#20197;&#23454;&#29616;&#25913;&#36827;&#30340;&#25511;&#21046;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#20851;&#20110;&#36319;&#36394;&#35823;&#24046;&#30340;&#26126;&#30830;&#19978;&#38480;&#12290;</title><link>https://arxiv.org/abs/2403.00381</link><description>&lt;p&gt;
&#22522;&#20110;&#32467;&#26500;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25289;&#26684;&#26391;&#26085;&#31995;&#32479;&#21453;&#27493;&#36712;&#36857;&#36319;&#36394;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Structured Deep Neural Networks-Based Backstepping Trajectory Tracking Control for Lagrangian Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00381
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#21270;DNN&#30340;&#25511;&#21046;&#22120;&#65292;&#36890;&#36807;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30830;&#20445;&#38381;&#29615;&#31283;&#23450;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#20248;&#21270;&#21442;&#25968;&#20197;&#23454;&#29616;&#25913;&#36827;&#30340;&#25511;&#21046;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#20851;&#20110;&#36319;&#36394;&#35823;&#24046;&#30340;&#26126;&#30830;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#23398;&#20064;&#25511;&#21046;&#22120;&#65292;&#22240;&#20026;&#20854;&#20986;&#33394;&#30340;&#36924;&#36817;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#40657;&#30418;&#29305;&#24615;&#23545;&#38381;&#29615;&#31283;&#23450;&#24615;&#20445;&#35777;&#21644;&#24615;&#33021;&#20998;&#26512;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#21270;DNN&#30340;&#25511;&#21046;&#22120;&#65292;&#29992;&#20110;&#37319;&#29992;&#21453;&#25512;&#25216;&#26415;&#23454;&#29616;&#25289;&#26684;&#26391;&#26085;&#31995;&#32479;&#30340;&#36712;&#36857;&#36319;&#36394;&#25511;&#21046;&#12290;&#36890;&#36807;&#36866;&#24403;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#25152;&#25552;&#20986;&#30340;&#25511;&#21046;&#22120;&#21487;&#20197;&#30830;&#20445;&#20219;&#20309;&#20860;&#23481;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#23454;&#29616;&#38381;&#29615;&#31283;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#36827;&#19968;&#27493;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25511;&#21046;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#36319;&#36394;&#35823;&#24046;&#30340;&#26126;&#30830;&#19978;&#38480;&#65292;&#36825;&#20801;&#35768;&#25105;&#20204;&#36890;&#36807;&#36866;&#24403;&#36873;&#25321;&#25511;&#21046;&#21442;&#25968;&#26469;&#23454;&#29616;&#25152;&#38656;&#30340;&#36319;&#36394;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#24403;&#31995;&#32479;&#27169;&#22411;&#26410;&#30693;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#25289;&#26684;&#26391;&#26085;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00381v1 Announce Type: cross  Abstract: Deep neural networks (DNN) are increasingly being used to learn controllers due to their excellent approximation capabilities. However, their black-box nature poses significant challenges to closed-loop stability guarantees and performance analysis. In this paper, we introduce a structured DNN-based controller for the trajectory tracking control of Lagrangian systems using backing techniques. By properly designing neural network structures, the proposed controller can ensure closed-loop stability for any compatible neural network parameters. In addition, improved control performance can be achieved by further optimizing neural network parameters. Besides, we provide explicit upper bounds on tracking errors in terms of controller parameters, which allows us to achieve the desired tracking performance by properly selecting the controller parameters. Furthermore, when system models are unknown, we propose an improved Lagrangian neural net
&lt;/p&gt;</description></item><item><title>CoFiI2P&#26159;&#19968;&#31181;&#31895;&#21040;&#31934;&#30340;&#22270;&#20687;&#21040;&#28857;&#20113;&#27880;&#20876;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20840;&#23616;&#20449;&#24687;&#21644;&#29305;&#24449;&#24314;&#31435;&#23545;&#24212;&#20851;&#31995;&#65292;&#23454;&#29616;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.14660</link><description>&lt;p&gt;
CoFiI2P: &#31895;&#21040;&#31934;&#30340;&#22270;&#20687;&#21040;&#28857;&#20113;&#27880;&#20876;&#30340;&#23545;&#24212;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
CoFiI2P: Coarse-to-Fine Correspondences for Image-to-Point Cloud Registration. (arXiv:2309.14660v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14660
&lt;/p&gt;
&lt;p&gt;
CoFiI2P&#26159;&#19968;&#31181;&#31895;&#21040;&#31934;&#30340;&#22270;&#20687;&#21040;&#28857;&#20113;&#27880;&#20876;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20840;&#23616;&#20449;&#24687;&#21644;&#29305;&#24449;&#24314;&#31435;&#23545;&#24212;&#20851;&#31995;&#65292;&#23454;&#29616;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#21040;&#28857;&#20113;&#65288;I2P&#65289;&#27880;&#20876;&#26159;&#26426;&#22120;&#20154;&#23548;&#33322;&#21644;&#31227;&#21160;&#24314;&#22270;&#39046;&#22495;&#20013;&#30340;&#19968;&#39033;&#22522;&#30784;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;I2P&#27880;&#20876;&#26041;&#27861;&#22312;&#28857;&#21040;&#20687;&#32032;&#32423;&#21035;&#19978;&#20272;&#35745;&#23545;&#24212;&#20851;&#31995;&#65292;&#24573;&#30053;&#20102;&#20840;&#23616;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#26469;&#33258;&#20840;&#23616;&#32422;&#26463;&#30340;&#39640;&#32423;&#24341;&#23548;&#30340;I2P&#21305;&#37197;&#23481;&#26131;&#25910;&#25947;&#21040;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;I2P&#27880;&#20876;&#32593;&#32476;CoFiI2P&#65292;&#36890;&#36807;&#31895;&#21040;&#31934;&#30340;&#26041;&#24335;&#25552;&#21462;&#23545;&#24212;&#20851;&#31995;&#65292;&#20197;&#24471;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#39318;&#20808;&#65292;&#23558;&#22270;&#20687;&#21644;&#28857;&#20113;&#36755;&#20837;&#21040;&#19968;&#20010;&#20849;&#20139;&#32534;&#30721;-&#35299;&#30721;&#32593;&#32476;&#20013;&#36827;&#34892;&#23618;&#27425;&#21270;&#29305;&#24449;&#25552;&#21462;&#12290;&#28982;&#21518;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#31895;&#21040;&#31934;&#30340;&#21305;&#37197;&#27169;&#22359;&#65292;&#21033;&#29992;&#29305;&#24449;&#24314;&#31435;&#31283;&#20581;&#30340;&#29305;&#24449;&#23545;&#24212;&#20851;&#31995;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#31895;&#21305;&#37197;&#22359;&#20013;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;I2P&#21464;&#25442;&#27169;&#22359;&#65292;&#20174;&#22270;&#20687;&#21644;&#28857;&#20113;&#20013;&#25429;&#25417;&#21516;&#36136;&#21644;&#24322;&#36136;&#30340;&#20840;&#23616;&#20449;&#24687;&#12290;&#36890;&#36807;&#21028;&#21035;&#25551;&#36848;&#23376;&#65292;&#23436;&#25104;&#31895;-&#32454;&#29305;&#24449;&#21305;&#37197;&#36807;&#31243;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#32454;&#21270;&#21305;&#37197;&#27169;&#22359;&#36827;&#19968;&#27493;&#25552;&#21319;&#23545;&#24212;&#20851;&#31995;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image-to-point cloud (I2P) registration is a fundamental task in the fields of robot navigation and mobile mapping. Existing I2P registration works estimate correspondences at the point-to-pixel level, neglecting the global alignment. However, I2P matching without high-level guidance from global constraints may converge to the local optimum easily. To solve the problem, this paper proposes CoFiI2P, a novel I2P registration network that extracts correspondences in a coarse-to-fine manner for the global optimal solution. First, the image and point cloud are fed into a Siamese encoder-decoder network for hierarchical feature extraction. Then, a coarse-to-fine matching module is designed to exploit features and establish resilient feature correspondences. Specifically, in the coarse matching block, a novel I2P transformer module is employed to capture the homogeneous and heterogeneous global information from image and point cloud. With the discriminate descriptors, coarse super-point-to-su
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24809;&#32602;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;P-CSG&#65292;&#32467;&#21512;&#35821;&#20041;&#29983;&#25104;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#30340;&#25972;&#20307;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#20102;&#20132;&#36890;&#35268;&#21017;&#36981;&#23432;&#21644;&#20256;&#24863;&#22120;&#24863;&#30693;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.07808</link><description>&lt;p&gt;
&#25552;&#21319;&#27169;&#20223;&#23398;&#20064;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#20132;&#36890;&#35268;&#21017;&#36981;&#23432;&#30340;&#20851;&#38190;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
What Matters to Enhance Traffic Rule Compliance of Imitation Learning for Automated Driving. (arXiv:2309.07808v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24809;&#32602;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;P-CSG&#65292;&#32467;&#21512;&#35821;&#20041;&#29983;&#25104;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#30340;&#25972;&#20307;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#20102;&#20132;&#36890;&#35268;&#21017;&#36981;&#23432;&#21644;&#20256;&#24863;&#22120;&#24863;&#30693;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#20840;&#31471;&#21040;&#31471;&#30340;&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#65292;&#22312;&#36825;&#31181;&#25216;&#26415;&#20013;&#65292;&#25972;&#20010;&#39550;&#39542;&#27969;&#31243;&#34987;&#26367;&#25442;&#20026;&#19968;&#20010;&#31616;&#21333;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#30001;&#20110;&#20854;&#32467;&#26500;&#31616;&#21333;&#21644;&#25512;&#29702;&#26102;&#38388;&#24555;&#65292;&#22240;&#27492;&#21464;&#24471;&#38750;&#24120;&#21560;&#24341;&#20154;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#22823;&#22823;&#20943;&#23569;&#20102;&#39550;&#39542;&#27969;&#31243;&#20013;&#30340;&#32452;&#20214;&#65292;&#20294;&#20854;&#31616;&#21333;&#24615;&#20063;&#23548;&#33268;&#35299;&#37322;&#24615;&#38382;&#39064;&#21644;&#23433;&#20840;&#38382;&#39064;&#12290;&#35757;&#32451;&#24471;&#21040;&#30340;&#31574;&#30053;&#24182;&#19981;&#24635;&#26159;&#31526;&#21512;&#20132;&#36890;&#35268;&#21017;&#65292;&#21516;&#26102;&#20063;&#24456;&#38590;&#21457;&#29616;&#20854;&#38169;&#35823;&#30340;&#21407;&#22240;&#65292;&#22240;&#20026;&#32570;&#20047;&#20013;&#38388;&#36755;&#20986;&#12290;&#21516;&#26102;&#65292;&#20256;&#24863;&#22120;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#34892;&#24615;&#20063;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#20197;&#24110;&#21161;&#24863;&#30693;&#22797;&#26434;&#39550;&#39542;&#22330;&#26223;&#19979;&#30340;&#21608;&#22260;&#29615;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#22522;&#20110;&#24809;&#32602;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;P-CSG&#65292;&#32467;&#21512;&#35821;&#20041;&#29983;&#25104;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
More research attention has recently been given to end-to-end autonomous driving technologies where the entire driving pipeline is replaced with a single neural network because of its simpler structure and faster inference time. Despite this appealing approach largely reducing the components in driving pipeline, its simplicity also leads to interpretability problems and safety issues arXiv:2003.06404. The trained policy is not always compliant with the traffic rules and it is also hard to discover the reason for the misbehavior because of the lack of intermediate outputs. Meanwhile, Sensors are also critical to autonomous driving's security and feasibility to perceive the surrounding environment under complex driving scenarios. In this paper, we proposed P-CSG, a novel penalty-based imitation learning approach with cross semantics generation sensor fusion technologies to increase the overall performance of End-to-End Autonomous Driving. We conducted an assessment of our model's perform
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#30784;&#25216;&#33021;&#20808;&#39564;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#22522;&#20110;&#35821;&#35328;&#26465;&#20214;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#22312;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#19979;&#65292;&#20197;&#22686;&#24378;&#31639;&#27861;&#22312;&#36866;&#24212;&#19981;&#29087;&#24713;&#30340;&#29615;&#22659;&#26041;&#38754;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#38646;-shot&#35774;&#32622;&#19979;&#65292;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#27979;&#35797;&#65292;&#25552;&#39640;&#20102;CALVIN&#22522;&#20934;&#27979;&#35797;&#30340;&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2305.19075</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#26465;&#20214;&#30340;&#27169;&#20223;&#23398;&#20064;&#19982;&#22522;&#30784;&#25216;&#33021;&#20808;&#39564;&#19979;&#30340;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Language-Conditioned Imitation Learning with Base Skill Priors under Unstructured Data. (arXiv:2305.19075v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#30784;&#25216;&#33021;&#20808;&#39564;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#22522;&#20110;&#35821;&#35328;&#26465;&#20214;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#22312;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#19979;&#65292;&#20197;&#22686;&#24378;&#31639;&#27861;&#22312;&#36866;&#24212;&#19981;&#29087;&#24713;&#30340;&#29615;&#22659;&#26041;&#38754;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#38646;-shot&#35774;&#32622;&#19979;&#65292;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#27979;&#35797;&#65292;&#25552;&#39640;&#20102;CALVIN&#22522;&#20934;&#27979;&#35797;&#30340;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#35328;&#26465;&#20214;&#19979;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#26088;&#22312;&#24320;&#21457;&#33021;&#22815;&#29702;&#35299;&#21644;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#30340;&#26426;&#22120;&#20154;&#65292;&#20197;&#23454;&#29616;&#26426;&#22120;&#20154;&#26681;&#25454;&#35821;&#35328;&#25351;&#20196;&#25805;&#20316;&#29289;&#20307;&#30340;&#30446;&#26631;&#12290;&#34429;&#28982;&#35821;&#35328;&#26465;&#20214;&#26041;&#27861;&#22312;&#29087;&#24713;&#30340;&#29615;&#22659;&#20013;&#22788;&#29702;&#20219;&#21153;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#36866;&#24212;&#19981;&#29087;&#24713;&#30340;&#29615;&#22659;&#35774;&#32622;&#26041;&#38754;&#36935;&#21040;&#20102;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#12289;&#22522;&#20110;&#35821;&#35328;&#26465;&#20214;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#22522;&#30784;&#25216;&#33021;&#20808;&#39564;&#21644;&#27169;&#20223;&#23398;&#20064;&#22312;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#19979;&#65292;&#20197;&#22686;&#24378;&#31639;&#27861;&#22312;&#36866;&#24212;&#19981;&#29087;&#24713;&#30340;&#29615;&#22659;&#26041;&#38754;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#20351;&#29992;&#38646;-shot&#35774;&#32622;&#26469;&#35780;&#20272;&#25105;&#20204;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;CALVIN&#22522;&#20934;&#27979;&#35797;&#26041;&#38754;&#36229;&#36807;&#20102;&#20197;&#21069;&#25253;&#21578;&#30340;&#24471;&#20998;&#65292;&#29305;&#21035;&#26159;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38646;-shot&#22810;&#29615;&#22659;&#35774;&#32622;&#20013;&#12290;&#23436;&#25104;&#20219;&#21153;&#30340;&#24179;&#22343;&#38271;&#24230;&#20026;...
&lt;/p&gt;
&lt;p&gt;
The growing interest in language-conditioned robot manipulation aims to develop robots capable of understanding and executing complex tasks, with the objective of enabling robots to interpret language commands and manipulate objects accordingly. While language-conditioned approaches demonstrate impressive capabilities for addressing tasks in familiar environments, they encounter limitations in adapting to unfamiliar environment settings. In this study, we propose a general-purpose, language-conditioned approach that combines base skill priors and imitation learning under unstructured data to enhance the algorithm's generalization in adapting to unfamiliar environments. We assess our model's performance in both simulated and real-world environments using a zero-shot setting. In the simulated environment, the proposed approach surpasses previously reported scores for CALVIN benchmark, especially in the challenging Zero-Shot Multi-Environment setting. The average completed task length, in
&lt;/p&gt;</description></item></channel></rss>