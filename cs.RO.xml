<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20808;&#22825;&#20215;&#20540;&#39537;&#21160;&#22686;&#24378;&#23398;&#20064;&#65288;IVRL&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#22810;&#26234;&#33021;&#20307;&#22312;&#21512;&#20316;&#20013;&#30340;&#22797;&#26434;&#34892;&#20026;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24314;&#31435;&#26234;&#33021;&#20307;&#23545;&#32676;&#20307;&#25928;&#29992;&#21644;&#31995;&#32479;&#25104;&#26412;&#30340;&#35748;&#30693;&#65292;&#28385;&#36275;&#20854;&#21512;&#20316;&#20249;&#20276;&#30340;&#38656;&#27714;&#65292;&#25903;&#25345;&#20854;&#31038;&#21306;&#24182;&#34701;&#20837;&#20154;&#31867;&#31038;&#20250;&#12290;</title><link>http://arxiv.org/abs/2401.05572</link><description>&lt;p&gt;
&#29992;&#20110;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#20808;&#22825;&#20215;&#20540;&#39537;&#21160;&#22686;&#24378;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Innate-Values-driven Reinforcement Learning for Cooperative Multi-Agent Systems. (arXiv:2401.05572v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20808;&#22825;&#20215;&#20540;&#39537;&#21160;&#22686;&#24378;&#23398;&#20064;&#65288;IVRL&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#22810;&#26234;&#33021;&#20307;&#22312;&#21512;&#20316;&#20013;&#30340;&#22797;&#26434;&#34892;&#20026;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24314;&#31435;&#26234;&#33021;&#20307;&#23545;&#32676;&#20307;&#25928;&#29992;&#21644;&#31995;&#32479;&#25104;&#26412;&#30340;&#35748;&#30693;&#65292;&#28385;&#36275;&#20854;&#21512;&#20316;&#20249;&#20276;&#30340;&#38656;&#27714;&#65292;&#25903;&#25345;&#20854;&#31038;&#21306;&#24182;&#34701;&#20837;&#20154;&#31867;&#31038;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#22825;&#20215;&#20540;&#25551;&#36848;&#20102;&#26234;&#33021;&#20307;&#30340;&#20869;&#22312;&#21160;&#26426;&#65292;&#21453;&#26144;&#20102;&#20182;&#20204;&#36861;&#27714;&#30446;&#26631;&#21644;&#21457;&#23637;&#22810;&#26679;&#25216;&#33021;&#20197;&#28385;&#36275;&#21508;&#31181;&#38656;&#27714;&#30340;&#22266;&#26377;&#20852;&#36259;&#21644;&#20559;&#22909;&#12290;&#24378;&#21270;&#23398;&#20064;&#30340;&#26412;&#36136;&#26159;&#22522;&#20110;&#22870;&#21169;&#39537;&#21160;&#65288;&#22914;&#25928;&#29992;&#65289;&#30340;&#34892;&#20026;&#20114;&#21160;&#23398;&#20064;&#65292;&#31867;&#20284;&#20110;&#33258;&#28982;&#26234;&#33021;&#20307;&#12290;&#29305;&#21035;&#26159;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#65292;&#24314;&#31435;&#26234;&#33021;&#20307;&#23545;&#24179;&#34913;&#32676;&#20307;&#25928;&#29992;&#21644;&#31995;&#32479;&#25104;&#26412;&#30340;&#35748;&#30693;&#65292;&#28385;&#36275;&#32676;&#20307;&#25104;&#21592;&#22312;&#21512;&#20316;&#20013;&#30340;&#38656;&#27714;&#65292;&#26159;&#20010;&#20307;&#20026;&#25903;&#25345;&#20854;&#31038;&#21306;&#21644;&#34701;&#20837;&#20154;&#31867;&#31038;&#20250;&#32780;&#23398;&#20064;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#22797;&#21512;&#20869;&#22312;&#20215;&#20540;&#22686;&#24378;&#23398;&#20064;&#27169;&#22411; - &#20808;&#22825;&#20215;&#20540;&#39537;&#21160;&#22686;&#24378;&#23398;&#20064;&#65292;&#29992;&#20110;&#25551;&#36848;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#20013;&#22797;&#26434;&#30340;&#20114;&#21160;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Innate values describe agents' intrinsic motivations, which reflect their inherent interests and preferences to pursue goals and drive them to develop diverse skills satisfying their various needs. The essence of reinforcement learning (RL) is learning from interaction based on reward-driven (such as utilities) behaviors, much like natural agents. It is an excellent model to describe the innate-values-driven (IV) behaviors of AI agents. Especially in multi-agent systems (MAS), building the awareness of AI agents to balance the group utilities and system costs and satisfy group members' needs in their cooperation is a crucial problem for individuals learning to support their community and integrate human society in the long term. This paper proposes a hierarchical compound intrinsic value reinforcement learning model -innate-values-driven reinforcement learning termed IVRL to describe the complex behaviors of multi-agent interaction in their cooperation. We implement the IVRL architec
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#23884;&#20837;&#24335;GPU&#23454;&#29616;&#30340;&#36731;&#37327;&#32423;&#31435;&#20307;&#21305;&#37197;&#31995;&#32479;-StereoVAE&#65292;&#35813;&#31995;&#32479;&#37319;&#29992;&#22522;&#20110;VAE&#30340;&#23567;&#22411;&#31070;&#32463;&#32593;&#32476;&#23545;&#20256;&#32479;&#21305;&#37197;&#26041;&#27861;&#29983;&#25104;&#30340;&#23567;&#23610;&#23544;&#31895;&#31961;&#35270;&#24046;&#22270;&#36827;&#34892;&#19978;&#37319;&#26679;&#19982;&#32454;&#21270;&#65292;&#36798;&#21040;&#20102;&#25552;&#39640;&#21305;&#37197;&#31934;&#24230;&#21644;&#20445;&#35777;&#23454;&#26102;&#22788;&#29702;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.11566</link><description>&lt;p&gt;
&#36890;&#36807;&#23884;&#20837;&#24335;GPU&#23454;&#29616;&#30340;&#36731;&#37327;&#32423;&#31435;&#20307;&#21305;&#37197;&#31995;&#32479;-StereoVAE
&lt;/p&gt;
&lt;p&gt;
StereoVAE: A lightweight stereo matching system through embedded GPUs. (arXiv:2305.11566v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#23884;&#20837;&#24335;GPU&#23454;&#29616;&#30340;&#36731;&#37327;&#32423;&#31435;&#20307;&#21305;&#37197;&#31995;&#32479;-StereoVAE&#65292;&#35813;&#31995;&#32479;&#37319;&#29992;&#22522;&#20110;VAE&#30340;&#23567;&#22411;&#31070;&#32463;&#32593;&#32476;&#23545;&#20256;&#32479;&#21305;&#37197;&#26041;&#27861;&#29983;&#25104;&#30340;&#23567;&#23610;&#23544;&#31895;&#31961;&#35270;&#24046;&#22270;&#36827;&#34892;&#19978;&#37319;&#26679;&#19982;&#32454;&#21270;&#65292;&#36798;&#21040;&#20102;&#25552;&#39640;&#21305;&#37197;&#31934;&#24230;&#21644;&#20445;&#35777;&#23454;&#26102;&#22788;&#29702;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#23884;&#20837;&#24335;GPU&#23454;&#29616;&#30340;&#36731;&#37327;&#32423;&#31435;&#20307;&#21305;&#37197;&#31995;&#32479;-StereoVAE&#65292;&#23427;&#25171;&#30772;&#20102;&#31435;&#20307;&#21305;&#37197;&#20013;&#31934;&#24230;&#21644;&#22788;&#29702;&#36895;&#24230;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#20351;&#24471;&#25105;&#20204;&#30340;&#23884;&#20837;&#24335;&#31995;&#32479;&#33021;&#22815;&#22312;&#20445;&#35777;&#23454;&#26102;&#22788;&#29702;&#30340;&#21516;&#26102;&#36827;&#19968;&#27493;&#25552;&#39640;&#21305;&#37197;&#31934;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#26500;&#24314;&#19968;&#20010;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#30340;&#23567;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#23545;&#20256;&#32479;&#21305;&#37197;&#26041;&#27861;&#29983;&#25104;&#30340;&#23567;&#23610;&#23544;&#31895;&#31961;&#35270;&#24046;&#22270;&#36827;&#34892;&#19978;&#37319;&#26679;&#19982;&#32454;&#21270;&#12290;&#36825;&#31181;&#28151;&#21512;&#32467;&#26500;&#19981;&#20165;&#21487;&#20197;&#24102;&#26469;&#20256;&#32479;&#26041;&#27861;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#20248;&#21183;&#65292;&#36824;&#21487;&#20197;&#20445;&#35777;&#31070;&#32463;&#32593;&#32476;&#30340;&#24433;&#21709;&#19979;&#30340;&#21305;&#37197;&#31934;&#24230;&#12290;&#23545;KITTI 2015&#22522;&#20934;&#27979;&#35797;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#36731;&#37327;&#32423;&#31435;&#20307;&#21305;&#37197;&#31995;&#32479;&#22312;&#25552;&#39640;&#30001;&#19981;&#21516;&#31639;&#27861;&#29983;&#25104;&#30340;&#31895;&#31961;&#35270;&#24046;&#22270;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#39640;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#22312;&#23884;&#20837;&#24335;GPU&#19978;&#23454;&#26102;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a lightweight system for stereo matching through embedded GPUs. It breaks the trade-off between accuracy and processing speed in stereo matching, enabling our embedded system to further improve the matching accuracy while ensuring real-time processing. The main idea of our method is to construct a tiny neural network based on variational auto-encoder (VAE) to upsample and refinement a small size of coarse disparity map, which is first generated by a traditional matching method. The proposed hybrid structure cannot only bring the advantage of traditional methods in terms of computational complexity, but also ensure the matching accuracy under the impact of neural network. Extensive experiments on the KITTI 2015 benchmark demonstrate that our tiny system exhibits high robustness in improving the accuracy of the coarse disparity maps generated by different algorithms, while also running in real-time on embedded GPUs.
&lt;/p&gt;</description></item></channel></rss>