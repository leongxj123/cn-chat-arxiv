<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#28508;&#22312;&#29366;&#24577;&#25512;&#26029;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#21160;&#21277;&#36947;&#21512;&#24182;&#38382;&#39064;&#65292;&#22312;&#27809;&#26377;&#35814;&#32454;&#20102;&#35299;&#21608;&#22260;&#36710;&#36742;&#24847;&#22270;&#25110;&#39550;&#39542;&#39118;&#26684;&#30340;&#24773;&#20917;&#19979;&#23433;&#20840;&#25191;&#34892;&#21277;&#36947;&#21512;&#24182;&#20219;&#21153;&#65292;&#24182;&#32771;&#34385;&#20102;&#35266;&#27979;&#24310;&#36831;&#65292;&#20197;&#22686;&#24378;&#20195;&#29702;&#22312;&#21160;&#24577;&#20132;&#36890;&#29366;&#20917;&#20013;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.11852</link><description>&lt;p&gt;
&#20855;&#26377;&#28508;&#22312;&#29366;&#24577;&#25512;&#26029;&#30340;&#24378;&#21270;&#23398;&#20064;&#22312;&#33258;&#21160;&#21277;&#36947;&#21512;&#24182;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Latent State Inference for Autonomous On-ramp Merging under Observation Delay
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#28508;&#22312;&#29366;&#24577;&#25512;&#26029;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#21160;&#21277;&#36947;&#21512;&#24182;&#38382;&#39064;&#65292;&#22312;&#27809;&#26377;&#35814;&#32454;&#20102;&#35299;&#21608;&#22260;&#36710;&#36742;&#24847;&#22270;&#25110;&#39550;&#39542;&#39118;&#26684;&#30340;&#24773;&#20917;&#19979;&#23433;&#20840;&#25191;&#34892;&#21277;&#36947;&#21512;&#24182;&#20219;&#21153;&#65292;&#24182;&#32771;&#34385;&#20102;&#35266;&#27979;&#24310;&#36831;&#65292;&#20197;&#22686;&#24378;&#20195;&#29702;&#22312;&#21160;&#24577;&#20132;&#36890;&#29366;&#20917;&#20013;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#33258;&#21160;&#21277;&#36947;&#21512;&#24182;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#20854;&#20013;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#38656;&#35201;&#26080;&#32541;&#22320;&#34701;&#20837;&#22810;&#36710;&#36947;&#39640;&#36895;&#20844;&#36335;&#19978;&#30340;&#36710;&#27969;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Lane-keeping, Lane-changing with Latent-state Inference and Safety Controller (L3IS)&#20195;&#29702;&#65292;&#26088;&#22312;&#22312;&#27809;&#26377;&#20851;&#20110;&#21608;&#22260;&#36710;&#36742;&#24847;&#22270;&#25110;&#39550;&#39542;&#39118;&#26684;&#30340;&#20840;&#38754;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#23433;&#20840;&#25191;&#34892;&#21277;&#36947;&#21512;&#24182;&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#35813;&#20195;&#29702;&#30340;&#22686;&#24378;&#29256;AL3IS&#65292;&#32771;&#34385;&#20102;&#35266;&#27979;&#24310;&#36831;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#22312;&#20855;&#26377;&#36710;&#36742;&#38388;&#36890;&#20449;&#24310;&#36831;&#30340;&#29616;&#23454;&#29615;&#22659;&#20013;&#20570;&#20986;&#26356;&#31283;&#20581;&#30340;&#20915;&#31574;&#12290;&#36890;&#36807;&#36890;&#36807;&#28508;&#22312;&#29366;&#24577;&#24314;&#27169;&#29615;&#22659;&#20013;&#30340;&#19981;&#21487;&#35266;&#23519;&#26041;&#38754;&#65292;&#22914;&#20854;&#20182;&#39550;&#39542;&#21592;&#30340;&#24847;&#22270;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#20195;&#29702;&#36866;&#24212;&#21160;&#24577;&#20132;&#36890;&#29366;&#20917;&#12289;&#20248;&#21270;&#21512;&#24182;&#25805;&#20316;&#24182;&#30830;&#20445;&#19982;&#20854;&#20182;&#36710;&#36742;&#36827;&#34892;&#23433;&#20840;&#20114;&#21160;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11852v1 Announce Type: cross  Abstract: This paper presents a novel approach to address the challenging problem of autonomous on-ramp merging, where a self-driving vehicle needs to seamlessly integrate into a flow of vehicles on a multi-lane highway. We introduce the Lane-keeping, Lane-changing with Latent-state Inference and Safety Controller (L3IS) agent, designed to perform the on-ramp merging task safely without comprehensive knowledge about surrounding vehicles' intents or driving styles. We also present an augmentation of this agent called AL3IS that accounts for observation delays, allowing the agent to make more robust decisions in real-world environments with vehicle-to-vehicle (V2V) communication delays. By modeling the unobservable aspects of the environment through latent states, such as other drivers' intents, our approach enhances the agent's ability to adapt to dynamic traffic conditions, optimize merging maneuvers, and ensure safe interactions with other vehi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#20027;&#21160;&#25512;&#29702;&#35745;&#31639;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#32452;&#21512;&#31163;&#25955;&#21644;&#36830;&#32493;&#27169;&#22411;&#20197;&#23454;&#29616;&#28789;&#27963;&#24037;&#20855;&#20351;&#29992;&#65292;&#25511;&#21046;&#21644;&#35268;&#21010;&#12290;&#22312;&#38750;&#24179;&#20961;&#20219;&#21153;&#20013;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10088</link><description>&lt;p&gt;
&#20998;&#23618;&#28151;&#21512;&#24314;&#27169;&#29992;&#20110;&#28789;&#27963;&#24037;&#20855;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hierarchical hybrid modeling for flexible tool use
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#20027;&#21160;&#25512;&#29702;&#35745;&#31639;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#32452;&#21512;&#31163;&#25955;&#21644;&#36830;&#32493;&#27169;&#22411;&#20197;&#23454;&#29616;&#28789;&#27963;&#24037;&#20855;&#20351;&#29992;&#65292;&#25511;&#21046;&#21644;&#35268;&#21010;&#12290;&#22312;&#38750;&#24179;&#20961;&#20219;&#21153;&#20013;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#25552;&#20986;&#30340;&#20027;&#21160;&#25512;&#29702;&#35745;&#31639;&#26694;&#26550;&#20013;&#65292;&#31163;&#25955;&#27169;&#22411;&#21487;&#20197;&#19982;&#36830;&#32493;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#20915;&#31574;&#12290;&#20174;&#21478;&#19968;&#20010;&#35282;&#24230;&#26469;&#30475;&#65292;&#31616;&#21333;&#30340;&#20195;&#29702;&#21487;&#20197;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#19990;&#30028;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#22914;&#20309;&#23558;&#36825;&#20004;&#20010;&#29305;&#28857;&#32467;&#21512;&#36215;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26550;&#26500;&#65292;&#30001;&#22810;&#20010;&#28151;&#21512; - &#36830;&#32493;&#21644;&#31163;&#25955; - &#21333;&#20803;&#32452;&#25104;&#65292;&#22797;&#21046;&#20195;&#29702;&#30340;&#37197;&#32622;&#65292;&#30001;&#39640;&#32423;&#31163;&#25955;&#27169;&#22411;&#25511;&#21046;&#65292;&#23454;&#29616;&#21160;&#24577;&#35268;&#21010;&#21644;&#21516;&#27493;&#34892;&#20026;&#12290;&#27599;&#20010;&#23618;&#27425;&#20869;&#37096;&#30340;&#36827;&#19968;&#27493;&#20998;&#35299;&#21487;&#20197;&#20197;&#20998;&#23618;&#26041;&#24335;&#34920;&#31034;&#19982;self&#30456;&#20851;&#30340;&#20854;&#20182;&#20195;&#29702;&#21644;&#23545;&#35937;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#38750;&#24179;&#20961;&#30340;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#36825;&#31181;&#20998;&#23618;&#28151;&#21512;&#27169;&#22411;&#65306;&#22312;&#25342;&#21462;&#19968;&#20010;&#31227;&#21160;&#24037;&#20855;&#21518;&#21040;&#36798;&#19968;&#20010;&#31227;&#21160;&#29289;&#20307;&#12290;&#36825;&#39033;&#30740;&#31350;&#25193;&#23637;&#20102;&#20197;&#25512;&#29702;&#20026;&#25511;&#21046;&#30340;&#20808;&#21069;&#24037;&#20316;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10088v1 Announce Type: cross  Abstract: In a recent computational framework called active inference, discrete models can be linked to their continuous counterparts to perform decision-making in changing environments. From another perspective, simple agents can be combined to better capture the causal relationships of the world. How can we use these two features together to achieve efficient goal-directed behavior? We present an architecture composed of several hybrid -- continuous and discrete -- units replicating the agent's configuration, controlled by a high-level discrete model that achieves dynamic planning and synchronized behavior. Additional factorizations within each level allow to represent hierarchically other agents and objects in relation to the self. We evaluate this hierarchical hybrid model on a non-trivial task: reaching a moving object after having picked a moving tool. This study extends past work on control as inference and proposes an alternative directi
&lt;/p&gt;</description></item><item><title>DiffTOP&#20351;&#29992;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#20316;&#20026;&#31574;&#30053;&#34920;&#31034;&#26469;&#29983;&#25104;&#21160;&#20316;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#8220;&#30446;&#26631;&#19981;&#21305;&#37197;&#8221;&#38382;&#39064;&#65292;&#24182;&#22312;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>https://arxiv.org/abs/2402.05421</link><description>&lt;p&gt;
DiffTOP: &#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#22312;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
DiffTOP: Differentiable Trajectory Optimization for Deep Reinforcement and Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05421
&lt;/p&gt;
&lt;p&gt;
DiffTOP&#20351;&#29992;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#20316;&#20026;&#31574;&#30053;&#34920;&#31034;&#26469;&#29983;&#25104;&#21160;&#20316;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#8220;&#30446;&#26631;&#19981;&#21305;&#37197;&#8221;&#38382;&#39064;&#65292;&#24182;&#22312;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DiffTOP&#65292;&#23427;&#21033;&#29992;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#20316;&#20026;&#31574;&#30053;&#34920;&#31034;&#65292;&#20026;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#29983;&#25104;&#21160;&#20316;&#12290;&#36712;&#36857;&#20248;&#21270;&#26159;&#19968;&#31181;&#22312;&#25511;&#21046;&#39046;&#22495;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#31639;&#27861;&#65292;&#30001;&#25104;&#26412;&#21644;&#21160;&#21147;&#23398;&#20989;&#25968;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#21033;&#29992;&#20102;&#26368;&#36817;&#22312;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#20351;&#24471;&#21487;&#20197;&#35745;&#31639;&#25439;&#22833;&#23545;&#20110;&#36712;&#36857;&#20248;&#21270;&#30340;&#21442;&#25968;&#30340;&#26799;&#24230;&#12290;&#22240;&#27492;&#65292;&#36712;&#36857;&#20248;&#21270;&#30340;&#25104;&#26412;&#21644;&#21160;&#21147;&#23398;&#20989;&#25968;&#21487;&#20197;&#31471;&#21040;&#31471;&#22320;&#23398;&#20064;&#12290;DiffTOP&#35299;&#20915;&#20102;&#20043;&#21069;&#27169;&#22411;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#8220;&#30446;&#26631;&#19981;&#21305;&#37197;&#8221;&#38382;&#39064;&#65292;&#22240;&#20026;DiffTOP&#20013;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#36890;&#36807;&#36712;&#36857;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#31574;&#30053;&#26799;&#24230;&#25439;&#22833;&#30452;&#25509;&#26368;&#22823;&#21270;&#20219;&#21153;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#23545;DiffTOP&#22312;&#26631;&#20934;&#26426;&#22120;&#20154;&#25805;&#32437;&#20219;&#21153;&#22871;&#20214;&#20013;&#36827;&#34892;&#20102;&#27169;&#20223;&#23398;&#20064;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces DiffTOP, which utilizes Differentiable Trajectory OPtimization as the policy representation to generate actions for deep reinforcement and imitation learning. Trajectory optimization is a powerful and widely used algorithm in control, parameterized by a cost and a dynamics function. The key to our approach is to leverage the recent progress in differentiable trajectory optimization, which enables computing the gradients of the loss with respect to the parameters of trajectory optimization. As a result, the cost and dynamics functions of trajectory optimization can be learned end-to-end. DiffTOP addresses the ``objective mismatch'' issue of prior model-based RL algorithms, as the dynamics model in DiffTOP is learned to directly maximize task performance by differentiating the policy gradient loss through the trajectory optimization process. We further benchmark DiffTOP for imitation learning on standard robotic manipulation task suites with high-dimensional sensory
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31163;&#32447;&#25216;&#33021;&#21457;&#29616;&#31639;&#27861;&#65292;&#36890;&#36807;Fenchel&#23545;&#20598;&#26041;&#27861;&#23558;&#24378;&#21270;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#23398;&#20064;&#19982;&#19987;&#23478;&#30456;&#19968;&#33268;&#30340;&#22810;&#26679;&#30340;&#25216;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11373</link><description>&lt;p&gt;
&#36890;&#36807;Fenchel&#23545;&#20598;&#23454;&#29616;&#22810;&#26679;&#30340;&#31163;&#32447;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
Diverse Offline Imitation via Fenchel Duality. (arXiv:2307.11373v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31163;&#32447;&#25216;&#33021;&#21457;&#29616;&#31639;&#27861;&#65292;&#36890;&#36807;Fenchel&#23545;&#20598;&#26041;&#27861;&#23558;&#24378;&#21270;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#23398;&#20064;&#19982;&#19987;&#23478;&#30456;&#19968;&#33268;&#30340;&#22810;&#26679;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#39046;&#22495;&#65292;&#26368;&#36817;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#21508;&#31181;&#24037;&#20316;&#25552;&#20986;&#20102;&#20197;&#20114;&#20449;&#24687;&#20026;&#22522;&#30784;&#30340;&#30446;&#26631;&#65292;&#20316;&#20026;&#20869;&#22312;&#39537;&#21160;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#35774;&#35745;&#38656;&#35201;&#22312;&#32447;&#29615;&#22659;&#35775;&#38382;&#30340;&#31639;&#27861;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;\textit{&#31163;&#32447;}&#25216;&#33021;&#21457;&#29616;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#38382;&#39064;&#24418;&#24335;&#21270;&#32771;&#34385;&#20102;&#22312;KL-&#25955;&#24230;&#32422;&#26463;&#19979;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#30446;&#26631;&#12290;&#26356;&#30830;&#20999;&#22320;&#35828;&#65292;&#32422;&#26463;&#30830;&#20445;&#27599;&#20010;&#25216;&#33021;&#30340;&#29366;&#24577;&#21344;&#29992;&#20445;&#25345;&#22312;&#19968;&#20010;&#20855;&#26377;&#33391;&#22909;&#29366;&#24577;&#25805;&#20316;&#35206;&#30422;&#29575;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#25903;&#25345;&#33539;&#22260;&#20869;&#19982;&#19987;&#23478;&#30340;&#29366;&#24577;&#21344;&#29992;&#36924;&#36817;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#36830;&#25509;Fenchel&#23545;&#20598;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#65292;&#24182;&#32473;&#20986;&#19968;&#20010;&#31616;&#21333;&#30340;&#31163;&#32447;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#19982;&#19987;&#23478;&#30456;&#19968;&#33268;&#30340;&#22810;&#26679;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been significant recent progress in the area of unsupervised skill discovery, with various works proposing mutual information based objectives, as a source of intrinsic motivation. Prior works predominantly focused on designing algorithms that require online access to the environment. In contrast, we develop an \textit{offline} skill discovery algorithm. Our problem formulation considers the maximization of a mutual information objective constrained by a KL-divergence. More precisely, the constraints ensure that the state occupancy of each skill remains close to the state occupancy of an expert, within the support of an offline dataset with good state-action coverage. Our main contribution is to connect Fenchel duality, reinforcement learning and unsupervised skill discovery, and to give a simple offline algorithm for learning diverse skills that are aligned with an expert.
&lt;/p&gt;</description></item><item><title>RoMo-HER&#26159;&#19968;&#20010;&#40065;&#26834;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#20107;&#21518;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#20154;&#25805;&#20316;&#29615;&#22659;&#20013;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#21644;&#21069;&#30651;&#37325;&#26032;&#26631;&#35760;&#25216;&#26415;&#65292;&#25552;&#39640;&#20102;&#26679;&#26412;&#21033;&#29992;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.16061</link><description>&lt;p&gt;
RoMo-HER: &#40065;&#26834;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#20107;&#21518;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RoMo-HER: Robust Model-based Hindsight Experience Replay. (arXiv:2306.16061v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16061
&lt;/p&gt;
&lt;p&gt;
RoMo-HER&#26159;&#19968;&#20010;&#40065;&#26834;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#20107;&#21518;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#20154;&#25805;&#20316;&#29615;&#22659;&#20013;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#21644;&#21069;&#30651;&#37325;&#26032;&#26631;&#35760;&#25216;&#26415;&#65292;&#25552;&#39640;&#20102;&#26679;&#26412;&#21033;&#29992;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#31232;&#30095;&#22870;&#21169;&#26159;&#23548;&#33268;&#26679;&#26412;&#21033;&#29992;&#25928;&#29575;&#20302;&#30340;&#22240;&#32032;&#20043;&#19968;&#12290;&#22522;&#20110;&#20107;&#21518;&#32463;&#39564;&#22238;&#25918;&#65288;HER&#65289;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#37325;&#26032;&#26631;&#35760;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#33719;&#21462;&#34394;&#25311;&#36712;&#36857;&#26469;&#37325;&#26032;&#26631;&#35760;&#30446;&#26631;&#65292;&#22312;&#20934;&#30830;&#21487;&#24314;&#27169;&#30340;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#33021;&#22815;&#26377;&#25928;&#22686;&#24378;&#26679;&#26412;&#21033;&#29992;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#29615;&#22659;&#20013;&#65292;&#23427;&#20204;&#26159;&#26080;&#25928;&#30340;&#12290;&#22312;&#25105;&#20204;&#30340;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31216;&#20026;RoMo-HER&#30340;&#40065;&#26834;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#26426;&#22120;&#20154;&#25805;&#20316;&#29615;&#22659;&#20013;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#26469;&#25552;&#39640;&#26679;&#26412;&#21033;&#29992;&#25928;&#29575;&#12290;RoMo-HER&#22522;&#20110;&#21160;&#21147;&#23398;&#27169;&#22411;&#21644;&#19968;&#31181;&#31216;&#20026;&#21069;&#30651;&#37325;&#26032;&#26631;&#35760;&#65288;FR&#65289;&#30340;&#26032;&#22411;&#30446;&#26631;&#37325;&#26032;&#26631;&#35760;&#25216;&#26415;&#26500;&#24314;&#65292;&#35813;&#25216;&#26415;&#36890;&#36807;&#29305;&#23450;&#31574;&#30053;&#36873;&#25321;&#39044;&#27979;&#36215;&#22987;&#29366;&#24577;&#65292;&#39044;&#27979;&#36215;&#22987;&#29366;&#24577;&#30340;&#26410;&#26469;&#36712;&#36857;&#65292;&#28982;&#21518;&#20351;&#29992;&#21160;&#21147;&#23398;&#27169;&#22411;&#21644;&#26368;&#26032;&#30340;&#20449;&#24687;&#37325;&#26032;&#26631;&#35760;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse rewards are one of the factors leading to low sample efficiency in multi-goal reinforcement learning (RL). Based on Hindsight Experience Replay (HER), model-based relabeling methods have been proposed to relabel goals using virtual trajectories obtained by interacting with the trained model, which can effectively enhance the sample efficiency in accurately modelable sparse-reward environments. However, they are ineffective in robot manipulation environment. In our paper, we design a robust framework called Robust Model-based Hindsight Experience Replay (RoMo-HER) which can effectively utilize the dynamical model in robot manipulation environments to enhance the sample efficiency. RoMo-HER is built upon a dynamics model and a novel goal relabeling technique called Foresight relabeling (FR), which selects the prediction starting state with a specific strategy, predicts the future trajectory of the starting state, and then relabels the goal using the dynamics model and the latest p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25299;&#25169;&#35821;&#20041;&#26144;&#23556;&#30340;&#20998;&#23618;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#35821;&#38899;&#25351;&#20196;&#21644;waypoint&#28789;&#27963;&#22320;&#35268;&#21010;&#36335;&#24452;&#65292;&#21253;&#25324;&#22320;&#28857;&#36830;&#36890;&#24615;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#24555;&#36895;&#36817;&#20284;&#25512;&#29702;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2203.10820</link><description>&lt;p&gt;
&#21033;&#29992;&#22522;&#20110;&#31354;&#38388;&#27010;&#24565;&#30340;&#25299;&#25169;&#35821;&#20041;&#26144;&#23556;&#36827;&#34892;&#20174;&#35821;&#38899;&#25351;&#20196;&#30340;&#20998;&#23618;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Path-planning from Speech Instructions with Spatial Concept-based Topometric Semantic Mapping. (arXiv:2203.10820v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.10820
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25299;&#25169;&#35821;&#20041;&#26144;&#23556;&#30340;&#20998;&#23618;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#35821;&#38899;&#25351;&#20196;&#21644;waypoint&#28789;&#27963;&#22320;&#35268;&#21010;&#36335;&#24452;&#65292;&#21253;&#25324;&#22320;&#28857;&#36830;&#36890;&#24615;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#24555;&#36895;&#36817;&#20284;&#25512;&#29702;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#20351;&#29992;&#20154;&#31867;&#35821;&#38899;&#25351;&#20196;&#36827;&#34892;&#26426;&#22120;&#20154;&#23548;&#33322;&#33267;&#30446;&#30340;&#22320;&#26159;&#33258;&#20027;&#31227;&#21160;&#26426;&#22120;&#20154;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#37319;&#29992;&#19981;&#21516;&#30340;&#36335;&#24452;&#21040;&#36798;&#21516;&#19968;&#30446;&#26631;&#65292;&#32780;&#26368;&#30701;&#36335;&#24452;&#19981;&#19968;&#23450;&#26159;&#26368;&#20248;&#30340;&#12290;&#22240;&#27492;&#38656;&#35201;&#19968;&#31181;&#26041;&#27861;&#26469;&#28789;&#27963;&#22320;&#25509;&#21463;waypoint&#30340;&#25351;&#26631;&#65292;&#35268;&#21010;&#26356;&#22909;&#30340;&#26367;&#20195;&#36335;&#24452;&#65292;&#21363;&#20351;&#38656;&#35201;&#32469;&#36335;&#12290;&#27492;&#22806;&#65292;&#26426;&#22120;&#20154;&#38656;&#35201;&#23454;&#26102;&#25512;&#29702;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25299;&#25169;&#35821;&#20041;&#26144;&#23556;&#23454;&#29616;&#19968;&#20010;&#20998;&#23618;&#30340;&#31354;&#38388;&#34920;&#31034;&#65292;&#24182;&#32467;&#21512;&#35821;&#38899;&#25351;&#20196;&#21644;waypoint&#36827;&#34892;&#36335;&#24452;&#35268;&#21010;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SpCoTMHP&#65292;&#19968;&#31181;&#21033;&#29992;&#22810;&#27169;&#24335;&#31354;&#38388;&#27010;&#24565;&#30340;&#23618;&#27425;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#21253;&#25324;&#22320;&#28857;&#36830;&#36890;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#27010;&#29575;&#29983;&#25104;&#27169;&#22411;&#21644;&#24555;&#36895;&#36817;&#20284;&#25512;&#29702;&#26041;&#27861;&#65292;&#23618;&#27425;&#32467;&#26500;&#20013;&#30340;&#21508;&#20010;&#32423;&#21035;&#20043;&#38388;&#21487;&#20197;&#30456;&#20114;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
Navigating to destinations using human speech instructions is essential for autonomous mobile robots operating in the real world. Although robots can take different paths toward the same goal, the shortest path is not always optimal. A desired approach is to flexibly accommodate waypoint specifications, planning a better alternative path, even with detours. Furthermore, robots require real-time inference capabilities. Spatial representations include semantic, topological, and metric levels, each capturing different aspects of the environment. This study aims to realize a hierarchical spatial representation by a topometric semantic map and path planning with speech instructions, including waypoints. We propose SpCoTMHP, a hierarchical path-planning method that utilizes multimodal spatial concepts, incorporating place connectivity. This approach provides a novel integrated probabilistic generative model and fast approximate inference, with interaction among the hierarchy levels. A formul
&lt;/p&gt;</description></item></channel></rss>