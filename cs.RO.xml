<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#19987;&#38376;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#30340;&#31561;&#21464;&#31574;&#30053;&#21644;&#19981;&#21464;&#20540;&#20989;&#25968;&#26500;&#24314;&#26041;&#27861;&#65292;&#22312;&#22522;&#20110;&#22320;&#22270;&#30340;&#36335;&#24452;&#35268;&#21010;&#20013;&#23637;&#31034;&#20102;&#31561;&#21464;&#38598;&#21512;&#21644;&#27491;&#21017;&#21270;&#22914;&#20309;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.12856</link><description>&lt;p&gt;
&#22522;&#20110;&#22320;&#22270;&#30340;&#36335;&#24452;&#35268;&#21010;&#20013;&#30340;&#31561;&#21464;&#38598;&#21512;&#21644;&#27491;&#21017;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Equivariant Ensembles and Regularization for Reinforcement Learning in Map-based Path Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#19987;&#38376;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#30340;&#31561;&#21464;&#31574;&#30053;&#21644;&#19981;&#21464;&#20540;&#20989;&#25968;&#26500;&#24314;&#26041;&#27861;&#65292;&#22312;&#22522;&#20110;&#22320;&#22270;&#30340;&#36335;&#24452;&#35268;&#21010;&#20013;&#23637;&#31034;&#20102;&#31561;&#21464;&#38598;&#21512;&#21644;&#27491;&#21017;&#21270;&#22914;&#20309;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#65292;&#21033;&#29992;&#29615;&#22659;&#30340;&#23545;&#31216;&#24615;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30830;&#20445;&#28145;&#24230;RL&#31574;&#30053;&#21644;&#20540;&#32593;&#32476;&#20998;&#21035;&#26159;&#31561;&#21464;&#21644;&#19981;&#21464;&#30340;&#20197;&#21033;&#29992;&#36825;&#20123;&#23545;&#31216;&#24615;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#30456;&#20851;&#24037;&#20316;&#23581;&#35797;&#36890;&#36807;&#26500;&#36896;&#20855;&#26377;&#31561;&#21464;&#24615;&#21644;&#19981;&#21464;&#24615;&#30340;&#32593;&#32476;&#26469;&#35774;&#35745;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#21482;&#33021;&#20351;&#29992;&#38750;&#24120;&#21463;&#38480;&#30340;&#32452;&#20214;&#24211;&#65292;&#36827;&#32780;&#38459;&#30861;&#20102;&#32593;&#32476;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#24314;&#31561;&#21464;&#31574;&#30053;&#21644;&#19981;&#21464;&#20540;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#32780;&#26080;&#38656;&#19987;&#38376;&#30340;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#31561;&#21464;&#38598;&#21512;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#28155;&#21152;&#20102;&#19968;&#20010;&#27491;&#21017;&#21270;&#39033;&#65292;&#29992;&#20110;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22686;&#21152;&#24402;&#32435;&#20559;&#24046;&#12290;&#22312;&#22522;&#20110;&#22320;&#22270;&#30340;&#36335;&#24452;&#35268;&#21010;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31561;&#21464;&#38598;&#21512;&#21644;&#27491;&#21017;&#21270;&#22914;&#20309;&#26377;&#30410;&#20110;&#26679;&#26412;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12856v1 Announce Type: new  Abstract: In reinforcement learning (RL), exploiting environmental symmetries can significantly enhance efficiency, robustness, and performance. However, ensuring that the deep RL policy and value networks are respectively equivariant and invariant to exploit these symmetries is a substantial challenge. Related works try to design networks that are equivariant and invariant by construction, limiting them to a very restricted library of components, which in turn hampers the expressiveness of the networks. This paper proposes a method to construct equivariant policies and invariant value functions without specialized neural network components, which we term equivariant ensembles. We further add a regularization term for adding inductive bias during training. In a map-based path planning case study, we show how equivariant ensembles and regularization benefit sample efficiency and performance.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31574;&#30053;&#25193;&#25955;&#21644;3D&#22330;&#26223;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;3D Diffuser Actor&#65292;&#19968;&#20010;&#31070;&#32463;&#31574;&#30053;&#26550;&#26500;&#65292;&#21487;&#20197;&#26681;&#25454;&#35821;&#35328;&#25351;&#20196;&#26500;&#24314;3D&#35270;&#35273;&#22330;&#26223;&#34920;&#31034;&#65292;&#24182;&#23545;&#26426;&#22120;&#20154;&#26411;&#31471;&#25191;&#34892;&#22120;&#30340;3D&#26059;&#36716;&#21644;&#24179;&#31227;&#36827;&#34892;&#36845;&#20195;&#21435;&#22122;&#12290;</title><link>https://arxiv.org/abs/2402.10885</link><description>&lt;p&gt;
&#22522;&#20110;3D&#22330;&#26223;&#34920;&#31034;&#30340;3D&#25193;&#25955;&#22120;Actor&#65306;&#36890;&#36807;&#31574;&#30053;&#25193;&#25955;&#36827;&#34892;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
3D Diffuser Actor: Policy Diffusion with 3D Scene Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10885
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31574;&#30053;&#25193;&#25955;&#21644;3D&#22330;&#26223;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;3D Diffuser Actor&#65292;&#19968;&#20010;&#31070;&#32463;&#31574;&#30053;&#26550;&#26500;&#65292;&#21487;&#20197;&#26681;&#25454;&#35821;&#35328;&#25351;&#20196;&#26500;&#24314;3D&#35270;&#35273;&#22330;&#26223;&#34920;&#31034;&#65292;&#24182;&#23545;&#26426;&#22120;&#20154;&#26411;&#31471;&#25191;&#34892;&#22120;&#30340;3D&#26059;&#36716;&#21644;&#24179;&#31227;&#36827;&#34892;&#36845;&#20195;&#21435;&#22122;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#25193;&#25955;&#31574;&#30053;&#21644;3D&#22330;&#26223;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;&#25193;&#25955;&#31574;&#30053;&#36890;&#36807;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#22522;&#20110;&#26426;&#22120;&#20154;&#21644;&#29615;&#22659;&#29366;&#24577;&#30340;&#21160;&#20316;&#20998;&#24067;&#12290;&#26368;&#36817;&#65292;&#23427;&#20204;&#24050;&#32463;&#34920;&#29616;&#20986;&#20248;&#20110;&#30830;&#23450;&#24615;&#21644;&#20854;&#20182;&#22522;&#20110;&#29366;&#24577;&#30340;&#21160;&#20316;&#20998;&#24067;&#23398;&#20064;&#26041;&#27861;&#12290;3D&#26426;&#22120;&#20154;&#31574;&#30053;&#20351;&#29992;&#20174;&#21333;&#20010;&#25110;&#22810;&#20010;&#25668;&#20687;&#22836;&#35270;&#35282;&#33719;&#21462;&#30340;&#24863;&#24212;&#28145;&#24230;&#32858;&#21512;&#30340;3D&#22330;&#26223;&#29305;&#24449;&#34920;&#31034;&#12290;&#23427;&#20204;&#24050;&#32463;&#35777;&#26126;&#27604;&#20854;2D&#23545;&#24212;&#29289;&#22312;&#25668;&#20687;&#26426;&#35270;&#35282;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#32479;&#19968;&#20102;&#36825;&#20004;&#26465;&#32447;&#36335;&#30340;&#24037;&#20316;&#65292;&#24182;&#25552;&#20986;&#20102;3D&#25193;&#25955;&#22120;Actor&#65292;&#36825;&#26159;&#19968;&#20010;&#31070;&#32463;&#31574;&#30053;&#26550;&#26500;&#65292;&#23427;&#22312;&#32473;&#23450;&#35821;&#35328;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#65292;&#26500;&#24314;&#35270;&#35273;&#22330;&#26223;&#30340;3D&#34920;&#31034;&#65292;&#24182;&#22312;&#20854;&#19978;&#36827;&#34892;&#26465;&#20214;&#36845;&#20195;&#21435;&#22122;&#26426;&#22120;&#20154;&#26411;&#31471;&#25191;&#34892;&#22120;&#30340;3D&#26059;&#36716;&#21644;&#24179;&#31227;&#12290;&#22312;&#27599;&#20010;&#21435;&#22122;&#36845;&#20195;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#24577;&#20272;&#35745;&#34920;&#31034;&#20026;3D&#22330;&#26223;&#20196;&#29260;&#65292;&#24182;&#39044;&#27979;t
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10885v1 Announce Type: cross  Abstract: We marry diffusion policies and 3D scene representations for robot manipulation. Diffusion policies learn the action distribution conditioned on the robot and environment state using conditional diffusion models. They have recently shown to outperform both deterministic and alternative state-conditioned action distribution learning methods. 3D robot policies use 3D scene feature representations aggregated from a single or multiple camera views using sensed depth. They have shown to generalize better than their 2D counterparts across camera viewpoints. We unify these two lines of work and present 3D Diffuser Actor, a neural policy architecture that, given a language instruction, builds a 3D representation of the visual scene and conditions on it to iteratively denoise 3D rotations and translations for the robot's end-effector. At each denoising iteration, our model represents end-effector pose estimates as 3D scene tokens and predicts t
&lt;/p&gt;</description></item></channel></rss>