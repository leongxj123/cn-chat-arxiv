<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#20351;&#29992;&#20851;&#38190;&#21160;&#20316;&#20196;&#29260;&#65288;KAT&#65289;&#26694;&#26550;&#65292;&#30740;&#31350;&#23637;&#31034;&#20102;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#21464;&#24418;&#22120;&#65288;GPT-4 Turbo&#65289;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#21487;&#23454;&#29616;&#35270;&#35273;&#27169;&#20223;&#23398;&#20064;&#65292;&#23558;&#35270;&#35273;&#35266;&#27979;&#26144;&#23556;&#20026;&#27169;&#25311;&#31034;&#33539;&#32773;&#34892;&#20026;&#30340;&#21160;&#20316;&#24207;&#21015;&#65292;&#34920;&#29616;&#20248;&#36234;&#20110;&#29616;&#26377;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.19578</link><description>&lt;p&gt;
&#20851;&#38190;&#21160;&#20316;&#20196;&#29260;&#22312;&#26426;&#22120;&#20154;&#23398;&#20013;&#23454;&#29616;&#19978;&#19979;&#25991;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Keypoint Action Tokens Enable In-Context Imitation Learning in Robotics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19578
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20851;&#38190;&#21160;&#20316;&#20196;&#29260;&#65288;KAT&#65289;&#26694;&#26550;&#65292;&#30740;&#31350;&#23637;&#31034;&#20102;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#21464;&#24418;&#22120;&#65288;GPT-4 Turbo&#65289;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#21487;&#23454;&#29616;&#35270;&#35273;&#27169;&#20223;&#23398;&#20064;&#65292;&#23558;&#35270;&#35273;&#35266;&#27979;&#26144;&#23556;&#20026;&#27169;&#25311;&#31034;&#33539;&#32773;&#34892;&#20026;&#30340;&#21160;&#20316;&#24207;&#21015;&#65292;&#34920;&#29616;&#20248;&#36234;&#20110;&#29616;&#26377;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#25104;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#21464;&#24418;&#22120;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#65292;&#23601;&#21487;&#20197;&#25191;&#34892;&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#20869;&#35270;&#35273;&#27169;&#20223;&#23398;&#20064;&#65292;&#23558;&#35270;&#35273;&#35266;&#27979;&#26144;&#23556;&#20026;&#27169;&#25311;&#31034;&#33539;&#32773;&#34892;&#20026;&#30340;&#21160;&#20316;&#24207;&#21015;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#35270;&#35273;&#35266;&#27979;&#65288;&#36755;&#20837;&#65289;&#21644;&#21160;&#20316;&#36712;&#36857;&#65288;&#36755;&#20986;&#65289;&#36716;&#25442;&#20026;&#19968;&#31995;&#21015;&#20196;&#29260;&#65292;&#36825;&#20123;&#20196;&#29260;&#21487;&#20197;&#34987;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#21464;&#24418;&#22120;&#65288;GPT-4 Turbo&#65289;&#25509;&#25910;&#21644;&#29983;&#25104;&#65292;&#36890;&#36807;&#25105;&#20204;&#31216;&#20043;&#20026;&#20851;&#38190;&#21160;&#20316;&#20196;&#29260;&#65288;KAT&#65289;&#30340;&#26694;&#26550;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#23613;&#31649;&#20165;&#22312;&#35821;&#35328;&#19978;&#35757;&#32451;&#65292;&#25105;&#20204;&#23637;&#31034;&#36825;&#20123;&#21464;&#24418;&#22120;&#25797;&#38271;&#23558;&#26631;&#35760;&#21270;&#30340;&#35270;&#35273;&#20851;&#38190;&#28857;&#35266;&#23519;&#32763;&#35793;&#20026;&#34892;&#20026;&#36712;&#36857;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#26085;&#24120;&#20219;&#21153;&#22871;&#20214;&#20013;&#65292;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#34920;&#29616;&#19982;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#20223;&#23398;&#20064;&#65288;&#25193;&#25955;&#31574;&#30053;&#65289;&#12290;KAT&#19981;&#21516;&#20110;&#36890;&#24120;&#22312;&#35821;&#35328;&#39046;&#22495;&#25805;&#20316;&#65292;&#23427;&#21033;&#29992;&#22522;&#20110;&#25991;&#26412;&#30340;&#21464;&#24418;&#22120;&#22312;&#35270;&#35273;&#21644;&#21160;&#20316;&#39046;&#22495;&#20013;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19578v1 Announce Type: cross  Abstract: We show that off-the-shelf text-based Transformers, with no additional training, can perform few-shot in-context visual imitation learning, mapping visual observations to action sequences that emulate the demonstrator's behaviour. We achieve this by transforming visual observations (inputs) and trajectories of actions (outputs) into sequences of tokens that a text-pretrained Transformer (GPT-4 Turbo) can ingest and generate, via a framework we call Keypoint Action Tokens (KAT). Despite being trained only on language, we show that these Transformers excel at translating tokenised visual keypoint observations into action trajectories, performing on par or better than state-of-the-art imitation learning (diffusion policies) in the low-data regime on a suite of real-world, everyday tasks. Rather than operating in the language domain as is typical, KAT leverages text-based Transformers to operate in the vision and action domains to learn ge
&lt;/p&gt;</description></item><item><title>LeTac-MPC&#26159;&#19968;&#31181;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65292;&#21033;&#29992;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;GelSight&#21644;&#19981;&#21516;iable MPC&#23618;&#65292;&#23454;&#29616;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#21644;&#20855;&#26377;&#19981;&#21516;&#29289;&#29702;&#23646;&#24615;&#30340;&#29289;&#20307;&#19978;&#36827;&#34892;&#31283;&#20581;&#25235;&#21462;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.04934</link><description>&lt;p&gt;
LeTac-MPC&#65306;&#29992;&#20110;&#35302;&#35273;&#21453;&#24212;&#25235;&#21462;&#30340;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
LeTac-MPC: Learning Model Predictive Control for Tactile-reactive Grasping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04934
&lt;/p&gt;
&lt;p&gt;
LeTac-MPC&#26159;&#19968;&#31181;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65292;&#21033;&#29992;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;GelSight&#21644;&#19981;&#21516;iable MPC&#23618;&#65292;&#23454;&#29616;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#21644;&#20855;&#26377;&#19981;&#21516;&#29289;&#29702;&#23646;&#24615;&#30340;&#29289;&#20307;&#19978;&#36827;&#34892;&#31283;&#20581;&#25235;&#21462;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25235;&#21462;&#26159;&#26426;&#22120;&#20154;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#38656;&#35201;&#35302;&#35273;&#21453;&#39304;&#21644;&#21453;&#24212;&#24615;&#25235;&#21462;&#35843;&#25972;&#65292;&#20197;&#23454;&#29616;&#22312;&#21508;&#31181;&#26465;&#20214;&#19979;&#21644;&#20855;&#26377;&#19981;&#21516;&#29289;&#29702;&#23646;&#24615;&#30340;&#23545;&#35937;&#30340;&#31283;&#20581;&#25235;&#21462;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;LeTac-MPC&#65292;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#29992;&#20110;&#35302;&#35273;&#21453;&#24212;&#24335;&#25235;&#21462;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#22841;&#29226;&#33021;&#22815;&#22312;&#21160;&#24577;&#21644;&#21147;&#20132;&#20114;&#20219;&#21153;&#20013;&#25235;&#21462;&#20855;&#26377;&#19981;&#21516;&#29289;&#29702;&#23646;&#24615;&#30340;&#23545;&#35937;&#12290;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;&#35270;&#35273;&#30340;&#35302;&#35273;&#20256;&#24863;&#22120;GelSight&#65292;&#35813;&#20256;&#24863;&#22120;&#33021;&#22815;&#24863;&#30693;&#21253;&#21547;&#25235;&#21462;&#23545;&#35937;&#30340;&#29289;&#29702;&#23646;&#24615;&#21644;&#29366;&#24577;&#20449;&#24687;&#30340;&#39640;&#20998;&#36776;&#29575;&#35302;&#35273;&#21453;&#39304;&#12290;LeTac-MPC&#21253;&#21547;&#19968;&#20010;&#21487;&#24494;&#20998;&#30340;MPC&#23618;&#65292;&#35774;&#35745;&#29992;&#20110;&#23545;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#20174;&#35302;&#35273;&#21453;&#39304;&#20013;&#25552;&#21462;&#30340;&#23884;&#20837;&#36827;&#34892;&#24314;&#27169;&#12290;&#36825;&#31181;&#35774;&#35745;&#26377;&#21161;&#20110;&#22312;25 Hz&#30340;&#39057;&#29575;&#19979;&#23454;&#29616;&#25910;&#25947;&#21644;&#31283;&#20581;&#30340;&#25235;&#21462;&#25511;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#25968;&#25454;&#25910;&#38598;&#27969;&#31243;&#65292;&#24182;&#25910;&#38598;&#20102;&#19968;&#32452;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04934v1 Announce Type: cross  Abstract: Grasping is a crucial task in robotics, necessitating tactile feedback and reactive grasping adjustments for robust grasping of objects under various conditions and with differing physical properties. In this paper, we introduce LeTac-MPC, a learning-based model predictive control (MPC) for tactile-reactive grasping. Our approach enables the gripper grasp objects with different physical properties on dynamic and force-interactive tasks. We utilize a vision-based tactile sensor, GelSight, which is capable of perceiving high-resolution tactile feedback that contains the information of physical properties and states of the grasped object. LeTac-MPC incorporates a differentiable MPC layer designed to model the embeddings extracted by a neural network (NN) from tactile feedback. This design facilitates convergent and robust grasping control at a frequency of 25 Hz. We propose a fully automated data collection pipeline and collect a dataset 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36712;&#36857;&#29983;&#25104;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36890;&#29992;&#24037;&#20855;&#20351;&#29992;&#25216;&#33021;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#24418;&#29366;&#30340;&#24037;&#20855;&#65292;&#20174;&#32780;&#20351;&#33258;&#20027;&#31995;&#32479;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.00156</link><description>&lt;p&gt;
&#36890;&#36807;&#36712;&#36857;&#29983;&#25104;&#23398;&#20064;&#20855;&#26377;&#36890;&#29992;&#24615;&#30340;&#24037;&#20855;&#20351;&#29992;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Learning Generalizable Tool-use Skills through Trajectory Generation. (arXiv:2310.00156v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00156
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36712;&#36857;&#29983;&#25104;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36890;&#29992;&#24037;&#20855;&#20351;&#29992;&#25216;&#33021;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#24418;&#29366;&#30340;&#24037;&#20855;&#65292;&#20174;&#32780;&#20351;&#33258;&#20027;&#31995;&#32479;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#21033;&#29992;&#24037;&#20855;&#30340;&#33258;&#20027;&#31995;&#32479;&#21487;&#20197;&#24110;&#21161;&#20154;&#20204;&#23436;&#25104;&#35768;&#22810;&#24120;&#35265;&#20219;&#21153;&#65292;&#22914;&#28921;&#39274;&#21644;&#28165;&#27905;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#31995;&#32479;&#22312;&#36866;&#24212;&#26032;&#24037;&#20855;&#26041;&#38754;&#36828;&#36828;&#19981;&#21450;&#20154;&#31867;&#30340;&#26234;&#33021;&#27700;&#24179;&#12290;&#22522;&#20110;&#21487;&#21450;&#24615;&#30340;&#20808;&#21069;&#24037;&#20316;&#36890;&#24120;&#23545;&#29615;&#22659;&#20570;&#20986;&#20102;&#24456;&#24378;&#30340;&#20551;&#35774;&#65292;&#24182;&#19988;&#26080;&#27861;&#25193;&#23637;&#21040;&#26356;&#22797;&#26434;&#12289;&#25509;&#35302;&#20016;&#23500;&#30340;&#20219;&#21153;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20010;&#25361;&#25112;&#65292;&#24182;&#25506;&#32034;&#20102;&#20195;&#29702;&#22914;&#20309;&#23398;&#20064;&#20351;&#29992;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#24037;&#20855;&#26469;&#25805;&#32437;&#21487;&#21464;&#24418;&#29289;&#20307;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#24037;&#20855;&#20351;&#29992;&#36712;&#36857;&#20316;&#20026;&#19968;&#31995;&#21015;&#28857;&#20113;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#24037;&#20855;&#24418;&#29366;&#12290;&#23545;&#20110;&#20219;&#20309;&#26032;&#30340;&#24037;&#20855;&#65292;&#25105;&#20204;&#39318;&#20808;&#29983;&#25104;&#19968;&#20010;&#24037;&#20855;&#20351;&#29992;&#36712;&#36857;&#65292;&#28982;&#21518;&#20248;&#21270;&#24037;&#20855;&#23039;&#21183;&#24207;&#21015;&#20197;&#19982;&#29983;&#25104;&#30340;&#36712;&#36857;&#23545;&#40784;&#12290;&#25105;&#20204;&#20026;&#22235;&#31181;&#19981;&#21516;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#25805;&#32437;&#20219;&#21153;&#35757;&#32451;&#20102;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20165;&#20351;&#29992;&#27599;&#20010;&#20219;&#21153;&#30340;&#21333;&#20010;&#24037;&#20855;&#30340;&#31034;&#33539;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
Autonomous systems that efficiently utilize tools can assist humans in completing many common tasks such as cooking and cleaning. However, current systems fall short of matching human-level of intelligence in terms of adapting to novel tools. Prior works based on affordance often make strong assumptions about the environments and cannot scale to more complex, contact-rich tasks. In this work, we tackle this challenge and explore how agents can learn to use previously unseen tools to manipulate deformable objects. We propose to learn a generative model of the tool-use trajectories as a sequence of point clouds, which generalizes to different tool shapes. Given any novel tool, we first generate a tool-use trajectory and then optimize the sequence of tool poses to align with the generated trajectory. We train a single model for four different challenging deformable object manipulation tasks. Our model is trained with demonstration data from just a single tool for each task and is able to 
&lt;/p&gt;</description></item></channel></rss>