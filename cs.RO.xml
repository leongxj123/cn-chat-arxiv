<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#25512;&#29702;&#33021;&#21147;&#65292;&#32467;&#21512;&#28145;&#24230;&#20449;&#24687;&#21644;&#35270;&#35273;&#25552;&#31034;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#20855;&#36523;&#38382;&#31572;&#20013;&#30340;&#26377;&#25928;&#25506;&#32034;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.15941</link><description>&lt;p&gt;
&#25506;&#32034;&#30452;&#21040;&#33258;&#20449;: &#38754;&#21521;&#20855;&#36523;&#38382;&#31572;&#30340;&#39640;&#25928;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Explore until Confident: Efficient Exploration for Embodied Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15941
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#25512;&#29702;&#33021;&#21147;&#65292;&#32467;&#21512;&#28145;&#24230;&#20449;&#24687;&#21644;&#35270;&#35273;&#25552;&#31034;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#20855;&#36523;&#38382;&#31572;&#20013;&#30340;&#26377;&#25928;&#25506;&#32034;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#20855;&#36523;&#38382;&#31572;&#65288;EQA&#65289;&#30340;&#38382;&#39064;&#65292;&#36825;&#25351;&#30340;&#26159;&#22312;&#38656;&#35201;&#20027;&#21160;&#25506;&#32034;&#29615;&#22659;&#20197;&#25910;&#38598;&#20449;&#24687;&#30452;&#21040;&#23545;&#38382;&#39064;&#30340;&#31572;&#26696;&#26377;&#33258;&#20449;&#30340;&#20855;&#36523;&#20195;&#29702;&#65292;&#20363;&#22914;&#26426;&#22120;&#20154;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#24378;&#22823;&#35821;&#20041;&#25512;&#29702;&#33021;&#21147;&#26469;&#39640;&#25928;&#25506;&#32034;&#21644;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;EQA&#20013;&#20351;&#29992;VLMs&#26102;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#23427;&#20204;&#27809;&#26377;&#20869;&#37096;&#35760;&#24518;&#23558;&#22330;&#26223;&#26144;&#23556;&#20197;&#20415;&#35268;&#21010;&#22914;&#20309;&#38543;&#26102;&#38388;&#25506;&#32034;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#32622;&#20449;&#24230;&#21487;&#33021;&#34987;&#38169;&#35823;&#26657;&#20934;&#24182;&#21487;&#33021;&#23548;&#33268;&#26426;&#22120;&#20154;&#36807;&#26089;&#20572;&#27490;&#25506;&#32034;&#25110;&#36807;&#24230;&#25506;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#39318;&#20808;&#22522;&#20110;&#28145;&#24230;&#20449;&#24687;&#21644;&#36890;&#36807;&#35270;&#35273;&#25552;&#31034;VLM&#26469;&#26500;&#24314;&#22330;&#26223;&#30340;&#35821;&#20041;&#22320;&#22270;-&#21033;&#29992;&#20854;&#23545;&#22330;&#26223;&#30456;&#20851;&#21306;&#22495;&#30340;&#24191;&#27867;&#30693;&#35782;&#26469;&#36827;&#34892;&#25506;&#32034;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20351;&#29992;&#31526;&#21512;&#39044;&#27979;&#26469;&#26657;&#20934;VLM&#30340;&#32622;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15941v1 Announce Type: cross  Abstract: We consider the problem of Embodied Question Answering (EQA), which refers to settings where an embodied agent such as a robot needs to actively explore an environment to gather information until it is confident about the answer to a question. In this work, we leverage the strong semantic reasoning capabilities of large vision-language models (VLMs) to efficiently explore and answer such questions. However, there are two main challenges when using VLMs in EQA: they do not have an internal memory for mapping the scene to be able to plan how to explore over time, and their confidence can be miscalibrated and can cause the robot to prematurely stop exploration or over-explore. We propose a method that first builds a semantic map of the scene based on depth information and via visual prompting of a VLM - leveraging its vast knowledge of relevant regions of the scene for exploration. Next, we use conformal prediction to calibrate the VLM's 
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#19968;&#31181;&#22312;&#21160;&#24577;&#28216;&#25103;&#20013;&#23558;&#25968;&#25454;&#39537;&#21160;&#21442;&#32771;&#25919;&#31574;&#19982;&#22522;&#20110;&#20248;&#21270;&#21338;&#24328;&#25919;&#31574;&#30456;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21512;&#20316;&#21160;&#24577;&#21338;&#24328;KLGame&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#38024;&#23545;&#27599;&#20010;&#20915;&#31574;&#32773;&#30340;&#21487;&#35843;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.14174</link><description>&lt;p&gt;
&#22312;&#21160;&#24577;&#28216;&#25103;&#20013;&#34701;&#21512;&#25968;&#25454;&#39537;&#21160;&#30340;&#20808;&#39564;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Blending Data-Driven Priors in Dynamic Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14174
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#19968;&#31181;&#22312;&#21160;&#24577;&#28216;&#25103;&#20013;&#23558;&#25968;&#25454;&#39537;&#21160;&#21442;&#32771;&#25919;&#31574;&#19982;&#22522;&#20110;&#20248;&#21270;&#21338;&#24328;&#25919;&#31574;&#30456;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21512;&#20316;&#21160;&#24577;&#21338;&#24328;KLGame&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#38024;&#23545;&#27599;&#20010;&#20915;&#31574;&#32773;&#30340;&#21487;&#35843;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26234;&#33021;&#26426;&#22120;&#20154;&#22914;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#20154;&#32676;&#20013;&#30340;&#37096;&#32626;&#36234;&#26469;&#36234;&#22810;&#65292;&#36825;&#20123;&#31995;&#32479;&#24212;&#35813;&#22312;&#23433;&#20840;&#30340;&#12289;&#19982;&#20154;&#20114;&#21160;&#24847;&#35782;&#30456;&#20851;&#30340;&#36816;&#21160;&#35268;&#21010;&#20013;&#21033;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#21338;&#24328;&#35770;&#35268;&#21010;&#22120;&#19982;&#25968;&#25454;&#39537;&#21160;&#25919;&#31574;&#30340;&#31243;&#24230;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#34701;&#21512;&#25968;&#25454;&#39537;&#21160;&#21442;&#32771;&#25919;&#31574;&#21644;&#22522;&#20110;&#20248;&#21270;&#30340;&#21338;&#24328;&#35770;&#25919;&#31574;&#30340;&#21407;&#21017;&#24615;&#26041;&#27861;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;KLGame&#65292;&#36825;&#26159;&#19968;&#31181;&#24102;&#26377;Kullback-Leibler&#65288;KL&#65289;&#27491;&#21017;&#21270;&#30340;&#38750;&#21512;&#20316;&#21160;&#24577;&#21338;&#24328;&#65292;&#38024;&#23545;&#19968;&#20010;&#19968;&#33324;&#30340;&#12289;&#38543;&#26426;&#30340;&#65292;&#21487;&#33021;&#26159;&#22810;&#27169;&#24335;&#30340;&#21442;&#32771;&#25919;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14174v1 Announce Type: cross  Abstract: As intelligent robots like autonomous vehicles become increasingly deployed in the presence of people, the extent to which these systems should leverage model-based game-theoretic planners versus data-driven policies for safe, interaction-aware motion planning remains an open question. Existing dynamic game formulations assume all agents are task-driven and behave optimally. However, in reality, humans tend to deviate from the decisions prescribed by these models, and their behavior is better approximated under a noisy-rational paradigm. In this work, we investigate a principled methodology to blend a data-driven reference policy with an optimization-based game-theoretic policy. We formulate KLGame, a type of non-cooperative dynamic game with Kullback-Leibler (KL) regularization with respect to a general, stochastic, and possibly multi-modal reference policy. Our method incorporates, for each decision maker, a tunable parameter that pe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#35328;&#25351;&#20196;&#22320;&#38754;&#21270;&#36816;&#21160;&#35268;&#21010;&#65288;LIMP&#65289;&#31995;&#32479;&#65292;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#21644;&#26102;&#38388;&#36923;&#36753;&#29983;&#25104;&#25351;&#20196;&#26465;&#20214;&#30340;&#35821;&#20041;&#22320;&#22270;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#21487;&#39564;&#35777;&#22320;&#36981;&#24490;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#38271;&#26399;&#30340;&#25351;&#20196;&#65292;&#21253;&#25324;&#24320;&#25918;&#35789;&#27719;&#21442;&#29031;&#21644;&#22797;&#26434;&#30340;&#26102;&#31354;&#32422;&#26463;&#12290;</title><link>https://arxiv.org/abs/2402.11498</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#21487;&#39564;&#35777;&#22320;&#36981;&#24490;&#22797;&#26434;&#26426;&#22120;&#20154;&#25351;&#20196;
&lt;/p&gt;
&lt;p&gt;
Verifiably Following Complex Robot Instructions with Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11498
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#35328;&#25351;&#20196;&#22320;&#38754;&#21270;&#36816;&#21160;&#35268;&#21010;&#65288;LIMP&#65289;&#31995;&#32479;&#65292;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#21644;&#26102;&#38388;&#36923;&#36753;&#29983;&#25104;&#25351;&#20196;&#26465;&#20214;&#30340;&#35821;&#20041;&#22320;&#22270;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#21487;&#39564;&#35777;&#22320;&#36981;&#24490;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#38271;&#26399;&#30340;&#25351;&#20196;&#65292;&#21253;&#25324;&#24320;&#25918;&#35789;&#27719;&#21442;&#29031;&#21644;&#22797;&#26434;&#30340;&#26102;&#31354;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35753;&#26426;&#22120;&#20154;&#33021;&#22815;&#36981;&#24490;&#22797;&#26434;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20154;&#20204;&#24076;&#26395;&#22312;&#25351;&#23548;&#26426;&#22120;&#20154;&#26102;&#33021;&#22815;&#28789;&#27963;&#34920;&#36798;&#32422;&#26463;&#65292;&#25351;&#21521;&#20219;&#24847;&#22320;&#26631;&#24182;&#39564;&#35777;&#34892;&#20026;&#12290;&#30456;&#21453;&#65292;&#26426;&#22120;&#20154;&#24517;&#39035;&#23558;&#20154;&#31867;&#25351;&#20196;&#28040;&#38500;&#27495;&#20041;&#65292;&#23558;&#25351;&#20196;&#21442;&#29031;&#29289;&#32852;&#31995;&#21040;&#30495;&#23454;&#19990;&#30028;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#35328;&#25351;&#20196;&#22320;&#38754;&#21270;&#36816;&#21160;&#35268;&#21010;&#65288;LIMP&#65289;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#21644;&#26102;&#38388;&#36923;&#36753;&#29983;&#25104;&#25351;&#20196;&#26465;&#20214;&#30340;&#35821;&#20041;&#22320;&#22270;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#21487;&#39564;&#35777;&#22320;&#36981;&#24490;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#38271;&#26399;&#30340;&#25351;&#20196;&#65292;&#28085;&#30422;&#20102;&#24320;&#25918;&#35789;&#27719;&#21442;&#29031;&#21644;&#22797;&#26434;&#30340;&#26102;&#31354;&#32422;&#26463;&#12290;&#19982;&#20808;&#21069;&#22312;&#26426;&#22120;&#20154;&#20219;&#21153;&#25191;&#34892;&#20013;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;LIMP&#26500;&#24314;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#25351;&#20196;&#34920;&#31034;&#65292;&#25581;&#31034;&#20102;&#26426;&#22120;&#20154;&#19982;&#25351;&#23548;&#32773;&#39044;&#26399;&#21160;&#26426;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#32508;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11498v1 Announce Type: cross  Abstract: Enabling robots to follow complex natural language instructions is an important yet challenging problem. People want to flexibly express constraints, refer to arbitrary landmarks and verify behavior when instructing robots. Conversely, robots must disambiguate human instructions into specifications and ground instruction referents in the real world. We propose Language Instruction grounding for Motion Planning (LIMP), a system that leverages foundation models and temporal logics to generate instruction-conditioned semantic maps that enable robots to verifiably follow expressive and long-horizon instructions with open vocabulary referents and complex spatiotemporal constraints. In contrast to prior methods for using foundation models in robot task execution, LIMP constructs an explainable instruction representation that reveals the robot's alignment with an instructor's intended motives and affords the synthesis of robot behaviors that 
&lt;/p&gt;</description></item><item><title>&#26465;&#20214;&#31070;&#32463;&#19987;&#23478;&#36807;&#31243;&#65288;CNEP&#65289;&#26159;&#19968;&#31181;&#23398;&#20064;&#20174;&#28436;&#31034;&#20013;&#33719;&#21462;&#25216;&#33021;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#27169;&#24335;&#30340;&#28436;&#31034;&#20998;&#37197;&#32473;&#19981;&#21516;&#30340;&#19987;&#23478;&#32593;&#32476;&#65292;&#24182;&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#20449;&#24687;&#23558;&#19987;&#23478;&#19982;&#32534;&#30721;&#34920;&#31034;&#21305;&#37197;&#65292;&#35299;&#20915;&#20102;&#30456;&#21516;&#25216;&#33021;&#28436;&#31034;&#30340;&#21464;&#21270;&#21644;&#22810;&#31181;&#26041;&#24335;&#33719;&#21462;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.08424</link><description>&lt;p&gt;
&#26465;&#20214;&#31070;&#32463;&#19987;&#23478;&#36807;&#31243;&#29992;&#20110;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Conditional Neural Expert Processes for Learning from Demonstration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08424
&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#31070;&#32463;&#19987;&#23478;&#36807;&#31243;&#65288;CNEP&#65289;&#26159;&#19968;&#31181;&#23398;&#20064;&#20174;&#28436;&#31034;&#20013;&#33719;&#21462;&#25216;&#33021;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#27169;&#24335;&#30340;&#28436;&#31034;&#20998;&#37197;&#32473;&#19981;&#21516;&#30340;&#19987;&#23478;&#32593;&#32476;&#65292;&#24182;&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#20449;&#24687;&#23558;&#19987;&#23478;&#19982;&#32534;&#30721;&#34920;&#31034;&#21305;&#37197;&#65292;&#35299;&#20915;&#20102;&#30456;&#21516;&#25216;&#33021;&#28436;&#31034;&#30340;&#21464;&#21270;&#21644;&#22810;&#31181;&#26041;&#24335;&#33719;&#21462;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#65288;LfD&#65289;&#26159;&#26426;&#22120;&#20154;&#23398;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#19968;&#31181;&#25216;&#26415;&#65292;&#29992;&#20110;&#25216;&#33021;&#33719;&#21462;&#12290;&#28982;&#32780;&#65292;&#30456;&#21516;&#25216;&#33021;&#30340;&#28436;&#31034;&#21487;&#33021;&#23384;&#22312;&#26174;&#33879;&#30340;&#21464;&#21270;&#65292;&#25110;&#32773;&#23398;&#20064;&#31995;&#32479;&#21487;&#33021;&#21516;&#26102;&#23581;&#35797;&#33719;&#21462;&#30456;&#21516;&#25216;&#33021;&#30340;&#19981;&#21516;&#26041;&#24335;&#65292;&#36825;&#20351;&#24471;&#23558;&#36825;&#20123;&#21160;&#20316;&#32534;&#30721;&#20026;&#36816;&#21160;&#21407;&#35821;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;LfD&#26694;&#26550;&#65292;&#21363;&#26465;&#20214;&#31070;&#32463;&#19987;&#23478;&#36807;&#31243;&#65288;CNEP&#65289;&#65292;&#23427;&#23398;&#20064;&#23558;&#26469;&#33258;&#19981;&#21516;&#27169;&#24335;&#30340;&#28436;&#31034;&#20998;&#37197;&#32473;&#19981;&#21516;&#30340;&#19987;&#23478;&#32593;&#32476;&#65292;&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#20869;&#22312;&#20449;&#24687;&#23558;&#19987;&#23478;&#19982;&#32534;&#30721;&#34920;&#31034;&#21305;&#37197;&#36215;&#26469;&#12290;CNEP&#19981;&#38656;&#35201;&#22312;&#21738;&#31181;&#27169;&#24335;&#19979;&#36712;&#36857;&#23646;&#20110;&#30340;&#30417;&#30563;&#12290;&#22312;&#20154;&#24037;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;CNEP&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;CNEP&#19982;&#21478;&#19968;&#20010;LfD&#26694;&#26550;&#8212;&#8212;&#26465;&#20214;&#31070;&#32463;&#36816;&#21160;&#21407;&#35821;&#65288;CNMP&#65289;&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21253;&#25324;&#23545;&#30495;&#23454;&#26426;&#22120;&#20154;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from Demonstration (LfD) is a widely used technique for skill acquisition in robotics. However, demonstrations of the same skill may exhibit significant variances, or learning systems may attempt to acquire different means of the same skill simultaneously, making it challenging to encode these motions into movement primitives. To address these challenges, we propose an LfD framework, namely the Conditional Neural Expert Processes (CNEP), that learns to assign demonstrations from different modes to distinct expert networks utilizing the inherent information within the latent space to match experts with the encoded representations. CNEP does not require supervision on which mode the trajectories belong to. Provided experiments on artificially generated datasets demonstrate the efficacy of CNEP. Furthermore, we compare the performance of CNEP with another LfD framework, namely Conditional Neural Movement Primitives (CNMP), on a range of tasks, including experiments on a real robo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#20351;&#22235;&#36275;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#20687;&#30495;&#23454;&#21160;&#29289;&#19968;&#26679;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#31574;&#30053;&#12290;&#36890;&#36807;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#20445;&#30041;&#20102;&#21160;&#29289;&#34892;&#20026;&#30340;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#36866;&#24212;&#29615;&#22659;&#65292;&#20811;&#26381;&#25361;&#25112;&#24615;&#30340;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2308.15143</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22235;&#36275;&#26426;&#22120;&#20154;&#19978;&#23454;&#29616;&#29983;&#21160;&#30340;&#28789;&#27963;&#24615;&#21644;&#28216;&#25103;&#24615;
&lt;/p&gt;
&lt;p&gt;
Lifelike Agility and Play on Quadrupedal Robots using Reinforcement Learning and Generative Pre-trained Models. (arXiv:2308.15143v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15143
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#20351;&#22235;&#36275;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#20687;&#30495;&#23454;&#21160;&#29289;&#19968;&#26679;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#31574;&#30053;&#12290;&#36890;&#36807;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#20445;&#30041;&#20102;&#21160;&#29289;&#34892;&#20026;&#30340;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#36866;&#24212;&#29615;&#22659;&#65292;&#20811;&#26381;&#25361;&#25112;&#24615;&#30340;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24635;&#32467;&#21160;&#29289;&#21644;&#20154;&#31867;&#30340;&#30693;&#35782;&#21551;&#21457;&#20102;&#26426;&#22120;&#20154;&#21019;&#26032;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#20351;&#22235;&#36275;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#20687;&#30495;&#23454;&#21160;&#29289;&#19968;&#26679;&#25317;&#26377;&#29983;&#21160;&#30340;&#28789;&#27963;&#24615;&#21644;&#31574;&#30053;&#12290;&#21463;&#21040;&#22312;&#35821;&#35328;&#21644;&#22270;&#20687;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20808;&#36827;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20197;&#29983;&#25104;&#27169;&#25311;&#30495;&#23454;&#21160;&#29289;&#21160;&#20316;&#30340;&#36816;&#21160;&#25511;&#21046;&#20449;&#21495;&#12290;&#19982;&#20256;&#32479;&#25511;&#21046;&#22120;&#21644;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21482;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#21160;&#29289;&#36816;&#21160;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#20445;&#30041;&#26377;&#34920;&#36798;&#21147;&#30340;&#21160;&#29289;&#34892;&#20026;&#30693;&#35782;&#12290;&#39044;&#35757;&#32451;&#27169;&#22411;&#25317;&#26377;&#36275;&#22815;&#30340;&#21407;&#22987;&#32423;&#30693;&#35782;&#65292;&#20294;&#19982;&#29615;&#22659;&#26080;&#20851;&#12290;&#28982;&#21518;&#65292;&#22312;&#23398;&#20064;&#30340;&#21518;&#32493;&#38454;&#27573;&#65292;&#36890;&#36807;&#31359;&#36234;&#19968;&#20123;&#20197;&#21069;&#30340;&#26041;&#27861;&#24456;&#23569;&#32771;&#34385;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38556;&#30861;&#65292;&#22914;&#31359;&#36807;&#29421;&#31364;&#30340;&#31354;&#38388;&#31561;&#65292;&#20351;&#20854;&#36866;&#24212;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Summarizing knowledge from animals and human beings inspires robotic innovations. In this work, we propose a framework for driving legged robots act like real animals with lifelike agility and strategy in complex environments. Inspired by large pre-trained models witnessed with impressive performance in language and image understanding, we introduce the power of advanced deep generative models to produce motor control signals stimulating legged robots to act like real animals. Unlike conventional controllers and end-to-end RL methods that are task-specific, we propose to pre-train generative models over animal motion datasets to preserve expressive knowledge of animal behavior. The pre-trained model holds sufficient primitive-level knowledge yet is environment-agnostic. It is then reused for a successive stage of learning to align with the environments by traversing a number of challenging obstacles that are rarely considered in previous approaches, including creeping through narrow sp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#21551;&#21457;&#30340;&#22320;&#22270;&#26500;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#21106;&#21644;&#22238;&#28335;&#26469;&#35299;&#20915;&#22823;&#22411;&#29615;&#22659;&#19979;&#30340;&#25506;&#32034;&#38382;&#39064;&#65292;&#24182;&#22522;&#20110;&#24847;&#22806;&#24615;&#30340;&#31354;&#38388;&#32858;&#31867;&#35774;&#32622;&#25506;&#32034;&#23376;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2307.05793</link><description>&lt;p&gt;
&#31070;&#32463;&#21551;&#21457;&#30340;&#39640;&#25928;&#22320;&#22270;&#26500;&#24314;&#36890;&#36807;&#20998;&#21106;&#21644;&#22238;&#28335;
&lt;/p&gt;
&lt;p&gt;
Neuro-Inspired Efficient Map Building via Fragmentation and Recall. (arXiv:2307.05793v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#21551;&#21457;&#30340;&#22320;&#22270;&#26500;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#21106;&#21644;&#22238;&#28335;&#26469;&#35299;&#20915;&#22823;&#22411;&#29615;&#22659;&#19979;&#30340;&#25506;&#32034;&#38382;&#39064;&#65292;&#24182;&#22522;&#20110;&#24847;&#22806;&#24615;&#30340;&#31354;&#38388;&#32858;&#31867;&#35774;&#32622;&#25506;&#32034;&#23376;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#29289;&#21644;&#26426;&#22120;&#20154;&#36890;&#36807;&#26500;&#24314;&#21644;&#23436;&#21892;&#31354;&#38388;&#22320;&#22270;&#26469;&#23548;&#33322;&#29615;&#22659;&#12290;&#36825;&#20123;&#22320;&#22270;&#20351;&#24471;&#21253;&#25324;&#22238;&#23478;&#12289;&#35268;&#21010;&#12289;&#25628;&#32034;&#21644;&#35269;&#39135;&#22312;&#20869;&#30340;&#21151;&#33021;&#25104;&#20026;&#21487;&#33021;&#12290;&#22312;&#22823;&#22411;&#29615;&#22659;&#20013;&#65292;&#25506;&#32034;&#31354;&#38388;&#26159;&#19968;&#20010;&#38590;&#39064;&#65306;&#20195;&#29702;&#21487;&#33021;&#20250;&#38519;&#20837;&#23616;&#37096;&#21306;&#22495;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20174;&#31070;&#32463;&#31185;&#23398;&#20013;&#27762;&#21462;&#32463;&#39564;&#65292;&#25552;&#20986;&#24182;&#24212;&#29992;&#20102;&#20998;&#21106;&#21644;&#22238;&#28335;&#65288;FarMap&#65289;&#30340;&#27010;&#24565;&#12290;&#20195;&#29702;&#36890;&#36807;&#22522;&#20110;&#24847;&#22806;&#24615;&#30340;&#31354;&#38388;&#32858;&#31867;&#26469;&#35299;&#20915;&#22320;&#22270;&#26500;&#24314;&#38382;&#39064;&#65292;&#21516;&#26102;&#23558;&#20854;&#29992;&#20110;&#35774;&#32622;&#31354;&#38388;&#25506;&#32034;&#30340;&#23376;&#30446;&#26631;&#12290;&#20195;&#29702;&#26500;&#24314;&#21644;&#20351;&#29992;&#26412;&#22320;&#22320;&#22270;&#26469;&#39044;&#27979;&#20182;&#20204;&#30340;&#35266;&#27979;&#32467;&#26524;&#65307;&#39640;&#24847;&#22806;&#24615;&#20250;&#23548;&#33268;&#8220;&#20998;&#21106;&#20107;&#20214;&#8221;&#65292;&#20174;&#32780;&#25130;&#26029;&#26412;&#22320;&#22320;&#22270;&#12290;&#22312;&#36825;&#20123;&#20107;&#20214;&#20013;&#65292;&#26368;&#36817;&#30340;&#26412;&#22320;&#22320;&#22270;&#34987;&#25918;&#20837;&#38271;&#26399;&#35760;&#24518;&#65288;LTM&#65289;&#20013;&#65292;&#24182;&#21021;&#22987;&#21270;&#21478;&#19968;&#20010;&#26412;&#22320;&#22320;&#22270;&#12290;&#22914;&#26524;&#26029;&#35010;&#28857;&#30340;&#35266;&#23519;&#32467;&#26524;&#19982;&#23384;&#20648;&#30340;&#26576;&#20010;&#26412;&#22320;&#22320;&#22270;&#30340;&#35266;&#23519;&#32467;&#26524;&#30456;&#21305;&#37197;&#65292;&#37027;&#20040;&#35813;&#22320;&#22270;&#23601;&#20250;&#34987;&#22238;&#28335;&#65288;&#24182;&#37325;&#29992;&#65289;&#33258;LTM&#12290;&#20998;&#21106;&#28857;&#35825;&#23548;.
&lt;/p&gt;
&lt;p&gt;
Animals and robots navigate through environments by building and refining maps of the space. These maps enable functions including navigating back to home, planning, search, and foraging. In large environments, exploration of the space is a hard problem: agents can become stuck in local regions. Here, we use insights from neuroscience to propose and apply the concept of Fragmentation-and-Recall (FarMap), with agents solving the mapping problem by building local maps via a surprisal-based clustering of space, which they use to set subgoals for spatial exploration. Agents build and use a local map to predict their observations; high surprisal leads to a ``fragmentation event'' that truncates the local map. At these events, the recent local map is placed into long-term memory (LTM), and a different local map is initialized. If observations at a fracture point match observations in one of the stored local maps, that map is recalled (and thus reused) from LTM. The fragmentation points induc
&lt;/p&gt;</description></item></channel></rss>