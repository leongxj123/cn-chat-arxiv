<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20132;&#20114;&#24335;&#22330;&#26223;&#25506;&#32034;&#20219;&#21153;&#65292;&#36890;&#36807;&#33258;&#20027;&#25506;&#32034;&#29615;&#22659;&#29983;&#25104;&#20102;&#21160;&#20316;&#26465;&#20214;&#21270;&#22330;&#26223;&#22270;&#65292;&#25429;&#25417;&#20102;&#29615;&#22659;&#30340;&#32467;&#26500;</title><link>https://arxiv.org/abs/2402.15487</link><description>&lt;p&gt;
RoboEXP: &#36890;&#36807;&#20132;&#20114;&#24335;&#25506;&#32034;&#23454;&#29616;&#21160;&#20316;&#26465;&#20214;&#21270;&#22330;&#26223;&#22270;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for Robotic Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20132;&#20114;&#24335;&#22330;&#26223;&#25506;&#32034;&#20219;&#21153;&#65292;&#36890;&#36807;&#33258;&#20027;&#25506;&#32034;&#29615;&#22659;&#29983;&#25104;&#20102;&#21160;&#20316;&#26465;&#20214;&#21270;&#22330;&#26223;&#22270;&#65292;&#25429;&#25417;&#20102;&#29615;&#22659;&#30340;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#38656;&#35201;&#25506;&#32034;&#21608;&#22260;&#29615;&#22659;&#20197;&#36866;&#24212;&#24182;&#24212;&#23545;&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20132;&#20114;&#24335;&#22330;&#26223;&#25506;&#32034;&#30340;&#26032;&#20219;&#21153;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#33258;&#20027;&#25506;&#32034;&#29615;&#22659;&#24182;&#29983;&#25104;&#19968;&#20010;&#25429;&#25417;&#22522;&#30784;&#29615;&#22659;&#32467;&#26500;&#30340;&#21160;&#20316;&#26465;&#20214;&#21270;&#22330;&#26223;&#22270;&#65288;ACSG&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15487v1 Announce Type: cross  Abstract: Robots need to explore their surroundings to adapt to and tackle tasks in unknown environments. Prior work has proposed building scene graphs of the environment but typically assumes that the environment is static, omitting regions that require active interactions. This severely limits their ability to handle more complex tasks in household and office environments: before setting up a table, robots must explore drawers and cabinets to locate all utensils and condiments. In this work, we introduce the novel task of interactive scene exploration, wherein robots autonomously explore environments and produce an action-conditioned scene graph (ACSG) that captures the structure of the underlying environment. The ACSG accounts for both low-level information, such as geometry and semantics, and high-level information, such as the action-conditioned relationships between different entities in the scene. To this end, we present the Robotic Explo
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#31163;&#25955;&#25193;&#25955;&#32467;&#21512;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21644;&#23569;&#37327;&#26426;&#22120;&#20154;&#35270;&#39057;&#24494;&#35843;&#65292;&#23454;&#29616;&#20174;&#20154;&#31867;&#35270;&#39057;&#21040;&#26426;&#22120;&#20154;&#31574;&#30053;&#23398;&#20064;&#30340;&#30693;&#35782;&#36801;&#31227;&#12290;</title><link>https://arxiv.org/abs/2402.14407</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#36827;&#34892;&#22823;&#35268;&#27169;&#26080;&#21160;&#20316;&#35270;&#39057;&#39044;&#35757;&#32451;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Large-Scale Actionless Video Pre-Training via Discrete Diffusion for Efficient Policy Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14407
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#31163;&#25955;&#25193;&#25955;&#32467;&#21512;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21644;&#23569;&#37327;&#26426;&#22120;&#20154;&#35270;&#39057;&#24494;&#35843;&#65292;&#23454;&#29616;&#20174;&#20154;&#31867;&#35270;&#39057;&#21040;&#26426;&#22120;&#20154;&#31574;&#30053;&#23398;&#20064;&#30340;&#30693;&#35782;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#23436;&#25104;&#22810;&#20010;&#20219;&#21153;&#30340;&#36890;&#29992;&#23454;&#20307;&#20195;&#29702;&#38754;&#20020;&#25361;&#25112;&#65292;&#20027;&#35201;&#28304;&#33258;&#32570;&#20047;&#26377;&#26631;&#35760;&#21160;&#20316;&#30340;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#23384;&#22312;&#22823;&#37327;&#25429;&#25417;&#22797;&#26434;&#20219;&#21153;&#21644;&#19982;&#29289;&#29702;&#19990;&#30028;&#20114;&#21160;&#30340;&#20154;&#31867;&#35270;&#39057;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#26694;&#26550;&#65292;&#21033;&#29992;&#32479;&#19968;&#30340;&#31163;&#25955;&#25193;&#25955;&#23558;&#20154;&#31867;&#35270;&#39057;&#19978;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#19982;&#23569;&#37327;&#26377;&#26631;&#35760;&#26426;&#22120;&#20154;&#35270;&#39057;&#19978;&#30340;&#31574;&#30053;&#24494;&#35843;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#20154;&#31867;&#21644;&#26426;&#22120;&#20154;&#35270;&#39057;&#21387;&#32553;&#25104;&#32479;&#19968;&#30340;&#35270;&#39057;&#26631;&#35760;&#12290;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#24102;&#26377;&#33945;&#29256;&#26367;&#25442;&#25193;&#25955;&#31574;&#30053;&#30340;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#26469;&#39044;&#27979;&#28508;&#31354;&#38388;&#20013;&#30340;&#26410;&#26469;&#35270;&#39057;&#26631;&#35760;&#12290;&#22312;&#24494;&#35843;&#38454;&#27573;&#65292;&#25105;&#20204; h
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14407v1 Announce Type: new  Abstract: Learning a generalist embodied agent capable of completing multiple tasks poses challenges, primarily stemming from the scarcity of action-labeled robotic datasets. In contrast, a vast amount of human videos exist, capturing intricate tasks and interactions with the physical world. Promising prospects arise for utilizing actionless human videos for pre-training and transferring the knowledge to facilitate robot policy learning through limited robot demonstrations. In this paper, we introduce a novel framework that leverages a unified discrete diffusion to combine generative pre-training on human videos and policy fine-tuning on a small number of action-labeled robot videos. We start by compressing both human and robot videos into unified video tokens. In the pre-training stage, we employ a discrete diffusion model with a mask-and-replace diffusion strategy to predict future video tokens in the latent space. In the fine-tuning stage, we h
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#23553;&#24314;&#23398;&#20064;&#30340;&#35270;&#35273;&#23548;&#33322;&#65292;&#36890;&#36807;&#39640;&#32423;&#31649;&#29702;&#32773;&#12289;&#20013;&#32423;&#31649;&#29702;&#32773;&#21644;&#24037;&#20316;&#20195;&#29702;&#30340;&#20998;&#23618;&#32467;&#26500;&#65292;&#22312;&#19981;&#21516;&#31354;&#38388;&#21644;&#26102;&#38388;&#23610;&#24230;&#19978;&#25805;&#20316;&#65292;&#20855;&#26377;&#29420;&#29305;&#27169;&#22359;&#26469;&#23454;&#29616;&#33258;&#30417;&#30563;&#23398;&#20064;&#35760;&#24518;&#20195;&#29702;&#22320;&#22270;&#12290;</title><link>https://arxiv.org/abs/2402.12498</link><description>&lt;p&gt;
&#23553;&#24314;&#32593;&#32476;&#29992;&#20110;&#35270;&#35273;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Feudal Networks for Visual Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12498
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23553;&#24314;&#23398;&#20064;&#30340;&#35270;&#35273;&#23548;&#33322;&#65292;&#36890;&#36807;&#39640;&#32423;&#31649;&#29702;&#32773;&#12289;&#20013;&#32423;&#31649;&#29702;&#32773;&#21644;&#24037;&#20316;&#20195;&#29702;&#30340;&#20998;&#23618;&#32467;&#26500;&#65292;&#22312;&#19981;&#21516;&#31354;&#38388;&#21644;&#26102;&#38388;&#23610;&#24230;&#19978;&#25805;&#20316;&#65292;&#20855;&#26377;&#29420;&#29305;&#27169;&#22359;&#26469;&#23454;&#29616;&#33258;&#30417;&#30563;&#23398;&#20064;&#35760;&#24518;&#20195;&#29702;&#22320;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#23548;&#33322;&#36981;&#24490;&#20154;&#31867;&#21487;&#20197;&#22312;&#27809;&#26377;&#35814;&#32454;&#22320;&#22270;&#30340;&#24773;&#20917;&#19979;&#23548;&#33322;&#30340;&#30452;&#35273;&#12290;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#26159;&#22312;&#24314;&#31435;&#21253;&#21547;&#21487;&#29992;&#20110;&#35268;&#21010;&#30340;&#22270;&#20687;&#33410;&#28857;&#30340;&#25299;&#25169;&#22270;&#30340;&#21516;&#26102;&#36827;&#34892;&#20132;&#20114;&#24335;&#25506;&#32034;&#12290;&#26368;&#36817;&#30340;&#21464;&#20307;&#20174;&#34987;&#21160;&#35270;&#39057;&#20013;&#23398;&#20064;&#65292;&#24182;&#21487;&#20197;&#21033;&#29992;&#22797;&#26434;&#30340;&#31038;&#20132;&#21644;&#35821;&#20041;&#32447;&#32034;&#36827;&#34892;&#23548;&#33322;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#35270;&#39057;&#65292;&#21033;&#29992;&#22823;&#22411;&#22270;&#24182;&#19988;&#30001;&#20110;&#20351;&#29992;&#20102;&#37324;&#31243;&#35745;&#65292;&#22330;&#26223;&#19981;&#26159;&#26410;&#30693;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#23553;&#24314;&#23398;&#20064;&#30340;&#35270;&#35273;&#23548;&#33322;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#30001;&#24037;&#20316;&#20195;&#29702;&#12289;&#20013;&#32423;&#31649;&#29702;&#32773;&#21644;&#39640;&#32423;&#31649;&#29702;&#32773;&#32452;&#25104;&#30340;&#20998;&#23618;&#32467;&#26500;&#12290;&#23553;&#24314;&#23398;&#20064;&#33539;&#24335;&#30340;&#20851;&#38190;&#22312;&#20110;&#65292;&#27599;&#20010;&#32423;&#21035;&#30340;&#20195;&#29702;&#30475;&#21040;&#20219;&#21153;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#23610;&#24230;&#19978;&#36816;&#20316;&#12290;&#22312;&#27492;&#26694;&#26550;&#20013;&#24320;&#21457;&#20102;&#20004;&#20010;&#29420;&#29305;&#30340;&#27169;&#22359;&#12290;&#23545;&#20110;&#39640;&#32423;&#31649;&#29702;&#32773;&#65292;&#25105;&#20204;&#33258;&#30417;&#30563;&#22320;&#23398;&#20064;&#19968;&#20010;&#35760;&#24518;&#20195;&#29702;&#22320;&#22270;&#20197;&#35760;&#24405;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12498v1 Announce Type: cross  Abstract: Visual navigation follows the intuition that humans can navigate without detailed maps. A common approach is interactive exploration while building a topological graph with images at nodes that can be used for planning. Recent variations learn from passive videos and can navigate using complex social and semantic cues. However, a significant number of training videos are needed, large graphs are utilized, and scenes are not unseen since odometry is utilized. We introduce a new approach to visual navigation using feudal learning, which employs a hierarchical structure consisting of a worker agent, a mid-level manager, and a high-level manager. Key to the feudal learning paradigm, agents at each level see a different aspect of the task and operate at different spatial and temporal scales. Two unique modules are developed in this framework. For the high- level manager, we learn a memory proxy map in a self supervised manner to record prio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25968;&#23383;&#23402;&#29983;&#30340;&#27010;&#24565;&#65292;&#20351;&#29992;&#19981;&#21516;&#25216;&#26415;&#26500;&#24314;&#22810;&#20010;&#36890;&#29992;&#20223;&#30495;&#22120;&#65292;&#24378;&#21270;&#20102;&#33258;&#21160;&#39550;&#39542;&#36719;&#20214;&#30340;&#22522;&#20110;&#20223;&#30495;&#30340;&#27979;&#35797;&#65292;&#25552;&#39640;&#20102;&#27979;&#35797;&#32467;&#26524;&#30340;&#26222;&#36866;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.08060</link><description>&lt;p&gt;
&#20004;&#20010;&#20248;&#20110;&#19968;&#20010;&#65306;&#25968;&#23383;&#23402;&#29983;&#20197;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Two is Better Than One: Digital Siblings to Improve Autonomous Driving Testing. (arXiv:2305.08060v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25968;&#23383;&#23402;&#29983;&#30340;&#27010;&#24565;&#65292;&#20351;&#29992;&#19981;&#21516;&#25216;&#26415;&#26500;&#24314;&#22810;&#20010;&#36890;&#29992;&#20223;&#30495;&#22120;&#65292;&#24378;&#21270;&#20102;&#33258;&#21160;&#39550;&#39542;&#36719;&#20214;&#30340;&#22522;&#20110;&#20223;&#30495;&#30340;&#27979;&#35797;&#65292;&#25552;&#39640;&#20102;&#27979;&#35797;&#32467;&#26524;&#30340;&#26222;&#36866;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20223;&#30495;&#30340;&#27979;&#35797;&#26159;&#30830;&#20445;&#33258;&#21160;&#39550;&#39542;&#36719;&#20214;&#21487;&#38752;&#24615;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;&#23454;&#38469;&#20013;&#65292;&#24403;&#20225;&#19994;&#20381;&#36182;&#31532;&#19977;&#26041;&#36890;&#29992;&#20223;&#30495;&#22120;&#36827;&#34892;&#20869;&#37096;&#25110;&#22806;&#21253;&#27979;&#35797;&#26102;&#65292;&#27979;&#35797;&#32467;&#26524;&#30340;&#26222;&#36866;&#24615;&#21463;&#21040;&#23041;&#32961;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#8220;&#25968;&#23383;&#23402;&#29983;&#8221;&#30340;&#27010;&#24565;&#21152;&#24378;&#20102;&#22522;&#20110;&#20223;&#30495;&#30340;&#27979;&#35797;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#22312;&#20854;&#20013;AV&#22312;&#22810;&#20010;&#20351;&#29992;&#19981;&#21516;&#25216;&#26415;&#26500;&#24314;&#30340;&#36890;&#29992;&#20223;&#30495;&#22120;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#39318;&#20808;&#65292;&#38024;&#23545;&#27599;&#20010;&#21333;&#29420;&#30340;&#20223;&#30495;&#22120;&#33258;&#21160;&#29983;&#25104;&#27979;&#35797;&#29992;&#20363;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#29305;&#24449;&#26144;&#23556;&#23558;&#27979;&#35797;&#36801;&#31227;&#33267;&#21508;&#20010;&#20223;&#30495;&#22120;&#20043;&#38388;&#65292;&#20197;&#34920;&#24449;&#25152;&#36827;&#34892;&#30340;&#34892;&#39542;&#26465;&#20214;&#12290;&#26368;&#21518;&#65292;&#35745;&#31639;&#32852;&#21512;&#39044;&#27979;&#22833;&#25928;&#27010;&#29575;&#65292;&#24182;&#20165;&#22312;&#23402;&#29983;&#20043;&#38388;&#36798;&#25104;&#19968;&#33268;&#30340;&#24773;&#20917;&#19979;&#25253;&#21578;&#25925;&#38556;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#24320;&#28304;&#20223;&#30495;&#22120;&#23454;&#29616;&#20102;&#35813;&#26694;&#26550;&#65292;&#24182;&#22312;&#25968;&#23383;&#23402;&#29983;&#30340;&#29289;&#29702;&#27604;&#20363;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#32463;&#39564;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulation-based testing represents an important step to ensure the reliability of autonomous driving software. In practice, when companies rely on third-party general-purpose simulators, either for in-house or outsourced testing, the generalizability of testing results to real autonomous vehicles is at stake.  In this paper, we strengthen simulation-based testing by introducing the notion of digital siblings, a novel framework in which the AV is tested on multiple general-purpose simulators, built with different technologies. First, test cases are automatically generated for each individual simulator. Then, tests are migrated between simulators, using feature maps to characterize of the exercised driving conditions. Finally, the joint predicted failure probability is computed and a failure is reported only in cases of agreement among the siblings.  We implemented our framework using two open-source simulators and we empirically compared it against a digital twin of a physical scaled a
&lt;/p&gt;</description></item></channel></rss>