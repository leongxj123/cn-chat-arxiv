<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#26412;&#30740;&#31350;&#28145;&#20837;&#30740;&#31350;&#20102;&#19968;&#31181;&#30495;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#19982;&#20154;&#31867;&#36827;&#34892;&#25968;&#30334;&#27425;&#30340;&#20050;&#20051;&#29699;&#22238;&#21512;&#65292;&#24182;&#19988;&#33021;&#22815;&#31934;&#30830;&#22320;&#23558;&#29699;&#36820;&#22238;&#21040;&#39044;&#23450;&#30340;&#30446;&#26631;&#20301;&#32622;&#12290;&#35813;&#31995;&#32479;&#25972;&#21512;&#20102;&#24863;&#30693;&#23376;&#31995;&#32479;&#12289;&#39640;&#36895;&#20302;&#24310;&#36831;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#12289;&#20223;&#30495;&#33539;&#20363;&#12289;&#33258;&#21160;&#37325;&#32622;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#31561;&#21151;&#33021;&#65292;&#24182;&#23545;&#31995;&#32479;&#30340;&#35774;&#35745;&#20915;&#31574;&#21644;&#37325;&#35201;&#24615;&#36827;&#34892;&#20102;&#35814;&#32454;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2309.03315</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#20050;&#20051;&#29699;&#65306;&#19968;&#20010;&#39640;&#36895;&#23398;&#20064;&#31995;&#32479;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Robotic Table Tennis: A Case Study into a High Speed Learning System. (arXiv:2309.03315v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#28145;&#20837;&#30740;&#31350;&#20102;&#19968;&#31181;&#30495;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#19982;&#20154;&#31867;&#36827;&#34892;&#25968;&#30334;&#27425;&#30340;&#20050;&#20051;&#29699;&#22238;&#21512;&#65292;&#24182;&#19988;&#33021;&#22815;&#31934;&#30830;&#22320;&#23558;&#29699;&#36820;&#22238;&#21040;&#39044;&#23450;&#30340;&#30446;&#26631;&#20301;&#32622;&#12290;&#35813;&#31995;&#32479;&#25972;&#21512;&#20102;&#24863;&#30693;&#23376;&#31995;&#32479;&#12289;&#39640;&#36895;&#20302;&#24310;&#36831;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#12289;&#20223;&#30495;&#33539;&#20363;&#12289;&#33258;&#21160;&#37325;&#32622;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#31561;&#21151;&#33021;&#65292;&#24182;&#23545;&#31995;&#32479;&#30340;&#35774;&#35745;&#20915;&#31574;&#21644;&#37325;&#35201;&#24615;&#36827;&#34892;&#20102;&#35814;&#32454;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#31995;&#32479;&#30340;&#28145;&#20837;&#30740;&#31350;&#65292;&#27492;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#34920;&#26126;&#35813;&#31995;&#32479;&#33021;&#22815;&#19982;&#20154;&#31867;&#36827;&#34892;&#25968;&#30334;&#27425;&#30340;&#20050;&#20051;&#29699;&#22238;&#21512;&#65292;&#24182;&#19988;&#33021;&#22815;&#31934;&#30830;&#22320;&#23558;&#29699;&#36820;&#22238;&#21040;&#39044;&#23450;&#30340;&#30446;&#26631;&#20301;&#32622;&#12290;&#35813;&#31995;&#32479;&#32467;&#21512;&#20102;&#39640;&#24230;&#20248;&#21270;&#30340;&#24863;&#30693;&#23376;&#31995;&#32479;&#12289;&#39640;&#36895;&#20302;&#24310;&#36831;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#12289;&#38450;&#27490;&#29616;&#23454;&#19990;&#30028;&#20013;&#25439;&#22351;&#24182;&#33021;&#22815;&#36827;&#34892;&#38646;-shot&#36716;&#31227;&#31574;&#30053;&#35757;&#32451;&#30340;&#20223;&#30495;&#33539;&#20363;&#65292;&#20197;&#21450;&#33258;&#21160;&#37325;&#32622;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#65292;&#20351;&#33258;&#20027;&#35757;&#32451;&#21644;&#35780;&#20272;&#22312;&#29289;&#29702;&#26426;&#22120;&#20154;&#19978;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#35814;&#32454;&#25551;&#36848;&#25972;&#20010;&#31995;&#32479;&#65292;&#21253;&#25324;&#36890;&#24120;&#19981;&#24191;&#27867;&#20256;&#25773;&#30340;&#22823;&#37327;&#35774;&#35745;&#20915;&#31574;&#65292;&#24182;&#32467;&#21512;&#19968;&#31995;&#21015;&#30740;&#31350;&#26469;&#38416;&#26126;&#32531;&#35299;&#21508;&#31181;&#24310;&#36831;&#28304;&#30340;&#37325;&#35201;&#24615;&#12289;&#32771;&#34385;&#35757;&#32451;&#21644;&#37096;&#32626;&#20998;&#24067;&#21464;&#21270;&#12289;&#24863;&#30693;&#31995;&#32479;&#30340;&#31283;&#20581;&#24615;&#12289;&#31574;&#30053;&#36229;&#21442;&#25968;&#30340;&#25935;&#24863;&#24615;&#21644;&#21160;&#20316;&#31354;&#38388;&#36873;&#25321;&#31561;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;&#35270;&#39057;&#23637;&#31034;&#20102;&#31995;&#32479;&#30340;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a deep-dive into a real-world robotic learning system that, in previous work, was shown to be capable of hundreds of table tennis rallies with a human and has the ability to precisely return the ball to desired targets. This system puts together a highly optimized perception subsystem, a high-speed low-latency robot controller, a simulation paradigm that can prevent damage in the real world and also train policies for zero-shot transfer, and automated real world environment resets that enable autonomous training and evaluation on physical robots. We complement a complete system description, including numerous design decisions that are typically not widely disseminated, with a collection of studies that clarify the importance of mitigating various sources of latency, accounting for training and deployment distribution shifts, robustness of the perception system, sensitivity to policy hyper-parameters, and choice of action space. A video demonstrating the components of the sys
&lt;/p&gt;</description></item><item><title>ArrayBot&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#20102;&#36890;&#29992;&#20998;&#24067;&#24335;&#25805;&#20316;&#65292;&#36890;&#36807;&#23545;&#21160;&#20316;&#31354;&#38388;&#30340;&#37325;&#26032;&#23450;&#20041;&#21644;&#37319;&#29992;&#35302;&#35273;&#35266;&#23519;&#35757;&#32451;&#65292;&#20854;&#25511;&#21046;&#31574;&#30053;&#19981;&#20165;&#33021;&#22815;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#24418;&#29366;&#65292;&#36824;&#33021;&#22312;&#23454;&#38469;&#26426;&#22120;&#20154;&#20013;&#36827;&#34892;&#36716;&#31227;&#65292;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.16857</link><description>&lt;p&gt;
ArrayBot: &#36890;&#36807;&#35302;&#35273;&#23454;&#29616;&#36890;&#29992;&#20998;&#24067;&#24335;&#25805;&#20316;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ArrayBot: Reinforcement Learning for Generalizable Distributed Manipulation through Touch. (arXiv:2306.16857v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16857
&lt;/p&gt;
&lt;p&gt;
ArrayBot&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#20102;&#36890;&#29992;&#20998;&#24067;&#24335;&#25805;&#20316;&#65292;&#36890;&#36807;&#23545;&#21160;&#20316;&#31354;&#38388;&#30340;&#37325;&#26032;&#23450;&#20041;&#21644;&#37319;&#29992;&#35302;&#35273;&#35266;&#23519;&#35757;&#32451;&#65292;&#20854;&#25511;&#21046;&#31574;&#30053;&#19981;&#20165;&#33021;&#22815;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#24418;&#29366;&#65292;&#36824;&#33021;&#22312;&#23454;&#38469;&#26426;&#22120;&#20154;&#20013;&#36827;&#34892;&#36716;&#31227;&#65292;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;ArrayBot&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;16&#215;16&#30340;&#31446;&#21521;&#28369;&#21160;&#26609;&#21644;&#35302;&#35273;&#20256;&#24863;&#22120;&#32452;&#25104;&#30340;&#20998;&#24067;&#24335;&#25805;&#20316;&#31995;&#32479;&#65292;&#21487;&#20197;&#21516;&#26102;&#25903;&#25345;&#12289;&#24863;&#30693;&#21644;&#25805;&#20316;&#26700;&#38754;&#19978;&#30340;&#29289;&#20307;&#12290;&#20026;&#20102;&#23454;&#29616;&#36890;&#29992;&#20998;&#24067;&#24335;&#25805;&#20316;&#65292;&#25105;&#20204;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#33258;&#21160;&#21457;&#29616;&#25511;&#21046;&#31574;&#30053;&#12290;&#38754;&#23545;&#22823;&#37327;&#20887;&#20313;&#30340;&#21160;&#20316;&#65292;&#25105;&#20204;&#25552;&#20986;&#32771;&#34385;&#31354;&#38388;&#23616;&#37096;&#21160;&#20316;&#22270;&#22359;&#21644;&#39057;&#22495;&#20013;&#20302;&#39057;&#21160;&#20316;&#26469;&#37325;&#26032;&#23450;&#20041;&#21160;&#20316;&#31354;&#38388;&#12290;&#36890;&#36807;&#36825;&#20010;&#37325;&#26032;&#23450;&#20041;&#30340;&#21160;&#20316;&#31354;&#38388;&#65292;&#25105;&#20204;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#21482;&#36890;&#36807;&#35302;&#35273;&#35266;&#23519;&#21363;&#21487;&#37325;&#26032;&#23450;&#20301;&#19981;&#21516;&#30340;&#29289;&#20307;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#21457;&#29616;&#30340;&#31574;&#30053;&#19981;&#20165;&#21487;&#20197;&#25512;&#24191;&#21040;&#27169;&#25311;&#22120;&#20013;&#30475;&#19981;&#35265;&#30340;&#29289;&#20307;&#24418;&#29366;&#65292;&#32780;&#19988;&#21487;&#20197;&#22312;&#29289;&#29702;&#26426;&#22120;&#20154;&#19978;&#36827;&#34892;&#36716;&#31227;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#22495;&#38543;&#26426;&#21270;&#12290;&#21033;&#29992;&#37096;&#32626;&#30340;&#31574;&#30053;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20016;&#23500;&#30340;&#30495;&#23454;&#19990;&#30028;&#25805;&#20316;&#20219;&#21153;&#65292;&#23637;&#31034;&#20102;&#20854;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ArrayBot, a distributed manipulation system consisting of a $16 \times 16$ array of vertically sliding pillars integrated with tactile sensors, which can simultaneously support, perceive, and manipulate the tabletop objects. Towards generalizable distributed manipulation, we leverage reinforcement learning (RL) algorithms for the automatic discovery of control policies. In the face of the massively redundant actions, we propose to reshape the action space by considering the spatially local action patch and the low-frequency actions in the frequency domain. With this reshaped action space, we train RL agents that can relocate diverse objects through tactile observations only. Surprisingly, we find that the discovered policy can not only generalize to unseen object shapes in the simulator but also transfer to the physical robot without any domain randomization. Leveraging the deployed policy, we present abundant real-world manipulation tasks, illustrating the vast potential of
&lt;/p&gt;</description></item></channel></rss>