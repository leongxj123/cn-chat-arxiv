<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#25552;&#20986;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#26465;&#20214;&#27169;&#20223;&#23398;&#20064;&#20195;&#29702;&#30340;&#37096;&#32626;&#26041;&#27861;&#65292;&#36890;&#36807;&#28201;&#24230;&#32553;&#25918;&#21644;&#26412;&#22320;&#20449;&#24687;&#32858;&#21512;&#20570;&#20986;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20915;&#31574;&#65292;&#26174;&#33879;&#25552;&#21319;&#20219;&#21153;&#23436;&#25104;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.18222</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#26465;&#20214;&#27169;&#20223;&#23398;&#20064;&#31574;&#30053;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#37096;&#32626;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Deployment of Pre-trained Language-Conditioned Imitation Learning Policies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18222
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#26465;&#20214;&#27169;&#20223;&#23398;&#20064;&#20195;&#29702;&#30340;&#37096;&#32626;&#26041;&#27861;&#65292;&#36890;&#36807;&#28201;&#24230;&#32553;&#25918;&#21644;&#26412;&#22320;&#20449;&#24687;&#32858;&#21512;&#20570;&#20986;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20915;&#31574;&#65292;&#26174;&#33879;&#25552;&#21319;&#20219;&#21153;&#23436;&#25104;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#26426;&#22120;&#20154;&#31574;&#30053;&#22312;&#26469;&#33258;&#19981;&#21516;&#20219;&#21153;&#21644;&#26426;&#22120;&#20154;&#24179;&#21488;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;&#20026;&#23454;&#29616;&#36890;&#29992;&#26426;&#22120;&#20154;&#24102;&#26469;&#24456;&#22823;&#24076;&#26395;&#65307;&#28982;&#32780;&#65292;&#21487;&#38752;&#22320;&#27867;&#21270;&#21040;&#26032;&#30340;&#29615;&#22659;&#26465;&#20214;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#26465;&#20214;&#27169;&#20223;&#23398;&#20064;&#20195;&#29702;&#30340;&#37096;&#32626;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#28201;&#24230;&#32553;&#25918;&#26469;&#26657;&#20934;&#36825;&#20123;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#26657;&#20934;&#30340;&#27169;&#22411;&#36890;&#36807;&#32858;&#21512;&#20505;&#36873;&#21160;&#20316;&#30340;&#26412;&#22320;&#20449;&#24687;&#26469;&#20570;&#20986;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20915;&#31574;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#20351;&#29992;&#19977;&#20010;&#36825;&#26679;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#23454;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20854;&#28508;&#21147;&#26174;&#33879;&#25552;&#21319;&#20219;&#21153;&#23436;&#25104;&#29575;&#12290;&#38468;&#24102;&#30340;&#20195;&#30721;&#21487;&#20197;&#22312;&#20197;&#19979;&#38142;&#25509;&#25214;&#21040;&#65306;https://github.com/BobWu1998/uncertainty_quant_all.git
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18222v1 Announce Type: cross  Abstract: Large-scale robotic policies trained on data from diverse tasks and robotic platforms hold great promise for enabling general-purpose robots; however, reliable generalization to new environment conditions remains a major challenge. Toward addressing this challenge, we propose a novel approach for uncertainty-aware deployment of pre-trained language-conditioned imitation learning agents. Specifically, we use temperature scaling to calibrate these models and exploit the calibrated model to make uncertainty-aware decisions by aggregating the local information of candidate actions. We implement our approach in simulation using three such pre-trained models, and showcase its potential to significantly enhance task completion rates. The accompanying code is accessible at the link: https://github.com/BobWu1998/uncertainty_quant_all.git
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;Large Language Model (LLM)&#20026;&#22522;&#30784;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#65292;&#29992;&#20110;&#19982;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#20799;&#31461;&#36827;&#34892;&#21475;&#22836;&#20132;&#27969;&#65292;&#25945;&#25480;&#36879;&#35270;&#33021;&#21147;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;LLM&#31649;&#36947;&#65292;&#21457;&#29616;GPT-2 + BART&#31649;&#36947;&#21487;&#20197;&#26356;&#22909;&#22320;&#29983;&#25104;&#38382;&#39064;&#21644;&#36873;&#25321;&#39033;&#12290;&#36825;&#31181;&#30740;&#31350;&#26377;&#21161;&#20110;&#25913;&#21892;&#33258;&#38381;&#30151;&#20799;&#31461;&#30340;&#31038;&#20132;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.00260</link><description>&lt;p&gt;
&#20197;LLM&#20026;&#22522;&#30784;&#23454;&#29616;&#38754;&#21521;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#20799;&#31461;&#30340;&#21487;&#25193;&#23637;&#26426;&#22120;&#20154;&#24178;&#39044;
&lt;/p&gt;
&lt;p&gt;
Towards scalable robotic intervention of children with Autism Spectrum Disorder using LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;Large Language Model (LLM)&#20026;&#22522;&#30784;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#65292;&#29992;&#20110;&#19982;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#20799;&#31461;&#36827;&#34892;&#21475;&#22836;&#20132;&#27969;&#65292;&#25945;&#25480;&#36879;&#35270;&#33021;&#21147;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;LLM&#31649;&#36947;&#65292;&#21457;&#29616;GPT-2 + BART&#31649;&#36947;&#21487;&#20197;&#26356;&#22909;&#22320;&#29983;&#25104;&#38382;&#39064;&#21644;&#36873;&#25321;&#39033;&#12290;&#36825;&#31181;&#30740;&#31350;&#26377;&#21161;&#20110;&#25913;&#21892;&#33258;&#38381;&#30151;&#20799;&#31461;&#30340;&#31038;&#20132;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#19982;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;(ASD)&#20799;&#31461;&#36827;&#34892;&#21475;&#22836;&#20132;&#27969;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#12290;&#36825;&#31181;&#20132;&#27969;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;Large Language Model (LLM)&#29983;&#25104;&#30340;&#25991;&#26412;&#26469;&#25945;&#25480;&#36879;&#35270;&#33021;&#21147;&#12290;&#31038;&#20132;&#26426;&#22120;&#20154;NAO&#25198;&#28436;&#20102;&#19968;&#20010;&#21050;&#28608;&#22120;(&#21475;&#22836;&#25551;&#36848;&#31038;&#20132;&#24773;&#26223;&#24182;&#25552;&#38382;)&#12289;&#25552;&#31034;&#22120;(&#25552;&#20379;&#19977;&#20010;&#36873;&#25321;&#39033;&#20379;&#36873;&#25321;)&#21644;&#22870;&#21169;&#22120;(&#24403;&#31572;&#26696;&#27491;&#30830;&#26102;&#32473;&#20104;&#31216;&#36190;)&#30340;&#35282;&#33394;&#12290;&#23545;&#20110;&#21050;&#28608;&#22120;&#30340;&#35282;&#33394;&#65292;&#31038;&#20132;&#24773;&#22659;&#12289;&#38382;&#39064;&#21644;&#36873;&#25321;&#39033;&#26159;&#20351;&#29992;&#25105;&#20204;&#30340;LLM&#31649;&#36947;&#29983;&#25104;&#30340;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181;&#26041;&#27861;&#65306;GPT-2 + BART&#21644;GPT-2 + GPT-2&#65292;&#20854;&#20013;&#31532;&#19968;&#20010;GPT-2&#22312;&#31649;&#36947;&#20013;&#26159;&#29992;&#20110;&#26080;&#30417;&#30563;&#31038;&#20132;&#24773;&#22659;&#29983;&#25104;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;SOCIALIQA&#25968;&#25454;&#38598;&#23545;&#25152;&#26377;LLM&#31649;&#36947;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-2 + BART&#31649;&#36947;&#22312;&#36890;&#36807;&#32467;&#21512;&#21508;&#33258;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#29983;&#25104;&#38382;&#39064;&#21644;&#36873;&#25321;&#39033;&#26041;&#38754;&#20855;&#26377;&#36739;&#22909;&#30340;BERTscore&#12290;&#36825;&#31181;&#35266;&#23519;&#32467;&#26524;&#20063;&#19982;&#20799;&#31461;&#22312;&#20132;&#20114;&#36807;&#31243;&#20013;&#30340;&#21512;&#20316;&#27700;&#24179;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a social robot capable of verbally interacting with children with Autism Spectrum Disorder (ASD). This communication is meant to teach perspective-taking using text generated using a Large Language Model (LLM) pipeline. The social robot NAO acts as a stimulator (verbally describes a social situation and asks a question), prompter (presents three options to choose from), and reinforcer (praises when the answer is correct). For the role of the stimulator, the social situation, questions, and options are generated using our LLM pipeline. We compare two approaches: GPT-2 + BART and GPT-2 + GPT-2, where the first GPT-2 common between the pipelines is used for unsupervised social situation generation. We use the SOCIALIQA dataset to fine-tune all of our LLM pipelines. We found that the GPT-2 + BART pipeline had a better BERTscore for generating the questions and the options by combining their individual loss functions. This observation was also consistent with the h
&lt;/p&gt;</description></item><item><title>GNFactor&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#20219;&#21153;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#20195;&#29702;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#21487;&#27867;&#21270;&#31070;&#32463;&#29305;&#24449;&#22330;&#21644;Perceiver Transformer&#27169;&#22359;&#65292;&#20197;&#21450;&#28145;&#24230;&#19977;&#32500;&#20307;&#32032;&#34920;&#31034;&#26469;&#23454;&#29616;&#23545;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#25805;&#20316;&#20219;&#21153;&#30340;&#25191;&#34892;&#12290;&#23427;&#36890;&#36807;&#23558;&#35270;&#35273;&#21644;&#35821;&#20041;&#20449;&#24687;&#32435;&#20837;&#19977;&#32500;&#34920;&#31034;&#26469;&#25552;&#39640;&#22330;&#26223;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.16891</link><description>&lt;p&gt;
GNFactor&#65306;&#20855;&#26377;&#21487;&#27867;&#21270;&#31070;&#32463;&#29305;&#24449;&#22330;&#30340;&#22810;&#20219;&#21153;&#30495;&#23454;&#26426;&#22120;&#20154;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GNFactor: Multi-Task Real Robot Learning with Generalizable Neural Feature Fields. (arXiv:2308.16891v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16891
&lt;/p&gt;
&lt;p&gt;
GNFactor&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#20219;&#21153;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#20195;&#29702;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#21487;&#27867;&#21270;&#31070;&#32463;&#29305;&#24449;&#22330;&#21644;Perceiver Transformer&#27169;&#22359;&#65292;&#20197;&#21450;&#28145;&#24230;&#19977;&#32500;&#20307;&#32032;&#34920;&#31034;&#26469;&#23454;&#29616;&#23545;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#25805;&#20316;&#20219;&#21153;&#30340;&#25191;&#34892;&#12290;&#23427;&#36890;&#36807;&#23558;&#35270;&#35273;&#21644;&#35821;&#20041;&#20449;&#24687;&#32435;&#20837;&#19977;&#32500;&#34920;&#31034;&#26469;&#25552;&#39640;&#22330;&#26223;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#32467;&#26500;&#30340;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#65292;&#20174;&#35270;&#35273;&#35266;&#23519;&#20013;&#24320;&#21457;&#33021;&#22815;&#25191;&#34892;&#22810;&#26679;&#21270;&#25805;&#20316;&#20219;&#21153;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;&#19968;&#30452;&#26159;&#26426;&#22120;&#20154;&#23398;&#20013;&#30340;&#19968;&#20010;&#38271;&#26399;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#26426;&#22120;&#20154;&#38656;&#35201;&#20840;&#38754;&#29702;&#35299;&#22330;&#26223;&#30340;&#19977;&#32500;&#32467;&#26500;&#21644;&#35821;&#20041;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GNFactor&#65292;&#19968;&#31181;&#29992;&#20110;&#22810;&#20219;&#21153;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#21487;&#35270;&#34892;&#20026;&#20811;&#38534;&#20195;&#29702;&#65292;&#23427;&#21033;&#29992;&#21487;&#27867;&#21270;&#31070;&#32463;&#29305;&#24449;&#22330;&#65288;GNF&#65289;&#20316;&#20026;&#37325;&#24314;&#27169;&#22359;&#65292;Perceiver Transformer&#20316;&#20026;&#20915;&#31574;&#27169;&#22359;&#65292;&#20849;&#20139;&#28145;&#24230;&#19977;&#32500;&#20307;&#32032;&#34920;&#31034;&#12290;&#20026;&#20102;&#23558;&#35821;&#20041;&#32435;&#20837;&#19977;&#32500;&#34920;&#31034;&#65292;&#37325;&#24314;&#27169;&#22359;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65288;&#20363;&#22914;&#65292;&#31283;&#23450;&#25193;&#25955;&#65289;&#23558;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#25552;&#21462;&#21040;&#28145;&#24230;&#19977;&#32500;&#20307;&#32032;&#20013;&#12290;&#25105;&#20204;&#22312;3&#20010;&#30495;&#23454;&#26426;&#22120;&#20154;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;GNFactor&#65292;&#24182;&#23545;10&#20010;RLBench&#20219;&#21153;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#28040;&#34701;&#23454;&#39564;&#65292;&#21482;&#20351;&#29992;&#20102;&#26377;&#38480;&#25968;&#37327;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is a long-standing problem in robotics to develop agents capable of executing diverse manipulation tasks from visual observations in unstructured real-world environments. To achieve this goal, the robot needs to have a comprehensive understanding of the 3D structure and semantics of the scene. In this work, we present $\textbf{GNFactor}$, a visual behavior cloning agent for multi-task robotic manipulation with $\textbf{G}$eneralizable $\textbf{N}$eural feature $\textbf{F}$ields. GNFactor jointly optimizes a generalizable neural field (GNF) as a reconstruction module and a Perceiver Transformer as a decision-making module, leveraging a shared deep 3D voxel representation. To incorporate semantics in 3D, the reconstruction module utilizes a vision-language foundation model ($\textit{e.g.}$, Stable Diffusion) to distill rich semantic information into the deep 3D voxel. We evaluate GNFactor on 3 real robot tasks and perform detailed ablations on 10 RLBench tasks with a limited number of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#22810;&#38754;&#20307;&#32593;&#32476;&#30340;&#26412;&#20307;&#24863;&#30693;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21160;&#21147;&#23398;&#29305;&#24615;&#21644;&#24341;&#20837;&#23884;&#20837;&#24335;&#35270;&#35273;&#65292;&#23454;&#29616;&#20102;&#22312;&#29289;&#29702;&#20132;&#20114;&#20013;&#30340;&#33258;&#36866;&#24212;&#21644;&#31896;&#24377;&#24615;&#24863;&#35273;&#65292;&#21487;&#20197;&#23454;&#26102;&#25512;&#26029;6&#32500;&#21147;&#21644;&#25197;&#30697;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.08538</link><description>&lt;p&gt;
&#20351;&#29992;&#36719;&#22810;&#38754;&#20307;&#32593;&#32476;&#30340;&#26412;&#20307;&#24863;&#30693;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Proprioceptive Learning with Soft Polyhedral Networks. (arXiv:2308.08538v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#22810;&#38754;&#20307;&#32593;&#32476;&#30340;&#26412;&#20307;&#24863;&#30693;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21160;&#21147;&#23398;&#29305;&#24615;&#21644;&#24341;&#20837;&#23884;&#20837;&#24335;&#35270;&#35273;&#65292;&#23454;&#29616;&#20102;&#22312;&#29289;&#29702;&#20132;&#20114;&#20013;&#30340;&#33258;&#36866;&#24212;&#21644;&#31896;&#24377;&#24615;&#24863;&#35273;&#65292;&#21487;&#20197;&#23454;&#26102;&#25512;&#26029;6&#32500;&#21147;&#21644;&#25197;&#30697;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#22791;&#23884;&#20837;&#24335;&#35270;&#35273;&#30340;&#36719;&#22810;&#38754;&#20307;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#29289;&#29702;&#20132;&#20114;&#20013;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;&#26412;&#20307;&#24863;&#30693;&#21644;&#31896;&#24377;&#24615;&#24863;&#35273;&#12290;&#35813;&#35774;&#35745;&#36890;&#36807;&#23398;&#20064;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#23454;&#29616;&#20102;&#23545;&#20840;&#21521;&#20132;&#20114;&#30340;&#34987;&#21160;&#36866;&#24212;&#65292;&#24182;&#36890;&#36807;&#20869;&#23884;&#30340;&#24494;&#22411;&#39640;&#36895;&#36816;&#21160;&#36319;&#36394;&#31995;&#32479;&#20197;&#35270;&#35273;&#26041;&#24335;&#25429;&#33719;&#26412;&#20307;&#24863;&#30693;&#30340;&#25968;&#25454;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36719;&#22810;&#38754;&#20307;&#32593;&#32476;&#33021;&#22815;&#20197;0.25/0.24/0.35 N&#21644;0.025/0.034/0.006 Nm&#30340;&#31934;&#24230;&#23454;&#26102;&#25512;&#26029;6&#32500;&#21147;&#21644;&#25197;&#30697;&#22312;&#21160;&#24577;&#20132;&#20114;&#20013;&#30340;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#28155;&#21152;&#31896;&#24377;&#24615;&#24863;&#21463;&#24615;&#26469;&#22312;&#38745;&#24577;&#36866;&#24212;&#20013;&#22686;&#21152;&#26412;&#20307;&#24863;&#30693;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;&#39044;&#27979;&#32467;&#26524;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proprioception is the "sixth sense" that detects limb postures with motor neurons. It requires a natural integration between the musculoskeletal systems and sensory receptors, which is challenging among modern robots that aim for lightweight, adaptive, and sensitive designs at a low cost. Here, we present the Soft Polyhedral Network with an embedded vision for physical interactions, capable of adaptive kinesthesia and viscoelastic proprioception by learning kinetic features. This design enables passive adaptations to omni-directional interactions, visually captured by a miniature high-speed motion tracking system embedded inside for proprioceptive learning. The results show that the soft network can infer real-time 6D forces and torques with accuracies of 0.25/0.24/0.35 N and 0.025/0.034/0.006 Nm in dynamic interactions. We also incorporate viscoelasticity in proprioception during static adaptation by adding a creep and relaxation modifier to refine the predicted results. The proposed 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#36965;&#25805;&#20316;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#20197;&#21450;&#23398;&#20064;&#25163;&#29289;&#20114;&#21160;&#20808;&#39564;&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#33021;&#22815;&#22312;&#32852;&#32467;&#30446;&#26631;&#21644;&#25163;&#37096;&#23039;&#24577;&#20272;&#35745;&#20013;&#23454;&#29616;&#26356;&#22909;&#30340;&#20851;&#38190;&#28857;&#23450;&#20301;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.01618</link><description>&lt;p&gt;
ContactArt&#65306;&#23398;&#20064;&#31867;&#21035;&#32423;&#32852;&#32467;&#29289;&#20307;&#21644;&#25163;&#37096;&#23039;&#24577;&#20272;&#35745;&#30340;&#19977;&#32500;&#20132;&#20114;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
ContactArt: Learning 3D Interaction Priors for Category-level Articulated Object and Hand Poses Estimation. (arXiv:2305.01618v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#36965;&#25805;&#20316;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#20197;&#21450;&#23398;&#20064;&#25163;&#29289;&#20114;&#21160;&#20808;&#39564;&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#33021;&#22815;&#22312;&#32852;&#32467;&#30446;&#26631;&#21644;&#25163;&#37096;&#23039;&#24577;&#20272;&#35745;&#20013;&#23454;&#29616;&#26356;&#22909;&#30340;&#20851;&#38190;&#28857;&#23450;&#20301;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#25163;&#37096;&#21644;&#32852;&#32467;&#30446;&#26631;&#23039;&#24577;&#20272;&#35745;&#20013;&#30340;&#25163;&#29289;&#20114;&#21160;&#20808;&#39564;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#35270;&#35273;&#36965;&#25805;&#20316;&#25910;&#38598;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#20154;&#31867;&#25805;&#20316;&#21592;&#21487;&#20197;&#30452;&#25509;&#22312;&#29289;&#29702;&#27169;&#25311;&#22120;&#20013;&#28216;&#25103;&#26469;&#25805;&#32437;&#32852;&#32467;&#23545;&#35937;&#12290; &#25105;&#20204;&#35760;&#24405;&#25968;&#25454;&#24182;&#20174;&#27169;&#25311;&#22120;&#33719;&#24471;&#26377;&#20851;&#30446;&#26631;&#23039;&#24577;&#21644;&#25509;&#35302;&#20449;&#24687;&#30340;&#20813;&#36153;&#21644;&#20934;&#30830;&#27880;&#37322;&#12290; &#25105;&#20204;&#30340;&#31995;&#32479;&#20165;&#38656;&#35201;&#20351;&#29992;iPhone&#26469;&#35760;&#24405;&#20154;&#25163;&#36816;&#21160;&#65292;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#24182;&#22823;&#22823;&#38477;&#20302;&#25968;&#25454;&#21644;&#27880;&#37322;&#25910;&#38598;&#30340;&#25104;&#26412;&#12290;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#19977;&#32500;&#20132;&#20114;&#20808;&#39564;&#65292;&#21253;&#25324;&#25429;&#33719;&#23545;&#35937;&#37096;&#20214;&#25490;&#21015;&#20998;&#24067;&#30340;&#37492;&#21035;&#22120;&#65288;&#22312;GAN&#20013;&#65289;&#65292;&#20197;&#21450;&#29983;&#25104;&#32852;&#32467;&#23545;&#35937;&#19978;&#25509;&#35302;&#21306;&#22495;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#25351;&#23548;&#25163;&#21183;&#20272;&#35745;&#12290;&#36825;&#20123;&#32467;&#26500;&#21644;&#25509;&#35302;&#20808;&#39564;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#36716;&#31227;&#21040;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#65292;&#20960;&#20046;&#27809;&#26377;&#20219;&#20309;&#39046;&#22495;&#24046;&#36317;&#12290;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#21644;&#23398;&#20064;&#30340;&#20808;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#20851;&#38190;&#28857;&#23450;&#20301;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new dataset and a novel approach to learning hand-object interaction priors for hand and articulated object pose estimation. We first collect a dataset using visual teleoperation, where the human operator can directly play within a physical simulator to manipulate the articulated objects. We record the data and obtain free and accurate annotations on object poses and contact information from the simulator. Our system only requires an iPhone to record human hand motion, which can be easily scaled up and largely lower the costs of data and annotation collection. With this data, we learn 3D interaction priors including a discriminator (in a GAN) capturing the distribution of how object parts are arranged, and a diffusion model which generates the contact regions on articulated objects, guiding the hand pose estimation. Such structural and contact priors can easily transfer to real-world data with barely any domain gap. By using our data and learned priors, our method signific
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31934;&#30830;&#22320;&#21051;&#30011;&#20102;&#20855;&#26377;&#26377;&#30028;&#25200;&#21160;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#21487;&#36798;&#38598;&#30340;&#20984;&#21253;&#20026;&#19968;&#38454;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#30340;&#20984;&#21253;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#12289;&#39640;&#31934;&#24230;&#30340;&#20272;&#35745;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#36807;&#36924;&#36817;&#21487;&#36798;&#38598;&#12290;</title><link>http://arxiv.org/abs/2303.17674</link><description>&lt;p&gt;
&#21487;&#36798;&#38598;&#30340;&#20984;&#21253;&#30340;&#31934;&#30830;&#21051;&#30011;
&lt;/p&gt;
&lt;p&gt;
Exact Characterization of the Convex Hulls of Reachable Sets. (arXiv:2303.17674v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31934;&#30830;&#22320;&#21051;&#30011;&#20102;&#20855;&#26377;&#26377;&#30028;&#25200;&#21160;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#21487;&#36798;&#38598;&#30340;&#20984;&#21253;&#20026;&#19968;&#38454;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#30340;&#20984;&#21253;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#12289;&#39640;&#31934;&#24230;&#30340;&#20272;&#35745;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#36807;&#36924;&#36817;&#21487;&#36798;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#26377;&#30028;&#25200;&#21160;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#21487;&#36798;&#38598;&#30340;&#20984;&#21253;&#12290;&#21487;&#36798;&#38598;&#22312;&#25511;&#21046;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20294;&#35745;&#31639;&#36215;&#26469;&#20173;&#28982;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29616;&#26377;&#30340;&#36807;&#36924;&#36817;&#24037;&#20855;&#24448;&#24448;&#36807;&#20110;&#20445;&#23432;&#25110;&#35745;&#31639;&#20195;&#20215;&#39640;&#26114;&#12290;&#26412;&#25991;&#31934;&#30830;&#22320;&#21051;&#30011;&#20102;&#21487;&#36798;&#38598;&#30340;&#20984;&#21253;&#65292;&#23558;&#20854;&#34920;&#31034;&#25104;&#19968;&#38454;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#30340;&#20984;&#21253;&#65292;&#36825;&#20010;&#26377;&#38480;&#32500;&#30340;&#21051;&#30011;&#24320;&#21551;&#20102;&#19968;&#31181;&#32039;&#23494;&#30340;&#20272;&#35745;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#36807;&#36924;&#36817;&#21487;&#36798;&#38598;&#65292;&#19988;&#25104;&#26412;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#20302;&#12289;&#26356;&#31934;&#20934;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#31070;&#32463;&#21453;&#39304;&#29615;&#20998;&#26512;&#21644;&#40065;&#26834;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the convex hulls of reachable sets of nonlinear systems with bounded disturbances. Reachable sets play a critical role in control, but remain notoriously challenging to compute, and existing over-approximation tools tend to be conservative or computationally expensive. In this work, we exactly characterize the convex hulls of reachable sets as the convex hulls of solutions of an ordinary differential equation from all possible initial values of the disturbances. This finite-dimensional characterization unlocks a tight estimation algorithm to over-approximate reachable sets that is significantly faster and more accurate than existing methods. We present applications to neural feedback loop analysis and robust model predictive control.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#36816;&#21160;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#21453;&#24212;&#24335;&#31574;&#30053;&#21644;&#35268;&#21010;&#30340;&#20248;&#28857;&#65292;&#22312;&#22810;&#30446;&#26631;&#20915;&#31574;&#38382;&#39064;&#20013;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2210.07890</link><description>&lt;p&gt;
&#20998;&#23618;&#31574;&#30053;&#28151;&#21512;&#20316;&#20026;&#21453;&#24212;&#24335;&#26426;&#22120;&#20154;&#25511;&#21046;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Policy Blending as Inference for Reactive Robot Control. (arXiv:2210.07890v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07890
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#36816;&#21160;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#21453;&#24212;&#24335;&#31574;&#30053;&#21644;&#35268;&#21010;&#30340;&#20248;&#28857;&#65292;&#22312;&#22810;&#30446;&#26631;&#20915;&#31574;&#38382;&#39064;&#20013;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26434;&#20081;&#12289;&#23494;&#38598;&#21644;&#21160;&#24577;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#36816;&#21160;&#29983;&#25104;&#26159;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#65292;&#34987;&#35270;&#20026;&#19968;&#20010;&#22810;&#30446;&#26631;&#20915;&#31574;&#38382;&#39064;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#22312;&#23433;&#20840;&#21644;&#24615;&#33021;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#19968;&#26041;&#38754;&#65292;&#21453;&#24212;&#24335;&#31574;&#30053;&#20445;&#35777;&#20102;&#23545;&#29615;&#22659;&#21464;&#21270;&#30340;&#24555;&#36895;&#21709;&#24212;&#65292;&#20294;&#20197;&#27425;&#20248;&#30340;&#34892;&#20026;&#20316;&#20026;&#20195;&#20215;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22522;&#20110;&#35268;&#21010;&#30340;&#36816;&#21160;&#29983;&#25104;&#25552;&#20379;&#21487;&#34892;&#30340;&#36712;&#36857;&#65292;&#20294;&#39640;&#35745;&#31639;&#25104;&#26412;&#21487;&#33021;&#20250;&#38480;&#21046;&#25511;&#21046;&#39057;&#29575;&#65292;&#20174;&#32780;&#29306;&#29298;&#23433;&#20840;&#24615;&#12290;&#20026;&#20102;&#32467;&#21512;&#21453;&#24212;&#24335;&#31574;&#30053;&#21644;&#35268;&#21010;&#30340;&#20248;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#36816;&#21160;&#29983;&#25104;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#27010;&#29575;&#25512;&#29702;&#26041;&#27861;&#26469;&#27491;&#24335;&#21270;&#20998;&#23618;&#27169;&#22411;&#21644;&#38543;&#26426;&#20248;&#21270;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#23454;&#29616;&#20026;&#38543;&#26426;&#21453;&#24212;&#24335;&#19987;&#23478;&#31574;&#30053;&#30340;&#21152;&#26435;&#20056;&#31215;&#65292;&#20854;&#20013;&#35268;&#21010;&#34987;&#29992;&#20110;&#33258;&#36866;&#24212;&#35745;&#31639;&#20219;&#21153;&#21608;&#26399;&#20869;&#30340;&#26368;&#20248;&#26435;&#37325;&#12290;&#36825;&#31181;&#38543;&#26426;&#20248;&#21270;&#36991;&#20813;&#20102;&#23616;&#37096;&#26368;&#20248;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#34892;&#30340;&#21453;&#24212;&#24335;&#35745;&#21010;&#65292;&#25214;&#21040;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motion generation in cluttered, dense, and dynamic environments is a central topic in robotics, rendered as a multi-objective decision-making problem. Current approaches trade-off between safety and performance. On the one hand, reactive policies guarantee fast response to environmental changes at the risk of suboptimal behavior. On the other hand, planning-based motion generation provides feasible trajectories, but the high computational cost may limit the control frequency and thus safety. To combine the benefits of reactive policies and planning, we propose a hierarchical motion generation method. Moreover, we adopt probabilistic inference methods to formalize the hierarchical model and stochastic optimization. We realize this approach as a weighted product of stochastic, reactive expert policies, where planning is used to adaptively compute the optimal weights over the task horizon. This stochastic optimization avoids local optima and proposes feasible reactive plans that find path
&lt;/p&gt;</description></item></channel></rss>