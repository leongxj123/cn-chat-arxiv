<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25345;&#32493;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#20998;&#25955;&#30340;&#23398;&#20064;&#26550;&#26500;&#65292;&#26469;&#35299;&#20915;&#20132;&#36890;&#36335;&#21475;&#31359;&#36234;&#21644;&#33258;&#20027;&#36187;&#36710;&#31561;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.10996</link><description>&lt;p&gt;
&#19968;&#20010;&#21487;&#25193;&#23637;&#19988;&#21487;&#24182;&#34892;&#21270;&#30340;&#25968;&#23383;&#23402;&#29983;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#21487;&#25345;&#32493;Sim2Real&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
A Scalable and Parallelizable Digital Twin Framework for Sustainable Sim2Real Transition of Multi-Agent Reinforcement Learning Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10996
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25345;&#32493;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#20998;&#25955;&#30340;&#23398;&#20064;&#26550;&#26500;&#65292;&#26469;&#35299;&#20915;&#20132;&#36890;&#36335;&#21475;&#31359;&#36234;&#21644;&#33258;&#20027;&#36187;&#36710;&#31561;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25345;&#32493;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#36873;&#25321;&#24615;&#22320;&#25353;&#38656;&#25193;&#23637;&#24182;&#34892;&#21270;&#35757;&#32451;&#24037;&#20316;&#36127;&#36733;&#65292;&#24182;&#21033;&#29992;&#26368;&#23569;&#30340;&#30828;&#20214;&#36164;&#28304;&#23558;&#35757;&#32451;&#22909;&#30340;&#31574;&#30053;&#20174;&#27169;&#25311;&#29615;&#22659;&#36716;&#31227;&#21040;&#29616;&#23454;&#19990;&#30028;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;AutoDRIVE&#29983;&#24577;&#31995;&#32479;&#20316;&#20026;&#19968;&#20010;&#21551;&#21160;&#25968;&#23383;&#23402;&#29983;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#12289;&#37096;&#32626;&#21644;&#36716;&#31227;&#21512;&#20316;&#21644;&#31454;&#20105;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#20174;&#27169;&#25311;&#29615;&#22659;&#21040;&#29616;&#23454;&#19990;&#30028;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#25506;&#31350;&#20102;4&#21488;&#21512;&#20316;&#36710;&#36742;(Nigel)&#22312;&#21333;&#26234;&#33021;&#20307;&#21644;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#29615;&#22659;&#20013;&#20849;&#20139;&#26377;&#38480;&#29366;&#24577;&#20449;&#24687;&#30340;&#20132;&#21449;&#36941;&#21382;&#38382;&#39064;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#36890;&#29992;&#31574;&#30053;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20010;&#20307;&#31574;&#30053;&#26041;&#27861;&#30740;&#31350;&#20102;2&#36742;&#36710;(F1TENTH)&#30340;&#23545;&#25239;&#24615;&#33258;&#20027;&#36187;&#36710;&#38382;&#39064;&#12290;&#22312;&#20219;&#20309;&#19968;&#32452;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#26550;&#26500;&#65292;&#36825;&#20801;&#35768;&#23545;&#31574;&#30053;&#36827;&#34892;&#26377;&#21147;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10996v1 Announce Type: cross  Abstract: This work presents a sustainable multi-agent deep reinforcement learning framework capable of selectively scaling parallelized training workloads on-demand, and transferring the trained policies from simulation to reality using minimal hardware resources. We introduce AutoDRIVE Ecosystem as an enabling digital twin framework to train, deploy, and transfer cooperative as well as competitive multi-agent reinforcement learning policies from simulation to reality. Particularly, we first investigate an intersection traversal problem of 4 cooperative vehicles (Nigel) that share limited state information in single as well as multi-agent learning settings using a common policy approach. We then investigate an adversarial autonomous racing problem of 2 vehicles (F1TENTH) using an individual policy approach. In either set of experiments, a decentralized learning architecture was adopted, which allowed robust training and testing of the policies 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;SERL&#36719;&#20214;&#22871;&#20214;&#65292;&#23427;&#26159;&#19968;&#20010;&#29992;&#20110;&#26679;&#26412;&#39640;&#25928;&#30340;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#30340;&#24211;&#12290;&#35813;&#24211;&#21253;&#21547;&#20102;&#19968;&#20010;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12289;&#35745;&#31639;&#22870;&#21169;&#21644;&#37325;&#32622;&#29615;&#22659;&#30340;&#26041;&#27861;&#65292;&#39640;&#36136;&#37327;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#65292;&#20197;&#21450;&#19968;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31034;&#20363;&#20219;&#21153;&#12290;&#36825;&#20010;&#36719;&#20214;&#22871;&#20214;&#30340;&#30446;&#26631;&#26159;&#35299;&#20915;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#30340;&#38590;&#20197;&#20351;&#29992;&#21644;&#33719;&#21462;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.16013</link><description>&lt;p&gt;
SERL: &#29992;&#20110;&#26679;&#26412;&#39640;&#25928;&#30340;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#30340;&#36719;&#20214;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
SERL: A Software Suite for Sample-Efficient Robotic Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.16013
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;SERL&#36719;&#20214;&#22871;&#20214;&#65292;&#23427;&#26159;&#19968;&#20010;&#29992;&#20110;&#26679;&#26412;&#39640;&#25928;&#30340;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#30340;&#24211;&#12290;&#35813;&#24211;&#21253;&#21547;&#20102;&#19968;&#20010;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12289;&#35745;&#31639;&#22870;&#21169;&#21644;&#37325;&#32622;&#29615;&#22659;&#30340;&#26041;&#27861;&#65292;&#39640;&#36136;&#37327;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#65292;&#20197;&#21450;&#19968;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31034;&#20363;&#20219;&#21153;&#12290;&#36825;&#20010;&#36719;&#20214;&#22871;&#20214;&#30340;&#30446;&#26631;&#26159;&#35299;&#20915;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#30340;&#38590;&#20197;&#20351;&#29992;&#21644;&#33719;&#21462;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20351;&#24471;&#21487;&#20197;&#22788;&#29702;&#22797;&#26434;&#30340;&#22270;&#20687;&#35266;&#23519;&#65292;&#23454;&#38469;&#35757;&#32451;&#65292;&#24182;&#32467;&#21512;&#36741;&#21161;&#25968;&#25454;&#65288;&#22914;&#31034;&#33539;&#21644;&#20808;&#21069;&#32463;&#39564;&#65289;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#20173;&#28982;&#38590;&#20197;&#20351;&#29992;&#12290;&#20174;&#23454;&#36341;&#32773;&#20013;&#35748;&#35782;&#21040;&#65292;&#36825;&#20123;&#31639;&#27861;&#30340;&#20855;&#20307;&#23454;&#29616;&#32454;&#33410;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#24120;&#24120;&#19982;&#31639;&#27861;&#36873;&#25321;&#21516;&#26679;&#37325;&#35201;&#65288;&#22914;&#26524;&#19981;&#26159;&#26356;&#37325;&#35201;&#65289;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#34987;&#24191;&#27867;&#37319;&#29992;&#20197;&#21450;&#36827;&#19968;&#27493;&#21457;&#23637;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#36825;&#20123;&#26041;&#27861;&#30340;&#30456;&#23545;&#38590;&#20197;&#33719;&#21462;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31934;&#24515;&#23454;&#29616;&#30340;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#19968;&#31181;&#39640;&#25928;&#26679;&#26412;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21450;&#35745;&#31639;&#22870;&#21169;&#21644;&#37325;&#32622;&#29615;&#22659;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#24191;&#27867;&#37319;&#29992;&#30340;&#26426;&#22120;&#20154;&#30340;&#39640;&#36136;&#37327;&#25511;&#21046;&#22120;&#65292;&#20197;&#21450;&#19968;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31034;&#20363;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, significant progress has been made in the field of robotic reinforcement learning (RL), enabling methods that handle complex image observations, train in the real world, and incorporate auxiliary data, such as demonstrations and prior experience. However, despite these advances, robotic RL remains hard to use. It is acknowledged among practitioners that the particular implementation details of these algorithms are often just as important (if not more so) for performance as the choice of algorithm. We posit that a significant challenge to widespread adoption of robotic RL, as well as further development of robotic RL methods, is the comparative inaccessibility of such methods. To address this challenge, we developed a carefully implemented library containing a sample efficient off-policy deep RL method, together with methods for computing rewards and resetting the environment, a high-quality controller for a widely-adopted robot, and a number of challenging example task
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#28436;&#21592;-&#35780;&#35770;&#23478;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#21033;&#29992;&#35299;&#32806;&#30340;&#23545;&#35937;&#34920;&#31034;&#26377;&#25928;&#22320;&#23398;&#20064;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#22635;&#34917;&#20102;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#39640;&#25928;&#19988;&#36866;&#29992;&#20110;&#31163;&#25955;&#25110;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2310.17178</link><description>&lt;p&gt;
&#22270;&#24418;&#21270;&#30340;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;Actor-Critic&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graphical Object-Centric Actor-Critic. (arXiv:2310.17178v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17178
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#28436;&#21592;-&#35780;&#35770;&#23478;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#21033;&#29992;&#35299;&#32806;&#30340;&#23545;&#35937;&#34920;&#31034;&#26377;&#25928;&#22320;&#23398;&#20064;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#22635;&#34917;&#20102;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#39640;&#25928;&#19988;&#36866;&#29992;&#20110;&#31163;&#25955;&#25110;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#26080;&#30417;&#30563;&#30340;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#23398;&#20064;&#21450;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#26368;&#26032;&#30340;&#30740;&#31350;&#25903;&#25345;&#36825;&#26679;&#19968;&#20010;&#35266;&#28857;&#65292;&#21363;&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#37319;&#29992;&#35299;&#32806;&#30340;&#23545;&#35937;&#34920;&#31034;&#33021;&#22815;&#20419;&#36827;&#31574;&#30053;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#26377;&#25928;&#21033;&#29992;&#36825;&#20123;&#34920;&#31034;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21464;&#25442;&#22120;&#32534;&#30721;&#22120;&#26469;&#25552;&#21462;&#23545;&#35937;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;&#29615;&#22659;&#30340;&#21160;&#21147;&#23398;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22635;&#34917;&#20102;&#24320;&#21457;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#21487;&#20197;&#29992;&#20110;&#31163;&#25955;&#25110;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#39640;&#25928;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#19968;&#20010;&#20855;&#26377;&#22797;&#26434;&#35270;&#35273;3D&#26426;&#22120;&#20154;&#29615;&#22659;&#21644;&#19968;&#20010;&#20855;&#26377;&#32452;&#21512;&#32467;&#26500;&#30340;2D&#29615;&#22659;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
There have recently been significant advances in the problem of unsupervised object-centric representation learning and its application to downstream tasks. The latest works support the argument that employing disentangled object representations in image-based object-centric reinforcement learning tasks facilitates policy learning. We propose a novel object-centric reinforcement learning algorithm combining actor-critic and model-based approaches to utilize these representations effectively. In our approach, we use a transformer encoder to extract object representations and graph neural networks to approximate the dynamics of an environment. The proposed method fills a research gap in developing efficient object-centric world models for reinforcement learning settings that can be used for environments with discrete or continuous action spaces. Our algorithm performs better in a visually complex 3D robotic environment and a 2D environment with compositional structure than the state-of-t
&lt;/p&gt;</description></item></channel></rss>