<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#25552;&#20986;&#20102;&#20010;&#24615;&#21270;&#19987;&#23478;&#31034;&#33539;&#30340;&#27010;&#24565;&#65292;&#20026;&#27599;&#20010;&#26234;&#20307;&#25110;&#19981;&#21516;&#31867;&#22411;&#30340;&#26234;&#20307;&#25552;&#20379;&#38024;&#23545;&#20010;&#20154;&#30446;&#26631;&#30340;&#25351;&#23548;&#65292;&#35299;&#20915;&#20102;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#32852;&#21512;&#31034;&#33539;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.08936</link><description>&lt;p&gt;
&#36229;&#36234;&#32852;&#21512;&#31034;&#33539;&#65306;&#20010;&#24615;&#21270;&#19987;&#23478;&#25351;&#23548;&#29992;&#20110;&#39640;&#25928;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Beyond Joint Demonstrations: Personalized Expert Guidance for Efficient Multi-Agent Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08936
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20010;&#24615;&#21270;&#19987;&#23478;&#31034;&#33539;&#30340;&#27010;&#24565;&#65292;&#20026;&#27599;&#20010;&#26234;&#20307;&#25110;&#19981;&#21516;&#31867;&#22411;&#30340;&#26234;&#20307;&#25552;&#20379;&#38024;&#23545;&#20010;&#20154;&#30446;&#26631;&#30340;&#25351;&#23548;&#65292;&#35299;&#20915;&#20102;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#32852;&#21512;&#31034;&#33539;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#38754;&#20020;&#26377;&#25928;&#25506;&#32034;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#32852;&#21512;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#30340;&#22823;&#23567;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#34429;&#28982;&#31034;&#33539;&#24341;&#23548;&#23398;&#20064;&#22312;&#21333;&#26234;&#20307;&#29615;&#22659;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#30410;&#30340;&#65292;&#20294;&#20854;&#30452;&#25509;&#24212;&#29992;&#20110;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#21463;&#21040;&#33719;&#24471;&#32852;&#21512;&#19987;&#23478;&#31034;&#33539;&#30340;&#23454;&#38469;&#22256;&#38590;&#30340;&#38459;&#30861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20010;&#24615;&#21270;&#19987;&#23478;&#31034;&#33539;&#30340;&#26032;&#27010;&#24565;&#65292;&#38024;&#23545;&#27599;&#20010;&#21333;&#20010;&#26234;&#20307;&#25110;&#26356;&#24191;&#27867;&#22320;&#35828;&#65292;&#22242;&#38431;&#20013;&#27599;&#31181;&#31867;&#22411;&#30340;&#26234;&#20307;&#36827;&#34892;&#20102;&#23450;&#21046;&#12290;&#36825;&#20123;&#31034;&#33539;&#20165;&#28041;&#21450;&#21333;&#26234;&#20307;&#34892;&#20026;&#20197;&#21450;&#27599;&#20010;&#26234;&#20307;&#22914;&#20309;&#23454;&#29616;&#20010;&#20154;&#30446;&#26631;&#65292;&#32780;&#19981;&#28041;&#21450;&#20219;&#20309;&#21512;&#20316;&#20803;&#32032;&#65292;&#22240;&#27492;&#30450;&#30446;&#27169;&#20223;&#23427;&#20204;&#19981;&#20250;&#23454;&#29616;&#21512;&#20316;&#30001;&#20110;&#28508;&#22312;&#20914;&#31361;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36873;&#25321;&#24615;&#22320;&#21033;&#29992;&#20010;&#24615;&#21270;&#19987;&#23478;&#31034;&#33539;&#20316;&#20026;&#25351;&#23548;&#65292;&#24182;&#20801;&#35768;&#26234;&#20307;&#23398;&#20064;&#21327;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08936v1 Announce Type: cross  Abstract: Multi-Agent Reinforcement Learning (MARL) algorithms face the challenge of efficient exploration due to the exponential increase in the size of the joint state-action space. While demonstration-guided learning has proven beneficial in single-agent settings, its direct applicability to MARL is hindered by the practical difficulty of obtaining joint expert demonstrations. In this work, we introduce a novel concept of personalized expert demonstrations, tailored for each individual agent or, more broadly, each individual type of agent within a heterogeneous team. These demonstrations solely pertain to single-agent behaviors and how each agent can achieve personal goals without encompassing any cooperative elements, thus naively imitating them will not achieve cooperation due to potential conflicts. To this end, we propose an approach that selectively utilizes personalized expert demonstrations as guidance and allows agents to learn to coo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#31574;&#30053;&#32467;&#26500;&#20808;&#39564;&#30340;&#39640;&#25928;&#26410;&#30693;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#31995;&#32479;&#65292;&#36890;&#36807;&#20869;&#22806;&#29615;&#30340;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#25235;&#21462;&#12289;&#35266;&#23519;&#21644;&#25918;&#32622;&#22312;&#24863;&#30693;&#22122;&#22768;&#20013;&#30340;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.15402</link><description>&lt;p&gt;
&#25235;&#21462;&#12289;&#35266;&#23519;&#21644;&#25918;&#32622;&#65306;&#20855;&#26377;&#31574;&#30053;&#32467;&#26500;&#20808;&#39564;&#30340;&#39640;&#25928;&#26410;&#30693;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;
&lt;/p&gt;
&lt;p&gt;
Grasp, See and Place: Efficient Unknown Object Rearrangement with Policy Structure Prior
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15402
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#31574;&#30053;&#32467;&#26500;&#20808;&#39564;&#30340;&#39640;&#25928;&#26410;&#30693;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#31995;&#32479;&#65292;&#36890;&#36807;&#20869;&#22806;&#29615;&#30340;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#25235;&#21462;&#12289;&#35266;&#23519;&#21644;&#25918;&#32622;&#22312;&#24863;&#30693;&#22122;&#22768;&#20013;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20851;&#27880;&#26410;&#30693;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#20219;&#21153;&#65292;&#21363;&#26426;&#22120;&#20154;&#24212;&#37325;&#26032;&#37197;&#32622;&#29289;&#20307;&#21040;&#30001;RGB-D&#22270;&#20687;&#25351;&#23450;&#30340;&#26399;&#26395;&#30446;&#26631;&#37197;&#32622;&#20013;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#22522;&#20110;&#23398;&#20064;&#30340;&#24863;&#30693;&#27169;&#22359;&#26469;&#25506;&#32034;&#26410;&#30693;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#24863;&#30693;&#35823;&#24046;&#25935;&#24863;&#65292;&#24182;&#19988;&#36739;&#23569;&#20851;&#27880;&#20219;&#21153;&#32423;&#24615;&#33021;&#12290;&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#26377;&#25928;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#24863;&#30693;&#22122;&#22768;&#20013;&#37325;&#26032;&#25490;&#21015;&#26410;&#30693;&#29289;&#20307;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#25581;&#31034;&#20102;&#22122;&#22768;&#24863;&#30693;&#22914;&#20309;&#20197;&#20998;&#31163;&#30340;&#26041;&#24335;&#24433;&#21709;&#25235;&#21462;&#21644;&#25918;&#32622;&#65292;&#24182;&#23637;&#31034;&#36825;&#26679;&#30340;&#20998;&#31163;&#32467;&#26500;&#19981;&#23481;&#26131;&#25913;&#21892;&#20219;&#21153;&#30340;&#26368;&#20248;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#20998;&#31163;&#32467;&#26500;&#20316;&#20026;&#20808;&#39564;&#30340;GSP&#65292;&#19968;&#20010;&#21452;&#29615;&#31995;&#32479;&#12290;&#23545;&#20110;&#20869;&#29615;&#65292;&#25105;&#20204;&#23398;&#20064;&#20027;&#21160;&#35266;&#23519;&#31574;&#30053;&#20197;&#25552;&#39640;&#25918;&#32622;&#30340;&#24863;&#30693;&#12290;&#23545;&#20110;&#22806;&#29615;&#65292;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;&#25235;&#21462;&#31574;&#30053;&#65292;&#24847;&#35782;&#21040;&#29289;&#20307;&#21305;&#37197;&#21644;&#25235;&#21462;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15402v1 Announce Type: cross  Abstract: We focus on the task of unknown object rearrangement, where a robot is supposed to re-configure the objects into a desired goal configuration specified by an RGB-D image. Recent works explore unknown object rearrangement systems by incorporating learning-based perception modules. However, they are sensitive to perception error, and pay less attention to task-level performance. In this paper, we aim to develop an effective system for unknown object rearrangement amidst perception noise. We theoretically reveal the noisy perception impacts grasp and place in a decoupled way, and show such a decoupled structure is non-trivial to improve task optimality. We propose GSP, a dual-loop system with the decoupled structure as prior. For the inner loop, we learn an active seeing policy for self-confident object matching to improve the perception of place. For the outer loop, we learn a grasp policy aware of object matching and grasp capability gu
&lt;/p&gt;</description></item></channel></rss>