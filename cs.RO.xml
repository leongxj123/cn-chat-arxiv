<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#23433;&#20840;&#23884;&#20837;&#24335;MDP&#20013;&#32467;&#21512;&#36712;&#36857;&#20248;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#23433;&#20840;&#32422;&#26463;&#23884;&#20837;&#21160;&#20316;&#31354;&#38388;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22312;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#36981;&#23432;&#23433;&#20840;&#32422;&#26463;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#24182;&#22312;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.06903</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#36712;&#36857;&#20248;&#21270;&#30340;&#23433;&#20840;&#23884;&#20837;&#24335;MDP&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning in a Safety-Embedded MDP with Trajectory Optimization. (arXiv:2310.06903v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#23433;&#20840;&#23884;&#20837;&#24335;MDP&#20013;&#32467;&#21512;&#36712;&#36857;&#20248;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#23433;&#20840;&#32422;&#26463;&#23884;&#20837;&#21160;&#20316;&#31354;&#38388;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22312;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#36981;&#23432;&#23433;&#20840;&#32422;&#26463;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#24182;&#22312;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#22312;&#23558;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24212;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#36215;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#35299;&#20915;&#20102;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#36981;&#23432;&#23433;&#20840;&#32422;&#26463;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#36712;&#36857;&#20248;&#21270;&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#22320;&#31649;&#29702;&#36825;&#31181;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#23433;&#20840;&#32422;&#26463;&#23884;&#20837;&#21040;&#20462;&#25913;&#21518;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#30340;&#21160;&#20316;&#31354;&#38388;&#20013;&#12290;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#36890;&#36807;&#36712;&#36857;&#20248;&#21270;&#22120;&#20135;&#29983;&#19968;&#31995;&#21015;&#34892;&#21160;&#65292;&#36825;&#20123;&#34892;&#21160;&#36716;&#21270;&#20026;&#23433;&#20840;&#36712;&#36857;&#65292;&#20174;&#32780;&#26377;&#25928;&#30830;&#20445;&#23433;&#20840;&#24182;&#25552;&#39640;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#22312;&#25361;&#25112;&#24615;&#30340;Safety Gym&#20219;&#21153;&#30340;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#26356;&#39640;&#30340;&#22870;&#21169;&#21644;&#20960;&#20046;&#38646;&#30340;&#23433;&#20840;&#36829;&#35268;&#12290;&#35813;&#26041;&#27861;&#22312;&#19968;&#20010;&#30495;&#23454;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#24615;&#24471;&#21040;&#20102;&#35777;&#26126;&#65292;&#35813;&#20219;&#21153;&#28041;&#21450;&#25512;&#21160;&#31665;&#23376;&#31359;&#36807;&#38556;&#30861;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safe Reinforcement Learning (RL) plays an important role in applying RL algorithms to safety-critical real-world applications, addressing the trade-off between maximizing rewards and adhering to safety constraints. This work introduces a novel approach that combines RL with trajectory optimization to manage this trade-off effectively. Our approach embeds safety constraints within the action space of a modified Markov Decision Process (MDP). The RL agent produces a sequence of actions that are transformed into safe trajectories by a trajectory optimizer, thereby effectively ensuring safety and increasing training stability. This novel approach excels in its performance on challenging Safety Gym tasks, achieving significantly higher rewards and near-zero safety violations during inference. The method's real-world applicability is demonstrated through a safe and effective deployment in a real robot task of box-pushing around obstacles.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#32452;&#21512;&#24335;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#20102;&#22235;&#20010;&#21253;&#21547;256&#20010;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#12290;&#27599;&#20010;&#25968;&#25454;&#38598;&#30001;&#24615;&#33021;&#19981;&#21516;&#30340;&#20195;&#29702;&#37319;&#38598;&#65292;&#21253;&#21547;2.56&#20159;&#26465;&#36716;&#25442;&#35760;&#24405;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;</title><link>http://arxiv.org/abs/2307.07091</link><description>&lt;p&gt;
&#31163;&#32447;&#32452;&#21512;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Robotic Manipulation Datasets for Offline Compositional Reinforcement Learning. (arXiv:2307.07091v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#32452;&#21512;&#24335;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#20102;&#22235;&#20010;&#21253;&#21547;256&#20010;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#12290;&#27599;&#20010;&#25968;&#25454;&#38598;&#30001;&#24615;&#33021;&#19981;&#21516;&#30340;&#20195;&#29702;&#37319;&#38598;&#65292;&#21253;&#21547;2.56&#20159;&#26465;&#36716;&#25442;&#35760;&#24405;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#21487;&#20197;&#35753;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#36991;&#20813;&#26114;&#36149;&#30340;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#30340;&#37325;&#22797;&#12290;&#20026;&#20102;&#25512;&#21160;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#29983;&#25104;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#33267;&#20851;&#37325;&#35201;&#12290;&#32452;&#21512;&#24335;&#24378;&#21270;&#23398;&#20064;&#23545;&#20110;&#29983;&#25104;&#36825;&#26679;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#23588;&#20026;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;1&#65289;&#23427;&#20801;&#35768;&#20174;&#23569;&#37327;&#32452;&#20214;&#20013;&#21019;&#24314;&#22810;&#20010;&#20219;&#21153;&#65292;2&#65289;&#20219;&#21153;&#32467;&#26500;&#21487;&#20197;&#35753;&#35757;&#32451;&#22909;&#30340;&#20195;&#29702;&#36890;&#36807;&#32452;&#21512;&#30456;&#20851;&#30340;&#23398;&#20064;&#32452;&#20214;&#26469;&#35299;&#20915;&#26032;&#20219;&#21153;&#65292;&#24182;&#19988;3&#65289;&#32452;&#21512;&#32500;&#24230;&#25552;&#20379;&#20102;&#20219;&#21153;&#20851;&#32852;&#24615;&#30340;&#27010;&#24565;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#22235;&#20010;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#27169;&#25311;&#26426;&#22120;&#20154;&#25805;&#20316;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20351;&#29992;&#20102;&#26469;&#33258;CompoSuite [Mendez et al., 2022a]&#30340;256&#20010;&#20219;&#21153;&#12290;&#27599;&#20010;&#25968;&#25454;&#38598;&#26159;&#20174;&#19968;&#20010;&#20855;&#26377;&#19981;&#21516;&#24615;&#33021;&#31561;&#32423;&#30340;&#20195;&#29702;&#25910;&#38598;&#30340;&#65292;&#21253;&#21547;&#20102;2.56&#20159;&#26465;&#36716;&#25442;&#35760;&#24405;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35757;&#32451;&#21644;&#35780;&#20272;&#35774;&#32622;&#65292;&#20197;&#35780;&#20272;&#20195;&#29702;&#23398;&#20064;&#32452;&#21512;&#20219;&#21153;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#27599;&#20010;&#35774;&#32622;&#19978;&#36827;&#34892;&#30340;&#22522;&#20934;&#23454;&#39564;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) is a promising direction that allows RL agents to pre-train on large datasets, avoiding the recurrence of expensive data collection. To advance the field, it is crucial to generate large-scale datasets. Compositional RL is particularly appealing for generating such large datasets, since 1) it permits creating many tasks from few components, 2) the task structure may enable trained agents to solve new tasks by combining relevant learned components, and 3) the compositional dimensions provide a notion of task relatedness. This paper provides four offline RL datasets for simulated robotic manipulation created using the 256 tasks from CompoSuite [Mendez et al., 2022a]. Each dataset is collected from an agent with a different degree of performance, and consists of 256 million transitions. We provide training and evaluation settings for assessing an agent's ability to learn compositional task policies. Our benchmarking experiments on each setting show that
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HACMan&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#28857;&#20113;&#35266;&#23519;&#36827;&#34892;6D&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#30340;&#29289;&#20307;&#25805;&#32437;&#12290;HACMan&#37325;&#28857;&#20851;&#27880;&#29289;&#20307;&#20013;&#24515;&#21160;&#20316;&#34920;&#31034;&#65292;&#23427;&#21253;&#25324;&#20174;&#29289;&#20307;&#28857;&#20113;&#20013;&#36873;&#25321;&#25509;&#35302;&#20301;&#32622;&#21644;&#19968;&#32452;&#25551;&#36848;&#26426;&#22120;&#20154;&#22312;&#25509;&#35302;&#21518;&#22914;&#20309;&#31227;&#21160;&#30340;&#36816;&#21160;&#21442;&#25968;&#12290;&#22312;&#23454;&#38469;&#27979;&#35797;&#20013;&#65292;HACMan&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.03942</link><description>&lt;p&gt;
&#23398;&#20064;6D&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#30340;&#28151;&#21512;&#28436;&#21592;-&#35780;&#35770;&#21592;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
Learning Hybrid Actor-Critic Maps for 6D Non-Prehensile Manipulation. (arXiv:2305.03942v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03942
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HACMan&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#28857;&#20113;&#35266;&#23519;&#36827;&#34892;6D&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#30340;&#29289;&#20307;&#25805;&#32437;&#12290;HACMan&#37325;&#28857;&#20851;&#27880;&#29289;&#20307;&#20013;&#24515;&#21160;&#20316;&#34920;&#31034;&#65292;&#23427;&#21253;&#25324;&#20174;&#29289;&#20307;&#28857;&#20113;&#20013;&#36873;&#25321;&#25509;&#35302;&#20301;&#32622;&#21644;&#19968;&#32452;&#25551;&#36848;&#26426;&#22120;&#20154;&#22312;&#25509;&#35302;&#21518;&#22914;&#20309;&#31227;&#21160;&#30340;&#36816;&#21160;&#21442;&#25968;&#12290;&#22312;&#23454;&#38469;&#27979;&#35797;&#20013;&#65292;HACMan&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#30340;&#28789;&#24039;&#24615;&#20013;&#65292;&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#26159;&#25805;&#20316;&#29289;&#20307;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#38750;&#25235;&#21462;&#24335;&#25805;&#32437;&#21487;&#20197;&#20351;&#19982;&#29289;&#20307;&#30340;&#20132;&#20114;&#26356;&#21152;&#22797;&#26434;&#65292;&#20294;&#20063;&#22312;&#25512;&#29702;&#20132;&#20114;&#26041;&#38754;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;HACMan&#30340;&#28151;&#21512;&#28436;&#21592;&#35780;&#35770;&#21592;&#22320;&#22270;&#65292;&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#28857;&#20113;&#35266;&#23519;&#30340;6D&#38750;&#25235;&#21462;&#24335;&#29289;&#20307;&#25805;&#20316;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;HACMan&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#25277;&#35937;&#21644;&#31354;&#38388;&#22522;&#30784;&#30340;&#29289;&#20307;&#20013;&#24515;&#21160;&#20316;&#34920;&#31034;&#65292;&#35813;&#34920;&#31034;&#21253;&#25324;&#20174;&#29289;&#20307;&#28857;&#20113;&#20013;&#36873;&#25321;&#25509;&#35302;&#20301;&#32622;&#21644;&#19968;&#32452;&#25551;&#36848;&#26426;&#22120;&#20154;&#22312;&#25509;&#35302;&#21518;&#22914;&#20309;&#31227;&#21160;&#30340;&#36816;&#21160;&#21442;&#25968;&#12290;&#25105;&#20204;&#20462;&#25913;&#20102;&#19968;&#20010;&#29616;&#26377;&#30340;&#31163;&#32447;&#31574;&#30053;RL&#31639;&#27861;&#65292;&#20197;&#22312;&#36825;&#31181;&#28151;&#21512;&#30340;&#31163;&#25955;-&#36830;&#32493;&#21160;&#20316;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#21644;&#29616;&#23454;&#19990;&#30028;&#20013;&#23545;HACMan&#36827;&#34892;&#20102;6D&#29289;&#20307;&#23039;&#24577;&#23545;&#40784;&#20219;&#21153;&#30340;&#35780;&#20272;&#12290;&#22312;&#26368;&#38590;&#30340;&#20219;&#21153;&#29256;&#26412;&#20013;&#65292;&#36890;&#36807;&#38543;&#26426;&#21021;&#22987;&#21270;&#29289;&#20307;&#21644;&#26426;&#22120;&#20154;&#37197;&#32622;&#65292;HACMan&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Manipulating objects without grasping them is an essential component of human dexterity, referred to as non-prehensile manipulation. Non-prehensile manipulation may enable more complex interactions with the objects, but also presents challenges in reasoning about the interactions. In this work, we introduce Hybrid Actor-Critic Maps for Manipulation (HACMan), a reinforcement learning approach for 6D non-prehensile manipulation of objects using point cloud observations. HACMan proposes a temporally-abstracted and spatially-grounded object-centric action representation that consists of selecting a contact location from the object point cloud and a set of motion parameters describing how the robot will move after making contact. We modify an existing off-policy RL algorithm to learn in this hybrid discrete-continuous action representation. We evaluate HACMan on a 6D object pose alignment task in both simulation and in the real world. On the hardest version of our task, with randomized init
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#24067;&#24335;&#34920;&#31034;&#36827;&#34892;&#21463;&#38480;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21487;&#20449;&#22235;&#26059;&#32764;&#26080;&#20154;&#26426;&#30340;&#36319;&#36394;&#25511;&#21046;&#12290;&#36890;&#36807;&#38598;&#25104;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#24178;&#25200;&#20272;&#35745;&#22120;&#21644;&#38543;&#26426;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#27668;&#21160;&#25928;&#24212;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#23454;&#29616;&#26368;&#20248;&#30340;&#20840;&#23616;&#25910;&#25947;&#36895;&#29575;&#21644;&#19968;&#23450;&#30340;&#20122;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2302.11694</link><description>&lt;p&gt;
&#21463;&#38480;&#24378;&#21270;&#23398;&#20064;&#22312;&#21487;&#20449;&#22235;&#26059;&#32764;&#26080;&#20154;&#26426;&#36319;&#36394;&#25511;&#21046;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Constrained Reinforcement Learning using Distributional Representation for Trustworthy Quadrotor UAV Tracking Control. (arXiv:2302.11694v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11694
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#24067;&#24335;&#34920;&#31034;&#36827;&#34892;&#21463;&#38480;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21487;&#20449;&#22235;&#26059;&#32764;&#26080;&#20154;&#26426;&#30340;&#36319;&#36394;&#25511;&#21046;&#12290;&#36890;&#36807;&#38598;&#25104;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#24178;&#25200;&#20272;&#35745;&#22120;&#21644;&#38543;&#26426;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#27668;&#21160;&#25928;&#24212;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#23454;&#29616;&#26368;&#20248;&#30340;&#20840;&#23616;&#25910;&#25947;&#36895;&#29575;&#21644;&#19968;&#23450;&#30340;&#20122;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22797;&#26434;&#30340;&#21160;&#24577;&#29615;&#22659;&#20013;&#65292;&#21516;&#26102;&#23454;&#29616;&#22235;&#26059;&#32764;&#26080;&#20154;&#26426;&#30340;&#20934;&#30830;&#21644;&#21487;&#38752;&#30340;&#36319;&#36394;&#25511;&#21046;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#30001;&#20110;&#26469;&#33258;&#27668;&#21160;&#21147;&#30340;&#38459;&#21147;&#21644;&#21147;&#30697;&#21464;&#21270;&#26159;&#28151;&#27788;&#30340;&#65292;&#24182;&#19988;&#38590;&#20197;&#31934;&#30830;&#35782;&#21035;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22235;&#26059;&#32764;&#36319;&#36394;&#31995;&#32479;&#23558;&#20854;&#35270;&#20026;&#20256;&#32479;&#25511;&#21046;&#26041;&#27861;&#20013;&#30340;&#31616;&#21333;&#8220;&#24178;&#25200;&#8221;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#21487;&#35299;&#37322;&#30340;&#36712;&#36857;&#36319;&#36394;&#22120;&#65292;&#23558;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#24178;&#25200;&#20272;&#35745;&#22120;&#19982;&#38543;&#26426;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#65288;SMPC&#65289;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#26410;&#30693;&#30340;&#27668;&#21160;&#25928;&#24212;&#12290;&#25152;&#25552;&#20986;&#30340;&#20272;&#35745;&#22120;&#8220;&#21463;&#38480;&#20998;&#24067;&#24335;&#24378;&#21270;&#24178;&#25200;&#20272;&#35745;&#22120;&#8221;&#65288;ConsDRED&#65289;&#20934;&#30830;&#22320;&#35782;&#21035;&#30495;&#23454;&#27668;&#21160;&#25928;&#24212;&#19982;&#20272;&#35745;&#20540;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#37319;&#29992;&#31616;&#21270;&#20223;&#23556;&#24178;&#25200;&#21453;&#39304;&#36827;&#34892;&#25511;&#21046;&#21442;&#25968;&#21270;&#65292;&#20197;&#20445;&#35777;&#20984;&#24615;&#65292;&#28982;&#21518;&#23558;&#20854;&#19982;SMPC&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20445;&#35777;ConsDRED&#33267;&#23569;&#23454;&#29616;&#26368;&#20248;&#30340;&#20840;&#23616;&#25910;&#25947;&#36895;&#29575;&#21644;&#19968;&#23450;&#30340;&#20122;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simultaneously accurate and reliable tracking control for quadrotors in complex dynamic environments is challenging. As aerodynamics derived from drag forces and moment variations are chaotic and difficult to precisely identify, most current quadrotor tracking systems treat them as simple `disturbances' in conventional control approaches. We propose a novel, interpretable trajectory tracker integrating a Distributional Reinforcement Learning disturbance estimator for unknown aerodynamic effects with a Stochastic Model Predictive Controller (SMPC). The proposed estimator `Constrained Distributional Reinforced disturbance estimator' (ConsDRED) accurately identifies uncertainties between true and estimated values of aerodynamic effects. Simplified Affine Disturbance Feedback is used for control parameterization to guarantee convexity, which we then integrate with a SMPC. We theoretically guarantee that ConsDRED achieves at least an optimal global convergence rate and a certain sublinear r
&lt;/p&gt;</description></item></channel></rss>