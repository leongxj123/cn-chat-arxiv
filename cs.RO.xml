<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#25991;&#26412;&#26144;&#23556;&#21644;&#23548;&#33322;&#33021;&#21147;&#30340;MANGO&#22522;&#20934;&#65292;&#21457;&#29616;&#21363;&#20351;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22909;&#30340;&#35821;&#35328;&#27169;&#22411;GPT-4&#22312;&#22238;&#31572;&#28041;&#21450;&#26144;&#23556;&#21644;&#23548;&#33322;&#30340;&#38382;&#39064;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;</title><link>https://arxiv.org/abs/2403.19913</link><description>&lt;p&gt;
MANGO&#65306;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26144;&#23556;&#21644;&#23548;&#33322;&#33021;&#21147;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19913
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#25991;&#26412;&#26144;&#23556;&#21644;&#23548;&#33322;&#33021;&#21147;&#30340;MANGO&#22522;&#20934;&#65292;&#21457;&#29616;&#21363;&#20351;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22909;&#30340;&#35821;&#35328;&#27169;&#22411;GPT-4&#22312;&#22238;&#31572;&#28041;&#21450;&#26144;&#23556;&#21644;&#23548;&#33322;&#30340;&#38382;&#39064;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;ChatGPT&#21644;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26368;&#36817;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MANGO&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#23427;&#20204;&#25191;&#34892;&#22522;&#20110;&#25991;&#26412;&#26144;&#23556;&#21644;&#23548;&#33322;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#21253;&#25324;&#26469;&#33258;&#19968;&#22871;&#25991;&#26412;&#28216;&#25103;&#30340;53&#20010;&#36855;&#23467;&#65306;&#27599;&#20010;&#36855;&#23467;&#37117;&#19982;&#19968;&#20010;&#28216;&#35272;&#35828;&#26126;&#37197;&#23545;&#65292;&#20854;&#20013;&#21253;&#21547;&#27599;&#20010;&#20301;&#32622;&#30340;&#35775;&#38382;&#20294;&#19981;&#28085;&#30422;&#25152;&#26377;&#21487;&#33021;&#30340;&#36335;&#24452;&#12290;&#20219;&#21153;&#26159;&#38382;&#31572;&#65306;&#23545;&#20110;&#27599;&#20010;&#36855;&#23467;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35835;&#21462;&#28216;&#35272;&#35828;&#26126;&#24182;&#22238;&#31572;&#25968;&#30334;&#20010;&#26144;&#23556;&#21644;&#23548;&#33322;&#38382;&#39064;&#65292;&#20363;&#22914;&#8220;&#20320;&#24212;&#35813;&#20174;&#25151;&#23376;&#35199;&#37096;&#22914;&#20309;&#21435;&#38401;&#27004;&#65311;&#8221;&#21644;&#8220;&#22914;&#26524;&#25105;&#20204;&#20174;&#22320;&#19979;&#23460;&#21521;&#21271;&#21644;&#19996;&#36208;&#65292;&#25105;&#20204;&#20250;&#22312;&#21738;&#37324;&#65311;&#8221;&#12290;&#23613;&#31649;&#36825;&#20123;&#38382;&#39064;&#23545;&#20154;&#31867;&#26469;&#35828;&#24456;&#23481;&#26131;&#65292;&#20294;&#20107;&#23454;&#35777;&#26126;&#65292;&#36804;&#20170;&#20026;&#27490;&#26368;&#22909;&#30340;&#35821;&#35328;&#27169;&#22411;GPT-4&#29978;&#33267;&#22312;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#24378;&#22823;&#30340;&#26144;&#23556;&#21644;&#23548;&#33322;&#33021;&#21147;&#23558;&#26377;&#21033;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19913v1 Announce Type: cross  Abstract: Large language models such as ChatGPT and GPT-4 have recently achieved astonishing performance on a variety of natural language processing tasks. In this paper, we propose MANGO, a benchmark to evaluate their capabilities to perform text-based mapping and navigation. Our benchmark includes 53 mazes taken from a suite of textgames: each maze is paired with a walkthrough that visits every location but does not cover all possible paths. The task is question-answering: for each maze, a large language model reads the walkthrough and answers hundreds of mapping and navigation questions such as "How should you go to Attic from West of House?" and "Where are we if we go north and east from Cellar?". Although these questions are easy to humans, it turns out that even GPT-4, the best-to-date language model, performs poorly at answering them. Further, our experiments suggest that a strong mapping and navigation ability would benefit large languag
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#24037;&#24341;&#23548;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65288;GuDA&#65289;&#29992;&#20110;&#25552;&#39640;&#28436;&#31034;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.18247</link><description>&lt;p&gt;
&#20026;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#25552;&#20379;&#25351;&#23548;&#24615;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Guided Data Augmentation for Offline Reinforcement Learning and Imitation Learning. (arXiv:2310.18247v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18247
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#24037;&#24341;&#23548;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65288;GuDA&#65289;&#29992;&#20110;&#25552;&#39640;&#28436;&#31034;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28436;&#31034;&#23398;&#20064;&#26159;&#19968;&#31181;&#20351;&#29992;&#19987;&#23478;&#28436;&#31034;&#26469;&#23398;&#20064;&#26426;&#22120;&#20154;&#25511;&#21046;&#31574;&#30053;&#30340;&#27969;&#34892;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#19987;&#23478;&#32423;&#28436;&#31034;&#30340;&#38590;&#24230;&#38480;&#21046;&#20102;&#28436;&#31034;&#23398;&#20064;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65306;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#25910;&#38598;&#36890;&#24120;&#24456;&#26114;&#36149;&#65292;&#24182;&#19988;&#28436;&#31034;&#30340;&#36136;&#37327;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#28436;&#31034;&#32773;&#30340;&#33021;&#21147;&#21644;&#23433;&#20840;&#38382;&#39064;&#12290;&#19968;&#20123;&#24037;&#20316;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#26469;&#24265;&#20215;&#29983;&#25104;&#39069;&#22806;&#30340;&#28436;&#31034;&#25968;&#25454;&#65292;&#20294;&#22823;&#22810;&#25968;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#20197;&#38543;&#26426;&#26041;&#24335;&#29983;&#25104;&#22686;&#24378;&#25968;&#25454;&#65292;&#26368;&#32456;&#20135;&#29983;&#39640;&#24230;&#27425;&#20248;&#30340;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#24037;&#24341;&#23548;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65288;GuDA&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22686;&#24378;&#25968;&#25454;&#12290;GuDA&#30340;&#20851;&#38190;&#27934;&#35265;&#26159;&#65292;&#34429;&#28982;&#28436;&#31034;&#21160;&#20316;&#24207;&#21015;&#21487;&#33021;&#24456;&#38590;&#23637;&#31034;&#20135;&#29983;&#19987;&#23478;&#25968;&#25454;&#25152;&#38656;&#30340;&#21160;&#20316;&#24207;&#21015;&#65292;&#20294;&#29992;&#25143;&#32463;&#24120;&#21487;&#20197;&#36731;&#26494;&#22320;&#36776;&#21035;&#20986;&#22686;&#24378;&#36712;&#36857;&#27573;&#34920;&#31034;&#30340;&#20219;&#21153;&#36827;&#23637;&#12290;&#22240;&#27492;&#65292;&#29992;&#25143;&#21487;&#20197;&#26045;&#21152;&#19968;&#31995;&#21015;s
&lt;/p&gt;
&lt;p&gt;
Learning from demonstration (LfD) is a popular technique that uses expert demonstrations to learn robot control policies. However, the difficulty in acquiring expert-quality demonstrations limits the applicability of LfD methods: real-world data collection is often costly, and the quality of the demonstrations depends greatly on the demonstrator's abilities and safety concerns. A number of works have leveraged data augmentation (DA) to inexpensively generate additional demonstration data, but most DA works generate augmented data in a random fashion and ultimately produce highly suboptimal data. In this work, we propose Guided Data Augmentation (GuDA), a human-guided DA framework that generates expert-quality augmented data. The key insight of GuDA is that while it may be difficult to demonstrate the sequence of actions required to produce expert data, a user can often easily identify when an augmented trajectory segment represents task progress. Thus, the user can impose a series of s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21516;&#29366;&#24577;&#34920;&#31034;&#23545;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#26426;&#22120;&#20154;&#25235;&#21462;&#20219;&#21153;&#19978;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#20351;&#29992;&#25968;&#23383;&#29366;&#24577;&#30340;&#20195;&#29702;&#33021;&#22815;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#25104;&#21151;&#35299;&#20915;&#38382;&#39064;&#65292;&#24182;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#23454;&#29616;&#20102;&#23398;&#20064;&#31574;&#30053;&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11984</link><description>&lt;p&gt;
&#34920;&#31034;&#25277;&#35937;&#20316;&#20026;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#28608;&#21169;&#65306;&#22522;&#20110;&#26426;&#22120;&#20154;&#25235;&#21462;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Representation Abstractions as Incentives for Reinforcement Learning Agents: A Robotic Grasping Case Study. (arXiv:2309.11984v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21516;&#29366;&#24577;&#34920;&#31034;&#23545;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#26426;&#22120;&#20154;&#25235;&#21462;&#20219;&#21153;&#19978;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#20351;&#29992;&#25968;&#23383;&#29366;&#24577;&#30340;&#20195;&#29702;&#33021;&#22815;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#25104;&#21151;&#35299;&#20915;&#38382;&#39064;&#65292;&#24182;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#23454;&#29616;&#20102;&#23398;&#20064;&#31574;&#30053;&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#19968;&#20010;&#36866;&#24403;&#30340;&#29615;&#22659;&#34920;&#31034;&#23545;&#20110;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#20915;&#31574;&#36807;&#31243;&#24182;&#19981;&#24635;&#26159;&#31616;&#21333;&#30340;&#12290;&#29366;&#24577;&#34920;&#31034;&#24212;&#35813;&#36275;&#22815;&#21253;&#23481;&#65292;&#20197;&#20415;&#35753;&#20195;&#29702;&#33021;&#22815;&#20449;&#24687;&#22320;&#20915;&#23450;&#20854;&#34892;&#21160;&#65292;&#24182;&#19988;&#36275;&#22815;&#32039;&#20945;&#65292;&#20197;&#25552;&#39640;&#31574;&#30053;&#35757;&#32451;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21516;&#29366;&#24577;&#34920;&#31034;&#23545;&#20195;&#29702;&#22312;&#29305;&#23450;&#26426;&#22120;&#20154;&#20219;&#21153;&#65288;&#23545;&#31216;&#21644;&#24179;&#38754;&#29289;&#20307;&#25235;&#21462;&#65289;&#19978;&#35299;&#20915;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#20174;&#20855;&#26377;&#23436;&#25972;&#31995;&#32479;&#30693;&#35782;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#24320;&#22987;&#65292;&#36890;&#36807;&#25163;&#24037;&#25968;&#23383;&#34920;&#31034;&#21040;&#22522;&#20110;&#22270;&#20687;&#30340;&#34920;&#31034;&#65292;&#36880;&#28176;&#20943;&#23569;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#30340;&#24341;&#20837;&#37327;&#65292;&#23450;&#20041;&#20102;&#19968;&#31995;&#21015;&#29366;&#24577;&#34920;&#31034;&#25277;&#35937;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#27599;&#31181;&#34920;&#31034;&#23545;&#20195;&#29702;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#35299;&#20915;&#20219;&#21153;&#20197;&#21450;&#23398;&#21040;&#30340;&#31574;&#30053;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#30340;&#21487;&#36716;&#31227;&#24615;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#25968;&#23383;&#29366;&#24577;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#33021;&#22815;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Choosing an appropriate representation of the environment for the underlying decision-making process of the \gls{RL} agent is not always straightforward. The state representation should be inclusive enough to allow the agent to informatively decide on its actions and compact enough to increase sample efficiency for policy training. Given this outlook, this work examines the effect of various state representations in incentivizing the agent to solve a specific robotic task: antipodal and planar object grasping. A continuum of state representation abstractions is defined, starting from a model-based approach with complete system knowledge, through hand-crafted numerical, to image-based representations with decreasing level of induced task-specific knowledge. We examine the effects of each representation in the ability of the agent to solve the task in simulation and the transferability of the learned policy to the real robot. The results show that RL agents using numerical states can per
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#33268;&#26102;&#38388;&#36923;&#36753;&#35268;&#21010;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#20010;&#39640;&#32423;&#23376;&#20219;&#21153;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#38382;&#39064;&#12290;&#20854;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#22914;&#20309;&#20197;&#27491;&#30830;&#24615;&#30340;&#35282;&#24230;&#25512;&#29702;&#26426;&#22120;&#20154;&#35745;&#21010;&#19982;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#36923;&#36753;&#20219;&#21153;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.10092</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#33268;&#26102;&#38388;&#36923;&#36753;&#35268;&#21010;&#65306;&#30693;&#36947;&#20309;&#26102;&#20570;&#20160;&#20040;&#21644;&#20309;&#26102;&#23547;&#27714;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conformal Temporal Logic Planning using Large Language Models: Knowing When to Do What and When to Ask for Help. (arXiv:2309.10092v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#33268;&#26102;&#38388;&#36923;&#36753;&#35268;&#21010;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#20010;&#39640;&#32423;&#23376;&#20219;&#21153;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#38382;&#39064;&#12290;&#20854;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#22914;&#20309;&#20197;&#27491;&#30830;&#24615;&#30340;&#35282;&#24230;&#25512;&#29702;&#26426;&#22120;&#20154;&#35745;&#21010;&#19982;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#36923;&#36753;&#20219;&#21153;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#19968;&#20010;&#26032;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#38382;&#39064;&#65292;&#20219;&#21153;&#26159;&#20197;&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#34920;&#36798;&#24182;&#20197;&#26102;&#38388;&#21644;&#36923;&#36753;&#39034;&#24207;&#23436;&#25104;&#22810;&#20010;&#39640;&#32423;&#23376;&#20219;&#21153;&#12290;&#20026;&#20102;&#27491;&#24335;&#23450;&#20041;&#36825;&#26679;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;NL&#30340;&#21407;&#23376;&#35859;&#35789;&#22312;LTL&#19978;&#23450;&#20041;&#20102;&#27169;&#22411;&#12290;&#36825;&#19982;&#30456;&#20851;&#30340;&#35268;&#21010;&#26041;&#27861;&#24418;&#25104;&#23545;&#27604;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#21407;&#23376;&#35859;&#35789;&#19978;&#23450;&#20041;&#20102;&#25429;&#25417;&#25152;&#38656;&#20302;&#32423;&#31995;&#32479;&#37197;&#32622;&#30340;LTL&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#26426;&#22120;&#20154;&#35745;&#21010;&#65292;&#28385;&#36275;&#22522;&#20110;NL&#30340;&#21407;&#23376;&#21629;&#39064;&#23450;&#20041;&#30340;LTL&#20219;&#21153;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#20986;&#29616;&#30340;&#19968;&#20010;&#26032;&#30340;&#25216;&#26415;&#25361;&#25112;&#22312;&#20110;&#25512;&#29702;&#26426;&#22120;&#20154;&#35745;&#21010;&#30340;&#27491;&#30830;&#24615;&#19982;&#36825;&#20123;LTL&#32534;&#30721;&#30340;&#20219;&#21153;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HERACLEs&#65292;&#19968;&#20010;&#20998;&#23618;&#19968;&#33268;&#30340;&#33258;&#28982;&#35821;&#35328;&#35268;&#21010;&#22120;&#65292;&#23427;&#20381;&#36182;&#20110;&#29616;&#26377;&#24037;&#20855;&#30340;&#26032;&#22411;&#25972;&#21512;&#65292;&#21253;&#25324;&#65288;i&#65289;&#33258;&#21160;&#26426;&#29702;&#35770;&#65292;&#20197;&#30830;&#23450;&#26426;&#22120;&#20154;&#24212;&#35813;&#23436;&#25104;&#30340;NL&#25351;&#23450;&#30340;&#23376;&#20219;&#21153;&#20197;&#25512;&#36827;&#20219;&#21153;&#36827;&#23637;&#65307;
&lt;/p&gt;
&lt;p&gt;
This paper addresses a new motion planning problem for mobile robots tasked with accomplishing multiple high-level sub-tasks, expressed using natural language (NL), in a temporal and logical order. To formally define such missions, we leverage LTL defined over NL-based atomic predicates modeling the considered NL-based sub-tasks. This is contrast to related planning approaches that define LTL tasks over atomic predicates capturing desired low-level system configurations. Our goal is to design robot plans that satisfy LTL tasks defined over NL-based atomic propositions. A novel technical challenge arising in this setup lies in reasoning about correctness of a robot plan with respect to such LTL-encoded tasks. To address this problem, we propose HERACLEs, a hierarchical conformal natural language planner, that relies on a novel integration of existing tools that include (i) automata theory to determine the NL-specified sub-task the robot should accomplish next to make mission progress; (
&lt;/p&gt;</description></item></channel></rss>