<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#25511;&#21046;&#29702;&#35770;&#20013;&#23545;&#31216;&#25216;&#26415;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#36890;&#36807;&#21033;&#29992;&#21160;&#24577;&#23545;&#31216;&#24615;&#23398;&#20064;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#32780;&#19981;&#35201;&#27714;&#22870;&#21169;&#20855;&#26377;&#30456;&#21516;&#30340;&#23545;&#31216;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.19024</link><description>&lt;p&gt;
&#21033;&#29992;&#21160;&#24577;&#23545;&#31216;&#24615;&#36827;&#34892;&#22522;&#20110;&#27169;&#22411;&#30340;&#38750;&#23545;&#31216;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Exploiting Symmetry in Dynamics for Model-Based Reinforcement Learning with Asymmetric Rewards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#25511;&#21046;&#29702;&#35770;&#20013;&#23545;&#31216;&#25216;&#26415;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#36890;&#36807;&#21033;&#29992;&#21160;&#24577;&#23545;&#31216;&#24615;&#23398;&#20064;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#32780;&#19981;&#35201;&#27714;&#22870;&#21169;&#20855;&#26377;&#30456;&#21516;&#30340;&#23545;&#31216;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#26368;&#36817;&#30340;&#24037;&#20316;&#21033;&#29992;&#27169;&#22411;&#20013;&#30340;&#23545;&#31216;&#24615;&#26469;&#25552;&#39640;&#31574;&#30053;&#35757;&#32451;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#19968;&#20010;&#24120;&#29992;&#30340;&#31616;&#21270;&#20551;&#35774;&#26159;&#21160;&#21147;&#23398;&#21644;&#22870;&#21169;&#37117;&#34920;&#29616;&#20986;&#30456;&#21516;&#30340;&#23545;&#31216;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#30495;&#23454;&#29615;&#22659;&#20013;&#65292;&#21160;&#21147;&#23398;&#27169;&#22411;&#34920;&#29616;&#20986;&#19982;&#22870;&#21169;&#27169;&#22411;&#29420;&#31435;&#30340;&#23545;&#31216;&#24615;&#65306;&#22870;&#21169;&#21487;&#33021;&#19981;&#28385;&#36275;&#19982;&#21160;&#21147;&#23398;&#30456;&#21516;&#30340;&#23545;&#31216;&#24615;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21482;&#20551;&#23450;&#21160;&#21147;&#23398;&#34920;&#29616;&#20986;&#23545;&#31216;&#24615;&#30340;&#24773;&#20917;&#65292;&#25193;&#23637;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#25511;&#21046;&#29702;&#35770;&#23398;&#20064;&#20013;&#21487;&#24212;&#29992;&#23545;&#31216;&#25216;&#26415;&#30340;&#38382;&#39064;&#33539;&#22260;&#12290;&#25105;&#20204;&#21033;&#29992;&#21345;&#22612;&#24681;&#31227;&#21160;&#26694;&#26550;&#26041;&#27861;&#24341;&#20837;&#19968;&#31181;&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#26500;&#36896;&#65292;&#36825;&#31181;&#21160;&#21147;&#23398;&#34920;&#29616;&#20986;&#25351;&#23450;&#30340;&#23545;&#31216;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23398;&#21040;&#20102;&#26356;&#20934;&#30830;&#30340;&#21160;&#24577;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19024v1 Announce Type: cross  Abstract: Recent work in reinforcement learning has leveraged symmetries in the model to improve sample efficiency in training a policy. A commonly used simplifying assumption is that the dynamics and reward both exhibit the same symmetry. However, in many real-world environments, the dynamical model exhibits symmetry independent of the reward model: the reward may not satisfy the same symmetries as the dynamics. In this paper, we investigate scenarios where only the dynamics are assumed to exhibit symmetry, extending the scope of problems in reinforcement learning and learning in control theory where symmetry techniques can be applied. We use Cartan's moving frame method to introduce a technique for learning dynamics which, by construction, exhibit specified symmetries. We demonstrate through numerical experiments that the proposed method learns a more accurate dynamical model.
&lt;/p&gt;</description></item><item><title>PRIME&#26159;&#19968;&#20010;&#22522;&#20110;&#34892;&#20026;&#21407;&#35821;&#35774;&#35745;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#21407;&#35821;&#24207;&#21015;&#24182;&#23398;&#20064;&#39640;&#32423;&#25511;&#21046;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#38454;&#27573;&#25805;&#20316;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.00929</link><description>&lt;p&gt;
&#21033;&#29992;&#34892;&#20026;&#21407;&#35821;&#25645;&#24314;&#20219;&#21153;&#30340;&#26694;&#26550;&#20197;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#30340;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PRIME: Scaffolding Manipulation Tasks with Behavior Primitives for Data-Efficient Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00929
&lt;/p&gt;
&lt;p&gt;
PRIME&#26159;&#19968;&#20010;&#22522;&#20110;&#34892;&#20026;&#21407;&#35821;&#35774;&#35745;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#21407;&#35821;&#24207;&#21015;&#24182;&#23398;&#20064;&#39640;&#32423;&#25511;&#21046;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#38454;&#27573;&#25805;&#20316;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#21487;&#20197;&#35753;&#26426;&#22120;&#20154;&#23398;&#20250;&#22797;&#26434;&#30340;&#25805;&#20316;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#22312;&#38271;&#26399;&#20219;&#21153;&#20013;&#65292;&#36825;&#20123;&#31639;&#27861;&#21463;&#21040;&#39640;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#22256;&#25200;&#65292;&#22240;&#20026;&#22797;&#21512;&#35823;&#24046;&#20250;&#22312;&#20219;&#21153;&#26102;&#27573;&#20869;&#32047;&#31215;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PRIME&#65288;&#22522;&#20110;&#34892;&#20026;&#21407;&#35821;&#30340;&#25968;&#25454;&#25928;&#29575;&#27169;&#20223;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#34892;&#20026;&#21407;&#35821;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#20223;&#23398;&#20064;&#30340;&#25968;&#25454;&#25928;&#29575;&#12290;PRIME&#36890;&#36807;&#23558;&#20219;&#21153;&#28436;&#31034;&#20998;&#35299;&#20026;&#21407;&#35821;&#24207;&#21015;&#26469;&#25645;&#24314;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#28982;&#21518;&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#23398;&#20064;&#19968;&#20010;&#39640;&#32423;&#25511;&#21046;&#31574;&#30053;&#26469;&#23545;&#21407;&#35821;&#24207;&#21015;&#36827;&#34892;&#25490;&#24207;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;PRIME&#22312;&#22810;&#38454;&#27573;&#25805;&#20316;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#30340;&#25104;&#21151;&#29575;&#27604;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#39640;&#20986;10-34&#65285;&#65292;&#22312;&#23454;&#38469;&#30828;&#20214;&#19978;&#39640;&#20986;20-48&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00929v1 Announce Type: cross  Abstract: Imitation learning has shown great potential for enabling robots to acquire complex manipulation behaviors. However, these algorithms suffer from high sample complexity in long-horizon tasks, where compounding errors accumulate over the task horizons. We present PRIME (PRimitive-based IMitation with data Efficiency), a behavior primitive-based framework designed for improving the data efficiency of imitation learning. PRIME scaffolds robot tasks by decomposing task demonstrations into primitive sequences, followed by learning a high-level control policy to sequence primitives through imitation learning. Our experiments demonstrate that PRIME achieves a significant performance improvement in multi-stage manipulation tasks, with 10-34% higher success rates in simulation over state-of-the-art baselines and 20-48% on physical hardware.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;SCRL&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#29992;&#28789;&#24039;&#30340;&#25163;&#25429;&#25417;&#22810;&#26679;&#21270;&#30340;&#29289;&#20307;&#12290;&#35813;&#31639;&#27861;&#22312;&#22522;&#32447;&#26041;&#27861;&#19978;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#22312;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#19978;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#38646;-shot&#36801;&#31227;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.08809</link><description>&lt;p&gt;
DexCatch: &#23398;&#20064;&#29992;&#28789;&#24039;&#30340;&#25163;&#25429;&#25417;&#20219;&#24847;&#29289;&#20307;
&lt;/p&gt;
&lt;p&gt;
DexCatch: Learning to Catch Arbitrary Objects with Dexterous Hands. (arXiv:2310.08809v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;SCRL&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#29992;&#28789;&#24039;&#30340;&#25163;&#25429;&#25417;&#22810;&#26679;&#21270;&#30340;&#29289;&#20307;&#12290;&#35813;&#31639;&#27861;&#22312;&#22522;&#32447;&#26041;&#27861;&#19978;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#22312;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#19978;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#38646;-shot&#36801;&#31227;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#65292;&#23454;&#29616;&#31867;&#20284;&#20110;&#20154;&#31867;&#28789;&#24039;&#25805;&#32437;&#30340;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25552;&#39640;&#25343;&#21462;&#21644;&#25918;&#32622;&#20219;&#21153;&#30340;&#25104;&#21151;&#29575;&#19978;&#12290;&#19982;&#25343;&#21462;&#21644;&#25918;&#32622;&#30456;&#27604;&#65292;&#25243;&#25509;&#34892;&#20026;&#26377;&#28508;&#21147;&#22312;&#26080;&#38656;&#23558;&#29289;&#20307;&#36816;&#36865;&#21040;&#30446;&#30340;&#22320;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#25343;&#21462;&#36895;&#24230;&#12290;&#28982;&#32780;&#65292;&#21160;&#24577;&#30340;&#28789;&#24039;&#25805;&#32437;&#30001;&#20110;&#22823;&#37327;&#30340;&#21160;&#24577;&#25509;&#35302;&#32780;&#38754;&#20020;&#30528;&#31283;&#23450;&#25511;&#21046;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#65288;SCRL&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#29992;&#28789;&#24039;&#30340;&#25163;&#25429;&#25417;&#22810;&#26679;&#21270;&#30340;&#29289;&#20307;&#12290;&#35813;&#31639;&#27861;&#22312;&#22522;&#32447;&#26041;&#27861;&#19978;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#22312;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#19978;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#38646;-shot&#36801;&#31227;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21363;&#20351;&#25163;&#20013;&#30340;&#29289;&#20307;&#38754;&#21521;&#20391;&#38754;&#38750;&#24120;&#19981;&#31283;&#23450;&#65292;&#30001;&#20110;&#32570;&#20047;&#26469;&#33258;&#25163;&#25484;&#30340;&#25903;&#25745;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20173;&#28982;&#21487;&#20197;&#22312;&#26368;&#20855;&#25361;&#25112;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#24456;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23398;&#21040;&#30340;&#34892;&#20026;&#30340;&#35270;&#39057;&#28436;&#31034;&#21644;&#21512;&#20316;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Achieving human-like dexterous manipulation remains a crucial area of research in robotics. Current research focuses on improving the success rate of pick-and-place tasks. Compared with pick-and-place, throw-catching behavior has the potential to increase picking speed without transporting objects to their destination. However, dynamic dexterous manipulation poses a major challenge for stable control due to a large number of dynamic contacts. In this paper, we propose a Stability-Constrained Reinforcement Learning (SCRL) algorithm to learn to catch diverse objects with dexterous hands. The SCRL algorithm outperforms baselines by a large margin, and the learned policies show strong zero-shot transfer performance on unseen objects. Remarkably, even though the object in a hand facing sideward is extremely unstable due to the lack of support from the palm, our method can still achieve a high level of success in the most challenging task. Video demonstrations of learned behaviors and the co
&lt;/p&gt;</description></item><item><title>CRITERIA&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#31934;&#32454;&#25490;&#21517;&#39044;&#27979;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#27169;&#22411;&#24615;&#33021;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2310.07794</link><description>&lt;p&gt;
CRITERIA&#65306;&#19968;&#31181;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#30340;&#26032;&#22522;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CRITERIA: a New Benchmarking Paradigm for Evaluating Trajectory Prediction Models for Autonomous Driving. (arXiv:2310.07794v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07794
&lt;/p&gt;
&lt;p&gt;
CRITERIA&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#31934;&#32454;&#25490;&#21517;&#39044;&#27979;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#27169;&#22411;&#24615;&#33021;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20934;&#27979;&#35797;&#26159;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#24120;&#29992;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#23545;&#20110;&#36739;&#24120;&#35265;&#30340;&#24773;&#20917;&#65288;&#22914;&#24033;&#33322;&#65289;&#23384;&#22312;&#20559;&#24046;&#65292;&#24182;&#36890;&#36807;&#23545;&#25152;&#26377;&#24773;&#20917;&#36827;&#34892;&#24179;&#22343;&#35745;&#31639;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#24230;&#37327;&#12290;&#36825;&#31181;&#26041;&#27861;&#24456;&#23569;&#33021;&#25552;&#20379;&#26377;&#20851;&#27169;&#22411;&#24615;&#33021;&#30340;&#27934;&#23519;&#65292;&#26080;&#35770;&#26159;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#23427;&#20204;&#33021;&#21542;&#33391;&#22909;&#22788;&#29702;&#65292;&#36824;&#26159;&#23427;&#20204;&#30340;&#36755;&#20986;&#26159;&#21542;&#20801;&#35768;&#21644;&#22810;&#26679;&#21270;&#12290;&#34429;&#28982;&#23384;&#22312;&#19968;&#20123;&#29992;&#20110;&#34913;&#37327;&#36712;&#36857;&#21487;&#20801;&#35768;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#34917;&#20805;&#25351;&#26631;&#65292;&#20294;&#23427;&#20204;&#21463;&#21040;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#22914;&#36712;&#36857;&#38271;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65288;CRITERIA&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26681;&#25454;&#36947;&#36335;&#32467;&#26500;&#12289;&#27169;&#22411;&#24615;&#33021;&#21644;&#25968;&#25454;&#29305;&#24615;&#25552;&#21462;&#39550;&#39542;&#22330;&#26223;&#30340;&#26041;&#27861;&#65292;&#20197;&#36827;&#34892;&#31934;&#32454;&#25490;&#21517;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Benchmarking is a common method for evaluating trajectory prediction models for autonomous driving. Existing benchmarks rely on datasets, which are biased towards more common scenarios, such as cruising, and distance-based metrics that are computed by averaging over all scenarios. Following such a regiment provides a little insight into the properties of the models both in terms of how well they can handle different scenarios and how admissible and diverse their outputs are. There exist a number of complementary metrics designed to measure the admissibility and diversity of trajectories, however, they suffer from biases, such as length of trajectories.  In this paper, we propose a new benChmarking paRadIgm for evaluaTing trajEctoRy predIction Approaches (CRITERIA). Particularly, we propose 1) a method for extracting driving scenarios at varying levels of specificity according to the structure of the roads, models' performance, and data properties for fine-grained ranking of prediction 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38544;&#24335;&#34892;&#20026;&#20811;&#38534;&#21644;&#21160;&#24577;&#36816;&#21160;&#21407;&#35821;&#26469;&#20419;&#36827;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#31034;&#33539;&#25968;&#25454;&#25552;&#39640;&#35757;&#32451;&#36895;&#24230;&#65292;&#20197;&#21450;&#23558;&#36816;&#21160;&#35268;&#21010;&#36716;&#21270;&#20026;&#26356;&#31616;&#21333;&#30340;&#35268;&#21010;&#31354;&#38388;&#65292;&#35813;&#26041;&#27861;&#22312;&#20223;&#30495;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2307.16062</link><description>&lt;p&gt;
&#20351;&#29992;&#38544;&#24335;&#34892;&#20026;&#20811;&#38534;&#21644;&#21160;&#24577;&#36816;&#21160;&#21407;&#35821;&#65292;&#20419;&#36827;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Using Implicit Behavior Cloning and Dynamic Movement Primitive to Facilitate Reinforcement Learning for Robot Motion Planning. (arXiv:2307.16062v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38544;&#24335;&#34892;&#20026;&#20811;&#38534;&#21644;&#21160;&#24577;&#36816;&#21160;&#21407;&#35821;&#26469;&#20419;&#36827;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#31034;&#33539;&#25968;&#25454;&#25552;&#39640;&#35757;&#32451;&#36895;&#24230;&#65292;&#20197;&#21450;&#23558;&#36816;&#21160;&#35268;&#21010;&#36716;&#21270;&#20026;&#26356;&#31616;&#21333;&#30340;&#35268;&#21010;&#31354;&#38388;&#65292;&#35813;&#26041;&#27861;&#22312;&#20223;&#30495;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#33258;&#30001;&#24230;&#26426;&#22120;&#20154;&#30340;&#21160;&#20316;&#35268;&#21010;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#20173;&#28982;&#38754;&#20020;&#35757;&#32451;&#36895;&#24230;&#24930;&#21644;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#26694;&#26550;&#65292;&#21033;&#29992;&#38544;&#24335;&#34892;&#20026;&#20811;&#38534;&#21644;&#21160;&#24577;&#36816;&#21160;&#21407;&#35821;&#26469;&#25552;&#39640;&#31163;&#32447;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#35757;&#32451;&#36895;&#24230;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#38544;&#24335;&#34892;&#20026;&#20811;&#38534;&#21033;&#29992;&#20154;&#31867;&#31034;&#33539;&#25968;&#25454;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#30340;&#35757;&#32451;&#36895;&#24230;&#65292;&#32780;&#21160;&#24577;&#36816;&#21160;&#21407;&#35821;&#20316;&#20026;&#19968;&#31181;&#21551;&#21457;&#24335;&#27169;&#22411;&#65292;&#23558;&#36816;&#21160;&#35268;&#21010;&#36716;&#21270;&#20026;&#26356;&#31616;&#21333;&#30340;&#35268;&#21010;&#31354;&#38388;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#19968;&#26694;&#26550;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#25342;&#21462;-&#25918;&#32622;&#23454;&#39564;&#21019;&#24314;&#20102;&#20154;&#31867;&#31034;&#33539;&#25968;&#25454;&#38598;&#65292;&#20379;&#31867;&#20284;&#30740;&#31350;&#20351;&#29992;&#12290;&#22312;&#20223;&#30495;&#27604;&#36739;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#35813;&#26041;&#27861;&#30456;&#27604;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20855;&#26377;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#24471;&#20998;&#12290;&#22312;&#23454;&#38469;&#26426;&#22120;&#20154;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#23637;&#31034;&#20102;&#22312;&#31616;&#21333;&#32452;&#35013;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#24378;&#21270;&#23398;&#20064;&#30340;&#35757;&#32451;&#36895;&#24230;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) for motion planning of multi-degree-of-freedom robots still suffers from low efficiency in terms of slow training speed and poor generalizability. In this paper, we propose a novel RL-based robot motion planning framework that uses implicit behavior cloning (IBC) and dynamic movement primitive (DMP) to improve the training speed and generalizability of an off-policy RL agent. IBC utilizes human demonstration data to leverage the training speed of RL, and DMP serves as a heuristic model that transfers motion planning into a simpler planning space. To support this, we also create a human demonstration dataset using a pick-and-place experiment that can be used for similar studies. Comparison studies in simulation reveal the advantage of the proposed method over the conventional RL agents with faster training speed and higher scores. A real-robot experiment indicates the applicability of the proposed method to a simple assembly task. Our work provides a novel pe
&lt;/p&gt;</description></item></channel></rss>