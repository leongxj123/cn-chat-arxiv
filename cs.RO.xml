<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#32500;&#24230;&#30340;&#20223;&#30495;&#26426;&#22120;&#20154;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;HumanoidBench&#65292;&#25581;&#31034;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#38754;&#20020;&#25361;&#25112;&#65292;&#32780;&#20855;&#22791;&#40065;&#26834;&#20302;&#32423;&#31574;&#30053;&#25903;&#25345;&#30340;&#20998;&#23618;&#23398;&#20064;&#22522;&#32447;&#34920;&#29616;&#26356;&#20248;&#31168;&#12290;</title><link>https://arxiv.org/abs/2403.10506</link><description>&lt;p&gt;
HumanoidBench&#65306;&#29992;&#20110;&#20840;&#36523;&#36816;&#21160;&#21644;&#25805;&#20316;&#30340;&#20223;&#30495;&#20154;&#22411;&#26426;&#22120;&#20154;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
HumanoidBench: Simulated Humanoid Benchmark for Whole-Body Locomotion and Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10506
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#32500;&#24230;&#30340;&#20223;&#30495;&#26426;&#22120;&#20154;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;HumanoidBench&#65292;&#25581;&#31034;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#38754;&#20020;&#25361;&#25112;&#65292;&#32780;&#20855;&#22791;&#40065;&#26834;&#20302;&#32423;&#31574;&#30053;&#25903;&#25345;&#30340;&#20998;&#23618;&#23398;&#20064;&#22522;&#32447;&#34920;&#29616;&#26356;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#22411;&#26426;&#22120;&#20154;&#22312;&#21327;&#21161;&#20154;&#31867;&#22312;&#19981;&#21516;&#29615;&#22659;&#21644;&#20219;&#21153;&#20013;&#26377;&#30528;&#24040;&#22823;&#28508;&#21147;&#65292;&#30001;&#20110;&#20854;&#28789;&#27963;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#21487;&#20197;&#21033;&#29992;&#31867;&#20154;&#24418;&#24577;&#12290;&#28982;&#32780;&#65292;&#20154;&#22411;&#26426;&#22120;&#20154;&#30340;&#30740;&#31350;&#24120;&#24120;&#21463;&#21040;&#26114;&#36149;&#19988;&#26131;&#25439;&#30340;&#30828;&#20214;&#35774;&#32622;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#21152;&#36895;&#20154;&#22411;&#26426;&#22120;&#20154;&#31639;&#27861;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#32500;&#24230;&#30340;&#20223;&#30495;&#26426;&#22120;&#20154;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#65292;HumanoidBench&#65292;&#35813;&#27979;&#35797;&#21253;&#25324;&#19968;&#20010;&#37197;&#22791;&#28789;&#24039;&#25163;&#37096;&#21644;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20840;&#36523;&#25805;&#20316;&#21644;&#36816;&#21160;&#20219;&#21153;&#30340;&#20154;&#22411;&#26426;&#22120;&#20154;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#34920;&#26126;&#65292;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#20855;&#22791;&#40065;&#26834;&#30340;&#20302;&#32423;&#31574;&#30053;&#25903;&#25345;&#30340;&#20998;&#23618;&#23398;&#20064;&#22522;&#32447;&#22312;&#34892;&#36208;&#25110;&#21040;&#36798;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;&#20511;&#21161;HumanoidBench&#65292;&#25105;&#20204;&#20026;&#26426;&#22120;&#20154;&#31038;&#21306;&#25552;&#20379;&#20102;&#19968;&#20010;&#24179;&#21488;&#65292;&#29992;&#20110;&#35782;&#21035;&#35299;&#20915;&#20154;&#22411;&#26426;&#22120;&#20154;&#22312;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20419;&#36827;&#31639;&#27861;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10506v1 Announce Type: cross  Abstract: Humanoid robots hold great promise in assisting humans in diverse environments and tasks, due to their flexibility and adaptability leveraging human-like morphology. However, research in humanoid robots is often bottlenecked by the costly and fragile hardware setups. To accelerate algorithmic research in humanoid robots, we present a high-dimensional, simulated robot learning benchmark, HumanoidBench, featuring a humanoid robot equipped with dexterous hands and a variety of challenging whole-body manipulation and locomotion tasks. Our findings reveal that state-of-the-art reinforcement learning algorithms struggle with most tasks, whereas a hierarchical learning baseline achieves superior performance when supported by robust low-level policies, such as walking or reaching. With HumanoidBench, we provide the robotics community with a platform to identify the challenges arising when solving diverse tasks with humanoid robots, facilitatin
&lt;/p&gt;</description></item><item><title>MapGPT&#24341;&#20837;&#20102;&#22312;&#32447;&#35821;&#35328;&#24418;&#25104;&#30340;&#22320;&#22270;&#65292;&#24110;&#21161;GPT&#29702;&#35299;&#25972;&#20307;&#29615;&#22659;&#65292;&#25552;&#20986;&#33258;&#36866;&#24212;&#35268;&#21010;&#26426;&#21046;&#20197;&#21327;&#21161;&#20195;&#29702;&#25191;&#34892;&#22810;&#27493;&#36335;&#24452;&#35268;&#21010;&#12290;</title><link>https://arxiv.org/abs/2401.07314</link><description>&lt;p&gt;
MapGPT&#65306;&#20855;&#26377;&#33258;&#36866;&#24212;&#36335;&#24452;&#35268;&#21010;&#30340;&#22320;&#22270;&#24341;&#23548;&#25552;&#31034;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
MapGPT: Map-Guided Prompting with Adaptive Path Planning for Vision-and-Language Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07314
&lt;/p&gt;
&lt;p&gt;
MapGPT&#24341;&#20837;&#20102;&#22312;&#32447;&#35821;&#35328;&#24418;&#25104;&#30340;&#22320;&#22270;&#65292;&#24110;&#21161;GPT&#29702;&#35299;&#25972;&#20307;&#29615;&#22659;&#65292;&#25552;&#20986;&#33258;&#36866;&#24212;&#35268;&#21010;&#26426;&#21046;&#20197;&#21327;&#21161;&#20195;&#29702;&#25191;&#34892;&#22810;&#27493;&#36335;&#24452;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;GPT&#20316;&#20026;&#22823;&#33041;&#30340;&#20307;&#39564;&#20195;&#29702;&#34920;&#29616;&#20986;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#38750;&#20961;&#20915;&#31574;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#38646;-shot&#20195;&#29702;&#21482;&#20419;&#20351;GPT-4&#22312;&#23616;&#37096;&#29615;&#22659;&#20013;&#36873;&#25321;&#28508;&#22312;&#20301;&#32622;&#65292;&#32780;&#27809;&#26377;&#20026;&#20195;&#29702;&#26500;&#24314;&#19968;&#20010;&#26377;&#25928;&#30340;&#8220;&#20840;&#23616;&#35270;&#22270;&#8221;&#26469;&#29702;&#35299;&#25972;&#20307;&#29615;&#22659;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22320;&#22270;&#24341;&#23548;&#30340;&#22522;&#20110;GPT&#30340;&#20195;&#29702;&#65292;&#21517;&#20026;MapGPT&#65292;&#23427;&#24341;&#20837;&#20102;&#19968;&#20010;&#22312;&#32447;&#35821;&#35328;&#24418;&#25104;&#30340;&#22320;&#22270;&#26469;&#40723;&#21169;&#20840;&#23616;&#25506;&#32034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22312;&#32447;&#22320;&#22270;&#65292;&#24182;&#23558;&#20854;&#21512;&#24182;&#21040;&#21253;&#21547;&#33410;&#28857;&#20449;&#24687;&#21644;&#25299;&#25169;&#20851;&#31995;&#30340;&#25552;&#31034;&#20013;&#65292;&#20197;&#24110;&#21161;GPT&#29702;&#35299;&#31354;&#38388;&#29615;&#22659;&#12290;&#20174;&#36825;&#19968;&#35774;&#35745;&#20013;&#33719;&#30410;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#35268;&#21010;&#26426;&#21046;&#65292;&#20197;&#24110;&#21161;&#20195;&#29702;&#26681;&#25454;&#22320;&#22270;&#25191;&#34892;&#22810;&#27493;&#35268;&#21010;&#65292;&#31995;&#32479;&#22320;&#25506;&#32034;&#22810;&#20010;&#20505;&#36873;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07314v2 Announce Type: replace  Abstract: Embodied agents equipped with GPT as their brain have exhibited extraordinary decision-making and generalization abilities across various tasks. However, existing zero-shot agents for vision-and-language navigation (VLN) only prompt the GPT-4 to select potential locations within localized environments, without constructing an effective "global-view" for the agent to understand the overall environment. In this work, we present a novel map-guided GPT-based agent, dubbed MapGPT, which introduces an online linguistic-formed map to encourage the global exploration. Specifically, we build an online map and incorporate it into the prompts that include node information and topological relationships, to help GPT understand the spatial environment. Benefiting from this design, we further propose an adaptive planning mechanism to assist the agent in performing multi-step path planning based on a map, systematically exploring multiple candidate 
&lt;/p&gt;</description></item><item><title>M2CURL&#26159;&#19968;&#31181;&#26679;&#26412;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#20174;&#35270;&#35302;&#35273;&#25968;&#25454;&#20013;&#23398;&#20064;&#20986;&#39640;&#25928;&#30340;&#34920;&#31034;&#65292;&#24182;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2401.17032</link><description>&lt;p&gt;
M2CURL: &#36890;&#36807;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#23454;&#29616;&#26679;&#26412;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#24378;&#21270;&#23398;&#20064;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
M2CURL: Sample-Efficient Multimodal Reinforcement Learning via Self-Supervised Representation Learning for Robotic Manipulation. (arXiv:2401.17032v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17032
&lt;/p&gt;
&lt;p&gt;
M2CURL&#26159;&#19968;&#31181;&#26679;&#26412;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#20174;&#35270;&#35302;&#35273;&#25968;&#25454;&#20013;&#23398;&#20064;&#20986;&#39640;&#25928;&#30340;&#34920;&#31034;&#65292;&#24182;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#24378;&#21270;&#23398;&#20064;&#20013;&#26368;&#37325;&#35201;&#30340;&#26041;&#38754;&#20043;&#19968;&#26159;&#26377;&#25928;&#22320;&#25972;&#21512;&#19981;&#21516;&#30340;&#35266;&#27979;&#27169;&#24577;&#12290;&#20174;&#36825;&#20123;&#27169;&#24577;&#20013;&#24471;&#21040;&#31283;&#20581;&#20934;&#30830;&#30340;&#34920;&#31034;&#23545;&#20110;&#25552;&#21319;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;&#26679;&#26412;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#35270;&#35302;&#35273;&#25968;&#25454;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#23398;&#20064;&#34920;&#31034;&#38754;&#20020;&#30528;&#37325;&#35201;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#25968;&#25454;&#30340;&#39640;&#32500;&#24230;&#21644;&#23558;&#35270;&#35302;&#35273;&#36755;&#20837;&#19982;&#21160;&#24577;&#29615;&#22659;&#21644;&#20219;&#21153;&#30446;&#26631;&#36827;&#34892;&#30456;&#20851;&#24615;&#20998;&#26512;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#23545;&#27604;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#65288;M2CURL&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#23398;&#20064;&#20986;&#39640;&#25928;&#30340;&#34920;&#31034;&#24182;&#21152;&#36895;&#20102;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26080;&#20851;&#65292;&#22240;&#27492;&#21487;&#20197;&#19982;&#20219;&#20309;&#21487;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#25972;&#21512;&#12290;&#25105;&#20204;&#22312;Tactile Gym 2&#27169;&#25311;&#22120;&#19978;&#35780;&#20272;&#20102;M2CURL&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most critical aspects of multimodal Reinforcement Learning (RL) is the effective integration of different observation modalities. Having robust and accurate representations derived from these modalities is key to enhancing the robustness and sample efficiency of RL algorithms. However, learning representations in RL settings for visuotactile data poses significant challenges, particularly due to the high dimensionality of the data and the complexity involved in correlating visual and tactile inputs with the dynamic environment and task objectives. To address these challenges, we propose Multimodal Contrastive Unsupervised Reinforcement Learning (M2CURL). Our approach employs a novel multimodal self-supervised learning technique that learns efficient representations and contributes to faster convergence of RL algorithms. Our method is agnostic to the RL algorithm, thus enabling its integration with any available RL algorithm. We evaluate M2CURL on the Tactile Gym 2 simulator 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#24555;&#36895;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#26368;&#20248;&#36335;&#24452;&#31034;&#33539;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#21644;&#22320;&#22270;&#34920;&#31034;&#65292;&#29983;&#25104;&#27010;&#29575;&#20998;&#24067;&#26469;&#25628;&#32034;&#26368;&#20248;&#36335;&#24452;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#34892;&#26143;&#25506;&#27979;&#36710;&#30340;&#25506;&#32034;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.04792</link><description>&lt;p&gt;
&#24555;&#36895;&#21644;&#26368;&#20248;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#34892;&#26143;&#25506;&#27979;&#36710;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Fast and Optimal Learning-based Path Planning Method for Planetary Rovers. (arXiv:2308.04792v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#24555;&#36895;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#26368;&#20248;&#36335;&#24452;&#31034;&#33539;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#21644;&#22320;&#22270;&#34920;&#31034;&#65292;&#29983;&#25104;&#27010;&#29575;&#20998;&#24067;&#26469;&#25628;&#32034;&#26368;&#20248;&#36335;&#24452;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#34892;&#26143;&#25506;&#27979;&#36710;&#30340;&#25506;&#32034;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#33258;&#20027;&#36335;&#24452;&#35268;&#21010;&#23545;&#20110;&#25552;&#39640;&#34892;&#26143;&#25506;&#27979;&#36710;&#30340;&#25506;&#32034;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#39640;&#31243;&#22270;&#20013;&#24555;&#36895;&#25628;&#32034;&#26368;&#20248;&#36335;&#24452;&#65292;&#31216;&#20026;NNPP&#12290;NNPP&#27169;&#22411;&#20174;&#22823;&#37327;&#39044;&#27880;&#37322;&#30340;&#26368;&#20248;&#36335;&#24452;&#31034;&#33539;&#20013;&#23398;&#20064;&#36215;&#22987;&#21644;&#30446;&#26631;&#20301;&#32622;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#20197;&#21450;&#22320;&#22270;&#34920;&#31034;&#65292;&#24182;&#29983;&#25104;&#27599;&#20010;&#20687;&#32032;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#34920;&#31034;&#20854;&#23646;&#20110;&#22320;&#22270;&#19978;&#26368;&#20248;&#36335;&#24452;&#30340;&#21487;&#33021;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#20174;DEM&#33719;&#21462;&#30340;&#22369;&#24230;&#12289;&#31895;&#31961;&#24230;&#21644;&#39640;&#24230;&#24046;&#35745;&#31639;&#27599;&#20010;&#32593;&#26684;&#21333;&#20803;&#30340;&#36941;&#21382;&#25104;&#26412;&#12290;&#38543;&#21518;&#65292;&#20351;&#29992;&#39640;&#26031;&#20998;&#24067;&#23545;&#36215;&#22987;&#21644;&#30446;&#26631;&#20301;&#32622;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#20998;&#26512;&#19981;&#21516;&#20301;&#32622;&#32534;&#30721;&#21442;&#25968;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#32463;&#36807;&#35757;&#32451;&#65292;NNPP&#27169;&#22411;&#33021;&#22815;&#22312;&#26032;&#30340;&#22320;&#22270;&#19978;&#25191;&#34892;&#36335;&#24452;&#35268;&#21010;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;NNPP&#29983;&#25104;&#30340;&#24341;&#23548;&#22330;&#33021;&#22815;&#20934;&#30830;&#25351;&#23548;&#34892;&#26143;&#25506;&#27979;&#36710;&#30340;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent autonomous path planning is crucial to improve the exploration efficiency of planetary rovers. In this paper, we propose a learning-based method to quickly search for optimal paths in an elevation map, which is called NNPP. The NNPP model learns semantic information about start and goal locations, as well as map representations, from numerous pre-annotated optimal path demonstrations, and produces a probabilistic distribution over each pixel representing the likelihood of it belonging to an optimal path on the map. More specifically, the paper computes the traversal cost for each grid cell from the slope, roughness and elevation difference obtained from the DEM. Subsequently, the start and goal locations are encoded using a Gaussian distribution and different location encoding parameters are analyzed for their effect on model performance. After training, the NNPP model is able to perform path planning on novel maps. Experiments show that the guidance field generated by the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22478;&#24066;&#31354;&#20013;&#20986;&#34892;&#65288;UAM&#65289;&#22312;&#22823;&#37117;&#24066;&#20132;&#36890;&#20013;&#30340;&#30740;&#31350;&#29616;&#29366;&#65292;&#30830;&#23450;&#20102;&#23558;UAM&#34701;&#20837;&#22478;&#24066;&#20132;&#36890;&#31995;&#32479;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#21253;&#25324;&#23545;&#29616;&#26377;&#20132;&#36890;&#27169;&#24335;&#21644;&#25317;&#22581;&#30340;&#24433;&#21709;&#65307;&#23433;&#20840;&#20998;&#26512;&#21644;&#39118;&#38505;&#35780;&#20272;&#65307;&#28508;&#22312;&#30340;&#32463;&#27982;&#21644;&#29615;&#22659;&#25928;&#30410;&#65307;&#20197;&#21450;&#20026;UAM&#21644;&#22320;&#38754;&#20132;&#36890;&#24320;&#21457;&#20849;&#20139;&#22522;&#30784;&#35774;&#26045;&#21644;&#36335;&#32447;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;UAM&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#22914;&#32553;&#30701;&#26053;&#34892;&#26102;&#38388;&#21644;&#25913;&#21892;&#26381;&#21153;&#19981;&#36275;&#22320;&#21306;&#30340;&#21487;&#36798;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.12901</link><description>&lt;p&gt;
&#27169;&#25311;&#22478;&#24066;&#31354;&#20013;&#20986;&#34892;&#34701;&#20837;&#29616;&#26377;&#20132;&#36890;&#31995;&#32479;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Simulating the Integration of Urban Air Mobility into Existing Transportation Systems: A Survey. (arXiv:2301.12901v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22478;&#24066;&#31354;&#20013;&#20986;&#34892;&#65288;UAM&#65289;&#22312;&#22823;&#37117;&#24066;&#20132;&#36890;&#20013;&#30340;&#30740;&#31350;&#29616;&#29366;&#65292;&#30830;&#23450;&#20102;&#23558;UAM&#34701;&#20837;&#22478;&#24066;&#20132;&#36890;&#31995;&#32479;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#21253;&#25324;&#23545;&#29616;&#26377;&#20132;&#36890;&#27169;&#24335;&#21644;&#25317;&#22581;&#30340;&#24433;&#21709;&#65307;&#23433;&#20840;&#20998;&#26512;&#21644;&#39118;&#38505;&#35780;&#20272;&#65307;&#28508;&#22312;&#30340;&#32463;&#27982;&#21644;&#29615;&#22659;&#25928;&#30410;&#65307;&#20197;&#21450;&#20026;UAM&#21644;&#22320;&#38754;&#20132;&#36890;&#24320;&#21457;&#20849;&#20139;&#22522;&#30784;&#35774;&#26045;&#21644;&#36335;&#32447;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;UAM&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#22914;&#32553;&#30701;&#26053;&#34892;&#26102;&#38388;&#21644;&#25913;&#21892;&#26381;&#21153;&#19981;&#36275;&#22320;&#21306;&#30340;&#21487;&#36798;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper surveys the current state of research on urban air mobility (UAM) in metropolitan-scale traffic using simulation techniques, identifying key challenges and opportunities for integrating UAM into urban transportation systems, including impacts on existing traffic patterns and congestion, safety analysis and risk assessment, potential economic and environmental benefits, and the development of shared infrastructure and routes for UAM and ground-based transportation. The potential benefits of UAM, such as reduced travel times and improved accessibility for underserved areas, are also discussed.
&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#31354;&#20013;&#20986;&#34892;&#65288;UAM&#65289;&#26377;&#21487;&#33021;&#24443;&#24213;&#25913;&#21464;&#22823;&#37117;&#24066;&#22320;&#21306;&#30340;&#20132;&#36890;&#26041;&#24335;&#65292;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340;&#20132;&#36890;&#26041;&#24335;&#65292;&#32531;&#35299;&#25317;&#22581;&#65292;&#25552;&#39640;&#21487;&#36798;&#24615;&#12290;&#28982;&#32780;&#65292;&#23558;UAM&#34701;&#20837;&#29616;&#26377;&#20132;&#36890;&#31995;&#32479;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#28145;&#20837;&#20102;&#35299;&#20854;&#23545;&#20132;&#36890;&#27969;&#37327;&#21644;&#23481;&#37327;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#35843;&#26597;&#65292;&#20351;&#29992;&#27169;&#25311;&#25216;&#26415;&#35843;&#26597;&#20102;UAM&#22312;&#22823;&#37117;&#24066;&#20132;&#36890;&#20013;&#30340;&#30740;&#31350;&#29616;&#29366;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#23558;UAM&#34701;&#20837;&#22478;&#24066;&#20132;&#36890;&#31995;&#32479;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#21253;&#25324;&#23545;&#29616;&#26377;&#20132;&#36890;&#27169;&#24335;&#21644;&#25317;&#22581;&#30340;&#24433;&#21709;&#65307;&#23433;&#20840;&#20998;&#26512;&#21644;&#39118;&#38505;&#35780;&#20272;&#65307;&#28508;&#22312;&#30340;&#32463;&#27982;&#21644;&#29615;&#22659;&#25928;&#30410;&#65307;&#20197;&#21450;&#20026;UAM&#21644;&#22320;&#38754;&#20132;&#36890;&#24320;&#21457;&#20849;&#20139;&#22522;&#30784;&#35774;&#26045;&#21644;&#36335;&#32447;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;UAM&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#22914;&#32553;&#30701;&#26053;&#34892;&#26102;&#38388;&#21644;&#25913;&#21892;&#26381;&#21153;&#19981;&#36275;&#22320;&#21306;&#30340;&#21487;&#36798;&#24615;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Urban air mobility (UAM) has the potential to revolutionize transportation in metropolitan areas, providing a new mode of transportation that could alleviate congestion and improve accessibility. However, the integration of UAM into existing transportation systems is a complex task that requires a thorough understanding of its impact on traffic flow and capacity. In this paper, we conduct a survey to investigate the current state of research on UAM in metropolitan-scale traffic using simulation techniques. We identify key challenges and opportunities for the integration of UAM into urban transportation systems, including impacts on existing traffic patterns and congestion; safety analysis and risk assessment; potential economic and environmental benefits; and the development of shared infrastructure and routes for UAM and ground-based transportation. We also discuss the potential benefits of UAM, such as reduced travel times and improved accessibility for underserved areas. Our survey 
&lt;/p&gt;</description></item></channel></rss>