<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#20998;&#21106;&#25513;&#27169;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36890;&#29992;&#22411;&#26426;&#22120;&#20154;&#30340;&#27867;&#21270;&#33021;&#21147;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#22312;&#24320;&#25918;&#22495;&#22330;&#26223;&#20013;&#26032;&#23545;&#35937;&#30340;&#25235;&#21462;&#25805;&#20316;&#30340;&#23398;&#20064;&#25928;&#29575;&#21644;&#25512;&#24191;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.05716</link><description>&lt;p&gt;
&#20026;&#25235;&#20303;&#20219;&#20309;&#29289;&#21697;&#38138;&#24179;&#36947;&#36335;&#65306;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;&#36890;&#29992;&#25235;&#21462;&#25918;&#32622;&#26426;&#22120;&#20154;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Pave the Way to Grasp Anything: Transferring Foundation Models for Universal Pick-Place Robots. (arXiv:2306.05716v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#20998;&#21106;&#25513;&#27169;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36890;&#29992;&#22411;&#26426;&#22120;&#20154;&#30340;&#27867;&#21270;&#33021;&#21147;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#22312;&#24320;&#25918;&#22495;&#22330;&#26223;&#20013;&#26032;&#23545;&#35937;&#30340;&#25235;&#21462;&#25805;&#20316;&#30340;&#23398;&#20064;&#25928;&#29575;&#21644;&#25512;&#24191;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#39640;&#36890;&#29992;&#22411;&#26426;&#22120;&#20154;&#30340;&#27867;&#21270;&#33021;&#21147;&#19968;&#30452;&#26159;&#30740;&#31350;&#31038;&#21306;&#38271;&#26399;&#36861;&#27714;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#25910;&#38598;&#22823;&#35268;&#27169;&#29616;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#25968;&#25454;&#65292;&#22914; RT-1 &#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#25928;&#29575;&#20302;&#19979;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20855;&#26377;&#26032;&#23545;&#35937;&#21644;&#22810;&#26679;&#32972;&#26223;&#30340;&#24320;&#25918;&#22495;&#22330;&#26223;&#20013;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;&#27169;&#22411;&#29983;&#25104;&#30340;&#22522;&#20110;&#35821;&#35328;&#30340;&#20998;&#21106;&#25513;&#27169;&#65292;&#20197;&#35299;&#20915;&#26085;&#24120;&#22330;&#26223;&#20013;&#24191;&#27867;&#30340;&#25342;&#25918;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#25513;&#27169;&#20256;&#36798;&#30340;&#31934;&#30830;&#35821;&#20041;&#21644;&#20960;&#20309;&#24418;&#29366;&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#22810;&#35270;&#35282;&#31574;&#30053;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24863;&#30693;&#20934;&#30830;&#30340;&#29289;&#20307;&#23039;&#24577;&#24182;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#65292;&#21516;&#26102;&#20063;&#26377;&#21161;&#20110;&#26377;&#25928;&#30340;&#26032;&#23545;&#35937;&#30340;&#25512;&#24191;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21516;&#26102;&#21487;&#20197;&#23454;&#29616;&#22312;&#35757;&#32451;&#26102;&#35266;&#23519;&#21040;&#30456;&#20284;&#24418;&#29366;&#30340;&#26032;&#29289;&#20307;&#30340;&#25235;&#21462;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving the generalization capabilities of general-purpose robotic agents has long been a significant challenge actively pursued by research communities. Existing approaches often rely on collecting large-scale real-world robotic data, such as the RT-1 dataset. However, these approaches typically suffer from low efficiency, limiting their capability in open-domain scenarios with new objects, and diverse backgrounds. In this paper, we propose a novel paradigm that effectively leverages language-grounded segmentation masks generated by state-of-the-art foundation models, to address a wide range of pick-and-place robot manipulation tasks in everyday scenarios. By integrating precise semantics and geometries conveyed from masks into our multi-view policy model, our approach can perceive accurate object poses and enable sample-efficient learning. Besides, such design facilitates effective generalization for grasping new objects with similar shapes observed during training. Our approach co
&lt;/p&gt;</description></item></channel></rss>