<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#26412;&#25991;&#26088;&#22312;&#35299;&#26500;&#20154;&#24037;&#26234;&#33021;&#36131;&#20219;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21253;&#21547;&#22235;&#31181;&#36131;&#20219;&#24847;&#20041;&#30340;&#26377;&#25928;&#32452;&#21512;&#65292;&#20197;&#25903;&#25345;&#23545;&#20154;&#24037;&#26234;&#33021;&#36131;&#20219;&#30340;&#23454;&#36341;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2308.02608</link><description>&lt;p&gt;
&#35299;&#26500;&#20154;&#24037;&#26234;&#33021;&#36131;&#20219;
&lt;/p&gt;
&lt;p&gt;
Unravelling Responsibility for AI. (arXiv:2308.02608v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35299;&#26500;&#20154;&#24037;&#26234;&#33021;&#36131;&#20219;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21253;&#21547;&#22235;&#31181;&#36131;&#20219;&#24847;&#20041;&#30340;&#26377;&#25928;&#32452;&#21512;&#65292;&#20197;&#25903;&#25345;&#23545;&#20154;&#24037;&#26234;&#33021;&#36131;&#20219;&#30340;&#23454;&#36341;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#28041;&#21450;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#22797;&#26434;&#24773;&#20917;&#19979;&#21512;&#29702;&#24605;&#32771;&#36131;&#20219;&#24212;&#35813;&#25918;&#22312;&#20309;&#22788;&#65292;&#25105;&#20204;&#39318;&#20808;&#38656;&#35201;&#19968;&#20010;&#36275;&#22815;&#28165;&#26224;&#21644;&#35814;&#32454;&#30340;&#36328;&#23398;&#31185;&#35789;&#27719;&#26469;&#35848;&#35770;&#36131;&#20219;&#12290;&#36131;&#20219;&#26159;&#19968;&#31181;&#19977;&#20803;&#20851;&#31995;&#65292;&#28041;&#21450;&#21040;&#19968;&#20010;&#34892;&#20026;&#32773;&#12289;&#19968;&#20010;&#20107;&#20214;&#21644;&#19968;&#31181;&#36131;&#20219;&#26041;&#24335;&#12290;&#20316;&#20026;&#19968;&#31181;&#26377;&#24847;&#35782;&#30340;&#20026;&#20102;&#25903;&#25345;&#23545;&#20154;&#24037;&#26234;&#33021;&#36131;&#20219;&#36827;&#34892;&#23454;&#36341;&#25512;&#29702;&#30340;&#8220;&#35299;&#26500;&#8221;&#36131;&#20219;&#27010;&#24565;&#30340;&#21162;&#21147;&#65292;&#26412;&#25991;&#37319;&#21462;&#20102;&#8220;&#34892;&#20026;&#32773;A&#23545;&#20107;&#20214;O&#36127;&#36131;&#8221;&#30340;&#19977;&#37096;&#20998;&#34920;&#36848;&#65292;&#24182;&#30830;&#23450;&#20102;A&#12289;&#36127;&#36131;&#12289;O&#30340;&#23376;&#31867;&#21035;&#30340;&#26377;&#25928;&#32452;&#21512;&#12290;&#36825;&#20123;&#26377;&#25928;&#32452;&#21512;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#36131;&#20219;&#20018;&#8221;&#65292;&#20998;&#20026;&#22235;&#31181;&#36131;&#20219;&#24847;&#20041;&#65306;&#35282;&#33394;&#36131;&#20219;&#12289;&#22240;&#26524;&#36131;&#20219;&#12289;&#27861;&#24459;&#36131;&#20219;&#21644;&#36947;&#24503;&#36131;&#20219;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#36816;&#34892;&#31034;&#20363;&#36827;&#34892;&#20102;&#35828;&#26126;&#65292;&#19968;&#20010;&#28041;&#21450;&#21307;&#30103;AI&#31995;&#32479;&#65292;&#21478;&#19968;&#20010;&#28041;&#21450;AV&#19982;&#34892;&#20154;&#30340;&#33268;&#21629;&#30896;&#25758;&#12290;
&lt;/p&gt;
&lt;p&gt;
To reason about where responsibility does and should lie in complex situations involving AI-enabled systems, we first need a sufficiently clear and detailed cross-disciplinary vocabulary for talking about responsibility. Responsibility is a triadic relation involving an actor, an occurrence, and a way of being responsible. As part of a conscious effort towards 'unravelling' the concept of responsibility to support practical reasoning about responsibility for AI, this paper takes the three-part formulation, 'Actor A is responsible for Occurrence O' and identifies valid combinations of subcategories of A, is responsible for, and O. These valid combinations - which we term "responsibility strings" - are grouped into four senses of responsibility: role-responsibility; causal responsibility; legal liability-responsibility; and moral responsibility. They are illustrated with two running examples, one involving a healthcare AI-based system and another the fatal collision of an AV with a pedes
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DRL with Symbolic Logics (DRLSL)&#30340;&#26032;&#39062;&#31070;&#32463;&#31526;&#21495;&#26080;&#27169;&#22411;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#23433;&#20840;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#31526;&#21495;&#36923;&#36753;&#39537;&#21160;&#30340;&#25512;&#29702;&#65292;&#20801;&#35768;&#36890;&#36807;&#19982;&#29289;&#29702;&#29615;&#22659;&#30340;&#23454;&#26102;&#20132;&#20114;&#26469;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#24182;&#30830;&#20445;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01316</link><description>&lt;p&gt;
&#29992;&#31070;&#32463;&#31526;&#21495;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#23433;&#20840;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Safe Autonomous Driving Policies using a Neuro-Symbolic Deep Reinforcement Learning Approach. (arXiv:2307.01316v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DRL with Symbolic Logics (DRLSL)&#30340;&#26032;&#39062;&#31070;&#32463;&#31526;&#21495;&#26080;&#27169;&#22411;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#23433;&#20840;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#31526;&#21495;&#36923;&#36753;&#39537;&#21160;&#30340;&#25512;&#29702;&#65292;&#20801;&#35768;&#36890;&#36807;&#19982;&#29289;&#29702;&#29615;&#22659;&#30340;&#23454;&#26102;&#20132;&#20114;&#26469;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#24182;&#30830;&#20445;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#20013;&#30340;&#21160;&#24577;&#39550;&#39542;&#29615;&#22659;&#21644;&#22810;&#26679;&#21270;&#36947;&#36335;&#20351;&#29992;&#32773;&#30340;&#23384;&#22312;&#32473;&#20915;&#31574;&#36896;&#25104;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#24050;&#25104;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23433;&#20840;&#38382;&#39064;&#30340;&#38480;&#21046;&#65292;&#29616;&#26377;&#30340;DRL&#35299;&#20915;&#26041;&#26696;&#30340;&#24212;&#29992;&#20027;&#35201;&#23616;&#38480;&#20110;&#27169;&#25311;&#29615;&#22659;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#37096;&#32626;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#23616;&#38480;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#31526;&#21495;&#26080;&#27169;&#22411;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#24102;&#26377;&#31526;&#21495;&#36923;&#36753;&#30340;DRL(DRLSL)&#65292;&#23427;&#23558;DRL(&#20174;&#32463;&#39564;&#20013;&#23398;&#20064;)&#21644;&#31526;&#21495;&#19968;&#38454;&#36923;&#36753;&#30693;&#35782;&#39537;&#21160;&#30340;&#25512;&#29702;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#22312;&#23454;&#38469;&#29615;&#22659;&#19979;&#23433;&#20840;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#30340;&#23454;&#26102;&#20132;&#20114;&#12290;&#36825;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#36807;&#31215;&#26497;&#19982;&#29289;&#29702;&#29615;&#22659;&#20114;&#21160;&#26469;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#25919;&#31574;&#24182;&#30830;&#20445;&#23433;&#20840;&#24615;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#20351;&#29992;&#39640;&#32500;&#24230;&#25968;&#25454;&#23454;&#29616;&#20102;&#33258;&#20027;&#39550;&#39542;&#30340;DRLSL&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dynamic nature of driving environments and the presence of diverse road users pose significant challenges for decision-making in autonomous driving. Deep reinforcement learning (DRL) has emerged as a popular approach to tackle this problem. However, the application of existing DRL solutions is mainly confined to simulated environments due to safety concerns, impeding their deployment in real-world. To overcome this limitation, this paper introduces a novel neuro-symbolic model-free DRL approach, called DRL with Symbolic Logics (DRLSL) that combines the strengths of DRL (learning from experience) and symbolic first-order logics knowledge-driven reasoning) to enable safe learning in real-time interactions of autonomous driving within real environments. This innovative approach provides a means to learn autonomous driving policies by actively engaging with the physical environment while ensuring safety. We have implemented the DRLSL framework in autonomous driving using the highD data
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;Actor-Critic&#31070;&#32463;&#32467;&#26500;&#30340;&#33258;&#25972;&#23450;PID&#25511;&#21046;&#22120;&#65292;&#29992;&#20110;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#30340;&#23039;&#24577;&#21644;&#39640;&#24230;&#25511;&#21046;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#35843;&#25972;PID&#22686;&#30410;&#65292;&#25552;&#39640;&#20102;&#31995;&#32479;&#30340;&#31283;&#20581;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01312</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#28151;&#21512;Actor-Critic&#31070;&#32463;&#32467;&#26500;&#30340;&#33258;&#25972;&#23450;PID&#25511;&#21046;&#22120;&#65292;&#23454;&#29616;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Self-Tuning PID Control via a Hybrid Actor-Critic-Based Neural Structure for Quadcopter Control. (arXiv:2307.01312v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;Actor-Critic&#31070;&#32463;&#32467;&#26500;&#30340;&#33258;&#25972;&#23450;PID&#25511;&#21046;&#22120;&#65292;&#29992;&#20110;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#30340;&#23039;&#24577;&#21644;&#39640;&#24230;&#25511;&#21046;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#35843;&#25972;PID&#22686;&#30410;&#65292;&#25552;&#39640;&#20102;&#31995;&#32479;&#30340;&#31283;&#20581;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27604;&#20363;&#31215;&#20998;&#24494;&#20998;&#65288;PID&#65289;&#25511;&#21046;&#22120;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24037;&#19994;&#21644;&#23454;&#39564;&#36807;&#31243;&#20013;&#65292;&#29616;&#26377;&#30340;&#31163;&#32447;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#35843;&#25972;PID&#22686;&#30410;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#22411;&#21442;&#25968;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#22806;&#37096;&#24178;&#25200;&#30340;&#23384;&#22312;&#65292;&#23454;&#38469;&#31995;&#32479;&#65288;&#22914;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#65289;&#38656;&#35201;&#26356;&#31283;&#20581;&#21487;&#38752;&#30340;PID&#25511;&#21046;&#22120;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#23454;&#29616;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#23039;&#24577;&#21644;&#39640;&#24230;&#25511;&#21046;&#30340;&#33258;&#25972;&#23450;PID&#25511;&#21046;&#22120;&#12290;&#37319;&#29992;&#20102;&#22686;&#37327;&#24335;PID&#25511;&#21046;&#22120;&#65292;&#24182;&#20165;&#23545;&#21487;&#21464;&#22686;&#30410;&#36827;&#34892;&#20102;&#35843;&#25972;&#12290;&#20026;&#20102;&#35843;&#25972;&#21160;&#24577;&#22686;&#30410;&#65292;&#20351;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26080;&#27169;&#22411;Actor-Critic&#28151;&#21512;&#31070;&#32463;&#32467;&#26500;&#65292;&#33021;&#22815;&#36866;&#24403;&#35843;&#25972;PID&#22686;&#30410;&#65292;&#21516;&#26102;&#20805;&#24403;&#26368;&#20339;&#35782;&#21035;&#22120;&#12290;&#22312;&#35843;&#25972;&#21644;&#35782;&#21035;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;&#20855;&#26377;&#20004;&#20010;&#38544;&#34255;&#23618;&#21644;Sigmoid&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#21033;&#29992;&#33258;&#36866;&#24212;&#21160;&#37327;&#65288;ADAM&#65289;&#20248;&#21270;&#22120;&#21644;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proportional-Integrator-Derivative (PID) controller is used in a wide range of industrial and experimental processes. There are a couple of offline methods for tuning PID gains. However, due to the uncertainty of model parameters and external disturbances, real systems such as Quadrotors need more robust and reliable PID controllers. In this research, a self-tuning PID controller using a Reinforcement-Learning-based Neural Network for attitude and altitude control of a Quadrotor has been investigated. An Incremental PID, which contains static and dynamic gains, has been considered and only the variable gains have been tuned. To tune dynamic gains, a model-free actor-critic-based hybrid neural structure was used that was able to properly tune PID gains, and also has done the best as an identifier. In both tunning and identification tasks, a Neural Network with two hidden layers and sigmoid activation functions has been learned using Adaptive Momentum (ADAM) optimizer and Back-Propagatio
&lt;/p&gt;</description></item></channel></rss>