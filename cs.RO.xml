<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22810;&#27169;&#24577;VAE&#22914;&#20309;&#22312;&#26080;&#30417;&#30563;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#23454;&#29616;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#27169;&#25311;&#22120;&#20013;&#30340;&#27169;&#22411;&#24615;&#33021;&#25552;&#39640;55%&#12290;</title><link>https://arxiv.org/abs/2404.01932</link><description>&lt;p&gt;
&#36328;&#36234;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#34892;&#21160;&#65306;&#22810;&#27169;&#24577;VAE&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Bridging Language, Vision and Action: Multimodal VAEs in Robotic Manipulation Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22810;&#27169;&#24577;VAE&#22914;&#20309;&#22312;&#26080;&#30417;&#30563;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#23454;&#29616;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#27169;&#25311;&#22120;&#20013;&#30340;&#27169;&#22411;&#24615;&#33021;&#25552;&#39640;55%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#26426;&#22120;&#20154;&#25805;&#20316;&#39046;&#22495;&#20013;&#26080;&#30417;&#30563;&#30340;&#35270;&#35273;-&#35821;&#35328;-&#21160;&#20316;&#26144;&#23556;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#22914;&#20309;&#34987;&#24212;&#29992;&#20110;&#26080;&#30417;&#30563;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#25913;&#36827;&#27169;&#22411;&#24615;&#33021;&#30340;&#27169;&#22411;&#19981;&#21464;&#24335;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#27169;&#25311;&#22120;&#20013;&#30340;&#27169;&#22411;&#24615;&#33021;&#25552;&#39640;&#39640;&#36798;55%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01932v1 Announce Type: cross  Abstract: In this work, we focus on unsupervised vision-language-action mapping in the area of robotic manipulation. Recently, multiple approaches employing pre-trained large language and vision models have been proposed for this task. However, they are computationally demanding and require careful fine-tuning of the produced outputs. A more lightweight alternative would be the implementation of multimodal Variational Autoencoders (VAEs) which can extract the latent features of the data and integrate them into a joint representation, as has been demonstrated mostly on image-image or image-text data for the state-of-the-art models. Here we explore whether and how can multimodal VAEs be employed in unsupervised robotic manipulation tasks in a simulated environment. Based on the obtained results, we propose a model-invariant training alternative that improves the models' performance in a simulator by up to 55%. Moreover, we systematically evaluate 
&lt;/p&gt;</description></item></channel></rss>