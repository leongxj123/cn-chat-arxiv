<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>SEAC&#26159;&#19968;&#31181;&#24377;&#24615;&#26102;&#38388;&#27493;&#38271;&#30340;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#36890;&#36807;&#21487;&#21464;&#25345;&#32493;&#26102;&#38388;&#30340;&#26102;&#38388;&#27493;&#38271;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#26681;&#25454;&#24773;&#20917;&#25913;&#21464;&#25511;&#21046;&#39057;&#29575;&#65292;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.14961</link><description>&lt;p&gt;
&#24377;&#24615;&#26102;&#38388;&#27493;&#38271;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Elastic Time Steps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14961
&lt;/p&gt;
&lt;p&gt;
SEAC&#26159;&#19968;&#31181;&#24377;&#24615;&#26102;&#38388;&#27493;&#38271;&#30340;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#36890;&#36807;&#21487;&#21464;&#25345;&#32493;&#26102;&#38388;&#30340;&#26102;&#38388;&#27493;&#38271;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#26681;&#25454;&#24773;&#20917;&#25913;&#21464;&#25511;&#21046;&#39057;&#29575;&#65292;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#36890;&#24120;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#23398;&#20064;&#20197;&#20197;&#22266;&#23450;&#25511;&#21046;&#39057;&#29575;&#25191;&#34892;&#21160;&#20316;&#30340;&#25511;&#21046;&#22120;&#12290;&#37492;&#20110;RL&#31639;&#27861;&#30340;&#31163;&#25955;&#24615;&#36136;&#65292;&#23427;&#20204;&#23545;&#25511;&#21046;&#39057;&#29575;&#30340;&#36873;&#25321;&#30340;&#24433;&#21709;&#35270;&#32780;&#19981;&#35265;&#65306;&#25214;&#21040;&#27491;&#30830;&#30340;&#25511;&#21046;&#39057;&#29575;&#21487;&#33021;&#24456;&#22256;&#38590;&#65292;&#38169;&#35823;&#24448;&#24448;&#20250;&#23548;&#33268;&#36807;&#24230;&#20351;&#29992;&#35745;&#31639;&#36164;&#28304;&#29978;&#33267;&#23548;&#33268;&#26080;&#27861;&#25910;&#25947;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36719;&#24377;&#24615;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;SEAC&#65289;, &#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;SEAC&#23454;&#29616;&#20102;&#24377;&#24615;&#26102;&#38388;&#27493;&#38271;&#65292;&#21363;&#20855;&#26377;&#24050;&#30693;&#21464;&#21270;&#25345;&#32493;&#26102;&#38388;&#30340;&#26102;&#38388;&#27493;&#38271;&#65292;&#20801;&#35768;&#20195;&#29702;&#26681;&#25454;&#24773;&#20917;&#25913;&#21464;&#20854;&#25511;&#21046;&#39057;&#29575;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;SEAC&#20165;&#22312;&#24517;&#35201;&#26102;&#24212;&#29992;&#25511;&#21046;&#65292;&#26368;&#23567;&#21270;&#35745;&#31639;&#36164;&#28304;&#21644;&#25968;&#25454;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;SEAC&#22312;&#29275;&#39039;&#36816;&#21160;&#23398;&#36855;&#23467;&#23548;&#33322;&#20219;&#21153;&#21644;&#19977;&#32500;&#36187;&#36710;&#35270;&#39057;&#28216;&#25103;Trackmania&#20013;&#30340;&#33021;&#21147;&#12290;SEAC&#22312;&#34920;&#29616;&#19978;&#20248;&#20110;SAC&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14961v1 Announce Type: cross  Abstract: Traditional Reinforcement Learning (RL) algorithms are usually applied in robotics to learn controllers that act with a fixed control rate. Given the discrete nature of RL algorithms, they are oblivious to the effects of the choice of control rate: finding the correct control rate can be difficult and mistakes often result in excessive use of computing resources or even lack of convergence.   We propose Soft Elastic Actor-Critic (SEAC), a novel off-policy actor-critic algorithm to address this issue. SEAC implements elastic time steps, time steps with a known, variable duration, which allow the agent to change its control frequency to adapt to the situation. In practice, SEAC applies control only when necessary, minimizing computational resources and data usage.   We evaluate SEAC's capabilities in simulation in a Newtonian kinematics maze navigation task and on a 3D racing video game, Trackmania. SEAC outperforms the SAC baseline in t
&lt;/p&gt;</description></item><item><title>&#21487;&#35299;&#37322;&#30340;AI&#25216;&#26415;&#23545;&#20110;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#23433;&#20840;&#38382;&#39064;&#21644;&#20449;&#20219;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#30340;&#26041;&#24335;&#65292;&#20998;&#26512;&#20102;&#21487;&#35299;&#37322;&#30340;AI&#26041;&#27861;&#22312;&#28385;&#36275;&#33258;&#21160;&#39550;&#39542;&#35201;&#27714;&#26041;&#38754;&#30340;&#20851;&#38190;&#36129;&#29486;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#30340;&#35774;&#35745;&#12289;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#27169;&#22411;&#12289;&#21487;&#35299;&#37322;&#30340;&#30417;&#25511;&#12289;&#36741;&#21161;&#25216;&#26415;&#21644;&#35299;&#37322;&#30340;&#21487;&#35270;&#21270;&#31561;&#20116;&#20010;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.10086</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#22312;&#23433;&#20840;&#21487;&#20449;&#30340;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#31995;&#32479;&#24615;&#35780;&#36848;
&lt;/p&gt;
&lt;p&gt;
Explainable AI for Safe and Trustworthy Autonomous Driving: A Systematic Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10086
&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;AI&#25216;&#26415;&#23545;&#20110;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#23433;&#20840;&#38382;&#39064;&#21644;&#20449;&#20219;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#30340;&#26041;&#24335;&#65292;&#20998;&#26512;&#20102;&#21487;&#35299;&#37322;&#30340;AI&#26041;&#27861;&#22312;&#28385;&#36275;&#33258;&#21160;&#39550;&#39542;&#35201;&#27714;&#26041;&#38754;&#30340;&#20851;&#38190;&#36129;&#29486;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#30340;&#35774;&#35745;&#12289;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#27169;&#22411;&#12289;&#21487;&#35299;&#37322;&#30340;&#30417;&#25511;&#12289;&#36741;&#21161;&#25216;&#26415;&#21644;&#35299;&#37322;&#30340;&#21487;&#35270;&#21270;&#31561;&#20116;&#20010;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#20854;&#22312;&#24863;&#30693;&#21644;&#35268;&#21010;&#20219;&#21153;&#20013;&#30456;&#23545;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#65288;AD&#65289;&#30340;&#24212;&#29992;&#26174;&#31034;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#38590;&#20197;&#29702;&#35299;&#30340;AI&#31995;&#32479;&#21152;&#21095;&#20102;&#23545;AD&#23433;&#20840;&#20445;&#35777;&#30340;&#29616;&#26377;&#25361;&#25112;&#12290;&#32531;&#35299;&#36825;&#19968;&#25361;&#25112;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;AI&#65288;XAI&#65289;&#25216;&#26415;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#20851;&#20110;&#21487;&#35299;&#37322;&#26041;&#27861;&#22312;&#23433;&#20840;&#21487;&#20449;&#30340;AD&#20013;&#30340;&#20840;&#38754;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#12290;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#22312;AD&#32972;&#26223;&#19979;AI&#30340;&#35201;&#27714;&#65292;&#37325;&#28857;&#20851;&#27880;&#25968;&#25454;&#12289;&#27169;&#22411;&#21644;&#26426;&#26500;&#36825;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;&#25105;&#20204;&#21457;&#29616;XAI&#23545;&#20110;&#28385;&#36275;&#36825;&#20123;&#35201;&#27714;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;AI&#20013;&#35299;&#37322;&#30340;&#26469;&#28304;&#65292;&#24182;&#25551;&#36848;&#20102;&#19968;&#31181;XAI&#30340;&#20998;&#31867;&#23398;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;XAI&#22312;&#23433;&#20840;&#21487;&#20449;&#30340;AD&#20013;&#30340;&#20116;&#20010;&#20027;&#35201;&#36129;&#29486;&#65292;&#21253;&#25324;&#21487;&#35299;&#37322;&#30340;&#35774;&#35745;&#12289;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#27169;&#22411;&#12289;&#21487;&#35299;&#37322;&#30340;&#30417;&#25511;&#65292;&#36741;&#21161;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10086v1 Announce Type: cross  Abstract: Artificial Intelligence (AI) shows promising applications for the perception and planning tasks in autonomous driving (AD) due to its superior performance compared to conventional methods. However, inscrutable AI systems exacerbate the existing challenge of safety assurance of AD. One way to mitigate this challenge is to utilize explainable AI (XAI) techniques. To this end, we present the first comprehensive systematic literature review of explainable methods for safe and trustworthy AD. We begin by analyzing the requirements for AI in the context of AD, focusing on three key aspects: data, model, and agency. We find that XAI is fundamental to meeting these requirements. Based on this, we explain the sources of explanations in AI and describe a taxonomy of XAI. We then identify five key contributions of XAI for safe and trustworthy AI in AD, which are interpretable design, interpretable surrogate models, interpretable monitoring, auxil
&lt;/p&gt;</description></item></channel></rss>