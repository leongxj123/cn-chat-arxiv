<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>GenNBV&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#36890;&#29992;&#30340;&#19979;&#19968;&#26368;&#20339;&#35270;&#35282;&#31574;&#30053;&#65292;&#36890;&#36807;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#21644;&#25193;&#23637;&#21040;5D&#33258;&#30001;&#31354;&#38388;&#30340;&#21160;&#20316;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#26080;&#20154;&#26426;&#20174;&#20219;&#24847;&#35270;&#35282;&#36827;&#34892;&#25195;&#25551;&#65292;&#29978;&#33267;&#19982;&#26410;&#30693;&#20960;&#20309;&#20307;&#36827;&#34892;&#20132;&#20114;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#22810;&#28304;&#29366;&#24577;&#23884;&#20837;&#20197;&#22686;&#24378;&#36328;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.16174</link><description>&lt;p&gt;
GenNBV: &#36890;&#29992;&#30340;&#20027;&#21160;&#24335;&#19977;&#32500;&#37325;&#24314;&#19979;&#19968;&#26368;&#20339;&#35270;&#35282;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
GenNBV: Generalizable Next-Best-View Policy for Active 3D Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16174
&lt;/p&gt;
&lt;p&gt;
GenNBV&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#36890;&#29992;&#30340;&#19979;&#19968;&#26368;&#20339;&#35270;&#35282;&#31574;&#30053;&#65292;&#36890;&#36807;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#21644;&#25193;&#23637;&#21040;5D&#33258;&#30001;&#31354;&#38388;&#30340;&#21160;&#20316;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#26080;&#20154;&#26426;&#20174;&#20219;&#24847;&#35270;&#35282;&#36827;&#34892;&#25195;&#25551;&#65292;&#29978;&#33267;&#19982;&#26410;&#30693;&#20960;&#20309;&#20307;&#36827;&#34892;&#20132;&#20114;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#22810;&#28304;&#29366;&#24577;&#23884;&#20837;&#20197;&#22686;&#24378;&#36328;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#25216;&#26415;&#36827;&#27493;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#22330;&#26223;&#30340;&#30495;&#23454;&#25968;&#23383;&#21270;, &#20294;&#26159;&#22270;&#20687;&#25429;&#33719;&#36807;&#31243;&#20173;&#28982;&#32791;&#26102;&#19988;&#21171;&#21160;&#23494;&#38598;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#23581;&#35797;&#20351;&#29992;&#20027;&#21160;&#24335;&#19977;&#32500;&#37325;&#24314;&#30340;&#19979;&#19968;&#26368;&#20339;&#35270;&#35282;&#65288;NBV&#65289;&#31574;&#30053;&#26469;&#33258;&#21160;&#21270;&#36825;&#19968;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;NBV&#31574;&#30053;&#20005;&#37325;&#20381;&#36182;&#25163;&#24037;&#35774;&#35745;&#30340;&#26631;&#20934;&#12289;&#26377;&#38480;&#30340;&#21160;&#20316;&#31354;&#38388;&#65292;&#25110;&#32773;&#26159;&#38024;&#23545;&#27599;&#20010;&#22330;&#26223;&#20248;&#21270;&#30340;&#34920;&#31034;&#12290;&#36825;&#20123;&#32422;&#26463;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#36328;&#25968;&#25454;&#38598;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GenNBV&#65292;&#19968;&#20010;&#31471;&#21040;&#31471;&#36890;&#29992;&#30340;NBV&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#37319;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#26694;&#26550;&#65292;&#23558;&#20856;&#22411;&#26377;&#38480;&#30340;&#21160;&#20316;&#31354;&#38388;&#25193;&#23637;&#21040;5D&#33258;&#30001;&#31354;&#38388;&#12290;&#23427;&#36171;&#20104;&#20102;&#25105;&#20204;&#30340;&#20195;&#29702;&#26426;&#26080;&#20154;&#26426;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#20197;&#20174;&#20219;&#20309;&#35270;&#35282;&#36827;&#34892;&#25195;&#25551;&#65292;&#29978;&#33267;&#21487;&#20197;&#19982;&#26410;&#35265;&#20960;&#20309;&#20307;&#36827;&#34892;&#20132;&#20114;&#12290;&#20026;&#20102;&#22686;&#24378;&#36328;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#28304;&#29366;&#24577;&#23884;&#20837;&#65292;&#21253;&#25324;&#20960;&#20309;&#12289;&#35821;&#20041;&#21644;&#21160;&#20316;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16174v1 Announce Type: cross  Abstract: While recent advances in neural radiance field enable realistic digitization for large-scale scenes, the image-capturing process is still time-consuming and labor-intensive. Previous works attempt to automate this process using the Next-Best-View (NBV) policy for active 3D reconstruction. However, the existing NBV policies heavily rely on hand-crafted criteria, limited action space, or per-scene optimized representations. These constraints limit their cross-dataset generalizability. To overcome them, we propose GenNBV, an end-to-end generalizable NBV policy. Our policy adopts a reinforcement learning (RL)-based framework and extends typical limited action space to 5D free space. It empowers our agent drone to scan from any viewpoint, and even interact with unseen geometries during training. To boost the cross-dataset generalizability, we also propose a novel multi-source state embedding, including geometric, semantic, and action repres
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KI-PMF&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20808;&#39564;&#30693;&#35782;&#65292;&#23545;&#20132;&#36890;&#21442;&#19982;&#32773;&#30340;&#26410;&#26469;&#34892;&#21160;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#65292;&#36981;&#24490;&#36710;&#36742;&#30340;&#36816;&#21160;&#32422;&#26463;&#21644;&#34892;&#39542;&#29615;&#22659;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#36890;&#36807;&#26465;&#20214;&#21270;&#32593;&#32476;&#20197;&#36981;&#24490;&#29289;&#29702;&#23450;&#24459;&#65292;&#21487;&#20197;&#33719;&#24471;&#20934;&#30830;&#21644;&#23433;&#20840;&#30340;&#39044;&#27979;&#65292;&#23545;&#20110;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#32500;&#25252;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#23433;&#20840;&#21644;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2310.12007</link><description>&lt;p&gt;
KI-PMF&#65306;&#30693;&#35782;&#32508;&#21512;&#30340;&#21512;&#29702;&#21160;&#20316;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
KI-PMF: Knowledge Integrated Plausible Motion Forecasting. (arXiv:2310.12007v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KI-PMF&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20808;&#39564;&#30693;&#35782;&#65292;&#23545;&#20132;&#36890;&#21442;&#19982;&#32773;&#30340;&#26410;&#26469;&#34892;&#21160;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#65292;&#36981;&#24490;&#36710;&#36742;&#30340;&#36816;&#21160;&#32422;&#26463;&#21644;&#34892;&#39542;&#29615;&#22659;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#36890;&#36807;&#26465;&#20214;&#21270;&#32593;&#32476;&#20197;&#36981;&#24490;&#29289;&#29702;&#23450;&#24459;&#65292;&#21487;&#20197;&#33719;&#24471;&#20934;&#30830;&#21644;&#23433;&#20840;&#30340;&#39044;&#27979;&#65292;&#23545;&#20110;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#32500;&#25252;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#23433;&#20840;&#21644;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#20132;&#36890;&#21442;&#19982;&#32773;&#30340;&#34892;&#21160;&#23545;&#22823;&#35268;&#27169;&#37096;&#32626;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#30340;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#20248;&#21270;&#29305;&#23450;&#24230;&#37327;&#30340;&#25439;&#22833;&#20989;&#25968;&#19978;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#39044;&#27979;&#19981;&#31526;&#21512;&#29289;&#29702;&#23450;&#24459;&#25110;&#36829;&#21453;&#22806;&#37096;&#32422;&#26463;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#32467;&#21512;&#26126;&#30830;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20351;&#32593;&#32476;&#33021;&#22815;&#39044;&#27979;&#26410;&#26469;&#36712;&#36857;&#65292;&#31526;&#21512;&#36710;&#36742;&#30340;&#36816;&#21160;&#32422;&#26463;&#21644;&#34892;&#39542;&#29615;&#22659;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38750;&#21442;&#25968;&#21098;&#26525;&#23618;&#21644;&#27880;&#24847;&#21147;&#23618;&#26469;&#25972;&#21512;&#23450;&#20041;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#30830;&#20445;&#20132;&#36890;&#21442;&#19982;&#32773;&#22312;&#22797;&#26434;&#21644;&#21160;&#24577;&#24773;&#20917;&#19979;&#30340;&#21040;&#36798;&#21487;&#36798;&#24615;&#20445;&#35777;&#12290;&#36890;&#36807;&#23558;&#32593;&#32476;&#26465;&#20214;&#21270;&#20026;&#36981;&#24490;&#29289;&#29702;&#23450;&#24459;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#20934;&#30830;&#21644;&#23433;&#20840;&#30340;&#39044;&#27979;&#65292;&#36825;&#23545;&#20110;&#22312;&#23454;&#38469;&#19990;&#30028;&#29615;&#22659;&#20013;&#32500;&#25252;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#23433;&#20840;&#21644;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately forecasting the motion of traffic actors is crucial for the deployment of autonomous vehicles at a large scale. Current trajectory forecasting approaches primarily concentrate on optimizing a loss function with a specific metric, which can result in predictions that do not adhere to physical laws or violate external constraints. Our objective is to incorporate explicit knowledge priors that allow a network to forecast future trajectories in compliance with both the kinematic constraints of a vehicle and the geometry of the driving environment. To achieve this, we introduce a non-parametric pruning layer and attention layers to integrate the defined knowledge priors. Our proposed method is designed to ensure reachability guarantees for traffic actors in both complex and dynamic situations. By conditioning the network to follow physical laws, we can obtain accurate and safe predictions, essential for maintaining autonomous vehicles' safety and efficiency in real-world settings
&lt;/p&gt;</description></item></channel></rss>