<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#21387;&#21147;&#27979;&#35797;&#26041;&#27861;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#31995;&#32479;&#25506;&#32034;&#21487;&#33021;&#23548;&#33268;&#39640;&#36895;&#20844;&#36335;&#20132;&#36890;&#22330;&#26223;&#20013;&#23433;&#20840;&#38382;&#39064;&#30340;&#36793;&#30028;&#24773;&#20917;</title><link>https://arxiv.org/abs/2402.11813</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#21387;&#21147;&#27979;&#35797;&#39640;&#36895;&#20844;&#36335;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A novel framework for adaptive stress testing of autonomous vehicles in highways
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11813
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#21387;&#21147;&#27979;&#35797;&#26041;&#27861;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#31995;&#32479;&#25506;&#32034;&#21487;&#33021;&#23548;&#33268;&#39640;&#36895;&#20844;&#36335;&#20132;&#36890;&#22330;&#26223;&#20013;&#23433;&#20840;&#38382;&#39064;&#30340;&#36793;&#30028;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#35777;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#30340;&#23433;&#20840;&#36816;&#34892;&#23545;&#20110;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#21644;&#20844;&#20247;&#25509;&#21463;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#19981;&#20165;&#23545;AV&#36827;&#34892;&#26631;&#20934;&#23433;&#20840;&#27979;&#35797;&#30340;&#35780;&#20272;&#65292;&#36824;&#21457;&#29616;&#21487;&#33021;&#23548;&#33268;&#19981;&#23433;&#20840;&#34892;&#20026;&#25110;&#24773;&#20917;&#30340;&#34987;&#27979;&#35797;AV&#30340;&#28508;&#22312;&#36793;&#30028;&#24773;&#20917;&#20855;&#26377;&#26497;&#20854;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#22320;&#25506;&#32034;&#21487;&#33021;&#23548;&#33268;&#39640;&#36895;&#20844;&#36335;&#20132;&#36890;&#22330;&#26223;&#20013;&#23433;&#20840;&#38382;&#39064;&#30340;&#36793;&#30028;&#24773;&#20917;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#19968;&#31181;&#33258;&#36866;&#24212;&#21387;&#21147;&#27979;&#35797;&#65288;AST&#65289;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#21046;&#23450;&#22330;&#26223;&#20197;&#21450;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#21457;&#29616;&#20195;&#34920;&#36793;&#30028;&#24773;&#20917;&#30340;&#29702;&#24819;&#27169;&#24335;&#30340;&#26032;&#20852;&#39564;&#35777;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20026;DRL&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20197;&#25351;&#23548;AST&#26681;&#25454;&#34987;&#27979;&#35797;AV&#65288;&#21363;&#33258;&#36710;&#65289;&#19982;&#20854;&#20182;&#36710;&#36742;&#20043;&#38388;&#30340;&#30896;&#25758;&#27010;&#29575;&#20272;&#35745;&#26469;&#35782;&#21035;&#30896;&#25758;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11813v1 Announce Type: cross  Abstract: Guaranteeing the safe operations of autonomous vehicles (AVs) is crucial for their widespread adoption and public acceptance. It is thus of a great significance to not only assess the AV against the standard safety tests, but also discover potential corner cases of the AV under test that could lead to unsafe behaviour or scenario. In this paper, we propose a novel framework to systematically explore corner cases that can result in safety concerns in a highway traffic scenario. The framework is based on an adaptive stress testing (AST) approach, an emerging validation method that leverages a Markov decision process to formulate the scenarios and deep reinforcement learning (DRL) to discover the desirable patterns representing corner cases. To this end, we develop a new reward function for DRL to guide the AST in identifying crash scenarios based on the collision probability estimate between the AV under test (i.e., the ego vehicle) and 
&lt;/p&gt;</description></item><item><title /><link>https://arxiv.org/abs/2402.07127</link><description>&lt;p&gt;
&#35266;&#23519;&#23398;&#20064;&#65306;&#22522;&#20110;&#35270;&#39057;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#23398;&#20064;&#26041;&#27861;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Learning by Watching: A Review of Video-based Learning Approaches for Robot Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07127
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#23398;&#20064;&#25805;&#20316;&#25216;&#33021;&#21463;&#21040;&#22810;&#26679;&#21270;&#12289;&#26080;&#20559;&#30340;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#24615;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#38382;&#39064;&#65292;&#20294;&#22312;&#27867;&#21270;&#24615;&#21644;&#29616;&#23454;&#19990;&#30028;&#30340;&#36716;&#31227;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#8220;&#37326;&#22806;&#8221;&#35270;&#39057;&#25968;&#25454;&#38598;&#30340;&#22823;&#35268;&#27169;&#23384;&#22312;&#36890;&#36807;&#33258;&#30417;&#30563;&#25216;&#26415;&#25512;&#21160;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#36827;&#23637;&#12290;&#23558;&#36825;&#19968;&#28857;&#24212;&#29992;&#21040;&#26426;&#22120;&#20154;&#39046;&#22495;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#36890;&#36807;&#34987;&#21160;&#35266;&#23519;&#26469;&#23398;&#20064;&#20016;&#23500;&#30340;&#22312;&#32447;&#35270;&#39057;&#20013;&#30340;&#25805;&#20316;&#25216;&#33021;&#12290;&#36825;&#31181;&#22522;&#20110;&#35270;&#39057;&#30340;&#23398;&#20064;&#33539;&#24335;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#23427;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#30340;&#30417;&#30563;&#26041;&#27861;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#25968;&#25454;&#38598;&#30340;&#20559;&#35265;&#12290;&#26412;&#32508;&#36848;&#22238;&#39038;&#20102;&#35270;&#39057;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#12289;&#29289;&#20307;&#21487;&#34892;&#24615;&#29702;&#35299;&#12289;&#19977;&#32500;&#25163;&#37096;/&#36523;&#20307;&#24314;&#27169;&#21644;&#22823;&#35268;&#27169;&#26426;&#22120;&#20154;&#36164;&#28304;&#31561;&#22522;&#30784;&#30693;&#35782;&#65292;&#20197;&#21450;&#20174;&#19981;&#21463;&#25511;&#21046;&#30340;&#35270;&#39057;&#28436;&#31034;&#20013;&#33719;&#21462;&#26426;&#22120;&#20154;&#25805;&#20316;&#25216;&#33021;&#30340;&#26032;&#20852;&#25216;&#26415;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20165;&#20174;&#35266;&#23519;&#22823;&#35268;&#27169;&#20154;&#31867;&#35270;&#39057;&#20013;&#23398;&#20064;&#22914;&#20309;&#22686;&#24378;&#26426;&#22120;&#20154;&#30340;&#27867;&#21270;&#24615;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robot learning of manipulation skills is hindered by the scarcity of diverse, unbiased datasets. While curated datasets can help, challenges remain in generalizability and real-world transfer. Meanwhile, large-scale "in-the-wild" video datasets have driven progress in computer vision through self-supervised techniques. Translating this to robotics, recent works have explored learning manipulation skills by passively watching abundant videos sourced online. Showing promising results, such video-based learning paradigms provide scalable supervision while reducing dataset bias. This survey reviews foundations such as video feature representation learning techniques, object affordance understanding, 3D hand/body modeling, and large-scale robot resources, as well as emerging techniques for acquiring robot manipulation skills from uncontrolled video demonstrations. We discuss how learning only from observing large-scale human videos can enhance generalization and sample efficiency for roboti
&lt;/p&gt;</description></item></channel></rss>