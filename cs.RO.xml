<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#25552;&#20986;&#20102;Human-Regularized PPO (HR-PPO)&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#25105;&#21338;&#24328;&#35757;&#32451;&#20195;&#29702;&#65292;&#23454;&#29616;&#22312;&#23553;&#38381;&#29615;&#22659;&#20013;&#36924;&#30495;&#19988;&#26377;&#25928;&#30340;&#39550;&#39542;&#20249;&#20276;</title><link>https://arxiv.org/abs/2403.19648</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#27491;&#21017;&#21270;&#30340;&#33258;&#25105;&#21338;&#24328;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#19982;&#20154;&#31867;&#20860;&#23481;&#30340;&#39550;&#39542;&#20249;&#20276;
&lt;/p&gt;
&lt;p&gt;
Human-compatible driving partners through data-regularized self-play reinforcement learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19648
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Human-Regularized PPO (HR-PPO)&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#25105;&#21338;&#24328;&#35757;&#32451;&#20195;&#29702;&#65292;&#23454;&#29616;&#22312;&#23553;&#38381;&#29615;&#22659;&#20013;&#36924;&#30495;&#19988;&#26377;&#25928;&#30340;&#39550;&#39542;&#20249;&#20276;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#27773;&#36710;&#38754;&#20020;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#26159;&#19982;&#20154;&#31867;&#36827;&#34892;&#21327;&#35843;&#12290;&#22240;&#27492;&#65292;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#65292;&#23558;&#36924;&#30495;&#30340;&#20154;&#31867;&#20195;&#29702;&#32435;&#20837;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#21487;&#25193;&#23637;&#35757;&#32451;&#21644;&#35780;&#20272;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Human-Regularized PPO (HR-PPO)&#30340;&#22810;&#26234;&#33021;&#20307;&#31639;&#27861;&#65292;&#20854;&#20013;&#20195;&#29702;&#36890;&#36807;&#33258;&#25105;&#21338;&#24328;&#36827;&#34892;&#35757;&#32451;&#65292;&#23545;&#20559;&#31163;&#20154;&#31867;&#21442;&#32771;&#31574;&#30053;&#30340;&#34892;&#20026;&#36827;&#34892;&#23567;&#24133;&#24809;&#32602;&#65292;&#20197;&#26500;&#24314;&#22312;&#23553;&#38381;&#29615;&#22659;&#20013;&#26082;&#36924;&#30495;&#21448;&#26377;&#25928;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19648v1 Announce Type: cross  Abstract: A central challenge for autonomous vehicles is coordinating with humans. Therefore, incorporating realistic human agents is essential for scalable training and evaluation of autonomous driving systems in simulation. Simulation agents are typically developed by imitating large-scale, high-quality datasets of human driving. However, pure imitation learning agents empirically have high collision rates when executed in a multi-agent closed-loop setting. To build agents that are realistic and effective in closed-loop settings, we propose Human-Regularized PPO (HR-PPO), a multi-agent algorithm where agents are trained through self-play with a small penalty for deviating from a human reference policy. In contrast to prior work, our approach is RL-first and only uses 30 minutes of imperfect human demonstrations. We evaluate agents in a large set of multi-agent traffic scenes. Results show our HR-PPO agents are highly effective in achieving goa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;Stackelberg&#21338;&#24328;&#20013;&#20248;&#21270;&#20247;&#22810;&#26426;&#22120;&#20154;&#30340;&#34892;&#21160;&#39034;&#24207;&#30340;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#39640;&#25928;&#20934;&#30830;&#30340;&#31639;&#27861;(B&amp;P)&#26469;&#27714;&#35299;&#30456;&#20851;&#30340;&#20248;&#21270;&#38382;&#39064;&#21644;&#22343;&#34913;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.09246</link><description>&lt;p&gt;
&#35841;&#20808;&#34892;&#21160;&#65311;&#20248;&#21270;Stackelberg&#21338;&#24328;&#20013;&#20247;&#22810;&#26426;&#22120;&#20154;&#30340;&#34892;&#21160;&#39034;&#24207;
&lt;/p&gt;
&lt;p&gt;
Who Plays First? Optimizing the Order of Play in Stackelberg Games with Many Robots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;Stackelberg&#21338;&#24328;&#20013;&#20248;&#21270;&#20247;&#22810;&#26426;&#22120;&#20154;&#30340;&#34892;&#21160;&#39034;&#24207;&#30340;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#39640;&#25928;&#20934;&#30830;&#30340;&#31639;&#27861;(B&amp;P)&#26469;&#27714;&#35299;&#30456;&#20851;&#30340;&#20248;&#21270;&#38382;&#39064;&#21644;&#22343;&#34913;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#35745;&#31639;&#22810;&#26234;&#33021;&#20307;&#31354;&#38388;&#23548;&#33322;&#38382;&#39064;&#30340;&#31038;&#20250;&#26368;&#20248;&#34892;&#21160;&#39034;&#24207;&#30340;&#38382;&#39064;&#65292;&#21363;&#26234;&#33021;&#20307;&#20915;&#31574;&#39034;&#24207;&#65292;&#20197;&#21450;&#19982;&#20043;&#30456;&#20851;&#30340;N&#20154;Stackelberg&#36712;&#36857;&#21338;&#24328;&#30340;&#22343;&#34913;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#24314;&#27169;&#20026;&#19968;&#20010;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#38382;&#39064;&#65292;&#28041;&#21450;&#21040;&#25152;&#26377;&#21487;&#33021;&#30340;&#34892;&#21160;&#39034;&#24207;&#30340;Stackelberg&#21338;&#24328;&#31354;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Branch and Play (B&amp;P)&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#25910;&#25947;&#21040;&#31038;&#20250;&#26368;&#20248;&#34892;&#21160;&#39034;&#24207;&#21450;&#20854;Stackelberg&#22343;&#34913;&#12290;&#20316;&#20026;B&amp;P&#30340;&#19968;&#20010;&#23376;&#20363;&#31243;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#25193;&#23637;&#20102;&#39034;&#24207;&#36712;&#36857;&#35268;&#21010;&#65292;&#21363;&#19968;&#31181;&#27969;&#34892;&#30340;&#22810;&#26234;&#33021;&#20307;&#25511;&#21046;&#26041;&#27861;&#65292;&#20197;&#20415;&#20026;&#20219;&#20309;&#32473;&#23450;&#30340;&#34892;&#21160;&#39034;&#24207;&#21487;&#25193;&#23637;&#22320;&#35745;&#31639;&#26377;&#25928;&#30340;&#26412;&#22320;Stackelberg&#22343;&#34913;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;B&amp;P&#22312;&#21327;&#35843;&#31354;&#20013;&#20132;&#36890;&#25511;&#21046;&#12289;&#32676;&#20307;&#24418;&#25104;&#21644;&#20132;&#20184;&#36710;&#38431;&#26041;&#38754;&#30340;&#23454;&#38469;&#25928;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;B&amp;P&#30340;&#32467;&#26524;&#26159;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09246v1 Announce Type: cross Abstract: We consider the multi-agent spatial navigation problem of computing the socially optimal order of play, i.e., the sequence in which the agents commit to their decisions, and its associated equilibrium in an N-player Stackelberg trajectory game. We model this problem as a mixed-integer optimization problem over the space of all possible Stackelberg games associated with the order of play's permutations. To solve the problem, we introduce Branch and Play (B&amp;P), an efficient and exact algorithm that provably converges to a socially optimal order of play and its Stackelberg equilibrium. As a subroutine for B&amp;P, we employ and extend sequential trajectory planning, i.e., a popular multi-agent control approach, to scalably compute valid local Stackelberg equilibria for any given order of play. We demonstrate the practical utility of B&amp;P to coordinate air traffic control, swarm formation, and delivery vehicle fleets. We find that B&amp;P consistent
&lt;/p&gt;</description></item></channel></rss>