<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;Transformer&#31070;&#32463;&#32593;&#32476;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#20174;&#25972;&#40784;&#25490;&#21015;&#30340;&#31034;&#33539;&#20013;&#29702;&#35299;&#21644;&#22797;&#21046;&#25972;&#27905;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#23454;&#29616;&#25972;&#29702;&#29289;&#21697;&#30340;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2310.04566</link><description>&lt;p&gt;
Knolling Bot: &#20174;&#25972;&#27905;&#30340;&#31034;&#33539;&#20013;&#23398;&#20064;&#26426;&#22120;&#20154;&#23545;&#35937;&#25490;&#21015;
&lt;/p&gt;
&lt;p&gt;
Knolling Bot: Learning Robotic Object Arrangement from Tidy Demonstrations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.04566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;Transformer&#31070;&#32463;&#32593;&#32476;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#20174;&#25972;&#40784;&#25490;&#21015;&#30340;&#31034;&#33539;&#20013;&#29702;&#35299;&#21644;&#22797;&#21046;&#25972;&#27905;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#23454;&#29616;&#25972;&#29702;&#29289;&#21697;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#22336;&#65306;arXiv:2310.04566v2  &#20844;&#21578;&#31867;&#22411;&#65306;replace-cross  &#25688;&#35201;&#65306;&#35299;&#20915;&#23478;&#24237;&#31354;&#38388;&#20013;&#25955;&#20081;&#29289;&#21697;&#30340;&#25972;&#29702;&#25361;&#25112;&#21463;&#21040;&#25972;&#27905;&#24615;&#30340;&#22810;&#26679;&#24615;&#21644;&#20027;&#35266;&#24615;&#30340;&#22797;&#26434;&#24615;&#24433;&#21709;&#12290;&#27491;&#22914;&#20154;&#31867;&#35821;&#35328;&#30340;&#22797;&#26434;&#24615;&#20801;&#35768;&#21516;&#19968;&#29702;&#24565;&#30340;&#22810;&#31181;&#34920;&#36798;&#19968;&#26679;&#65292;&#23478;&#24237;&#25972;&#27905;&#20559;&#22909;&#21644;&#32452;&#32455;&#27169;&#24335;&#21464;&#21270;&#24191;&#27867;&#65292;&#22240;&#27492;&#39044;&#35774;&#29289;&#20307;&#20301;&#32622;&#23558;&#38480;&#21046;&#23545;&#26032;&#29289;&#20307;&#21644;&#29615;&#22659;&#30340;&#36866;&#24212;&#24615;&#12290;&#21463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#36827;&#23637;&#21551;&#21457;&#65292;&#26412;&#25991;&#24341;&#20837;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#20174;&#25972;&#27905;&#24067;&#23616;&#30340;&#31034;&#33539;&#20013;&#29702;&#35299;&#21644;&#22797;&#21046;&#25972;&#27905;&#30340;&#27010;&#24565;&#65292;&#31867;&#20284;&#20110;&#20351;&#29992;&#20250;&#35805;&#25968;&#25454;&#38598;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;Transformer&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#21518;&#32493;&#29289;&#20307;&#30340;&#25670;&#25918;&#20301;&#32622;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#8220;&#25972;&#29702;&#8221;&#31995;&#32479;&#65292;&#21033;&#29992;&#26426;&#26800;&#33218;&#21644;RGB&#30456;&#26426;&#22312;&#26700;&#23376;&#19978;&#32452;&#32455;&#19981;&#21516;&#22823;&#23567;&#21644;&#25968;&#37327;&#30340;&#29289;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.04566v2 Announce Type: replace-cross  Abstract: Addressing the challenge of organizing scattered items in domestic spaces is complicated by the diversity and subjective nature of tidiness. Just as the complexity of human language allows for multiple expressions of the same idea, household tidiness preferences and organizational patterns vary widely, so presetting object locations would limit the adaptability to new objects and environments. Inspired by advancements in natural language processing (NLP), this paper introduces a self-supervised learning framework that allows robots to understand and replicate the concept of tidiness from demonstrations of well-organized layouts, akin to using conversational datasets to train Large Language Models(LLM). We leverage a transformer neural network to predict the placement of subsequent objects. We demonstrate a ``knolling'' system with a robotic arm and an RGB camera to organize items of varying sizes and quantities on a table. Our 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#24863;&#30693;-&#34892;&#21160;-&#36890;&#20449;(LPAC)&#26550;&#26500;&#65292;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#29615;&#22659;&#24863;&#30693;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#20449;&#24687;&#20132;&#27969;&#65292;&#27973;&#23618;&#22810;&#23618;&#24863;&#30693;&#26426;&#35745;&#31639;&#26426;&#22120;&#20154;&#30340;&#21160;&#20316;&#12290;&#20351;&#29992;&#38598;&#20013;&#24335;&#26174;&#24494;&#31639;&#27861;&#35757;&#32451;&#27169;&#22411;&#65292;&#23454;&#29616;&#26426;&#22120;&#20154;&#32676;&#20307;&#30340;&#21327;&#20316;&#12290;</title><link>http://arxiv.org/abs/2401.04855</link><description>&lt;p&gt;
LPAC: &#21487;&#23398;&#20064;&#30340;&#24863;&#30693;-&#34892;&#21160;-&#36890;&#20449;&#24490;&#29615;&#21450;&#20854;&#22312;&#35206;&#30422;&#25511;&#21046;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
LPAC: Learnable Perception-Action-Communication Loops with Applications to Coverage Control. (arXiv:2401.04855v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04855
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#24863;&#30693;-&#34892;&#21160;-&#36890;&#20449;(LPAC)&#26550;&#26500;&#65292;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#29615;&#22659;&#24863;&#30693;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#20449;&#24687;&#20132;&#27969;&#65292;&#27973;&#23618;&#22810;&#23618;&#24863;&#30693;&#26426;&#35745;&#31639;&#26426;&#22120;&#20154;&#30340;&#21160;&#20316;&#12290;&#20351;&#29992;&#38598;&#20013;&#24335;&#26174;&#24494;&#31639;&#27861;&#35757;&#32451;&#27169;&#22411;&#65292;&#23454;&#29616;&#26426;&#22120;&#20154;&#32676;&#20307;&#30340;&#21327;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35206;&#30422;&#25511;&#21046;&#26159;&#25351;&#23548;&#26426;&#22120;&#20154;&#32676;&#20307;&#21327;&#21516;&#30417;&#27979;&#26410;&#30693;&#30340;&#24863;&#20852;&#36259;&#29305;&#24449;&#25110;&#29616;&#35937;&#30340;&#38382;&#39064;&#12290;&#22312;&#26377;&#38480;&#30340;&#36890;&#20449;&#21644;&#24863;&#30693;&#33021;&#21147;&#30340;&#20998;&#25955;&#35774;&#32622;&#20013;&#65292;&#36825;&#20010;&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#24863;&#30693;-&#34892;&#21160;-&#36890;&#20449;(LPAC)&#26550;&#26500;&#26469;&#35299;&#20915;&#35206;&#30422;&#25511;&#21046;&#38382;&#39064;&#12290;&#22312;&#35813;&#35299;&#20915;&#26041;&#26696;&#20013;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#22788;&#29702;&#20102;&#29615;&#22659;&#30340;&#23616;&#37096;&#24863;&#30693;&#65307;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#23454;&#29616;&#20102;&#37051;&#36817;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#30456;&#20851;&#20449;&#24687;&#36890;&#20449;&#65307;&#26368;&#21518;&#65292;&#27973;&#23618;&#22810;&#23618;&#24863;&#30693;&#26426;(MLP)&#35745;&#31639;&#26426;&#22120;&#20154;&#30340;&#21160;&#20316;&#12290;&#36890;&#20449;&#27169;&#22359;&#20013;&#30340;GNN&#36890;&#36807;&#35745;&#31639;&#24212;&#35813;&#19982;&#37051;&#23621;&#36890;&#20449;&#21738;&#20123;&#20449;&#24687;&#20197;&#21450;&#22914;&#20309;&#21033;&#29992;&#25509;&#25910;&#21040;&#30340;&#20449;&#24687;&#37319;&#21462;&#36866;&#24403;&#30340;&#34892;&#21160;&#26469;&#23454;&#29616;&#26426;&#22120;&#20154;&#32676;&#20307;&#30340;&#21327;&#20316;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#30693;&#26195;&#25972;&#20010;&#29615;&#22659;&#30340;&#38598;&#20013;&#24335;&#26174;&#24494;&#31639;&#27861;&#26469;&#36827;&#34892;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coverage control is the problem of navigating a robot swarm to collaboratively monitor features or a phenomenon of interest not known a priori. The problem is challenging in decentralized settings with robots that have limited communication and sensing capabilities. This paper proposes a learnable Perception-Action-Communication (LPAC) architecture for the coverage control problem. In the proposed solution, a convolution neural network (CNN) processes localized perception of the environment; a graph neural network (GNN) enables communication of relevant information between neighboring robots; finally, a shallow multi-layer perceptron (MLP) computes robot actions. The GNN in the communication module enables collaboration in the robot swarm by computing what information to communicate with neighbors and how to use received information to take appropriate actions. We train models using imitation learning with a centralized clairvoyant algorithm that is aware of the entire environment. Eva
&lt;/p&gt;</description></item></channel></rss>