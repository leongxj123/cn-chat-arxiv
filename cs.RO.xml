<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>Synapse&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#20174;&#26377;&#38480;&#28436;&#31034;&#20013;&#39640;&#25928;&#23398;&#20064;&#20559;&#22909;&#27010;&#24565;&#65292;&#36890;&#36807;&#23558;&#20559;&#22909;&#34920;&#31034;&#20026;&#31070;&#32463;&#31526;&#21495;&#31243;&#24207;&#24182;&#21033;&#29992;&#35270;&#35273;&#35299;&#26512;&#12289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#31243;&#24207;&#21512;&#25104;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#26469;&#23398;&#20064;&#20010;&#20154;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2403.16689</link><description>&lt;p&gt;
Synapse: &#20174;&#35270;&#35273;&#28436;&#31034;&#20013;&#23398;&#20064;&#20248;&#20808;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Synapse: Learning Preferential Concepts from Visual Demonstrations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16689
&lt;/p&gt;
&lt;p&gt;
Synapse&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#20174;&#26377;&#38480;&#28436;&#31034;&#20013;&#39640;&#25928;&#23398;&#20064;&#20559;&#22909;&#27010;&#24565;&#65292;&#36890;&#36807;&#23558;&#20559;&#22909;&#34920;&#31034;&#20026;&#31070;&#32463;&#31526;&#21495;&#31243;&#24207;&#24182;&#21033;&#29992;&#35270;&#35273;&#35299;&#26512;&#12289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#31243;&#24207;&#21512;&#25104;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#26469;&#23398;&#20064;&#20010;&#20154;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#20559;&#22909;&#23398;&#20064;&#38382;&#39064;&#65292;&#26088;&#22312;&#20174;&#35270;&#35273;&#36755;&#20837;&#20013;&#23398;&#20064;&#29992;&#25143;&#29305;&#23450;&#20559;&#22909;&#65288;&#20363;&#22914;&#65292;&#8220;&#22909;&#20572;&#36710;&#20301;&#8221;&#65292;&#8220;&#26041;&#20415;&#30340;&#19979;&#36710;&#20301;&#32622;&#8221;&#65289;&#12290;&#23613;&#31649;&#19982;&#23398;&#20064;&#20107;&#23454;&#27010;&#24565;&#65288;&#20363;&#22914;&#65292;&#8220;&#32418;&#33394;&#31435;&#26041;&#20307;&#8221;&#65289;&#30456;&#20284;&#65292;&#20294;&#20559;&#22909;&#23398;&#20064;&#26159;&#19968;&#20010;&#22522;&#26412;&#26356;&#21152;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#20027;&#35266;&#24615;&#36136;&#21644;&#20010;&#20154;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#30340;&#32570;&#20047;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Synapse&#30340;&#26032;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#26377;&#25928;&#22320;&#20174;&#26377;&#38480;&#28436;&#31034;&#20013;&#23398;&#20064;&#20559;&#22909;&#27010;&#24565;&#12290;Synapse&#23558;&#20559;&#22909;&#34920;&#31034;&#20026;&#22312;&#22270;&#20687;&#19978;&#36816;&#20316;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#65288;DSL&#65289;&#20013;&#30340;&#31070;&#32463;&#31526;&#21495;&#31243;&#24207;&#65292;&#24182;&#21033;&#29992;&#35270;&#35273;&#35299;&#26512;&#12289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#31243;&#24207;&#21512;&#25104;&#30340;&#26032;&#32452;&#21512;&#26469;&#23398;&#20064;&#20195;&#34920;&#20010;&#20154;&#20559;&#22909;&#30340;&#31243;&#24207;&#12290;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#20102;Synapse&#65292;&#21253;&#25324;&#19968;&#20010;&#20851;&#27880;&#19982;&#31227;&#21160;&#30456;&#20851;&#30340;&#29992;&#25143;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16689v1 Announce Type: cross  Abstract: This paper addresses the problem of preference learning, which aims to learn user-specific preferences (e.g., "good parking spot", "convenient drop-off location") from visual input. Despite its similarity to learning factual concepts (e.g., "red cube"), preference learning is a fundamentally harder problem due to its subjective nature and the paucity of person-specific training data. We address this problem using a new framework called Synapse, which is a neuro-symbolic approach designed to efficiently learn preferential concepts from limited demonstrations. Synapse represents preferences as neuro-symbolic programs in a domain-specific language (DSL) that operates over images, and leverages a novel combination of visual parsing, large language models, and program synthesis to learn programs representing individual preferences. We evaluate Synapse through extensive experimentation including a user case study focusing on mobility-related
&lt;/p&gt;</description></item><item><title>&#33258;&#21160;&#39550;&#39542;&#20013;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#23433;&#20840;&#24433;&#21709;&#23545;&#20110;&#30830;&#20445;&#36710;&#36742;&#33258;&#21160;&#21270;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#24403;&#21069;&#30740;&#31350;&#20013;&#23433;&#20840;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#24448;&#24448;&#34987;&#20998;&#24320;&#30752;&#12290;</title><link>https://arxiv.org/abs/2403.12176</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#20013;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#23433;&#20840;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Safety Implications of Explainable Artificial Intelligence in End-to-End Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12176
&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#20013;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#23433;&#20840;&#24433;&#21709;&#23545;&#20110;&#30830;&#20445;&#36710;&#36742;&#33258;&#21160;&#21270;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#24403;&#21069;&#30740;&#31350;&#20013;&#23433;&#20840;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#24448;&#24448;&#34987;&#20998;&#24320;&#30752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26411;&#31471;&#21040;&#26411;&#31471;&#23398;&#20064;&#31649;&#36947;&#27491;&#22312;&#36880;&#28176;&#25913;&#21464;&#39640;&#24230;&#33258;&#20027;&#36710;&#36742;&#30340;&#25345;&#32493;&#21457;&#23637;&#65292;&#36825;&#20027;&#35201;&#24402;&#21151;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#27493;&#12289;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#20197;&#21450;&#32508;&#21512;&#20256;&#24863;&#22120;&#35774;&#22791;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#24403;&#20195;&#23398;&#20064;&#26041;&#27861;&#22312;&#23454;&#26102;&#20915;&#31574;&#20013;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#22952;&#30861;&#20102;&#29992;&#25143;&#30340;&#20449;&#20219;&#65292;&#24182;&#20943;&#24369;&#20102;&#36825;&#31867;&#36710;&#36742;&#30340;&#24191;&#27867;&#37096;&#32626;&#21644;&#21830;&#19994;&#21270;&#12290;&#27492;&#22806;&#65292;&#24403;&#36825;&#20123;&#27773;&#36710;&#21442;&#19982;&#25110;&#23548;&#33268;&#20132;&#36890;&#20107;&#25925;&#26102;&#65292;&#38382;&#39064;&#20250;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#12290;&#36825;&#31181;&#32570;&#28857;&#20174;&#31038;&#20250;&#21644;&#27861;&#24459;&#30340;&#35282;&#24230;&#24341;&#36215;&#20102;&#20005;&#37325;&#30340;&#23433;&#20840;&#25285;&#24551;&#12290;&#22240;&#27492;&#65292;&#22312;&#26411;&#31471;&#21040;&#26411;&#31471;&#33258;&#21160;&#39550;&#39542;&#20013;&#35299;&#37322;&#24615;&#26159;&#20419;&#36827;&#36710;&#36742;&#33258;&#21160;&#21270;&#23433;&#20840;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#24403;&#20170;&#26368;&#20808;&#36827;&#25216;&#26415;&#20013;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#23558;&#33258;&#21160;&#39550;&#39542;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#20998;&#24320;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12176v1 Announce Type: cross  Abstract: The end-to-end learning pipeline is gradually creating a paradigm shift in the ongoing development of highly autonomous vehicles, largely due to advances in deep learning, the availability of large-scale training datasets, and improvements in integrated sensor devices. However, a lack of interpretability in real-time decisions with contemporary learning methods impedes user trust and attenuates the widespread deployment and commercialization of such vehicles. Moreover, the issue is exacerbated when these cars are involved in or cause traffic accidents. Such drawback raises serious safety concerns from societal and legal perspectives. Consequently, explainability in end-to-end autonomous driving is essential to enable the safety of vehicular automation. However, the safety and explainability aspects of autonomous driving have generally been investigated disjointly by researchers in today's state of the art. In this paper, we aim to brid
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#39318;&#27425;&#24341;&#20837;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#25351;&#20196;&#38169;&#35823;&#65292;&#32771;&#34385;&#21040;&#28508;&#22312;&#30340;&#20154;&#31867;&#21407;&#22240;&#65292;&#20197;&#35780;&#20272;&#36830;&#32493;&#29615;&#22659;&#20013; VLN &#31995;&#32479;&#30340;&#20581;&#22766;&#24615;</title><link>https://arxiv.org/abs/2403.10700</link><description>&lt;p&gt;
&#27880;&#24847;&#38169;&#35823;&#65281;&#26816;&#27979;&#21644;&#23450;&#20301;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#20013;&#30340;&#25351;&#20196;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Mind the Error! Detection and Localization of Instruction Errors in Vision-and-Language Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10700
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#39318;&#27425;&#24341;&#20837;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#25351;&#20196;&#38169;&#35823;&#65292;&#32771;&#34385;&#21040;&#28508;&#22312;&#30340;&#20154;&#31867;&#21407;&#22240;&#65292;&#20197;&#35780;&#20272;&#36830;&#32493;&#29615;&#22659;&#20013; VLN &#31995;&#32479;&#30340;&#20581;&#22766;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Vision-and-Language Navigation in Continuous Environments (VLN-CE) &#26159;&#19968;&#39033;&#30452;&#35266;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20307;&#39564;&#26234;&#33021;&#20219;&#21153;&#12290;&#20195;&#29702;&#20154;&#34987;&#35201;&#27714;&#36890;&#36807;&#25191;&#34892;&#19968;&#31995;&#21015;&#20302;&#32423;&#21160;&#20316;&#12289;&#36981;&#24490;&#19968;&#31995;&#21015;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26469;&#23548;&#33322;&#21040;&#30446;&#26631;&#30446;&#26631;&#12290;&#25152;&#26377;&#25991;&#29486;&#20013;&#30340; VLN-CE &#26041;&#27861;&#37117;&#20551;&#35774;&#35821;&#35328;&#25351;&#20196;&#26159;&#20934;&#30830;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#20154;&#31867;&#32473;&#20986;&#30340;&#25351;&#20196;&#21487;&#33021;&#30001;&#20110;&#19981;&#20934;&#30830;&#30340;&#35760;&#24518;&#25110;&#28151;&#28102;&#32780;&#21253;&#21547;&#31354;&#38388;&#29615;&#22659;&#25551;&#36848;&#20013;&#30340;&#38169;&#35823;&#12290;&#24403;&#21069; VLN-CE &#22522;&#20934;&#27809;&#26377;&#35299;&#20915;&#36825;&#31181;&#24773;&#20917;&#65292;&#20351;&#24471; VLN-CE &#20013;&#30340;&#26368;&#26032;&#26041;&#27861;&#22312;&#38754;&#23545;&#26469;&#33258;&#20154;&#31867;&#29992;&#25143;&#30340;&#38169;&#35823;&#25351;&#20196;&#26102;&#21464;&#24471;&#33030;&#24369;&#12290;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#20010;&#24341;&#20837;&#21508;&#31181;&#31867;&#22411;&#25351;&#20196;&#38169;&#35823;&#32771;&#34385;&#28508;&#22312;&#20154;&#31867;&#21407;&#22240;&#30340;&#26032;&#22411;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#35813;&#22522;&#20934;&#25968;&#25454;&#38598;&#20026;&#36830;&#32493;&#29615;&#22659;&#20013;&#30340; VLN &#31995;&#32479;&#30340;&#20581;&#22766;&#24615;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040; noticeable...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10700v1 Announce Type: cross  Abstract: Vision-and-Language Navigation in Continuous Environments (VLN-CE) is one of the most intuitive yet challenging embodied AI tasks. Agents are tasked to navigate towards a target goal by executing a set of low-level actions, following a series of natural language instructions. All VLN-CE methods in the literature assume that language instructions are exact. However, in practice, instructions given by humans can contain errors when describing a spatial environment due to inaccurate memory or confusion. Current VLN-CE benchmarks do not address this scenario, making the state-of-the-art methods in VLN-CE fragile in the presence of erroneous instructions from human users. For the first time, we propose a novel benchmark dataset that introduces various types of instruction errors considering potential human causes. This benchmark provides valuable insight into the robustness of VLN systems in continuous environments. We observe a noticeable 
&lt;/p&gt;</description></item></channel></rss>