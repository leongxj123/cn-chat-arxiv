<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#22312;&#32447;POMDP&#35268;&#21010;&#20013;&#19968;&#20010;&#31616;&#21270;&#35299;&#20915;&#26041;&#26696;&#19982;&#29702;&#35770;&#19978;&#26368;&#20248;&#35299;&#20043;&#38388;&#30340;&#30830;&#23450;&#24615;&#20851;&#31995;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#36817;&#20284;&#31639;&#27861;&#21482;&#33021;&#25552;&#20379;&#27010;&#29575;&#24615;&#21644;&#36890;&#24120;&#21576;&#29616;&#28176;&#36827;&#24615;&#20445;&#35777;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.01791</link><description>&lt;p&gt;
&#20855;&#26377;&#20219;&#24847;&#30830;&#23450;&#24615;&#20445;&#35777;&#30340;&#22312;&#32447;POMDP&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Online POMDP Planning with Anytime Deterministic Guarantees. (arXiv:2310.01791v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#22312;&#32447;POMDP&#35268;&#21010;&#20013;&#19968;&#20010;&#31616;&#21270;&#35299;&#20915;&#26041;&#26696;&#19982;&#29702;&#35770;&#19978;&#26368;&#20248;&#35299;&#20043;&#38388;&#30340;&#30830;&#23450;&#24615;&#20851;&#31995;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#36817;&#20284;&#31639;&#27861;&#21482;&#33021;&#25552;&#20379;&#27010;&#29575;&#24615;&#21644;&#36890;&#24120;&#21576;&#29616;&#28176;&#36827;&#24615;&#20445;&#35777;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#33258;&#20027;&#26234;&#33021;&#20307;&#32463;&#24120;&#36935;&#21040;&#19981;&#30830;&#23450;&#24615;&#24182;&#22522;&#20110;&#19981;&#23436;&#25972;&#20449;&#24687;&#20570;&#20986;&#20915;&#31574;&#12290;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#35268;&#21010;&#21487;&#20197;&#20351;&#29992;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#36827;&#34892;&#25968;&#23398;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#23547;&#25214;POMDP&#30340;&#26368;&#20248;&#35268;&#21010;&#22312;&#35745;&#31639;&#19978;&#26159;&#26114;&#36149;&#30340;&#65292;&#21482;&#26377;&#22312;&#23567;&#35268;&#27169;&#20219;&#21153;&#20013;&#21487;&#34892;&#12290;&#36817;&#24180;&#26469;&#65292;&#36817;&#20284;&#31639;&#27861;&#65288;&#22914;&#26641;&#25628;&#32034;&#21644;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#65289;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#36739;&#22823;&#38382;&#39064;&#30340;&#20808;&#36827;POMDP&#27714;&#35299;&#22120;&#12290;&#23613;&#31649;&#36825;&#20123;&#31639;&#27861;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#20165;&#25552;&#20379;&#27010;&#29575;&#24615;&#21644;&#36890;&#24120;&#21576;&#29616;&#28176;&#36827;&#24615;&#20445;&#35777;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#20381;&#36182;&#20110;&#37319;&#26679;&#30340;&#32536;&#25925;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#20010;&#31616;&#21270;&#35299;&#20915;&#26041;&#26696;&#19982;&#29702;&#35770;&#19978;&#26368;&#20248;&#35299;&#20043;&#38388;&#30340;&#30830;&#23450;&#24615;&#20851;&#31995;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#36873;&#25321;&#19968;&#32452;&#35266;&#27979;&#20197;&#22312;&#35745;&#31639;&#27599;&#20010;&#21518;&#39564;&#33410;&#28857;&#26102;&#20998;&#25903;&#30340;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agents operating in real-world scenarios frequently encounter uncertainty and make decisions based on incomplete information. Planning under uncertainty can be mathematically formalized using partially observable Markov decision processes (POMDPs). However, finding an optimal plan for POMDPs can be computationally expensive and is feasible only for small tasks. In recent years, approximate algorithms, such as tree search and sample-based methodologies, have emerged as state-of-the-art POMDP solvers for larger problems. Despite their effectiveness, these algorithms offer only probabilistic and often asymptotic guarantees toward the optimal solution due to their dependence on sampling. To address these limitations, we derive a deterministic relationship between a simplified solution that is easier to obtain and the theoretically optimal one. First, we derive bounds for selecting a subset of the observations to branch from while computing a complete belief at each posterior nod
&lt;/p&gt;</description></item></channel></rss>