<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#20351;&#29992;Transformer&#30340;&#28145;&#24230;&#27169;&#20223;&#23398;&#20064;&#32467;&#26500;&#25104;&#21151;&#35299;&#20915;&#20102;&#21452;&#33218;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#19981;&#20339;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2108.00385</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#21452;&#33218;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#28145;&#24230;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transformer-based deep imitation learning for dual-arm robot manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2108.00385
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Transformer&#30340;&#28145;&#24230;&#27169;&#20223;&#23398;&#20064;&#32467;&#26500;&#25104;&#21151;&#35299;&#20915;&#20102;&#21452;&#33218;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#19981;&#20339;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#27169;&#20223;&#23398;&#20064;&#23545;&#35299;&#20915;&#29087;&#32451;&#25805;&#20316;&#20219;&#21153;&#20855;&#26377;&#28508;&#21147;&#65292;&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#29615;&#22659;&#27169;&#22411;&#21644;&#39044;&#32534;&#31243;&#30340;&#26426;&#22120;&#20154;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#21452;&#33218;&#25805;&#20316;&#20219;&#21153;&#20173;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#21452;&#33218;&#25805;&#20316;&#35774;&#32622;&#20013;&#65292;&#30001;&#20110;&#38468;&#21152;&#26426;&#22120;&#20154;&#25805;&#20316;&#22120;&#24341;&#36215;&#30340;&#29366;&#24577;&#32500;&#24230;&#22686;&#21152;&#65292;&#23548;&#33268;&#20102;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#19981;&#20339;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#26426;&#21046;&#35745;&#31639;&#39034;&#24207;&#36755;&#20837;&#20013;&#20803;&#32032;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#19987;&#27880;&#20110;&#37325;&#35201;&#20803;&#32032;&#12290;Transformer&#65292;&#20316;&#20026;&#33258;&#27880;&#24847;&#21147;&#26550;&#26500;&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#34987;&#24212;&#29992;&#20110;&#28145;&#24230;&#27169;&#20223;&#23398;&#20064;&#20013;&#65292;&#20197;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#21452;&#33218;&#25805;&#20316;&#20219;&#21153;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24050;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#30340;&#21452;&#33218;&#25805;&#20316;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#27169;&#20223;&#23398;&#20064;&#26550;&#26500;&#21487;&#20197;&#36827;&#34892;&#20851;&#27880;
&lt;/p&gt;
&lt;p&gt;
arXiv:2108.00385v2 Announce Type: replace-cross  Abstract: Deep imitation learning is promising for solving dexterous manipulation tasks because it does not require an environment model and pre-programmed robot behavior. However, its application to dual-arm manipulation tasks remains challenging. In a dual-arm manipulation setup, the increased number of state dimensions caused by the additional robot manipulators causes distractions and results in poor performance of the neural networks. We address this issue using a self-attention mechanism that computes dependencies between elements in a sequential input and focuses on important elements. A Transformer, a variant of self-attention architecture, is applied to deep imitation learning to solve dual-arm manipulation tasks in the real world. The proposed method has been tested on dual-arm manipulation tasks using a real robot. The experimental results demonstrated that the Transformer-based deep imitation learning architecture can attend 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#20154;&#31867;&#22522;&#20110;&#20957;&#35270;&#30340;&#21452;&#20998;&#36776;&#29575;&#35270;&#35273;&#36816;&#21160;&#25511;&#21046;&#31995;&#32479;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#27169;&#20223;&#23398;&#20064;&#35299;&#20915;&#39640;&#31934;&#24230;&#28789;&#24039;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2102.01295</link><description>&lt;p&gt;
&#22522;&#20110;&#20957;&#35270;&#30340;&#21452;&#20998;&#36776;&#29575;&#28145;&#24230;&#27169;&#20223;&#23398;&#20064;&#29992;&#20110;&#39640;&#31934;&#24230;&#28789;&#24039;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Gaze-based dual resolution deep imitation learning for high-precision dexterous robot manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2102.01295
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20154;&#31867;&#22522;&#20110;&#20957;&#35270;&#30340;&#21452;&#20998;&#36776;&#29575;&#35270;&#35273;&#36816;&#21160;&#25511;&#21046;&#31995;&#32479;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#27169;&#20223;&#23398;&#20064;&#35299;&#20915;&#39640;&#31934;&#24230;&#28789;&#24039;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#39640;&#31934;&#24230;&#25805;&#32437;&#20219;&#21153;&#65292;&#22914;&#31359;&#38024;&#24341;&#32447;&#65292;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#29983;&#29702;&#23398;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#20302;&#20998;&#36776;&#29575;&#22806;&#22260;&#35270;&#35273;&#21644;&#24555;&#36895;&#31227;&#21160;&#36830;&#25509;&#36215;&#26469;&#65292;&#23558;&#25163;&#20256;&#36865;&#21040;&#23545;&#35937;&#30340;&#38468;&#36817;&#65292;&#24182;&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#30340;&#20985;&#38519;&#35270;&#35273;&#26469;&#23454;&#29616;&#25163;&#31934;&#30830;&#23545;&#20934;&#23545;&#35937;&#12290;&#26412;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21463;&#20154;&#31867;&#22522;&#20110;&#20957;&#35270;&#30340;&#21452;&#20998;&#36776;&#29575;&#35270;&#35273;&#36816;&#21160;&#25511;&#21046;&#31995;&#32479;&#30340;&#21551;&#21457;&#65292;&#22522;&#20110;&#28145;&#24230;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#31359;&#38024;&#24341;&#32447;&#20219;&#21153;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35760;&#24405;&#20102;&#36828;&#31243;&#25805;&#20316;&#26426;&#22120;&#20154;&#30340;&#20154;&#31867;&#25805;&#20316;&#21592;&#30340;&#20957;&#35270;&#36816;&#21160;&#12290;&#28982;&#21518;&#65292;&#22312;&#38752;&#36817;&#30446;&#26631;&#26102;&#65292;&#25105;&#20204;&#20165;&#20351;&#29992;&#22260;&#32469;&#20957;&#35270;&#28857;&#30340;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#26469;&#31934;&#30830;&#25511;&#21046;&#32447;&#30340;&#20301;&#32622;&#12290;&#25105;&#20204;&#20351;&#29992;&#20302;&#20998;&#36776;&#29575;&#30340;&#22806;&#22260;&#22270;&#20687;&#21040;&#36798;&#30446;&#26631;&#38468;&#36817;&#12290;&#26412;&#30740;&#31350;&#33719;&#24471;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#31934;&#20934;&#30340;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
arXiv:2102.01295v3 Announce Type: replace-cross  Abstract: A high-precision manipulation task, such as needle threading, is challenging. Physiological studies have proposed connecting low-resolution peripheral vision and fast movement to transport the hand into the vicinity of an object, and using high-resolution foveated vision to achieve the accurate homing of the hand to the object. The results of this study demonstrate that a deep imitation learning based method, inspired by the gaze-based dual resolution visuomotor control system in humans, can solve the needle threading task. First, we recorded the gaze movements of a human operator who was teleoperating a robot. Then, we used only a high-resolution image around the gaze to precisely control the thread position when it was close to the target. We used a low-resolution peripheral image to reach the vicinity of the target. The experimental results obtained in this study demonstrate that the proposed method enables precise manipulat
&lt;/p&gt;</description></item></channel></rss>