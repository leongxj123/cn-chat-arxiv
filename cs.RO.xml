<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31038;&#20250;&#25972;&#21512;&#23548;&#33322;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#20154;&#30340;&#20114;&#21160;&#20351;&#26426;&#22120;&#20154;&#30340;&#31038;&#20132;&#34892;&#20026;&#33258;&#36866;&#24212;&#65292;&#24182;&#20174;&#20854;&#20182;&#22522;&#20110;DRL&#30340;&#23548;&#33322;&#26041;&#27861;&#20013;&#21306;&#20998;&#20986;&#20855;&#26377;&#26126;&#30830;&#39044;&#23450;&#20041;&#31038;&#20132;&#34892;&#20026;&#30340;&#31038;&#20250;&#24847;&#35782;&#26041;&#27861;&#21644;&#32570;&#20047;&#31038;&#20132;&#34892;&#20026;&#30340;&#31038;&#20250;&#30896;&#25758;&#22238;&#36991;&#12290;</title><link>https://arxiv.org/abs/2403.09793</link><description>&lt;p&gt;
&#31038;&#20250;&#25972;&#21512;&#23548;&#33322;&#65306;&#20855;&#26377;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31038;&#20132;&#34892;&#21160;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Socially Integrated Navigation: A Social Acting Robot with Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09793
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31038;&#20250;&#25972;&#21512;&#23548;&#33322;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#20154;&#30340;&#20114;&#21160;&#20351;&#26426;&#22120;&#20154;&#30340;&#31038;&#20132;&#34892;&#20026;&#33258;&#36866;&#24212;&#65292;&#24182;&#20174;&#20854;&#20182;&#22522;&#20110;DRL&#30340;&#23548;&#33322;&#26041;&#27861;&#20013;&#21306;&#20998;&#20986;&#20855;&#26377;&#26126;&#30830;&#39044;&#23450;&#20041;&#31038;&#20132;&#34892;&#20026;&#30340;&#31038;&#20250;&#24847;&#35782;&#26041;&#27861;&#21644;&#32570;&#20047;&#31038;&#20132;&#34892;&#20026;&#30340;&#31038;&#20250;&#30896;&#25758;&#22238;&#36991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#26426;&#22120;&#20154;&#27491;&#22312;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#25317;&#25380;&#22330;&#26223;&#65292;&#24182;&#25104;&#20026;&#25105;&#20204;&#31038;&#20250;&#30340;&#19968;&#37096;&#20998;&#12290;&#19968;&#20010;&#20855;&#26377;&#20010;&#20307;&#20154;&#31867;&#32771;&#34385;&#30340;&#31038;&#20250;&#21487;&#25509;&#21463;&#30340;&#23548;&#33322;&#34892;&#20026;&#23545;&#20110;&#21487;&#25193;&#23637;&#30340;&#24212;&#29992;&#21644;&#20154;&#31867;&#25509;&#21463;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26041;&#27861;&#26469;&#23398;&#20064;&#26426;&#22120;&#20154;&#30340;&#23548;&#33322;&#31574;&#30053;&#65292;&#24182;&#23545;&#26426;&#22120;&#20154;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#22797;&#26434;&#20132;&#20114;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#24314;&#35758;&#26681;&#25454;&#26426;&#22120;&#20154;&#23637;&#31034;&#30340;&#31038;&#20132;&#34892;&#20026;&#23558;&#29616;&#26377;&#22522;&#20110;DRL&#30340;&#23548;&#33322;&#26041;&#27861;&#20998;&#20026;&#20855;&#26377;&#32570;&#20047;&#31038;&#20132;&#34892;&#20026;&#30340;&#31038;&#20250;&#30896;&#25758;&#22238;&#36991;&#21644;&#20855;&#26377;&#26126;&#30830;&#39044;&#23450;&#20041;&#31038;&#20132;&#34892;&#20026;&#30340;&#31038;&#20250;&#24847;&#35782;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31038;&#20250;&#25972;&#21512;&#23548;&#33322;&#26041;&#27861;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#30340;&#31038;&#20132;&#34892;&#20026;&#26159;&#33258;&#36866;&#24212;&#30340;&#65292;&#24182;&#19988;&#26159;&#36890;&#36807;&#19982;&#20154;&#31867;&#30340;&#20114;&#21160;&#32780;&#20135;&#29983;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26500;&#24335;&#28304;&#33258;&#31038;&#20250;&#23398;&#23450;&#20041;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09793v1 Announce Type: cross  Abstract: Mobile robots are being used on a large scale in various crowded situations and become part of our society. The socially acceptable navigation behavior of a mobile robot with individual human consideration is an essential requirement for scalable applications and human acceptance. Deep Reinforcement Learning (DRL) approaches are recently used to learn a robot's navigation policy and to model the complex interactions between robots and humans. We propose to divide existing DRL-based navigation approaches based on the robot's exhibited social behavior and distinguish between social collision avoidance with a lack of social behavior and socially aware approaches with explicit predefined social behavior. In addition, we propose a novel socially integrated navigation approach where the robot's social behavior is adaptive and emerges from the interaction with humans. The formulation of our approach is derived from a sociological definition, 
&lt;/p&gt;</description></item><item><title>CGGM&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#31232;&#30095;&#24615;&#29983;&#25104;&#37051;&#25509;&#30697;&#38453;&#65292;&#35299;&#20915;&#20102;&#29289;&#32852;&#32593;&#32593;&#32476;&#20013;&#33410;&#28857;&#24322;&#24120;&#26816;&#27979;&#20013;&#33410;&#28857;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.17363</link><description>&lt;p&gt;
CGGM&#65306;&#19968;&#31181;&#20855;&#26377;&#33258;&#36866;&#24212;&#31232;&#30095;&#24615;&#30340;&#26465;&#20214;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#29289;&#32852;&#32593;&#32593;&#32476;&#20013;&#33410;&#28857;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
CGGM: A conditional graph generation model with adaptive sparsity for node anomaly detection in IoT networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17363
&lt;/p&gt;
&lt;p&gt;
CGGM&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#31232;&#30095;&#24615;&#29983;&#25104;&#37051;&#25509;&#30697;&#38453;&#65292;&#35299;&#20915;&#20102;&#29289;&#32852;&#32593;&#32593;&#32476;&#20013;&#33410;&#28857;&#24322;&#24120;&#26816;&#27979;&#20013;&#33410;&#28857;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22270;&#34987;&#24191;&#27867;&#29992;&#20110;&#26816;&#27979;&#29289;&#32852;&#32593;&#20013;&#33410;&#28857;&#30340;&#24322;&#24120;&#34892;&#20026;&#12290;&#29983;&#25104;&#27169;&#22411;&#36890;&#24120;&#29992;&#20110;&#35299;&#20915;&#21160;&#24577;&#22270;&#20013;&#33410;&#28857;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#38754;&#20020;&#30340;&#32422;&#26463;&#21253;&#25324;&#37051;&#25509;&#20851;&#31995;&#30340;&#21333;&#35843;&#24615;&#65292;&#20026;&#33410;&#28857;&#26500;&#24314;&#22810;&#32500;&#29305;&#24449;&#30340;&#22256;&#38590;&#65292;&#20197;&#21450;&#32570;&#20047;&#31471;&#21040;&#31471;&#29983;&#25104;&#22810;&#31867;&#33410;&#28857;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CGGM&#30340;&#26032;&#39062;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#29983;&#25104;&#23569;&#25968;&#31867;&#21035;&#20013;&#26356;&#22810;&#33410;&#28857;&#12290;&#36890;&#36807;&#33258;&#36866;&#24212;&#31232;&#30095;&#24615;&#29983;&#25104;&#37051;&#25509;&#30697;&#38453;&#30340;&#26426;&#21046;&#22686;&#24378;&#20102;&#20854;&#32467;&#26500;&#30340;&#28789;&#27963;&#24615;&#12290;&#29305;&#24449;&#29983;&#25104;&#27169;&#22359;&#21517;&#20026;&#22810;&#32500;&#29305;&#24449;&#29983;&#25104;&#22120;&#65288;MFG&#65289;&#65292;&#21487;&#29983;&#25104;&#21253;&#25324;&#25299;&#25169;&#20449;&#24687;&#22312;&#20869;&#30340;&#33410;&#28857;&#29305;&#24449;&#12290;&#26631;&#31614;&#34987;&#36716;&#25442;&#20026;&#23884;&#20837;&#21521;&#37327;&#65292;&#29992;&#20316;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17363v1 Announce Type: cross  Abstract: Dynamic graphs are extensively employed for detecting anomalous behavior in nodes within the Internet of Things (IoT). Generative models are often used to address the issue of imbalanced node categories in dynamic graphs. Nevertheless, the constraints it faces include the monotonicity of adjacency relationships, the difficulty in constructing multi-dimensional features for nodes, and the lack of a method for end-to-end generation of multiple categories of nodes. This paper presents a novel graph generation model, called CGGM, designed specifically to generate a larger number of nodes belonging to the minority class. The mechanism for generating an adjacency matrix, through adaptive sparsity, enhances flexibility in its structure. The feature generation module, called multidimensional features generator (MFG) to generate node features along with topological information. Labels are transformed into embedding vectors, serving as condition
&lt;/p&gt;</description></item><item><title>SoftMAC&#25552;&#20986;&#20102;&#19968;&#20010;&#19981;&#21516;&#20110;&#20197;&#24448;&#30340;&#21487;&#24494;&#20223;&#30495;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#36719;&#20307;&#12289;&#20851;&#33410;&#21018;&#20307;&#21644;&#34915;&#29289;&#32806;&#21512;&#22312;&#19968;&#36215;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#39044;&#27979;&#30340;&#25509;&#35302;&#27169;&#22411;&#21644;&#31359;&#36879;&#36861;&#36394;&#31639;&#27861;&#65292;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#31359;&#36879;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2312.03297</link><description>&lt;p&gt;
SoftMAC&#65306;&#22522;&#20110;&#39044;&#27979;&#25509;&#35302;&#27169;&#22411;&#21644;&#19982;&#20851;&#33410;&#21018;&#20307;&#21644;&#34915;&#29289;&#21452;&#21521;&#32806;&#21512;&#30340;&#21487;&#24494;&#36719;&#20307;&#20223;&#30495;
&lt;/p&gt;
&lt;p&gt;
SoftMAC: Differentiable Soft Body Simulation with Forecast-based Contact Model and Two-way Coupling with Articulated Rigid Bodies and Clothes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03297
&lt;/p&gt;
&lt;p&gt;
SoftMAC&#25552;&#20986;&#20102;&#19968;&#20010;&#19981;&#21516;&#20110;&#20197;&#24448;&#30340;&#21487;&#24494;&#20223;&#30495;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#36719;&#20307;&#12289;&#20851;&#33410;&#21018;&#20307;&#21644;&#34915;&#29289;&#32806;&#21512;&#22312;&#19968;&#36215;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#39044;&#27979;&#30340;&#25509;&#35302;&#27169;&#22411;&#21644;&#31359;&#36879;&#36861;&#36394;&#31639;&#27861;&#65292;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#31359;&#36879;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#24494;&#29289;&#29702;&#20223;&#30495;&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35299;&#20915;&#26426;&#22120;&#20154;&#30456;&#20851;&#38382;&#39064;&#30340;&#25928;&#29575;&#12290;&#20026;&#22312;&#21508;&#31181;&#26426;&#22120;&#20154;&#25805;&#32437;&#22330;&#26223;&#20013;&#24212;&#29992;&#21487;&#24494;&#20223;&#30495;&#65292;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#23558;&#21508;&#31181;&#26448;&#26009;&#38598;&#25104;&#21040;&#32479;&#19968;&#26694;&#26550;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SoftMAC&#65292;&#19968;&#20010;&#21487;&#24494;&#20223;&#30495;&#26694;&#26550;&#65292;&#23558;&#36719;&#20307;&#19982;&#20851;&#33410;&#21018;&#20307;&#21644;&#34915;&#29289;&#32806;&#21512;&#22312;&#19968;&#36215;&#12290;SoftMAC&#20351;&#29992;&#22522;&#20110;&#36830;&#32493;&#21147;&#23398;&#30340;&#26448;&#26009;&#28857;&#27861;&#26469;&#27169;&#25311;&#36719;&#20307;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#39044;&#27979;&#30340;MPM&#25509;&#35302;&#27169;&#22411;&#65292;&#26377;&#25928;&#20943;&#23569;&#20102;&#31359;&#36879;&#65292;&#32780;&#19981;&#20250;&#24341;&#20837;&#20854;&#20182;&#24322;&#24120;&#29616;&#35937;&#65292;&#22914;&#19981;&#33258;&#28982;&#30340;&#21453;&#24377;&#12290;&#20026;&#20102;&#23558;MPM&#31890;&#23376;&#19982;&#21487;&#21464;&#24418;&#21644;&#38750;&#20307;&#31215;&#34915;&#29289;&#32593;&#26684;&#32806;&#21512;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31359;&#36879;&#36861;&#36394;&#31639;&#27861;&#65292;&#37325;&#24314;&#23616;&#37096;&#21306;&#22495;&#30340;&#26377;&#31526;&#21495;&#36317;&#31163;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03297v2 Announce Type: replace-cross  Abstract: Differentiable physics simulation provides an avenue to tackle previously intractable challenges through gradient-based optimization, thereby greatly improving the efficiency of solving robotics-related problems. To apply differentiable simulation in diverse robotic manipulation scenarios, a key challenge is to integrate various materials in a unified framework. We present SoftMAC, a differentiable simulation framework that couples soft bodies with articulated rigid bodies and clothes. SoftMAC simulates soft bodies with the continuum-mechanics-based Material Point Method (MPM). We provide a novel forecast-based contact model for MPM, which effectively reduces penetration without introducing other artifacts like unnatural rebound. To couple MPM particles with deformable and non-volumetric clothes meshes, we also propose a penetration tracing algorithm that reconstructs the signed distance field in local area. Diverging from prev
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#35270;&#35273;&#36830;&#25509;&#21160;&#20316;&#21644;&#20854;&#25928;&#26524;&#30340;&#27010;&#24565;&#65288;CATE&#65289;&#65292;&#29992;&#20110;&#35270;&#39057;&#29702;&#35299;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#20219;&#21153;&#24418;&#24335;&#20135;&#29983;&#20102;&#25429;&#25417;&#30452;&#35266;&#21160;&#20316;&#29305;&#24615;&#30340;&#34920;&#31034;&#65292;&#20294;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#65292;&#20154;&#31867;&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#23427;&#20204;&#12290;&#35813;&#30740;&#31350;&#20026;&#26410;&#26469;&#30340;&#21162;&#21147;&#22880;&#23450;&#20102;&#22522;&#30784;&#65292;&#24182;&#24076;&#26395;&#33021;&#28608;&#21457;&#20986;&#39640;&#32423;&#24418;&#24335;&#21644;&#27169;&#22411;&#30340;&#28789;&#24863;&#12290;</title><link>http://arxiv.org/abs/2401.10805</link><description>&lt;p&gt;
&#23398;&#20064;&#35270;&#35273;&#36830;&#25509;&#21160;&#20316;&#21644;&#20854;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Learning to Visually Connect Actions and their Effects. (arXiv:2401.10805v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10805
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#35270;&#35273;&#36830;&#25509;&#21160;&#20316;&#21644;&#20854;&#25928;&#26524;&#30340;&#27010;&#24565;&#65288;CATE&#65289;&#65292;&#29992;&#20110;&#35270;&#39057;&#29702;&#35299;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#20219;&#21153;&#24418;&#24335;&#20135;&#29983;&#20102;&#25429;&#25417;&#30452;&#35266;&#21160;&#20316;&#29305;&#24615;&#30340;&#34920;&#31034;&#65292;&#20294;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#65292;&#20154;&#31867;&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#23427;&#20204;&#12290;&#35813;&#30740;&#31350;&#20026;&#26410;&#26469;&#30340;&#21162;&#21147;&#22880;&#23450;&#20102;&#22522;&#30784;&#65292;&#24182;&#24076;&#26395;&#33021;&#28608;&#21457;&#20986;&#39640;&#32423;&#24418;&#24335;&#21644;&#27169;&#22411;&#30340;&#28789;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35270;&#35273;&#36830;&#25509;&#21160;&#20316;&#21644;&#20854;&#25928;&#26524;&#65288;CATE&#65289;&#30340;&#26032;&#27010;&#24565;&#65292;&#29992;&#20110;&#35270;&#39057;&#29702;&#35299;&#12290;CATE&#21487;&#20197;&#22312;&#20219;&#21153;&#35268;&#21010;&#21644;&#20174;&#31034;&#33539;&#20013;&#23398;&#20064;&#31561;&#39046;&#22495;&#20013;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#22522;&#20110;CATE&#30340;&#20219;&#21153;&#24418;&#24335;&#65292;&#22914;&#21160;&#20316;&#36873;&#25321;&#21644;&#21160;&#20316;&#25351;&#23450;&#65292;&#20854;&#20013;&#35270;&#39057;&#29702;&#35299;&#27169;&#22411;&#20197;&#35821;&#20041;&#21644;&#32454;&#31890;&#24230;&#30340;&#26041;&#24335;&#36830;&#25509;&#21160;&#20316;&#21644;&#25928;&#26524;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#19981;&#21516;&#30340;&#24418;&#24335;&#20135;&#29983;&#20102;&#25429;&#25417;&#30452;&#35266;&#21160;&#20316;&#29305;&#24615;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#21508;&#31181;&#22522;&#32447;&#27169;&#22411;&#29992;&#20110;&#21160;&#20316;&#36873;&#25321;&#21644;&#21160;&#20316;&#25351;&#23450;&#12290;&#23613;&#31649;&#20219;&#21153;&#20855;&#26377;&#30452;&#35266;&#24615;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#27169;&#22411;&#22256;&#38590;&#37325;&#37325;&#65292;&#20154;&#31867;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#23427;&#20204;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20026;&#26410;&#26469;&#30340;&#21162;&#21147;&#22880;&#23450;&#22522;&#30784;&#65292;&#23637;&#31034;&#20102;&#36830;&#25509;&#35270;&#39057;&#29702;&#35299;&#20013;&#21160;&#20316;&#21644;&#25928;&#26524;&#30340;&#28789;&#27963;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#24076;&#26395;&#33021;&#28608;&#21457;&#20986;&#39640;&#32423;&#24418;&#24335;&#21644;&#27169;&#22411;&#30340;&#28789;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce the novel concept of visually Connecting Actions and Their Effects (CATE) in video understanding. CATE can have applications in areas like task planning and learning from demonstration. We propose different CATE-based task formulations, such as action selection and action specification, where video understanding models connect actions and effects at semantic and fine-grained levels. We observe that different formulations produce representations capturing intuitive action properties. We also design various baseline models for action selection and action specification. Despite the intuitive nature of the task, we observe that models struggle, and humans outperform them by a large margin. The study aims to establish a foundation for future efforts, showcasing the flexibility and versatility of connecting actions and effects in video understanding, with the hope of inspiring advanced formulations and models.
&lt;/p&gt;</description></item></channel></rss>