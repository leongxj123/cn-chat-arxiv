<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>Co-NavGPT&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20840;&#23616;&#35268;&#21010;&#22120;&#65292;&#23454;&#29616;&#22810;&#26426;&#22120;&#20154;&#21512;&#20316;&#30340;&#35270;&#35273;&#30446;&#26631;&#23548;&#33322;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#36229;&#36234;&#29616;&#26377;&#27169;&#22411;&#30340;&#25104;&#21151;&#29575;&#21644;&#25928;&#29575;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.07937</link><description>&lt;p&gt;
Co-NavGPT: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#26426;&#22120;&#20154;&#21512;&#20316;&#35270;&#35273;&#35821;&#20041;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Co-NavGPT: Multi-Robot Cooperative Visual Semantic Navigation using Large Language Models. (arXiv:2310.07937v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07937
&lt;/p&gt;
&lt;p&gt;
Co-NavGPT&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20840;&#23616;&#35268;&#21010;&#22120;&#65292;&#23454;&#29616;&#22810;&#26426;&#22120;&#20154;&#21512;&#20316;&#30340;&#35270;&#35273;&#30446;&#26631;&#23548;&#33322;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#36229;&#36234;&#29616;&#26377;&#27169;&#22411;&#30340;&#25104;&#21151;&#29575;&#21644;&#25928;&#29575;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#32423;&#20154;&#26426;&#20132;&#20114;&#20219;&#21153;&#20013;&#65292;&#23545;&#20110;&#33258;&#20027;&#26426;&#22120;&#20154;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#36827;&#34892;&#35270;&#35273;&#30446;&#26631;&#23548;&#33322;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#36807;&#21435;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#26041;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#37117;&#26159;&#35774;&#35745;&#29992;&#20110;&#21333;&#19968;&#26426;&#22120;&#20154;&#25805;&#20316;&#65292;&#36825;&#24448;&#24448;&#30001;&#20110;&#29615;&#22659;&#22797;&#26434;&#24615;&#32780;&#23548;&#33268;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#38477;&#20302;&#12290;&#27492;&#22806;&#65292;&#23398;&#20064;&#22810;&#26426;&#22120;&#20154;&#21327;&#20316;&#30340;&#31574;&#30053;&#38656;&#35201;&#36164;&#28304;&#23494;&#38598;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Co-NavGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#22810;&#26426;&#22120;&#20154;&#21512;&#20316;&#35270;&#35273;&#30446;&#26631;&#23548;&#33322;&#30340;&#20840;&#23616;&#35268;&#21010;&#22120;&#12290;Co-NavGPT&#23558;&#25506;&#32034;&#30340;&#29615;&#22659;&#25968;&#25454;&#32534;&#30721;&#20026;&#25552;&#31034;&#65292;&#22686;&#24378;LLMs&#23545;&#22330;&#26223;&#30340;&#29702;&#35299;&#12290;&#28982;&#21518;&#65292;&#23427;&#20026;&#27599;&#20010;&#26426;&#22120;&#20154;&#20998;&#37197;&#25506;&#32034;&#21069;&#27839;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#30446;&#26631;&#25628;&#32034;&#12290;&#22312;Habitat-Matterport 3D (HM3D)&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Co-NavGPT&#22312;&#25104;&#21151;&#29575;&#21644;&#25928;&#29575;&#26041;&#38754;&#36229;&#36807;&#20102;&#29616;&#26377;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#23398;&#20064;&#36807;&#31243;&#65292;&#23637;&#31034;&#20102;LLMs&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In advanced human-robot interaction tasks, visual target navigation is crucial for autonomous robots navigating unknown environments. While numerous approaches have been developed in the past, most are designed for single-robot operations, which often suffer from reduced efficiency and robustness due to environmental complexities. Furthermore, learning policies for multi-robot collaboration are resource-intensive. To address these challenges, we propose Co-NavGPT, an innovative framework that integrates Large Language Models (LLMs) as a global planner for multi-robot cooperative visual target navigation. Co-NavGPT encodes the explored environment data into prompts, enhancing LLMs' scene comprehension. It then assigns exploration frontiers to each robot for efficient target search. Experimental results on Habitat-Matterport 3D (HM3D) demonstrate that Co-NavGPT surpasses existing models in success rates and efficiency without any learning process, demonstrating the vast potential of LLMs
&lt;/p&gt;</description></item></channel></rss>