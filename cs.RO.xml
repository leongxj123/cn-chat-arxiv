<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#38656;&#28436;&#31034;&#30340;&#20998;&#23618;&#35268;&#21010;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#38271;&#26102;&#38388;&#20219;&#21153;&#65292;&#20026;&#27599;&#20010;&#38454;&#27573;&#25552;&#20379;&#24037;&#20855;&#21517;&#31216;&#21644;Python&#20195;&#30721;&#12290;</title><link>https://arxiv.org/abs/2311.02787</link><description>&lt;p&gt;
&#21046;&#20316;&#19968;&#20010;&#29980;&#29980;&#22280;&#65306;&#29992;&#20110;&#38646;&#26679;&#26412;&#21464;&#24418;&#25805;&#32437;&#30340;&#20998;&#23618;EMD&#31354;&#38388;&#35268;&#21010;&#19982;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Make a Donut: Hierarchical EMD-Space Planning for Zero-Shot Deformable Manipulation with Tools
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02787
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#38656;&#28436;&#31034;&#30340;&#20998;&#23618;&#35268;&#21010;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#38271;&#26102;&#38388;&#20219;&#21153;&#65292;&#20026;&#27599;&#20010;&#38454;&#27573;&#25552;&#20379;&#24037;&#20855;&#21517;&#31216;&#21644;Python&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#24418;&#29289;&#20307;&#25805;&#32437;&#26159;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#26368;&#36855;&#20154;&#21448;&#26368;&#33392;&#24040;&#30340;&#25361;&#25112;&#20043;&#19968;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#25216;&#26415;&#20027;&#35201;&#20381;&#36182;&#20110;&#36890;&#36807;&#28436;&#31034;&#23398;&#20064;&#28508;&#22312;&#21160;&#24577;&#65292;&#36890;&#24120;&#34920;&#31034;&#20026;&#31890;&#23376;&#25110;&#22270;&#20687;&#20043;&#19968;&#65292;&#20294;&#23384;&#22312;&#19968;&#20010;&#37325;&#35201;&#38480;&#21046;&#65306;&#33719;&#21462;&#36866;&#24403;&#30340;&#28436;&#31034;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38271;&#26102;&#38388;&#20219;&#21153;&#65292;&#21487;&#33021;&#26159;&#22256;&#38590;&#30340;&#12290;&#27492;&#22806;&#65292;&#23436;&#20840;&#22522;&#20110;&#28436;&#31034;&#36827;&#34892;&#23398;&#20064;&#21487;&#33021;&#20250;&#38459;&#30861;&#27169;&#22411;&#36229;&#36234;&#28436;&#31034;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#38656;&#28436;&#31034;&#30340;&#20998;&#23618;&#35268;&#21010;&#26041;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#30340;&#38271;&#26102;&#38388;&#20219;&#21153;&#32780;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#12290;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#34920;&#36798;&#19982;&#25351;&#23450;&#20219;&#21153;&#23545;&#24212;&#30340;&#39640;&#23618;&#12289;&#38454;&#27573;-by-&#38454;&#27573;&#35745;&#21010;&#12290;&#23545;&#20110;&#27599;&#20010;&#21333;&#29420;&#38454;&#27573;&#65292;LLM&#25552;&#20379;&#24037;&#20855;&#30340;&#21517;&#31216;&#21644;Python&#20195;&#30721;&#65292;&#20197;&#21046;&#20316;&#20013;&#38388;&#23376;&#30446;&#26631;&#28857;&#20113;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02787v2 Announce Type: replace-cross  Abstract: Deformable object manipulation stands as one of the most captivating yet formidable challenges in robotics. While previous techniques have predominantly relied on learning latent dynamics through demonstrations, typically represented as either particles or images, there exists a pertinent limitation: acquiring suitable demonstrations, especially for long-horizon tasks, can be elusive. Moreover, basing learning entirely on demonstrations can hamper the model's ability to generalize beyond the demonstrated tasks. In this work, we introduce a demonstration-free hierarchical planning approach capable of tackling intricate long-horizon tasks without necessitating any training. We employ large language models (LLMs) to articulate a high-level, stage-by-stage plan corresponding to a specified task. For every individual stage, the LLM provides both the tool's name and the Python code to craft intermediate subgoal point clouds. With the
&lt;/p&gt;</description></item></channel></rss>