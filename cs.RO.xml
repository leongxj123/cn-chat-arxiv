<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#35813;&#39033;&#30446;&#23558;&#38750;&#32447;&#24615;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20195;&#29702;&#24341;&#20837;&#26080;&#20154;&#26426;&#25511;&#21046;&#20013;&#65292;&#21462;&#20195;&#20256;&#32479;&#32447;&#24615;PID&#25511;&#21046;&#22120;&#65292;&#23454;&#29616;&#20102;&#26080;&#32541;&#36807;&#28193;&#12289;&#25552;&#39640;&#21709;&#24212;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#32467;&#21512;PPO&#31574;&#30053;&#35757;&#32451;DRL&#20195;&#29702;&#65292;&#24182;&#21033;&#29992;&#39640;&#31934;&#24230;&#36319;&#36394;&#31995;&#32479;&#25552;&#39640;&#33258;&#20027;&#39134;&#34892;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2404.00204</link><description>&lt;p&gt;
&#22522;&#20110;PPO&#30340;DRL&#33258;&#35843;PID&#38750;&#32447;&#24615;&#26080;&#20154;&#26426;&#25511;&#21046;&#22120;&#29992;&#20110;&#31283;&#20581;&#33258;&#20027;&#39134;&#34892;
&lt;/p&gt;
&lt;p&gt;
A PPO-based DRL Auto-Tuning Nonlinear PID Drone Controller for Robust Autonomous Flights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00204
&lt;/p&gt;
&lt;p&gt;
&#35813;&#39033;&#30446;&#23558;&#38750;&#32447;&#24615;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20195;&#29702;&#24341;&#20837;&#26080;&#20154;&#26426;&#25511;&#21046;&#20013;&#65292;&#21462;&#20195;&#20256;&#32479;&#32447;&#24615;PID&#25511;&#21046;&#22120;&#65292;&#23454;&#29616;&#20102;&#26080;&#32541;&#36807;&#28193;&#12289;&#25552;&#39640;&#21709;&#24212;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#32467;&#21512;PPO&#31574;&#30053;&#35757;&#32451;DRL&#20195;&#29702;&#65292;&#24182;&#21033;&#29992;&#39640;&#31934;&#24230;&#36319;&#36394;&#31995;&#32479;&#25552;&#39640;&#33258;&#20027;&#39134;&#34892;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#39033;&#30446;&#26088;&#22312;&#36890;&#36807;&#23558;&#38750;&#32447;&#24615;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20195;&#29702;&#20316;&#20026;&#20256;&#32479;&#32447;&#24615;&#27604;&#20363;&#31215;&#20998;&#24494;&#20998;&#65288;PID&#65289;&#25511;&#21046;&#22120;&#30340;&#26367;&#20195;&#21697;&#65292;&#20174;&#32780;&#24443;&#24213;&#25913;&#21464;&#26080;&#20154;&#26426;&#39134;&#34892;&#25511;&#21046;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#25163;&#21160;&#21644;&#33258;&#20027;&#27169;&#24335;&#20043;&#38388;&#23454;&#29616;&#26080;&#32541;&#36807;&#28193;&#65292;&#25552;&#39640;&#21709;&#24212;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#22312;Gazebo&#27169;&#25311;&#22120;&#20013;&#21033;&#29992;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#26469;&#35757;&#32451;DRL&#20195;&#29702;&#12290;&#28155;&#21152;20000&#32654;&#20803;&#30340;&#23460;&#20869;Vicon&#36319;&#36394;&#31995;&#32479;&#25552;&#20379;&lt;1mm&#30340;&#23450;&#20301;&#31934;&#24230;&#65292;&#26174;&#30528;&#25552;&#39640;&#20102;&#33258;&#20027;&#39134;&#34892;&#31934;&#24230;&#12290;&#20026;&#20102;&#22312;&#26368;&#30701;&#30340;&#26080;&#30896;&#25758;&#36712;&#36857;&#20013;&#23548;&#33322;&#26080;&#20154;&#26426;&#65292;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#19968;&#20010;&#19977;&#32500;A*&#36335;&#24452;&#35268;&#21010;&#22120;&#24182;&#25104;&#21151;&#22320;&#23558;&#20854;&#23454;&#26045;&#21040;&#23454;&#38469;&#39134;&#34892;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00204v1 Announce Type: cross  Abstract: This project aims to revolutionize drone flight control by implementing a nonlinear Deep Reinforcement Learning (DRL) agent as a replacement for traditional linear Proportional Integral Derivative (PID) controllers. The primary objective is to seamlessly transition drones between manual and autonomous modes, enhancing responsiveness and stability. We utilize the Proximal Policy Optimization (PPO) reinforcement learning strategy within the Gazebo simulator to train the DRL agent. Adding a $20,000 indoor Vicon tracking system offers &lt;1mm positioning accuracy, which significantly improves autonomous flight precision. To navigate the drone in the shortest collision-free trajectory, we also build a 3 dimensional A* path planner and implement it into the real flight successfully.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#19968;&#33268;&#24615;&#23547;&#27714;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#27809;&#26377;&#26126;&#30830;&#25351;&#23548;&#30340;&#24773;&#20917;&#19979;&#65292;&#26234;&#33021;&#20307;&#20027;&#35201;&#20351;&#29992;&#24179;&#22343;&#31574;&#30053;&#36827;&#34892;&#19968;&#33268;&#24615;&#23547;&#27714;&#65292;&#21516;&#26102;&#36824;&#20998;&#26512;&#20102;&#26234;&#33021;&#20307;&#25968;&#37327;&#12289;&#26234;&#33021;&#20307;&#20010;&#24615;&#21644;&#32593;&#32476;&#25299;&#25169;&#23545;&#21327;&#21830;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.20151</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#19968;&#33268;&#24615;&#23547;&#27714;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Consensus Seeking via Large Language Models. (arXiv:2310.20151v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#19968;&#33268;&#24615;&#23547;&#27714;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#27809;&#26377;&#26126;&#30830;&#25351;&#23548;&#30340;&#24773;&#20917;&#19979;&#65292;&#26234;&#33021;&#20307;&#20027;&#35201;&#20351;&#29992;&#24179;&#22343;&#31574;&#30053;&#36827;&#34892;&#19968;&#33268;&#24615;&#23547;&#27714;&#65292;&#21516;&#26102;&#36824;&#20998;&#26512;&#20102;&#26234;&#33021;&#20307;&#25968;&#37327;&#12289;&#26234;&#33021;&#20307;&#20010;&#24615;&#21644;&#32593;&#32476;&#25299;&#25169;&#23545;&#21327;&#21830;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39537;&#21160;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#22312;&#21327;&#20316;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;&#19968;&#33268;&#24615;&#23547;&#27714;&#12290;&#24403;&#22810;&#20010;&#26234;&#33021;&#20307;&#19968;&#36215;&#24037;&#20316;&#26102;&#65292;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#23427;&#20204;&#22914;&#20309;&#36890;&#36807;&#26234;&#33021;&#20307;&#38388;&#30340;&#21327;&#21830;&#36798;&#25104;&#19968;&#33268;&#12290;&#20026;&#27492;&#65292;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#19968;&#20010;&#19968;&#33268;&#24615;&#23547;&#27714;&#20219;&#21153;&#65292;&#20854;&#20013;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#29366;&#24577;&#26159;&#19968;&#20010;&#25968;&#20540;&#65292;&#24182;&#19988;&#23427;&#20204;&#36890;&#36807;&#30456;&#20114;&#21327;&#21830;&#26469;&#36798;&#25104;&#19968;&#33268;&#20540;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#27809;&#26377;&#26126;&#30830;&#25351;&#23548;&#24212;&#37319;&#29992;&#21738;&#31181;&#31574;&#30053;&#26102;&#65292;LLM&#39537;&#21160;&#30340;&#26234;&#33021;&#20307;&#20027;&#35201;&#20351;&#29992;&#24179;&#22343;&#31574;&#30053;&#36827;&#34892;&#19968;&#33268;&#24615;&#23547;&#27714;&#65292;&#23613;&#31649;&#23427;&#20204;&#21487;&#33021;&#20598;&#23572;&#20250;&#20351;&#29992;&#20854;&#20182;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#20998;&#26512;&#20102;&#26234;&#33021;&#20307;&#25968;&#37327;&#12289;&#26234;&#33021;&#20307;&#20010;&#24615;&#21644;&#32593;&#32476;&#25299;&#25169;&#23545;&#21327;&#21830;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#30340;&#21457;&#29616;&#26377;&#26395;&#20026;&#29702;&#35299;LLM&#39537;&#21160;&#30340;&#22810;&#26234;&#33021;&#20307;&#34892;&#20026;&#22880;&#23450;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent systems driven by large language models (LLMs) have shown promising abilities for solving complex tasks in a collaborative manner. This work considers a fundamental problem in multi-agent collaboration: consensus seeking. When multiple agents work together, we are interested in how they can reach a consensus through inter-agent negotiation. To that end, this work studies a consensus-seeking task where the state of each agent is a numerical value and they negotiate with each other to reach a consensus value. It is revealed that when not explicitly directed on which strategy should be adopted, the LLM-driven agents primarily use the average strategy for consensus seeking although they may occasionally use some other strategies. Moreover, this work analyzes the impact of the agent number, agent personality, and network topology on the negotiation process. The findings reported in this work can potentially lay the foundations for understanding the behaviors of LLM-driven multi-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;TSformer-VO&#26041;&#27861;&#65292;&#23558;&#21333;&#30446;&#35270;&#35273;&#37324;&#31243;&#35745;&#20316;&#20026;&#19968;&#39033;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#24182;&#36890;&#36807;&#26102;&#31354;&#33258;&#27880;&#24847;&#26426;&#21046;&#20174;&#35270;&#39057;&#29255;&#27573;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#20197;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#36816;&#21160;&#20272;&#35745;&#65292;&#36798;&#21040;&#20102;&#26368;&#26032;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.06121</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#21333;&#30446;&#35270;&#35273;&#37324;&#31243;&#35745;&#65306;&#19968;&#31181;&#35270;&#39057;&#29702;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Transformer-based model for monocular visual odometry: a video understanding approach. (arXiv:2305.06121v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;TSformer-VO&#26041;&#27861;&#65292;&#23558;&#21333;&#30446;&#35270;&#35273;&#37324;&#31243;&#35745;&#20316;&#20026;&#19968;&#39033;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#24182;&#36890;&#36807;&#26102;&#31354;&#33258;&#27880;&#24847;&#26426;&#21046;&#20174;&#35270;&#39057;&#29255;&#27573;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#20197;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#36816;&#21160;&#20272;&#35745;&#65292;&#36798;&#21040;&#20102;&#26368;&#26032;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31227;&#21160;&#26426;&#22120;&#20154;&#21644;&#33258;&#20027;&#36710;&#36742;&#20013;&#65292;&#32473;&#23450;&#21333;&#20010;&#25668;&#20687;&#26426;&#22270;&#20687;&#20272;&#35745;&#25668;&#20687;&#26426;&#23039;&#21183;&#26159;&#19968;&#39033;&#20256;&#32479;&#20219;&#21153;&#12290;&#36825;&#20010;&#38382;&#39064;&#31216;&#20026;&#21333;&#30446;&#35270;&#35273;&#37324;&#31243;&#35745;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#38656;&#35201;&#38024;&#23545;&#29305;&#23450;&#22330;&#26223;&#36827;&#34892;&#24037;&#31243;&#21270;&#30340;&#20960;&#20309;&#26041;&#27861;&#12290;&#32463;&#36807;&#36866;&#24403;&#35757;&#32451;&#21644;&#36275;&#22815;&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#26159;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;&#12290;Transformer&#26550;&#26500;&#24050;&#32479;&#27835;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#30340;&#26368;&#21069;&#27839;&#65292;&#20363;&#22914;&#22270;&#20687;&#21644;&#35270;&#39057;&#29702;&#35299;&#12290;&#26412;&#25991;&#23558;&#21333;&#30446;&#35270;&#35273;&#37324;&#31243;&#35745;&#20316;&#20026;&#19968;&#39033;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#36827;&#34892;&#22788;&#29702;&#65292;&#20197;&#20272;&#35745;6-DoF&#25668;&#20687;&#26426;&#30340;&#23039;&#21183;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#26102;&#31354;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;TSformer-VO&#27169;&#22411;&#65292;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#20174;&#35270;&#39057;&#29255;&#27573;&#20013;&#25552;&#21462;&#29305;&#24449;&#24182;&#20272;&#35745;&#36816;&#21160;&#65292;&#19982;&#20960;&#20309;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#26368;&#26032;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the camera pose given images of a single camera is a traditional task in mobile robots and autonomous vehicles. This problem is called monocular visual odometry and it often relies on geometric approaches that require engineering effort for a specific scenario. Deep learning methods have shown to be generalizable after proper training and a considerable amount of available data. Transformer-based architectures have dominated the state-of-the-art in natural language processing and computer vision tasks, such as image and video understanding. In this work, we deal with the monocular visual odometry as a video understanding task to estimate the 6-DoF camera's pose. We contribute by presenting the TSformer-VO model based on spatio-temporal self-attention mechanisms to extract features from clips and estimate the motions in an end-to-end manner. Our approach achieved competitive state-of-the-art performance compared with geometry-based and deep learning-based methods on the KITTI
&lt;/p&gt;</description></item></channel></rss>