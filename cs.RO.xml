<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#36890;&#36807;&#23558;&#28436;&#31034;&#23398;&#20064;&#38598;&#25104;&#21040;&#36816;&#21160;&#29983;&#25104;&#20013;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#23454;&#26102;&#29983;&#25104;&#36866;&#24212;&#22797;&#26434;&#29615;&#22659;&#30340;&#36816;&#21160;</title><link>https://arxiv.org/abs/2403.15239</link><description>&lt;p&gt;
&#24341;&#23548;&#35299;&#30721;&#29992;&#20110;&#26426;&#22120;&#20154;&#36816;&#21160;&#29983;&#25104;&#21644;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Guided Decoding for Robot Motion Generation and Adaption
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15239
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#28436;&#31034;&#23398;&#20064;&#38598;&#25104;&#21040;&#36816;&#21160;&#29983;&#25104;&#20013;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#23454;&#26102;&#29983;&#25104;&#36866;&#24212;&#22797;&#26434;&#29615;&#22659;&#30340;&#36816;&#21160;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#38024;&#23545;&#20855;&#26377;&#38556;&#30861;&#29289;&#12289;&#36890;&#36807;&#28857;&#31561;&#22797;&#26434;&#29615;&#22659;&#19979;&#30340;&#39640;&#33258;&#30001;&#24230;&#26426;&#22120;&#20154;&#33218;&#36816;&#21160;&#29983;&#25104;&#38382;&#39064;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;&#36890;&#36807;&#23558;&#28436;&#31034;&#23398;&#20064;&#65288;LfD&#65289;&#38598;&#25104;&#21040;&#36816;&#21160;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#21462;&#24471;&#20102;&#35813;&#39046;&#22495;&#30340;&#37325;&#22823;&#36827;&#23637;&#12290;&#36825;&#31181;&#38598;&#25104;&#25903;&#25345;&#26426;&#22120;&#20154;&#24555;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#20801;&#35768;&#26426;&#22120;&#20154;&#20174;&#28436;&#31034;&#36712;&#36857;&#20013;&#23398;&#20064;&#21644;&#27867;&#21270;&#26469;&#20248;&#21270;&#31215;&#32047;&#30340;&#32463;&#39564;&#21033;&#29992;&#12290;&#25105;&#20204;&#22312;&#22823;&#37327;&#27169;&#25311;&#36712;&#36857;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#19968;&#20010;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21464;&#25442;&#22120;&#30340;transformer&#26550;&#26500;&#12290;&#36825;&#31181;&#22522;&#20110;&#26465;&#20214;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21464;&#25442;&#22120;&#30340;&#26550;&#26500;&#23398;&#20064;&#20102;&#22522;&#26412;&#30340;&#36816;&#21160;&#29983;&#25104;&#25216;&#33021;&#65292;&#24182;&#23558;&#20854;&#35843;&#25972;&#20197;&#28385;&#36275;&#36741;&#21161;&#20219;&#21153;&#21644;&#32422;&#26463;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#33258;&#22238;&#24402;&#26041;&#27861;&#23454;&#29616;&#20102;&#29289;&#29702;&#31995;&#32479;&#21453;&#39304;&#30340;&#23454;&#26102;&#38598;&#25104;&#65292;&#22686;&#24378;&#20102;&#36816;&#21160;&#29983;&#25104;&#30340;&#36866;&#24212;&#24615;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#20174;&#21021;&#22987;&#28857;&#21644;&#30446;&#26631;&#28857;&#29983;&#25104;&#36816;&#21160;&#65292;&#21516;&#26102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15239v1 Announce Type: cross  Abstract: We address motion generation for high-DoF robot arms in complex settings with obstacles, via points, etc. A significant advancement in this domain is achieved by integrating Learning from Demonstration (LfD) into the motion generation process. This integration facilitates rapid adaptation to new tasks and optimizes the utilization of accumulated expertise by allowing robots to learn and generalize from demonstrated trajectories.   We train a transformer architecture on a large dataset of simulated trajectories. This architecture, based on a conditional variational autoencoder transformer, learns essential motion generation skills and adapts these to meet auxiliary tasks and constraints. Our auto-regressive approach enables real-time integration of feedback from the physical system, enhancing the adaptability and efficiency of motion generation. We show that our model can generate motion from initial and target points, but also that it 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20855;&#36523;&#22522;&#30784;&#20808;&#39564;&#30340;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#26469;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.02635</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22522;&#30784;&#65306;&#26397;&#21521;&#20855;&#26377;&#22522;&#30784;&#20808;&#39564;&#36741;&#21161;&#30340;&#20855;&#36523;&#36890;&#29992;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Foundation Reinforcement Learning: towards Embodied Generalist Agents with Foundation Prior Assistance. (arXiv:2310.02635v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20855;&#36523;&#22522;&#30784;&#20808;&#39564;&#30340;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#26469;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#20204;&#24050;&#32463;&#34920;&#26126;&#65292;&#20174;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#25968;&#25454;&#20013;&#36827;&#34892;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#26159;&#26500;&#24314;&#36890;&#29992;&#27169;&#22411;&#30340;&#20851;&#38190;&#65292;&#27491;&#22914;&#22312;NLP&#20013;&#25152;&#35265;&#12290;&#20026;&#20102;&#26500;&#24314;&#20855;&#36523;&#36890;&#29992;&#26234;&#33021;&#20307;&#65292;&#25105;&#20204;&#21644;&#35768;&#22810;&#20854;&#20182;&#30740;&#31350;&#32773;&#20551;&#35774;&#36825;&#31181;&#22522;&#30784;&#20808;&#39564;&#20063;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#22914;&#20309;&#20197;&#36866;&#24403;&#30340;&#20855;&#20307;&#24418;&#24335;&#34920;&#31034;&#36825;&#20123;&#20855;&#36523;&#22522;&#30784;&#20808;&#39564;&#65292;&#20197;&#21450;&#23427;&#20204;&#24212;&#35813;&#22914;&#20309;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#30452;&#35266;&#26377;&#25928;&#30340;&#20855;&#36523;&#20808;&#39564;&#65292;&#21253;&#25324;&#22522;&#30784;&#31574;&#30053;&#12289;&#20215;&#20540;&#21644;&#25104;&#21151;&#22870;&#21169;&#12290;&#25152;&#25552;&#20986;&#30340;&#20808;&#39564;&#26159;&#22522;&#20110;&#30446;&#26631;&#26465;&#20214;&#30340;MDP&#12290;&#20026;&#20102;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23454;&#20363;&#21270;&#20102;&#19968;&#20010;&#30001;&#36825;&#20123;&#20808;&#39564;&#36741;&#21161;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#65292;&#31216;&#20043;&#20026;&#22522;&#30784;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;FAC&#65289;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#21629;&#21517;&#20026;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#65288;FRL&#65289;&#65292;&#22240;&#20026;&#23427;&#23436;&#20840;&#20381;&#36182;&#20110;&#20855;&#36523;&#22522;&#30784;&#20808;&#39564;&#26469;&#36827;&#34892;&#25506;&#32034;&#12289;&#23398;&#20064;&#21644;&#24378;&#21270;&#12290;FRL&#30340;&#22909;&#22788;&#26377;&#19977;&#20010;&#12290;(1)&#26679;&#26412;&#25928;&#29575;&#39640;&#12290;&#36890;&#36807;&#22522;&#30784;&#20808;&#39564;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#65292;&#20943;&#23569;&#26679;&#26412;&#20351;&#29992;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, people have shown that large-scale pre-training from internet-scale data is the key to building generalist models, as witnessed in NLP. To build embodied generalist agents, we and many other researchers hypothesize that such foundation prior is also an indispensable component. However, it is unclear what is the proper concrete form to represent those embodied foundation priors and how they should be used in the downstream task. In this paper, we propose an intuitive and effective set of embodied priors that consist of foundation policy, value, and success reward. The proposed priors are based on the goal-conditioned MDP. To verify their effectiveness, we instantiate an actor-critic method assisted by the priors, called Foundation Actor-Critic (FAC). We name our framework as Foundation Reinforcement Learning (FRL), since it completely relies on embodied foundation priors to explore, learn and reinforce. The benefits of FRL are threefold. (1) Sample efficient. With foundation p
&lt;/p&gt;</description></item></channel></rss>