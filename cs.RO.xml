<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#36523;&#26426;&#22120;&#20154;&#23398;&#20064;&#20195;&#29702;NBCagent&#65292;&#36890;&#36807;&#25216;&#33021;&#29305;&#23450;&#30340;&#28436;&#21270;&#35268;&#21010;&#22120;&#21644;&#25216;&#33021;&#20849;&#20139;&#30340;&#35821;&#20041;&#28210;&#26579;&#27169;&#22359;&#65292;&#23454;&#29616;&#20174;&#35270;&#35273;&#35266;&#27979;&#20013;&#36830;&#32493;&#23398;&#20064;&#26032;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#25216;&#33021;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.00336</link><description>&lt;p&gt;
&#27704;&#19981;&#20572;&#27490;&#30340;&#20855;&#36523;&#26426;&#22120;&#20154;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Never-Ending Embodied Robot Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00336
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#36523;&#26426;&#22120;&#20154;&#23398;&#20064;&#20195;&#29702;NBCagent&#65292;&#36890;&#36807;&#25216;&#33021;&#29305;&#23450;&#30340;&#28436;&#21270;&#35268;&#21010;&#22120;&#21644;&#25216;&#33021;&#20849;&#20139;&#30340;&#35821;&#20041;&#28210;&#26579;&#27169;&#22359;&#65292;&#23454;&#29616;&#20174;&#35270;&#35273;&#35266;&#27979;&#20013;&#36830;&#32493;&#23398;&#20064;&#26032;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#25216;&#33021;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20381;&#36182;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20855;&#36523;&#26426;&#22120;&#20154;&#21487;&#20197;&#36890;&#36807;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20174;&#35270;&#35273;&#35266;&#27979;&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#35270;&#35273;&#34892;&#20026;&#20811;&#38534;&#20195;&#29702;&#22312;&#36866;&#24212;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26410;&#35265;&#20219;&#21153;&#26102;&#65292;&#20250;&#36973;&#21463;&#25805;&#32437;&#24615;&#33021;&#19979;&#38477;&#20197;&#21450;&#25216;&#33021;&#30693;&#35782;&#36951;&#24536;&#30340;&#22256;&#25200;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;NBCagent&#22312;&#20855;&#36523;&#26426;&#22120;&#20154;&#20013;&#25506;&#35752;&#20102;&#19978;&#36848;&#25361;&#25112;&#65292;&#36825;&#26159;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#12289;&#20197;&#35821;&#35328;&#20026;&#26465;&#20214;&#30340;&#27704;&#19981;&#20572;&#27490;&#34892;&#20026;&#20811;&#38534;&#20195;&#29702;&#65292;&#21487;&#20197;&#19981;&#26029;&#20174;&#29305;&#23450;&#25216;&#33021;&#21644;&#20849;&#20139;&#25216;&#33021;&#23646;&#24615;&#20013;&#23398;&#20064;&#26032;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#25216;&#33021;&#30340;&#35266;&#23519;&#30693;&#35782;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#29305;&#23450;&#25216;&#33021;&#19981;&#26029;&#28436;&#21270;&#30340;&#35268;&#21010;&#22120;&#26469;&#36827;&#34892;&#30693;&#35782;&#35299;&#32806;&#65292;&#36825;&#21487;&#20197;&#20174;&#28508;&#22312;&#21644;&#20302;&#31209;&#31354;&#38388;&#20013;&#19981;&#26029;&#21521;&#25105;&#20204;&#30340;NBCagent&#20195;&#29702;&#23884;&#20837;&#26032;&#30340;&#25216;&#33021;&#29305;&#23450;&#30693;&#35782;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25216;&#33021;&#20849;&#20139;&#35821;&#20041;&#28210;&#26579;&#27169;&#22359;&#21644;&#19968;&#20010;&#25216;&#33021;&#20849;&#20139;&#34920;&#31034;&#21306;&#20998;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00336v1 Announce Type: cross  Abstract: Relying on large language models (LLMs), embodied robots could perform complex multimodal robot manipulation tasks from visual observations with powerful generalization ability. However, most visual behavior-cloning agents suffer from manipulation performance degradation and skill knowledge forgetting when adapting into a series of challenging unseen tasks. We here investigate the above challenge with NBCagent in embodied robots, a pioneering language-conditioned Never-ending Behavior-Cloning agent, which can continually learn observation knowledge of novel robot manipulation skills from skill-specific and skill-shared attributes. Specifically, we establish a skill-specific evolving planner to perform knowledge decoupling, which can continually embed novel skill-specific knowledge in our NBCagent agent from latent and low-rank space. Meanwhile, we propose a skill-shared semantics rendering module and a skill-shared representation disti
&lt;/p&gt;</description></item></channel></rss>