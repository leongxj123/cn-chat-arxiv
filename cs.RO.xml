<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>DexCap&#26159;&#19968;&#20010;&#21487;&#31227;&#26893;&#30340;&#25163;&#37096;&#21160;&#20316;&#25429;&#25417;&#31995;&#32479;&#65292;&#32467;&#21512;DexIL&#31639;&#27861;&#20174;&#20154;&#31867;&#25163;&#37096;&#36816;&#21160;&#25968;&#25454;&#20013;&#35757;&#32451;&#26426;&#22120;&#20154;&#25216;&#33021;&#65292;&#20855;&#26377;&#31934;&#30830;&#36861;&#36394;&#21644;&#22797;&#21046;&#20154;&#31867;&#21160;&#20316;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.07788</link><description>&lt;p&gt;
DexCap&#65306;&#29992;&#20110;&#28789;&#24039;&#25805;&#20316;&#30340;&#21487;&#25193;&#23637;&#21644;&#21487;&#31227;&#26893;&#21160;&#20316;&#25429;&#25417;&#25968;&#25454;&#25910;&#38598;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
DexCap: Scalable and Portable Mocap Data Collection System for Dexterous Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07788
&lt;/p&gt;
&lt;p&gt;
DexCap&#26159;&#19968;&#20010;&#21487;&#31227;&#26893;&#30340;&#25163;&#37096;&#21160;&#20316;&#25429;&#25417;&#31995;&#32479;&#65292;&#32467;&#21512;DexIL&#31639;&#27861;&#20174;&#20154;&#31867;&#25163;&#37096;&#36816;&#21160;&#25968;&#25454;&#20013;&#35757;&#32451;&#26426;&#22120;&#20154;&#25216;&#33021;&#65292;&#20855;&#26377;&#31934;&#30830;&#36861;&#36394;&#21644;&#22797;&#21046;&#20154;&#31867;&#21160;&#20316;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#25163;&#37096;&#36816;&#21160;&#25968;&#25454;&#20013;&#23398;&#20064;&#26159;&#20026;&#26426;&#22120;&#20154;&#36171;&#20104;&#31867;&#20154;&#28789;&#24039;&#22312;&#29616;&#23454;&#25805;&#32437;&#20219;&#21153;&#20013;&#30340;&#28508;&#22312;&#36884;&#24452;&#65292;&#28982;&#32780;&#65292;&#29616;&#23384;&#25163;&#37096;&#21160;&#20316;&#25429;&#25417;&#31995;&#32479;&#30340;&#21487;&#31227;&#26893;&#24615;&#20197;&#21450;&#23558;&#21160;&#20316;&#25429;&#25417;&#25968;&#25454;&#36716;&#21270;&#20026;&#26377;&#25928;&#25511;&#21046;&#31574;&#30053;&#30340;&#22256;&#38590;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DexCap&#65292;&#19968;&#20010;&#20415;&#25658;&#24335;&#25163;&#37096;&#21160;&#20316;&#25429;&#25417;&#31995;&#32479;&#65292;&#20197;&#21450;DexIL&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#20223;&#31639;&#27861;&#65292;&#21487;&#30452;&#25509;&#20174;&#20154;&#31867;&#25163;&#37096;&#21160;&#20316;&#25429;&#25417;&#25968;&#25454;&#35757;&#32451;&#28789;&#24039;&#26426;&#22120;&#20154;&#25216;&#33021;&#12290;DexCap&#22522;&#20110;SLAM&#21644;&#30005;&#30913;&#22330;&#20197;&#21450;&#29615;&#22659;&#30340;3D&#35266;&#23519;&#65292;&#25552;&#20379;&#20102;&#23545;&#25163;&#33109;&#21644;&#25163;&#25351;&#36816;&#21160;&#30340;&#31934;&#30830;&#12289;&#25239;&#36974;&#25377;&#30340;&#36319;&#36394;&#12290;&#21033;&#29992;&#36825;&#19968;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#65292;DexIL&#37319;&#29992;&#36870;&#36816;&#21160;&#23398;&#21644;&#22522;&#20110;&#28857;&#20113;&#30340;&#27169;&#20223;&#23398;&#20064;&#26469;&#22797;&#21046;&#20154;&#31867;&#21160;&#20316;&#19982;&#26426;&#22120;&#20154;&#25163;&#12290;&#38500;&#20102;&#20174;&#20154;&#31867;&#36816;&#21160;&#20013;&#23398;&#20064;&#22806;&#65292;DexCap&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;op
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07788v1 Announce Type: cross  Abstract: Imitation learning from human hand motion data presents a promising avenue for imbuing robots with human-like dexterity in real-world manipulation tasks. Despite this potential, substantial challenges persist, particularly with the portability of existing hand motion capture (mocap) systems and the difficulty of translating mocap data into effective control policies. To tackle these issues, we introduce DexCap, a portable hand motion capture system, alongside DexIL, a novel imitation algorithm for training dexterous robot skills directly from human hand mocap data. DexCap offers precise, occlusion-resistant tracking of wrist and finger motions based on SLAM and electromagnetic field together with 3D observations of the environment. Utilizing this rich dataset, DexIL employs inverse kinematics and point cloud-based imitation learning to replicate human actions with robot hands. Beyond learning from human motion, DexCap also offers an op
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21160;&#24577;&#22270;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#20449;&#24687;&#36335;&#24452;&#35268;&#21010;&#65292;&#33021;&#22815;&#22312;&#26410;&#30693;&#30340;&#19977;&#32500;&#29615;&#22659;&#20013;&#26144;&#23556;&#20986;&#24863;&#20852;&#36259;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2402.04894</link><description>&lt;p&gt;
&#21033;&#29992;&#21160;&#24577;&#22270;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#33258;&#36866;&#24212;&#20449;&#24687;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning with Dynamic Graphs for Adaptive Informative Path Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04894
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21160;&#24577;&#22270;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#20449;&#24687;&#36335;&#24452;&#35268;&#21010;&#65292;&#33021;&#22815;&#22312;&#26410;&#30693;&#30340;&#19977;&#32500;&#29615;&#22659;&#20013;&#26144;&#23556;&#20986;&#24863;&#20852;&#36259;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#26426;&#22120;&#20154;&#24120;&#24120;&#34987;&#29992;&#20110;&#25968;&#25454;&#25910;&#38598;&#65292;&#22240;&#20026;&#23427;&#20204;&#39640;&#25928;&#19988;&#21171;&#21160;&#25104;&#26412;&#20302;&#12290;&#26426;&#22120;&#20154;&#25968;&#25454;&#37319;&#38598;&#30340;&#20851;&#38190;&#20219;&#21153;&#26159;&#22312;&#21021;&#22987;&#26410;&#30693;&#29615;&#22659;&#20013;&#35268;&#21010;&#36335;&#24452;&#65292;&#20197;&#28385;&#36275;&#24179;&#21488;&#29305;&#23450;&#30340;&#36164;&#28304;&#32422;&#26463;&#65292;&#20363;&#22914;&#26377;&#38480;&#30340;&#30005;&#27744;&#23551;&#21629;&#12290;&#22312;&#19977;&#32500;&#29615;&#22659;&#20013;&#36827;&#34892;&#33258;&#36866;&#24212;&#22312;&#32447;&#36335;&#24452;&#35268;&#21010;&#38754;&#20020;&#30528;&#24456;&#22810;&#25361;&#25112;&#65292;&#21253;&#25324;&#22823;&#37327;&#26377;&#25928;&#21160;&#20316;&#30340;&#23384;&#22312;&#20197;&#21450;&#26410;&#30693;&#36974;&#25377;&#29289;&#30340;&#23384;&#22312;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#37325;&#26032;&#35268;&#21010;&#26426;&#22120;&#20154;&#36335;&#24452;&#20197;&#22312;&#26410;&#30693;&#30340;&#19977;&#32500;&#29615;&#22659;&#20013;&#26144;&#23556;&#20986;&#24863;&#20852;&#36259;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#20043;&#22788;&#22312;&#20110;&#26500;&#24314;&#21160;&#24577;&#22270;&#65292;&#23558;&#35268;&#21010;&#21160;&#20316;&#38480;&#21046;&#22312;&#26426;&#22120;&#20154;&#38468;&#36817;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#24555;&#36895;&#21709;&#24212;&#26032;&#21457;&#29616;&#30340;&#38556;&#30861;&#21644;&#24863;&#20852;&#36259;&#30340;&#30446;&#26631;&#12290;&#23545;&#20110;&#37325;&#26032;&#35268;&#21010;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#24179;&#34913;&#25506;&#32034;&#26410;&#30693;&#29615;&#22659;&#21644;&#21033;&#29992;&#22312;&#32447;&#25910;&#38598;&#30340;&#26377;&#20851;&#24863;&#20852;&#36259;&#30446;&#26631;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous robots are often employed for data collection due to their efficiency and low labour costs. A key task in robotic data acquisition is planning paths through an initially unknown environment to collect observations given platform-specific resource constraints, such as limited battery life. Adaptive online path planning in 3D environments is challenging due to the large set of valid actions and the presence of unknown occlusions. To address these issues, we propose a novel deep reinforcement learning approach for adaptively replanning robot paths to map targets of interest in unknown 3D environments. A key aspect of our approach is a dynamically constructed graph that restricts planning actions local to the robot, allowing us to quickly react to newly discovered obstacles and targets of interest. For replanning, we propose a new reward function that balances between exploring the unknown environment and exploiting online-collected data about the targets of interest. Our experi
&lt;/p&gt;</description></item><item><title>DexDiffuser&#26159;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#28789;&#24039;&#25235;&#21462;&#23039;&#21183;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#29289;&#20307;&#28857;&#20113;&#30340;&#29983;&#25104;&#12289;&#35780;&#20272;&#21644;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#25235;&#21462;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.02989</link><description>&lt;p&gt;
DexDiffuser: &#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#28789;&#24039;&#25235;&#21462;&#23039;&#21183;
&lt;/p&gt;
&lt;p&gt;
DexDiffuser: Generating Dexterous Grasps with Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02989
&lt;/p&gt;
&lt;p&gt;
DexDiffuser&#26159;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#28789;&#24039;&#25235;&#21462;&#23039;&#21183;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#29289;&#20307;&#28857;&#20113;&#30340;&#29983;&#25104;&#12289;&#35780;&#20272;&#21644;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#25235;&#21462;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;DexDiffuser&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#28789;&#24039;&#25235;&#21462;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#37096;&#20998;&#29289;&#20307;&#28857;&#20113;&#19978;&#29983;&#25104;&#12289;&#35780;&#20272;&#21644;&#20248;&#21270;&#25235;&#21462;&#23039;&#21183;&#12290;DexDiffuser&#21253;&#25324;&#26465;&#20214;&#25193;&#25955;&#22411;&#25235;&#21462;&#37319;&#26679;&#22120;DexSampler&#21644;&#28789;&#24039;&#25235;&#21462;&#35780;&#20272;&#22120;DexEvaluator&#12290;DexSampler&#36890;&#36807;&#23545;&#38543;&#26426;&#25235;&#21462;&#36827;&#34892;&#36845;&#20195;&#21435;&#22122;&#65292;&#29983;&#25104;&#19982;&#29289;&#20307;&#28857;&#20113;&#26465;&#20214;&#30456;&#20851;&#30340;&#39640;&#36136;&#37327;&#25235;&#21462;&#23039;&#21183;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#20004;&#31181;&#25235;&#21462;&#20248;&#21270;&#31574;&#30053;&#65306;&#22522;&#20110;&#35780;&#20272;&#22120;&#30340;&#25193;&#25955;(Evaluator-Guided Diffusion&#65292;EGD)&#21644;&#22522;&#20110;&#35780;&#20272;&#22120;&#30340;&#37319;&#26679;&#20248;&#21270;(Evaluator-based Sampling Refinement&#65292;ESR)&#12290;&#25105;&#20204;&#22312;&#34394;&#25311;&#29615;&#22659;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#23454;&#39564;&#20013;&#65292;&#20351;&#29992;Allegro Hand&#36827;&#34892;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;DexDiffuser&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#22810;&#25351;&#25235;&#21462;&#29983;&#25104;&#26041;&#27861;FFHNet&#65292;&#24179;&#22343;&#25235;&#21462;&#25104;&#21151;&#29575;&#25552;&#39640;&#20102;21.71-22.20%&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce DexDiffuser, a novel dexterous grasping method that generates, evaluates, and refines grasps on partial object point clouds. DexDiffuser includes the conditional diffusion-based grasp sampler DexSampler and the dexterous grasp evaluator DexEvaluator. DexSampler generates high-quality grasps conditioned on object point clouds by iterative denoising of randomly sampled grasps. We also introduce two grasp refinement strategies: Evaluator-Guided Diffusion (EGD) and Evaluator-based Sampling Refinement (ESR). Our simulation and real-world experiments on the Allegro Hand consistently demonstrate that DexDiffuser outperforms the state-of-the-art multi-finger grasp generation method FFHNet with an, on average, 21.71--22.20\% higher grasp success rate.
&lt;/p&gt;</description></item><item><title>PhotoBot&#26159;&#19968;&#20010;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#24341;&#23548;&#21644;&#26426;&#22120;&#20154;&#25668;&#24433;&#24072;&#30456;&#20114;&#20316;&#29992;&#30340;&#33258;&#21160;&#21270;&#29031;&#29255;&#33719;&#21462;&#26694;&#26550;&#12290;&#23427;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21644;&#29289;&#20307;&#26816;&#27979;&#22120;&#26469;&#25552;&#20379;&#25668;&#24433;&#24314;&#35758;&#65292;&#24182;&#36890;&#36807;&#35270;&#35273;&#21464;&#25442;&#22120;&#35745;&#31639;&#30456;&#26426;&#30340;&#23039;&#24577;&#35843;&#25972;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#29031;&#29255;&#33719;&#21462;&#12290;</title><link>http://arxiv.org/abs/2401.11061</link><description>&lt;p&gt;
PhotoBot&#65306;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#24341;&#23548;&#30340;&#21442;&#32771;&#20114;&#21160;&#25668;&#24433;
&lt;/p&gt;
&lt;p&gt;
PhotoBot: Reference-Guided Interactive Photography via Natural Language. (arXiv:2401.11061v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11061
&lt;/p&gt;
&lt;p&gt;
PhotoBot&#26159;&#19968;&#20010;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#24341;&#23548;&#21644;&#26426;&#22120;&#20154;&#25668;&#24433;&#24072;&#30456;&#20114;&#20316;&#29992;&#30340;&#33258;&#21160;&#21270;&#29031;&#29255;&#33719;&#21462;&#26694;&#26550;&#12290;&#23427;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21644;&#29289;&#20307;&#26816;&#27979;&#22120;&#26469;&#25552;&#20379;&#25668;&#24433;&#24314;&#35758;&#65292;&#24182;&#36890;&#36807;&#35270;&#35273;&#21464;&#25442;&#22120;&#35745;&#31639;&#30456;&#26426;&#30340;&#23039;&#24577;&#35843;&#25972;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#29031;&#29255;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;PhotoBot&#30340;&#26694;&#26550;&#65292;&#23427;&#22522;&#20110;&#39640;&#32423;&#20154;&#31867;&#35821;&#35328;&#24341;&#23548;&#21644;&#26426;&#22120;&#20154;&#25668;&#24433;&#24072;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#30340;&#29031;&#29255;&#33719;&#21462;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#20174;&#31574;&#23637;&#30011;&#24266;&#20013;&#26816;&#32034;&#21040;&#30340;&#21442;&#32771;&#22270;&#29255;&#21521;&#29992;&#25143;&#20256;&#36798;&#25668;&#24433;&#24314;&#35758;&#12290;&#25105;&#20204;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#21644;&#29289;&#20307;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#25991;&#26412;&#25551;&#36848;&#23545;&#21442;&#32771;&#22270;&#29255;&#36827;&#34892;&#29305;&#24449;&#21270;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#22522;&#20110;&#29992;&#25143;&#35821;&#35328;&#26597;&#35810;&#30340;&#25991;&#26412;&#25512;&#29702;&#26816;&#32034;&#30456;&#20851;&#30340;&#21442;&#32771;&#22270;&#29255;&#12290;&#20026;&#20102;&#23545;&#24212;&#21442;&#32771;&#22270;&#29255;&#21644;&#35266;&#23519;&#21040;&#30340;&#22330;&#26223;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#26174;&#33879;&#19981;&#21516;&#30340;&#22270;&#20687;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#39044;&#35757;&#32451;&#29305;&#24449;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#65292;&#36890;&#36807;&#35299;&#20915;&#19968;&#20010;&#36879;&#35270;n-&#28857;&#65288;PnP&#65289;&#38382;&#39064;&#26469;&#35745;&#31639;RGB-D&#30456;&#26426;&#30340;&#23039;&#24577;&#35843;&#25972;&#12290;&#25105;&#20204;&#22312;&#37197;&#22791;&#26377;&#25163;&#33109;&#30456;&#26426;&#30340;&#30495;&#23454;&#26426;&#26800;&#25163;&#33218;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#30001;PhotoBot&#25293;&#25668;&#30340;&#29031;&#29255;&#20855;&#26377;&#33391;&#22909;&#30340;&#36136;&#37327;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce PhotoBot, a framework for automated photo acquisition based on an interplay between high-level human language guidance and a robot photographer. We propose to communicate photography suggestions to the user via a reference picture that is retrieved from a curated gallery. We exploit a visual language model (VLM) and an object detector to characterize reference pictures via textual descriptions and use a large language model (LLM) to retrieve relevant reference pictures based on a user's language query through text-based reasoning. To correspond the reference picture and the observed scene, we exploit pre-trained features from a vision transformer capable of capturing semantic similarity across significantly varying images. Using these features, we compute pose adjustments for an RGB-D camera by solving a Perspective-n-Point (PnP) problem. We demonstrate our approach on a real-world manipulator equipped with a wrist camera. Our user studies show that photos taken by PhotoBo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#25509;&#35302;&#38544;&#24335;&#21452;&#23618;&#20248;&#21270;&#26469;&#35268;&#21010;&#25903;&#28857;&#25805;&#32437;&#24182;&#22686;&#21152;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#21033;&#29992;&#25705;&#25830;&#21147;&#26469;&#24357;&#34917;&#29289;&#20307;&#21644;&#29615;&#22659;&#29289;&#29702;&#23646;&#24615;&#20272;&#35745;&#20013;&#30340;&#19981;&#20934;&#30830;&#24615;&#65292;&#20197;&#24212;&#23545;&#19981;&#30830;&#23450;&#24615;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.08965</link><description>&lt;p&gt;
&#20351;&#29992;&#25509;&#35302;&#38544;&#24335;&#21452;&#23618;&#20248;&#21270;&#23454;&#29616;&#40065;&#26834;&#30340;&#25903;&#28857;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Robust Pivoting Manipulation using Contact Implicit Bilevel Optimization. (arXiv:2303.08965v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#25509;&#35302;&#38544;&#24335;&#21452;&#23618;&#20248;&#21270;&#26469;&#35268;&#21010;&#25903;&#28857;&#25805;&#32437;&#24182;&#22686;&#21152;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#21033;&#29992;&#25705;&#25830;&#21147;&#26469;&#24357;&#34917;&#29289;&#20307;&#21644;&#29615;&#22659;&#29289;&#29702;&#23646;&#24615;&#20272;&#35745;&#20013;&#30340;&#19981;&#20934;&#30830;&#24615;&#65292;&#20197;&#24212;&#23545;&#19981;&#30830;&#23450;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#25805;&#32437;&#35201;&#27714;&#26426;&#22120;&#20154;&#33021;&#22815;&#19982;&#26032;&#29289;&#20307;&#21644;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#12290;&#36825;&#20010;&#35201;&#27714;&#20351;&#24471;&#25805;&#32437;&#21464;&#24471;&#24322;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#26426;&#22120;&#20154;&#24517;&#39035;&#32771;&#34385;&#21040;&#19981;&#30830;&#23450;&#22240;&#32032;&#19979;&#30340;&#22797;&#26434;&#25705;&#25830;&#30456;&#20114;&#20316;&#29992;&#21450;&#29289;&#20307;&#21644;&#29615;&#22659;&#30340;&#29289;&#29702;&#23646;&#24615;&#20272;&#35745;&#30340;&#19981;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#25903;&#28857;&#25805;&#20316;&#35268;&#21010;&#30340;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#22914;&#20309;&#21033;&#29992;&#25705;&#25830;&#21147;&#26469;&#24357;&#34917;&#29289;&#29702;&#29305;&#24615;&#20272;&#35745;&#20013;&#30340;&#19981;&#20934;&#30830;&#24615;&#30340;&#35265;&#35299;&#12290;&#22312;&#26576;&#20123;&#20551;&#35774;&#19979;&#65292;&#23548;&#20986;&#20102;&#25705;&#25830;&#21147;&#25552;&#20379;&#30340;&#25903;&#28857;&#25805;&#20316;&#31283;&#23450;&#35029;&#24230;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#12290;&#28982;&#21518;&#65292;&#22312;&#25509;&#35302;&#38544;&#24335;&#21452;&#23618;&#20248;&#21270;(CIBO)&#26694;&#26550;&#20013;&#20351;&#29992;&#35813;&#35029;&#24230;&#26469;&#20248;&#21270;&#36712;&#36857; &#65292;&#20197;&#22686;&#24378;&#23545;&#29289;&#20307;&#22810;&#20010;&#29289;&#29702;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#26426;&#22120;&#20154;&#19978;&#30340;&#23454;&#39564;&#20013;&#65292;&#23545;&#20110;&#20005;&#37325;&#24178;&#25200;&#30340;&#21442;&#25968;&#65292;&#20998;&#26512;&#20102;&#31283;&#23450;&#35029;&#24230;&#65292;&#24182;&#26174;&#31034;&#20102;&#20248;&#21270;&#36712;&#36857;&#30340;&#25913;&#21892;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalizable manipulation requires that robots be able to interact with novel objects and environment. This requirement makes manipulation extremely challenging as a robot has to reason about complex frictional interactions with uncertainty in physical properties of the object and the environment. In this paper, we study robust optimization for planning of pivoting manipulation in the presence of uncertainties. We present insights about how friction can be exploited to compensate for inaccuracies in the estimates of the physical properties during manipulation. Under certain assumptions, we derive analytical expressions for stability margin provided by friction during pivoting manipulation. This margin is then used in a Contact Implicit Bilevel Optimization (CIBO) framework to optimize a trajectory that maximizes this stability margin to provide robustness against uncertainty in several physical parameters of the object. We present analysis of the stability margin with respect to sever
&lt;/p&gt;</description></item></channel></rss>