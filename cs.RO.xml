<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#35813;&#30740;&#31350;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#26426;&#20114;&#21160;&#20013;&#26159;&#21542;&#33021;&#22815;&#25429;&#25417;&#21040;&#20154;&#20204;&#30340;&#34892;&#20026;&#21028;&#26029;&#21644;&#27807;&#36890;&#20559;&#22909;&#65292;&#32467;&#26524;&#34920;&#26126;GPT-4&#22312;&#29983;&#25104;&#31038;&#20250;&#21487;&#25509;&#21463;&#34892;&#20026;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.05701</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#19982;&#20154;&#20204;&#30340;&#31038;&#20132;&#30452;&#35273;&#30456;&#19968;&#33268;&#65292;&#29992;&#20110;&#20154;&#26426;&#20114;&#21160;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Models Aligned with People's Social Intuitions for Human-Robot Interactions?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05701
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#26426;&#20114;&#21160;&#20013;&#26159;&#21542;&#33021;&#22815;&#25429;&#25417;&#21040;&#20154;&#20204;&#30340;&#34892;&#20026;&#21028;&#26029;&#21644;&#27807;&#36890;&#20559;&#22909;&#65292;&#32467;&#26524;&#34920;&#26126;GPT-4&#22312;&#29983;&#25104;&#31038;&#20250;&#21487;&#25509;&#21463;&#34892;&#20026;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#26426;&#22120;&#20154;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#39640;&#23618;&#27425;&#30340;&#34892;&#21160;&#35268;&#21010;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#35768;&#22810;&#26426;&#22120;&#20154;&#24212;&#29992;&#28041;&#21450;&#20154;&#31867;&#30417;&#30563;&#21592;&#25110;&#21512;&#20316;&#32773;&#12290;&#22240;&#27492;&#65292;&#23545;LLMs&#29983;&#25104;&#19982;&#20154;&#20204;&#20559;&#22909;&#21644;&#20215;&#20540;&#35266;&#30456;&#19968;&#33268;&#30340;&#31038;&#20250;&#21487;&#25509;&#21463;&#34892;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#27979;&#35797;LLMs&#26159;&#21542;&#25429;&#25417;&#21040;&#20154;&#20204;&#22312;&#20154;&#26426;&#20114;&#21160;&#65288;HRI&#65289;&#22330;&#26223;&#20013;&#34892;&#20026;&#21028;&#26029;&#21644;&#27807;&#36890;&#20559;&#22909;&#26041;&#38754;&#30340;&#30452;&#35273;&#12290;&#20026;&#20102;&#35780;&#20272;&#65292;&#25105;&#20204;&#37325;&#29616;&#20102;&#19977;&#20010;HRI&#29992;&#25143;&#30740;&#31350;&#65292;&#23558;LLMs&#30340;&#36755;&#20986;&#19982;&#30495;&#23454;&#21442;&#19982;&#32773;&#30340;&#36755;&#20986;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;GPT-4&#22312;&#38750;&#24120;&#20986;&#33394;&#22320;&#34920;&#29616;&#65292;&#29983;&#25104;&#30340;&#31572;&#26696;&#19982;&#20004;&#39033;&#30740;&#31350;&#30340;&#29992;&#25143;&#31572;&#26696;&#20855;&#26377;&#24456;&#24378;&#30456;&#20851;&#24615;&#8212;&#8212;&#31532;&#19968;&#39033;&#30740;&#31350;&#28041;&#21450;&#22312;&#21508;&#31181;&#24773;&#22659;&#20013;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#27807;&#36890;&#20030;&#21160;&#32473;&#26426;&#22120;&#20154;&#65288;$r_s$ = 0.82&#65289;&#65292;&#31532;&#20108;&#39033;&#28041;&#21450;&#21028;&#26029;&#34892;&#20026;&#30340;&#21487;&#21462;&#24615;&#12289;&#24847;&#22270;&#24615;&#21644;&#20196;&#20154;&#24778;&#35766;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05701v1 Announce Type: cross  Abstract: Large language models (LLMs) are increasingly used in robotics, especially for high-level action planning. Meanwhile, many robotics applications involve human supervisors or collaborators. Hence, it is crucial for LLMs to generate socially acceptable actions that align with people's preferences and values. In this work, we test whether LLMs capture people's intuitions about behavior judgments and communication preferences in human-robot interaction (HRI) scenarios. For evaluation, we reproduce three HRI user studies, comparing the output of LLMs with that of real participants. We find that GPT-4 strongly outperforms other models, generating answers that correlate strongly with users' answers in two studies $\unicode{x2014}$ the first study dealing with selecting the most appropriate communicative act for a robot in various situations ($r_s$ = 0.82), and the second with judging the desirability, intentionality, and surprisingness of beh
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36870;&#21521;&#25235;&#21462;&#36807;&#31243;&#24182;&#21033;&#29992;&#25342;&#21462;&#21644;&#25918;&#32622;&#38382;&#39064;&#30340;&#23545;&#31216;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25342;&#21462;&#30340;&#25918;&#32622;&#26041;&#27861;&#65292;&#24182;&#29992;&#33258;&#20027;&#25910;&#38598;&#30340;&#28436;&#31034;&#30452;&#25509;&#35757;&#32451;&#31574;&#30053;&#65292;&#23454;&#29616;&#22312;&#25509;&#35302;&#21463;&#38480;&#29615;&#22659;&#19979;&#29289;&#20307;&#25918;&#32622;&#20219;&#21153;&#30340;&#33258;&#20027;&#25910;&#38598;&#21644;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2312.02352</link><description>&lt;p&gt;
&#36870;&#21521;&#23398;&#20064;&#65306;&#36890;&#36807;&#25441;&#21462;&#23398;&#20064;&#25918;&#32622;
&lt;/p&gt;
&lt;p&gt;
Working Backwards: Learning to Place by Picking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02352
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36870;&#21521;&#25235;&#21462;&#36807;&#31243;&#24182;&#21033;&#29992;&#25342;&#21462;&#21644;&#25918;&#32622;&#38382;&#39064;&#30340;&#23545;&#31216;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25342;&#21462;&#30340;&#25918;&#32622;&#26041;&#27861;&#65292;&#24182;&#29992;&#33258;&#20027;&#25910;&#38598;&#30340;&#28436;&#31034;&#30452;&#25509;&#35757;&#32451;&#31574;&#30053;&#65292;&#23454;&#29616;&#22312;&#25509;&#35302;&#21463;&#38480;&#29615;&#22659;&#19979;&#29289;&#20307;&#25918;&#32622;&#20219;&#21153;&#30340;&#33258;&#20027;&#25910;&#38598;&#21644;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25342;&#21462;&#65288;PvP&#65289;&#30340;&#25918;&#32622;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#20027;&#25910;&#38598;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#25918;&#32622;&#20219;&#21153;&#30340;&#29616;&#23454;&#19990;&#30028;&#28436;&#31034;&#65292;&#20854;&#20013;&#29289;&#20307;&#24517;&#39035;&#34987;&#25805;&#32437;&#21040;&#29305;&#23450;&#30340;&#25509;&#35302;&#38480;&#21046;&#20301;&#32622;&#12290;&#36890;&#36807;PvP&#65292;&#25105;&#20204;&#36890;&#36807;&#39072;&#20498;&#25235;&#21462;&#36807;&#31243;&#24182;&#21033;&#29992;&#25342;&#21462;&#21644;&#25918;&#32622;&#38382;&#39064;&#22266;&#26377;&#30340;&#23545;&#31216;&#24615;&#65292;&#25509;&#36817;&#20110;&#26426;&#22120;&#20154;&#29289;&#20307;&#25918;&#32622;&#28436;&#31034;&#30340;&#25910;&#38598;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#19968;&#32452;&#26368;&#21021;&#20301;&#20110;&#30446;&#26631;&#25918;&#32622;&#20301;&#32622;&#30340;&#29289;&#20307;&#30340;&#25235;&#21462;&#24207;&#21015;&#20013;&#33719;&#24471;&#25918;&#32622;&#28436;&#31034;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#22312;&#25509;&#35302;&#21463;&#38480;&#29615;&#22659;&#20013;&#25910;&#38598;&#25968;&#30334;&#20010;&#28436;&#31034;&#65292;&#32780;&#26080;&#38656;&#20154;&#31867;&#24178;&#39044;&#65292;&#36825;&#26159;&#36890;&#36807;&#32467;&#21512;&#20004;&#20010;&#27169;&#22359;&#23454;&#29616;&#30340;&#65306;&#35302;&#35273;&#37325;&#26032;&#25235;&#21462;&#21644;&#29992;&#20110;&#25235;&#21462;&#30340;&#39034;&#20174;&#25511;&#21046;&#12290;&#25105;&#20204;&#36890;&#36807;&#34892;&#20026;&#20811;&#38534;&#30452;&#25509;&#20174;&#35270;&#35273;&#35266;&#23519;&#20013;&#36890;&#36807;&#33258;&#20027;&#25910;&#38598;&#30340;&#28436;&#31034;&#20013;&#35757;&#32451;&#31574;&#30053;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#31574;&#30053;&#21487;&#20197;&#25512;&#24191;&#21040;&#36229;&#20986;&#35757;&#32451;&#29615;&#22659;&#33539;&#22260;&#30340;&#29289;&#20307;&#25918;&#32622;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02352v2 Announce Type: replace-cross  Abstract: We present placing via picking (PvP), a method to autonomously collect real-world demonstrations for a family of placing tasks in which objects must be manipulated to specific contact-constrained locations. With PvP, we approach the collection of robotic object placement demonstrations by reversing the grasping process and exploiting the inherent symmetry of the pick and place problems. Specifically, we obtain placing demonstrations from a set of grasp sequences of objects initially located at their target placement locations. Our system can collect hundreds of demonstrations in contact-constrained environments without human intervention by combining two modules: tactile regrasping and compliant control for grasps. We train a policy directly from visual observations through behavioral cloning, using the autonomously-collected demonstrations. By doing so, the policy can generalize to object placement scenarios outside of the tra
&lt;/p&gt;</description></item></channel></rss>