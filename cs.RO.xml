<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#26412;&#32508;&#36848;&#35770;&#25991;&#22238;&#39038;&#20102;40&#22810;&#31687;&#30740;&#31350;&#35770;&#25991;&#65292;&#24635;&#32467;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#33258;&#21160;&#39550;&#39542;&#22312;&#35268;&#21010;&#12289;&#20223;&#30495;&#21644;&#20851;&#38190;&#20219;&#21153;&#26041;&#38754;&#30340;&#37325;&#35201;&#36129;&#29486;&#65292;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21644;&#32763;&#35793;&#33021;&#21147;&#65292;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#22312;&#29289;&#20307;&#26816;&#27979;&#21644;&#39550;&#39542;&#22330;&#26223;&#21019;&#24314;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01105</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey for Foundation Models in Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#22238;&#39038;&#20102;40&#22810;&#31687;&#30740;&#31350;&#35770;&#25991;&#65292;&#24635;&#32467;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#33258;&#21160;&#39550;&#39542;&#22312;&#35268;&#21010;&#12289;&#20223;&#30495;&#21644;&#20851;&#38190;&#20219;&#21153;&#26041;&#38754;&#30340;&#37325;&#35201;&#36129;&#29486;&#65292;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21644;&#32763;&#35793;&#33021;&#21147;&#65292;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#22312;&#29289;&#20307;&#26816;&#27979;&#21644;&#39550;&#39542;&#22330;&#26223;&#21019;&#24314;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21457;&#29983;&#20102;&#38761;&#21629;&#65292;&#20026;&#33258;&#21160;&#39550;&#39542;&#24212;&#29992;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#23545;40&#22810;&#31687;&#30740;&#31350;&#35770;&#25991;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#65292;&#23637;&#31034;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#25552;&#21319;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#20316;&#29992;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#30340;&#35268;&#21010;&#21644;&#20223;&#30495;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20854;&#22312;&#25512;&#29702;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#32763;&#35793;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#22312;&#20851;&#38190;&#20219;&#21153;&#20013;&#24471;&#21040;&#36234;&#26469;&#36234;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#21644;&#36319;&#36394;&#65292;&#20197;&#21450;&#20026;&#20223;&#30495;&#21644;&#27979;&#35797;&#21019;&#24314;&#36924;&#30495;&#30340;&#39550;&#39542;&#22330;&#26223;&#12290;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#21487;&#20197;&#25972;&#21512;&#22810;&#26679;&#30340;&#36755;&#20837;&#65292;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#65292;&#23545;&#20110;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#32508;&#36848;&#19981;&#20165;&#25552;&#20379;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#20998;&#31867;&#65292;&#26681;&#25454;&#27169;&#24577;&#21644;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#20013;&#30340;&#21151;&#33021;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#65292;&#36824;&#28145;&#20837;&#30740;&#31350;&#20102;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of foundation models has revolutionized the fields of natural language processing and computer vision, paving the way for their application in autonomous driving (AD). This survey presents a comprehensive review of more than 40 research papers, demonstrating the role of foundation models in enhancing AD. Large language models contribute to planning and simulation in AD, particularly through their proficiency in reasoning, code generation and translation. In parallel, vision foundation models are increasingly adapted for critical tasks such as 3D object detection and tracking, as well as creating realistic driving scenarios for simulation and testing. Multi-modal foundation models, integrating diverse inputs, exhibit exceptional visual understanding and spatial reasoning, crucial for end-to-end AD. This survey not only provides a structured taxonomy, categorizing foundation models based on their modalities and functionalities within the AD domain but also delves into the meth
&lt;/p&gt;</description></item><item><title>LLM^3&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#20855;&#22791;&#24378;&#22823;&#30340;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#65292;&#36890;&#36807;&#25509;&#21475;&#25552;&#20986;&#31526;&#21495;&#21160;&#20316;&#24207;&#21015;&#21644;&#36873;&#25321;&#36830;&#32493;&#21160;&#20316;&#21442;&#25968;&#36827;&#34892;&#36816;&#21160;&#35268;&#21010;&#65292;&#24182;&#36890;&#36807;&#36816;&#21160;&#35268;&#21010;&#30340;&#21453;&#39304;&#26469;&#36845;&#20195;&#20248;&#21270;&#25552;&#35758;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#22788;&#29702;&#39046;&#22495;&#29305;&#23450;&#28040;&#24687;&#30340;&#35774;&#35745;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.11552</link><description>&lt;p&gt;
LLM^3:&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#20197;&#21450;&#36816;&#21160;&#22833;&#36133;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
LLM^3:Large Language Model-based Task and Motion Planning with Motion Failure Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11552
&lt;/p&gt;
&lt;p&gt;
LLM^3&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#20855;&#22791;&#24378;&#22823;&#30340;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#65292;&#36890;&#36807;&#25509;&#21475;&#25552;&#20986;&#31526;&#21495;&#21160;&#20316;&#24207;&#21015;&#21644;&#36873;&#25321;&#36830;&#32493;&#21160;&#20316;&#21442;&#25968;&#36827;&#34892;&#36816;&#21160;&#35268;&#21010;&#65292;&#24182;&#36890;&#36807;&#36816;&#21160;&#35268;&#21010;&#30340;&#21453;&#39304;&#26469;&#36845;&#20195;&#20248;&#21270;&#25552;&#35758;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#22788;&#29702;&#39046;&#22495;&#29305;&#23450;&#28040;&#24687;&#30340;&#35774;&#35745;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#65288;TAMP&#65289;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#24037;&#35774;&#35745;&#30340;&#30028;&#38754;&#65292;&#23558;&#31526;&#21495;&#20219;&#21153;&#35268;&#21010;&#19982;&#36830;&#32493;&#36816;&#21160;&#29983;&#25104;&#36830;&#25509;&#36215;&#26469;&#12290;&#36825;&#20123;&#29305;&#23450;&#39046;&#22495;&#30340;&#12289;&#21171;&#21160;&#23494;&#38598;&#22411;&#30340;&#27169;&#22359;&#22312;&#22788;&#29702;&#29616;&#23454;&#19990;&#30028;&#35774;&#32622;&#20013;&#20986;&#29616;&#30340;&#26032;&#20219;&#21153;&#26041;&#38754;&#26377;&#38480;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLM^3&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;TAMP&#26694;&#26550;&#65292;&#20855;&#26377;&#39046;&#22495;&#26080;&#20851;&#30340;&#25509;&#21475;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#30340;&#24378;&#22823;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#26469;&#25552;&#20986;&#31526;&#21495;&#21160;&#20316;&#24207;&#21015;&#65292;&#24182;&#36873;&#25321;&#36830;&#32493;&#21160;&#20316;&#21442;&#25968;&#36827;&#34892;&#36816;&#21160;&#35268;&#21010;&#12290;&#20851;&#38190;&#26159;&#65292;LLM^3&#36890;&#36807;&#25552;&#31034;&#23558;&#36816;&#21160;&#35268;&#21010;&#21453;&#39304;&#21040;&#20854;&#20013;&#65292;&#20351;&#24471;LLM&#33021;&#22815;&#36890;&#36807;&#23545;&#36816;&#21160;&#22833;&#36133;&#36827;&#34892;&#25512;&#29702;&#26469;&#36845;&#20195;&#22320;&#20248;&#21270;&#20854;&#25552;&#35758;&#12290;&#22240;&#27492;&#65292;LLM^3&#22312;&#20219;&#21153;&#35268;&#21010;&#21644;&#36816;&#21160;&#35268;&#21010;&#20043;&#38388;&#24314;&#31435;&#25509;&#21475;&#65292;&#20943;&#36731;&#20102;&#22788;&#29702;&#23427;&#20204;&#20043;&#38388;&#29305;&#23450;&#39046;&#22495;&#28040;&#24687;&#30340;&#22797;&#26434;&#35774;&#35745;&#36807;&#31243;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#20223;&#30495;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11552v1 Announce Type: cross  Abstract: Conventional Task and Motion Planning (TAMP) approaches rely on manually crafted interfaces connecting symbolic task planning with continuous motion generation. These domain-specific and labor-intensive modules are limited in addressing emerging tasks in real-world settings. Here, we present LLM^3, a novel Large Language Model (LLM)-based TAMP framework featuring a domain-independent interface. Specifically, we leverage the powerful reasoning and planning capabilities of pre-trained LLMs to propose symbolic action sequences and select continuous action parameters for motion planning. Crucially, LLM^3 incorporates motion planning feed- back through prompting, allowing the LLM to iteratively refine its proposals by reasoning about motion failure. Consequently, LLM^3 interfaces between task planning and motion planning, alleviating the intricate design process of handling domain- specific messages between them. Through a series of simulat
&lt;/p&gt;</description></item><item><title>ComTraQ-MPC&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;DQN&#21644;MPC&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#20248;&#21270;&#22312;&#26377;&#38480;&#20027;&#21160;&#23450;&#20301;&#26356;&#26032;&#19979;&#30340;&#36712;&#36857;&#36319;&#36394;&#12290;</title><link>https://arxiv.org/abs/2403.01564</link><description>&lt;p&gt;
ComTraQ-MPC&#65306;&#20803;&#35757;&#32451;&#30340;DQN-MPC&#38598;&#25104;&#29992;&#20110;&#20855;&#26377;&#26377;&#38480;&#20027;&#21160;&#23450;&#20301;&#26356;&#26032;&#30340;&#36712;&#36857;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
ComTraQ-MPC: Meta-Trained DQN-MPC Integration for Trajectory Tracking with Limited Active Localization Updates
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01564
&lt;/p&gt;
&lt;p&gt;
ComTraQ-MPC&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;DQN&#21644;MPC&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#20248;&#21270;&#22312;&#26377;&#38480;&#20027;&#21160;&#23450;&#20301;&#26356;&#26032;&#19979;&#30340;&#36712;&#36857;&#36319;&#36394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23616;&#37096;&#21487;&#35266;&#23519;&#12289;&#38543;&#26426;&#29615;&#22659;&#20013;&#36827;&#34892;&#36712;&#36857;&#36319;&#36394;&#30340;&#26368;&#20339;&#20915;&#31574;&#24448;&#24448;&#38754;&#20020;&#30528;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#21363;&#20027;&#21160;&#23450;&#20301;&#26356;&#26032;&#25968;&#37327;&#26377;&#38480;&#65292;&#36825;&#26159;&#25351;&#20195;&#29702;&#20174;&#20256;&#24863;&#22120;&#33719;&#21462;&#30495;&#23454;&#29366;&#24577;&#20449;&#24687;&#30340;&#36807;&#31243;&#12290;&#20256;&#32479;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#24179;&#34913;&#36164;&#28304;&#20445;&#23384;&#12289;&#20934;&#30830;&#29366;&#24577;&#20272;&#35745;&#21644;&#31934;&#30830;&#36319;&#36394;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#23548;&#33268;&#24615;&#33021;&#27425;&#20248;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ComTraQ-MPC&#65292;&#36825;&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;Deep Q-Networks (DQN)&#21644;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;(MPC)&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#26088;&#22312;&#20248;&#21270;&#26377;&#38480;&#20027;&#21160;&#23450;&#20301;&#26356;&#26032;&#19979;&#30340;&#36712;&#36857;&#36319;&#36394;&#12290;&#20803;&#35757;&#32451;&#30340;DQN&#30830;&#20445;&#20102;&#33258;&#36866;&#24212;&#20027;&#21160;&#23450;&#20301;&#35843;&#24230;&#65292;&#21516;&#26102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01564v1 Announce Type: cross  Abstract: Optimal decision-making for trajectory tracking in partially observable, stochastic environments where the number of active localization updates -- the process by which the agent obtains its true state information from the sensors -- are limited, presents a significant challenge. Traditional methods often struggle to balance resource conservation, accurate state estimation and precise tracking, resulting in suboptimal performance. This problem is particularly pronounced in environments with large action spaces, where the need for frequent, accurate state data is paramount, yet the capacity for active localization updates is restricted by external limitations. This paper introduces ComTraQ-MPC, a novel framework that combines Deep Q-Networks (DQN) and Model Predictive Control (MPC) to optimize trajectory tracking with constrained active localization updates. The meta-trained DQN ensures adaptive active localization scheduling, while the
&lt;/p&gt;</description></item></channel></rss>