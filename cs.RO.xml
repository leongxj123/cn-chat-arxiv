<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>ViSaRL&#25552;&#20986;&#20102;Visual Saliency-Guided Reinforcement Learning&#65288;&#21463;&#35270;&#35273;&#26174;&#33879;&#24615;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#35270;&#35273;&#34920;&#31034;&#26469;&#26174;&#33879;&#25552;&#39640;RL&#20195;&#29702;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#25104;&#21151;&#29575;&#12289;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.10940</link><description>&lt;p&gt;
ViSaRL&#65306;&#21463;&#20154;&#31867;&#26174;&#33879;&#24615;&#24341;&#23548;&#30340;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ViSaRL: Visual Reinforcement Learning Guided by Human Saliency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10940
&lt;/p&gt;
&lt;p&gt;
ViSaRL&#25552;&#20986;&#20102;Visual Saliency-Guided Reinforcement Learning&#65288;&#21463;&#35270;&#35273;&#26174;&#33879;&#24615;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#35270;&#35273;&#34920;&#31034;&#26469;&#26174;&#33879;&#25552;&#39640;RL&#20195;&#29702;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#25104;&#21151;&#29575;&#12289;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20174;&#39640;&#32500;&#20687;&#32032;&#36755;&#20837;&#22521;&#35757;&#26426;&#22120;&#20154;&#25191;&#34892;&#22797;&#26434;&#25511;&#21046;&#20219;&#21153;&#22312;&#26679;&#26412;&#25928;&#29575;&#19978;&#26159;&#20302;&#25928;&#30340;&#65292;&#22240;&#20026;&#22270;&#20687;&#35266;&#23519;&#20027;&#35201;&#30001;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#20449;&#24687;&#32452;&#25104;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#33021;&#22815;&#22312;&#35270;&#35273;&#19978;&#20851;&#27880;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#23545;&#35937;&#21644;&#21306;&#22495;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21463;&#35270;&#35273;&#26174;&#33879;&#24615;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;ViSaRL&#65289;&#12290;&#20351;&#29992;ViSaRL&#23398;&#20064;&#35270;&#35273;&#34920;&#31034;&#26174;&#30528;&#25552;&#39640;&#20102;RL&#20195;&#29702;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#65292;&#21253;&#25324;DeepMind&#25511;&#21046;&#22522;&#20934;&#12289;&#20223;&#30495;&#20013;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#21644;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#30340;&#25104;&#21151;&#29575;&#12289;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#26174;&#33879;&#24615;&#25972;&#21512;&#21040;&#22522;&#20110;CNN&#21644;Transformer&#30340;&#32534;&#30721;&#22120;&#20013;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20351;&#29992;ViSaRL&#23398;&#20064;&#30340;&#35270;&#35273;&#34920;&#31034;&#23545;&#21508;&#31181;&#35270;&#35273;&#25200;&#21160;&#65292;&#21253;&#25324;&#24863;&#30693;&#22122;&#22768;&#21644;&#22330;&#26223;&#21464;&#21270;&#65292;&#37117;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;ViSaRL&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#25104;&#21151;&#29575;&#20960;&#20046;&#32763;&#20102;&#19968;&#30058;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10940v1 Announce Type: cross  Abstract: Training robots to perform complex control tasks from high-dimensional pixel input using reinforcement learning (RL) is sample-inefficient, because image observations are comprised primarily of task-irrelevant information. By contrast, humans are able to visually attend to task-relevant objects and areas. Based on this insight, we introduce Visual Saliency-Guided Reinforcement Learning (ViSaRL). Using ViSaRL to learn visual representations significantly improves the success rate, sample efficiency, and generalization of an RL agent on diverse tasks including DeepMind Control benchmark, robot manipulation in simulation and on a real robot. We present approaches for incorporating saliency into both CNN and Transformer-based encoders. We show that visual representations learned using ViSaRL are robust to various sources of visual perturbations including perceptual noise and scene variations. ViSaRL nearly doubles success rate on the real-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#26434;&#20081;&#29615;&#22659;&#20013;&#21327;&#35843;&#26080;&#20154;&#26426;&#22242;&#38431;&#25293;&#25668;&#22797;&#26434;&#20154;&#32676;&#30340;&#22810;&#26080;&#20154;&#26426;&#22810;&#28436;&#21592;&#35270;&#35282;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20855;&#26377;&#36974;&#25377;&#24863;&#30693;&#30446;&#26631;&#30340;&#35270;&#35282;&#35268;&#21010;&#22120;&#36827;&#34892;&#24615;&#33021;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2310.10863</link><description>&lt;p&gt;
&#36138;&#24515;&#35270;&#35282;&#65306;&#22810;&#26080;&#20154;&#26426;&#35270;&#37326;&#35268;&#21010;&#22312;&#26434;&#20081;&#29615;&#22659;&#20013;&#30340;&#21327;&#21516;&#35206;&#30422;
&lt;/p&gt;
&lt;p&gt;
Greedy Perspectives: Multi-Drone View Planning for Collaborative Coverage in Cluttered Environments. (arXiv:2310.10863v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#26434;&#20081;&#29615;&#22659;&#20013;&#21327;&#35843;&#26080;&#20154;&#26426;&#22242;&#38431;&#25293;&#25668;&#22797;&#26434;&#20154;&#32676;&#30340;&#22810;&#26080;&#20154;&#26426;&#22810;&#28436;&#21592;&#35270;&#35282;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20855;&#26377;&#36974;&#25377;&#24863;&#30693;&#30446;&#26631;&#30340;&#35270;&#35282;&#35268;&#21010;&#22120;&#36827;&#34892;&#24615;&#33021;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20154;&#26426;&#22242;&#38431;&#30340;&#37096;&#32626;&#21487;&#20197;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#25293;&#25668;&#21160;&#24577;&#20154;&#32676;&#65288;&#28436;&#21592;&#65289;&#30340;&#22823;&#35268;&#27169;&#24433;&#20687;&#65292;&#29992;&#20110;&#22242;&#38431;&#36816;&#21160;&#21644;&#30005;&#24433;&#21046;&#20316;&#31561;&#26032;&#24212;&#29992;&#39046;&#22495;&#12290;&#20026;&#20102;&#23454;&#29616;&#35813;&#30446;&#26631;&#65292;&#21487;&#20197;&#20351;&#29992;&#36890;&#36807;&#39034;&#24207;&#36138;&#24515;&#35268;&#21010;&#36827;&#34892;&#23376;&#27169;&#26368;&#22823;&#21270;&#30340;&#26041;&#27861;&#65292;&#20197;&#20415;&#22312;&#26080;&#20154;&#26426;&#22242;&#38431;&#20043;&#38388;&#36827;&#34892;&#25668;&#20687;&#26426;&#35270;&#37326;&#30340;&#21487;&#25193;&#23637;&#20248;&#21270;&#65292;&#20294;&#22312;&#26434;&#20081;&#29615;&#22659;&#20013;&#21327;&#21516;&#25928;&#26524;&#38754;&#20020;&#25361;&#25112;&#12290;&#38556;&#30861;&#29289;&#21487;&#33021;&#20135;&#29983;&#36974;&#25377;&#24182;&#22686;&#21152;&#26080;&#20154;&#26426;&#30896;&#25758;&#30340;&#20960;&#29575;&#65292;&#36825;&#21487;&#33021;&#36829;&#21453;&#36817;&#20284;&#26368;&#20248;&#24615;&#30340;&#35201;&#27714;&#12290;&#20026;&#20102;&#22312;&#31264;&#23494;&#29615;&#22659;&#20013;&#21327;&#35843;&#26080;&#20154;&#26426;&#22242;&#38431;&#25293;&#25668;&#20154;&#32676;&#65292;&#38656;&#35201;&#19968;&#31181;&#26356;&#36890;&#29992;&#30340;&#35270;&#35282;&#35268;&#21010;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#20855;&#26377;&#36974;&#25377;&#24863;&#30693;&#30446;&#26631;&#30340;&#22810;&#26080;&#20154;&#26426;&#22810;&#28436;&#21592;&#35270;&#35282;&#35268;&#21010;&#22120;&#65292;&#24182;&#19982;&#36138;&#24515;&#24418;&#25104;&#35268;&#21010;&#22120;&#36827;&#34892;&#27604;&#36739;&#65292;&#25506;&#35752;&#36974;&#25377;&#21644;&#30896;&#25758;&#23545;&#25293;&#25668;&#24212;&#29992;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35780;&#20272;&#24615;&#33021;&#65292;
&lt;/p&gt;
&lt;p&gt;
Deployment of teams of aerial robots could enable large-scale filming of dynamic groups of people (actors) in complex environments for novel applications in areas such as team sports and cinematography. Toward this end, methods for submodular maximization via sequential greedy planning can be used for scalable optimization of camera views across teams of robots but face challenges with efficient coordination in cluttered environments. Obstacles can produce occlusions and increase chances of inter-robot collision which can violate requirements for near-optimality guarantees. To coordinate teams of aerial robots in filming groups of people in dense environments, a more general view-planning approach is required. We explore how collision and occlusion impact performance in filming applications through the development of a multi-robot multi-actor view planner with an occlusion-aware objective for filming groups of people and compare with a greedy formation planner. To evaluate performance,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#38480;&#20803;&#30340;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#21160;&#21147;&#23398;&#32593;&#32476;&#21644;&#29289;&#29702;&#24863;&#30693;&#32534;&#30721;&#22120;&#65292;&#20174;&#37096;&#20998;&#35266;&#23519;&#20013;&#23398;&#20064;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#21160;&#21147;&#23398;&#65292;&#24182;&#36890;&#36807;&#27491;&#36816;&#21160;&#23398;&#35299;&#30721;&#22120;&#36827;&#34892;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#29289;&#29702;&#35299;&#37322;&#24615;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.07975</link><description>&lt;p&gt;
&#22522;&#20110;&#26377;&#38480;&#20803;&#30340;&#32593;&#32476;: &#20174;&#37096;&#20998;&#35266;&#23519;&#20013;&#23398;&#20064;&#21512;&#29702;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Finite element inspired networks: Learning physically-plausible deformable object dynamics from partial observations. (arXiv:2307.07975v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07975
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#38480;&#20803;&#30340;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#21160;&#21147;&#23398;&#32593;&#32476;&#21644;&#29289;&#29702;&#24863;&#30693;&#32534;&#30721;&#22120;&#65292;&#20174;&#37096;&#20998;&#35266;&#23519;&#20013;&#23398;&#20064;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#21160;&#21147;&#23398;&#65292;&#24182;&#36890;&#36807;&#27491;&#36816;&#21160;&#23398;&#35299;&#30721;&#22120;&#36827;&#34892;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#29289;&#29702;&#35299;&#37322;&#24615;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#27169;&#25311;&#21487;&#21464;&#24418;&#32447;&#24615;&#29289;&#20307;&#65288;DLO&#65289;&#30340;&#21160;&#21147;&#23398;&#22312;&#38656;&#35201;&#19968;&#20010;&#21487;&#20197;&#34987;&#20154;&#35299;&#35835;&#21644;&#25968;&#25454;&#39640;&#25928;&#30340;&#27169;&#22411;&#24182;&#33021;&#22815;&#25552;&#20379;&#24555;&#36895;&#39044;&#27979;&#30340;&#20219;&#21153;&#20013;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#24471;&#21040;&#36825;&#26679;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#21018;&#24615;&#26377;&#38480;&#20803;&#26041;&#27861;&#65288;R-FEM&#65289;&#65292;&#23558;DLO&#24314;&#27169;&#20026;&#19968;&#31995;&#21015;&#21018;&#20307;&#38142;&#65292;&#20854;&#20869;&#37096;&#29366;&#24577;&#36890;&#36807;&#21160;&#21147;&#23398;&#32593;&#32476;&#20197;&#26102;&#38388;&#23637;&#24320;&#12290;&#30001;&#20110;&#35813;&#29366;&#24577;&#19981;&#33021;&#30452;&#25509;&#35266;&#23519;&#21040;&#65292;&#21160;&#21147;&#23398;&#32593;&#32476;&#19982;&#19968;&#20010;&#29289;&#29702;&#24863;&#30693;&#30340;&#32534;&#30721;&#22120;&#20849;&#21516;&#35757;&#32451;&#65292;&#23558;&#35266;&#23519;&#21040;&#30340;&#36816;&#21160;&#21464;&#37327;&#26144;&#23556;&#21040;&#21018;&#20307;&#38142;&#30340;&#29366;&#24577;&#12290;&#20026;&#20102;&#20419;&#20351;&#29366;&#24577;&#33719;&#24471;&#29289;&#29702;&#19978;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#21033;&#29992;&#24213;&#23618;R-FEM&#27169;&#22411;&#30340;&#27491;&#36816;&#21160;&#23398;&#65288;FK&#65289;&#20316;&#20026;&#35299;&#30721;&#22120;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#26426;&#22120;&#20154;&#23454;&#39564;&#20013;&#35777;&#26126;&#65292;&#36825;&#31181;&#34987;&#31216;&#20026;&#8220;&#26377;&#38480;&#20803;&#21551;&#21457;&#32593;&#32476;&#8221;&#30340;&#26550;&#26500;&#26159;&#19968;&#20010;&#26131;&#20110;&#22788;&#29702;&#20294;&#21151;&#33021;&#24378;&#22823;&#30340;DLO&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#37096;&#20998;&#35266;&#23519;&#20013;&#24471;&#20986;&#20855;&#26377;&#29289;&#29702;&#35299;&#37322;&#24615;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accurate simulation of deformable linear object (DLO) dynamics is challenging if the task at hand requires a human-interpretable and data-efficient model that also yields fast predictions. To arrive at such model, we draw inspiration from the rigid finite element method (R-FEM) and model a DLO as a serial chain of rigid bodies whose internal state is unrolled through time by a dynamics network. As this state is not observed directly, the dynamics network is trained jointly with a physics-informed encoder mapping observed motion variables to the body chain's state. To encourage that the state acquires a physically meaningful representation, we leverage the forward kinematics (FK) of the underlying R-FEM model as a decoder. We demonstrate in a robot experiment that this architecture - being termed "Finite element inspired network" - forms an easy to handle, yet capable DLO dynamics model yielding physically interpretable predictions from partial observations.  The project code is ava
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31995;&#32479;&#31070;&#32463;&#22810;&#26679;&#24615;&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24230;&#37327;&#20855;&#26377;&#38543;&#26426;&#31574;&#30053;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#34892;&#20026;&#24322;&#36136;&#24615;&#65292;&#25506;&#35752;&#20102;&#22810;&#26679;&#24615;&#23545;&#38598;&#20307;&#24377;&#24615;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.02128</link><description>&lt;p&gt;
&#31995;&#32479;&#31070;&#32463;&#22810;&#26679;&#24615;&#65306;&#22312;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#20013;&#24230;&#37327;&#34892;&#20026;&#24322;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;
System Neural Diversity: Measuring Behavioral Heterogeneity in Multi-Agent Learning. (arXiv:2305.02128v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31995;&#32479;&#31070;&#32463;&#22810;&#26679;&#24615;&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24230;&#37327;&#20855;&#26377;&#38543;&#26426;&#31574;&#30053;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#34892;&#20026;&#24322;&#36136;&#24615;&#65292;&#25506;&#35752;&#20102;&#22810;&#26679;&#24615;&#23545;&#38598;&#20307;&#24377;&#24615;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#31185;&#23398;&#25552;&#20379;&#20102;&#22810;&#26679;&#24615;&#20855;&#26377;&#38887;&#24615;&#30340;&#35777;&#25454;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#36890;&#24120;&#24378;&#21046;&#35201;&#27714;&#21516;&#36136;&#24615;&#20197;&#22686;&#21152;&#35757;&#32451;&#26679;&#26412;&#30340;&#25928;&#29575;&#12290;&#24403;&#23398;&#20064;&#20195;&#29702;&#31995;&#32479;&#19981;&#21463;&#21516;&#36136;&#31574;&#30053;&#30340;&#38480;&#21046;&#26102;&#65292;&#20010;&#20307;&#20195;&#29702;&#21487;&#33021;&#20250;&#21457;&#23637;&#20986;&#19981;&#21516;&#30340;&#34892;&#20026;&#65292;&#20174;&#32780;&#20135;&#29983;&#26377;&#21033;&#20110;&#31995;&#32479;&#30340;&#26032;&#20852;&#20114;&#34917;&#24615;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#32570;&#20047;&#34913;&#37327;&#23398;&#20064;&#20195;&#29702;&#31995;&#32479;&#20013;&#34892;&#20026;&#22810;&#26679;&#24615;&#30340;&#24037;&#20855;&#24847;&#21619;&#30528;&#25105;&#20204;&#26080;&#27861;&#28145;&#20837;&#20102;&#35299;&#22810;&#26679;&#24615;&#23545;&#38598;&#20307;&#24377;&#24615;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#31995;&#32479;&#31070;&#32463;&#22810;&#26679;&#24615;&#65288;SND&#65289;&#65306;&#19968;&#31181;&#29992;&#20110;&#20855;&#26377;&#38543;&#26426;&#31574;&#30053;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#34892;&#20026;&#24322;&#36136;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#25506;&#35752;&#24182;&#35777;&#26126;&#20102;&#20854;&#29702;&#35770;&#24615;&#36136;&#65292;&#24182;&#23558;&#20854;&#19982;&#36328;&#23398;&#31185;&#39046;&#22495;&#20013;&#20351;&#29992;&#30340;&#26368;&#26032;&#34892;&#20026;&#22810;&#26679;&#24615;&#25351;&#26631;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evolutionary science provides evidence that diversity confers resilience. Yet, traditional multi-agent reinforcement learning techniques commonly enforce homogeneity to increase training sample efficiency. When a system of learning agents is not constrained to homogeneous policies, individual agents may develop diverse behaviors, resulting in emergent complementarity that benefits the system. Despite this feat, there is a surprising lack of tools that measure behavioral diversity in systems of learning agents. Such techniques would pave the way towards understanding the impact of diversity in collective resilience and performance. In this paper, we introduce System Neural Diversity (SND): a measure of behavioral heterogeneity for multi-agent systems where agents have stochastic policies. %over a continuous state space. We discuss and prove its theoretical properties, and compare it with alternate, state-of-the-art behavioral diversity metrics used in cross-disciplinary domains. Through
&lt;/p&gt;</description></item></channel></rss>