<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#29366;&#24577;&#37325;&#26500;&#25193;&#25955;&#31574;&#30053; (SRDP) &#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#26368;&#26032;&#30340;&#25193;&#25955;&#31574;&#30053;&#31867;&#20013;&#24341;&#20837;&#20102;&#29366;&#24577;&#37325;&#26500;&#29305;&#24449;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#21644;&#26377;&#25928;&#34920;&#31034;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.04726</link><description>&lt;p&gt;
&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31163;&#25955;&#31574;&#30053;&#30340;&#25193;&#25955;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Diffusion Policies for Out-of-Distribution Generalization in Offline Reinforcement Learning. (arXiv:2307.04726v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04726
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#29366;&#24577;&#37325;&#26500;&#25193;&#25955;&#31574;&#30053; (SRDP) &#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#26368;&#26032;&#30340;&#25193;&#25955;&#31574;&#30053;&#31867;&#20013;&#24341;&#20837;&#20102;&#29366;&#24577;&#37325;&#26500;&#29305;&#24449;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#21644;&#26377;&#25928;&#34920;&#31034;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33073;&#26426;&#24378;&#21270;&#23398;&#20064; (RL) &#26041;&#27861;&#21033;&#29992;&#20197;&#21069;&#30340;&#32463;&#39564;&#26469;&#23398;&#20064;&#27604;&#29992;&#20110;&#25968;&#25454;&#25910;&#38598;&#30340;&#34892;&#20026;&#31574;&#30053;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;&#19982;&#34892;&#20026;&#20811;&#38534;&#30456;&#21453;&#65292;&#34892;&#20026;&#20811;&#38534;&#20551;&#35774;&#25968;&#25454;&#26159;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#25910;&#38598;&#30340;&#65292;&#32780;&#33073;&#26426; RL &#21487;&#20197;&#20351;&#29992;&#38750;&#19987;&#23478;&#25968;&#25454;&#21644;&#22810;&#27169;&#24577;&#34892;&#20026;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#33073;&#26426; RL &#31639;&#27861;&#22312;&#22788;&#29702;&#20998;&#24067;&#20559;&#31227;&#21644;&#26377;&#25928;&#34920;&#31034;&#31574;&#30053;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#35757;&#32451;&#36807;&#31243;&#20013;&#32570;&#20047;&#22312;&#32447;&#20132;&#20114;&#12290;&#20808;&#21069;&#20851;&#20110;&#33073;&#26426; RL &#30340;&#24037;&#20316;&#20351;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26469;&#34920;&#31034;&#25968;&#25454;&#38598;&#20013;&#30340;&#22810;&#27169;&#24577;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24182;&#27809;&#26377;&#38024;&#23545;&#32531;&#35299;&#33073;&#26426;&#20998;&#24067;&#29366;&#24577;&#27867;&#21270;&#32780;&#21046;&#23450;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;&#29366;&#24577;&#37325;&#26500;&#25193;&#25955;&#31574;&#30053; (SRDP)&#65292;&#23558;&#29366;&#24577;&#37325;&#26500;&#29305;&#24449;&#23398;&#20064;&#32435;&#20837;&#21040;&#26368;&#26032;&#30340;&#25193;&#25955;&#31574;&#30053;&#31867;&#20013;&#65292;&#20197;&#35299;&#20915;&#33073;&#26426;&#20998;&#24067;&#36890;&#29992;&#21270;&#38382;&#39064;&#12290;&#29366;&#24577;&#37325;&#26500;&#25439;&#22833;&#20419;&#36827;&#20102;&#26356;&#35814;&#32454;&#30340;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline Reinforcement Learning (RL) methods leverage previous experiences to learn better policies than the behavior policy used for data collection. In contrast to behavior cloning, which assumes the data is collected from expert demonstrations, offline RL can work with non-expert data and multimodal behavior policies. However, offline RL algorithms face challenges in handling distribution shifts and effectively representing policies due to the lack of online interaction during training. Prior work on offline RL uses conditional diffusion models to represent multimodal behavior in the dataset. Nevertheless, these methods are not tailored toward alleviating the out-of-distribution state generalization. We introduce a novel method, named State Reconstruction for Diffusion Policies (SRDP), incorporating state reconstruction feature learning in the recent class of diffusion policies to address the out-of-distribution generalization problem. State reconstruction loss promotes more descript
&lt;/p&gt;</description></item></channel></rss>