<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>SRLM &#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#22411;&#28151;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#20154;&#26426;&#20132;&#20114;&#24335;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#65292;&#36890;&#36807;&#23454;&#26102;&#30340;&#20154;&#31867;&#35821;&#35328;&#25351;&#20196;&#25512;&#26029;&#20840;&#23616;&#35268;&#21010;&#65292;&#24182;&#22312;&#20844;&#20849;&#31354;&#38388;&#20013;&#25552;&#20379;&#22810;&#31181;&#31038;&#20132;&#26381;&#21153;&#65292;&#34920;&#29616;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.15648</link><description>&lt;p&gt;
SRLM: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20154;&#26426;&#20132;&#20114;&#24335;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
SRLM: Human-in-Loop Interactive Social Robot Navigation with Large Language Model and Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15648
&lt;/p&gt;
&lt;p&gt;
SRLM &#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#22411;&#28151;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#20154;&#26426;&#20132;&#20114;&#24335;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#65292;&#36890;&#36807;&#23454;&#26102;&#30340;&#20154;&#31867;&#35821;&#35328;&#25351;&#20196;&#25512;&#26029;&#20840;&#23616;&#35268;&#21010;&#65292;&#24182;&#22312;&#20844;&#20849;&#31354;&#38388;&#20013;&#25552;&#20379;&#22810;&#31181;&#31038;&#20132;&#26381;&#21153;&#65292;&#34920;&#29616;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#21517;&#20132;&#20114;&#24335;&#31038;&#20132;&#26426;&#22120;&#20154;&#21161;&#25163;&#24517;&#39035;&#22312;&#22797;&#26434;&#25317;&#25380;&#30340;&#31354;&#38388;&#20013;&#25552;&#20379;&#26381;&#21153;&#65292;&#26681;&#25454;&#23454;&#26102;&#30340;&#20154;&#31867;&#35821;&#35328;&#25351;&#20196;&#25110;&#21453;&#39304;&#35843;&#25972;&#20854;&#34892;&#20026;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Social Robot Planner (SRLM) &#30340;&#26032;&#22411;&#28151;&#21512;&#26041;&#27861;&#65292;&#23427;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#25972;&#21512;&#36215;&#26469;&#65292;&#20197;&#22312;&#20805;&#26021;&#30528;&#20154;&#32676;&#30340;&#20844;&#20849;&#31354;&#38388;&#20013;&#23548;&#33322;&#65292;&#24182;&#25552;&#20379;&#22810;&#31181;&#31038;&#20132;&#26381;&#21153;&#12290;SRLM &#36890;&#36807;&#23454;&#26102;&#30340;&#20154;&#26426;&#20132;&#20114;&#25351;&#20196;&#25512;&#26029;&#20840;&#23616;&#35268;&#21010;&#65292;&#24182;&#23558;&#31038;&#20132;&#20449;&#24687;&#32534;&#30721;&#21040;&#22522;&#20110;LLM&#30340;&#22823;&#22411;&#23548;&#33322;&#27169;&#22411;&#65288;LNM&#65289;&#20013;&#65292;&#29992;&#20110;&#20302;&#23618;&#27425;&#30340;&#36816;&#21160;&#25191;&#34892;&#12290;&#27492;&#22806;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;DRL&#30340;&#35268;&#21010;&#22120;&#26469;&#20445;&#25345;&#22522;&#20934;&#24615;&#33021;&#65292;&#36890;&#36807;&#22823;&#22411;&#21453;&#39304;&#27169;&#22411;&#65288;LFM&#65289;&#19982;LNM&#34701;&#21512;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#25991;&#26412;&#21644;LLM&#39537;&#21160;&#30340;LNM&#30340;&#19981;&#31283;&#23450;&#24615;&#12290;&#26368;&#21518;&#65292;SRLM &#22312;&#24191;&#27867;&#30340;&#23454;&#39564;&#20013;&#23637;&#31034;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#26377;&#20851;&#27492;&#24037;&#20316;&#30340;&#26356;&#22810;&#35814;&#32454;&#20449;&#24687;&#65292;&#35831;&#35775;&#38382;&#65306;https://sites.g
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15648v1 Announce Type: cross  Abstract: An interactive social robotic assistant must provide services in complex and crowded spaces while adapting its behavior based on real-time human language commands or feedback. In this paper, we propose a novel hybrid approach called Social Robot Planner (SRLM), which integrates Large Language Models (LLM) and Deep Reinforcement Learning (DRL) to navigate through human-filled public spaces and provide multiple social services. SRLM infers global planning from human-in-loop commands in real-time, and encodes social information into a LLM-based large navigation model (LNM) for low-level motion execution. Moreover, a DRL-based planner is designed to maintain benchmarking performance, which is blended with LNM by a large feedback model (LFM) to address the instability of current text and LLM-driven LNM. Finally, SRLM demonstrates outstanding performance in extensive experiments. More details about this work are available at: https://sites.g
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22810;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#28151;&#21512;&#20559;&#22909;&#20998;&#24067;&#24182;&#20351;&#29992;MaxMin&#23545;&#40784;&#30446;&#26631;&#26469;&#26356;&#22909;&#22320;&#34920;&#31034;&#20154;&#31867;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.08925</link><description>&lt;p&gt;
MaxMin-RLHF:&#38754;&#21521;&#20855;&#26377;&#22810;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20844;&#24179;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
MaxMin-RLHF: Towards Equitable Alignment of Large Language Models with Diverse Human Preferences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08925
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22810;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#28151;&#21512;&#20559;&#22909;&#20998;&#24067;&#24182;&#20351;&#29992;MaxMin&#23545;&#40784;&#30446;&#26631;&#26469;&#26356;&#22909;&#22320;&#34920;&#31034;&#20154;&#31867;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;(RLHF)&#36890;&#36807;&#20351;&#29992;&#20174;&#20559;&#22909;&#25968;&#25454;&#20013;&#27966;&#29983;&#30340;&#21333;&#19968;&#22870;&#21169;&#27169;&#22411;&#26469;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24573;&#35270;&#20102;&#20174;&#22810;&#20010;&#29992;&#25143;&#25910;&#38598;&#30340;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#20154;&#31867;&#20559;&#22909;&#30340;&#20016;&#23500;&#22810;&#26679;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25512;&#23548;&#20986;&#20102;&#20351;&#29992;&#21333;&#19968;&#22870;&#21169;RLHF&#36827;&#34892;&#23545;&#40784;&#30340;&#19981;&#21487;&#33021;&#24615;&#32467;&#26524;&#65292;&#20174;&#32780;&#20984;&#26174;&#20102;&#20854;&#26080;&#27861;&#34920;&#31034;&#22810;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#12290;&#20026;&#20102;&#25552;&#20379;&#19968;&#20010;&#20844;&#24179;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#36890;&#36807;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#23398;&#20064;&#20102;&#19968;&#31181;&#28151;&#21512;&#20559;&#22909;&#20998;&#24067;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#20013;&#30340;&#24179;&#31561;&#21407;&#21017;&#21551;&#21457;&#30340;MaxMin&#23545;&#40784;&#30446;&#26631;&#26469;&#26356;&#22909;&#22320;&#34920;&#31034;&#22810;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#12290;&#25105;&#20204;&#38416;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#20998;&#24067;&#31283;&#20581;&#20248;&#21270;&#21644;&#36890;&#29992;&#25928;&#29992;RL&#30340;&#32852;&#31995;&#65292;&#20174;&#32780;&#31361;&#26174;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08925v1 Announce Type: cross Abstract: Reinforcement Learning from Human Feedback (RLHF) aligns language models to human preferences by employing a singular reward model derived from preference data. However, such an approach overlooks the rich diversity of human preferences inherent in data collected from multiple users. In this work, we first derive an impossibility result of alignment with single reward RLHF, thereby highlighting its insufficiency in representing diverse human preferences. To provide an equitable solution to the problem, we learn a mixture of preference distributions via an expectation-maximization algorithm and propose a MaxMin alignment objective for policy learning inspired by the Egalitarian principle in social choice theory to better represent diverse human preferences. We elucidate the connection of our proposed approach to distributionally robust optimization and general utility RL, thereby highlighting the generality and robustness of our proposed
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36870;&#21521;&#25235;&#21462;&#36807;&#31243;&#24182;&#21033;&#29992;&#25342;&#21462;&#21644;&#25918;&#32622;&#38382;&#39064;&#30340;&#23545;&#31216;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25342;&#21462;&#30340;&#25918;&#32622;&#26041;&#27861;&#65292;&#24182;&#29992;&#33258;&#20027;&#25910;&#38598;&#30340;&#28436;&#31034;&#30452;&#25509;&#35757;&#32451;&#31574;&#30053;&#65292;&#23454;&#29616;&#22312;&#25509;&#35302;&#21463;&#38480;&#29615;&#22659;&#19979;&#29289;&#20307;&#25918;&#32622;&#20219;&#21153;&#30340;&#33258;&#20027;&#25910;&#38598;&#21644;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2312.02352</link><description>&lt;p&gt;
&#36870;&#21521;&#23398;&#20064;&#65306;&#36890;&#36807;&#25441;&#21462;&#23398;&#20064;&#25918;&#32622;
&lt;/p&gt;
&lt;p&gt;
Working Backwards: Learning to Place by Picking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02352
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36870;&#21521;&#25235;&#21462;&#36807;&#31243;&#24182;&#21033;&#29992;&#25342;&#21462;&#21644;&#25918;&#32622;&#38382;&#39064;&#30340;&#23545;&#31216;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25342;&#21462;&#30340;&#25918;&#32622;&#26041;&#27861;&#65292;&#24182;&#29992;&#33258;&#20027;&#25910;&#38598;&#30340;&#28436;&#31034;&#30452;&#25509;&#35757;&#32451;&#31574;&#30053;&#65292;&#23454;&#29616;&#22312;&#25509;&#35302;&#21463;&#38480;&#29615;&#22659;&#19979;&#29289;&#20307;&#25918;&#32622;&#20219;&#21153;&#30340;&#33258;&#20027;&#25910;&#38598;&#21644;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25342;&#21462;&#65288;PvP&#65289;&#30340;&#25918;&#32622;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#20027;&#25910;&#38598;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#25918;&#32622;&#20219;&#21153;&#30340;&#29616;&#23454;&#19990;&#30028;&#28436;&#31034;&#65292;&#20854;&#20013;&#29289;&#20307;&#24517;&#39035;&#34987;&#25805;&#32437;&#21040;&#29305;&#23450;&#30340;&#25509;&#35302;&#38480;&#21046;&#20301;&#32622;&#12290;&#36890;&#36807;PvP&#65292;&#25105;&#20204;&#36890;&#36807;&#39072;&#20498;&#25235;&#21462;&#36807;&#31243;&#24182;&#21033;&#29992;&#25342;&#21462;&#21644;&#25918;&#32622;&#38382;&#39064;&#22266;&#26377;&#30340;&#23545;&#31216;&#24615;&#65292;&#25509;&#36817;&#20110;&#26426;&#22120;&#20154;&#29289;&#20307;&#25918;&#32622;&#28436;&#31034;&#30340;&#25910;&#38598;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#19968;&#32452;&#26368;&#21021;&#20301;&#20110;&#30446;&#26631;&#25918;&#32622;&#20301;&#32622;&#30340;&#29289;&#20307;&#30340;&#25235;&#21462;&#24207;&#21015;&#20013;&#33719;&#24471;&#25918;&#32622;&#28436;&#31034;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#22312;&#25509;&#35302;&#21463;&#38480;&#29615;&#22659;&#20013;&#25910;&#38598;&#25968;&#30334;&#20010;&#28436;&#31034;&#65292;&#32780;&#26080;&#38656;&#20154;&#31867;&#24178;&#39044;&#65292;&#36825;&#26159;&#36890;&#36807;&#32467;&#21512;&#20004;&#20010;&#27169;&#22359;&#23454;&#29616;&#30340;&#65306;&#35302;&#35273;&#37325;&#26032;&#25235;&#21462;&#21644;&#29992;&#20110;&#25235;&#21462;&#30340;&#39034;&#20174;&#25511;&#21046;&#12290;&#25105;&#20204;&#36890;&#36807;&#34892;&#20026;&#20811;&#38534;&#30452;&#25509;&#20174;&#35270;&#35273;&#35266;&#23519;&#20013;&#36890;&#36807;&#33258;&#20027;&#25910;&#38598;&#30340;&#28436;&#31034;&#20013;&#35757;&#32451;&#31574;&#30053;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#31574;&#30053;&#21487;&#20197;&#25512;&#24191;&#21040;&#36229;&#20986;&#35757;&#32451;&#29615;&#22659;&#33539;&#22260;&#30340;&#29289;&#20307;&#25918;&#32622;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02352v2 Announce Type: replace-cross  Abstract: We present placing via picking (PvP), a method to autonomously collect real-world demonstrations for a family of placing tasks in which objects must be manipulated to specific contact-constrained locations. With PvP, we approach the collection of robotic object placement demonstrations by reversing the grasping process and exploiting the inherent symmetry of the pick and place problems. Specifically, we obtain placing demonstrations from a set of grasp sequences of objects initially located at their target placement locations. Our system can collect hundreds of demonstrations in contact-constrained environments without human intervention by combining two modules: tactile regrasping and compliant control for grasps. We train a policy directly from visual observations through behavioral cloning, using the autonomously-collected demonstrations. By doing so, the policy can generalize to object placement scenarios outside of the tra
&lt;/p&gt;</description></item><item><title>PhotoBot&#26159;&#19968;&#20010;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#24341;&#23548;&#21644;&#26426;&#22120;&#20154;&#25668;&#24433;&#24072;&#30456;&#20114;&#20316;&#29992;&#30340;&#33258;&#21160;&#21270;&#29031;&#29255;&#33719;&#21462;&#26694;&#26550;&#12290;&#23427;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21644;&#29289;&#20307;&#26816;&#27979;&#22120;&#26469;&#25552;&#20379;&#25668;&#24433;&#24314;&#35758;&#65292;&#24182;&#36890;&#36807;&#35270;&#35273;&#21464;&#25442;&#22120;&#35745;&#31639;&#30456;&#26426;&#30340;&#23039;&#24577;&#35843;&#25972;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#29031;&#29255;&#33719;&#21462;&#12290;</title><link>http://arxiv.org/abs/2401.11061</link><description>&lt;p&gt;
PhotoBot&#65306;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#24341;&#23548;&#30340;&#21442;&#32771;&#20114;&#21160;&#25668;&#24433;
&lt;/p&gt;
&lt;p&gt;
PhotoBot: Reference-Guided Interactive Photography via Natural Language. (arXiv:2401.11061v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11061
&lt;/p&gt;
&lt;p&gt;
PhotoBot&#26159;&#19968;&#20010;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#24341;&#23548;&#21644;&#26426;&#22120;&#20154;&#25668;&#24433;&#24072;&#30456;&#20114;&#20316;&#29992;&#30340;&#33258;&#21160;&#21270;&#29031;&#29255;&#33719;&#21462;&#26694;&#26550;&#12290;&#23427;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21644;&#29289;&#20307;&#26816;&#27979;&#22120;&#26469;&#25552;&#20379;&#25668;&#24433;&#24314;&#35758;&#65292;&#24182;&#36890;&#36807;&#35270;&#35273;&#21464;&#25442;&#22120;&#35745;&#31639;&#30456;&#26426;&#30340;&#23039;&#24577;&#35843;&#25972;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#29031;&#29255;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;PhotoBot&#30340;&#26694;&#26550;&#65292;&#23427;&#22522;&#20110;&#39640;&#32423;&#20154;&#31867;&#35821;&#35328;&#24341;&#23548;&#21644;&#26426;&#22120;&#20154;&#25668;&#24433;&#24072;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#30340;&#29031;&#29255;&#33719;&#21462;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#20174;&#31574;&#23637;&#30011;&#24266;&#20013;&#26816;&#32034;&#21040;&#30340;&#21442;&#32771;&#22270;&#29255;&#21521;&#29992;&#25143;&#20256;&#36798;&#25668;&#24433;&#24314;&#35758;&#12290;&#25105;&#20204;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#21644;&#29289;&#20307;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#25991;&#26412;&#25551;&#36848;&#23545;&#21442;&#32771;&#22270;&#29255;&#36827;&#34892;&#29305;&#24449;&#21270;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#22522;&#20110;&#29992;&#25143;&#35821;&#35328;&#26597;&#35810;&#30340;&#25991;&#26412;&#25512;&#29702;&#26816;&#32034;&#30456;&#20851;&#30340;&#21442;&#32771;&#22270;&#29255;&#12290;&#20026;&#20102;&#23545;&#24212;&#21442;&#32771;&#22270;&#29255;&#21644;&#35266;&#23519;&#21040;&#30340;&#22330;&#26223;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#26174;&#33879;&#19981;&#21516;&#30340;&#22270;&#20687;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#39044;&#35757;&#32451;&#29305;&#24449;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#65292;&#36890;&#36807;&#35299;&#20915;&#19968;&#20010;&#36879;&#35270;n-&#28857;&#65288;PnP&#65289;&#38382;&#39064;&#26469;&#35745;&#31639;RGB-D&#30456;&#26426;&#30340;&#23039;&#24577;&#35843;&#25972;&#12290;&#25105;&#20204;&#22312;&#37197;&#22791;&#26377;&#25163;&#33109;&#30456;&#26426;&#30340;&#30495;&#23454;&#26426;&#26800;&#25163;&#33218;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#30001;PhotoBot&#25293;&#25668;&#30340;&#29031;&#29255;&#20855;&#26377;&#33391;&#22909;&#30340;&#36136;&#37327;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce PhotoBot, a framework for automated photo acquisition based on an interplay between high-level human language guidance and a robot photographer. We propose to communicate photography suggestions to the user via a reference picture that is retrieved from a curated gallery. We exploit a visual language model (VLM) and an object detector to characterize reference pictures via textual descriptions and use a large language model (LLM) to retrieve relevant reference pictures based on a user's language query through text-based reasoning. To correspond the reference picture and the observed scene, we exploit pre-trained features from a vision transformer capable of capturing semantic similarity across significantly varying images. Using these features, we compute pose adjustments for an RGB-D camera by solving a Perspective-n-Point (PnP) problem. We demonstrate our approach on a real-world manipulator equipped with a wrist camera. Our user studies show that photos taken by PhotoBo
&lt;/p&gt;</description></item></channel></rss>