<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#26412;&#30740;&#31350;&#25311;&#36890;&#36807;&#38750;&#35821;&#35328;&#34892;&#20026;&#25552;&#31034;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#39044;&#27979;&#20154;&#20204;&#23545;&#26426;&#22120;&#20154;&#34892;&#20026;&#21360;&#35937;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#21644;&#20998;&#26512;&#32467;&#26524;&#65292;&#21457;&#29616;&#22312;&#23548;&#33322;&#22330;&#26223;&#20013;&#65292;&#31354;&#38388;&#29305;&#24449;&#26159;&#26368;&#20851;&#38190;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2310.11590</link><description>&lt;p&gt;
&#25506;&#32034;&#22312;&#23548;&#33322;&#22330;&#26223;&#19979;&#25512;&#26029;&#29992;&#25143;&#23545;&#26426;&#22120;&#20154;&#24615;&#33021;&#30340;&#21360;&#35937;
&lt;/p&gt;
&lt;p&gt;
Towards Inferring Users' Impressions of Robot Performance in Navigation Scenarios. (arXiv:2310.11590v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25311;&#36890;&#36807;&#38750;&#35821;&#35328;&#34892;&#20026;&#25552;&#31034;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#39044;&#27979;&#20154;&#20204;&#23545;&#26426;&#22120;&#20154;&#34892;&#20026;&#21360;&#35937;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#21644;&#20998;&#26512;&#32467;&#26524;&#65292;&#21457;&#29616;&#22312;&#23548;&#33322;&#22330;&#26223;&#20013;&#65292;&#31354;&#38388;&#29305;&#24449;&#26159;&#26368;&#20851;&#38190;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#23545;&#26426;&#22120;&#20154;&#24615;&#33021;&#30340;&#21360;&#35937;&#36890;&#24120;&#36890;&#36807;&#35843;&#26597;&#38382;&#21367;&#26469;&#34913;&#37327;&#12290;&#20316;&#20026;&#19968;&#31181;&#26356;&#21487;&#25193;&#23637;&#19988;&#25104;&#26412;&#25928;&#30410;&#26356;&#39640;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#38750;&#35821;&#35328;&#34892;&#20026;&#25552;&#31034;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#39044;&#27979;&#20154;&#20204;&#23545;&#26426;&#22120;&#20154;&#34892;&#20026;&#21360;&#35937;&#30340;&#21487;&#33021;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;SEAN TOGETHER&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#22312;&#34394;&#25311;&#29616;&#23454;&#27169;&#25311;&#20013;&#20154;&#19982;&#31227;&#21160;&#26426;&#22120;&#20154;&#30456;&#20114;&#20316;&#29992;&#30340;&#35266;&#23519;&#32467;&#26524;&#65292;&#20197;&#21450;&#29992;&#25143;&#23545;&#26426;&#22120;&#20154;&#24615;&#33021;&#30340;5&#28857;&#37327;&#34920;&#35780;&#20215;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;&#20154;&#31867;&#21644;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#22914;&#20309;&#22522;&#20110;&#19981;&#21516;&#30340;&#35266;&#23519;&#31867;&#22411;&#65288;&#20363;&#22914;&#38754;&#37096;&#12289;&#31354;&#38388;&#21644;&#22320;&#22270;&#29305;&#24449;&#65289;&#26469;&#39044;&#27979;&#24863;&#30693;&#21040;&#30340;&#26426;&#22120;&#20154;&#24615;&#33021;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20165;&#20165;&#38754;&#37096;&#34920;&#24773;&#23601;&#33021;&#25552;&#20379;&#20851;&#20110;&#20154;&#20204;&#23545;&#26426;&#22120;&#20154;&#24615;&#33021;&#21360;&#35937;&#30340;&#26377;&#29992;&#20449;&#24687;&#65307;&#20294;&#22312;&#25105;&#20204;&#27979;&#35797;&#30340;&#23548;&#33322;&#22330;&#26223;&#20013;&#65292;&#31354;&#38388;&#29305;&#24449;&#26159;&#36825;&#31181;&#25512;&#26029;&#20219;&#21153;&#26368;&#20851;&#38190;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human impressions of robot performance are often measured through surveys. As a more scalable and cost-effective alternative, we study the possibility of predicting people's impressions of robot behavior using non-verbal behavioral cues and machine learning techniques. To this end, we first contribute the SEAN TOGETHER Dataset consisting of observations of an interaction between a person and a mobile robot in a Virtual Reality simulation, together with impressions of robot performance provided by users on a 5-point scale. Second, we contribute analyses of how well humans and supervised learning techniques can predict perceived robot performance based on different combinations of observation types (e.g., facial, spatial, and map features). Our results show that facial expressions alone provide useful information about human impressions of robot performance; but in the navigation scenarios we tested, spatial features are the most critical piece of information for this inference task. Als
&lt;/p&gt;</description></item></channel></rss>