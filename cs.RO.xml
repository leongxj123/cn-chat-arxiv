<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#36890;&#36807;&#24341;&#20837; BorealTC &#25968;&#25454;&#38598;&#65292;&#32467;&#21512;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#26032;&#39062;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;-Mamba&#20307;&#31995;&#32467;&#26500;&#22312;&#21271;&#26041;&#26862;&#26519;&#22320;&#24418;&#20998;&#31867;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.16877</link><description>&lt;p&gt;
&#24863;&#30693;&#21147;&#23601;&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;&#65306;&#21271;&#26041;&#26862;&#26519;&#30340;&#22320;&#24418;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Proprioception Is All You Need: Terrain Classification for Boreal Forests
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16877
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837; BorealTC &#25968;&#25454;&#38598;&#65292;&#32467;&#21512;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#26032;&#39062;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;-Mamba&#20307;&#31995;&#32467;&#26500;&#22312;&#21271;&#26041;&#26862;&#26519;&#22320;&#24418;&#20998;&#31867;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#39046;&#22495;&#26426;&#22120;&#20154;&#23398;&#30740;&#31350;&#24378;&#35843;&#20102;&#25269;&#24481;&#19981;&#21516;&#31867;&#22411;&#22320;&#24418;&#30340;&#37325;&#35201;&#24615;&#12290;&#21271;&#26041;&#26862;&#26519;&#29305;&#21035;&#21463;&#21040;&#35768;&#22810;&#38480;&#21046;&#26426;&#21160;&#24615;&#30340;&#22320;&#24418;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#22320;&#24418;&#24212;&#35813;&#22312;&#36234;&#37326;&#33258;&#20027;&#23548;&#33322;&#20013;&#21152;&#20197;&#32771;&#34385;&#12290;&#27492;&#22806;&#65292;&#20316;&#20026;&#22320;&#29699;&#19978;&#26368;&#22823;&#30340;&#38470;&#22320;&#29983;&#29289;&#32676;&#33853;&#20043;&#19968;&#65292;&#21271;&#26041;&#26862;&#26519;&#26159;&#39044;&#35745;&#33258;&#20027;&#36710;&#36742;&#23558;&#26085;&#30410;&#26222;&#21450;&#30340;&#22320;&#21306;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;BorealTC&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22522;&#20110;&#24863;&#30693;&#21147;&#30340;&#22320;&#24418;&#20998;&#31867;&#65288;TC&#65289;&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#35760;&#24405;&#20102;Husky A200&#30340;116&#20998;&#38047;&#30340;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#65288;IMU&#65289;&#12289;&#30005;&#26426;&#30005;&#27969;&#21644;&#36718;&#32974;&#37324;&#31243;&#25968;&#25454;&#65292;&#37325;&#28857;&#20851;&#27880;&#20856;&#22411;&#30340;&#21271;&#26041;&#26862;&#26519;&#22320;&#24418;&#65292;&#29305;&#21035;&#26159;&#38634;&#12289;&#20912;&#21644;&#28132;&#27877;&#22756;&#12290;&#32467;&#21512;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19982;&#21478;&#19968;&#20010;&#26469;&#33258;&#26368;&#26032;&#25216;&#26415;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#22312;TC t
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16877v1 Announce Type: cross  Abstract: Recent works in field robotics highlighted the importance of resiliency against different types of terrains. Boreal forests, in particular, are home to many mobility-impeding terrains that should be considered for off-road autonomous navigation. Also, being one of the largest land biomes on Earth, boreal forests are an area where autonomous vehicles are expected to become increasingly common. In this paper, we address this issue by introducing BorealTC, a publicly available dataset for proprioceptive-based terrain classification (TC). Recorded with a Husky A200, our dataset contains 116 min of Inertial Measurement Unit (IMU), motor current, and wheel odometry data, focusing on typical boreal forest terrains, notably snow, ice, and silty loam. Combining our dataset with another dataset from the state-of-the-art, we evaluate both a Convolutional Neural Network (CNN) and the novel state space model (SSM)-based Mamba architecture on a TC t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#35302;&#35273;&#20449;&#24687;&#32435;&#20837;&#27169;&#20223;&#23398;&#20064;&#24179;&#21488;&#20197;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#65292;&#36890;&#36807;&#35757;&#32451;&#26426;&#22120;&#20154;&#20195;&#29702;&#25554;&#25300;USB&#30005;&#32518;&#65292;&#23454;&#29616;&#20102;&#22312;&#24494;&#32454;&#25805;&#32437;&#20219;&#21153;&#20013;&#30340;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.11898</link><description>&lt;p&gt;
&#35270;&#35273;-&#35302;&#35273;&#39044;&#35757;&#32451;&#29992;&#20110;&#25554;&#25300;&#30005;&#32518;
&lt;/p&gt;
&lt;p&gt;
Visuo-Tactile Pretraining for Cable Plugging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#35302;&#35273;&#20449;&#24687;&#32435;&#20837;&#27169;&#20223;&#23398;&#20064;&#24179;&#21488;&#20197;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#65292;&#36890;&#36807;&#35757;&#32451;&#26426;&#22120;&#20154;&#20195;&#29702;&#25554;&#25300;USB&#30005;&#32518;&#65292;&#23454;&#29616;&#20102;&#22312;&#24494;&#32454;&#25805;&#32437;&#20219;&#21153;&#20013;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35302;&#35273;&#20449;&#24687;&#26159;&#36827;&#34892;&#31934;&#32454;&#25805;&#32437;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;&#20316;&#20026;&#20154;&#31867;&#65292;&#25105;&#20204;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#35302;&#35273;&#20449;&#24687;&#26469;&#29702;&#35299;&#21608;&#22260;&#30340;&#29289;&#20307;&#20197;&#21450;&#22914;&#20309;&#19982;&#20854;&#20114;&#21160;&#12290;&#25105;&#20204;&#19981;&#20165;&#20351;&#29992;&#35302;&#25720;&#26469;&#25191;&#34892;&#25805;&#32437;&#20219;&#21153;&#65292;&#36824;&#29992;&#23427;&#26469;&#23398;&#20064;&#22914;&#20309;&#25191;&#34892;&#36825;&#20123;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#21019;&#24314;&#33021;&#22815;&#23398;&#20064;&#20197;&#20154;&#31867;&#25110;&#36229;&#20154;&#31867;&#27700;&#24179;&#23436;&#25104;&#25805;&#32437;&#20219;&#21153;&#30340;&#26426;&#22120;&#20154;&#20195;&#29702;&#65292;&#25105;&#20204;&#38656;&#35201;&#27491;&#30830;&#22320;&#23558;&#35302;&#35273;&#20449;&#24687;&#34701;&#20837;&#25216;&#33021;&#25191;&#34892;&#21644;&#25216;&#33021;&#23398;&#20064;&#20013;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#35302;&#35273;&#20449;&#24687;&#32435;&#20837;&#27169;&#20223;&#23398;&#20064;&#24179;&#21488;&#20197;&#25552;&#39640;&#22797;&#26434;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30528;&#25163;&#35299;&#20915;&#25554;&#25300;USB&#30005;&#32518;&#30340;&#25361;&#25112;&#65292;&#36825;&#26159;&#19968;&#39033;&#20381;&#36182;&#20110;&#24494;&#35266;&#35270;&#35273;-&#35302;&#35273;&#21327;&#20316;&#30340;&#29087;&#32451;&#25805;&#32437;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#35302;&#35273;&#20449;&#24687;&#32435;&#20837;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#25105;&#20204;&#33021;&#22815;&#35757;&#32451;&#19968;&#20010;&#26426;&#22120;&#20154;&#20195;&#29702;&#25554;&#25300;USB&#30005;&#32518;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11898v1 Announce Type: cross  Abstract: Tactile information is a critical tool for fine-grain manipulation. As humans, we rely heavily on tactile information to understand objects in our environments and how to interact with them. We use touch not only to perform manipulation tasks but also to learn how to perform these tasks. Therefore, to create robotic agents that can learn to complete manipulation tasks at a human or super-human level of performance, we need to properly incorporate tactile information into both skill execution and skill learning. In this paper, we investigate how we can incorporate tactile information into imitation learning platforms to improve performance on complex tasks. To do this, we tackle the challenge of plugging in a USB cable, a dexterous manipulation task that relies on fine-grain visuo-tactile serving. By incorporating tactile information into imitation learning frameworks, we are able to train a robotic agent to plug in a USB cable - a firs
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#36125;&#21494;&#26031;&#26410;&#26469;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#36234;&#37326;&#22320;&#22270;&#30340;&#21046;&#20316;&#65292;&#20026;&#38271;&#31243;&#39044;&#27979;&#25552;&#20379;&#26356;&#22909;&#30340;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2403.11876</link><description>&lt;p&gt;
&#28145;&#24230;&#36125;&#21494;&#26031;&#26410;&#26469;&#34701;&#21512;&#29992;&#20110;&#33258;&#30417;&#30563;&#12289;&#39640;&#20998;&#36776;&#29575;&#12289;&#36234;&#37326;&#22320;&#22270;&#21046;&#20316;
&lt;/p&gt;
&lt;p&gt;
Deep Bayesian Future Fusion for Self-Supervised, High-Resolution, Off-Road Mapping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11876
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#36125;&#21494;&#26031;&#26410;&#26469;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#36234;&#37326;&#22320;&#22270;&#30340;&#21046;&#20316;&#65292;&#20026;&#38271;&#31243;&#39044;&#27979;&#25552;&#20379;&#26356;&#22909;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36164;&#28304;&#21463;&#38480;&#30340;&#36234;&#37326;&#36710;&#36742;&#30340;&#20256;&#24863;&#22120;&#20998;&#36776;&#29575;&#26377;&#38480;&#65292;&#36825;&#32473;&#21487;&#38752;&#30340;&#36234;&#37326;&#33258;&#20027;&#24615;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#34701;&#21512;&#26410;&#26469;&#20449;&#24687;&#65288;&#21363;&#26410;&#26469;&#34701;&#21512;&#65289;&#36827;&#34892;&#33258;&#30417;&#30563;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#21033;&#29992;&#26410;&#26469;&#20449;&#24687;&#20197;&#21450;&#25163;&#24037;&#21046;&#20316;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#30452;&#25509;&#30417;&#30563;&#30446;&#26631;&#19979;&#28216;&#20219;&#21153;&#65288;&#20363;&#22914;&#21487;&#31359;&#36234;&#24615;&#20272;&#35745;&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#19968;&#20010;&#26356;&#20026;&#36890;&#29992;&#30340;&#21457;&#23637;&#26041;&#21521; - &#36890;&#36807;&#26410;&#26469;&#34701;&#21512;&#20197;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#26102;&#38388;&#39640;&#25928;&#22320;&#23436;&#25104;&#26368;&#39640;&#20998;&#36776;&#29575;&#65288;&#21363;&#27599;&#20687;&#32032;2&#21400;&#31859;&#65289;BEV&#22320;&#22270;&#65292;&#21487;&#29992;&#20110;&#20219;&#20309;&#19979;&#28216;&#20219;&#21153;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#38271;&#31243;&#39044;&#27979;&#12290;&#20026;&#27492;&#65292;&#39318;&#20808;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#39640;&#20998;&#36776;&#29575;&#26410;&#26469;&#34701;&#21512;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#65288;RGB / &#39640;&#24230;&#65289;&#21407;&#22987;&#31232;&#30095;&#22122;&#38899;&#36755;&#20837;&#21644;&#22522;&#20110;&#22320;&#22270;&#30340;&#23494;&#38598;&#26631;&#31614;&#30340;&#25104;&#23545;&#25968;&#25454;&#12290;&#25509;&#19979;&#26469;&#65292;&#20026;&#20102;&#36866;&#24212;&#20256;&#24863;&#22120;&#30340;&#22122;&#22768;&#21644;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11876v1 Announce Type: cross  Abstract: The limited sensing resolution of resource-constrained off-road vehicles poses significant challenges towards reliable off-road autonomy. To overcome this limitation, we propose a general framework based on fusing the future information (i.e. future fusion) for self-supervision. Recent approaches exploit this future information alongside the hand-crafted heuristics to directly supervise the targeted downstream tasks (e.g. traversability estimation). However, in this paper, we opt for a more general line of development - time-efficient completion of the highest resolution (i.e. 2cm per pixel) BEV map in a self-supervised manner via future fusion, which can be used for any downstream tasks for better longer range prediction. To this end, first, we create a high-resolution future-fusion dataset containing pairs of (RGB / height) raw sparse and noisy inputs and map-based dense labels. Next, to accommodate the noise and sparsity of the sens
&lt;/p&gt;</description></item></channel></rss>