<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35270;&#35273;&#20840;&#36523;&#25511;&#21046;&#30340;&#26694;&#26550;&#65292;&#20351;&#33151;&#24335;&#26426;&#22120;&#20154;&#33021;&#22815;&#21516;&#26102;&#25511;&#21046;&#33151;&#37096;&#21644;&#25163;&#33218;&#65292;&#20197;&#25193;&#23637;&#25805;&#20316;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20223;&#30495;&#35757;&#32451;&#21644;Sim2Real&#36716;&#31227;&#23454;&#29616;&#20102;&#22312;&#25441;&#36215;&#19981;&#21516;&#29289;&#20307;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.16967</link><description>&lt;p&gt;
&#29992;&#20110;&#33151;&#24335;&#23450;&#28857;&#26426;&#22120;&#20154;&#36816;&#21160;&#25805;&#20316;&#30340;&#35270;&#35273;&#20840;&#36523;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Visual Whole-Body Control for Legged Loco-Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16967
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35270;&#35273;&#20840;&#36523;&#25511;&#21046;&#30340;&#26694;&#26550;&#65292;&#20351;&#33151;&#24335;&#26426;&#22120;&#20154;&#33021;&#22815;&#21516;&#26102;&#25511;&#21046;&#33151;&#37096;&#21644;&#25163;&#33218;&#65292;&#20197;&#25193;&#23637;&#25805;&#20316;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20223;&#30495;&#35757;&#32451;&#21644;Sim2Real&#36716;&#31227;&#23454;&#29616;&#20102;&#22312;&#25441;&#36215;&#19981;&#21516;&#29289;&#20307;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#37197;&#22791;&#25163;&#33218;&#30340;&#33151;&#24335;&#26426;&#22120;&#20154;&#36827;&#34892;&#31227;&#21160;&#25805;&#20316;&#30340;&#38382;&#39064;&#65292;&#21363;&#33151;&#24335;&#23450;&#28857;&#25805;&#20316;&#12290;&#23613;&#31649;&#26426;&#22120;&#20154;&#30340;&#33151;&#36890;&#24120;&#29992;&#20110;&#31227;&#21160;&#65292;&#20294;&#36890;&#36807;&#36827;&#34892;&#20840;&#36523;&#25511;&#21046;&#65292;&#21487;&#20197;&#25193;&#22823;&#20854;&#25805;&#20316;&#33021;&#21147;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#21516;&#26102;&#25511;&#21046;&#33151;&#37096;&#21644;&#25163;&#33218;&#65292;&#20197;&#25193;&#23637;&#20854;&#24037;&#20316;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#20351;&#29992;&#35270;&#35273;&#35266;&#27979;&#33258;&#20027;&#36827;&#34892;&#20840;&#36523;&#25511;&#21046;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;\ourFull~(\our)&#65292;&#30001;&#19968;&#20010;&#20302;&#32423;&#31574;&#30053;&#21644;&#19968;&#20010;&#39640;&#32423;&#31574;&#30053;&#32452;&#25104;&#12290;&#20302;&#32423;&#31574;&#30053;&#20351;&#29992;&#25152;&#26377;&#33258;&#30001;&#24230;&#26469;&#36319;&#36394;&#26411;&#31471;&#25191;&#34892;&#22120;&#30340;&#20301;&#32622;&#65292;&#39640;&#32423;&#31574;&#30053;&#26681;&#25454;&#35270;&#35273;&#36755;&#20837;&#25552;&#20986;&#26411;&#31471;&#25191;&#34892;&#22120;&#20301;&#32622;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#20013;&#35757;&#32451;&#20102;&#20004;&#20010;&#32423;&#21035;&#30340;&#31574;&#30053;&#65292;&#24182;&#36827;&#34892;&#20102;&#20174;Sim&#21040;&#23454;&#29289;&#30340;&#36716;&#31227;&#20197;&#36827;&#34892;&#23454;&#38469;&#26426;&#22120;&#20154;&#37096;&#32626;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#19981;&#21516;&#37197;&#32622;&#19979;&#65288;&#39640;&#24230;&#12289;&#65289;&#25441;&#36215;&#19981;&#21516;&#29289;&#20307;&#26041;&#38754;&#65292;&#30456;&#23545;&#22522;&#32447;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16967v1 Announce Type: cross  Abstract: We study the problem of mobile manipulation using legged robots equipped with an arm, namely legged loco-manipulation. The robot legs, while usually utilized for mobility, offer an opportunity to amplify the manipulation capabilities by conducting whole-body control. That is, the robot can control the legs and the arm at the same time to extend its workspace. We propose a framework that can conduct the whole-body control autonomously with visual observations. Our approach, namely \ourFull~(\our), is composed of a low-level policy using all degrees of freedom to track the end-effector manipulator position and a high-level policy proposing the end-effector position based on visual inputs. We train both levels of policies in simulation and perform Sim2Real transfer for real robot deployment. We perform extensive experiments and show significant improvements over baselines in picking up diverse objects in different configurations (heights,
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31616;&#21333;&#30340;&#36845;&#20195;&#36125;&#21494;&#26031;&#26356;&#26032;&#26041;&#26696;&#21644;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#28151;&#21512;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#27169;&#22411;&#20026;&#20154;&#32676;&#23548;&#33322;&#25552;&#20379;&#20102;&#23454;&#26102;&#19988;&#21487;&#25193;&#23637;&#30340;&#20915;&#31574;&#21046;&#23450;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.01537</link><description>&lt;p&gt;
&#28151;&#21512;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#29992;&#20110;&#20154;&#32676;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Mixed-Strategy Nash Equilibrium for Crowd Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01537
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31616;&#21333;&#30340;&#36845;&#20195;&#36125;&#21494;&#26031;&#26356;&#26032;&#26041;&#26696;&#21644;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#28151;&#21512;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#27169;&#22411;&#20026;&#20154;&#32676;&#23548;&#33322;&#25552;&#20379;&#20102;&#23454;&#26102;&#19988;&#21487;&#25193;&#23637;&#30340;&#20915;&#31574;&#21046;&#23450;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#38024;&#23545;&#20154;&#32676;&#23548;&#33322;&#25214;&#21040;&#28151;&#21512;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#30340;&#38382;&#39064;&#12290;&#28151;&#21512;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#20026;&#26426;&#22120;&#20154;&#25552;&#20379;&#20102;&#19968;&#20010;&#20005;&#35880;&#30340;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#39044;&#27979;&#20154;&#32676;&#20013;&#19981;&#30830;&#23450;&#20294;&#21512;&#20316;&#30340;&#20154;&#31867;&#34892;&#20026;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#36890;&#24120;&#22826;&#39640;&#65292;&#26080;&#27861;&#36827;&#34892;&#21487;&#25193;&#23637;&#21644;&#23454;&#26102;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#36845;&#20195;&#36125;&#21494;&#26031;&#26356;&#26032;&#26041;&#26696;&#25910;&#25947;&#20110;&#28151;&#21512;&#31574;&#30053;&#31038;&#20132;&#23548;&#33322;&#28216;&#25103;&#30340;&#32435;&#20160;&#22343;&#34913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20195;&#29702;&#31574;&#30053;&#21021;&#22987;&#21270;&#20026;&#20174;&#20154;&#31867;&#25968;&#25454;&#38598;&#23398;&#20064;&#30340;&#39640;&#26031;&#36807;&#31243;&#65292;&#26469;&#26500;&#24314;&#35813;&#28216;&#25103;&#12290;&#22522;&#20110;&#25152;&#25552;&#20986;&#30340;&#28151;&#21512;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#27169;&#22411;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#37319;&#26679;&#30340;&#20154;&#32676;&#23548;&#33322;&#26694;&#26550;&#65292;&#21487;&#20197;&#38598;&#25104;&#21040;&#29616;&#26377;&#23548;&#33322;&#26041;&#27861;&#20013;&#65292;&#24182;&#21487;&#22312;&#31508;&#35760;&#26412;&#30005;&#33041; CPU &#19978;&#23454;&#26102;&#36816;&#34892;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#29615;&#22659;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#20154;&#31867;&#25968;&#25454;&#38598;&#23545;&#25105;&#20204;&#30340;&#26694;&#26550;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01537v1 Announce Type: cross  Abstract: We address the problem of finding mixed-strategy Nash equilibrium for crowd navigation. Mixed-strategy Nash equilibrium provides a rigorous model for the robot to anticipate uncertain yet cooperative human behavior in crowds, but the computation cost is often too high for scalable and real-time decision-making. Here we prove that a simple iterative Bayesian updating scheme converges to the Nash equilibrium of a mixed-strategy social navigation game. Furthermore, we propose a data-driven framework to construct the game by initializing agent strategies as Gaussian processes learned from human datasets. Based on the proposed mixed-strategy Nash equilibrium model, we develop a sampling-based crowd navigation framework that can be integrated into existing navigation methods and runs in real-time on a laptop CPU. We evaluate our framework in both simulated environments and real-world human datasets in unstructured environments. Our framework
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25311;&#36890;&#36807;&#38750;&#35821;&#35328;&#34892;&#20026;&#25552;&#31034;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#39044;&#27979;&#20154;&#20204;&#23545;&#26426;&#22120;&#20154;&#34892;&#20026;&#21360;&#35937;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#21644;&#20998;&#26512;&#32467;&#26524;&#65292;&#21457;&#29616;&#22312;&#23548;&#33322;&#22330;&#26223;&#20013;&#65292;&#31354;&#38388;&#29305;&#24449;&#26159;&#26368;&#20851;&#38190;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2310.11590</link><description>&lt;p&gt;
&#25506;&#32034;&#22312;&#23548;&#33322;&#22330;&#26223;&#19979;&#25512;&#26029;&#29992;&#25143;&#23545;&#26426;&#22120;&#20154;&#24615;&#33021;&#30340;&#21360;&#35937;
&lt;/p&gt;
&lt;p&gt;
Towards Inferring Users' Impressions of Robot Performance in Navigation Scenarios. (arXiv:2310.11590v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25311;&#36890;&#36807;&#38750;&#35821;&#35328;&#34892;&#20026;&#25552;&#31034;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#39044;&#27979;&#20154;&#20204;&#23545;&#26426;&#22120;&#20154;&#34892;&#20026;&#21360;&#35937;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#21644;&#20998;&#26512;&#32467;&#26524;&#65292;&#21457;&#29616;&#22312;&#23548;&#33322;&#22330;&#26223;&#20013;&#65292;&#31354;&#38388;&#29305;&#24449;&#26159;&#26368;&#20851;&#38190;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#23545;&#26426;&#22120;&#20154;&#24615;&#33021;&#30340;&#21360;&#35937;&#36890;&#24120;&#36890;&#36807;&#35843;&#26597;&#38382;&#21367;&#26469;&#34913;&#37327;&#12290;&#20316;&#20026;&#19968;&#31181;&#26356;&#21487;&#25193;&#23637;&#19988;&#25104;&#26412;&#25928;&#30410;&#26356;&#39640;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#38750;&#35821;&#35328;&#34892;&#20026;&#25552;&#31034;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#39044;&#27979;&#20154;&#20204;&#23545;&#26426;&#22120;&#20154;&#34892;&#20026;&#21360;&#35937;&#30340;&#21487;&#33021;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;SEAN TOGETHER&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#22312;&#34394;&#25311;&#29616;&#23454;&#27169;&#25311;&#20013;&#20154;&#19982;&#31227;&#21160;&#26426;&#22120;&#20154;&#30456;&#20114;&#20316;&#29992;&#30340;&#35266;&#23519;&#32467;&#26524;&#65292;&#20197;&#21450;&#29992;&#25143;&#23545;&#26426;&#22120;&#20154;&#24615;&#33021;&#30340;5&#28857;&#37327;&#34920;&#35780;&#20215;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;&#20154;&#31867;&#21644;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#22914;&#20309;&#22522;&#20110;&#19981;&#21516;&#30340;&#35266;&#23519;&#31867;&#22411;&#65288;&#20363;&#22914;&#38754;&#37096;&#12289;&#31354;&#38388;&#21644;&#22320;&#22270;&#29305;&#24449;&#65289;&#26469;&#39044;&#27979;&#24863;&#30693;&#21040;&#30340;&#26426;&#22120;&#20154;&#24615;&#33021;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20165;&#20165;&#38754;&#37096;&#34920;&#24773;&#23601;&#33021;&#25552;&#20379;&#20851;&#20110;&#20154;&#20204;&#23545;&#26426;&#22120;&#20154;&#24615;&#33021;&#21360;&#35937;&#30340;&#26377;&#29992;&#20449;&#24687;&#65307;&#20294;&#22312;&#25105;&#20204;&#27979;&#35797;&#30340;&#23548;&#33322;&#22330;&#26223;&#20013;&#65292;&#31354;&#38388;&#29305;&#24449;&#26159;&#36825;&#31181;&#25512;&#26029;&#20219;&#21153;&#26368;&#20851;&#38190;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human impressions of robot performance are often measured through surveys. As a more scalable and cost-effective alternative, we study the possibility of predicting people's impressions of robot behavior using non-verbal behavioral cues and machine learning techniques. To this end, we first contribute the SEAN TOGETHER Dataset consisting of observations of an interaction between a person and a mobile robot in a Virtual Reality simulation, together with impressions of robot performance provided by users on a 5-point scale. Second, we contribute analyses of how well humans and supervised learning techniques can predict perceived robot performance based on different combinations of observation types (e.g., facial, spatial, and map features). Our results show that facial expressions alone provide useful information about human impressions of robot performance; but in the navigation scenarios we tested, spatial features are the most critical piece of information for this inference task. Als
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#24378;&#21270;&#23398;&#20064;&#30340;&#38646;&#26679;&#26412;&#26080;&#32447;&#23460;&#20869;&#23548;&#33322;&#26041;&#27861;&#65292;&#36890;&#36807;&#26679;&#26412;&#39640;&#25928;&#23398;&#20064;&#21644;&#38646;&#26679;&#26412;&#27867;&#21270;&#26469;&#25552;&#39640;&#23548;&#33322;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.06766</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#26080;&#32447;&#23460;&#20869;&#23548;&#33322;&#36890;&#36807;&#29289;&#29702;&#20449;&#24687;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Wireless Indoor Navigation through Physics-Informed Reinforcement Learning. (arXiv:2306.06766v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06766
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#24378;&#21270;&#23398;&#20064;&#30340;&#38646;&#26679;&#26412;&#26080;&#32447;&#23460;&#20869;&#23548;&#33322;&#26041;&#27861;&#65292;&#36890;&#36807;&#26679;&#26412;&#39640;&#25928;&#23398;&#20064;&#21644;&#38646;&#26679;&#26412;&#27867;&#21270;&#26469;&#25552;&#39640;&#23548;&#33322;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#23460;&#20869;&#26426;&#22120;&#20154;&#23548;&#33322;&#21033;&#29992;&#26080;&#32447;&#20449;&#21495;&#30340;&#19981;&#26029;&#20851;&#27880;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#24378;&#21270;&#23398;&#20064;&#65288;PIRL&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#26679;&#26412;&#23398;&#20064;&#21644;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;&#30456;&#23545;&#20110;&#22522;&#20110;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#20256;&#32479;&#26041;&#27861;&#65292;&#22522;&#20110;&#23556;&#39057;&#20256;&#25773;&#30340;&#26041;&#27861;&#30452;&#35266;&#19988;&#33021;&#22815;&#36866;&#24212;&#31616;&#21333;&#30340;&#22330;&#26223;&#65292;&#20294;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#38590;&#20197;&#23548;&#33322;&#12290;&#32780;&#22522;&#20110;&#31471;&#21040;&#31471;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#20808;&#36827;&#30340;&#35745;&#31639;&#26426;&#25216;&#26415;&#26469;&#25506;&#32034;&#25972;&#20010;&#29366;&#24577;&#31354;&#38388;&#65292;&#22312;&#38754;&#23545;&#22797;&#26434;&#30340;&#26080;&#32447;&#29615;&#22659;&#26102;&#34920;&#29616;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#32780;&#19988;&#24471;&#21040;&#30340;&#31574;&#30053;&#22312;&#26410;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#22312;&#35757;&#32451;&#38454;&#27573;&#26410;&#35265;&#36807;&#30340;&#26032;&#22330;&#26223;&#20013;&#26377;&#25928;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing focus on indoor robot navigation utilizing wireless signals has stemmed from the capability of these signals to capture high-resolution angular and temporal measurements. Prior heuristic-based methods, based on radio frequency propagation, are intuitive and generalizable across simple scenarios, yet fail to navigate in complex environments. On the other hand, end-to-end (e2e) deep reinforcement learning (RL), powered by advanced computing machinery, can explore the entire state space, delivering surprising performance when facing complex wireless environments. However, the price to pay is the astronomical amount of training samples, and the resulting policy, without fine-tuning (zero-shot), is unable to navigate efficiently in new scenarios unseen in the training phase. To equip the navigation agent with sample-efficient learning and {zero-shot} generalization, this work proposes a novel physics-informed RL (PIRL) where a distance-to-target-based cost (standard in e2e) is a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#21644;&#21327;&#35843;&#28151;&#21512;&#20132;&#36890;&#65292;&#29305;&#21035;&#26159;&#20154;&#39550;&#39542;&#36710;&#36742;&#21644;&#26426;&#22120;&#20154;&#36710;&#36742;&#22312;&#23454;&#38469;&#22797;&#26434;&#20132;&#21449;&#21475;&#30340;&#24212;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;5%&#30340;&#26426;&#22120;&#20154;&#36710;&#36742;&#21487;&#20197;&#26377;&#25928;&#38450;&#27490;&#20132;&#21449;&#21475;&#20869;&#30340;&#25317;&#22581;&#24418;&#25104;&#12290;</title><link>http://arxiv.org/abs/2301.05294</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#20154;&#36710;&#36742;&#22312;&#22797;&#26434;&#21644;&#26080;&#20449;&#21495;&#30340;&#20132;&#21449;&#21475;&#20013;&#23398;&#20064;&#25511;&#21046;&#21644;&#21327;&#35843;&#28151;&#21512;&#20132;&#36890;
&lt;/p&gt;
&lt;p&gt;
Learning to Control and Coordinate Mixed Traffic Through Robot Vehicles at Complex and Unsignalized Intersections. (arXiv:2301.05294v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#21644;&#21327;&#35843;&#28151;&#21512;&#20132;&#36890;&#65292;&#29305;&#21035;&#26159;&#20154;&#39550;&#39542;&#36710;&#36742;&#21644;&#26426;&#22120;&#20154;&#36710;&#36742;&#22312;&#23454;&#38469;&#22797;&#26434;&#20132;&#21449;&#21475;&#30340;&#24212;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;5%&#30340;&#26426;&#22120;&#20154;&#36710;&#36742;&#21487;&#20197;&#26377;&#25928;&#38450;&#27490;&#20132;&#21449;&#21475;&#20869;&#30340;&#25317;&#22581;&#24418;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#21449;&#21475;&#26159;&#29616;&#20195;&#22823;&#37117;&#24066;&#20132;&#36890;&#20013;&#24517;&#19981;&#21487;&#23569;&#30340;&#36947;&#36335;&#22522;&#30784;&#35774;&#26045;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20132;&#36890;&#20107;&#25925;&#25110;&#32570;&#20047;&#20132;&#36890;&#21327;&#35843;&#26426;&#21046;&#65288;&#22914;&#20132;&#36890;&#20449;&#21495;&#28783;&#65289;&#65292;&#23427;&#20204;&#20063;&#21487;&#33021;&#25104;&#20026;&#20132;&#36890;&#27969;&#30340;&#29942;&#39048;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#36229;&#36234;&#20256;&#32479;&#25511;&#21046;&#26041;&#27861;&#30340;&#25511;&#21046;&#21644;&#21327;&#35843;&#26426;&#21046;&#65292;&#20197;&#25552;&#39640;&#20132;&#21449;&#21475;&#20132;&#36890;&#30340;&#25928;&#29575;&#12290;&#22312;&#36825;&#20123;&#26041;&#27861;&#20013;&#65292;&#25511;&#21046;&#21487;&#39044;&#35265;&#30340;&#21253;&#21547;&#20154;&#39550;&#39542;&#36710;&#36742;&#65288;HVs&#65289;&#21644;&#26426;&#22120;&#20154;&#36710;&#36742;&#65288;RVs&#65289;&#30340;&#28151;&#21512;&#20132;&#36890;&#24050;&#32463;&#20986;&#29616;&#12290;&#22312;&#26412;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#38469;&#22797;&#26434;&#20132;&#21449;&#21475;&#30340;&#28151;&#21512;&#20132;&#36890;&#30340;&#25511;&#21046;&#21644;&#21327;&#35843;&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#21069;&#26410;&#34987;&#25506;&#32034;&#36807;&#30340;&#20027;&#39064;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#23454;&#38469;&#20132;&#36890;&#26465;&#20214;&#19979;&#65292;&#20351;&#29992;5%&#30340;RVs&#65292;&#25105;&#20204;&#21487;&#20197;&#38450;&#27490;&#22797;&#26434;&#20132;&#21449;&#21475;&#20869;&#30340;&#25317;&#22581;&#24418;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intersections are essential road infrastructures for traffic in modern metropolises. However, they can also be the bottleneck of traffic flows as a result of traffic incidents or the absence of traffic coordination mechanisms such as traffic lights. Recently, various control and coordination mechanisms that are beyond traditional control methods have been proposed to improve the efficiency of intersection traffic. Amongst these methods, the control of foreseeable mixed traffic that consists of human-driven vehicles (HVs) and robot vehicles (RVs) has emerged. In this project, we propose a decentralized multi-agent reinforcement learning approach for the control and coordination of mixed traffic at real-world, complex intersections--a topic that has not been previously explored. Comprehensive experiments are conducted to show the effectiveness of our approach. In particular, we show that using 5% RVs, we can prevent congestion formation inside a complex intersection under the actual traf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#20250;&#21512;&#65292;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#23548;&#33322;&#21644;&#25511;&#21046;&#26694;&#26550;&#65292;&#29992;&#20110;&#21487;&#38752;&#12289;&#20934;&#30830;&#21644;&#33258;&#20027;&#22320;&#36973;&#36935;&#24555;&#36895;&#31227;&#21160;&#30340;&#26143;&#38469;&#29289;&#20307;&#12290;&#23427;&#36890;&#36807;&#28857;&#26368;&#23567;&#33539;&#25968;&#36861;&#36394;&#25511;&#21046;&#21644;&#35889;&#24402;&#19968;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24341;&#23548;&#31574;&#30053;&#26469;&#25552;&#20379;&#39640;&#27010;&#29575;&#25351;&#25968;&#19978;&#30028;&#30340;&#39134;&#34892;&#22120;&#20132;&#20184;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2208.04883</link><description>&lt;p&gt;
&#31070;&#32463;&#20250;&#21512;&#65306;&#38754;&#21521;&#26143;&#38469;&#29289;&#20307;&#30340;&#21487;&#38752;&#23548;&#33322;&#21644;&#25511;&#21046;&#30340;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Neural-Rendezvous: Provably Robust Guidance and Control to Encounter Interstellar Objects. (arXiv:2208.04883v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.04883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#20250;&#21512;&#65292;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#23548;&#33322;&#21644;&#25511;&#21046;&#26694;&#26550;&#65292;&#29992;&#20110;&#21487;&#38752;&#12289;&#20934;&#30830;&#21644;&#33258;&#20027;&#22320;&#36973;&#36935;&#24555;&#36895;&#31227;&#21160;&#30340;&#26143;&#38469;&#29289;&#20307;&#12290;&#23427;&#36890;&#36807;&#28857;&#26368;&#23567;&#33539;&#25968;&#36861;&#36394;&#25511;&#21046;&#21644;&#35889;&#24402;&#19968;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24341;&#23548;&#31574;&#30053;&#26469;&#25552;&#20379;&#39640;&#27010;&#29575;&#25351;&#25968;&#19978;&#30028;&#30340;&#39134;&#34892;&#22120;&#20132;&#20184;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26143;&#38469;&#29289;&#20307;&#65288;ISOs&#65289;&#24456;&#21487;&#33021;&#26159;&#19981;&#21487;&#26367;&#20195;&#30340;&#21407;&#22987;&#26448;&#26009;&#65292;&#22312;&#29702;&#35299;&#31995;&#22806;&#34892;&#26143;&#26143;&#31995;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#36816;&#34892;&#36712;&#36947;&#38590;&#20197;&#32422;&#26463;&#65292;&#36890;&#24120;&#20855;&#26377;&#36739;&#39640;&#30340;&#20542;&#35282;&#21644;&#30456;&#23545;&#36895;&#24230;&#65292;&#20351;&#29992;&#20256;&#32479;&#30340;&#20154;&#22312;&#29615;&#36335;&#26041;&#27861;&#25506;&#32034;ISOs&#20855;&#26377;&#30456;&#24403;&#22823;&#30340;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#20250;&#21512;&#30340;&#28145;&#24230;&#23398;&#20064;&#23548;&#33322;&#21644;&#25511;&#21046;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#23454;&#26102;&#20013;&#20197;&#21487;&#38752;&#12289;&#20934;&#30830;&#21644;&#33258;&#20027;&#30340;&#26041;&#24335;&#36973;&#36935;&#24555;&#36895;&#31227;&#21160;&#30340;&#29289;&#20307;&#65292;&#21253;&#25324;ISOs&#12290;&#23427;&#22312;&#22522;&#20110;&#35889;&#24402;&#19968;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24341;&#23548;&#31574;&#30053;&#20043;&#19978;&#20351;&#29992;&#28857;&#26368;&#23567;&#33539;&#25968;&#36861;&#36394;&#25511;&#21046;&#65292;&#20854;&#20013;&#21442;&#25968;&#36890;&#36807;&#30452;&#25509;&#24809;&#32602;MPC&#29366;&#24577;&#36712;&#36857;&#36319;&#36394;&#35823;&#24046;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35843;&#20248;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31070;&#32463;&#20250;&#21512;&#22312;&#39044;&#26399;&#30340;&#39134;&#34892;&#22120;&#20132;&#20184;&#35823;&#24046;&#19978;&#25552;&#20379;&#20102;&#39640;&#27010;&#29575;&#25351;&#25968;&#19978;&#30028;&#65292;&#20854;&#35777;&#26126;&#21033;&#29992;&#20102;&#38543;&#26426;&#36882;&#22686;&#31283;&#23450;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interstellar objects (ISOs) are likely representatives of primitive materials invaluable in understanding exoplanetary star systems. Due to their poorly constrained orbits with generally high inclinations and relative velocities, however, exploring ISOs with conventional human-in-the-loop approaches is significantly challenging. This paper presents Neural-Rendezvous, a deep learning-based guidance and control framework for encountering fast-moving objects, including ISOs, robustly, accurately, and autonomously in real time. It uses pointwise minimum norm tracking control on top of a guidance policy modeled by a spectrally-normalized deep neural network, where its hyperparameters are tuned with a loss function directly penalizing the MPC state trajectory tracking error. We show that Neural-Rendezvous provides a high probability exponential bound on the expected spacecraft delivery error, the proof of which leverages stochastic incremental stability analysis. In particular, it is used to
&lt;/p&gt;</description></item></channel></rss>