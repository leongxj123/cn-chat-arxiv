<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;OUTDOOR&#20197;&#21450;&#19968;&#31181;&#26032;&#30340;&#26426;&#21046;&#65292;&#29992;&#20110;&#22312;&#22797;&#26434;&#30340;&#23460;&#22806;&#29615;&#22659;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35745;&#31639;&#24863;&#30693;&#30340;&#25104;&#21151;&#24230;&#37327;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2309.10103</link><description>&lt;p&gt;
&#23545;&#20110;&#39640;&#25928;&#30340;&#23460;&#22806;&#29289;&#20307;&#23548;&#33322;&#65292;&#20851;&#20110;&#26410;&#35265;&#20043;&#29289;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Reasoning about the Unseen for Efficient Outdoor Object Navigation. (arXiv:2309.10103v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10103
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;OUTDOOR&#20197;&#21450;&#19968;&#31181;&#26032;&#30340;&#26426;&#21046;&#65292;&#29992;&#20110;&#22312;&#22797;&#26434;&#30340;&#23460;&#22806;&#29615;&#22659;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35745;&#31639;&#24863;&#30693;&#30340;&#25104;&#21151;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#24212;&#35813;&#23384;&#22312;&#20110;&#20154;&#31867;&#23384;&#22312;&#30340;&#20219;&#20309;&#22320;&#26041;: &#23460;&#20869;&#12289;&#23460;&#22806;&#65292;&#29978;&#33267;&#26159;&#26410;&#32472;&#21046;&#30340;&#29615;&#22659;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#22312;&#30446;&#26631;&#23548;&#33322;(OGN)&#26041;&#38754;&#30340;&#36827;&#23637;&#20027;&#35201;&#38598;&#20013;&#22312;&#21033;&#29992;&#23460;&#20869;&#31354;&#38388;&#21644;&#35821;&#20041;&#32447;&#32034;&#26469;&#36827;&#34892;&#23460;&#20869;&#23548;&#33322;&#65292;&#32780;&#36825;&#20123;&#32447;&#32034;&#22312;&#23460;&#22806;&#19981;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;&#23613;&#31649;&#36825;&#20123;&#36129;&#29486;&#20026;&#23460;&#20869;&#22330;&#26223;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#65292;&#20294;&#23454;&#38469;&#30340;&#26426;&#22120;&#20154;&#24212;&#29992;&#24448;&#24448;&#25193;&#23637;&#21040;&#23460;&#22806;&#29615;&#22659;&#65292;&#32780;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#23384;&#22312;&#26032;&#30340;&#25361;&#25112;&#12290;&#19982;&#23460;&#20869;&#32467;&#26500;&#21270;&#24067;&#23616;&#19981;&#21516;&#65292;&#23460;&#22806;&#29615;&#22659;&#32570;&#20047;&#28165;&#26224;&#30340;&#31354;&#38388;&#30028;&#23450;&#65292;&#24182;&#19988;&#23384;&#22312;&#22266;&#26377;&#30340;&#35821;&#20041;&#27495;&#20041;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#20154;&#31867;&#21487;&#20197;&#36731;&#26494;&#22320;&#36827;&#34892;&#23548;&#33322;&#65292;&#22240;&#20026;&#25105;&#20204;&#33021;&#22815;&#25512;&#29702;&#26410;&#35265;&#20043;&#29289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;OUTDOOR&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20934;&#30830;&#22320;&#34394;&#26500;&#21487;&#33021;&#30340;&#26410;&#26469;&#65292;&#24182;&#20026;&#25512;&#21160;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35745;&#31639;&#24863;&#30693;&#30340;&#25104;&#21151;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robots should exist anywhere humans do: indoors, outdoors, and even unmapped environments. In contrast, the focus of recent advancements in Object Goal Navigation(OGN) has targeted navigating in indoor environments by leveraging spatial and semantic cues that do not generalize outdoors. While these contributions provide valuable insights into indoor scenarios, the broader spectrum of real-world robotic applications often extends to outdoor settings. As we transition to the vast and complex terrains of outdoor environments, new challenges emerge. Unlike the structured layouts found indoors, outdoor environments lack clear spatial delineations and are riddled with inherent semantic ambiguities. Despite this, humans navigate with ease because we can reason about the unseen. We introduce a new task OUTDOOR, a new mechanism for Large Language Models (LLMs) to accurately hallucinate possible futures, and a new computationally aware success metric for pushing research forward in this more com
&lt;/p&gt;</description></item></channel></rss>