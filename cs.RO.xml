<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#25552;&#20986;&#20102;DeepMIF&#65292;&#36890;&#36807;&#35774;&#35745;&#23398;&#20064;&#31995;&#32479;&#38598;&#25104;&#21333;&#35843;&#24615;&#25439;&#22833;&#65292;&#22312;&#22823;&#35268;&#27169;3D&#22320;&#22270;&#32472;&#21046;&#20013;&#20248;&#21270;&#31070;&#32463;&#21333;&#35843;&#22330;&#65292;&#36991;&#20813;&#20102;LiDAR&#27979;&#37327;&#30340;&#22024;&#26434;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.17550</link><description>&lt;p&gt;
DeepMIF: &#29992;&#20110;&#22823;&#35268;&#27169;LiDAR 3D&#22320;&#22270;&#32472;&#21046;&#30340;&#28145;&#24230;&#21333;&#35843;&#38544;&#24335;&#22330;
&lt;/p&gt;
&lt;p&gt;
DeepMIF: Deep Monotonic Implicit Fields for Large-Scale LiDAR 3D Mapping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17550
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;DeepMIF&#65292;&#36890;&#36807;&#35774;&#35745;&#23398;&#20064;&#31995;&#32479;&#38598;&#25104;&#21333;&#35843;&#24615;&#25439;&#22833;&#65292;&#22312;&#22823;&#35268;&#27169;3D&#22320;&#22270;&#32472;&#21046;&#20013;&#20248;&#21270;&#31070;&#32463;&#21333;&#35843;&#22330;&#65292;&#36991;&#20813;&#20102;LiDAR&#27979;&#37327;&#30340;&#22024;&#26434;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#20351;&#29992;&#29616;&#20195;&#33719;&#21462;&#35774;&#22791;&#22914;LiDAR&#20256;&#24863;&#22120;&#65292;&#22312;&#24863;&#30693;&#30495;&#23454;&#22823;&#35268;&#27169;&#23460;&#22806;3D&#29615;&#22659;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#29983;&#25104;&#31264;&#23494;&#12289;&#23436;&#25972;&#30340;3D&#22330;&#26223;&#26041;&#38754;&#23384;&#22312;&#22266;&#26377;&#38480;&#21046;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#38598;&#25104;&#20102;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#21644;&#21487;&#20248;&#21270;&#29305;&#24449;&#32593;&#26684;&#65292;&#20197;&#36924;&#36817;3D&#22330;&#26223;&#30340;&#34920;&#38754;&#12290;&#28982;&#32780;&#65292;&#31616;&#21333;&#22320;&#27839;&#21407;&#22987;LiDAR&#20809;&#32447;&#25311;&#21512;&#26679;&#26412;&#20250;&#23548;&#33268;&#30001;&#20110;&#31232;&#30095;&#12289;&#20114;&#30456;&#30683;&#30462;&#30340;LiDAR&#27979;&#37327;&#30340;&#29305;&#24615;&#32780;&#20135;&#29983;&#22024;&#26434;&#30340;3D&#32472;&#22270;&#32467;&#26524;&#12290;&#30456;&#21453;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19981;&#20877;&#31934;&#30830;&#25311;&#21512;LiDAR&#25968;&#25454;&#65292;&#32780;&#26159;&#35753;&#32593;&#32476;&#20248;&#21270;&#22312;3D&#31354;&#38388;&#20013;&#23450;&#20041;&#30340;&#38750;&#24230;&#37327;&#21333;&#35843;&#38544;&#24335;&#22330;&#12290;&#20026;&#36866;&#24212;&#25105;&#20204;&#30340;&#22330;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#23398;&#20064;&#31995;&#32479;&#65292;&#38598;&#25104;&#20102;&#19968;&#20010;&#21333;&#35843;&#24615;&#25439;&#22833;&#65292;&#20351;&#24471;&#33021;&#22815;&#20248;&#21270;&#31070;&#32463;&#21333;&#35843;&#22330;&#24182;&#21033;&#29992;&#20102;&#22823;&#35268;&#27169;3D&#22320;&#22270;&#32472;&#21046;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17550v1 Announce Type: cross  Abstract: Recently, significant progress has been achieved in sensing real large-scale outdoor 3D environments, particularly by using modern acquisition equipment such as LiDAR sensors. Unfortunately, they are fundamentally limited in their ability to produce dense, complete 3D scenes. To address this issue, recent learning-based methods integrate neural implicit representations and optimizable feature grids to approximate surfaces of 3D scenes. However, naively fitting samples along raw LiDAR rays leads to noisy 3D mapping results due to the nature of sparse, conflicting LiDAR measurements. Instead, in this work we depart from fitting LiDAR data exactly, instead letting the network optimize a non-metric monotonic implicit field defined in 3D space. To fit our field, we design a learning system integrating a monotonicity loss that enables optimizing neural monotonic fields and leverages recent progress in large-scale 3D mapping. Our algorithm ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#29992;&#20110;&#35757;&#32451;&#22312;&#39640;&#24230;&#38750;&#32447;&#24615;&#29615;&#22659;&#20013;&#36816;&#34892;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#21453;&#39304;&#25511;&#21046;&#22120;&#65292;&#36890;&#36807;&#23545;&#20219;&#21153;&#36827;&#34892;&#24418;&#24335;&#21270;&#34920;&#36848;&#65292;&#23454;&#29616;&#23545;&#29305;&#23450;&#20219;&#21153;&#30446;&#26631;&#30340;&#23450;&#37327;&#28385;&#36275;&#35821;&#20041;&#65292;&#24182;&#21033;&#29992;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21453;&#39304;&#25511;&#21046;&#22120;&#12290;</title><link>https://arxiv.org/abs/2403.15826</link><description>&lt;p&gt;
&#36890;&#36807;Dropout&#23545;&#26102;&#38388;&#20219;&#21153;&#36827;&#34892;&#27604;&#20363;&#23398;&#20064;&#30340;&#31574;&#30053;&#20248;&#21270;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Scaling Learning based Policy Optimization for Temporal Tasks via Dropout
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#29992;&#20110;&#35757;&#32451;&#22312;&#39640;&#24230;&#38750;&#32447;&#24615;&#29615;&#22659;&#20013;&#36816;&#34892;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#21453;&#39304;&#25511;&#21046;&#22120;&#65292;&#36890;&#36807;&#23545;&#20219;&#21153;&#36827;&#34892;&#24418;&#24335;&#21270;&#34920;&#36848;&#65292;&#23454;&#29616;&#23545;&#29305;&#23450;&#20219;&#21153;&#30446;&#26631;&#30340;&#23450;&#37327;&#28385;&#36275;&#35821;&#20041;&#65292;&#24182;&#21033;&#29992;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21453;&#39304;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#22312;&#39640;&#24230;&#38750;&#32447;&#24615;&#29615;&#22659;&#20013;&#36816;&#34892;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#21453;&#39304;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#24076;&#26395;&#32463;&#36807;&#35757;&#32451;&#30340;&#31574;&#30053;&#33021;&#22815;&#30830;&#20445;&#35813;&#26234;&#33021;&#20307;&#28385;&#36275;&#29305;&#23450;&#30340;&#20219;&#21153;&#30446;&#26631;&#65292;&#36825;&#20123;&#30446;&#26631;&#20197;&#31163;&#25955;&#26102;&#38388;&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#65288;DT-STL&#65289;&#34920;&#31034;&#12290;&#36890;&#36807;&#23558;&#20219;&#21153;&#37325;&#26032;&#34920;&#36848;&#20026;&#24418;&#24335;&#21270;&#26694;&#26550;&#65288;&#22914;DT-STL&#65289;&#65292;&#19968;&#20010;&#20248;&#21183;&#26159;&#20801;&#35768;&#23450;&#37327;&#28385;&#36275;&#35821;&#20041;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#32473;&#23450;&#19968;&#20010;&#36712;&#36857;&#21644;&#19968;&#20010;DT-STL&#20844;&#24335;&#65292;&#25105;&#20204;&#21487;&#20197;&#35745;&#31639;&#40065;&#26834;&#24615;&#65292;&#36825;&#21487;&#20197;&#35299;&#37322;&#20026;&#36712;&#36857;&#19982;&#28385;&#36275;&#35813;&#20844;&#24335;&#30340;&#36712;&#36857;&#38598;&#20043;&#38388;&#30340;&#36817;&#20284;&#26377;&#31526;&#21495;&#36317;&#31163;&#12290;&#25105;&#20204;&#21033;&#29992;&#21453;&#39304;&#25511;&#21046;&#22120;&#65292;&#24182;&#20551;&#35774;&#20351;&#29992;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#36825;&#20123;&#21453;&#39304;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#23398;&#20064;&#38382;&#39064;&#19982;&#35757;&#32451;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#31867;&#20284;&#30340;&#22320;&#26041;&#65292;&#20854;&#20013;&#36882;&#24402;&#21333;&#20803;&#30340;&#25968;&#37327;&#19982;&#26234;&#33021;&#20307;&#30340;&#26102;&#38388;&#35270;&#37326;&#25104;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15826v1 Announce Type: cross  Abstract: This paper introduces a model-based approach for training feedback controllers for an autonomous agent operating in a highly nonlinear environment. We desire the trained policy to ensure that the agent satisfies specific task objectives, expressed in discrete-time Signal Temporal Logic (DT-STL). One advantage for reformulation of a task via formal frameworks, like DT-STL, is that it permits quantitative satisfaction semantics. In other words, given a trajectory and a DT-STL formula, we can compute the robustness, which can be interpreted as an approximate signed distance between the trajectory and the set of trajectories satisfying the formula. We utilize feedback controllers, and we assume a feed forward neural network for learning these feedback controllers. We show how this learning problem is similar to training recurrent neural networks (RNNs), where the number of recurrent units is proportional to the temporal horizon of the agen
&lt;/p&gt;</description></item><item><title>Vid2Robot&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#35270;&#39057;&#26465;&#20214;&#21270;&#31574;&#30053;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#34701;&#21512;&#35270;&#39057;&#29305;&#24449;&#21644;&#26426;&#22120;&#20154;&#29366;&#24577;&#65292;&#30452;&#25509;&#29983;&#25104;&#27169;&#20223;&#25152;&#35266;&#23519;&#20219;&#21153;&#30340;&#21160;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.12943</link><description>&lt;p&gt;
Vid2Robot&#65306;&#22522;&#20110;&#35270;&#39057;&#26465;&#20214;&#21270;&#31574;&#30053;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#20132;&#21449;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Vid2Robot: End-to-end Video-conditioned Policy Learning with Cross-Attention Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12943
&lt;/p&gt;
&lt;p&gt;
Vid2Robot&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#35270;&#39057;&#26465;&#20214;&#21270;&#31574;&#30053;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#34701;&#21512;&#35270;&#39057;&#29305;&#24449;&#21644;&#26426;&#22120;&#20154;&#29366;&#24577;&#65292;&#30452;&#25509;&#29983;&#25104;&#27169;&#20223;&#25152;&#35266;&#23519;&#20219;&#21153;&#30340;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#35268;&#27169;&#26426;&#22120;&#20154;&#31995;&#32479;&#36890;&#24120;&#20381;&#36182;&#25991;&#26412;&#25351;&#20196;&#36827;&#34892;&#20219;&#21153;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#25506;&#32034;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65306;&#26426;&#22120;&#20154;&#33021;&#21542;&#30452;&#25509;&#20174;&#35266;&#23519;&#20154;&#31867;&#25512;&#26029;&#20219;&#21153;&#65311;&#36825;&#31181;&#36716;&#21464;&#35201;&#27714;&#26426;&#22120;&#20154;&#33021;&#22815;&#35299;&#30721;&#20154;&#31867;&#24847;&#22270;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#21487;&#22312;&#20854;&#29289;&#29702;&#32422;&#26463;&#21644;&#29615;&#22659;&#20869;&#25191;&#34892;&#30340;&#21160;&#20316;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Vid2Robot&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#26426;&#22120;&#20154;&#30340;&#31471;&#21040;&#31471;&#22522;&#20110;&#35270;&#39057;&#30340;&#23398;&#20064;&#26694;&#26550;&#12290;&#32473;&#23450;&#19968;&#20010;&#25805;&#20316;&#20219;&#21153;&#30340;&#35270;&#39057;&#28436;&#31034;&#21644;&#24403;&#21069;&#30340;&#35270;&#35273;&#35266;&#23519;&#65292;Vid2Robot&#30452;&#25509;&#29983;&#25104;&#26426;&#22120;&#20154;&#21160;&#20316;&#12290;&#36825;&#26159;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#20154;&#31867;&#35270;&#39057;&#21644;&#26426;&#22120;&#20154;&#36712;&#36857;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#32479;&#19968;&#34920;&#31034;&#27169;&#22411;&#23454;&#29616;&#30340;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#34701;&#21512;&#25552;&#31034;&#35270;&#39057;&#29305;&#24449;&#19982;&#26426;&#22120;&#20154;&#30340;&#24403;&#21069;&#29366;&#24577;&#65292;&#24182;&#29983;&#25104;&#27169;&#20223;&#25152;&#35266;&#23519;&#20219;&#21153;&#30340;&#36866;&#24403;&#21160;&#20316;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#31574;&#30053;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36741;&#21161;&#23545;&#27604;&#25439;&#22833;&#65292;&#20197;&#22686;&#24378;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12943v1 Announce Type: cross  Abstract: While large-scale robotic systems typically rely on textual instructions for tasks, this work explores a different approach: can robots infer the task directly from observing humans? This shift necessitates the robot's ability to decode human intent and translate it into executable actions within its physical constraints and environment. We introduce Vid2Robot, a novel end-to-end video-based learning framework for robots. Given a video demonstration of a manipulation task and current visual observations, Vid2Robot directly produces robot actions. This is achieved through a unified representation model trained on a large dataset of human video and robot trajectory. The model leverages cross-attention mechanisms to fuse prompt video features to the robot's current state and generate appropriate actions that mimic the observed task. To further improve policy performance, we propose auxiliary contrastive losses that enhance the alignment b
&lt;/p&gt;</description></item></channel></rss>