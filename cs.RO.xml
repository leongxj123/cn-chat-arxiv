<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#32852;&#37030;&#23398;&#20064;&#22312;&#22810;&#26234;&#33021;&#20307;&#26426;&#22120;&#20154;&#25506;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#21033;&#29992;&#38544;&#24335;&#31070;&#32463;&#26144;&#23556;&#21644;&#22320;&#29699;&#25968;&#25454;&#38598;&#19978;&#30340;&#20803;&#21021;&#22987;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#39046;&#22495;&#22914;&#28779;&#26143;&#22320;&#24418;&#21644;&#20912;&#24029;&#30340;&#24378;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.02289</link><description>&lt;p&gt;
&#34892;&#26143;&#25506;&#27979;&#30340;&#32852;&#37030;&#22810;&#26234;&#33021;&#20307;&#24314;&#22270;
&lt;/p&gt;
&lt;p&gt;
Federated Multi-Agent Mapping for Planetary Exploration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02289
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#22312;&#22810;&#26234;&#33021;&#20307;&#26426;&#22120;&#20154;&#25506;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#21033;&#29992;&#38544;&#24335;&#31070;&#32463;&#26144;&#23556;&#21644;&#22320;&#29699;&#25968;&#25454;&#38598;&#19978;&#30340;&#20803;&#21021;&#22987;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#39046;&#22495;&#22914;&#28779;&#26143;&#22320;&#24418;&#21644;&#20912;&#24029;&#30340;&#24378;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#26426;&#22120;&#20154;&#25506;&#27979;&#20013;&#65292;&#31649;&#29702;&#21644;&#26377;&#25928;&#21033;&#29992;&#21160;&#24577;&#29615;&#22659;&#20135;&#29983;&#30340;&#22823;&#37327;&#24322;&#26500;&#25968;&#25454;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#20998;&#24067;&#24335;&#26144;&#23556;&#26041;&#27861;&#65292;&#23427;&#35299;&#20915;&#20102;&#21327;&#20316;&#23398;&#20064;&#20013;&#21435;&#20013;&#24515;&#21270;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;FL&#20351;&#22810;&#20010;&#26234;&#33021;&#20307;&#20043;&#38388;&#21487;&#20197;&#36827;&#34892;&#32852;&#21512;&#27169;&#22411;&#35757;&#32451;&#65292;&#32780;&#26080;&#38656;&#38598;&#20013;&#21270;&#25110;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#65292;&#20811;&#26381;&#20102;&#24102;&#23485;&#21644;&#23384;&#20648;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#38544;&#24335;&#31070;&#32463;&#26144;&#23556;&#65292;&#23558;&#22320;&#22270;&#34920;&#31034;&#20026;&#30001;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#36830;&#32493;&#20989;&#25968;&#65292;&#20197;&#20415;&#23454;&#29616;&#32039;&#20945;&#21644;&#36866;&#24212;&#24615;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#22312;&#22320;&#29699;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20803;&#21021;&#22987;&#21270;&#26469;&#22686;&#24378;&#36825;&#19968;&#26041;&#27861;&#65292;&#39044;&#35757;&#32451;&#32593;&#32476;&#20197;&#24555;&#36895;&#23398;&#20064;&#26032;&#30340;&#22320;&#22270;&#32467;&#26500;&#12290;&#36825;&#31181;&#32452;&#21512;&#22312;&#35832;&#22914;&#28779;&#26143;&#22320;&#24418;&#21644;&#20912;&#24029;&#31561;&#19981;&#21516;&#39046;&#22495;&#23637;&#29616;&#20102;&#36739;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545;&#36825;&#19968;&#26041;&#27861;&#36827;&#34892;&#20102;&#20005;&#26684;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02289v1 Announce Type: cross  Abstract: In multi-agent robotic exploration, managing and effectively utilizing the vast, heterogeneous data generated from dynamic environments poses a significant challenge. Federated learning (FL) is a promising approach for distributed mapping, addressing the challenges of decentralized data in collaborative learning. FL enables joint model training across multiple agents without requiring the centralization or sharing of raw data, overcoming bandwidth and storage constraints. Our approach leverages implicit neural mapping, representing maps as continuous functions learned by neural networks, for compact and adaptable representations. We further enhance this approach with meta-initialization on Earth datasets, pre-training the network to quickly learn new map structures. This combination demonstrates strong generalization to diverse domains like Martian terrain and glaciers. We rigorously evaluate this approach, demonstrating its effectiven
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#31361;&#20986;&#25506;&#35752;&#20102;&#22312;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25152;&#24102;&#26469;&#30340;&#23433;&#20840;&#24615;&#21644;&#20581;&#22766;&#24615;&#20851;&#38190;&#38382;&#39064;&#65292;&#25351;&#20986;&#36825;&#31181;&#25972;&#21512;&#21487;&#33021;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25915;&#20987;&#24182;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.10340</link><description>&lt;p&gt;
&#35770;&#37096;&#32626;LLMs/VLMs&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#23384;&#22312;&#30340;&#23433;&#20840;&#38382;&#39064;&#65306;&#31361;&#26174;&#39118;&#38505;&#21644;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
On the Safety Concerns of Deploying LLMs/VLMs in Robotics: Highlighting the Risks and Vulnerabilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10340
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#31361;&#20986;&#25506;&#35752;&#20102;&#22312;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25152;&#24102;&#26469;&#30340;&#23433;&#20840;&#24615;&#21644;&#20581;&#22766;&#24615;&#20851;&#38190;&#38382;&#39064;&#65292;&#25351;&#20986;&#36825;&#31181;&#25972;&#21512;&#21487;&#33021;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25915;&#20987;&#24182;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30528;&#37325;&#35752;&#35770;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#25972;&#21512;&#21040;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#25152;&#28041;&#21450;&#30340;&#20581;&#22766;&#24615;&#21644;&#23433;&#20840;&#24615;&#20851;&#38190;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#30528;&#37325;&#20110;&#21033;&#29992;LLMs&#21644;VLMs&#26469;&#25552;&#39640;&#26426;&#22120;&#20154;&#20219;&#21153;&#65288;&#22914;&#25805;&#20316;&#65292;&#23548;&#33322;&#31561;&#65289;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25972;&#21512;&#21487;&#33021;&#20250;&#24341;&#20837;&#26174;&#30528;&#30340;&#28431;&#27934;&#65292;&#21363;&#30001;&#20110;&#35821;&#35328;&#27169;&#22411;&#23545;&#24694;&#24847;&#25915;&#20987;&#30340;&#25935;&#24863;&#24615;&#65292;&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#36890;&#36807;&#30740;&#31350;LLMs/VLMs&#19982;&#26426;&#22120;&#20154;&#30028;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36731;&#26494;&#25805;&#32437;&#25110;&#35823;&#23548;&#26426;&#22120;&#20154;&#30340;&#34892;&#20026;&#65292;&#23548;&#33268;&#23433;&#20840;&#38544;&#24739;&#12290;&#25105;&#20204;&#23450;&#20041;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#21487;&#33021;&#30340;&#24694;&#24847;&#25915;&#20987;&#31034;&#20363;&#65292;&#24182;&#23545;&#38598;&#25104;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#19977;&#20010;&#30693;&#21517;&#26426;&#22120;&#20154;&#26694;&#26550;&#65288;&#21253;&#25324;KnowNo VIMA&#21644;Instruct2Act&#65289;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;&#23427;&#20204;&#23545;&#36825;&#20123;&#25915;&#20987;&#30340;&#25935;&#24863;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10340v1 Announce Type: cross  Abstract: In this paper, we highlight the critical issues of robustness and safety associated with integrating large language models (LLMs) and vision-language models (VLMs) into robotics applications. Recent works have focused on using LLMs and VLMs to improve the performance of robotics tasks, such as manipulation, navigation, etc. However, such integration can introduce significant vulnerabilities, in terms of their susceptibility to adversarial attacks due to the language models, potentially leading to catastrophic consequences. By examining recent works at the interface of LLMs/VLMs and robotics, we show that it is easy to manipulate or misguide the robot's actions, leading to safety hazards. We define and provide examples of several plausible adversarial attacks, and conduct experiments on three prominent robot frameworks integrated with a language model, including KnowNo VIMA, and Instruct2Act, to assess their susceptibility to these atta
&lt;/p&gt;</description></item></channel></rss>