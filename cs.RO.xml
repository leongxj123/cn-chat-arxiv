<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#25552;&#20986;&#20102;GeFF&#65288;&#36890;&#29992;&#29305;&#24449;&#22330;&#65289;&#65292;&#20316;&#20026;&#23548;&#33322;&#21644;&#25805;&#20316;&#30340;&#32479;&#19968;&#34920;&#31034;&#65292;&#21487;&#20197;&#23454;&#26102;&#25191;&#34892;&#65292;&#36890;&#36807;&#23558;&#29983;&#25104;&#30340;&#20016;&#23500;&#22330;&#26223;&#20808;&#39564;&#19982;&#33258;&#28982;&#35821;&#35328;&#23545;&#40784;&#26469;&#25552;&#39640;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.07563</link><description>&lt;p&gt;
&#23398;&#20064;&#31227;&#21160;&#25805;&#20316;&#30340;&#36890;&#29992;&#29305;&#24449;&#22330;
&lt;/p&gt;
&lt;p&gt;
Learning Generalizable Feature Fields for Mobile Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07563
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;GeFF&#65288;&#36890;&#29992;&#29305;&#24449;&#22330;&#65289;&#65292;&#20316;&#20026;&#23548;&#33322;&#21644;&#25805;&#20316;&#30340;&#32479;&#19968;&#34920;&#31034;&#65292;&#21487;&#20197;&#23454;&#26102;&#25191;&#34892;&#65292;&#36890;&#36807;&#23558;&#29983;&#25104;&#30340;&#20016;&#23500;&#22330;&#26223;&#20808;&#39564;&#19982;&#33258;&#28982;&#35821;&#35328;&#23545;&#40784;&#26469;&#25552;&#39640;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#25805;&#20316;&#20013;&#30340;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#26159;&#22914;&#20309;&#20197;&#32479;&#19968;&#30340;&#26041;&#24335;&#34920;&#31034;&#29289;&#20307;&#21644;&#22330;&#26223;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#21487;&#20197;&#21516;&#26102;&#29992;&#20110;&#22312;&#29615;&#22659;&#20013;&#23548;&#33322;&#21644;&#25805;&#20316;&#29289;&#20307;&#12290;&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;GeFF&#65288;&#36890;&#29992;&#29305;&#24449;&#22330;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22330;&#26223;&#32423;&#30340;&#36890;&#29992;&#31070;&#32463;&#29305;&#24449;&#22330;&#65292;&#20316;&#20026;&#23548;&#33322;&#21644;&#25805;&#20316;&#30340;&#32479;&#19968;&#34920;&#31034;&#65292;&#21487;&#20197;&#23454;&#26102;&#25191;&#34892;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#29983;&#25104;&#26032;&#35270;&#22270;&#21512;&#25104;&#35270;&#20026;&#19968;&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#28982;&#21518;&#36890;&#36807;CLIP&#29305;&#24449;&#25552;&#28860;&#23558;&#29983;&#25104;&#30340;&#20016;&#23500;&#22330;&#26223;&#20808;&#39564;&#19982;&#33258;&#28982;&#35821;&#35328;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07563v1 Announce Type: cross  Abstract: An open problem in mobile manipulation is how to represent objects and scenes in a unified manner, so that robots can use it both for navigating in the environment and manipulating objects. The latter requires capturing intricate geometry while understanding fine-grained semantics, whereas the former involves capturing the complexity inherit to an expansive physical scale. In this work, we present GeFF (Generalizable Feature Fields), a scene-level generalizable neural feature field that acts as a unified representation for both navigation and manipulation that performs in real-time. To do so, we treat generative novel view synthesis as a pre-training task, and then align the resulting rich scene priors with natural language via CLIP feature distillation. We demonstrate the effectiveness of this approach by deploying GeFF on a quadrupedal robot equipped with a manipulator. We evaluate GeFF's ability to generalize to open-set objects as 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35302;&#35273;&#28789;&#24039;&#24615;&#23547;&#25214;&#21644;&#25805;&#20316;&#29289;&#20307;&#30340;&#22810;&#25351;&#26426;&#22120;&#20154;&#31995;&#32479;&#12290;&#36890;&#36807;&#20351;&#29992;&#35302;&#35273;&#20256;&#24863;&#22120;&#36827;&#34892;&#29289;&#20307;&#25628;&#32034;&#21644;&#25805;&#20316;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21363;&#20351;&#22312;&#27809;&#26377;&#20381;&#36182;&#20110;&#35270;&#35273;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#26426;&#22120;&#20154;&#20063;&#33021;&#22815;&#20855;&#22791;&#31867;&#20284;&#20154;&#31867;&#30340;&#35302;&#35273;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.12496</link><description>&lt;p&gt;
DexTouch&#65306;&#23398;&#20064;&#20351;&#29992;&#35302;&#35273;&#28789;&#24039;&#24615;&#23547;&#25214;&#21644;&#25805;&#20316;&#29289;&#20307;
&lt;/p&gt;
&lt;p&gt;
DexTouch: Learning to Seek and Manipulate Objects with Tactile Dexterity. (arXiv:2401.12496v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35302;&#35273;&#28789;&#24039;&#24615;&#23547;&#25214;&#21644;&#25805;&#20316;&#29289;&#20307;&#30340;&#22810;&#25351;&#26426;&#22120;&#20154;&#31995;&#32479;&#12290;&#36890;&#36807;&#20351;&#29992;&#35302;&#35273;&#20256;&#24863;&#22120;&#36827;&#34892;&#29289;&#20307;&#25628;&#32034;&#21644;&#25805;&#20316;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21363;&#20351;&#22312;&#27809;&#26377;&#20381;&#36182;&#20110;&#35270;&#35273;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#26426;&#22120;&#20154;&#20063;&#33021;&#22815;&#20855;&#22791;&#31867;&#20284;&#20154;&#31867;&#30340;&#35302;&#35273;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35302;&#35273;&#33021;&#21147;&#23545;&#20110;&#29087;&#32451;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#23427;&#33021;&#22815;&#22312;&#27809;&#26377;&#20381;&#36182;&#20110;&#35270;&#35273;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#25628;&#32034;&#21644;&#25805;&#20316;&#29289;&#20307;&#12290;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#23558;&#20154;&#31867;&#30340;&#35302;&#35273;&#33021;&#21147;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#25351;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#26088;&#22312;&#21033;&#29992;&#35302;&#35273;&#24863;&#21463;&#22120;&#25628;&#32034;&#21644;&#25805;&#20316;&#29289;&#20307;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#35270;&#35273;&#20449;&#24687;&#12290;&#20351;&#29992;&#35302;&#35273;&#20256;&#24863;&#22120;&#26469;&#25628;&#32034;&#38543;&#26426;&#25918;&#32622;&#30340;&#30446;&#26631;&#29289;&#20307;&#65292;&#24182;&#36827;&#34892;&#27169;&#25311;&#26085;&#24120;&#20219;&#21153;&#30340;&#29289;&#20307;&#25805;&#20316;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#36171;&#20104;&#26426;&#22120;&#20154;&#31867;&#20284;&#20154;&#31867;&#30340;&#35302;&#35273;&#33021;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#22312;&#26426;&#22120;&#20154;&#25163;&#30340;&#19968;&#20391;&#23454;&#29616;&#20102;&#20108;&#20540;&#35302;&#35273;&#20256;&#24863;&#22120;&#65292;&#20197;&#23613;&#37327;&#20943;&#23569;&#27169;&#25311;&#19982;&#30495;&#23454;&#29615;&#22659;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#22312;&#20223;&#30495;&#20013;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#31574;&#30053;&#65292;&#24182;&#23558;&#35757;&#32451;&#22909;&#30340;&#31574;&#30053;&#36716;&#31227;&#21040;&#30495;&#23454;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#35302;&#35273;&#20256;&#24863;&#22120;&#36827;&#34892;&#29289;&#20307;&#25628;&#32034;&#21644;&#25805;&#20316;&#26159;&#21487;&#34892;&#30340;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#20381;&#36182;&#20110;&#35270;&#35273;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The sense of touch is an essential ability for skillfully performing a variety of tasks, providing the capacity to search and manipulate objects without relying on visual information. Extensive research has been conducted over time to apply these human tactile abilities to robots. In this paper, we introduce a multi-finger robot system designed to search for and manipulate objects using the sense of touch without relying on visual information. Randomly located target objects are searched using tactile sensors, and the objects are manipulated for tasks that mimic daily-life. The objective of the study is to endow robots with human-like tactile capabilities. To achieve this, binary tactile sensors are implemented on one side of the robot hand to minimize the Sim2Real gap. Training the policy through reinforcement learning in simulation and transferring the trained policy to the real environment, we demonstrate that object search and manipulation using tactile sensors is possible even in 
&lt;/p&gt;</description></item></channel></rss>