<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#25552;&#20986;&#20102;&#23433;&#20840;&#38388;&#38548;RRT*&#65288;SI-RRT*&#65289;&#20004;&#32423;&#26041;&#27861;&#65292;&#20302;&#32423;&#37319;&#29992;&#37319;&#26679;&#35268;&#21010;&#22120;&#25214;&#21040;&#21333;&#20010;&#26426;&#22120;&#20154;&#30340;&#26080;&#30896;&#25758;&#36712;&#36857;&#65292;&#39640;&#32423;&#36890;&#36807;&#20248;&#20808;&#35268;&#21010;&#25110;&#22522;&#20110;&#20914;&#31361;&#30340;&#25628;&#32034;&#35299;&#20915;&#26426;&#22120;&#20154;&#38388;&#20914;&#31361;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;SI-RRT*&#33021;&#22815;&#24555;&#36895;&#25214;&#21040;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;</title><link>https://arxiv.org/abs/2404.01752</link><description>&lt;p&gt;
&#23433;&#20840;&#38388;&#38548;RRT*&#29992;&#20110;&#36830;&#32493;&#31354;&#38388;&#20013;&#21487;&#25193;&#23637;&#30340;&#22810;&#26426;&#22120;&#20154;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Safe Interval RRT* for Scalable Multi-Robot Path Planning in Continuous Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01752
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#23433;&#20840;&#38388;&#38548;RRT*&#65288;SI-RRT*&#65289;&#20004;&#32423;&#26041;&#27861;&#65292;&#20302;&#32423;&#37319;&#29992;&#37319;&#26679;&#35268;&#21010;&#22120;&#25214;&#21040;&#21333;&#20010;&#26426;&#22120;&#20154;&#30340;&#26080;&#30896;&#25758;&#36712;&#36857;&#65292;&#39640;&#32423;&#36890;&#36807;&#20248;&#20808;&#35268;&#21010;&#25110;&#22522;&#20110;&#20914;&#31361;&#30340;&#25628;&#32034;&#35299;&#20915;&#26426;&#22120;&#20154;&#38388;&#20914;&#31361;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;SI-RRT*&#33021;&#22815;&#24555;&#36895;&#25214;&#21040;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#35299;&#20915;&#22810;&#26426;&#22120;&#20154;&#36335;&#24452;&#35268;&#21010;&#65288;MRPP&#65289;&#38382;&#39064;&#20197;&#25214;&#21040;&#26080;&#20914;&#31361;&#36335;&#24452;&#30340;&#38382;&#39064;&#12290;&#38382;&#39064;&#30340;&#22256;&#38590;&#20027;&#35201;&#26469;&#33258;&#20004;&#20010;&#22240;&#32032;&#12290;&#39318;&#20808;&#65292;&#28041;&#21450;&#22810;&#20010;&#26426;&#22120;&#20154;&#20250;&#23548;&#33268;&#32452;&#21512;&#20915;&#31574;&#65292;&#20351;&#25628;&#32034;&#31354;&#38388;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#20854;&#27425;&#65292;&#36830;&#32493;&#31354;&#38388;&#21576;&#29616;&#20986;&#28508;&#22312;&#26080;&#38480;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#32423;&#26041;&#27861;&#65292;&#20302;&#32423;&#26159;&#22522;&#20110;&#37319;&#26679;&#30340;&#35268;&#21010;&#22120;&#23433;&#20840;&#38388;&#38548;RRT*&#65288;SI-RRT*&#65289;&#65292;&#29992;&#20110;&#25214;&#21040;&#21333;&#20010;&#26426;&#22120;&#20154;&#30340;&#26080;&#30896;&#25758;&#36712;&#36857;&#12290;&#39640;&#32423;&#21487;&#20197;&#20351;&#29992;&#33021;&#22815;&#35299;&#20915;&#26426;&#22120;&#20154;&#38388;&#20914;&#31361;&#30340;&#20219;&#20309;&#26041;&#27861;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20004;&#31181;&#20195;&#34920;&#24615;&#26041;&#27861;&#65292;&#21363;&#20248;&#20808;&#35268;&#21010;&#65288;SI-CPP&#65289;&#21644;&#22522;&#20110;&#20914;&#31361;&#30340;&#25628;&#32034;&#65288;SI-CCBS&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SI-RRT*&#33021;&#22815;&#24555;&#36895;&#25214;&#21040;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#37327;&#36739;&#23569;&#12290;SI-CPP&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#32780;SI-CCBS&#20135;&#29983;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01752v1 Announce Type: cross  Abstract: In this paper, we consider the problem of Multi-Robot Path Planning (MRPP) in continuous space to find conflict-free paths. The difficulty of the problem arises from two primary factors. First, the involvement of multiple robots leads to combinatorial decision-making, which escalates the search space exponentially. Second, the continuous space presents potentially infinite states and actions. For this problem, we propose a two-level approach where the low level is a sampling-based planner Safe Interval RRT* (SI-RRT*) that finds a collision-free trajectory for individual robots. The high level can use any method that can resolve inter-robot conflicts where we employ two representative methods that are Prioritized Planning (SI-CPP) and Conflict Based Search (SI-CCBS). Experimental results show that SI-RRT* can find a high-quality solution quickly with a small number of samples. SI-CPP exhibits improved scalability while SI-CCBS produces 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#19987;&#38376;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#30340;&#31561;&#21464;&#31574;&#30053;&#21644;&#19981;&#21464;&#20540;&#20989;&#25968;&#26500;&#24314;&#26041;&#27861;&#65292;&#22312;&#22522;&#20110;&#22320;&#22270;&#30340;&#36335;&#24452;&#35268;&#21010;&#20013;&#23637;&#31034;&#20102;&#31561;&#21464;&#38598;&#21512;&#21644;&#27491;&#21017;&#21270;&#22914;&#20309;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.12856</link><description>&lt;p&gt;
&#22522;&#20110;&#22320;&#22270;&#30340;&#36335;&#24452;&#35268;&#21010;&#20013;&#30340;&#31561;&#21464;&#38598;&#21512;&#21644;&#27491;&#21017;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Equivariant Ensembles and Regularization for Reinforcement Learning in Map-based Path Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#19987;&#38376;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#30340;&#31561;&#21464;&#31574;&#30053;&#21644;&#19981;&#21464;&#20540;&#20989;&#25968;&#26500;&#24314;&#26041;&#27861;&#65292;&#22312;&#22522;&#20110;&#22320;&#22270;&#30340;&#36335;&#24452;&#35268;&#21010;&#20013;&#23637;&#31034;&#20102;&#31561;&#21464;&#38598;&#21512;&#21644;&#27491;&#21017;&#21270;&#22914;&#20309;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#65292;&#21033;&#29992;&#29615;&#22659;&#30340;&#23545;&#31216;&#24615;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30830;&#20445;&#28145;&#24230;RL&#31574;&#30053;&#21644;&#20540;&#32593;&#32476;&#20998;&#21035;&#26159;&#31561;&#21464;&#21644;&#19981;&#21464;&#30340;&#20197;&#21033;&#29992;&#36825;&#20123;&#23545;&#31216;&#24615;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#30456;&#20851;&#24037;&#20316;&#23581;&#35797;&#36890;&#36807;&#26500;&#36896;&#20855;&#26377;&#31561;&#21464;&#24615;&#21644;&#19981;&#21464;&#24615;&#30340;&#32593;&#32476;&#26469;&#35774;&#35745;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#21482;&#33021;&#20351;&#29992;&#38750;&#24120;&#21463;&#38480;&#30340;&#32452;&#20214;&#24211;&#65292;&#36827;&#32780;&#38459;&#30861;&#20102;&#32593;&#32476;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#24314;&#31561;&#21464;&#31574;&#30053;&#21644;&#19981;&#21464;&#20540;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#32780;&#26080;&#38656;&#19987;&#38376;&#30340;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#31561;&#21464;&#38598;&#21512;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#28155;&#21152;&#20102;&#19968;&#20010;&#27491;&#21017;&#21270;&#39033;&#65292;&#29992;&#20110;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22686;&#21152;&#24402;&#32435;&#20559;&#24046;&#12290;&#22312;&#22522;&#20110;&#22320;&#22270;&#30340;&#36335;&#24452;&#35268;&#21010;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31561;&#21464;&#38598;&#21512;&#21644;&#27491;&#21017;&#21270;&#22914;&#20309;&#26377;&#30410;&#20110;&#26679;&#26412;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12856v1 Announce Type: new  Abstract: In reinforcement learning (RL), exploiting environmental symmetries can significantly enhance efficiency, robustness, and performance. However, ensuring that the deep RL policy and value networks are respectively equivariant and invariant to exploit these symmetries is a substantial challenge. Related works try to design networks that are equivariant and invariant by construction, limiting them to a very restricted library of components, which in turn hampers the expressiveness of the networks. This paper proposes a method to construct equivariant policies and invariant value functions without specialized neural network components, which we term equivariant ensembles. We further add a regularization term for adding inductive bias during training. In a map-based path planning case study, we show how equivariant ensembles and regularization benefit sample efficiency and performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21019;&#24314;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21452;&#36275;&#26426;&#22120;&#20154;&#21160;&#24577;&#36816;&#21160;&#25511;&#21046;&#22120;&#65292;&#35813;&#25511;&#21046;&#22120;&#21487;&#20197;&#24212;&#29992;&#20110;&#22810;&#31181;&#21160;&#24577;&#21452;&#36275;&#25216;&#33021;&#65292;&#24182;&#19988;&#22312;&#27169;&#25311;&#29615;&#22659;&#21644;&#23454;&#38469;&#29615;&#22659;&#20013;&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.16889</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#22810;&#21151;&#33021;&#12289;&#21160;&#24577;&#21644;&#31283;&#20581;&#30340;&#21452;&#36275;&#36816;&#21160;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning for Versatile, Dynamic, and Robust Bipedal Locomotion Control. (arXiv:2401.16889v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21019;&#24314;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21452;&#36275;&#26426;&#22120;&#20154;&#21160;&#24577;&#36816;&#21160;&#25511;&#21046;&#22120;&#65292;&#35813;&#25511;&#21046;&#22120;&#21487;&#20197;&#24212;&#29992;&#20110;&#22810;&#31181;&#21160;&#24577;&#21452;&#36275;&#25216;&#33021;&#65292;&#24182;&#19988;&#22312;&#27169;&#25311;&#29615;&#22659;&#21644;&#23454;&#38469;&#29615;&#22659;&#20013;&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#20851;&#20110;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21019;&#24314;&#21452;&#36275;&#26426;&#22120;&#20154;&#21160;&#24577;&#36816;&#21160;&#25511;&#21046;&#22120;&#30340;&#32508;&#21512;&#30740;&#31350;&#12290;&#25105;&#20204;&#19981;&#20165;&#20165;&#19987;&#27880;&#20110;&#21333;&#19968;&#30340;&#36816;&#21160;&#25216;&#33021;&#65292;&#32780;&#26159;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25511;&#21046;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#29992;&#20110;&#19968;&#31995;&#21015;&#21160;&#24577;&#21452;&#36275;&#25216;&#33021;&#65292;&#20174;&#21608;&#26399;&#24615;&#34892;&#36208;&#21644;&#22868;&#36305;&#21040;&#38750;&#21608;&#26399;&#24615;&#36339;&#36291;&#21644;&#31449;&#31435;&#12290;&#25105;&#20204;&#22522;&#20110;RL&#30340;&#25511;&#21046;&#22120;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#21382;&#21490;&#26550;&#26500;&#65292;&#21033;&#29992;&#26426;&#22120;&#20154;&#30340;&#38271;&#26399;&#21644;&#30701;&#26399;&#36755;&#20837;/&#36755;&#20986;&#65288;I/O&#65289;&#21382;&#21490;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#31471;&#21040;&#31471;RL&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#36825;&#31181;&#25511;&#21046;&#26550;&#26500;&#22312;&#27169;&#25311;&#29615;&#22659;&#21644;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#22810;&#26679;&#21270;&#25216;&#33021;&#19978;&#22987;&#32456;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;&#35813;&#30740;&#31350;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;&#25152;&#25552;&#20986;&#30340;RL&#31995;&#32479;&#22312;&#24320;&#21457;&#36816;&#21160;&#25511;&#21046;&#22120;&#26041;&#38754;&#24341;&#20837;&#30340;&#36866;&#24212;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#21487;&#20197;&#36866;&#24212;&#26102;&#38388;&#19981;&#21464;&#30340;&#21160;&#21147;&#23398;&#21464;&#21270;&#21644;&#26102;&#38388;&#21464;&#21270;&#30340;&#21464;&#21270;&#65292;&#22914;&#25509;&#35302;&#20107;&#20214;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive study on using deep reinforcement learning (RL) to create dynamic locomotion controllers for bipedal robots. Going beyond focusing on a single locomotion skill, we develop a general control solution that can be used for a range of dynamic bipedal skills, from periodic walking and running to aperiodic jumping and standing. Our RL-based controller incorporates a novel dual-history architecture, utilizing both a long-term and short-term input/output (I/O) history of the robot. This control architecture, when trained through the proposed end-to-end RL approach, consistently outperforms other methods across a diverse range of skills in both simulation and the real world.The study also delves into the adaptivity and robustness introduced by the proposed RL system in developing locomotion controllers. We demonstrate that the proposed architecture can adapt to both time-invariant dynamics shifts and time-variant changes, such as contact events, by effectivel
&lt;/p&gt;</description></item></channel></rss>