<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;LLM&#30340;&#19987;&#27880;&#25903;&#25345;&#20132;&#20114;&#27010;&#24565;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#19981;&#24178;&#25200;&#32676;&#20307;&#30340;&#24773;&#20917;&#19979;&#25903;&#25345;&#21644;&#24110;&#21161;&#20154;&#31867;&#12290;</title><link>https://arxiv.org/abs/2403.12533</link><description>&lt;p&gt;
&#26159;&#21542;&#24110;&#21161;&#65306;&#22522;&#20110;LLM&#30340;&#19987;&#27880;&#25903;&#25345;&#19982;&#20154;&#26426;&#32676;&#20307;&#20114;&#21160;
&lt;/p&gt;
&lt;p&gt;
To Help or Not to Help: LLM-based Attentive Support for Human-Robot Group Interactions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12533
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;LLM&#30340;&#19987;&#27880;&#25903;&#25345;&#20132;&#20114;&#27010;&#24565;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#19981;&#24178;&#25200;&#32676;&#20307;&#30340;&#24773;&#20917;&#19979;&#25903;&#25345;&#21644;&#24110;&#21161;&#20154;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#22914;&#20309;&#22312;&#20154;&#31867;&#32676;&#20307;&#20013;&#25552;&#20379;&#19981;&#24341;&#20154;&#27880;&#30446;&#30340;&#29289;&#29702;&#25903;&#25345;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;Attentive Support&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26426;&#22120;&#20154;&#19982;&#20154;&#31867;&#32676;&#20307;&#36827;&#34892;&#25903;&#25345;&#30340;&#20132;&#20114;&#27010;&#24565;&#12290;&#23427;&#23558;&#22330;&#26223;&#24863;&#30693;&#12289;&#23545;&#35805;&#33719;&#21462;&#12289;&#24773;&#20917;&#29702;&#35299;&#21644;&#34892;&#20026;&#29983;&#25104;&#19982;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#30456;&#32467;&#21512;&#12290;&#38500;&#20102;&#36981;&#24490;&#29992;&#25143;&#30340;&#25351;&#20196;&#65292;Attentive Support&#33021;&#22815;&#20915;&#23450;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#25903;&#25345;&#20154;&#31867;&#65292;&#24182;&#22312;&#19981;&#24178;&#25200;&#32676;&#20307;&#26102;&#20445;&#25345;&#27785;&#40664;&#12290;&#36890;&#36807;&#22810;&#26679;&#21270;&#30340;&#22330;&#26223;&#65292;&#25105;&#20204;&#23637;&#31034;&#21644;&#35780;&#20272;&#20102;&#26426;&#22120;&#20154;&#30340;&#19987;&#27880;&#34892;&#20026;&#65292;&#24403;&#38656;&#35201;&#26102;&#25903;&#25345;&#21644;&#24110;&#21161;&#20154;&#31867;&#65292;&#32780;&#22914;&#26524;&#19981;&#38656;&#35201;&#24110;&#21161;&#65292;&#21017;&#19981;&#20250;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12533v1 Announce Type: cross  Abstract: How can a robot provide unobtrusive physical support within a group of humans? We present Attentive Support, a novel interaction concept for robots to support a group of humans. It combines scene perception, dialogue acquisition, situation understanding, and behavior generation with the common-sense reasoning capabilities of Large Language Models (LLMs). In addition to following user instructions, Attentive Support is capable of deciding when and how to support the humans, and when to remain silent to not disturb the group. With a diverse set of scenarios, we show and evaluate the robot's attentive behavior, which supports and helps the humans when required, while not disturbing if no help is needed.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#20154;&#21160;&#20316;&#32416;&#27491;&#35268;&#21010;&#31995;&#32479;&#65292;&#36890;&#36807;&#22788;&#29702;&#29983;&#25104;&#35745;&#21010;&#20013;&#30340;&#29289;&#29702;&#22522;&#30784;&#12289;&#36923;&#36753;&#21644;&#35821;&#20041;&#38169;&#35823;&#30340;&#20877;&#35268;&#21010;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#12290;&#36890;&#36807;&#20223;&#30495;&#21644;&#23454;&#38469;&#22330;&#26223;&#30340;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#35813;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07263</link><description>&lt;p&gt;
CoPAL:&#20855;&#26377;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#20154;&#21160;&#20316;&#32416;&#27491;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
CoPAL: Corrective Planning of Robot Actions with Large Language Models. (arXiv:2310.07263v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#20154;&#21160;&#20316;&#32416;&#27491;&#35268;&#21010;&#31995;&#32479;&#65292;&#36890;&#36807;&#22788;&#29702;&#29983;&#25104;&#35745;&#21010;&#20013;&#30340;&#29289;&#29702;&#22522;&#30784;&#12289;&#36923;&#36753;&#21644;&#35821;&#20041;&#38169;&#35823;&#30340;&#20877;&#35268;&#21010;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#12290;&#36890;&#36807;&#20223;&#30495;&#21644;&#23454;&#38469;&#22330;&#26223;&#30340;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#35813;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#23436;&#20840;&#33258;&#20027;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#33021;&#22815;&#25509;&#31649;&#20154;&#31867;&#20256;&#32479;&#25191;&#34892;&#30340;&#20219;&#21153;&#65292;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#25552;&#20986;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#19968;&#32972;&#26223;&#19979;&#65292;&#26412;&#30740;&#31350;&#20026;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#26550;&#26500;&#65292;&#21327;&#35843;&#22810;&#20010;&#35748;&#30693;&#23618;&#27425;&#20043;&#38388;&#30340;&#26080;&#32541;&#20132;&#20114;&#65292;&#21253;&#25324;&#25512;&#29702;&#12289;&#35268;&#21010;&#21644;&#21160;&#20316;&#29983;&#25104;&#12290;&#20854;&#26680;&#24515;&#26159;&#19968;&#31181;&#22788;&#29702;&#29983;&#25104;&#30340;&#35745;&#21010;&#20013;&#30340;&#29289;&#29702;&#22522;&#30784;&#12289;&#36923;&#36753;&#21644;&#35821;&#20041;&#38169;&#35823;&#30340;&#26032;&#22411;&#20877;&#35268;&#21010;&#31574;&#30053;&#12290;&#36890;&#36807;&#22312;&#20223;&#30495;&#29615;&#22659;&#21644;&#20004;&#20010;&#22797;&#26434;&#30340;&#23454;&#38469;&#22330;&#26223;&#65288;&#26041;&#22359;&#19990;&#30028;&#12289;&#37202;&#21543;&#21644;&#27604;&#33832;&#21046;&#20316;&#65289;&#20013;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#21453;&#39304;&#26550;&#26500;&#30340;&#26377;&#25928;&#24615;&#65292;&#23588;&#20854;&#26159;&#23545;&#21487;&#25191;&#34892;&#24615;&#12289;&#27491;&#30830;&#24615;&#21644;&#26102;&#38388;&#22797;&#26434;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the pursuit of fully autonomous robotic systems capable of taking over tasks traditionally performed by humans, the complexity of open-world environments poses a considerable challenge. Addressing this imperative, this study contributes to the field of Large Language Models (LLMs) applied to task and motion planning for robots. We propose a system architecture that orchestrates a seamless interplay between multiple cognitive levels, encompassing reasoning, planning, and motion generation. At its core lies a novel replanning strategy that handles physically grounded, logical, and semantic errors in the generated plans. We demonstrate the efficacy of the proposed feedback architecture, particularly its impact on executability, correctness, and time complexity via empirical evaluation in the context of a simulation and two intricate real-world scenarios: blocks world, barman and pizza preparation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36890;&#29992;&#22411;&#31526;&#21495;&#35268;&#21010;&#21160;&#20316;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32473;&#23450;&#23454;&#20307;&#23618;&#27425;&#32467;&#26500;&#21644;&#35266;&#23519;&#21040;&#30340;&#30456;&#20284;&#34892;&#20026;&#26469;&#23454;&#29616;&#36890;&#29992;&#21270;&#12290;&#22312;&#27169;&#25311;&#30340;&#21416;&#25151;&#29615;&#22659;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04867</link><description>&lt;p&gt;
&#23398;&#20064;&#36890;&#29992;&#22411;&#31526;&#21495;&#35268;&#21010;&#30340;&#21160;&#20316;
&lt;/p&gt;
&lt;p&gt;
Learning Type-Generalized Actions for Symbolic Planning. (arXiv:2308.04867v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36890;&#29992;&#22411;&#31526;&#21495;&#35268;&#21010;&#21160;&#20316;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32473;&#23450;&#23454;&#20307;&#23618;&#27425;&#32467;&#26500;&#21644;&#35266;&#23519;&#21040;&#30340;&#30456;&#20284;&#34892;&#20026;&#26469;&#23454;&#29616;&#36890;&#29992;&#21270;&#12290;&#22312;&#27169;&#25311;&#30340;&#21416;&#25151;&#29615;&#22659;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#35268;&#21010;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#38656;&#35201;&#38271;&#24207;&#21015;&#21160;&#20316;&#24182;&#35013;&#22791;&#26234;&#33021;&#20307;&#20855;&#26377;&#22797;&#26434;&#34892;&#20026;&#30340;&#22797;&#26434;&#20219;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#32570;&#28857;&#26159;&#38656;&#35201;&#21512;&#36866;&#30340;&#31526;&#21495;&#34920;&#31034;&#26469;&#25551;&#36848;&#29615;&#22659;&#30340;&#29366;&#24577;&#20197;&#21450;&#33021;&#22815;&#25913;&#21464;&#29366;&#24577;&#30340;&#21160;&#20316;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20123;&#34920;&#31034;&#26159;&#30001;&#19987;&#23478;&#20026;&#19981;&#21516;&#30340;&#38382;&#39064;&#22495;&#31934;&#24515;&#35774;&#35745;&#30340;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#38382;&#39064;&#21644;&#29615;&#22659;&#22797;&#26434;&#24615;&#19978;&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#24565;&#65292;&#20351;&#29992;&#32473;&#23450;&#30340;&#23454;&#20307;&#23618;&#27425;&#32467;&#26500;&#21644;&#35266;&#23519;&#21040;&#30340;&#30456;&#20284;&#34892;&#20026;&#26469;&#36890;&#29992;&#21270;&#31526;&#21495;&#21160;&#20316;&#12290;&#22312;&#19968;&#20010;&#27169;&#25311;&#30340;&#22522;&#20110;&#32593;&#26684;&#30340;&#21416;&#25151;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;&#23569;&#37327;&#35266;&#23519;&#20013;&#23398;&#20064;&#21040;&#30340;&#36890;&#29992;&#22411;&#21160;&#20316;&#33021;&#22815;&#27867;&#21270;&#21040;&#26032;&#30340;&#24773;&#20917;&#12290;&#22312;&#35268;&#21010;&#36807;&#31243;&#20013;&#24341;&#20837;&#39069;&#22806;&#30340;&#21363;&#26102;&#36890;&#29992;&#21270;&#26426;&#21046;&#65292;&#21487;&#20197;&#22788;&#29702;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#32452;&#21512;&#12289;&#36739;&#38271;&#24207;&#21015;&#12289;&#26032;&#23454;&#20307;&#21644;&#24847;&#22806;&#29615;&#22659;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic planning is a powerful technique to solve complex tasks that require long sequences of actions and can equip an intelligent agent with complex behavior. The downside of this approach is the necessity for suitable symbolic representations describing the state of the environment as well as the actions that can change it. Traditionally such representations are carefully hand-designed by experts for distinct problem domains, which limits their transferability to different problems and environment complexities. In this paper, we propose a novel concept to generalize symbolic actions using a given entity hierarchy and observed similar behavior. In a simulated grid-based kitchen environment, we show that type-generalized actions can be learned from few observations and generalize to novel situations. Incorporating an additional on-the-fly generalization mechanism during planning, unseen task combinations, involving longer sequences, novel entities and unexpected environment behavior,
&lt;/p&gt;</description></item></channel></rss>