<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29615;&#22659;&#24863;&#30693;&#30340;&#21487;&#20379;&#24615;&#26694;&#26550;&#65292;&#32771;&#34385;&#20102;&#29289;&#20307;&#32423;&#30340;&#21487;&#34892;&#24615;&#20808;&#39564;&#21644;&#29615;&#22659;&#32422;&#26463;&#65292;&#20197;&#35299;&#20915;&#22810;&#20010;&#36974;&#25377;&#30340;&#22797;&#26434;&#24773;&#20917;&#19979;&#30340;&#19977;&#32500;&#20851;&#33410;&#29289;&#20307;&#25805;&#20316;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.07510</link><description>&lt;p&gt;
&#23398;&#20064;&#29615;&#22659;&#24863;&#30693;&#30340;&#36974;&#25377;&#19979;&#19977;&#32500;&#20851;&#33410;&#29289;&#20307;&#25805;&#20316;&#30340;&#21487;&#20379;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learning Environment-Aware Affordance for 3D Articulated Object Manipulation under Occlusions. (arXiv:2309.07510v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29615;&#22659;&#24863;&#30693;&#30340;&#21487;&#20379;&#24615;&#26694;&#26550;&#65292;&#32771;&#34385;&#20102;&#29289;&#20307;&#32423;&#30340;&#21487;&#34892;&#24615;&#20808;&#39564;&#21644;&#29615;&#22659;&#32422;&#26463;&#65292;&#20197;&#35299;&#20915;&#22810;&#20010;&#36974;&#25377;&#30340;&#22797;&#26434;&#24773;&#20917;&#19979;&#30340;&#19977;&#32500;&#20851;&#33410;&#29289;&#20307;&#25805;&#20316;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26679;&#30340;&#29615;&#22659;&#20013;&#24863;&#30693;&#21644;&#25805;&#20316;&#19977;&#32500;&#20851;&#33410;&#29289;&#20307;&#23545;&#20110;&#23478;&#24237;&#21161;&#29702;&#26426;&#22120;&#20154;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28857;&#32423;&#21487;&#20379;&#24615;&#20026;&#19979;&#28216;&#25805;&#20316;&#20219;&#21153;&#25552;&#20379;&#20102;&#21487;&#34892;&#24615;&#20808;&#39564;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#20010;&#29289;&#20307;&#22330;&#26223;&#20013;&#30340;&#22343;&#36136;&#20195;&#29702;&#65292;&#24573;&#35270;&#20102;&#29615;&#22659;&#21644;&#20195;&#29702;&#24418;&#24577;&#25152;&#26045;&#21152;&#30340;&#29616;&#23454;&#32422;&#26463;&#65292;&#22914;&#36974;&#25377;&#21644;&#29289;&#29702;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29615;&#22659;&#24863;&#30693;&#30340;&#21487;&#20379;&#24615;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#29289;&#20307;&#32423;&#21487;&#34892;&#24615;&#20808;&#39564;&#21644;&#29615;&#22659;&#32422;&#26463;&#12290;&#19982;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#21487;&#20379;&#24615;&#26041;&#27861;&#19981;&#21516;&#65292;&#23398;&#20064;&#29615;&#22659;&#24863;&#30693;&#30340;&#21487;&#20379;&#24615;&#38754;&#20020;&#30528;&#30001;&#21508;&#31181;&#36974;&#25377;&#30340;&#22797;&#26434;&#24615;&#24341;&#36215;&#30340;&#32452;&#21512;&#29190;&#28856;&#25361;&#25112;&#65292;&#36825;&#20123;&#36974;&#25377;&#20197;&#20854;&#25968;&#37327;&#12289;&#20960;&#20309;&#24418;&#29366;&#12289;&#20301;&#32622;&#21644;&#23039;&#21183;&#26469;&#21051;&#30011;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#24335;&#21487;&#20379;&#24615;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#21547;&#26377;&#36974;&#25377;&#30340;&#22330;&#26223;&#20013;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Perceiving and manipulating 3D articulated objects in diverse environments is essential for home-assistant robots. Recent studies have shown that point-level affordance provides actionable priors for downstream manipulation tasks. However, existing works primarily focus on single-object scenarios with homogeneous agents, overlooking the realistic constraints imposed by the environment and the agent's morphology, e.g., occlusions and physical limitations. In this paper, we propose an environment-aware affordance framework that incorporates both object-level actionable priors and environment constraints. Unlike object-centric affordance approaches, learning environment-aware affordance faces the challenge of combinatorial explosion due to the complexity of various occlusions, characterized by their quantities, geometries, positions and poses. To address this and enhance data efficiency, we introduce a novel contrastive affordance learning framework capable of training on scenes containin
&lt;/p&gt;</description></item></channel></rss>