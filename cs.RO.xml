<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#24494;&#20998;&#20223;&#30495;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22797;&#26434;&#30340;&#20840;&#36523;&#20223;&#30495;&#35299;&#32806;&#20026;&#20004;&#20010;&#21333;&#29420;&#30340;&#36830;&#32493;&#22495;&#65292;&#24182;&#19982;&#26356;&#31934;&#30830;&#30340;&#27169;&#22411;&#23545;&#40784;&#65292;&#26469;&#20811;&#26381;&#22235;&#36275;&#21160;&#20316;&#20013;&#30340;&#19981;&#36830;&#32493;&#24615;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.14864</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#24494;&#20998;&#20223;&#30495;&#23398;&#20064;&#22235;&#36275;&#21160;&#20316;
&lt;/p&gt;
&lt;p&gt;
Learning Quadruped Locomotion Using Differentiable Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#24494;&#20998;&#20223;&#30495;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22797;&#26434;&#30340;&#20840;&#36523;&#20223;&#30495;&#35299;&#32806;&#20026;&#20004;&#20010;&#21333;&#29420;&#30340;&#36830;&#32493;&#22495;&#65292;&#24182;&#19982;&#26356;&#31934;&#30830;&#30340;&#27169;&#22411;&#23545;&#40784;&#65292;&#26469;&#20811;&#26381;&#22235;&#36275;&#21160;&#20316;&#20013;&#30340;&#19981;&#36830;&#32493;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#37096;&#20998;&#26426;&#22120;&#20154;&#36816;&#21160;&#25511;&#21046;&#30340;&#36827;&#23637;&#37117;&#26159;&#30001;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#21487;&#24494;&#20998;&#20223;&#30495;&#30340;&#28508;&#21147;&#12290;&#21487;&#24494;&#20998;&#20223;&#30495;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#20154;&#27169;&#22411;&#35745;&#31639;&#20302;&#21464;&#24322;&#19968;&#38454;&#26799;&#24230;&#65292;&#25215;&#35834;&#20102;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#65292;&#20294;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#20854;&#22312;&#22235;&#36275;&#26426;&#22120;&#20154;&#25511;&#21046;&#26041;&#38754;&#30340;&#24212;&#29992;&#20173;&#28982;&#26377;&#38480;&#12290;&#21487;&#24494;&#20998;&#20223;&#30495;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#30001;&#20110;&#25509;&#35302;&#20016;&#23500;&#29615;&#22659;&#65288;&#22914;&#22235;&#36275;&#21160;&#20316;&#65289;&#20013;&#30340;&#19981;&#36830;&#32493;&#24615;&#65292;&#23548;&#33268;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#22797;&#26434;&#20248;&#21270;&#26223;&#35266;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21487;&#24494;&#20998;&#20223;&#30495;&#26694;&#26550;&#20197;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;&#20851;&#38190;&#24819;&#27861;&#21253;&#25324;&#23558;&#21487;&#33021;&#30001;&#20110;&#25509;&#35302;&#32780;&#20986;&#29616;&#19981;&#36830;&#32493;&#24615;&#30340;&#22797;&#26434;&#20840;&#36523;&#20223;&#30495;&#35299;&#32806;&#20026;&#20004;&#20010;&#21333;&#29420;&#30340;&#36830;&#32493;&#22495;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23558;&#31616;&#21270;&#27169;&#22411;&#20135;&#29983;&#30340;&#26426;&#22120;&#20154;&#29366;&#24577;&#19982;&#26356;&#31934;&#30830;&#30340;&#19981;&#21487;&#24494;&#20998;&#27169;&#22411;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14864v1 Announce Type: cross  Abstract: While most recent advancements in legged robot control have been driven by model-free reinforcement learning, we explore the potential of differentiable simulation. Differentiable simulation promises faster convergence and more stable training by computing low-variant first-order gradients using the robot model, but so far, its use for legged robot control has remained limited to simulation. The main challenge with differentiable simulation lies in the complex optimization landscape of robotic tasks due to discontinuities in contact-rich environments, e.g., quadruped locomotion. This work proposes a new, differentiable simulation framework to overcome these challenges. The key idea involves decoupling the complex whole-body simulation, which may exhibit discontinuities due to contact, into two separate continuous domains. Subsequently, we align the robot state resulting from the simplified model with a more precise, non-differentiable 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25345;&#32493;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#20998;&#25955;&#30340;&#23398;&#20064;&#26550;&#26500;&#65292;&#26469;&#35299;&#20915;&#20132;&#36890;&#36335;&#21475;&#31359;&#36234;&#21644;&#33258;&#20027;&#36187;&#36710;&#31561;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.10996</link><description>&lt;p&gt;
&#19968;&#20010;&#21487;&#25193;&#23637;&#19988;&#21487;&#24182;&#34892;&#21270;&#30340;&#25968;&#23383;&#23402;&#29983;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#21487;&#25345;&#32493;Sim2Real&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
A Scalable and Parallelizable Digital Twin Framework for Sustainable Sim2Real Transition of Multi-Agent Reinforcement Learning Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10996
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25345;&#32493;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#20998;&#25955;&#30340;&#23398;&#20064;&#26550;&#26500;&#65292;&#26469;&#35299;&#20915;&#20132;&#36890;&#36335;&#21475;&#31359;&#36234;&#21644;&#33258;&#20027;&#36187;&#36710;&#31561;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25345;&#32493;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#36873;&#25321;&#24615;&#22320;&#25353;&#38656;&#25193;&#23637;&#24182;&#34892;&#21270;&#35757;&#32451;&#24037;&#20316;&#36127;&#36733;&#65292;&#24182;&#21033;&#29992;&#26368;&#23569;&#30340;&#30828;&#20214;&#36164;&#28304;&#23558;&#35757;&#32451;&#22909;&#30340;&#31574;&#30053;&#20174;&#27169;&#25311;&#29615;&#22659;&#36716;&#31227;&#21040;&#29616;&#23454;&#19990;&#30028;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;AutoDRIVE&#29983;&#24577;&#31995;&#32479;&#20316;&#20026;&#19968;&#20010;&#21551;&#21160;&#25968;&#23383;&#23402;&#29983;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#12289;&#37096;&#32626;&#21644;&#36716;&#31227;&#21512;&#20316;&#21644;&#31454;&#20105;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#20174;&#27169;&#25311;&#29615;&#22659;&#21040;&#29616;&#23454;&#19990;&#30028;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#25506;&#31350;&#20102;4&#21488;&#21512;&#20316;&#36710;&#36742;(Nigel)&#22312;&#21333;&#26234;&#33021;&#20307;&#21644;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#29615;&#22659;&#20013;&#20849;&#20139;&#26377;&#38480;&#29366;&#24577;&#20449;&#24687;&#30340;&#20132;&#21449;&#36941;&#21382;&#38382;&#39064;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#36890;&#29992;&#31574;&#30053;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20010;&#20307;&#31574;&#30053;&#26041;&#27861;&#30740;&#31350;&#20102;2&#36742;&#36710;(F1TENTH)&#30340;&#23545;&#25239;&#24615;&#33258;&#20027;&#36187;&#36710;&#38382;&#39064;&#12290;&#22312;&#20219;&#20309;&#19968;&#32452;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#26550;&#26500;&#65292;&#36825;&#20801;&#35768;&#23545;&#31574;&#30053;&#36827;&#34892;&#26377;&#21147;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10996v1 Announce Type: cross  Abstract: This work presents a sustainable multi-agent deep reinforcement learning framework capable of selectively scaling parallelized training workloads on-demand, and transferring the trained policies from simulation to reality using minimal hardware resources. We introduce AutoDRIVE Ecosystem as an enabling digital twin framework to train, deploy, and transfer cooperative as well as competitive multi-agent reinforcement learning policies from simulation to reality. Particularly, we first investigate an intersection traversal problem of 4 cooperative vehicles (Nigel) that share limited state information in single as well as multi-agent learning settings using a common policy approach. We then investigate an adversarial autonomous racing problem of 2 vehicles (F1TENTH) using an individual policy approach. In either set of experiments, a decentralized learning architecture was adopted, which allowed robust training and testing of the policies 
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#20223;&#30495;&#21040;&#30495;&#23454;&#19990;&#30028;&#30340;&#36716;&#31227;&#20026;&#35299;&#20915;&#29289;&#20307;&#25805;&#32437;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#21147;&#25903;&#25345;</title><link>https://arxiv.org/abs/2403.02338</link><description>&lt;p&gt;
&#29992;&#21452;&#25163;&#25197;&#24320;&#30422;&#23376;
&lt;/p&gt;
&lt;p&gt;
Twisting Lids Off with Two Hands
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02338
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#20223;&#30495;&#21040;&#30495;&#23454;&#19990;&#30028;&#30340;&#36716;&#31227;&#20026;&#35299;&#20915;&#29289;&#20307;&#25805;&#32437;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#21147;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#20004;&#21482;&#22810;&#25351;&#25163;&#33218;&#25805;&#32437;&#29289;&#20307;&#19968;&#30452;&#26159;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#19968;&#39033;&#38271;&#26399;&#25361;&#25112;&#65292;&#21407;&#22240;&#22312;&#20110;&#35768;&#22810;&#25805;&#32437;&#20219;&#21153;&#30340;&#20016;&#23500;&#25509;&#35302;&#24615;&#36136;&#20197;&#21450;&#21327;&#35843;&#39640;&#32500;&#24230;&#21452;&#25163;&#31995;&#32479;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20351;&#29992;&#20004;&#21482;&#25163;&#25197;&#24320;&#21508;&#31181;&#29942;&#23376;&#30422;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20986;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#20223;&#30495;&#20013;&#35757;&#32451;&#30340;&#31574;&#30053;&#21487;&#20197;&#26377;&#25928;&#22320;&#36716;&#31227;&#21040;&#29616;&#23454;&#19990;&#30028;&#12290;&#36890;&#36807;&#23545;&#29289;&#29702;&#24314;&#27169;&#12289;&#23454;&#26102;&#24863;&#30693;&#21644;&#22870;&#21169;&#35774;&#35745;&#30340;&#26032;&#24037;&#31243;&#35265;&#35299;&#65292;&#35813;&#31574;&#30053;&#23637;&#31034;&#20102;&#19968;&#33324;&#21270;&#33021;&#21147;&#65292;&#33021;&#22815;&#36143;&#31359;&#21508;&#31181;&#30475;&#19981;&#35265;&#30340;&#29289;&#20307;&#65292;&#23637;&#31034;&#20986;&#21160;&#24577;&#21644;&#28789;&#24039;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#35777;&#26126;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#20223;&#30495;&#21040;&#30495;&#23454;&#19990;&#30028;&#30340;&#36716;&#31227;&#20173;&#28982;&#26159;&#35299;&#20915;&#21069;&#25152;&#26410;&#26377;&#22797;&#26434;&#38382;&#39064;&#30340;&#25805;&#32437;&#38382;&#39064;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02338v1 Announce Type: cross  Abstract: Manipulating objects with two multi-fingered hands has been a long-standing challenge in robotics, attributed to the contact-rich nature of many manipulation tasks and the complexity inherent in coordinating a high-dimensional bimanual system. In this work, we consider the problem of twisting lids of various bottle-like objects with two hands, and demonstrate that policies trained in simulation using deep reinforcement learning can be effectively transferred to the real world. With novel engineering insights into physical modeling, real-time perception, and reward design, the policy demonstrates generalization capabilities across a diverse set of unseen objects, showcasing dynamic and dexterous behaviors. Our findings serve as compelling evidence that deep reinforcement learning combined with sim-to-real transfer remains a promising approach for addressing manipulation problems of unprecedented complexity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;S.T.A.R.-Track&#65292;&#19968;&#20010;&#37319;&#29992;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;Transformer&#26694;&#26550;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;3D&#29289;&#20307;&#36319;&#36394;&#12290;&#36890;&#36807;&#26032;&#39062;&#30340;&#28508;&#22312;&#36816;&#21160;&#27169;&#22411;&#21644;&#23398;&#20064;&#22411;&#36319;&#36394;&#23884;&#20837;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20934;&#30830;&#24314;&#27169;&#29289;&#20307;&#30340;&#20960;&#20309;&#36816;&#21160;&#21644;&#21464;&#21270;&#65292;&#24182;&#22312;nuScenes&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.17602</link><description>&lt;p&gt;
S.T.A.R.-Track&#65306;&#33258;&#36866;&#24212;&#26102;&#31354;&#22806;&#35980;&#34920;&#31034;&#30340;&#31471;&#21040;&#31471;3D&#29289;&#20307;&#36319;&#36394;&#30340;&#28508;&#22312;&#36816;&#21160;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
S.T.A.R.-Track: Latent Motion Models for End-to-End 3D Object Tracking with Adaptive Spatio-Temporal Appearance Representations. (arXiv:2306.17602v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;S.T.A.R.-Track&#65292;&#19968;&#20010;&#37319;&#29992;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;Transformer&#26694;&#26550;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;3D&#29289;&#20307;&#36319;&#36394;&#12290;&#36890;&#36807;&#26032;&#39062;&#30340;&#28508;&#22312;&#36816;&#21160;&#27169;&#22411;&#21644;&#23398;&#20064;&#22411;&#36319;&#36394;&#23884;&#20837;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20934;&#30830;&#24314;&#27169;&#29289;&#20307;&#30340;&#20960;&#20309;&#36816;&#21160;&#21644;&#21464;&#21270;&#65292;&#24182;&#22312;nuScenes&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#36319;&#36394;-&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#22522;&#20110;Transformer&#30340;3D&#36319;&#36394;&#26694;&#26550;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#36319;&#36394;&#26041;&#27861;&#36890;&#36807;&#20960;&#20309;&#36816;&#21160;&#27169;&#22411;&#34701;&#21512;&#24103;&#20043;&#38388;&#30340;&#29289;&#20307;&#21644;&#33258;&#36816;&#21160;&#30340;&#20960;&#20309;&#25928;&#24212;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;S.T.A.R.-Track&#65292;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#28508;&#22312;&#36816;&#21160;&#27169;&#22411;&#26469;&#35843;&#25972;&#23545;&#35937;&#26597;&#35810;&#65292;&#20197;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#30452;&#25509;&#32771;&#34385;&#35270;&#35282;&#21644;&#20809;&#29031;&#26465;&#20214;&#30340;&#21464;&#21270;&#65292;&#21516;&#26102;&#26126;&#30830;&#24314;&#27169;&#20960;&#20309;&#36816;&#21160;&#12290;&#32467;&#21512;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#23398;&#20064;&#30340;&#36319;&#36394;&#23884;&#20837;&#65292;&#26377;&#21161;&#20110;&#24314;&#27169;&#36712;&#36857;&#30340;&#23384;&#22312;&#27010;&#29575;&#65292;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#36319;&#36394;&#26694;&#26550;&#65292;&#21487;&#20197;&#19982;&#20219;&#20309;&#22522;&#20110;&#26597;&#35810;&#30340;&#26816;&#27979;&#22120;&#38598;&#25104;&#12290;&#22312;nuScenes&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#23637;&#31034;&#20102;&#22522;&#20110;DETR3D&#30340;&#36319;&#36394;&#22120;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#21516;&#26102;&#22823;&#22823;&#20943;&#23569;&#20102;&#36712;&#36857;&#30340;&#36523;&#20221;&#36716;&#25442;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following the tracking-by-attention paradigm, this paper introduces an object-centric, transformer-based framework for tracking in 3D. Traditional model-based tracking approaches incorporate the geometric effect of object- and ego motion between frames with a geometric motion model. Inspired by this, we propose S.T.A.R.-Track, which uses a novel latent motion model (LMM) to additionally adjust object queries to account for changes in viewing direction and lighting conditions directly in the latent space, while still modeling the geometric motion explicitly. Combined with a novel learnable track embedding that aids in modeling the existence probability of tracks, this results in a generic tracking framework that can be integrated with any query-based detector. Extensive experiments on the nuScenes benchmark demonstrate the benefits of our approach, showing state-of-the-art performance for DETR3D-based trackers while drastically reducing the number of identity switches of tracks at the s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#37319;&#26679;&#22120;&#30340;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#26041;&#27861;&#65292;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#19979;&#33021;&#22815;&#23454;&#29616;&#38271;&#21608;&#26399;&#21463;&#32422;&#26463;&#30340;&#25805;&#20316;&#35745;&#21010;&#12290;</title><link>http://arxiv.org/abs/2306.13196</link><description>&lt;p&gt;
DiMSam:&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#37096;&#20998;&#21487;&#35266;&#27979;&#20219;&#21153;&#19982;&#21160;&#20316;&#35268;&#21010;&#20013;&#30340;&#37319;&#26679;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
DiMSam: Diffusion Models as Samplers for Task and Motion Planning under Partial Observability. (arXiv:2306.13196v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#37319;&#26679;&#22120;&#30340;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#26041;&#27861;&#65292;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#19979;&#33021;&#22815;&#23454;&#29616;&#38271;&#21608;&#26399;&#21463;&#32422;&#26463;&#30340;&#25805;&#20316;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#65288;TAMP&#65289;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#22320;&#35745;&#21010;&#38271;&#21608;&#26399;&#33258;&#20027;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#23427;&#20204;&#38656;&#35201;&#19968;&#20010;&#35268;&#21010;&#27169;&#22411;&#65292;&#22240;&#27492;&#22312;&#29615;&#22659;&#21644;&#20854;&#21160;&#24577;&#19981;&#23436;&#20840;&#20102;&#35299;&#30340;&#39046;&#22495;&#20013;&#24212;&#29992;&#23427;&#20204;&#21487;&#33021;&#38750;&#24120;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#65292;&#29305;&#21035;&#26159;&#25193;&#25955;&#27169;&#22411;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#23398;&#20064;&#25429;&#33719;&#35268;&#21010;&#27169;&#22411;&#20013;&#38590;&#20197;&#35774;&#35745;&#30340;&#32422;&#26463;&#21644;&#37319;&#26679;&#22120;&#12290;&#36825;&#20123;&#23398;&#20064;&#37319;&#26679;&#22120;&#22312;TAMP&#27714;&#35299;&#22120;&#20013;&#32452;&#21512;&#21644;&#21512;&#24182;&#65292;&#20197;&#32852;&#21512;&#25214;&#21040;&#28385;&#36275;&#35268;&#21010;&#20013;&#32422;&#26463;&#30340;&#34892;&#21160;&#21442;&#25968;&#20540;&#12290;&#20026;&#20102;&#20415;&#20110;&#23545;&#29615;&#22659;&#20013;&#26410;&#30693;&#23545;&#35937;&#36827;&#34892;&#39044;&#27979;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#37319;&#26679;&#22120;&#23450;&#20041;&#20026;&#23398;&#20064;&#30340;&#20302;&#32500;&#28508;&#21464;&#37327;&#23884;&#20837;&#30340;&#21487;&#21464;&#23545;&#35937;&#29366;&#24577;&#12290;&#25105;&#20204;&#22312;&#20851;&#33410;&#24335;&#29289;&#20307;&#25805;&#20316;&#39046;&#22495;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#32463;&#20856;TAMP&#12289;&#29983;&#25104;&#23398;&#20064;&#21644;&#28508;&#22312;&#23884;&#20837;&#30340;&#32452;&#21512;&#22914;&#20309;&#20351;&#24471;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#19979;&#36827;&#34892;&#38271;&#21608;&#26399;&#21463;&#32422;&#26463;&#30340;&#25805;&#20316;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task and Motion Planning (TAMP) approaches are effective at planning long-horizon autonomous robot manipulation. However, because they require a planning model, it can be difficult to apply them to domains where the environment and its dynamics are not fully known. We propose to overcome these limitations by leveraging deep generative modeling, specifically diffusion models, to learn constraints and samplers that capture these difficult-to-engineer aspects of the planning model. These learned samplers are composed and combined within a TAMP solver in order to find action parameter values jointly that satisfy the constraints along a plan. To tractably make predictions for unseen objects in the environment, we define these samplers on low-dimensional learned latent embeddings of changing object state. We evaluate our approach in an articulated object manipulation domain and show how the combination of classical TAMP, generative learning, and latent embeddings enables long-horizon constra
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21333;&#38454;&#27573;&#29305;&#26435;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65288;HIB&#65289;&#65292;&#36890;&#36807;&#25429;&#25417;&#21382;&#21490;&#36712;&#36857;&#30340;&#29305;&#26435;&#30693;&#35782;&#34920;&#31034;&#26469;&#23398;&#20064;&#65292;&#32553;&#23567;&#27169;&#25311;&#21644;&#30495;&#23454;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2305.18464</link><description>&lt;p&gt;
&#29305;&#26435;&#30693;&#35782;&#33976;&#39311;&#29992;&#20110; Sim-to-Real &#31574;&#30053;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Privileged Knowledge Distillation for Sim-to-Real Policy Generalization. (arXiv:2305.18464v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18464
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21333;&#38454;&#27573;&#29305;&#26435;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65288;HIB&#65289;&#65292;&#36890;&#36807;&#25429;&#25417;&#21382;&#21490;&#36712;&#36857;&#30340;&#29305;&#26435;&#30693;&#35782;&#34920;&#31034;&#26469;&#23398;&#20064;&#65292;&#32553;&#23567;&#27169;&#25311;&#21644;&#30495;&#23454;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26368;&#36817;&#22312;&#26426;&#22120;&#20154;&#25511;&#21046;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#22810;&#25968;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#36816;&#34892;&#65292;&#37027;&#37324;&#30340;&#29305;&#26435;&#30693;&#35782;&#65288;&#20363;&#22914;&#21160;&#21147;&#23398;&#65292;&#29615;&#22659;&#65292;&#22320;&#24418;&#65289;&#26159;&#36731;&#26494;&#33719;&#21462;&#30340;&#12290;&#30456;&#21453;&#65292;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#65292;&#26426;&#22120;&#20154;&#20195;&#29702;&#36890;&#24120;&#20165;&#20381;&#36182;&#20110;&#26412;&#22320;&#29366;&#24577;&#65288;&#20363;&#22914;&#26426;&#22120;&#20154;&#20851;&#33410;&#30340;&#26412;&#20307;&#24863;&#21453;&#39304;&#65289;&#26469;&#36873;&#25321;&#21160;&#20316;&#65292;&#23548;&#33268;&#26174;&#33879;&#30340;&#27169;&#25311;&#21040;&#30495;&#23454;&#30340;&#24046;&#36317;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#36880;&#28176;&#20943;&#23569;&#23545;&#29305;&#26435;&#30693;&#35782;&#30340;&#20381;&#36182;&#25110;&#25191;&#34892;&#20004;&#38454;&#27573;&#31574;&#30053;&#27169;&#20223;&#26469;&#35299;&#20915;&#36825;&#20010;&#24046;&#36317;&#12290;&#20294;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#20805;&#20998;&#21033;&#29992;&#29305;&#26435;&#30693;&#35782;&#30340;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#23548;&#33268;&#24615;&#33021;&#27425;&#20248;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21382;&#21490;&#20449;&#24687;&#29942;&#39048;&#65288;HIB&#65289;&#30340;&#26032;&#22411;&#21333;&#38454;&#27573;&#29305;&#26435;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#20197;&#32553;&#23567;&#27169;&#25311;&#21040;&#30495;&#23454;&#30340;&#24046;&#36317;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;HIB&#36890;&#36807;&#25429;&#25417;&#21382;&#21490;&#36712;&#36857;&#30340;&#29305;&#26435;&#30693;&#35782;&#34920;&#31034;&#26469;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) has recently achieved remarkable success in robotic control. However, most RL methods operate in simulated environments where privileged knowledge (e.g., dynamics, surroundings, terrains) is readily available. Conversely, in real-world scenarios, robot agents usually rely solely on local states (e.g., proprioceptive feedback of robot joints) to select actions, leading to a significant sim-to-real gap. Existing methods address this gap by either gradually reducing the reliance on privileged knowledge or performing a two-stage policy imitation. However, we argue that these methods are limited in their ability to fully leverage the privileged knowledge, resulting in suboptimal performance. In this paper, we propose a novel single-stage privileged knowledge distillation method called the Historical Information Bottleneck (HIB) to narrow the sim-to-real gap. In particular, HIB learns a privileged knowledge representation from historical trajectories by capturing 
&lt;/p&gt;</description></item></channel></rss>