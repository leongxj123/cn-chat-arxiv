<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#26412;&#32508;&#36848;&#20840;&#38754;&#23457;&#35270;&#20102;&#22522;&#20110;&#20248;&#21270;&#30340;&#20219;&#21153;&#19982;&#36816;&#21160;&#35268;&#21010;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#22914;&#20309;&#36890;&#36807;&#28151;&#21512;&#20248;&#21270;&#26041;&#27861;&#35299;&#20915;&#39640;&#24230;&#22797;&#26434;&#12289;&#25509;&#35302;&#20016;&#23500;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#21644;&#25805;&#20316;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.02817</link><description>&lt;p&gt;
&#20248;&#21270;&#22411;&#20219;&#21153;&#19982;&#36816;&#21160;&#35268;&#21010;&#32508;&#36848;&#65306;&#20174;&#32463;&#20856;&#21040;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Survey of Optimization-based Task and Motion Planning: From Classical To Learning Approaches
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20840;&#38754;&#23457;&#35270;&#20102;&#22522;&#20110;&#20248;&#21270;&#30340;&#20219;&#21153;&#19982;&#36816;&#21160;&#35268;&#21010;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#22914;&#20309;&#36890;&#36807;&#28151;&#21512;&#20248;&#21270;&#26041;&#27861;&#35299;&#20915;&#39640;&#24230;&#22797;&#26434;&#12289;&#25509;&#35302;&#20016;&#23500;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#21644;&#25805;&#20316;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#19982;&#36816;&#21160;&#35268;&#21010;&#65288;TAMP&#65289;&#23558;&#39640;&#23618;&#20219;&#21153;&#35268;&#21010;&#21644;&#20302;&#23618;&#36816;&#21160;&#35268;&#21010;&#32467;&#21512;&#36215;&#26469;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#29702;&#35299;&#20915;&#38271;&#26102;&#22495;&#12289;&#21160;&#24577;&#20219;&#21153;&#12290;&#22522;&#20110;&#20248;&#21270;&#30340;TAMP&#19987;&#27880;&#20110;&#36890;&#36807;&#30446;&#26631;&#20989;&#25968;&#23450;&#20041;&#30446;&#26631;&#26465;&#20214;&#30340;&#28151;&#21512;&#20248;&#21270;&#26041;&#27861;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#24320;&#25918;&#24335;&#30446;&#26631;&#12289;&#26426;&#22120;&#20154;&#21160;&#24577;&#21644;&#26426;&#22120;&#20154;&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#29289;&#29702;&#20132;&#20114;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#20248;&#21270;&#30340;TAMP&#29305;&#21035;&#36866;&#21512;&#35299;&#20915;&#39640;&#24230;&#22797;&#26434;&#12289;&#25509;&#35302;&#20016;&#23500;&#30340;&#36816;&#21160;&#21644;&#25805;&#20316;&#38382;&#39064;&#12290;&#26412;&#32508;&#36848;&#20840;&#38754;&#23457;&#35270;&#20102;&#22522;&#20110;&#20248;&#21270;&#30340;TAMP&#65292;&#28085;&#30422;&#20102;&#65288;i&#65289;&#35268;&#21010;&#39046;&#22495;&#34920;&#31034;&#65292;&#21253;&#25324;&#21160;&#20316;&#25551;&#36848;&#35821;&#35328;&#21644;&#26102;&#24577;&#36923;&#36753;&#65292;&#65288;ii&#65289;TAMP&#21508;&#32452;&#20214;&#30340;&#20010;&#21035;&#35299;&#20915;&#31574;&#30053;&#65292;&#21253;&#25324;&#20154;&#24037;&#26234;&#33021;&#35268;&#21010;&#21644;&#36712;&#36857;&#20248;&#21270;&#65288;TO&#65289;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#22522;&#20110;&#36923;&#36753;&#30340;&#20219;&#21153;&#35268;&#21010;&#19982;&#22522;&#20110;&#27169;&#22411;&#30340;TO&#20043;&#38388;&#30340;&#21160;&#24577;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02817v1 Announce Type: cross  Abstract: Task and Motion Planning (TAMP) integrates high-level task planning and low-level motion planning to equip robots with the autonomy to effectively reason over long-horizon, dynamic tasks. Optimization-based TAMP focuses on hybrid optimization approaches that define goal conditions via objective functions and are capable of handling open-ended goals, robotic dynamics, and physical interaction between the robot and the environment. Therefore, optimization-based TAMP is particularly suited to solve highly complex, contact-rich locomotion and manipulation problems. This survey provides a comprehensive review on optimization-based TAMP, covering (i) planning domain representations, including action description languages and temporal logic, (ii) individual solution strategies for components of TAMP, including AI planning and trajectory optimization (TO), and (iii) the dynamic interplay between logic-based task planning and model-based TO. A 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#21333;&#27493;&#32452;&#35013;&#38169;&#35823;&#26657;&#27491;&#20219;&#21153;&#21644;LEGO&#38169;&#35823;&#26657;&#27491;&#32452;&#35013;&#25968;&#25454;&#38598;&#65288;LEGO-ECA&#65289;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#36825;&#19968;&#20219;&#21153;&#30340;&#33258;&#26657;&#27491;&#32452;&#35013;&#32593;&#32476;&#65288;SCANet&#65289;&#12290;</title><link>https://arxiv.org/abs/2403.18195</link><description>&lt;p&gt;
&#29992;&#33258;&#26657;&#27491;&#32452;&#35013;&#32593;&#32476;&#32416;&#27491;LEGO&#32452;&#35013;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
SCANet: Correcting LEGO Assembly Errors with Self-Correct Assembly Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18195
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#21333;&#27493;&#32452;&#35013;&#38169;&#35823;&#26657;&#27491;&#20219;&#21153;&#21644;LEGO&#38169;&#35823;&#26657;&#27491;&#32452;&#35013;&#25968;&#25454;&#38598;&#65288;LEGO-ECA&#65289;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#36825;&#19968;&#20219;&#21153;&#30340;&#33258;&#26657;&#27491;&#32452;&#35013;&#32593;&#32476;&#65288;SCANet&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#23398;&#21644;3D&#35270;&#35273;&#20013;&#65292;&#33258;&#20027;&#32452;&#35013;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#30830;&#20445;&#32452;&#35013;&#27491;&#30830;&#24615;&#12290;&#20027;&#27969;&#26041;&#27861;&#22914;MEPNet&#30446;&#21069;&#19987;&#27880;&#20110;&#22522;&#20110;&#25163;&#21160;&#25552;&#20379;&#30340;&#22270;&#20687;&#36827;&#34892;&#32452;&#20214;&#32452;&#35013;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#38656;&#35201;&#38271;&#26399;&#35268;&#21010;&#30340;&#20219;&#21153;&#20013;&#24448;&#24448;&#38590;&#20197;&#21462;&#24471;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#22312;&#21516;&#19968;&#26102;&#38388;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25972;&#21512;&#33258;&#26657;&#27491;&#27169;&#22359;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#12290;&#21463;&#27492;&#38382;&#39064;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21333;&#27493;&#32452;&#35013;&#38169;&#35823;&#26657;&#27491;&#20219;&#21153;&#65292;&#20854;&#20013;&#28041;&#21450;&#35782;&#21035;&#21644;&#32416;&#27491;&#32452;&#20214;&#32452;&#35013;&#38169;&#35823;&#12290;&#20026;&#25903;&#25345;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LEGO&#38169;&#35823;&#26657;&#27491;&#32452;&#35013;&#25968;&#25454;&#38598;&#65288;LEGO-ECA&#65289;&#65292;&#21253;&#25324;&#29992;&#20110;&#32452;&#35013;&#27493;&#39588;&#21644;&#32452;&#35013;&#22833;&#36133;&#23454;&#20363;&#30340;&#25163;&#21160;&#22270;&#20687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#26657;&#27491;&#32452;&#35013;&#32593;&#32476;&#65288;SCANet&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#12290;SCANet&#23558;&#32452;&#35013;&#30340;&#37096;&#20214;&#35270;&#20026;&#26597;&#35810;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18195v1 Announce Type: cross  Abstract: Autonomous assembly in robotics and 3D vision presents significant challenges, particularly in ensuring assembly correctness. Presently, predominant methods such as MEPNet focus on assembling components based on manually provided images. However, these approaches often fall short in achieving satisfactory results for tasks requiring long-term planning. Concurrently, we observe that integrating a self-correction module can partially alleviate such issues. Motivated by this concern, we introduce the single-step assembly error correction task, which involves identifying and rectifying misassembled components. To support research in this area, we present the LEGO Error Correction Assembly Dataset (LEGO-ECA), comprising manual images for assembly steps and instances of assembly failures. Additionally, we propose the Self-Correct Assembly Network (SCANet), a novel method to address this task. SCANet treats assembled components as queries, de
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#21644;&#39118;&#38505;&#24847;&#35782;&#30340;TAMP&#31574;&#30053;&#65288;TAMPURA&#65289;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#35299;&#20915;&#20855;&#26377;&#21021;&#22987;&#29366;&#24577;&#21644;&#21160;&#20316;&#32467;&#26524;&#19981;&#30830;&#23450;&#24615;&#30340;&#38271;&#26102;&#31243;&#35268;&#21010;&#38382;&#39064;&#65292;&#21253;&#25324;&#38656;&#35201;&#20449;&#24687;&#25910;&#38598;&#21644;&#36991;&#20813;&#19981;&#33391;&#21644;&#19981;&#21487;&#36870;&#32467;&#26524;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.10454</link><description>&lt;p&gt;
&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#21644;&#39118;&#38505;&#24847;&#35782;&#30340;&#37096;&#20998;&#21487;&#35266;&#27979;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Partially Observable Task and Motion Planning with Uncertainty and Risk Awareness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10454
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#21644;&#39118;&#38505;&#24847;&#35782;&#30340;TAMP&#31574;&#30053;&#65288;TAMPURA&#65289;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#35299;&#20915;&#20855;&#26377;&#21021;&#22987;&#29366;&#24577;&#21644;&#21160;&#20316;&#32467;&#26524;&#19981;&#30830;&#23450;&#24615;&#30340;&#38271;&#26102;&#31243;&#35268;&#21010;&#38382;&#39064;&#65292;&#21253;&#25324;&#38656;&#35201;&#20449;&#24687;&#25910;&#38598;&#21644;&#36991;&#20813;&#19981;&#33391;&#21644;&#19981;&#21487;&#36870;&#32467;&#26524;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#65288;TAMP&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#20215;&#20540;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36890;&#29992;&#30340;&#38271;&#26102;&#31243;&#26426;&#22120;&#20154;&#25805;&#32437;&#21644;&#23548;&#33322;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20856;&#22411;&#30340;TAMP&#38382;&#39064;&#20844;&#24335;&#21270;&#20551;&#35774;&#23436;&#20840;&#21487;&#35266;&#27979;&#21644;&#30830;&#23450;&#24615;&#21160;&#20316;&#25928;&#26524;&#12290;&#36825;&#20123;&#20551;&#35774;&#38480;&#21046;&#20102;&#35268;&#21010;&#32773;&#33719;&#21462;&#20449;&#24687;&#21644;&#20570;&#20986;&#20855;&#26377;&#39118;&#38505;&#24847;&#35782;&#30340;&#20915;&#31574;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#21644;&#39118;&#38505;&#24847;&#35782;&#30340;TAMP&#31574;&#30053;&#65288;TAMPURA&#65289;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#35299;&#20915;&#20855;&#26377;&#21021;&#22987;&#29366;&#24577;&#21644;&#21160;&#20316;&#32467;&#26524;&#19981;&#30830;&#23450;&#24615;&#30340;&#38271;&#26102;&#31243;&#35268;&#21010;&#38382;&#39064;&#65292;&#21253;&#25324;&#38656;&#35201;&#20449;&#24687;&#25910;&#38598;&#21644;&#36991;&#20813;&#19981;&#33391;&#21644;&#19981;&#21487;&#36870;&#32467;&#26524;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#35268;&#21010;&#32773;&#22312;&#25277;&#35937;&#20219;&#21153;&#32423;&#21035;&#21644;&#36830;&#32493;&#25511;&#21046;&#22120;&#32423;&#21035;&#22343;&#22312;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#26465;&#20214;&#19979;&#36827;&#34892;&#25512;&#29702;&#12290;&#37492;&#20110;&#19968;&#32452;&#22312;&#22522;&#26412;&#21160;&#20316;&#31354;&#38388;&#20013;&#36816;&#34892;&#30340;&#38381;&#29615;&#30446;&#26631;&#39537;&#21160;&#25511;&#21046;&#22120;&#65292;&#24182;&#25551;&#36848;&#20102;&#23427;&#20204;&#30340;&#21069;&#25552;&#26465;&#20214;&#21644;&#28508;&#22312;&#33021;&#21147;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10454v1 Announce Type: cross  Abstract: Integrated task and motion planning (TAMP) has proven to be a valuable approach to generalizable long-horizon robotic manipulation and navigation problems. However, the typical TAMP problem formulation assumes full observability and deterministic action effects. These assumptions limit the ability of the planner to gather information and make decisions that are risk-aware. We propose a strategy for TAMP with Uncertainty and Risk Awareness (TAMPURA) that is capable of efficiently solving long-horizon planning problems with initial-state and action outcome uncertainty, including problems that require information gathering and avoiding undesirable and irreversible outcomes. Our planner reasons under uncertainty at both the abstract task level and continuous controller level. Given a set of closed-loop goal-conditioned controllers operating in the primitive action space and a description of their preconditions and potential capabilities, w
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MASK&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#36890;&#36807;&#38750;&#35328;&#35821;&#20114;&#21160;&#19982;&#35266;&#20247;&#36827;&#34892;&#20114;&#21160;&#65292;&#24182;&#21033;&#29992;&#26377;&#38480;&#29366;&#24577;&#26426;&#32467;&#26500;&#35843;&#25972;&#26426;&#22120;&#20154;&#34892;&#20026;&#65292;&#23454;&#29616;&#22810;&#31181;&#19981;&#21516;&#35282;&#33394;&#30340;&#21160;&#24577;&#34920;&#36798;&#12290;</title><link>https://arxiv.org/abs/2403.10041</link><description>&lt;p&gt;
&#22312;&#20132;&#20114;&#24335;&#26426;&#22120;&#20154;&#20013;&#23884;&#20837;&#21160;&#24577;&#35282;&#33394;: &#20266;&#35013;&#21160;&#30011;&#31038;&#20132;&#36816;&#21160;&#23398;&#65288;MASK&#65289;
&lt;/p&gt;
&lt;p&gt;
Towards Embedding Dynamic Personas in Interactive Robots: Masquerading Animated Social Kinematics (MASK)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10041
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MASK&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#36890;&#36807;&#38750;&#35328;&#35821;&#20114;&#21160;&#19982;&#35266;&#20247;&#36827;&#34892;&#20114;&#21160;&#65292;&#24182;&#21033;&#29992;&#26377;&#38480;&#29366;&#24577;&#26426;&#32467;&#26500;&#35843;&#25972;&#26426;&#22120;&#20154;&#34892;&#20026;&#65292;&#23454;&#29616;&#22810;&#31181;&#19981;&#21516;&#35282;&#33394;&#30340;&#21160;&#24577;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20132;&#20114;&#24335;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#35774;&#35745;&#21644;&#24320;&#21457;&#65292;&#20197;&#22686;&#24378;&#35266;&#20247;&#21442;&#19982;&#24230;&#65292;&#20351;&#29992;&#31867;&#20284;&#35282;&#33394;&#30340;&#20154;&#29289;&#24418;&#35937;&#12290;&#22522;&#20110;&#20197;&#35282;&#33394;&#20026;&#39537;&#21160;&#30340;&#23545;&#35805;&#20195;&#29702;&#31995;&#32479;&#65292;&#26412;&#30740;&#31350;&#23558;&#35813;&#20195;&#29702;&#24212;&#29992;&#25193;&#23637;&#21040;&#20102;&#29289;&#29702;&#39046;&#22495;&#65292;&#21033;&#29992;&#26426;&#22120;&#20154;&#25552;&#20379;&#26356;&#20855;&#27785;&#28024;&#24863;&#21644;&#20114;&#21160;&#20307;&#39564;&#12290;&#25552;&#20986;&#30340;&#31995;&#32479;&#21517;&#20026;Masquerading Animated Social Kinematics (MASK)&#65292;&#21033;&#29992;&#31867;&#20154;&#26426;&#22120;&#20154;&#36890;&#36807;&#38750;&#35328;&#35821;&#20114;&#21160;&#19982;&#23458;&#20154;&#20114;&#21160;&#65292;&#21253;&#25324;&#38754;&#37096;&#34920;&#24773;&#21644;&#25163;&#21183;&#12290;&#19968;&#31181;&#22522;&#20110;&#26377;&#38480;&#29366;&#24577;&#26426;&#32467;&#26500;&#30340;&#34892;&#20026;&#29983;&#25104;&#31995;&#32479;&#26377;&#25928;&#22320;&#35843;&#25972;&#26426;&#22120;&#20154;&#34892;&#20026;&#20197;&#20256;&#36798;&#19981;&#21516;&#30340;&#20154;&#29289;&#35282;&#33394;&#12290;MASK&#26694;&#26550;&#38598;&#25104;&#20102;&#24863;&#30693;&#24341;&#25806;&#12289;&#34892;&#20026;&#36873;&#25321;&#24341;&#25806;&#21644;&#32508;&#21512;&#21160;&#20316;&#24211;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#34892;&#20026;&#35774;&#35745;&#20013;&#38656;&#35201;&#26368;&#23569;&#20154;&#24037;&#24178;&#39044;&#22320;&#23454;&#29616;&#23454;&#26102;&#12289;&#21160;&#24577;&#20114;&#21160;&#12290;&#22312;&#29992;&#25143;&#23545;&#35937;&#30740;&#31350;&#36807;&#31243;&#20013;&#65292;&#25506;&#35752;&#20102;&#31995;&#32479;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#28508;&#21147;&#20197;&#28608;&#21457;&#26410;&#26469;&#30740;&#31350;&#30340;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10041v1 Announce Type: cross  Abstract: This paper presents the design and development of an innovative interactive robotic system to enhance audience engagement using character-like personas. Built upon the foundations of persona-driven dialog agents, this work extends the agent application to the physical realm, employing robots to provide a more immersive and interactive experience. The proposed system, named the Masquerading Animated Social Kinematics (MASK), leverages an anthropomorphic robot which interacts with guests using non-verbal interactions, including facial expressions and gestures. A behavior generation system based upon a finite-state machine structure effectively conditions robotic behavior to convey distinct personas. The MASK framework integrates a perception engine, a behavior selection engine, and a comprehensive action library to enable real-time, dynamic interactions with minimal human intervention in behavior design. Throughout the user subject studi
&lt;/p&gt;</description></item><item><title>SELFI&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22312;&#32447;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#19982;&#31163;&#32447;&#22522;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#24555;&#36895;&#25913;&#36827;&#65292;&#24182;&#22312;&#36991;&#25758;&#21644;&#31038;&#20132;&#36981;&#20174;&#34892;&#20026;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.00991</link><description>&lt;p&gt;
SELFI: &#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#33258;&#20027;&#33258;&#25105;&#25913;&#36827;&#20197;&#36827;&#34892;&#31038;&#20132;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
SELFI: Autonomous Self-Improvement with Reinforcement Learning for Social Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00991
&lt;/p&gt;
&lt;p&gt;
SELFI&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22312;&#32447;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#19982;&#31163;&#32447;&#22522;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#24555;&#36895;&#25913;&#36827;&#65292;&#24182;&#22312;&#36991;&#25758;&#21644;&#31038;&#20132;&#36981;&#20174;&#34892;&#20026;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#33258;&#25105;&#25913;&#36827;&#30340;&#26426;&#22120;&#20154;&#36890;&#36807;&#19982;&#29615;&#22659;&#20114;&#21160;&#21644;&#32463;&#39564;&#31215;&#32047;&#26469;&#23454;&#29616;&#23558;&#26159;&#26426;&#22120;&#20154;&#31995;&#32479;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#25237;&#20837;&#20351;&#29992;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;SELFI&#65292;&#21033;&#29992;&#22312;&#32447;&#26426;&#22120;&#20154;&#32463;&#39564;&#26469;&#24555;&#36895;&#39640;&#25928;&#22320;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;SELFI&#23558;&#22312;&#32447;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#31163;&#32447;&#22522;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#20043;&#19978;&#65292;&#20197;&#21457;&#25381;&#36825;&#20004;&#31181;&#23398;&#20064;&#33539;&#24335;&#30340;&#20248;&#28857;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;SELFI&#36890;&#36807;&#23558;&#31163;&#32447;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#23398;&#20064;&#30446;&#26631;&#19982;&#22312;&#32447;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#20013;&#23398;&#20064;&#21040;&#30340;Q&#20540;&#30456;&#32467;&#21512;&#65292;&#31283;&#23450;&#20102;&#22312;&#32447;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#29616;&#23454;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;SELFI&#65292;&#24182;&#25253;&#21578;&#20102;&#22312;&#36991;&#25758;&#26041;&#38754;&#30340;&#25913;&#21892;&#65292;&#20197;&#21450;&#36890;&#36807;&#20154;&#31867;&#29992;&#25143;&#30740;&#31350;&#27979;&#37327;&#30340;&#26356;&#20855;&#31038;&#20132;&#36981;&#20174;&#34892;&#20026;&#12290;SELFI&#20351;&#25105;&#20204;&#33021;&#22815;&#24555;&#36895;&#23398;&#20064;&#26377;&#29992;&#30340;&#26426;&#22120;&#20154;&#34892;&#20026;&#65292;&#20943;&#23569;&#20102;&#39044;&#20808;&#24178;&#39044;&#30340;&#20154;&#21592;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00991v1 Announce Type: cross  Abstract: Autonomous self-improving robots that interact and improve with experience are key to the real-world deployment of robotic systems. In this paper, we propose an online learning method, SELFI, that leverages online robot experience to rapidly fine-tune pre-trained control policies efficiently. SELFI applies online model-free reinforcement learning on top of offline model-based learning to bring out the best parts of both learning paradigms. Specifically, SELFI stabilizes the online learning process by incorporating the same model-based learning objective from offline pre-training into the Q-values learned with online model-free reinforcement learning. We evaluate SELFI in multiple real-world environments and report improvements in terms of collision avoidance, as well as more socially compliant behavior, measured by a human user study. SELFI enables us to quickly learn useful robotic behaviors with less human interventions such as pre-e
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#31163;&#25955;&#25193;&#25955;&#32467;&#21512;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21644;&#23569;&#37327;&#26426;&#22120;&#20154;&#35270;&#39057;&#24494;&#35843;&#65292;&#23454;&#29616;&#20174;&#20154;&#31867;&#35270;&#39057;&#21040;&#26426;&#22120;&#20154;&#31574;&#30053;&#23398;&#20064;&#30340;&#30693;&#35782;&#36801;&#31227;&#12290;</title><link>https://arxiv.org/abs/2402.14407</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#36827;&#34892;&#22823;&#35268;&#27169;&#26080;&#21160;&#20316;&#35270;&#39057;&#39044;&#35757;&#32451;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Large-Scale Actionless Video Pre-Training via Discrete Diffusion for Efficient Policy Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14407
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#31163;&#25955;&#25193;&#25955;&#32467;&#21512;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21644;&#23569;&#37327;&#26426;&#22120;&#20154;&#35270;&#39057;&#24494;&#35843;&#65292;&#23454;&#29616;&#20174;&#20154;&#31867;&#35270;&#39057;&#21040;&#26426;&#22120;&#20154;&#31574;&#30053;&#23398;&#20064;&#30340;&#30693;&#35782;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#23436;&#25104;&#22810;&#20010;&#20219;&#21153;&#30340;&#36890;&#29992;&#23454;&#20307;&#20195;&#29702;&#38754;&#20020;&#25361;&#25112;&#65292;&#20027;&#35201;&#28304;&#33258;&#32570;&#20047;&#26377;&#26631;&#35760;&#21160;&#20316;&#30340;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#23384;&#22312;&#22823;&#37327;&#25429;&#25417;&#22797;&#26434;&#20219;&#21153;&#21644;&#19982;&#29289;&#29702;&#19990;&#30028;&#20114;&#21160;&#30340;&#20154;&#31867;&#35270;&#39057;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#26694;&#26550;&#65292;&#21033;&#29992;&#32479;&#19968;&#30340;&#31163;&#25955;&#25193;&#25955;&#23558;&#20154;&#31867;&#35270;&#39057;&#19978;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#19982;&#23569;&#37327;&#26377;&#26631;&#35760;&#26426;&#22120;&#20154;&#35270;&#39057;&#19978;&#30340;&#31574;&#30053;&#24494;&#35843;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#20154;&#31867;&#21644;&#26426;&#22120;&#20154;&#35270;&#39057;&#21387;&#32553;&#25104;&#32479;&#19968;&#30340;&#35270;&#39057;&#26631;&#35760;&#12290;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#24102;&#26377;&#33945;&#29256;&#26367;&#25442;&#25193;&#25955;&#31574;&#30053;&#30340;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#26469;&#39044;&#27979;&#28508;&#31354;&#38388;&#20013;&#30340;&#26410;&#26469;&#35270;&#39057;&#26631;&#35760;&#12290;&#22312;&#24494;&#35843;&#38454;&#27573;&#65292;&#25105;&#20204; h
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14407v1 Announce Type: new  Abstract: Learning a generalist embodied agent capable of completing multiple tasks poses challenges, primarily stemming from the scarcity of action-labeled robotic datasets. In contrast, a vast amount of human videos exist, capturing intricate tasks and interactions with the physical world. Promising prospects arise for utilizing actionless human videos for pre-training and transferring the knowledge to facilitate robot policy learning through limited robot demonstrations. In this paper, we introduce a novel framework that leverages a unified discrete diffusion to combine generative pre-training on human videos and policy fine-tuning on a small number of action-labeled robot videos. We start by compressing both human and robot videos into unified video tokens. In the pre-training stage, we employ a discrete diffusion model with a mask-and-replace diffusion strategy to predict future video tokens in the latent space. In the fine-tuning stage, we h
&lt;/p&gt;</description></item><item><title>NOD-TAMP&#26159;&#19968;&#20010;&#22522;&#20110;TAMP&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#31070;&#32463;&#29289;&#20307;&#25551;&#36848;&#31526;&#26469;&#35299;&#20915;&#22797;&#26434;&#25805;&#32437;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#20174;&#23569;&#37327;&#20154;&#31867;&#28436;&#31034;&#20013;&#25552;&#21462;&#36712;&#36857;&#24182;&#36827;&#34892;&#35843;&#25972;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#38271;&#26102;&#31243;&#20219;&#21153;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.01530</link><description>&lt;p&gt;
NOD-TAMP:&#22810;&#27493;&#39588;&#25805;&#32437;&#35268;&#21010;&#20013;&#30340;&#31070;&#32463;&#29289;&#20307;&#25551;&#36848;&#31526;
&lt;/p&gt;
&lt;p&gt;
NOD-TAMP: Multi-Step Manipulation Planning with Neural Object Descriptors. (arXiv:2311.01530v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01530
&lt;/p&gt;
&lt;p&gt;
NOD-TAMP&#26159;&#19968;&#20010;&#22522;&#20110;TAMP&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#31070;&#32463;&#29289;&#20307;&#25551;&#36848;&#31526;&#26469;&#35299;&#20915;&#22797;&#26434;&#25805;&#32437;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#20174;&#23569;&#37327;&#20154;&#31867;&#28436;&#31034;&#20013;&#25552;&#21462;&#36712;&#36857;&#24182;&#36827;&#34892;&#35843;&#25972;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#38271;&#26102;&#31243;&#20219;&#21153;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23478;&#23621;&#21644;&#24037;&#21378;&#29615;&#22659;&#20013;&#24320;&#21457;&#22797;&#26434;&#25805;&#32437;&#20219;&#21153;&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#38271;&#26102;&#31243;&#20219;&#21153;&#12289;&#25509;&#35302;&#20016;&#23500;&#30340;&#25805;&#32437;&#20197;&#21450;&#38656;&#35201;&#22312;&#21508;&#31181;&#29289;&#20307;&#24418;&#29366;&#21644;&#22330;&#26223;&#24067;&#23616;&#20043;&#38388;&#36827;&#34892;&#27867;&#21270;&#12290;&#34429;&#28982;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#65288;TAMP&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#26159;&#23427;&#30340;&#20551;&#35774;&#65292;&#22914;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#38480;&#21046;&#20102;&#23427;&#22312;&#26032;&#39062;&#32972;&#26223;&#20013;&#30340;&#36866;&#24212;&#24615;&#12290;&#31070;&#32463;&#29289;&#20307;&#25551;&#36848;&#31526;&#65288;NODs&#65289;&#22312;&#29289;&#20307;&#21644;&#22330;&#26223;&#27867;&#21270;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#22312;&#22788;&#29702;&#26356;&#24191;&#27867;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;TAMP&#30340;&#26694;&#26550;NOD-TAMP&#20174;&#23569;&#25968;&#20154;&#31867;&#28436;&#31034;&#20013;&#25552;&#21462;&#30701;&#30340;&#25805;&#32437;&#36712;&#36857;&#65292;&#20351;&#29992;NOD&#29305;&#24449;&#26469;&#35843;&#25972;&#36825;&#20123;&#36712;&#36857;&#65292;&#24182;&#32452;&#21512;&#23427;&#20204;&#26469;&#35299;&#20915;&#24191;&#27867;&#30340;&#38271;&#26102;&#31243;&#20219;&#21153;&#12290;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#39564;&#35777;&#21518;&#65292;NOD-TAMP&#26377;&#25928;&#24212;&#23545;&#21508;&#31181;&#25361;&#25112;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#25805;&#32437;&#35268;&#21010;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing intelligent robots for complex manipulation tasks in household and factory settings remains challenging due to long-horizon tasks, contact-rich manipulation, and the need to generalize across a wide variety of object shapes and scene layouts. While Task and Motion Planning (TAMP) offers a promising solution, its assumptions such as kinodynamic models limit applicability in novel contexts. Neural object descriptors (NODs) have shown promise in object and scene generalization but face limitations in addressing broader tasks. Our proposed TAMP-based framework, NOD-TAMP, extracts short manipulation trajectories from a handful of human demonstrations, adapts these trajectories using NOD features, and composes them to solve broad long-horizon tasks. Validated in a simulation environment, NOD-TAMP effectively tackles varied challenges and outperforms existing methods, establishing a cohesive framework for manipulation planning. For videos and other supplemental material, see the pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TRADYN&#30340;&#27010;&#29575;&#22320;&#24418;&#21644;&#26426;&#22120;&#20154;&#24863;&#30693;&#21069;&#21521;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#33021;&#22815;&#36866;&#24212;&#22312;&#33258;&#20027;&#23548;&#33322;&#29615;&#22659;&#20013;&#30340;&#22320;&#24418;&#21644;&#26426;&#22120;&#20154;&#30340;&#21464;&#21270;&#65292;&#36890;&#36807;&#22312;&#27169;&#25311;&#30340;&#20108;&#32500;&#23548;&#33322;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#38271;&#35270;&#31243;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2307.09206</link><description>&lt;p&gt;
&#24102;&#26377;&#22522;&#20110;&#23398;&#20064;&#30340;&#22320;&#24418;&#21644;&#26426;&#22120;&#20154;&#24863;&#30693;&#21160;&#21147;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#26465;&#20214;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Context-Conditional Navigation with a Learning-Based Terrain- and Robot-Aware Dynamics Model. (arXiv:2307.09206v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TRADYN&#30340;&#27010;&#29575;&#22320;&#24418;&#21644;&#26426;&#22120;&#20154;&#24863;&#30693;&#21069;&#21521;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#33021;&#22815;&#36866;&#24212;&#22312;&#33258;&#20027;&#23548;&#33322;&#29615;&#22659;&#20013;&#30340;&#22320;&#24418;&#21644;&#26426;&#22120;&#20154;&#30340;&#21464;&#21270;&#65292;&#36890;&#36807;&#22312;&#27169;&#25311;&#30340;&#20108;&#32500;&#23548;&#33322;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#38271;&#35270;&#31243;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#20027;&#23548;&#33322;&#29615;&#22659;&#20013;&#65292;&#22810;&#20010;&#21442;&#25968;&#21487;&#33021;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#22320;&#24418;&#29305;&#24615;&#22914;&#25705;&#25830;&#31995;&#25968;&#21487;&#33021;&#20250;&#26681;&#25454;&#26426;&#22120;&#20154;&#30340;&#20301;&#32622;&#32780;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#26426;&#22120;&#20154;&#30340;&#21160;&#21147;&#23398;&#21487;&#33021;&#20250;&#22240;&#19981;&#21516;&#36127;&#36733;&#12289;&#31995;&#32479;&#36136;&#37327;&#21464;&#21270;&#12289;&#30952;&#25439;&#31561;&#21407;&#22240;&#32780;&#21457;&#29983;&#21464;&#21270;&#65292;&#20174;&#32780;&#25913;&#21464;&#25191;&#34892;&#22120;&#22686;&#30410;&#25110;&#20851;&#33410;&#25705;&#25830;&#21147;&#12290;&#33258;&#20027;&#20195;&#29702;&#24212;&#35813;&#33021;&#22815;&#36866;&#24212;&#36825;&#20123;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#22320;&#24418;&#21644;&#26426;&#22120;&#20154;&#24863;&#30693;&#21069;&#21521;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#31216;&#20026;TRADYN&#65292;&#23427;&#33021;&#22815;&#36866;&#24212;&#19978;&#36848;&#21464;&#21270;&#12290;&#23427;&#22522;&#20110;&#22522;&#20110;&#31070;&#32463;&#36807;&#31243;&#30340;&#20803;&#23398;&#20064;&#21069;&#21521;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#30340;&#20108;&#32500;&#23548;&#33322;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;&#31867;&#20284;&#33258;&#34892;&#36710;&#30340;&#26426;&#22120;&#20154;&#21644;&#20855;&#26377;&#31354;&#38388;&#21464;&#21270;&#25705;&#25830;&#31995;&#25968;&#30340;&#19981;&#21516;&#22320;&#24418;&#24067;&#23616;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#19982;&#38750;&#33258;&#36866;&#24212;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#38271;&#35270;&#31243;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In autonomous navigation settings, several quantities can be subject to variations. Terrain properties such as friction coefficients may vary over time depending on the location of the robot. Also, the dynamics of the robot may change due to, e.g., different payloads, changing the system's mass, or wear and tear, changing actuator gains or joint friction. An autonomous agent should thus be able to adapt to such variations. In this paper, we develop a novel probabilistic, terrain- and robot-aware forward dynamics model, termed TRADYN, which is able to adapt to the above-mentioned variations. It builds on recent advances in meta-learning forward dynamics models based on Neural Processes. We evaluate our method in a simulated 2D navigation setting with a unicycle-like robot and different terrain layouts with spatially varying friction coefficients. In our experiments, the proposed model exhibits lower prediction error for the task of long-horizon trajectory prediction, compared to non-ada
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#30446;&#26631;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#31574;&#30053;&#26799;&#24230;&#35757;&#32451;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#22312;&#21333;&#27425;&#35757;&#32451;&#36816;&#34892;&#20013;&#36817;&#20284;&#33719;&#21462;&#25972;&#20010;&#24085;&#32047;&#25176;&#38598;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#30446;&#26631;&#30340;&#32447;&#24615;&#26631;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2303.08909</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28508;&#22312;&#26465;&#20214;&#31574;&#30053;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Latent-Conditioned Policy Gradient for Multi-Objective Deep Reinforcement Learning. (arXiv:2303.08909v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08909
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#30446;&#26631;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#31574;&#30053;&#26799;&#24230;&#35757;&#32451;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#22312;&#21333;&#27425;&#35757;&#32451;&#36816;&#34892;&#20013;&#36817;&#20284;&#33719;&#21462;&#25972;&#20010;&#24085;&#32047;&#25176;&#38598;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#30446;&#26631;&#30340;&#32447;&#24615;&#26631;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#24207;&#21015;&#20915;&#31574;&#36890;&#24120;&#38656;&#35201;&#25214;&#21040;&#24179;&#34913;&#30456;&#20114;&#30683;&#30462;&#30340;&#30446;&#26631;&#30340;&#33391;&#22909;&#24179;&#34913;&#28857;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#23384;&#22312;&#22823;&#37327;&#30340;&#24085;&#32047;&#25176;&#26368;&#20248;&#31574;&#30053;&#65292;&#23427;&#20204;&#20307;&#29616;&#20102;&#19981;&#21516;&#30340;&#30446;&#26631;&#26435;&#34913;&#27169;&#24335;&#65292;&#24182;&#19988;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20840;&#38754;&#33719;&#24471;&#23427;&#20204;&#20855;&#26377;&#25216;&#26415;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#65288;MORL&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#31574;&#30053;&#26799;&#24230;&#35757;&#32451;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#22312;&#21333;&#27425;&#35757;&#32451;&#36816;&#34892;&#20013;&#36817;&#20284;&#33719;&#21462;&#25972;&#20010;&#24085;&#32047;&#25176;&#38598;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#30446;&#26631;&#30340;&#32447;&#24615;&#26631;&#37327;&#21270;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#36830;&#32493;&#21644;&#31163;&#25955;&#30340;&#34892;&#21160;&#31354;&#38388;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20462;&#25913;&#31574;&#30053;&#32593;&#32476;&#30340;&#35774;&#35745;&#12290;&#22312;&#22522;&#20934;&#29615;&#22659;&#20013;&#30340;&#25968;&#23383;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26631;&#20934;MORL&#22522;&#32447;&#30456;&#27604;&#30340;&#23454;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential decision making in the real world often requires finding a good balance of conflicting objectives. In general, there exist a plethora of Pareto-optimal policies that embody different patterns of compromises between objectives, and it is technically challenging to obtain them exhaustively using deep neural networks. In this work, we propose a novel multi-objective reinforcement learning (MORL) algorithm that trains a single neural network via policy gradient to approximately obtain the entire Pareto set in a single run of training, without relying on linear scalarization of objectives. The proposed method works in both continuous and discrete action spaces with no design change of the policy network. Numerical experiments in benchmark environments demonstrate the practicality and efficacy of our approach in comparison to standard MORL baselines.
&lt;/p&gt;</description></item></channel></rss>