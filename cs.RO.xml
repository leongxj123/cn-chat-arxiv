<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23545;&#25239;&#24615;&#22810;&#26234;&#33021;&#20307;&#28216;&#25103;&#20013;&#24212;&#29992;&#25193;&#25955;-&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#23618;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#39640;&#32423;&#25193;&#25955;&#27169;&#22411;&#21644;&#20302;&#32423;RL&#31639;&#27861;&#65292;&#23454;&#29616;&#27604;&#22522;&#20934;&#26041;&#27861;&#26356;&#39640;&#25928;&#29575;&#30340;&#36816;&#21160;&#35268;&#21010;&#12290;</title><link>https://arxiv.org/abs/2403.10794</link><description>&lt;p&gt;
&#22312;&#23545;&#25239;&#24615;&#22810;&#26234;&#33021;&#20307;&#28216;&#25103;&#20013;&#30340;&#25193;&#25955;-&#24378;&#21270;&#23398;&#20064;&#20998;&#23618;&#36816;&#21160;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Diffusion-Reinforcement Learning Hierarchical Motion Planning in Adversarial Multi-agent Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10794
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23545;&#25239;&#24615;&#22810;&#26234;&#33021;&#20307;&#28216;&#25103;&#20013;&#24212;&#29992;&#25193;&#25955;-&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#23618;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#39640;&#32423;&#25193;&#25955;&#27169;&#22411;&#21644;&#20302;&#32423;RL&#31639;&#27861;&#65292;&#23454;&#29616;&#27604;&#22522;&#20934;&#26041;&#27861;&#26356;&#39640;&#25928;&#29575;&#30340;&#36816;&#21160;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;-&#22522;&#20110;&#30340;&#36816;&#21160;&#35268;&#21010;&#26368;&#36817;&#23637;&#29616;&#20986;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#28508;&#21147;&#65292;&#20174;&#33258;&#20027;&#23548;&#33322;&#21040;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#37096;&#20998;&#21487;&#35266;&#23519;&#22810;&#26234;&#33021;&#20307;&#23545;&#25239;&#36861;&#36880;&#36867;&#36991;&#28216;&#25103;&#65288;PEG&#65289;&#20013;&#23545;&#36867;&#36991;&#30446;&#26631;&#30340;&#36816;&#21160;&#35268;&#21010;&#20219;&#21153;&#12290;&#36825;&#20123;&#36861;&#36880;&#36867;&#36991;&#38382;&#39064;&#19982;&#21508;&#31181;&#24212;&#29992;&#30456;&#20851;&#65292;&#20363;&#22914;&#25628;&#32034;&#21644;&#25937;&#25588;&#34892;&#21160;&#20197;&#21450;&#30417;&#35270;&#26426;&#22120;&#20154;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#24517;&#39035;&#26377;&#25928;&#35268;&#21010;&#20182;&#20204;&#30340;&#34892;&#21160;&#26469;&#25910;&#38598;&#24773;&#25253;&#25110;&#23436;&#25104;&#20219;&#21153;&#65292;&#21516;&#26102;&#36991;&#20813;&#34987;&#20390;&#26597;&#25110;&#34987;&#20440;&#34383;&#33258;&#24049;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#25972;&#21512;&#20102;&#19968;&#20010;&#39640;&#32423;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#35268;&#21010;&#23545;&#29615;&#22659;&#25968;&#25454;&#25935;&#24863;&#30340;&#20840;&#23616;&#36335;&#24452;&#65292;&#21516;&#26102;&#20302;&#32423;RL&#31639;&#27861;&#25512;&#29702;&#38378;&#36991;&#34892;&#20026;&#19982;&#20840;&#23616;&#36335;&#24452;&#36319;&#38543;&#34892;&#20026;&#12290;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#24341;&#23548;RL&#31639;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#22522;&#32447;&#25552;&#39640;&#20102;51.2&#65285;&#65292;&#23454;&#29616;&#26356;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10794v1 Announce Type: cross  Abstract: Reinforcement Learning- (RL-)based motion planning has recently shown the potential to outperform traditional approaches from autonomous navigation to robot manipulation. In this work, we focus on a motion planning task for an evasive target in a partially observable multi-agent adversarial pursuit-evasion games (PEG). These pursuit-evasion problems are relevant to various applications, such as search and rescue operations and surveillance robots, where robots must effectively plan their actions to gather intelligence or accomplish mission tasks while avoiding detection or capture themselves. We propose a hierarchical architecture that integrates a high-level diffusion model to plan global paths responsive to environment data while a low-level RL algorithm reasons about evasive versus global path-following behavior. Our approach outperforms baselines by 51.2% by leveraging the diffusion model to guide the RL algorithm for more efficien
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#20027;&#21160;&#25512;&#29702;&#35745;&#31639;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#32452;&#21512;&#31163;&#25955;&#21644;&#36830;&#32493;&#27169;&#22411;&#20197;&#23454;&#29616;&#28789;&#27963;&#24037;&#20855;&#20351;&#29992;&#65292;&#25511;&#21046;&#21644;&#35268;&#21010;&#12290;&#22312;&#38750;&#24179;&#20961;&#20219;&#21153;&#20013;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10088</link><description>&lt;p&gt;
&#20998;&#23618;&#28151;&#21512;&#24314;&#27169;&#29992;&#20110;&#28789;&#27963;&#24037;&#20855;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hierarchical hybrid modeling for flexible tool use
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#20027;&#21160;&#25512;&#29702;&#35745;&#31639;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#32452;&#21512;&#31163;&#25955;&#21644;&#36830;&#32493;&#27169;&#22411;&#20197;&#23454;&#29616;&#28789;&#27963;&#24037;&#20855;&#20351;&#29992;&#65292;&#25511;&#21046;&#21644;&#35268;&#21010;&#12290;&#22312;&#38750;&#24179;&#20961;&#20219;&#21153;&#20013;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#25552;&#20986;&#30340;&#20027;&#21160;&#25512;&#29702;&#35745;&#31639;&#26694;&#26550;&#20013;&#65292;&#31163;&#25955;&#27169;&#22411;&#21487;&#20197;&#19982;&#36830;&#32493;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#20915;&#31574;&#12290;&#20174;&#21478;&#19968;&#20010;&#35282;&#24230;&#26469;&#30475;&#65292;&#31616;&#21333;&#30340;&#20195;&#29702;&#21487;&#20197;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#19990;&#30028;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#22914;&#20309;&#23558;&#36825;&#20004;&#20010;&#29305;&#28857;&#32467;&#21512;&#36215;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26550;&#26500;&#65292;&#30001;&#22810;&#20010;&#28151;&#21512; - &#36830;&#32493;&#21644;&#31163;&#25955; - &#21333;&#20803;&#32452;&#25104;&#65292;&#22797;&#21046;&#20195;&#29702;&#30340;&#37197;&#32622;&#65292;&#30001;&#39640;&#32423;&#31163;&#25955;&#27169;&#22411;&#25511;&#21046;&#65292;&#23454;&#29616;&#21160;&#24577;&#35268;&#21010;&#21644;&#21516;&#27493;&#34892;&#20026;&#12290;&#27599;&#20010;&#23618;&#27425;&#20869;&#37096;&#30340;&#36827;&#19968;&#27493;&#20998;&#35299;&#21487;&#20197;&#20197;&#20998;&#23618;&#26041;&#24335;&#34920;&#31034;&#19982;self&#30456;&#20851;&#30340;&#20854;&#20182;&#20195;&#29702;&#21644;&#23545;&#35937;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#38750;&#24179;&#20961;&#30340;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#36825;&#31181;&#20998;&#23618;&#28151;&#21512;&#27169;&#22411;&#65306;&#22312;&#25342;&#21462;&#19968;&#20010;&#31227;&#21160;&#24037;&#20855;&#21518;&#21040;&#36798;&#19968;&#20010;&#31227;&#21160;&#29289;&#20307;&#12290;&#36825;&#39033;&#30740;&#31350;&#25193;&#23637;&#20102;&#20197;&#25512;&#29702;&#20026;&#25511;&#21046;&#30340;&#20808;&#21069;&#24037;&#20316;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10088v1 Announce Type: cross  Abstract: In a recent computational framework called active inference, discrete models can be linked to their continuous counterparts to perform decision-making in changing environments. From another perspective, simple agents can be combined to better capture the causal relationships of the world. How can we use these two features together to achieve efficient goal-directed behavior? We present an architecture composed of several hybrid -- continuous and discrete -- units replicating the agent's configuration, controlled by a high-level discrete model that achieves dynamic planning and synchronized behavior. Additional factorizations within each level allow to represent hierarchically other agents and objects in relation to the self. We evaluate this hierarchical hybrid model on a non-trivial task: reaching a moving object after having picked a moving tool. This study extends past work on control as inference and proposes an alternative directi
&lt;/p&gt;</description></item></channel></rss>