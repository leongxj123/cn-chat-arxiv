<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>DiffTOP&#20351;&#29992;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#20316;&#20026;&#31574;&#30053;&#34920;&#31034;&#26469;&#29983;&#25104;&#21160;&#20316;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#8220;&#30446;&#26631;&#19981;&#21305;&#37197;&#8221;&#38382;&#39064;&#65292;&#24182;&#22312;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>https://arxiv.org/abs/2402.05421</link><description>&lt;p&gt;
DiffTOP: &#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#22312;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
DiffTOP: Differentiable Trajectory Optimization for Deep Reinforcement and Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05421
&lt;/p&gt;
&lt;p&gt;
DiffTOP&#20351;&#29992;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#20316;&#20026;&#31574;&#30053;&#34920;&#31034;&#26469;&#29983;&#25104;&#21160;&#20316;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#8220;&#30446;&#26631;&#19981;&#21305;&#37197;&#8221;&#38382;&#39064;&#65292;&#24182;&#22312;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DiffTOP&#65292;&#23427;&#21033;&#29992;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#20316;&#20026;&#31574;&#30053;&#34920;&#31034;&#65292;&#20026;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#29983;&#25104;&#21160;&#20316;&#12290;&#36712;&#36857;&#20248;&#21270;&#26159;&#19968;&#31181;&#22312;&#25511;&#21046;&#39046;&#22495;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#31639;&#27861;&#65292;&#30001;&#25104;&#26412;&#21644;&#21160;&#21147;&#23398;&#20989;&#25968;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#21033;&#29992;&#20102;&#26368;&#36817;&#22312;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#20351;&#24471;&#21487;&#20197;&#35745;&#31639;&#25439;&#22833;&#23545;&#20110;&#36712;&#36857;&#20248;&#21270;&#30340;&#21442;&#25968;&#30340;&#26799;&#24230;&#12290;&#22240;&#27492;&#65292;&#36712;&#36857;&#20248;&#21270;&#30340;&#25104;&#26412;&#21644;&#21160;&#21147;&#23398;&#20989;&#25968;&#21487;&#20197;&#31471;&#21040;&#31471;&#22320;&#23398;&#20064;&#12290;DiffTOP&#35299;&#20915;&#20102;&#20043;&#21069;&#27169;&#22411;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#8220;&#30446;&#26631;&#19981;&#21305;&#37197;&#8221;&#38382;&#39064;&#65292;&#22240;&#20026;DiffTOP&#20013;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#36890;&#36807;&#36712;&#36857;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#31574;&#30053;&#26799;&#24230;&#25439;&#22833;&#30452;&#25509;&#26368;&#22823;&#21270;&#20219;&#21153;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#23545;DiffTOP&#22312;&#26631;&#20934;&#26426;&#22120;&#20154;&#25805;&#32437;&#20219;&#21153;&#22871;&#20214;&#20013;&#36827;&#34892;&#20102;&#27169;&#20223;&#23398;&#20064;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces DiffTOP, which utilizes Differentiable Trajectory OPtimization as the policy representation to generate actions for deep reinforcement and imitation learning. Trajectory optimization is a powerful and widely used algorithm in control, parameterized by a cost and a dynamics function. The key to our approach is to leverage the recent progress in differentiable trajectory optimization, which enables computing the gradients of the loss with respect to the parameters of trajectory optimization. As a result, the cost and dynamics functions of trajectory optimization can be learned end-to-end. DiffTOP addresses the ``objective mismatch'' issue of prior model-based RL algorithms, as the dynamics model in DiffTOP is learned to directly maximize task performance by differentiating the policy gradient loss through the trajectory optimization process. We further benchmark DiffTOP for imitation learning on standard robotic manipulation task suites with high-dimensional sensory
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#34892;&#21160;&#31354;&#38388;&#31163;&#25955;&#21270;&#24182;&#37319;&#29992;&#20998;&#35789;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31232;&#30095;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#20013;&#29983;&#25104;&#25216;&#24039;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20943;&#23569;&#25506;&#32034;&#30340;&#38590;&#24230;&#65292;&#24182;&#22312;&#36830;&#32493;&#34892;&#21160;&#31354;&#38388;&#20013;&#36798;&#21040;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04459</link><description>&lt;p&gt;
&#23376;&#35789;&#20316;&#20026;&#25216;&#24039;&#65306;&#31232;&#30095;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#35789;&#21270;
&lt;/p&gt;
&lt;p&gt;
Subwords as Skills: Tokenization for Sparse-Reward Reinforcement Learning. (arXiv:2309.04459v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04459
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#34892;&#21160;&#31354;&#38388;&#31163;&#25955;&#21270;&#24182;&#37319;&#29992;&#20998;&#35789;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31232;&#30095;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#20013;&#29983;&#25104;&#25216;&#24039;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20943;&#23569;&#25506;&#32034;&#30340;&#38590;&#24230;&#65292;&#24182;&#22312;&#36830;&#32493;&#34892;&#21160;&#31354;&#38388;&#20013;&#36798;&#21040;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#20855;&#26377;&#22256;&#38590;&#65292;&#22240;&#20026;&#38656;&#35201;&#36890;&#36807;&#38271;&#26399;&#30340;&#12289;&#21327;&#35843;&#30340;&#34892;&#21160;&#24207;&#21015;&#25165;&#33021;&#33719;&#24471;&#20219;&#20309;&#22870;&#21169;&#12290;&#32780;&#19988;&#65292;&#22312;&#36830;&#32493;&#30340;&#34892;&#21160;&#31354;&#38388;&#20013;&#65292;&#21487;&#33021;&#30340;&#34892;&#21160;&#25968;&#37327;&#26159;&#26080;&#31351;&#22810;&#30340;&#65292;&#36825;&#21482;&#20250;&#22686;&#21152;&#25506;&#32034;&#30340;&#38590;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#19968;&#31867;&#26041;&#27861;&#36890;&#36807;&#22312;&#21516;&#19968;&#39046;&#22495;&#25910;&#38598;&#30340;&#20132;&#20114;&#25968;&#25454;&#20013;&#24418;&#25104;&#26102;&#38388;&#19978;&#24310;&#20280;&#30340;&#34892;&#21160;&#65292;&#36890;&#24120;&#31216;&#20026;&#25216;&#24039;&#65292;&#24182;&#22312;&#36825;&#20010;&#26032;&#30340;&#34892;&#21160;&#31354;&#38388;&#19978;&#36827;&#34892;&#31574;&#30053;&#30340;&#20248;&#21270;&#12290;&#36890;&#24120;&#36825;&#26679;&#30340;&#26041;&#27861;&#22312;&#36830;&#32493;&#34892;&#21160;&#31354;&#38388;&#20013;&#38656;&#35201;&#19968;&#20010;&#28459;&#38271;&#30340;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#24320;&#22987;&#20043;&#21069;&#24418;&#25104;&#25216;&#24039;&#12290;&#37492;&#20110;&#20808;&#21069;&#30340;&#35777;&#25454;&#34920;&#26126;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#24182;&#19981;&#38656;&#35201;&#23436;&#25972;&#30340;&#36830;&#32493;&#34892;&#21160;&#31354;&#38388;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#24039;&#29983;&#25104;&#26041;&#27861;&#65292;&#21253;&#25324;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#32858;&#31867;&#23558;&#34892;&#21160;&#31354;&#38388;&#31163;&#25955;&#21270;&#65292;&#28982;&#21518;&#25105;&#20204;&#21033;&#29992;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20511;&#37492;&#26469;&#30340;&#20998;&#35789;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploration in sparse-reward reinforcement learning is difficult due to the requirement of long, coordinated sequences of actions in order to achieve any reward. Moreover, in continuous action spaces there are an infinite number of possible actions, which only increases the difficulty of exploration. One class of methods designed to address these issues forms temporally extended actions, often called skills, from interaction data collected in the same domain, and optimizes a policy on top of this new action space. Typically such methods require a lengthy pretraining phase, especially in continuous action spaces, in order to form the skills before reinforcement learning can begin. Given prior evidence that the full range of the continuous action space is not required in such tasks, we propose a novel approach to skill-generation with two components. First we discretize the action space through clustering, and second we leverage a tokenization technique borrowed from natural language pro
&lt;/p&gt;</description></item></channel></rss>