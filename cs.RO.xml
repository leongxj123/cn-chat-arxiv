<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#31867;&#20013;&#24515;&#30340;&#24314;&#31569;&#26426;&#22120;&#20154;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#21161;&#25163;&#26426;&#22120;&#20154;&#20026;&#26408;&#24037;&#21171;&#21160;&#32773;&#25552;&#20379;&#29615;&#22659;&#19978;&#19979;&#25991;&#21327;&#21161;&#65292;&#25512;&#36827;&#20102;&#26426;&#22120;&#20154;&#22312;&#24314;&#31569;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.19060</link><description>&lt;p&gt;
&#20154;&#31867;&#20013;&#24515;&#26045;&#24037;&#26426;&#22120;&#20154;&#65306;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#21161;&#25163;&#26426;&#22120;&#20154;&#20026;&#26408;&#24037;&#21171;&#21160;&#32773;&#25552;&#20379;&#29615;&#22659;&#19978;&#19979;&#25991;&#21327;&#21161;
&lt;/p&gt;
&lt;p&gt;
Towards Human-Centered Construction Robotics: An RL-Driven Companion Robot For Contextually Assisting Carpentry Workers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#31867;&#20013;&#24515;&#30340;&#24314;&#31569;&#26426;&#22120;&#20154;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#21161;&#25163;&#26426;&#22120;&#20154;&#20026;&#26408;&#24037;&#21171;&#21160;&#32773;&#25552;&#20379;&#29615;&#22659;&#19978;&#19979;&#25991;&#21327;&#21161;&#65292;&#25512;&#36827;&#20102;&#26426;&#22120;&#20154;&#22312;&#24314;&#31569;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20010;&#20805;&#28385;&#27963;&#21147;&#30340;&#24314;&#31569;&#34892;&#19994;&#20013;&#65292;&#20256;&#32479;&#30340;&#26426;&#22120;&#20154;&#38598;&#25104;&#20027;&#35201;&#38598;&#20013;&#22312;&#33258;&#21160;&#21270;&#29305;&#23450;&#20219;&#21153;&#65292;&#36890;&#24120;&#24573;&#30053;&#20102;&#24314;&#31569;&#24037;&#20316;&#27969;&#31243;&#20013;&#20154;&#31867;&#22240;&#32032;&#30340;&#22797;&#26434;&#24615;&#21644;&#21464;&#21270;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20154;&#20026;&#26412;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#8220;&#24037;&#20316;&#20276;&#20387;&#28459;&#28216;&#22120;&#8221;&#65292;&#26088;&#22312;&#21327;&#21161;&#24314;&#31569;&#24037;&#20154;&#23436;&#25104;&#20854;&#29616;&#26377;&#23454;&#36341;&#65292;&#26088;&#22312;&#22686;&#24378;&#23433;&#20840;&#24615;&#21644;&#24037;&#20316;&#27969;&#31243;&#30340;&#27969;&#30021;&#24615;&#65292;&#21516;&#26102;&#23562;&#37325;&#24314;&#31569;&#21171;&#21160;&#30340;&#25216;&#26415;&#24615;&#36136;&#12290;&#25105;&#20204;&#23545;&#22312;&#26408;&#24037;&#27169;&#26495;&#24037;&#31243;&#20013;&#37096;&#32626;&#26426;&#22120;&#20154;&#31995;&#32479;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#19968;&#20010;&#21407;&#22411;&#65292;&#36890;&#36807;&#29615;&#22659;&#30456;&#20851;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#39537;&#21160;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#37325;&#28857;&#24378;&#35843;&#20102;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#26426;&#21160;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#33298;&#36866;&#30340;&#24037;&#20154;-&#26426;&#22120;&#20154;&#21327;&#20316;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25512;&#36827;&#20102;&#26426;&#22120;&#20154;&#22312;&#24314;&#31569;&#20013;&#30340;&#24212;&#29992;&#65292;&#20513;&#23548;&#21327;&#20316;&#27169;&#22411;&#65292;&#20854;&#20013;&#33258;&#36866;&#24212;&#26426;&#22120;&#20154;&#25903;&#25345;&#32780;&#19981;&#26159;&#21462;&#20195;&#20154;&#31867;&#65292;&#24378;&#35843;&#20102;&#20132;&#20114;&#24335;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19060v1 Announce Type: cross  Abstract: In the dynamic construction industry, traditional robotic integration has primarily focused on automating specific tasks, often overlooking the complexity and variability of human aspects in construction workflows. This paper introduces a human-centered approach with a ``work companion rover" designed to assist construction workers within their existing practices, aiming to enhance safety and workflow fluency while respecting construction labor's skilled nature. We conduct an in-depth study on deploying a robotic system in carpentry formwork, showcasing a prototype that emphasizes mobility, safety, and comfortable worker-robot collaboration in dynamic environments through a contextual Reinforcement Learning (RL)-driven modular framework. Our research advances robotic applications in construction, advocating for collaborative models where adaptive robots support rather than replace humans, underscoring the potential for an interactive a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25345;&#32493;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#20998;&#25955;&#30340;&#23398;&#20064;&#26550;&#26500;&#65292;&#26469;&#35299;&#20915;&#20132;&#36890;&#36335;&#21475;&#31359;&#36234;&#21644;&#33258;&#20027;&#36187;&#36710;&#31561;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.10996</link><description>&lt;p&gt;
&#19968;&#20010;&#21487;&#25193;&#23637;&#19988;&#21487;&#24182;&#34892;&#21270;&#30340;&#25968;&#23383;&#23402;&#29983;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#21487;&#25345;&#32493;Sim2Real&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
A Scalable and Parallelizable Digital Twin Framework for Sustainable Sim2Real Transition of Multi-Agent Reinforcement Learning Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10996
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25345;&#32493;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#20998;&#25955;&#30340;&#23398;&#20064;&#26550;&#26500;&#65292;&#26469;&#35299;&#20915;&#20132;&#36890;&#36335;&#21475;&#31359;&#36234;&#21644;&#33258;&#20027;&#36187;&#36710;&#31561;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25345;&#32493;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#36873;&#25321;&#24615;&#22320;&#25353;&#38656;&#25193;&#23637;&#24182;&#34892;&#21270;&#35757;&#32451;&#24037;&#20316;&#36127;&#36733;&#65292;&#24182;&#21033;&#29992;&#26368;&#23569;&#30340;&#30828;&#20214;&#36164;&#28304;&#23558;&#35757;&#32451;&#22909;&#30340;&#31574;&#30053;&#20174;&#27169;&#25311;&#29615;&#22659;&#36716;&#31227;&#21040;&#29616;&#23454;&#19990;&#30028;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;AutoDRIVE&#29983;&#24577;&#31995;&#32479;&#20316;&#20026;&#19968;&#20010;&#21551;&#21160;&#25968;&#23383;&#23402;&#29983;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#12289;&#37096;&#32626;&#21644;&#36716;&#31227;&#21512;&#20316;&#21644;&#31454;&#20105;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#20174;&#27169;&#25311;&#29615;&#22659;&#21040;&#29616;&#23454;&#19990;&#30028;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#25506;&#31350;&#20102;4&#21488;&#21512;&#20316;&#36710;&#36742;(Nigel)&#22312;&#21333;&#26234;&#33021;&#20307;&#21644;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#29615;&#22659;&#20013;&#20849;&#20139;&#26377;&#38480;&#29366;&#24577;&#20449;&#24687;&#30340;&#20132;&#21449;&#36941;&#21382;&#38382;&#39064;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#36890;&#29992;&#31574;&#30053;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20010;&#20307;&#31574;&#30053;&#26041;&#27861;&#30740;&#31350;&#20102;2&#36742;&#36710;(F1TENTH)&#30340;&#23545;&#25239;&#24615;&#33258;&#20027;&#36187;&#36710;&#38382;&#39064;&#12290;&#22312;&#20219;&#20309;&#19968;&#32452;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#26550;&#26500;&#65292;&#36825;&#20801;&#35768;&#23545;&#31574;&#30053;&#36827;&#34892;&#26377;&#21147;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10996v1 Announce Type: cross  Abstract: This work presents a sustainable multi-agent deep reinforcement learning framework capable of selectively scaling parallelized training workloads on-demand, and transferring the trained policies from simulation to reality using minimal hardware resources. We introduce AutoDRIVE Ecosystem as an enabling digital twin framework to train, deploy, and transfer cooperative as well as competitive multi-agent reinforcement learning policies from simulation to reality. Particularly, we first investigate an intersection traversal problem of 4 cooperative vehicles (Nigel) that share limited state information in single as well as multi-agent learning settings using a common policy approach. We then investigate an adversarial autonomous racing problem of 2 vehicles (F1TENTH) using an individual policy approach. In either set of experiments, a decentralized learning architecture was adopted, which allowed robust training and testing of the policies 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#26032;&#22411;&#20840;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#25104;&#21151;&#24212;&#29992;&#20110;&#22788;&#29702;&#33151;&#24335;&#26426;&#22120;&#20154;&#65292;&#22312;&#21508;&#31181;&#27169;&#25311;&#22320;&#24418;&#20013;&#21462;&#24471;&#20102;&#26480;&#20986;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2310.05022</link><description>&lt;p&gt;
&#29992;&#20110;&#33151;&#24335;&#26426;&#22120;&#20154;&#30340;&#20840;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Fully Spiking Neural Network for Legged Robots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#26032;&#22411;&#20840;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#25104;&#21151;&#24212;&#29992;&#20110;&#22788;&#29702;&#33151;&#24335;&#26426;&#22120;&#20154;&#65292;&#22312;&#21508;&#31181;&#27169;&#25311;&#22320;&#24418;&#20013;&#21462;&#24471;&#20102;&#26480;&#20986;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#33151;&#24335;&#26426;&#22120;&#20154;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#22235;&#36275;&#26426;&#22120;&#20154;&#23637;&#31034;&#20102;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#23436;&#25104;&#20855;&#26377;&#25361;&#25112;&#24615;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#24050;&#37096;&#32626;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#20197;&#21327;&#21161;&#20154;&#31867;&#12290;&#21516;&#26102;&#65292;&#20004;&#36275;&#21644;&#31867;&#20154;&#26426;&#22120;&#20154;&#22312;&#21508;&#31181;&#39640;&#38590;&#24230;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#31361;&#30772;&#12290;&#26412;&#30740;&#31350;&#25104;&#21151;&#23558;&#19968;&#31181;&#26032;&#22411;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#24212;&#29992;&#20110;&#22788;&#29702;&#33151;&#24335;&#26426;&#22120;&#20154;&#65292;&#22312;&#19968;&#31995;&#21015;&#27169;&#25311;&#22320;&#24418;&#20013;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05022v2 Announce Type: replace-cross  Abstract: In recent years, legged robots based on deep reinforcement learning have made remarkable progress. Quadruped robots have demonstrated the ability to complete challenging tasks in complex environments and have been deployed in real-world scenarios to assist humans. Simultaneously, bipedal and humanoid robots have achieved breakthroughs in various demanding tasks. Current reinforcement learning methods can utilize diverse robot bodies and historical information to perform actions. However, prior research has not emphasized the speed and energy consumption of network inference, as well as the biological significance of the neural networks themselves. Most of the networks employed are traditional artificial neural networks that utilize multilayer perceptrons (MLP). In this paper, we successfully apply a novel Spiking Neural Network (SNN) to process legged robots, achieving outstanding results across a range of simulated terrains. S
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22871;&#20214;&#65292;&#29992;&#20110;&#25511;&#21046;&#28014;&#21160;&#24179;&#21488;&#30340;&#36712;&#36857;&#12290;&#36890;&#36807;&#35757;&#32451;&#31934;&#30830;&#25805;&#20316;&#31574;&#30053;&#20197;&#24212;&#23545;&#21160;&#24577;&#21644;&#19981;&#21487;&#39044;&#27979;&#30340;&#26465;&#20214;&#65292;&#35299;&#20915;&#20102;&#25511;&#21046;&#28014;&#21160;&#24179;&#21488;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;&#35813;&#22871;&#20214;&#20855;&#26377;&#31283;&#20581;&#24615;&#12289;&#36866;&#24212;&#24615;&#21644;&#21487;&#20256;&#36882;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#21487;&#35270;&#21270;&#36873;&#39033;&#21644;&#19982;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#31995;&#32479;&#38598;&#25104;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.04266</link><description>&lt;p&gt;
DRIFT: &#26234;&#33021;&#28014;&#21160;&#24179;&#21488;&#36712;&#36857;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DRIFT: Deep Reinforcement Learning for Intelligent Floating Platforms Trajectories. (arXiv:2310.04266v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22871;&#20214;&#65292;&#29992;&#20110;&#25511;&#21046;&#28014;&#21160;&#24179;&#21488;&#30340;&#36712;&#36857;&#12290;&#36890;&#36807;&#35757;&#32451;&#31934;&#30830;&#25805;&#20316;&#31574;&#30053;&#20197;&#24212;&#23545;&#21160;&#24577;&#21644;&#19981;&#21487;&#39044;&#27979;&#30340;&#26465;&#20214;&#65292;&#35299;&#20915;&#20102;&#25511;&#21046;&#28014;&#21160;&#24179;&#21488;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;&#35813;&#22871;&#20214;&#20855;&#26377;&#31283;&#20581;&#24615;&#12289;&#36866;&#24212;&#24615;&#21644;&#21487;&#20256;&#36882;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#21487;&#35270;&#21270;&#36873;&#39033;&#21644;&#19982;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#31995;&#32479;&#38598;&#25104;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#22411;&#22871;&#20214;&#65292;&#29992;&#20110;&#25511;&#21046;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&#28014;&#21160;&#24179;&#21488;&#12290;&#28014;&#21160;&#24179;&#21488;&#21487;&#20316;&#20026;&#22810;&#21151;&#33021;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#22312;&#22320;&#29699;&#19978;&#27169;&#25311;&#24494;&#37325;&#21147;&#29615;&#22659;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#33021;&#22815;&#22312;&#21160;&#24577;&#21644;&#19981;&#21487;&#39044;&#27979;&#30340;&#26465;&#20214;&#19979;&#36827;&#34892;&#31934;&#30830;&#25805;&#20316;&#30340;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#25511;&#21046;&#27492;&#31867;&#24179;&#21488;&#20013;&#30340;&#31995;&#32479;&#21644;&#29615;&#22659;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#25105;&#20204;&#30340;&#22871;&#20214;&#23454;&#29616;&#20102;&#31283;&#20581;&#24615;&#12289;&#36866;&#24212;&#24615;&#21644;&#20174;&#27169;&#25311;&#21040;&#29616;&#23454;&#30340;&#33391;&#22909;&#21487;&#20256;&#36882;&#24615;&#12290;&#25105;&#20204;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26694;&#26550;&#25552;&#20379;&#20102;&#24555;&#36895;&#35757;&#32451;&#26102;&#38388;&#12289;&#22823;&#35268;&#27169;&#27979;&#35797;&#33021;&#21147;&#12289;&#20016;&#23500;&#30340;&#21487;&#35270;&#21270;&#36873;&#39033;&#20197;&#21450;&#19982;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#31995;&#32479;&#38598;&#25104;&#30340;ROS&#32465;&#23450;&#12290;&#38500;&#20102;&#31574;&#30053;&#24320;&#21457;&#65292;&#25105;&#20204;&#30340;&#22871;&#20214;&#36824;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#24179;&#21488;&#65292;&#25552;&#20379;&#24320;&#25918;&#35775;&#38382;&#65292;&#32593;&#22336;&#20026;https://github.com/elharirymatteo/RANS/tree/ICRA24&#12290;
&lt;/p&gt;
&lt;p&gt;
This investigation introduces a novel deep reinforcement learning-based suite to control floating platforms in both simulated and real-world environments. Floating platforms serve as versatile test-beds to emulate microgravity environments on Earth. Our approach addresses the system and environmental uncertainties in controlling such platforms by training policies capable of precise maneuvers amid dynamic and unpredictable conditions. Leveraging state-of-the-art deep reinforcement learning techniques, our suite achieves robustness, adaptability, and good transferability from simulation to reality. Our Deep Reinforcement Learning (DRL) framework provides advantages such as fast training times, large-scale testing capabilities, rich visualization options, and ROS bindings for integration with real-world robotic systems. Beyond policy development, our suite provides a comprehensive platform for researchers, offering open-access at https://github.com/elharirymatteo/RANS/tree/ICRA24.
&lt;/p&gt;</description></item><item><title>DoReMi&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#35328;&#27169;&#22411;&#22522;&#30784;&#26550;&#26500;&#65292;&#36890;&#36807;&#26816;&#27979;&#21644;&#20462;&#22797;&#35745;&#21010;&#19982;&#25191;&#34892;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#26469;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#12290;&#35813;&#26550;&#26500;&#21033;&#29992;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#26816;&#26597;&#32422;&#26463;&#26465;&#20214;&#20197;&#21457;&#29616;&#19981;&#19968;&#33268;&#65292;&#24182;&#35843;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#37325;&#26032;&#35268;&#21010;&#20197;&#23454;&#29616;&#24674;&#22797;&#12290;</title><link>http://arxiv.org/abs/2307.00329</link><description>&lt;p&gt;
DoReMi: &#36890;&#36807;&#26816;&#27979;&#21644;&#20462;&#22797;&#35745;&#21010;&#25191;&#34892;&#19981;&#19968;&#33268;&#26469;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
DoReMi: Grounding Language Model by Detecting and Recovering from Plan-Execution Misalignment. (arXiv:2307.00329v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00329
&lt;/p&gt;
&lt;p&gt;
DoReMi&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#35328;&#27169;&#22411;&#22522;&#30784;&#26550;&#26500;&#65292;&#36890;&#36807;&#26816;&#27979;&#21644;&#20462;&#22797;&#35745;&#21010;&#19982;&#25191;&#34892;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#26469;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#12290;&#35813;&#26550;&#26500;&#21033;&#29992;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#26816;&#26597;&#32422;&#26463;&#26465;&#20214;&#20197;&#21457;&#29616;&#19981;&#19968;&#33268;&#65292;&#24182;&#35843;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#37325;&#26032;&#35268;&#21010;&#20197;&#23454;&#29616;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21253;&#21547;&#22823;&#37327;&#30340;&#35821;&#20041;&#30693;&#35782;&#65292;&#24182;&#20855;&#22791;&#20986;&#33394;&#30340;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;&#22914;&#20309;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#26426;&#22120;&#20154;&#20219;&#21153;&#30456;&#32467;&#21512;&#65292;&#20197;&#30830;&#20445;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#24207;&#21015;&#22312;&#36923;&#36753;&#19978;&#27491;&#30830;&#19988;&#21487;&#25191;&#34892;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29615;&#22659;&#25200;&#21160;&#25110;&#25511;&#21046;&#22120;&#35774;&#35745;&#30340;&#19981;&#23436;&#21892;&#65292;&#24213;&#23618;&#25191;&#34892;&#21487;&#33021;&#20250;&#20559;&#31163;&#39640;&#32423;&#35745;&#21010;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DoReMi&#30340;&#26032;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#30784;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#33021;&#22815;&#21450;&#26102;&#26816;&#27979;&#21644;&#20462;&#22797;&#35745;&#21010;&#19982;&#25191;&#34892;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;LLM&#36827;&#34892;&#35268;&#21010;&#65292;&#24182;&#29983;&#25104;&#35745;&#21010;&#27493;&#39588;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#36825;&#20123;&#32422;&#26463;&#26465;&#20214;&#21487;&#20197;&#25351;&#31034;&#35745;&#21010;&#19982;&#25191;&#34892;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#19988;&#25105;&#20204;&#20351;&#29992;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#27169;&#22411;&#22312;&#20302;&#23618;&#25216;&#33021;&#25191;&#34892;&#36807;&#31243;&#20013;&#26816;&#26597;&#32422;&#26463;&#26465;&#20214;&#12290;&#22914;&#26524;&#21457;&#29983;&#29305;&#23450;&#30340;&#19981;&#19968;&#33268;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#35843;&#29992;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#35268;&#21010;&#20197;&#20174;&#20013;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models encode a vast amount of semantic knowledge and possess remarkable understanding and reasoning capabilities. Previous research has explored how to ground language models in robotic tasks to ensure that the sequences generated by the language model are both logically correct and practically executable. However, low-level execution may deviate from the high-level plan due to environmental perturbations or imperfect controller design. In this paper, we propose DoReMi, a novel language model grounding framework that enables immediate Detection and Recovery from Misalignments between plan and execution. Specifically, LLMs are leveraged for both planning and generating constraints for planned steps. These constraints can indicate plan-execution misalignments and we use a vision question answering (VQA) model to check constraints during low-level skill execution. If certain misalignment occurs, our method will call the language model to re-plan in order to recover from mi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#21452;&#26354;&#27969;&#24418;&#19978;&#20351;&#29992;GPLVM&#26469;&#22312;&#36830;&#32493;&#39046;&#22495;&#20013;&#24212;&#29992;&#26426;&#22120;&#20154;&#20998;&#31867;&#27861;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#30456;&#20851;&#23618;&#27425;&#32467;&#26500;&#30340;&#21452;&#26354;&#23884;&#20837;&#26469;&#24314;&#27169;&#20998;&#31867;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#22270;&#24418;&#20808;&#39564;&#21644;&#20445;&#25345;&#36317;&#31163;&#30340;&#21518;&#21521;&#32422;&#26463;&#26469;&#23454;&#29616;&#20998;&#31867;&#27861;&#32467;&#26500;&#30340;&#32435;&#20837;&#12290;</title><link>http://arxiv.org/abs/2210.01672</link><description>&lt;p&gt;
&#36890;&#36807;&#22312;&#21452;&#26354;&#27969;&#24418;&#19978;&#20351;&#29992;GPLVM&#23558;&#26426;&#22120;&#20154;&#20998;&#31867;&#24102;&#20837;&#36830;&#32493;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
Bringing robotics taxonomies to continuous domains via GPLVM on hyperbolic manifolds. (arXiv:2210.01672v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#21452;&#26354;&#27969;&#24418;&#19978;&#20351;&#29992;GPLVM&#26469;&#22312;&#36830;&#32493;&#39046;&#22495;&#20013;&#24212;&#29992;&#26426;&#22120;&#20154;&#20998;&#31867;&#27861;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#30456;&#20851;&#23618;&#27425;&#32467;&#26500;&#30340;&#21452;&#26354;&#23884;&#20837;&#26469;&#24314;&#27169;&#20998;&#31867;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#22270;&#24418;&#20808;&#39564;&#21644;&#20445;&#25345;&#36317;&#31163;&#30340;&#21518;&#21521;&#32422;&#26463;&#26469;&#23454;&#29616;&#20998;&#31867;&#27861;&#32467;&#26500;&#30340;&#32435;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#20998;&#31867;&#34987;&#29992;&#20316;&#23558;&#20154;&#31867;&#30340;&#31227;&#21160;&#21644;&#19982;&#29615;&#22659;&#20114;&#21160;&#30340;&#26041;&#24335;&#36827;&#34892;&#39640;&#23618;&#27425;&#30340;&#20998;&#23618;&#25277;&#35937;&#12290;&#23427;&#20204;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#20998;&#26512;&#25235;&#21462;&#12289;&#25805;&#32437;&#25216;&#33021;&#21644;&#20840;&#36523;&#25903;&#25745;&#23039;&#21183;&#38750;&#24120;&#26377;&#29992;&#12290;&#23613;&#31649;&#25105;&#20204;&#22312;&#35774;&#35745;&#23618;&#27425;&#32467;&#26500;&#21644;&#22522;&#30784;&#31867;&#21035;&#26041;&#38754;&#20570;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#24212;&#29992;&#39046;&#22495;&#30340;&#20351;&#29992;&#20173;&#28982;&#26377;&#38480;&#12290;&#36825;&#21487;&#33021;&#26159;&#22240;&#20026;&#32570;&#20047;&#22635;&#34917;&#20998;&#31867;&#23618;&#32423;&#32467;&#26500;&#21644;&#19982;&#20854;&#31867;&#21035;&#30456;&#20851;&#32852;&#30340;&#39640;&#32500;&#24322;&#26500;&#25968;&#25454;&#20043;&#38388;&#24046;&#36317;&#30340;&#35745;&#31639;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#25429;&#25417;&#30456;&#20851;&#23618;&#27425;&#32467;&#26500;&#30340;&#21452;&#26354;&#23884;&#20837;&#26469;&#24314;&#27169;&#20998;&#31867;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#26032;&#39062;&#30340;&#39640;&#26031;&#36807;&#31243;&#21452;&#26354;&#28508;&#21464;&#37327;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#22270;&#24418;&#20808;&#39564;&#21644;&#20445;&#25345;&#36317;&#31163;&#30340;&#21518;&#21521;&#32422;&#26463;&#23558;&#20998;&#31867;&#27861;&#32467;&#26500;&#32435;&#20837;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#26426;&#22120;&#20154;&#20998;&#31867;&#27861;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic taxonomies serve as high-level hierarchical abstractions that classify how humans move and interact with their environment. They have proven useful to analyse grasps, manipulation skills, and whole-body support poses. Despite substantial efforts devoted to design their hierarchy and underlying categories, their use in application fields remains limited. This may be attributed to the lack of computational models that fill the gap between the discrete hierarchical structure of the taxonomy and the high-dimensional heterogeneous data associated to its categories. To overcome this problem, we propose to model taxonomy data via hyperbolic embeddings that capture the associated hierarchical structure. We achieve this by formulating a novel Gaussian process hyperbolic latent variable model that incorporates the taxonomy structure through graph-based priors on the latent space and distance-preserving back constraints. We validate our model on three different robotics taxonomies to lear
&lt;/p&gt;</description></item></channel></rss>