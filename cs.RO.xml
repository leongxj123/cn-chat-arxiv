<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;MindArm&#65292;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23558;&#22823;&#33041;&#20449;&#21495;&#32763;&#35793;&#25104;&#20551;&#32930;&#36816;&#21160;&#65292;&#24110;&#21161;&#24739;&#32773;&#25191;&#34892;&#21508;&#31181;&#27963;&#21160;</title><link>https://arxiv.org/abs/2403.19992</link><description>&lt;p&gt;
MindArm: &#26426;&#26800;&#26234;&#33021;&#38750;&#20405;&#20837;&#24335;&#31070;&#32463;&#39537;&#21160;&#20551;&#32930;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
MindArm: Mechanized Intelligent Non-Invasive Neuro-Driven Prosthetic Arm System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19992
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;MindArm&#65292;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23558;&#22823;&#33041;&#20449;&#21495;&#32763;&#35793;&#25104;&#20551;&#32930;&#36816;&#21160;&#65292;&#24110;&#21161;&#24739;&#32773;&#25191;&#34892;&#21508;&#31181;&#27963;&#21160;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#27531;&#30142;&#25110;&#38590;&#20197;&#31227;&#21160;&#25163;&#33218;&#30340;&#20154;&#65288;&#31616;&#31216;&#8220;&#24739;&#32773;&#8221;&#65289;&#22312;&#26377;&#25928;&#35299;&#20915;&#29983;&#29702;&#38480;&#21046;&#26041;&#38754;&#26377;&#38750;&#24120;&#26377;&#38480;&#30340;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#20004;&#20010;&#21407;&#22240;&#65306;&#19968;&#26159;&#20687;&#20197;&#24605;&#32500;&#25511;&#21046;&#20026;&#20027;&#30340;&#20551;&#32930;&#35774;&#22791;&#36890;&#24120;&#38750;&#24120;&#26114;&#36149;&#24182;&#38656;&#35201;&#26114;&#36149;&#30340;&#32500;&#25252;&#65307;&#20108;&#26159;&#20854;&#20182;&#35299;&#20915;&#26041;&#26696;&#38656;&#35201;&#26114;&#36149;&#30340;&#20405;&#20837;&#24615;&#33041;&#37096;&#25163;&#26415;&#65292;&#36825;&#31181;&#25163;&#26415;&#39118;&#38505;&#39640;&#65292;&#26114;&#36149;&#19988;&#32500;&#25252;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#24403;&#21069;&#30340;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#24182;&#19981;&#36866;&#29992;&#20110;&#20855;&#26377;&#19981;&#21516;&#36130;&#21153;&#32972;&#26223;&#30340;&#25152;&#26377;&#24739;&#32773;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#65292;&#21517;&#20026;MindArm&#65292;&#21363;&#19968;&#31181;&#26426;&#26800;&#26234;&#33021;&#38750;&#20405;&#20837;&#24335;&#31070;&#32463;&#39537;&#21160;&#20551;&#32930;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;MindArm&#31995;&#32479;&#37319;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#24341;&#25806;&#23558;&#22823;&#33041;&#20449;&#21495;&#32763;&#35793;&#25104;&#39044;&#26399;&#30340;&#20551;&#32930;&#36816;&#21160;&#65292;&#20174;&#32780;&#24110;&#21161;&#24739;&#32773;&#23454;&#26045;&#35768;&#22810;&#27963;&#21160;&#65292;&#23613;&#31649;&#20182;&#20204;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19992v1 Announce Type: new  Abstract: Currently, people with disability or difficulty to move their arms (referred to as "patients") have very limited technological solutions to efficiently address their physiological limitations. It is mainly due to two reasons: (1) the non-invasive solutions like mind-controlled prosthetic devices are typically very costly and require expensive maintenance; and (2) other solutions require costly invasive brain surgery, which is high risk to perform, expensive, and difficult to maintain. Therefore, current technological solutions are not accessible for all patients with different financial backgrounds. Toward this, we propose a low-cost technological solution called MindArm, a mechanized intelligent non-invasive neuro-driven prosthetic arm system. Our MindArm system employs a deep neural network (DNN) engine to translate brain signals into the intended prosthetic arm motion, thereby helping patients to perform many activities despite their 
&lt;/p&gt;</description></item><item><title>UADA3D&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#23545;&#25239;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;3D&#29289;&#20307;&#26816;&#27979;&#20013;&#22788;&#29702;&#31232;&#30095;LiDAR&#25968;&#25454;&#21644;&#22823;&#39046;&#22495;&#24046;&#36317;&#65292;&#24182;&#22312;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#21644;&#31227;&#21160;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.17633</link><description>&lt;p&gt;
UADA3D&#65306;&#38754;&#21521;&#31232;&#30095;LiDAR&#21644;&#22823;&#39046;&#22495;&#24046;&#36317;&#30340;&#26080;&#30417;&#30563;&#23545;&#25239;&#39046;&#22495;&#33258;&#36866;&#24212;&#22312;3D&#29289;&#20307;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
UADA3D: Unsupervised Adversarial Domain Adaptation for 3D Object Detection with Sparse LiDAR and Large Domain Gaps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17633
&lt;/p&gt;
&lt;p&gt;
UADA3D&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#23545;&#25239;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;3D&#29289;&#20307;&#26816;&#27979;&#20013;&#22788;&#29702;&#31232;&#30095;LiDAR&#25968;&#25454;&#21644;&#22823;&#39046;&#22495;&#24046;&#36317;&#65292;&#24182;&#22312;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#21644;&#31227;&#21160;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#29616;&#26377;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#22312;&#22522;&#20110;LiDAR&#30340;3D&#29289;&#20307;&#26816;&#27979;&#20013;&#30340;&#19968;&#20010;&#38382;&#39064;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#36866;&#24212;&#24050;&#24314;&#31435;&#30340;&#39640;&#23494;&#24230;&#33258;&#21160;&#39550;&#39542;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#36716;&#21464;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#26356;&#31232;&#30095;&#30340;&#28857;&#20113;&#65292;&#25429;&#25417;&#26469;&#33258;&#19981;&#21516;&#35270;&#35282;&#30340;&#22330;&#26223;&#65306;&#19981;&#20165;&#26469;&#33258;&#36947;&#36335;&#19978;&#30340;&#36710;&#36742;&#65292;&#36824;&#26469;&#33258;&#20154;&#34892;&#36947;&#19978;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#65292;&#36973;&#36935;&#30528;&#26126;&#26174;&#19981;&#21516;&#30340;&#29615;&#22659;&#26465;&#20214;&#21644;&#20256;&#24863;&#22120;&#37197;&#32622;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26080;&#30417;&#30563;&#23545;&#25239;&#39046;&#22495;&#33258;&#36866;&#24212;3D&#29289;&#20307;&#26816;&#27979;&#65288;UADA3D&#65289;&#12290;UADA3D&#19981;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#28304;&#27169;&#22411;&#25110;&#24072;&#29983;&#26550;&#26500;&#12290;&#30456;&#21453;&#65292;&#23427;&#20351;&#29992;&#23545;&#25239;&#26041;&#27861;&#30452;&#25509;&#23398;&#20064;&#22495;&#19981;&#21464;&#29305;&#24449;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#22312;&#21508;&#31181;&#36866;&#24212;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#21644;&#31227;&#21160;&#26426;&#22120;&#20154;&#39046;&#22495;&#22343;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#26159;&#24320;&#28304;&#30340;&#65292;&#24456;&#24555;&#23558;&#20250;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17633v1 Announce Type: cross  Abstract: In this study, we address a gap in existing unsupervised domain adaptation approaches on LiDAR-based 3D object detection, which have predominantly concentrated on adapting between established, high-density autonomous driving datasets. We focus on sparser point clouds, capturing scenarios from different perspectives: not just from vehicles on the road but also from mobile robots on sidewalks, which encounter significantly different environmental conditions and sensor configurations. We introduce Unsupervised Adversarial Domain Adaptation for 3D Object Detection (UADA3D). UADA3D does not depend on pre-trained source models or teacher-student architectures. Instead, it uses an adversarial approach to directly learn domain-invariant features. We demonstrate its efficacy in various adaptation scenarios, showing significant improvements in both self-driving car and mobile robot domains. Our code is open-source and will be available soon.
&lt;/p&gt;</description></item><item><title>ViSaRL&#25552;&#20986;&#20102;Visual Saliency-Guided Reinforcement Learning&#65288;&#21463;&#35270;&#35273;&#26174;&#33879;&#24615;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#35270;&#35273;&#34920;&#31034;&#26469;&#26174;&#33879;&#25552;&#39640;RL&#20195;&#29702;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#25104;&#21151;&#29575;&#12289;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.10940</link><description>&lt;p&gt;
ViSaRL&#65306;&#21463;&#20154;&#31867;&#26174;&#33879;&#24615;&#24341;&#23548;&#30340;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ViSaRL: Visual Reinforcement Learning Guided by Human Saliency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10940
&lt;/p&gt;
&lt;p&gt;
ViSaRL&#25552;&#20986;&#20102;Visual Saliency-Guided Reinforcement Learning&#65288;&#21463;&#35270;&#35273;&#26174;&#33879;&#24615;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#35270;&#35273;&#34920;&#31034;&#26469;&#26174;&#33879;&#25552;&#39640;RL&#20195;&#29702;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#25104;&#21151;&#29575;&#12289;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20174;&#39640;&#32500;&#20687;&#32032;&#36755;&#20837;&#22521;&#35757;&#26426;&#22120;&#20154;&#25191;&#34892;&#22797;&#26434;&#25511;&#21046;&#20219;&#21153;&#22312;&#26679;&#26412;&#25928;&#29575;&#19978;&#26159;&#20302;&#25928;&#30340;&#65292;&#22240;&#20026;&#22270;&#20687;&#35266;&#23519;&#20027;&#35201;&#30001;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#20449;&#24687;&#32452;&#25104;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#33021;&#22815;&#22312;&#35270;&#35273;&#19978;&#20851;&#27880;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#23545;&#35937;&#21644;&#21306;&#22495;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21463;&#35270;&#35273;&#26174;&#33879;&#24615;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;ViSaRL&#65289;&#12290;&#20351;&#29992;ViSaRL&#23398;&#20064;&#35270;&#35273;&#34920;&#31034;&#26174;&#30528;&#25552;&#39640;&#20102;RL&#20195;&#29702;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#65292;&#21253;&#25324;DeepMind&#25511;&#21046;&#22522;&#20934;&#12289;&#20223;&#30495;&#20013;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#21644;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#30340;&#25104;&#21151;&#29575;&#12289;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#26174;&#33879;&#24615;&#25972;&#21512;&#21040;&#22522;&#20110;CNN&#21644;Transformer&#30340;&#32534;&#30721;&#22120;&#20013;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20351;&#29992;ViSaRL&#23398;&#20064;&#30340;&#35270;&#35273;&#34920;&#31034;&#23545;&#21508;&#31181;&#35270;&#35273;&#25200;&#21160;&#65292;&#21253;&#25324;&#24863;&#30693;&#22122;&#22768;&#21644;&#22330;&#26223;&#21464;&#21270;&#65292;&#37117;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;ViSaRL&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#25104;&#21151;&#29575;&#20960;&#20046;&#32763;&#20102;&#19968;&#30058;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10940v1 Announce Type: cross  Abstract: Training robots to perform complex control tasks from high-dimensional pixel input using reinforcement learning (RL) is sample-inefficient, because image observations are comprised primarily of task-irrelevant information. By contrast, humans are able to visually attend to task-relevant objects and areas. Based on this insight, we introduce Visual Saliency-Guided Reinforcement Learning (ViSaRL). Using ViSaRL to learn visual representations significantly improves the success rate, sample efficiency, and generalization of an RL agent on diverse tasks including DeepMind Control benchmark, robot manipulation in simulation and on a real robot. We present approaches for incorporating saliency into both CNN and Transformer-based encoders. We show that visual representations learned using ViSaRL are robust to various sources of visual perturbations including perceptual noise and scene variations. ViSaRL nearly doubles success rate on the real-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HIntS&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;&#26080;&#30417;&#30563;&#26816;&#27979;&#22120;&#65292;&#22522;&#20110;Granger&#22240;&#26524;&#24615;&#25429;&#25417;&#22240;&#32032;&#20043;&#38388;&#30340;&#20851;&#38190;&#20107;&#20214;&#65292;&#21457;&#29616;&#21644;&#35757;&#32451;&#19968;&#31995;&#21015;&#25805;&#20316;&#22240;&#32032;&#21270;&#29615;&#22659;&#20013;&#30340;&#22240;&#32032;&#30340;&#25216;&#33021;&#65292;&#20854;&#23637;&#31034;&#20102;&#22312;&#26426;&#22120;&#20154;&#25512;&#21160;&#20219;&#21153;&#19978;&#26377;2-3&#20493;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#26368;&#32456;&#24615;&#33021;&#30340;&#25552;&#39640;&#65292;&#26377;&#25928;&#30340;&#22788;&#29702;&#20102;&#22797;&#26434;&#38382;&#39064;&#21644;&#36716;&#31227;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2306.09509</link><description>&lt;p&gt;
Granger&#22240;&#26524;&#30340;&#20998;&#23618;&#25216;&#33021;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Granger-Causal Hierarchical Skill Discovery. (arXiv:2306.09509v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HIntS&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;&#26080;&#30417;&#30563;&#26816;&#27979;&#22120;&#65292;&#22522;&#20110;Granger&#22240;&#26524;&#24615;&#25429;&#25417;&#22240;&#32032;&#20043;&#38388;&#30340;&#20851;&#38190;&#20107;&#20214;&#65292;&#21457;&#29616;&#21644;&#35757;&#32451;&#19968;&#31995;&#21015;&#25805;&#20316;&#22240;&#32032;&#21270;&#29615;&#22659;&#20013;&#30340;&#22240;&#32032;&#30340;&#25216;&#33021;&#65292;&#20854;&#23637;&#31034;&#20102;&#22312;&#26426;&#22120;&#20154;&#25512;&#21160;&#20219;&#21153;&#19978;&#26377;2-3&#20493;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#26368;&#32456;&#24615;&#33021;&#30340;&#25552;&#39640;&#65292;&#26377;&#25928;&#30340;&#22788;&#29702;&#20102;&#22797;&#26434;&#38382;&#39064;&#21644;&#36716;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#22312;&#23398;&#20064;&#22797;&#26434;&#20219;&#21153;&#30340;&#31574;&#30053;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#24448;&#24448;&#20250;&#36973;&#21463;&#20302;&#26679;&#26412;&#25928;&#29575;&#21644;&#26377;&#38480;&#36716;&#31227;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HIntS&#30340;&#31639;&#27861;&#65292;&#23427;&#20351;&#29992;&#23398;&#20064;&#24471;&#21040;&#30340;&#20132;&#20114;&#26816;&#27979;&#22120;&#26469;&#21457;&#29616;&#21644;&#35757;&#32451;&#19968;&#31995;&#21015;&#25216;&#33021;&#65292;&#36825;&#20123;&#25216;&#33021;&#25805;&#20316;&#22240;&#32032;&#21270;&#29615;&#22659;&#20013;&#30340;&#22240;&#32032;&#12290;&#21463;Granger&#22240;&#26524;&#24615;&#30340;&#21551;&#21457;&#65292;&#36825;&#20123;&#26080;&#30417;&#30563;&#26816;&#27979;&#22120;&#25429;&#25417;&#21040;&#22240;&#32032;&#20043;&#38388;&#30340;&#20851;&#38190;&#20107;&#20214;&#65292;&#20197;&#20415;&#39640;&#25928;&#22320;&#23398;&#20064;&#26377;&#29992;&#30340;&#25216;&#33021;&#65292;&#24182;&#23558;&#36825;&#20123;&#25216;&#33021;&#36716;&#31227;&#21040;&#20854;&#20182;&#30456;&#20851;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#26159;&#35768;&#22810;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#25152;&#38754;&#20020;&#30340;&#22256;&#22659;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#24102;&#26377;&#38556;&#30861;&#29289;&#30340;&#26426;&#22120;&#20154;&#25512;&#21160;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;HIntS - &#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39046;&#22495;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#65292;&#20854;&#20182;RL&#21644;HRL&#26041;&#27861;&#37117;&#34920;&#29616;&#19981;&#20339;&#12290;&#23398;&#20064;&#21040;&#30340;&#25216;&#33021;&#19981;&#20165;&#23637;&#31034;&#20102;&#20351;&#29992;Breakout&#30340;&#21464;&#20307;&#30340;&#36716;&#31227;&#65292;&#32780;&#19988;&#19982;&#21487;&#27604;&#36739;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#32447;&#30456;&#27604;&#65292;&#36824;&#34920;&#29616;&#20986;2-3&#20493;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#26368;&#32456;&#24615;&#33021;&#30340;&#25552;&#39640;&#12290;HIntS&#19968;&#36215;&#35777;&#26126;&#20102;&#19968;&#31181;&#23618;&#27425;&#32467;&#26500;&#30340;&#25216;&#33021;&#21457;&#29616;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) has shown promising results learning policies for complex tasks, but can often suffer from low sample efficiency and limited transfer. We introduce the Hierarchy of Interaction Skills (HIntS) algorithm, which uses learned interaction detectors to discover and train a hierarchy of skills that manipulate factors in factored environments. Inspired by Granger causality, these unsupervised detectors capture key events between factors to sample efficiently learn useful skills and transfer those skills to other related tasks -- tasks where many reinforcement learning techniques struggle. We evaluate HIntS on a robotic pushing task with obstacles -- a challenging domain where other RL and HRL methods fall short. The learned skills not only demonstrate transfer using variants of Breakout, a common RL benchmark, but also show 2-3x improvement in both sample efficiency and final performance compared to comparable RL baselines. Together, HIntS demonstrates a proof of co
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38647;&#36798;&#36870;&#21521;&#20256;&#24863;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#23558;&#31232;&#30095;&#38647;&#36798;&#26816;&#27979;&#26144;&#23556;&#21040;&#26497;&#22352;&#26631;&#27979;&#37327;&#32593;&#26684;&#65292;&#24182;&#29983;&#25104;&#21160;&#24577;&#32593;&#26684;&#22320;&#22270;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#25163;&#24037;&#21046;&#20316;&#30340;&#20960;&#20309;ISM&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20026;&#20174;&#26377;&#38480;&#35270;&#22330;&#30340;&#38647;&#36798;&#20013;&#23398;&#20064;&#26497;&#22352;&#26631;&#26041;&#26696;&#30340;&#21333;&#24103;&#27979;&#37327;&#32593;&#26684;&#30340;&#31532;&#19968;&#20010;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.12409</link><description>&lt;p&gt;
&#21160;&#24577;&#21344;&#25454;&#32593;&#26684;&#22320;&#22270;&#30340;&#28145;&#24230;&#38647;&#36798;&#36870;&#21521;&#20256;&#24863;&#22120;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Deep Radar Inverse Sensor Models for Dynamic Occupancy Grid Maps. (arXiv:2305.12409v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12409
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38647;&#36798;&#36870;&#21521;&#20256;&#24863;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#23558;&#31232;&#30095;&#38647;&#36798;&#26816;&#27979;&#26144;&#23556;&#21040;&#26497;&#22352;&#26631;&#27979;&#37327;&#32593;&#26684;&#65292;&#24182;&#29983;&#25104;&#21160;&#24577;&#32593;&#26684;&#22320;&#22270;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#25163;&#24037;&#21046;&#20316;&#30340;&#20960;&#20309;ISM&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20026;&#20174;&#26377;&#38480;&#35270;&#22330;&#30340;&#38647;&#36798;&#20013;&#23398;&#20064;&#26497;&#22352;&#26631;&#26041;&#26696;&#30340;&#21333;&#24103;&#27979;&#37327;&#32593;&#26684;&#30340;&#31532;&#19968;&#20010;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#30340;&#19968;&#20010;&#37325;&#35201;&#27493;&#39588;&#26159;&#22522;&#20110;&#20256;&#24863;&#22120;&#36755;&#20837;&#23545;&#36710;&#36742;&#29615;&#22659;&#36827;&#34892;&#24314;&#27169;&#12290;&#30001;&#20110;&#20854;&#20247;&#25152;&#21608;&#30693;&#30340;&#20248;&#21183;&#65292;&#38647;&#36798;&#25104;&#20026;&#25512;&#26029;&#22260;&#32469;&#36710;&#36742;&#30340;&#32593;&#26684;&#21333;&#20803;&#21344;&#29992;&#29366;&#24577;&#30340;&#27969;&#34892;&#36873;&#25321;&#12290;&#20026;&#20102;&#35299;&#20915;&#38647;&#36798;&#26816;&#27979;&#25968;&#25454;&#31232;&#30095;&#24615;&#21644;&#22122;&#22768;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36870;&#21521;&#20256;&#24863;&#22120;&#27169;&#22411;&#65288;ISM&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#20174;&#31232;&#30095;&#38647;&#36798;&#26816;&#27979;&#21040;&#26497;&#22352;&#26631;&#27979;&#37327;&#32593;&#26684;&#30340;&#26144;&#23556;&#12290;&#25913;&#36827;&#30340;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#27979;&#37327;&#30340;&#32593;&#26684;&#29992;&#20316;&#21442;&#32771;&#12290;&#23398;&#20064;&#21040;&#30340;&#38647;&#36798;&#27979;&#37327;&#32593;&#26684;&#19982;&#38647;&#36798;&#22810;&#26222;&#21202;&#36895;&#24230;&#27979;&#37327;&#30456;&#32467;&#21512;&#65292;&#36827;&#19968;&#27493;&#29992;&#20110;&#29983;&#25104;&#21160;&#24577;&#32593;&#26684;&#22320;&#22270;&#65288;DGM&#65289;&#12290;&#22312;&#23454;&#38469;&#30340;&#39640;&#36895;&#20844;&#36335;&#24773;&#26223;&#23454;&#39564;&#20013;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#25163;&#24037;&#21046;&#20316;&#30340;&#20960;&#20309;ISM&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#31532;&#19968;&#20010;&#20174;&#26377;&#38480;&#35270;&#22330;&#65288;FOV&#65289;&#30340;&#38647;&#36798;&#20013;&#23398;&#20064;&#26497;&#22352;&#26631;&#26041;&#26696;&#30340;&#21333;&#24103;&#27979;&#37327;&#32593;&#26684;&#30340;&#26041;&#27861;&#12290;&#23398;&#20064;&#26694;&#26550;&#20351;&#23398;&#20064;&#21040;&#30340;ISM&#21487;&#20197;&#30452;&#25509;&#23884;&#20837;&#21040;&#29616;&#26377;&#30340;&#36125;&#21494;&#26031;&#29366;&#24577;&#20272;&#35745;&#26041;&#26696;&#20013;&#65292;&#20197;&#25552;&#39640;&#29615;&#22659;&#24314;&#27169;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To implement autonomous driving, one essential step is to model the vehicle environment based on the sensor inputs. Radars, with their well-known advantages, became a popular option to infer the occupancy state of grid cells surrounding the vehicle. To tackle data sparsity and noise of radar detections, we propose a deep learning-based Inverse Sensor Model (ISM) to learn the mapping from sparse radar detections to polar measurement grids. Improved lidar-based measurement grids are used as reference. The learned radar measurement grids, combined with radar Doppler velocity measurements, are further used to generate a Dynamic Grid Map (DGM). Experiments in real-world highway scenarios show that our approach outperforms the hand-crafted geometric ISMs. In comparison to state-of-the-art deep learning methods, our approach is the first one to learn a single-frame measurement grid in the polar scheme from radars with a limited Field Of View (FOV). The learning framework makes the learned ISM
&lt;/p&gt;</description></item></channel></rss>