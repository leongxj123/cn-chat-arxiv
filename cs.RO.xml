<rss version="2.0"><channel><title>Chat Arxiv cs.RO</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.RO</description><item><title>&#36890;&#36807;&#26465;&#20214;&#35270;&#39057;&#25193;&#25955;&#23398;&#20064;&#22870;&#21169;&#65292;&#35299;&#20915;&#22797;&#26434;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#20219;&#21153;&#25104;&#21151;&#29575;&#65292;&#36229;&#36234;&#20102;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2312.14134</link><description>&lt;p&gt;
&#36890;&#36807;&#26465;&#20214;&#35270;&#39057;&#25193;&#25955;&#23398;&#20064;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;
Diffusion Reward: Learning Rewards via Conditional Video Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14134
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26465;&#20214;&#35270;&#39057;&#25193;&#25955;&#23398;&#20064;&#22870;&#21169;&#65292;&#35299;&#20915;&#22797;&#26434;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#20219;&#21153;&#25104;&#21151;&#29575;&#65292;&#36229;&#36234;&#20102;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#19987;&#23478;&#35270;&#39057;&#20013;&#23398;&#20064;&#22870;&#21169;&#20026;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#25351;&#23450;&#39044;&#26399;&#34892;&#20026;&#25552;&#20379;&#20102;&#19968;&#31181;&#32463;&#27982;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Diffusion Reward&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#26465;&#20214;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#20174;&#19987;&#23478;&#35270;&#39057;&#20013;&#23398;&#20064;&#22870;&#21169;&#20197;&#35299;&#20915;&#22797;&#26434;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#22312;&#19987;&#23478;&#36712;&#36857;&#30340;&#26465;&#20214;&#19979;&#35266;&#23519;&#21040;&#36739;&#20302;&#30340;&#29983;&#25104;&#22810;&#26679;&#24615;&#12290;&#22240;&#27492;&#65292;Diffusion Reward&#34987;&#24418;&#24335;&#21270;&#20026;&#36127;&#30340;&#26465;&#20214;&#29109;&#65292;&#40723;&#21169;&#19987;&#23478;&#24335;&#34892;&#20026;&#30340;&#26377;&#25928;&#25506;&#32034;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;MetaWorld&#21644;Adroit&#30340;10&#20010;&#26426;&#22120;&#20154;&#25805;&#32437;&#20219;&#21153;&#20013;&#20197;&#35270;&#35273;&#36755;&#20837;&#21644;&#31232;&#30095;&#22870;&#21169;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;Diffusion Reward&#29978;&#33267;&#33021;&#22815;&#25104;&#21151;&#26377;&#25928;&#22320;&#35299;&#20915;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#65292;&#22823;&#22823;&#36229;&#36234;&#20102;&#22522;&#32447;&#26041;&#27861;&#12290;&#39033;&#30446;&#39029;&#38754;&#21644;&#20195;&#30721;&#65306;https://diffusion-reward.github.io/&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14134v2 Announce Type: replace  Abstract: Learning rewards from expert videos offers an affordable and effective solution to specify the intended behaviors for reinforcement learning tasks. In this work, we propose Diffusion Reward, a novel framework that learns rewards from expert videos via conditional video diffusion models for solving complex visual RL problems. Our key insight is that lower generative diversity is observed when conditioned on expert trajectories. Diffusion Reward is accordingly formalized by the negative of conditional entropy that encourages productive exploration of expert-like behaviors. We show the efficacy of our method over 10 robotic manipulation tasks from MetaWorld and Adroit with visual input and sparse reward. Moreover, Diffusion Reward could even solve unseen tasks successfully and effectively, largely surpassing baseline methods. Project page and code: https://diffusion-reward.github.io/.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;CRAMP&#65292;&#19968;&#31181;&#22522;&#20110;&#22686;&#24378;&#24335;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#30340;&#20247;&#21253;&#24863;&#30693;&#20998;&#25955;&#24335;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#25317;&#25380;&#29615;&#22659;&#19979;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2309.10275</link><description>&lt;p&gt;
&#20855;&#26377;&#22686;&#24378;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#30340;&#20247;&#21253;&#24863;&#30693;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Crowd-Aware Multi-Agent Pathfinding With Boosted Curriculum Reinforcement Learning. (arXiv:2309.10275v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10275
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;CRAMP&#65292;&#19968;&#31181;&#22522;&#20110;&#22686;&#24378;&#24335;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#30340;&#20247;&#21253;&#24863;&#30693;&#20998;&#25955;&#24335;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#25317;&#25380;&#29615;&#22659;&#19979;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25317;&#25380;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36816;&#21160;&#35268;&#21010;&#38382;&#39064;&#65292;&#26088;&#22312;&#20026;&#31995;&#32479;&#20013;&#30340;&#25152;&#26377;&#26234;&#33021;&#20307;&#25214;&#21040;&#26080;&#30896;&#25758;&#36335;&#24452;&#12290;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#37117;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#31354;&#20013;&#32676;&#20307;&#12289;&#33258;&#21160;&#21270;&#20179;&#20648;&#26426;&#22120;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#12290;&#24403;&#21069;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#21487;&#20197;&#22823;&#33268;&#20998;&#20026;&#20004;&#31181;&#20027;&#35201;&#31867;&#21035;&#65306;&#38598;&#20013;&#24335;&#35268;&#21010;&#21644;&#20998;&#25955;&#24335;&#35268;&#21010;&#12290;&#38598;&#20013;&#24335;&#35268;&#21010;&#21463;&#21040;&#32500;&#24230;&#28798;&#38590;&#30340;&#22256;&#25200;&#65292;&#22240;&#27492;&#22312;&#22823;&#22411;&#21644;&#22797;&#26434;&#29615;&#22659;&#20013;&#19981;&#20855;&#22791;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20998;&#25955;&#24335;&#35268;&#21010;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#36827;&#34892;&#23454;&#26102;&#36335;&#24452;&#35268;&#21010;&#65292;&#23637;&#31034;&#20102;&#38544;&#24335;&#30340;&#21327;&#35843;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#23494;&#38598;&#29615;&#22659;&#20013;&#23427;&#20204;&#30340;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#19988;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CRAMP&#30340;&#20247;&#21253;&#24863;&#30693;&#20998;&#25955;&#24335;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#24335;&#35838;&#31243;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Path Finding (MAPF) in crowded environments presents a challenging problem in motion planning, aiming to find collision-free paths for all agents in the system. MAPF finds a wide range of applications in various domains, including aerial swarms, autonomous warehouse robotics, and self-driving vehicles. The current approaches for MAPF can be broadly categorized into two main categories: centralized and decentralized planning. Centralized planning suffers from the curse of dimensionality and thus does not scale well in large and complex environments. On the other hand, decentralized planning enables agents to engage in real-time path planning within a partially observable environment, demonstrating implicit coordination. However, they suffer from slow convergence and performance degradation in dense environments. In this paper, we introduce CRAMP, a crowd-aware decentralized approach to address this problem by leveraging reinforcement learning guided by a boosted curriculum-b
&lt;/p&gt;</description></item></channel></rss>