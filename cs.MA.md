# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Learning Progress Driven Multi-Agent Curriculum](https://arxiv.org/abs/2205.10016) | 提出了自主式MARL（SPMARL）以解决当前多智能体强化学习中课程生成的问题，优先考虑基于任务的优先级。 |

# 详细

[^1]: 学习进度驱动的多智能体课程

    Learning Progress Driven Multi-Agent Curriculum

    [https://arxiv.org/abs/2205.10016](https://arxiv.org/abs/2205.10016)

    提出了自主式MARL（SPMARL）以解决当前多智能体强化学习中课程生成的问题，优先考虑基于任务的优先级。

    

    课程强化学习（CRL）旨在通过逐渐增加任务的难度（通常由可实现的预期回报量化）来加快学习速度。受CRL在单智能体环境中的成功启发，一些研究尝试将CRL应用于多智能体强化学习（MARL），使用智能体数量来控制任务难度。然而，现有的工作通常使用手动定义的课程，如线性方案。本文首先将最先进的单智能体自主式CRL应用于稀疏奖励MARL。虽然表现令人满意，但我们确定了现有基于奖励的CRL方法生成的课程存在两个潜在缺陷：（1）高回报的任务可能不提供信息量大的学习信号，（2）在多智能体产生更高回报的任务中，加剧了学分分配困难。因此，我们进一步提出了自主式MARL（SPMARL），以基于任务的优先级进行安排。

    arXiv:2205.10016v2 Announce Type: replace  Abstract: Curriculum reinforcement learning (CRL) aims to speed up learning by gradually increasing the difficulty of a task, usually quantified by the achievable expected return. Inspired by the success of CRL in single-agent settings, a few works have attempted to apply CRL to multi-agent reinforcement learning (MARL) using the number of agents to control task difficulty. However, existing works typically use manually defined curricula such as a linear scheme. In this paper, we first apply state-of-the-art single-agent self-paced CRL to sparse reward MARL. Although with satisfying performance, we identify two potential flaws of the curriculum generated by existing reward-based CRL methods: (1) tasks with high returns may not provide informative learning signals and (2) the exacerbated credit assignment difficulty in tasks where more agents yield higher returns. Thereby, we further propose self-paced MARL (SPMARL) to prioritize tasks based on
    

