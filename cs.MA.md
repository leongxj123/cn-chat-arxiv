# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Policy Optimization finds Nash Equilibrium in Regularized General-Sum LQ Games](https://arxiv.org/abs/2404.00045) | 引入相对熵正则化对一般和总 $N$-agent 游戏的纳什均衡产生影响，证明了NE符合线性高斯策略，并提出了政策优化算法以及增强技术来找到游戏内的NE。 |

# 详细

[^1]: 政策优化在正则化广义和总 LQ 游戏中找到纳什均衡

    Policy Optimization finds Nash Equilibrium in Regularized General-Sum LQ Games

    [https://arxiv.org/abs/2404.00045](https://arxiv.org/abs/2404.00045)

    引入相对熵正则化对一般和总 $N$-agent 游戏的纳什均衡产生影响，证明了NE符合线性高斯策略，并提出了政策优化算法以及增强技术来找到游戏内的NE。

    

    在本文中，我们研究了引入相对熵正则化对一般和总 $N$-agent 游戏的纳什均衡 (NE) 的影响，揭示了这类游戏的NE符合线性高斯策略的事实。此外，它描绘了在熵正则化的适当性方面，对游戏内NE独特性的充分条件。由于政策优化是强化学习 (RL) 技术的基础方法，旨在找到 NE，在这项工作中，我们证明了一个政策优化算法的线性收敛性，该算法 (在熵正则化的适当性下) 能够明显地实现 NE。此外，在熵正则化证明不足的情况下，我们提出了一个 $\delta$-增强技术，有助于实现游戏内的 $\epsilon$-NE。

    arXiv:2404.00045v1 Announce Type: cross  Abstract: In this paper, we investigate the impact of introducing relative entropy regularization on the Nash Equilibria (NE) of General-Sum $N$-agent games, revealing the fact that the NE of such games conform to linear Gaussian policies. Moreover, it delineates sufficient conditions, contingent upon the adequacy of entropy regularization, for the uniqueness of the NE within the game. As Policy Optimization serves as a foundational approach for Reinforcement Learning (RL) techniques aimed at finding the NE, in this work we prove the linear convergence of a policy optimization algorithm which (subject to the adequacy of entropy regularization) is capable of provably attaining the NE. Furthermore, in scenarios where the entropy regularization proves insufficient, we present a $\delta$-augmentation technique, which facilitates the achievement of an $\epsilon$-NE within the game.
    

