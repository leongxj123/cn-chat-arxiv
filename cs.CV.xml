<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>SportsNGEN&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#65292;&#32463;&#36807;&#35757;&#32451;&#33021;&#25345;&#32493;&#29983;&#25104;&#36924;&#30495;&#30340;&#22810;&#20154;&#20307;&#32946;&#28216;&#25103;&#65292;&#21253;&#25324;&#27169;&#25311;&#25972;&#20010;&#32593;&#29699;&#27604;&#36187;&#21644;&#20026;&#25945;&#32451;&#21644;&#24191;&#25773;&#21592;&#25552;&#20379;&#27934;&#23519;&#21147;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.12977</link><description>&lt;p&gt;
SportsNGEN: &#25345;&#32493;&#29983;&#25104;&#22810;&#20154;&#20307;&#32946;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
SportsNGEN: Sustained Generation of Multi-player Sports Gameplay
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12977
&lt;/p&gt;
&lt;p&gt;
SportsNGEN&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#65292;&#32463;&#36807;&#35757;&#32451;&#33021;&#25345;&#32493;&#29983;&#25104;&#36924;&#30495;&#30340;&#22810;&#20154;&#20307;&#32946;&#28216;&#25103;&#65292;&#21253;&#25324;&#27169;&#25311;&#25972;&#20010;&#32593;&#29699;&#27604;&#36187;&#21644;&#20026;&#25945;&#32451;&#21644;&#24191;&#25773;&#21592;&#25552;&#20379;&#27934;&#23519;&#21147;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;SportsNGEN&#65292;&#35813;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#20351;&#29992;&#36816;&#21160;&#21592;&#21644;&#29699;&#36861;&#36394;&#24207;&#21015;&#65292;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#19988;&#25345;&#32493;&#30340;&#28216;&#25103;&#22330;&#26223;&#12290;&#25105;&#20204;&#22312;&#22823;&#37327;&#19987;&#19994;&#32593;&#29699;&#36861;&#36394;&#25968;&#25454;&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;SportsNGEN&#65292;&#24182;&#23637;&#31034;&#36890;&#36807;&#23558;&#29983;&#25104;&#30340;&#27169;&#25311;&#19982;&#23556;&#20987;&#20998;&#31867;&#22120;&#21644;&#36923;&#36753;&#30456;&#32467;&#21512;&#26469;&#24320;&#22987;&#21644;&#32467;&#26463;&#29699;&#36187;&#65292;&#31995;&#32479;&#33021;&#22815;&#27169;&#25311;&#25972;&#20010;&#32593;&#29699;&#27604;&#36187;&#12290;&#27492;&#22806;&#65292;SportsNGEN&#30340;&#36890;&#29992;&#29256;&#26412;&#21487;&#20197;&#36890;&#36807;&#22312;&#21253;&#21547;&#35813;&#29699;&#21592;&#30340;&#27604;&#36187;&#25968;&#25454;&#19978;&#24494;&#35843;&#26469;&#23450;&#21046;&#29305;&#23450;&#29699;&#21592;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#32463;&#36807;&#33391;&#22909;&#26657;&#20934;&#65292;&#21487;&#20197;&#36890;&#36807;&#35780;&#20272;&#21453;&#20107;&#23454;&#25110;&#20551;&#35774;&#36873;&#39033;&#20026;&#25945;&#32451;&#21644;&#24191;&#25773;&#21592;&#25552;&#20379;&#27934;&#23519;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36136;&#37327;&#32467;&#26524;&#34920;&#26126;&#30456;&#21516;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#36275;&#29699;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12977v1 Announce Type: cross  Abstract: We present a transformer decoder based model, SportsNGEN, that is trained on sports player and ball tracking sequences that is capable of generating realistic and sustained gameplay. We train and evaluate SportsNGEN on a large database of professional tennis tracking data and demonstrate that by combining the generated simulations with a shot classifier and logic to start and end rallies, the system is capable of simulating an entire tennis match. In addition, a generic version of SportsNGEN can be customized to a specific player by fine-tuning on match data that includes that player. We show that our model is well calibrated and can be used to derive insights for coaches and broadcasters by evaluating counterfactual or what if options. Finally, we show qualitative results indicating the same approach works for football.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#27493;&#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#19968;&#33268;&#24615;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#20043;&#38388;&#25554;&#20540;&#65292;&#23454;&#29616;&#20102;&#37319;&#26679;&#36895;&#24230;&#21644;&#37319;&#26679;&#36136;&#37327;&#30340;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2403.06807</link><description>&lt;p&gt;
&#22810;&#27493;&#19968;&#33268;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Multistep Consistency Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#27493;&#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#19968;&#33268;&#24615;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#20043;&#38388;&#25554;&#20540;&#65292;&#23454;&#29616;&#20102;&#37319;&#26679;&#36895;&#24230;&#21644;&#37319;&#26679;&#36136;&#37327;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30456;&#23545;&#23481;&#26131;&#35757;&#32451;&#65292;&#20294;&#29983;&#25104;&#26679;&#26412;&#38656;&#35201;&#35768;&#22810;&#27493;&#39588;&#12290;&#19968;&#33268;&#24615;&#27169;&#22411;&#26356;&#38590;&#35757;&#32451;&#65292;&#20294;&#21487;&#20197;&#22312;&#19968;&#20010;&#27493;&#39588;&#20013;&#29983;&#25104;&#26679;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#27493;&#19968;&#33268;&#24615;&#27169;&#22411;&#65306;&#36890;&#36807;&#19968;&#33268;&#24615;&#27169;&#22411;&#21644;TRACT&#30340;&#32479;&#19968;&#65292;&#21487;&#20197;&#22312;&#19968;&#33268;&#24615;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#20043;&#38388;&#36827;&#34892;&#25554;&#20540;&#65306;&#22312;&#37319;&#26679;&#36895;&#24230;&#21644;&#37319;&#26679;&#36136;&#37327;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;1&#27493;&#19968;&#33268;&#24615;&#27169;&#22411;&#26159;&#20256;&#32479;&#30340;&#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#32780;&#25105;&#20204;&#23637;&#31034;&#20102;$\infty$&#27493;&#19968;&#33268;&#24615;&#27169;&#22411;&#26159;&#25193;&#25955;&#27169;&#22411;&#12290;&#22810;&#27493;&#19968;&#33268;&#24615;&#27169;&#22411;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#23558;&#26679;&#26412;&#39044;&#31639;&#20174;&#21333;&#27493;&#22686;&#21152;&#21040;2-8&#27493;&#65292;&#25105;&#20204;&#21487;&#20197;&#26356;&#36731;&#26494;&#22320;&#35757;&#32451;&#27169;&#22411;&#65292;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#21516;&#26102;&#20445;&#30041;&#22823;&#37096;&#20998;&#37319;&#26679;&#36895;&#24230;&#20248;&#21183;&#12290;&#22312;Imagenet 64&#19978;8&#27493;&#36798;&#21040;1.4&#30340;FID&#65292;&#22312;Imagenet128&#19978;8&#27493;&#36798;&#21040;2.1&#30340;FID&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06807v1 Announce Type: new  Abstract: Diffusion models are relatively easy to train but require many steps to generate samples. Consistency models are far more difficult to train, but generate samples in a single step.   In this paper we propose Multistep Consistency Models: A unification between Consistency Models (Song et al., 2023) and TRACT (Berthelot et al., 2023) that can interpolate between a consistency model and a diffusion model: a trade-off between sampling speed and sampling quality. Specifically, a 1-step consistency model is a conventional consistency model whereas we show that a $\infty$-step consistency model is a diffusion model.   Multistep Consistency Models work really well in practice. By increasing the sample budget from a single step to 2-8 steps, we can train models more easily that generate higher quality samples, while retaining much of the sampling speed benefits. Notable results are 1.4 FID on Imagenet 64 in 8 step and 2.1 FID on Imagenet128 in 8 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#26368;&#26032;&#30340;&#22270;&#29255;&#32858;&#31867;&#21644;&#25193;&#25955;&#27169;&#22411;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32771;&#34385;&#26368;&#20339;&#32858;&#31867;&#31890;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20808;&#36827;FID&#24182;&#20855;&#26377;&#36739;&#24378;&#35757;&#32451;&#26679;&#26412;&#25928;&#29575;&#30340;&#32858;&#31867;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#26469;&#20943;&#23569;&#35270;&#35273;&#32452;&#25628;&#32034;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.00570</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22522;&#20110;&#32858;&#31867;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Rethinking cluster-conditioned diffusion models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00570
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#26368;&#26032;&#30340;&#22270;&#29255;&#32858;&#31867;&#21644;&#25193;&#25955;&#27169;&#22411;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32771;&#34385;&#26368;&#20339;&#32858;&#31867;&#31890;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20808;&#36827;FID&#24182;&#20855;&#26377;&#36739;&#24378;&#35757;&#32451;&#26679;&#26412;&#25928;&#29575;&#30340;&#32858;&#31867;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#26469;&#20943;&#23569;&#35270;&#35273;&#32452;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#38024;&#23545;&#20351;&#29992;&#32858;&#31867;&#20998;&#37197;&#30340;&#22270;&#29255;&#32423;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#30740;&#31350;&#12290;&#25105;&#20204;&#38416;&#26126;&#20102;&#20851;&#20110;&#22270;&#29255;&#32858;&#31867;&#30340;&#20010;&#21035;&#32452;&#20214;&#22914;&#20309;&#24433;&#21709;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#22270;&#29255;&#21512;&#25104;&#12290;&#36890;&#36807;&#32467;&#21512;&#22270;&#29255;&#32858;&#31867;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#22312;&#32771;&#34385;&#21040;&#22270;&#29255;&#21512;&#25104;&#65288;&#35270;&#35273;&#32452;&#65289;&#30340;&#26368;&#20339;&#31751;&#31890;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#32858;&#31867;&#26465;&#20214;&#21487;&#20197;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;FID&#65288;&#21363;&#22312;CIFAR10&#21644;CIFAR100&#19978;&#20998;&#21035;&#20026;1.67&#21644;2.17&#65289;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#36739;&#24378;&#30340;&#35757;&#32451;&#26679;&#26412;&#25928;&#29575;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#22522;&#20110;&#29305;&#24449;&#30340;&#32858;&#31867;&#26469;&#25512;&#23548;&#20943;&#23569;&#35270;&#35273;&#32452;&#25628;&#32034;&#31354;&#38388;&#30340;&#19978;&#38480;&#31751;&#36793;&#30028;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#21457;&#29616;&#32858;&#31867;&#19982;&#22522;&#20110;&#32858;&#31867;&#30340;&#22270;&#29255;&#29983;&#25104;&#20043;&#38388;&#27809;&#26377;&#26174;&#33879;&#32852;&#31995;&#12290;&#20195;&#30721;&#21644;&#32858;&#31867;&#20998;&#37197;&#23558;&#20250;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00570v1 Announce Type: cross  Abstract: We present a comprehensive experimental study on image-level conditioning for diffusion models using cluster assignments. We elucidate how individual components regarding image clustering impact image synthesis across three datasets. By combining recent advancements from image clustering and diffusion models, we show that, given the optimal cluster granularity with respect to image synthesis (visual groups), cluster-conditioning can achieve state-of-the-art FID (i.e. 1.67, 2.17 on CIFAR10 and CIFAR100 respectively), while attaining a strong training sample efficiency. Finally, we propose a novel method to derive an upper cluster bound that reduces the search space of the visual groups using solely feature-based clustering. Unlike existing approaches, we find no significant connection between clustering and cluster-conditional image generation. The code and cluster assignments will be released.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;SpikingNeRF&#65292;&#23427;&#36890;&#36807;&#23558;&#36752;&#23556;&#20809;&#32447;&#19982;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#32500;&#24230;&#23545;&#40784;&#65292;&#20197;&#33410;&#30465;&#33021;&#37327;&#24182;&#20943;&#23569;&#35745;&#31639;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.10987</link><description>&lt;p&gt;
Spiking NeRF&#65306;&#20351;&#29983;&#29289;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#31359;&#36879;&#29616;&#23454;&#19990;&#30028;
&lt;/p&gt;
&lt;p&gt;
Spiking NeRF: Making Bio-inspired Neural Networks See through the Real World. (arXiv:2309.10987v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SpikingNeRF&#65292;&#23427;&#36890;&#36807;&#23558;&#36752;&#23556;&#20809;&#32447;&#19982;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#32500;&#24230;&#23545;&#40784;&#65292;&#20197;&#33410;&#30465;&#33021;&#37327;&#24182;&#20943;&#23569;&#35745;&#31639;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#21033;&#29992;&#20854;&#20855;&#26377;&#28508;&#22312;&#29983;&#29289;&#23398;&#21487;&#34892;&#24615;&#30340;&#33021;&#37327;&#25928;&#29575;&#21644;&#28508;&#21147;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#20197;&#22823;&#37327;&#33021;&#37327;&#28040;&#32791;&#28210;&#26579;&#39640;&#36136;&#37327;&#30340;3D&#22330;&#26223;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#28145;&#20837;&#25506;&#32034;&#20197;&#29983;&#29289;&#21551;&#21457;&#30340;&#26041;&#27861;&#36827;&#34892;&#33410;&#33021;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#33033;&#20914;NeRF&#65288;SpikingNeRF&#65289;&#65292;&#23558;&#36752;&#23556;&#20809;&#32447;&#19982;SNN&#30340;&#26102;&#38388;&#32500;&#24230;&#23545;&#40784;&#65292;&#20197;&#33258;&#28982;&#22320;&#36866;&#24212;SNN&#23545;&#36752;&#23556;&#22330;&#30340;&#37325;&#24314;&#12290;&#22240;&#27492;&#65292;&#35745;&#31639;&#20197;&#22522;&#20110;&#33033;&#20914;&#12289;&#26080;&#20056;&#27861;&#30340;&#26041;&#24335;&#36827;&#34892;&#65292;&#20174;&#32780;&#20943;&#23569;&#33021;&#37327;&#28040;&#32791;&#12290;&#22312;SpikingNeRF&#20013;&#65292;&#20809;&#32447;&#19978;&#30340;&#27599;&#20010;&#37319;&#26679;&#28857;&#21305;&#37197;&#21040;&#29305;&#23450;&#30340;&#26102;&#38388;&#27493;&#65292;&#24182;&#20197;&#28151;&#21512;&#26041;&#24335;&#34920;&#31034;&#65292;&#20854;&#20013;&#20307;&#32032;&#32593;&#26684;&#20063;&#24471;&#21040;&#32500;&#25252;&#12290;&#22522;&#20110;&#20307;&#32032;&#32593;&#26684;&#65292;&#30830;&#23450;&#37319;&#26679;&#28857;&#26159;&#21542;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#36807;&#31243;&#20013;&#34987;&#23631;&#34109;&#20197;&#36827;&#34892;&#26356;&#22909;&#30340;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#25805;&#20316;&#20063;&#20250;&#20135;&#29983;&#19981;&#21487;&#36870;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neuron networks (SNNs) have been thriving on numerous tasks to leverage their promising energy efficiency and exploit their potentialities as biologically plausible intelligence. Meanwhile, the Neural Radiance Fields (NeRF) render high-quality 3D scenes with massive energy consumption, and few works delve into the energy-saving solution with a bio-inspired approach. In this paper, we propose spiking NeRF (SpikingNeRF), which aligns the radiance ray with the temporal dimension of SNN, to naturally accommodate the SNN to the reconstruction of Radiance Fields. Thus, the computation turns into a spike-based, multiplication-free manner, reducing the energy consumption. In SpikingNeRF, each sampled point on the ray is matched onto a particular time step, and represented in a hybrid manner where the voxel grids are maintained as well. Based on the voxel grids, sampled points are determined whether to be masked for better training and inference. However, this operation also incurs irre
&lt;/p&gt;</description></item><item><title>S-HR-VQVAE&#26159;&#19968;&#31181;&#24207;&#21015;&#20998;&#23618;&#27531;&#24046;&#23398;&#20064;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#32467;&#21512;&#20998;&#23618;&#27531;&#24046;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;HR-VQVAE&#65289;&#21644;&#26102;&#31354;PixelCNN&#65288;ST-PixelCNN&#65289;&#30340;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#35270;&#39057;&#39044;&#27979;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#22312;KTH&#20154;&#20307;&#21160;&#20316;&#21644;Moving-MNIST&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.06701</link><description>&lt;p&gt;
S-HR-VQVAE: &#24207;&#21015;&#20998;&#23618;&#27531;&#24046;&#23398;&#20064;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#35270;&#39057;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
S-HR-VQVAE: Sequential Hierarchical Residual Learning Vector Quantized Variational Autoencoder for Video Prediction. (arXiv:2307.06701v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06701
&lt;/p&gt;
&lt;p&gt;
S-HR-VQVAE&#26159;&#19968;&#31181;&#24207;&#21015;&#20998;&#23618;&#27531;&#24046;&#23398;&#20064;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#32467;&#21512;&#20998;&#23618;&#27531;&#24046;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;HR-VQVAE&#65289;&#21644;&#26102;&#31354;PixelCNN&#65288;ST-PixelCNN&#65289;&#30340;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#35270;&#39057;&#39044;&#27979;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#22312;KTH&#20154;&#20307;&#21160;&#20316;&#21644;Moving-MNIST&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#23558;&#25105;&#20204;&#26368;&#36817;&#25552;&#20986;&#30340;&#20998;&#23618;&#27531;&#24046;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;HR-VQVAE&#65289;&#19982;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#31354;PixelCNN&#65288;ST-PixelCNN&#65289;&#30456;&#32467;&#21512;&#65292;&#29992;&#26469;&#35299;&#20915;&#35270;&#39057;&#39044;&#27979;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;&#24207;&#21015;&#20998;&#23618;&#27531;&#24046;&#23398;&#20064;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;S-HR-VQVAE&#65289;&#12290;&#36890;&#36807;&#21033;&#29992;HR-VQVAE&#22312;&#23545;&#38745;&#27490;&#22270;&#20687;&#36827;&#34892;&#24314;&#27169;&#26102;&#30340;&#20869;&#22312;&#33021;&#21147;&#21644;&#32039;&#20945;&#34920;&#31034;&#65292;&#20197;&#21450;ST-PixelCNN&#22788;&#29702;&#26102;&#31354;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292; S-HR-VQVAE&#33021;&#22815;&#26356;&#22909;&#22320;&#24212;&#23545;&#35270;&#39057;&#39044;&#27979;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#21253;&#25324;&#23398;&#20064;&#26102;&#31354;&#20449;&#24687;&#12289;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#12289;&#28040;&#38500;&#27169;&#31946;&#39044;&#27979;&#21644;&#38544;&#24335;&#24314;&#27169;&#29289;&#29702;&#29305;&#24615;&#12290;&#23545;KTH&#20154;&#20307;&#21160;&#20316;&#21644;Moving-MNIST&#20219;&#21153;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#26041;&#38754;&#19982;&#39030;&#32423;&#35270;&#39057;&#39044;&#27979;&#25216;&#26415;&#30456;&#27604;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the video prediction task by putting forth a novel model that combines (i) our recently proposed hierarchical residual vector quantized variational autoencoder (HR-VQVAE), and (ii) a novel spatiotemporal PixelCNN (ST-PixelCNN). We refer to this approach as a sequential hierarchical residual learning vector quantized variational autoencoder (S-HR-VQVAE). By leveraging the intrinsic capabilities of HR-VQVAE at modeling still images with a parsimonious representation, combined with the ST-PixelCNN's ability at handling spatiotemporal information, S-HR-VQVAE can better deal with chief challenges in video prediction. These include learning spatiotemporal information, handling high dimensional data, combating blurry prediction, and implicit modeling of physical characteristics. Extensive experimental results on the KTH Human Action and Moving-MNIST tasks demonstrate that our model compares favorably against top video prediction techniques both in quantitative and qualitative evalu
&lt;/p&gt;</description></item></channel></rss>