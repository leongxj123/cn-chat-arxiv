<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>CLEVRER-Humans&#26159;&#19968;&#20010;&#29992;&#20110;&#22240;&#26524;&#21028;&#26029;&#30340;&#35270;&#39057;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20154;&#24037;&#26631;&#27880;&#26469;&#35299;&#20915;&#21512;&#25104;&#20107;&#20214;&#21644;&#21512;&#25104;&#35821;&#35328;&#25551;&#36848;&#30340;&#32570;&#20047;&#22810;&#26679;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20107;&#20214;&#22635;&#31354;&#21644;&#31070;&#32463;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#25552;&#39640;&#25968;&#25454;&#25910;&#38598;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.03635</link><description>&lt;p&gt;
CLEVRER-Humans: &#29992;&#20154;&#31867;&#30340;&#26041;&#24335;&#25551;&#36848;&#29289;&#29702;&#21644;&#22240;&#26524;&#20107;&#20214;
&lt;/p&gt;
&lt;p&gt;
CLEVRER-Humans: Describing Physical and Causal Events the Human Way. (arXiv:2310.03635v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03635
&lt;/p&gt;
&lt;p&gt;
CLEVRER-Humans&#26159;&#19968;&#20010;&#29992;&#20110;&#22240;&#26524;&#21028;&#26029;&#30340;&#35270;&#39057;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20154;&#24037;&#26631;&#27880;&#26469;&#35299;&#20915;&#21512;&#25104;&#20107;&#20214;&#21644;&#21512;&#25104;&#35821;&#35328;&#25551;&#36848;&#30340;&#32570;&#20047;&#22810;&#26679;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20107;&#20214;&#22635;&#31354;&#21644;&#31070;&#32463;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#25552;&#39640;&#25968;&#25454;&#25910;&#38598;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#33021;&#22815;&#25512;&#29702;&#29289;&#29702;&#20107;&#20214;&#21450;&#20854;&#22240;&#26524;&#20851;&#31995;&#30340;&#26426;&#22120;&#23545;&#20110;&#19982;&#29289;&#29702;&#19990;&#30028;&#36827;&#34892;&#28789;&#27963;&#20114;&#21160;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#29289;&#29702;&#21644;&#22240;&#26524;&#25512;&#29702;&#22522;&#20934;&#37117;&#20165;&#22522;&#20110;&#21512;&#25104;&#20107;&#20214;&#21644;&#21512;&#25104;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#36825;&#31181;&#35774;&#35745;&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;&#65306;&#19968;&#26159;&#20107;&#20214;&#31867;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#32570;&#20047;&#22810;&#26679;&#24615;&#65307;&#20108;&#26159;&#22522;&#20110;&#25163;&#21160;&#23450;&#20041;&#30340;&#21551;&#21457;&#24335;&#35268;&#21017;&#30340;&#22240;&#26524;&#20851;&#31995;&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CLEVRER-Humans&#22522;&#20934;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20154;&#24037;&#26631;&#27880;&#30340;&#35270;&#39057;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#23545;&#29289;&#29702;&#20107;&#20214;&#30340;&#22240;&#26524;&#21028;&#26029;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#20004;&#31181;&#25216;&#26415;&#26469;&#25552;&#39640;&#25968;&#25454;&#25910;&#38598;&#25928;&#29575;&#65306;&#39318;&#20808;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#36845;&#20195;&#20107;&#20214;&#22635;&#31354;&#20219;&#21153;&#65292;&#20197; eliciting &#35270;&#39057;&#20013;&#20107;&#20214;&#30340;&#26032;&#34920;&#31034;&#26041;&#24335;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#22240;&#26524;&#20107;&#20214;&#22270; (CEGs)&#65307;&#20854;&#27425;&#65292;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building machines that can reason about physical events and their causal relationships is crucial for flexible interaction with the physical world. However, most existing physical and causal reasoning benchmarks are exclusively based on synthetically generated events and synthetic natural language descriptions of causal relationships. This design brings up two issues. First, there is a lack of diversity in both event types and natural language descriptions; second, causal relationships based on manually-defined heuristics are different from human judgments. To address both shortcomings, we present the CLEVRER-Humans benchmark, a video reasoning dataset for causal judgment of physical events with human labels. We employ two techniques to improve data collection efficiency: first, a novel iterative event cloze task to elicit a new representation of events in videos, which we term Causal Event Graphs (CEGs); second, a data augmentation technique based on neural language generative models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#27169;&#22411;&#21387;&#32553;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#36731;&#37327;&#32423;&#25945;&#24072;&#27169;&#22411;&#20013;&#25552;&#21462;&#24402;&#32435;&#20559;&#24046;&#65292;&#20351;Vision Transformers (ViTs) &#30340;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#21253;&#25324;&#20351;&#29992;&#19968;&#32452;&#19981;&#21516;&#26550;&#26500;&#30340;&#25945;&#24072;&#27169;&#22411;&#26469;&#25351;&#23548;&#23398;&#29983;Transformer&#65292;&#20174;&#32780;&#26377;&#25928;&#25552;&#39640;&#23398;&#29983;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.00369</link><description>&lt;p&gt;
&#25552;&#28860;&#24402;&#32435;&#20559;&#24046;&#65306;&#36229;&#36234;&#27169;&#22411;&#21387;&#32553;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Distilling Inductive Bias: Knowledge Distillation Beyond Model Compression. (arXiv:2310.00369v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#27169;&#22411;&#21387;&#32553;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#36731;&#37327;&#32423;&#25945;&#24072;&#27169;&#22411;&#20013;&#25552;&#21462;&#24402;&#32435;&#20559;&#24046;&#65292;&#20351;Vision Transformers (ViTs) &#30340;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#21253;&#25324;&#20351;&#29992;&#19968;&#32452;&#19981;&#21516;&#26550;&#26500;&#30340;&#25945;&#24072;&#27169;&#22411;&#26469;&#25351;&#23548;&#23398;&#29983;Transformer&#65292;&#20174;&#32780;&#26377;&#25928;&#25552;&#39640;&#23398;&#29983;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;Vision Transformers (ViTs) &#25552;&#20379;&#20102;&#22312;&#35270;&#35273;&#21644;&#25991;&#26412;&#39046;&#22495;&#20013;&#23454;&#29616;&#32479;&#19968;&#20449;&#24687;&#22788;&#29702;&#30340;&#35825;&#20154;&#21069;&#26223;&#12290;&#20294;&#26159;&#30001;&#20110;ViTs&#32570;&#20047;&#22266;&#26377;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#20026;&#20102;&#20351;&#23427;&#20204;&#30340;&#24212;&#29992;&#23454;&#38469;&#21487;&#34892;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#38598;&#25104;&#30340;&#33976;&#39311;&#26041;&#27861;&#65292;&#20174;&#36731;&#37327;&#32423;&#30340;&#25945;&#24072;&#27169;&#22411;&#20013;&#25552;&#21462;&#24402;&#32435;&#20559;&#24046;&#12290;&#20197;&#21069;&#30340;&#31995;&#32479;&#20165;&#20381;&#38752;&#22522;&#20110;&#21367;&#31215;&#30340;&#25945;&#23398;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23558;&#19968;&#32452;&#20855;&#26377;&#19981;&#21516;&#26550;&#26500;&#20542;&#21521;&#30340;&#36731;&#37327;&#32423;&#25945;&#24072;&#27169;&#22411;&#65288;&#20363;&#22914;&#21367;&#31215;&#21644;&#38750;&#32447;&#24615;&#21367;&#31215;&#65289;&#21516;&#26102;&#29992;&#20110;&#25351;&#23548;&#23398;&#29983;Transformer&#12290;&#30001;&#20110;&#36825;&#20123;&#29420;&#29305;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#25945;&#24072;&#27169;&#22411;&#21487;&#20197;&#20174;&#21508;&#31181;&#23384;&#20648;&#25968;&#25454;&#38598;&#20013;&#33719;&#24471;&#24191;&#27867;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#29983;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#36824;&#28041;&#21450;&#39044;&#20808;&#35745;&#31639;&#21644;&#23384;&#20648;logits&#65292;&#20174;&#26681;&#26412;&#19978;&#23454;&#29616;&#20102;&#38750;&#24402;&#19968;&#21270;&#30340;&#29366;&#24577;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of computer vision, Vision Transformers (ViTs) offer the tantalizing prospect of unified information processing across visual and textual domains. But due to the lack of inherent inductive biases in ViTs, they require enormous amount of data for training. To make their applications practical, we introduce an innovative ensemble-based distillation approach distilling inductive bias from complementary lightweight teacher models. Prior systems relied solely on convolution-based teaching. However, this method incorporates an ensemble of light teachers with different architectural tendencies, such as convolution and involution, to instruct the student transformer jointly. Because of these unique inductive biases, instructors can accumulate a wide range of knowledge, even from readily identifiable stored datasets, which leads to enhanced student performance. Our proposed framework also involves precomputing and storing logits in advance, essentially the unnormalize
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31639;&#27861;&#21644;&#24037;&#20855;&#65292;&#21487;&#20197;&#20026;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#35745;&#31639;&#22810;&#20010;&#35299;&#37322;&#65292;&#20174;&#32780;&#25552;&#39640;&#23545;&#20998;&#31867;&#22120;&#34892;&#20026;&#30340;&#27934;&#23519;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.14309</link><description>&lt;p&gt;
&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#22810;&#20010;&#19981;&#21516;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Multiple Different Explanations for Image Classifiers. (arXiv:2309.14309v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14309
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31639;&#27861;&#21644;&#24037;&#20855;&#65292;&#21487;&#20197;&#20026;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#35745;&#31639;&#22810;&#20010;&#35299;&#37322;&#65292;&#20174;&#32780;&#25552;&#39640;&#23545;&#20998;&#31867;&#22120;&#34892;&#20026;&#30340;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#35299;&#37322;&#24037;&#20855;&#36890;&#24120;&#21482;&#20250;&#32473;&#20986;&#19968;&#31181;&#23545;&#20110;&#22270;&#20687;&#30340;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#22270;&#20687;&#26469;&#35828;&#65292;&#26080;&#35770;&#26159;&#20154;&#31867;&#36824;&#26159;&#22270;&#20687;&#20998;&#31867;&#22120;&#37117;&#25509;&#21463;&#22810;&#20010;&#35299;&#37322;&#26469;&#35299;&#37322;&#22270;&#20687;&#26631;&#31614;&#12290;&#22240;&#27492;&#65292;&#38480;&#21046;&#35299;&#37322;&#30340;&#25968;&#37327;&#21482;&#26377;&#19968;&#20010;&#20005;&#37325;&#38480;&#21046;&#20102;&#23545;&#20998;&#31867;&#22120;&#34892;&#20026;&#30340;&#27934;&#23519;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#31639;&#27861;&#21644;&#24037;&#20855;REX&#65292;&#29992;&#20110;&#35745;&#31639;&#40657;&#30418;&#22270;&#20687;&#20998;&#31867;&#22120;&#23545;&#32473;&#23450;&#22270;&#20687;&#30340;&#36755;&#20986;&#30340;&#22810;&#20010;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22522;&#20110;&#22240;&#26524;&#29702;&#35770;&#30340;&#21487;&#38752;&#26041;&#27861;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20854;&#29702;&#35770;&#22797;&#26434;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#39564;&#32467;&#26524;&#65292;&#26174;&#31034;REX&#22312;ImageNet-mini&#22522;&#20934;&#27979;&#35797;&#20013;&#25214;&#21040;&#30340;&#22810;&#20010;&#35299;&#37322;&#27604;&#20043;&#21069;&#30340;&#24037;&#20316;&#22810;7&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing explanation tools for image classifiers usually give only one single explanation for an image. For many images, however, both humans and image classifiers accept more than one explanation for the image label. Thus, restricting the number of explanations to just one severely limits the insight into the behavior of the classifier. In this paper, we describe an algorithm and a tool, REX, for computing multiple explanations of the output of a black-box image classifier for a given image. Our algorithm uses a principled approach based on causal theory. We analyse its theoretical complexity and provide experimental results showing that REX finds multiple explanations on 7 times more images than the previous work on the ImageNet-mini benchmark.
&lt;/p&gt;</description></item></channel></rss>