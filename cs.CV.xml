<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#39318;&#27425;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#37327;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#21322;&#30417;&#30563;&#28145;&#24230;&#22240;&#26524;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2403.18717</link><description>&lt;p&gt;
&#28145;&#24230;&#22240;&#26524;&#29983;&#25104;&#27169;&#22411;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Learning for Deep Causal Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18717
&lt;/p&gt;
&lt;p&gt;
&#39318;&#27425;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#37327;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#21322;&#30417;&#30563;&#28145;&#24230;&#22240;&#26524;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#33021;&#22815;&#22238;&#31572;&#8220;&#22914;&#26524;$y$&#21464;&#20026;$z$&#65292;$x$&#20250;&#22914;&#20309;&#21464;&#21270;&#65311;&#8221;&#36825;&#31867;&#38382;&#39064;&#30340;&#27169;&#22411;&#23545;&#20110;&#25512;&#21160;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#33021;&#22815;&#35299;&#20915;&#36825;&#31867;&#21453;&#20107;&#23454;&#38382;&#39064;&#30340;&#22240;&#26524;&#29983;&#25104;&#27169;&#22411;&#30446;&#21069;&#35201;&#27714;&#25152;&#26377;&#30456;&#20851;&#21464;&#37327;&#22343;&#24050;&#34987;&#35266;&#23519;&#21040;&#65292;&#24182;&#19988;&#30456;&#24212;&#30340;&#26631;&#31614;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#21487;&#29992;&#12290;&#25105;&#20204;&#39318;&#27425;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#37327;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#21322;&#30417;&#30563;&#28145;&#24230;&#22240;&#26524;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18717v1 Announce Type: cross  Abstract: Developing models that can answer questions of the form "How would $x$ change if $y$ had been $z$?" is fundamental for advancing medical image analysis. Training causal generative models that address such counterfactual questions, though, currently requires that all relevant variables have been observed and that corresponding labels are available in training data. However, clinical data may not have complete records for all patients and state of the art causal generative models are unable to take full advantage of this. We thus develop, for the first time, a semi-supervised deep causal generative model that exploits the causal relationships between variables to maximise the use of all available data. We explore this in the setting where each sample is either fully labelled or fully unlabelled, as well as the more clinically realistic case of having different labels missing for each sample. We leverage techniques from causal inference t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#31354;&#38388;&#24863;&#30693;3D&#24418;&#29366;&#29983;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;2D&#24179;&#38754;&#34920;&#31034;&#22686;&#24378;&#24314;&#27169;&#65292;&#24182;&#32467;&#21512;&#28151;&#21512;&#24418;&#29366;&#34920;&#31034;&#25216;&#26415;&#30452;&#25509;&#23398;&#20064;&#36830;&#32493;&#26377;&#21521;&#36317;&#31163;&#22330;&#34920;&#31034;&#65292;&#20174;&#32780;&#30830;&#20445;&#31354;&#38388;&#19968;&#33268;&#24615;&#21644;&#38477;&#20302;&#20869;&#23384;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.18241</link><description>&lt;p&gt;
NeuSDFusion: &#19968;&#31181;&#31354;&#38388;&#24863;&#30693;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;3D&#24418;&#29366;&#30340;&#23436;&#25104;&#12289;&#37325;&#24314;&#21644;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion, Reconstruction, and Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18241
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#31354;&#38388;&#24863;&#30693;3D&#24418;&#29366;&#29983;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;2D&#24179;&#38754;&#34920;&#31034;&#22686;&#24378;&#24314;&#27169;&#65292;&#24182;&#32467;&#21512;&#28151;&#21512;&#24418;&#29366;&#34920;&#31034;&#25216;&#26415;&#30452;&#25509;&#23398;&#20064;&#36830;&#32493;&#26377;&#21521;&#36317;&#31163;&#22330;&#34920;&#31034;&#65292;&#20174;&#32780;&#30830;&#20445;&#31354;&#38388;&#19968;&#33268;&#24615;&#21644;&#38477;&#20302;&#20869;&#23384;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#24418;&#29366;&#29983;&#25104;&#26088;&#22312;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#26465;&#20214;&#21644;&#32422;&#26463;&#30340;&#21019;&#26032;&#24615;3D&#20869;&#23481;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#23558;3D&#24418;&#29366;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#23616;&#37096;&#32452;&#20214;&#65292;&#23558;&#27599;&#20010;&#20803;&#32032;&#23396;&#31435;&#22788;&#29702;&#32780;&#19981;&#32771;&#34385;&#31354;&#38388;&#19968;&#33268;&#24615;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;3D&#25968;&#25454;&#34920;&#31034;&#21644;&#24418;&#29366;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#38480;&#30340;&#22810;&#26679;&#24615;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#29983;&#25104;&#39640;&#24230;&#22810;&#26679;&#21270;&#19988;&#31526;&#21512;&#25351;&#23450;&#32422;&#26463;&#30340;3D&#24418;&#29366;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31354;&#38388;&#24863;&#30693;3D&#24418;&#29366;&#29983;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;2D&#24179;&#38754;&#34920;&#31034;&#26469;&#22686;&#24378;3D&#24418;&#29366;&#24314;&#27169;&#12290;&#20026;&#30830;&#20445;&#31354;&#38388;&#19968;&#33268;&#24615;&#24182;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#19968;&#31181;&#28151;&#21512;&#24418;&#29366;&#34920;&#31034;&#25216;&#26415;&#65292;&#30452;&#25509;&#20351;&#29992;&#27491;&#20132;&#30340;2D&#24179;&#38754;&#23398;&#20064;3D&#24418;&#29366;&#30340;&#36830;&#32493;&#26377;&#21521;&#36317;&#31163;&#22330;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20256;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18241v1 Announce Type: cross  Abstract: 3D shape generation aims to produce innovative 3D content adhering to specific conditions and constraints. Existing methods often decompose 3D shapes into a sequence of localized components, treating each element in isolation without considering spatial consistency. As a result, these approaches exhibit limited versatility in 3D data representation and shape generation, hindering their ability to generate highly diverse 3D shapes that comply with the specified constraints. In this paper, we introduce a novel spatial-aware 3D shape generation framework that leverages 2D plane representations for enhanced 3D shape modeling. To ensure spatial coherence and reduce memory usage, we incorporate a hybrid shape representation technique that directly learns a continuous signed distance field representation of the 3D shape using orthogonal 2D planes. Additionally, we meticulously enforce spatial correspondences across distinct planes using a tra
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;OTSeg&#20013;&#30340;Multi-Prompts Sinkhorn Attention&#26426;&#21046;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#22810;&#20010;&#25991;&#26412;&#25552;&#31034;&#26469;&#21305;&#37197;&#30456;&#20851;&#20687;&#32032;&#23884;&#20837;&#65292;&#20174;&#32780;&#25552;&#21319;&#38646;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.14183</link><description>&lt;p&gt;
OTSeg&#65306;&#22810;&#25552;&#31034;Sinkhorn&#27880;&#24847;&#21147;&#29992;&#20110;&#38646;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
OTSeg: Multi-prompt Sinkhorn Attention for Zero-Shot Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14183
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;OTSeg&#20013;&#30340;Multi-Prompts Sinkhorn Attention&#26426;&#21046;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#22810;&#20010;&#25991;&#26412;&#25552;&#31034;&#26469;&#21305;&#37197;&#30456;&#20851;&#20687;&#32032;&#23884;&#20837;&#65292;&#20174;&#32780;&#25552;&#21319;&#38646;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CLIP&#30340;&#26368;&#26032;&#25104;&#21151;&#35777;&#26126;&#20102;&#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#30693;&#35782;&#36716;&#31227;&#21040;&#20687;&#32032;&#32423;&#20998;&#31867;&#26469;&#36827;&#34892;&#38646;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#30340;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;CLIP&#30693;&#35782;&#26469;&#32039;&#23494;&#23545;&#40784;&#25991;&#26412;&#23884;&#20837;&#21644;&#20687;&#32032;&#23884;&#20837;&#20173;&#28982;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OTSeg&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#26088;&#22312;&#22686;&#24378;&#22810;&#20010;&#25991;&#26412;&#25552;&#31034;&#21305;&#37197;&#30456;&#20851;&#20687;&#32032;&#23884;&#20837;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#31639;&#27861;&#30340;&#22810;&#25552;&#31034;Sinkhorn&#65288;MPS&#65289;&#65292;&#36825;&#20351;&#24471;&#22810;&#20010;&#25991;&#26412;&#25552;&#31034;&#21487;&#20197;&#26377;&#36873;&#25321;&#22320;&#20851;&#27880;&#22270;&#20687;&#20687;&#32032;&#20869;&#30340;&#21508;&#31181;&#35821;&#20041;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#21463;&#21040;Sinkformers&#22312;&#21333;&#27169;&#24577;&#35774;&#32622;&#20013;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MPS&#30340;&#25193;&#23637;&#65292;&#31216;&#20026;&#22810;&#25552;&#31034;Sinkhorn&#27880;&#24847;&#21147;&#65288;MPSA&#65289;&#65292;&#23427;&#26377;&#25928;&#22320;&#21462;&#20195;&#20102;Transformer&#26694;&#26550;&#20013;&#22810;&#27169;&#24577;&#35774;&#32622;&#20013;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14183v1 Announce Type: cross  Abstract: The recent success of CLIP has demonstrated promising results in zero-shot semantic segmentation by transferring muiltimodal knowledge to pixel-level classification. However, leveraging pre-trained CLIP knowledge to closely align text embeddings with pixel embeddings still has limitations in existing approaches. To address this issue, we propose OTSeg, a novel multimodal attention mechanism aimed at enhancing the potential of multiple text prompts for matching associated pixel embeddings. We first propose Multi-Prompts Sinkhorn (MPS) based on the Optimal Transport (OT) algorithm, which leads multiple text prompts to selectively focus on various semantic features within image pixels. Moreover, inspired by the success of Sinkformers in unimodal settings, we introduce the extension of MPS, called Multi-Prompts Sinkhorn Attention (MPSA), which effectively replaces cross-attention mechanisms within Transformer framework in multimodal settin
&lt;/p&gt;</description></item><item><title>BaCon&#36890;&#36807;&#24179;&#34913;&#29305;&#24449;&#32423;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30452;&#25509;&#35268;&#33539;&#20102;&#23454;&#20363;&#34920;&#31034;&#30340;&#20998;&#24067;&#65292;&#22312;&#35299;&#20915;&#19981;&#24179;&#34913;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2403.12986</link><description>&lt;p&gt;
BaCon&#65306;&#36890;&#36807;&#24179;&#34913;&#29305;&#24449;&#32423;&#23545;&#27604;&#23398;&#20064;&#22686;&#24378;&#19981;&#24179;&#34913;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
BaCon: Boosting Imbalanced Semi-supervised Learning via Balanced Feature-Level Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12986
&lt;/p&gt;
&lt;p&gt;
BaCon&#36890;&#36807;&#24179;&#34913;&#29305;&#24449;&#32423;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30452;&#25509;&#35268;&#33539;&#20102;&#23454;&#20363;&#34920;&#31034;&#30340;&#20998;&#24067;&#65292;&#22312;&#35299;&#20915;&#19981;&#24179;&#34913;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#20943;&#23569;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#23545;&#22823;&#37327;&#26631;&#27880;&#30340;&#38656;&#27714;&#65292;&#20294;&#26159;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#26356;&#29616;&#23454;&#30340;&#25361;&#25112;&#8212;&#8212;&#25968;&#25454;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#20173;&#28982;&#36739;&#23569;&#34987;&#25506;&#35752;&#12290;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;(CISSL)&#20013;&#65292;&#19981;&#24179;&#34913;&#25968;&#25454;&#20998;&#24067;&#21487;&#33021;&#20250;&#21152;&#21095;&#30001;&#19981;&#21487;&#38752;&#20266;&#26631;&#31614;&#24341;&#20837;&#30340;&#20559;&#35265;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#25110;&#37325;&#37319;&#26679;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#20294;&#30001;&#20110;&#23427;&#20204;&#20381;&#36182;&#20110;&#26377;&#20559;&#30340;&#39592;&#24178;&#34920;&#31034;&#65292;&#20854;&#24615;&#33021;&#21463;&#21040;&#20005;&#37325;&#38480;&#21046;&#12290;&#19968;&#20123;&#20854;&#20182;&#26041;&#27861;&#30830;&#23454;&#36827;&#34892;&#20102;&#29305;&#24449;&#32423;&#35843;&#25972;&#65292;&#27604;&#22914;&#29305;&#24449;&#34701;&#21512;&#65292;&#20294;&#21487;&#33021;&#24341;&#20837;&#19981;&#21033;&#30340;&#22122;&#22768;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#26356;&#24179;&#34913;&#30340;&#29305;&#24449;&#20998;&#24067;&#23545;CISSL&#38382;&#39064;&#30340;&#22909;&#22788;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#34913;&#29305;&#24449;&#32423;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;(BaCon)&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#19968;&#31181;&#31934;&#24515;&#35774;&#35745;&#30340;&#23545;&#27604;&#26041;&#24335;&#30452;&#25509;&#35268;&#33539;&#20102;&#23454;&#20363;&#34920;&#31034;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12986v1 Announce Type: cross  Abstract: Semi-supervised Learning (SSL) reduces the need for extensive annotations in deep learning, but the more realistic challenge of imbalanced data distribution in SSL remains largely unexplored. In Class Imbalanced Semi-supervised Learning (CISSL), the bias introduced by unreliable pseudo-labels can be exacerbated by imbalanced data distributions. Most existing methods address this issue at instance-level through reweighting or resampling, but the performance is heavily limited by their reliance on biased backbone representation. Some other methods do perform feature-level adjustments like feature blending but might introduce unfavorable noise. In this paper, we discuss the bonus of a more balanced feature distribution for the CISSL problem, and further propose a Balanced Feature-Level Contrastive Learning method (BaCon). Our method directly regularizes the distribution of instances' representations in a well-designed contrastive manner. 
&lt;/p&gt;</description></item><item><title>StainFuser&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#26550;&#26500;&#65292;&#23558;&#26579;&#33394;&#26631;&#20934;&#21270;&#38382;&#39064;&#35270;&#20026;&#39118;&#26684;&#36801;&#31227;&#20219;&#21153;&#65292;&#26080;&#38656;&#25163;&#24037;&#21046;&#20316;&#39068;&#33394;&#32452;&#20998;&#65292;&#22312;2&#30334;&#19975;&#22810;&#20010;&#32452;&#32455;&#23398;&#22270;&#20687;&#19978;&#35757;&#32451;&#30340;&#32467;&#26524;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.09302</link><description>&lt;p&gt;
StainFuser&#65306;&#22312;&#22810;&#21513;&#21152;&#20687;&#32032;&#32452;&#32455;&#23398;&#22270;&#20687;&#20013;&#25511;&#21046;&#25193;&#25955;&#20197;&#21152;&#24555;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
StainFuser: Controlling Diffusion for Faster Neural Style Transfer in Multi-Gigapixel Histology Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09302
&lt;/p&gt;
&lt;p&gt;
StainFuser&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#26550;&#26500;&#65292;&#23558;&#26579;&#33394;&#26631;&#20934;&#21270;&#38382;&#39064;&#35270;&#20026;&#39118;&#26684;&#36801;&#31227;&#20219;&#21153;&#65292;&#26080;&#38656;&#25163;&#24037;&#21046;&#20316;&#39068;&#33394;&#32452;&#20998;&#65292;&#22312;2&#30334;&#19975;&#22810;&#20010;&#32452;&#32455;&#23398;&#22270;&#20687;&#19978;&#35757;&#32451;&#30340;&#32467;&#26524;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26579;&#33394;&#26631;&#20934;&#21270;&#31639;&#27861;&#26088;&#22312;&#23558;&#28304;&#22810;&#21513;&#21152;&#20687;&#32032;&#32452;&#32455;&#23398;&#22270;&#20687;&#30340;&#39068;&#33394;&#21644;&#24378;&#24230;&#29305;&#24449;&#36716;&#25442;&#20026;&#19982;&#30446;&#26631;&#22270;&#20687;&#30456;&#21305;&#37197;&#65292;&#20174;&#32780;&#20943;&#36731;&#22270;&#20687;&#20013;&#29992;&#20110;&#31361;&#20986;&#26174;&#31034;&#32454;&#32990;&#32452;&#20998;&#30340;&#26579;&#33394;&#21058;&#22806;&#35266;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;StainFuser&#65292;&#23558;&#36825;&#20010;&#38382;&#39064;&#35270;&#20026;&#19968;&#20010;&#39118;&#26684;&#36801;&#31227;&#20219;&#21153;&#65292;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#26550;&#26500;&#65292;&#28040;&#38500;&#20102;&#25163;&#24037;&#21046;&#20316;&#39068;&#33394;&#32452;&#20998;&#30340;&#38656;&#35201;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#20026;&#39640;&#36136;&#37327;&#36716;&#25442;&#31579;&#36873;&#20102;&#36804;&#20170;&#20026;&#27490;&#21253;&#21547;&#36229;&#36807;200&#19975;&#20010;&#32452;&#32455;&#23398;&#22270;&#20687;&#30340;&#26368;&#22823;&#26579;&#33394;&#26631;&#20934;&#21270;&#25968;&#25454;&#38598;SPI-2M&#65292;&#24182;&#36827;&#34892;&#20102;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#35757;&#32451;&#12290;&#22312;&#36825;&#20123;&#25968;&#25454;&#19978;&#35757;&#32451;&#21518;&#65292;StainFuser&#22312;&#36136;&#37327;&#19978;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;GAN&#21644;&#25163;&#24037;&#21046;&#20316;&#26041;&#27861;&#30340;&#26631;&#20934;&#21270;&#22270;&#20687;&#12290;&#27492;&#22806;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#29992;&#20316;te&#26102;&#65292;&#23427;&#25913;&#21892;&#20102;&#32454;&#32990;&#26680;&#23454;&#20363;&#20998;&#21106;&#21644;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09302v1 Announce Type: cross  Abstract: Stain normalization algorithms aim to transform the color and intensity characteristics of a source multi-gigapixel histology image to match those of a target image, mitigating inconsistencies in the appearance of stains used to highlight cellular components in the images. We propose a new approach, StainFuser, which treats this problem as a style transfer task using a novel Conditional Latent Diffusion architecture, eliminating the need for handcrafted color components. With this method, we curate SPI-2M the largest stain normalization dataset to date of over 2 million histology images with neural style transfer for high-quality transformations. Trained on this data, StainFuser outperforms current state-of-the-art GAN and handcrafted methods in terms of the quality of normalized images. Additionally, compared to existing approaches, it improves the performance of nuclei instance segmentation and classification models when used as a te
&lt;/p&gt;</description></item><item><title>&#22312;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#20013;&#21033;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#65292;&#21487;&#20197;&#23454;&#29616;&#19981;&#23384;&#22312;&#30340;&#35780;&#20272;&#21644;&#22686;&#24378;&#29616;&#26377;&#35780;&#20272;&#30340;&#39057;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20943;&#23569;&#24037;&#20316;&#20154;&#21592;&#24037;&#20316;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.06322</link><description>&lt;p&gt;
&#22312;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#65288;ICU&#65289;&#20013;&#21033;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#26816;&#26597;&#25506;&#35270;&#21644;&#27963;&#21160;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Leveraging Computer Vision in the Intensive Care Unit (ICU) for Examining Visitation and Mobility
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06322
&lt;/p&gt;
&lt;p&gt;
&#22312;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#20013;&#21033;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#65292;&#21487;&#20197;&#23454;&#29616;&#19981;&#23384;&#22312;&#30340;&#35780;&#20272;&#21644;&#22686;&#24378;&#29616;&#26377;&#35780;&#20272;&#30340;&#39057;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20943;&#23569;&#24037;&#20316;&#20154;&#21592;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23494;&#20999;&#30417;&#27979;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#65288;ICU&#65289;&#24739;&#32773;&#30340;&#37325;&#35201;&#24615;&#65292;&#30001;&#20110;&#21307;&#25252;&#20154;&#21592;&#38754;&#20020;&#30340;&#26102;&#38388;&#38480;&#21046;&#65292;&#35768;&#22810;&#26041;&#38754;&#20173;&#28982;&#21463;&#21040;&#38480;&#21046;&#35780;&#20272;&#12290;&#36807;&#24230;&#30340;&#25506;&#35270;&#21487;&#33021;&#22312;&#20241;&#24687;&#26102;&#38388;&#21152;&#21095;&#24490;&#29615;&#33410;&#24459;&#32010;&#20081;&#21644;&#35893;&#22916;&#30340;&#39118;&#38505;&#65292;&#20294;&#22312;ICU&#20013;&#24182;&#26410;&#34987;&#25429;&#25417;&#12290;&#21516;&#26679;&#65292;&#27963;&#21160;&#33021;&#21147;&#21487;&#20197;&#26159;ICU&#24739;&#32773;&#24247;&#22797;&#25110;&#24694;&#21270;&#30340;&#37325;&#35201;&#25351;&#26631;&#65292;&#20294;&#21482;&#34987;&#38646;&#26143;&#22320;&#25429;&#25417;&#25110;&#26681;&#26412;&#19981;&#34987;&#25429;&#25417;&#12290;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#25214;&#21040;&#20102;&#24212;&#29992;&#65292;&#20943;&#36731;&#20102;&#20154;&#21147;&#36127;&#25285;&#12290;&#22312;ICU&#20013;&#20351;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#20063;&#26377;&#21487;&#33021;&#23454;&#29616;&#19981;&#23384;&#22312;&#30340;&#35780;&#20272;&#25110;&#22686;&#24378;&#29616;&#26377;&#35780;&#20272;&#30340;&#39057;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20943;&#23569;&#24037;&#20316;&#20154;&#21592;&#30340;&#24037;&#20316;&#37327;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#22522;&#20110;&#28145;&#24230;&#25104;&#20687;&#30340;&#26368;&#26032;&#38750;&#20405;&#20837;&#24335;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06322v1 Announce Type: cross  Abstract: Despite the importance of closely monitoring patients in the Intensive Care Unit (ICU), many aspects are still assessed in a limited manner due to the time constraints imposed on healthcare providers. For example, although excessive visitations during rest hours can potentially exacerbate the risk of circadian rhythm disruption and delirium, it is not captured in the ICU. Likewise, while mobility can be an important indicator of recovery or deterioration in ICU patients, it is only captured sporadically or not captured at all. In the past few years, the computer vision field has found application in many domains by reducing the human burden. Using computer vision systems in the ICU can also potentially enable non-existing assessments or enhance the frequency and accuracy of existing assessments while reducing the staff workload. In this study, we leverage a state-of-the-art noninvasive computer vision system based on depth imaging to c
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#28508;&#22312;&#25968;&#25454;&#38598;&#33976;&#39311;&#65288;LD3M&#65289;&#65292;&#32467;&#21512;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#25193;&#25955;&#21644;&#25968;&#25454;&#38598;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#19981;&#21516;&#27169;&#22411;&#26550;&#26500;&#23548;&#33268;&#20934;&#30830;&#24615;&#19979;&#38477;&#21644;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.03881</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#28508;&#22312;&#25968;&#25454;&#38598;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Latent Dataset Distillation with Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03881
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#28508;&#22312;&#25968;&#25454;&#38598;&#33976;&#39311;&#65288;LD3M&#65289;&#65292;&#32467;&#21512;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#25193;&#25955;&#21644;&#25968;&#25454;&#38598;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#19981;&#21516;&#27169;&#22411;&#26550;&#26500;&#23548;&#33268;&#20934;&#30830;&#24615;&#19979;&#38477;&#21644;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#20256;&#32479;&#19978;&#20381;&#36182;&#20110;&#36234;&#26469;&#36234;&#22823;&#30340;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#25968;&#25454;&#38598;&#24102;&#26469;&#23384;&#20648;&#25361;&#25112;&#65292;&#24182;&#19988;&#21253;&#21547;&#19968;&#20123;&#38750;&#24433;&#21709;&#21147;&#26679;&#26412;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#20197;&#34987;&#24573;&#30053;&#32780;&#19981;&#24433;&#21709;&#27169;&#22411;&#26368;&#32456;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38480;&#21046;&#65292;&#20986;&#29616;&#20102;&#23558;&#25968;&#25454;&#38598;&#20449;&#24687;&#33976;&#39311;&#25104;&#19968;&#32452;&#21387;&#32553;&#26679;&#26412;&#65288;&#21512;&#25104;&#26679;&#26412;&#65289;&#65292;&#21363;&#33976;&#39311;&#25968;&#25454;&#38598;&#30340;&#27010;&#24565;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#36873;&#25321;&#29992;&#20110;&#36830;&#25509;&#21407;&#22987;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#26550;&#26500;&#65288;&#36890;&#24120;&#26159;ConvNet&#65289;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#25152;&#20351;&#29992;&#30340;&#27169;&#22411;&#26550;&#26500;&#19982;&#33976;&#39311;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#27169;&#22411;&#19981;&#21516;&#65292;&#21017;&#26368;&#32456;&#20934;&#30830;&#24615;&#20250;&#38477;&#20302;&#12290;&#21478;&#19968;&#20010;&#25361;&#25112;&#26159;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#65292;&#20363;&#22914;128x128&#21450;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03881v1 Announce Type: cross  Abstract: The efficacy of machine learning has traditionally relied on the availability of increasingly larger datasets. However, large datasets pose storage challenges and contain non-influential samples, which could be ignored during training without impacting the final accuracy of the model. In response to these limitations, the concept of distilling the information on a dataset into a condensed set of (synthetic) samples, namely a distilled dataset, emerged. One crucial aspect is the selected architecture (usually ConvNet) for linking the original and synthetic datasets. However, the final accuracy is lower if the employed model architecture differs from the model used during distillation. Another challenge is the generation of high-resolution images, e.g., 128x128 and higher. In this paper, we propose Latent Dataset Distillation with Diffusion Models (LD3M) that combine diffusion in latent space with dataset distillation to tackle both chal
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#21152;&#36895;&#20102;&#21307;&#23398;&#20013;&#33258;&#28982;&#35821;&#35328;&#21644;&#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#24182;&#26631;&#24535;&#30528;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;&#24320;&#21457;&#21644;&#37096;&#32626;&#26041;&#24335;&#30340;&#37325;&#22823;&#33539;&#24335;&#36716;&#21464;&#12290;</title><link>https://arxiv.org/abs/2403.02558</link><description>&lt;p&gt;
&#20026;&#29983;&#25104;&#24314;&#27169;&#30740;&#31350;&#26356;&#26032;&#26377;&#20851;&#20020;&#24202;&#20154;&#24037;&#26234;&#33021;&#65288;MI-CLAIM&#65289;&#26816;&#26597;&#34920;
&lt;/p&gt;
&lt;p&gt;
Updating the Minimum Information about CLinical Artificial Intelligence (MI-CLAIM) checklist for generative modeling research
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02558
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#21152;&#36895;&#20102;&#21307;&#23398;&#20013;&#33258;&#28982;&#35821;&#35328;&#21644;&#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#24182;&#26631;&#24535;&#30528;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;&#24320;&#21457;&#21644;&#37096;&#32626;&#26041;&#24335;&#30340;&#37325;&#22823;&#33539;&#24335;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12289;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#21152;&#36895;&#20102;&#21307;&#23398;&#20013;&#33258;&#28982;&#35821;&#35328;&#21644;&#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#24182;&#26631;&#24535;&#30528;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;&#24320;&#21457;&#21644;&#37096;&#32626;&#26041;&#24335;&#30340;&#37325;&#22823;&#33539;&#24335;&#36716;&#21464;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#38750;&#24120;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#20294;&#22312;&#25193;&#23637;&#21644;&#35780;&#20272;&#23427;&#20204;&#30340;&#20351;&#29992;&#36807;&#31243;&#20013;&#20986;&#29616;&#20102;&#21069;&#20154;&#26694;&#26550;&#26410;&#35299;&#20915;&#30340;&#26032;&#25361;&#25112;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#20197;&#23569;&#37327;&#25110;&#26080;&#38656;&#19987;&#38376;&#35757;&#32451;&#25968;&#25454;&#21363;&#21487;&#20135;&#29983;&#26377;&#29992;&#36755;&#20986;&#30340;&#33021;&#21147;&#65288;&#8220;&#38646;&#26679;&#26412;&#8221;&#25110;&#8220;&#23569;&#26679;&#26412;&#8221;&#26041;&#27861;&#65289;&#65292;&#20197;&#21450;&#23427;&#20204;&#36755;&#20986;&#30340;&#24320;&#25918;&#24615;&#36136;&#65292;&#38656;&#35201;&#21046;&#23450;&#26356;&#26032;&#30340;&#20351;&#29992;&#21644;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#30340;&#25351;&#21335;&#12290;&#32654;&#22269;&#34892;&#25919;&#21629;&#20196;141103&#30830;&#23450;&#20102;&#26377;&#20851;&#20020;&#24202;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#24320;&#21457;&#30340;&#26631;&#20934;&#21644;&#26368;&#20339;&#23454;&#36341;&#23384;&#22312;&#30340;&#24046;&#36317;&#65292;&#20197;&#21450;&#20960;&#20010;&#26032;&#20852;&#22269;&#23478;&#20020;&#24202;&#20154;&#24037;&#26234;&#33021;&#35780;&#20272;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02558v1 Announce Type: new  Abstract: Recent advances in generative models, including large language models (LLMs), vision language models (VLMs), and diffusion models, have accelerated the field of natural language and image processing in medicine and marked a significant paradigm shift in how biomedical models can be developed and deployed. While these models are highly adaptable to new tasks, scaling and evaluating their usage presents new challenges not addressed in previous frameworks. In particular, the ability of these models to produce useful outputs with little to no specialized training data ("zero-" or "few-shot" approaches), as well as the open-ended nature of their outputs, necessitate the development of updated guidelines in using and evaluating these models. In response to gaps in standards and best practices for the development of clinical AI tools identified by US Executive Order 141103 and several emerging national networks for clinical AI evaluation, we be
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#23545;&#27604;&#20102;&#20154;&#31867;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#34892;&#20026;&#24046;&#24322;&#65292;&#21457;&#29616;&#20154;&#31867;&#20855;&#26377;&#21363;&#26102;&#27010;&#25324;&#33021;&#21147;&#65292;&#32780;DNNs&#23384;&#22312;&#28382;&#21518;&#27010;&#25324;&#29616;&#35937;&#65292;&#36825;&#34920;&#26126;&#20102;&#34920;&#31034;&#20998;&#27495;&#30340;&#23384;&#22312;&#12290;</title><link>https://arxiv.org/abs/2402.09303</link><description>&lt;p&gt;
&#20154;&#31867;&#20013;&#30340;&#21363;&#26102;&#27010;&#25324;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28382;&#21518;&#27010;&#25324;&#8212;&#8212;&#34920;&#31034;&#20998;&#27495;&#30340;&#35777;&#25454;&#65311;
&lt;/p&gt;
&lt;p&gt;
Immediate generalisation in humans but a generalisation lag in deep neural networks$\unicode{x2014}$evidence for representational divergence?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09303
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#23545;&#27604;&#20102;&#20154;&#31867;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#34892;&#20026;&#24046;&#24322;&#65292;&#21457;&#29616;&#20154;&#31867;&#20855;&#26377;&#21363;&#26102;&#27010;&#25324;&#33021;&#21147;&#65292;&#32780;DNNs&#23384;&#22312;&#28382;&#21518;&#27010;&#25324;&#29616;&#35937;&#65292;&#36825;&#34920;&#26126;&#20102;&#34920;&#31034;&#20998;&#27495;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#22312;&#22270;&#20687;&#20998;&#31867;&#39046;&#22495;&#20013;&#23545;&#27604;&#20102;&#20154;&#31867;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#35768;&#22810;&#34892;&#20026;&#27604;&#36739;&#12290;&#36890;&#24120;&#65292;&#27604;&#36739;&#30740;&#31350;&#20851;&#27880;&#30340;&#26159;&#23398;&#20064;&#36807;&#31243;&#30340;&#26368;&#32456;&#32467;&#26524;&#65292;&#36890;&#36807;&#27979;&#37327;&#21644;&#27604;&#36739;&#30446;&#26631;&#31867;&#21035;&#34920;&#31034;&#30340;&#30456;&#20284;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#34920;&#31034;&#22914;&#20309;&#24418;&#25104;&#21363;&#20854;&#36807;&#31243;&#8212;&#8212;&#21363;&#22312;&#33719;&#21462;&#36807;&#31243;&#20013;&#35266;&#23519;&#21040;&#30340;&#34892;&#20026;&#21464;&#21270;&#21644;&#20013;&#38388;&#38454;&#27573;&#8212;&#8212;&#24448;&#24448;&#23569;&#26377;&#30452;&#25509;&#21644;&#23454;&#35777;&#30340;&#27604;&#36739;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#23545;&#20154;&#31867;&#35266;&#23519;&#32773;&#21644;&#19981;&#21516;&#32463;&#20856;&#19982;&#26368;&#26032;&#25216;&#26415;&#30340;DNNs&#20013;&#21487;&#36716;&#31227;&#34920;&#31034;&#26159;&#22914;&#20309;&#34987;&#33719;&#21462;&#30340;&#30340;&#35814;&#32454;&#35843;&#26597;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21463;&#38480;&#30340;&#30417;&#30563;&#23398;&#20064;&#29615;&#22659;&#65292;&#35813;&#29615;&#22659;&#20013;&#25105;&#20204;&#23545;&#40784;&#20102;&#23398;&#20064;&#30456;&#20851;&#30340;&#21442;&#25968;&#65292;&#22914;&#36215;&#22987;&#28857;&#12289;&#36755;&#20837;&#27169;&#24335;&#12289;&#21487;&#29992;&#36755;&#20837;&#25968;&#25454;&#20197;&#21450;&#25552;&#20379;&#30340;&#21453;&#39304;&#12290;&#22312;&#25972;&#20010;&#23398;&#20064;&#36807;&#31243;&#20013;&#25105;&#20204;&#35780;&#20272;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09303v1 Announce Type: cross Abstract: Recent research has seen many behavioral comparisons between humans and deep neural networks (DNNs) in the domain of image classification. Often, comparison studies focus on the end-result of the learning process by measuring and comparing the similarities in the representations of object categories once they have been formed. However, the process of how these representations emerge$\unicode{x2014}$that is, the behavioral changes and intermediate stages observed during the acquisition$\unicode{x2014}$is less often directly and empirically compared.   Here we report a detailed investigation of how transferable representations are acquired in human observers and various classic and state-of-the-art DNNs. We develop a constrained supervised learning environment in which we align learning-relevant parameters such as starting point, input modality, available input data and the feedback provided. Across the whole learning process we evaluate 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#22686;&#24378;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#37319;&#29992;&#27491;&#26679;&#26412;&#25277;&#26679;&#21644;&#26435;&#34913;&#37325;&#35201;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#24212;&#23545;&#22122;&#22768;AU&#26631;&#31614;&#21644;AU&#31867;&#22411;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.06165</link><description>&lt;p&gt;
&#23398;&#20064;&#23545;&#27604;&#29305;&#24449;&#34920;&#31034;&#26469;&#36827;&#34892;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning Contrastive Feature Representations for Facial Action Unit Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06165
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#22686;&#24378;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#37319;&#29992;&#27491;&#26679;&#26412;&#25277;&#26679;&#21644;&#26435;&#34913;&#37325;&#35201;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#24212;&#23545;&#22122;&#22768;AU&#26631;&#31614;&#21644;AU&#31867;&#22411;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#65288;AU&#65289;&#26816;&#27979;&#30340;&#20027;&#35201;&#26041;&#27861;&#28041;&#21450;&#30417;&#30563;&#30340;&#22810;&#26631;&#31614;&#20108;&#36827;&#21046;&#20998;&#31867;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#24120;&#24120;&#23545;AU&#30340;&#20687;&#32032;&#32423;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#65292;&#20174;&#32780;&#23545;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#34920;&#36798;&#33021;&#21147;&#25552;&#20986;&#20102;&#24456;&#22823;&#30340;&#35201;&#27714;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23384;&#22312;&#22122;&#22768;AU&#26631;&#31614;&#65292;&#36825;&#31181;&#20570;&#27861;&#22686;&#21152;&#20102;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#22686;&#24378;&#12290;&#30446;&#26631;&#26159;&#22312;AU&#26816;&#27979;&#39046;&#22495;&#20013;&#25670;&#33073;&#20256;&#32479;&#30340;&#20687;&#32032;&#32423;&#23398;&#20064;&#33539;&#24335;&#65292;&#33719;&#24471;&#21028;&#21035;&#29305;&#24449;&#12290;&#20026;&#20102;&#24212;&#23545;&#22122;&#22768;AU&#26631;&#31614;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#22686;&#24378;&#30417;&#30563;&#20449;&#21495;&#12290;&#36825;&#31181;&#22686;&#24378;&#26159;&#36890;&#36807;&#27491;&#26679;&#26412;&#25277;&#26679;&#23454;&#29616;&#30340;&#65292;&#21253;&#25324;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#27491;&#26679;&#26412;&#23545;&#12290;&#21478;&#22806;&#65292;&#20026;&#20102;&#20943;&#36731;&#27599;&#20010;AU&#31867;&#22411;&#30340;&#20998;&#24067;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26435;&#34913;&#37325;&#35201;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The predominant approach to facial action unit (AU) detection revolves around a supervised multi-label binary classification problem. Existing methodologies often encode pixel-level information of AUs, thereby imposing substantial demands on model complexity and expressiveness. Moreover, this practice elevates the susceptibility to overfitting due to the presence of noisy AU labels. In the present study, we introduce a contrastive learning framework enhanced by both supervised and self-supervised signals. The objective is to acquire discriminative features, deviating from the conventional pixel-level learning paradigm within the domain of AU detection. To address the challenge posed by noisy AU labels, we augment the supervised signal through the introduction of a self-supervised signal. This augmentation is achieved through positive sample sampling, encompassing three distinct types of positive sample pairs. Furthermore, to mitigate the imbalanced distribution of each AU type, we empl
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#33014;&#36136;&#30244;&#32452;&#32455;&#20999;&#29255;&#20998;&#26512;&#20013;&#20855;&#26377;&#35786;&#26029;&#21644;&#39044;&#27979;&#30340;&#28508;&#21147;&#65292;&#24403;&#21069;&#30740;&#31350;&#32858;&#28966;&#20110;&#25104;&#20154;&#22411;&#24357;&#28459;&#24615;&#33014;&#36136;&#30244;&#30340;&#33487;&#26408;&#31934;&#21644;&#20234;&#32418;&#26579;&#33394;&#32452;&#32455;&#20999;&#29255;&#65292;&#20197;&#21450;&#23545;&#35813;&#30142;&#30149;&#30340;&#20998;&#31867;&#12289;&#20998;&#32423;&#12289;&#20998;&#23376;&#26631;&#35760;&#39044;&#27979;&#21644;&#29983;&#23384;&#39044;&#27979;&#31561;&#20020;&#24202;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2401.15022</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33014;&#36136;&#30244;&#32452;&#32455;&#20999;&#29255;&#20998;&#26512;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Machine learning-based analysis of glioma tissue sections: a review. (arXiv:2401.15022v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15022
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#33014;&#36136;&#30244;&#32452;&#32455;&#20999;&#29255;&#20998;&#26512;&#20013;&#20855;&#26377;&#35786;&#26029;&#21644;&#39044;&#27979;&#30340;&#28508;&#21147;&#65292;&#24403;&#21069;&#30740;&#31350;&#32858;&#28966;&#20110;&#25104;&#20154;&#22411;&#24357;&#28459;&#24615;&#33014;&#36136;&#30244;&#30340;&#33487;&#26408;&#31934;&#21644;&#20234;&#32418;&#26579;&#33394;&#32452;&#32455;&#20999;&#29255;&#65292;&#20197;&#21450;&#23545;&#35813;&#30142;&#30149;&#30340;&#20998;&#31867;&#12289;&#20998;&#32423;&#12289;&#20998;&#23376;&#26631;&#35760;&#39044;&#27979;&#21644;&#29983;&#23384;&#39044;&#27979;&#31561;&#20020;&#24202;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33014;&#36136;&#30244;&#30340;&#35786;&#26029;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#12290;&#20351;&#29992;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23545;&#33014;&#36136;&#30244;&#32452;&#32455;&#36827;&#34892;&#32452;&#32455;&#23398;&#35780;&#20272;&#65292;&#20026;&#35786;&#26029;&#21644;&#39044;&#27979;&#32467;&#26524;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;&#20026;&#20102;&#23545;&#24403;&#21069;&#30740;&#31350;&#30340;&#29616;&#29366;&#36827;&#34892;&#27010;&#36848;&#65292;&#26412;&#32508;&#36848;&#23545;70&#20010;&#20844;&#24320;&#21487;&#24471;&#30340;&#30740;&#31350;&#35770;&#25991;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36825;&#20123;&#35770;&#25991;&#20851;&#20110;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#26579;&#33394;&#30340;&#33014;&#36136;&#30244;&#32452;&#32455;&#20999;&#29255;&#65292;&#28085;&#30422;&#20102;&#20998;&#31867;&#65288;16/70&#65289;&#65292;&#20998;&#32423;&#65288;23/70&#65289;&#65292;&#20998;&#23376;&#26631;&#35760;&#39044;&#27979;&#65288;13/70&#65289;&#21644;&#29983;&#23384;&#39044;&#27979;&#65288;27/70&#65289;&#31561;&#35786;&#26029;&#20219;&#21153;&#12290;&#25152;&#26377;&#30340;&#30740;&#31350;&#37117;&#22312;&#26041;&#27861;&#23398;&#26041;&#38754;&#21450;&#20854;&#20020;&#24202;&#36866;&#29992;&#24615;&#26041;&#38754;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#21457;&#29616;&#24403;&#21069;&#30740;&#31350;&#30340;&#37325;&#28857;&#26159;&#23545;&#25104;&#20154;&#22411;&#24357;&#28459;&#24615;&#33014;&#36136;&#30244;&#30340;&#33487;&#26408;&#31934;&#21644;&#20234;&#32418;&#26579;&#33394;&#32452;&#32455;&#20999;&#29255;&#36827;&#34892;&#35780;&#20272;&#12290;&#22810;&#25968;&#30740;&#31350;&#65288;49/70&#65289;&#22522;&#20110;&#20844;&#24320;&#30340;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#21644;&#20302;&#32423;&#21035;&#33014;&#36136;&#30244;&#25968;&#25454;&#38598;&#65292;&#20165;&#26377;&#23569;&#25968;&#30740;&#31350;&#20351;&#29992;&#20854;&#20182;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the diagnosis of gliomas has become increasingly complex. Histological assessment of glioma tissue using modern machine learning techniques offers new opportunities to support diagnosis and outcome prediction. To give an overview of the current state of research, this review examines 70 publicly available research studies on machine learning-based analysis of stained human glioma tissue sections, covering the diagnostic tasks of subtyping (16/70), grading (23/70), molecular marker prediction (13/70), and survival prediction (27/70). All studies were reviewed with regard to methodological aspects as well as clinical applicability. It was found that the focus of current research is the assessment of hematoxylin and eosin-stained tissue sections of adult-type diffuse gliomas. The majority of studies (49/70) are based on the publicly available glioblastoma and low-grade glioma datasets from The Cancer Genome Atlas (TCGA) and only a few studies employed other datasets in is
&lt;/p&gt;</description></item><item><title>&#22312;&#37325;&#30151;&#30417;&#25252;&#23460;&#20013;&#24320;&#21457;&#33021;&#22815;&#26816;&#27979;&#35270;&#35273;&#32447;&#32034;&#24182;&#19982;&#24739;&#32773;&#20020;&#24202;&#29366;&#24577;&#20851;&#32852;&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#23458;&#35266;&#21644;&#35814;&#32454;&#30340;&#30417;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.00565</link><description>&lt;p&gt;
&#22312;&#37325;&#30151;&#30417;&#25252;&#23460;&#20013;&#26816;&#27979;&#35270;&#35273;&#32447;&#32034;&#19982;&#24739;&#32773;&#20020;&#24202;&#29366;&#24577;&#30340;&#20851;&#32852;
&lt;/p&gt;
&lt;p&gt;
Detecting Visual Cues in the Intensive Care Unit and Association with Patient Clinical Status. (arXiv:2311.00565v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00565
&lt;/p&gt;
&lt;p&gt;
&#22312;&#37325;&#30151;&#30417;&#25252;&#23460;&#20013;&#24320;&#21457;&#33021;&#22815;&#26816;&#27979;&#35270;&#35273;&#32447;&#32034;&#24182;&#19982;&#24739;&#32773;&#20020;&#24202;&#29366;&#24577;&#20851;&#32852;&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#23458;&#35266;&#21644;&#35814;&#32454;&#30340;&#30417;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#30151;&#30417;&#25252;&#23460;&#65288;ICU&#65289;&#20026;&#24739;&#26377;&#29983;&#21629;&#23041;&#32961;&#30340;&#24739;&#32773;&#25552;&#20379;&#23494;&#20999;&#30417;&#25252;&#21644;&#36830;&#32493;&#25252;&#29702;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26102;&#38388;&#38480;&#21046;&#21644;&#21307;&#30103;&#20445;&#20581;&#24037;&#20316;&#32773;&#30340;&#24037;&#20316;&#36127;&#33655;&#65292;ICU&#20013;&#30340;&#36830;&#32493;&#24739;&#32773;&#35780;&#20272;&#20173;&#28982;&#26377;&#38480;&#12290;&#29616;&#26377;&#30340;ICU&#24739;&#32773;&#35780;&#20272;&#65292;&#22914;&#30140;&#30171;&#25110;&#27963;&#21160;&#33021;&#21147;&#35780;&#20272;&#65292;&#22823;&#22810;&#26159;&#38646;&#25955;&#21644;&#25163;&#21160;&#23454;&#26045;&#30340;&#65292;&#20174;&#32780;&#24341;&#20837;&#20102;&#20154;&#20026;&#38169;&#35823;&#30340;&#28508;&#22312;&#21487;&#33021;&#24615;&#12290;&#24320;&#21457;&#33021;&#22815;&#22686;&#24378;ICU&#20013;&#20154;&#31867;&#35780;&#20272;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24037;&#20855;&#21487;&#20197;&#26377;&#21033;&#20110;&#25552;&#20379;&#26356;&#23458;&#35266;&#21644;&#35814;&#32454;&#30340;&#30417;&#27979;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;&#25429;&#25417;&#19982;&#30140;&#30171;&#25110;&#19981;&#23433;&#30456;&#20851;&#30340;&#24739;&#32773;&#38754;&#37096;&#32447;&#32034;&#30340;&#21464;&#21270;&#21487;&#20197;&#24110;&#21161;&#35843;&#25972;&#19982;&#30140;&#30171;&#30456;&#20851;&#30340;&#33647;&#29289;&#25110;&#26816;&#27979;&#21487;&#33021;&#24341;&#36215;&#19981;&#23433;&#30340;&#24773;&#20917;&#65292;&#22914;&#35893;&#22916;&#12290;&#27492;&#22806;&#65292;&#22312;&#19981;&#33391;&#20020;&#24202;&#20107;&#20214;&#21457;&#29983;&#26399;&#38388;&#25110;&#20043;&#21069;&#65292;&#35270;&#35273;&#32447;&#32034;&#30340;&#24494;&#22937;&#21464;&#21270;&#19982;&#39640;&#20998;&#36776;&#29575;&#29983;&#29702;&#20449;&#21495;&#30456;&#32467;&#21512;&#21487;&#33021;&#26377;&#21161;&#20110;&#36830;&#32493;&#24739;&#32773;&#30417;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intensive Care Units (ICU) provide close supervision and continuous care to patients with life-threatening conditions. However, continuous patient assessment in the ICU is still limited due to time constraints and the workload on healthcare providers. Existing patient assessments in the ICU such as pain or mobility assessment are mostly sporadic and administered manually, thus introducing the potential for human errors. Developing Artificial intelligence (AI) tools that can augment human assessments in the ICU can be beneficial for providing more objective and granular monitoring capabilities. For example, capturing the variations in a patient's facial cues related to pain or agitation can help in adjusting pain-related medications or detecting agitation-inducing conditions such as delirium. Additionally, subtle changes in visual cues during or prior to adverse clinical events could potentially aid in continuous patient monitoring when combined with high-resolution physiological signal
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#24555;&#36895;&#23398;&#20064;&#30340;&#22522;&#30784;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#23545;&#21738;&#20123;&#29305;&#24449;&#26356;&#20559;&#22909;&#65292;&#21363;&#21487;&#39044;&#27979;&#24615;&#21644;&#21487;&#29992;&#24615;&#22914;&#20309;&#30456;&#20114;&#24433;&#21709;&#27169;&#22411;&#30340;&#29305;&#24449;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.16228</link><description>&lt;p&gt;
&#20851;&#20110;&#24555;&#36895;&#23398;&#20064;&#30340;&#22522;&#30784;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Foundations of Shortcut Learning. (arXiv:2310.16228v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16228
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#24555;&#36895;&#23398;&#20064;&#30340;&#22522;&#30784;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#23545;&#21738;&#20123;&#29305;&#24449;&#26356;&#20559;&#22909;&#65292;&#21363;&#21487;&#39044;&#27979;&#24615;&#21644;&#21487;&#29992;&#24615;&#22914;&#20309;&#30456;&#20114;&#24433;&#21709;&#27169;&#22411;&#30340;&#29305;&#24449;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#20016;&#23500;&#30340;&#29305;&#24449;&#12290;&#27169;&#22411;&#20351;&#29992;&#21738;&#20123;&#29305;&#24449;&#19981;&#20165;&#21462;&#20915;&#20110;&#39044;&#27979;&#33021;&#21147; - &#19968;&#20010;&#29305;&#24449;&#21487;&#38752;&#22320;&#25351;&#31034;&#35757;&#32451;&#38598;&#26631;&#31614;&#30340;&#31243;&#24230;&#65292;&#36824;&#21462;&#20915;&#20110;&#21487;&#29992;&#24615; - &#19968;&#20010;&#29305;&#24449;&#21487;&#20197;&#20174;&#36755;&#20837;&#20013;&#34987;&#36731;&#26494;&#25552;&#21462;&#25110;&#21033;&#29992;&#30340;&#31243;&#24230;&#12290;&#26377;&#20851;&#24555;&#36895;&#23398;&#20064;&#30340;&#25991;&#29486;&#24050;&#32463;&#25351;&#20986;&#20102;&#27169;&#22411;&#20559;&#22909;&#19968;&#20010;&#29305;&#24449;&#32780;&#19981;&#26159;&#21478;&#19968;&#20010;&#29305;&#24449;&#30340;&#20363;&#23376;&#65292;&#20363;&#22914;&#22312;&#32441;&#29702;&#21644;&#24418;&#29366;&#20043;&#38388;&#20197;&#21450;&#22312;&#22270;&#20687;&#32972;&#26223;&#21644;&#21069;&#26223;&#23545;&#35937;&#20043;&#38388;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#27979;&#35797;&#20851;&#20110;&#21738;&#20123;&#36755;&#20837;&#23646;&#24615;&#23545;&#20110;&#27169;&#22411;&#26356;&#23481;&#26131;&#33719;&#21462;&#30340;&#20551;&#35774;&#65292;&#24182;&#31995;&#32479;&#22320;&#30740;&#31350;&#39044;&#27979;&#33021;&#21147;&#21644;&#21487;&#29992;&#24615;&#22914;&#20309;&#30456;&#20114;&#20316;&#29992;&#26469;&#22609;&#36896;&#27169;&#22411;&#30340;&#29305;&#24449;&#20351;&#29992;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26368;&#23567;&#30340;&#12289;&#26126;&#30830;&#30340;&#29983;&#25104;&#26694;&#26550;&#26469;&#21512;&#25104;&#20855;&#26377;&#20004;&#20010;&#28508;&#22312;&#29305;&#24449;&#30340;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#36825;&#20004;&#20010;&#29305;&#24449;&#22312;&#39044;&#27979;&#33021;&#21147;&#21644;&#25105;&#20204;&#20551;&#35774;&#19982;&#21487;&#29992;&#24615;&#26377;&#20851;&#30340;&#22240;&#32032;&#19978;&#26377;&#25152;&#19981;&#21516;&#65292;&#24182;&#37327;&#21270;&#20102;&#27169;&#22411;&#30340;&#24555;&#25463;&#20559;&#24046; - &#23427;&#36807;&#24230;&#20381;&#36182;&#24555;&#25463;&#65288;&#26356;&#21487;&#29992;&#12289;&#19981;&#22826;&#39044;&#27979;&#65289;&#29305;&#24449;&#32780;&#24573;&#35270;&#20102;&#26680;&#24515;&#65288;&#19981;&#22826;&#21487;&#29992;)&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep-learning models can extract a rich assortment of features from data. Which features a model uses depends not only on predictivity-how reliably a feature indicates train-set labels-but also on availability-how easily the feature can be extracted, or leveraged, from inputs. The literature on shortcut learning has noted examples in which models privilege one feature over another, for example texture over shape and image backgrounds over foreground objects. Here, we test hypotheses about which input properties are more available to a model, and systematically study how predictivity and availability interact to shape models' feature use. We construct a minimal, explicit generative framework for synthesizing classification datasets with two latent features that vary in predictivity and in factors we hypothesize to relate to availability, and quantify a model's shortcut bias-its over-reliance on the shortcut (more available, less predictive) feature at the expense of the core (less avail
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20998;&#24067;&#24335;&#36793;&#32536;&#24212;&#29992;&#20013;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#20013;&#32676;&#20307;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#35745;&#31639;&#36328;&#22495;&#32676;&#20307;&#37325;&#35201;&#24615;&#26469;&#20943;&#36731;&#20840;&#23616;&#27169;&#22411;&#30340;&#20559;&#35265;&#65292;&#24182;&#20445;&#25345;&#38544;&#31169;&#21644;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.07085</link><description>&lt;p&gt;
&#22312;&#24322;&#26500;&#35774;&#22791;&#19978;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#32676;&#20307;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Mitigating Group Bias in Federated Learning for Heterogeneous Devices. (arXiv:2309.07085v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20998;&#24067;&#24335;&#36793;&#32536;&#24212;&#29992;&#20013;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#20013;&#32676;&#20307;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#35745;&#31639;&#36328;&#22495;&#32676;&#20307;&#37325;&#35201;&#24615;&#26469;&#20943;&#36731;&#20840;&#23616;&#27169;&#22411;&#30340;&#20559;&#35265;&#65292;&#24182;&#20445;&#25345;&#38544;&#31169;&#21644;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#27491;&#22312;&#20998;&#24067;&#24335;&#36793;&#32536;&#24212;&#29992;&#20013;&#23853;&#38706;&#22836;&#35282;&#20316;&#20026;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36793;&#32536;&#37096;&#32626;&#26159;&#24322;&#26500;&#30340;&#65292;&#21363;&#23427;&#20204;&#30340;&#24863;&#30693;&#33021;&#21147;&#21644;&#29615;&#22659;&#22312;&#37096;&#32626;&#20013;&#21508;&#19981;&#30456;&#21516;&#12290;&#36825;&#31181;&#36793;&#32536;&#24322;&#26500;&#36829;&#21453;&#20102;&#26412;&#22320;&#25968;&#25454;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#29420;&#31435;&#19988;&#20998;&#24067;&#30456;&#21516; (IID) &#30340;&#29305;&#24615;&#65292;&#20135;&#29983;&#20102;&#26377;&#20559;&#35265;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#21363;&#23545;&#29305;&#23450;&#31038;&#21306;&#25110;&#32676;&#20307;&#20570;&#20986;&#19981;&#20844;&#24179;&#30340;&#20915;&#31574;&#21644;&#27495;&#35270;&#12290;&#29616;&#26377;&#30340;&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#21482;&#20851;&#27880;&#38750;IID&#25968;&#25454;&#20013;&#30001;&#26631;&#31614;&#24322;&#26500;&#24341;&#36215;&#30340;&#20559;&#35265;&#65292;&#24182;&#27809;&#26377;&#32771;&#34385;&#30001;&#29305;&#24449;&#24322;&#26500;&#23548;&#33268;&#30340;&#39046;&#22495;&#21464;&#21270;&#65292;&#20063;&#27809;&#26377;&#35299;&#20915;&#20840;&#23616;&#32676;&#20307;&#20844;&#24179;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20445;&#25252;&#38544;&#31169;&#21644;&#19981;&#22686;&#21152;&#36164;&#28304;&#21033;&#29992;&#24320;&#38144;&#30340;&#24773;&#20917;&#19979;&#65292;&#20943;&#23569;&#32676;&#20307;&#20559;&#35265;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#21033;&#29992;&#24179;&#22343;&#26465;&#20214;&#27010;&#29575;&#26469;&#35745;&#31639;&#36328;&#22495;&#32676;&#20307;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning is emerging as a privacy-preserving model training approach in distributed edge applications. As such, most edge deployments are heterogeneous in nature i.e., their sensing capabilities and environments vary across deployments. This edge heterogeneity violates the independence and identical distribution (IID) property of local data across clients and produces biased global models i.e. models that contribute to unfair decision-making and discrimination against a particular community or a group. Existing bias mitigation techniques only focus on bias generated from label heterogeneity in non-IID data without accounting for domain variations due to feature heterogeneity and do not address global group-fairness property.  Our work proposes a group-fair FL framework that minimizes group-bias while preserving privacy and without resource utilization overhead. Our main idea is to leverage average conditional probabilities to compute a cross-domain group \textit{importance we
&lt;/p&gt;</description></item><item><title>ABC123&#26159;&#19968;&#31181;&#26080;&#38656;&#20351;&#29992;&#31034;&#20363;&#36827;&#34892;&#35757;&#32451;&#25110;&#25512;&#26029;&#30340;&#22810;&#31867;&#21035;&#31867;&#21035;&#26080;&#20851;&#35745;&#25968;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#33539;&#24335;&#65292;&#23427;&#22312;&#22810;&#31181;&#23545;&#35937;&#21516;&#26102;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.04820</link><description>&lt;p&gt;
ABC&#31616;&#21333;&#22914;123&#65306;&#19968;&#31181;&#29992;&#20110;&#26080;&#20808;&#20363;&#22810;&#31867;&#21035;&#31867;&#21035;&#26080;&#20851;&#35745;&#25968;&#30340;&#30450;&#30446;&#35745;&#25968;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ABC Easy as 123: A Blind Counter for Exemplar-Free Multi-Class Class-agnostic Counting. (arXiv:2309.04820v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04820
&lt;/p&gt;
&lt;p&gt;
ABC123&#26159;&#19968;&#31181;&#26080;&#38656;&#20351;&#29992;&#31034;&#20363;&#36827;&#34892;&#35757;&#32451;&#25110;&#25512;&#26029;&#30340;&#22810;&#31867;&#21035;&#31867;&#21035;&#26080;&#20851;&#35745;&#25968;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#33539;&#24335;&#65292;&#23427;&#22312;&#22810;&#31181;&#23545;&#35937;&#21516;&#26102;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#21035;&#26080;&#20851;&#35745;&#25968;&#26041;&#27861;&#21487;&#20197;&#23545;&#20219;&#24847;&#31867;&#21035;&#30340;&#23545;&#35937;&#36827;&#34892;&#35745;&#25968;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#25552;&#20379;&#20102;&#24040;&#22823;&#30340;&#23454;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#21482;&#33021;&#36866;&#29992;&#20110;&#38656;&#35201;&#19968;&#32452;&#29305;&#23450;&#31867;&#22411;&#30340;&#31034;&#20363;&#25110;&#22270;&#20687;&#20013;&#20165;&#21253;&#21547;&#19968;&#31181;&#31867;&#22411;&#23545;&#35937;&#30340;&#24773;&#20917;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#23616;&#38480;&#20043;&#19968;&#26159;&#32570;&#20047;&#36866;&#29992;&#20110;&#22810;&#31181;&#23545;&#35937;&#21516;&#26102;&#23384;&#22312;&#30340;&#35745;&#25968;&#35774;&#32622;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#31867;&#21035;&#12289;&#31867;&#21035;&#26080;&#20851;&#35745;&#25968;&#25968;&#25454;&#38598;&#65288;MCAC&#65289;&#20197;&#21450;&#19968;&#31181;&#21517;&#20026;ABC123&#30340;&#30450;&#30446;&#35745;&#25968;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#35757;&#32451;&#25110;&#25512;&#26029;&#36807;&#31243;&#20013;&#19981;&#20351;&#29992;&#29305;&#23450;&#31867;&#22411;&#31034;&#20363;&#26469;&#21516;&#26102;&#35745;&#25968;&#22810;&#31181;&#23545;&#35937;&#12290;ABC123&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#22312;&#35745;&#25968;&#38454;&#27573;&#21518;&#25214;&#21040;&#31034;&#20363;&#26469;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#29983;&#25104;&#30340;&#36755;&#20986;&#65292;&#32780;&#19981;&#38656;&#35201;&#20808;&#23548;&#26679;&#26412;&#26469;&#24341;&#23548;&#35745;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;ABC123&#22312;MCAC&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#32780;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#36827;&#34892;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class-agnostic counting methods enumerate objects of an arbitrary class, providing tremendous utility in many fields. Prior works have limited usefulness as they require either a set of examples of the type to be counted or that the image contains only a single type of object. A significant factor in these shortcomings is the lack of a dataset to properly address counting in settings with more than one kind of object present. To address these issues, we propose the first Multi-class, Class-Agnostic Counting dataset (MCAC) and A Blind Counter (ABC123), a method that can count multiple types of objects simultaneously without using examples of type during training or inference. ABC123 introduces a new paradigm where instead of requiring exemplars to guide the enumeration, examples are found after the counting stage to help a user understand the generated outputs. We show that ABC123 outperforms contemporary methods on MCAC without the requirement of human in-the-loop annotations. We also 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#23433;&#20840;&#32771;&#34385;&#20316;&#20026;&#32467;&#26500;&#20803;&#32032;&#65292;&#20197;&#31995;&#32479;&#32508;&#21512;&#30340;&#26041;&#24335;&#30830;&#20445;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;&#36825;&#19968;&#27010;&#24565;&#19981;&#20165;&#19982;&#29616;&#26377;&#30340;&#23433;&#20840;&#26631;&#20934;&#30456;&#22865;&#21512;&#65292;&#36824;&#20026;AI&#23433;&#20840;&#30456;&#20851;&#30340;&#23398;&#26415;&#20986;&#29256;&#29289;&#21644;&#26631;&#20934;&#25552;&#20379;&#20102;&#26032;&#30340;&#21551;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.03774</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#24863;&#30693;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#23433;&#20840;&#32771;&#34385;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Safety Concerns in Automated Driving Perception. (arXiv:2309.03774v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#23433;&#20840;&#32771;&#34385;&#20316;&#20026;&#32467;&#26500;&#20803;&#32032;&#65292;&#20197;&#31995;&#32479;&#32508;&#21512;&#30340;&#26041;&#24335;&#30830;&#20445;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;&#36825;&#19968;&#27010;&#24565;&#19981;&#20165;&#19982;&#29616;&#26377;&#30340;&#23433;&#20840;&#26631;&#20934;&#30456;&#22865;&#21512;&#65292;&#36824;&#20026;AI&#23433;&#20840;&#30456;&#20851;&#30340;&#23398;&#26415;&#20986;&#29256;&#29289;&#21644;&#26631;&#20934;&#25552;&#20379;&#20102;&#26032;&#30340;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#20197;&#21450;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#22312;&#24863;&#30693;&#26041;&#38754;&#30340;&#20986;&#33394;&#24615;&#33021;&#23548;&#33268;&#20102;&#23545;&#20854;&#22312;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#24212;&#29992;&#30340;&#22686;&#21152;&#38656;&#27714;&#12290;&#36825;&#31867;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#27492;&#38656;&#35201;&#32771;&#34385;DNNs&#30340;&#29420;&#29305;&#23646;&#24615;&#12290;&#20026;&#20102;&#20197;&#31995;&#32479;&#32508;&#21512;&#30340;&#26041;&#24335;&#30830;&#20445;&#22522;&#20110;DNNs&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#65292;&#24341;&#20837;&#20102;&#25152;&#35859;&#30340;&#23433;&#20840;&#32771;&#34385;&#20316;&#20026;&#36866;&#24403;&#30340;&#32467;&#26500;&#20803;&#32032;&#12290;&#19968;&#26041;&#38754;&#65292;&#23433;&#20840;&#32771;&#34385;&#30340;&#27010;&#24565;&#35774;&#35745;&#19982;&#29616;&#26377;&#30340;&#19982;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#23433;&#20840;&#30456;&#20851;&#30340;&#26631;&#20934;&#22914;ISO 21448&#65288;SOTIF&#65289;&#38750;&#24120;&#22865;&#21512;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23427;&#24050;&#32463;&#28608;&#21457;&#20102;&#20960;&#31687;&#23398;&#26415;&#20986;&#29256;&#29289;&#21644;&#21363;&#23558;&#20986;&#21488;&#30340;&#20851;&#20110;AI&#23433;&#20840;&#30340;&#26631;&#20934;&#65292;&#22914;ISO PAS 8800&#12290;&#34429;&#28982;&#23433;&#20840;&#32771;&#34385;&#30340;&#27010;&#24565;&#20197;&#21069;&#24050;&#32463;&#34987;&#20171;&#32461;&#36807;&#65292;&#20294;&#26412;&#25991;&#23545;&#20854;&#36827;&#34892;&#20102;&#25193;&#23637;&#21644;&#20248;&#21270;&#65292;&#20511;&#37492;&#20102;&#21508;&#20010;&#39046;&#22495;&#21644;&#23433;&#20840;&#19987;&#23478;&#30340;&#21453;&#39304;&#24847;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in the field of deep learning and impressive performance of deep neural networks (DNNs) for perception have resulted in an increased demand for their use in automated driving (AD) systems. The safety of such systems is of utmost importance and thus requires to consider the unique properties of DNNs.  In order to achieve safety of AD systems with DNN-based perception components in a systematic and comprehensive approach, so-called safety concerns have been introduced as a suitable structuring element. On the one hand, the concept of safety concerns is -- by design -- well aligned to existing standards relevant for safety of AD systems such as ISO 21448 (SOTIF). On the other hand, it has already inspired several academic publications and upcoming standards on AI safety such as ISO PAS 8800.  While the concept of safety concerns has been previously introduced, this paper extends and refines it, leveraging feedback from various domain and safety experts in the field. In par
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22810;&#38454;&#27573;&#20307;&#31995;&#32467;&#26500;&#21644;&#20004;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#19987;&#38376;&#29992;&#20110;&#22522;&#20110;&#36816;&#21160;&#23398;&#25968;&#25454;&#30340;&#34892;&#21160;&#20998;&#21106;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#22312;&#19977;&#20010;&#25163;&#26415;&#32541;&#21512;&#20219;&#21153;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2303.07814</link><description>&lt;p&gt;
&#22522;&#20110;&#36816;&#21160;&#23398;&#25968;&#25454;&#30340;&#25163;&#26415;&#34892;&#20026;&#20999;&#20998;
&lt;/p&gt;
&lt;p&gt;
Kinematic Data-Based Action Segmentation for Surgical Applications. (arXiv:2303.07814v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22810;&#38454;&#27573;&#20307;&#31995;&#32467;&#26500;&#21644;&#20004;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#19987;&#38376;&#29992;&#20110;&#22522;&#20110;&#36816;&#21160;&#23398;&#25968;&#25454;&#30340;&#34892;&#21160;&#20998;&#21106;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#22312;&#19977;&#20010;&#25163;&#26415;&#32541;&#21512;&#20219;&#21153;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#21160;&#20999;&#20998;&#26159;&#39640;&#32423;&#27969;&#31243;&#20998;&#26512;&#20013;&#30340;&#19968;&#20010;&#25361;&#25112;&#24615;&#20219;&#21153;&#65292;&#36890;&#24120;&#22312;&#35270;&#39057;&#25110;&#20174;&#21508;&#31181;&#20256;&#24863;&#22120;&#33719;&#21462;&#30340;&#36816;&#21160;&#23398;&#25968;&#25454;&#19978;&#25191;&#34892;&#12290;&#22312;&#25163;&#26415;&#36807;&#31243;&#20013;&#65292;&#34892;&#21160;&#20999;&#20998;&#23545;&#20110;&#24037;&#20316;&#27969;&#20998;&#26512;&#31639;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#19982;&#36816;&#21160;&#23398;&#25968;&#25454;&#30456;&#20851;&#30340;&#34892;&#21160;&#20998;&#21106;&#26041;&#38754;&#30340;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#22810;&#38454;&#27573;&#20307;&#31995;&#32467;&#26500;&#65292;MS-TCN-BiLSTM&#21644;MS-TCN-BiGRU&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#36816;&#21160;&#23398;&#25968;&#25454;&#12290; &#36825;&#20123;&#20307;&#31995;&#32467;&#26500;&#30001;&#20855;&#26377;&#38454;&#20869;&#35268;&#21017;&#21270;&#21644;&#21452;&#21521;LSTM&#25110;GRU&#30340;&#32454;&#21270;&#38454;&#27573;&#30340;&#39044;&#27979;&#29983;&#25104;&#22120;&#32452;&#25104;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;World Frame Rotation&#21644;Horizontal-Flip&#65292;&#21033;&#29992;&#36816;&#21160;&#23398;&#25968;&#25454;&#30340;&#24378;&#20960;&#20309;&#32467;&#26500;&#26469;&#25552;&#39640;&#31639;&#27861;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25163;&#26415;&#32541;&#21512;&#20219;&#21153;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65306;&#21487;&#21464;&#32452;&#32455;&#27169;&#25311;&#65288;VTS&#65289;&#25968;&#25454;&#38598;&#21644;&#26032;&#25512;&#20986;&#30340;&#32928;&#36947;&#20462;&#22797;&#27169;&#25311;&#65288;BRS&#65289;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Action segmentation is a challenging task in high-level process analysis, typically performed on video or kinematic data obtained from various sensors. In the context of surgical procedures, action segmentation is critical for workflow analysis algorithms. This work presents two contributions related to action segmentation on kinematic data. Firstly, we introduce two multi-stage architectures, MS-TCN-BiLSTM and MS-TCN-BiGRU, specifically designed for kinematic data. The architectures consist of a prediction generator with intra-stage regularization and Bidirectional LSTM or GRU-based refinement stages. Secondly, we propose two new data augmentation techniques, World Frame Rotation and Horizontal-Flip, which utilize the strong geometric structure of kinematic data to improve algorithm performance and robustness. We evaluate our models on three datasets of surgical suturing tasks: the Variable Tissue Simulation (VTS) Dataset and the newly introduced Bowel Repair Simulation (BRS) Dataset,
&lt;/p&gt;</description></item><item><title>SynthMorph&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;DL&#24037;&#20855;&#65292;&#29992;&#20110;&#26080;&#38656;&#39044;&#22788;&#29702;&#21363;&#21487;&#30452;&#25509;&#20174;MRI&#25195;&#25551;&#20202;&#19978;&#23545;&#20219;&#20309;&#33041;&#22270;&#20687;&#36827;&#34892;&#32852;&#21512;&#20223;&#23556;-&#21487;&#21464;&#24418;&#37197;&#20934;&#65292;&#37319;&#29992;&#20102;&#20174;&#26631;&#31614;&#22270;&#29983;&#25104;&#20855;&#26377;&#26497;&#22823;&#24046;&#24322;&#22270;&#20687;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#21644;&#40065;&#26834;&#30340;&#22270;&#20687;&#37197;&#20934;&#12290;</title><link>http://arxiv.org/abs/2301.11329</link><description>&lt;p&gt;
SynthMorph&#23454;&#29616;&#30340;&#32771;&#34385;&#35299;&#21078;&#32467;&#26500;&#21644;&#26080;&#20851;&#37319;&#38598;&#26041;&#27861;&#30340;&#32852;&#21512;&#37197;&#20934;
&lt;/p&gt;
&lt;p&gt;
Anatomy-aware and acquisition-agnostic joint registration with SynthMorph. (arXiv:2301.11329v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11329
&lt;/p&gt;
&lt;p&gt;
SynthMorph&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;DL&#24037;&#20855;&#65292;&#29992;&#20110;&#26080;&#38656;&#39044;&#22788;&#29702;&#21363;&#21487;&#30452;&#25509;&#20174;MRI&#25195;&#25551;&#20202;&#19978;&#23545;&#20219;&#20309;&#33041;&#22270;&#20687;&#36827;&#34892;&#32852;&#21512;&#20223;&#23556;-&#21487;&#21464;&#24418;&#37197;&#20934;&#65292;&#37319;&#29992;&#20102;&#20174;&#26631;&#31614;&#22270;&#29983;&#25104;&#20855;&#26377;&#26497;&#22823;&#24046;&#24322;&#22270;&#20687;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#21644;&#40065;&#26834;&#30340;&#22270;&#20687;&#37197;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20223;&#23556;&#22270;&#20687;&#37197;&#20934;&#26159;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#22522;&#30707;&#12290;&#34429;&#28982;&#20256;&#32479;&#31639;&#27861;&#21487;&#20197;&#23454;&#29616;&#20248;&#31168;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#20026;&#27599;&#19968;&#23545;&#22270;&#20687;&#36827;&#34892;&#32791;&#26102;&#30340;&#20248;&#21270;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#23558;&#22270;&#20687;&#23545;&#26144;&#23556;&#21040;&#36755;&#20986;&#21464;&#25442;&#30340;&#20989;&#25968;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#35780;&#20272;&#36825;&#20010;&#20989;&#25968;&#26159;&#24555;&#36895;&#30340;&#65292;&#20294;&#25429;&#25417;&#22823;&#30340;&#21464;&#25442;&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#32780;&#19988;&#22914;&#26524;&#27979;&#35797;&#22270;&#20687;&#30340;&#29305;&#24449;&#20174;&#35757;&#32451;&#39046;&#22495;&#21464;&#21270;&#65292;&#22914;&#20998;&#36776;&#29575;&#65292;&#32593;&#32476;&#24448;&#24448;&#20250;&#20986;&#29616;&#22256;&#38590;&#12290;&#22823;&#22810;&#25968;&#20223;&#23556;&#26041;&#27861;&#26159;&#23545;&#35299;&#21078;&#32467;&#26500;&#26080;&#30693;&#30340;&#65292;&#24847;&#21619;&#30528;&#22914;&#26524;&#31639;&#27861;&#32771;&#34385;&#22270;&#20687;&#20013;&#30340;&#25152;&#26377;&#32467;&#26500;&#65292;&#37197;&#20934;&#20250;&#19981;&#20934;&#30830;&#12290;&#25105;&#20204;&#36890;&#36807;SynthMorph&#35299;&#20915;&#20102;&#36825;&#20123;&#32570;&#28857;&#65292;&#23427;&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;DL&#24037;&#20855;&#65292;&#29992;&#20110;&#23545;&#20219;&#20309;&#33041;&#22270;&#20687;&#36827;&#34892;&#32852;&#21512;&#20223;&#23556;-&#21487;&#21464;&#24418;&#37197;&#20934;&#65292;&#26080;&#38656;&#39044;&#22788;&#29702;&#21363;&#21487;&#30452;&#25509;&#20174;MRI&#25195;&#25551;&#20202;&#36827;&#34892;&#25805;&#20316;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#20174;&#26631;&#31614;&#22270;&#29983;&#25104;&#30340;&#20855;&#26377;&#26497;&#22823;&#24046;&#24322;&#30340;&#22270;&#20687;&#26469;&#35757;&#32451;&#32593;&#32476;&#30340;&#31574;&#30053;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#30340;&#22810;&#26679;&#21270;&#37319;&#38598;&#35268;&#33539;&#30340;&#40065;&#26834;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20248;&#21270;&#32593;&#32476;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20351;&#20854;&#33021;&#22815;&#32771;&#34385;&#19981;&#21516;&#30340;&#35299;&#21078;&#29305;&#24449;&#21644;&#23398;&#20064;&#25269;&#21046;&#37319;&#38598;&#29305;&#23450;&#38480;&#21046;&#30340;&#21464;&#25442;&#12290;&#36890;&#36807;&#36825;&#20123;&#21019;&#26032;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#21644;&#40065;&#26834;&#30340;&#22270;&#20687;&#37197;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Affine image registration is a cornerstone of medical-image analysis. While classical algorithms can achieve excellent accuracy, they solve a time-consuming optimization for every image pair. Deep-learning (DL) methods learn a function that maps an image pair to an output transform. Evaluating the function is fast, but capturing large transforms can be challenging, and networks tend to struggle if a test-image characteristic shifts from the training domain, such as resolution. Most affine methods are agnostic to anatomy, meaning the registration will be inaccurate if algorithms consider all structures in the image.  We address these shortcomings with SynthMorph, an easy-to-use DL tool for joint affine-deformable registration of any brain image without preprocessing, right off the MRI scanner. First, we leverage a strategy to train networks with wildly varying images synthesized from label maps, yielding robust performance across acquisition specifics unseen at training. Second, we opti
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#20102;&#35299;&#20915;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#25805;&#32437;&#20013;&#30340;&#21487;&#20998;&#31163;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25511;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#23450;&#20041;&#22522;&#20110;&#30456;&#20851;&#25552;&#31034;&#30340;&#35821;&#26009;&#24211;&#23376;&#31354;&#38388;&#26469;&#33719;&#21462;&#29305;&#23450;&#22270;&#20687;&#29305;&#24449;&#24182;&#24341;&#20837;CLIP&#25237;&#24433;&#22686;&#24378;&#23884;&#20837;&#65288;PAE&#65289;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#22788;&#29702;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.03919</link><description>&lt;p&gt;
CLIP-PAE&#65306;&#25237;&#24433;&#22686;&#24378;&#23884;&#20837;&#20197;&#25552;&#21462;&#30456;&#20851;&#29305;&#24449;&#29992;&#20110;&#21487;&#20998;&#31163;&#12289;&#21487;&#35299;&#37322;&#12289;&#21487;&#25511;&#30340;&#25991;&#26412;&#25351;&#23548;&#33080;&#37096;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
CLIP-PAE: Projection-Augmentation Embedding to Extract Relevant Features for a Disentangled, Interpretable, and Controllable Text-Guided Face Manipulation. (arXiv:2210.03919v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03919
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#20102;&#35299;&#20915;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#25805;&#32437;&#20013;&#30340;&#21487;&#20998;&#31163;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25511;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#23450;&#20041;&#22522;&#20110;&#30456;&#20851;&#25552;&#31034;&#30340;&#35821;&#26009;&#24211;&#23376;&#31354;&#38388;&#26469;&#33719;&#21462;&#29305;&#23450;&#22270;&#20687;&#29305;&#24449;&#24182;&#24341;&#20837;CLIP&#25237;&#24433;&#22686;&#24378;&#23884;&#20837;&#65288;PAE&#65289;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#22788;&#29702;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24341;&#20837;&#30340;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#23558;&#22270;&#20687;&#21644;&#25991;&#26412;&#23884;&#20837;&#21040;&#20849;&#21516;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;&#36825;&#25171;&#24320;&#20102;&#19968;&#20010;&#22823;&#38376;&#65292;&#21363;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#25991;&#23383;&#35828;&#26126;&#26469;&#25805;&#20316;&#36755;&#20837;&#22270;&#20687;&#30340;&#20016;&#23500;&#25991;&#23398;&#36164;&#26009;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32852;&#21512;&#31354;&#38388;&#20013;&#22270;&#20687;&#21644;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#23558;&#25991;&#26412;&#23884;&#20837;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#36890;&#24120;&#20250;&#23548;&#33268;&#32467;&#26524;&#22270;&#20687;&#20013;&#20986;&#29616;&#24847;&#22806;&#30340;&#20266;&#24433;&#12290;&#23545;&#20110;&#25805;&#32437;&#26469;&#35828;&#65292;&#21487;&#20998;&#31163;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25511;&#24615;&#20063;&#24456;&#38590;&#20445;&#35777;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#23450;&#20041;&#30001;&#30456;&#20851;&#25552;&#31034;&#23637;&#24320;&#30340;&#35821;&#26009;&#24211;&#23376;&#31354;&#38388;&#26469;&#25429;&#33719;&#29305;&#23450;&#30340;&#22270;&#20687;&#29305;&#24449;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;CLIP&#25237;&#24433;&#22686;&#24378;&#23884;&#20837;&#65288;PAE&#65289;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#65292;&#20197;&#25552;&#39640;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#25805;&#32437;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#33539;&#20363;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#35745;&#31639;&#21644;&#36866;&#24212;&#65292;&#24182;&#24179;&#31283;&#22320;&#34701;&#20837;&#21040;&#20219;&#20309;&#22522;&#20110;CLIP&#30340;&#22270;&#20687;&#25805;&#20316;&#31639;&#27861;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently introduced Contrastive Language-Image Pre-Training (CLIP) bridges images and text by embedding them into a joint latent space. This opens the door to ample literature that aims to manipulate an input image by providing a textual explanation. However, due to the discrepancy between image and text embeddings in the joint space, using text embeddings as the optimization target often introduces undesired artifacts in the resulting images. Disentanglement, interpretability, and controllability are also hard to guarantee for manipulation. To alleviate these problems, we propose to define corpus subspaces spanned by relevant prompts to capture specific image characteristics. We introduce CLIP Projection-Augmentation Embedding (PAE) as an optimization target to improve the performance of text-guided image manipulation. Our method is a simple and general paradigm that can be easily computed and adapted, and smoothly incorporated into any CLIP-based image manipulation algorithm. To demo
&lt;/p&gt;</description></item></channel></rss>