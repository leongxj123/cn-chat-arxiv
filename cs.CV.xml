<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#25552;&#20986;&#19968;&#31181;&#24179;&#22343;L1&#26657;&#20934;&#35823;&#24046;&#65288;mL1-ACE&#65289;&#20316;&#20026;&#36741;&#21161;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25913;&#21892;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#20687;&#32032;&#32423;&#26657;&#20934;&#65292;&#20943;&#23569;&#20102;&#26657;&#20934;&#35823;&#24046;&#24182;&#24341;&#20837;&#20102;&#25968;&#25454;&#38598;&#21487;&#38752;&#24615;&#30452;&#26041;&#22270;&#20197;&#25552;&#39640;&#26657;&#20934;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.06759</link><description>&lt;p&gt;
&#24179;&#22343;&#26657;&#20934;&#35823;&#24046;&#65306;&#19968;&#31181;&#21487;&#24494;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25913;&#21892;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Average Calibration Error: A Differentiable Loss for Improved Reliability in Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06759
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#24179;&#22343;L1&#26657;&#20934;&#35823;&#24046;&#65288;mL1-ACE&#65289;&#20316;&#20026;&#36741;&#21161;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25913;&#21892;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#20687;&#32032;&#32423;&#26657;&#20934;&#65292;&#20943;&#23569;&#20102;&#26657;&#20934;&#35823;&#24046;&#24182;&#24341;&#20837;&#20102;&#25968;&#25454;&#38598;&#21487;&#38752;&#24615;&#30452;&#26041;&#22270;&#20197;&#25552;&#39640;&#26657;&#20934;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32463;&#24120;&#20135;&#29983;&#19982;&#32463;&#39564;&#35266;&#23519;&#19981;&#19968;&#33268;&#30340;&#36807;&#20110;&#33258;&#20449;&#30340;&#32467;&#26524;&#65292;&#36825;&#31181;&#26657;&#20934;&#38169;&#35823;&#25361;&#25112;&#30528;&#23427;&#20204;&#30340;&#20020;&#24202;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#24179;&#22343;L1&#26657;&#20934;&#35823;&#24046;&#65288;mL1-ACE&#65289;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#30340;&#36741;&#21161;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#25913;&#21892;&#20687;&#32032;&#32423;&#26657;&#20934;&#32780;&#19981;&#20250;&#25439;&#23475;&#20998;&#21106;&#36136;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#23613;&#31649;&#20351;&#29992;&#30828;&#20998;&#31665;&#65292;&#36825;&#31181;&#25439;&#22833;&#26159;&#30452;&#25509;&#21487;&#24494;&#30340;&#65292;&#36991;&#20813;&#20102;&#38656;&#35201;&#36817;&#20284;&#20294;&#21487;&#24494;&#30340;&#26367;&#20195;&#25110;&#36719;&#20998;&#31665;&#26041;&#27861;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36824;&#24341;&#20837;&#20102;&#25968;&#25454;&#38598;&#21487;&#38752;&#24615;&#30452;&#26041;&#22270;&#30340;&#27010;&#24565;&#65292;&#36825;&#19968;&#27010;&#24565;&#25512;&#24191;&#20102;&#26631;&#20934;&#30340;&#21487;&#38752;&#24615;&#22270;&#65292;&#29992;&#20110;&#22312;&#25968;&#25454;&#38598;&#32423;&#21035;&#32858;&#21512;&#30340;&#35821;&#20041;&#20998;&#21106;&#20013;&#32454;&#21270;&#26657;&#20934;&#30340;&#35270;&#35273;&#35780;&#20272;&#12290;&#20351;&#29992;mL1-ACE&#65292;&#25105;&#20204;&#23558;&#24179;&#22343;&#21644;&#26368;&#22823;&#26657;&#20934;&#35823;&#24046;&#20998;&#21035;&#38477;&#20302;&#20102;45%&#21644;55%&#65292;&#21516;&#26102;&#22312;BraTS 2021&#25968;&#25454;&#38598;&#19978;&#20445;&#25345;&#20102;87%&#30340;Dice&#20998;&#25968;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#20998;&#20139;&#25105;&#20204;&#30340;&#20195;&#30721;: https://github
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06759v1 Announce Type: cross  Abstract: Deep neural networks for medical image segmentation often produce overconfident results misaligned with empirical observations. Such miscalibration, challenges their clinical translation. We propose to use marginal L1 average calibration error (mL1-ACE) as a novel auxiliary loss function to improve pixel-wise calibration without compromising segmentation quality. We show that this loss, despite using hard binning, is directly differentiable, bypassing the need for approximate but differentiable surrogate or soft binning approaches. Our work also introduces the concept of dataset reliability histograms which generalises standard reliability diagrams for refined visual assessment of calibration in semantic segmentation aggregated at the dataset level. Using mL1-ACE, we reduce average and maximum calibration error by 45% and 55% respectively, maintaining a Dice score of 87% on the BraTS 2021 dataset. We share our code here: https://github
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#22330;&#26223;&#32972;&#26223;&#21644;&#35266;&#23519;&#21040;&#30340;&#36712;&#36857;&#20449;&#24687;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34892;&#20154;&#30446;&#26631;&#21306;&#22495;&#30340;&#36712;&#36857;&#39044;&#27979;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#23558;&#19981;&#30830;&#23450;&#24615;&#38480;&#21046;&#22312;&#20960;&#20010;&#30446;&#26631;&#21306;&#22495;&#20869;&#12290;</title><link>https://arxiv.org/abs/2402.19002</link><description>&lt;p&gt;
GoalNet: &#38754;&#21521;&#30446;&#26631;&#21306;&#22495;&#30340;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
GoalNet: Goal Areas Oriented Pedestrian Trajectory Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19002
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22330;&#26223;&#32972;&#26223;&#21644;&#35266;&#23519;&#21040;&#30340;&#36712;&#36857;&#20449;&#24687;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34892;&#20154;&#30446;&#26631;&#21306;&#22495;&#30340;&#36712;&#36857;&#39044;&#27979;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#23558;&#19981;&#30830;&#23450;&#24615;&#38480;&#21046;&#22312;&#20960;&#20010;&#30446;&#26631;&#21306;&#22495;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#36947;&#36335;&#19978;&#34892;&#20154;&#26410;&#26469;&#30340;&#36712;&#36857;&#26159;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#21463;&#22330;&#26223;&#36335;&#24452;&#12289;&#34892;&#20154;&#24847;&#22270;&#21644;&#20915;&#31574;&#24433;&#21709;&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#22823;&#22810;&#20351;&#29992;&#36807;&#21435;&#30340;&#36712;&#36857;&#26469;&#39044;&#27979;&#21508;&#31181;&#28508;&#22312;&#30340;&#26410;&#26469;&#36712;&#36857;&#20998;&#24067;&#65292;&#36825;&#24182;&#26410;&#32771;&#34385;&#22330;&#26223;&#32972;&#26223;&#21644;&#34892;&#20154;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30452;&#25509;&#39044;&#27979;&#26410;&#26469;&#36712;&#36857;&#30340;&#26041;&#27861;&#65292;&#21363;&#39318;&#20808;&#20351;&#29992;&#22330;&#26223;&#32972;&#26223;&#21644;&#35266;&#23519;&#21040;&#30340;&#36712;&#36857;&#26469;&#39044;&#27979;&#30446;&#26631;&#28857;&#65292;&#28982;&#21518;&#37325;&#22797;&#20351;&#29992;&#30446;&#26631;&#28857;&#26469;&#39044;&#27979;&#26410;&#26469;&#36712;&#36857;&#12290;&#36890;&#36807;&#21033;&#29992;&#22330;&#26223;&#32972;&#26223;&#21644;&#35266;&#23519;&#21040;&#30340;&#36712;&#36857;&#20449;&#24687;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#19981;&#30830;&#23450;&#24615;&#38480;&#21046;&#22312;&#20960;&#20010;&#30446;&#26631;&#21306;&#22495;&#20869;&#65292;&#36825;&#20123;&#21306;&#22495;&#20195;&#34920;&#20102;&#34892;&#20154;&#30340;&#8220;&#30446;&#26631;&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GoalNet&#65292;&#19968;&#31181;&#22522;&#20110;&#34892;&#20154;&#30446;&#26631;&#21306;&#22495;&#30340;&#26032;&#36712;&#36857;&#39044;&#27979;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19002v1 Announce Type: cross  Abstract: Predicting the future trajectories of pedestrians on the road is an important task for autonomous driving. The pedestrian trajectory prediction is affected by scene paths, pedestrian's intentions and decision-making, which is a multi-modal problem. Most recent studies use past trajectories to predict a variety of potential future trajectory distributions, which do not account for the scene context and pedestrian targets. Instead of predicting the future trajectory directly, we propose to use scene context and observed trajectory to predict the goal points first, and then reuse the goal points to predict the future trajectories. By leveraging the information from scene context and observed trajectory, the uncertainty can be limited to a few target areas, which represent the "goals" of the pedestrians. In this paper, we propose GoalNet, a new trajectory prediction neural network based on the goal areas of a pedestrian. Our network can pr
&lt;/p&gt;</description></item><item><title>SDR-GAIN&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#34892;&#20154;&#23039;&#24577;&#20013;&#37096;&#20998;&#36974;&#25377;&#38382;&#39064;&#30340;&#20851;&#38190;&#28857;&#34917;&#20840;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23545;&#19981;&#23436;&#25972;&#30340;&#20851;&#38190;&#28857;&#36827;&#34892;&#38477;&#32500;&#65292;&#32479;&#19968;&#29305;&#24449;&#20998;&#24067;&#65292;&#24182;&#20351;&#29992;GAN&#26694;&#26550;&#30340;&#20004;&#31181;&#29983;&#25104;&#27169;&#22411;&#26469;&#23436;&#25104;&#23039;&#24577;&#30340;&#34917;&#20840;&#12290;&#35813;&#26041;&#27861;&#30340;&#23454;&#39564;&#34920;&#26126;&#24615;&#33021;&#20248;&#20110;&#22522;&#26412;&#30340;GAIN&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2306.03538</link><description>&lt;p&gt;
SDR-GAIN&#65306;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#39640;&#23454;&#26102;&#36974;&#25377;&#34892;&#20154;&#23039;&#24577;&#23436;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SDR-GAIN: A High Real-Time Occluded Pedestrian Pose Completion Method for Autonomous Driving. (arXiv:2306.03538v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03538
&lt;/p&gt;
&lt;p&gt;
SDR-GAIN&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#34892;&#20154;&#23039;&#24577;&#20013;&#37096;&#20998;&#36974;&#25377;&#38382;&#39064;&#30340;&#20851;&#38190;&#28857;&#34917;&#20840;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23545;&#19981;&#23436;&#25972;&#30340;&#20851;&#38190;&#28857;&#36827;&#34892;&#38477;&#32500;&#65292;&#32479;&#19968;&#29305;&#24449;&#20998;&#24067;&#65292;&#24182;&#20351;&#29992;GAN&#26694;&#26550;&#30340;&#20004;&#31181;&#29983;&#25104;&#27169;&#22411;&#26469;&#23436;&#25104;&#23039;&#24577;&#30340;&#34917;&#20840;&#12290;&#35813;&#26041;&#27861;&#30340;&#23454;&#39564;&#34920;&#26126;&#24615;&#33021;&#20248;&#20110;&#22522;&#26412;&#30340;GAIN&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#32531;&#35299;&#22522;&#20110;&#20154;&#20307;&#23039;&#24577;&#20851;&#38190;&#28857;&#30340;&#34892;&#20154;&#26816;&#27979;&#31639;&#27861;&#20013;&#37096;&#20998;&#36974;&#25377;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20998;&#31163;&#21644;&#38477;&#32500;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#24615;&#34917;&#20840;&#32593;&#32476;(SDR-GAIN)&#30340;&#26032;&#22411;&#34892;&#20154;&#23039;&#21183;&#20851;&#38190;&#28857;&#34917;&#20840;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;OpenPose&#22312;&#22270;&#20687;&#20013;&#20272;&#35745;&#34892;&#20154;&#30340;&#23039;&#24577;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#30001;&#20110;&#36974;&#25377;&#25110;&#20854;&#20182;&#22240;&#32032;&#32780;&#19981;&#23436;&#25972;&#30340;&#34892;&#20154;&#22836;&#37096;&#21644;&#36527;&#24178;&#20851;&#38190;&#28857;&#36827;&#34892;&#32500;&#24230;&#32553;&#20943;&#65292;&#20197;&#22686;&#24378;&#29305;&#24449;&#24182;&#36827;&#19968;&#27493;&#32479;&#19968;&#29305;&#24449;&#20998;&#24067;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GAN)&#26694;&#26550;&#30340;&#20004;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#34701;&#21512;&#20102;Huber&#25439;&#22833;&#12289;&#27531;&#24046;&#32467;&#26500;&#21644;L1&#27491;&#21017;&#21270;&#26469;&#29983;&#25104;&#37096;&#20998;&#36974;&#25377;&#34892;&#20154;&#19981;&#23436;&#25972;&#22836;&#37096;&#21644;&#36527;&#24178;&#23039;&#24577;&#20851;&#38190;&#28857;&#30340;&#32570;&#22833;&#37096;&#20998;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23039;&#24577;&#34917;&#20840;&#12290;&#25105;&#20204;&#22312;MS COCO&#21644;JAAD&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SDR-GAIN&#30340;&#24615;&#33021;&#20248;&#20110;&#22522;&#26412;&#30340;GAIN&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
To mitigate the challenges arising from partial occlusion in human pose keypoint based pedestrian detection methods , we present a novel pedestrian pose keypoint completion method called the separation and dimensionality reduction-based generative adversarial imputation networks (SDR-GAIN) . Firstly, we utilize OpenPose to estimate pedestrian poses in images. Then, we isolate the head and torso keypoints of pedestrians with incomplete keypoints due to occlusion or other factors and perform dimensionality reduction to enhance features and further unify feature distribution. Finally, we introduce two generative models based on the generative adversarial networks (GAN) framework, which incorporate Huber loss, residual structure, and L1 regularization to generate missing parts of the incomplete head and torso pose keypoints of partially occluded pedestrians, resulting in pose completion. Our experiments on MS COCO and JAAD datasets demonstrate that SDR-GAIN outperforms basic GAIN framework
&lt;/p&gt;</description></item></channel></rss>