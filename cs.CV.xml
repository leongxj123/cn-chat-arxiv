<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#36234;&#29425;&#25915;&#20987;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#25554;&#20837;&#24694;&#24847;&#25991;&#26412;&#25552;&#31034;&#65292;&#25104;&#21151;&#23454;&#26045;&#36234;&#29425;&#25915;&#20987;&#65292;&#24182;&#20998;&#26512;&#20102;&#26377;&#27602;&#25968;&#25454;&#27604;&#29575;&#21644;&#21487;&#35757;&#32451;&#21442;&#25968;&#20301;&#32622;&#23545;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.02910</link><description>&lt;p&gt;
ImgTrojan: &#29992;&#19968;&#24352;&#22270;&#29255;&#23545;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36234;&#29425;
&lt;/p&gt;
&lt;p&gt;
ImgTrojan: Jailbreaking Vision-Language Models with ONE Image
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#36234;&#29425;&#25915;&#20987;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#25554;&#20837;&#24694;&#24847;&#25991;&#26412;&#25552;&#31034;&#65292;&#25104;&#21151;&#23454;&#26045;&#36234;&#29425;&#25915;&#20987;&#65292;&#24182;&#20998;&#26512;&#20102;&#26377;&#27602;&#25968;&#25454;&#27604;&#29575;&#21644;&#21487;&#35757;&#32451;&#21442;&#25968;&#20301;&#32622;&#23545;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#23545;&#40784;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#19982;&#35270;&#35273;&#27169;&#22359;&#38598;&#25104;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#21363;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#65292;&#20173;&#28982;&#30456;&#23545;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;VLMs&#30340;&#26032;&#22411;&#36234;&#29425;&#25915;&#20987;&#65292;&#26088;&#22312;&#24403;&#29992;&#25143;&#36755;&#20837;&#26377;&#23475;&#25351;&#20196;&#26102;&#32469;&#36807;&#20854;&#23433;&#20840;&#38459;&#30861;&#12290;&#20551;&#35774;&#25105;&#20204;&#30340;&#26377;&#27602;&#65288;&#22270;&#20687;&#65292;&#25991;&#26412;&#65289;&#25968;&#25454;&#23545;&#21253;&#21547;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#12290;&#36890;&#36807;&#29992;&#24694;&#24847;&#36234;&#29425;&#25552;&#31034;&#26367;&#25442;&#21407;&#22987;&#25991;&#26412;&#26631;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;&#26377;&#27602;&#22270;&#20687;&#25191;&#34892;&#36234;&#29425;&#25915;&#20987;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#26377;&#27602;&#27604;&#29575;&#21644;&#21487;&#35757;&#32451;&#21442;&#25968;&#20301;&#32622;&#23545;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35780;&#20272;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#25105;&#20204;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#21644;&#38544;&#34109;&#24615;&#12290;&#32467;&#21512;&#19968;&#31995;&#21015;&#31574;&#21010;&#30340;&#26377;&#23475;&#25351;&#20196;&#65292;&#21487;&#20197;&#34913;&#37327;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02910v1 Announce Type: cross  Abstract: There has been an increasing interest in the alignment of large language models (LLMs) with human values. However, the safety issues of their integration with a vision module, or vision language models (VLMs), remain relatively underexplored. In this paper, we propose a novel jailbreaking attack against VLMs, aiming to bypass their safety barrier when a user inputs harmful instructions. A scenario where our poisoned (image, text) data pairs are included in the training data is assumed. By replacing the original textual captions with malicious jailbreak prompts, our method can perform jailbreak attacks with the poisoned images. Moreover, we analyze the effect of poison ratios and positions of trainable parameters on our attack's success rate. For evaluation, we design two metrics to quantify the success rate and the stealthiness of our attack. Together with a list of curated harmful instructions, a benchmark for measuring attack efficac
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;NAS&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#25628;&#32034;&#36816;&#34892;&#20013;&#32534;&#30721;&#29992;&#25143;&#23545;&#24615;&#33021;&#21644;&#30828;&#20214;&#25351;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#20559;&#22909;&#65292;&#29983;&#25104;&#31934;&#24515;&#36873;&#25321;&#30340;&#22810;&#35774;&#22791;&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.18213</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#21487;&#24494;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multi-objective Differentiable Neural Architecture Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18213
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;NAS&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#25628;&#32034;&#36816;&#34892;&#20013;&#32534;&#30721;&#29992;&#25143;&#23545;&#24615;&#33021;&#21644;&#30828;&#20214;&#25351;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#20559;&#22909;&#65292;&#29983;&#25104;&#31934;&#24515;&#36873;&#25321;&#30340;&#22810;&#35774;&#22791;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#20013;&#30340;Pareto&#21069;&#27839;&#36718;&#24275;&#21078;&#26512;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#23588;&#20854;&#26159;&#22312;&#20687;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36825;&#26679;&#30340;&#26114;&#36149;&#30446;&#26631;&#20013;&#12290; &#30456;&#23545;&#20110;&#20256;&#32479;&#30340;NAS&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;NAS&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#19968;&#20010;&#25628;&#32034;&#36816;&#34892;&#20013;&#32534;&#30721;&#29992;&#25143;&#23545;&#24615;&#33021;&#21644;&#30828;&#20214;&#25351;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#20559;&#22909;&#65292;&#24182;&#29983;&#25104;&#31934;&#24515;&#36873;&#25321;&#30340;&#22810;&#35774;&#22791;&#26550;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#36229;&#32593;&#32476;&#21442;&#25968;&#21270;&#36328;&#22810;&#20010;&#35774;&#22791;&#21644;&#22810;&#20010;&#30446;&#26631;&#30340;&#32852;&#21512;&#26550;&#26500;&#20998;&#24067;&#65292;&#36229;&#32593;&#32476;&#21487;&#20197;&#26681;&#25454;&#30828;&#20214;&#29305;&#24449;&#21644;&#20559;&#22909;&#21521;&#37327;&#36827;&#34892;&#26465;&#20214;&#21270;&#65292;&#23454;&#29616;&#38646;&#27425;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18213v1 Announce Type: new  Abstract: Pareto front profiling in multi-objective optimization (MOO), i.e. finding a diverse set of Pareto optimal solutions, is challenging, especially with expensive objectives like neural network training. Typically, in MOO neural architecture search (NAS), we aim to balance performance and hardware metrics across devices. Prior NAS approaches simplify this task by incorporating hardware constraints into the objective function, but profiling the Pareto front necessitates a search for each constraint. In this work, we propose a novel NAS algorithm that encodes user preferences for the trade-off between performance and hardware metrics, and yields representative and diverse architectures across multiple devices in just one search run. To this end, we parameterize the joint architectural distribution across devices and multiple objectives via a hypernetwork that can be conditioned on hardware features and preference vectors, enabling zero-shot t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#27963;&#20307;&#32454;&#32990;&#26174;&#24494;&#38236;&#25429;&#25417;&#21040;&#30340;&#20116;&#32500;&#35270;&#39057;&#20013;&#23547;&#25214;&#32454;&#32990;&#20449;&#21495;&#21160;&#21147;&#23398;&#26102;&#31354;&#27169;&#24335;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20219;&#20309;&#20808;&#39564;&#30340;&#39044;&#26399;&#27169;&#24335;&#21160;&#21147;&#23398;&#21644;&#35757;&#32451;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#32454;&#32990;&#20449;&#21495;&#32467;&#26500;&#20989;&#25968;&#65288;SSF&#65289;&#65292;&#36890;&#36807;&#27979;&#37327;&#32454;&#32990;&#20449;&#21495;&#29366;&#24577;&#21644;&#21608;&#22260;&#32454;&#32990;&#36136;&#20043;&#38388;&#30340;&#26680;&#31958;&#20307;&#24378;&#24230;&#65292;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26680;&#31958;&#20307;&#19982;&#32454;&#32990;&#26680;&#27604;&#20540;&#30456;&#27604;&#26377;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#36890;&#36807;&#24402;&#19968;&#21270;&#21387;&#32553;&#36317;&#31163;&#65288;NCD&#65289;&#26469;&#35782;&#21035;&#30456;&#20284;&#30340;&#27169;&#24335;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#23558;&#36755;&#20837;&#30340;SSF&#26500;&#22270;&#34920;&#31034;&#20026;&#20302;&#32500;&#23884;&#20837;&#20013;&#30340;&#28857;&#65292;&#26368;&#20248;&#22320;&#25429;&#25417;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.02501</link><description>&lt;p&gt;
&#32454;&#32990;&#20449;&#21495;&#20256;&#23548;&#32467;&#26500;&#21644;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;
The cell signaling structure function. (arXiv:2401.02501v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02501
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#27963;&#20307;&#32454;&#32990;&#26174;&#24494;&#38236;&#25429;&#25417;&#21040;&#30340;&#20116;&#32500;&#35270;&#39057;&#20013;&#23547;&#25214;&#32454;&#32990;&#20449;&#21495;&#21160;&#21147;&#23398;&#26102;&#31354;&#27169;&#24335;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20219;&#20309;&#20808;&#39564;&#30340;&#39044;&#26399;&#27169;&#24335;&#21160;&#21147;&#23398;&#21644;&#35757;&#32451;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#32454;&#32990;&#20449;&#21495;&#32467;&#26500;&#20989;&#25968;&#65288;SSF&#65289;&#65292;&#36890;&#36807;&#27979;&#37327;&#32454;&#32990;&#20449;&#21495;&#29366;&#24577;&#21644;&#21608;&#22260;&#32454;&#32990;&#36136;&#20043;&#38388;&#30340;&#26680;&#31958;&#20307;&#24378;&#24230;&#65292;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26680;&#31958;&#20307;&#19982;&#32454;&#32990;&#26680;&#27604;&#20540;&#30456;&#27604;&#26377;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#36890;&#36807;&#24402;&#19968;&#21270;&#21387;&#32553;&#36317;&#31163;&#65288;NCD&#65289;&#26469;&#35782;&#21035;&#30456;&#20284;&#30340;&#27169;&#24335;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#23558;&#36755;&#20837;&#30340;SSF&#26500;&#22270;&#34920;&#31034;&#20026;&#20302;&#32500;&#23884;&#20837;&#20013;&#30340;&#28857;&#65292;&#26368;&#20248;&#22320;&#25429;&#25417;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27963;&#20307;&#32454;&#32990;&#26174;&#24494;&#38236;&#25429;&#25417;&#21040;&#30340;&#20116;&#32500;$(x,y,z,channel,time)$&#35270;&#39057;&#26174;&#31034;&#20102;&#32454;&#32990;&#36816;&#21160;&#21644;&#20449;&#21495;&#21160;&#21147;&#23398;&#30340;&#27169;&#24335;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20986;&#19968;&#31181;&#22312;&#20116;&#32500;&#27963;&#20307;&#32454;&#32990;&#26174;&#24494;&#38236;&#35270;&#39057;&#20013;&#23547;&#25214;&#32454;&#32990;&#20449;&#21495;&#21160;&#21147;&#23398;&#26102;&#31354;&#27169;&#24335;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#19981;&#38656;&#35201;&#39044;&#20808;&#20102;&#35299;&#39044;&#26399;&#30340;&#27169;&#24335;&#21160;&#21147;&#23398;&#20197;&#21450;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#12290;&#25152;&#25552;&#20986;&#30340;&#32454;&#32990;&#20449;&#21495;&#32467;&#26500;&#20989;&#25968;&#65288;SSF&#65289;&#26159;&#19968;&#31181;Kolmogorov&#32467;&#26500;&#20989;&#25968;&#65292;&#21487;&#20197;&#36890;&#36807;&#26680;&#24515;&#21306;&#22495;&#30456;&#23545;&#20110;&#21608;&#22260;&#32454;&#32990;&#36136;&#30340;&#26680;&#31958;&#20307;&#24378;&#24230;&#26469;&#26368;&#20248;&#22320;&#27979;&#37327;&#32454;&#32990;&#20449;&#21495;&#29366;&#24577;&#65292;&#30456;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26680;&#31958;&#20307;&#19982;&#32454;&#32990;&#26680;&#27604;&#20540;&#26377;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#36890;&#36807;&#24230;&#37327;&#24402;&#19968;&#21270;&#21387;&#32553;&#36317;&#31163;&#65288;NCD&#65289;&#26469;&#35782;&#21035;&#30456;&#20284;&#30340;&#27169;&#24335;&#12290;NCD&#26159;&#19968;&#20010;&#29992;&#20110;&#34920;&#31034;&#36755;&#20837;&#30340;SSF&#26500;&#22270;&#22312;&#20302;&#32500;&#23884;&#20837;&#20013;&#20316;&#20026;&#28857;&#30340;Hilbert&#31354;&#38388;&#30340;&#20877;&#29983;&#26680;&#65292;&#21487;&#20197;&#26368;&#20248;&#22320;&#25429;&#25417;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Live cell microscopy captures 5-D $(x,y,z,channel,time)$ movies that display patterns of cellular motion and signaling dynamics. We present here an approach to finding spatiotemporal patterns of cell signaling dynamics in 5-D live cell microscopy movies unique in requiring no \emph{a priori} knowledge of expected pattern dynamics, and no training data. The proposed cell signaling structure function (SSF) is a Kolmogorov structure function that optimally measures cell signaling state as nuclear intensity w.r.t. surrounding cytoplasm, a significant improvement compared to the current state-of-the-art cytonuclear ratio. SSF kymographs store at each spatiotemporal cell centroid the SSF value, or a functional output such as velocity. Patterns of similarity are identified via the metric normalized compression distance (NCD). The NCD is a reproducing kernel for a Hilbert space that represents the input SSF kymographs as points in a low dimensional embedding that optimally captures the pattern
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32508;&#21512;&#22797;&#26434;&#30340;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#65292;&#21487;&#20197;&#26377;&#25928;&#25913;&#21892;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35780;&#20272;&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21019;&#24314;&#39640;&#36136;&#37327;&#22797;&#26434;&#35270;&#35273;&#25512;&#29702;&#25351;&#20196;&#30340;&#31995;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.01487</link><description>&lt;p&gt;
&#20248;&#31168;&#30340;&#35270;&#35273;&#25351;&#23548;&#26377;&#20160;&#20040;&#29305;&#28857;&#65311;&#32508;&#21512;&#22797;&#26434;&#30340;&#35270;&#35273;&#25512;&#29702;&#25351;&#20196;&#29992;&#20110;&#35270;&#35273;&#25351;&#23548;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
What Makes for Good Visual Instructions? Synthesizing Complex Visual Reasoning Instructions for Visual Instruction Tuning. (arXiv:2311.01487v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01487
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32508;&#21512;&#22797;&#26434;&#30340;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#65292;&#21487;&#20197;&#26377;&#25928;&#25913;&#21892;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35780;&#20272;&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21019;&#24314;&#39640;&#36136;&#37327;&#22797;&#26434;&#35270;&#35273;&#25512;&#29702;&#25351;&#20196;&#30340;&#31995;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25351;&#23548;&#35843;&#25972;&#26159;&#25552;&#39640;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#30340;&#37325;&#35201;&#26041;&#27861;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#35768;&#22810;&#30528;&#30524;&#20110;&#19981;&#21516;&#28966;&#28857;&#21644;&#29305;&#24449;&#30340;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#38598;&#65292;&#20351;&#24471;MLLMs&#22312;&#35780;&#20272;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#24320;&#21457;&#26356;&#24378;&#22823;&#30340;MLLMs&#65292;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#19968;&#20010;&#26356;&#22522;&#26412;&#30340;&#38382;&#39064;&#65306;&#8220;&#20160;&#20040;&#26679;&#30340;&#35270;&#35273;&#25351;&#23548;&#25165;&#26159;&#22909;&#30340;&#65311;&#8221;&#36890;&#36807;&#36827;&#34892;&#20840;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#20391;&#37325;&#20110;&#22797;&#26434;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#30340;&#25351;&#23548;&#23545;&#20110;&#25913;&#21892;MLLMs&#22312;&#35780;&#20272;&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#29305;&#21035;&#26377;&#25928;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#33258;&#21160;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#22797;&#26434;&#35270;&#35273;&#25512;&#29702;&#25351;&#20196;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#21512;&#25104;-&#22797;&#26434;&#21270;-&#37325;&#26500;&#30340;&#33539;&#24335;&#65292;&#21033;&#29992;&#22810;&#20010;&#38454;&#27573;&#36880;&#28176;&#22686;&#21152;&#25351;&#20196;&#30340;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#20445;&#35777;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual instruction tuning is an essential approach to improving the zero-shot generalization capability of Multi-modal Large Language Models (MLLMs). A surge of visual instruction datasets with various focuses and characteristics have been proposed recently, enabling MLLMs to achieve surprising results on evaluation benchmarks. To develop more capable MLLMs, in this paper, we aim to investigate a more fundamental question: ``what makes for good visual instructions?''. By conducting a comprehensive empirical study, we find that instructions focused on complex visual reasoning tasks are particularly effective in improving the performance of MLLMs on evaluation benchmarks. Building upon this finding, we design a systematic approach to automatically creating high-quality complex visual reasoning instructions. Our approach employs a synthesis-complication-reformulation paradigm, leveraging multiple stages to gradually increase the complexity of the instructions while guaranteeing quality. B
&lt;/p&gt;</description></item></channel></rss>