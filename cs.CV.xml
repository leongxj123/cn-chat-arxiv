<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#20351;&#29992;ProtoVerse&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#21407;&#22411;&#35774;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#26894;&#20307;&#39592;&#25240;&#30340;&#20998;&#31867;&#20915;&#31574;&#65292;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#21407;&#22411;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.02830</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#31867;&#21407;&#22411;&#25552;&#21319;&#26894;&#20307;&#39592;&#25240;&#20998;&#32423;&#30340;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Interpretability of Vertebrae Fracture Grading using Human-interpretable Prototypes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02830
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;ProtoVerse&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#21407;&#22411;&#35774;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#26894;&#20307;&#39592;&#25240;&#30340;&#20998;&#31867;&#20915;&#31574;&#65292;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#21407;&#22411;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26894;&#20307;&#39592;&#25240;&#20998;&#32423;&#20998;&#31867;&#39592;&#25240;&#20005;&#37325;&#31243;&#24230;&#65292;&#36825;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36817;&#24180;&#26469;&#21560;&#24341;&#20102;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#12290;&#23613;&#31649;DL&#36741;&#21161;&#21307;&#23398;&#35786;&#26029;&#31561;&#20851;&#38190;&#24212;&#29992;&#22330;&#26223;&#38656;&#35201;&#36879;&#26126;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#24037;&#20316;&#23581;&#35797;&#20351;&#36825;&#31181;&#27169;&#22411;&#20855;&#26377;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#35201;&#20040;&#20381;&#36182;&#20110;&#20107;&#21518;&#26041;&#27861;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#39069;&#22806;&#27880;&#37322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;-by-design&#26041;&#27861;ProtoVerse&#65292;&#20197;&#22312;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#26041;&#24335;&#20013;&#25214;&#21040;&#30456;&#20851;&#30340;&#26894;&#20307;&#39592;&#25240;&#23376;&#37096;&#20998;&#65288;&#21407;&#22411;&#65289;&#65292;&#21487;&#21487;&#38752;&#22320;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26679;&#24615;&#20419;&#36827;&#25439;&#22833;&#65292;&#20197;&#20943;&#36731;&#22312;&#20855;&#26377;&#22797;&#26434;&#35821;&#20041;&#30340;&#23567;&#25968;&#25454;&#38598;&#20013;&#21407;&#22411;&#37325;&#22797;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;VerSe'19&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#21407;&#22411;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02830v1 Announce Type: cross  Abstract: Vertebral fracture grading classifies the severity of vertebral fractures, which is a challenging task in medical imaging and has recently attracted Deep Learning (DL) models. Only a few works attempted to make such models human-interpretable despite the need for transparency and trustworthiness in critical use cases like DL-assisted medical diagnosis. Moreover, such models either rely on post-hoc methods or additional annotations. In this work, we propose a novel interpretable-by-design method, ProtoVerse, to find relevant sub-parts of vertebral fractures (prototypes) that reliably explain the model's decision in a human-understandable way. Specifically, we introduce a novel diversity-promoting loss to mitigate prototype repetitions in small datasets with intricate semantics. We have experimented with the VerSe'19 dataset and outperformed the existing prototype-based method. Further, our model provides superior interpretability agains
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#31361;&#20986;&#30340;&#23545;&#35937;&#21306;&#22495;&#36827;&#34892;&#36807;&#37319;&#26679;&#30340;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#22788;&#29702;&#65292;&#20197;&#21450;&#38024;&#23545;&#23545;&#35937;&#21306;&#22495;&#37319;&#26679;&#30340;&#23454;&#20363;&#32423;&#21464;&#24418;&#24341;&#23548;&#65292;&#26377;&#25928;&#20943;&#36731;&#22495;&#33258;&#36866;&#24212;&#20013;&#30340;&#28304;&#23610;&#24230;&#20559;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.12712</link><description>&lt;p&gt;
&#36890;&#36807;&#22270;&#20687;&#21464;&#24418;&#35299;&#20915;&#22495;&#33258;&#36866;&#24212;&#20013;&#30340;&#28304;&#23610;&#24230;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing Source Scale Bias via Image Warping for Domain Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12712
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#31361;&#20986;&#30340;&#23545;&#35937;&#21306;&#22495;&#36827;&#34892;&#36807;&#37319;&#26679;&#30340;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#22788;&#29702;&#65292;&#20197;&#21450;&#38024;&#23545;&#23545;&#35937;&#21306;&#22495;&#37319;&#26679;&#30340;&#23454;&#20363;&#32423;&#21464;&#24418;&#24341;&#23548;&#65292;&#26377;&#25928;&#20943;&#36731;&#22495;&#33258;&#36866;&#24212;&#20013;&#30340;&#28304;&#23610;&#24230;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#35273;&#35782;&#21035;&#20013;&#65292;&#30001;&#20110;&#30495;&#23454;&#22330;&#26223;&#25968;&#25454;&#38598;&#20013;&#23545;&#35937;&#21644;&#22270;&#20687;&#22823;&#23567;&#20998;&#24067;&#30340;&#19981;&#24179;&#34913;&#65292;&#23610;&#24230;&#20559;&#24046;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#20256;&#32479;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#27880;&#20837;&#23610;&#24230;&#19981;&#21464;&#24615;&#20808;&#39564;&#12289;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#25968;&#25454;&#38598;&#22312;&#19981;&#21516;&#23610;&#24230;&#36827;&#34892;&#36807;&#37319;&#26679;&#65292;&#25110;&#32773;&#22312;&#25512;&#26029;&#26102;&#35843;&#25972;&#23610;&#24230;&#12290;&#34429;&#28982;&#36825;&#20123;&#31574;&#30053;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20943;&#36731;&#20102;&#23610;&#24230;&#20559;&#24046;&#65292;&#20294;&#23427;&#20204;&#22312;&#36328;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#26102;&#30340;&#36866;&#24212;&#33021;&#21147;&#26377;&#38480;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20250;&#22686;&#21152;&#35757;&#32451;&#36807;&#31243;&#30340;&#35745;&#31639;&#36127;&#36733;&#21644;&#25512;&#26029;&#36807;&#31243;&#30340;&#24310;&#36831;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#33258;&#36866;&#24212;&#30340;&#27880;&#24847;&#21147;&#22788;&#29702;&#8212;&#8212;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23601;&#22320;&#25197;&#26354;&#22270;&#20687;&#26469;&#23545;&#31361;&#20986;&#30340;&#23545;&#35937;&#21306;&#22495;&#36827;&#34892;&#36807;&#37319;&#26679;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#25913;&#21464;&#28304;&#23610;&#24230;&#20998;&#24067;&#21487;&#20197;&#25913;&#21892;&#20027;&#24178;&#29305;&#24449;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#38754;&#21521;&#23545;&#35937;&#21306;&#22495;&#37319;&#26679;&#30340;&#23454;&#20363;&#32423;&#21464;&#24418;&#24341;&#23548;&#65292;&#20197;&#20943;&#36731;&#22495;&#33258;&#36866;&#24212;&#20013;&#30340;&#28304;&#23610;&#24230;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#23545;&#22320;&#29702;&#12289;&#20809;&#29031;&#21644;&#22825;&#27668;&#26465;&#20214;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12712v1 Announce Type: cross  Abstract: In visual recognition, scale bias is a key challenge due to the imbalance of object and image size distribution inherent in real scene datasets. Conventional solutions involve injecting scale invariance priors, oversampling the dataset at different scales during training, or adjusting scale at inference. While these strategies mitigate scale bias to some extent, their ability to adapt across diverse datasets is limited. Besides, they increase computational load during training and latency during inference. In this work, we use adaptive attentional processing -- oversampling salient object regions by warping images in-place during training. Discovering that shifting the source scale distribution improves backbone features, we developed a instance-level warping guidance aimed at object region sampling to mitigate source scale bias in domain adaptation. Our approach improves adaptation across geographies, lighting and weather conditions, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#22122;&#22768;&#27700;&#24179;&#33258;&#36866;&#24212;&#29305;&#24615;&#30340;&#21518;&#39564;&#37319;&#26679;&#31574;&#30053;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;MRI&#37325;&#24314;&#36807;&#31243;&#20013;&#22240;&#30495;&#23454;&#22122;&#22768;&#27700;&#24179;&#21464;&#21270;&#23548;&#33268;&#30340;&#37325;&#24314;&#19981;&#20934;&#30830;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.05245</link><description>&lt;p&gt;
&#22122;&#22768;&#27700;&#24179;&#33258;&#36866;&#24212;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#21152;&#36895;MRI&#30340;&#31283;&#20581;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Noise Level Adaptive Diffusion Model for Robust Reconstruction of Accelerated MRI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05245
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#22122;&#22768;&#27700;&#24179;&#33258;&#36866;&#24212;&#29305;&#24615;&#30340;&#21518;&#39564;&#37319;&#26679;&#31574;&#30053;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;MRI&#37325;&#24314;&#36807;&#31243;&#20013;&#22240;&#30495;&#23454;&#22122;&#22768;&#27700;&#24179;&#21464;&#21270;&#23548;&#33268;&#30340;&#37325;&#24314;&#19981;&#20934;&#30830;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#65292;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;MRI&#37325;&#24314;&#26041;&#27861;&#20250;&#36880;&#27493;&#21435;&#38500;&#20154;&#20026;&#28155;&#21152;&#30340;&#22122;&#22768;&#65292;&#24182;&#24378;&#35843;&#25968;&#25454;&#19968;&#33268;&#24615;&#20197;&#37325;&#24314;&#28508;&#22312;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;MRI&#37319;&#38598;&#24050;&#32463;&#21253;&#21547;&#30001;&#28909;&#28072;&#33853;&#24341;&#36215;&#30340;&#22266;&#26377;&#22122;&#22768;&#12290;&#20351;&#29992;&#36229;&#24555;&#36895;&#12289;&#39640;&#20998;&#36776;&#29575;&#25104;&#20687;&#24207;&#21015;&#36827;&#34892;&#39640;&#32423;&#30740;&#31350;&#65292;&#25110;&#32773;&#20351;&#29992;&#20302;&#22330;&#31995;&#32479;&#65288;&#21463;&#20302;&#25910;&#20837;&#21644;&#20013;&#31561;&#25910;&#20837;&#22269;&#23478;&#38738;&#30544;&#65289;&#26102;&#65292;&#36825;&#31181;&#29616;&#35937;&#23588;&#20854;&#26126;&#26174;&#12290;&#36825;&#20123;&#24120;&#35265;&#22330;&#26223;&#21487;&#33021;&#23548;&#33268;&#29616;&#26377;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#37325;&#24314;&#25216;&#26415;&#24615;&#33021;&#20122;&#20248;&#25110;&#23436;&#20840;&#22833;&#36133;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#38543;&#30528;&#36880;&#28176;&#21435;&#38500;&#20154;&#20026;&#28155;&#21152;&#30340;&#22122;&#22768;&#65292;&#22266;&#26377;&#30340;MRI&#22122;&#22768;&#21464;&#24471;&#36234;&#26469;&#36234;&#26126;&#26174;&#65292;&#20351;&#23454;&#38469;&#22122;&#22768;&#27700;&#24179;&#19982;&#39044;&#23450;&#20041;&#21435;&#22122;&#26102;&#38388;&#34920;&#19981;&#19968;&#33268;&#65292;&#20174;&#32780;&#23548;&#33268;&#22270;&#20687;&#37325;&#24314;&#19981;&#20934;&#30830;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26032;&#39062;&#22122;&#22768;&#27700;&#24179;&#33258;&#36866;&#24212;&#29305;&#24615;&#30340;&#21518;&#39564;&#37319;&#26679;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05245v1 Announce Type: cross  Abstract: In general, diffusion model-based MRI reconstruction methods incrementally remove artificially added noise while imposing data consistency to reconstruct the underlying images. However, real-world MRI acquisitions already contain inherent noise due to thermal fluctuations. This phenomenon is particularly notable when using ultra-fast, high-resolution imaging sequences for advanced research, or using low-field systems favored by low- and middle-income countries. These common scenarios can lead to sub-optimal performance or complete failure of existing diffusion model-based reconstruction techniques. Specifically, as the artificially added noise is gradually removed, the inherent MRI noise becomes increasingly pronounced, making the actual noise level inconsistent with the predefined denoising schedule and consequently inaccurate image reconstruction. To tackle this problem, we propose a posterior sampling strategy with a novel NoIse Lev
&lt;/p&gt;</description></item></channel></rss>