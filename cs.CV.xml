<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DIRECT&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#30830;&#23450;&#31867;&#21035;&#20998;&#21106;&#38408;&#20540;&#24182;&#26631;&#35760;&#26368;&#19981;&#30830;&#23450;&#19988;&#31163;&#20854;&#26368;&#36817;&#30340;&#31034;&#20363;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#32597;&#35265;&#31867;&#21644;&#23569;&#25968;&#31867;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#25209;&#27425;&#26631;&#35760;&#21644;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#23481;&#24525;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2312.09196</link><description>&lt;p&gt;
DIRECT: &#22788;&#29702;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DIRECT: Deep Active Learning under Imbalance and Label Noise
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.09196
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DIRECT&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#30830;&#23450;&#31867;&#21035;&#20998;&#21106;&#38408;&#20540;&#24182;&#26631;&#35760;&#26368;&#19981;&#30830;&#23450;&#19988;&#31163;&#20854;&#26368;&#36817;&#30340;&#31034;&#20363;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#32597;&#35265;&#31867;&#21644;&#23569;&#25968;&#31867;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#25209;&#27425;&#26631;&#35760;&#21644;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#23481;&#24525;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#31867;&#21035;&#19981;&#24179;&#34913;&#26159;&#19968;&#20010;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#36890;&#24120;&#20250;&#23548;&#33268;&#32597;&#35265;&#21644;&#23569;&#25968;&#31867;&#30340;&#24615;&#33021;&#36739;&#24046;&#12290;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20027;&#21160;&#23398;&#20064;&#21487;&#33021;&#26159;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#26368;&#26377;&#25928;&#25216;&#26415;&#65292;&#23427;&#20174;&#26681;&#26412;&#19978;&#37319;&#38598;&#26356;&#24179;&#34913;&#21644;&#20855;&#26377;&#20449;&#24687;&#37327;&#30340;&#26631;&#35760;&#31034;&#20363;&#36827;&#34892;&#27880;&#37322;&#12290;&#26631;&#31614;&#22122;&#22768;&#26159;&#25968;&#25454;&#27880;&#37322;&#20219;&#21153;&#20013;&#21478;&#19968;&#20010;&#24120;&#35265;&#38382;&#39064;&#65292;&#23545;&#20110;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#26469;&#35828;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#20027;&#21160;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#31283;&#20581;&#22320;&#30830;&#23450;&#31867;&#21035;&#20998;&#21106;&#38408;&#20540;&#24182;&#26631;&#35760;&#26368;&#19981;&#30830;&#23450;&#19988;&#31163;&#20854;&#26368;&#36817;&#30340;&#31034;&#20363;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#31616;&#21270;&#20026;&#19968;&#32500;&#20027;&#21160;&#23398;&#20064;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;DIRECT&#33021;&#22815;&#21033;&#29992;&#32463;&#20856;&#30340;&#20027;&#21160;&#23398;&#20064;&#25991;&#29486;&#26469;&#35299;&#20915;&#25209;&#27425;&#26631;&#35760;&#21644;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#23481;&#24525;&#31561;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#22823;&#37327;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class imbalance is a prevalent issue in real world machine learning applications, often leading to poor performance in rare and minority classes. With an abundance of wild unlabeled data, active learning is perhaps the most effective technique in solving the problem at its root -- collecting a more balanced and informative set of labeled examples during annotation. Label noise is another common issue in data annotation jobs, which is especially challenging for active learning methods. In this work, we conduct the first study of active learning under both class imbalance and label noise. We propose a novel algorithm that robustly identifies the class separation threshold and annotates the most uncertain examples that are closest from it. Through a novel reduction to one-dimensional active learning, our algorithm DIRECT is able to leverage the classic active learning literature to address issues such as batch labeling and tolerance towards label noise. We present extensive experiments on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MARINE&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#26469;&#20943;&#23569;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#29289;&#20307;&#24187;&#35273;&#12290;&#35813;&#26694;&#26550;&#26080;&#38656;&#35757;&#32451;&#25110;API&#35775;&#38382;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#35270;&#35273;&#27169;&#22411;&#21644;&#24341;&#20837;&#39069;&#22806;&#30340;&#29289;&#20307;&#22522;&#30784;&#29305;&#24449;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#29983;&#25104;&#31934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.08680</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#26469;&#20943;&#36731;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#29289;&#20307;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MARINE&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#26469;&#20943;&#23569;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#29289;&#20307;&#24187;&#35273;&#12290;&#35813;&#26694;&#26550;&#26080;&#38656;&#35757;&#32451;&#25110;API&#35775;&#38382;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#35270;&#35273;&#27169;&#22411;&#21644;&#24341;&#20837;&#39069;&#22806;&#30340;&#29289;&#20307;&#22522;&#30784;&#29305;&#24449;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#29983;&#25104;&#31934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLM&#65289;&#30340;&#36827;&#23637;&#36234;&#26469;&#36234;&#31361;&#20986;&#20102;&#23427;&#20204;&#22312;&#22270;&#20687;&#20013;&#20135;&#29983;&#34394;&#20551;&#29289;&#20307;&#30340;&#20005;&#37325;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#30528;&#37325;&#20110;&#20351;&#29992;&#29305;&#27530;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#25110;&#24378;&#22823;&#30340;LLM&#65288;&#20363;&#22914;GPT-3.5&#65289;&#26469;&#32416;&#27491;LVLM&#30340;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#27714;&#26114;&#36149;&#30340;&#35757;&#32451;/&#24494;&#35843;&#25110;API&#35775;&#38382;&#20808;&#36827;&#30340;LLM&#26469;&#22312;&#29983;&#25104;&#21518;&#32416;&#27491;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#21517;&#20026;&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#32531;&#35299;&#24187;&#35273;&#30340;&#26694;&#26550;&#65288;MARINE&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#35813;&#26694;&#26550;&#26082;&#26080;&#38656;&#35757;&#32451;&#20063;&#26080;&#38656;API&#35775;&#38382;&#65292;&#21487;&#20197;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#26377;&#25928;&#22320;&#20943;&#23569;&#29289;&#20307;&#24187;&#35273;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MARINE&#36890;&#36807;&#38598;&#25104;&#29616;&#26377;&#30340;&#24320;&#28304;&#35270;&#35273;&#27169;&#22411;&#20016;&#23500;LVLM&#30340;&#35270;&#35273;&#35821;&#22659;&#65292;&#24182;&#20351;&#29992;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#26469;&#25972;&#21512;&#39069;&#22806;&#30340;&#29289;&#20307;&#22522;&#30784;&#29305;&#24449;&#65292;&#20197;&#25552;&#39640;LVLM&#29983;&#25104;&#30340;&#31934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancement of Large Vision-Language Models (LVLMs) has increasingly highlighted the critical issue of their tendency to hallucinate non-existing objects in the images. To address this issue, previous works focused on using specially curated datasets or powerful LLMs (e.g., GPT-3.5) to rectify the outputs of LVLMs. However, these approaches require either expensive training/fine-tuning or API access to advanced LLMs to correct the model's output post-generation. In this paper, we tackle this challenge by introducing a framework called Mitigating hallucinAtion via classifieR-Free guIdaNcE (MARINE), which is both training-free and API-free, and can effectively and efficiently reduce object hallucinations during the generation process. Specifically, MARINE enriches the visual context of LVLMs by integrating existing open-source vision models, and employs classifier-free guidance to incorporate the additional object grounding features to improve the precision of LVLMs' generations. Thr
&lt;/p&gt;</description></item></channel></rss>