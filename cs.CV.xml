<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25200;&#21160;&#27880;&#24847;&#21147;&#24341;&#23548;&#65288;PAG&#65289;&#30340;&#26032;&#22411;&#25277;&#26679;&#24341;&#23548;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#25193;&#25955; U-Net &#20013;&#26367;&#25442;&#33258;&#27880;&#24847;&#21147;&#26144;&#23556;&#26469;&#29983;&#25104;&#32467;&#26500;&#38477;&#32423;&#30340;&#20013;&#38388;&#26679;&#26412;&#65292;&#20174;&#32780;&#22312;&#26080;&#26465;&#20214;&#21644;&#26377;&#26465;&#20214;&#35774;&#32622;&#19979;&#25913;&#21892;&#25193;&#25955;&#26679;&#26412;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.17377</link><description>&lt;p&gt;
&#20855;&#26377;&#25200;&#21160;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#33258;&#30699;&#27491;&#25193;&#25955;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Self-Rectifying Diffusion Sampling with Perturbed-Attention Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17377
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25200;&#21160;&#27880;&#24847;&#21147;&#24341;&#23548;&#65288;PAG&#65289;&#30340;&#26032;&#22411;&#25277;&#26679;&#24341;&#23548;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#25193;&#25955; U-Net &#20013;&#26367;&#25442;&#33258;&#27880;&#24847;&#21147;&#26144;&#23556;&#26469;&#29983;&#25104;&#32467;&#26500;&#38477;&#32423;&#30340;&#20013;&#38388;&#26679;&#26412;&#65292;&#20174;&#32780;&#22312;&#26080;&#26465;&#20214;&#21644;&#26377;&#26465;&#20214;&#35774;&#32622;&#19979;&#25913;&#21892;&#25193;&#25955;&#26679;&#26412;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#34920;&#26126;&#65292;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#26679;&#26412;&#65292;&#20294;&#20854;&#36136;&#37327;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#25277;&#26679;&#24341;&#23548;&#25216;&#26415;&#65292;&#27604;&#22914;&#20998;&#31867;&#22120;&#24341;&#23548;&#65288;CG&#65289;&#21644;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#65288;CFG&#65289;&#12290;&#36825;&#20123;&#25216;&#26415;&#36890;&#24120;&#22312;&#26080;&#26465;&#20214;&#29983;&#25104;&#25110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#22914;&#22270;&#20687;&#24674;&#22797;&#20013;&#26080;&#27861;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25277;&#26679;&#24341;&#23548;&#25216;&#26415;&#65292;&#31216;&#20026;&#25200;&#21160;&#27880;&#24847;&#21147;&#24341;&#23548;&#65288;PAG&#65289;&#65292;&#23427;&#25913;&#36827;&#20102;&#25193;&#25955;&#26679;&#26412;&#30340;&#36136;&#37327;&#65292;&#19981;&#31649;&#26159;&#22312;&#26080;&#26465;&#20214;&#36824;&#26159;&#26377;&#26465;&#20214;&#30340;&#35774;&#32622;&#20013;&#65292;&#37117;&#33021;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#25110;&#25972;&#21512;&#22806;&#37096;&#27169;&#22359;&#12290;PAG &#26088;&#22312;&#36890;&#36807;&#25972;&#20010;&#21435;&#22122;&#36807;&#31243;&#36880;&#27493;&#22686;&#24378;&#26679;&#26412;&#30340;&#32467;&#26500;&#12290;&#23427;&#28041;&#21450;&#36890;&#36807;&#29992;&#24658;&#31561;&#30697;&#38453;&#26367;&#25442;&#25193;&#25955; U-Net &#20013;&#36873;&#25321;&#30340;&#33258;&#27880;&#24847;&#21147;&#26144;&#23556;&#29983;&#25104;&#32467;&#26500;&#38477;&#32423;&#30340;&#20013;&#38388;&#26679;&#26412;&#65292;&#32771;&#34385;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17377v1 Announce Type: cross  Abstract: Recent studies have demonstrated that diffusion models are capable of generating high-quality samples, but their quality heavily depends on sampling guidance techniques, such as classifier guidance (CG) and classifier-free guidance (CFG). These techniques are often not applicable in unconditional generation or in various downstream tasks such as image restoration. In this paper, we propose a novel sampling guidance, called Perturbed-Attention Guidance (PAG), which improves diffusion sample quality across both unconditional and conditional settings, achieving this without requiring additional training or the integration of external modules. PAG is designed to progressively enhance the structure of samples throughout the denoising process. It involves generating intermediate samples with degraded structure by substituting selected self-attention maps in diffusion U-Net with an identity matrix, by considering the self-attention mechanisms
&lt;/p&gt;</description></item><item><title>VGMShield&#25552;&#20986;&#20102;&#19977;&#39033;&#31616;&#21333;&#20294;&#24320;&#21019;&#24615;&#30340;&#25514;&#26045;&#65292;&#36890;&#36807;&#26816;&#27979;&#34394;&#20551;&#35270;&#39057;&#12289;&#28335;&#28304;&#38382;&#39064;&#21644;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#31354;&#38388;-&#26102;&#38388;&#21160;&#24577;&#27169;&#22411;&#65292;&#38450;&#33539;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#30340;&#35823;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.13126</link><description>&lt;p&gt;
VGMShield&#65306;&#32531;&#35299;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#30340;&#35823;&#29992;
&lt;/p&gt;
&lt;p&gt;
VGMShield: Mitigating Misuse of Video Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13126
&lt;/p&gt;
&lt;p&gt;
VGMShield&#25552;&#20986;&#20102;&#19977;&#39033;&#31616;&#21333;&#20294;&#24320;&#21019;&#24615;&#30340;&#25514;&#26045;&#65292;&#36890;&#36807;&#26816;&#27979;&#34394;&#20551;&#35270;&#39057;&#12289;&#28335;&#28304;&#38382;&#39064;&#21644;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#31354;&#38388;-&#26102;&#38388;&#21160;&#24577;&#27169;&#22411;&#65292;&#38450;&#33539;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#30340;&#35823;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35270;&#39057;&#29983;&#25104;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20154;&#20204;&#21487;&#20197;&#26041;&#20415;&#22320;&#21033;&#29992;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#21019;&#24314;&#31526;&#21512;&#20854;&#29305;&#23450;&#38656;&#27714;&#30340;&#35270;&#39057;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#20063;&#36234;&#26469;&#36234;&#25285;&#24515;&#36825;&#20123;&#25216;&#26415;&#34987;&#29992;&#20110;&#21019;&#20316;&#21644;&#20256;&#25773;&#34394;&#20551;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;VGMShield&#65306;&#19968;&#22871;&#21253;&#21547;&#19977;&#39033;&#30452;&#25509;&#20294;&#24320;&#21019;&#24615;&#30340;&#25514;&#26045;&#65292;&#29992;&#20110;&#38450;&#33539;&#34394;&#20551;&#35270;&#39057;&#29983;&#25104;&#36807;&#31243;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;&#8220;&#34394;&#20551;&#35270;&#39057;&#26816;&#27979;&#8221;&#24320;&#22987;&#65292;&#23581;&#35797;&#29702;&#35299;&#29983;&#25104;&#30340;&#35270;&#39057;&#20013;&#26159;&#21542;&#23384;&#22312;&#29420;&#29305;&#24615;&#65292;&#20197;&#21450;&#25105;&#20204;&#26159;&#21542;&#33021;&#22815;&#21306;&#20998;&#23427;&#20204;&#19982;&#30495;&#23454;&#35270;&#39057;&#30340;&#19981;&#21516;&#65307;&#28982;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#8220;&#28335;&#28304;&#8221;&#38382;&#39064;&#65292;&#21363;&#23558;&#19968;&#27573;&#34394;&#20551;&#35270;&#39057;&#36861;&#28335;&#22238;&#29983;&#25104;&#23427;&#30340;&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#20851;&#27880;&#8220;&#26102;&#31354;&#21160;&#24577;&#8221;&#30340;&#27169;&#22411;&#20316;&#20026;&#39592;&#24178;&#65292;&#20197;&#35782;&#21035;&#35270;&#39057;&#20013;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;&#23545;&#19971;&#20010;&#26368;&#20808;&#36827;&#30340;&#24320;&#28304;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13126v1 Announce Type: cross  Abstract: With the rapid advancement in video generation, people can conveniently utilize video generation models to create videos tailored to their specific desires. Nevertheless, there are also growing concerns about their potential misuse in creating and disseminating false information.   In this work, we introduce VGMShield: a set of three straightforward but pioneering mitigations through the lifecycle of fake video generation. We start from \textit{fake video detection} trying to understand whether there is uniqueness in generated videos and whether we can differentiate them from real videos; then, we investigate the \textit{tracing} problem, which maps a fake video back to a model that generates it. Towards these, we propose to leverage pre-trained models that focus on {\it spatial-temporal dynamics} as the backbone to identify inconsistencies in videos. Through experiments on seven state-of-the-art open-source models, we demonstrate that
&lt;/p&gt;</description></item><item><title>PIP-Net&#26159;&#19968;&#20010;&#26032;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#32508;&#21512;&#21033;&#29992;&#21160;&#24577;&#23398;&#25968;&#25454;&#21644;&#22330;&#26223;&#31354;&#38388;&#29305;&#24449;&#65292;&#37319;&#29992;&#24490;&#29615;&#21644;&#26102;&#38388;&#27880;&#24847;&#21147;&#26426;&#21046;&#35299;&#20915;&#26041;&#26696;&#65292;&#25104;&#21151;&#39044;&#27979;&#34892;&#20154;&#36890;&#36807;&#39532;&#36335;&#30340;&#24847;&#22270;&#65292;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.12810</link><description>&lt;p&gt;
PIP-Net&#65306;&#22478;&#24066;&#20013;&#34892;&#20154;&#24847;&#22270;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
PIP-Net: Pedestrian Intention Prediction in the Wild
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12810
&lt;/p&gt;
&lt;p&gt;
PIP-Net&#26159;&#19968;&#20010;&#26032;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#32508;&#21512;&#21033;&#29992;&#21160;&#24577;&#23398;&#25968;&#25454;&#21644;&#22330;&#26223;&#31354;&#38388;&#29305;&#24449;&#65292;&#37319;&#29992;&#24490;&#29615;&#21644;&#26102;&#38388;&#27880;&#24847;&#21147;&#26426;&#21046;&#35299;&#20915;&#26041;&#26696;&#65292;&#25104;&#21151;&#39044;&#27979;&#34892;&#20154;&#36890;&#36807;&#39532;&#36335;&#30340;&#24847;&#22270;&#65292;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#20934;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#23545;&#34892;&#20154;&#24847;&#22270;&#30340;&#39044;&#27979;&#26159;&#24403;&#21069;&#35813;&#39046;&#22495;&#30340;&#19968;&#39033;&#30740;&#31350;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PIP-Net&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#39044;&#27979;AVs&#22312;&#29616;&#23454;&#19990;&#30028;&#22478;&#24066;&#22330;&#26223;&#20013;&#30340;&#34892;&#20154;&#36807;&#39532;&#36335;&#24847;&#22270;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#31181;&#38024;&#23545;&#19981;&#21516;&#25668;&#20687;&#22836;&#23433;&#35013;&#21644;&#35774;&#32622;&#35774;&#35745;&#30340;PIP-Net&#21464;&#31181;&#12290;&#21033;&#29992;&#26469;&#33258;&#34892;&#39542;&#22330;&#26223;&#30340;&#21160;&#21147;&#23398;&#25968;&#25454;&#21644;&#31354;&#38388;&#29305;&#24449;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#37319;&#29992;&#24490;&#29615;&#21644;&#26102;&#38388;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;&#20026;&#20102;&#22686;&#24378;&#36947;&#36335;&#29992;&#25143;&#30340;&#35270;&#35273;&#34920;&#31034;&#21450;&#20854;&#19982;&#33258;&#36710;&#30340;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20998;&#31867;&#28145;&#24230;&#29305;&#24449;&#22270;&#65292;&#32467;&#21512;&#23616;&#37096;&#36816;&#21160;&#27969;&#29305;&#24449;&#65292;&#20026;&#22330;&#26223;&#21160;&#24577;&#25552;&#20379;&#20016;&#23500;&#30340;&#27934;&#23519;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;&#25668;&#20687;&#22836;&#30340;&#35270;&#37326;&#20174;&#19968;&#20010;&#25193;&#23637;&#21040;&#22260;&#32469;&#33258;&#36710;&#30340;&#19977;&#20010;&#25668;&#20687;&#22836;&#30340;&#24433;&#21709;&#65292;&#20197;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12810v1 Announce Type: cross  Abstract: Accurate pedestrian intention prediction (PIP) by Autonomous Vehicles (AVs) is one of the current research challenges in this field. In this article, we introduce PIP-Net, a novel framework designed to predict pedestrian crossing intentions by AVs in real-world urban scenarios. We offer two variants of PIP-Net designed for different camera mounts and setups. Leveraging both kinematic data and spatial features from the driving scene, the proposed model employs a recurrent and temporal attention-based solution, outperforming state-of-the-art performance. To enhance the visual representation of road users and their proximity to the ego vehicle, we introduce a categorical depth feature map, combined with a local motion flow feature, providing rich insights into the scene dynamics. Additionally, we explore the impact of expanding the camera's field of view, from one to three cameras surrounding the ego vehicle, leading to enhancement in the
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LadaGAN&#30340;&#39640;&#25928;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#23427;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;Ladaformer&#30340;&#26032;&#22411;Transformer&#22359;&#65292;&#36890;&#36807;&#32447;&#24615;&#21152;&#27861;&#27880;&#24847;&#26426;&#21046;&#26469;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#35299;&#20915;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.09596</link><description>&lt;p&gt;
&#20351;&#29992;&#32447;&#24615;&#21152;&#27861;&#27880;&#24847;&#21147;Transformer&#30340;&#39640;&#25928;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Efficient generative adversarial networks using linear additive-attention Transformers. (arXiv:2401.09596v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09596
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LadaGAN&#30340;&#39640;&#25928;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#23427;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;Ladaformer&#30340;&#26032;&#22411;Transformer&#22359;&#65292;&#36890;&#36807;&#32447;&#24615;&#21152;&#27861;&#27880;&#24847;&#26426;&#21046;&#26469;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#35299;&#20915;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20687;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#31561;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;&#36817;&#24180;&#26469;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#39640;&#65292;&#20294;&#26159;&#23427;&#20204;&#30340;&#25104;&#21151;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#21151;&#20110;&#35745;&#31639;&#22797;&#26434;&#30340;&#26550;&#26500;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#30740;&#31350;&#23454;&#39564;&#23460;&#21644;&#36164;&#28304;&#20805;&#36275;&#30340;&#20844;&#21496;&#20013;&#30340;&#37319;&#29992;&#21644;&#20351;&#29992;&#65292;&#21516;&#26102;&#20063;&#26497;&#22823;&#22320;&#22686;&#21152;&#20102;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#25512;&#29702;&#30340;&#30899;&#36275;&#36857;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LadaGAN&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#23427;&#24314;&#31435;&#22312;&#19968;&#31181;&#21517;&#20026;Ladaformer&#30340;&#26032;&#22411;Transformer&#22359;&#19978;&#12290;&#35813;&#22359;&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#19968;&#20010;&#32447;&#24615;&#21152;&#27861;&#27880;&#24847;&#26426;&#21046;&#65292;&#23427;&#27599;&#20010;&#22836;&#37096;&#35745;&#31639;&#19968;&#20010;&#27880;&#24847;&#21521;&#37327;&#65292;&#32780;&#19981;&#26159;&#20108;&#27425;&#30340;&#28857;&#31215;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#22312;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#20013;&#37117;&#37319;&#29992;&#20102;Ladaformer&#65292;&#36825;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#20811;&#26381;&#20102;Transformer GAN&#32463;&#24120;&#20986;&#29616;&#30340;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#12290;LadaGAN&#19968;&#30452;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;GANs&#12290;
&lt;/p&gt;
&lt;p&gt;
Although the capacity of deep generative models for image generation, such as Diffusion Models (DMs) and Generative Adversarial Networks (GANs), has dramatically improved in recent years, much of their success can be attributed to computationally expensive architectures. This has limited their adoption and use to research laboratories and companies with large resources, while significantly raising the carbon footprint for training, fine-tuning, and inference. In this work, we present LadaGAN, an efficient generative adversarial network that is built upon a novel Transformer block named Ladaformer. The main component of this block is a linear additive-attention mechanism that computes a single attention vector per head instead of the quadratic dot-product attention. We employ Ladaformer in both the generator and discriminator, which reduces the computational complexity and overcomes the training instabilities often associated with Transformer GANs. LadaGAN consistently outperforms exist
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;AVTENet&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26159;&#19968;&#20010;&#22522;&#20110;&#38899;&#39057;-&#35270;&#35273;Transformer&#30340;&#22810;&#19987;&#23478;&#38598;&#25104;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#35270;&#39057;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#20013;&#32771;&#34385;&#22768;&#23398;&#21644;&#35270;&#35273;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2310.13103</link><description>&lt;p&gt;
AVTENet: &#22522;&#20110;&#38899;&#39057;-&#35270;&#35273;Transformer&#30340;&#22810;&#19987;&#23478;&#38598;&#25104;&#32593;&#32476;&#22312;&#35270;&#39057;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AVTENet: Audio-Visual Transformer-based Ensemble Network Exploiting Multiple Experts for Video Deepfake Detection. (arXiv:2310.13103v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;AVTENet&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26159;&#19968;&#20010;&#22522;&#20110;&#38899;&#39057;-&#35270;&#35273;Transformer&#30340;&#22810;&#19987;&#23478;&#38598;&#25104;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#35270;&#39057;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#20013;&#32771;&#34385;&#22768;&#23398;&#21644;&#35270;&#35273;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#24191;&#27867;&#20998;&#20139;&#30340;&#20266;&#36896;&#20869;&#23481;&#26159;&#19968;&#20010;&#37325;&#22823;&#31038;&#20250;&#38382;&#39064;&#65292;&#35201;&#27714;&#21152;&#24378;&#30417;&#31649;&#24182;&#32473;&#30740;&#31350;&#31038;&#21306;&#24102;&#26469;&#26032;&#30340;&#25361;&#25112;&#12290;&#36817;&#24180;&#26469;&#65292;&#36229;&#30495;&#23454;&#30340;&#28145;&#24230;&#20266;&#36896;&#35270;&#39057;&#30340;&#26222;&#21450;&#24341;&#36215;&#20102;&#23545;&#38899;&#39057;&#21644;&#35270;&#35273;&#20266;&#36896;&#23041;&#32961;&#30340;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#20851;&#20110;&#26816;&#27979;AI&#29983;&#25104;&#30340;&#20266;&#36896;&#35270;&#39057;&#30340;&#20808;&#21069;&#24037;&#20316;&#21482;&#21033;&#29992;&#20102;&#35270;&#35273;&#27169;&#24577;&#25110;&#38899;&#39057;&#27169;&#24577;&#12290;&#34429;&#28982;&#25991;&#29486;&#20013;&#26377;&#19968;&#20123;&#26041;&#27861;&#21033;&#29992;&#38899;&#39057;&#21644;&#35270;&#35273;&#27169;&#24577;&#26469;&#26816;&#27979;&#20266;&#36896;&#35270;&#39057;&#65292;&#20294;&#23427;&#20204;&#23578;&#26410;&#22312;&#28041;&#21450;&#22768;&#23398;&#21644;&#35270;&#35273;&#25805;&#20316;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#20266;&#36896;&#35270;&#39057;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#22522;&#20110;CNN&#65292;&#24182;&#19988;&#26816;&#27979;&#20934;&#30830;&#29575;&#36739;&#20302;&#12290;&#21463;&#21040;Transformer&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#26368;&#26032;&#25104;&#21151;&#21551;&#21457;&#65292;&#20026;&#20102;&#35299;&#20915;&#28145;&#24230;&#20266;&#36896;&#25216;&#26415;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#22768;&#23398;&#25805;&#20316;&#30340;&#38899;&#39057;-&#35270;&#35273;Transformer&#38598;&#25104;&#32593;&#32476;&#65288;AVTENet&#65289;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forged content shared widely on social media platforms is a major social problem that requires increased regulation and poses new challenges to the research community. The recent proliferation of hyper-realistic deepfake videos has drawn attention to the threat of audio and visual forgeries. Most previous work on detecting AI-generated fake videos only utilizes visual modality or audio modality. While there are some methods in the literature that exploit audio and visual modalities to detect forged videos, they have not been comprehensively evaluated on multi-modal datasets of deepfake videos involving acoustic and visual manipulations. Moreover, these existing methods are mostly based on CNN and suffer from low detection accuracy. Inspired by the recent success of Transformer in various fields, to address the challenges posed by deepfake technology, in this paper, we propose an Audio-Visual Transformer-based Ensemble Network (AVTENet) framework that considers both acoustic manipulatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#36125;&#21494;&#26031;&#22238;&#24402;&#21644;&#21160;&#37327;&#20256;&#25773;&#30340;&#39044;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#26102;&#22312;&#23884;&#20837;&#24335;&#30828;&#20214;&#19978;&#20135;&#29983;&#26377;&#24847;&#20041;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#20351;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#20154;&#26426;&#20132;&#20114;&#31561;&#39046;&#22495;&#30340;&#23454;&#26102;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.01201</link><description>&lt;p&gt;
&#23884;&#20837;&#24335;&#31995;&#32479;&#23454;&#26102;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Uncertainty in Real-Time Semantic Segmentation on Embedded Systems. (arXiv:2301.01201v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#36125;&#21494;&#26031;&#22238;&#24402;&#21644;&#21160;&#37327;&#20256;&#25773;&#30340;&#39044;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#26102;&#22312;&#23884;&#20837;&#24335;&#30828;&#20214;&#19978;&#20135;&#29983;&#26377;&#24847;&#20041;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#20351;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#20154;&#26426;&#20132;&#20114;&#31561;&#39046;&#22495;&#30340;&#23454;&#26102;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#20154;&#26426;&#20132;&#20114;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#38656;&#35201;&#23454;&#26102;&#39044;&#27979;&#33021;&#21147;&#12290;&#23454;&#26102;&#24212;&#29992;&#30340;&#25361;&#25112;&#34987;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#25152;&#21152;&#21095;&#12290;&#34429;&#28982;&#36825;&#20123;&#24179;&#21488;&#19978;&#23454;&#26102;&#26041;&#27861;&#30340;&#24320;&#21457;&#24471;&#21040;&#20102;&#22686;&#21152;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#36275;&#22815;&#22320;&#32771;&#34385;&#21040;&#23384;&#22312;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#30340;&#28145;&#23618;&#29305;&#24449;&#25552;&#21462;&#19982;&#36125;&#21494;&#26031;&#22238;&#24402;&#21644;&#21160;&#37327;&#20256;&#25773;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#27880;&#19981;&#30830;&#23450;&#24615;&#30340;&#39044;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#35813;&#26041;&#27861;&#22914;&#20309;&#22312;&#23884;&#20837;&#24335;&#30828;&#20214;&#19978;&#23454;&#26102;&#20135;&#29983;&#26377;&#24847;&#20041;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Application for semantic segmentation models in areas such as autonomous vehicles and human computer interaction require real-time predictive capabilities. The challenges of addressing real-time application is amplified by the need to operate on resource constrained hardware. Whilst development of real-time methods for these platforms has increased, these models are unable to sufficiently reason about uncertainty present. This paper addresses this by combining deep feature extraction from pre-trained models with Bayesian regression and moment propagation for uncertainty aware predictions. We demonstrate how the proposed method can yield meaningful uncertainty on embedded hardware in real-time whilst maintaining predictive performance.
&lt;/p&gt;</description></item></channel></rss>