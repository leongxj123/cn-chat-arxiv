<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#36890;&#36807;&#28508;&#22312;&#37096;&#20998;&#22240;&#26524;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#27169;&#24335;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#22312;&#35782;&#21035;&#28508;&#22312;&#32806;&#21512;&#21464;&#37327;&#26041;&#38754;&#30340;&#20248;&#31168;&#33021;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#36890;&#36807;&#32447;&#24615;&#29420;&#31435;&#20998;&#37327;&#20998;&#26512;&#23398;&#20064;&#20998;&#31163;&#34920;&#31034;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.06223</link><description>&lt;p&gt;
&#36890;&#36807;&#28508;&#22312;&#37096;&#20998;&#22240;&#26524;&#27169;&#22411;&#25581;&#31034;&#22810;&#27169;&#24335;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Revealing Multimodal Contrastive Representation Learning through Latent Partial Causal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06223
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28508;&#22312;&#37096;&#20998;&#22240;&#26524;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#27169;&#24335;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#22312;&#35782;&#21035;&#28508;&#22312;&#32806;&#21512;&#21464;&#37327;&#26041;&#38754;&#30340;&#20248;&#31168;&#33021;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#36890;&#36807;&#32447;&#24615;&#29420;&#31435;&#20998;&#37327;&#20998;&#26512;&#23398;&#20064;&#20998;&#31163;&#34920;&#31034;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24335;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#30001;&#20110;&#23427;&#20204;&#33021;&#22815;&#29983;&#25104;&#22797;&#26434;&#29616;&#35937;&#30340;&#26377;&#24847;&#20041;&#30340;&#20849;&#20139;&#34920;&#31034;&#12290;&#20026;&#20102;&#22686;&#24378;&#23545;&#36825;&#20123;&#33719;&#24471;&#30340;&#34920;&#31034;&#30340;&#28145;&#24230;&#20998;&#26512;&#21644;&#29702;&#35299;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29305;&#21035;&#38024;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#35774;&#35745;&#30340;&#32479;&#19968;&#22240;&#26524;&#27169;&#22411;&#12290;&#36890;&#36807;&#30740;&#31350;&#36825;&#20010;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#27169;&#24335;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#22312;&#35782;&#21035;&#22312;&#25552;&#20986;&#30340;&#32479;&#19968;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#32806;&#21512;&#21464;&#37327;&#26041;&#38754;&#30340;&#20248;&#31168;&#33021;&#21147;&#65292;&#21363;&#20351;&#22312;&#19981;&#21516;&#20551;&#35774;&#19979;&#23548;&#33268;&#30340;&#32447;&#24615;&#25110;&#32622;&#25442;&#21464;&#25442;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#36890;&#36807;&#32447;&#24615;&#29420;&#31435;&#20998;&#37327;&#20998;&#26512;&#36825;&#19968;&#20196;&#20154;&#24778;&#35766;&#30340;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#24037;&#20855;&#23398;&#20064;&#20998;&#31163;&#34920;&#31034;&#30340;&#28508;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#21457;&#29616;&#30340;&#40065;&#26834;&#24615;&#65292;&#21363;&#20351;&#22312;&#34987;&#36829;&#21453;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#22312;&#23398;&#20064;&#30142;&#30149;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal contrastive representation learning methods have proven successful across a range of domains, partly due to their ability to generate meaningful shared representations of complex phenomena. To enhance the depth of analysis and understanding of these acquired representations, we introduce a unified causal model specifically designed for multimodal data. By examining this model, we show that multimodal contrastive representation learning excels at identifying latent coupled variables within the proposed unified model, up to linear or permutation transformations resulting from different assumptions. Our findings illuminate the potential of pre-trained multimodal models, eg, CLIP, in learning disentangled representations through a surprisingly simple yet highly effective tool: linear independent component analysis. Experiments demonstrate the robustness of our findings, even when the assumptions are violated, and validate the effectiveness of the proposed method in learning dise
&lt;/p&gt;</description></item><item><title>Gemini&#23478;&#26063;&#26159;&#19968;&#31995;&#21015;&#22312;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#25991;&#26412;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#20854;&#20013;&#26368;&#20855;&#33021;&#21147;&#30340;Gemini Ultra&#27169;&#22411;&#22312;30&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#25512;&#36827;&#20102;&#25216;&#26415;&#21069;&#27839;&#65292;&#24182;&#25913;&#36827;&#20102;&#25152;&#26377;20&#20010;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#30340;&#25216;&#26415;&#29366;&#24577;&#12290;</title><link>https://arxiv.org/abs/2312.11805</link><description>&lt;p&gt;
Gemini&#65306;&#19968;&#31995;&#21015;&#39640;&#24615;&#33021;&#22810;&#27169;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gemini: A Family of Highly Capable Multimodal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11805
&lt;/p&gt;
&lt;p&gt;
Gemini&#23478;&#26063;&#26159;&#19968;&#31995;&#21015;&#22312;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#25991;&#26412;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#20854;&#20013;&#26368;&#20855;&#33021;&#21147;&#30340;Gemini Ultra&#27169;&#22411;&#22312;30&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#25512;&#36827;&#20102;&#25216;&#26415;&#21069;&#27839;&#65292;&#24182;&#25913;&#36827;&#20102;&#25152;&#26377;20&#20010;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#30340;&#25216;&#26415;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25253;&#21578;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#31995;&#21015;Gemini&#65292;&#23637;&#31034;&#20986;&#22312;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#25991;&#26412;&#29702;&#35299;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;Gemini&#31995;&#21015;&#21253;&#25324;Ultra&#12289;Pro&#21644;Nano&#23610;&#23544;&#65292;&#36866;&#29992;&#20110;&#20174;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#21040;&#35774;&#22791;&#20869;&#23384;&#21463;&#38480;&#24212;&#29992;&#30340;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#12290;&#22312;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#26368;&#20855;&#33021;&#21147;&#30340;Gemini Ultra&#27169;&#22411;&#22312;32&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;30&#20010;&#20013;&#25512;&#36827;&#20102;&#25216;&#26415;&#21069;&#27839; - &#26174;&#33879;&#22320;&#26159;&#31532;&#19968;&#20010;&#22312;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#32771;&#35797;&#22522;&#20934;&#27979;&#35797;MMLU&#19978;&#23454;&#29616;&#20154;&#31867;&#19987;&#23478;&#27700;&#24179;&#34920;&#29616;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#25105;&#20204;&#30740;&#31350;&#30340;&#27599;&#19968;&#20010;20&#20010;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#20013;&#25913;&#36827;&#20102;&#25216;&#26415;&#21069;&#27839;&#12290;&#25105;&#20204;&#30456;&#20449;Gemini&#31995;&#21015;&#22312;&#36328;&#27169;&#24577;&#25512;&#29702;&#21644;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#30340;&#26032;&#33021;&#21147;&#23558;&#33021;&#22815;&#25903;&#25345;&#21508;&#31181;&#29992;&#20363;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36127;&#36131;&#20219;&#22320;&#21521;&#29992;&#25143;&#25552;&#20379;Gemini&#27169;&#22411;&#30340;&#35757;&#32451;&#21518;&#21644;&#37096;&#32626;&#26041;&#27861;&#65292;&#21253;&#25324;&#20351;&#29992;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11805v2 Announce Type: replace-cross  Abstract: This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to users through services includi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#25551;&#36848;&#22312;&#19981;&#21516;&#35821;&#35328;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#35821;&#20041;&#24046;&#24322;&#65292;&#22810;&#35821;&#35328;&#25968;&#25454;&#26377;&#26356;&#39640;&#30340;&#35821;&#20041;&#35206;&#30422;&#29575;&#65292;&#24182;&#19988;&#22522;&#20110;&#22810;&#35821;&#35328;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.14356</link><description>&lt;p&gt;
&#25991;&#21270;&#21644;&#35821;&#35328;&#22810;&#26679;&#24615;&#25552;&#39640;&#20102;&#35270;&#35273;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Cultural and Linguistic Diversity Improves Visual Representations. (arXiv:2310.14356v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14356
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#25551;&#36848;&#22312;&#19981;&#21516;&#35821;&#35328;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#35821;&#20041;&#24046;&#24322;&#65292;&#22810;&#35821;&#35328;&#25968;&#25454;&#26377;&#26356;&#39640;&#30340;&#35821;&#20041;&#35206;&#30422;&#29575;&#65292;&#24182;&#19988;&#22522;&#20110;&#22810;&#35821;&#35328;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#36890;&#24120;&#23558;&#24863;&#30693;&#35270;&#20026;&#23458;&#35266;&#30340;&#65292;&#24182;&#19988;&#36825;&#31181;&#20551;&#35774;&#22312;&#25968;&#25454;&#38598;&#25910;&#38598;&#21644;&#27169;&#22411;&#35757;&#32451;&#20013;&#24471;&#21040;&#21453;&#26144;&#12290;&#20363;&#22914;&#65292;&#19981;&#21516;&#35821;&#35328;&#30340;&#22270;&#20687;&#25551;&#36848;&#36890;&#24120;&#34987;&#20551;&#23450;&#20026;&#30456;&#21516;&#35821;&#20041;&#20869;&#23481;&#30340;&#32763;&#35793;&#12290;&#28982;&#32780;&#65292;&#36328;&#25991;&#21270;&#24515;&#29702;&#23398;&#21644;&#35821;&#35328;&#23398;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20010;&#20307;&#30340;&#35270;&#35273;&#24863;&#30693;&#22240;&#20854;&#25991;&#21270;&#32972;&#26223;&#21644;&#25152;&#35828;&#30340;&#35821;&#35328;&#32780;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#29983;&#25104;&#30340;&#26631;&#39064;&#20013;&#65292;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#35821;&#20041;&#20869;&#23481;&#24046;&#24322;&#12290;&#24403;&#25968;&#25454;&#26159;&#22810;&#35821;&#35328;&#32780;&#19981;&#26159;&#21333;&#35821;&#35328;&#26102;&#65292;&#26631;&#39064;&#30340;&#35821;&#20041;&#35206;&#30422;&#29575;&#24179;&#22343;&#26356;&#39640;&#65292;&#20197;&#22330;&#26223;&#22270;&#12289;&#23884;&#20837;&#21644;&#35821;&#35328;&#22797;&#26434;&#24615;&#36827;&#34892;&#27979;&#37327;&#12290;&#20363;&#22914;&#65292;&#19982;&#19968;&#32452;&#21333;&#35821;&#26631;&#39064;&#30456;&#27604;&#65292;&#22810;&#35821;&#26631;&#39064;&#24179;&#22343;&#26377;21.8&#65285;&#26356;&#22810;&#30340;&#23545;&#35937;&#65292;24.5&#65285;&#26356;&#22810;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;27.1&#65285;&#26356;&#22810;&#30340;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#26469;&#33258;&#19981;&#21516;&#35821;&#35328;&#30340;&#20869;&#23481;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computer vision often treats perception as objective, and this assumption gets reflected in the way that datasets are collected and models are trained. For instance, image descriptions in different languages are typically assumed to be translations of the same semantic content. However, work in cross-cultural psychology and linguistics has shown that individuals differ in their visual perception depending on their cultural background and the language they speak. In this paper, we demonstrate significant differences in semantic content across languages in both dataset and model-produced captions. When data is multilingual as opposed to monolingual, captions have higher semantic coverage on average, as measured by scene graph, embedding, and linguistic complexity. For example, multilingual captions have on average 21.8% more objects, 24.5% more relations, and 27.1% more attributes than a set of monolingual captions. Moreover, models trained on content from different languages perform bes
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#30340;&#30417;&#30563;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#21033;&#29992;&#35780;&#35770;&#26469;&#24110;&#21161;&#23398;&#29983;&#32593;&#32476;&#30340;&#23398;&#20064;&#12290;&#36890;&#36807;&#25552;&#21462;&#23398;&#29983;&#32593;&#32476;&#22312;&#19981;&#21516;&#35757;&#32451;&#38454;&#27573;&#30340;&#26102;&#31354;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#21160;&#24577;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#23545;&#23398;&#29983;&#32593;&#32476;&#20013;&#26087;&#30693;&#35782;&#30340;&#20248;&#21270;&#21644;&#21033;&#29992;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#35757;&#32451;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.00811</link><description>&lt;p&gt;
&#35780;&#35770;&#24110;&#21161;&#26356;&#22909;&#22320;&#23398;&#20064;&#65306;&#22522;&#20110;&#26102;&#38388;&#30340;&#30417;&#30563;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Review helps learn better: Temporal Supervised Knowledge Distillation. (arXiv:2307.00811v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#30340;&#30417;&#30563;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#21033;&#29992;&#35780;&#35770;&#26469;&#24110;&#21161;&#23398;&#29983;&#32593;&#32476;&#30340;&#23398;&#20064;&#12290;&#36890;&#36807;&#25552;&#21462;&#23398;&#29983;&#32593;&#32476;&#22312;&#19981;&#21516;&#35757;&#32451;&#38454;&#27573;&#30340;&#26102;&#31354;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#21160;&#24577;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#23545;&#23398;&#29983;&#32593;&#32476;&#20013;&#26087;&#30693;&#35782;&#30340;&#20248;&#21270;&#21644;&#21033;&#29992;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#35757;&#32451;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23398;&#20064;&#30693;&#35782;&#26102;&#65292;&#35780;&#35770;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#26576;&#20010;&#26102;&#38388;&#28857;&#33719;&#21462;&#30340;&#30693;&#35782;&#21487;&#33021;&#22312;&#20043;&#21069;&#30340;&#32463;&#39564;&#24110;&#21161;&#19979;&#24471;&#21040;&#26497;&#22823;&#30340;&#21551;&#21457;&#12290;&#22240;&#27492;&#65292;&#30693;&#35782;&#22686;&#38271;&#36807;&#31243;&#24212;&#35813;&#22312;&#26102;&#38388;&#32500;&#24230;&#19978;&#23637;&#29616;&#20986;&#24378;&#28872;&#30340;&#20851;&#32852;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#29305;&#24449;&#22270;&#30340;&#28436;&#21270;&#36981;&#24490;&#26102;&#38388;&#24207;&#21015;&#29305;&#24615;&#12290;&#36866;&#24403;&#30340;&#26102;&#38388;&#30417;&#30563;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#32593;&#32476;&#35757;&#32451;&#24615;&#33021;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26102;&#38388;&#30340;&#30417;&#30563;&#30693;&#35782;&#33976;&#39311;&#65288;TSKD&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#21367;&#31215;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;Conv-LSTM&#65289;&#25552;&#21462;&#23398;&#29983;&#32593;&#32476;&#22312;&#19981;&#21516;&#35757;&#32451;&#38454;&#27573;&#30340;&#26102;&#31354;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#21160;&#24577;&#30446;&#26631;&#35757;&#32451;&#23398;&#29983;&#32593;&#32476;&#65292;&#32780;&#19981;&#26159;&#38745;&#24577;&#30340;&#25945;&#24072;&#32593;&#32476;&#29305;&#24449;&#12290;&#36825;&#20010;&#36807;&#31243;&#23454;&#29616;&#20102;&#23398;&#29983;&#32593;&#32476;&#20013;&#26087;&#30693;&#35782;&#30340;&#20248;&#21270;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#36741;&#21161;&#24403;&#21069;&#30340;&#23398;&#20064;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reviewing plays an important role when learning knowledge. The knowledge acquisition at a certain time point may be strongly inspired with the help of previous experience. Thus the knowledge growing procedure should show strong relationship along the temporal dimension. In our research, we find that during the network training, the evolution of feature map follows temporal sequence property. A proper temporal supervision may further improve the network training performance. Inspired by this observation, we propose Temporal Supervised Knowledge Distillation (TSKD). Specifically, we extract the spatiotemporal features in the different training phases of student by convolutional Long Short-term memory network (Conv-LSTM). Then, we train the student net through a dynamic target, rather than static teacher network features. This process realizes the refinement of old knowledge in student network, and utilizes it to assist current learning. Extensive experiments verify the effectiveness and 
&lt;/p&gt;</description></item></channel></rss>