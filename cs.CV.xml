<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#37327;&#21270;&#22122;&#22768;&#26657;&#27491;&#26041;&#26696;&#65288;QNCD&#65289;&#65292;&#26088;&#22312;&#20943;&#23567;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#37327;&#21270;&#22122;&#22768;&#65292;&#35299;&#20915;&#20102;&#21518;&#35757;&#32451;&#37327;&#21270;&#23545;&#37319;&#26679;&#21152;&#36895;&#30340;&#24433;&#21709;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.19140</link><description>&lt;p&gt;
QNCD&#65306;&#25193;&#25955;&#27169;&#22411;&#30340;&#37327;&#21270;&#22122;&#22768;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
QNCD: Quantization Noise Correction for Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19140
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#37327;&#21270;&#22122;&#22768;&#26657;&#27491;&#26041;&#26696;&#65288;QNCD&#65289;&#65292;&#26088;&#22312;&#20943;&#23567;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#37327;&#21270;&#22122;&#22768;&#65292;&#35299;&#20915;&#20102;&#21518;&#35757;&#32451;&#37327;&#21270;&#23545;&#37319;&#26679;&#21152;&#36895;&#30340;&#24433;&#21709;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#21512;&#25104;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#65292;&#24314;&#31435;&#20102;&#36136;&#37327;&#21644;&#21019;&#36896;&#21147;&#30340;&#26032;&#22522;&#20934;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#36845;&#20195;&#21435;&#22122;&#36807;&#31243;&#20013;&#38656;&#35201;&#30340;&#23494;&#38598;&#35745;&#31639;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#37319;&#29992;&#12290;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#21152;&#36895;&#37319;&#26679;&#65292;&#23613;&#31649;&#20197;&#20302;&#27604;&#29305;&#35774;&#32622;&#26497;&#22823;&#38477;&#20302;&#20102;&#26679;&#26412;&#36136;&#37327;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#37327;&#21270;&#22122;&#22768;&#26657;&#27491;&#26041;&#26696;&#65288;QNCD&#65289;&#65292;&#26088;&#22312;&#20943;&#23567;&#25972;&#20010;&#37319;&#26679;&#36807;&#31243;&#20013;&#30340;&#37327;&#21270;&#22122;&#22768;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#20004;&#20010;&#20027;&#35201;&#30340;&#37327;&#21270;&#25361;&#25112;&#65306;&#20869;&#37096;&#21644;&#22806;&#37096;&#37327;&#21270;&#22122;&#22768;&#12290;&#20869;&#37096;&#37327;&#21270;&#22122;&#22768;&#20027;&#35201;&#30001;&#20110;&#23884;&#20837;&#22312;resblock&#27169;&#22359;&#20013;&#32780;&#21152;&#21095;&#65292;&#25193;&#23637;&#20102;&#28608;&#27963;&#37327;&#21270;&#33539;&#22260;&#65292;&#22312;&#27599;&#20010;&#21333;&#29420;&#30340;&#21435;&#22122;&#27493;&#39588;&#20013;&#22686;&#21152;&#20102;&#24178;&#25200;&#12290;&#27492;&#22806;&#65292;&#22806;&#37096;&#37327;&#21270;&#22122;&#22768;&#28304;&#33258;&#25972;&#20010;&#21435;&#22122;&#36807;&#31243;&#20013;&#30340;&#32047;&#31215;&#37327;&#21270;&#20559;&#24046;&#65292;&#25913;&#21464;&#20102;&#25968;&#25454;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19140v1 Announce Type: cross  Abstract: Diffusion models have revolutionized image synthesis, setting new benchmarks in quality and creativity. However, their widespread adoption is hindered by the intensive computation required during the iterative denoising process. Post-training quantization (PTQ) presents a solution to accelerate sampling, aibeit at the expense of sample quality, extremely in low-bit settings. Addressing this, our study introduces a unified Quantization Noise Correction Scheme (QNCD), aimed at minishing quantization noise throughout the sampling process. We identify two primary quantization challenges: intra and inter quantization noise. Intra quantization noise, mainly exacerbated by embeddings in the resblock module, extends activation quantization ranges, increasing disturbances in each single denosing step. Besides, inter quantization noise stems from cumulative quantization deviations across the entire denoising process, altering data distributions 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#36319;&#36394;&#31574;&#30053;&#21644;&#33258;&#21160;&#26631;&#35760;&#31639;&#27861;&#65292;&#38024;&#23545;&#20107;&#20214;&#30456;&#26426;&#20013;&#30340;&#38590;&#20197;&#36776;&#35782;&#30340;&#23545;&#35937;&#65292;&#25581;&#31034;&#20854;&#29305;&#24449;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.18330</link><description>&lt;p&gt;
&#20351;&#29992;&#36319;&#36394;&#36741;&#21161;&#30340;&#20107;&#20214;&#30456;&#26426;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Tracking-Assisted Object Detection with Event Cameras
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#36319;&#36394;&#31574;&#30053;&#21644;&#33258;&#21160;&#26631;&#35760;&#31639;&#27861;&#65292;&#38024;&#23545;&#20107;&#20214;&#30456;&#26426;&#20013;&#30340;&#38590;&#20197;&#36776;&#35782;&#30340;&#23545;&#35937;&#65292;&#25581;&#31034;&#20854;&#29305;&#24449;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20107;&#20214;&#30456;&#26426;&#20855;&#26377;&#39640;&#21160;&#24577;&#33539;&#22260;&#21644;&#26080;&#21160;&#24577;&#27169;&#31946;&#31561;&#29305;&#27530;&#23646;&#24615;&#65292;&#20107;&#20214;&#39537;&#21160;&#30446;&#26631;&#26816;&#27979;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24341;&#36215;&#20102;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29305;&#24449;&#30340;&#19981;&#21516;&#27493;&#24615;&#21644;&#31232;&#30095;&#24615;&#23548;&#33268;&#20102;&#30001;&#20110;&#30456;&#26426;&#19982;&#20043;&#27809;&#26377;&#30456;&#23545;&#36816;&#21160;&#32780;&#23548;&#33268;&#30340;&#30475;&#19981;&#35265;&#30340;&#23545;&#35937;&#65292;&#36825;&#23545;&#20219;&#21153;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#23558;&#36825;&#20123;&#30475;&#19981;&#35265;&#30340;&#23545;&#35937;&#35270;&#20026;&#20266;&#36974;&#25377;&#23545;&#35937;&#65292;&#24182;&#26088;&#22312;&#25581;&#31034;&#20854;&#29305;&#24449;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#35937;&#30340;&#21487;&#35265;&#24615;&#23646;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#26631;&#35760;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#29616;&#26377;&#30340;&#20107;&#20214;&#30456;&#26426;&#25968;&#25454;&#38598;&#19978;&#38468;&#21152;&#39069;&#22806;&#30340;&#21487;&#35265;&#24615;&#26631;&#31614;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;&#36319;&#36394;&#31574;&#30053;&#26469;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18330v1 Announce Type: cross  Abstract: Event-based object detection has recently garnered attention in the computer vision community due to the exceptional properties of event cameras, such as high dynamic range and no motion blur. However, feature asynchronism and sparsity cause invisible objects due to no relative motion to the camera, posing a significant challenge in the task. Prior works have studied various memory mechanisms to preserve as many features as possible at the current time, guided by temporal clues. While these implicit-learned memories retain some short-term information, they still struggle to preserve long-term features effectively. In this paper, we consider those invisible objects as pseudo-occluded objects and aim to reveal their features. Firstly, we introduce visibility attribute of objects and contribute an auto-labeling algorithm to append additional visibility labels on an existing event camera dataset. Secondly, we exploit tracking strategies fo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Hyper-STTN&#65292;&#19968;&#31181;&#22522;&#20110;&#36229;&#22270;&#30340;&#26102;&#31354;&#36716;&#25442;&#32593;&#32476;&#65292;&#29992;&#20110;&#20154;&#32676;&#36712;&#36857;&#39044;&#27979;&#12290;&#36890;&#36807;&#26500;&#24314;&#22810;&#23610;&#24230;&#36229;&#22270;&#26469;&#25429;&#25417;&#25317;&#25380;&#22330;&#26223;&#20013;&#30340;&#32676;&#20307;&#38388;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#21033;&#29992;&#31354;&#38388;-&#26102;&#38388;&#36716;&#25442;&#22120;&#26469;&#25429;&#25417;&#34892;&#20154;&#30340;&#25104;&#23545;&#28508;&#22312;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#20123;&#24322;&#26500;&#30340;&#32676;&#20307;&#38388;&#21644;&#25104;&#23545;&#38388;&#30456;&#20114;&#20316;&#29992;&#36890;&#36807;&#19968;&#20010;&#22810;&#27169;&#24577;&#36716;&#25442;&#32593;&#32476;&#36827;&#34892;&#34701;&#21512;&#21644;&#23545;&#20934;&#12290;</title><link>http://arxiv.org/abs/2401.06344</link><description>&lt;p&gt;
&#36229;&#32423;-STTN&#65306;&#31038;&#20132;&#32676;&#20307;&#24863;&#30693;&#30340;&#26102;&#31354;&#36716;&#25442;&#32593;&#32476;&#29992;&#20110;&#20154;&#20307;&#36712;&#36857;&#39044;&#27979;&#19982;&#36229;&#22270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Hyper-STTN: Social Group-aware Spatial-Temporal Transformer Network for Human Trajectory Prediction with Hypergraph Reasoning. (arXiv:2401.06344v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Hyper-STTN&#65292;&#19968;&#31181;&#22522;&#20110;&#36229;&#22270;&#30340;&#26102;&#31354;&#36716;&#25442;&#32593;&#32476;&#65292;&#29992;&#20110;&#20154;&#32676;&#36712;&#36857;&#39044;&#27979;&#12290;&#36890;&#36807;&#26500;&#24314;&#22810;&#23610;&#24230;&#36229;&#22270;&#26469;&#25429;&#25417;&#25317;&#25380;&#22330;&#26223;&#20013;&#30340;&#32676;&#20307;&#38388;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#21033;&#29992;&#31354;&#38388;-&#26102;&#38388;&#36716;&#25442;&#22120;&#26469;&#25429;&#25417;&#34892;&#20154;&#30340;&#25104;&#23545;&#28508;&#22312;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#20123;&#24322;&#26500;&#30340;&#32676;&#20307;&#38388;&#21644;&#25104;&#23545;&#38388;&#30456;&#20114;&#20316;&#29992;&#36890;&#36807;&#19968;&#20010;&#22810;&#27169;&#24577;&#36716;&#25442;&#32593;&#32476;&#36827;&#34892;&#34701;&#21512;&#21644;&#23545;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#21253;&#25324;&#26381;&#21153;&#26426;&#22120;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65292;&#39044;&#27979;&#25317;&#25380;&#30340;&#24847;&#22270;&#21644;&#36712;&#36857;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#29702;&#35299;&#29615;&#22659;&#21160;&#24577;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#19981;&#20165;&#22240;&#20026;&#23545;&#24314;&#27169;&#25104;&#23545;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#22797;&#26434;&#24615;&#65292;&#36824;&#22240;&#20026;&#32676;&#20307;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#35299;&#30721;&#25317;&#25380;&#22330;&#26223;&#20013;&#20840;&#38754;&#30340;&#25104;&#23545;&#21644;&#32676;&#20307;&#38388;&#30456;&#20114;&#20316;&#29992;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Hyper-STTN&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#36229;&#22270;&#30340;&#26102;&#31354;&#36716;&#25442;&#32593;&#32476;&#65292;&#29992;&#20110;&#20154;&#32676;&#36712;&#36857;&#39044;&#27979;&#12290;&#22312;Hyper-STTN&#20013;&#65292;&#36890;&#36807;&#19968;&#32452;&#22810;&#23610;&#24230;&#36229;&#22270;&#26500;&#24314;&#20102;&#25317;&#25380;&#30340;&#32676;&#20307;&#38388;&#30456;&#20851;&#24615;&#65292;&#36825;&#20123;&#36229;&#22270;&#20855;&#26377;&#19981;&#21516;&#30340;&#32676;&#20307;&#22823;&#23567;&#65292;&#36890;&#36807;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#27010;&#29575;&#30340;&#36229;&#22270;&#35889;&#21367;&#31215;&#36827;&#34892;&#25429;&#25417;&#12290;&#27492;&#22806;&#65292;&#36824;&#37319;&#29992;&#20102;&#31354;&#38388;-&#26102;&#38388;&#36716;&#25442;&#22120;&#26469;&#25429;&#25417;&#34892;&#20154;&#22312;&#31354;&#38388;-&#26102;&#38388;&#32500;&#24230;&#19978;&#30340;&#23545;&#29031;&#30456;&#20114;&#20316;&#29992;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#24322;&#26500;&#30340;&#32676;&#20307;&#38388;&#21644;&#25104;&#23545;&#38388;&#30456;&#20114;&#20316;&#29992;&#36890;&#36807;&#19968;&#20010;&#22810;&#27169;&#24577;&#36716;&#25442;&#32593;&#32476;&#36827;&#34892;&#34701;&#21512;&#21644;&#23545;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting crowded intents and trajectories is crucial in varouls real-world applications, including service robots and autonomous vehicles. Understanding environmental dynamics is challenging, not only due to the complexities of modeling pair-wise spatial and temporal interactions but also the diverse influence of group-wise interactions. To decode the comprehensive pair-wise and group-wise interactions in crowded scenarios, we introduce Hyper-STTN, a Hypergraph-based Spatial-Temporal Transformer Network for crowd trajectory prediction. In Hyper-STTN, crowded group-wise correlations are constructed using a set of multi-scale hypergraphs with varying group sizes, captured through random-walk robability-based hypergraph spectral convolution. Additionally, a spatial-temporal transformer is adapted to capture pedestrians' pair-wise latent interactions in spatial-temporal dimensions. These heterogeneous group-wise and pair-wise are then fused and aligned though a multimodal transformer net
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;"&#24425;&#31080;&#31080;&#25454;&#20551;&#35774;"&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#23376;&#32593;&#32476;&#21644;FSO&#36827;&#34892;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#12289;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#21644;&#35270;&#39057;&#22686;&#37327;&#23398;&#20064;&#65292;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#21644;&#26377;&#25928;&#30340;&#26435;&#37325;&#37325;&#29992;&#12290;</title><link>http://arxiv.org/abs/2312.11973</link><description>&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;: &#38754;&#21521;&#35270;&#39057;&#34920;&#31034;&#30340;&#20813;&#36951;&#24536;&#20248;&#32988;&#23376;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Continual Learning: Forget-free Winning Subnetworks for Video Representations. (arXiv:2312.11973v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;"&#24425;&#31080;&#31080;&#25454;&#20551;&#35774;"&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#23376;&#32593;&#32476;&#21644;FSO&#36827;&#34892;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#12289;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#21644;&#35270;&#39057;&#22686;&#37327;&#23398;&#20064;&#65292;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#21644;&#26377;&#25928;&#30340;&#26435;&#37325;&#37325;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;"&#24425;&#31080;&#31080;&#25454;&#20551;&#35774;"&#65288;LTH&#65289;&#30340;&#21551;&#21457;&#65292;&#35813;&#20551;&#35774;&#24378;&#35843;&#22312;&#36739;&#22823;&#30340;&#23494;&#38598;&#32593;&#32476;&#20013;&#23384;&#22312;&#39640;&#25928;&#23376;&#32593;&#32476;&#65292;&#30740;&#31350;&#20102;&#22312;&#36866;&#24403;&#30340;&#31232;&#30095;&#26465;&#20214;&#19979;&#34920;&#29616;&#20248;&#31168;&#30340;&#20248;&#32988;&#23376;&#32593;&#32476;&#65288;WSN&#65289;&#22312;&#21508;&#31181;&#36830;&#32493;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#23427;&#21033;&#29992;&#26469;&#33258;&#23494;&#38598;&#32593;&#32476;&#30340;&#39044;&#20808;&#23384;&#22312;&#30340;&#26435;&#37325;&#65292;&#22312;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#65288;TIL&#65289;&#22330;&#26223;&#20013;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#12290;&#22312;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;FSCIL&#65289;&#20013;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#31216;&#20026;&#36719;&#23376;&#32593;&#32476;&#65288;SoftNet&#65289;&#30340;WSN&#21464;&#20307;&#65292;&#20197;&#38450;&#27490;&#25968;&#25454;&#26679;&#26412;&#31232;&#32570;&#26102;&#30340;&#36807;&#25311;&#21512;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#20102;WSN&#26435;&#37325;&#30340;&#31232;&#30095;&#37325;&#29992;&#65292;&#29992;&#20110;&#35270;&#39057;&#22686;&#37327;&#23398;&#20064;&#65288;VIL&#65289;&#12290;&#32771;&#34385;&#20102;&#22312;WSN&#20013;&#20351;&#29992;&#20613;&#31435;&#21494;&#23376;&#31070;&#32463;&#36816;&#31639;&#22120;&#65288;FSO&#65289;&#65292;&#23427;&#33021;&#22815;&#23545;&#35270;&#39057;&#36827;&#34892;&#32039;&#20945;&#32534;&#30721;&#65292;&#24182;&#22312;&#19981;&#21516;&#24102;&#23485;&#19979;&#35782;&#21035;&#21487;&#37325;&#29992;&#30340;&#23376;&#32593;&#32476;&#12290;&#25105;&#20204;&#23558;FSO&#38598;&#25104;&#21040;&#19981;&#21516;&#30340;&#36830;&#32493;&#23398;&#20064;&#26550;&#26500;&#20013;&#65292;&#21253;&#25324;VIL&#12289;TIL&#21644;FSCIL&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the Lottery Ticket Hypothesis (LTH), which highlights the existence of efficient subnetworks within larger, dense networks, a high-performing Winning Subnetwork (WSN) in terms of task performance under appropriate sparsity conditions is considered for various continual learning tasks. It leverages pre-existing weights from dense networks to achieve efficient learning in Task Incremental Learning (TIL) scenarios. In Few-Shot Class Incremental Learning (FSCIL), a variation of WSN referred to as the Soft subnetwork (SoftNet) is designed to prevent overfitting when the data samples are scarce. Furthermore, the sparse reuse of WSN weights is considered for Video Incremental Learning (VIL). The use of Fourier Subneural Operator (FSO) within WSN is considered. It enables compact encoding of videos and identifies reusable subnetworks across varying bandwidths. We have integrated FSO into different architectural frameworks for continual learning, including VIL, TIL, and FSCIL. Our c
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#32467;&#21512;Copula&#29702;&#35770;&#26469;&#35299;&#20915;&#24322;&#26500;&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#21464;&#21270;&#26816;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.17448</link><description>&lt;p&gt;
NN-Copula-CD&#65306;&#19968;&#31181;&#22522;&#20110;Copula&#30340;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#24322;&#26500;&#36965;&#24863;&#22270;&#20687;&#21464;&#21270;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
NN-Copula-CD: A Copula-Guided Interpretable Neural Network for Change Detection in Heterogeneous Remote Sensing Images. (arXiv:2303.17448v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17448
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#32467;&#21512;Copula&#29702;&#35770;&#26469;&#35299;&#20915;&#24322;&#26500;&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#21464;&#21270;&#26816;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#21464;&#21270;&#26816;&#27979;&#26159;&#19968;&#20010;&#23454;&#38469;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#36807;&#21435;&#21313;&#24180;&#26469;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#21457;&#23637;&#35753;&#24322;&#26500;&#21464;&#21270;&#26816;&#27979;&#38382;&#39064;&#21463;&#30410;&#21290;&#27973;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;DNN&#22987;&#32456;&#20687;&#40657;&#21283;&#23376;&#19968;&#26679;&#65292;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#36825;&#38480;&#21046;&#20102;DNN&#22312;&#22823;&#22810;&#25968;&#23454;&#38469;&#21464;&#21270;&#26816;&#27979;&#24212;&#29992;&#20013;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#25511;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Copula&#30340;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#24322;&#26500;&#21464;&#21270;&#26816;&#27979;&#26041;&#27861;(NN-Copula-CD)&#12290;&#22312;NN-Copula-CD&#20013;&#65292;Copula&#30340;&#25968;&#23398;&#29305;&#24449;&#34987;&#35774;&#35745;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#30417;&#30563;&#19968;&#20010;&#31616;&#21333;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Change detection (CD) in heterogeneous remote sensing images is a practical and challenging issue for real-life emergencies. In the past decade, the heterogeneous CD problem has significantly benefited from the development of deep neural networks (DNN). However, the data-driven DNNs always perform like a black box where the lack of interpretability limits the trustworthiness and controllability of DNNs in most practical CD applications. As a strong knowledge-driven tool to measure correlation between random variables, Copula theory has been introduced into CD, yet it suffers from non-robust CD performance without manual prior selection for Copula functions. To address the above issues, we propose a knowledge-data-driven heterogeneous CD method (NN-Copula-CD) based on the Copula-guided interpretable neural network. In our NN-Copula-CD, the mathematical characteristics of Copula are designed as the losses to supervise a simple fully connected neural network to learn the correlation betwe
&lt;/p&gt;</description></item></channel></rss>