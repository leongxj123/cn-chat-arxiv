<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24191;&#20041;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;GCTMs&#65289;&#65292;&#33021;&#22815;&#22312;&#20219;&#20309;&#22122;&#22768;&#20998;&#24067;&#21644;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#23454;&#29616;&#36716;&#25442;&#12290;</title><link>https://arxiv.org/abs/2403.12510</link><description>&lt;p&gt;
&#22270;&#20687;&#25805;&#20316;&#30340;&#24191;&#20041;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generalized Consistency Trajectory Models for Image Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24191;&#20041;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;GCTMs&#65289;&#65292;&#33021;&#22815;&#22312;&#20219;&#20309;&#22122;&#22768;&#20998;&#24067;&#21644;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#23454;&#29616;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#26080;&#26465;&#20214;&#29983;&#25104;&#20197;&#21450;&#22270;&#20687;&#32534;&#36753;&#21644;&#24674;&#22797;&#31561;&#24212;&#29992;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#25193;&#25955;&#27169;&#22411;&#30340;&#25104;&#21151;&#22312;&#20110;&#25193;&#25955;&#30340;&#36845;&#20195;&#24615;&#36136;&#65306;&#25193;&#25955;&#23558;&#23558;&#22122;&#22768;&#21040;&#25968;&#25454;&#30340;&#22797;&#26434;&#26144;&#23556;&#36807;&#31243;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#31616;&#21333;&#30340;&#21435;&#22122;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#27880;&#20837;&#24341;&#23548;&#39033;&#65292;&#25105;&#20204;&#33021;&#22815;&#23545;&#29983;&#25104;&#36807;&#31243;&#36827;&#34892;&#31934;&#32454;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#36845;&#20195;&#36807;&#31243;&#20063;&#24120;&#24120;&#35745;&#31639;&#23494;&#38598;&#65292;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#25968;&#21313;&#27425;&#29978;&#33267;&#25968;&#21315;&#27425;&#20989;&#25968;&#35780;&#20272;&#12290;&#34429;&#28982;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;CTMs&#65289;&#21487;&#20197;&#22312;&#27010;&#29575;&#27969;ODE&#65288;PFODE&#65289;&#19978;&#20219;&#24847;&#26102;&#38388;&#28857;&#20043;&#38388;&#36827;&#34892;&#36941;&#21382;&#65292;&#24182;&#19988;&#36890;&#36807;&#21333;&#27425;&#20989;&#25968;&#35780;&#20272;&#36827;&#34892;&#24471;&#20998;&#25512;&#23548;&#65292;&#20294;CTMs&#20165;&#20801;&#35768;&#20174;&#39640;&#26031;&#22122;&#22768;&#36716;&#25442;&#20026;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#24191;&#20041;CTMs&#65288;GCTMs&#65289;&#26469;&#21457;&#25381;CTMs&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#23454;&#29616;&#22312;&#20219;&#20309;&#22122;&#22768;&#20998;&#24067;&#21644;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#36827;&#34892;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12510v1 Announce Type: cross  Abstract: Diffusion-based generative models excel in unconditional generation, as well as on applied tasks such as image editing and restoration. The success of diffusion models lies in the iterative nature of diffusion: diffusion breaks down the complex process of mapping noise to data into a sequence of simple denoising tasks. Moreover, we are able to exert fine-grained control over the generation process by injecting guidance terms into each denoising step. However, the iterative process is also computationally intensive, often taking from tens up to thousands of function evaluations. Although consistency trajectory models (CTMs) enable traversal between any time points along the probability flow ODE (PFODE) and score inference with a single function evaluation, CTMs only allow translation from Gaussian noise to data. Thus, this work aims to unlock the full potential of CTMs by proposing generalized CTMs (GCTMs), which translate between arbit
&lt;/p&gt;</description></item><item><title>RACR-MIL&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#24369;&#30417;&#30563;&#30340;&#30382;&#32932;&#30284;&#20998;&#32423;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#26080;&#38656;&#32454;&#31890;&#24230;&#30340;&#32959;&#30244;&#27880;&#37322;&#65292;&#36890;&#36807;&#22312;&#20999;&#29255;&#22270;&#20687;&#20013;&#30340;&#29926;&#29255;&#31456;&#33410;&#20013;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#65292;&#21487;&#20197;&#20026;&#20999;&#29255;&#22270;&#20687;&#20998;&#37197;&#20998;&#32423;&#12290;&#35813;&#26041;&#27861;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#20351;&#29992;&#31354;&#38388;&#21644;&#35821;&#20041;&#25509;&#36817;&#24615;&#23450;&#20041;&#20999;&#29255;&#22270;&#20687;&#22270;&#20687;&#20197;&#32534;&#30721;&#32959;&#30244;&#21306;&#22495;&#30340;&#23616;&#37096;&#21644;&#38750;&#23616;&#37096;&#20381;&#36182;&#20851;&#31995;&#20197;&#21450;&#20351;&#29992;&#24207;&#25968;&#25490;&#21517;&#32422;&#26463;&#20445;&#35777;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#24615;&#33021;</title><link>http://arxiv.org/abs/2308.15618</link><description>&lt;p&gt;
RACR-MIL&#65306;&#20351;&#29992;&#22522;&#20110;&#25490;&#21517;&#24863;&#30693;&#32972;&#26223;&#25512;&#29702;&#30340;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#36827;&#34892;&#24369;&#30417;&#30563;&#30340;&#30382;&#32932;&#30284;&#20998;&#32423;
&lt;/p&gt;
&lt;p&gt;
RACR-MIL: Weakly Supervised Skin Cancer Grading using Rank-Aware Contextual Reasoning on Whole Slide Images. (arXiv:2308.15618v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15618
&lt;/p&gt;
&lt;p&gt;
RACR-MIL&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#24369;&#30417;&#30563;&#30340;&#30382;&#32932;&#30284;&#20998;&#32423;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#26080;&#38656;&#32454;&#31890;&#24230;&#30340;&#32959;&#30244;&#27880;&#37322;&#65292;&#36890;&#36807;&#22312;&#20999;&#29255;&#22270;&#20687;&#20013;&#30340;&#29926;&#29255;&#31456;&#33410;&#20013;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#65292;&#21487;&#20197;&#20026;&#20999;&#29255;&#22270;&#20687;&#20998;&#37197;&#20998;&#32423;&#12290;&#35813;&#26041;&#27861;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#20351;&#29992;&#31354;&#38388;&#21644;&#35821;&#20041;&#25509;&#36817;&#24615;&#23450;&#20041;&#20999;&#29255;&#22270;&#20687;&#22270;&#20687;&#20197;&#32534;&#30721;&#32959;&#30244;&#21306;&#22495;&#30340;&#23616;&#37096;&#21644;&#38750;&#23616;&#37096;&#20381;&#36182;&#20851;&#31995;&#20197;&#21450;&#20351;&#29992;&#24207;&#25968;&#25490;&#21517;&#32422;&#26463;&#20445;&#35777;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Cutaneous squamous cell cancer (cSCC) is the second most common skin cancer in the US. It is diagnosed by manual multi-class tumor grading using a tissue whole slide image (WSI), which is subjective and suffers from inter-pathologist variability. We propose an automated weakly-supervised grading approach for cSCC WSIs that is trained using WSI-level grade and does not require fine-grained tumor annotations. The proposed model, RACR-MIL, transforms each WSI into a bag of tiled patches and leverages attention-based multiple-instance learning to assign a WSI-level grade. We propose three key innovations to address general as well as cSCC-specific challenges in tumor grading. First, we leverage spatial and semantic proximity to define a WSI graph that encodes both local and non-local dependencies between tumor regions and leverage graph attention convolution to derive contextual patch features. Second, we introduce a novel ordinal ranking constraint on the patch attention network to ensure
&lt;/p&gt;
&lt;p&gt;
Cutaneous squamous cell cancer (cSCC) is the second most common skin cancer in the US. It is diagnosed by manual multi-class tumor grading using a tissue whole slide image (WSI), which is subjective and suffers from inter-pathologist variability. We propose an automated weakly-supervised grading approach for cSCC WSIs that is trained using WSI-level grade and does not require fine-grained tumor annotations. The proposed model, RACR-MIL, transforms each WSI into a bag of tiled patches and leverages attention-based multiple-instance learning to assign a WSI-level grade. We propose three key innovations to address general as well as cSCC-specific challenges in tumor grading. First, we leverage spatial and semantic proximity to define a WSI graph that encodes both local and non-local dependencies between tumor regions and leverage graph attention convolution to derive contextual patch features. Second, we introduce a novel ordinal ranking constraint on the patch attention network to ensure
&lt;/p&gt;</description></item><item><title>PFB-Diff &#26159;&#19968;&#20010;&#36890;&#36807;&#28176;&#36827;&#29305;&#24449;&#28151;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25991;&#26412;&#39537;&#21160;&#30340;&#22270;&#20687;&#32534;&#36753;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#20687;&#32032;&#32423;&#28151;&#21512;&#20013;&#20135;&#29983;&#30340;&#20266;&#24433;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22810;&#32423;&#29305;&#24449;&#28151;&#21512;&#21644;&#27880;&#24847;&#21147;&#23631;&#34109;&#26426;&#21046;&#30830;&#20445;&#20102;&#32534;&#36753;&#22270;&#20687;&#30340;&#35821;&#20041;&#36830;&#36143;&#24615;&#21644;&#39640;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.16894</link><description>&lt;p&gt;
PFB-Diff: &#28176;&#36827;&#29305;&#24449;&#28151;&#21512;&#25193;&#25955;&#29992;&#20110;&#25991;&#26412;&#39537;&#21160;&#30340;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
PFB-Diff: Progressive Feature Blending Diffusion for Text-driven Image Editing. (arXiv:2306.16894v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16894
&lt;/p&gt;
&lt;p&gt;
PFB-Diff &#26159;&#19968;&#20010;&#36890;&#36807;&#28176;&#36827;&#29305;&#24449;&#28151;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25991;&#26412;&#39537;&#21160;&#30340;&#22270;&#20687;&#32534;&#36753;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#20687;&#32032;&#32423;&#28151;&#21512;&#20013;&#20135;&#29983;&#30340;&#20266;&#24433;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22810;&#32423;&#29305;&#24449;&#28151;&#21512;&#21644;&#27880;&#24847;&#21147;&#23631;&#34109;&#26426;&#21046;&#30830;&#20445;&#20102;&#32534;&#36753;&#22270;&#20687;&#30340;&#35821;&#20041;&#36830;&#36143;&#24615;&#21644;&#39640;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#23637;&#31034;&#20102;&#20854;&#21512;&#25104;&#22810;&#26679;&#24615;&#21644;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#21331;&#36234;&#33021;&#21147;&#65292;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#23558;&#20854;&#24212;&#29992;&#20110;&#23454;&#38469;&#22270;&#20687;&#32534;&#36753;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#23616;&#37096;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#24120;&#24120;&#22240;&#20026;&#30446;&#26631;&#22270;&#20687;&#21644;&#25193;&#25955;&#28508;&#22312;&#21464;&#37327;&#30340;&#20687;&#32032;&#32423;&#28151;&#21512;&#32780;&#20135;&#29983;&#19981;&#26399;&#26395;&#30340;&#20266;&#24433;&#65292;&#32570;&#20047;&#32500;&#25345;&#22270;&#20687;&#19968;&#33268;&#24615;&#25152;&#24517;&#38656;&#30340;&#35821;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PFB-Diff&#65292;&#19968;&#31181;&#36880;&#27493;&#29305;&#24449;&#28151;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#32534;&#36753;&#12290;&#19982;&#20197;&#24448;&#26041;&#27861;&#19981;&#21516;&#65292;PFB-Diff&#36890;&#36807;&#22810;&#32423;&#29305;&#24449;&#28151;&#21512;&#23558;&#25991;&#26412;&#24341;&#23548;&#29983;&#25104;&#30340;&#20869;&#23481;&#19982;&#30446;&#26631;&#22270;&#20687;&#26080;&#32541;&#38598;&#25104;&#22312;&#19968;&#36215;&#12290;&#28145;&#23618;&#29305;&#24449;&#20013;&#32534;&#30721;&#30340;&#20016;&#23500;&#35821;&#20041;&#21644;&#20174;&#39640;&#21040;&#20302;&#32423;&#21035;&#30340;&#28176;&#36827;&#28151;&#21512;&#26041;&#26696;&#30830;&#20445;&#20102;&#32534;&#36753;&#22270;&#20687;&#30340;&#35821;&#20041;&#36830;&#36143;&#24615;&#21644;&#39640;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#20132;&#21449;&#27880;&#24847;&#21147;&#23618;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#27880;&#24847;&#21147;&#23631;&#34109;&#26426;&#21046;&#65292;&#20197;&#38480;&#21046;&#29305;&#23450;&#35789;&#35821;&#23545;&#32534;&#36753;&#22270;&#20687;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have showcased their remarkable capability to synthesize diverse and high-quality images, sparking interest in their application for real image editing. However, existing diffusion-based approaches for local image editing often suffer from undesired artifacts due to the pixel-level blending of the noised target images and diffusion latent variables, which lack the necessary semantics for maintaining image consistency. To address these issues, we propose PFB-Diff, a Progressive Feature Blending method for Diffusion-based image editing. Unlike previous methods, PFB-Diff seamlessly integrates text-guided generated content into the target image through multi-level feature blending. The rich semantics encoded in deep features and the progressive blending scheme from high to low levels ensure semantic coherence and high quality in edited images. Additionally, we introduce an attention masking mechanism in the cross-attention layers to confine the impact of specific words to 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ST-SSAD&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#31995;&#32479;&#22320;&#35843;&#25972;&#25968;&#25454;&#22686;&#24378;&#30340;&#36229;&#21442;&#25968;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#25552;&#39640;&#33258;&#25105;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65288;SSAD&#65289;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12033</link><description>&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#31471;&#21040;&#31471;&#22686;&#24378;&#36229;&#21442;&#25968;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
End-to-End Augmentation Hyperparameter Tuning for Self-Supervised Anomaly Detection. (arXiv:2306.12033v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12033
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ST-SSAD&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#31995;&#32479;&#22320;&#35843;&#25972;&#25968;&#25454;&#22686;&#24378;&#30340;&#36229;&#21442;&#25968;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#25552;&#39640;&#33258;&#25105;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65288;SSAD&#65289;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#33539;&#20363;&#65292;&#23427;&#20026;&#29616;&#23454;&#38382;&#39064;&#25552;&#20379;&#33258;&#20135;&#29983;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#36991;&#20813;&#20102;&#32321;&#29712;&#30340;&#25163;&#21160;&#26631;&#27880;&#24037;&#20316;&#12290;SSL&#23545;&#20110;&#26080;&#30417;&#30563;&#20219;&#21153;&#65292;&#22914;&#24322;&#24120;&#26816;&#27979;&#23588;&#20854;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#26631;&#35760;&#30340;&#24322;&#24120;&#36890;&#24120;&#19981;&#23384;&#22312;&#25110;&#38590;&#20197;&#33719;&#24471;&#12290;&#34429;&#28982;&#33258;&#25105;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65288;SSAD&#65289;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#25991;&#29486;&#21364;&#26410;&#23558;&#25968;&#25454;&#22686;&#24378;&#35270;&#20026;&#36229;&#21442;&#25968;&#12290;&#21516;&#26102;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22686;&#24378;&#36873;&#25321;&#23545;&#26816;&#27979;&#24615;&#33021;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ST-SSAD&#65288;&#33258;&#25105;&#35843;&#25972;&#33258;&#25105;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20851;&#20110;&#20005;&#26684;&#35843;&#25972;&#22686;&#24378;&#30340;SSAD&#30340;&#31532;&#19968;&#20010;&#31995;&#32479;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#36129;&#29486;&#12290;&#31532;&#19968;&#26159;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#39564;&#35777;&#25439;&#22833;&#20989;&#25968;&#65292;&#37327;&#21270;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#19982;&#65288;&#26080;&#26631;&#31614;&#65289;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#30340;&#23545;&#40784;&#31243;&#24230;&#12290;&#22312;&#21407;&#21017;&#19978;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#26368;&#36817;&#39640;&#25928;&#30340;&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20511;&#37492;&#30340;&#26080;&#30417;&#30563;&#39564;&#35777;&#26041;&#26696;&#21644;&#22686;&#24378;&#25968;&#25454;&#25628;&#32034;&#31574;&#30053;&#65292;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;SSAD&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#24418;&#24335;&#65292;&#23558;&#36731;&#37327;&#32423;&#25968;&#25454;&#22686;&#24378;&#25628;&#32034;&#22120;&#30340;&#31616;&#21333;&#38598;&#25104;&#12290;&#22312;&#21508;&#31181;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#22686;&#24378;&#35843;&#25972;&#26041;&#27861;&#30456;&#23545;&#20110;&#20197;&#21069;&#30340;&#26368;&#26032;&#32467;&#26524;&#21487;&#20197;&#33719;&#24471;&#19968;&#33268;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#26368;&#36817;&#30340;&#26377;&#30417;&#30563;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has emerged as a promising paradigm that presents self-generated supervisory signals to real-world problems, bypassing the extensive manual labeling burden. SSL is especially attractive for unsupervised tasks such as anomaly detection, where labeled anomalies are often nonexistent and costly to obtain. While self-supervised anomaly detection (SSAD) has seen a recent surge of interest, the literature has failed to treat data augmentation as a hyperparameter. Meanwhile, recent works have reported that the choice of augmentation has significant impact on detection performance. In this paper, we introduce ST-SSAD (Self-Tuning Self-Supervised Anomaly Detection), the first systematic approach to SSAD in regards to rigorously tuning augmentation. To this end, our work presents two key contributions. The first is a new unsupervised validation loss that quantifies the alignment between the augmented training data and the (unlabeled) test data. In principle we adop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#22788;&#29702;&#23398;&#20064;&#31639;&#27861;&#65288;CBAGAN-RRT&#65289;&#30340;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#20351;&#29992;&#21367;&#31215;&#22359;&#27880;&#24847;&#21147;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25214;&#21040;&#26356;&#20248;&#30340;&#26368;&#20339;&#36335;&#24452;&#24182;&#25552;&#39640;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#19982;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#30456;&#27604;&#65292;&#22312;&#22270;&#20687;&#36136;&#37327;&#29983;&#25104;&#25351;&#26631;&#21644;&#36335;&#24452;&#35268;&#21010;&#25351;&#26631;&#26041;&#38754;&#37117;&#34920;&#29616;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2305.10442</link><description>&lt;p&gt;
CBAGAN-RRT: &#21367;&#31215;&#22359;&#27880;&#24847;&#21147;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29992;&#20110;&#22522;&#20110;&#37319;&#26679;&#30340;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
CBAGAN-RRT: Convolutional Block Attention Generative Adversarial Network for Sampling-Based Path Planning. (arXiv:2305.10442v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#22788;&#29702;&#23398;&#20064;&#31639;&#27861;&#65288;CBAGAN-RRT&#65289;&#30340;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#20351;&#29992;&#21367;&#31215;&#22359;&#27880;&#24847;&#21147;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25214;&#21040;&#26356;&#20248;&#30340;&#26368;&#20339;&#36335;&#24452;&#24182;&#25552;&#39640;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#19982;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#30456;&#27604;&#65292;&#22312;&#22270;&#20687;&#36136;&#37327;&#29983;&#25104;&#25351;&#26631;&#21644;&#36335;&#24452;&#35268;&#21010;&#25351;&#26631;&#26041;&#38754;&#37117;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#37319;&#26679;&#30340;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;&#22312;&#33258;&#20027;&#26426;&#22120;&#20154;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20294;&#26159;&#65292;&#22522;&#20110;RRT&#31639;&#27861;&#30340;&#19968;&#20010;&#24120;&#35265;&#38382;&#39064;&#26159;&#29983;&#25104;&#30340;&#21021;&#22987;&#36335;&#24452;&#19981;&#26159;&#26368;&#20248;&#30340;&#65292;&#32780;&#19988;&#25910;&#25947;&#36895;&#24230;&#36807;&#24930;&#65292;&#26080;&#27861;&#24212;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21367;&#31215;&#22359;&#27880;&#24847;&#21147;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#22270;&#20687;&#22788;&#29702;&#23398;&#20064;&#31639;&#27861;&#65288;CBAGAN-RRT&#65289;&#65292;&#20197;&#35774;&#35745;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#25214;&#21040;&#26356;&#20248;&#30340;&#26368;&#20339;&#36335;&#24452;&#65292;&#24182;&#25552;&#39640;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;GAN&#27169;&#22411;&#29983;&#25104;&#30340;&#36335;&#24452;&#27010;&#29575;&#20998;&#24067;&#29992;&#20110;&#24341;&#23548;RRT&#31639;&#27861;&#30340;&#37319;&#26679;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#30001; \cite {zhang2021generative} &#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#32593;&#32476;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#22270;&#20687;&#36136;&#37327;&#29983;&#25104;&#25351;&#26631;&#65288;&#22914;IOU&#20998;&#25968;&#65292;Dice&#20998;&#25968;&#65289;&#21644;&#36335;&#24452;&#35268;&#21010;&#25351;&#26631;&#65288;&#22914;&#36335;&#24452;&#38271;&#24230;&#21644;&#25104;&#21151;&#29575;&#65289;&#26041;&#38754;&#22343;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sampling-based path planning algorithms play an important role in autonomous robotics. However, a common problem among the RRT-based algorithms is that the initial path generated is not optimal and the convergence is too slow to be used in real-world applications. In this paper, we propose a novel image-based learning algorithm (CBAGAN-RRT) using a Convolutional Block Attention Generative Adversarial Network with a combination of spatial and channel attention and a novel loss function to design the heuristics, find a better optimal path, and improve the convergence of the algorithm both concerning time and speed. The probability distribution of the paths generated from our GAN model is used to guide the sampling process for the RRT algorithm. We train and test our network on the dataset generated by \cite{zhang2021generative} and demonstrate that our algorithm outperforms the previous state-of-the-art algorithms using both the image quality generation metrics like IOU Score, Dice Score
&lt;/p&gt;</description></item></channel></rss>