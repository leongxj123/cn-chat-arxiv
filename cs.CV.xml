<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#25552;&#20986;&#20102;GeFF&#65288;&#36890;&#29992;&#29305;&#24449;&#22330;&#65289;&#65292;&#20316;&#20026;&#23548;&#33322;&#21644;&#25805;&#20316;&#30340;&#32479;&#19968;&#34920;&#31034;&#65292;&#21487;&#20197;&#23454;&#26102;&#25191;&#34892;&#65292;&#36890;&#36807;&#23558;&#29983;&#25104;&#30340;&#20016;&#23500;&#22330;&#26223;&#20808;&#39564;&#19982;&#33258;&#28982;&#35821;&#35328;&#23545;&#40784;&#26469;&#25552;&#39640;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.07563</link><description>&lt;p&gt;
&#23398;&#20064;&#31227;&#21160;&#25805;&#20316;&#30340;&#36890;&#29992;&#29305;&#24449;&#22330;
&lt;/p&gt;
&lt;p&gt;
Learning Generalizable Feature Fields for Mobile Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07563
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;GeFF&#65288;&#36890;&#29992;&#29305;&#24449;&#22330;&#65289;&#65292;&#20316;&#20026;&#23548;&#33322;&#21644;&#25805;&#20316;&#30340;&#32479;&#19968;&#34920;&#31034;&#65292;&#21487;&#20197;&#23454;&#26102;&#25191;&#34892;&#65292;&#36890;&#36807;&#23558;&#29983;&#25104;&#30340;&#20016;&#23500;&#22330;&#26223;&#20808;&#39564;&#19982;&#33258;&#28982;&#35821;&#35328;&#23545;&#40784;&#26469;&#25552;&#39640;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#25805;&#20316;&#20013;&#30340;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#26159;&#22914;&#20309;&#20197;&#32479;&#19968;&#30340;&#26041;&#24335;&#34920;&#31034;&#29289;&#20307;&#21644;&#22330;&#26223;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#21487;&#20197;&#21516;&#26102;&#29992;&#20110;&#22312;&#29615;&#22659;&#20013;&#23548;&#33322;&#21644;&#25805;&#20316;&#29289;&#20307;&#12290;&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;GeFF&#65288;&#36890;&#29992;&#29305;&#24449;&#22330;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22330;&#26223;&#32423;&#30340;&#36890;&#29992;&#31070;&#32463;&#29305;&#24449;&#22330;&#65292;&#20316;&#20026;&#23548;&#33322;&#21644;&#25805;&#20316;&#30340;&#32479;&#19968;&#34920;&#31034;&#65292;&#21487;&#20197;&#23454;&#26102;&#25191;&#34892;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#29983;&#25104;&#26032;&#35270;&#22270;&#21512;&#25104;&#35270;&#20026;&#19968;&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#28982;&#21518;&#36890;&#36807;CLIP&#29305;&#24449;&#25552;&#28860;&#23558;&#29983;&#25104;&#30340;&#20016;&#23500;&#22330;&#26223;&#20808;&#39564;&#19982;&#33258;&#28982;&#35821;&#35328;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07563v1 Announce Type: cross  Abstract: An open problem in mobile manipulation is how to represent objects and scenes in a unified manner, so that robots can use it both for navigating in the environment and manipulating objects. The latter requires capturing intricate geometry while understanding fine-grained semantics, whereas the former involves capturing the complexity inherit to an expansive physical scale. In this work, we present GeFF (Generalizable Feature Fields), a scene-level generalizable neural feature field that acts as a unified representation for both navigation and manipulation that performs in real-time. To do so, we treat generative novel view synthesis as a pre-training task, and then align the resulting rich scene priors with natural language via CLIP feature distillation. We demonstrate the effectiveness of this approach by deploying GeFF on a quadrupedal robot equipped with a manipulator. We evaluate GeFF's ability to generalize to open-set objects as 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#25552;&#20379;&#20102;&#20851;&#20110;&#22522;&#20110;&#20266;&#26631;&#31614;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#39046;&#22495;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#30340;&#20840;&#38754;&#19988;&#26377;&#32452;&#32455;&#30340;&#27010;&#36848;&#65292;&#25506;&#35752;&#20102;&#20266;&#26631;&#31614;&#25216;&#26415;&#22312;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#30340;&#20855;&#20307;&#26041;&#27861;&#65292;&#36824;&#30740;&#31350;&#20102;&#20854;&#22312;&#21307;&#23398;&#21644;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.01909</link><description>&lt;p&gt;
&#22522;&#20110;&#20266;&#26631;&#31614;&#30340;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Semantic Segmentation Based on Pseudo-Labels: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01909
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#25552;&#20379;&#20102;&#20851;&#20110;&#22522;&#20110;&#20266;&#26631;&#31614;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#39046;&#22495;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#30340;&#20840;&#38754;&#19988;&#26377;&#32452;&#32455;&#30340;&#27010;&#36848;&#65292;&#25506;&#35752;&#20102;&#20266;&#26631;&#31614;&#25216;&#26415;&#22312;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#30340;&#20855;&#20307;&#26041;&#27861;&#65292;&#36824;&#30740;&#31350;&#20102;&#20854;&#22312;&#21307;&#23398;&#21644;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#20998;&#21106;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#28909;&#38376;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#20391;&#37325;&#20110;&#22522;&#20110;&#35821;&#20041;&#23545;&#22270;&#20687;&#20013;&#30340;&#20687;&#32032;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#30417;&#30563;&#23398;&#20064;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#32780;&#36880;&#20687;&#32032;&#26631;&#35760;&#22270;&#20687;&#30340;&#36807;&#31243;&#32791;&#26102;&#19988;&#32321;&#29712;&#12290;&#26412;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#39046;&#22495;&#20013;&#20266;&#26631;&#31614;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#30340;&#39318;&#27425;&#32508;&#21512;&#21644;&#26377;&#32452;&#32455;&#30340;&#27010;&#36848;&#65292;&#25105;&#20204;&#20174;&#19981;&#21516;&#35282;&#24230;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#29305;&#23450;&#24212;&#29992;&#39046;&#22495;&#30340;&#20855;&#20307;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#20266;&#26631;&#31614;&#25216;&#26415;&#22312;&#21307;&#23398;&#21644;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20123;&#21487;&#34892;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01909v1 Announce Type: cross  Abstract: Semantic segmentation is an important and popular research area in computer vision that focuses on classifying pixels in an image based on their semantics. However, supervised deep learning requires large amounts of data to train models and the process of labeling images pixel by pixel is time-consuming and laborious. This review aims to provide a first comprehensive and organized overview of the state-of-the-art research results on pseudo-label methods in the field of semi-supervised semantic segmentation, which we categorize from different perspectives and present specific methods for specific application areas. In addition, we explore the application of pseudo-label technology in medical and remote-sensing image segmentation. Finally, we also propose some feasible future research directions to address the existing challenges.
&lt;/p&gt;</description></item><item><title>BioNeRF&#26159;&#19968;&#31181;&#29983;&#29289;&#21512;&#29702;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#36752;&#23556;&#22330;&#23545;&#22330;&#26223;&#36827;&#34892;3D&#34920;&#31034;&#24182;&#21512;&#25104;&#26032;&#35270;&#22270;&#12290;&#23427;&#23454;&#29616;&#20102;&#19968;&#31181;&#35748;&#30693;&#21551;&#21457;&#30340;&#26426;&#21046;&#65292;&#25552;&#39640;&#20102;&#23384;&#20648;&#33021;&#21147;&#21644;&#25552;&#21462;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#21644;&#21512;&#25104;&#25968;&#25454;&#30340;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#20197;&#20154;&#30340;&#24863;&#30693;&#20026;&#22522;&#30784;&#30340;&#36136;&#37327;&#24230;&#37327;&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.07310</link><description>&lt;p&gt;
BioNeRF: &#29992;&#20110;&#35270;&#22270;&#21512;&#25104;&#30340;&#29983;&#29289;&#21512;&#29702;&#31070;&#32463;&#36752;&#23556;&#22330;
&lt;/p&gt;
&lt;p&gt;
BioNeRF: Biologically Plausible Neural Radiance Fields for View Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07310
&lt;/p&gt;
&lt;p&gt;
BioNeRF&#26159;&#19968;&#31181;&#29983;&#29289;&#21512;&#29702;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#36752;&#23556;&#22330;&#23545;&#22330;&#26223;&#36827;&#34892;3D&#34920;&#31034;&#24182;&#21512;&#25104;&#26032;&#35270;&#22270;&#12290;&#23427;&#23454;&#29616;&#20102;&#19968;&#31181;&#35748;&#30693;&#21551;&#21457;&#30340;&#26426;&#21046;&#65292;&#25552;&#39640;&#20102;&#23384;&#20648;&#33021;&#21147;&#21644;&#25552;&#21462;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#21644;&#21512;&#25104;&#25968;&#25454;&#30340;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#20197;&#20154;&#30340;&#24863;&#30693;&#20026;&#22522;&#30784;&#30340;&#36136;&#37327;&#24230;&#37327;&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;BioNeRF&#65292;&#19968;&#31181;&#29983;&#29289;&#21512;&#29702;&#30340;&#26550;&#26500;&#65292;&#23427;&#36890;&#36807;&#36752;&#23556;&#22330;&#23545;&#22330;&#26223;&#36827;&#34892;3D&#34920;&#31034;&#24182;&#21512;&#25104;&#26032;&#35270;&#22270;&#12290;&#30001;&#20110;NeRF&#20381;&#36182;&#20110;&#32593;&#32476;&#26435;&#37325;&#26469;&#23384;&#20648;&#22330;&#26223;&#30340;&#19977;&#32500;&#34920;&#31034;&#65292;BioNeRF&#23454;&#29616;&#20102;&#19968;&#31181;&#21463;&#35748;&#30693;&#21551;&#21457;&#30340;&#26426;&#21046;&#65292;&#23558;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#30340;&#36755;&#20837;&#34701;&#21512;&#25104;&#20869;&#23384;&#31867;&#20284;&#30340;&#32467;&#26500;&#65292;&#25552;&#39640;&#23384;&#20648;&#33021;&#21147;&#24182;&#25552;&#21462;&#26356;&#22810;&#20869;&#22312;&#21644;&#30456;&#20851;&#20449;&#24687;&#12290;BioNeRF&#36824;&#27169;&#20223;&#20102;&#37329;&#23383;&#22612;&#32454;&#32990;&#20013;&#20851;&#20110;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#19968;&#31181;&#34892;&#20026;&#65292;&#20854;&#20013;&#20869;&#23384;&#20316;&#20026;&#19978;&#19979;&#25991;&#25552;&#20379;&#65292;&#24182;&#19982;&#20004;&#20010;&#21518;&#32493;&#31070;&#32463;&#27169;&#22411;&#30340;&#36755;&#20837;&#30456;&#32467;&#21512;&#65292;&#19968;&#20010;&#36127;&#36131;&#29983;&#25104;&#23481;&#31215;&#23494;&#24230;&#65292;&#21478;&#19968;&#20010;&#36127;&#36131;&#28210;&#26579;&#22330;&#26223;&#30340;&#39068;&#33394;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;BioNeRF&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#65288;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#21644;&#21512;&#25104;&#25968;&#25454;&#65289;&#19978;&#36229;&#36807;&#20102;&#20197;&#20154;&#30340;&#24863;&#30693;&#20026;&#22522;&#30784;&#30340;&#36136;&#37327;&#24230;&#37327;&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents BioNeRF, a biologically plausible architecture that models scenes in a 3D representation and synthesizes new views through radiance fields. Since NeRF relies on the network weights to store the scene's 3-dimensional representation, BioNeRF implements a cognitive-inspired mechanism that fuses inputs from multiple sources into a memory-like structure, improving the storing capacity and extracting more intrinsic and correlated information. BioNeRF also mimics a behavior observed in pyramidal cells concerning contextual information, in which the memory is provided as the context and combined with the inputs of two subsequent neural models, one responsible for producing the volumetric densities and the other the colors used to render the scene. Experimental results show that BioNeRF outperforms state-of-the-art results concerning a quality measure that encodes human perception in two datasets: real-world images and synthetic data.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26041;&#21521;&#30340;&#35270;&#35273;-&#35821;&#20041;&#23884;&#20837;&#27169;&#22411;&#65288;DOVE&#65289;&#65292;&#36890;&#36807;&#21306;&#22495;&#23548;&#21521;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#36731;&#37327;&#32423;&#30340;&#25991;&#23383;&#22522;&#22240;&#36741;&#21161;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#36965;&#24863;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#20013;&#30340;&#35270;&#35273;-&#35821;&#20041;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08276</link><description>&lt;p&gt;
&#38754;&#21521;&#26041;&#21521;&#30340;&#35270;&#35273;-&#35821;&#20041;&#23884;&#20837;&#27169;&#22411;&#22312;&#36965;&#24863;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Direction-Oriented Visual-semantic Embedding Model for Remote Sensing Image-text Retrieval. (arXiv:2310.08276v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08276
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26041;&#21521;&#30340;&#35270;&#35273;-&#35821;&#20041;&#23884;&#20837;&#27169;&#22411;&#65288;DOVE&#65289;&#65292;&#36890;&#36807;&#21306;&#22495;&#23548;&#21521;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#36731;&#37327;&#32423;&#30340;&#25991;&#23383;&#22522;&#22240;&#36741;&#21161;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#36965;&#24863;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#20013;&#30340;&#35270;&#35273;-&#35821;&#20041;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#22312;&#36817;&#24180;&#26469;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#65292;&#28982;&#32780;&#22312;&#36965;&#24863;&#39046;&#22495;&#20173;&#28982;&#23384;&#22312;&#30528;&#35270;&#35273;-&#35821;&#20041;&#19981;&#24179;&#34913;&#30340;&#25361;&#25112;&#65292;&#36825;&#23548;&#33268;&#20102;&#38750;&#35821;&#20041;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#30340;&#38169;&#35823;&#21305;&#37197;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#26041;&#21521;&#30340;&#35270;&#35273;-&#35821;&#20041;&#23884;&#20837;&#27169;&#22411;&#65288;DOVE&#65289;&#65292;&#26469;&#25366;&#25496;&#35270;&#35273;&#21644;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#21306;&#22495;&#23548;&#21521;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#65288;ROAM&#65289;&#65292;&#22312;&#28508;&#22312;&#30340;&#35821;&#20041;&#31354;&#38388;&#20013;&#65292;&#26681;&#25454;&#21306;&#22495;&#35270;&#35273;&#29305;&#24449;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#26368;&#32456;&#30340;&#35270;&#35273;&#21644;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#21516;&#26102;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#25991;&#23383;&#22522;&#22240;&#36741;&#21161;&#27169;&#22359;&#65288;DTGA&#65289;&#65292;&#29992;&#36739;&#23569;&#30340;&#27880;&#24847;&#21147;&#25805;&#20316;&#26469;&#25193;&#23637;&#21487;&#22788;&#29702;&#30340;&#25991;&#26412;&#34920;&#31034;&#33539;&#22260;&#65292;&#22686;&#24378;&#20840;&#23616;&#35789;&#32423;&#35821;&#20041;&#36830;&#25509;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#20840;&#23616;&#35270;&#35273;-&#35821;&#20041;&#32422;&#26463;&#26469;&#20943;&#23569;&#21333;&#19968;&#35270;&#35273;&#20381;&#36182;&#65292;&#24182;&#20026;&#26368;&#32456;&#30340;&#35270;&#35273;&#21644;&#25991;&#26412;&#34920;&#31034;&#25552;&#20379;&#22806;&#37096;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image-text retrieval has developed rapidly in recent years. However, it is still a challenge in remote sensing due to visual-semantic imbalance, which leads to incorrect matching of non-semantic visual and textual features. To solve this problem, we propose a novel Direction-Oriented Visual-semantic Embedding Model (DOVE) to mine the relationship between vision and language. Concretely, a Regional-Oriented Attention Module (ROAM) adaptively adjusts the distance between the final visual and textual embeddings in the latent semantic space, oriented by regional visual features. Meanwhile, a lightweight Digging Text Genome Assistant (DTGA) is designed to expand the range of tractable textual representation and enhance global word-level semantic connections using less attention operations. Ultimately, we exploit a global visual-semantic constraint to reduce single visual dependency and serve as an external constraint for the final visual and textual representations. The effectiveness and su
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36861;&#36394;&#21644;&#24635;&#32467;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#25972;&#12289;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#22810;&#27169;&#24577;&#24605;&#32500;&#38142;&#21644;LLM&#36741;&#21161;&#35270;&#35273;&#25512;&#29702;&#31561;&#24212;&#29992;&#65292;&#25351;&#20986;&#20102;&#29616;&#26377;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.13549</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Multimodal Large Language Models. (arXiv:2306.13549v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36861;&#36394;&#21644;&#24635;&#32467;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#25972;&#12289;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#22810;&#27169;&#24577;&#24605;&#32500;&#38142;&#21644;LLM&#36741;&#21161;&#35270;&#35273;&#25512;&#29702;&#31561;&#24212;&#29992;&#65292;&#25351;&#20986;&#20102;&#29616;&#26377;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#30740;&#31350;&#28909;&#28857;&#65292;&#20351;&#29992;&#24378;&#22823;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22823;&#33041;&#25191;&#34892;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;MLLM &#30340;&#24778;&#20154;&#33021;&#21147;&#65292;&#22914;&#22522;&#20110;&#22270;&#20687;&#32534;&#20889;&#25925;&#20107;&#21644;&#26080;OCR&#25968;&#23398;&#25512;&#29702;&#31561;&#65292;&#22312;&#20256;&#32479;&#26041;&#27861;&#20013;&#24456;&#23569;&#35265;&#65292;&#34920;&#26126;&#20102;&#36890;&#21521;&#20154;&#24037;&#26234;&#33021;&#30340;&#28508;&#22312;&#36335;&#24452;&#12290;&#26412;&#25991;&#26088;&#22312;&#36861;&#36394;&#21644;&#24635;&#32467; MLLM &#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; MLLM &#30340;&#26500;&#25104;&#65292;&#27010;&#36848;&#20102;&#30456;&#20851;&#27010;&#24565;&#12290;&#28982;&#21518;&#65292;&#35752;&#35770;&#20102;&#20851;&#38190;&#25216;&#26415;&#21644;&#24212;&#29992;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#25972;&#65288;M-IT&#65289;&#12289;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;M-ICL&#65289;&#12289;&#22810;&#27169;&#24577;&#24605;&#32500;&#38142;&#65288;M-CoT&#65289;&#21644;LLM&#36741;&#21161;&#35270;&#35273;&#25512;&#29702;&#65288;LAVR&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#29616;&#26377;&#30340;&#25361;&#25112;&#65292;&#24182;&#25351;&#20986;&#20102;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#37492;&#20110; MLLM &#26102;&#20195;&#25165;&#21018;&#21018;&#24320;&#22987;&#65292;&#25105;&#20204;&#20250;&#19981;&#26029;&#26356;&#26032;&#36825;&#20010;&#32508;&#36848;&#65292;&#24182;&#24076;&#26395;&#33021;&#28608;&#21457;&#26356;&#22810;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Large Language Model (MLLM) recently has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional methods, suggesting a potential path to artificial general intelligence. In this paper, we aim to trace and summarize the recent progress of MLLM. First of all, we present the formulation of MLLM and delineate its related concepts. Then, we discuss the key techniques and applications, including Multimodal Instruction Tuning (M-IT), Multimodal In-Context Learning (M-ICL), Multimodal Chain of Thought (M-CoT), and LLM-Aided Visual Reasoning (LAVR). Finally, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated
&lt;/p&gt;</description></item></channel></rss>