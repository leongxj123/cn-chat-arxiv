<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>RadCLIP&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#36328;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#21033;&#29992;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#20197;&#25913;&#36827;&#25918;&#23556;&#23398;&#22270;&#20687;&#20998;&#26512;&#65292;&#21253;&#21547;&#38024;&#23545;&#20307;&#31215;&#22270;&#20687;&#20998;&#26512;&#23450;&#21046;&#30340;&#26032;&#39062;3D&#20999;&#29255;&#27744;&#21270;&#26426;&#21046;&#65292;&#24182;&#20351;&#29992;&#20016;&#23500;&#22810;&#26679;&#30340;&#25918;&#23556;&#23398;&#22270;&#20687;-&#25991;&#26412;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2403.09948</link><description>&lt;p&gt;
RadCLIP: &#36890;&#36807;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#22686;&#24378;&#25918;&#23556;&#23398;&#22270;&#20687;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
RadCLIP: Enhancing Radiologic Image Analysis through Contrastive Language-Image Pre-training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09948
&lt;/p&gt;
&lt;p&gt;
RadCLIP&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#36328;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#21033;&#29992;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#20197;&#25913;&#36827;&#25918;&#23556;&#23398;&#22270;&#20687;&#20998;&#26512;&#65292;&#21253;&#21547;&#38024;&#23545;&#20307;&#31215;&#22270;&#20687;&#20998;&#26512;&#23450;&#21046;&#30340;&#26032;&#39062;3D&#20999;&#29255;&#27744;&#21270;&#26426;&#21046;&#65292;&#24182;&#20351;&#29992;&#20016;&#23500;&#22810;&#26679;&#30340;&#25918;&#23556;&#23398;&#22270;&#20687;-&#25991;&#26412;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09948v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495;  &#25688;&#35201;: &#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#19982;&#25918;&#23556;&#23398;&#30340;&#25972;&#21512;&#26631;&#24535;&#30528;&#21307;&#23398;&#35786;&#26029;&#39046;&#22495;&#30340;&#21464;&#38761;&#26102;&#20195;&#12290;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#24050;&#34987;&#37319;&#29992;&#26469;&#22686;&#24378;&#25918;&#23556;&#23398;&#22270;&#20687;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#25918;&#23556;&#23398;&#22270;&#20687;&#30340;&#29420;&#29305;&#22797;&#26434;&#24615;&#65292;&#21253;&#25324;&#23545;2D&#21644;3D&#25918;&#23556;&#23398;&#25968;&#25454;&#30340;&#35299;&#35835;&#65292;&#24102;&#26469;&#20102;&#29616;&#26377;&#27169;&#22411;&#26080;&#27861;&#20805;&#20998;&#24212;&#23545;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#26159;&#22312;&#36890;&#29992;&#38750;&#21307;&#23398;&#22270;&#20687;&#19978;&#35757;&#32451;&#30340;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#24182;&#20805;&#20998;&#21033;&#29992;&#21307;&#23398;&#25104;&#20687;&#25152;&#38656;&#30340;&#35786;&#26029;&#31934;&#24230;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;RadCLIP&#65306;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#36328;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#21033;&#29992;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#26469;&#25913;&#36827;&#25918;&#23556;&#23398;&#22270;&#20687;&#20998;&#26512;&#12290;RadCLIP&#21253;&#21547;&#19968;&#31181;&#26032;&#39062;&#30340;3D&#20999;&#29255;&#27744;&#21270;&#26426;&#21046;&#65292;&#19987;&#20026;&#20307;&#31215;&#22270;&#20687;&#20998;&#26512;&#23450;&#21046;&#65292;&#20351;&#29992;&#20102;&#20016;&#23500;&#22810;&#26679;&#30340;&#25918;&#23556;&#23398;&#22270;&#20687;-&#25991;&#26412;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;RadCLIP&#33021;&#26377;&#25928;&#22320;&#23545;&#40784;&#25918;&#23556;&#23398;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09948v1 Announce Type: cross  Abstract: The integration of artificial intelligence (AI) with radiology has marked a transformative era in medical diagnostics. Vision foundation models have been adopted to enhance radiologic imaging analysis. However, the distinct complexities of radiological imaging, including the interpretation of 2D and 3D radiological data, pose unique challenges that existing models, trained on general non-medical images, fail to address adequately. To bridge this gap and capitalize on the diagnostic precision required in medical imaging, we introduce RadCLIP: a pioneering cross-modal foundational model that harnesses Contrastive Language-Image Pre-training (CLIP) to refine radiologic image analysis. RadCLIP incorporates a novel 3D slice pooling mechanism tailored for volumetric image analysis and is trained using a comprehensive and diverse dataset of radiologic image-text pairs. Our evaluations demonstrate that RadCLIP effectively aligns radiological i
&lt;/p&gt;</description></item><item><title>Res-VMamba&#21033;&#29992;&#20855;&#26377;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#28145;&#24230;&#27531;&#24046;&#23398;&#20064;&#65292;&#25552;&#20379;&#20102;&#27604;Transformer&#32467;&#26500;&#26356;&#20986;&#33394;&#30340;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#26159;&#39135;&#21697;&#32454;&#31890;&#24230;&#20998;&#31867;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.15761</link><description>&lt;p&gt;
&#20351;&#29992;&#20855;&#26377;&#28145;&#24230;&#27531;&#24046;&#23398;&#20064;&#30340;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;&#32454;&#31890;&#24230;&#39135;&#21697;&#31867;&#21035;&#35270;&#35273;&#20998;&#31867;&#30340;Res-VMamba
&lt;/p&gt;
&lt;p&gt;
Res-VMamba: Fine-Grained Food Category Visual Classification Using Selective State Space Models with Deep Residual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15761
&lt;/p&gt;
&lt;p&gt;
Res-VMamba&#21033;&#29992;&#20855;&#26377;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#28145;&#24230;&#27531;&#24046;&#23398;&#20064;&#65292;&#25552;&#20379;&#20102;&#27604;Transformer&#32467;&#26500;&#26356;&#20986;&#33394;&#30340;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#26159;&#39135;&#21697;&#32454;&#31890;&#24230;&#20998;&#31867;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39135;&#21697;&#20998;&#31867;&#26159;&#21457;&#23637;&#39135;&#21697;&#35270;&#35273;&#20219;&#21153;&#30340;&#22522;&#30784;&#65292;&#24182;&#22312;&#35745;&#31639;&#33829;&#20859;&#23398;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#30001;&#20110;&#39135;&#29289;&#30340;&#22797;&#26434;&#24615;&#38656;&#35201;&#32454;&#31890;&#24230;&#20998;&#31867;&#65292;&#26368;&#36817;&#30340;&#23398;&#26415;&#30740;&#31350;&#20027;&#35201;&#20462;&#25913;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#21644;/&#25110;&#35270;&#35273;&#21464;&#21387;&#22120;(ViTs)&#26469;&#25191;&#34892;&#39135;&#21697;&#31867;&#21035;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#23398;&#20064;&#32454;&#31890;&#24230;&#29305;&#24449;&#65292;CNN&#39592;&#24178;&#38656;&#35201;&#39069;&#22806;&#30340;&#32467;&#26500;&#35774;&#35745;&#65292;&#32780;&#21253;&#21547;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#30340;ViT&#20855;&#26377;&#26356;&#39640;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#26368;&#36817;&#25512;&#20986;&#30340;&#26032;&#30340;&#24207;&#21015;&#29366;&#24577;&#31354;&#38388;(S4)&#27169;&#22411;&#65292;&#36890;&#36807;&#36873;&#25321;&#26426;&#21046;&#21644;&#19982;&#25195;&#25551;(S6)&#30340;&#35745;&#31639;&#65292;&#20439;&#31216;&#20026;Mamba&#65292;&#30456;&#36739;&#20110;&#21464;&#21387;&#22120;&#26550;&#26500;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#23558;Mamba&#26426;&#21046;&#25972;&#21512;&#21040;&#22270;&#20687;&#20219;&#21153;(&#22914;&#20998;&#31867;)&#20013;&#30340;VMamba&#27169;&#22411;&#30446;&#21069;&#24314;&#31435;&#20102;&#26368;&#20808;&#36827;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15761v1 Announce Type: cross  Abstract: Food classification is the foundation for developing food vision tasks and plays a key role in the burgeoning field of computational nutrition. Due to the complexity of food requiring fine-grained classification, recent academic research mainly modifies Convolutional Neural Networks (CNNs) and/or Vision Transformers (ViTs) to perform food category classification. However, to learn fine-grained features, the CNN backbone needs additional structural design, whereas ViT, containing the self-attention module, has increased computational complexity. In recent months, a new Sequence State Space (S4) model, through a Selection mechanism and computation with a Scan (S6), colloquially termed Mamba, has demonstrated superior performance and computation efficiency compared to the Transformer architecture. The VMamba model, which incorporates the Mamba mechanism into image tasks (such as classification), currently establishes the state-of-the-art 
&lt;/p&gt;</description></item><item><title>This paper introduces a novel approach called Membership Inference Test (MINT) to empirically assess if specific data was used during the training of AI models. Two MINT architectures based on MLP and CNN are proposed and evaluated on a challenging face recognition task, achieving promising results with up to 90% accuracy.</title><link>https://arxiv.org/abs/2402.09225</link><description>&lt;p&gt;
&#25105;&#30340;&#25968;&#25454;&#22312;&#20320;&#30340;AI&#27169;&#22411;&#20013;&#21527;&#65311;&#36890;&#36807;&#24212;&#29992;&#20110;&#20154;&#33080;&#22270;&#20687;&#30340;&#25104;&#21592;&#25512;&#26029;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Is my Data in your AI Model? Membership Inference Test with Application to Face Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09225
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel approach called Membership Inference Test (MINT) to empirically assess if specific data was used during the training of AI models. Two MINT architectures based on MLP and CNN are proposed and evaluated on a challenging face recognition task, achieving promising results with up to 90% accuracy.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#25104;&#21592;&#25512;&#26029;&#27979;&#35797;&#65288;MINT&#65289;&#65292;&#19968;&#31181;&#29992;&#20110;&#32463;&#39564;&#24615;&#35780;&#20272;&#29305;&#23450;&#25968;&#25454;&#26159;&#21542;&#34987;&#29992;&#20110;&#35757;&#32451;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;MINT&#26550;&#26500;&#65292;&#26088;&#22312;&#23398;&#20064;&#22312;&#32463;&#36807;&#23457;&#35745;&#30340;&#27169;&#22411;&#26292;&#38706;&#20110;&#20854;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#25968;&#25454;&#26102;&#20986;&#29616;&#30340;&#19981;&#21516;&#28608;&#27963;&#27169;&#24335;&#12290;&#31532;&#19968;&#20010;&#26550;&#26500;&#22522;&#20110;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#32593;&#32476;&#65292;&#31532;&#20108;&#20010;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;MINT&#26550;&#26500;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20154;&#33080;&#35782;&#21035;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#32771;&#34385;&#20102;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#20154;&#33080;&#35782;&#21035;&#27169;&#22411;&#12290;&#20351;&#29992;&#20845;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#24211;&#36827;&#34892;&#23454;&#39564;&#65292;&#24635;&#20849;&#21253;&#21547;&#36229;&#36807;2200&#19975;&#24352;&#20154;&#33080;&#22270;&#20687;&#12290;&#26681;&#25454;&#21487;&#29992;&#30340;AI&#27169;&#22411;&#27979;&#35797;&#30340;&#19978;&#19979;&#25991;&#65292;&#32771;&#34385;&#20102;&#19981;&#21516;&#30340;&#23454;&#39564;&#22330;&#26223;&#12290;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#36798;&#21040;&#20102;90%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09225v1 Announce Type: cross Abstract: This paper introduces the Membership Inference Test (MINT), a novel approach that aims to empirically assess if specific data was used during the training of Artificial Intelligence (AI) models. Specifically, we propose two novel MINT architectures designed to learn the distinct activation patterns that emerge when an audited model is exposed to data used during its training process. The first architecture is based on a Multilayer Perceptron (MLP) network and the second one is based on Convolutional Neural Networks (CNNs). The proposed MINT architectures are evaluated on a challenging face recognition task, considering three state-of-the-art face recognition models. Experiments are carried out using six publicly available databases, comprising over 22 million face images in total. Also, different experimental scenarios are considered depending on the context available of the AI model to test. Promising results, up to 90% accuracy, are a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#22270;&#20687;&#24402;&#23646;&#38382;&#39064;&#37325;&#26032;&#24314;&#27169;&#20026;&#27425;&#27169;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#26356;&#23569;&#30340;&#21306;&#22495;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#24402;&#23646;&#35299;&#20915;&#26041;&#26696;&#38754;&#20020;&#30340;&#19981;&#20934;&#30830;&#21306;&#22495;&#21644;&#39044;&#27979;&#38169;&#35823;&#26679;&#26412;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.09164</link><description>&lt;p&gt;
&#31616;&#32422;&#21363;&#26159;&#32654;&#65306;&#36890;&#36807;&#27425;&#27169;&#23376;&#38598;&#36873;&#25321;&#20943;&#23569;&#21487;&#35299;&#37322;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
Less is More: Fewer Interpretable Region via Submodular Subset Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#22270;&#20687;&#24402;&#23646;&#38382;&#39064;&#37325;&#26032;&#24314;&#27169;&#20026;&#27425;&#27169;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#26356;&#23569;&#30340;&#21306;&#22495;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#24402;&#23646;&#35299;&#20915;&#26041;&#26696;&#38754;&#20020;&#30340;&#19981;&#20934;&#30830;&#21306;&#22495;&#21644;&#39044;&#27979;&#38169;&#35823;&#26679;&#26412;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#24402;&#23646;&#31639;&#27861;&#26088;&#22312;&#30830;&#23450;&#19982;&#27169;&#22411;&#20915;&#31574;&#39640;&#24230;&#30456;&#20851;&#30340;&#37325;&#35201;&#21306;&#22495;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#24402;&#23646;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#26377;&#25928;&#22320;&#32473;&#30446;&#26631;&#20803;&#32032;&#20998;&#37197;&#37325;&#35201;&#24615;&#65292;&#20294;&#20173;&#38754;&#20020;&#20197;&#19979;&#25361;&#25112;&#65306;1&#65289;&#29616;&#26377;&#30340;&#24402;&#23646;&#26041;&#27861;&#29983;&#25104;&#19981;&#20934;&#30830;&#30340;&#23567;&#21306;&#22495;&#65292;&#20174;&#32780;&#35823;&#23548;&#27491;&#30830;&#24402;&#23646;&#30340;&#26041;&#21521;&#65307;2&#65289;&#27169;&#22411;&#26080;&#27861;&#20026;&#39044;&#27979;&#38169;&#35823;&#30340;&#26679;&#26412;&#20135;&#29983;&#33391;&#22909;&#30340;&#24402;&#23646;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#26412;&#25991;&#23558;&#19978;&#36848;&#22270;&#20687;&#24402;&#23646;&#38382;&#39064;&#37325;&#26032;&#24314;&#27169;&#20026;&#27425;&#27169;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#65292;&#26088;&#22312;&#20351;&#29992;&#26356;&#23569;&#30340;&#21306;&#22495;&#22686;&#24378;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#23545;&#23616;&#37096;&#21306;&#22495;&#30340;&#20851;&#27880;&#19981;&#36275;&#65292;&#25105;&#20204;&#26500;&#36896;&#20102;&#19968;&#20010;&#26032;&#30340;&#27425;&#27169;&#20989;&#25968;&#26469;&#21457;&#29616;&#26356;&#20934;&#30830;&#30340;&#31934;&#32454;&#35299;&#37322;&#21306;&#22495;&#12290;&#20026;&#20102;&#22686;&#24378;&#25152;&#26377;&#26679;&#26412;&#30340;&#24402;&#23646;&#25928;&#26524;&#65292;&#25105;&#20204;&#36824;&#23545;&#23376;&#21306;&#22495;&#36873;&#25321;&#26045;&#21152;&#20102;&#22235;&#20010;&#19981;&#21516;&#30340;&#32422;&#26463;&#65292;&#21363;&#32622;&#20449;&#24230;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09164v1 Announce Type: cross Abstract: Image attribution algorithms aim to identify important regions that are highly relevant to model decisions. Although existing attribution solutions can effectively assign importance to target elements, they still face the following challenges: 1) existing attribution methods generate inaccurate small regions thus misleading the direction of correct attribution, and 2) the model cannot produce good attribution results for samples with wrong predictions. To address the above challenges, this paper re-models the above image attribution problem as a submodular subset selection problem, aiming to enhance model interpretability using fewer regions. To address the lack of attention to local regions, we construct a novel submodular function to discover more accurate fine-grained interpretation regions. To enhance the attribution effect for all samples, we also impose four different constraints on the selection of sub-regions, i.e., confidence, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FRS-Nets&#30340;&#26032;&#22411;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#21033;&#29992;&#20613;&#37324;&#21494;&#21442;&#25968;&#21270;&#23454;&#29616;&#20102;&#23545;&#26059;&#36716;&#21644;&#23610;&#24230;&#30340;&#31561;&#21464;&#24615;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#35270;&#32593;&#33180;&#34880;&#31649;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#22312;U-Net&#21644;Iter-Net&#20013;&#26367;&#25442;&#20256;&#32479;&#21367;&#31215;&#28388;&#27874;&#22120;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20998;&#21106;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.15638</link><description>&lt;p&gt;
FRS-Nets: Fourier&#21442;&#25968;&#21270;&#30340;&#26059;&#36716;&#21644;&#23610;&#24230;&#31561;&#21464;&#32593;&#32476;&#29992;&#20110;&#35270;&#32593;&#33180;&#34880;&#31649;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
FRS-Nets: Fourier Parameterized Rotation and Scale Equivariant Networks for Retinal Vessel Segmentation. (arXiv:2309.15638v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FRS-Nets&#30340;&#26032;&#22411;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#21033;&#29992;&#20613;&#37324;&#21494;&#21442;&#25968;&#21270;&#23454;&#29616;&#20102;&#23545;&#26059;&#36716;&#21644;&#23610;&#24230;&#30340;&#31561;&#21464;&#24615;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#35270;&#32593;&#33180;&#34880;&#31649;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#22312;U-Net&#21644;Iter-Net&#20013;&#26367;&#25442;&#20256;&#32479;&#21367;&#31215;&#28388;&#27874;&#22120;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20998;&#21106;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#24179;&#31227;&#31561;&#21464;&#24615;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#22312;&#35270;&#32593;&#33180;&#34880;&#31649;&#20998;&#21106;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;CNNs&#27809;&#26377;&#23545;&#34880;&#31649;&#24418;&#24577;&#30340;&#20854;&#20182;&#23545;&#31216;&#24615;&#36827;&#34892;&#24314;&#27169;&#65292;&#20363;&#22914;&#26059;&#36716;&#21644;&#23610;&#24230;&#23545;&#31216;&#24615;&#12290;&#20026;&#20102;&#22312;CNNs&#20013;&#23884;&#20837;&#26356;&#22810;&#31561;&#21464;&#24615;&#24182;&#28385;&#36275;&#35270;&#32593;&#33180;&#34880;&#31649;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#35201;&#27714;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21367;&#31215;&#31639;&#23376;&#65288;FRS-Conv&#65289;&#65292;&#23427;&#26159;&#20613;&#37324;&#21494;&#21442;&#25968;&#21270;&#30340;&#65292;&#24182;&#19988;&#23545;&#26059;&#36716;&#21644;&#32553;&#25918;&#31561;&#21464;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#37319;&#29992;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#21270;&#26041;&#26696;&#65292;&#20351;&#21367;&#31215;&#28388;&#27874;&#22120;&#33021;&#22815;&#20197;&#39640;&#31934;&#24230;&#20219;&#24847;&#36827;&#34892;&#21464;&#25442;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#26059;&#36716;&#21644;&#23610;&#24230;&#31561;&#21464;&#21367;&#31215;&#26144;&#23556;&#30340;&#20844;&#24335;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#25552;&#20986;&#30340;&#20844;&#24335;&#26500;&#24314;&#20102;FRS-Conv&#65292;&#24182;&#23558;U-Net&#21644;Iter-Net&#20013;&#30340;&#20256;&#32479;&#21367;&#31215;&#28388;&#27874;&#22120;&#26367;&#25442;&#20026;FRS-Conv&#65288;FRS-Nets&#65289;&#12290;&#25105;&#20204;&#24544;&#23454;&#22320;&#22797;&#29616;&#20102;&#25152;&#26377;&#23545;&#27604;&#26041;&#27861;&#65292;&#24182;&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
With translation equivariance, convolution neural networks (CNNs) have achieved great success in retinal vessel segmentation. However, some other symmetries of the vascular morphology are not characterized by CNNs, such as rotation and scale symmetries. To embed more equivariance into CNNs and achieve the accuracy requirement for retinal vessel segmentation, we construct a novel convolution operator (FRS-Conv), which is Fourier parameterized and equivariant to rotation and scaling. Specifically, we first adopt a new parameterization scheme, which enables convolutional filters to arbitrarily perform transformations with high accuracy. Secondly, we derive the formulations for the rotation and scale equivariant convolution mapping. Finally, we construct FRS-Conv following the proposed formulations and replace the traditional convolution filters in U-Net and Iter-Net with FRS-Conv (FRS-Nets). We faithfully reproduce all compared methods and conduct comprehensive experiments on three public
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EgoPoser&#65292;&#19968;&#31181;&#33021;&#22815;&#22312;&#22823;&#22330;&#26223;&#20013;&#40065;&#26834;&#22320;&#23454;&#26102;&#20272;&#35745;&#33258;&#25105;&#36523;&#20307;&#23039;&#21183;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#37325;&#26032;&#24605;&#32771;&#36755;&#20837;&#34920;&#31034;&#12289;&#24341;&#20837;&#26032;&#30340;&#36816;&#21160;&#20998;&#35299;&#26041;&#27861;&#20197;&#21450;&#24314;&#27169;&#36523;&#20307;&#23039;&#21183;&#65292;EgoPoser&#22312;&#23450;&#24615;&#21644;&#23450;&#37327;&#19978;&#22343;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.06493</link><description>&lt;p&gt;
EgoPoser&#65306;&#22823;&#22330;&#26223;&#19979;&#40065;&#26834;&#30340;&#23454;&#26102;&#33258;&#25105;&#36523;&#20307;&#23039;&#21183;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
EgoPoser: Robust Real-Time Ego-Body Pose Estimation in Large Scenes. (arXiv:2308.06493v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EgoPoser&#65292;&#19968;&#31181;&#33021;&#22815;&#22312;&#22823;&#22330;&#26223;&#20013;&#40065;&#26834;&#22320;&#23454;&#26102;&#20272;&#35745;&#33258;&#25105;&#36523;&#20307;&#23039;&#21183;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#37325;&#26032;&#24605;&#32771;&#36755;&#20837;&#34920;&#31034;&#12289;&#24341;&#20837;&#26032;&#30340;&#36816;&#21160;&#20998;&#35299;&#26041;&#27861;&#20197;&#21450;&#24314;&#27169;&#36523;&#20307;&#23039;&#21183;&#65292;EgoPoser&#22312;&#23450;&#24615;&#21644;&#23450;&#37327;&#19978;&#22343;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22836;&#37096;&#21644;&#25163;&#37096;&#23039;&#21183;&#20165;&#36890;&#36807;&#23436;&#25972;&#36523;&#20307;&#33258;&#25105;&#23039;&#21183;&#20272;&#35745;&#24050;&#25104;&#20026;&#30740;&#31350;&#30340;&#19968;&#20010;&#28909;&#28857;&#39046;&#22495;&#65292;&#20197;&#20026;&#22836;&#25140;&#24335;&#24179;&#21488;&#19978;&#30340;&#34394;&#25311;&#35282;&#33394;&#34920;&#36798;&#25552;&#20379;&#21160;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36807;&#20110;&#20381;&#36182;&#25968;&#25454;&#38598;&#35760;&#24405;&#26102;&#30340;&#36816;&#21160;&#25429;&#25417;&#31354;&#38388;&#30340;&#38480;&#21046;&#65292;&#21516;&#26102;&#20551;&#35774;&#36830;&#32493;&#25429;&#25417;&#20851;&#33410;&#36816;&#21160;&#21644;&#22343;&#21248;&#36523;&#20307;&#23610;&#23544;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EgoPoser&#65292;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#20811;&#26381;&#20102;&#36825;&#20123;&#38480;&#21046;&#65306;1&#65289;&#37325;&#26032;&#24605;&#32771;&#22522;&#20110;&#22836;&#25140;&#24335;&#24179;&#21488;&#30340;&#33258;&#25105;&#23039;&#21183;&#20272;&#35745;&#30340;&#36755;&#20837;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#36816;&#21160;&#20998;&#35299;&#26041;&#27861;&#26469;&#39044;&#27979;&#19982;&#20840;&#23616;&#20301;&#32622;&#26080;&#20851;&#30340;&#23436;&#25972;&#36523;&#20307;&#23039;&#21183;&#65292;2&#65289;&#20174;&#22836;&#25140;&#24335;&#35774;&#22791;&#35270;&#37326;&#20869;&#30340;&#38388;&#27463;&#24615;&#25163;&#37096;&#23039;&#21183;&#36319;&#36394;&#20013;&#40065;&#26834;&#22320;&#24314;&#27169;&#36523;&#20307;&#23039;&#21183;&#65292;3&#65289;&#38024;&#23545;&#19981;&#21516;&#29992;&#25143;&#30340;&#21508;&#31181;&#36523;&#20307;&#23610;&#23544;&#36827;&#34892;&#36890;&#29992;&#21270;&#25512;&#24191;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;EgoPoser&#22312;&#23450;&#24615;&#21644;&#23450;&#37327;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#24182;&#20445;&#25345;&#36739;&#39640;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Full-body ego-pose estimation from head and hand poses alone has become an active area of research to power articulate avatar representation on headset-based platforms. However, existing methods over-rely on the confines of the motion-capture spaces in which datasets were recorded, while simultaneously assuming continuous capture of joint motions and uniform body dimensions. In this paper, we propose EgoPoser, which overcomes these limitations by 1) rethinking the input representation for headset-based ego-pose estimation and introducing a novel motion decomposition method that predicts full-body pose independent of global positions, 2) robustly modeling body pose from intermittent hand position and orientation tracking only when inside a headset's field of view, and 3) generalizing across various body sizes for different users. Our experiments show that EgoPoser outperforms state-of-the-art methods both qualitatively and quantitatively, while maintaining a high inference speed of over
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#35270;&#39057;&#23545;&#40784;&#26041;&#27861;&#65292;&#21033;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#23558;&#24103;&#36716;&#21270;&#20026;&#26102;&#38388;&#24207;&#21015;&#24182;&#20351;&#29992;&#23545;&#35282;&#21270;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#31639;&#27861;&#36827;&#34892;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2304.06841</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#29992;&#20110;&#35270;&#39057;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Video alignment using unsupervised learning of local and global features. (arXiv:2304.06841v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#35270;&#39057;&#23545;&#40784;&#26041;&#27861;&#65292;&#21033;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#23558;&#24103;&#36716;&#21270;&#20026;&#26102;&#38388;&#24207;&#21015;&#24182;&#20351;&#29992;&#23545;&#35282;&#21270;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#31639;&#27861;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#35299;&#20915;&#35270;&#39057;&#23545;&#40784;&#30340;&#38382;&#39064;&#65292;&#21363;&#21305;&#37197;&#21253;&#21547;&#30456;&#20284;&#27963;&#21160;&#30340;&#19968;&#23545;&#35270;&#39057;&#30340;&#24103;&#12290;&#35270;&#39057;&#23545;&#40784;&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#65292;&#23613;&#31649;&#20004;&#20010;&#35270;&#39057;&#20043;&#38388;&#30340;&#25191;&#34892;&#36807;&#31243;&#21644;&#22806;&#35266;&#26377;&#25152;&#19981;&#21516;&#65292;&#20294;&#20173;&#38656;&#35201;&#24314;&#31435;&#31934;&#30830;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24103;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#36827;&#34892;&#23545;&#40784;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21033;&#29992;&#20154;&#29289;&#26816;&#27979;&#12289;&#23039;&#24577;&#20272;&#35745;&#21644;VGG&#32593;&#32476;&#19977;&#31181;&#26426;&#22120;&#35270;&#35273;&#24037;&#20855;&#20026;&#27599;&#20010;&#35270;&#39057;&#24103;&#24341;&#20837;&#26377;&#25928;&#30340;&#29305;&#24449;&#12290;&#28982;&#21518;&#23545;&#36825;&#20123;&#29305;&#24449;&#36827;&#34892;&#22788;&#29702;&#21644;&#32452;&#21512;&#20197;&#26500;&#24314;&#20195;&#34920;&#35270;&#39057;&#30340;&#22810;&#32500;&#26102;&#38388;&#24207;&#21015;&#12290;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;&#23545;&#35282;&#21270;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#30340;&#26032;&#29256;&#26412;&#65288;Diagonalized Dynamic Time Warping, DDTW&#65289;&#23545;&#29983;&#25104;&#30340;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20027;&#35201;&#20248;&#28857;&#22312;&#20110;&#19981;&#38656;&#35201;&#20219;&#20309;&#35757;&#32451;&#65292;&#22240;&#27492;&#36866;&#29992;&#20110;&#20219;&#20309;&#26032;&#31867;&#22411;&#30340;&#27963;&#21160;&#32780;&#26080;&#38656;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we tackle the problem of video alignment, the process of matching the frames of a pair of videos containing similar actions. The main challenge in video alignment is that accurate correspondence should be established despite the differences in the execution processes and appearances between the two videos. We introduce an unsupervised method for alignment that uses global and local features of the frames. In particular, we introduce effective features for each video frame by means of three machine vision tools: person detection, pose estimation, and VGG network. Then the features are processed and combined to construct a multidimensional time series that represent the video. The resulting time series are used to align videos of the same actions using a novel version of dynamic time warping named Diagonalized Dynamic Time Warping(DDTW). The main advantage of our approach is that no training is required, which makes it applicable for any new type of action without any need
&lt;/p&gt;</description></item></channel></rss>