<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#22522;&#20110;&#20869;&#23481;&#30340;&#21307;&#23398;&#22270;&#20687;&#26816;&#32034;&#20013;&#65292;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#26080;&#38656;&#24494;&#35843;&#21363;&#21487;&#21462;&#24471;&#19982;&#19987;&#38376;&#27169;&#22411;&#31454;&#20105;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#26816;&#32034;&#30149;&#29702;&#29305;&#24449;&#26041;&#38754;&#20855;&#26377;&#36739;&#22823;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2403.06567</link><description>&lt;p&gt;
&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#25918;&#23556;&#23398;&#20013;&#22522;&#20110;&#20869;&#23481;&#30340;&#21307;&#23398;&#22270;&#20687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Leveraging Foundation Models for Content-Based Medical Image Retrieval in Radiology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06567
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20869;&#23481;&#30340;&#21307;&#23398;&#22270;&#20687;&#26816;&#32034;&#20013;&#65292;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#26080;&#38656;&#24494;&#35843;&#21363;&#21487;&#21462;&#24471;&#19982;&#19987;&#38376;&#27169;&#22411;&#31454;&#20105;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#26816;&#32034;&#30149;&#29702;&#29305;&#24449;&#26041;&#38754;&#20855;&#26377;&#36739;&#22823;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Content-based image retrieval&#65288;CBIR&#65289;&#26377;&#26395;&#26174;&#33879;&#25913;&#21892;&#25918;&#23556;&#23398;&#20013;&#30340;&#35786;&#26029;&#36741;&#21161;&#21644;&#21307;&#23398;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#24378;&#22823;&#19988;&#22810;&#21151;&#33021;&#30340;&#29616;&#25104;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#29992;&#20110;&#22522;&#20110;&#20869;&#23481;&#30340;&#21307;&#23398;&#22270;&#20687;&#26816;&#32034;&#12290;&#36890;&#36807;&#22312;&#28085;&#30422;&#22235;&#31181;&#27169;&#24577;&#21644;161&#31181;&#30149;&#29702;&#23398;&#30340;160&#19975;&#24352;2D&#25918;&#23556;&#22270;&#20687;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#19978;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#21457;&#29616;&#24369;&#30417;&#30563;&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;&#65292;P@1&#21487;&#36798;0.594&#12290;&#36825;&#31181;&#24615;&#33021;&#19981;&#20165;&#19982;&#19987;&#38376;&#21270;&#27169;&#22411;&#31454;&#20105;&#65292;&#32780;&#19988;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#26816;&#32034;&#30149;&#29702;&#23398;&#19982;&#35299;&#21078;&#32467;&#26500;&#30340;&#25361;&#25112;&#65292;&#34920;&#26126;&#20934;&#30830;&#26816;&#32034;&#30149;&#29702;&#29305;&#24449;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06567v1 Announce Type: cross  Abstract: Content-based image retrieval (CBIR) has the potential to significantly improve diagnostic aid and medical research in radiology. Current CBIR systems face limitations due to their specialization to certain pathologies, limiting their utility. In response, we propose using vision foundation models as powerful and versatile off-the-shelf feature extractors for content-based medical image retrieval. By benchmarking these models on a comprehensive dataset of 1.6 million 2D radiological images spanning four modalities and 161 pathologies, we identify weakly-supervised models as superior, achieving a P@1 of up to 0.594. This performance not only competes with a specialized model but does so without the need for fine-tuning. Our analysis further explores the challenges in retrieving pathological versus anatomical structures, indicating that accurate retrieval of pathological features presents greater difficulty. Despite these challenges, our
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#26410;&#30693;&#25298;&#32477;&#12289;&#26032;&#31867;&#21035;&#21457;&#29616;&#21644;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#65292;&#26412;&#25991;&#25299;&#23637;&#20102;&#24320;&#25918;&#19990;&#30028;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#22810;&#20010;&#28508;&#22312;&#26041;&#21521;</title><link>https://arxiv.org/abs/2403.01759</link><description>&lt;p&gt;
&#24320;&#25918;&#19990;&#30028;&#26426;&#22120;&#23398;&#20064;&#65306;&#22238;&#39038;&#19982;&#26032;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Open-world Machine Learning: A Review and New Outlooks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01759
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#26410;&#30693;&#25298;&#32477;&#12289;&#26032;&#31867;&#21035;&#21457;&#29616;&#21644;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#65292;&#26412;&#25991;&#25299;&#23637;&#20102;&#24320;&#25918;&#19990;&#30028;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#22810;&#20010;&#28508;&#22312;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#22522;&#20110;&#23553;&#38381;&#19990;&#30028;&#20551;&#35774;&#65292;&#21363;&#20551;&#23450;&#29615;&#22659;&#26159;&#38745;&#24577;&#30340;&#65292;&#27169;&#22411;&#19968;&#26086;&#37096;&#32626;&#23601;&#26159;&#22266;&#23450;&#30340;&#12290;&#22312;&#35768;&#22810;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;&#36825;&#31181;&#22522;&#26412;&#19988;&#30456;&#24403;&#24188;&#31258;&#30340;&#20551;&#35774;&#21487;&#33021;&#19981;&#25104;&#31435;&#65292;&#22240;&#20026;&#24320;&#25918;&#29615;&#22659;&#22797;&#26434;&#12289;&#21160;&#24577;&#19988;&#20805;&#28385;&#26410;&#30693;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25298;&#32477;&#26410;&#30693;&#12289;&#21457;&#29616;&#26032;&#22855;&#28857;&#65292;&#28982;&#21518;&#36880;&#27493;&#23398;&#20064;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#20687;&#29983;&#29289;&#31995;&#32479;&#19968;&#26679;&#23433;&#20840;&#22320;&#24182;&#25345;&#32493;&#36827;&#21270;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#26410;&#30693;&#25298;&#32477;&#12289;&#26032;&#31867;&#21035;&#21457;&#29616;&#21644;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#22312;&#32479;&#19968;&#33539;&#24335;&#20013;&#65292;&#25552;&#20379;&#20102;&#23545;&#24320;&#25918;&#19990;&#30028;&#26426;&#22120;&#23398;&#20064;&#30340;&#25972;&#20307;&#35266;&#28857;&#12290;&#35814;&#32454;&#35752;&#35770;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#25361;&#25112;&#12289;&#21407;&#21017;&#21644;&#23616;&#38480;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20960;&#20010;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20221;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01759v1 Announce Type: new  Abstract: Machine learning has achieved remarkable success in many applications. However, existing studies are largely based on the closed-world assumption, which assumes that the environment is stationary, and the model is fixed once deployed. In many real-world applications, this fundamental and rather naive assumption may not hold because an open environment is complex, dynamic, and full of unknowns. In such cases, rejecting unknowns, discovering novelties, and then incrementally learning them, could enable models to be safe and evolve continually as biological systems do. This paper provides a holistic view of open-world machine learning by investigating unknown rejection, novel class discovery, and class-incremental learning in a unified paradigm. The challenges, principles, and limitations of current methodologies are discussed in detail. Finally, we discuss several potential directions for future research. This paper aims to provide a compr
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#32593;&#33180;&#24213;&#22270;&#20687;&#32676;&#20307;&#27169;&#22411;&#65292;&#26377;&#25928;&#35299;&#24320;&#24739;&#32773;&#23646;&#24615;&#19982;&#30456;&#26426;&#25928;&#26524;&#65292;&#23454;&#29616;&#21487;&#25511;&#19988;&#39640;&#24230;&#36924;&#30495;&#30340;&#22270;&#20687;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.19186</link><description>&lt;p&gt;
&#29992;&#29983;&#25104;&#27169;&#22411;&#35299;&#24320;&#35270;&#32593;&#33180;&#22270;&#20687;&#30340;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
Disentangling representations of retinal images with generative models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19186
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#32593;&#33180;&#24213;&#22270;&#20687;&#32676;&#20307;&#27169;&#22411;&#65292;&#26377;&#25928;&#35299;&#24320;&#24739;&#32773;&#23646;&#24615;&#19982;&#30456;&#26426;&#25928;&#26524;&#65292;&#23454;&#29616;&#21487;&#25511;&#19988;&#39640;&#24230;&#36924;&#30495;&#30340;&#22270;&#20687;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#32593;&#33180;&#24213;&#22270;&#20687;&#22312;&#26089;&#26399;&#26816;&#27979;&#30524;&#37096;&#30142;&#30149;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#29978;&#33267;&#34920;&#26126;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36825;&#20123;&#22270;&#20687;&#36824;&#21487;&#20197;&#29992;&#20110;&#26816;&#27979;&#24515;&#34880;&#31649;&#39118;&#38505;&#22240;&#32032;&#21644;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22270;&#20687;&#21463;&#25216;&#26415;&#22240;&#32032;&#30340;&#24433;&#21709;&#21487;&#33021;&#23545;&#30524;&#31185;&#39046;&#22495;&#21487;&#38752;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#26500;&#25104;&#25361;&#25112;&#12290;&#20363;&#22914;&#65292;&#22823;&#22411;&#24213;&#22270;&#38431;&#21015;&#24448;&#24448;&#21463;&#21040;&#30456;&#26426;&#31867;&#22411;&#12289;&#22270;&#20687;&#36136;&#37327;&#25110;&#29031;&#26126;&#27700;&#24179;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#23384;&#22312;&#23398;&#20064;&#24555;&#25463;&#26041;&#24335;&#32780;&#19981;&#26159;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#32972;&#21518;&#22240;&#26524;&#20851;&#31995;&#30340;&#39118;&#38505;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35270;&#32593;&#33180;&#24213;&#22270;&#20687;&#32676;&#20307;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#35299;&#24320;&#20102;&#24739;&#32773;&#23646;&#24615;&#19982;&#30456;&#26426;&#25928;&#26524;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21487;&#25511;&#19988;&#39640;&#24230;&#36924;&#30495;&#30340;&#22270;&#20687;&#29983;&#25104;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36317;&#31163;&#30456;&#20851;&#24615;&#30340;&#26032;&#39062;&#35299;&#24320;&#25439;&#22833;&#12290;&#36890;&#36807;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19186v1 Announce Type: cross  Abstract: Retinal fundus images play a crucial role in the early detection of eye diseases and, using deep learning approaches, recent studies have even demonstrated their potential for detecting cardiovascular risk factors and neurological disorders. However, the impact of technical factors on these images can pose challenges for reliable AI applications in ophthalmology. For example, large fundus cohorts are often confounded by factors like camera type, image quality or illumination level, bearing the risk of learning shortcuts rather than the causal relationships behind the image generation process. Here, we introduce a novel population model for retinal fundus images that effectively disentangles patient attributes from camera effects, thus enabling controllable and highly realistic image generation. To achieve this, we propose a novel disentanglement loss based on distance correlation. Through qualitative and quantitative analyses, we demon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#20998;&#24067;&#23545;&#40784;&#26041;&#27861;&#20197;&#35299;&#20915;&#21518;&#35757;&#32451;&#37327;&#21270;&#23545;&#20110;&#24357;&#25955;&#27169;&#22411;&#30340;&#20998;&#24067;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#20302;&#24310;&#36831;&#24212;&#29992;&#20013;&#20855;&#26377;&#36739;&#39640;&#30340;&#28508;&#21147;&#65292;&#24182;&#19988;&#33021;&#26377;&#25928;&#25552;&#21319;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04585</link><description>&lt;p&gt;
&#25193;&#23637;&#20998;&#24067;&#23545;&#40784;&#26469;&#23454;&#29616;&#24357;&#25955;&#27169;&#22411;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Enhanced Distribution Alignment for Post-Training Quantization of Diffusion Models. (arXiv:2401.04585v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#20998;&#24067;&#23545;&#40784;&#26041;&#27861;&#20197;&#35299;&#20915;&#21518;&#35757;&#32451;&#37327;&#21270;&#23545;&#20110;&#24357;&#25955;&#27169;&#22411;&#30340;&#20998;&#24067;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#20302;&#24310;&#36831;&#24212;&#29992;&#20013;&#20855;&#26377;&#36739;&#39640;&#30340;&#28508;&#21147;&#65292;&#24182;&#19988;&#33021;&#26377;&#25928;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#22122;&#22768;&#20272;&#35745;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#32321;&#37325;&#30340;&#21435;&#22122;&#36807;&#31243;&#21644;&#22797;&#26434;&#30340;&#31070;&#32463;&#32593;&#32476;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#20302;&#24310;&#36831;&#24212;&#29992;&#12290;&#37327;&#21270;&#21487;&#20197;&#26377;&#25928;&#38477;&#20302;&#27169;&#22411;&#22797;&#26434;&#24230;&#65292;&#32780;&#21518;&#35757;&#32451;&#37327;&#21270;(PTQ)&#22312;&#21152;&#36895;&#21435;&#22122;&#36807;&#31243;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#28508;&#21147;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#24494;&#35843;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#30001;&#20110;&#19981;&#21516;&#21435;&#22122;&#27493;&#39588;&#20013;&#28608;&#27963;&#30340;&#39640;&#24230;&#21160;&#24577;&#20998;&#24067;&#65292;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;PTQ&#26041;&#27861;&#22312;&#26657;&#20934;&#26679;&#26412;&#21644;&#37325;&#26500;&#36755;&#20986;&#20004;&#20010;&#23618;&#38754;&#19978;&#37117;&#23384;&#22312;&#20998;&#24067;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#65292;&#23548;&#33268;&#24615;&#33021;&#36828;&#20302;&#20110;&#20196;&#20154;&#28385;&#24847;&#30340;&#27700;&#24179;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#20301;&#24773;&#20917;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#24378;&#30340;&#20998;&#24067;&#23545;&#40784;&#29992;&#20110;&#24357;&#25955;&#27169;&#22411;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;(EDA-DM)&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#26657;&#20934;&#26679;&#26412;&#23618;&#38754;&#65292;&#25105;&#20204;&#22522;&#20110;...[&#32570;&#30465;]
&lt;/p&gt;
&lt;p&gt;
Diffusion models have achieved great success in image generation tasks through iterative noise estimation. However, the heavy denoising process and complex neural networks hinder their low-latency applications in real-world scenarios. Quantization can effectively reduce model complexity, and post-training quantization (PTQ), which does not require fine-tuning, is highly promising in accelerating the denoising process. Unfortunately, we find that due to the highly dynamic distribution of activations in different denoising steps, existing PTQ methods for diffusion models suffer from distribution mismatch issues at both calibration sample level and reconstruction output level, which makes the performance far from satisfactory, especially in low-bit cases. In this paper, we propose Enhanced Distribution Alignment for Post-Training Quantization of Diffusion Models (EDA-DM) to address the above issues. Specifically, at the calibration sample level, we select calibration samples based on the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#36827;&#34892;&#20102;&#33539;&#22260;&#35780;&#20272;&#65292;&#24182;&#35201;&#27714;&#23545;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#27979;&#35797;&#12289;&#20351;&#29992;&#20844;&#35748;&#25968;&#25454;&#38598;&#20197;&#21450;&#30830;&#20445;&#32467;&#26524;&#21487;&#22797;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.04532</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#26368;&#26032;&#36235;&#21183;&#65306;&#19968;&#20010;&#33539;&#22260;&#35780;&#20272;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Latest Trends in Artificial Intelligence Technology: A Scoping Review. (arXiv:2305.04532v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#36827;&#34892;&#20102;&#33539;&#22260;&#35780;&#20272;&#65292;&#24182;&#35201;&#27714;&#23545;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#27979;&#35797;&#12289;&#20351;&#29992;&#20844;&#35748;&#25968;&#25454;&#38598;&#20197;&#21450;&#30830;&#20445;&#32467;&#26524;&#21487;&#22797;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#12290;&#26234;&#33021;&#25163;&#26426;&#12289;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#12289;&#25628;&#32034;&#24341;&#25806;&#21644;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#31561;&#24212;&#29992;&#31243;&#24207;&#37117;&#21033;&#29992;&#20102;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25353;&#29031; PRISMA &#26694;&#26550;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#36827;&#34892;&#20102;&#33539;&#22260;&#35780;&#20272;&#12290;&#30446;&#26631;&#26159;&#23547;&#25214;&#24212;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30740;&#31350;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;&#20174;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#36873;&#21462;&#20102;&#19977;&#20010;&#30693;&#21517;&#26399;&#21002;&#65306;&#12298;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#26434;&#24535;&#12299;&#12289;&#12298;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#26434;&#24535;&#12299;&#21644;&#12298;&#26426;&#22120;&#23398;&#20064;&#12299;&#65292;&#24182;&#35266;&#23519;&#20102;2022&#24180;&#21457;&#34920;&#30340;&#25991;&#31456;&#12290;&#23545;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#19968;&#23450;&#30340;&#36164;&#26684;&#35201;&#27714;&#65306;&#25216;&#26415;&#24517;&#39035;&#38024;&#23545;&#21487;&#27604;&#36739;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#27979;&#35797;&#65292;&#24517;&#39035;&#20351;&#29992;&#20844;&#35748;&#25110;&#20854;&#20182;&#20805;&#20998;&#35777;&#26126;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#24212;&#29992;&#65292;&#24182;&#30830;&#20445;&#32467;&#26524;&#21487;&#22797;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence is more ubiquitous in multiple domains. Smartphones, social media platforms, search engines, and autonomous vehicles are just a few examples of applications that utilize artificial intelligence technologies to enhance their performance. This study carries out a scoping review of the current state-of-the-art artificial intelligence technologies following the PRISMA framework. The goal was to find the most advanced technologies used in different domains of artificial intelligence technology research. Three recognized journals were used from artificial intelligence and machine learning domain: Journal of Artificial Intelligence Research, Journal of Machine Learning Research, and Machine Learning, and articles published in 2022 were observed. Certain qualifications were laid for the technological solutions: the technology must be tested against comparable solutions, commonly approved or otherwise well justified datasets must be used while applying, and results must 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#36890;&#29992;&#27169;&#22411;&#8212;&#8212;&#19981;&#23450;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#65307;&#23427;&#21487;&#20197;&#36827;&#34892;&#26080;&#30417;&#30563;&#32858;&#31867;&#21644;&#20351;&#29992;&#24456;&#23567;&#30340;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#22823;&#35268;&#27169;&#20998;&#31867;&#65292;&#20854;&#29702;&#35770;&#20248;&#21183;&#20307;&#29616;&#22312;&#26032;&#30340;&#27010;&#29575;&#29702;&#35770;&#21644;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#20013;&#12290;</title><link>http://arxiv.org/abs/2303.11536</link><description>&lt;p&gt;
&#19981;&#23450;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Indeterminate Probability Neural Network. (arXiv:2303.11536v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#36890;&#29992;&#27169;&#22411;&#8212;&#8212;&#19981;&#23450;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#65307;&#23427;&#21487;&#20197;&#36827;&#34892;&#26080;&#30417;&#30563;&#32858;&#31867;&#21644;&#20351;&#29992;&#24456;&#23567;&#30340;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#22823;&#35268;&#27169;&#20998;&#31867;&#65292;&#20854;&#29702;&#35770;&#20248;&#21183;&#20307;&#29616;&#22312;&#26032;&#30340;&#27010;&#29575;&#29702;&#35770;&#21644;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;IPNN&#30340;&#26032;&#22411;&#36890;&#29992;&#27169;&#22411;&#65292;&#23427;&#23558;&#31070;&#32463;&#32593;&#32476;&#21644;&#27010;&#29575;&#35770;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;&#22312;&#20256;&#32479;&#27010;&#29575;&#35770;&#20013;&#65292;&#27010;&#29575;&#30340;&#35745;&#31639;&#26159;&#22522;&#20110;&#20107;&#20214;&#30340;&#21457;&#29983;&#65292;&#32780;&#36825;&#22312;&#24403;&#21069;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#20960;&#20046;&#19981;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#29702;&#35770;&#65292;&#23427;&#26159;&#32463;&#20856;&#27010;&#29575;&#35770;&#30340;&#25193;&#23637;&#65292;&#24182;&#20351;&#32463;&#20856;&#27010;&#29575;&#35770;&#25104;&#20026;&#25105;&#20204;&#29702;&#35770;&#30340;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#34987;&#23450;&#20041;&#20026;&#27010;&#29575;&#20107;&#20214;&#65292;&#24182;&#22522;&#20110;&#36825;&#20123;&#20107;&#20214;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#25512;&#23548;&#20986;&#20998;&#31867;&#20219;&#21153;&#30340;&#25512;&#29702;&#27169;&#22411;&#12290;IPNN&#23637;&#29616;&#20102;&#26032;&#30340;&#29305;&#24615;&#65306;&#23427;&#22312;&#36827;&#34892;&#20998;&#31867;&#30340;&#21516;&#26102;&#21487;&#20197;&#25191;&#34892;&#26080;&#30417;&#30563;&#32858;&#31867;&#12290;&#27492;&#22806;&#65292;IPNN&#33021;&#22815;&#20351;&#29992;&#38750;&#24120;&#23567;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#38750;&#24120;&#22823;&#30340;&#20998;&#31867;&#65292;&#20363;&#22914;100&#20010;&#36755;&#20986;&#33410;&#28857;&#30340;&#27169;&#22411;&#21487;&#20197;&#20998;&#31867;10&#20159;&#31867;&#21035;&#12290;&#29702;&#35770;&#20248;&#21183;&#20307;&#29616;&#22312;&#26032;&#30340;&#27010;&#29575;&#29702;&#35770;&#21644;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#20013;&#65292;&#24182;&#19988;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;IPNN&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new general model called IPNN - Indeterminate Probability Neural Network, which combines neural network and probability theory together. In the classical probability theory, the calculation of probability is based on the occurrence of events, which is hardly used in current neural networks. In this paper, we propose a new general probability theory, which is an extension of classical probability theory, and makes classical probability theory a special case to our theory. Besides, for our proposed neural network framework, the output of neural network is defined as probability events, and based on the statistical analysis of these events, the inference model for classification task is deduced. IPNN shows new property: It can perform unsupervised clustering while doing classification. Besides, IPNN is capable of making very large classification with very small neural network, e.g. model with 100 output nodes can classify 10 billion categories. Theoretical advantages are refl
&lt;/p&gt;</description></item></channel></rss>