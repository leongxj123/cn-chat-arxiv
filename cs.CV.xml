<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QUAVE&#30340;&#22235;&#20803;&#25968;&#23567;&#27874;&#32593;&#32476;&#65292;&#21487;&#20197;&#20174;&#21307;&#23398;&#22270;&#20687;&#20013;&#25552;&#21462;&#26174;&#33879;&#29305;&#24449;&#12290;&#35813;&#32593;&#32476;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#25110;&#32508;&#21512;&#20219;&#21153;&#32467;&#21512;&#20351;&#29992;&#65292;&#24182;&#25512;&#24191;&#20102;&#23545;&#21333;&#36890;&#36947;&#25968;&#25454;&#30340;&#37319;&#29992;&#12290;&#36890;&#36807;&#22235;&#20803;&#25968;&#23567;&#27874;&#21464;&#25442;&#21644;&#21152;&#26435;&#22788;&#29702;&#65292;QUAVE&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#36739;&#22823;&#21464;&#21270;&#30340;&#21307;&#23398;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2310.10224</link><description>&lt;p&gt;
&#36890;&#36807;&#22235;&#20803;&#25968;&#23567;&#27874;&#32593;&#32476;&#25512;&#24191;&#21307;&#23398;&#22270;&#20687;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Generalizing Medical Image Representations via Quaternion Wavelet Networks. (arXiv:2310.10224v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QUAVE&#30340;&#22235;&#20803;&#25968;&#23567;&#27874;&#32593;&#32476;&#65292;&#21487;&#20197;&#20174;&#21307;&#23398;&#22270;&#20687;&#20013;&#25552;&#21462;&#26174;&#33879;&#29305;&#24449;&#12290;&#35813;&#32593;&#32476;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#25110;&#32508;&#21512;&#20219;&#21153;&#32467;&#21512;&#20351;&#29992;&#65292;&#24182;&#25512;&#24191;&#20102;&#23545;&#21333;&#36890;&#36947;&#25968;&#25454;&#30340;&#37319;&#29992;&#12290;&#36890;&#36807;&#22235;&#20803;&#25968;&#23567;&#27874;&#21464;&#25442;&#21644;&#21152;&#26435;&#22788;&#29702;&#65292;QUAVE&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#36739;&#22823;&#21464;&#21270;&#30340;&#21307;&#23398;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#21644;&#21508;&#31181;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#26085;&#30410;&#22686;&#21152;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#26222;&#36866;&#24615;&#25104;&#20026;&#19968;&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#24403;&#22788;&#29702;&#21307;&#23398;&#25968;&#25454;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#23588;&#20026;&#24191;&#27867;&#65292;&#22240;&#20026;&#32570;&#20047;&#26041;&#27861;&#35770;&#26631;&#20934;&#23548;&#33268;&#19981;&#21516;&#30340;&#25104;&#20687;&#20013;&#24515;&#25110;&#20351;&#29992;&#19981;&#21516;&#35774;&#22791;&#21644;&#36741;&#21161;&#22240;&#32032;&#33719;&#21462;&#30340;&#25968;&#25454;&#23384;&#22312;&#36739;&#22823;&#21464;&#21270;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#26222;&#36866;&#30340;&#12289;&#25968;&#25454;-&#21644;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#21307;&#23398;&#22270;&#20687;&#20013;&#25552;&#21462;&#26174;&#33879;&#29305;&#24449;&#12290;&#25152;&#25552;&#20986;&#30340;&#22235;&#20803;&#25968;&#23567;&#27874;&#32593;&#32476;&#65288;QUAVE&#65289;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#19982;&#20219;&#20309;&#29616;&#26377;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#25110;&#32508;&#21512;&#20219;&#21153;&#30456;&#32467;&#21512;&#65292;&#24182;&#19988;&#21487;&#20197;&#32467;&#21512;&#23454;&#38469;&#12289;&#22235;&#20803;&#25968;&#25110;&#36229;&#22797;&#20540;&#27169;&#22411;&#65292;&#25512;&#24191;&#23427;&#20204;&#23545;&#21333;&#36890;&#36947;&#25968;&#25454;&#30340;&#37319;&#29992;&#12290;QUAVE&#39318;&#20808;&#36890;&#36807;&#22235;&#20803;&#25968;&#23567;&#27874;&#21464;&#25442;&#25552;&#21462;&#19981;&#21516;&#30340;&#23376;&#24102;&#65292;&#24471;&#21040;&#20302;&#39057;/&#36817;&#20284;&#39057;&#24102;&#21644;&#39640;&#39057;/&#32454;&#31890;&#24230;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#23427;&#23545;&#26368;&#26377;&#20195;&#34920;&#24615;&#30340;&#29305;&#24449;&#36827;&#34892;&#21152;&#26435;&#22788;&#29702;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#29305;&#24449;&#37325;&#35201;&#24615;&#19981;&#22343;&#21248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network generalizability is becoming a broad research field due to the increasing availability of datasets from different sources and for various tasks. This issue is even wider when processing medical data, where a lack of methodological standards causes large variations being provided by different imaging centers or acquired with various devices and cofactors. To overcome these limitations, we introduce a novel, generalizable, data- and task-agnostic framework able to extract salient features from medical images. The proposed quaternion wavelet network (QUAVE) can be easily integrated with any pre-existing medical image analysis or synthesis task, and it can be involved with real, quaternion, or hypercomplex-valued models, generalizing their adoption to single-channel data. QUAVE first extracts different sub-bands through the quaternion wavelet transform, resulting in both low-frequency/approximation bands and high-frequency/fine-grained features. Then, it weighs the most repr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;SpikeCLIP&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#23454;&#29616;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#27169;&#24577;&#25193;&#23637;&#65292;&#24182;&#22312;&#33021;&#28304;&#25928;&#29575;&#21644;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.06488</link><description>&lt;p&gt;
SpikeCLIP&#65306;&#19968;&#31181;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SpikeCLIP: A Contrastive Language-Image Pretrained Spiking Neural Network. (arXiv:2310.06488v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;SpikeCLIP&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#23454;&#29616;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#27169;&#24577;&#25193;&#23637;&#65292;&#24182;&#22312;&#33021;&#28304;&#25928;&#29575;&#21644;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#24050;&#32463;&#35777;&#26126;&#20854;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#39046;&#22495;&#20013;&#33021;&#22815;&#23454;&#29616;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#33021;&#25928;&#25552;&#39640;&#21644;&#31526;&#21512;&#29983;&#29289;&#21512;&#29702;&#24615;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#31181;&#21333;&#27169;&#24577;&#30340;SNNs&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#30340;&#24773;&#26223;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#12290;&#21463;&#21040;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#27010;&#24565;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SpikeCLIP&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#8220;&#23545;&#40784;&#39044;&#35757;&#32451;+&#21452;&#25439;&#22833;&#24494;&#35843;&#8221;&#30340;&#20004;&#27493;&#39588;&#37197;&#26041;&#65292;&#26469;&#35299;&#20915;&#33033;&#20914;&#35745;&#31639;&#32972;&#26223;&#19979;&#20004;&#31181;&#27169;&#24577;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#24120;&#29992;&#30340;&#29992;&#20110;&#22810;&#27169;&#24577;&#27169;&#22411;&#35780;&#20272;&#30340;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#65292;SNNs&#21462;&#24471;&#20102;&#19982;&#20854;DNNs&#23545;&#24212;&#29289;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#26174;&#33879;&#38477;&#20302;&#20102;&#33021;&#28304;&#28040;&#32791;&#12290;&#27492;&#22806;&#65292;SpikeCLIP&#22312;&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#20445;&#25345;&#20102;&#31283;&#23450;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks (SNNs) have demonstrated the capability to achieve comparable performance to deep neural networks (DNNs) in both visual and linguistic domains while offering the advantages of improved energy efficiency and adherence to biological plausibility. However, the extension of such single-modality SNNs into the realm of multimodal scenarios remains an unexplored territory. Drawing inspiration from the concept of contrastive language-image pre-training (CLIP), we introduce a novel framework, named SpikeCLIP, to address the gap between two modalities within the context of spike-based computing through a two-step recipe involving ``Alignment Pre-training + Dual-Loss Fine-tuning". Extensive experiments demonstrate that SNNs achieve comparable results to their DNN counterparts while significantly reducing energy consumption across a variety of datasets commonly used for multimodal model evaluation. Furthermore, SpikeCLIP maintains robust performance in image classification 
&lt;/p&gt;</description></item></channel></rss>