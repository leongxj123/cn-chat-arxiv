<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#26080;&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#24320;&#25918;&#35789;&#27719;&#30340;&#35270;&#35273;&#20851;&#31995;&#26816;&#27979;&#65292;&#36890;&#36807;Transformer-based&#22270;&#20687;&#32534;&#30721;&#22120;&#38544;&#24335;&#24314;&#27169;&#23545;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#21462;&#20851;&#31995;&#20449;&#24687;&#65292;&#22312;&#28151;&#21512;&#25968;&#25454;&#19978;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20851;&#31995;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.14270</link><description>&lt;p&gt;
&#22330;&#26223;&#22270;ViT&#65306;&#31471;&#21040;&#31471;&#30340;&#24320;&#25918;&#35789;&#27719;&#35270;&#35273;&#20851;&#31995;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Scene-Graph ViT: End-to-End Open-Vocabulary Visual Relationship Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14270
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#26080;&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#24320;&#25918;&#35789;&#27719;&#30340;&#35270;&#35273;&#20851;&#31995;&#26816;&#27979;&#65292;&#36890;&#36807;Transformer-based&#22270;&#20687;&#32534;&#30721;&#22120;&#38544;&#24335;&#24314;&#27169;&#23545;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#21462;&#20851;&#31995;&#20449;&#24687;&#65292;&#22312;&#28151;&#21512;&#25968;&#25454;&#19978;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20851;&#31995;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#20851;&#31995;&#26816;&#27979;&#26088;&#22312;&#35782;&#21035;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#21450;&#20854;&#20851;&#31995;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#29616;&#26377;&#30446;&#26631;&#26816;&#27979;&#26550;&#26500;&#20013;&#28155;&#21152;&#21333;&#29420;&#30340;&#20851;&#31995;&#27169;&#22359;&#25110;&#35299;&#30721;&#22120;&#26469;&#22788;&#29702;&#27492;&#20219;&#21153;&#12290;&#36825;&#31181;&#20998;&#31163;&#22686;&#21152;&#20102;&#22797;&#26434;&#24615;&#65292;&#38459;&#30861;&#20102;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#38480;&#21046;&#20102;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#39640;&#25928;&#30340;&#26080;&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#24320;&#25918;&#35789;&#27719;&#30340;&#35270;&#35273;&#20851;&#31995;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#30001;&#22522;&#20110;Transformer&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#32452;&#25104;&#65292;&#23558;&#23545;&#35937;&#34920;&#31034;&#20026;&#26631;&#35760;&#65292;&#24182;&#38544;&#21547;&#22320;&#24314;&#27169;&#23427;&#20204;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#25552;&#21462;&#20851;&#31995;&#20449;&#24687;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36873;&#25321;&#21487;&#33021;&#24418;&#25104;&#20851;&#31995;&#30340;&#23545;&#35937;&#23545;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21333;&#38454;&#27573;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#28151;&#21512;&#23545;&#35937;&#21644;&#20851;&#31995;&#26816;&#27979;&#25968;&#25454;&#19978;&#35757;&#32451;&#27492;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Visual Genome&#21644;&#22823;&#35789;&#27719;GQA&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20851;&#31995;&#26816;&#27979;&#24615;&#33021;&#65292;&#21487;&#23454;&#29616;&#23454;&#26102;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14270v1 Announce Type: cross  Abstract: Visual relationship detection aims to identify objects and their relationships in images. Prior methods approach this task by adding separate relationship modules or decoders to existing object detection architectures. This separation increases complexity and hinders end-to-end training, which limits performance. We propose a simple and highly efficient decoder-free architecture for open-vocabulary visual relationship detection. Our model consists of a Transformer-based image encoder that represents objects as tokens and models their relationships implicitly. To extract relationship information, we introduce an attention mechanism that selects object pairs likely to form a relationship. We provide a single-stage recipe to train this model on a mixture of object and relationship detection data. Our approach achieves state-of-the-art relationship detection performance on Visual Genome and on the large-vocabulary GQA benchmark at real-tim
&lt;/p&gt;</description></item><item><title>&#35777;&#26126;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#23384;&#22312;&#19968;&#20123;&#26080;&#20559;&#23376;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20381;&#36182;&#31639;&#27861;&#20559;&#35265;&#30340;&#24773;&#20917;&#19979;&#34987;&#25552;&#21462;&#20986;&#26469;&#65292;&#24182;&#19988;&#36825;&#31181;&#29305;&#23450;&#26550;&#26500;&#26080;&#27861;&#23398;&#20064;&#20219;&#20309;&#29305;&#23450;&#30340;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2403.14200</link><description>&lt;p&gt;
&#25163;&#26415;&#21592;&#21435;&#20559;&#35265;&#65306;&#31070;&#22855;&#30340;&#26435;&#37325;&#21450;&#22914;&#20309;&#25214;&#21040;&#23427;&#20204;
&lt;/p&gt;
&lt;p&gt;
Debiasing surgeon: fantastic weights and how to find them
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14200
&lt;/p&gt;
&lt;p&gt;
&#35777;&#26126;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#23384;&#22312;&#19968;&#20123;&#26080;&#20559;&#23376;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20381;&#36182;&#31639;&#27861;&#20559;&#35265;&#30340;&#24773;&#20917;&#19979;&#34987;&#25552;&#21462;&#20986;&#26469;&#65292;&#24182;&#19988;&#36825;&#31181;&#29305;&#23450;&#26550;&#26500;&#26080;&#27861;&#23398;&#20064;&#20219;&#20309;&#29305;&#23450;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20170;&#19968;&#20010;&#26085;&#30410;&#20851;&#27880;&#30340;&#29616;&#35937;&#26159;&#31639;&#27861;&#20559;&#35265;&#30340;&#20986;&#29616;&#65292;&#23427;&#21487;&#33021;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#27169;&#22411;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#21435;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#26356;&#25110;&#22810;&#25110;&#23569;&#22797;&#26434;&#30340;&#26041;&#27861;&#26469;&#38459;&#27490;&#36825;&#20123;&#27169;&#22411;&#22823;&#35268;&#27169;&#22320;&#20351;&#29992;&#36825;&#20123;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#36825;&#31181;&#39069;&#22806;&#30340;&#22797;&#26434;&#24615;&#30495;&#30340;&#26377;&#24517;&#35201;&#21527;&#65311;&#19968;&#20010;&#26222;&#36890;&#35757;&#32451;&#30340;&#27169;&#22411;&#26159;&#21542;&#24050;&#32463;&#21253;&#21547;&#20102;&#19968;&#20123;&#21487;&#20197;&#29420;&#31435;&#20351;&#29992;&#30340;&#8220;&#26080;&#20559;&#23376;&#32593;&#32476;&#8221;&#65292;&#24182;&#19988;&#21487;&#20197;&#25552;&#20986;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#32780;&#19981;&#20381;&#36182;&#20110;&#31639;&#27861;&#20559;&#35265;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#26679;&#30340;&#23376;&#32593;&#32476;&#36890;&#24120;&#23384;&#22312;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#19968;&#20010;&#26222;&#36890;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#25552;&#21462;&#20986;&#26469;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#36825;&#31181;&#29305;&#23450;&#30340;&#26550;&#26500;&#26080;&#27861;&#23398;&#20064;&#29305;&#23450;&#30340;&#20559;&#35265;&#65292;&#34920;&#26126;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#26377;&#21487;&#33021;&#36890;&#36807;&#26550;&#26500;&#19978;&#30340;&#23545;&#31574;&#26469;&#35299;&#20915;&#20559;&#35265;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14200v1 Announce Type: cross  Abstract: Nowadays an ever-growing concerning phenomenon, the emergence of algorithmic biases that can lead to unfair models, emerges. Several debiasing approaches have been proposed in the realm of deep learning, employing more or less sophisticated approaches to discourage these models from massively employing these biases. However, a question emerges: is this extra complexity really necessary? Is a vanilla-trained model already embodying some ``unbiased sub-networks'' that can be used in isolation and propose a solution without relying on the algorithmic biases? In this work, we show that such a sub-network typically exists, and can be extracted from a vanilla-trained model without requiring additional training. We further validate that such specific architecture is incapable of learning a specific bias, suggesting that there are possible architectural countermeasures to the problem of biases in deep neural networks.
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#29983;&#25104;&#22823;&#35268;&#27169;3D&#25968;&#25454;&#38598;&#30340;VFusion3D&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.12034</link><description>&lt;p&gt;
VFusion3D: &#20174;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#20013;&#23398;&#20064;&#21487;&#25193;&#23637;&#30340;3D&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
VFusion3D: Learning Scalable 3D Generative Models from Video Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12034
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#29983;&#25104;&#22823;&#35268;&#27169;3D&#25968;&#25454;&#38598;&#30340;VFusion3D&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33539;&#24335;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#26500;&#24314;&#21487;&#25193;&#23637;&#30340;3D&#29983;&#25104;&#27169;&#22411;&#12290;&#26500;&#24314;&#22522;&#30784;3D&#29983;&#25104;&#27169;&#22411;&#30340;&#20027;&#35201;&#38556;&#30861;&#26159;3D&#25968;&#25454;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#12290;&#19982;&#22270;&#20687;&#12289;&#25991;&#26412;&#25110;&#35270;&#39057;&#19981;&#21516;&#65292;3D&#25968;&#25454;&#19981;&#23481;&#26131;&#33719;&#21462;&#19988;&#38590;&#20197;&#33719;&#24471;&#65292;&#36825;&#23548;&#33268;&#19982;&#20854;&#20182;&#31867;&#22411;&#25968;&#25454;&#30340;&#25968;&#37327;&#30456;&#27604;&#23384;&#22312;&#26174;&#30528;&#30340;&#35268;&#27169;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19968;&#20010;&#36890;&#36807;&#22823;&#37327;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#35270;&#39057;&#35757;&#32451;&#30340;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;3D&#25968;&#25454;&#30340;&#30693;&#35782;&#28304;&#12290;&#36890;&#36807;&#24494;&#35843;&#35299;&#38145;&#20854;&#22810;&#35270;&#35282;&#29983;&#25104;&#33021;&#21147;&#65292;&#25105;&#20204;&#29983;&#25104;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#21512;&#25104;&#22810;&#35270;&#35282;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#21069;&#39304;3D&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12034v1 Announce Type: cross  Abstract: This paper presents a novel paradigm for building scalable 3D generative models utilizing pre-trained video diffusion models. The primary obstacle in developing foundation 3D generative models is the limited availability of 3D data. Unlike images, texts, or videos, 3D data are not readily accessible and are difficult to acquire. This results in a significant disparity in scale compared to the vast quantities of other types of data. To address this issue, we propose using a video diffusion model, trained with extensive volumes of text, images, and videos, as a knowledge source for 3D data. By unlocking its multi-view generative capabilities through fine-tuning, we generate a large-scale synthetic multi-view dataset to train a feed-forward 3D generative model. The proposed model, VFusion3D, trained on nearly 3M synthetic multi-view data, can generate a 3D asset from a single image in seconds and achieves superior performance when compare
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#20010;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21512;&#20316;&#25512;&#29702;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;V-HOI Multi-LLMs Collaborated Reasoning&#65288;V-HOI MLCR&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#24403;&#21069;V-HOI&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.10107</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#20010;LLM&#21512;&#20316;&#25512;&#29702;&#25552;&#21319;&#20154;&#31867;&#20013;&#24515;&#21160;&#24577;&#22330;&#26223;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Enhancing Human-Centered Dynamic Scene Understanding via Multiple LLMs Collaborated Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10107
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#20010;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21512;&#20316;&#25512;&#29702;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;V-HOI Multi-LLMs Collaborated Reasoning&#65288;V-HOI MLCR&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#24403;&#21069;V-HOI&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20013;&#24515;&#30340;&#21160;&#24577;&#22330;&#26223;&#29702;&#35299;&#22312;&#22686;&#24378;&#26426;&#22120;&#20154;&#21644;&#33258;&#20027;&#31995;&#32479;&#30340;&#33021;&#21147;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20854;&#20013;&#35270;&#39057;&#20154;-&#29289;&#20132;&#20114;&#65288;V-HOI&#65289;&#26816;&#27979;&#26159;&#35821;&#20041;&#22330;&#26223;&#29702;&#35299;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#26088;&#22312;&#20840;&#38754;&#29702;&#35299;&#35270;&#39057;&#20013;&#30340;HOI&#20851;&#31995;&#65292;&#20197;&#20351;&#31227;&#21160;&#26426;&#22120;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#34892;&#20026;&#20915;&#31574;&#21463;&#30410;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;V-HOI&#26816;&#27979;&#27169;&#22411;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#32570;&#20047;&#20687;&#20154;&#31867;&#19968;&#26679;&#30340;&#36890;&#29992;&#25512;&#29702;&#33021;&#21147;&#65292;&#26080;&#27861;&#26377;&#25928;&#24341;&#23548;HOI&#20851;&#31995;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;V-HOI&#22810;LLM&#21327;&#21516;&#25512;&#29702;&#65288;V-HOI MLCR&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#30001;&#19968;&#31995;&#21015;&#21363;&#25554;&#21363;&#29992;&#30340;&#27169;&#22359;&#32452;&#25104;&#65292;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#29616;&#25104;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#65292;&#20419;&#36827;&#24403;&#21069;V-HOI&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10107v1 Announce Type: cross  Abstract: Human-centered dynamic scene understanding plays a pivotal role in enhancing the capability of robotic and autonomous systems, in which Video-based Human-Object Interaction (V-HOI) detection is a crucial task in semantic scene understanding, aimed at comprehensively understanding HOI relationships within a video to benefit the behavioral decisions of mobile robots and autonomous driving systems. Although previous V-HOI detection models have made significant strides in accurate detection on specific datasets, they still lack the general reasoning ability like human beings to effectively induce HOI relationships. In this study, we propose V-HOI Multi-LLMs Collaborated Reasoning (V-HOI MLCR), a novel framework consisting of a series of plug-and-play modules that could facilitate the performance of current V-HOI detection models by leveraging the strong reasoning ability of different off-the-shelf pre-trained large language models (LLMs). 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22330;&#26223;&#22270;&#39044;&#27979;&#65288;SGA&#65289;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;SceneSayer&#65292;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;ODE&#21644;&#31070;&#32463;SDE&#30340;&#27010;&#24565;&#65292;&#32467;&#21512;&#23545;&#35937;-centric&#30340;&#20851;&#31995;&#34920;&#31034;&#65292;&#23454;&#29616;&#23545;&#35937;&#20043;&#38388;&#26410;&#26469;&#20851;&#31995;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.04899</link><description>&lt;p&gt;
&#26397;&#21521;&#22330;&#26223;&#22270;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Towards Scene Graph Anticipation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04899
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22330;&#26223;&#22270;&#39044;&#27979;&#65288;SGA&#65289;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;SceneSayer&#65292;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;ODE&#21644;&#31070;&#32463;SDE&#30340;&#27010;&#24565;&#65292;&#32467;&#21512;&#23545;&#35937;-centric&#30340;&#20851;&#31995;&#34920;&#31034;&#65292;&#23454;&#29616;&#23545;&#35937;&#20043;&#38388;&#26410;&#26469;&#20851;&#31995;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#22330;&#26223;&#22270;&#36890;&#36807;&#23558;&#22330;&#26223;&#20998;&#35299;&#20026;&#21333;&#20010;&#23545;&#35937;&#21450;&#20854;&#20004;&#20004;&#26102;&#38388;&#20851;&#31995;&#26469;&#34920;&#31034;&#35270;&#39057;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#38271;&#26399;&#39044;&#27979;&#23545;&#35937;&#20043;&#38388;&#31934;&#32454;&#31890;&#24230;&#30340;&#20004;&#20004;&#20851;&#31995;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22330;&#26223;&#22270;&#39044;&#27979;&#65288;SGA&#65289;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;&#26368;&#20808;&#36827;&#30340;&#22330;&#26223;&#22270;&#29983;&#25104;&#26041;&#27861;&#29992;&#20316;&#22522;&#32447;&#65292;&#20197;&#39044;&#27979;&#23545;&#35937;&#20043;&#38388;&#26410;&#26469;&#30340;&#20004;&#20004;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;SceneSayer&#12290;&#22312;SceneSayer&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#38754;&#21521;&#23545;&#35937;&#30340;&#20851;&#31995;&#34920;&#31034;&#26469;&#25512;&#26029;&#35266;&#23519;&#21040;&#30340;&#35270;&#39057;&#24103;&#24182;&#24314;&#27169;&#23545;&#35937;&#20043;&#38388;&#20851;&#31995;&#30340;&#28436;&#21464;&#12290;&#25105;&#20204;&#37319;&#29992;&#36830;&#32493;&#26102;&#38388;&#35270;&#35282;&#65292;&#24182;&#20998;&#21035;&#20351;&#29992;&#31070;&#32463;ODE&#21644;&#31070;&#32463;SDE&#30340;&#27010;&#24565;&#26469;&#24314;&#27169;&#23545;&#35937;&#30456;&#20114;&#20316;&#29992;&#30340;&#28508;&#22312;&#21160;&#24577;&#28436;&#21464;&#12290;&#25105;&#20204;&#36890;&#36807;&#35299;&#20915;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#21644;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#26469;&#25512;&#26029;&#26410;&#26469;&#20851;&#31995;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04899v1 Announce Type: cross  Abstract: Spatio-temporal scene graphs represent interactions in a video by decomposing scenes into individual objects and their pair-wise temporal relationships. Long-term anticipation of the fine-grained pair-wise relationships between objects is a challenging problem. To this end, we introduce the task of Scene Graph Anticipation (SGA). We adapt state-of-the-art scene graph generation methods as baselines to anticipate future pair-wise relationships between objects and propose a novel approach SceneSayer. In SceneSayer, we leverage object-centric representations of relationships to reason about the observed video frames and model the evolution of relationships between objects. We take a continuous time perspective and model the latent dynamics of the evolution of object interactions using concepts of NeuralODE and NeuralSDE, respectively. We infer representations of future relationships by solving an Ordinary Differential Equation and a Stoch
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#23618;&#22797;&#26434;&#24230;&#21305;&#37197;&#23398;&#20064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#19979;&#32780;&#19978;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#26041;&#27861;&#65292;&#26368;&#22823;&#21270;&#20102;&#29305;&#24449;&#30456;&#20284;&#24615;&#21516;&#26102;&#22312;&#19981;&#21516;&#20301;&#32622;&#30340;&#34917;&#19969;&#19978;&#35299;&#38500;&#29305;&#24449;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.11436</link><description>&lt;p&gt;
&#20998;&#23618;&#22797;&#26434;&#24230;&#21305;&#37197;&#23398;&#20064;&#20135;&#29983;&#20102;&#25913;&#36827;&#30340;&#22823;&#33041;&#30382;&#23618;V2&#21306;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Layerwise complexity-matched learning yields an improved model of cortical area V2
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11436
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#23618;&#22797;&#26434;&#24230;&#21305;&#37197;&#23398;&#20064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#19979;&#32780;&#19978;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#26041;&#27861;&#65292;&#26368;&#22823;&#21270;&#20102;&#29305;&#24449;&#30456;&#20284;&#24615;&#21516;&#26102;&#22312;&#19981;&#21516;&#20301;&#32622;&#30340;&#34917;&#19969;&#19978;&#35299;&#38500;&#29305;&#24449;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35782;&#21035;&#22797;&#26434;&#35270;&#35273;&#27169;&#24335;&#30340;&#33021;&#21147;&#26159;&#36890;&#36807;&#39034;&#27425;&#21306;&#22495;&#22312;&#33145;&#20391;&#35270;&#35273;&#30382;&#23618;&#20013;&#25191;&#34892;&#30340;&#21464;&#25442;&#25152;&#24418;&#25104;&#30340;&#12290;&#26368;&#36817;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#20102;&#20154;&#31867;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#36804;&#20170;&#20026;&#27490;&#23545;&#23618;&#27425;&#32467;&#26500;&#30340;&#21518;&#26399;&#31070;&#32463;&#21453;&#24212;&#30340;&#26368;&#20339;&#25551;&#36848;&#12290;&#28982;&#32780;&#65292;&#19982;&#20256;&#32479;&#30340;&#25163;&#24037;&#35774;&#35745;&#27169;&#22411;&#30456;&#27604;&#65292;&#25110;&#32773;&#19982;&#20248;&#21270;&#32534;&#30721;&#25928;&#29575;&#25110;&#39044;&#27979;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#36825;&#20123;&#32593;&#32476;&#23545;&#21069;&#26399;&#38454;&#27573;&#25552;&#20379;&#20102;&#36739;&#24046;&#30340;&#25551;&#36848;&#12290;&#27492;&#22806;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;&#23398;&#20064;&#30340;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#36890;&#24120;&#34987;&#35748;&#20026;&#22312;&#29983;&#29289;&#19978;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#33258;&#19979;&#32780;&#19978;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#26041;&#27861;&#65292;&#29420;&#31435;&#22320;&#20316;&#29992;&#20110;&#36830;&#32493;&#23618;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#36825;&#20004;&#20010;&#38480;&#21046;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#26368;&#22823;&#21270;&#20102;&#23545;&#23616;&#37096;&#21464;&#24418;&#33258;&#28982;&#22270;&#20687;&#34917;&#19969;&#23545;&#20043;&#38388;&#30340;&#29305;&#24449;&#30456;&#20284;&#24615;&#65292;&#24182;&#22312;&#37319;&#26679;&#33258;&#20854;&#20182;&#20301;&#32622;&#30340;&#34917;&#19969;&#26102;&#20351;&#29305;&#24449;&#21435;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11436v2 Announce Type: replace-cross  Abstract: Human ability to recognize complex visual patterns arises through transformations performed by successive areas in the ventral visual cortex. Deep neural networks trained end-to-end for object recognition approach human capabilities, and offer the best descriptions to date of neural responses in the late stages of the hierarchy. But these networks provide a poor account of the early stages, compared to traditional hand-engineered models, or models optimized for coding efficiency or prediction. Moreover, the gradient backpropagation used in end-to-end learning is generally considered to be biologically implausible. Here, we overcome both of these limitations by developing a bottom-up self-supervised training methodology that operates independently on successive layers. Specifically, we maximize feature similarity between pairs of locally-deformed natural image patches, while decorrelating features across patches sampled from oth
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#34394;&#25311;&#29616;&#23454;&#22836;&#20687;&#27880;&#20876;&#21644;&#38754;&#37096;&#21160;&#30011;&#38382;&#39064;&#65292;&#21457;&#29616;&#22836;&#20687;&#21644;&#22836;&#26174;&#30456;&#26426;&#22270;&#20687;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#26159;&#20027;&#35201;&#38590;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#35774;&#35745;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.11002</link><description>&lt;p&gt;
&#24555;&#36895;&#27880;&#20876;&#36924;&#30495;&#30340;&#34394;&#25311;&#29616;&#23454;&#22836;&#20687;&#29992;&#20110;&#38754;&#37096;&#21160;&#30011;
&lt;/p&gt;
&lt;p&gt;
Fast Registration of Photorealistic Avatars for VR Facial Animation. (arXiv:2401.11002v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#34394;&#25311;&#29616;&#23454;&#22836;&#20687;&#27880;&#20876;&#21644;&#38754;&#37096;&#21160;&#30011;&#38382;&#39064;&#65292;&#21457;&#29616;&#22836;&#20687;&#21644;&#22836;&#26174;&#30456;&#26426;&#22270;&#20687;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#26159;&#20027;&#35201;&#38590;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#35774;&#35745;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#22312;&#31038;&#20132;&#20114;&#21160;&#26041;&#38754;&#25317;&#26377;&#26356;&#20855;&#27785;&#28024;&#24863;&#30340;&#28508;&#21147;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#26159;&#33021;&#22815;&#22312;&#20329;&#25140;VR&#22836;&#26174;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#22320;&#27169;&#25311;&#19968;&#20010;&#36924;&#30495;&#30340;&#22836;&#20687;&#12290;&#34429;&#28982;&#22312;&#31163;&#32447;&#29615;&#22659;&#20013;&#21487;&#20197;&#23454;&#29616;&#23545;&#29305;&#23450;&#20010;&#20154;&#22836;&#20687;&#36827;&#34892;&#39640;&#36136;&#37327;&#27880;&#20876;&#65292;&#24182;&#36827;&#34892;&#21160;&#30011;&#29983;&#25104;&#65292;&#20294;&#36890;&#29992;&#23454;&#26102;&#27169;&#22411;&#30340;&#24615;&#33021;&#26126;&#26174;&#19979;&#38477;&#12290;&#22312;&#32447;&#27880;&#20876;&#20063;&#38754;&#20020;&#35832;&#22810;&#25361;&#25112;&#65292;&#21253;&#25324;&#20542;&#26012;&#30340;&#25668;&#20687;&#26426;&#35270;&#35282;&#21644;&#19981;&#21516;&#30340;&#27169;&#24577;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#34920;&#26126;&#22836;&#20687;&#19982;&#22836;&#26174;&#30456;&#26426;&#22270;&#20687;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#26159;&#22256;&#38590;&#30340;&#20027;&#35201;&#28304;&#27849;&#20043;&#19968;&#65292;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#26550;&#26500;&#22312;&#39046;&#22495;&#19968;&#33268;&#30340;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#65292;&#22312;&#24341;&#20837;&#39046;&#22495;&#24046;&#36317;&#21518;&#24615;&#33021;&#19979;&#38477;&#12290;&#22522;&#20110;&#27492;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#35774;&#35745;&#65292;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#20004;&#20010;&#37096;&#20998;&#65306;1&#65289;&#19968;&#20010;&#36845;&#20195;&#32454;&#21270;&#27169;&#22359;&#65292;&#25509;&#25910;&#39046;&#22495;&#20869;&#36755;&#20837;&#65292;&#21644;2&#65289;&#19968;&#20010;&#36890;&#29992;&#30340;&#22836;&#20687;&#24341;&#23548;&#22270;&#20687;&#29983;&#25104;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Virtual Reality (VR) bares promise of social interactions that can feel more immersive than other media. Key to this is the ability to accurately animate a photorealistic avatar of one's likeness while wearing a VR headset. Although high quality registration of person-specific avatars to headset-mounted camera (HMC) images is possible in an offline setting, the performance of generic realtime models are significantly degraded. Online registration is also challenging due to oblique camera views and differences in modality. In this work, we first show that the domain gap between the avatar and headset-camera images is one of the primary sources of difficulty, where a transformer-based architecture achieves high accuracy on domain-consistent data, but degrades when the domain-gap is re-introduced. Building on this finding, we develop a system design that decouples the problem into two parts: 1) an iterative refinement module that takes in-domain inputs, and 2) a generic avatar-guided imag
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#23545;&#25239;&#26679;&#26412;&#65288;PAEs&#65289;&#30340;&#29305;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#20854;&#29305;&#24449;&#30340;&#20840;&#38754;&#20998;&#26512;&#21644;&#20998;&#31867;&#26694;&#26550;&#65292;&#28085;&#30422;&#20102;100&#22810;&#20010;&#30740;&#31350;&#65292;&#20197;&#22635;&#34917;&#23545;PAEs&#29420;&#29305;&#29305;&#24449;&#30340;&#29616;&#26377;&#30740;&#31350;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2311.01473</link><description>&lt;p&gt;
&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#23545;&#25239;&#26679;&#26412;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Adversarial Examples in the Physical World: A Survey. (arXiv:2311.01473v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#23545;&#25239;&#26679;&#26412;&#65288;PAEs&#65289;&#30340;&#29305;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#20854;&#29305;&#24449;&#30340;&#20840;&#38754;&#20998;&#26512;&#21644;&#20998;&#31867;&#26694;&#26550;&#65292;&#28085;&#30422;&#20102;100&#22810;&#20010;&#30740;&#31350;&#65292;&#20197;&#22635;&#34917;&#23545;PAEs&#29420;&#29305;&#29305;&#24449;&#30340;&#29616;&#26377;&#30740;&#31350;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#23545;&#23545;&#25239;&#26679;&#26412;&#34920;&#29616;&#20986;&#39640;&#24230;&#30340;&#33030;&#24369;&#24615;&#12290;&#38500;&#20102;&#22312;&#25968;&#23383;&#19990;&#30028;&#20013;&#30340;&#25915;&#20987;&#22806;&#65292;&#23545;&#25239;&#26679;&#26412;&#22312;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#23454;&#38469;&#24433;&#21709;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#21644;&#23433;&#20840;&#24615;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#23545;&#29289;&#29702;&#23545;&#25239;&#26679;&#26412;&#65288;PAEs&#65289;&#30340;&#30740;&#31350;&#32570;&#20047;&#23545;&#20854;&#29420;&#29305;&#29305;&#24449;&#30340;&#20840;&#38754;&#29702;&#35299;&#65292;&#23548;&#33268;&#20854;&#37325;&#35201;&#24615;&#21644;&#29702;&#35299;&#30340;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#35757;&#32451;&#12289;&#21046;&#36896;&#21644;&#37325;&#37319;&#26679;&#36807;&#31243;&#20013;&#20840;&#38754;&#32771;&#23519;PAEs&#30340;&#29305;&#28857;&#26469;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#36890;&#36807;&#20998;&#26512;&#29289;&#29702;&#23545;&#25239;&#25915;&#20987;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25105;&#20204;&#30830;&#23450;&#21046;&#36896;&#21644;&#37325;&#37319;&#26679;&#26159;PAEs&#20013;&#29420;&#29305;&#23646;&#24615;&#21644;&#29305;&#27530;&#24615;&#30340;&#20027;&#35201;&#26469;&#28304;&#12290;&#21033;&#29992;&#36825;&#19968;&#30693;&#35782;&#65292;&#25105;&#20204;&#22522;&#20110;&#20854;&#29305;&#23450;&#29305;&#24449;&#24320;&#21457;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;PAEs&#20998;&#26512;&#21644;&#20998;&#31867;&#26694;&#26550;&#65292;&#28085;&#30422;&#20102;100&#22810;&#20010;&#29289;&#29702;&#23545;&#25239;&#19990;&#30028;&#30740;&#31350;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have demonstrated high vulnerability to adversarial examples. Besides the attacks in the digital world, the practical implications of adversarial examples in the physical world present significant challenges and safety concerns. However, current research on physical adversarial examples (PAEs) lacks a comprehensive understanding of their unique characteristics, leading to limited significance and understanding. In this paper, we address this gap by thoroughly examining the characteristics of PAEs within a practical workflow encompassing training, manufacturing, and re-sampling processes. By analyzing the links between physical adversarial attacks, we identify manufacturing and re-sampling as the primary sources of distinct attributes and particularities in PAEs. Leveraging this knowledge, we develop a comprehensive analysis and classification framework for PAEs based on their specific characteristics, covering over 100 studies on physical-world adversarial e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20998;&#23618;&#29305;&#24449;&#20849;&#20139;&#30340;&#25968;&#25454;&#21442;&#25968;&#21270;&#26550;&#26500;&#65288;HMN&#65289;&#65292;&#26088;&#22312;&#26356;&#39640;&#25928;&#22320;&#21387;&#32553;&#25968;&#25454;&#12290;&#36890;&#36807;&#23558;&#25968;&#25454;&#23384;&#20648;&#22312;&#19977;&#23618;&#32467;&#26500;&#20013;&#65292;HMN&#33021;&#22815;&#25429;&#25417;&#21040;&#25968;&#25454;&#38598;&#32423;&#21035;&#12289;&#31867;&#21035;&#32423;&#21035;&#21644;&#26679;&#26412;&#32423;&#21035;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2310.07506</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#23618;&#29305;&#24449;&#20849;&#20139;&#36827;&#34892;&#39640;&#25928;&#25968;&#25454;&#38598;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Leveraging Hierarchical Feature Sharing for Efficient Dataset Condensation. (arXiv:2310.07506v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20998;&#23618;&#29305;&#24449;&#20849;&#20139;&#30340;&#25968;&#25454;&#21442;&#25968;&#21270;&#26550;&#26500;&#65288;HMN&#65289;&#65292;&#26088;&#22312;&#26356;&#39640;&#25928;&#22320;&#21387;&#32553;&#25968;&#25454;&#12290;&#36890;&#36807;&#23558;&#25968;&#25454;&#23384;&#20648;&#22312;&#19977;&#23618;&#32467;&#26500;&#20013;&#65292;HMN&#33021;&#22815;&#25429;&#25417;&#21040;&#25968;&#25454;&#38598;&#32423;&#21035;&#12289;&#31867;&#21035;&#32423;&#21035;&#21644;&#26679;&#26412;&#32423;&#21035;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#65292;&#25968;&#25454;&#21387;&#32553;&#65288;DC&#65289;&#26088;&#22312;&#21512;&#25104;&#19968;&#20010;&#26174;&#33879;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#39640;&#24615;&#33021;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#25968;&#25454;&#21442;&#25968;&#21270;&#22686;&#24378;DC&#65292;&#23558;&#25968;&#25454;&#21387;&#32553;&#20026;&#21442;&#25968;&#21270;&#30340;&#25968;&#25454;&#23481;&#22120;&#32780;&#19981;&#26159;&#20687;&#32032;&#31354;&#38388;&#12290;&#25968;&#25454;&#21442;&#25968;&#21270;&#30340;&#30452;&#35273;&#26159;&#32534;&#30721;&#22270;&#20687;&#30340;&#20849;&#20139;&#29305;&#24449;&#65292;&#20197;&#36991;&#20813;&#39069;&#22806;&#30340;&#23384;&#20648;&#25104;&#26412;&#12290;&#26412;&#25991;&#35748;&#35782;&#21040;&#30001;&#20110;&#20998;&#31867;&#31995;&#32479;&#30340;&#20869;&#22312;&#20998;&#23618;&#32467;&#26500;&#65292;&#22270;&#20687;&#20197;&#20998;&#23618;&#26041;&#24335;&#20849;&#20139;&#20849;&#21516;&#30340;&#29305;&#24449;&#65292;&#36825;&#26159;&#24403;&#21069;&#25968;&#25454;&#21442;&#25968;&#21270;&#26041;&#27861;&#25152;&#24573;&#35270;&#30340;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#20351;DC&#19982;&#36825;&#31181;&#20998;&#23618;&#24615;&#36136;&#23545;&#40784;&#65292;&#24182;&#22312;&#25968;&#25454;&#23481;&#22120;&#20869;&#37096;&#40723;&#21169;&#26356;&#39640;&#25928;&#30340;&#20449;&#24687;&#20849;&#20139;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#21442;&#25968;&#21270;&#26550;&#26500;&#65292;&#20998;&#23618;&#35760;&#24518;&#32593;&#32476;&#65288;HMN&#65289;&#12290;HMN&#23558;&#21387;&#32553;&#25968;&#25454;&#23384;&#20648;&#22312;&#19977;&#23618;&#32467;&#26500;&#20013;&#65292;&#34920;&#31034;&#25968;&#25454;&#38598;&#32423;&#21035;&#12289;&#31867;&#21035;&#32423;&#21035;&#21644;&#26679;&#26412;&#32423;&#21035;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a real-world dataset, data condensation (DC) aims to synthesize a significantly smaller dataset that captures the knowledge of this dataset for model training with high performance. Recent works propose to enhance DC with data parameterization, which condenses data into parameterized data containers rather than pixel space. The intuition behind data parameterization is to encode shared features of images to avoid additional storage costs. In this paper, we recognize that images share common features in a hierarchical way due to the inherent hierarchical structure of the classification system, which is overlooked by current data parameterization methods. To better align DC with this hierarchical nature and encourage more efficient information sharing inside data containers, we propose a novel data parameterization architecture, Hierarchical Memory Network (HMN). HMN stores condensed data in a three-tier structure, representing the dataset-level, class-level, and instance-level fea
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#20266;&#26631;&#31614;&#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#25913;&#21892;&#22522;&#20934;Yolo&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;&#20896;&#29366;&#21160;&#33033;&#20998;&#21106;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.05990</link><description>&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#22312;&#33258;&#21160;&#21306;&#22495;&#24615;&#20896;&#29366;&#21160;&#33033;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#65306;&#20266;&#26631;&#31614;&#27861;&#29992;&#20110;&#30142;&#30149;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation through Pseudolabels in Automatic Region Based Coronary Artery Segmentation for Disease Diagnosis. (arXiv:2310.05990v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05990
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#20266;&#26631;&#31614;&#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#25913;&#21892;&#22522;&#20934;Yolo&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;&#20896;&#29366;&#21160;&#33033;&#20998;&#21106;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20896;&#29366;&#21160;&#33033;&#30142;&#30149;&#65288;CAD&#65289;&#26159;&#21487;&#39044;&#38450;&#30340;&#20027;&#35201;&#27515;&#20129;&#21644;&#27531;&#30142;&#21407;&#22240;&#20043;&#19968;&#12290;&#36825;&#20123;&#30142;&#30149;&#30340;&#35786;&#26029;&#36890;&#24120;&#22256;&#38590;&#19988;&#36164;&#28304;&#23494;&#38598;&#12290;&#34880;&#31649;&#36896;&#24433;&#22270;&#20687;&#20013;&#30340;&#21160;&#33033;&#20998;&#21106;&#24050;&#32463;&#28436;&#21464;&#25104;&#20026;&#19968;&#31181;&#36741;&#21161;&#24037;&#20855;&#65292;&#21487;&#20197;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#36827;&#34892;&#20934;&#30830;&#30340;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#37327;&#26377;&#38480;&#19988;&#26500;&#24314;&#25968;&#25454;&#38598;&#30340;&#22256;&#38590;&#65292;&#20998;&#21106;&#20219;&#21153;&#19968;&#30452;&#24456;&#20855;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20351;&#29992;&#20266;&#26631;&#31614;&#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#25913;&#21892;&#22522;&#20934;Yolo&#27169;&#22411;&#24615;&#33021;&#30340;&#24605;&#24819;&#12290;&#35813;&#26041;&#27861;&#22312;&#39564;&#35777;&#25968;&#25454;&#38598;&#20013;&#23558;&#22522;&#32447;&#30340;F1&#20998;&#25968;&#25552;&#39640;&#20102;9&#65285;&#65292;&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#20013;&#25552;&#39640;&#20102;3&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coronary Artery Diseases(CADs) though preventable are one of the leading causes of death and disability. Diagnosis of these diseases is often difficult and resource intensive. Segmentation of arteries in angiographic images has evolved as a tool for assistance, helping clinicians in making accurate diagnosis. However, due to the limited amount of data and the difficulty in curating a dataset, the task of segmentation has proven challenging. In this study, we introduce the idea of using pseudolabels as a data augmentation technique to improve the performance of the baseline Yolo model. This method increases the F1 score of the baseline by 9% in the validation dataset and by 3% in the test dataset.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26816;&#27979;&#30340;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#26088;&#22312;&#24357;&#21512;&#22270;&#20687;&#32423;&#39044;&#35757;&#32451;&#21644;&#24320;&#25918;&#35789;&#27719;&#30446;&#26631;&#26816;&#27979;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#26816;&#27979;&#22120;&#26550;&#26500;&#21644;&#23545;&#27604;&#25439;&#22833;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#22122;&#22768;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20013;&#23398;&#20064;&#21040;&#26032;&#20986;&#29616;&#30340;&#29289;&#20307;-&#35821;&#20041;&#32447;&#32034;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#31227;&#31383;&#21475;&#23398;&#20064;&#26041;&#27861;&#26469;&#25913;&#36827;&#20027;&#24178;&#32593;&#32476;&#30340;&#34920;&#31034;&#12290;&#22312;LVIS&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#22522;&#20934;&#19978;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;40.4&#30340;&#25513;&#30721;AP$_r$&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.00161</link><description>&lt;p&gt;
&#38754;&#21521;&#26816;&#27979;&#30340;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#29992;&#20110;&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Detection-Oriented Image-Text Pretraining for Open-Vocabulary Detection. (arXiv:2310.00161v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00161
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26816;&#27979;&#30340;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#26088;&#22312;&#24357;&#21512;&#22270;&#20687;&#32423;&#39044;&#35757;&#32451;&#21644;&#24320;&#25918;&#35789;&#27719;&#30446;&#26631;&#26816;&#27979;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#26816;&#27979;&#22120;&#26550;&#26500;&#21644;&#23545;&#27604;&#25439;&#22833;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#22122;&#22768;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20013;&#23398;&#20064;&#21040;&#26032;&#20986;&#29616;&#30340;&#29289;&#20307;-&#35821;&#20041;&#32447;&#32034;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#31227;&#31383;&#21475;&#23398;&#20064;&#26041;&#27861;&#26469;&#25913;&#36827;&#20027;&#24178;&#32593;&#32476;&#30340;&#34920;&#31034;&#12290;&#22312;LVIS&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#22522;&#20934;&#19978;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;40.4&#30340;&#25513;&#30721;AP$_r$&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38754;&#21521;&#26816;&#27979;&#30340;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#26032;&#30340;&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#26041;&#27861;&#65292;&#20197;&#22635;&#34917;&#22270;&#20687;&#32423;&#39044;&#35757;&#32451;&#21644;&#24320;&#25918;&#35789;&#27719;&#30446;&#26631;&#26816;&#27979;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#29992;&#26816;&#27979;&#22120;&#26550;&#26500;&#26367;&#20195;&#24120;&#29992;&#30340;&#20998;&#31867;&#26550;&#26500;&#65292;&#36890;&#36807;&#20351;&#26816;&#27979;&#22120;&#22836;&#37096;&#33021;&#22815;&#20174;&#22122;&#22768;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20013;&#23398;&#20064;&#65292;&#26356;&#22909;&#22320;&#28385;&#36275;&#26816;&#27979;&#30340;&#21306;&#22495;&#32423;&#35782;&#21035;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#20351;&#29992;&#26631;&#20934;&#30340;&#23545;&#27604;&#25439;&#22833;&#32780;&#19981;&#20351;&#29992;&#20266;&#26631;&#31614;&#65292;&#26159;&#23545;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25193;&#23637;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#26032;&#20986;&#29616;&#30340;&#29289;&#20307;-&#35821;&#20041;&#32447;&#32034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31383;&#21475;&#27880;&#24847;&#21147;&#30340;&#24179;&#31227;&#31383;&#21475;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#20027;&#24178;&#32593;&#32476;&#30340;&#34920;&#31034;&#26356;&#21152;&#40065;&#26834;&#12289;&#24179;&#31227;&#19981;&#21464;&#65292;&#24182;&#19988;&#19981;&#21463;&#31383;&#21475;&#27169;&#24335;&#30340;&#20559;&#24046;&#24433;&#21709;&#12290;&#22312;&#27969;&#34892;&#30340;LVIS&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#22522;&#20934;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#24120;&#35265;&#30340;ViT-L&#20027;&#24178;&#32593;&#32476;&#21462;&#24471;&#20102;40.4&#30340;&#25513;&#30721;AP$_r$&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#65292;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new open-vocabulary detection approach based on detection-oriented image-text pretraining to bridge the gap between image-level pretraining and open-vocabulary object detection. At the pretraining phase, we replace the commonly used classification architecture with the detector architecture, which better serves the region-level recognition needs of detection by enabling the detector heads to learn from noisy image-text pairs. Using only standard contrastive loss and no pseudo-labeling, our approach is a simple yet effective extension of the contrastive learning method to learn emergent object-semantic cues. In addition, we propose a shifted-window learning approach upon window attention to make the backbone representation more robust, translation-invariant, and less biased by the window pattern. On the popular LVIS open-vocabulary detection benchmark, our approach sets a new state of the art of 40.4 mask AP$_r$ using the common ViT-L backbone, significantly outperforming t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#22797;&#20132;&#21449;&#39564;&#35777;(Repeated Cross-Validation)&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22122;&#22768;&#30452;&#26041;&#22270;&#24182;&#25552;&#20986;&#19977;&#31181;&#22522;&#20110;&#35813;&#30452;&#26041;&#22270;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#26631;&#31614;&#22122;&#22768;&#24182;&#28165;&#29702;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#32467;&#26524;&#39044;&#27979;&#20998;&#26512;&#20013;&#30340;&#25968;&#25454;&#28165;&#27927;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.13990</link><description>&lt;p&gt;
&#20132;&#21449;&#39564;&#35777;&#23601;&#26159;&#20320;&#25152;&#38656;&#30340;&#65306;&#19968;&#31181;&#32479;&#35745;&#26041;&#27861;&#26469;&#20272;&#35745;&#26631;&#31614;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-Validation Is All You Need: A Statistical Approach To Label Noise Estimation. (arXiv:2306.13990v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13990
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#22797;&#20132;&#21449;&#39564;&#35777;(Repeated Cross-Validation)&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22122;&#22768;&#30452;&#26041;&#22270;&#24182;&#25552;&#20986;&#19977;&#31181;&#22522;&#20110;&#35813;&#30452;&#26041;&#22270;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#26631;&#31614;&#22122;&#22768;&#24182;&#28165;&#29702;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#32467;&#26524;&#39044;&#27979;&#20998;&#26512;&#20013;&#30340;&#25968;&#25454;&#28165;&#27927;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#22122;&#22768;&#22312;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#37492;&#23450;&#21644;&#28040;&#38500;&#26631;&#31614;&#22122;&#22768;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#22312;&#22122;&#22768;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20250;&#22823;&#24133;&#38477;&#20302;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26631;&#31614;&#22122;&#22768;&#26816;&#27979;&#26041;&#27861;&#37117;&#26159;&#20026;&#20998;&#31867;&#20219;&#21153;&#35774;&#35745;&#30340;&#65292;&#32780;&#22522;&#20110;&#32467;&#26524;&#39044;&#27979;&#20998;&#26512;&#30340;&#25968;&#25454;&#28165;&#29702;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#21463;&#21040;&#20132;&#21449;&#39564;&#35777;&#20013;&#19981;&#21516;&#25240;&#30340;&#24615;&#33021;&#27874;&#21160;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#26631;&#31614;&#22122;&#22768;&#20272;&#35745;&#30340;&#37325;&#22797;&#20132;&#21449;&#39564;&#35777;&#65288;ReCoV&#65289;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;ReCoV&#36890;&#36807;&#35760;&#24405;&#27599;&#20010;&#26368;&#24046;&#34920;&#29616;&#25240;&#20013;&#30340;&#26679;&#26412;ID&#26469;&#26500;&#24314;&#19968;&#20010;&#22122;&#22768;&#30452;&#26041;&#22270;&#65292;&#20197;&#27492;&#26469;&#25490;&#21517;&#26679;&#26412;&#30340;&#22122;&#22768;&#27700;&#24179;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;&#22122;&#22768;&#30452;&#26041;&#22270;&#26469;&#37492;&#21035;&#22024;&#26434;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#22122;&#22768;&#20998;&#24067;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;ReCoV&#22312;&#20998;&#31867;&#20219;&#21153;&#22522;&#20934;&#20013;&#30340;&#20248;&#36234;&#34920;&#29616;&#65292;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26631;&#31614;&#28165;&#29702;&#31639;&#27861;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;
&lt;/p&gt;
&lt;p&gt;
Label noise is prevalent in machine learning datasets. It is crucial to identify and remove label noise because models trained on noisy data can have substantially reduced accuracy and generalizability. Most existing label noise detection approaches are designed for classification tasks, and data cleaning for outcome prediction analysis is relatively unexplored. Inspired by the fluctuations in performance across different folds in cross-validation, we propose Repeated Cross-Validations for label noise estimation (ReCoV) to address this gap. ReCoV constructs a noise histogram that ranks the noise level of samples based on a large number of cross-validations by recording sample IDs in each worst-performing fold. We further propose three approaches for identifying noisy samples based on noise histograms to address increasingly complex noise distributions. We show that ReCoV outperforms state-of-the-art algorithms for label cleaning in a classification task benchmark. More importantly, we 
&lt;/p&gt;</description></item></channel></rss>