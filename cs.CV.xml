<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>RoPE&#22312;&#35270;&#35273;&#21464;&#21387;&#22120;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#22806;&#25512;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;ImageNet-1k&#12289;COCO&#26816;&#27979;&#21644;ADE-20k&#20998;&#21106;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13298</link><description>&lt;p&gt;
&#35270;&#35273;&#21464;&#21387;&#22120;&#30340;&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Rotary Position Embedding for Vision Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13298
&lt;/p&gt;
&lt;p&gt;
RoPE&#22312;&#35270;&#35273;&#21464;&#21387;&#22120;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#22806;&#25512;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;ImageNet-1k&#12289;COCO&#26816;&#27979;&#21644;ADE-20k&#20998;&#21106;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#65288;RoPE&#65289;&#22312;&#35821;&#35328;&#27169;&#22411;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;Transformer&#30340;&#38271;&#24230;&#22806;&#25512;&#12290;&#28982;&#32780;&#65292;RoPE&#23545;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#24433;&#21709;&#23578;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#65292;&#23613;&#31649;RoPE&#20284;&#20046;&#33021;&#22815;&#20687;&#35821;&#35328;&#39046;&#22495;&#19968;&#26679;&#22686;&#24378;&#35270;&#35273;&#21464;&#21387;&#22120;&#65288;ViT&#65289;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#23545;&#23558;RoPE&#24212;&#29992;&#20110;ViT&#26102;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#21033;&#29992;RoPE&#22312;2D&#35270;&#35273;&#25968;&#25454;&#19978;&#30340;&#23454;&#38469;&#23454;&#29616;&#12290;&#20998;&#26512;&#26174;&#31034;&#65292;RoPE&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#22806;&#25512;&#24615;&#33021;&#65292;&#21363;&#22312;&#25512;&#26029;&#26102;&#22312;&#22686;&#21152;&#22270;&#20687;&#20998;&#36776;&#29575;&#30340;&#21516;&#26102;&#20445;&#25345;&#31934;&#24230;&#12290;&#26368;&#32456;&#23548;&#33268;&#20102;ImageNet-1k&#12289;COCO&#26816;&#27979;&#21644;ADE-20k&#20998;&#21106;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#25105;&#20204;&#30456;&#20449;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#23558;RoPE&#24212;&#29992;&#20110;ViT&#30340;&#35814;&#23613;&#25351;&#23548;&#65292;&#25215;&#35834;&#36890;&#36807;&#26368;&#23567;&#30340;&#39069;&#22806;&#35745;&#31639;&#24320;&#38144;&#25552;&#39640;&#39592;&#24178;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#22312;&#32593;&#22336;https://&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13298v1 Announce Type: cross  Abstract: Rotary Position Embedding (RoPE) performs remarkably on language models, especially for length extrapolation of Transformers. However, the impacts of RoPE on computer vision domains have been underexplored, even though RoPE appears capable of enhancing Vision Transformer (ViT) performance in a way similar to the language domain. This study provides a comprehensive analysis of RoPE when applied to ViTs, utilizing practical implementations of RoPE for 2D vision data. The analysis reveals that RoPE demonstrates impressive extrapolation performance, i.e., maintaining precision while increasing image resolution at inference. It eventually leads to performance improvement for ImageNet-1k, COCO detection, and ADE-20k segmentation. We believe this study provides thorough guidelines to apply RoPE into ViT, promising improved backbone performance with minimal extra computational overhead. Our code and pre-trained models are available at https://
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#31526;&#21495;&#35270;&#39057;&#25628;&#32034;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#29702;&#35299;&#65292;&#24182;&#36890;&#36807;&#29366;&#24577;&#26426;&#21644;&#26102;&#38388;&#36923;&#36753;&#20844;&#24335;&#23545;&#20107;&#20214;&#30340;&#38271;&#26399;&#28436;&#21464;&#36827;&#34892;&#25512;&#29702;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#30340;&#22330;&#26223;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2403.11021</link><description>&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#35270;&#39057;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Neuro-Symbolic Video Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11021
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#31526;&#21495;&#35270;&#39057;&#25628;&#32034;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#29702;&#35299;&#65292;&#24182;&#36890;&#36807;&#29366;&#24577;&#26426;&#21644;&#26102;&#38388;&#36923;&#36753;&#20844;&#24335;&#23545;&#20107;&#20214;&#30340;&#38271;&#26399;&#28436;&#21464;&#36827;&#34892;&#25512;&#29702;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#30340;&#22330;&#26223;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#35270;&#39057;&#25968;&#25454;&#29983;&#20135;&#30340;&#31354;&#21069;&#28608;&#22686;&#38656;&#27714;&#39640;&#25928;&#30340;&#24037;&#20855;&#65292;&#20197;&#20174;&#35270;&#39057;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#24103;&#20379;&#19979;&#28216;&#20219;&#21153;&#20351;&#29992;&#12290; &#38271;&#26399;&#26102;&#38388;&#25512;&#29702;&#26159;&#24103;&#26816;&#32034;&#31995;&#32479;&#30340;&#19968;&#20010;&#20851;&#38190;&#35201;&#27714;&#12290; &#34429;&#28982; VideoLLaMA &#21644; ViCLIP &#31561;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;&#27169;&#22411;&#22312;&#30701;&#26399;&#35821;&#20041;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#23427;&#20204;&#22312;&#36328;&#24103;&#30340;&#38271;&#26399;&#25512;&#29702;&#26041;&#38754;&#21364;&#20196;&#20154;&#24778;&#35766;&#22320;&#22833;&#36133;&#12290; &#36825;&#31181;&#22833;&#36133;&#30340;&#19968;&#20010;&#20851;&#38190;&#21407;&#22240;&#26159;&#23427;&#20204;&#23558;&#36880;&#24103;&#24863;&#30693;&#21644;&#26102;&#38388;&#25512;&#29702;&#20132;&#32455;&#25104;&#21333;&#20010;&#28145;&#24230;&#32593;&#32476;&#12290; &#22240;&#27492;&#65292;&#35299;&#32806;&#20294;&#20849;&#21516;&#35774;&#35745;&#35821;&#20041;&#29702;&#35299;&#21644;&#26102;&#38388;&#25512;&#29702;&#23545;&#20110;&#39640;&#25928;&#30340;&#22330;&#26223;&#35782;&#21035;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#65292;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23545;&#21333;&#20010;&#24103;&#36827;&#34892;&#35821;&#20041;&#29702;&#35299;&#65292;&#20294;&#26377;&#25928;&#22320;&#36890;&#36807;&#20351;&#29992;&#29366;&#24577;&#26426;&#21644;&#26102;&#38388;&#36923;&#36753;&#65288;TL&#65289;&#20844;&#24335;&#23545;&#20107;&#20214;&#30340;&#38271;&#26399;&#28436;&#21464;&#36827;&#34892;&#25512;&#29702;&#65292;&#36825;&#20123;&#20844;&#24335;&#22312;&#26412;&#36136;&#19978;&#25429;&#25417;&#20102;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11021v1 Announce Type: cross  Abstract: The unprecedented surge in video data production in recent years necessitates efficient tools to extract meaningful frames from videos for downstream tasks. Long-term temporal reasoning is a key desideratum for frame retrieval systems. While state-of-the-art foundation models, like VideoLLaMA and ViCLIP, are proficient in short-term semantic understanding, they surprisingly fail at long-term reasoning across frames. A key reason for this failure is that they intertwine per-frame perception and temporal reasoning into a single deep network. Hence, decoupling but co-designing semantic understanding and temporal reasoning is essential for efficient scene identification. We propose a system that leverages vision-language models for semantic understanding of individual frames but effectively reasons about the long-term evolution of events using state machines and temporal logic (TL) formulae that inherently capture memory. Our TL-based reas
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#27010;&#24565;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#21457;&#29616;&#20849;&#20139;&#30340;&#31163;&#25955;&#27010;&#24565;&#26469;&#20943;&#36731;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#32780;&#26080;&#38656;&#20154;&#24037;&#26631;&#35760;&#23376;&#32452;&#65292;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#28040;&#38500;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.13368</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#27010;&#24565;&#21457;&#29616;&#20943;&#36731;&#34394;&#20551;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Concept Discovery Mitigates Spurious Correlations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13368
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#27010;&#24565;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#21457;&#29616;&#20849;&#20139;&#30340;&#31163;&#25955;&#27010;&#24565;&#26469;&#20943;&#36731;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#32780;&#26080;&#38656;&#20154;&#24037;&#26631;&#35760;&#23376;&#32452;&#65292;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#28040;&#38500;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#23481;&#26131;&#20135;&#29983;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#27169;&#22411;&#36890;&#24120;&#20250;&#20135;&#29983;&#33030;&#24369;&#30340;&#39044;&#27979;&#24182;&#24341;&#20837;&#24847;&#22806;&#30340;&#20559;&#35265;&#12290;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#36890;&#24120;&#28041;&#21450;&#20381;&#36182;&#20808;&#39564;&#30693;&#35782;&#21644;&#32676;&#32452;&#27880;&#37322;&#30340;&#26041;&#27861;&#65292;&#20197;&#28040;&#38500;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#32780;&#36825;&#20123;&#20449;&#24687;&#22312;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#20013;&#21487;&#33021;&#24182;&#19981;&#23481;&#26131;&#33719;&#24471;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#26080;&#30417;&#30563;&#29289;&#20307;&#20013;&#24515;&#23398;&#20064;&#19982;&#20943;&#36731;&#34394;&#20551;&#30456;&#20851;&#24615;&#20043;&#38388;&#30340;&#19968;&#31181;&#26032;&#32852;&#31995;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#26159;&#30452;&#25509;&#25512;&#26029;&#19982;&#26631;&#31614;&#20855;&#26377;&#19981;&#21516;&#30456;&#20851;&#24615;&#30340;&#23376;&#32452;&#65292;&#32780;&#26159;&#19987;&#27880;&#20110;&#21457;&#29616;&#27010;&#24565;&#65306;&#22312;&#36755;&#20837;&#26679;&#26412;&#20043;&#38388;&#20849;&#20139;&#30340;&#31163;&#25955;&#24605;&#24819;&#12290;&#20511;&#21161;&#29616;&#26377;&#30340;&#29289;&#20307;&#20013;&#24515;&#34920;&#31034;&#23398;&#20064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CoBalT&#65306;&#19968;&#31181;&#27010;&#24565;&#24179;&#34913;&#25216;&#26415;&#65292;&#26377;&#25928;&#20943;&#36731;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#32780;&#26080;&#38656;&#20154;&#31867;&#23545;&#23376;&#32452;&#36827;&#34892;&#26631;&#35760;&#12290;&#22312;&#27700;&#40479;&#12289;CelebA&#21644;ImageNet-9&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#38024;&#23545;&#23376;&#32676;&#20307;&#21464;&#21270;&#30340;&#35780;&#20272;&#34920;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13368v1 Announce Type: new  Abstract: Models prone to spurious correlations in training data often produce brittle predictions and introduce unintended biases. Addressing this challenge typically involves methods relying on prior knowledge and group annotation to remove spurious correlations, which may not be readily available in many applications. In this paper, we establish a novel connection between unsupervised object-centric learning and mitigation of spurious correlations. Instead of directly inferring sub-groups with varying correlations with labels, our approach focuses on discovering concepts: discrete ideas that are shared across input samples. Leveraging existing object-centric representation learning, we introduce CoBalT: a concept balancing technique that effectively mitigates spurious correlations without requiring human labeling of subgroups. Evaluation across the Waterbirds, CelebA and ImageNet-9 benchmark datasets for subpopulation shifts demonstrate superio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#22686;&#24378;&#30340;&#24320;&#38598;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#26469;&#35299;&#20915;&#36328;&#39046;&#22495;&#25968;&#25454;&#24046;&#24322;&#24102;&#26469;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03094</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#36890;&#36807;&#22686;&#24378;&#30340;&#24320;&#38598;&#30446;&#26631;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object Detector
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#22686;&#24378;&#30340;&#24320;&#38598;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#26469;&#35299;&#20915;&#36328;&#39046;&#22495;&#25968;&#25454;&#24046;&#24322;&#24102;&#26469;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#65288;CD-FSOD&#65289;&#30340;&#25361;&#25112;&#65292;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#20934;&#30830;&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#65292;&#29992;&#26368;&#23569;&#30340;&#26631;&#35760;&#26679;&#26412;&#26469;&#26816;&#27979;&#26032;&#39046;&#22495;&#30340;&#30446;&#26631;&#12290;&#34429;&#28982;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#24320;&#38598;&#26816;&#27979;&#22120;&#65288;&#20363;&#22914;DE-ViT&#65289;&#22312;&#24320;&#25918;&#35789;&#27719;&#30446;&#26631;&#26816;&#27979;&#21644;&#20256;&#32479;&#30340;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#33021;&#22815;&#26816;&#27979;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#27809;&#26377;&#35265;&#36807;&#30340;&#31867;&#21035;&#65292;&#25105;&#20204;&#33258;&#28982;&#20250;&#25552;&#20986;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;1&#65289;&#36825;&#31181;&#24320;&#38598;&#26816;&#27979;&#26041;&#27861;&#33021;&#21542;&#23481;&#26131;&#22320;&#25512;&#24191;&#21040;CD-FSOD&#65311;2&#65289;&#22914;&#26524;&#19981;&#33021;&#65292;&#22914;&#20309;&#22312;&#38754;&#23545;&#26174;&#33879;&#30340;&#39046;&#22495;&#24046;&#24322;&#26102;&#22686;&#24378;&#24320;&#38598;&#26041;&#27861;&#30340;&#32467;&#26524;&#65311;&#20026;&#20102;&#22238;&#31572;&#31532;&#19968;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20960;&#20010;&#34913;&#37327;&#39046;&#22495;&#24046;&#24322;&#30340;&#25351;&#26631;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#20855;&#26377;&#22810;&#26679;&#39046;&#22495;&#24230;&#37327;&#20540;&#30340;&#26032;&#30340;CD-FSOD&#22522;&#20934;&#12290;&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;&#24320;&#38598;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#22312;&#22495;&#22806;&#25968;&#25454;&#38598;&#20013;&#35266;&#23519;&#21040;&#26126;&#26174;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#36825;&#34920;&#26126;&#37319;&#29992;&#36825;&#20123;&#26041;&#27861;&#22312;CD-FSOD&#19978;&#22833;&#36133;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the challenge of cross-domain few-shot object detection (CD-FSOD), aiming to develop an accurate object detector for novel domains with minimal labeled examples. While transformer-based open-set detectors e.g., DE-ViT~\cite{zhang2023detect} have excelled in both open-vocabulary object detection and traditional few-shot object detection, detecting categories beyond those seen during training, we thus naturally raise two key questions: 1) can such open-set detection methods easily generalize to CD-FSOD? 2) If no, how to enhance the results of open-set methods when faced with significant domain gaps? To address the first question, we introduce several metrics to quantify domain variances and establish a new CD-FSOD benchmark with diverse domain metric values. Some State-Of-The-Art (SOTA) open-set object detection methods are evaluated on this benchmark, with evident performance degradation observed across out-of-domain datasets. This indicates the failure of adopting 
&lt;/p&gt;</description></item><item><title>LHRS-Bot &#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#24895;&#22320;&#29702;&#20449;&#24687;(VGI)&#22686;&#24378;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#36817;&#26399;MLLM&#22312;&#36965;&#24863;&#39046;&#22495;&#20013;&#26410;&#23545;&#22810;&#26679;&#30340;&#22320;&#29702;&#26223;&#35266;&#21644;&#29289;&#20307;&#36827;&#34892;&#20805;&#20998;&#32771;&#34385;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#23618;&#27425;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#31574;&#30053;&#21644;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;LHRS-Bot&#23637;&#29616;&#20986;&#23545;RS&#22270;&#20687;&#30340;&#28145;&#21051;&#29702;&#35299;&#20197;&#21450;&#22312;RS&#39046;&#22495;&#20869;&#36827;&#34892;&#32454;&#33268;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02544</link><description>&lt;p&gt;
LHRS-Bot&#65306;&#21033;&#29992;VGI&#22686;&#24378;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#36171;&#33021;&#36965;&#24863;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02544
&lt;/p&gt;
&lt;p&gt;
LHRS-Bot &#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#24895;&#22320;&#29702;&#20449;&#24687;(VGI)&#22686;&#24378;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#36817;&#26399;MLLM&#22312;&#36965;&#24863;&#39046;&#22495;&#20013;&#26410;&#23545;&#22810;&#26679;&#30340;&#22320;&#29702;&#26223;&#35266;&#21644;&#29289;&#20307;&#36827;&#34892;&#20805;&#20998;&#32771;&#34385;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#23618;&#27425;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#31574;&#30053;&#21644;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;LHRS-Bot&#23637;&#29616;&#20986;&#23545;RS&#22270;&#20687;&#30340;&#28145;&#21051;&#29702;&#35299;&#20197;&#21450;&#22312;RS&#39046;&#22495;&#20869;&#36827;&#34892;&#32454;&#33268;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38761;&#21629;&#24615;&#33021;&#21147;&#24320;&#21019;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#24182;&#20419;&#36827;&#20102;&#22312;&#21508;&#20010;&#19987;&#19994;&#39046;&#22495;&#30340;&#22810;&#26679;&#21270;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#36965;&#24863;&#65288;RS&#65289;&#39046;&#22495;&#20013;&#65292;&#36817;&#26399;&#30340;MLLM&#21162;&#21147;&#26410;&#33021;&#20805;&#20998;&#32771;&#34385;&#21040;&#36965;&#24863;&#22270;&#20687;&#20013;&#22810;&#26679;&#30340;&#22320;&#29702;&#26223;&#35266;&#21644;&#29289;&#20307;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;RS&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;LHRS-Align&#65292;&#20197;&#21450;&#19968;&#20010;&#20449;&#24687;&#20016;&#23500;&#30340;RS&#29305;&#23450;&#25351;&#23548;&#25968;&#25454;&#38598;LHRS-Instruct&#65292;&#21033;&#29992;&#20016;&#23500;&#30340;&#33258;&#24895;&#22320;&#29702;&#20449;&#24687;&#65288;VGI&#65289;&#21644;&#20840;&#29699;&#21487;&#29992;&#30340;RS&#22270;&#20687;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LHRS-Bot&#65292;&#19968;&#31181;&#38024;&#23545;RS&#22270;&#20687;&#29702;&#35299;&#30340;MLLM&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#23618;&#27425;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#31574;&#30053;&#21644;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;LHRS-Bot&#23637;&#29616;&#20986;&#23545;RS&#22270;&#20687;&#30340;&#28145;&#21051;&#29702;&#35299;&#20197;&#21450;&#22312;RS&#39046;&#22495;&#20869;&#36827;&#34892;&#32454;&#33268;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The revolutionary capabilities of large language models (LLMs) have paved the way for multimodal large language models (MLLMs) and fostered diverse applications across various specialized domains. In the remote sensing (RS) field, however, the diverse geographical landscapes and varied objects in RS imagery are not adequately considered in recent MLLM endeavors. To bridge this gap, we construct a large-scale RS image-text dataset, LHRS-Align, and an informative RS-specific instruction dataset, LHRS-Instruct, leveraging the extensive volunteered geographic information (VGI) and globally available RS images. Building on this foundation, we introduce LHRS-Bot, an MLLM tailored for RS image understanding through a novel multi-level vision-language alignment strategy and a curriculum learning method. Comprehensive experiments demonstrate that LHRS-Bot exhibits a profound understanding of RS images and the ability to perform nuanced reasoning within the RS domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;ConTextual&#65292;&#29992;&#20110;&#35780;&#20272;&#33021;&#22815;&#36827;&#34892;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#25991;&#26412;&#23500;&#26377;&#35270;&#35273;&#25512;&#29702;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#26368;&#22909;&#30340;&#27169;&#22411;GPT-4V&#22312;&#25277;&#35937;&#31867;&#21035;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#25972;&#20307;&#24615;&#33021;&#19978;&#20173;&#28982;&#33853;&#21518;&#20110;&#20154;&#31867;&#65292;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2401.13311</link><description>&lt;p&gt;
ConTextual: &#22312;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#35780;&#20272;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#25991;&#26412;&#23500;&#26377;&#35270;&#35273;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
ConTextual: Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models. (arXiv:2401.13311v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;ConTextual&#65292;&#29992;&#20110;&#35780;&#20272;&#33021;&#22815;&#36827;&#34892;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#25991;&#26412;&#23500;&#26377;&#35270;&#35273;&#25512;&#29702;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#26368;&#22909;&#30340;&#27169;&#22411;GPT-4V&#22312;&#25277;&#35937;&#31867;&#21035;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#25972;&#20307;&#24615;&#33021;&#19978;&#20173;&#28982;&#33853;&#21518;&#20110;&#20154;&#31867;&#65292;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#27493;&#23548;&#33268;&#20102;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#30340;&#21457;&#23637;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#28041;&#21450;&#25991;&#26412;&#21644;&#22270;&#20687;&#20869;&#23481;&#30340;&#22797;&#26434;&#20219;&#21153;&#65292;&#20363;&#22914;&#22312;&#20844;&#20849;&#22330;&#25152;&#23548;&#33322;&#22320;&#22270;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ConTextual&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;&#19987;&#38376;&#35774;&#35745;&#30340;&#25351;&#20196;&#65292;&#29992;&#20110;&#35780;&#20272;LMMs&#22312;&#25191;&#34892;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#25991;&#26412;&#23500;&#26377;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;ConTextual&#24378;&#35843;&#20102;&#22810;&#26679;&#30340;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#65288;&#20363;&#22914;&#26102;&#38388;&#38405;&#35835;&#12289;&#23548;&#33322;&#12289;&#36141;&#29289;&#31561;&#65289;&#65292;&#35201;&#27714;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#25991;&#26412;&#21644;&#35270;&#35273;&#20803;&#32032;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#26368;&#20339;&#34920;&#29616;&#30340;LMM&#65292;GPT-4V(ision)&#65292;&#19982;&#20154;&#31867;&#33021;&#21147;&#20043;&#38388;&#23384;&#22312;30.8%&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#20351;&#29992;&#20154;&#31867;&#35780;&#20272;&#25351;&#20986;&#22312;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#25991;&#26412;&#23500;&#26377;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#36824;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#34429;&#28982;GPT-4V&#22312;&#25277;&#35937;&#31867;&#21035;&#65288;&#22914;&#27169;&#22240;&#21644;&#24341;&#25991;&#35299;&#37322;&#65289;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20854;&#25972;&#20307;&#24615;&#33021;&#20173;&#28982;&#33853;&#21518;&#20110;&#20154;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in AI have led to the development of large multimodal models (LMMs) capable of processing complex tasks involving joint reasoning over text and visual content in the image (e.g., navigating maps in public places). This paper introduces ConTextual, a novel benchmark comprising instructions designed explicitly to evaluate LMMs' ability to perform context-sensitive text-rich visual reasoning. ConTextual emphasizes diverse real-world scenarios (e.g., time-reading, navigation, shopping and more) demanding a deeper understanding of the interactions between textual and visual elements. Our findings reveal a significant performance gap of 30.8% between the best-performing LMM, GPT-4V(ision), and human capabilities using human evaluation indicating substantial room for improvement in context-sensitive text-rich visual reasoning. Notably, while GPT-4V excelled in abstract categories like meme and quote interpretation, its overall performance still lagged behind humans. In add
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31283;&#23450;&#25193;&#25955;&#30340;&#28508;&#22312;&#25193;&#25955;&#26041;&#27861;&#65292;&#29992;&#20110;&#20840;&#26223;&#20998;&#21106;&#21644;&#36974;&#32617;&#20462;&#22797;&#65292;&#36890;&#36807;&#31616;&#21270;&#26550;&#26500;&#26469;&#36991;&#20813;&#22797;&#26434;&#24615;&#65292;&#23454;&#29616;&#20102;&#29983;&#25104;&#27169;&#22411;&#35299;&#38145;&#36974;&#32617;&#20462;&#22797;&#21151;&#33021;&#65292;&#20855;&#26377;&#24212;&#29992;&#20110;&#20132;&#20114;&#24335;&#20998;&#21106;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.10227</link><description>&lt;p&gt;
&#19968;&#20010;&#31616;&#21333;&#30340;&#28508;&#22312;&#25193;&#25955;&#26041;&#27861;&#24212;&#29992;&#20110;&#20840;&#26223;&#20998;&#21106;&#21644;&#36974;&#32617;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
A Simple Latent Diffusion Approach for Panoptic Segmentation and Mask Inpainting. (arXiv:2401.10227v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10227
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31283;&#23450;&#25193;&#25955;&#30340;&#28508;&#22312;&#25193;&#25955;&#26041;&#27861;&#65292;&#29992;&#20110;&#20840;&#26223;&#20998;&#21106;&#21644;&#36974;&#32617;&#20462;&#22797;&#65292;&#36890;&#36807;&#31616;&#21270;&#26550;&#26500;&#26469;&#36991;&#20813;&#22797;&#26434;&#24615;&#65292;&#23454;&#29616;&#20102;&#29983;&#25104;&#27169;&#22411;&#35299;&#38145;&#36974;&#32617;&#20462;&#22797;&#21151;&#33021;&#65292;&#20855;&#26377;&#24212;&#29992;&#20110;&#20132;&#20114;&#24335;&#20998;&#21106;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#26223;&#21644;&#23454;&#20363;&#20998;&#21106;&#32593;&#32476;&#36890;&#24120;&#36890;&#36807;&#19987;&#38376;&#30340;&#30446;&#26631;&#26816;&#27979;&#27169;&#22359;&#65292;&#22797;&#26434;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#29305;&#27530;&#30340;&#21518;&#22788;&#29702;&#27493;&#39588;&#26469;&#35757;&#32451;&#65292;&#20197;&#22788;&#29702;&#23454;&#20363;&#36974;&#32617;&#30340;&#32622;&#25442;&#19981;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Panoptic and instance segmentation networks are often trained with specialized object detection modules, complex loss functions, and ad-hoc post-processing steps to handle the permutation-invariance of the instance masks. This work builds upon Stable Diffusion and proposes a latent diffusion approach for panoptic segmentation, resulting in a simple architecture which omits these complexities. Our training process consists of two steps: (1) training a shallow autoencoder to project the segmentation masks to latent space; (2) training a diffusion model to allow image-conditioned sampling in latent space. The use of a generative model unlocks the exploration of mask completion or inpainting, which has applications in interactive segmentation. The experimental validation yields promising results for both panoptic segmentation and mask inpainting. While not setting a new state-of-the-art, our model's simplicity, generality, and mask completion capability are desirable properties.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#37325;&#26032;&#22312;PyTorch&#19978;&#23454;&#29616;&#20102;VMAF&#65292;&#19982;&#26631;&#20934;&#23454;&#29616;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;VMAF&#21333;&#20301;&#19978;&#30340;&#24046;&#24322;&#23567;&#20110;$10^{-2}$&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#20102;&#22312;&#20351;&#29992;VMAF&#20316;&#20026;&#30446;&#26631;&#20989;&#25968;&#26102;&#30340;&#26799;&#24230;&#35745;&#31639;&#65292;&#24182;&#35777;&#26126;&#20351;&#29992;&#35813;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#19981;&#20250;&#23548;&#33268;&#26799;&#24230;&#19981;&#33391;&#12290;</title><link>http://arxiv.org/abs/2310.15578</link><description>&lt;p&gt;
&#22312;PyTorch&#19978;&#37325;&#26032;&#23454;&#29616;&#30340;VMAF&#65306;&#19968;&#20123;&#23454;&#39564;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
VMAF Re-implementation on PyTorch: Some Experimental Results. (arXiv:2310.15578v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15578
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#37325;&#26032;&#22312;PyTorch&#19978;&#23454;&#29616;&#20102;VMAF&#65292;&#19982;&#26631;&#20934;&#23454;&#29616;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;VMAF&#21333;&#20301;&#19978;&#30340;&#24046;&#24322;&#23567;&#20110;$10^{-2}$&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#20102;&#22312;&#20351;&#29992;VMAF&#20316;&#20026;&#30446;&#26631;&#20989;&#25968;&#26102;&#30340;&#26799;&#24230;&#35745;&#31639;&#65292;&#24182;&#35777;&#26126;&#20351;&#29992;&#35813;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#19981;&#20250;&#23548;&#33268;&#26799;&#24230;&#19981;&#33391;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26631;&#20934;&#30340;VMAF&#23454;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;PyTorch&#26694;&#26550;&#23454;&#29616;VMAF&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;&#36825;&#20010;&#23454;&#29616;&#65292;&#19982;&#26631;&#20934;&#30340;(libvmaf)&#36827;&#34892;&#27604;&#36739;&#65292;VMAF&#21333;&#20301;&#19978;&#30340;&#24046;&#24322;&#23567;&#20110;$10^{-2}$&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20351;&#29992;VMAF&#20316;&#20026;&#30446;&#26631;&#20989;&#25968;&#26102;&#30340;&#26799;&#24230;&#35745;&#31639;&#65292;&#24182;&#35777;&#26126;&#20351;&#29992;&#35813;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#19981;&#20250;&#23548;&#33268;&#26799;&#24230;&#19981;&#33391;&#12290;
&lt;/p&gt;
&lt;p&gt;
Based on the standard VMAF implementation we propose an implementation of VMAF using PyTorch framework. For this implementation comparisons with the standard (libvmaf) show the discrepancy $\lesssim 10^{-2}$ in VMAF units. We investigate gradients computation when using VMAF as an objective function and demonstrate that training using this function does not result in ill-behaving gradients.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#31264;&#23494;&#19977;&#32500;&#24341;&#29992;&#32593;&#32476;ConcreteNet&#65292;&#21253;&#21547;&#19977;&#20010;&#26032;&#27169;&#22359;&#65292;&#26088;&#22312;&#25913;&#21892;&#20855;&#26377;&#30456;&#21516;&#35821;&#20041;&#31867;&#21035;&#24178;&#25200;&#22240;&#32032;&#30340;&#37325;&#22797;&#23454;&#20363;&#30340;&#24341;&#29992;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04561</link><description>&lt;p&gt;
&#25913;&#36827;&#31264;&#23494;&#19977;&#32500;&#35270;&#35273;&#24341;&#29992;&#30340;&#19977;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Three Ways to Improve Verbo-visual Fusion for Dense 3D Visual Grounding. (arXiv:2309.04561v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04561
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#31264;&#23494;&#19977;&#32500;&#24341;&#29992;&#32593;&#32476;ConcreteNet&#65292;&#21253;&#21547;&#19977;&#20010;&#26032;&#27169;&#22359;&#65292;&#26088;&#22312;&#25913;&#21892;&#20855;&#26377;&#30456;&#21516;&#35821;&#20041;&#31867;&#21035;&#24178;&#25200;&#22240;&#32032;&#30340;&#37325;&#22797;&#23454;&#20363;&#30340;&#24341;&#29992;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19977;&#32500;&#35270;&#35273;&#24341;&#29992;&#26159;&#25351;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#23450;&#20301;&#19977;&#32500;&#22330;&#26223;&#20013;&#34987;&#24341;&#29992;&#30340;&#29289;&#20307;&#30340;&#20219;&#21153;&#12290;&#35813;&#20219;&#21153;&#22312;&#33258;&#20027;&#23460;&#20869;&#26426;&#22120;&#20154;&#21040;AR/VR&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#24191;&#27867;&#24212;&#29992;&#12290;&#30446;&#21069;&#19968;&#31181;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#36890;&#36807;&#26816;&#27979;&#26469;&#23436;&#25104;&#19977;&#32500;&#35270;&#35273;&#24341;&#29992;&#65292;&#21363;&#36890;&#36807;&#36793;&#30028;&#26694;&#26469;&#23450;&#20301;&#12290;&#28982;&#32780;&#65292;&#22312;&#38656;&#35201;&#36827;&#34892;&#29289;&#29702;&#20132;&#20114;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#36793;&#30028;&#26694;&#19981;&#36275;&#20197;&#25551;&#36848;&#29289;&#20307;&#30340;&#20960;&#20309;&#23646;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#31264;&#23494;&#19977;&#32500;&#35270;&#35273;&#24341;&#29992;&#30340;&#38382;&#39064;&#65292;&#21363;&#22522;&#20110;&#24341;&#29992;&#30340;&#19977;&#32500;&#23454;&#20363;&#20998;&#21106;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31264;&#23494;&#19977;&#32500;&#24341;&#29992;&#32593;&#32476;ConcreteNet&#65292;&#20854;&#20013;&#21253;&#21547;&#19977;&#20010;&#29420;&#31435;&#30340;&#26032;&#27169;&#22359;&#65292;&#26088;&#22312;&#25913;&#36827;&#20855;&#26377;&#30456;&#21516;&#35821;&#20041;&#31867;&#21035;&#24178;&#25200;&#22240;&#32032;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#37325;&#22797;&#23454;&#20363;&#30340;&#24341;&#29992;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#33258;&#19979;&#32780;&#19978;&#30340;&#27880;&#24847;&#21147;&#34701;&#21512;&#27169;&#22359;&#65292;&#26088;&#22312;&#28040;&#38500;&#23454;&#20363;&#38388;&#20851;&#31995;&#32447;&#32034;&#30340;&#27495;&#20041;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#26500;&#36896;&#19968;&#20010;cont
&lt;/p&gt;
&lt;p&gt;
3D visual grounding is the task of localizing the object in a 3D scene which is referred by a description in natural language. With a wide range of applications ranging from autonomous indoor robotics to AR/VR, the task has recently risen in popularity. A common formulation to tackle 3D visual grounding is grounding-by-detection, where localization is done via bounding boxes. However, for real-life applications that require physical interactions, a bounding box insufficiently describes the geometry of an object. We therefore tackle the problem of dense 3D visual grounding, i.e. referral-based 3D instance segmentation. We propose a dense 3D grounding network ConcreteNet, featuring three novel stand-alone modules which aim to improve grounding performance for challenging repetitive instances, i.e. instances with distractors of the same semantic class. First, we introduce a bottom-up attentive fusion module that aims to disambiguate inter-instance relational cues, next we construct a cont
&lt;/p&gt;</description></item><item><title>MOCA&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#25513;&#30721;&#24335;&#22312;&#32447;&#30721;&#26412;&#20998;&#37197;&#26469;&#23454;&#29616;&#34920;&#31034;&#23398;&#20064;&#12290;&#23427;&#21516;&#26102;&#20855;&#22791;&#33391;&#22909;&#30340;&#35821;&#22659;&#25512;&#29702;&#23646;&#24615;&#21644;&#23545;&#22270;&#20687;&#25200;&#21160;&#30340;&#19981;&#21464;&#24615;&#65292;&#24182;&#22312;&#20302;&#26679;&#26412;&#35774;&#32622;&#21644;&#21508;&#31181;&#35780;&#20272;&#21327;&#35758;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#35757;&#32451;&#36895;&#24230;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#24555;3&#20493;&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2307.09361</link><description>&lt;p&gt;
MOCA: &#33258;&#30417;&#30563;&#23398;&#20064;&#36890;&#36807;&#39044;&#27979;&#25513;&#30721;&#24335;&#22312;&#32447;&#30721;&#26412;&#20998;&#37197;&#23454;&#29616;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MOCA: Self-supervised Representation Learning by Predicting Masked Online Codebook Assignments. (arXiv:2307.09361v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09361
&lt;/p&gt;
&lt;p&gt;
MOCA&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#25513;&#30721;&#24335;&#22312;&#32447;&#30721;&#26412;&#20998;&#37197;&#26469;&#23454;&#29616;&#34920;&#31034;&#23398;&#20064;&#12290;&#23427;&#21516;&#26102;&#20855;&#22791;&#33391;&#22909;&#30340;&#35821;&#22659;&#25512;&#29702;&#23646;&#24615;&#21644;&#23545;&#22270;&#20687;&#25200;&#21160;&#30340;&#19981;&#21464;&#24615;&#65292;&#24182;&#22312;&#20302;&#26679;&#26412;&#35774;&#32622;&#21644;&#21508;&#31181;&#35780;&#20272;&#21327;&#35758;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#35757;&#32451;&#36895;&#24230;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#24555;3&#20493;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#21487;&#20197;&#29992;&#20110;&#32531;&#35299;Vision Transformer&#32593;&#32476;&#23545;&#22823;&#22411;&#20840;&#27880;&#37322;&#25968;&#25454;&#38598;&#30340;&#36138;&#23146;&#38656;&#27714;&#12290;&#19981;&#21516;&#31867;&#21035;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#20102;&#20855;&#26377;&#33391;&#22909;&#35821;&#22659;&#25512;&#29702;&#23646;&#24615;&#30340;&#34920;&#31034;&#65292;&#20363;&#22914;&#20351;&#29992;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#31574;&#30053;&#65292;&#25110;&#32773;&#23545;&#22270;&#20687;&#25200;&#21160;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#34920;&#31034;&#65292;&#20363;&#22914;&#20351;&#29992;&#23545;&#27604;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#38454;&#27573;&#12289;&#29420;&#31435;&#30340;&#26041;&#27861;MOCA&#65292;&#20351;&#29992;&#22522;&#20110;&#39640;&#32423;&#29305;&#24449;&#65288;&#32780;&#19981;&#26159;&#20687;&#32032;&#32423;&#32454;&#33410;&#65289;&#23450;&#20041;&#30340;&#26032;&#22411;&#25513;&#30721;&#21644;&#39044;&#27979;&#30446;&#26631;&#26469;&#32479;&#19968;&#36825;&#20004;&#31181;&#26399;&#26395;&#30340;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20197;&#21327;&#21516;&#21644;&#35745;&#31639;&#39640;&#25928;&#30340;&#26041;&#24335;&#26377;&#25928;&#22320;&#24212;&#29992;&#36825;&#20004;&#31181;&#23398;&#20064;&#33539;&#24335;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#22312;&#20302;&#26679;&#26412;&#35774;&#32622;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#35780;&#20272;&#21327;&#35758;&#20013;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#20854;&#35757;&#32451;&#36895;&#24230;&#33267;&#23569;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#24555;3&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning can be used for mitigating the greedy needs of Vision Transformer networks for very large fully-annotated datasets. Different classes of self-supervised learning offer representations with either good contextual reasoning properties, e.g., using masked image modeling strategies, or invariance to image perturbations, e.g., with contrastive methods. In this work, we propose a single-stage and standalone method, MOCA, which unifies both desired properties using novel mask-and-predict objectives defined with high-level features (instead of pixel-level details). Moreover, we show how to effectively employ both learning paradigms in a synergistic and computation-efficient way. Doing so, we achieve new state-of-the-art results on low-shot settings and strong experimental results in various evaluation protocols with a training that is at least 3 times faster than prior methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#24471;&#21487;&#20197;&#23545;&#20854;&#36827;&#34892;&#25915;&#20987;&#24182;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#35780;&#20272;&#21327;&#35758;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#24494;&#35843;&#40065;&#26834;&#30340;&#20027;&#24178;&#65292;&#21487;&#20197;&#26377;&#38480;&#30340;&#35745;&#31639;&#20195;&#20215;&#35757;&#32451;&#23545;&#25239;&#24615;&#40065;&#26834;&#30340;&#20998;&#21106;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.12941</link><description>&lt;p&gt;
&#40065;&#26834;&#35821;&#20041;&#20998;&#21106;&#65306;&#24378;&#40065;&#26834;&#24615;&#25915;&#20987;&#21644;&#24555;&#36895;&#35757;&#32451;&#40065;&#26834;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Robust Semantic Segmentation: Strong Adversarial Attacks and Fast Training of Robust Models. (arXiv:2306.12941v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#24471;&#21487;&#20197;&#23545;&#20854;&#36827;&#34892;&#25915;&#20987;&#24182;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#35780;&#20272;&#21327;&#35758;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#24494;&#35843;&#40065;&#26834;&#30340;&#20027;&#24178;&#65292;&#21487;&#20197;&#26377;&#38480;&#30340;&#35745;&#31639;&#20195;&#20215;&#35757;&#32451;&#23545;&#25239;&#24615;&#40065;&#26834;&#30340;&#20998;&#21106;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#37327;&#30340;&#24037;&#20316;&#24050;&#32463;&#38598;&#20013;&#22312;&#35774;&#35745;&#38024;&#23545;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#19978;&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#26041;&#27861;&#23384;&#22312;&#29992;&#20110;&#25915;&#20987;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25915;&#20987;&#20998;&#21106;&#27169;&#22411;&#30340;&#20219;&#21153;&#29305;&#23450;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26368;&#32456;&#35780;&#20272;&#21327;&#35758;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#39640;&#20272;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#33267;&#20170;&#26368;&#25104;&#21151;&#30340;&#33719;&#24471;&#40065;&#26834;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26080;&#27861;&#25104;&#21151;&#24212;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#22240;&#20026;&#35201;&#23398;&#20064;&#30340;&#20219;&#21153;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#27604;&#22270;&#20687;&#20998;&#31867;&#26356;&#39640;&#30340;&#35745;&#31639;&#37327;&#12290;&#20316;&#20026;&#35299;&#20915;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#21033;&#29992;&#26368;&#36817;&#22312;&#40065;&#26834;ImageNet&#20998;&#31867;&#22120;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#21487;&#20197;&#36890;&#36807;&#24494;&#35843;&#40065;&#26834;&#30340;&#20027;&#24178;&#65292;&#20197;&#26377;&#38480;&#30340;&#35745;&#31639;&#20195;&#20215;&#35757;&#32451;&#23545;&#25239;&#24615;&#40065;&#26834;&#30340;&#20998;&#21106;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
While a large amount of work has focused on designing adversarial attacks against image classifiers, only a few methods exist to attack semantic segmentation models. We show that attacking segmentation models presents task-specific challenges, for which we propose novel solutions. Our final evaluation protocol outperforms existing methods, and shows that those can overestimate the robustness of the models. Additionally, so far adversarial training, the most successful way for obtaining robust image classifiers, could not be successfully applied to semantic segmentation. We argue that this is because the task to be learned is more challenging, and requires significantly higher computational effort than for image classification. As a remedy, we show that by taking advantage of recent advances in robust ImageNet classifiers, one can train adversarially robust segmentation models at limited computational cost by fine-tuning robust backbones.
&lt;/p&gt;</description></item><item><title>CompoDiff &#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#30340;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#25509;&#21463;&#21508;&#31181;&#26465;&#20214;&#65292;&#20855;&#26377;&#28508;&#22312;&#25193;&#25955;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312; FashionIQ &#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#38646;&#26679;&#26412;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;&#20854;&#29305;&#24449;&#20301;&#20110;&#23436;&#25972;&#30340; CLIP &#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#25152;&#26377;&#21033;&#29992; CLIP &#31354;&#38388;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.11916</link><description>&lt;p&gt;
CompoDiff: &#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#30340;&#22810;&#21151;&#33021;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion. (arXiv:2303.11916v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11916
&lt;/p&gt;
&lt;p&gt;
CompoDiff &#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#30340;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#25509;&#21463;&#21508;&#31181;&#26465;&#20214;&#65292;&#20855;&#26377;&#28508;&#22312;&#25193;&#25955;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312; FashionIQ &#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#38646;&#26679;&#26412;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;&#20854;&#29305;&#24449;&#20301;&#20110;&#23436;&#25972;&#30340; CLIP &#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#25152;&#26377;&#21033;&#29992; CLIP &#31354;&#38388;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411; CompoDiff&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#28508;&#22312;&#25193;&#25955;&#30340;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#65288;CIR&#65289;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#30001; 1800 &#19975;&#20010;&#21442;&#32771;&#22270;&#20687;&#12289;&#26465;&#20214;&#21644;&#30456;&#24212;&#30340;&#30446;&#26631;&#22270;&#20687;&#19977;&#20803;&#32452;&#32452;&#25104;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#12290;CompoDiff &#19981;&#20165;&#22312;&#20687; FashionIQ &#36825;&#26679;&#30340; CIR &#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#38646;&#26679;&#26412;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#32780;&#19988;&#36824;&#36890;&#36807;&#25509;&#25910;&#21508;&#31181;&#26465;&#20214;&#65288;&#22914;&#36127;&#25991;&#26412;&#21644;&#22270;&#20687;&#36974;&#32617;&#26465;&#20214;&#65289;&#65292;&#20351;&#24471; CIR &#26356;&#21152;&#22810;&#21151;&#33021;&#65292;&#36825;&#26159;&#29616;&#26377; CIR &#26041;&#27861;&#25152;&#19981;&#20855;&#22791;&#30340;&#12290;&#27492;&#22806;&#65292;CompoDiff &#29305;&#24449;&#20301;&#20110;&#23436;&#25972;&#30340; CLIP &#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#22240;&#27492;&#23427;&#20204;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#21033;&#29992; CLIP &#31354;&#38388;&#30340;&#25152;&#26377;&#29616;&#26377;&#27169;&#22411;&#12290;&#35757;&#32451;&#25152;&#20351;&#29992;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#39044;&#35757;&#32451;&#26435;&#37325;&#21487;&#22312; https://github.com/navervision/CompoDiff &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel diffusion-based model, CompoDiff, for solving Composed Image Retrieval (CIR) with latent diffusion and presents a newly created dataset of 18 million reference images, conditions, and corresponding target image triplets to train the model. CompoDiff not only achieves a new zero-shot state-of-the-art on a CIR benchmark such as FashionIQ but also enables a more versatile CIR by accepting various conditions, such as negative text and image mask conditions, which are unavailable with existing CIR methods. In addition, the CompoDiff features are on the intact CLIP embedding space so that they can be directly used for all existing models exploiting the CLIP space. The code and dataset used for the training, and the pre-trained weights are available at https://github.com/navervision/CompoDiff
&lt;/p&gt;</description></item></channel></rss>