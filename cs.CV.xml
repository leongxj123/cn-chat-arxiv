<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#22312;&#30740;&#31350;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#35268;&#27169;&#29305;&#24615;&#26102;&#21457;&#29616;&#65292;&#36739;&#23567;&#30340;&#27169;&#22411;&#22312;&#30456;&#21516;&#25512;&#29702;&#39044;&#31639;&#19979;&#24448;&#24448;&#27604;&#36739;&#22823;&#30340;&#27169;&#22411;&#26356;&#26377;&#25928;&#22320;&#29983;&#25104;&#39640;&#36136;&#37327;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.01367</link><description>&lt;p&gt;
&#22823;&#24182;&#38750;&#24635;&#26159;&#26356;&#22909;&#65306;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#35268;&#27169;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Bigger is not Always Better: Scaling Properties of Latent Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01367
&lt;/p&gt;
&lt;p&gt;
&#22312;&#30740;&#31350;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#35268;&#27169;&#29305;&#24615;&#26102;&#21457;&#29616;&#65292;&#36739;&#23567;&#30340;&#27169;&#22411;&#22312;&#30456;&#21516;&#25512;&#29702;&#39044;&#31639;&#19979;&#24448;&#24448;&#27604;&#36739;&#22823;&#30340;&#27169;&#22411;&#26356;&#26377;&#25928;&#22320;&#29983;&#25104;&#39640;&#36136;&#37327;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65288;LDMs&#65289;&#30340;&#35268;&#27169;&#29305;&#24615;&#65292;&#37325;&#28857;&#20851;&#27880;&#23427;&#20204;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#23613;&#31649;&#25913;&#36827;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#25512;&#29702;&#31639;&#27861;&#24050;&#32463;&#35777;&#26126;&#21487;&#20197;&#26377;&#25928;&#25552;&#21319;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#25928;&#29575;&#65292;&#20294;&#27169;&#22411;&#22823;&#23567;&#30340;&#20316;&#29992;&#8212;&#8212;&#37319;&#26679;&#25928;&#29575;&#30340;&#20851;&#38190;&#20915;&#23450;&#22240;&#32032;&#8212;&#8212;&#23578;&#26410;&#21463;&#21040;&#24443;&#24213;&#30340;&#23457;&#26597;&#12290;&#36890;&#36807;&#23545;&#24050;&#24314;&#31435;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#27169;&#22411;&#22823;&#23567;&#22914;&#20309;&#24433;&#21709;&#22312;&#19981;&#21516;&#37319;&#26679;&#27493;&#39588;&#19979;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#36235;&#21183;&#65306;&#22312;&#32473;&#23450;&#25512;&#29702;&#39044;&#31639;&#19979;&#36816;&#34892;&#26102;&#65292;&#36739;&#23567;&#30340;&#27169;&#22411;&#32463;&#24120;&#32988;&#36807;&#20854;&#36739;&#22823;&#30340;&#31561;&#20215;&#29289;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#32467;&#26524;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#30740;&#31350;&#65292;&#36890;&#36807;&#24212;&#29992;&#21508;&#31181;&#25193;&#25955;&#37319;&#26679;&#22120;&#65292;&#25506;&#32034;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#35780;&#20272;&#21518;&#31934;&#39311;&#27169;&#22411;&#65292;&#20197;&#21450;&#36827;&#34892;&#27604;&#36739;&#65292;&#26469;&#23637;&#31034;&#36825;&#20123;&#21457;&#29616;&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01367v1 Announce Type: cross  Abstract: We study the scaling properties of latent diffusion models (LDMs) with an emphasis on their sampling efficiency. While improved network architecture and inference algorithms have shown to effectively boost sampling efficiency of diffusion models, the role of model size -- a critical determinant of sampling efficiency -- has not been thoroughly examined. Through empirical analysis of established text-to-image diffusion models, we conduct an in-depth investigation into how model size influences sampling efficiency across varying sampling steps. Our findings unveil a surprising trend: when operating under a given inference budget, smaller models frequently outperform their larger equivalents in generating high-quality results. Moreover, we extend our study to demonstrate the generalizability of the these findings by applying various diffusion samplers, exploring diverse downstream tasks, evaluating post-distilled models, as well as compar
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#30340;&#19981;&#21487;&#23398;&#20064;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#20351;&#24471;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#37117;&#22833;&#25928;&#65292;&#25552;&#20986;&#36890;&#36807;&#22686;&#21152;&#29305;&#23450;&#24230;&#37327;&#26469;&#20943;&#36731;&#19981;&#21487;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.18403</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#20687;&#32032;&#30340;&#22270;&#20687;&#36716;&#25442;&#30772;&#22351;&#22522;&#20110;&#21367;&#31215;&#30340;&#19981;&#21487;&#23398;&#20064;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Corrupting Convolution-based Unlearnable Datasets with Pixel-based Image Transformations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18403
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#30340;&#19981;&#21487;&#23398;&#20064;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#20351;&#24471;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#37117;&#22833;&#25928;&#65292;&#25552;&#20986;&#36890;&#36807;&#22686;&#21152;&#29305;&#23450;&#24230;&#37327;&#26469;&#20943;&#36731;&#19981;&#21487;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21487;&#23398;&#20064;&#30340;&#25968;&#25454;&#38598;&#20250;&#36890;&#36807;&#21521;&#24178;&#20928;&#35757;&#32451;&#38598;&#24341;&#20837;&#31934;&#24515;&#35774;&#35745;&#19988;&#38590;&#20197;&#23519;&#35273;&#30340;&#25200;&#21160;&#65292;&#23548;&#33268;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#24613;&#21095;&#19979;&#38477;&#12290;&#35768;&#22810;&#29616;&#26377;&#38450;&#24481;&#26041;&#27861;&#65292;&#22914;JPEG&#21387;&#32553;&#21644;&#23545;&#25239;&#35757;&#32451;&#65292;&#33021;&#22815;&#26377;&#25928;&#23545;&#25239;&#22522;&#20110;&#33539;&#25968;&#32422;&#26463;&#30340;&#38468;&#21152;&#22122;&#22768;&#30340;&#19981;&#21487;&#23398;&#20064;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#26368;&#26032;&#25552;&#20986;&#30340;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#30340;&#19981;&#21487;&#23398;&#20064;&#25968;&#25454;&#38598;&#35753;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#26080;&#25928;&#65292;&#32473;&#38450;&#24481;&#32773;&#24102;&#26469;&#26356;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#31616;&#21270;&#30340;&#24773;&#26223;&#20013;&#23558;&#22522;&#20110;&#21367;&#31215;&#30340;&#19981;&#21487;&#23398;&#20064;&#26679;&#26412;&#34920;&#36798;&#20026;&#23558;&#30697;&#38453;&#20056;&#20197;&#24178;&#20928;&#26679;&#26412;&#30340;&#32467;&#26524;&#65292;&#24182;&#23558;&#31867;&#20869;&#30697;&#38453;&#19981;&#19968;&#33268;&#24615;&#24418;&#24335;&#21270;&#20026;$\Theta_{imi}$&#65292;&#23558;&#31867;&#38388;&#30697;&#38453;&#19968;&#33268;&#24615;&#24418;&#24335;&#21270;&#20026;$\Theta_{imc}$&#20197;&#30740;&#31350;&#22522;&#20110;&#21367;&#31215;&#30340;&#19981;&#21487;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#24037;&#20316;&#26426;&#21046;&#12290;&#25105;&#20204;&#25512;&#27979;&#22686;&#21152;&#36825;&#20004;&#20010;&#24230;&#37327;&#23558;&#26377;&#21161;&#20110;&#20943;&#36731;&#19981;&#21487;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.18403v2 Announce Type: replace-cross  Abstract: Unlearnable datasets lead to a drastic drop in the generalization performance of models trained on them by introducing elaborate and imperceptible perturbations into clean training sets. Many existing defenses, e.g., JPEG compression and adversarial training, effectively counter UDs based on norm-constrained additive noise. However, a fire-new type of convolution-based UDs have been proposed and render existing defenses all ineffective, presenting a greater challenge to defenders. To address this, we express the convolution-based unlearnable sample as the result of multiplying a matrix by a clean sample in a simplified scenario, and formalize the intra-class matrix inconsistency as $\Theta_{imi}$, inter-class matrix consistency as $\Theta_{imc}$ to investigate the working mechanism of the convolution-based UDs. We conjecture that increasing both of these metrics will mitigate the unlearnability effect. Through validation experi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29305;&#24449;&#35299;&#32544;&#26469;&#32531;&#35299;&#23545;&#25239;&#40065;&#26834;&#24615;&#20013;&#29305;&#24449;&#24046;&#36317;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26126;&#30830;&#24314;&#27169;&#21644;&#28040;&#38500;&#23548;&#33268;&#29305;&#24449;&#24046;&#36317;&#30340;&#28508;&#22312;&#29305;&#24449;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.14707</link><description>&lt;p&gt;
&#36890;&#36807;&#29305;&#24449;&#35299;&#32544;&#26469;&#32531;&#35299;&#23545;&#25239;&#40065;&#26834;&#24615;&#20013;&#30340;&#29305;&#24449;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Mitigating Feature Gap for Adversarial Robustness by Feature Disentanglement. (arXiv:2401.14707v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14707
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29305;&#24449;&#35299;&#32544;&#26469;&#32531;&#35299;&#23545;&#25239;&#40065;&#26834;&#24615;&#20013;&#29305;&#24449;&#24046;&#36317;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26126;&#30830;&#24314;&#27169;&#21644;&#28040;&#38500;&#23548;&#33268;&#29305;&#24449;&#24046;&#36317;&#30340;&#28508;&#22312;&#29305;&#24449;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#23545;&#25239;&#26679;&#26412;&#24456;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#23545;&#25239;&#24494;&#35843;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#23545;&#24050;&#32463;&#22312;&#33258;&#28982;&#24773;&#20917;&#19979;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#24335;&#24494;&#35843;&#26469;&#25552;&#21319;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#23545;&#25239;&#26679;&#26412;&#20013;&#30340;&#19968;&#20123;&#28508;&#22312;&#29305;&#24449;&#34987;&#23545;&#25239;&#25200;&#21160;&#25152;&#28151;&#28102;&#65292;&#24182;&#23548;&#33268;&#33258;&#28982;&#26679;&#26412;&#21644;&#23545;&#25239;&#26679;&#26412;&#22312;&#26368;&#21518;&#19968;&#23618;&#38544;&#34255;&#23618;&#30340;&#29305;&#24449;&#20043;&#38388;&#20986;&#29616;&#24847;&#22806;&#22686;&#21152;&#30340;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#32544;&#30340;&#26041;&#27861;&#26469;&#26126;&#30830;&#24314;&#27169;&#21644;&#36827;&#19968;&#27493;&#28040;&#38500;&#23548;&#33268;&#29305;&#24449;&#24046;&#36317;&#30340;&#28508;&#22312;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29305;&#24449;&#35299;&#32544;&#22120;&#65292;&#23558;&#23545;&#25239;&#26679;&#26412;&#30340;&#28508;&#22312;&#29305;&#24449;&#19982;&#23545;&#25239;&#26679;&#26412;&#30340;&#29305;&#24449;&#20998;&#31163;&#24320;&#26469;&#65292;&#20174;&#32780;&#36890;&#36807;&#28040;&#38500;&#28508;&#22312;&#29305;&#24449;&#26469;&#25552;&#21319;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#29305;&#24449;&#19982;&#23545;&#25239;&#26679;&#26412;&#22312;&#24494;&#35843;&#27169;&#22411;&#20013;&#30340;&#29305;&#24449;&#23545;&#40784;&#65292;&#36827;&#19968;&#27493;&#20174;&#33258;&#28982;&#26679;&#26412;&#30340;&#29305;&#24449;&#20013;&#33719;&#30410;&#65292;&#36991;&#20813;&#28151;&#28102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are vulnerable to adversarial samples. Adversarial fine-tuning methods aim to enhance adversarial robustness through fine-tuning the naturally pre-trained model in an adversarial training manner. However, we identify that some latent features of adversarial samples are confused by adversarial perturbation and lead to an unexpectedly increasing gap between features in the last hidden layer of natural and adversarial samples. To address this issue, we propose a disentanglement-based approach to explicitly model and further remove the latent features that cause the feature gap. Specifically, we introduce a feature disentangler to separate out the latent features from the features of the adversarial samples, thereby boosting robustness by eliminating the latent features. Besides, we align features in the pre-trained model with features of adversarial samples in the fine-tuned model, to further benefit from the features from natural samples without confusion. Empirical 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#20998;&#24067;&#22806;&#26816;&#27979;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#22312;&#26816;&#27979;&#26410;&#30693;&#31867;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#36935;&#21040;&#20854;&#20182;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#26102;&#24615;&#33021;&#19981;&#31283;&#23450;&#12290;</title><link>http://arxiv.org/abs/2308.11480</link><description>&lt;p&gt;
&#23545;&#24191;&#27867;&#30340;&#20998;&#24067;&#22806;&#26816;&#27979;&#30340;&#26399;&#26395;&#65306;&#26399;&#26395;&#20043;&#22806;&#30340;&#26410;&#30693;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Expecting The Unexpected: Towards Broad Out-Of-Distribution Detection. (arXiv:2308.11480v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11480
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#20998;&#24067;&#22806;&#26816;&#27979;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#22312;&#26816;&#27979;&#26410;&#30693;&#31867;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#36935;&#21040;&#20854;&#20182;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#26102;&#24615;&#33021;&#19981;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#39640;&#37096;&#32626;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#36890;&#24120;&#28041;&#21450;&#24320;&#21457;&#26041;&#27861;&#26469;&#26816;&#27979;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#30340;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#24120;&#24120;&#29421;&#31364;&#22320;&#20851;&#27880;&#35757;&#32451;&#38598;&#20013;&#32570;&#22833;&#30340;&#31867;&#21035;&#26679;&#26412;&#65292;&#24573;&#30053;&#20102;&#20854;&#20182;&#31867;&#22411;&#30340;&#21487;&#33021;&#20998;&#24067;&#21464;&#21270;&#12290;&#36825;&#31181;&#38480;&#21046;&#38477;&#20302;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#22240;&#20026;&#31995;&#32479;&#20250;&#36935;&#21040;&#21508;&#31181;&#21508;&#26679;&#30340;&#24322;&#24120;&#36755;&#20837;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#20116;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#23545;&#26368;&#36817;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#22312;&#27599;&#19968;&#31181;&#20998;&#24067;&#21464;&#21270;&#19978;&#36827;&#34892;&#20102;&#20851;&#38190;&#35780;&#20272;&#12290;&#25105;&#20204;&#20197;BROAD&#65288;Benchmarking Resilience Over Anomaly Diversity&#65289;&#30340;&#21517;&#20041;&#20844;&#24320;&#21457;&#24067;&#25105;&#20204;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#22312;&#26816;&#27979;&#26410;&#30693;&#31867;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#36935;&#21040;&#20854;&#20182;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#26102;&#24615;&#33021;&#19981;&#19968;&#33268;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#23427;&#20204;&#21482;&#33021;&#21487;&#38752;&#22320;&#26816;&#27979;&#21040;&#23427;&#20204;&#29305;&#21035;&#35774;&#35745;&#26469;&#39044;&#26399;&#30340;&#24847;&#22806;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving the reliability of deployed machine learning systems often involves developing methods to detect out-of-distribution (OOD) inputs. However, existing research often narrowly focuses on samples from classes that are absent from the training set, neglecting other types of plausible distribution shifts. This limitation reduces the applicability of these methods in real-world scenarios, where systems encounter a wide variety of anomalous inputs. In this study, we categorize five distinct types of distribution shifts and critically evaluate the performance of recent OOD detection methods on each of them. We publicly release our benchmark under the name BROAD (Benchmarking Resilience Over Anomaly Diversity). Our findings reveal that while these methods excel in detecting unknown classes, their performance is inconsistent when encountering other types of distribution shifts. In other words, they only reliably detect unexpected inputs that they have been specifically designed to expec
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MACO&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#30456;&#20301;&#35889;&#29983;&#25104;&#22270;&#20687;&#65292;&#21516;&#26102;&#20445;&#25345;&#24133;&#24230;&#24658;&#23450;&#65292;&#35299;&#20915;&#20102;&#29305;&#24449;&#21487;&#35270;&#21270;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#21644;&#39640;&#25928;&#30340;&#21487;&#35299;&#37322;&#29305;&#24449;&#21487;&#35270;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.06805</link><description>&lt;p&gt;
&#29992;&#24133;&#24230;&#21463;&#38480;&#21046;&#20248;&#21270;&#35299;&#38145;&#26356;&#28145;&#23618;&#32593;&#32476;&#30340;&#29305;&#24449;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Unlocking Feature Visualization for Deeper Networks with MAgnitude Constrained Optimization. (arXiv:2306.06805v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06805
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MACO&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#30456;&#20301;&#35889;&#29983;&#25104;&#22270;&#20687;&#65292;&#21516;&#26102;&#20445;&#25345;&#24133;&#24230;&#24658;&#23450;&#65292;&#35299;&#20915;&#20102;&#29305;&#24449;&#21487;&#35270;&#21270;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#21644;&#39640;&#25928;&#30340;&#21487;&#35299;&#37322;&#29305;&#24449;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#21487;&#35270;&#21270;&#22312;Olah&#31561;&#20154;2017&#24180;&#30340;&#26377;&#24433;&#21709;&#21147;&#30340;&#24037;&#20316;&#20043;&#21518;&#33719;&#24471;&#20102;&#24456;&#22823;&#30340; popularity&#65292;&#23558;&#20854;&#30830;&#31435;&#20026;&#21487;&#35299;&#37322;&#24615;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20381;&#36182;&#20110;&#29983;&#25104;&#21487;&#35299;&#37322;&#22270;&#20687;&#30340;&#25216;&#24039;&#20197;&#21450;&#22312;&#23558;&#20854;&#25193;&#23637;&#21040;&#26356;&#28145;&#30340;&#31070;&#32463;&#32593;&#32476;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20854;&#24191;&#27867;&#24212;&#29992;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;MACO&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#36890;&#36807;&#20248;&#21270;&#30456;&#20301;&#35889;&#29983;&#25104;&#22270;&#20687;&#65292;&#21516;&#26102;&#20445;&#25345;&#24133;&#24230;&#24658;&#23450;&#65292;&#20197;&#30830;&#20445;&#29983;&#25104;&#30340;&#35299;&#37322;&#20301;&#20110;&#33258;&#28982;&#22270;&#20687;&#31354;&#38388;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23450;&#24615;&#21644;&#23450;&#37327;&#19978;&#37117;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#65292;&#24182;&#20026;&#22823;&#22411;&#26368;&#20808;&#36827;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#21487;&#35270;&#21270;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#19968;&#20010;&#23646;&#24615;&#26426;&#21046;&#65292;&#21487;&#20197;&#22686;&#24378;&#29305;&#24449;&#21487;&#35270;&#21270;&#30340;&#31354;&#38388;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature visualization has gained substantial popularity, particularly after the influential work by Olah et al. in 2017, which established it as a crucial tool for explainability. However, its widespread adoption has been limited due to a reliance on tricks to generate interpretable images, and corresponding challenges in scaling it to deeper neural networks. Here, we describe MACO, a simple approach to address these shortcomings. The main idea is to generate images by optimizing the phase spectrum while keeping the magnitude constant to ensure that generated explanations lie in the space of natural images. Our approach yields significantly better results (both qualitatively and quantitatively) and unlocks efficient and interpretable feature visualizations for large state-of-the-art neural networks. We also show that our approach exhibits an attribution mechanism allowing us to augment feature visualizations with spatial importance. We validate our method on a novel benchmark for compa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#32852;&#21512;&#24314;&#27169;&#21333;&#27169;&#24577;VAE&#30340;&#28508;&#22312;&#31354;&#38388;&#23454;&#29616;&#20102;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#19968;&#33268;&#24615;&#25972;&#21512;&#65292;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;VAE&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15708</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#22810;&#27169;&#24577;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Score-Based Multimodal Autoencoders. (arXiv:2305.15708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#32852;&#21512;&#24314;&#27169;&#21333;&#27169;&#24577;VAE&#30340;&#28508;&#22312;&#31354;&#38388;&#23454;&#29616;&#20102;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#19968;&#33268;&#24615;&#25972;&#21512;&#65292;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;VAE&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26159;&#19968;&#31867;&#33021;&#22815;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#26500;&#24314;&#21487;&#22788;&#29702;&#21518;&#39564;&#30340;&#26377;&#21069;&#36884;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22810;&#31181;&#27169;&#24577;&#30340;&#25968;&#25454;&#12290;&#20294;&#38543;&#30528;&#27169;&#24577;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#27599;&#19968;&#20010;&#27169;&#24577;&#30340;&#29983;&#25104;&#36136;&#37327;&#37117;&#20250;&#38477;&#20302;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#20998;&#25968;&#30340;&#27169;&#22411;&#32852;&#21512;&#24314;&#27169;&#21333;&#27169;&#24577;VAE&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#20197;&#22686;&#24378;&#22810;&#27169;&#24577;VAE&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;&#20998;&#25968;&#27169;&#22411;&#30340;&#20316;&#29992;&#26159;&#36890;&#36807;&#23398;&#20064;&#28508;&#22312;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#23454;&#29616;&#22810;&#27169;&#24577;&#30340;&#19968;&#33268;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#32467;&#21512;&#20102;&#21333;&#27169;&#24577;VAE&#21331;&#36234;&#30340;&#29983;&#25104;&#36136;&#37327;&#21644;&#23545;&#19981;&#21516;&#27169;&#24577;&#30340;&#19968;&#33268;&#24615;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Variational Autoencoders (VAEs) represent a promising group of generative models that facilitate the construction of a tractable posterior within the latent space, given multiple modalities. Daunhawer et al. (2022) demonstrate that as the number of modalities increases, the generative quality of each modality declines. In this study, we explore an alternative approach to enhance the generative performance of multimodal VAEs by jointly modeling the latent space of unimodal VAEs using score-based models (SBMs). The role of the SBM is to enforce multimodal coherence by learning the correlation among the latent variables. Consequently, our model combines the superior generative quality of unimodal VAEs with coherent integration across different modalities.
&lt;/p&gt;</description></item><item><title>ERM++&#26159;&#19968;&#20010;&#29992;&#20110;&#22495;&#36890;&#29992;&#24615;&#30340;&#25913;&#36827;&#22522;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#22909;&#22320;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#12289;&#27169;&#22411;&#21442;&#25968;&#36873;&#25321;&#21644;&#26435;&#37325;&#31354;&#38388;&#27491;&#21017;&#21270;&#31561;&#20851;&#38190;&#25216;&#26415;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#27604;&#26631;&#20934;ERM&#26356;&#26377;&#25928;&#65292;&#21516;&#26102;&#35745;&#31639;&#22797;&#26434;&#24230;&#26356;&#20302;&#65292;&#34920;&#29616;&#20063;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.01973</link><description>&lt;p&gt;
ERM++&#65306;&#29992;&#20110;&#22495;&#36890;&#29992;&#24615;&#30340;&#25913;&#36827;&#22522;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ERM++: An Improved Baseline for Domain Generalization. (arXiv:2304.01973v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01973
&lt;/p&gt;
&lt;p&gt;
ERM++&#26159;&#19968;&#20010;&#29992;&#20110;&#22495;&#36890;&#29992;&#24615;&#30340;&#25913;&#36827;&#22522;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#22909;&#22320;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#12289;&#27169;&#22411;&#21442;&#25968;&#36873;&#25321;&#21644;&#26435;&#37325;&#31354;&#38388;&#27491;&#21017;&#21270;&#31561;&#20851;&#38190;&#25216;&#26415;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#27604;&#26631;&#20934;ERM&#26356;&#26377;&#25928;&#65292;&#21516;&#26102;&#35745;&#31639;&#22797;&#26434;&#24230;&#26356;&#20302;&#65292;&#34920;&#29616;&#20063;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#28304;&#22495;&#36890;&#29992;&#24615;&#65288;DG&#65289;&#34913;&#37327;&#20998;&#31867;&#22120;&#23545;&#20110;&#23427;&#27809;&#26377;&#25509;&#21463;&#36807;&#35757;&#32451;&#30340;&#26032;&#25968;&#25454;&#20998;&#24067;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#32771;&#34385;&#20102;&#22810;&#20010;&#35757;&#32451;&#22495;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#22810;&#28304;DG&#26041;&#27861;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#22495;&#26631;&#31614;&#22686;&#21152;&#20102;&#39069;&#22806;&#30340;&#22797;&#26434;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#32463;&#36807;&#33391;&#22909;&#35843;&#25972;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#35757;&#32451;&#36807;&#31243;&#65292;&#21363;&#22312;&#28304;&#22495;&#19978;&#31616;&#21333;&#22320;&#26368;&#23567;&#21270;&#32463;&#39564;&#39118;&#38505;&#65292;&#21487;&#20197;&#32988;&#36807;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;DG&#26041;&#27861;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#20960;&#20010;&#20851;&#38190;&#20505;&#36873;&#25216;&#26415;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;ERM&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#26356;&#22909;&#22320;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#12289;&#27169;&#22411;&#21442;&#25968;&#36873;&#25321;&#21644;&#26435;&#37325;&#31354;&#38388;&#27491;&#21017;&#21270;&#12290;&#25105;&#20204;&#23558;&#32467;&#26524;&#31216;&#20026;ERM ++&#65292;&#24182;&#23637;&#31034;&#23427;&#30456;&#23545;&#20110;&#26631;&#20934;ERM&#22312;&#20116;&#20010;&#22810;&#28304;&#25968;&#25454;&#38598;&#19978;&#23558;DG&#30340;&#24615;&#33021;&#26174;&#30528;&#25552;&#39640;&#20102;5&#65285;&#20197;&#19978;&#65292;&#24182;&#19988;&#23613;&#31649;&#35745;&#31639;&#22797;&#26434;&#24230;&#26356;&#20302;&#65292;&#20294;&#20987;&#36133;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;ERM ++&#22312;WILDS-FMOW&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-source Domain Generalization (DG) measures a classifier's ability to generalize to new distributions of data it was not trained on, given several training domains. While several multi-source DG methods have been proposed, they incur additional complexity during training by using domain labels. Recent work has shown that a well-tuned Empirical Risk Minimization (ERM) training procedure, that is simply minimizing the empirical risk on the source domains, can outperform most existing DG methods. We identify several key candidate techniques to further improve ERM performance, such as better utilization of training data, model parameter selection, and weight-space regularization. We call the resulting method ERM++, and show it significantly improves the performance of DG on five multi-source datasets by over 5% compared to standard ERM, and beats state-of-the-art despite being less computationally expensive. Additionally, we demonstrate the efficacy of ERM++ on the WILDS-FMOW dataset,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#38750;&#30417;&#30563;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#20013;&#22256;&#25200;UISS&#27169;&#22411;&#30340;&#29305;&#24449;&#23545;&#40784;&#21644;&#29305;&#24449;&#22343;&#21248;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;Semantic Attention Network(SAN) &#27169;&#22411;&#65292;&#21253;&#21547;&#19968;&#20010;&#26032;&#27169;&#22359; semantic attention&#65288;SEAT&#65289;&#65292;&#20197;&#21160;&#24577;&#29983;&#25104;&#36880;&#20687;&#32032;&#21644;&#35821;&#20041;&#29305;&#24449;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#19968;&#38750;&#30417;&#30563;&#20998;&#21106;&#26694;&#26550;&#19987;&#27880;&#20110;&#25429;&#25417;&#35821;&#20041;&#34920;&#31034;&#65292;&#22312;&#22810;&#20010;&#35821;&#20041;&#20998;&#21106;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2211.14513</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#38750;&#30417;&#30563;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#23545;&#40784;&#21644;&#22343;&#21248;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Rethinking Alignment and Uniformity in Unsupervised Image Semantic Segmentation. (arXiv:2211.14513v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#38750;&#30417;&#30563;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#20013;&#22256;&#25200;UISS&#27169;&#22411;&#30340;&#29305;&#24449;&#23545;&#40784;&#21644;&#29305;&#24449;&#22343;&#21248;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;Semantic Attention Network(SAN) &#27169;&#22411;&#65292;&#21253;&#21547;&#19968;&#20010;&#26032;&#27169;&#22359; semantic attention&#65288;SEAT&#65289;&#65292;&#20197;&#21160;&#24577;&#29983;&#25104;&#36880;&#20687;&#32032;&#21644;&#35821;&#20041;&#29305;&#24449;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#19968;&#38750;&#30417;&#30563;&#20998;&#21106;&#26694;&#26550;&#19987;&#27880;&#20110;&#25429;&#25417;&#35821;&#20041;&#34920;&#31034;&#65292;&#22312;&#22810;&#20010;&#35821;&#20041;&#20998;&#21106;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#30417;&#30563;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;(UISS)&#26088;&#22312;&#23558;&#20302;&#23618;&#35270;&#35273;&#29305;&#24449;&#19982;&#35821;&#20041;&#32423;&#21035;&#30340;&#34920;&#31034;&#21305;&#37197;&#65292;&#32780;&#26080;&#38656;&#22806;&#37096;&#30417;&#31649;&#12290;&#26412;&#25991;&#20174;&#29305;&#24449;&#23545;&#40784;&#21644;&#29305;&#24449;&#22343;&#21248;&#24615;&#30340;&#35282;&#24230;&#25506;&#31350;&#20102;UISS&#27169;&#22411;&#30340;&#20851;&#38190;&#24615;&#36136;&#65292;&#24182;&#23558;UISS&#19982;&#25972;&#24133;&#22270;&#20687;&#30340;&#34920;&#31034;&#23398;&#20064;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#22522;&#20110;&#20998;&#26512;&#65292;&#25105;&#20204;&#35748;&#20026;UISS&#20013;&#29616;&#26377;&#30340;&#22522;&#20110;&#20114;&#20449;&#24687;&#30340;&#26041;&#27861;&#23384;&#22312;&#34920;&#31034;&#23849;&#28291;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#32593;&#32476;&#27169;&#22411;&#8212;&#8212;Semantic Attention Network(SAN)&#65292;&#20854;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#27169;&#22359;Semantic Attention(SEAT)&#65292;&#20197;&#21160;&#24577;&#29983;&#25104;&#36880;&#20687;&#32032;&#21644;&#35821;&#20041;&#29305;&#24449;&#12290;&#22312;&#22810;&#20010;&#35821;&#20041;&#20998;&#21106;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#38750;&#30417;&#30563;&#20998;&#21106;&#26694;&#26550;&#19987;&#27880;&#20110;&#25429;&#25417;&#35821;&#20041;&#34920;&#31034;&#65292;&#34920;&#29616;&#20248;&#24322;&#65292;&#36229;&#36807;&#20102;&#25152;&#26377;&#26410;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#19968;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised image semantic segmentation(UISS) aims to match low-level visual features with semantic-level representations without outer supervision. In this paper, we address the critical properties from the view of feature alignments and feature uniformity for UISS models. We also make a comparison between UISS and image-wise representation learning. Based on the analysis, we argue that the existing MI-based methods in UISS suffer from representation collapse. By this, we proposed a robust network called Semantic Attention Network(SAN), in which a new module Semantic Attention(SEAT) is proposed to generate pixel-wise and semantic features dynamically. Experimental results on multiple semantic segmentation benchmarks show that our unsupervised segmentation framework specializes in catching semantic representations, which outperforms all the unpretrained and even several pretrained methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#24102;&#26377;&#23545;&#27604;&#23398;&#20064;&#30340;&#20840;&#33258;&#20027;&#20998;&#24067;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#26816;&#27979;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#21644;&#23545;&#25239;&#24615;&#25200;&#21160;&#26679;&#26412;&#12290;&#36890;&#36807;&#23558;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#19982;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;CADet&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#21516;&#19968;&#26679;&#26412;&#30340;&#23545;&#27604;&#21464;&#25442;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#36827;&#34892;OOD&#26816;&#27979;&#65292;&#24182;&#22312;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#35782;&#21035;&#26041;&#38754;&#27604;&#29616;&#26377;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2210.01742</link><description>&lt;p&gt;
CADet:&#20840;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#24102;&#26377;&#23545;&#27604;&#23398;&#20064;&#30340;&#20840;&#33258;&#20027;&#20998;&#24067;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
CADet: Fully Self-Supervised Out-Of-Distribution Detection With Contrastive Learning. (arXiv:2210.01742v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#24102;&#26377;&#23545;&#27604;&#23398;&#20064;&#30340;&#20840;&#33258;&#20027;&#20998;&#24067;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#26816;&#27979;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#21644;&#23545;&#25239;&#24615;&#25200;&#21160;&#26679;&#26412;&#12290;&#36890;&#36807;&#23558;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#19982;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;CADet&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#21516;&#19968;&#26679;&#26412;&#30340;&#23545;&#27604;&#21464;&#25442;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#36827;&#34892;OOD&#26816;&#27979;&#65292;&#24182;&#22312;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#35782;&#21035;&#26041;&#38754;&#27604;&#29616;&#26377;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#24102;&#26377;&#20998;&#24067;&#20043;&#22806;&#65288;OOD&#65289;&#26679;&#26412;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#29616;&#23454;&#19990;&#30028;&#37096;&#32626;&#20013;&#30340;&#20027;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#21516;&#26102;&#26816;&#27979;&#20004;&#31181;&#31867;&#22411;&#30340;OOD&#26679;&#26412;&#65306;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#21644;&#23545;&#25239;&#24615;&#25200;&#21160;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#19982;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#21452;&#26679;&#26412;&#26816;&#39564;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#40065;&#26834;&#22320;&#27979;&#35797;&#20004;&#20010;&#29420;&#31435;&#26679;&#26412;&#38598;&#26159;&#21542;&#26469;&#33258;&#30456;&#21516;&#30340;&#20998;&#24067;&#65292;&#24182;&#19988;&#25105;&#20204;&#35777;&#26126;&#20102;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#65292;&#23427;&#22312;&#21306;&#20998;CIFAR-10&#21644;CIFAR-10.1&#26102;&#20855;&#26377;&#26356;&#39640;&#30340;&#32622;&#20449;&#24230;&#12290;&#22312;&#27492;&#25104;&#21151;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CADet&#65288;&#23545;&#27604;&#24322;&#24120;&#26816;&#27979;&#65289;&#65292;&#19968;&#31181;&#29992;&#20110;&#21333;&#26679;&#26412;OOD&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#12290;CADet&#20511;&#37492;&#20102;MMD&#30340;&#24605;&#24819;&#65292;&#20294;&#21033;&#29992;&#20102;&#21516;&#19968;&#26679;&#26412;&#30340;&#23545;&#27604;&#21464;&#25442;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;CADet&#22312;&#35782;&#21035;&#34987;&#23545;&#25239;&#24615;&#25200;&#21160;&#24178;&#25200;&#30340;&#26679;&#26412;&#26041;&#38754;&#32988;&#36807;&#29616;&#26377;&#30340;&#23545;&#25239;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Handling out-of-distribution (OOD) samples has become a major stake in the real-world deployment of machine learning systems. This work explores the use of self-supervised contrastive learning to the simultaneous detection of two types of OOD samples: unseen classes and adversarial perturbations. First, we pair self-supervised contrastive learning with the maximum mean discrepancy (MMD) two-sample test. This approach enables us to robustly test whether two independent sets of samples originate from the same distribution, and we demonstrate its effectiveness by discriminating between CIFAR-10 and CIFAR-10.1 with higher confidence than previous work. Motivated by this success, we introduce CADet (Contrastive Anomaly Detection), a novel method for OOD detection of single samples. CADet draws inspiration from MMD, but leverages the similarity between contrastive transformations of a same sample. CADet outperforms existing adversarial detection methods in identifying adversarially perturbed
&lt;/p&gt;</description></item></channel></rss>