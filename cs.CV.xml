<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#30740;&#31350;&#22312;&#26080;&#30417;&#30563;&#25163;&#26415;&#22120;&#26800;&#20998;&#21106;&#20013;&#35299;&#20915;&#20102;&#30001;&#20110;&#20302;&#36136;&#37327;&#20809;&#27969;&#32780;&#24341;&#36215;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#37325;&#31574;&#30053;&#65306;&#30452;&#25509;&#20174;&#20809;&#27969;&#20013;&#25552;&#21462;&#36793;&#30028;&#12289;&#36873;&#25321;&#24615;&#20002;&#24323;&#36136;&#37327;&#36739;&#24046;&#30340;&#24103;&#12289;&#20197;&#21450;&#21033;&#29992;&#21487;&#21464;&#24103;&#29575;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20805;&#20998;&#35780;&#20272;&#65292;&#23637;&#31034;&#20986;&#26377;&#21069;&#26223;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.10039</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#26080;&#30417;&#30563;&#25163;&#26415;&#22120;&#26800;&#20998;&#21106;&#20013;&#20302;&#36136;&#37327;&#20809;&#27969;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Rethinking Low-quality Optical Flow in Unsupervised Surgical Instrument Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10039
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#26080;&#30417;&#30563;&#25163;&#26415;&#22120;&#26800;&#20998;&#21106;&#20013;&#35299;&#20915;&#20102;&#30001;&#20110;&#20302;&#36136;&#37327;&#20809;&#27969;&#32780;&#24341;&#36215;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#37325;&#31574;&#30053;&#65306;&#30452;&#25509;&#20174;&#20809;&#27969;&#20013;&#25552;&#21462;&#36793;&#30028;&#12289;&#36873;&#25321;&#24615;&#20002;&#24323;&#36136;&#37327;&#36739;&#24046;&#30340;&#24103;&#12289;&#20197;&#21450;&#21033;&#29992;&#21487;&#21464;&#24103;&#29575;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20805;&#20998;&#35780;&#20272;&#65292;&#23637;&#31034;&#20986;&#26377;&#21069;&#26223;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#30340;&#25163;&#26415;&#22120;&#26800;&#20998;&#21106;&#22312;&#26426;&#22120;&#20154;&#36741;&#21161;&#25163;&#26415;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#19982;&#30417;&#30563;&#35774;&#32622;&#19981;&#21516;&#65292;&#26080;&#30417;&#30563;&#20998;&#21106;&#20027;&#35201;&#20381;&#36182;&#20110;&#36816;&#21160;&#32447;&#32034;&#65292;&#28982;&#32780;&#30001;&#20110;&#25163;&#26415;&#38236;&#22836;&#20013;&#20809;&#27969;&#36890;&#24120;&#27604;&#33258;&#28982;&#22330;&#26223;&#20013;&#30340;&#35201;&#20302;&#36136;&#37327;&#65292;&#36825;&#20123;&#36816;&#21160;&#32447;&#32034;&#24456;&#38590;&#35782;&#21035;&#12290;&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#35299;&#20915;&#21363;&#20351;&#38754;&#23545;&#20302;&#36136;&#37327;&#20809;&#27969;&#22266;&#26377;&#38480;&#21046;&#65292;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#19977;&#20010;&#26041;&#38754;&#20837;&#25163;&#65306;&#30452;&#25509;&#20174;&#20809;&#27969;&#20013;&#25552;&#21462;&#36793;&#30028;&#12289;&#26377;&#36873;&#25321;&#22320;&#20002;&#24323;&#36136;&#37327;&#36739;&#24046;&#30340;&#24103;&#12289;&#20197;&#21450;&#21033;&#29992;&#21487;&#21464;&#24103;&#29575;&#30340;&#24494;&#35843;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;EndoVis2017 VOS&#25968;&#25454;&#38598;&#21644;Endovis2017&#25361;&#25112;&#25968;&#25454;&#38598;&#19978;&#23545;&#25105;&#20204;&#30340;&#31574;&#30053;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#65292;&#27169;&#22411;&#23637;&#29616;&#20986;&#26377;&#21069;&#26223;&#30340;&#32467;&#26524;&#65292;&#23454;&#29616;&#20102;&#22343;&#20540;&#20132;&#21449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10039v1 Announce Type: cross  Abstract: Video-based surgical instrument segmentation plays an important role in robot-assisted surgeries. Unlike supervised settings, unsupervised segmentation relies heavily on motion cues, which are challenging to discern due to the typically lower quality of optical flow in surgical footage compared to natural scenes. This presents a considerable burden for the advancement of unsupervised segmentation techniques. In our work, we address the challenge of enhancing model performance despite the inherent limitations of low-quality optical flow. Our methodology employs a three-pronged approach: extracting boundaries directly from the optical flow, selectively discarding frames with inferior flow quality, and employing a fine-tuning process with variable frame rates. We thoroughly evaluate our strategy on the EndoVis2017 VOS dataset and Endovis2017 Challenge dataset, where our model demonstrates promising results, achieving a mean Intersection-o
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35843;&#30740;&#20102;&#38754;&#21521;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21442;&#25968;&#20462;&#25913;&#36229;&#36234;&#20840;&#38754;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#27010;&#36848;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#36164;&#28304;&#25910;&#34255;&#12290;</title><link>https://arxiv.org/abs/2402.02242</link><description>&lt;p&gt;
&#38754;&#21521;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35843;&#30740;&#20102;&#38754;&#21521;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21442;&#25968;&#20462;&#25913;&#36229;&#36234;&#20840;&#38754;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#27010;&#36848;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#36164;&#28304;&#25910;&#34255;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#27169;&#22411;&#65288;PVMs&#65289;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#19979;&#28216;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#26368;&#20808;&#36827;&#30340;PVMs&#36798;&#21040;&#25968;&#21313;&#20159;&#29978;&#33267;&#25968;&#19975;&#20159;&#20010;&#21442;&#25968;&#65292;&#26631;&#20934;&#30340;&#20840;&#38754;&#24494;&#35843;&#33539;&#24335;&#30001;&#20110;&#39640;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#21464;&#24471;&#19981;&#21487;&#25345;&#32493;&#12290;&#20316;&#20026;&#21709;&#24212;&#65292;&#30740;&#31350;&#20154;&#21592;&#27491;&#22312;&#25506;&#32034;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#65292;&#26088;&#22312;&#20197;&#26368;&#23567;&#21442;&#25968;&#20462;&#25913;&#36229;&#36234;&#20840;&#38754;&#24494;&#35843;&#30340;&#24615;&#33021;&#12290;&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#35270;&#35273;PEFT&#30340;&#20840;&#38754;&#27010;&#36848;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#23545;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#31995;&#32479;&#23457;&#26597;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;PEFT&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#35752;&#35770;&#20102;&#27169;&#22411;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#29616;&#26377;&#26041;&#27861;&#20998;&#20026;&#19977;&#31867;&#65306;&#22522;&#20110;&#28155;&#21152;&#30340;&#12289;&#22522;&#20110;&#37096;&#20998;&#30340;&#21644;&#22522;&#20110;&#32479;&#19968;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24120;&#29992;&#30340;&#25968;&#25454;&#38598;&#21644;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#30740;&#31350;&#25361;&#25112;&#12290;&#35813;&#32508;&#36848;&#36824;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#36164;&#28304;&#25910;&#34255;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale pre-trained vision models (PVMs) have shown great potential for adaptability across various downstream vision tasks. However, with state-of-the-art PVMs growing to billions or even trillions of parameters, the standard full fine-tuning paradigm is becoming unsustainable due to high computational and storage demands. In response, researchers are exploring parameter-efficient fine-tuning (PEFT), which seeks to exceed the performance of full fine-tuning with minimal parameter modifications. This survey provides a comprehensive overview and future directions for visual PEFT, offering a systematic review of the latest advancements. First, we provide a formal definition of PEFT and discuss model pre-training methods. We then categorize existing methods into three categories: addition-based, partial-based, and unified-based. Finally, we introduce the commonly used datasets and applications and suggest potential future research challenges. A comprehensive collection of resources is
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#30495;&#23454;&#19990;&#30028;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#38382;&#39064;&#65292;&#22312;&#20840;&#23616;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#27979;&#35797;&#38598;&#19978;&#34917;&#20805;&#20102;&#29616;&#26377;&#30340;&#21327;&#35758;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#34913;&#24402;&#19968;&#21270;&#23618;&#26469;&#36866;&#24212;&#19981;&#24179;&#34913;&#30340;&#27979;&#35797;&#25968;&#25454;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#30340;&#22833;&#36133;&#12290;</title><link>http://arxiv.org/abs/2309.14949</link><description>&lt;p&gt;
&#38754;&#21521;&#30495;&#23454;&#19990;&#30028;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65306;&#20855;&#26377;&#24179;&#34913;&#24402;&#19968;&#21270;&#30340;&#19977;&#32593;&#32476;&#33258;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Towards Real-World Test-Time Adaptation: Tri-Net Self-Training with Balanced Normalization. (arXiv:2309.14949v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30495;&#23454;&#19990;&#30028;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#38382;&#39064;&#65292;&#22312;&#20840;&#23616;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#27979;&#35797;&#38598;&#19978;&#34917;&#20805;&#20102;&#29616;&#26377;&#30340;&#21327;&#35758;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#34913;&#24402;&#19968;&#21270;&#23618;&#26469;&#36866;&#24212;&#19981;&#24179;&#34913;&#30340;&#27979;&#35797;&#25968;&#25454;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#30340;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26088;&#22312;&#23558;&#28304;&#22495;&#27169;&#22411;&#36866;&#24212;&#21040;&#25512;&#26029;&#38454;&#27573;&#30340;&#27979;&#35797;&#25968;&#25454;&#20013;&#65292;&#22312;&#36866;&#24212;&#21040;&#26410;&#35265;&#36807;&#30340;&#30772;&#25439;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#30495;&#23454;&#19990;&#30028;&#24773;&#22659;&#19979;&#65292;&#36825;&#20123;&#23581;&#35797;&#21487;&#33021;&#20250;&#22833;&#36133;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#32771;&#34385;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#27969;&#21644;&#25345;&#32493;&#30340;&#39046;&#22495;&#36716;&#31227;&#19979;&#30340;&#30495;&#23454;&#19990;&#30028;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#29992;&#20840;&#23616;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#27979;&#35797;&#38598;&#26469;&#34917;&#20805;&#29616;&#26377;&#30340;&#30495;&#23454;&#19990;&#30028;TTA&#21327;&#35758;&#12290;&#25105;&#20204;&#35777;&#26126;&#25226;&#25152;&#26377;&#35774;&#32622;&#32467;&#21512;&#36215;&#26469;&#23545;&#29616;&#26377;&#26041;&#27861;&#25552;&#20986;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#35748;&#20026;&#26368;&#20808;&#22833;&#36133;&#30340;&#29616;&#26377;&#26041;&#27861;&#26159;&#22240;&#20026;&#19981;&#21152;&#36873;&#25321;&#22320;&#23558;&#24402;&#19968;&#21270;&#23618;&#36866;&#24212;&#21040;&#19981;&#24179;&#34913;&#30340;&#27979;&#35797;&#25968;&#25454;&#19978;&#25152;&#23548;&#33268;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#34913;&#25209;&#37327;&#24402;&#19968;&#21270;&#23618;&#65292;&#22312;&#25512;&#26029;&#38454;&#27573;&#26367;&#25442;&#21407;&#26469;&#30340;&#25209;&#37327;&#24402;&#19968;&#21270;&#12290;&#26032;&#30340;&#25209;&#37327;&#24402;&#19968;&#21270;&#23618;&#33021;&#22815;&#36866;&#24212;&#32780;&#19981;&#20559;&#21521;&#22810;&#25968;&#31867;&#21035;&#12290;&#25105;&#20204;&#21463;&#21040;&#33258;&#23398;&#20064;&#65288;ST&#65289;&#22312;&#26080;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#25104;&#21151;&#21551;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
Test-Time Adaptation aims to adapt source domain model to testing data at inference stage with success demonstrated in adapting to unseen corruptions. However, these attempts may fail under more challenging real-world scenarios. Existing works mainly consider real-world test-time adaptation under non-i.i.d. data stream and continual domain shift. In this work, we first complement the existing real-world TTA protocol with a globally class imbalanced testing set. We demonstrate that combining all settings together poses new challenges to existing methods. We argue the failure of state-of-the-art methods is first caused by indiscriminately adapting normalization layers to imbalanced testing data. To remedy this shortcoming, we propose a balanced batchnorm layer to swap out the regular batchnorm at inference stage. The new batchnorm layer is capable of adapting without biasing towards majority classes. We are further inspired by the success of self-training~(ST) in learning from unlabeled 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#35760;&#24518;&#39537;&#21160;&#21464;&#21387;&#22120;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#26469;&#39537;&#21160;&#39044;&#27979;&#36807;&#31243;&#65292;&#36880;&#27493;&#24341;&#20837;&#22122;&#22768;&#20197;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26356;&#20248;&#31168;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2207.07827</link><description>&lt;p&gt;
&#22810;&#20803;&#38271;&#24207;&#21015;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36890;&#29992;&#35760;&#24518;&#39537;&#21160;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Generalizable Memory-driven Transformer for Multivariate Long Sequence Time-series Forecasting. (arXiv:2207.07827v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#35760;&#24518;&#39537;&#21160;&#21464;&#21387;&#22120;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#26469;&#39537;&#21160;&#39044;&#27979;&#36807;&#31243;&#65292;&#36880;&#27493;&#24341;&#20837;&#22122;&#22768;&#20197;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26356;&#20248;&#31168;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#38271;&#24207;&#21015;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;(M-LSTF)&#26159;&#19968;&#20010;&#23454;&#38469;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#19982;&#20256;&#32479;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#19981;&#21516;&#65292;M-LSTF&#20219;&#21153;&#20174;&#20004;&#20010;&#26041;&#38754;&#26356;&#20855;&#25361;&#25112;&#24615;&#65306;1) M-LSTF&#27169;&#22411;&#38656;&#35201;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#26102;&#38388;&#29305;&#24449;&#20043;&#38388;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#24335;&#65307;2)&#22312;&#28378;&#21160;&#39044;&#27979;&#35774;&#32622;&#20013;&#65292;&#20004;&#20010;&#36830;&#32493;&#35757;&#32451;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#38543;&#30528;&#39044;&#27979;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#65292;&#36825;&#20351;&#24471;&#27169;&#22411;&#26356;&#26131;&#20110;&#36807;&#25311;&#21512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#35760;&#24518;&#39537;&#21160;&#21464;&#21387;&#22120;&#26469;&#35299;&#20915;M-LSTF&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#23616;&#23618;&#38754;&#30340;&#35760;&#24518;&#32452;&#20214;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#26469;&#39537;&#21160;&#39044;&#27979;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#28176;&#36827;&#24335;&#30340;&#26041;&#24335;&#26469;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#20197;&#22686;&#24378;&#20854;&#27867;&#21270;&#33021;&#21147;&#65292;&#36880;&#27493;&#22312;&#35757;&#32451;&#26679;&#26412;&#20013;&#24341;&#20837;&#20271;&#21162;&#21033;&#22122;&#22768;&#12290;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#20116;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26356;&#20248;&#24322;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate long sequence time-series forecasting (M-LSTF) is a practical but challenging problem. Unlike traditional timer-series forecasting tasks, M-LSTF tasks are more challenging from two aspects: 1) M-LSTF models need to learn time-series patterns both within and between multiple time features; 2) Under the rolling forecasting setting, the similarity between two consecutive training samples increases with the increasing prediction length, which makes models more prone to overfitting. In this paper, we propose a generalizable memory-driven Transformer to target M-LSTF problems. Specifically, we first propose a global-level memory component to drive the forecasting procedure by integrating multiple time-series features. In addition, we adopt a progressive fashion to train our model to increase its generalizability, in which we gradually introduce Bernoulli noises to training samples. Extensive experiments have been performed on five different datasets across multiple fields. Exper
&lt;/p&gt;</description></item></channel></rss>