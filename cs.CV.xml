<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MoPE&#25216;&#26415;&#65292;&#36890;&#36807;&#35299;&#24320;&#25552;&#31034;&#20197;&#33258;&#36866;&#24212;&#25429;&#33719;&#25968;&#25454;&#38598;&#32423;&#21644;&#23454;&#20363;&#32423;&#29305;&#24449;&#65292;&#24341;&#20837;&#20102;&#28151;&#21512;Prompt&#19987;&#23478;&#26469;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10568</link><description>&lt;p&gt;
MoPE&#65306;&#36890;&#36807;Prompt&#19987;&#23478;&#28151;&#21512;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
MoPE: Parameter-Efficient and Scalable Multimodal Fusion via Mixture of Prompt Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MoPE&#25216;&#26415;&#65292;&#36890;&#36807;&#35299;&#24320;&#25552;&#31034;&#20197;&#33258;&#36866;&#24212;&#25429;&#33719;&#25968;&#25454;&#38598;&#32423;&#21644;&#23454;&#20363;&#32423;&#29305;&#24449;&#65292;&#24341;&#20837;&#20102;&#28151;&#21512;Prompt&#19987;&#23478;&#26469;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt&#35843;&#25972;&#24050;&#32463;&#35777;&#26126;&#22312;&#34701;&#21512;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#21333;&#27169;&#22522;&#30784;&#27169;&#22411;&#26102;&#20855;&#26377;&#21442;&#25968;&#25928;&#29575;&#24615;&#12290;&#28982;&#32780;&#65292;&#20854;&#26377;&#38480;&#30340;&#36866;&#24212;&#24615;&#21644;&#34920;&#36798;&#33021;&#21147;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#19982;&#20854;&#20182;&#35843;&#25972;&#26041;&#27861;&#30456;&#27604;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#31616;&#21333;&#25552;&#31034;&#35299;&#24320;&#20197;&#33258;&#36866;&#24212;&#22320;&#25429;&#33719;&#25968;&#25454;&#38598;&#32423;&#21644;&#23454;&#20363;&#32423;&#29305;&#24449;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#24314;&#31435;&#22312;&#36825;&#31181;&#35299;&#24320;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Prompt&#19987;&#23478;&#30340;&#28151;&#21512;&#65288;MoPE&#65289;&#25216;&#26415;&#26469;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#12290;MoPE&#21033;&#29992;&#22810;&#27169;&#24577;&#37197;&#23545;&#20808;&#39564;&#22312;&#27599;&#20010;&#23454;&#20363;&#22522;&#30784;&#19978;&#36335;&#30001;&#26368;&#26377;&#25928;&#30340;&#25552;&#31034;&#12290;&#19982;&#31616;&#21333;&#25552;&#31034;&#30456;&#27604;&#65292;&#25105;&#20204;&#22522;&#20110;MoPE&#30340;&#26465;&#20214;&#25552;&#31034;&#23545;&#22810;&#27169;&#24577;&#34701;&#21512;&#20855;&#26377;&#26356;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#21644;&#21487;&#35757;&#32451;&#21442;&#25968;&#24635;&#25968;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19968;&#20010;&#19987;&#23478;&#36335;&#30001;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#23548;&#33268;&#19987;&#23478;&#30340;&#19981;&#26029;&#21457;&#23637;&#19987;&#38271;&#65292;&#19981;&#21516;&#19987;&#23478;&#19987;&#27880;&#20110;&#19981;&#21516;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10568v1 Announce Type: cross  Abstract: Prompt-tuning has demonstrated parameter-efficiency in fusing unimodal foundation models for multimodal tasks. However, its limited adaptivity and expressiveness lead to suboptimal performance when compared with other tuning methods. In this paper, we address this issue by disentangling the vanilla prompts to adaptively capture dataset-level and instance-level features. Building upon this disentanglement, we introduce the mixture of prompt experts (MoPE) technique to enhance expressiveness. MoPE leverages multimodal pairing priors to route the most effective prompt on a per-instance basis. Compared to vanilla prompting, our MoPE-based conditional prompting exhibits greater expressiveness for multimodal fusion, scaling better with the training data and the overall number of trainable parameters. We also study a regularization term for expert routing, leading to emergent expert specialization, where different experts focus on different c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#35805;&#24335;&#32593;&#31449;&#23548;&#33322;&#30340;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010; WEBLINX &#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#20195;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#22823;&#37327;&#20449;&#24687;&#30340;&#22788;&#29702;&#29942;&#39048;&#65292;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#26816;&#32034;&#21551;&#21457;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#22810;&#31181;&#22330;&#26223;&#19979;&#22797;&#21046;&#20154;&#31867;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05930</link><description>&lt;p&gt;
WebLINX: &#22810;&#36718;&#23545;&#35805;&#19979;&#30340;&#30495;&#23454;&#19990;&#30028;&#32593;&#31449;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
WebLINX: Real-World Website Navigation with Multi-Turn Dialogue
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#35805;&#24335;&#32593;&#31449;&#23548;&#33322;&#30340;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010; WEBLINX &#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#20195;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#22823;&#37327;&#20449;&#24687;&#30340;&#22788;&#29702;&#29942;&#39048;&#65292;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#26816;&#32034;&#21551;&#21457;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#22810;&#31181;&#22330;&#26223;&#19979;&#22797;&#21046;&#20154;&#31867;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#35805;&#24335;&#32593;&#31449;&#23548;&#33322;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#25968;&#23383;&#20195;&#29702;&#25511;&#21046;&#30528;&#19968;&#20010;&#32593;&#39029;&#27983;&#35272;&#22120;&#65292;&#24182;&#25353;&#29031;&#29992;&#25143;&#30340;&#25351;&#20196;&#20197;&#22810;&#36718;&#23545;&#35805;&#30340;&#26041;&#24335;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; WEBLINX - &#19968;&#20010;100K&#20132;&#20114;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#65292;&#22312;2300&#20010;&#19987;&#23478;&#28436;&#31034;&#20013;&#36827;&#34892;&#20102;&#23545;&#35805;&#24335;&#32593;&#31449;&#23548;&#33322;&#30340;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#28085;&#30422;&#20102;150&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#32593;&#31449;&#19978;&#30340;&#24191;&#27867;&#27169;&#24335;&#65292;&#21487;&#20197;&#29992;&#20110;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#35757;&#32451;&#21644;&#35780;&#20272;&#20195;&#29702;&#12290;&#30001;&#20110;&#23384;&#22312;&#22823;&#37327;&#20449;&#24687;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#26080;&#27861;&#23454;&#26102;&#22788;&#29702;&#25972;&#20010;&#32593;&#39029;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#29942;&#39048;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21463;&#26816;&#32034;&#21551;&#21457;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#25490;&#21517;&#30456;&#20851;&#20803;&#32032;&#26469;&#39640;&#25928;&#22320;&#20462;&#21098; HTML &#39029;&#38754;&#12290;&#25105;&#20204;&#20351;&#29992;&#36873;&#23450;&#30340;&#20803;&#32032;&#65292;&#20197;&#21450;&#23631;&#24149;&#25130;&#22270;&#21644;&#25805;&#20316;&#21382;&#21490;&#35760;&#24405;&#65292;&#35780;&#20272;&#21508;&#31181;&#27169;&#22411;&#22312;&#23548;&#33322;&#32593;&#39029;&#26102;&#22797;&#21046;&#20154;&#31867;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20174;&#23567;&#22411;&#32431;&#25991;&#26412;&#27169;&#22411;&#21040;&#19987;&#26377;&#30340;&#22810;&#27169;&#24577; LLMs &#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the problem of conversational web navigation, where a digital agent controls a web browser and follows user instructions to solve real-world tasks in a multi-turn dialogue fashion. To support this problem, we introduce WEBLINX - a large-scale benchmark of 100K interactions across 2300 expert demonstrations of conversational web navigation. Our benchmark covers a broad range of patterns on over 150 real-world websites and can be used to train and evaluate agents in diverse scenarios. Due to the magnitude of information present, Large Language Models (LLMs) cannot process entire web pages in real-time. To solve this bottleneck, we design a retrieval-inspired model that efficiently prunes HTML pages by ranking relevant elements. We use the selected elements, along with screenshots and action history, to assess a variety of models for their ability to replicate human behavior when navigating the web. Our experiments span from small text-only to proprietary multimodal LLMs. We fi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#37325;&#26032;&#21442;&#25968;&#21270;&#20302;&#31209;&#25552;&#31034;&#65288;RLP&#65289;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#36866;&#24212;&#36807;&#31243;&#20013;&#23454;&#29616;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#21487;&#35843;&#21442;&#25968;&#21644;&#23384;&#20648;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2312.10813</link><description>&lt;p&gt;
&#37325;&#26032;&#21442;&#25968;&#21270;&#20302;&#31209;&#25552;&#31034;&#65306;&#22312;0.5K&#21442;&#25968;&#20869;&#25512;&#24191;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Re-parameterized Low-rank Prompt: Generalize a Vision-Language Model within 0.5K Parameters. (arXiv:2312.10813v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10813
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#37325;&#26032;&#21442;&#25968;&#21270;&#20302;&#31209;&#25552;&#31034;&#65288;RLP&#65289;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#36866;&#24212;&#36807;&#31243;&#20013;&#23454;&#29616;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#21487;&#35843;&#21442;&#25968;&#21644;&#23384;&#20648;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#22914;&#20309;&#26377;&#25928;&#22320;&#23558;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#25104;&#20026;&#19968;&#20010;&#28909;&#38376;&#35805;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#25968;&#25454;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#12290;&#26368;&#36817;&#65292;&#25552;&#31034;&#35843;&#20248;&#24050;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#35843;&#25972;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#30740;&#31350;&#20154;&#21592;&#20923;&#32467;&#39592;&#24178;&#37096;&#20998;&#30340;&#21442;&#25968;&#65292;&#21482;&#35774;&#35745;&#21644;&#35843;&#25972;&#25552;&#31034;&#12290;&#19968;&#26041;&#38754;&#65292;&#25552;&#31034;&#35843;&#20248;&#30340;&#31934;&#24515;&#35774;&#35745;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22797;&#26434;&#30340;&#32467;&#26500;&#21644;&#26356;&#26032;&#35268;&#21017;&#22823;&#22823;&#22686;&#21152;&#20102;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#12290;&#21463;&#21040;&#35266;&#23519;&#21040;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#27867;&#21270;&#33021;&#21147;&#30340;&#28436;&#21464;&#27169;&#24335;&#19982;&#36866;&#24212;&#36807;&#31243;&#20013;&#25552;&#31034;&#30697;&#38453;&#31209;&#21464;&#21270;&#36235;&#21183;&#30340;&#35843;&#21644;&#19968;&#33268;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#22411;&#25552;&#31034;&#65292;&#37325;&#26032;&#21442;&#25968;&#21270;&#20302;&#31209;&#25552;&#31034;&#65288;RLP&#65289;&#65292;&#29992;&#20110;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#36866;&#24212;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22823;&#22823;&#20943;&#23569;&#21487;&#35843;&#21442;&#25968;&#21644;&#23384;&#20648;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of large pre-trained vision-language models, how to effectively transfer the knowledge of such foundational models to downstream tasks becomes a hot topic, especially in a data-deficient scenario. Recently, prompt tuning has become a popular solution. When adapting the vision-language models, researchers freeze the parameters in the backbone and only design and tune the prompts. On the one hand, the delicate design of prompt tuning exhibits strong performance. On the other hand, complicated structures and update rules largely increase the computation and storage cost. Motivated by the observation that the evolution pattern of the generalization capability in visual-language models aligns harmoniously with the trend of rank variations in the prompt matrix during adaptation, we design a new type of prompt, Re-parameterized Low-rank Prompt (RLP), for both efficient and effective adaptation. Our method could largely reduce the number of tunable parameters and storage s
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#37325;&#26032;&#24605;&#32771;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#39044;&#27979;&#21644;&#35268;&#21010;&#30340;&#25972;&#21512;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#23558;&#20854;&#20316;&#20026;&#30456;&#20114;&#20381;&#36182;&#30340;&#32852;&#21512;&#27493;&#39588;&#26469;&#25552;&#39640;&#23433;&#20840;&#24615;&#12289;&#25928;&#29575;&#24615;&#21644;&#33298;&#36866;&#24615;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.05731</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#30340;&#39044;&#27979;&#21644;&#35268;&#21010;&#30340;&#25972;&#21512;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Rethinking Integration of Prediction and Planning in Deep Learning-Based Automated Driving Systems: A Review. (arXiv:2308.05731v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05731
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#37325;&#26032;&#24605;&#32771;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#39044;&#27979;&#21644;&#35268;&#21010;&#30340;&#25972;&#21512;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#23558;&#20854;&#20316;&#20026;&#30456;&#20114;&#20381;&#36182;&#30340;&#32852;&#21512;&#27493;&#39588;&#26469;&#25552;&#39640;&#23433;&#20840;&#24615;&#12289;&#25928;&#29575;&#24615;&#21644;&#33298;&#36866;&#24615;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#26377;&#21487;&#33021;&#24443;&#24213;&#25913;&#21464;&#20010;&#20154;&#12289;&#20844;&#20849;&#21644;&#36135;&#36816;&#20132;&#36890;&#30340;&#26041;&#24335;&#12290;&#38500;&#20102;&#24863;&#30693;&#29615;&#22659;&#30340;&#24040;&#22823;&#25361;&#25112;&#22806;&#65292;&#21363;&#20934;&#30830;&#22320;&#20351;&#29992;&#21487;&#29992;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#24863;&#30693;&#29615;&#22659;&#65292;&#33258;&#21160;&#39550;&#39542;&#36824;&#21253;&#25324;&#35268;&#21010;&#19968;&#20010;&#23433;&#20840;&#12289;&#33298;&#36866;&#21644;&#39640;&#25928;&#30340;&#36816;&#21160;&#36712;&#36857;&#12290;&#20026;&#20102;&#20419;&#36827;&#23433;&#20840;&#21644;&#36827;&#27493;&#65292;&#35768;&#22810;&#24037;&#20316;&#20381;&#36182;&#20110;&#27169;&#22359;&#21270;&#30340;&#20132;&#36890;&#26410;&#26469;&#36816;&#21160;&#30340;&#39044;&#27979;&#12290;&#27169;&#22359;&#21270;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#36890;&#24120;&#23558;&#39044;&#27979;&#21644;&#35268;&#21010;&#20316;&#20026;&#39034;&#24207;&#30340;&#29420;&#31435;&#20219;&#21153;&#22788;&#29702;&#12290;&#34429;&#28982;&#36825;&#32771;&#34385;&#20102;&#21608;&#22260;&#20132;&#36890;&#23545;&#33258;&#36710;&#30340;&#24433;&#21709;&#65292;&#20294;&#23427;&#26410;&#33021;&#39044;&#27979;&#20132;&#36890;&#21442;&#19982;&#32773;&#23545;&#33258;&#36710;&#34892;&#20026;&#30340;&#21453;&#24212;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;&#39044;&#27979;&#21644;&#35268;&#21010;&#25972;&#21512;&#20026;&#30456;&#20114;&#20381;&#36182;&#30340;&#32852;&#21512;&#27493;&#39588;&#26159;&#23454;&#29616;&#23433;&#20840;&#12289;&#39640;&#25928;&#21644;&#33298;&#36866;&#39550;&#39542;&#25152;&#24517;&#38656;&#30340;&#12290;&#34429;&#28982;&#26377;&#21508;&#31181;&#27169;&#22411;&#23454;&#29616;&#20102;&#36825;&#31181;&#38598;&#25104;&#31995;&#32479;&#65292;&#20294;&#23545;&#19981;&#21516;&#21407;&#29702;&#30340;&#20840;&#38754;&#27010;&#36848;&#21644;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#32570;&#20047;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated driving has the potential to revolutionize personal, public, and freight mobility. Besides the enormous challenge of perception, i.e. accurately perceiving the environment using available sensor data, automated driving comprises planning a safe, comfortable, and efficient motion trajectory. To promote safety and progress, many works rely on modules that predict the future motion of surrounding traffic. Modular automated driving systems commonly handle prediction and planning as sequential separate tasks. While this accounts for the influence of surrounding traffic on the ego-vehicle, it fails to anticipate the reactions of traffic participants to the ego-vehicle's behavior. Recent works suggest that integrating prediction and planning in an interdependent joint step is necessary to achieve safe, efficient, and comfortable driving. While various models implement such integrated systems, a comprehensive overview and theoretical understanding of different principles are lacking.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19981;&#30830;&#23450;&#26368;&#22823;&#29109;&#21407;&#29702;&#65292;&#35813;&#21407;&#29702;&#21487;&#20197;&#22788;&#29702;&#27169;&#22411;&#20803;&#32032;&#19981;&#21487;&#35266;&#27979;&#30340;&#24773;&#20917;&#65292;&#24182;&#20248;&#20110;&#29305;&#23450;&#26465;&#20214;&#19979;&#30340;&#26368;&#22823;&#29109;&#26041;&#27861;&#12290;&#21516;&#26102;&#23558;&#40657;&#21283;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20986;&#29992;&#20316;&#19981;&#30830;&#23450;&#26426;&#22120;&#29109;&#26694;&#26550;&#30340;&#36755;&#20837;&#65292;&#24615;&#33021;&#24471;&#21040;&#20102;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.09868</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#26368;&#22823;&#29109;&#21407;&#29702;
&lt;/p&gt;
&lt;p&gt;
The Principle of Uncertain Maximum Entropy. (arXiv:2305.09868v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09868
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19981;&#30830;&#23450;&#26368;&#22823;&#29109;&#21407;&#29702;&#65292;&#35813;&#21407;&#29702;&#21487;&#20197;&#22788;&#29702;&#27169;&#22411;&#20803;&#32032;&#19981;&#21487;&#35266;&#27979;&#30340;&#24773;&#20917;&#65292;&#24182;&#20248;&#20110;&#29305;&#23450;&#26465;&#20214;&#19979;&#30340;&#26368;&#22823;&#29109;&#26041;&#27861;&#12290;&#21516;&#26102;&#23558;&#40657;&#21283;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20986;&#29992;&#20316;&#19981;&#30830;&#23450;&#26426;&#22120;&#29109;&#26694;&#26550;&#30340;&#36755;&#20837;&#65292;&#24615;&#33021;&#24471;&#21040;&#20102;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#29109;&#21407;&#29702;&#22312;&#20449;&#24687;&#29702;&#35770;&#20013;&#30340;&#24341;&#20837;&#65292;&#20026;&#32479;&#35745;&#21147;&#23398;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#29983;&#24577;&#23398;&#31561;&#21508;&#20010;&#39046;&#22495;&#30340;&#21457;&#23637;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#20854;&#24471;&#21040;&#30340;&#35299;&#20915;&#26041;&#26696;&#20316;&#20026;&#20652;&#21270;&#21058;&#65292;&#20419;&#36827;&#30740;&#31350;&#20154;&#21592;&#23558;&#20182;&#20204;&#30340;&#32463;&#39564;&#35266;&#23519;&#26144;&#23556;&#21040;&#33719;&#21462;&#26080;&#20559;&#27169;&#22411;&#65292;&#21516;&#26102;&#21152;&#28145;&#20102;&#23545;&#22797;&#26434;&#31995;&#32479;&#21644;&#29616;&#35937;&#30340;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#27169;&#22411;&#20803;&#32032;&#19981;&#30452;&#25509;&#21487;&#35266;&#27979;&#30340;&#24773;&#20917;&#19979;&#65292;&#20363;&#22914;&#23384;&#22312;&#22122;&#22768;&#25110;&#30524;&#37096;&#36974;&#25377;&#30340;&#24773;&#20917;&#19979;&#65292;&#26631;&#20934;&#26368;&#22823;&#29109;&#26041;&#27861;&#21487;&#33021;&#20250;&#22833;&#36133;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#21305;&#37197;&#29305;&#24449;&#32422;&#26463;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19981;&#30830;&#23450;&#26368;&#22823;&#29109;&#21407;&#29702;&#20316;&#20026;&#19968;&#31181;&#26041;&#27861;&#65292;&#23613;&#31649;&#23384;&#22312;&#20219;&#24847;&#22122;&#22768;&#35266;&#23519;&#65292;&#23427;&#21516;&#26102;&#23558;&#25152;&#26377;&#21487;&#29992;&#20449;&#24687;&#32534;&#30721;&#65292;&#32780;&#19988;&#20248;&#20110;&#19968;&#20123;&#29305;&#23450;&#26465;&#20214;&#19979;&#30340;&#26368;&#22823;&#29109;&#26041;&#27861;&#30340;&#20934;&#30830;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#40657;&#21283;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20986;&#29992;&#20316;&#19981;&#30830;&#23450;&#26426;&#22120;&#29109;&#26694;&#26550;&#30340;&#36755;&#20837;&#65292;&#20174;&#32780;&#22312;&#19982;&#26368;&#22823;&#20284;&#28982;&#31639;&#27861;&#30456;&#27604;&#26102;&#24314;&#31435;&#20102;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The principle of maximum entropy, as introduced by Jaynes in information theory, has contributed to advancements in various domains such as Statistical Mechanics, Machine Learning, and Ecology. Its resultant solutions have served as a catalyst, facilitating researchers in mapping their empirical observations to the acquisition of unbiased models, whilst deepening the understanding of complex systems and phenomena. However, when we consider situations in which the model elements are not directly observable, such as when noise or ocular occlusion is present, possibilities arise for which standard maximum entropy approaches may fail, as they are unable to match feature constraints. Here we show the Principle of Uncertain Maximum Entropy as a method that both encodes all available information in spite of arbitrarily noisy observations while surpassing the accuracy of some ad-hoc methods. Additionally, we utilize the output of a black-box machine learning model as input into an uncertain ma
&lt;/p&gt;</description></item></channel></rss>