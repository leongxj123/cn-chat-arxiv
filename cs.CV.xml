<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>FlexCap&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#22270;&#20687;&#20013;&#20855;&#26377;&#19981;&#21516;&#38271;&#24230;&#30340;&#21306;&#22495;&#25551;&#36848;&#65292;&#22312;&#23494;&#38598;&#23383;&#24149;&#20219;&#21153;&#21644;&#35270;&#35273;&#38382;&#31572;&#31995;&#32479;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12026</link><description>&lt;p&gt;
FlexCap&#65306;&#22312;&#22270;&#20687;&#20013;&#29983;&#25104;&#20016;&#23500;&#12289;&#26412;&#22320;&#21270;&#21644;&#28789;&#27963;&#30340;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
FlexCap: Generating Rich, Localized, and Flexible Captions in Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12026
&lt;/p&gt;
&lt;p&gt;
FlexCap&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#22270;&#20687;&#20013;&#20855;&#26377;&#19981;&#21516;&#38271;&#24230;&#30340;&#21306;&#22495;&#25551;&#36848;&#65292;&#22312;&#23494;&#38598;&#23383;&#24149;&#20219;&#21153;&#21644;&#35270;&#35273;&#38382;&#31572;&#31995;&#32479;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#21151;&#33021;&#30340;$\textit{&#28789;&#27963;&#23383;&#24149;}$&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#65292;&#33021;&#22815;&#29983;&#25104;&#38271;&#24230;&#19981;&#21516;&#30340;&#29305;&#23450;&#21306;&#22495;&#25551;&#36848;&#12290;&#35813;&#27169;&#22411;FlexCap&#32463;&#36807;&#35757;&#32451;&#65292;&#21487;&#20026;&#36755;&#20837;&#30340;&#36793;&#30028;&#26694;&#29983;&#25104;&#38271;&#24230;&#26465;&#20214;&#30340;&#23383;&#24149;&#65292;&#20174;&#32780;&#21487;&#20197;&#25511;&#21046;&#20854;&#36755;&#20986;&#30340;&#20449;&#24687;&#23494;&#24230;&#65292;&#25551;&#36848;&#33539;&#22260;&#20174;&#31616;&#27905;&#30340;&#23545;&#35937;&#26631;&#31614;&#21040;&#35814;&#32454;&#30340;&#23383;&#24149;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#20174;&#24102;&#23383;&#24149;&#30340;&#22270;&#20687;&#24320;&#22987;&#21019;&#24314;&#20102;&#22823;&#35268;&#27169;&#30340;&#22270;&#20687;&#21306;&#22495;&#25551;&#36848;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#36825;&#31181;&#28789;&#27963;&#30340;&#23383;&#24149;&#21151;&#33021;&#26377;&#20960;&#20010;&#23453;&#36149;&#30340;&#24212;&#29992;&#12290;&#39318;&#20808;&#65292;FlexCap&#22312;Visual Genome&#25968;&#25454;&#38598;&#19978;&#30340;&#23494;&#38598;&#23383;&#24149;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#21487;&#20197;&#36890;&#36807;&#37319;&#29992;FlexCap&#29983;&#25104;&#26412;&#22320;&#21270;&#25551;&#36848;&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20837;&#26469;&#26500;&#24314;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#31995;&#32479;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#31995;&#32479;&#22312;&#35768;&#22810;VQ&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#25216;&#26415;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12026v1 Announce Type: cross  Abstract: We introduce a versatile $\textit{flexible-captioning}$ vision-language model (VLM) capable of generating region-specific descriptions of varying lengths. The model, FlexCap, is trained to produce length-conditioned captions for input bounding boxes, and this allows control over the information density of its output, with descriptions ranging from concise object labels to detailed captions. To achieve this we create large-scale training datasets of image region descriptions of varying length, starting from captioned images. This flexible-captioning capability has several valuable applications.   First, FlexCap demonstrates superior performance in dense captioning tasks on the Visual Genome dataset. Second, a visual question answering (VQA) system can be built by employing FlexCap to generate localized descriptions as inputs to a large language model. The resulting system achieves state-of-the-art zero-shot performance on a number of VQ
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#20013;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#22270;&#20687;&#36136;&#37327;&#30340;&#26032;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#30830;&#20445;&#25991;&#26412;&#21644;&#22270;&#20687;&#20869;&#23481;&#30340;&#23545;&#40784;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#27861;&#26469;&#24402;&#32435;&#36825;&#20123;&#25351;&#26631;</title><link>https://arxiv.org/abs/2403.11821</link><description>&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#65306;&#22270;&#20687;&#36136;&#37327;&#24230;&#37327;&#30340;&#35843;&#26597;&#19982;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Evaluating Text to Image Synthesis: Survey and Taxonomy of Image Quality Metrics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11821
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#20013;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#22270;&#20687;&#36136;&#37327;&#30340;&#26032;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#30830;&#20445;&#25991;&#26412;&#21644;&#22270;&#20687;&#20869;&#23481;&#30340;&#23545;&#40784;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#27861;&#26469;&#24402;&#32435;&#36825;&#20123;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#21644;&#35270;&#35273;&#32467;&#21512;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#25512;&#21160;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20114;&#32852;&#32593;&#25110;&#20854;&#20182;&#22823;&#35268;&#27169;&#25968;&#25454;&#24211;&#20013;&#30340;&#28023;&#37327;&#25991;&#26412;-&#22270;&#20687;&#23545;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#12290;&#38543;&#30528;&#23545;&#39640;&#36136;&#37327;&#22270;&#20687;&#29983;&#25104;&#30340;&#38656;&#27714;&#36716;&#21521;&#30830;&#20445;&#25991;&#26412;&#19982;&#22270;&#20687;&#20043;&#38388;&#30340;&#20869;&#23481;&#23545;&#40784;&#65292;&#24050;&#24320;&#21457;&#20102;&#26032;&#39062;&#30340;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#65292;&#26088;&#22312;&#27169;&#25311;&#20154;&#31867;&#21028;&#26029;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#25910;&#38598;&#20855;&#26377;&#36234;&#26469;&#36234;&#22797;&#26434;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#30740;&#31350;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#25104;&#24615;&#21450;&#20854;&#20316;&#20026;&#25991;&#26412;&#19982;&#22270;&#20687;&#20869;&#23481;&#32452;&#25104;&#23545;&#40784;&#36136;&#37327;&#24230;&#37327;&#30340;&#20854;&#32435;&#20837;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#20171;&#32461;&#20102;&#29616;&#26377;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#26469;&#23545;&#36825;&#20123;&#25351;&#26631;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#36824;&#23457;&#26597;&#20102;&#32463;&#24120;&#37319;&#29992;&#30340;&#25991;&#26412;-&#22270;&#20687;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11821v1 Announce Type: cross  Abstract: Recent advances in text-to-image synthesis have been enabled by exploiting a combination of language and vision through foundation models. These models are pre-trained on tremendous amounts of text-image pairs sourced from the World Wide Web or other large-scale databases. As the demand for high-quality image generation shifts towards ensuring content alignment between text and image, novel evaluation metrics have been developed with the aim of mimicking human judgments. Thus, researchers have started to collect datasets with increasingly complex annotations to study the compositionality of vision-language models and their incorporation as a quality measure of compositional alignment between text and image contents. In this work, we provide a comprehensive overview of existing text-to-image evaluation metrics and propose a new taxonomy for categorizing these metrics. We also review frequently adopted text-image benchmark datasets befor
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#27491;&#21017;&#21270;&#27969;&#22312;&#20302;&#32500;&#25968;&#25454;&#27969;&#24418;&#19978;&#36827;&#34892;&#22806;&#22495;&#26816;&#27979;&#65292;&#36890;&#36807;&#20272;&#35745;&#23494;&#24230;&#21644;&#27979;&#37327;&#19982;&#27969;&#24418;&#30340;&#36317;&#31163;&#26469;&#21028;&#26029;&#22806;&#22495;&#25968;&#25454;&#65292;&#26377;&#25928;&#25552;&#39640;&#22806;&#22495;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13792</link><description>&lt;p&gt;
&#21033;&#29992;&#25968;&#25454;&#27969;&#24418;&#19978;&#30340;&#27491;&#21017;&#21270;&#27969;&#36827;&#34892;&#22806;&#22495;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution detection using normalizing flows on the data manifold. (arXiv:2308.13792v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13792
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#27491;&#21017;&#21270;&#27969;&#22312;&#20302;&#32500;&#25968;&#25454;&#27969;&#24418;&#19978;&#36827;&#34892;&#22806;&#22495;&#26816;&#27979;&#65292;&#36890;&#36807;&#20272;&#35745;&#23494;&#24230;&#21644;&#27979;&#37327;&#19982;&#27969;&#24418;&#30340;&#36317;&#31163;&#26469;&#21028;&#26029;&#22806;&#22495;&#25968;&#25454;&#65292;&#26377;&#25928;&#25552;&#39640;&#22806;&#22495;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22806;&#22495;&#26816;&#27979;&#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#26159;&#20272;&#35745;&#22522;&#30784;&#25968;&#25454;&#20998;&#24067;&#65292;&#20026;&#22806;&#22495;&#25968;&#25454;&#20998;&#37197;&#36739;&#20302;&#30340;&#21487;&#33021;&#24615;&#20540;&#12290;&#27491;&#21017;&#21270;&#27969;&#26159;&#22522;&#20110;&#21487;&#33021;&#24615;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#20445;&#25345;&#32500;&#24230;&#30340;&#21487;&#36870;&#21464;&#25442;&#25552;&#20379;&#21487;&#35745;&#31639;&#30340;&#23494;&#24230;&#20272;&#35745;&#12290;&#20256;&#32479;&#30340;&#27491;&#21017;&#21270;&#27969;&#22312;&#22806;&#22495;&#26816;&#27979;&#20013;&#23481;&#26131;&#22833;&#36133;&#65292;&#22240;&#20026;&#22522;&#20110;&#21487;&#33021;&#24615;&#30340;&#27169;&#22411;&#38754;&#20020;&#30528;&#32500;&#24230;&#35781;&#21650;&#30340;&#38382;&#39064;&#12290;&#26681;&#25454;&#27969;&#24418;&#20551;&#35774;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#36890;&#24120;&#20301;&#20110;&#20302;&#32500;&#27969;&#24418;&#19978;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#27491;&#21017;&#21270;&#27969;&#36827;&#34892;&#22806;&#22495;&#26816;&#27979;&#26102;&#30340;&#27969;&#24418;&#23398;&#20064;&#25928;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20302;&#32500;&#27969;&#24418;&#19978;&#20272;&#35745;&#23494;&#24230;&#65292;&#24182;&#32467;&#21512;&#27979;&#37327;&#19982;&#27969;&#24418;&#30340;&#36317;&#31163;&#20316;&#20026;&#22806;&#22495;&#26816;&#27979;&#30340;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#21333;&#29420;&#20351;&#29992;&#23427;&#20204;&#23545;&#20110;&#36825;&#20010;&#20219;&#21153;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#27969;&#24418;&#23398;&#20064;&#23545;&#22806;&#22495;&#26816;&#27979;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common approach for out-of-distribution detection involves estimating an underlying data distribution, which assigns a lower likelihood value to out-of-distribution data. Normalizing flows are likelihood-based generative models providing a tractable density estimation via dimension-preserving invertible transformations. Conventional normalizing flows are prone to fail in out-of-distribution detection, because of the well-known curse of dimensionality problem of the likelihood-based models. According to the manifold hypothesis, real-world data often lie on a low-dimensional manifold. This study investigates the effect of manifold learning using normalizing flows on out-of-distribution detection. We proceed by estimating the density on a low-dimensional manifold, coupled with measuring the distance from the manifold, as criteria for out-of-distribution detection. However, individually, each of them is insufficient for this task. The extensive experimental results show that manifold lea
&lt;/p&gt;</description></item></channel></rss>