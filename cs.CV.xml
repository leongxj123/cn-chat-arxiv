<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;FedDG&#26550;&#26500;&#26041;&#27861;gPerXAN&#65292;&#36890;&#36807;&#35268;&#33539;&#21270;&#26041;&#26696;&#21644;&#24341;&#23548;&#27491;&#21017;&#21270;&#22120;&#37197;&#21512;&#24037;&#20316;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#26174;&#24335;&#32452;&#35013;&#35268;&#33539;&#21270;&#65292;&#26377;&#21161;&#20110;&#23458;&#25143;&#31471;&#27169;&#22411;&#23545;&#39046;&#22495;&#29305;&#24449;&#36827;&#34892;&#26377;&#36873;&#25321;&#24615;&#36807;&#28388;&#12290;</title><link>https://arxiv.org/abs/2403.15605</link><description>&lt;p&gt;
&#20026;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#39640;&#25928;&#32452;&#21512;&#35268;&#33539;&#21270;&#23618;&#19982;&#27491;&#21017;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficiently Assemble Normalization Layers and Regularization for Federated Domain Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15605
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;FedDG&#26550;&#26500;&#26041;&#27861;gPerXAN&#65292;&#36890;&#36807;&#35268;&#33539;&#21270;&#26041;&#26696;&#21644;&#24341;&#23548;&#27491;&#21017;&#21270;&#22120;&#37197;&#21512;&#24037;&#20316;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#26174;&#24335;&#32452;&#35013;&#35268;&#33539;&#21270;&#65292;&#26377;&#21161;&#20110;&#23458;&#25143;&#31471;&#27169;&#22411;&#23545;&#39046;&#22495;&#29305;&#24449;&#36827;&#34892;&#26377;&#36873;&#25321;&#24615;&#36807;&#28388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#36716;&#31227;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20010;&#20005;&#23803;&#30340;&#38382;&#39064;&#65292;&#20250;&#23548;&#33268;&#27169;&#22411;&#22312;&#26410;&#30693;&#39046;&#22495;&#27979;&#35797;&#26102;&#24615;&#33021;&#19979;&#38477;&#12290;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#65288;FedDG&#65289;&#26088;&#22312;&#20197;&#38544;&#31169;&#20445;&#25252;&#30340;&#26041;&#24335;&#20351;&#29992;&#21327;&#20316;&#23458;&#25143;&#31471;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#65292;&#33021;&#22815;&#24456;&#22909;&#22320;&#27867;&#21270;&#21040;&#21487;&#33021;&#23384;&#22312;&#39046;&#22495;&#36716;&#31227;&#30340;&#26410;&#30693;&#23458;&#25143;&#31471;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;FedDG&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#39069;&#22806;&#30340;&#25968;&#25454;&#27844;&#38706;&#38544;&#31169;&#39118;&#38505;&#65292;&#25110;&#32773;&#22312;&#23458;&#25143;&#31471;&#36890;&#20449;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#20135;&#29983;&#26174;&#33879;&#24320;&#38144;&#65292;&#36825;&#22312;&#32852;&#37030;&#23398;&#20064;&#33539;&#24335;&#20013;&#26159;&#20027;&#35201;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;FedDG&#26550;&#26500;&#26041;&#27861;&#65292;&#21363;gPerXAN&#65292;&#23427;&#20381;&#36182;&#20110;&#19968;&#20010;&#35268;&#33539;&#21270;&#26041;&#26696;&#19982;&#24341;&#23548;&#27491;&#21017;&#21270;&#22120;&#37197;&#21512;&#24037;&#20316;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#20010;&#24615;&#21270;&#26174;&#24335;&#32452;&#35013;&#35268;&#33539;&#21270;&#65292;&#20197;&#24378;&#21046;&#23458;&#25143;&#31471;&#27169;&#22411;&#26377;&#36873;&#25321;&#22320;&#36807;&#28388;&#23545;&#26412;&#22320;&#25968;&#25454;&#26377;&#20559;&#21521;&#30340;&#29305;&#23450;&#39046;&#22495;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15605v1 Announce Type: cross  Abstract: Domain shift is a formidable issue in Machine Learning that causes a model to suffer from performance degradation when tested on unseen domains. Federated Domain Generalization (FedDG) attempts to train a global model using collaborative clients in a privacy-preserving manner that can generalize well to unseen clients possibly with domain shift. However, most existing FedDG methods either cause additional privacy risks of data leakage or induce significant costs in client communication and computation, which are major concerns in the Federated Learning paradigm. To circumvent these challenges, here we introduce a novel architectural method for FedDG, namely gPerXAN, which relies on a normalization scheme working with a guiding regularizer. In particular, we carefully design Personalized eXplicitly Assembled Normalization to enforce client models selectively filtering domain-specific features that are biased towards local data while ret
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22312;&#26377;&#38480;&#25968;&#25454;&#12289;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#26465;&#20214;&#19979;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#30340;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#20110;&#20219;&#21153;&#21644;&#26041;&#27861;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#30740;&#31350;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.14397</link><description>&lt;p&gt;
&#20851;&#20110;&#26377;&#38480;&#25968;&#25454;&#12289;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#29983;&#25104;&#24314;&#27169;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Generative Modeling with Limited Data, Few Shots, and Zero Shot. (arXiv:2307.14397v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22312;&#26377;&#38480;&#25968;&#25454;&#12289;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#26465;&#20214;&#19979;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#30340;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#20110;&#20219;&#21153;&#21644;&#26041;&#27861;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#30740;&#31350;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#29983;&#25104;&#24314;&#27169;&#26088;&#22312;&#23398;&#20064;&#29983;&#25104;&#19982;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#32479;&#35745;&#30456;&#20284;&#30340;&#26032;&#25968;&#25454;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#22312;&#26377;&#38480;&#25968;&#25454;&#12289;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#26465;&#20214;&#19979;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#30340;&#24773;&#20917;&#65292;&#31216;&#20026;&#25968;&#25454;&#32422;&#26463;&#19979;&#30340;&#29983;&#25104;&#24314;&#27169;&#65288;GM-DC&#65289;&#12290;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20027;&#39064;&#65292;&#24403;&#25968;&#25454;&#33719;&#21462;&#20855;&#26377;&#25361;&#25112;&#24615;&#26102;&#65292;&#20363;&#22914;&#21307;&#30103;&#24212;&#29992;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#32972;&#26223;&#12289;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#20998;&#31867;&#20307;&#31995;&#65306;&#19968;&#20010;&#26159;GM-DC&#20219;&#21153;&#20998;&#31867;&#65292;&#21478;&#19968;&#20010;&#26159;GM-DC&#26041;&#27861;&#20998;&#31867;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;GM-DC&#20219;&#21153;&#21644;&#26041;&#27861;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#30740;&#31350;&#31354;&#30333;&#12289;&#30740;&#31350;&#36235;&#21183;&#21644;&#26410;&#26469;&#25506;&#32034;&#30340;&#28508;&#22312;&#36884;&#24452;&#12290;&#39033;&#30446;&#32593;&#31449;&#65306;https://gmdc-survey.github.io&#12290;
&lt;/p&gt;
&lt;p&gt;
In machine learning, generative modeling aims to learn to generate new data statistically similar to the training data distribution. In this paper, we survey learning generative models under limited data, few shots and zero shot, referred to as Generative Modeling under Data Constraint (GM-DC). This is an important topic when data acquisition is challenging, e.g. healthcare applications. We discuss background, challenges, and propose two taxonomies: one on GM-DC tasks and another on GM-DC approaches. Importantly, we study interactions between different GM-DC tasks and approaches. Furthermore, we highlight research gaps, research trends, and potential avenues for future exploration. Project website: https://gmdc-survey.github.io.
&lt;/p&gt;</description></item><item><title>TKN&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#23454;&#26102;&#35270;&#39057;&#39044;&#27979;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21463;&#38480;&#20449;&#24687;&#25552;&#21462;&#21644;&#24182;&#34892;&#39044;&#27979;&#26041;&#26696;&#26469;&#25552;&#21319;&#39044;&#27979;&#36807;&#31243;&#30340;&#36895;&#24230;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2303.09807</link><description>&lt;p&gt;
TKN&#65306;&#22522;&#20110;Transformer&#30340;&#23454;&#26102;&#35270;&#39057;&#20851;&#38190;&#28857;&#39044;&#27979;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
TKN: Transformer-based Keypoint Prediction Network For Real-time Video Prediction. (arXiv:2303.09807v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09807
&lt;/p&gt;
&lt;p&gt;
TKN&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#23454;&#26102;&#35270;&#39057;&#39044;&#27979;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21463;&#38480;&#20449;&#24687;&#25552;&#21462;&#21644;&#24182;&#34892;&#39044;&#27979;&#26041;&#26696;&#26469;&#25552;&#21319;&#39044;&#27979;&#36807;&#31243;&#30340;&#36895;&#24230;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#39044;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#24191;&#27867;&#29992;&#36884;&#30340;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26041;&#27861;&#36807;&#20110;&#24378;&#35843;&#20934;&#30830;&#24615;&#65292;&#24573;&#35270;&#20102;&#30001;&#20110;&#36807;&#20110;&#22797;&#26434;&#30340;&#27169;&#22411;&#32467;&#26500;&#32780;&#23548;&#33268;&#30340;&#36739;&#24930;&#30340;&#39044;&#27979;&#36895;&#24230;&#20197;&#21450;&#36807;&#22810;&#30340;&#20887;&#20313;&#20449;&#24687;&#23398;&#20064;&#21644;GPU&#20869;&#23384;&#28040;&#32791;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TKN&#65292;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#20851;&#38190;&#28857;&#39044;&#27979;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#21463;&#38480;&#20449;&#24687;&#25552;&#21462;&#21644;&#24182;&#34892;&#39044;&#27979;&#26041;&#26696;&#26469;&#25552;&#21319;&#39044;&#27979;&#36807;&#31243;&#30340;&#36895;&#24230;&#12290;TKN&#26159;&#25105;&#20204;&#30446;&#21069;&#25152;&#30693;&#30340;&#31532;&#19968;&#20010;&#23454;&#26102;&#35270;&#39057;&#39044;&#27979;&#35299;&#20915;&#26041;&#26696;&#65292;&#21516;&#26102;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#24182;&#20445;&#25345;&#20854;&#20182;&#24615;&#33021;&#12290;&#22312;KTH&#21644;Human Action 3D&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;TKN&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video prediction is a complex time-series forecasting task with great potential in many use cases. However, conventional methods overemphasize accuracy while ignoring the slow prediction speed caused by complicated model structures that learn too much redundant information with excessive GPU memory consumption. Furthermore, conventional methods mostly predict frames sequentially (frame-by-frame) and thus are hard to accelerate. Consequently, valuable use cases such as real-time danger prediction and warning cannot achieve fast enough inference speed to be applicable in reality. Therefore, we propose a transformer-based keypoint prediction neural network (TKN), an unsupervised learning method that boost the prediction process via constrained information extraction and parallel prediction scheme. TKN is the first real-time video prediction solution to our best knowledge, while significantly reducing computation costs and maintaining other performance. Extensive experiments on KTH and Hum
&lt;/p&gt;</description></item></channel></rss>