<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#20013;&#39118;&#20998;&#21106;&#30340;&#21512;&#25104;&#26694;&#26550;&#65292;&#20351;&#29992;&#30149;&#21464;&#29305;&#23450;&#22686;&#24378;&#31574;&#30053;&#25193;&#23637;&#20102;SynthSeg&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#23545;&#20581;&#24247;&#32452;&#32455;&#21644;&#30149;&#29702;&#30149;&#21464;&#30340;&#20998;&#21106;&#65292;&#26080;&#38656;&#29305;&#23450;&#24207;&#21015;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#22806;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.01946</link><description>&lt;p&gt;
&#29992;&#20110;&#40065;&#26834;&#24615;&#20013;&#39118;&#20998;&#21106;&#30340;&#21512;&#25104;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Synthetic Data for Robust Stroke Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01946
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#20013;&#39118;&#20998;&#21106;&#30340;&#21512;&#25104;&#26694;&#26550;&#65292;&#20351;&#29992;&#30149;&#21464;&#29305;&#23450;&#22686;&#24378;&#31574;&#30053;&#25193;&#23637;&#20102;SynthSeg&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#23545;&#20581;&#24247;&#32452;&#32455;&#21644;&#30149;&#29702;&#30149;&#21464;&#30340;&#20998;&#21106;&#65292;&#26080;&#38656;&#29305;&#23450;&#24207;&#21015;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#22806;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01946v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#30446;&#21069;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31070;&#32463;&#24433;&#20687;&#35821;&#20041;&#20998;&#21106;&#38656;&#35201;&#39640;&#20998;&#36776;&#29575;&#25195;&#25551;&#21644;&#22823;&#37327;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#36825;&#32473;&#20020;&#24202;&#36866;&#29992;&#24615;&#24102;&#26469;&#20102;&#26174;&#33879;&#38556;&#30861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21512;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#30149;&#21464;&#20998;&#21106;&#20219;&#21153;&#65292;&#25193;&#23637;&#20102;&#24050;&#24314;&#31435;&#30340;SynthSeg&#26041;&#27861;&#30340;&#33021;&#21147;&#65292;&#20197;&#36866;&#24212;&#20855;&#26377;&#30149;&#21464;&#29305;&#23450;&#22686;&#24378;&#31574;&#30053;&#30340;&#22823;&#22411;&#24322;&#36136;&#30149;&#21464;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#20174;&#20581;&#24247;&#21644;&#20013;&#39118;&#25968;&#25454;&#38598;&#27966;&#29983;&#30340;&#26631;&#31614;&#26144;&#23556;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#36825;&#37324;&#28436;&#31034;&#20102;UNet&#26550;&#26500;&#65292;&#20419;&#36827;&#20102;&#20581;&#24247;&#32452;&#32455;&#21644;&#30149;&#29702;&#30149;&#21464;&#30340;&#20998;&#21106;&#65292;&#32780;&#26080;&#38656;&#29305;&#23450;&#20110;&#24207;&#21015;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#38024;&#23545;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#22806;&#65288;OOD&#65289;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#33021;&#65292;&#19982;&#35757;&#32451;&#39046;&#22495;&#20869;&#30340;&#24403;&#21069;&#26041;&#27861;&#30456;&#23218;&#32654;&#65292;&#24182;&#22312;OOD&#25968;&#25454;&#19978;&#26174;&#30528;&#20248;&#20110;&#23427;&#20204;&#12290;&#36825;&#19968;&#36129;&#29486;&#26377;&#26395;&#25512;&#21160;&#21307;&#23398;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01946v1 Announce Type: cross  Abstract: Deep learning-based semantic segmentation in neuroimaging currently requires high-resolution scans and extensive annotated datasets, posing significant barriers to clinical applicability. We present a novel synthetic framework for the task of lesion segmentation, extending the capabilities of the established SynthSeg approach to accommodate large heterogeneous pathologies with lesion-specific augmentation strategies. Our method trains deep learning models, demonstrated here with the UNet architecture, using label maps derived from healthy and stroke datasets, facilitating the segmentation of both healthy tissue and pathological lesions without sequence-specific training data. Evaluated against in-domain and out-of-domain (OOD) datasets, our framework demonstrates robust performance, rivaling current methods within the training domain and significantly outperforming them on OOD data. This contribution holds promise for advancing medical
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#23545;&#27604;&#23398;&#20064;&#35299;&#24320;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#20013;&#28023;&#39532;&#24418;&#29366;&#21464;&#24322;&#30340;&#20851;&#38190;&#28508;&#21464;&#37327;&#65292;&#36229;&#36234;&#20102;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#22312;&#35299;&#24320;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2404.00785</link><description>&lt;p&gt;
&#35299;&#24320;&#28023;&#39532;&#24418;&#29366;&#21464;&#24322;&#20043;&#35868;&#65306;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30740;&#31350;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;
&lt;/p&gt;
&lt;p&gt;
Disentangling Hippocampal Shape Variations: A Study of Neurological Disorders Using Graph Variational Autoencoder with Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#23545;&#27604;&#23398;&#20064;&#35299;&#24320;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#20013;&#28023;&#39532;&#24418;&#29366;&#21464;&#24322;&#30340;&#20851;&#38190;&#28508;&#21464;&#37327;&#65292;&#36229;&#36234;&#20102;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#22312;&#35299;&#24320;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#32508;&#21512;&#30740;&#31350;&#65292;&#19987;&#27880;&#20110;&#22312;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#32972;&#26223;&#19979;&#20174;&#25193;&#25955;&#24352;&#37327;&#25104;&#20687;&#65288;DTI&#65289;&#25968;&#25454;&#38598;&#20013;&#35299;&#24320;&#28023;&#39532;&#24418;&#29366;&#21464;&#24322;&#12290;&#20511;&#21161;&#22686;&#24378;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#21306;&#20998;&#20195;&#34920;&#24180;&#40836;&#21644;&#26159;&#21542;&#24739;&#30149;&#30340;&#20004;&#20010;&#19981;&#21516;&#28508;&#21464;&#37327;&#26469;&#25552;&#39640;&#35299;&#37322;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#28040;&#34701;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19968;&#31995;&#21015;VAE&#26550;&#26500;&#21644;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22686;&#24378;&#30340;&#35299;&#24320;&#33021;&#21147;&#12290;&#36825;&#20010;&#35780;&#20272;&#20351;&#29992;&#20102;&#26469;&#33258;DTI&#28023;&#39532;&#25968;&#25454;&#38598;&#30340;&#21512;&#25104;3D&#29615;&#24418;&#32593;&#26684;&#25968;&#25454;&#21644;&#30495;&#23454;&#30340;3D&#28023;&#39532;&#32593;&#26684;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#30417;&#30563;&#35299;&#24320;&#27169;&#22411;&#22312;&#35299;&#24320;&#20998;&#25968;&#26041;&#38754;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#22914;&#23646;&#24615;&#21644;&#24341;&#23548;VAE&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#21306;&#20998;&#19981;&#21516;&#24180;&#40836;&#32452;&#21644;&#30142;&#30149;&#29366;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00785v1 Announce Type: cross  Abstract: This paper presents a comprehensive study focused on disentangling hippocampal shape variations from diffusion tensor imaging (DTI) datasets within the context of neurological disorders. Leveraging a Graph Variational Autoencoder (VAE) enhanced with Supervised Contrastive Learning, our approach aims to improve interpretability by disentangling two distinct latent variables corresponding to age and the presence of diseases. In our ablation study, we investigate a range of VAE architectures and contrastive loss functions, showcasing the enhanced disentanglement capabilities of our approach. This evaluation uses synthetic 3D torus mesh data and real 3D hippocampal mesh datasets derived from the DTI hippocampal dataset. Our supervised disentanglement model outperforms several state-of-the-art (SOTA) methods like attribute and guided VAEs in terms of disentanglement scores. Our model distinguishes between age groups and disease status in pa
&lt;/p&gt;</description></item><item><title>FSS&#26159;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#21333;&#35270;&#22270;&#20154;&#20307;&#37325;&#24314;&#20013;&#20687;&#32032;&#23545;&#40784;&#38544;&#24335;&#27169;&#22411;&#30340;&#37319;&#26679;&#35757;&#32451;&#26041;&#26696;&#65292;&#36890;&#36807;&#20027;&#21160;&#36866;&#24212;&#34920;&#38754;&#30340;&#21402;&#24230;&#21644;&#22797;&#26434;&#24615;&#65292;&#20197;&#21450;&#21033;&#29992;&#26679;&#26412;&#28857;&#30340;&#27861;&#32447;&#26469;&#25913;&#21892;&#32467;&#26524;&#65292;&#21516;&#26102;&#24341;&#20837;&#32593;&#26684;&#21402;&#24230;&#25439;&#22833;&#20449;&#21495;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#35757;&#32451;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.19197</link><description>&lt;p&gt;
&#32454;&#32467;&#26500;&#24863;&#30693;&#37319;&#26679;: &#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#21333;&#35270;&#22270;&#20154;&#20307;&#37325;&#24314;&#20013;&#20687;&#32032;&#23545;&#40784;&#38544;&#24335;&#27169;&#22411;&#30340;&#37319;&#26679;&#35757;&#32451;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Fine Structure-Aware Sampling: A New Sampling Training Scheme for Pixel-Aligned Implicit Models in Single-View Human Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19197
&lt;/p&gt;
&lt;p&gt;
FSS&#26159;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#21333;&#35270;&#22270;&#20154;&#20307;&#37325;&#24314;&#20013;&#20687;&#32032;&#23545;&#40784;&#38544;&#24335;&#27169;&#22411;&#30340;&#37319;&#26679;&#35757;&#32451;&#26041;&#26696;&#65292;&#36890;&#36807;&#20027;&#21160;&#36866;&#24212;&#34920;&#38754;&#30340;&#21402;&#24230;&#21644;&#22797;&#26434;&#24615;&#65292;&#20197;&#21450;&#21033;&#29992;&#26679;&#26412;&#28857;&#30340;&#27861;&#32447;&#26469;&#25913;&#21892;&#32467;&#26524;&#65292;&#21516;&#26102;&#24341;&#20837;&#32593;&#26684;&#21402;&#24230;&#25439;&#22833;&#20449;&#21495;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20687;&#32032;&#23545;&#40784;&#30340;&#38544;&#24335;&#27169;&#22411;&#65292;&#22914;PIFu&#12289;PIFuHD&#21644;ICON&#65292;&#29992;&#20110;&#21333;&#35270;&#22270;&#30528;&#35013;&#20154;&#20307;&#37325;&#24314;&#12290;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#20351;&#29992;&#37319;&#26679;&#35757;&#32451;&#26041;&#26696;&#36827;&#34892;&#35757;&#32451;&#12290;&#29616;&#26377;&#30340;&#37319;&#26679;&#35757;&#32451;&#26041;&#26696;&#35201;&#20040;&#26080;&#27861;&#25429;&#25417;&#34180;&#34920;&#38754;&#65288;&#22914;&#32819;&#26421;&#12289;&#25163;&#25351;&#65289;&#65292;&#35201;&#20040;&#20250;&#23548;&#33268;&#37325;&#24314;&#32593;&#26684;&#20013;&#30340;&#22122;&#22768;&#20266;&#24433;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32454;&#32467;&#26500;&#24863;&#30693;&#37319;&#26679;&#65288;FSS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#21333;&#35270;&#22270;&#20154;&#20307;&#37325;&#24314;&#20013;&#35757;&#32451;&#20687;&#32032;&#23545;&#40784;&#38544;&#24335;&#27169;&#22411;&#30340;&#37319;&#26679;&#35757;&#32451;&#26041;&#26696;&#12290;FSS&#36890;&#36807;&#20027;&#21160;&#36866;&#24212;&#34920;&#38754;&#30340;&#21402;&#24230;&#21644;&#22797;&#26434;&#24615;&#26469;&#35299;&#20915;&#21069;&#36848;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#19982;&#29616;&#26377;&#30340;&#37319;&#26679;&#35757;&#32451;&#26041;&#26696;&#19981;&#21516;&#65292;FSS&#26174;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#26679;&#26412;&#28857;&#30340;&#27861;&#32447;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#39640;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#20026;&#36827;&#19968;&#27493;&#25913;&#36827;&#35757;&#32451;&#36807;&#31243;&#65292;FSS&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20687;&#32032;&#23545;&#40784;&#38544;&#24335;&#27169;&#22411;&#30340;&#32593;&#26684;&#21402;&#24230;&#25439;&#22833;&#20449;&#21495;&#12290;&#36825;&#20351;&#24471;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21033;&#29992;&#27861;&#32447;&#21464;&#24471;&#35745;&#31639;&#19978;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19197v1 Announce Type: cross  Abstract: Pixel-aligned implicit models, such as PIFu, PIFuHD, and ICON, are used for single-view clothed human reconstruction. These models need to be trained using a sampling training scheme. Existing sampling training schemes either fail to capture thin surfaces (e.g. ears, fingers) or cause noisy artefacts in reconstructed meshes. To address these problems, we introduce Fine Structured-Aware Sampling (FSS), a new sampling training scheme to train pixel-aligned implicit models for single-view human reconstruction. FSS resolves the aforementioned problems by proactively adapting to the thickness and complexity of surfaces. In addition, unlike existing sampling training schemes, FSS shows how normals of sample points can be capitalized in the training process to improve results. Lastly, to further improve the training process, FSS proposes a mesh thickness loss signal for pixel-aligned implicit models. It becomes computationally feasible to int
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#22686;&#24378;&#26159;&#19968;&#31181;&#21033;&#29992;dropout&#25110;PCA&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#36716;&#25442;&#30446;&#26631;&#23618;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#25913;&#21892;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#22312;Transformers&#12289;ResNets&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#31561;&#22522;&#30784;&#27169;&#22411;&#19978;&#65292;&#36890;&#36807;&#28145;&#24230;&#22686;&#24378;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#22312;&#30417;&#30563;&#38382;&#39064;&#19978;&#25928;&#26524;&#30456;&#21453;&#12290;</title><link>https://arxiv.org/abs/2303.14537</link><description>&lt;p&gt;
&#28145;&#24230;&#22686;&#24378;&#65306;&#22312;&#28608;&#27963;&#31354;&#38388;&#20013;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Deep Augmentation: Self-Supervised Learning with Transformations in Activation Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.14537
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#22686;&#24378;&#26159;&#19968;&#31181;&#21033;&#29992;dropout&#25110;PCA&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#36716;&#25442;&#30446;&#26631;&#23618;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#25913;&#21892;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#22312;Transformers&#12289;ResNets&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#31561;&#22522;&#30784;&#27169;&#22411;&#19978;&#65292;&#36890;&#36807;&#28145;&#24230;&#22686;&#24378;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#22312;&#30417;&#30563;&#38382;&#39064;&#19978;&#25928;&#26524;&#30456;&#21453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28145;&#24230;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36749;&#23398;&#25110;PCA&#26469;&#36716;&#25442;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#30446;&#26631;&#23618;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#22270;&#23398;&#20064;&#20013;&#30340;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#26469;&#23637;&#31034;&#28145;&#24230;&#22686;&#24378;&#12290; &#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#23545;&#27604;&#23398;&#20064;&#30340;&#22522;&#30784;&#27169;&#22411;&#20013;&#65292;&#22914;Transformers&#12289;ResNets&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#19978;&#28145;&#24230;&#22686;&#24378;&#33021;&#22815;&#24102;&#26469;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#22312;&#30456;&#24212;&#30340;&#30417;&#30563;&#38382;&#39064;&#19978;&#35266;&#23519;&#21040;&#30456;&#21453;&#30340;&#25928;&#26524;&#12290; &#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#28145;&#24230;&#22686;&#24378;&#20943;&#36731;&#20102;&#23618;&#20043;&#38388;&#30340;&#30456;&#20114;&#36866;&#24212;&#65292;&#21363;"&#23849;&#28291;"&#24418;&#24335;&#30340;&#38382;&#39064;&#12290; &#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#21046;&#23450;&#20102;&#19968;&#31181;&#36873;&#25321;&#30446;&#26631;&#23618;&#30340;&#26041;&#27861;&#65307;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#29992;&#28145;&#24230;&#22686;&#24378;&#23450;&#20301;&#26356;&#28145;&#23618;&#27425;&#30340;&#23618;&#35201;&#20248;&#20110;&#22686;&#24378;&#36755;&#20837;&#25968;&#25454;&#12290; &#36825;&#31181;&#26041;&#27861;&#30340;&#31616;&#21333;&#32593;&#32476;&#21644;&#27169;&#24577;&#26080;&#20851;&#24615;&#20351;&#20854;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.14537v2 Announce Type: replace-cross  Abstract: We introduce Deep Augmentation, an approach to implicit data augmentation using dropout or PCA to transform a targeted layer within a neural network to improve performance and generalization. We demonstrate Deep Augmentation through extensive experiments on contrastive learning tasks in NLP, computer vision, and graph learning. We observe substantial performance gains with Transformers, ResNets, and Graph Neural Networks as the underlying models in contrastive learning, but observe inverse effects on the corresponding supervised problems. Our analysis suggests that Deep Augmentation alleviates co-adaption between layers, a form of "collapse." We use this observation to formulate a method for selecting which layer to target; in particular, our experimentation reveals that targeting deeper layers with Deep Augmentation outperforms augmenting the input data. The simple network- and modality-agnostic nature of this approach enables
&lt;/p&gt;</description></item><item><title>&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;&#26159;&#19968;&#31181;&#22312;&#22797;&#26434;&#25968;&#25454;&#19978;&#36827;&#34892;&#40065;&#26834;&#24615;&#35748;&#35777;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21482;&#22312;&#19968;&#20010;&#23545;&#35937;&#30340;&#23376;&#38598;&#19978;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#65292;&#20197;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#24335;&#25552;&#20379;&#20102;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#21644;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16221</link><description>&lt;p&gt;
&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Randomized Smoothing. (arXiv:2310.16221v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16221
&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;&#26159;&#19968;&#31181;&#22312;&#22797;&#26434;&#25968;&#25454;&#19978;&#36827;&#34892;&#40065;&#26834;&#24615;&#35748;&#35777;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21482;&#22312;&#19968;&#20010;&#23545;&#35937;&#30340;&#23376;&#38598;&#19978;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#65292;&#20197;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#24335;&#25552;&#20379;&#20102;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#21644;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#26159;&#22797;&#26434;&#30340;&#65292;&#36890;&#24120;&#30001;&#21487;&#20998;&#35299;&#20026;&#22810;&#20010;&#23454;&#20307;&#30340;&#23545;&#35937;&#32452;&#25104;&#65288;&#20363;&#22914;&#65292;&#23558;&#22270;&#20687;&#20998;&#35299;&#20026;&#20687;&#32032;&#65292;&#23558;&#22270;&#24418;&#20998;&#35299;&#20026;&#30456;&#20114;&#36830;&#25509;&#30340;&#33410;&#28857;&#65289;&#12290;&#38543;&#26426;&#24179;&#28369;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#22312;&#20854;&#36755;&#20837;&#30340;&#24494;&#23567;&#21464;&#21270;&#19978;&#20855;&#26377;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;-&#36890;&#36807;&#22312;&#20998;&#31867;&#20043;&#21069;&#38543;&#26426;&#28155;&#21152;&#22122;&#22768;&#26469;&#20445;&#35777;&#22810;&#25968;&#25237;&#31080;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#23545;&#25163;&#19981;&#26159;&#20219;&#24847;&#24178;&#25200;&#25972;&#20010;&#23545;&#35937;&#65288;&#20363;&#22914;&#22270;&#20687;&#65289;&#65292;&#32780;&#26159;&#23545;&#35937;&#30340;&#26576;&#20010;&#23454;&#20307;&#30340;&#23376;&#38598;&#65288;&#20363;&#22914;&#20687;&#32032;&#65289;&#26102;&#65292;&#36890;&#36807;&#38543;&#26426;&#24179;&#28369;&#23545;&#36825;&#31181;&#22797;&#26434;&#25968;&#25454;&#36827;&#34892;&#40065;&#26834;&#24615;&#35748;&#35777;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;&#65306;&#25105;&#20204;&#36890;&#36807;&#20165;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#23454;&#20307;&#23376;&#38598;&#19978;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#26469;&#37096;&#20998;&#24179;&#28369;&#23545;&#35937;&#12290;&#36890;&#36807;&#20197;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#24335;&#28155;&#21152;&#22122;&#22768;&#65292;&#25105;&#20204;&#33719;&#24471;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#22122;&#22768;&#20998;&#24067;&#21021;&#22987;&#21270;&#20998;&#23618;&#24179;&#28369;&#65292;&#24471;&#21040;&#20102;&#26032;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world data is complex and often consists of objects that can be decomposed into multiple entities (e.g. images into pixels, graphs into interconnected nodes). Randomized smoothing is a powerful framework for making models provably robust against small changes to their inputs - by guaranteeing robustness of the majority vote when randomly adding noise before classification. Yet, certifying robustness on such complex data via randomized smoothing is challenging when adversaries do not arbitrarily perturb entire objects (e.g. images) but only a subset of their entities (e.g. pixels). As a solution, we introduce hierarchical randomized smoothing: We partially smooth objects by adding random noise only on a randomly selected subset of their entities. By adding noise in a more targeted manner than existing methods we obtain stronger robustness guarantees while maintaining high accuracy. We initialize hierarchical smoothing using different noising distributions, yielding novel robustness
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23450;&#21521;&#36793;&#30028;&#26694;&#21644;&#28145;&#24230;&#32593;&#32476;&#36827;&#34892;&#29289;&#20307;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#29615;&#22659;&#26465;&#20214;&#19979;&#23545;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#21457;&#29616;&#65292;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#23567;&#29289;&#20307;&#26102;&#33021;&#22815;&#25552;&#39640;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.16798</link><description>&lt;p&gt;
&#35780;&#20272;&#29615;&#22659;&#26465;&#20214;&#23545;&#20351;&#29992;&#23450;&#21521;&#36793;&#30028;&#26694;&#36827;&#34892;AR&#24212;&#29992;&#30340;&#29289;&#20307;&#26816;&#27979;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Environmental Conditions on Object Detection using Oriented Bounding Boxes for AR Applications. (arXiv:2306.16798v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23450;&#21521;&#36793;&#30028;&#26694;&#21644;&#28145;&#24230;&#32593;&#32476;&#36827;&#34892;&#29289;&#20307;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#29615;&#22659;&#26465;&#20214;&#19979;&#23545;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#21457;&#29616;&#65292;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#23567;&#29289;&#20307;&#26102;&#33021;&#22815;&#25552;&#39640;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#24378;&#29616;&#23454;&#65288;AR&#65289;&#30340;&#30446;&#26631;&#26159;&#23558;&#25968;&#23383;&#20869;&#23481;&#28155;&#21152;&#21040;&#33258;&#28982;&#22270;&#20687;&#21644;&#35270;&#39057;&#20013;&#65292;&#20197;&#21019;&#24314;&#29992;&#25143;&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#20132;&#20114;&#20307;&#39564;&#12290;&#22330;&#26223;&#20998;&#26512;&#21644;&#29289;&#20307;&#35782;&#21035;&#22312;AR&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#24517;&#39035;&#24555;&#36895;&#19988;&#20934;&#30830;&#22320;&#25191;&#34892;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#23450;&#21521;&#36793;&#30028;&#26694;&#19982;&#26816;&#27979;&#21644;&#35782;&#21035;&#28145;&#24230;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#22788;&#29702;&#26102;&#38388;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20004;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65306;&#19968;&#20010;&#24120;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#30340;&#30495;&#23454;&#22270;&#20687;&#25968;&#25454;&#38598;&#65288;DOTA&#25968;&#25454;&#38598;&#65289;&#21644;&#19968;&#20010;&#27169;&#25311;&#19981;&#21516;&#29615;&#22659;&#12289;&#29031;&#26126;&#21644;&#37319;&#38598;&#26465;&#20214;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#35780;&#20272;&#30340;&#37325;&#28857;&#26159;&#23567;&#29289;&#20307;&#65292;&#36825;&#20123;&#29289;&#20307;&#24448;&#24448;&#38590;&#20197;&#26816;&#27979;&#21644;&#35782;&#21035;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22823;&#22810;&#25968;&#27979;&#35797;&#26465;&#20214;&#19979;&#65292;&#23545;&#20110;&#23567;&#29289;&#20307;&#24448;&#24448;&#33021;&#20135;&#29983;&#26356;&#22909;&#30340;&#24179;&#22343;&#31934;&#24230;&#21644;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of augmented reality (AR) is to add digital content to natural images and videos to create an interactive experience between the user and the environment. Scene analysis and object recognition play a crucial role in AR, as they must be performed quickly and accurately. In this study, a new approach is proposed that involves using oriented bounding boxes with a detection and recognition deep network to improve performance and processing time. The approach is evaluated using two datasets: a real image dataset (DOTA dataset) commonly used for computer vision tasks, and a synthetic dataset that simulates different environmental, lighting, and acquisition conditions. The focus of the evaluation is on small objects, which are difficult to detect and recognise. The results indicate that the proposed approach tends to produce better Average Precision and greater accuracy for small objects in most of the tested conditions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22635;&#20805;&#24335;&#36974;&#30422;&#31574;&#30053;MixMask&#65292;&#22312;Siamese ConvNets&#20013;&#23454;&#29616;&#36974;&#30422;&#21644;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#30340;&#21305;&#37197;&#65292;&#25552;&#39640;&#20102;Siamese ConvNets&#30340;&#24615;&#33021;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.11456</link><description>&lt;p&gt;
MixMask: &#37325;&#26032;&#23457;&#35270;Siamese ConvNets&#30340;&#36974;&#30422;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
MixMask: Revisiting Masking Strategy for Siamese ConvNets. (arXiv:2210.11456v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22635;&#20805;&#24335;&#36974;&#30422;&#31574;&#30053;MixMask&#65292;&#22312;Siamese ConvNets&#20013;&#23454;&#29616;&#36974;&#30422;&#21644;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#30340;&#21305;&#37197;&#65292;&#25552;&#39640;&#20102;Siamese ConvNets&#30340;&#24615;&#33021;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#36827;&#23637;&#23558;Masked Image Modeling&#65288;MIM&#65289;&#21644;Siamese&#32593;&#32476;&#25972;&#21512;&#25104;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#20004;&#31181;&#25216;&#26415;&#30340;&#20248;&#28857;&#12290;&#28982;&#32780;&#65292;&#22312;Siamese ConvNets&#20013;&#24212;&#29992;&#20256;&#32479;&#30340;&#22522;&#20110;&#25830;&#38500;&#30340;&#36974;&#30422;&#31574;&#30053;&#26102;&#65292;&#23384;&#22312;&#19968;&#20123;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#65288;I&#65289;&#22312;&#36830;&#32493;&#22788;&#29702;&#25968;&#25454;&#26102;&#19981;&#33021;&#25918;&#24323;&#19981;&#30456;&#20851;&#30340;&#36974;&#30422;&#21306;&#22495;&#65292;&#23548;&#33268;&#35757;&#32451;&#25928;&#29575;&#20302;&#20110;ViT&#27169;&#22411;;&#65288;II&#65289;&#22522;&#20110;&#25830;&#38500;&#30340;&#36974;&#30422;&#19982;Siamese ConvNets&#20013;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#19981;&#21305;&#37197;&#65292;&#19982;MIM&#26041;&#27861;&#19981;&#21516;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MixMask&#30340;&#22635;&#20805;&#24335;&#36974;&#30422;&#31574;&#30053;&#65292;&#20197;&#38450;&#27490;&#39321;&#33609;&#36974;&#30422;&#26041;&#27861;&#20013;&#22270;&#20687;&#20013;&#30340;&#38543;&#26426;&#36974;&#30422;&#21306;&#22495;&#23548;&#33268;&#20449;&#24687;&#19981;&#23436;&#25972;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#25439;&#22833;&#20989;&#25968;&#35774;&#35745;&#65292;&#32771;&#34385;&#20004;&#20010;&#19981;&#21516;&#28151;&#21512;&#35270;&#22270;&#20043;&#38388;&#30340;&#35821;&#20041;&#36317;&#31163;&#21464;&#21270;&#65292;&#20197;&#36866;&#24212;&#38598;&#25104;&#26550;&#26500;&#24182;&#38450;&#27490;&#36974;&#30422;&#21644;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;MixMask&#26174;&#30528;&#25552;&#39640;&#20102;Siamese ConvNets&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#20960;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in self-supervised learning have integrated Masked Image Modeling (MIM) and Siamese Networks into a unified framework that leverages the benefits of both techniques. However, several issues remain unaddressed when applying conventional erase-based masking with Siamese ConvNets. These include (I) the inability to drop uninformative masked regions in ConvNets as they process data continuously, resulting in low training efficiency compared to ViT models; and (II) the mismatch between erase-based masking and the contrastive-based objective in Siamese ConvNets, which differs from the MIM approach. In this paper, we propose a filling-based masking strategy called MixMask to prevent information incompleteness caused by the randomly erased regions in an image in the vanilla masking method. Furthermore, we introduce a flexible loss function design that considers the semantic distance change between two different mixed views to adapt the integrated architecture and prevent mismat
&lt;/p&gt;</description></item></channel></rss>