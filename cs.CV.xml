<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#21453;&#20107;&#23454;&#26694;&#26550;&#21644;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#25511;&#21046;&#22238;&#28335;&#30340;&#33539;&#22260;&#65292;&#29983;&#25104;&#19982;&#23454;&#38469;&#19990;&#30028;&#30340;&#25968;&#25454;&#20998;&#24067;&#30456;&#21305;&#37197;&#30340;&#33258;&#28982;&#21453;&#20107;&#23454;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#21453;&#20107;&#23454;&#25512;&#29702;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01607</link><description>&lt;p&gt;
&#20855;&#26377;&#24517;&#35201;&#22238;&#28335;&#30340;&#33258;&#28982;&#21453;&#20107;&#23454;
&lt;/p&gt;
&lt;p&gt;
Natural Counterfactuals With Necessary Backtracking
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#21453;&#20107;&#23454;&#26694;&#26550;&#21644;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#25511;&#21046;&#22238;&#28335;&#30340;&#33539;&#22260;&#65292;&#29983;&#25104;&#19982;&#23454;&#38469;&#19990;&#30028;&#30340;&#25968;&#25454;&#20998;&#24067;&#30456;&#21305;&#37197;&#30340;&#33258;&#28982;&#21453;&#20107;&#23454;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#21453;&#20107;&#23454;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#25512;&#29702;&#23545;&#20110;&#20154;&#31867;&#35748;&#30693;&#38750;&#24120;&#37325;&#35201;&#65292;&#23588;&#20854;&#23545;&#20110;&#25552;&#20379;&#35299;&#37322;&#21644;&#20570;&#20986;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;Judea Pearl&#30340;&#30740;&#31350;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#24456;&#20248;&#38597;&#65292;&#20294;&#20854;&#29983;&#25104;&#21453;&#20107;&#23454;&#24773;&#26223;&#24448;&#24448;&#38656;&#35201;&#36807;&#20110;&#33073;&#31163;&#23454;&#38469;&#24773;&#26223;&#30340;&#24178;&#39044;&#65292;&#22240;&#27492;&#38590;&#20197;&#23454;&#26045;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#21453;&#20107;&#23454;&#30340;&#26694;&#26550;&#21644;&#19968;&#31181;&#26681;&#25454;&#23454;&#38469;&#19990;&#30028;&#25968;&#25454;&#20998;&#24067;&#29983;&#25104;&#33258;&#28982;&#21453;&#20107;&#23454;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#23545;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#25913;&#36827;&#65292;&#20801;&#35768;&#23545;&#22240;&#26524;&#21069;&#32622;&#21464;&#37327;&#36827;&#34892;&#25913;&#21464;&#20197;&#26368;&#23567;&#21270;&#19982;&#23454;&#38469;&#24773;&#26223;&#30340;&#20559;&#24046;&#12290;&#20026;&#20102;&#29983;&#25104;&#33258;&#28982;&#21453;&#20107;&#23454;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#28982;&#24615;&#20934;&#21017;&#20801;&#35768;&#20294;&#25511;&#21046;&#22238;&#28335;&#30340;&#33539;&#22260;&#12290;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual reasoning is pivotal in human cognition and especially important for providing explanations and making decisions. While Judea Pearl's influential approach is theoretically elegant, its generation of a counterfactual scenario often requires interventions that are too detached from the real scenarios to be feasible. In response, we propose a framework of natural counterfactuals and a method for generating counterfactuals that are natural with respect to the actual world's data distribution. Our methodology refines counterfactual reasoning, allowing changes in causally preceding variables to minimize deviations from realistic scenarios. To generate natural counterfactuals, we introduce an innovative optimization framework that permits but controls the extent of backtracking with a naturalness criterion. Empirical experiments indicate the effectiveness of our method.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#30340;&#37051;&#22495;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#38480;&#21046;&#22312;&#26368;&#36817;&#30340;&#37051;&#23621;&#20043;&#38388;&#26469;&#38477;&#20302;&#33258;&#27880;&#24847;&#21147;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.04690</link><description>&lt;p&gt;
&#26356;&#24555;&#30340;&#37051;&#22495;&#27880;&#24847;&#21147;: &#22312;&#32447;&#31243;&#22359;&#32423;&#21035;&#20943;&#23569;&#33258;&#27880;&#24847;&#21147;&#30340;O(n^2)&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self Attention at the Threadblock Level
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04690
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#30340;&#37051;&#22495;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#38480;&#21046;&#22312;&#26368;&#36817;&#30340;&#37051;&#23621;&#20043;&#38388;&#26469;&#38477;&#20302;&#33258;&#27880;&#24847;&#21147;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37051;&#22495;&#27880;&#24847;&#21147;&#36890;&#36807;&#38480;&#21046;&#27599;&#20010;&#26631;&#35760;&#30340;&#27880;&#24847;&#21147;&#33539;&#22260;&#20026;&#20854;&#26368;&#36817;&#30340;&#37051;&#23621;&#26469;&#38477;&#20302;&#33258;&#27880;&#24847;&#21147;&#30340;&#25104;&#26412;&#12290;&#35813;&#38480;&#21046;&#30001;&#31383;&#21475;&#22823;&#23567;&#21644;&#25193;&#24352;&#22240;&#23376;&#21442;&#25968;&#21270;&#65292;&#20171;&#20110;&#32447;&#24615;&#25237;&#24433;&#21644;&#33258;&#27880;&#24847;&#21147;&#20043;&#38388;&#32472;&#21046;&#20102;&#21487;&#33021;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#35889;&#12290;&#37051;&#22495;&#27880;&#24847;&#21147;&#65292;&#20197;&#21450;&#26356;&#19968;&#33324;&#22320;&#28369;&#21160;&#31383;&#21475;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#22312;&#22522;&#30784;&#35774;&#26045;&#26041;&#38754;&#38271;&#26399;&#21463;&#21040;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#22312;&#26356;&#39640;&#31209;&#30340;&#31354;&#38388;&#65288;2-D&#21644;3-D&#65289;&#65292;&#20419;&#20351;&#24320;&#21457;&#23450;&#21046;&#20869;&#26680;&#30340;&#21457;&#23637;&#65292;&#36825;&#20123;&#20869;&#26680;&#22312;&#21151;&#33021;&#25110;&#24615;&#33021;&#26041;&#38754;&#21463;&#38480;&#65292;&#22914;&#26524;&#19981;&#26159;&#20004;&#32773;&#37117;&#26377;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#37051;&#22495;&#27880;&#24847;&#21147;&#21487;&#20197;&#34920;&#31034;&#20026;&#25209;&#37327;&#21270;&#30340;GEMM&#38382;&#39064;&#65292;&#31867;&#20284;&#20110;&#26631;&#20934;&#27880;&#24847;&#21147;&#65292;&#24182;&#20026;1-D&#21644;2-D&#37051;&#22495;&#27880;&#24847;&#21147;&#23454;&#29616;&#23427;&#12290;&#19982;&#29616;&#26377;&#30340;&#31616;&#21333;&#20869;&#26680;&#30456;&#27604;&#65292;&#36825;&#20123;&#20869;&#26680;&#24179;&#22343;&#25552;&#20379;&#20102;&#20998;&#21035;&#26159;1-D&#21644;2-D&#37051;&#22495;&#27880;&#24847;&#21147;&#30340;&#20840;&#31934;&#24230;&#24310;&#36831;&#25913;&#36827;&#20998;&#21035;&#20026;895%&#21644;272%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04690v1 Announce Type: cross  Abstract: Neighborhood attention reduces the cost of self attention by restricting each token's attention span to its nearest neighbors. This restriction, parameterized by a window size and dilation factor, draws a spectrum of possible attention patterns between linear projection and self attention. Neighborhood attention, and more generally sliding window attention patterns, have long been bounded by infrastructure, particularly in higher-rank spaces (2-D and 3-D), calling for the development of custom kernels, which have been limited in either functionality, or performance, if not both. In this work, we first show that neighborhood attention can be represented as a batched GEMM problem, similar to standard attention, and implement it for 1-D and 2-D neighborhood attention. These kernels on average provide 895% and 272% improvement in full precision latency compared to existing naive kernels for 1-D and 2-D neighborhood attention respectively. 
&lt;/p&gt;</description></item><item><title>NM-FlowGAN&#26159;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#27491;&#35268;&#21270;&#27969;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#26088;&#22312;&#26356;&#20934;&#30830;&#22320;&#24314;&#27169;sRGB&#22122;&#22768;&#65292;&#24357;&#34917;&#20102;&#21333;&#19968;&#29983;&#25104;&#27169;&#22411;&#22266;&#26377;&#29305;&#24615;&#25152;&#24102;&#26469;&#30340;&#24615;&#33021;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2312.10112</link><description>&lt;p&gt;
NM-FlowGAN: &#22522;&#20110;&#27491;&#35268;&#21270;&#27969;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#28151;&#21512;&#26041;&#27861;&#23545;sRGB&#22122;&#22768;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
NM-FlowGAN: Modeling sRGB Noise with a Hybrid Approach based on Normalizing Flows and Generative Adversarial Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10112
&lt;/p&gt;
&lt;p&gt;
NM-FlowGAN&#26159;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#27491;&#35268;&#21270;&#27969;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#26088;&#22312;&#26356;&#20934;&#30830;&#22320;&#24314;&#27169;sRGB&#22122;&#22768;&#65292;&#24357;&#34917;&#20102;&#21333;&#19968;&#29983;&#25104;&#27169;&#22411;&#22266;&#26377;&#29305;&#24615;&#25152;&#24102;&#26469;&#30340;&#24615;&#33021;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#27169;&#21644;&#21512;&#25104;&#30495;&#23454;&#30340;sRGB&#22122;&#22768;&#23545;&#20110;&#21508;&#31181;&#20302;&#32423;&#21035;&#35270;&#35273;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#20363;&#22914;&#26500;&#24314;&#29992;&#20110;&#35757;&#32451;&#22270;&#20687;&#21435;&#22122;&#31995;&#32479;&#30340;&#25968;&#25454;&#38598;&#12290;&#30495;&#23454;sRGB&#22122;&#22768;&#30340;&#20998;&#24067;&#26497;&#20026;&#22797;&#26434;&#65292;&#24182;&#21463;&#22810;&#31181;&#22240;&#32032;&#24433;&#21709;&#65292;&#20351;&#24471;&#20854;&#20934;&#30830;&#24314;&#27169;&#26497;&#20855;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#37319;&#29992;&#25968;&#25454;&#39537;&#21160;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21644;&#27491;&#35268;&#21270;&#27969;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#30740;&#31350;&#30456;&#27604;&#20256;&#32479;&#30340;&#22122;&#22768;&#24314;&#27169;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;sRGB&#22122;&#22768;&#30340;&#26356;&#20934;&#30830;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27599;&#31181;&#29983;&#25104;&#27169;&#22411;&#30340;&#22266;&#26377;&#29305;&#24615;&#65292;&#23384;&#22312;&#24615;&#33021;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NM-FlowGAN&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;GAN&#21644;&#27491;&#35268;&#21270;&#27969;&#30340;&#20248;&#21183;&#30340;&#28151;&#21512;&#26041;&#27861;&#12290;&#25105;&#20204;&#21516;&#26102;&#37319;&#29992;&#22522;&#20110;&#27491;&#35268;&#21270;&#27969;&#30340;&#20687;&#32032;&#32423;&#22122;&#22768;&#24314;&#27169;&#32593;&#32476;&#65292;&#20197;&#21450;&#22522;&#20110;GAN&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#24314;&#27169;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10112v2 Announce Type: replace-cross  Abstract: Modeling and synthesizing real sRGB noise is crucial for various low-level vision tasks, such as building datasets for training image denoising systems. The distribution of real sRGB noise is highly complex and affected by a multitude of factors, making its accurate modeling extremely challenging. Therefore, recent studies have proposed methods that employ data-driven generative models, such as generative adversarial networks (GAN) and Normalizing Flows. These studies achieve more accurate modeling of sRGB noise compared to traditional noise modeling methods. However, there are performance limitations due to the inherent characteristics of each generative model. To address this issue, we propose NM-FlowGAN, a hybrid approach that exploits the strengths of both GAN and Normalizing Flows. We simultaneously employ a pixel-wise noise modeling network based on Normalizing Flows, and spatial correlation modeling networks based on GAN
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#20351;&#29992;&#29983;&#25104;&#24335;AI&#21487;&#20197;&#28165;&#38500;&#38544;&#24418;&#22270;&#20687;&#27700;&#21360;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23478;&#26063;&#21270;&#20877;&#29983;&#25915;&#20987;&#26041;&#27861;&#12290;&#36890;&#36807;&#24418;&#24335;&#21270;&#35777;&#26126;&#21644;&#23454;&#35777;&#32467;&#26524;&#65292;&#35770;&#25991;&#23637;&#31034;&#20102;&#25152;&#26377;&#38544;&#24418;&#27700;&#21360;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#65292;&#24182;&#38024;&#23545;&#19968;&#31181;&#20855;&#26377;&#24377;&#24615;&#30340;&#27700;&#21360;RivaGAN&#65292;&#20877;&#29983;&#25915;&#20987;&#21487;&#20197;&#21435;&#38500;93-99%&#30340;&#27700;&#21360;&#12290;</title><link>http://arxiv.org/abs/2306.01953</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#24335;AI&#65292;&#35777;&#26126;&#20102;&#38544;&#24418;&#22270;&#20687;&#27700;&#21360;&#26159;&#21487;&#28165;&#38500;&#30340;
&lt;/p&gt;
&lt;p&gt;
Invisible Image Watermarks Are Provably Removable Using Generative AI. (arXiv:2306.01953v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01953
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#20351;&#29992;&#29983;&#25104;&#24335;AI&#21487;&#20197;&#28165;&#38500;&#38544;&#24418;&#22270;&#20687;&#27700;&#21360;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23478;&#26063;&#21270;&#20877;&#29983;&#25915;&#20987;&#26041;&#27861;&#12290;&#36890;&#36807;&#24418;&#24335;&#21270;&#35777;&#26126;&#21644;&#23454;&#35777;&#32467;&#26524;&#65292;&#35770;&#25991;&#23637;&#31034;&#20102;&#25152;&#26377;&#38544;&#24418;&#27700;&#21360;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#65292;&#24182;&#38024;&#23545;&#19968;&#31181;&#20855;&#26377;&#24377;&#24615;&#30340;&#27700;&#21360;RivaGAN&#65292;&#20877;&#29983;&#25915;&#20987;&#21487;&#20197;&#21435;&#38500;93-99%&#30340;&#27700;&#21360;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24418;&#27700;&#21360;&#36890;&#36807;&#23884;&#20837;&#21482;&#26377;&#26435;&#21033;&#25317;&#26377;&#32773;&#21487;&#20197;&#26816;&#27979;&#21040;&#30340;&#38544;&#34255;&#20449;&#24687;&#26469;&#20445;&#25252;&#22270;&#20687;&#30340;&#29256;&#26435;&#12290;&#23427;&#20204;&#36824;&#38450;&#27490;&#20154;&#20204;&#28389;&#29992;&#30001;AI&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23478;&#26063;&#21270;&#20877;&#29983;&#25915;&#20987;&#26469;&#28165;&#38500;&#36825;&#20123;&#38544;&#24418;&#27700;&#21360;&#12290;&#25152;&#25552;&#20986;&#30340;&#25915;&#20987;&#26041;&#27861;&#39318;&#20808;&#21521;&#22270;&#20687;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#26469;&#30772;&#22351;&#27700;&#21360;&#65292;&#28982;&#21518;&#37325;&#24314;&#22270;&#20687;&#12290;&#36825;&#31181;&#26041;&#27861;&#28789;&#27963;&#65292;&#21487;&#20197;&#19982;&#35768;&#22810;&#29616;&#26377;&#30340;&#22270;&#20687;&#38477;&#22122;&#31639;&#27861;&#21644;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;&#25193;&#25955;&#27169;&#22411;&#65289;&#23454;&#20363;&#21270;&#12290;&#36890;&#36807;&#24418;&#24335;&#21270;&#35777;&#26126;&#21644;&#23454;&#35777;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#26377;&#38544;&#24418;&#27700;&#21360;&#37117;&#23481;&#26131;&#21463;&#21040;&#25152;&#25552;&#20986;&#30340;&#25915;&#20987;&#12290;&#23545;&#20110;&#19968;&#20010;&#29305;&#21035;&#26377;&#24377;&#24615;&#30340;&#27700;&#21360;RivaGAN&#65292;&#20877;&#29983;&#25915;&#20987;&#21487;&#20197;&#21435;&#38500;93-99%&#30340;&#38544;&#24418;&#27700;&#21360;&#65292;&#32780;&#22522;&#32447;&#25915;&#20987;&#21482;&#33021;&#21435;&#38500;&#19981;&#36229;&#36807;3%&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#25105;&#20204;&#19981;&#35201;&#27714;&#24102;&#27700;&#21360;&#30340;&#22270;&#20687;&#19982;&#21407;&#22987;&#22270;&#20687;&#30456;&#21516;&#65292;&#20445;&#25345;&#22270;&#20687;&#35821;&#20041;&#30456;&#20284;&#30340;&#27700;&#21360;&#21487;&#33021;&#26159;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Invisible watermarks safeguard images' copyright by embedding hidden messages only detectable by owners. They also prevent people from misusing images, especially those generated by AI models. We propose a family of regeneration attacks to remove these invisible watermarks. The proposed attack method first adds random noise to an image to destroy the watermark and then reconstructs the image. This approach is flexible and can be instantiated with many existing image-denoising algorithms and pre-trained generative models such as diffusion models. Through formal proofs and empirical results, we show that all invisible watermarks are vulnerable to the proposed attack. For a particularly resilient watermark, RivaGAN, regeneration attacks remove 93-99% of the invisible watermarks while the baseline attacks remove no more than 3%. However, if we do not require the watermarked image to look the same as the original one, watermarks that keep the image semantically similar can be an alternative
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#38450;&#24481;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26041;&#26696;RaPiD&#65288;Rapid Plug-in Defender&#65289;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;Transformer&#24494;&#35843;&#26469;&#25552;&#32431;&#23545;&#25239;&#26679;&#26412;&#65292;&#20351;&#20854;&#36924;&#36817;&#28165;&#27905;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.01762</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;Transformer&#29992;&#20110;&#23545;&#25239;&#24615;&#26679;&#26412;&#25552;&#32431;
&lt;/p&gt;
&lt;p&gt;
Pre-trained transformer for adversarial purification. (arXiv:2306.01762v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#38450;&#24481;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26041;&#26696;RaPiD&#65288;Rapid Plug-in Defender&#65289;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;Transformer&#24494;&#35843;&#26469;&#25552;&#32431;&#23545;&#25239;&#26679;&#26412;&#65292;&#20351;&#20854;&#36924;&#36817;&#28165;&#27905;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#34987;&#37096;&#32626;&#20026;&#21508;&#31181;&#26085;&#24120;&#26381;&#21153;&#65292;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#36867;&#36991;&#25915;&#20987;&#26159;&#26368;&#26222;&#36941;&#30340;&#19968;&#31181;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#24120;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#25110;&#21033;&#29992;&#22823;&#37327;&#28165;&#27905;&#25968;&#25454;&#30340;&#30693;&#35782;&#26469;&#22686;&#24378;&#20854;&#20581;&#22766;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#37325;&#26032;&#35757;&#32451;&#21644;&#37096;&#32626;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#23545;&#22312;&#32447;&#26381;&#21153;&#36896;&#25104;&#37325;&#22823;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#24403;&#26816;&#27979;&#21040;&#26576;&#31181;&#25915;&#20987;&#30340;&#23545;&#25239;&#24615;&#20363;&#23376;&#26102;&#65292;&#26381;&#21153;&#25552;&#20379;&#32773;&#21482;&#33021;&#33719;&#24471;&#26377;&#38480;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#32780;&#22823;&#37327;&#30340;&#28165;&#27905;&#25968;&#25454;&#21487;&#33021;&#26080;&#27861;&#33719;&#21462;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#26696;&#65292;&#21517;&#20026;RaPiD&#65288;Rapid Plug-in Defender&#65289;&#65292;&#26088;&#22312;&#24555;&#36895;&#38450;&#24481;&#20855;&#26377;&#23569;&#37327;&#24178;&#20928;&#21644;&#23545;&#25239;&#24615;&#31034;&#20363;&#38480;&#21046;&#30340;&#21407;&#22987;&#26381;&#21153;&#27169;&#22411;&#30340;&#26576;&#31181;&#25915;&#20987;&#12290;&#21463;&#21040;&#39044;&#35757;&#32451;&#27169;&#22411;&#25552;&#20379;&#36716;&#31227;&#23398;&#20064;&#33391;&#22909;&#21021;&#22987;&#21270;&#30340;&#36890;&#29992;&#36235;&#21183;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;Transformer&#26469;&#25552;&#32431;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;&#39044;&#35757;&#32451;&#30340;Transformer&#20316;&#20026;&#27491;&#21017;&#21270;&#22120;&#65292;&#40723;&#21169;&#25552;&#32431;&#21518;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#25509;&#36817;&#28165;&#26224;&#25968;&#25454;&#30340;&#20998;&#24067;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RaPiD&#22312;&#38450;&#24481;&#21508;&#31181;&#20855;&#26377;&#38480;&#25968;&#25454;&#30340;&#25915;&#20987;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With more and more deep neural networks being deployed as various daily services, their reliability is essential. It's frightening that deep neural networks are vulnerable and sensitive to adversarial attacks, the most common one of which for the services is evasion-based. Recent works usually strengthen the robustness by adversarial training or leveraging the knowledge of an amount of clean data. However, in practical terms, retraining and redeploying the model need a large computational budget, leading to heavy losses to the online service. In addition, when adversarial examples of a certain attack are detected, only limited adversarial examples are available for the service provider, while much clean data may not be accessible. Given the mentioned problems, we propose a new scenario, RaPiD (Rapid Plug-in Defender), which is to rapidly defend against a certain attack for the frozen original service model with limitations of few clean and adversarial examples. Motivated by the general
&lt;/p&gt;</description></item></channel></rss>