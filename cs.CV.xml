<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#20107;&#20214;&#30340;&#23545;&#35937;&#26816;&#27979;&#30340;&#28151;&#21512;SNN-ANN&#32593;&#32476;&#65292;&#21253;&#25324;&#20102;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26725;&#25509;&#27169;&#22359;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#31232;&#30095;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.10173</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#20107;&#20214;&#30340;&#23545;&#35937;&#26816;&#27979;&#30340;&#28151;&#21512;SNN-ANN&#32593;&#32476;&#65292;&#20855;&#26377;&#31354;&#38388;&#21644;&#26102;&#38388;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
A Hybrid SNN-ANN Network for Event-based Object Detection with Spatial and Temporal Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10173
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#20107;&#20214;&#30340;&#23545;&#35937;&#26816;&#27979;&#30340;&#28151;&#21512;SNN-ANN&#32593;&#32476;&#65292;&#21253;&#25324;&#20102;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26725;&#25509;&#27169;&#22359;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#31232;&#30095;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#30456;&#26426;&#25552;&#20379;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#21644;&#21160;&#24577;&#33539;&#22260;&#65292;&#20960;&#20046;&#27809;&#26377;&#36816;&#21160;&#27169;&#31946;&#65292;&#38750;&#24120;&#36866;&#21512;&#23545;&#35937;&#26816;&#27979;&#20219;&#21153;&#12290;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#19982;&#20107;&#20214;&#39537;&#21160;&#24863;&#30693;&#25968;&#25454;&#22825;&#29983;&#21305;&#37197;&#65292;&#22312;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#19978;&#33021;&#22815;&#23454;&#29616;&#36229;&#20302;&#21151;&#32791;&#21644;&#20302;&#24310;&#36831;&#25512;&#26029;&#65292;&#32780;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#21017;&#23637;&#31034;&#20986;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#21160;&#24577;&#21644;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20174;&#32780;&#20855;&#26377;&#26356;&#22909;&#30340;&#20219;&#21153;&#24615;&#33021;&#12290;&#28151;&#21512;SNN-ANN&#26041;&#27861;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#33021;&#22815;&#21033;&#29992;SNN&#21644;ANN&#20307;&#31995;&#32467;&#26500;&#30340;&#20248;&#21183;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#28151;&#21512;&#27880;&#24847;&#21147;&#30340;SNN-ANN&#39592;&#24178;&#32593;&#32476;&#65292;&#29992;&#20110;&#20351;&#29992;&#20107;&#20214;&#30456;&#26426;&#36827;&#34892;&#23545;&#35937;&#26816;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;SNN-ANN&#26725;&#25509;&#27169;&#22359;&#65292;&#20174;SNN&#23618;&#20013;&#25429;&#25417;&#31232;&#30095;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20851;&#31995;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#23494;&#38598;&#29305;&#24449;&#22270;&#65292;&#20379;&#39592;&#24178;&#32593;&#32476;&#30340;ANN&#37096;&#20998;&#20351;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;m
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10173v1 Announce Type: cross  Abstract: Event cameras offer high temporal resolution and dynamic range with minimal motion blur, making them promising for object detection tasks. While Spiking Neural Networks (SNNs) are a natural match for event-based sensory data and enable ultra-energy efficient and low latency inference on neuromorphic hardware, Artificial Neural Networks (ANNs) tend to display more stable training dynamics and faster convergence resulting in greater task performance. Hybrid SNN-ANN approaches are a promising alternative, enabling to leverage the strengths of both SNN and ANN architectures. In this work, we introduce the first Hybrid Attention-based SNN-ANN backbone for object detection using event cameras. We propose a novel Attention-based SNN-ANN bridge module to capture sparse spatial and temporal relations from the SNN layer and convert them into dense feature maps for the ANN part of the backbone. Experimental results demonstrate that our proposed m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22270;&#20687;&#25551;&#36848;&#20855;&#20307;&#24615;&#65292;&#29992;&#20110;&#35780;&#20272;&#26631;&#39064;&#25991;&#26412;&#30340;&#20855;&#20307;&#24615;&#21644;&#30456;&#20851;&#24615;&#65292;&#20197;&#24110;&#21161;&#22312;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#38548;&#31163;&#25552;&#20379;&#26368;&#24378;&#20449;&#21495;&#30340;&#26368;&#20855;&#20307;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.01306</link><description>&lt;p&gt;
ICC&#65306;&#29992;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#31579;&#36873;&#30340;&#22270;&#20687;&#25551;&#36848;&#20855;&#20307;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
ICC: Quantifying Image Caption Concreteness for Multimodal Dataset Curation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01306
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22270;&#20687;&#25551;&#36848;&#20855;&#20307;&#24615;&#65292;&#29992;&#20110;&#35780;&#20272;&#26631;&#39064;&#25991;&#26412;&#30340;&#20855;&#20307;&#24615;&#21644;&#30456;&#20851;&#24615;&#65292;&#20197;&#24110;&#21161;&#22312;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#38548;&#31163;&#25552;&#20379;&#26368;&#24378;&#20449;&#21495;&#30340;&#26368;&#20855;&#20307;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01306v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#38024;&#23545;&#37197;&#23545;&#25991;&#26412;-&#22270;&#20687;&#25968;&#25454;&#30340;Web&#35268;&#27169;&#35757;&#32451;&#22312;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#20294;&#25361;&#25112;&#22312;&#37326;&#22806;&#25968;&#25454;&#38598;&#30340;&#39640;&#22122;&#22768;&#29305;&#24615;&#12290;&#26631;&#20934;&#25968;&#25454;&#36807;&#28388;&#26041;&#27861;&#25104;&#21151;&#21435;&#38500;&#20102;&#19981;&#21305;&#37197;&#30340;&#25991;&#26412;-&#22270;&#20687;&#23545;&#65292;&#20294;&#20801;&#35768;&#35821;&#20041;&#30456;&#20851;&#20294;&#38750;&#24120;&#25277;&#35937;&#25110;&#20027;&#35266;&#30340;&#25991;&#26412;&#12290;&#36825;&#20123;&#26041;&#27861;&#32570;&#20047;&#32454;&#31890;&#24230;&#30340;&#33021;&#21147;&#26469;&#38548;&#31163;&#25552;&#20379;&#22312;&#22024;&#26434;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#26368;&#24378;&#20449;&#21495;&#30340;&#26368;&#20855;&#20307;&#26679;&#26412;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22270;&#20687;&#25551;&#36848;&#20855;&#20307;&#24615;&#65292;&#35780;&#20272;&#27809;&#26377;&#22270;&#20687;&#21442;&#32771;&#30340;&#26631;&#39064;&#25991;&#26412;&#20197;&#34913;&#37327;&#20854;&#20855;&#20307;&#24615;&#21644;&#30456;&#20851;&#24615;&#65292;&#20197;&#20379;&#22312;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#34913;&#37327;&#35270;&#35273;-&#35821;&#20041;&#20449;&#24687;&#25439;&#22833;&#30340;&#24378;&#22522;&#30784;&#27169;&#22411;&#26469;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#19982;&#20154;&#31867;&#23545;&#21333;&#35789;&#21644;&#21477;&#23376;&#32423;&#25991;&#26412;&#20855;&#20307;&#24615;&#30340;&#35780;&#20272;&#39640;&#24230;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01306v1 Announce Type: new  Abstract: Web-scale training on paired text-image data is becoming increasingly central to multimodal learning, but is challenged by the highly noisy nature of datasets in the wild. Standard data filtering approaches succeed in removing mismatched text-image pairs, but permit semantically related but highly abstract or subjective text. These approaches lack the fine-grained ability to isolate the most concrete samples that provide the strongest signal for learning in a noisy dataset. In this work, we propose a new metric, image caption concreteness, that evaluates caption text without an image reference to measure its concreteness and relevancy for use in multimodal learning. Our approach leverages strong foundation models for measuring visual-semantic information loss in multimodal representations. We demonstrate that this strongly correlates with human evaluation of concreteness in both single-word and sentence-level texts. Moreover, we show tha
&lt;/p&gt;</description></item></channel></rss>