<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#35821;&#20041;&#20998;&#21106;&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35821;&#20041;&#20998;&#21106;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#22270;&#20687;&#32423;&#32622;&#20449;&#24230;&#27979;&#37327;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;</title><link>https://arxiv.org/abs/2402.10665</link><description>&lt;p&gt;
&#20351;&#29992;&#20107;&#21518;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;&#22312;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#24615;&#33021;&#21450;&#20854;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Selective Prediction for Semantic Segmentation using Post-Hoc Confidence Estimation and Its Performance under Distribution Shift
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#35821;&#20041;&#20998;&#21106;&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35821;&#20041;&#20998;&#21106;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#22270;&#20687;&#32423;&#32622;&#20449;&#24230;&#27979;&#37327;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#20998;&#21106;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#28982;&#32780;&#20854;&#26377;&#25928;&#24615;&#24120;&#24120;&#21463;&#21040;&#39640;&#36136;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#32570;&#20047;&#25152;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#19968;&#20010;&#24120;&#35265;&#31574;&#30053;&#26159;&#21033;&#29992;&#22312;&#19981;&#21516;&#31181;&#32676;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#22914;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23548;&#33268;&#20102;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#22312;&#20852;&#36259;&#31181;&#32676;&#19978;&#34920;&#29616;&#20986;&#38477;&#20302;&#30340;&#24615;&#33021;&#12290;&#22312;&#27169;&#22411;&#38169;&#35823;&#21487;&#33021;&#24102;&#26469;&#37325;&#22823;&#21518;&#26524;&#30340;&#24773;&#20917;&#19979;&#65292;&#36873;&#25321;&#24615;&#39044;&#27979;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#20943;&#36731;&#39118;&#38505;&#12289;&#20943;&#23569;&#23545;&#19987;&#23478;&#30417;&#30563;&#20381;&#36182;&#30340;&#25163;&#27573;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36164;&#28304;&#21294;&#20047;&#29615;&#22659;&#19979;&#35821;&#20041;&#20998;&#21106;&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;&#65292;&#30528;&#37325;&#20110;&#24212;&#29992;&#20110;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#36816;&#34892;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20107;&#21518;&#32622;&#20449;&#24230;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35821;&#20041;&#20998;&#21106;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#22270;&#20687;&#32423;&#32622;&#20449;&#24230;&#27979;&#37327;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10665v1 Announce Type: new  Abstract: Semantic segmentation plays a crucial role in various computer vision applications, yet its efficacy is often hindered by the lack of high-quality labeled data. To address this challenge, a common strategy is to leverage models trained on data from different populations, such as publicly available datasets. This approach, however, leads to the distribution shift problem, presenting a reduced performance on the population of interest. In scenarios where model errors can have significant consequences, selective prediction methods offer a means to mitigate risks and reduce reliance on expert supervision. This paper investigates selective prediction for semantic segmentation in low-resource settings, thus focusing on post-hoc confidence estimators applied to pre-trained models operating under distribution shift. We propose a novel image-level confidence measure tailored for semantic segmentation and demonstrate its effectiveness through expe
&lt;/p&gt;</description></item><item><title>&#22312;DDIM&#26694;&#26550;&#20013;&#20351;&#29992;GMM&#20316;&#20026;&#21453;&#21521;&#36716;&#31227;&#31639;&#23376;&#65292;&#36890;&#36807;&#30697;&#21305;&#37197;&#21487;&#20197;&#33719;&#24471;&#36136;&#37327;&#26356;&#39640;&#30340;&#26679;&#26412;&#12290;&#22312;&#26080;&#26465;&#20214;&#27169;&#22411;&#21644;&#31867;&#26465;&#20214;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#36890;&#36807;FID&#21644;IS&#25351;&#26631;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.04938</link><description>&lt;p&gt;
&#20351;&#29992;&#30697;&#21305;&#37197;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#25913;&#36827;&#20102;DDIM&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Improved DDIM Sampling with Moment Matching Gaussian Mixtures. (arXiv:2311.04938v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.04938
&lt;/p&gt;
&lt;p&gt;
&#22312;DDIM&#26694;&#26550;&#20013;&#20351;&#29992;GMM&#20316;&#20026;&#21453;&#21521;&#36716;&#31227;&#31639;&#23376;&#65292;&#36890;&#36807;&#30697;&#21305;&#37197;&#21487;&#20197;&#33719;&#24471;&#36136;&#37327;&#26356;&#39640;&#30340;&#26679;&#26412;&#12290;&#22312;&#26080;&#26465;&#20214;&#27169;&#22411;&#21644;&#31867;&#26465;&#20214;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#36890;&#36807;FID&#21644;IS&#25351;&#26631;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#22312;Denoising Diffusion Implicit Models (DDIM)&#26694;&#26550;&#20013;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#20316;&#20026;&#21453;&#21521;&#36716;&#31227;&#31639;&#23376;&#65288;&#20869;&#26680;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20174;&#39044;&#35757;&#32451;&#30340;Denoising Diffusion Probabilistic Models (DDPM)&#20013;&#21152;&#36895;&#37319;&#26679;&#30340;&#24191;&#27867;&#24212;&#29992;&#26041;&#27861;&#20043;&#19968;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#32422;&#26463;GMM&#30340;&#21442;&#25968;&#65292;&#21305;&#37197;DDPM&#21069;&#21521;&#36793;&#38469;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#20013;&#24515;&#30697;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#30697;&#21305;&#37197;&#65292;&#21487;&#20197;&#33719;&#24471;&#19982;&#20351;&#29992;&#39640;&#26031;&#26680;&#30340;&#21407;&#22987;DDIM&#30456;&#21516;&#25110;&#26356;&#22909;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;CelebAHQ&#21644;FFHQ&#30340;&#26080;&#26465;&#20214;&#27169;&#22411;&#20197;&#21450;ImageNet&#25968;&#25454;&#38598;&#30340;&#31867;&#26465;&#20214;&#27169;&#22411;&#19978;&#25552;&#20379;&#20102;&#23454;&#39564;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#37319;&#26679;&#27493;&#39588;&#36739;&#23569;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;GMM&#20869;&#26680;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#65292;&#36825;&#26159;&#36890;&#36807;FID&#21644;IS&#25351;&#26631;&#34913;&#37327;&#30340;&#12290;&#20363;&#22914;&#65292;&#22312;ImageNet 256x256&#19978;&#65292;&#20351;&#29992;10&#20010;&#37319;&#26679;&#27493;&#39588;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;FID&#20540;&#20026;...
&lt;/p&gt;
&lt;p&gt;
We propose using a Gaussian Mixture Model (GMM) as reverse transition operator (kernel) within the Denoising Diffusion Implicit Models (DDIM) framework, which is one of the most widely used approaches for accelerated sampling from pre-trained Denoising Diffusion Probabilistic Models (DDPM). Specifically we match the first and second order central moments of the DDPM forward marginals by constraining the parameters of the GMM. We see that moment matching is sufficient to obtain samples with equal or better quality than the original DDIM with Gaussian kernels. We provide experimental results with unconditional models trained on CelebAHQ and FFHQ and class-conditional models trained on ImageNet datasets respectively. Our results suggest that using the GMM kernel leads to significant improvements in the quality of the generated samples when the number of sampling steps is small, as measured by FID and IS metrics. For example on ImageNet 256x256, using 10 sampling steps, we achieve a FID of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MESAHA-Net&#30340;&#39640;&#25928;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#38598;&#25104;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#36755;&#20837;&#65292;&#36890;&#36807;&#37319;&#29992;&#33258;&#36866;&#24212;&#30828;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36880;&#23618;2D&#20998;&#21106;&#65292;&#23454;&#29616;&#20102; CT&#25195;&#25551;&#20013;&#31934;&#30830;&#30340;&#32954;&#32467;&#33410;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2304.01576</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#32534;&#30721;&#22120;&#30340;&#26368;&#22823;&#24378;&#24230;&#25237;&#24433;&#33258;&#36866;&#24212;&#30828;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;CT&#25195;&#25551;&#32954;&#32467;&#33410;&#20998;&#21106; MESAHA-Net&#65288;arXiv&#65306;2304.01576v1 [eess.IV]&#65289;
&lt;/p&gt;
&lt;p&gt;
MESAHA-Net: Multi-Encoders based Self-Adaptive Hard Attention Network with Maximum Intensity Projections for Lung Nodule Segmentation in CT Scan. (arXiv:2304.01576v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01576
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MESAHA-Net&#30340;&#39640;&#25928;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#38598;&#25104;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#36755;&#20837;&#65292;&#36890;&#36807;&#37319;&#29992;&#33258;&#36866;&#24212;&#30828;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36880;&#23618;2D&#20998;&#21106;&#65292;&#23454;&#29616;&#20102; CT&#25195;&#25551;&#20013;&#31934;&#30830;&#30340;&#32954;&#32467;&#33410;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#32954;&#32467;&#33410;&#20998;&#21106;&#23545;&#26089;&#26399;&#32954;&#30284;&#35786;&#26029;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#24739;&#32773;&#30340;&#29983;&#23384;&#29575;&#12290;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#22270;&#20687;&#34987;&#24191;&#27867;&#29992;&#20110;&#32954;&#32467;&#33410;&#20998;&#26512;&#30340;&#26089;&#26399;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#32954;&#32467;&#33410;&#30340;&#24322;&#36136;&#24615;&#65292;&#22823;&#23567;&#22810;&#26679;&#24615;&#20197;&#21450;&#21608;&#22260;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#23545;&#24320;&#21457;&#40065;&#26834;&#30340;&#32467;&#33410;&#20998;&#21106;&#26041;&#27861;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#21363;&#22522;&#20110;&#22810;&#32534;&#30721;&#22120;&#30340;&#33258;&#36866;&#24212;&#30828;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;MESAHA-Net&#65289;&#65292;&#29992;&#20110;CT&#25195;&#25551;&#20013;&#31934;&#30830;&#30340;&#32954;&#32467;&#33410;&#20998;&#21106;&#12290;MESAHA-Net&#21253;&#25324;&#19977;&#20010;&#32534;&#30721;&#36335;&#24452;&#65292;&#19968;&#20010;&#27880;&#24847;&#21147;&#22359;&#21644;&#19968;&#20010;&#35299;&#30721;&#22120;&#22359;&#65292;&#26377;&#21161;&#20110;&#38598;&#25104;&#19977;&#31181;&#31867;&#22411;&#30340;&#36755;&#20837;&#65306;CT&#20999;&#29255;&#34917;&#19969;&#65292;&#21069;&#21521;&#21644;&#21518;&#21521;&#30340;&#26368;&#22823;&#24378;&#24230;&#25237;&#24433;&#65288;MIP&#65289;&#22270;&#20687;&#20197;&#21450;&#21253;&#21547;&#32467;&#33410;&#30340;&#24863;&#20852;&#36259;&#21306;&#22495;&#65288;ROI&#65289;&#25513;&#30721;&#12290;&#36890;&#36807;&#37319;&#29992;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#30828;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;MESAHA-Net&#36880;&#23618;&#25191;&#34892;&#36880;&#23618;2D&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate lung nodule segmentation is crucial for early-stage lung cancer diagnosis, as it can substantially enhance patient survival rates. Computed tomography (CT) images are widely employed for early diagnosis in lung nodule analysis. However, the heterogeneity of lung nodules, size diversity, and the complexity of the surrounding environment pose challenges for developing robust nodule segmentation methods. In this study, we propose an efficient end-to-end framework, the multi-encoder-based self-adaptive hard attention network (MESAHA-Net), for precise lung nodule segmentation in CT scans. MESAHA-Net comprises three encoding paths, an attention block, and a decoder block, facilitating the integration of three types of inputs: CT slice patches, forward and backward maximum intensity projection (MIP) images, and region of interest (ROI) masks encompassing the nodule. By employing a novel adaptive hard attention mechanism, MESAHA-Net iteratively performs slice-by-slice 2D segmentation 
&lt;/p&gt;</description></item></channel></rss>