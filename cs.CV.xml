<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CAMSIC&#30340;&#31435;&#20307;&#22270;&#20687;&#21387;&#32553;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#38754;&#21521;&#20869;&#23481;&#24863;&#30693;&#30340;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#25216;&#26415;&#65292;&#20351;&#24471;&#26080;&#38656;&#39069;&#22806;Transformer&#35299;&#30721;&#22120;&#23601;&#33021;&#25429;&#25417;&#31354;&#38388;&#21644;&#35270;&#24046;&#20381;&#36182;&#20851;&#31995;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#29575;&#22833;&#30495;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.08505</link><description>&lt;p&gt;
&#38754;&#21521;&#20869;&#23481;&#24863;&#30693;&#30340;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#21464;&#21387;&#22120;&#29992;&#20110;&#31435;&#20307;&#22270;&#20687;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Content-aware Masked Image Modeling Transformer for Stereo Image Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08505
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CAMSIC&#30340;&#31435;&#20307;&#22270;&#20687;&#21387;&#32553;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#38754;&#21521;&#20869;&#23481;&#24863;&#30693;&#30340;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#25216;&#26415;&#65292;&#20351;&#24471;&#26080;&#38656;&#39069;&#22806;Transformer&#35299;&#30721;&#22120;&#23601;&#33021;&#25429;&#25417;&#31354;&#38388;&#21644;&#35270;&#24046;&#20381;&#36182;&#20851;&#31995;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#29575;&#22833;&#30495;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#22522;&#20110;&#23398;&#20064;&#30340;&#31435;&#20307;&#22270;&#20687;&#32534;&#35299;&#30721;&#22120;&#37319;&#29992;&#20102;&#22797;&#26434;&#30340;&#36716;&#25442;&#26041;&#27861;&#65292;&#20294;&#22312;&#32534;&#30721;&#28508;&#22312;&#34920;&#31034;&#26102;&#21364;&#37319;&#29992;&#20102;&#20174;&#21333;&#20010;&#22270;&#20687;&#32534;&#35299;&#30721;&#22120;&#23548;&#20986;&#30340;&#31616;&#21333;&#29109;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29109;&#27169;&#22411;&#38590;&#20197;&#26377;&#25928;&#25429;&#25417;&#31435;&#20307;&#22270;&#20687;&#22266;&#26377;&#30340;&#31354;&#38388;-&#35270;&#24046;&#29305;&#24449;&#65292;&#23548;&#33268;&#20122;&#26368;&#20248;&#30340;&#29575;&#22833;&#30495;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CAMSIC&#30340;&#31435;&#20307;&#22270;&#20687;&#21387;&#32553;&#26694;&#26550;&#12290; CAMSIC &#29420;&#31435;&#22320;&#23558;&#27599;&#20010;&#22270;&#20687;&#36716;&#25442;&#20026;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#37319;&#29992;&#24378;&#22823;&#30340;&#26080;&#35299;&#30721;&#22120;&#21464;&#21387;&#22120;&#29109;&#27169;&#22411;&#26469;&#25429;&#25417;&#31354;&#38388;&#21644;&#35270;&#24046;&#20381;&#36182;&#20851;&#31995;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#20869;&#23481;&#24863;&#30693;&#30340;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#38754;&#21521;&#20869;&#23481;&#24863;&#30693;&#30340;MIM&#20419;&#36827;&#20102;&#20808;&#39564;&#20449;&#24687;&#19982;&#20272;&#35745;&#20196;&#29260;&#20043;&#38388;&#30340;&#39640;&#25928;&#21452;&#21521;&#20132;&#20114;&#65292;&#33258;&#28982;&#22320;&#28040;&#38500;&#20102;&#39069;&#22806;&#30340;Transformer&#35299;&#30721;&#22120;&#30340;&#38656;&#27714;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#31435;&#20307;&#22270;&#20687;&#32534;&#35299;&#30721;&#22120;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#29575;&#22833;&#30495;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08505v1 Announce Type: cross  Abstract: Existing learning-based stereo image codec adopt sophisticated transformation with simple entropy models derived from single image codecs to encode latent representations. However, those entropy models struggle to effectively capture the spatial-disparity characteristics inherent in stereo images, which leads to suboptimal rate-distortion results. In this paper, we propose a stereo image compression framework, named CAMSIC. CAMSIC independently transforms each image to latent representation and employs a powerful decoder-free Transformer entropy model to capture both spatial and disparity dependencies, by introducing a novel content-aware masked image modeling (MIM) technique. Our content-aware MIM facilitates efficient bidirectional interaction between prior information and estimated tokens, which naturally obviates the need for an extra Transformer decoder. Experiments show that our stereo image codec achieves state-of-the-art rate-d
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#65292;&#25552;&#20986;&#20102;Design2Code&#20219;&#21153;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#22810;&#27169;&#24577;LLMs&#30452;&#25509;&#23558;&#35270;&#35273;&#35774;&#35745;&#36716;&#25442;&#20026;&#20195;&#30721;&#23454;&#29616;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.03163</link><description>&lt;p&gt;
Design2Code&#65306;&#25105;&#20204;&#31163;&#33258;&#21160;&#21270;&#21069;&#31471;&#24037;&#31243;&#26377;&#22810;&#36828;&#65311;
&lt;/p&gt;
&lt;p&gt;
Design2Code: How Far Are We From Automating Front-End Engineering?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03163
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#65292;&#25552;&#20986;&#20102;Design2Code&#20219;&#21153;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#22810;&#27169;&#24577;LLMs&#30452;&#25509;&#23558;&#35270;&#35273;&#35774;&#35745;&#36716;&#25442;&#20026;&#20195;&#30721;&#23454;&#29616;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#39134;&#29467;&#36827;&#30340;&#36827;&#23637;&#65292;&#23454;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#33021;&#21147;&#12290;&#36825;&#21487;&#20197;&#23454;&#29616;&#19968;&#31181;&#26032;&#30340;&#21069;&#31471;&#24320;&#21457;&#33539;&#24335;&#65292;&#20854;&#20013;&#22810;&#27169;&#24577;LLMs&#21487;&#33021;&#30452;&#25509;&#23558;&#35270;&#35273;&#35774;&#35745;&#36716;&#25442;&#20026;&#20195;&#30721;&#23454;&#29616;&#12290;&#26412;&#25991;&#23558;&#36825;&#19968;&#36807;&#31243;&#24418;&#24335;&#21270;&#20026;Design2Code&#20219;&#21153;&#65292;&#24182;&#36827;&#34892;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#25163;&#21160;&#31574;&#21010;&#20102;&#19968;&#20010;&#21253;&#21547;484&#20010;&#22810;&#26679;&#21270;&#30495;&#23454;&#32593;&#39029;&#30340;&#22522;&#20934;&#27979;&#35797;&#29992;&#20363;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#22871;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#35780;&#20272;&#24403;&#21069;&#22810;&#27169;&#24577;LLMs&#33021;&#21542;&#29983;&#25104;&#30452;&#25509;&#28210;&#26579;&#20026;&#32473;&#23450;&#21442;&#32771;&#32593;&#39029;&#30340;&#20195;&#30721;&#23454;&#29616;&#65292;&#20197;&#36755;&#20837;&#20026;&#23631;&#24149;&#25130;&#22270;&#12290;&#25105;&#20204;&#36824;&#32467;&#21512;&#20102;&#20840;&#38754;&#30340;&#20154;&#24037;&#35780;&#20272;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#22871;&#22810;&#27169;&#24577;&#25552;&#31034;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;GPT-4V&#21644;Gemini Pro Vision&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#19968;&#20010;&#24320;&#28304;&#30340;Design2Code-18B&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03163v1 Announce Type: new  Abstract: Generative AI has made rapid advancements in recent years, achieving unprecedented capabilities in multimodal understanding and code generation. This can enable a new paradigm of front-end development, in which multimodal LLMs might directly convert visual designs into code implementations. In this work, we formalize this as a Design2Code task and conduct comprehensive benchmarking. Specifically, we manually curate a benchmark of 484 diverse real-world webpages as test cases and develop a set of automatic evaluation metrics to assess how well current multimodal LLMs can generate the code implementations that directly render into the given reference webpages, given the screenshots as input. We also complement automatic metrics with comprehensive human evaluations. We develop a suite of multimodal prompting methods and show their effectiveness on GPT-4V and Gemini Pro Vision. We further finetune an open-source Design2Code-18B model that su
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;WiFi&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#23454;&#29616;&#31359;&#22681;&#25104;&#20687;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#23460;&#20869;&#29615;&#22659;&#21487;&#35270;&#21270;&#30417;&#27979;&#21040;&#25151;&#38388;&#36793;&#30028;&#20043;&#22806;&#65292;&#26080;&#38656;&#25668;&#20687;&#26426;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2401.17417</link><description>&lt;p&gt;
&#22522;&#20110;WiFi&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#30340;&#31359;&#22681;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
Through-Wall Imaging based on WiFi Channel State Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;WiFi&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#23454;&#29616;&#31359;&#22681;&#25104;&#20687;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#23460;&#20869;&#29615;&#22659;&#21487;&#35270;&#21270;&#30417;&#27979;&#21040;&#25151;&#38388;&#36793;&#30028;&#20043;&#22806;&#65292;&#26080;&#38656;&#25668;&#20687;&#26426;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;WiFi&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#22312;&#31359;&#22681;&#22330;&#26223;&#20013;&#21512;&#25104;&#22270;&#20687;&#12290;&#21033;&#29992;WiFi&#30340;&#20248;&#21183;&#65292;&#22914;&#25104;&#26412;&#25928;&#30410;&#65292;&#20809;&#29031;&#19981;&#21464;&#24615;&#21644;&#31359;&#22681;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#23460;&#20869;&#29615;&#22659;&#30340;&#21487;&#35270;&#21270;&#30417;&#27979;&#65292;&#36234;&#36807;&#25151;&#38388;&#36793;&#30028;&#65292;&#26080;&#38656;&#25668;&#20687;&#26426;&#12290;&#26356;&#19968;&#33324;&#22320;&#65292;&#23427;&#36890;&#36807;&#35299;&#38145;&#25191;&#34892;&#22522;&#20110;&#22270;&#20687;&#30340;&#19979;&#28216;&#20219;&#21153;&#65288;&#20363;&#22914;&#65292;&#35270;&#35273;&#27963;&#21160;&#35782;&#21035;&#65289;&#30340;&#36873;&#39033;&#65292;&#25552;&#39640;&#20102;WiFi CSI&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#20174;WiFi CSI&#21040;&#22270;&#20687;&#30340;&#36328;&#27169;&#24577;&#36716;&#25442;&#65292;&#25105;&#20204;&#20381;&#36182;&#20110;&#19968;&#20010;&#36866;&#24212;&#25105;&#20204;&#38382;&#39064;&#29305;&#23450;&#30340;&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#26550;&#26500;&#37197;&#32622;&#30340;&#21076;&#38500;&#30740;&#31350;&#21644;&#37325;&#24314;&#22270;&#20687;&#30340;&#23450;&#37327;/&#23450;&#24615;&#35780;&#20272;&#23545;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#31361;&#26174;&#20102;&#20854;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents a seminal approach for synthesizing images from WiFi Channel State Information (CSI) in through-wall scenarios. Leveraging the strengths of WiFi, such as cost-effectiveness, illumination invariance, and wall-penetrating capabilities, our approach enables visual monitoring of indoor environments beyond room boundaries and without the need for cameras. More generally, it improves the interpretability of WiFi CSI by unlocking the option to perform image-based downstream tasks, e.g., visual activity recognition. In order to achieve this crossmodal translation from WiFi CSI to images, we rely on a multimodal Variational Autoencoder (VAE) adapted to our problem specifics. We extensively evaluate our proposed methodology through an ablation study on architecture configuration and a quantitative/qualitative assessment of reconstructed images. Our results demonstrate the viability of our method and highlight its potential for practical applications.
&lt;/p&gt;</description></item><item><title>ScreenQA&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;86K&#20010;&#38382;&#31572;&#23545;&#22312;RICO&#25968;&#25454;&#38598;&#19978;&#27880;&#37322;&#65292;&#26088;&#22312;&#35780;&#20272;&#23631;&#24149;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2209.08199</link><description>&lt;p&gt;
ScreenQA: &#31227;&#21160;&#24212;&#29992;&#25130;&#22270;&#19978;&#30340;&#22823;&#35268;&#27169;&#38382;&#31572;&#23545;
&lt;/p&gt;
&lt;p&gt;
ScreenQA: Large-Scale Question-Answer Pairs over Mobile App Screenshots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.08199
&lt;/p&gt;
&lt;p&gt;
ScreenQA&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;86K&#20010;&#38382;&#31572;&#23545;&#22312;RICO&#25968;&#25454;&#38598;&#19978;&#27880;&#37322;&#65292;&#26088;&#22312;&#35780;&#20272;&#23631;&#24149;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;ScreenQA&#65292;&#29992;&#20110;&#36890;&#36807;&#38382;&#31572;&#26469;&#29702;&#35299;&#23631;&#24149;&#20869;&#23481;&#12290;&#29616;&#26377;&#30340;&#23631;&#24149;&#25968;&#25454;&#38598;&#35201;&#20040;&#20391;&#37325;&#20110;&#32467;&#26500;&#21644;&#32452;&#20214;&#32423;&#21035;&#30340;&#29702;&#35299;&#65292;&#35201;&#20040;&#20391;&#37325;&#20110;&#20687;&#23548;&#33322;&#21644;&#20219;&#21153;&#23436;&#25104;&#20043;&#31867;&#30340;&#26356;&#39640;&#32423;&#21035;&#30340;&#32452;&#21512;&#20219;&#21153;&#12290;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#22312;RICO&#25968;&#25454;&#38598;&#19978;&#27880;&#37322;86K&#20010;&#38382;&#31572;&#23545;&#26469;&#24357;&#21512;&#36825;&#20004;&#32773;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24076;&#26395;&#33021;&#22815;&#22522;&#20934;&#21270;&#23631;&#24149;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.08199v2 Announce Type: replace  Abstract: We present a new task and dataset, ScreenQA, for screen content understanding via question answering. The existing screen datasets are focused either on structure and component-level understanding, or on a much higher-level composite task such as navigation and task completion. We attempt to bridge the gap between these two by annotating 86K question-answer pairs over the RICO dataset in hope to benchmark the screen reading comprehension capacity.
&lt;/p&gt;</description></item><item><title>SITTA&#26159;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#25551;&#36848;&#30340;&#35821;&#20041;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#32447;&#24615;&#26144;&#23556;&#25104;&#21151;&#22320;&#23558;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#20016;&#23500;&#30340;&#35821;&#35328;&#33021;&#21147;&#21644;&#33391;&#22909;&#30340;&#22270;&#20687;-&#35821;&#35328;&#26144;&#23556;&#12290;</title><link>http://arxiv.org/abs/2307.05591</link><description>&lt;p&gt;
SITTA: &#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#25551;&#36848;&#30340;&#35821;&#20041;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SITTA: A Semantic Image-Text Alignment for Image Captioning. (arXiv:2307.05591v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05591
&lt;/p&gt;
&lt;p&gt;
SITTA&#26159;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#25551;&#36848;&#30340;&#35821;&#20041;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#32447;&#24615;&#26144;&#23556;&#25104;&#21151;&#22320;&#23558;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#20016;&#23500;&#30340;&#35821;&#35328;&#33021;&#21147;&#21644;&#33391;&#22909;&#30340;&#22270;&#20687;-&#35821;&#35328;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22270;&#20687;&#30340;&#25991;&#26412;&#21644;&#35821;&#20041;&#29702;&#35299;&#23545;&#20110;&#29983;&#25104;&#36866;&#24403;&#30340;&#25551;&#36848;&#38750;&#24120;&#37325;&#35201;&#12290;&#36825;&#38656;&#35201;&#26816;&#27979;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#65292;&#24314;&#27169;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#35780;&#20272;&#22330;&#26223;&#30340;&#35821;&#20041;&#65292;&#24182;&#23558;&#25552;&#21462;&#30340;&#30693;&#35782;&#34920;&#31034;&#22312;&#35821;&#35328;&#31354;&#38388;&#20013;&#12290;&#20026;&#20102;&#22312;&#20445;&#35777;&#33391;&#22909;&#30340;&#22270;&#20687;-&#35821;&#35328;&#26144;&#23556;&#30340;&#21516;&#26102;&#23454;&#29616;&#20016;&#23500;&#30340;&#35821;&#35328;&#33021;&#21147;&#65292;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#34987;&#26465;&#20214;&#21270;&#20026;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#65288;&#22270;&#20687;-&#25991;&#26412;&#65289;&#27169;&#22411;&#65292;&#20801;&#35768;&#20351;&#29992;&#22270;&#20687;&#36755;&#20837;&#12290;&#36825;&#35201;&#27714;&#23558;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#20013;&#26816;&#27979;&#21040;&#30340;&#35821;&#20041;&#19982;&#29983;&#25104;&#24615;LM&#30340;&#35821;&#35328;&#34920;&#31034;&#36827;&#34892;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#26368;&#22909;&#22320;&#23558;&#35270;&#35273;&#32534;&#30721;&#22120;&#26816;&#27979;&#21040;&#30340;&#35821;&#20041;&#20256;&#36882;&#32473;LM&#36824;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#26500;&#24314;&#32447;&#24615;&#26144;&#23556;&#30340;&#26032;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#23558;&#20004;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#20043;&#38388;&#30340;&#35821;&#20041;&#36716;&#31227;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#23558;&#22810;&#27169;&#24577;&#35821;&#35328;&#32534;&#30721;&#22120;&#30340;&#23884;&#20837;&#31354;&#38388;&#19982;&#29983;&#25104;&#24615;LM&#30340;&#23884;&#20837;&#31354;&#38388;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Textual and semantic comprehension of images is essential for generating proper captions. The comprehension requires detection of objects, modeling of relations between them, an assessment of the semantics of the scene and, finally, representing the extracted knowledge in a language space. To achieve rich language capabilities while ensuring good image-language mappings, pretrained language models (LMs) were conditioned on pretrained multi-modal (image-text) models that allow for image inputs. This requires an alignment of the image representation of the multi-modal model with the language representations of a generative LM. However, it is not clear how to best transfer semantics detected by the vision encoder of the multi-modal model to the LM. We introduce two novel ways of constructing a linear mapping that successfully transfers semantics between the embedding spaces of the two pretrained models. The first aligns the embedding space of the multi-modal language encoder with the embe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20351;&#29992;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#65292;&#37325;&#28857;&#20851;&#27880; BLIP2 &#27169;&#22411;&#65292;&#26469;&#25552;&#39640;&#38646;&#26679;&#26412; VQA &#30340;&#24615;&#33021;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#38382;&#39064;&#27169;&#26495;&#30340;&#26377;&#25928;&#24615;&#12289;&#23569;&#37327;&#26679;&#26412;&#31034;&#20363;&#30340;&#20316;&#29992;&#12289;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#24433;&#21709;&#20197;&#21450;&#23558;&#22270;&#20687;&#26631;&#39064;&#20316;&#20026;&#39069;&#22806;&#35270;&#35273;&#32447;&#32034;&#34701;&#21512;&#30340;&#22909;&#22788;&#12290;&#31934;&#24515;&#35774;&#35745;&#30340;&#38382;&#39064;&#27169;&#26495;&#21644;&#25972;&#21512;&#39069;&#22806;&#35270;&#35273;&#32447;&#32034;&#21487;&#20197;&#20419;&#36827; VQA &#24615;&#33021;&#30340;&#25552;&#39640;&#65292;&#29305;&#21035;&#26159;&#24403;&#23427;&#20204;&#32467;&#21512;&#20351;&#29992;&#26102;&#12290;</title><link>http://arxiv.org/abs/2306.09996</link><description>&lt;p&gt;
&#25506;&#31350;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#35270;&#35273;&#38382;&#31572;&#25552;&#31034;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Investigating Prompting Techniques for Zero- and Few-Shot Visual Question Answering. (arXiv:2306.09996v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20351;&#29992;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#65292;&#37325;&#28857;&#20851;&#27880; BLIP2 &#27169;&#22411;&#65292;&#26469;&#25552;&#39640;&#38646;&#26679;&#26412; VQA &#30340;&#24615;&#33021;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#38382;&#39064;&#27169;&#26495;&#30340;&#26377;&#25928;&#24615;&#12289;&#23569;&#37327;&#26679;&#26412;&#31034;&#20363;&#30340;&#20316;&#29992;&#12289;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#24433;&#21709;&#20197;&#21450;&#23558;&#22270;&#20687;&#26631;&#39064;&#20316;&#20026;&#39069;&#22806;&#35270;&#35273;&#32447;&#32034;&#34701;&#21512;&#30340;&#22909;&#22788;&#12290;&#31934;&#24515;&#35774;&#35745;&#30340;&#38382;&#39064;&#27169;&#26495;&#21644;&#25972;&#21512;&#39069;&#22806;&#35270;&#35273;&#32447;&#32034;&#21487;&#20197;&#20419;&#36827; VQA &#24615;&#33021;&#30340;&#25552;&#39640;&#65292;&#29305;&#21035;&#26159;&#24403;&#23427;&#20204;&#32467;&#21512;&#20351;&#29992;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#20855;&#22791;&#29702;&#35299;&#21644;&#25512;&#29702;&#35270;&#35273;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#36817;&#26399;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#22312;&#38646;&#26679;&#26412;VQA&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#22797;&#26434;&#32452;&#21512;&#38382;&#39064;&#21644;&#36866;&#24212;&#26032;&#39046;&#22495;&#65292;&#22914;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#29702;&#26041;&#38754;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21508;&#31181;&#25552;&#31034;&#31574;&#30053;&#30340;&#20351;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;BLIP2&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;VQA&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;VQA&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#38382;&#39064;&#27169;&#26495;&#30340;&#26377;&#25928;&#24615;&#12289;&#23569;&#37327;&#26679;&#26412;&#31034;&#20363;&#30340;&#20316;&#29992;&#12289;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#24433;&#21709;&#20197;&#21450;&#23558;&#22270;&#20687;&#26631;&#39064;&#20316;&#20026;&#39069;&#22806;&#35270;&#35273;&#32447;&#32034;&#34701;&#21512;&#30340;&#22909;&#22788;&#12290;&#23613;&#31649;&#32467;&#26524;&#21508;&#24322;&#65292;&#20294;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#31934;&#24515;&#35774;&#35745;&#30340;&#38382;&#39064;&#27169;&#26495;&#21644;&#25972;&#21512;&#39069;&#22806;&#35270;&#35273;&#32447;&#32034;&#65288;&#22914;&#22270;&#20687;&#26631;&#39064;&#65289;&#21487;&#20197;&#20419;&#36827;VQA&#24615;&#33021;&#30340;&#25552;&#39640;&#65292;&#29305;&#21035;&#26159;&#24403;&#23427;&#20204;&#32467;&#21512;&#20351;&#29992;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual question answering (VQA) is a challenging task that requires the ability to comprehend and reason with visual information. While recent vision-language models have made strides, they continue to struggle with zero-shot VQA, particularly in handling complex compositional questions and adapting to new domains i.e. knowledge-based reasoning. This paper explores the use of various prompting strategies, focusing on the BLIP2 model, to enhance zero-shot VQA performance. We conduct a comprehensive investigation across several VQA datasets, examining the effectiveness of different question templates, the role of few-shot exemplars, the impact of chain-of-thought (CoT) reasoning, and the benefits of incorporating image captions as additional visual cues. Despite the varied outcomes, our findings demonstrate that carefully designed question templates and the integration of additional visual cues, like image captions, can contribute to improved VQA performance, especially when used in conj
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27969;&#31243;&#65292;&#22312;&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#20013;&#21462;&#24471;&#20102;&#21487;&#38752;&#30340;&#32467;&#26524;&#65292;&#19982;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#30456;&#20284;&#30340;&#27979;&#37327;&#20934;&#30830;&#24230;&#65292;&#24182;&#19988;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#24322;&#24120;&#24515;&#33108;&#22823;&#23567;&#21644;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.04979</link><description>&lt;p&gt;
&#26080;&#26631;&#31614;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Label-free segmentation from cardiac ultrasound using self-supervised learning. (arXiv:2210.04979v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27969;&#31243;&#65292;&#22312;&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#20013;&#21462;&#24471;&#20102;&#21487;&#38752;&#30340;&#32467;&#26524;&#65292;&#19982;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#30456;&#20284;&#30340;&#27979;&#37327;&#20934;&#30830;&#24230;&#65292;&#24182;&#19988;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#24322;&#24120;&#24515;&#33108;&#22823;&#23567;&#21644;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#30340;&#20998;&#21106;&#21644;&#27979;&#37327;&#23545;&#20110;&#24515;&#33039;&#36229;&#22768;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#26159;&#36825;&#20123;&#20219;&#21153;&#32791;&#26102;&#19988;&#38590;&#20197;&#37325;&#29616;&#12290;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#25552;&#20379;&#36741;&#21161;&#65292;&#20294;&#26159;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#20154;&#21147;&#36827;&#34892;&#25163;&#21160;&#26631;&#27880;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27969;&#31243;&#65292;&#32467;&#21512;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#20020;&#24202;&#39046;&#22495;&#30693;&#35782;&#21644;&#28145;&#24230;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;450&#20010;&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#65288;93000&#24352;&#22270;&#29255;&#65289;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#24182;&#22312;8393&#20010;&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#65288;4476266&#24352;&#22270;&#29255;&#65292;&#24179;&#22343;&#24180;&#40836;61&#23681;&#65292;&#22899;&#24615;&#21344;51%&#65289;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21033;&#29992;&#20998;&#21106;&#32467;&#26524;&#36827;&#34892;&#29983;&#29289;&#27979;&#37327;&#12290;&#25105;&#20204;&#36824;&#23545;&#26469;&#33258;&#39069;&#22806;10030&#21517;&#24739;&#32773;&#30340;&#22806;&#37096;&#22270;&#20687;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#36825;&#20123;&#22270;&#20687;&#20855;&#26377;&#25163;&#21160;&#25551;&#36857;&#30340;&#24038;&#23460;&#20449;&#24687;&#12290;&#22312;&#20960;&#31181;&#19981;&#21516;&#30340;&#27979;&#37327;&#25351;&#26631;&#65288;r2 0.56-0.84&#65289;&#19978;&#65292;&#20020;&#24202;&#27979;&#37327;&#21644;&#25105;&#20204;&#30340;&#27969;&#31243;&#39044;&#27979;&#20043;&#38388;&#30340;r2&#20540;&#19982;&#24050;&#25253;&#36947;&#30340;&#20020;&#24202;&#21307;&#29983;&#20043;&#38388;&#30340;&#21464;&#24322;&#31243;&#24230;&#30456;&#20284;&#65292;&#24182;&#19988;&#19982;&#30417;&#30563;&#23398;&#20064;&#30340;&#32467;&#26524;&#30456;&#24403;&#12290;&#26816;&#27979;&#24322;&#24120;&#24515;&#33108;&#22823;&#23567;&#21644;&#21151;&#33021;&#30340;&#24179;&#22343;&#20934;&#30830;&#24230;&#20026;0.85&#65288;&#33539;&#22260;0.71-0.97&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Segmentation and measurement of cardiac chambers is critical in cardiac ultrasound but is laborious and poorly reproducible. Neural networks can assist, but supervised approaches require the same laborious manual annotations. We built a pipeline for self-supervised (no manual labels) segmentation combining computer vision, clinical domain knowledge, and deep learning. We trained on 450 echocardiograms (93,000 images) and tested on 8,393 echocardiograms (4,476,266 images; mean 61 years, 51% female), using the resulting segmentations to calculate biometrics. We also tested against external images from an additional 10,030 patients with available manual tracings of the left ventricle. r2 between clinically measured and pipeline-predicted measurements were similar to reported inter-clinician variation and comparable to supervised learning across several different measurements (r2 0.56-0.84). Average accuracy for detecting abnormal chamber size and function was 0.85 (range 0.71-0.97) compar
&lt;/p&gt;</description></item></channel></rss>