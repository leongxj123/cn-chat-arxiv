<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>LS&#26041;&#27861;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#35757;&#32451;&#20013;&#30340;&#26631;&#31614;&#24179;&#28369;&#25928;&#26524;&#34987;&#21457;&#29616;&#20250;&#36127;&#38754;&#24433;&#21709;&#36873;&#25321;&#24615;&#20998;&#31867;&#65292;&#36890;&#36807;&#24433;&#21709;&#27169;&#22411;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#27492;&#30740;&#31350;&#38416;&#26126;&#20102;&#36825;&#19968;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2403.14715</link><description>&lt;p&gt;
&#29702;&#35299;&#20026;&#20309;&#26631;&#31614;&#24179;&#28369;&#20250;&#38477;&#20302;&#36873;&#25321;&#24615;&#20998;&#31867;&#30340;&#25928;&#26524;&#20197;&#21450;&#22914;&#20309;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Understanding Why Label Smoothing Degrades Selective Classification and How to Fix It
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14715
&lt;/p&gt;
&lt;p&gt;
LS&#26041;&#27861;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#35757;&#32451;&#20013;&#30340;&#26631;&#31614;&#24179;&#28369;&#25928;&#26524;&#34987;&#21457;&#29616;&#20250;&#36127;&#38754;&#24433;&#21709;&#36873;&#25321;&#24615;&#20998;&#31867;&#65292;&#36890;&#36807;&#24433;&#21709;&#27169;&#22411;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#27492;&#30740;&#31350;&#38416;&#26126;&#20102;&#36825;&#19968;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#24179;&#28369;&#65288;LS&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#35757;&#32451;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#22312;&#25552;&#39640;&#27979;&#35797;&#20934;&#30830;&#24615;&#26041;&#38754;&#25928;&#26524;&#26174;&#33879;&#65292;&#24182;&#19988;&#23454;&#29616;&#31616;&#21333;&#12290;"&#30828;"&#30340;one-hot&#26631;&#31614;&#36890;&#36807;&#23558;&#27010;&#29575;&#36136;&#37327;&#22343;&#21248;&#20998;&#37197;&#32473;&#20854;&#20182;&#31867;&#21035;&#26469;&#36827;&#34892;"&#24179;&#28369;&#21270;"&#65292;&#20174;&#32780;&#20943;&#23569;&#36807;&#24230;&#25311;&#21512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;LS&#22914;&#20309;&#36127;&#38754;&#24433;&#21709;&#36873;&#25321;&#24615;&#20998;&#31867;&#65288;SC&#65289;- &#20854;&#30446;&#26631;&#26159;&#21033;&#29992;&#27169;&#22411;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#26469;&#25298;&#32477;&#38169;&#35823;&#20998;&#31867;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#21644;&#26550;&#26500;&#20013;&#20174;&#32463;&#39564;&#19978;&#35777;&#26126;LS&#20250;&#23548;&#33268;SC&#30340;&#19968;&#33268;&#24615;&#38477;&#32423;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;logit&#32423;&#21035;&#30340;&#26799;&#24230;&#26469;&#35299;&#37322;&#36825;&#19968;&#28857;&#65292;&#34920;&#26126;LS&#36890;&#36807;&#22312;&#38169;&#35823;&#27010;&#29575;&#20302;&#26102;&#26356;&#21152;&#27491;&#21017;&#21270;&#26368;&#22823;logit&#65292;&#32780;&#22312;&#38169;&#35823;&#27010;&#29575;&#39640;&#26102;&#26356;&#23569;&#27491;&#21017;&#21270;&#65292;&#21152;&#21095;&#20102;&#36807;&#24230;&#33258;&#20449;&#21644;&#20302;&#33258;&#20449;&#12290;&#36825;&#38416;&#26126;&#20102;&#20197;&#21069;&#25253;&#36947;&#30340;&#24378;&#20998;&#31867;&#22120;&#22312;SC&#20013;&#24615;&#33021;&#19981;&#20339;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14715v1 Announce Type: cross  Abstract: Label smoothing (LS) is a popular regularisation method for training deep neural network classifiers due to its effectiveness in improving test accuracy and its simplicity in implementation. "Hard" one-hot labels are "smoothed" by uniformly distributing probability mass to other classes, reducing overfitting. In this work, we reveal that LS negatively affects selective classification (SC) - where the aim is to reject misclassifications using a model's predictive uncertainty. We first demonstrate empirically across a range of tasks and architectures that LS leads to a consistent degradation in SC. We then explain this by analysing logit-level gradients, showing that LS exacerbates overconfidence and underconfidence by regularising the max logit more when the probability of error is low, and less when the probability of error is high. This elucidates previously reported experimental results where strong classifiers underperform in SC. We
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#35760;&#24518;&#21270;&#34892;&#20026;&#65292;&#21457;&#29616;&#35760;&#24518;&#21270;&#20542;&#21521;&#20110;&#22312;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#21457;&#29983;&#12290;&#36890;&#36807;&#23450;&#20041;&#26377;&#25928;&#27169;&#22411;&#35760;&#24518;&#21270; (EMM) &#36825;&#19968;&#25351;&#26631;&#65292;&#37327;&#21270;&#20102;&#25968;&#25454;&#20998;&#24067;&#21644;&#27169;&#22411;&#37197;&#32622;&#23545;&#35760;&#24518;&#21270;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.02664</link><description>&lt;p&gt;
&#20851;&#20110;&#25193;&#25955;&#27169;&#22411;&#35760;&#24518;&#21270;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Memorization in Diffusion Models. (arXiv:2310.02664v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#35760;&#24518;&#21270;&#34892;&#20026;&#65292;&#21457;&#29616;&#35760;&#24518;&#21270;&#20542;&#21521;&#20110;&#22312;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#21457;&#29983;&#12290;&#36890;&#36807;&#23450;&#20041;&#26377;&#25928;&#27169;&#22411;&#35760;&#24518;&#21270; (EMM) &#36825;&#19968;&#25351;&#26631;&#65292;&#37327;&#21270;&#20102;&#25968;&#25454;&#20998;&#24067;&#21644;&#27169;&#22411;&#37197;&#32622;&#23545;&#35760;&#24518;&#21270;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#20854;&#29983;&#25104;&#26032;&#39062;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#33021;&#21147;&#65292;&#25193;&#25955;&#27169;&#22411;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#20856;&#22411;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#21363;&#21435;&#22122;&#24471;&#20998;&#21305;&#37197;&#65292;&#25193;&#25955;&#27169;&#22411;&#21482;&#33021;&#29983;&#25104;&#22797;&#21046;&#35757;&#32451;&#25968;&#25454;&#30340;&#26679;&#26412;&#65292;&#36825;&#34920;&#26126;&#22312;&#29702;&#35770;&#19978;&#20250;&#20986;&#29616;&#35760;&#24518;&#21270;&#30340;&#34892;&#20026;&#65292;&#36825;&#19982;&#29616;&#26377;&#20808;&#36827;&#25193;&#25955;&#27169;&#22411;&#30340;&#26222;&#36941;&#27867;&#21270;&#33021;&#21147;&#30456;&#30683;&#30462;&#65292;&#22240;&#27492;&#38656;&#35201;&#28145;&#20837;&#29702;&#35299;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#35760;&#24518;&#21270;&#34892;&#20026;&#20542;&#21521;&#20110;&#22312;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#21457;&#29983;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26377;&#25928;&#27169;&#22411;&#35760;&#24518;&#21270;(EMM)&#30340;&#23450;&#20041;&#65292;&#36825;&#26159;&#19968;&#31181;&#34913;&#37327;&#23398;&#20064;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#26368;&#22823;&#25968;&#25454;&#38598;&#19978;&#36817;&#20284;&#20854;&#29702;&#35770;&#26368;&#20248;&#28857;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#24433;&#21709;&#36825;&#20123;&#35760;&#24518;&#21270;&#34892;&#20026;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#37325;&#28857;&#20851;&#27880;&#25968;&#25454;&#20998;&#24067;&#21644;&#27169;&#22411;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to their capacity to generate novel and high-quality samples, diffusion models have attracted significant research interest in recent years. Notably, the typical training objective of diffusion models, i.e., denoising score matching, has a closed-form optimal solution that can only generate training data replicating samples. This indicates that a memorization behavior is theoretically expected, which contradicts the common generalization ability of state-of-the-art diffusion models, and thus calls for a deeper understanding. Looking into this, we first observe that memorization behaviors tend to occur on smaller-sized datasets, which motivates our definition of effective model memorization (EMM), a metric measuring the maximum size of training data at which a learned diffusion model approximates its theoretical optimum. Then, we quantify the impact of the influential factors on these memorization behaviors in terms of EMM, focusing primarily on data distribution, model configuratio
&lt;/p&gt;</description></item></channel></rss>