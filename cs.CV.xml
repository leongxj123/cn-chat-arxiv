<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>FOCIL&#36890;&#36807;&#35757;&#32451;&#38543;&#26426;&#20462;&#21098;&#31232;&#30095;&#23376;&#32593;&#32476;&#23454;&#29616;&#22312;&#32447;&#25345;&#32493;&#31867;&#36882;&#22686;&#23398;&#20064;&#65292;&#22312;&#36991;&#20813;&#23384;&#20648;&#37325;&#25918;&#25968;&#25454;&#30340;&#21516;&#26102;&#26377;&#25928;&#38450;&#27490;&#36951;&#24536;&#12290;</title><link>https://arxiv.org/abs/2403.14684</link><description>&lt;p&gt;
FOCIL: &#36890;&#36807;&#35757;&#32451;&#38543;&#26426;&#20462;&#21098;&#31232;&#30095;&#19987;&#23478;&#36827;&#34892;&#22312;&#32447;&#31867;&#36882;&#22686;&#23398;&#20064;&#30340;&#24494;&#35843;&#21644;&#20923;&#32467;
&lt;/p&gt;
&lt;p&gt;
FOCIL: Finetune-and-Freeze for Online Class Incremental Learning by Training Randomly Pruned Sparse Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14684
&lt;/p&gt;
&lt;p&gt;
FOCIL&#36890;&#36807;&#35757;&#32451;&#38543;&#26426;&#20462;&#21098;&#31232;&#30095;&#23376;&#32593;&#32476;&#23454;&#29616;&#22312;&#32447;&#25345;&#32493;&#31867;&#36882;&#22686;&#23398;&#20064;&#65292;&#22312;&#36991;&#20813;&#23384;&#20648;&#37325;&#25918;&#25968;&#25454;&#30340;&#21516;&#26102;&#26377;&#25928;&#38450;&#27490;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#31867;&#36882;&#22686;&#23398;&#20064;&#65288;CIL&#65289;&#26088;&#22312;&#20174;&#25968;&#25454;&#27969;&#20013;&#33719;&#21462;&#19968;&#31995;&#21015;&#26032;&#31867;&#30340;&#30693;&#35782;&#65292;&#20165;&#20351;&#29992;&#27599;&#20010;&#25968;&#25454;&#28857;&#36827;&#34892;&#19968;&#27425;&#35757;&#32451;&#12290;&#19982;&#31163;&#32447;&#27169;&#24335;&#30456;&#27604;&#65292;&#36825;&#26356;&#21152;&#29616;&#23454;&#65292;&#31163;&#32447;&#27169;&#24335;&#20551;&#23450;&#25152;&#26377;&#26032;&#31867;&#30340;&#25968;&#25454;&#24050;&#32463;&#20934;&#22791;&#22909;&#12290;&#24403;&#21069;&#30340;&#22312;&#32447;CIL&#26041;&#27861;&#23384;&#20648;&#20808;&#21069;&#25968;&#25454;&#30340;&#23376;&#38598;&#65292;&#36825;&#20250;&#22312;&#20869;&#23384;&#21644;&#35745;&#31639;&#26041;&#38754;&#36896;&#25104;&#27785;&#37325;&#30340;&#24320;&#38144;&#65292;&#36824;&#23384;&#22312;&#38544;&#31169;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FOCIL&#30340;&#26032;&#22411;&#22312;&#32447;CIL&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#35757;&#32451;&#38543;&#26426;&#20462;&#21098;&#31232;&#30095;&#23376;&#32593;&#32476;&#19981;&#26029;&#24494;&#35843;&#20027;&#20307;&#31995;&#32467;&#26500;&#65292;&#28982;&#21518;&#20923;&#32467;&#35757;&#32451;&#36830;&#25509;&#20197;&#38450;&#27490;&#36951;&#24536;&#12290;FOCIL&#36824;&#33258;&#36866;&#24212;&#30830;&#23450;&#27599;&#20010;&#20219;&#21153;&#30340;&#31232;&#30095;&#24230;&#32423;&#21035;&#21644;&#23398;&#20064;&#36895;&#29575;&#65292;&#24182;&#30830;&#20445;&#65288;&#20960;&#20046;&#65289;&#38646;&#36951;&#24536;&#36328;&#25152;&#26377;&#20219;&#21153;&#65292;&#19988;&#19981;&#23384;&#20648;&#20219;&#20309;&#37325;&#25918;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14684v1 Announce Type: cross  Abstract: Class incremental learning (CIL) in an online continual learning setting strives to acquire knowledge on a series of novel classes from a data stream, using each data point only once for training. This is more realistic compared to offline modes, where it is assumed that all data from novel class(es) is readily available. Current online CIL approaches store a subset of the previous data which creates heavy overhead costs in terms of both memory and computation, as well as privacy issues. In this paper, we propose a new online CIL approach called FOCIL. It fine-tunes the main architecture continually by training a randomly pruned sparse subnetwork for each task. Then, it freezes the trained connections to prevent forgetting. FOCIL also determines the sparsity level and learning rate per task adaptively and ensures (almost) zero forgetting across all tasks without storing any replay data. Experimental results on 10-Task CIFAR100, 20-Task
&lt;/p&gt;</description></item><item><title>This paper introduces a novel approach called Membership Inference Test (MINT) to empirically assess if specific data was used during the training of AI models. Two MINT architectures based on MLP and CNN are proposed and evaluated on a challenging face recognition task, achieving promising results with up to 90% accuracy.</title><link>https://arxiv.org/abs/2402.09225</link><description>&lt;p&gt;
&#25105;&#30340;&#25968;&#25454;&#22312;&#20320;&#30340;AI&#27169;&#22411;&#20013;&#21527;&#65311;&#36890;&#36807;&#24212;&#29992;&#20110;&#20154;&#33080;&#22270;&#20687;&#30340;&#25104;&#21592;&#25512;&#26029;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Is my Data in your AI Model? Membership Inference Test with Application to Face Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09225
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel approach called Membership Inference Test (MINT) to empirically assess if specific data was used during the training of AI models. Two MINT architectures based on MLP and CNN are proposed and evaluated on a challenging face recognition task, achieving promising results with up to 90% accuracy.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#25104;&#21592;&#25512;&#26029;&#27979;&#35797;&#65288;MINT&#65289;&#65292;&#19968;&#31181;&#29992;&#20110;&#32463;&#39564;&#24615;&#35780;&#20272;&#29305;&#23450;&#25968;&#25454;&#26159;&#21542;&#34987;&#29992;&#20110;&#35757;&#32451;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;MINT&#26550;&#26500;&#65292;&#26088;&#22312;&#23398;&#20064;&#22312;&#32463;&#36807;&#23457;&#35745;&#30340;&#27169;&#22411;&#26292;&#38706;&#20110;&#20854;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#25968;&#25454;&#26102;&#20986;&#29616;&#30340;&#19981;&#21516;&#28608;&#27963;&#27169;&#24335;&#12290;&#31532;&#19968;&#20010;&#26550;&#26500;&#22522;&#20110;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#32593;&#32476;&#65292;&#31532;&#20108;&#20010;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;MINT&#26550;&#26500;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20154;&#33080;&#35782;&#21035;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#32771;&#34385;&#20102;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#20154;&#33080;&#35782;&#21035;&#27169;&#22411;&#12290;&#20351;&#29992;&#20845;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#24211;&#36827;&#34892;&#23454;&#39564;&#65292;&#24635;&#20849;&#21253;&#21547;&#36229;&#36807;2200&#19975;&#24352;&#20154;&#33080;&#22270;&#20687;&#12290;&#26681;&#25454;&#21487;&#29992;&#30340;AI&#27169;&#22411;&#27979;&#35797;&#30340;&#19978;&#19979;&#25991;&#65292;&#32771;&#34385;&#20102;&#19981;&#21516;&#30340;&#23454;&#39564;&#22330;&#26223;&#12290;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#36798;&#21040;&#20102;90%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09225v1 Announce Type: cross Abstract: This paper introduces the Membership Inference Test (MINT), a novel approach that aims to empirically assess if specific data was used during the training of Artificial Intelligence (AI) models. Specifically, we propose two novel MINT architectures designed to learn the distinct activation patterns that emerge when an audited model is exposed to data used during its training process. The first architecture is based on a Multilayer Perceptron (MLP) network and the second one is based on Convolutional Neural Networks (CNNs). The proposed MINT architectures are evaluated on a challenging face recognition task, considering three state-of-the-art face recognition models. Experiments are carried out using six publicly available databases, comprising over 22 million face images in total. Also, different experimental scenarios are considered depending on the context available of the AI model to test. Promising results, up to 90% accuracy, are a
&lt;/p&gt;</description></item><item><title>PuriDefense&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#36890;&#36807;&#20351;&#29992;&#36731;&#37327;&#32423;&#20928;&#21270;&#27169;&#22411;&#36827;&#34892;&#38543;&#26426;&#36335;&#24452;&#20928;&#21270;&#65292;&#20943;&#32531;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#26377;&#25928;&#38450;&#24481;&#40657;&#30418;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2401.10586</link><description>&lt;p&gt;
PuriDefense&#65306;&#29992;&#20110;&#38450;&#24481;&#40657;&#30418;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#30340;&#38543;&#26426;&#23616;&#37096;&#38544;&#24335;&#23545;&#25239;&#20928;&#21270;
&lt;/p&gt;
&lt;p&gt;
PuriDefense: Randomized Local Implicit Adversarial Purification for Defending Black-box Query-based Attacks. (arXiv:2401.10586v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10586
&lt;/p&gt;
&lt;p&gt;
PuriDefense&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#36890;&#36807;&#20351;&#29992;&#36731;&#37327;&#32423;&#20928;&#21270;&#27169;&#22411;&#36827;&#34892;&#38543;&#26426;&#36335;&#24452;&#20928;&#21270;&#65292;&#20943;&#32531;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#26377;&#25928;&#38450;&#24481;&#40657;&#30418;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#30418;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#23545;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#26381;&#21153;&#31995;&#32479;&#26500;&#25104;&#37325;&#22823;&#23041;&#32961;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#32780;&#19981;&#38656;&#35201;&#35775;&#38382;&#30446;&#26631;&#27169;&#22411;&#30340;&#26550;&#26500;&#21644;&#21442;&#25968;&#12290;&#20256;&#32479;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#22914;&#23545;&#25239;&#35757;&#32451;&#12289;&#26799;&#24230;&#25513;&#30422;&#21644;&#36755;&#20837;&#36716;&#25442;&#65292;&#35201;&#20040;&#24102;&#26469;&#24040;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#35201;&#20040;&#25439;&#23475;&#38750;&#23545;&#25239;&#36755;&#20837;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#38450;&#24481;&#26426;&#21046;PuriDefense&#65292;&#22312;&#20302;&#25512;&#29702;&#25104;&#26412;&#30340;&#32423;&#21035;&#19978;&#20351;&#29992;&#36731;&#37327;&#32423;&#20928;&#21270;&#27169;&#22411;&#30340;&#38543;&#26426;&#36335;&#24452;&#20928;&#21270;&#12290;&#36825;&#20123;&#27169;&#22411;&#21033;&#29992;&#23616;&#37096;&#38544;&#24335;&#20989;&#25968;&#24182;&#37325;&#24314;&#33258;&#28982;&#22270;&#20687;&#27969;&#24418;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#23558;&#38543;&#26426;&#24615;&#32435;&#20837;&#20928;&#21270;&#36807;&#31243;&#26469;&#20943;&#32531;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#23545;CIFAR-10&#21644;ImageNet&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#20928;&#21270;&#22120;&#38450;&#24481;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Black-box query-based attacks constitute significant threats to Machine Learning as a Service (MLaaS) systems since they can generate adversarial examples without accessing the target model's architecture and parameters. Traditional defense mechanisms, such as adversarial training, gradient masking, and input transformations, either impose substantial computational costs or compromise the test accuracy of non-adversarial inputs. To address these challenges, we propose an efficient defense mechanism, PuriDefense, that employs random patch-wise purifications with an ensemble of lightweight purification models at a low level of inference cost. These models leverage the local implicit function and rebuild the natural image manifold. Our theoretical analysis suggests that this approach slows down the convergence of query-based attacks by incorporating randomness into purifications. Extensive experiments on CIFAR-10 and ImageNet validate the effectiveness of our proposed purifier-based defen
&lt;/p&gt;</description></item><item><title>StyleNAT&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;transformer&#30340;&#22270;&#20687;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#37051;&#22495;&#27880;&#24847;&#21147;&#65288;NA&#65289;&#26469;&#25429;&#25417;&#23616;&#37096;&#21644;&#20840;&#23616;&#20449;&#24687;&#65292;&#33021;&#22815;&#39640;&#25928;&#28789;&#27963;&#22320;&#36866;&#24212;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;FFHQ-256&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.05770</link><description>&lt;p&gt;
StyleNAT&#65306;&#32473;&#27599;&#20010;&#22836;&#37096;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
StyleNAT: Giving Each Head a New Perspective. (arXiv:2211.05770v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05770
&lt;/p&gt;
&lt;p&gt;
StyleNAT&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;transformer&#30340;&#22270;&#20687;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#37051;&#22495;&#27880;&#24847;&#21147;&#65288;NA&#65289;&#26469;&#25429;&#25417;&#23616;&#37096;&#21644;&#20840;&#23616;&#20449;&#24687;&#65292;&#33021;&#22815;&#39640;&#25928;&#28789;&#27963;&#22320;&#36866;&#24212;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;FFHQ-256&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#29983;&#25104;&#19968;&#30452;&#26159;&#19968;&#20010;&#26082;&#26399;&#26395;&#21448;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#25191;&#34892;&#29983;&#25104;&#20219;&#21153;&#21516;&#26679;&#22256;&#38590;&#12290;&#36890;&#24120;&#65292;&#30740;&#31350;&#20154;&#21592;&#35797;&#22270;&#21019;&#24314;&#19968;&#20010;&#8220;&#19968;&#20992;&#20999;&#8221;&#30340;&#29983;&#25104;&#22120;&#65292;&#22312;&#21442;&#25968;&#31354;&#38388;&#20013;&#65292;&#21363;&#20351;&#26159;&#25130;&#28982;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#20063;&#26377;&#24456;&#23569;&#30340;&#24046;&#24322;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;transformer&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;StyleNAT&#65292;&#26088;&#22312;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#29983;&#25104;&#65292;&#24182;&#20855;&#26377;&#21331;&#36234;&#30340;&#25928;&#29575;&#21644;&#28789;&#27963;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#27169;&#22411;&#26680;&#24515;&#26159;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#26694;&#26550;&#65292;&#23427;&#23558;&#27880;&#24847;&#21147;&#22836;&#37096;&#21010;&#20998;&#20026;&#25429;&#25417;&#23616;&#37096;&#21644;&#20840;&#23616;&#20449;&#24687;&#30340;&#26041;&#24335;&#65292;&#36825;&#26159;&#36890;&#36807;&#20351;&#29992;&#37051;&#22495;&#27880;&#24847;&#21147;&#65288;NA&#65289;&#23454;&#29616;&#30340;&#12290;&#30001;&#20110;&#19981;&#21516;&#30340;&#22836;&#37096;&#33021;&#22815;&#20851;&#27880;&#19981;&#21516;&#30340;&#24863;&#21463;&#37326;&#65292;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#32467;&#21512;&#36825;&#20123;&#20449;&#24687;&#65292;&#24182;&#20197;&#39640;&#24230;&#28789;&#27963;&#30340;&#26041;&#24335;&#36866;&#24212;&#25163;&#22836;&#30340;&#25968;&#25454;&#12290;StyleNAT&#22312;FFHQ-256&#19978;&#33719;&#24471;&#20102;&#26032;&#30340;SOTA FID&#24471;&#20998;2.046 &#65292;&#20987;&#36133;&#20102;&#20197;&#21367;&#31215;&#27169;&#22411;&#65288;&#22914;StyleGAN-XL&#65289;&#21644;transformer&#27169;&#22411;&#65288;&#22914;HIT&#65289;&#20026;&#22522;&#30784;&#30340;&#20808;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image generation has been a long sought-after but challenging task, and performing the generation task in an efficient manner is similarly difficult. Often researchers attempt to create a "one size fits all" generator, where there are few differences in the parameter space for drastically different datasets. Herein, we present a new transformer-based framework, dubbed StyleNAT, targeting high-quality image generation with superior efficiency and flexibility. At the core of our model, is a carefully designed framework that partitions attention heads to capture local and global information, which is achieved through using Neighborhood Attention (NA). With different heads able to pay attention to varying receptive fields, the model is able to better combine this information, and adapt, in a highly flexible manner, to the data at hand. StyleNAT attains a new SOTA FID score on FFHQ-256 with 2.046, beating prior arts with convolutional models such as StyleGAN-XL and transformers such as HIT 
&lt;/p&gt;</description></item></channel></rss>