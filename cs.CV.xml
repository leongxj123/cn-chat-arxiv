<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#30740;&#31350;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#20013;&#24341;&#20837;&#20102;&#21452;&#23398;&#20064;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;DualAdapter&#26041;&#27861;&#65292;&#36890;&#36807;&#27491;&#38754;&#21644;&#36127;&#38754;&#20004;&#26041;&#38754;&#30340;&#21452;&#36335;&#24452;&#36866;&#37197;&#65292;&#21516;&#26102;&#36827;&#34892;&#34917;&#20805;&#27491;&#21521;&#36873;&#25321;&#21644;&#36127;&#21521;&#25490;&#38500;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#25972;&#20307;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12964</link><description>&lt;p&gt;
&#36127;&#24471;&#27491;&#65306;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#21452;&#36335;&#24452;&#36866;&#37197;&#22120;
&lt;/p&gt;
&lt;p&gt;
Negative Yields Positive: Unified Dual-Path Adapter for Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#20013;&#24341;&#20837;&#20102;&#21452;&#23398;&#20064;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;DualAdapter&#26041;&#27861;&#65292;&#36890;&#36807;&#27491;&#38754;&#21644;&#36127;&#38754;&#20004;&#26041;&#38754;&#30340;&#21452;&#36335;&#24452;&#36866;&#37197;&#65292;&#21516;&#26102;&#36827;&#34892;&#34917;&#20805;&#27491;&#21521;&#36873;&#25321;&#21644;&#36127;&#21521;&#25490;&#38500;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#25972;&#20307;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#23637;&#31034;&#20102;&#23398;&#20064;&#24320;&#25918;&#19990;&#30028;&#35270;&#35273;&#34920;&#31034;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#39640;&#25928;&#24494;&#35843;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21019;&#26032;&#22320;&#23558;&#21452;&#23398;&#20064;&#27010;&#24565;&#24341;&#20837;&#24494;&#35843;VLMs&#20013;&#65292;&#21363;&#25105;&#20204;&#19981;&#20165;&#23398;&#20064;&#22270;&#20687;&#26159;&#20160;&#20040;&#65292;&#36824;&#23398;&#20064;&#22270;&#20687;&#19981;&#26159;&#20160;&#20040;&#12290;&#22522;&#20110;&#36825;&#19968;&#27010;&#24565;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;DualAdapter&#26041;&#27861;&#65292;&#20351;VLMs&#33021;&#22815;&#20174;&#27491;&#38754;&#21644;&#36127;&#38754;&#20004;&#26041;&#38754;&#36827;&#34892;&#21452;&#36335;&#24452;&#36866;&#37197;&#65292;&#20165;&#20351;&#29992;&#26377;&#38480;&#30340;&#27880;&#37322;&#26679;&#26412;&#12290;&#22312;&#25512;&#29702;&#38454;&#27573;&#65292;&#25105;&#20204;&#30340;DualAdapter&#36890;&#36807;&#38024;&#23545;&#30446;&#26631;&#31867;&#21035;&#21516;&#26102;&#36827;&#34892;&#34917;&#20805;&#27491;&#21521;&#36873;&#25321;&#21644;&#36127;&#21521;&#25490;&#38500;&#65292;&#23454;&#29616;&#20102;&#32479;&#19968;&#39044;&#27979;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;VLMs&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#25972;&#20307;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#36328;&#36234;15&#20010;&#25968;&#25454;&#38598;&#65292;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;DualAda
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12964v1 Announce Type: cross  Abstract: Recently, large-scale pre-trained Vision-Language Models (VLMs) have demonstrated great potential in learning open-world visual representations, and exhibit remarkable performance across a wide range of downstream tasks through efficient fine-tuning. In this work, we innovatively introduce the concept of dual learning into fine-tuning VLMs, i.e., we not only learn what an image is, but also what an image isn't. Building on this concept, we introduce a novel DualAdapter approach to enable dual-path adaptation of VLMs from both positive and negative perspectives with only limited annotated samples. In the inference stage, our DualAdapter performs unified predictions by simultaneously conducting complementary positive selection and negative exclusion across target classes, thereby enhancing the overall recognition accuracy of VLMs in downstream tasks. Our extensive experimental results across 15 datasets validate that the proposed DualAda
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#25552;&#31034;&#20248;&#21270;&#31639;&#27861;&#65288;RPO&#65289;&#29992;&#20110;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#30772;&#35299;&#25915;&#20987;&#65292;&#36890;&#36807;&#26799;&#24230;&#20248;&#21270;&#26469;&#30830;&#20445;&#36755;&#20986;&#30340;&#26080;&#23475;&#24615;&#65292;&#24182;&#25104;&#21151;&#38477;&#20302;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2401.17263</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#25552;&#31034;&#20248;&#21270;&#29992;&#20110;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#30772;&#35299;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17263
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#25552;&#31034;&#20248;&#21270;&#31639;&#27861;&#65288;RPO&#65289;&#29992;&#20110;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#30772;&#35299;&#25915;&#20987;&#65292;&#36890;&#36807;&#26799;&#24230;&#20248;&#21270;&#26469;&#30830;&#20445;&#36755;&#20986;&#30340;&#26080;&#23475;&#24615;&#65292;&#24182;&#25104;&#21151;&#38477;&#20302;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#20154;&#24037;&#26234;&#33021;&#23545;&#40784;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#25110;&#30772;&#35299;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#23545;&#25163;&#20462;&#25913;&#36755;&#20837;&#25552;&#31034;&#20197;&#35825;&#23548;&#26377;&#23475;&#34892;&#20026;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#38450;&#24481;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#20165;&#20851;&#27880;&#29421;&#31364;&#30340;&#23041;&#32961;&#27169;&#22411;&#65292;&#24182;&#19981;&#33021;&#25552;&#20379;&#24378;&#22823;&#30340;&#38450;&#24481;&#12290;&#20026;&#20102;&#23454;&#29616;&#24378;&#22823;&#30340;&#38450;&#24481;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#29992;&#20110;&#23545;&#25239;&#30772;&#35299;&#25915;&#20987;&#30340;&#23545;&#25239;&#30446;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#40065;&#26834;&#25552;&#31034;&#20248;&#21270;&#65288;RPO&#65289;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#20196;&#29260;&#20248;&#21270;&#26469;&#30830;&#20445;&#36755;&#20986;&#30340;&#26080;&#23475;&#24615;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#26131;&#20110;&#35775;&#38382;&#30340;&#21518;&#32512;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#23545;&#30772;&#35299;&#25915;&#20987;&#30340;&#24378;&#38887;&#24615;&#65292;&#21253;&#25324;&#20248;&#21270;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#30772;&#35299;&#25915;&#20987;&#20197;&#21450;&#26410;&#30693;&#30340;&#30772;&#35299;&#25915;&#20987;&#65292;&#23558;&#25915;&#20987;&#25104;&#21151;&#29575;&#20174;84%&#38477;&#20302;&#21040;8.66%&#65292;&#22312;20&#20010;&#30772;&#35299;&#25915;&#20987;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;RPO&#23545;&#27491;&#24120;LM&#20351;&#29992;&#30340;&#24433;&#21709;&#36739;&#23567;&#65292;&#22312;&#36866;&#24212;&#24615;&#25915;&#20987;&#19979;&#20173;&#28982;&#26377;&#25928;&#65292;&#24182;&#19988;&#21487;&#20197;&#36801;&#31227;&#21040;&#40657;&#30418;&#27169;&#22411;&#20013;&#65292;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite advances in AI alignment, language models (LM) remain vulnerable to adversarial attacks or jailbreaking, in which adversaries modify input prompts to induce harmful behavior. While some defenses have been proposed, they focus on narrow threat models and fall short of a strong defense, which we posit should be effective, universal, and practical. To achieve this, we propose the first adversarial objective for defending LMs against jailbreaking attacks and an algorithm, robust prompt optimization (RPO), that uses gradient-based token optimization to enforce harmless outputs. This results in an easily accessible suffix that significantly improves robustness to both jailbreaks seen during optimization and unknown, held-out jailbreaks, reducing the attack success rate on Starling-7B from 84% to 8.66% across 20 jailbreaks. In addition, we find that RPO has a minor effect on normal LM use, is successful under adaptive attacks, and can transfer to black-box models, reducing the success
&lt;/p&gt;</description></item><item><title>SINCERE&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#21512;&#29702;&#30340;&#30417;&#30563;&#25193;&#23637;&#65292;&#36991;&#20813;&#20102;&#21516;&#19968;&#31867;&#21035;&#30340;&#22270;&#20687;&#30456;&#20114;&#25490;&#26021;&#65292;&#36890;&#36807;&#26356;&#22909;&#22320;&#20998;&#31163;&#19981;&#21516;&#31867;&#21035;&#30340;&#23884;&#20837;&#65292;&#22312;&#20445;&#25345;&#31454;&#20105;&#24615;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2309.14277</link><description>&lt;p&gt;
SINCERE: &#30417;&#30563;&#20449;&#24687;&#22122;&#22768;-&#23545;&#27604;&#20272;&#35745;&#20877;&#23457;
&lt;/p&gt;
&lt;p&gt;
SINCERE: Supervised Information Noise-Contrastive Estimation REvisited
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.14277
&lt;/p&gt;
&lt;p&gt;
SINCERE&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#21512;&#29702;&#30340;&#30417;&#30563;&#25193;&#23637;&#65292;&#36991;&#20813;&#20102;&#21516;&#19968;&#31867;&#21035;&#30340;&#22270;&#20687;&#30456;&#20114;&#25490;&#26021;&#65292;&#36890;&#36807;&#26356;&#22909;&#22320;&#20998;&#31163;&#19981;&#21516;&#31867;&#21035;&#30340;&#23884;&#20837;&#65292;&#22312;&#20445;&#25345;&#31454;&#20105;&#24615;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#65288;InfoNCE&#65289;&#25439;&#22833;&#20989;&#25968;&#30001;&#20110;&#20854;&#24378;&#22823;&#30340;&#23454;&#35777;&#32467;&#26524;&#21644;&#29702;&#35770;&#21160;&#26426;&#65292;&#20026;&#35768;&#22810;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#30417;&#30563;&#23545;&#27604;&#65288;SupCon&#65289;&#25439;&#22833;&#21487;&#25193;&#23637;InfoNCE&#20197;&#20174;&#21487;&#29992;&#31867;&#26631;&#31614;&#20013;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20808;&#21069;&#30340;SupCon&#25439;&#22833;&#20844;&#24335;&#23384;&#22312;&#30097;&#38382;&#30340;&#29702;&#30001;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#20250;&#20419;&#20351;&#26469;&#33258;&#21516;&#19968;&#31867;&#21035;&#30340;&#26576;&#20123;&#22270;&#20687;&#22312;&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#30456;&#20114;&#25490;&#26021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#30417;&#30563;&#20449;&#24687;&#22122;&#22768;-&#23545;&#27604;&#20272;&#35745;&#20877;&#23457;&#65288;SINCERE&#65289;&#25439;&#22833;&#65292;&#20316;&#20026;&#20449;&#24687;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#29702;&#35770;&#19978;&#21512;&#29702;&#30340;&#30417;&#30563;&#25193;&#23637;&#65292;&#23427;&#27704;&#36828;&#19981;&#20250;&#23548;&#33268;&#26469;&#33258;&#21516;&#19968;&#31867;&#21035;&#30340;&#22270;&#20687;&#30456;&#20114;&#25490;&#26021;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;SINCERE&#23548;&#33268;&#19981;&#21516;&#31867;&#21035;&#30340;&#23884;&#20837;&#26356;&#22909;&#22320;&#20998;&#31163;&#65292;&#21516;&#26102;&#23545;&#20110;&#30417;&#30563;&#21644;&#36801;&#31227;&#23398;&#20064;&#25552;&#20379;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#19978;&#30340;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.14277v2 Announce Type: replace-cross  Abstract: The information noise-contrastive estimation (InfoNCE) loss function provides the basis of many self-supervised deep learning methods due to its strong empirical results and theoretic motivation. Previous work suggests a supervised contrastive (SupCon) loss to extend InfoNCE to learn from available class labels. However, in this work we find that the prior SupCon loss formulation has questionable justification because it can encourage some images from the same class to repel one another in the learned embedding space. We propose the Supervised InfoNCE REvisited (SINCERE) loss as a theoretically-justified supervised extension of InfoNCE that never causes images from the same class to repel one another. Experiments show that SINCERE leads to better separation of embeddings from different classes while delivering competitive classification accuracy for supervised and transfer learning. We further show an information-theoretic boun
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;3D&#24314;&#27169;&#65292;&#21046;&#20316;&#20986;&#19982;&#26085;&#24120;&#26381;&#35013;&#32441;&#29702;&#30456;&#20284;&#30340;&#23545;&#25239;&#24615;&#20266;&#35013;&#32441;&#29702;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#35270;&#35282;&#19979;&#36991;&#24320;&#20154;&#29289;&#26816;&#27979;&#65292;&#23454;&#29616;&#33258;&#28982;&#22806;&#35266;&#30340;&#26381;&#35013;&#32441;&#29702;&#12290;</title><link>http://arxiv.org/abs/2307.01778</link><description>&lt;p&gt;
&#36890;&#36807;3D&#24314;&#27169;&#65292;&#23454;&#29616;&#33258;&#28982;&#22806;&#35266;&#30340;&#26381;&#35013;&#32441;&#29702;&#20197;&#36867;&#36991;&#20154;&#29289;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Physically Realizable Natural-Looking Clothing Textures Evade Person Detectors via 3D Modeling. (arXiv:2307.01778v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01778
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;3D&#24314;&#27169;&#65292;&#21046;&#20316;&#20986;&#19982;&#26085;&#24120;&#26381;&#35013;&#32441;&#29702;&#30456;&#20284;&#30340;&#23545;&#25239;&#24615;&#20266;&#35013;&#32441;&#29702;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#35270;&#35282;&#19979;&#36991;&#24320;&#20154;&#29289;&#26816;&#27979;&#65292;&#23454;&#29616;&#33258;&#28982;&#22806;&#35266;&#30340;&#26381;&#35013;&#32441;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21046;&#20316;&#23545;&#25239;&#24615;&#26381;&#35013;&#26469;&#36867;&#36991;&#20154;&#29289;&#26816;&#27979;&#22120;&#65292;&#20294;&#35201;&#20040;&#21482;&#23545;&#38480;&#23450;&#30340;&#35270;&#35282;&#26377;&#25928;&#65292;&#35201;&#20040;&#23545;&#20154;&#31867;&#38750;&#24120;&#26126;&#26174;&#12290;&#25105;&#20204;&#26088;&#22312;&#22522;&#20110;3D&#24314;&#27169;&#26469;&#21046;&#20316;&#23545;&#25239;&#24615;&#30340;&#26381;&#35013;&#32441;&#29702;&#65292;&#36825;&#20010;&#24819;&#27861;&#24050;&#32463;&#34987;&#29992;&#20110;&#21046;&#20316;&#21018;&#24615;&#30340;&#23545;&#25239;&#24615;&#29289;&#20307;&#65292;&#22914;3D&#25171;&#21360;&#30340;&#20044;&#40863;&#12290;&#19982;&#21018;&#24615;&#29289;&#20307;&#19981;&#21516;&#65292;&#20154;&#31867;&#21644;&#26381;&#35013;&#26159;&#38750;&#21018;&#24615;&#30340;&#65292;&#36825;&#23548;&#33268;&#20102;&#22312;&#23454;&#38469;&#21046;&#20316;&#20013;&#30340;&#22256;&#38590;&#12290;&#20026;&#20102;&#21046;&#20316;&#20986;&#30475;&#36215;&#26469;&#33258;&#28982;&#30340;&#23545;&#25239;&#24615;&#26381;&#35013;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#35270;&#35282;&#19979;&#36991;&#24320;&#20154;&#29289;&#26816;&#27979;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31867;&#20284;&#20110;&#26085;&#24120;&#26381;&#35013;&#32441;&#29702;&#20043;&#19968;&#30340;&#23545;&#25239;&#24615;&#20266;&#35013;&#32441;&#29702;&#65288;AdvCaT&#65289;&#65292;&#21363;&#20266;&#35013;&#32441;&#29702;&#12290;&#25105;&#20204;&#21033;&#29992;Voronoi&#22270;&#21644;Gumbel-softmax&#25216;&#24039;&#26469;&#21442;&#25968;&#21270;&#20266;&#35013;&#32441;&#29702;&#65292;&#24182;&#36890;&#36807;3D&#24314;&#27169;&#26469;&#20248;&#21270;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#22686;&#24378;&#31649;&#36947;&#65292;&#23558;&#25299;&#25169;&#21512;&#29702;&#30340;&#25237;&#24433;&#65288;TopoProj&#65289;&#21644;Thin Plate Spline&#65288;TPS&#65289;&#32467;&#21512;&#22312;3D&#32593;&#26684;&#19978;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have proposed to craft adversarial clothes for evading person detectors, while they are either only effective at limited viewing angles or very conspicuous to humans. We aim to craft adversarial texture for clothes based on 3D modeling, an idea that has been used to craft rigid adversarial objects such as a 3D-printed turtle. Unlike rigid objects, humans and clothes are non-rigid, leading to difficulties in physical realization. In order to craft natural-looking adversarial clothes that can evade person detectors at multiple viewing angles, we propose adversarial camouflage textures (AdvCaT) that resemble one kind of the typical textures of daily clothes, camouflage textures. We leverage the Voronoi diagram and Gumbel-softmax trick to parameterize the camouflage textures and optimize the parameters via 3D modeling. Moreover, we propose an efficient augmentation pipeline on 3D meshes combining topologically plausible projection (TopoProj) and Thin Plate Spline (TPS) to narr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20197;&#21450;&#30456;&#20851;&#24212;&#29992;&#65292;&#24635;&#32467;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2303.07909</link><description>&lt;p&gt;
&#29983;&#25104;AI&#20013;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Text-to-image Diffusion Model in Generative AI: A Survey. (arXiv:2303.07909v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20197;&#21450;&#30456;&#20851;&#24212;&#29992;&#65292;&#24635;&#32467;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#22810;&#31181;&#29983;&#25104;&#20219;&#21153;&#20013;&#27969;&#34892;&#30340;&#27169;&#22411;&#12290;&#20316;&#20026;&#19968;&#20010;&#33258;&#21253;&#21547;&#30340;&#24037;&#20316;&#65292;&#26412;&#35843;&#26597;&#20174;&#31616;&#21333;&#20171;&#32461;&#22522;&#26412;&#25193;&#25955;&#27169;&#22411;&#22914;&#20309;&#29992;&#20110;&#22270;&#20687;&#21512;&#25104;&#24320;&#22987;&#65292;&#25509;&#30528;&#26159;&#26465;&#20214;&#25110;&#24341;&#23548;&#22914;&#20309;&#25913;&#36827;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#24635;&#32467;&#20102;&#25991;&#26412;&#26465;&#20214;&#19979;&#30340;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#24182;&#19988;&#36827;&#19968;&#27493;&#24635;&#32467;&#20102;&#25991;&#26412;&#24341;&#23548;&#21019;&#24847;&#29983;&#25104;&#21644;&#22270;&#20687;&#32534;&#36753;&#30340;&#24212;&#29992;&#12290;&#38500;&#20102;&#36804;&#20170;&#20026;&#27490;&#25152;&#21462;&#24471;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#29616;&#26377;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey reviews text-to-image diffusion models in the context that diffusion models have emerged to be popular for a wide range of generative tasks. As a self-contained work, this survey starts with a brief introduction of how a basic diffusion model works for image synthesis, followed by how condition or guidance improves learning. Based on that, we present a review of state-of-the-art methods on text-conditioned image synthesis, i.e., text-to-image. We further summarize applications beyond text-to-image generation: text-guided creative generation and text-guided image editing. Beyond the progress made so far, we discuss existing challenges and promising future directions.
&lt;/p&gt;</description></item></channel></rss>