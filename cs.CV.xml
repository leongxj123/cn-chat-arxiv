<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#22788;&#29702;&#26465;&#20214;&#35774;&#32622;&#22312;EHR&#30340;&#20540;&#21644;&#27979;&#37327;&#19978;&#65292;&#20197;&#25972;&#21512;&#20020;&#24202;&#25104;&#20687;&#21644;&#34920;&#26684;&#25968;&#25454;&#65292;&#26088;&#22312;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#20013;&#30340;&#20114;&#34917;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.13319</link><description>&lt;p&gt;
HyperFusion&#65306;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#24314;&#27169;&#30340;&#22810;&#27169;&#24577;&#25972;&#21512;&#34920;&#26684;&#21644;&#21307;&#23398;&#25104;&#20687;&#25968;&#25454;&#30340;&#36229;&#32593;&#32476;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HyperFusion: A Hypernetwork Approach to Multimodal Integration of Tabular and Medical Imaging Data for Predictive Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13319
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#22788;&#29702;&#26465;&#20214;&#35774;&#32622;&#22312;EHR&#30340;&#20540;&#21644;&#27979;&#37327;&#19978;&#65292;&#20197;&#25972;&#21512;&#20020;&#24202;&#25104;&#20687;&#21644;&#34920;&#26684;&#25968;&#25454;&#65292;&#26088;&#22312;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#20013;&#30340;&#20114;&#34917;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ARXIV: 2403.13319v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;&#25688;&#35201;: &#25972;&#21512;&#21508;&#31181;&#20020;&#24202;&#27169;&#24335;&#65292;&#22914;&#21307;&#23398;&#25104;&#20687;&#21644;&#24739;&#32773;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#33719;&#24471;&#30340;&#34920;&#26684;&#25968;&#25454;&#65292;&#26159;&#29616;&#20195;&#21307;&#30103;&#20445;&#20581;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#22810;&#28304;&#25968;&#25454;&#30340;&#32508;&#21512;&#20998;&#26512;&#21487;&#20197;&#20840;&#38754;&#20102;&#35299;&#24739;&#32773;&#30340;&#29366;&#20917;&#65292;&#24182;&#21487;&#20197;&#22686;&#24378;&#35786;&#26029;&#21644;&#27835;&#30103;&#20915;&#31574;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#19968;&#30452;&#23637;&#31034;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#22320;&#23558;&#21307;&#23398;&#25104;&#20687;&#19982;&#20197;&#25968;&#23383;&#34920;&#26684;&#25968;&#25454;&#34920;&#31034;&#30340;&#20020;&#24202;&#12289;&#20154;&#21475;&#32479;&#35745;&#21644;&#36951;&#20256;&#20449;&#24687;&#36827;&#34892;&#34701;&#21512;&#30340;&#22797;&#26434;&#21162;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#39640;&#24230;&#27963;&#36291;&#30340;&#25345;&#32493;&#30740;&#31350;&#36861;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13319v1 Announce Type: cross  Abstract: The integration of diverse clinical modalities such as medical imaging and the tabular data obtained by the patients' Electronic Health Records (EHRs) is a crucial aspect of modern healthcare. The integrative analysis of multiple sources can provide a comprehensive understanding of a patient's condition and can enhance diagnoses and treatment decisions. Deep Neural Networks (DNNs) consistently showcase outstanding performance in a wide range of multimodal tasks in the medical domain. However, the complex endeavor of effectively merging medical imaging with clinical, demographic and genetic information represented as numerical tabular data remains a highly active and ongoing research pursuit.   We present a novel framework based on hypernetworks to fuse clinical imaging and tabular data by conditioning the image processing on the EHR's values and measurements. This approach aims to leverage the complementary information present in these
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RegionSpot&#30340;&#26032;&#22411;&#12289;&#36890;&#29992;&#19988;&#39640;&#25928;&#30340;&#21306;&#22495;&#35782;&#21035;&#26550;&#26500;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#29702;&#35299;&#26080;&#32422;&#26463;&#22270;&#20687;&#20013;&#21306;&#22495;&#30340;&#35821;&#20041;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2311.01373</link><description>&lt;p&gt;
&#35748;&#35777;&#20219;&#20309;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
Recognize Any Regions. (arXiv:2311.01373v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RegionSpot&#30340;&#26032;&#22411;&#12289;&#36890;&#29992;&#19988;&#39640;&#25928;&#30340;&#21306;&#22495;&#35782;&#21035;&#26550;&#26500;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#29702;&#35299;&#26080;&#32422;&#26463;&#22270;&#20687;&#20013;&#21306;&#22495;&#30340;&#35821;&#20041;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#26080;&#32422;&#26463;&#22270;&#20687;&#20013;&#21508;&#20010;&#21306;&#22495;&#25110;&#22359;&#30340;&#35821;&#20041;&#65292;&#20363;&#22914;&#22312;&#24320;&#25918;&#19990;&#30028;&#29289;&#20307;&#26816;&#27979;&#20013;&#65292;&#20195;&#34920;&#20102;&#19968;&#39033;&#20851;&#38190;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#12290;&#22312;&#22522;&#20110;&#24378;&#22823;&#30340;&#22270;&#20687;&#32423;&#35270;&#35273;&#35821;&#35328;&#65288;ViL&#65289;&#22522;&#30784;&#27169;&#22411;&#22914;CLIP&#30340;&#25104;&#21151;&#22522;&#30784;&#19978;&#65292;&#26368;&#36817;&#30340;&#21162;&#21147;&#35201;&#20040;&#36890;&#36807;&#20351;&#29992;&#24191;&#27867;&#30340;&#21306;&#22495;-&#26631;&#31614;&#23545;&#38598;&#21512;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#23545;&#27604;&#27169;&#22411;&#65292;&#35201;&#20040;&#23558;&#26816;&#27979;&#27169;&#22411;&#30340;&#36755;&#20986;&#19982;&#21306;&#22495;&#24314;&#35758;&#30340;&#22270;&#20687;&#32423;&#34920;&#31034;&#23545;&#40784;&#65292;&#20197;&#21457;&#25381;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#37117;&#21463;&#21040;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#35757;&#32451;&#38656;&#27714;&#12289;&#25968;&#25454;&#22122;&#22768;&#30340;&#24433;&#21709;&#20197;&#21450;&#29615;&#22659;&#20449;&#24687;&#30340;&#19981;&#36275;&#31561;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#29616;&#25104;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#21327;&#21516;&#28508;&#21147;&#65292;&#21033;&#29992;&#23427;&#20204;&#22312;&#23450;&#20301;&#21644;&#35821;&#20041;&#26041;&#38754;&#30340;&#21508;&#33258;&#20248;&#21183;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#36890;&#29992;&#30340;&#12289;&#39640;&#25928;&#30340;&#21306;&#22495;&#35782;&#21035;&#26550;&#26500;&#65292;&#31216;&#20026;RegionSpot&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the semantics of individual regions or patches within unconstrained images, such as in open-world object detection, represents a critical yet challenging task in computer vision. Building on the success of powerful image-level vision-language (ViL) foundation models like CLIP, recent efforts have sought to harness their capabilities by either training a contrastive model from scratch with an extensive collection of region-label pairs or aligning the outputs of a detection model with image-level representations of region proposals. Despite notable progress, these approaches are plagued by computationally intensive training requirements, susceptibility to data noise, and deficiency in contextual information. To address these limitations, we explore the synergistic potential of off-the-shelf foundation models, leveraging their respective strengths in localization and semantics. We introduce a novel, generic, and efficient region recognition architecture, named RegionSpot, de
&lt;/p&gt;</description></item></channel></rss>