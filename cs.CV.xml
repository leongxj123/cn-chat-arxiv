<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>TT-BLIP&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;BLIP&#21644;Tri-Transformer&#25216;&#26415;&#65292;&#32467;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#25552;&#21462;&#65292;&#37319;&#29992;Multimodal Tri-Transformer&#34701;&#21512;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#22686;&#24378;&#30340;&#32508;&#21512;&#34920;&#24449;&#21644;&#25913;&#36827;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.12481</link><description>&lt;p&gt;
TT-BLIP&#65306;&#20351;&#29992;BLIP&#21644;Tri-Transformer&#22686;&#24378;&#20551;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
TT-BLIP: Enhancing Fake News Detection Using BLIP and Tri-Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12481
&lt;/p&gt;
&lt;p&gt;
TT-BLIP&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;BLIP&#21644;Tri-Transformer&#25216;&#26415;&#65292;&#32467;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#25552;&#21462;&#65292;&#37319;&#29992;Multimodal Tri-Transformer&#34701;&#21512;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#22686;&#24378;&#30340;&#32508;&#21512;&#34920;&#24449;&#21644;&#25913;&#36827;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12481v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;   &#25688;&#35201;&#65306;&#26816;&#27979;&#20551;&#26032;&#38395;&#21463;&#21040;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;&#26041;&#27861;&#23558;&#29420;&#31435;&#32534;&#30721;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#36827;&#34892;&#20018;&#32852;&#65292;&#24573;&#30053;&#20102;&#32508;&#21512;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#22909;&#22788;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#25991;&#26412;&#21644;&#22270;&#20687;&#32570;&#20047;&#19987;&#38376;&#30340;&#29305;&#24449;&#25552;&#21462;&#36827;&#19968;&#27493;&#38480;&#21046;&#20102;&#36825;&#20123;&#26041;&#27861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;TT-BLIP&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23545;&#19977;&#31181;&#31867;&#22411;&#30340;&#20449;&#24687;&#24212;&#29992;&#20102;&#24341;&#23548;&#24335;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#29992;&#20110;&#32479;&#19968;&#30340;&#35270;&#35273;-&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#65288;BLIP&#65289;&#65306;BERT &#21644; BLIP\textsubscript{Txt} &#29992;&#20110;&#25991;&#26412;&#65292;ResNet &#21644; BLIP\textsubscript{Img} &#29992;&#20110;&#22270;&#20687;&#65292;&#20197;&#21450;&#29992;&#20110;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#21452;&#21521; BLIP &#32534;&#30721;&#22120;&#12290;&#22810;&#27169;&#24577;&#19977;&#35282;&#21464;&#25442;&#22120;&#20351;&#29992;&#19977;&#31181;&#31867;&#22411;&#30340;&#22810;&#22836;&#27880;&#24847;&#26426;&#21046;&#34701;&#21512;&#19977;&#27169;&#24577;&#29305;&#24449;&#65292;&#30830;&#20445;&#20102;&#22686;&#24378;&#34920;&#31034;&#21644;&#25913;&#36827;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#20998;&#26512;&#12290;&#23454;&#39564;&#20351;&#29992;&#20102;&#20004;&#20010;&#20551;&#26032;&#38395;&#25968;&#25454;&#38598;&#65292;&#24494;&#21338;&#21644;Gossipcop&#12290; &#32467;&#26524;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12481v1 Announce Type: new  Abstract: Detecting fake news has received a lot of attention. Many previous methods concatenate independently encoded unimodal data, ignoring the benefits of integrated multimodal information. Also, the absence of specialized feature extraction for text and images further limits these methods. This paper introduces an end-to-end model called TT-BLIP that applies the bootstrapping language-image pretraining for unified vision-language understanding and generation (BLIP) for three types of information: BERT and BLIP\textsubscript{Txt} for text, ResNet and BLIP\textsubscript{Img} for images, and bidirectional BLIP encoders for multimodal information. The Multimodal Tri-Transformer fuses tri-modal features using three types of multi-head attention mechanisms, ensuring integrated modalities for enhanced representations and improved multimodal data analysis. The experiments are performed using two fake news datasets, Weibo and Gossipcop. The results in
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#29289;&#20307;&#20013;&#24515;&#21270;&#23398;&#20064;&#20013;&#26126;&#30830;&#35299;&#24320;&#24418;&#29366;&#21644;&#32441;&#29702;&#25104;&#20998;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#28508;&#22312;&#31354;&#38388;&#21010;&#20998;&#20026;&#20004;&#20010;&#19981;&#37325;&#21472;&#30340;&#23376;&#38598;&#65292;&#20351;&#24471;&#27169;&#22411;&#26356;&#21152;&#31283;&#23450;&#21644;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2401.10148</link><description>&lt;p&gt;
&#22312;&#29289;&#20307;&#20013;&#24515;&#21270;&#23398;&#20064;&#20013;&#26126;&#30830;&#35299;&#24320;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Explicitly Disentangled Representations in Object-Centric Learning. (arXiv:2401.10148v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10148
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#29289;&#20307;&#20013;&#24515;&#21270;&#23398;&#20064;&#20013;&#26126;&#30830;&#35299;&#24320;&#24418;&#29366;&#21644;&#32441;&#29702;&#25104;&#20998;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#28508;&#22312;&#31354;&#38388;&#21010;&#20998;&#20026;&#20004;&#20010;&#19981;&#37325;&#21472;&#30340;&#23376;&#38598;&#65292;&#20351;&#24471;&#27169;&#22411;&#26356;&#21152;&#31283;&#23450;&#21644;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#21407;&#22987;&#35270;&#35273;&#25968;&#25454;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#34920;&#31034;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#26080;&#30417;&#30563;&#23398;&#20064;&#29289;&#20307;&#20013;&#24515;&#21270;&#34920;&#31034;&#30340;&#25216;&#26415;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#22686;&#24378;&#28508;&#22312;&#29305;&#24449;&#30340;&#31283;&#23450;&#24615;&#21487;&#20197;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#35757;&#32451;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#27493;&#39588;&#26159;&#35299;&#24320;&#23548;&#33268;&#25968;&#25454;&#21464;&#21270;&#30340;&#22240;&#32032;&#12290;&#20808;&#21069;&#65292;&#19981;&#21464;&#21345;&#27133;&#27880;&#24847;&#23454;&#29616;&#20102;&#20174;&#20854;&#20182;&#29305;&#24449;&#20013;&#35299;&#24320;&#20301;&#32622;&#12289;&#23610;&#24230;&#21644;&#26041;&#21521;&#12290;&#25193;&#23637;&#36825;&#19968;&#26041;&#27861;&#65292;&#25105;&#20204;&#30528;&#37325;&#20110;&#20998;&#31163;&#24418;&#29366;&#21644;&#32441;&#29702;&#32452;&#25104;&#37096;&#20998;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#23558;&#29289;&#20307;&#20013;&#24515;&#21270;&#27169;&#22411;&#20013;&#30340;&#24418;&#29366;&#21644;&#32441;&#29702;&#25104;&#20998;&#20559;&#32622;&#20026;&#28508;&#22312;&#31354;&#38388;&#32500;&#24230;&#30340;&#20004;&#20010;&#19981;&#37325;&#21472;&#23376;&#38598;&#12290;&#36825;&#20123;&#23376;&#38598;&#26159;&#20808;&#39564;&#24050;&#30693;&#30340;&#65292;&#22240;&#27492;&#22312;&#35757;&#32451;&#36807;&#31243;&#20043;&#21069;&#12290;&#22312;&#19968;&#31995;&#21015;&#29289;&#20307;&#20013;&#24515;&#21270;&#27979;&#35797;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Extracting structured representations from raw visual data is an important and long-standing challenge in machine learning. Recently, techniques for unsupervised learning of object-centric representations have raised growing interest. In this context, enhancing the robustness of the latent features can improve the efficiency and effectiveness of the training of downstream tasks. A promising step in this direction is to disentangle the factors that cause variation in the data. Previously, Invariant Slot Attention disentangled position, scale, and orientation from the remaining features. Extending this approach, we focus on separating the shape and texture components. In particular, we propose a novel architecture that biases object-centric models toward disentangling shape and texture components into two non-overlapping subsets of the latent space dimensions. These subsets are known a priori, hence before the training process. Experiments on a range of object-centric benchmarks reveal t
&lt;/p&gt;</description></item></channel></rss>