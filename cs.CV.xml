<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PrimeComposer&#65292;&#19968;&#31181;&#26356;&#24555;&#30340;&#36880;&#27493;&#32452;&#21512;&#25193;&#25955;&#26041;&#24335;&#65292;&#29992;&#20110;&#22270;&#20687;&#21512;&#25104;&#65292;&#20027;&#35201;&#19987;&#27880;&#20110;&#21069;&#26223;&#29983;&#25104;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#21512;&#25104;&#20013;&#30340;&#20957;&#32858;&#28151;&#20081;&#21644;&#22806;&#35266;&#20449;&#24687;&#20002;&#22833;&#38382;&#39064;&#65292;&#24182;&#36991;&#20813;&#20102;&#19981;&#24517;&#35201;&#30340;&#32972;&#26223;&#29983;&#25104;&#23548;&#33268;&#30340;&#21069;&#26223;&#29983;&#25104;&#36136;&#37327;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.05053</link><description>&lt;p&gt;
PrimeComposer&#65306;&#29992;&#20110;&#22270;&#20687;&#21512;&#25104;&#30340;&#24555;&#36895;&#36880;&#27493;&#32452;&#21512;&#25193;&#25955;&#26041;&#27861;&#21644;&#24102;&#26377;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
PrimeComposer: Faster Progressively Combined Diffusion for Image Composition with Attention Steering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PrimeComposer&#65292;&#19968;&#31181;&#26356;&#24555;&#30340;&#36880;&#27493;&#32452;&#21512;&#25193;&#25955;&#26041;&#24335;&#65292;&#29992;&#20110;&#22270;&#20687;&#21512;&#25104;&#65292;&#20027;&#35201;&#19987;&#27880;&#20110;&#21069;&#26223;&#29983;&#25104;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#21512;&#25104;&#20013;&#30340;&#20957;&#32858;&#28151;&#20081;&#21644;&#22806;&#35266;&#20449;&#24687;&#20002;&#22833;&#38382;&#39064;&#65292;&#24182;&#36991;&#20813;&#20102;&#19981;&#24517;&#35201;&#30340;&#32972;&#26223;&#29983;&#25104;&#23548;&#33268;&#30340;&#21069;&#26223;&#29983;&#25104;&#36136;&#37327;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#21512;&#25104;&#28041;&#21450;&#23558;&#32473;&#23450;&#23545;&#35937;&#26080;&#32541;&#22320;&#25972;&#21512;&#21040;&#29305;&#23450;&#30340;&#35270;&#35273;&#29615;&#22659;&#20013;&#12290;&#30446;&#21069;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#20174;&#20960;&#20010;&#37319;&#26679;&#22120;&#20013;&#32452;&#21512;&#27880;&#24847;&#21147;&#26435;&#37325;&#26469;&#24341;&#23548;&#29983;&#25104;&#22120;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#26435;&#37325;&#26469;&#33258;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#65292;&#23427;&#20204;&#30340;&#32452;&#21512;&#23548;&#33268;&#22312;&#21512;&#25104;&#20013;&#20957;&#32858;&#28151;&#20081;&#21644;&#22806;&#35266;&#20449;&#24687;&#30340;&#20002;&#22833;&#12290;&#22312;&#35813;&#20219;&#21153;&#20013;&#65292;&#23427;&#20204;&#36807;&#22810;&#20851;&#27880;&#32972;&#26223;&#29983;&#25104;&#65292;&#21363;&#20351;&#22312;&#36825;&#39033;&#20219;&#21153;&#20013;&#26159;&#19981;&#24517;&#35201;&#30340;&#65292;&#36825;&#20123;&#38382;&#39064;&#24694;&#21270;&#12290;&#36825;&#19981;&#20165;&#20943;&#24930;&#20102;&#25512;&#29702;&#36895;&#24230;&#65292;&#36824;&#25439;&#23475;&#20102;&#21069;&#26223;&#29983;&#25104;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#36824;&#22312;&#36807;&#28193;&#21306;&#22495;&#24341;&#20837;&#20102;&#19981;&#38656;&#35201;&#30340;&#20266;&#24433;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#22270;&#20687;&#21512;&#25104;&#24418;&#24335;&#21270;&#20026;&#19968;&#39033;&#22522;&#20110;&#20027;&#39064;&#30340;&#23616;&#37096;&#32534;&#36753;&#20219;&#21153;&#65292;&#20165;&#19987;&#27880;&#20110;&#21069;&#26223;&#29983;&#25104;&#12290;&#22312;&#27599;&#19968;&#27493;&#20013;&#65292;&#32534;&#36753;&#21518;&#30340;&#21069;&#26223;&#19982;&#22122;&#22768;&#32972;&#26223;&#30456;&#32467;&#21512;&#65292;&#20197;&#20445;&#25345;&#22330;&#26223;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#21097;&#19979;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PrimeComposer&#65292;&#19968;&#31181;&#26356;&#24555;&#30340;tr
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05053v1 Announce Type: cross  Abstract: Image composition involves seamlessly integrating given objects into a specific visual context. The current training-free methods rely on composing attention weights from several samplers to guide the generator. However, since these weights are derived from disparate contexts, their combination leads to coherence confusion in synthesis and loss of appearance information. These issues worsen with their excessive focus on background generation, even when unnecessary in this task. This not only slows down inference but also compromises foreground generation quality. Moreover, these methods introduce unwanted artifacts in the transition area. In this paper, we formulate image composition as a subject-based local editing task, solely focusing on foreground generation. At each step, the edited foreground is combined with the noisy background to maintain scene consistency. To address the remaining issues, we propose PrimeComposer, a faster tr
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#24369;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#25351;&#20195;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20998;&#35299;&#20026;&#33719;&#21462;&#23454;&#20363;&#25513;&#27169;&#12289;&#36873;&#25321;&#27491;&#30830;&#25513;&#27169;&#21644;&#32416;&#27491;&#38169;&#35823;&#25513;&#27169;&#30340;&#19977;&#20010;&#27493;&#39588;&#65292;&#22635;&#34917;&#20102;&#24369;&#30417;&#30563;&#21644;&#38646;&#26679;&#26412;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2310.13479</link><description>&lt;p&gt;
&#20998;&#27573;&#12289;&#36873;&#25321;&#12289;&#32416;&#27491;&#65306;&#19968;&#31181;&#24369;&#30417;&#30563;&#25351;&#20195;&#20998;&#21106;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Segment, Select, Correct: A Framework for Weakly-Supervised Referring Segmentation. (arXiv:2310.13479v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13479
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#24369;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#25351;&#20195;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20998;&#35299;&#20026;&#33719;&#21462;&#23454;&#20363;&#25513;&#27169;&#12289;&#36873;&#25321;&#27491;&#30830;&#25513;&#27169;&#21644;&#32416;&#27491;&#38169;&#35823;&#25513;&#27169;&#30340;&#19977;&#20010;&#27493;&#39588;&#65292;&#22635;&#34917;&#20102;&#24369;&#30417;&#30563;&#21644;&#38646;&#26679;&#26412;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20195;&#22270;&#20687;&#20998;&#21106;&#65288;RIS&#65289;&#26159;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#22312;&#22270;&#20687;&#20013;&#35782;&#21035;&#23545;&#35937;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#30446;&#21069;&#20027;&#35201;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#26469;&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#25351;&#20195;&#26631;&#27880;&#25513;&#27169;&#26159;&#19968;&#20010;&#32791;&#26102;&#30340;&#36807;&#31243;&#65292;&#29616;&#26377;&#30340;&#24369;&#30417;&#30563;&#21644;&#38646;&#26679;&#26412;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#36828;&#36828;&#19981;&#21450;&#23436;&#20840;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#22635;&#34917;&#24615;&#33021;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24369;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;RIS&#20998;&#35299;&#25104;&#19977;&#20010;&#27493;&#39588;&#36827;&#34892;&#22788;&#29702;&#65306;&#33719;&#21462;&#34987;&#25552;&#21450;&#25351;&#20196;&#20013;&#30340;&#23545;&#35937;&#30340;&#23454;&#20363;&#25513;&#27169;&#65288;&#20998;&#27573;&#65289;&#65292;&#20351;&#29992;&#38646;&#26679;&#26412;&#23398;&#20064;&#26469;&#36873;&#25321;&#32473;&#23450;&#25351;&#20196;&#30340;&#28508;&#22312;&#27491;&#30830;&#25513;&#27169;&#65288;&#36873;&#25321;&#65289;&#65292;&#24182;&#36890;&#36807;&#24341;&#23548;&#27169;&#22411;&#26469;&#20462;&#27491;&#38646;&#26679;&#26412;&#36873;&#25321;&#30340;&#38169;&#35823;&#65288;&#32416;&#27491;&#65289;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#20165;&#20351;&#29992;&#21069;&#20004;&#20010;&#27493;&#39588;&#65288;&#38646;&#26679;&#26412;&#20998;&#27573;&#21644;&#36873;&#25321;&#65289;&#27604;&#20854;&#20182;&#38646;&#26679;&#26412;&#22522;&#32447;&#25552;&#39640;&#20102;&#22810;&#36798;19%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Referring Image Segmentation (RIS) - the problem of identifying objects in images through natural language sentences - is a challenging task currently mostly solved through supervised learning. However, while collecting referred annotation masks is a time-consuming process, the few existing weakly-supervised and zero-shot approaches fall significantly short in performance compared to fully-supervised learning ones. To bridge the performance gap without mask annotations, we propose a novel weakly-supervised framework that tackles RIS by decomposing it into three steps: obtaining instance masks for the object mentioned in the referencing instruction (segment), using zero-shot learning to select a potentially correct mask for the given instruction (select), and bootstrapping a model which allows for fixing the mistakes of zero-shot selection (correct). In our experiments, using only the first two steps (zero-shot segment and select) outperforms other zero-shot baselines by as much as 19%,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#35889;&#30340;&#21487;&#35299;&#37322;&#24180;&#40836;&#39044;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20840;&#36523;&#22270;&#20687;&#30740;&#31350;&#20102;&#21508;&#20010;&#36523;&#20307;&#37096;&#20301;&#30340;&#24180;&#40836;&#30456;&#20851;&#21464;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#35299;&#37322;&#24615;&#26041;&#27861;&#21644;&#37197;&#20934;&#25216;&#26415;&#65292;&#30830;&#23450;&#20102;&#26368;&#33021;&#39044;&#27979;&#24180;&#40836;&#30340;&#36523;&#20307;&#21306;&#22495;&#65292;&#24182;&#21019;&#19979;&#20102;&#25972;&#20010;&#36523;&#20307;&#24180;&#40836;&#39044;&#27979;&#30340;&#26368;&#26032;&#27700;&#24179;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#33034;&#26609;&#12289;&#26412;&#21407;&#24615;&#32972;&#37096;&#32908;&#32905;&#21644;&#24515;&#33039;&#21306;&#22495;&#26159;&#26368;&#37325;&#35201;&#30340;&#20851;&#27880;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2307.07439</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#35889;&#30340;&#21487;&#35299;&#37322;&#24180;&#40836;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Atlas-Based Interpretable Age Prediction. (arXiv:2307.07439v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#35889;&#30340;&#21487;&#35299;&#37322;&#24180;&#40836;&#39044;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20840;&#36523;&#22270;&#20687;&#30740;&#31350;&#20102;&#21508;&#20010;&#36523;&#20307;&#37096;&#20301;&#30340;&#24180;&#40836;&#30456;&#20851;&#21464;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#35299;&#37322;&#24615;&#26041;&#27861;&#21644;&#37197;&#20934;&#25216;&#26415;&#65292;&#30830;&#23450;&#20102;&#26368;&#33021;&#39044;&#27979;&#24180;&#40836;&#30340;&#36523;&#20307;&#21306;&#22495;&#65292;&#24182;&#21019;&#19979;&#20102;&#25972;&#20010;&#36523;&#20307;&#24180;&#40836;&#39044;&#27979;&#30340;&#26368;&#26032;&#27700;&#24179;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#33034;&#26609;&#12289;&#26412;&#21407;&#24615;&#32972;&#37096;&#32908;&#32905;&#21644;&#24515;&#33039;&#21306;&#22495;&#26159;&#26368;&#37325;&#35201;&#30340;&#20851;&#27880;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24180;&#40836;&#39044;&#27979;&#26159;&#21307;&#23398;&#35780;&#20272;&#21644;&#30740;&#31350;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#21487;&#20197;&#36890;&#36807;&#31361;&#20986;&#23454;&#38469;&#24180;&#40836;&#21644;&#29983;&#29289;&#24180;&#40836;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#24110;&#21161;&#26816;&#27979;&#30142;&#30149;&#21644;&#24322;&#24120;&#34928;&#32769;&#12290;&#20026;&#20102;&#20840;&#38754;&#20102;&#35299;&#21508;&#20010;&#36523;&#20307;&#37096;&#20301;&#30340;&#24180;&#40836;&#30456;&#20851;&#21464;&#21270;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20840;&#36523;&#22270;&#20687;&#36827;&#34892;&#30740;&#31350;&#12290;&#25105;&#20204;&#21033;&#29992;Grad-CAM&#35299;&#37322;&#24615;&#26041;&#27861;&#30830;&#23450;&#26368;&#33021;&#39044;&#27979;&#19968;&#20010;&#20154;&#24180;&#40836;&#30340;&#36523;&#20307;&#21306;&#22495;&#12290;&#36890;&#36807;&#20351;&#29992;&#37197;&#20934;&#25216;&#26415;&#29983;&#25104;&#25972;&#20010;&#20154;&#32676;&#30340;&#35299;&#37322;&#24615;&#22270;&#65292;&#25105;&#20204;&#23558;&#20998;&#26512;&#25193;&#23637;&#21040;&#20010;&#20307;&#20043;&#22806;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20197;&#19968;&#20010;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#20026;2.76&#24180;&#30340;&#27169;&#22411;&#65292;&#21019;&#19979;&#20102;&#25972;&#20010;&#36523;&#20307;&#24180;&#40836;&#39044;&#27979;&#30340;&#26368;&#26032;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#19977;&#20010;&#20027;&#35201;&#30340;&#20851;&#27880;&#39046;&#22495;&#65306;&#33034;&#26609;&#12289;&#26412;&#21407;&#24615;&#32972;&#37096;&#32908;&#32905;&#21644;&#24515;&#33039;&#21306;&#22495;&#65292;&#20854;&#20013;&#24515;&#33039;&#21306;&#22495;&#20855;&#26377;&#26368;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Age prediction is an important part of medical assessments and research. It can aid in detecting diseases as well as abnormal ageing by highlighting the discrepancy between chronological and biological age. To gain a comprehensive understanding of age-related changes observed in various body parts, we investigate them on a larger scale by using whole-body images. We utilise the Grad-CAM interpretability method to determine the body areas most predictive of a person's age. We expand our analysis beyond individual subjects by employing registration techniques to generate population-wide interpretability maps. Furthermore, we set state-of-the-art whole-body age prediction with a model that achieves a mean absolute error of 2.76 years. Our findings reveal three primary areas of interest: the spine, the autochthonous back muscles, and the cardiac region, which exhibits the highest importance.
&lt;/p&gt;</description></item><item><title>MMBench&#26159;&#19968;&#20010;&#26032;&#22411;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#24320;&#21457;&#20840;&#38754;&#30340;&#35780;&#20272;&#27969;&#31243;&#21644;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#32454;&#31890;&#24230;&#33021;&#21147;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2307.06281</link><description>&lt;p&gt;
MMBench: &#24744;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#26159;&#20840;&#33021;&#29699;&#21592;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
MMBench: Is Your Multi-modal Model an All-around Player?. (arXiv:2307.06281v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06281
&lt;/p&gt;
&lt;p&gt;
MMBench&#26159;&#19968;&#20010;&#26032;&#22411;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#24320;&#21457;&#20840;&#38754;&#30340;&#35780;&#20272;&#27969;&#31243;&#21644;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#32454;&#31890;&#24230;&#33021;&#21147;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#20449;&#24687;&#30340;&#24863;&#30693;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#26377;&#25928;&#35780;&#20272;&#36825;&#20123;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#65292;&#38459;&#30861;&#20102;&#26410;&#26469;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#20256;&#32479;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#22914;VQAv2&#25110;COCO Caption&#25552;&#20379;&#20102;&#23450;&#37327;&#30340;&#24615;&#33021;&#27979;&#37327;&#65292;&#20294;&#22312;&#32454;&#31890;&#24230;&#33021;&#21147;&#35780;&#20272;&#21644;&#38750;&#40065;&#26834;&#35780;&#20272;&#25351;&#26631;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#26368;&#36817;&#30340;&#20027;&#35266;&#22522;&#20934;&#27979;&#35797;&#65292;&#22914;OwlEval&#65292;&#36890;&#36807;&#25972;&#21512;&#20154;&#21147;&#36164;&#28304;&#65292;&#23545;&#27169;&#22411;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#20294;&#19981;&#21487;&#25193;&#23637;&#24182;&#19988;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#35265;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MMBench&#65292;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#12290;MMBench&#31995;&#32479;&#22320;&#24320;&#21457;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#27969;&#31243;&#65292;&#20027;&#35201;&#30001;&#20004;&#20010;&#20803;&#32032;&#32452;&#25104;&#12290;&#31532;&#19968;&#20010;&#20803;&#32032;&#26159;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#65292;&#22312;&#35780;&#20272;&#25968;&#37327;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#31867;&#20284;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large vision-language models have recently achieved remarkable progress, exhibiting great perception and reasoning abilities concerning visual information. However, how to effectively evaluate these large vision-language models remains a major obstacle, hindering future model development. Traditional benchmarks like VQAv2 or COCO Caption provide quantitative performance measurements but suffer from a lack of fine-grained ability assessment and non-robust evaluation metrics. Recent subjective benchmarks, such as OwlEval, offer comprehensive evaluations of a model's abilities by incorporating human labor, but they are not scalable and display significant bias. In response to these challenges, we propose MMBench, a novel multi-modality benchmark. MMBench methodically develops a comprehensive evaluation pipeline, primarily comprised of two elements. The first element is a meticulously curated dataset that surpasses existing similar benchmarks in terms of the number and variety of evaluatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#30340;&#35757;&#32451;&#25928;&#29575;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26680;&#24515;&#38598;&#36873;&#25321;&#21644;&#20004;&#20010;&#37325;&#35201;&#24615;&#25351;&#26631;&#26469;&#36873;&#25321;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.07215</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#19982;&#33258;&#36866;&#24212;&#26680;&#24515;&#38598;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Efficient Quantization-aware Training with Adaptive Coreset Selection. (arXiv:2306.07215v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#30340;&#35757;&#32451;&#25928;&#29575;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26680;&#24515;&#38598;&#36873;&#25321;&#21644;&#20004;&#20010;&#37325;&#35201;&#24615;&#25351;&#26631;&#26469;&#36873;&#25321;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#27169;&#22411;&#22823;&#23567;&#21644;&#35745;&#31639;&#37327;&#30340;&#22686;&#21152;&#65292;&#22686;&#21152;&#20102;&#23545;&#26377;&#25928;&#27169;&#22411;&#37096;&#32626;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65288;QAT&#65289;&#26159;&#19968;&#31181;&#20195;&#34920;&#24615;&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#26435;&#37325;&#21644;&#28608;&#27963;&#20013;&#30340;&#20887;&#20313;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;QAT&#26041;&#27861;&#38656;&#35201;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#36825;&#20250;&#23548;&#33268;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#21644;&#39640;&#33021;&#32791;&#12290;&#26680;&#24515;&#38598;&#36873;&#25321;&#26159;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#30340;&#20887;&#20313;&#24615;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#30340;&#26041;&#27861;&#65292;&#22312;&#39640;&#25928;&#35757;&#32451;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35282;&#24230;&#65292;&#36890;&#36807;&#26680;&#24515;&#38598;&#36873;&#25321;&#26469;&#25552;&#39640;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;&#22522;&#20110;QAT&#30340;&#29305;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#25351;&#26631;&#65306;&#35823;&#24046;&#21521;&#37327;&#20998;&#25968;&#21644;&#19981;&#19968;&#33268;&#20998;&#25968;&#65292;&#29992;&#20110;&#37327;&#21270;&#35757;&#32451;&#36807;&#31243;&#20013;&#27599;&#20010;&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#12290;&#22522;&#20110;&#36825;&#20004;&#20010;&#37325;&#35201;&#24615;&#25351;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#24863;&#30693;&#30340;&#33258;&#36866;&#24212;&#26680;&#24515;&#38598;&#36873;&#25321;&#65288;ACS&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
The expanding model size and computation of deep neural networks (DNNs) have increased the demand for efficient model deployment methods. Quantization-aware training (QAT) is a representative model compression method to leverage redundancy in weights and activations. However, most existing QAT methods require end-to-end training on the entire dataset, which suffers from long training time and high energy costs. Coreset selection, aiming to improve data efficiency utilizing the redundancy of training data, has also been widely used for efficient training. In this work, we propose a new angle through the coreset selection to improve the training efficiency of quantization-aware training. Based on the characteristics of QAT, we propose two metrics: error vector score and disagreement score, to quantify the importance of each sample during training. Guided by these two metrics of importance, we proposed a quantization-aware adaptive coreset selection (ACS) method to select the data for the
&lt;/p&gt;</description></item><item><title>DiracDiffusion&#26159;&#19968;&#31181;&#26032;&#30340;&#36870;&#38382;&#39064;&#27714;&#35299;&#26694;&#26550;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#22270;&#20687;&#21435;&#22122;&#21644;&#22686;&#37327;&#37325;&#24314;&#65292;&#24182;&#20445;&#35777;&#25968;&#25454;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.14353</link><description>&lt;p&gt;
DiracDiffusion: &#30830;&#20445;&#25968;&#25454;&#19968;&#33268;&#24615;&#30340;&#21435;&#22122;&#21644;&#22686;&#37327;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
DiracDiffusion: Denoising and Incremental Reconstruction with Assured Data-Consistency. (arXiv:2303.14353v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14353
&lt;/p&gt;
&lt;p&gt;
DiracDiffusion&#26159;&#19968;&#31181;&#26032;&#30340;&#36870;&#38382;&#39064;&#27714;&#35299;&#26694;&#26550;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#22270;&#20687;&#21435;&#22122;&#21644;&#22686;&#37327;&#37325;&#24314;&#65292;&#24182;&#20445;&#35777;&#25968;&#25454;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#65288;&#21253;&#25324;&#22270;&#20687;&#24674;&#22797;&#65289;&#24050;&#32463;&#24314;&#31435;&#20102;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;&#22522;&#20110;&#25193;&#25955;&#30340;&#36870;&#38382;&#39064;&#27714;&#35299;&#22120;&#20174;&#20005;&#37325;&#25439;&#22351;&#30340;&#27979;&#37327;&#25968;&#25454;&#20013;&#29983;&#25104;&#20986;&#20855;&#26377;&#20986;&#33394;&#35270;&#35273;&#36136;&#37327;&#30340;&#37325;&#24314;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#25152;&#35859;&#30340;&#24863;&#30693;-&#22833;&#30495;&#26435;&#34913;&#20013;&#65292;&#24863;&#30693;&#25928;&#26524;&#20248;&#31168;&#30340;&#37325;&#24314;&#32467;&#26524;&#36890;&#24120;&#26159;&#20197;&#36864;&#21270;&#30340;&#22833;&#30495;&#24230;&#37327;&#65288;&#22914;PSNR&#65289;&#20026;&#20195;&#20215;&#30340;&#12290;&#22833;&#30495;&#24230;&#37327;&#34913;&#37327;&#23545;&#35266;&#23519;&#30340;&#24544;&#23454;&#24230;&#65292;&#36825;&#22312;&#36870;&#38382;&#39064;&#20013;&#26159;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#35201;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36870;&#38382;&#39064;&#27714;&#35299;&#26694;&#26550;&#65292;&#21363;&#25105;&#20204;&#20551;&#35774;&#35266;&#23519;&#20540;&#26469;&#33258;&#19968;&#20010;&#38543;&#26426;&#21155;&#21270;&#36807;&#31243;&#65292;&#36880;&#28176;&#38477;&#20302;&#21644;&#22122;&#22768;&#21270;&#21407;&#22987;&#24178;&#20928;&#22270;&#20687;&#65292;&#28982;&#21518;&#23398;&#20064;&#36870;&#36716;&#21155;&#21270;&#36807;&#31243;&#20197;&#24674;&#22797;&#24178;&#20928;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;&#25972;&#20010;&#36870;&#36716;&#36807;&#31243;&#20013;&#20445;&#25345;&#19982;&#21407;&#22987;&#27979;&#37327;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#20801;&#35768;&#22312;&#24863;&#30693;&#36136;&#37327;&#21644;&#25968;&#25454;&#19968;&#33268;&#24615;&#20043;&#38388;&#21462;&#24471;&#24040;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;DiracDiffusion&#65292;&#22240;&#20026;&#23427;&#22522;&#20110;&#30001;Dirac&#33021;&#37327;&#20989;&#25968;&#24341;&#23548;&#30340;&#25193;&#25955;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#21435;&#22122;&#21644;&#22686;&#37327;&#37325;&#24314;&#22312;&#20869;&#30340;&#20960;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have established new state of the art in a multitude of computer vision tasks, including image restoration. Diffusion-based inverse problem solvers generate reconstructions of exceptional visual quality from heavily corrupted measurements. However, in what is widely known as the perception-distortion trade-off, the price of perceptually appealing reconstructions is often paid in declined distortion metrics, such as PSNR. Distortion metrics measure faithfulness to the observation, a crucial requirement in inverse problems. In this work, we propose a novel framework for inverse problem solving, namely we assume that the observation comes from a stochastic degradation process that gradually degrades and noises the original clean image. We learn to reverse the degradation process in order to recover the clean image. Our technique maintains consistency with the original measurement throughout the reverse process, and allows for great flexibility in trading off perceptual qu
&lt;/p&gt;</description></item></channel></rss>