<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>AVIBench&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#21508;&#31181;&#24418;&#24335;&#30340;&#23545;&#25239;&#24615;&#35270;&#35273;&#25351;&#20196;&#30340;&#40065;&#26834;&#24615;&#65292;&#21253;&#25324;&#22270;&#20687;&#21644;&#25991;&#26412;&#25915;&#20987;&#20197;&#21450;&#20869;&#23481;&#20559;&#35265;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.09346</link><description>&lt;p&gt;
AVIBench&#65306;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#23545;&#25239;&#24615;&#35270;&#35273;&#25351;&#23548;&#19978;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Adversarial Visual-Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09346
&lt;/p&gt;
&lt;p&gt;
AVIBench&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#21508;&#31181;&#24418;&#24335;&#30340;&#23545;&#25239;&#24615;&#35270;&#35273;&#25351;&#20196;&#30340;&#40065;&#26834;&#24615;&#65292;&#21253;&#25324;&#22270;&#20687;&#21644;&#25991;&#26412;&#25915;&#20987;&#20197;&#21450;&#20869;&#23481;&#20559;&#35265;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#22312;&#23545;&#29992;&#25143;&#30340;&#35270;&#35273;&#25351;&#20196;&#20316;&#20986;&#33391;&#22909;&#21709;&#24212;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25351;&#20196;&#28085;&#30422;&#22270;&#20687;&#21644;&#25991;&#26412;&#65292;&#23481;&#26131;&#21463;&#21040;&#26377;&#24847;&#21644;&#26080;&#24847;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;LVLMs&#23545;&#25239;&#27492;&#31867;&#23041;&#32961;&#30340;&#40065;&#26834;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#24403;&#21069;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AVIBench&#65292;&#19968;&#20010;&#26088;&#22312;&#20998;&#26512;LVLMs&#22312;&#38754;&#23545;&#21508;&#31181;&#23545;&#25239;&#24615;&#35270;&#35273;&#25351;&#20196;&#65288;AVIs&#65289;&#26102;&#30340;&#40065;&#26834;&#24615;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#22235;&#31181;&#22522;&#20110;&#22270;&#20687;&#30340;AVIs&#12289;&#21313;&#31181;&#22522;&#20110;&#25991;&#26412;&#30340;AVIs&#21644;&#20061;&#31181;&#20869;&#23481;&#20559;&#35265;AVIs&#65288;&#22914;&#24615;&#21035;&#12289;&#26292;&#21147;&#12289;&#25991;&#21270;&#21644;&#31181;&#26063;&#20559;&#35265;&#31561;&#65289;&#12290;&#25105;&#20204;&#29983;&#25104;&#20102;26&#19975;&#20010;AVIs&#65292;&#28085;&#30422;&#20116;&#31867;&#22810;&#27169;&#24577;&#33021;&#21147;&#65288;&#20061;&#39033;&#20219;&#21153;&#65289;&#21644;&#20869;&#23481;&#20559;&#35265;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#21253;&#25324;14&#20010;&#24320;&#28304;LVLMs&#22312;&#20869;&#30340;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#20197;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;AVIBench&#36824;&#21487;&#20316;&#20026;&#19968;&#20010;&#20415;&#21033;&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09346v1 Announce Type: cross  Abstract: Large Vision-Language Models (LVLMs) have shown significant progress in well responding to visual-instructions from users. However, these instructions, encompassing images and text, are susceptible to both intentional and inadvertent attacks. Despite the critical importance of LVLMs' robustness against such threats, current research in this area remains limited. To bridge this gap, we introduce AVIBench, a framework designed to analyze the robustness of LVLMs when facing various adversarial visual-instructions (AVIs), including four types of image-based AVIs, ten types of text-based AVIs, and nine types of content bias AVIs (such as gender, violence, cultural, and racial biases, among others). We generate 260K AVIs encompassing five categories of multimodal capabilities (nine tasks) and content bias. We then conduct a comprehensive evaluation involving 14 open-source LVLMs to assess their performance. AVIBench also serves as a convenie
&lt;/p&gt;</description></item><item><title>CLIP&#30340;&#25104;&#21151;&#20027;&#35201;&#24402;&#21151;&#20110;&#20854;&#25968;&#25454;&#32780;&#38750;&#27169;&#22411;&#26550;&#26500;&#25110;&#39044;&#35757;&#32451;&#30446;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#20803;&#25968;&#25454;&#25972;&#29702;&#26041;&#27861;&#24341;&#20837;&#20102;MetaCLIP&#65292;&#35813;&#26041;&#27861;&#20174;&#21407;&#22987;&#25968;&#25454;&#27744;&#21644;&#20803;&#25968;&#25454;&#20013;&#29983;&#25104;&#19968;&#20010;&#24179;&#34913;&#30340;&#23376;&#38598;&#65292;&#25552;&#20379;&#20102;&#26356;&#21152;&#35814;&#32454;&#30340;&#25968;&#25454;&#20449;&#24687;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;MetaCLIP&#22312;&#22788;&#29702;400M&#20010;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#23545;&#26102;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.16671</link><description>&lt;p&gt;
&#25581;&#31192;CLIP&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Demystifying CLIP Data. (arXiv:2309.16671v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16671
&lt;/p&gt;
&lt;p&gt;
CLIP&#30340;&#25104;&#21151;&#20027;&#35201;&#24402;&#21151;&#20110;&#20854;&#25968;&#25454;&#32780;&#38750;&#27169;&#22411;&#26550;&#26500;&#25110;&#39044;&#35757;&#32451;&#30446;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#20803;&#25968;&#25454;&#25972;&#29702;&#26041;&#27861;&#24341;&#20837;&#20102;MetaCLIP&#65292;&#35813;&#26041;&#27861;&#20174;&#21407;&#22987;&#25968;&#25454;&#27744;&#21644;&#20803;&#25968;&#25454;&#20013;&#29983;&#25104;&#19968;&#20010;&#24179;&#34913;&#30340;&#23376;&#38598;&#65292;&#25552;&#20379;&#20102;&#26356;&#21152;&#35814;&#32454;&#30340;&#25968;&#25454;&#20449;&#24687;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;MetaCLIP&#22312;&#22788;&#29702;400M&#20010;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#23545;&#26102;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#26159;&#19968;&#31181;&#25512;&#21160;&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#21644;&#24212;&#29992;&#30340;&#26041;&#27861;&#65292;&#20026;&#29616;&#20195;&#35782;&#21035;&#31995;&#32479;&#21644;&#29983;&#25104;&#27169;&#22411;&#27880;&#20837;&#20102;&#27963;&#21147;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;CLIP&#25104;&#21151;&#30340;&#20027;&#35201;&#22240;&#32032;&#26159;&#20854;&#25968;&#25454;&#65292;&#32780;&#19981;&#26159;&#27169;&#22411;&#26550;&#26500;&#25110;&#39044;&#35757;&#32451;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;CLIP&#21482;&#25552;&#20379;&#20102;&#20851;&#20110;&#20854;&#25968;&#25454;&#21644;&#22914;&#20309;&#25910;&#38598;&#25968;&#25454;&#30340;&#38750;&#24120;&#26377;&#38480;&#30340;&#20449;&#24687;&#65292;&#23548;&#33268;&#20854;&#20182;&#30740;&#31350;&#21162;&#21147;&#36890;&#36807;&#20351;&#29992;&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#36807;&#28388;&#26469;&#37325;&#29616;CLIP&#30340;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24847;&#22312;&#25581;&#31034;CLIP&#30340;&#25968;&#25454;&#25972;&#29702;&#26041;&#27861;&#65292;&#24182;&#22312;&#20844;&#24320;&#32473;&#31038;&#21306;&#30340;&#36807;&#31243;&#20013;&#24341;&#20837;&#20803;&#25968;&#25454;&#25972;&#29702;&#30340;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;MetaCLIP&#65289;&#12290;MetaCLIP&#36890;&#36807;&#23545;&#20803;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#24179;&#34913;&#65292;&#20174;&#21407;&#22987;&#25968;&#25454;&#27744;&#21644;&#20803;&#25968;&#25454;&#65288;&#20174;CLIP&#30340;&#27010;&#24565;&#20013;&#24471;&#20986;&#65289;&#20013;&#20135;&#29983;&#19968;&#20010;&#24179;&#34913;&#30340;&#23376;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#30740;&#31350;&#20005;&#26684;&#38548;&#31163;&#20102;&#27169;&#22411;&#21644;&#35757;&#32451;&#35774;&#32622;&#65292;&#20165;&#19987;&#27880;&#20110;&#25968;&#25454;&#12290;MetaCLIP&#24212;&#29992;&#20110;&#21253;&#21547;400M&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#23545;&#30340;CommonCrawl&#65292;&#24182;&#33719;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Language-Image Pre-training (CLIP) is an approach that has advanced research and applications in computer vision, fueling modern recognition systems and generative models. We believe that the main ingredient to the success of CLIP is its data and not the model architecture or pre-training objective. However, CLIP only provides very limited information about its data and how it has been collected, leading to works that aim to reproduce CLIP's data by filtering with its model parameters. In this work, we intend to reveal CLIP's data curation approach and in our pursuit of making it open to the community introduce Metadata-Curated Language-Image Pre-training (MetaCLIP). MetaCLIP takes a raw data pool and metadata (derived from CLIP's concepts) and yields a balanced subset over the metadata distribution. Our experimental study rigorously isolates the model and training settings, concentrating solely on data. MetaCLIP applied to CommonCrawl with 400M image-text data pairs outper
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23545;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#39046;&#22495;&#30340;&#35268;&#33539;&#22240;&#32032;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22240;&#23376;&#29305;&#24449;&#23481;&#37327;&#34429;&#28982;&#31616;&#21333;&#39640;&#25928;&#65292;&#20294;&#23384;&#22312;&#19981;&#33391;&#20559;&#24046;&#12290;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#35268;&#33539;&#21270;&#21464;&#25442;&#65292;&#35813;&#26041;&#27861;&#25104;&#21151;&#28040;&#38500;&#20559;&#24046;&#65292;&#24182;&#22312;&#22270;&#20687;&#12289;&#36317;&#31163;&#21644;&#36752;&#23556;&#22330;&#37325;&#24314;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#36136;&#37327;&#12289;&#40065;&#26834;&#24615;&#21644;&#36816;&#34892;&#26102;&#38388;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.15461</link><description>&lt;p&gt;
&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#39046;&#22495;&#30340;&#35268;&#33539;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Canonical Factors for Hybrid Neural Fields. (arXiv:2308.15461v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15461
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23545;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#39046;&#22495;&#30340;&#35268;&#33539;&#22240;&#32032;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22240;&#23376;&#29305;&#24449;&#23481;&#37327;&#34429;&#28982;&#31616;&#21333;&#39640;&#25928;&#65292;&#20294;&#23384;&#22312;&#19981;&#33391;&#20559;&#24046;&#12290;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#35268;&#33539;&#21270;&#21464;&#25442;&#65292;&#35813;&#26041;&#27861;&#25104;&#21151;&#28040;&#38500;&#20559;&#24046;&#65292;&#24182;&#22312;&#22270;&#20687;&#12289;&#36317;&#31163;&#21644;&#36752;&#23556;&#22330;&#37325;&#24314;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#36136;&#37327;&#12289;&#40065;&#26834;&#24615;&#21644;&#36816;&#34892;&#26102;&#38388;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#23376;&#29305;&#24449;&#23481;&#37327;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#26356;&#32039;&#20945;&#12289;&#39640;&#25928;&#21644;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#39046;&#22495;&#65292;&#20294;&#20063;&#24341;&#20837;&#20102;&#19981;&#19968;&#23450;&#26377;&#30410;&#20110;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#20559;&#24046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#65288;1&#65289;&#23545;&#36825;&#20123;&#20307;&#31995;&#32467;&#26500;&#23545;&#40784;&#36724;&#20449;&#21495;&#30340;&#19981;&#33391;&#20559;&#24046;&#36827;&#34892;&#20102;&#34920;&#24449; - &#23427;&#20204;&#21487;&#20197;&#23548;&#33268;&#36752;&#23556;&#22330;&#37325;&#24314;&#30340;&#24046;&#24322;&#39640;&#36798;2 PSNR - &#24182;&#65288;2&#65289;&#25506;&#32034;&#20102;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#35268;&#33539;&#21270;&#21464;&#25442;&#26469;&#25552;&#39640;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#28040;&#38500;&#36825;&#20123;&#20559;&#24046;&#12290;&#22312;&#19968;&#20010;&#20108;&#32500;&#27169;&#22411;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#21516;&#26102;&#23398;&#20064;&#36825;&#20123;&#21464;&#25442;&#20197;&#21450;&#22330;&#26223;&#22806;&#35266;&#21487;&#20197;&#20197;&#22823;&#22823;&#25552;&#39640;&#30340;&#25928;&#29575;&#25104;&#21151;&#12290;&#25105;&#20204;&#20351;&#29992;&#22270;&#20687;&#12289;&#26377;&#31526;&#21495;&#36317;&#31163;&#21644;&#36752;&#23556;&#22330;&#37325;&#24314;&#20219;&#21153;&#39564;&#35777;&#20102;&#25152;&#24471;&#21040;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36136;&#37327;&#12289;&#40065;&#26834;&#24615;&#12289;&#32039;&#20945;&#24615;&#21644;&#36816;&#34892;&#26102;&#38388;&#26041;&#38754;&#30340;&#25913;&#36827;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;TILTED&#21487;&#20197;&#23454;&#29616;&#19982;&#22522;&#32447;&#30456;&#24403;&#30340;&#33021;&#21147;&#65292;&#32780;&#22522;&#32447;&#26159;2&#20493;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Factored feature volumes offer a simple way to build more compact, efficient, and intepretable neural fields, but also introduce biases that are not necessarily beneficial for real-world data. In this work, we (1) characterize the undesirable biases that these architectures have for axis-aligned signals -- they can lead to radiance field reconstruction differences of as high as 2 PSNR -- and (2) explore how learning a set of canonicalizing transformations can improve representations by removing these biases. We prove in a two-dimensional model problem that simultaneously learning these transformations together with scene appearance succeeds with drastically improved efficiency. We validate the resulting architectures, which we call TILTED, using image, signed distance, and radiance field reconstruction tasks, where we observe improvements across quality, robustness, compactness, and runtime. Results demonstrate that TILTED can enable capabilities comparable to baselines that are 2x lar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#33041;&#23545;&#40784;&#30340;&#30740;&#31350;&#65292;&#37325;&#28857;&#22312;&#20110;&#33041;&#32534;&#30721;&#21644;&#35299;&#30721;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#29702;&#35299;&#22823;&#33041;&#30340;&#20449;&#24687;&#22788;&#29702;&#26426;&#21046;&#20197;&#21450;&#35774;&#35745;&#33041;&#26426;&#25509;&#21475;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.10246</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#33041;&#23545;&#40784;&#65306;&#33041;&#32534;&#30721;&#21644;&#35299;&#30721;&#65288;&#32508;&#36848;&#65289;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding (Survey). (arXiv:2307.10246v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#33041;&#23545;&#40784;&#30340;&#30740;&#31350;&#65292;&#37325;&#28857;&#22312;&#20110;&#33041;&#32534;&#30721;&#21644;&#35299;&#30721;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#29702;&#35299;&#22823;&#33041;&#30340;&#20449;&#24687;&#22788;&#29702;&#26426;&#21046;&#20197;&#21450;&#35774;&#35745;&#33041;&#26426;&#25509;&#21475;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#22914;&#20309;&#34920;&#31034;&#19981;&#21516;&#30340;&#20449;&#24687;&#27169;&#24335;&#65311;&#25105;&#20204;&#33021;&#21542;&#35774;&#35745;&#20986;&#19968;&#20010;&#21487;&#20197;&#33258;&#21160;&#29702;&#35299;&#29992;&#25143;&#24605;&#32771;&#20869;&#23481;&#30340;&#31995;&#32479;&#65311;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#30740;&#31350;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#31561;&#22823;&#33041;&#35760;&#24405;&#26469;&#22238;&#31572;&#12290;&#20316;&#20026;&#31532;&#19968;&#27493;&#65292;&#31070;&#32463;&#31185;&#23398;&#30028;&#20026;&#34987;&#21160;&#38405;&#35835;/&#21548;&#35273;/&#35266;&#30475;&#27010;&#24565;&#35789;&#27719;&#12289;&#21465;&#36848;&#12289;&#22270;&#29255;&#21644;&#30005;&#24433;&#30456;&#20851;&#30340;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#25968;&#25454;&#38598;&#20316;&#20986;&#20102;&#36129;&#29486;&#12290;&#36807;&#21435;&#20108;&#21313;&#24180;&#20013;&#65292;&#36824;&#25552;&#20986;&#20102;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#32534;&#30721;&#21644;&#35299;&#30721;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#20316;&#20026;&#22522;&#30784;&#30740;&#31350;&#20013;&#30340;&#39069;&#22806;&#24037;&#20855;&#65292;&#22312;&#35748;&#30693;&#31185;&#23398;&#21644;&#31070;&#32463;&#31185;&#23398;&#39046;&#22495;&#26377;&#30528;&#22810;&#31181;&#23454;&#38469;&#24212;&#29992;&#12290;&#32534;&#30721;&#27169;&#22411;&#26088;&#22312;&#33258;&#21160;&#22320;&#29983;&#25104;fMRI&#22823;&#33041;&#34920;&#24449;&#65292;&#32473;&#23450;&#19968;&#20010;&#21050;&#28608;&#12290;&#23427;&#20204;&#22312;&#35780;&#20272;&#21644;&#35786;&#26029;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#20197;&#21450;&#35774;&#35745;&#22823;&#33041;&#25439;&#20260;&#27835;&#30103;&#26041;&#27861;&#26041;&#38754;&#26377;&#30528;&#22810;&#31181;&#23454;&#38469;&#24212;&#29992;&#12290;&#35299;&#30721;&#27169;&#22411;&#35299;&#20915;&#20102;&#26681;&#25454;fMRI&#37325;&#26500;&#21050;&#28608;&#30340;&#36870;&#38382;&#39064;&#12290;&#23427;&#20204;&#23545;&#20110;&#29702;&#35299;&#22823;&#33041;&#22914;&#20309;&#22788;&#29702;&#20449;&#24687;&#20197;&#21450;&#35774;&#35745;&#33041;&#26426;&#25509;&#21475;&#30340;&#21457;&#23637;&#37117;&#26377;&#30528;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
How does the brain represent different modes of information? Can we design a system that automatically understands what the user is thinking? Such questions can be answered by studying brain recordings like functional magnetic resonance imaging (fMRI). As a first step, the neuroscience community has contributed several large cognitive neuroscience datasets related to passive reading/listening/viewing of concept words, narratives, pictures and movies. Encoding and decoding models using these datasets have also been proposed in the past two decades. These models serve as additional tools for basic research in cognitive science and neuroscience. Encoding models aim at generating fMRI brain representations given a stimulus automatically. They have several practical applications in evaluating and diagnosing neurological conditions and thus also help design therapies for brain damage. Decoding models solve the inverse problem of reconstructing the stimuli given the fMRI. They are useful for 
&lt;/p&gt;</description></item></channel></rss>