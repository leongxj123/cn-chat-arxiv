<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#36890;&#36807;&#35782;&#21035;CLIP&#25991;&#26412;&#23884;&#20837;&#20013;&#30340;&#35821;&#20041;&#26041;&#21521;&#65292;&#23454;&#29616;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#23545;&#39640;&#32423;&#23646;&#24615;&#30340;&#32454;&#31890;&#24230;&#20027;&#39064;&#29305;&#23450;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.17064</link><description>&lt;p&gt;
&#22312;T2I&#27169;&#22411;&#20013;&#36890;&#36807;&#35782;&#21035;&#35821;&#20041;&#26041;&#21521;&#23454;&#29616;&#36830;&#32493;&#12289;&#20027;&#39064;&#29305;&#23450;&#30340;&#23646;&#24615;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Continuous, Subject-Specific Attribute Control in T2I Models by Identifying Semantic Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17064
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35782;&#21035;CLIP&#25991;&#26412;&#23884;&#20837;&#20013;&#30340;&#35821;&#20041;&#26041;&#21521;&#65292;&#23454;&#29616;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#23545;&#39640;&#32423;&#23646;&#24615;&#30340;&#32454;&#31890;&#24230;&#20027;&#39064;&#29305;&#23450;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#26174;&#33879;&#25552;&#39640;&#20102;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30340;&#38480;&#21046;&#65288;&#20363;&#22914;&#8220;&#20154;&#8221;&#21644;&#8220;&#32769;&#24180;&#20154;&#8221;&#20043;&#38388;&#19981;&#23384;&#22312;&#36830;&#32493;&#30340;&#20013;&#38388;&#25551;&#36848;&#30340;&#38598;&#21512;&#65289;&#65292;&#23454;&#29616;&#23545;&#23646;&#24615;&#30340;&#32454;&#31890;&#24230;&#25511;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#23613;&#31649;&#24341;&#20837;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#22686;&#24378;&#27169;&#22411;&#25110;&#29983;&#25104;&#36807;&#31243;&#20197;&#23454;&#29616;&#36825;&#31181;&#25511;&#21046;&#65292;&#20294;&#19981;&#38656;&#35201;&#22266;&#23450;&#21442;&#32771;&#22270;&#20687;&#30340;&#26041;&#27861;&#20165;&#38480;&#20110;&#21551;&#29992;&#20840;&#23616;&#32454;&#31890;&#24230;&#23646;&#24615;&#34920;&#36798;&#25511;&#21046;&#25110;&#20165;&#38480;&#20110;&#29305;&#23450;&#20027;&#39064;&#30340;&#31895;&#31890;&#24230;&#23646;&#24615;&#34920;&#36798;&#25511;&#21046;&#65292;&#32780;&#19981;&#33021;&#21516;&#26102;&#20860;&#39038;&#20004;&#32773;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#24120;&#29992;&#30340;&#22522;&#20110;&#26631;&#35760;&#32423;&#21035;&#30340;CLIP&#25991;&#26412;&#23884;&#20837;&#20013;&#23384;&#22312;&#21487;&#23454;&#29616;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#39640;&#32423;&#23646;&#24615;&#30340;&#32454;&#31890;&#24230;&#20027;&#39064;&#29305;&#23450;&#25511;&#21046;&#30340;&#26041;&#21521;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17064v1 Announce Type: cross  Abstract: In recent years, advances in text-to-image (T2I) diffusion models have substantially elevated the quality of their generated images. However, achieving fine-grained control over attributes remains a challenge due to the limitations of natural language prompts (such as no continuous set of intermediate descriptions existing between ``person'' and ``old person''). Even though many methods were introduced that augment the model or generation process to enable such control, methods that do not require a fixed reference image are limited to either enabling global fine-grained attribute expression control or coarse attribute expression control localized to specific subjects, not both simultaneously. We show that there exist directions in the commonly used token-level CLIP text embeddings that enable fine-grained subject-specific control of high-level attributes in text-to-image models. Based on this observation, we introduce one efficient op
&lt;/p&gt;</description></item><item><title>&#23558;&#21452;&#32534;&#30721;&#22120;&#25945;&#24072;&#27169;&#22411;&#33719;&#24471;&#30340;&#31354;&#38388;&#20960;&#20309;&#20808;&#39564;&#30693;&#35782;&#38544;&#24335;&#27880;&#20837;&#21333;&#32534;&#30721;&#22120;&#23398;&#29983;&#27169;&#22411;&#65292;&#36890;&#36807;&#26032;&#30340;logit&#33976;&#39311;&#21644;&#29305;&#24449;&#33976;&#39311;&#26041;&#27861;&#65292;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#35270;&#35273;&#35821;&#20041;&#20998;&#21106;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.08215</link><description>&lt;p&gt;
LIX&#65306;&#23558;&#31354;&#38388;&#20960;&#20309;&#20808;&#39564;&#30693;&#35782;&#38544;&#24335;&#27880;&#20837;&#35270;&#35273;&#35821;&#20041;&#20998;&#21106;&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
LIX: Implicitly Infusing Spatial Geometric Prior Knowledge into Visual Semantic Segmentation for Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08215
&lt;/p&gt;
&lt;p&gt;
&#23558;&#21452;&#32534;&#30721;&#22120;&#25945;&#24072;&#27169;&#22411;&#33719;&#24471;&#30340;&#31354;&#38388;&#20960;&#20309;&#20808;&#39564;&#30693;&#35782;&#38544;&#24335;&#27880;&#20837;&#21333;&#32534;&#30721;&#22120;&#23398;&#29983;&#27169;&#22411;&#65292;&#36890;&#36807;&#26032;&#30340;logit&#33976;&#39311;&#21644;&#29305;&#24449;&#33976;&#39311;&#26041;&#27861;&#65292;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#35270;&#35273;&#35821;&#20041;&#20998;&#21106;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25968;&#25454;&#34701;&#21512;&#32593;&#32476;&#22312;&#35270;&#35273;&#35821;&#20041;&#20998;&#21106;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#24403;&#32570;&#20047;&#31354;&#38388;&#20960;&#20309;&#25968;&#25454;&#26102;&#65292;&#21452;&#32534;&#30721;&#22120;&#21464;&#24471;&#26080;&#25928;&#12290;&#23558;&#21452;&#32534;&#30721;&#22120;&#25945;&#24072;&#27169;&#22411;&#33719;&#24471;&#30340;&#31354;&#38388;&#20960;&#20309;&#20808;&#39564;&#30693;&#35782;&#38544;&#24335;&#27880;&#20837;&#21333;&#32534;&#30721;&#22120;&#23398;&#29983;&#27169;&#22411;&#26159;&#19968;&#20010;&#23454;&#29992;&#20294;&#19981;&#22826;&#25506;&#32034;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20010;&#20027;&#39064;&#65292;&#24182;&#37319;&#29992;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Learning to Infuse "X" (LIX) &#26694;&#26550;&#65292;&#22312;logit&#33976;&#39311;&#21644;&#29305;&#24449;&#33976;&#39311;&#26041;&#38754;&#36827;&#34892;&#20102;&#26032;&#39062;&#36129;&#29486;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#35777;&#26126;&#65292;&#24378;&#35843;&#22312;&#35299;&#32806;&#30693;&#35782;&#33976;&#39311;&#20013;&#20351;&#29992;&#21333;&#19968;&#22266;&#23450;&#26435;&#37325;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;logit&#26234;&#33021;&#21160;&#24577;&#26435;&#37325;&#25511;&#21046;&#22120;&#20316;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#37325;&#26032;&#26657;&#20934;&#30340;&#29305;&#24449;&#33976;&#39311;&#31639;&#27861;&#65292;&#21253;&#25324;&#20004;&#31181;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08215v1 Announce Type: cross  Abstract: Despite the impressive performance achieved by data-fusion networks with duplex encoders for visual semantic segmentation, they become ineffective when spatial geometric data are not available. Implicitly infusing the spatial geometric prior knowledge acquired by a duplex-encoder teacher model into a single-encoder student model is a practical, albeit less explored research avenue. This paper delves into this topic and resorts to knowledge distillation approaches to address this problem. We introduce the Learning to Infuse "X" (LIX) framework, with novel contributions in both logit distillation and feature distillation aspects. We present a mathematical proof that underscores the limitation of using a single fixed weight in decoupled knowledge distillation and introduce a logit-wise dynamic weight controller as a solution to this issue. Furthermore, we develop an adaptively-recalibrated feature distillation algorithm, including two tec
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31934;&#35843;UNet&#36827;&#34892;&#20302;&#21058;&#37327;CT&#22270;&#20687;&#37325;&#24314;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#31532;&#20108;&#38454;&#27573;&#30340;&#35757;&#32451;&#31574;&#30053;&#20026;CT&#22270;&#20687;&#22686;&#24378;&#38454;&#27573;&#12290;</title><link>https://arxiv.org/abs/2403.03551</link><description>&lt;p&gt;
&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#20026;&#39640;&#26031;&#38477;&#22122;&#32780;&#35757;&#32451;&#30340;UNet&#36827;&#34892;&#20302;&#21058;&#37327;CT&#22270;&#20687;&#37325;&#24314;&#65292;&#29992;&#20110;&#22270;&#20687;&#22686;&#24378;&#30340;&#19979;&#28216;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Low-Dose CT Image Reconstruction by Fine-Tuning a UNet Pretrained for Gaussian Denoising for the Downstream Task of Image Enhancement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03551
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31934;&#35843;UNet&#36827;&#34892;&#20302;&#21058;&#37327;CT&#22270;&#20687;&#37325;&#24314;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#31532;&#20108;&#38454;&#27573;&#30340;&#35757;&#32451;&#31574;&#30053;&#20026;CT&#22270;&#20687;&#22686;&#24378;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#21307;&#23398;&#25104;&#20687;&#27169;&#24577;&#65292;&#30001;&#20110;&#20854;&#22522;&#20110;&#30005;&#31163;&#36752;&#23556;&#65292;&#22240;&#27492;&#24076;&#26395;&#23613;&#37327;&#20943;&#23569;&#36752;&#23556;&#21058;&#37327;&#12290;&#28982;&#32780;&#65292;&#38477;&#20302;&#36752;&#23556;&#21058;&#37327;&#20250;&#23548;&#33268;&#22270;&#20687;&#36136;&#37327;&#19979;&#38477;&#65292;&#20174;&#20302;&#21058;&#37327;CT&#65288;LDCT&#65289;&#25968;&#25454;&#37325;&#24314;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20540;&#24471;&#36827;&#34892;&#30740;&#31350;&#12290;&#26681;&#25454;LoDoPaB-CT&#22522;&#20934;&#65292;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20351;&#29992;&#28041;&#21450;UNet&#22411;&#26550;&#26500;&#30340;&#27969;&#31243;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25490;&#21517;&#31532;&#19968;&#30340;&#26041;&#27861;ItNet&#20351;&#29992;&#21253;&#25324;&#28388;&#27874;&#21453;&#25237;&#24433;&#65288;FBP&#65289;&#12289;&#22312;CT&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;UNet&#21644;&#36845;&#20195;&#32454;&#21270;&#27493;&#39588;&#30340;&#19977;&#38454;&#27573;&#27969;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#31616;&#21333;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#12290;&#31532;&#19968;&#38454;&#27573;&#20063;&#20351;&#29992;&#20102;FBP&#65292;&#32780;&#26032;&#39062;&#20043;&#22788;&#22312;&#20110;&#31532;&#20108;&#38454;&#27573;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#29305;&#28857;&#26159;CT&#22270;&#20687;&#22686;&#24378;&#38454;&#27573;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#28857;&#22312;&#20110;&#31070;&#32463;&#32593;&#32476;&#26159;&#39044;&#35757;&#32451;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03551v1 Announce Type: cross  Abstract: Computed Tomography (CT) is a widely used medical imaging modality, and as it is based on ionizing radiation, it is desirable to minimize the radiation dose. However, a reduced radiation dose comes with reduced image quality, and reconstruction from low-dose CT (LDCT) data is still a challenging task which is subject to research. According to the LoDoPaB-CT benchmark, a benchmark for LDCT reconstruction, many state-of-the-art methods use pipelines involving UNet-type architectures. Specifically the top ranking method, ItNet, employs a three-stage process involving filtered backprojection (FBP), a UNet trained on CT data, and an iterative refinement step. In this paper, we propose a less complex two-stage method. The first stage also employs FBP, while the novelty lies in the training strategy for the second stage, characterized as the CT image enhancement stage. The crucial point of our approach is that the neural network is pretrained
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LoGoNet&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#37319;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#24212;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#25361;&#25112;&#12290;LoGoNet&#36890;&#36807;&#37319;&#29992;&#22823;&#20869;&#26680;&#27880;&#24847;&#21147;&#21644;&#21452;&#37325;&#32534;&#30721;&#31574;&#30053;&#65292;&#28789;&#27963;&#25429;&#25417;&#38271;&#12289;&#30701;&#36317;&#31163;&#29305;&#24449;&#30456;&#20851;&#24615;&#12290;&#36825;&#31181;&#21019;&#26032;&#30340;&#32452;&#21512;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#29305;&#21035;&#26377;&#30410;&#12290;</title><link>https://arxiv.org/abs/2402.06190</link><description>&lt;p&gt;
Masked LoGoNet&#65306;&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#24555;&#36895;&#20934;&#30830;3D&#22270;&#20687;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Masked LoGoNet: Fast and Accurate 3D Image Analysis for Medical Domain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LoGoNet&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#37319;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#24212;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#25361;&#25112;&#12290;LoGoNet&#36890;&#36807;&#37319;&#29992;&#22823;&#20869;&#26680;&#27880;&#24847;&#21147;&#21644;&#21452;&#37325;&#32534;&#30721;&#31574;&#30053;&#65292;&#28789;&#27963;&#25429;&#25417;&#38271;&#12289;&#30701;&#36317;&#31163;&#29305;&#24449;&#30456;&#20851;&#24615;&#12290;&#36825;&#31181;&#21019;&#26032;&#30340;&#32452;&#21512;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#29305;&#21035;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#22270;&#20687;&#26041;&#27861;&#22312;&#21307;&#23398;&#24212;&#29992;&#20013;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#25968;&#25454;&#38598;&#26500;&#24314;&#30340;&#39640;&#25104;&#26412;&#21644;&#26377;&#38480;&#30340;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#37096;&#32626;&#26102;&#36890;&#24120;&#29992;&#20110;&#27599;&#22825;&#22788;&#29702;&#22823;&#37327;&#25968;&#25454;&#65292;&#32473;&#21307;&#30103;&#35774;&#26045;&#24102;&#26469;&#39640;&#32500;&#25252;&#25104;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;LoGoNet&#65292;&#37319;&#29992;&#23450;&#21046;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20123;&#25361;&#25112;&#12290;LoGoNet&#22312;U&#24418;&#26550;&#26500;&#20869;&#25972;&#21512;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#21033;&#29992;&#22823;&#20869;&#26680;&#27880;&#24847;&#21147;&#65288;LKA&#65289;&#21644;&#21452;&#37325;&#32534;&#30721;&#31574;&#30053;&#65292;&#28789;&#27963;&#22320;&#25429;&#25417;&#38271;&#12289;&#30701;&#36317;&#31163;&#29305;&#24449;&#30456;&#20851;&#24615;&#12290;&#36825;&#19982;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#22686;&#21152;&#32593;&#32476;&#23481;&#37327;&#20197;&#22686;&#24378;&#29305;&#24449;&#25552;&#21462;&#30340;&#26041;&#24335;&#24418;&#25104;&#23545;&#27604;&#12290;&#25105;&#20204;&#27169;&#22411;&#20013;&#36825;&#20123;&#26032;&#25216;&#26415;&#30340;&#32452;&#21512;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#29305;&#21035;&#26377;&#30410;&#65292;&#32771;&#34385;&#21040;&#20854;&#22256;&#38590;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard modern machine-learning-based imaging methods have faced challenges in medical applications due to the high cost of dataset construction and, thereby, the limited labeled training data available. Additionally, upon deployment, these methods are usually used to process a large volume of data on a daily basis, imposing a high maintenance cost on medical facilities. In this paper, we introduce a new neural network architecture, termed LoGoNet, with a tailored self-supervised learning (SSL) method to mitigate such challenges. LoGoNet integrates a novel feature extractor within a U-shaped architecture, leveraging Large Kernel Attention (LKA) and a dual encoding strategy to capture both long-range and short-range feature dependencies adeptly. This is in contrast to existing methods that rely on increasing network capacity to enhance feature extraction. This combination of novel techniques in our model is especially beneficial in medical image segmentation, given the difficulty of le
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#23569;&#27425;&#35843;&#29992;&#30340;&#26041;&#27861;&#31363;&#21462;&#40657;&#30418;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#31361;&#30772;&#20102;&#35775;&#38382;&#38480;&#21046;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2310.00096</link><description>&lt;p&gt;
&#36817;&#20284;&#40657;&#30418;&#27169;&#22411;&#31363;&#21462;&#30340;&#23569;&#27425;&#35843;&#29992;&#26041;&#27861;&#65306;&#27963;&#36291;&#33258;&#36866;&#24212;&#30693;&#35782;&#33976;&#39311;&#21644;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Towards Few-Call Model Stealing via Active Self-Paced Knowledge Distillation and Diffusion-Based Image Generation. (arXiv:2310.00096v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#23569;&#27425;&#35843;&#29992;&#30340;&#26041;&#27861;&#31363;&#21462;&#40657;&#30418;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#31361;&#30772;&#20102;&#35775;&#38382;&#38480;&#21046;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29992;&#20363;&#65292;&#21363;&#22312;&#27809;&#26377;&#35775;&#38382;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#12289;&#26550;&#26500;&#21644;&#27169;&#22411;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#22797;&#21046;&#40657;&#30418;&#20998;&#31867;&#27169;&#22411;&#65292;&#21363;&#21482;&#33021;&#36890;&#36807;&#25512;&#29702;API&#20351;&#29992;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21482;&#33021;&#35266;&#23519;&#21040;&#19968;&#20123;&#22270;&#20687;&#26679;&#26412;&#20316;&#20026;&#36755;&#20837;&#20256;&#36882;&#32473;&#27169;&#22411;&#26102;&#30340;&#65288;&#36719;&#24615;&#25110;&#30828;&#24615;&#65289;&#26631;&#31614;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32771;&#34385;&#21040;&#38480;&#21046;&#27169;&#22411;&#35843;&#29992;&#27425;&#25968;&#30340;&#39069;&#22806;&#32422;&#26463;&#65292;&#20027;&#35201;&#20851;&#27880;&#20110;&#23569;&#27425;&#35843;&#29992;&#30340;&#27169;&#22411;&#31363;&#21462;&#12290;&#20026;&#20102;&#22312;&#24212;&#29992;&#38480;&#21046;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#27169;&#22411;&#25552;&#21462;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20197;&#19979;&#26694;&#26550;&#12290;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#65292;&#25105;&#20204;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#36924;&#30495;&#19988;&#22810;&#26679;&#21270;&#30340;&#22270;&#20687;&#21019;&#24314;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#65288;&#31216;&#20026;&#20195;&#29702;&#25968;&#25454;&#38598;&#65289;&#12290;&#32473;&#23450;&#20801;&#35768;&#30340;&#26368;&#22823;API&#35843;&#29992;&#27425;&#25968;&#65292;&#25105;&#20204;&#20256;&#36882;&#30456;&#24212;&#25968;&#37327;&#30340;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models showcased strong capabilities in image synthesis, being used in many computer vision tasks with great success. To this end, we propose to explore a new use case, namely to copy black-box classification models without having access to the original training data, the architecture, and the weights of the model, \ie~the model is only exposed through an inference API. More specifically, we can only observe the (soft or hard) labels for some image samples passed as input to the model. Furthermore, we consider an additional constraint limiting the number of model calls, mostly focusing our research on few-call model stealing. In order to solve the model extraction task given the applied restrictions, we propose the following framework. As training data, we create a synthetic data set (called proxy data set) by leveraging the ability of diffusion models to generate realistic and diverse images. Given a maximum number of allowed API calls, we pass the respective number of sampl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#30340;&#34987;&#24573;&#35270;&#30340;&#25928;&#29575;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#26469;&#25506;&#35752;16&#20301;&#21644;32&#20301;&#27169;&#22411;&#30340;&#24046;&#24322;&#65292;&#24182;&#21487;&#20197;&#23450;&#37327;&#35299;&#37322;16&#20301;&#27169;&#22411;&#19982;&#20854;32&#20301;&#23545;&#24212;&#29289;&#20043;&#38388;&#30340;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2305.10947</link><description>&lt;p&gt;
&#20851;&#20110;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#30340;&#36777;&#25252;
&lt;/p&gt;
&lt;p&gt;
In Defense of Pure 16-bit Floating-Point Neural Networks. (arXiv:2305.10947v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#30340;&#34987;&#24573;&#35270;&#30340;&#25928;&#29575;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#26469;&#25506;&#35752;16&#20301;&#21644;32&#20301;&#27169;&#22411;&#30340;&#24046;&#24322;&#65292;&#24182;&#21487;&#20197;&#23450;&#37327;&#35299;&#37322;16&#20301;&#27169;&#22411;&#19982;&#20854;32&#20301;&#23545;&#24212;&#29289;&#20043;&#38388;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#23569;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#21644;&#28608;&#27963;&#25152;&#38656;&#30340;&#20301;&#25968;&#26159;&#38750;&#24120;&#21487;&#21462;&#30340;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#21152;&#24555;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#12290;&#22240;&#27492;&#65292;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20197;&#24320;&#21457;&#21033;&#29992;&#26356;&#20302;&#31934;&#24230;&#35745;&#31639;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#27604;&#22914;&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#30446;&#21069;&#19981;&#23384;&#22312;&#32431;16&#20301;&#28014;&#28857;&#35774;&#32622;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#34987;&#24573;&#35270;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#26469;&#25506;&#35752;&#36896;&#25104;16&#20301;&#21644;32&#20301;&#27169;&#22411;&#30340;&#24046;&#24322;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#35268;&#33539;&#21270;&#20102;&#28014;&#28857;&#35823;&#24046;&#21644;&#23481;&#24525;&#24230;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#21487;&#20197;&#23450;&#37327;&#35299;&#37322;16&#20301;&#27169;&#22411;&#19982;&#20854;32&#20301;&#23545;&#24212;&#29289;&#20043;&#38388;&#23494;&#20999;&#36924;&#36817;&#32467;&#26524;&#30340;&#26465;&#20214;&#12290;&#36825;&#31181;&#29702;&#35770;&#25506;&#32034;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reducing the number of bits needed to encode the weights and activations of neural networks is highly desirable as it speeds up their training and inference time while reducing memory consumption. For these reasons, research in this area has attracted significant attention toward developing neural networks that leverage lower-precision computing, such as mixed-precision training. Interestingly, none of the existing approaches has investigated pure 16-bit floating-point settings. In this paper, we shed light on the overlooked efficiency of pure 16-bit floating-point neural networks. As such, we provide a comprehensive theoretical analysis to investigate the factors contributing to the differences observed between 16-bit and 32-bit models. We formalize the concepts of floating-point error and tolerance, enabling us to quantitatively explain the conditions under which a 16-bit model can closely approximate the results of its 32-bit counterpart. This theoretical exploration offers perspect
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#34394;&#25311;&#23548;&#33322;&#8221;&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#26234;&#33021;&#20307;&#30340;&#30456;&#26426;&#35270;&#22270;&#19978;&#21472;&#21152;&#24425;&#33394;&#36335;&#24452;&#25110;&#29699;&#30340;&#24418;&#24335;&#30340;&#35270;&#35273;&#25351;&#24341;&#65292;&#20197;&#26131;&#20110;&#29702;&#35299;&#30340;&#23548;&#33322;&#25351;&#20196;&#20256;&#36798;&#25277;&#35937;&#30340;&#23548;&#33322;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#65292;&#34394;&#25311;&#23548;&#33322;&#22312;&#36981;&#24490;&#35745;&#21010;&#36335;&#24452;&#21644;&#36991;&#24320;&#38556;&#30861;&#29289;&#31561;&#22810;&#20010;&#25351;&#26631;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.02731</link><description>&lt;p&gt;
&#23548;&#33322;&#30340;&#20013;&#23618;&#34920;&#31034;&#8212;&#8212;&#34394;&#25311;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Virtual Guidance as a Mid-level Representation for Navigation. (arXiv:2303.02731v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02731
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#34394;&#25311;&#23548;&#33322;&#8221;&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#26234;&#33021;&#20307;&#30340;&#30456;&#26426;&#35270;&#22270;&#19978;&#21472;&#21152;&#24425;&#33394;&#36335;&#24452;&#25110;&#29699;&#30340;&#24418;&#24335;&#30340;&#35270;&#35273;&#25351;&#24341;&#65292;&#20197;&#26131;&#20110;&#29702;&#35299;&#30340;&#23548;&#33322;&#25351;&#20196;&#20256;&#36798;&#25277;&#35937;&#30340;&#23548;&#33322;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#65292;&#34394;&#25311;&#23548;&#33322;&#22312;&#36981;&#24490;&#35745;&#21010;&#36335;&#24452;&#21644;&#36991;&#24320;&#38556;&#30861;&#29289;&#31561;&#22810;&#20010;&#25351;&#26631;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#20027;&#23548;&#33322;&#30340;&#32972;&#26223;&#19979;&#65292;&#26377;&#25928;&#22320;&#20256;&#36798;&#25277;&#35937;&#30340;&#23548;&#33322;&#25351;&#24341;&#32473;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#26234;&#33021;&#20307;&#23384;&#22312;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#24403;&#23548;&#33322;&#20449;&#24687;&#26159;&#22810;&#27169;&#24577;&#30340;&#26102;&#20505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#34394;&#25311;&#23548;&#33322;&#8221;&#30340;&#26032;&#25216;&#26415;&#65292;&#26088;&#22312;&#20197;&#35270;&#35273;&#26041;&#24335;&#21576;&#29616;&#38750;&#35270;&#35273;&#25351;&#20196;&#20449;&#21495;&#12290;&#36825;&#20123;&#35270;&#35273;&#25351;&#24341;&#20197;&#24425;&#33394;&#36335;&#24452;&#25110;&#29699;&#30340;&#24418;&#24335;&#21472;&#21152;&#22312;&#26234;&#33021;&#20307;&#30340;&#30456;&#26426;&#35270;&#22270;&#19978;&#65292;&#20316;&#20026;&#26131;&#20110;&#29702;&#35299;&#30340;&#23548;&#33322;&#25351;&#20196;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#36827;&#34892;&#23454;&#39564;&#26469;&#35780;&#20272;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#30340;&#34394;&#25311;&#23548;&#33322;&#22312;&#22810;&#39033;&#25351;&#26631;&#19978;&#20248;&#20110;&#22522;&#32447;&#28151;&#21512;&#26041;&#27861;&#65292;&#21253;&#25324;&#36981;&#24490;&#35745;&#21010;&#36335;&#24452;&#21644;&#36991;&#24320;&#38556;&#30861;&#29289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#34394;&#25311;&#23548;&#33322;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#23558;&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#30340;&#25351;&#20196;&#36716;&#25442;&#20026;&#29992;&#20110;&#30495;&#23454;&#29615;&#22659;&#23454;&#39564;&#30340;&#30452;&#35266;&#35270;&#35273;&#26684;&#24335;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#39564;&#35777;&#20102;&#34394;&#25311;&#23548;&#33322;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of autonomous navigation, effectively conveying abstract navigational cues to agents in dynamic environments poses challenges, particularly when the navigation information is multimodal. To address this issue, the paper introduces a novel technique termed "Virtual Guidance," which is designed to visually represent non-visual instructional signals. These visual cues, rendered as colored paths or spheres, are overlaid onto the agent's camera view, serving as easily comprehensible navigational instructions. We evaluate our proposed method through experiments in both simulated and real-world settings. In the simulated environments, our virtual guidance outperforms baseline hybrid approaches in several metrics, including adherence to planned routes and obstacle avoidance. Furthermore, we extend the concept of virtual guidance to transform text-prompt-based instructions into a visually intuitive format for real-world experiments. Our results validate the adaptability of virtua
&lt;/p&gt;</description></item></channel></rss>