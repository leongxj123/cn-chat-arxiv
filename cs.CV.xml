<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#20363;&#32423;&#26816;&#32034;&#20219;&#21153;&#65306;PointCloud-Text&#21305;&#37197;&#65288;PTM&#65289;&#65292;&#24182;&#26500;&#24314;&#20102;&#19977;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#20197;&#35299;&#20915;&#25968;&#25454;&#31232;&#30095;&#12289;&#25991;&#26412;&#27169;&#31946;&#31561;&#25361;&#25112;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;RoMa&#26041;&#27861;&#20316;&#20026;PTM&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.19386</link><description>&lt;p&gt;
PointCloud-Text&#21305;&#37197;&#65306;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
PointCloud-Text Matching: Benchmark Datasets and a Baseline
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#20363;&#32423;&#26816;&#32034;&#20219;&#21153;&#65306;PointCloud-Text&#21305;&#37197;&#65288;PTM&#65289;&#65292;&#24182;&#26500;&#24314;&#20102;&#19977;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#20197;&#35299;&#20915;&#25968;&#25454;&#31232;&#30095;&#12289;&#25991;&#26412;&#27169;&#31946;&#31561;&#25361;&#25112;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;RoMa&#26041;&#27861;&#20316;&#20026;PTM&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#21644;&#30740;&#31350;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#20363;&#32423;&#26816;&#32034;&#20219;&#21153;&#65306;PointCloud-Text Matching&#65288;PTM&#65289;&#65292;&#26088;&#22312;&#25214;&#21040;&#19982;&#32473;&#23450;&#30340;&#28857;&#20113;&#26597;&#35810;&#25110;&#25991;&#26412;&#26597;&#35810;&#21305;&#37197;&#30340;&#30830;&#20999;&#36328;&#27169;&#24577;&#23454;&#20363;&#12290;PTM&#21487;&#24212;&#29992;&#20110;&#21508;&#31181;&#22330;&#26223;&#65292;&#22914;&#23460;&#20869;/&#22478;&#24066;&#23777;&#35895;&#23450;&#20301;&#21644;&#22330;&#26223;&#26816;&#32034;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#23578;&#26080;&#36866;&#29992;&#30340;&#12289;&#26377;&#38024;&#23545;&#24615;&#30340;PTM&#25968;&#25454;&#38598;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19977;&#20010;&#26032;&#30340;PTM&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#20026;3D2T-SR&#12289;3D2T-NR&#21644;3D2T-QA&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#25968;&#25454;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#30001;&#20110;&#28857;&#20113;&#30340;&#31232;&#30095;&#12289;&#22122;&#22768;&#25110;&#26080;&#24207;&#65292;&#20197;&#21450;&#25991;&#26412;&#30340;&#27169;&#31946;&#12289;&#21547;&#31946;&#25110;&#19981;&#23436;&#25972;&#65292;&#23548;&#33268;&#23384;&#22312;&#22024;&#26434;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#20351;&#24471;&#29616;&#26377;&#30340;&#36328;&#27169;&#24577;&#21305;&#37197;&#26041;&#27861;&#23545;PTM&#26080;&#25928;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;PTM&#22522;&#32447;&#65292;&#21629;&#21517;&#20026;Robust PointCloud-Text Matching&#26041;&#27861;&#65288;RoMa&#65289;&#12290;RoMa&#21253;&#21547;&#20004;&#20010;&#27169;&#22359;&#65306;&#21452;&#37325;&#27880;&#24847;&#24863;&#30693;&#27169;&#22359;&#65288;DAP&#65289;&#21644;&#40065;&#26834;&#36127;&#23545;&#27604;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19386v1 Announce Type: cross  Abstract: In this paper, we present and study a new instance-level retrieval task: PointCloud-Text Matching~(PTM), which aims to find the exact cross-modal instance that matches a given point-cloud query or text query. PTM could be applied to various scenarios, such as indoor/urban-canyon localization and scene retrieval. However, there exists no suitable and targeted dataset for PTM in practice. Therefore, we construct three new PTM benchmark datasets, namely 3D2T-SR, 3D2T-NR, and 3D2T-QA. We observe that the data is challenging and with noisy correspondence due to the sparsity, noise, or disorder of point clouds and the ambiguity, vagueness, or incompleteness of texts, which make existing cross-modal matching methods ineffective for PTM. To tackle these challenges, we propose a PTM baseline, named Robust PointCloud-Text Matching method (RoMa). RoMa consists of two modules: a Dual Attention Perception module (DAP) and a Robust Negative Contrast
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;T-TAME&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;&#21367;&#31215;&#32593;&#32476;&#21644;&#35270;&#35273;Transformer&#30340;&#21487;&#35757;&#32451;&#27880;&#24847;&#26426;&#21046;&#65292;&#20026;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#36890;&#29992;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.04523</link><description>&lt;p&gt;
T-TAME&#65306;&#29992;&#20110;&#35299;&#37322;&#21367;&#31215;&#32593;&#32476;&#21644;&#35270;&#35273;Transformer&#30340;&#21487;&#35757;&#32451;&#27880;&#24847;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
T-TAME: Trainable Attention Mechanism for Explaining Convolutional Networks and Vision Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;T-TAME&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;&#21367;&#31215;&#32593;&#32476;&#21644;&#35270;&#35273;Transformer&#30340;&#21487;&#35757;&#32451;&#27880;&#24847;&#26426;&#21046;&#65292;&#20026;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#36890;&#29992;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Vision Transformers&#21644;&#20854;&#20182;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#24555;&#36895;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#8220;&#40657;&#21283;&#23376;&#8221;&#29305;&#24615;&#26159;&#22312;&#38656;&#35201;&#35299;&#37322;&#24615;&#30340;&#24212;&#29992;&#20013;&#37319;&#29992;&#30340;&#38556;&#30861;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#29992;&#20110;&#29983;&#25104;&#35299;&#37322;&#30340;&#25216;&#26415;&#65292;&#20027;&#35201;&#29992;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#20294;&#26159;&#23558;&#36825;&#20123;&#25216;&#26415;&#36866;&#24212;&#21040;&#35270;&#35273;Transformer&#30340;&#26032;&#33539;&#24335;&#26159;&#38750;&#24179;&#20961;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;T-TAME&#65292;Transformer&#20860;&#23481;&#30340;&#21487;&#35757;&#32451;&#27880;&#24847;&#26426;&#21046;&#29992;&#20110;&#35299;&#37322;&#65292;&#36825;&#26159;&#19968;&#31181;&#35828;&#26126;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#25216;&#26415;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#20219;&#20309;&#21367;&#31215;&#25110;&#31867;&#20284;Vision Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;&#31934;&#31616;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;&#35757;&#32451;&#21518;&#65292;&#35299;&#37322;&#22270;&#21487;&#20197;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#35745;&#31639;&#20986;&#65307;&#36825;&#20123;&#35299;&#37322;&#22270;&#21487;&#20197;&#19982;Convolutional Neural Networks&#20013;&#29983;&#25104;&#30340;&#35299;&#37322;&#22270;&#30456;&#23218;&#32654;&#25110;&#32773;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04523v1 Announce Type: cross  Abstract: The development and adoption of Vision Transformers and other deep-learning architectures for image classification tasks has been rapid. However, the "black box" nature of neural networks is a barrier to adoption in applications where explainability is essential. While some techniques for generating explanations have been proposed, primarily for Convolutional Neural Networks, adapting such techniques to the new paradigm of Vision Transformers is non-trivial. This paper presents T-TAME, Transformer-compatible Trainable Attention Mechanism for Explanations, a general methodology for explaining deep neural networks used in image classification tasks. The proposed architecture and training technique can be easily applied to any convolutional or Vision Transformer-like neural network, using a streamlined training approach. After training, explanation maps can be computed in a single forward pass; these explanation maps are comparable to or 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#26410;&#30693;&#25298;&#32477;&#12289;&#26032;&#31867;&#21035;&#21457;&#29616;&#21644;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#65292;&#26412;&#25991;&#25299;&#23637;&#20102;&#24320;&#25918;&#19990;&#30028;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#22810;&#20010;&#28508;&#22312;&#26041;&#21521;</title><link>https://arxiv.org/abs/2403.01759</link><description>&lt;p&gt;
&#24320;&#25918;&#19990;&#30028;&#26426;&#22120;&#23398;&#20064;&#65306;&#22238;&#39038;&#19982;&#26032;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Open-world Machine Learning: A Review and New Outlooks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01759
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#26410;&#30693;&#25298;&#32477;&#12289;&#26032;&#31867;&#21035;&#21457;&#29616;&#21644;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#65292;&#26412;&#25991;&#25299;&#23637;&#20102;&#24320;&#25918;&#19990;&#30028;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#22810;&#20010;&#28508;&#22312;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#22522;&#20110;&#23553;&#38381;&#19990;&#30028;&#20551;&#35774;&#65292;&#21363;&#20551;&#23450;&#29615;&#22659;&#26159;&#38745;&#24577;&#30340;&#65292;&#27169;&#22411;&#19968;&#26086;&#37096;&#32626;&#23601;&#26159;&#22266;&#23450;&#30340;&#12290;&#22312;&#35768;&#22810;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;&#36825;&#31181;&#22522;&#26412;&#19988;&#30456;&#24403;&#24188;&#31258;&#30340;&#20551;&#35774;&#21487;&#33021;&#19981;&#25104;&#31435;&#65292;&#22240;&#20026;&#24320;&#25918;&#29615;&#22659;&#22797;&#26434;&#12289;&#21160;&#24577;&#19988;&#20805;&#28385;&#26410;&#30693;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25298;&#32477;&#26410;&#30693;&#12289;&#21457;&#29616;&#26032;&#22855;&#28857;&#65292;&#28982;&#21518;&#36880;&#27493;&#23398;&#20064;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#20687;&#29983;&#29289;&#31995;&#32479;&#19968;&#26679;&#23433;&#20840;&#22320;&#24182;&#25345;&#32493;&#36827;&#21270;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#26410;&#30693;&#25298;&#32477;&#12289;&#26032;&#31867;&#21035;&#21457;&#29616;&#21644;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#22312;&#32479;&#19968;&#33539;&#24335;&#20013;&#65292;&#25552;&#20379;&#20102;&#23545;&#24320;&#25918;&#19990;&#30028;&#26426;&#22120;&#23398;&#20064;&#30340;&#25972;&#20307;&#35266;&#28857;&#12290;&#35814;&#32454;&#35752;&#35770;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#25361;&#25112;&#12289;&#21407;&#21017;&#21644;&#23616;&#38480;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20960;&#20010;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20221;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01759v1 Announce Type: new  Abstract: Machine learning has achieved remarkable success in many applications. However, existing studies are largely based on the closed-world assumption, which assumes that the environment is stationary, and the model is fixed once deployed. In many real-world applications, this fundamental and rather naive assumption may not hold because an open environment is complex, dynamic, and full of unknowns. In such cases, rejecting unknowns, discovering novelties, and then incrementally learning them, could enable models to be safe and evolve continually as biological systems do. This paper provides a holistic view of open-world machine learning by investigating unknown rejection, novel class discovery, and class-incremental learning in a unified paradigm. The challenges, principles, and limitations of current methodologies are discussed in detail. Finally, we discuss several potential directions for future research. This paper aims to provide a compr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25104;&#23545;&#21051;&#26495;&#21360;&#35937;&#27979;&#35797;&#65288;PST&#65289;&#26694;&#26550;&#65292;&#22312;&#25991;&#26412;-&#22270;&#20687;&#27169;&#22411;&#20013;&#25506;&#31350;&#24615;&#21035;&#20559;&#35265;&#65292;&#24182;&#35780;&#20272;&#20102;DALLE-3&#22312;&#24615;&#21035;&#32844;&#19994;&#21644;&#32452;&#32455;&#26435;&#21147;&#26041;&#38754;&#30340;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.11089</link><description>&lt;p&gt;
&#30007;&#24615;CEO&#21644;&#22899;&#24615;&#21161;&#29702;&#65306;&#36890;&#36807;&#25104;&#23545;&#21051;&#26495;&#21360;&#35937;&#27979;&#35797;&#25506;&#31350;&#25991;&#26412;-&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
The Male CEO and the Female Assistant: Probing Gender Biases in Text-To-Image Models Through Paired Stereotype Test
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11089
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25104;&#23545;&#21051;&#26495;&#21360;&#35937;&#27979;&#35797;&#65288;PST&#65289;&#26694;&#26550;&#65292;&#22312;&#25991;&#26412;-&#22270;&#20687;&#27169;&#22411;&#20013;&#25506;&#31350;&#24615;&#21035;&#20559;&#35265;&#65292;&#24182;&#35780;&#20272;&#20102;DALLE-3&#22312;&#24615;&#21035;&#32844;&#19994;&#21644;&#32452;&#32455;&#26435;&#21147;&#26041;&#38754;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#65288;&#22914;DALLE-3&#65289;&#23637;&#31034;&#20102;&#22312;&#26032;&#24212;&#29992;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#20063;&#38754;&#20020;&#21069;&#25152;&#26410;&#26377;&#30340;&#20844;&#24179;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#21333;&#20154;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#20294;T2I&#27169;&#22411;&#24212;&#29992;&#21487;&#33021;&#38656;&#35201;&#21516;&#26102;&#25551;&#32472;&#20004;&#20010;&#25110;&#26356;&#22810;&#20154;&#12290;&#35813;&#35774;&#23450;&#20013;&#30340;&#28508;&#22312;&#20559;&#35265;&#20173;&#26410;&#34987;&#25506;&#31350;&#65292;&#23548;&#33268;&#20351;&#29992;&#20013;&#30340;&#20844;&#24179;&#30456;&#20851;&#39118;&#38505;&#12290;&#20026;&#20102;&#30740;&#31350;T2I&#27169;&#22411;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#22522;&#26412;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25104;&#23545;&#21051;&#26495;&#21360;&#35937;&#27979;&#35797;&#65288;PST&#65289;&#20559;&#35265;&#35780;&#20272;&#26694;&#26550;&#12290;PST&#20419;&#20351;&#27169;&#22411;&#29983;&#25104;&#21516;&#19968;&#22270;&#20687;&#20013;&#30340;&#20004;&#20010;&#20010;&#20307;&#65292;&#29992;&#19982;&#30456;&#21453;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#30456;&#20851;&#32852;&#30340;&#20004;&#20010;&#31038;&#20250;&#36523;&#20221;&#26469;&#25551;&#36848;&#20182;&#20204;&#12290;&#36890;&#36807;&#29983;&#25104;&#30340;&#22270;&#20687;&#36981;&#20174;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#30340;&#31243;&#24230;&#26469;&#34913;&#37327;&#20559;&#35265;&#12290;&#21033;&#29992;PST&#65292;&#25105;&#20204;&#20174;&#20004;&#20010;&#35282;&#24230;&#35780;&#20272;DALLE-3&#65306;&#24615;&#21035;&#32844;&#19994;&#20013;&#30340;&#20559;&#35265;&#21644;&#32452;&#32455;&#26435;&#21147;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11089v1 Announce Type: cross  Abstract: Recent large-scale Text-To-Image (T2I) models such as DALLE-3 demonstrate great potential in new applications, but also face unprecedented fairness challenges. Prior studies revealed gender biases in single-person image generation, but T2I model applications might require portraying two or more people simultaneously. Potential biases in this setting remain unexplored, leading to fairness-related risks in usage. To study these underlying facets of gender biases in T2I models, we propose a novel Paired Stereotype Test (PST) bias evaluation framework. PST prompts the model to generate two individuals in the same image. They are described with two social identities that are stereotypically associated with the opposite gender. Biases can then be measured by the level of conformation to gender stereotypes in generated images. Using PST, we evaluate DALLE-3 from 2 perspectives: biases in gendered occupation and biases in organizational power.
&lt;/p&gt;</description></item></channel></rss>