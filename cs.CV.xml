<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MoPE&#25216;&#26415;&#65292;&#36890;&#36807;&#35299;&#24320;&#25552;&#31034;&#20197;&#33258;&#36866;&#24212;&#25429;&#33719;&#25968;&#25454;&#38598;&#32423;&#21644;&#23454;&#20363;&#32423;&#29305;&#24449;&#65292;&#24341;&#20837;&#20102;&#28151;&#21512;Prompt&#19987;&#23478;&#26469;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10568</link><description>&lt;p&gt;
MoPE&#65306;&#36890;&#36807;Prompt&#19987;&#23478;&#28151;&#21512;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
MoPE: Parameter-Efficient and Scalable Multimodal Fusion via Mixture of Prompt Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MoPE&#25216;&#26415;&#65292;&#36890;&#36807;&#35299;&#24320;&#25552;&#31034;&#20197;&#33258;&#36866;&#24212;&#25429;&#33719;&#25968;&#25454;&#38598;&#32423;&#21644;&#23454;&#20363;&#32423;&#29305;&#24449;&#65292;&#24341;&#20837;&#20102;&#28151;&#21512;Prompt&#19987;&#23478;&#26469;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt&#35843;&#25972;&#24050;&#32463;&#35777;&#26126;&#22312;&#34701;&#21512;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#21333;&#27169;&#22522;&#30784;&#27169;&#22411;&#26102;&#20855;&#26377;&#21442;&#25968;&#25928;&#29575;&#24615;&#12290;&#28982;&#32780;&#65292;&#20854;&#26377;&#38480;&#30340;&#36866;&#24212;&#24615;&#21644;&#34920;&#36798;&#33021;&#21147;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#19982;&#20854;&#20182;&#35843;&#25972;&#26041;&#27861;&#30456;&#27604;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#31616;&#21333;&#25552;&#31034;&#35299;&#24320;&#20197;&#33258;&#36866;&#24212;&#22320;&#25429;&#33719;&#25968;&#25454;&#38598;&#32423;&#21644;&#23454;&#20363;&#32423;&#29305;&#24449;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#24314;&#31435;&#22312;&#36825;&#31181;&#35299;&#24320;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Prompt&#19987;&#23478;&#30340;&#28151;&#21512;&#65288;MoPE&#65289;&#25216;&#26415;&#26469;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#12290;MoPE&#21033;&#29992;&#22810;&#27169;&#24577;&#37197;&#23545;&#20808;&#39564;&#22312;&#27599;&#20010;&#23454;&#20363;&#22522;&#30784;&#19978;&#36335;&#30001;&#26368;&#26377;&#25928;&#30340;&#25552;&#31034;&#12290;&#19982;&#31616;&#21333;&#25552;&#31034;&#30456;&#27604;&#65292;&#25105;&#20204;&#22522;&#20110;MoPE&#30340;&#26465;&#20214;&#25552;&#31034;&#23545;&#22810;&#27169;&#24577;&#34701;&#21512;&#20855;&#26377;&#26356;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#21644;&#21487;&#35757;&#32451;&#21442;&#25968;&#24635;&#25968;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19968;&#20010;&#19987;&#23478;&#36335;&#30001;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#23548;&#33268;&#19987;&#23478;&#30340;&#19981;&#26029;&#21457;&#23637;&#19987;&#38271;&#65292;&#19981;&#21516;&#19987;&#23478;&#19987;&#27880;&#20110;&#19981;&#21516;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10568v1 Announce Type: cross  Abstract: Prompt-tuning has demonstrated parameter-efficiency in fusing unimodal foundation models for multimodal tasks. However, its limited adaptivity and expressiveness lead to suboptimal performance when compared with other tuning methods. In this paper, we address this issue by disentangling the vanilla prompts to adaptively capture dataset-level and instance-level features. Building upon this disentanglement, we introduce the mixture of prompt experts (MoPE) technique to enhance expressiveness. MoPE leverages multimodal pairing priors to route the most effective prompt on a per-instance basis. Compared to vanilla prompting, our MoPE-based conditional prompting exhibits greater expressiveness for multimodal fusion, scaling better with the training data and the overall number of trainable parameters. We also study a regularization term for expert routing, leading to emergent expert specialization, where different experts focus on different c
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#22312;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#12289;&#20934;&#30830;&#22320;&#38477;&#23610;&#24230;&#20219;&#24847;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#65292;&#24182;&#20135;&#29983;&#27010;&#29575;&#24615;&#38477;&#23610;&#24230;&#22330;&#12290;</title><link>https://arxiv.org/abs/2403.02774</link><description>&lt;p&gt;
&#24555;&#36895;&#12289;&#33258;&#36866;&#24212;&#23610;&#24230;&#21644;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#24847;&#35782;&#30340;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#22330;&#38477;&#23610;&#24230;&#19982;&#29983;&#25104;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fast, Scale-Adaptive, and Uncertainty-Aware Downscaling of Earth System Model Fields with Generative Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02774
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#22312;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#12289;&#20934;&#30830;&#22320;&#38477;&#23610;&#24230;&#20219;&#24847;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#65292;&#24182;&#20135;&#29983;&#27010;&#29575;&#24615;&#38477;&#23610;&#24230;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#21644;&#39640;&#20998;&#36776;&#29575;&#30340;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;(ESM)&#27169;&#25311;&#23545;&#20110;&#35780;&#20272;&#20154;&#20026;&#27668;&#20505;&#21464;&#21270;&#23545;&#29983;&#24577;&#21644;&#31038;&#20250;&#32463;&#27982;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#12290;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;ESM&#27169;&#25311;&#30340;&#38477;&#23610;&#24230;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#32479;&#35745;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23545;&#27599;&#20010;ESM&#37117;&#38656;&#35201;&#35745;&#31639;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#27668;&#20505;&#39044;&#27979;&#25928;&#26524;&#24046;&#12290;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#19968;&#33268;&#24615;&#27169;&#22411;(CM)&#65292;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#39640;&#25928;&#20934;&#30830;&#22320;&#38477;&#23610;&#24230;&#20219;&#24847;ESM&#27169;&#25311;&#26469;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#12290;&#25105;&#20204;&#30340;&#22522;&#30784;&#27169;&#22411;&#26041;&#27861;&#20197;&#21482;&#21463;&#35266;&#27979;&#21442;&#32771;&#25968;&#25454;&#38480;&#21046;&#30340;&#20998;&#36776;&#29575;&#20135;&#29983;&#27010;&#29575;&#24615;&#38477;&#23610;&#24230;&#22330;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CM&#22312;&#32500;&#25345;&#39640;&#21487;&#25511;&#24615;&#30340;&#21516;&#26102;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02774v1 Announce Type: cross  Abstract: Accurate and high-resolution Earth system model (ESM) simulations are essential to assess the ecological and socio-economic impacts of anthropogenic climate change, but are computationally too expensive. Recent machine learning approaches have shown promising results in downscaling ESM simulations, outperforming state-of-the-art statistical approaches. However, existing methods require computationally costly retraining for each ESM and extrapolate poorly to climates unseen during training. We address these shortcomings by learning a consistency model (CM) that efficiently and accurately downscales arbitrary ESM simulations without retraining in a zero-shot manner. Our foundation model approach yields probabilistic downscaled fields at resolution only limited by the observational reference data. We show that the CM outperforms state-of-the-art diffusion models at a fraction of computational cost while maintaining high controllability on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#20248;&#33539;&#24335;&#65292;&#36890;&#36807;&#20248;&#21270;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#65292;&#36843;&#20351;&#27169;&#22411;&#21033;&#29992;&#30495;&#27491;&#30340;&#22240;&#26524;&#19981;&#21464;&#29305;&#24449;&#65292;&#20197;&#35299;&#20915;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#20219;&#21153;&#38656;&#27714;&#19978;&#26080;&#27861;&#26377;&#25928;&#21033;&#29992;&#39044;&#35757;&#32451;&#29305;&#24449;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.00376</link><description>&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#27867;&#21270;&#30340;&#19981;&#21464;&#27979;&#35797;&#26102;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Invariant Test-Time Adaptation for Vision-Language Model Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#20248;&#33539;&#24335;&#65292;&#36890;&#36807;&#20248;&#21270;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#65292;&#36843;&#20351;&#27169;&#22411;&#21033;&#29992;&#30495;&#27491;&#30340;&#22240;&#26524;&#19981;&#21464;&#29305;&#24449;&#65292;&#20197;&#35299;&#20915;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#20219;&#21153;&#38656;&#27714;&#19978;&#26080;&#27861;&#26377;&#25928;&#21033;&#29992;&#39044;&#35757;&#32451;&#29305;&#24449;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00376v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;&#25688;&#35201;: &#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#22312;&#22823;&#37327;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#19978;&#30340;&#21487;&#25193;&#23637;&#24615;&#20351;&#20854;&#22312;&#20247;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#24212;&#29992;&#20110;&#38271;&#23614;&#20219;&#21153;&#65288;&#22914;&#32454;&#31890;&#24230;&#22270;&#20687;&#20998;&#31867;&#65289;&#26102;&#26174;&#31034;&#20986;&#26126;&#26174;&#23616;&#38480;&#65292;&#36825;&#26159;&#30001;&#20110;&#8220;&#20915;&#31574;&#25463;&#24452;&#8221;&#23548;&#33268;&#20102;&#23427;&#20204;&#30340;&#27867;&#21270;&#33021;&#21147;&#21463;&#38480;&#12290;&#26412;&#25991;&#21457;&#29616;CLIP&#27169;&#22411;&#20855;&#26377;&#20016;&#23500;&#30340;&#29305;&#24449;&#38598;&#65292;&#28085;&#30422;&#20102;&#26082;&#26377;&#30340;\textit{&#26399;&#26395;&#19981;&#21464;&#22240;&#26524;&#29305;&#24449;}&#21448;&#26377;&#30340;\textit{&#19981;&#24076;&#26395;&#30340;&#20915;&#31574;&#25463;&#24452;}&#12290;&#27492;&#22806;&#65292;CLIP&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#19981;&#20339;&#28304;&#33258;&#20854;&#26080;&#27861;&#26377;&#25928;&#21033;&#29992;&#39044;&#35757;&#32451;&#29305;&#24449;&#20197;&#31526;&#21512;&#29305;&#23450;&#20219;&#21153;&#35201;&#27714;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#19968;&#31181;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#20248;&#33539;&#24335;&#65292;&#20248;&#21270;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#20419;&#20351;&#27169;&#22411;&#21033;&#29992;&#30495;&#27491;&#30340;&#22240;&#26524;&#19981;&#21464;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00376v1 Announce Type: cross  Abstract: Vision-language foundation models have exhibited remarkable success across a multitude of downstream tasks due to their scalability on extensive image-text paired datasets. However, these models display significant limitations when applied to long-tail tasks, such as fine-grained image classification, as a result of "decision shortcuts" that hinders their generalization capabilities. In this work, we find that the CLIP model possesses a rich set of features, encompassing both \textit{desired invariant causal features} and \textit{undesired decision shortcuts}. Moreover, the underperformance of CLIP on downstream tasks originates from its inability to effectively utilize pre-trained features in accordance with specific task requirements. To address this challenge, this paper introduces a test-time prompt tuning paradigm that optimizes a learnable prompt, thereby compelling the model to exploit genuine causal invariant features while dis
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#25991;&#26412;&#20013;&#30340;&#25991;&#23383;&#30340;&#21147;&#37327;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#22320;&#23558;&#25277;&#35937;&#30340;&#25991;&#26412;&#25551;&#36848;&#26144;&#23556;&#21040;&#20855;&#20307;&#30340;&#22270;&#20687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#20154;&#29289;&#26816;&#32034;&#12290;</title><link>http://arxiv.org/abs/2307.09059</link><description>&lt;p&gt;
&#25991;&#23383;&#24819;&#35937;&#30340;&#37322;&#25918;&#65306;&#36890;&#36807;&#25506;&#32034;&#25991;&#23383;&#30340;&#21147;&#37327;&#23454;&#29616;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#20154;&#29289;&#26816;&#32034;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Imagination of Text: A Novel Framework for Text-to-image Person Retrieval via Exploring the Power of Words. (arXiv:2307.09059v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#25991;&#26412;&#20013;&#30340;&#25991;&#23383;&#30340;&#21147;&#37327;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#22320;&#23558;&#25277;&#35937;&#30340;&#25991;&#26412;&#25551;&#36848;&#26144;&#23556;&#21040;&#20855;&#20307;&#30340;&#22270;&#20687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#20154;&#29289;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#20154;&#29289;&#26816;&#32034;&#30340;&#30446;&#26631;&#26159;&#20174;&#22823;&#22411;&#22270;&#24211;&#20013;&#26816;&#32034;&#19982;&#32473;&#23450;&#25991;&#26412;&#25551;&#36848;&#30456;&#21305;&#37197;&#30340;&#20154;&#29289;&#22270;&#20687;&#12290;&#36825;&#20010;&#20219;&#21153;&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#20449;&#24687;&#34920;&#31034;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;&#25991;&#26412;&#27169;&#24577;&#36890;&#36807;&#35789;&#27719;&#21644;&#35821;&#27861;&#32467;&#26500;&#20256;&#36882;&#25277;&#35937;&#21644;&#31934;&#30830;&#30340;&#20449;&#24687;&#65292;&#32780;&#35270;&#35273;&#27169;&#24577;&#36890;&#36807;&#22270;&#20687;&#20256;&#36882;&#20855;&#20307;&#21644;&#30452;&#35266;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#25991;&#23383;&#34920;&#31034;&#30340;&#34920;&#36798;&#21147;&#65292;&#20934;&#30830;&#22320;&#23558;&#25277;&#35937;&#30340;&#25991;&#26412;&#25551;&#36848;&#26144;&#23556;&#21040;&#20855;&#20307;&#22270;&#20687;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#21477;&#23376;&#20013;&#30340;&#25991;&#23383;&#30340;&#21147;&#37327;&#65292;&#37322;&#25918;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#20154;&#29289;&#26816;&#32034;&#20013;&#30340;&#25991;&#23383;&#24819;&#35937;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#20840;&#38754;CLIP&#27169;&#22411;&#20316;&#20026;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#21452;&#32534;&#30721;&#22120;&#65292;&#21033;&#29992;&#20808;&#21069;&#30340;&#36328;&#27169;&#24577;&#23545;&#40784;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of Text-to-image person retrieval is to retrieve person images from a large gallery that match the given textual descriptions. The main challenge of this task lies in the significant differences in information representation between the visual and textual modalities. The textual modality conveys abstract and precise information through vocabulary and grammatical structures, while the visual modality conveys concrete and intuitive information through images. To fully leverage the expressive power of textual representations, it is essential to accurately map abstract textual descriptions to specific images.  To address this issue, we propose a novel framework to Unleash the Imagination of Text (UIT) in text-to-image person retrieval, aiming to fully explore the power of words in sentences. Specifically, the framework employs the pre-trained full CLIP model as a dual encoder for the images and texts , taking advantage of prior cross-modal alignment knowledge. The Text-guided Imag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PastNet&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20613;&#37324;&#21494;&#22495;&#20013;&#24341;&#20837;&#35889;&#21367;&#31215;&#31639;&#23376;&#65292;&#21033;&#29992;&#20869;&#22312;&#30340;&#29289;&#29702;&#30693;&#35782;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26102;&#31354;&#35270;&#39057;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#31163;&#25955;&#21270;&#23616;&#37096;&#29305;&#24449;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.11421</link><description>&lt;p&gt;
PastNet&#65306;&#24341;&#20837;&#29289;&#29702;&#24402;&#32435;&#20559;&#24046;&#29992;&#20110;&#26102;&#31354;&#35270;&#39057;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
PastNet: Introducing Physical Inductive Biases for Spatio-temporal Video Prediction. (arXiv:2305.11421v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PastNet&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20613;&#37324;&#21494;&#22495;&#20013;&#24341;&#20837;&#35889;&#21367;&#31215;&#31639;&#23376;&#65292;&#21033;&#29992;&#20869;&#22312;&#30340;&#29289;&#29702;&#30693;&#35782;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26102;&#31354;&#35270;&#39057;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#31163;&#25955;&#21270;&#23616;&#37096;&#29305;&#24449;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#31354;&#35270;&#39057;&#39044;&#27979;&#30340;&#25361;&#25112;&#65292;&#20854;&#20013;&#28041;&#21450;&#26681;&#25454;&#21382;&#21490;&#25968;&#25454;&#27969;&#29983;&#25104;&#26410;&#26469;&#35270;&#39057;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#35821;&#20041;&#22320;&#22270;&#31561;&#22806;&#37096;&#20449;&#24687;&#22686;&#24378;&#35270;&#39057;&#39044;&#27979;&#65292;&#20294;&#24120;&#24120;&#24573;&#35270;&#35270;&#39057;&#20869;&#22266;&#26377;&#30340;&#29289;&#29702;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#30340;&#39640;&#35745;&#31639;&#38656;&#27714;&#21487;&#33021;&#20250;&#38459;&#30861;&#23545;&#39640;&#20998;&#36776;&#29575;&#35270;&#39057;&#30340;&#24212;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#29289;&#29702;&#36741;&#21161;&#26102;&#31354;&#32593;&#32476;&#65288;PastNet&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35270;&#39057;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;PastNet&#26680;&#24515;&#22312;&#20110;&#22312;&#20613;&#37324;&#21494;&#22495;&#20013;&#24341;&#20837;&#35889;&#21367;&#31215;&#31639;&#23376;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#24341;&#20837;&#22522;&#26412;&#29289;&#29702;&#23450;&#24459;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#20869;&#22312;&#32500;&#24230;&#20272;&#35745;&#30340;&#23384;&#20648;&#22120;&#24211;&#65292;&#22312;&#22788;&#29702;&#22797;&#26434;&#30340;&#26102;&#31354;&#20449;&#21495;&#26102;&#31163;&#25955;&#21270;&#23616;&#37096;&#29305;&#24449;&#65292;&#20174;&#32780;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the challenge of spatio-temporal video prediction, which involves generating future videos based on historical data streams. Existing approaches typically utilize external information such as semantic maps to enhance video prediction, which often neglect the inherent physical knowledge embedded within videos. Furthermore, their high computational demands could impede their applications for high-resolution videos. To address these constraints, we introduce a novel approach called Physics-assisted Spatio-temporal Network (PastNet) for generating high-quality video predictions. The core of our PastNet lies in incorporating a spectral convolution operator in the Fourier domain, which efficiently introduces inductive biases from the underlying physical laws. Additionally, we employ a memory bank with the estimated intrinsic dimensionality to discretize local features during the processing of complex spatio-temporal signals, thereby reducing computational costs 
&lt;/p&gt;</description></item></channel></rss>