<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#36866;&#37197;&#22120;&#35843;&#25972;&#26041;&#27861;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#23637;&#29616;&#20986;&#36739;&#20248;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#22686;&#37327;&#35843;&#25972;&#20849;&#20139;&#36866;&#37197;&#22120;&#21644;&#21033;&#29992;&#23384;&#20648;&#21407;&#22411;&#36827;&#34892;&#29305;&#24449;&#37319;&#26679;&#21644;&#26356;&#26032;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#27169;&#22411;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.19979</link><description>&lt;p&gt;
&#35821;&#20041;&#36716;&#31227;&#22686;&#37327;&#36866;&#37197;&#22120;&#35843;&#25972;&#26159;&#19968;&#31181;&#25345;&#32493;&#30340; ViTransformer
&lt;/p&gt;
&lt;p&gt;
Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19979
&lt;/p&gt;
&lt;p&gt;
&#36866;&#37197;&#22120;&#35843;&#25972;&#26041;&#27861;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#23637;&#29616;&#20986;&#36739;&#20248;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#22686;&#37327;&#35843;&#25972;&#20849;&#20139;&#36866;&#37197;&#22120;&#21644;&#21033;&#29992;&#23384;&#20648;&#21407;&#22411;&#36827;&#34892;&#29305;&#24449;&#37319;&#26679;&#21644;&#26356;&#26032;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#27169;&#22411;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;CIL&#65289;&#26088;&#22312;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#21516;&#26102;&#25345;&#32493;&#23398;&#20064;&#26032;&#30340;&#31867;&#21035;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22312;&#25345;&#32493;&#23398;&#20064;&#32972;&#26223;&#19979;&#30340;&#19981;&#21516;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#65288;PET&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#36866;&#37197;&#22120;&#35843;&#25972;&#34920;&#29616;&#20248;&#20110;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#29978;&#33267;&#22312;&#27599;&#20010;&#23398;&#20064;&#20250;&#35805;&#20013;&#27809;&#26377;&#21442;&#25968;&#25193;&#23637;&#30340;&#24773;&#20917;&#19979;&#20063;&#22914;&#27492;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#37327;&#35843;&#25972;&#20849;&#20139;&#36866;&#37197;&#22120;&#32780;&#19981;&#26045;&#21152;&#21442;&#25968;&#26356;&#26032;&#32422;&#26463;&#65292;&#22686;&#24378;&#39592;&#24178;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#23384;&#20648;&#30340;&#21407;&#22411;&#20013;&#25277;&#21462;&#29305;&#24449;&#26679;&#26412;&#26469;&#37325;&#26032;&#35757;&#32451;&#32479;&#19968;&#30340;&#20998;&#31867;&#22120;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#20272;&#35745;&#26087;&#21407;&#22411;&#30340;&#35821;&#20041;&#36716;&#31227;&#65292;&#32780;&#26080;&#27861;&#35775;&#38382;&#36807;&#21435;&#30340;&#26679;&#26412;&#65292;&#24182;&#36880;&#20010;&#20250;&#35805;&#26356;&#26032;&#23384;&#20648;&#30340;&#21407;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#28040;&#38500;&#20102;&#27169;&#22411;&#30340;&#25193;&#23637;&#21644;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19979v1 Announce Type: cross  Abstract: Class-incremental learning (CIL) aims to enable models to continuously learn new classes while overcoming catastrophic forgetting. The introduction of pre-trained models has brought new tuning paradigms to CIL. In this paper, we revisit different parameter-efficient tuning (PET) methods within the context of continual learning. We observe that adapter tuning demonstrates superiority over prompt-based methods, even without parameter expansion in each learning session. Motivated by this, we propose incrementally tuning the shared adapter without imposing parameter update constraints, enhancing the learning capacity of the backbone. Additionally, we employ feature sampling from stored prototypes to retrain a unified classifier, further improving its performance. We estimate the semantic shift of old prototypes without access to past samples and update stored prototypes session by session. Our proposed method eliminates model expansion and
&lt;/p&gt;</description></item></channel></rss>