<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#23545;&#29031;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#21253;&#21547;&#35780;&#20272;&#23545;&#29031;&#22810;&#20010;&#26041;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#20197;&#21450;&#35780;&#20272;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.20287</link><description>&lt;p&gt;
&#22522;&#20934;&#23545;&#29031;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Counterfactual Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20287
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#23545;&#29031;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#21253;&#21547;&#35780;&#20272;&#23545;&#29031;&#22810;&#20010;&#26041;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#20197;&#21450;&#35780;&#20272;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#29031;&#22270;&#20687;&#29983;&#25104;&#22312;&#29702;&#35299;&#21464;&#37327;&#22240;&#26524;&#20851;&#31995;&#26041;&#38754;&#20855;&#26377;&#20851;&#38190;&#20316;&#29992;&#65292;&#22312;&#35299;&#37322;&#24615;&#21644;&#29983;&#25104;&#26080;&#20559;&#21512;&#25104;&#25968;&#25454;&#26041;&#38754;&#26377;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#22270;&#20687;&#29983;&#25104;&#26412;&#36523;&#23601;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#23545;&#20110;&#35780;&#20272;&#23545;&#29031;&#29983;&#25104;&#30340;&#38656;&#27714;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#36825;&#19968;&#25361;&#25112;&#65292;&#22240;&#20026;&#26681;&#25454;&#23450;&#20041;&#65292;&#23545;&#29031;&#24773;&#26223;&#26159;&#27809;&#26377;&#21487;&#35266;&#27979;&#22522;&#20934;&#20107;&#23454;&#30340;&#20551;&#35774;&#24773;&#20917;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#23545;&#29031;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#26032;&#39062;&#32508;&#21512;&#26694;&#26550;&#12290;&#25105;&#20204;&#32467;&#21512;&#20102;&#20391;&#37325;&#20110;&#35780;&#20272;&#23545;&#29031;&#30340;&#19981;&#21516;&#26041;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#20363;&#22914;&#32452;&#25104;&#12289;&#26377;&#25928;&#24615;&#12289;&#24178;&#39044;&#30340;&#26368;&#23567;&#24615;&#21644;&#22270;&#20687;&#36924;&#30495;&#24230;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#20110;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#33539;&#24335;&#30340;&#19977;&#31181;&#19981;&#21516;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#31867;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36824;&#37197;&#22791;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;Python&#36719;&#20214;&#21253;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20287v1 Announce Type: cross  Abstract: Counterfactual image generation is pivotal for understanding the causal relations of variables, with applications in interpretability and generation of unbiased synthetic data. However, evaluating image generation is a long-standing challenge in itself. The need to evaluate counterfactual generation compounds on this challenge, precisely because counterfactuals, by definition, are hypothetical scenarios without observable ground truths. In this paper, we present a novel comprehensive framework aimed at benchmarking counterfactual image generation methods. We incorporate metrics that focus on evaluating diverse aspects of counterfactuals, such as composition, effectiveness, minimality of interventions, and image realism. We assess the performance of three distinct conditional image generation model types, based on the Structural Causal Model paradigm. Our work is accompanied by a user-friendly Python package which allows to further eval
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#25928;&#31209;&#30340;&#29305;&#24449;&#20016;&#23500;&#24615;&#22686;&#24378;&#65288;RFR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#34920;&#31034;&#30340;&#26377;&#25928;&#31209;&#65292;&#23454;&#29616;&#20102;&#25552;&#39640;&#21069;&#21521;&#20860;&#23481;&#24615;&#30340;&#30446;&#26631;&#12290;&#21516;&#26102;&#65292;&#22312;&#21518;&#21521;&#20860;&#23481;&#24615;&#21644;&#21069;&#21521;&#20860;&#23481;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#21452;&#37325;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.15517</link><description>&lt;p&gt;
&#36890;&#36807;&#22686;&#21152;&#34920;&#31034;&#31209;&#21644;&#29305;&#24449;&#20016;&#23500;&#24615;&#26469;&#25913;&#21892;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#21069;&#21521;&#20860;&#23481;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Forward Compatibility in Class Incremental Learning by Increasing Representation Rank and Feature Richness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15517
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#25928;&#31209;&#30340;&#29305;&#24449;&#20016;&#23500;&#24615;&#22686;&#24378;&#65288;RFR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#34920;&#31034;&#30340;&#26377;&#25928;&#31209;&#65292;&#23454;&#29616;&#20102;&#25552;&#39640;&#21069;&#21521;&#20860;&#23481;&#24615;&#30340;&#30446;&#26631;&#12290;&#21516;&#26102;&#65292;&#22312;&#21518;&#21521;&#20860;&#23481;&#24615;&#21644;&#21069;&#21521;&#20860;&#23481;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#21452;&#37325;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;CIL&#65289;&#26159;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#23376;&#39046;&#22495;&#65292;&#26088;&#22312;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#20445;&#30041;&#20808;&#21069;&#20219;&#21153;&#30693;&#35782;&#30340;&#21516;&#26102;&#36880;&#28176;&#23398;&#20064;&#26032;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#25928;&#31209;&#30340;&#29305;&#24449;&#20016;&#23500;&#24615;&#22686;&#24378;&#65288;RFR&#65289;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#21069;&#21521;&#20860;&#23481;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22686;&#21152;&#22522;&#30784;&#38454;&#27573;&#34920;&#31034;&#30340;&#26377;&#25928;&#31209;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#21512;&#24182;&#26356;&#22810;&#19982;&#26410;&#35265;&#26032;&#20219;&#21153;&#30456;&#20851;&#30340;&#20449;&#24687;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;RFR&#22312;&#21518;&#21521;&#20860;&#23481;&#24615;&#21644;&#21069;&#21521;&#20860;&#23481;&#24615;&#26041;&#38754;&#23454;&#29616;&#20102;&#21452;&#37325;&#30446;&#26631;&#65306;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23567;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15517v1 Announce Type: new  Abstract: Class Incremental Learning (CIL) constitutes a pivotal subfield within continual learning, aimed at enabling models to progressively learn new classification tasks while retaining knowledge obtained from prior tasks. Although previous studies have predominantly focused on backward compatible approaches to mitigate catastrophic forgetting, recent investigations have introduced forward compatible methods to enhance performance on novel tasks and complement existing backward compatible methods. In this study, we introduce an effective-Rank based Feature Richness enhancement (RFR) method, designed for improving forward compatibility. Specifically, this method increases the effective rank of representations during the base session, thereby facilitating the incorporation of more informative features pertinent to unseen novel tasks. Consequently, RFR achieves dual objectives in backward and forward compatibility: minimizing feature extractor mo
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#22312;&#25552;&#39640;&#26893;&#20837;&#24335;&#21548;&#35273;&#35774;&#22791;&#30340;&#35821;&#38899;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#21069;&#30651;&#24615;&#65292;&#24182;&#36890;&#36807;&#20808;&#36827;&#30340;&#20449;&#21495;&#22788;&#29702;&#25216;&#26415;&#20197;&#21450;&#24212;&#23545;&#22810;&#28304;&#35821;&#38899;&#21644;&#29615;&#22659;&#22122;&#38899;&#25361;&#25112;&#31561;&#26041;&#27861;&#26469;&#20811;&#26381;&#35821;&#38899;&#22833;&#30495;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.15442</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#32819;&#34583;&#26893;&#20837;&#35013;&#32622;&#20013;&#30340;&#20808;&#36827;&#31639;&#27861;&#65306;&#21307;&#30103;&#31574;&#30053;&#12289;&#25361;&#25112;&#21644;&#23637;&#26395;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Advanced Artificial Intelligence Algorithms in Cochlear Implants: Review of Healthcare Strategies, Challenges, and Perspectives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15442
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#25552;&#39640;&#26893;&#20837;&#24335;&#21548;&#35273;&#35774;&#22791;&#30340;&#35821;&#38899;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#21069;&#30651;&#24615;&#65292;&#24182;&#36890;&#36807;&#20808;&#36827;&#30340;&#20449;&#21495;&#22788;&#29702;&#25216;&#26415;&#20197;&#21450;&#24212;&#23545;&#22810;&#28304;&#35821;&#38899;&#21644;&#29615;&#22659;&#22122;&#38899;&#25361;&#25112;&#31561;&#26041;&#27861;&#26469;&#20811;&#26381;&#35821;&#38899;&#22833;&#30495;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15442v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#22312;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#19981;&#20165;&#20026;&#19982;&#26426;&#22120;&#20132;&#20114;&#25552;&#20379;&#20102;&#20415;&#21033;&#65292;&#36824;&#20026;&#37096;&#20998;&#25110;&#23436;&#20840;&#21548;&#21147;&#21463;&#25439;&#30340;&#20010;&#20307;&#25552;&#20379;&#20102;&#27807;&#36890;&#30340;&#26426;&#20250;&#12290;&#36825;&#19968;&#36807;&#31243;&#28041;&#21450;&#20197;&#27169;&#25311;&#24418;&#24335;&#25509;&#25910;&#35821;&#38899;&#20449;&#21495;&#65292;&#28982;&#21518;&#36890;&#36807;&#21508;&#31181;&#20449;&#21495;&#22788;&#29702;&#31639;&#27861;&#20351;&#20854;&#19982;&#23481;&#37327;&#26377;&#38480;&#30340;&#35774;&#22791;&#65288;&#22914;CI&#65289;&#20860;&#23481;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#37197;&#22791;&#26377;&#26377;&#38480;&#25968;&#37327;&#30005;&#26497;&#30340;&#26893;&#20837;&#35013;&#32622;&#22312;&#21512;&#25104;&#36807;&#31243;&#20013;&#24448;&#24448;&#23548;&#33268;&#35821;&#38899;&#22833;&#30495;&#12290;&#23613;&#31649;&#30740;&#31350;&#20154;&#21592;&#22312;&#20351;&#29992;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#20449;&#21495;&#22788;&#29702;&#25216;&#26415;&#25913;&#21892;&#25509;&#25910;&#21040;&#30340;&#35821;&#38899;&#36136;&#37327;&#26041;&#38754;&#20570;&#20986;&#20102;&#21162;&#21147;&#65292;&#20294;&#22312;&#28041;&#21450;&#22810;&#20010;&#35821;&#38899;&#28304;&#12289;&#29615;&#22659;&#22122;&#22768;&#21644;&#20854;&#20182;&#24773;&#20917;&#30340;&#22330;&#26223;&#20013;&#65292;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#12290;&#26032;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#27861;&#30340;&#20986;&#29616;&#24341;&#20837;&#20102;&#20808;&#36827;&#30340;&#31574;&#30053;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15442v1 Announce Type: cross  Abstract: Automatic speech recognition (ASR) plays a pivotal role in our daily lives, offering utility not only for interacting with machines but also for facilitating communication for individuals with either partial or profound hearing impairments. The process involves receiving the speech signal in analogue form, followed by various signal processing algorithms to make it compatible with devices of limited capacity, such as cochlear implants (CIs). Unfortunately, these implants, equipped with a finite number of electrodes, often result in speech distortion during synthesis. Despite efforts by researchers to enhance received speech quality using various state-of-the-art signal processing techniques, challenges persist, especially in scenarios involving multiple sources of speech, environmental noise, and other circumstances. The advent of new artificial intelligence (AI) methods has ushered in cutting-edge strategies to address the limitations
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#22270;&#20687;&#36755;&#20837;&#23545;&#27169;&#22411;&#30340;&#28431;&#27934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#20687;&#38544;&#34255;&#24694;&#24847;&#24847;&#22270;&#12289;&#25104;&#21151;&#30772;&#35299;&#29616;&#26377;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.09792</link><description>&lt;p&gt;
&#22270;&#20687;&#26159;&#23545;&#40784;&#30340;&#36719;&#32907;&#65306;&#21033;&#29992;&#35270;&#35273;&#28431;&#27934;&#30772;&#35299;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Images are Achilles' Heel of Alignment: Exploiting Visual Vulnerabilities for Jailbreaking Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09792
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#22270;&#20687;&#36755;&#20837;&#23545;&#27169;&#22411;&#30340;&#28431;&#27934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#20687;&#38544;&#34255;&#24694;&#24847;&#24847;&#22270;&#12289;&#25104;&#21151;&#30772;&#35299;&#29616;&#26377;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#26080;&#23475;&#23545;&#40784;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#20195;&#34920;&#24615;&#30340;MLLMs&#30340;&#26080;&#23475;&#24615;&#33021;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#24182;&#25581;&#31034;&#20102;&#22270;&#20687;&#36755;&#20837;&#23545;MLLMs&#36896;&#25104;&#30340;&#23545;&#40784;&#28431;&#27934;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HADES&#30340;&#26032;&#22411;&#30772;&#35299;&#26041;&#27861;&#65292;&#21033;&#29992;&#31934;&#24515;&#21046;&#20316;&#30340;&#22270;&#20687;&#38544;&#34255;&#21644;&#25918;&#22823;&#25991;&#26412;&#36755;&#20837;&#20013;&#30340;&#24694;&#24847;&#24847;&#22270;&#30340;&#26377;&#23475;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;HADES&#21487;&#20197;&#26377;&#25928;&#22320;&#30772;&#35299;&#29616;&#26377;&#30340;MLLMs&#65292;&#20026;LLaVA-1.5&#23454;&#29616;&#20102;90.26&#65285;&#30340;&#24179;&#22343;&#25915;&#20987;&#25104;&#21151;&#29575;&#65288;ASR&#65289;&#65292;&#20026;Gemini Pro Vision&#23454;&#29616;&#20102;71.60&#65285;&#30340;ASR&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#23558;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09792v1 Announce Type: cross  Abstract: In this paper, we study the harmlessness alignment problem of multimodal large language models~(MLLMs). We conduct a systematic empirical analysis of the harmlessness performance of representative MLLMs and reveal that the image input poses the alignment vulnerability of MLLMs. Inspired by this, we propose a novel jailbreak method named HADES, which hides and amplifies the harmfulness of the malicious intent within the text input, using meticulously crafted images. Experimental results show that HADES can effectively jailbreak existing MLLMs, which achieves an average Attack Success Rate~(ASR) of 90.26% for LLaVA-1.5 and 71.60% for Gemini Pro Vision. Our code and data will be publicly released.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CAMSIC&#30340;&#31435;&#20307;&#22270;&#20687;&#21387;&#32553;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#38754;&#21521;&#20869;&#23481;&#24863;&#30693;&#30340;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#25216;&#26415;&#65292;&#20351;&#24471;&#26080;&#38656;&#39069;&#22806;Transformer&#35299;&#30721;&#22120;&#23601;&#33021;&#25429;&#25417;&#31354;&#38388;&#21644;&#35270;&#24046;&#20381;&#36182;&#20851;&#31995;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#29575;&#22833;&#30495;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.08505</link><description>&lt;p&gt;
&#38754;&#21521;&#20869;&#23481;&#24863;&#30693;&#30340;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#21464;&#21387;&#22120;&#29992;&#20110;&#31435;&#20307;&#22270;&#20687;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Content-aware Masked Image Modeling Transformer for Stereo Image Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08505
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CAMSIC&#30340;&#31435;&#20307;&#22270;&#20687;&#21387;&#32553;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#38754;&#21521;&#20869;&#23481;&#24863;&#30693;&#30340;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#25216;&#26415;&#65292;&#20351;&#24471;&#26080;&#38656;&#39069;&#22806;Transformer&#35299;&#30721;&#22120;&#23601;&#33021;&#25429;&#25417;&#31354;&#38388;&#21644;&#35270;&#24046;&#20381;&#36182;&#20851;&#31995;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#29575;&#22833;&#30495;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#22522;&#20110;&#23398;&#20064;&#30340;&#31435;&#20307;&#22270;&#20687;&#32534;&#35299;&#30721;&#22120;&#37319;&#29992;&#20102;&#22797;&#26434;&#30340;&#36716;&#25442;&#26041;&#27861;&#65292;&#20294;&#22312;&#32534;&#30721;&#28508;&#22312;&#34920;&#31034;&#26102;&#21364;&#37319;&#29992;&#20102;&#20174;&#21333;&#20010;&#22270;&#20687;&#32534;&#35299;&#30721;&#22120;&#23548;&#20986;&#30340;&#31616;&#21333;&#29109;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29109;&#27169;&#22411;&#38590;&#20197;&#26377;&#25928;&#25429;&#25417;&#31435;&#20307;&#22270;&#20687;&#22266;&#26377;&#30340;&#31354;&#38388;-&#35270;&#24046;&#29305;&#24449;&#65292;&#23548;&#33268;&#20122;&#26368;&#20248;&#30340;&#29575;&#22833;&#30495;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CAMSIC&#30340;&#31435;&#20307;&#22270;&#20687;&#21387;&#32553;&#26694;&#26550;&#12290; CAMSIC &#29420;&#31435;&#22320;&#23558;&#27599;&#20010;&#22270;&#20687;&#36716;&#25442;&#20026;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#37319;&#29992;&#24378;&#22823;&#30340;&#26080;&#35299;&#30721;&#22120;&#21464;&#21387;&#22120;&#29109;&#27169;&#22411;&#26469;&#25429;&#25417;&#31354;&#38388;&#21644;&#35270;&#24046;&#20381;&#36182;&#20851;&#31995;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#20869;&#23481;&#24863;&#30693;&#30340;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#38754;&#21521;&#20869;&#23481;&#24863;&#30693;&#30340;MIM&#20419;&#36827;&#20102;&#20808;&#39564;&#20449;&#24687;&#19982;&#20272;&#35745;&#20196;&#29260;&#20043;&#38388;&#30340;&#39640;&#25928;&#21452;&#21521;&#20132;&#20114;&#65292;&#33258;&#28982;&#22320;&#28040;&#38500;&#20102;&#39069;&#22806;&#30340;Transformer&#35299;&#30721;&#22120;&#30340;&#38656;&#27714;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#31435;&#20307;&#22270;&#20687;&#32534;&#35299;&#30721;&#22120;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#29575;&#22833;&#30495;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08505v1 Announce Type: cross  Abstract: Existing learning-based stereo image codec adopt sophisticated transformation with simple entropy models derived from single image codecs to encode latent representations. However, those entropy models struggle to effectively capture the spatial-disparity characteristics inherent in stereo images, which leads to suboptimal rate-distortion results. In this paper, we propose a stereo image compression framework, named CAMSIC. CAMSIC independently transforms each image to latent representation and employs a powerful decoder-free Transformer entropy model to capture both spatial and disparity dependencies, by introducing a novel content-aware masked image modeling (MIM) technique. Our content-aware MIM facilitates efficient bidirectional interaction between prior information and estimated tokens, which naturally obviates the need for an extra Transformer decoder. Experiments show that our stereo image codec achieves state-of-the-art rate-d
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#30340;&#27010;&#29575;&#31163;&#25955;&#28508;&#21464;&#37327;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30524;&#37096;&#30142;&#30149;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#27969;&#32593;&#32476;&#26469;&#23398;&#20064;&#30524;&#24213;&#22270;&#20687;&#20013;&#30524;&#37096;&#30142;&#30149;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.16865</link><description>&lt;p&gt;
&#36890;&#36807;&#23558;&#21487;&#23398;&#20064;&#30340;&#27010;&#29575;&#31163;&#25955;&#28508;&#21464;&#37327;&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#25552;&#39640;&#30524;&#37096;&#30142;&#30149;&#26816;&#27979;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improve Robustness of Eye Disease Detection by including Learnable Probabilistic Discrete Latent Variables into Machine Learning Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16865
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#30340;&#27010;&#29575;&#31163;&#25955;&#28508;&#21464;&#37327;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30524;&#37096;&#30142;&#30149;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#27969;&#32593;&#32476;&#26469;&#23398;&#20064;&#30524;&#24213;&#22270;&#20687;&#20013;&#30524;&#37096;&#30142;&#30149;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30524;&#37096;&#30142;&#30149;&#20174;&#31958;&#23615;&#30149;&#24615;&#35270;&#32593;&#33180;&#30149;&#21464;&#21040;&#38738;&#20809;&#30524;&#31561;&#65292;&#30001;&#20110;&#20854;&#39640;&#21457;&#30149;&#29575;&#21644;&#21487;&#33021;&#23548;&#33268;&#35270;&#21147;&#25439;&#23475;&#65292;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#20844;&#20849;&#21355;&#29983;&#25361;&#25112;&#12290;&#21450;&#26089;&#21644;&#20934;&#30830;&#30340;&#35786;&#26029;&#23545;&#20110;&#26377;&#25928;&#27835;&#30103;&#21644;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#20998;&#26512;&#21307;&#23398;&#22270;&#20687;&#65288;&#21253;&#25324;&#30524;&#37096;&#22270;&#20687;&#65289;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#65292;&#36825;&#23545;&#20020;&#24202;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;GFlowOut&#30340;&#26032;&#39062;&#24212;&#29992;&#65292;&#21033;&#29992;&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#30340;&#27010;&#29575;&#26694;&#26550;&#26469;&#23398;&#20064;&#20851;&#20110;&#36749;&#23398;&#25513;&#30721;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#29992;&#20110;&#20351;&#29992;&#30524;&#24213;&#22270;&#20687;&#23545;&#30524;&#37096;&#30142;&#30149;&#36827;&#34892;&#20998;&#31867;&#21644;&#20998;&#26512;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31283;&#20581;&#19988;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20197;ResNet18&#21644;ViT&#27169;&#22411;&#20026;&#20027;&#24178;&#30340;GFlowOut&#26469;&#35782;&#21035;&#21508;&#31181;&#30524;&#37096;&#29366;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16865v1 Announce Type: cross  Abstract: Ocular diseases, ranging from diabetic retinopathy to glaucoma, present a significant public health challenge due to their prevalence and potential for causing vision impairment. Early and accurate diagnosis is crucial for effective treatment and management.In recent years, deep learning models have emerged as powerful tools for analysing medical images, including ocular imaging . However, challenges persist in model interpretability and uncertainty estimation, which are critical for clinical decision-making. This study introduces a novel application of GFlowOut, leveraging the probabilistic framework of Generative Flow Networks (GFlowNets) to learn the posterior distribution over dropout masks, for the classification and analysis of ocular diseases using eye fundus images. We develop a robust and generalizable method that utilizes GFlowOut integrated with ResNet18 and ViT models as backbone in identifying various ocular conditions. Th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#30340;&#30340;&#21487;&#35299;&#37322;&#29305;&#24449;&#25216;&#26415;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#25552;&#21319;&#26426;&#65288;EBMs&#65289;&#23454;&#29616;&#20102;&#22312;&#37327;&#23376;&#28857;&#35843;&#35856;&#20013;&#36739;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13699</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#37327;&#23376;&#28857;&#22120;&#20214;&#27979;&#37327;&#20998;&#31867;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Explainable Classification Techniques for Quantum Dot Device Measurements
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13699
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#30340;&#30340;&#21487;&#35299;&#37322;&#29305;&#24449;&#25216;&#26415;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#25552;&#21319;&#26426;&#65288;EBMs&#65289;&#23454;&#29616;&#20102;&#22312;&#37327;&#23376;&#28857;&#35843;&#35856;&#20013;&#36739;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#29702;&#31185;&#23398;&#20013;&#65292;&#23545;&#22270;&#20687;&#25968;&#25454;&#30340;&#31283;&#20581;&#29305;&#24449;&#34920;&#31034;&#38656;&#27714;&#22686;&#21152;&#65306;&#22270;&#20687;&#37319;&#38598;&#65292;&#22312;&#24191;&#20041;&#19978;&#25351;&#20108;&#32500;&#25968;&#25454;&#65292;&#29616;&#22312;&#22312;&#35768;&#22810;&#39046;&#22495;&#24191;&#27867;&#24212;&#29992;&#65292;&#21253;&#25324;&#25105;&#20204;&#22312;&#27492;&#32771;&#34385;&#30340;&#37327;&#23376;&#20449;&#24687;&#31185;&#23398;&#12290;&#34429;&#28982;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#24191;&#27867;&#20351;&#29992;&#20256;&#32479;&#22270;&#20687;&#29305;&#24449;&#65292;&#20294;&#23427;&#20204;&#30340;&#20351;&#29992;&#27491;&#22312;&#36805;&#36895;&#34987;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#25152;&#21462;&#20195;&#65292;&#21518;&#32773;&#24448;&#24448;&#20197;&#29306;&#29298;&#21487;&#35299;&#37322;&#24615;&#20026;&#20195;&#20215;&#25442;&#21462;&#39640;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#31181;&#26435;&#34913;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#25552;&#21319;&#26426;&#65288;EBMs&#65289;&#23637;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#21331;&#36234;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#19981;&#20250;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#37327;&#23376;&#28857;&#35843;&#35856;&#30340;&#32972;&#26223;&#19979;&#65292;&#36825;&#31181;&#25216;&#26415;&#24102;&#26469;&#20102;&#23454;&#36136;&#24615;&#30340;&#30410;&#22788;&#65292;&#24403;&#21069;&#21457;&#23637;&#38454;&#27573;&#38656;&#35201;&#20154;&#31867;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13699v1 Announce Type: cross  Abstract: In the physical sciences, there is an increased need for robust feature representations of image data: image acquisition, in the generalized sense of two-dimensional data, is now widespread across a large number of fields, including quantum information science, which we consider here. While traditional image features are widely utilized in such cases, their use is rapidly being supplanted by Neural Network-based techniques that often sacrifice explainability in exchange for high accuracy. To ameliorate this trade-off, we propose a synthetic data-based technique that results in explainable features. We show, using Explainable Boosting Machines (EBMs), that this method offers superior explainability without sacrificing accuracy. Specifically, we show that there is a meaningful benefit to this technique in the context of quantum dot tuning, where human intervention is necessary at the current stage of development.
&lt;/p&gt;</description></item><item><title>SemPLeS&#26694;&#26550;&#21033;&#29992;&#35821;&#20041;&#25552;&#31034;&#23398;&#20064;&#35299;&#20915;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23398;&#20064;&#26377;&#25928;&#25552;&#31034;&#26469;&#22686;&#24378;&#20998;&#21106;&#21306;&#22495;&#19982;&#30446;&#26631;&#23545;&#35937;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2401.11791</link><description>&lt;p&gt;
SemPLeS: &#35821;&#20041;&#25552;&#31034;&#23398;&#20064;&#29992;&#20110;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
SemPLeS: Semantic Prompt Learning for Weakly-Supervised Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11791
&lt;/p&gt;
&lt;p&gt;
SemPLeS&#26694;&#26550;&#21033;&#29992;&#35821;&#20041;&#25552;&#31034;&#23398;&#20064;&#35299;&#20915;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23398;&#20064;&#26377;&#25928;&#25552;&#31034;&#26469;&#22686;&#24378;&#20998;&#21106;&#21306;&#22495;&#19982;&#30446;&#26631;&#23545;&#35937;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#65288;WSSS&#65289;&#26088;&#22312;&#21033;&#29992;&#20165;&#20855;&#26377;&#22270;&#20687;&#32423;&#30417;&#30563;&#30340;&#22270;&#20687;&#25968;&#25454;&#26469;&#35757;&#32451;&#20998;&#21106;&#27169;&#22411;&#12290;&#30001;&#20110;&#26080;&#27861;&#33719;&#24471;&#31934;&#30830;&#30340;&#20687;&#32032;&#32423;&#26631;&#27880;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20391;&#37325;&#20110;&#36890;&#36807;&#20248;&#21270;CAM&#26679;&#24335;&#30340;&#28909;&#22270;&#26469;&#29983;&#25104;&#29992;&#20110;&#35757;&#32451;&#20998;&#21106;&#27169;&#22411;&#30340;&#20266;&#26631;&#35760;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#30340;&#28909;&#22270;&#21487;&#33021;&#20165;&#25429;&#33719;&#23545;&#35937;&#31867;&#21035;&#30340;&#20855;&#26377;&#21306;&#20998;&#24615;&#30340;&#22270;&#20687;&#21306;&#22495;&#25110;&#30456;&#20851;&#30340;&#20849;&#21516;&#20986;&#29616;&#30340;&#32972;&#26223;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;WSSS&#30340;&#35821;&#20041;&#25552;&#31034;&#23398;&#20064;&#65288;SemPLeS&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23398;&#20064;&#26377;&#25928;&#22320;&#25552;&#31034;CLIP&#28508;&#31354;&#38388;&#20197;&#22686;&#24378;&#20998;&#21106;&#21306;&#22495;&#19982;&#30446;&#26631;&#23545;&#35937;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#23545;&#20934;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#25552;&#31034;&#23398;&#20064;&#21644;&#25552;&#31034;&#24341;&#23548;&#30340;&#35821;&#20041;&#32454;&#21270;&#65292;&#20197;&#23398;&#20064;&#36866;&#24403;&#25551;&#36848;&#21644;&#25233;&#21046;&#19982;&#27599;&#20010;&#30446;&#26631;&#23545;&#35937;&#31867;&#21035;&#30456;&#20851;&#30340;&#20849;&#21516;&#20986;&#29616;&#30340;&#32972;&#26223;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11791v2 Announce Type: replace-cross  Abstract: Weakly-Supervised Semantic Segmentation (WSSS) aims to train segmentation models using image data with only image-level supervision. Since precise pixel-level annotations are not accessible, existing methods typically focus on producing pseudo masks for training segmentation models by refining CAM-like heatmaps. However, the produced heatmaps may capture only the discriminative image regions of object categories or the associated co-occurring backgrounds. To address the issues, we propose a Semantic Prompt Learning for WSSS (SemPLeS) framework, which learns to effectively prompt the CLIP latent space to enhance the semantic alignment between the segmented regions and the target object categories. More specifically, we propose Contrastive Prompt Learning and Prompt-guided Semantic Refinement to learn the prompts that adequately describe and suppress the co-occurring backgrounds associated with each target object category. In thi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026; Quilt-1M &#30340;&#30284;&#30151;&#32452;&#32455;&#23398;&#22270;&#20687;&#21644;&#25991;&#23383;&#23545;&#30340;&#30334;&#19975;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992; YouTube &#19978;&#30340;&#19987;&#23478;&#21307;&#29983;&#25945;&#31243;&#35270;&#39057;&#20026;&#20027;&#35201;&#26469;&#28304;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#23558;&#20351;&#24471;&#30284;&#30151;&#32452;&#32455;&#23398;&#39046;&#22495;&#30340;&#34920;&#24449;&#23398;&#20064;&#21462;&#24471;&#31867;&#20284;&#20110;&#20854;&#20182;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.11207</link><description>&lt;p&gt;
Quilt-1M: &#30284;&#30151;&#32452;&#32455;&#23398;&#22270;&#20687;&#25991;&#23383;&#23545;&#30340;&#30334;&#19975;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Quilt-1M: One Million Image-Text Pairs for Histopathology. (arXiv:2306.11207v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026; Quilt-1M &#30340;&#30284;&#30151;&#32452;&#32455;&#23398;&#22270;&#20687;&#21644;&#25991;&#23383;&#23545;&#30340;&#30334;&#19975;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992; YouTube &#19978;&#30340;&#19987;&#23478;&#21307;&#29983;&#25945;&#31243;&#35270;&#39057;&#20026;&#20027;&#35201;&#26469;&#28304;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#23558;&#20351;&#24471;&#30284;&#30151;&#32452;&#32455;&#23398;&#39046;&#22495;&#30340;&#34920;&#24449;&#23398;&#20064;&#21462;&#24471;&#31867;&#20284;&#20110;&#20854;&#20182;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#24212;&#29992;&#30340;&#21152;&#36895;&#20351;&#24471;&#22312;&#32447;&#22270;&#20687;&#21644;&#25991;&#23383;&#25968;&#25454;&#22823;&#37327;&#28044;&#29616;&#65292;&#20294;&#21307;&#23398;&#39046;&#22495;&#65288;&#29305;&#21035;&#26159;&#30284;&#30151;&#32452;&#32455;&#23398;&#65289;&#31867;&#20284;&#30340;&#25968;&#25454;&#21364;&#24456;&#31232;&#23569;&#65292;&#36825;&#38459;&#30861;&#20102;&#21307;&#23398;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#21033;&#29992;YouTube&#19978;&#30340;&#19987;&#23478;&#21307;&#29983;&#25945;&#31243;&#35270;&#39057;&#65292;&#20174;&#20013;&#36873;&#25321;&#20102; 1,087 &#23567;&#26102;&#30340;&#21307;&#23398;&#32452;&#32455;&#23398;&#35270;&#39057;&#65292;&#20197;&#27492;&#33258;&#21160;&#31579;&#36873;&#20986;&#20849;&#21253;&#21547; 768,826 &#20010;&#30284;&#30151;&#32452;&#32455;&#23398;&#22270;&#20687;&#21450;&#20854;&#23545;&#24212;&#30340;&#25991;&#23383;&#23545;&#30340; Quilt &#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent accelerations in multi-modal applications have been made possible with the plethora of image and text data available online. However, the scarcity of analogous data in the medical field, specifically in histopathology, has halted comparable progress. To enable similar representation learning for histopathology, we turn to YouTube, an untapped resource of videos, offering $1,087$ hours of valuable educational histopathology videos from expert clinicians. From YouTube, we curate Quilt: a large-scale vision-language dataset consisting of $768,826$ image and text pairs. Quilt was automatically curated using a mixture of models, including large language models, handcrafted algorithms, human knowledge databases, and automatic speech recognition. In comparison, the most comprehensive datasets curated for histopathology amass only around $200$K samples. We combine Quilt with datasets from other sources, including Twitter, research papers, and the internet in general, to create an even l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#23454;&#29616;&#21516;&#24577;&#21152;&#23494;&#25216;&#26415;&#19979;&#38544;&#31169;&#20445;&#25252;&#30340;CNN&#35757;&#32451;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#36716;&#25442;&#24605;&#24819;&#21644;&#26356;&#24555;&#30340;&#26799;&#24230;&#21464;&#20307;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.03807</link><description>&lt;p&gt;
&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#30340;CNN&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving CNN Training with Transfer Learning. (arXiv:2304.03807v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#23454;&#29616;&#21516;&#24577;&#21152;&#23494;&#25216;&#26415;&#19979;&#38544;&#31169;&#20445;&#25252;&#30340;CNN&#35757;&#32451;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#36716;&#25442;&#24605;&#24819;&#21644;&#26356;&#24555;&#30340;&#26799;&#24230;&#21464;&#20307;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#30340;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#24050;&#32463;&#24471;&#21040;&#24456;&#22909;&#30340;&#30740;&#31350;&#65292;&#21516;&#26102;&#20445;&#25345;&#21516;&#24577;CNN&#35757;&#32451;&#20173;&#28982;&#26159;&#19968;&#39033;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#23454;&#29616;&#22522;&#20110;&#21516;&#24577;&#21152;&#23494;&#25216;&#26415;&#30340;&#38544;&#31169;&#20445;&#25252;CNN&#35757;&#32451;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#27425;&#25104;&#21151;&#31361;&#30772;&#36825;&#20010;&#38590;&#39064;&#65292;&#20197;&#21069;&#27809;&#26377;&#20219;&#20309;&#24037;&#20316;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#12290;&#37319;&#29992;&#20102;&#20960;&#31181;&#25216;&#26415;&#65306;&#65288;1&#65289;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#65292;&#21487;&#20197;&#23558;&#38544;&#31169;&#20445;&#25252;&#30340;CNN&#35757;&#32451;&#31616;&#21270;&#20026;&#21516;&#24577;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#29978;&#33267;&#26159;&#22810;&#31867;&#36923;&#36753;&#22238;&#24402;&#65288;MLR&#65289;&#35757;&#32451;&#65307;&#65288;2&#65289;&#36890;&#36807;&#26356;&#24555;&#30340;&#26799;&#24230;&#21464;&#20307;$\texttt{Quadratic Gradient}$&#65292;&#24212;&#29992;&#20110;MLR&#30340;&#22686;&#24378;&#26799;&#24230;&#26041;&#27861;&#65292;&#22312;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65307;&#65288;3&#65289;&#25105;&#20204;&#37319;&#29992;&#25968;&#23398;&#20013;&#30340;&#21464;&#25442;&#24605;&#24819;&#65292;&#23558;&#21152;&#23494;&#22495;&#20013;&#30340;&#36817;&#20284;Softmax&#20989;&#25968;&#36716;&#25442;&#25104;&#24050;&#32463;&#30740;&#31350;&#36807;&#30340;&#36924;&#36817;&#26041;&#27861;&#65292;&#20174;&#32780;&#24471;&#21040;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy-preserving nerual network inference has been well studied while homomorphic CNN training still remains an open challenging task. In this paper, we present a practical solution to implement privacy-preserving CNN training based on mere Homomorphic Encryption (HE) technique. To our best knowledge, this is the first attempt successfully to crack this nut and no work ever before has achieved this goal. Several techniques combine to make it done: (1) with transfer learning, privacy-preserving CNN training can be reduced to homomorphic neural network training, or even multiclass logistic regression (MLR) training; (2) via a faster gradient variant called $\texttt{Quadratic Gradient}$, an enhanced gradient method for MLR with a state-of-the-art performance in converge speed is applied in this work to achieve high performance; (3) we employ the thought of transformation in mathematics to transform approximating Softmax function in encryption domain to the well-studied approximation of 
&lt;/p&gt;</description></item></channel></rss>