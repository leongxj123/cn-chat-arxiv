<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#25193;&#25955;&#27169;&#22411;&#22312;&#30740;&#31350;&#25968;&#25454;&#30340;&#20998;&#23618;&#29983;&#25104;&#27169;&#22411;&#20013;&#23637;&#31034;&#20986;&#20102;&#22312;&#38408;&#20540;&#26102;&#38388;&#21457;&#29983;&#30456;&#21464;&#30340;&#29305;&#24615;&#65292;&#36825;&#24433;&#21709;&#20102;&#39640;&#32423;&#29305;&#24449;&#21644;&#20302;&#32423;&#29305;&#24449;&#30340;&#37325;&#24314;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.16991</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#30456;&#21464;&#25581;&#31034;&#20102;&#25968;&#25454;&#30340;&#20998;&#23618;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
A Phase Transition in Diffusion Models Reveals the Hierarchical Nature of Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16991
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#30740;&#31350;&#25968;&#25454;&#30340;&#20998;&#23618;&#29983;&#25104;&#27169;&#22411;&#20013;&#23637;&#31034;&#20986;&#20102;&#22312;&#38408;&#20540;&#26102;&#38388;&#21457;&#29983;&#30456;&#21464;&#30340;&#29305;&#24615;&#65292;&#36825;&#24433;&#21709;&#20102;&#39640;&#32423;&#29305;&#24449;&#21644;&#20302;&#32423;&#29305;&#24449;&#30340;&#37325;&#24314;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#30495;&#23454;&#25968;&#25454;&#30340;&#32467;&#26500;&#22312;&#25512;&#21160;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#33258;&#28982;&#25968;&#25454;&#65292;&#22914;&#22270;&#20687;&#65292;&#34987;&#35748;&#20026;&#26159;&#30001;&#20197;&#23618;&#27425;&#21644;&#32452;&#21512;&#26041;&#24335;&#32452;&#32455;&#30340;&#29305;&#24449;&#32452;&#25104;&#30340;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#25429;&#25417;&#21040;&#36825;&#20123;&#29305;&#24449;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#26174;&#31034;&#65292;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#26263;&#31034;&#20102;&#23427;&#20204;&#25429;&#25417;&#21040;&#36825;&#31181;&#28508;&#22312;&#32467;&#26500;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25968;&#25454;&#30340;&#20998;&#23618;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#36825;&#19968;&#29616;&#35937;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#26102;&#38388;$t$&#21518;&#20316;&#29992;&#30340;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#21463;&#21040;&#26576;&#20010;&#38408;&#20540;&#26102;&#38388;&#22788;&#30340;&#30456;&#21464;&#25511;&#21046;&#65292;&#27492;&#26102;&#37325;&#24314;&#39640;&#32423;&#29305;&#24449;&#65288;&#22914;&#22270;&#20687;&#30340;&#31867;&#21035;&#65289;&#30340;&#27010;&#29575;&#31361;&#28982;&#19979;&#38477;&#12290;&#30456;&#21453;&#65292;&#20302;&#32423;&#29305;&#24449;&#65288;&#22914;&#22270;&#20687;&#30340;&#20855;&#20307;&#32454;&#33410;&#65289;&#30340;&#37325;&#24314;&#22312;&#25972;&#20010;&#25193;&#25955;&#36807;&#31243;&#20013;&#24179;&#31283;&#28436;&#21464;&#12290;&#36825;&#19968;&#32467;&#26524;&#26263;&#31034;&#65292;&#22312;&#36229;&#20986;&#36716;&#21464;&#26102;&#38388;&#30340;&#26102;&#21051;&#65292;&#31867;&#21035;&#24050;&#21464;&#21270;&#65292;&#20294;&#26159;&#22522;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16991v1 Announce Type: cross  Abstract: Understanding the structure of real data is paramount in advancing modern deep-learning methodologies. Natural data such as images are believed to be composed of features organised in a hierarchical and combinatorial manner, which neural networks capture during learning. Recent advancements show that diffusion models can generate high-quality images, hinting at their ability to capture this underlying structure. We study this phenomenon in a hierarchical generative model of data. We find that the backward diffusion process acting after a time $t$ is governed by a phase transition at some threshold time, where the probability of reconstructing high-level features, like the class of an image, suddenly drops. Instead, the reconstruction of low-level features, such as specific details of an image, evolves smoothly across the whole diffusion process. This result implies that at times beyond the transition, the class has changed but the gene
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;&#30456;&#20114;&#28608;&#21169;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;me-GCN&#65289;&#65292;&#29992;&#20110;&#25163;&#23545;&#25163;&#21644;&#20154;&#23545;&#20154;&#20132;&#20114;&#35782;&#21035;&#12290;&#36890;&#36807;&#22534;&#21472;&#30456;&#20114;&#28608;&#21169;&#22270;&#21367;&#31215;&#23618;&#65288;me-GC&#65289;&#65292;&#35813;&#32593;&#32476;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#24314;&#27169;&#25104;&#23545;&#23454;&#20307;&#20043;&#38388;&#30340;&#20114;&#30456;&#32422;&#26463;&#65292;&#24182;&#25552;&#21462;&#21644;&#21512;&#24182;&#28145;&#24230;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.02431</link><description>&lt;p&gt;
&#23398;&#20064;&#30456;&#20114;&#28608;&#21169;&#20197;&#23454;&#29616;&#25163;&#23545;&#25163;&#21644;&#20154;&#23545;&#20154;&#20132;&#20114;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Learning Mutual Excitation for Hand-to-Hand and Human-to-Human Interaction Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;&#30456;&#20114;&#28608;&#21169;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;me-GCN&#65289;&#65292;&#29992;&#20110;&#25163;&#23545;&#25163;&#21644;&#20154;&#23545;&#20154;&#20132;&#20114;&#35782;&#21035;&#12290;&#36890;&#36807;&#22534;&#21472;&#30456;&#20114;&#28608;&#21169;&#22270;&#21367;&#31215;&#23618;&#65288;me-GC&#65289;&#65292;&#35813;&#32593;&#32476;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#24314;&#27169;&#25104;&#23545;&#23454;&#20307;&#20043;&#38388;&#30340;&#20114;&#30456;&#32422;&#26463;&#65292;&#24182;&#25552;&#21462;&#21644;&#21512;&#24182;&#28145;&#24230;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#20132;&#20114;&#21160;&#20316;&#65292;&#21253;&#25324;&#25163;&#23545;&#25163;&#20132;&#20114;&#21644;&#20154;&#23545;&#20154;&#20132;&#20114;&#65292;&#22312;&#35270;&#39057;&#20998;&#26512;&#21644;&#20154;&#26426;&#20132;&#20114;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#32771;&#34385;&#21040;&#22270;&#21367;&#31215;&#22312;&#24314;&#27169;&#39592;&#39612;&#25968;&#25454;&#30340;&#25299;&#25169;&#24863;&#30693;&#29305;&#24449;&#26041;&#38754;&#30340;&#25104;&#21151;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#36890;&#24120;&#23558;&#22270;&#21367;&#31215;&#24212;&#29992;&#20110;&#29420;&#31435;&#23454;&#20307;&#65292;&#24182;&#22312;&#20132;&#20114;&#21160;&#20316;&#35782;&#21035;&#26102;&#20351;&#29992;&#21518;&#26399;&#34701;&#21512;&#65292;&#36825;&#20960;&#20046;&#26080;&#27861;&#24314;&#27169;&#25104;&#23545;&#23454;&#20307;&#20043;&#38388;&#30340;&#20114;&#30456;&#35821;&#20041;&#20851;&#31995;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#22534;&#21472;&#30456;&#20114;&#28608;&#21169;&#22270;&#21367;&#31215;&#65288;me-GC&#65289;&#23618;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#30456;&#20114;&#28608;&#21169;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;me-GCN&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;me-GC&#20351;&#29992;&#30456;&#20114;&#25299;&#25169;&#28608;&#21169;&#27169;&#22359;&#39318;&#20808;&#20174;&#21333;&#20010;&#23454;&#20307;&#20013;&#25552;&#21462;&#37051;&#25509;&#30697;&#38453;&#65292;&#28982;&#21518;&#33258;&#36866;&#24212;&#22320;&#23545;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#32422;&#26463;&#36827;&#34892;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;me-GC&#36827;&#19968;&#27493;&#20351;&#29992;&#30456;&#20114;&#29305;&#24449;&#28608;&#21169;&#27169;&#22359;&#20174;&#25104;&#23545;&#23454;&#20307;&#20013;&#25552;&#21462;&#21644;&#21512;&#24182;&#28145;&#24230;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recognizing interactive actions, including hand-to-hand interaction and human-to-human interaction, has attracted increasing attention for various applications in the field of video analysis and human-robot interaction. Considering the success of graph convolution in modeling topology-aware features from skeleton data, recent methods commonly operate graph convolution on separate entities and use late fusion for interactive action recognition, which can barely model the mutual semantic relationships between pairwise entities. To this end, we propose a mutual excitation graph convolutional network (me-GCN) by stacking mutual excitation graph convolution (me-GC) layers. Specifically, me-GC uses a mutual topology excitation module to firstly extract adjacency matrices from individual entities and then adaptively model the mutual constraints between them. Moreover, me-GC extends the above idea and further uses a mutual feature excitation module to extract and merge deep features from pairw
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20811;&#26381;&#32852;&#37030;&#23398;&#20064;&#20013;&#36890;&#20449;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;FL&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#21516;&#26102;&#20943;&#23569;&#36890;&#20449;&#36127;&#25285;&#12290;</title><link>https://arxiv.org/abs/2210.01708</link><description>&lt;p&gt;
&#20811;&#26381;&#36890;&#20449;&#32422;&#26463;&#65292;&#23454;&#29616;&#32852;&#37030;&#23398;&#20064;&#20013;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Conquering the Communication Constraints to Enable Large Pre-Trained Models in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.01708
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20811;&#26381;&#32852;&#37030;&#23398;&#20064;&#20013;&#36890;&#20449;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;FL&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#21516;&#26102;&#20943;&#23569;&#36890;&#20449;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26088;&#22312;&#22312;&#26412;&#22320;&#35774;&#22791;&#19978;&#21327;&#21147;&#35757;&#32451;&#27169;&#22411;&#32780;&#19981;&#38656;&#35201;&#23545;&#21407;&#22987;&#25968;&#25454;&#36827;&#34892;&#20013;&#24515;&#21270;&#35775;&#38382;&#30340;&#26377;&#21069;&#26223;&#30340;&#33539;&#24335;&#12290;&#22312;&#20856;&#22411;&#30340;FL&#33539;&#24335;&#65288;&#20363;&#22914;FedAvg&#65289;&#20013;&#65292;&#27599;&#19968;&#36718;&#27169;&#22411;&#26435;&#37325;&#37117;&#20250;&#34987;&#21457;&#36865;&#21040;&#21442;&#19982;&#23458;&#25143;&#31471;&#24182;&#22238;&#20256;&#21040;&#26381;&#21153;&#22120;&#12290;&#26368;&#36817;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20248;&#21270;&#21644;&#25910;&#25947;&#25913;&#36827;&#26041;&#38754;&#23637;&#31034;&#20102;&#20351;&#29992;&#23567;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#26368;&#20808;&#36827;&#39044;&#35757;&#32451;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#24378;&#22823;&#65292;&#20294;&#20063;&#25317;&#26377;&#26356;&#22810;&#21442;&#25968;&#12290;&#22312;&#20256;&#32479;&#30340;FL&#20013;&#65292;&#20849;&#20139;&#24040;&#22823;&#30340;&#27169;&#22411;&#26435;&#37325;&#21487;&#20197;&#36805;&#36895;&#32473;&#31995;&#32479;&#24102;&#26469;&#24040;&#22823;&#30340;&#36890;&#20449;&#36127;&#25285;&#65292;&#23588;&#20854;&#26159;&#22914;&#26524;&#37319;&#29992;&#26356;&#21152;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#33021;&#21542;&#25214;&#21040;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;FL&#20013;&#21551;&#29992;&#36825;&#20123;&#24378;&#22823;&#19988;&#29616;&#25104;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#23454;&#29616;&#20986;&#33394;&#24615;&#33021;&#30340;&#21516;&#26102;&#20943;&#23569;&#36890;&#20449;&#36127;&#25285;&#65311;&#20026;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.01708v3 Announce Type: replace  Abstract: Federated learning (FL) has emerged as a promising paradigm for enabling the collaborative training of models without centralized access to the raw data on local devices. In the typical FL paradigm (e.g., FedAvg), model weights are sent to and from the server each round to participating clients. Recently, the use of small pre-trained models has been shown effective in federated learning optimization and improving convergence. However, recent state-of-the-art pre-trained models are getting more capable but also have more parameters. In conventional FL, sharing the enormous model weights can quickly put a massive communication burden on the system, especially if more capable models are employed. Can we find a solution to enable those strong and readily-available pre-trained models in FL to achieve excellent performance while simultaneously reducing the communication burden? To this end, we investigate the use of parameter-efficient fin
&lt;/p&gt;</description></item><item><title>ProCNS&#26159;&#19968;&#31181;&#29992;&#20110;&#24369;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#26032;&#26041;&#27861;&#65292;&#37319;&#29992;&#28176;&#36827;&#24335;&#21407;&#22411;&#26657;&#20934;&#21644;&#22122;&#22768;&#25233;&#21046;&#30340;&#21407;&#21017;&#26469;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14074</link><description>&lt;p&gt;
ProCNS: &#29992;&#20110;&#24369;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#28176;&#36827;&#24335;&#21407;&#22411;&#26657;&#20934;&#21644;&#22122;&#22768;&#25233;&#21046;
&lt;/p&gt;
&lt;p&gt;
ProCNS: Progressive Prototype Calibration and Noise Suppression for Weakly-Supervised Medical Image Segmentation. (arXiv:2401.14074v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14074
&lt;/p&gt;
&lt;p&gt;
ProCNS&#26159;&#19968;&#31181;&#29992;&#20110;&#24369;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#26032;&#26041;&#27861;&#65292;&#37319;&#29992;&#28176;&#36827;&#24335;&#21407;&#22411;&#26657;&#20934;&#21644;&#22122;&#22768;&#25233;&#21046;&#30340;&#21407;&#21017;&#26469;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#20998;&#21106;&#65288;WSS&#65289;&#20316;&#20026;&#32531;&#35299;&#27880;&#37322;&#25104;&#26412;&#21644;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#20914;&#31361;&#30340;&#35299;&#20915;&#26041;&#26696;&#32780;&#20986;&#29616;&#65292;&#37319;&#29992;&#31232;&#30095;&#30340;&#27880;&#37322;&#26684;&#24335;&#65288;&#20363;&#22914;&#28857;&#12289;&#28034;&#40486;&#12289;&#22359;&#31561;&#65289;&#12290;&#20856;&#22411;&#30340;&#26041;&#27861;&#35797;&#22270;&#21033;&#29992;&#35299;&#21078;&#21644;&#25299;&#25169;&#20808;&#39564;&#23558;&#31232;&#30095;&#27880;&#37322;&#30452;&#25509;&#25193;&#23637;&#20026;&#20266;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23545;&#21307;&#23398;&#22270;&#20687;&#20013;&#27169;&#31946;&#36793;&#32536;&#30340;&#20851;&#27880;&#19981;&#36275;&#21644;&#23545;&#31232;&#30095;&#30417;&#30563;&#30340;&#19981;&#20805;&#20998;&#25506;&#32034;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#20250;&#22312;&#22122;&#22768;&#21306;&#22495;&#29983;&#25104;&#38169;&#35823;&#19988;&#36807;&#20110;&#33258;&#20449;&#30340;&#20266;&#24314;&#35758;&#65292;&#23548;&#33268;&#27169;&#22411;&#35823;&#24046;&#32047;&#31215;&#21644;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;WSS&#26041;&#27861;&#65292;&#21517;&#20026;ProCNS&#65292;&#23427;&#21253;&#21547;&#20004;&#20010;&#21327;&#21516;&#27169;&#22359;&#65292;&#35774;&#35745;&#21407;&#21017;&#26159;&#28176;&#36827;&#24335;&#21407;&#22411;&#26657;&#20934;&#21644;&#22122;&#22768;&#25233;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#30340;&#21306;&#22495;&#31354;&#38388;&#30456;&#20284;&#24615;&#65288;PRSA&#65289;&#25439;&#22833;&#20989;&#25968;&#65292;&#26368;&#22823;&#21270;&#31354;&#38388;&#21644;&#35821;&#20041;&#20803;&#32032;&#20043;&#38388;&#30340;&#25104;&#23545;&#30456;&#20284;&#24230;&#65292;&#20026;&#25105;&#20204;&#24863;&#20852;&#36259;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;
&lt;/p&gt;
&lt;p&gt;
Weakly-supervised segmentation (WSS) has emerged as a solution to mitigate the conflict between annotation cost and model performance by adopting sparse annotation formats (e.g., point, scribble, block, etc.). Typical approaches attempt to exploit anatomy and topology priors to directly expand sparse annotations into pseudo-labels. However, due to a lack of attention to the ambiguous edges in medical images and insufficient exploration of sparse supervision, existing approaches tend to generate erroneous and overconfident pseudo proposals in noisy regions, leading to cumulative model error and performance degradation. In this work, we propose a novel WSS approach, named ProCNS, encompassing two synergistic modules devised with the principles of progressive prototype calibration and noise suppression. Specifically, we design a Prototype-based Regional Spatial Affinity (PRSA) loss to maximize the pair-wise affinities between spatial and semantic elements, providing our model of interest 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#32858;&#31867;&#30340;&#22270;&#20687;-&#25991;&#26412;&#22270;&#21305;&#37197;&#26469;&#24357;&#21512;&#39046;&#22495;&#24046;&#36317;&#65292;&#23398;&#20064;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#20197;&#23454;&#29616;&#22312;&#26410;&#35265;&#36807;&#39046;&#22495;&#19978;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02692</link><description>&lt;p&gt;
&#22522;&#20110;&#32858;&#31867;&#30340;&#22270;&#20687;-&#25991;&#26412;&#22270;&#21305;&#37197;&#26469;&#24357;&#21512;&#39046;&#22495;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Bridging the Domain Gap by Clustering-based Image-Text Graph Matching. (arXiv:2310.02692v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02692
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#32858;&#31867;&#30340;&#22270;&#20687;-&#25991;&#26412;&#22270;&#21305;&#37197;&#26469;&#24357;&#21512;&#39046;&#22495;&#24046;&#36317;&#65292;&#23398;&#20064;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#20197;&#23454;&#29616;&#22312;&#26410;&#35265;&#36807;&#39046;&#22495;&#19978;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#39046;&#22495;&#19981;&#21464;&#34920;&#31034;&#23545;&#20110;&#35757;&#32451;&#21487;&#20197;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30446;&#26631;&#20219;&#21153;&#39046;&#22495;&#30340;&#27169;&#22411;&#38750;&#24120;&#37325;&#35201;&#12290;&#25991;&#26412;&#25551;&#36848;&#26412;&#36523;&#21253;&#21547;&#27010;&#24565;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;&#36825;&#26679;&#30340;&#36741;&#21161;&#35821;&#20041;&#32447;&#32034;&#21487;&#20197;&#29992;&#20316;&#39046;&#22495;&#27010;&#25324;&#38382;&#39064;&#30340;&#26377;&#25928;&#26530;&#32445;&#23884;&#20837;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#27169;&#24577;&#22270;&#20687;&#21644;&#25991;&#26412;&#34701;&#21512;&#30340;&#22270;&#34920;&#31034;&#26469;&#33719;&#24471;&#22312;&#23616;&#37096;&#22270;&#20687;&#21644;&#25991;&#26412;&#25551;&#36848;&#31526;&#20043;&#38388;&#32771;&#34385;&#20869;&#22312;&#35821;&#20041;&#32467;&#26500;&#30340;&#39046;&#22495;&#19981;&#21464;&#26530;&#32445;&#23884;&#20837;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;(i)&#29992;&#22270;&#34920;&#31034;&#22270;&#20687;&#21644;&#25991;&#26412;&#25551;&#36848;&#65292;&#20197;&#21450;(ii)&#23558;&#22522;&#20110;&#22270;&#20687;&#33410;&#28857;&#29305;&#24449;&#30340;&#32858;&#31867;&#21644;&#21305;&#37197;&#24212;&#29992;&#21040;&#25991;&#26412;&#22270;&#20013;&#65292;&#26469;&#23398;&#20064;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;&#22823;&#35268;&#27169;&#20844;&#20849;&#25968;&#25454;&#38598;&#65288;&#22914;CUB-DG&#21644;DomainBed&#65289;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#19982;&#25110;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#22312;&#20986;&#29256;&#21518;&#20844;&#24320;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning domain-invariant representations is important to train a model that can generalize well to unseen target task domains. Text descriptions inherently contain semantic structures of concepts and such auxiliary semantic cues can be used as effective pivot embedding for domain generalization problems. Here, we use multimodal graph representations, fusing images and text, to get domain-invariant pivot embeddings by considering the inherent semantic structure between local images and text descriptors. Specifically, we aim to learn domain-invariant features by (i) representing the image and text descriptions with graphs, and by (ii) clustering and matching the graph-based image node features into textual graphs simultaneously. We experiment with large-scale public datasets, such as CUB-DG and DomainBed, and our model achieves matched or better state-of-the-art performance on these datasets. Our code will be publicly available upon publication.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25506;&#32034;&#25163;&#21160;&#35843;&#25972;&#21644;&#33258;&#21160;&#21270;&#31383;&#21475;&#35774;&#32622;&#20248;&#21270;&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#20020;&#24202;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#22270;&#20687;&#20013;&#26816;&#27979;&#24930;&#24615;&#38459;&#22622;&#24615;&#32954;&#30142;&#30149;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#28155;&#21152;&#33258;&#23450;&#20041;&#23618;&#23454;&#29616;&#30340;&#33258;&#21160;&#21270;&#31383;&#21475;&#35774;&#32622;&#20248;&#21270;&#21487;&#20197;&#25913;&#21892;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.07189</link><description>&lt;p&gt;
&#22312;&#20020;&#24202;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#25104;&#20687;&#20013;&#20248;&#21270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#24930;&#24615;&#38459;&#22622;&#24615;&#32954;&#30142;&#30149;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Optimizing Convolutional Neural Networks for Chronic Obstructive Pulmonary Disease Detection in Clinical Computed Tomography Imaging. (arXiv:2303.07189v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25506;&#32034;&#25163;&#21160;&#35843;&#25972;&#21644;&#33258;&#21160;&#21270;&#31383;&#21475;&#35774;&#32622;&#20248;&#21270;&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#20020;&#24202;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#22270;&#20687;&#20013;&#26816;&#27979;&#24930;&#24615;&#38459;&#22622;&#24615;&#32954;&#30142;&#30149;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#28155;&#21152;&#33258;&#23450;&#20041;&#23618;&#23454;&#29616;&#30340;&#33258;&#21160;&#21270;&#31383;&#21475;&#35774;&#32622;&#20248;&#21270;&#21487;&#20197;&#25913;&#21892;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#36890;&#36807;&#25506;&#32034;&#25163;&#21160;&#35843;&#25972;&#21644;&#33258;&#21160;&#21270;&#31383;&#21475;&#35774;&#32622;&#20248;&#21270;&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#32954;&#37096;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#22270;&#20687;&#20013;&#26816;&#27979;&#24930;&#24615;&#38459;&#22622;&#24615;&#32954;&#30142;&#30149;&#65288;COPD&#65289;&#30340;&#23384;&#22312;&#65292;&#26469;&#20248;&#21270;&#20108;&#36827;&#21046;COPD&#30340;&#26816;&#27979;&#12290;&#26041;&#27861;&#65306;&#22238;&#39038;&#24615;&#36873;&#25321;&#20102;78&#21517;&#21463;&#35797;&#32773;&#65288;43&#21517;COPD&#24739;&#32773;&#65307;35&#21517;&#20581;&#24247;&#23545;&#29031;&#32452;&#65289;&#30340;7,194&#20010;CT&#22270;&#20687;&#65288;3,597&#20010;COPD&#65307;3,597&#20010;&#20581;&#24247;&#23545;&#29031;&#32452;&#65289;&#65288;2018&#24180;10&#26376;&#33267;2019&#24180;12&#26376;&#65289;&#12290;&#23545;&#27599;&#20010;&#22270;&#20687;&#65292;&#23558;&#24378;&#24230;&#20540;&#25163;&#21160;&#35009;&#21098;&#21040;&#32954;&#27668;&#32959;&#31383;&#21475;&#35774;&#32622;&#21644;&#22522;&#20934;&#30340;&#8220;&#20840;&#33539;&#22260;&#8221;&#31383;&#21475;&#35774;&#32622;&#12290;&#31867;&#24179;&#34913;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#27979;&#35797;&#38598;&#21253;&#21547;&#20102;3,392&#12289;1,114&#21644;2,688&#20010;&#22270;&#20687;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;CNN&#26550;&#26500;&#26469;&#20248;&#21270;&#32593;&#32476;&#20027;&#24178;&#12290;&#27492;&#22806;&#65292;&#36824;&#36890;&#36807;&#21521;&#27169;&#22411;&#28155;&#21152;&#33258;&#23450;&#20041;&#23618;&#26469;&#23454;&#29616;&#33258;&#21160;&#21270;&#30340;&#31383;&#21475;&#35774;&#32622;&#20248;&#21270;&#12290;&#26681;&#25454;&#21463;&#35797;&#32773;&#24037;&#20316;&#29305;&#24449;&#26354;&#32447;&#65288;ROC&#65289;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#30340;&#22270;&#20687;&#27700;&#24179;&#65292;&#35745;&#31639;&#20986;P&#20540;&#26469;&#35780;&#20272;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: To optimize the binary detection of Chronic Obstructive Pulmonary Disease (COPD) based on emphysema presence in the lung with convolutional neural networks (CNN) by exploring manually adjusted versus automated window-setting optimization (WSO) on computed tomography (CT) images.  Methods: 7,194 CT images (3,597 with COPD; 3,597 healthy controls) from 78 subjects (43 with COPD; 35 healthy controls) were selected retrospectively (10.2018-12.2019) and preprocessed. For each image, intensity values were manually clipped to the emphysema window setting and a baseline 'full-range' window setting. Class-balanced train, validation, and test sets contained 3,392, 1,114, and 2,688 images. The network backbone was optimized by comparing various CNN architectures. Furthermore, automated WSO was implemented by adding a customized layer to the model. The image-level area under the Receiver Operating Characteristics curve (AUC) [lower, upper limit 95% confidence] and P-values calculated from
&lt;/p&gt;</description></item></channel></rss>