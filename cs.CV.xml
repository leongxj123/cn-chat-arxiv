<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#20998;&#26041;&#27861;&#26469;&#32479;&#19968;&#20998;&#26512;&#29983;&#25104;&#22120;&#30340;&#20248;&#21270;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;f-&#25955;&#24230;&#26368;&#23567;&#21270;&#21644;IPM GAN&#20013;&#29983;&#25104;&#22120;&#30340;&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#24179;&#28369;&#20998;&#25968;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2306.01654</link><description>&lt;p&gt;
GANs&#35299;&#20915;&#20998;&#25968;&#20105;&#35758;&#38382;&#39064;&#65281;
&lt;/p&gt;
&lt;p&gt;
GANs Settle Scores!. (arXiv:2306.01654v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01654
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#20998;&#26041;&#27861;&#26469;&#32479;&#19968;&#20998;&#26512;&#29983;&#25104;&#22120;&#30340;&#20248;&#21270;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;f-&#25955;&#24230;&#26368;&#23567;&#21270;&#21644;IPM GAN&#20013;&#29983;&#25104;&#22120;&#30340;&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#24179;&#28369;&#20998;&#25968;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30001;&#19968;&#20010;&#29983;&#25104;&#22120;&#21644;&#19968;&#20010;&#21028;&#21035;&#22120;&#32452;&#25104;&#65292;&#29983;&#25104;&#22120;&#34987;&#35757;&#32451;&#20197;&#23398;&#20064;&#26399;&#26395;&#25968;&#25454;&#30340;&#22522;&#30784;&#20998;&#24067;&#65292;&#32780;&#21028;&#21035;&#22120;&#21017;&#34987;&#35757;&#32451;&#20197;&#21306;&#20998;&#30495;&#23454;&#26679;&#26412;&#21644;&#29983;&#25104;&#22120;&#36755;&#20986;&#30340;&#26679;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#20998;&#26041;&#27861;&#26469;&#20998;&#26512;&#29983;&#25104;&#22120;&#20248;&#21270;&#12290;&#22312;f-&#25955;&#24230;&#26368;&#23567;&#21270; GAN &#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#26368;&#20248;&#29983;&#25104;&#22120;&#26159;&#36890;&#36807;&#23558;&#20854;&#36755;&#20986;&#20998;&#24067;&#30340;&#24471;&#20998;&#19982;&#25968;&#25454;&#20998;&#24067;&#30340;&#24471;&#20998;&#36827;&#34892;&#21305;&#37197;&#24471;&#21040;&#30340;&#12290;&#22312;IPM GAN&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#36825;&#20010;&#26368;&#20248;&#29983;&#25104;&#22120;&#21305;&#37197;&#24471;&#20998;&#22411;&#20989;&#25968;&#65292;&#21253;&#25324;&#19982;&#25152;&#36873;IPM&#32422;&#26463;&#31354;&#38388;&#30456;&#20851;&#30340;&#26680;&#27969;&#22330;&#12290;&#27492;&#22806;&#65292;IPM-GAN&#20248;&#21270;&#21487;&#20197;&#30475;&#20316;&#26159;&#24179;&#28369;&#20998;&#25968;&#21305;&#37197;&#20013;&#30340;&#19968;&#31181;&#65292;&#20854;&#20013;&#25968;&#25454;&#21644;&#29983;&#25104;&#22120;&#20998;&#24067;&#30340;&#24471;&#20998;&#19982;&#22312;&#26680;&#20989;&#25968;&#19978;&#36827;&#34892;&#21367;&#31215;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative adversarial networks (GANs) comprise a generator, trained to learn the underlying distribution of the desired data, and a discriminator, trained to distinguish real samples from those output by the generator. A majority of GAN literature focuses on understanding the optimality of the discriminator through integral probability metric (IPM) or divergence based analysis. In this paper, we propose a unified approach to analyzing the generator optimization through variational approach. In $f$-divergence-minimizing GANs, we show that the optimal generator is the one that matches the score of its output distribution with that of the data distribution, while in IPM GANs, we show that this optimal generator matches score-like functions, involving the flow-field of the kernel associated with a chosen IPM constraint space. Further, the IPM-GAN optimization can be seen as one of smoothed score-matching, where the scores of the data and the generator distributions are convolved with the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#22810;&#23545;&#35937;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#31181;&#19978;&#19979;&#25991;&#20998;&#38548;&#30340;&#27133;&#32467;&#26500;&#26469;&#23558;&#35270;&#35273;&#22330;&#20998;&#21106;&#20026;&#29420;&#31435;&#36816;&#21160;&#21306;&#22495;&#65292;&#24182;&#29992;&#23545;&#25239;&#24615;&#26631;&#20934;&#26469;&#20445;&#35777;&#35299;&#30721;&#22120;&#26080;&#27861;&#37325;&#26500;&#25972;&#20010;&#20809;&#27969;&#12290;</title><link>http://arxiv.org/abs/2304.01430</link><description>&lt;p&gt;
&#20998;&#31163;&#30340;&#20851;&#27880;&#21147;&#65306;&#22522;&#20110;&#19978;&#19979;&#25991;&#20998;&#31163;&#27133;&#30340;&#26080;&#30417;&#30563;&#22810;&#23545;&#35937;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Divided Attention: Unsupervised Multi-Object Discovery with Contextually Separated Slots. (arXiv:2304.01430v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01430
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#22810;&#23545;&#35937;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#31181;&#19978;&#19979;&#25991;&#20998;&#38548;&#30340;&#27133;&#32467;&#26500;&#26469;&#23558;&#35270;&#35273;&#22330;&#20998;&#21106;&#20026;&#29420;&#31435;&#36816;&#21160;&#21306;&#22495;&#65292;&#24182;&#29992;&#23545;&#25239;&#24615;&#26631;&#20934;&#26469;&#20445;&#35777;&#35299;&#30721;&#22120;&#26080;&#27861;&#37325;&#26500;&#25972;&#20010;&#20809;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35270;&#35273;&#22330;&#20998;&#21106;&#20026;&#29420;&#31435;&#36816;&#21160;&#21306;&#22495;&#30340;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#22522;&#30784;&#30495;&#20540;&#25110;&#30417;&#30563;&#12290;&#23427;&#30001;&#22522;&#20110;&#27133;&#20851;&#27880;&#30340;&#23545;&#25239;&#26465;&#20214;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#32452;&#25104;&#65292;&#20462;&#25913;&#20026;&#20351;&#29992;&#22270;&#20687;&#20316;&#20026;&#19978;&#19979;&#25991;&#26469;&#35299;&#30721;&#20809;&#27969;&#65292;&#32780;&#19981;&#26159;&#23581;&#35797;&#37325;&#26500;&#22270;&#20687;&#26412;&#36523;&#12290;&#22312;&#32467;&#26524;&#30340;&#22810;&#27169;&#24335;&#34920;&#31034;&#20013;&#65292;&#19968;&#31181;&#27169;&#24335;&#65288;&#27969;&#65289;&#23558;&#39304;&#36865;&#32473;&#32534;&#30721;&#22120;&#20197;&#20135;&#29983;&#21333;&#29420;&#30340;&#28508;&#22312;&#20195;&#30721;&#65288;&#27133;&#65289;&#65292;&#32780;&#21478;&#19968;&#31181;&#27169;&#24335;&#65288;&#22270;&#20687;&#65289;&#23558;&#20915;&#23450;&#35299;&#30721;&#22120;&#20174;&#27133;&#29983;&#25104;&#31532;&#19968;&#20010;&#27169;&#24335;&#65288;&#27969;&#65289;&#12290;&#30001;&#20110;&#24815;&#24120;&#30340;&#33258;&#32534;&#30721;&#22522;&#20110;&#26368;&#23567;&#21270;&#37325;&#26500;&#35823;&#24046;&#65292;&#24182;&#19981;&#33021;&#38450;&#27490;&#25972;&#20010;&#27969;&#34987;&#32534;&#30721;&#21040;&#19968;&#20010;&#27133;&#20013;&#65292;&#22240;&#27492;&#25105;&#20204;&#23558;&#25439;&#22833;&#20462;&#25913;&#20026;&#22522;&#20110;&#19978;&#19979;&#25991;&#20449;&#24687;&#20998;&#31163;&#30340;&#23545;&#25239;&#24615;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a method to segment the visual field into independently moving regions, trained with no ground truth or supervision. It consists of an adversarial conditional encoder-decoder architecture based on Slot Attention, modified to use the image as context to decode optical flow without attempting to reconstruct the image itself. In the resulting multi-modal representation, one modality (flow) feeds the encoder to produce separate latent codes (slots), whereas the other modality (image) conditions the decoder to generate the first (flow) from the slots. This design frees the representation from having to encode complex nuisance variability in the image due to, for instance, illumination and reflectance properties of the scene. Since customary autoencoding based on minimizing the reconstruction error does not preclude the entire flow from being encoded into a single slot, we modify the loss to an adversarial criterion based on Contextual Information Separation. The resulting min-m
&lt;/p&gt;</description></item></channel></rss>