<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26088;&#22312;&#33258;&#21160;&#21270;&#25968;&#25454;&#22686;&#24378;&#36807;&#31243;&#65292;&#20026;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#27867;&#21270;&#24615;&#33021;&#25552;&#20379;&#20102;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.08352</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21450;&#19982;&#20256;&#32479;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#24615;&#33021;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Data augmentation with automated machine learning: approaches and performance comparison with classical data augmentation methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08352
&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26088;&#22312;&#33258;&#21160;&#21270;&#25968;&#25454;&#22686;&#24378;&#36807;&#31243;&#65292;&#20026;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#27867;&#21270;&#24615;&#33021;&#25552;&#20379;&#20102;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#34987;&#35748;&#20026;&#26159;&#24120;&#29992;&#20110;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#27867;&#21270;&#24615;&#33021;&#30340;&#26368;&#37325;&#35201;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#12290;&#23427;&#20027;&#35201;&#28041;&#21450;&#24212;&#29992;&#36866;&#24403;&#30340;&#25968;&#25454;&#36716;&#25442;&#25805;&#20316;&#65292;&#20197;&#21019;&#24314;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#26032;&#25968;&#25454;&#26679;&#26412;&#12290;&#23613;&#31649;&#20854;&#26377;&#25928;&#24615;&#65292;&#36825;&#19968;&#36807;&#31243;&#36890;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#25163;&#21160;&#21019;&#24314;&#21644;&#27979;&#35797;&#19981;&#21516;&#20505;&#36873;&#22686;&#24378;&#21450;&#20854;&#36229;&#21442;&#25968;&#38656;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#12290;&#33258;&#21160;&#21270;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26088;&#22312;&#33258;&#21160;&#21270;&#36825;&#19968;&#36807;&#31243;&#12290;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#21407;&#21017;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#20110;AutoML&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#20840;&#38754;&#35843;&#26597;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20351;&#29992;AutoML&#23454;&#29616;&#25968;&#25454;&#22686;&#24378;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#25968;&#25454;&#25805;&#20316;&#12289;&#25968;&#25454;&#38598;&#25104;&#21644;&#25968;&#25454;&#21512;&#25104;&#25216;&#26415;&#12290;&#25105;&#20204;&#35814;&#32454;&#35752;&#35770;&#20102;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08352v1 Announce Type: cross  Abstract: Data augmentation is arguably the most important regularization technique commonly used to improve generalization performance of machine learning models. It primarily involves the application of appropriate data transformation operations to create new data samples with desired properties. Despite its effectiveness, the process is often challenging because of the time-consuming trial and error procedures for creating and testing different candidate augmentations and their hyperparameters manually. Automated data augmentation methods aim to automate the process. State-of-the-art approaches typically rely on automated machine learning (AutoML) principles. This work presents a comprehensive survey of AutoML-based data augmentation techniques. We discuss various approaches for accomplishing data augmentation with AutoML, including data manipulation, data integration and data synthesis techniques. We present extensive discussion of technique
&lt;/p&gt;</description></item><item><title>Polyp-DDPM&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#21106;&#25513;&#30721;&#29983;&#25104;&#36924;&#30495;&#30340;&#32963;&#32928;&#36947;&#24687;&#32905;&#22270;&#20687;&#65292;&#25552;&#21319;&#20102;&#20998;&#21106;&#25928;&#26524;&#65292;&#24182;&#22312;&#22270;&#20687;&#36136;&#37327;&#21644;&#20998;&#21106;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#20026;&#35757;&#32451;&#25552;&#20379;&#20102;&#39640;&#36136;&#37327;&#12289;&#22810;&#26679;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20351;&#24471;&#20998;&#21106;&#27169;&#22411;&#36798;&#21040;&#19982;&#30495;&#23454;&#22270;&#20687;&#30456;&#27604;&#21487;&#27604;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.04031</link><description>&lt;p&gt;
Polyp-DDPM: &#22522;&#20110;&#25193;&#25955;&#30340;&#35821;&#20041;&#24687;&#32905;&#21512;&#25104;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#20998;&#21106;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Polyp-DDPM: Diffusion-Based Semantic Polyp Synthesis for Enhanced Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04031
&lt;/p&gt;
&lt;p&gt;
Polyp-DDPM&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#21106;&#25513;&#30721;&#29983;&#25104;&#36924;&#30495;&#30340;&#32963;&#32928;&#36947;&#24687;&#32905;&#22270;&#20687;&#65292;&#25552;&#21319;&#20102;&#20998;&#21106;&#25928;&#26524;&#65292;&#24182;&#22312;&#22270;&#20687;&#36136;&#37327;&#21644;&#20998;&#21106;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#20026;&#35757;&#32451;&#25552;&#20379;&#20102;&#39640;&#36136;&#37327;&#12289;&#22810;&#26679;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20351;&#24471;&#20998;&#21106;&#27169;&#22411;&#36798;&#21040;&#19982;&#30495;&#23454;&#22270;&#20687;&#30456;&#27604;&#21487;&#27604;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Polyp-DDPM&#65292;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26465;&#20214;&#25513;&#30721;&#19978;&#29983;&#25104;&#36924;&#30495;&#30340;&#24687;&#32905;&#22270;&#20687;&#65292;&#26088;&#22312;&#22686;&#24378;&#32963;&#32928;&#36947;&#24687;&#32905;&#30340;&#20998;&#21106;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38480;&#21046;&#12289;&#39640;&#26114;&#30340;&#27880;&#37322;&#25104;&#26412;&#21644;&#38544;&#31169;&#38382;&#39064;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#23558;&#25193;&#25955;&#27169;&#22411;&#26465;&#20214;&#21270;&#20110;&#20998;&#21106;&#25513;&#30721;&#65288;&#34920;&#31034;&#24322;&#24120;&#21306;&#22495;&#30340;&#20108;&#36827;&#21046;&#25513;&#30721;&#65289;&#65292;Polyp-DDPM&#22312;&#22270;&#20687;&#36136;&#37327;&#21644;&#20998;&#21106;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65288;Frechet Inception Distance (FID) &#35780;&#20998;&#20026;78.47&#65292;&#32780;&#39640;&#20110;83.79&#30340;&#35780;&#20998;&#65307;Intersection over Union (IoU) &#20026;0.7156&#65292;&#32780;&#22522;&#20934;&#27169;&#22411;&#21512;&#25104;&#22270;&#20687;&#20026;0.6694&#20197;&#19979;&#65292;&#30495;&#23454;&#25968;&#25454;&#20026;0.7067&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#20102;&#39640;&#36136;&#37327;&#12289;&#22810;&#26679;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#24687;&#32905;&#20998;&#21106;&#27169;&#22411;&#19982;&#30495;&#23454;&#22270;&#20687;&#30340;&#21487;&#27604;&#24615;&#65292;&#24182;&#25552;&#20379;&#26356;&#22823;&#30340;&#25968;&#25454;&#22686;&#24378;&#33021;&#21147;&#20197;&#25913;&#21892;&#20998;&#21106;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study introduces Polyp-DDPM, a diffusion-based method for generating realistic images of polyps conditioned on masks, aimed at enhancing the segmentation of gastrointestinal (GI) tract polyps. Our approach addresses the challenges of data limitations, high annotation costs, and privacy concerns associated with medical images. By conditioning the diffusion model on segmentation masks-binary masks that represent abnormal areas-Polyp-DDPM outperforms state-of-the-art methods in terms of image quality (achieving a Frechet Inception Distance (FID) score of 78.47, compared to scores above 83.79) and segmentation performance (achieving an Intersection over Union (IoU) of 0.7156, versus less than 0.6694 for synthetic images from baseline models and 0.7067 for real data). Our method generates a high-quality, diverse synthetic dataset for training, thereby enhancing polyp segmentation models to be comparable with real images and offering greater data augmentation capabilities to improve seg
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#21453;&#28436;&#36807;&#31243;&#65288;ZIP&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22270;&#20687;&#23646;&#24615;&#32534;&#36753;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#29983;&#25104;&#30340;&#35270;&#35273;&#21442;&#32771;&#21644;&#25991;&#26412;&#24341;&#23548;&#27880;&#20837;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#28508;&#31354;&#38388;&#65292;&#21487;&#20197;&#22312;&#25991;&#26412;&#25552;&#31034;&#30340;&#30452;&#35266;&#25511;&#21046;&#19979;&#20135;&#29983;&#22810;&#26679;&#30340;&#20869;&#23481;&#21644;&#23646;&#24615;&#65292;&#24182;&#23637;&#29616;&#20986;&#23545;&#19981;&#21516;&#23646;&#24615;&#25805;&#20316;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.15854</link><description>&lt;p&gt;
&#22270;&#20687;&#23646;&#24615;&#32534;&#36753;&#30340;&#38646;&#26679;&#26412;&#21453;&#28436;&#36807;&#31243;&#19982;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Inversion Process for Image Attribute Editing with Diffusion Models. (arXiv:2308.15854v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15854
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#21453;&#28436;&#36807;&#31243;&#65288;ZIP&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22270;&#20687;&#23646;&#24615;&#32534;&#36753;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#29983;&#25104;&#30340;&#35270;&#35273;&#21442;&#32771;&#21644;&#25991;&#26412;&#24341;&#23548;&#27880;&#20837;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#28508;&#31354;&#38388;&#65292;&#21487;&#20197;&#22312;&#25991;&#26412;&#25552;&#31034;&#30340;&#30452;&#35266;&#25511;&#21046;&#19979;&#20135;&#29983;&#22810;&#26679;&#30340;&#20869;&#23481;&#21644;&#23646;&#24615;&#65292;&#24182;&#23637;&#29616;&#20986;&#23545;&#19981;&#21516;&#23646;&#24615;&#25805;&#20316;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38477;&#22122;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#32534;&#36753;&#20013;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20542;&#21521;&#20110;&#20351;&#29992;&#22270;&#20687;&#24341;&#23548;&#26041;&#27861;&#65292;&#25552;&#20379;&#35270;&#35273;&#21442;&#32771;&#20294;&#32570;&#20047;&#35821;&#20041;&#36830;&#36143;&#24615;&#30340;&#25511;&#21046;&#65292;&#25110;&#32773;&#20351;&#29992;&#25991;&#26412;&#24341;&#23548;&#26041;&#27861;&#65292;&#30830;&#20445;&#23545;&#25991;&#26412;&#24341;&#23548;&#30340;&#24544;&#23454;&#65292;&#20294;&#32570;&#20047;&#35270;&#35273;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38646;&#26679;&#26412;&#21453;&#28436;&#36807;&#31243;&#65288;ZIP&#65289;&#26694;&#26550;&#65292;&#23427;&#23558;&#29983;&#25104;&#30340;&#35270;&#35273;&#21442;&#32771;&#21644;&#25991;&#26412;&#24341;&#23548;&#30340;&#34701;&#21512;&#27880;&#20837;&#21040;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#28508;&#31354;&#38388;&#20013;&#12290;&#20165;&#20351;&#29992;&#19968;&#20010;&#24494;&#23567;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#20986;&#30340;ZIP&#22312;&#25991;&#26412;&#25552;&#31034;&#30340;&#30452;&#35266;&#25511;&#21046;&#19979;&#20135;&#29983;&#22810;&#26679;&#30340;&#20869;&#23481;&#21644;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;ZIP&#22312;&#30495;&#23454;&#22270;&#20687;&#19978;&#23637;&#31034;&#20102;&#23545;&#22495;&#20869;&#21644;&#22495;&#22806;&#23646;&#24615;&#25805;&#20316;&#30340;&#26174;&#33879;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23454;&#39564;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;ZIP&#20135;&#29983;&#20102;&#19982;&#20043;&#30456;&#24403;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#36924;&#30495;&#30340;&#32534;&#36753;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion models have shown outstanding performance in image editing. Existing works tend to use either image-guided methods, which provide a visual reference but lack control over semantic coherence, or text-guided methods, which ensure faithfulness to text guidance but lack visual quality. To address the problem, we propose the Zero-shot Inversion Process (ZIP), a framework that injects a fusion of generated visual reference and text guidance into the semantic latent space of a \textit{frozen} pre-trained diffusion model. Only using a tiny neural network, the proposed ZIP produces diverse content and attributes under the intuitive control of the text prompt. Moreover, ZIP shows remarkable robustness for both in-domain and out-of-domain attribute manipulation on real images. We perform detailed experiments on various benchmark datasets. Compared to state-of-the-art methods, ZIP produces images of equivalent quality while providing a realistic editing effect.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25193;&#23637;&#20102;&#27979;&#35797;&#26102;&#22521;&#35757;&#65288;TTT&#65289;&#21040;&#35270;&#39057;&#27969;&#30340;&#35774;&#32622;&#20013;&#65292;&#25552;&#20986;&#20102;&#22312;&#32447;TTT&#26041;&#27861;&#65292;&#30456;&#23545;&#20110;&#22266;&#23450;&#27169;&#22411;&#22522;&#32447;&#21644;&#31163;&#32447;TTT&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#37117;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#65292;&#21253;&#25324;&#23454;&#20363;&#21644;&#20840;&#26223;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2307.05014</link><description>&lt;p&gt;
&#35270;&#39057;&#27969;&#19978;&#30340;&#27979;&#35797;&#26102;&#22521;&#35757;
&lt;/p&gt;
&lt;p&gt;
Test-Time Training on Video Streams. (arXiv:2307.05014v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05014
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25193;&#23637;&#20102;&#27979;&#35797;&#26102;&#22521;&#35757;&#65288;TTT&#65289;&#21040;&#35270;&#39057;&#27969;&#30340;&#35774;&#32622;&#20013;&#65292;&#25552;&#20986;&#20102;&#22312;&#32447;TTT&#26041;&#27861;&#65292;&#30456;&#23545;&#20110;&#22266;&#23450;&#27169;&#22411;&#22522;&#32447;&#21644;&#31163;&#32447;TTT&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#37117;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#65292;&#21253;&#25324;&#23454;&#20363;&#21644;&#20840;&#26223;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#23558;&#27979;&#35797;&#26102;&#22521;&#35757;&#65288;TTT&#65289;&#30830;&#23450;&#20026;&#19968;&#31181;&#22312;&#27979;&#35797;&#26102;&#36827;&#19968;&#27493;&#25913;&#36827;&#35757;&#32451;&#27169;&#22411;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#22312;&#23545;&#27599;&#20010;&#27979;&#35797;&#23454;&#20363;&#36827;&#34892;&#39044;&#27979;&#20043;&#21069;&#65292;&#27169;&#22411;&#20250;&#20351;&#29992;&#33258;&#30417;&#30563;&#20219;&#21153;&#65288;&#20363;&#22914;&#20351;&#29992;&#25513;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#22270;&#20687;&#37325;&#24314;&#65289;&#22312;&#21516;&#19968;&#23454;&#20363;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#23558;TTT&#25193;&#23637;&#21040;&#27969;&#24335;&#35774;&#32622;&#20013;&#65292;&#20854;&#20013;&#22810;&#20010;&#27979;&#35797;&#23454;&#20363;&#65288;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#20026;&#35270;&#39057;&#24103;&#65289;&#25353;&#26102;&#38388;&#39034;&#24207;&#21040;&#36798;&#12290;&#25105;&#20204;&#30340;&#25193;&#23637;&#26159;&#22312;&#32447;TTT&#65306;&#24403;&#21069;&#27169;&#22411;&#20174;&#19978;&#20010;&#27169;&#22411;&#21021;&#22987;&#21270;&#65292;&#28982;&#21518;&#22312;&#24403;&#21069;&#24103;&#21644;&#21069;&#20960;&#20010;&#24103;&#30340;&#23567;&#31383;&#21475;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#32447;TTT&#22312;&#22235;&#20010;&#20219;&#21153;&#19978;&#26126;&#26174;&#20248;&#20110;&#22266;&#23450;&#27169;&#22411;&#22522;&#32447;&#65292;&#22312;&#19977;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#30456;&#23545;&#25913;&#36827;&#20998;&#21035;&#20026;45%&#21644;66%&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#32447;TTT&#20063;&#20248;&#20110;&#20854;&#31163;&#32447;&#29256;&#26412;&#65292;&#21518;&#32773;&#35775;&#38382;&#26356;&#22810;&#20449;&#24687;&#65292;&#21487;&#20197;&#35757;&#32451;&#25152;&#26377;&#24103;&#32780;&#19981;&#32771;&#34385;&#26102;&#38388;&#39034;&#24207;&#12290;&#36825;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior work has established test-time training (TTT) as a general framework to further improve a trained model at test time. Before making a prediction on each test instance, the model is trained on the same instance using a self-supervised task, such as image reconstruction with masked autoencoders. We extend TTT to the streaming setting, where multiple test instances - video frames in our case - arrive in temporal order. Our extension is online TTT: The current model is initialized from the previous model, then trained on the current frame and a small window of frames immediately before. Online TTT significantly outperforms the fixed-model baseline for four tasks, on three real-world datasets. The relative improvement is 45% and 66% for instance and panoptic segmentation. Surprisingly, online TTT also outperforms its offline variant that accesses more information, training on all frames from the entire test video regardless of temporal order. This differs from previous findings using 
&lt;/p&gt;</description></item></channel></rss>