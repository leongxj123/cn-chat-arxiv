<rss version="2.0"><channel><title>Chat Arxiv cs.CV</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CV</description><item><title>&#35813;&#35770;&#25991;&#22522;&#20110;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#25968;&#25454;&#26816;&#32034;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#39046;&#22495;&#27867;&#21270;&#30340;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;&#12290;&#22312;&#22810;&#27169;&#24577;&#26080;&#30417;&#30563;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#23567;&#22411;&#30340;&#28304;&#25968;&#25454;&#23376;&#38598;&#65292;&#32780;&#19981;&#26159;&#20381;&#36182;&#20016;&#23500;&#30340;&#28304;&#25968;&#25454;&#65292;&#26469;&#35299;&#20915;&#30446;&#26631;&#26631;&#31614;&#31354;&#38388;&#25968;&#25454;&#33719;&#21462;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.04416</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#25968;&#25454;&#26816;&#32034;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#27867;&#21270;&#30340;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Data Centric Approach for Unsupervised Domain Generalization via Retrieval from Web Scale Multimodal Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04416
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22522;&#20110;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#25968;&#25454;&#26816;&#32034;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#39046;&#22495;&#27867;&#21270;&#30340;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;&#12290;&#22312;&#22810;&#27169;&#24577;&#26080;&#30417;&#30563;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#23567;&#22411;&#30340;&#28304;&#25968;&#25454;&#23376;&#38598;&#65292;&#32780;&#19981;&#26159;&#20381;&#36182;&#20016;&#23500;&#30340;&#28304;&#25968;&#25454;&#65292;&#26469;&#35299;&#20915;&#30446;&#26631;&#26631;&#31614;&#31354;&#38388;&#25968;&#25454;&#33719;&#21462;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#27867;&#21270;(DG)&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#23427;&#36890;&#36807;&#21033;&#29992;&#19968;&#20010;&#25110;&#22810;&#20010;&#28304;&#39046;&#22495;&#22312;&#20849;&#20139;&#26631;&#31614;&#31354;&#38388;&#30340;&#20551;&#35774;&#19979;&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#25512;&#24191;&#21040;&#26410;&#35265;&#27979;&#35797;&#39046;&#22495;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;DG&#26041;&#27861;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#20016;&#23500;&#30340;&#30446;&#26631;&#26631;&#31614;&#31354;&#38388;&#20013;&#30340;&#28304;&#25968;&#25454;&#65292;&#36825;&#20010;&#35201;&#27714;&#22312;&#35768;&#22810;&#29616;&#23454;&#24212;&#29992;&#20013;&#22826;&#36807;&#20005;&#26684;&#65292;&#22240;&#20026;&#33719;&#21462;&#19982;&#30446;&#26631;&#20219;&#21153;&#30456;&#21516;&#30340;&#26631;&#31614;&#31354;&#38388;&#36153;&#29992;&#39640;&#26114;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22788;&#29702;&#20102;&#26080;&#30417;&#30563;&#39046;&#22495;&#27867;&#21270;(UDG)&#38382;&#39064;&#30340;&#22810;&#27169;&#24577;&#29256;&#26412;&#65292;&#35813;&#38382;&#39064;&#20351;&#29992;&#19968;&#20010;&#22823;&#22411;&#30340;&#20219;&#21153;&#26080;&#20851;&#30340;&#26410;&#26631;&#35760;&#30340;&#28304;&#25968;&#25454;&#38598;&#65292;&#20363;&#22914;LAION-2B&#22312;&#24494;&#35843;&#26399;&#38388;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#19981;&#26174;&#24335;&#22320;&#20551;&#35774;&#28304;&#25968;&#25454;&#38598;&#19982;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#20219;&#20309;&#20851;&#31995;&#12290;&#30456;&#21453;&#65292;&#23427;&#21482;&#20381;&#36182;&#20110;&#28304;&#25968;&#25454;&#38598;&#21487;&#20197;&#22312;&#32852;&#21512;&#35270;&#35273;-&#35821;&#35328;&#31354;&#38388;&#20013;&#39640;&#25928;&#25628;&#32034;&#30340;&#21069;&#25552;&#12290;&#38024;&#23545;&#36825;&#31181;&#22810;&#27169;&#24577;UDG&#35774;&#32622;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#19968;&#20010;&#23567;&#22411;&#65288;&#23567;&#20110;100K&#65289;&#30340;&#28304;&#25968;&#25454;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization (DG) is an important problem that learns a model that can generalize to unseen test domains leveraging one or more source domains, under the assumption of shared label spaces. However, most DG methods assume access to abundant source data in the target label space, a requirement that proves overly stringent for numerous real-world applications, where acquiring the same label space as the target task is prohibitively expensive. For this setting, we tackle the multimodal version of the unsupervised domain generalization (UDG) problem, which uses a large task-agnostic unlabeled source dataset, such as LAION-2B during finetuning. Our framework does not explicitly assume any relationship between the source dataset and target task. Instead, it relies only on the premise that the source dataset can be efficiently searched in a joint vision-language space. For this multimodal UDG setting, we propose a novel method to build a small ($&lt;$100K) subset of the source data in th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#23884;&#20837;&#24335;GPU&#23454;&#29616;&#30340;&#36731;&#37327;&#32423;&#31435;&#20307;&#21305;&#37197;&#31995;&#32479;-StereoVAE&#65292;&#35813;&#31995;&#32479;&#37319;&#29992;&#22522;&#20110;VAE&#30340;&#23567;&#22411;&#31070;&#32463;&#32593;&#32476;&#23545;&#20256;&#32479;&#21305;&#37197;&#26041;&#27861;&#29983;&#25104;&#30340;&#23567;&#23610;&#23544;&#31895;&#31961;&#35270;&#24046;&#22270;&#36827;&#34892;&#19978;&#37319;&#26679;&#19982;&#32454;&#21270;&#65292;&#36798;&#21040;&#20102;&#25552;&#39640;&#21305;&#37197;&#31934;&#24230;&#21644;&#20445;&#35777;&#23454;&#26102;&#22788;&#29702;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.11566</link><description>&lt;p&gt;
&#36890;&#36807;&#23884;&#20837;&#24335;GPU&#23454;&#29616;&#30340;&#36731;&#37327;&#32423;&#31435;&#20307;&#21305;&#37197;&#31995;&#32479;-StereoVAE
&lt;/p&gt;
&lt;p&gt;
StereoVAE: A lightweight stereo matching system through embedded GPUs. (arXiv:2305.11566v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#23884;&#20837;&#24335;GPU&#23454;&#29616;&#30340;&#36731;&#37327;&#32423;&#31435;&#20307;&#21305;&#37197;&#31995;&#32479;-StereoVAE&#65292;&#35813;&#31995;&#32479;&#37319;&#29992;&#22522;&#20110;VAE&#30340;&#23567;&#22411;&#31070;&#32463;&#32593;&#32476;&#23545;&#20256;&#32479;&#21305;&#37197;&#26041;&#27861;&#29983;&#25104;&#30340;&#23567;&#23610;&#23544;&#31895;&#31961;&#35270;&#24046;&#22270;&#36827;&#34892;&#19978;&#37319;&#26679;&#19982;&#32454;&#21270;&#65292;&#36798;&#21040;&#20102;&#25552;&#39640;&#21305;&#37197;&#31934;&#24230;&#21644;&#20445;&#35777;&#23454;&#26102;&#22788;&#29702;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#23884;&#20837;&#24335;GPU&#23454;&#29616;&#30340;&#36731;&#37327;&#32423;&#31435;&#20307;&#21305;&#37197;&#31995;&#32479;-StereoVAE&#65292;&#23427;&#25171;&#30772;&#20102;&#31435;&#20307;&#21305;&#37197;&#20013;&#31934;&#24230;&#21644;&#22788;&#29702;&#36895;&#24230;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#20351;&#24471;&#25105;&#20204;&#30340;&#23884;&#20837;&#24335;&#31995;&#32479;&#33021;&#22815;&#22312;&#20445;&#35777;&#23454;&#26102;&#22788;&#29702;&#30340;&#21516;&#26102;&#36827;&#19968;&#27493;&#25552;&#39640;&#21305;&#37197;&#31934;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#26500;&#24314;&#19968;&#20010;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#30340;&#23567;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#23545;&#20256;&#32479;&#21305;&#37197;&#26041;&#27861;&#29983;&#25104;&#30340;&#23567;&#23610;&#23544;&#31895;&#31961;&#35270;&#24046;&#22270;&#36827;&#34892;&#19978;&#37319;&#26679;&#19982;&#32454;&#21270;&#12290;&#36825;&#31181;&#28151;&#21512;&#32467;&#26500;&#19981;&#20165;&#21487;&#20197;&#24102;&#26469;&#20256;&#32479;&#26041;&#27861;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#20248;&#21183;&#65292;&#36824;&#21487;&#20197;&#20445;&#35777;&#31070;&#32463;&#32593;&#32476;&#30340;&#24433;&#21709;&#19979;&#30340;&#21305;&#37197;&#31934;&#24230;&#12290;&#23545;KITTI 2015&#22522;&#20934;&#27979;&#35797;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#36731;&#37327;&#32423;&#31435;&#20307;&#21305;&#37197;&#31995;&#32479;&#22312;&#25552;&#39640;&#30001;&#19981;&#21516;&#31639;&#27861;&#29983;&#25104;&#30340;&#31895;&#31961;&#35270;&#24046;&#22270;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#39640;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#22312;&#23884;&#20837;&#24335;GPU&#19978;&#23454;&#26102;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a lightweight system for stereo matching through embedded GPUs. It breaks the trade-off between accuracy and processing speed in stereo matching, enabling our embedded system to further improve the matching accuracy while ensuring real-time processing. The main idea of our method is to construct a tiny neural network based on variational auto-encoder (VAE) to upsample and refinement a small size of coarse disparity map, which is first generated by a traditional matching method. The proposed hybrid structure cannot only bring the advantage of traditional methods in terms of computational complexity, but also ensure the matching accuracy under the impact of neural network. Extensive experiments on the KITTI 2015 benchmark demonstrate that our tiny system exhibits high robustness in improving the accuracy of the coarse disparity maps generated by different algorithms, while also running in real-time on embedded GPUs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#31639;&#27861;&#20998;&#26512;&#20102;&#26469;&#33258;15&#20010;&#19981;&#21516;&#22269;&#23478;&#30340;220&#20010;&#25919;&#27835;&#39046;&#34966;&#30340;YouTube&#35270;&#39057;&#65292;&#24635;&#32467;&#20102;&#25919;&#27835;&#39046;&#34966;&#38754;&#37096;&#24773;&#24863;&#34920;&#36798;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2304.09914</link><description>&lt;p&gt;
&#26623;&#23376;&#25919;&#27835;&#30340;&#38754;&#23380;&#65306;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27604;&#36739;&#25919;&#27835;&#39046;&#34966;&#38754;&#37096;&#24773;&#24863;&#34920;&#36798;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
The Face of Populism: Examining Differences in Facial Emotional Expressions of Political Leaders Using Machine Learning. (arXiv:2304.09914v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#31639;&#27861;&#20998;&#26512;&#20102;&#26469;&#33258;15&#20010;&#19981;&#21516;&#22269;&#23478;&#30340;220&#20010;&#25919;&#27835;&#39046;&#34966;&#30340;YouTube&#35270;&#39057;&#65292;&#24635;&#32467;&#20102;&#25919;&#27835;&#39046;&#34966;&#38754;&#37096;&#24773;&#24863;&#34920;&#36798;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#23186;&#20307;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#25919;&#27835;&#20449;&#24687;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#20256;&#25773;&#21644;&#28040;&#36153;&#26041;&#24335;&#65292;&#36825;&#31181;&#36716;&#21464;&#20419;&#20351;&#25919;&#27835;&#20154;&#29289;&#37319;&#21462;&#26032;&#30340;&#31574;&#30053;&#26469;&#25429;&#25417;&#21644;&#20445;&#25345;&#36873;&#27665;&#30340;&#27880;&#24847;&#21147;&#12290;&#36825;&#20123;&#31574;&#30053;&#24448;&#24448;&#20381;&#36182;&#20110;&#24773;&#24863;&#35828;&#26381;&#21644;&#21560;&#24341;&#12290;&#38543;&#30528;&#34394;&#25311;&#31354;&#38388;&#20013;&#35270;&#35273;&#20869;&#23481;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#24456;&#22810;&#25919;&#27835;&#27807;&#36890;&#20063;&#34987;&#26631;&#24535;&#30528;&#21796;&#36215;&#24773;&#24863;&#30340;&#35270;&#39057;&#20869;&#23481;&#21644;&#22270;&#20687;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#22522;&#20110;&#29616;&#26377;&#35757;&#32451;&#22909;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25552;&#20379;&#30340;Python&#24211;fer&#65292;&#24212;&#29992;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#31639;&#27861;&#65292;&#23545;&#25551;&#32472;&#26469;&#33258;15&#20010;&#19981;&#21516;&#22269;&#23478;&#30340;&#25919;&#27835;&#39046;&#34966;&#30340;220&#20010;YouTube&#35270;&#39057;&#26679;&#26412;&#36827;&#34892;&#20998;&#26512;&#12290;&#35813;&#31639;&#27861;&#36820;&#22238;&#24773;&#32490;&#20998;&#25968;&#65292;&#27599;&#19968;&#24103;&#37117;&#20195;&#34920;6&#31181;&#24773;&#32490;&#29366;&#24577;&#65288;&#24868;&#24594;&#65292;&#21388;&#24694;&#65292;&#24656;&#24807;&#65292;&#24555;&#20048;&#65292;&#24754;&#20260;&#21644;&#24778;&#35766;&#65289;&#21644;&#19968;&#20010;&#20013;&#24615;&#34920;&#24773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online media has revolutionized the way political information is disseminated and consumed on a global scale, and this shift has compelled political figures to adopt new strategies of capturing and retaining voter attention. These strategies often rely on emotional persuasion and appeal, and as visual content becomes increasingly prevalent in virtual space, much of political communication too has come to be marked by evocative video content and imagery. The present paper offers a novel approach to analyzing material of this kind. We apply a deep-learning-based computer-vision algorithm to a sample of 220 YouTube videos depicting political leaders from 15 different countries, which is based on an existing trained convolutional neural network architecture provided by the Python library fer. The algorithm returns emotion scores representing the relative presence of 6 emotional states (anger, disgust, fear, happiness, sadness, and surprise) and a neutral expression for each frame of the pr
&lt;/p&gt;</description></item></channel></rss>